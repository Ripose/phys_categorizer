_________________________________________________________________________________________
Insight
Lessons From the Physics-Education Reform Effort1
Richard R. Hake
_________________________________________________________________________________________

Indiana University
_________________________________________________________________________________________

•  Abstract
• 
Introduction
•  Survey Summary
•  Fourteen Lessons
_________________________________________________________________________________________

ABSTRACT
Several years ago I reported a survey (Hake 1998a,b,c) of pre/post test data for 62 introductory physics
courses enrolling a total of 6542 students.  The present article provides a summary of that survey and
presents fourteen lessons from the physics-education reform effort that may assist the general
upgrading of education and science literacy.

KEY WORDS: physics education, education reform, education research, interactive engagement,
science literacy, cognitive science.
_________________________________________________________________________________________

I. INTRODUCTION

For over three decades, physics-education researchers repeatedly showed that Traditional (T)
introductory physics courses with passive-student lectures, recipe labs, and algorithmic problem exams
were of limited value in enhancing conceptual understanding of the subject (McDermott & Redish
1999). Unfortunately, this work was largely ignored by the physics and education communities until
Halloun & Hestenes (HH) (1985a,b) devised the “Mechanics Diagnostic” (MD) test of conceptual
understanding of Newtonian mechanics. Among the virtues of the MD, and the subsequent “Force
Concept Inventory” (FCI) (Hestenes et al. 1992, Halloun et al. 1995) tests, are (a) the multiple-choice
format facilitates relatively easy administration of the tests to thousands of students, (b) the questions
probe for conceptual understanding of basic concepts of Newtonian mechanics in a way that is
understandable to the novice who has never taken a physics course (and thus can be given as an
introductory-course pre-test), while at the same time rigorous enough for the initiate. A typical HH-type
question is as follows (an actual HH question is avoided to help preserve the confidentiality of the test):

                                                  
1 Submitted on 3/25/01 to Conservation Ecology < http://www.consecol.org/Journal/ >, a “peer-
      reviewed journal of integrative science and fundamental policy research.”

1

A student in a lab holds a brick of weight W in her outstretched horizontal palm and lifts the brick
vertically upward at a constant speed.  While the brick is moving vertically upward at a constant speed,
the magnitude of the force on the brick by the student’s hand is:

A. constant in time and zero.
B. constant in time, greater than zero, but less than W.
C. constant in time and W.
D. constant in time and greater than W.
E. decreasing in time but always greater than W.

Note that the responses include as distractors not only “D,” the common Aristotelian misconception
that “motion requires a net force,” but also other less common student misconceptions “A” and “E”
that might not be known to traditional teachers. Unfortunately, too few teachers “shut up and listen to
their students” so as to find out what they are thinking (Arons 1981).  The distractors are based on my
years of listening to students as they worked through the experiments in Socratic Dialogue Inducing
Lab #1 “Newton’s First and Third Laws” (Hake 2001a). For actual HH questions the distractors were
usually gleaned through careful qualitative research involving interviews with students and the
analysis of their oral and written responses to mechanics questions.

Using the MD, Halloun & Hestenes (1985a,b) published a careful study using massive pre- and post-
course testing of students in both calculus and non-calculus-based introductory physics courses at
Arizona State University, and concluded that: (1) “. . . . the student’s initial qualitative, common-sense
beliefs about motion and . . . .(its) . . . .  causes have a large effect on performance in physics, but
conventional instruction induces only a small change in those beliefs.”  (2) Considering the wide
differences in the teaching styles of the four professors . . . .  (involved in the study) . . . . the basic
knowledge gain under conventional instruction is essentially independent of the professor.” These
outcomes were consistent with work done prior to the HH study as recently reviewed by McDermott &
Redish (1999).

The HH results stimulated a flurry of research and development aimed at improving introductory
mechanics courses. Most of the courses so generated sought to promote conceptual understanding
through use of pedagogical methods published by physics-education researchers (see, e.g., Physical
Science Resource Center 2001, Galileo Project 2001, UMd-PERG 2001a).  These methods are usually
based on the insights of cognitive science (Gardner 1985; Mestre & Touger 1989; Redish 1994; Bruer
1994, 1997; Bransford et al. 1999; Donovan et al. 1999) and/or outstanding classroom teachers (e.g.,
Karplus 1977; Minstrell 1989; Arons 1990; McDermott 1991, 1993; Fuller 1993; Reif 1995; Wells et
al. 1995; Zollman 1996; Laws 1997).  Although the methods differ in detail, they all attempt to guide
students to construct their understandings by heads-on (always) and hands-on (usually) activities that
yield immediate feedback through discussion with peers and/or instructors [Interactive Engagement
(IE)], so as to finally arrive at the viewpoint of the professional physicist.

2

The survey summarized below documents some of the successes and failures of courses employing IE
methods, may assist a much needed further improvement in introductory mechanics instruction in the
light of practical experience, and may serve as a model for promoting educational reform in other
disciplines.  [Since the summary omits some important aspects, serious education scholars are urged to
consult the original sources (Hake 1998 a,b,c).] I then present fourteen somewhat subjective lessons
from my own interpretation of the physics-education reform effort with the hope that they may assist
the general upgrading of education and science literacy.

_________________________________________________________________________________________

2. SURVEY SUMMARY

Starting in 1992, I requested that pre/post-FCI data and post-test Mechanics Baseline (a problem-
solving test due to Hestenes & Wells, 1992) data be sent to me. Since instructors are more likely to
report higher-gain courses, the detector is biased in favor of those courses, but can still answer a
crucial research question: Can the use of Interactive Engagement (IE) methods increase the
effectiveness of introductory mechanics courses well beyond that obtained by traditional methods?

The Data
Figure 1 shows data from the survey (Hake 1998a,b,c) of 62 introductory physics courses enrolling a
total 6542 students.  The data are derived from pre/post scores of the MD and FCI tests indicated
above, recognized for high validity and consistent reliability (Beichner 1994, Slavin 1992). Average
pre/post test scores, standard deviations, instructional methods, materials used, institutions, and
instructors for each of the survey courses are tabulated and referenced in Hake, 1998b. The latter paper
also gives case histories for the seven IE courses whose effectiveness as gauged by pre-to-post test
gains was close to those of T courses, advice for implementing IE methods, and suggestions for further
research. Various criticisms of the survey (and physics-education research generally) are countered by
Hake, 1998c.   

For survey classification and analysis purposes I operationally defined:

a.  Interactive Engagement (IE) methods as those designed at least in part to promote conceptual
understanding through interactive engagement of students in heads-on (always) and hands-on
(usually) activities which yield immediate feedback through discussion with peers and/or
instructors, all as judged by their literature descriptions;

b. IE courses as those reported by instructors to make substantial use of IE methods;

c.  Traditional (T) courses as those reported by instructors to make little or no use of IE methods,
relying primarily on passive-student lectures, recipe labs, and algorithmic-problem exams.

3

Fig. 1.  The %<Gain> vs %<Pretest> score for 62 courses enrolling a total of 6542 students.
Here %<Gain> = %<posttest> – %<pretest>, where the angle brackets “<....>” indicate an
average over all students in the course.  Points for high school (HS), college (COLL), and
university (UNIV) courses are shown in green for Interactive Engagement (IE), and in red for
Traditional (T) courses. The straight negative-slope lines are lines of constant average
normalized gain <g>. The two dashed purple lines show that most IE courses achieved <g>’s
between 0.34 and 0.69.  The definition of <g>, and its justification as an index of course
effectiveness, is discussed in the text.  The average of <g>’s for the 48 IE courses is <<g>>48IE =
0.48 ± 0.14 (standard deviation) while the average of <g>’s for the 14 T courses is <<g>>14T =
0.23 ± 0.04 (sd).  Here the double angle brackets “<<....>>” indicate an average of averages.

4

A histogram of the data of Fig. 1 is shown in Fig. 2.

T  Courses

IE  Courses

Fraction  of 
Courses

0 . 5

0 . 4

0 . 3

0 . 2

0 . 1

0

0 . 0 8 0 . 1 6 0 . 2 4 0 . 3 2

0 . 4

0 . 4 8 0 . 5 6 0 . 6 4 0 . 7 2

< g >

Fig. 2.  Histogram of the average normalized gain <g>: red bars show the fraction of 14 Traditional
    (T) courses (2108 students) and green bars show the fraction of 48 Interactive Engagement (IE)
    courses (4458 students), both within bins of width  <g> = 0.04, centered on the <g> values shown.
    (From Hake 1998a.)

Average Normalized Gain
To understand the graphical interpretation of the average normalized gain <g> of Figs. 1 and 2,
consider the point [%<pretest> = 44%, %<Gain> = 19%] at the tip of the white arrowhead in Fig. 1.
This point has an abscissa (100% - 44%) = 56% and ordinate 19%.  The absolute value of the slope s
of the purple dashed line connecting this point to the lower right vertex of the graph is |s|  = 19%/56%
= 0.34.  This absolute slope |s|  = %<Gain>/(100% - %<pretest>) =  %<Gain>/ (maximum possible %
<Gain>) = %<Gain>/ %<Gain>max is taken to be an index of that course’s effectiveness (as justified
below – see Conclusion “A” ), and is called the average normalized gain <g> for that particular course.
Thus, all courses with points close to the purple dashed line are judged to be of about equal average
effectiveness, regardless of their average pretest scores.  A similar calculation for the point
[%<pretest> = 32%, %<Gain> = 47%] at the tip of the blue arrowhead, yields <g> = 0.69.  The
maximum of value of <g> occurs when the <Gain> is equal to  %<Gain>max and is therefore 1.00, as
shown in Fig. 1.

5

Popular Interactive Engagement Methods

For the 48 interactive-engagement courses of Figs. 1 & 2, the ranking in terms of number of IE courses
using each of the more popular methods follows.  [See the paragraph below the listing for an
explanation of the abbreviations within the “{…}” ].
(1) Collaborative Peer Instruction (Johnson et al. 1991; Heller et al. 1992a,b): 48 (all courses)
       {CA}.
(2) Microcomputer-Based Labs (Thornton and Sokoloff 1990, 1998): 35 courses {DT}.
(3) Concept Tests (Mazur 1997, Crouch & Mazur 2001): 20 courses {DT}; such tests for physics,
        biology, and chemistry are available on the web along with a description of the Peer Instruction
        method at the Galileo Project (2001).
(4) Modeling (Halloun & Hestenes 1987; Hestenes 1987, 1992; Wells et al. 1995): 19 courses
       {DT + CA}; a description is on the web at < http://modeling.la.asu.edu/ >.
(5) Active Learning Problem Sets or Overview Case Studies (Van Heuvelen 1991a,b; 1995):
       17 courses {CA}; information on these materials is online at
        < http://www.physics.ohio-state.edu/~physedu/ >.
(6) Physics-education-research based text (referenced in Hake 1998b, Table II) or no text: 13 courses.
(7) Socratic Dialogue Inducing Labs (Hake 1987, 1991, 1992; Tobias & Hake 1988): 9 courses
       {DT +  CA}; a description and lab manuals are on the web at the Galileo Project (2001) and
       < http://www.physics.indiana.edu/~sdi >.

The notations within the curly brackets “{ . . .}” follow Heller (1999) in loosely associating the
methods with “learning theories” from cognitive science. Here “DT” stands for “Developmental
Theory,” originating with Piaget (Inhelder & Piaget 1958, Gardner 1985); and “CA” stands for
“Cognitive Apprenticeship” (Collins et al. 1989, Brown et al. 1989).  All the methods (save #6)
recognize the important role of social interactions in learning (Vygotsky 1978, Lave & Wenger 1991,
Dewey 1997).  It should be emphasized that the above rankings are by popularity within the survey,
and have no necessary connection with the effectiveness of the methods relative to one another. In fact,
it is quite possible that some of the less popular methods used in some survey courses, as listed by
Hake (1998b), could be more effective in terms of promoting student understanding than any of the
above popular strategies.

6

Conclusions of the Survey
The conclusions of the survey (Hake 1998 a,b,c; 1999a) may be summarized as follows:

A. The average normalized gain <g> affords a consistent analysis of pre/post test data on
conceptual understanding over diverse student populations in high schools, colleges, and
universities. The correlation of the average normalized gain <g> with (%<pretest>) for the 62
courses of Fig. 1 (Hake 1998a) is a very low +0.02.  This constitutes an experimental justification
for the use of <g> as a comparative measure of course effectiveness over diverse student
populations with widely varying average pretest scores.  In contrast, the average posttest score
(%<postest >) and the average actual gain (%<Gain>) are less suitable for comparing course
effectiveness over diverse groups since their correlation with (%<pretest) is significant.  The
correlation of
                       (%<posttest>) with (%<pretest>) is + 0.55, and the correlation of
                            (%<Gain>) with (%<pretest>) is – 0.49,
both of which correlations would be anticipated.   Note that in the absence of instruction, a high
positive correlation of (%<posttest >) with (%<pretest>) would be expected.  The successful use of
the normalized gain for the analysis of pre/post test data in this and other physics-education research
(see “Is it Scientific Research” below), calls into question the common dour appraisals of pre/post
test designs (Cook & Campbell 1979, Cronbach & Furby 1970).

B.  Fourteen Traditional (T) courses (2084 students) of the survey yielded <<g>>14T =
0.23 ± 0.04sd. Considering the elemental nature of the MD/FCI questions (many physics teachers
regard them as too easy to be used on examinations) and the definition <g>  =
%<Gain>/ %<Gain>max, this suggests that traditional (T) courses fail to convey much basic
conceptual understanding of Newtonian mechanics to the average student.

C.  Forty-eight Interactive Engagement (IE) courses (4458 students)  of the survey yielded
<<g>>48IE = 0.48 ± 0.14sd. The <<g>>48IE is over twice that of <<g>>14T, and is almost two sd’s of
<<g>>48IE above that of the T courses, reminiscent of differences seen in comparing instruction
delivered to students in large groups with one-on-one instruction (Bloom 1984). This suggests that
IE courses can be much more effective than T courses in enhancing conceptual
understanding. Although it was not possible in this survey to randomly assign students from a
single large homogeneous population to the T and IE courses, the T and IE data are all drawn from
the same institutions and the same generic introductory program regimes (Hake 1998b).  Thus it
seems very unlikely that the nearly-two-sd difference between <<g>>’s for the IE and T courses
could be accounted for by differences in the student populations.

7

An alert critic of an early draft [and more recently Becker (2001) – see below] have pointed out that
the <<g>> difference might be due in part to the apparent smaller average enrollment for IE courses
(4458/48 = 93) than for T courses (2084/14 = 149).  However such calculation of average class size
is invalid because in several cases (Hake, 1998a,b) classes of fairly homogeneous instruction and
student population were combined into one “course” whose <g> was calculated as a number-of-
student-weighted average of the <g>’s of the individual classes.  A correct calculation yields an
average class enrollment of 4458/63 = 71 for IE classes and 2084/34 = 61 for T classes, so the
average class sizes are quite similar.

D.  A detailed analysis of random and systematic errors has been carried out (Hake 1998a) but will
not be repeated here. It was concluded that “it is extremely unlikely that random or systematic
error plays a significant role in the nearly two-standard deviation difference in the <<g>>’s of
T and IE courses.”

E.  Conclusions “A” and “C” are bolstered by an analysis of the Fig. 1 data in terms of the “effect
size” <d> (Hake 1999a).  The effect size is commonly used in meta-analyses (Cohen 1988, Hunt
1997), and strongly recommended by many psychologists (B. Thompson 1996, 1998, 2000), and
biologists (Johnson 1999, Anderson et al. 2000, W.L. Thompson 2001) as the preferred alternative
to the usual t-tests and p values associated with null-hypothesis testing. (See also Anderson 1998.)
Here <d> is defined as the ratio of the actual average gain (%<posttest> - %<pretest>) to the
average of the standard deviations (sd’s).  I obtain an average effect size <d> = 0.88 for 9 T
courses (1620 students), and an average <d> = 2.18 for 24 IE courses (1843 students) for
which sd’s are available.  The latter can be compared with: (1) the similar <d> = 1.91 reported by
Zeilik, Schau, Mattern (1998) for a single IE introductory astronomy course (N = 221) given in
Spring 1995 at the University of New Mexico, and (2) the much smaller average <d> = 0.51
obtained in a meta-analysis of small-group learning by Springer et al. (1999).  In the Springer study,
as for much research reported in the educational literature, (a) in many cases there was no pretesting
to disclose initial knowledge states of the test or control groups, (b) the quality of the “achievement
tests” was not critically examined (were they of the plug-in-regurgitation type so common in
introductory physics courses?).  I think that the Springer et al. meta-analysis probably understates
the effectiveness of small-group learning in advancing conceptual understanding and problem-
solving ability.

As in “C” above, the critic of an early draft pointed out that <d> difference might be due in part to
the apparent smaller average enrollment for IE courses (1843/24 = 77) than for T courses (1620/9 =
169).  However such calculation of average class size is invalid for the reason given in “C.”  A
correct calculation of average class size indicates an average class enrollment of 1843/39 = 47 for
IE classes and 1620/31 = 52 for T classes, so the average class enrollments are quite similar.

8

F.  Considering the elemental nature of the MD/FCI tests, current IE methods and their
implementation need to be improved, since none of the IE courses achieves <g> greater than
0.69.  In fact, as can be seen in Figs. 1, seven of the IE courses (717 students) achieved <g>’s close
to those of the T courses.  Case histories of the seven low-<g> courses (Hake 1998b) suggest that
implementation problems occurred that might be mitigated by:

 (1) apprenticeship education of instructors new to IE methods,
 (2) emphasis on the nature of science and learning throughout the course,
 (3) careful attention to motivational factors and the provision of grade incentives for taking
         IE_activities seriously,
 (4) recognition of and positive intervention for potential low-gain students,
 (5) administration of exams in which a substantial number of the questions probe the degree of
         conceptual understanding induced by the IE methods,
 (6) use of IE methods in all components of a course and tight integration of those components.

Personal experience with the Indiana IE courses and communications with most of the IE
instructors in the survey suggest that similar implementation difficulties probably occurred to a
greater or lesser extent in all the IE courses and are probably partially responsible for the wide
spread in the <g>’s, apparent for IE courses in Figs. 1 & 2.

G.  I have plotted (Hake 1998a) average post-course scores on the problem-solving Mechanics
Baseline test (Hestenes & Wells, 1992) [available for 30 (3259 students) of the 62 courses of the
survey] vs those on the conceptual FCI.  There is a very strong positive correlation r = + 0.91 of the
MB and FCI scores.  This correlation and the comparison of IE and T courses at the same institution
(Hake 1998a) imply that IE methods enhance problem-solving ability.

9

Criticisms of the Survey.
Early criticisms of the survey have been countered in Hake 1998c.  Becker (2001) has recently raised
other objections:

1. “The amount of variability around a mean test score for a class of 20 students versus a mean of
200 students cannot be expected to be the same. Estimation of a standard error for sample of 62. . .
(courses) . . . , where each of the 62 receives an equal weight ignores this heterogeniety.”  But only
the standard deviations (sd’s) of the <<g>>’s for the 48 IE and 14T courses were given (not the
standard errors).  For example, I  give <<g>>48IE = 0.48 ± 0.14 (sd). The spread (sd) in <g> values
for the IE courses is large, 29% of <<g>>48IE.  In the error analysis (Hake 1998a), I adduce evidence
that the large spread in the <g> IE distribution is due to random errors plus other factors: e.g., course
to course variations in the systematic errors and in the effectiveness of the pedagogy and/or
implementation.  In my opinion, attempts to take into account only the heterogeniety due to course
enrollment would add little to the analysis. The crucial point, seemingly ignored by Becker, is that
the difference (<<g>>48IE  — <<g>>14T) is almost two sd’s of <<g>>48IE , and therefore large in
comparison to the spread in the data; more technically, the “effect size” is large as emphasized in
“C” and “E” above.

2. “Unfortunately, the gap closing outcome measure g is algebraically related to the starting position
of the student as reflected in the pretest: g falls as the pretest score rises, for maximum score >
posttest score > pretest score.”  This assertion is based on Becker’s partial differentiation ∂g/∂y =
(x – Q)/(Q – y)2 , in agreement with Hake, 1998a, footnote 45.  Here Q is the number of questions
on the exam, y and x are the number of correct responses on the pretest and posttest, respectively,
and g = (x – y)/(Q – y). But Becker’s differentiation, while mathematically correct, has little
physical significance because for a single student x is not, in general, independent of y; and for a
single class the average <x> is not independent of the average <y>.  In fact, as indicated above, for
the 62 courses of the survey, correlation of the average posttest score <x> with the average pretest
score <y> is +0.55, while the correlation of average normalized gain <g> with <y> is a very low
+0.02.  [As an aside, Becker relates g to the “Tobit model” (named after economics Nobel laureate
James Tobin) and implies that economist Frank Ghery (1972) was the first to propose use of g,
evidently unaware (as was I) that g was earlier used by psychologists Hovland et al. (1949).]

3. “When studies ignore the class size . . . (see my counter to this in “C” above under “Conclusions
of the Survey”) . . . and sample selection issues, readers should question the study’s findings
regardless of the sample size or diversity in explanatory values. . . .   Hake does not give us any
indication of beginning versus ending enrollments, which is critical information if one wants to
address the consequences of attrition.”  Becker is either unaware or has chosen to ignore ref. 17a
(the same as Hake 1998b) of Hake (1998a).  The data Tables Ia,b,c of Hake 1998b clearly indicate

10

which courses were and which were not analyzed with "matched" data, i.e., data in which only
posttest scores of students who had also taken the pretest were included in the average posttest
score.  In a majority of courses matched data were used.  Tables Ia,b,c show no obvious dependence
of <g> on whether or not the data were matched. In footnote “c” of that table, I estimate, from my
experience with the pre/post testing of 1263 students at Indiana University, "that the error in the
normalized gain is probably less than 5% for classes with 20 – 50 students and less that 2% for
classes with more than 50 students.”  Saul (1998) on page 117 states ". . . . I found that the matched
and unmatched results . . . from his extensive pre/post FCI studies . . . .  are not significantly
different."

4.  “. . . . there is relatively strong inferential evidence . . . [evidently from Almer et al. (1998) and
Chizmar & Ostrosky (1999)] . . .  supporting the hypothesis that periodic use of variants of the one-
minute paper (wherein an instructor stops class and asks each student to write down what he or she
thought was the key point and what still needed clarification at the end of a class period) increases
student learning. Similar support could not be found for other methods.. . .”  (My italics.) In
Becker’s econocentric view, all quantitative educational research, including that done over the past
30 years in physics (Redish & McDermott 1998), has evidently been in vain, save for two isolated
recent studies by economists on the one-minute paper! Becker’s “11-point set of criteria that all
inferential studies can be expected to address in varying degrees of detail” omits what to most
physical scientists is the most crucial criterion: the extent to which the conclusions are
independently verified by other investigators under other circumstances so as to contribute to a
community map. (See “Is it Scientific Research” below.)

As an aside, my own experience with minute papers (Hake 1998b, ref. 58 and Table IIc) is that they
can constitute a significant but relatively minor segment of effective interactive engagement. Becker
continues the usual literature misattribution of minute papers to Wilson (1986) [and indirectly to
CAT champions Angelo and Cross (1993)] rather than to Berkeley physicist Charles Schwartz.
Becker is evidently either unaware or chooses to ignore ref. 58 of Hake, 1998b. (See also the
footnote at the reference to Davis et al. 1983 in Hake 2000d.)

11

Is it Scientific Research?
There has been a long standing debate over whether or not education research is or should be
“scientific” (Lagemann 2000, Mayer 2000, Phillips & Burbules 2000, Phillips 2000, Eisner 1997,
Dewey 1929). In my opinion, substantive education research must be “scientific.” My biased
prediction (Hake 2000b) is that for physics-education research, and possibly even education research
generally: (a) the bloody “paradigm wars” (Gage 1989) will have ceased by the year 2009, with, in
Gage’s words, a “productive rapprochement of the paradigms,” (b) some will follow paths of
pragmatism or Popper’s “piecemeal social engineering” to this paradigm peace, as suggested by Gage,
but (c) most will enter onto this “sunlit plain” from the path marked “scientific method” as practiced
by most research physicists:

(1) EMPIRICAL: Systematic investigation ...... (by quantitative, qualitative, or any other means)
         ......... of nature to find reproducible patterns in the structure of things and the ways they
         change (processes).
(2) THEORETICAL: Construction and analysis of models representing patterns of nature.
         (Hestenes 1999).

     (3) Continual interaction, exchange, evaluation, and criticism so as to build a . . . . community map
               (Redish 1999).

For the presently discussed research, the latter feature is demonstrated by the fact that FCI normalized
gain results for IE and T courses that are consistent with those of (Hake 1998a,b,c) have now been
obtained by physics-education research groups at the Univ. of Maryland (Redish et al. 1997, Saul
1998, Redish & Steinberg 1999, Redish 1999); Univ. of Montana (Francis et al. 1998); Rennselaer and
Tufts (Cummings et al. 1999); North Carolina State Univ. (Beichner et al. 1999);  and Hogskolan
Dalarna - Sweden (Bernhard 1999).  Thus in physics education research, just as in traditional physics
research, it is possible to perform quantitative experiments that can be reproduced (or refuted) by other
investigators and thus contribute to the construction of a “community map.”

12

_________________________________________________________________________________________

III. FOURTEEN LESSONS FROM THE PHYSICS-EDUCATION REFORM EFFORT

The lessons (L) below are derived from my own interpretation of the physics-education reform
movement and are therefore somewhat subjective and incomplete. They are meant to stimulate
discussion rather than present any definitive final analysis.
L1. The use of IE strategies can increase the effectiveness of conceptually difficult courses well
beyond that obtained with traditional methods.

Education research in biology (Hake 1999b,c), chemistry (Herron & Nurrenbern 1999), and
engineering (Felder et al. 2000a,b), although neither as extensive nor as systematic as that in
physics (McDermott & Redish 1999, Redish 1999), is consistent with the latter in suggesting that in
conceptually difficult areas, interactive engagement methods are more effective than traditional
passive-student methods in enhancing students’ understanding.  I see no reason to doubt that such is
not also the case in other science and even non-science areas.

L2. The use of interactive-engagement and/or high-tech methods, by themselves, does not insure
superior student learning.

As previously indicated, the data of Fig. 1 show that seven of the IE courses (717 students) achieved
<g>’s close to those of the T courses.  Five of those made extensive use of high-tech
microcomputer-based labs (Thornton and Sokoloff 1990, 1998). Case histories of the seven low-
<g> courses (Hake 1998b) suggest that implementation problems occurred.

Another example of the apparent failure of IE/high-tech methods has been described by Cummings
et al. (1999).   They considered a standard physics Studio Course at Rensselaer in which  group
work and computer use had been introduced as components of in-class instruction, the classrooms
appeared to be interactive, and students seemed to be engaged in their own learning. Their
measurement of <g>’s using the FCI and the Force Motion Concept Evaluation (Thornton &
Sokoloff 1998) yielded values close to those characteristic of T courses (Hake 1998a,b,c).
Cummings et al. suggest that the low <g> of the standard Rensselaer studio course may have been
due to the fact that “the activities used in the studio classroom are predominately ‘traditional’
activities adapted to fit the studio environment and incorporate the use of computers.” Thus the
apparent “interactivity” was a product of traditional methods (supported by high technology), not
published IE methods developed by physics-education researchers or outstanding teachers, as for
the survey courses.  This explanation is consistent with the fact that Cummings et al. measured
<g>’s  in the 0.35 – 0.45 range for Rensselaer Studio courses using physics-education research
methods: (a) Interactive Lecture Demonstrations (Thornton & Sokoloff (1998), and (b) Cooperative
Group Problem Solving (Heller et al. 1992a,b) .

13

It should be emphasized that while high technology, by itself, is no panacea, it can be very
advantageous when it promotes interactive engagement, as in computerized classroom
communication systems (see, e.g., Mazur, 1997), properly implemented microcomputer-based labs
(Thornton and Sokoloff 1990), and Just-In-Time Teaching (Novak et al. 1998, 1999).

L3. Teachers who possess both content knowledge and “pedagogical content knowledge” are
more apt to deliver effective instruction.

“Pedagogical content knowledge” is evidently a term due to Shulman (1986, 1987), but its
importance has long been well known to effective classroom teachers. The difference between
content knowledge and “pedagogical content knowledge,” can be illustrated by consideration of the
HH-type question given in the Introduction. Content knowledge informs the teacher that, according
to Newton’s First Law, while the brick is moving vertically upward at a constant speed in the
inertial reference frame of the lab, the magnitude of the force on the brick by the student’s hand is
constant in time and of magnitude W, so that the net force on the brick is zero. On the other hand,
pedagogical content knowledge would inform the teacher that students may think that e.g.: (a) since
a net force is required to produce motion, the force on the brick by the student’s hand is constant in
time and greater than W; or (b) since the weight of the brick diminishes as it moves upward away
from the Earth, the force on the brick by the student’s hand decreases in time but is always greater
than W; or (c) no force is exerted on the brick by the student’s hand because as the students hand
moves up the brick must simply move up to stay out of the hand’s way.  In addition, pedagogical
content knowledge provides a hard-won toolkit of strategies (see, e.g., the list of “Popular IE
Methods” in Sec. 2 above) for guiding the student away from these misconceptions and towards the
Newtonian interpretation.  In my opinion, the need for teachers to possess pedagogical content
knowledge is strikingly confirmed by the marked differences in normalized gains <g> for traditional
and IE-oriented physics teachers (Hake 1998a). Unfortunately, such knowledge may take many
years to acquire (Wells et al. 1995).

L4. Faculty tend to overestimate the effectiveness of their own instructional efforts and thus tend
to see little need for educational reform.

As examples of this tendency see Geilker (1997) [countered by Hilborn (1998)]; Griffiths (1997)
[countered by Hestenes (1998)]; Goldman (1998); Mottman (1999a,b) [countered by Kolitch
(1999), Steinberg (1999), and Hilborn (1999)]; and Carr (2000).

L5.  Such complacency can sometimes be countered by the administration of high-quality
standardized tests of understanding and by “video snooping.”

a.  Harvard’s Eric Mazur (1997) was very satisfied with his introductory-course teaching - he
received very positive student evaluations and his students did reasonably well on “difficult” exam
problems. Thus it came as a shock when his students fared hardly better on the “simple” FCI than

14

on their “difficult” midterm exam.  As a result, Mazur developed and implemented his interactive-
engagement “Peer Instruction” method as a replacement for his previous traditional passive-student
lectures. This change resulted in much higher <g>’s on the FCI as shown by comparison of the red
and green triangular points with average pretest scores in the vicinity of 70% in Fig. 1.

b.  Like Mazur, most Harvard faculty members are proud of their undergraduate science courses.
However, the videotape Private Universe (Schneps & Sadler 1985) shows Harvard graduating
seniors being asked “What causes the seasons?” Most of them confidently explain that the seasons
are caused by yearly changes in the distance between the Sun and the Earth! Similarly most MIT
faculty regard their courses as very effective preparation for the difficult engineering problems that
will confront their elite graduates in professional life. However the videotape “Simple Minds”
(Shapiro et al. 1997) shows MIT graduating seniors having great trouble getting a flashlight bulb to
light, given one bulb, one battery, and one piece of wire.

L6. High-quality standardized tests of the cognitive and affective impact of courses are essential
for gauging the relative effectiveness of non-traditional educational methods.

As indicated in the introduction, so great is the inertia of the educational establishment (see L13)
that three decades of physics-education research demonstrating the futility of the passive-student
lecture in introductory courses were ignored until high-quality standardized tests that could easily
be administered to thousands of students became available.  These tests are yielding increasingly
convincing evidence that interactive engagement methods enhance conceptual understanding and
problem solving abilities far more than do traditional methods. Such tests may also indicate
implementation problems in IE courses (Hake 1998b) and differences in the effectiveness of various
IE methods (Saul 1998, Redish 1999). As far as I know, disciplines other than physics and
astronomy (Adams et al. 2000; Zeilik et al. 1997, 1998, 1999) have yet to develop any such tests
and therefore cannot effectively gauge either the need for or the efficacy of their reform efforts. In
my opinion, all disciplines should consider the construction of high-quality standardized tests of
essential introductory course concepts.

The lengthy and arduous process of constructing valid and reliable multiple choice tests has been
discussed by Halloun & Hestenes (1985a), Hestenes et al. (1992), Beichner (1994), Aubrecht
(1991), and McKeachie (1999). In my opinion such hard-won Diagnostic Tests that cover important
parts of common introductory courses are national assets whose confidentiality should be as well
protected as the MCAT (Medical College Admission Test). Otherwise the test questions may
migrate to student files and thereby undermine education research that relies upon the validity of
such tests. Suggestions for both administering Diagnostic Tests and reporting their results so as to
preserve confidentiality and enhance assessment value have been given by Hake (2001b).

15

Regarding tests of affective impact, administration of the “Maryland Physics Expectations” (MPEX)
survey to 1500 students in introductory calculus-based physics courses in six colleges and
universities . . . . (showed). . . . “a large gap between the expectations of experts and novices and . . .
.  a tendency for student expectations to deteriorate rather than improve as a result of introductory
calculus-based physics” (Redish et al. 1998).   Here the term “expectations” is used to mean a
combination of students’ epistemological beliefs about learning and understanding physics and
students’ expectations about their physics course (Elby 1999). The Arizona State University “Views
About Sciences Survey”  (VASS) (Halloun & Hestenes 1998, Halloun 1997)  - available for
physics, chemistry, biology and mathematics at < http://modeling.la.asu.edu/R&E/Research.html >
- indicates that students have views about physics that (a) often diverge from physicists’ views;
(b) can be grouped into four distinct profiles: expert, high transitional, low transitional, and folk;
(c) are similar in college and high school; and (d) correlate significantly with physics achievement.
It may well be that students’ attitudes and understanding of science and education are irreversibly
imprinted in the early years.  If so, corrective measures await a badly needed drastic improvement
in K-12 education (Hake 2000c,d; Mahajan & Hake 2000; Benezet 1935/36) – see L10.

L7. Education Research and Development (R&D) by disciplinary experts (DE’s), and of the same
quality and nature as traditional science/engineering R&D, is needed to develop potentially
effective educational methods within each discipline.  But the DE’s should take advantage of the
insights of (a) DE’s doing education R&D in other disciplines, (b) cognitive scientists, (c) faculty
and graduates of education schools, and (d) classroom teachers.

Redish (1999) has marshaled the arguments for the involvement of physicists in physics
departments – not just faculty of education schools - in physics-education research.  Similar
arguments apply more generally to other disciplines:  (a) physicists have good access to physics
courses and students on which to test new curricula, (b) physicists and their departments directly
benefit from physics education research, (c) education schools have limited funds for disciplinary
education research, (d) understanding what’s going on in physics classes requires deep rethinking of
physics and the cognitive psychology of understanding physics. One might add that the researchers
themselves must be excellent physics teachers with both content and “pedagogical content”
knowledge (see L3) of a depth unlikely to be found among non-physicists.

The education of disciplinary experts in education research requires Ph.D. programs at least as rigorous
as those for experts in traditional research. The programs should include, in addition to the standard
disciplinary graduate courses, some exposure to: the history and philosophy of education, computer
science, statistics, political science, social science, economics, engineering - see L11, and, most
importantly, cognitive science (i.e., philosophy, psychology, artificial intelligence, linguistics,
anthropology, and neuroscience).  The breadth of knowledge required for effective education research is

16

similar to that required in ecological research (Holling 1997). In the U.S. there are now about a dozen
Ph.D. programs in physics education within physics departments and about half that number of
interdisciplinary programs between physics and education or cognitive psychology (Physical Science
Resource Center 2001, UMd-PERG 2001b.).   In my opinion, all scientific disciplines should consider
offering Ph.D. programs in education research.

But how can disciplinary education researchers, and for that matter, university faculty generally,
take advantage of the insights of: disciplinary experts doing education R&D in other disciplines;
cognitive scientists; faculty and graduates of education schools; and classroom teachers?  In my
opinion, even despite the rigid departmental separation of disciplines in most research universities,
the web has the potential to dramatically enhance cooperation  and interchange among these groups
(Hake, 1999c, 2000e).  Certainly the success of Conservation Ecology   
< http://www.consecol.org/Journal/ >  testifies to the value of the web in promoting
interdisciplinary effort.  A starting point might be the construction of web guides for various
disciplines similar to REDCUBE   < http://www.physics.indiana.edu/~redcube > (Hake 1999b),
which provides a point of entry into the vast literature and web resources relevant to REsearch,
Development, and Change in Undergraduate Biology Education.   The 9/8/99 version contains 47
biology-educator profiles; 446 references (including 124 relevant to general science-education
reform); and 490 hot-linked URL’s on (a) Biology Associations, (b) Biology Teachers’ Web Sites,
(c) Scientific Societies and Projects (not confined to Biology), (d) Higher Education,  (e) Cognitive
Science and Psychology, (f) U.S. Government, and (g) Searches and Directories.

Regarding the value of tapping into cognitive science, J.J. Duderstadt (2000), president emeritus of
the University of Michigan - Ann Arbor writes: “Few faculty members have any awareness of the
expanding knowledge about learning from psychology and cognitive science. Almost no one in the
academy has mastered or used this knowledge base. One of my colleagues observed that if doctors
used science the way college teachers do, they would still be trying to heal with leeches.”  (My
italics.)

L8.  The development of effective educational  methods within each discipline requires a redesign
process of continuous long-term classroom use, feedback, assessment, research analysis, and
revision.

Wilson and Davis (1994) suggest that the “redesign process,” used so successfully to advance
technology in aviation, railroads, automobiles, and computers can be adapted to K-12 education
reform through “System Redesign Schools.”  Redesign processes in the reform of introductory
undergraduate physics education have been undertaken and described by McDermott (1991) and by
Hake (1998a).   In my opinion “redesign” at both the K-12 and undergraduate levels can be greatly
assisted by the promising Scholarship of Teaching & Learning movement (Carnegie Academy
2000) inspired by Boyer (1990) and the Boyer Commission (1998).

17

L9.  Although non-traditional interactive-engagement  methods appear to be much more
effective than traditional methods, there is need for more research to develop better strategies
for the enhancement of student learning.

On a test as elemental as the FCI it would seem that reasonably effective courses should yield <g>’s
above 0.8, but thus far none much above 0.7 have, to my knowledge, been reported. This and the
poor showing on the pre/post  MPEX test of student understanding of the nature of science and
education (Redish et al. 1998) indicates that more work needs to be done to improve IE methods.  It
would appear that understanding of science might be improved by students’ apprenticeship research
experiences (Collins et al. 1989, Brown et al. 1989), and enrollment in courses featuring interactive
engagement among students and disciplinary experts from different fields, all in the same
classroom at the same time (Benbasat & Gass 2001).

In my opinion, more support should be given by universities, foundations, and governments to the
development of a science of education spearheaded by disciplinary education researchers working
in concert with cognitive scientists and education specialists.  In the words of cognitive
psychologists Anderson et al. (1998): “The time has come to abandon philosophies of education and
turn to a science of education . . . . . . If progress is to be made to a more scientific approach,
traditional philosophies . . . .(such as radical constructivism) . . . .  will be found to be like the
doctrines of folk medicine. They contain some elements of truth and some elements of
misinformation. . . . . .Only when a science of education develops that sorts truth from fancy - as it
is beginning to develop now will dramatic improvements in educational practice be seen.” (My
italics.)

The imperative for educational improvement has been set forth by the National Research Council
(1997): “The education that many students receive in science, mathematics, and technology is not
adequate for a world that is being transformed by scientific and technological advances. People
have to be familiar with the basic concepts of science, mathematics, engineering, and technology to
think critically about the world and to make informed decisions about personal and societal issues.
Literacy in these fields is essential also for an appreciation of the rapid expansion of human
knowledge – surely one of the great adventures of the 20th century.”

Wilson and Barsky (1998) see the need “for a launch of a research and development initiative in
education, paralleling existing national research initiatives related to AIDS or global climate change
. . . . Today we have to think of education as demanding in multiple dimensions: as a science, as a
design challenge, and as a performing art while still being an imperative for life in a democracy.
Handed down traditions are no longer enough.” See also
< http://www.physics.ohio-state.edu/~redesign/ >.

18

The House Committee on Science (1998) states that: “Currently, the U.S. spends approximately
$300 billion a year on education and less than 30 million, 0.01% of the overall education budget, on
education research. At a time when technology promises to revolutionize both teaching and
learning, this miniscule investment suggests a feeble long-term commitment to improving our
educational system.”

However, it should be emphasized that the development of better strategies for the enhancement of
student learning will not improve the educational system unless (a) university and K-12 teachers (see
L10) are educated to effectively implement those strategies, and (b) research universities start to
think of education in terms of student learning rather than the delivery of instruction (see L12h).

L10.  A major problem for undergraduate education is the inadequate preparation of incoming
students, in part due to the inadequate university education of K-12 teachers.

According to the National Research Council (1999), the Third International Mathematics and
Sciences Survey (TIMSS) indicates that: “U.S. students’ worst showing was in population 3 . . . .
(final year of secondary School. . . . corresponding to U.S. high school seniors). . . . In the
assessment of general mathematics and science knowledge, U.S. high school seniors scored near the
bottom of the participating nations. In the assessments of advanced mathematics and physics given
to a subset of students who had studied those topics, no nations had significantly lower mean scores
than the United States. The TIMSS results indicate that a considerably smaller percentage of U.S.
students meet high performance standards than do students in other countries.” Consistent with the
foregoing, I have observed (Hake 2000d) that FCI pretest averages for students entering the
introductory physics course at Indiana University are quite low (30% - 45%) and about the same
regardless of whether or not the students are graduates of high-school physics classes.

But it’s not just a matter of physics floundering. According to Epstein (1997-98): “While it is now
well known that large numbers of students arrive at college with large educational and cognitive
deficits many faculty and administrative colleagues are not aware that many students lost all sense
of meaning or understanding in elementary school……In large numbers our students …… [at
Bloomfield College (New Jersey) and Lehman (CUNY)]…..cannot order a set of fractions and
decimals and cannot place them on a number line. Many do not comprehend division by a fraction
and have no concrete comprehension of the process of division itself. Reading rulers where there
are other than 10 subdivisions, basic operational meaning of area and volume, are pervasive
difficulties. Most cannot deal with proportional reasoning nor any sort of problem that has to be
translated from English. Our diagnostic test, which has now been given at more than a dozen
institutions shows that there are such students everywhere . . . . . .[even Wellesley (Epstein 1999)].

19

Kati Haycock (1999), director of the American Association of Higher Education’s (AAHE’s)
Education Trust  < http://www.edtrust.org/ > hits the nail on the head: “Higher education…. (unlike
Governors and CEO’s) ….. has been left out of the loop and off the hook …. (in the effort to
improve America’s public schools since release of A Nation at Risk in 1983)…. Present neither at
the policy tables where improvement strategies are formulated nor on the ground where they are
being put into place, most college and university leaders remain blithely ignorant of the roles their
institutions play in helping K-12 schools get better - and the roles they currently play in maintaining
the status quo …. How are we going to get our students to meet high standards if higher education
continues to produce teachers who don’t even meet those same standards?  How are we going to get
our high school students to work hard to meet new, higher standards if most colleges and
universities will continue to admit them regardless of whether or not they even crack a book in high
school?”  (My italics.)

According to the NSF Advisory Committee (1996): “Many faculty in SME&T. . . . (Science, Math,
Engineering, and Technology) . . . .  at the post-secondary level continue to blame the schools for
sending underprepared students to them.  But, increasingly, the higher education community has
come to recognize the fact that teachers and principals in the K-12 system are all people who have
been educated at the undergraduate level, mostly in situations in which SME&T programs have not
taken seriously enough their vital part of the responsibility for the quality of America’s teachers.”
(My italics.) See also NSF Advisory Committee (1998).

     Fortunately, despite the general failure of pre-service teacher education, several programs have
     been established over the past few years to enhance the pedagogical skills and content knowledge of
     in-service physics teachers. For a hot-linked list of 25 such programs see Hake (2000d).

     The recent Glenn Commission (2000) proposals may be a step in the right direction. The
     commission requests 5 billion dollars in the first year to initiate (my italics):
          a. establishment of an ongoing system to improve the quality of mathematics and  science
                teaching in grades K–12,
          b. significant increase in the number of mathematics and science teachers with improved quality
                of their preparation,
          c. improvement of the working environment and so as to make the teaching profession more
                attractive for K–12 mathematics and science teachers.

20

L11. Interdisciplinary cooperation of instructors, departments, institutions, and professional
organizations is required for synthesis, integration, and change in the entire chaotic educational
system.

Although more research to develop better strategies for the enhancement of student learning (L9) is
required, that by itself will not reform the entire chaotic educational system, as has been emphasized
by Tobias (1992a,b; 2000), Sarason (1990, 1996), Hilborn (1997), and Wilson & Davis (1994). In
my opinion, an engineering approach to the improvement of education (Felder 2000a,b) seems to be
required. Bordogna (1997) conveys the essence of engineering as “integrating all knowledge for
some purpose. . . . The engineer must be able to work across many different disciplines and fields -
and make the connections that will lead to deeper insights, more creative solutions, and getting
things done. In a poetic sense, paraphrasing the words of Italo Calvino (1988), the engineer must be
adept at correlating exactitude with chaos to bring visions into focus.” (My italics).  It would
appear that “engineering” as seen by Bordogna is similar to “integrative science” as seen by Holling
(1998).

L12. Various institutional and political factors, including the culture of research universities,
slow educational reform.

Among the institutional and political factors listed by Tobias (2000) as thwarting educational
reform are (those most associated with the culture of research universities are indicated in italics):

a. Advanced Placement (AP) courses serve as a filter rather than a pump.
b. In-class and standardized tests (MCAT, SAT, GRE) drive the curriculum in a traditional
       direction.
c. Effectiveness of teaching has little effect on promotion/tenure decisions or on national
       departmental rankings.
d. High-school science courses are not required for college admission; many colleges require
       little or no science for graduation.
e. Clients for the sciences aren’t cultivated among those who do not wish to obtain PhD.’s.
f. Class sizes are too large.

To Tobias’s list I would add:

g.  The failure of the K-12 system to incorporate physics – the most basic of the sciences and
essential for any proper understanding of biology and chemistry – into all grades for all
students (Hammer 1999, Neuschatz 1999, Lederman 1999, Livanis 2000). In the words of
physics Nobelist Leon Lederman: “We have observed that 99 percent of our high schools teach
biology in 9th (or 10th) grade, chemistry in 10th or 11th grade, and, for survivors, physics in
11th or 12th grade. This is alphabetically correct, but by any logical scientific or pedagogical
criteria, the wrong order. . . . This reform . . . .(“physics first”). . . . concentrates on installing a
coherent, integrated science curriculum, which matches the standards of what high school

21

graduates should understand and be able to do . . . . And wouldn’t it be a natural next step to
invite the history teachers, the teachers of arts and literature, to help develop those connections
of the fields of learning that the biologist E.O. Wilson (1998) calls ‘consilience’?”

h.  The failure of research universities to:

         (1) Discharge their obligation to adequately educate prospective K-12 teachers

   
                            (Hake 2000c) – see L10.

            (2) Think of education in terms of student learning rather than the delivery of instruction
                  (Barr & Tagg 1995).  An emphasis on the learning paradigm may be encouraged by:

                                 (a) the previously mentioned Scholarship of Teaching & Learning movement

   (Carnegie Academy 2000) inspired by Boyer (1990) and the Boyer
   Commission (1998);

                                 (b) the National Academy for Academic Leadership

   < http://www.thenationalacademy.org/ >, which strives to “educate academic
   decision makers to be leaders for sustained, integrated institutional change

                                          that significantly improves student learning;”
                                 (c) threats from accrediting agencies such as ABET (Accreditation Board for
                                          Engineering and Technology < http://www.abet.org/ >) with its emphasis on
                                          accountability for actual student learning (Van Heuvelen & Andre 2000,
                                          Heller (2000); and
                                (d) competition for transmission-mode lecture services from distance-education
                                         conglomerates  (Marchese 1998).

            (3) Effectively consider crucial multidisciplinary societal problems such as education.

In the words of Karl Pister (1996), former Chancellor of UC - Santa Cruz: “. . . .  we
need to encourage innovative ways of  looking at problems, moving away from the
increasing specialization of  academia to develop new interdisciplinary fields that can
address complex real-world  problems from new perspectives.”

i. The failure of society to pay good K-12 teachers what they are worth. Physicist Don
       Langenberg (1999), chancellor of the University System of Maryland and president of the
       National Association of System Heads < http://mdk16.usmd.edu/nash.html >, suggests that
       “on average, teacher’s salaries ought to be about 50% higher than they are now. Some
        teachers, including the very best, those who teach in shortage fields (e.g., math and science)
        and those who teach in the most challenging environments (e.g., inner cities) ought to have
        salaries about twice the current norm. . . . Simple arithmetic applied to publicly available
        data shows that the increased cost would be only 0.6% of the GDP, about one twentieth of
        what we pay for health care. I’d assert that if we can’t bring ourselves to pony up that
        amount, we will pay far more dearly in the long run.” (My italics.)

22

L13. The monumental inertia of the educational system may thwart long-term national reform.
The glacial inertia of the nearly immovable U.S. educational system is not well understood.  A
recent issue of Daedalus (1998) contains essays by researchers in education and by historians of
more rapidly developing institutions such as power systems, communications, health care, and
agriculture.  The issue was intended to help answer a challenge posed by physics Nobelist Kenneth
Wilson: “If other major American ‘systems’ have so effectively demonstrated the ability to change,
why has the education ‘system’ been so singularly resistant to change? What might the lessons
learned from other systems’ efforts to adapt and evolve have to teach us about bringing about
change - successful change – in America’s schools?” As far as I know, no definitive answer has yet
been forthcoming.

Clifford Swartz (1999), former editor of The Physics Teacher and long-time acerbic critic of
physics-education research, wrote: “ There is a variety of evidence, and claims of evidence, that
each of the latest fads . . .(constructivism, ‘group’ and ‘peer’ instruction, ‘interaction’) . . . produces
superior learning and happier students. In particular, students who interact with apparatus or lecture
do better on the Force Concept Inventory exam (Hestenes et al. 1992).  The evidence of Richard
Hake’s (1998a) metastatistical study is so dramatic that the only surprising result is that many
schools and colleges are still teaching in old-fashioned ways.  Perhaps the interaction technique
reduces coverage of topics, or perhaps the method requires new teaching skills that teachers find
awkward. At any rate the new methodology is not sweeping the nation.”  (My italics.)

New educational methodologies have from time to time swept the nation (e.g., “the new math,”
PSSC (Physical Science Study Committee) physics, the Keller Plan (Personalized System of
Instruction) but then faded from sight.  History (Holton 1986;  Arons 1993, 1997; Sarason 1990,
1996; Cuban 1999) suggests that the present educational reform effort may, like its predecessors,
have little lasting impact. This would be most unfortunate, considering the current imperative to:

a.  educate more effective science majors and science-trained professionals,
b.  raise the appallingly low level of science literacy among the general population,
c.  solve the monumental science-intensive problems (economic, social, political, and
       environmental) that beset us.

23

L14. “Education is not rocket science, it’s much harder.”
              George Nelson, astronaut and astrophysicist, as quoted by Redish (1999).

My own belief, conditioned by 40 years of research in superconductivity and magnetism, 28 years
in physics teaching, and 16 years in education research, is that effective education (both physics
teaching and education research) is harder than solid-state physics.  The latter is, of course, several
orders of magnitude harder than rocket science. Nuclear physicist Joe Redish (1999) writes: “The
principles of our first draft of a community map for physics education are different in character
from the laws we would write down for a community map of the physical world.  They are much
less like mathematical theorems and much more like heuristics.  This is not a surprise, since the
phenomena we are discussing are more complex and at a much earlier stage of development.”
Since education is a complex, early-stage, dynamic, non-linear, scientific/sociopolitical, high-stakes
system, it might benefit from the expertise of conservation ecologists who are well used to dealing
with such challenging systems. (Holling 1999).

_________________________________________________________________________________________

RESPONSES TO THIS ARTICLE

Responses to this article are invited.  If accepted for publication, your responses will be hyperlinked to
the article.  To submit a comment, follow this link.  To read comments already accepted, follow this
link.

_________________________________________________________________________________________

Acknowledgements:

I should like to dedicate this paper to the late Arnold Arons, farsighted pioneer of U.S. physics
education research; physics education’s leading guru; and the major source of wisdom, educational
inspiration, and encouragement to me (Hake 1991) and many others over the decades.  I thank David
Hestenes for insights and assistance.  I am also indebted to Lee Gass for suggesting that I write this
review, and for his very valuable comments on the manuscript. Finally, I thank the National Science
Foundation for funding through NSF Grant DUE/MDR-9253965.

24

_________________________________________________________________________________________

LITERATURE CITED

Adams, J. , R.L. Adrian, C. Brick, G. Brissenden, G. Deming, B. Hufnagel, T. Slater, M. Zeilik,
and the Collaboration for Astronomy Education Research (CAER). 2000. Astronomy Diagnostic
Test (ADT) Version 2.0. [online] URL: < http://solar.physics.montana.edu/aae/adt/ >.

Almer, E.D., K. Jones, & C. Moeckel. 1998. The Impact of One-Minute Papers on Learning in an
Introductory Accounting Course. Issues in  Accounting Education. 13(3): 485-497. [abstract online]
URL: < http://accounting.rutgers.edu/raw/aaa/pubs/is8-98.htm#aa >

Anderson, J.L. 1998. Embracing uncertainty: The interface of Bayesian statistics and cognitive
psychology. Conservation Ecology 2(1): 2. [online] URL:
< http://www.consecol.org/Journal/vol2/iss1/art2/index.html >

Anderson, J.R., L.M. Reder, and H. A. Simon. 1998. Radical constructivism and cognitive
psychology. In Brookings Papers on Education Policy - 1998, D. Ravitch, ed., Brookings Institution
Press, pp. 227-278.

Anderson, D.R., K.P. Burnham, W.L. Thompson. 2000. Null Hypothesis Testing: Problems,
Prevalence, and an Alternative, J. Wildlife Management 64(4): 912-923. [online] URL:
< http://biology.uark.edu/Coop/thompson4.html >.

Angelo T. A.  & K. P. Cross. 1993. Classroom Assessment Techniques: A Handbook for College
Teachers, Jossey-Bass, 2nd ed.

Arons, A.B. 1981. Thinking, reasoning, and understanding in introductory physics courses. Phys.
Teach. 19: 166-172.

_______. 1990. A guide to introductory physics teaching. Wiley; reprinted with minor updates in
Teaching introductory physics (Wiley, 1997).

_______. 1993. Uses of the past: reflections on United States physics curriculum development, 1955 to
1990.  Interchange 24(1&2): 105-128.

25

_______. 1997. Improvement of physics teaching in the heyday of the 1960’s. In Conference on the
introductory physics course on the occasion of the retirement of Robert Resnick, J.  Wilson, ed. Wiley,
pp. 13-20.

Aubrecht,  G.J. 1991. Is there a connection between testing and teaching? J. Coll. Sci. Teach. 20: 152-
157.

Barr, R.B. & J. Tagg. 1995. From teaching to learning – a new paradigm for undergraduate
education. Change, Nov./Dec.: 13-25.

Becker, W.E. 2001. What Does the Quantitative Research Literature Really Show About Teaching
Methods? [preprint online] URL:
< http://www.indiana.edu/~deanfac/sotl/download/010302.doc.pdf >.

Beichner, R.J. 1994. Testing student interpretation of kinematics graphs.  Am. J. Phys. 62(8): 750-
762.

Beichner, R., L. Bernold, E. Burniston, P. Dail, R. Felder, J. Gastineau, M. Gjertsen, and J.
Risley. 1999. Case study of the physics component of an integrated curriculum. Physics Ed. Res.,
supplement 1 to the Am. J. Phys. 67(7): S16-S24.

Benbasat, J.A.  & C.L. Gass.  2001. Reflections on Integration, Interaction, and Community: The
Science One Program and Beyond This Issue #?: ?. [online] URL:
< http://www.concol.org/Journal/vol?/iss?/index.html >.

Benezet, L.P. 1935-1936. The Teaching of Arithmetic I, II, III: The Story of an Experiment, Journal
of the National Education Association 24(8), 241-244 (1935); 24(9), 301-303 (1935); 25(1), 7-8
(1936). The articles were (a) reprinted in the Humanistic Mathematics Newsletter #6: 2-14 (May
1991); (b) placed on the web along with other Benezetia at the Benezet Centre
< http://wol.ra.phy.cam.ac.uk/sanjoy/benezet/ >. See also Mahajan & Hake (2000).

Bernhard, J. 1999. How long-lived is post-course understanding of mechanics concepts? submitted to
Phys. Teach. [online] URL: < http://www.du.se/~jbe/fou/didaktik/papers/fixed.html >.

Bloom, B.S. 1984. The 2 sigma problem: the search for methods of group instruction as effective as
one-to-on tutoring. Educational Researcher 13(6): 4-16.

26

Bordogna, J. 1997. Making connections: the role of engineers and engineering education, The Bridge
27(1): Spring. [online] URL:
< http://www.nae.edu/nae/naehome.nsf/weblinks/NAEW-4NHMPY?opendocument >

Boyer, E.L. 1990. Scholarship reconsidered: priorities for the professoriate. Carnegie Foundation for
the Advancement of Teaching.

Boyer Commission. 1998. Reinventing undergraduate education: A blueprint for America’s
research universities. The Boyer Commission on Educating Undergraduates in the Research
University (Carnegie Foundation for the Advancement of Teaching). [online] URL:
< http://notes.cc.sunysb.edu/Pres/boyer.nsf >:

Bransford, J.D., A.L. Brown, R.R. Cocking, eds. 1999. How people learn: brain, mind, experience,
and school. Nat. Acad. Press. [online] URL: < http://www.nap.edu/catalog/6160.html >.

Brown, J.S., A. Collins, and P. Duguid. 1989. Situated cognition and the culture of learning.
Educational Researcher 18(1):34-41. [online] URL:
< http://www.ilt.columbia.edu/ilt/papers/JohnBrown.html >.

Bruer, J.T. 1994. Schools for thought: a science of learning in the classroom. MIT Press.

_______. 1997. Education and the brain: a bridge too far. Educational Researcher 26(8): 4-16.

Calvino, I. 1988. Six memos for the next millennium. Harvard University Press.

Carnegie Academy. 2000. Scholarship of Teaching and Learning. [online] URL:
< http://www.carnegiefoundation.org/CASTL/index.htm >.

Carr, J.J. 2000. The physics tutorial: some cautionary remarks. Am. J. Phys. 68(11): 977-978.

Chizmar, J.F. & A.L. Ostrosky. 1998. "The One-Minute Paper: Some Empirical Findings. " Journal
of Economic Education. Winter 29(1). [online] URL:
< http://www.indiana.edu/~econed/issues/v29_1/1.htm >.

Cohen, J. 1988. Statistical power analysis for the behavioral sciences. Lawrence Erlbaum, 2nd ed.

27

Collins, A.,  J.S. Brown, and S. Newman. 1989. Cognitive apprenticeship: teaching students the craft
of reading, writing, and mathematics. In L.B. Resnick, ed., Knowing, learning, an instruction:  Essays
in honor of Robert Glaser, pp. 453-494. Lawrence Erlbaum.

Cook, T.D., & D.T. Campbell. 1979. Quasi-experimentation: design & analysis issues for field
settings. Houghton Mifflin.

Cronbach, L.J. & L. Furby. 1970. How should we measure “change”  – or should we? Psychological
Bulletin 74: 68-80.

Crouch, C.H. & E. Mazur, 2001. Peer Instruction: Ten Years of Experience and Results.
Am. J. Phys., in press.

Cuban, L. 1999. How scholars trumped teachers: change without reform in university curriculum,
teaching, and research, 1890 – 1990. Teachers College Press.

Cummings, K., J. Marx, R. Thornton, D. Kuhl. 1999. Evaluating innovations in studio physics.
Physics Ed. Res., supplement 1 to the Am. J. Phys. 67(7): S38-S44.

Daedalus. 1998. Education yesterday, education tomorrow. Daedalus 127(4). [online description]
URL: < http://daedalus.amacad.org/inprint.html >.

Davis, B.G., L. Wood, R.C. Wilson. 1983. “A Berkeley Compendium of Suggestions for Teaching
with Excellence” [online] URL: < http://www.uga.berkeley.edu/sled/compendium/ >.

Dewey, J. 1929. The Sources of a Science of Education, in John Dewey: The Later Works, 1925-1953,
vol. 5 (1929-1930), ed. by J. A. Boydston. Southern Illinois University, 1984, 17 volumes [online
description] URL: < http://www.siu.edu/~deweyctr/lworks.html >.  Also available in The Collected
Works of John Dewey, 1882-1953: The Electronic Edition (CD ROM) [online description] URL:
< http://www.siu.edu/~deweyctr/colworks.html >.

Dewey, J. 1997. Experience and Education.  MacMillan.

Donovan, M.S., J.D. Bransford, and J.W. Pellegrino. 1999. How people learn: bridging research
and practice. National Academy Press. [online] URL:  < http://www.nap.edu/catalog/9457.html >.

28

Duderstadt, J.J. 2000.  A University for the 21st Century. University of Michigan Press. [synopsis
online] URL: < http://www.press.uchicago.edu/cgi-bin/hfs.cgi/500/11091.ctl >, also
< http://www.nap.edu/issues/16.2/duderstadt.htm >.

Elby, A. 1999. Another reason that physics students learn by rote. Physics Ed. Res., supplement 1 to
the Am. J. Phys. 67(7):S52-S57.

Eisner, E.W. 1997. The promise and perils of alternative forms of data representation. Educational
Researcher 26(6): 4-10.

Epstein, J. 1997-98.  Cognitive development in an integrated mathematics and science program. J. of
College Science Teaching 12/97 & 1/98: 194-201.

_______. 1999.  What is the real level of our students? unpublished.

Felder, R.M., D. R. Woods, J. E. Stice, A. Rugarcia.  2000a. The future of engineering education II:
Teaching methods that work. Chem. Engr. Education 34(1): 26-39. [online] URL:
< http://www2.ncsu.edu/unity/lockers/users/f/felder/public/RMF.html > /“Education-related papers”.

Felder, R.M., J. E. Stice, A. Rugarcia.  2000b. The future of engineering education VI. Making
reform happen. Chem. Engr. Education 34(3): 208-215. [online] URL:
< http://www2.ncsu.edu/unity/lockers/users/f/felder/public/RMF.html > /“Education-related papers”.

Francis, G.E., J.P. Adams, E.J. Noonan. 1998. Do they stay fixed? Phys. Teach. 36(8): 488-491.

Fuller, R.G. 1993. Millikan Lecture 1992. Hypermedia and the knowing of physics: standing upon the
shoulders of giants. Am. J. Phys. 61(4): 300-304.

Gage, N.L. 1989. The paradigm wars and their aftermath: a “historical” sketch of research on teaching
since 1989. Educational Researcher 18(7): 4-10.

Galileo Project. 2001. A leading resource for teaching materials on the Web. [online] URL;
< http://galileo.harvard.edu/ >.

Gardner, H. 1985. The mind’s new science: a history of the cognitive revolution. Basic Books.

29

Geilker, C.D. 1997. Guest Comment:  In defense of the lecture-demonstration method of teaching
physics. Am. J. Phys. 65(2): 107.

Ghery, F.W. 1972. Does Mathematics Matter. In Research Papers in Economic Education, Arthur
Welch, ed. Joint Council on Economic Education, pp. 142-157.

Glenn Commission. 2000. Before It's Too Late: A Report to the National Commission on
Mathematics and Science  Teaching for the 21st Century; online at
< http://www.ed.gov/americacounts/glenn/toc.html >.

Goldman, P. 1998.  Long live the lecture. Physics World, December: 15-16. [online as “Thoughts on
Teaching”] URL: < http://gandalf.physics.uwo.ca/spg/spgFolder/spg >.

Griffiths, D. 1997. Millikan Lecture 1997: Is there a text in this class? Am. J. Phys. 65(12): 1141-
1143.

Hake, R.R. 1987. Promoting student crossover to the Newtonian world. Am J. Phys. 55(10): 878-884.

_______. 1991. My Conversion To The Arons-Advocated Method Of Science Education, Teaching
Education 3(2), 109-111. [online] URL: < http://www.physics.indiana.edu/~hake >.

_______. 1992. Socratic pedagogy in the introductory physics lab. Phys. Teach. 30: 546-552. [updated
version (4/27/98) online] URL: < http://physics.indiana.edu/~sdi/ >.

_______. 1998a. Interactive-engagement vs traditional methods: A six-thousand-student survey of
mechanics test data for introductory physics courses. Am. J. Phys. 66(1): 64-74. [online] URL:
< http://www.physics.indiana.edu/~sdi/ >.

_______. 1998b. Interactive-engagement methods in introductory mechanics courses, submitted to
Phys. Ed. Res., supplement to Am. J. Phys [online] URL: < http://www.physics.indiana.edu/~sdi/ >.

_______. 1998c.  Interactive-engagement vs traditional methods in mechanics instruction. APS Forum
on Education Newsletter, Summer: 5-7. [online] URL: < http://www.physics.indiana.edu/~sdi/ >.

_______. 1999a.  Analyzing change/gain scores. Unpublished. [online] URL:
< http://www.physics.indiana.edu/~sdi/ >.

_______. 1999b. REsearch, Development, and Change in undergraduate biology education: a web
guide for non-biologists (REDCUBE). [online] URL: < http://www.physics.indiana.edu/~redcube >.

30

_______. 1999c. What can we learn from the biologists about research, development, and change in
undergraduate education? AAPT Announcer 29(4): 99. [online] URL:
< http://www.physics.indiana.edu/~hake/ >.

_______. 2000b. Towards paradigm peace in physics-education research.  Presented at the annual
meeting of the American Educational Research Association, New Orleans, April 24-28. [online] URL:
< http://www.physics.indiana.edu/~hake/ >.

_______. 2000c. The general population’s ignorance of science-related societal issues - a challenge for
the university. AAPT Announcer 30(2): 105. [online] URL:
< http://www.physics.indiana.edu/~hake/ >.

_______. 2000d. Is it Finally Time to Implement Curriculum S? AAPT Announcer 30(4): 103 (2000).
[online] URL: < http://www.physics.indiana.edu/~hake/ >.

_______. 2000e. Using the Web to Promote Interdisciplinary Synergy in Undergraduate Education
Reform. AAPT Announcer 30(4):120 . [online] URL: < http://www.physics.indiana.edu/~hake/ >.

_______. 2001a. Socratric Dialogue Inducing Labs for Introductory Physics. [online] URL:
< http://www.physics.indiana.edu/~sdi/ >.

_______. 2001b. Suggestions for the administration and reporting of disciplinary diagnostic tests,
unpublished. [online] URL: < http://www.physics.indiana.edu/~hake/ >.

Halloun, I. & D. Hestenes. 1985a.  The initial knowledge state of college physics students.  Am. J.
Phys. 53: 1043-1055.

_______. 1985b.  Common sense concepts about motion. Am. J. Phys. 53: 1056-1065.

_______. 1987. Modeling instruction in mechanics. Am. J. Phys. 55: 455-462.

_______. 1998. Interpreting VASS Dimensions and Profiles. Science & Education, 7(6): 553-577.
[online – password protected] URL:  < http://modeling.la.asu.edu/R&E/Research.html >.

Halloun, I., R.R. Hake, E.P Mosca, D. Hestenes. 1995. Force Concept Inventory (Revised, 1995),
[online – password protected] URL: < http://modeling.la.asu.edu/R&E/Research.html >.

31

Halloun, I. 1997. Views About Science and Physics Achievement: The VASS Story. In The
Changing Role of Physics Departments in Modern Universities: Proceedings of the ICUPE, ed. by
E.F. Redish and J.S. Rigden, (American Institute of Physics), pp. 605–613.

Hammer, D. 1999. Physics for first graders? Science Education 83(6):797-799. online] URL:
< http://www2.physics.umd.edu/~davidham/1stgrdrs.html >.

Haycock, K. 1999. The role of higher education in the standards movement in 1999 National
Education Summit Briefing Book. [online] URL: < http://www.achieve.org/achieve/achievestart.nsf > .

Heller, K.J. 1999. Introductory physics reform in the traditional format: an intellectual framework,
AIP Forum on Education Newsletter, Summer: pp. 7-9. [online] URL:
< http://webs.csu.edu/~bisb2/FEdnl/heller.htm >.

_______2000. Meeting the needs of other departments:  introductory physics in the ABET 2000 Era.
Proceeding of the 2000 Physics Chairs Conference: Undergraduate Physics for the New Century.
[online]  URL: < http://www.aapt.org/ >.

Heller, P., R. Keith, S. Anderson. 1992a. Teaching problem solving through cooperative grouping,
Part 1:  Group vs individual problem solving.  Am. J. Phys. 60(7): 627-636.

Heller, P. and M. Hollabaugh. 1992b.  Teaching problem solving through cooperative grouping, Part
2: Designing problems and structuring groups._ Am. J. Phys. 60(7): 637-644.

Herron J. D. & S.C. Nurrenbern. 1999.  Chemical education research: improving chemistry learning.
J. Chem. Ed. 76(10): 1353-1361.

Hestenes, D. 1987. Toward a modeling theory of physics instruction. Am. J. Phys. 55: 440-454.

_______. 1992. Modeling Games in the Newtonian World, Am. J. Phys. 60(8): 732-748.

_______. 1998. Guest Comment: Who needs physics education research!?  Am. J. Phys. 66(6): 465-467.

_______. 1999. The scientific method. Am. J. Phys. 67(4): 274.

32

Hestenes, D., M. Wells, & G. Swackhamer, 1992.  Force Concept Inventory.  Phys. Teach. 30: 141-158.

Hestenes D. & M. Wells. 1992.  A mechanics baseline test. Phys. Teach. 30: 159-166. [online-
password protected] URL: < http://modeling.la.asu.edu/R&E/Research.html >.

Hilborn, R.C. 1997. Guest Comment: revitalizing undergraduate physics – Who needs it? Am. J. Phys.
65(3): 175-177.

_______. 1998.  A reply to C.D. Geilker’s guest comment.  Am. J. Phys. 66(4): 273-274.

_______. 1999.  On teaching – innovative  and traditional.  Phys. Teach. 38: 250-251.

Holling, C.S. 1997. The inaugural issue of Conservation Ecology. Conservation Ecology 1(1): 1.
[online] URL: < http://www.consecol.org/Journal/vol1/iss1/art1/index.html >.

_______. 1998. Two cultures of ecology. Conservation Ecology 2(2): 4. [online] URL:
< http://www.consecol.org/Journal/vol2/iss2/art4/index.html >.

_______. 1999. Introduction to the special feature: just complex enough for understanding; just simple
enough for communication. Conservation Ecology 3(2):1. [online] URL:
< http://www.consecol.org/Journal/vol3/iss2/art1/index.html >.

Holton, G. 1986.  A nation at risk revisited.  In The advancement of science and its burdens. Univ. of
Cambridge Press.

House Committee on Science, chaired by Vernon Ehlers. 1998. Unlocking Our Future: Toward a
New National Science Policy. Report to Congress. online] URL:
< http://www.house.gov/science/science_policy_study.htm >.

Hovland, C.I.  A.A. Lumsdaine, and F.D. Sheffield. 1949. Experiments on Mass Communication.
Princeton University Press, pp: 284-289; reprinted as "A Baseline for Measurement of Percentage
Change," The Language of Social Research, Lazarsfeld and Rosenberg, eds. Free Press, 1955, pp. 77-
82.

Hunt, M. 1997. How science takes stock: the story of meta-analysis. Russell Sage Foundation.

33

Inhelder, B. & J. Piaget. 1958. Growth of logical thinking from childhood to adolescence: an essay
on the construction of formal operational structures. Basic Books.

Johnson, D.H. 1999. The insignificance of statistical significance testing.  Journal of Wildlife
Management 63: 763-772. [online] URL:
< http://www.npwrc.usgs.gov/resource/1999/statsig/statsig.htm >

Johnson, D.W. , R.T. Johnson, and K.A Smith. 1991. Cooperative learning: increasing college
faculty instructional productivity. George Washington University.

Karplus, R. 1977. Science teaching and the development of reasoning. J. Res. Sci. Ed. 14: 169.

Kolitch, S. 1999.  Studio physics at Cal Poly. Phys. Teach. 37: 260 (1999).

Lagemann, E.C. 2000. An elusive science: the troubling history of education research. Univ. of
Chicago Press.

Langenberg, D.N. 2000. Rising to the challenge. Thinking K-16 4(1):19. [online as “Honor in the
Boxcar”] URL: < http://www.edtrust.org/main/reports.asp >.

Lave, J. & E. Wenger. 1991. Situated learning: legitimate peripheral participation. Cambridge Univ.
Press.

Laws, P. 1997. Millikan Lecture 1996: Promoting active learning based on physics education research
in introductory physics courses,_ Am. J. Phys. 65(1): 13-21.

Lederman, L.M. 1999. A science way of thinking. Education Week, 16 June. [online] URL:
< http://www.edweek.org/ew/1999/40leder.h18 >.

Livanis, O. 2000. Physics First [online] URL: < http://members.aol.com/physicsfirst/index.html >.

Mahajan, S. & R.R. Hake. 2000.  Is it finally time for a physics counterpart of the Benezet/Berman
math experiment of the 1930_s? Physics Education Research Conference 2000: Teacher Education.
[online] URL: < http://www.sci.ccny.cuny.edu/~rstein/perc2000.htm >.

34

Marchese, T. 1998. Not-so-distant competitors: how new providers are remaking the postsecondary
marketplace.  AAHE Bulletin, May 1998. [online] URL:
< http://www.aahe.org/Bulletin/Not-So-Distant%20Competitors.htm >.

Mayer, R.E. 2000. What is the place of science in educational research? Educational Researcher
29(6): 38-39. [online] URL: < http://www.aera.net/pubs/er/toc/er2906.htm >.

Mazur, E. 1997. Peer instruction: a user’s manual.  Prentice Hall. [online] URL:
< http://galileo.harvard.edu/ >.

McDermott, L.C. 1991. Millikan lecture 1990: what we teach and what is learned: Closing the gap.
Am. J. Phys. 59(4): 301-315.

_______. 1993. Guest Comment: How we teach and how students learn – A mismatch? Am. J. Phys.
61(4): 295-298.

McDermott, L.C. & E.F. Redish. 1999. RL-PER1: Resource letter on physics education research.
Am. J. Phys. 67(9):755-767. [online] URL:
< http://www.physics.umd.edu/rgroups/ripe/perg/cpt.html >.

McKeachie, W.J. 1999. Teaching tips: strategies, research, and theory for university teachers.
Houghton Mifflin.

Mestre, J. & J. Touger. 1989. Cognitive research - what’s in it for physics teachers? Phys. Teach. 27:
447-456.

Minstrell, J. 1989. Teaching science for understanding. In L.B. Resnick & L.E. Klopfer, eds., Toward
the thinking curriculum: current cognitive research. Association for Supervision and Curriculum
Development (ASCD) Yearbook.

Mottmann, J. 1999a. Innovations in physics teaching – A cautionary tale. Phys. Teach. 37: 74-77.
_______, 1999b.  Mottmann replies.  Phys. Teach. 37: 260-261.

National Research Council. 1997. Preparing for the 21st century: the education imperative. National
Academy Press, USA. [online] URL:  < http://books.nap.edu/catalog/9537.html >

35

_______. 1999. Global perspectives for local action: using TIMSS to improve U.S. mathematics and
engineering education.  National Academy Press, USA. [online] URL:
< http://www.nap.edu/catalog/9605.html  >.

Neuschatz, M. 1999.  What can the TIMSS teach us? The Science Teacher 66(1): 23-26.

Novak, G.M. & E. Patterson. 1998.  Just-in-time teaching: active learner pedagogy with the WWW.
IASTED International Conference on Computers and Advanced Technology in Education, May 27 -30,
Cancun, Mexico. [online] URL:  < http://webphysics.iupui.edu/JITT/ccjitt.html >.

Novak, G. M., E.T. Patterson, A.D. Gavrin, W. Christian. 1999. Just in time teaching. Prentice
Hall. [description online] URL: < http://webphysics.iupui.edu/jitt/jitt.html >.

NSF Advisory Committee. 1996. Shaping the future: new expectations for undergraduate education
in science, mathematics, engineering, and technology. [online] URL:   
< http://www.nsf.gov/cgi-bin/getpub?nsf96139 >.

NSF Advisory Committee. 1998. Shaping the future, volume II: perspectives on undergraduate
education in science, mathematics, engineering, and technology. [online] URL:
< http://www.nsf.gov/cgi-bin/getpub?nsf98128 >.

Phillips, D.C. 2000. Expanded social scientist’s bestiary: a guide to fabled threats to, and defenses of,
naturalistic social science. Rowman & Littlefield.

Phillips, D.C. & N.C. Burbules 2000. Postpositivism and educational research. Rowman &
Littlefield, esp. Chap. 4: Can, and should, educational inquiry be scientific?

Physical Science Resource Center. 2001. American Association of Physics Teachers. Physical
Science Resource Center, especially under “Resource Center”/”Physics Education Research.” [online]
URL: < http://www.psrc-online.org/ >.

Pister, K. 1996. Renewing the research university. University of California at Santa Cruz Review
(Winter) [online] URL:
< http://www.ucsc.edu/news_events/review/text_only/Winter-96/Win_96-Pister-Renewing_.html >

36

Redish, E.F. 1994. Implications of cognitive studies for teaching physics. Am. J. Phys. 62(9): 796-803.
[online] URL: < http://www.physics.umd.edu/rgroups/ripe/perg/cpt.html >.

_______. 1999.  Millikan lecture 1998: building a science of teaching physics.  Am. J. Phys. 67(7):
562-573. [online] URL: < http://www.physics.umd.edu/rgroups/ripe/perg/cpt.html >.

Redish, E.F., J.M. Saul, R.N. Steinberg. 1997. On the effectiveness of active-engagement
microcomputer-based laboratories.  Am. J. Phys. 65(1): 45-54. [online] URL:
< http://www.physics.umd.edu/rgroups/ripe/perg/cpt.html >.

_______. 1998. Student expectations in introductory physics. Am. J. Phys. 66(3): 212-224. [online]
URL: < http://www.physics.umd.edu/rgroups/ripe/perg/cpt.html >.

Redish, E.F. & R.N. Steinberg. 1999. Teaching physics: figuring out what works. Phys. Today 52(1):
24-30. [online] URL:  < http://www.physics.umd.edu/rgroups/ripe/perg/cpt.html >.

Reif, F. 1995. Millikan lecture 1994: understanding and teaching important scientific thought
processes. Am. J. Phys. 63(1): 17-32.

Sarason, S.B. 1990. The predictable failure of educational reform.  Jossey-Bass.

_______. 1996.  Revisiting “The culture of the school and the problem of change.” Teachers College
Press.

Saul, J.M. 1998.  Beyond problem solving: evaluating introductory physics courses through the
hidden curriculum. Ph.D. thesis, Univ. of Maryland.

Schneps, M.H. & P.M. Sadler. 1985. Private Universe Project. Harvard –Smithsonian Center for
Astrophysics, Science Education Department. [online] URL:
< http://cfa-www.harvard.edu/cfa/sed/resources/privateuniv.html >.

Shapiro, I., C. Whitney, P. Sadler, M. Schneps. 1997. Harvard-Smithsonian Center for
Astrophysics, Science Education Department, Media Group, [online] URL:
< http://www.learner.org/catalog/science/mooo/ > and
< http://www.learner.org/catalog/science/mooo/mooodes.html >.

37

Shulman, L. 1986. Those who understand:  knowledge growth in teaching. Educational Researcher
15(2):4-14.

_______. 1987. Knowledge and teaching: foundations of the new reform.  Harvard Educational
Review 57: 1-22.

Slavin, R.E. 1992.  Research methods in education.  Allyn & Bacon.

Springer, L., M.E. Stanne, & S.D. Donovan. 1999. Undergraduates in science, mathematics,
engineering, and technology: a meta-analysis.  Review of Educational Research 69(1): 21-51. [abstract
online] URL: < http://www.aera.net/pubs/rer/abs/rer691-3.htm >.

Steinberg, R.N. 1999.  Expression of concern. Phys. Teach. 37, 260 (1999).

Swartz, C. 1999. Demise of a shibboleth. Phys. Teach. 37: 330.

Thompson, B. 1996. AERA editorial policies regarding statistical significance testing: three suggested
reforms. Educational Researcher 25(2): 26-30 (1996).

_______. 1998. Five methodology errors in educational research: the pantheon of statistical
significance and other faux pas, Invited address, annual meeting of AERA, San Diego, 4/15/98.
[online] URL: < http://acs.tamu.edu/~bbt6147/aeraaddr.htm >.

_______. 2000. A suggested revision to the forthcoming 5th edition of the APA Publication Manual.
[online] URL: < http://acs.tamu.edu/~bbt6147/apaeffec.htm >.

Thompson, W.L. 2001. 402 Citations Questioning the Indiscriminate Use of Null Hypothesis
Significance Tests in Observational Studies. [online] URL:
< http://biology.uark.edu/Coop/thompson5.html >.

Thornton, R.K.  & D. R. Sokoloff, 1990. Learning motion concepts using real-time microcomputer-
based laboratory tools. Am. J. Phys. 58(9): 858-867.

_______. 1998. Assessing student learning of Newton’s laws:  The Force and Motion Conceptual
Evaluation and the evaluation of active learning laboratory and lecture curricula. Am. J. Phys. 66(4):
338-351.

38

Tobias, S. & R.R. Hake. 1988. Professors as physics students: what can they teach us? Am. J. Phys.
56(9): 786-794.

Tobias, S. 1992a.  Guest comment: science education reform: what’s wrong with the process? Am. J.
Phys. 60(8):679-681.

_______. 1992b. Revitalizing undergraduate science: why some things work and most don’t. Research
Corporation.

_______. 2000. Guest comment: from innovation to change: forging a physics education agenda for
the 21st century, Am. J. Phys. 68(2): 103-104.

UMd-PERG. 2001a. Univ. of Maryland Physics Education Research Group, listing of physics 
curriculum materials based on physics-education research. [online] URL:
< http://www.physics.umd.edu/perg/ecs/matper.htm >.

UMd-PERG. 2001b. Univ. of Maryland Physics Education Research Group, listing of physics
education groups with web homepages [online] URL: < http://www.physics.umd.edu/perg/hp.html >.

Van Heuvelen, A. 1991a. Learning to think like a physicist: A review of research-based instructional
strategies. Am. J. Phys. 59(10): 891-897.

_______.  1991b.  Overview, case study physics, Am. J. Phys. 59(10): 898-907.

_______.  1995.  Experiment problems for mechanics, Phys. Teach. 33: 176-180.

Van Heuvelen, A.  & K. Andre. 2000 Calculus-based physics and the engineering ABET 2000
criteria.  Proceeding of the 2000 Physics Chairs Conference: Undergraduate Physics for the New
Century. [online]  URL: < http://www.aapt.org/ >.

Vygotsky, L.S. 1978. Mind in society:  the development of higher psychological processes. Ed. by M.
Cole, V. John-Steiner, S. Scribner, & E. Souberman. Harvard Univ. Press.

Wells, M., D. Hestenes, G. Swackhamer. 1995.  A modeling method for high school physics
instruction.  Am. J. Phys. 63(7): 606-619. [online] URL:
< http://modeling.la.asu.edu/modeling/MalcolmMeth.html >

39

Wilson, E.O. 1998.  Consilience: the unity of knowledge. Knopf.

Wilson, K.G. & B. Daviss. 1994. Redesigning education (Henry Holt, 1994), [description online]
URL:  < http://www-physics.mps.ohio-state.edu/~kgw/RE.html >.

Wilson, K.G.  & C.K. Barsky. 1998. Applied research and development: support for continuing
improvement in education.  Daedalus 127(4): 233- 258.

Wilson, R.C. 1986. Improving Faculty Teaching Effectiveness: Use of Student Evaluations and
Consultants. J. of Higher Ed. 57(2), 196-211.

Zeilik, M., C. Schau, N. Mattern, S. Hall, K.W. Teague, & W. Bisard. 1997. Conceptual
astronomy: A novel model for teaching postsecondary science. Am. J. Phys. 65: 987-996.

Zeilik, M., C. Schau, N. Mattern. 1998. Misconceptions and their change in university-level
astronomy courses. Phys. Teach. 36(2): 104-107.

Zeilik, M., C. Schau, N. Mattern. 1999. Conceptual astronomy. II. Replicating conceptual gains,
probing attitude changes across three semesters. Am. J. Phys. 67(10): 923-927.

Zollman, D.A. 1996. Millikan lecture 1995: Do they just sit there? Reflections on helping students
learn physics. Am. J. Phys. 64(2): 114-119.

_________________________________________________________________________________________

Address of Correspondent:
Richard R. Hake
Emeritus Professor of Physics, Indiana University
24245 Hatteras Street, Woodland Hills, CA 91367 USA
Phone: 818-992-0632
<rrhake@earthlink.net>
< http://www.physics.indiana.edu/~hake/ >

40


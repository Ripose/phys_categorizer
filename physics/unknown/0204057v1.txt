Eddington and Uncertainty 
 
BY IAN T. DURHAM 
 
Mathematical Institute, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 
9SS Scotland and Department of Physics, Simmons College, 300 The Fenway, Boston, 
MA 02115 
 
Sir  Arthur  Eddington  is  considered  one  of  the  greatest  astrophysicists  of  the  twentieth 
century  and  yet  he  gained  a  stigma  when,  in  the  1930s,  he  embarked  on  a  quest  to 
develop  a  unified  theory  of  gravity  and  quantum  mechanics.    His  attempts  ultimately 
proved  fruitless  and  he  was  unfortunately  partially  shunned  by  some  physicists  in  the 
latter portion of his career.  In addition some historians have been less than kind to him 
regarding  this  portion  of  his  work.    However,  a  detailed  analysis  of  how  this  work  got 
started shows that Eddington’s theories were not as outlandish as they are often purported 
to be.  His entire theory rested on the use of quantum mechanical methods of uncertainty 
in  the  reference  frames  of  relativity.    Though  the  work  was  not  ultimately  fruitful,  in 
hindsight  it  did  foreshadow  several  later  results  in  physics  and  his  methods  were 
definitely  rigorous.    In  addition,  his  philosophy  regarding  determinism  and  uncertainty 
was actually fairly orthodox for his time.  In this paper a brief background will be given 
regarding  the  larger  context  of  the  search  for  quantum  gravity.    Eddington’s  work 
regarding uncertainty in the reference frame will then be developed in detail.  This will be 
followed  by  a  look  at  the  historical  development  of  the  work  and  his  philosophy 
concerning  it.    Finally  the  paper  will  briefly  look  at  Eddington’s  subsequent  treatment 
historically and will point out some misrepresentations.  Critical analysis of his work is 
included throughout. 
 

Keywords: Sir Arthur Eddington; history of cosmology; relativity; quantum mechanics 
 

1.  Introduction 
 
Modern  cosmology  has  its  origins  in  many  revolutions  in  science.    Arguing  for  a  true 
beginning  may  be  a  matter  of  opinion.    But,  undoubtedly,  Einstein’s  theory  of  General 
Relativity stands among the most significant points in the history of the development of 
modern  cosmology.    The  true  breakthrough  of  relativity  was  the  fact  that  it  described 
gravity purely in terms of geometry.  Gravity, in fact, was a consequence of geometry.  It 
did not take long for physicists to attempt to link this groundbreaking theory to the other 
known force of nature at that time: electromagnetism.  Therefore, it stood to reason that 
electromagnetism would be the first place to turn in an effort to bring a sense of unity to 
physics. German physicist Theodor Kaluza first attempted the unification of relativity and 
electromagnetism  in  1919  by  extending  Einstein’s  field  equations  to  five  dimensions.  
The idea was later improved upon by the Swedish physicist Oskar Klein in 1926.  Interest 
in unification waned, however, in the face of quantum mechanics and it’s strike against 
determinism.    However,  there  were  physicists  who  attempted  very  early  on  to  unity 
relativity with the new quantum mechanics and so the quest for unification did not die out 

 

 

1 

entirely in the 1930s.  In fact, Einstein devoted the last thirty  years of his career to this 
quest. 
 
Today unification is widely considered the Holy Grail of physics.  Physicists have 
successfully wedded the strong, weak, and electromagnetic forces, but the marriage with 
gravity has  yet  to be  accomplished.   But,  nonetheless,  the  search  for  unity  has returned 
with a vengeance.  Wedding gravity to quantum theory is the most essential part of this 
quest and theories of quantum gravity have been at the forefront of physics research for 
nearly forty  years.   It is  important to recognize, however, that this unification trend did 
not  die  out  entirely  during  the  intervening  years  following  the  initial  interest  in  Kaluza 
and Klein’s work and the current work.  One of the first attempts at unification came with 
Paul Dirac’s famous papers of 1928 and 1929 in which he described a relativistic wave 
equation for the electron.  Sir Arthur Eddington, disappointed that Dirac’s work did not 
appear in tensor form, sought to rework the formulae to essentially  put  quantum theory 
into the language of relativity.  The work was the starting point for a grand, though not 
often  fruitful,  series  of  cosmological  theories  developed  by  Eddington,  Dirac,  and 
Edward A. Milne (known to most as Arthur).  Eddington’s work, which is arguably the 
most  thoroughly  studied  of  the  theories,  began  with  the  simple  premise  that  quantum 
mechanics  and  relativity  could  be  united  under  a  common  framework,  specifically 
centered around the idea of coordinates. 
 
The core of Eddington’s work thus was forced to center on uncertainty and how to 
incorporate  it  into  relativistic  mechanics.    Eddington  also  made  the  point  that  the 
introduction  of  uncertainty  into  physics  heralded  such  a  monumental  change  in  the 
philosophy  of  science  that  even  those  who  did  not  normally  find  the  philosophy  of 
science  to  be  of  interest  were  forced  to  consider  the  philosophical  implications  of  the 
concept.   It would be safe to say Eddington was firmly in the Copenhagen camp which 
not  only  expressed  the  view  that  uncertainty  was  inherent  and  inescapable,  but  the 
philosophical  implications  were  important  and  should  not  be  overlooked.    He  even 
denigrates the opposing interpretation (sometimes called the EPR interpretation of hidden 
variables) by saying that a scientist who suggests the idea of variables yet unknown to us 
as being the explanation for indeterminacy “wants shaking up and waking.” (Eddington, 
1934, 84).  This argument between determinacy and indeterminacy can also be described 
in  terms  of  inductive  versus  deductive  reasoning  with  the  deductivists  falling  in  the 
indeterminacy  camp  and  the  inductivists  (most  notably  Einstein)  falling  into  the 
determinacy camp.  The argument was not new having been a heated subject of debate in 
the  1820s  (early  Victorian  times).    At  that  time  William  Whewell  and  others  argued 
against  the  introduction  of  probability  theory  into  the  curriculum  at  Cambridge  and 
Oxford  on  the  grounds  that  it,  being  a  deductivist  argument,  deigned  to  tread  on  holy 
ground, supposing to solve problems better left to faith.  One of the probabilists Whewell 
argued  against  was  Laplace,  someone  Eddington  references  often  in  his  work  on 
uncertainty.  It is not impossible that Eddington knew of this early Victorian debate when 
working out  the philosophical implications of his work.    But  there is  no doubt that this 
fundamental  indeterminacy  was  important  to  Eddington’s  theory.    In  fact,  it  is  the 
indeterminate nature of the quantum world that Eddington saw as the fundamental block 
on which to build a larger, more unified theory. 
 
Eddington’s  detailed  work  on  uncertainty,  though  not  a  completely  successful 
theory,  actually  predicted  a  number  of  later  accomplishments  in  physics  including  the 

 

2 

need  for  a  quantum  mechanical  standard  of  measurement  (at  the  time  of  Eddington’s 
writing the Paris metre was the standard).  In fact it was this very assertion that led to the 
ultimate  framework  of  his  later  versions  of  the  theory  that  held  that  physical  events 
depended solely on dimensionless numbers.  This idea was later taken up by Dirac who 
developed an entire theory based solely on dimensionless numbers; he even called it the 
Large Numbers Hypothesis. 
 
Other  portions  of this work hint  at some  of the underlying  principles  of  modern 
quantum  field  theory  and  string  theory.    Certainly  modern  unification  theories  have 
drawn  heavily  on  early  20th  century  concepts  and  methods.    Kaluza-Klein  theory  is  an 
active  area  of  research  within  theoretical  physics  today.    But  Eddington  makes 
suggestions  that  are  in  themselves  close  to  ideas  in  use  today.    In  addition,  there  was 
some argument that persisted after Einstein’s work that suggested the speed of light was 
not  a  constant.    Eddington  firmly  rejected  this  hypothesis  but  not  on  philosophical 
grounds entirely (though he did suggest the measurement techniques weren’t up to snuff) 
but also through rigorous mathematics.  This latter fact should not be a surprise to anyone 
who has studied Eddington at any length.  He was a consummate mathematician choosing 
to prove all of his theories through rigorous mathematics.  Even   his earliest works were 
intensely mathematical in nature (Eddington, 1910).  But he had the knack of interpreting 
the mathematics, of sorting out the meaning behind them.  It is precisely the mathematics 
that launched this attempt to unify relativity and quantum mechanics.  Dirac’s quantum 
theory of the electron did not appear in tensor form originally and it was this problem that 
launched  Eddington’s  work  on  uncertainty  that  ended  with  his  death  in  1944  and  the 
posthumous  publication  of  his  monumental  tome  Fundamental  Theory  in  1946,  which 
contains the single best summary of this theory.   

A  detailed  synopsis  of  this  work  incorporating  uncertainty  into  the  reference  frame 
(for the uninitiated) will begin our look at Eddington.  We will then trace the historical 
development  of the work,  and, finally, will  critically  analyze  the philosophy behind his 
reasoning.    Discussing  this,  though,  requires  a  certain  knowledge  of  the  theory  and  his 
methods.  This, then, is where we begin. 
 

2.  Unification 

2.1  The Meeting Point 
 
In order to unify quantum mechanics and relativity one must find some common aspect 
shared  by  the  two.    Eddington  used  coordinate  systems  and  reference  frames  as  his 
meeting point.  Coordinates are, of course, vital to both theories being contained within 
Heisenberg’s Uncertainty Principle and being the basis of constructing a reference frame 
within relativity.  The marriage thus seems natural. 
 
Due  to  the  Principle  of  Equivalence,  which  ultimately  implies  that  there  is  no 
global inertial reference frame, the assumption is made that a reference frame is equal to 
a coordinate system.  In general relativity it also can be equivalent to a manifold that is an 
m-dimensional  ‘hyperplane’  in  n-dimensional  Euclidean  space  (m  ≤  n).    Basically,  a 
manifold  is  any  set  that  can  be  continuously  parametrized  where  the  number  of 
independent parameters is the number of dimensions and the parameters themselves are 
coordinates.  Metrics are often introduced onto manifolds in order to carry information. 

 

 

3 

In  quantum  mechanics  uncertainty  is  the  basis  of  observation.    The  observable 
 
parameters are then either the set of 1.) coordinates and momenta; 2.) energy and time; or 
3.) any two Cartesian coordinates of angular momentum.   In current language we often 
relate  a  quantum  field,  which  is  a  collection  of  position  dependent  operators  (often 
modeled  as  simple  harmonic  oscillators),  to  a  manifold  where  the  field  matches  the 
manifold point-for-point.  Relative to Eddington’s time the coordinates themselves were 
the underlying link between relativity and quantum mechanics. 
What Eddington did to relate the coordinates of relativity with the coordinates of 
 
quantum  mechanics  was  to  create  a  probability  distribution  for  the  origin  of  the 
coordinate system in relativity.  In doing this he is forced to make a distinction between 
the  physical  and  geometrical  origin,  relating  the  two  by  the  centroid  of  the  probability 
distribution.  So the centroid is the geometrical origin which is specified by the observer 
while  the  probability  distribution  is  the  physical  origin.    The  physical  origin  is  then 
correspondent  to  the  actual  physical  object  being  observed.    The  uncertainty  is  thus 
introduced  through  the  standard  deviation  of  the  Gaussian  distribution  of  this  physical 
origin.   His  assertion  is  that the centroid  is not an  actual  physical  landmark  and that in 
order to make the connection the distribution must be Gaussian because it is the only way 
to find the supposedly ‘unobservable’ centroid. 
All this may seem rather confusing but the point Eddington is attempting to make 
 
is that the physical coordinate system, which has a probability distribution as its origin, 
corresponds to the actual physical manifold of spacetime.  Therefore there is an inherent 
uncertainty  in  the  physical  coordinates  of  anything  on  the  manifold.    The  geometrical 
coordinate system, on the other hand, is simply a rigid set of coordinates designed by the 
observer  and  overlaid  onto  the  manifold  with  its  origin  at  the  centroid  of  the  physical 
coordinate  system’s  origin  (probability  distribution).    Eddington,  in  a  1937  paper 
[Eddington, 1937], better states the physical nature of this by saying that the curvature of 
the  universe  corresponds  to  the  standard  or  a  priori  probability  distribution  and  not  the 
vacuum of spacetime! 
 

2.2  Fluctuations and Perturbations 

 
Eddington also introduced fluctuations within the physical coordinate system in order to 
account  for  uncertainty.    The  ‘ordinary  fluctuation’  was  similar  to  the  standard  error 
which we know as the square root of the mean of a distribution.  It was thus related to the 
standard deviation.  The ‘extraordinary fluctuation’ was related to the number of particles 
and  the  particle  density  on  the  manifold.    He  makes  a  jump  with  this  last  definition, 
though,  defining  the  entire  volume  as  being  uncertain  in  the  extraordinary  fluctuation.  
The  purpose  of  these  fluctuations  is  to  fully  relate  the  physical  and  geometrical 
coordinate systems that alternately represent the actual physical location and the overlaid 
measurement system.  He states the exact different between these coordinate systems as 
having a fluctuation with a standard deviation in all directions due to the uncertainty of 
the physical origin and a fluctuation with a standard deviation σ∈r in the radial direction 
which is due only to the uncertainty in the scale of the measurement of r.  In this direction 
 
 
1
2

σ∈ =

(1.1) 

N

 

 

 

4 

 
where  N  is  the  number  of  particles,  assumed  to  be  very  large.    This  derivation  comes 
∗
from  a  theorem  developed  by  James  Bernoulli
.    The  second  fluctuation  is  the 
extraordinary fluctuation and it is combined negatively (no explanation is given) with the 
other fluctuations which makes the total standard deviation in the radial direction 
 

(
σ σ∈−

2

2 2r

)1/ 2

. 

(1.2) 

 
This  is  described  as  the  ‘local  uncertainty.’    There  is  also  a  transverse  uncertainty  that 
appears without an extraordinary fluctuation. 
What exactly is this combined uncertainty physically speaking?  It is derived as a 
 
combination  of  the  uncertainty  of  the  distance  to  the  origin  (speaking  in  physical 
coordinates)  with  the uncertainty  in  the  scale itself.   It  is  described  by  Eddington as an 
uncertainty  of  the  local  physical  origin  relative  to  the  local  geometrical  origin.    This 
really  implies  that  the  fluctuations  should  be  able  to  be  combined  both  negatively  and 
positively  depending  on  their  relation  to  each  other  rather  than  just  negatively  as 
Eddington has implied. 

So Eddington actually has two uncertainties in his merger of quantum mechanics 
and relativity.  He first defines an uncertainty in the actual physical location of objects in 
the reference frame which translates as the uncertainty in the physical coordinate system.  
But  he  then  adds  to  that  an  uncertainty  in  the  relative  distance  between  this  physical 
origin and any overlaid geometrical origin.   If σ∈r is small then the local uncertainty is 
simply the standard deviation due to the uncertainty of the origin and thus the uncertainty 
in  the  relative  distance  between  the  physical  and  geometrical  origins  is  negligible.    He 
further says that local uncertainty could be adopted as the standard of measurement in a 
particular  direction  and,  if  done,  it  would  be  called  the  ‘σ-metric’  or  the  σ  system  for 
defining lengths. 

How does this relate to the uncertainty in quantum mechanics?  The second of the 
two  uncertainties  that  Eddington  described  is  the  uncertainty  we  know  through  our 
standard understanding of quantum mechanics.  It is, quite simply, the uncertainty in the 
location  of  the  object  relative  to  our  coordinates  of  measurement  (geometrical 
coordinates in  Eddington’s  terms).    The other  uncertainty Eddington  introduces implies 
an inherent uncertainty in the particle’s actual physicality.  It implies that even if we were 
to  find  some  way  to  overcome  our  standard  idea  of  uncertainty  as  embodied  in 
Heisenberg’s  principle,  we  would  still  be  constrained  by  the  very  nature  of  spacetime 
itself. 

 

 

2.3 The σ-metric 

Returning to Eddington’s σ-metric, let ds be a length of a line element given by dr, rdθ, 
and rsinθdϕ.  Given the definitions of the radial and transverse uncertainties, we find that 
radial and transverse lengths are proportional to 
 
                                                 
∗ The fluctuation y has the distribution law 

}
)

/
n N
0

( )
y

(
1

(
−
1

=

−

1/ 2

e

. 

n
0

/ 2

−

−

)

y

2

{
π
2
n
0

n N
/
0

Nf

 

 

5 

dr
(
σ σ∈−

2

)1/ 2

2 2
r

,

r

θ
d
σ

r
, sin

θ

ϕ
d
σ

 

 

(1.3) 

which then produces the metric 
 

2

ds

=

2

dr
(
σ σ∈
2
/

2

)

2

r

−

1

+

2
r d

θ
2

+

2

r

sin

2

θ ϕ
2
d

 

(1.4) 

 
This  assumes  a  uniform  particle  distribution  that  Eddington  refers  to  as  the  ‘steady’ 
distribution.  But there is an inherent scale uncertainty present that arises from the idea 
that the measurement standard is not known exactly.  In fact, this is absolutely true and is 
due  to  equipment  inadequacies  and  quantum  randomness.    But  both  are  physical 
manifestations  and  not  geometric.    Minimizing  σ∈r  ensures  uncertainty  by  ensuring  a 
non-zero  value  for  Eq.  1.2.    Eddington  says  that  this  makes  any  fluctuation  in  r 
attributable to uncertainty in the standard of measurement. 
 
Eddington’s  solution  was  to  introduce  curvature  which  eliminated  this  problem.  
His  desire  is  to  find  a  single  Gaussian  distribution  attributable  to  probability  which 
eliminates any contribution from the standard of length.  This makes the space spherical, 
not  flat,  which  replaces  the  curvature  of  space  with  uncertainty.    This  means  in 
Eddington’s solution the curvature of space is equivalent to the uncertainty!  It is a rather 
novel idea and is reminiscent of some applications of modern theoretical physics. 
 

2.4 The Standard of Measurement 

 
This  led  Eddington  to  the  conclusion  that  traditional  dimensional  quantities  cannot  be 
used  to  describe  or  define  a  unit  of  length  such  as  a  metre,  the  reason  being  that  the 
physical nature of traditional dimensional quantities constrained the accuracy.  Eddington 
held that pure numbers only  could be used,  at least  in the quantitative  analysis.   At  the 
time of Eddington’s writing the specification of physical structure by pure numbers (e.g. 
numbers  of  elementary  particles  in  configurations  of  states  is  defined  by  quantum 
numbers) had already been developed in quantum theory.  So Eddington required that the 
standard of length be a quantum specified value, which is basically what it is now. 

The  reason  Eddington  went  through  the  difficult  and  abstract  process  described 
above was that he recognized the limitations of the measurement system of the time.  In 
the 1930s the standard of length measurement was the Paris Metre.  No definitive atomic 
or light-related (quantum) length or time interval had yet been developed and thus there 
was  an  inherent  uncertainty  in  the  standard  of  length  that  was  even  evident 
macroscopically in the many attempts at copying the Paris Metre for use elsewhere.   In 
fact, it was not until 1960 that an atomic standard was adopted.  But even this standard 
had trouble as it  was  based on a particular  red-orange  light  emitted  by  Krypton-86 in a 
gas discharge tube.  The reproducibility of Krypton-86 was a limiting factor in itself.  In 
1983 this idea was dropped in favor of the current standard which defines a meter as the 
path taken by light in a vacuum during the time interval of 1/299,792,458 seconds.  This 
implies that the speed of light has to be exact.  This is a quantum mechanical definition 
and is just the type of description Eddington wanted. 

 

 

 

6 

Eddington, not having a uniformly accepted quantum mechanical standard at the 
time,  found  his  own  way  around  the  difficulty.    He  defined  everything  in  terms  of  the 
periods  of  light  waves  and  the  vibrations  of  crystal  lattices  –  the  former  being  very 
similar to our current definition – and found that the consequence was a constant speed of 
light.  This is something he had always believed but he now held that it was necessary in 
order  to  have  exact  measurement  standards.    In  fact  he  is  quite  abrupt  in  regard  to  the 
constancy  of  light  speed,  going  so  far  as  to  make  derogatory  comments  about  the 
opposing viewpoint. 
 

2.5 The Observer 

 
He was then forced to deal with the idea of the observer.  Normally there are two ways in 
which to measure the distance between two objects.  The first is to measure the distance 
ξ ,  then  measure  the  distance  from  the 
from  the  first  point  to  some  observation  point, 
ξ .    Obviously,  this  observation  point 
second  point  to  the  same  observation  point, 
represents  the  location  of  the  observer.    Thinking  in  terms  of  vectors,  the  distance 
− .    The  observation  point  in  this  case  can  be 
between  the  two  is  then 
considered as the origin of a coordinate system. 

ξ ξ ξ
r
s

=

rs

r

s

The other way in which to measure the distance between the two, however, is to 
ξ ′ .  Both 
simply make a direct measurement from one to the other which we will call 
ξ ′  and 
ξ  have different probability distributions.  When measuring in relation to the 
origin,  then,  an  additional  level  of  uncertainty  is  introduced  by  the  uncertainty  in  the 
actual location of the origin.  The difference in the location of the origin with respect to 
the  first  object  and  its  location  with  respect  to  the  second  object  has  a  Gaussian 

rs

rs

rs

distribution  with  a  standard  deviation  of 
ξ ′  and 

says that the difference between 
and is otherwise insignificant at larger sizes.   

rs

rs

2σ .    Therefore 

ξ ξ σ′

±

=

rs

rs

2

.    Eddington 

ξ  only becomes important at the nuclear level 

 

 

2.6 The Range Constant and the Hubble Parameter 

This relates then to something he called the range constant of non-Coulombian energy.  It 
should be noted that this range constant has not been found in the literature of the time 
and is not a commonly known constant for nuclear physics.  Presumably it relates to the 
distance  over  which  certain  nuclear  forces  act  as  Eddington  describes  it  in  terms  of 
proton-proton collisions and claims the constant has been measured in a laboratory.  But 
it  is  possible  that  there  is  some  relation  to  electromagnetic  forces  at  certain  collision 
energies as well.  Unfortunately this remains a bit of a mystery.   

What Eddington does with this constant is rather interesting, and, in fact, it would 
be  even  more  interesting  if  it  were  shown  that  this  constant  is  indeed  something 
measurable  in  a  laboratory  (which  he  claimed)  and  not  simply  some  product  of 
Eddington’s  head.    The  interesting  thing  he  does  with  this  constant  is  apply  it  to  an 
.    The  second  half  of  this  equation  is  explained  in 
Einstein  universe, 
further  detail  in  New  Pathways  in  Science  [Eddington,  1934].    R0  is  the  radius  of  the 

σ=
2

R
0

N

=

k

/

 

7 

universe  which  also  represents  the  maximum  radial  uncertainty  of  any  particle  in  the 
universe (see Section 3.3 on the derivation of h).  If there are N particles in the universe, 
then k represents the uncertainty of the origin of those same N particles.  (The nature of 
these values is also discussed in New Pathways in Science).  The value for k is given as 
1.9 x 10-13 cm.  In 1916 Einstein derived: 
 

(1.5) 

(1.6) 

(1.7) 

 
If N is taken as the number of particles in the universe then: 
 

GM
2
c

1
π=
2

R
0

. 

=

M

Nm
h

 

1
2

hm  is the  mass  of  the hydrogen  atom.   This  gave  an  approximation  for the total 

 
where 
amount of mass in the universe.  Combining equations 1.5 and 1.6 yields: 
 

R
0
N

=

Gm
h
cπ
2

=

×
3.95 10

−

53

cm

. 

0R   and  N  and 
The  equation  for  k  combined  with  equation  1.7  yields  values  for 
establishes a relationship between the number of particles in the universe and its radius.  
The  values  derived  for 
0R   and  N  are  the  fundamental  basis  of  Eddington’s  entire 
cosmology.  He finds that N is equal to roughly 1079 which is his estimate of the number 
of particles in the universe.   
0R  is then roughly 296 MPc.  It is interesting to note that in 
New Pathways in Science Eddington derives this in a slightly different way.   He solves 

for the mass unit supplied by the universe as a comparison object as 

=

m
0

N
h
c Rπ
2

.  It is 

the starting point for his work on the cosmical constant but, suffice it to say, he derives a 
value via another method that allows him to calculate the values for R0 and N which are 
the  same  as  those  found  in  the  alternative  way  in  Fundamental  Theory.    The  first 
derivation of this sort, however, appeared in 1931 [Eddington, 1931].  In this derivation 

π α
2 mc
h

=

N
R

he finds that 

where 

α

=

hc

/ 2

π
e

2

.  This is similar to his derivation from 

New Pathways in Science with the addition of α.  Eddington makes an argument that α 
should  be  equal  to  136  (not  137),  but  if  one  ignores  the  numerical  value  and  simply 

substitutes the algebraic expression into this latter derivation, one finds 

which 

relates  the  statistical  variance  of  a  large  assemblage  of  particles  (with  radius  R) 
represented by the right side of the equation, with a ratio of the rest energy of a particle to 
the square of its charge.  This is a very curious result.  Eddington relies on the fact that it 
‘is well known that the ratio of hc to e2 is a pure number.’ [Eddington, 1929]  This is an 

2

mc
2
e

=

N
R

 

 

 

 

 

8 

early  attempt  by  Eddington  to  explain  the  fine-structure  constant,  though  some  definite 
unit  manipulation  is  required  to  make  it  work  and  this  manipulation  appears  severely 
artificial. 
 
 

The limiting speed for the recession of the galaxies is given by 

=

V
0

c
R
0 3

 

(1.8) 

0R   he  found 

 
Given  his  value  for 
0V   to  be  585  km/s/MPc.    Hubble  and  Humason  had 
observed  a  value  of  560  km/s/MPc.    Later,  with  a  new  value  for  the  range  constant, 
Eddington refined this to 572.4 km/s/MPc.   In his 1931 paper he found 528 km/s/MPc.  
At that time both Hubble and De Sitter had found 465 km/s/MPc.  Eddington attributes 
the  difference  to  uncertainties  in  the  distance-scale  and  other  astronomical  problems 
which  he  says  can  be  upwards  of  20%.    In  1937,  Eddington  found  a  value  of  432 
km/s/MPc [Eddington, 1937].  Of course, this number has been greatly refined in the past 
60  years  and  is  now  estimated  to  be  close  to  only  one-tenth  of  that  value  (∼50-70 
km/s/MPc). 
 

2.7 Scale Fluctuations 

 
The  so-called  range  constant  that  led  Eddington  on  this  road  can  be  understood  by 
returning briefly to the discussion of measuring distances between objects.  If we take our 
hypothetical situation of measuring the distance  between two particles and focus on the 
uncertainty  in  the  measurements  with  respect  to  the  origin  we  can  then  apply  it  to 
spherical space.  If we do we can then find that a projection of the distance between the 
geometrical origin of a hypersphere and an actual centroid of N particles is roughly equal 
to the range constant.  By this method he makes the conclusion that the physical scale of 
the  universe  is  actually  larger  than  the  geometrical  scale.    This  makes  sense  from  his 
standpoint  because  it was his  assumption  that  geometry  is  a projection of  an  imaginary 
measurement  grid  onto something that actually can’t be measured that precisely.   It’s a 
bit analogous, perhaps, to a leaky paper cup.  A  little bit of liquid (the physical aspect) 
will always escape the boundary of the cup (the geometry).  His physical rational for this 
[Eddington,  1937]  is  that  when  the  whole  energy  tensor  is  represented  by  waves,  the 
waves are in flat space and force the spherical space to be projected. 
 
The  difference  between  the  physical  and  geometrical  scales,  then,  is  called  the 
scale  fluctuation.    He  went  on  to  suggest  that  in  order  to  find  local  irregularities  in 
general  relativity  which  were  already  known  to  be  present,  one  should  not  employ  the 
concept of statistical fluctuations, though it might seem logical.  This is because there is 
an  additional  fluctuation  in  the  actual  scale  of  measurement  –  the  scale  fluctuation  we 
have just introduced.  Interestingly enough, he loosely equated quantum mechanics with 
the method of statistical fluctuations.  Therefore he finds that the only meeting point of 
quantum mechanics and relativity is a steady distribution in an Einstein universe because 
statistical fluctuations are not used, but scale fluctuations are, which are still probabilistic 
and therefore, in his view, quantum mechanical. 
 
By doing this he appears to gloss over Newtonian mechanics entirely, and, in fact, 
he  does  not  even  mention  Newtonian  mechanics.    But  Newtonian  mechanics  can  be 

 

 

9 

properly  derived  from  general  relativity  and  could  thus  be  seen  as  an  extremely 
specialized case.  He does mention briefly an intermediate theory that is curved but in a 
uniform  space-time  in  which  the  metric  is  unchanging.    This  certainly  could  be  his 
approximation  of  Newtonian  mechanics  but  little  elaboration  on  his  part  leaves  the 
question still open. 
 
The derivation of his theory, however, utilizes a changing metric.  He begins by 
considering  small  perturbations  which  he  equates  to  quantisation  and  moves  to  larger 
perturbations  which  he  equates  with  irregular  curvature.    So  the  underlying  aspects  of 
both  quantum  mechanics  and  general  relativity  are  related  to  the  idea  of  perturbation.  
The problem  lies in the  fact  that  in  a  gravitational  field the  wave  equation  contains  the 
metric which does not change during quantum mechanical perturbations.  But in general 
relativity  the  metric  is  required  to  change  and  thus  there  is  an  inherent  conflict.    In  a 
footnote in his Fundamental Theory he says: 
 

Thus  attempts  to  ‘extend  Dirac’s  wave  equation  to  general  relativity’ 
are  misguided,  but  probably  the  intention  is  only  to  extend  it  to 
generalized  coordinates  in  flat  space  by  putting  it  into  tensor  form.  
This is a purely  mathematical transformation  in no  way dependent on 
the theory of relativity. [Eddington, 1946, 12] 
 

2.8 Uranoids 

 
In  order  to  understand  the  fluctuations  and  perturbations  which  were  the  essence  of 
Eddington’s  theory,  he  realized  that  it  was  impossible  to  consider  an  object  without 
considering the surrounding environment as well.  By the time Fundamental Theory had 
been  published  in  1946  the  term  he  used  to  describe  simple  surrounding  environments 
was uranoid.  We will examine later from where this idea arises, but it should be noted 
for  consistency  that  he  appears  to  have  still  considered  the  aether  as  something  quite 
tangible and far apart from any vacuum-type space.  This should be kept in mind when 
discussing his concept of the uranoid. 

The  uranoid  to  Eddington  was  a  simple  environment  much  like  an  ideally 
simplified  universe  (electrically neutral  and  uniformly distributed).   His  uranoid for the 
merged  system  of  quantum  mechanics  and  relativity  was  a  uniform  probability 
distribution of particles.  This is an Einstein universe that occupies a hyperspherical space 
and  is  at  zero  temperature  which  means  the  particles  are  nearly  (or  completely)  at  rest.  
This  simplifies  the  situations  by  removing  any  potential  radiation  –  mass  is  the  only 
consideration  in  relation  to  these  particles.    This  standard  uranoid,  then,  must  also  be 
electrically  neutral  which  means  any  electromagnetic  fields  must  be  considered  in  the 
object-system and not the environment. 

 Even  though  the  universe  was  supposed  to  appear  as  a  uranoid  in  every  single 
problem, it would not affect local problems very much.  So he claims that, for example, 
even radical changes in extra-galactic nebulae (galaxies) would only alter a local metric 
which  can  be  fixed  through  a  coordinate  transformation.    The  reason  he  insists  on 
including  the  entire  universe  as  an  environment  is  to  save  the  trouble  of  dividing  it 
because,  by  dividing  it,  boundary  conditions  must  be  introduced  which,  he  says,  will 
simply continue the environment beyond the boundary.  He is thus jointly considering a 

 

10 

system of a very large number of particles in conjunction with some small system that’s 
being studied (like a planet in a solar system). 
 

2.9 Unification by Parts 

 
Until this point, Eddington has considered a metric where a large system determines the 
uncertainty in the physical frame and thus the scale of the various structures in the frame.  
Now  he  takes  this  a  step  further  by  bringing  in  mechanics.    The  uranoid  is  a  vast 
assemblage  of  particles  and  we  now  must  consider  any  physical  interactions  within  the 
assemblage.    In  essence,  we  at  first  were  considering  a  metric  where  as  now  we  are 
dealing  with mechanics.  But Eddington  says  that  the two concepts  are  really identical.  
Specifically, he says: 
 

Einstein’s  theory,  by  unifying  geometry  and  mechanics,  unifies  the 
metrical and mechanical effects of the environment; both are included 
in  the  description  of  the  influence  as  a  ‘field  of  gµυ ’.  [Eddington, 
1946, 14] 
 

In  relativity,  gµυ ,  is  usually  a  gravitational  field.    Sometimes  it  is  considered  as  being 
inertial-gravitational which allows it to be divided into Newtonian and Einsteinian parts.  
According to  Eddington, the uranoid is inertial and  any  deviation  from the surrounding 
environment is gravitational.  This means Eddington can use gravity in his development 
in  his  work 
of  relativistic  quantum  mechanics,  something  Dirac  did  not  do 
(understandably)  and  which  then  led  Eddington  into  his  first  work  in  this  area.    By 
including the inertial sense in the guise of the uranoid he then also manages to include the 
particle  masses.    This  is,  ultimately,  the  physical  nature  behind  his  unification  concept.  
The  mathematical  nature  deals  directly  with  the  uncertainty  relationship  while  the 
physical nature involves dividing out the problem into inertial and gravitational parts. 
 

2.10 The Standard of Measurement, Part II: Natural Units 

 
Returning  to  standards  of  measurement  for  a  moment,  Eddington  found  it  necessary  to 
adopt  a  somewhat  unique  system  of  natural  units  in  order  to  properly  measure  and 
characterize  various  quantities.    Naturally  he  set  c=1.    However,  he  also  set  8πκ! 2=1.  
This  second  naturalization  is  designed  to,  once  again,  relate  relativity  and  quantum 
mechanics  through  the  gravitational  constant,  κ,  and  ! ,  which  is  Planck’s  constant 
divided  by  2π.    This  ultimately  leaves  us  with  only  a  single  dimension-index  to  worry 
about  and  it  thus  eliminates  the  need  for  extra  physical  standards  (e.g.  a  kg,  J,  etc.).  
Everything is then measured in units of length or something else entirely if one desires.  
This  also  allowed  Eddington  to  describe  the  internal  structure  of  a  system  entirely  in 
terms  of  numerical  ratios  which  was  yet  another  backbone  of  Eddington’s  entire 
Fundamental Theory (but not one that is central to this particular paper). 
 
It  should  be  noted  that  Eddington’s  use  of  natural  units,  particularly  if  one 
measures everything in units of length, is fairly common in modern relativity, particularly 
in those sources that utilize a geometrical approach [Schutz, 1990].  The major problem 
with his formulation is that it is inconsistent with itself.  There are several common ways 

 

11 

of devising natural units, the most common among relativists being the setting of c equal 
to 1 which makes 3.0 x 108 m = 1s.  This is something Eddington employed.  But he then 
goes  on  to  set  8πκ! 2=1.    In  this  formulation,  2.652  x  10-26  m2  =  1s.    This  is  entirely 
inconsistent with the previous formulation using c!  Being a very careful man Eddington 
had an answer, at least mathematically. 
 
His  point  was  supposed  to  be  that  everything  could  not  only  be  measured  in  a 
single  unit  (like  length),  but,  contained  within  that  same  equality  was  a  connection 
between  the  gravitational  constant  and  ! .    He  had  said  that  this  single  unit  of 
measurement would have worked well with the idea of uncertainty in the reference frame 
(particularly  if  it  was  a  length  unit)  and  said  that  the  scale  of  the  system  was  actually 
measured  by  an  outside  observer  unless  the  system  was  the  universe  itself  in  which 
nothing needed to be done.  Thus if the system were the entire universe, everything rolled 
along as planned.  If the system was smaller, however, an extraneous standard was left in 
the system that had to be dealt with. 
Something  called  the  extraneous  standard  was  supposed  to  link  the  fragmented 
 
divisions  of  the  universe  into  a  single  cohesive  whole.    So  what  is  this  extraneous 
standard?  Let us consider the standard of length, for instance.  The extraneous standard 
may  be  defined,  for  example,  as  1 σ±   cm.    In  general,  then,  for  any  physical  quantity 
with the dimensions of (length)y the scale uncertainty can then be written 1 yσ±
.  So for 
the usual system of natural units with c set to 1, the scale uncertainty is  1 σ± .  For the 
second system where 8πκ! 2=1, the scale uncertainty is  1 2σ±
s∼ .  By this 
 because 
interpretation, the latter system of natural units has a greater uncertainty. 
 
But what does this really mean in relation to linking the fragmented divisions of 
the  universe?    It  is  unclear  exactly  what  he  meant  by  this,  but  based  on  his  next  few 
statements and derivations, a meaning does suggest itself.  It is probable that Eddington 
saw this fragmentation in the different measurement values in the universe – e.g. meters, 
grams, Joules, Volts, etc.  Eddington hints that the simplest interpretation of the universe 
is as a single underlying phenomenon  with infinite  manifestations, these  manifestations 
being everything we can measure in the universe.  This is philosophically quite profound. 
For  instance,  his  system  of  natural  units  defines  the  standard  gram  and  the 
 
standard  meter  in  relation  to  each  other  and  not  separately.    This  means  that  the 
uncertainty  in  their  scales  is  also  related  which  means,  when  dealing  with  probability 
distributions, defining separate standards is not sufficient.  By employing such a standard 
of natural units it abolishes redundant standards and narrows the choice of the procedure 
for a measurement.  In Eddington’s eyes, a quantum mechanical standard of length would 
then have produced quantum mechanical standards for every other measurable  property 
in  the  universe  but  would  leave  only  the  length  one  to  be  calibrated.    It  eliminates  the 
ambiguity developed in Section 2.5. 
 

2m

2.11 The Standard of Measurement, Part III: The Field Equations and Momentum 

 
As an introduction to his discussion of the field equations we should first pause and make 
a note of the notation Eddington used.  It is common today to identify the Ricci scalar as 
R and the Ricci tensor as  Rαβ .  Eddington, however, used the notation G and  Gαβ.  This 

can  often  be  confusing  because  the  Einstein  tensor  is  today  often  identified  as  Gαβ.  

 

12 

Eddington did not use the Einstein tensor in his work on uncertainty and instead chose to 
make the direct relation between the Ricci tensor and scalar and the stress-energy tensor 
skipping the intermediate step.  Therefore, Eddington wrote: 
 

πκ−
T
8

µν

=

G

µν

−

g G
µν

 

1
2

 
where  Gαβ and G are the Ricci tensor and scalar respectively. 
 
as 
 

Eddington then defines the familiar momentum vector from quantum mechanics 

(1.9) 

(1.10) 

p

µ

= −

i

!

∂
∂
x

µ

. 

 
Given the system of natural units we have adopted, then  8 Tµνπκ  has the same number of 

2

pµ





!

dimensions  as 

.    We  are  also  assuming  that  8 Tµνπκ   is  describing  a  spherical 

curved space and  this  means that  Tµν  has the same units as 

2pµ, or more appropriately, 
p pµ ν .    Therefore,  the  stress-energy  tensor  (which  Eddington  often  simply  called  the 
energy tensor) is both dimensionally and tensorially a product of two momentum vectors.  
Through similar reasoning he is able to conclude that, both dimensionally and tensorially, 
a particle density and a momentum vector are the same thing.  Once again he has, through 
this process, drawn a link between relativity and quantum mechanics.  He also held that 
this was evidence that his system of natural units was the simplest and that it required less 
reworking to produce the desired units. 
 

2.12 Eliminating the Scale 
 

The  reduction  of  all  physical  measurements  to  a  single  unit  thus  led  Eddington  to 
consider the idea of a completely scale-free form of physics.  In order to accomplish this 
task  it  would  be  necessary  to  eliminate  both  σ  and  R0.    All  structures  needed  to  be 
adjustable to any scale.  If we work with an assemblage or particles and mass ratios we 
are still scale-free because we’re not dealing with actual masses – simply their ratios (a 
unitless number).  In Eddington’s own words: 
 

If we specify the characteristics of a system in terms of an extraneous 
standard,  and  consider  the  series  of  systems  formed  by  varying  the 
standard  but  keeping  the  specification  the  same,  then  (for  a  scale-free 
system),  if  one  system  of  the  series  is  physically  possible,  all  are 
possible. [Eddington, 1946, 17] 
 

In  essence  this  gives  him  the  freedom  to  move  between  various  extraneous  standards 
which are defined using different combinations of units (see Section 2.10) and allows him 
to state that if a system defined using one of the standards is physically possible then all 

 

 

 

13 

are  physically  possible  in  every  extraneous  standard.    This  certainly  feels  like  a  cheap 
way of extracting assumptions on his part but he unfortunately does not elaborate much. 
 

2.13 Pseudo-discrete States 

 
In  a  similar  spirit  he  defines  pseudo-discrete  states  as  particles  that  cannot  be  precisely 
pinpointed within an assemblage which  requires  that the  particle  must  be considered  in 
concert  with  its  environment,  returning  us  to  the  concept  of  the  uranoid.    This  is  very 
reminiscent  of  quantum  field  theory  in  which  particles  are  often  considered  in  concert 
with  various  things  such  as  vacuum  energies,  fluctuations,  etc.    In  another  somewhat 
vague  jump,  however,  he  states  explicitly  that  pseudo-discrete  states  are  scale-free.    In 
terms  of  wave  mechanics  they  would  be  defined  as  infinite  plane  waves  while  discrete 
wave  functions  predict  a  particle  through  a  particle  density  or  the  probability  that  a 
particle  is  located  at  a  particular  point.    But  this  is  not  scale-free  because  it  involves  a 
deviation.    How  he  makes  the  assumption  that  pseudo-discrete  are  thus  scale-free, 
however, is not made clear. 
 
The  infinite  plane  wave  interpretation  is  defined  by  Eddington  as  falling 
‘between’  R0  and  σ.    Being  undefined  each  particle  is  not  distributed  over  a  cell  like  a 
wave  packet  but  instead  has  a  uniform  probability  over  the  entire  plane  wave  which  is 
infinite.  As such it cannot be specified as being in a particular location.  So, a pseudo-
discrete  wave  function  represents  a  particle  unidentified  in  a  large  assemblage.    A 
pseudo-discrete  wave  function  is  thus  not  continuous.    In  normal  distribution  functions 
some  discretized  parameter  becomes  continuous  making  the  function  continuous.    But 
wave functions contain a phase which contributes to interference when the functions are 
spatially aligned. 
The  parameters  within  a  normal  distribution  function  can  be  subdivided  into 
 
sections with each section being represented by (or containing) a wave function.  These 
are  what  Eddington  refers  to  as  his  pseudo-discrete  wave  functions.    If  the  entire 
occupation is contained within one of these pseudo-discrete states then the system is said 
to be nearly exact.  Eddington gives the example of a system nearly at rest as being one 
that can be considered under these assumptions. 
 

2.14 Pseudo-discrete States and the  Stress-Energy Tensor 

 
Eddington  then  makes  a  very  curious  definition  that,  again,  is  reminiscent  of  certain 
recent  theoretical  developments  in  physics.    He  first  says  that  the  proper  mass  of  a 
particle is a fixed characteristic.  But he then says that the proper volume is not fixed but 
can vary by varying the volume over which the probability distribution is extended.  This 
means that mass is scale-fixed and density is scale-free. 
 
Density,  of  course,  appears  in  the  stress-energy  tensor  in  general  relativity.  
Therefore  the  stress-energy  tensor  is  scale-free  (according  to  Eddington  –  he 
conveniently  ignores  the  other  components  of  the  stress-energy  tensor).    Momentum  in 
quantum mechanics is scale-fixed and so the transition can be made by using the pseudo-
discrete  wave  functions.    For  instance,  a  molar  object  can  be  represented  as  a  large 
  for  i 
ensemble  of  particles  in  these  pseudo-discrete  states  which  means 

∆∑
T

T

=

µν

µν

i

 

14 

particles where  Tµν∆
 is a characteristic representing  a pseudo-discrete  state.    However, 
only the probability of this state is known.  Thus the occupation factor, j, is interpreted as 
a probability of an individual particle as well as it’s frequency within the assemblage. 
 

2.15 Atomic Structure and the Philosophy of Free Information 

 
Eddington goes on to discuss the scope of scale-free theory, something he delves into in 
greater  depth  later  in  Fundamental  Theory.    One  of  the  curious  points  that  should  be 
mentioned  is  that  he,  once  again,  lays  groundwork  for  the  overall  themes  that  large 
dimensionless ratios are at the core of his theory.  He notes that the ratio of the masses of 
an  electron  and  proton  could  equally-well  be  described  as  a  density  ratio  of  two 
constituents of a hydrogen atom.  If it is a density ratio then, in fact, it is scale-free. 
 
The application to atoms can be expanded by describing generalized systems that 
include  atoms  with  a  correlation  of  coordinates  rather  than  a  more  complicated  wave.  
Infinite plane waves might seem like they need replacement by these more complicated 
waves in order to best represent practical applications.  Correlating coordinates eliminates 
this  problem.    For  instance,  given  a  proton  and  electron  in  a  closed  vessel,  they  are 
equally  likely  to  be  anywhere  in  the  vessel  at  any  given  time.    Eventually,  they  will 
(based on probability) combine to form a hydrogen atom and release a photon.  They are 
still equally  likely to be  anywhere  in the  vessel  but  their  distribution  functions  are now 
correlated.  Thus atomic wave functions are just correlated wave functions of the atomic 
constituents  and  not  distributed  wave  functions.    This  description  parallels  that  first 
employed by Schrödinger in his first paper on wave mechanics. 
 
Atomic structure can then be broken into two parts – mechanical and electrical – 
with the former being scale-free and the latter being scale-fixed.  To Eddington, mass and 
charge  (the  mechanical  and  electrical  information  of  an  atom  respectively)  are  ‘free 
information’ and not dependent on uncertainty – i.e. there is no uncertainty in the mass of 
the  electron.    This  view  could  be  debated,  of  course,  as  the  mere  act  of  measuring  a 
variable introduces an uncertainty in the resultant measurement.  This is because mass is 
often measured in relation to collisions, which involves momentum which automatically 
brings  in  some  level  of  uncertainty.    So,  in  fact,  Eddington  is  not  really  correct  in 
assuming that these properties are truly free information.  What Eddington is forced to do 
is to accept this problem but he does so by pointing out that there must be a break with 
observation – for instance, how do we know it is an electron?  It’s behavior, perhaps, is 
something still governed by uncertainty, which we know is, in fact, to some extent, true.  
An argument of semantics could be waged on labeling something an electron, but that is 
more  philosophical  and,  to  a  great  extent,  useless.    Scientists  are  required  to  make 
generalized labels in order to carry out their work.  The sum of the discussion, however, 
is that Eddington absolutely recognizes this and agrees that a pure observer really can’t 
have  any  ‘free  information.’    He,  then,  is  making  a  generalization  and  knows  it.    The 
trouble is that his generalization has been shown – and was shown even at that time – to 
be too much of a generalization when working with atomic particles. 
 
In recognizing that nothing really works this way Eddington calls the tabular data 
for mass, charge, etc. stabilized characteristics.  Technically, in Eddington’s theory, they 
are  not  observable  because  all  observations  are  probability  distributions  determined  by 
experiment  while  stabilized  characteristics  are  presented  as  part  of  a  theory.    This  is  a 

 

15 

2

. 

+

=

+

2
p
2

2
p
3

2
p
4

(1.11) 

+
2
p m
1

As an example, Eddington states that the four-momentum is 

very interesting idea.  In essence, Eddington is saying that a stabilized characteristic must 
be  derived  from  a  theory  and  not  found  experimentally  –  something  Eddington  was 
attempting to do with his theory. 
 
 
 
 
If p4 is a Hamiltonian, is m stabilized or not?  Eddington actually considers both cases.  If 
m  is  stabilized,  then  the  four-momentum  is  really  a  three-dimensional  distribution 
(because the stabilization of m limits the distributions to the other three momenta values).  
The  domain  of  the  probability  distribution  is  called  phase  space  by  Eddington  and  is 
somewhat  analogous  to  the  common  interpretation  of  phase  space.    The  number  of 
dimensions within this phase space enters Eddington’s  formulae as  a coefficient,  k, and 
most systems are based on this number.  So a particle with k-dimensions is called Vk.  In 
this notation a simple spinless particle is V3 and V4 depending on whether or not the mass 
is stabilized.  Therefore k can be interpreted as the number of degrees-of-freedom that a 
system has.  As such stabilization reduces the number of degrees-of-freedom of a system.  
This  certainly  makes  sense  from  an  intuitive  point-of-view.    Stabilization  essentially  is 
analogous to eliminating a variable from a system. 
 
Eddington is quick to point out, however, that, in order to maintain consistency (at 
least within his theory), limiting the system means also limiting the object-environment 
(e.g the uranoid).  If it is uniform, static, at zero-temperature, and electrically neutral (a 
uranoid), the heavy amount of restriction leaves  only two observables – R0 and σ.  The 
information about the environment (i.e. that it is uniform, static, etc.) Eddington says is 
free  information  similar  to  that  of  an  object-system  with  free  information  because  it  is 
specified and not derived.  This would then be the stabilization of the environment.  Any 
stabilization comes at a price, however: abandoning the tensor transformation properties.  
However, he states that invariant conditions as a whole can be used to stabilize it – e.g. 
the condition that it is anti-symmetrical or it is the outer-product of two vectors.  These 
examples  are  invariant  for  tensor  transformation  but  they  reduce  k  and  thus  introduce 
stabilization. 
 

2.16 Unification in a Nutshell 

 
The mathematical and physical derivation of Eddington’s unification thus begins with a 
thoroughly  complex  and  in-depth  treatment  of  uncertainty  in  the  reference  frame.  
Eddington  felt  that  this  was  the  only  point  on  which  to  build  a  common  theory  of 
quantum mechanics and relativity.  As he was able to show introducing uncertainty into 
the reference frame was not a simple procedure.  It introduced philosophical problems in 
dealing with observers and observables and led Eddington to suggest several unique ideas 
for his time including the concept of  a quantum mechanical standard of  measurement – 
something  that  is  employed  today.    Some  of  his  work  is  also  reminiscent  of  modern 
techniques  employed  in  quantum  field  theory.    The  physical  nature  of  his  theory  dealt 
with  pulling  apart  the  inertial  and  gravitational  aspects  of  gravitation  while  the 
mathematical  dealt  directly  with  uncertainty.    The  mixture  of  the  two  was  the  basis  of 
Fundamental Theory and contained early snippets of his philosophy of science in general.  

 

16 

But it is important to realize that the work, as described above, represents the final draft 
published posthumously.  There were numerous other drafts and, had he lived, there may 
have  been  more  before  the  work  was  ever  finally  published.    It  is  of  interest,  then,  to 
explore  these  to  perhaps  see  if  any  other  items  of  interest  were  buried  in  his  notes, 
removed from the final draft, or altered for whatever reason.  If the reader wonders at the 
necessity of doing this, recall that Einstein called his cosmological constant his greatest 
blunder and, yet, it has recently proved to be observationally ‘correct’ even if not entirely 
understood.    Therefore,  we  now  turn  our  attention  to  the  historical  development  of  this 
portion  of  Eddington’s  Fundamental  Theory  in  order  to  better  understand  what  he  was 
attempting to accomplish and, perhaps, to find something of merit in its development. 
 

3.  Historical Development of Unification 
 

3.1 The Aether 

 

We should begin by stating that Eddington’s support of indeterminacy was not unique to 
his time.  Louis de Broglie wrote in 1937 
 

In any case, in the present state of our knowledge, the Cartesian ideal of 
representing  the  physical  world  by  means  of  “figures  and  motion” 
seems to have suffered bankruptcy. [de Broglie, 1937, 247] 

 
It  is,  in  fact,  highly  unlikely  that  Eddington  didn’t  think  of  applying  uncertainty  very 
early  in  his  career.    Pinpointing  an  exact  date  would  be  a  useless  endeavour  as  it  was 
most  likely  a  slow  gestation  of  several  ideas  over  a  period  of  time.    However,  the  first 
clear  evidence  that  Eddington  gives  that  he  had  any  thought  of  introducing  some  level 
uncertainty into the reference frame of a system itself is in a letter addressed to Sir Joseph 
Larmor in 1932.  The idea that a particle was inseparable from its environment had been 
hinted  at  in  earlier  work,  but  the  uncertainty  of  the  frame  itself  was  not  discussed.  
Larmor and Eddington had exchanged letters for many years beginning in 1915 and had a 
curious relationship.  It began rather tersely in an argument regarding pacifism in World 
War I.  The importance of this early part of their relationship is born out in later letters 
where Eddington shows a great deal of respect for the older  Larmor that seems to have 
evolved out of the argument.   

It is of particular interest that Larmor wrote, in 1900, a book entitled Aether and 
Matter.    E.T.  Whittaker,  in  his  history  of  aether  and  electricity,  characterizes  Larmor’s 
views on the aether as being of the classical mold.  Larmor clearly believed in a ‘fluid’ 
description  of  the  aether  and  endeavoured  to  describe  electrical  charge  via  this  fluid 
concept.    This  imparts  definite  structure  to  the  idea  of  space-time  (aether)  and  was  the 
classical view held by most scientists into the early part of the twentieth century (debate 
still rages as to the true nature of space-time and the ‘aether’ – see, for example, Wilczek, 
1999). 

It  is  unclear  precisely  what  Eddington  believed  in  relation  to  the  nature  of  the 
aether itself, but he could, most assuredly, be attributed to the camp that assumed it had 
some  definite  structure  apart  from  the  pure  vacuum.    This  is  where  Eddington’s  first 
assumptions  about  uncertainty  manifest  themselves.    In  his  1932  letter  to  Larmor, 
Eddington,  by  this  time,  felt  comfortable  enough  in  their  relationship  to  request  that 

 

17 

Larmor peruse some  calculations  Eddington had made.  These calculations appear on a 
separate sheet and discuss the calculational details of radiation from a ring of n electrons.  
Eddington  sought  Larmor’s  advice,  specifically,  on  the  idea  of  movement  within  this 
system.  For instance, what if one electron were removed from the ring?  The ring would 
become  discontinuous  and  he  held  that  the  ‘propagation  of  a  discontinuity  is  a 
discontinuous  process.’    Therefore  he  introduced  a  vector  to  properly  describe  this 
discontinuity and he labels it the ‘aether displacement.’  Fundamentally, this declared the 
inseparability  of  the  electron  from  the  aether.    Thus  any  measurement  of  the  electron 
would  require  consideration  of  the  aether  as  well.    This  is  clearly  the  same  idea  as  the 
uranoid  concept  that  is  so  fundamental  to  Eddington’s  work  with  uncertainty  in  the 
reference frame.  Uncertainty is inherent because the frame cannot be separated from the 
object being observed! 

This specific development is also of interest with regard to the overall picture of 
aether, electricity, and gravitation.  Obviously gravitation is wedded to the concept of a 
structured  space-time:  curvature  of  space-time  produces  gravity.    One  cannot  have 
curvature  in  something  that  does  not  exist.    Thus  the  aether  is  simply  an  earlier 
description  of  space-time  and  early  modern  cosmologists  such  as  Eddington  certainly 
viewed the aether as something tangible apart from the vacuum of space.  What is curious 
is the relationship of the aether to electricity.  The relationship is born out in Whittaker’s 
work which, in fact, is titled A History of the Theories of Aether and Electricity.  And it 
was  electromagnetism  that  was  the  first  force  to  be  combined  with  gravity  in  a  single 
theory,  first  by  Theodor  Kaluza  and  then  by  Oskar  Klein.    It  is  not  surprising  that 
Eddington,  then,  in  his  letter  to  Larmor  relates  charge  to  the  aether  and  finds  they  are 
inseperable.    And  so  it  is  through  electromagnetism  that  Eddington  first  expresses  the 
concept of uncertainty in the reference frame. 

In the  Larmor letter of 1932, Eddington refers to the vector displacement of the 
aether as having the coordinates ξ, η, and ζ.  His derivation came from the motion of a 
charged  particle.    In  an  early  draft  of  his  Fundamental  Theory  (as  compiled  by  N.B. 
Slater, 1957) he carries this idea over and refers to a particle as a ‘conceptual carrier’ of 
these coordinates and their conjugate momenta.  By the time the final version had been 
completed particle coordinates were described by Gaussian wave packets.  But it is clear 
the concept of a particle, its relation to coordinates, and to the aether was well established 
early in his work. 

It  should  be  noted  that  he  recognized  that  the  aether  was  not  matter,  though  it 
certainly  had  substance.    He  makes  this  point  in  another  early  draft  of  the  manuscript 
when he expresses the opinion that the use of the term ‘field’ in place of ‘aether’ was ‘ill-
advised’  but  was  done  to  emphasize  this  point.    He  felt,  however,  that  the  strictly 
mechanical  properties  described  by  both  second-rank  tensors  and  wave  mechanics  then 
allowed matter to look  more like  a  field  which  led  to this  choice  of wording.    It was  a 
simple  way  of  describing  the  behavior  but  he  felt  it  was  important  to  realize  that  the 
aether was not matter. 
 

3.2 Observables and Coordinate Relations 

 
One of the early drafts has a very nice description of observables and the product of two 
functions.  This allows for the drawing of a fairly neat line between his earliest ideas and 

 

18 

*ϕ ϕ  and 

*ψ ψ,  will  be  observationally  equivalent  to 

the  final  version.    In  this  draft  he  notes  that  wave  mechanics  utilizes  a  product  of  two 
functions to describe observables while in relativity  an observable is a relation between 
two  (or  more)  bodies  (an  observation  point  and  the  object  under  observation,  or  one 
object that is reference point for measurement between two others). 
In  quantum  mechanical  terms,  the  self-properties  of  the  observables  (together 
 
*ϕψ.    If  a 
with  their  conjugates), 
wave  function,  then,  represents  a  definite  momentum  and  energy  then  the  position  is 
entirely uncertain.  In relativity, then, these self-properties are represented by the stress-
energy  tensor,  Tµν.    The  reason  that  Eddington  gives  for  the  idea  of  uncertainty  of  the 
position  of  a  particle  (i.e.  his  explanation  for  Heisenberg’s  Uncertainty  Principle)  is 
contained  in  this  development.    These  self-properties  represent  geometrical  coordinate 
positions and can thus measure certain items such as energy and momenta, but position is 
representative  of  an  object’s  physical  coordinate  system  and  thus  has  a  certain  level  of 
uncertainty.    It  should  be  noted  that  Eddington,  in  the  final  version,  mentions  that  the 
outer-product  of  two  vectors  stabilizes  the  system  thereby  reducing  the  number  of 
degrees of freedom.  He does not seem to mention this point in this draft, however. 
 
The exact locations of objects are thus unobservable.  But the stress-energy tensor 
contains the metric gµν which he called the fundamental tensor.  This tensor describes the 
physical system (and thus the physical coordinates) of a reference body.  The geometrical 
reference frame is intermediary and gµν is used in relation to a positional characteristic.  
He related gµν to Tµν by the following relation: 
 

λ
g
µν
πκ
8

=

(
T

µν

)

c

 

πκ
8

(
T

µν

)

c

λ=
g

. 

µν

 
where the right-hand side of the equation is called the ‘cosmical energy tensor’ while the 
left-hand side is the energy tensor of a reference body.  This can also be expressed as: 
 

 
The idea is that gµν and Tµν have a similar representation.  Therefore the energy tensor of 
the object body is given as: 
 

πκ
8

(
T

µν

)

o





= −

G

µν

−

(
g G
µν

−

2


)
λ



. 

1
2

 
The total energy of combination, which is equivalent to a mutual energy tensor, is then 
given by the usual formulation (identical to Equation 1.9): 
 

πκ−
T
8

µν

=

G

µν

−

g G
µν

. 

1
2

 
Eddington  thus  introduces  a  factor  of  λ  to  relate  the  object  and  reference  bodies.    This 
was Eddington’s way of showing that relativity could be described as a relation between 

 

19 

object and reference bodies just as quantum mechanics could be described as a relation 
between two observables (embodied in the wave relations given above).  He does briefly 
mention, however, that Equation 1.9 is a description over partial space and that one must 
make  a  multiplication  as  if  it  was  a  probability  distribution,  similar  to  the  wave-
mechanical  description,  in  order  to  produce  a  ‘whole’  energy  tensor.    He  is  not  clear, 
however, on exactly what this means or how it is to be done. 
 
This description was reduced in another draft to simply saying that tensors can be 
divided into two pieces like vectors (magnitude and orientation) where T=λT0 where λ is 
invariant  (like  a  magnitude)  and  is  called  the  extensor.    T0  is  a  unit  tensor  (like  an 
orientation) and is called the rotor (Eddington seems to have been fond of creating new 
words –  e.g. extensor,  rotor, uranoid).   The standard  uranoid arises from  stabilizing the 
rotor.  In field theory the environment is specified by gµν or: 
 

(
µνλ=
g

)

g

µν

. 

0

 
This gives us a new description of proper distance: 
 

2

ds

=

g dx dx
µ ν

µν

(
λ

=

g

µν

dx dx
µ ν

. 

)

0

0

gµν

 
In this formulation the rotor (given as  (
)
) is given stabilized Galilean values of δµν.  
The extensor, or λ, then determines the extension of the coordinate mesh in terms of an 
extension  of  the  standard  of  length.    The  extensor  is  then  an  observable  with  an 
uncertainty built in to it.  However, previously he makes the point that it is invariant like 
a magnitude.  I believe his choice of the word ‘invariant’ is misleading.  His point is that 
the  extensor’s  value  would  contain  the  uncertainty  in  it  but,  as  part  of  the  tensor,  it 
behaves like a magnitude. 
 
In  yet another draft, he better explains the extraneous standard introduced in the 
final draft as an intermediary in a comparison of scale between an object-system and the 
uranoid which acts as another object-system.  This should not be confused with the actual 
vector displacement he first mentions in the letter to Larmor which describes the relative 
location of the uranoid.   Related  to  this  concept  is the idea that the  curvature of  space, 
then,  ‘arises  out  of  the  statistical  fluctuations  of  a  distribution  of  a  large  number  of 
particles.’    In  other  words,  gravity  ends  up  being  a  consequence  of  the  exclusion 
principle!    This  latter  fact  is  explained  in  greater  detail  in  the  latter  portions  of 
Eddington’s tome, but, suffice it to say, it is a very interesting conclusion. 
 
Eddington  has  thus  drawn  a  ‘chain  of  connections’  between  relativity  and 
quantum  mechanics  which  he  referred  to  (perhaps  immodestly)  as  the  ‘trunk  road  of 
relativistic  quantum  mechanics.’    The  importance  of  this  description,  which  Eddington 
largely leaves out of the final version of Fundamental Theory, is the attachment of other 
physical representations to the physical and geometrical coordinates that play such a vital 
role  in  his  development  of  uncertainty  in  the  reference  frame.    Thus  the  relation  of 
physical and geometrical coordinates goes beyond the purely statistical description given 
in the first  chapter  of the final version  and encompasses  actual physical  representations 
within both quantum mechanics and relativity independently. 

 

20 

 

 

3.3 Against the Grain 

 
In another early draft, when dealing with time coordinates, he finds an interesting oddity.  
It appears, from his particular relation, that limiting wave mechanics to three dimensions 
introduces a factor of 4/5 into other relationships.  Eddington attributes this anomaly to 
employing stabilized instead of measured masses which ultimately simplifies the solution 
(see  Section  2.15).    This  is  something  he  employs  more  than  once  and  is  essentially  a 
Kaluza-Klein  solution  in  reverse  –  instead  of  extending  the  number  of  dimensions, 
Eddington  reduced  them  (or  constrained  coordinates  to  act  within  a  certain  number  of 
them) in order to simplify the solution. 
Eddington moved against the grain in a similar way when he denounced the idea 
 
that  a  singularity  could  also  be  a  meeting  point  of  relativity  and  quantum  mechanics.  
Apparently there were scientists who suggested that the idea of a singularity could be the 
only  meeting  point  between  the  two  by  saying  that  a  singularity  could  be  treated  as  a 
pseudo-particle (note that it was in the 1930s that Dirac developed his famous magnetic 
monopole problem which was suggested from time-to-time as having a role in quantum 
gravity).    In  quantum  mechanics  a  singularity  is  created  by  letting  h  approach  0.  
Eddington claimed that this could not work because hc/2πe2 was exactly equal to 137 and 
was  invariant.    Thus  Eddington  insisted  that  the  proper  distance  when  measured  by  a 
quantum-specified standard in an empty space-time (Minkowski space) was the only link 
between quantum mechanics and gravity. 

This last bit brings up a disturbing point.  Eddington really insists in many places 
that it is Minkowski space that is the only true meeting point of the two theories and yet 
he introduces curvature time and time again.  It is possible that he viewed the universe on 
a  large  scale  as  being  approximated  as  Minkowski-like  with  curvature  being  a  local 
phenomenon, but he is never explicit about this fact. 

Returning for a moment to a discussion of h, an interesting development that he 
highlights in New Pathways in Science, is the origin of  ! .  First of all, he notes that the 
multiplication  of  uncertainties  is  on  the  order  of  one  quantum.    This  is  due  to  the 
Uncertainty  Principle.    In  an  angular  situation  the  greatest  possible  uncertainty  is  2π.  
Hence  h= ! /2π.    This is, of  course,  used  to  explain  the  discrete  orbitals  of  electrons  in 
atoms.    So  uncertainty  can  be  seen  as  an  inherent  natural  phenomenon  and  it  is  not 
unusual that Eddington extrapolated this to other situations. 

Additionally, in New Pathways in Science, he explains the difficulties introduced 
by  indeterminacy  on  the  microscopic  scale.    He  also  discusses  here  the  randomness  of 
radiation.    In  a  macroscopic  example  of  the  difference  between  his  physical  and 
geometrical coordinates, he points out that the frequency of eclipses (e.g. lunar and solar) 
is a purely geometrical phenomenon.  Additionally, he attributes the minimum energy of 
an atomic electron to geometry – excess energy is simply an excitation (which is certainly 
true).    He  then  went  on  to  say,  however,  that  the  fixing  of  an  electron’s  exact  position 
(orbital) is required to be known in order to know the constitution of the atom (i.e. what 
element  it  is).    He  has  thus  attempted  to  ground  his  concepts  of  uncertainty  and 
indeterminacy with physical examples. 
 

 

4.  Eddington’s Philosophy 

21 

 

4.1 Explicit Philosophy 

 
Uncertainty was definitely the cornerstone of Eddington’s attempt to unify relativity and 
quantum theory.  The mathematical underpinnings of uncertainty are rooted in probability 
theory  and  so  it  is  natural  for  Eddington  to  have  spent  much  time  grappling  with  the 
nuances  of  probability.    In  public  he  regularly  lectured  on  the  subject  and  included 
several topics in his 1934 Messenger Lectures, later published in expanded form as New 
Pathways  in  Science.    In  it  he  devotes  entire  chapters  to  the  decline  of  determinism, 
indeterminacy and quantum theory, and probability. 
 
In his discussion he explicitly references Laplace numerous times.  Laplace was a 
deductivist and his work in mathematics, including probability theory, was derived from 
logical reasoning.  Eddington felt that a truly perfect unified theory of the universe should 
be able to be derived solely from logical reasoning, so it is safe to say that Eddington also 
was a deductivist.  It is therefore no surprise that he often quotes Laplace in his work. 
 
Laplace was at the center of a controversy in the 1820s when there was an attempt 
made  to  introduce  the  teaching  of  probability  theory  at  Cambridge  and  Oxford 
Universities.    At  the  time  both  universities  were  still  sectarian  (versus  the  secular 
University  of  London)  and  several  religiously  fervent  individuals  including  William 
Whewell argued against introducing probability theory to the curriculum and against the 
Continental  deductivists  such  as  Laplace,  D’Alambert,  Clairault,  Lagrange,  and  Euler 
(note that Euler is the only non-Frenchman) on the grounds that probability theory sought 
to  answer  questions  better  left  to  the  Divine.    Eddington,  in  New  Pathways  in  Science, 
explicitly relates the story of the Marquis de Condorcet, another French probabalist, who 
attempted  to  work  out  the  problem  of  probability  theory  in  relation  to  the  fairness  of 
judges (Eddington also  points  out that  Condorcet  took  his  own life  rather  than trust his 
own calculated odds with the tribunal during the Revolution).  As can be seen from these 
and many other examples, the issue of religion (and, to some extent, class) was no small 
matter in Victorian England where it sometimes dominated society. 
Whewell was in favor of inductive science which was experimental in nature and 
 
thus  (supposedly)  more  supportive  of  religious  ideas.    To  Whewell,  deductive  science 
was too mechanistic.  J. Richards attests that ‘a religion that rested on evidence attested to 
by  personal  experience  and  conviction  had  no  standing  in  probabilistic  discourse.’ 
[Lightman,  1997,  59]    The  irony  was  that  Eddington  really  began  his  career  as  an 
observational  astronomer.    His  early  work  included  extensive  mathematical  analyses  of 
the envelopes of Comet Morehouse based on observations, as well as his now landmark 
observation  (and,  yes,  it  really  was  only  a  single  viable  observation)  that  proved 
Einstein’s theory that gravity would bend light.  In another ironic twist, in 1909, while he 
was  working  at  the  Royal  Observatory  in  Greenwich,  he  turned  down  the  offer  of  a 
position at the University of Manchester, as proposed by Dr. Arthur Schuster, because it 
was  in  physics  and  he,  according  to  his  letter  to  Schuster,  preferred  the  observational 
work at the observatory. 
 
However,  it  is  very  possible  that  this  heavy  observational  work  would  have  led 
Eddington down the deductivist path.  Observational astronomy is, arguably, currently at 
its  peak  in  the  early  twenty-first  century  in  terms  of  instrumentation  and  measurement 
and  yet  much  remains  unanswered  and  (quite  literally)  ‘fuzzy.’    The  exactitude  of  the 

 

22 

science was  very  near horrific by today’s standards a century ago when  Eddington was 
working.    Eddington  was  a  consummate  mathematician  and  in  attempting  to  formulate 
exact  solutions  to  match  observations,  such  as  the  work  he  performed  with  Comet 
Morehouse  in  1909  where  he  attempted  to  fit  three-dimensional  paraboloids  to  the 
envelopes  of  the  comet  from  fuzzy,  two-dimensional  photographic  plates,  he  found 
considerable  room for error.    In  his famous eclipse observations of 1919 that  first gave 
observational  evidence  to  Einstein’s  General  Theory  of  Relativity,  out  of  numerous 
observations,  it  turned  out  only  one  was  good  enough  to  support  the  desired  outcome 
(true,  weather  conditions  played  a  role). 
  But,  psychologically,  the  consistent 
inadequacies  of  observational  techniques,  particularly  in  comparison  to  rigorous 
mathematics,  must  have  driven  Eddington  to  assume  some  level  of  uncertainty  was 
inherent  in  any  observation.    The  introduction  of  Heiseinberg’s  Uncertainty  Principle 
must have been exactly what Eddington was seeking in terms of a philosophy to support 
his  observations.    He  simply  extended  it  to  macroscopic  phenomena  by  introducing  an 
uncertainty in the reference frame of any observation.  This also fits in with his obvious 
mental  grappling  with  the  problem  of  the  definition  of  measurement  standards  like  the 
metre (see Section 2.4).  Eddington thus turned to a purely deductivist solution in order to 
determine  everything  from  logical  reasoning.    Ironically,  Whewell’s  inductivism 
incorporated what were known as ‘fundamental ideas’ (could Eddington have gotten his 
title from here?) which, in essence, supported observation but were derived by ‘thinking 
properly.’  
 

4.2 Implicit Philosophy 

 
The problem highlights the difference between inductivism and deductivism.  In seeking 
a general definition of inductive and deductive reasoning, a very straightforward example 
is given in Warriner’s English Grammar and Composition [Warriner, 1986].  Inductive 
reasoning  begins  with  a  set  of  observations  and  proceeds  to  draw  a  generalization  that 
describes that set of observations.  Deductive reasoning begins with a generalization and 
attempts  to  draw  conclusions  based  on  that  generalization.    In  a  nutshell,  inductive 
reasoning moves from the specific to the general while deductive reasoning moves from 
the  general  to  the  specific.    Eddington’s  early  career  was  marked  by  the  inductivism 
inherent  in  observational  astronomy.    Perhaps  motivated  by  the  inadequacies  of  the 
science  (note  to  observational  astronomers:  all  science  is  inadequate  so  do  not  feel 
particularly  slighted)  he  moved  to  the  more  deductivist  science  of  cosmology.    But 
deductivism has a definite problem in relation to science.  Bertrand Russell wrote in his 
Introduction to Mathematical Philosophy 
 

Since all terms that are defined are defined by means of other terms, it 
is clear that human knowledge must always be content to accept some 
terms as intelligible without definition, in order to have a starting-point 
for  definition.    It  is  not  clear  that  there  must  be  terms  which  are 
incapable of definition:  it is possible that, however far back we go in 
defining, we always might go further still. [Russell, 1919, 3-4] 
 

This sums up quite succinctly the overall philosophy of Eddington in his grappling with 
uncertainty.    The  uncertainty  arose  from  the  fact  that  there  was  never,  in  his  mind,  a 
suitable definition of a starting point in measurement.  Eddington’s deductivist approach 

 

23 

can  also  be  paralleled  in  another  passage  by  Russell  where  he  discusses  the  fact  that 
Frege  ‘first  succeeded  in  “logicising”  mathematics,  i.e.  in  reducing  to  logic  the 
arithmetical notions which his predecessors had shown to be sufficient for mathematics.’ 
[Russell,  1919,  7]    Eddington  was  attempting  to  “logicise”  physics  by  reducing  it  to  a 
logical set of arithmetical notions which had been shown to be sufficient for physics (or 
mathematics in certain cases where it can then be applied to physics).  The problem with 
this combination should be obvious – how can one properly deduce a conclusion without 
a  definite  starting-point?    Eddington’s  major  philosophical  flaw,  then,  was  in  the 
assumption  that  any  proper  theory  of  quantum  gravity  could  be  deduced  from  logical 
reasoning alone for, if uncertainty truly permeated all reasoning as he implied, there was 
no starting point from which to begin the process of logical reasoning.  Eddington does 
appear to recognize small limitations in this by introducing stabilization by which there 
are certain quantitative values (like the mass and charge of the electron) that are assumed 
to be taken for granted,  but the argument is still circular because the measured mass of 
the  electron  is  often  found  via  experiments  that  involve  the  Uncertainty  Principle  (via 
momenta in collisions – see Section 2.15). 
 
says that  
 

In  dealing  with  this  problem  of  knowledge  in  his  New  Pathways  in  Science  he 

we have been concerned to show that probability is always relative to 
knowledge (actual or presumed) and that there is no a priori probability 
of things in a metaphysical sense, i.e. a probability relative to complete 
ignorance. [Eddington, 1934, 133] 
 

Philosophically,  this  is,  once  again,  a  circular  argument!    For,  if  probability  is  always 
relative to actual or presumed knowledge, then what is the first bit of knowledge and how 
is  it  gleaned?    Isn’t  there  some  probability  that  can  be  assigned  to  this  first  bit  of 
knowledge itself?  Eddington has once again locked himself in by saying that there must 
be a first principle or quantitative value that is the catalyst but then saying that everything 
much  be  deduced  within  the  bounds  of  uncertainty  and  applies  an  uncertainty  to  these 
first  principles  (or  simply  assumes  them  through  stabilization  which  violates  his  broad 
philosophical  rules  regarding  uncertainty). 
  The  more  general  problem  here, 
philosophically speaking, is that mathematics is a deductive science which doesn’t leave 
Eddington much room to play with.  Perhaps if he took a more physical interpretation he 
might  have  been  able  to  satisfactorily  resolve  the  contradiction  via  some  physical 
explanation.  But, certainly, it is a problem still labored over in physics – e.g. what came 
before the Big Bang? 
 

5.  Eddington Unwrapped 
 

Several things are evident from the analysis of Eddington’s work.  The most obvious is 
that  he  was  a  consummate  mathematician  who  was  meticulous  in  everything  he  did.  
Even his handwriting is extremely compact and carefully drafted – in strong contrast to 
the usual flowing script prevalent at the time.  Another item that is evident, particularly 
from reading his letters to Larmor as well as in some portions of his papers and books, is 
that he was  quite  passionate  about his  point  of  view and defended  it  very  aggressively.  
He often appears to be denigrating the opposition.   In fact, this appears to be his initial 

 

24 

approach only and his heart appears to always have been in the right place, so to speak.  
Despite  the  very  rocky  start  to  his  relationship  with  Larmor,  it  ended  in  a  definite 
friendship  –  Eddington  even  helped  to  organize  Larmor’s  retirement  gala.    Eddington 
also  often  received  a  great  deal  of  criticism  from  E.A.  Milne  (Eddington,  in  turn,  dealt 
quite a bit of criticism right back) and it is often thought that the two greatly disliked each 
other. That, in fact, is far from the truth.  One letter sent by Eddington to Milne in regards 
to  a  request  for  the  review  of  a  book  Milne  had  written,  criticizes  the  book  but 
specifically says the following 
 

I realise that the review can scarcely be pleasing to you; but I hope you 
will  recognise  that  it  might  have  been  worse  if  (holding  the  opinion  I 
do)  I  had  let  myself  go  without  regard  to  our  friendship.  [Eddington, 
1935] 

 
He goes on to justify, almost guiltily, the review in light of the length restriction put on 
him and he ends the letter by chatting wistfully about an eight-day bicycle tour he intends 
to begin the next day in which he plans to let ‘the worries of the universe fade into their 
proper significance.’ 
 
This last bit does indicate that Eddington was probably well-grounded despite his 
seemingly  passionate  nature.    Evidence  for  his  well-groundedness  does  appear  in  his 
scientific work as well – specifically he makes certain to find physical justifications for 
his mathematical work, despite its deductive nature.  This does not mean he was always 
correct, simply that he attempted to ground himself in physical reality as best he could. 
 
The  other  thing  that  is  evident  from  an  analysis  of  Eddington’s  work  is  that, 
despite  the  uniqueness  of  his  methods  and  conclusions,  his  general  philosophy  was  far 
from  unusual  for  the  time  (see  de  Broglie’s  quote  in  Section  3.1).    Eddington  is  often 
viewed as having been somewhat of a recluse and even renegade in his later years and his 
relationship with the physics community at large perhaps goes to show just how narrow 
the  definition  is  for  acceptable  physical  theory.    But  he  really  wasn’t  that  different  in 
terms  of  his  general  philosophy  and  that  fact  is  often  overlooked  by  historians.    These 
days it is often popular to refer to Eddington’s work as heterodoxical (see Kragh, 1999) 
both  philosophically  and  scientifically.    In  fact,  even  his  mathematical  work  was  fairly 
standard.    He  was  always  extremely  careful  to  ensure  that  his  mathematics  worked  out 
properly.  As such it really was only his physical interpretation that suffered the stigma of 
both heterodoxy and incorrectness.  But his was hardly the only theory that suffered this 
fate  and  to  treat  Eddington  as  a  recluse  and  renegade  might  be  too  harsh  a  historical 
treatment.    For  instance,  it  is  well  understood  in  particle  physics  today  that  the  hole 
interpretation of negative energy states by Dirac is far too simplistic and, in fact, much of 
his original interpretation is incorrect.  But not  only is this not deemed problematic but 
the hole interpretation still appears in textbooks as a pedagogical tool!  (Please note that 
the author does not advocate the teaching of Fundamental Theory outside of its historical 
context). 
 
What we can say for certain about Eddington’s work was that it did successfully 
predict a number of methods and results that are in use today.  He correctly predicted the 
need  for  a  quantum  mechanical  standard  for  the  measurement  of  length  and,  in  his 
development of the concept of the uranoid, he employed an early version of a philosophy 
that appeared later in some versions of quantum field theory – namely the inseparability 

 

25 

of  an  object  and  its  environment.    His  idea  that  there  if  a  fundamental  link  between 
quantum  mechanics  and  relativity  based  on  the  concept  of  coordinates  isn’t  that  far  off 
either.    Both  theories  rely  heavily  on  coordinates  and  both  theories  delve  into  concepts 
relating  to  topology.    Thus  the  initial  seed  thought  that  brought  about  his  grand  theory 
was not as off-base as many historians might suggest.  In either case, the analysis of his 
work with uncertainty in the reference frame gives us a glimpse into the mind and heart 
of one of the greatest mathematical astronomers of the twentieth century. 
 

6.  Acknowledgements 
 

I  would  like  to  gratefully  acknowledge  the  support  and  encouragement  of  Edmund 
Robertson, John O’Connor and the School of Mathematics and Statistics at the University 
of  St.  Andrews.    I  would  also  like  to  thank  Velda  Goldberg  and  the  Department  of 
Physics  at  Simmons  College  for  their  support  in  this  endeavour.    I  would  like  to 
additionally  acknowledge  the  helpful  staff  at  the  Royal  Society’s  Library  and  Sackler 
Archive  who  were  quite  helpful  in  providing  me  with  the  opportunity  to  peruse  many 
letters in their collection.  I am also greatly indebted to the Simmons College library and 
their Inter-Library Loan staff who were quite helpful in obtaining hard-to-find books for 
me.  In addition, their subscription to JSTOR, which provided me access to hundreds of 
original archived articles in numerous journals, was quite helpful.  Additionally, I would 
like to thank Len Soltzberg of Simmons College for several useful conversations on the 
history  of  quantum  mechanics,  and  Alex  Craik  of  the  University  of  St.  Andrews  for  a 
nice discussion regarding the general topic of this paper. 

I would also like to finally acknowledge Mrs. Meg Weston Smith, the daughter of 
E.A. Milne, whose generosity and openness throughout this process has made this project 
all  the  more  enjoyable.    The  terrific  collection  of  materials  that  she  maintains  that 
belonged to her father as well as  her first-hand  knowledge were  a tremendous  resource 
due  to  Milne’s  friendship  with  Eddington.    It  has  also  given  me  quite  a  wealth  of 
information for a future paper on Milne himself.  Additionally, her warm hospitality has 
made studying this material a pleasure.  Without her assistance this paper would be sorely 
lacking. 
 

7.  References 
 
de Broglie, L. (1937) Matter and Light, Norton 
Dirac, P.A.M. (1928) Proc. Roy. Soc. Lon., A117, 610 
Dirac, P.A.M. (1928) Proc. Roy. Soc. Lon., A118, 351 
Eddington, A.S. (1909) letter to A. Schuster 
Eddington, A.S. (1910) Mon. Not. Roy. Ast. Soc., 70, 442 
Eddington, A.S. (1915) letter to J. Larmor 
Eddington, A.S. (1916) letter to J. Larmor 
Eddington, A.S. (1927) letter to J. Larmor 
Eddington, A.S. (1928) Proc. Roy. Soc. Lon., A121, 524 
Eddington, A.S. (1929) Proc. Roy. Soc. Lon., A122, 358 
Eddington, A.S. (1930) Proc. Roy. Soc. Lon., A126, 696 
Eddington, A.S. (1931) Proc. Roy. Soc. Lon., A133, 605 

 

26 

Eddington, A.S. (1932) letter to J. Larmor 
Eddington, A.S. (1934) New Pathways in Science, Macmillan 
Eddington, A.S. (1935) letter to E.A. Milne 
Eddington, A.S. (1937) Am. J. Math., 59, 1 
Eddington, A.S. (1946) Fundamental Theory, Cambridge 
Kragh, H. (1999) Quantum Generations, Princeton 
Liboff, R. (1998) Introductory Quantum Mechanics, Addison-Wesley 
Lightman, B. (Ed.) (1997) Victorian Science in Context, Chicago 
Pauling, L., Wilson, E.B. (1935) Introduction to Quantum Mechanics, McGraw-Hill 
Russell, B. (1919) Introduction to Mathematical Philosophy, George Allen and Unwin 
Schutz, B. (1990) A First Course in General Relativity, Cambridge 
Slater, N.B. (1957) The Development and Meaning of Eddington’s ‘Fundamental Theory’  

Including a Compilation from Eddington’s Unpublished Manuscripts, Cambridge 

Warriner, J.E. (1986) Warriner’s English Grammar and Composition, Harcourt Brace  

Whittaker, E.T. (1951) A History of the Theories of Aether & Electricity, Volume I: The  

Whittaker, E.T. (1953) A History of the Theories of Aether & Electricity, Volume II: The  

Jovanovich 

Classical Theories, Harper 

Modern Theories, Harper 
Wilczek, F. (1999) Physics Today, 52, 11 

 

27 


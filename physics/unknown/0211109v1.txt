Statistical Inverse Problem

Yu. I. Bogdanov

OAO "Angstrem", Moscow, Russia
E-mail: bogdanov@angstrem.ru

November 27, 2002

"Get at the root of it!"
Koz’ma Prutkov

Abstract
A  fundamental  problem  of  statistical  data  analysis,  density  estimation  by  experimental  data,  is
considered.  A  new  method  with  optimal  asymptotic  behavior,  the  root  density  estimator,  is
proposed to solve the problem. The method is based on the representation of the probability density
as a squared absolute value of a certain function, which is referred to as a psi function in analogy
with  quantum  mechanics.  The  psi  function  is  represented  by  an  expansion  in  terms  of  an
orthonormal set of functions. The expansion coefficients are estimated by the maximum likelihood
method.  An  iteration  algorithm  for  solving  the  likelihood  equation  is  presented.  The  stability  and
rate  of  convergence  of  the  solution  are  studied.  A  special  iteration  parameter  is  introduced:  its
optimal  value  is  chosen  on  the  basis  of  the  maximin  strategy.  Numerical  simulation  is  performed
using the set of the Chebyshev—Hermite functions as a basis. It is shown that the introduction of
the  psi  function  allows  one  to  represent  the  Fisher  information  matrix  as  well  as  statistical
properties  of  the  estimator  of  the  state  vector  (state  estimator)  in  simple  analytical  forms.  A  new
statistical characteristic, a confidence cone, is introduced instead of a standard confidence interval.
The chi-square test is considered to test the hypotheses that the estimated vector converges to the
state  vector  of  a  general  population  and  that  both  samples  are  homogeneous.  The  problem  of
choosing an optimal number of harmonics in the expansion is discussed. The method proposed may
be applied to its full extent to solve the statistical inverse problem of quantum mechanics, namely,
estimating the psi function on the basis of the results of mutually complementing experiments. The
maximum likelihood technique and likelihood equation are generalized in order to analyze quantum
mechanical experiments. The Fisher information matrix and covariance matrix are considered for a
quantum statistical ensemble. The constraints on the energy are shown to result in high-frequency
noise reduction in the reconstructed state vector.

Introduction

A  key  problem  of  statistical  data  analysis  is  the  problem  of  estimating  the  probability
distribution  density.  Almost  all  problems  related  to  experimental  data  processing  are  reduced  to
either estimating the probability density (when experimental data have to be described in terms of
statistical  distributions)  or  determining  the  goodness  of  fit  between  data  observed  experimentally
and theoretical density model (if exists).

Such a statement of the fundamental problem is recognized only de jure in the literature on
mathematical statistics (and even not always). De facto, classical objects of mathematical statistics
  with  either  one  or  two  unknown
are  smooth  parametrized  families  of  densities 

(
,
1 θθxp

2

)
,...

1 θθ
,

,...

2

parameters 
 to be estimated using the observed data. The functional form of the density is
assumed  to  be  initially  prescribed.  Such  a  parametric  analysis  is  well  developed  to  estimate  the
parameters  of  a  rather  small  number  of  distributions  (including  Gaussian,  exponential,  binomial,
Poisson  and  several  other  specific  distributions).  The  maximum  likelihood  method  is  regarded  as
the most perfect one to estimate the parameters. This method yields estimators that are close, in a
certain sense, to the best possible estimators (see below).

The basic limitation of the traditional parametric approach is that it is impossible to describe
distributions  of  an  arbitrary  form.  This  drawback  has  objective  underlying  causes.  Indeed,  the
problem of statistical density estimator is an inverse problem of probability theory (while the direct
problems  are  calculating  various  frequency  quantities  on  the  basis  of  a  given  model  of  a  random
event). The problem under consideration turns out to be ill-posed. This implies that in the absence

( )xp

 of a random variable, the problem of

of any a priori information about the distribution law 
density estimation does not admit a solution.

The  ill-posedness  is  a  common  feature  of  inverse  problems.  In  the  absence  of  additional
information based on either objective knowledge or, at least, common sense (i.e., when there is no
any  a  priori  information),  a  researcher  can  try  to  seek  a  correct  dependence  in  a  wide  class  of
functions. In this case, empirical data are sometimes insufficient to reliably estimate the statistical
distribution, since there are a lot of functions that essentially differ from each other and, at the same
time, correctly describe statistical data. Additional a priori considerations resulting in narrowing the
class  of  functions  are  related  to  ranging  solutions  in  their  complexity.  For  example,  one  may
consider  lower  harmonics  as  simpler  compared  to  higher  in  standard  sets  of  basis  functions,  or
introduce so-called smoothing functionals etc.

A general approach to ill-posed problems was developed by Tikhonov [1]. An interpretation
of  an  inverse  problem  of  probability  theory  as  ill-posed  was  given  in  the  Prokhorov—Chentsov
theory of probability measures (see Appendix 2 in [2]). Regularization of the problem of probability
density  estimation  by  smoothing  an  empirical  distribution  function  is  presented  by  Vapnik  and
Stefanyuk  [3,  4].  Sometimes,  it  is  convenient  to  smooth  quantities  found  from  an  empirical
distribution function by monotonous transformations rather than an empirical distribution itself. For
instance,  in  the  reliability  assurance  problems,  it  is  convenient  to  perform  smoothing  in  the  so-
called Weibull coordinates [5, 6].

In mathematical statistics, two basic kinds of estimators are usually considered: the kernel

density estimator and orthogonal series estimator.

Kernel density estimators [7—10] (also called the Rosenblatt—Parzen estimators) are based
on smoothing each point in a sample over its certain neighborhood. In this case, the density has the
form: 

( )
ˆ
xp

=

1
nh
n

n

∑

k

1
=

K

x
k





 , (I.1)

 −
x

h

n
  is  the  sample  of  the  size  n ; 

x ,...,
1

nx

where 
sequence of the parameters describing the bandwidth.

( )xK

∞→n

If 

 and 

0→nh

,  

1
nnh

→

0

,  the  distribution  density;  and 

nh ,  the

 , under certain sufficiently general conditions,

the  kernel  density  estimator  approaches  the  true  density  characterizing  the  general  population.
Corresponding density estimator is asymptotically unbiased, consistent, and asymptotically normal.
A  number  of  papers  were  devoted  to  the  optimal  selection  of  the  bandwidth  that  is  of  a  primary
importance for kernel density estimation (see, e.g., [11—14]).

Now,  the  development  of  the  theory  of  kernel  density  estimators  is  concerned  with
performing estimations  in  spaces  of  arbitrary nature  allowing  one  to  consider  nonnumeric  objects
[15, 16].

The  orthogonal  series  estimator  [17—20]  proposed  by    Chentsov    (1962)  is  based  on  the
expansion of an unknown distribution density into the Fourier series, and subsequent estimation of
the  expansion  coefficients  by  a  sample.  The  density  estimator  by  first  m   terms  of  the  Fourier
series is

2

( )
ˆ
xp

=

c

ϕ

j

j

( )
x

, (I.2)

m

∑

j

1
=

n

∑

1
n

( ) ( )
ˆ
dxxpx

∗
j

j

c

≈

=

ϕ

∫
( )
xjϕ
,
    
Both kernel and orthogonal series estimators can be regarded as delta sequence estimators

 is the orthonormal basis.

1,2,...

 (I.3)

ϕ

=

x

=
1

j

k

k

(

∗
j

)

where 

and 

[21].

The sequence of functions 

 of two arguments is referred to as a delta sequence if
the  integrals  of  these  functions  multiplied  by  arbitrary  sufficiently  good  (for  example,  finite  and

δ

)yxm
(
,

A delta-like character of kernel density estimators (at any finite  n  and 

0→h

) is evident

infinitely differentiable) function 
(
dyyfyxm

) ( )

=

∫→∞
lim δ
m

,

( )xf

( )xf

 (I.4)

 satisfy the condition

directly.

For orthogonal series estimators, from (I.1)—(I.3) we find
1
1
n
n

,
xx
k

, (I.5)

( )
x

∑

)
ϕ

δ

)

(

x

n

k

k

1
=



=


k

n

m

1
=

∗
j

(

≈

ϕ

( )
ˆ
xp


∑ ∑



where the delta-like kernel is
m
( )
∑
x

,
xx
k

)
ϕ

x
k

ϕ

=

δ

(

(

)

∗
j

1
=

j

j

j

j

1
=

, (I.6)

∞

functions:
∑

x
k

ϕ

(

∗
j

j

1
=

)
ϕ

( )
x

j

=

δ

(

x

−

)k
x

 (I.7)

The  delta-like  character  of  the  kernel  follows  from  the  completeness  of  the  set  of  basis

The delta-like character of the estimators makes it possible to illustrate the ill-posedness of
the  inverse  problem  of  probability  theory.  For  arbitrary  given  set  of  experimental  data,  having
passed  through  a  certain  optimum  value,  the  density  estimator  begins  gradually  falling  apart  with
increasing  number  of  terms  m   in  the  case  of  an  orthogonal  series  estimator  (or  with  decreasing
bandwidth  h  in the case of a kernel estimator) turning finally into a set of sharp peaks related to
sample points.

Note  that  a  delta  sequence  (I.6)  is  not  a  priori  nonnegative.  This  may  result  in  appearing
meaningless  negative  values  of  probability  density.  Kernel  density  estimators  have  no  this
drawback,  since  the  kernel  is  chosen  by  a  researcher  and,  hence,  can  always  be  selected
nonnegative.  However,  in  the  theory  of  kernel  density  estimators,  kernels  that  are  not  positively
defined are sometimes used in order to decrease the bias of density estimator [15].

A certain advantage of orthogonal density estimators is that they can yield analytical density
approximation. Anyway, some data reduction takes place if the consideration is restricted to a few
terms in the Fourier series. At the same time, kernel density estimators, generally speaking, do not
provide data reduction, since they require permanently storing of all the initial data.

An orthogonal density estimator may be referred to as the Gram-Charlier estimator [22, 23].
In the Gram-Charlier estimators, it is assumed that the density is determined by a reference density
( )xp0

 (as a rule, the Gaussian distribution) in zero-order approximation. If this density is given, it

3

is  better  to  use  the  functions  that  are  orthonormal  with  respect  to  the  weight  function 
instead of the ordinary orthonormal set:
( )
( )
dxxpx
0

( )
x
ϕ

. (I.8)

=

δ

ij

j

∫ ∗
ϕ

i

The Chentsov estimator corresponds to the case 

( ) 1
=xp
Note  that  a  set  of  functions  that  are  orthonormal  with  respect  to  a  weight  function  can  be
derived  from  any  given  system  of  linearly  independent  functions  by  a  standard  orthogonalization
procedure.

0

.

An  unknown  density  estimator  in  the  Gram-Charlier  method  is  sought  in  the  form  of  the

( )xp0

expansion
( )
ˆ
xp

=

( )
xp
0

c

ϕ
j

j

( )
x

, (I.9)

m

∑

j

1
=

where the expansion coefficients are estimated by
1
n

( ) ( )
ˆ
dxxpx

. (I.10)

∑

x
k

∫

ϕ

ϕ

≈

=

c

(

)

∗
j

∗
j

n

j

k

1
=

This  paper  is  based  on  a  symbiosis  of  mathematical  tools  of  quantum  mechanics  and  the
Fisher  maximum  likelihood  principle  in  order  to  find  nonparametric  (or,  more  precisely,
multiparametric)  effective  density  estimators  with  most  simple  (and  fundamental)  statistical
properties.

The  method  proposed,  the  root  density  estimator,  is  based  on  the  representation  of  the
density in the form of a squared absolute value of a certain function, which is referred to as a psi
function in analogy with quantum mechanics.

The introduction of the psi function results in substantial reduction in the structure of both
the Fisher information matrix and covariance matrix of estimators, making them independent of a
basis,  allows  one  to  provide  positive  definiteness  of  the  density  and  represent  results  in  a  simple
analytical form.

The  root  density  estimator  being  based  on  the  maximum  likelihood  method  has  optimal

asymptotic behavior in contrast to kernel and orthogonal series estimators.

The likelihood equation in the method of the root density estimator has a simple quasilinear
structure and admits developing effective rapidly converging iteration procedure even in the case of
multiparametric problems (for example, when the number or parameters to be estimated runs up to
many tens or even hundreds). That is why the problem under consideration favorably differs from
the other well-known problems solved by the maximum likelihood method when the complexity of
numerical  simulations  rapidly  increases  and  the  stability  of  algorithms  decreases  with  increasing
number of parameters to be estimated.

Basic objects of the theory (state vectors, information and covariance matrices etc.) become
simple  geometrical  objects  in  the  Hilbert  space  that  are  invariant  with  respect  to  unitary
(orthogonal) transformations.

1.  Maximum Likelihood Method and Fisher Information Matrix

x

(
x
1=

,...,

)nx

Let 

  be  a  sample  under  consideration  represented  by  n   independent

)θxp
(

.  Here,  θ   is  the  distribution  parameter  (in

observations  from  the  same  distribution 

general, vector valued).

The likelihood function is determined by the following product:

(
xL

θ

)

 

=

(
ixp

)

θ

 . (1.1)

n

∏

i

1
=

4

The  formula  under  consideration  is  the  n -dimensional  joint  density  of  random
(
x
1=

distributions of the components of the vector 

,...,

x

)nx
(
x
1=

x

 interpreted as a set of independent
,...,

)nx

  is  a  certain  realization
random  variables  with  the  same  distribution.  But  if 
(fixed  sample),  the  likelihood  function  as  a  function  of  θ   characterizes  the  likeliness  of  various
values of the distribution parameter.

According  to  the  maximum  likelihood  principle  put  forward  by  Fisher  in  1912  [24]  and

developed in the twenties of the last century [25], the value θˆ
where the likelihood function reaches its maximum value, should be taken as an estimation for θ .

  from the region of acceptability,

As a rule, it is more convenient to deal with the log likelihood function:

Both  the  log  likelihood  and  likelihood  functions  have  extrema  at  the  same  points  due  to  the
monotonicity of the logarithm function.

The  necessary  condition  for  the  extremum  of the  log  likelihood  function  is  determined  by

ln

L

=

ln

(
ixp

)
θ . (1.2)

n

∑

i

1
=

the likelihood equation of the form
∂

L

ln
∂
θ

=

0

If 

. (1.3)
(
1=
θ
θ
likelihood equations
ln
∂
θ
∂

0
        
i

L

=

i

=

,...,1

s

 (1.4)

,...,

)sθ

  is  an  s -dimensional  parameter  vector,  we  have  the  set  of  the

The  basic  result  of  the  theory  of  maximum  likelihood  estimation  is  that  under  certain

ˆ
θ

(
ˆ
1=
θ

)sθ
ˆ

,...,

,...,

)sθ

 [22, 26-28].

sufficiently general conditions, the likelihood equations have a solution 
 that is a
consistent,  asymptotically  normal,  and  asymptotically  efficient  estimator  of  the  parameter
θ

(
1=
θ
Formally, the aforesaid may be expressed as
(
−IN
1
θ
,
The  last  formula  means  that  the  estimator  θˆ   is  asymptotically  (at  large  n )  a  random
variable with a multidimensional normal distribution with the mean equal to the true value of the

)θ
( )

~ˆ
θ

. (1.5)

parameter θ  and the covariance matrix equal to the inverse of the Fisher information matrix.

The Fisher information matrix elements are
(
xp
ln
θ
∂

(
xp
ln
θ
∂

(
xp

)dx

n
⋅=

∫

θ

θ

θ

)

)

∂

∂

 (1.6)

i

j

( )
θ

I

ij

The factor  n  indicates that the Fisher information is additive (the information of a sample
∞→n
consists  of  the  information  in  its  points).  At 
,  the  covariance  matrix  asymptotically
tends  to  zero  matrix,  and,  in  particular,  the  variances  of  all  the  components  become  zero
(consistency).

5

The  fundamental  significance  of  the  Fisher  information  consists  in  its  property  to  set  the
constraint on achievable (in principle) accuracy of statistical estimators. According to the Cramer-
  is nonnegative for any unbiased estimator θˆ  of
( )θˆΣ

( )θ
Rao inequality [28], the matrix 
an  unknown  vector  valued  parameter  θ .  Here, 
  is  the  covariance  matrix  for  the  estimator
θˆ . The corresponding difference asymptotically tends to a zero matrix for the maximum likelihood
estimators (asymptotic efficiency).

( )
ˆ
θ

−−
I

Σ

1

2.  Psi Function and Likelihood Equation

A psi function considered further is a mathematical object of statistical data analysis. This
function is introduced in the same way as in quantum mechanics (see, e.g., [29-31]) to drastically
simplify statistical density estimators obtained by the maximum likelihood method.

The  introduction  of  the  psi  function  implies  that  the  “square  root”  of  the  distribution

function
( )
xp
is considered instead of the distribution function itself.

( ) 2x

ψ=

 (2.1)

Let  the  psi  function  depend  on  s   unknown  parameters 

  (according  to
quantum mechanics, the basis functions are traditionally numbered from zero corresponding to the
ground state). The parameters introduced are the coefficients of an expansion in terms of a set of
basis functions:

cc
,
1
0

−sc
1

,...,

Assume that the set of the functions is orthonormal. Then, the normalization condition (the

ψ

( )
x

=

c
ϕ
i

i

( )
x

. (2.2)

s

1
−

∑

i

=

0

1
−

total probability is equal to unity) is given by
s
* =∑
icc
i
i
Hereafter, the asterisk denotes the complex conjugation.

. (2.3)

1

=

0

∞→s

We will consider sets of basis functions that are complete for 

. At a finite  s , the
estimation of the function by (2.2) involves certain error. The necessity to restrict the consideration
to a finite number of terms is related to the ill-posedness of the problem. Because of the finite set of
experimental  data,  a  class  of  functions,  where  the  psi  function  is  sought,  should  not  be  too  wide;
otherwise a stable description of a statistical distribution would be impossible. Limiting the number

of terms in the expansion by a finite number  s  results in narrowing the class of functions where a
small  s ), the dependence to be found would be estimated within too much error. The problem of

solution  is  sought.  On  the  other  hand,  if  the  class  of  functions  turns  out  to  be  too  narrow  (at  too

an optimal choice of the number of terms in the expansion is discussed in greater detail in Sec. 7.

The maximum likelihood method implies that the values maximizing the likelihood function

and its logarithm
∑

ln

ln

L

=

n

k

1
=

(
k cxp

)

→

max

 (2.4)

are used as most likely estimators for unknown parameters 

cc
,
1
0

,...,

−sc

.

1

The probability density is

6

*
ψψ

( )
xp

*
ϕ
j

( )
x
ϕ

( )x

*
j

=

cc
i

=
Hereafter,  we  imply  the  summation  over  recurring  indices  numbering  the  terms  of  the
expansion  in  terms  of  basis  functions  (unless  otherwise  stated).  On  the  contrary,  statistical  sums
denoting the summation over the sample points will be written in an explicit form.

. (2.5)

i

(
xp
k

ln

L

=

i

*
j

(

x

=

*
ϕ
j

)k

cc
i

*
ψψ

At sample points, the distribution density is
)
(
)
x
=
ϕ
k
In our case, the likelihood function has the form
(
∑
k
In view of the normalization condition, seeking an extremum of the log likelihood function

[
cc
i

. (2.6)

. (2.7)

*
ϕ
j

)
ϕ

]
)

ln

x

x

(

*
j

1
=

n

k

k

i

=

ln

is reduced to that for the following function:
(
S
icc
λ
where  λ  is the Lagrange multiplier.

* −
i

)1

, (2.8)

L

−

The necessary condition for an extremum yields the likelihood equation

n

)
c

ϕ

(

*
i

x

x

)
ϕ
k
(
xp

(
)

j

k

k

1
=

λ

−

S
∂ ∑
=
*
c
∂
i
Thus, the problem of looking for the extremum is reduced to the eigenvalue problem
cR
s
ij
where

,...,1,0

        

, (2.10)

. (2.9)

−

=

=

λ

c
i

1

c

=

0

j

i

,

k

j

i

j

R
ij

=

n

∑

k

1
=

ϕ

(

*
i

)

x

k

x

)
ϕ
k
(
xp

k

j

(
)

. (2.11)

( )xp

density 
solved by the iteration method (see below).

solved straightforwardly.

The problem (2.10) is formally linear. However, the matrix 

ijR  depends on an unknown

. Therefore, the problem under consideration is actually nonlinear, and should be

An exception is the histogram density estimator presented below when the problem can be

Multiplying  both  parts  of  Eq.  (2.10)  by 

∗
ic   and  summing  with  respect  to  i ,  in  view  of
(2.3)  and (2.5),  we  find that the most likely state  vector  c   always  corresponds  to  its  eigenvalue
n=λ

.

Let  us  verify  whether  the  substitution  of  the  true  state  vector  into  the  likelihood  equation
turns  it  into  an  identical  relation  (in  the  asymptotic  limit).  Indeed,  at  a  large  sample  size
∞→n
(
),  according  to  the  law  of  large  numbers  (the  sample  mean  tends  to  the  population
mean) and the orthonormality of basis functions, we have

7

1
n

R

ij

=

ϕ

∗
i

→

∫

n

k

1

=

∑

1
n
( )
x
ϕ
( )
xp

j

ϕ

(

x

∗
i

)
ϕ
k
(
xp

k

j

(
)

x

k

)

→

( )
x

xp

( )

dx

=

δ

ij

. (2.12)

1
n

Thus, the matrix  R

 asymptotically tends to unit matrix. In other words, Eq. (2.10) shows

that  the  true  state  vector  is  its  solution  for 

  (consistency).  The  matrix  R

  may  be

∞→n

1
n

referred to as a quasi-unit.

Let  us  assume  that  the  basis  functions 

( )xiϕ

  and  the  state  vector  c   are  real  valued.

Then, the basic equation for the state vector (2.10) can be expressed in the form

1
n

n




∑

∑=



−
1

=

1

0

k

s

j

ϕ

i

(

x

k

)

c

ϕ

j

j

(

x

k

)








=

c

i

i
       

=

,...,1,0

s

−

1

. (2.13)

Here,  we  have  written  the  summation  signs  for  clearness.  As  is  easily  seen,  the  solution  of  this
equation satisfies the normalization condition (2.3).

3.  Histogram Density Estimator

In  order  to  study  the  histogram  density  estimator,  one  has  to  assume  that  a  distribution  is
given in a finite region (in the case of variables distributed along an infinite interval, it is necessary
to  cut  off  the  distribution  tails,  e.g.,  by  using  maximum  and  minimum  values  in  the  sample  as
bounds).

Let  us  divide  the  full  range  of  variation  for  a  random  variable  into  a  finite  number  of

xx
, 1
0

,...,

sx

 divide the full range of variation for a random variable into  s

intervals. Points 
intervals (bins).

( )
xϕ
i

=

x

x

x

2/1

   

1
−

at     

x
≤≤

Assume that


(

i
1
+

0
          

1
The functions 
Equation (2.3) yields the following most likely estimator for the psi function:

)
i
          
 
otherwise
( )
xiϕ

,...,1,0

. (3.1)

     
i

−

=

x

s

1
+

i

i

 form an orthonormal but, of course, incomplete set.

ψ

( )
x

=

c

i

ϕ

i

( )
x

,

s

1
−

∑

i

=

0

c
i =

 

, (3.2)

n
i
n

where 

in  is the number of points in  i -th interval.
In order to avoid appearing indeterminate forms (zero divided by zero) while calculating the
0>in
As  is  easily  seen,  the  square  of  the  psi  function  constructed  in  this  way  is  a  histogram

ic , one has to assume that 

 in each interval.

expansion coefficients 

density estimator.

Applying a unitary transformation to the found state vector shows a natural way to smooth a

histogram density estimator that is as follows.

8

Let  us  transform  the  column  vector 

ic   and  basis  functions 

( )xiϕ

  by  a  unitary  matrix

U :
cUc =′
i
k
ik
( )
∗=
Ux
ϕ
il
The  psi  function  and,  hence,  the  density  turn  out  to  be  invariant  with  respect  to  this

, (3.3)
ϕ

( )x

. (3.4)

′
i

l

k

′
i

=

∗
ϕ
il

( )
′
x

( )
UcUx
ik

transformation. Indeed,
( )
′=
x
c
ψ
ϕ
=
i
Here, we have taken into account that due to the unitarity of the matrix U ,
∗
UU
δ=
il
The plus superscript denotes the Hermitian conjugation.

= +
UU
li

( )x

. (3.6)

. (3.5)

c
l

ϕ

ik

lk

ik

l

l

The aforementioned transformation will be useful if the basis functions 

 in a new
ic′
representation  may  be  ranged  in  increasing  complexity  in  such  a  way  that  the  amplitudes 
corresponding  to  first  (most  simple)  basis  functions  turn  out  to  be  large,  whereas  those
corresponding to more complex basis functions, relatively small. Then, a histogram density can be
smoothed by truncating higher harmonics.

A classical example of such a unitary transformation is the discrete Fourier transform given

( )xiϕ′

by the matrix

U kl

=

1

s


exp



i

π2
s





kl

. (3.7)

A  state  vector  resulting  from  the  unitary  transformation  with  the  matrix  (3.7)  may  be

interpreted as a frequency spectrum of a signal that is the histogram estimator of a psi function.

In  general  case,  choosing  a  unitary  transformation  and  a  way  in  which  to  filter  noise,  and
ranging basis functions in ascending order of complexity should be performed on the basis of the
analysis  of  a  particular  problem.  A  statistical  fluctuation  level  for  an  empirical  psi  function  is
discussed in Sec. 5.

4.  Computational Approach to Solving Likelihood Equation
In order to develop an iteration procedure for Eq. (2.13), let us rewrite it in the form










∑

∑=



)
(

. (4.1)

1
n

(
1

α

α

ϕ

ϕ

=

−

+

)

(

)

c

c

c

x

x

−

=

n

1

0

1

k

k

k

s

i

j

i

j

i

j

Here, we have introduced an additional parameter 
 that does not change the equation
itself but substantially influences the solution stability and the rate of  convergence of an  iteration
procedure.

0

< α

<

1

Let us represent an iteration procedure (transition from  r -th to 

1+r

-th approximation) in

the form

9

+

1

c

r
i

=

α

⋅

c

r
i

+

(
1

−

α

)

⋅

1
n

n




∑

∑=



−

=

1

k

s

j

1

0

ϕ

i

(

x

k

)
(

x

c

r
j

ϕ

j

)

k

. (4.2)








.

Let us study the conditions of stable convergence of the iteration procedure to the solution.
We restrict our consideration to the case of small deviations. Let  cδ  be any small deviation of an
'cδ , that at the
approximate state vector from the exact solution of Eq. (4.1) at arbitrary step; and 
next  iteration  step.  The  squared  distance  between  the  exact  and  approximate  solutions  is
) (
(
c T δ
δ

)c
A  fundamental  condition  for  an  iteration  procedure  to  converge  is  that  the  corresponding
mapping has to be contracting (see the principle of contracting mappings and fixed point theorem
[32, 33]). A mapping is contracting if (
)
<′
δ
δ ='
c
, (4.3)
where  A  is the perturbation matrix:
)

. It can be proved that

cA
δ

) (
δ

) (
δ

)c

(
δ

(
1

c

c

c

′

T

T

A

=

α

E

−

R

. (4.4)

α
−
n

)s

s ×  unit matrix.

Here,  E  is an (
After an iteration, the squared distance is decreased by
) (
)
(
) (
(
(
=′
c
c
δ
δ
δ
δ
δ
T−
AAEB
=
 
where 
is the contracting matrix.

(
)cBc
)
δ
(4.6)

, (4.5)

−

c

c

)

′

T

T

T

The mapping is contracting if  B  is a positive matrix.
The minimum  eigenvalue  minλ
contractility. An eigenfunction related to  minλ
convergence  standpoint.  Thus,  the  parameter  minλ
min ⋅
since the squared distance decreases at least by 

λ

 of the  B   matrix  is  expedient  to  consider  as  a  measure  of

 corresponds to perturbation that is worst from the

  characterizes  the  guaranteed  convergence,
%100
0R  be the vector of eigenvalues for the  R  matrix.

Let 
The  B -matrix eigenvalues are expressed in terms of the  R -matrix eigenvalues by

 at each step.

α

)

(
12
−
α
n

2

)

(
1

−
α
2
n

λ

i

1
−=

2

+

α
The  minimum  of  this  expression  at  any  given  α   is  determined  by  either  maximum  (at

 (4.7)

R
0

−

i

2
R
i
0

small α ) or minimum (at large α ) 

iR0 .

As  an  optimal  value  of  α ,  we  will  use  the  value  at  which  minλ

  reaches  its  maximum
(maximin  rule).  An  optimal  value  of  α   is  determined  by  the  sum  of  maximum  and  minimum
values of 

iR0 :
D
0
2 Dn
+
0

=α
opt

 , 

D
0

=

max

(

R
0

i

)

+

min

(

R
0

)i

. (4.8)

10

For the difference of distances between the approximate and exact solutions before and after

  and  exact  c   solutions  decreases  not  slower

iteration, we have
ρ ≤′
ερ

, where 

ε

=

1 λ
−

. (4.9)

min

( )rc
The  distance  between  the  approximate 
than infinitely decreasing geometric progression [33]
)

r
ρε

(
c

(
c

(
c

( )
0

( )
0

( )
r

≤

ρ

ρ

≤

( )

c

c

c

,

,

,

r

)
The result obtained implies that the number of iterations required for the distance between

. (4.10)

)1

ε

ε
1
−

the approximate and exact solutions to decrease by the factor of 

(
exp k

)0

 is

r
0

≈

2
k
0
λ−

−
(
1ln
Figure  1  shows  an  example  of  the  analysis  of  the  iteration  procedure  convergence.  A

. (4.11)

)min

λmin

)α
(

dependence 
  to  be  found  is  shown  by  a  solid  curve  consisted  of  two  segments  of
parabolas.  Besides  that,  Fig.  1  shows  the  number  of  iterations  resulted  in  the  same  solution  for
various  α  at a given accuracy, as well as the approximation of the number of iterations by (4.1)
(dotted line). The log likelihood function was controlled in the course of the iteration procedure: the

procedure was stopped when the log likelihood function changed by less than 

10

10−

.

Fig 1. A dependence of the minimum eigenvalue 
of the contracting matrix (left scale) and the number
of iterations (right scale) on the iteration parameter

Minimum eigenvalue 

Optimum

5000

1000
500

100
50

10
5

1,0

0,8

0,6

0,4

0,2

0,0

Number of iterations

Stability boundary

-0,2

0,0

0,2

0,4

0,6

0,8

1
1,0

alfa

5.  Statistical Properties of State Estimator

For the sake of simplicity, consider a real valued psi function.
Let an expansion have the form
2
s
−

c
ϕ
11

(
2
c
1

( )x

( )
x

( )
x

)
ϕ

ϕ
1

...

...

+

+

−

+

+

+

1

c

c

−

−

1

0

1

s

s

ψ

( )
x

=

.  (5.1)
)2
sc
−++
1

...

(
2
c
1

Here, we have eliminated the coefficient 

c
0
estimated, since it is expressed via the other coefficients by the normalization condition.

 from the set of parameters to be

−

=

1

The  parameters 

,
cc
1

2

,...,

−sc
1

  are  independent.  We  will  study  their  asymptotic  behavior

using the Fisher information matrix [22, 26-28]
(
cxp
,
c
∂
i

(
cxp
,
c
∂

( )
cI
ij

n
⋅=

ln

ln

∫

∂

∂

)

)

j

)dxcxp
(
,

. (5.2)

11

It  is  of  particular  importance  for  our  study  that  the  Fisher  information  matrix  drastically

simplifies if the psi function is introduced:
(
cx
,
∂
ψ
∂
c
i

(
cx
,
∂
ψ
∂
c

dx

∫

=

n

4

)

)

I

⋅

ij

j

. (5.3)

In the case of the expansion (5.1), the information matrix 

ijI  is  (
s

(
)
1
s
−×−

)1

 matrix of

the form

I

ij

=

4

n

δ

ij

+





j

cc
i
2
c
0





,       

c
0

=

1

−

(
2
c
1

)2
sc
−++
1

...

. (5.4)

A noticeable feature of the expression (5.4) is its independence on the choice of basis functions. Let
us show that only the root density estimator has this property.

Consider  the  following  problem  that  can  be  referred  to  as  a  generalized  orthogonal  series
density  estimator.  Let  the  density  p   be  estimated  by  a  composite  function  of  another  (for
simplicity, real-valued) function  g . The latter function, in its turn, is represented in the form of the
expansion in terms of a set of orthonormal functions, i.e.,
s
∑

( )gp

, where  

( )
xg

. (5.5)

p =

( )
x

c
ϕ
i

=

1
−

i

 be estimated by the maximum likelihood method.

0
i
=
,...,1,0

Let the coefficients 

=

ci
i
s
−
  ;
Consider the following matrix:
)
∂

ln

∂

)

1

(
,
cxp
c
∂

i

(
ln
,
cxp
c
∂

j

(
,
dxcxp

)

=

~
I
ij

( )
c

⋅=
n

∫

=

n

∫

1
p

)

(
∂
,
cxp
c
∂

i

)

(
∂
,
cxp
c
∂

j

dx

=

n

∫

2

1
p

∂
p
g
∂









)

(
∂
,
cxg
c
∂

i

)

(
∂
,
cxg
c
∂

j

dx

The structure of this matrix is simplest if its  elements  are  independent  of  both  the  density

and basis functions. This can be achieved if (and only if) 

 satisfies the condition

. (5.6)

( )gp

const

2



1
p
∂

=
p
g
∂


yielding
g =

const

p

. (5.7)

p
~
The  I
δ
n
4

ij

          
~
The  I

~
I
ij

=

 matrix has the form
ji
,  

,...,1,0

=

s

−

1

. (5.8)

Choosing  unity  as  the  constant  in  the  last  expression,  we  arrive  at  the  psi  function
=

 with the simplest normalization condition (2.3).

g

=ψ

  matrix  under  consideration  is  not  the  true  Fisher  information  matrix,  since  the
ic   are  dependent.  They  are  related  to  each  other  by  the  normalization

expansion  parameters 
condition. That is why we will refer to this matrix as a prototype of the Fisher information matrix.

As is seen from the asymptotic expansion of the log likelihood function in the vicinity of a
stationary point, statistical properties of the distribution parameters are determined by the quadratic

12

.  Separating  out  zero  component  of  the  variation  and  taking  into  account  that

 (see the expression (5.12) below), we find

j

i

~s
1
−
form  ∑
ccI
δδ
i
ij
0
cc
i
2
c
0

∑

2
c
0

=

δ

,
ji

cc
δδ
i

1
−

1
=

=

s

j

,

j

j

~
I
ij

s

1
−

∑

ji
,

=

0

s

1
−

∑

ji
,

=
1

cc
δδ
i

j

=

I

cc
δδ
i

ij

j

, (5.9)

where the true information matrix  I  has the form of (5.4).
Thus, the representation of the density in the form 

2ψ=p

 (and  only this representation)

results in a universal (and simplest) structure of the Fisher information matrix.

In  view  of  the  asymptotic  efficiency,  the  covariance  matrix  of  the  state  estimator  is  the

inverse Fisher information matrix:
( )c
Σ

( )
ˆ
c

1
−=
I
 (5.10)
The matrix components are
1
(
δ
n
4
Now,  let  us  extend  the  covariance  matrix found  by  appending  the  covariance  between  the

. (5.11)

,...,1

cc
i

)j

−

=

1

−

 ,

s

i

j

ij

=Σ
ij

0c  component of the state vector and the other components.
Note that
c
∂
0
c
∂
i
This yields

c
−
i
c
0

. (5.12)

c
0

c
i

c
i

=

=

δ

δ

δ

=Σ
j
0

cc
δδ
0

j

=

cc
δδ
i

j

=

i

c
−
c
0
(
δ

ji
nc

4

0

−

cc
j

i

)

−=

j

cc
0
4

n

. (5.13)

Σ−

c

i

ji

−

c

i

=

=

c
Similarly,

0

2
c
0
n

cc
i
2
c
0

cc
i
2
c
0

j

j

j

1

=

=

=Σ
ij

cc
δδ
i

. (5.14)

=Σ
00

cc
δδ
0
0

−
4
Finally, we find that the covariance matrix has the same form as (5.11):
1
n
4
This result seems to be almost evident, since the zero component is not singled out from the
others  (or  more  precisely,  it  has  been  singled  out  to  provide  the  fulfillment  of  the  normalization
condition). From the geometrical standpoint, the covariance matrix (5.15) is a second-order tensor.

,...,1,0

=Σ
ij

. (5.15)

cc
i

(
δ

)j

=

−

1

−

 ,

s

  

j

i

ij

Moreover,  the  covariance  matrix  (up  to  a  constant  factor)  is  a  single  second-order  tensor

satisfying the normalization condition.

Indeed, according to the normalization condition,
)

. (5.16)

δ
cc
i
i

=

=

0

2

(
cc
i
i

δ

Multiplying  the  last  equation  by  an  arbitrary  variation 

jcδ   and  averaging  over  the

statistical ensemble, we find

13

(
ccEc
δδ
i
i

j

)

Σ=

c
i

ji

0=

. (5.17)

ic : 

ijδ  and

Only two different second-order tensors can be constructed on the basis of the vector 
icc
these tensors have to appear in the matrix only in the combination (5.15).

.  In  order  to  provide  the  fulfillment  of  (5.17)  following  from  the  normalization  condition,

j

It  is  useful  to  consider  another  derivation  of  the  covariance  matrix.  According  to  the
icδ  are dependent, since they are related to each other by
normalization condition, the variations 
the  linear  relationship  (5.16).  In  order  to  make  the  analysis  symmetric  (in  particular,  to  avoid
expressing  one  component  via  the  others  as  it  has  been  done  in  (5.1)),  one  may  turn  to  other
variables that will be referred to as principle components.

cU
δ
ij

j

Consider the following unitary (orthogonal) transformation:
=
−
Let the first  (to be more precise, zero)  row of  the transformation  matrix coincide  with the

          

,...,1,0

. (5.18)

=

1

δ

s

j

f

i

,

i

state  vector: 

.  Then,  according  to  (5.16),  the  zero  variation  is  identically  zero  in  new

c

j

U =0
j
0 =fδ
0

coordinates: 

.
The inverse transformation is
,...,1,0

          

=

=

δ

c

j

i

,

i

+
fU
δ
ij

j

s

−

1

. (5.19)

, the first (more precisely, zero) column of the matrix 

In view of the fact that 

0
can be eliminated turning the matrix into the factor loadings matrix  L . Then
     
δ

,...,1,0

0 =fδ

,...,1

=

−

s

s

j

c
i

j

=

;1

         
i

1
fL
−
δ
ij
The relationship found shows that  s  components of the state-vector variation are expressed
1−s
In terms of principle components, the Fisher information matrix and covariance matrix are

 principal components (that are independent Gaussian variables).

.   (5.20)

=

through 

+U

proportional to a unit matrix:

I

f
ij

n
δ4=

ij

i

 ,

j

=

,...,1

s

−

1

, (5.21)

f
=Σ
ij

f
f
δδ
i

j

=

i

 ,

j

=

,...,1

s

−

1

. (5.22)

δ

ij
4
n

independent and have the same variance 

1
n4

.

on the basis of (5.22). Indeed,

=Σ
ij

cc
δδ
i

j

=

fLL
ik
js

f
δδ
k

s

=

LL
ik

js

The  last  relationship  particularly  shows  that  the  principal  variation  components  are

The expression for the covariance matrix of the state vector components can be easily found

LL
ik

jk

In view of the unitarity of the 
+
Taking into account two last formulas, we finally find the result presented above:

. (5.24)

cc
i

δ=

ij

j

jk

=

LL
δ
ik
ks
4
4
n
n
+U  matrix, we have

. (5.23)

14

=Σ
ij

−

1
(
δ
n
4
In quantum mechanics, the matrix

,...,1,0

cc
i

)j

=

   

 ,

s

i

j

ij

−

1

. (5.25)

ij

cc=ρ
i
is referred to as a density matrix (of a pure state). Thus,

 (5.26)

j

(

E

=Σ

)ρ−

1
n4
where  E  is the 

, (5.27)

ss×  unit matrix.
In the diagonal representation,

+

=Σ UDU , (5.28)
where U  and  D  are unitary (orthogonal) and diagonal matrices, respectively.

As is well known from quantum mechanics and readily seen straightforwardly, the density
matrix of a pure state has the only (equal to unity) element in the diagonal representation. Thus, in
our  case,  the  diagonal  of  the  D  matrix  has  the  only  element  equal  to  zero  (the  corresponding
1  (corresponding
n4

eigenvector is the state vector); whereas the other diagonal elements are equal to 

eigenvectors  and  their  linear  combinations  form  a  subspace  that  is  orthogonal  complement  to  the
state vector). The zero element at a principle diagonal indicates that the inverse matrix (namely, the
Fisher  information  matrix  of  the  s -th  order)  does  not  exist.  It  is  clear  since  there  are  only 
1−s
independent parameters in the distribution.

The  results  on  statistical  properties  of  the  state  vector  reconstructed  by  the  maximum
likelihood  method  can  be  summarized  as  follows.  In  contrast  to  a  true  state  vector,  the  estimated
one involves noise in the form of a random deviation vector located in the space orthogonal to the
  components)  are
true  state  vector.  The  components  of  the  deviation  vector  (totally, 
1 .  In  the
asymptotically  normal  independent  random  variables  with  the  same  variance 
n4

1−s

aforementioned 

1−s

squared length is the random variable 

-dimensional space, the deviation vector has an isotropic distribution, and its
1−sχ  is the random variable with the chi-square

, where 

2

2
1−χ
s
n
4

1−s
distribution of 
)
(
( )
0
,
c
cc
c
s
⋅
−
+
i
i
( )0c   and  c   are  true  and  estimated  state  vectors,  respectively; 
where 

 degrees of freedom, i.e.,
1
ξ
=

          
i

,...,1,0

. (5.29)

=

( )
0

i

product; and 

iξ , the deviation vector. The deviation vector is orthogonal to the vector 

(
cc
,

( )
0

)

=

( )0
icc
i

,  their  scalar
( )0c  and has

the squared length of 

 determined by chi-square distribution of 

1−s

 degrees of freedom, i.e.,

2
1−χ
s
n
4

,

c

)

( )
0

(
ξ

2
χ
s
1−=
n
4
Squaring (5.29), in view of (5.30), we have

( )
0
ic
i

   (

ξξ
,

ξξ
i

=

=

0

ξ

=

)

i

(5.30)

1

−

(
cc
,

20
( )

)

=

. (5.31)

2
χ
s
1
−
n
4

15

This expression means that the squared scalar product of the true and estimated state vectors

is smaller than unity by asymptotically small random variable 

2
1−χ
s
n
4

.

The  results  found  allow  one  to  introduce  a  new  stochastic  characteristic,  namely,  a
confidence  cone  (instead  of  a  standard  confidence  interval).  Let  ϑ   be  the  angle  between  an
unknown true state vector 

( )0c  and that  c  found by solving the likelihood equation. Then,

2
sin

ϑ

1
−=

2
cos

ϑ

1
−=

(
cc
,

20
( )

)

=

2
χ
s
1
− ≤
n
4

2
αχ
s
,1
−
n
4

. (5.32)

Here, 

2
,1 αχ −s
distribution of 

  is  the  quantile  corresponding  to  the  significance  level  α     for  the  chi-square
1−s

 degrees of freedom.

The  set  of  directions  determined  by  the  inequality  (5.32)  constitutes  the  confidence  cone.
The axis of a confidence cone is the reconstructed state vector  c . The confidence cone covers the
direction of an unknown state vector at a given confidence level 

α−=1P

.

From the standpoint of theory of unitary transformations in quantum mechanics (in our case
transformations are reduced to orthogonal), it can be found an expansion basis in (5.1) such that the
sum will contain the only nonzero term, namely, the true psi function. This result means that if the
best basis is guessed absolutely right and the true state vector is  (
vector  estimated  by 
the  maximum 
(
ccc
,
,...,
0
1

,...,0,0,1
likelihood  method  will  be 
(
2
c
1

, the empirical state
random  vector

)2
sc
−++
1

)0
the 

components

,  where 

other 

and 

the 

...

=

−

, 

1

c

,

2

0

=

,...,1

s

−

1

 will be independent as it has been noted earlier.

)1
−sc

i
      



ci

~

N

,0





1
n
4

6.  Chi-Square Criterion. Test  of  the  Hypothesis  That  the  Estimated  State  Vector  Equals  to
the  State  Vector  of  a  General  Population.  Estimation  of  the  Statistical  Significance  of
Differences between Two Samples.

Rewrite (5.31) in the form
2
−=
χ . (6.1)
1
s

)

(

)

−

2 0
( )

n
14

(
cc
,
This  relationship  is  a  chi-square  criterion  to  test  the  hypothesis  that  the  state  vector
estimated  by  the  maximum  likelihood  method  c   equals  to  the  state  vector  of  general  population
( )0c .

In view of the fact that for 

∞→n

1

+

 

(
,
cc

)
( )
0 →

2

, the last inequality may be rewritten in

another asymptotically equivalent form
)

χ . (6.2)

(
−∑
c
i

( )
0
c
i

2
s
1
−

=

4

n

1
−

2 

s

i

=

0

1
−

Here, we have taken into account that
s
s
1
−
∑
∑

cc
2
i

( )
0
i

( )
0
i

(
c

(

−

−

=

+

)

c

c

c

2 

2

i

i

20
( )
i

i

=

0

i

=

0

)

=

s

1
−

∑

i

=

0

(
12

−

cc
i

( )
0
i

)

.

As  is  easily  seen,  the  approach  under  consideration  involves  the  standard  chi-square
criterion as a particular case corresponding to a histogram basis. Indeed, the chi-square parameter is
usually defined as [22, 28]

16

s

1
−

(
n
i

2

χ

=

2 

)

( )
0
n
−
i
( )
0
n
i

i

=

,

∑
0
( )0
in  is the number of points expected in  i -th interval  according to theoretical distribution.

where 

In the histogram basis, 

i =
c

 is the empirical state vector and 

, theoretical

n
i
n

c

( )
0 =
i

( )
0
n
i
n

state vector. Then,

2

χ

=

n

s

1
−

∑

i

=

0

(
2
c
i

)

2 2 0
( )
c
i
 
( )
2 0
i
 

−
c

→

4

n

(
c
i

20
( )
c
i

)

−

. (6.3)

s

1
−

∑

i

=

0

Here, the sign of passage to the limit means that random variables appearing in both sides of (6.3)
have the same distribution. We have used also the asymptotic approximation
2
c
c

. (6.4)

→

−

−

−

+

=

c

c

c

c

( )
0
i

(
)
c

i

( )
0
i

(
c

i

)0

( )
i

( )
0
i

)

( )
2 0
i
 

(
c

i

2
i

Comparing (6.2) and (6.3) shows that the parameter 

2χ -
distribution of 
 degrees of freedom (if the tested hypothesis is valid). Thus, the standard chi-
square  criterion  is  a  particular  case  (corresponding  to  a  histogram  basis)  of  the  general  approach
developed here (that can be used in arbitrary basis).

2χ  is a random variable with 

1−s

The  chi-square  criterion  can  be  applied  to  test  the  homogeneity  of  two  different  set  of
observations  (samples).  In  this  case,  the  hypothesis  that  the  observations  belong  to  the  same
statistical ensemble (the same general population) is tested.

In  the  case  under  consideration,  the  standard  chi-square  criterion  [22,  28]  may  be

represented in new terms as follows:

2

s

1
−

−









∑

( )
2
n
i
n

( )
1
n
i
n
1
( )
1
n
i
2n  are the sizes of the first and second samples, respectively; 
1n  and 

nn
21
n
n
+
1

, (6.5)

2
( )
2
i

∑

( )
2
i

(
c

( )
1
i

→

−

+

)

n

4

c

1
−

=

=

0

2

0

2

s

i

i

2

χ

=

nn
21

where 

( )1
n  and 
i

( )2
n , the
i

( )1
c   and 
numbers  of  points  in  the  i -th  interval;  and 
i
samples. In the left side of (6.5), the chi-square criterion in a histogram basis is presented; in the
2χ   defined  in  such  a  way  is  a

right  side,  the  same  criterion  in  general  case.  The  parameter 

,  the  empirical  state  vectors  of  the

( )2
i

c

2χ  distribution of 

1−s

 degrees of freedom (the sample homogeneity is

random variable with the 
assumed).

7.  Optimization of the Number of Harmonics

i

i

∞

c

0i
=

ϕ

( )
x

. (7.1)

Let an exact (usually, unknown) psi function be
∑
Represent the psi-function estimator in the form
s
∑

. (7.2)

( )
x

ϕ

ˆ
c

1
−

i

i

ψ

( )
x

=

ˆ
ψ

( )
x

=

i

=

0

17

Here,  the  statistical  estimators  are  denoted  by  caps  in  order  to  distinguish  them  from  exact
quantities.

Comparison  of  two  formulas  shows  that  difference  between  the  exact  and  estimated  psi
functions is caused by two reasons [34]. First, we neglect  s -th and higher harmonics by truncating
the infinite series. Second, the estimated Fourier series coefficients (with caps) differ from unknown
exact values.

Let
c

i

i

c

. (7.3)

δ+

=ˆ
c
i
Then,  in  view  of  the  basis  orthonormality,  the  squared  deviation  of  the  exact  function  from  the
approximate one may be written as
)
2ˆ

(
ψψ
−

( )
sF

dx

=

+

=

δ

1
−

∞

s

. (7.4)

2
c
i

∑

i

=

0

2
c
i

∑

si
=

∫
Introduce the notation
∞

( ) ∑
sQ
=

2
ic

. (7.5)

si
=

δ

c

2
i

~

∑

i

=

0

2
χ
s
1
−
n
4

,

By  implication, 

( )sQ   is  deterministic  (not  a  random)  variable.  As  for  the  first  term,  we

will consider it as a random variable asymptotically related to the chi-square distribution:
s

1
−

where 

1−sχ  is the random variable with the chi-square distribution of 

2

1−s

 degrees of freedom.

Thus, we find that 

 is a random variable of the form

( )sF

We will look for an optimal number of harmonics using the condition for minimum of the

( )
sF

=

2
1χ
−
s +
4
n

( )sQ

. (7.6)

:

=

( )sF
( ) min
→

function mean value 
s
−
1
n
4

( )
sQ
sF
Assume that, at sufficiently large  s ,
( )
sQ =

. (7.8)

+

. (7.7)

f
rs

The optimal value resulting from the condition 

( )
sF
∂
s
∂

0=

 is

s
opt

rfn

1 4+= r
The  formula  (7.9)  has  a  simple  meaning:  the  Fourier  series  should  be  truncated  when  its

. (7.9)

coefficients  become  equal  to  or  smaller  than  the  error,  i.e., 

.  From  (7.8)  it  follows  that

2 ≤
cs

1
n
4

c

2
s

rf
≈ r
1
+
s

. The combination of the last two formulas yields the estimation (7.9).

The  coefficients  f   and  r   can  be  calculated  by  the  regression  method.  The  regression

function (7.8) is smoothed by taking a logarithm

18

ln

=

ln

( )
sQ
Another  way  to  numerically  minimize 

. (7.10)

ln

−

r

s

f

( )sF
  is  systematically  satisfied.  It  is  necessary  to  determine  that  the  inequality  is  met  just

  is  to  detect  the  step  when  the  inequality

2 ≤
cs

1
n
4

systematically  in  contrast  to  the  case  when  several  coefficients  are  equal  to  zero  in  result  of  the
symmetry of the function.

Our approach to estimate the number of expansion terms does not pretend to high rigor. For
instance, strictly speaking, our estimation of the statistical noise level does not directly concern the
coefficients in the infinite Fourier series. Indeed, the estimation was performed in the case when the
estimated  function  is  exactly  described  by  a  finite  (preset)  number  of  terms  in  the  Fourier  series
with  coefficients  involving  certain  statistical  error  due  to  the  finite  size  of  a  sample.  Moreover,
since the function to be determined is unknown (otherwise, there is no a problem), any estimation of
the truncation error is approximate, since it is related to the introduction of additional assumptions.

The  optimization  of  the  number  of  terms  in  the  Fourier  series  may  be  performed  by  the

Tikhonov regularization methods [1, 34].

8.  Numerical Simulations. Chebyshev-Hermite Basis

In this paper, the set of the Chebyshev - Hermite functions corresponding to the stationary
states of a quantum harmonic oscillator is used for numerical simulations. In particular, this basis is
convenient  since  the  Gaussian  distribution  in  zero-order  approximation  can  be  achieved  by
choosing the ground oscillator state; and adding the contributions of higher harmonics into the state
vector provides deviations from the gaussianity.

π
 is the Chebyshev-Hermite polynomial of the  k -th order. The first two polynomials

k

1

2/1

=

−

=

)

ϕ

( )
x

x
2

. (8.1)

,...2,1,0

( )
xH
k



exp


The set of the Chebyshev-Hermite basis functions is [29, 31]
2


     
,
k


(
k
!2
k
( )xH k
Here, 
have the form
( ) 1
=xH
, (8.2)
0
( )
2
xH
x
=
1
The other polynomials can be found by the following recurrent relationship:
( ) 0
H
x
=

( )
x

. (8.3)

. (8.4)

xH

kH

−

+

2

2

k

k

k

1
−

1
+

( )
x
The  algorithms  proposed  here  have  been  tested  by  the  Monte  Carlo  method  using  the
Chebyshev-Hermite  functions  for  a  wide  range  of  distributions  (mixture  of  several  Gaussian
components, gamma distribution, beta distribution etc.). The results of numerical simulations show
that the estimation of the number of terms in the Fourier series is close to optimal. It turns out that
the approximation accuracy decays more sharply in the case when less terms than optimal are taken
into account than in the opposite case when several extra noise harmonics are allowed for. From the
aforesaid, it follows that choosing larger number of terms does not result in sharp deterioration of
the approximation results. For example, the approximate density of the mixture of two components
weakly  varies  in  the  range  from  8—10  to  50  and  more  terms  for  a  sample  of  several  hundreds
points.

Figure 2 shows an example of comparison of a double-humped curve (dotted line) with an
exact  distribution  (solid  line),  as  well  as  smoothing  the  dependence  by  (7.10)  (the  sample  size  is
=n

). Note that Figs. 1 and 2 correspond to the same statistical data.

200

This  approach  implies  that  the  basis  for  the  psi-function  expansion  can  be  arbitrary  but  it
should be preset. In this case, the found results turn out to be universal (independent of basis). This
concerns the Fisher information matrix, covariance matrix, chi-square parameter etc. The set of the
Chebyshev-Hermite  functions  can  certainly  be  generalized  by  introducing  translation  and  scaling

19

parameters that have to be estimated by the maximum likelihood method. The Fisher information
matrix, covariance matrix etc. found in such a way would be related only to the Chebyshev-Hermite
basis and nothing else.

Fig. 2 (a) An example of comparison of a double-humped 
curve (dotted line) with an exact distribution (solid line); 
(b) smoothing the dependence by (7.10)

-2

-1

0

2

3

4

5

1

X

0,20

a)

0,15

P

0,10

0,05

0,00

1,000

0,500

0,100

0,050

0,010

0,005

)

(

S
Q

b)

0,001

1

5

10

S

Practically, the expansion basis can be fixed beforehand if the data describe a well-known
physical  system  (e.g.,  in  atomic  systems,  the  basis  is  preset  by  nature  in  the  form  of  the  set  of
stationary states).

In  other  cases,  the  basis  has  to  be  chosen  in  view  of  the  data  under  consideration.  For
instance, in the case of the Chebyshev-Hermite functions, it can be easily done if one assumes that
the distribution is Gaussian in the zero-order approximation.

Note  that  the  formalism  presented  here  is  equally  applicable  to  both  one-dimensional  and
multidimensional  data.  In  the  latter  case,  if  the  Chebyshev-Hermite  functions  are  used,  one  may
assume that multidimensional normal distribution takes place in the zero-order approximation that,
in  its  turn,  can  be  transformed  to  the  standard  form  by  translation,  scale,  and  rotational
transformations.

9.  Density Matrix

The  density  matrix  method 

to  study
inhomogeneous  statistical  populations  (mixtures).  The  corresponding  technique  can  be  used  in
statistical data analysis as well.

is  a  general  quantum  mechanical  method 

First,  for  example,  consider  a  case  when  the  analysis  of  Sec.  6  shows  that  two  statistical
samples  are  inhomogeneous.  In  this  case,  the  density  matrix  represented  by  a  mixture  of  two
components can be constructed for the total population:

20

ρ

=

( )
1

ρ

+

( )2

ρ

, (9.1)

n
1
n

n
2
n

where
=
n
n
1
( )
1
ρ
ij
( )
2
ij

ρ

+
n
,
2
( )
1
cc
=
i
( )
2
cc
i

( )∗
1
j
( )∗
2
j

, (9.2)

. (9.3)

=
Any density matrix can be transformed to the diagonal form by a unitary transformation. In
the diagonal representation, the density matrix of the pure state will have the only element equal to
unity  and  the  other,  equal  to  zero.  In  the  case of  two  component  mixture  (9.1),  there  will  be  two
nonzero elements etc.

Note that only the density matrix of a pure state satisfies the condition

In  the  diagonal  representation,  the  density  for  the  mixture  of  m   components  can  be

represented in terms of eigenvalues and eigenfunctions of the density matrix:

ρ =2

ρ

. (9.4)

( )
xp

=

ψλ
i

i

2

( )
x

. (9.5)

m

∑

i

1
=

In  the  case  when  the  first  component  prevails  in  the  expansion  (9.5),  it  may  be  considered  as
responsible for the basic density; whereas the other, describing noise.

Now, we cite some information on the density matrix from quantum mechanics.
The mean value of a physical quantity  A  is expressed in terms of psi function as follows:
ψ

( ) ( ) ( )
xAx

dxx

ψ

=

∗

A

=

∫
cc
i

∗
j

∗
j

( ) ( )
xAx
ϕ

( )
dxx

∫

i

ρ

=

ϕ

A
=
Here, the density matrix  ρ  and the matrix element of  A  are given by
ρ

=

Tr

=

ji

ij

(

)A
ρ

, (9.7)

ij

. (9.6)

∗
j

cc
i
ϕ∫ ∗

j

ji

=

A

( ) ( )
xAx
ϕ

( )dxx
and Tr  denotes the trace of a matrix.

i

, (9.8)

The density matrix introduced in such a way relates to a so-called pure state described by a
psi function. In general case, a matrix satisfying the following three conditions can be referred to as
a density matrix:
1.  Hermitian matrix:
ρ =+

ρ

Recall that 

. (9.9)
ρ =+
ij
2.  Positive (nonnegative) matrix
ρ

ρ

≥

≡

ρ

*
ji

0

z

z

.

∗
zz
i

ij

j

 (9.9)

∑

ji
,

21

for any column vector  z . The sign of equality takes place only for the identically zero column
vector. Thus the diagonal elements of the density matrix are always nonnegative.
3.  The matrix trace is equal to unity:
) 1=ρTr

. (9.11)

(

( )xiϕ

Each  density  matrix  ρ   defined  on  a  orthonormal  basis 

  may  be  placed  in

correspondence with a density operator
ρ
ϕρ
ij

( )x

)
ϕ

∗
i

)

(

(

=

1,
xx
In the case when the arguments of the density operator coincide, we obtain the basic object

. (9.12)

x
1

j

of probability theory, namely, probability density
( )
xp
ϕρ
ij

( )
x
ϕ

. (9.13)

( )x

xx
,

∗
i

(

)

ρ

=
The  only  probability  density  corresponds  to  the  density  matrix  (in  a  given  basis).  The

=

j

opposite statement is incorrect.

The mean value (mathematical expectation) of any physical quantity given by an arbitrary

ji

ij

(

=

=

Tr

)A
ρ

operator  A  is
ρ
A
A
Now, consider the case when the analysis by the algorithm of Sec. 6 shows the homogeneity
of  both  statistical  samples.  In  this  case,  the  state  estimator  for  the  total  population  should  be
represented by a certain superposition of the state estimators for separate samples. Let us show that
the corresponding optimal estimator is the first principle component of the joint density matrix (9.1)
in the expansion (9.5).

. (9.14)

Let us express the eigenvectors of the joint density matrix (9.1) in terms of eigenvectors of

the components:
ρ
=

c

ij

j

, (9.15)

c
λ
i
( )
1
j

( )2
j

j

+

−

λ

=

ca
1

. (9.16)

ca
2

c
Substituting  (9.16)  into  (9.15),  in  view  of  (9.2)  and  (9.3),  we  have  a  set  of  the  homogeneous
equations in two unknowns:
(
n
  
+
+
1
∗
arn
2
1
where
r
=
is a scalar product of two vectors.

)
)
an
2
1
(
n
λ
−
1

ran
2
1
)
)
an
2
2

(
n
1
(
n
+
2

( )
( )2
1
∗
c
i c
i

, (9.17)





 (9.18)

( )
1 ,

(
c

( )
2

=

=

=

+

)

0

0

c

∗

The system admits a solution if its determinant is equal to zero. Finally, the eigenvalues of

the joint density matrix are
41
k−
2

=λ
2,1

, (9.19)

±

1

where

k

=

)

(
1
+

2

r
)2

−

n
2

nn
21
(
n
1

. (9.20)

22

For simplicity's sake, we restrict our consideration to real valued vectors (just as in Sec. 6).
.  Then,

−→−

−

+

=

1

r

r

r

2

(
12

)(
1

)r

(
1

)

In  the  case  of  homogeneous  samples,  asymptotically 
according to (6.5), we asymptotically have

2
sχ
1
−
n
+
2

n
1

~

O





1
+

n
1

n
2





. (9.21)

4

k

=

Then,

λ
1

1
−=

O

, (9.22)





1
+

n
1

n
2





Oλ
=

2





1
+




Thus, the first principal component has maximum weight while merging two homogeneous

. (9.23)

n
1

n

2

samples. The second component has a weight of an order of 

 and should be interpreted as a

statistical fluctuation. If one drops the second component, the density matrix would become pure.

1
n +
1

n

2

Equation (9.17) and the normalization condition yield
n

a
1

≈

1

a

 

2

≈

n
1

+

n

2

n
2
+

n
1

n

2

 (9.24)

1
n +
1

n

2

up to terms of an order of 

. In view of (5.25), the deviation of the resulting state vector is

( )
1

( )2

ξ

E

=

j

+

+

=

a
ξ
2
(
( )
1
2
Ea
ξξ
i
1

a
ξ
=
1
)
(
ξξ
i
1
+
The  last  equation  shows  that  the  fist  principal  component  of  a  joint  density  matrix  is

, (9.25)
)
( )
1
j
)
i
      

(
( )
2
ξξ
i

.     (9.26)

,...,1,0

(
n
4
1

2
Ea
2

cc
i

(
δ

( )
2
j

=

−

=

−

)

1

n

)

s

j

,

ij

2

j

asymptotically efficient estimator of an unknown state vector.

Thus,  in  merging  two  large  homogeneous  samples  with  certain  state  estimators,  it  is  not
necessary to return to the initial data and solve the likelihood equation for a total population. It is
sufficient to find the first principle component of a joint density matrix. This component will be an
estimator for the state vector of statistical ensemble that is refined over the sample population. As it
has  been  shown  above,  such  an  estimator  is  asymptotically  effective  and  is  not  worth  than  the
estimator  on  the  basis  of  initial  data  (to  be  precise,  the  loss  in  accuracy  is  of  a  higher  order  of
magnitude than (9.26)).

This property is of essential importance. It implies that the estimated psi function involves
practically  all  useful  information  contained  in  a  large  sample.  In  other  words,  psi  function  is
asymptotically  sufficient  statistics.  This  property  can  also  be  interpreted  as  an  asymptotic
quasilinearity of a state vector satisfying the nonlinear likelihood equation.

10.    Phase  Role.  Statistical  Analysis  of  Mutually  Complementing  Experiments.  Inverse
Statistical Problem in Quantum Mechanics.

We  have  defined  the  psi  function  as  a  complex-valued  function  with  the  squared  absolute
value equal to the probability density. From this point of view, any psi function can be determined
. In particular, the psi function can be chosen real-valued.
up to arbitrary phase factor 
For  instance,  in  estimating  the  psi  function  in  a  histogram  basis,  the  phases  of  amplitudes  (3.2),
which have been chosen equal to zero, could be arbitrary.

(
exp
iS

)x
( )

23

At the same time, from the physical standpoint, the phase of psi function is not redundant.
The  psi  function  becomes  essentially  complex  valued  function  in  analysis  of  mutually
complementing (according to Bohr) experiments with micro objects [35].

According to quantum mechanics, experimental study of statistical ensemble in coordinate
space is incomplete and has to be completed by study of the same ensemble in another (canonically
conjugate,  namely,  momentum)  space.  Note  that  measurements  of  ensemble  parameters  in
canonically  conjugate  spaces  (e.g.,  coordinate  and  momentum  spaces)  cannot  be  realized  in  the
same experimental setup.

The uncertainty relation implies that the two-dimensional density in phase space 

 is
physically  senseless,  since  the  coordinates  and  momenta  of  micro  objects  cannot  be  measured

simultaneously.  The  coordinate 
  distributions  should  be  studied
  and  momentum 
separately  in  mutually  complementing  experiments  and  then  combined  by  introducing  the  psi
function.

The  coordinate-space  and  momentum-space  psi  functions  are  related  to  each  other  by  the

( )xP

~
)pP
(

)pxP ,
(

Fourier transform

ψ

( )
x

=

~
ψ

(

p

)

=

1
2
π
1
2
π

~
ψ

(

p

)

exp

(
ipx

)
dp

∫

, (10.1)

( )
x

exp

(
−

ipx

)
dx

. (10.2)

ψ

∫

( )xψ

)pψ~
(

Consider  a  problem  of  estimating  an  unknown  psi  function  (

)  by
experimental data observed both in coordinate and momentum spaces. We will refer to this problem
as an inverse statistical problem of quantum mechanics (do not confuse it with an inverse problem
in the scattering theory). The predictions of quantum mechanics are considered as a direct problem.
Thus,  we  consider  quantum  mechanics  as  a  stochastic  theory,  i.e.,  a  theory  describing  statistical
(frequency)  properties  of  experiments  with  random  events.  However,  quantum  mechanics  is  a
special  stochastic  theory,  since  one  has  to  perform  mutually  complementing  experiments  (space-
time description has to be completed by momentum-energy one) to get statistically full description
of a population (ensemble). In order for various representations to be mutually consistent, the theory
should be expressed in terms of probability amplitude rather than probabilities themselves.

  or 

A  simplified  approach  to  the  inverse  statistical  problem,  which  will  be  exemplified  by

( )xP

~
)pP
(

  and 

  have
numerical  example,  may  be  as  follows.  Assume  that  density  estimators 
already been found (e.g., by histogram estimation). It is required to approximate the psi function for
a  statistical  ensemble.  Figure  3  shows  the  comparison  between  exact  densities  that  could  be
calculated  if  the  psi  function  of  an  ensemble  is  known  (solid  line),  and  histogram  estimators
obtained  in  mutually  complementing  experiments.  In  each  experiment,  the  sample  size  is  10000
points. In Fig. 4, the exact psi function is compared to that estimated by samples. The solution was
found  by  iteration  procedure  of  adjusting  the  phase  of  psi  function  in  coordinate  and  momentum
)0=r
representations. In zero-order approximation ((
momentum-space phase in the 
psi function in the  r  approximation in the coordinate space and vice versa.

 approximation was determined by the Fourier transform of the

), the phases were assumed to be zero. The

1+r

The  histogram  density  estimator  results  in  the  discretization  of  distributions,  and  hence,

natural use of the discrete Fourier transform instead of a continuous one.

∗

From (10.1) and (10.2), we straightforwardly have
( )
x
∂
ψ
x
∂

( )
x
∂
ψ
x
∂

)
dpp

. (10.3)

~2
ψ

~
)
ψ

dx

∫

=

p

p

(

(

∗

∫

From  the  standpoint  of  quantum  mechanics,  the  formula  (10.3)  implies  that  the  same
quantity,  namely,  the  mean  square  momentum,  is  defined  in  two  different  representations

24

(coordinate  and  momentum).  This  quantity  has  a  simple  form  in  the  momentum  representation,
whereas  in  the  coordinate  representation,  it  is  rather  complex  characteristic  of  distribution  shape
(irregularity).  The  corresponding  quantity  is  proportional  to  the  Fisher  information  on  the
translation parameter of the distribution center (see Sec. 12).

Fig 3. Comparison between exact densities (solid lines) 
and histogram estimators (dots) in coordinate and 
momentum spaces.

P(p)

P(x)

50

100

150

200

250

Fig. 4 Comparison between exact psi function (solid line) 
and that estimated by a sample (dots).

0,04

P

0,08

0,07

0,06

0,05

0,03

0,02

0,01

0,00

0

)
c
(
e
R

)
c
(
m

I

0,25

0,20

0,15

0,10

0,05

0,00

-0,05

-0,10

-0,15

-0,20

75

0,25

0,15

0,05

-0,05

-0,15

-0,25

75

100

125

150

175

100

125

150

175

25

The  irregularity  cannot  be  measured  in  the  coordinate  space  in  principle,  since  it  refers  to
another  (momentum)  space.  In  other  words,  if  there  is  no  any  information  from  the  canonically
conjugate space, the distribution irregularity in the initial space may turn out to be arbitrary high.
Singular  distributions  used  in  probability  theory  can  serve  as  density  models  with  an  infinite
irregularity. From mathematical statistics, it is well-known that arbitrary large sample size does not
allow one to determine whether the distribution under consideration is continuous or singular. This
causes  the  ill-posedness  of  the  inverse  problem  of  the  probability  theory  that  has  already  been
discussed in Introduction.

Thus, from the standpoint of quantum mechanics, the ill-posedness of the classical problem
of  density  estimation  by  a  sample  is  due  to  lack  of  information  from  the  canonically  conjugate
space. Regularization methods for inverse problem consist in excluding a priory strongly-irregular
functions from consideration. This is equivalent to suppression of high momenta in the momentum
space.

Let  us  turn  now  to  more  consistent  description  of  the  method  for  estimation  of  the  state
vector  of  a  statistical  ensemble  on  the  basis  of  experimental  data  obtained  in  mutually
complementing  experiments.  Consider  corresponding  generalization  of  the  maximum  likelihood
principle and likelihood equation. To  be  specific,  we  will  assume  that  corresponding  experiments
relate to coordinate and momentum spaces.

We define the likelihood function as (compare to (1.1))
)
)

~
(
cpP

(
cxP
i

. (10.4)

=

m

n

j

(
cpxL
,

∏
)cxP i
(

=
1

i

)
∏
=
1
j
~
)cpP j
(

  and 

Here, 
in  mutually  complementing  experiments
corresponding  to  the  same  state  vector  c .  We  assume  that  n   measurements  were  made  in  the
coordinate space; and  m , in the momentum one.

the  densities 

  are 

n

Then, the log likelihood function has the form (instead of (1.2))
∑

~
(
cpP

(
cxP
i

. (10.5)

∑

ln

ln

+

)

)

m

j

ln

L

=

i

1
=

j

1
=

The  maximum  likelihood  principle  together  with  the  normalization  condition  evidently

=

results in the problem of maximization of the following functional:
(
∗
S
icc
i
where  λ  is the Lagrange multiplier and

, (10.6)

)1

ln

L

−

−

λ

n

m

(

)
)

(
cc
i

(
cc
i

~
∗
ϕ
j
i

(

)
ϕ

(

l

l

i

k

k

k

1
=

1
=

∗
j

x

x

p

+

L

ln

ln

∗
ϕ
j

Here, 

∑

∑
ln
=
)piϕ~
(
( )xiϕ
 is the Fourier transform of the function 
The likelihood equation has the form similar to (2.10)
cR
i
       
,...,1,0
=
ij
where the  R  matrix is determined by
~
)
)
(
p
ϕ
ϕ
l
k
~
(
(
xP
pP
l

. (10.9)

, (10.8)

~
*
ϕ
i

∑

∑

(
)

(
)

c
i

R
ij

=

λ

−

1

p
l

s

ϕ

=

+

)

(

)

x

x

*
i

j

,

m

n

k

k

j

l

j

j

.

~
)
ϕ

(

∗
j

p

l

)
)

.   (10.7)

1
k
=
By full analogy with calculations conducted in Sec. 2, it can be proved that the most likely
 of the  R  matrix (equal to sum of

mn +=λ

1
=

state vector always corresponds to the eigenvalue 
measurements).

The likelihood equation can be easily expressed in the form similar to (2.13):

26

1
+

n

m

n

x

∗
i

ϕ

(







The Fisher information matrix (prototype) is determined by the total information contained

∑
∑=

. (10.10)

)
(

)
(

(
~
ϕ








∑

∑

~
ϕ

)

)

ϕ

∗
i

=

+

p

p

c

c

x

c

∗
j

∗
j

∗
j

∗
j

m

=

=

=

1

1

1

1

k

k

k

s

s

j

l

l

i

l

j

i

∗

,

⋅

,

,

)

(

)

)

∂

∂

∂

∂

+

)

)

∫

+

ln

ln

∫

m

dx

~
I
ij

n
⋅=

n
⋅=

,    (10.11)

cx
,
∗
j

~
)dpcpP
,

(
cxP
,
∂
c

)
(
mdxcxP

~
(
cpP
∗
c
∂
j

~
(
cpP
∂
c
i
~
ψ
∂

(
cxP
ln
,
∗
c
∂
j
(
ψ
∂
c
∂

in mutually complementing experiments (compare to (5.2) and (5.3)):
)
ln
~
( )
cI
ij

(
cx
,
ψ
∂
c
∂
i

∫
,~
(
cp
ψ
∂
c
∂
i
Note that the factor of 4 is absent in (10.12) in contrast to the similar formula (5.3). This is
( )xψ
Consider  the  following  simple  transformation  of  a  state  vector  that  is  of  vital  importance
(global gauge transformation). It is reduced to multiplying the initial state vector by arbitrary phase
factor:
) c
=′
 
c
where α  is arbitrary real number.

because of the fact that it is necessary to distinguish 

 as well as  c  and 

(
exp α
i

cp
,
∗
j

(
mn
+

( )x∗ψ

(
c
∂

, (10.13)

. (10.12)

) ij
δ

∗c .

 and 

dp

∫

=

)

)

⋅

∗

One  can  easily  verify  that  the  likelihood  function  is  invariant  against  the  gauge
transformation (10.13). This implies that the state vector can be estimated by experimental data up
to arbitrary phase factor. In other words, two state vectors that differ only in a phase factor describe
the same statistical ensemble. The gauge invariance, of course, also manifests itself in theory, e.g.,
in the gauge invariance of the Schrödinger equation.

The  variation  of  a  state  vector  that  corresponds  to  infinitesimal  gauge  transformation  is

c

evidently
i
s
        
δ
α
=
where α  is a small real number.

,...,1,0

c
 

=

j

j

j

−

1

, (10.14)

Consider  how  the  gauge  invariance  has  to  be  taken  into  account  in  considering  statistical

fluctuations of the components of a state vector. The normalization condition (

jcc

1=∗
j

) yields that

the variations of the components of a state vector satisfy the condition
cc
δ
j

. (10.15)

0=

+

∗
j

(
δ
Here, 

)
cc
j
δ

∗
j
c

= ˆ
c

j

−

c

j

j

  is  the  deviation  of  the  state  estimator  found  by  the  maximum

likelihood method from the true state vector characterizing the statistical ensemble.

In view of the gauge invariance, let us divide the variation of a state vector into two terms
c
δ
=
1
c2δ

  corresponds  to  gauge  arbitrariness,  and  the  second

 is a real physical fluctuation.

.  The  first  term 

c
δ =
1

 
c

α

+

δ

c

i

2

δ

c

one 

i

ε

An algorithm of dividing of the variation into gauge and physical terms can be represented
as follows. Let  cδ  be arbitrary variation meeting the normalization condition. Then, (10.15) yields
(
δ

, where  ε  is a small real number.

)
cc
j

=∗
j
Dividing the variation  cδ  into two parts in this way, we have
(
(
∗
δα
δ
+
j
Choosing the phase of the gauge transformation according to the condition 

)
cc
j

)
cc
j

)
cc
j

(
ci
α

. (10.16)

εα = , we find

+

=

=

=

δ

ε

∗
j

∗
j

i

i

2

2

j

27

(
)
j ccδ
2

0

=∗
j
Let us show that this gauge transformation provides minimization of the sum of squares of

. (10.17)

. Having performed infinitesimal gauge transformation,

i

ε

c

=∗
j

)
cc
j

−=′
j

variation absolute values. Let (
δ
we get the new variation
ci
δ
α
Our aim is to minimize the following expression:
′
cc
δδ
δ
+
=
j
Evidently, the last expression has a minimum at 

(
∗
−=′
j

)(
ci
α

. (10.18)

ci
α

+

+

)

δ

δ

c

∗
j

∗
j

j

j

j

j

variation achieves two aims.

c

∗
cc
δδ
j
j
εα = .
Thus,  the  gauge  transformation  providing  separation  of  the  physical  fluctuation  from  the

2 →+
α

. (10.19)

min

2
εα

−

c

First, the condition (10.15) is divided into two independent conditions:
)
(
j ccδ
Here, we have dropped the subscript 2 assuming that the state vector variation is a physical

)
∗
j ccδ

 and (

      (10.20).

0=∗
j

0=

j

fluctuation free of the gauge component).

Second, this transformation results in mean square minimization of possible variations of a

Let  cδ   be  a  column  vector,  then  the  Hermitian  conjugate  value 
fluctuations 

are  determined  by 

the 

+cδ
the  quadratic 

  is  a  row  vector.
form

.  In  order  to  switch  to  independent  variables,  we  will  explicitly  express

(as  in  Sec.  5)  a  zero  component  in  terms  of  the  others.  According  to  (10.20),  we  have

δ

c
0

−=

. This leads us to 

∗ =
cc
δδ
0
0

∗
cc
δδ
j
i

. The quadratic form under consideration can

state vector.

Statistical  properties  of 
1
−
∑

~
+
cIc
δ

∗
cc
δδ
j
i

~
I
ij

=

δ

s

ji
,

=

0

j

∗
cc
jδ
∗
c
0

s

1
−

∑

~
I
ij

∗ =
cc
δδ
j
i

I

∗
cc
δδ
j
i

ij

, where the true Fisher information matrix

∗
j
2

cc
i

c
0

s

1
−

∑

ji
,

=
1

be represented in the form 

,
ji

=
0
has the form (compare to (5.4))


)





          



(
mn
+

cc
i

*
j
2

=

+

δ

c

I

ij

ij

0

where

c
0

=

1

−

sc
−++
1

...

. (10.22)

(

c

2

2

)2

i
  

,

j

=

,...,1

s

−

1

, (10.21)

The inversion of the Fisher matrix yields the truncated covariance matrix (without zero component).
Having  calculated  covariations  with  zero  components  in  an  explicit  form,  we  finally  find  the
expression for the total covariance matrix that is similar to (5.15):
)∗
cc
i

∗
cc
δδ
j
i

. (10.23)

,...,1,0

=Σ
ij

(
δ

=

−

−

=

1

 ,

s

  

i

j

ij

j

1
(
)
mn
+

The Fisher information matrix and covariance matrix are Hermitian. It is easy to see that the

covariance matrix (10.23) satisfies the condition similar to (5.17):
ijc
Σ j

. (10.24)

0=

28

By full analogy with the reasoning of Sec. 5, it is readily seen that the matrix (10.23) is the
only (up to a factor) Hermitian tensor of the second order that can be constructed from a state vector
satisfying the normalization condition.

=Σ

The formula (10.23) can be evidently written in the form
1
(
)
mn
+
where  E  is the 

ss ×  unit matrix, and  ρ  is the density matrix.

, (10.25)

)ρ−

E

(

In the diagonal representation

+
=Σ UDU , (10.26)
where U  is the unitary matrix, and  D  is the diagonal matrix.

The diagonal of the  D matrix has the only zero element (the corresponding eigenvector is
1   (the  corresponding  eigenvectors

the  state  vector).  The  other  diagonal  elements  are  equal  to 

mn +

and their linear combinations form subspace that is orthogonal complement to a state vector).

The chi-square criterion determining whether the scalar product between the estimated and

( )0∗cc
2 0
( )
∗
cc

true vectors 
 −
(
)
mn
1
+


This method is illustrated in Fig. 5. In this figure, the density estimator is compared to the

 is close to unity that is similar to (6.1) is

=


χ . (10.27)

2
s
1
−

true densities in coordinate and momentum spaces.

Fig. 5. Comparison between density estimators and 
true densities in (a) coordinate and (b) momentum spaces

a)

Estimated density

True density

Estimated density

True density

)
x
(
P

)
p
(
P

0,0

-3

b)

0,7

0,6

0,5

0,4

0,3

0,2

0,1

0,7

0,6

0,5

0,4

0,3

0,2

0,1

0,0

-3

0

x

0

p

-2

-1

1

2

3

-2

-1

1

2

3

29

The  sample  of  a  size  of 

= mn

=

200

  (

+ mn

=

400

)  was  taken  from  a  statistical

ensemble of harmonic oscillators with a state vector with three nonzero components (

3=s

).

11.  Constraint on the Energy

As it has been already noted, the estimation of a state vector is associated with the problem
of  suppressing  high  frequency  terms  in  the  Fourier  series.  In  this  section,  we  consider  the
regularization  method  based  on  the  law  of  conservation  of  energy.  Consider  an  ensemble  of
harmonic oscillators (although formal expressions are written in general case). Taking into account
the normalization condition without any constraints on the energy, the terms may appear that make
a  negligible  contribution  to  the  norm  but  arbitrary  high  contribution  to  the  energy.  In  order  to
suppress  these  terms,  we  propose  to  introduce  both  constraints  on  the  norm  and  energy  in  the
maximum  likelihood  method.  The  energy  is  readily  estimated  by  the  data  of  mutually
complementing experiments.

It  is  worth  noting  that  in  the  case  of  potentials  with  a  finite  number  of  discrete  levels  in
quantum  mechanics  [29,  31],  the  problem  of  truncating  the  series  does  not  arise  (if  solutions
bounded at infinity are considered).

We  assume  that  the  psi  function  is  expanded  in  a  series  in  terms  of  eigenfunctions  of  the

energy operator  Hˆ  (Hamiltonian):
( )
x

( )
x

ψ

=

c
ϕ
i

, (11.1)

∑

1
−

s

i

i

0
=

=

ϕ
E
i

where basis functions satisfy the equation
ˆ
( )
( )x
ϕ
xH
i
iE  is the energy level corresponding to  i -th state.
The mean energy corresponding to a statistical ensemble with a wave function 

. (11.2)

Here, 

i

( )xψ  is

E

=

∫
ψ

∗

ˆ
( )
dxxHx
ψ

( ) ∑
=

∗
i ccE
i
i

. (11.3)

s

1
−

i

0
=

i

i

s

ij

ˆ

=

=

ϕ

E

H

, where 

∫ ∗
ϕ

∗
ij ccH
j
i

( )
dxxHx
j

In arbitrary basis
1
−
∑
0
=
Consider a problem of finding a maximum likelihood estimator of a state vector in view of a
constraint  on  the  energy  and  norm  of  the  state  vector.  In  energy  basis,  the  problem  is  reduced  to
maximization of the following functional:
(
∗
cc
S
−
i
i
2λ  are the Lagrange multipliers and  Lln  is given by (10.7).

ln
L
λ
−
1
1λ  and 

(
∗
ccE
i
i
i

)
1
−−

. (11.4)

, (11.5)

)E

( )

=

λ

2

where 
In this case, the likelihood equation has the form
) i
cR
cE
i
ij
where the  R  matrix is determined by (10.9).

(
λ +
1 λ

, (11.6)

=

2

j

In arbitrary basis, the variational functional and the likelihood equation have the forms
ln

(
∗
ccH
i
ij
j

)E

, (11.7)

L

−

λ

−

2

=

S
(
R
ij

−

λ

2

(
∗
cc
λ
i
i
1
)
cH
=
ij

j

)
1
−−
c
λ
1
i

. (11.8)

30

Having  multiplied  the  both  parts  of  (11.6)  (or  (11.8))  by 

obtain the same result representing the relationship between the Lagrange multipliers:
(
+
mn

. (11.9)

λ +
1 λ

E

=

)

2

∗
ic   and  summed  over  i ,  we

The  sample  mean  energy  E   (i.e.,  the  sum  of  the  mean  potential  energy  that  can  be
measured in the coordinate space and the mean kinetic energy measured in the momentum space)
was used as the estimator of the mean energy in numerical simulations below.

Now, let us turn to the study of statistical fluctuations of a state vector in the problem under
consideration.  We  restrict  our  consideration  to  the  energy  representation  in  the  case  when  the
expansion basis is formed by stationary energy states (that are assumed to be nondegenerate).

Additional  condition  (11.3)  related  to  the  conservation  of  energy  results  in  the  following

relationship between the components

E
δ

=

s

1
−

∑

(
∗
ccE
δ
j
j

j

+

∗
ccE
δ
j
j

j

)

=

0

. (11.10)

0
=

j
It  turns  out  that  both  parts  of  the  equality  can  be  reduced  to  zero  independently  if  one
assumes that a state vector to be estimated involves a time uncertainty, i.e., may differ from the true
one by a small time translation. The possibility of such a translation is related  to  the time-energy
uncertainty relation.

The well-known expansion of the psi function in terms of stationary energy states, in view

of time dependence, has the form (
t
ψ
−

(
exp
−

( )
x

iE

(
t

=

c

j

j

∑

1=h
)
)
ϕ

)
( )
x

j

0

=

j

=

∑

c

j

j

(
iE
exp

t

j

0

)

(
−
exp

iE

t

j

)
ϕ

( )
x

j

(11.11)

j

c

)0

(
exp
iE

In the case of estimating the state vector up to translation in time, the transformation
j =′
c
related to arbitrariness of zero-time reference  0t  may be used to fit the estimated state vector to the
true one.

 (11.12)

t

j

The corresponding infinitesimal time translation leads us to the following variation of a state

vector:
c
0=δ
j

j

. (11.13)

conservation. Then, from (10.15) and (11.3) it follows that

cEit
j
Let  cδ  be  any variation  meeting both the normalization  condition  and  the  law  of  energy
(
)
cc
δ
j
)
(
cEc
δ
j

, (11.14)

∑

i
2ε

i
1ε

=

=

∗
j

∗
j

j

j

, (11.15)

∑

j

where  1ε  and 

2ε  are arbitrary small real numbers.

In  analogy  with  Sec.  10,  we  divide  the  total  variation  cδ   into  unavoidable  physical

fluctuation 

 and variations caused by the gauge and time invariances:

c2δ

31

c
δ

j

=

ci
α

+

cEit
j
0

j

+

δ

c

2

j

j

. (11.16)

We  will  separate  out  the  physical  variation 

,  so  that  it  fulfills  the  conditions  (11.14)
and (11.15) with a zero right part. It is possible if the transformation parameters α   and   0t  satisfy
the following set of linear equations:
ε
1

  
 
tE
+

   
α

=

0

c2δ

   

E

α

. (11.17)

2
tE

ε

=

+
The determinant of (11.17) is the energy variance

2

0






2
EE
=σ

2

2

−

E

.   (11.18)

We  assume  that  the  energy  variance  is  a  positive  number.  Then,  there  exists  a  unique
solution of the set (11.17). If the energy dissipation is equal to zero, the state vector has the only
nonzero component. In this case, the gauge transformation and time translation are dependent, since
they are reduced to a simple phase shift.

In  full  analogy  with  the  reasoning  on  the  gauge  invariance,  one  can  show  that  in  view  of
both  the  gauge  invariance  and  time  homogeneity,  the  transformation  satisfying  (11.17)  provides
minimization  of  the  total  variance  of  the  variations  (sum  of  squares  of  the  components  absolute
values).  Thus,  one  may  infer  that  physical  fluctuations  are  minimum  possible  fluctuations
compatible with the conservation of norm and energy.

Assuming that the total variations are reduced to the physical ones, we assume hereafter that

The  relationships  found  yield  (in  analogy  with  Sec.  10)  the  conditions  for  the  covariance
=Σ
ij

∗
j

:

Consider the unitary matrix 

+U  with the following two rows (zero and first):

       

j

=

,...,1,0

s

−

1

. (11.24)

This matrix determines the transition to principle components of the variation

According to (11.19) and (11.20), we have 

 identically in new variables so

f
δ

0

= f
δ
1

=

0

that there remain only 

 independent degrees of freedom.

2−s

The inverse transformation is

32

∗
j

)
(
j ccδ
)
(
cEcδ
j
j

∑

∑

j

j

0=

, (11.19)

∗
j

0=

. (11.20)

cc δδ
i
) 0=

ijc

j

cE
j

j

ij

) 0=

matrix 

(
Σ∑
(
Σ∑

j

j

, (11.21)

. (11.22)

+

(
)
U 0
)
(
U

+

1

j

j

=

∗
c
= j
(
E
−

j

, (11.23)
)
cE

∗
j

σ

E

+
cU
δ =
ij
j

f
δ

i

. (11.25)

fU

δ =

c
δ

. (11.26)

On account of the fact that 

, one may drop two columns (zero and first) in

f
δ

0

= f
δ
1

=

0

=

c
δ
i

the U  matrix turning it into the factor loadings matrix  L
−
fL
s
i
       
δ
ij
The  L  matrix has  s  rows and 
independent variation principle components to  s  components of the initial variation.

j
    
;1
2−s

,...,3,2

,...,1,0

. (11.27)

 columns. Therefore, it provides the transition from 

−

=

=

1

s

j

2−s

In principal components, the Fisher information matrix and covariance matrix are given by
(
mn
+

) ij
δ

I

f
ij

=

f
=Σ
ij

f
f
δδ
i

∗
j

=

ij

. (11.29)

, (11.28)
1
(
mn
+

)

δ

In  order  to  find  the  covariance  matrix  for  the  state  vector  components,  we  will  take  into
account  the  fact  that  the  factor  loadings  matrix  L   differs  form  the  unitary  matrix  U   by  the
absence of two aforementioned columns, and hence,
(
E
i

E

−

−

)

j

+
LL
kj
ik

=

δ

ij

−

cc
i

∗
j

−

cc
i

∗
j

, (11.30)

)(
EE
2σ
E

=Σ
ij

∗
j

=

cc
δδ
i

∗
fLL
jr
ik

+
LL
kj
ik
mn
+
Finally, the covariance matrix in the energy representation takes the form
−

f
δδ
k

. (11.31)

∗
r

=

E

−

cc
i

∗
j

ij

i

)(
(
EEE
−
2
σ
E

j

+



1



)








 



. (11.32)

=Σ
ij

1
(
mn
+

δ

)





s
−

ji
, 

=

1

,...,1,0
It is easily verified that this matrix satisfies the conditions (11.21) and (11.22) resulting from

the conservation of norm and energy.

The mean square fluctuation of the psi function is

δψδψ

dx

=

∗

∫

∫

ϕδ
c
i

i

( )
cx

∗
ϕδ
j

∗
j

( )
dxx

=

∗
δδ
cc
i
i

=

Tr

( )
=Σ

2
s
−
mn
+

. (11.33)

The estimation of optimal number of harmonics in the Fourier series, similar to that in Sec.

7, has the form
= r
s
opt
where the parameters  r  and  f  determine the asymptotics for the sum of squares of residuals:

(
+
mnrf

, (11.34)

)

1+

( )
sQ

∞

= ∑

c
i

si
=

2

=

f
r
s

. (11.35)

The  norm  existence  implies  only  that 
1>r

oscillators with existing energy, 

0>r
. If the energy variance is defined as well, 

.  In  the  case  of  statistical  ensemble  of  harmonic

2>r

.

33

Figure 6 shows how the constraint on energy decreases high-energy noise. The momentum-
space  density  estimator  disregarding  the  constraint  on  energy  (upper  plot)  is  compared  to  that
accounting for the constraint.

Fig. 6 (a) An estimator without the constraint on energy; 
(b) An estimator with the constraint on energy. 

0,6

a)

s=3

)
p
(
P

0,5

0,4

0,3

0,2

0,1

0,0

)
p
(
P

0,6

0,5

0,4

0,3

0,2

0,1

0,0

-2

-1

1

2

b)

s=3

s=100

0

p

s=100

-2

-1

0

1

2

p
= mn

 (

=

100

50=

+ mn
 The sample of the size of 
harmonic  oscillators.  The  state  vector  of  the  ensemble  had  three  nonzero  components  (

3=s
Figure  6  shows  the  calculation  results  in  bases  involving 
  functions,
respectively. In the latter case, the number of basis functions coincided with the total sample size.
Figure  6  shows  that  in  the  case  when  the  constraint  on  energy  was  taken  into  account,  the  97
additional  noise  components  influenced  the  result  much  weaker  than  in  the  case  without  the
constraint.

) was taken from the statistical ensemble of
).

3=s

100

  and 

=s

12.  Fisher Information and Variational Principle in Quantum Mechanics

The aim of this section is to show a certain relation between the mathematical problem of
minimizing  the  energy  by  the  variational  principle  in  quantum  mechanics  and  the  problem  of
minimizing  the  Fisher  information  (more  precisely,  the  Fisher  information  on  the  translation
parameter) that may be used (and is really used) in some model problems of statistical data analysis
(for detail, see [36, 37]).

Let  us  show  that  there  exists  a  certain  analogy  between  the  Fisher  information  on  the
translation parameter and kinetic energy in quantum mechanics. This analogy results in the fact that
variational  problems  of  robust  statistics  are  mathematically  equivalent  to  the  problems  of  finding
the ground state of a stationary Schrödinger equation [36, 37].

34

Indeed, kinetic energy in quantum mechanics is (see, e.g., [29]; for simplicity, we consider

one-dimensional case):

2

h
2
m

∗

( )
x

( )
x

∂

2
ψ
x
∂

T

∫

ψ

−=

∫
Here,  m is the mass of a particle and  h  is the Planck’s constant.

. (12.1)

dx

dx

=

2

2

h
2
m

∗
ψψ
∂
∂
x
x
∂
∂

and its derivative are equal to zero at the infinity.

The last equality follows from integration by parts in view of the fact that the psi function

Assume that the psi function is real-valued and equals to the square root of the probability

. (12.2)

, and hence,

density:
( )
x =
ψ
∂ψ
x
∂

Then 

( )xp
′
p

=

2

p

(

2

)
′
p
p

2





′
p
p





2

2

2

=

=

=

T

∫

∫

dx

pdx

(
pI

h
m
8

h
m
8

h
m
8
Here,  we  have  taken  into  account  that  the  Fisher  information  on  the  translation  parameter,  by
definition, is [36]
+∞
( )
p x
'



−∞

( )
p x dx
⋅

2

 ⋅


( )
I p

. (12.4)

. (12.3)

∫

=

p

)

( )p x .
The Fisher information (12.4) is a functional with respect to the distribution density 
Let us consider the following variational problem: it is required to find a distribution density
( )p x  minimizing the Fisher information (12.4) at a given constraints on the mean value of the loss
function
= ∫
U

0UdxxpxU

( ) ( )

. (12.5)

≤

In terms of quantum mechanics, the “loss function” 

( )xU  is a potential.

The  problem  under  consideration  is  linearized  if  the  square  root  of  the  density,  i.e.,  psi
function is considered instead of the density itself. The variational problem is evidently reduced to
minimization of the following functional:

2

( )
)
′
x

⋅

dx

−

2

( )
Udxx

−

(
∫
ψλ
1
)

0

2

( )
dxx

)
1
+−

. (12.6)

S

(
ψ

+

λ

2

+∞

∞−

=

∫

(
ψ

)
(
∫
1λ  and 

( )
xU
ψ

Here, 
and the loss function, respectively.

+′′−
ψ

ψλψλ

=

U
2

1

. (12.7)

2λ  are the Lagrange multipliers providing constraints on the norm of a state vector

From the Lagrange-Euler equation it follows the equation for the psi function

The last equation turns into a stationary Schrödinger equation if one introduces the notation
1
λ

. (12.8)

λ
1
λ

=E

2

2

h
m
2

=

2

35

Minimization  of  kinetic  energy  at  a  given  constraints  on  potential  energy  is  equivalent  to
minimization of the total energy. Therefore, the solution of the corresponding problem is the ground
state for a particle in a given field. The corresponding result is well-known in quantum mechanics
as the variational principle. It is frequently used to estimate the energy of a ground state.

The  kinetic  energy  representations  in  two  different  forms  (12.1)  and  (12.3)  are  known  at

least from the works by Bohm on quantum mechanics ([38], see also [30]).

The  variational  principle  considered  here  is  employed  in  papers  on  robust  statistics
developed, among others, by Huber [36]. The aim of robust procedures is, first of all, to find such
estimators  of  distribution  parameters  (e.g.,  translation  parameters)  that  would  be  stable  against
(weakly sensible to) small deviations of a real distribution from the theoretical one. A basic model
in  this  approach  is  a  certain  given  distribution  (usually,  Gaussian  distribution)  with  few  given
outlying observations.

For example, if the estimator of the translation parameter is of the M- type (i.e., estimators
of  maximum  likelihood  type),  the  maximum  estimator  variance  (due  to  its  efficiency)  will  be
determined by minimal Fisher information characterizing the distribution in a given neighborhood
[36].

Minimization of the Fisher information shows the way to construct robust distributions. As
is seen from the definition (12.4), the Fisher information is a positive quantity making it possible to
estimate the complexity of the density curve. Indeed, the Fisher information is related to the squared
derivative  of  the  distribution  density;  therefore,  the  more  complex,  irregular,  and  oscillating  the
distribution density, the greater the Fisher information. From this point of view, the simplicity can
be achieved  by providing minimization of the Fisher  information  at  given  constraints.  The  Fisher
information may be considered as a penalty function for the irregularity of the density curve. The
introduction of such penalty functions aims at regularization of data analysis problems and is based
on  the  compromise  between  two  tendencies:  to  obtain  the  data  description  as  detailed  as  possible
using functions without fast local variations [37, 39-41].

In  the  work  by  Good  and  Gaskins  [39],  the  problem  of  minimization  of  smoothing
functional,  which  is  equal  to  the  difference  between  the  Fisher  information  and  log  likelihood
function,  is  stated  in  order  to  approximate  the  distribution  density.  The  corresponding  method  is
referred to as the maximum penalized likelihood method [40-41].

Among  all  statistical  characteristics,  the  most  popular  are  certainly  the  sample  mean
(estimation of the center of probability distribution) and sample variance (to estimate the deviation).
Assuming  that  these  are  the  only  parameters  of  interest,  let  us  find  the  simplest  distribution  (in
terms of the Fisher information). The corresponding variational problem is evidently equivalent to
the  problem  of  finding  the  minimum  energy  solution  of  the  Schrödinger  equation  (12.7)  with  a
quadratic  potential.  The  corresponding  solution  (density  of  the  ground  state  of  a  harmonic
oscillator) is the Gaussian distribution.

If  the  median  and  quartiles  are  used  as  a  given  parameters  instead  of  sample  mean  and
variance,  which  are  very  sensitive  to  outlying  observations,  the  family  of  distributions  that  are
nonparametric  analogue  of  Gaussian  distribution  and  accounting  for  possible  data  asymmetry  can
be found [37].

Conclusions

Let us state a short summary.
The  root  density  estimator  is  based  on  the  representation  of  the  probability  density  as  a
squared absolute value of a certain function, which is referred to as a psi function in analogy with
quantum  mechanics.  The  method  proposed  is  an  efficient  tool  to  solve  the  basic  problem  of
statistical data analysis, i.e., estimation of distribution density on the basis of experimental data.

The coefficients of the psi-function expansion in terms of orthonormal set of functions are
estimated  by  the  maximum  likelihood  method  providing  optimal  asymptotic  properties  of  the
method (asymptotic unbiasedness, consistency, and asymptotic efficiency). An optimal number of
harmonics in the expansion is appropriate to choose, on the basis of the compromise, between two

36

opposite tendencies: the accuracy of the estimation of the function approximated by a finite series
increases with increasing number of harmonics, however, the statistical noise level also increases.

The  likelihood  equation  in  the  root  density  estimator  method  has  a  simple  quasilinear
structure and admits developing an effective fast-converging iteration procedure even in the case of
multiparametric  problems.  It  is  shown  that  an  optimal  value  of  the  iteration  parameter  should  be
found  by  the  maximin  strategy.  The  numerical  implementation  of  the  proposed  algorithm  is
considered by the use of the set of Chebyshev-Hermite functions as a basis set of functions.

The introduction of the psi function allows one to represent the Fisher information matrix as
well as statistical properties of the sate vector estimator in simple analytical forms. Basic objects of
the  theory  (state  vectors,  information  and  covariance  matrices  etc.)  become  simple  geometrical
objects in the Hilbert space that are invariant with respect to unitary (orthogonal) transformations.

A  new  statistical  characteristic,  a  confidence  cone,  is  introduced  instead  of  a  standard
confidence  interval.  The  chi-square  test  is  considered  to  test  the  hypotheses  that  the  estimated
vector equals to the state vector of general population and that both samples are homogeneous.

It is shown that it is convenient to analyze the sample populations (both homogeneous and

inhomogeneous) using the density matrix.

The root density estimator may be applied to analyze the results of experiments with micro
objects as a natural instrument to solve the inverse problem of quantum mechanics: estimation of
psi  function  by  the  results  of  mutually  complementing  (according  to  Bohr)  experiments.
Generalization  of  the  maximum  likelihood  principle  to  the  case  of  statistical  analysis  of  mutually
complementing  experiments  is  proposed.  The  principle  of  complementarity  makes  it  possible  to
interpret the ill-posedness of the classical inverse problem of probability theory as a consequence of
the lacking of the information from canonically conjugate probabilistic space.

The  Fisher  information  matrix  and  covariance  matrix  are  considered  for  a  quantum
statistical ensemble. It is shown that the constraints on the norm and energy are related to the gauge
and time translation invariances. The constraint on the energy is shown to result in the suppression
of high-frequency noise in a state vector approximated.

The  analogy  between  the  variational  method  in  quantum  mechanics  and  certain  model

problems of mathematical statistics is shown.

References

D.C. 1977 .

York. 1985.

1.  A. N. Tikhonov and V. A. Arsenin. Solutions of ill-posed problems. W.H. Winston. Washington

2.  L. Devroye and L. Györfi. Nonparametric Density Estimation: The 

1L -View. John Wiley. New

3.  V.N. Vapnik and A.R. Stefanyuk. Nonparametric methods for reconstructing probability densities

Avtomatika i Telemekhanika 1978. Vol. 39. No. 8. P.38-52.

4.  V. N. Vapnik, T. G. Glazkova, V. A. Koscheev et al. Algorithms for dependencies estimations.

Nauka. Moscow. 1984 (in Russian).

5.    Yu.  I.  Bogdanov,  N.  A.  Bogdanova,  S.  I.  Zemtsovskii  et  al.  Statistical  study  of  the  time-to-
failure of the gate dielectric under electrical stress conditions. Microelectronics. 1994. V. 23. N
1. P. 51 – 59. Translated from  Mikroelektronika. 1994. V. 23. N1. P. 75-85.

6.   Yu. I. Bogdanov, N. A. Bogdanova, S. I. Zemtsovskii Statistical modeling and analysis of data
on time dependent breakdown in thin dielectric layers, Radiotekhnika i Electronika. 1995. N.12.
P. 1874-1882.

7.  M.  Rosenblatt  Remarks  on  some  nonparametric  estimates  of  a  density  function  //  Ann.  Math.

Statist. 1956. V.27. N3. P.832-837.

37

8.   E. Parzen On the estimation of a probability density function and mode // Ann. Math. Statist.

1962. V.33. N3. P.1065-1076.

9.  E. A. Nadaraya On Nonparametric Estimators of Probability Density and Regression, Teoriya

Veroyatnostei i ee Primeneniya. 1965. V. 10. N. 1. P. 199-203.

10.   E.  A.  Nadaraya    Nonparametric  Estimation  of  Probability  Densities  and  Regression  Curves.

Kluwer Academic Publishers. Boston. 1989.

11. J.S.  Marron  An  asymptotically  efficient  solution  to  the  bandwidth  problem  of  kernel  density

estimation. // Ann. Statist. 1985. V.13. №3. P.1011-1023.

12.   J.S. Marron A Comparison of cross-validation techniques in density estimation // Ann. Statist.

1987. V.15. №1. P.152-162.

13. B.U.  Park,  J.S.  Marron  Comparison  of  data-driven  bandwidth  selectors  //  J.  Amer.  Statist.

Assoc. 1990. V.85. №409. P.66-72.

14.   S.J. Sheather, M.C. Jones A reliable data-based bandwidth selection method for kernel density

estimation // J. Roy. Statist. Soc. B. 1991. V.53. №3. P.683-690.

15.   A.  I.  Orlov  Kernel  Density  Estimators  in  Arbitrary  Spaces.  in:  Statistical  Methods  for

Estimation and Testing Hypotheses. P. 68-75. Perm'. 1996 (in Russian).

16.   A.  I.  Orlov  Statistics  of  Nonnumerical  Objects.  Zavodskaya  Laboratoriya.  Diagnostika

Materialov. 1990. V. 56. N. 3. P. 76-83.

17.  N. N. Chentsov (Čensov)  Evaluation of unknown distribution  density based on observations.

Doklady. 1962. V. 3. P.1559 - 1562.

18. N.  N.  Chentsov  (Čensov)  Statistical  Decision  Rules  and  Optimal  Inference.  Translations  of
Mathematical  Monographs.  American  Mathematical  Society.  Providence.  1982  (Translated
from Russian Edition. Nauka. Moscow. 1972).

19.  G.S. Watson Density estimation by orthogonal series. Ann. Math. Statist. 1969. V.40. P.1496-

20.   G.  Walter    Properties  of  hermite  series  estimation  of  probability  density.  Ann.  Statist.  1977.

21.  G. Walter, J. Blum Probability density estimation using delta sequences // Ann. Statist. 1979.

22.  H. Cramer Mathematical Methods of Statistics, Princeton University Press, Princeton, 1946.

23. A.  V.  Kryanev  Application  of  Modern  Methods  of  Parametric  and  Nonparametric  Statistics  in

Experimental Data Processing on Computers, MIPhI, Moscow, 1987 (in Russian).

24.  R.A. Fisher On an absolute criterion for fitting frequency curves // Massager of  Mathematics.

1912. V.41.P.155-160.

25.   R.A.  Fisher  On  mathematical  foundation  of  theoretical  statistics  //  Phil.  Trans.  Roy.  Soc.

(London). Ser. A. 1922. V.222. P. 309 – 369.

26.  M. Kendall and A. Stuart The Advanced Theory of Statistics. Inference and Relationship.  U.K.

Charles Griffin. London. 1979.

1498.

V.5. N6. P.1258-1264.

V.7. №2. P. 328-340.

38

1982 (in Russian).

Berlin. 1985.

Russian).

27.   I.  A.  Ibragimov  and  R.  Z.  Has'minskii  Statistical  Estimation:  Asymptotic  Theory.  Springer.

New York. 1981.

28.  S. A. Aivazyan and I. S. Enyukov, and L. D. Meshalkin Applied Statistics: Bases of Modelling

and Initial Data Processing. Finansy i Statistika. Moscow. 1983 (in Russian).

29.   L.  D.  Landau  and  E.  M.  Lifschitz  Quantum  Mechanics  (Non-Relativistic  Theory).  3rd  ed.

Pergamon Press. Oxford. 1991.

30.  D. I. Blokhintsev Principles of Quantum Mechanics, Allyn & Bacon, Boston, 1964.

31.  V. V. Balashov and V. K. Dolinov.  Quantum  mechanics.  Moscow University Press.  Moscow.

32.   A.  N.  Tikhonov,  A.  B.  Vasil`eva,  A.  G.  Sveshnikov  Differential  Equations.  Springer-Verlag.

33.  N. S. Bakhvalov, N.P. Zhidkov, G. M. Kobel'kov Numerical Methods. Nauka. Moscow. 1987 (in

34.  N. N. Kalitkin Numerical Methods. Nauka. Moscow. 1978 (in Russian).

35.  N. Bohr Selected Scientific Papers in Two Volumes. Nauka. Moscow. 1971 (in Russian).

36.  P. J. Huber Robust statistics. Wiley. New York. 1981.

37.   Yu.  I.  Bogdanov  Fisher  Information  and  a  Nonparametric  Approximation  of  the  Distribution
Density//  Industrial  Laboratory.  Diagnostics  of  Materials.  1998.  V.  64.  N  7.  P.  472-477.
Translated from Zavodskaya Laboratoriya. Diagnostika Materialov. 1998. V. 64. N. 7. P. 54-60.

38.  D. Bohm A suggested interpretation of the quantum theory in terms of  “hidden” variables. Part

I and II //  Phys. Rev. 1952. V.85. P.166-179 and 180-193

39.     I.J.  Good,  R.A.  Gaskins  Nonparametric  roughness  penalties  for  probability  densities  //

Biometrica. 1971. V.58. №2. P. 255-277.

40.   C. Gu, C. Qiu Smoothing spline density estimation: Theory. // Ann. Statist. 1993. V. 21. №1.

41.   P.  Green  Penalized  likelihood  //  in  Encyclopedia  of  Statistical  Sciences.  Update  V.2.  John

P. 217 – 234.

Wiley. 1998.

About the Author

Yurii Ivanovich Bogdanov

Graduated  with  honours  from  the  Physics  Department  of  Moscow  State  University  in  1986.
Finished  his  post-graduate  work  at  the  same  department  in  1989.  Received  his  PhD  Degree  in
physics and mathematics in 1990. Scientific interests include statistical methods in fundamental and
engineering researches. Author of more than 40 scientific publications (free electron lasers, applied
statistics,  statistical  modeling  for  semiconductor  manufacture).  At  present  he  is  the  head  of  the
Statistical Methods Laboratory (OAO “Angstrem”, Moscow).

e-mail: bogdanov@angstrem.ru

39


F.V.Tkachov  

2000-Jan-09  17:21

Page 1 of 4

APPROACHING THE PARAMETER ESTIMATION QUALITY
OF MAXIMUM LIKELIHOOD VIA GENERALIZED MOMENTS

Fyodor  V. Tkachov
Institute for Nuclear Research
of Russian Academy of Sciences
Moscow 117312 Russia

A simple criterion is presented for a practical  construction  of  generalized  moments  that  allow  one  to  ap-
proach  the  theoretical  Rao-Cramer  limit  for  parameter  estimation  while  avoiding  the  complexity  of  the
maximum likelihood method in the cases  of  complicated  probability  distributions  and/or  very  large  event
samples.

0
0
0
2

 

n
a
J
 

9

 
 
 

9
1
0
1
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

IN T R O D U C T IO N.   The purpose of this note is to describe a
result  that  was  discovered  in  a  rather  special  context  of  the
theory  of  so-called  jet  finding  algorithms  [1]  but  seems  to  be
basic  enough  to  belong  to  the  core  statistical  wisdom  of  pa-
rameter estimation.

Namely, I would like to present a simple formula (Eq. (20))
that  connects  the  method  of  generalized  moments  with  the
maximum  likelihood  method  by  explicitly  describing  devia-
tions from the Rao-Cramer limit on precision of parameter es-
timation with a given event sample; see e.g. [2], [3].

The formula leads to practical prescriptions (the method  of
quasi-optimal moments a; see after Eq. (24)) that offer a practi-
cal alternative to the maximum likelihood method in precision
measurement  problems  when  the  use  of  the  maximum  likeli-
hood  method  is  impractical  due  to  complexity  of  theoretical
expressions  for  the  probability  distribution  or  a  large  size  of
the sample of events.

Although  closely  related  to  the  well-known  results  and
mathematical techniques, the prescription is new to the extent
that  I’ve  seen  no  trace  in  the  literature  of  its  being  known  to
physicists  despite  its  immediate  relevance  to  precision  meas-
urements.

 T H E   P R O B L E M.  One  deals  with  a  random  variable  P
whose  instances  (specific  values)  are  called  events.  Their
probability density is denoted as p (P). It is assumed to depend
on  a  parameter  M   which  has  to  be  estimated  from  an  experi-
mental sample of events {Pi }i .

 The  standard  method  of  generalized  moments  consists  in
choosing  a  function  f (P)  defined  on  events  (the  generalized
moment), and then finding M  by fitting its theoretical average
value,
  f

P P
( )

P
( ) , 

(1)

p

f

= z d

 against the corresponding experimental value:

O P T IMA L   MO ME N T S.   In  the  context  of  precision  meas-
urements one can assume the magnitude of errors to be small.
Then fluctuations in the values of M  are related to fluctuations
in the values of 
- 1
I
d
KJ

= ¶
F
HG

f
M

(3)

 as

M

 d

f

f

.

 The derivative is applied only to the probability distribution:

 

f
M

=

z

dP P
( )

f

p

P
( )
M

.

(4)

 This  is  because  M   is  unknown,  so  even  though  the  solution,
f opt,  will  depend  on  M ,  any  such  dependence  is  coincidental
and therefore “frozen” in this calculation.

 For small fluctuations  d

=

- 1 2/ Var

N

f

f

, where

 Var

f

=

z d

p

P P
)

(

b

f

P
( )

f

g2

2

f

f

2 .

 In terms of variances, Eq. (3) becomes:

 Var

[
M f

]

= ¶
F
HG

f
M

I
KJ

- 2

Var

f

.

(5)

(6)

 The problem is to minimized this by a suitable choice of f .

 A  necessary  condition  for  a  minimum  can  be  written  in

terms of functional derivatives:b

d
d f

 

Var

[
M f

]

= 0 .

P
( )

(7)

 Substitute Eq. (6) into (7) and use the following relations:

  f

exp

=

(cid:229)1
N

f

(

)P .
i

i

(2)

The problem is to find f  which would allow  one  to  extract  M
with the highest precision from the event sample.

                                                            
a  In  the  quantum-theoretic  context  of  [1]  generalized  moments  are  natu-
rally  interpreted  as  quantum  observables,  so  the  method  was  called  the
method of quasi-optimal observables.

                                                            
b An interesting mathematical exercise of casting the  following  reasoning
(the  functional  derivatives,  etc.)  into  a  rigorous  form  is  left  to  interested
mathematical parties. A premature emphasis on rigor would have obscured
the simple analogy with the study of minima of ordinary functions via the
usual Taylor expansion.
    For practical purposes it is sufficient to remember that the range of va-
lidity  of  the  prescriptions  we  obtain  is  practically  the  same  as  for  the
maximum  likelihood  method.  Note  that  the  derivation  in  terms  of  func-
tional derivatives can be related to the proofs of the Rao-Cramer inequality
in terms of Hilbert statistics, etc.; cf. e.g. [4].

¶
¶
¶
¶
¶
-
”
-
¶
 After some simple algebra one obtains:

  f

P
( )

=

f

+

const

P
ln ( )

p
M

,

(9)

 where the constant is independent of P. The constant plays no
role since f  is defined by  this  reasoning  only  up  to  a  constant
factor. Noticing that

  d

P P
( )

p

z

)

= ¶

P

p
ln (
M

M

z

p

P P
( )

d

=

1 0 ,

M

 we arrive at the following general family of solutions:

  f

P
( )

=

C
1

P
ln ( )

p
M

+

C
2

,

(10)

(11)

 where C i  are independent of P but may depend on M .

 For convenience of formal investigation we will usually deal

with the following member of the family (11):

f

P
opt ( )

= ¶

P
ln ( )

p
M

.

 Then Eq. (10) is essentially the same as
  fopt = 0 .

This agrees with (12) thanks to (13).

D E V IA T IO N S   F R O M  f opt.   Next  we  are  going  to  consider
how small deviations from f opt affect the precision of extracted
M . Consider (6) as a functional of  f , Var M [f ]. Assume j  is a
function of events such that 
. We are going to evalu-
ate the functional Taylor expansion of Var M [f opt+j ] with re-
spect to j  through quadratic terms:
Var

j 2 < ¥

[
M f

+

]

j

opt

[
M f
L
d
NM
d

z

opt

2

f

Var
d
P
( )

=
] Var
O
QP =

[
]
M f
Q
)
(
f

f

+

1
2

j
P Q P Q K
( )

) d d

+

(

(18)

j

f

opt

 The term which is linear in j  does not occur because f opt satis-
fies (7).

To evaluate the quadratic term in (18), it is sufficient to use

functional derivatives and relations such as (8) and

f

(

Q

)

=

d

P Q
( ,

),

P
( )

z
d

P Q P P
( ,

( )

d

)

j

j
=

(

Q

) .

(19)

d
f

 A straightforward calculation yields our main technical result:

(12)

d

(13)

Var

+

opt
+

[
M f
1
2
opt

f

j

f

]
1
2
opt

{

3

f

2
opt

j

2

j

+
2

f

opt

} K (20)

F.V.Tkachov  

d
f
d
f

P
( )

P
( )

d

 

d

=

f

f
M

p
P
( ) ,
= ¶

p

P
( )
M

2000-Jan-09  17:21

Page 2 of 4

2

f

=

2

f

P
( )

p

P
( ) ,

d
f

P
( )

d

.

where  summation  runs  over  all  events  from  the  sample.  The
necessary condition for the maximum of (16) is

(8)

M

i

p

ln (

P
i

)

=

P
i

)

p
ln (
M

i

f

opt exp

=

0 .

(17)

 A  S IMP L E   E X A MP L E.   Consider the familiar Breit-Wigner

shape. Let P be random real numbers distributed according to

=

      

 p ( )
P

1
P
)

+
2

2

(

M

(14)

in some fixed interval around P = M . Suppose M  is unknown.
Then the optimal moment is

  f

opt P
( )

M ,

= ¶

M

p

P
ln ( )

= -

(

2
M

M
P
)

(

P
)
2G
+
2

.

(15)

(Remember  that  P-independent  additive  and  multiplicative
constants can be dropped in such expressions; see Eq. (11).)

 It is interesting  to  observe  how  f M ,opt  emphasizes  contribu-
tions of the slopes of the bump — exactly where the magnitude
of p (P) is most sensitive to variations of M  — and taking con-
tributions from the two slopes with a different sign maximizes
the  signal.  At  the  same  time  the  expression  (15)  suppresses
contributions from the middle part of the bump  (14)  that  gen-
erates mostly noise as far as M  is concerned.

C O N N E C T IO N   W IT H   MA X IMU M  L IK E L IH O O D.   Eq. (12)
can  be  regarded  as  a  translation  of  the  method  of  maximum
likelihood (which is known to yield the theoretically best esti-
mate  for  M ;  cf.  the  Rao-Cramer  inequality  [2],  [3])  into  the
language of generalized moments.c Indeed, the maximum like-
lihood  method  prescribes  to  estimate  M   by  the  value  which
maximizes the likelihood function,

p P ,
)

ln (

i

i

(16)

where  j

j=
j

.

 Non-negativity of the factor in curly braces follows from the

standard Schwartz inequality.d

2
fopt

The first term on the r.h.s. of (20), 

- 1 , is the absolute
minimum  for  the  variance  of  M  as  established  by  the  Rao-
Cramer  inequality  [2],  [3].  The  latter  is  valid  for  all  j   and
therefore  is  somewhat  stronger  than  the  result  (20)  which  we
have obtained only for sufficiently small j . However, Eq. (20)
gives a  simple  explicit  description  of  the  deviation  from  opti-
mality  and  so  makes  possible  the  practical  prescriptions  pre-
sented below after Eq. (24).

It is convenient to talk about  informativeness  I f  of a gener-

alized moment  f  with respect to the parameter M , defined by

=

b

I

f

Var

[
M f

g 1
]

.

The informativeness of  f opt is

=

I

opt

f

2
opt

,

(21)

(22)

which corresponds to the Rao-Cramer limit. And the expansion
(20) explicitly describes the deviations from the limit.

Informativeness  is  closely  related  to  Fischer’s  information
[2], [3] which, however,  is  an  attribute  of  data  whereas  infor-
mativeness is a property of the moment.

                                                            
c Rather surprisingly, none of a dozen or so textbooks and monographs on
mathematical statistics that I checked (including a comprehensive practical
guide [2] and a comprehensive mathematical treatment [3]) explicitly for-
mulated  the  prescription  in  terms  of  the  method  of  moments although
equivalent formulas do occur e.g. in simple examples of specific estimates
for the parameters of standard distributions; cf. [4].

                                                            
d Note that the Schwartz inequality figures in standard rigorous proofs of
the Rao-Cramer theorem.

¶
¶
¶
¶
¶
¶
¶
¶
”
¶
¶
¶
¶
¶
(cid:181)
-
G
¶
-
-
(cid:229)
¶
¶
¶
¶
(cid:181)
(cid:229)
(cid:229)
·
-
·
-
-
F.V.Tkachov  

2000-Jan-09  17:21

Page 3 of 4

T H E   ME T H O D   O F   Q U A S I- O P T IMA L   MO ME N T S.   The
fact that the solution (12) is the point of a quadratic minimum
means that any  moment  f quasi  which  is  close  to  (12)  would  be
practically  as  good  as  the  optimal  solution  (we  will  call  such
moments  quasi-optimal).  A  quantitative  measure  of  closeness
is given by comparing the O(1) and O(j 2) terms on the  r.h.s.
of (20):

f

2
opt

j

2

2
f
opt
where  j =

f

j
opt

f

2

2

<<

1

,

quasi

f

quasi

f

opt .

(23)

The  subtracted  term  in  the  numerator  of  (23)  is  non-
negative,  so  dropping  it  results  in  a  sufficient  condition  for
  would  tend  to  be  suppressed
Eq. (23).  Furthermore, 
anyway  whenever  f quasi  oscillates  around  f opt.  Assuming  with-
fquasi = 0,  we obtain the following
out loss of generality that 
convenient sufficient criterion:

fopt j

    

f

quasi

f

opt

<<2

f

2
opt

.

(24)

 Taking into account this and Eq. (20) and denoting the usual
s   for  M   for  the  optimal  and  quasi-optimal  cases  as  s opt  and
s quasi, respectively, one obtains:
2
s

.

(25)

quasi

opt

s

+

1

1
2

f

opt

f

quasi
f

2
opt

Now the method of quasi-optimal moments  is as follows:

(i) 
construct  a  generalized  moment  f quasi  using  (12)  as  a
guide  so  that  f quasi  were  close  to  f opt  in  the  integral  sense  of
Eq. (24);

(ii) 

find  M  by fitting 

fquasi

 against 

fquasi exp ;

(iii)  estimate the error for  M  via (6);
(iv)  f quasi may depend on  M  to find which one can optionally
use an iterative procedure starting from some value M0 close to
the true one.

For practical construction of quasi-optimal moments f quasi it
is  useful  to  reformulate  (24)  in  terms  of  integrands.  The  ex-
plicit form for (24) is

p

P P
d
( )

z

f

quasi

P
( )

f

opt

P
( )

2
<<

z

p
P P
( )

d

f

2
opt

P
( ) .

(26)

As a rule of thumb, one would aim to minimize the bracketed
expression on the l.h.s. of (26):

    

f

quasi

P
(

)

f

opt

P
( )

<<2

f

2
opt

P
( ) .

(27)

This  should  hold  for  “most”  P,  i.e.  taking  into  account  the
magnitude  of p (P):  the  inequality  (27)  may  be  relaxed  in  the
regions  which  yield  small  contributions  to  the  integral  on  the
l.h.s. of (26).

 T H E   E X A MP L E   (14).   Suppose the exact probability distri-
bution  differs  from  (14)  by,  say,  a  mild  but  complicated  de-
pendence of G
 on P (as seen e.g. from some sort of perturbative
calculations of  theoretical  corrections  —  a  situation  typical  of
high-energy physics problems [5]). Then the r.h.s. of (15) with
a constant G
 would correspond to a generalized moment which
is  only  quasi-optimal  but  deviations  from  optimality  may  be

practically  negligible  (depending  on  the  “mildness”  of  the  P-
dependence).  So  one  could  still  use  the  moment  given  by  the
simplest  formula  (15)  without  significant  loss  of  informative-
ness.

 Alternatively, one could replace the analytical shape (15) by
cruder piecewise constant or, better, piecewise linear approxi-
mations that would imitate the expression (15):
p ( )P

fquasi( )P

fopt( )P

PM

(a)

(b)

(c)

(d)

(28)

 In either case, the effect of non-optimality can be easily es-
timated  via  Eq. (25):  the  piecewise  linear  shape  (d)  deviates
from optimality in the sense of (25) by a few per cent (in infi-
nite  domains,  the  slowly  decreasing  tails  of  the  probability
distribution  may  spoil  this  conclusion  somewhat  so  one  may
wish to extend f quasi by  additional  linear  pieces  as  well  as  in-
sert flat linear pieces at the sharp peaks).

D IS C U S S IO N.   Eq. (27)  allows  one  to  talk  about  non-
optimality  of  moments  (i.e.  their  lower  informativeness  com-
pared  with  fopt)  in  terms  of  sources  of  non-optimality,  i.e.  the
deviations of  fquasi(P)  from  fopt(P)  which give sizeable contri-
butions to the l.h.s. of (24). The simplest example is when f opt
is  a  continuous  smoothly  varying  function  whereas  f quasi  is  a
piecewise  constant  approximation  (see  (28),  figure  (c)).  Then
f quasi would usually deviate most from f opt near the discontinui-
ties which, therefore, are naturally identified as sources of non-
optimality.  Then  a  natural  way  to  improve  f quasi  is  by
“regulating”  discontinuities  via  continuous  (e.g.  linear)  inter-
polations.

Intuitively, one could think about sources of non-optimality
as “leaks” through which information about M  is lost, and the
improvement  of  f quasi  would  then  correspond  to  patching  up
those leaks.

It  is  practically  sufficient  to  take  Eq. (12)  at  some  value
M =M 0 close to the true one (which is unknown anyway). This
is usually possible in the case of precision measurements. One
could also  perform  an  iterative  procedure  for  M   starting  from
M 0,  then  replacing  M 0  with  the  value  newly  found,  etc.  —  a
procedure closely  related  to  the  optimization  in  the  maximum
likelihood method.

If p (P)  is  given  by  a  perturbation  theory  with  increasingly
complex but decreasingly important contributions, it is possible
to use an approximate shape for the r.h.s. of (12) such as given
by  a  few  terms  of  a  perturbative  expansion  in  which  the  de-
pendence  on  the  parameter  manifests  itself.  Theoretical  up-
dates of the complete p (P) need not be always reflected in the
quasi-optimal moments.

If the dimensionality of the space of events is not large then
it may be possible to construct a suitable f quasi in a brute force
fashion, i.e. build an interpolation formula for p (P) for two or
more  values  of  M   near  the  value  of  interest,  and  perform  the
differentiation in M  numerically.

Also,  one  can  use  different  expressions  for  f quasi:  e.g.  per-
form a few first iterations with a simple shape for faster calcu-
lations  and  then  switch  to  a  more  sophisticated  interpolation
formula for best precision.

-
-
-
-
»
·
-
-
-
F.V.Tkachov  

2000-Jan-09  17:21

Page 4 of 4

A C K N O W L E D G ME N T S.     I  thank  Dima  Bardin  for  a  help
with clarifying  the  bibliographic  status  of  the  concept  of  opti-
mal moments. This work was supported in part by the Russian
Foundation for Basic Research under grant 99-02-18365.

References

[1] F.V.Tkachov,  A theory of jet definition [hep-ph/9901444, revised

January, 2000].

[2] W.T. Eadie et al., Statistical methods in experimental physics.

North-Holland, 1971.

[3] A.A.Borovkov, Mathematical statistics. Parameter estimation
and tests of hypotheses. NAUKA: Moscow, 1984 (in Russian).

[4] Yu.P. Pyt’ev and I.A. Shyshmarev, A course of the theory of prob-

ability and mathematical statistics for physicists. Moscow State
Univ.: Moscow, 1983 (in Russian).
J. Ellis and R. Peccei (eds.), Physics at LEP, CERN: Geneva, 1986.

[5]
[6] F.V. Tkachov, in: Proc. of  The V St. Petersburg School on Theo-

retical Physics, 8–12 February, 1999, Gatchina, eds.
Ya.I. Azimov et al. [hep-ph/9802307].

S E V E R A L   P A R A ME T E R S.   With several parameters  to  be
extracted  from  data  there  are  the  usual  ambiguities  due  to
reparametrizations but one can always define a moment per pa-
rameter  according  to  (12).  Then  the  informativeness  (21)  is  a
matrix (as is Fischer’s information).

Since the covariance  matrix  of  (quasi-) optimal  moments  is
known (or can be computed from data), the mapping of the cor-
responding error ellipsoids for different confidence levels from
the space of moments into the space of parameters is straight-
forward.

O P T IMA L   MO ME N T S   A N D   T H E   L E A S T   S Q U A R E S
ME T H O D .   The popular c 2 method makes a fit with a number
of non-optimal moments (bins of a histogram). The histogram-
ming implies a loss of information but the method is universal,
verifies  the  probability  distribution  as  a  whole,  and  is  imple-
mented  in  standard  software  routines.  On  the  other  hand,  the
choice  of  f quasi  requires  a  problem-specific  effort  but  then  the
loss of information can in principle be made negligible by suit-
able adjustments of  f quasi.

The balance is, as usual, between the quality of custom so-
lutions  and  the  readiness  of  universal  ones.  However,  once
quasi-optimal  moments  are  found,  the  quality  of  maximum
likelihood method seems to become available at  a  lower  com-
putational cost.

The two methods are best regarded as complementary: One
could  first  employ  the  c 2  method  to  verify  the  shape  of  the
probability distribution and obtain the value of  M 0  to  be  used
as a starting point in the method of quasi-optimal moments in
order to obtain the best final estimate for M .

An  additional  advantage  of  the  method  of  quasi-optimal
moments may be that some of the more sophisticated theoreti-
cal formalisms yield predictions for probability densities in the
form of singular (and therefore not necessarily positive-definite
everywhere)  generalized  functions  (cf.  the  systematic  gauge-
invariant  quantum-field-theoretic  perturbation  theory  with  un-
stable  particles  outlined  in  [6]).  In  such  cases  theoretical  pre-
dictions  for  generalized  moments  (quasi-optimal  or  not)  may
exceed  in  quality  predictions  for  probability  densities,  so  that
the use of the c 2 method would be somewhat disfavored com-
pared with the method of quasi-optimal moments for the high-
est-precision measurements of unknown parameters.

Note that the data processing for the LEP1 experiments [5]
has been performed in several iterations over several years and
it would have been entirely possible to design, say, five quasi-
optimal  moments  for  the  five  parameters  measured  at  the  Z
resonance back in the ‘80s and to use them ever since.

C O N C L U S IO N S.     It  is  clear  that  the  method  of  quasi-
optimal  moments  may  be  a  useful  addition  to  the  data-
processing  arsenal  e.g.  in  situations  encountered  in  precision
measurement problems in high-energy particle physics (cf. [5])
where  one  deals  with  O(106)  events  and  very  complicated
probability  distributions  obtained  via  quantum-field-theoretic
perturbation  theory  so  that  the  optimization  involved  in  the
maximum  likelihood  method  is  unfeasible.  It  also  does  not
seem  impossible  to  design  universal  software  routines  for  a
numerical construction of  f quasi in the form of dynamically gen-
erated interpolation formulas.

Lastly,  the  usefulness  of  the  concept  of  quasi-optimal  mo-
ments  is  not  limited  to  purely  numerical  situations:  It  also
proved to be useful in a theoretical context of [1] as a guiding
principle for studying an important class of data processing al-
gorithms (the so-called jet finding algorithms).


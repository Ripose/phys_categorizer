A Dynamic Theory of Information and Entropy 

Fujitsu Laboratories of Europe, Columba House, Adastral Park, Ipswich, IP5 3RE, UK. 

Michael C. Parker 

M.Parker@ftel.co.uk 

Stuart D. Walker 

Essex, CO4 3SQ, UK. 

stuwal@essex.ac.uk 

 

 

University of Essex, Department of Electronic Systems Engineering, Wivenhoe Park, Colchester, 

(Received 2nd April 2004) 

Abstract 

We propose a new thermodynamic, relativistic relationship between information and entropy, which is 

closely  analogous  to  the  classic  Maxwell  electro-magnetic  equations.  Determination  of  whether 

information  resides  in  points  of  non-analyticity  or  is  more  distributed  in  nature  therefore  relates 

directly to the well-known wave-particle duality of light. At cosmological scales our vector differential 

equations predict conservation of information in black holes, whereas regular and Z-DNA correspond 

to helical solutions at microscopic levels. We further propose that regular and Z-DNA are equivalent to 

the  alternative  words  chosen  from  an  alphabet  to  maintain  the  equilibrium  of  an  information 

transmission system. 

 

 

PACS numbers: 89.70.+c, 65.40.Gr, 42.25.Bs, 03.30.+p 

The close relationship between information and entropy is well recognised [1,2], e.g. Brillouin considered a 

negative  change  in  entropy  to  be  equivalent  to  a  change  in  information  (negentropy  [3]),  and  Landauer 

considered  the  erasure  of  information 

ID   to  be  associated  with  an  increase  in  entropy  SD ,  via  the 

relationship 

-D = D  [1,4]. In previous work, we qualitatively indicated the dynamic relationship between 

S

I

information and entropy, stating that the movement of information is accompanied by the increase of entropy 

in time [5,6], and suggesting that information propagates at the speed of light in vacuum c [5-9]. Opinions on 

the physical nature of information have tended to be contradictory. One view is that information is inherent to 

points  of  non-analyticity  (discontinuities) [5,7,9]  that  do  not  allow  ‘prediction’,  whereas  other researchers 

consider the information to be more distributed (delocalised) in nature [10-12]. Such considerations are akin 

to the paradoxes arising from a wave-particle duality, with the question of which of the two complements best 

characterises  information.  In  the  following  analysis,  by  initially  adopting  the  localised,  point  of  non-

analyticity definition of information, we ultimately find that information and entropy do indeed also exhibit 

wavelike properties, and can be described by  a pair of coupled wave equations,  analogous  to  the Maxwell 

equations for electromagnetic (EM) radiation, and a distant echo of the simple difference relationship above. 

We start our analysis by consideration of a meromorphic [13] function 

( )zy

 in a reduced 1+1 (i.e. one space 

dimension  and  one  time  dimension)  complex  space-time  z

= +

x ict

,  with  a  simple,  isolated  pole  at 

z
0

=

x
0

+

ict
0

, as indicated in figure 1: 

 

y

z
( )

=

ct
0
p

1
-

z

z
0

  

 

 

 

(1) 

The point of non-analyticity 

0z  travels at the speed of light in vacuum c [5-9]. We note that 

( )zy

acts as an 

inverse-square  law  function, e.g. as proportional to  the  field around an isolated charge, or  the gravitational 

field around a point mass. It is also square-integrable, such that 

¥

ò

-¥

r

t dx

0

=

=

1

, and 

ic

¥

ò

-¥

r

x

=

0

dt

=

1

, where 

r y=

( )z

2

,  and  obeys  the  Paley-Wiener  criterion  for  causality  [14].  We  now  calculate  the  differential 

information [5,15] (entropy) of the  function 

( )zy

. By convention, in this paper, we define the logarithmic 

integration along the spatial x-axis to yield the entropy S of the function; whereas the same integration along 

the  imaginary  t-axis  yields  the  information I  of the  function.  This  can  be qualitatively understood  to  arise 

¥

ò

-¥

¥

ò

-¥

from the entropy of a system being related to its spatial permutations (e.g. the spatial fluxions of a volume of 

gas), whereas the information of a system is related to its temporal distribution (e.g. the arrival of a data train 

of light pulses down an optical fibre.) Considering the differential entropy first, substituting equation (1) with 

0t =  and calculating the sum of  the residues of the appropriate  closed contour integral  in the z-plane, S is 

given by: 

by: 

S

=

k

r

log

r

dx

=

k

log

2

ct
0

 

2

 

 

 

(2) 

where  k  is  the  Boltzmann  constant.  We  have  defined  S  using  a  positive  sign  in  front  of  the  integral,  as 

opposed  to  the  conventional  negative  sign  [15],  so  that  both  the  entropy  and  information  of  a  space-time 

function are calculated in a unified fashion. In addition, this has the effect of aligning equation (2) with the 2nd 

Law  of  Thermodynamics,  since  S  increases  monotonically  with  increasing  time 

0t .  Performing  the 

appropriate closed contour integral in the z-plane with 

0x =  we find that the differential information is given 

I

=

k

r

log

r

2

=
cdt k

log

x
0

. 

2

 

 

 

(3) 

Note  that  in  (2)  and  (3)  some  constant  background  values  of  integration  have  been  ignored,  since  they 

disappear  in  the  following  differential  analyses.  We  see  that  the  entropy  and  information  are  given  by 

surprisingly simple expressions. The information is the logarithm of the space-time distance 

0x  of the point 

of non-analyticity from the temporal t-axis; whilst the entropy content is simply the logarithm of the distance 

(i.e. time 

0ct ) of the pole from the spatial x-axis. The overall info-entropy  S  of the function 

( )zy

 is given by 

the summation of the two orthogonal logarithmic integrals, 

S = +
I

iS

, where the entropy S is in quadrature to 

the  information I, since I  depends on  the real  axis  quantity 

0x , whereas  S  depends on  the  imaginary  axis 

quantity 

0ct . Since the choice of reference axes is arbitrary, we could have chosen an alternative set of co-

ordinate  axes  to  calculate  the  information  and  entropy.  For  example,  consider  a  co-ordinate  set  of  axes 

¢
x

¢-
ct

, rotated an angle  q  about the origin with respect to the original  x ct-

 axes. In this case, the unitary 

transformation describing the position of the pole  0z¢  in the new co-ordinate framework is: 

¢
ct
0
¢
x
0

æ
ç
è

ö
÷
ø

=

cos

q

sin

q

æ
ç
è

-

sin

q

cos

q

ö
÷
ø

æ
ç
è

ct
0

x
0

ö
÷
ø

. 

 

 

 

(4) 

Using (2) and (3), the resulting values of the information and entropy for the new frame of reference are given 

¢

I

=

k

log
2

¢
x
0

 

 (5a) 

 

 

S

¢

=

k

log

¢
ct
0

 

2

 

 

(5b) 

with overall info-entropy 

¢S  of the function again given by the summation of the two quadrature logarithmic 

integrals, 

¢
S =

¢

I

+

iS

¢

. We next perform a dynamic calculus on the equations (5), with respect to the original 

x ct-

 axes, using (4) to calculate: 

¢
¶
I
¶
x

=

k
¢
x
0

¢
¶
x
0
¶
x

=

k
¢
x
0

¢
¶
x
0
¶
x
0

k
¢
x
0

=

cos

q

 

       (6a)  

¢
¶
I
1
¶
c t

=

1
k
¢
x c
0

¢
¶
x
0
¶
t

=

¢
¶
x
k
1
0
¢
¶
x c t
0
0

k
¢
x
0

=

sin

q

 

(6b) 

¢

¶
S
1
¶
c t

=

¢
¶
t
k
1
0
¢
¶
t c t
0

=

k
¢
ct
0

¢
¶
t
0
¶
t
0

=

k
¢
ct
0

cos

q

     (7a)  

¢
¶
S
¶
x

=

k
¢
ct
0

¢
¶
t
0
¶
x

=

k
¢
ct
0

¢
¶
t
0
¶
x
0

=

-
k
¢
ct
0

sin

q

   

(7b) 

where we have ignored the common factor  log 2 , and we have assumed the equality of the calculus operators 

¶ ¶ = ¶ ¶   and 
/

x

/

x
0

¶ ¶ = ¶ ¶ ,  since  the  trajectory  of  the  pole  in  the  z-plane  is  a  straight  line  with 

t

/

/

t
0

dx dx=
0

, and 

dt dt=
0

. Since the point of non-analyticity is moving at the speed of light in vacuum c [5-9], 

0z

ict

ict¢

q

0x

0x¢

0ct¢

0ct

q

ix¢

ix

S

=

k

r

log

2

r

dx

=

k

log
2

ct
0

¥

ò

-¥

I

=

k

r

log

2

r

cdt

=

k

log

x
0

2

¥

ò

-¥

 

Figure 1: Information I (integration over imaginary ct-axis) and entropy S (integration over real x-axis) due 

to a point of non-analyticity (pole) at the space-time position z0. 

by: 

 

 

 

 

 

 

 

 

 

 

 

such that 

x
0

ct=
0

, we must also have 

¢
x
0

¢=
ct
0

, as c is the same constant in any frame of reference [16]. We 

can now see that equations (6) and (7) can be equated to yield a pair of Cauchy-Riemann equations [17]: 

¶
¶

I
x

=

¶
S
1
¶
c t

 

 

(8a) 

 

 

 

(8b) 

¶
S
¶
x

= -

¶
I
1
¶
c t

 

where we have dropped the primes, since equations (8) are true for all frames of reference. Hence, the info-

entropy  function  S   is  analytic  (i.e.  holographic  in  nature  [13]),  and  the  Cauchy-Riemann  equations  (8) 

indicate that information and entropy propagate as waves travelling at the speed c. In the Appendix we extend 

the analysis from 1+1 complex space-time to the full 3+1 (three-space and one-time dimensions) case, so that 

it can be straightforwardly shown that equations (8) generalise to: 

Ñ ´ =
I

 

(9a) 

 

 

Ñ ´ =
S

 

 

(9b) 

1 dS
c dt

 

-
1 dI
c dt

 

 

 

 

where  I  and  S  are in 3D vector form. Further basic calculus manipulations reveal that the scalar gradient 

(divergence) of the information and entropy fields are both equal to zero: 

IÑ × =  
0

(10a) 

 

 

0SÑ × =  

(10b) 

Together, equations (9) and (10) form a set of equations analogous to the Maxwell equations [16] (with no 

sources  or  isolated  charges,  and  in  an  isotropic  medium).  Equations  (9)  can  be  combined  to  yield  wave 

equations for the information and entropy fields: 

2
Ñ -
I

2
d I
1
2
2
c dt

=  
0

(11a) 

 

2
Ñ -
S

2
d S
2

1
2
c dt

=  
0

(11b) 

As defined previously, the information and entropy fields are mutually orthogonal, since 

I S× = . Analogous 

0

to the Poynting vector describing energy  flow of an EM wave,  the direction of propagation of the wave  is 

given by  I S´ , e.g. see figure 2. The dynamic relationship between I and S means that information flow must 

be accompanied by entropy flux, such that movement of data is dissipative in order to satisfy the 2nd Law of 

Thermodynamics. Analytic functions such as I and S only have points of inflection, so that, again, the 2nd Law 

with respect to the S-field is a natural consequence of its monotonicity. However, in analogy to an EM-wave, 

equations (9) and (10) in combination imply that the sum of information and entropy 

I

2

2

S+

 is a conserved 

quantity. Hence, the 2nd Law of Thermodynamics should be viewed as a conservation law for info-entropy, 

ix

 

jx

jI

 

 

 

 

 

 

 

 

similar to the 1st Law for energy. This has implications for the treatment of information falling into a black 

hole. The reason that the differential equations (6) and (7) obey the laws of relativity is that they are simple 

spatial  reciprocal  quantities,  with  their  ratios  equivalent  to  velocities,  such  that  the  relativistic  laws  are 

applicable. This reciprocal space aspect implies that calculation of Fourier-space info-entropy quantities result 

in quantities proportional to real space, which are therefore also relativistically-invariant and holographic.  

Right-handed helix

¶
S
¶
t

>

0

Left-handed helix

jx

¶
S
¶
t

<

0

ix

I

=

k
[0, cos(
I

x
i

-

w

t

k
), sin(

I

x
i

-

w

t

)]

kS

S

=

[0,

-

S

k
sin(

x
i

-

w

k
), cos(
t S

x
i

-

w

t

)]

kx

 

I

=

k
[0, sin(
I

x
i

-

w

t

k
), cos(

I

x
i

-

w

t

)]

S

=

[0,

-

S

k
cos(

x
i

-

w

k
), sin(
t S

x
i

-

w

t

)]

kI

jS

kx

 

 
Figure 2: a) Right-handed polarisation helical info-entropy wave, propagating in positive xi-direction given by 

I´S,  b) Left-handed polarisation helical info-entropy wave travelling in same direction. 

Equation (9a) shows that a right-handed helical spatial information distribution is associated with an increase 

of entropy with time (Fig 2a); in contrast to a left-handed chirality where entropy decreases with time (Fig 

2b).  A  link  is  revealed  between  the  high  information  density  of  right-handed  DNA  [18]  and  the  overall 

increase  of  entropy  with  time  in  our  universe  [19].  A  molecule  such  as  DNA  is  an  efficient  system  for 

information storage. However,  our  dynamic  theory  suggests  that  information would radiate  away,  unless a 

localisation mechanism were present, e.g. the existence of a standing wave. Such waves require forward and 

backward  wave  propagation  of  the  same  polarisation.  We  see  that  the  complementary  anti-parallel  (C2 

spacegroup)  symmetry  of  DNA’s  double  helix  means  that  information  waves  of  the  same  polarisation 

(chirality) will  travel  in  both  directions  along  the  helix  axis  to  form  a  standing  wave,  thus  localising  the 

information. Small left-handed molecules (e.g. amino acids, enzymes and proteins, as well as short sections of 

Z-DNA [20]) are known to exist alongside DNA within the cell nucleus. Their chirality is associated with a 

decrease of entropy with time. In conjunction with right-handed molecules they can be understood to regulate 

the  thermodynamic  function  of  the  DNA  molecule.  As  is  well  known,  highly  dynamic  systems  require 

negative  feedback  to  direct  and  control  their  output,  without  which  they  become  unstable.  We  can  draw 

parallels  to  the  high  entropic  potential  of  DNA,  with  left-handed  molecules  (as  well  as  right-handed 

molecules) acting as damping agents to control its thermodynamic action. 

In  conclusion, our theory  grounds  information and entropy onto a physical,  relativistic  basis,  and  provides 

thermodynamic insights into the double-helix geometry of DNA. It offers the prediction that non-DNA-based 

life forms will still tend to have helical structures, and suggests that black holes conserve info-entropy. 

 

 

Appendix 

We generalise the x-dimension to the ith space-dimension (

i =

1, 2,3

), such that for a pole travelling in the xi-

direction (i.e. equivalent to a plane wave travelling in the xi-direction) the info- and entropy-fields vibrate in 

the mutually-orthogonal xj- and xk-directions respectively, where again 

j k =
,

1,2,3

 and  i

¹ ¹ . The vector 

k

j

descriptions of the I and S fields are: 

I

=

I x
1 1

+

I x
2 2

+

I x
3 3

 

(A1a) 

 

S

=

S x
1 1

+

S x
2 2

+

S x
3 3

 

 

(A1b) 

In analogy to an EM plane-wave, two plane-wave polarisations are therefore possible:  

(a) 

(b) 

iI = , 
0

I

j

=

k

log

x¢
i

, 

kI =  
0

2

iS = , 

0

jS = , 

0

kS

=

k

log

2

ct¢

 

(A2a)  

iI = , 
0

jI = , 
0

I

k

= -

k

log
2

x¢
i

 

iS = , 

0

jS

=

k

log
2

ct¢

, 

kS =  
0

(A2b) 

 

 

where we have generalised the position of the point of non-analyticity (pole) to a position 

¢

z

=

¢
x
i

+

ict

¢

, i.e. 

dropped  the  subscript  ‘0’.  We  see  that  the  information  and  entropy  fields  are  mutually  orthogonal,  since 

I S× = . Also, the direction of propagation of the wave is given by  I S´  with the sign of 

0

,j kI

 chosen so that 

flow  is  in  the  positive  xi-direction.  We  make  use  of  the  4 4´   transformation  matrix  g ,  relating  one 

relativistic frame of reference to another [16]: 

¢

æ
ç
ç
ç
ç
ç
è

ict
¢
x
1
¢
x
2
¢
x
3

ö
÷
÷
÷
÷
÷
ø

=

æ
ç
ç
ç
ç
ç
è

g

-

gb

-

gb

-

gb

1

2

3

1

+

-

gb

A

1
2
b
1
b b
2
b b
3 1

1

A

A

-

A

gb

2
b b
1
A

2
2
b
2

1

+

A

b b
3

2

-

A

A

gb

3
b b
1
b b
2
A

b

1

+

3

3
2
3

ö æ
÷ ç
÷ ç
÷ ç
÷ ç
÷ ç
ø è

ict

x
1
x
2
x
3

ö
÷
÷
÷
÷
÷
ø

 

 

 

(A3) 

where 

1b , 

2b , and 

3b  are the normalised velocities (with respect to c) in the co-ordinate directions, with 

2

b

=

2
b
1

+

b

2
2

+

b

2
3

=  in this case, since the overall wave velocity is equal to c. In addition the parameters  g  

1

and A are related by 

g

=

1/ 1

-

b

, and also 

g

= +

b
1 A

. We note that equation (A3) is made equivalent to 

2

2

(4),  by  substituting 

g

=

q
coshi

,  and 

b
1

=

q
tanhi

,  with 

b

2

b=

3

= .  Performing  a  dynamic  calculus  on 

0

equations (A2a), we derive the following expressions: 

Consideration of the second polarisation (b) allows us to derive the additional following expressions: 

¶

I

j
¶
x
i

=

¢¶
xk
i
¢
¶
x
x
i
i

=

g

ii

 

k
¢
x
i

¶

I

j
ict

¶

=

k
¢
x
i

¢¶
x
i
¶
ict

=

g

i

0

 

k
¢
x
i

¶
I
k
¶
x
i

=

¢
¶-
xk
i
¢
¶
x
x
i
i

=

-
k
¢
x
i

g

ii

 

¶
I
k
¶
ict

=

¢
¶-
x
k
i
¢
¶
ict
x
i

=

-
k
¢
x
i

g
i

0

 

 

 

 

 

(A4a) 

(A4c) 

(A5a) 

(A5c) 

¶
kS
¶
ict

=

k
ict

¢

¢
¶
ict
¶
ict

=

k
ict

¢

g

00

  (A4b) 

¶
S
k
¶
x
i

=

k
ict

¢

¢

¶
ict
¶
x
i

=

k
ict

¢

g

0

i

  (A4d) 

¶

¶

jS
ict

=

k
ict

¢

¢
¶
ict
¶
ict

=

k
ict

¢

g

00

  (A5b) 

¶

S

j

¶
x
i

=

k
ict

¢

¢

¶
ict
¶
x
i

=

k
ict

¢

g

i
0

  (A5d) 

Since the direction of propagation is in the xi-direction, we must have that 

b

j

b=

k

= , so that 

0

ib

b=

, and 

therefore 

g

= +
1

2

b
iA

, such that 

iig

g=

00

, as well as 

g
i

0

g=

i
0

. Given 

¢
ix

¢=
ct

 as usual, in the xj-direction the 

following pair of equations holds: 

-

¶
I
k
¶
x
i

=

1
c

¶

S

j

¶

t

 

 

(A6a) 

 

-

¶
S
k
¶
x
i

= -

1
c

¶
I

j
¶
t

   

(A6b) 

Likewise, in the xk-direction we find the following pair of equations holds: 

¶

I

j
¶
x
i

=

1
c

¶
S
k
¶
t

 

 

(A7a) 

 

 

 

(A7b) 

¶

S

j

¶
x
i

= -

1
c

¶
I
k
¶
t

We see that both pairs of equations (A6) and (A7) respectively obey the Cauchy-Riemann symmetries evident 

in equations (8). Considering the three cyclic permutations of i,j and k, we can write: 

3

å

i

=

1

æ
ç
ç
è

¶
I

j

¶

x
k

-

¶
I
k
¶
x

j

ö
÷
÷
ø

x
i

=

1
c

3

å

i

=
1

¶
S
i
¶
t

x
i

 

(A8a) 

 

3

å

i

=
1

æ
ç
ç
è

¶

S

j

¶

x
k

-

¶
S
k
¶
x

j

ö
÷
÷
ø

x
i

=

-
1
c

3

å

i

=
1

¶
I
i
¶
t

x
i

 

(A8b) 

Equations (A8) can be alternatively written in the compact vector notation of equations (9). 

 

 

 

 

 

 

References  

[1] C.H. Bennett, Scientific American, 257, 88 (1987) 

[2] R. Landauer, Physica Scripta, 35, 88 (1987) 

[3] L. Brillouin, Science & Information Theory, (Academic, New York, 1956) 

[4] R. Landauer, Physics Today, 44, 23 (1991) 

[5] M.C. Parker and S.D. Walker, Optics Communications, 229, 23 (2004) 

[6] M.C. Parker and S.D. Walker, http://arxiv.org/abs/physics/0401077 (2004) 

[7] J.C. Garrison, M.W. Mitchell, R.Y. Chiao, and E.L. Bolda, Physics Letters A, 245, 19 (1998) 

[8] L. Brillouin, Wave propagation and group velocity, (Academic, New York, 1960) 

[9] M.D. Stenner, D. J. Gauthier, and M.A. Neifeld, Nature, 425, 695 (2003) 

[10] K. Wynne, Optics Communications, 209, 85 (2002) 

[11] J.J. Carey, J. Zawadzka, D. A. Jaroszynski, and K. Wynne, Physical Review Letters, 84, 1431 (2000) 

[12] W. Heitmann, G. Nimtz, Physics Letters A, 196, 154 (1994) 

[13] G.B. Arfken, H.J. Weber, Mathematical Methods for Physicists, (Academic, New York, 1995), Chap. 6 

[14] H. Primas, Time, Temporality, Now: The representation of facts in physical theories, (Springer, Berlin, 1997) p.241-

263 

[15] N. Gershenfeld, The physics of information technology, (Cambridge University Press, Cambridge, 2000), Chap. 4 

[16] J.D. Jackson, Classical Electrodynamics, (John Wiley & Sons, New York, 1999), Chap. 11 

 [17] K.-E. Peiponen, E.M. Vartianen, and T. Asakura, Dispersion, complex analysis, and optical spectroscopy: classical 

theory, (Springer, Berlin, 1999) 

[18] J.D. Watson and F.H.C. Crick, Nature, 171, 737 (1953) 

[19] W.H. Zurek, Complexity, entropy and the physics of information, (Addison-Wesley, Redwood City, 1989), Vol. 8 

[20] J.S. Siegel, Nature, 409, 777 (2001) 


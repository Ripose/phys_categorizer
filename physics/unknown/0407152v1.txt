PC Cluster Machine Equipped with High-Speed Communication Software 
 

Motohiko Tanaka 

Coordinated Research Center, Institute for Fusion Science, Toki 509-5292, Japan 

Email: mtanaka@nifs.ac.jp    http://dphysique.nifs.ac.jp/ 

LANL Arxiv: physics/0407156 (July 2004) 

Originally: NIFS TECH Series No.12 (May 2004) 

 

 

 

Abstract 

 
   A  high  performance  Beowulf  (PC  cluster)  machine  installed  with  Linux  operating  system  and 

MPI  (Message  Passing  Interface)  for  interprocessor  communications  has  been  constructed  using 

Gigabit  Ethernet  and  the  communication  software  GAMMA  (Genoa  Active  Message  Machine), 

instead  of  the  standard  TCP/IP  protocol.  Fast  C/Fortran  compilers  have  been  exploited  with  the 

GAMMA  communication  libraries.  This  method  has  eliminated  large  communication  overhead  of 

TCP/IP  and  resulted  in  significant  increase  in  the  computational  performance  of  real  application 

programs including the first-principle molecular dynamics simulation code.   

Keywords: non TCP/IP, active messages, small latency, fast C/Fortran compilers, 

                    materials science, first-principle molecular dynamics 

 

 

1. Reducing Large Latency of TCP/IP 

An old dream of “Constructing a supercomputer with personal computers” became reality in 1990’s 

North  America  [1,2]  where  numbers  of  personal  computers  (PC)  with  distributed  memories  were 

connected  with  each  other  by  network.  Each  unit  of  the  PC  cluster  machine,  which  is  sometimes 

called a Beowulf cluster machine, has a scalar-type processing unit and its own memory. Thus, high 

performance  computation comparable to a supercomputer is made possible at orders of magnitude 

lower cost. In this country, PC cluster machines  were introduced in various universities [3,4]. Our 

group  also  made  a  series  of  PC  cluster  machines  with  Pentium  III,  Xeon  and  Pentium  4  with  the 

Linux operating system and MPI (Message Passing Interface) [5] to study physics and chemistry of 

ionic materials [6,7]. Concurrent grid computing aiming for large memory and computational time 

appears to be a modern extension of the previous attempts of PC cluster machines. 

      For high performance  parallel computations, all the three features of fast processors, high data 

bandwidth  around  processors,  and  inter-processor  communications  are  required.  It  is  customary  to 

use special hardware to realize fast computations and communications, which are most advanced on 

state-of-art  vector/parallel  supercomputers  and  as  designated  chips  for  workstations.  Examples  are 

Grape  chips  specialized  for  calculating  gravity  and  Coulomb  forces  [8],  and  Myrinet  for  fast 

communications [9]. However, these hardware are more expensive than a PC cluster itself, and their 

use in the PC cluster is not without a question in terms of the total cost of the system. 

      Despite  of  the  popularity  of  PC  cluster  machines  because  of  ultra  high  cost-performances,  its 

massive  communication  overhead  due  to  latency  of  the  TCP/IP  protocol  has  not  widely  been 

recognized.  Replacing  the  fast  Ethernet  (100Mbits/s)  with  gigabit  Ethernet  does  not  speed  up  the 

computation  of  real  application  programs.    For  example,  the  timing  of  the  same  program  with 

frequent MPI communications for the same number  of Pentium 4 and RISC processors  makes the 

communication problem very clear, as depicted in Table 1. For the former with the TCP/IP protocol, 

the  wallclock  time  is  significantly  larger  than  the  cpu  time;  their  ratio  is  almost  1.4  with  four 

processors  of  Pentium  4 

in 

the  first-principle  molecular  dynamics  simulation.  But, 

the 

communication overhead is very small for the latter. 

      In  the  following  sections  of  this  article,  we  describe  the  method  of  installing  and  using  the 

low-latency communication software GAMMA (Genoa Active Message Machine) [10], which is a 

non  TCP/IP  protocol  and  works  on  fast/gigabit  Ethernet.  Also,  we  show  the  way  to  use  fast 

C/Fortran compilers that  are not included in the standard Linux distributions.  These new elements 

result  in  removal  of  the  large  communication  overhead  of  TCP/IP  and  significant  increase  in 

computational  abilities  of  the  PC  cluster  machine  for  real  application  programs,  only  at  small 

additional cost for the second Ethernet. The Score project also provides good performance of high 

asymptotic bandwidth but more latency on the gigabit Ethernet [11] compared with GAMMA. 

2. Utilization of GAMMA and Fast C/Fortran Compilers 

In this section, we describe the method of installing GAMMA [10] to a PC cluster machine.    It is 

based  on  the  active  message  mechanism  [12]  that  makes  direct  communications  between  the 

application program and the network interface bypassing the operating system. We also show how 

the fast C/Fortran compilers that are not contained in the Linux distributions can be used with the 

GAMMA  system.  (Detailed  procedures  of  the  installation  are  found  in  the  GAMMA  installation 

manual, and in Appendices here; update information will be given at URL of Ref.[7].) 

      Nice features of the present method is that all the software including the Linux operating system 

and GAMMA software are either free or low priced, and that the hardware including processors and 

gigabit  Ethernet  cards  are  reasonably  priced  commercial  products.  On  the  other  hand,  for  high 

efficiency  of  communications,  it  is  recommended  to  have  dual  networks;  the  first  network  is  a 

gigabit Ethernet for the GAMMA data transmissions, and the second one is a TCP/IP network for the 

NFS file system and general administration purposes, as shown in Fig. 1.   

      The  programs  to  be  installed  are  GAMMA,  (modified)  MPI-1.1  [5]  and  the  MPI  interface  for 

GAMMA,  all  of  which  are  downloaded  from  the  same  site  [10].  Some  remarks  are:  (i)  limited 

species of the NIC (network interface card) are supported for fast/gigabit Ethernet, and (ii) the Linux 

kernel  must  be  currently  of  the  version  2.4.21  due  to  assembler  of  the  GNU  gcc  compiler.  If 

necessary, the Linux kernel should be downloaded and upgraded.   

 

 

2(a) Linux Kernel Upgrade 

The Linux kernel is an independent part of the Linux distributions, and can be updated by following 

the  procedures  given  in  Appendix  A.  Since  upgrading  the  kernel  requires  special  carefulness  and 

some trials and errors, old data and configurations should be backed up in a safe place for emergency. 

 

 

During  the  upgrade,  the  existing  kernel  should  never  be  removed,  which  may  be  used  under  the 

dual-boot environment for emergency recovery.   

2(b) GAMMA Installation 

Installing procedures of GAMMA are easier than the Linux kernel upgrade. Configure and compile 

the  source  code,  and  write  it  onto  the  kernel  (Appendix  B).  In  the  configuration,  the  flow  control 

option should be turned on (see Sec.3). After the GAMMA installation, some environmental settings 

are  necessary  to  start  communications  (Appendix  C).  This  includes  creating  the  list  of  nodes  that 

participate  in  the  GAMMA  communications.  Then,  test  the  communication  by  invoking  the 

pingpong  program;  if  everything  is  properly  established,  the  communication  speed  is  shown  for 

specified size of transmitted data. 

 

2(c) MPI/GAMMA Installation 

To  utilize  the  GAMMA  communication  from  application  programs,  the  MPI  and  interface  to 

GAMMA programs need to be installed (Appendix D). For consistency with the GAMMA libraries, 

the  GNU  C/Fortran  of  the  Linux  distributions  should  be  used  for  this  compilation.  Note  that,  if 

messages “No rule to make.…Stop.” has appeared, the instruction should be followed and include 

files  be  copied  as  directed  (unfortunately,  compilation  does  not  stop  here).  The  GAMMA  system 

guarantees correctness of the transmitted data for blocking communications. On the other hand, it is 

essential to reset communication status by invoking the “gammaresetall” command to maintain node 

synchronization when tasks have failed by any reasons. 

2(d) Fast C/Fortran Compilers 

For  getting  high  computational  efficiency  or  using  Fortran90  standards,  commercial  compilers  are 

recommended instead of the Linux’s GNU C/Fortran compilers. It is very important here to arrange 

for  consistency  between  the  MPI/GAMMA  libraries  and  execution  binaries  of  user’s  application 

program (Appendix E). Most essential is to attach two trailing underscores to function names, which 

is the GNU C/Fortran standard. Due to this reason, all the mathematics libraries including BLAS and 

LAPACK, and their extensions to parallel computation BLACS and SCALAPACK [13] need to be 

recompiled.     

      There  is  a  warning  about  the  logical  operations  of  the  mpi_(all)reduce  function,  which  always 

returns .false. irrespectively of the input values on some compilers. This is due to the definition (use 

of integers) of the logical true and false values which differs from that of the GNU compilers [14]. 

This is a very hard trap to be located if not notified [15]. 

 

3. Performance of MPI/GAMMA with Fast Compilers 

Let us examine how much the computational speed increases for real application programs when the 

MPI/GAMMA communication system is utilized with fast C/Fortran compilers. 

      First, the data  transmission  speed  is  shown  in  Fig.2  as  a  function of  the  data  size.  This  timing 

refers  to  a  point-to-point  communication  measured  by  the  pingpong  program  included  in  the 

GAMMA program. The data transmission speed is defined by transmitted data size divided by the 

time  spent.  A  3Com996  gigabit  network  interface  card  is  used  with  Pentium  4  (3.0GHz).  The 

transmission speed 0.6Mbits/s for one byte data corresponds to the latency 15micro sec. This small 

latency is quite advantageous for real application programs with frequent small data exchanges over 

the  TCP/IP  protocol  whose  latency  is  about  100micro  sec  (this  point  will  be  again  mentioned  in 

Table  1).  The  transmission  speed  increases  with  the  data  size,  and  saturates  around  105  bytes. 

Asymptotic band width for the present environment (no optimization) is 706Mbits/s which amounts 

to 70% of the gigabit Ethernet. 

      The  best  value  shown  on  the  GAMMA  homepage  [10]  are  the  following.  For  Myrinet 

(1.28Gbits/s)  on  the  BIP  platform,  the  latency  and  asymptotic  bandwidth  are  4.3micro  sec  and 

1005Mbits/s, respectively, and are 8.5micro sec and 976Mbits/s for GAMMA + Netgear GA621 NIC, 

respectively. 

      Next,  the  computational  efficiency of  the  density-functional  first-principle  molecular  dynamics 

code Siesta [16] on the GAMMA system is described. This is a tight-binding ab initio code adopting 

atomic orbital bases, and its most demanding operation is density matrix diagonalization. As a test 

case, a system of 180 atoms and a proton is used under a slab geometry and over the gamma point 

[17].  Table  1  shows  that  the  communication  overhead  decreases  dramatically  when  the  TCP/IP 

communication  (top  row)  is  replaced  by  the  MPI/GAMMA  communication  with  the  flow  control 

enabled  (middle  row);  the  overhead  time  26sec  for  TCP/IP  becomes  0.1sec  for  GAMMA,  and 

consequently,  the  wallclock  time  for  one  SCF  cycle  of  the  Siesta  run  decreases  from  93sec  for 

TCP/IP  to  66sec  for  GAMMA.  Their  times  spent  on  the  processing  units  are  similar.  If  the  flow 

control is not enabled on the GAMMA communications, the wallclock time degrades substantially 

due to retransmission of the data already sent. This may apply to usual application programs that we 

use in our research. 

      As a reference, the timing of the same Siesta code for the standard RISC machine IBM Power 4 

(slightly slower than SGI Altix3000) is shown at the bottom row of Table 1. It tells us that the PC 

cluster machine of Pentium 4 processors via the MPI/GAMMA is nearly equivalent in computational 

speed with the same number of RISC machines with half the clock speed. Small overhead times are 

common  to  these  machines.  In  some  cases,  the  computational  speed  on  the  PC  cluster  machine  is 

comparable to that on the so-called vector/parallel supercomputer of the same number of processors. 

This can happen for the first-principle molecular dynamics simulations. 

      Figure  3  shows  how  the  computational  speed  scales  with  the  number  of  processors  when  the 

Siesta code is used on the GAMMA system. Here, the ordinate is the inverse of the wallclock time 

for one SCF cycle of the run (181 atoms, slab geometry) in reference to the uni-processor case. The 

computational  speed  increases  nearly  linearly  up  to  four  processors,  and  improves  but  gradually 

beyond that point. We can estimate the portion of non-parallelizable part P in the simulation program. 

The  relative  computational  speed  is  approximated  by  1/[P+(1-P)/N]  where  N  is  the  number  of 

processors. From this formula and Fig.3, one estimates P as 0.1 (10%). Thus, it is important to use a 

low  latency  communication  system  for  the  computational  speed  to  scale  well  with  the  number  of 

processors (P << 1) since the communication overhead constitutes a non-parallelized part. 

 

4. Conclusion 

In this article, it was shown that the large latency of the TCP/IP protocol, which was a bottleneck for 

computational  speedup  of  the  PC  cluster  machine,  can  be  removed  by  adopting  the  low  latency 

communication software GAMMA which works via the gigabit Ethernet. Also, the method of using 

efficient  C/Fortran  compilers  on  this  low  latency  communication  system  was  described,  with 

detailed procedures given in Appendices. 

      The timing results show that the PC cluster machine of Pentium 4 and the GAMMA system via 

the gigabit Ethernet is almost equivalent in the computational speed with the RISC machine (of half 

the clock speed) of the same number of processors.    It was also found that the PC cluster machine 

can  be  as  useful  as  a  vector/parallel  supercomputer  in  the  application  programs  containing  many 

inter-processor communications. This includes the tight-binding first-principle molecular dynamics 

simulations. 

 

Acknowledgments:   

The  author  thanks  Dr.  Giuseppe  Chiaccio  for  his  kind  advices  on  the  installation  of  the  GAMMA 

system  to  his  Beowulf  cluster  machine.  He  also  thanks  Dr.Y.Zempo  for  close collaboration  on  the 

construction  of  the  cluster  machine  and  the  installation  of  the  first-principle  molecular  dynamics 

code  Siesta.  The  timing  with  IBM  Power  4  was  performed  using  the  machine  of  the  Minnesota 

Supercomputing  Institute,  University  of  Minnesota.  The  present  work  was  supported  by  the 

Grand-in-Aid  No.16032217  (2003-2004)  from  the  Japan  Ministry  of  Education,  Science  and 

Culture. 

Appendices: 

The following procedures and numerical accuracies have been examined for the combination of 

Pentium  4  and  Red  Hat  Linux  7.3.  Note  that  the  kernel  upgrade  and  program  installation 

procedures  require  special  carefulness  and  some  efforts  with  trials  and  errors.  Please  try  these 

things  at  your  own  risk  with  sufficient  preparations  and  cautions  including  backup  of  existing 

data and configurations. The location of files may slightly differ among Linux distributions. 

Appendix A: Kernel Upgrade 

A  kernel  is  a  Linux  core  and  is  independent  of  the  Linux  distributions.  To  upgrade  it,  first 

download  the  kernel  source  code  (from  http://www.linux.or.jp/,  for  example).  In  the  following 

procedures, # stands for a command prompt (waiting for your command input). You need to be a 

root to alter the system directories. 

 

 

#mv linux-2.4.21.tar.gz /usr/src    (2.4.21 kernel is used)   

#cd /usr/src 

#rm -rf linux    (delete the old link only, and never delete the old kernel itself !) 

#tar xvzf linux-2.4.21.tar.gz 

#ln -s linux-2.4.21 linux    (establish a new link) 

#cd /usr/src/linux 

        Now edit the Makefile in this directory; uncomment the #export INSTALL_PATH= /boot 

    (remove #). To reuse existing system configurations, copy the old configuration to .config: 

#cp /usr/src/linux-(old)/configs/kernel-(old)-(arch).config /usr/src/linux/.config   

      Here (old) should be a previous kernel version number, and (arch) is the architecture number   

  like i386 or i686. 

 

#make oldconfig    (skip all lines by an Enter key) 

#make xconfig    (Use an X window to work in the interactive mode) 

        Choose a proper “Processor Family” for your system, set “Symmetric multi-processing 

    support” on. Choice items are extensive; help messages are shown by clicking on HELP 

    located at the right-hand side of each line of the menu panel. You may use default values if you 

    are not sure of them. 

#make dep 

#make clean    (always remove stale files) 

#make bzImage    (build the kernel image) 

      If you find errors in this step, some of your choices above were not right. Return to “make 

    xconfig”, and redo “make dep; clean; bzImage” steps until you see no errors. These steps may 

    require trials and errors.    After done, you are ready to write the properly configured kernel 

    image to your hard disk. 

#mkdir /lib/modules/2.4.21 

#/sbin/installkernel 2.4.21 arch/i386/boot/bzImage System.map 

    Now edit either /etc/lilo.conf or /etc/grub.conf if you use the LILO or GRUB boot loader. The 

    label (LILO) or the number (GRUB) after “default=” specifies the new kernel to be booted 

    automatically. It is essential that you retain the old kernel entry for emergency rescue. 

 

 

Next, make modules for the new kernel: 

#make modules 

#make modules_install 

#/sbin/depmod -a (write out module dependencies; make sure no errors appear)   

# /sbin/mkinitrd -ifneeded /boot/initrd-2.4.21.img 2.4.21 

#/sbin/lilo -v    (if LILO is used)    or    # /sbin/grub-install /dev/had (if GRUB) 

    In above, replace /dev/hda with /dev/sda if the boot disk is a SCSI drive.   

        After done, reboot your PC and choose the new kernel. Should you not find an X Window or 

    Ethernet connection, you only need to add a video or NIC driver to the new kernel; download 

    the source and compile it. However, if you encounter serious errors like “Kernel panic”, you 

    have to reboot the PC and work on the old kernel to correct errors. Go back to the beginning of 

    the kernel upgrade; start with “#make clean” to delete stale files and dependencies. If you 

    remake the modules, delete old modules by “#rm -rf    /lib/modules/2.4.21/*” first. 

Appendix B: Installing GAMMA 

Download  from  http://www.disi.unige.it/project/gamma/  the  source  of  GAMMA,  MPICH-1.1.2, 

and MPI/GAMMA. The GAMMA program is assumed to be put under the /usr/local directory. 

(Read the GAMMA Installation Manual located in the /doc directory.) 

#cp gamma-(version).tar.gz /usr/local 

#tar xvzf gamma-(version).tar.gz 

#cd /usr/local/gamma 

#./configure 

        Here configure GAMMA with a proper choice of NIC and mode of communications. In 

    order to avoid data collisions during communication, set “Flow control” on. For the remote 

    command shell, “rsh” is conveniently chosen; list up the host names that participate in 

    GAMMA communications in /etc/hosts.equiv on all nodes, and enable rsh, rexec, 

    rlogin daemons by “# /sbin/chkconfig rsh on” etc. Instead, you can choose ssh.   

    Then, compile GAMMA and place it into the Linux kernel. 

#cd /usr/src/linux 

#make xconfig    (work on an X window) 

    Deselect the TCP/IP driver that conflicts with the GAMMA driver. 

#make dep 

#cd /usr/local/gamma 

#make    (compile GAMMA) 

#make install 

#cd /usr/src/linux 

#make bzImage    (make a new kernel image that contains GAMMA) 

#/sbin/installkernel 2.4.21 arch/i386/boot/bzImage System.map 

        Edit /etc/grub.conf and delete one of duplicated boot entries. Set GAMMA environments 

    (see Reference C). Then, reboot your PC to put the new configuration into effect.. 

        To make use of the two independent networks per PC, mount the home directory of the 

    master node via TCP/IP network as NFS (network file system). This is required for a job to 

    refer to the execution binary located on the master node.   

Appendix C: Environmental Settings for GAMMA 

Following settings are necessary for GAMMA to operate properly. 

* When rsh is used for the remote shell, start the rshd, rlogind, rexecd daemons by 

    “# /sbin/chkconfig rlogin on”, etc.   

* NFS mount the home directory of the master node on the slave nodes. To do this, list up in 

    /etc/exports of the master node the directory names to be exported and the slave node names to 

    which the directories are exported (to export /home to all nodes under 192.168.1 with 

    read/write permissions, the entry is: /home 192.168.1.0/24(rw)). Then, start the service by   

    “# /sbin/service nfs start”. On the slave nodes, NSF mount the directory by “#mount 

    master_node_name:directory_name local_name”. Conveniently, add the NSF entry in the 

    /etc/fstab of the slave nodes so that “#mount local_name” should work. 

(1) In /etc/gamma.conf of all nodes, list up the pairs of the host name of eth0 NIC and its MAC 

    address (12-digit hardware address, shown by “# /sbin/ifconfig”), one entry per line. For 

    example, pc001 as a host name and 01:23:45:67:89:ab as MAC address should be paired as: 

          pc001 0x01 0x23 0x45 0x67 0x89 0xab 

(2) Add “export PWD” in .bash_profile of user’s home directory. 

(3) To boot GAMMA automatically, add “/usr/local/bin/gammagetconfig” in the file 

    /etc/rc.d/rc.local    (may differ among Linux distributions). 

Run the test program pingpong under the user mode (rsh is usually disabled under the root mode 

for security). 

 

 

 

Appendix D: Installing MPI/GAMMA 

Use mpich-1.1.2 specially modified to use with GAMMA.   

#cp mpich-1.1.2.tar.gz mpigamma-(version).tar.gz /usr/local 

#tar xvzf mpich-1.1.2.tar.gz    (after expansion, sub-directory mpich is created) 

#tar xvzf mpigamma-(version).tar.gz 

#cd /usr/local/mpich 

          Now configure MPI/GAMMA using the following script (make this script file executable; 

      in the below, ¥ stands for a line continuation.) 

          ./configure -cc=gcc -fc=f77 -cflags=-fomit-frame-pointer -optcc=-O3 ¥ 

          -noromio -noc++ -nompe -prefix=/usr/local/lib ¥ 

          -lib=/usr/lib/libgamma.a -device=gamma   

#make    (do not execute “#make install”) 

[Warning]  During  this  make,  an  error  message  “No  rule  to  make  target  ….Stop.”  appears  but 

compilation  never  stops  !  (Strange  errors  while  compiling  or  executing  an  application  program 

using MPI/GAMMA very often originate here). To correct reference errors, copy include files to 

designated directories as required, and do make again. Specifically, the new directories are those 

with  “linux” 

instead  of  “linux-2.4.21”.  Copy 

/usr/src/linux-2.4.21/include/linux/gamma/ 

libgamma.h and /usr/src/linux-2.4.21/include/asm/page.h to appropriate new directories.   

      After  the  make,  libraries  and  include  files  are  placed  in  /usr/local/mpich/build/LINUX/ 

gamma/lib and include directories. 

Appendix E: Fast C/Fortran Compilers with GAMMA 

* First of all, copy mpif.h, mpi.h, mpi_errno.h and binding.h files in the /usr/local/ mpich/include 

directory to /usr/local/mpich/build/LINUX/gamma/include.    Make sure that the first columns of 

mpif.h are (!) instead of (c). 

*  Consistency  is  required  among  the  GAMMA  libraries  and  application  program  execution 

binaries. Mathematics libraries need to be recompiled under the following conditions: 

      (i) Linux’s GNU C/Fortran compilers put two trailing underscores (__) after the subroutine 

        /function names. This rule must be obeyed by other compilers, 

      (ii) Cite two include files of C/Fortran compilers and GAMMA in this order, 

      (iii) Recompile linear algebra libraries BLAS, LAPACK, and their extension to parallel 

        computation BLACS, SCALAPACK [13] under the condition (i), 

      (iv) Link farg.f    which ports different C/Fortran compilers [14]. 

      (v) A wrapper program for the mpi_(all)reduce function is required with regard to differences 
        in the definition of logical constant of .true. [15]. 

 

 

 

* A standard compilation script to use Portland’s pgf90 compiler with MPI /GAMMA is the 

    following: 

            pgf90 -o ax1.out -Msecond_underscore -Mvect (program name) ¥ 

            -I/usr/local/pgi/linux86/5.1/include ¥ 

            -I/usr/local/mpich/build/LINUX/gamma/include ¥ 

            farg.o wrap_allreduce.o ¥ 

            -L/usr/lib/libgamma.a ¥ 

            (SCALAPACK and BLACS libraries) ¥ 

            -L/usr/ local/mpich/build/LINUX/gamma/lib -lfmpich -lmpich ¥ 

            /usr/local/BLAS/libblas.a /usr/local/LAPACK/liblapack.a 

 
* An example of “wrap_allreduce.f”   
c    Replace mpi_allreduce (logical…) by mpi_allreducel (logical…) 
c    # Compile this routine by 
c          pgf77 -c -Msecond_underscore ¥ 
c        -I/usr/local/mpich/build/LINUX/gamma/include ¥ 
c        -I/usr/local/pgi/linux86/5.1/include wrap_allreduce.f 
c------------------------------------------------------------------- 
            subroutine mpi_allreducel ( sendbuf,recvbuf,comm,ierror) 
c------------------------------------------------------------------- 
            logical    sendbuf, recvbuf 
            integer    comm, ierror, buf1, buf2 
            include ‘mpif.h’ 
 
            if(sendbuf) then 
                buf1= 1 
            else 
                buf1= 0 
            end if 
 
            call mpi_allreduce ( buf1, buf2, 1, MPI_integer, MPI_sum, 
          *                                          comm, ierror ) 
 
            if(buf2.ne.0) then 
                recvbuf= .true. 
            else 
                recvbuf= .false. 
            end if 
 
            return 
            end 
 

*  In  conjuction  with  the  above  wrapper,  all  “call  mpi_allreduce”  must  be  replaced  by  “call 

mpi_allreducel”.    In  the  Siesta  program,  four  routines  (atomlwf.F,  extrapol.F,  extrapolon.F, 

iorho.F) are affected. 

References 

[1] Purdue University, Beowulf Project: http://www.psych.purdue.edu/~beowulf/ 

[2] D. Becker et al. (NASA), Beowulf Project: http://www.beowulf.org/ 

[3] Aoyama Gakuin University, Faculty of Science, PC Cluster Project: 

    http://www.phys.aoyama.ac.jp/~aoyama/ 

[4] Doshisha University, Faculty of Engineering, PC Cluster Project: 

    http://www.is.doshisha.ac.jp/SMPP/ 

[5] M.Snir, S.Otto, S.Huss-Lederman, D.Walker, and J.Dongara, MPI - The Complete 

    Reference, Vol. I, and II    (The MIT Press, Cambridge, 1998) 

[6] Y.Zempo, and M.Tanaka, Installation of First-Principle Molecular Dynamics Code on 

    PC Cluster Machine, NIFS Annual Review (2001). 

[7] M.Tanaka, Beowulf Cluster Equipped with High-Speed Communication Software, 

    NIFS TECH No.12 (2004); Plasma and Ionic Condensed Matters by Molecular Dynamics 

    Simulations: http://dphysique.nifs.ac.jp/   

[8] National Astronomical Observatory, Grape Project: http://www.astrogrape.org/ 

[9] Myrinet: http://www.myri.com/ 

[10] G.Chiola and G.Ciaccio, GAMMA Project: Genoa Active Message Machine   

    (Genoa University): http://www.disi.unige.it/project/gamma/ 

[11] SCore Consortium: http://www.pccluster.org/ 

[12] A.Mainwaring and D.Culler, Active message applications programming interface   

    and communication subsystem organization (1996): http://now.cs.berkeley.edu/AM/ 

[13] Archive of linear algebra: http://www.netlib.org/ . 

[14] Portland Group’s FAQ (Frequently Asked Questions): http://www.pgroup.com/ . 

[15] When Portland’s Fortran compiler pgf90 [14] is used with GAMMA, calls to MPI_Allreduce 

    (logical AND, OR) always return .false. This is due to differences of assigned integer to .true. in the 

    pfg90 compiler. The workaround is to use a wrapper program that assigns 1 and 0 for .true. and 

    .false., respectively, and let MPI_Allreduce process it. (Compile switch -Munixlogical does not work). 

[16] A. Garcia et al., Siesta (Spanish initiative for electronic simulations with thousands 

    of atoms): http://www.uam.es/departamentos/ciencias/fismateriac/siesta/ 

[17] Y.Zempo and M.Tanaka, Materials Science with First-Principle Molecular Dynamics Simulations 

    NIFS Newsletter, February /March issue (2004) [in Japanese]. 

[18] MPICH (MPI Chameleon), Argonne National Laboratory: http://www.mcs.anl.gov/ 

Table 1. 

Timing  of  different  communication  methods  using  the  density-functional  ab  initio  molecular 

dynamics  code  Siesta  v1.3  [16]  for  one  SCF  cycle  of  a  181  atom  system.  Four  Pentium  4 

(3.0GHz), Gigabit Ethernet NIC (3Com996), and PGI Fortran pgf90 are used. MPICH is Argonne 

National Lab’s MPI [18] via TCP/IP, and MPI/GAMMA [10] is via non TCP/IP; “FC on” stands 

for the use of the flow control during data transmission. For comparison, the last column shows 

the timing with four typical RISC processors - IBM Power 4 (Regatta, 1.5GHz). 

MPICH TCP/IP 

93 sec 

67 sec            26 sec 

1.39 

Wallclock a) 

CPU time b)      (a)-(b) 

(a) / (b) 

 

 

MPI/GAMMA 

FC on 

66 sec 

66 sec            0.1sec 

FC off 

    115 sec 

98 sec            17 sec 

RISC machine 

 

64 sec 

64 sec            0.4 sec 

(1.5GHz) 

1.00 

1.17 

1.01 

 

 

 

Figure Captions 

Figure 1: 

 

 

 

 

 

Figure 2: 

Figure 3: 

A  dual  network  system  is  recommended  for  the  Beowulf  cluster  machine  when  the  GAMMA 

communication system is used. The first network should be a gigabit Ethernet for the GAMMA 

data communications, and the second one is TCP/IP either in fast or gigabit Ethernet. The latter is 

required for the NFS file system (execution binaries are referred to through this network) and for 

administration purposes. 

The relation between transmitted data size (in Bytes) and data transmission speed (in Mbits/s) for the 

GAMMA communications. For this measurement, Pentium 4 (3.0GHz), and 3Com996 NIC (gigabit 

Ethernet)  were  used.  The  latency  at  zero  data  limit  is  15micro  sec,  and  the  maximum  data 

transmission speed is 706Mbits/s, which is about 70% of the gigabit Ethernet.   

Scalability  of  the  density-functional  ab  initio  molecular  dynamics  code  Siesta  [16]  under  the 

GAMMA communication system. The computational speed is represented in comparison with the 

uni-processor case. Here, Pentium 4 (3.0GHz), 3Com996 NIC, and Portland’s pgf90 compiler are 

used. Nearly linear scalability is obtained up to four processors for the Siesta run with 181 atoms. 

Gigabit Ethernet for GAMMA

192.168.2.100

192.168.2.101

192.168.2.102

192.168.2.103

Node 0

Node 1

Node 2

Node 3

192.168.1.100

192.168.1.101

192.168.1.102

192.168.1.103

TCP/IP Ethernet for NFS, Administration

External Network

Firewall

Fig.1  M.Tanaka

 
t
u
p
h
g
u
o
r
h
T
 
d
n
e
S

)
s
/
s
t
i
b
M

(

Data Length 
(bytes)

Fig.2 M.Tanaka

 

 

d
e
e
p
S
n
o
i
t
a
t
u
p
m
o
C

)
t
i
n
u

 
.

b
r
a
n
i
(

 

Number of Nodes

Fig.3  M.Tanaka


Inductive Logic 
From Data Analysis to Experimental Design  

Kevin H. Knuth 

Center for Advanced Brain Imaging 
Nathan Kline Institute, Orangeburg NY 10962 

Abstract. In celebration of the work of Richard Threlkeld Cox, we explore inductive logic and 
its role in science touching on both experimental design and analysis of experimental results.  In 
this exploration we demonstrate that the duality between the logic of assertions and the logic of 
questions has important consequences.  We discuss the conjecture that the relevance or bearing, 
b, of a question on an issue can be expressed in terms of the probabilities, p, of the assertions 
that answer the question via the entropy. 

In  its  application  to  the  scientific  method,  the  logic  of  questions,  inductive  inquiry,  can  be 
applied  to  design  an  experiment  that  most  effectively  addresses  a  scientific  issue.    This  is 
performed by maximizing the relevance of the experimental question to the scientific issue to be 
resolved.    It  is  shown  that  these  results  are  related  to  the  mutual  information  between  the 
experiment  and  the  scientific  issue,  and  that  experimental  design  is  akin  to  designing  a 
communication channel that most efficiently communicates information relevant to the scientific 
issue to the experimenter.  Application of the logic of assertions, inductive inference (Bayesian 
inference)  completes  the  experimental  process  by  allowing  the  researcher  to  make  inferences 
based on the information obtained from the experiment. 

THE LOGIC OF INFERENCE AND INQUIRY 

These workshops have spanned over two decades of research during which the power 
of  Bayesian  (or  inductive)  inference  has  been  demonstrated  time  and  time  again.  
Slowly,  but  surely,  these  techniques  have  become  more  accepted  in  mainstream 
science with applications in virtually every field.  Even as I write, the Office Assistant 
on this word processor, which uses a Bayesian network to infer my intentions from my 
actions is offering a suggestion to help me with the formatting of this document.  It is 
performing  the  equivalent  of  data  analysis,  which  is  arriving  at  the  most  probable 
conclusions given one's prior knowledge and newly acquired data. 

While  data  analysis  is  an  extremely  important  part  of  scientific  investigation,  its 
counterpart,  experimental  design  is  equally  important.    Intuitively,  the  problem  of 
experimental  design,  which  consists  of  choosing  an  experimental  question  most 
relevant  to  the  scientific  issue  to  be  resolved,  is  related  to  data  analysis.    However, 
there does not yet exist a complete theory of the logic of inference and inquiry.  The 
goal of this paper is to introduce the reader to the overarching framework of inductive 
logic,  to  describe  what  is  known  regarding  the  relationships  between  inference, 
inquiry, probability theory and information theory, and to highlight what is not known. 

Deductive and Inductive Inference 

the 
)
d

From 
∧∧→
(
(
cb
a
particular solution 
 

)

, if 

b→

, written  a
ba ∧

As deductive inference refers to implication among logical assertions in situations of 
 implies an assertion 
complete certainty, we begin with Boolean logic.  An assertion 
a
, where  ∧  is the logical and operation such 
=∨
=∧
bba
aba
 and 
b
 and  b  tell jointly, and  ∨  is the logical or 
 is an assertion that tells what 
that 
a
 is an assertion that tells what 
 tell in common. As 
operation such that 
b
"!
and 
Kangaroo
a
a
an 
example 
=
 as jointly the two 
 implies the assertion 
b
It
an
b
assertions say "It is a Kangaroo !".  In addition, the common assertion 
says "It is 
an Animal!".  Table 1 (below) lists the Boolean identities for assertions. 

ba ∨
consider 
"!

assertions 
a

1  The assertion 
.

 and 
It
is

Animal

a
"=

ba ∨

two 

the 

is

"

 

Richard  T.  Cox's  major  contribution  [1,2]  to  inductive  inference  arises  from 
generalizing  Boolean  implication  to  implication  of  varying  degree,  where  the  real 
 is implied by the implicant 
number representing the degree to which the implicate 
.    The  inferential  utility  of  this  formalism  is  readily  apparent 
a
when  the  implicant  is  an  assertion  representing  a  premise  and  the  implicate  is  an 
assertion representing a hypothesis.   

  is  written  as  (

a→

)b

b

associativity 
(
a
dc

∧∧→=
b

(

))

of 

assertions, 
, Cox derived a functional equation, which has as a 

conjunction 

the 

of 

(

a

∧→

cb

)

→=
(
a

bab

()

→∧

c

)

. 

(1) 

In  addition,  if  you  know  something  about  an  assertion,  you  also  know  something 
about  its  contradictory.    In  other  words,  the  degree  to  which  a  premise  implies  an 
assertion  b  determines  the  degree  to  which  the  premise  implies  its  contradictory  ~b.  
This logical principle can be applied twice to obtain a functional equation, which has 
as a particular solution 
 

→+→

)~
b

(2) 

=

. 

1

a

a

b

(

(

)

In  general  the  first  functional  equation  puts  some  constraints  on  the  second,  which 
results in a general solution 

 
 

where  r and 
solutions above. 

C

(

a

∧→

r

)
cb
a
b

→=
r
(
()
a
b
→+→
(
)~
b
a

→∧
ba
=
r
C

, 

)

r

(

r

c

)

 

(3) 
(4) 

  are  arbitrary  constants.    Setting 

= Cr

1=

  one  obtains  the  particular 

Cox  demonstrated  that  this  measure  of  relative  degree  of  implication  among 
assertions is the unique logically consistent measure.  We do well to define probability 
as  this  relative  degree  of  implication  among  assertions.    In  fact,  a  simple  change  of 
notation 
 

 reveals that the equations (1) and (2) above 

→≡
a
(

abp
|

b

(

)

)

                                                 
1 Here we adopt the notation used by Cox where an assertion is denoted by a lowercase Roman character, and a question is 
denoted by an uppercase Roman character.  In addition, we adopt the notation used by Fry where assertions are stated with 
exclamation marks and questions with question marks. 

 
 

 
 

 

 

are the product and sum rules, respectively, of probability theory. 

Utilizing  the  commutativity  of  the  conjunction  of  two  assertions 

cb

∧≡∧

bc

, 

equation (5) can be applied to obtain 

(

∧
acbp
|
)
abp
|

(

=
+

)

bacpabp
|(
(
=
p
)

|
)
ab
|~(

1

 

∧

)

 

∧
acbp
(
)
∧
abcp
(
)

|
|

=
=

bacpabp
)
|(
(
|
)
abpacp
c
)
|(
(
|
)

∧
∧

 
. 

abp
|

(

∧

c

)

=

abp
|

(

)

∧

bacp
|(
acp
|(
)

)

, 

(5) 
(6) 

(7) 
(8) 

(9) 

Equating the right-hand sides of (7) and (8), we obtains Bayes' Theorem 

which  allows  one  to  evaluate  the  probability  of  a  hypothesis  given  one's  prior 
knowledge  and  newly  acquired  data.    The  foundation  of  data  analysis  rests  on  this 
theorem. 

 Two important points should be noted.  First, this formalism allows one to perform 
inductive inference over a broad range of applications.  Given a set of assertions this 
calculus  allows  one  to  determine  the  relative  degree  to  which  any  assertion  implies 
any  other.    This  is  far  beyond  the  scope  supported  by  frequentist  statistics.    Second, 
there  cannot  be  implication  without  an  implicant.    In  short,  probabilities  are  always 
conditional on some state of prior knowledge. 

Deductive and Inductive Inquiry 

While  it  is  possible  to  examine  the  logical  relationships  among  what  is  known,  it  is 
equally  possible  to  examine  the  logic  of  what  is  unknown.    Cox's  second  major 
contribution  [3]  was  to  lay  the  foundations  for  the  logic  of  questions.    He  defined  a 
question as the set of assertions that answer the question.  For example, the question 
K =
?
  can  be  expressed  in  terms  of  assertions 
"
by 

kangaroo

what

state

does

live

my

In

"

!

=
k
Tasmania
"1
=
Queensland
"3
=
!
k
Victoria
"5

,"

,"
!

,"

k

K

=









=
k
"2
=
k
"4
=
k
"6
=
7
"

!
!

South

New
Western
South
Northern

Wales
Australia
!
Australia

,"
,"
,"
!
Territory

k







"


. 

(10) 

This  defining  set  of  assertions  can  be  extended  without  changing  the  question  by 
including  assertions  like  k
,  as  this  conjunction  implies  k1  and  k3,  which  are 
already  in  the  set.    A  system  of  assertions  is  a  set,  which  includes  every  assertion 
implying any assertion in the set.  The irreducible set is a subset of the system such 
that no assertion in the irreducible set implies any other in that set, except itself. 

1 k∧

3

The conjunction of two questions is called the joint question.  It asks what the two 

questions ask jointly.  In terms of assertions, the joint question can be written as 

 

∧
BA

=

(11) 

∧
∧

a
1
a
2

b
,1
b
,1

M
∧

an

b
,1









O

L

∧

a
1

b

,2

L

∧

a
1

bm
,

, 









M

∧

an

bm

which is not a matrix, but a set of all possible pairs of conjunctions of the assertions 
defining the questions A and B.  Similarly, the disjunction of two questions, called the 
common question, is defined as the question that the two questions ask in common.  In 
terms of assertions it can be answered by the union of the sets of assertions answering 
each question 
 

{
,2,1
aa

bban

,2,1,

∨
BA

}
bm

(12) 

=

. 

,

,

L

L

With these definitions, one can derive the Boolean identities for questions shown in 
Table  1.    Note  that  they  are  symmetric  with  the  relations  for  assertions  under 
interchange of disjunction 

 and conjunction 

∧

∨

. 

 

 

 

 

 

 

)

(

)

)

(

(

)

(

(

(

c

c

a

b

a

a

b

a

a

a

b

a

b

a

=

=

∨

∧

∧

∧

∨

∨

∨

(
 

(
 

~~

b
~

a
b

c
∨

(
∨

a =

(
(~

(
(~

a
(
∧
b

a
=
∨

a
(
∨
b

c
∧
b

a
=
∧

~
b
~

a
∧
c
=

a
∨
c
=

Assertions 

~
 
b
~
b

 
a
∨∨
b
∨

a
∧∧
b
∧

b
a
)~
∧
∨
)~
a
a
∧
=
)
a
b

a
b
)~
∨
∧
)~
a
a
∨
=
)
a
b

=∧
a
a
a
∧=∧
b
∧
=

b
a
∧ )
a
b
c
(
∧
∨
=
b
c
)
∧ )
b
a

=∨
a
a
a
∨=∨
b
b
a
∨
=
∨ )
b
c
∧
=
c
)
∨ )
a
b

TABLE 1.  Boolean Identities 
 
A1 
A2 
A3 
A4 
A5 
A6 
A7 
A8 
A9 
 
Q1 
Q2 
Q3 
Q4 
Q5 
Q6 
Q7 
Q8 
Q9 
 
Analogous to implication among assertions, one can define an ordering relation on 
questions, which we shall call inclusion2, such that a question A includes question B, 
=∨
A →
BBA
.  This can be more easily visualized by 
written 
B
?it
Kangaroo
is
animal
A
"
considering 
.  
B =
What
kind
"
Jointly,  the  questions  ask 
  and  in  common  the 

AAA
∨
=
∨
A
∨)
∨
BA
C
∧
=
)
C
∨ )
BA

∧
AA
∧
=
A
B
∧)
∧
BA
C
∨
=
)
C
∧ )
BA

CBA
∨
∧
∨
(
CA
CB
∨
=
B

CBA
∧
∨
∧
(
CA
CB
∧
=
B

B
)~
A
∨
∧
A
A
)~
∨
=
)
BA

A
B
)~
∧
∨
A
A
)~
∧
=
)
BA

 and 
of
A ∧

=∧
ABA
kind

, if 
=
What
"

B =
animal

  and 
of

ait
"?it

~
 
B
~
B

(
∨
BA

(
∧
BA

Questions 

Is
"
is

~
B
~

 
A
∨

 
A
∧

A
=
∧

A
=
∨

?not

(
(~

(
(~

A =

B
~

B
~

A
∧

b
~

or

~~

(
 

(
 

A

B

A

A

B

B

A

A

A

A

B

B

B

∨

∨

∨

∧

∧

∧

∧

∨

"

=

=

=

=

=

=

a

(

(

)

(

)

)

)

(

(

(

 

 

 

 

 

 

 

 

 

                                                 
2 It was suggested by Anton Garrett at the MaxEnt 2000 workshop that the relation 

A →

B

 be read as A includes B. 

=∨

BA

  Thus  question  A  includes 
questions  ask 
question B.  This can also be verified by considering the questions in terms of the set 
of assertions that answer them. 

Kangaroo

?not

ait

or

Is

. 

"

"

Inclusion  can  be  generalized  from  a  binary  relation  to  a  degree  of  inclusion 
.    This  real  number  can  be  thought  of  as 
represented  by  a  real  number  (
describing the bearing that question A has on issue B or the relevance3 of question A 
issue  B. 
this  function  as 
on 
→≡
4  Note that the position of the questions relative to the solidus "|" 
BAb
A
(
|
.
(p
is opposite of that for the definition of probability, 

  Adopting  a  notational  change,  we  denote 
)B

→≡
a
(

A →

ab
|

)B

. )

b

)

(

)

Just as with assertions, one can derive the sum and product rules for relevance 

The  commutativity  of  the  disjunction  operation  can  be  used  with  the  product  rule  to 
derive the equivalent of Bayes' Theorem for questions 

(

∨
CBAb
)
|
BAb
(
|

∨

=
CBAb
(
|
+
b
BA
~(
|
)

)

( CBb
)
|
=
1
. 

)

 

CABb
|

(

)

∨

=

CBb
|

(

)

Ab
(
b
(

CB ∨
|
CA
)
|

)

. 

RELATIONSHIPS 

(13) 
(14) 

(15) 

In this section we examine some relationships between the logic of assertions and the 
logic  of  questions.    In  doing  so  we  shed  new  light  on  the  relationship  between 
Bayesian inference and information theory. 

Bayes Theorem for Assertions, Entropy and Information Theory 

We begin with the product rule (as we did when deriving Bayes' Theorem): 

∧
hcbp
(
)
∧
hbcp
(
)

|
|

=
=

cphbp
(
|
)
(
bphcp
|(
)
(

|
|

b ∧
h
)
ch ∧
)

 
, 

(16) 
(17) 

where the assertion h is a joint or compound assertion representing all that is known.  
Taking the logarithm of both sides of equation (16) 
+

∧

=

log

∧
hcbp

(

|

)

log

hbp
|

(

)

log

bcp
|(

(18) 

h

)

, 

 

and taking the expected value over all possible assertions 

cb ∧

we find 

 
 

 

 
 

                   

                              
3 The term 'bearing' was adopted by Robert Fry who saw its long-standing use in English law as being appropriately descriptive.  
However it was brought up at the meeting that this term may be difficult for non-English speakers who do not have an obvious 
equivalent in their language and thus may find this term obscure.  The term 'relevance' was suggested as an alternative. 
4 Regardless of whether the term 'bearing' or 'relevance' is used, we adopt Robert Fry's notation for this function, which is based 
on the term 'bearing'.  This notation is especially pleasing as the letter 'b' is an upside-down 'p' (for probability), which highlights 
the symmetry between the logic of assertions and the logic of questions. 

(
bp

∧

|
hc

)

log

(
bp

∧

|
hc

)

∑
,
cb

=

∑
,
cb

(
bp

∧

|
hc

)

log

|
hbp

(

)

+

(
bp

∧

|
hc

)

log

|
bcp

(

∧

h

)

∑
,
cb

(19) 

By the sum rule of probability, the sum over c  in  the  first  term  on  the  right-hand 
side  marginalizes  to  one  leaving  only  the  sum  over  b.    One  can  easily  see  that  each 
term is negative one times some entropy.  More specifically, one can write (19) as 

cbH
),(

=

bH
)(

+

bcH
|(

)

, 

(20) 

where H(b, c) is defined in information theory as the joint entropy of b and c, H(b) is 
the entropy of b, and H(c | b) is the conditional entropy of c given b.  Application of 
the same procedure to (17), equating the right-hand sides and solving for H(b | c) gives 
+

=

−

bcH
|(

)

cH
(

)

, 

cbH
(

|

)

bH
)(

(21) 

which is the information-theoretic equivalent of Bayes' Theorem 

cbp
|

(

∧

h

)

=

hbp
|

(

)

∧

bcp
|(
hcp
|(

h
)

)

. 

(22) 

With this in mind, the notation adopted by information theory is quite pleasing as one 
can easily visualize the correspondence between equations (21) and (22). 
 

(a) 

ap ∨
(

hb
|

)

h 

(b) 
 

∧
HBAb

(

|

)

H 

hap
|

(

)

hbp
|

(

)

( HAb
|

)

( HBb
|

)

ap ∧
(

hb
|

)

 

 

 

 

 

 

 

 

 
(c) 

∨
HBAb

(

|

)

),( baH

)(aH

)(bH

 

 

 

 

 

 

 
 
FIGURE  1.    Venn  diagrams  demonstrating  the  symmetries  between  (a)  the  logic  of 
assertions and (b) the logic of questions.  (c) An I-diagram representing the analogous 
situation in information theory.  Note that in the I-diagram the function H(·) is entropy. 

);( baI

 

 

 
 

 

 
 
 

 

The Relevance - Entropy Conjecture 

As  questions  can  be  defined  in  terms  of  assertions,  one  would  expect  that  the 
relevance  or  bearing  of  one  question  on  another  could  be  expressed  in  terms  of  the 
assertions  that  answer  those  questions.  This  should  depend  on  the  probability  of  (or 
degree  of  implication  among)  those  assertions  that  answer  the  questions.    The 
symmetries  between  the  Venn  diagram  for  two  questions  (Figure  1b)  and  the  I-
diagram for entropy in information theory (Figure 1c) suggest strongly that entropy is 
the appropriate measure of relevance in terms of probability [4].  In addition, Cox [3] 
demonstrates that the properties of entropy seem to make it a convenient measure of 
relevance.  However, as no proof yet exists, it is still only conjecture that the relevance 
or bearing of a question on an issue can be written as the entropy of the probabilities 
of the assertions that answer those questions. 

It  is  interesting  to  rewrite  equation  (21)  in  terms  of  the  relevance  assuming  the 

conjecture is true 

Bb
(

∨

~

HC
|

)

=

Cb
(

∨

~

HB
|

)

HBb
|

(

)

HCb
|

(

)

, 

+

−

(23) 

where H in this equation represents the issue to be resolved.  Not surprisingly, this is a 
true equation and is easily proved using the algebra of questions or visualized in the 
Venn diagram of Figure 1b.  While this logical notation may obscure the relation of 
this  equation  to  Bayes'  Theorem  for  assertions,  it  is  much  more  easily  interpreted  in 
application [5]. 

Symmetries 

The  symmetry  between  the  logic  of  assertions  and  the  logic  of  questions  seems  to 
possess more secrets.  These relationships can be made clearer by looking again at the 
commutativity of the conjunction of assertions   

which gives Bayes’ theorem 

∧
hcbp
(
)
∧
hbcp
(
)

|
|

=
=

bhcphbp
(
|
|(
chbphcp
|(
(
|

)
)

∧
∧

)
)

 
 

chbp
|

(

)

∧

=

hbp
|

(

)

∧

bhcp
|(
hcp
|(
)

)

. 

However, the sum rule could have been applied to (24) to obtain 

bp
(

∧

hc
|

)

=
=
=

(
(
|
hbp
1)
−
hbp
(
|
)
−
hbp
(
|
)

−

∧

(~
bc
|
p
phbp
|
)
(
∧
bp
(
|~

(~
hc
)

)
 
)
h
bc
|
. 

∧

h

)

 

Applying the same procedure to (25) and equating the right-hand sides we get 

bp
(

∧

|~

hc

)

=

p

(~

∧

hcb

|

)

+

hbp
|

(

)

−

hcp
|(

)

, 

(30) 

(24) 
(25) 

(26) 

(27) 
(28) 
(29) 

which is an alternate expression resulting from the commutativity of the conjunction 
of assertions. 

The  same  game  can  be  played  with  the  commutativity  of  the  disjunction  of 

questions.  As described above, we can derive Bayes’ Theorem for questions 

(31) 
(32) 

(33) 

(34) 
(35) 

(36) 

∨
HCBb
∨
HBCb

(
(

|
|

)
)

=
=

HBCbHBb
(
HCBbHCb
(

)
)

(
(

|
|

|
|

)
)

, 
, 

∨
∨

HCBb
|

(

)

∨

=

HBb
|

(

)

∨

HBCb
(
|
HCb
|
)

(

)

, 

 
 
giving 

 
 
which gives 
 

 

 

 

which is analogous to Bayes’ Theorem for assertions (26) above.  Applying the sum 
rule for questions to (31) and (32) above we find 

∨
HCBb
∨
HBCb

(
(

|
|

)
)

=
=

∨

Bb
HC
~
(
|
∨
HCBb
(~
|

)
)

+
+

HCb
|
(
HBb
(
|

)
)

, 
, 

Bb
(

∨

~

HC
|

)

=

∨
HCBb

(~

|

)

+

HBb
|

(

)

−

HCb
|

(

)

, 

analogous to (30) above. 

Notice  that  (36)  is  the  equation  that  was  previously  suggested  (via  the  relevance-
entropy conjecture) by its information-theoretic counterpart (21) derived from Bayes’ 
Theorem  for  assertions  (26).    We  can  in  fact  perform  the  same  operations  to  obtain 
another interesting relation.  First we take the logarithm of (31) 

log

∨
HCBb

(

|

)

=

log

HBb
|

(

)

+

log

HBCb
|

(

)

, 

∨

(37) 

followed by the expected value over all possible questions 

CB ∨

 to obtain 

∧
HCBb

(

|

)

log

∧
HCBb

(

|

)

∑

,
CB
=

∑

,
CB

∧
HCBb

(

|

)

log

HBb
|

(

)

+

∧
HCBb

(

|

)

log

HBCb
|

(

)

∧

(38) 

∑

,
CB

By the sum rule for relevance, the sum over C in the first term on the right-hand side 
marginalizes  to  one  leaving  only  the  sum  over  B.    We  define  new  functions  G  such 
that the term of the left is –G(B,C), the first term on the right is –G(B) and the final 
term is –G(C|B).  Performing the same operations on (35) and equating the right-hand 
sides we obtain 
 

BCG

CBG

(39) 

BG
(

CG
(

−

+

=

. 

)

)

(

)

)

(

|

|

As expected, this is similar in form to (30) above.   

This  leads  us  to  conjecture  that  the  probability  of  an  assertion  can  be  written  in 
terms of the relevance of the questions that have that assertion as an answer.  This can 
be written explicitly as 
 

hap
|

(40) 

=

, 

(

)

)

HAG
i

({

},

where {Ai} is the set of all questions which have assertion a as their answer.  This is a 
novel conjecture analogous to the relevance-entropy conjecture, which can be written 
similarly 

HAb
|

(

)

=

aH
({

h
)},

, 

i

(41) 

)

(

)

=

},

h 

({

hap
|

HAG
i

where {ai} is the set of all assertions answering question A.  Note that the question H 
in (40) should not be confused with the function H(·).  In addition, H(·) and G(·) must 
have  the  same  form  with  the  usual  assertion-question  and  conjunction-disjunction 
interchange.  More importantly, these functions are not inverses of one another as they 
take a set of elements as arguments.  Thus schematically we have the situation shown 
in Figure 2. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
FIGURE  2.    This  is  a  cartoon  depicting  the  conjectured  relationship  between  the 
probabilities of assertions and the relevance of questions. 
 

HAb
|

( HAb
|

hap
|

aH
({

h
)},

H

=

(

)

(

)

)

i

Finally, there are two forms of relations derived from commutativity in each of the 

two spaces: the product-form for assertions  

chbp
|

(

)

∧

=

hbp
|

(

)

∧

bhcp
|(
hcp
|(
)

)

, 

known as Bayes’ Theorem, its associated sum-form for assertions 

bp
(

∧

|~

hc

)

=

p

(~

∧

hcb

|

)

+

hbp
|

(

)

−

hcp
|(

)

, 

(30) 

the product-form for questions  

HCBb
|

(

)

∨

=

HBb
|

(

)

∨

HBCb
(
|
HCb
|
)

(

)

, 

(26) 

(33) 

 

 

 

 

 
and its associated sum-form for questions  
∨

=

~

HC
|

)

Bb
(

 

∨
HCBb

(~

|

)

+

HBb
|

(

)

−

HCb
|

(

)

. 

(36) 

The product-forms in each space are analogous to one another, as are the sum-forms 
(with  interchange  of  probability-relevance,  assertion-question  and  conjunction-
disjunction).    Now  the  function  H  takes  the  product-form  for  assertions  to  the  sum-
form  for  questions,  while  the  function G  takes  the  product-form  for  questions  to  the 
sum-form for assertions. 

PRACTICALITIES 

While some may find these theoretical issues interesting, others may be wondering if 
this viewpoint has any practical use.  The multitudes of papers presented at previous 
Maximum  Entropy  and  Bayesian  Methods  workshops  have  well  demonstrated  the 
power  of  inductive  inference  when  applied  to  data  analysis.  Even  more  so,  these 
previous works demonstrate the merit and utility of the general viewpoints of Jaynes 
and  Cox  regarding  probability  as  representing  the  relative  degree  of  implication 
among logical assertions. For this reason, I refer the interested reader to another source 
[6]  for  a  detailed  description  of  the  process  of  data  analysis  using  Bayesian  or 
inductive inference. 

There  presently  exist  few  applications  that  demonstrate  inductive  inquiry  or 
inductive  logic  in  general.    Most  notable  are  the  works  of  Robert  Fry  [7-11].    In 
addition,  this  author  has  demonstrated  the  side-by-side  application  of  inductive 
inference and inductive inquiry with application to a source separation problem [5]. 

Experimental Design 

The problem of experimental design has received much less attention than the problem 
of  data  analysis.    This  is  perhaps  because  the  logic  of  questions  is  much  less 
understood  than  the  logic  of  assertions.    In  terms  of  inductive  inquiry,  the  problem 
statement and the form of its solution are straightforward.  There exists an unresolved 
scientific  issue  of  interest,  S.    For  practical  reasons  this  question  cannot  be  asked 
directly,  and  we  are  forced  to  resort  to  asking  an  experimental  question,  E,  in  an 
attempt to resolve the issue.  Out of all that can be asked, H, we focus our inquiry on 
.  More relevant experimental questions are those that have 
the scientific issue, 
greater relevance toward (or bearing on) this focused issue, written as 
.  
Using the product rule, we can write this relevance as 
∨
HSEb
|
HSb
|
)

HSEb
|

HSEb
|

HS ∨

(42) 

∨

∨

=

. 

(

)

)

)

(

(

(

 

Finding  the  experimental  question  with  maximal  relevance  requires  maximizing  this 
quantity with respect to all possible experimental questions.  Note that the term in the 
denominator does not vary as different experimental questions are considered.  Thus 
the most relevant experiment can be determined by maximizing the relevance of the 

 

 

 

 

SE ∨

.    If  the  relevance-entropy  conjecture  is  in  fact  true,  this  is 
common  question 
identical  to  maximizing  the  mutual  information  between  the  experimental  question 
and  the  scientific  issue.    The  process  of  experimental  design  could  then  be  viewed 
information-theoretically  as  the  process  of  designing  a  communication  channel 
between the system of interest and the experimenter. 

From (42), to maximize the relevance of the experimental question to the scientific 

issue, one must maximize the relevance of the common question 
=
∧
HSEb

∨
HSEb

HEb
|

HSb
|

−

+

)

(

)

(

(

)

(

 

|

|

)

. 

(43) 

By re-writing the conjunction on the right-hand side, we get 

∨
HSEb

(

|

)

=

HSb
|

(

)

HEb
|

(

)

+

−

(
HSb
|

(

)

+

Eb
(

∧

~

HS
|

))

,  (44) 

which simplifies to 

∨
HSEb

(

|

)

=

HEb
|

(

)

−

Eb
(

∧

~

HS
|

)

. 

(45) 

This  can  be  written  in  terms  of  the  probabilities  of  the  assertions  that  answer  the 
experimental question and scientific issue.  The possible answers to the experimental 
question  are  a  set  of  statements  describing  the  data  that  could  be  recorded.    Any 
particular set of data will be denoted by the joint assertion ei.  Similarly, the possible 
answers to the scientific issue are a set of statements describing the possible models 
for the physical situation under consideration.  Any particular model will be denoted 
by the joint assertion sj. 

If the relevance-entropy conjecture is correct, the relevances in (45) can be written 
in terms of the entropy of the probabilities of the experimental and scientific answers.  
This is equivalent to (in information-theoretical notation) 

SEI
(
;

)

=

EH
(

)

−

SEH
(

|

)

, 

(46) 

which after simplification gives 
∨
HSEb

∑−=

)

(

|

hep
i

(

|

log)

hep
i

(

|

)

i
−−

∑
j

sp
(

j

|

h

)

ep
(

i

|

s

j

h

log)

ep
(

|

s

j

i

∧

h

)

∑
i

. 

(47) 

+

∧

Examining the terms on the right, one can see that this result is quite intuitive.  To find 
an experimental question that has greatest relevance to the scientific issue at hand, one 
must  choose  an  experiment  that  has  two  qualities.    First,  the  experiment  should 
maximize  the  entropy  of  the  set  of  possible  results  (first  term).    In  other  words,  the 
experiment  should  be  maximally  unbiased.    Second,  the  entropy  of  the  likelihood 
function  summed  over  all  possible  scientific  scenarios  should  be  minimized  (second 
term).  This means that a good experiment will result in data that provides the sharpest 
estimates of the model parameters on average.  While these ideas are not new to the 
problem  of  experimental  design,  seeing  them  derived  here  using  inductive  logic  and 
the  relevance-entropy  conjecture  is  quite  satisfying.    For  example,  one  standard 
technique in experimental design is to simulate one of the possible physical situations 
and  choose  an  experimental  design  that  minimizes  the  variance  of  the  likelihood 
function [6].  This is an approximation to the second term derived above. 

OPEN QUESTIONS 

Richard  Cox's  work  was  essential  in  putting  modern  probability  theory  on  a  firm 
ground based on sound logical principles.  In addition, we were fortunate to have as 
his last work a glimpse into the logical duality between assertions and questions thus 
opening  a  broader  scope:  logical  induction.    This  glimpse  suggests  a  richer 
relationship between probability and entropy. 

One  of  the  current  difficulties  is  that  it  is  yet  unclear  how  to  completely  relate 
these  two  spaces  to  one  another.    It  is  expected  that  the  relevance,  or  bearing,  of  a 
question must be expressible in terms of the probabilities of the assertions that answer 
that question via some function H.  While much evidence suggests that this function 
may be the entropy, this is not yet proven.  However, something in this resonates with 
intuition.  Probability describes the degree of certainty, whereas entropy describes the 
degree of uncertainty.  Again we have what is known versus what is unknown.  More 
unusual  is  the  hypothesized  relationship,  denoted  by  the  function  G,  which  takes 
relevance to probability.  The duality between the spaces suggests that this function G 
has  the  same  form  as  H,  with  the  usual  probability  interchanged  with  relevance, 
assertions 
interchanged  with 
disjunctions.    However,  this  actually  depends  on  whether  the  space  of  assertions  is 
isomorphic to the space of questions.  This is not immediately obvious even given the 
duality  we  have  explored.    Clearly  more  investigation  is  needed  to  fully  explore  the 
structure of these spaces. 

interchanged  with  questions  and  conjunctions 

Finally, playing with questions proves to be quite difficult at first, as the intuition 
seems  to  be  lacking.5    This  may  explain  our  reliance  on  the  function  H,  which 
translates everything back to assertions where we are more comfortable.  However, it 
should be possible to derive quantities like prior relevance and to perform calculations 
in question space without ever resorting to assertions.  There may be some fascinating 
research here. 

ACKNOWLEDGMENTS 

I  would  like  to  thank  Robert  Fry  for  introducing  me  to  Richard  Cox's  work  in 

inductive inquiry and for exciting discussions on potential applications of this theory. 

REFERENCES 

1.  Cox R.T. 1946. Probability, frequency, and reasonable expectation, Am. J. Physics, 14:1-13. 
2.  Cox R.T. 1961. The Algebra of Probable Inference, The Johns Hopkins Press, Baltimore. 
3.  Cox R.T. 1979. Of inference and inquiry, In Proc. 1978 Maximum Entropy Formalism Conference, 

MIT Press, pp.119-167. 

4.  Fry  R.L.  Electronic  course  notes,  525.475  Maximum  Entropy  and  Bayesian  Methods,  Johns 

Hopkins University, available from the author. 

                                                 
5 Guillaume Marrelec and I spent several hours during the workshop trying to get our heads around a simple problem dealing with 
the relations between the questions one can ask regarding the state of a four-sided die. 

5.  Knuth K.H. 2001. Source separation as an exercise in logical induction, In Bayesian Inference and 
Maximum  Entropy  Methods  in  Science  and  Engineering.  Proceedings  of  the  20th  International 
Workshop,  Gif-sur-Yvette,  France,  Ed.  A.  Mohammad-Djafari,  American  Institute  of  Physics, 
Melville NY, pp.340-349. 

6.  Sivia D.S. 1996. Data Analysis. A Bayesian Tutorial, Clarendon Press, Oxford. 
7.  Fry R.L. 1995. "Observer-participant models of neural processing", IEEE Trans. Neural Networks, 

6:918-28. 

8.  Fry  R.L.  &  Sova  R.M.  1998.  "A  logical  basis  for  neural  network  design",  in:  Techniques  and 

Applications of Artificial Neural Networks, Vol. 3, Academic Press. 

9.  Fry R.L. 1999. "Constructive bases for BMD algorithm design and adaptation", BMDO Battlespace 

Study, Phase III Final Report. 

10. Fry  R.L.  1998.  "Transmission  and  transduction  of  information",  presented  at  1998  Workshop  on 

Maximum Entropy and Bayesian Methods, Garching, Germany. Available from the author. 

11. Fry R.L. 2000. "Cybernetic systems based on inductive logic", In Bayesian Inference and Maximum 
Entropy Methods in Science and Engineering. Proceedings of the 20th International Workshop, Gif-
sur-Yvette, France, Ed. A. Mohammad-Djafari, American Institute of Physics, Melville NY, pp.106-
19. 


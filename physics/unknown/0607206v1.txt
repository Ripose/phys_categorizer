 

Effective memory of the minority game 

Chia-Hsiang Hung and Sy-Sang Liaw 
 

Department of Physics, National Chung-Hsing University, 250 Guo-Kuang Road, Taichung, Taiwan 
 
 
 

Abstract 
 

It is known that the memory is relevant in the symmetric phase of the minority 

game.  In  our  previous  work  we  have  successfully  explained  the  quasi-periodic 

behavior of the game in the symmetric phase with the help of the probability theory. 

Based  on  this  explanation,  we  are  able  to  determine  how  the  memory  affects  the 

variance  of  the  system  in  this  paper.  By  using  some  particular  types  of  fake  history 

such as periodic type and random type, we determine how efficient the memory has 

been  used  in  the  standard  game.  Furthermore,  the  analysis  on  the  effective  memory 

strongly supports the result we proposed previously that there are three distinct phases 

in the minority game. 

 
 
 
 
 
 
PACS numbers: 02.50.Le, 05.65.+b, 05.70.Fh, 87.23.Ge 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

Introduction 

I. 
 

The minority game (MG)[1] is a simple model that can capture the essence of 

some features of the financial markets. The game is played by N agents in choosing 

one of two decisions. Those who are in the minority are rewarded one point. Suppose 

these N agents play the game many times, what outcome would be expected? Is there 

any statistical pattern? Can one of the agents manage to gain most? To answer these 

questions we need to take data from a MG played by real people which waits to be 

done[2]. Current study of the MG is to simulate the game on computers by assuming 

that each agent has a given type of strategy in choosing a decision. The standard MG 

[1] assumes that at each time, the winning decision of the last M time steps is known 

to each agent and each agent uses this information to make his decision according to 

one of his S strategies set up in the beginning of the game. A strategy is nothing but a 

list of all possible pieces of information with an arbitrary decision assigned to each of 

them. The parameter M is called the memory. A common piece of information consists 

of the winning decisions of last M time steps is called a history. There are 

P 2=

M

 

possible histories in a strategy. Agents evaluate their strategies by giving a virtual 

point to all strategies that have matched the winning decision at a time step, and use 

the best-scored one among his S strategies to make decision. 

As one simulates the standard MG many times, the average of the number of 

winning agents is found to be, as expected, N/2. The variance per agent of winning 

population  χ, on the other hand, has an interesting dependence on the parameter 

NP /=α

[3] that attracts many researchers. Cavagna[4] claimed that the global 

properties of  χ  is irrelevant to the information of the winning history by simulating 

the game with a random history at each time step and found the results were the same. 

Further quantitative studies[5] showed that the variance were not quite the same when 

different histories (winning history or random history) were used. Recently, Ho et al[6] 

has carried out a large scale numerical simulation to calculate  χ  in the 

1<<α

 

region, called symmetric phase[7,8]. Results of using winning and random histories 

differ substantially in the symmetric phase. 

People have found the behavior of the MG very interesting in the symmetric 

phase. In particular, the winning population changes quasi-periodically with a period 

of 2P[9,10,11]. It is now understood[12] that when  α  is very small, all strategies 

will be almost guaranteed to gain the same (P) points at the end of time step 2P so 

that at the time step 

2 +P

1

  no strategy is preferred and the system is like going back 

to time step 1. Liaw et al[13] has calculated analytically the critical value 

1cα   such 

that in the region 

1cαα<

  the quasi-periodic behavior would appear. They employed 

a simple series of histories to explain the quasi-periodic behavior and calculated 

1cα . 

We will introduce this series of histories in Sec. II and compare its values of  χ  with 

results of using true histories. We define effective memory in Sec. III and then 

calculate it for different types of rules in the symmetric phase. Sec. V is conclusion. 

 

II. 
 

Three kinds of history-updating rules 

Let two possible decisions be 0 and 1. A history is then an M-digit binary number. 

At each time step, in the standard game the history is updated by dropping its first 

digit and attaching the winning decision to the end. We call this the standard updating 

rule for the history. In the standard updating rule, exactly two histories can be updated 

to each history, and each history can be updated to exactly two possible histories. A de 

Bruijn graph shows all the paths of the standard updating rule(Fig. 1a). One can see 

from Fig. 1a there are many loops in a de Bruijn graph. If instead an arbitrary history 

is given at each time step, we call it the random updating rule. Any history can follow 

a history with probability 

P/1

. Random rule has been used by Caragna[4] to show 

the variation of  χ  with respect to  α  is qualitatively the same with that of using 

standard rule. Challet et al[14] has developed an exact solution for MG using random 

rule. A third updating rule, introduced by Liaw et al[12], is all possible histories take 

turns to appear regardless of what winning decisions are. If we order all possible 

histories by their decimal values, the rule can be arranged simply as history 0 is 

followed by history 1, and history 1 followed by 2, etc., and history P followed by 0. 

We call this the periodic updating rule whose path has only one loop(Fig. 1b). The 

periodic rule is simple and suitable for analytic analysis[12,13]. It can also be 

extended to study so called Thermal minority game[15]. Other updating rules are also 

available, but we will consider only the standard, random, and periodic rules in this 

paper. 

Fig. 2 is a plot of typical results of variance per agent  χ  as a function of  α 

using the standard, random, and periodic rules respectively. We see that results of 

these three rules are qualitatively alike. We separate the plot into three regions by two 

dotted line: 

1cαα=

  and 

2cαα=

. 

2cα   is defined as the critical value of  α  at 

which  χ  has its minimum. It is not clear whether 

2cα   has a common value for 

three different rules. For  α  larger than 

2cα ,  χ  monotonically increases and 

approaches ¼ which is the result of a random system—every agent uses random 

strategy at each time. For  α  smaller than 

1cα ,  χ  is larger than ¼ and 

approximately proportional to  α/1

[13]. In this region the values of  χ  depend on 

the rules strongly. In the second region, as  α  increases from 

1cα   to 

2cα , the value 

of  χ  decreases, drops below ¼ and reaches its minimum. Behavior of the system in 

this transition region is very interesting but hardly studied. In what follows we will 

focus on the symmetric phase 

1cαα<

  only. 

 

III.  Definition of effective memory 
 

It is known that the system shows a quasi-periodic behavior in the first region 

1cαα<

  no matter what rule is used. From Fig. 2 we see that the log-plot of  χ 

versus  α  are roughly parallel in the first region and the order of the values of  χ  is 

χχχ
pd

<

<

rd

st

  for a given P. We may consider the  χ  versus  α  curve as a 

demand curve in microeconomics. When the demand decreases, the curve shifts to the 

left. Thus if we define the demand (of information) as the effective information  P′ , or 

equicalently, effective memory 

M

=′

log 2 P
(

′
)

  of the game, we expect 

′<′<′
P
P
P
rd
st
pd

  and  χ  (the price) will be a monotonic increasing function of  P′ . 

To find a suitable definition for  P′ , we simply construct a relation between  χ 

and  P′ . As we play the game again and again, when a history appears a second time, 

the path of the histories—the history and all histories between its two consecutive 

appearances, forms a loop. It was found[16,12] for small  α  case that when a loop is 

found, or equivalently, when a history appears a second time, the winning decision is 

most likely be different from that of the last time when the history appeared. The 

quasi-periodic behavior is result of this property. In Ref. [12] we have shown that 

when a history appears a second time, the population variance will shift away from 0 

by a value which is inversely proportional to the number l of histories that have 

appeared odd times. And the average variance per agent over a quasi-period 2P is 

given by[13] 

 

 

 

 

 

χ

c
∑+=
S
α

1
8

l

1
l

                                                                                              (1) 

where 

Sc   is a constant dependent on the number of strategies S, and 1/8 comes from 

the contributions of P time steps when system behaves in random fashion. The 

summation is over all loops. Let us consider the case of using periodic rule first. At 

time step 

1+P

, the history that appears in the time step 1 appears the second time, 

and all P histories have appeared once right before the loop is closed. At time 

step

2+P

, the second history appears the second time, and 

1−P

  histories have 

appeared once. In the same manner, the number l of histories that have appeared once 

when a loop is just to be closed decreases by 1 in the subsequent time steps. At time 

step 2P, l is equal to 1. Consequently, average variance per agent over a quasi-period 

2P is given by 

 

χ
pd

1
+=
8

c
S
α

P

∑

l

1
=

1
l

1
+≈
8

c
S
α

 

(log

P

+

)
γ

                                                          (2) 

where  γ  is Euler’s constant: 

.0=γ

5772

. For fixed P, 

8/1−pdχ

  is proportional to 

1−α , as expected from numerical simulation. For fixed  α, 

8/1−pdχ

  is 

proportional to 

γ+Plog

. Because the periodic rule is the simplest rule for analysis, 

we use it as a base for comparison. That is, we define 

Ppd =′

P

. We then calculate 

rdχ   and 

stχ   and assume they have the form Eq. (2) with the effective information 

rdP′   and 

stP′   respectively.   

IV.  Calculation of the effective memory 
 

As we play the game again and again, there will be loops in the path of 

histories. Let us define the time-step difference between two consecutive appearances 

of a history the length of the loop. A longer loop means there are more histories have 

appeared odd times when the end of the loop is reached. And we have seen in the last 

section that χ  is affected by the number of histories have appeared odd times when a 

particular history appears a second time. Therefore, the knowledge of the distribution 

of the lengths in a game is important to understand the variance.   

When we use the periodic rule to update the history in playing the game, it is 

very easy to see that all loops have the same length, which is simply equal to P. If 

either the standard rule or random rule is used, the lengths of loops vary. We plot the 

frequency versus length ranging from 1 to 2P in Fig. 3 by playing the game 2P time 

steps and taking average of 650 runs. The length-distribution plot for the case of 

random rule can be calculated as follows. The probability of length 1 is 1/P because 

there is only one choice out of P that matches current history. The probability of 

length 2 is the probability of choosing a history not the same as the current one in the 

second time step and getting the current history again in the third time step. It is 

. Similarly, we can obtain the probability of length x: 

P

−
P

11
⋅
P

 

p

rd

x
)(

=

(

1

)

x

1
−

(

P

−
P

1
P

)

                                                                                  (3) 

At each of 2P time steps we can check whether a length-1 loop will appear. However, 

only the first 

xP −2

  time steps are possible to be the beginning of a length-x loop. 

Thus the length distribution is given by 

           

g

)(
x

=

2(

xP
−

)

⋅

p

)(
x

=

1(2

−

rd

rd

1)(

−

x

1
−

)

                                            (4) 

x
2
P

1
P

The average length for random rule can be found for large P to be 

           

x

rd

xg

rd

)(
x

dx

≈

P

≈

.0

477

P

                                                          (5) 

2

P

= ∫

0

4
+

1

2

e

  When the standard updating rule is used, each history is followed by one of two 

fixed histories (Fig. 1a). If we order all histories according to their decimal values 

i

=

,2,1,0

,

P

−

1

, we see that the i-history is followed by either 2i mod P or 

2 +i

1

 

L

mod P(Fig. 1a). If we start from i-history, what is the probability that the i-history will 

appear again after x time steps[5]? A moment’s reflection shows that the final history 

is given by 

ix +2

j

  mod P, with j ranging from 0 to

2 −x

1

. Assume there are s final 

 

 

 

 

 

 

 

 
 

 

values among all 

x2   possible values satisfying   

ix +2

j

  mod P i= , which is 

equivalent to 

               

x

2(

− )1
i

+

j

  mod P 

0=                                                                             (6) 

For the small  α  case, because its quasi-periodic behavior, most histories appear the 

same times within first 2P time steps (Fig. 4). It is thus a good approximation that the 

probability of length x is related to the average of s over all possible i, denoted by

)(xs

. 

Now, i can be one of 0, 1, 2, …, P-1, so the value 

x

2(

− )1
i

+

j

  can be any integer in 

0, 1, 2, …, 

x

2(

−

)1

P

. It is easy to see that there are 

x2   values among them are 

multiple of P. So we have 

)(
xs

=

x /2

P

. Notice these 

x2   solutions for length x have 

included all solutions of smaller lengths which are factors of x. So we have to subtract 

them from 

x2   in obtaining the probability of length x. For example, the probabilities 

of length 1 to length 6 are given below. 

               

                                                    (7) 

p

st

)1(

=

1
)2(

=

1
P

p

st

)2(

=

2

2(

−

1
)2

=

p

st

)3(

=

3

2(

−

1
)2

=

p

st

)4(

=

4

2(

−

2
)2

=

p

st

)5(

=

5

2(

−

1
)2

=

2

1
1
P
1
2
P

2

2

2

2

2

1
3
P
1
4
P

1
5
P
1
6
P

1
2
P
3
4
P
3
4
P
15
P
16

p

st

)6(

=

6

2(

3

−

2

2

−

2

+

1
)2

=

27
P
32

These values are consistent with the numerical results shown in inset of Fig. 3b. The 

probability of length x is close to 1/P for large x because the contributions from its 

factors are very small comparing to 

x2 . To obtain the length distribution one has to 

take two factors into consideration. First, the probability 

)(x

  has to be weighted 

pst

by the factor 

xP −2

  as explained in the random case. Second, in the standard 

updating rule, when a history appears first time, it is followed by one of two possible 

histories. In the case of small  α  that we consider here, when the history appears the 

second time, it will be followed by the other history of the two[16,12]. That is, two 

appearances of a history compensate for each other in determining the probability of 

finding a loop of length x. Thus we have to weight the distribution by a factor 1/2. The 

length distribution is then given by 

               

g

st

x
)(

2(

xP
−

)

⋅

p

)(
x

st

1
−≈
1
>>

x

                                                      (8) 

1
⋅=
2

x
2
P

The average length for the standard rule is 

   

x

st

xg

st

)(
x

dx

≈

                                                                              (9) 

2

P

= ∫

0

2
P
3

From Eqs. (5) and (9), we have 

x

rd

<

x

st

<

x

pd

=

P

, with its order consistent with 

order of   

rdχ ,

stχ ,

pdχ   shown in Fig. 2. 

To calculate 

rdχ and stχ according to Eq. (1), we need to know the distribution 

of l for standard and random rules: 

)(lhst

  and 

hrd

)(l

. Results of simulations, 

averaging 1000 runs, are plotted in Fig. 5. The values for 

rdχ and stχ   are therefore 

given by 

and 

             

χ
st

1
=−
8

c
S
α

P

∑

l

1
=

lh
)(
st
l

≡

c
S
α

(log

+′
P
st

)
γ

                                                      (10) 

               

χ
rd

1
=−
8

c
S
α

P

∑

l

1
=

h
rd
l

l
)(

≡

c
S
α

(log

+′
P
rd

)
γ

                                                  (11) 

From Eqs. (10), (11), we can determine effective information for standard and random 

rules. Results of 

rdP′   and 

stP′   for some different values of P are listed in Table 1. For 

fixed N, P can not be too small in order the statistical analysis to be valid. P can not 

be too large either so that  α  will not leave symmetric phase[17]. 

 

 

 

 

 

 

 

 
 

Table 1    Effective information 

(

=N

20001

, 

2=S

, average of 5000 runs) 

64

128

512

256

16  32
P 
pdP′   16  32
128
rdP′   3.9  4.0 4.1 4.1 
4.1 
stP′   5.0  6.4 8.0 10.4 14.0 18.9 24.2

512

4.1 

256

4.1 

64

1024

1024

 
 
As a test for our values of effective memories given in Table 1, in Fig. 6 we plot 

rdχ and stχ   versusα  for 

64=P

. The former coincides very well with the curve 

of 

pdχ   for 

4=P

  as predicted in Table 1. The latter coincides better with the curve 

of 

pdχ for 

10=P

  instead of 

8=P

  as predicted in Table 1. Qualitatively, the 

calculations of the effective memory are satisfactory. 

From Table 1 we found the results of effective information are very interesting. 

The effective information for random rule 

rdP′   is almost independent of P. In terms 

of effective memory, 

2≈′rdM

  for any M. On the other hand, the effective 

information for standard rule 

stP′   is a monotonic function of P. The corresponding 

effective memory has a linear relation with M: 

M st

=′

384.0

M

+

.0

742

(Fig. 7). 

Further investigation is in order. 

Conclusion 

 
V. 
 
        We have simulated the minority game in the symmetric phase. The results 

showed that the variance of winning population  χ  in this phase is approximately 

inverse-proportional to information per agent per strategy  α. We used three kinds of 

rules-- standard, random, and periodic rules--to update the history and found their 

results were qualitatively similar but differed in quantity substantially in the 

symmetric phase. We defined the effective memory for each updating rule based on a 

previous analysis to quantify their differences and showed how the results were 

dependent on the memory.   

 

 
 
 
 

References 
 
[1] D. Challet and Y.C. Zhang, Emergence of cooperation and organization in an 

evolutionary game, Physica A246, 407-418 (1997). 

[2] An experiment was carried by T. Platkowski and M. Ramsza, Playing minority 

game, Physica A323, 726 (2003). Because there were only few (15) agents 

playing the game, their results offered limited information. P. Laureti, P. Ruth, J. 

Wakeling, and Y-C Zhang, Physica A331, 651 (2004) designed an interactive 

game to study how real agents among many other computer agents can affect the 

results of the game. 

[3] R. Savit, R. Manuca, and R. Riolo, Adaptive competition, market efficiency, and 

phase transitions, Phys. Rev. Lett. 82, 2203 (1999). 

[4] A. Cavagna, Irrelevance of memory in the minority game, Phys. Rev. E59, R3783 

[5] D. Challet and M. Marsili, Relevance of memory in minority games, Phys. Rev. 

(1999). 

E62, 1862 (2000). 

[6] K.H. Ho, W.C. Man, F.K. Chow, and H.F. Chau, Memory is relevant in the 

symmetric phase of the minority game, Phys. Rev. E71, 066120 (2005). 

[7] M.A.R. de Cara, O. Pla, and F. Guinea, Competition, efficiency and collective 

behavior in the “El Farol” bar model, Eur. Phys. J. B10, 187 (1999). 

[8] D. Challet and Y.C. Zhang, On the minority game: analytical and numerical 

studies, Physica A256, 514-532 (1998). 

[9] D. Zheng and B-H. Wang, Statistical properties of the attendance time series in the 

minority game, Physica A301, 560 (2001). 

[10] C.-Y. Lee, Is memory in the minority game irrelevant?, Phys. Rev. E64, 015102 

(2001). 

[11] C.-L. Hsieh, H.-C. Tseng, and H.-J. Chen, Emergence of a periodic profile of the 

time series of population in the minority game, Int. J. Mod. Phys. B18, 3015 

(2004). 

[12] S.S. Liaw and C. Liu, The quasi-periodic time sequence of the population in 

minority game, Physica A351, 571 (2005). 

[13] S.S. Liaw, C.H. Hung, and C. Liu, Three phases of the minority game, to be 

published in Physica A (2006). 

[14] D. Challet, M. Marsili, and R. Zecchina, Statistical mechanics of systems with 

heterogeneous agents: Minority game, Phys. Rev. Letts. 84, 1824 (2000). 

[15] A. Cavagna, J.P. Garrahan, I. Giardina, and D. Sherrington, Thermal model for 

adaptive competition ina market, Phys. Rev. Letts. 83, 4429 (1999). 

[16] R. Manuca, Y. Li, R. Riolo, and R. Savit, The structure of adaptive competition in 

minority game, Physica A282, 559-608 (2000). 

[17] According to analytical results of Ref. [13], for

2=S

case, we have 

α

=

/
NP

<

α
c
1

=

052.0

. Thus for 

=N

20001

, the value of P for our analysis to 

be valid has to satisfy 

P

<

cα

1

N

=

1040

. 

 

 

 

 

 

 

 

 

 

Figure captions 

Fig. 1 

(a) Standard game uses de Bruijn graph to update its history. A history i can be 

updated to history 2i or 

2 +i

1

  mod 

P 2=

. Here 

3=M

.   

M

(b) In the periodic rule, a history i is updated to 

1+i

  mod P regardless of the 

winning results. 

Variance per agent  χ  versus information per agent  α  using three different 

updating rules: periodic(triangle), standard(circle), and random(square). These curves 

show distinct behavior in each of the three regions separated at 

1cα   and 

2cα .   

Histogram(gray) of the length-distribution of the loops in a quasi-period 2P for (a) 

random rule and (b) standard rule. Thick lines are analytical results. Inset of Fig. 3(b) 

shows details of numerical results (black) for 6 short lengths. Analytical results (white) 

of Eq. (7) are also shown for comparison. 

A typical plot of appearing frequency versus history in a quasi-period 2P for standard 

 

 

 

 

Fig. 2 

Fig. 3 

Fig. 4 

rule.   

Fig. 5 

Distributions of the number of histories(l) that appear odd times when a loop is found 

for random rule(square) and standard rule(circle). 

χ  versus  α  in the symmetric region. Three solid lines are results using periodic 

rule with 

64=P

, 10, 4. Simulation results of standard rule(circle) and random 

rule(triangle) with 

64=P

  show to have effective information 

10=′stP

  and 

4=′rdP

  respectively. 

Effective memory of standard rule 

stM ′   has a linear relation with the memory M. 

Fig. 6 

Fig. 7 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Fig. 1a                                                                   

 

 

 

Fig. 1b 

Fig. 2 

s
e
m

i
t

1.0

2.0

1.5

0.5

0.0

     
 
 

 
Fig. 3a                                              Fig. 3b 

50

100

150

200

250

length

 

 

Fig.4 

s
e
m

i
t
 
e
g
a
r
e
v
a

3.0

2.5

2.0

1.5

1.0

0.5

0.0

 
Fig. 5 

s
e
m

i
t

4.0

3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0

-0.5

Fig. 6 

 
 
 

0

10

20

30

40

50

60

entrances

 

0

10

20

30

40

50

60

70

l

 

 

Fig. 7 
 

4.5

3.5

4

3

y
r
o
m
e
m
 
e
v
i
t
c
e
f
f
E

2.5

4

5

6

7
Memory

8

9

10
 


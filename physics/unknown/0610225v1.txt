 

 
 

 

 Emergence of Human Cooperation and Altruism  
by Evolutionary Feedback Selection 

D. Darcet1 and D. Sornette2 
1Insight Research LLC, Los Angeles, California, USA 

2Department of Management, Technology and Economics 
ETH Zurich, CH-8032 Zurich, Switzerland 
ddarcet@noos.fr and dsornette@ethz.ch 

 
 
Abstract:  Strong  reciprocity  is  a  fundamental  human  characteristic  associated  with 
our extraordinary sociality and cooperation. Laboratory experiments on social dilemma games 
and many field  studies have quantified well-defined levels of cooperation  and propensity to 
punish/reward.  The  level  of  cooperation  is  observed  to  be  strongly  dependent  on  the 
availability of punishments and/or rewards. Here, we suggest that the propensity for altruistic 
punishment  and  reward  is  an  emergent  property  that  has  co-evolved  with  cooperation  by 
providing an efficient feedback mechanism through both biological and cultural interactions. 
By favoring high survival probability and large individual gains, the propensity for altruistic 
punishment  and  rewards  reconciles  self-  and  group  interests.  We  show  that  a  simple 
cost/benefit analysis at the level of a single agent, who anticipates the action of her fellows, 
determines  an  optimal  level  of  altruistic  punishment,  which  explains  quantitatively 
experimental  results  on  the  third-party  punishment  game,  the  ultimatum  game  and  altruistic 
punishment  games.  We  also  report  numerical  simulations  of  an  evolutionary  agent-based 
model of repeated agent interactions with feedback-by-punishments, which confirms that the 
propensity  to  punish  is  a robust  emergent  property  selected  by  the  evolutionary rules  of the 
model. 
 
.  
 
1-Introduction 
The  potential  for  cooperation  is  everywhere  in  nature,  yet  evolution  seems  to  rarely 
 
take advantage of it. When it does - social insects, multi-cellular organisms, human societies - 
the results can be spectacularly successful. One of the most striking characteristics of Homo 
sapiens  is  our  sociality  and  cooperation  [1].  Social  relationships  pervade  every  aspect  of 
human  life  and  these  relationships  are  probably  far  more  extensive,  complex,  and  diverse 
within and across societies than those of any other species. Several theories have attempted to 
explain the puzzle of human cooperation. (1) Evolutionary kin selection [2] roots behavior to 
genetic  relatedness  using  Hamilton’s  notion  of  inclusive  fitness.  (2)  Costly  signaling  theory 
[3,4]  and  indirect  reciprocity  [5-7]  base  cooperation  in  large  groups  on  the  build-up  of  the 
reputation  of  cooperators.  (3)  Reciprocal  altruism  or  direct  reciprocity  [8-10]  derives 
cooperation from selfish motives in the presence of long-term repeated interactions.  
However,  a  growing  number  of  experiments  [11]  have  documented  behaviors  which 
 
seem incompatible with the self-regarding nature of humans involved in these mechanisms (1-
3):  humans  show  a  predisposition  to  cooperate  in  unrepeated  interactions  with  strangers  at 
their own cost. (4) Strong reciprocity [12] proposes that humans exhibit an inclination to both 
reward  others  for  cooperative  norm-abiding  behaviors,  and  to  punish  others  for  norm 
violations  (called  respectively  “altruistic  rewarding”  [11]  and  “altruistic  punishment”  [13]). 

1 

This provides the potential for group selection and culture evolution [14] above the individual 
level  as  a  mechanism  for  spontaneous  cooperation  within  human  groups,  because  of  the 
competitive advantage such cooperation affords. However, at the same time, groups face the 
problems of internal competition between individuals (or subgroups) and of the free-rider, to 
which altruistic punishments/rewards constitute a possible remedy.  
 
Economics  traditionally  conceptualizes  a  world  populated  by  selfish  individualistic 
agents (Homo Economicus) behaving fundamentally so as to maximize their own well-being. 
In contrast, the theory of strong reciprocity posits that humans are still self-centered but also 
inequity  adverse,  that  is,  their  utility  function  includes  terms  sensitive  to  unfairness.  But 
where  does  this  sense  of  altruistic  reward/punishment  originate?  Here,  we  develop  a  theory 
deriving  strong  reciprocity  as  resulting  from  the  evolutionary  selection  of  self-centered 
humans interacting in groups and subjected to feedbacks within groups resulting in particular 
from  rewards  and  punishments.  The  theory  of  evolutionary  feedback  selection  allows  us  to 
explain  quantitatively  the  experimental  results  on  third-party  punishment  games,  on  the 
ultimatum game and on altruistic punishment games. We also report numerical simulations of 
a simple evolutionary agent-based model of repeated agent interactions with feedbacks, which 
supports  evolutionary  feedback  selection,  and  suggests  that  the  propensity  to  punish  can  be 
seen  as  an  emergent  property  [15,16].  The  theory  of  evolutionary  feedback  selection 
emphasizes  an  evolutionary  mechanism,  in  which  the  trait  of  providing  certain  levels  of 
feedbacks  is  selected  for  and  co-evolves  with  an  increasing  individual  fitness,  which  is 
mediated by the collective output of the group. 

Consider  for  instance  Fehr  and  Gächter’s  experiments  on  altruistic  punishments 
[13,17], where the adjective ``altruistic’’ refers to a costly behavior with no personal material 
gain. They show that cooperation flourishes if altruistic punishment is an option, and breaks 
down if it is ruled out. They suggest that altruistic behavior in simple dilemma games may, in 
fact, obey rational choice theory, if people indeed pursue altruistic aims, that is, if their utility 
function includes terms sensitive to unfairness [18]. Instead of postulating the co-existence of 
altruistic and selfish aims at the individual level, the theory of evolutionary feedback selection 
considers  humans  as  following  a  consistent  approach  towards  survival  and  reproduction. 
Human  survival  and  reproduction,  like  that  of  any  other  species,  is  the  result  of  an 
evolutionary  process.  As  social  animals,  it  is  likely  that  human  evolution  has  selected 
instinctive and emotional behaviors [19-21], which promote better survival and reproduction 
abilities  through  collective  actions.  Evolutionary  feedback  selection  is  a  simple  bounded 
rational  choice  theory,  based  on  evolutionary  psychology  [22,23,24],  whose  rationality  can 
only be ultimately understood at the group level, through a kind of “renormalization” by the 
emergence  mechanism  [15,16].  The  feedback  levels  through  rewards  and  punishments  are 
proposed to have co-evolved with emotions to ensure high probable survival as well as large 
collective gains (reproduction success). The potential gain of altruistic reward and the risk of 
altruistic  punishment,  that  anyone  can  perceive  when  acting  fairly  and  unfairly,  serve  as 
constraints  to  one’s  utility  function,  promoting  better  consistency  between  selfish  aims  and 
group efficiency. It becomes in anyone’s interest to act fairly under the feedbacks provided by 
the  anticipation  of  other  group  members’  altruistic  reward  and  the  threat  of  other  group 
members’  altruistic  punishment.  Altruism  is  neither  irrational  nor  rational  at  the  individual 
level; it’s an emotion, mental state that arises spontaneously, promoting socialization, thereby 
group efficiency, cascading to individual efficiency.  

We first propose a general formulation of these concepts, which is then applied to the 
third-party punishment game, to the ultimatum game and to altruistic punishment games. In 
the first two games, unfairness appears in the payoffs while the contributions of the agents are 
irrelevant. In the third game, the payoffs are equal but inequity appears via possible different 
contributions to the common project. 

 

2 

 

 

 

 

 

(1a) 
(1b) 

 
for all i’s. 

Pi(0, 0, …, 0, ci, 0, …, 0) < ci , 
 
Pi(c1, c2, …, ci-1, 0, ci+1, …, cn) > Pi(c1, c2, …, ci, …, cn) -  ci  

 
2-General formulation of the theory of evolutionary feedback selection 
 
Let  us  consider  n  agents  in  a  social  dilemma  situation,  in  which  voluntary 
contributions are needed to obtain some shared end-result, and where the individual rational 
choice  is  to  free-ride.  We  denote  by  ci  i=1,  …,  n,  the  contributions  of  each  agent  to  the 
common project. Then, the shared end-result is quantified by n payoff functions Pi(c1, c2, …, 
ci, …, cn), which are possibly distinct. After the one-period play, the total wealth of agent i is 
therefore Pi(c1, c2, …, ci, …, cn) – ci. The conditions for a social dilemma to hold is that  
 
 
 
 
Condition  (1a)  writes  that  the  project  does  not  remunerate  sufficiently  the  individual 
contributions  and  thus  discourage  agents  to  contribute  independently  of  the  actions  of  the 
other  agents.  Condition  (1b),  which  should  hold  for  arbitrary  n-plets  (c1,  c2,  …,  ci,  …,  cn), 
states that the rational choice is to free-ride. 
 
It is well known that the introduction of punishment opportunities after every public 
goods  interaction  creates  high  incentives  to  cooperate  and  contribute  to  the  public  good. 
However,  because  punishment  is  costly,  social  behavior  may  also  get  undermined  by  non-
punishing  strategies  and  subsequently  cooperation  breaks  down.  This  is  the  second-order 
cooperation problem in which the rational behavior is not to punish. In order to get persistent 
cooperative behavior, additional mechanisms are required. One such mechanism is reputation, 
which is built up during past encounters. Here, we emphasize another mechanism, associated 
with the collective co-evolution of cooperation and feedback by punishment in a population of 
agents under repeated interactions. In other words, in the mechanism studied by the theory of 
evolutionary  feedback  selection,  cooperation  and  punishment  are  emergent  characteristics 
within an evolving population. 
 
We  thus  assume  that  the  agents  have  evolved  the  possibility  to  exert  a  feedback  on 
level  of 
other  agents 
reward/punishment  is  determined  from  the  evidence  that  people  assign  and  maintain  a  self-
perspective  both  in  action  and  social  interactions  [25,26]  within  their  ‘theory  of  mind’ 
[27,28],  which  is  the  capacity  to  attribute  opinions,  perceptions  or  attitudes  to  others.  The 
first-person-perspective suggests that an agent will measure the unfairness of the allocation by 
comparing pair wise contributions and pair wise payoffs. We first discuss the mechanism of 
feedback by punishment and refer to section 7 for the case where the feedback is by reward. 
The feedback by punishment should take into account the following situations:  
 
(i)  The  contributions  are  different  but  the  payoffs  are  identical,  a  situation  often 
encountered in global resource sharing, such as for instance in public good interactions and in 
the  tragedy  of  the  commons.  The  level  of  punishment  exerted  by  an  agent  k  on  an  agent  i  
observing the contributions ci and cj of two agents i and j (k can also be j herself) should be an 
increasing function of |ci - cj|, with the punishment applied to the smallest contributor. Here, 
the motivation is to punish the free rider. 
(ii) The contributions are identical but the payoffs are different, perhaps due to random 
 
factors or to some structural asymmetry. This situation is characteristic of the ultimatum and 
dictator  games  for  instance.  The  level  of  punishment  exerted  by  an  agent  k  (on  agent  i) 
observing the payoffs Pi and Pj of the two agents i and j should be an increasing function of |Pi 
-  Pj|  with  the  punishment  applied  to  the  greatest  payoff.  Here,  the  rational  is  to  punish  the 
largest unjustified/unfair endowment. 

through  rewards  and/or  punishments.  We  suggest 

that 

the 

 

3 

(iii) A mixture of (i) and (ii) in which both contributions and payoffs are different. The 

 
arguably simplest functional form accounting for these situations is 
 

 

 

 

P(Pi – Pj). 

Puk→i =  Σ[j=1 to n]  κkji

c(cj- ci)+ Σ[j=1 to n]  κkji

(2) 

c  and  κkji

c(cj-  ci)  and  κkji

P(Pi  –  Pj)  by  κkji

 
 
Puk→i  is  the  punishment  of  agent  k  on  agent  i,  based  on  the  observation  of  both  the 
contributions  and  payoffs  of  agent  i  and  of  all  the  other  agents  j’s.  It  is  estimated  as  a 
weighted sum of functions of the observed differences of the contributions and payoffs of all 
P(Pi  –  Pj)  are  non-decreasing 
agents  j  and  the  agent  i.  We  assume  that  κkji
functions of their arguments. One could also consider coupled terms combining (cj- ci) and (Pi 
– Pj). In this first approach, we consider functions which are linear by part: specifically, we 
c  ∗ (cj- ci) for cj- ci>0 and 0 otherwise. Here, 
c(cj- ci)= κkji
expand the function as follows: κkji
c(cj-  ci)  for  the  function  is  replaced  by  a 
the  symbol  ∗  stresses  that  the  notation  κkji
c  times  the  discrepancy  (cj-  ci)  of  the  contributions.  Similarly,  we 
proportionality  factor  κkji
P  ∗  (Pi  –  Pj)  for  Pi  –  Pj>0  and  0  otherwise.  In  this  linearization 
replace  κkji
scheme, coupled terms combining (cj- ci) and (Pi – Pj) are higher order in a general functional 
P  (which  we  refer  to  as 
expansion  and  are  not  considered.  The  weight  factors  κkji
“propensities  to  reward/punish”)  can  be  heterogeneous  to  reflect  prior  knowledge  such  as 
reputation  or  to  take  into  account  that  not  all  agents  have  the  ability  to  punish.  Note  the 
change of order of the indices in the two  sums in (2), reflecting the following property: the 
punition of agent k on agent i is  activated if the  contribution ci tends to be  smaller than the 
other agents cj’s and/or when its payoff Pi tends to be larger than those of the other agents. In 
P) are 
the sequel, when the situation allows, we use the simplification that the κkji
the same κc (resp. κP) for all agents k and all pairs (i, j). In this special case, expression (2) 
simplifies into  
 
 
 
where <cj>I (resp. <Pj>i) is the average contribution (resp. payoff) of the n-1 agents other than 
agent i. 
 
We hold that people are aware that they can profit from cooperation, that they can gain 
even  more  if  they  are  among  a  minority  of  non-cooperators  with  a  majority  of  cooperators, 
but  they  also  know  that  they  may  be  punished  if  they  behave  unfairly,  and  they  possess  a 
drive  to  punish  other  agents  who  are  unfair  from  their  perspective.  We  propose  that  these 
elements  have  part  of  the  inputs  and  brain  processing  abilities  that  human  beings  have 
developed  over  many  generations  of  cooperation/defection  experiences.  In  this  context,  the 
level  of  cooperation  of  a  given  agent  i  is  assumed  to  be  determined  by  the  following 
optimization problem that a given agent i must solve to determine her contribution ci 
 

 (n-1) (<cj>i- ci) + κP

 (n-1) (Pi -<Pj>i),   

Puk→i =  κc

c (resp. κkji

(3) 

 

 

 

 

Max ci  E[ Pi(c1, c2, …, ci, …, cN) - Σ[k=1 to n] Pui→k - rp Σ[j=1 to n] Puj→i ] 

 
 
In  expression  (4),  E[  .  ]  denotes  the  expectation  of  agent  i  and  rp  is  the  so-called 
reward/punishment  efficiency  parameter  (see  below  for  its  precise  meaning  in  the  three 
treated examples). The optimization problem for agent i consists in determining her optimal 
contribution  ci  based  on  her  expectation  of  the  contributions  of  the  other  agents,  which 
determines her expectation of the punishment she may have to endure from other agents and 
the punishment she may be inclined to impose on the other agents. 

(4) 

 

 

4 

The theory of evolutionary feedback selection holds that the coefficients κkji

This  formulation  (4)  is  reminiscent  of  the  approach  in  terms  of  fairness  utility 
 
functions,  which  take  it  as  (exogenously)  given  that  people  have  preferences  for  fair 
contributions  [29].  The  fact  that  cooperation  is  stable  given  such  fairness  preferences  then 
emerges naturally, since contributors will anticipate punishment. Our approach differs by (i) 
seeing fairness as resulting from the propensity to punish in the presence of unfairness (rather 
than  being  an  innate  contribution  to  one’s  utility)  which  allows  us  to  (ii)  determine 
quantitatively  the  level  of  punishment.  In  particular,  the  main  target  of  our  theory  is  to 
determine endogenously the quantitative value of the propensity to punish by a condition of 
maximum selfish gain in an evolutionary stable collective equilibrium point. The next critical 
step,  which  is  to  explain  how  such  punishment  behavior  could  evolve  in  the  first  place,  is 
addressed in section 6 within an agent-based simulation. 
P 
 
have evolved to some non-zero values, which ensure the cooperation of the members of the 
group (i.e. the resolution of the free-rider social dilemma), and lead to increased average pay-
offs  for  each  member  of  the  group  compared  with  the  Nash  equilibrium  in  which  nobody 
cooperates. This evolution has occurred endogenously and self-consistently via the existence 
of the feedback mechanism (2) which has been active over many generations, and which has 
adjusted itself so that the marginal expected gain of any agent becomes zero, yielding a Pareto 
efficient  dynamical  fixed  point.  In  the  examples  treated  below,  we  implement  this  program 
and  show  how  it  allows  us  to  determine  the  Pareto  optimal  propensity  to  punish,  which  is 
compared quantitatively with experimental results. 
 
 
3-Analysis of a third-party punishment game  
 
In  Ref. [11],  Fehr  and  Fischbacher  report  experiments  on  a  variant  of  the  ultimatum 
game, in which the possibility of punishing is not given to the recipient, but to a third party 
not involved in the allocator-recipient collective action. The existence of punishment in this 
experiment  materializes  what  Adam  Smith  called  “sympathy”  in  his  Theory  of  Moral 
Sentiments, defined in his words as “our fellow-feeling with any passion whatever”.  

c and κkji

In the experiment reported in [11], an allocator is endowed with 100 MUs (Monetary 
Units) and is given the free option to donate part of her endowment to a recipient, who had 
received nothing at the beginning of the game. A third party, who receives 50 MUs, can spend 
money to punish the allocator when she is informed of the money transfer from the allocator 
to  the  recipient.  Every  MU  spent  on  punishment  by  the  third  party  reduces  the  allocator’s 
wealth by rp=3 MUs. The coefficient rp can be thought of as the “punishment efficiency.” The 
expected  rational  action  is  for  the  third  party  not  to  punish  whatever  the  transfer  from  the 
allocator  to  the  recipient,  since  she  can  only  lose  if  she  punishes.  This  expectation  is 
contradicted by experimental observations [11]. 

In the notations of section 2, we have n=3. Let us refer to the allocator with the index 
1, the recipient with the index 2 and the third party with the index 3. Then, by the structure of 
the game, all contributions are identical (or irrelevant) and only the payoffs need to be taken 
P which will be called 
into account. In addition, by construction, all κkji
`k’. The payoff functions are P1 = 100 -ma, P2 =ma and P3=0.  

P are zero except κ321

In the framework of the evolutionary feedback selection theory, the expected gain Ga 

of the allocator, who transfers ma MUs to the recipient, is: 
 

E[Ga | ma] = 100-ma - rp E[Puthird party→a] 

 

 

 

 

(5) 

 
It  is  the  sum  of  her  remaining  endowment  100-ma  after  the  transfer  of  ma  MUs  to  the 
recipient, decreased by the specter of the punishment by the third party. The punishment term 

 

5 

 

 

 

if ma<=50 
if ma>50 

 
E[Puthird party→a)] = k [(100-ma) – ma]   
 
E[Puthird party→a)] = 0 

E[Puthird  party→a]  must  express  the  fact  that  the  third  party  will  measure  the  unfairness  of  the 
allocation  by  comparing  the  two  payoffs.  The  simplest  assumption,  justified  as  a  Taylor 
expansion  of  an  arbitrary  non-singular  function  limited  to  the  first-order,  is  that  the 
punishment Puthird party→a of the third party onto the allocator is proportional to the mismatch or 
deviation from fairness (100-ma) – ma of their gains: 
 
 
 
 
where k is a proportionality factor that the theory must determine self-consistently. Note that 
P  equal  to  zero  except 
expression  (6)  is  nothing  but  (2)  with  all  ci’s  being  equal  and  all  κkji
P=k. In (6), we take a threshold ma=50 corresponding to an average preference for 50%-
κ321
50% splits. This assumption is not crucial and can be relaxed: taking for instance a 60%-40% 
split preference actually improves the fit between the prediction of the theory and the realized 
punishments  shown  in  figure  1.  Our  formalism  (2)  can  account  for  different  cultures  or 
contexts  in  which  the  preferred  split  is  not  50%-50%  [30],  by  interpreting  the  asymmetry 
between  allocator  and  recipient  in  terms  of  distinct  contributions  c1  and  c2.  In  this  first 
approach, we do not explore further this interesting line of investigation. We refer to the rule 
(5)  as  the  “linear  punishment  response  function.”  The  rule  is  assumed  to  be  known 
(endogenized consciously or instinctively) by both the allocator and the third party, reflecting 
a common evolution of feedback rewards and punishments over many generations.  

(6) 

 

 

We  propose  that  the  level  of  punishment  of  a  human  being  playing  this  game  in 
modern times results from hard-wired emotional decision modules selected by evolution over 
many  generations.  In  the  experiments  reported  in  Ref.[11],  third  parties  punish  allocators  at 
their own cost without any monetary gain, despite the fact that they will never see each other 
in  the  future.  Evolutionary  feedback  selection  holds  that  the  interplay  between  selfish 
optimization  and  feedback  has  tuned  the  level  of  punishment  so  that  the  third  party  can 
empathize with the allocator, and determine that the latter will take into account her gains as 
well as her potential punishment, in such a way to maximize her expected gain.  
Quantitatively,  the  theory  proceeds  as  follows:  we  calculate  the  marginal  expected 
 
gain  of  the  allocator  who  hesitates  between  transferring  ma  or  ma+1  MUs  and  use  the 
evolutionary  self-centered  optimization  to  determine  the  punishment  coefficient  k.  The 
marginal expected gain of the allocator reads 
 

E[Ga | ma+1] - E[Ga | ma] = -1 +2 k rp   
 
E[Ga | ma+1] - E[Ga | ma] = -1  

if ma<=50 
if ma>50 

 

(7) 

  
It  becomes  the  selfish  interest  of  the  allocator  to  act  fairly  if  k  >  1/2rp.  Thus,  the  cheapest 
altruistic punishment to ensure cooperation is reached when 
 
 
 
The theoretical punishment level then reads: 
 

k→1/2rp 

(8) 

 

 

 

 

 

 

 

 

E[Puthird party→a)] = (1/2rp) [(100-ma) – ma] 
E[Puthird party→a)] = 0 

 
 

 

 
 
Figure 1 compares this prediction (9) with the punishment beliefs by recipients and with the 
actual punishment by third parties, as reported by Fehr and Fischerbach [11]. Given the fact 
that there are no adjustable parameter (since rp is fixed to the value used in Ref.[11]) and that 

 

 

if ma<=50  
if ma>50 

(9) 
 

 

 
 

 

6 

we  use  the  approximation  of  a  linear  punishment  response  function  (6),  the  agreement  is 
good. 
 

 
Fig.1:  Mean  expenditure  by  the  punishing  third  party  (in  MUs)  as  a  function  of  the  transfer  by  the 
allocator  to  the  recipient  (in  MUs)  obtained  in  the  experiments  reported  in  Ref.[11].  The  theoretical 
punishment  represented  by  the  thick  line  is  given  by  expression  (9)  with  the  value  rp=3  of  the 
punishment efficiency used in ref.[11]. 
 
 
4-The ultimatum game 
 
In the ultimatum game, two subjects have to agree on the division of a fixed sum of 
money, for example 100 MUs. The proposer makes exactly one proposal 0 ≤ma≤100 of how 
to divide the money. Then, the responder can accept or reject the proposed division (100-ma 
to  the  proposer  and  ma  to  the  responder).  In  the  case  of  rejection,  both  receive  nothing, 
whereas in the case of  acceptance, the proposal is implemented. The rational solution is for 
the  responder  to  accept  any  positive  transfer,  and  thus  for  the  proposer  to  propose  the 
minimum possible non-zero amount. A robust result in this experiment across many cultures 
is  that  responders  reject  the  division  with  a  high  probability  if  ma  is  too  low  [29,31].  The 
ultimatum game belongs to the class of games with altruistic punishment, in the sense that the 
responder often sacrifices his share ma as his cost for punishing the proposer with no personal 
gain. 

 

Analyzed  from  the  viewpoint  of  evolutionary  feedback  selection,  the 
particularity of this game is that the punition leverage coefficient rp previously defined in (4) 
and (5) now depends on the contribution ma itself. Indeed, using expression (6) and the fact 
that punishment is equivalent to rejection by the responder which amounts to losing the entire 
sum 100-ma for the proposer, this leads to define a punishment efficiency rp = (100-ma)/ma, 
equal to the ratio of the loss incurred by the proposer (punished) to the loss incurred by the 
responder  (punisher).  This  game  differs  from  the  previous  third  party  game  and  from  the 
altruistic punishment game discussed below by the fact that its punishment efficiency rp is not 

 

7 

P  being  zero  except  κ221

Proceeding as for the preceding example, we write the expected gain E[Gp | ma] of the 

 
E[Gp | ma] = 100-ma - k rp (100-2ma) = 100-ma - k(100-ma)(100-2ma)/ma 

fixed but is determined by the level of cooperation quantified by ma. A small offer ma from 
the  proposer  to  the  responder  not  only  implies  a  poor  cooperation  but  also  a  strong 
punishment  efficiency,  both  factors  favoring  a  strong  potential  for  punishment.  In  the 
notations  of  section  2,  using  the  index  1 for  the proposer  and  the  index  2  for  the  responder 
(n=2),  we  have  all  contributions  being  a  priori  equal,  only  payoffs  enter  in  the  punishment 
P,  which  will  be  called  `k’.  The  payoff 
function  (2)  with  all  κkji
functions are P1=100-ma and P2 =ma. 
 
proposer, given a proposal ma: 
 
 
 
The marginal gain of the proposer who hesitates between proposing ma and ma+η, where η is 
a small increment, is  
 
 
 
Taking the expectation of this expression, with respect to the distribution of propositions ma, 
and  equating  to  zero,  allows  us  to  determine  the  coefficient  k  of  punishment,  which  results 
from the evolution of selfish maximizers in the presence of punishment feedbacks: 
 
 
 
 
The  probability  pr(ma)  that  the  responder  rejects  the  proposition  ma  can  be  obtained 
from  the  fact  that  the  expected  gain  of  the  proposer  can  be  written  as  the  product  of  her 
endowment multiplied by the probability 1-pr that the offer is accepted: E[Gp | ma] = (100-ma) 
(1-pr).  This yields 
 

η dE[Gp | ma]/dma= η [-1 + k (104/ma

k = E[1 /(104/ma

2  - 2)]   

2  - 2)] 

(11) 

(10) 

(12) 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

where k is given by (12) 

 pr = k (100-2ma)/ma,  
 
According  to  the  theory  of  evolutionary  feedback  selection,  the  level  of  altruistic 
punishment  in  the  ultimatum  game  should  be  highly  sensitive  to  cultural  and/or  economic 
differences, measured by the ma distribution. The factor k should have co-evolved with social 
norms  represented  by  the  distribution  of  “adequate”  allocations  by  the  proposer.  Human 
groups exhibiting a low propensity to cooperate in the ultimatum game should as well select 
low  rejection  rates.  Such  sensitivity  is  indeed  observed  in  laboratory  experiments,  see  for 
instance the comparison between Indian and French participants [32]. 

(13) 

 

 

Figure  2  and  figure  3  compare  the  theoretical  and  experimental  rejection  rates  in 
ultimatum games involving French and Indian participants [32]. Experimental observations on 
French  participants  are  well  anticipated  by  the  theory  while  one  can  observe  significant 
discrepancies between theory and data for Indian participants. Interestingly, this discrepancy 
comes with another important difference between French and Indian behaviors, as reported by 
Boarini, Laslier and Robin [32]. Indeed, while French’s offers remain constant over repetition 
of the game, suggesting that the appropriate level of punishment leads to a  stable collective 
feedback process, Indians’ offers continuously decrease over time as the games are repeated. 
This  suggest  that  the  observed  level  of  rejection  of  Indians  is  not  a  true  characteristic 
associated  with  a  stable  cooperating  behavior  but  rather  a  transient  learning  phase.  The 
conclusion that one can draw from the theory of evolutionary feedback selection is that Indian 
rejections  rates  are  not  optimized  to  the  experiment  performed  in  ref  [32],  leading  to 
cooperation  rapidly  vanishing.  This  may  be  due  to  a  wealth  effect  (obviously,  very  poor 

 

8 

people  barely  able  to  survive  will  not  turn  down  an  offer,  however  small  it  is;  they  do  not 
have the luxury to punish).  

 
Fig.2: Experimental rejection rates in an ultimatum game involving French participants performed by 
Boarini, Laslier and Robin [32] compared with theoretical rejection rates. The theoretical rejection rate 
is  calculated  using  expressions  (13)  with  (12),  where  the  expectation  in  (12)  is  performed  over  the 
observed  distribution  p(ma)  reported  in  [32],  which  is  shown  as  the  continuous  line  joining  the 
triangles.  There  is  thus  no  adjustable  parameter  in  the  theory.  Note  that  the  cooperation  reported  in 
[32] is stable over time. 

 

 

9 

 

 

 
 

 

 

Fig.3: Experimental rejection rates in an ultimatum game involving Indian participants, performed by 
Boarini, Laslier and Robin [32], compared with theoretical rejection rates obtained with the procedure 
explained  in  the  caption  of  figure  2.  Indians’  offers  are  observed  to  decrease  with  repetition  of  the 
game  leading  to  rapidly  vanishing  cooperation,  showing  the  absence  of  a  stationary  state.  The 
conditions for application of the theoretical predictions (12) and (13) are thus not met. 

 

 
5-Analysis of Fehr and Gächter’s experiments on altruistic punishments [13,17] 
5.1 Quantitative formulation of the Evolutionary Feedback Selection theory 
 
 
In  the  experiments  performed  in  Ref.[13],  n=4  members  of  a  group  receive  each  an 
endowment of M=20 monetary units (MUs), and each can contribute to a project between 0 
and 20 MUs. One MU invested by the group in a project yields back r1=1.6 MUs. This yield 
is equally shared among the n participants, which implies that 1 MUs invested by an agent is 
returned as r1/n=0.4 MUs to her, that is, with a loss of 1- r1/n=0.6 MUs: it is thus always in the 
material  interest  of  any  subject  to  keep  away  from  the  project.  However,  if  all  members 
follow  this  reasoning  and  do  not  contribute  to  the  project,  they  just  earn  their  initial 
endowment  of  20  MUs  while,  if  they  all  contribute  maximally,  they  earn  each  20  r1  =  32 
MUs, showing that this game is an example of a social dilemma. Indeed, a free-rider among 
n-1  maximally  cooperating  agents  will  earn  even  more,  that  is,  20+20(n-1)r1/n=44  MUs. 
Without feedbacks, rational decision theory predicts that cooperation should break down after 
a few rounds, which is indeed observed experimentally.  
 
However,  more  interesting  behaviors  occur  when  feedback  by  punishment  is 
introduced. After the above first step, according to the rules implemented in the experiments 
of  Ref.[13],  each  member  j  of  the  group  can  decide  to  spend  mj→i  MUs  to  punish  another 
agent i (agent j can punish several agents) whom she feels has not contributed enough to the 
group, where 0≤ mj→i ≤mp=10. By spending mj→i MUs, agent j inflicts a cost equal to rp mj→i 
MUs to a punished member i (where rp=3 is the efficiency of the punishment). Each agent is 
aware of this possibility before she decides the level of her contribution to the group. In the 
notations  of  section  2,  n=4,  all  payoffs  are  identical  but  the  contributions  differ.  All 
c  are  a  priori  non-zero  and,  in  view  of  the  symmetry  between  all  agents,  we 
coefficients  κkji
assume that they are all equal to some common value `k.’ 

In the experiments reported in Ref.[13], group members punish each other, despite the 
fact  that  they  will  never  see  each  other  in  the future.  Evolutionary feedback  selection  holds 
that the interplay between selfish optimization and feedback has tuned the level of punishment 
(quantified by the propensity k to punish) so that an agent, who takes into account her gains as 
well as both her potential cost to punish and her  potential punishment by others, has  a zero 
average expected marginal gain. The experiment shows that defectors with very low or zero 
investment in the project incur a larger probability of being punished. Our goal is to predict 
the  observed  experimental  expenditure  by  punishing  group  members  as  a  function  of  the 
deviation from mean cooperation level of other group members.  

As an agent does not know the contributions of the other members when she decides 
her  own  contribution,  she  must  form  a  judgment  on  the  distribution  of  other  members’ 
contributions. Given this judgment, she adjusts her contribution according to some criterion of 
optimization. Following the same reasoning as in the previous sections, we propose the simple 
rule that the agent maximizes her expected gain, given her anticipation of the distribution of 
contributions by the other players and the existence of punishment. 
 
If m1, m2, …, mi-1, mi+1, …, mn are the amounts of MUs invested by members 1, 2…,i-
1, i+1,…, n in the project and which are unknown to agent i, the expected gain Ei[Gi |mi]  for 
agent i in absence of punishment, and conditioned on her contribution mi, is: 
 

 

10 

 

 

 

 

= -mi + (r1/n) E[m1+m2+…+mn] = -mi (1-r1/n)+ (r1/n) E[m1+mi-1+mi+1+…+mn]  
(14) 
= -mi (1-r1/n)+ (n-1) (r1/n) Ei[mj] 

Ei[Gi |mi]  
 
 
where  Ei[mj]  is  the  expected  contribution  of  another  typical  agent  j≠i.  The  last  equality 
expresses the fact that the agent i does not have any prior information on the other members 
of  the  group  and  thus  forms  identical  expectations  on  their  contributions.  Thus,  from  the 
viewpoint of agent i, all other members of the group are inter-changeable in absence of prior 
information. This is a prerequisite of the experimental conditions described in [13]. Therefore, 
it is convenient to reason per agent, that is, to split agent i’s total expected gain into a series of 
one-to-one gains with respect to each other member. This reads  
  

 

 

Ei[Gi |mi] = Σ [j=1 to n; j≠i] Ei[Gi,j]     

 

 

 

 

 

 

(15)  

with     

Ei[Gi,j | mi] = - a mi + (r1/n) Ei[mj],   with   a =(1-r1/n)/(n-1)]>0 ,     for j≠i .  

(16) 

 
Since r1/n<1, a selfish agent has the incentive to defect (mi=0) to maximize Ei[Gi,j | mi], but 
profits from others’ cooperation (mj≠i>0) embodied in Ei[mj]. We assume that agent i uses  a 
(possibly subjective) probability Pj(p) to quantify her belief that the agent j will invest p MUs 
in the group project. We can thus write 
 

 

 

 

 

 

 

 

(17) 

Ei[mj]  = Σ[p=0 à 20]    p Pj(p)  
 
 
 
In addition to the accounting of her potential gains given by (15) and (16), in order to 
make her decision on how much to contribute, agent i takes into account (possibly through not 
fully  conscious  decision  making  modulii  developed  by  evolution)  both  her  potential  cost  to 
punish  and  her  potential  punishment  by  others.  For  these  two  additional  contributions,  our 
starting point is, once again, the evidence that people assign and maintain a self-perspective 
both in action and social interactions [25,26] within their ‘theory of mind’ [27,28]. The first-
person-perspective suggests that a member j will measure the lack of cooperation of agent i by 
comparing the contribution mi of agent i to the group project to her own contribution mj. The 
simplest  assumption,  justified  as  a  Taylor  expansion  of  an  arbitrary  non-singular  function 
limited to the first-order, is that the punishment Puj→i of agent j on agent i is proportional to 
the mismatch mj-mi of their contributions, as in expression (2): 
 
 
 
This “linear punishment response function” has the same meaning as expression (2) used for 
the two previous games. Predicting the value of the positive coefficient k is one of the major 
goals of the theory as it quantifies the propensity to punish. The total expected gain of agent i 
in her relation with agent j, which takes into account both possibilities that she punishes agent 
j or agent j punishes her, conditioned to her contribution mi, then reads  
 

Puj→i  = k (mj-mi),  for  mj>mi   ; 
  
 
 

Puj→i=0, for  mj ≤mi 

(18) 

 

 

 

 

 

 

Ei[Gi,j | mi] = - a mi  + (r1/n) Ei[mj]  - rp  Ei[Puj→i] - Ei[Pui→j]  

 

 
where rp (=3 in [1]) is the punishment efficiency and  
 
Ei[Pui→j] = k Σ[p=0 to mi]  (mi-p) Pj(p) ;  Ei[Puj→i] = k  Σ[p= mi to 20]  (p-mi) Pj(p) 
 

 

 

 

(19) 

(20) 

11 

 

The total expected marginal gain of  agent i, who hesitates between investing mi  and 

The term Ei[Pui→j] is the expectation of agent i of how much she will punish another typical 
agent  j,  given  her  contribution  mi.  The  term  rp  Ei[Puj→i]  is  the  expectation  of  how  much 
punishment she will endure from agent j, conditioned on her contribution mi. 
 
mi+1 (with i<20), then reads 
 
 
 
where 

Ei[Gi,j | mi+1] - Ei[Gi,j | mi]  = -a + k [ rp (1-Fi) – Fi]   

(21) 

 

 

 

 

 

 

 

 

 

 

Fi = Σ[p=0 to mi]  Pj(p)    

(22) 

  
 
is the probability perceived by agent i that another agent j will contribute less than or equally 
to her. Equation (21) expresses the fact that an increment of 1 MU of agent i’s contribution 
gives conflicting terms: (1) an anticipated loss “–a” due to the equal sharing of any investment 
between team members, (2) an anticipated gain krp(1-Fi) due to the decrease of the probability 
of  being  punished  by  agent  j  (which  increases  with  the  probability  1-Fi  that  the  other  agent 
contributes  more  than  herself)  and  (3)  an  anticipated  loss  -kFi  due  to  the  increase  in  the 
probability of punishing the other agent j (which grows with the probability Fi that the other 
agent contributes less than herself).  
 
According  to  the  theory  of  evolutionary  feedback  selection,  the  level  of  feedback 
provided by punishment, quantified by the factor k, has been optimized globally over many 
human generations so that the selfish optimization of an arbitrary agent ensures a maximum 
gain when investing in the group project. The value of k has been selected and transmitted to 
ensure that cooperative actions are maximally efficient from the self-centered point of view of 
an agent. As  a consequence, humans unconsciously select  a level of punishment, which has 
been fine-tuned by the evolutionary success of cooperation kept under control by the feedback 
provided  by  punishments.  Similarly,  in  more  general  situations,  they  may  select  group 
members they are able to understand and anticipate. The ideal situation is reached when agent 
i invests exactly the same amount as agent j, because no one incurs any punishment cost. This 
performance  is  heavily  dependent  on  the  ability  of  agent  i  to  anticipate  the  contribution  of 
agent j and reciprocally.  
 
Whether the selection and transmission of the optimal level of feedback has occurred 
through  biological  and/or  cultural  processes  can  make  a  significant  difference  in  some 
situations [33], as for instance cultural transmission can result in transmission of maladaptive 
traits. Effects reminiscent of this result are observed in our agent-based model in section 6, in 
which  some  groups  are  found  to  evolve  spontaneously  towards  the  non-cooperative  Nash 
equilibrium.  
 
Since all players play a symmetric role, the distribution of contributions mi describing 
the  heterogeneity  of  the  players’  contributions  is  the  same  as  the  distribution  Pj(p)  of  the 
contributions of the other agents. The fact that the level of punishment k has been selected by 
evolution  and  cultural  interactions  implies  that  it  constitutes  a  collective  characteristic  of 
human  cultures.  But  k  is  not  the  sole  parameter  characterizing  the  evolved  adaptation.  The 
distribution  Fi  can  also  be  expected  to  adapt  to  the  punishment  efficiency  rp,  as  shown  in  a 
simple agent-based model discussed in section 6.  
 
In  order  to  determine  k,  we  consider  the  following  situation.  Let  us  assume  that  the 
adaptation is dominated by many stochastic factors, so that the only robust response for k is to 
maximize Ei[Gi,j | mi], i.e., make (21) vanish after averaging all possible contribution levels mi 
weighted  by  their  corresponding  probabilities  Pj(mi).  Since  by  construction  the  random 
variable Fi is uniformly distributed between 0 and 1, its average with respect to mi weighted 

 

12 

 

 

 

 

 

(23) 

k = 2a/(rp-1) = 2 (1-r1/n) / [(n-1) (rp-1)] 

by  Pj(mi)  is  always  equal  to  ½,  irrespective  of  the  arbitrary  distribution  of  Pj(mi)  of  the 
contributions of the agents. One can interpret this result by saying that the average probability 
for a contribution to be smaller than or equal to the median is exactly ½. In other words, the 
best  guess  for  the  contribution  of  others  is  the  median  (which  is  however  not  specified  and 
disappears  from  the  unknowns  by  the  averaging  procedure).  Then,  equating  the  average  of 
(21) (with E[Fi]=1/2) to zero to express the condition of a maximum expected gain yields 
 
 
 
For n=4, rp=3, and r1=1.6, we obtain k=0.2. According to the theory of evolutionary feedback 
selection, humans will punish (i.e., k>0) typically in situations of social dilemma (r1/n<1) and 
when punishment is effective (rp>1). It is important to realize that this result (23) amounts to 
assuming  that  the  distribution  of  contribution  is  quenched,  i.e.,  does  not  respond  to  the 
punishment efficiency rp. 
The  simulations  of  the  agent-based  model  discussed  below  as  well  as  experiments 
 
[34,35] suggest that the distribution P(p) is effectively distorted for large values of rp so that 
F(<mi>) tends to increase with rp and the factor k remains almost constant as a function of rp. 
This phenomenon is neglected by the averaging of Fi but, for the value rp=3 of the experiment 
of Ref.[13], the factor k selected in the agent-based model is undistinguishable from the value 
k=a predicted by equation (23). 
 
The punishment given by agent j, when she realizes that agent i has invested unfairly, 
is given by expression (18) with (23). The total punishment exerted by the group of n-1 agents 
towards  agent  i  sums  up  the  individual  punishments  from  each  of  the  n-1  other  agents 
participating  in  the  collective  action.  The  total  expenditure  by  punishing  members  towards 
agent i reads: 
 
Pugroup→i=Σ[j, ; j≠i, mj>mi]  Puj→I 
Pugroup→i = [2(1-r1/n)/(rp-1)](<mj; j≠i, , mj>mi > - mi) = 0.6 (<mj; j≠i, , mj>mi > -mi),  

(24) 

 

 

 

 

 

5.2  Comparison  between  the  theoretical  punishment  level  and  experimental 

where  the  coefficient  0.6  is  obtained  for  n=4,  rp=3,  and  r1=1.6.  The  term  <mj;  j≠i,  ,  mj>mi  > 
denotes  the  mean  cooperation  level  of  the  n-1  other  group  members,  where  the  average  is 
conditioned on the contributions being larger than mi.  
 
 
results 
 
Expression (24) predicts the expenditure by punishing group members as a function of 
the deviation from the mean cooperation level of the other group members. Figure 4 compares 
Fehr  and  Gächter’s  experimental  results  [13]  shown  in  their  figure  1  with  the  theoretical 
expenditure by punishing group members predicted by expression (24). For the experimental 
data, we use the average of the mean expenditure over all the six periods studied by Fehr and 
Gächter.  We  augment  our  prediction  (24)  by  providing  a  standard  deviation  obtained  as 
follows. We simulated 20’000 synthetic random games with rp=3, and r1=1.6 and n=4 players. 
Each  agent  has  the  same  punishment  coefficient  k  given  by  (23)  and  her  contribution  mi  is 
taken  from  a  uniform  distribution  between  0  and  20.  The  punishment  she  exerts  on  other 
agents is given by the rule (18).  
 

 

13 

 
Fig.4:  Mean  expenditure  by  punishing  group  members  (in  MUs)  as  a  function  of  the  deviation 
between  the  contribution  of  an  agent  from  the  mean  cooperation  level  of  the  other  group  members. 
The vertical bars are the averages of the mean expenditure over all the six periods studied by Fehr and 
Gächter. The thick horizontal segments give the punishment level predicted by expression (24). They 
are accompanied by vertical segments indicating ± 1 std, obtained as described in the text.  
 
The  agreement  between  prediction  (24)  and  Fehr  and  Gächter’s  experimental  results  is 
striking,  especially  given  the  fact  that  there  are  no  adjustable  parameters.  The  only 
discrepancy occurs for the largest positive deviation 8-14 from the mean cooperation level of 
other group members. For strictly positive deviations, rule (18) implies no punishment. The 
fact that we predict some punishment for a positive deviation in the range 2-8 results from the 
heterogeneity of the agents’ contributions: even if the deviation of agent i’s contribution from 
the  mean  contribution  is  positive  (namely,  agent i  gives  more  that  the  average),  there  is  the 
possibility that  some other  agent j has contributed still even more (while at least one of the 
two others has necessarily contributed less), in which case agent i is susceptible to agent j’s 
punishment,  according  to  (18).  For  the  largest  positive  deviation  8-14  from  the  mean 
cooperation  level  of  the  other  group  members,  this  occurrence  is  not  observed  in  our 
simulations as it becomes essentially impossible to have an agent contributing much more that 
the average of the others, and still find one of them contributing even more. The discrepancy 
between  our  prediction  (24)  and  the  data  for  the  deviation  8-14  thus  results  from  the 
punishment by low contributors of large contributors, an effect not taken into account in our 
theory. 
 
 
6-A simple model of repeated agent interactions with punishment  
 
The  evolutionary  feedback  selection  theory  argues  that  the  average  level  -  and 
probably  the  full  distribution  as  well  -  of  feedbacks  by  reward  and/or  punishment  has  been 
tuned  over  many  generations  of  human  beings  by  repeated  interactions  in  which  humans 
strive to optimize their own selfish gains in the presence of the other agents’ feedbacks. Using 

 

14 

this  concept,  we  have  been  able  to  account  quantitatively  for  several  experiments  on  social 
cooperative dilemma games. Now, we present a simple model of evolution of players, in order 
to  test  whether  the  level  of  punishment  indeed  evolves  or  not  to  the  optimal  equilibrium 
predicted  by  our  theory  of  equilibrium  (see  expressions  (8),  (12)  and  (23)  for  three 
cooperative games with punishment). In other words, our goal is to check whether or not the 
level of punishment predicted by the equilibrium argument applied above is indeed  selected 
by some reasonable implementations of the dynamics of interacting agents. Our simulations 
also  address  the  question  of  whether  a  population  of  agents  with  optimal  punishment  level 
remains stable against random variations and inclusion of invaders with different punishment 
levels. 
 
We consider a synthetic universe of n agents who play together repeatedly the game of 
section  5,  over  T=500  periods.    At  a  given  time t,  each  agent  i  cooperates  according  to  her 
frozen contribution mi and punishes other agents according to rule (18) with a personal frozen 
punishment coefficient ki. Each agent is thus characterized by a fixed pair of parameters (mi, 
ki), where mi is uniformly drawn between 0 and 20 and ki is uniformly drawn between 0 and 
1. After each game session, each agent sees her “wealth” change according to her profit-and-
loss, which includes the revenue from the group project and the losses from her punishments 
to  others  and  from  others  to  herself,  as  well  as  a  fixed  consumption  equal  to  3  MU  per 
session. “Wealth” is like a monetary reserve for food and other exchanges of life-supporting 
commodities.  The  cooperative  group  project  mimics  a  cooperative  hunter-gatherer  food 
collection.  The  consumption  is  introduced  to  obtain  a  stationary  wealth  and  allows 
implementing a selection process in which big losers disappear. The initial “wealth’’ is set to 
50 MUs for each agent. 
 

The updates, which allow for both adaptation and evolution, are as follows. 

(i) 

(ii) 

Adaptation: if the profit-and-loss of agent i at time t is negative, she modifies her 
(mi,  ki)  by  small  random  unbiased  increments  around  her  previous  values.  This 
step represents the natural ability of agents to explore other solutions when failing 
in the recent past. Here, we call `adaptation’ the generation of random variations 
within an agent in response to failure, such as for instance the proposition that the 
genome may be lowering the activity of its own repair mechanisms in response to 
environmental  stress.  We  have  found  this  ingredient  to  be  secondary  for  the 
determination of the equilibrium, while it controls significantly the duration of the 
transient dynamics. 
Evolution: if the wealth Wi of agent i becomes smaller than mi, she disappears and 
is  replaced  by  another  agent  endowed  with  a  punishment  coefficient  kj  and  a 
contribution  mj  drawn  from  uniform  distributions  centered  on  the  respective 
averages all the other agents of the universe. This step reflects the genetic mixing 
among humans along the course of their co-evolution and the cultural influence of 
group  members  towards  new  generations  of  participants.  Here,  the  variations  are 
between individuals rather than within individuals in (i). 

(iii)  The  global  wealth  of  all  agents  has  no  real  significance  and  may  increase 
(inflation)  or  decrease  (deflation).  To  reflect  the  invariance  of  the  cost  of 
consumption  with  respect  to  a  global  change  of  wealth,  we  change  the 
consumption cost to ensure an average stationary global wealth: if the total wealth 
increases  (resp.  decreases)  above  (resp.  below)  the  initial  wealth  (50n),  the 
consumption cost per agent and per unit time is increased (resp. decreased) by 0.5 
MU for the next time step. 

 
We  stress  that  ingredients  (i)  and  (ii)  refer  respectively  to  variations  within  and  between 
individuals,  both  being  subjected  to  selection  and  thus  result  in  evolved  adaptation  (natural 

 

15 

selection on variations). Moreover, this agent-based model does not incorporate the third level 
of variation, in terms of group selection. This does not imply that we take this third level of 
variation  as  negligible.  Our  goal  here  is  simply  to  demonstrate,  with  perhaps  the  simplest 
possible model, the plausibility of evolutionary feedback selection. 
We  have  run  this  game  200  times  for  500  time  steps  for  various  values  of  the 
 
parameters  r1<n,  rp>1  and  n>1,  and  have  constructed  the  distribution  of  punishment 
coefficients k of the evolved population of agents in the last 200 time steps to remove possible 
biases from initial transients. For rp=3.5, we find that the distribution of k’s develops a mode 
centered  very  close  to  the  prediction  (23)  derived  from  the  evolutionary  feedback  selection 
theory.  Fig  5  shows  that  prediction  (23)  is  able  to  account  quantitatively  for  the  non-
monotonic dependence of k as a function of the number n of agents in a group.  
 
 

 

Our  simulations  also  find  an  elaborate  balance  between  diversity,  punishment  and 

Fig.5:  Dependence of the factor k - the linear punishment response function factor - as a function of 
the number n of agents in a cooperating group. The vertical bars are the predictions of the theory given 
by  (23),  while  the  vertical  segments  define  the  25%  quartile  (lower  endpoint),  the  median  (thick 
middle  horizontal  segment)  and  the  75%  quartile  (upper  endpoint)  obtained  from  simulations  of  the 
agent-based model with parameters r1=1.6 and rp=3.5. Note the nonlinear scale used in the abscissa. 
 
 
cooperation, controlled by the punishment efficiency rp.  
(1) For small rp’s, punishment which promotes cooperation would require a large propensity k 
to  punish  which  should  be  selected  simultaneously  by  a  large  fraction  of  the  population, 
which  is  unlikely.  As  a  consequence,  strong  punishers  (i.e.,  with  large  k’s)  find  themselves 
rather isolated and have a high probability to be eliminated by the selection process due to the 
cost  paid  for  punishment.  Then,  cooperation  rapidly  vanishes  and  the  average  contribution 
<mi>  is  close  to  zero.  No  cooperation  (with  everyone  contributing  little  or  nothing)  implies 
low or no global punishment since the diversity of contributions is low or zero. Consequently, 
the diversity in the distribution of k’s becomes irrelevant as the act of punishing disappears 
with the absence of cooperation. In this non-cooperative equilibrium, there is no co-evolution 
of cooperation and feedback by punishment.  

 

16 

(2) In contrast, large rp’s increase the probability of reaching a stable cooperation regime with 
minimal  costs  incurred  by  punishers  during  transient  regimes.  Indeed,  an  agent  contributing 
more than the average and with a strong propensity to punish will be quite effective alone in 
promoting cooperation at a tolerable cost.  
(3) In between, one can expect a transition between low rp and large rp’s in which the global 
cooperation  level  and  the  average  propensity  to  punish  are  held  in  balance  by  a  sustained 
stable diversity of cooperation. 
Figure  6  illustrates  these  three  regimes  by  plotting  as  a  function  of  rp  the  mean 
 
contribution  <mi>.  The  large  range  of  mean  contribution  levels  in  the  regime  (3)  of 
intermediate  rp’s  reflects  the  sensitive  dependence  of  cooperation  from  one  realization  to 
another realization.  Equations (21) and (22) indicate that the optimal level for the propensity 
k to punish depends on the final distribution of contributions, measured by F(<mi>) (which is 
defined as the probability that any participant invests less than or equal to the average). In our 
simulations,  we  observe  that  F(<mi>)  remains  close  to  ½  from  rp=1  up  to  rp  =4.5  and  then 
grows while exhibiting large fluctuations from one realization to another. As a consequence, 
the sensitivity of the factor k to rp is found smaller than anticipated by equation (23), where 
F(<mi>)  is approximated by  ½ for all regimes. 
 
  

 
Fig.6:  Dependence  of  the  cooperation  level  –  measured  by  the  25%  quartile  (lower  endpoint),  the 
median  (thick  middle  horizontal  segment)  and  the  75%  quartile  (upper  endpoint)  obtained  from 
simulations  of  the  agent-based  model  –  as  a  function  of  the  punishment  efficiency  rp,  for  n=8  and 
r1=1.6. 
 
 
While  this  simple  agent-based  model  does  not  claim  to  represent  faithfully  the 
evolutionary process and co-evolution of genes and culture of human beings [14], it suggests 
that the level of feedback, quantified by the punishment coefficient k, is an emergent property 
of  the  co-evolution  of  interaction  agents  with  feedbacks.  In  other  words,  the  level  of 
punishment that the theory of evolutionary feedback selection determines from a robust self-

 

 

17 

centered optimization in the presence of feedbacks can be seen as an attractive stable point of 
the dynamics of human interactions. 
 
 
7-Reward versus punishment 

The  emphasis  of  our  analysis  has  been  on  the  experiments  in  which  feedback  may 
occur via punishment. But the evolutionary feedback selection theory is not one of feedback 
by punishment alone. It rests on the general notion that feedbacks of any kind may participate 
in  favoring  long-term  cooperation  between  humans.  One  such  feedback  is  reward,  which  is 
technically symmetric to punishment: when an agent has done better than expected according 
to  some  reference,  other  agents  may  reward  her.  This  leads  us  to  suggest  variations  of  the 
games analyzed here and others (in addition to the gift exchange games [36], trust games [37] 
or sequentially played prisoners’ dilemmas [38]), in which punishment is replaced by reward.  
We  have  argued  above  for  the  co-evolution  of  cooperation  and  feedback  by 
punishment,  probably  aided  by  organization  of  psychological  and  emotional  brain  modulii. 
Assuming  a  similar  co-evolution  of  cooperation  and  feedback  by  reward,  we  can  apply  the 
evolutionary  feedback  selection  theory  to  the  game  of  section  5  in  which  punishments  are 
replaced by rewards. Assuming that the reward Rej→i of an agent j to another agent i follows a 
rule similar to (18) (namely Rej→i = kr (mj-mi),  for  mi>mj ; Rej→i=0, for mi ≤mj), and using a 
formalism parallel to that of section 5, we find that expression (21) is changed by replacing 1-
Fi by Fi and vice-versa, while expression (23) still holds for the coefficient of reward kr but 
with rp now interpreted as the efficiency of a reward. Similar results are obtained for the third 
party  game  of  section  3  in  which  rewards  replace  punishments.  Should  one  believe  these 
predictions? 

Feedback  by  punishment  is  probably  associated  with  “negative”  emotions,  such  as 
anxiety,  anger,  fear,  shame  and  guilt,  at  various  degrees  either  for  the  punisher  and  the 
punished one. In contrast, feedback by reward may trigger different kinds of emotions, such 
as  desire,  hope,  joy,  and  pleasure.  Recent  advances  in  evolutionary  psychology  and 
neurobiology  [19,39]  indicate  that  different  emotions  are  associated  with  distinct  complex 
activations of parts distributed over many locations in the brain. In addition, the efficacy and 
robustness of punishment as a feedback mechanism seem much larger than reward, as nicely 
captured by Machiavelli in The Prince: “… Upon this a question arises: whether it be better to 
be  loved  than  feared  or  feared  than  loved?  …  because  it  is  difficult  to  unite  them  in  one 
person,  it  is  much  safer  to  be  feared  than  loved  …  for  love  is  preserved  by  the  link  of 
obligation which … is broken at every opportunity for their advantage; but fear preserves you 
by  a  dread  of  punishment  which  never  fails."  It  is  thus  an  open  question  in  our  mind  as  to 
whether feedback by reward has been selected by evolution with an intensity similar to that of  
punishment. 

 

 
8-Concluding remarks 

According to the theory of “strong reciprocity” [12], humans care about fairness and 
the  welfare  of  others  beyond  what  can  be  explained  by  evolutionary  kin  theory,  costly 
signaling theory and indirect reciprocity, as well  as reciprocal altruism or direct reciprocity. 
Humans  are  characterized  by  extraordinary  cooperation  among  themselves,  with  strong 
feedbacks  through  reward  and  punishment.  The  theory  of  evolutionary  feedback  selection, 
that  we  have  outlined,  is  not  in  opposition  to  other  theories  advanced  to  explain  the  many 
puzzles  posed  by  human  behavior.  Instead,  it  aims  at  providing  an  explanation  and 
underpinning for strong reciprocity, by deriving quantitatively its predictions on the basis of 
the  evolutionary  selection  of  feedback  processes  that  have  benefited  most  to  self-centered 

 

18 

individuals. It emphasizes the role of feedbacks, which lead to the selection of behaviors well 
described by strong reciprocity. These feedbacks may occur at, as well as promote, different 
levels of selection (within or between individuals and groups). Notwithstanding the fact that 
our  arguments  have  been  framed  in  terms  of  individual  incentives  leading  to  individual 
selection along the line of Tooby and Cosmides [24], with a derived role for group selection, 
our formalism could as well apply to the group level. 

According  to  the  theory  of  strong  reciprocity,  people  maximize  a  utility  function, 
which is the sum of a selfish gain and of a term favoring fairness [18]. In contrast, the theory 
of  evolutionary  feedback  selection  differs  by  emphasizing  that  people  have  evolved  to 
maximize  their  selfish  gain  in  the  presence  of  feedbacks.  The  end  result  is  the  same  as 
feedback  constraints  can  always  be  endogenized  in  a  Lyapunov  function  using  Lagrange 
multipliers  [40],  leading  to  a  reinterpretation  in  terms  of  a  utility  function.  However,  we 
suggest  that,  by  stressing  the  importance  of  feedbacks  in  an  evolutionary  dynamical 
framework, the theory of Evolutionary feedback selection provides a better underpinning as 
well  as  understanding  of  human  cooperation,  as  witnessed  by  its  ability  to  provide 
quantitative  predictions  to  experiments.  We  stress  also  that  evolutionary  feedback  selection 
differs  from  group  selection  theory  [41],  as  it  does  not  necessarily  rely  on  the  competition 
between and/or the selection of groups. 
We have illustrated the theory of Evolutionary feedback selection by taking examples 
 
in which the feedbacks occur via punishments, namely, third party punishment, the ultimatum 
game and altruistic punishment, on which satisfactory quantitative agreement with available 
experiments  is  obtained.  The  general  conceptual  conclusion  that  can  be  drawn  from  our 
analysis  is  that  what  is  called  altruism  and  selfishness  might  not  be  paradoxical  behaviors. 
They  are  in  fact  the  two  faces  of  Janus.  Looking  back  in  the  past,  we  suggest  that  the 
Darwinian  selection  process  has  promoted  a  biological  and  cultural  reconciliation  between 
self-regarding and others-regarding preferences. Feedback by punishment (and probably also 
by reward and other mechanisms) might have been the trigger for adjusting social emotions to 
a level ensuring group efficiency, cascading to individual self-interest. Evolutionary feedback 
selection addresses human rationality in a systemic and evolutionary manner. An individual’s 
rationality,  her  social  emotions,  her  ability  to  understand  others’  desires,  to  transmit 
knowledge and social learning, influences group efficiency on a longer time scale. Survivors 
are  rationally  efficient  from  an  ecological  perspective.  The  extensive  cooperation  within 
human  societies,  the  biological  development  of  a  human  “social  brain”  [42]  and  the 
adjustment  of  reward-punishment  to  a  fine-tuned  level  constitute  co-evolutionary  processes, 
strengthening human groups in a Darwinian selection process. 

Looking  forward,  competition  could  keep  on  favoring  larger  groups  of  connected 
individuals, sharing common views and reciprocal fairness, enforced by Education and Law. 
The  future  could  however  look  very  different.  The  optimization  process  that  we  outlined 
regarding  altruistic  emotions is  not  unconditional.  It  constitutes  an  ongoing  process  of  local 
selection  under  ecological  constraints.  The  theory  does  not  claim  that  altruism  is  “optimal” 
but only that it has evolved to adapt to situations that have been experienced by our ancestors 
over many generations.  
 
Acknowledgements:  The  ideas  presented  here  have  benefited  from  stimulating  exchanges 
with Rob Boyd, Robin Dunbar, Alan Fiske, Stephen Le, Karthik Panchanathan, Jeff Satinover 
and  the  Human  Complex  Systems  group  at  UCLA.  We  are  grateful  to  Riley  Crane,  Urs 
Fischbacher, Georges Harras, Stephen Le, Yannick Malevergne and Francis Steen for helpful 
feedbacks on the manuscript. 
  
 

  

 

19 

(1964). 

76-81 (1995). 

103-119 (2001). 

561-574 (1998). 

67, 603-605 (1977). 

393, 573-577 (1998). 

Proc. R. Soc. Lond. B 268, 745-753 (2001). 

enforcement of social norms, Human Nature 13(1), 1-25 (2002). 

 
 
 
 
Bibliography 
 
[1] Fehr, E., Fischbacher, U,. and Gächter, S., Strong reciprocity, human cooperation, and the 
 
[2] Hamilton, W. D., Genetical evolution of social behavior I and II. J. Theor. Biol. 7, 1-52 
 
[3] Zahavi, A. The cost of honesty (further remarks on the handicap principle). J. Theor. Biol. 
 
[4] Gintis, H., Smith, E. & Bowles, S. Costly signalling and cooperation. J. Theor. Biol. 213, 
 
[5] Nowak, M. A.  and Sigmund, K. The dynamics of indirect reciprocity. J. Theor. Biol. 194, 
 
[6]Nowak, M. A. and Sigmund, K. Evolution of indirect reciprocity by image scoring. Nature 
 
[7] Leimar, O. and Hammerstein, P. Evolution of cooperation through indirect reciprocity. 
 
[8]  Trivers, R. The evolution of reciprocal altruism. Q. Rev. Biol. 46, 35-57 (1971). 
[9] Axelrod, R. The Evolution of Cooperation (Basic Books, New York, 1984). 
[10] Nowak, M. A.,May, R.M. & Sigmund, K. The arithmetics of mutual help. Sci. Am. 272, 
 
[11] Fehr, E. & Fischbacher, U. The nature of human altruism. Nature 425, 785-791 (2003)  
[12] Gintis, H., Bowles, S., Boyd, R. and Fehr, E., eds., Moral sentiments and material 
 
 
[13] Fehr, E. & Gächter, S. Altruistic punishment in humans. Nature 415, 137-140 (2002) 
[14] Richerson. P.J. and Boyd, R., Not By Genes Alone: How Culture Transformed Human 
 
Evolution, The University of Chicago Press, Chicago (2005). 
[15] Anderson, P.W., More is different, Science 177 (4047), 393-396 (1972). 
[16] Goldenfeld, N. and Kadanoff, L.P., Simple lessons from complexity, Science 284 (5411), 
 
[17] Fehr, E. & Gächter, S. The puzzle of human cooperation. Nature 421, 912-912 (2003). 
[18] Fehr, E. and Schmidt, K.M., A theory of fairness, competition, and cooperation, The 
 
[19] Damasio, A.R., Descartes’ error, emotion, reason, and the human brain, Grosset/Putnam, 
 
[20]  Bonanno,  G.A.,  Emotion  self-regulation,  In  Mayne,  T.J.  and  Bonanno,  G.A.  (Eds.), 
Emotions:  Current  Issues  and  Future  Directions,  251-285,  New  York,  London: 
 
 
Guilford Press (2001). 
[21]  Bechara,  A.,  Damasio,  H.  and  Damasio,  A.R.,  Emotion,  decision  making  and  the 
 
[22] Ghiselin, M.T. Darwin and Evolutionary Psychology. Science 179, 964-968 (1973). 
[23]  Barkow,  J.,  Cosmides,  L.  and  Tooby,  J.,  The  Adapted  Mind:  Evolutionary  Psychology 
 
[24] Tooby, J. and Cosmides, L. Friendship and the Banker's Paradox: Other pathways to the 
evolution of adaptations for altruism. In W. G. Runciman, J. Maynard Smith, & R. I. 
 

interests (the foundation of cooperation in economic life), The MIT Press, Cambridge, 
Massachussetts (2005). 

and The Generation of Culture, Oxford University Press (1992). 

orbitofrontal cortex, Cerebral Cortex 10, 295-307 (2000). 

Quarterly Journal of Economics August, 817-868 (1999). 

New York (1994) 

87-89 (1999). 

 

20 

Sci. 7,77-83 (2003). 

Cognitive Sciences 7, 38-42 (2003) 

perspective. Neuroimage 14, 170181 (2001) 

mind’ in story comprehension. Cognition 57, 109128 (1995) 

M.  Dunbar  (Eds.),  Evolution  of  Social  Behaviour  Patterns  in  Primates  and  Man. 
Proceedings of the British Academy, 88, 119-143 (1996). 

 
 
[25]  Vogeley,  K.  et  al.  Mind  reading :  neural  mechanisms  of  theory  of  mind  and  self-
 
[26] Vogeley, K. and Funk, G.R. Neural correlates of the first-person-perspective. Trends in 
 
[27]  Flechter,  P.  et  al.  Other  minds  in  the  brain :  a  functional  imaging  study  of  ‘theory  of 
 
[28] Gallagher, H.L. and Fritch, C.D. Functional imaging of ‘theory of mind’. Trends Cogn. 
 
[29] Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., and Gintis, H., Foundations of 
Human  Sociality:  Economic  Experiments  and  Ethnographic  Evidence  from  Fifteen 
 
 
Small-Scale Societies, Oxford University Press (2004). 
[30]  Camerer, C.F., Behavioral Game Theory, Princeton University Press  (2003) 
[31]  Oosterbeek,  H.,  Sloof,  R.  and  van  de  Kuilen,  G.,  Differences  in  Ultimatum  Game 
Experiments:  Evidence  from  a  Meta-Analysis,  Experimental  Economics  7,  171-188 
 
 
(2004). 
[32]  Boarini,  R.,  Laslier,  J-F.,  and  Robin,  S.,  Interpersonal  comparisons  of  utility  in 
bargaining: Evidence  from a transcontinental Ultimatum Game. Ecole Polytechnique. 
Laboratoire   d’Econométrie. Cahier n°2004-007 (2004). 

912-915 (2006). 

Chicago Press (1985) 

experimental investigation. Q. J. Econ. 108, 437-459 (1993). 

[33]  Boyd,  R.  and  P.J.  Richerson    Culture  and  the  Evolutionary  Process,  University  of 
 
[34]  Bernhard,  H.,  U.  Fischbacher  and  E.  Fehr,  Parochial  altruism  in  humans,  Nature  442, 
 
[35] Anderson, C.M. and L.Putterman, Do non-strategic sanctions obey the law of demand? 
The demand for punishment in the voluntary contribution mechanism, working paper 
 
 
of the Department of Economics, Brown University, Providence, RI (2006). 
[36]  Fehr,  E.,  Kirchsteiger,  G.  and  Riedl,  A.  Does  fairness  prevent  market  clearing?  An 
 
[37] Berg, J., Dickhaut, J. and McCabe, K. Trust, reciprocity and social history. Game Econ. 
 
[38] Hayashi, N., Ostrom, E., Walker, J. and Yamagishi, T. Reciprocity, trust, and the sense 
 
[39]  Damasio,  A.,The  Feeling  of  What  Happens:  Body  and  Emotion  in  the  Making  of 
 
[40] Hahn, W. Theory and Application of Liapunov's Direct Method. Englewood  Cliffs, NJ: 
 
[41] Wilson, D.S. and Sober, E.  Reintroducing group selection to the human behavioral 
 
[42] Dunbar, R.I.M., The social brain hypothesis. Evol. Anthropol. 6, 178-190 (1998). 

of control—a cross-societal study. Rational. Soc. 11, 27-46 (1999). 

sciences, Behavioral and Brain Sciences 17 (4), 585-654 (1994). 

Consciousness, Harcourt Brace and Company (1999). 

Behav. 10, 122-142 (1995). 

Prentice-Hall (1963). 

 

21 


 

4
0
0
2
 
n
a
J
 
4
 
 
 
9
0
0
1
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Pouzyry: a novel class of algorithms  
for restoring a function from a random sample 
Talk at ACAT’2003, KEK, 1-6 Dec. 2003 

F.V.Tkachov 

Institute for Nuclear Research of Russian Academy of Sciences 
Moscow, 117312, Russian Federation 

A novel class of algorithms for restoring a function from a random sample is based on the concept of 
weak convergence, borrows algorithmic solutions from the Optimal Jet Finder (hep-ph/0301185), of-
fers a considerable algorithmic flexibility, is applicable to non-positive functions, is insensitive to the 
choice of coordinate axes. A first implementation demonstrates feasibility of the approach. 

Small random raindrops 
can hardly hurt pouzyry. 
Oh! joy of research 

The fundamental problem of modelling a function 
from a random sample has two important applications: 
multi-dimensional adaptive MC integration (for a re-
cent review see [1]) and construction of quasi-optimal 
observables for data analysis [2].  

The conventional view is that a function is a way to 
provide a number 
f x  for any number x. However, 
( )
with x  measured via a finite precision measurement 
procedure, increasing the number of (unbiased, inde-
pendent) measurements increases the precision of the 
estimate of x (by taking the standard average) — but 
 
x dx
for  f   one  only  obtains  the  average 
where j  is the probability distribution for individual 
measurements of x . By improving the measurement 
procedure  one  makes  j   more 
narrow  and  thus  can  approach 
∫
 — but only 
f x
(
0
if  f   is  continuous  at  x0.  Otherwise 
the  result  depends  on  the  shape  of  j .  However,  in 
practice one rarely if ever cares about how the function 
is defined at the points of discontinuity.  

j
( ) ( )

j
( ) ( )

x dx

f x

f x

x0

x

)

j

∫

Therefore, it is logically sufficient to define a func-
j
  with  all 
tion  by  its  averages 
possible test functions j  that possess continuous de-
rivatives  of  any  order  (this  is a  technical restriction 

( ) ( )

x dx

j
f x

= ∫

f

,

,f j

imposed  for  technical  convenience  without  loss  of 
meaning) and are equal to zero outside  bounded re-
gions (each j  has its own such region). The averages 
 are linear in j , and one can define a “general-
ized function” as an arbitrary linear correspondence 
; a convenient abuse of notation is to 
f
:
j
. The familiar example is 
f x
( ) ( )
write 
the Dirac’s d -function.  

j
,
f
j

x dx

= ∫

j

f

,

To  discuss  approximations,  one  must  specify  the 
meaning of the proposition that a sequence of (gener-
alized)  functions  f n   convergence  to  a  (generalized) 
function f . The convergence motivated by the conven-
tional definition of functions is the pointwise conver-
gence,  i.e.  a  convergence  (albeit  with  uncorrelated 
rates) of all numerical sequences 
 for all 
x . Within the framework of the new interpretation, the 
true argument of a function is not x  but j , and the no-
is  modified  accordingly: 
tion  of  convergence 
  for  all  j ,  albeit  with  uncorrelated 
j
,
f
rates.  In  practice  n  stays  finite,  and  one  ensures  a 
smallness of the differences 
 for a fi-
j=

f x
( )

x
( )

j
,
f

nf

nf

nf

j

j

j

,

,

nite set of 

. 

k

We will write 

f

n

weak
n

f

 and use the qualifier 

“weak” to describe this type of convergence and re-
lated notions (closeness, etc.). As a first heuristic ap-
proximation, one may rely on the analogy between the 

 

ﬁ
ﬁ
ﬁ
ﬁ
-
ﬁ
¥
(cid:190)
(cid:190)
(cid:190)
ﬁ
 

weak convergence and metric convergences. 

j

x

j
,f

f x
( )

There are many advantages in replacing the archaic 
, with the 
“general functions”, i.e. mappings 
subtler notion  of  “generalized  functions”, i.e. linear 
mappings 
,  in  our  mental  arsenal  of 
mathematical concepts [3]. It is remarkable that such a 
finess of interpretation results in truly powerful new 
options for constructive problem solving. In the con-
text of particle physics, one example is the long-sought 
solution of the problem of asymptotic expansions of 
Feynman diagrams [4], which required an essential use 
of techniques of generalized functions (see a discus-
sion in [5]). Another example is the discovery of the 
optimal jet definition [6] where the optimal configura-
tion  of  jets  is  regarded  as  an  approximation  in  the 
sense of the weak convergence. It turns out that the lat-
ter idea has a much wider range of applicability, as is 
shown below. 

Consider a random sample of values  { }
N
=  of a 
x
n n
1
xp
. 
0
( )
random variable x  distributed according to 
The opening idea of the theory of statistics is that for 
N ﬁ
, the sample reproduces the probability distri-
( )xp
bution 
. What would be a precise interpretation 
for  that?  More  generally,  given  a  random  sample 

=

(
f x
n

)

, what’s the pre-

F
N

=

{

x
n

,

f

}
N
=
n n
1

, where 

f

xp
( )

 increasingly well for  N ﬁ

n
cise  meaning  of  the  statement  that  FN  represent 
( )
f x
Represent  FN   as  a 
∑
x
F x
( )
N
n
cise interpretation is as follows: 

? 
d -functions: 
sum  of 
. Then the required pre-
)

N

x

d

(

1

n

n

f

F x
( )
N

weak
N

xp
( ) ( )

f x

, 

(1) 

i.e. for any test function j , the sequence of its inte-
grals with the l.h.s. converges to its integral with the 
r.h.s., 

, in the usual sense. 

j
x
( ) ( )

p
( )
f x

x dx

∫

I am not aware of a textbook that would state this in 
an explicit fashion. This may be explained by the fact 
that mathematical statistics had already matured [7] by 
the time the ideas of generalized functions only began 
to be publicised [8]. It is important to clearly under-
stand, however, that the interpretation (1) is an essen-
tial starting point for all the thinking about how to 
obtain more tractable approximations for the r.h.s. in-
stead of the l.h.s.; the latter, however, is the only pos-
sible starting point (along with, perhaps, some a priori 
information about f p ).  

We will call such convenient approximations mod-
els, generically denote them as M(x), and require that 
such models M(x) provide constructive algorithms for: 

x M x
( )
A) the mapping 
B) generation of random x distributed according to 

 for any real x ; 

M(x), provided the latter is non-negative. 

One has to construct a model 

N fM
,

x  that would 
( )

be close in the weak sense to the “raw” approximation 
x  
NF x ,  which  fact  would  guarantee  that 
( )
f x
( )
remains close to 

 in the weak sense: 

N fM
,

xp
( )

( )

 
N fM
,

Moreover, if we impose restrictions on 

x  in 
( )

accordance with whatever a priori information we may 
x  is 
have about 
( )

x , we may hope that 
( )

N fM
,
x  in a stronger sense (uniform, etc.). 
( )

close to 

N fM
,

N fM
,

Mathematical results of this type are well known [9]. 

In practice, the following types of models are used: 
(i)  Decompositional models: the function’s domain 
of  definition  D is  split  into non-intersecting  subdo-
D
, and the model is 
mains, 
defined 
in  each  subdomain: 
.  D k  can  either  be  fixed  (as  with 
M x
( )

,
to  be  constant 

const

=D

= ˘
k¢

D

D

D

∪

=

k

k

k

k

k

m

.  

x
( )

x
( )

P
k

= ∑

M x
( )

, where 

dx F x
( )
N

standard histograms) or found adaptively. 
P
k k

  is  an  orthogonal  system  of  functions,  and  

(ii)  Galiorkin models: 
k xP
( )
= ∫
m
k
(iii)  Parametric models: one chooses a function pa-
rameterized by a number of parameters and adjusts the 
latter to fit the sample. 
(iv)  The Vegas model is employed in the Vegas rou-
tine for multidimensional integration [10]. It is a direct 
product of one-dimensional adaptive decompositional 
models, 
. The popularity of Vegas 
M x
)
(
i
i
shows that even a very crude approximation can be a 
valuable model in many dimensions. 
(v)  Kernel  models  consist  in  the  following.  Let 
( )K x   be  any  convenient  (usually  hat-like)  function 
such that  

M x
( )

= (cid:213)

i

 

ﬁ
ﬁ
‡
¥
¥
-
”
-
ﬁ
¥
(cid:190)
(cid:190)
(cid:190)
ﬁ
ﬁ
˙
 

s
 

(

)

(

)

d

x

. 

R

dim

( )

x
)n

xd
( )

x-
n

1
K R x

NF x  with 

weak
(2) 
K x
( )
R
0
R
Then  it  is  sufficient  to  replace  the  individual  d -
. In 
 in 
functions 
K x
(
R
general, R should be smaller for larger N. 
(vi)  NN models. The most popular simplest neural 
networks [11] are described by the analytical expres-
∑
sion 
, where g is a 

j
smooth step-like function. If one drops the outermost 
g  (which  is  irrelevant  in the  present  context),  there 
remains  a  linear  combination  of  rotated  and  shifted 
step-like functions. This is to be compared with the 
kernel models: rotated and shifted step-like functions 
roughly correspond to infinite-R kernels positioned at 
infinite points in various directions. 

(
= ∑
g

M x
( )

A x
kj

c g
k

B
k

+

(

)

)

k

j

The kernel models (v) remain, perhaps, least stud-
ied.  The  approach  seems  to  become  impractical  for 
large N — the case which is often the most interesting. 
It could be advantageous to “condense” the sum of d -
functions to a smaller number. This, of course, must be 
done  so  as  to  ensure  a  weak  closeness  of  the  con-
densed sum to the original one: 
∑ ɶ
f

ɶ
F x
( )
P

F x
( )
N

(3) 

ɶ
f
0

weak

+
p

ɶ
x

, 

x

d

(

)

p

1
P

p

i.e. so as to minimize the differences  

j

F
N

ɶ
j
F
P

 

for test functions j . Then the scheme becomes 

The replacement (3) based on minimization of (4) is 
exactly  what is effected in the optimal jet definition 
[6]. Repeating the reasoning of [6] with appropriate 
simple modifications, one arrives at the following cri-
ɶ
teria for finding  pxɶ
pf

 and 

: 
One  introduces  an  N P·

  matrix, 

0

n pz
,

1

, 

- ∑
1

z
p n p

,

0

,  and  sets 

ɶ
f

p

= ∑

z

f
,
n n p n

,

 

= ∑

z

f x
n n p n n

,

,

  so  that 

,n pz

  becomes  the  un-

known. The matrix z is found by from the requirement 
of minimization of the following expression: 

z
n
ɶ ɶ
f x
p p

W =
R

1
2

R

∑

p n
,

z

n p
,

f

p

(

x
n

ɶ
x

)2
+
p

∑

n

z

n

f

p

. 

This controls the remainder of a Taylor expansion of 
, with the leading terms nullified by the 
(4) in  n
x
ɶ
pf

above  restrictions on 

. The parameter R  

 and 

ɶ
x-

pxɶ

p

remains free. 

,n pz

with 

with 

The  minima  of  W

R   correspond  to  configurations 
 equal to 0  or 1. The set of  sample points 
ɶ
=  constitutes the p-th pouzyr 1.  pxɶ
 and  pf
1
are the pouzyr’s location and weight (or charge). The 
domain spanned by a pouzyr is always convex, cen-
tered at  pxɶ

, and its radius does not exceed R .  

n pz
,

 

It should be empasized that the criterion of mini-
mizing W
R  is a constructive expression of the require-
ment  to  make  the  original  configuration  of  sample 
points and the resulting configuration of pouzyry as 
close as possible in the weak sense. 

It is remarkable that the first step of the model con-
struction within the pouzyry approach (the left down-
ward arrow in (5)) is performed within the realm of 
singular  generalized  functions.  In  this  respect  the 
pouzyry scheme is entirely novel and unusual.  

An exploratory algorithm to minimize W

R  was ob-
tained by modifying the Optimal Jet Finder [12]. To 
ensure mathematical correctness of the resulting modi-
fied algorithm and that subtle bugs are not introduced 
during the modification, I chose to work with the stati-
cally safe, modular, object-oriented programming lan-
guage Component Pascal [13], and chose the verifica-
tion version of the optimal jet finder [14], designed for 
robustness, as a starting point for modifications. Below 
is a discussion of the first findings: 
1)  One has to choose P and R  prior to running the 
minimization. From general considerations, P should 
not  be  chosen  greater  than  N   (which  is  large 
enough for practical purposes). An optimal choice of 
R  depends on the function; it may e.g. correspond to a 
typical distance over which first order derivatives of 
the function vary appreciably.  
2)  The key element of the minimum search algo-
rithm here is an iteration step which scans all sample 
points once and modifies the configuration of pouzyry. 
This iteration step is the only highly technical element 

                                                        
1 Pouzyr means “bubble” in Russian, the plural being pouzyry, to be 
pronounced according to the rules of the French language, with the 
stress on the last syllable. 

(4) 

(5) 

 

-
-
ﬁ
”
(cid:190)
(cid:190)
(cid:190)
ﬁ
-
»
-
”
-
£
£
”
‡
-
 

)

(

O P N·

of the algorithm not really subject to variations (apart 
from possible optimizations). All other elements allow 
variations. For instance, the shapes of kernels are arbi-
trary. Similarly, each kernel in the resulting sum can 
be given its own R depending on the effective radius 
of the corresponding pouzyr. Such variations should 
be employed to incorporate the properties of the solu-
tion to maximal degree. 
3)  The  time required to  execute  a  single  iteration 
step  is  the  same  as  in  the  case  of  the  Optimal  Jet 
Finder [12], 
, i.e. linear in both the number 
of sample points and the number of pouzyry. The CPU 
time per iteration is a fraction of a second on a 866 
MHz computer for dim = 7, N = 200, P = 10. 
4)  The resulting configuration of pouzyry depends 
on  the  initial  one  (the  starting  point  for  minimum 
search). With a purely random choice (all zn,p random) 
all the pouzyry are initially located near the middle of 
the integration domain (the effect of averaging). This 
may not be optimal. It may help to devising smarter 
ways to choose the initial configuration. 
5) 
In some cases (e.g. for large R) one may observe 
the  following  behavior:  at  first  the  configuration  of 
pouzyry  converges  pretty  fast,  but  then  the  conver-
gence  slows  down  greatly;  the  configuration  may 
change  significantly  over  O(100)  iterations.  This 
means that a straightforward minimization may not be 
an optimal strategy in the more complex cases. 
6)  Several  ideas  to  improve  upon  the  simplest 
pouzyry  scheme  suggest  themselves,  namely:  (a)  to 
“breed” better configurations of pouzyry using e.g. the 
ideas of genetic algorithms; (b) to make R depend on p 
in the formula for W
R ; (c) to seek a model which is a 
sum of pouzyry configurations with various R : contri-
butions with larger R  would describe larger-scale be-
havior  of  the  function,  whereas  contributions  with 
smaller R would describe narrow structures. 
7)  Since OJF reliably finds narrow clusters [12], the 
pouzyry scheme is guaranteed to reliably find narrow 
spikes in the initial sample. More generally, the better 
a narrow structure can be approximated by a sum of 
spikes, the better pouzyry would work.  
8)  As  an  example,  consider 
function 
r r
, where r is the euclidean dis-
f x
/
tance from x  to the diagonal of the hypercube, and r  
describes the width of the diagonal strip within which 
f  is non-zero. For dim = 7, r = 0.1, N = 200 (only sam-
ple points with non-zero values of f  were retained and 
counted), P = 10, R = 0.1, it takes about 15 iterations to 
reach a minimum. The quality of the resulting model 

( ) MAX(1

the 

, 0)

=

can be judged from how much better is the MC inte-
gration with the probability distribution corresponding 
to the model compared with the simplest MC with uni-
formly distributed sample points. In the present case, 
despite the obviously too low P (not enough to cover 
the entire diagonal at the chosen R ), the improvement 
of the statistical integration error is by a factor of 7. 
Note that the Vegas algorithm is completely  defeated 
by  such  diagonal  structures,  whereas  the  pouzyry 
scheme is completely insensitive to the choice of co-
ordinates.  On  the  other  hand,  with  an  unfortunate 
choice of P or R , the pouzyry scheme may not yield 
any significant improvement over the simplest Monte 
Carlo integration. 
9)  A promising idea is to combine the pouzyry ap-
proach, which deals well with narrow structures, with 
other  methods  geared  towards  description  of  the 
smooth  global  structure  of  the  function,  e.g.  the 
Galiorkin type methods. Note that the pouzyry scheme 
does contain a constant background already (f 0 in (3)) 
The described pouzyry scheme is novel and rather 
unusual, and given all the algorithmic options it opens, 
it is hard to assess its potential at present. I should be 
content to have demonstrated its feasibility. 

I thank A.Czarnecki for hospitality at the University 
of  Alberta  (Edmonton,  Canada)  where  the  idea  of 
pouzyry  came 
to  me;  V.Ilyin  for  comments; 
K.Tobimatsu for encouragement. This work was sup-
ported in parts by the Natural Sciences and Engineer-
ing Research Council of Canada, KEK, and the orga-
nizing committee of the ACAT’03 workshop.  

References 
[1]  E. de Doncker, NIM A502 (2003) 358. 
[2]  F.V. Tkachov, arxiv.org: physics/0001019. 
[3]  R.D. Richtmyer, Principles of Advanced Mathematical 

Physics, vol. 1 (Springer-Verlag, 1978). 

[4]  F.V. Tkachov, hep-ph/9703424; hep-ph/9701272. 
[5]  F.V. Tkachov, hep-th/9911236. 
[6]  F.V. Tkachov, hep-ph/9901444. 
[7]  C.R. Rao, Bull. Calcutta Math. Soc. 37 (1945) 81-91;  
H. Cramer, Aktuariestidskrift 29 (1946) 458-463. 
[8]  L.Schwartz, Théorie des Distributions. Hermann, Paris, 

1950–1951 (1st ed.). 

[9]  A.N. Tikhonov and V.Ya. Arsenin, Methods of Solving 

Ill-Posed Problems, NAUKA, Moscow, 1986. 
[10] G.P. Lepage, J. Comput. Phys. 27, 192 (1978). 
[11] P.C. Bhat, hep-ex/0106099. 
[12] D.Yu.  Grigoriev,  E.  Jankowski,  F.V.  Tkachov,  hep-

ph/0301185; hep-ph/0301226 

[13] www.oberon.ch 
[14] F.V. Tkachov, hep-ph/0111035. 

 

-

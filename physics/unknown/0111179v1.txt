National  Ignition  Facility  (NIF)  Control  Network  Design  and  Analysis

R. M Bryant, R. W. Carey, R. V. Claybourn, G. Pavel, W. J. Schaefer
LLNL, Livermore, CA 94550, USA

THDT004

Abstract

The  control  network  for  the  National  Ignition
Facility  (NIF)  is  designed  to  meet  the  needs  for
common  object  request  broker  architecture  (CORBA)
inter-process communication, multicast video transport,
device triggering,  and  general  TCP/IP  communication
within  the  NIF  facility.  The network will  interconnect
approximately  650  systems,  including  the  embedded
controllers,  front-end  processors  (FEPs),  supervisory
systems,  and  centralized  servers  involved  in  operation
of the NIF. All systems are networked  with  Ethernet to
serve  the  majority  of  communication  needs,  and
asynchronous transfer mode (ATM) is  used to  transport
multicast  video and  synchronization  triggers.  CORBA
software  infra-structure  provides  location-independent
communication  services  over  TCP/IP  between  the
application  processes  in  the  15  supervisory  and  300
FEP  systems.  Video  images  sampled  from  500  video
cameras at  a  10-Hz frame rate will  be  multicast  using
direct ATM  Application  Programming  Interface  (API)
com-munication  from  video  FEPs 
to  any  selected
operator  console.  The  Ethernet  and  ATM  control
networks  are  used  to  broadcast  two  types  of  device
triggers for  last-second functions  in  a  large  number  of
FEPs, 
the  need  for  a  separate
infrastructure  for  these  functions.  Analysis,  design,
modeling,  and  testing  of  the  NIF  network  has  been
performed to provide confidence that the network  design
will meet NIF control requirements.

thus  eliminating 

1 INTRODUCTION

the 

The NIF is being developed for laser fusion and high-
energy-density  experimental  studies  [1].  NIF  will
consist of 192  laser beam lines  that  are focused onto  a
target  within 
target  chamber.  The  Integrated
Computer  Control  System  (ICCS)  is  being  developed
to  provide  distributed  control  and  monitoring  of  the
approximately  60,000  control  points  and  over  500
video sources in  NIF  [2].  This  integration  is  provided
by  approximately  650  computer  systems  distributed
throughout  the  facility.  The  ICCS  network  provides
connections  for  the  16  operator  workstations,  300
FEPs,  275  embedded controllers,  40  industrial  control
and  safety  systems,  14  distributed  workstations,  the
central process server, the  file  server, and 150  network
outlets  distributed 
for
throughout 
connection  of  portable  computers  and  diagnostic
systems.  FEPs  and  embedded  controllers  interface  to,
and  are  located  in  close  proximity  to,  the  control

facility 

the 

is  orchestrated  by 

points.  Overall  control 
the
supervisory systems  and operator workstations  that  are
centrally  located  in  the  NIF  computer  and  control
rooms.  Real-time  control,  where  required,  is  provided
within  the  FEPs.  A  separate  timing  system  provides
precise  timing  signals  down  to  30-psec  accuracy  for
control and diagnostic systems during the 2-second shot
interval [3].

The  ICCS  is  divided  into  subsystems  to  partition
activity  and  ensure  performance.  There  are  10
supervisor software applications that conduct NIF  shots
in  collaboration with  17  kinds  of  FEPs  as  shown  in
Figure 1.

S hot Director
NC11

Be am
Co n trol
N C 14

Laser
Di agnost ic
NC15

Injec tio n
L aser
MOR N L11133
PA M NL11224

Target
Di agnost ic
NT82

Po wer
Con ditioning
N L13142

PEPC
NL 123252

Shot
Servic es
N C12

Supervis ory Subsys tem Layer

A utomatic
A lig nment
N C 13

O ptics
Insp ec tio n
N C 16

Wavef ront
C ontr oller
NL12151
Qty 24

Hartmann
Image Pr oc.
NL12455
Qty 24

Mast er
Os cilla tor
N L11131
Qty 2

Preamp
Mo dule
N L11222
Qty 48

Pre cisi on
Di agnost ic
N L2233
Qty 2

Laser
En ergy
N L42
Qty 6

Laser
Pow er
N L43
Qty 12

Target
Di agnost ic
NT321
Qty 1

Po wer
Con ditioning
N L13141
Qty 4

Indust ri al
Cont rols
N C44
Qty 1

Swit ch
Puls er
NL 123251
Qty 4

Plasma
Puls er
NL 123251
Qty 4

Pu lse
Dia gno st ic
NL 123251
Qty 4

Front-End Proc essor
Layer

A lig nment
Co ntr ols
N L41
Qty 102

Sp eci al
CCD
N L44
Qty 15

Pr oc es s
Vid eo
N C41
Qty 26

Timin g
N C21
Qty 14

Figure 1 NIF Integrated Computer Control Subsystems

The FEP computers are based on  either VxWorks on
PowerPC or Solaris on UltraSPARC  processors. These
systems are primarily diskless and will  be  bootstrapped
over the network.

The supervisory system hardware consists of operator
consoles  and  workstations  in  the  main  control  room
and  a  central  multiprocessor  server  in  the  computer
room.  These  systems  are  based  on  Sun  Solaris
platforms.  There  are  eight  operator  consoles—shot
director,  industrial  controls,  laser  diagnostics,  optical
pulse  generation, 
alignment/
wavefront,  power  conditioning,  and  an  auxiliary
console.  Each  console  consists  of 
two  operator
workstations  with  three  20-in.  liquid  crystal  displays
each.

target  diagnostics, 

The ICCS architecture uses the  client-server software
model  with  event-driven  communications.  It  is  based
on  a  scalable  software  framework  that  is  distributed

over  the  supervisory  and  FEP  computers  [4].  The
framework  offers 
interoperability  among  different
computers and operating systems  by  using  CORBA  to
provide  TCP/IP  message  transport  for  interprocess
communication.

2 NETWORK  REQUIREMENTS

Network  communication  requirements  have  been
identified  for  each  FEP,  embedded  controller,  and
supervisory  system  as  a  basis  for  network  design.
Performance requirements collected included boot  image
size,  initialization  message  rates  and  sizes,  peak
message  rates  and  destination,  and  shot  archive  and
history data sizes.

The traffic flow  is  primarily  between the  centralized
Supervisory  systems  and  the  distributed  FEPs.  The
majority of network traffic will  be  asynchronous point-
to-point messages that do  not  have stringent  latency or
jitter  requirements.  This  includes  control  messages,
sensor data, boot  images,  archive and history  data, etc.
This 
standard  TCP/IP
communication  using  Ethernet  switching  technology.
The exceptions to  this  are the  requirements  for  digital
video transport and network triggers.

suitable 

traffic 

for 

is 

to 

the  500  video  cameras 

Digital  video traffic comes from  the  54  video  FEPs
located
that  interface 
throughout  the  NIF  facility.  A  video  FEP  can
selectively  grab  single  camera  images  when  requested
(for  example  by  the  automatic  alignment  system)  or
can send a  stream of  frames at  up  to  10  frames/sec for
real-time  viewing  at  the  operator  workstations.  All
digitized frames are uncompressed  640  ×  480  ×  8  bits
or  roughly  2.5  Mbits  per  frame.  At  10  frames/sec,  a
video  stream  requires  approximately  25  Mbits/s  of
bandwidth. Compression  is  not  used  because  it  would
impose  excessive  processing  load  on  the  sending  and
receiving  systems,  and  there  is  sufficient  network
bandwidth  to  support  uncompressed  video  transfer.
Each operator workstation  is  required to  display  up  to
two  video  streams.  Each  video  FEP  will  be  able  to
source at least two concurrent video streams.  The video
stream  will  be  multicast  when  multiple  operator
workstations want to view the same camera.
triggers  are  short  messages 

that  are
broadcast  or  multicast  to  particular  FEPs  to  trigger
specific  events  just  prior  to  firing  the  laser.  These
require  a  network  latency  of  less  than  5  msecs.  The
FEPs  use  network  triggers  to  initiate  a  time-critical
function,  such  as  to  notify  the  video  FEPs  to  capture
the  next  video  frame  and  to  prepare  the  alignment
control  systems  for  an  imminent  shot.  The  video

Network 

capture trigger is  sent  over the  ATM  network  across a
multicast  permanent  virtual  circuit 
(PVC).  The
alignment  control  system  trigger is  broadcast  over  the
Ethernet network at 100 Mb/s.

3 NETWORK  DESIGN  AND
ANALYSIS

All  ICCS  systems  are  networked  with  Ethernet,
which  provides  for  the  majority  of  communication
needs  (Figure  2).  ATM  is  used  for  specific  video
transport.  The  Ethernet  and  ATM  networks  are  not
directly connected. All systems  with  ATM  connections
also have Ethernet connections.

S u per v iso r y Conso le s (8 x  2)
S u per v iso r y Conso le s (8 x  2)

. . .
. . .

LLNL
LLNL
Netwo rk
Netwo rk

C entra l
C entra l
C entra l
Server
Server
Server

File
File
File
Server
Server
Server

Info
Info
Diodes
Diodes

Core Ether net
Core Ether net

Sw itch
Sw itch
(100 Mb/s ,  Gb/s )
(100 Mb/s ,  Gb/s )
. . .
. . .

ICCS
ICCS
Network
Network

Edge Switch
Edge Switch
(10, 100 Mb/s)
(10, 100 Mb/s)
...
...

Edg e Switch
Edg e Switch
(10, 100 Mb/s)
(10, 100 Mb/s)
...
...

Core ATM
Core ATM
Sw itch
Sw itch

(155 Mb/s )
(155 Mb/s )
. . .
. . .

FEPFEP
FEP

FEPFEP
FEP

FEPFEP
FEP

FEPFEP
FEP

F EPF EP
F EP

FEPFEP
FEP

Di ag nostic an d control points
Di ag nostic an d control points

Ca mera
Ca mera
Ca mera

Ca me ra
Ca me ra
Ca me ra

...
...

Figure 2: NIF ICCS Network Diagram

The  Ethernet  network  uses  switching  technology
throughout  and  will  be  operated  as  a  flat,  layer  2
network.  Thus,  all  switching  will  be  based  on  the
Ethernet media access control (MAC) address. This  will
simplify  system  configuration and  maintainability  and
will  ensure that  network triggers on  the  Ethernet  will
propagate with  a  minimum  of  latency  throughout  the
network. A  core Ethernet switch  is  centrally located in
the  NIF  computer  room  and  provides  connectivity  to
the  edge  switches  that  are  located  thoughout  the
facility.  Fiber  cables  connect  the  core  switch  to  the
edge  switches  using  100  Mbit/s  Ethernet.  With  this
architecture,  there  are  generally  only  one  or  two
switches  between  any  two  communicating  systems,
keeping the end-to-end latency low.

The core Ethernet switch is a modular chassis system
supporting  100  Mbit/s  and  Gbit/s.  It  has  layer  2
(MAC)  switching  and layer 3  (IP)  routing  capabilities
and redundant common logic boards and power supplies
for  increased  reliability.  The  central  server  and  file

the  10-msec 

requirements.  This 

network  loads  over  Ethernet  and  ATM.    The  results
verified  that  the  trigger  latencies  will  consistently  be
well  within 
is
primarily due to the wire-speed performance of  the  full-
the
duplex  network 
throughput  and  latency  of  a  video  FEP  and  operator
workstation  using  the  peak  network  design  loading
were  also  performed.  The  results  confirmed  that  the
systems  and  network  will  provide 
required
performance.

switches.  Simulations  of 

the 

4 SUMMARY

switching 

off-the-shelf 

The  ICCS  network  has  been  designed  using  high-
performance 
technology.
Testing  and  analysis  indicate  that  the  network  design
will  easily  meet  the  control  system  throughput  and
latency  requirements  for  various  types  of  traffic.  The
network design also  provides sufficient capabilities  for
expansion and  higher performance as  may  be  needed in
the future.

This  work performed under  the  auspices of  the  U.S.

DOE by LLNL under contract No. W-7405-Eng-48.

REFERENCES
[1] E.  Moses  et  al.,  “The  National  Ignition  Facility:
Status  and  Plans  for  Laser  Fusion  and  High-
Energy-Density 
Studies,”
ICALEPCS 2001, San Jose, CA, Nov. 2001.

Experimental 

[2] L.  J.  Lagin  et  al.,  “Overview  of  the  National
Ignition  Facility  Distributed  Computer  Control
System,”  ICALEPCS  2001,  San  Jose,  CA,  Nov.
2001.

[3] R.  A.  Lerche et  al.,  “The  NIF  Integrated  Timing
System  –  Design  and  Performance,”  ICALEPCS
2001, San Jose, CA, Nov. 2001.

[4] R.  W  Carey  et  al.,  “Large-scale  CORBA-
distributed Software Framework for NIF  controls,”
ICALEPCS 2001, San Jose, CA, Nov. 2001.

server,  the  most  heavily  loaded  systems,  connect
directly to this switch using Gbit Ethernet.

Two  types  of  edge  switches  are  used:  copper-based
10/100  switches  and  fiber-based  10  Mbit/s  switches.
Most of  the  edge Ethernet switches  are 24-port  10/100
units  with  two  100  Mbit/s  multimode  fiber  uplink
ports. The 10 Mbit/s fiber switches  connect to  systems
that  require  electrical  isolation  such  as  the  power
conditioning  embedded  controllers.  The  edge  switches
connect to the core Ethernet switch over 100 Mb/s  fiber
links.

Based  on 

traffic  design
the  peak  Ethernet 
requirements  for  the  various  FEP  and  supervisor
systems, aggregate message rates and individual system
message rates leading up  to  and following  a  shot  were
calculated.  Peak  aggregate  message  rate  requirements
will be less than 2,200 messages  per second, which  are
well  within  the  performance  capabilities  of  current
Ethernet  switches  that  provide  wire-speed  throughput.
Individual  system  message  rates  will  also  be  well
within the capabilities  of  the  network switches and the
computer systems.

The  ATM  network  provides  OC-3  (155  Mbit/s)
connections  to 
the  video  FEPs  and  the  operator
workstations  for multicast  digital  video  transport.  The
ATM network  also  transports  a  multicast  video trigger
from the master timing system  to  the  video FEPs  over
a multicast PVC.

ATM  was selected for  its  efficiency  in  transporting
the  network  design  phase,
digital  video.  During 
performance  tests  were  executed  to  compare  Ethernet
and ATM performance when sending large video images
(307  KBytes/image). A  key  goal  was to  minimize  the
central  processing  unit  (CPU)  utilization  required  for
communication on  the  operator workstations  and video
FEPs.  It  was found that  using  the  ATM  API  directly
provided very high  throughput  with  only  a  small  load
on the CPUs involved. The key reason  for this  was the
larger message transfer unit  (MTU) size available with
ATM compared to  Ethernet,  which  reduces the  number
of packets that need to  be  processed per image  transfer.
The multicast  video application  software uses  the  XTI
API provided with the network interface card.

attached 

Simulations were performed using the MIL3 OPNET
network  modeling 
tool.  Network  models  were
developed  consisting  of  Ethernet  and  ATM  switches
systems.
with 
Customized  models  were  developed  for  dual-homed
systems  attached 
to  both  Ethernet  and  ATM.
Simulations  of  the  expected  latency  for  the  network
triggers  were  performed  given  various  background

supervisory 

and  FEP 


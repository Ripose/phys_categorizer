SEPARATION OF MULTIPLE EVOKED RESPONSES USING
DIFFERENTIAL AMPLITUDE AND LATENCY VARIABILITY

Kevin H. Knuth1, Wilson A. Truccolo2, Steven L. Bressler2, Mingzhou Ding2

1 Center for Advanced Brain Imaging and Cognitive Neuroscience and Schizophrenia Dept.,
Nathan S. Kline Institute, Orangeburg NY 10962
2 Center for Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton FL 33431

ABSTRACT

In  neuroelectrophysiology  one  records  electric  potentials
ensembles  of
fields  generated  by 
or  magnetic 
synchronously  active  neurons  in  response  to  externally
presented  stimuli.    These  evoked  responses  are  often
produced  by  multiple  generators  in  the  presence  of
ongoing  background  activity.    While  source  localization
techniques or current source density estimation are usually
used  to  identify  generators,  application  of  blind  source
separation  techniques  to  obtain  independent  components
has become more popular.

We  approach  this  problem  by  applying  the  Bayesian
methodology  to  a  more  physiologically-realistic  source
model.  As it is generally accepted that single trials vary in
amplitude and latency, we incorporate this variability into
the model.  Rather than making the unrealistic assumption
that  these  cortical  components  are  independent  of  one
another,  our  algorithm  utilizes  the  differential  amplitude
and  latency  variability  of  the  evoked  waveforms  to
identify the cortical components.  The algorithm is applied
to 
in
monkeys performing a visuomotor task.

intracortically-recorded 

field  potentials 

local 

1. INTRODUCTION

The  techniques  of  neuroelectrophysiology  rely  on  the
recording of electric potentials or  magnetic  fields evoked
in the brain during the presentation of stimuli in cognitive
or sensorimotor tasks.  These evoked responses are  often
generated  by  multiple  ensembles  of  neurons  firing
synchronously  in  response  to  the  presented  stimuli.    Far
from  being  independent,  these  neural  ensembles,  also
referred to as generators or sources, are often dynamically
coupled  in  unknown  ways  that  are  of  interest  to  the
experimenter.  Thus the recording channels, electrodes in
superconducting
electroencephalography 

(EEG)  and 

________________________________________________________________

Thanks to the Charles E. Schmidt College of Science computing
facilities at FAU. Supported by NIMH, NSF, ONR and a CNPq
(Brazil) fellowship.

devices 

(SQUIDs) 

interference 

quantum 
in
magnetoencephalography  (MEG),  record  linear  mixtures
of  evoked  responses  from  these  sources  approximately
time-locked  to  the  experimental  stimulus  in  addition  to
ongoing  background  activity.    Because  the  mixing  is
linear, 
recent
developments in linear blind source separation (BSS) and
independent  component  analysis  (ICA)  have  been  useful
in analyzing EEG and MEG signals using both ensemble
averaged data [1,2,3] and single-trials [4,5], when applied
with care [6].

instantaneous  and  often 

stationary, 

However,  by  assuming  independence  of  the  sources,
the  experimenter  assumes  away  one  of 
the  most
interesting  aspects  of  the  active  neural  ensembles  in  the
brain,  the  nature  of  their  dynamical  interactions.    In
addition,  by  working  with  ensemble  averages  of  the
responses, which is typically done to improve the signal to
noise  ratio,  one  is  implicitly  assuming  that  the  evoked
waveform is identical in all respects in every experimental
trial.  In this work we introduce a more realistic model of
the evoked response, one which includes the possibility of
trial-to-trial  variability  of  source  amplitude  and  latency.
By adopting this modeling approach, we find that we can
(i) more accurately account for trial-to-trial variability [7],
(ii) utilize the differential variation in the amplitudes and
latencies 
identify  sources,  (iii)  avoid  enforcing
statistical  independence  of  the  sources,  and  (iv)  more
accurately estimate the ongoing background activity.

to 

2. MODELING EVOKED RESPONSES

We  model  the  evoked  response  from  a  single  source  by
assuming that the signal has a stereotypic waveshape, but
can vary  in amplitude from trial-to-trial.  In  addition,  the
response  is  not  assumed  to  be  strictly  time-locked  to  the
stimulus, rather the onset latency of the response can vary
from  trial-to-trial.    We  write  this  mathematically  as

t

 

)

-ts
(

)((cid:215)s

represents 

a
stereotypic
the 
,  where 
waveshape of the response,  a
 represents the amplitude of
the response in that trial and  t represents the onset latency
shift.    As  we  are  estimating  both  the  waveshape  and  the
amplitude and latency, there is a degeneracy in the model.
To eliminate this degeneracy, we take as a convention that
the  ensemble  average  amplitude  of  the  response  over  the
recorded  trials  is  unity  and  the  ensemble  average  latency
is zero.

As  described  above  there  may  be  multiple  neural
sources,  as  well  as  multiple  detectors.    To  describe  the
source-detector coupling  we introduce  a  coupling  matrix,
C , which is commonly known as a mixing matrix in BSS.
In addition, an experimenter records many trials.  For the
rth  recorded  trial  we  write  the  signal  recorded  in  the  mth
detector in component form as

x

mr

)(
t

=

a

mn

nr

(
t

s

n

)

t

nr

+

h

)(
t

,

mr

 (1)

N

C
=
1

n

t

nr

  is  the

where n indexes the N neural sources,  mnC  is the coupling
a
between  the  mth  detector  and  the  nth  source, 
  is  the
amplitude of the nth source during  the  rth  trial, 
latency  of  the  nth  source  during  the  rth  trial, 
ns
the  nth  source,  and 
the
)(tmr
waveshape  of 
unpredictable  signal  component  recorded  in  the  mth
detector  during  the  rth  trial.    This  unpredictable  signal
component  is  a  combination  of  the  recorded  background
activity  along  with  any  noise  in  the  channel.    For
simplicity,  we  assume  that  this  unpredictable  signal
component has zero mean.

  is  the

nr
( )(cid:215)

is 

 

h

3. BAYESIAN DERIVATION OF THE
ALGORITHM

Bayes'  Theorem  is  the  natural  starting  point  because  it
allows  one  to  describe  the  probability  of  the  model  in
terms  of  the  likelihood  of  the  data  and  the  prior
probability of the model

(
model

p

|

data

,

I

)

=

(
data

p

|

,
model
(
data
p

)
(
pI
model
)I
|

)

|

I

, (1)

where  I  represents  any  prior  information  one  may  have
about  the  physical  situation.  Bayes'  Theorem  can  be
viewed  as  describing  how  one's  prior  probability,
P(model |  I), is  modified by  the  acquisition  of  some  new
information.

To apply this to our problem, we consider the change
in our knowledge about the model with the acquisition of
new  data  consisting  of  a  set  of  recorded  trials  x(t)
recorded  by  a  set  of  detectors.    In  this  case,  Bayes'
Theorem can be written as

(
(,
t
sC

p

),

) =I

(
t

),

x(cid:1)(cid:2)
|
,
(
x

|)(
t

p

,
sC

(
)
pI,
,
sC
)I
|)(
t

(cid:1)(cid:2)
,
(
x

)(
,t
p

)(
,t

(cid:1)(cid:2)
,

|

I

)

,

  (2)

a

a

2

R

}

=(cid:2)

.,..,

,
1
that  maximizes 

where  boldface  symbols  represent  the  entire  set  of
{
a
parameters of each type, eg. 
.  As  we
would  like  to  find  the  model 
the
probability  in  Equation  (2),  in  practice  we  rewrite  the
equation as a proportionality and equate the inverse of the
  to  the  implicit
prior  probability  of  the  data 
proportionality constant
)(cid:181)I

(
|)(x
t

(
(,
sC

  (3)

)I

(
t

),

),

p

p

t

x(cid:1)(cid:2)
|
,
(
x

|)(
t

p

,
sC

)(
,t

(cid:1)(cid:2)
,

)
pI,

(
,
sC

)(
,t

(cid:1)(cid:2)
,

|

)I

.

The  probability  on  the  left-hand  side  of  Equation  (3)  is
referred  to  as  the  posterior  probability.    It  represents  the
probability that a given  set of hypothesized values  of  the
model  parameters  accurately  describes 
the  physical
situation.    The  first  term  in  the  on  the  right-hand  side  is
the  likelihood  of  the  data  given  the  model.      It  describes
the  degree  of  accuracy  with  which  we  believe  the  model
can  predict  the  data.    The  final  term  in  the  numerator  is
the  joint  prior  probability  of  the  model,  also  called  the
prior.  This prior describes the degree to which we believe
the  model  to  be  correct  based  only  on  our  prior
information  about  the  problem. 
  It  is  through  the
assignment of the likelihood and priors that we express all
of  our  knowledge  about  the  particular  source  separation
problem.

For  simplicity,  the  joint  prior  can  be  factored  into
independent  physical

terms  each 

representing 

four 
processes,

p

(
(,
sC
(
x

p

  

t

),

x(cid:1)(cid:2)
|
,

(
t

),

)(cid:181)I

|)(
t

,
sC

)(
,t

(cid:1)(cid:2)
,

)
pI,

(
C

|

)
pI

(
s

|)(
t

)
pI

(
(cid:2)

|

)
pI

(
(cid:1)

  (4)
)I

|

.

For  the  amplitude  and  latency  priors,  we  assign  uniform
densities  with  appropriate  cutoffs  denoting  a  range  of
physiologically realizable values.

At  this  point  the  relationship  between  our  emerging
algorithm  (disregarding  trial-to-trial  variability)  and  the
popular ICA algorithm introduced by Bell and Sejnowski
[8] can be most easily noted. In the Bayesian derivation of
ICA  [6,9]  one  assigns  a  delta  function 
likelihood
expressing  noise-free  linear  mixing,  as  well  as  a  source
(often  a  super-  or  sub-Gaussian
amplitude  prior 
probability density).  With these two assignments, one can
easily obtain a posterior probability for the mixing matrix
alone  by  marginalizing  over  all  possible 
source
amplitudes.    A  gradient  ascent  method  to  find  the  most
probable separation matrix completes the derivation.

(cid:229)
-
Our derivation continues by utilizing the principle of
maximum entropy to assign a Gaussian likelihood [10,11]
by  introducing  a  parameter  s   reflecting  the  expected
square-deviation between our predictions and the mean

(
(,
sC

),

(cid:1)(cid:2)
,

,

s

|

x

(
t

),

)(cid:181)I

p
(
p
2

t
)

MRT
2

2

s

Exp

1
s
2

Q

2

s
(

p

|

)
pI

(
C

|

)
pI

|(
s

I

)

,

(
s

)I

|

p

  is  the  prior  probability  for  s   and  Q
where 
represents the sum of the square of the residuals between
the data and our model in (1)

=

Q

M

R

T

m

=
1

=
1

r

=
1

t

N

=
1

n

x

mr

)(
t

a

C

mn

s

n

(
t

nr

t

)

nr

,

(6)

2

with  M  representing  the  number  of  detectors,  R  the
number  of  experimental  trials,  and  T  the  number  of
recorded time points per trial.  Assigning a  Jeffreys prior
joint
for 
posterior  over  all  possible  values  of  s   we  obtain  a
marginal  posterior 
for  our  original  set  of  model
parameters

,  and  marginalizing 

-= s

the 

s , 

(
s

p

)

I

1

|

(
(,
sC

p

t

),

x(cid:1)(cid:2)
|
,

(
t

),

I

)

MRT
2

Q

p

(
C

|

)
pI

|(
s

I

)

,

  (7)

which  is  related  to  the  Student  t-distribution.    Note  that
the  uncertainty  in  our  predictions  expressed  as  s   is  not
only  dependent  on  the  noise  covariance,  but  is  also
dependent  on 
in  our
measurements  and  the  inadequacies  of  our  model  to
describe the physical situation.

the  potential  uncertainties 

At  this  point  prior  information  regarding  the  source
waveforms could be used to further constrain the possible
solutions.    In  addition,  knowledge  of  the  source-detector
coupling,  which  is  found  by  solving  the  electromagnetic
forward problem, could be utilized to create an algorithm
that  simultaneously  performs  source  separation  and
localization  [12,13].    For  simplicity,  we  choose  rather  to
assume  complete  ignorance  and  assign  uniform  priors.
The 
the  posterior  probability  can  be
logarithm  of 
compactly written as

ln

P

-=

ln

Q

+

const

,

(8)

MRT
2

.

t

),

)I

the 

is 
(
),
t

posterior 

probability

P 
x(cid:1)(cid:2)
,
|

where 
(
(,
p
sC
The  algorithm  is  completed  by  solving  for  the  most
probable  set  of  model  parameters,  also  called 
the
Maximum  A  Posteriori  (MAP)  estimate.    Examining  the
first partial derivative of the log posterior  with respect to
the jth source waveshape at time q gives

ln
P
)(
qs
j

-=

MRT
2

1

Q

Q
)(
qs
j

(9)

  (5)

Q
)(
qs
j

-=

2

[

M

R

m

=
1

=
1

r

CW

mj

a

jr

(
C

a

mj

jr

)

2

)(
qs
j

  (10)

]

with

where

=
xW

mr

+

t

(

q

)

jr

a

C

mn

(
qs
n

nr

t

nr

+

t

jr

)

.    (11)

N

=
1
n
jn

The  term  W  is  important  as  it  deals  with  the  data,  which
has  been  time-shifted  according  to  the  latency  of  the
.  From this one
component being estimated, 
x

t+

)

(
mr q

jr

subtracts off all the other components after they have been
time-shifted,
appropriately 
(
C
qs
.    The  derivative  of  the  log
n

scaled 
t
)

and 

mn

+

nr

nr

jr

t

a

probability  is  zero  when  the  scaled  estimated  waveshape
equals  W.    Thus  one  can  obtain  an  expression  for  the
optimal  waveshape of the jth source  in  terms  of  the  other
sources

)(ˆ
qs
j

=

M

R

=
1
m
M

=
1
r
R

m

=
1

=
1

r

CW
(
C

mj

a

mj

a

jr

jr

)

2

.

Similarly for the source amplitudes, one obtains

ˆa

jp

=

M

T

m

=

1
M

=

t

1
T

m

=

1

=

1

t

[
VU

]

2

V

where

and

U

=

x

mp

)(
t

a

C

mn

s

n

(

t

np

t

)

np

N

=
1
n
jn

=

V

sC
mj

j

(
t

t

)

,

jp

(12)

(13)

(14)

(15)

such  that  the  solution  is  given  by  the  projection  of  the
detector-scaled  component 
onto  the  data

t

sC
mj
the  other  scaled  and 

(
t

jp

)

j

removing 

time-shifted
after 
components.    This  is  related  to  matching  filter  solutions.
The  optimal  source-detector  coupling  coefficients  are
found similarly

œ
ß
ø
Œ
º
Ø
-
-
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:247)
(cid:247)
ł
(cid:246)
(cid:231)
(cid:231)
Ł
(cid:230)
-
-
-
(cid:181)
¶
¶
¶
¶
-
(cid:229)
(cid:229)
-
¶
¶
(cid:229)
„
-
-
-
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:247)
(cid:247)
(cid:247)
ł
(cid:246)
(cid:231)
(cid:231)
(cid:231)
Ł
(cid:230)
-
-
(cid:229)
„
-
-
(16)

(17)

(18)

(19)

ˆ
C

ij

=

r

=
1
S

t

=
1
T

S

T

[
YX

]

2

Y

=
1

r

=
1

t

where

and

=

X

x

ir

)(
t

a

C

in

s

n

(

t

nr

t

)

nr

N

=
1
n
jn

=

a

Y

s

j

(

t

jr

t

)

.

jr

for  maximizing 

Estimating  the  latency  parameter  using  the  approach
taken for the other parameters leads to a complex solution
as  the  latency  appears  implicitly  as  the  argument  of  the
waveshape  function.    Rather  we  examine  the  necessary
form  Q.
conditions 
Expanding  the  square  in  (6),  one  can  see  that  as  the
 is varied, only the cross-terms corresponding
latency 
to the jth source change as long as the source waveshapes
are  zero  outside  of  a  closed  time  interval.    The  optimal
estimate of the latency 

 can be found by maximizing

the  quadratic 

tˆ

jp

t

jp

=Z

M

T

m

= =
1 1
t

a

C

mj

s

j

(
t

jp

t

)

jp

x

mp

)(
t

a

C

mn

s

n

(
t

np

t

)

np

,

N

=
1
n
jn

which  is  the  cross-correlation  between  the  estimated
source and the data after the contributions from the other
sources  have  been  subtracted  off.    This  is  then  averaged
over all the detectors.  In practice, as  a  discrete  model  is
,  we  utilize  a
being  used  for  the  source  waveshapes 
discrete  set  of  latencies  with  resolution  equal  to  the
sampling rate.

)(ts

Iterating  equations  (12),  (13),  (16)  and  (19)  over  all

sources and trials completes the algorithm.

4. RESULTS

To  demonstrate  this  approach,  we  apply  the  technique  to
intra-cortically  recorded 
local  field  potentials  from
macaque striate cortex during a visuomotor GO - NOGO
pattern recognition task [14].  We consider data recorded
from  only  a  single  channel  and  show  that  sufficient
component
information 
waveforms.  The algorithm is easily modified to consider
single  channel  recordings  by  setting  the  number  of
the  source-detector  coupling
channels 

infer  multiple 

exists 

to 

coefficients 
ensemble of 222 trials recorded during the GO response.

.    The  data  utilized  consists  of  the

1=M
  and 
=
nC
1

1

the 

the  degree  of 

To  demonstrate 

trial-to-trial
variability  that  exists  in  these  data  sets,  Figure  1  below
shows  three  examples  of  recorded  single  trials  (noisy
waveforms).    We  calculate  the  average  event-related
potential  (AERP)  by  taking  the  ensemble  average  of  the
waveforms in the 222 trials and overlay this waveform on
the  single-trial  waveforms  after  amplitude  scaling  and
latency  shifting  according  to  Eqns.  13  and  19  where  the
AERP is used as the sole source waveform.

Fig. 1. Examples  of  three  single-trial  recordings  from  a
striate channel with the AERP overlaid to demonstrate the
trial-to-trial variability in the evoked responses.

Examination  of  the  shape  of  the  AERP  suggests  the
contribution  of  multiple  neural  sources.    In  this  example
we set the number of components to be identified to three,
and utilize the local shapes of the AERP around the three
extrema as initial guesses for the component waveforms in
the  MAP  estimation.    The  algorithm  will  utilize  the  fact

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:247)
(cid:247)
(cid:247)
ł
(cid:246)
(cid:231)
(cid:231)
(cid:231)
Ł
(cid:230)
-
-
(cid:229)
„
-
(cid:229)
(cid:229)
(cid:229)
„
œ
œ
œ
ß
ø
Œ
Œ
Œ
º
Ø
(cid:247)
(cid:247)
(cid:247)
ł
(cid:246)
(cid:231)
(cid:231)
(cid:231)
Ł
(cid:230)
-
-
-
that  the  different  components  will  exhibit  differential
variability to identify them.  Figure 2 shows the resulting
evoked  component  waveforms.    As  expected  the  three
components exhibit different variances in their single-trial
}14.0,0.1,05.0
latencies
amplitudes, 
}2
{
s
  for  the  first,  second  and
third  components  respectively.    An  examination  of  the
residual  variance,  as  described  in  [7],  shows  that  these
more detailed  models better account  for the event-related
variability.

,0.123

6.132

,0.24

2 =a

2
=t

and 

ms

{

 

s

Fig. 2 Three  extracted  component  waveforms  each  of
which  displays  unique  variability  in  both  amplitude  and
latency.

It  should  be  noted  that  although  three  component
waveforms  were  extracted,  one  cannot  be  sure  of  the
number of neural sources responsible for these signals.  It
is  possible  that  a  single  neural  ensemble  generated
Component  1  in  response  to  initial  feedforward  input  to
the  area,  followed  by  Component  3  resulting  from
subsequent 
from  a  higher  cortical  area.
However,  it  is  known  that  these  component  waveforms
exhibit  different  trial-to-trial  variability.    To  attempt  to
identify  whether  two  components  are  from  the  same
source, one must perform a multiple detector experiment.
In that case, the coupling matrix would have two identical
columns corresponding  to  identical  coupling  between  the
sources of these components and the array of detectors.

feedback 

5. CONCLUSIONS

to 

  There  are  several  advantages 

In  the  past,  several  researchers  have  utilized  maximum
likelihood techniques to approach the problem of trial-to-
trial  variability  and  source  identification  [15,16,17,18].
the  evoked
Here  with  a  more  detailed  model  for 
responses,  we  have  presented  a  more  general  algorithm,
which  in  addition  to  characterizing  responses  in  single
trial  data  identifies  component  waveforms  based  on  their
differential  trial-to-trial  variability.    The  algorithm  is
derived  by  approaching  the  problem  as  an  exercise  in
model  parameter  estimation  by  applying  Bayesian
inference. 
this
methodology.  First, the strategy is strongly model-based,
such that a failure of the algorithm can be traced back to
inadequacies  of  the  model  to  represent  the  physical
situation  or  to  assumptions  made  in  its  implementation.
Second,  any  inadequacies  of  the  model,  once  identified,
can be readily remedied given sufficient knowledge about
the  situation.    Third,  once  the  model  has  been  estimated
the  residual  data  can  be  examined  to  investigate  the
possibility  of  additional  unimagined  phenomena.    For
the
instance, 
identification 
low-frequency
components.    However,  it  is  known  that  there  exist  high
frequency  signals  in  this  data  (such  as  gamma  band
bursts),  but  historically  their  characterization  has  been
difficult.  By accurately estimating the contributions from
identifiable sources, these effects can be removed to allow
researchers to investigate  more sensitive signals.  Fourth,
the  Bayesian  methodology  allows  one  to  incorporate
additional  prior  knowledge  into  the  problem  to  improve
one's inferences.

technique  seems  well  suited 
of 

relatively 

large, 

this 

to 

into 

some 

insights 

the  dynamical 

[6,9]  provides 

Comparing  the  derivation  of  this  algorithm  to  the
Bayesian  derivation  of  Bell  and  Sejnowski's  ICA
the
algorithm 
relationships  between  these  two  techniques.    While  both
algorithms  assume  that  the  signal  mixing  is  linear,
stationary  and  instantaneous,  this  algorithm  does  not
require  independence  of  the  sources.    This  is  key  in
neuroelectrophysiology  as 
interactions
between  neural  source  generators  is  a  matter  of  great
scientific 
algorithm
accommodates  noise  as  well  as  different  numbers  of
sources  and  detectors.    Finally,  by  defining  the  source
model  to  explicitly  allow  for  the  potential  variability  of
the  source  activity  in  individual  trials  we  make  this
additional 
in  source
  The  algorithm  was  demonstrated  by
identification. 
identifying  three  components  in  a  set  of  data  recorded
from  a  single  channel.    As  expected,  these  components
exhibited  differential  variability  in  both  amplitude  and
latency.

information  available 

addition, 

interest. 

to  aid 

this 

In 

 

In this algorithm, we chose as a  model of the source
the

waveforms  a  set  of  discrete  points  describing 

waveform  amplitude  at  regular  intervals.    This  source
model is typically used BSS and ICA applications, where
the prior probability of these source amplitudes have been
given by  super- or sub-Gaussian  probability  densities.    It
is  important  to  note,  for  this  algorithm  and  others,  that
other  source  models  are  possible  and  in  many  cases
desirable.    These  models  could  be  continuous  in  nature
(especially  in  the  case  of  continuous  latency  shifts)  such
as  linear-piecewise  or  cubic  spline  models,  or  could  be
dynamical in nature such as linear autoregressive moving
average (ARMA) models.

into 

Finally,  as  described  in  previous  works  [9,12,13],
there  are  often  cases  where 
the  experimenter  has
knowledge  about  the  forward  problem,  which  describes
the  propagation  of  the  signals  to  the  detectors.    In  such
situations,  one  can  incorporate  information  about  the
forward  problem 
the  algorithm  along  with
information  about  the  geometry  of  the  detector  array  by
deriving  appropriate  prior  probabilities  for  the  coupling
(or  mixing)  matrix.    In  situations  where  the  source
locations  are  of  interest,  abandoning  the  coupling  matrix
in favor of a more detailed model of the  source positions
and  orientations  may  be  more  fruitful.    By  modeling  the
source locations in addition to the source waveforms, one
can  easily  design  an  algorithm 
that  simultaneously
performs source separation and localization [13].

6.  REFERENCES

in 

brain 

[1] S. Makeig, T.-P. Jung, A. Bell, D. Ghahremani, T.J.
Sejnowski,  "Blind  separation  of  auditory  event-
independent
responses 
related 
components.",  Proc.  Natl.  Acad.  Sci.  USA,
94:10979-84, 1997.
J. Särelä, R. Vigário, V. Jousmäki, R,  Hari,  E.  Oja,
"ICA for the extraction of auditory evoked fields", in
4th International Conference on Functional Mapping
of  the  Human  Brain  (HBM'98),  Montreal,  Canada,
1998.

[2]

[3] R.  Vigário,  J.  Särelä,  V.  Jousmäki,  E.  Oja,
"Independent  component  analysis  in  decomposition
of  auditory  and  somatosensory  evoked  fields",  in
International Workshop on Independent Component
Analysis (ICA'99), pp. 167-72, 1999.

[4] T.-P. Jung, S. Makeig, M. Westerfeld, J. Townsend,
E.  Courchesne,  T.J.  Sejnowski,  "Independent
component  analysis  of  single-trial  event-related
potentials", 
on
Independent Component Analysis (ICA'99), pp. 173-
8, 1999.

International  Workshop 

in 

[5] T.-P. Jung, S. Makeig, M. Westerfeld, J. Townsend,
E.  Courchesne,  T.J.  Sejnowski,  "Analyzing  and
visualizing  single-trial  event-related  potentials",  in
Advances in Neural Information processing Systems,
11:118-24, 1999.

[6] K.H.  Knuth,  "Difficulties  applying  recent  blind
source separation techniques to EEG and  MEG",  in
Maximum  Entropy  and  Bayesian  Methods,  Boise
1997, G.J. Erickson, J.T. Rychert, C.R. Smith (eds.),
Dordrecht:  Kluwer  Academic  Publishers,  pp.  209-
22, 1998.

[7] W.A. Truccolo, M. Ding, K.H. Knuth, R. Nakamura,
S.L.  Bressler,  "Trial-to-trial  variability  of  cortical
evoked  responses:  implications  for  the  analysis  of
functional connectivity", submitted, 2001.

[8] A.J.  Bell,  T.J.  Sejnowski,  "An 

information-
maximization  approach  to  blind  source  separation
and deconvolution", Neural Comp, vol. 7, pp. 1129-
1159, 1995.

[9] K.H.  Knuth,  "A  Bayesian  approach  to  source
separation", 
on
Independent Component Analysis (ICA'99), pp. 283-
8, 1999.

International  Workshop 

in 

[10] E.  T.  Jaynes,  Probability  Theory  -  The  Logic  of

Science, unpublished, available at:
http://bayes.wustl.edu

[11] D.  S.  Sivia,  Data  Analysis.  A  Bayesian  Tutorial,

Oxford: Clarendon Press, 1996.

[12] K.H.  Knuth,  "Bayesian  source  separation  and
localization",  in  Proceedings  of  SPIE:  Bayesian
Inference  for  Inverse  Problems,  vol.  3459,  A.
Mohammad-Djafari (ed.), pp. 147-58, 1998.

electromagnetic 

[13] K.H.  Knuth  and  H.G.  Vaughan,  Jr.,  "Convergent
Bayesian  formulations  of  blind  source  separation
and 
in
Maximum  Entropy 
and  Bayesian  Methods,
Garching,  Germany  1998,  W.  von  der  Linden,  V.
Dose,  R.  Fischer,  R.  Preuss  (eds.),  Dordrecht:
Kluwer Academic Publishers, 217-26, 1999.

estimation", 

source 

[14] S.L.  Bressler,  R.  Coppola,  R.  Nakamura,  "Episodic
at  multiple
multiregional 
frequencies during visual task performance.", Nature
266:153-6, 1993.

coherence 

cortical 

[15]  C.D.  Woody  "Characterization  of  an  adaptive  filter
for  the  analysis  of  variable  latency  neuroelectric
signals.",    Medical  Biological  Engineering,  5:539-
53, 1967.

[16] D.T.  Pham,  J.  Mocks,  W.  Kohler,  T.  Gasser,
"Variable  latencies  of  noisy  signals:  estimation  and
testing in brain potential data." , Biometrika, 74:525-
33, 1987.

[17] D.H.  Lange,  H.  Pratt,  G.F.  Ingbar,  "Modeling  and
estimation  of 
single  evoked  brain  potential
components.", IEEE Trans. Biomed. Eng., 44:791-9,
1997.

[18] P. Jaskowski, R. Verleger, "Amplitude and latencies
of  single-trial  ERP's  estimated  by  a  maximum-
likelihood  method.",  IEEE  Trans.  Biomed.  Eng.,
46:987-93, 1999.


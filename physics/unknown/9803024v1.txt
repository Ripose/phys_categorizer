8
9
9
1
 
r
a

M
 
8
1
 
 
]
h
p
-
h
t
a
m

[
 
 
1
v
4
2
0
3
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Algebras, Derivations and Integrals

R. Casalbuoni1
D´epartement de Physique Th´eorique, Universit´e de Gen`eve
CH-1211 Gen`eve 4, Suisse

e-mail:

CASALBUONI@FI.INFN.IT

ABSTRACT

In the context of the integration over algebras introduced in a previous paper,
we obtain several results for a particular class of associative algebras with
identity. The algebras of this class are called self-conjugated, and they in-
clude, for instance, the paragrassmann algebras of order p, the quaternionic
algebra and the toroidal algebras. We study the relation between deriva-
tions and integration, proving a generalization of the standard result for the
Riemann integral about the translational invariance of the measure and the
vanishing of the integral of a total derivative (for convenient boundary condi-
tions). We consider also the possibility, given the integration over an algebra,
to deﬁne from it the integral over a subalgebra, in a way similar to the usual
integration over manifolds. That is projecting out the submanifold in the
integration measure. We prove that this is possible for paragrassmann alge-
bras of order p, once we consider them as subalgebras of the algebra of the
(p + 1) × (p + 1) matrices. We ﬁnd also that the integration over the subal-
gebra coincides with the integral deﬁned in the direct way. As a by-product
we can deﬁne the integration over a one-dimensional Grassmann algebra as
a trace over 2 × 2 matrices.

UGVA-DPT 1998/03-1000

PACS: 02.10, 02.10.S, 03.65.F

1On leave from Dipartimento di Fisica, Universit`a di Firenze, I-50125 Firenze, Italia

1 Introduction

Quantum mechanics has modiﬁed in a very profound way the classical under-
standing of the phase space of a physical system, making it non-commutative.
This is reﬂected in a drastical change of the mathematics involved by pro-
moting the classical phase space variables to operators acting on a Hilbert
space. However, the ﬂavor of the classical description is retained in the path-
integral formulation of quantum mechanics.
In this case, although in the
context of a rather diﬀerent physical interpretation, one retains the concept
of trajectories in the phase space. The situation changed again with the dis-
covery of supersymmetric theories [1]. Although there are no experimental
hints about their physical reality, the beauty of the mathematical structure
involved has lead to an enormous amount of eﬀorts in their understanding.
But then, being supersymmetry tied to space-time invariance, one has to give
up also to path-integration in terms of commuting variables. The space where
the Feynman trajectories are deﬁned gets enlarged to involve anticommuting
variables (elements of a Grassmann algebra) related in an unavoidable way
to the phase space coordinates by supersymmetry transformations. Also,
considering the simple example of the supersymmetric particle [2], one real-
izes that due to the constraints involved, the space-time variables, xµ, loose
their commutation properties. This aspect is not often emphasized, because
it can be avoided by requiring the constraints to be satisﬁed as conditions
on the physical states (the deﬁnition of chiral superﬁelds), rather than solve
them directly [2]. Finally the matrix realization of the M-theory introduces
non-commuting coordinates (matrix-valued) for the D0-branes [3]. Following
these considerations we found interesting to introduce, for a general algebra,
the concept of integration [4], since it would play a vital role in the deﬁ-
nition of the path-integration for these more general theories. To this end
we started from the general approach to noncommutative geometry [5, 6].
That is from the observation that, in the commutative case, a space can be
reconstructed from the algebra of its functions. Starting directly with the
algebraic structure one can face situations where there are no concrete re-
alizations of the space. Said that we have still to ﬁnd a way to deﬁne the
integration over an algebra. This can be done by lifting up to the algebra
level the concept of integration over the space. To this end, let us look at the
physics beyond the path-integral formalism. The physical amplitudes satisfy
the composition law in an automatic way within this formalism, and this

2

arises from the completeness relation which, in the case of a one-dimensional
system, reads

|xihx| dx = 1

Z

(1.1)

Suppose that we have a set of orthonormal states in our Hilbert space, {|ψni}.
Then we can convert the completeness relation in the space R1 into the
orthogonality relation for the wave functions ψn(x) = hx|ψni,

hψm|xihx|ψnidx =

ψ∗

m(x)ψn(x) = δmn

Z

Z

(1.2)

On the other side, given this equation, and the completeness relation for the
set {|ψni}, we can reconstruct the completeness in the original space R1, that
is the integration over the line. Now, we can translate the previous properties
of the set {|ψni}, in the following two statements

1. The set of functions {ψn(x)} span a vector space.

2. The product ψn(x)ψm(x) can be expressed as a linear combination of

the functions ψn(x), since the set {ψn(x)} is complete.

All this amounts to say that the set {ψn(x)} is a basis of an algebra. In order
to capture completely the context of eq. (1.2), we need also to understand
the general meaning of ψ∗
n(x)
can be expressed as a linear combination of the functions ψn(x),

n(x). From the completeness it follows that ψ∗

ψ∗

n(x) =

ψm(x)Cmn

Xm

The matrix C has to satisfy certain conditions that we will discuss in the
text. In the following we will consider associative algebras with identity and
with a matrix C satisfying suitable conditions. These algebras will be called
self-conjugated. In these cases we will deﬁne the integral over the algebra by
eq. (1.2)

(1.3)

(1.4)

where {xi} is a basis of the algebra. The properties that the C matrix has to
(x) xi,
satisfy are such that the integral of an arbitrary element of the algebra,
R
must be compatible with (1.4) and with the algebra product. This will be

xjCjixk = δik

Z(x) Xj

3

discussed in the following Section. This procedure and his motivations have
been widely illustrated, in many examples, in ref.
[4]. There we discussed
also how to deal with important cases as the bosonic oscillator, or the q-
bosonic oscillator algebras, where a suitable C matrix does not exist.
In
this paper we will be interested in discussing some general result valid for
self-conjugated algebras. In more detail we will prove the following results:

1. A theorem relating derivations on the algebra satisfying the integration
by part rule (the vanishing of the integral of the derivation of an arbi-
trary element of the algebra) and automorphisms leaving invariant the
integration measure. This is an extension of the theorem relating the
invariance of the Riemann integral under translations and the vanishing
of a total derivative (for convenient boundary conditions). We stress
this point for its relevance within the path-integral approach, where
the validity of the Schwinger’s quantum principle depends precisely on
the validity of this theorem.

2. All inner derivations, that is the derivations given by commutators,

satisfy the integration by part rule.

3. The algebra of the N × N matrices, AN , is a self-conjugated algebra,
with the integration given by the trace. The integration by part rule
corresponds here to the cyclic property of the trace.

4. Given the integral over an algebra one can think of inducing it on a
subalgebra. This is done in the particular case of a paragrassmann
algebra of order p, Gp, (that is generated by an element θ such that
θp+1 = 0) thought as a subalgebra of Ap+1, the algebra of the (p + 1) ×
(p + 1) matrices. The idea is to project out the subalgebra from the
algebra, as one deﬁnes the integration over a submanifold by projecting
it out from the manifold.
In fact we will express the corresponding
integral as a trace of the representative of the elements of Gp in Ap+1,
times an operator which projects out Gp from Ap+1. In particular, this
will allow us to get the integral over a Grassmann algebra (p = 1) as a
trace over 2 × 2 matrices.

This paper is organized as follows:

in Section 2 we will recall the main
concepts necessary to deﬁne the integration over an algebra [4]. In Section

4

3 we will study the relation between self-conjugated and involutive algebras.
In Section 4 we will deﬁne the integration over the algebra of the N × N
matrices. In Section 5 we will introduce the concept of derivation and we
will derive the results 1), 2) and 3) mentioned above. The result 4) will be
obtained in Section 6.

2 Algebraic integration

We recall here some of the concepts introduced in [4], in order to deﬁne
the integration rules over a generic algebra. We start by considering an
algebra A given by n + 1 basis elements xi, with i = 0, 1, · · · n (we do not
exclude the possibility of n → ∞, or of a continuous index). We assume the
multiplication rules

xixj = fijkxk

(2.1)

with the usual convention of sum over the repeated indices. For the future
manipulations it is convenient to organize the basis elements xi of the algebra
in a ket

|xi =

(2.2)

or in the corresponding bra

hx| = ( x0 x1

· · · xn )

(2.3)

Important tools for the study of a generic algebra are the right and left
multiplication algebras. We deﬁne the associated matrices by

Ri|xi = |xixi,

hx|Li = xihx|

(2.4)

i aiRi, and a
For a generic element a =
similar equation for the left multiplication. In the following we will use also

i aixi of the algebra we have Ra =

P

P

LT

i |xi = xi|xi

(2.5)

x0
x1

·
·
xn

























5

The matrix elements of Ri and Li are obtained from their deﬁnition

(Ri)jk = fjik,

(Li)jk = fikj

(2.6)

The algebra is completely characterized by the structure constants. The
matrices Ri and Li are just a convenient way of encoding their properties.
In the following we will be interested in associative algebras. By using the
associativity condition

xi(xjxk) = (xixj)xk

(2.7)

one can easily show the following relations (all equivalent to the previous
relation)

RiRj = fijkRk, LiLj = fijkLk,

[Ri, LT

j ] = 0

(2.8)

The ﬁrst two say that Ri and Li are linear representations of the algebra,
called the regular representations. The third that the right and left multipli-
cations commute for associative algebras. In this paper we will be interested
in associative algebras with identity, and such that there exists a matrix C,
satisfying

We will call these algebras self-conjugated. The condition (2.9) is consistent
with Li and Ri satisfying the same algebra (see eq. (2.8)). Therefore, the
non existence of the matrix C boils down two the possibility that the algebra
admits inequivalent regular representations. This happens, for instance, in
the case of the bosonic algebra [4]. The condition of symmetry on C can be
interpreted in terms of the opposite algebra AD, deﬁned by

The left and right multiplication in the dual algebra are related to those in
A by

Therefore the matrices LT

RD

i = LT
i ,

i = RT
LD
i
i are a representation of the dual algebra

i LT
LT

j |xi = xjxi|xi = fjikLT

k |xi

(2.9)

(2.10)

(2.11)

(2.12)

(2.13)

Li = CRiC −1
C T = C

i xD
xD

j = fjikxD
k

6

We see that the condition C T = C is equivalent to require that the relation
(2.9) holds also for the right and left multiplication in the opposite algebra

i = CRD
LD

i C −1

(2.14)

Since we are considering associative algebras, the requirement of existence
of an identity is not a strong one, because we can always extend the given
algebra to another associative algebra with identity. In fact, let us call F
the ﬁeld over which the algebra is deﬁned (usually F is the ﬁeld of real or
complex numbers). Then, the extension of A (call it A1) is deﬁned by the
pairs

(α, a) ∈ A1, α ∈ F,

a ∈ A

with the product rule

(α, a)(β, b) = (αβ, αa + βb + ab)

The identity in A1 is given by the pair

I = (1, 0)

αI + a

Of course, this is the same as adding to any element of A a term proportional
to the identity, that is

and deﬁning the multiplication by distributivity. One can check easily that
A1 is an associative algebra. An extension of this type exists also for many
other algebras, but not for all. For instance, in the case of a Lie algebra one
cannot add an identity with respect to the Lie product (since I 2 = 0). For
self-conjugated algebras, Ri has an eigenbra given by

hxC| = hx|C,

hxC|Ri = xihxC|

(2.19)

as it follows from (2.9) and (2.4). Then, as explained in the Introduction, we
deﬁne the integration for a self-conjugated algebra by the formula

|xihxC| = 1

Z(x)

where 1 is the identity in the space of the linear mappings on the algebra.
In components the previous deﬁnition means

(2.15)

(2.16)

(2.17)

(2.18)

(2.20)

(2.21)

xixkCkj = fikpCkj Z(x)

Z(x)

xp = δij

7

This equation is meaningful only if it is possible to invert it in terms of
(x) xp. This is indeed the case if A is an algebra with identity (say x0 = I)
[4], because by taking xi = I in eq. (2.21), we get
R

xj = (C −1)0j

Z(x)

(2.22)

We see now the reason for requiring the condition (2.9). In fact it ensures
that the value (2.22) of the integral of an element of the basis of the algebra
gives the solution to the equation (2.21). In fact we have

xixkCkj = fikpCkjC −1

0p = (C −1LiC)0j = (Ri)0j = f0ij = δij

(2.23)

Z(x)

as it follows from x0xi = xi. Notice that the symmetry of C allows us to
write the integration as

|xCihx| = 1

Z(x)

(2.24)

which is the form we would have obtained if we had started with the same
assumptions but with the transposed version of eq. (2.4). All the examples
considered in ref. [4], where the C matrix exists, turn out to correspond to
self-conjugated algebras. The examples we are referring to are the algebra
over the circle, the paragrassmann algebras of order p, and the quaternionic
algebra.
[7] we have considered noncommuting toroidal algebras,
which also turn out to be self-conjugated ones [8].

In ref.

We will deﬁne an arbitrary function on the algebra by

f (x) =

fixi ≡ hx|f i

Xi

f ∗(x) =

f ∗
i xjCji = hf |xCi

Xij

(2.25)

(2.26)

and its conjugated as

where

|f i =

hf | = ( f ∗
0

f ∗
1

· · · f ∗

n )

(2.27)

f0
f1
·
·
xn










,










8

and ∗ is the complex-conjugation on the coeﬃcients fi belonging to the ﬁeld
|C. Then a scalar product on the algebra is given by

hf |gi =

hf |xCihx|gi =

Z(x)

f ∗
i gi

Xi

(2.28)

3 Algebras with involution

In some case, as for the toroidal algebras [7], the matrix C turns out to deﬁne
a mapping which is an involution of the algebra. Let us consider the property
of the involution on a given algebra A. An involution is a linear mapping
∗ : A → A, such that

(x∗)∗ = x,

(xy)∗ = y∗x∗,

x, y ∈ A

(3.1)

Furthermore, if the deﬁnition ﬁeld of the algebra is |C, the involution acts as
the complex-conjugation on the ﬁeld itself. Given a basis {xi} of the algebra,
the involution can be expressed in terms of a matrix C such that

The eqs. (3.1) imply

from which

(x∗

i )∗ = x∗

j C ∗

ji = xkCkjC ∗
ji

From the product property applied to the equality

x∗
i = xjCji

CC ∗ = 1

Ri|xi = |xixi

we get

and therefore

that is

(Ri|xi)∗ = hx∗|R†

i = hx|CR†

i = (|xixi)∗ = x∗

i hx∗| = x∗

i hx|C

hx|CR†

i C −1 = xjCjihx| = hx|LjCji

CR†

i C −1 = LjCji

9

(3.2)

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

(3.8)

If Ri and Li are ∗-representations, that is

CR†

xiC −1 = Lx∗

i

R†

xi = Rx∗

i = Rxj Cji

or also

we obtain

CR†

xiC −1 = CRx∗

i

C −1 = Lx∗

i

Since the involution is non-singular, we get

CRiC −1 = Li

and comparing with the adjoint of eq. (3.11), we see that C is a unitary ma-
trix which, from eq. (3.4), implies C T = C. Therefore we have the theorem:

Given an associative algebra with involution, if the right and left multipli-
cations are ∗-representations, then the algebra is self-conjugated.

In this case our integration is a state in the Connes terminology [5].
If the C matrix is an involution we can write the integration as

|xihx∗| =

|x∗ihx| = 1

Z(x)

Z(x)

(3.13)

4 The algebra of matrices

Since an associative algebra admits always a matrix representation, it is
interesting to consider the deﬁnition of the integral over the algebra AN of
the N × N matrices. These can be expanded in the following general way

A =

e(nm)anm

N

Xn,m=1

where e(nm) are N 2 matrices deﬁned by

e(nm)
ij = δn

i δm
j ,

i.j = 1, · · · , N

10

(3.9)

(3.10)

(3.11)

(3.12)

(4.1)

(4.2)

These special matrices satisfy the algebra

Therefore the structure constants of the algebra are given by

e(nm)e(pq) = δmpe(nq)

f(nm)(pq)(rs) = δmpδnrδqs

Recalling the deﬁnitions given in eq. (2.6), we have

(R(pq))(nm)(rs) = δpmδqsδnr,

(L(pq))(nm)(rs) = δprδqnδms

(4.5)

The matrix C can be found by requiring that hxC| is an eigenstate of Ri,
that is

[F (e)](nm)(R(pq))(nm)(rs) = e(pq)[F (e)](rs)

By looking at the eq. (4.3), we see that this equation is satisﬁed by

where

We get

It follows

F (e)(nm) = e(rs)C(rs)(nm)

[F (e)](rp)δqs = e(pq)[F (e)](rs)

[F (e)](rs) = e(sr)

C(mn)(rs) = δmsδnr

C T = C ∗ = C,

C 2 = 1

It is seen easily that C satisﬁes

Therefore the matrix algebra is a self-conjugated one. One easily checks that
the right multiplications satisfy eq. (3.11), and therefore C is an involution.
More precisely, since

e(mn)∗

= e(pq)C(pq)(mn) = e(nm)

the involution is nothing but the hermitian conjugation

A∗ = A†, A ∈ AN

11

(4.3)

(4.4)

(4.6)

(4.7)

(4.8)

(4.9)

(4.10)

(4.11)

(4.12)

(4.13)

The integration rules give

We see that this is satisﬁed by

(C −1)(rp)(qs) = δrsδpq =

e(rp)e(qs) = δpq Z(e)

e(rs)

Z(e)

(4.14)

e(rs) = δrs

Z(e)

(4.15)

This result can be obtained also using directly eq. (2.22), noticing that the
identity of the algebra is given by I =

n e(n,n). Therefore

e(rs) =

(C −1)(nn)(rs) =

δnsδnr = δrs

(4.16)

Z(e)

Xn

Xn

P

and, for a generic matrix

N

A =

Z(e)

anm Z(e)

Xm,n=1

e(nm) = T r(A)

(4.17)

5 Derivations

We will discuss now the derivations on associative algebras with identity.
Recall that a derivation is a linear mapping on the algebra satisfying

D(ab) = (Da)b + a(Db),

a, b ∈ A

We deﬁne the action of D on the basis elements in terms of its representative
matrix, d,

If D is a derivation, then

Dxi = dijxj

S = exp(αD)

is an automorphism of the algebra. In fact, it is easily proved that

exp(αD)(ab) = (exp(αD)a)(exp(αD)b)

On the contrary, if S(α) is an automorphism depending on the continuous
parameter α, then from (5.4), the following equation deﬁnes a derivation

(5.1)

(5.2)

(5.3)

(5.4)

(5.5)

D = lim
α→0

S(α) − 1
α

12

In our formalism the automorphisms play a particular role. In fact, from eq.
(5.4) we get

S(α)(|xixi) = (S(α)|xi)(S(α)xi)

(5.6)

and therefore

Ri(S(α)|xi) = S(α)(Ri|xi) = S(α)(|xixi) = (S(α)|xi)(S(α)xi)

(5.7)

meaning that S(α)|xi is an eigenvector of Ri with eigenvalue S(α)xi. This
equation shows that the basis x′
i = S(α)xi satisﬁes an algebra with the same
structure constants as those of the basis xi. Therefore the matrices Ri and Li
constructed in the two basis, and as a consequence the C matrix, are identical.
In other words, our formulation is invariant under automorphisms of the
algebra (of course this is not true for a generic change of basis). The previous
equation can be rewritten in terms of the matrix s(α) of the automorphism
S(α), as

Ri (s(α)|xi) = (s(α)|xi) sijxj = sijs(α)Rj|xi

or

If the algebra has an identity element, I, (say x0 = I), then

s(α)−1Ris(α) = RS(α)x

Dx0 = 0

and therefore

Dx0 = d0ixi = 0 =⇒ d0i = 0

We will prove now some properties of the derivations. First of all, from the
basic deﬁning equation (5.1) we get

Rid|xi = RiD|xi = D(Ri|xi = D(|xixi)

= d|xixi + |xiDxi = dRi|xi + RDxi|xi

(5.12)

from which

[Ri, d ] = RDxi

which is nothing but the inﬁnitesimal version of eq. (5.9). From the integra-
tion rules (for an algebra with identity) we get immediately

(5.8)

(5.9)

(5.10)

(5.11)

(5.13)

(5.14)

Dxi = dij Z(x)

Z(x)

xj = dij(C −1)0j

13

Showing that in order that the derivation D satisﬁes the integration by parts
rule for any function, f (x), on the algebra

the necessary and suﬃcient condition is

D(f (x)) = 0

Z(x)

dij(C −1)0j = 0

implying that the d matrix must be singular and have (C −1)j0 as a null
eigenvector.

Next we show that, if a derivation satisﬁes the integration by part formula
(5.15), then the matrix of related automorphism S(α) = exp(αD) obeys the
equation

C −1sT (α)C = s−1(α)

(5.17)

and it leaves invariant the measure of integration. The converse of this
theorem is also true. Let us start assuming that D satisﬁes eq.
(5.15),
then

0 =

D(|xihx|C) =

d|xihx|C +

|xihDx|C

Z(x)

Z(x)

Z(x)

= d +

|xihx|CC −1dT C = d + C −1dT C

Z(x)

that is

d + C −1dT C = 0

The previous expression can be exponentiated getting

C −1 exp(αdT )C = exp(−αd)

from which the equation (5.17) follows, for s(α) = exp(αd). To show the
invariance of the measure, let us consider the following identity

1 =

Z(x)

s|xihxC|s−1 =

s|xihx|sT C =

|SxihSx|C =

|x′ihx′C|

Z(x)

Z(x)

Z(x)

(5.21)
where x′ = Sx, and we have used eq. (5.17). For any automorphism of the
algebra we have

(5.15)

(5.16)

(5.18)

(5.19)

(5.20)

(5.22)

|x′ihx′C| = 1

Z(x′)

14

since the numerical values of the matrices Ri and Li, and consequently the
C matrix, are left invariant. Comparing eqs. (5.21) and (5.22) we get

=

Z(x′)

Z(x)

(5.23)

On the contrary, if the measure is invariant under an automorphism of the
algebra, the chain of equalities

1 =

Z(x′)

|x′ihx′C| =

|x′ihx′C| =

s|xihx|C(C −1sT C) = s(C −1sT C)

Z(x)

Z(x)

(5.24)
implies eq. (5.17), together with its inﬁnitesimal version eq. (5.19). From
this (see the derivation in (5.18)), we get

and by taking xi = I,

0 =

D(xixjCjk)

Z(x)

Dxj = 0

Z(x)

(5.25)

(5.26)

for any basis element of the algebra. Therefore we have proven the following
theorem:

If a derivation D satisﬁes the integration by part rule, eq. (5.15), the integra-
tion is invariant under the related automorphism exp (αD). On the contrary,
if the integration is invariant under a continuous automorphism, exp (αD),
the related derivation, D, satisﬁes (5.15).

This theorem generalizes the classical result about the Riemann integral re-
lating the invariance under translations of the measure and the integration
by parts formula.

Next we will show that, always in the case of an associative self-conjugated
algebra, A, with identity, there exists a set of automorphisms such that the
measure of integration is invariant. These are the so called inner deriva-
tions, that is derivations such that

D ∈ L(A)

(5.27)

15

where L(A) is the Lie multiplication algebra associated to A. To deﬁne
L(A) one starts with the linear space of left and right multiplications and
deﬁnes

that is the space generated by the vectors

Then

M1 = MR + MLT

Ra + LT
b ,

a, b ∈ A

L(A) =

Mi

∞

Xi=1

Mi+1 = [M1, Mi]

where the spaces Mi are deﬁned by induction

Therefore L(A) is deﬁned in terms of all the multiple commutators of the
elements given in (5.29).

It is not diﬃcult to prove that for a Lie algebra, L(A) coincides with
the adjoint representation [9]. We will prove now an analogous result for
associative algebras with identity. That is that L(A) coincides with the
adjoint representation of the Lie algebra associated to A (the Lie algebra
generated by [a, b] = ab − ba, for a, b ∈ A). The proof can be found, for
example, in ref.
[9], but for completeness we will repeat it here. From the
associativity conditions (2.8), and (2.13) one gets

[Ra + LT

b , Rc + LT

d ] ∈ M1,

a, b, c, d ∈ A

or

showing that

[M1, M1] ⊂ M1

L(A) = M1 = MR + MLT

Therefore the matrix associated to an inner derivation of an associative al-
gebra must be of the form

d = Ra + LT
b
We have now to require that this indeed a derivation, that is that eq. (5.13)
holds. We start evaluating

(5.35)

[Rc, d] = [Rc, Ra + LT

b ] = R[c,a]

(5.36)

16

(5.28)

(5.29)

(5.30)

(5.31)

(5.32)

(5.33)

(5.34)

where we have used the fact that the right multiplications form a represen-
tation of the algebra and that right and left multiplications commute. Then
comparing with

RDc = Rca+cb

(5.37)

we see that the two agree for b = −a. Then we get

Dxi = xia − axi = −[a, xi] = −(adj a)ijxj

(5.38)

This shows indeed that the inner derivations span the adjoint representation
of the Lie algebra associated to A.

We can now proof the following theorem:

For an associative self-conjugated algebra with identity, the measure of inte-
gration is invariant under the automorphisms generated by the inner deriva-
tions, or, equivalently, the inner derivations satisfy the rule of integration by
parts.

In fact, this follows because the inner derivations satisfy eq. (5.19)

C −1dT C = C −1(RT

a − La)C = (C T RaC T −1

)T − Ra = LT

a − Ra = −d (5.39)

As an example let us consider the algebra of the matrices studied in the
previous Section. In this case the inner derivations are simply given by

Therefore

DBA = [A, B]

DBA =

[A, B] = 0

Z(e)

Z(e)

(5.40)

(5.41)

and we see that the integration by parts formula corresponds to the cyclic
property of the trace.

6 Paragrassmann algebras as subalgebras of

an algebra of matrices

Since an associative algebra can be represented in terms of matrices, and
having shown that, in this case, the integration is simply given by the trace,

17

one can ask if it is possible to use this result in order to get the integration
over a subalgebra of AN . The idea is simply that one should integrate with
the trace formula, but using a weight which selects the particular subalgebra
one is interested to. We will illustrate this procedure for a paragrassmann
algebra of order p, that is an algebra generated by an element θ, such that

θp+1 = 0

(6.1)

For p = 1 we get a Grassmann algebra. Any element of the algebra is given
by a power of θ

xk = θk,

(6.2)
Being the algebra an associative one, the elements θk can be represented in
terms of the right multiplication matrices, Rk. These are (p + 1) × (p + 1)
matrices given by (see [4])

k = 0, 1, · · · , p

we can write, in terms of the matrices deﬁned in eq. (4.2)

Deﬁning

and

(Ri)jk = δi+j,k

Xθ ≡ R1

Xθ =

e(i,i+1)

p

Xi=1

p+1−k

X k

θ =

e(i,i+k)

(6.3)

(6.4)

(6.5)

(6.6)

Xi=1
Therefore, the most general function on the paragrassmann algebra (as a
subalgebra of the matrices (p + 1) × (p + 1)) is given by

f (Xθ) =

aiX p+1−i
θ

=

ai

e(j,p+1+j−i)

(6.7)

p+1

Xi=1

p+1

i

Xi=1

Xj=1

As we said, the idea is to look for a matrix P such that it projects out of the
algebra, Ap+1 of the (p+1)×(p+1) matrices, the paragrassmann subalgebra.
To deﬁne such an operator, let us consider a generic matrix B ∈ Ap+1. We
can always decompose it as (see later)

B = f (Xθ) + ˜B

(6.8)

18

The operator P should satisfy

BP = f (Xθ)P

˜BP = 0

Then, we can deﬁne the integration over the paragrassmann algebra in terms
of the integration over the isomorphic subalgebra of Ap+1 through the equa-
tion

f (θ) =

f (Xθ)P = T r[f (Xθ)P ]

Z(θ)

Z(e)

In order to deﬁne the decomposition (6.8) and the operator P , let us consider
the most general (p + 1) × (p + 1) matrix. We can write

B =

bije(ij) =

bije(ij) +

bi,p+1e(i,p+1)

(6.12)

p+1

Xi,j=1

p+1

p

Xi=1

Xj=1

p+1

Xi=1

By adding and subtracting

bi,p+1

e(j,p+1+j−i)

(6.13)

we get the decomposition (6.8) with

f (Xθ) =

bi,p+1X p+1−i
θ

(6.14)

p+1

Xi=2

i−1

Xj=1

p+1

Xi=1

˜B =

p+1

p

Xi=1

Xj=1

p+1

Xi=2

i−1

Xj=1

bije(ij) −

bi,p+1

e(j,p+1+j−i)

(6.15)

Let us notice that for any integer k, 1 ≤ k ≤ p + 1, we have

or

and

or

But using the identity

˜Be(p+1,k) = 0

Be(p+1,k) = f (Xθ)e(p+1,k)

e(p+1,k) = e(p+1,1)X k−1

θ

19

(6.9)

(6.10)

(6.11)

(6.16)

(6.17)

(6.18)

we get

T r[Be(p+1,k)] = T r[X k−1

f (Xθ)e(p+1,1)] ≡ T r[g(Xθ)e(p+1,1)]

θ

(6.19)

where g(xθ) = X k−1f (Xθ). This shows that we can always deﬁne the in-
tegration trough the operator P = e(p+1,1). Then, the integration over the
subalgebra is given by

f (θ) =

Zθ

Z(e)

f (Xθ)e(p+1,1) = T r[f (Xθ)e(p+1,1)]

(6.20)

It follows from eq. (6.14), and eq. (4.15)

f (θ) = T r[

bi,p+1e(i,1)] = b1,p+1

(6.21)

Z(θ)

p+1

Xi=1

Meaning that

or

f (θ) =

[b1,p+1θp + b2,p+1θp−1 + · · · + bp+1,p+1] = b1,p+1

(6.22)

Z(θ)

Z(θ)

(6.23)

(6.24)

θk = δkp

Z(θ)

θk−1f (θ)

Z(θ)

which coincides with the direct way of deﬁning the integral over a paragrass-
mann algebra (see [4]). Of course, by choosing P = e(p+1,k) with k 6= 1 would
lead to an integral, which can be expressed in terms of the one in eq. (6.23),
as

In the particular case of a Grassmann algebra we have

Xθ = e(1,2) =

= σ+, P = e(2,1) =

0 1
0 0 (cid:19)

(cid:18)

0 0
1 0 (cid:19)

(cid:18)

= σ−

(6.25)

The decomposition in eq. (6.8), for a 2 × 2 matrix

B = a + bσ3 + cσ+ + dσ−

(6.26)

is given by

˜B = b(1 + σ3) + dσ−,

f (Xθ) = f (σ+) = a − b + cσ+

(6.27)

20

and the integration is

from which

f (θ) = T r[f (σ+)σ−]

Z(θ)

(6.28)

1 = T r[σ−] = 0,

θ = T r[σ+σ−] = 1

(6.29)

Z(θ)

Z(θ)

We notice that the matrices ˜B and f (Xθ) appearing in the decomposition
(6.8) can be written more explicitly as

and

˜B =

˜b1,1
·
·
˜bp,1
˜bp+1,1










˜b1,2
·
·
˜bp,2
˜bp+1,2

˜b1,p
· · ·
·
·
·
·
˜bp,p
· · ·
· · · ˜bp+1,p

0
·
·
0
0










f (Xθ) =

ap+1
0
0
·
·
0
0
0


















ap
ap+1
0
·
·
0
0
0

ap−1
ap
ap+1
·
·
0
0
0

a2
· · ·
a3
· · ·
a4
· · ·
·
·
·
·
ap
· · ·
· · · ap+1
· · ·

0

a1
a2
a3
·
·
ap−1
ap
ap+1


















(6.30)

(6.31)

The p × (p + 1) parameters appearing in ˜B and the p + 1 parameters in f (Xθ)
can be easily expressed in terms of the (p + 1) × (p + 1) parameters deﬁning
the matrix B.

7 Conclusions

In this paper we have studied some general properties of the integration over
self-conjugated associative algebra with identity. We have proven a theorem
showing that continuous automorphisms, leaving invariant the measure of
integration, give rise to derivations satisfying the integration by part rule

21

(that is the vanishing of the integral of a derivative). The relevance of this
result is that in quantum mechanics the Schwinger’s principle follows trivially
from the previous theorem, therefore it opens the avenue to extensions to
more general theories.

The other important problem we have considered is the following: given
the integral over an algebra, is there a natural way to induce the integral over
a subalgebra? To face this problem we have followed the way suggested by
the standard integration over manifolds, that is to project out (via the char-
acteristic function) the submanifold in the measure. We have illustrated this
procedure for a paragrassmann algebra of order p, thought as a subalgebra
of an algebra of (p + 1) × (p + 1) matrices, showing that it is possible deﬁne
a projector selecting the paragrassmann component out of a given matrix.
Since in the text we have shown that the integral for an algebra of matrices
coincides with the trace, we arrive to evaluate the integral over a paragrass-
mann algebra via a trace of ordinary matrices. In particular the integration
over the one-dimensional Grassmann algebra can be expressed as a trace of
2 × 2 matrices. This result might be of some interest for fermionic theories
on the lattice.

Acknowledgements

The author would like Prof. J. P. Eckmann, Director of the Department of
Theoretical Physics of the University of Geneva, for the very kind hospitality.

22

References

[1] Y.A. Golfand and E.S. Likhtman, JETP Lett. 13 (1071)313; D.V.
Volkov and V.P. Akulov, Pis’ma Zh. Eksp. Teor. Fiz. 16 (1972) 621;
ibidem Phys. Lett. B46 (1973) 109; J. Wess and B. Zumino, Nucl. Phys.
B70 (1974) 139.

[2] R. Casalbuoni, Il Nuovo Cimento, 33A (1976) 389.

[3] T. Banks, W. Fischler, S.H. Shenker and L. Susskind, Phys. Rev. D55

(1997) 5112, hep-th/9610043.

[4] R.Casalbuoni, Int. J. Mod. Phys. A12 (1997) 5803, physics/9702019.

[5] A. Connes, Noncommutative geometry, Academic Press (1994).

[6] V.G. Drinfeld, Quantum Groups, in Proceedings of the International
Congress of Mathematicians, Berkeley 1986, pp. 798-820, AMS, Provi-
dence, RI.

[7] R. Casalbuoni, hep-th/9801170.

[8] R. Casalbuoni, in preparation.

[9] R.D. Schafer, An introduction to nonassociative algebras, Academic

Press (1966).

23


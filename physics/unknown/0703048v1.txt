Illusion of Control in a Brownian Game 

J.B. Satinover1 and D. Sornette2 
1Laboratoire de Physique de la Matière Condensée 
CNRS UMR6622  and Université des Sciences, Parc Valrose 
06108 Nice Cedex 2, France 
2Department of Management, Technology and Economics 
ETH Zurich, CH-8032 Zurich, Switzerland 
jsatinov@princeton.edu and dsornette@ethz.ch 
 
 
 
 

Abstract:  Both  single-player  Parrondo  games  (SPPG)  and  multi-player  Parrondo 
games (MPPG) display the Parrondo Effect (PE) wherein two or more individually fair 
(or  losing)  games  yield  a  net  winning  outcome  if  alternated  periodically  or  randomly. 
(There  is  a  more  formal,  less  restrictive  definition  of  the  PE.)  We  illustrate  that,  when 
subject  to  an  elementary  optimization  rule,  the  PG  displays  degraded  rather  than  en-
hanced  returns.  Optimization  provides  only  the  illusion  of  control,  when  low-entropy 
strategies  (i.e.  which  use  more  information)  under-perform  random  strategies  (with 
maximal entropy). This illusion is unfortunately widespread in many human attempts to 
manage or predict complex systems. For the PG, the illusion is especially striking in that 
the  optimization  rule  reverses  an  already  paradoxical-seeming  positive  gain—the  Par-
rondo effect proper—and turns it negative. While this phenomenon has been previously 
demonstrated  using  somewhat  artificial  conditions  in  the  MPPG  (L.  Dinis  and  J.M.R. 
Parrondo,  Europhysics  Letters  63,  319  (2003);    J.  M.  R.  Parrondo,  L.  Dinis,  J.  Buceta, 
and  K.  Lindenberg,  Advances  in  Condensed  Matter  and  Statistical  Mechanics,  eds.  E. 
Korutcheva  and  R.  Cuerno,  Nova  Science  Publishers,  2003),  we  demonstrate  it  in  the 
natural setting of a history-dependent SPPG. 

PACS numbers: 02.50.Le, 05.40.Je 

A. Formalism of the Parrondo Effect (PE) 
The  Parrondo  effect  is  the  counterintuitive  result  where  mixing  two  or  more  losing 
games can surprisingly produce a winning outcome. The basic Parrondo effect (PE) was 
first identified as the game-theoretic equivalent to directional drift of Brownian particles 
in a time-varying “ratchet”-shaped potential [1,2]. Consider N > 1 s-state Markov games 
, denote the 

transition matrices, 

,and their N 

. For every 

, 

vector of s conditional winning probabilities as 

 and their steady-

 

 

 

 

1 

iG{}1,2,,iN!Kss!()ˆiM()ˆiM()()()(){}12,,iiiisppp=prKstate  probability  vectors  as 

probability of winning is : 

.  For  each  game,  the  steady-state 

 

(1) 

Consider  also  a  lengthy  sequence  of  randomly  alternating  with  individual  time-

averaged proportion of play

. The transition matrix for the combined 

sequence of games is the convex linear combination  

with condi-

tional  winning  probability  vector 

and  steady-state  probability  vector 

. The steady-state probability of winning for the combined game is therefore 

A PE occurs whenever (and in general it is the case that) : 

 

 

 

(2) 

(3) 

(4) 

hence the PE, or “paradox”, when the left hand side of (4) is less than zero and the right-
hand side greater. 

The original Parrondo game (PG) discussed in [1,2] employs three differently-biased 
coins. The coin to be tossed at a given time-step is determined by the net number of wins 
or losses. This “capital-dependent” PG can be expressed in terms of a 
 Markov tran-
sition matrix. The extension to PE games that are history-dependent was initially made in 
[3] with one history dependent game and the other not; Ref.[4] extends the concept to the 
linear convex combination of two history dependent games. A history-dependent game is 
one  in  which  the  prior  sequence  of  wins  and  losses  (of  defined  length  m)  determines 
which coin to toss, rather than the value of the accumulated capital. Similar to the use of 
the  finite  time-horizon  τ  in  the  time-horizon  Minority  Game  (THMG)  in  [3,  4],  a  two-
state  memory-dependent  process  with  memory  of  two  bits  is  recast  as  a  22  =  4  -state 
memory-independent Markov process. 

The most basic conditions under which a non-trivial, history-dependent PE might be 

sought are: 

  m = 2 
  N = 2 

 

 

2 

 

i.e., 

 

 

 

 

 

 

()()()(){}12,,,iiiis!!!"=rK()()()iiiwinP=!"prriG[]10,1, 1 Niii!!="=#12(,,,)()1ˆˆNNiii!!!!="#MMK()12,,,n!!!pKr()12,,,n!!!"Kr()()()121212,,,,,,,,,NNNwinP!!!!!!!!!="#pKKKrr()()12,,,1NNiiwinwiniPP!!!!="#K()()()()1212,,,,,,1NNNiiii!!!!!!!="#$"#%ppKKrrrr33!1122!!== 

 

 

and 
The value m =2 means that the system has 22=4 states: 
= lose, 1 = win. The 4 steady state probabilities are simple algebraic functions of the con-
ditional probabilities, up to a normalization factor: 

, with 0 

 

 

(5) 

Here we have two fair games alternating at random in equal proportion to yield a winning 
,  a  finite  amount.  We  suppose  that  it  is 
game.  Note  that

possible to reduce all conditional probabilities in both games by some sufficiently small 
bias

, such that: 

 

 

 

 

We thus obtain two losing games alternating at random in equal proportion to yield a 
 (games) and bias 

(still) winning game. We define two fair transition matrices 
matrix 

 with 

: 

The example provided in [5], and further analyzed in [6], will be extended here as well: 

Letting 

 = 0.005 and substituting (8) in (6), (5) and (2), we obtain (as in [4]): 

 

(6) 

(7) 

(8) 

 

(9) 

 

 

 

3 

 

 

 

 

 

 

 

Then 

()()()121,212winwinwinPPP==<()()()111winP=!"prr()()()222winP=!"prr{}{}00,01,10,111,2,3,4!()()()()()()()()()()()()()()()()()()()()()()()()()()1122343411224141121122414111221212111111, 11pppppppppppppppp!"!"####$%$%$%$%##$%$%&’&’$%$%##$%$%$%$%$%$%()()vv()()()()()()1,211,22winwinwinwinPPPP!=!"#!()()()()121,21,212,winwinwinwinPPPP!!!!<<<()()()()1,2111, 02winwinwinPPP!!!!""#+$%#<<%()()12ˆˆ,MMˆå12!=()()()()()()()()()1311323231010000000ˆˆˆ; 0001010000iiiiiiiiipppppppp!!!!!!!!"#$$$$"#%&%&%&++%&=+=%&%&$$$$%&%&%&++’(’(Måå()()()()()(),1121212ˆˆˆˆˆ(1)!!!!"=+"=+MMMMM(){}{}(){}{}12971111111044102222,,,;  ,,,pp==!()()()()()1122,1212120.4945;  0.4950; 0.49470.5010winwinwinwinwinPPPPP!"==+=<=#$which illustrates quantitatively the Parrondo effect Pwin
 

(1) >0.5. 

B.  Time Horizon PG (THPG): Reversal of the PE under Optimization 

Ref.[5.6] present a capital-dependent multi-player PG (MPPG): At (every) time-step t, 
a constant-size subset of all participants is randomly re-selected actually to play. All par-
ticipants keep individual track of their own capital but do not alternate games independ-
ently based on it. Instead, this data is used to select which game (all) selected participants 
must use at t. Given the individual values of the capital at 
, the known matrices of the 
two games and their linear convex combination, the chosen game is the one which has the 
most positive expected aggregate gain in capital, summed over all participants. 

This rule may be thought of as a static optimization procedure—static in the sense that 
the “optimal” choice appears to be known in advance. It appears exactly quantifiable be-
cause of access to each player’s individual history. Indeed, if the game is chosen at ran-
dom,  the  change  in  wealth  averaged  over  all  participants  is  significantly  positive.  But 
when the “optimization” rule is employed, the gain becomes a loss  significantly greater 
than that of either game alone. The intended “optimization” scheme actually reverses the 
positive (collective) PE. The reversal arises in this way: The “optimization” rule causes 
the system to spend much more time playing one of the games, and individually, any one 
game is losing.  This collective phenomenon is of interest as an example the phenomenon 
of  “illusion  of  control.”  Here,  we  defined  “illusion  of  control”  as  situations  when  low-
entropy  strategies  (i.e.  which  use  more  information)  under-perform  random  strategies 
(with maximal entropy). 

However,  the  study  of  Ref.[5,6]  has  certain  “artificial”  features  in  both  design  and 
outcome. For example, all active players are constrained to “choose” the same rule. Such 
a constraint removes the example from the domain of complex (and most real-world) sys-
tems. The same applies when, as shown in [7], the enforced game is that which appears to 
maximize the wealth of (voted for by) the largest number of players; and even when the 
games being played are history-dependent. Second, the reversal of the PE occurs simply 
because  the  (enforced)  “choices”  turn  out  to  be  driven  largely  toward  a  single  game. 
(Some authors have nonetheless attempted to draw social policy lessons from such artifi-
cial collective situations [8].) Of greater interest is the phenomenon presented in [9]: Col-
lective games are shown to undergo a “current reversal” for certain mixing probabilities 

.  
The term “current reversal” highlights the value of examining optimization rules in the 
setting  of  a  PE,  especially  a  positive  one:  An  optimization  rule  that  leads  to  a  loss 
“against the current” of a positive PE is an especially good illustration of the illusion of 
control.  

We present a pointed illustration of the illusion: under the most natural kind of optimi-
zation rule, a “current reversal” (reversal of a positive PE) appears in single-player PG’s. 
This provides the most natural illustration of the illusion of control in PGs, and a suitable 
counterpoint to the analogous phenomenon in the natively collective MG as discussed in 
[10].  

Furthermore, the reversal of the single-player PE under “optimization” is not  caused 
by a significant imbalance of the system with respect to one game or another. For the PE 

 

4 

1t!i!in general, algorithms that actually  do maximize the positive PE can be easily generated 
as the consequence of the Markov analysis presented above [5, 11]. These algorithms de-
rive from the transition matrices which are known. They do not include the self-evident-
seeming optimization rule that we employ here, on analogy to that employed in the Mi-
nority Games: At time t, play whichever game has accumulated the most points (wealth) 
over a sliding window of τ prior time-steps from 

to 

.  

We compare a numerical simulation using the same example games as in (6) through 
 have 
located before the 
 in memory. At the start, a random history of three bits initializes play and 

(9) (refs [3,4]) to their corresponding analytical formulation. Games 
memory m = 2 prior time steps. We include a time-horizon of length 
history 

and 

 steps require a random choice of game. In the simplest instance we let τ = 1. 
the first 
The  binary  sequence  of  subsequent  choices  is  thus  dependent  on  a  sliding  window  of 
 is not 
prior binary wins/losses of length 

—the first such window of which 

a subset. (Otherwise the exact winning game is defined in advance and the “optimization” 
succeeds  trivially.)  Whichever  game  would  have  won  on  the  previous  step  had  it  been 
played (regardless of whether it actually was) is chosen to be played next. The player’s 
wealth is based on the sequence of games actually played. If both games actually would 
have yielded the same outcome, win or lose, one of the two is chosen at random instead.  

By construction, the individual games 

played individually are both los-
ing; random alternation between them is winning (the PE effect (4)) The one-player two-
game history-dependent PE in our example is  as  follows: 
have respective 
. Alternated at random in equal pro-
winning probabilities 

 and 

and 

and 

portion 

, 

.We now express the choose-best optimization 

rule in Markov form. Under this rule, the two 
bine as a linear convex 
  matrix 

 do not com-
matrix sum. Instead, the combined game is represented by an 
.    The  2s  conditional  winning  probabilities  are  now 

matrices 

and 

with

  and 

indices 

.  (Under  the  choose-worst  rule 

).  

If  the  previously  winning  game  is  selected, 

,  while  if  the  previously 

losing  one  is, 

.  Unexpectedly,  choosing  the  previously  best-performing 

game  yields  losses  only  slightly  less  than  either 
  individually:  The  PE  is 
almost entirely eliminated. Choosing the previously worst-performing games yields gains 
that exceed the PE proper. 

and 

The steady state probabilities for a simulation over 50 runs  and 200 steps for each of 
the eight different possible initial states are shown in Table 1. The R2 between the fre-
quency of states obtained numerically and analytically is 0.988 over 40,000 runs. 

 

5 

1t!t!"()1ˆM()2ˆM!()tµ!3m!+=()tµ()1ˆM()2ˆM()1ˆM()2ˆM10.494winP=20.495winP=()120.5!!==120.5,0.50.501winP!!===ss!()1ˆM()2ˆMss!()()ss!!+"+()1,2ˆQ()()()()()()()()()()()(){}1122121211jjjjjjjqpppppp!""!""#$#$=+%+%+&’&’1,2,,2js=K()[][][]()121,41,  1,21jModjjjModj!"=#+=##+()()()()()()()()()()()(){}1122121211jjjjjjjqpppppp!""!""#$#$=%+++%&’&’(1,2)0.496bestwinP=(1,2)0.507bestwinP=()1ˆM()2ˆM 

 

 

 

 

 

 

Table 1: Analytically predicted and numerically simulated frequencies 
of  the  eight  3-state  binary  histories  for  a  single  player  of  two  history-
dependent Parrondo games under the “choose previously best” optimi-
zation rule. R2=0.988 over 40,000 runs. 

π 
000 
001 
010 
011 
100 
101 
110 
111 

Analytic 
0.075 
0.164 
0.172 
0.093 
0.164 
0.101 
0.093 
0.138 

Numeric 
0.072 
0.165 
0.165 
0.097 
0.165 
0.097 
0.097 
0.142 

The mechanism for this illusion-of-control effect characterized by the reversing of the 
PE under optimization is not the same as under a similar optimization rule for the Minor-
ity Game [10], as there is no collective effect and thus no-crowding out of strategies or 
games. (Nor is it the same as for the Multi-Player PG) As seen from (4), the PE proper 
 into 
results from a distortion of the steady-state equilibrium distributions 

 and 

a vector 

 (for the n=2 version) which is more co-linear to the conditional win-

ning probability vector 

 than are either individual game (this is just a geometric 

restatement of the fact that the combined game is winning). One may say that the random 
alternation of the two games tends on average to align these two vectors under the action 
of the other game. Choosing the previously best performing game amounts to removing 
this combined effect, while choosing the previously worst performing game tends to in-
tensify this effect.  

Consider the following simple illustration from [12], with 

: 

From (6), (5) and (2): 

There is no PE since  

 

(10) 

(11) 

(12) 

 

 

6 

The long term gain (loss) associated with 
positive or negative reward per time-step. We may however associate arbitrary, differ-

 is proportional to 

. This implies unit 

  ! (1)r "   ! (2)r "   ! (1",2")r #   ! (1",2")r p 12!=()()()()()()511211121,2132633625112212633621ˆˆˆˆˆ;  ;  2!"!"!"===+=#$#$#$%&%&%&MMMMM()()()121,2311442; ; winwinwinPPP===()()()()1,21212winwinwinPPP=+()ˆiM()iwinPently-valued gains and losses. Suppose that for the transitions (elements) of both 

and

, we associate instead the following reward matrix 

[12]: 

 

(13) 

Then the time-averaged unit change in wealth associated with  
computed as: 

, 

 and 

 are 

 

(14) 

Thus, two fair games combine to make a winning game, a PE. The Hadamard product 

matrices 

  are not Markovian in that their elements are not probabilities, but 

products of a probability and a reward value. One may tinker with either the probabilities 
or the rewards to increase, decrease, eliminate or reverse the direction of the PE. The 
same result is obtained for any set of identical product values regardless of whether it is 
the probability or the reward that is thought of as altered. 

Further light is shed on this phenomenon by considering a fully deterministic variant, 

i.e., where the elements of all 

 Consider indeed the following set of 

, 

which are simply (one possibility for ) equations (10) rounded : 

 

(15) 

and 

are now fully deterministic games. 

has the form of a single probabil-

istic game, but is indistinguishable from an alternation between games 1 and 2:  

 

(16) 

The alternation may be periodic or equiprobably random. Furthermore, we may generate 
a strictly periodic sequence by imposing our “counteradaptive” optimization rule: Play at 
time 
ward matrix 
and 

, the time-averaged unit changes in wealth associated with  

 the game that would have lost at time 

are multiplied by re-

 are: 

and 

. If 

, 

 

 

(17) 

Thus, we have reproduced what looks like a (winning) PE by imposing a “paradoxi-
cal” optimization rule on the alternation of two wholly deterministic systems. As it hap-
pens for this example, the alternation may be itself a simple deterministic cycle. But the 
same results arise if the deterministic games are alternated at random with 

. 

 

 

 

 

 

 

 
 
 

7 

()1ˆM()2ˆMˆR13ˆ31!"#=$%!&’R()1ˆM()2ˆM()1,2ˆM()()()(){}()()(){}{}121,213ˆˆ1,1;  ,,0,0,iiiGGGG!="##!!!=RMvoT()()ˆˆiRMoT(){}ˆ0,1.i!M()ˆiM()()121100ˆˆ;  0011DetDet!"!"==#$#$%&%&MM()1ˆM()2ˆM()1,2ˆM()111,2221122ˆ!"=#$%&Mt1t!()1ˆM()2ˆMˆR()1ˆM()2ˆM()1,2ˆM()()()(){}()()(){}{}121,2ˆˆ1,1;  ,,1,1,1iiiGGGG!="##!!!=$$+RMvoT12!= 

C. Concluding remarks 

In many social and economic activities, human agents attempt to maximize value. We 
often do so by adjusting our present strategy in accord with what has previously worked 
best. Yet this very adjustment often proves to have exactly the opposite effect—causing 
greater losses than if we had left well enough alone. A classic everyday example which 
has been analyzed in these terms is weaving in and out of traffic—we rarely gain, and 
often lose by doing so. We would do better sticking to whatever lane we find ourselves in 
[13]. The negative power of this effect is demonstrated by the perverse phenomenon 
which we have here highlighted as well: that in certain games, deliberately selecting what 
appears to be the worst approach can “paradoxically” enhance gains. While this effect 
follows directly in the MG (and in lane switching) from the “minority wins” rule, here a 
similar effect arises without requiring any such competitive mechanism. 

G. P. Harmer and D. Abbott, Nature 402, 864 (1999). 
G. P. Harmer and D. Abbott, Statistical Science 14, 206 (1999). 
J. M. R. Parrondo, G. P. Harmer, and D. Abbott, Physical Review Letters 85, 
5226 (2000). 
R. J. Kay and N. F. Johnson, Physical Review E 67, 56128 (2003). 
L. Dinis and J.M.R. Parrondo, Europhysics Letters 63, 319 (2003). 
J. M. R. Parrondo, L. Dinis, J. Buceta, and K. Lindenberg, Advances in Con-
densed Matter and Statistical Mechanics, eds. E. Korutcheva and R. Cuerno 
(Nova Science Publishers, 2003). 
L. Dinís and J. M. R. Parrondo, Physica A 343, 701 (2004). 
R. Toral, Fluctuation and Noise Letters 2, L305 (2002). 
P. Amengual, P. Meurs, B. Cleuren, et al., Arxiv preprint math.PR/0601404  
(2006). 
J. Satinover and D. Sornette, Illusion of control in Minority and Parrondo games, 
preprint(2007). 
D. Zeilberger, in Personal Journal of Shalosh B. Ekhad and Doron Zeilberger, 
2000). 
A. Allison and D. Abbott, in Annals of the International Society of Dynamic 
Games, edited by A. S. Nowack and K. Szajowski (Birkhäuser, Adelaide, 2003 ). 
T. Chmura and T. Pitz, Physica A 363, 477 (2006). 

 

 
 
 
References: 
 
1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

 
 

 

 

8 


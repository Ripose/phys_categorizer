Quasi-optimal observables: Attaining the quality
of maximal likelihood in parameter estimation
when only a MC event generator is available

Fyodor V. Tkachov

Institute for Nuclear Research of Russian Academy of Sciences,
7a, 60th October Ave., Moscow, 117312, Russian Federation

1
0
0
2
 
g
u
A
 
6
1
 
 
 
0
3
0
8
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A new method of quasi-optimal observables allows one to approach the quality of
data  processing  usually  associated  with  the  method  of  maximal  likelihood  within
the simpler algorithmic context of generalized moments.

In this lecture, I’d like to explain a recent finding [1] which connects the two basic
methods of parameter estimation, the method of maximal likelihood and the method of
generalized moments (see e.g. [2]). The two methods (along with the c 2 method, which I
won’t discuss) are very well known and widely used in experimental physics.

In a sense, the connection views the method of maximal likelihood as corresponding to a
special point in the space of generalized moments, and considers small deviations from that
point. The point corresponds to the minimum of the fundamental Cramer-Rao inequality, and
small deviations from it introduce non-optimalities (compared with the maximal likelihood
method) that are only quadratic in the deviations. This approach offers what appears to be a
new and useful algorithmic scheme which combines the theoretical advantage of the method
of maximal likelihood (i.e. the fact that it yields the absolute minimum for the variance of the
parameter being estimated with a given data sample) with the algorithmic simplicity of the
method of moments.

I call the resulting method the method of quasi-optimal observables. It is useful in
situations where the method of maximal likelihood fails or cannot be applied, e.g. in high
energy physics where typically only a Monte Carlo event generator is available but no explicit
formula for the probability density.

 One deals with a random variable P whose instances (specific values) are called events.
Their probability density is denoted as p (P). It is assumed to depend on a parameter M which
has to be estimated from an experimental sample of events {Pi}i.

 The method of generalized moments consists in choosing a function f (P) defined on
events (the generalized moment or, using the language of quantum theory, observable), and
then finding M by fitting its theoretical average value,

 against the corresponding experimental value:

 

f

= (cid:242) P P
p
d
(

)

f

P , 
( )

 

f

exp

=

1
N

f

P .
(
)i

i

1

The result of the fit is an estimate for M denoted as M[f ] . Once the  observable  f   is  chosen,

(1)

(2)

(cid:229)
the  method  is  rather  easy  to  use.  However,  the  method  says  nothing  about  how  to  find  a
good  f , i.e. one which would minimize the variance D(M[f ] ) of the resulting estimate M[f ] .
The method of maximal likelihood, on the other hand, prescribes to choose M which

maximizes the likelihood function

= (cid:213)

L
 

(
P
i

) .

p

i

The necessary condition for the minimum then is

 

L
M

=

L

p
ln (

P

)

j

=

0 .

j

M

The  method,  if  applicable,  yields  an  estimate  M opt  for  M  whose  variance  D(M opt)  is  optimal
because  it  is  asymptotically  equal  to  the  minimal  value  established  by  the  fundamental
Cramer-Rao inequality (cf. Eq. (8.10) in [2]; N is the number of events Pi):

(
D M
 

)

opt

N

1

N

2

1

.

p

ln
M

Although theoretically ideal, the method of maximal  likelihood  may  be  difficult  to  make  use
of,  e.g.  if  the  number  of  events  is  large  and/or  there  is  no  sufficiently  simple  regular
expression  for  the  probability  density  p .  The  worst  case  is,  of  course,  when  the  explicit
expression  for  p   is  unavailable;  this  case  occurs  when  all  one  has  is  a  Monte  Carlo  event
generator.

So, on the one hand, there is a simple but non-optimal method of generalized moments.
On the other hand, there is a theoretically ideal but cumbersome and often unusable method
of maximal likelihood. And there is no apparent connection between the two.

Following [1], let us ask a natural question: is it possible to find an observable f  which
would minimize D(M[f ] )? If such observable f opt exists, the corresponding D(M[f  opt] ) must
be directly connected to the r.h.s. of (5).

The trick used in [1] is as follows. Asymptotically,

(
ND M f
[
 

)

]

Var

[
M f

]

= (cid:231)

N

2

Var

f

,

f
M

where 

Var f

=

(

f

)2

f

. Then it is sufficient to consider Var M[f ]  as a numeric  function

in  the  functional  space  of  f   and  to  use  the  apparatus  of  functional  derivatives  to  study  the
problem  similarly  to  how  one  studies  minima  in  ordinary  spaces.  (A  note  concerning
mathematical rigor: the method is valid under the same conditions as the method of maximal
likelihood, and the usual Hilbert norm of mathematical statistics is to be chosen in the space
of f .) The necessary condition for the minimum is

d

 
d

f

P
( )

Var

[
M f

]

=

0 .

After simple calculations (see [1] for details) one finds the following solution:

(3)

(4)

(5)

(6)

(7)

2

¶
¶
¶
¶
(cid:229)
-
-
ﬁ
¥
¶
(cid:230)
(cid:246)
»
(cid:231)
(cid:247)
¶
Ł
ł
-
ﬁ
¥
¶
(cid:230)
(cid:246)
»
(cid:247)
Ł
ł
¶
-
(In  fact,  there  is  a  family  of  solutions, 

f

P
( )

=

C f

1 opt

P
( )

+

C
2

.

)  Another  simple  calculation

f
  opt

P
( )

=

p
P
ln ( )
M

.

yields

Var
 

M f

opt

ø =

f

2
opt

1

.

In  view  of  (8)  and  (5)  we  see  that  extracting  M  using  the  observable  (8)  is  asymptotically
equivalent to the method of maximal likelihood.

Once we adopted the viewpoint of analogy with ordinary functions, a natural next step is

to consider small deviations from f opt and their effect on Var M[f ] .  To this end, expand
Var M[f ]  in f  around f opt; what we are doing here is a functional analog of the Taylor
theorem:

 
Var

M f
[

opt

+

j

=

] Var

M f
[

+

]

opt

1
2

d
d

2

Var
d
P
( )

M f
[
Q
(
f

]
)

f

j

P Q P Q
(

) d d

)

(

j

+

!

=

f

f

op

 The  term  which  is  linear  in  j   does  not  occur  because  f opt  satisfies  (7).  Explicit  calculations
(see [1] for details) yield:

 
Var

M f
[

opt

+

j

=

]

f

2
opt

j

2

f

opt

j

+

1
2
opt

f

1
2
opt

f

{

3

}2

+

!

j
= -

where  j
Schwartz inequality. From the viewpoint of (11), j  is small if

j

.  Non-negativity  of  the  factor  in  curly  braces  follows  from  the  standard

f

2
opt

j

2

 

2

j

f

opt

<<

1.

2

f

2
opt

Since the deviation from optimality is quadratic with respect to the deviation  of  observables
from f opt, one realizes that the exact knowledge of the probability distribution p  is not really
necessary:  an  approximation  f quasi  to  f opt  in  the  sense  of  (12)  may  be  sufficient.  Such  an
approximation could be constructed even using a Monte Carlo generator.

There are several interesting points about this method.

•  The usual procedures of imposing cuts on events to enhance the signal/background ratio fully
,  where  only  the  signal
agree  with  the  above  prescriptions.  Indeed,  suppose 

+

=

p

p

p

bg

signal

contribution depends on M. Then

p

 

f

opt

=

p

M
+

signal
p

bg

signal

~

p

M
p

signal

.

bg

This vanishes where the background is large compared with the signal.

3

(8)

(9)

(10)

(11)

(12)

(13)

¶
¶
-
Ø
º
ß
Ø
ø
Œ
œ
º
ß
(cid:242)
·
-
·
-
¶
¶
•  The  optimal  observable  is  localized  on  events  where  p   exhibits  the  largest  variation  with
respect  to  the  parameter  being  studies  —  not  where  p   is  largest.  In  addition,  such  observables
may have different signs in different regions of phase space, e.g. in the case of parameters such as

masses.  Indeed,  for 

p

P
( )

,  the  optimal  observable  with  respect  to  M  has  the

form 

f

P
( ) ~

M

,opt

.  Then  one  has  an  array  of  simple  shapes  to  choose  from  in

construction of quasi-optimal observables as shown below:

1
P
)M

2

+ G

2

(
P
)
+ G
2

2

(

M
P

)

(

M

p ( )P

f opt ( )P

f quasi (

)P

 

PM

(a)

(b)

(c)

(d)

(14)

In the above example, there is another parameter, G

• 
. It is straightforward to define an optimal
observable for this parameter too. In general, with several parameters to be estimated, there is an
optimal observable per parameter. Error ellipsoids are constructed in the usual fashion.
•  A  theoretical  prediction  for  p   may  involve  a  low  order  result  and  some  higher  order
corrections.  In  some  cases  such  corrections  will  only  marginally  affect  f quasi ,  so  one  could
construct  f quasi  using the simplest expression for p theor . However, this affects only the construction
of  f quasi : once the latter is fixed, the extraction of M from data must involve 
 computed by

f

quasi

numerical integration of the  f quasi  thus fixed against the theoretical probability distribution with all
the corrections taken into account.
•  From  the  algorithmic  viewpoint,  the  problem  of  numerical  construction  of  a  quasi-optimal
observable  from  a  MC  generator  is  sister  to  the  problem  of  MC  integration.  There  is  a
considerable array of options here (cf. [3]), and given the described firm analytical foundation of
the  method,  I’d  expect  it  to  eventually  become  a  tool  of  choice  in  many  situations  where  at
present less focused methods are used, such as based on neural networks.

To summarize, parameter estimation via quasi-optimal observables combines, within a

flexible algorithmic scheme, the optimality of maximal likelihood with the simplicity of
generalized moments.

The method of quasi-optimal observables may be useful in experimental situations

high precision requirements and/or low signal;

characterized by:
(cid:222) 
(cid:222)  many events to be processed and/or the signal not localized sufficiently well for cuts to work;
a  complicated  underlying  theory  (absence  of  explicit  formula  for  probability  distribution  p ;
(cid:222) 
complicated higher order corrections; singular theoretical predictions for p ).

Finally, the author is rather uncomfortable with the claim to have discovered a new

algorithmic scheme for parameter estimation based on such a simple connection between the
two venerable methods — the methods of generalized moments and maximal likelihood —
both learnt by O(10000) students worldwide for about half century. However, I checked a

4

(cid:181)
-
-
-
large number of textbooks and monographs on mathematical statistics and its applications
and failed to find any trace of it being known to the experts. Also, there is an indirect
evidence: it is safe to say that all known methods of parameter estimation are used in high
energy physics one way or another (cf. [4]), and although attempts to construct better
observables using e.g. neural networks abound in high energy physics (cf. [4]), there seems
to be no trace of the notion of (quasi-) optimal observables being known to high energy
physicists. This is utterly puzzling as the connection is so simple. So, if the claim of novelty is
correct, the inevitable question is, why the connection was not discovered sooner?

The only explanations I can offer involve history and psychology. Indeed, the geometrical
viewpoint of functional analysis was not wide-spread at the time of discovery of the methods
of maximal likelihood and generalized moments, and programmers and calculationists still
have little working knowledge of it. On top of that, neither students nor researchers feel the
perfunctory proofs of elementary textbook results deserve more than a cursory glance:
students have so much to learn; mathematicians, so much to prove; data processing experts,
so much code to debug. In short, no one can afford to indulge in dwelling upon elementary
results when there is so much hard work to be done to earn one’s living. In the case of [1],
the pattern was broken by an unconventional motivation from the theory of jet observables
developed in [5]; the theory ran contrary to some prevailing prejudices, and as is usual in
such cases, the author was under pressure to seek all sorts of arguments to fortify it, which
led to a foray into the domain of mathematical statistics. Actually, the solution of the old
problem of finding optimal jet-finding algorithms described in [5] is per se a sufficient proof
(if such were needed) of usefulness of the concept of quasi-optimal observables.

I thank M. Kienzle-Focacci and P. Bhat for their interest. This work was supported in

parts by the RFBR grant 99-02-18365 and the NATO grant PST.CLG.977751.

References

[1] F.V. Tkachov: Approaching the parameter estimation quality of maximum likelihood

[2] W.T. Eadie, D. Dryard, F.E. James, M. Roos and B. Sadoulet: Statistical methods in

via generalized moments, physics/0001019.

experimental physics, North-Holland, 1971.

[3] G.P. Lepage: A new algorithm for adaptive multidimensional integration,  J. Comp.

Phys. 27 (1978) 192;
S. Kawabata: A new Monte-Carlo event generator for high-energy physics,  Comp.
Phys. Comm. 41 (1986) 127;
G.I. Manankova, A.F. Tatarchenko and F.V. Tkachov: MILXy Way: How much better
than VEGAS can one integrate in many dimensions? FERMILAB-Conf-95/213-T;
S. Jadach: Foam: multi-dimensional general purpose Monte Carlo generator with self-
adapting simplectic grid, physics/9910004.

[4] P.C. Bhat, H. Prosper and S.S. Snyder: Top quark physics at the Tevatron, hep-

ex/9809011 [Int. J. Mod. Phys. A13 (1998) 5113].

[5] F.V. Tkachov: Measuring the number of hadronic jets, Phys. Rev. Lett. 73 (1994)

2405 [hep-ph/9901332];
~: Measuring multijet structure of hadronic energy flow, or, What is a jet?
hep-ph/9601308 [Int. J. Mod. Phys. A12 (1997) 5411];
~: A theory of jet definition, hep-ph/9901444; rev. Jan. 2000.

5


A Bayesian Mean-Value Approach 
with a Self-Consistently Determined 
Prior Distribution for the Ranking of 
College Football Teams 
 
James R. Ashburn* 

Paul M. Colvert* 

 
 
 
  We  introduce  a  Bayesian  mean-value  approach  for  ranking  all  college  football 
teams using only win-loss data.  This approach is unique in that the prior distribution 
necessary  to  handle  undefeated  and  winless  teams  is  calculated  self-consistently.  
Furthermore, we will show statistics supporting the validity of the prior distribution.  
Finally, a brief comparison with other football rankings will be presented. 
 
KEYWORDS:  Ranking; Football; Bayesian 
 
 
 
 
 
 

Initial Version, 12 January 2006 
Minor Revisions, 28 February 2006 

 
 
 
 
 
 
 
*Atomic Football, GP, 1015 Harrison Avenue, Huntsville, AL  35801. 
 
Copyright ©2006 James R. Ashburn.  All rights reserved. 

 

1 

1. INTRODUCTION 

In 1998, Division I-A college football took a major step forward with the introduction 
of the Bowl Championship Series.  For the first time in history, the top  two ranked 
teams  would  play  for  the  title  by  design.    For  many  fans,  there  were  hopes  that  the 
seeds of a playoff system would eventually eliminate the need for ranking of teams.  
Unfortunately,  the  question  of  which  teams  would  participate  in  a  playoff  still 
remained.  The BCS committee chose to place that burden upon a composite ranking 
employing both voting polls and computer ranking systems.  While the inclusion of 
computer ranking systems represented a significant stride towards unbiased rankings 
(at least towards individual teams, conferences, or regions), other potential biases still 
remained, among the more prominent being the relative importance given to margin 
of victory. 
 
In 2002, the BCS committee moved to constrain the computer ranking systems to 
ignore  margin  of  victory  and  consider  only  wins  and  losses.    From  a  mathematical 
point of view, this decision was very significant.  Each game was now reduced to a 
single  bit of  information.  If we consider about 700 Division I-A, I-AA, II, III, and 
NAIA teams scheduled to play about 3600 games this year (2005), we will have 3600 
bits of data at the conclusion of the season.1  This is the equivalent of 450 bytes of 
information,  about  the  same  amount  as  one  verse  of  “The  Star-Spangled  Banner.”  
From  this  relatively  small  amount  of  information,  we  hope  to  accurately  rank 
hundreds of teams.  There is little wonder why the ranking of college football teams 
remains such a controversial problem. 
  Aside from the problem of limited information, another side effect of using only 
win/loss  information  is  perhaps  more  significant.    The  problem  arises  from  the  fact 
that many ranking systems, particularly those based upon more statistical approaches, 
rely  on  some  form  of  what  is  sometimes  called  a  win  probability  function  (WPF).2  
The  win  probability  function  describes  the  probability  of  one  team  winning  over 
another given the relative ratings of the two teams.  Note that the rating variables to 
which  we  refer  are  not  the  integer  rankings,  but  instead  some  measure  of  mean 
performance calculated such that the win probability functions best match the actual 
game  outcomes.    A  “best  match”  is  often  defined  as  the  set  of  rating  values  that 

                                                 
1 The limited amount of data supports the argument that rankings, for the purpose of rewarding teams 
with postseason play and the like, should equally weight all games.  Giving greater weight to games 
later in the season for the purpose of determining those teams that are “best” upon conclusion of the 
season (similar to methods employed in so-called “predictive” models) reduces the effective amount of 
data employed in the computations. 
2 Massey (2001) uses the term “game likelihood function.” 

 

2 

maximizes  a  composite  likelihood  function combining  the  individual  WPFs.    David 
(1988) points out that many of the more popular forms for the WPF can be written (or 
at least transformed to be written) as a cumulative distribution function (CDF) of the 
difference  between  the  ratings  of  the  two  teams  in  a  game.    As  a  CDF,  it 
asymptotically  approaches  zero  on  one  end  and  unity  on  the  other.    Herein  lies  a 
problem –  simply  maximizing  the  likelihood  against  a  set  of  data  that  includes 
undefeated teams will result in those teams having ratings approaching infinity.  This 
problem has its intuitive equivalent – how does one rate undefeated teams relative to 
teams with one or two losses, particularly when the undefeated team plays a relatively 
weak schedule?  After all, an upper bound to the undefeated team’s performance has 
not really been determined.  While one can assume that the undefeated team is better 
than all of its opponents, the question of how much better still remains. 
  While many methods have been formulated to address the problem of undefeated 
and  winless  teams,  these  methods  usually  mark  the  point  where  the  statistical 
foundations of many ranking systems tend to weaken.  Perhaps the most statistically 
sound  path,  and  one  recognized  by  a  number  of  authors  of  ranking  systems  on  the 
web,  is  the  introduction  of  a  Bayesian  prior  distribution.    Mease  (2003)  recently 
offered  an  excellent  discussion  of  this  idea.    Unfortunately,  a  new  problem  arises  – 
how does one best determine the prior distribution?  Mease proposed that all teams be 
considered to have n wins and n losses each against a common virtual opponent.  He 
states  that  n  was  determined  to  best  be  unity  “after  testing  the  model  on  various 
football seasons” but no details are provided.  Presumably, results were evaluated by 
comparisons  with  other  ranking  models.    Whether  or  not  this  is  the  case  here, 
comparing one model to others is a popular benchmark for validating a model, which 
brings us to our next topic. 
 
2.  THE VALIDITY OF A RANKING MODEL 

A brief survey of the literature3 seems to suggest that the primary historical method of 
validating ranking systems4 is by comparison  to  either other  ranking systems or the 
voting polls.  While this method may be useful for identifying very poor approaches, 
any system being evaluated in this manner can only show itself to not be substantially 
worse  than  its  peers.    Kenneth  Massey  (whose  rankings  have  been  part  of  the  BCS 

                                                 
3 See, for example, Mease (2003), Fainmesser et al. (2005), Colley (2003), and Boginski et al. (2004). 
4 While we may, at times, use the terms ranking system and ranking model interchangeably, our 
preference is to use the term model when referring to a system that indeed has some underlying model 
from which it is derived.  Massey (2001) appears to characterize systems without an underlying model 
as “formula-based systems.” 

 

3 

since  1999)  has  a  very  popular  page  on  his  website  entitled  “College  Football 
Ranking  Comparison”  where  he  ranks  ranking  systems  based  upon  the  degree  to 
which they correlate with a “consensus ranking” determined by averaging all of the 
systems  included  on  the  page.5    According  to  Massey’s  website,  this  page  is  a  key 
source  for  the  BCS  committee  in  their  selection  of  computer  ranking  systems.    To 
whatever degree the collective knowledge of a consensus ranking can be better than 
any  of  its  individual  components,  such  a  ranking  comparison  has  merits.  
Unfortunately, the fact that it is regarded as a resource for the BCS committee creates 
a great temptation for the authors of the ranking systems to tune their algorithms to 
better  match  the  consensus.    This  creates  a  potential  problem  –  if  any  substantial 
number of ranking systems were to be tuned for this purpose, the computer consensus 
becomes  a  “dog  chasing  its  own  tail,”  given  that  these  ranking  systems  are  also 
included in the consensus calculation.  In effect, the consistency between the ranking 
systems can theoretically continue to improve completely decoupled from the degree 
to  which  the  rankings  actually  reflect  the  performance  of  the  teams.    In  order  to 
elaborate on this idea, we must first introduce another metric. 
 
This second metric, ranking violations, is also included on Massey’s comparison 
page, although the ranking systems are not explicitly sorted by it.  A ranking violation 
occurs when a team is ranked higher than another team to which it lost.  This metric is 
often referred to in a more general sense as retrodiction, implying the degree to which 
rankings  are  consistent  with  past  game  results.    At  first  glance,  retrodiction  would 
seem  to  be  an  excellent  measure  of  the  validity  of  a  ranking  system.    However,  its 
utility is limited for a couple of reasons.  First, approaches that seek only to minimize 
ranking violations are unlikely to yield unique solutions.6  Thus, an undefeated team 
is presumed to be ranked equally well at any point above its best opponent.  Second, 
in 
teams  by  simply  minimizing  ranking  violations, 
counterintuitive situations may arise.  Consider the following example: 
 
 
 
 
 
While a case where three teams have played 10, 2, and 10 games, respectively, might 
not occur in practice, the point here is to create two teams (A and C) where one team 
is  unquestionably  superior  to  the  other.    We  then  introduce  a  third  team  (B)  that 

Team A 9-1 (9 wins over team C, 1 loss to team B) 
Team B 1-1 (1 win over team A, 1 loss to team C) 
Team C 1-9 (1 win over team B, 9 losses to team A) 

the  course  of  ranking 

                                                 
5 See http://www.masseyratings.com/compare.htm. 
6 According to Jay Coleman, “there are literally trillions of different rankings at any given point in time 
that would yield the same minimum number of violations” (see http://www.unf.edu/~jcoleman 
/minv.htm). 

 

4 

defeats the better team (A) and loses to the lesser team C.  Intuitively, we might tend 
to rank team B between teams A and C.  This would translate to attempting to balance 
a ranking violation in one direction with a ranking violation in the opposite direction.   
However, we can reduce the number of ranking violations from two (in the case of an 
ABC  ordering)  to  one  by  ranking  team  B  as  either  first  (BAC)  or  last  (ACB).    In 
short, a minimum ranking violation approach would dictate that team B is either best 
or worst but not in between.  This is obviously a paradoxical conclusion. 
  Does  this  paradox  occur  in  practice?    One  result  that  is  often  seen  in  computer 
rankings  but  seen  with  markedly  less  frequency  in  polls  is  where  a  team  is  ranked 
slightly below a team it defeated.  When such a situation occurs, fans will often cry 
foul,  invoking  the  concept  of  “head-to-head”  competition.    Similarly,  voters  in  the 
polls respond to naturally “correct” such situations, flipping the closely ranked teams 
to  alleviate  the  perceived  inconsistency.    The  typical  human  response,  however,  is 
flawed.    These  ranking  violation  “corrections”  are  most  often  demanded  when  the 
two  teams  involved  fall  closely  in  the  polls.    Curiously  though,  ranking  violations 
involving teams sufficiently separated in the polls seem to be comfortably ignored.  In 
effect,  big  upsets  are  more  easily  believed  than  small  ones.7    Obviously,  were  two 
teams involved in a  ranking  violation to fall consecutively in the rankings, why  not 
“correct”  the  violation?    After  all,  a  correction  in  this  situation  cannot  induce  other 
ranking  violations.    However,  the  implications  of  this  line  of  reasoning  are 
counterintuitive, suggesting that if team A defeats team B, then team A is either much 
inferior to team B (as determined by other games played by both teams) or team A is 
at least slightly superior to team B (as determined by the “head-to-head” match), but 
team A cannot be slightly worse than team B.  As we will soon show, upsets are not 
only a natural part of college football, they are extremely common, occurring about 
20%  of  the  time.    Thus,  ranking  violations  should  be  expected  (including  these 
“minor violations”), and it should not simply be the purpose of a ranking system to 
minimize them but to instead be able to estimate the frequency of them. 
  Now that we have introduced retrodiction, we will return to our discussion of the 
potential  problem  mentioned  above  –  the  risk  of  ranking  systems  being  tuned  to 
match  a  consensus  of  which  they  are  a  part,  resulting  in  the  computer  consensus 
“chasing  itself.”    While  we  cannot  verify  a  cause,  there  is  perhaps  evidence  for  the 
effect.  Initial analysis performed by us indicates that Massey’s computer consensus 

                                                 
7 Of course, we define an upset here based upon a team’s mean performance/rating for a season.  There 
is little doubt that some teams strengthen during the course of a season while others wane.  It is 
currently beyond the scope of this paper to attempt to quantify this effect.  Furthermore, attempting to 
do so within any ranking model will at least double the number of unknowns that must be estimated 
from the same data, although this may admittedly be of value to predictive models. 

 

5 

may  indeed  be  undergoing  a  transition  of  this  kind.    Among  the  indicators  is  an 
apparent shift in the correlation between the “correlation to consensus” scores and the 
“ranking violation” scores.  Assuming the majority of these algorithms are not based 
upon  explicitly  minimizing  ranking  violations,  then  one  would  expect  that  the 
“better” systems, as measured by a high correlation with the consensus, would tend to 
have a lower occurrence of ranking violations.  Presumably, this general relationship 
should be detectable when a sufficient number of ranking systems are examined.  In 
other  words,  there  should  be  a  negative  correlation  between  the  “correlation  to 
consensus” scores and the respective “ranking violations.”  However, Figure 1 shows 
that  from  2000  to  2005,  there  appears  to  be  a  trend  (as  indicated  by  the  arrow)  in 
which the highest ranked ranking systems show an increasingly positive correlation.  
In 2000, a consistent negative correlation is detectable when about 20 or more of the 
highest  ranked  systems  are  examined.    By  2005,  however,  a  consistent  positive 
correlation becomes apparent when the 35 highest ranked systems are considered, and 
the  coefficient  remains  positive  until  about  70  systems  are  included.    This  suggests 
the possibility that the consensus is beginning to decouple with actual outcomes. 

2000
2001
2002

2003
2004
2005

h
t
i

w
 
n
o
i
t
a
l
e
r
r
o
C
"
 
n
e
e
w
t
e
B
 
n
o
i
t
a
l
e
r
r
o
C

"
s
n
o
i
t
a
l
o
i
V
 
g
n
i
k
n
a
R
"
 
d
n
a
 
"
s
u
s
n
e
s
n
o
C

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

0

20

40

60

80

100

Number of "Highest Ranked" Ranking Systems Considered

 

Figure  1.    Correlation  (ρ)  Between  the  “Correlation  to  Consensus”  Score  and  the 
“Ranking Violations” Score for the N “Highest Ranked” Ranking Systems 
 

 

6 

 
Thus,  we  suggest  that  the  more  popular  metrics  for  validating  ranking  systems 
have  potential  weaknesses,  and  we  are  still  faced  with  the  dilemma  of  how  to 
unambiguously validate a ranking system.  While we do not claim to offer a complete 
solution, we would like to suggest a different path.  In doing so, we begin by making 
the following proposition: 
 

A valid ranking system should, first of all, be based upon a model.  As 
such, the algorithm derived from it should be able to determine metrics 
that  can  be  validated  against  equivalent  metrics  that  can  be 
independently derived from the game data. 

 
We  will  attempt  to  clarify  this  and  concurrently  achieve  it  in  the  analysis  section 
below.  However, we must first introduce the model. 
 
3.  THE MODEL 

The model introduced here closely parallels the model documented by Mease (2003).  
Other models with similar features -- Bayesian prior, WPF based upon a normal CDF 
per Thurstone (1927) and Mosteller (1951), etc. -- can be found documented on the 
web  by  Massey  (2001)  and  Dolphin,8  among  others.    The  fundamental  difference 
between the model herein and other similar models is the method by which the prior 
distribution  is  obtained.    For  this  reason,  we  will  move  quickly  through  an 
introduction of the basics of the model (referring the reader to Mease’s work for more 
details) in order to focus our attention on the derivation of the prior distribution. 
  Assume  that  over  the  course  of  a  season,  each  team  i  can  be  characterized  by  a 
single “mean performance” variable  ri.  From game to game, each team’s “effective 
'  is  assumed  to  deviate  about  the  mean  in  a  manner  described  by  a 
performance”  ri
normal  distribution  of  variance  1/2.    We  shall  call  this  variance  the  performance 
variability.    The  specific  value  chosen  is  irrelevant  (defining  only  the  scale  of  the 
final  ratings)  and  has  been  selected  simply  for  convenience.9    Assuming  the 
performance  deviations  of  two  teams  in  a  game  are  independent,10  the  relative 
performance variability associated with the relative performance  ri

'
' − rj

 becomes 

                                                 
8 See http://www.dolphinsim.com/ratings/. 
9 Mease (2003) independently selected the same value. 
10 To the degree that teams “play up” or “play down” to their competition can, in the mean, be modeled 
by a simple scale factor between the mean performance values of teams in a game, such an effect 
simply rescales the ratings results, with no effect on the final rankings.  Given that the scale is 
arbitrary, only second order effects would then affect the final rankings.  Such a second order effect 
would be, for example, that teams are more prone to “play up” to competition of comparable 

 

7 

 

 

σv =

+

= 1, 

1
2

1
2

 
thus revealing the rationale for the choice of scale. 
 
' − rj
ri
probability function 
 

In a game between team i and team j, team i is assumed to win when  ri

'  (or 
' > 0).11    Thus,  the  probability  of  team  i  defeating  team  j  is  given  by  the  win 

' > rj

(
P ri − rj ,σv

) 

 
where P(x,σ) is the cumulative normal distribution12 
 

P x,σ(

) =

x∫

−∞

p x,σ(

)dx

 

 
and p(x,σ) is the normal distribution 
 

p x,σ(

) =

1
2πσ

⎛ 
exp −
⎜ 
⎝ 

x 2
2σ2

⎞ 
⎟ .
⎠ 

 

(1)

 

(2)

 

(3)

 

(4)

 

 
  We will now formally define the term rating to refer to the mean performance of 
a team, whether true or estimated.  Let us also define the following: 
 

i = team index 
ri =true rating of team i 
˜ r i =estimated rating of team i 
2 =estimated uncertainty of team i’s rating 
σi
N i = number of games scheduled by team i 
j =index indicating teams played by team i 

                                                                                                                                           
performance – when the likelihood of the added effort affecting the game outcome is highest – and less 
likely to do so when the performance differential is high. 
11 Neglecting home field advantage, which we will address in a later section. 
12 While Mease (2003) argues for the validity of the normal CDF (based upon the central limit 
theorem), others contend that the logit CDF -- functionally equivalent to the Zermelo model, aka the 
Bradley-Terry (1952) model -- is best suited for most real world “binary symmetric games.”  See, for 
example, Smith (1994).  In considering Smith’s arguments, note that he did not point out that the logit 
CDF can be derived from a model where teams have independent exponentially distributed 
performance distributions. 

8 

k = index indicating teams scheduled by team i (including teams played) 

 
Two subscripts, ij for example, indicate a reference to a game between teams i and j.  
In  some  contexts,  ij  and  ik  may  be  abbreviated  as  j  and  k,  respectively,  where  i  is 
k  indicates  a  uniformly  weighted  average  over  all 
understood.    The  operation 
scheduled games. 
  Given  these  definitions  and  assumptions,  let  us  assume  that  we  have  uncertain 
estimates of the ratings for team i’s opponents.  In order to estimate team i’s rating, 
we will need to develop the WPF as part of an integrand with which we will integrate 
over all possible values of team i’s rating: 
 

Φij (r) =

⎧ 
(
)
P ∆ j r( ),σjv
⎪ 
(
P −∆ j r( ),σjv
⎨ 
⎪ 
)
(
p ∆ j r( ),σjv
⎩ 

if i wins
) if j wins
if a tie

 

 
where 
 

 
and 
 

∆ j r( ) = r − ˜ r j  

σjv = σj

2 +σv

2 = σj

2 + 1 . 

 
Note that the first two forms for  Φij r( ) are cumulative distribution functions while the 
third is the normal distribution function.  While the latter should be unnecessary for 
modern college football with the universal use of overtime, it has been included here 
for completeness. 
  We  now  have  a  form  for  the  WPF  Φij r( )  where  it  will  be  used  to  iteratively 
estimate the rating of team i given estimates of the ratings  ˜ r j  of its opponents, thus 
the  inclusion  of  the  opponent’s  rating  uncertainty.    Equation  (7)  represents  the 
combined  effects  of  the  relative  performance  variability  from  equation  (1)  and  the 
uncertainty of the opponent’s rating σj
  Given these definitions, we define a set of intermediate moments 
 

2 to be derived below. 

m i n( ) =

+∞∫

−∞

dr r nΦ p r,σp

(

∏
) Φ ij r( )

 

j

 

9 

(5)

 

(6)

 

(7)

 

(8)

 

 
where n = 0, 1, 2, and where we have introduced the very important prior distribution 
(
Φ p r,σp
  With these definitions, we can now estimate each team’s rating by the following: 
 

) will be derived in the next section. 

) discussed earlier.   Φ p r,σp

(

 
where  the  zeroth  moment  is  necessary  for  proper  normalization  of  the  result.  
Furthermore, the uncertainty in the rating can be estimated by 
 

˜ r i =

mi(1)
mi(0)

 

2 =

σi

mi(2)
mi(0)

−

⎛ 
mi (1)
⎜ 
mi(0)
⎝ 

2
⎞ 
⎟ 
⎠ 

. 

 
Thus, once we have an equation for the width of the prior distribution, we will have a 
complete set of self-contained, self-consistent equations that can be solved iteratively 
for the team ratings.  It is important to note here that maximum likelihood approaches 
are far more popular for football rankings than those based upon mean values.  While 
maximum  likelihood  solutions  often  reduce  or  eliminate  the  need  for  numerical 
integration, they also impair the potential utility of the higher order statistics such as 
rating uncertainty estimates.  As will become apparent in the derivations that follow, 
a mean value approach and the options it affords are critical to our model. 
  Before describing the process for determining the prior distribution, we will take a 
moment  to  further  discuss  its  necessity.    Consider  equation  (8)  less  the  prior 
distribution.  If a team i is undefeated or winless, the composite distribution 
 

∏

Φij (r)

 

j

 
will have but a single tail, resulting in moments (and ratings) of infinite magnitude.  
Thus, without a confining prior distribution, all undefeated teams will be estimated to 
be  of  infinite  performance.    Based  strictly  upon  a  model  employing  only  wins  and 
losses, the mean (and maximum likelihood) performance must be infinite, leaving all 
undefeated teams indistinguishable.  Similarly, winless teams will be estimated to be 
of infinite negative rating.  It is apparent that such a problem is most serious in sports 
where  conditions  are  conducive  to  undefeated  and  winless  teams,  namely  relatively 
small  numbers  of  games  and  often  uncompetitive  schedules.    College  football  is 

(9)

 

(10)

 

(11)

 

 

10 

perhaps  most  prominent  in  this  regard,  while  sports  such  as  professional  baseball 
might fair quite well without the prior distribution. 
  A survey of the internet has revealed a few rating methods that are similar to ours 
up to this point in the derivation.  A handful of which we are aware have introduced 
prior  distributions,  but,  to  our  knowledge,  none  of  the  parameters  in  the  prior 
distributions  described  in  those  models  appear  to  have  been  derived  in  any  self-
consistent manner.  Furthermore, we are not aware of any where attempts have been 
made to quantitatively demonstrate the validity of the prior distributions.  Thus, it is 
here where we suggest that our model is a significant step towards achieving a truly 
self-contained model of its kind. 
 
4.  THE PRIOR DISTRIBUTION 

We begin this section with the following conjecture:13 
 

In  the  presence  of  large  numbers  of  games  per  team,  a  prior 
distribution  can  be  easily  estimated  but  is  relatively  unnecessary.    In 
the  presence  of  very  small  numbers  of  games  per  team,  a  prior 
distribution  cannot  be  easily  determined  but  is  critical  to  achieving 
well-behaved  results.    In  the  presence  of  intermediate  numbers  of 
games  per  team,  it  may  be  possible  to  describe  the  relationships 
between  the  posterior  and  prior  distributions  from  which  a  self-
consistent prior distribution can be estimated. 

 
In order to elaborate on this supposition, we begin with an intermediate (and simpler) 
form for what will eventually become our prior distribution.  Let us assume that the 
ratings across all teams are normally distributed according to 
 

Φp (r,σp ) =

1
2πσp

⎛ 
exp −
⎜ 
⎝ 

r 2
2
2σp

⎞ 
⎟  
⎠ 

(12)

 

                                                 
13 AUTHOR’S NOTE:  This paper in its current form was developed prior to our introduction to 
Empirical Bayes methods.  Until we have the opportunity to replace the derivations below with a 
rigorous application of empirical Bayes, we provide in the interim the original text (albeit a somewhat 
clumsy and parochial “rediscovery” and application of empirical Bayes).  The reader may note that the 
analysis which follows suggests that the hyperparameters (another new term to us) which we derived 
are likely not particularly poor estimates of those we hope to derive more precisely in the future. 

 

 

11 

where σp is as yet undetermined.  Now assume that no games have yet been played.  
Then, using equations (9) and (10) we find the following 
 

 
and 
 

˜ r i = 0  

σi

2 
2 =σp

(13) 

(14) 

(15)

 

(16) 

(17)

 

 
for all teams.  Next, consider the other extreme case where all the teams have played 
an  infinite  number  of  games.    We  then  expect  each  team’s  estimated  rating  to 
converge to its true rating.  If the variance of the prior estimate from equation (12) is 
correct, then the variance of the rating estimates will be 
 

2  
2 = σp
˜ r i

 
where,  for  the  purposes  of  this  argument,  we  are  assuming  that  the  mean  estimated 
rating  is  defined  to  be  exactly  zero.    Given  an  infinite  number  of  games,  the 
corresponding rating uncertainties will approach zero, 
 

σi

2 = 0. 

 
Thus, we hypothesize that 
 

σp

2 = ˜ r i

2 + σi

2  

 
in  all  cases,  including  intermediate  (non-zero,  finite)  numbers  of  games.    Another 
way to view this is that the prior distribution can be represented by the superposition 
of the distributions that are described by the rating estimates and their corresponding 
rating  uncertainties.    In  the  previous  equation,  we  have  assumed  that  all  teams 
schedule  and  play  the  same  number  of  games,  an  issue  we  will  address  in  greater 
detail later. 
 
If  teams  randomly  selected  their  opposition  from  among  all  teams,  then  our 
current  form  for  the  prior  distribution  should  be  valid.    However,  given  that  most 
games  are  within  the  same  division  and  conference,  we  will  now  expand  our  prior 
distribution to the following form 
 

 

12 

Φ p (r,σp ) =

1
2πσp

⎡ 
⎢ 
exp −
⎢ 
⎣ 

)2

(

r − rik k
2
2σp

⎤ 
⎥ 
⎥ 
⎦ 
.

 

(18)

 

 
We  have  now  centered  the  prior  distribution  on  the  mean  scheduled  opponent’s 
rating.  While we could have centered it on the mean played opponent, by the end of 
the season the two will be equivalent anyway.  Furthermore, prior to the end of the 
season, this form appears to lend greater stability to the midseason ratings. 
 
To  estimate  σp  for  this  new  form  of  the  prior  distribution,  we  will  need  to 
determine two other quantities.  The first quantity will measure the variance of ratings 
within  the  typical  schedule.    It  will  be  referred  to  as  the  mean  schedule  variance 
(MSV).    A  small  value  indicates  that  teams  tend  to  schedule  opponents  of  very 
similar  performance  to  each  other.    A  large  value  indicates  that  teams  schedule 
opponents  of  widely  varying  performance.    The  second  quantity  will  measure  how 
well  teams  compare  to  their  mean  opponent’s  rating.    It  will  be  referred  to  as  the 
mean  team  variance  (MTV).    A  small  value  indicates  that  teams  tend  to  be  well 
matched  to  their  “average  opponent.”    A  large  value  indicates  that  teams  are  often 
poorly matched to their “average opponent.” 
  Before  we  proceed  further,  let  us  examine  the  significance  of  the  relative 
magnitudes  of  the  mean  schedule  variance  and  the  mean  team  variance.    First, 
consider the case where the MSV is much larger than the MTV.  In such a case, teams 
will more consistently play schedules where about half of their opponents are inferior 
and  half  are  superior.    Thus,  winning  percentages  will  have  a  tendency  to  cluster 
around  0.500.    Next,  consider  the  case  where  the  MSV  is  much  smaller  than  the 
MTV.    In  this  case,  a  significant  number  of  teams  will  play  schedules  where  all  of 
their  opponents  are  either  substantially  inferior  or  substantially  superior.    Thus, 
winning percentages will have a wide variation, with many teams going undefeated or 
winless. 
 
Therefore, we would expect the relative magnitudes of the MSV and the MTV to 
be  reflected  in  the  distribution  of  winning  percentages.    In  fact,  the  variance  of 
winning  percentages  was  our  first  attempt  at  a  second  metric  for  validating  our 
estimates  of  the  MSV  and  MTV.    However,  because  the  distribution  of  winning 
percentages is sensitive to the numbers of games played by the various teams (which 
often ranges from about nine to thirteen games in a typical season), we formulated a 
related but more practical metric which we call the outcome correlation  coefficient.  
This metric is effectively a correlation coefficient describing the degree to which the 
outcomes of two games are correlated (i.e., win-win/loss-loss vs. win-loss) where one 
team is common to both games.  Ultimately, we wish to evaluate this metric across all 

 

13 

valid game pairs across all teams, thus yielding a quantity that characterizes a given 
season. 
  We  will  start  by  deriving  the  outcome  correlation  coefficient  for  a  single  team.  
Note that for any given team, the number of independent game pairings is effectively 
the number of games less one.  While we could pair games according to the order in 
which  they  were  played,  this  might  make  the  result  vulnerable  to  “trends”  in 
performance  over  the  course  of  a  season.    Since  the  result  will  obviously  vary 
depending  upon  how  the  games  are  paired,  we  will  instead  evaluate  the  coefficient 
across all possible pairings within each team’s schedule and then weight that team’s 
contribution  to  the  season’s  coefficient  by  the  effective  number  of  independent 
pairings.  As a matter of expediency, we define a win by the team common to the pair 
of games as an outcome of 1.0 and a loss as an outcome of -1.0.  Thus, the correlation 
coefficient for a team i with Wi wins and Li losses is 
 

ρi =

1
2

)+

(
W i W i −1
1
2

(
W i + Li

)− W iLi

(
Li Li −1

1
2
)
) W i + Li −1
(

. 

 
For the sake of clarity, we have neglected to simplify the result.  From this equation, 
one can see that ρ is 1.0 for undefeated and winless teams, approaches zero for 0.500 
teams as the number of games goes to infinity, and is -1.0 only for teams with exactly 
one win and one loss.  It is, of course, indeterminate for teams with fewer than two 
games.    In  order  to  achieve  well-behaved  results,  we  generate  a  weighted  average 
outcome correlation coefficient for a season according to 
 

∑

(
W i + Li −1

)ρi

)
(
W i + Li −1

ρ=

i

∑

i

∑

i

1
2

=

1
2

 
)+
1
2
)
(
W i + Li −1

(
W i + Li

)

∑

i

(
W i W i −1

(
Li Li −1

)− W iLi

. 

(19)

 

(20)

 

 
We now have a parameter that measures overall scheduling parity, that is, how well 
teams  are  matched  to  their  opposition.    It  also  serves  as  a  measure  of  the  relative 

 

14 

magnitudes  of  the  mean  team  variance  and  the  mean  schedule  variance.    A  value 
approaching unity indicates that the MTV is much larger than the MSV (see Figure 
2), while a value approaching zero indicates that the MTV is much smaller than the 
MSV (see Figure 3). 
 

Team Performance,
sigma=sqrt(MTV)
Opponents' Performance,
sigma=sqrt(MSV)

As outcome correlation 
approaches 1.0

Team Performance,
sigma=sqrt(MTV)
Opponents' Performance,
sigma=sqrt(MSV)

As outcome correlation 
approaches 0.0

 

 
Figure  2.    Relationship  between  the  Outcome  Correlation  Coefficient  and  the 
Relative  Magnitudes  of  Mean  Schedule  Variance  (MSV)  and  Mean  Team  Variance 
(MTV) as the MTV Increases. 
 

 

 

 
Figure  3.    Relationship  between  the  Outcome  Correlation  Coefficient  and  the 
Relative  Magnitudes  of  Mean  Schedule  Variance  (MSV)  and  Mean  Team  Variance 
(MTV) as the MTV Decreases. 

 

 

15 

Table 1 below shows the values of the outcome correlation coefficient derived for 

 
recent years. 
 

Outcome Correlation 
Coefficient (ρ) 
0.164 
0.158 
0.164 
0.151 
0.157 
*Games through 11/19/05. 

Season 
2001 
2002 
2003 
2004 
2005* 

Estimated 
“Precision” (σρ) 
0.011 
0.011 
0.010 
0.010 
0.011 

 
Table  1.    Outcome  Correlation  Coefficient  for  all  Division  I-A,  I-AA,  II,  III,  and 
NAIA College Football Teams for the Years 2001-2005. 
 
Without going into details, the quantity σρ is a coarse estimate of the “precision” of 
the correlation coefficient based upon the premise that a season represents a sample 
of some hypothetical large population and that if the given season could be replayed 
multiple  times,  with  the  teams  maintaining  their  mean  performance  levels,  then  the 
coefficient  would  tend  to  vary  according  to  the  indicated  precision.    Note  that  the 
estimates  of  σρ  are  on  the  order  of  the  seasonally  variability  in  the  correlation 
coefficient  (about  0.005  in  this  small  sample),  indicating  that  scheduling  parity  has 
not varied to any statistically significant degree over this relatively brief period. 
 
To  this  point,  we  have  not  been  particularly  concise  about  distinctions  between 
sample and population variances.  In our model, we presume that a season of games 
represents a sampling of errored “measurements” of relative performance from some 
potentially  infinite  hypothetical  population.    As  more  “measurements”  (i.e.,  games) 
are  taken,  the  accuracy  with  which  true  mean  performance  can  be  measured 
improves.    We  will  also  assume  that  schedules  represent  samples  as  well.    Thus,  if 
more games could be scheduled and played, the “true” mean opponents’ ratings could 
be measured with increasing accuracy.  Given the relatively small samples inherent in 
college  football  (and  football  in  general),  we  must  be  careful  in  the  following 
derivations  to  account  for  the  differences  between  sample  and  population  variance.  
This is also necessary to account for the effects of schedules with differing numbers 
of games. 
  We will defer until later in our discussion a derivation of the relationship between 
the outcome correlation coefficient and the MSV and MTV.  We will proceed instead 
with  the  derivation  of  the  MSV  and  MTV  from  within  the  ranking  model  itself, 

 

16 

followed by a demonstration of how the prior distribution introduced in equation (18) 
is a function of these variables. 
 
ratings about the mean opponent’s rating 
 

First,  we  determine  the  sample  variance  of  each  team’s  scheduled  opponents’ 

'2 =
si

(
rik − rik

)2

k

 

k .

 
Note  that  this  expression  is  currently  in  terms  of  the  true  ratings.    Later,  we  will 
translate  it  (and  several  of  the  expressions  to  follow)  into  forms  operating  on  the 
rating estimates – where we will be required to consider also the effect of the rating 
uncertainties. 
  Now,  since  this  sample  variance  is  a  biased  estimator14  that  is  sensitive  to  the 
number of scheduled games played by each team, we cannot determine a meaningful 
average of this quantity across teams without an equivalent unbiased estimator, in this 
case the equivalent population variance as given by 
 

σi

' 2 =

N i
Ni −1

'2
si

. 

 
'2),  is  inversely  proportional  to  Ni-1,15 
Because  the  variance  of  this  estimator,  var(σi
we  will  generate  a  well-behaved  least-squares  weighted  average  –  our  “mean” 
schedule variance – as follows: 
 

σ' 2 =

(
N i −1

)σi

'2

)
(
N i −1

∑

i

∑

i

' 2
Nisi

∑

i

 

)
(
N i −1

.

=

∑

i

(21)

 

(22)

 

(23)

 

 
This  pattern  of  Ni-1  weighting  will  be  used  a  number  of  times  in  the  derivations  to 
follow.  Note that in the last few equations, the prime symbol is used to denote the 
“schedule  variance,”  that  is,  the  distribution  of  ratings  within  a  schedule  about  the 
mean rating. 

                                                 
14 See, for example, Mathworld, http://mathworld.wolfram.com/SampleVariance.html. 
15 Assuming underlying normal distributions.  See Mathworld, http://mathworld.wolfram.com 
/k-Statistic.html and http://mathworld.wolfram.com/NormalDistribution.html. 

 

17 

 
Similarly,  we  wish  to  determine  the  distribution  of  the  ratings  of  the  teams 
themselves about their mean opponents’ ratings.  For each team, the sample variance 
is but a single “measurement” 
 

2 = ri − rik k
si

(

)2

. 

 
Just as the prime symbol above indicated schedule variance, its absence here indicates 
team  variance.    Correcting  for  the  bias  between  sample  and  population  that  comes 
through the mean opponents’ rating yields 
 

 
Weighting by Ni-1 to yield the mean team variance produces 
 

 
In  order  to  replace  the  true  ratings  with  the  estimated  ratings,  we  must  include  the 
effects  of  the  rating  uncertainties.    Assuming  that  game-to-game  performance 
deviations are independent yields the following substitutions: 
 

σi

2 = si

2 −

' 2
si
N i − 1.

 

∑

i

σ2 =

(
Ni −1

)si

'2
2 − si

. 

)
(
Ni −1

∑

i

ri → ˜ r i 
rik → ˜ r ik  

'2 → si
si

'2 +

Ni −1
N i

2
σik

 

k

2
σik

Ni

 

.

2 → si
si

2 +σi

2 +

(24)

 

(25)

(26)

(27) 
(28) 
(29)

(30)

 

 

 

 

 
Given  that  the  final  terms  in  the  latter  two  expressions  would  cancel  neatly  upon 
substitution  into  equation  (26)  above,  we  were  very  tempted  to  proceed  with  this 
form.    However,  from  the  form  of  equation  (8)  above,  it  is  clear  that  were  all  of  a 
team  i’s  opponents’  ratings  errored  by  the  same  quantity,  then  the  rating  for  team i 
would  be  offset  by  the  same  amount,  as  both  the  prior  distribution  and  the  win 

 

18 

probability functions would be similarly translated.  In the simplest possible model of 
this correlation, 16 we simply eliminate the final term in equation (30) yielding 
 

2 → si
si

2 +σi

2. 

 
Completing all the substitutions yields the MSV (σ' 2) and MTV (σ2), respectively, 
 

∑

Ni

⎡ 
⎣ ⎢ 

σ'2 =

i

)2

+

N i −1
N i

2
σik

(
˜ r ik − ˜ r ik k
∑

i

)
(
N i −1

⎤ 
⎦ ⎥ 
 

k

(
Ni −1

[
(
) ˜ r i − ˜ r ik k

)2

∑

i

σ2 =

(
˜ r ik − ˜ r ik k

)2

+

N i −1
Ni

2
σik

⎤ 
⎦ ⎥ 
 

k

2
+σi

⎡ 
]−
⎣ ⎢ 
∑
)
(
N i −1

i

 
which  are  in  terms  only  of  the  team  ratings,  rating  uncertainties,  and  numbers  of 
games scheduled. 
  Analogous  to  equations  (22)  and  (25),  the  estimates  of  the  prior  team  and 
pσ ,  respectively)  for  each  team  should  follow 
schedule  sample  variances  (
from 
 

pσ   and 

2'

2

σ' 2 =

Ni
Ni −1

'2
σp

 

σ2 = σp

2 −

' 2
σp
N i −1 .

 

 
and 
 

 
Eliminating 
 

2'

pσ  and solving for 

pσ  yields 

2

(31) 

(32)

(33)

(34)

(35)

 

 

 

 

                                                 
16 This can be derived by assuming that the team rating error includes a component that is the average 
of the errors in the ratings of its opponents.  This results in cross-terms that will effectively cancel out 
the final term in equation (30). 

19 

 

 

σp

2 =σ2 −

σ' 2
N i ,

 

(36)

 

 
which becomes our method of estimating of the variance in the prior distribution of 
equation (18).  Note that in equation (18) we did not anticipate that the variance of the 
prior distribution would vary from team to team.  From the form above, however, it is 
clear that it must be vary between teams according to the number of scheduled games. 
  We have gone to great lengths to derive a prior distribution centered on a mean 
opponent.  Given division and conference play, we believe that this is a valid model, 
and we will support this with the results we shall derive below.  Of course, this might 
not  necessarily  be  a  good  model  for  all  sports.    If  typical  schedules  were 
representative  of  the  full  set  of  teams  (for  example,  if  opponents  were  randomly 
selected  from  all  teams),  then  a  prior  distribution  centered  on  some  arbitrary  point 
(e.g., zero) would be more valid.  This was, in fact, the form we derived previously in 
equation (17). 
 
In any case, we now possess a complete set of equations.  The unknowns are the 
team ratings, the rating uncertainties, the mean schedule variance, and the mean team 
variance from which the variance of the prior distribution can be determined for each 
team.    To  solve  the  equations,  we  do  so  iteratively.    All  teams  are  given  an  initial 
rating of zero and rating uncertainty of one, and the MSV and MTV are each set to 
one.    The  team  ratings  and  rating  uncertainties  are  updated  based  upon  the  game 
outcomes,  from  which  new  estimates  for  the  MSV  and  MTV  are  determined.    The 
process is repeated until convergence is achieved.  As a practical matter, because the 
ratings  are  not  anchored  to  any  absolute  point,  only  the  relative  ratings  (i.e., 
differences) are significant.  As a result of this, there is often a slight tendency in the 
course  of  numerical  simulation  for  the  ratings  to  drift  slightly  as  a  group,  thus 
frustrating  attempts  to  determine  convergence.    Thus,  at  the  conclusion  of  each 
iteration, a mean rating is determined and then subtracted from all ratings, restoring 
an  average  rating  of  zero.17    This  “normalized”  result  can  then  be  compared  to  the 
previous iteration to determine if convergence has been achieved. 

                                                 
17 It is also necessary that there be no disconnected subgroups.  For example, the Division III NESCAC 
Conference teams play only games among themselves and must therefore be excluded for convergence 
to be achieved. 

 

20 

5.  ANALYSIS 

From the MSV and MTV in equations (32) and (33), respectively, we can estimate an 
outcome correlation coefficient for the prior distributions they represent 
 

ρ=

+∞∫

−∞

dri

+∞∫

−∞

drj

−∞

+∞∫
)×
)Φ rk,σ'
(
(
Φ rj ,σ'
)
drkΦ ri,σ(
 
]
[
]1 − 2P ri − rk
[
)
(
)
(
1− 2P ri − rj

 
where we have exploited the fact that 
 

P(idefeats j)P(i defeats k) + P( j defeats i)P(k defeats i)
− P(i defeats j)P(k defeats i) − P( j defeats i)P(i defeats k) 

(
= P ri − rj
(
− P ri − rj

(
= P ri − rj

)P ri − rk
(
(
)P rk − ri

)P rk − ri
)
(
(
)P ri − rk

)+ P rj − ri
(
(
)− P rj − ri
) 
] P ri − rk
[
) − P rj − ri
(
(
)
) − P rk − ri
(
]. 
[
]1− 2P ri − rk
)
(
)

= 1− 2P ri − rj

(

[
[

]
)

 

(37)

 

(38)

 

 
Applying our algorithm to the results from the 2001-2005 seasons and then estimating 
the outcome correlation coefficient from the final values for the MSV and MTV, we 
obtained the following results (where we have repeated here the previous results from 
above to expedite the comparison): 
 

Season 
2001 
2002 
2003 
2004 
2005* 

Actual ρ 
0.164 
0.158 
0.164 
0.151 
0.157 

*Games through 11/19/05. 

Estimated ρ 
0.161 
0.158 
0.164 
0.153 
0.151 

Difference 
-0.003 
0.000 
0.000 
+0.002 
-0.006 

 
Table  2.    Outcome  Correlation  Coefficient  for  all  Division  I-A,  I-AA,  II,  III,  and 
NAIA College Football Teams for the Years 2001-2005 Derived from Actual Game 
Data and from Prior Distributions Obtained from the Model. 

 

21 

From  the  results,  it  is  clear  that  the  estimates  are  both  quite  accurate  and  relatively 
unbiased. 
  We have not noted in the table above the values derived for the MSV and MTV.  
These are, however, worthy of some discussion.  For the years 2001-2005, the square 
root  of  the  MSV  ranged  from  about  0.93  to  1.02  with  no  discernable  trend.    The 
square  root  of  the  MTV  ranged  from  about  0.76  to  0.85.    Prior  to  being  able  to 
calculate these quantities, we speculated as to their relative magnitude.  A value of the 
MTV much larger than that of the MSV seemed counterintuitive – why would teams 
tend  to  often  schedule  opponents  either  all  much  better  or  all  much  worse  than 
themselves?  Thus, it seemed reasonable that the MTV would  be no larger than the 
MSV.  Furthermore, it seemed reasonable that teams would desire to be well-matched 
to their opposition.18  For this reason, one might expect the MTV to be much smaller 
than  the  MSV.    However,  because  conference  play  usually  accounts  for  more  than 
half  of  all  games,  the  MTV  could  not  be  substantially  smaller.    Thus,  our  final 
expectation  was  that  the  MTV  would  be  comparable  to  or  possibly  slightly  smaller 
than the MSV, which we indeed found. 
  Unfortunately,  validation  of  the  prior  distribution  in  our  case  is  not  complete.  
While the outcome correlation coefficient describes the magnitudes of the MSV and 
MTV  relative  to  each  other,  a  second  metric  is  required  to  specify  them  uniquely.  
More concisely, a second metric is required to describe the magnitudes of the MSV 
and  MTV  relative  to  the  performance  variability  from  equation  (1).    Formulating  a 
purely objective metric has proven frustrating.  In the interim, we can only bound the 
results as follow. 
  Consider the case where the MSV and MTV are much larger than σv.  In this case, 
games  will  tend  to  match  teams  with  performance  differences  much  larger  than σv.  
Thus, upsets – games where the “better” team loses – will be very rare.  By “better,” 
we refer to the true rating.  Consider now the other extreme case where the MSV and 
MTV  are  much  smaller  than σv.    In  this  case,  games  will  often  match  teams  with 
performance differences much smaller than σv, and upsets will consequently be very 
common. 
  Unfortunately, unlike the outcome correlation coefficient, the frequency of upsets 
cannot  be  determined  directly  from  the  game  results  because  we  must  know  the 
ratings  to  determine  which  team  in  a  game  is  “better.”    Even  less  fortunate,  the 

                                                 
18 One might argue that teams would desire an advantage over their opponents, but obviously this 
cannot be achieved on average across the entire population. 

 

22 

estimated ratings from the model itself are insufficient for this purpose since they are 
both uncertain, and they represent a biased19 posterior distribution, not a prior one. 
  One  alternative  metric  that  was  explored  involves  sets  of  three  teams  that  each 
play  the  other  two.    If  upsets  are  frequent,  then  it  would  be  common  to  find  cases 
where each of the three teams goes 1-1.  Infrequent upsets would mean that the teams 
most  often  go  2-0,  1-1,  and  0-2.    Unfortunately,  there  appears  to  be  no  means  to 
demonstrate that teams that play these “round robins” are representative samples.  In 
fact, we suspect that they are specifically unrepresentative of these distributions since 
they  would  often  be  confined  to  conference  play  where  teams  tend  to  be  better 
matched than in nonconference play.  Since we have not yet been able to formulate a 
second  metric  with  the  suitable  properties,  we  must  concede  that  we  have  not  fully 
achieved what we  proposed  at the end of Section 2.  In the meantime, we can  only 
offer  a  “sanity  check”  of  sorts  on  our  results  using  the  frequency  of  upsets  as  our 
metric. 
  Using the MSV (σ'2) and MTV (σ2), we can estimate the frequency of upsets as 
 

U =

+∞∫

−∞

dri

+∞∫

−∞

drj

Φ ri,σ(

(
)  Φ rj,σ'

) 

⎡ 
1
⎢ 
2
⎣ 

1
2

−  

(
− P ri − rj

) 

⎤ 
⎥ 
⎦ 

 

.

(39)

 

 
Calculating this quantity for the 2001-2005 seasons yields the results in Table 3. 
 

Estimated Upset 
Frequency (U) 
21.2% 
21.3% 
20.7% 
21.6% 
22.2% 
*Games through 11/19/05. 

Season 
2001 
2002 
2003 
2004 
2005* 

 
Table 3.  Estimated Frequency of Upsets Obtained from the Model via the MSV and 
MTV for the Years 2001-2005. 
 

                                                 
19 In the sense that the width of the posterior distribution described by the rating estimates alone 
(without consideration of their uncertainty) will be smaller than the width of the corresponding prior 
distribution.  Thus, estimated rating differences will tend to underestimate the corresponding true 
rating differences. 

 

23 

  We next wish to determine some reasonable upper and lower bounds on the upset 
frequency.  Let us assume, for a moment, that the ratings produced by our model are 
good  estimates  of  the  mean  performance  levels  of  all  of  the  teams.    From  the 
indicated values, we can determine the frequency of apparent upsets at various points 
in the season (i.e., Massey’s “ranking violations” score).  For our model, and typical 
of  the  ranking  systems  listed  on  Massey’s  ranking  comparison  page,20  the  ranking 
violations  start  out  very  low  early  in  the  season  and  climb  steadily,  presumably 
towards  some  asymptote  that  we  would  suggest  is  the  “true  [prior]  frequency  of 
upsets.”  Since the ranking violations from our model are fairly representative of the 
ranking  systems  covered  by  Massey,  we  will  use  it  to  depict  this  behavior.    Using 
data from last complete season available as of this writing (2004), Figure 4 shows the 
frequency of ranking violations for our model as a function of the number of games 
considered (consecutively from the start of the season).  The extrapolation shown is 
strictly notional – given the very limited data – and is meant only to demonstrate that 
an  asymptote  around  22%  (per  Table  3)  is  not  unreasonable.    In  any  case,  end-of-
season  ranking  violations  (~16-19%  for  most  ranking  systems  for  Division  I-A) 
provide an estimate of a lower bound on the “true” frequency of upsets. 

?

 
 
 
 
s
n
o
i
t
a
l
o
i
V
 
g
n
i
k
n
a
R

 
f
o
 
y
c
n
e
u
q
e
r
F

25%

20%

15%

10%

5%

0%

0

5000

10000

15000

Number of Games Considered

Figure 4.  Frequency of Ranking Violations from the Model vs. Number of Games 
Considered for the 2004 Season (extrapolation is notional). 

 

                                                 
20 Note that Massey’s website includes statistics only for Divisions I-A (http://www.mratings.com 
/cf/compare.htm) and I-AA (http://www.mratings.com/cf/compare1aa.htm). 

 

24 

  We now seek to estimate some upper bound on the frequency of upsets.  Consider 
for  a  moment  a  hypothetical  perfect  rating  system.    Assuming  that  game-to-game 
performance  variability  is  independent  and therefore  unpredictable  to  this  otherwise 
perfect system, the upset frequency will define the frequency with which this perfect 
system’s  predictions21  are  in  error.    Put  another  way,  the  degree  to  which  rating 
systems can predict game outcomes is limited by the frequency of upsets as driven by 
uncorrelated,  and  unpredictable,  game-to-game  variations  in  performance.    Fair  and 
Oster  (2002)  recently  demonstrated  that  composite  ranking  systems  (combining 
results  from  a  number  of  individual  ranking  systems)  are  capable  of  predicting 
winners in Division I-A games on the order of 72% of the time.  Thus, to the extent 
I-A games are representative of college football as a whole, we would estimate that 
upsets should occur no more than 28% of the time.22 
 
Thus,  these  rough  estimates  of  the  upper  (28%)  and  lower  (16-19%)  bounds  on 
the  upset  frequency  suggest  that  our  estimates  from  the  model  (20-23%)  are 
reasonable.  This, in turn, suggests that the magnitudes of the MSV and MTV relative 
to the performance variability are also reasonable. 
  As  we  close  our  discussion  on  the  analysis  of  the  model,  we  wish  to  revisit  our 
claim that a prior distribution centered on the mean opponent’s rating is superior to 
one  centered  on  zero.    Again,  if  schedules  were  generated  by  randomly  selecting 
opponents  from  all  possible  teams,  then  the  two  methods  should  be  roughly 
indistinguishable.    However,  if  opponents  are  selected  from  a  more  narrow  range, 
then  that  should  be  reflected  in  the  MSV  when  compared  to  the  spread  of  ratings 
across  all  teams.    This  is,  in  fact,  the  case.    Typically,  the  standard  deviation  of 
estimated  ratings  across  all  teams  is  on  the  order  of  1.65  to  1.80  times  the 
corresponding  value  of  σ' .    Note  also,  that  the  posterior  distribution  of  ratings  is 
estimated  to  be  smaller  than  the  prior  by  about  15%  by  the  end  of  the  season,  thus 
amplifying  the  difference  to  about  a  factor  of  two.    Some  questions  still  remain 
concerning  independents  (i.e.,  teams  without  a  conference  affiliation),  and  further 
analysis  will  be  required  to  determine  how  well  they  fit  the  model.    Possible 
modifications might consist of evaluating separate values of the MSV and MTV for 
independents as a group. 
 
                                                 
21 We are neglecting here the fact that some rating systems are explicitly predictive in nature while 
others are retrodictive.  Consider that we are only seeking a rough estimate of an upper bound on the 
frequency of upsets. 
22 Massey’s website includes both Division I-A (http://www.mratings.com/cf/compare.htm) and I-AA 
(http://www.mratings.com/cf/compare1aa.htm) ranking comparison pages.  Typically, ranking 
violations are slightly lower on the I-AA page, suggesting that an upper bound of 28% on the upset 
frequency is still valid when lower divisions are included. 

 

25 

(40) 

(41)

 

 

(42)

6.  HOME FIELD ADVANTAGE 

Up to this point, we have neglected the effects of home field advantage.   While we 
are  still  examining  approaches  to  modeling  its  effects,  we  will  offer  here  what  we 
consider to be our basic approach.23  We begin by modifying equation (6) to include a 
home field advantage term, 
 

∆ ij r( ) = r − ˜ r j + hij  

 
where 
 

hij =

⎧ 
+h,
⎪ 
−h,
⎨ 
⎪ 
0,
⎩ 

if i is at home
if j is at home

 

if at a neutral site

,

 
and h is the mean home field advantage.24 
 
 

To evaluate h, our basic approach is to use the following: 

N h =

∑

all non− neutral
site played games

(
P ˜ r home − ˜ r away + h, σhome

2

2
+ σaway

2
+σv

)

 

 
where Nh is the total number of games where the home team wins.  This expression is 
best solved for h using Newton iteration.  Typically, a single correction is made to h 
for  each  iteration  where  the  team  ratings,  rating  uncertainties,  MSV,  and  MTV  are 
updated.    Note  in  this  equation  that  we  are  currently  attempting  to  account  for  the 
uncertainty  in  the  ratings  of  the  two  teams.    For  reference,  the  values  of  h  for 
2001-2005 averaged 0.244 with a standard deviation of 0.016.25 
  Concerning the prior distribution, it is unclear whether or not it should be adjusted 
for  home  field  advantage  and  equally  unclear  what  form  such  an  adjustment  would 
assume.  Excepting conference play, there is some tendency for well-matched teams 
to schedule pairs of games against each other (at alternating sites year-to-year) while 

                                                 
23 The equations to follow are tentative and is the focus of our current work. 
24 While some teams may be able to exploit home field advantage to a greater or lesser degree, this 
parameter exists only to “level the field” between teams that play more or fewer home games.  
Obviously, a predictive system might attempt to evaluate a home field advantage for each team. 
25 Using data from each season through the games considered in the final BCS ranking (typically 
games ending 3-8 December). 

 

26 

games  between  mismatched  teams  are  often  isolated  encounters  at  the  site  of  the 
superior  team  (which  is  often  better  equipped  to  host  the  larger  crowds  associated 
with its fan base).  In any case, any such correction is expected to have a minor effect 
on the final rating results. 
 
7.  PUBLISHED RANKINGS 

For purposes of our published rankings (http://www.atomicfootball.com), we utilize a 
slightly different ranking metric.  In an ideal world, one might argue that every team 
would play every other team and the team with the highest winning percentage would 
simply be declared the winner.  Since 700+ game schedules are obviously prohibitive, 
we produce instead an equivalent statistic as follows: 
 

(
˜ ˜ r i = P ˜ r i − ˜ r j, σi

2 +σj

2
2 +σv

) all   teams   j≠ i

 

(43)

 

 
where we have assumed a neutral site for all of these “hypothetical” games.  Note that 
we have again attempted to account for the uncertainty in the team rankings (but have 
assumed here that the errors are uncorrelated).  In this form, highly ranked teams will 
be penalized slightly for any factors that increase ranking uncertainty such as playing 
poorly matched opposition or inconsistent play – beating superior teams while losing 
to  inferior  ones.    Otherwise,  this  ranking  metric  produces  only  slightly  different 
results,  occasionally  flip-flopping  two  teams  in  the  rankings.    An  interesting 
mathematical feature of this statistic is that it is independent of the value chosen for 
the  relative  performance  variability.    Note  also  that  it  takes  the  form  of  a  winning 
percentage, falling in the range (0,1). 
 
8.  COMPARISON AND FINAL REMARKS 

While we have downplayed the importance of comparisons with other rating systems, 
it may seem difficult to accept arguments as to the merits of any model without the 
benefit  of  at  least  a  cursory  comparison.    For  this  purpose,  we  will  include  results 
from the model with and without the home field advantage adjustment given above.  
For our reference, we will use the BCS computer top ten rankings (not the composite 
BCS rankings that include the voting polls).  The  results shown  in the tables  below 
are coincident with the final official BCS computer rankings.26 

                                                 
26 There is no BCS ranking, official or otherwise, after completion of the bowl games. 

 

27 

 
Table  4.    Comparison  Between  the  BCS  Computer  Rankings  and  Our  Model  With 
and Without Home Field Advantage for the 2001 and 2002 Seasons. 
 

 
r
e
t
u
p
m
o
C
S
C
B

 

 
g
n
i
k
n
a
R

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

 
r
e
t
u
p
m
o
C
S
C
B

 

 
g
n
i
k
n
a
R

2001 Season 

2002 Season 

 

m
a
e
T

 
t
u
o
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i
F
 
e
m
o
H

 
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i
F
 
e
m
o
H

 

 
r
e
t
u
p
m
o
C
S
C
B

 

 
g
n
i
k
n
a
R

 

m
a
e
T

 
t
u
o
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i
F
 
e
m
o
H

 
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i
F
 
e
m
o
H

Miami FL 
Nebraska 
Colorado 
Oregon 
Florida 
Tennessee 
Texas 
Stanford 
Oklahoma 
Illinois 

1 
2 
4 
3 
8 
5 
7 
6 
11 
10 

1 
4 
3 
2 
10 
5 
7 
6 
11 
9 

Miami FL 
1 
Ohio State 
2 
Georgia 
3 
Southern Cal
4 
Iowa 
5 
Oklahoma 
6 
7  Notre Dame 
8  Washington St
9 
10 

Michigan 
Texas 

2 
1 
4 
3 
7 
8 
6 
5 
9 
11 

1 
2 
4 
3 
8 
7 
6 
5 
10 
9 

2003 Season 

2004 Season 

 

m
a
e
T

 
t
u
o
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i

F
 
e
m
o
H

 
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i

F
 
e
m
o
H

 

 
r
e
t
u
p
m
o
C
S
C
B

 

 
g
n
i
k
n
a
R

 

m
a
e
T

 
t
u
o
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i

F
 
e
m
o
H

 
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i

F
 
e
m
o
H

Oklahoma 
1 
LSU 
2 
Southern Cal 
3 
Michigan 
4 
Ohio St 
5 
Miami OH 
6 
Texas 
7 
8 
Florida St 
9T  Miami FL 
Tennessee 
9T 

1 
2 
3 
5 
4 
8 
7 
6 
10 
11 

1 
2 
3 
6 
5 
4 
8 
7 
9 
11 

Oklahoma 
Southern Cal
Auburn 
Texas 
Utah 
California 
Boise St 
Georgia 

1 
2 
3 
4 
5 
6 
7T 
7T 
9  Virginia Tech
10 

LSU 

2 
1 
3 
4 
5 
6 
7 
9 
10 
11 

2 
1 
3 
4 
5 
6 
7 
11 
9 
12 

 

 
 
 
 
 
 
 
 
 
 

 

 
 
 
 
 
 
 
 
 
 

 
Table  5.    Comparison  Between  the  BCS  Computer  Rankings  and  Our  Model  With 
and Without Home Field Advantage for the 2003 and 2004 Seasons. 
 

 

28 

2005 Season 

 
r
e
t
u
p
m
o
C
S
C
B

 

 
g
n
i
k
n
a
R

 

m
a
e
T

 
t
u
o
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i
F
 
e
m
o
H

 
h
t
i

W

 
l
e
d
o
M

 
d
l
e
i
F
 
e
m
o
H

Texas 
Southern Cal
Penn State 
Ohio State 
Oregon 

1 
2 
3 
4 
5 
6  Virginia Tech
Miami FL 
7 
8 
Georgia 
9  West Virginia

10T
10T Notre Dame 

LSU 

1 
2 
3 
4 
6 
5 
8 
7 
10 
9 
14 

1 
2 
3 
4 
6 
5 
7 
8 
9 
10 
13 

 
Table  6.    Comparison  Between  the  BCS  Computer  Rankings  and  Our  Model  With 
and Without Home Field Advantage for the 2005 Seasons. 
 
  Overall, the matches are very good and appear to steadily improve from year to 
year.27  We also found that our match with the computer average (as measured by the 
sum square of the ranking differences across all five years) is better than the median 
match among the current six BCS computer rankings.  Since the current systems have 
the  advantage  of  contributing  to  the  average  against  which  they  are  compared, 
recalculating  the  computer  average  with  our  model  included  as  one  of  the  systems 
(for all five years but including only the six BCS computer rankings current to 2005), 
our  model  has  the  best  match  by  a  comfortable  margin  (38.4  vs.  66.1)  when  home 
field  advantage  is  disabled  and  the  second  best  match  by  only  a  very  small  margin 
(62.9 vs. 62.6) when home field advantage is enabled. 28  Without going into more in-
depth comparisons, we will simply state that to the extent that comparisons  such as 
these have merit (at the very least as a “sanity check”), we believe the results of our 
model  can  be  declared  “very  reasonable,”  as  often  seems  to  be  the  decree  upon 
favorable comparisons of this kind. 
In closing, we have described a completely self-contained statistical model for the 
 
ranking of college football teams using only win/loss information.  Most importantly, 
                                                 
27 Note that the 2001 season predated the BCS restriction to using only win/loss information. 
28 Not all of the current BCS computer rankings model home field advantage.  

 

29 

we have achieved well-behaved results for undefeated and winless teams through the 
introduction  of  a  Bayesian  prior  distribution  for  which  the  parameters  describing  it 
are calculated in a self-consistent manner.  We have further demonstrated how those 
parameters can be validated against the actual game data.  Finally, even though this 
model has no means by which it can be tuned, it shows remarkable agreement with 
the existing BCS computer rankings. 
 
9.  REFERENCES 

Boginski, Vladimir, Sergiy Butenko, and Panos M. Pardalos, “Matrix-based Methods 
for College Football Rankings,” in Economics, Management and Optimization in 
Sports, pages 1-13.  Springer, 2004. 

 
Bradley, R. A. and M. E. Terry (1952), “Rank Analysis of Incomplete Block Designs, 

I: The Method of Paired Comparisons,” Biometrika 39, 324-345. 

 
Colley, Wes (2002), “Colley’s Bias Free College Football Ranking Method:  The 

Colley Matrix Explained,” unpublished (available at 
http://www.colleyrankings.com). 

 
David, H. A. (1988), The Method of Paired Comparisons, Charles-Griffin & 

Company Ltd., London, second edition. 

 
Fair, Ray and John Oster (2002), "Comparing the Predictive Information Content of 

College Football Rankings," Cowles Foundation Discussion Paper No. 1381; Yale 
ICF Working Paper No. 02-35 (available at http://cowles.econ.yale.edu/P/cd/d13b 
/d1381.pdf). 

 
 
Fainmesser, Itay, Chaim Fershtman, and Neil Gandal (2005), “A Consistent Weighted 
Ranking Scheme with an Application to NCAA College Football” (available at 
http://ideas.repec.org and http://www.cepr.org). 

 
Massey, Kenneth (2001), “Mathematics Invades Competitive Sports,” 
 

presentation at the University of Tennessee, unpublished (available at 
http://www.masseyratings.com/theory/). 

 

 

30 

Mease, David (2003), “A Penalized Maximum Likelihood Approach for the Ranking 
of College Football Teams Independent of Victory Margins,” The American 
Statistician 57, 241-248 (available at http://members.accesstoledo.com 
/measefam/paper.pdf). 

 
 
Mosteller, F. (1951), “Remarks on the Method of Paired Comparisons I:  The Least 
Squares Solution Assuming Equal Standard Deviations and Equal Correlations,” 
Psychometrika 16(3), 9. 

 
Smith, Warren (1994), “Rating Systems for Gameplayers, and Learning” (available at 

http://citeseer.ist.psu.edu). 

 
Thurstone, L. (1927), “A Law of Comparative Judgment,” Psychological Review 34, 

273-286. 

 
10. RELATED WORKS 

Annis, David H. and Bruce A. Craig (2005), “Hybrid Paired Comparison Analysis, 
with Applications to the Ranking of College Football Teams,” Journal of 
Quantitative Analysis in Sports 1(1), Article 3. 

 
Callaghan, Thomas, Peter J. Mucha, and Mason A. Porter, “The Bowl Championship 
Series: A Mathematical Review,” Notices of the AMS, September 2004, 887-893. 

 
Callaghan, Thomas, Peter J. Mucha, and Mason A. Porter, “Random Walker Ranking 
for NCAA Division I-A Football,” to be published in the American Mathematical 
Monthly (preprint available at http://www.arxiv.org). 

 
Frey, Jesse (2005), “A Ranking Methods Based on Minimizing the Number of In-

Sample Errors,” The American Statistician, 59(3), 207-216. 

 
Stern, Hal (1995), “Who’s Number One in College Football?... And How Might we 

Decide?” CHANCE 8, 7-14. 

 
Stern, Hal (2006), “In Favor of a Quantitative Boycott of the Bowl Championship 

Series,” Journal of Quantitative Analysis in Sports 2(1), Article 4. 

 

31 


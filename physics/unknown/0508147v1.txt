Submitted to Pure and Applied Geophysics 

 

 

 

 

 

 

 

 

Systematic Procedural and Sensitivity Analysis of the 

Pattern Informatics Method for Forecasting Large 

(M > 5) Earthquake Events in Southern California 

J. R. Holliday1,2, J. B. Rundle1,2, K. F. Tiampo3, W. Klein4, and A. Donnellan5 

1Center for Computational Science and Engineering, University of California, One Shields Avenue, 

Davis, CA 95616-8677, USA.  

2Department of Physics, University of California, One Shields Avenue, Davis, CA 95616-8677, USA.  

3Department of Earth Sciences, University of Western Ontario, Biology and Geological Sciences Bldg., 

London, Ontario, CANADA N6A 5B7. 

4Department of Physics, Boston University, 590 Commonwealth Avenue, Boston, MA 02215, USA.   

5Earth and Space Sciences Division, Jet Propulsion Laboratory, Mail Stop 183-335, 4800 Oak Grove 

Drive, Pasadena, CA 91109-8099, USA.   

E-mail: holliday@cse.ucdavis.edu, jbrundle@ucdavis.edu, ktiampo@uwo.ca, klein@buphyc.bu.edu, 

donnellan@jpl.nasa.gov

Center for Computational Science and Engineering 

Corresponding author: 

J.R. Holliday 

University of California 

One Shields Avenue 

Davis, CA 95616-8677 

USA 

E-mail: holliday@cse.ucdavis.edu 

Tel: +1-530-752-6419 

Fax: +1-530-754-4885 

 

 

 

 

Abbreviated title:  Analysis of Pattern Informatics Model 

Keywords:  Pattern Informatics, earthquake forecasting

Abstract 

Recent studies in the literature have introduced a new approach to earthquake 

forecasting based on representing the space-time patterns of localized seismicity by a 

time-dependent system state vector in a real-valued Hilbert space and deducing 

information about future space-time fluctuations from the phase angle of the state 

vector. While the success rate of this Pattern Informatics (PI) method has been 

encouraging, the method is still in its infancy. Procedural analysis, statistical testing, 

parameter sensitivity investigation and optimization all still need to be performed. In 

this paper, we attempt to optimize the PI approach by developing quantitative values 

for “predictive goodness” and analyzing possible variations in the proposed 

procedure. In addition, we attempt to quantify the systematic dependence on the 

quality of the input catalog of historic data and develop methods for combining 

catalogs from regions of different seismic rates. 

1. Introduction 

Large magnitude earthquakes are devastating events which can have great social, 

scientific, and economic impact. The 26 December 2003 magnitude 6.7 Iran 

earthquake killed nearly 30,000 persons. The 16 January 1995 Japan magnitude 6.9 

earthquake produced an estimated $200 billion loss. Similar scenarios are possible at 

any time in San Francisco, Seattle, and other U.S. urban centers along the Pacific 

plate boundary, especially in Southern California. The gravity of potential loss of life 

and property is so great that reliable earthquake forecasting should be at the forefront 

of research goals.  

 

 

While millions of dollars and thousands of work years have been spent on 

observational programs searching for reliable precursory phenomena, to date few 

successes have been reported and no precursors to large earthquake events have been 

detected that provide reliable forecasts. Indeed, many wonder if earthquake 

forecasting is even possible (see, for example, the online debate hosted at 

http://www.nature.com/nature/debates/earthquake).  

A new approach to earthquake forecasting, the pattern informatics (PI) approach, has 

been proposed by Rundle et al. (2000a, 2000b, 2002, 2003) and Tiampo et al. (2002a, 

2002b, 2002c). This approach is based on the strong space-time correlations that are 

responsible for the cooperative behavior of driven threshold systems and arise both 

from threshold dynamics as well as from the mean field (long range) nature of the 

interactions.  

Using both simulations and observed earthquake data, they have shown that the 

space-time patterns of threshold events (earthquakes) can be represented by a time-

dependent system state vector in a Hilbert space. The length of the state vector 

represents the average temporal frequency of events throughout the region and is 

closely related to the rate at which stress is dissipated. It can be deduced that the 

information about space-time fluctuations in the system state is represented solely by 

the phase angle of the state vector. Changes in the norm of the state vector represent 

only random fluctuations and can for the most part be removed by requiring the 

system state vector to have a constant norm. A more detailed summary of the method 

is given in section 4. 

 

 

 

2. Background 

Earthquake fault systems are now believed to be a complex example of a highly 

nonlinear system (Bak and Tang, 1989; Rundle and Klein, 1995). Interactions among 

a spatial network of fault segments are mediated by means of a potential that allows 

stresses to be redistributed to other segments following slip on any particular segment. 

For faults embedded in a linear elastic host, this potential is a stress Green’s function 

whose exact form can be calculated from the equations of linear elasticity, once the 

current geometry of the fault system is specified. A persistent driving force, arising 

from plate tectonic motions, increases stress on the fault segments. Once the stresses 

reach a threshold characterizing the limit of stability of the fault, a sudden slip event 

results. The slipping segment can also trigger slip at other locations on the fault 

surface whose stress levels are near the failure threshold as the event begins. In this 

manner, earthquakes occur that result from the interactions and nonlinear nature of the 

stress thresholds.  

 

The Karhunen-Loeve method (Fukunaga, 1970; Holmes et al., 1996), a linear 

decomposition technique in which a dynamical system is decomposed into a complete 

set of orthonormal subspaces, has been applied to a number of other complex 

nonlinear systems over the last fifty years, including the ocean-atmosphere interface, 

turbulence, meteorology, biometrics, statistics, and even solid earth geophysics 

(Hotelling, 1993; Fukunaga, 1970; Aubrey and Emery, 1983; Preisendorfer, 1988; 

Savage, 1988; Penland, 1989; Vautard and Ghil, 1989; Garcia and Penland, 1991; 

Penland and Magorian, 1993; Penland and Sardeshmukh, 1995; Holmes et al., 1996; 

Moghaddam et al., 1998).  The notable success of this method in analyzing the ocean-

atmosphere interface and such features as the El Niño Southern Oscillation (ENSO), a 

nonlinear system whose underlying physics is governed by the Navier-Stokes 

equation, suggested its application to the analysis of the earthquake fault system 

(North, 1984; Preisendorfer, 1988; Penland and Magorian, 1993; Penland and 

Sardeshmukh, 1995).  Building on these methods for analyzing nonlinear threshold 

systems, space-time seismicity patterns can be identified in both observed phenomena 

and numerical simulations using realistic earthquake models for southern California 

(Bufe and Varnes, 1993; Bowman et al., 1998; Gross and Rundle, 1998; Brehm and 

Braile, 1999; Jaume and Sykes, 1999; Tiampo et al., 1999, 2000; Rundle et al., 2000b. 

The PI method is an adaptation of the Karhunen-Loeve expansion technique to the 

analysis of observed seismicity data from southern California in order to identify basis 

patterns for all possible space-time seismicity configurations. These basis states 

represent a complete, orthonormal set of eigenvectors and associated eigenvalues, 

obtained from the diagonalization of the correlation operators computed for the 

regional historic seismicity data, and, as such, can be used to reconstitute the data for 

various subset time periods of the entire data set.  

 

 

3. Data 

The primary data set employed in this analysis is the entire historic seismic catalog 

from 1 January 1932 through 31 December 1999, obtained from the Southern 

California Earthquake Data Center (SCEDC) online searchable database1, with all 

non-local and blast events specifically removed. The relevant data consists of 

location, in East longitude and North latitude, and the date the event occurred. 

Seismic events between -122o and -115o longitude and between 32o and 37o latitude 

(any depth and quality) and with magnitude greater than or equal to Mmin = 3.0 were 

                                                 
1http://www.data.scec.org/catalog_search/index.html 

selected.  

 

 

 

While the SCEDC catalog is among the best available, both in completeness and 

historic depth, there are a number of known deficiencies2 that undoubtedly affect the 

quality of our constructed forecast hot-spot maps. The most notable of these issues is 

that the four-year span of data from 1977-1980 is currently not available to web 

searching. Fortunately, data for these missing years is available from the older 

Southern California Seismic Network (SCSN) archives3 and was hand inserted for 

this analysis. Unless otherwise indicated, all analysis was performed using SCEDC 

data with the additional SCSN data.  

A second source of data employed in this analysis was acquired from the Northern 

California Earthquake Data Center (NCEDC) online searchable database4, with all 

non-local and blast events again specifically removed. When incorporating this 

catalog, seismic events between -122o and -115o longitude and between 35o and 37o 

latitude (any depth and quality) and with magnitude greater than or equal to Mmin = 

3.0 were selected. The necessity for utilizing an additional catalog in some of our 

analysis arises from various earthquake events in the vicinity of 35o North latitude 

missing from the SCEDC catalog but present in the NCEDC collection.  

4. Basic Method 

Here we summarize the current PI method as described by Rundle et al. (2003) and 

Tiampo et al. (2002c). The PI approach is a six step process that creates a time-

                                                 
2http://www.data.scec.org/catalog_search/known_issues.html 
3http://www.data.scec.org/ftp/catalogs/SCSN/ 
4http://quake.geo.berkeley.edu/ncedc/catalog-search.html 

dependent system state vector in a real valued Hilbert space and uses the phase angle 

to predict future states (Rundle et al., 2003).  The method is based on the idea that the 

future time evolution of seismicity can be described by pure phase dynamics (Mori 

and Kuramoto, 1998; Rundle et al., 2000a,  2000b).  Hence, a real-valued seismic 

phase function  S xi ,t b , t

 is constructed and allowed to rotate in its Hilbert space. 

Since seismicity in active regions is a noisy function (Kanamori, 1981), only temporal 

averages of seismic activity are utilized in the method. The geographic area of interest 

is partitioned into N square bins centered on a point xi and with an edge length dx 

determined by the nature of the physical system. For our analysis we chose dx = 0.1o 

~ 11km, corresponding to the linear size of a magnitude M ~ 6 earthquake. Within 

each box, a time series 

obs xi ,t

 is defined by counting how many earthquakes with 

magnitude greater than Mmin occurred during the time period t to t + dt. Next, the 

activity rate function  S xi ,t b ,T  is defined as the average rate of occurrence of 

earthquakes in box i over the period tb to T:  

(
=T,t,xS
b

)

i

(cid:1)

)

(
t,x(cid:1)
i
tT
-

b

. 

If tb is held to be a fixed time,  S xi ,t b ,T  can be interpreted as the ith component of 

a general, time-dependent vector evolving in an N-dimensional space (Tiampo et al., 

2002c). Furthermore, it can be shown that this N-dimensional correlation space is 

defined by the eigenvectors of an NxN correlation matrix (Rundle et al., 2000a, 

2000b). The activity rate function is then normalized by subtracting the spatial mean 

over all boxes and scaling to give a unit-norm:  

ˆ
(
=T,t,xS
b

)

i

(
T,t,xS
i

b

)

-

(cid:1)

(
T,t,xS
j

b

)

(cid:1)

(
T,t,xS
j

b

)

-

(cid:1)

(
T,t,xS
k

b

(cid:7)
(cid:5)(cid:6)

. 

2

(cid:4)
)
(cid:2)(cid:3)

1
N

1
N

 

 

(1) 

(2) 

The requirement that the rate functions have a constant norm helps remove random 

fluctuations from the system. Following the assumption of pure phase dynamics 

(Rundle et al., 2000a, 2000b), the important changes in seismicity will be given by the 

change in the normalized activity rate function for the time period t1 to t2:  

S xi ,t b , t 1 ,t2 S xi ,t b , t 2 S xi ,t b ,t 1 . 

(3) 

 

 

 

 

 

 

This is simply a pure rotation of the N-dimensional unit vector  S xi ,t b ,T  through 

time. In order to remove the last free parameter in the system, the choice of base year, 

and to further reduce random noise components, changes in the normalized activity 

rate function are averaged over all possible base-time periods:  

S xi , t 0 ,t 1 , t2

t1

tb

t 0

S xi ,t b , t 1 ,t 2

. 

t

1

t

0

Finally, the probability of change of activity in a given box is deduced from the 

square of its base averaged, mean normalized change in activity rate:  

P xi ,t 0 ,t1 ,t 2

S xi ,t b , t 1 ,t 2

2

. 

In phase dynamical systems, probabilities are related to the square of the associated 

vector phase function (Mori and Kuramoto, 1998; Rundle et al., 2000b). This 

probability function is often given relative to the background by subtracting off its 

spatial mean: 

background.  

)
t,t,t,xP=t,t,t,xP'

(

(

0

1

2

1

0

i

i

2

)

1
(cid:1)-
N

(
t,t,t,xP
0

1

j

)2

, 

where P' indicates the probability of change in activity and is measured relative to the 

Schematically, this whole process can be represented by  

N S S

S

S P , 

where the hat symbol is understood to mean “calculate normalization in space”, the 

(4) 

(5) 

(6) 

 

 

capital Delta means “calculate the change in rate”, and the underscore symbol means 

“average over base times”. Note that this method implicitly assumes earthquake fault 

systems are in an unstable equilibrium state and can be treated linearly about their 

equilibrium points.  

4.1. Variations in Order 

To determine the optimal application of the PI method, we identified and analyzed all 

physically meaningful variations of the described procedure. While we have outlined 

above a six step process, there are considerably fewer than 6! = 720 variations that 

need to be investigated. A forecast analysis must always begin with binning the 

available data and end with a calculation of probability change. Also, base-time 

averaging and calculation of changes in the activity rate functions can only be 

performed after creating the activity rate vectors. With these constraints imposed, 

there are only eight possible variations in the order to which each step is performed. 

Table 1 lists these eight variations with the original method denoted Method I.  

On the basis of theoretical arguments and assumptions of linearity within the system, 

we expect that Methods I through VI should perform qualitatively similar to each 

other. This is due largely to the fact that the operations being permuted are all linear 

and commute with each other.  Qualitatively it is unclear which variation should yield 

the best correlation with actual future events other than to expect Methods II and III 

might perform better than Method I due to the movement of when the change in 

activity rate is calculated to after the normalization and base-time averaging steps. 

This essentially places all of the activity rate vectors on equal footing and legitimizes 

the vector rotation. We also expect that Methods VII and VIII will yield both 

qualitatively and quantitatively inferior forecast hot-spot maps. This is due to the 

direct normalization of the binned data. Such a step destroys correlations between 

different spatial locations by independently scaling the relative historic intensity rates.  

Each of these expectations are verified in the results section below. 

 

 

 

 

4.2. Variations in Binning 

In addition to the original binning method, we also analyzed time-centered, 

cumulative, and detrended binning.  For time-centered binning, we took each time 

series and removed the temporal mean: 

For cumulative binning we allowed each time series to build on its past events: 

obs xi ,t

obs xi ,t

t 2

t

t0

obs xi ,t

. 

t

2

t

0

obs xi ,t

t

T t 0

obs xi ,T . 

For detrended binning, we took each cumulative time series, fit it to a first order 

polynomial, and subtracted the fitted line: 

obs xi ,t

obs xi ,T

A Bt

, 

t

T t 0

where A and B are the parameters of the regression fit.  Figure 1 shows the effect of 

each binning procedure on a synthetic data sample.  We will denote the four different 

binning methods with the labels A, B, C, and D, respectively, with A denoting the 

unmodified method. Methods B and D are significant in that they remove the mean 

for each time series from the data. Thus, anomalous activity away from background 

seismicity is expected to be emphasized. Method C is reminiscent of an unbiased 

estimator in the cumulative distribution Kolmogorov-Smirnov Test (Press et al., 

2002) and could in theory allow more accurate comparisons among the different time 

(7) 

(8) 

(9) 

series.  

 

 

 

2. 

We also investigated magnitude- and energy-weighted binning where the value at 

each time step is proportional to either the total magnitude Mtot of all the events in the 

time period or to the total energy (~10Mtot ) of all the events. These weighting factors, 

however, had the effect of selecting out time periods surrounding only the largest 

events and were thus unsuitable for the analysis. We did not investigate Boolean 

binning where each time step is given an initial value of either 1 if one or more events 

occur in that time period or 0 otherwise due to the realization that this effect can be 

achieved by sufficiently reducing the time step dt. Also, we desired the method to 

scale appropriately as dt is increased.  

4.3. Variations in Projection 

In addition to calculating the change in the activity rate function through the vector 

rotation during the time period t1 to t2, we also investigated the effect of linear 

projection of change into future times: 

S xi ,t b ,t 1 , t2

S xi ,t b ,t 2

S xi ,t b ,t 1 , t 2 . 

(10) 

The motivation behind this investigation was that for regions with a near constant rate 

of seismicity (or with frequencies higher than an inverse time step), 

ˆ

»t,t,t,xS(cid:2)
b

) 0

(

1

2

i

.  By linear projection, we mean that the future seismic activity for 

this type of situation would be approximately equal to the present seismic activity 

with a small correction added. For notational purposes, we will denote the unmodified 

approach of calculating the change in the activity rate function with the label 1 after 

the method specification. We will denote the linear projection approach with the label 

 

 

 

4.4. Variations in dt 

While the spatial width of the boxes, dx, is determined by the nature of the physical 

system, the temporal binning width dt is arbitrary.  Larger values of dt result in 

greater bin statistics and faster execution time of the algorithm while lower values 

may potentially yield greater sensitivity to high frequency periodicity.  

To investigate the effect, we performed the analysis with representative values for dt 

ranging from one day to one year. If the catalog is uniform in its completeness and not 

missing bands of data at quasi- periodic intervals, we would expect to find a smooth 

transition through the varying choices of dt with perhaps some optimal selection. On 

the other hand, large fluctuations in the forecast as dt is slowly modified may indicate 

underlying chaotic phenomena and would bring into question the assumptions and 

treatment of linearity within the system.  

5. Statistical Tests 

To test the hypothesis that the probability measure Pi can forecast future (t > t2) large 

(M > 5) events, we performed a set of maximum likelihood tests [Bevington and 

Robinson, 1992; Gross and Rundle, 1998; Kagan and Jackson, 2000; Tiampo et al., 

2002b; Schorlemmer et al., 2003].  The likelihood L is a probability measure that can 

be used to assess the quality of one forecast measure over another. Typically, one 

computes L = log(L) for the proposed forecast measure L and compares that to the 

likelihood measure L0 = log(L0) for a representative null hypothesis. The ratio of 

these two values then yields information about which measure is more accurate in 

forecasting future events. In the likelihood ratio test, a probability density function 

(PDF) is required. Two different PDFs were used in this analysis: a global, Gaussian 

model and a local, Poissonian model.  These distributions differ significantly in that 

the Gaussian model assumes purely random, normal statistics while the Poissonian 

model assumes independent statistics over small time intervals with no temporal 

clustering [Walpole and Myers, 1993]. 

5.1. Global Gaussian Model 

In their original analysis, Tiampo et al. (2002b) calculated likelihood values by 

defining Pi = P[xi] to be the union of a set of N Gaussian density functions pG(|x-xi|) 

(Bevington and Robinson, 1992) centered at each location xi. Each individual 

Gaussian density has a standard deviation equal to the box width dx and a peak value 

equal to the calculated probability of change in activity Pi divided by the standard 

deviation squared. P[x(ej)] is therefore a probability measure that a future large event 

ej occurs at location x(ej): 

If there are J future events, the normalized likelihood L that all J events are forecast 

P x ej

x ej

2

xi

2

. 

Pi

i

2

e

L

P e x j
P xi

i

j

. 

log L

j log

P e x j
P xi

i

. 

(11) 

(12) 

(13) 

Furthermore, the log-likelihood value L for a given calculation can be calculated and 

used in ratio comparison tests: 

 

 

 

 

is: 

 

 

 

 

Before performing the statistical analysis, the change in activity values Pi were first 

truncated by scaling all the probabilities equally up-wards and performing a 

histogram cut to enforce the restriction P 1 . This was used to eliminate the 

exponential tail on the high end of the PDF and ensure that events that occurred 

during the forecasting time period had a probability  P 1  of occurring (which, in 

fact, they did).  

5.2. Local Poissonian Model 

The second model used is based on work performed by the Regional Earthquake 

Likelihood Models (RELM) group (Schorlemmer et al., 2003). For each bin i an 

expectation value 

i  is calculated by scaling the local probability Pi by the number of 

earthquakes that occurred over all space during the forecast time period: 

i n Pi , 

(14) 

where n is the number of post-t2 events. Note that for any future time interval (t2, t3), n 

could in principle be estimated by using the Gutenberg-Richter relation. For each bin 

an observation value wi is also calculated such that wi contains the number of post-t2 

earthquakes that actually occurred in bin i.  For the RELM model, it is assumed that 

earthquakes are independent of each other. Thus, the probability of observing wi 

events in bin i with expectation 

i  is the Poissonian probability 

pi wi ,

i

e i . 

wi
i

wi !

(15) 

The log-likelihood for observing w earthquakes at a given expectation 

is defined as 

the logarithm of the probability pi wi ,

i

, thus 

log L w ,

log p w,

wlog

log w!

. 

(16) 

Since the joint probability is the product of the individual bin probabilities, the log-

likelihood value for a given calculation is the sum of  log L w,

over all bins i. 

When using this PDF function, we preprocess the change in activity values Pi by 

performing the same histogram cut as with the Gaussian model.  

6. Results 

Results for the procedural analysis with variations in binning and calculation of 

activity rate are presented in tables 2 and 3. All values of L are given relative to L0 

defined to be the value supplied by our original, unaltered Method I-A1. Since these 

are ratio tests, greater values indicate better predictive ability. 

As statistical evaluations of earthquake forecasts are still under development, it is 

instructive to weigh the quantitative (“predictive goodness” values) against the 

qualitative (pictorial representation of the forecast hot-spot maps). Thus, 

representative maps for each procedural variation are given in figures 2 and 3.  

Only Methods II and III, using normal binning and change of activity calculation, 

performed better than the original method under the two statistical tests. Naively, this 

result is expected as both methods wait until after normalization and base year 

averaging to calculate the change in activity rate, thus giving the calculations in each 

box equal statistical weight. For all other investigated variations, no method 

performed better on both likelihood tests and qualitative analysis.  

 

 

 

 

 

 

While a few of the binning and change of activity variations fared well on one or the 

other likelihood tests (for example, III-B1), most performed poorly qualitatively. 

Probability calculations gave predictions of activity that spread well into areas with no 

recorded activity. These results can be understood by considering their mathematical 

operations.  By linearly projecting the change in activity rate, heavy weight is placed 

on the most recent seismic history.  For the procedure to identify anomalous changes 

in the seismicity, however, the entire history must be considered equally.  Also, the 

cumulative and detrended variations in the binning method create time series that are 

significantly altered from those apparent in nature. 

While only Methods II-A1 and III-A1 performed better than the original PI procedure 

on both statistical tests, it should be stressed that at this time none of the methods can 

be claimed to be superior. There is still a subjective element over which forecast hot-

spot map to prefer. Based on theoretical and mathematical considerations, Method 

III-A1 is the authors’ preferred choice.  This method creates a unique state vector at 

every time step and allows the purest interpretation of a vector rotation. 

 

 

Table 4 shows the results of varying the time step in the analysis (note that Method 

III-A1 was used). Likelihood values for this investigation were referenced against a 

choice of dt = 1 day.  Note that the accuracy of the calculated forecast decreases with 

increasing time step, slowly decreasing up to around dt = 1 week and then rapidly 

decreasing. While larger choices of dt decrease time of computation for the PI 

algorithm, they do so at the cost of accuracy.  Evaluating the data from Table 4, along 

with the corresponding forecast hot-spot maps, the authors believe dt = 7 days to be a 

suitable compromise.  This choice of time step is low enough to probe the seismic 

periodicity at all scales with reasonable accuracy while being large enough to 

 

 

 

significantly speed up the computation. 

7. Catalog Sensitivity 

To gauge the sensitivity of the PI method on the quality of the input catalog, we 

decimated the available data by systematically increasing both the starting date of 

catalog information (and thus affecting t0) and the minimum magnitude threshold. 

Figures 4 and 5 show the effect on the relative likelihood values of varying either 

parameter individually. Both probability density functions–Poissonian and Gaussian–

were used to calculate log likelihood indexes.  

In Figure 4 we see the surprising result that the forecast is relatively stable as t0 is 

increased, up to around 1965. This would indicate that accurate forecast hot-spot 

maps can be created using only approximately 40 years of historic data. When the 

normalized activity rate functions are averaged over all possible base-time periods, 

more recent data gets weighted heavier than more historic data. The threshold for 

when historic data no longer influences the forecast appears to be approximately 40 

years before the onset of the forecast, i.e., t2. With less than 40 years of historic data, 

however, the likelihood values drop sharply.  

The Poissonian analysis in Figure 5 seems to indicate that higher accuracy in the 

forecast can be obtained by raising the minimum magnitude cut-off threshold of the 

analysis from Mmin = 3.0 to ~3.7. This may have the effect of removing low 

magnitude events that are uncorrelated with future large magnitude events and 

thereby eliminate background noise from the analysis. Care must be taken, however, 

as the likelihood values drop quickly as the magnitude threshold is raised too high. It 

is interesting to note the sudden drop in likelihood values as the magnitude threshold 

reaches 4.5 (and again near 4.8, 5.5, and 6.3). While statistics may be playing a role in 

the latter three drops, the discontinuity at Mmin = 4.5 appears to identify an unknown 

deficiency in the catalog. 

Figures 6 and 7 show the effect on the relative likelihood values of varying both 

parameters simultaneously. For these two-dimensional plots, warmer colors indicate 

better correlation between the forecast and actual events. All of the features 

mentioned above are again evident as well as the surprising observation that 

increasing Mmin allows accurate forecasts with less historic data (as indicated by the 

positive slope of the high-likelihood-edge surrounding Mmin = 3.6 and t0 = 1967).  

8. Application Of The Method 

To test the our optimization on the PI method, we recreated the forecast seismic hot-

spot map originally presented by Rundle et al. (2002) for the time period 1 January 

2000 to 31 December 2009 using Method III-A1. The result is shown in Figure 8. 

The original forecast was made using only data from the SCEDC catalog, which does 

not contain earthquakes from the San Simeon region (location of the M=6.5, 2003 

event; label #7 in Figure 8). Our revised forecast was made using data from both the 

NCEDC catalog (for latitude above 35o) and the SCEDC catalog (for latitude below 

 

 

35o). 

 

Since the cut-off date for the forecast of 31 December 1999, eight large earthquake 

events with M>5 have occurred in central or southern California. The first seven 

events all occurred either on areas of forecasted anomalous activity or within the 

margin of error of +/- 11km. While this hot-spot map was made after each of these 

events occurred, it was done so using only data prior to 31 December 1999 and could 

have in principle predicted these events. Scorecards using the original method and the 

current optimized method can be found at the JPL QuakeSim website5.  

 

 

 

9. Combining Catalogs 

The issue of how to combine historic catalogs in order to create forecast hot-spot 

maps for large regions is a difficult one. Problems arise from the fact that different 

areas will normally have widely different seismic rates, and these differences get 

smoothed out when we normalize our state vectors.  

One way to try and account for these differences is to apply a weighting factor to the 

different catalogs as they are merged into an aggregate catalog. This method, 

however, tends to emphasize near threshold-level anomalous activity in the catalog 

with the highest weighted activity rate. In Figure 9 we created a forecast hot-spot map 

by combining data from the NCEDC and SCEDC catalogs with two different 

weighting ratios. With equal weighting between the two catalogs (Figure 9A), event 

#3 (Anza) occurs near a threshold-level anomalous region. Event #7 (San Simeon), 

however, is missed completely. As the relative weighting for the northern catalog is 

increased to account for its lower total seismic rate (Figure 9B), anomalous activity 

begins to appear under event #7, but disappears from event #3.  

Another way to try and account for the differences is to apply a weighting factor to 

each individual time series based on its own statistics. This method, unfortunately, 

                                                 
5http://www-aig.jpl.nasa.gov/public/dus/quakesim/scorecard.html 

also has failings. By weighing each time series individually, correlations between 

local events are destroyed. In practice, this approach has effects similar to the earlier 

proposed modifications VII and VIII to the PI procedure and simply results in more 

apparent noise in the forecast and less correlation with actual future events.  

Currently, the best approach (at least for this time period and these catalogs) appears 

to be to treat all catalogs and regions separately, combining only at the end of the 

analysis and normalizing over all spatial bins to allow for correlations across the 

 

 

catalog seams.  

10. Conclusion 

We have analyzed the current PI procedure and developed a more optimized approach 

for creating accurate forecast hot-spot maps. First, historic seismic data is binned by 

counting the number of earthquakes per unit time, of any size greater than or equal to 

Mmin, within a geographic box centered at xi at some time t. The geographic region 

defined by dx is taken large enough so that seismic activity can be considered an 

incoherent superposition of phase functions.  Second, an activity rate function is 

defined as the average rate of occurrence of earthquakes in box i over the period tb to 

T. Third, the activity rate function is averaged over all possible base-time periods. 

Forth, the base-year averaged activity rate function is normalized by subtracting the 

spatial mean over all boxes and scaling to give a unit-norm. Fifth, changes in the 

base-year averaged, mean-normalized activity rate function are calculated by allowing 

the vector to rotate over time. Finally, the probability of change of activity in a given 

box–calculated relative to the background–is deduced from the square of its base-year 

averaged, mean-normalized change in activity rate. 

We also showed that the choice of dt is relatively unimportant to the calculation if it is 

taken low enough, that only approximately 40 years of complete historic data is 

necessary for accurate forecasts, and that the assumptions of linearity and near-

equilibrium appear valid for Southern California seismic fault systems. Applying our 

new procedure, we recalculated and updated the southern California forecast hot-spot 

map presented by Rundle et al. (2002) and showed that the 22 December 2003 San 

Simeon event could have been foreseen. Finally, we identified pitfalls associated with 

combining seismic catalogs from different regions in an attempt to create a composite 

forecast hot-spot map. 

There is movement in the forecast verification community to part with likelihood 

calculations, which lightly reward successes and heavily penalize failures, and 

embrace ROC verification diagrams (Joliffee and Stephenson, 2003).  Additional 

analyses that utilize these verification techniques are currently underway. 

Acknowledgments 
The authors are grateful to the anonymous reviewers for their helpful criticisms and 

suggestions.  This work has been supported by a grant from US Department of 

Energy, Office of Basic Energy Sciences to the University of California, Davis DE-

FG03-95ER14499 (JRH and JBR), by a NASA Earth Science Fellowship NN-

6046Q98H (JRH), and through additional funding from the National Aeronautics and 

Space Administration under grants through the Jet Propulsion Laboratory to the 

University of California, Davis.   

 

 

 

 

References 
Aubrey, D. G., and K. O. Emery (1983), Eigenanalysis of recent united states sea 

levels, Continental Shelf Res., 2, 21–33.  

Bak, P., and C. Tang (1989), Earthquakes as self-organized critical phenomena, J. 

Geophys. Res., 94, 15,635–15,637.  

Bevington, P. R., and D. K. Robinson (1992), Data Reduction and Error Analysis for 

the Physical Sciences, McGraw-Hill.  

Bowman, D. D., G. Ouillon, C. G. Sammis, A. Sornette, and D. Sornette (1998), An 

observational test of the critical earthquake concept, J. Geophys. Res., 103, 24,359–

24,372.  

Brehm, D. J., and L. W. Braile (1999), Intermediate-term earthquake prediction using 

the modified time-to-failure method in southern California, BSSA, 89, 275–293.  

Bufe, C. G., and D. J. Varnes (1993), Predictive modeling of the seismic cycle of the 

greater San Francisco bay region, J. Geophys. Res., 98, 98719883.  

Fukunaga, K. (1970), Introduction to Statistical Pattern Recognition, Academic 

Press, New York.  

Garcia, A., and C. Penland (1991), Fluctuating hydrodynamics and principal 

oscillation pattern analysis, J. Stat. Phys., 64, 1121–1132.  

 

 

 

 

 

 

 

 

Gross, S., and J. B. Rundle (1998), A systematic test of time-to-failure analysis, 

Geophys. J. Int., 133, 57–64.  

Holmes, P., J. L. Lumley, and G. Berkooz (1996), Turbulence, Coherent Structures, 

Dynamical Systems and Symmetry, Cambridge University Press, Cambridge, U.K.  

Hotelling, H. (1993), Analysis of a complex of statistical variables into principal 

components, J. Educ. Psych., 24, 417–520.  

Jaumé, S. C., and L. R. Sykes (1999), Evolving towards a critical point: A review of 

accelerating seismic moment/energy release prior to large and great earthquakes, Pure 

Appl. Geophys., 155, 279–306. 

Joliffee, I. T. and Stephenson, D. B. (2003), Forecast Verification, John Wiley. 

Kagan, Y. Y., and D. D. Jackson (2000), Probabilistic forecasting of earthquakes, 

Geophys. J. Int., 143, 438–453.  

Kanamori, H. (1981), The nature of seismicity patterns before large earthquakes, in 

Earthquake Prediction: An International Review, Geophys. Monogr. Ser., pp. 1–19, 

AGU, Washington, D. C.  

Moghaddam, B., W. Wahid, and A. Pentland (1998), Beyond eigenfaces: Probabilistic 

matching for face recognition, in Third IEEE Intl. Conf. on Automatic Face and 

Gesture Recognition, pp. 1–6.  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Mori, H., and Y. Kuramoto (1998), Dissipative Structures and Chaos, Springer-

Verlag, Berlin.  

41(5), 879887.  

North, G. R. (1984), Empirical orthogonal functions and normal modes, J. Atm. Sci., 

Penland, C. (1989), Random forcing and forecasting using principal oscillation 

pattern analysis, Monthly Weather Rev., 117, 21652185.  

Penland, C., and T. Magorian (1993), Prediction of Niño 3 sea surface temperatures 

using linear inverse modeling, J. Climate, 6, 1067–1076.  

Penland, C., and P. D. Sardeshmukh (1995), The optimal growth of tropical sea 

surface temperature anomalies, J. Climate, 8, 1999–2024.  

Preisendorfer, R. W. (1988), Principle Component Analysis in Meteorology and 

Oceanography, Elsevier, Amsterdam.  

Press, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery (2002), 

Numerical Recipes in C, Cambridge University Press, Cambridge, MA.  

Rundle, J. B., and W. Klein (1995), New ideas about the physics of earthquakes, Rev. 

Geophys. Space Phys. Suppl. (July), 283, 283–286.  

Rundle, J. B., W. Klein, S. J. Gross, and K. F. Tiampo (2000a), Dynamics of 

seismicity patterns in systems of earthquake faults, in Geocomplexity and the Physics 

of Earthquakes, Geophys. Monogr. Ser., vol. 120, edited by J. B. Rundle, D. L. 

Turcotte, and W. Klein, pp. 127–146, AGU, Washington, D. C.  

Rundle, J. B., W. Klein, K. F. Tiampo, and S. J. Gross (2000b), Linear pattern 

dynamics in nonlinear threshold systems, Phys. Rev. E., 61, 2418–2432.  

Rundle, J. B., K. F. Tiampo, W. Klein, and J. S. S. Martins (2002), Self-organization 

in leaky threshold systems: The influence of near-mean field dynamics and its 

implications for earthquakes, neurobiology, and forecasting, Proc. Natl. Acad. Sci. 

U. S. A., 99, 2514–2521: Suppl. 1.  

Rundle, J. B., D. L. Turcotte, R. Shcherbakov, W. Klein, and C. Sammis (2003), 

Statistical physics approach to understanding the multiscale dynamics of earthquake 

fault systems, Rev. Geophys., 41(4), 1019, doi:10.1029/2003RG000135.  

Savage, J. C. (1988), Principal component analysis of geodetically measured 

deformation in long valley caldera, eastern California, 19831987, J. Geophys. Res., 

93, 13,297–13,305.  

Schorlemmer, D., D. D. Jackson, and M. Gerstenberger (2003), Earthquake likelihood 

model testing, http://moho.ess.ucla.edu/~kagan/sjg.pdf.  

Tiampo, K. F., J. B. Rundle, W. Klein, and S. J. Gross (1999), Systematic evolution 

of nonlocal space-time earthquake patterns in southern California, EOS Trans. AGU, 

80, 1013.  

 

 

 

 

 

 

 

Tiampo, K. F., J. B. Rundle, S. McGinnis, S. J. Gross, and W. Klein (2000), 

Observation of systematic variations in non-local seismicity patterns from southern 

California, in Geocomplexity and the Physics of Earthquakes, Geophys. Monogr. Ser., 

vol. 120, edited by J. B. Rundle, D. L. Turcotte, and W. Klein, pp. 211–218, AGU, 

Washington, D. C.  

 

 

 

 

 

Tiampo, K. F., J. B. Rundle, S. McGinnis, S. J. Gross, and W. Klein (2002a), 

Eigenpatterns in southern California seismicity, J. Geophys. Res., 107(B12), 2354, 

doi:10.1029/2001JB000562.  

Tiampo, K. F., J. B. Rundle, S. McGinnis, S. J. Gross, and W. Klein (2002b), Mean 

field threshold systems and earthquakes: An application to earthquake fault systems, 

Europhys. Lett., 60(3), 481–487.  

Tiampo, K. F., J. B. Rundle, S. McGinnis, and W. Klein (2002c), Pattern dynamics 

and forecast methods in seismically active regions, Pure App. Geophys, 159, 2429–

2467.  

Vautard, R., and M. Ghil (1989), Singular spectrum analysis in nonlinear dynamics, 

with applications to paleodynamic time series, Physica D, 35, 395–424. 

Walpole, R. E. and Myers R. H. (1993), Probability and Statistics for Engineers and 

Scientists, Prentice Hall.

Table 1:  Possible variations in the procedure ordering. The analysis must always 

begin with data binning and end with probability calculation. Recall N is binned data, 

S is the activity rate, P is a probability calculation, the ˆ symbol represents 

normalization in space, the D symbol represents calculation of change in rate, and the 

underscore symbol represents averaging over base times. 

Method 

Procedure 

I  N  (cid:1)  S  (cid:1)  (cid:3)  (cid:1)  D(cid:3)  (cid:1)  D(cid:3)  (cid:1)  P 

II  N  (cid:1)  S  (cid:1)  (cid:3)  (cid:1)  (cid:3)  (cid:1)  D(cid:3)  (cid:1)  P 

III  N  (cid:1)  S  (cid:1)  S  (cid:1)  (cid:3)  (cid:1)  D(cid:3)  (cid:1)  P 

IV  N  (cid:1)  S  (cid:1)  DS  (cid:1)  D(cid:3)  (cid:1)  D(cid:3)  (cid:1)  P 

V  N  (cid:1)  S  (cid:1)  DS  (cid:1)  DS  (cid:1)  D(cid:3)  (cid:1)  P 

VI  N  (cid:1)  S  (cid:1)  S  (cid:1)  DS  (cid:1)  D(cid:3)  (cid:1)  P 

VII  N  (cid:1)  (cid:4)  (cid:1)  (cid:3)  (cid:1)  D(cid:3)  (cid:1)  D(cid:3)  (cid:1)  P 

VIII  N  (cid:1)  (cid:4)  (cid:1)  (cid:3)  (cid:1)  (cid:3)  (cid:1)  D(cid:3)  (cid:1)  P 

 

Table 2:  Relative likelihood values LG− L0 using a global Gaussian model over the 

time period t = 1984 (cid:1) 1994 for the various variations in order, binning, and 

calculation of change in activity rate. Recall that A – D denote normal, time-centered, 

cumulative, and detrended binning, respectively, while 1 and 2 denote normal and 

projected calculations of change in activity rate. For our null hypothesis, L0, we took 

the value from Method I-A1.  Larger (more positive) values are better correlated with 

actual events. 

Method  A1 

B1 

C1 

D1 

A2 

B2 

C2 

D2 

I 

II 

III 

IV 

V 

VI 

VII 

VIII 

0.00 

-13.06  -11.27  -18.80  -36.47  -32.23 

-19.43  -24.62 

3.33 

-8.65 

-21.91  -17.96  -36.14  -30.92 

-14.17  -23.27 

2.70 

-1.04 

-32.58  -19.89  -15.28  -15.28 

-14.74  -21.99 

-2.89 

-2.08 

-16.10  -13.87  -31.20  -16.43 

-15.94  -12.57 

-7.99 

-4.75 

-14.35  -19.70  -34.48  -12.94 

-14.67  -21.51 

-2.76 

-2.92 

-17.63  -19.92  -33.23  -10.88 

-14.54  -21.05 

-20.32  -17.41  -14.87  -32.44  -48.93  -10.90 

-16.03  -33.38 

-16.65  -21.57  -37.77  -32.02  -47.32  -10.99 

-15.05  -33.42 

 

Table 3:  Relative likelihood values LP− L0 using a local Poissonian model over the 

time period t = 1984 (cid:1) 1994 for the various variations in order, binning, and 

calculation of change in activity rate. Recall that A – D denote normal, time-centered, 

cumulative, and detrended binning, respectively, while 1 and 2 denote normal and 

projected calculations of change in activity rate. For our null hypothesis, L0, we took 

the value from Method I-A1.  Larger (more positive) values are better correlated with 

actual events. 

Method  A1 

B1 

C1 

D1 

A2 

B2 

C2 

D2 

I 

II 

III 

IV 

V 

VI 

VII 

VIII 

-0.00 

1.29 

-38.14  -30.87  -57.74  -44.65 

-5.77 

-74.67 

4.93 

5.58 

-60.65  -28.60  -18.05  -29.54 

-2.09 

-48.88 

2.94 

14.74 

-59.22  -26.22 

5.04 

5.04 

-2.01 

-35.93 

7.75 

6.77 

-7.27 

-12.30  -32.11  -14.98 

-3.15 

-11.46 

0.43 

-0.52 

-7.38 

-43.10  -45.47 

-5.94 

-2.12 

-45.89 

0.84 

0.63 

-9.89 

-40.51  -21.67 

-3.99 

-2.04 

-55.60 

-59.34  -51.33  -61.89  -85.76  -81.90  -47.86 

-44.11  -81.12 

-45.73  -57.16  -76.22  -87.33  -83.09  -48.66 

-44.12  -81.55 

 

Table 4:  Relative likelihood values using Method III-A1 with varying time steps (in 

days) over the time period t = 1984 (cid:1) 1994.  For our null hypothesis, we took the 

value at dt = 1 day.  Larger (more positive) values are better correlated with actual 

events. 

 

dt  = 

1 

3 

5 

7 

15 

30 

60 

90 

180 

365 

LG− L0  =  0.00 

-0.07   -0.16   -0.13   -0.63   -1.33   -2.69   -17.00   -34.06   -20.17 

LP− L0  =  0.00   -0.55   -0.70   -1.43   -4.52   -7.31   -9.66   -24.43   -85.22   -33.66 

Figure 1:  The topmost plot represents random earthquake events over an arbitrary 

time scale. The four lower plots show the results of the different binning methods: A) 

normal, B) time-centered, C) cumulative, and D) detrended. 

 

 

Figure 2:  Representative forecast hot-spot maps created using each of the order 

variations with normal binning and calculation of change in activity rate for the time 

period t = 1984 to 1994. Note the increase in apparent noise for Methods VII and 

VIII. 

Figure 3:  Representative forecast hot-spot maps created using each of the variations 

in binning and calculation of change in activity rate for Method I over the time period 

t = 1984 to 1994. 

 

 

 

 

Figure 4:  Relative likelihood values for two different probability density functions, 

Gaussian (solid) and Poissonian (dashed), as a function of t0.  Larger (more positive) 

values are better correlated with actual events.  The plateau in the data before t0 = 

1965 indicates that only ~40 years of historic data is necessary for the analysis. 

 

 

Figure 5:  Relative likelihood values for two different probability density functions, 

Gaussian (solid) and Poissonian (dashed), as a function of the minimum magnitude 

cut-off threshold.  Larger (more positive) values are better correlated with actual 

events.  Using the Poissonian PDF, more probable forecasts appear possible by 

increasing the magnitude threshold slightly. 

 

 

Figure 6:  Relative likelihood index calculated using a Gaussian density function as a 

function of both t0 and minimum magnitude cut-off threshold.  Warmer colors are 

better correlated with actual events. 

 

 

Figure 7:  Relative likelihood index calculated using a Poissonian density function as 

a function of both t0 and minimum magnitude cut-off threshold.  Warmer colors are 

better correlated with actual events. 

 

 

Figure 8:  Seismic hot-spot map for large earthquake events with M >5 for the 

forecast time period 1 January 2000 to 31 December 2009.  Since the cut-off date for 

the forecast, eight large earthquake events with M >5 have occurred in central or 

southern California. Seven of the eight events occurred either on areas of forecasted 

anomalous activity or within the margin of error of ±11km.  Data from the SCEDC 

catalog was used below 35 o North latitude, and from the NCEDC catalog above 35 o 

North latitude. 

 

Figure 9:  Equal weight for both catalogs (A) vs. higher weighting for northern 

catalog (B).  With the equally weight map, event #3 occurs near a threshold-level 

anomalous region while event #7 does not. The opposite is true with the unequally 

weight map. 

 


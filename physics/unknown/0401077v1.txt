1 

Is Computation Reversible? 

Michael C. Parker* & Stuart D. Walker**  

* Fujitsu Laboratories of Europe Ltd., Columba House, Adastral Park, Ipswich IP5 

3RE, UK 

Colchester CO4 3SQ, UK 

** Department of Electronic Systems Engineering, University of Essex, Wivenhoe Park, 

Recent investigations into the physical nature of information and fundamental 

limits to information transmission have revealed questions such as the possibility 

of superluminal data transfer [1] or not [2]; and whether reversible computation 

(information processing) is feasible [3]. In some respects these uncertainties stem 

from the determination of whether information is inherent in points of non-

analyticity (discontinuities) [4] or smoother functions [5-7]. The close relationship 

between information and entropy is also well known [8,9], e.g. Brillouin’s concept 

of negentropy (negative entropy) as a measure for information [10]. Since the 

leading edge of a step-discontinuity propagates in any dispersive medium at the 

speed of light in vacuum as a precursor to the main body of the dispersed pulse 

[11], we propose in this paper to treat information as being intrinsic to points of 

non-analyticity (discontinuities). This allows us to construct a theory addressing 

these dilemmas in a fashion consistent with causality, and the fundamental laws of 

thermodynamics. A consequence of our proposition is that the movement of 

information is always associated with the dissipation of heat, and therefore that the 

concept of reversible classical computation is not tenable. 

Consideration of information as being intrinsic to points of non-analyticity is akin to the 

treatment  of  it  as  a  ‘particle’,  despite  it  having  apparent  dual  “wave-particle”  (i.e. 

localised  and/or  distributed)  characteristics  [4-7].  Such  complementary  aspects  are  of 

2 

course  paradoxical  in  nature,  but  by  adopting  this  particular  viewpoint,  the  following 

analysis aims to be self-consistent. Landauer’s principle [8,12] in its ‘static’ form states 

that erasure of information requires energy and hence is associated with an increase in 

entropy;  whereas  the  creation  of  information  doesn’t  require  energy  and  so  is  not 

associated with an increase in entropy. We have recently formulated a dynamic version 

of this principle [13], where transfer of information from A to B can be thought of as the 

annihilation of the information at A (therefore accompanied by an increase in entropy) 

followed by its re-creation at B (which is not associated with any change in entropy). It 

is  well  known  that  computation  is  associated  with  the  ‘shuttling’  of  input  information 

through logic gates according to a given algorithm (the program), in order to transform 

that information into an alternative ‘more interesting’ output form. A computation does 

not  intrinsically  add  ‘new’  information  to  the  input  information;  and  the  use  of 

‘reversible’ logic gates, such as Toffoli gates to ensure zero information loss, means that 

a  computation  doesn’t  necessarily  lose  any  information  either  [3,14].  In  conjunction 

with the static form of Landauer’s principle, this has led to the concept of ‘reversible’ 

computation, with its associations with reversible thermodynamics, and the implication 

of energy-neutral computation. 

To  continue  our  discussion  of  information  transfer  in  a  meaningful  sense,  the 

concept of differential information must be introduced [13,15], since spatial differences 

in  information  are  best  described  by  the  differential  information.  We  note,  that  in  the 

infinitesimal limit of the well-known discrete summation describing information, there 

is a diverging part that we ignore when discussing information transfer. This is because 

when  considering  differences  between  information  at  different  spatial  positions,  the 

diverging parts cancel; the constant (infinite) divergent part due to the infinitesimal limit 

3 

being the same everywhere. We have previously shown that the differential information 

I, i.e. that information which can be transferred from one spatial location to another, is 

given by the sum of the associated residues,  2 i

Rπ∑ , from the Cauchy residue theorem 

[13], where 

( )xψ  is the normalised wave function encoding the information:  

=

I

ψ

x
( )

2

log

2

ψ

x
( )

2

dx

= ∑
π
2
i

R

 

(1a) 

and 

ψ

x
( )

2

dx

=

1

. 

(1b) 

∞

∫

−∞

∞

∫

−∞

Residues  are  a  consequence  of  localised  discontinuities  or  points  of  non-analyticity  in 

the complex plane, and as indicated in Figures 1a and 1b these points can be considered 

to  be  the  inherent  ‘location’  of  the  differential  information.  Functions  obeying  the 

Cauchy-Riemann  equations  allow  the  process  of  analytic  continuation,  such  that  the 

function can be completely reconstructed in a self-consistent fashion from any point in 

those  regions  where  the  Cauchy-Riemann  symmetry  conditions  hold  [16].  In  regions 

where the Cauchy-Riemann conditions do not hold, analytic continuation of the function 

cannot  be  performed  and  the  function  becomes  in  effect  “non-predictable”  in  those 

regions. Figure 1b shows such a discontinuity due to a simple pole in the complex plane 

at z0. We note that a simple pole obeys the Paley-Wiener criterion for causality [17], as 

well as being square-integrable and having zero power at infinite frequencies. The lack 

of smoothness at the peak of the function is obvious, as is the case for any discontinuity. 

Points  of  non-analyticity  are  therefore  inimical  to  any  assumptions  of  ‘smoothness’, 

such  that  in  general,  when  the  system  is  allowed  to  dynamically  evolve  in  time, 

assumptions  of  adiabiticity  are  not  tenable,  and  one  would  expect  the  entropy  to 

increase.  

Present day photonic data links may be taken as an example of this principle of 

entropy  increase  with  information  transfer.  Here,  light  modulated  with  data  is 

4 

transmitted through a passive medium, e.g. in free-space, or down an optical fibre. The 

light will either suffer the degradations of diffraction in the former case, or waveguide 

and material dispersion in the latter. Both these processes lead to the gradual attenuation 

of the light as it travels ever-longer distances, and a reduction in optical signal to noise 

ratio (SNR). Shannon’s theorem [18] requires that the channel capacity must therefore 

also reduce with distance, e.g. [19], given no redundancy in the information for error-

correction  purposes.  The  reduction  in  capacity  reveals  itself  as  an  increase  in  the 

associated  bit-error-rate  (BER),  and  consequent  loss  of  information.  Assuming 

information  and  entropy  to  be  negatively  correlated  [10],  this  means  an  increase  in 

entropy.  The  use  of  amplifiers  to  compensate  for  such  attenuation  also  fundamentally 

leads to an increase in entropy, due to the impossibility of noiseless amplification [20] 

(noise  must  always  be  added  to  an  amplified  signal),  and  the  no-cloning  theorem  at 

quantum levels [21]. At the receiver end, imperfect photon to charge carrier conversion 

intrinsically  degrades  signal  strength,  and  other  well-known  effects  such  as  photon 

related shot noise, leakage current and noise from active and passive components [22], 

all  act  to  further  degrade  electrical  SNR.  Even  coherent  detection  schemes  suffer 

impairments as the photon statistics of the local oscillator also serve to introduce noise. 

Hence  data  transmission  is  associated  with  an  increase  in  entropy.  In  figure  2,  we 

attempt  to  schematically  show  a  number  of  different  aspects  of  the  information 

transmission problem. At a time  0t , a localised quantity of information 

(
I z

A

)

 moves in 

the  direction  from  A  towards  B.  Erasing  that  information  at  its  spatial  location  A 

requires  energy  E∆ .  Arriving  at  B  at  a  later  time  1t ,  Landauer’s  principle  determines 

that  no  energy  is  required  to  reinstate  (create)  that  information  I,  now  at  location 

Bz . 

Overall, assuming the set-up can be characterised by a finite temperature T, the entropy 

5 

of  the  system  increases  by 

∆ = ∆

S

E T
/

.  The  required  amount  of  energy  E∆   tends  to 

increase  with  distance,  for  example,  for  the  case  of  photonic  networks  energy  is 

dissipated due to attenuation at typically 0.2dB/km [23]. 

Sommerfeld and Brillouin studied the propagation of a step-discontinuous pulse 

through  a  dispersive  (causal)  medium  [11],  and  found  that  the  pulse  leading  edge 

propagates at the speed of light in vacuum c, via forerunners as a precursor to the main 

body  of  the  dispersed  pulse,  whatever  the  refractive  index  of  the  medium.  Since  the 

point  of  information  must  essentially  remain  a  discontinuity,  it  therefore  travels  at  c. 

This is therefore consistent with classical causality (where superluminal data transfer is 

not  allowed),  as  well  as  agreeing  with  the  spirit  of  Sommerfeld  and  Brillouin’s  work, 

where  the  forerunners  that  potentially  could  carry  a  signal,  propagate  at c.  Hence,  the 

relation 

L

/

∆ =
t

z

B

−

z

A

(

/

t
1

−

t

0

)

=   holds.  We  note,  that  the  impossibility  of 

c

superluminal  information  transmission  is,  in  itself,  an  indication  of  the  discontinuous 

nature of information. 

Moore’s  Law  implies  that  logic  gate  density  will  continue  to  double  every  18 

months,  such  that  computers  will  continue  to  decrease  in  size  for  a  given  processing 

power [15]. Ultimate physical limits to computers have already been discussed in detail 

[14],  such  that  computers  will  always  have  a  finite  volume,  i.e.  any  physical  process 

(such as a computation) cannot take place in a point volume. A finite volume computer 

means that information will always have to travel some distance whilst being shuttled 

through  the  various  logic  gates,  such  that  heat  will  always  be  dissipated  due  to  the 

dynamic  form  of  Landauer’s  principle.  Hence,  although  for  a  given  processing  power 

computers will dissipate less heat as they reduce in size, they will only become 100% 

efficient  (i.e.  dissipate  no  heat)  in  the  limit  of  being  infinitely  small.  We  note  that 

6 

quantum  computing  [24]  appears  to  suffer  a  similar  restriction,  since  the  quantum 

system needs to be completely isolated from the environment to avoid decoherence of 

the  wave  function.  Entropy  might  then  be  expected  to  remain  constant  during 

computation.  However,  control  of  the  environmental  isolation  becomes  increasingly 

difficult as the physical size of the quantum computer increases [25].   

Overall,  our  investigations  indicate  that  a  perpetual  calculating  machine  is  not 

possible,  with  computers  operating  under  the  same  constraints  as  conventional 

mechanical  machines:  obeying  the  Second  Law  of  Thermodynamics  and  always 

operating  at  below  100%  efficiency.  We  conclude  that  a  finite  physical  classical 

computer will always have to dissipate some heat due to the movement of information, 

such that the concept of thermodynamically reversible computation is not tenable. 

1.  L.J. Wang, A. Kuzmich, A. Dogariu, “Gain-assisted superluminal light propagation”, 

 

Nature, vol.406, p.277–279, 2000 

2.  M.D.  Stenner,  D.  J.  Gauthier,  M.A.  Neifeld,  “The  speed  of  information  in  a  ‘fast-

light’ optical medium”, Nature, vol.425, p.695-8, 2003 

3. W.H. Zurek, “Thermodynamic cost of computation, algorithmic complexity and the 

information metric”, Nature, vol.341, p119-124, 1989 

4. J.C. Garrison, M.W. Mitchell, R.Y. Chiao, E.L. Bolda, “Superluminal signals: causal  

loop paradoxes revisited”, Physics Letters A, vol. 245, p.19-25, 1998 

5. K. Wynne, “Causality and the nature of information”, Optics Communications, vol. 

209, p.85-100, 2002 

6. J.J. Carey, J. Zawadzka, D. A. Jaroszynski, K. Wynne, “Noncausal time response in 

frustrated total internal reflection”, Physical Review Letters, 84(7), p1431-4, 2000 

7 

7.  W.  Heitmann,  G.  Nimtz,  “On  causality  proofs  of  superluminal  barrier  traversal  of 

frequency band limited wave packets”, Physics Letters A, vol 196, p.154-158, 1994 

8. C.H. Bennett, “Demons, engines and the Second Law”, Scientific American, p.88-96, 

9.  R.  Landauer,  “Computation:  A  fundamental  physical  view  ”,  Physica  Scripta,  35, 

November 1987 

p.88-95, 1987 

10. L. Brillouin, “Science & Information Theory”, Academic Press, 1956 

11. L. Brillouin, “Wave propagation and group velocity”, Academic Press, New York, 
1960 
12. R. Landauer, “Information is physical”, Physics Today, p.23-9, May 1991 
13. M.C. Parker, S.D. Walker, ‘Information transfer and Landauer’s principle’, Optics 

Communications, vol.229, p23-7, 2004 

14.  S.  Lloyd,  “Ultimate  physical  limits  to  computation”,  Nature,  vol.406,  p1047-54, 

15.  N.  Gershenfeld,  “The  physics  of  information  technology”,  Cambridge  University 

2000 

Press, Chapter 4, 2000 

16.  K.-E.  Peiponen,  E.M.  Vartianen,  T.  Asakura,  “Dispersion,  complex  analysis,  and 

optical spectroscopy: classical theory”, Springer Verlag, 1999 

17.  H.  Primas,  “Time,  Temporality,  Now:  The  representation  of  facts  in  physical 

theories”, Springer Verlag, Berlin, p.241-263, 1997 

18.  C.E.  Shannon,  “A  mathematical  theory  of  communication”,  Bell  System  Tech. 

Journ., vol. 27, p.379-423, p.623-656, 1948 

19.  P.P Mitra, J.B. Stark, “Nonlinear limits to the information capacity of optical fibre 

communications”, Nature, vol. 411, p.1027-1030, 2001 

20. E. Desurvire, “Erbium-Doped Fibre Amplifiers: Principles and Applications”, John 

Wiley & Sons, 1994 

8 

21.  W.K.  Wootters,  W.H.  Zurek,  “A  single  quantum  cannot  be  cloned”,  Nature, 

22.  H.A.  Haus,  “Electromagnetic  noise  and  quantum  optical  measurements”,  Springer 

vol.299, p.802-3, 1982. 

Verlag, Berlin, 2000 

23. G.P. Agrawal, “Fiber-optic communication systems”, John Wiley & Sons, 2002 

24.  M.A.  Nielsen,  I.L.  Chuang,  “Quantum  computation  and  quantum  information”, 

Cambridge University Press, Chapter 2, 2000 

25. A.J. Fisher, “Quantum computing in the solid state: the challenge of decoherence”, 

Phil. Trans. R. Soc. Lon. A, 361, p.1441-50, 2003 

 

 

Figure 1: a) A holomorphic function f(z) allowing complete analytic continuation 

across the complex z-plane such that it contains zero differential information, b) 

A meromorphic function with a simple pole at z0 in the z-plane, with a finite 

residue sum, containing differential information that can be transferred. 

Figure 2: Transfer of information from A to B requires energy ∆E to ‘erase’ the 

information I at position zA, time t0, followed by recreation of the information at 

position zB, at a later time t1, such that the speed of information transfer is |zB-

zA|/(t1- t0)=c. Given a system temperature of T, the overall entropy will increase 

by an amount ∆E /T. 

( )
z dz

f

=

0

∫(cid:0)

Ω

( )
z dz

= ∑
Rπ
i
2

f

∫(cid:0)

Ω

0z

iy

(a)

Ω

= +
x

iy

z

x

iy

(b)

Ω

x

= +
x

iy

z

(
I z

A

)

Entropy
0S

0t

Direction of information flow

B

=

L

z

B

−

z

A

Landauer’s
Principle:

Energy to erase
information at A

E∆

No energy required
to recreate information at B

t
1

t>

0

A

(
I z

B

)

Information speed:
z

−

z

A

=

c

B
t
1

−

t

0

Entropy change:
+

=

S
1

S

0

∆
E
T


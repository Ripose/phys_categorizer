 
 

http://arxiv.org/abs/physics/0604127 

Page 1 of 6 

TRANSCENDING THE LEAST SQUARES 

Fyodor  V. Tkachov 

Institute for Nuclear Research  
of Russian Academy of Sciences 
Moscow, 117312, Russia 

The  method  of  quasi-optimal  weights  provides  a  comprehensive,  asymptotically  
optimal,  transparent  and  flexible  alternative  to  the  least  squares  method.  The  opti-
mality holds for a general non-linear, non-gaussian case.  

“La méthode qui me paroit la plus simple et la plus générale  
consiste à rendre minimum la somme des quarrés des erreurs, 
... et que j'appelle méthode des moindres quarrés.”  

Adrien-Marie Legendre, 1806 

“So the last shall be first, and the first last…” 

Matthew 20:16 

1 .   I N T R O D U C T I O N.   The  least  squares  method  [1]  is  a 
staple  of  experimental  sciences.a  However,  physicists  in-
volved with the ever more delicate and expensive experi-
ments — and  with the computing power at their disposal 
exponentiating by Moore’s law — are increasingly impa-
tient with a sub-optimality of the venerable method in the 
non-linear, non-gaussian situations they have to deal with. 
Two  examples  are:  (1)  the  neutrino  mass  measurement 
experiment [10] observing Poisson-distributed events, that 
has been providing context and a test bed for the methods 
described in the present Letter, and that will be referred to 
for the purposes of illustration; (2) the very high precision 
muon  anomalous  magnetic  moment  measurements  where 
a non-trivial special case of the method we advocate was 
discovered [12] and put to use [17]. 

The concept of counting weighted events was advanced 
by  Karl  Pearson  in  [2]  and  is  known  as  the  method  of 
moments.  In  Pearson’s  original  treatment,  power  weights 
xn  were used, motivated partly by the need to simplify his 
formidable  computations  (involving  data  on  1000  crabs), 
partly  by  the  then  popular  mathematical  problem  of  mo-
ments  (restoring  a  function  from  its  integrals  with  xn ), 
whence  the  terminology.  A  generalization  to  arbitrary 
continuous weights is obvious, and  we henceforth use — 
following  refs.  [12]  and  [17]  —  the  term  weights  as  the 
most  suggestive  one  in  place  of  the  somewhat  obscure 
“moments” or quantum-theoretic “observables”. 

After  the  arrival  of  the  maximal  likelihood  (ML) 
method in Fisher’s research [3], Pearson’s trick  was stig-
matized as non-optimalb — despite the obvious analytical 

                                                           
a  Coincidentally,  it  was  first  made  public  exactly  two  centuries 
ago — a fact that should have been a cause for symposia, but is, 
surprisingly, not. 
b The rivalry between the two men must have contributed to this. 

advantages it offered, and for that reason alone deserving 
a scrutiny rather then the role of a mathematical warm-up 
in textbooks before the complicated, therefore interesting 
issues  are  addressed.  The  stigma  is  manifest  in  how  its 
presentation always has to be justified by its historic prior-
ity  —  followed  by  the  non-optimality  mantra.  From  the 
Particle  Data  Group  summaries  of  statistical  methods,  its 
mention  has  been  dropped  altogether  for  a  number  of 
years already.  

Yet the analytical transparency of the method remains a 
temptation. Also, a numerical maximum search in several 
dimensions  would  in  general  be  somewhat  less  attractive 
than  solving equations (not that the latter is always  with-
out problems, of course). 

The  development  of  functional  analysis  that  occurred 
after  Kolmogorov  postulated  his  axioms  for  probability 
[4] — in particular, the emergence of the concept of gen-
eralized  function  interpreted  as  a  linear  functional  on 
suitably  chosen  “test  functions”  [5]  —  supplied  theorists 
with  some  very  powerful  calculational  techniques  [13].c 
From  the  functional-analytic  point  of  view,  a  measure 
(probability  distribution)  is  defined  as  a  linear  functional 
on  arbitrary  continuous  test  functions j  —  which  for  a 
non-singular  probability  distribution  corresponds  to  its 
integrals  with  continuous  weights  j.  This  highlights  a 
basic  character  of  the  concept  of  weighting,  and  induces 
one  to  regard  Pearson’s  method  as  a  foundational  one. 
With one’s mind thus focused, a meditation brings forth a 
simple  question:  Which  weight  yields  an  estimate  with 
lowest variance? 

                                                           
c Note also how test functions are curiously similar to acceptance 
functions  of  elementary  detector  cells  in  modern  high  energy 
physics detectors (about 105 cells of several types and sizes). 

F.V.Tkachov. Transcending The Least Squares  http://arxiv.org/abs/physics/0604127 
 

Page 2 of 6 

Even if the question was too simple for mathematicians 
to bother with, practical needs compelled a few physicists 
to arrive, in  specific  ways, at specific answers — special 
cases of the formula (6) below (see [11] and further refer-
ences to this  line of research  in [15]). Curiously, the for-
mulae in those papers are similar to those in section 8.2.2 
of [6] — the section being explicitly devoted to a demon-
stration that the  method of  moments is less efficient than 
the  ML  method.  Also,  the  fact  that  the  differential  maxi-
mum condition of the ML method can be formally derived 
from  the  method  of  moments  with  a  specially  chosen 
weight (cf. Eq. (8)) was recorded, in small print, in a me-
ticulous treatise [9] — but remained disconnected from a 
traditional  cursory  discussion,  complete  with  the  mantra, 
of  Pearson’s  method;  the  small  print  paragraph  stopped 
short of realizing the significance of the fact.  

It is simply incredible how experts — pre-conditioned 
by the non-optimality mantra — would bump into the key 
formulae yet fail to see the whole picture. 

A  different  and  complex  special  case  of  an  optimal 
weight was independently worked out in [12] and chosen 
for data processing in a significant measurement [17].  

In  short,  the  concept  of  counting  weighted  events  was 
virtually begging to be given a most serious consideration. 
Refs.  [14],  [15],  [18] (stemming  from  the  earlier  find-
ings of powerful calculational methods based on general-
ized  functions  [13])  promulgated  the  functional-analytic 
approach  to  the  problems  of  statistics  as  advantageous 
over the traditional one. The advantages are due, in the fi-
nal respect, to the resulting true algebraization of the sub-
ject  (because  the  term s-algebra  is,  really,  a  masquerade 
— exacerbated by a constructive deficiency of the notion 
of  a  function  on  arbitrary  sets).  Ref.  [14]  tried  to  shed  a 
systematic light on Pearson’s unfairly neglected trick and 
develop it into a general method: 

2 .   T H E   M E T H O D   O F   Q U A S I - O P T I M A L   W E I G T H S.  

Given a sample of events {Pi}i governed by a probabil-
ity distribution p(P) that depends on a parameter q whose 
exact value q* is unknown, one estimates q* by choosing 
a weight j( P)  and equating the theoretical mean, 

j

th

=

(cid:1) P P
d
( )
p j

( )
P

”

h

(
)
q

,  

assumed  to  be  a  calculable  function  of  q,  to  the  corre-
sponding experimental value 

j

exp

= (cid:2) P , 
)i
(
j
i

1
N

and to solve the resulting equation 

j

exp

h
)
(
q=

 

to obtain an estimate qexp for the unknown value q*: 

(1) 

(2) 

(3) 

(4) 

 

The procedure is perfectly transparent and yields estimates 
with nice properties [6], [9].  
NB  The  procedure  remains  meaningful  even  when  the 
classical  assumption  of  the  asymptotic  normality  of  the 
distribution  of  qexp  does  not  hold  for  whatever  reason: 
with modern computing power, it may be possible to per-
form a brute-force study of the distribution of qexp. 

But  if  (theoretical)  Varj  exists,  then,  asymptotically 

for large N,  

N

Var

q

exp

=

Var
j
2

H

,

H

=

h
¶

)

(
q
exp
¶
q

. 

Ref. [14] showed that under similar technical assumptions 
to the ML method, the minimum of Varqexp is located in 
the space of j at 

(5) 

(6) 

j

opt

( )
P

=

¶

ln ( )
p
P
¶
q

.

 

The  corresponding  calculation  would  be  a  piece  of  cake 
already for Euler or Bernoullis: one deals here with a ratio 
of two quadratic forms of j, so the intricacies of defining 
functional derivatives etc., are essentially irrelevant. 

Moreover, the deviations from the value of Varq at (6) 
being non-negative and quadratic in j – jopt (see ref. [14] 
for details), the resulting minimum happens to exactly cor-
respond  to  the  Fisher-Fréchet-Rao-Cramér  boundary  (ac-
cording  to  [9],  the  inequality  was  essentially  known  to 
Fisher in the 20’s). The quadraticity is of a critical impor-
tance as it ensures that even imperfect approximations to 
jopt would yield practically optimal results. 

One  would start  with an arbitrarily chosen  weight j(0) 
and  obtain  a  valid  (if  non-optimal)  estimate  qexp,(1)  — 
complete with an error estimate. One would then use this 
to construct the corresponding jopt ,(1) and use the latter to 
obtain an estimate qexp, ,(2) together with an improved vari-
ance. As to the initial weight j(0), one can take it to be the 
optimal weight jopt , (0) corresponding to some q0  (because 
one  should  be  able  to  construct jopt   for  any q  anyway). 
The  successive  weights  jopt , (0),  jopt , (1)  etc.  are  optimal 
for the values of q that are not necessarily equal to the un-
known exact q*, whence the prefix “quasi”. 
Each estimate in the resulting chain 

ﬁ
q q

0

exp(1)

ﬁ

q

exp(2)

...
ﬁ  

(7) 

asymptotically (for large N) converges to the true value of 
q for any initial q0, so the issue of convergence of itera-
tions  is  theoretically  (i.e.  for  large  N)  non-existent.  The 
purpose  of  iterations  is,  basically,  to  reduce  the  variance 
of the estimate. In the context of [10], a few iterations of-
ten suffice (thanks to the quadraticity).  
NB  The  described  iterations  together  with  those  implicit 
in  the  solution  of  (3)  correspond  to  the  optimizations  of 
the conventional methods. 

NB  Note a simple and useful identity that holds under the 
obvious regularity assumptions: 

j = . 

0

opt

(8) 

F.V.Tkachov. Transcending The Least Squares  http://arxiv.org/abs/physics/0604127 
 

Page 3 of 6 

NB  The resulting estimate won’t be affected if one drops 
a  P-independent  factor  or  addendum  from  (6).  However, 
dropping addenda (that usually originate from normaliza-
tion factors of the probability densities) spoils Eq.(8). 
3 .   T H E   M U L T I - P A R A M E T E R   C A S E. d
  If  there  are  sev-
eral parameters qa to be estimated, one needs (at least) the 
same  number  of  weights  j(a)(P)  and  the  corresponding 
functions h(a)(q). Eq. (17) becomes a system of equations 
(one for each weight) to determine the vector qexp. 

What is the quantity to minimize in the multi-parameter 
case?  The  issue  is  important  for  two  reasons.  First,  be-
cause unlike the one-parameter case where one deals with 
a  single  scalar  Varq,  here  one  deals  with  the  covariance 
matrix  Covarq ” Q.  Second,  because  the  issue  of  select-
ing one among several possible solutions (e.g. when solv-
ing  the  system  (3))  is  more  pronounced  with  many  pa-
rameters, and a good criterion is of value. 

Let us pursue the functional-analytic line of reasoning. 
Let q* be the unknown exact value of the vector variable 
q, and let qexp be the solution of the system (17) for some 
fixed  set  of  weights j. 
expj   is  a  random  vector  con-
structed  from  {Pi}i  —  denote  its  probability  distribution 
as 

p j(cid:1)
(
The first, sine qua non assumption is that the latter dis-
tribution  converges,  for  large  N,  to  the  d-function 
(
. This assumption already  makes Pear-
d j

h
(
q-

)*
)

exp

exp

.  

)

son’s method meaningful as it allows one to obtain an es-
timate qexp by solving the system (3). 

To pursue optimality, one assumes existence of second 
moments,  and  makes  a  more  detailed  statement  that 
p j(cid:1)
(
  approaches  the  limiting  d-function  when 

exp

)

N ﬁ ¥  via  a  multi-dimensional  normal  distribution  cen-
tered at h(q*):  

k
(cid:4)
2
(cid:6)
(cid:8)

(cid:3)
(cid:5)
(cid:7)

N
2
p

-

1/ 2

det

F

(cid:1)
(
p j

)

ﬁ

exp

N
2

(

·

exp

(cid:9)
-
(cid:11)
(cid:13)
(
d j

ﬁ

-

h

(
*
q

)

exp

,

)

j

exp

-

h

(
*
q

)

T

)

-

1

F

(

j

exp

-

h

(
*
q

)

)

(cid:10)
(cid:12)
(cid:14)

where H is the matrix of the tangent mapping for h (q): 

,H

a b

(
)
q

= ¶

(
)
h a

(
)
¶
q q
b

. 

(12) 

The error hyperellipsoid for qexp for a given confidence 

1/ 2

level  has  the  hypervolume 

~ det Q

exp

.  The  missing 

numerical  coefficient  is  determined  by  the  confidence 
level  chosen,  whereas  the  exact  shape  and  orientation  of 
the  hyperellipsoid  (a  thin  disk  or  a  cigar,  variously  ori-
ented, etc.) — and, most importantly, its hypervolume de-
pends on the choice of weights.  

So, the determinant 

kN

det

Q

exp

=

det

F
)

H

2

 

det

(

(13) 

is  the  first  candidate  to  consider  for  the  role  of  a  single 
quantity to minimize in order to find the optimal weights. 

The choice of determinant is also reasonable because it 

respects physical dimensionalities of parameters. 

NB  One might also wish to minimize the volume of pro-
jection  of  the  hyperellipsoid  to  the  subspace  of  some  pa-
rameters deemed more interesting than others. (In the con-
text  of  [10],  the  interesting  parameter  would  be  the  neu-
trino  mass;  the  other  parameters  describe  uncertainties  in 
the knowledge of the experimental setup.) This would re-
quire  a  more  complicated  analysis  beyond  the  scope  of 
this  Letter.  However,  the  standard  results  summarized  in 
[9]  concerning  the  multidimensional  FFRC  inequality, 
seem  to  indicate  that  nothing  of  interest  can  be  found  in 
this direction. 

The  requirement  of  minimizing  the  determinant  (13) 
leads  to  a  reasoning  that  follows  the  one-parameter  case 
considered in [14] in a straightforward fashion: one evalu-
ates the variational derivatives of (13) with respect to j(a) 
and equates them to zero to obtain a set of equations to de-
termine the point of minimum, etc. To get past the deter-
minants, one employs the following purely algebraic iden-
tity for derivatives: 

det

(

A

¢
)

=

det

(

A

)

·

Tr

1
¢(cid:9)
A A-
·(cid:13)

(cid:10)
(cid:14). 

(14) 

 

(9) 

With the determinants out of  the  way, the remaining cal-
culations are completely straightforward if a little cumber-
some. One eventually arrives at the same result as if each 
parameter were considered separately (as stated in [14]): 

where  k  is  the  number  of  parameters  and  F   is  the  theo-
retical covariance matrix for the weights. Then the asymp-
totic probability distribution of qexp is described by a simi-
lar expression,  

(
)
a
j
opt

( )
P

=

¶

ln ( )
p
P
¶
q
a

.

 

(15) 

Thus equipped, we are ready to tackle our main problem. 

k
(cid:4)
2
(cid:6)
(cid:8)

(cid:3)
(cid:5)
(cid:7)

1
2
p

·

exp

(cid:9)
-
(cid:11)
(cid:13)

1
2

(
*
q q

exp

-

)

Q

1
-
exp

(
*
q q

exp

-

)

T

(cid:10)
(cid:12)
(cid:14)

 

1/ 2

det

Q

exp

with the covariance matrix (cf. Eq.(5)) 

N

Q =
exp

H

1
-
F

H

(

1
-

)T

, 

                                                           
d The following discussion properly belongs to [14]. 

(10) 

(11) 

F.V.Tkachov. Transcending The Least Squares  http://arxiv.org/abs/physics/0604127 
 

Page 4 of 6 

4 .   T H E   M U L T I - D I S T R I B U T I O N   C A S E.   First  consider 
the case of one unknown parameter q. Suppose each event 
Pi   (now  called  measurement)  in  the  sample  {Pi}i  (now 
called measurement set) is governed by its own probabil-
ity  distribution  pi (Pi).  Various  i  correspond  to  various 
experimental conditions controlled or recorded by the ex-
perimenter (Fisher’s independent variables). All pi(Pi) are 
supposed  to  be  known  modulo  the  unknown  exact  value 
q* of a common parameter q. For example, in the studies 
of  rare  decays  in  [10],  one  encounters  Poisson  distribu-
tions  with  the  means  depending  via  known  formulae  on 
the  unknown  neutrino  mass.  Another  simple  class  of  ex-
amples is considered below in sec. 6. 

Some pi may be equal — the eventual formalism must 
still  work  correctly.  In  particular,  the  case  when  all  of 
them  are  equal,  is  the  basic  situation  already  considered 
(several  equally  distributed  events).  This  will  provide  an 
important guidance. 

With independent Pi , one can regard the sample {Pi}i  
as  a  cumulative  event  whose  probability  density  is 
p({Pi}i) = P ipi(Pi). Then the optimal weight is: 

j

opt

(

{ }
P
i
i

)

=

¶

ln

)

(
{ }
p
P
i
i
¶
q

(cid:2)

=

i

¶

ln

P
i

)

(
p
i
¶
q

(cid:2)

=

i

j

opt,

i

(

)
.
P  
i

(16) 

This  is  already  enough  for  a  direct  generalization  of  the 
previously  described  method  with  all  its  nice  properties 
preserved: one estimates q* by solving the equation 
(cid:2)

d
( )
p j
P P
i
i

(17) 

(cid:2)

(cid:2)

(
)
q

(
)
q

( )
P

j
i

P
i

h
i

=

”

=

h

, 

)

(

(cid:1)

i

i

i

with ji taken from (16) with some initial q(0), etc. 

Compared  with  the  case  of  a  single-distribution  event 
sample, missing in (17) is the factor N-1, but it is missing 
on both sides, so the resulting q is not affected. 

NB  Similarly to the single-distribution case, one can drop 
a  common  P-independent  factor  from  all  ji,  or  a  P-
independent addendum from each ji (different for differ-
ent i; usually such terms correspond to normalizations of 
pi). This won’t affect the results. 

A  subtlety  concerns  obtaining  a  proper  expression  for 
the variance of (17), on which the variance of the estimate 
hinges. First, due to independence of different Pi , 

Var

(cid:2)

j
i

i

(

(cid:2)P
)
=
i

i

Var

j
i

(

) .
P  
i

(18) 

Next, a predicament: how to estimate Varji(Pi) if there is 
just  one  measurement  Pi   for  each  i?  After  a  first  shock, 
one  realizes  that  the  same  reasoning  would  result  in  the 
same difficulty when all Pi  are distributed identically, for 
which  case  a  meaningful  answer  is  nevertheless  known: 
the mean value to subtract from ji(Pi) before squaring is 
the  cumulative  mean.  This  observation  provides  the 
needed clue. Indeed, the already obtained estimate qexp in-
corporates the cumulative information from the set {Pi}i. 
But  then  the  required  estimate  for  the  mean  is  hi(qexp) 

(remember that all functions h i are assumed to be calcula-
ble). The right-hand side of Eq.(18) becomes: 
) 2
(cid:10)
(cid:14)
Finally, one obtains an estimate for Varqexp :  

(19) 

(cid:9)
j
(cid:13)
i

(
q

(cid:2)

(
P
i

exp

h
i

-

. 

)

i

N

Var

q

exp

=

1
2

H

(cid:2) P
(cid:9)
(
j
(cid:13)
i
i
i

)

-

h
i

(
q

exp

) 2
(cid:10)
(cid:14)

. 

(20) 

A  consistency  check  is  to  set  all  pi   equal.  Then  all 
weights ji,  and  all  functions  hi  are  equal  too.  Then  the 
  is  exactly  the  one  used  to  ob-
equation 

j=

(
h q

exp

)

exp

tain  qexp ,  and  the  familiar  formula  for  the  variance 
emerges. Which is (cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2).  

Extension of the above formulae to the case of several 

parameters (sec. 3) is straightforward. One defines 

(
)
a
j
opt,
i

( )
P

=

¶

ln

( )
P

p
i
¶
q
a

, 

(21) 

etc.  The  covariance  matrix  of  the  weights  (the  independ-
ence of Pi is important here): 
)(
q
)(
q

(
)
(cid:9)
a
j
(cid:13)
i

(
a
h
i

(22) 

(
(cid:9)
j
(cid:13)
i

(cid:2)

(cid:10)
.
(cid:14)

,
ab

(
h
i

(cid:10)
(cid:14)

P
i

P
i

exp

exp

F

)
b

-

=

-

·

)

)

b

(

)

(

)

 

i

Finally,  the  covariance  matrix  for  the  parameters q  is 
given by the relation (11) — but the determinant (13) can 
be evaluated directly from (22) and (12). 

From here, one can proceed to programming.  

5 .   O P T I M I Z E D   L E A S T   S Q U A R E S.   Instead of the struc-
tured iterations (7), with each q a proper estimate obtained 
by solving the equations (17), one could choose to blindly 
minimize  the  “optimized”  sum  of  squares  (20)  with  re-
spect to q — one should then use the same q in the con-
struction of ji  (in place of q0 etc.) and in the argument of 
hi in place of qexp, i.e. in the probability distribution used 
the  example  below).  
to  evaluate  hi  from  ji  (see 
The  resulting  way  to  obtain  the  estimate  is  a  generaliza-
tion  of  the  least  squares  method  preserving,  by  construc-
tion, optimality in a general non-linear, non-gaussian case.  
NB  Unlike the venerable simplest sum of quarrés, the op-
timized sum of squares would be directly connected with 
the variance of the resulting estimate for q. 
NB  With several parameters to estimate, the determinant 
(13) replaces the sum (20) as a quantity to minimize.  
NB  An advantage of the iterative algorithm (7) over the 
blind  minimization  of  the  optimized  sum  of  squares  (20) 
(or  the  determinant  (13))  is  that  one  has  a  nice  control 
point  (with  plots  (4))  after  each  iteration  because  each 
qexp(i) is a meaningful estimate per se. (The plots are par-
ticularly  useful  if  the  weights  are  chosen  such  that  the 
identity (8) holds.)  

Those familiar with the character of multi-dimensional 
optimization  would  be  reluctant  to  forgo  such  an  advan-
tage.  

F.V.Tkachov. Transcending The Least Squares  http://arxiv.org/abs/physics/0604127 
 

Page 5 of 6 

6 .   E X A M P L E .   Consider  the  problem  of  fitting  a  curve 
y = f( x,q)   with  unknown  q  against  a  set  of  points 
{x i ,y i }i , where xi  are controlled by the experimenter and 
known precisely, whereas y i  are normally distributed with 
the same variance s2 around the corresponding mean val-
ues  f( x i ,q) .  Then  Pﬁ y,  and  the  quasi-optimal  weights 
that 
can  be  chosen  as 
(

y

)

y

f
q

j
i

,  so 

(
)
x
0,
q¢=
i
,(0)
(
)
f x
,
.  To  find  qexp ,  one  fits  the 
q
i
¢(cid:2)
(
x
f
0,
  against  the  ex-
q q
i
i
q q¢(cid:2)
)0,
(
y
x
i
i
2
-

)
)
,
q
. Then from (20):  

(
f x
i

f

i

)

h
i

f
(
¢=
qq

)
theoretical  number 

0,
q

x
i

(

perimental one, 

N

Var

q

exp

=

H

[

f
¢
q

(

x
i

,
q
0

2
]

)

(cid:9)
y
-(cid:13)
i

(
f x
i

,
q

exp

2
(cid:10)
(cid:14)

)

, 

(23) 

·

(cid:2)
i

where  

H

= (cid:2)

i

h
¢
i

(
q

exp

)

= (cid:2)

i

f
¢
q

(

x
i

,
q
0

)

f
¢
q

(

x
i

,
q

exp

)

. 

(24) 

Then  one  can  make  a  few  iterations,  using  the  newly 
found qexp for q0.  

If  one  prefers  to  follow  the  optimized  least  squares 
route  (sec.  5),  then  the  expression  to  minimize  would  be 
the following sum: 
(cid:2)

)
,
q

)
,
q

-

2
]

2
]

[

[

y
i

x
i

i

. 

(25) 

(
(cid:2)

f
¢
q
(

i

(
f x
i
22
)
]

[

f
¢
q

(

x
i

)
,
q

Its mininum value will be N Varqexp  (unlike the minimum 
value of the simplest sum of squares). 

NB  The new factors in the optimized sum of squares (25) 
are  present  despite  the  fact  that  the  variance  of  y  is  the 
same for all x. 

Notice  how  the  weights 

  enhance  (sup-
[
press) contributions from xi  according to how fast (slow) 
f( x i ,q)  changes with varying q — somewhat similarly to 
how the optimal weight (6) works.  

)
(
xq q¢
f
,i

]2

To visualize this, consider examples where q is a shift 
parameter, f( x,q) = F ( x-q) ,  so that the steepness of the 
curve  y = F ( x-q)   directly  reflects  its  sensitivity  to 
changes in q. For example, 

f x
(

,
)
q

=

sin

x

(

-

)
q

,

f
¢
q

(

x

,
)
q

= -

cos

x

(

-

)
q

, 

(26) 

or a resonance curve  

f x
(

,
)
q

=

(cid:9)
(
(cid:13)

x

-

)
q

2

2

+ G

1

-
(cid:10)
(cid:14)

,

f
¢
q

(

x
,
)
q

=

2

(

x

-

q

(cid:9)
) (
(cid:13)

x

-

)
q

2

2

+ G

2
-
(cid:10)
(cid:14)

.

 

(27) 

NB    The  minimum  of  (25)  corresponds  to  the  Fisher-
Fréchet-Rao-Cramér boundary, by construction. 

7 .   C O N C L U S I O N S.   The derived formulae set up a com-
prehensive  successor  to  the  least  squares  method,  via  a 
straightforward  simple  extension  of  the  method  of  quasi-
optimal  weights  [14],  itself  a  systematic  development  of 
Pearson’s  long-neglected  method  of  moments  [2].  The 

new  method  inherits  the  nice  analytical  character  of  the 
classical  method of  moments — but also ensures asymp-
totic  optimality  of  estimates.  (An  equivalence  to  the  ML 
method was discussed in [14].) Unlike the ML method, it 
is insensitive to the details of shape of the probability den-
sity,  and  is  thus  applicable  in  some  situations  where  the 
ML  method  fails  due  to  the  probability  density  having 
non-unique  maxima  or  being  only  known  approximately. 
The latter case may occur e.g. in perturbative calculations 
in  quantum  field  theory,  then  evaluating  weighted  cross 
sections  with  smooth  pre-agreed  phase-space  weights 
could significantly reduce the complexity of the task, by-
passing the problem of singular non-positive higher-order 
corrections to matrix elements squared.  

The  method  of  quasi-optimal  weights  also  leads,  in  a 
rather straightforward fashion (via the standard regulariza-
tion trick), to other essential results of theoretical statistics 
x+
such as the estimator 
-  for the q of a uni-

max

)
1

1
2

(

x
min

form  distribution  on  the  segment  [q,1+q].  More  gener-
ally,  the  method  remains  meaningful  with  non-regular 
probability  densities:  then  the  FFRC  minimum  is  simply 
located outside the space of continuous weights — but can 
be approached arbitrarily closely via regularization. 

In the interesting case of experiments with rare decays 
where pi are Poisson distributions (e.g. [10]), the method 
has proved to be not in the least harder to implement than 
the standard weighted least squares method.  

As for the potential improvement of physical results, a 
seasoned expert would not expect miracles. For example, 
Monte  Carlo  studies  with  simple  models  involving  Pois-
son  distributions  show  that  a  reduction  of  confidence  in-
tervals over the conventional least squares beyond 15% in 
the most opportune cases, is hardly likely (although an oc-
casional  bigger  effect  cannot  be  excluded  with  other  dis-
tributions). But even a 10% reduction of  statistical errors 
is welcome as it would come essentially for free while be-
ing worth a 20% increase of data taking time, data storage, 
etc. In any event, the gain proved to be compelling enough 
for [17]. Not to mention one thing less to worry about. 

On the theoretical side, the most important result would 
not be any particular formula (although Eqs. (20) and (25) 
are cute). Rather, I believe, it would be the demonstration 
of economy and expediency of the functional-analytic in-
terpretation  of  probability  distributions.  In  a  still  broader 
context,  this  would  be  another  argument  to  support  the 
view (cf. also [7]) that although the interpretation of func-
tions as correspondences x ﬁ y is an unassailable truth in 
the  case  of  continuous  functions,  its  naïve  set-theoretic 
generalization,  disrespectful  of  constructiveness,  should 
give way in teaching and textbooks to the concept of gen-
eralized  functions  (distributions)  as  linear  (perforce  con-
tinuous  [8],  therefore  constructive)  functionals  on  proper 
test  functions  [5].  For  “the  ultimate  purpose  which  one 
should  always  keep  in  sight,  is  to  find  a  correct  point  of 
view  on  the  foundations  of  science”  (Karl  Weierstrass). 
Perhaps  instead  of  foundations  one  ought  to  be  talking 
here  about  high-level  abstractions  that  give  the  direction 
to our thoughts. 

F.V.Tkachov. Transcending The Least Squares  http://arxiv.org/abs/physics/0604127 
 

Page 6 of 6 

A C K N O W L E D G M E N T S .  I  first  heard  least  squares  and 
optimal  weights  compared  from  the  optimality  viewpoint 
by Jörg Pretz [12], [17], while enjoying hospitality of the 
CERN  theorists.  V.M. Lobashev  (of  the  Troitsk  n-mass 
Experiment [10]) questioned adequacy of the least squares 
in  a  most  specific  context  and  kindly  permitted  the  pro-
posed answer to be tested on real data, in which undertak-
ing the collaboration with A.A. Nozik, A.K. Skassyrskaya 
and  S.V. Zadorozhny  has  been  essential  and  enjoyable. 
V.Z. Nozik  expressed  an  early  appreciation  of  the  idea. 
A.S. Barabash 
(of  neutrinoless  double  beta-decay), 
V.Grigoriev  (of  hadron  peakology)  and  Yu.G. Kudenko 
(of  neutrino  oscillations)  showed  a  hearty  interest  in  this 
work. Yu.M. Andreev (of supersymmetry), S.N. Gninenko 
(of  solar  axions)  and  Kh.S. Nirov  (of  quantum  field  the-

ory)  read  parts  of  the  manuscript.  A.G. Dolgolenko  and 
M.G. Kuzmina  arranged  presentations  of  the  method  be-
fore two sharp audiences — at the Institute of Theoretical 
and  Experimental  Physics  and  the  Keldysh  Institute  of 
Applied 
Moscow). 
V.V. Vedenyapin, having independently arrived in [19] at 
the  conclusion  that  the  functional-analytic  approach  to 
probability is better, stood up for the cause.  

Mathematics 

(both 

in 

I  thank  the  two  audiences  and  the  INR  theorists  for 
their  attention,  and  all  the  listed  people  for  their  under-
standing. 

Support came from the CERN theory group, the RFBR 
grant 05-02-17238a, and the Neutrino Physics Programme 
of the Russian Academy of Sciences. 

References 
arranged chronologically 

 [1]  A.-M. Legendre. Nouvelles méthodes pour la déter-

mination des orbites des comètes. 1806.  

[10]  The Troitsk n-mass Experiment. 

http://www.inr.ru/~trdat/ 

  C.F. Gauss. Theoria motus corporum coelestium in 

[11]  D. Atwood and A. Soni. Analysis for magnetic mo-

sectionibus conicis Solem ambientium. 1809. 

  P.-S. Laplace. Théorie Analytique des probabilités. 

1812. 

 [2]  K. Pearson. Contributions to the mathematical the-

ory of evolution. Phil. Trans. Roy. Soc. London A 185 
(1894) 71.  

 [3]  R.A. Fisher. On an absolute criterion for fitting fre-
quency curves. Messenger of Mathematics 41  
(1912)  155.  

 [4]  A.N. Kolmogorow. Über die analytischen Methoden 
in der Wahrscheinlichkeitsrechnung. Math. Ann. 104 
(1931) 415. 

 [5]  S.L. Sobolev. Méthode nouvelle à résoudre le pro-

blème de Cauchy pour les équations linéaires hyper-
boliques normales. Mat. Sbornik 1, (cid:5) 1 (1936) 39. 
  L. Schwartz. Généralisation de la notion de fonction, 
de dérivation, de transformation de Fourier et appli-
cations mathématiques et physiques. Annales Univ. 
Grenoble 21 (1945) 57. 

 [6]  W.T. Eadie, D. Dryard, F.E. James, M. Roos and B. 

Sadoulet. Statistical methods in experimental phys-
ics. North-Holland, 1971. 

 [7]  R.D. Richtmyer. Principles of advanced mathemati-

cal physics. Springer-Verlag, 1978. 

 [8]  V.M. Kanovey. Axiom of choice and axiom of de-
terminacy. NAUKA: Moscow, 1984 (in Russian). 
 [9]  A.A. Borovkov. Mathematical statistics. NAUKA: 

Moscow, 1986 (in Russian). 

ment and electric dipole moment form-factors of the 
top quark via  e e
2405. 

+ -ﬁ . Phys. Rev. D45 (1992) 

tt

[12]  J. Pretz. Internal memo of the Muon g–2 Collabora-

tion, 1998 (unpublished). 

[13]  F.V. Tkachov. Distribution theoretic methods in 
quantum field theory. arXiv: hep-th/9911236; 
Sov. J. Part. Nucl. 31-7A (2000) 200. 

[14]  F.V. Tkachov. Approaching the parameter estima-
tion quality of maximum likelihood via generalized 
moments. arXiv: physics/0001019; Part. Nucl. Lett. 
111 (2002) 28. 

[15]  F.V. Tkachov, Quasi-optimal observables vs. event 
selection cuts. arXiv: hep-ph/0210116; NIMA 502 
(2003) 758. 

[16]  D.Yu. Grigoriev, E. Jankowski, F.V. Tkachov, To-
wards a standard jet definition. arXiv: hep-
ph/0301185; Phys. Rev. Lett. 91 (2003) 061801. 
[17]  G.W. Bennett, et al., Muon g-2 Collaboration, Meas-
urement of the Negative Muon Anomalous Magnetic 
Moment to 0.7 ppm. arXiv: hep-ex/0401008; 
Phys.Rev.Lett. 92 (2004) 161802. 

[18]  F.V. Tkachov. Pouzyry: a novel class of algorithms 
for restoring a function from a random sample. 
arXiv: physics/0401012; NIMA 534 (2004) 274. 
[19]  V.V. Vedenyapin. Kinetic theory according to Max-

well, Boltzmann and Vlasov. MGOU: Moscow, 2005 
(in Russian). 

 

 

 

 


Texas A&M University, Department of Electrical Engineering, College Station, TX 77843-3128,

USA;   email: Laszlo.Kish@ee.tamu.edu

Johnson-noise driven computing

Laszlo B. Kisha)

(July 2, 2006)

Abstract. The information channel capacity and energy dissipation are studied in a simple digital

system with zero threshold, for the case of error probability close to 0.5, that is, when the thermal

noise is equal to or greater than the digital signal. Using a simplified unit, a comparator, to process

the signal (similarly to Stock's suprathreshold stochastic resonance with analog signals), the energy

need of the digital driving is about 1.6*kT/bit. The advantage of the arrangement is that it does not

have leakage current, crosstalk and ground plane EMI problems. Disadvantage is the large number

(>100 device/gate) of elements needed for low-error-rate operation when a simple majority-vote is

used for error correction.

Keywords: Thermal noise; energy dissipation; suprathreshold stochastic resonator; leakage current.

                                                  
a) Until 1999: L.B. Kiss

Recently, it has been shown [1-4] that modulated Johnson-noise is an interesting information carrier

and it can be used for stealth communication [1] and totally secure non-quantum communication

[2,3],  including  the  realization  of  totally  secure  networks  [4].  These  results  inspire  a  further

question:

and computing, too?

If Johnson-noise is such peculiar information carrier, can it perhaps be used for data processing

Though, we do not know the full answer to this question, in this Letter, we shall show a potential

application of Johnson noise to reduce energy dissipation in microprocessors. This idea may first

sound as nonsense because the ultimate lower limits of energy dissipation in computers is dictated

by Johnson noise, see [5,6] and references therein. However, we are trying to put thermal noise to

work for us by driving the logic devices.

We  note  that  Palem  and  coworkers  [7,8]  have  been  proposing  interesting  ways  of  working  with

probabilistic switches, which are noisy digital logic units in the low logic-threshold-voltage limit, in

order  to  reduce  power  dissipation.  However,  our  approach  is  different  and  independent  at  the

moment (even though the two approaches may meet at further developments) because the system

proposed in the present paper works in the absolutely zero logic-threshold-voltage limit and the very

small, sub-noise signal limit, out of the range used by Palem and coworkers [7,8]. Moreover, we

base  our  study  on  Shannon's  channel  coding  theorem  because  we  believe  that  this  theory  is  the

proper application tool to characterize the information channel capacity in digital channels. Another

relevant works are Stock's suprathreshold stochastic resonance results obtained with analog signals

[9,10]; see below.

Johnson-noise driven computer is a computer where most of the gates are driven by digital signal

voltage  U s equal  or  less  than  the  effective  Johnson-noise  voltage    s .  The  highest  possible
information content of a digital channel is given by Shannon's channel coding theorem:

C = fc 1+ p log2 p + (1- p) log2 (1- p)
  

[

]  ,

(1)

2

where  fc is the clock frequency,  p is the probability of correct bit and    1- p  is the error probability,
see Figure 1.

The lowest order of the Taylor expansion of Eq. 1 around the value   p = 0.5 yields

2

)
Dp(
2

8
3 ln2

fc   , 

C pº0.5 =
  

where   Dp = p - 0.5 .

(2)

(3)

(4)

Let us suppose that a weak digital signal (  U s(t) § s ) drives a simple comparator with zero reference
voltage  U ref . The output of the comparator simply provides the digitalized version of the signum

operation: if the input voltage is greater than zero, the output is "Hi" and if the input voltage is less

than  zero,  the  output  is  "Low".  Such  units  have  been  used  by  Stocks  [9,10]  to  propose  and

demonstrate the so-called suprathreshold stochastic resonance for analog  signals where the noise

driving was not chosen specifically Johnson-noise but arbitrary band-limited Gaussian white noise.
In the case of   U S (t) = 0 at the input ("Low"), the output shows   p = 0.5 because the Johnson-noise
has  zero  mean  and  a  symmetric  (Gaussian)  amplitude  density  function.  However,  for
  U s(t) = U sH > 0  ("High"), the input Johnson noise will be superimposed on a nonzero DC signal and
  p > 0.5 .

If  we  alternate  the  input  signal  between  "Low"  and  "High"  levels  with  the  clock  frequency,  we
dissipate the charging energy of the input capacitance  C  of the comparator both times, when we
charge the capacitor and when we discharge it, so:

By approximating the top of the Gaussian curve with a constant in our range we get:

Ps = fc
  

1
2

2
CU sH

Dp º
  

U sH
2p  s

=

U sH
2pkT / C

 .

3

Because only "High" input case yields non-zero information, we obtain from Eqs. 2,3 :

From Eqs. 3,5 we obtain that, in this range, the energy cost/bit operation is constant:

C pº0.5 =
  

1
2

 

2
U s
2pkT / C

 

8
3 ln2

fc  .

Ps
C
  

=

3p ln2
4

 kT / bit º 1.6  kT / bit  .

(5)

(6)

It  is  interesting  to  note  that  the  above  considerations  contradict  to  the  common  opinion  that  no
digital  signal  can  be  used  if  the  energy  difference  between  logic  levels  is  less  than  kT * ln(2).

However, such a statement is valid only for digital memories. The existence of Shannon's Eq. 1 and

the  derivation  above  prove  that  the  statement  does  not  hold  for  digital  signal  channels  without
information storage elements.

Today's microprocessors dissipate over 3000  kT / bit  energy (though for the low-error operation 70
kT/bit  would  be  far  enough  [6]  so  Eq.  (6)  looks  promising.  However,  even  if  we  drive  with  the

largest allowed signal to stay in the regime of Johnson-noise driven operation, we will need to drive

at least 100 parallel units to be able to reach an acceptably low error rate with majority-vote error
correction. That means, the energy dissipation will be at least 160  kT / bit  even if we neglect the
other necessary circuits of error correction.

Thus, the main advantage of Johnson-noise driven computing is an obvious lack of leakage current,

cross-talk and ground EMI problems due the very low DC voltages. An apparent disadvantage is the
large number of extra elements for error reduction.

4

References

1. 

L.B. Kish, Appl. Physics Lett. 87 (2005), Art. No. 234109.

2 .   L.  B.  Kish,  "Totally  secure  classical  communication  utilizing  Johnson  (-like)  noise  and

Kirchoff's law", Phys. Lett. A 352, 178-182, (2006); also at http://arxiv.org/physics/0509136.

3.  A.  Cho,  "Simple  noise  may  stymie  spies  without  quantum  weirdness",  Science 309, 2148,

(2005).

4.  L.B.  Kish  and  P.  Mingesz,  "Totally  secure  classical  networks  with  multipoint  telecloning
(teleportation) of classical bits through loops with Johnson-like noise", Fluct. Noise Lett. 6,
C9-C21, (2006).

5.  W. Porod, Appl. Phys. Lett. 52, 2191 (1988); and references therein; W. Porod, R.O. Grondin,
D.K. Ferry, Phys.  Rev.  Lett.  52,  232-235,  (1984);  W.  Porod,  R.O.  Grondin,  D.K.  Ferry,  G.
Porod, Phys. Rev. Lett. 52, 1206, (1984); and references therein.

6.  R.K. Cavin, V.V. Zhirnov, J.A. Hutchby, G.I Bourianoff, Fluct. Noise Lett. 5, C29-38, (2005).

7.  K.V. Palem, IEEE Trans. Comput. 54, 1123, (2005).

8. 

P. Korkmaz, B.E.S. Akgul, K.V. Palem, L.N. Chakrapani, Jap. Journ. Appl. Phys. 45, 3307-
3316, (2006).

9.  N.G. Stocks, Phys. Lett. A 279,  308-312 (2001).

10.  N.G. Stocks, Phys. Rev E 63, 041114, 1-9, (2001).

5

Figure caption

Figure 1.

Shannon's channel capacity of digital channels and the working regimes of the Johnson-noise driven
and classical computers, respectively.

6

The theoretical information content of the noisy bit

Classical
computing

Johnson-noise
driven computing

0

0.1

0.2

0.3

0.4

0.5

0.6

0.9
p  (probability of correct state)

0.8

0.7

1

)
t
i
b
(
 
 
 

f
 
/
 

C

c

1

0.8

0.6

0.4

0.2

0

Figure 1.

7


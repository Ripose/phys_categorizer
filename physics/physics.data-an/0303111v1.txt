3
0
0
2
 
r
a

M
 
7
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
1
1
3
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Setting conﬁdence intervals for bounded
parameters: a diﬀerent perspective

D.A.S. Fraser and N. Reid
Department of Statistics, University of Toronto
Toronto Canada M5S 3G3
A. Wong
Department of Mathematics and Statistics, York University
Toronto Canada M3J 1P3

February 21, 2014

Abstract

The estimation of signal frequency count in the presence of back-
ground noise has had much recent discussion in the physics literature,
and Mandelkern [1] brings the core issues to the statistical commu-
nity, in turn leading to extensive discussion by statisticians. The pri-
mary focus in [1] and in the discussion rests on conﬁdence interval
procedures. We discuss various anomalies and misleading features in
this use of conﬁdence theory, and argue that the usage is essentially
decision theoretic and is being applied in a context that invites an
inferential approach. We then extract what we view as the inference
elements, the fundamental information available from the model and
the data. This is illustrated using some simple data and some recent
data from the physics literature.

1 INTRODUCTION

Mandelkern [1] brings to the statistical community a seemingly simple sta-
tistical problem that arose in high energy physics; see for example, [2], [3].
The statistical problem may appear elementary but the original problem has

1

substantial scientiﬁc presence: as Pekka Sinervo, a coauthor of Abe et al.
[2], [3] expresses, “High energy physicists have struggled with Bayesian and
frequentist perspectives, with delays of several years in certain experimental
programmes hanging in the balance”.

≥

The core statistical problem can be expressed simply. A count variable
y is Poisson (b + µ), where b > 0 is known and µ
0. The goal is to
extract the evidence concerning the parameter µ, particularly whether or
In the physical setting the count
not the parameter is greater than zero.
y is viewed as the sum of y1 counting events from background radiation,
and y2 counting events from a possible signal. In [2] and [3], the signal is the
presence of a possible top quark and the data come from the collider detector
at Fermilab. The background radiation count y1 is modelled as Poisson(b)
and the count from the possible signal as Poisson(µ). In context there are
additional aspects to the problem: for example the background mean count
b is estimated, the data are obtained as subsets of more complex counts, and
so on. Here however we address the simpliﬁed statistical problem on its own
merits, as is largely the case in Mandelkern [1]. We write y
Poisson(b + µ)
∼
and let θ = b + µ be the Poisson mean, so the restriction is θ

b.

Much statistical literature, and most of the physics proposals cited by
Mandelkern [1], emphasize the construction of conﬁdence bands for θ at a
prescribed level of conﬁdence.

For the Poisson, a simple 95% conﬁdence interval for θ is given by (√y
±
0.98)2, based on √y being approximately distributed as N(√θ, 0.5). One
diﬃculty that this approximation ignores is the discretenss of the data. A
more serious diﬃculty concerns the lower bound b for the parameter space; for
example if y is small then the interval can be partly or completely outside the
permissible range [b,
), making nonsense of the assertion of 95% conﬁdence.
Various proposals put forward seek to modify the conﬁdence approach to
overcome these diﬃculties.

∞

The discussants of Mandelkern [1] also focus on the conﬁdence interval
approach. An exception is Gleser, who suggests the use of the “likelihood
function as a measure of evidence about the parameters of the model used
to describe the data”; and Mandelkern in his rejoinder concurrs: “it may
be most appropriate to, at least in ambiguous cases, give up the notion of
characterizing experimental uncertainty with a conﬁdence interval... and to
present the likelihood function for this purpose.”

It is our view that the emphasis on conﬁdence bounds for ﬁxed levels
has led to procedures that are not satistfactory from many points of view.

≥

2

In particular a conﬁdence interval in a technical sense is derived from de-
cision theory: we “accept” parameter values within the conﬁdence interval
and “reject” parameter values outside the interval. By contrast plotting the
likelihood function is an inferential approach.

In Section 2 we record some discussion of the uniﬁed approach and its

variants, and also record various anomalies associated with their use.

In Section 3 we present an inferential approach where inference elements
are recorded, these being the observed likelihood function and the observed p-
value function. We propose that these present the full essentials of inference
concerning the parameter, and in turn allow appropriate judgments to be
made concerning the parameter. An example is given using data from [3].

2 The uniﬁed approach and variants

Conﬁdence intervals based on the theory of optimal testing can often lead to
rather anomalous behavior. An optimality criterion typically involves averag-
ing over the sample space, and in many situations there are what Fisher called
‘recognizable subsets’ of the sample space that partition the sample space. In
this setting use of overall optimality can mean that intervals are constructed
which eﬀectively trade performance in a single instance for average perfor-
mance in a series of instances, some of which may have recognizably diﬀerent
features. In particular this can give a conﬁdence interval that is empty or a
conﬁdence interval that is the full range for the parameter: in such cases the
overt conﬁdence is clearly zero or 100% in contradiction to the prescribed or
targetted conﬁdence. For some recent discussions with examples, see Fraser
[5] where fault is strongly attached to the use of optimality criteria.

The conventional Neyman intervals applied to examples with a bounded
parameter space can also lead to anomalous conﬁdence intervals. In order
to avoid for the moment the issues associated with the discreteness of the
Poisson distribution we let y be a scalar variable with a continuous distribu-
tion stochastically increasing in a scalar parameter θ. Suppose the natural
b is prescribed.
ranges for these are (
For example, this could be the Normal (θ, 1) with θ > b. Let F (y; θ) and
f (y; θ) be the distribution and density functions for y.

) but that the restriction θ

−∞

∞

≥

,

An optimum conﬁdence interval derived for the unrestricted case may
well lap into the inappropriate region θ < b, this being the key issue in the
Poisson case and mentioned for the continuous case in Mandelkern [1].

3

The adjustments discussed in Mandelkern [1] can be described as follows.
Let yL(θ), yU (θ) be the γ and 95% + γ points of the θ distribution; these
form a 95% acceptance region. Now let γ = γ(θ) vary with θ but of course
θ-space
be restricted to the interval (0, 5%). The conﬁdence belt in the y
is the set union of the acceptance regions (yL(θ), yU (θ))
; and the y-
section of the two dimensional conﬁdence belt is a 95% conﬁdence region and
under moderate regularity will have the form (θL(y), θU (y)). The objective is
to have these sets stay within the acceptable range [b,
) by some natural-
seeming choice of the adjustment function γ(θ).

× {

∞

×

}

θ

The likelihood ratio is used as one basis for deciding which points are to
go into the acceptance interval (yL(θ), yU (θ)) and thus for determining γ(θ).
For inclusion in an acceptance interval the points are ordered from the largest
using the ratio

R =

L(θ; y)
L(˜θ; y)

(1)

≥

where L(θ; y) = f (y; θ) and ˜θ = ˜θ(y) is a reference parameter value associated
with y. The Uniﬁed Approach of Feldman & Cousins [6] takes ˜θ = ˆθ(y) to
be the maximizing θ value as calculated in conformity with the restriction
b; in the Normal(θ, 1) case, ˆθ = max(b, y). The New Ordering approach
θ
of Giunti [7] takes ˆθ to be a Bayesian expected value for θ. Using a somewhat
diﬀerent starting point Mandelkern & Shultz [8] obtain likelihood from the
distribution of ˆθ(y), which is a marginalisation from the distribution of y
itself. For the normal case this ˆθ does not depend on y for y < b and not
surprisingly the conﬁdence intervals obtained by this approach are found not
to depend on y for y < b.

The use of these optimizing or ordering criteria can have rather insidious
eﬀects. As noted, the criteria involve shifting the acceptances to the left
for low parameter values so that the 2.5% tail probabilities on the left and
the right are changed to have less on the left and more on the right; this
has the eﬀect for small data values of shifting the conﬁdence intervals to the
right, away from the excluded parameter value range. The disturbing result
however is that the lower conﬁdence bound is no longer a 2.5% bound but
something larger and perhaps undeﬁned. And the upper conﬁdence bound is
no longer a 97.5% bound but something larger and perhaps undeﬁned. The
conﬁdence interval has been moved around in response to the optimizing
or ordering criterion. Do the bounds of the conﬁdence interval then have
any real statistical meaning individually? And this in a context where the

4

location of the conﬁdence bounds in relation to the lower limit b is of funda-
mental and focal interest. Is this proper science to move around the interval
of parameter values that get statistical sanction? We feel the use of ﬁxed
level conﬁdence intervals particularly two sided intervals is inappropriate in
the present context. In Section 3 we recommend presenting the available ev-
idence: Saying it as it is. And then leaving the scientiﬁc issues to judgement
given the evidence.

|

≤

≤

≤

y1

For the Poisson problem described in the introduction, Roe & Woodroofe
[9] propose the use of certain conditional probabilities as the basis for the
conﬁdence belt construction following Feldman & Cousins [6]. As before let
y = y1+y2 where y1 is the background Poisson (b) and y2 is Poisson (µ). They
y0; and accordingly they
note that if y = y0 is observed then necessarily y1
y0, say
recommend the use of the conditional distribution of y given y1
y0; µ) as recorded as (4) in Mandelkern [1]. However, the
g(y; µ) = f (y
variable y1 is not an observable variable and hence not ancillary in the usual
sense. And in accord with this the proposed conditioning does not generate
a partition of the sample space; this was noted in Woodroofe & Wang [10].
If (y1, y2) = (0, 0), then y = 0 and it follows that the indicated conditioning
is that (y1, y2) is in the set C =
corresponding to
y1
0. If however we consider another point in C, say (y1, y2) = (0, 1) we
would have y = 1 and the indicated conditioning would be that (y1, y2) is
in the set C ′ =
(0, 0), (0, 1), (0, 2), . . . , (1, 0), (1, 1), (1, 2), . . .
corresponding
}
to y1
1; this is clearly diﬀerent from C. Thus the nominal conditional
distribution does not satisfy the standard conditions for validity in describing
conditional frequencies given observed information. Also not surprisingly,
Mandelkern [1] notes that there is a related undercoverage which can be
severe for the nominal conﬁdence intervals constructed.

(0, 0), (0, 1), (0, 2), . . .
}

≤

≤

{

{

3 Likelihood and p-value functions

With a model function and observed data the recommendation to plot the
observed likelihood has had a long presence in statistics, having appeared in
Fisher [11]. Among the statistical discussants of [1], Gleser [4] was alone in
recommending this approach.

As a simple example consider a sample (y1, . . . , yn) form the Normal
0). The likelihood function is L(µ) = cφ(n1/2(¯y
µ)/σ0) and the p-
µ)/σ0) where φ and Φ are the normal density

(µ, σ2
value function is Φ(n1/2(¯y

−

−

5

and distribution functions. One could reasonably plot them one above the
other for ease of comparison. Because µ is a location parameter, we have
the formal property that the p-value function is the right tail integral of the
likelihood function.

We now return to the Poisson (θ) with θ = b + µ where b is known and
0. The likelihood function from data y is

µ

≥

L(θ) = cθye−θ

where θ = b + µ. This can be plotted as a function of µ for µ in (
): for
µ in [0,
) it describes the probability at the observed data point in accord
with the model, and for µ in [
b, 0) it can serve as a diagnostic concerning
b. To accommodate the discreteness we propose that the p-value function at
the data y be taken to be the interval

∞

∞

−

−

b,

p(θ) =

F −(y; θ), F (y; θ)

{

}

−

of associated numerical values, where F (y; θ) is the Poisson(θ) distribution
function and F −(y; θ) is the probability up to, but not including, y and is
given by F (y
1; θ). Thus an observed y leads in general to a continuum of
numerical p-values for each θ being assessed. This proposal acknowledges the
discreteness explicitly and yet ensures the familiar sampling property of the
p-value function, that it have a uniform distribution on (0, 1). Other aspects
of the discreteness problem are addressed in Brown et al. [12] and Baker [13].
As a simple example consider b = 2 and data y0 = 3. The likelihood
and p-value functions are recorded in Figure 1. The p-value for a chosen µ is
now an interval of p-values. The likelihood for µ is easily understood. The
interpretation of the p-value for a given data value is exactly analogous to
it expresses the
the percentile score on, for example, a standardized test:
percentile position of the data point relative to the parameter. For the null
condition θ = b or µ = 0 the p-value interval is (0.677, 0.857).

In Abe et al.

[3] after preliminary simpliﬁcation from their Table 1 we
have b = 6.7 with y0 = 17. The likelihood function and p-value functions are
plotted in Figure 2. For the null condition θ = b or µ = 0 the p-value interval
is (0.99940, 0.99978) thus oﬀering a clear statement concerning whether or
not µ > 0.

6

Figure 1: The likelihood function (top) and p-value function (bottom) for
the Poisson model, with b = 2 and y0 = 3. For µ = 0 the p-value interval is
(0.677, 0.857).

d
o
o
h

i
l

e
k

i
l

)
u
m
(
p

0
2
.
0

5
1
.
0

0
1
.
0

5
0
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2

.
0

0
.
0

0

0

..........
.......
.....
.......
.........
..........
...........
.............
..............
...............
................
.................
..................
...................
...................
....................
.....................
.....................
......................
......................
.......................
......................
......................
......................
.......................
.......................
......................
.....................
.....................
.....................
.....................
....................
....................
...................
..................
..................
.................
.................
................
...............
...............
..............
..............
..............
.............
............
............
............
...........
...........
..........
.........
.........
.........
........
........
.......
.......
.......
......
......
......
.....
.....
.....
...........................................................

10

10

5

mu

5

mu

7

Figure 2: The likelihood function (top) and p-value function (bottom) for
the Poisson model, with b = 6.7 and y0 = 17. For µ = 0 the p-value interval
is (0.99940, 0.99978).

d
o
o
h

i
l

e
k

i
l

)
u
m
(
p

0
1
.
0

6
0
.
0

2
0
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2

.
0

0
.
0

0

10

20

30

mu

........................
....
....
.....
......
.....
......
......
.......
........
........
........
.........
.........
........
.........
.........
..........
.........
..........
.........
..........
..........
.........
..........
..........
.........
.........
.........
.........
.........
........
........
........
.......
.......
.......
......
.....
......
......
.....
....
....
....
....
..........................................

0

10

20

30

mu

8

[1] M. Mandelkern, Statist. Sci. 17 149 (2002).
[2] F. Abe et al., Phys. Rev. Lett. 7(2) 225 (1994).
[3] F. Abe et al., Phys. Rev. Lett. 74(14) 2626 (1995).
[4] G. Casella et al., Statist. Sci 17, 159 (2002).
[5] D.A.S. Fraser, Statist. Sci, to appear (2003).
[6] G.J. Feldman and R.D. Cousins, Phys. Rev. D 57 3873 (1998).
[7] C. Giunti, Phys. Rev. D 59 113000 (1999).
[8] M. Mandelkern and J. Shultz, J. High Energy Phys. 11 036 (2000).
[9] B. P. Roe and M.B. Woodroofe, Phys. Rev. D 60 053009 (1999).
[10] M.B. Woodroofe and H. Wang, Ann. Statist. 28 1561 (2000).
[11] R.A. Fisher, Statistical Methods and Scientiﬁc Inference. Edinburgh:

Oliver and Boyd (1956).

[12] L.T. Brown, T. Cai and A. Dasgupta, Statist. Sci. 16, 101, (2001).
[13] L. Baker, Amer. Statist. 56 85, (2002).

9


6
0
0
2
 
v
o
N
 
5
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
5
1
1
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Mixture models and exploratory data analysis in networks

M. E. J. Newman and E. A. Leicht
Department of Physics, University of Michigan, Ann Arbor, MI 48109, USA

Networks are widely used in the biological, physical, and social sciences as a concise mathematical
representation of the topology of systems of interacting components. Understanding the structure
of these networks is one of the outstanding challenges in the study of complex systems. Here
we describe a technique for detecting structural features in large-scale network data which works
by dividing the nodes of a network into classes such that the members of each class have similar
patterns of connection to other nodes. Some previously studied network features such as community
structure and bipartite structure can be regarded as examples of such divisions, but the structures
we consider are substantially more general than this. Using the machinery of probabilistic mixture
models and the expectation-maximization algorithm, we show that it is possible to detect, without
prior knowledge of what we are looking for, a very broad range of types of structure in networks,
including types that have not been considered explicitly in the past.

INTRODUCTION

In the last few years, networks have found use in many
ﬁelds as a powerful and illuminating tool for represent-
ing the structure of complex systems [1–4]. Metabolic,
protein interaction, and genetic regulatory networks are
now heavily studied in biology and medicine, the Internet
and the world wide web in computer and information sci-
ences, food webs and other species interaction networks
in ecology, and networks of personal or social contacts in
epidemiology, sociology, and the management sciences.

The study of networks goes back much further than
the current surge of interest in it, but recent work dif-
fers fundamentally from earlier studies because of the
sheer scale of the networks being analyzed. The networks
studied 50 years ago by pioneers in the information and
social sciences had, typically, a few dozen vertices and
were small enough that they could easily be drawn on a
piece of paper and perused for interesting features.
In
the 21st century, on the other hand, networks of thou-
sands or millions of vertices are not unusual and network
data on this scale cannot easily be represented in a way
that allows quantitative analysis to be conducted by eye.
Instead we have been obliged to turn to topological mea-
sures, computer algorithms, and statistics to understand
the structure of today’s networks. Much of the current
research on networks is, in eﬀect, aimed at answering the
question “How can we tell what a network looks like,
when we can’t actually look at it?”

The typical approach to this problem involves deﬁn-
ing measures or statistics to quantify network features of
interest: centrality indices [5, 6], degree distributions [7–
9], clustering coeﬃcients [10], community structure mea-
surements [11, 12], correlation functions [13, 14], and mo-
tif counts [15] are all invaluable tools for shedding light on
the topology of networks. Our reliance on measures like
these, however, has a downside: they require us to know
what we are looking for in advance before we can decide
what to measure. People measure correlation functions,
for instance, because (presumably) they think there may
be interesting correlations in a network; they measure

degree distributions because they believe the degree dis-
tribution may show interesting features. This approach
has certainly worked well—many illuminating discover-
ies have been made this way. But it raises an uncom-
fortable question: could there be interesting and relevant
structural features of networks that we have failed to ﬁnd
simply because we haven’t thought to measure the right
thing?

To some extent this is an issue with the whole of scien-
tiﬁc endeavor. In any ﬁeld thinking of the right question
can demand as much insight as thinking of the answer.
However, there are also things we can do to help our-
selves. In this paper we describe a technique that allows
us to detect structure in network data while making only
rather general assumptions about what that structure is.
Methods of this type are referred to by statisticians as
“exploratory” data analysis techniques, and we will make
use of a number of ideas from the statistical literature in
the developments that follow.

We focus on the problem of classifying or clustering the
vertices of a network into groups such that the members
of each group are similar in some sense. This already nar-
rows the types of structure we consider substantially, but
leaves a large and useful selection of types still in play.
Some of these types have been considered by researchers
in the past. For instance, methods for detecting “commu-
nity structure”, “homophily,” or “assortative mixing” in
networks involve dividing vertices into groups such that
the members of each group are mostly connected to other
members of the same group [11, 12]. Methods for detect-
ing “bipartite” or “k-partite” structure or “disassortative
mixing” look for groups such that vertices have most of
their connections outside their group [16–18]. Methods
for detecting vertex similarity or “structural equivalence”
aim to group together vertices that have common net-
work neighbors [5, 19]. Eﬀective techniques have been
developed that can detect structure of each of these sep-
arate types, but what should we do if we do not know in
advance which type to expect or if our network does not
ﬁt one of the standard types and has some other form
entirely whose existence we are not even aware of? We
can try in turn each of the established methods, looking

for signiﬁcant results, but in many cases we may fail to
detect the truly interesting features of the network.

So instead we propose a diﬀerent approach that
employs a very general deﬁnition of vertex classes,
parametrized by an extensive number of variables and
hence encompassing an essentially inﬁnite variety of
structural types in the limit of large network size, in-
cluding the standard types described above—community
structure, bipartite structure, and so forth—but also in-
cluding many others. We show that it is possible to de-
tect structure of these types in observed network data
quickly and simply using the iterative technique known as
the expectation-maximization algorithm and, crucially,
that we can do so without specifying in advance which
particular type we are looking for: the algorithm simul-
taneously optimizes the assignment of vertices to groups
and the parameters deﬁning the meaning of those groups,
so that upon completion the calculation tells us not only
the best way of grouping the vertices but also the def-
initions of the groups themselves. We demonstrate the
algorithm with applications to a selection of real-world
networks and computer-generated test networks.

THE METHOD

The method we describe is based on a mixture model, a
standard construct in statistics, though one that has not
yet found wide use in studies of networks. The method
works well for both directed and undirected networks,
but is somewhat simpler in the directed case, so let us
start there.

Suppose we have a network of n vertices connected
by directed edges, such as a web graph or a food web.
The network can be represented mathematically by an
adjacency matrix A with elements Aij = 1 if there is an
edge from i to j and 0 otherwise.

Suppose also that the vertices fall into some number c
of classes or groups and let us denote by gi the group
to which vertex i belongs. We will assume that these
group memberships are unknown to us and that we can-
not measure them directly. In the language of statistical
inference they are “hidden” or “missing” data. Our goal
is to infer them from the observed network structure.
(For the moment we will treat the number of groups c
as given, although ultimately we will want to infer this
too.) To infer the group memberships we adopt a stan-
dard approach for such problems: we propose a general
(mixture) model for the groups and their properties, ﬁt
the model to our data, and then predict the group mem-
berships using the best-ﬁt model.

The model we use is a stochastic one that parametrizes
the probability of each possible conﬁguration of group as-
signments and edges as follows. We deﬁne θri to be the
probability that a (directed) link from a particular vertex
in group r connects to vertex i. In the world wide web,
for instance, θri would represent the probability that a
hyperlink from a web page in group r links to web page i.

2

In eﬀect θri represents the “preferences” of vertices in
group r about which other vertices they link to. In our
approach it is these preferences that deﬁne the groups: a
“group” is a set of vertices that all have similar patterns
of connection to others [20]. (The idea is similar in philos-
ophy to the block models proposed by White and others
for the analysis of social networks [21], although the re-
alization and the mathematical techniques employed are
diﬀerent.)

We also deﬁne πr be the (currently unknown) fraction
of vertices in group or class r, or equivalently the prob-
ability that a randomly chosen vertex falls in r. The
parameters πr, θri satisfy the normalization conditions

c

Xr=1

n

Xi=1

πr = 1,

θri = 1.

(1)

Between them these quantities specify a network model
ﬂexible enough to describe many diﬀerent types of struc-
ture. For instance, if θri is larger than average for ver-
tices i that are themselves members of group r, the
model displays assortative mixing—the standard commu-
nity structure studied widely in previous work, with ver-
tices being connected primarily within their own groups.
Conversely, if θri is large for vertices not in r we have
disassortative or k-partite structure. And many other
more complex types of structure are possible for other
parameter choices.

The quantities in our theory thus fall into three classes:
measured data {Aij}, missing data {gi}, and model pa-
rameters {πr, θri}. To simplify the notation we will
henceforth denote by A the entire set {Aij} and simi-
larly for {gi}, {πr}, and {θri}. The standard framework
for ﬁtting models like the one above to a given data set
is likelihood maximization, in which one maximizes with
respect to the model parameters the probability that the
data were generated by the given model. Maximum like-
lihood methods have occasionally been employed in net-
work calculations in the past, for instance in the hier-
archy analysis of Clauset et al. [22], as well as in many
other problems in the study of complex systems more
generally.
In the present case, our ﬁtting problem re-
quires us to maximize the likelihood Pr(A, g|π, θ) with
respect to π and θ, which can be done by writing

Pr(A, g|π, θ) = Pr(A|g, π, θ) Pr(g|π, θ),

(2)

where

Pr(A|g, π, θ) =

θAij
gi,j, Pr(g|π, θ) =

πgi ,

(3)

Yij

Yi

so that the likelihood is

Pr(A, g|π, θ) =

πgi

(cid:20)

Yi

Yj

θAij
gi,j(cid:21)

.

(4)

In fact, one commonly works not with the likelihood itself
but with its logarithm:

L = ln Pr(A, g|π, θ) =

ln πgi +

Aij ln θgi,j

Xi h

Xj

. (5)
i

The maximum of the two functions is in the same place,
since the logarithm is a monotonically increasing func-
tion.

Unfortunately, g is unknown in our case, which
means the value of the log-likelihood is also unknown.
We can however calculate the probability distribution
Pr(g|A, π, θ) of g given the observed network structure A
and the model parameters π, θ and hence the distribu-
tion of the log-likelihood. The expected value of the log-
likelihood is then given by averaging over g thus:

c

c

L =

. . .

Pr(g|A, π, θ)

ln πgi +

Aij ln θgi,j

Xg1=1

Xgn=1

Xi h

Xj

i

=

=

Pr(gi = r|A, π, θ)
h

Xir

ln πr +

Aij ln θrj

Xj

i

qir

ln πr +

Aij ln θrj

Xir

h

Xj

,
i

(6)

where to simplify the notation we have deﬁned qir =
Pr(gi = r|A, π, θ), which is the probability that vertex i
is a member of group r.
(In fact, it is precisely these
probabilities that will be the principal output of our cal-
culation.)

This expected log-likelihood represents our best esti-
mate of the value of L and its maximum represents our
best estimate of the most likely values of the model pa-
rameters. Finding this maximum still presents a prob-
lem, however, since the calculation of q requires the val-
ues of π and θ and the calculation of π and θ requires q.
The solution is to adopt an iterative, self-consistent ap-
proach that evaluates both simultaneously. This type of
approach, known as an expectation-maximization or EM
algorithm, is common in the literature on missing data
problems. In its modern form it is usually attributed to
Dempster et al. [23], who built on theoretical foundations
laid previously by a number of other authors [24].

Following the conventional derivation of the method,
we calculate the expected probabilities q of the group
memberships given π, θ and the observed data using
Bayes’ theorem:

qir = Pr(gi = r|A, π, θ)

= Pr(A|gi = r, π, θ)

Pr(gi = r|π, θ)
Pr(A|π, θ)

.

(7)

The factors on the right are given by

Pr(A|gi = r, π, θ) =

. . .

δgi,r Pr(A|g, π, θ)

c

c

Xg1=1
c

Xgn=1
c

. . .

δgi,r

Xg1=1

Xgn=1

θAkj
gk,j

Ykj

c

θAij
rj (cid:21)(cid:20)Yk6=i

Xt=1 Yj

θAkj
tj (cid:21)

,

=

=

(8)

(9)

(cid:20)Yj
Pr(gi = r|π, θ) = πr,

3

(11)

and

Pr(A|π, θ) =

Pr(A|gi = s, π, θ) Pr(gi = s|π, θ)

Xs

=

πs

Xs

θAij
sj (cid:21)(cid:20)Yk6=i

(cid:20)Yj

Xt=1 Yj

θ

Akj
tj (cid:21)

, (10)

c

where δij is the Kronecker δ symbol and we have made
use of Eq. (3). Substituting into (7), we then ﬁnd

qir =

πr

j θAij

rj

Q
s πs

j θAij

sj

.

P

Q

Note that qir correctly satisﬁes the normalization condi-
tion

r qir = 1.

P

Once we have the values of the qir, we can use them to
evaluate the expected log-likelihood, Eq. (6), and hence
to ﬁnd the values of π, θ that maximize it. One advan-
tage of the current approach now becomes clear: because
the qir are known, ﬁxed quantities, the maximization can
be carried out purely analytically, obviating the need for
numerical techniques such as Markov chain Monte Carlo.
Introducing Lagrange multipliers to enforce the normal-
ization conditions, Eq. (1), and diﬀerentiating, we easily
ﬁnd that the maximum of the likelihood occurs when

πr =

qir,

θrj =

(12)

1
n Xi

i Aij qir
i kiqir

,

P
P

where ki =
j Aij is the out-degree of vertex i and we
have explicitly evaluated the Lagrange multipliers using
the normalization conditions.

P

Equations (11) and (12) deﬁne our expectation-
maximization algorithm.
Implementation of the algo-
rithm consists merely of iterating these equations to con-
vergence from a suitable set of starting values. The out-
put of the algorithm is the probability qir for each vertex
to belong to each group, plus the probabilities θri of links
from vertices in each group to every other vertex, the lat-
ter eﬀectively giving the deﬁnitions of the groups.

The developments so far apply to the case of a directed
network. Most of the networks studied in the recent lit-
erature, however, are undirected. The model used above
is inappropriate for the undirected case because its edges
represent an inherently asymmetric, directed relationship
between vertices in which one vertex chooses unilaterally
to link to another, the receiving vertex having no say
in the matter. The edges in an undirected network, by
contrast, typically represent symmetric relationships. In
a social network of friendships, for instance, two people
can become friends only if both choose to be friendly to-
wards the other. To extend our method to undirected
networks we need to incorporate this symmetry into our
model, which we do as follows. Once again we deﬁne θri
to be the probability that a vertex in group r “chooses”
to link to vertex i, but we now specify that a link will
be formed only if two vertices both choose each other.
Thus the probability that an edge falls between between

vertices i and j, given that i is in group s and j is in
group r, is θriθsj, which is now symmetric. This proba-
ij θriθsj = 1
bility satisﬁes the normalization condition
for all r, s and setting r = s we ﬁnd

P

2

θriθrj =

θri

= 1,

Xij

hXi

i

(13)

and hence

i θri = 1 as before.

Now the probability Pr(A|g, π, θ) in Eq. (4) is given by

P

Pr(A|g, π, θ) =

θgi,jθgj ,i

Yi>j(cid:2)

Aij =
(cid:3)

Yij

θAij
gi,j,

(14)

exactly as in the directed case, where we have made use
of the fact that Aji = Aij for an undirected network.
(We have also assumed there are no self-edges in the
network—edges that connect a vertex to itself—so that
Aii = 0 for all i.)

The remainder of the derivation now follows as before
and results in precisely the same equations, (11) and (12),
for the ﬁnal algorithm.

EXAMPLE APPLICATIONS

For our ﬁrst examples of the operation of our method,
we apply it to two small networks, one known to have
conventional assortative community structure, the other
known to have approximately bipartite (i.e., disassorta-
tive) structure. The ﬁrst is the much-discussed “karate
club” network of friendships between 34 members of a
karate club at a US university, assembled by Zachary [25]
by direct observation of the club’s members. This net-
work is of particular interest because the club split in
two during the study as a result of an internal dispute
and Zachary recorded the membership of the two factions
after the split.

Figure 1 shows the best division of this network into
two groups found using the expectation-maximization
method with c set equal to 2. The shades of the ver-
tices in the ﬁgure represent the values of the variables qi1
for each vertex (or equivalently the values of qi2, since
qi1 + qi2 = 1 for all i.) As we can see the algorithm as-
signs most of the vertices strongly to one group or the
other; in fact, all but 13 vertices are assigned 100% to
one of the groups. Thus the algorithm ﬁnds a strong
split into two clusters in this case, and indeed if one sim-
ply divides the vertices into two groups according to the
cluster to which each is most strongly assigned, the result
corresponds perfectly to the division observed in real life
(denoted by the shaded regions in the ﬁgure).

But the algorithm reveals much more about the net-
work than this. First, where appropriate it can return
probabilities for assignment to the two groups that are
not 0 or 1 but lie somewhere between these limits, and
for 13 of the vertices in this network it does so. For some
of these 13 vertices the values of qir are still very close to
0 or 1, but for some they are not. Inspection of the ﬁgure

4

FIG. 1: Application of our decomposition method to the
“karate club” network of Ref. [25]. The two shaded regions
indicate the division of the network in the real world, while
the shades of the individual vertices indicate the decomposi-
tion chosen by the algorithm. Sizes of the vertices indicate the
probabilities θ1i for each vertex to be connected to vertices in
the left group.

reveals in particular a small number of vertices with in-
termediate shades of gray along the border between the
groups. There has been some discussion in the recent
literature of methods for divining “fuzzy” or overlapping
groups in networks [17, 26–28]: rather than dividing a
network sharply into groups, it is sometimes desirable
to assign vertices to more than one group. The present
method accomplishes this task in an elegant and rigorous
fashion. The values of qir spell out exactly which group
or groups each vertex belongs to and in what proportions.
The algorithm also returns the distributions or prefer-
ences θri for connections from vertices in group r to each
other vertex i. For instance, in Fig. 1 we indicate by the
sizes of vertices the distribution θ1i of connections from
vertices in group 1, which is the left-hand group in the
ﬁgure, to each other vertex. As we can see, two vertices
central to the group have high connection probabilities,
while some of the more peripheral vertices have smaller
probabilities. Thus the values of θri behave as a kind of
centrality measure, indicating how important a particu-
lar vertex is to a particular group. This could form the
basis for a practical measure of within-group inﬂuence or
attraction in social or other networks. Note that in this
case this measure is not high for vertices that are central
to the other group, group 2; this measure is sensitive to
the particular preferences of the vertices in just a single
group.

We can take the method further. In Fig. 2 we show
the results of its application to an adjacency network
of English words taken from Ref. [17]. In this network
the vertices represent 112 commonly occurring adjectives
and nouns in a particular body of text (the novel David
Copperﬁeld by Charles Dickens), with edges connecting
any pair of words that appear adjacent to each other at
any point in the text. Since adjectives typically occur
next to nouns in English, most edges connect an adjec-
tive to a noun and the network is thus approximately
bipartite. This can be seen clearly in the ﬁgure, where

assortative

disassortative

5

1

s
s
e
c
c
u
s

0.5

FIG. 2: The adjacency network of English words described
in the text. The two shaded groups contain adjectives and
nouns respectively and the shades of the individual vertices
represent the classiﬁcation found by the algorithm.

0

0.1

max. modularity
min. modularity
this paper

1
probability ratio  pout /pin

10

the two shaded groups represent the adjectives and nouns
and most edges are observed to run between groups.

Analyzing this network using our algorithm we ﬁnd
the classiﬁcation shown by the shades of the vertices.
Once again most vertices are assigned 100% to one class
or the other, although there are a few ambiguous cases,
visible as the intermediate shades. As the ﬁgure makes
clear, the algorithm’s classiﬁcation corresponds closely to
the adjective/noun division of the words—almost all the
black vertices are in one group and the white ones in the
other. In fact, 89% of the vertices are correctly classiﬁed
by our algorithm in this case.

The crucial point to notice, however, is that the algo-
rithm is not merely able to detect the bipartite structure
in this network, but it is able to do so without being told
in advance that it is to look for bipartite structure. The
exact same algorithm, unmodiﬁed, ﬁnds both the assor-
tative structure of Fig. 1 and the disassortative structure
of Fig. 2. This is the strength of the present method: it
is able to detect a wide range of structure types with-
out knowing in advance what type is expected. Other
methods, such as modularity optimization [17], are able
to detect both assortative and disassortative structures,
but separate algorithms must be used in each case, in
eﬀect requiring us to stipulate in advance what we are
looking for.

To emphasize this point further, consider Fig. 3, in
which we show the results of the application of our
method to a set of computer generated networks.
In
this test, we generated networks of ﬁxed size n = 128,
divided into two groups of 64 vertices each. Edges were
placed between pairs of vertices in the same group with
probability pin and between pairs in diﬀerent groups with
probability pout. We then varied the ratio pout/pin of the
two probabilities, while keeping the mean degree of all
vertices ﬁxed, in this case at 16. When pout/pin takes
values below 1, we thus produce a network with assor-
tative mixing, while for values above 1 the network is
disassortative.

The ﬁgure shows how successful (or unsuccessful) our
algorithm is in detecting the known groups in these net-
works, as quantiﬁed using the mutual information index

FIG. 3: Results of the application of three algorithms to a
set of computer generated networks with two groups each.
The horizontal axis varies the structure of the networks from
assortative to disassortative, while the vertical axis indicates
the success of the algorithms at detecting the groups, as mea-
sured by the mutual information index of Danon et al. [12].
Each point is averaged over 100 network realizations.

of Danon et al. [12], which is 1 when the groups are identi-
ﬁed perfectly and 0 when there is no correlation between
the true groups and those found by the algorithm. The
circles (•) in the ﬁgure show the results for our algorithm
and as we can see the algorithm successfully detects the
known groups for values of pout/pin both above and be-
low 1, i.e., for both assortative and disassortative cases.
When the ratio is close to 1, meaning that edges are
placed without regard for the group structure, then, un-
surprisingly, the algorithm is unable to detect the groups,
since the network contains no signature of their presence.
The two remaining curves in the ﬁgure show the
performance of the spectral modularity maximization
(squares (cid:4)) and minimization (triangles N) algorithms
of Ref. [17], which are designed speciﬁcally to detect as-
sortative and disassortative structure respectively. Two
interesting features deserve comment. (1) The special-
ized spectral algorithms slightly out-perform our maxi-
mum likelihood method on the tasks for which they were
designed—they are able to detect structure for values of
pout/pin slightly closer to 1. This is not surprising: the
spectral algorithms are, in a sense, given more informa-
tion to start with, since we tell them what type of struc-
ture to look for. The maximum likelihood algorithm on
the other hand is told very little about what to look for
and has to work more out for itself. (2) On the other
hand, the modularity-based algorithms fail to detect any
structure outside their domains of validity. The modu-
larity maximization method is incapable of detecting the
disassortative structure present for pout/pin > 1, and the
minimization method is similarly incapable of detecting
assortative structure. This illustrates the advantages of
the present method, as a ﬂexible technique that detects

assortative

CHOICE OF NUMBER OF GROUPS

6

e
v
i
t
a
t
r
o
s
s
a
s
i
d

A

B

C

D

In the examples of the previous section, the number
of clusters c was chosen in advance to correspond to the
known number of groups of vertices in each network. Of-
ten, however, this number is not known. How should we
proceed in this case? This question arises in other appli-
cations of mixture models and a number of approaches
have been developed, although none seems perfect [29].
Most are based on maximization with respect to c of
the marginal likelihood, which is the probability of the
data A, g given the number of clusters c:

Pr(A, g|c) =

Pr(A, g|π, θ, c) Pr(π, θ|c) dπ dθ.

(15)

Z

Unfortunately, the integral cannot normally be com-
pleted in closed form, but we can derive an approximate
result by using a saddle-point expansion in which we rep-
resent ln Pr(A, g|π, θ, c) by its expected value, Eq. (6),
and then expand to leading (i.e., quadratic) order about
the position ˜π, ˜θ of the maximum found by our iterative
algorithm. Assuming a prior probability Pr(π, θ|c) on
the model parameters that varies slowly about the maxi-
mum, we can perform the resulting Gaussian integrals to
derive an expression for Pr(A, g|c) in terms of the value
at the maximum and the determinant of the correspond-
ing Hessian matrix. This expression is still diﬃcult to
evaluate because we do not normally know the Hessian,
but Schwarz [30] has suggested that in cases where the
system size n is large, it may be acceptable to drop terms
below leading order in n, which gets rid of the terms in-
volving the Hessian. In the present case, this results in
an expression for the marginal likelihood thus:

ln Pr(A, g|c) ≃ L − 1

2 cn ln m,

(16)

where m is the number of edges in the network and L
is given by Eq. (6). Within this approximation, the cor-
rect choice for the number of components c is the one
that maximizes this expression. This approximation is
not always a good one, however, since it discards terms
that may be only slightly smaller than the dominant
terms. (Terms O(n ln m) are retained while terms O(n)
are dropped.) In practice, it is often found to err on the
conservative side, underestimating the appropriate num-
ber of groups. Akaike [31] has proposed an alternative
and less conservative criterion, which in our nomencla-
ture is equivalent to maximizing L − cn, and we have
found this to give better results in some cases. A more
detailed discussion of these developments is given else-
where (Leicht and Newman, in preparation).

CONCLUSIONS

In this paper we have described a method for ex-
ploratory analysis of network data in which the ver-
tices in a network are classiﬁed or clustered into groups

FIG. 4: Results of the application of our method to the four-
group network described in the text, which has assortative
mixing with respect to the split AB/CD but disassortative
mixing with respect to the split AC/BD. (The probability of
edges between groups A and B and between groups C and D
in this network is 3 times as great as the probability of other
edges.) The method divides the vertices into groups as in-
dicated by the four colors, which correspond closely to the
original groups used to create the network, denoted by the
dashed boxes.

whatever type of structure is present, rather than being
focused on answering one speciﬁc question.

Lest we give the impression, however, that the point
of our method is solely to detect simple assortative and
disassortative mixing in networks, let us give one more
example. In Fig. 4 we show a computer-generated net-
work composed of four groups whose connection patterns
are neither purely assortative nor disassortative. In this
network we have given a high probability of connection
to edges that run between groups A and B, and between
groups C and D. All other pairs have a lower probabil-
ity of connection. In eﬀect, this network is assortative
with respect to the division between AB and CD, and
disassortative with respect to the division between AC
and BD. None of the standard methods for detecting as-
sortative or disassortative divisions will detect all four
groups in this network—each will detect only one of two
possible divisions into two groups. As the ﬁgure shows,
however, our method with c = 4 has no diﬃculty de-
tecting the more complex structure: the shades of the
vertices show the division found by the method, which
corresponds well with the known structure. Moreover an
examination of the ﬁnal values of the model parameters
θ tells us exactly what type of structure the algorithm
has discovered. In principle, considerably more complex
structures than this can be detected as well.

based on the observed patterns of connections between
them. The method is more general than previous clus-
tering methods, making use of maximum likelihood tech-
niques to classify vertices and simultaneously determine
the deﬁnitive properties of the classes. The result is a
remarkably simple algorithm that is capable of detecting
a broad range of structural signatures in networks, in-
cluding conventional community structure, bipartite or
k-partite structure, fuzzy or overlapping classiﬁcations,
and many mixed or hybrid structural forms that have not
been considered explicitly in the past. We have demon-
strated the method with applications to a variety of
examples, including real-world networks and computer-
generated test networks. The method’s strength is its
ﬂexibility, which will allow researchers to probe observed

networks for very general types of structure without hav-
ing to specify in advance what type they expect to ﬁnd.

7

Acknowledgments

The authors thank Aaron Clauset, Carrie Ferrario,
Cristopher Moore, and Kerby Shedden for useful conver-
sations and Northwestern University for hospitality and
support while part of this work was conducted. The work
was funded in part by the National Science Foundation
under grant number DMS–0405348 and by the James S.
McDonnell Foundation.

[1] M. E. J. Newman, A.-L. Barab´asi, and D. J. Watts, The
Structure and Dynamics of Networks. Princeton Univer-
sity Press, Princeton (2006).

[2] R. Albert and A.-L. Barab´asi, Statistical mechanics of
complex networks. Rev. Mod. Phys. 74, 47–97 (2002).
[3] M. E. J. Newman, The structure and function of complex

networks. SIAM Review 45, 167–256 (2003).

[4] S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and D.-
U. Hwang, Complex networks: Structure and dynamics.
Physics Reports 424, 175–308 (2006).

[5] S. Wasserman and K. Faust, Social Network Analysis.

Cambridge University Press, Cambridge (1994).

[6] J. Scott, Social Network Analysis: A Handbook. Sage,

London, 2nd edition (2000).

[7] R. Albert, H. Jeong, and A.-L. Barab´asi, Diameter of the

world-wide web. Nature 401, 130–131 (1999).

[8] M. Faloutsos, P. Faloutsos, and C. Faloutsos, On power-
law relationships of the internet topology. Computer
Communications Review 29, 251–262 (1999).

[9] J. M. Kleinberg, S. R. Kumar, P. Raghavan, S. Ra-
jagopalan, and A. Tomkins, The Web as a graph: Mea-
surements, models and methods. In T. Asano, H. Imai,
D. T. Lee, S.-I. Nakano, and T. Tokuyama (eds.), Pro-
ceedings of the 5th Annual International Conference on
Combinatorics and Computing, number 1627 in Lecture
Notes in Computer Science, pp. 1–18, Springer, Berlin
(1999).

[10] D. J. Watts and S. H. Strogatz, Collective dynamics of
‘small-world’ networks. Nature 393, 440–442 (1998).
[11] M. Girvan and M. E. J. Newman, Community structure
in social and biological networks. Proc. Natl. Acad. Sci.
USA 99, 7821–7826 (2002).

[12] L. Danon, J. Duch, A. Diaz-Guilera, and A. Arenas,
Comparing community structure identiﬁcation. J. Stat.
Mech. p. P09008 (2005).

[13] R. Pastor-Satorras, A. V´azquez, and A. Vespignani, Dy-
namical and correlation properties of the Internet. Phys.
Rev. Lett. 87, 258701 (2001).

[14] M. E. J. Newman, Assortative mixing in networks. Phys.

Rev. Lett. 89, 208701 (2002).
[15] R. Milo, S. Shen-Orr, S.

Itzkovitz, N. Kashtan,
D. Chklovskii, and U. Alon, Network motifs: Simple
building blocks of complex networks. Science 298, 824–

827 (2002).

[16] P. Holme, F. Liljeros, C. R. Edling, and B. J. Kim, Net-
work bipartivity. Phys. Rev. E 68, 056107 (2003).
[17] M. E. J. Newman, Finding community structure in net-
works using the eigenvectors of matrices. Phys. Rev. E
74, 036104 (2006).

[18] E. Estrada and J. A. Rodr´ıguez-Vel´azquez, Spectral mea-
sures of bipartivity in complex networks. Phys. Rev. E
72, 046105 (2005).

[19] F. Lorrain and H. C. White, Structural equivalence of
individuals in social networks. Journal of Mathematical
Sociology 1, 49–80 (1971).

[20] We could alternatively base our calculation on the pat-
terns of ingoing rather than outgoing links and for some
networks this may be a useful approach. The mathemat-
ical developments are entirely analogous to the case pre-
sented here.

[21] H. C. White, S. A. Boorman, and R. L. Breiger, Social
structure from multiple networks: I. Blockmodels of roles
and positions. Am. J. Sociol. 81, 730–779 (1976).
[22] A. Clauset, M. E. J. Newman, and C. Moore, Structural
inference of hierarchies in networks. In Proceedings of the
23rd International Conference on Machine Learning, As-
sociation of Computing Machinery, New York (2006).
[23] A. P. Dempster, N. M. Laird, and D. B. Rubin, Maximum
likelihood from incomplete data via the em algorithm. J.
R. Statist. Soc. B 39, 185–197 (1977).

[24] G. J. McLachlan and T. Krishnan, The EM Algorithm
and Extensions. Wiley-Interscience, New York (1996).
[25] W. W. Zachary, An information ﬂow model for conﬂict
and ﬁssion in small groups. Journal of Anthropological
Research 33, 452–473 (1977).

[26] J. Reichardt and S. Bornholdt, Detecting fuzzy commu-
nity structures in complex networks with a Potts model.
Phys. Rev. Lett. 93, 218701 (2004).

[27] G. Palla, I. Der´enyi, I. Farkas, and T. Vicsek, Uncover-
ing the overlapping community structure of complex net-
works in nature and society. Nature 435, 814–818 (2005).
[28] J. Baumes, M. Goldberg, and M. Magdon-Ismail, Eﬃ-
cient identiﬁcation of overlapping communities. In Pro-
ceedings of the IEEE International Conference on Intel-
ligence and Security Informatics, Institute of Electrical
and Electronics Engineers, New York (2005).

[29] D. M. Chickering and D. Heckerman, Eﬃcient approxi-
mations for the marginal likelihood of Bayesian networks
with hidden variables. Machine Learning 29, 181–212
(1997).

[30] G. Schwarz, Estimating the dimension of a model. Annals

of Statistics 6, 461–464 (1978).

[31] H. Akaike, A new look at the statistical identiﬁcation
model. IEEE Trans. Auto. Control 19, 716–723 (1974).

8


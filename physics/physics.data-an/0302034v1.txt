3
0
0
2
 
b
e
F
 
1
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
3
0
2
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

POWER AND BEAUTY OF INTERVAL METHODS

Marek W. Gutowski
Institute of Physics, Polish Academy of Sciences
02–668 Warszawa, Al. Lotnik´ow 32/46, Poland
e-mail: gutow@ifpan.edu.pl

Abstract

Interval calculus is a relatively new branch of mathematics. Initially understood as a set
of tools to assess the quality of numerical calculations (rigorous control of rounding errors), it
became a discipline in its own rights today. Interval methods are useful whenever we have to
deal with uncertainties, which can be rigorously bounded. Fuzzy sets, rough sets and probability
calculus can perform similar tasks, yet only the interval methods are able to (dis)prove, with
mathematical rigor, the (non)existence of desired solution(s). Known are several problems, not
presented here, which cannot be eﬀectively solved by any other means.

This paper presents basic notions and main ideas of interval calculus and two examples of

useful algorithms.

result veriﬁcation

reliable computations; guaranteed results; global optimization; algebraic systems; automatic

Keywords

I. What is an interval anyway?

Deﬁnition: The interval is a bounded subset of real numbers. Formally:

(X = [a, b] is an interval) ⇐⇒ (X = {x ∈ R| a ≤ x ≤ b}),

where a, b ∈ R (set of all real numbers); in particular a, b, or even both of them may be
inﬁnite.

Geometrically, interval is just a section of a real line, uniquely determined by its own
endpoints. The set of all intervals is commonly denoted as IR. Lower and upper endpoint
of interval X is usually referred to as X and X, respectively.
Intervals with property
X = X are called thin (or degenerate), any of them contains exactly one real number
and can be thus formally identiﬁed with this very number. Two basic real-valued functions
deﬁned on intervals (i.e. of type IR 7→ R) are:

width : w (X) = |X|
def
= 1
center : mid (X)

def
= |X − X|
2|X + X|

Algebraic operations on intervals are deﬁned in such a way that their results always
contain every possible outcome of the corresponding algebraic operation on real numbers.
More speciﬁcally: the result of X1 ⋄ X2 is again an interval Y with property

X1 ⋄ X2 = Y = {y = x1 ⋄ x2| x1 ∈ X1, x2 ∈ X2},

where ⋄ belongs to the set {+, −, ·, /}. One can easily prove that arithmetic operations
on intervals can be expressed in terms of ordinary arithmetics on their endpoints:

X1 + X2 =

X 1 + X 2, X 1 + X 2
h

i

X1 − X2 =

X 1 − X 2, X 1 − X 2
h

i

X1 · X2 =

min (X 1X 2, X 1X 2, X 1X 2, X 1X 2), max (X 1X 2, X 1X 2, X 1X 2, X 1X 2)
h

i

X1/X2 =

min (X 1/X 2, X 1/X 2, X 1/X 2, X 1/X 2), max (X 1/X 2, X 1/X 2, X 1/X 2, X 1/X 2)
h

i

with an extra condition for division: 0 /∈ X2.

In computer realization we have to take care of proper rounding of every intermediate
result in order to preserve the property that the ﬁnal results are guaranteed. The appro-
priate rounding is called outward or directed rounding, i.e. Y must be always rounded
down (‘towards −∞’) and Y has to be rounded up (‘towards +∞’). This is achieved
either in hardware, by proper switching back and forth the processor’s rounding mode
(still rare), or in software as simulated rounding (majority of existing software packages).

Intervals as sets. Since intervals are sets, it is possible to carry typical set operations
on them. For example we can consider the intersections of intervals, like Y = X1 ∩ X2.
However, the intersection of two disjoint intervals is an empty set! This shows the necessity
of considering the empty interval as a legitimate member of the set IR. It is usually
denoted as ∅ and in machine representation should be, for many reasons, expressed as
[INF, −INF ], where INF is the largest machine-representable positive number.

Unfortunately, the union of two intervals is not always an interval.
Instead, we can
deﬁne the interval hull of two arbitrary intervals (or any other subset of R as well) as
the smallest interval containing them both.

hull (X1, X2) = [] (X1, X2) = X1∪X2

def
=

min (X 1, X 2), max (X 1, X 2)
h

i

There is no problem with checking whether X1 ⊂ X2.

It is worth to mention that addition and multiplication of intervals are both commutative
(X1 ⋄ X2 = X2 ⋄ X1) and associative (X1 ⋄ X2 ⋄ X3 = (X1 ⋄ X2) ⋄ X3 = X1 ⋄ (X2 ⋄ X3)).
However, it is surprising that in general the following holds:

X · (X + Z) ⊆ X · Z + X · Y

(and not just equality!)
i.e. the multiplication is only subdistributive with respect to
addition. We can also see, that using the same variable more than once (here: X),
in rational expression, leads inadvertently to the overestimation of the ﬁnal result. The
phenomenon is known under the name of dependency problem. This property, together
with the lack of good order in IR (IR is only partially ordered set (poset)) makes conversion
of ordinary computer programs into their interval equivalents not a straightforward task.
In particular, every computer instruction of the type

if x < y

then . . .
else . . .

endif

has to be carefully redesigned.

Example:

Before shopping I had exactly 100.00 monetary units in my pocket and my wife had something
between 42.00 and 45.00 units. So we had together [142, 145] units. We have already spent
127.99 units, so we still have [14.01, 17.01] units. Can we aﬀord to call a taxi with estimated
cost [13, 15] units? Possibly . . .
But, without any credit, we certainly can buy two bottles of milk at the cost 2 × [1.19, 3.59] =
[2.38, 7.18] units. Guaranteed.

Interval vectors and matrices. Any n-dimensional vector with at least one interval
component may be called interval vector or box. Any matrix with at least one interval

entry will be called an interval matrix. Ordinary linear algebra can be done on those
objects, if only every elementary arithmetic operation is substituted by its suitable interval
counterpart, as deﬁned earlier. The most often used norms for interval vectors are: k · k1,
equal to the sum of widths of all its components, and k · k∞ being the width of the widest
component.

II. Interval functions

An obvious requirement for the good interval substitute F of the real-valued function f
is following

F (X) = [] f (x)
x∈X

=

inf
x∈X

f (x), sup
x∈X

"

f (x)

#

We would call such an F (X) a range function for f . The explicit construction of range
function may be diﬃcult, so we often work with the so called inclusion or inclusive
functions. These are not unique, but any such function satisﬁes

F (X) ⊇ f (x)

∀x ∈ X

F is also called an interval extension for f . Note, that:

• F may be ‘broader’ than the range function, i.e. it usually overestimates the range of

function f , and

• there is no explicit speciﬁcation how large (or small) this overestimation can be.

The most desirable are the so called monotonic inclusion functions, i.e. such inclusion
functions, which additionally satisfy the implication

X → x

⇒

F (X) → f (x)

∀x ∈ X

(cid:18)

(cid:19)

(cid:18)

(cid:19)

more properly formulated as

X1 ⊆ X2

⇒

F (X1) ⊆ F (X2)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

This is only possible for functions, which are everywhere continuous. Shortly one can
say that range functions and monotonically inclusive functions produce thin intervals
for thin arguments, while functions, which are only inclusive generally return ‘true’ (i.e.
non-degenerate) intervals — even if their arguments are thin.

Example:

Let f (x) = sign x =

+1 when x > 0
0 when x = 0
−1 when x < 0

(

The range function corresponding to sign is:

SIGN (X) =

when X > 0
+ 1
[ 0, +1] when X = 0
[−1, +1] when XX < 0
[−1, 0] when X = 0
when X < 0
− 1
when X = [0, 0]

0






F (X) = [−1.5, 2.5]

and one of its many inclusion functions may be given as

while no monotonically inclusive function exists for this case, except for the trivial one, when
the domain of original function is restricted to the interval not containing the number 0.

Important remark. The value of the range function for argument X should be calcu-
lated only for x ∈ X ∩ D (f ), where D(f ) is domain of f . Empty set should be returned
whenever X ∩ D(f ) = ∅. Therefore

[−4, +4] = [0, 2] and

[−20, −10] = ∅.

III. Interval oriented algorithms

q

q

As George Corliss pointed out, usual (i.e. non-interval) algorithms only rarely are a good
starting point for interval oriented ones. The vast majority of work done so far was
concentrated on optimization problems and on solving systems of algebraic equations in
many variables. There are remarkable results achieved in this ﬁeld with interval version
of Newton method being the most honored.

The typical example of interval methods is the algorithm due to Ramon E. Moore and
Stieg Skelboe, which belongs to the class of ‘divide and bound’ algorithms. Suppose our
task is to ﬁnd the global minimum of a real-valued function f of n variables over the box
V0 = X1 × X2 × · · · × Xn. The initial step is to construct an interval extension F for the
function f . The algorithm operates on the list of n-dimensional boxes, L, which initially
contains the only element, a pair: the box V0 and the interval F (V0). We will also need a
real number, ftest, initially equal to ↑f (any x ∈ V0) or just F (V0). The outline of the rest
of algorithm, in pseudo code, follows:

repeat

pick the ﬁrst element V and its bounds F (V ) from list L
remove V from L
if F (V ) ≤ ftest then

bisect V perpendicularly to its longest edge obtaining V1 ∪ V2 = V
calculate intervals F1 = F (V1) and F2 = F (V2)
for i = 1, 2 do

if Fi > ftest
then discard box Vi
else put pair (Vi, Fi(Vi)) at the end of L

ftest ← min

ftest, Fi, ↑f (center of Vi)

(cid:16)

(cid:17)

end if

end for

end if

until diameter of the ﬁrst box on the list L is small enough

The operation ‘↑’ means round the next number up. The algorithm continuously ‘grinds’
boxes on the list L, making them smaller and smaller. Some of them disappear forever.
At exit we can say that the global minimizer(s) x⋆, such that f (x⋆) = minx∈V0 f (x), is
(are) contained with certainty in the union of all the boxes still present on L. Numerous
variants of the above algorithm do exist for less general cases, for example, when f is
diﬀerentiable almost everywhere in V0. It is absolutely essential, from the performance
point of view, to get rid of ‘bad’ (sub)boxes as early as possible. And the reason is clear:
to test all subboxes, which are twice smaller than V0, it is necessary to consider up to 2n
of them. The properties of f and its interval extension F as well, can inﬂuence the speed
of convergence, which may be arbitrarily slow.

Due to space limitations, we have to stop here with this introductory course. More, and
most likely better, materials can be found in the web [1]. The excellent starting point,

with pointers to other valuable sites, is also [2]. Those, who prefer classical forms are
encouraged to see the book [3].

IV. Where are we today?

Interval analysis started as a part of numerical analysis, devoted mainly to automatic
veriﬁcation of computer-generated results. The basic four arithmetic operations were
everything what was needed for this purpose. There were two goals in front of researchers
and users of interval calculus:

• to obtain guaranteed bounds for results in every case, and
• to make every possible eﬀort to have those bounds as tight as possible.

They are still important, therefore better and better methods for construction of inclusion
functions are discussed. Besides naive (natural) expressions we have at our disposal mean
value theorem, Lipschitz forms, centered forms, and — recently — Taylor centered forms.
After (re)discovering various old theorems, and proving new ones, it became clear, that
interval methods, mostly those based on ﬁxed point theorems, have enormous power to
prove or disprove, with mathematical rigor, the existence of solutions to nonlinear systems
of equations. As a complete surprise we learned that some problems, thought hopeless,
can be successfully attacked with interval methods, while no other method apply.

Two kinds of research activity is visible today:

• introduction of interval methods into other branches of ‘hard’ science, like physics,
astronomy or chemistry, as well as into engineering and business everyday practice
• establishing connections with other branches of pure and applied mathematics like,

for example, fuzzy set theory, mathematical statistics and others.

The ﬁrst area is ‘easy’. Just learn, implement and use. Continuously increasing computing
power makes interval calculations feasible and acceptable, regardless that they are usually
8–20 times slower than their regular ﬂoating-point counterparts. This is no longer a serious
problem. Commercial and free software is also easily available.

The second kind of activity goes much deeper. New ideas are emerging, interval methods
inspire specialists from other ﬁelds. One can clearly notice gradual shift of interest into,
generally speaking, imprecise probability theory. Practical consequences are important
in environmental protection, risk analysis, robotics, fuzzy sets theory and applications,
experimental data processing, quality control, electric power distribution, constraint prop-
agation, logic programming, diﬀerential equations — to name a few.

V. New paradigms in experimental sciences

Parameter identiﬁcation in engineering and data ﬁtting in experimental sciences are code
words for nearly the same thing. The task of reconstruction of values of unknown pa-
rameters, given experimental observations, lies at heart of the so called inverse problems.
The problem is usually formulated as follows:

Given:
• N observations y1, y2, . . . , yN ,
• taken for the corresponding values x1, x2, . . . , xN of the control variable x,
• depending additionally on p unknown parameters a1, a2, . . . , ap,
• and the mathematical model, f (x, y, a) = 0, relating y’s with x’s and the
constant vector a

p < N

ﬁnd the numerical values of all parameters a1, a2, . . . , ap.

There is a bunch of, more or less standard, approaches to this problem, especially, when
the relation f (x, y, a) = 0 is simply a function y = f (x, a). The most popular are: least
squares method (LSQ), least absolute deviations (LAD) and maximum entropy methods
(MEM). All they are based on ﬁnding the absolute (global) minimum of the appropriately
chosen functional. We would like to ﬁnd the most appropriate set of unknown parameters,
which is also the minimizer of such a functional. It is obvious, that the ﬁnal result may
vary, depending on which functional shall be used.

Let us now present the interval-type approach to this very problem. Both the x’s and y’s,
due to unavoidable experimental uncertainties, should be treated as intervals containing
the (unknown) true value. We will assume, that those intervals are guaranteed, i.e.
they contain the true values of control variables and measured results respectively with
probability equal to exactly 1. We will search not for the most likely values of unknown
parameters a, but for their possible values instead. For example, when ﬁtting the straight
line (extension for more complicated cases is immediate) y = a x + b, (parameters a =
(a, b)), we will consider the set of relations:

{(a xj + b) ∩ yj 6= ∅

j = 1, 2, . . . , N

In geometrical interpretation the above means that the straight line with slope in the inter-
val a and intercept in the interval b, both yet unknown, passes through every ‘uncertainty
rectangle’ xi × yi, i = 1, 2, . . . N. In purely algebraic terms:

((a xj + b) ∩ yj 6= ∅) ⇐⇒ (a xj + b ≤ yj ∧ a xj + b ≥ yj)

(1)

This way the data themselves and their uncertainties, with no additional assump-
tions, will determine the intervals for possible values of unknown parameters a and b.
Such a possibility was ﬁrst pointed out by Wallster [4] in 1988. To discover the intervals
a and b we will use the following procedure:

1. start with initial box V = (a, b) such that all inequalities (1) are possibly
satisﬁed somewhere within V but certainly not on their faces.
2. working with V ′, the exact copy of V , and using box slicing algorithm obtain
its new version taking into account all the inequalities a xj + b ≤ yj only.
3. working with V ′′, another exact copy of V , and using box slicing algorithm
again obtain its new version when only the inequalities a xj + b ≥ yj are all
satisﬁed.
4. if V ′ ∩ V ′′ 6= V

then V ← V ′ ∩ V ′′

else stop.

if V 6= ∅ then goto step 2 else stop

The last step illustrates very important and often used rule of interval calculations: if the
result can be obtained on more than one way — do so and take the intersection of partial
results as the ﬁnal one. Of course, at this step V ′ ∩ V ′′ may appear empty. If this ever
happens, then we can be sure, that there are no solutions within the initial box V . This
may mean one of two things:

• either our data set contains one or more outliers, or
• our mathematical model (f ) is inadequate, the theory is invalidated by present obser-

vations.

The box slicing algorithm, reducing p-dimensional initial box V , is given below. Ex-
plicitly shown is the phase called slicing from the left. Slicing from the right is obtained

using comments (surrounded by ’/*’ and ‘*/’ pair) instead of original text in lines marked
as 2, 5 and 7. The complete algorithm consists of both phases, applied in any order.

/* ξ ← 0 */

1: for j = 1 to p do
ξ ← 1
2:
k = 1
3:
repeat
4:
5:
6:
7:

/*

8:
9:
10:
11: end for

ξ ← ξ/2
k ← k + 1
consider box V ′ = a1 × a2 × · · · ×

consider box V ′ = a1 × a2 × · · · ×
success ← not all conditions satisﬁed in V ′
if success then V ← V \ V ′

× · · · × ap

aj, ξ
aj − aj
h
(cid:16)
ξ
aj − aj
h

(cid:16)

(cid:17)i

(cid:17)

i

, aj

× · · · × ap */

/* ξ ← (1 + ξ)/2 */

until success or k > M

The number M denotes simply the number of bits in ﬂoating point representations of real
numbers used by a given processor/compiler pair; for example M = 25 for single precision
reals and M = 56 for double precision type in PC-compatibles equipped with g77 or gcc
compiler.

It must be noted, that the procedure outlined in this article produces the interval hull
of possible solutions. Not every point within the ﬁnal box V represents possible solution
of the problem, but — on the other hand — no other point, outside V , is feasible. For
graphical illustration see for example [5].

The ideas expressed here are closely related to the ones described in [6], [7], however they
go much further: instead of producing just the interval version of well known least squares
procedure, like in [8], we have developed completely diﬀerent approach, much stronger.
There are, of course, some drawbacks:

• the correlations between searched parameters are lost, and
• the relations of our method with the familiar conﬁdence level and other statistical
terms are still to be determined. Probably the famous Tchebysheﬀ inequality will be
the only eﬀective tool for this purpose.
And what are the advantages? Well, several:

• no assumptions are made concerning the distributions of experimental uncertainties,

in particular they need not to be gaussian (Ockham’s razor principle at work),

• the results are always valid, no matter whether the experimental uncertainties are

‘small’ or not,

• it is easy to reliably identify outliers in collected data,
• uncertainties in both variables are handled naturally and easily,
• more data usually means less wide intervals for the searched parameters, in full ac-

• possibly no solution will be obtained, if any uncertainties are underestimated, deli-

cordance with common sense,

berately or otherwise,

• reliable bounds for searched parameters (their accuracies) are produced automatically,
without the need for additional analysis. They are directly and precisely related to
input uncertainties.

It is interesting to note, that in [5] we have found an example, in which the ‘most likely’
estimates for a and b, obtained by least squares method, are outside the bounds produced
by our box slicing algorithm.

This work was done as a part of author’s statutory activity of Institute of Physics, Polish
Academy of Sciences.

VI. Acknowledgment

VII. Historical note

First traces of ‘interval thinking’ might be attributed to Archimedes from Syracuse, Greece (287–212
b.c.), famous physicist and mathematician, who found two-sided bounds for the value of a number π:
71 ≤ π ≤ 3 10
3 10
70 and a method to successively improve them. More than 2000 years later, the american
mathematician and physicist, Norbert Wiener, published two papers: A contribution to the theory of
relative position (Proc. Cambridge Philos. Soc. 17, 441-449, 1914) and A new theory of measurements:
a study in the logic of mathematics (Proc. of the London Math. Soc., 19, 181–205, 1921), in which
the two fundamental physical quantities, namely the position and the time respectively, were given an
interval interpretation. Only after Second World War more papers on the subject were written. Here
we have, probably among several others: chapter 2 of the book Linear Computations by Paul S. Dwyer
(John Wiley & Sons, Inc., 1951, chapter Computation with Approximate Numbers) and Theory of an
Interval Algebra and its Application to Numerical Analysis by Teruo Sunaga, (RAAG Memoirs, 2, 29–46,
1958). Facsimile of those and other early papers on interval analysis are freely available in the web at
http://www.cs.utep.edu/ interval-comp/early.html. Here we can also ﬁnd two papers by polish
mathematician Mieczys law Warmus: Calculus of Approximations (Bull. Acad. Pol. Sci. C1. III, vol. IV
(5), 253–259, 1956) and Approximations and Inequalities in the Calculus of Approximations. Classiﬁcation
of Approximate Numbers (Bull. Acad. Pol. Sci. math. astr. & phys., vol. IX (4), 241–245, 1961).
And, ﬁnally, there are two technical reports from Lockheed Aircraft Corporation, Missiles and Space
Division, Sunnyvale, California, Interval Analysis I by R.E. Moore with C.T. Yang, LMSD-285875, dated
September 1959, and Interval Integrals by R.E. Moore, Wayman Strother and C.T. Yang, LMSD-703073,
dated August 1960. R.E. Moore later developed more systematical studies in this area, with still more
results presented in his Ph.D. Thesis (Stanford, 1962). He also wrote the ﬁrst widely available monograph
Interval Analysis (Prentice Hall, Englewood Cliﬀs, NJ, 1966) on this topic. Almost nobody was willing
to make any progress in this direction until it was discovered, that the same problem, programmed in
the same computer language, produces sometimes drastically diﬀerent results when solved on diﬀerent
machines. Due to his accomplishments, R.E. Moore is regarded as a founding father of interval analysis.
He is still (2003) active. Besides other things, we owe him the proof of convergence of interval Newton
Method (A test for existence of solutions to nonlinear systems, SIAM J. Numer. Anal., 14 (4), 611–615,
1977). Since that time we observe growing interest into interval methods, not only inside numerical
analysis community.

References

[1] R.B. Kearfott Interval Computations: Introduction, Uses, and Resources Euromath. Bull. 2 (1), 95 – 112,

1996, http://interval.louisiana.edu/ preprints/survey.ps

[2] http://www.cs.utep.edu/ interval-comp/
[3] Luc Jaulin, Michel Kieﬀer, Olivier Didrit and ´Eric Walter Applied Interval Analysis Springer-Verlag London

Limited 2001

[4] G. William Walster Philosophy and Practicalities of Interval Arithmetic, in Reliability in Computing, R. Moore

(ed.). Academic Press: San Diego, California (1988), pp 309-323.

[5] Marek W. Gutowski Interval straight line ﬁtting http://arXiv.org/ abs/math/0108163

in polish: Marek W. Gutowski Prosta dostatecznie gruba Post¸epy Fizyki, 53 (4), 181–192, 2002,
http://pupil.ifpan.edu.pl/~postepy/dodatki/prosta/prosta.pdf

[6] Hung T. Nguyen, Vladik Kreinovich, and CHin-Wang Tao Why 95% and Two Sigma? A Theoretical Justiﬁ-

cation for an Empirical Measurement Practice http://utep.edu/ vladik/2000/tr0026a.ps.gz

[7] Luc Longpr´e, William Gasarch, G. William Walster, and Vladik Kreinovich m Solutions Good, m−1 Solutions

Better http://utep.edu/ vladik/2000/tr0040.ps.gz

[8] Jie Yang and R. Baker Kearfott Interval Linear and Nonlinear Regression — New Paradigms, Implementa-
tions, and Experiments or New Ways of Thinking of Data Fitting the talk given at 2002 SIAM Symposium,
Toronto, May 22, 2002, http://interval.louisiana.edu/ preprints/2002_SIAM_minisymposium.ps


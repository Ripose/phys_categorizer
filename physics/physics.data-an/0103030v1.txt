1
0
0
2
 
r
a

M
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
3
0
3
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A Good Measure for Bayesian Inference

Hanns L. Harney
Max-Planck-Institut f¨ur Kernphysik∗
Heidelberg

January 16, 2014

Abstract

The Gaussian theory of errors has been generalized to situations,
where the Gaussian distribution and, hence, the Gaussian rules of error
propagation are inadequate. The generalizations are based on Bayes’
theorem and a suitable measure. The following text sketches some
chapters of a monograph 1 that is presently prepared. We concentrate
on the material that is — to the best of our knowledge — not yet in the
statistical literature. See especially the extension of form invariance to
discrete data in section 4, the criterion on the compatibility between a
proposed distribution and sparse data in section 7 and the “discovery”
of probability amplitudes in section 9.

1 The Prior Distribution

Bayes’ theorem [1] allows one to deduce the distribution P (ξ
x) of the pa-
|
rameter ξ conditioned by the data x. The distribution p(x
ξ) of the data
|
conditioned by the parameter ξ must be given. The theorem reads

P (ξ

ξ)µ(ξ)
x)m(x) = p(x
|
|

m(x) =

ξ)µ(ξ).
dξ p(x
|

Z

(1)

(2)

See e.g. [2]. Here, µ(ξ) is called the prior and P the posterior distribution
of ξ. The posterior can be used to deduce an interval I of error: We deﬁne
. This is called
it as the smallest interval in which ξ is with probability

∗

Postfach

103980, D-69029 Heidelberg, Germany;

harney@mpi-hd.mpg.de;

K

http://www.mpi-hd.mpg.de/harney

1submitted to Springer Verlag, Heidelberg

1

the Bayesian interval I = I(
reparametrisation η = T (ξ), one has to judge the size
help of a measure µ(ξ), i.e.

In order to make it independent of any
of an interval I by

A

K

).

(3)

(4)

We identify this measure with the prior distribution of µ.

=

dξ µ(ξ) .

A

ZI

2 Form Invariance

ξ) possesses a symmetry called form
Ideally the conditional distribution p(x
|
invariance. This family of distributions then emerges by a mathematical
group of transformations Gξx from one and the same basic distribution w,
i.e.

ξ)dx = w(Gξ x)dGξ x.
p(x
|

It is not required that every acceptable p has this symmetry. But the sym-
metry guarantees an unbiased inference in the sense of section 3. If there is
no form invariance, unbiased inference can be achieved only approximately.
The prior distribution is deﬁned as the invariant measure of the group
of transformations. Symmetry arguments were ﬁrst discussed in [3, 4, 5, 6].
They were not generally accepted because not all reasonable distributions
possess the symmetry (4). It cannot exist at all if x is discrete. Since ξ is
assumed to be continuous, it can be changed inﬁnitesimally. However, no
inﬁnitesimal transformation of a discrete variable is possible. In section 4,
we generalize form invariance to this case.

Form invariance is a property of ideal, well behaved distributions. How-
ever, its existence is not a prerequisite of statistical inference, see section
6.

The invariant measure can be found from p — without analysis of the

group — by evaluating the expression

µ(ξ)

det

∝

(cid:18)Z

ξ)∂ξL ∂T
dx p(x
|

ξ L

1/2

.

(cid:19)

(5)

Here, the function L is

ξ)
L(ξ) = ln p(x
|
ξ L means the dyadic product of the vector ∂ξL of partial deriva-

and ∂ξL ∂T
tives with itself. Eq.(5) is known as Jeﬀreys’ rule [7].

(6)

One shall see in section 6 that this expression deﬁnes µ in any case that

is to say in the absence of form invariance, too.

2

3

Invariance of the Entropy of the Posterior Dis-
tribution

The posterior distribution P (ξ
ξ) if form invariance exists. The entropy
distribution p(x
|

x) has the same symmetry as the conditional
|

is then independent of the true value ˆξ of the parameter ξ because one has

H(x) =

−

Z

dx P (ξ

x) ln
|

P (ξ

x)
|
µ(ξ)

H(x) = H(Gρ x)

(7)

(8)

for every transformation Gρ of the symmetry group. This entails that H(x)
does not depend on ˆξ but only on the number N of the data x1 . . . xN . One
can say that all values of the parameter ξ are equally diﬃcult to measure.
In this sense, form invariance guarantees unbiased estimation of ξ and by
the same token the invariant measure µ is the parametrization of ignorance
about ξ.

4 Form Invariance for Discrete x

If the variable x is discrete — e.g. a number of counts — then form invariance
cannot exist in the sense of eq.(4) since an inﬁnitesimal shift of ξ cannot be
compensated by an inﬁnitesimal transformation of x. One then has to deﬁne
a vector a(ξ) the components of which are labelled by x. The probability
ξ) must be a unique function of ax(ξ). Form invariance then means that
p(x
|

a(ξ) = Gξ a(ξ = 0) .

(9)

Again µ is the invariant measure of the group. The transformation Gξ shall
be linear so that it is the linear representation of the symmetry group of
form invariance. It is necessarily unitary.

ξ) is precluded because a group of transforma-
The choice ax(ξ) = p(x
|
tions cannot — for all of its elements — map a vector with positive elements
onto one with the same property. With the choice

ax(ξ) =

ξ)
p(x
|

(10)

q
one succeeds. That means: Important discrete distributions — such as the
Poisson and the binomial distributions — possess form invariance. Further-
more the property (4) can be recast into a relation corresponding to eq.(9),

3

it can be written as a linear transformation of the space of functions
i.e.
ξ))1/2. Hence, (9) is not diﬀerent from (4); it is a generalization.
(p(x
|
Note that (10) is a probability amplitude as it is used in quantum me-
chanics. However, it is real up to this point. The generalization to complex
probability amplitudes is sketched in section 8.

5 The Poisson Distribution

Form invariance in the sense of section 4 does not seem to have been treated
in the literature on statistics. As an example let us consider the Poisson
distribution

With

one obtains the amplitudes

The derivative of a is found to be

ξ) =
p(x
|

λx
x!
x = 0, 1, 2 . . .

exp(

λ)

−

ξ = λ1/2

ax(ξ) =

ξx
√x!

exp(

ξ2/2).

−

∂
∂ξ

a(ξ) = (A+

A)a(ξ),

−

where A, A+ are linear operators independent of ξ. They have the commu-
tator

[A, A+] = 1.
(15)
Hence, A, A+ are destruction and creation operators of numbers of counts
or events. Integrating the diﬀerential equation (14) one ﬁnds

a(ξ) = exp

.
0
i
|
is the vector that provides zero counts with probability

(16)

−

(cid:1)(cid:1)

A

ξ

(cid:0)

(cid:0)

A+

Here, the vacuum
1. Equation (16) means that the linear transformation Gξ is

0
i
|

The measure µ of this group of transformations is

(cid:0)

(cid:0)

(cid:1)(cid:1)

Gξ = exp

ξ

A+

−

A

.

µ(ξ)

const.

≡

4

(11)

(12)

(13)

(14)

(17)

(18)

It can also be obtained by straightforward application of Jeﬀreys’ rule (5)
without analysis of the symmetry group.

This can be generalized to the joint Poisson distribution

p(x1 . . . xM |

ξ1 . . . ξM ) =

2xk
ξ
k
xk!

M

Yk=1

exp(

ξ2
k)

−

of the numbers xk of counts in a histogram with M bins. One ﬁnds the
amplitude vector

a(ξ1 . . . ξM ) = exp

ξk(A+

k −

Ak)

0
i
! |

M

 

Xk

≡

and again the uniform measure µ(ξ)

const.

As a further generalization, one can introduce destruction and creation

operators Bν, B+

ν of quasi-events ν = 1 . . . n via

M

Bν =

ckνAk.

Xk=1
for ν = 1 . . . n are orthonormal then

[Bν, B+

ν′] = δνν′,

If the vectors

cνi
|

whence Bν , B+
plitude vector

a(ξ) = exp

n

ξν

B+

ν −

Bν

 

Xν=1

(cid:0)

0
i
! |
(cid:1)

The amplitude ax to ﬁnd the event x is given by

ax(ξ) =

(Ξk)xk exp

M

Yk=1

1
√xk!

1
2

 −

ξ2
ν

.

!

ν
X

ν are destruction and creation operators. One ﬁnds the am-

Here, the amplitude

Ξk =

ξνckν

Xν=1
to ﬁnd events in the k-th bin is given by an expansion into the orthogonal
. More precisely: By working with the
system of amplitude vectors
creation operators B+
in terms of

ν , one infers an expansion of the vector

cν i
|

Ξ
|

i

n

5

(19)

(20)

(21)

(22)

(23)

(24)

(25)

the orthogonal system
again uniform,

cνi
|

. The prior distribution of the amplitudes ξν is

µ(ξ1 . . . ξν)

const.

≡

(26)

On Summary: The problem of ﬁnding the expansion coeﬃcients ξν from the
counting rates xk is form invariant and thus guarantees unbiased inference.
One should therefore expand probability amplitudes and not probabilities
in terms of an orthogonal system if one performs e.g. a Fourier analysis.

6 The Prior Probability in the Absence of Form

Invariance

Jeﬀreys’ rule (5) can be rewritten in the form

µ(ξ)

det

∝

dx ∂ξa ∂T

ξ a

1/2

.

(cid:19)
The integral means a summation if x is discrete.

(cid:18)Z

(27)

In diﬀerential geometry [8, 9], it is shown that (27) is the measure on the
surface deﬁned by the parametrisation a(ξ). A prerequisite for this measure
is the assumption that one has the same uniform measure on each coordinate
axis in the space; more precisely, the metric tensor of the space must be
proportional to the unit matrix. Since the coordinates ax are probability
amplitudes, this is justiﬁed by the last result of section 5.

Hence, Jeﬀreys’ rule provides the prior distribution in any case. In the
absence of form invariance, however, one cannot guarantee that all values of
the parameter ξ are equally diﬃcult to measure, i.e. one cannot guarantee
unbiased inference.

7 Does a Proposed Distribution Fit an Observed

Histogram?

The Poisson distribution (19) yields the posterior

P (ξ1 . . . ξM |

x1 . . . xM )

M

∝

Yk=1

ξ

2xk
k

exp(

ξ2
k)

−

(28)

We want to decide whether — in the light of the data — the proposal τk is
a reasonable estimate of ξk, k = 1 . . . M . This is equivalent to the question
). The Bayesian interval is
whether τ is in the Bayesian Interval I = I(

K

6

bordered by the “contour line” Γ(
deﬁned as the set of points with the property P (ξ
that τ

) which is — in the case at hand —
). This means
x) = C(
|

I exactly if

K

K

∈

P (τ

x) > C(
|

K

)

(29)

or that τ is accepted if and only if (29) holds. The number C(
calculated.

K

) can be

If the count rates xk are large in every bin k, the procedure essentially

yields the well-known χ2-criterion.

If, however, M

N =

k xk, i.e. if the data are sparse, then this leads

to the condition

≥

P

xk

1
N

M

Xk=1
ln

N
xk

 

τ 2
k −

1

−

ln

1 +

M
2N

+ N

−1/2Φ

N τ 2
k
xk !
−1(

<

).

K

≥

Xk

τ 2
k = 1.

(cid:19)
Here, Φ−1 is the inverse of the probability function. Note that the expression
in brackets (. . .) on the l.h.s. is

0 if

(cid:18)

(30)

(31)

Hence, the inequality (30) sets an upper limit to a positive expression. This
criterion is new. It is needed because the situation M
N is surely met if
≥
k is a multidimensional variable i.e.
if the observable is multidimensional.
See [10]. Any attempt to apply Gaussian arguments is hopeless in this case.

8 Does a Proposed Probability Density Fit Ob-

served Data?

Suppose that the data x1 . . . xN have been observed. Each xk is supposed
to follow, say, an exponential distribution

ξ) = ξ
p(x
|

−1 exp(

x/ξ) .

−

(32)

They shall all be conditioned by one and the same hypothesis parameter ξ.
x1 . . . xN ) yields the distribution of ξ and,
If this is true, the posterior P (ξ
|
hence, the Bayesian interval for ξ. It is intuitively clear that — at least for
large N — one can learn from the data not only the best ﬁtting values of ξ
but one can even decide whether the exponential (32) is justiﬁed at all. I.e.

7

one can ﬁnd out whether the model is satisfactory. How does this work?
We do not want to produce a histogram by binning the data. This would
reduce the problem to the one solved in section 7 but it would introduce an
arbitrary element into the decision: The deﬁnition of the bins.

The basic idea is to determine ξ from every data point, i.e. N times,
and to decide whether this result is compatible with ξ having the same value
everywhere.

One deﬁnes the distribution q of the N -dimensional event (x1 . . . xN )

conditioned by the N -dimensional hypothesis (ξ1 . . . ξN ) as the product

q(x1 . . . xN |

ξ1 . . . ξN ) =

p(xk|

ξk) .

(33)

N

Yk=1

x1 . . . xN ) of the N -
One writes down the posterior distribution Q(ξ1 . . . ξN |
).
dimensional hypothesis (ξ1 . . . ξN ). One studies its Bayesian interval I(
A proposed hypothesis (τ1 . . . τN ) is acceptable exactly if it is an element of
I. In the case at hand, one determines the best value α of the hypothesis
ξ from the model that assigns one and the same hypothesis to all the data.
One then asks whether the N -dimensional τ with τk = α for all k is in I.

K

The criterion (30) has been derived by help of this argument.
Note, however, that the argument fails, when one wants to know whether
the data (x1 . . . xN ) follow the proposed distribution t(x). There is no hy-
pothesis ξ. The family of distributions is not deﬁned from which t(x) is
taken. Indeed the above argument does not judge the distribution p(x
α)
|
all by itself. It actually judges whether the family of distributions, i.e. the
whole model p(x
ξ), is compatible with the data. The question whether t(x)
|
ﬁts the data, is too general to be answered. One must specify which features
of the distribution are important — its form in a region, where one ﬁnds
most events or in a region where there are very few events? The relevant
features are expressed by the parametric dependence on ξ and the measure
derived from it.

9 The Logic of Quantum Mechanics

The results of section 5 show that probability amplitudes rather than prob-
abilities can be inferred in an unbiased way from counting events. Alterna-
. Each vector characterizes
cν i
tives ν, ν
i
|
a distribution over the bins k = 1 . . . M of a histogram. A decision between
amounts to assess the amplitudes ξν and ξν′. They determine
ν and ν
are present in the data.
the strength with which the distributions ν and ν

are deﬁned by two vectors

cν′
|

and

′

′

′

8

However, the amplitudes can interfere — the probabilities cannot. The real
amplitudes introduced so far can be generalized to complex ones: We arrive
at the quantum mechanical way to treat alternatives.

The parameters ξ deduced from counting events are then completely
analogous with quantum mechanical probability amplitudes. It may be bet-
ter to turn this statement around and to say: The logic of quantum me-
chanics is the logic of unbiased inference from random events; it is not a
collection of the rules according to which the microworld “exists”.

The generalization of real amplitudes to complex ones is achieved by

generalizing the amplitude vector (23) to

a(ξ, ζ, φ) = exp

i

n

 

Xν=1

Dν

,
0
i
! |

where the operator Dν is

Dν = ζν(Bν + B+

ν ) + iξν(Bν −

B+

ν ) + φν .

Here, the three generators do generate a group since one has the commutator

Bν −
(cid:2)
The invariant measure is

B+

ν , Bν + B+

ν

= 2 .

(cid:3)

µ(ξ, ζ, φ)

const.

By explicit evaluation of eq.(34) one ﬁnds

ax =

xk

Ξk

!

M

Yk=1
exp

1
√xk!  

1
2

 −

Xν=1
(ξ2

ν + ζ 2

ν −

2iφν )

!

ν
X
This is a generalization of expression (24). It is again a Poisson distribution,
but now the amplitude Ξk to ﬁnd events in the k-th bin is

Ξk =

∗
kν.
(ξν + iζν )c

n

Xν=1

This is an expansion of the probability amplitude in terms of the system
which may be complex. The expansion
of mutually orthogonal vectors
coeﬃcients ξν + iζν may be complex, too.

c∗
ν i
|

(34)

(35)

(36)

(37)

(38)

(39)

≡

n

9

The phase

ν φν that appears in (38) cannot be measured since only

the modulus of (38) is accessible.
P

The Poisson distribution possesses form invariance with respect to the
probability amplitudes even if these are complex. Put diﬀerently, one should
expand the square root of a distribution into a system of orthonormal vec-
tors. They may be complex. The expansion coeﬃcients deduced from the
data may also be complex.
Inference on the real and imaginary parts of
the expansion coeﬃcients is unbiased. The Fourier expansion is an example;
however, it must be the square root of the probability distribution that is
expanded.

10 Alternatives that cannot Interfere

In quantum physics alternatives can interfere. Suppose that a cross section
σ = σ(E) is observed as a function of energy E — e.g. in neutron scattering
by heavy nuclei. Suppose that this excitation function shows a resonance
line plus a smooth background. The book [11] is full of examples. Look e.g at
the middle part of page 691. There is a ﬂat background with superimposed
resonances. The resonance lines destructively and constructively interfere
with the background.

Speaking in the language of section 5, the ﬁgure oﬀers a simple alter-
native ν = 1, 2. The ﬁrst possibility (ν = 1) is that the incoming neutron
together with the target forms a compound system which decays after some
time. The second possibility (ν = 2) is the reaction to occur without de-
lay. The probability amplitudes ξν + iζν for these two possibilities interfere.
The interference pattern is visible if the resolution of the detection system
is better than the width of the resonance. If the resolution is much worse,
the interference pattern disappears and the cross section due to the reso-
nance is added to the cross section due to the background, i.e. one adds the
probabilities πν = ξ2
ν instead of the amplitudes.
The situation of insuﬃcient resolution is the situation of classical physics
and classical statistics: Alternatives do not interfere. Their probabilities are
added.

ν + ζ 2

The typical situation of classical physics is that the detection system
lumps many events together that have distinguishable properties.
In our
example: It does not well enough discriminate the energies of the scattered
particles. The events recorded in classical physics can in principle be diﬀer-
entiated according to more properties than are actually used to distinguish
them. The tacit assumption of classical physics was that this were always

10

so.

If objects are observed that allow for a small number of distinctions
only, one is lead to the logic of interfering probability amplitudes by the
way sketched in sections 5 and 9.

Consider the two slit experiment as a further example. If it is performed
with polarized electrons, an impressive interference pattern appears. Use of
unpolarized electrons reduces the contrast of the pattern. Had the scattered
particles more than two “ways to be”, the contrast of the interference would
be reduced up to the point, where the probability of a particle going through
the ﬁrst slit would be added to the probability of the particle going through
the second slit. See chapter 1 of [12].

Suppose that we know that there is interference between the two possi-
bilities in the above neutron scattering experiment. The amplitudes ξν + iζν
for the possibilities ν = 1, 2 would be inferred from the data x1 . . . xk as
follows. The distribution of the data is

(40)

(41)

p(x1 . . . xN |

ξ1ζ1ξ2ζ2) =

exp(

λk) ,

−

λxk
k
xk!

M

Yk=1

where the expectation value λk in the k-th bin is a function of ξν, ζν, namely

λk =

(ξ1 + iζ1)Line(k) + (ξ2 + iζ2)Bg(k)
|
|

2

.

Here, Line(k) is the line shape and Bg(k) is the shape of the background.
By section 9, this is a form invariant model allowing for unbiased inference.
Suppose on the contrary that there cannot be any interference between
the two possibilities in the neutron experiment. The probabilities π1 and π2
π1π2) which is again given by eq. (40).
are inferred via the model p(x1 . . .N |
But now λk is the incoherent sum

λk = π1

2 + π2
Line(k)
|
|
The prior distribution for this model must be calculated by help of (5). The
model is not form invariant, whence unbiased inference cannot be guaran-
teed. A closer inspection shows that the model “has a prejudice against”
very small values of π1 or π2. This means: Small values are harder to
establish than large ones.

2 .
Bg(k)
|
|

(42)

11 Summary

The basis of the foregoing work is twofold: (i) All statements and relations
in statistical inference must be invariant under reparametrizations and (ii)
to state ignorance about ξ means to claim a symmetry.

11

It is the symmetry of form invariance that guarantees unbiased infer-
ence of the hypothesis ξ, if the invariant measure of the symmetry group is
identiﬁed with the prior distribution in Bayesian inference. The invariant
measure is obtained in a straightforward way — i.e. without analysis of
the group — by Jeﬀreys’ rule. We have shown that even distributions of
counted numbers possess form invariance.

A study of the Poisson distribution shows that the basic quantities in
statistical inference are probability amplitudes not probabilities. The am-
plitudes may even be complex. This is not only an analogy to the logic of
quantum mechanics. This says that the logic of quantum mechanics is the
logic of unbiased inference from counted events.

These considerations do not mean that form invariance is a condition
for the possibility of inference. Lack of form invariance precludes unbiased
inference; it does not preclude inference. In the absence of form invariance,
the prior distribution is deﬁned as the diﬀerential geometrical measure on
a suitably deﬁned surface: The surface must lie in a space of probability
amplitudes. The measure on the surface is again given by Jeﬀreys’ rule.

As a practically useful result, we have formulated the decision whether
a proposed distribution ﬁts an observed histogram. The decision covers the
case of sparse data. This case does not allow a Gaussian approximation and,
hence, no χ2-test.

References

[1] Thomas Bayes, Phil. Trans. Roy. Soc. 53(1763)330–418. Reprinted in
Biometrika 45(1958)293–315 and in Studies in the History of Statis-
tics and Probability, E.S. Pearson and M.G. Kendall eds., C. Griﬃn &
Co., London 1970, and in Two Papers by Bayes with Commentaries,
W.E. Deming ed., Hafner Publishing, N.Y. 1963

[2] P.M. Lee. Bayesian Statistics: An Introduction Arnold, London 1997

[3] J. Hartigan, Ann. Math. Statist. 35(1964)836–845

[4] C.M. Stein, Approximation of Improper Prior Measures by Proper Prob-

ability Measures in Neyman et al. [13] p. 217–240

[5] , E.T. Jaynes, IEEE Transactions on Systems Science and Cybernetics,

SSC-4(3)227–241, September 1968

[6] C. Villegas, in Godambe and Sprott eds. [14] p. 409–414

12

[7] H. Jeﬀreys, Theory of Probability Oxford University Press, Oxford 1939;
2nd edition 1948; 3rd edition 1961, here Jeﬀreys’ rule is found in iii$
3.10

[8] Shun-ichi Amari, Diﬀerential Geometrical Methods in Statistics, Vol-

ume 28 of Lecture Notes in Statistics Springer, Heidelberg 1985

[9] C.C. Rodriguez, Objective Bayesianism and Geometry in Foug`ere ed.

[15] p. 31–39

[10] J. Levin, D. Kella, and Z. Vager, Phys. Rev. A53(1996)1469–1475

[11] V. McLane, C.L. Dunford, and Ph.F. Rose Neutron Cross Sections,

Volume 2, Academic Press, Boston 1988

[12] M. Sands, R.P. Feynman, and R.B. Leighton The Feynman Lectures
on Physics. Quantum Mechanics Volume III, Addison-Wesley, Reading
1965. Reprinted 1989

[13] J. Neyman et al. eds. Bernoulli, Bayes, Laplace. Proceedings of an In-
ternational Research Seminar. Statistical Laboratory. Springer, N.Y.
1965

[14] V.P. Godambe and D.A. Sprott eds., Foundations of Statistical Infer-
ence. Waterloo, Ontario 1970. Holt, Rinehart & Winston, Toronto 1971

[15] P.F. Foug`ere ed., Maximum Entropy and Bayesian Methods, Dartmouth

1989. Kluwer, Dordrecht 1990

13


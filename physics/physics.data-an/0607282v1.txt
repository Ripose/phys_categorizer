Minimum Entropy Density Method for the Time Series Analysis

Jeong Won Lee,∗ Joongwoo Brian Park,∗ Hang-Hyun Jo, Jae-Suk Yang,† and Hie-Tae Moon

Department of Physics, Korea Advanced Institute of Science and Technology,

Daejeon 305-701, Republic of Korea

(Dated: February 20, 2014)

Abstract

The entropy density is an intuitive and powerful concept to study the complicated nonlinear pro-

cesses derived from physical systems. We develop the minimum entropy density method (MEDM)

to detect the most correlated time interval of a given time series and deﬁne the eﬀective delay of

information (EDI) as the correlation length that minimizes the entropy density in relation to the

velocity of information ﬂow. The MEDM is applied to the ﬁnancial time series of Standard and

Poor’s 500 (S&P500) index from February 1983 to April 2006. It is found that EDI of S&P500

index has decreased for the last twenty years, which suggests that the eﬃciency of the U.S. market

dynamics became close to the eﬃcient market hypothesis.

PACS numbers: 89.65.-s, 89.65.Gh, 89.70.+c

Keywords: econophysics, entropy density

6
0
0
2
 
l
u
J
 
0
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
8
2
7
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗The ﬁrst two authors contributed equally to this work.
†Electronic address: yang@kaist.ac.kr

1

I.

INTRODUCTION

In recent years, physicists have enlarged the research area to many interdisciplinary ﬁelds.

Econophysics is one of the active research areas where many statistical methods are applied

to investigate ﬁnancial systems. Many analytic methods are introduced, such as the correla-

tion function, multifractality, minimal spanning tree, and agent-based models [1, 2, 3, 4, 5, 6].

And the empirical time series in ﬁnancial markets have been investigated by using various

methods such as rescaled range (R/S) analysis to test the presence of correlations [7] and

detrended ﬂuctuation analysis to detect long-range correlations embedded in seemingly non-

stationary time series [8, 9].

In this paper, we focus on how to measure the ‘length’ of correlation in the time series

data, which is a characteristic time scale representing the most correlated time interval of

the given time series. We will be referring to this scale as the correlation length. It should

be noted that we would use the term ‘correlation’ only to represent autocorrelation, not

cross-correlation. One way to measure the correlation length is to obtain a decay time by

exponentially ﬁtting the autocorrelation function of the data and taking the reciprocal of

the exponent [2]. This quantity has been widely used because many empirical ﬁnancial data

show exponentially decaying correlations. However, the decay time cannot be considered as

a universal measurement of the correlation length due to a few reasons. Firstly, it can be

only applied to the cases when the correlation decays exponentially. Therefore we cannot

obtain this value when the correlation decreases linearly. Moreover, if the original time

series is periodic, the autocorrelation also becomes periodic. Secondly, there is a problem

of arbitrariness when we carry out the ﬁtting in order to measure the autocorrelation decay

time. For instance, it is the case when the autocorrelation decays fast so that there are only

a few points we can use in the ﬁtting process.

A more fundamental way of measuring the correlation length of the time series applicable

to more general cases, which we call the minimum entropy density method, will be suggested

and exempliﬁed with cases of ﬁnite periodic time series in Section II. At ﬁrst a periodic time

series to demonstrate the method is used because it is simple and has correlation among it

deﬁnitely. However, our method can be applied to any time series and as an example we

apply this method to S&P500 index in Section III. We ﬁnd the temporal behavior of the

most correlated time intervals of the index and analyze the implications of the change in

2

correlation length throughout time in relation to the eﬃcient market hypothesis (EMH).

II. MINIMUM ENTROPY DENSITY METHOD

A. Backgrounds

Here we demonstrate a method of measuring the correlation length of the ﬁnite time

series data using the entropy density, which comes from information theory. We start with

brieﬂy explaining the concepts in information theory that we have used in the analysis [10].

First, we consider an inﬁnite string,

←→
S = · · · S−1S0S1S2 · · · where Si = si chosen from
ﬁnite alphabet A. Denoting the block of L consecutive variables as SL = S1, · · · , SL, the
probability over blocks of L consecutive symbols is Pr(si, si+1, · · · , si+L) = Pr(sL). The

Shannon entropy of a block of L consecutive variables is deﬁned as

H(L) = −

· · ·

Pr(s1, · · · , sL) log2 Pr(s1, · · · , sL),

(1)

Xs1∈A

XsL∈A

which is a monotonic increasing function of L. We can measure the entropy of the entire

system

←→
S by taking L → ∞. However, the Shannon entropy H(L) may diverge as L goes

to inﬁnity, so an entropy density is introduced as follows:

equivalently

hµ ≡ lim
L→∞

H(L)
L

,

hµ = lim
L→∞

{H(L + 1) − H(L)}.

(2)

(3)

The existence of limit in Eq. (2) was discussed by Feldman [10]. We point out that the
←→
S has more patterns, and close to 1 when the

entropy density is close to 0 when the system

system is more random. It is also interpreted as the uncertainty of a given symbol when all

the preceding symbols are known. If there is strong correlation in the time series, knowledge

of all the previous symbols will greatly decrease the uncertainty of the next symbol. This

characteristic of entropy density will be used in our method of measuring the most correlated

interval of the series.

is approximated as

For the case of ﬁnite series, the entropy density cannot be measured using Eq. (3) so it

hµ(L) ≡ H(L) − H(L − 1), L = 1, 2, · · · ,

(4)

3

where H(0) is set to 0. If L is not suﬃciently large so that the information of the data

is not fully detected by hµ(L), the data would look more random, that is, hµ(L) would

overestimate the true entropy density hµ by an amount of hµ(L) − hµ. As L increases,

hµ(L) saturates to hµ from above and after a certain value of L, hµ(L) becomes a good

approximation of hµ. Since we are given only ﬁnite series, we will refer hµ(L) as the entropy

density for convenience.

For the entropy density analysis we coarse-grain the time series with an appropriate scale

and then digitize the amplitudes into discrete symbols. Consider a temporal data set with

span S. We decide the interval s to grain the data, then the resulting time series has

N = S/s equally spaced measurements. And it has been suggested that unless the resulting

series length is extremely large, the number of digits should be low [11]. Since we deal with

the ﬁnite time series we take the smallest set of digits such as {0, 1}. We ﬁrst calculate the

log returns of the function Y (t) and then convert it into binary series as follows:

Ri ≡ ln Y (st + s) − ln Y (st),

0 ≤ t ≤ S − 1,

i = 0, 1, · · · , N − 1,

(5)

Fi = 


0

1

for Ri < 0

for Ri > 0

.

(6)

The same function can be converted into diﬀerent sets of series if the diﬀerent values of s are



used. s becomes the time interval of each resulting series and has an eﬀect on the entropy

density obtained. To make the point clear we deﬁne hµ(s, L) as the entropy density of the

series with time interval s. We can ﬁnd the most correlated interval by minimizing hµ(s, L)

by tuning the time interval s. The coarse-graining scale s∗ that minimize the entropy density

can be deﬁned as the correlation length. We will refer to this whole process as the minimum

entropy density method, which is to be described in more details with examples in the next

Subsection.

B. Model examples

The minimum entropy density method (MEDM) is applied to a periodic function in order

to ﬁnd the correlation length of it. Although a periodic function is strongly correlated the

correlation length cannot be measured with conventional methods such as the autocorre-

lation decay time. Therefore the periodic function would be a good example to show the

4

validity of our method. We take a ﬁnite length periodic function,

Y (t) = cos2

0 ≤ t ≤ S − 1,

π
p

(cid:18)

,

t
(cid:19)

(7)

where p is the period of Y (t). We set the total span of the data to 105 and p equal to 50, then

construct 200 diﬀerent sets of time series with s varying from 1 to 200. Then we calculate

the entropy density of each set using the appropriate block size L. In this case we set L

equal to 6 for all sets of series. The reason for this will be discussed in the last part of this

Subsection. Figure 1(a) shows that the entropy densities are relatively low when s takes the

multiple value of 25. The minimum value of the entropy density occurs at s = 25, which is

the same as the half period. It can be inferred that the half period is the correlation length

of the periodic function. At this point the correlation length as the most correlated interval

should be clariﬁed. The correlation length can be misunderstood as the only interval with

strong correlation. However, the correlations can appear in diﬀerent scales simultaneously;

the short-range and long-range correlations can coexist in a series. In our example strongly

correlated intervals turn out to be the multiple values of the ﬁrst interval as shown in Fig.

1(a). Among these the ‘most’ correlated interval is the half period.

The MEDM is now applied to the periodic time series with corrupted bits to see the eﬀect

of the data corruption. We take the function as

cos2

π
p t
(cid:17)

(cid:16)

Y (t) = 


r

for (100 − n)% of the data

for n% of the data

,

(8)

where r is a random number uniformly drawn from [0, 1] and n is the intensity of the



corruption. The period, total span, and the variation of s are the same as the ﬁrst example.

The corruption is equally distributed throughout the smallest scale of measurement. In this

example the smallest scale of measurement is 1 because the smallest value of s is 1. Hence

we grain the continuous periodic function with s = 1 and before the digitizing process we

replace n% of all the resulting values with random number uniformly drawn from [0, 1].

Figure 1(b) and (c) show the entropy densities according to s when 5% and 10% of data

are corrupted respectively. Surprisingly the corruption of the data helps to make clear the

(downward) peaks observed in Fig. 1(a). For both cases the minimum values of the entropy

density occur at s = 25 = p/2, which is the same as for the case without data corruption.

It is also noticeable that the lowest values of the entropy densities occur when s is the

odd multiples of p/2 as shown in Fig. 1(b) and (c). It is because when there are no data

5

corruptions the binary time series of our example with interval 25, 75, 125, 175 and so on

are exactly the same, i.e., ‘01010101 · · · .’ However, the entropy densities are minimized at

s = p/2 in all cases. The main factor for this is the fact that as s increases the total number

of data points decreases. Since if the pattern repeats itself more the uncertainty for the next

symbol decreases, the entropy density is minimal when the total number is maximal, that

is, for the case with s = 25.

Then why are the entropy densities higher for the other values of s? Especially the series

with s = 1, i.e.

‘025125025125 · · · ’ has much higher value of the entropy density in spite of

its periodicity. L has to be larger than the pattern size in order to recognize that pattern.

When s = 1, L should be larger than 50 to recognize ‘025125’ as a pattern. However, we

had set L equal to 6 in the ﬁrst place, which is not large enough to recognize ‘025125.’ On

the other hand, L larger than 2 is enough for the repeated pattern of ‘01’ for the case with

s = 25. Therefore the entropy density for the series with s = 25 is lower than that for the

case with s = 1.

Firstly, it has been suggested that

Next, we turn to the important factors we should consider when choosing the value of L.

L < logk N,

(9)

where N is the total number of data points in the series and k is the number of symbols

set to 2 in this paper. The upper bound for L varies with s since N = S/s. To investigate

only the eﬀect of s on the entropy density we have to ﬁx the value of L throughout all sets
of time series. Since we set the maximum of s to 200, the smallest value of N is 105

200 = 500
and the upper bound for this series is directly calculated as L = 8 by Eq. (9). This value

becomes the upper bound of L for all the other time series too.

Secondly, we should choose an appropriate value of L to approximate hµ by hµ(s, L). If

hµ(s, L) saturates to hµ monotonically as discussed in the previous Subsection, all we have

to do is to take the value of L as large as possible under the limit of the upper bound.

However, when considering real data with complex structure this is not always true. Figure

2 shows the entropy densities for various values of s and L using the function deﬁned in

Eq. (8) with 10% data corruption, where L is limited to the upper bound 8. The key point

is to ﬁnd the horizontally stable area of hµ(s, L) according to L for all values of s. This

is the important criterion when choosing the value of L to avoid the unreasonable results.

As shown in Fig. 2 it is reasonable to set L to 6 or 7 and we had chosen 6 in the ﬁrst

6

place. Conclusively we believe that the following two rules would be enough when choosing

an appropriate value of L.

1. L should be lower than logk N (N = S/smax and k is the number of symbols).

2. Satisfying 1, L should be chosen inside the range where the entropy density hµ(s, L)

is stable for all s.

The MEDM that we have discussed so far can be summarized into four main steps

including how to choose an appropriate value of L:

1. Decide the range of coarse-graining scale s.

2. Choose the appropriate L∗ giving the reasonable value of hµ(s, L) for the whole range

of s.

3. Find the scale s∗ minimizing the entropy density hµ(s, L∗) by tuning s.

4. The value s∗ is the correlation length of the original function.

C. Model applications

For more general cases we can take smaller scales to increase the precision of measuring

the correlation length of the given time series. We now show an eﬃcient way to vary s by

considering a corrupted periodic function with non-integer period, for example p = 77.6.

Y (t) = 


r

cos2

π
77.6 t
(cid:1)

(cid:0)

for 90% of the data

for 10% of the data

.

(10)

In this case, to measure the exact correlation length (p/2 for the case of periodic functions)



the scale of s should be smaller than 0.1. To scan the whole range of s from 0.1 to 100.0 by

the increment of 0.1 is very ineﬃcient. Instead of varying s in the smallest scale we vary s in

larger scale ﬁrst and then move down to the smaller scales. The value of L is set to 6 again.

Figure 3(a) shows the entropy densities for various s in the order of 10. The minimum of

the entropy density occurs at s = 40. We narrow the variation of s down to 1 around 40.

Then the minimum of the entropy density occurs at s = 39 in Fig. 3(b) and we repeat the

same process again. Finally, in Fig. 3(c) we obtain s = 38.8 minimizing the entropy density,

7

which is exactly half period (p/2). Deﬁnitely this process is much more eﬃcient than that

of increasing s linearly from 0.1 to 100.0.

Finally, the MEDM is applied to the function with changing period for more realistic

considerations. To investigate the change of correlation lengths we divide the total span

into several regions and apply MEDM to each of them. We take the periodic function with

linearly decreasing period:

cos2

π
p(t) t
(cid:17)

(cid:16)

Y (t) = 


r

for 90% of the data

for 10% of the data

,

(11)

p(t) = p(0) −



∆p
S

t,

where the total change of the period is ∆p. p(0) is set to 50, ∆p to 10, and S to 105,

therefore the period of the function decreases from 50 to 40.

We should decide on which scale we want to see the change. The total temporal span

is divided into 10 regions, for each of which the MEDM is applied. For all the regions, we

vary s in the order of 1 and set L to 6 after testing in a way we described for all the regions.

Figure 4 shows that the correlation length s∗ decreases from 25 in the ﬁrst region to 20 in

the last one. These s∗s are the exact half periods of the starting and ending points of the

original function. Interestingly even when the correlation length inside each region changes

continuously, the MEDM detects the closest integer. If the total span is divided into more

regions and the variation of s is narrowed we can get a more straight line than that shown

in Fig. 4.

III. EMPIRICAL DATA ANALYSIS

Now we apply the MEDM to analyze the ﬁnancial time series of the S&P500 index from

year 1983 to 2006. It is reasonable to think that the correlation length of S&P500 index for

24 years would change from time to time. Hence the formalism of the last example in the

previous Section is used. It should be noted that although the time series of the S&P500

index is not periodic, we can always measure the correlation length using MEDM whenever

the series has patterns.

The total time span of the index data is divided into 279 months from February 1983 to

April 2006, i.e. each region for each month. For each region the most correlated time interval

8

is obtained then the temporal behavior of the intervals for all the regions is analyzed. The

unit of coarse-graining scale s is set to 1 tick, the lowest resolution of the empirical S&P500

index data. On average there are 4 ticks in one minute though the real time intervals

between adjacent ticks are not equally distributed. One reasonable way to ﬁx this problem

is to obtain the correlation length s∗

tick in a unit of tick for each month and multiply it

by the average real time interval ¯τtick between ticks of that month. The resulting value

s∗ = s∗

tick · ¯τtick will be the most correlated time interval each month.

Then we follow the four main steps of the MEDM to measure the correlation length of

tick every month. For the ﬁrst step the range of stick is set to 1 tick to 30 ticks. The tick

series with stick = 30 has less than 900 data points each month. By Eq. (9) the upper

bound of L is 9. Considering the second rule of choosing L, we set L∗ to 5 by ﬁnding the

stable area of the entropy densities for the whole range of stick. Figure 5 shows the entropy

densities hµ(stick, L) only for the regions of February 1983 and April 2006. For the third
step the scale s∗

tick that minimizes the value hµ(stick, 5) is determined for each month. Three

examples are shown in Fig. 6, where hµ(stick, 5) is minimized at stick = 8 for January 1987,

at stick = 6 for January 1996, and at stick = 2 for January 2001 respectively. After ﬁnding

all the tick intervals that minimize hµ(stick, 5), we convert them into the real time scales

by multiplying the average time interval between ticks for each month. Figure 7 shows the

temporal behavior of the correlation length as the most correlated time interval.

We analyze the meaning of this result by considering the interactions among agents in

the stock market. The stock market price changes only when the agents in the stock market

buy or sell. Since the agents make decisions based on the information they have, the velocity

of information ﬂow can be the most important factor for the changing rate of price. We

deﬁne the time required to deliver information as ∆tI and the time interval we choose to

measure the entropy density as ∆tM . The information delivery time ∆tI is assumed to be

directly proportional to the average price change cycle.

If ∆tM < ∆tI the scale smaller

than the intrinsic correlation length makes us observe the time series in more detail so

that the time series looks more random due to the high frequency noise. On the other

hand if ∆tM > ∆tI we overlook the pattern embedded in the time series so fail to detect

the correlation length and the time series looks more random in a sense. For detecting

the correlation length optimally ∆tM closest to ∆tI can be obtained to ﬁnd the value of
tick that minimizes the entropy density for each month. The results s∗
s∗

tick shown in Fig. 7

9

correspond to ∆tM closest to ∆tI . The long-term decrease of s∗

tick from year 1986 to 2006

suggests the decrease of the information delivery time ∆tI. Around year 1997 the value of

s∗
tick suddenly jumps down, when the Internet was starting to spread widely, the ratio of

online traders increased exponentially, and this inﬂuenced the information delivery time of

the stock market to become much shorter.

Since there is not any standardized way to measure the information delivery time ∆tI

we suggest ∆tM or s∗

tick as the standard and call it the eﬀective delay of information (EDI).

EDI can be also used to measure the eﬃciency of the stock market: when the market is

idealized with EMH [2] EDI becomes 0. In addition from our quantitative analysis EDI of

the S&P500 index is about 17 seconds in year 2006.

IV. CONCLUSIONS

In this paper we have developed the minimum entropy density method and the concept of

eﬀective delay of information to detect the most correlated time interval of given time series

in relation to the velocity of information ﬂow. By applying the MEDM to the ﬁnancial time

series of S&P500 index we identiﬁed that the most correlated time interval is getting shorter

for the last twenty years. The result that EDI minimizing the entropy density has decreased

for the last twenty years implies that the velocity of information ﬂow is getting faster. The

tendency of increasing velocity of information ﬂow has been mentioned by Ref. [12]. Since

EDI measures the eﬃciency of the stock market, by quantitative analysis we conclude that

the eﬃciency of the U.S. stock market dynamics became close to EMH.

[1] W. B. Arthur, S. N. Durlauf, and D. A. Lane, The Economy as an Evolving Complex System

II (Perseus Books, 1997).

[2] R. N. Mantegna and H. E. Stanley, An Introduction to Econophysics: Correlations and Com-

plexity in Finance (Cambridge University Press, 2000).

[3] J.-P. Bouchaud and M. Potters, Theory of Financial Risks (Cambridge University Press,

2000).

[4] B. B. Mandelbrot, Quant. Finance 1, 124 (2001).

[5] A. D. Martino and M. Marsili, arXiv:physics/0606107 (2006).

10

[6] T. Kaizoji, Physica A 287, 493 (2000).

[7] E. E. Peters, Chaos and order in the capital markets (Wiely, 1991).

[8] C.-K. Peng, S. V. Buldyrev, S. Havlin, M. Simons, H. E. Stanley, and A. L. Goldberger, Phys.

[9] Y. Liu, P. Gopikrishnan, P. Cizeau, M. Meyer, C.-K. Peng, and H. E. Stanley, Phys. Rev. E

Rev. E 49, 1685 (1994).

60, 1390 (1999).

[10] D. Feldman, A Brief Introduction to Information Theory, Excess Entropy and Computational

Mechanics, http://hornacek.coa.edu/dave/Tutorial/index.html (April 1998).

[11] R. W. Clarke, M. P. Freeman, and N. W. Watkins, Phys. Rev. E 67, 016203 (2003).

[12] J.-S. Yang, S. Chae, W.-S. Jung, and H.-T. Moon, Physica A 363, 377 (2006).

11

(a)

)
6
,
s
(

µ

h

0.6

0.4

0.2

0

0

(b)

0.8

)
6
,
s
(

µ

h

(c)

)
6

,
s
(

µ

h

0

0

0.6

0.4

0.2

0.8

0.6

0.4

0.2

50

100

150

200

50

100

150

200

0

50

150

200

100
s

12

FIG. 1: The entropy densities hµ(s, L = 6) of periodic time series as functions of time interval s (a)

without any corruption within the data, (b) with 5% data corruption, and (c) 10% data corruption.

0.2

 
0

2

4

L

6

8

FIG. 2: The entropy densities hµ(s, L) of series with time intervals 20, 80, 140, and 200 when 10%

of the data is corrupted.

1

0.8

0.6

0.4

)
L
,
s
(

µ

h

(a)

)
6

,
s
(

h

µ

0.8

0.6

0.4

0.2

0

0.5

0.4

0.3

0.2

0.3

0.25

0.2

0.15

(b)

)
6

,
s
(

h

µ

(c)

)
6
,
s
(

h

µ

 

s=20
s=80
s=140
s=200

20

40

60

80

100

34

36

38

40

42

44

38.4

38.6

38.8

39

39.2

39.4

s

13

FIG. 3: The entropy densities hµ(s, L = 6) measured in the precision of 10 (a), 1 (b), and 0.1 (c).

0

2

8

10

4

6
Region

FIG. 4: The correlation length s∗ of each partitioned region.

*

s

26

25

24

23

22

21

20

19

(a)

1

)
L

,
 
k
c
i
t

s
(

h

µ

0.9

0.8

0.7

(b)

)
L
,
 
k
c
i
t

s
(

h

µ

 
0

1

0.98

0.96

 
0

 

 

2

4

6

8

10

2

4

8

10

6

L

14

FIG. 5: The entropy densities hµ(stick, L) of S&P500 index in February 1983 (a) and in April 2006

(b) using the time series with time interval 1, 2, 5, and 10 minutes respectively.

FIG. 6: The entropy densities hµ(stick, L = 5) of S&P500 index measured in January 1987, January

1996, and January 2001 respectively.

 

Jan 1987
Jan 1996
Jan 2001

10

20

30

stick

1

0.95

)
5
,
 
k
c
i
t

s
(

h

µ

0.9

 
0

)
n
m

i

(
 

s

*

12

10

8

6

4

2

0

’85

’90

’00

’05

’95
Year

FIG. 7: The temporal behavior of the most correlated time interval s∗ of the S&P500 index

measured monthly.

15


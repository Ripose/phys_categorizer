NUHEP Report 1010
May 1, 2005

Sifting data in the real world

M. M. Block

Department of Physics and Astronomy,
Northwestern University, Evanston, IL 60208

Abstract

In the real world, experimental data are rarely, if ever, distributed as a normal (Gaussian) distribu-
tion. As an example, a large set of data—such as the cross sections for particle scattering as a function
of energy contained in the archives of the Particle Data Group[1]—is a compendium of all published
data, and hence, unscreened. Inspection of similar data sets quickly shows that, for many reasons, these
data sets have many outliers—points well beyond what is expected from a normal distribution—thus
ruling out the use of conventional χ2 techniques. This note suggests an adaptive algorithm that allows a
phenomenologist to apply to the data sample a sieve whose mesh is coarse enough to let the background
fall through, but ﬁne enough to retain the preponderance of the signal, thus sifting the data. A prescrip-
tion is given for ﬁnding a robust estimate of the best-ﬁt model parameters in the presence of a noisy
background, together with a robust estimate of the model parameter errors, as well as a determination of
the goodness-of-ﬁt of the data to the theoretical hypothesis. Extensive computer simulations are carried
out to test the algorithm for both its accuracy and stability under varying background conditions.

5
0
0
2
 
n
u
J
 
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
0
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

Introduction

In an idealized world where all of the data follow a normal (Gaussian) distribution, the use of the χ2 likelihood
technique, through minimization of χ2, described in detail in A.2, oﬀers a powerful statistical analysis tool
when ﬁtting models to a data sample. It allows the phenomenologist to conclude either:

The model is accepted, based on the value of its χ2
min, when compared
to ν, the numbers of degrees of freedom, has a reasonably high probability (χ2
ν). On the other
hand, it might be accepted with a much poorer χ2
min, depending on the phenomenologist’s judgment.
In any event, the goodness-of-ﬁt of the data to the model is known and an informed judgment can be
made.

min. It certainly ﬁts well when χ2
min ∼

Its parameter errors are such that a change of ∆χ2 = 1 from χ2
min corresponds to changing a parameter
by its standard error σ. These errors and their correlations are summarized in the standard covariance
matrix C discussed in Appendix A.2.

•

•

or

The model is rejected, because the probability that the data set ﬁts the model is too low, i.e., χ2
ν.

•

min >>

This decision-making capability (of accepting or rejecting the model) is of the primary importance, as is the
ability to estimate the parameter errors and their correlations.

Unfortunately, in the real world, experimental data sets are at best, only approximately Gaussian, and
often are riddled with outliers—points far oﬀ from a best ﬁt curve to the data, being many standard deviations
away. This can be due to many sources, copying errors, bad measurements, wrong calibrations, misassignment
of experimental errors, etc. It is this world that our note wishes to address—a world with many data points,
and perhaps, many diﬀerent experiments from many diﬀerent experimenters, with possibly a signiﬁcant
number of outliers.

In Section 2, we will propose our “Sieve” algorithm, an adaptive technique for discarding outliers while
retaining the vast majority of the good data. This then allows us to estimate the goodness-of ﬁt and make
a robust determination of both the parameters and their errors—for a discussion of the term “robust”, see
Appendix A. In essence, we then retain all of the statistical beneﬁts of the conventional χ2 technique.

In Sections 3.7.1 and 3.7.2, we will apply the algorithm to high energy ¯pp and pp scattering, as well as to
π−p and π+p scattering. Eight examples of real world experimental data, for both ¯pp and pp scattering and
π+p and π−p scattering, are taken from the Particle Data Group archives[1] and are illustrated in Figures 1,
2, 3 and 4, respectively. The data in Fig. 1 are all of the known published data for the total cross sections
σ¯pp and σpp for cms (center of mass) energies greater than 6 GeV. The measured ρ ¯pp and ρpp, where ρ is the
ratio of the real to the imaginary portion of the forward scattering amplitude, are shown in Fig. 2, again
for cms energies greater that 6 GeV. The data in Fig. 3 are all of the known published data for the total
cross sections σπ−p and σπ+p for cms energies greater that 6 GeV. The measured ρπ−p and ρπ+p are shown
in Fig. 4, again for cms energies greater that 6 GeV. Detailed examination of Figures 1, 2, 3 and 4 show
many points far oﬀ of the common trend, often at the same energy. Attempts to use the χ2 technique to ﬁt
these data with a model will always come up short. These ﬁts will always return a huge value of χ2
min/ν,
together with model parameters that are likely to be unreliable.

In Section 3, we make three types of computer simulations, generating data normally distributed about
a straight line, a constant, and about a parabola, along with outliers—artiﬁcial worlds where we know all
of the answers, i.e., which points are signal and which are noise. Examples for the straight line, a constant
(two cases) and the parabola are shown in Fig. 5, 6, 7 and 8. Details are given in Sections 3.1, 3.3 and 3.6.
The noise points in Fig. 5a, 6a, 7a, and Fig. 8a are the diamonds, whereas the signal points are the circles.
The dashed curve in Fig. 5b is the result of a χ2 ﬁt to all of the noisy data (100 signal plus 20 noise
points) in Figure 5a and is not a very good ﬁt to the data. The solid line is the ﬁt with the “ Sieve” algorithm
2x that was used to
proposed in the next Section. It reproduces nicely the theoretical straight line y = 1
computer-generate data that were normally distributed about it, using random numbers. In this case, the
20 noise points penetrated the signal down to a level ∆χ2

−

i > 6.

– 1 –

In Fig. 6b we show the results for ﬁtting the constant y = 10. The noise points (diamonds) in Fig. 6a

penetrate the signal down to ∆χ2

i > 4.

7a penetrate the signal down to ∆χ2

i > 9.

In Fig. 7b we show the results for ﬁtting the constant y = 10, where the noise points (diamonds) in Fig.

In Fig. 8a the data were generated about the parabola y = 1 + 2x + 0.5x2, with background noise. Figure
8b shows the result of sifting the data according to our Sieve algorithm, described below. The noise points
that are retained after invoking our algorithm are the diamonds in Fig. 8b and the circles are the signal
points that are retained.

In Sections 3.2 and 3.3, we will calibrate the algorithm with extensive computer-generated numerical
simulations and test it for stability and accuracy. The lessons learned from these computer simulations of
events are summarized in Section 3.4.

Finally, in Appendix A, we give mathematical details about ﬁtting data using the robust Λ2 (Lorentzian)
maximum likelihood estimator that we employ in our “Sieve” algorithm and in particular, Λ2
0, which mini-
mizes the rms (root mean square) widths of the parameter distributions, making them essentially the same
as the rms distributions of a χ2 ﬁt. We also discuss ﬁtting data with the more conventional χ2 maximum
likelihood estimator.

2 The Adaptive Sieve Algorithm

2.1 Major assumptions

Our major assumptions about the experimental data are:

1. The experimental data can be ﬁtted by a model which successfully describes the data.

2. The signal data are Gaussianly distributed, with Gaussian errors.

3. That we have “outliers” only, so that the background consists only of points “far away” from the true

signal.

4. The noise data, i.e. the outliers, do not completely swamp the signal data.

2.2 Algorithmic steps

We now outline our adaptive Sieve algorithm, consisting of several steps:

1. Make a robust ﬁt (see Appendix A) of all of the data (presumed outliers and all) by minimizing Λ2
0,

the Lorentzian squared, deﬁned as

N

Λ2

0(α; x)

≡

i=1
X

(cid:8)

ln

1 + 0.179∆χ2

i (xi; α)

,

(cid:9)

(1)

described in detail in the Appendix A.4. The M -dimensional parameter space of the ﬁt is given by
represents the abscissa of the N experimental measurements
α =

α1, . . . , αM }
{
y1, . . . , yN }

, x =
x1, . . . , xN }
that are being ﬁt and ∆χ2

{

{

i (xi; α)

y =
, where y(xi; α) is the theoretical
value at xi and σi is the experimental error. As discussed in Appendix A.4, minimizing Λ2
0 gives the
same total χ2
i (xi; α) from eq. (1) as that found in a χ2 ﬁt, as well as rms widths (errors)
for the parameters—for Gaussianly distributed data—that are almost the same as those found in a χ2
ﬁt. The quantitative measure of “far away” from the true signal, i.e., point i is an outlier corresponding
to Assumption (3), is the magnitude of its ∆χ2

N
i=1 ∆χ2

min ≡

P

≡

(cid:17)

(cid:16)

yi

−

−

2

.

i (xi; α) =

y(xi;α)
σi

min is satisfactory, make a conventional χ2 ﬁt to get the errors and you are ﬁnished. If χ2

min is not

(cid:16)

(cid:17)

If χ2
satisfactory, proceed to step 2.

yi

y(xi;α)
σi

2

– 2 –

2. Using the above robust Λ2

0 ﬁt as the initial estimator for the theoretical curve, evaluate ∆χ2

i (xi; α),

for each of the N experimental points.

3. A largest cut, ∆χ2
i (xi; α)max = 9.

i (xi; α)max, must now be selected. For example, we might start the process with
i (xi; α) > ∆χ2
i (xi; α)max, reject them—they fell
i (xi; α)max is an attempt to pick the largest “Sieve” size
i (xi; α)max) that rejects all of the outliers, while minimizing the number of signal points

∆χ2
through the “Sieve”. The choice of ∆χ2
(largest ∆χ2
rejected.

If any of the points have ∆χ2

i (xi; α) > ∆χ2

4. Next, make a conventional χ2 ﬁt to the sifted set—these data points are the ones that have been
min. Since the data set has been truncated by
min found

retained in the “Sieve”. This ﬁt is used to estimate χ2
i (xi; α)max, we must slightly renormalize the χ2
eliminating the points with ∆χ2
to take this into account, by the factor
. This eﬀect is discussed later in detail in Section 3.4.
If the renormalized χ2
χ2
min is acceptable—in the conventional sense, using the χ2 distribu-
tion probability function—we consider the ﬁt of the data to the model to be satisfactory and proceed
to the next step. If the renormalized χ2
i (xi; α)max is not too small, we
pick a smaller ∆χ2
i (xi; α)max that makes
much sense, in our opinion, is ∆χ2
i (xi; α)max > 2. After all, one of our primary assumptions is that
the noise doesn’t swamp the signal. If it does, then we must discard the model—we can do nothing
further with this model and data set!

i (xi; α)max and go back to step 3. The smallest value of ∆χ2

min is not acceptable and ∆χ2

min, i.e.,

R ×

R

×

5. From the χ2 ﬁt that was made to the “sifted” data in the preceding step, evaluate the parameters α.
M covariance (squared error) matrix of the parameter space which was found
Next, evaluate the M
in the χ2 ﬁt. We ﬁnd the new squared error matrix for the Λ2 ﬁt by multiplying the covariance matrix
by the square of the factor rχ2 (for example, as shown later in Section 3.2.2, rχ2
1.02, 1.05, 1.11
and 1.14 for ∆χ2
i (xi; α)max = 9, 6, 4 and 2, respectively ). The values of rχ2 > 1 reﬂect the fact that
a χ2 ﬁt to the truncated Gaussian distribution that we obtain—after ﬁrst making a robust ﬁt—has a
rms (root mean square) width which is somewhat greater than the rms width of the χ2 ﬁt to the same
untruncated distribution. Extensive computer simulations, summarized in Section 3.4, demonstrate
that this robust method of error estimation yields accurate error estimates and error correlations, even
in the presence of large backgrounds.

∼

You are now ﬁnished. The initial robust Λ2

0 ﬁt has been used to allow the phenomenologist to ﬁnd a
sifted data set. The subsequent application of a χ2 ﬁt to the sifted set gives stable estimates of the model
parameters α, as well as a goodness-of-ﬁt of the data to the model when χ2
min is renormalized for the eﬀect
of truncation due to the cut ∆χ2
i (xi; α)max. Model parameter errors are found when the covariance (squared
error) matrix of the χ2 ﬁt is multiplied by the appropriate factor (rχ2 )2 for the cut ∆χ2

i (xi; α)max.

It is the combination of using both Λ2

0 (robust) ﬁtting and χ2 ﬁtting techniques on the sifted set that
gives the Sieve algorithm its power to make both a robust estimate of the parameters α as well as a robust
estimate of their errors, along with an estimate of the goodness-of-ﬁt.

Using this same sifted data set, you might then try to ﬁt to a diﬀerent theoretical model and ﬁnd χ2
min
for this second model. Now one can compare the probability of each model in a meaningful way, by using
the χ2 probability distribution function of the numbers of degrees of freedom for each of the models. If the
second model had a very unlikely χ2
min, it could now be eliminated. In any event, the model maker would
now have an objective comparison of the probabilities of the two models.

2.3 Evaluating the Sieve algorithm

We will give two separate types of examples which illustrate the Sieve algorithm.
computer-generated data, normally distributed about

In the ﬁrst type, we

a straight line, along with random noise to provide outliers,

a constant, along with random noise to provide outliers,

•

•

•

a parabola, with background noise normally distributed about a slightly diﬀerent parabola,

– 3 –

the details of which are described below. The advantage here, of course, is that we know which points are
signal and which points are noise.

For our real world example, we took four types of experimental data for elementary particle scattering
from the archives of the Particle Data Group[1]. For all energies above 6 GeV, we took total cross sections
and ρ-values and made a ﬁt to these data. These were all published data points and the entire sample was
used in our ﬁt. We then made separate ﬁts to

¯pp and pp total cross sections and ρ-values,

π−p and π+p total cross sections σ and ρ-values,

•

•

using eqns. (7), (8) and (9) below.

3 Studies using large computer-generated data sets

Extensive computer simulations were made using the straight line model yi = 1
2xi and the constant model
yi = 10. Over 500,000 events were computer-generated, with normal distributions of 100 signal points per
event, some with no noise and others with 20% and 40% noise added, in order to investigate the accuracy
and stability of the “Sieve” algorithm. The cuts ∆χ2

i > 9, 6, 4 and 2 were investigated in detail.

−

3.1 A straight line model

An event consisted of generating 100 signal points plus either 20 or 40 background points, for a total of
120 or 140 points, depending on the background level desired. Let RND be a random number, uniformly
distributed from 0 to 1. Using random number generators, the ﬁrst 100 points used xi = 10
RND, where i
is the point number. This gives a signal randomly distributed between x = 0 and x = 10. For each point xi,
2xi. Next, the value of σi, the “experimental error”, i.e, the
a theoretical value ¯yi was found using ¯yi = 1
RND. Using these σi, the yi’s were generated,
error bar assigned to point i, was generated as σi = ai + αi ×
normally distributed[3] about the value of ¯yi For i = 1 to 50, ai = 0.2, αi = 1.5, and for i = 51 to 100,
ai = 0.2, αi = 3. This sample of 100 points made up the signal.

×

−

RND. The xi were generated as xi = di +δi×

The 40 noise points, i = 101 to 140 were generated as follows. Each point was assigned an “experimental
RND. In order to provide outliers, the value
σi and the points were then placed at this ﬁxed value of
i (xi; α)max
i (xi; α)max = 2, 4, 6 or 9, respectively, and was independent

error” σi = ai+αi×
of yi was ﬁxed at yi = 1
−
yi and given the “experimental error” σi. The parameter fcut depended only on the value of ∆χ2
that was chosen, being 1.9, 2.8, 3.4 or 4, for ∆χ2
of i. These choices of fcut made outliers that only existed for values of ∆χ2

2xi +fcut ×

Signi ×

(bi +βi)

i (xi; α) > ∆χ2

i (xi; α)max.

×

For i = 101 to 116, di = 0, δi = 10, ai = 0.75, αi = 0.5, bi = 1.0, βi = 0.6. To make “doubles” at the
1, so that the

100 we pick Signi = +1; otherwise Signi =

100 > 1

2xi

same xi as a signal point, if yi
−
outlier is on the same side of the reference line 1

−

−

2xi as is the signal point.

−

For i = 117 to 128, di = 0, δi = 10, ai = 0.5, αi = 0.5, bi = 1.0, βi = 0.6; Signi was randomly chosen as
+1 or -1. This generates outliers randomly distributed above and below the reference line, with xi randomly
distributed from 0 to 10.

−

For i = 129 to 140, di = 8, δi = 2, ai = 0.5, αi = 0.5, bi = 1.0, βi = 0.6; Signi = +1. This makes points
in a “corner” of the plot, since xi is now randomly distributed at the “edge” of the plot, between 8 and 10.
Further, all of this points are above the line, since Signi is ﬁxed at +1, giving these points a large lever arm
in the ﬁt.

For the events generated with 20 noise points, the above recipes for background were simply halved. An
i (xi; α)max = 6, is shown in Fig. 5a, with the

example of such an event containing 120 points, for which ∆χ2
100 squares being the normally distributed data and the 20 circles being the noise data.

After a robust ﬁt to the entire 120 points, the sifted data set retained 100 points after the ∆χ2

i > 6
condition was applied. This ﬁt had χ2
min/ν = 0.905.
min/ν = 1.01—see Section 3.4 for details
Using a renormalization factor
of the renormalization factor. After using the Sieve algorithm, by minimizing χ2 for the sifted set, we found
that the best-ﬁt straight line, y =< a > + < b > x, had < a >= 0.998
0.020.
The parameter errors given above come from multiplying the errors found in a conventional χ2 ﬁt to the

min = 88.69, with an expected χ2 = ν = 98, giving χ2

= 1/0.901, we get a renormalized χ2

0.12 and < b >=

2.014

R

±

−

±

– 4 –

sifted data by the factor rχ2 = 1.05—for details see Section 3.4. This turns out to be a high probability ﬁt[2]
with a probability of 0.48 (since the renormalized χ2
0.14).
i max = 6. Of the original 120
points, all 100 of the signal points were retained (squares), while no noise points (diamonds) were retained.
The solid line is the best χ2 ﬁt, y = 0.998

Figure 5b shows the results after the use of the Sieve procedure with ∆χ2

min/ν = 1.01, whereas we expect < χ2/ν >= 1.0

2.014x.

±

Had we applied a χ2 minimization to original 120 point data set, we would have found χ2 = 570, which
1.98x, is also

has inﬁnitesimal statistical probability. The straight line resulting from that ﬁt, y = 0.925
shown in Fig. 5b as the dot-dashed curve. For large x, it tends to overestimate the true values.

−

−

To investigate the stability of our procedure with respect to our choice of ∆χ2

i , we reanalyzed the full
i max = 4. The evaluation of the parameters a and b was completely stable,
i . The robustness of this procedure on this particular data set is

data set for the cut-oﬀ, ∆χ2
essentially independent of the choice of ∆χ2
evident.

3.2 Distributional widths for the straight line model

We now generate extensive computer simulations of data sets resulting from the straight line yi = 1
2xi using
the recipe of Section 3.1, with and without outliers, in order to test the Sieve algorithm. We have generated
50,000 events with 20% background and 50,000 events with 40% background, for each cut ∆χ2
i max = 9, 6, 4
and 2. We also generated 100,000 Gaussianly distributed events with no noise.

−

3.2.1 Case 1

We generated 100,000 Gaussianly distributed events with no noise. Let a and b be the intercept and slope
2x and deﬁne < a > as the average a, < b > as the average b found for the
of the straight line y = 1
0 (robust) ﬁt and a χ2 ﬁt.
100,000 straight-line events, each generated with 100 data points, using both a Λ2
The purpose of this exercise was to ﬁnd r(Λ2
0) divided by
Σ, the parameter error from the χ2 ﬁt, i.e.

0 rms parameter width σ(Λ2

0), the ratio of the Λ2

−

ra(Λ2
0)

σa(Λ2
0)
Σa

,

≡

rb(Λ2
0)

σb(Λ2
0)
Σb

,

≡

as well as demonstrate that there were no biases (oﬀsets) in parameter determinations found in Λ2 and χ2
ﬁts.

The measured oﬀsets 1

< aΛ2 >,
compatible with zero, as expected, indicating that the parameter expectations were not biased.

< bχ2 > and

< aχ2 >, 1

< bΛ2 > were all numerically

−

−

−

−

−

−

2

2

Let σ be the rms width of a parameter distribution and Σ the error from the χ2 covariant matrix. We

found:

σa(χ2) = 0.139
±
σb(χ2) = 0.0261

and Σa = 0.138

0.002
0.003 and Σb = 0.0241,

±

ra(Λ2
rb(Λ2

0) = 1.034
0) = 1.029

0.010

0.011,

±

±

demonstrating that:

showing that the rms widths σ and parameter errors Σ were the same for the χ2 ﬁt, as expected. Further,
the width ratios r for the Λ2

0 ﬁt are given by

the r’s of the Λ2

0 are almost as good as that of the χ2 distribution, r(χ2) = 1.

the ratios of the rms Λ2 width to the rms χ2 width for both parameters a and b are the same, i.e., we
can now simply write

•

•

rΛ2 =

σΛ2
Σ ∼

1.03.

(2)

Finally, we ﬁnd that 1

< χ2/ν >= 0.00034

0.00044, which is approximately zero, as expected.

−

±

– 5 –

3.2.2 Case 2

For Case 2, we investigate data generated with 20% and 40% noise that have been subjected to the adaptive
Sieve algorithm, i.e.
i (xi; α)max = 9, 6, 4 and 2. We investigated this
truncated sample to measure possible biases and to obtain numerical values for r’s.

the sifted data after cuts of ∆χ2

We generated 50,000 events, each with 100 points normally distributed and with either 20 or 40 outliers,
for each cut. A robust ﬁt was made to the entire sample (either 120 or 140 points) and we sifted the data,
rejecting all points with either ∆χ2
i (xi; α) > 9, 6, 4 and 2, according to how the data were generated. A
conventional χ2 analysis was then made to the sifted data. The results are summarized in Table 1. As
before, we found that the widths from the χ2 ﬁt were slightly smaller than the widths from a robust ﬁt, so
we adopted only the results for the χ2 ﬁt.

There were negligible oﬀsets 1
and σb, for both the robust and χ2 ﬁts.

−

< a > and

2

−

−

< b >, being

1 to 5% of the relevant rms widths, σa

∼

In any individual χ2 ﬁt to the jth data set, one measures aj, bi, Σaj , Σbj and (χ2

min/ν)j. Thus, we

characterize all of our computer simulations in terms of these 7 observables.

We again ﬁnd that the rχ2 values—deﬁned as σ/Σ—are the same, whether we are measuring a or b.
They are given by rχ2 = σ/Σ = 1.034, 1.054, 1.098 and 1.162 for the cuts ∆χ2
i (xi; α)max = 9, 6, 4 and 2,
respectively[4]. Further, they are the same for 20% noise and 40% noise, since the cuts rejected all of the
noise points. In addition, the r values were found to be the same as the r values for the case of truncated
pure signal, using the same ∆χ2
i (xi; α)max cuts. The signal retained was 99.7, 98.57, 95.5 and 84.3 % for the
cuts ∆χ2
i (xi; α)max = 9, 6, 4 and 2, respectively—see Section 3.4 and eq. (6) for theoretical values of the
amount of signal retained.

We experimentally determine the rms widths σ (the errors of the parameter) by multiplying the r value,
a known quantity independent of the particular event, by the appropriate Σ which is measured for that event,
i.e.,

σa = Σa ×
σb = Σb ×

rχ2
rχ2 .

∼

σa = rχ2 Σa
σb = rχ2 Σb,

The rms widths are now determined for any particular data set by multiplying the known factors rχ2 by the
appropriate Σ found (measured) from the covariant matrix of the χ2 ﬁt of that data set.

Also shown in Table 1 are the values of χ2

min/ν found for the various cuts. We will compare these results

later with those for the constant case, in Section 3.3

We again see that a sensible approach for data analysis–even where there are large backgrounds of
40%—is to use the parameter estimates for a and b from the truncated χ2 ﬁt and assign their errors as

(3)

where rχ2 is a function of the ∆χ2
the observed χ2

This strategy of using an adaptive ∆χ2

min/ν by the appropriate numerical factor for the ∆χ2

i max cut used.
i (xi; α)max cut minimizes the error assignments, guarantees robust

i max cut utilized. Before estimating the goodness-of-ﬁt, we must renormalize

ﬁt parameters with no signiﬁcant bias and also returns a goodness-of-ﬁt estimate.

3.3 The constant model, yi = 10
For this case, we investigate a diﬀerent theoretical model (yi = 10) with a diﬀerent background distribution,
to measure the values of rχ2 and < χ2

min/ν >.

An event consisted of generating 100 signal points plus either 20 or 40 background points, for a total
of 120 or 140 points, depending on the background level desired. Again, let RND be a random number,
uniformly distributed from 0 to 1. Using random number generators, for the ﬁrst 100 points i, a theoretical
value ¯yi = 10 was chosen. Next, the value of σi, the “experimental error”, i.e, the error bar assigned to point
RND. Using these σi, the yi’s were generated, normally distributed[3]
i, was generated as σi = ai + αi ×
about the value of ¯yi = 10 . For i = 1 to 50, ai = 0.2, αi = 1.5, and for i = 51 to 100, ai = 0.2, αi = 3.
This sample of 100 points made up the signal.

– 6 –

The 40 noise points, i = 101 to 140 were generated as follows. Each point was assigned an “experimental
signi ×
RND. In order to provide outliers, the value of yi was ﬁxed at yi = 10 + fcut ×
σi and the points were then placed at this ﬁxed value of yi and given the “experimental error”
i (xi; α)max that was chosen, being 1.9, 2.8, 3.4 or

error” σ = ai + αi ×
(bi + βi)
×
σi. The parameter fcut depended only on the value of ∆χ2
4, for ∆χ2

i (xi; α)max = 2, 4, 6 or 9, respectively, and was independent of i.

For i = 101 to 116, ai = 0.75, αi = 0.5, bi = 1.0, βi = 0.6; Signi was randomly chosen at +1 or -1.
For i = 117 to 128, ai = 0.5, αi = 0.5, bi = 1.0, βi = 0.6; This generates outliers randomly distributed

above and below the reference line, with xi randomly distributed from 0 to 10.

For i = 129 to 140, ai = 0.5, αi = 0.5, bi = 1.0, βi = 0.6; Signi = +1. This forces 12 points to be
greater than 10, since Signi is ﬁxed at +1. For the events generated with 20 noise points, the above recipes
for background were simply halved.

Two examples of events with 40 background points are shown in Figures 6a and 7a, with the 100 squares

being the normally distributed data and the 40 circles being the noise data.

In Fig. 6b we show the results after using the cut ∆χ2

retained, and 98 signal points (circles) are shown. The best ﬁt, y = 9.98
the dashed-dot curve is the ﬁt to all 140 points. The observed χ2
min/ν = 1.09, in good agreement with the expected value χ2
χ2

min/ν = 1

i (xi; α)max = 4. No noise points (diamonds) were
0.074, is the solid line, whereas
min/ν = 0.84 yields a renormalized value
0.14. If we had ﬁt to the entire

±

R ×
140 points, we would ﬁnd χ2

min/ν = 4.39, with the ﬁt being the dashed-dot curve.

±

In Fig. 7b we show the results after using the cut ∆χ2

retained, and 98 signal points (circles) are shown. The best ﬁt, y = 10.05
the dashed-dot curve is the ﬁt to all 140 points. The observed χ2

min/ν = 1.11, in good agreement with the expected value χ2
χ2

i (xi; α)max = 9. No noise points (diamonds) were
0.074, is the solid line, whereas
min/ν = 1.08 yields a renormalized value
0.14. If we had ﬁt to the
min/ν = 8.10, with the ﬁt being the dashed-dot curve. The details of the

min/ν = 1

±

±

R ×
entire 140 points, we would ﬁnd χ2
renormalization of χ2

min/ν and the assignment of the errors are given in Section 3.4

We computer-generated a total of 500,000 events, 50,000 events with 20% noise and an additional 50,000

events with 40% noise, for each of the cuts ∆χ2

i > 9, 6, 4 and 2, and 100,000 events with no noise.

For the sample with no cut and no noise, we found rΛ2

= 1.03

0.02, equal to the value rΛ2

= 1.03 that

0

0

±

was found for the straight line case.

Again, we found that our results for rχ2 were independent of background, as well as model, and only
< aχ2 >), although

depended on the cut. We also found that the biases (oﬀsets) for the constant case, (10
non-zero for the noise cases, were small in comparison to σ, the rms width.

−

The results for cuts ∆χ2

i max = 9, 6, 4 and 2 are detailed in Table 1. We see in Table 1, compared with
the straight line results of Section 3.2.2, that the rχ2 values for the constant case are essentially identical, as
expected. Further, we ﬁnd the same results for the values of χ2
i (xi; α)max.

min/ν as a function of the cut ∆χ2

3.4 Lessons learned from computer studies of a straight line model and a con-

stant model

•

•

As found in Sections 3.2.2 and 3.3 and detailed in Table 1, we have universal values of rχ2 and
< χ2

i (xi; α)max, independent of both background and model.

min > /ν, as a function of the cut ∆χ2

A sensible conservative approach for large backgrounds (less than or the order 40%) is to use the
parameter estimates from the χ2 ﬁt to the sifted data and assign the parameter errors to the ﬁtted
robust parameters as

σ(χ2) = rχ2

Σ,

×

where rχ2 is a function of the cut ∆χ2
i (xi; α)max, given by the average of the straight line and constant
cases of Table 1. This strategy gives us a minimum parameter error, with only very small biases to the
parameter estimates.

We must then renormalize the value found for χ2
for the straight line and constant case, again as a function of the cut ∆χ2

min/ν by the appropriate averaged value of < χ2

•

i (xi; α)max.

min > /ν

– 7 –

(4)

(5)

(6)

Let us deﬁne ∆ as the ∆χ2

i max cut and

as the renormalization factor that multiplies χ2

min/ν.

•

R

We ﬁnd from inspection of Cases 1 to 2 for the straight line and of Section 3.3 for the case of the
constant ﬁt that a best ﬁt parameterization of rχ2 , valid for ∆

2 is given by

≥

rχ2 = 1 + 0.246e−

0.263∆.

We note that

1, for large ν, is given analytically by

−

R

+√∆

1

−

R

≡

x2e−

x2/2 dx/

x2/2 dx

e−

+√∆

√∆

Z

−

Z

−

√∆
2
√π

−

= 1

∆/2

e−

.

erf(

∆/2)

1 are shown in Figures 9a and 9b, respectively. Some numerical
Graphical representations of rχ2 and
values are given in Table 1 and are compared to the computer-generated values found numerically for
the straight line and constant cases. The agreement is excellent.

R

−

p

•

Let us deﬁne σ0 as the rms parameter width that we would have had for a χ2 ﬁt to the uncut sample,
where the sample had had no background, and deﬁne Σ0 the error found from the covariant matrix.
They are, of course, equal to each other, as well as being the smallest error possible. We note that the
Σ/Σ0. This ratio is a function of the cut ∆ through both rχ2 and Σ, since for a
ratio σ/σ0 = rχ2
truncated distribution, Σ/Σ0 depends inversely on the square root of the fraction of signal points that
survive the cut ∆. In particular, the survival fraction S.F. is given by

×

S.F. =

+√∆

√∆

Z

−

1
√2π

x2/2 dx = erf(

e−

∆/2)

p

and is 99.73, 98.57, 95.45 and 84.27 % for the cuts ∆ = 9, 6, 4 and 2, respectively. The survival fraction
S.F..is shown in Table 1 as a function of the cut ∆χ2
i max, as well as is the ratio σ/σ0. We note that the
true cost of truncating a Gaussian distribution, i.e., the enlargement of the error due to truncation, is
not rχ2 , but rather rχ2 /√S.F., which ranges from
i max goes from 9 to
2. This rapid loss of accuracy is why the errors become intolerable for cuts ∆χ2

1.02 to 1.25 when the cut ∆χ2

i max smaller than 2.

∼

3.5 Fitting strategy

We ﬁnd that an eﬀective strategy for eliminating noise and making robust parameter estimates, together
with robust error assignments, is:

1. Make an initial Λ2

0 ﬁt to the entire data sample. If χ2
to the data and you are ﬁnished. If not, then proceed to the next step.

min/ν is satisfactory, then make a standard χ2 ﬁt

2. Pick a large value of ∆χ2

i (xi; α)max, e.g., ∆χ2

i (xi; α)max = 9.

3. Obtain a sifted sample by throwing away all points with ∆χ2

i (xi; α) > ∆χ2

i (xi; α)max.

4. Make a conventional χ2 ﬁt to the sifted sample. For your choice of ∆χ2

If the renormalized value
go to the next step. If, on the other hand,
and go to step 3 (for example, if you had used a cut of 9, now pick ∆χ2
Finally, if you reach ∆χ2
penetrated too much into the signal for the “Sieve” algorithm to work properly.

1 from eq. (5).
min/ν is suﬃciently near 1, i.e., the goodness-of-ﬁt is satisfactory, then
i (xi; α)max
i (xi; α)max = 6 and start again).
i (xi; α)max = 2 and you still don’t have success, quit—the background has

min/ν is too large, pick a smaller value of ∆χ2
χ2

i (xi; α)max, ﬁnd

R ×

R×

χ2

R

−

– 8 –

5. a) Use the parameter estimates found from the ∆χ2

i (xi; α)max ﬁt in the previous step.

b) Find a new squared error matrix by multiplying the covariant matrix C found in the χ2 ﬁt by (rχ2 )2.
Use the value of rχ2 found in eq. (4) for the chosen value of the cut ∆χ2
i (xi; α)max to obtain a robust
error estimate essentially independent of background distribution.

You are now ﬁnished. You have made a robust determination of the parameters, their errors and
the goodness-of-ﬁt.

∼

R

100, the error expected in χ2/ν is

The renormalization factors
are only used in estimating the value of the goodness-of-ﬁt, where small
changes in this value are not very important. Indeed, it hardly matters if the estimated renormalized χ2/ν
is between 1.00 and 1.01—the possible variation of the expected renormalized χ2/ν due to the two diﬀerent
background distributions. After all, it is a subjective judgment call on the part of the phenomenologist
as to whether the goodness-of-ﬁt is satisfactory. For large ν, only when χ2/ν starts approaching 1.5 does
0.14, so
one really begin to start worrying about the model. For ν
uncertainties in the renormalized χ2/ν of the order of several percent play no critical role. The accuracy of
the renormalized values is perfectly adequate for the purpose of judging whether to keep or discard a model.
In summary, extensive computer simulations for sifted data sets show that by combining the χ2 parameter
determinations with the corrected covariance matrix from the χ2 ﬁt, we obtain also a “robust” estimate of the
errors, basically independent of both the background distribution and the model. Further, the renormalized
0 ﬁt to sift the data and then a χ2 ﬁt
χ2
min/ν is a good predictor of the goodness-of-ﬁt. Having to make a Λ2
to the sifted data is a small computing cost to pay compared to the ability to make accurate predictions.
Clearly, if the data are not badly contaminated with outliers, e.g., if a ∆χ2
i (xi; α)max = 6 ﬁt is satisfactory,
the additional penalty paid is that the errors are enlarged by a factor of
1.06 (see Table 1), which is not
unreasonable to rescue a data set. Finally, if you are not happy about the error determinations, you can use
the parameter estimates you have found to make Monte Carlo simulations of your model[7]. By repeating a
Λ2
i (xi; α)max as was used
in the initial determination of the parameters, and ﬁnally, by making a χ2 ﬁt to the simulated sifted set you
can make an error determination based on the spread in the parameters found from the simulated data sets.

0 ﬁt to the simulated distributions and then sifting them to the same value of ∆χ2

∼

∼

3.6 The parabola

As a ﬁnal example of computer-generated data, we generated one noisy data set using a parabolic model.
A total of 135 points were generated by computer. Using random number generators, the ﬁrst 50 points
generated picked xi’s distributed randomly[3] from 0 to 10. For each point xi, a theoretical value ¯yi was found
using ¯yi = 1 + 2xi + 0.5x2
i . Next, the value of σi, the “experimental error”, i.e, the error bar assigned to point
i, was generated randomly on the interval 0.2 to 2.7. The yi’s were then generated, normally distributed[3]
about the value of ¯yi using the σi that had been previously found. The next 50 points were chosen in the
same manner, except that these σi were randomly distributed between 0.2 and 5.2. This sample of 100 points
made up the signal.

The 35 noise points were generated around a “nearby” parabola, given by ¯yi = 12 + 2xi + 0.2x2
i . The
ﬁrst 15 points had their xi again randomly generated in the interval 0 to 10. The error bars assigned to
each point were randomly distributed in the interval 0.2 to 5.2. To provide the outliers, the value of the
theoretical ¯yi was found using a new parabola ¯yi = 12 + 2xi + 0.2x2
i . These points were then normally
distributed using σi’s uniformly distributed in the interval 0.8 to 20.8. The next 20 were generated in the
same fashion, except that the error bars were uniformly distributed in the interval 0.2 to 8.2 and the yi
values normally distributed with σi’s in the interval 1.6 to 65.6. In this case, we not only made “outliers”,
but also contaminated the sample with substantial “inliers”, since we used a “nearby parabola” to generate
the background data. Of course, this violates our Assumption 3 that we only have outliers, but gives us a
feeling of what happens if substantial amounts of “inliers” are also present.

The resulting distribution of 135 points is shown in Fig. 8a, with the 100 squares being the normally

distributed data and the 35 circles being the noise data.

The sifted data set, shown in Fig. 8b, retained 113 points after the ∆χ2

i max = 6 condition was applied to
the original 135 points. At that point, we made both a conventional χ2 ﬁt to the sifted data set in order to
evaluate the parameters, their errors and the goodness of ﬁt. The χ2 ﬁt to the sifted data had χ2
min = 123.6,

– 9 –

±

±

±

R

χ2

with ν = 110, giving χ2

min/ν = 1.24, whereas we expect 1

min/ν = 1.12. Renormalizing using

found from eq. (5) , we get the corrected
0.06.
R ×
After using the Sieve algorithm, by minimizing χ2, we found that the best-ﬁt parabola, y = c0 + c1x + c2x2,
had c0 = 1.18
0.005, where the errors have been renormalized
by the factor rχ2 = 1.05 found from eq. (4).

0.13. This is a reasonable ﬁt[2] with a probability of

0.05 and c2 = 0.489

0.23 and c1 = 2.05

Figure 8b shows the results of using the Sieve procedure with the cut ∆χ2

i (xi; α)max = 6. Of the original
135 points, all 100 of the signal points were retained (squares). There were 13 noise points (circles) also
retained, all very close to the ﬁtted straight line. These points are the “inliers” that resulted from the
background generation using the “nearby parabola”, violating our primary assumption that there are only
“outliers” as background. Thus, it is of great interest to see how well the Sieve procedure worked.
Had we applied a χ2 minimization to original 130 point data set, we would have found χ2

min/ν = 19.93,
which clearly has inﬁnitesimal statistical probability. The parabola resulting from this χ2 ﬁt is also shown
in Fig. 8b. It clearly misses many of the data points in the sifted set.

±

∼

±

0.21, c1 = 2.13

When we ﬁtted the parabola to only the 100 signal points, with no noise included, we got the parameters:
0.005, using a conventional χ2 ﬁt. These parameters, within
c0 = 0.97
errors the same as those found using the “Sieve” algorithm, give a curve that is essentially indistinguishable
from the solid line in Fig. 8b obtained using the Sieve algorithm. We note that even when the background
produces some “inliers”, i.e., the cut ∆χ2
i max does not remove all of the background, the Sieve procedure is
still very useful.

0.05 and c2 = 0.480

±

±

i > 4, 6 or 9. Thus, even in the presence of

Finally, our procedure was completely stable for reasonable choices of ∆χ2

i , giving essentially the same
answer for ∆χ2
13% “inliers”, the answer after using the “Sieve”
was reasonable. The parameter values are relatively unaﬀected, as are the errors. The main concern is the
higher corrected χ2
min/ν that is due to the background points that are close to the true signal and thus can
not be “Sieved” out. However, this only aﬀects the goodness-of-ﬁt estimate, making χ2
min/ν somewhat larger.
In the end, the conclusion as to whether to accept the model or reject it on the basis of the goodness-of-ﬁt
estimate is a subjective judgment of the phenomenologist. Many models have been accepted when the χ2
probability has been as low as a few tenths of a percent.

∼

3.7 Real World data

We will illustrate the Sieve algorithm by simultaneously ﬁtting all of the published experimental data above
√s > 6 GeV for both the total cross sections σ and ρ values for ¯pp and pp scattering, as well as for π−p
and π+p scattering. The ρ value is the ratio of the real to the imaginary forward scattering amplitude and
√s is the cms energy Ecms. The data sets used have been taken from the Web site of the Particle Data
Group[1] and have not been modiﬁed. They provide the energy (xi), the measurement value (yi) and the
experimental error(σi), assumed to be a standard deviation, for each experimental point.

Testing the hypothesis that the cross sections rise asymptotically as ln2 s, as s

, the four functions

σ± and ρ± that we will simultaneously ﬁt for √s > 6 GeV are:

→ ∞

σ± = c0 + c1 ln

+ c2 ln2

ν
m

ρ± =

(cid:16)

(cid:17)

c1 + c2π ln

1
σ± (cid:26)

π
2

dσ±
d(ν/m)

= c1

1
(ν/m))

(cid:26)

(cid:27)

+ c2

ν
m

+ β

′

P

′ cot(

µ

1

−

ν
m

(cid:16)
πµ
2

)

(cid:17)

ν
m

±
µ

−

1

δ

(cid:16)
+

α

1

−

,

(cid:17)
f+(0)

ν
m
4π
ν

P

(cid:16)

(cid:17)
β

ν
m
−
2 ln(ν/m))
(ν/m))

(cid:17)

(cid:16)

(cid:26)
δ

±

(α

−

(cid:16)
′

(cid:17)
(µ

−

+ β

P

(cid:27)
1)(ν/m))α

2

−

(cid:8)
,

1)(ν/m))µ

−

2

(cid:9)

δ tan(

±

πα
2

)

ν
m

α

1

−

(cid:16)

(cid:17)

(cid:27)

(7)

, (8)

(9)

(cid:8)

where the upper sign is for pp (π+p) and the lower sign is for ¯pp (π−p) scattering[5]. The laboratory energy
is given by ν and m is the proton (pion) mass. The exponents µ and α are real, as are the 6 constants
′, δ and the dispersion relation subtraction constant f+(0). We set µ = 0.5, appropriate
c0, c1, c2, β
for a Regge-descending trajectory, leaving us 7 parameters. We then require the ﬁt to be anchored by the
dσ±
d(ν/m) , at √s = 4 GeV for
experimental values of σ¯pp and σpp (σπ−p and σπ+p), as well as their slopes,
nucleon scattering and √s = 2.6 GeV for pion scattering. This in turn imposes 4 conditions on the above
equations and we thus have three free parameters to ﬁt: c1, c2 and f+(0).

(cid:9)

P

– 10 –

3.7.1

¯pp and pp scattering

The raw experimental data for ¯pp and pp scattering that are shown in Figures 1 and 2 were taken from the
Particle Data Group[1]. Figure 1 shows the σ¯pp and σpp data for Ecms > 6 GeV, whereas Fig. 2 shows all
of the experimental ρ ¯pp and ρpp data for Ecms > 6 GeV. There are a total of 218 points in these 4 data sets.
We ﬁt these 4 data sets simultaneously using eq. (7), eq. (8) and eq. (9). Before we applied the Sieve, we
obtained χ2
min = 1185.6, whereas we expected 215. Clearly, either the model doesn’t work or there are a
substantial number of outliers giving very large ∆χ2
i contributions. The Sieve technique shows the latter to
be the case.

pp scattering using 3 diﬀerent choices of the cut-oﬀ, ∆χ2

We now study the eﬀectiveness and stability of the Sieve. Table 2 contains the ﬁtted results for ¯pp and
i max cut it tabulates:

i max = 4, 6 and 9. For each ∆χ2
the ﬁtted parameters from the χ2 ﬁt together with the errors found in the χ2 ﬁt,

•

•

•

•

•

the total χ2

min,

ν, the number of degrees of freedom (d.f.) after the data have been sifted by the indicated ∆χ2

i cut-oﬀ.

To get robust errors, the errors quoted in Table 2 for for each parameter should be multiplied by the common
factor rχ2 =1.05, from eq. (4), using the cut ∆ = 6.

We note that for ∆χ2

min changing from 1185.6 to 182.8. We ﬁnd χ2

i max = 6, the number of retained data points is 193, whereas we started with 218,
13%. We have rejected 25 outlier points (5 σpp, 5 σ¯pp, 15 ρpp and no ρ ¯pp points)
min/ν = 0.96, which when renormalized using eq. (5) for

∼

min/ν = 1.04, a very likely value with a probability[2] of 0.35.

giving a background of
with χ2
∆ = 6 becomes

χ2

R ×

Obviously, we have cleaned up the sample and have demonstrated that: (1) the goodness-of-ﬁt of the
i contributions from the outliers that we were able to
min/ν, severely distort the parameters found in a
0, followed

model is excellent, and (2) we had very large ∆χ2
Sieve out. These outliers, in addition to giving a huge χ2
conventional χ2 minimization, whereas they were easily handled by a robust ﬁt which minimized Λ2
by a χ2 ﬁt to the sifted data.

∆χ2

Inspection of Table 2 shows that the parameter values c1, c2 and f+(0) eﬀectively do not depend on
i max, our cut-oﬀ choice, having only very small changes compared to the predicted parameter errors.
A further indication of the stability of the Sieve is illustrated in Table 3. As a function of √s, we have

tabulated:

the predicted total cross sections and ρ-values for ¯pp and pp

the errors in their predictions generated by the errors in the ﬁt parameters c1, c2 and f+(0),

for two diﬀerent cut-oﬀ values, ∆χ2
values of ∆χ2
used with four diﬀerent types of real-world experimental data.

i max = 4 and 6. The predicted cross sections and ρ-values for the two
i max are virtually indistinguishable, giving us strong conﬁdence in the Sieve technique when

The results of applying the Sieve algorithm to the 4 data sets, along with the ﬁtted curves, are graphically
shown in Fig. 10 for σ¯pp and σpp and in Fig. 11 for ρ ¯pp and ρpp. The total number of data points shown in
Fig. 10 and in Fig. 11 is 193, whereas we started with 218 points. The ﬁts shown are in excellent agreement
with the 193 data points.

As a ﬁnal test, we tried ﬁtting another model which had its cross section energy dependence asymptotically
rising as ln s. This is the equivalent of setting the parameter c2 = 0, leaving us two free parameters to ﬁt, c1
min = 182.8 for the ln2 s model we now obtained
and f+(0). Using the same sifted data set which had given χ2
χ2
min = 1185.6 for only one more degree of freedom, clearly indicating that the ln s model was a very bad ﬁt
and could be excluded, whereas the ln2 s model gave a very good ﬁt to the same data subset.

3.7.2 π−p and π+p scattering

The raw experimental data for π−p and π+p scattering shown in Figures 3 and 4 were taken from the Particle
Data Group[1]. For Ecms > 6 GeV, Figure 3 shows the σπ−p and σπ+p data and Fig. 4 shows the ρπ−p and
ρπ+p data. There are a total of 155 points in these 4 data sets. Before we applied the Sieve algorithm, we
obtained χ2 = 527.8, whereas we expected 152, leading us to conclude that either the model doesn’t work

– 11 –

•

•

•

•

or there are a substantial number of outliers giving very large ∆χ2
technique shows the latter to be the case.

i contributions. Once again, the Sieve

Table 4 contains the ﬁtted results for π−p and π+p scattering using 3 diﬀerent choices of the cut-oﬀ,
i max = 4, 6 and 9. For each ∆χ2

i max it tabulates:

∆χ2

the ﬁtted parameters from the χ2 ﬁt together with the errors found in the χ2 ﬁt,

the total χ2

min,

ν, the number of degrees of freedom (d.f.) after the data have been sifted by the indicated ∆χ2
cut-oﬀ.

i max

To get robust errors, the errors quoted in Table 4 for ∆χ2
multiplied by the common factor rχ2 =1.05 of eq. (4) for the cut ∆ = 6.

i (xi; α)max = 6 for each parameter should be

For ∆χ2

∼

19%. We have rejected 25 outlier points (2 σπ+p, 19 σπ−p, 4 ρπ+p and no ρπ−p points) with χ2

i max = 6, the number of retained data points is 130, whereas we started with 155, a background
of
min
changing from 527.8 to 148.1. We ﬁnd χ2
min/ν = 1.166, which when renormalized using eq. (5) for ∆ = 6
min/ν = 1.26, corresponding to a probability of 0.03, which is acceptable being about a 2σ
becomes
R ×
eﬀect. Again, we have cleaned up the sample and have demonstrated that: (1) the model works, and (2) we
had large ∆χ2

i contributions from the outliers that we were able to Sieve out.

χ2

Inspection of Table 4 shows that the parameter values eﬀectively do not depend on our choice of cut-oﬀ,
i max, not changing signiﬁcantly compared to the predicted parameter errors. Another and perhaps better

∆χ2
indication of the stability of the Sieve is illustrated in Table 5. Tabulated as a function of √s are:

the predicted total cross sections and ρ-values for π−p and π+p

•

the errors in their predictions generated by the errors in the ﬁt parameters c1, c2 and f+(0)
i max = 4 and ∆χ2

for two diﬀerent values of the cut-oﬀ, ∆χ2
values for the two values of ∆χ2
the Sieve technique when used with these four diﬀerent examples of real-world experimental data.

i max = 6. The predicted cross sections and ρ
i max are essentially indistinguishable, again generating strong conﬁdence in

The results of applying the Sieve algorithm to the 4 data sets, along with the ﬁtted curves, are graphically
shown in Fig. 12 for σπ−p and σπ+p and in Fig. 13 for ρπ−p and ρπ+p. The ﬁts shown are in reasonable
agreement with the 155 data points retained by the Sieve.

Again, when we attempted to ﬁt the sifted data set of 130 points with a ln s ﬁt, we found χ2

min = 942.5,
45. Thus, again a ln2 s ﬁts well and a ln s ﬁt

with ν = 128, giving χ2/ν = 7.35, with a probability of << 10−
is ruled out for the πp system.

4 Comments and conclusions

40%,
We have shown that the Sieve algorithm works well in the case of backgrounds in the range of 0 to
i.e., for extensive computer data that were generated about a straight line, as well as about a constant,
and for a single event with a 20% outlier contamination as well as a 13%“inlier” contamination, that was
generated about a parabola. It also works well for the
13 to 19% contamination for the eight real-world
data sets taken from the Particle Data Group[1]. However, the Sieve algorithm is clearly inapplicable in the
situation where the outliers (noise) swamps the signal. In that case, nothing can be done.

∼

∼

≡

N
i=1 ln

0(α; x)

There are many possible choices for distributions resulting in robust ﬁts. Our particular choice of minimiz-
1 + 0.179∆χ2
ing the Lorentzian squared, Λ2
, in order to extract the parameters
needed to apply our Sieve technique seems to be a sensible one for both artiﬁcial computer-
α1, . . . , αM }
{
generated noisy distributions, as well as for real-world experimental data. This statement should not be
interpreted as meaning that real-world data is truly well-approximated as a Lorentz distribution, but rather,
as demonstrating that using the Lorentz distribution to get rid of outliers without sensibly aﬀecting the ﬁt pa-
rameters works well in the real world. Next, the choice of ﬁltering out all points with ∆χ2
i max—where
∆χ2
i max is as large as possible—is optimal in both minimizing the loss of good data and maximizing the loss
of outliers, resulting in a renormalized
1 for both the computer-generated and the real-world
∼
sample, as well as minimizing the distribution widths, and thus, the errors assigned to the parameters.

i > ∆χ2

i (xi; α)

min/ν

R ×

χ2

P

(cid:9)

(cid:8)

In detail, the utilization of the “Sieved” sample with ∆χ2

i < ∆χ2

i max allows one to

– 12 –

•

•

•

•

use the unbiased parameter values found in a χ2 ﬁt to the truncated sample for the cut ∆χ2
even in the presence of considerable background.

i (xi; α)max,

min/ν, i.e.,

ﬁnd the renormalized χ2
a function of ∆ = ∆χ2

χ2
R
i (xi; α)max and plotted in Figure 9.
min/ν to estimate the goodness-of-ﬁt of the model employing the standard χ2
use the renormalized χ2
probability distribution function. We thus estimate the probability that the data set ﬁts the model,
allowing one to decide whether to accept or reject the model.

is the inverse of the factor given in eq. (5) as

min/ν, where

R ×

make a robust evaluation of the parameter errors and their correlations, by multiplying the standard
covariance matrix C found in the χ2 ﬁt by the appropriate value of (rχ2 )2 for the cut ∆χ2
i max. The
value of rχ2 is given by eq. (4) and shown in Figure 9 as a function of the cut ∆χ2
i max, where it is
called ∆. It ranges from 1 for very large ∆ to
1.14 for ∆ = 2 in eq. (4). However, this is not the
complete story. The parameter error is σ = rχ2
Σ and we must also take into account the increase in
Σ due to the cut ∆, which causes the loss of signal points. As shown in Table 1 and discussed in detail
in Section 3.4, the true loss of accuracy at ∆ = 2—relative to an unsifted sample of signal data—is the
factor

1.25. Thus, the algorithm starts failing rapidly for cuts ∆ smaller than 2.

∼
×

∼

In conclusion, the “ Sieve” algorithm gains its strength from the combination of making ﬁrst a Λ2
0 ﬁt to
get rid of the outliers and then a χ2 ﬁt to the sifted data set. By varying the ∆χ2
i (xi; α)max to suit the
data set needs, we easily adapt to the diﬀerent contaminations of outliers that can be present in real-world
experimental data samples.

Not only do we now have a robust goodness-of-ﬁt estimate, but we also have also a robust estimate of the
parameters and, equally important, a robust estimate of their errors and correlations. The phenomenologist
can now eliminate the use of possible personal bias and guesswork in “cleaning up” a large data set.

5 Acknowledgements

I would like to thank Professor Steven Block of Stanford University for valuable criticism and contributions
to this manuscript and Professor Louis Lyons of Oxford University for many valuable discussions. Further,
I would like to acknowledge the hospitality of the Aspen Center for Physics.

A Robust Estimation

The terminology, “robust” statistical estimators[6], was ﬁrst introduced to deal with small numbers of data
points which have a large departure from the model predictions, i.e., outlier points. We will discuss one
possible technique for handling such points when we introduce the Lorentz probability density function in
Section A.4.

A.1 Maximum Likelihood Estimates

Let Pi be the probability density of the ith individual measurement, i = 1, . . . , N , in the interval ∆y. Then
the probability of the total data set is

N

=

P

Pi∆y.

i=1
Y
Let us deﬁne the quantity

∆χ2

i (xi; α)

yi −

y(xi; α)
σi

2

,

(cid:19)

≡

(cid:18)

(10)

(11)

where yi is the measured value at xi, y(xi; α) is the expected (theoretical) value from the model under
consideration, and σi is the experimental error of the ith measurement. The M model parameters αk are
.
given by the M -dimensional vector α =

α1, . . . , αM }

{

– 13 –

P
α =
{

is identiﬁed as the likelihood function, which we shall maximize as a function of the parameters
α1, . . . , αM }
likelihood function

For the special case where the errors are normally distributed (Gaussian distribution), we have the

given as

.

P

=

P

N

i=1 (
Y

exp

1
2

yi −

y(xi; α)
σi

"−

(cid:18)

2

#

(cid:19)

∆y
√2πσi )

=

N

exp

1
2

∆χ2
i

∆y
√2πσi (cid:27)

,

(cid:21)

−

(cid:20)

i=1 (cid:26)
Y

in eq. (12) is the same as minimizing the negative logarithm of

Since N , ∆y and σi are constants, after using eq. (11), this is equivalent to minimizing the quantity

Maximizing the likelihood function
namely,

P

N

1
2

yi −

y(xi; α)
σi

i=1
X

(cid:18)

2

−

(cid:19)

N ln

∆y
√2πσi

.

1
2

N

i=1
X

∆χ2

i (xi; α).

We now deﬁne χ2(α; x) as

χ2(α; x) =

∆χ2

i (xi; α),

N

i=1
X

where x

x1, . . . , xi, . . . , xN }

.

≡ {

Hence, the χ2 minimization problem, appropriate to the Gaussian distribution, reduces to

minimize over α,

χ2(α; x) =

∆χ2

i (xi; α)

N

i=1
X

for the set of N experimental points at xi having the value yi and error σi.

A.2 Gaussian Distribution

To minimize χ2, we must solve the (in general, non-linear) set of M equations

N

i=1
X

1
σi (cid:18)

yi −

y(xi; α)
σi

∂y(xi; . . . αj . . .)
∂αj

(cid:19) (cid:18)

(cid:19)

= 0,

j = 1, . . . , M.

(17)

The Gaussian distribution allows a χ2 minimization routine to return several exceedingly useful statistical
quantities. Firstly, it returns the best-ﬁt parameter space αmin. Secondly, the value of χ2
min, when compared
to the number of degrees of freedom ( d.f.
M , the number of data points minus the number
of ﬁtted parameters) allows one to make standard estimates of the goodness of the ﬁt of the data set to
the model used, using the χ2 probability distribution function, given in standard texts[7], for ν degrees of
freedom. Further, C−

M matrix of the partial derivatives at the minimum, given by

1, the M

ν = N

−

≡

×

1
2

∂2χ2
∂αj∂αk (cid:19)α=αmin

,

(cid:18)

1

C−

jk =

(cid:2)

(cid:3)

allows us to compute the standard covariance matrix C for the individual parameters αi, as well as the
correlations between αj and αk[7]. Thus, when the errors are distributed normally, the χ2 technique not
only gives us the desired parameters αmin, but also furnishes us with statistically meaningful error estimates
of the ﬁtted parameters, along with goodness-of-ﬁt information for the data to the chosen model—very
valuable quantities for any model under consideration.

– 14 –

(12)

,

P

(13)

(14)

(15)

(16)

(18)

A.3 Robust Distributions
We can generalize the maximum likelihood function of eq. (12), which is a function of the variable yi
as

y(xi;α)
σi

,

−

is the negative logarithm of the probability density. Thus, we now have

=

P

N

i=1 (cid:26)
Y

exp

β

−

(cid:20)

(cid:18)

yi −

y(xi; α)
σi

∆y

,

(cid:19)(cid:21)

(cid:27)

where the function β
to minimize the generalization of eq. (14), i.e.,
(cid:16)

(cid:17)

−

yi

y(xi;α)
σi

minimize over α,

i=1
X
for the N -dimensional vector x.

N

yi −

y(xi; α)
σi

,

(cid:19)

β

(cid:18)

This yields the more general set of M equations

N

i=1
X

1
σi

w

yi −

y(xi; α)
σi

(cid:18)

(cid:19) (cid:18)

∂y(xi; . . . αj . . .)
∂αj

(cid:19)

where the weight function w(z) in eq. (21) is given by

= 0,

j = 1, . . . , M,

w(z)

dβ(z)
dz

,

≡

z

≡

yi −

y(xi; α)
σi

=

∆χ2

i (xi; α).

q

Comparison of eq. (21) with the Gaussian equivalent of eq. (17) shows that

β(z) =

z2, w(z) = z

(for a Gaussian distribution).

1
2

∆χ2

We note that for a Gaussian distribution, the weight function w(z) for each experimental point i is pro-
portional to
i , the normalized departure of the point from the theoretical value. Thus, the more the
departure from the theoretical value, the more the point is weighted in minimizing χ2. This gives outliers
(points with large departures from their theoretical values) unduly large weight in computing the best vector
α, easily skewing the answer due to the inclusion of these outliers.

p

Consider the normalized Lorentz probability density distribution (also known as the Cauchy distribution or
the Breit-Wigner line width distribution), given by

A.4 Lorentz Distribution

P (z) =

√γ
π

1
1 + γz2 ,

where γ is a constant whose signiﬁcance will be discussed later. Using eq. (11) and eq. (22), we rewrite
eq. (24) in terms of the measurement errors σi and the experimental measurements yi at xi as

It has long tails and therefore often oﬀers a more realistic description of the world than does the Gaussian
distribution. Taking the negative logarithm of eq. (25) and using it in eq. (20), we see that

yi −

y(xi; α)
σi

P

(cid:18)

(cid:19)

=

=

1

√γ
π

1 + γ

yi

y(xi;α)
σi

−

2

√γ
π

(cid:16)

1
1 + γ∆χ2
i (xi; α)

(cid:17)
.

β(z) = ln

1 + γz2

= ln

1 + γ∆χ2

i (xi; α)

and

w(z) =

(cid:0)

z
1 + γz2 =

∆χ2
i (xi; α)
(cid:1)
(cid:8)
1 + γ∆χ2

i (xi; α)

.

p

(cid:9)

– 15 –

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

In analogy to χ2 minimization, we must now minimize Λ2(α; x), the Lorentzian squared, with respect to

the parameters α, for a given set of experimental points x, i.e.,

(27)

(28)

minimize over α,

Λ2(α; x)

ln

1 + γ∆χ2

i (xi; α)

,

N

≡

i=1
X

(cid:8)

(cid:9)

(cid:9)

for the set of N experimental points at xi having the value yi and error σi.

We have made extensive computer simulations using Gaussianly generated data which showed that the
choice γ = 0.178 minimized the rms (root mean square) parameter widths found in Λ2 minimization. Further,
it gave rms widths that were almost as narrow as those found in χ2 minimization on the same data. We will
adopt this value of γ, since it eﬀectively minimizes the width for the Λ2 routine, which we now call Λ2
0(α; x).
Thus we select for our robust algorithm,

minimize over α,

ln

1 + 0.179∆χ2

i (xi; α)

.

N

Λ2

0(α; x)

≡

i=1
X

(cid:8)

An important property of Λ2
ﬁt, i.e. χ2
min found using a standard χ2 minimization on the same data.
same as the χ2
P

0(α; x) is that it numerically gives the same total χ2

i (xi; α), where the ∆χ2

i (xi; α) come from the minimization of Λ2

N
i=1 ∆χ2

0 =

0min as that found in a χ2
0 in eq. (28), is the

∆χ2

i (just like the Gaussian distribution does), whereas for large

We note from eq. (26) that the weight function for a point i for small
∆χ2
∆χ2

i increases proportional to
i . Thus,
large outliers have much less weight than points close to the model curve, which gives Λ2 minimization its
p
robust features. Hence, outliers have little inﬂuence on the choice of the parameters αmin resulting from the
minimization of Λ2

0, a major consideration for a robust minimization method.

i , it decreases as 1/
p

Unlike the minimization of χ2, the minimization of Λ2

0, while yielding the desired robust estimate of
αmin, gives neither parameter error information on αmin nor a conventional goodness-of-ﬁt. These are major
failings, since one has no objective grounds for accepting or rejecting the model. We will rectify these
shortcomings in the main section of the text, Section 2, where we describe the adaptive “Sieve” algorithm.
Extensive computer studies, summarized in Section 3.4, demonstrate that use of this algorithm enables one
to make a robust error estimate of αmin, as well as a robust estimate of the goodness-of-ﬁt of the data to the
model.

∆χ2

p

p

References

[1] Particle Data Group, H. Hagiwara et al., Phys. Rev. D 66, 010001 (2002).

[2] The χ2 probability density distribution has ν, the number of degrees of freedom, as its mean value and
has a variance equal to 2ν. To have an intuitive feeling for the goodness-of-ﬁt, i.e., the probability that
χ2 > χ2
min, we note that for the large number of degrees of freedom ν that we are considering in this
note, the probability density distribution for χ2 is well approximated by a Gaussian, with a mean at ν
and a width of √2ν, where 0 < χ2 <
is truncated here to 0, since
∞
by deﬁnition χ2
min/ν = 1, which
corresponds to a goodness-of-ﬁt probability of 0.5. The chance of having small χ2
0, corresponding to
a goodness-of-ﬁt probability
1, is exceedingly small. In our computer-generated example of a straight
∼
line ﬁt with ν = 103, the ﬁt ﬁrst can be considered to become poor—say by three standard deviations—
when χ2
min/ν > 1.41. We found a renormalized χ2
min > 146, yielding χ2
min/ν = 1.01, indicating a very
good ﬁt.

0). In this approximation, we have the most probable situation if χ2

(n.b., the usual lower limit of

min ∼

−∞

≥

[3] In this context, a random distribution means a uniform distribution between a and b, generated by a
random number generator that has a ﬂat output between 0 and 1. A normally distributed (Gaussian)
distribution means using a Gaussian random number generator that has as its output random numbers

yi distributed normally about ¯y, with a probability density
the error (standard deviation) of the point yi.

1
√2π

exp

1
2

−

2

¯y

yi

−
σi

(cid:16)

(cid:17)

, where σi represents

– 16 –

x

−

exp(

x2/2) for

distribution with unit variance about the value y = 0. If we were to deﬁne ∆χ2
with ∆ being the cut ∆χ2
√∆

[4] The fact that rχ2 is greater than 1 is counter-intuitive. Consider the case of generating a Gaussian
0)2 = y2
i ,
i max, then the truncated diﬀerential probability distribution would be P (x) =
1
+√∆, whose rms value clearly is less than 1—after all, this distribution
√2π
is truncated compared to its parent Gaussian distribution. However, that is not what we are doing. What
we do is to ﬁrst make a robust ﬁt to each untruncated event that was Gaussianly generated with unit
variance about the mean value zero. For every event we then ﬁnd the value y0, its best ﬁt parameter,
which, although close to zero with a mean of zero, is non-zero. In order to obtain the truncated event
y0)2. It is the jitter in y0 about zero
whose width we sample with the next χ2 ﬁt, we use ∆χ2
that is responsible for the rms width becoming greater than 1. This result is true even if the ﬁrst ﬁt to
the untruncated data were a χ2 ﬁt.

(yi −

(yi −

i ≡

i ≡

≤

−

≤

[5] In deriving these equations, we have employed real analytic amplitudes derived using unitarity, analyt-

icity, crossing symmetry, Regge theory and the Froissart bound.

[6] Attributed in “Numerical Recipes”[7] to G. E. P. Box in 1953. A very simple example of a robust
estimator is to use the median of a discrete distribution rather than the mean to characterize a typical
characteristic of the distribution. For example, the “average price” of a home in a luxury resort area,
which had a few twenty-ﬁve million dollar homes—a few outliers at very large values of the distribution—
could be seriously distorted and essentially meaningless, whereas the median would scarcely be aﬀected.

[7] “Numerical Recipes, The Art of Scientiﬁc Computing”, W. H. Press, B. P. Flannery, S. A. Teukolsky and
W. T. Vettering, Cambridge University Press, p. 289-293 (1986). There is also an excellent discussion
of modeling of data, including a section on conﬁdence limits by Monte Carlo simulation, in Chapter 14.

– 17 –

∆χ2

i max = 9 ∆χ2
1.034

i max = 6 ∆χ2
1.054

i max = 4 ∆χ2
1.098

i max = 2
1.162

rχ2,str. line

rχ2,constant

average

< χ2

min > /ν

str. line

constant

average

1

−

R
S.F.

σ/σ0

1.00

1.018

0.974

0.973

0.973

0.9733

0.9973

1.02

1.05

1.052

0.901

0.902

0.901

0.9013

0.9857

1.06

1.088

1.093

0.774

0.774

0.774

0.7737

0.9545

1.19

1.108

1.148

0.508

0.507

0.507

0.5074

0.8427

1.25

Table 1: Results for rχ2 = σ/Σ, the ratio of the rms width to Σ, the error for the χ2 ﬁt; < χ2
min > /ν, for both the
straight line case and the constant case; σ/σ0, the ratio of the rms width (error) of the parameter relative to what
the error would be if the sample were not truncated, i.e., the total loss of accuracy due to truncation, as functions
of the cut ∆χ2
min > /ν are graphically shown in Fig. 9. See Sections 3.2,
−1 are from eq. (5) and the survival
3.3 and 3.4 for details. The theoretical values for the renormalization factor
fractions S.F. are from eq. (6). See Section 3.4 for a discussion of the error-broadening factor σ/σ0.

i max. The average results for rχ2 and < χ2

R

Fitted

Parameters

4

∆χ2

i max

6

c1

c2

(mb)

(mb)

f (0) (mb GeV)

1.452

−
0.2828

±

±
0.065

−

0.066

0.0061

0.56

1.448

−
0.2825

±

±
0.020

−

0.066

0.0060

0.56

1.423

−
0.2801

±

±
0.065

−

0.065

0.0059

0.56

χ2

min

ν (d.f).

χ2

min/ν

R ×

±
142.8

182

1.014

±
182.8

190

1.067

9

±
217.9

195

1.143

Table 2: The ﬁtted results for a 3-parameter ﬁt to the total cross sections and ρ-values for pp and ¯pp scattering. The
renormalized χ2/νmin, taking into account the eﬀects of the ∆χ2

i max cut, is given in the row labeled

min/ν.

χ2

R ×

– 18 –

(GeV)

σ¯pp

σpp

ρpp

σ¯pp

σpp

ρpp

σ¯pp

σpp

ρ ¯pp

ρpp

∆χ2

i max = 4
ρ ¯pp

∆χ2

i max = 6
ρ ¯pp

√s

10

100

540

43.77

38.34

-0.0368

-0.1501

43.77

38.33

-0.0365

-0.1498

46.61

46.25

0.1083

0.1031

46.61

46.25

0.1082

0.1031

60.87

60.82

0.1368

0.1363

60.86

60.81

0.1367

0.1362

1800

75.30

75.29

0.1396

0.1395

75.28

75.27

0.1396

0.1395

14000

107.6

107.6

0.1318

0.1318

107.5

107.5

0.1318

0.1318

Predicted Error

.01

.08

.28

.50

1.0

.01

.08

.28

.50

1.0

.003

.004

.001

.001

.001

.001

.001

.001

.001

.001

Table 3: The predicted results for σ ¯pp, σpp, ρ ¯pp and ρpp, together with their errors, as a function of √s, the cms
energy in GeV, for ∆χ2
i max = 6 . The cross sections and their errors are in mb. The predicted
errors are those found from a standard χ2 analysis.

i max = 4 and ∆χ2

Fitted

Parameters

4

9

c1

c2

(mb)

(mb)

0.895

−
0.174

0.11

±
0.0083

0.921

−
0.177

0.11

±
0.0081

0.982

−
0.182

0.10

±
0.0075

f (0) (mb GeV)

0.34

0.34

0.34

∆χ2

i max

6

±
2.307

−

±
148.1

127

1.293

±
2.327

−

±
204.4

135

1.556

±
2.281

−

±
128.7

122

1.364

χ2

min

ν (d.f).

χ2

min/ν

R ×

Table 4: The ﬁtted results for a 3-parameter ﬁt to the total cross sections and ρ-values for π+p and π
The renormalized χ2/νmin, taking into account the eﬀects of the ∆χ2
i max cut, is given in the row labeled

−

p scattering.
χ2
min/ν.

R ×

(GeV)

σπ−p

σπ+p

ρπ+p

σπ−p

σπ+p

ρπ+p

σπ−p

σπ+p

ρπ−p

ρπ+p

∆χ2

i max = 4
ρπ−p

∆χ2

i max = 6
ρπ−p

Predicted Error

25.40

23.70

-0.1391

-0.2704

25.40

23.70

-0.1396

-0.2708

24.26

23.35

0.0392

-0.0248

24.27

23.36

0.0396

-0.0243

24.92

24.25

0.0827

0.0385

24.94

24.27

0.0833

0.0393

28.15

27.81

0.1309

0.1117

28.20

27.86

0.1318

0.1127

.01

.01

.02

.09

.01

.01

.02

.09

.009

.002

.002

.003

.010

.002

.002

.003

√s

6

15

23.5

62.5

Table 5: The predicted results for σπ−p, σπ+p, ρπ−p and ρπ+p, together with their errors, as a function of √s, the
cms energy in GeV, for ∆χ2
i max = 6 . The cross sections and their errors are in mb. The predicted
errors are those found from a standard χ2 analysis.

i max = 4 and ∆χ2

– 19 –

b
m
 
n
i
 
,

σ

100

95

90

85

80

75

70

65

60

55

50

45

40

35

0.2

0.1

0.0

-0.2

-0.3

-0.4

pp data
pbar p data

10

100

1000

Ecms, in GeV

Figure 1: The data points shown are all of the experimental data listed in the Particle Data Group[1] site for ¯pp and pp total
cross sections in the energy interval Ecms > 6 GeV. The circles are σ ¯pp and the squares are σpp.

ρ

-0.1

pp       data
pbar p data

10

100

1000

Ecms, in GeV

Figure 2: The data points shown are all of the experimental data listed in the Particle Data Group[1] site for ¯pp and pp
ρ-values (ratio of the real to the imaginary portion of the forward scattering amplitude) in the energy interval Ecms > 6 GeV.
The circles are ρ ¯pp and the squares are ρpp.

– 20 –

π+
π−

p data, PDG
p data, PDG

10
√s, in GeV

b
m
 
n
i
 
,

σ

34

32

30

28

26

24

22

20

0.2

0.1

0.0

-0.1

-0.2

-0.3

-0.4

-0.5

-0.6

-0.7

Figure 3: The data points shown are all of the experimental data listed in the Particle Data Group[1] site for π−p and π+p
total cross sections in the energy interval Ecms > 6 GeV. The circles are σπ−p and the squares are σπ+p.

ρ

ρπ+ 
ρπ− 

p  data

p  data

10

√s, in GeV

100

Figure 4: The data points shown are all of the experimental data listed in the Particle Data Group[1] site for π−p and π+p
ρ-values (ratio of the real to the imaginary portion of the forward scattering amplitude) in the energy interval Ecms > 6 GeV.
The circles are ρπ+p and the squares are ρπ−p.

– 21 –

2

4

6

8

10

i < 6

gaussian data, with ∆χ2
noise data, with ∆χ2
best fit, y = 0.998 - 2.014x
fit using χ2 routine on all data,
            including outliers

i < 6

y

y

15

10

5

0

-5

-10

-15

-20

-25

-30

15

10

5

0

-5

-10

-15

-20

-25

-30

0
a)

0
b)

gaussian data
noise data

x

x

– 22 –

2

4

6

8

10

a) The 100 circles are a computer-generated Gaussianly distributed data set about the straight line y = 1 − 2x.

Figure 5:
The 20 diamonds are randomly distributed noise data. See Section 3.1 for details.
b) The 100 data points shown are the result of screening all 120 data points for those points having ∆χ2
i < 6. There were no
noise points (diamonds) retained in the Sieve and the 100 circles are the Gaussian data retained in the Sieve. The best ﬁt curve
to all points with ∆χ2
min/ν = 0.91,
min/ν = 1.01 compared to the expected < χ2 > /ν = 1.0 ± 0.14. The dashed-dot curve is
yielding a renormalized value R × χ2
a χ2 ﬁt to the totality of data—100 signal plus 20 noise points—which has χ2

i < 6, y = a + bx, is the solid curve, where a = 0.998 ± 0.12, b = −2.014 ± 0.020, and χ2

min/ν = 4.8.

2

4

6

8

10

i < 4
i < 4

gaussian data, with ∆χ2
noise data,       with ∆χ2
best fit, y = 9.98
fit using χ2 routine on all data,
            including outliers

y

y

26

24

22

20

18

16

14

12

10

8

6

4

2

26

24

22

20

18

16

14

12

10

8

6

4

2

0
a)

0

b)

gaussian data
noise data

x

x

– 23 –

2

4

6

8

10

a) The 100 circles are a computer-generated Gaussianly distributed data set about the constant y = 10. The 40

Figure 6:
diamonds are randomly distributed noise data. See Section 3.3 for details.
b) The 98 data points shown are the result of screening all 140 data points for those points having ∆χ2
i < 4. There were no
noise points (diamonds) retained in the Sieve and the 98 circles are the Gaussian data retained in the Sieve. The best ﬁt curve
to all points with ∆χ2
min/ν = 0.84, yielding a renormalized value
min/ν = 1.09 compared to the expected < χ2 > /ν = 1.0 ± 0.14. The dashed-dot curve is a χ2 ﬁt to the totality of
R × χ2
data—100 signal plus 40 noise points—which has χ2

i < 4, y = c, is the solid curve, where c = 9.98 ± 0.074, and χ2

min/ν = 4.39.

2

4

6

8

10

i < 9
i < 9

gaussian data, with ∆χ2
noise data,       with ∆χ2
best fit, y = 10.05
fit using χ2 routine on all data,
            including outliers

y

y

24

22

20

18

16

14

12

10

8

6

4

2

0

24

22

20

18

16

14

12

10

8

6

4

2

0

0
a)

0

b)

gaussian data
noise data

x

x

– 24 –

2

4

6

8

10

a) The 100 circles are a computer-generated Gaussianly distributed data set about the constant y = 10. The 40

Figure 7:
diamonds are randomly distributed noise data. See Section 3.3 for details.
b) The 99 data points shown are the result of screening all 140 data points for those points having ∆χ2
i < 9. There were no
noise points (diamonds) retained in the Sieve and the 98 circles are the Gaussian data retained in the Sieve. The best ﬁt curve
to all points with ∆χ2
min/ν = 1.08, yielding a renormalized
min/ν = 1.11 compared to the expected < χ2 > /ν = 1.0 ± 0.14. The dashed-dot curve is a χ2 ﬁt to the totality of
value R × χ2
data—100 signal plus 40 noise points—which has χ2

i < 9, y = c, is the solid curve, where c = 10.05 ± 0.074, and χ2

min/ν = 8.10.

0

2

4

6

8

10

12

best fit, y = 1.23 + 2.04x + 0.488x2

fit using χ2 routine on all data,
            including outliers
          
noise data, with       ∆χ2
gaussian data, with ∆χ2

i <  6
i <  6

gaussian data
noise data

120

100

80

60

20

0

-20

120

100

80

60

20

0

-20

y

40

-40

-2

a)

y

40

-40

-2

b)

x

x

– 25 –

0

2

4

6

8

10

12

Figure 8: a) The 100 circles are a computer-generated Gaussianly distributed data set about the parabola y = 1 + 2x + 0.5x2.
The 35 diamonds are randomly distributed noise data around the parabola y = 12 + 2x + 0.2x2. See Section 3.6 for details.
b) The 113 data points shown are the result of screening all of the data for those points having ∆χ2
i < 6. The diamonds are
the 13 noise points retained in the Sieve and the 100 circles are the Gaussian data retained in the Sieve. The best ﬁt curve to
i < 6, y = 1.23 + 2.04x + 0.48x2, is the solid curve. The dashed curve is a χ2 ﬁt to the totality of data in
all points with ∆χ2
Fig. 8, consisting of signal plus noise.

1
-

R

2
χ
r

1.00

0.95

0.90

0.85

0.80

0.75

0.70

0.65

0.60

0.55

0.50

1.16

1.15

1.14

1.13

1.12

1.11

1.10

1.09

1.08

1.07

1.06

1.05

1.04

1.03

1.02

1.01

2

a)

3

4

5

6

7

8

9

10

∆χ2

i cut

3

4

5

6

7

8

9

10

2

b)

∆χ2

i cut

Figure 9: a) A plot of eq. (5): R−1, the reciprocal of the factor that multiplies χ2
set vs. ∆χ2
the χ2 ﬁt to the sifted data set vs. ∆χ2
cut is called ∆.

min/ν found in the χ2 ﬁt to the sifted data
b) A plot of eq. (4): rχ2 , the factor whose square multiplies the covariant matrix found in
i , the χ2 cut. See Sections 3.2, 3.3 and 3.4 for details. In eq. (4) and eq. (5), the ∆χ2

i cut, the ∆χ2

i max cut.

i

– 26 –

10

100

1000

Ecms, in GeV

Figure 10: The data points shown are the result of screening all of the points of Fig. 1 for those cross section points with
∆χ2
i < 6. The circles are σ ¯pp and the squares are σpp. The solid line is the theoretical ﬁt to σ ¯pp and the dashed line is the
theoretical ﬁt to σpp.

b
m
 
n
i
 
,

σ

100

95

90

85

80

75

70

65

60

55

50

45

40

35

ρ

-0.1

0.2

0.1

0.0

-0.2

-0.3

-0.4

pp data,        ∆χ2
pbar p data,  ∆χ2
pp           theory
pbarp      theory

i < 6
i < 6

i <  6
i <  6

pp       data, ∆χ2
pbar p data, ∆χ2
pp            theory
pbar p      theory

– 27 –

10

100

1000

Ecms, in GeV

Figure 11: The data points shown are the result of screening all of the points in Fig. 2 for those ρ-value points with ∆χ2
i < 6.
The circles are ρ ¯pp and the squares are ρpp. The solid line is the theoretical ﬁt to ρ ¯pp and the dashed line is the theoretical ﬁt
to ρpp.

π+p data,  ∆χ2
π-p data,  ∆χ2

i < 6
i < 6
π+p theory
             π-p theory

b
m
 
n
i
 
,

σ

34

32

30

28

26

24

22

20

ρ

0.2

0.1

0.0

-0.1

-0.2

-0.3

-0.4

-0.5

-0.6

-0.7

10

√s, in GeV

100

Figure 12: The data points shown are the result of screening all of the points of Fig. 3 for those cross section points with
∆χ2
i < 6. The circles are σπ−p and the squares are σπ+p. The solid line is the theoretical ﬁt to σπ− p and the dashed line is
the theoretical ﬁt to σπ+p.

i <  6

i <  6

p  data, ∆χ2
p  data, ∆χ2

ρπ+ 
ρπ− 
π- p theory
π+p theory

10

√s, in GeV

100

Figure 13: The data points shown are the result of screening all of the points in Fig. 4 for those ρ-value points with ∆χ2
i < 6.
The circles are ρπ− p and the squares are ρπ+p. The solid line is the theoretical ﬁt to ρ ¯pp and the dashed line is the theoretical
ﬁt to ρpp.

– 28 –


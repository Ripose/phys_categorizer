Application of Conditioning to the Gaussian-with-Boundary

Problem in the Uniﬁed Approach to Conﬁdence Intervals

Department of Physics and Astronomy, University of California, Los Angeles, CA 90095

Robert D. Cousins†

(January 14, 2000)

Abstract

Roe and Woodroofe (RW) have suggested that certain conditional probabil-
ities be incorporated into the “uniﬁed approach” for constructing conﬁdence
intervals, previously described by Feldman and Cousins (FC). RW illustrated
this conditioning technique using one of the two prototype problems in the
FC paper, that of Poisson processes with background. The main eﬀect was
on the upper curve in the conﬁdence belt. In this paper, we attempt to apply
this style of conditioning to the other prototype problem, that of Gaussian
errors with a bounded physical region. We ﬁnd that the lower curve on the
conﬁdence belt is also moved signiﬁcantly, in an undesirable manner.

PACS numbers: 06.20.Dk, 14.60.Pq

0
0
0
2

 

n
a
J
 

5
1

 
 
]
n
a
-
a
t
a
d

.
s
c
i
s
y
h
p
[
 
 

1
v
1
3
0
1
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Typeset using REVTEX

1

I. INTRODUCTION

Roe and Woodroofe [1] have made an interesting suggestion for modifying the “uni-
ﬁed approach” to classical conﬁdence intervals which Feldman and I advocated in Ref. [2].
They invoke the use of “conditioning”, namely replacing frequentist coverage probabilities
with conditional probabilities, still calculated in a frequentist manner, but conditioned on
knowledge gained from the result of the particular experiment at hand.

Roe and Woodroofe (RW) illustrate their suggestion using one of the two prototype prob-
lems, that of Poisson processes with background. Suppose, for example that an experiment
observes 3 events (signal plus background). Then the experimenters know that, in that
particular experiment, there were 3 or fewer background events. RW therefore calculate the
frequentist coverage using an ensemble of experiments with 3 or fewer background events,
rather than the larger unrestricted ensemble which we used. Thus, the RW ensemble changes
from experiment to experiment. Conditioning on an equality has a long history in classical
statistics. (Ref. [1] contains key references.) However, conditioning on an inequality, as RW
do when the number of events is greater than zero, is perhaps less well founded, and it is
interesting to explore the consequences.

In this paper, we attempt to apply RW-like conditioning to the other prototype problem,
that of Gaussian errors with a bounded physical region. The result is similar to the Poisson
problem analyzed by RW, but diﬃculties which were apparently masked by the discrete
nature of the Poisson problem now arise. In particular, the lower endpoints of conﬁdence
intervals are moved signiﬁcantly in an undesirable direction.

II. THE UNIFIED APPROACH TO THE GAUSSIAN-WITH-BOUNDARY

PROBLEM

As in Ref. [2], we consider an observable x which is the measured value of parameter µ
in an experiment with a Gaussian resolution function with known ﬁxed rms deviation σ, set
here to unity. I.e.,

P (x|µ) =

1
√2π

exp(−(x − µ)2/2).

(2.1)

We consider the interesting case where only non-negative values for µ are physically allowed
(for example, if µ is a mass).

The conﬁdence-belt construction in Ref. [2] proceeded as follows. For a particular x,
we let µbest be the physically allowed value of µ for which P (x|µ) is maximum. Then
µbest = max(0, x), and

P (x|µbest) =( 1/√2π,

x ≥ 0
exp(−x2/2)/√2π, x < 0.

We then compute the likelihood ratio R,

R(x) =

P (x|µ)
P (x|µbest)

=( exp(−(x − µ)2/2), x ≥ 0

exp(xµ − µ2/2),

x < 0.

2

(2.2)

(2.3)

During our Neyman construction of conﬁdence intervals, R determines the order in which
values of x are added to the acceptance region at a particular value of µ. In practice, this
means that for a given value of µ, one ﬁnds the interval [x1, x2] such that R(x1) = R(x2)
and

Z x2

x1

P (x|µ)dx = α,

(2.4)

where α is the conﬁdence level (C.L.). We solve for x1 and x2 numerically to the desired
precision, for each µ in a ﬁne grid. With the acceptance regions all constructed, we then
read oﬀ the conﬁdence intervals [µ1, µ2] as in Ref. [2].

III. INVOKING CONDITIONING IN THE GAUSSIAN-WITH-BOUNDARY

PROBLEM

In order to formulate the conditioning, we ﬁnd it helpful to think of the measured value
x as being the sum of two parts, the true mean µt and the random “noise” which we call ε:

x = µt + ε.

(3.1)

We are considering the case where it is known on physical grounds that µt ≥ 0. Thus, if an
experimenter obtains the value x0 in an particular experiment, then he or she knows that,
in that particular experiment,

ε ≤ x0.

(3.2)

qx0

exp(−(x − µ)2/2)/(erf(x0/√2) + 1), x ≤ µ + x0

For example, if the experimenter measures µ and obtains x0 = −2, then the experimenter
knows that ε ≤ −2 in that particular experiment. This information is analogous to the
information in the Poisson problem above in which one knows that in the particular experi-
ment, the number of background events is 3 or fewer. We thus use it the manner analogous
to that of RW: our particular experimenter will consider the ensemble of experiments with
ε ≤ x0 when constructing the conﬁdence belt relevant to his or her experiment.
We let P (x|µ, ε ≤ x0) be the (normalized) conditional probability for obtaining x, given
that ε ≤ x0. In notation similar to that of RW, this can be denoted as qx0
µ (x):
µ (x) ≡ P (x|µ, ε ≤ x0) =( 2√2π
Given x0, at each x we ﬁnd µbest, that value of µ which maximizes P (x|µ, ε ≤ x0):
µbest =
√2π(erf(x0/√2) + 1) ×

x0 ≥ 0 and x ≥ 0
x,
x − x0, x0 < 0 and x ≥ x0
otherwise
0,

x0 ≥ 0 and x ≥ 0
0)/2, x0 < 0 and x ≥ x0

1,
exp(−x2
exp(−x2)/2, otherwise

In the notation of Ref. [1], P (x|µbest, ε ≤ x0) is then

max

µ′

qx0
µ′ (x) =

x > µ + x0.

(3.3)

0,

2

(3.4)

(3.5)

3

eRx0(µ, x) =

exp(−(x − µ)2/2),
exp((−(x − µ)2 + x2
exp(xµ − µ2/2),

x0 ≥ 0 and x ≥ 0
0)/2), x0 < 0 and x ≥ x0
otherwise

Figures 1 through 3 show graphs of qx0
µ, for each of three values of x0.

µ (x), maxµ′ qx0

µ′ (x), and eRx0(µ, x), for three values of

(3.6)

(3.7)

(3.8)

(3.9)

Then the ratio R of Eqn. 2.3 is replaced by

eRx0(µ, x) =

qx0
µ (x)
maxµ′ qx0

µ′ (x)

,

which vanishes if x > µ + x0, and otherwise is given by

We let ecx0(µ) be the value of c for which
Zx:eRx0 (µ,x)<c
eRx0(µ, x0) ≥ ecx0(µ).

qx0
µ (x)dx = α.

The modiﬁed conﬁdence interval consists of those µ for which

Note that this entire construction depends on the value of x0 obtained by the particular
experiment. An experiment obtaining a diﬀerent value of x0 will have a diﬀerent function
in Eqn. 3.3, and hence a diﬀerent conﬁdence belt construction. Figure 4 shows examples of
such constructions for six values of x0. The vertical axis gives the endpoints of the conﬁ-
dence intervals. Each diﬀerent conﬁdence belt construction is used only for an experiment
obtaining the value x0 which was used to construct the belt. The interval [µ1, µ2] at x = x0
is read oﬀ for that experiment; the rest of that plot is not used.

Finally, we can form the graph shown in Fig. 5 by taking the modiﬁed conﬁdence interval
for each x0, and plotting them all on one plot. These are tabulated in Table I, which includes
for comparison the unconditioned intervals from Table X of Ref. [2].

Fig. 6 shows the modiﬁed intervals plotted together with the uniﬁed intervals of Ref. [2].
The modiﬁed upper curve is shifted upward for negative x, which results in a less stringent
upper limit when ε is known to be negative; this feature is considered desirable by some.
The lower curve, however, is also shifted upward: for all x0 > 0, the interval is two-sided.
We ﬁnd this to be a highly undesirable side-eﬀect.

It is interesting to consider what happens if one applies Fig. 5 to an unconditioned
ensemble. The result can be seen by drawing a horizontal line at any µ in Fig. 5 and inte-
grating P (x|µ) (Eqn.2.1) along that line between the belts. For small µ, there is signiﬁcant
undercoverage, while for µ near 1.0, there is signiﬁcant overcoverage. The undercoverage
was surprising, since the conditioned intervals always cover within the relevant subset of the
ensemble. However, conditioning on an inequality means that these subsets are not disjoint.
The undesirable raising of the lower curve is present in the Poisson case, as can be seen
in Figure 1 of Ref. [1]. However, there the discreteness of the Poisson problem apparently
prevents the curve from being shifted so dramatically, and the two-sided intervals do not
extend to such low values of the measured n.

4

IV. CONCLUSION

In this paper, we apply conditioning in the style Roe and Woodroofe to the Gaussian-
with-boundary problem. We ﬁnd that the transition from one-sided intervals to two-sided
intervals undesirably moves to the origin. This reﬂects a general feature of conﬁdence interval
construction: when moving one of the two curves, the other curve moves also. In the Poisson-
with-background problem, the undesirable movement was not large, but in the Gaussian-
with-boundary problem, the eﬀect is quite substantial.

ACKNOWLEDGMENTS

I thank Gary Feldman, Byron Roe, and Michael Woodroofe for comments on the paper.

This work was supported by the U.S. Department of Energy.

5

REFERENCES

cousins@physics.ucla.edu

†
[1] B.P. Roe and M.B. Woodroofe, Phys. Rev. D60 053009 (1999).
[2] G.J. Feldman and R.D. Cousins, Phys. Rev. D57 3873 (1998).

6

FIGURES

x0=-1.0
µ= 0.01

1.5

1

0.5

)
x
(
q

x0= 0.0
µ= 0.01

0.75

0.5

)
x
(
q

0.25

x0= 1.0
µ= 0.01

0.4

0.2

)
x
(
q

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

x0=-1.0
µ= 0.01

1.5

1

0.5

 

)
x
(
q
x
a
m

x0= 0.0
µ= 0.01

0.75

0.5

 

)
x
(
q
x
a
m

0.25

x0= 1.0
µ= 0.01

0.4

0.2

 

)
x
(
q
x
a
m

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

1

x0=-1.0
µ= 0.01

,

)
x
µ
(
R

0.5

1

x0= 0.0
µ= 0.01

,

)
x
µ
(
R

0.5

1

x0= 1.0
µ= 0.01

,

)
x
µ
(
R

0.5

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

FIG. 1. Graphs of qx0

µ′ (x) (middle row), and eRx0(µ, x) (bottom row),
for µ = 0.01. The columns are for x0 = −1, 0, and 1. Each graph in the bottom row is the quotient
of the two graphs above it.

µ (x) (top row), maxµ′ qx0

7

x0=-1.0
µ= 0.50

1.5

1

0.5

)
x
(
q

x0= 0.0
µ= 0.50

0.75

0.5

)
x
(
q

0.25

x0= 1.0
µ= 0.50

0.4

0.2

)
x
(
q

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

x0=-1.0
µ= 0.50

1.5

1

0.5

 

)
x
(
q
x
a
m

x0= 0.0
µ= 0.50

0.75

0.5

 

)
x
(
q
x
a
m

0.25

x0= 1.0
µ= 0.50

0.4

0.2

 

)
x
(
q
x
a
m

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

1

x0=-1.0
µ= 0.50

,

)
x
µ
(
R

0.5

1

x0= 0.0
µ= 0.50

,

)
x
µ
(
R

0.5

1

x0= 1.0
µ= 0.50

,

)
x
µ
(
R

0.5

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

FIG. 2. Graphs of qx0

µ′ (x) (middle row), and eRx0(µ, x) (bottom row),
for µ = 0.5. The columns are for x0 = −1, 0, and 1. Each graph in the bottom row is the quotient
of the two graphs above it.

µ (x) (top row), maxµ′ qx0

8

x0=-1.0
µ= 2.50

1.5

1

0.5

)
x
(
q

x0= 0.0
µ= 2.50

0.75

0.5

)
x
(
q

0.25

x0= 1.0
µ= 2.50

0.4

0.2

)
x
(
q

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

x0=-1.0
µ= 2.50

1.5

1

0.5

 

)
x
(
q
x
a
m

x0= 0.0
µ= 2.50

0.75

0.5

 

)
x
(
q
x
a
m

0.25

x0= 1.0
µ= 2.50

0.4

0.2

 

)
x
(
q
x
a
m

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

1

x0=-1.0
µ= 2.50

,

)
x
µ
(
R

0.5

1

x0= 0.0
µ= 2.50

,

)
x
µ
(
R

0.5

1

x0= 1.0
µ= 2.50

,

)
x
µ
(
R

0.5

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

0

-4

-2

0
x

2

4

FIG. 3. Graphs of qx0

µ′ (x) (middle row), and eRx0(µ, x) (bottom row),
for µ = 2.5. The columns are for x0 = −1, 0. and 1. Each graph in the bottom row is the quotient
of the two graphs above it.

µ (x) (top row), maxµ′ qx0

9

 

µ
n
a
e

M

µ
 
n
a
e

M

 

µ
n
a
e

M

6
6

5
5

4
4

3
3

2
2

1
1

0
0

6
6

5
5

4
4

3
3

2
2

1
1

0
0

6
6

5
5

4
4

3
3

2
2

1
1

0
0

-2
-2

-1
-1

-2
-2

-1
-1

-2
-2

-1
-1

x0 = -1.0

0
0

1
1

2
2

Measured Mean x

x0 = 0.0

0
0

1
1

2
2

Measured Mean x

x0 = 1.0

0
0

1
1

2
2

Measured Mean x

 

µ
n
a
e

M

µ
 
n
a
e

M

 

µ
n
a
e

M

6
6

5
5

4
4

3
3

2
2

1
1

0
0

6
6

5
5

4
4

3
3

2
2

1
1

0
0

6
6

5
5

4
4

3
3

2
2

1
1

0
0

-2
-2

-1
-1

-2
-2

-1
-1

-2
-2

-1
-1

3
3

4
4

3
3

4
4

3
3

4
4

x0 = -0.5

0
0

1
1

2
2

Measured Mean x

x0 = 0.5

0
0

1
1

2
2

Measured Mean x

x0 = 3.0

0
0

1
1

2
2

Measured Mean x

3
3

4
4

3
3

4
4

3
3

4
4

FIG. 4. Conditional conﬁdence belts for the six sample values of x0 indicated. Each plot is
used only to ﬁnd the [µ1, µ2] interval at x equal to the x0 used to construct it; that interval is
transferred to Fig.5.

10

 

µ
n
a
e

M

6
6

5
5

4
4

3
3

2
2

1
1

0
0

-2
-2

-1
-1

0
0

1
1

Measured Mean x

2
2

3
3

4
4

FIG. 5. Plot of RW-inspired 90% conditional conﬁdence intervals for mean of a Gaussian,

constrained to be non-negative, described in the text.

11

 

µ
n
a
e

M

6
6

5
5

4
4

3
3

2
2

1
1

0
0

-2
-2

-1
-1

0
0

1
1

Measured Mean x

2
2

3
3

4
4

FIG. 6. Plot of RW-inspired 90% conditional conﬁdence intervals (solid curves) , superimposed

on the unconditioned intervals of Ref. [2] (dotted curves).

12

TABLES

TABLE I. 90% C.L. conﬁdence intervals for the mean µ of a Gaussian, constrained to be
non-negative, as a function of the measured mean x0, for the RW conditioning method, and for
the uniﬁed approach of Feldman and Cousins. All numbers are in units of σ. The conditioned
numbers may be inaccurate at the level of ±0.01 due to the computational grid used.

x0
-3.0
-2.9
-2.8
-2.7
-2.6
-2.5
-2.4
-2.3
-2.2
-2.1
-2.0
-1.9
-1.8
-1.7
-1.6
-1.5
-1.4
-1.3
-1.2
-1.1
-1.0
-0.9
-0.8
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7

conditioned
( 0.00, 0.63)
( 0.00, 0.66)
( 0.00, 0.68)
( 0.00, 0.68)
( 0.00, 0.70)
( 0.00, 0.73)
( 0.00, 0.75)
( 0.00, 0.77)
( 0.00, 0.78)
( 0.00, 0.80)
( 0.00, 0.84)
( 0.00, 0.86)
( 0.00, 0.89)
( 0.00, 0.92)
( 0.00, 0.94)
( 0.00, 0.97)
( 0.00, 1.01)
( 0.00, 1.04)
( 0.00, 1.07)
( 0.00, 1.11)
( 0.00, 1.15)
( 0.00, 1.19)
( 0.00, 1.23)
( 0.00, 1.27)
( 0.00, 1.32)
( 0.00, 1.37)
( 0.00, 1.42)
( 0.00, 1.47)
( 0.00, 1.53)
( 0.00, 1.58)
( 0.00, 1.65)
( 0.00, 1.71)
( 0.01, 1.77)
( 0.02, 1.84)
( 0.04, 1.91)
( 0.06, 1.98)
( 0.08, 2.06)
( 0.11, 2.13)

13

unconditioned

0.00, 0.26
0.00, 0.27
0.00, 0.28
0.00, 0.29
0.00, 0.30
0.00, 0.32
0.00, 0.33
0.00, 0.34
0.00, 0.36
0.00, 0.38
0.00, 0.40
0.00, 0.43
0.00, 0.45
0.00, 0.48
0.00, 0.52
0.00, 0.56
0.00, 0.60
0.00, 0.64
0.00, 0.70
0.00, 0.75
0.00, 0.81
0.00, 0.88
0.00, 0.95
0.00, 1.02
0.00, 1.10
0.00, 1.18
0.00, 1.27
0.00, 1.36
0.00, 1.45
0.00, 1.55
0.00, 1.64
0.00, 1.74
0.00, 1.84
0.00, 1.94
0.00, 2.04
0.00, 2.14
0.00, 2.24
0.00, 2.34

0.00, 2.44
0.00, 2.54
0.00, 2.64
0.00, 2.74
0.00, 2.84
0.02, 2.94
0.12, 3.04
0.22, 3.14
0.31, 3.24
0.38, 3.34
0.45, 3.44
0.51, 3.54
0.58, 3.64
0.65, 3.74
0.72, 3.84
0.79, 3.94
0.87, 4.04
0.95, 4.14
1.02, 4.24
1.11, 4.34
1.19, 4.44
1.28, 4.54
1.37, 4.64
1.46, 4.74

0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0
3.1

( 0.13, 2.21)
( 0.16, 2.29)
( 0.19, 2.38)
( 0.22, 2.46)
( 0.26, 2.55)
( 0.29, 2.64)
( 0.33, 2.76)
( 0.38, 2.90)
( 0.42, 3.04)
( 0.47, 3.18)
( 0.52, 3.30)
( 0.57, 3.43)
( 0.63, 3.55)
( 0.69, 3.67)
( 0.75, 3.79)
( 0.82, 3.90)
( 0.89, 4.01)
( 0.96, 4.12)
( 1.04, 4.22)
( 1.12, 4.33)
( 1.20, 4.43)
( 1.28, 4.54)
( 1.37, 4.64)
( 1.47, 4.74)

14


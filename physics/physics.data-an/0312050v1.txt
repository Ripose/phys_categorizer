PhyStat2003, SLAC, September 8-11

1

Challenges in Moving the LEP Higgs Statistics to the LHC

K.S. Cranmer, B. Mellado, W. Quayle, Sau Lan Wu
University of Wisconsin-Madison, Madison, WI 53706, USA

3
0
0
2
 
c
e
D
 
8
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
5
0
2
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

We examine computational, conceptual, and philosophical issues in moving the statistical techniques used in
the LEP Higgs working group to the LHC.

1. Introduction

Higgs searches at LEP were based on marginal sig-
nal expectations and small background uncertainties.
In contrast, Higgs searches at the LHC are based on
strong signal expectations and relatively large back-
ground uncertainties. Based on our experience with
the LEP Higgs search, our group tried to move the
tools we had developed at LEP to the LHC environ-
ment. In particular, our calculation of conﬁdence lev-
els was based on an analytic computation with the
Fast Fourier Transform and the log-likelihood ratio as
a test statistic (and systematic errors based on the
Cousins-Highland approach). We encountered three
types of problems when calculating ATLAS’ com-
bined sensitivity to the Standard Model Higgs Boson:
problems associated with large numbers of expected
events, problems arising from very high signiﬁcance
levels, and problems related to the incorporation of
systematic errors.

Previously, it was shown that the migration of the
statistical techniques that were used in the LEP Higgs
Working Group to the LHC environment is not as
straightforward as one might na¨ively expect [1]. Af-
ter a brief overview in Section 2, those diﬃculties and
their ultimate solution are discussed in Section 3. Our
group has developed two independent software solu-
tions (both in C++; both with FORTRAN bindings; one
ROOT based and the other standalone) which can be
found at:

http://wisconsin.cern.ch/software
In Section 4 we discuss the incorporation of sys-
tematic errors and compare a few diﬀerent strategies.
In Section 5 we present and discuss the discovery lu-
minosity (the luminosity expected to be required for
discovery). Lastly, in Section 6 we discuss the statis-
tical notion of power (which is related to the probabil-
ity of Type II error (the probability we do reject the
“signal-plus-background hypothesis” when it is true).

2. The Formalism

approach to the calculation and to [5] for the ana-
lytic calculation using Fast Fourier Transform (FFT)
techniques. For completeness, we introduce the ba-
sic approach below using the notation found in [1].
For a counting experiment where we expect, on aver-
age, b background events and s signal events, we con-
sider two hypotheses: the null (or background-only)
hypothesis in which the number of expected events,
n, is described by a Poisson distribution P (n; b) and
the alternate (or signal-plus-background) hypothesis
in which the number of expected events is described
by a Poisson distribution P (n; s+b). Here the number
of events serves the purpose of a test statistic: a real
number which quantities an experiment.

It is possible to include a discriminating variable
x which has some probability density function (pdf)
for the background, fb(x), and some pdf for the sig-
nal, fs(x), both normalized to unity. Given an ob-
servation at x we can construct the Likelihood Ratio
Q = (sfs(x) + bfb(x))/bfb(x). With several indepen-
we can consider the combined
dent observations
{
likelihood ratio Q =
Qi . It is possible, and in some
sense optimal, to use Q (or in practice q = ln Q) as a
test statistic.

Q

ˆxi

}

The computational challenge of using the log-
likelihood ratio in conjunction with a discriminating
variable x is the construction of the log-likelihood ra-
tio distribution for the background-only hypothesis,
ρb(q), and for the signal-plus-background hypothesis
ρs+b(q). In this case, there are not only the Poisson
ﬂuctuations of the number of events, but also the con-
tinuously varying discriminating variable x. In partic-
ular, for a single background event the log-likelihood
ratio distribution, ρ1,b(q), must incorporate all possi-
ble values of x. From these single event distributions
we can build up the expected log-likelihood ratio dis-
tribution by repeated convolution. This is most eﬀec-
tively done by using a Fast Fourier Transform (FFT)
where convolution can be expressed as multiplication
in the frequency domain (denoted with a bar).
In
particular we arrive at:

Our starting point for this note is a brief review of
the techniques that were used at LEP. We refer the
interested reader to [2] for an introduction to the fun-
damentals, to [3] for why the likelihood ratio has been
chosen as a test statistic, to [4] for a Monte Carlo

ρb(q) = eb[ρ1,b(q)−1]

and
ρs+b(q) = e(s+b)[ρ1,s+b(q)−1].

(1)

From the log-likelihood distribution of the two hy-
potheses we can calculate a number of useful quan-

MODT004

2

PhyStat2003, SLAC, September 8-11

tities. Given some experiment with an observed log-
likelihood ratio, q∗, we can calculate the background-
only conﬁdence level, CLb :

CLb(q∗) =

ρb(q′)dq′

(2)

∞

Z

q∗

In the absence of an observation we can calculate the
expected CLb given the signal-plus-background hy-
pothesis is true. To do this we ﬁrst must ﬁnd the me-
dian of the signal-plus-background distribution qs+b.
From these we can calculate the expected CLb by us-
ing Eq. 2 evaluated at q∗ = qs+b.

Finally, we can convert the expected background
conﬁdence level into an expected Gaussian signiﬁ-
cance, N σ, by ﬁnding the value of N which satisﬁes

CLb(qs+b) =

1

−

erf(N/√2)

.

2

(3)

where erf(N ) = (2/π)
readily available in most numerical libraries.

−

N
0 exp(
R

y2)dy is a function

3. Numerical Difﬁculties

The methods described in the previous section have
been applied to the combined ATLAS Higgs eﬀort
with some caveats related to numerical diﬃculties [1].
In particular, in the extreme tails of ρb(q), the prob-
ability density is dominated by numerical noise. This
numerical noise is an artifact of round-oﬀ error in
the double precision numbers used in the Fast Fourier
Transform1. The noise is on the order of 10−17 (for
double precision ﬂoating point numbers), which trans-
lates into a limit on the signiﬁcance of about 8σ. For
particular values of the Higgs mass, ATLAS has an
expected signiﬁcance well above 8σ with only 10 fb−1
of data. In order to produce signiﬁcance values above
the 8σ limit, various extrapolation methods were used
in [1]. We now introduce a deﬁnitive solution to this
problem based on arbitrary precision ﬂoating point
numbers.

It should be made clear that the numerical precision
problem is not due to the fact that the CLb is so small
that the evaluation of the integral in Eq. 2 cannot be
treated with double precision ﬂoating point numbers.
Instead, the numerical precision problem is due to the
many (approximately 220) Fourier modes which must
in total produce a number very close to 0. In order
to rectify this problem we have implemented the Fast
Fourier Transform with the arbitrary-precision ﬂoat-
ing point numbers provided in the CLN library2 [6].

ρ

b

ρ

s+b

)
q
(
ρ

−1

−2

−3

−4

−5

−6

−7

−8

−9

10

10

10

10

10

10

10

10

10

−10

−11

−12

−13

−14

−15

−16

10

10

10

10

10

10

10

−500

0

500

1000

1500

2000

2500

3000

q

2

x 10

Figure 1: The distribution of the log-likelihood ratio ρ(q)
for the null and alternate hypothesis (the axis labels refer
to bins of q, not q itself). For q > 105 the distribution is
contaminated by numerical noise (see text for details).

One might protest that above 5σ we are not interested
in the precise value of the signiﬁcance and that this
exercise is purely academic. We refer the interested
reader to Sections 5 & 6 for diﬀerent summaries of
an experiments discovery potential.

3.1. Extrapolation

While the arbitrary precision FFT approach is the
deﬁnitive solution to the problem of calculating very
high expected signiﬁcance, it is also incredibly time
consuming. A much faster, approximate solution is
to approximate the CLb by ﬁtting the ρb distribution
to a functional form. The ﬁrst method of extrapola-
tion studied was a simple Gaussian ﬁt to the ρb dis-
tribution. This method works fairly well, but tends
to overestimate the signiﬁcance. The second method
we studied was based on a Poisson ﬁt to the ρb dis-
tribution. The Poisson distribution has the desirable
properties that it will have no probability below the
hard limit q
s and that its shape is more appro-
priate [1]. Figure 2 compares these diﬀerent extrapo-
lation methods.

≥ −

4. Incorporating Systematic Uncertainty

One encounters both philosophical and technical
diﬃculties when one tries to incorporate uncertainty
on the predicted values s and b found in Eq. 1. In a
Frequentist formalism the unknown s and b become
nuisance parameters. In a Bayesian formalism, s and
b can be marginalized by integration over their respec-
tive priors. At LEP the practice was to smear ρb and
ρs+b by integrating s and b with a multivariate nor-
mal distribution as a prior. This smearing technique is
commonly referred to as the Cousins-Highland Tech-
nique, and it is has some Bayesian aspects.

1We use the FFTW library: http://www.ﬀtw.org
2CLN is available at http://www.ginac.de

MODT004

PhyStat2003, SLAC, September 8-11

3

Added in Quadrature

Likelihood Combination - no extrapolation

Likelihood Combination - Gaussian extrapolation
Likelihood Combination - Poisson extrapolation

e
c
n
a
c

i
f
i
n
g
S

i

10 2

5σ

10

1

Statistical Demonstration
NO SYSTEMATIC ERRORS

ATLAS

∫L dt = 10 fb-1
(no K-factors)

100

120

140

160

180

200

MH

Figure 2: Comparison of the ATLAS Higgs combined
signiﬁcance obtained from several approximate
techniques. The (red) dashed line corresponds to the
unmodiﬁed likelihood ratio which can not produce
signiﬁcance values above about 8σ (see text). This ﬁgure
is meant to demonstrate the diﬀerent methods of
combination and does not include up-to-date results from
the various Higgs analyses.

4.1. A Purely Frequentist Technique

At the PhysStat2003 conference a purely frequentist
approach to hypothesis testing with background un-
certainty was presented [7]. This method relies on the
full Neyman construction and uses a likelihood ratio
similar to the proﬁle method as an ordering rule. In
this formalism, a systematic uncertainty at the level
of 10% has a much larger eﬀect than when treated
with the Cousins-Highland technique.

4.2. The CousinsHighland Technique

The Cousins-Highland formalism for including sys-
tematic errors on the normalization of the signal and
background is provided in [8] and generalized in [4, 5].
In particular, for a multivariate normal distribution3
as a prior for the ni the distribution of the log-
likelihood ratio is given by:

ρsys(q) =

...

e

P

K
i ni[ρ1,i(q)−1]

Z

Z
K
i

e

P

P

K

j − 1

2 (ni−hnii)S

1
√2π (cid:19)

(cid:18)
−1
ij (nj −hnj i)

K 1
S
|
|
p
dni

(4)

Yi

3In principle, any distribution could be used within this

framework.

MODT004

ni

nj

− h

− h

(ni
h

where Sij =
. Reference [5]
)
)(nj
i
i
i
provides an analytic expression for the resulting log-
likelihood ratio distribution including a correlated er-
ror matrix; however, this equation was obtained with
an integration over negative numbers of expected
events and does not hold. Attempts to provide a
closed form solution for the positive semi-deﬁnite re-
gion require analytical continuation of the error func-
tion over a wide range of the complex plane. Instead,
a numerical integration over the positive semi-deﬁnite
region has been adopted for our software packages.

5. Discovery Luminosity

Because the calculation of expected signiﬁcance is
technically very diﬃcult at the LHC, other summaries
of the discovery potential have been explored. While
these techniques are not new, it is important to con-
sider their pros and cons. One such alternate sum-
mary of the discovery potential is based on the dis-
covery luminosity”. Deﬁne the discovery luminosity,
L∗(mH ), to be the integrated luminosity necessary for
the expected signiﬁcance to reach 5σ. The discov-
ery luminosity is an informative quantity; however, it
must be interpreted with some care:

•

•

Collecting an integrated luminosity equal to the
nominal discovery luminosity does not guaran-
tee that a discovery will be made. Instead, with
L∗(mH ) of data the median of ρs+b will be at the
5σ level – which corresponds to a 50% chance of
discovery. See Section 6 for more details.

In practice an analysis’ cuts, systematic er-
ror, and signal and background eﬃciencies are
luminosity-dependent quantities. When we cal-
culate the discovery luminosity, we treat the
analysis as constant.

6. The Power of a 5σ Test

The traditional quantity which is used to summarize
an experiment’s discovery potential is the combined
signiﬁcance; however, as was noted in Section 3 this
plot becomes very diﬁcult to make when the signiﬁ-
cance goes beyond about 8σ. Furthermore, the plot
itself starts to loose relevance when the signiﬁcance
is far above 5σ. The discovery luminosity is another
possible way of illustrating an experiment’s discov-
ery potential, but it must be interpreted with some
care. A third summary of an experiment’s discovery
potential which is related to the probability of Type
II error: the power. First, it should be noted that the
expected signiﬁcance is a measure of separation be-
tween the medians of the background-only and signal-
plus-background hypotheses. Thus, when we see the

4

PhyStat2003, SLAC, September 8-11

5σ

Power = 0.98

Power = 0.5

y
t
i
s
n
e
D
 
y
t
i
l
i

b
a
b
o
r
P

0.05

0.045

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

0

0

50

100

150

200

250

300

Number of Events Expected

Figure 3: Examples of power for two diﬀerent
signal-plus-background hypotheses with respect to a
single background-only hypothesis with 100 expected
events (black).

signiﬁcance curve cross the 5σ line in Fig. 2 there is
only a 50% chance that we would observe a 5σ eﬀect if
the Higgs does indeed exist at that mass. In practice,
we claim a discovery if the observed data exceeds the
5σ critical region, and do not claim a discovery if it
doesn’t. The meaning of the 5σ discovery threshold
is a convention which sets the probability of Type I
10−7 . With that in mind, the idea
error to be 2.85
that the signiﬁcance is 20σ at mH = 160 GeV is irrel-
evant. What is relevant is the probability that we will
claim discovery of the Higgs if it is indeed there: that
quantity is called the power. The power is deﬁned as
1
β where β is the probability of Type II error: the
probability that we reject the signal-plus-background
hypothesis when it is true [2].

−

·

Consider Figure 3 with a background expectation of
100 events. The black vertical arrow denotes the 5σ
discovery threshold. The (red) dashed curve shows
the distribution of the number of expected events for
a signal-plus-background hypothesis with 150 events.
Normally, we would say the expected signiﬁcance is
5σ for this hypothesis; however, we can see that only
50% of the time we would actually claim discovery.
The rightmost (blue) curve shows the distribution
of the number of expected events for a signal-plus-
background hypothesis with 180 events. Normally, we
would say the expected signiﬁcance is 8σ for this hy-
pothesis; however, a more meaningful quantity – the
power – is associated with the probability we would
claim discovery which is about 98%. In addition to
the power being a germane quantity, it is much easier
to calculate.

7. Conclusion

In conclusion, the migration of the statistical tool-
set developed at LEP to the LHC environment is not

MODT004

as straightforward as one might expect. The ﬁrst diﬃ-
culties are computational and arise from the combina-
tion of channels with many events and channels with
few events (these are easily solved). The next dif-
ﬁculties are numerical and arise from the extremely
high expected signiﬁcance of the high-energy fron-
tier. These problems can be solved by brute force;
or they can be reinterpreted as conceptual problems,
and solved by asking diﬀerent questions (i.e. power).
Lastly, there is a philosophical split related to the
Bayesian and Frequentist approach to uncertainty. At
the LHC, the choice of the formalism is no longer a
second-order eﬀect, and this problem is not so easy to
solve.

Acknowledgments

This work was supported by a graduate research fel-
lowship from the National Science Foundation and US
Department of Energy Grant DE-FG0295-ER40896.

References

→

W +W −

[1] K.S. Cranmer et. al. Conﬁdence level calculations
l+l−/pT for 115 < MH <
for H
130 GeV using vector boson fusion. ATLAS com-
munication ATL-COM-PHYS-2002-049 (2002).
[2] J.K Stuart, A. Ord and S. Arnold. Kendall’s Ad-
vanced Theory of Statistics, Vol 2A (6th Ed.). Ox-
ford University Press, New York, 1994.

→

[3] A.L. Read. Optimal statistical analysis of search
results based on teh likelihood ratio and its ap-
plication to the search for the MSM higgs boson
at √s = 161 and 172 GeV. DELPHI note 97-158
PHYS 737 (1997).

[4] T. Junk. Conﬁdence level computation for combin-
ing searches with small statistics. Nucl. Instrum.
Meth., A434:435–443, 1999.

[5] J. Nielsen H. Hu. Analytic conﬁdence level calcu-
lations using the likelihood ratio and fourier trans-
form. “Workshop on Conﬁdence Limits”, Eds. F.
James, L. Lyons and Y. Perrin, CERN 2000-005
(2000), p. 109.

[6] C. Bauer et. al. Introduction to the GiNaC frame-
work for symbolic computation within the c++
programing language. J. Symbolic Computation,
33:1–12, 2002.

[7] K.S. Cranmer. Frequentist hypothesis testing with
“PhyStat2003”,

background uncertainty., 2003.
SLAC. physics/0310108.

[8] R.D. Cousins and V.L. Highland.

Incorporating
systematic uncertainties into an upper limit. Nucl.
Instrum. Meth., A320:331–335, 1992.


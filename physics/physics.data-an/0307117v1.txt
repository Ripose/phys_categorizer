3
0
0
2
 
l
u
J
 
3
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
1
1
7
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Symbolic Stochastic Dynamical Systems Viewed as Binary N-Step Markov Chains

O. V. Usatenko ∗, V. A. Yampol’skii
A. Ya. Usikov Institute for Radiophysics and Electronics
Ukrainian Academy of Science, 12 Proskura Street, 61085 Kharkov, Ukraine

K. E. Kechedzhy, S. S. Mel’nyk
Department of Physics, Kharkov National University, 4 Svoboda Sq., Kharkov 61077, Ukraine
(Dated: February 2, 2008)

A theory of systems with long-range correlations based on the consideration of binary N-step
Markov chains is developed. In the model, the conditional probability that the i-th symbol in the
chain equals zero (or unity) is a linear function of the number of unities among the preceding N
symbols. The correlation and distribution functions as well as the variance of number of symbols
in the words of arbitrary length L are obtained analytically and numerically. A self-similarity
of the studied stochastic process is revealed and the similarity group transformation of the chain
parameters is presented. The diﬀusion Fokker-Planck equation governing the distribution function
of the L-words is explored. If the persistent correlations are not extremely strong, the distribution
function is shown to be the Gaussian with the variance being nonlinearly dependent on L. The
applicability of the developed theory to the coarse-grained written and DNA texts is discussed.

PACS numbers: 05.40.-a, 02.50.Ga, 87.10.+e

I.

INTRODUCTION

The problem of systems with long-range spatial and/or
temporal correlations (LRCS) is one of the topics of in-
tensive research in modern physics, as well as in the
theory of dynamical systems and the theory of proba-
bility. The LRC-systems are usually characterized by a
complex structure and contain a number of hierarchic
objects as their subsystems. The LRC-systems are the
subject of study in physics, biology, economics, linguis-
tics, sociology, geography, psychology, etc. [1, 2, 3, 4].
At the present time, there is no generally accepted the-
oretical model that adequately describes the dynamical
and statistical properties of the LRC-systems. Attempts
to describe the behavior of the LRCS in the framework
of the Tsalis non-extensive thermodynamics [5, 6] were
undertaken in Ref. [7]. However, the non-extensive ther-
modynamics is not well-grounded and requires the con-
struction of the additional models which could clarify the
properties of the LRC-systems.

One of the eﬃcient methods to investigate the corre-
lated systems is based on a decomposition of the space
of states into a ﬁnite number of parts labelled by deﬁnite
symbols. This procedure referred to as coarse graining is
accompanied by the loss of short-range memory between
states of system but does not aﬀect and does not damage
its robust invariant statistical properties on large scales.
The most frequently used method of the decomposition
is based on the introduction of two parts of the phase
space.
In other words, it consists in mapping the two
parts of states onto two symbols, say 0 and 1. Thus, the
problem is reduced to investigating the statistical prop-

erties of the symbolic binary sequences. This method is
applicable for the examination of both discrete and con-
tinuous systems.

One of the ways to get a correct insight into the na-
ture of correlations consists in an ability of constructing a
mathematical object (for example, a correlated sequence
of symbols) possessing the same statistical properties as
the initial system. There are many algorithms to gener-
ate long-range correlated sequences: the inverse Fourier
transform [8], the expansion-modiﬁcation Li method [9],
the Voss procedure of consequent random addition [10],
the correlated Levy walks [11], etc. [8]. We believe that,
among the above-mentioned methods, using the Markov
chains is one of the most important. We would like to
demonstrate this statement in the present paper.

In the following sections, the statistical properties of
the binary many-steps Markov chain is examined.
In
spite of the long-time history of studying the Markov
sequences (see, for example, [4, 12, 13] and references
therein), the concrete expressions for the variance of sums
of random variables in such strings have not yet been
obtained. Our model operates with two parameters gov-
erning the conditional probability of the discrete Markov
process, speciﬁcally with the memory length N and the
correlation parameter µ. The correlation and distribu-
tion functions as well as the variance D being nonlin-
early dependent on the length L of a word are derived
analytically and calculated numerically. The nonlinear-
ity of the D(L) function reﬂects the existence of strong
correlations in the system. The evolved theory is applied
to the coarse-grained written texts and dictionaries, and
to DNA strings as well.

Some preliminary results of this study were published

∗usatenko@ire.kharkov.ua

in Ref. [14].

II. FORMULATION OF THE PROBLEM

A. Markov Processes

{

}

0, 1

Let us consider a homogeneous binary sequence of sym-
. To determine the N -step Markov
bols, ai =
chain we have to introduce the conditional probabil-
ity P (ai
ai−N , ai−N +1, . . . , ai−1) of occurring the def-
inite symbol ai (for example, ai = 0) after symbols
ai−N , ai−N +1, . . . , ai−1. Thus, it is necessary to deﬁne
2N values of the P -function corresponding to each possi-
ble conﬁguration of the symbols ai−N , ai−N +1, . . . , ai−1.
We suppose that the P -function has the form,

|

P (ai = 0

ai−N , ai−N +1, . . . , ai−1)

=

1
N

N

k=1
X

f (ai−k, k).

(1)

Such a relation corresponds to the additive inﬂuence of
the previous symbols on the generated one. The homo-
geneity of the Markov chain is provided by the indepen-
dence of the conditional probability Eq. (1) of the index
i.

It is reasonable to assume the function f to be de-
creasing with an increase of the distance k between the
symbols ai−k and ai in the Markov chain. However, for
the sake of simplicity we consider here a step-like memory
function f (ai−k, k) independent of the second argument
k. As a result, the model is characterized by three pa-
rameters only, speciﬁcally by f (0), f (1), and N :

P (ai = 0

ai−N , ai−N +1, . . . , ai−1)

|

|

=

1
N

N

k=1
X

f (ai−k).

(2)

Note that the probability P in Eq. (2) depends on the
numbers of symbols 0 and 1 in the N -word but is inde-
pendent of the arrangement of the elements ai−k. We
also suppose that

f (0) + f (1) = 1.

(3)

This relation provides the statistical equality of the num-
bers of symbols zero and unity in the Markov chain under
consideration. In other words, the chain is non-biased.
Indeed, taking into account Eqs. (2) and (3) and the se-
quence of equations,

↔

one can see the symmetry with respect to interchange
˜ai
ai in the Markov chain. Here ˜ai is the symbol
ai. Therefore, the probabilities
opposite to ai, ˜ai = 1
of occurring the words (a1, . . . , aL) and (˜a1, . . . , ˜aL) are
equal to each other for any word length L. At L = 1 this
yields equal average probabilities that symbols 0 and 1
occur in the chain.

−

Taking into account the symmetry of the conditional
probability P with respect to a permutation of symbols ai
(see Eq. (2)), we can simplify the notations and introduce
the conditional probability pk of occurring the symbol
zero after the N -word containing k unities, e.g., after the
word (11...1

00...0

),

k

N −k

| {z }

| {z }

pk = P (aN +1 = 0

11 . . . 1

00 . . . 0

)

|

k

N −k

=

+ µ(1

1
2

| {z }
2k
N

−

),

| {z }

with the correlation parameter µ being deﬁned by the
relation

µ = f (0)

1
2

.

−

We focus our attention on the region of µ determined
by the persistence inequality 0 < µ < 1/2. In this case,
each of the symbols unity in the preceding N-word pro-
motes the birth of new symbol unity. Nevertheless, the
major part of our results is valid for the anti-persistent
region

1/2 < µ < 0 as well.

A similar rule for the production of an N -word
(a1, . . . , aN ) that follows after a word (a0, a1, . . . , aN −1)
was suggested in Ref. [4]. However, the conditional prob-
ability pk of occurring the symbols aN does not depend
on the previous ones in the model [4].

−

B. Statistical characteristics of the chain

In order to investigate the statistical properties of the
Markov chain, we consider the distribution WL(k) of the
words of deﬁnite length L by the number k of unities in
them,

P (ai = 1

ai−N , . . . , ai−1) = 1
|

−

P (ai = 0

ai−N , . . . , ai−1)
|

where

=

1
N

N

k=1
X

f (˜ai−N ) = P (ai = 0

˜ai−N , . . . , ˜ai−1),

(4)

f (k) =

f (k)WL(k).

|

and the variance of k,

ki(L) =

ai+l,

L

l=1
X

D(L) = k2

2

k

,

−

L

k=0
X

2

(5)

(6)

(7)

(8)

(9)

If µ = 0, one arrives at the known result for the non-
correlated Brownian diﬀusion,

D(L) = L/4.

(10)

We will show that the distribution function WL(k) for
the sequence determined by Eq. (5) (with nonzero but not
extremely close to 1/2 parameter µ) is the Gaussian with
the variance D(L) nonlinearly dependent on L. However,
at µ
1/2 the distribution function can diﬀer from the
Gaussian.

→

C. Main equation

For the stationary Markov chain, the probability
b(a1a2 . . . aN ) of occurring a certain word (a1, a2, . . . , aN )
satisﬁes the condition of compatibility for the Chapmen-
Kolmogorov equation (see, for example, Ref. [15]):

b(a1 . . . aN )

=

b(aa1 . . . aN −1)P (aN

a, a1, . . . , aN −1).

(11)

|

a=0,1
X

Thus, we have 2N homogeneous algebraic equations for
the 2N probabilities b of occurring the N -words and the
b = 1. In the case under con-
normalization equation
sideration, the set of equations can be substantially sim-
pliﬁed owing to the following statement.

P

Proposition

: The probability b(a1a2 . . . aN ) de-
pends on the number k of unities in the N -word only,
i. e., it is independent of the arrangement of symbols in
the word (a1, a2, . . . , aN ).

♠

0,008

0,007

0,006

)
z
(
b

0,005

0,004

0,003

0

50

100

150

200

250

z

FIG. 1: The probability b of occurring a word (a1, a2, . . . , aN )
N
vs its number z expressed in the binary code, z =
i=1 ai ·
i−1, for N = 8, µ = 0.4.
2

P

This statement illustrated by Fig. 1 is valid owing to
the chosen simple model (2), (5) of the Markov chain.

3

It can be easily veriﬁed directly by substituting the ob-
tained below solution (15) into the set (11). Note that
according to the Markov theorem, Eqs. (11) do not have
other solutions [16].

Proposition

leads to the very important property
of isotropy: any word (a1, a2, . . . , aL) appears with the
same probability as the inverted one, (aL, aL−1, . . . , a1).
Let us apply the set of Eqs. (11) to the word

♠

(11 . . . 1

00 . . . 0

):

k

N −k

| {z }

b(11 . . . 1
| {z }
k

This
b(11...1

yields
00...0

),

k

N −k

| {z }

| {z }

00 . . . 0

) = b(0 11 . . . 1

00 . . . 0

)pk

N −k

k

N −k−1

| {z }

| {z }
+ b(1 11 . . . 1

| {z }

00 . . . 0

)pk+1.

| {z }

(12)

k

N −k−1

the

recursion
| {z }

| {z }

relation

for

b(k) =

b(k) =

1

−

pk−1
pk

b(k

1)

−

=

N

2µ(N
−
−
N + 2µ(N

2k + 2)
2k)

b(k

1).

−

(13)

−
The probabilities b(k) for µ > 0 satisfy the sequence of
inequalities,

b(0) = b(N ) > b(1) = b(N

1) > ... > b(N/2),

(14)

−

which is the reﬂection of persistent properties for the
chain. At µ = 0 all probabilities are equal to each other.

The solution of Eq. (11) is

b(k) = A

Γ(n + k)Γ(n + N

k)

(15)

·

−

with the parameter n deﬁned by

n =

N (1

2µ)

−
4µ

.

The constant A will be found below by normalizing the
distribution function. Its value is,

A =

4n
2√π

Γ(1/2 + n)
Γ(n)Γ(2n + N )

.

(16)

(17)

III. DISTRIBUTION FUNCTION OF L-WORDS

In this section we investigate the statistical properties
of the Markov chain, speciﬁcally, the distribution of the
words of deﬁnite length L by the number k of unities.
The length L can also be interpreted as the number of
jumps of some particle over an integer-valued 1-D lattice
or as the time of the diﬀusion imposed by the Markov
chain under consideration. The form of the distribution
function WL(k) depends, to a large extent, on the relation
between the word length L and the memory length N .
Therefore, the ﬁrst thing we will do is to examine the
simplest case L = N .

4

A. Statistics of N -words

2.

Intermediate case, n >

∼ 1

The value b(k) is the probability that an N -word con-
tains k unities with a deﬁnite order of symbols ai. There-
fore, the probability WN (k) that an N -word contains k
unities with arbitrary order of symbols ai is b(k) mul-
tiplied by the number Ck
k)! of diﬀerent
−
permutations of k unities in the N -word,

N = N !/k!(N

WN (k) = Ck

N b(k).

(18)

Combining Eqs. (15) and (18), we ﬁnd the distribution
function,

WN (k) = WN (0)Ck
N

Γ(n + k)Γ(n + N

Γ(n)Γ(n + N )

k)

.

−

(19)

The normalization constant WN (0) can be obtained from

the equality

WN (k) = 1,

N

k=0
P
WN (0) =

4n
2√π

Γ(n + N )Γ(1/2 + n)
Γ(2n + N )

.

(20)

Comparing Eqs. (15), (18)-(20), one can get Eq. (17) for
the constant A in Eq. (15).

Note that the distribution WN (k) is an even function

of the variable κ = k

N/2,

−
WN (N

−

k) = WN (k).

(21)

This fact is a direct consequence of the above-mentioned
statistical equivalence of zeros and unities in the Markov
chain being considered. Let us analyze the distribution
function WN (k) for diﬀerent relations between the pa-
rameters N and µ.

1. Limiting case of weak persistence, n ≫ 1

In the absence of correlations, n

, Eq. (19) and
the Stirling formula yield the Gaussian distribution at
1. Given the persistence is not too strong,
k, N, N

→ ∞

k

−

≫

n

1,

If the parameter n is an integer of the order of unity,
the distribution function WN (k) is a polynomial of degree
2(n
1). In particular, at n = 1, the function WN (k) is
−
constant,

WN (k) =

1
N + 1

.

(25)

= 1, WN (k) has a maximum in the middle of the

At n
interval [0, N ].

3. Limiting case of strong persistence

If the parameter n satisﬁes the inequality,

ln−1 N,

n

≪

(26)

one can neglect the parameter n in the arguments of the
functions Γ(n+k), Γ(n+N ), and Γ(n+N
k) in Eq. (19).
In this case, the distribution function WN (k) assumes its
maximal values at k = 0 and k = N ,

−

WN (1) = WN (0)

WN (0).

(27)

nN

N

1 ≪

−

Formula (27) describes the sharply decreasing WN (k) as
1). Then, at
k varies from 0 to 1 (and from N to N
1 < k < N/2, the function WN (k) decreases more slowly
with an increase in k,

−

WN (k) = WN (0)

(28)

nN

k(N

−

.

k)

At k = N/2, the probability WN (k) achieves its minimal
value,

WN

= WN (0)

(29)

N
2

(cid:18)

(cid:19)

4n
N

.

(22)

(23)

≫
one can also obtain the Gaussian form for the distribution
function,

It follows from normalization (20) that the values
WN (0) = WN (N ) are approximatively equal to 1/2. Ne-
glecting the terms of the order of n2, one gets

WN (k) =

(cid:26)
with the µ-dependent variance,
p

2πD(N )

1

exp

(k

N/2)2

−
2D(N )

−

,

(cid:27)

D(N ) =

N (N + 2n)
8n

=

N

−

4(1

2µ)

.

(24)

Equation (23) says that the N -words with equal numbers
of zeros and unities, k = N/2, are most probable. Note
that the persistence results in an increase of the variance
D(N ) with respect to its value N/4 at µ = 0. In other
words, the persistence is conductive to the intensiﬁcation
N 2.
of the diﬀusion.
Therefore, despite the increase of D(N ), the ﬂuctuations
N/2) of the order of N are exponentially small.
of (k

1 gives D(N )

Inequality n

≪

≫

−

WN (0) =

(1

n ln N ).

(30)

1
2

−

In the straightforward calculation using Eqs.
(28) the variance D is

(8) and

D(N ) =

N 2
4 −

nN (N
2

1)

.

−

(31)

Thus, the variance D(N ) is equal to N 2/2 in the lead-
ing approximation in the parameter n. This fact has a
simple explanation. The probability of occurrence the
N -word containing N unities is approximatively equal
= N 2/4
to 1/2. So, the relations k2
give (31). The case of strong persistence corresponds

N 2/2 and k

≈

2

6
to the so-called ballistic regime of diﬀusion: if we chose
randomly some symbol ai in the sequence, it will be sur-
rounded by the same symbols with the probability close
to unity.

The evolution of the distribution function WN (k) from
the Gaussian form to the inverse one with a decrease
In the interval
of the parameter n is shown in Fig. 2.
ln−1 N < n < 1 the curve WN (k) is concave and the
maximum of function WN (k) inverts into minimum. At
1 and ln−1 N < n < 1, the curve remains a smooth
N
function of its argument k as shown by curve with n =
0.5 in Fig. 2. Below, we will not consider this relatively
narrow region of the change in the parameter n.

≫

Formulas (23), (24), (28), (30) and (31) describe the
statistical properties of L-words for the ﬁxed ”diﬀusion
time” L = N . It is necessary to examine the distribution
function WL(k) for the general situation, L
= N . We
start the analysis with L < N .

0,15

0,10

)
k
(

W

0
2

0,05

0,00

40

3

1

k

0.5

0

5

10

15

20

FIG. 2: The distribution function WN (k) for N =20 and dif-
ferent values of the parameter n shown near the curves.

B. Statistics of L-words with L < N

1. Distribution function WL(k)

The distribution function WL(k) at L < N can be

given as

WL(k) =

b(i)Ck

LCi−k

N −L.

(32)

k+N −L

i=k
X

5

(34)

(35)

(36)

(37)

The remaining (i
k) unities are situated within the left-
−
hand part of the word (within (N
L)-sub-word). The
−
LCi−k
multiplier Ck
N −L in Eq. (32) takes into account all
possible permutations of the symbols ”1” within the N -
word on condition that the L-sub-word always contains k
unities. Then we perform the summation over all possible
values of the number i. Note that Eq. (32) is a direct
consequence of the proposition
formulated in Subsec. C
of the previous section.

♠

The straightforward summation in Eq. (32) yields the
following formula that is valid at any value of the param-
eter n:

WL(k) = WL(0)Ck
L

Γ(n + k)Γ(n + L
Γ(n)Γ(n + L)

−

k)

where

WL(0) =

4n
2√π

Γ(1/2 + n)Γ(n + L)
Γ(2n + L)

.

It is of interest to note that the parameter of per-
sistence µ and the memory length N are presented in
Eqs. (34), (35) via the parameter n only. This means
that the statistical properties of the L-words with L < N
are deﬁned by this single ”combined” parameter.
In the limiting case of weak persistence, n

1, at
1, Eq. (34) along with the Stirling formula

k, L
give the Gaussian distribution function,

≫

≫

−

k

WL(k) =

1

2πD(L)

exp

−

(cid:26)

(k

L/2)2

−
2D(L)

(cid:27)

with the variance D(L),

p

D(L) =

1 +

=

1 +

L
4

(cid:18)

L
2n

(cid:19)

L
4

(cid:20)

2µL

N (1

2µ)

−

.

(cid:21)

In the case of strong persistence (26), the asymptotic
expression for the distribution function Eq. (34) can be
written as

WL(k) = WL(0)

, k

= 0, k

= L,

(38)

nL

k(L

k)

−

WL(0) = WL(L) =

(1

n ln L).

(39)

1
2

−

Both the distribution WL(k) (38) and the function
WN (k) (28) has a concave form. The former assumes
the maximal value (39) at the edges of the interval [0, L]
and has a minimum at k = L/2.

This equation follows from the consideration of N -words
consisting of two parts,

2. Variance D(L)

(a1, . . . , aN −L,

aN −L+1, . . . , aN

).

(33)

i−k unities

k unities

Using the deﬁnition Eq. (8) and the distribution func-
tion Eq. (34) one can obtain a very simple formula for
the variance D(L),

|
The total number of unities in this word is i. The right-
hand part of the word (L-sub-word) contains k unities.

{z

{z

}

}

|

D(L) =

[1 + m(L

1)],

(40)

L
4

−

6
6
6
6

(46)

(47)

with

m =

1
1 + 2n

=

2µ
2µ(N

.

1)

−

N

−

Eq. (40) shows that the variance D(L) obeys the para-
bolic law independently of the correlation strength in the
Markov chain.

In the case of weak persistence, at n

1, we obtain
the asymptotics Eq. (37). It allows one to analyze the
behavior of the variance D(L) with an increase in the
1, the dependence
“diﬀusion time” L. At small mL
D(L) follows the classical law of the Brownian diﬀusion,
D(L)
1, the function D(L)
becomes super-linear.

L/4. Then, at mL

≪

≫

≈

∼

For the case of strong persistence, n

1, Eq. (40)

≪

gives the asymptotics,

D(L) =

L2
4 −

nL(L
2

1)

.

−

(42)

≪

The ballistic regime of diﬀusion leads to the quadratic
law of the D(L) dependence in the zero approximation
in the parameter n

1.

The unusual behavior of the variance D(L) raises an
issue as to what particular type of the diﬀusion equa-
tion corresponds to the nonlinear dependence D(L) in
Eq. (37). In the following subsection, when solving this
problem, we will obtain the conditional probability p(0)
of occurring the symbol zero after a given L-word with
L < N . The ability to ﬁnd p(0), with some reduced in-
formation about the preceding symbols being available,
is very important for the study of the self-similarity of
the Markov chain (see Subsubsec. 4 of this Subsection).

3. Generalized diﬀusion equation at L < N , n ≫ 1

It is quite obvious that the distribution WL(k) satisﬁes

the equation

WL+1(k) = WL(k)p(0)(k) + WL(k

1)p(1)(k

1). (43)

−

−

Here p(0)(k) is the probability of occurring ”0” after
an average-statistical L-word containing k unities and
p(1)(k
1) is the probability of occurring ”1” after an
L-word containing (k
1) unities. At L < N , the prob-
ability p(0)(k) can be written as

−

−

p(0)(k) =

1
WL(k)

k+N −L

i=k
X

pib(i)Ck

LCi−k

N −L.

(44)

LCi−k

The product b(i)Ck
N −L in this formula represents the
conditional probability of occurring the N -word contain-
ing i unities, the right-hand part of which, the L-sub-
word, contains k unities (compare with Eqs. (32), (33)).
N −L in Eq. (44) is a sharp function
of i with a maximum at some point i = i0 whereas pi
obeys the linear law (5). This implies that pi can be

The product b(i)Ci−k

factored out of the summation sign being taken at point
i = i0. The asymptotical calculation shows that point i0
is described by the equation,

(41)

i0 =

N
2 −

L/2

1

2µ(1

L/N )

−
Expression (5) taken at point i0 gives the desired formula
for p(0) because

−

(cid:19)

2k
L

1
(cid:18)

−

.

(45)

k+N −L

b(i)Ck

LCi−k
N −L

i=k
X
is obviously equal to WL(k). Thus, we have

p(0)(k) =

+

1
2

µL
2µ(N

1
(cid:18)

−

L)

−

2k
L

.

(cid:19)

N

−

−

<

1
2

k)/(N

k)/(N

−
k)/(N

L) is less than k/L,

Let us consider a very important point relating to
Eq. (45).
If the concentration of unities in the right-
hand part of the word (33) is higher than 1/2, k/L > 1/2,
then the most probable concentration (i0 −
L)
of unities in the left-hand part of this word is likewise
increased, (i0 −
L) > 1/2. At the same time, the
concentration (i0 −

−
i0 −
N
−
This implies that the increased concentration of unities
in the L-words is necessarily accompanied by the exis-
tence of a certain tail with an increased concentration of
unities as well. Such a phenomenon is referred by us as
the macro-persistence. An analysis performed in the fol-
lowing section will indicate that the correlation length lc
1 dependent on the parameter
of this tail is γN with γ
µ only. It is evident from the above-mentioned property
of the isotropy of the Markov chain that there are two
correlation tails from both sides of the L-word.

(48)

k
L

k
L

≥

<

.

Note that the distribution WL(k) is a smooth function
of arguments k and L near its maximum in the case of
weak persistence and k, L
1. By going over to the
k
−
≫
continuous limit in Eq. (43) and using Eq. (47) with the
relation p(1)(k
p(0)(k
1), we obtain the dif-
fusion Fokker-Planck equation for the correlated Markov
process,

1) = 1

−

−

−

∂W
∂L

=

1
8

∂2W
∂κ2 −

∂
∂κ

η(L)

(κW ),

(49)

where κ = k

L/2 and

−

η(L) =

.

(50)

2µ

(1

2µ)N + 2µL

−

Equation (49) has a solution of the Gaussian form
Eq. (36) with the variance D(L) satisfying the ordinary
diﬀerential equation,

dD
dL

1
4

=

+ 2η(L)D.

(51)

Its solution, given the boundary condition D(0) = 0,
coincides with (37).

4. Self-similarity of the persistent Brownian diﬀusion

In this subsection, we point to one of the most in-
teresting properties of the Markov chain being consid-
ered, namely, its self-similarity. Let us reduce the N -step
Markov sequence by regularly (or randomly) removing
some symbols and introduce the decimation parameter
λ,

λ = N ∗/N

1.

≤

(52)

Here N ∗ is a renormalized memory length for the reduced
N ∗-step Markov chain. According to Eq. (47), the con-
ditional probability p∗
k of occurring the symbol zero after
k unities among the preceding N ∗ symbols is described
by the formula,

(53)

(54)

p∗
k =

+ µ∗

1
2

2k
N ∗

1

−

,

(cid:19)

(cid:18)

with

µ∗ = µ

1

λ
2µ(1

.

λ)

−

−
The comparison between Eqs. (5) and (53) shows that the
reduced chain possesses the same statistical properties as
the initial one but it is characterized by the renormalized
parameters (N ∗, µ∗) instead of (N , µ). Thus, Eqs. (52)
and (54) determine the one-parametrical renormalization
of the parameters of the stochastic process deﬁned by
Eq. (5).

The astonishing property of the reduced sequence con-
sists in that the variance D∗(L) is invariant with respect
to the one-parametric decimation transformation (52),
(54). In other words, it coincides with the function D(L)
for the initial Markov chain:

D∗(L) =

[1 + m∗(L

1)] = D(L),

L < N ∗. (55)

L
4

−

−

2µ∗(N ∗

Indeed, according to Eqs. (52), (54), the renormalized
parameter m∗ = 2µ∗/[N ∗
1)] of the re-
duced sequence coincides exactly with the parameter
m = 2µ/[N
1)] of the initial Markov chain.
Since the shape of the function WL(k) Eq. (34) is de-
ﬁned by the invariant parameter n = n∗, the distribution
WL(k) is also invariant with respect to the decimation
transformation.

2µ(N

−

−

−

The transformation (N , µ)

→
(N ∗, µ∗) and (N ∗, µ∗)

(N ∗, µ∗) (52), (54) pos-
sesses the properties of semi-group, i. e., the composition
of transformations (N , µ)
→
→
(N ∗∗, µ∗∗) with transformation parameters λ1 and λ2 is
likewise the transformation from the same semi-group,
(N ∗∗, µ∗∗), with parameter λ = λ1λ2.
(N , µ)
The invariance of the function D(L) at L < N was
referred to by us as the phenomenon of self-similarity.
It is demonstrated in Fig. 3 and is accordingly discussed
below, in Sec. IV A.

→

7

It is interesting to note that the property of self-
similarity is valid for any strength of the persistency. In-
deed, the result Eq. (47) can be obtained directly from
Eqs. (15)-(17), and (44) not only for n
1 but also for
the arbitrary value of n.

≫

100

10

D

1

1

10

100

L

FIG. 3: The dependence of the variance D on the tuple length
L for the generated sequence with N = 100 and µ = 0.4
(solid line) and for the decimated sequences (the parameter
of decimation λ = 0.5). Squares and circles correspond to
the stochastic and deterministic reduction, respectively. The
thin solid line describes the non-correlated Brownian diﬀu-
sion, D(L) = L/4.

C. Long-range diﬀusion, L > N

♠

≤

Unfortunately, the very useful proposition

is valid
for the words of the length L
N only and is not ap-
plicable to the analysis of the long words with L > N .
Therefore, investigating the statistical properties of the
long words represents a rather challenging combinatorial
problem and requires new physical approaches for its sim-
pliﬁcation. Thus, we start this subsection by analyzing
the correlation properties of the long words (L > N ) in
the Markov chains with N
1. The two ﬁrst subsub-
≫
sections of this subsection mainly deal with the case of
relatively weak correlations, n

1.

≫

i

1. Correlation length at weak persistence

Let us rewrite Eq. (5) in the form,

< ai+1 >=

+ µ

1
2

2
N

 

< ak >

.

1

−

!

(56)

k=i−N +1
X

The angle brackets denote the averaging of the density of
unities in some region of the Markov chain for its deﬁnite
realization. The averaging is performed over distances
much greater than unity but far less than the memory

length N and correlation length lc (see Eq. (60) below).
Note that this averaging diﬀers from the statistical av-
eraging over the ensemble of realizations of the Markov
chain denoted by the bar in Eqs. (8) and (9). Equation
(56) is a relationship between the average densities of uni-
ties in two diﬀerent macroscopic regions of the Markov
chain, namely, in the vicinity of (i + 1)-th element and
in the region (i
N, i). Such an approach is similar to
the mean ﬁeld approximation in the theory of the phase
transitions and is asymptotically exact at N
. In
the continuous limit, Eq. (56) can be rewritten in the
integral form,

→ ∞

−

1
2

1
2

2
N

i

i−N

Z
It has the obvious solution,

(cid:18)

< a(i) >=

+ µ

< a(k) > dk

1

.

(57)

−

(cid:19)

< a(i)

>=< a(0)

> exp (

i/γN ) ,

(58)

−
where the parameter γ is determined by the relation,

−

−

1
2

γ

exp

1
γ

1

=

−

1
2µ

.

(59)

(cid:19)
A unique solution γ of the last equation is an increasing
function of µ

(0, 1/2).

(cid:19)

(cid:18)

(cid:18)

Formula (58) shows that any ﬂuctuation (the diﬀerence
between < a(i) > and the equilibrium value of ai = 1/2)
is exponentially damped at distances of the order of the
correlation length lc,

∈

lc = γN.

(60)

Law (58) describes the phenomenon of the persistent
macroscopic correlations discussed in the previous sub-
section. This phenomenon is governed by both parame-
ters, N and µ. According to Eqs. (59), (60), the corre-
lation length lc grows as γ = 1/4δ with an increase in µ
1/N is satisﬁed.
(at µ
Here

1/2) until the inequality δ

→

≫

(61)

K

δ = 1/2

µ.

−

≪

≫

N/4. At δ

1/N deﬁning the
Let us note that the inequality δ
regime of weak persistence can be rewritten in terms of
γ, γ
1/N , the correlation length lc
achieves its maximum value N 2/4. With the following
increase of µ, the diﬀusion goes to the regime of strongly
correlated diﬀusion that will be discussed in Subsubsec 3
of this Subsection.

≈

At µ

0, the macro-persistence is broken and the

correlation length tends to zero.

→

2. Correlation function at weak persistence

Using the studied correlation properties of the Markov
sequence and some heuristic reasons, one can obtain the
correlation function

(r) being deﬁned as,

K
(r) = aiai+r

K

ai

2,

−

8

(63)

(r),

K

1
8

,

and then the variance D(L). Comparing Eq. (62) with
Eqs. (7), (8) and taking into account the property of
sequence, ai = 1/2, it is easy to derive the general rela-
tionship between functions

(r) and D(L),

D(L) =

+ 4

L2
4

K

L−1

L−i

(r).

K

i=1
X

r=1
X

Considering (63) as an equation with respect to
one can ﬁnd its solution,

(1) =

D(2)

(2) =

D(3)

D(2) +

1
2

1
4

,

−

K

1
2

−

K

(r) =

[D(r + 1)

2D(r) + D(r

1)] ,

r

3. (64)

−

−

≥

K

1
2

This solution has a very simple form in the continuous
limit,

(r) =

K

1
2

d2D(r)
dr2

.

(65)

Equations (64) and (40) give the correlation function

at r < N , n

1,

≫

with

(r) = Crm,

K

C1 = 1/2,

C2 = 1/8,

C3≤r≤N = 1/4,

and m determined by Eq. (41).
In the continuous ap-
proximation, the correlation function is described by the
formula,

(r) =

K

m
4

,

r

N.

≤

(66)

The independence of the correlation function of r at r <
N results from our choice of the conditional probability
in the simplest form (5). At r > N , the function
(r)
should decrease because of the loss of memory. Therefore,
using Eqs. (58) and (60), let us prolongate the correlator
(r) as the exponentially decreasing function at r > N ,

K

(r) =

K

1,
exp

m
4 (

r−N
lc

−

r

N,
≤
, r > N.

(67)

(cid:16)
The lower curve in Fig. 4 presents the plot of the corre-
lation function at µ = 0.1.

(cid:17)

According to Eqs. (65), (67), the variance D(L) can be

D(L) =

(1 + mF (L))

(68)

L
4

written as

with

(62)

F (L) = 




L,
2(1 + γ)N
2γ2 N 2

L

−

(1 + 2γ) N 2
exp

L
L−N
lc

−

(cid:16)

−
1

−

h

(cid:17)i

L < N,

, L > N.

(69)

9

This phenomenon can be interpreted as a result of the
diﬀusion in which every independent step
D(L) of
wandering represents a path traversed by a particle dur-
ing the characteristic “ﬂuctuating time” L
(N + lc).
Since these steps of wandering are quasi-independent, the
distribution function WL(k) is the Gaussian. Thus, in the
1, WL(k) is the
case of relatively weak persistence, n
Gaussian not only at L < N (see Eq. (36)) but also for
L > N, lc.

≫

p

∼

∼

Note that the above-mentioned property of the self-
similarity is valid only at the portion L < N of the
curve D(L). Since the decimation procedure leads to
the decrease of the parameter µ (see Eq. (54)), the plot
N ∗
of asymptotics (70) for the reduced sequence at L
goes below the D(L) plot for the initial chain.

≫

3. Statistics of the L-words for the case of strong
persistence, n ≪ ln−1 N

In this subsection, we study the statistical properties
of long words (L > N ) in the sequences of symbols with
strong correlations. It is convenient to rewrite formula
(5) for the conditional probability of occurring the sym-
bol zero after the N -word containing k unities in the
form,

pν = δ + 2µ

ν
N

,

(71)

where ν is the number of zeros in the precedent N -word,
ν = N

k.

In the case of strong persistence, n

ln−1 N , the pa-
µ is much smaller than 1/N . Therefore,

≪

rameter δ = 1/2
the probability pν can be written as

−

−

pν

≈ (

δ,
ν/N,
δ,
1

−

ν = 0,
ν
ν = N.

= 0, ν

= N,

(72)

It is seen that the probability of occurring the symbol
zero after the N -word which contains only unities (ν = 0)
represents very small value δ and it increases signiﬁcantly
if ν
= 0. This situation diﬀers drastically from the case
1, the parameter δ exceeds
of weak persistency. At n
≫
noticeably the value 1/N , and the probability pν does
not actually change with an increase in the number of
zeros in the preceding N -word.

The analysis of the symbol generation process in the
Markov chain in the case of strong persistence gives the
following picture of the ﬂuctuations. There exist the en-
tire portions of the chain consisting of the same symbols,
say unities. The characteristic length of such portions is
N . These portions are separated by one or more
1/δ
symbols zero. The number of such packets of the same
symbols in one ﬂuctuation zone is about N . Thus, the
characteristic correlation distance at which the N -word
containing the same symbols converts into the N -word
with ν = N/2 is about N/δ,

≫

lc

≈

N
δ

.

(73)

FIG. 4: The dependence of the correlation function K on the
distance r between the symbols for the sequence with N = 20.
The dots correspond to the generated sequence with µ = 0.1
and µ = 50/101. The lower line is analytical result (67) with
lc = γN and γ = 0.38.

5

4

3

2

1

0

10

10

10

D

10

10

10

0

1

2

3

4

10

10

10

10

10

L

FIG. 5: The numerical simulation of the dependence D(L) for
the generated sequence with N = 100 and µ = 0.4 (circles).
The solid line is the plot of function Eq. (68) with the same
values of N and µ.

As an illustration of the result Eq. (68), we present the
plot of D(L) for N = 100 and µ = 0.4 by the solid line in
Fig. 5. The straight line in the ﬁgure corresponds to the
dependence D(L) = L/4 for the usual Brownian diﬀusion
without correlations (for µ = 0). It is clearly seen that
the plot of variance (68) contains two qualitatively diﬀer-
ent portions. One of them, at L <
N , is the super-linear
∼
curve that moves away from the line D = L/4 with an
increase of L as a result of the persistence. For L
N ,
the curve D(L) achieves the linear asymptotics,

≫

D(L) ∼=

1 +

L
4

(cid:18)

4µ(1 + γ)

1

2µ

−

.

(cid:19)

(70)

6
6
6
10

The described structure of the ﬂuctuations deﬁnes the
statical properties of the L-words with L > N in the
case of strong persistence. The distribution function dif-
fers signiﬁcantly from the Gaussian and is characterized
by a concave form at L <
N/δ. As L increases, the
lc
∼
correlations between diﬀerent parts of the L-words get
weaker and the L-word can be considered as consisting
of a number of independent sub-words. So, according to
the general mathematical theorems [12, 17], the distri-
bution function takes on the usual Gaussian form. Such
an evolution of the distribution function is depicted in
Fig. 6.

∼

D

10

10

10

10

10

10

10

7

6

5

4

3

2

1

0

10

-1

10

6

4

2

0

)
L
/
k
(
w

15

L=2

L=2048

L=1024

L=8

k/L

0,0

0,2

0,4

0,6

0,8

1,0

FIG. 6: The distribution function w(k/L) = LWL(k) for N =8
and δ = 1/150. Diﬀerent values of the length L of words is
shown near the curves.

The variance D(L) follows the quadratic law D = L2/4
(see Eq. (42)) up to the range of L <
N/δ and
∼
then approaches to the asymptotics D(L) = BL with
B

N/4δ (see Fig. 7).

∼
The upper curve in Fig. 4 presents the correlation
function for the case of strong persistence (µ = 50/101,
N = 20).

∼

lc

IV. RESULTS OF NUMERICAL SIMULATIONS
AND APPLICATIONS

In this section, we support the obtained analytical re-
sults by numerical simulations of the Markov chain with
the conditional probability Eq. (5). Besides, the proper-
ties of the studied binary N -step Markov chain are com-
pared with those for the natural objects, speciﬁcally for
the coarse-grained written and DNA texts.

A. Numerical simulations of the Markov chain

The ﬁrst stage of the construction of the N -step
Markov chain was a generation of the initial non-correla-
ted N symbols, zeros and unities, identically distributed

0

1

2

3

4

5

10

10

10

10

10

10

L

FIG. 7: The dependence of the variance D on the word length
L for the sequence with N = 20 and µ = 50/101 (solid line).
The thin solid line describes the non-correlated Brownian dif-
fusion, D(L) = L/4.

with equal probabilities 1/2. Each consequent symbol
was then added to the chain with the conditional prob-
ability determined by the previous N symbols in accor-
dance with Eq. (5). Then we numerically calculated the
variance D(L) by means of Eq. (8). The circles in Fig. 5
represent the calculated variance D(L) for the case of
weak persistence (n = 12.5
1). A very good agreement
≫
between the analytical result (68) and the numerical sim-
ulation can be observed. The case of strong persistence is
illustrated by Figs. 6 and 7 where the distribution func-
tion WL(k) and the variance D(L) are calculated numer-
ically for n = 4/37 and n = 0.1, respectively. The dots
on the curves in Fig. 4 represent the calculated results
for the correlation function
(r) for n = 0.1 (the upper
curve) and n = 40 (the lower curve).

K

♠

The numerical simulation was also used for the demon-
stration of the proposition
(Fig. 1) and the self-
similarity property of the Markov sequence (Fig. 3). The
squares in Fig. 3 represent the variance D(L) for the
sequence obtained by the stochastic decimation of the
initial Markov chain (solid line) where each symbol was
omitted with the probability 1/2. The circles in this ﬁg-
ure correspond to the regular reduction of the sequence
by removing each second symbol.

And ﬁnally, the numerical simulations have allowed us
to make sure that we are able to determine the parame-
ters N and µ of a given binary sequence. We generated
the Markov sequences with diﬀerent parameters N and µ
and deﬁned numerically the corresponding curves D(L).
Then we solved the inverse problem of the reconstruc-
tion of the parameters N and µ by analyzing the curves
D(L). The reconstructed parameters were always in good
agreement with their prescribed values. In the following
subsections we apply this ability to the treatment of the
statistical properties of literary and DNA texts.

11

10

10

10

10

7

6

5

4

3

2

1

0

D

10

10

10

10

-1

10

B. Literary texts

It is well-known that the statistical properties of the
coarse-grained texts written in any language exhibit a
remarkable deviation from random sequences [4, 18]. In
order to check the applicability of the theory of the binary
N -step Markov chains to literary texts we resorted to the
procedure of coarse graining by the random mapping of
all characters of the text onto the binary set of symbols,
zeros and unities. The statistical properties of the coarse-
grained texts depend, but not signiﬁcantly, on the kind
of mapping. This is illustrated by the curves in Fig. 8
where the variance D(L) for ﬁve diﬀerent kinds of the
mapping of Bible is presented. In general, the random
mapping leads to nonequal numbers of unities and zeros,
k1 and k0, in the coarse-grained sequence. A particular
analysis indicates that the variance D(L) (37) gets the
additional multiplier,

4k0k1
(k0 + k1)2 ,
in this biased case. In order to derive the function D(L)
for the non-biased sequence, we divided the numerically
calculated value of the variance by this multiplier.

10

10

10

10

7

6

5

4

3

2

1

0

D

10

10

10

10

-1

10

1

2

3

4

5

10

10

10

10

10

L

FIG. 8: The dependence D(L) for the coarse-grained text
of Bible obtained by means of ﬁve diﬀerent kinds of random
mapping.

The study of diﬀerent written texts has suggested that
all of them are featured by the pronounced persistent cor-
relations. It is demonstrated by Fig. 9 where the ﬁve vari-
ance curves go signiﬁcantly higher than the straight line
D = L/4. However, it should be emphasized that regard-
less of the kind of mapping the initial portions, L < 80, of
the curves correspond to a slight anti-persistent behavior
(see insert to Fig. 10). Moreover, for some inappropriate
kinds of mapping (e.g., when all vowels are mapped onto
the same symbol) the anti-persistent portions can reach
the values of L
1000. To avoid this problem, all the
curves in Fig. 9 are obtained for the deﬁnite representa-
0; (n-z)
tive mapping: (a-m)

1.

∼

→

→

1

2

3

4

5

10

10

10

10

10

L

FIG. 9: The dependence D(L) for the coarse-grained texts of
collection of works on the computer science (m = 2.2 · 10−3,
−3, dashed line), Bible
solid line), Bible in Russian (m = 1.9·10
−3, dotted line), “History of Russians
in English (m = 1.5 · 10
−4,
in the 20-th Century” by Oleg Platonov (m = 6.4 · 10
dash-dotted line), and “Alice’s Adventures in Wonderland”
by Lewis Carroll (m = 2.7 · 10

−4, dash-dot-dotted line).

→

→
books.

Thus, the persistence is the common property of the
binary N -step Markov chains that have been considered
in this paper and the coarse-grained written texts at large
scales. Moreover, the written texts as well as the Markov
sequences possess the property of the self-similarity. In-
deed, the curves in Fig. 10 obtained from the text of
Bible with diﬀerent levels of the deterministic decima-
tion demonstrate the self-similarity. Presumably, this
property is the mathematical reﬂection of the well-known
words
hierarchy in the linguistics: letters

syllables

→

→

→

M

chapters

paragraphs

sentences
→
All the above-mentioned circumstances allow us to
suppose that our theory of the binary N -step Markov
chains can be applied to the description of the statistical
properties of the texts of natural languages. However, in
contrast to the generated Markov sequence (see Fig. 5)
of the chain is far greater than
where the full length
the memory length N , the coarse-grained texts described
by Fig. 9 are of relatively short length
N . In other
words, the coarse-grained texts are similar not to the
Markov chains but rather to some non-stationary short
fragments. This implies that each of the written texts is
correlated throughout the whole of its length. Therefore,
as fae as the written texts are concerned, it is impossible
to observe the second portion of the curve D(L) parallel
(in the log-log scale) to the line D(L) = L/4, similar to
that shown in Fig. 5. As a result, one cannot deﬁne the
values of both parameters N and µ for the coarse-grained
texts. The analysis of the curves in Fig. 6 can give the
2µ) only (see Eq. (37)). Per-
combination m = 2µ/N (1
haps, this particular combination is the real parameter
governing the persistent properties of the literary texts.
We would like to note that the origin of the long-range

<
∼

M

−

D

10(cid:13)2(cid:13)

20(cid:13)

40(cid:13)

60(cid:13)

80(cid:13)

100(cid:13)

L(cid:13)

10(cid:13)5(cid:13)

10(cid:13)4(cid:13)

D

10(cid:13)3(cid:13)

25(cid:13)

20(cid:13)

15(cid:13)

10(cid:13)

5(cid:13)

10(cid:13)1(cid:13)

10(cid:13)0(cid:13)

10(cid:13)-1(cid:13)

10(cid:13)0(cid:13)

10(cid:13)1(cid:13)

10(cid:13)2(cid:13)

10(cid:13)3(cid:13)

10(cid:13)4(cid:13)

L(cid:13)

FIG. 10: The dependence of the variance D on the tuple
length L for the coarse-grained text of Bible (solid line)
and for the decimated sequences with diﬀerent parameters
λ: λ = 3/4 (squares), λ = 1/2 (stars), and λ = 1/256 (trian-
gles). The insert demonstrates the anti-persistent portion of
the D(L) plot for Bible.

≤

correlations in the literary texts is hardly related to the
grammatical rules as is claimed in Ref. [4]. At short scales
L
80 where the grammatical rules are in fact applica-
ble the character of correlations is anti-persistent (see
the insert in Fig. 11) whereas semantic correlations lead
to the global persistent behavior of the variance D(L)
throughout the entire of literary text.

The numerical estimations of the persistent parameter
m and the characterization of the languages and diﬀerent
authors using this parameter can be regarded as a new
intriguing problem of linguistics. For instance, the un-
precedented low value of m for the very inventive work
by Lewis Carroll as well as the closeness of m for the
texts of English and Russian versions of Bible are of cer-
tain interest.

It should be noted that there exist special kinds of
short-range correlated texts which can be speciﬁed by
both of the parameters, N and µ. For example, all dictio-
naries consist of the families of words where some prefer-
able letters are repeated more frequently than in their
other parts. Yet another example of the shortly corre-
lated texts is any lexicographically ordered list of words.
The analysis of written texts of this kind is given below.

C. Dictionaries

As an example, we have investigated the statisti-
cal properties of the coarse-grained alphabetical (lexi-
cographically ordered) list of the most frequently used
15462 English words. In contrast to other texts, the sta-
tistical properties of the coarse-grained dictionaries are
If one uses the
very sensitive to the kind of mapping.
1, the
above-mentioned mapping, (a-m)

0; (n-z)

→

→

12

behavior of the variance D(L) similar to that shown in
Fig. 9 would be obtained. The particular construction of
the dictionary manifests itself if the preferable letters in
the neighboring families of words are mapped onto the
diﬀerent symbols. The variance D(L) for the dictionary
coarse-grained by means of such mapping is shown by
It is clearly seen that the graph of
circles in Fig. 11.
the function D(L) consists of two portions similarly to
the curve in Fig. 5 obtained for the generated N -step
Markov sequence. The ﬁtting of the curve in Fig. 11 by
function (68) (solid line in Fig. 11) yielded the values of
the parameters N = 180 and µ = 0.44.

D(L)=L(1+0.04L)/4

D(L)=L(1+72)/4

10

10

10

D

10

10

6

5

4

3

2

1

0

10

10

-1

10

0

1

2

3

4

10

10

10

10

10

L

FIG. 11: The dependence D(L) for the coarse-grained alpha-
betical list of 15462 English words (circles). The solid line
is the plot of function Eq. (55) with the ﬁtting parameters
N = 180 and µ = 0.44.

D. DNA texts

It is known that any DNA text is written by four “char-
acters”, speciﬁcally by adenine (A), cytosine (C), gua-
nine (G), and thymine (T). Therefore, there are three
nonequivalent types of the DNA text mapping onto one-
dimensional binary sequences of zeros and unities. The
ﬁrst of them is the so-called purine-pyrimidine rule,
1. The second one is the hydrogen-
1. And, ﬁnally, the third

} →
0,

A,G

C,T

0,

{
} →
bond rule,
A,C
is

{
} →

{
A,T
0,

} →
G,T
{

{
} →

C,G
1.

} →

{
By way of example, the variance D(L) for the coar-
se-grained text of Bacillus subtilis, complete genome
(ftp://ftp.ncbi.nih.gov/genomes/bacteria/bacillus subti-
lis/NC 000964.gbk) is displayed in Fig. 12 for all possible
types of mapping. One can see that the persistent prop-
erties of DNA are more pronounced than for the writ-
ten texts and, contrary to the written texts, the D(L)
dependence for DNA does not exhibit the anti-persistent
behavior at small values of L. In our view, the noticeable
deviation of diﬀerent curves in Fig. 12 from each other
demonstrates that the DNA texts are much more com-

(cid:13)
(cid:13)
10(cid:13)9(cid:13)

10(cid:13)7(cid:13)

10(cid:13)5(cid:13)

10(cid:13)3(cid:13)

10(cid:13)1(cid:13)

10(cid:13)-1(cid:13)

D

10(cid:13)0(cid:13)

10(cid:13)1(cid:13)

10(cid:13)2(cid:13)

10(cid:13)4(cid:13)

10(cid:13)5(cid:13)

10(cid:13)6(cid:13)

10(cid:13)3(cid:13)

L(cid:13)

FIG. 12: The dependence D(L) for the coarse-grained DNA
text of Bacillus subtilis, complete genome, for three nonequiv-
alent kinds of mapping.
Solid, dashed, and dash-dotted
lines correspond to the mappings {A,G} → 0, {C,T} → 1
−2), {A,T} → 0, {C,G} → 1
(the parameter m = 4.1 · 10
−2),
(m = 2.5·10
respectively.

−2 ), and {A,C} → 0, {G,T} → 1 (m = 1.5·10

↔

↔

A,G

T and C

plex objects in comparison with the written ones. Indeed,
the diﬀerent kinds of mapping reveal and emphasize var-
ious types of physical attractive correlations between the
nucleotides in DNA, such as the strong purine-purine and
pyrimidine-pyrimidine persistent correlations (the upper
curve), and the correlations caused by a weaker attrac-
tion A

G (the middle curve).
It is interesting to compare the correlation properties
of the DNA texts for three diﬀerent species that belong
to the major domains of living organisms: the Bacte-
ria, the Archaea, and the Eukarya [19]. Figure 13 shows
the variance D(L) for the coarse-grained DNA texts of
Bacillus subtilis (the Bacteria), Methanosarcina acetivo-
ransthe (the Archaea), and Drosophila melanogaster -
fruit ﬂy - (the Eukarya) for the most representative map-
ping
1. It is seen that the D(L)
curve for the DNA text of Bacillus subtilis is character-
ized by the highest persistence. As well as for the written
texts, the D(L) curves for the DNA of both the Bacteria
and the Archaea do not contain the linear portions given
by Eq. (70). This suggests that their DNA chains are
not stationary sequences. In this connection, we would
like to point out that their DNA molecules are circular
and represent the collection of extended coding regions
interrupted by small non-coding regions. According to
Figs. 12, 13, the non-coding regions do not disrupts the
correlation between the coding areas, and the DNA sys-
tems of the Bacteria and the Archaea are fully correlated
throughout their entire lengths. Contrary to them, the
DNA molecules of the Eukarya have the linear structure
and contain long non-coding portions. As evident from
Fig. 13, the DNA sequence of the representative of the
Eukarya is not entirely correlated. The D(L) curve for

} →

} →

C,T

0,

{

{

13

0.35 and N

the X-chromosome of the fruit ﬂy corresponds qualita-
tively to Eqs. (68), (69) with µ
250.
If one draws an analogy between the DNA sequences
and the literary texts, the resemblance of the correlation
properties of integral literary novels and the DNA texts
of the Bacteria and Archaea are to be found, whereas
the DNA texts of the Eukarya are more similar to the
collections of 104–105 short stories.

≈

≈

10(cid:13)9(cid:13)

10(cid:13)7(cid:13)

10(cid:13)5(cid:13)

10(cid:13)3(cid:13)

10(cid:13)1(cid:13)

10(cid:13)-1(cid:13)

D

10(cid:13)0(cid:13)

10(cid:13)1(cid:13)

10(cid:13)2(cid:13)

10(cid:13)4(cid:13)

10(cid:13)5(cid:13)

10(cid:13)6(cid:13)

10(cid:13)3(cid:13)

L(cid:13)

FIG. 13: The dependence D(L) for the coarse-grained DNA
texts of Bacillus subtilis, complete genome, the Bacteria,
(solid line); Methanosarcina acetivorans, complete genome,
the Archaea, (dashed line); Drosophila melanogaster chromo-
some X, complete sequence, the Eukarya, (dotted line) for the
mapping {A,G} → 0, {C,T} → 1.

V. CONCLUSION

Thus, we have developed a new approach to describ-
ing the strongly correlated one-dimensional systems. The
simple, exactly solvable model of the uniform binary N -
step Markov chain is presented. The memory length N
and the parameter µ of the persistent correlations are two
(r)
parameters in our theory. The correlation function
is usually employed as the input characteristics for the
description of the correlated random systems. Yet, the
function
(r) describes not only the direct interconnec-
tion of the elements ai and ai+r, but also takes into ac-
count their indirect interaction via other elements. Since
our approach operates with the “original” parameters N
and µ, we believe that it allows us to reveal the intrinsic
properties of the system which provide the correlations
between the elements.

K

K

We have demonstrated the applicability of the devel-
oped theoretical model to the diﬀerent kinds of relatively
weakly correlated stochastic systems. Perhaps, the case
of strong persistency is also of interest from the stand-
point of possible applications. Indeed, the domain struc-
ture of the symbol ﬂuctuations at n
1 is very sim-
ilar to the domains in magnetics. Thus, an attempt to

≪

(cid:13)
(cid:13)
model the magnetic structures by the Markov chains with
strongly pronounced persistent properties can be appro-
priate.

We would like to note that there exist some features of
the real correlated systems which cannot be interpreted
in terms of our two-parametric model. For example, the
interference of the grammatical anti-persistent and se-
mantic persistent correlations in the literary texts re-
quires more than two parameters for their description.
Obviously, more complex models should be worked out
for the adequate interpretation of the statistical proper-
ties of the DNA texts and other real correlated systems.

In particular, the Markov chains consisting of more than
two diﬀerent elements (non-binary chains) can be suit-
able for modelling the DNA systems.

14

Acknowledgments

We acknowledge to Yu. L. Rybalko and A. L. Pat-
senker for the assistance in the numerical simulations,
M. E. Serbin and R. Zomorrody for the helpful discus-
sions.

[1] H. E. Stanley et. al., Physica A 224,302 (1996).
[2] A. Provata and Y. Almirantis, Physica A 247, 482

Russian).

[12] C. V. Nagaev, Theor. Probab. & Appl., 2, 389 (1957) (In

[3] R. N. Mantegna, H. E. Stanley, Nature (London) 376,

[4] I. Kanter and D. F. Kessler, Phys. Rev. Lett. 74, 4559

(1997).

46 (1995).

(1995).

[5] C. Tsalis, J. Stat. Phis. 52, 479 (1988).
[6] Nonextensive Statistical Mechanics and Its Applications,
eds. S. Abe and Yu. Okamoto (Springer, Berlin, 2001).

[7] S. Denisov, Phys. Lett. A, 235, 447 (1997).
[8] A. Czirok, R. N. Mantegna, S. Havlin, and H. E. Stanley,

Phys. Rev. E 52, 446 (1995).

[9] W. Li, Europhys. Let. 10, 395 (1989).
[10] R. F. Voss, in: Fundamental Algorithms in Computer
Graphics, ed. R.A. Earnshaw (Springer, Berlin, 1985) p.
805.

[11] M. F. Shlesinger, G. M. Zaslavsky, and J. Klafter, Nature

(London) 363, 31 (1993).

[13] M. I. Tribelsky, Phys. Rev. Lett. 87, 070201 (2002).
[14] O. V. Usatenko and V. A. Yampol’skii, Phys. Rev. Lett.

90, 110601 (2003).

[15] C. W. Gardiner: Handbook of Stochastic Methods for
Physics, Chemistry, and the Natural Sciences, (Springer
Series in Synergetics, Vol. 13) Springer-Verlag, 1985).

[16] A. Katok, B. Hasselblat:

Introduction to the Modern
Theory of Dynamical Systems, (Cambridge University
Press; Factorial, Moscow, 1999) p. 768.

[17] I. A. Ibragimov, Yu. V. Linnik: Independent and station-
ary connected variables, (Nauka, Moscow, 1965) p. 524.
[18] A. Schenkel, J. Zhang, and Y. C. Zhang, Fractals 1, 47

(1993).

[19] M. T. Madigan, J. M. Martinko, J. Parker: Brock Biology

of Microorganisms, (Prentice Hall, 2002) p. 1104.


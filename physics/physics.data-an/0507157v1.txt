Optimization of Signal Signiﬁcance by Bagging Decision Trees ∗

Ilya Narsky†

California Institute of Technology
(Dated: February 2, 2008)
Abstract
An algorithm for optimization of signal signiﬁcance or any other classiﬁcation ﬁgure of merit
suited for analysis of high energy physics (HEP) data is described. This algorithm trains decision
trees on many bootstrap replicas of training data with each tree required to optimize the signal
signiﬁcance or any other chosen ﬁgure of merit. New data are then classiﬁed by a simple majority
vote of the built trees. The performance of this algorithm has been studied using a search for the
γlν at BABAR and shown to be superior to that of all other attempted
radiative leptonic decay B
classiﬁers including such powerful methods as boosted decision trees. In the B
γeν channel, the
described algorithm increases the expected signal signiﬁcance from 2.4σ obtained by an original
method designed for the B

γlν analysis to 3.0σ.

→

→

PACS numbers: 02.50.Tt, 02.50.Sk, 02.60.Pn.

→

5
0
0
2
 
l
u
J
 
1
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
5
1
7
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗ Work partially supported by Department of Energy under Grant DE-FG03-92-ER40701.
†Electronic address: narsky@hep.caltech.edu

1

1.

INTRODUCTION

Separation of signal and background is perhaps the most important problem in analy-
sis of HEP data. Various pattern classiﬁcation tools have been employed in HEP practice
to solve this problem. Fisher discriminant [1] and feedforward backpropagation neural net-
works [2] are the two most popular methods chosen by HEP analysts at present. Alternative
algorithms for classiﬁcation such as decision trees [3], bump hunting [4], and AdaBoost [5]
have been recently explored by the HEP community as well [6, 7, 8]. These classiﬁers can
be characterized by such features as predictive power, interpretability, stability and ease
of training, CPU time required for training and classifying new events, and others. It is
important to remember that the choice of a classiﬁer for each problem should be driven by
speciﬁcs of the analysis. For example, if the major goal of pattern classiﬁcation is to achieve
a high quality of signal and background separation, ﬂexible classiﬁers such as AdaBoost and
neural nets should be the prime choice. While neural nets generally perform quite well in
low dimensions, they become too slow and unstable in high-dimensional problems losing the
competition to AdaBoost. If the analyst, however, is mostly concerned with a clear inter-
pretation of the classiﬁer output, decision trees and bump hunting algorithms are a more
appealing option. These classiﬁers produce rectangular regions, easy to visualize in many
dimensions.

One of the problems faced by HEP analysts is the indirect nature of available classiﬁers.
In HEP analysis, one typically wants to optimize a ﬁgure of merit expressed as a function
of signal and background, S and B, expected in the signal region. An example of such
ﬁgure of merit is signal signiﬁcance, S/√S + B, often used by physicists to express the
cleanliness of the signal in the presence of statistical ﬂuctuations of observed signal and
background. None of the available popular classiﬁers optimizes this ﬁgure of merit directly.
CART [3], a popular commercial implementation of decision trees, splits training data into
signal- and background-dominated rectangular regions using the Gini index, Q = 2p(1
p),
as the optimization criterion, where p is the correctly classiﬁed fraction of events in a tree
node. Neural networks typically minimize a quadratic classiﬁcation error,
−
f (xn))2, where yn is the true class of an event, -1 for background and 1 for signal, f (xn) is the
1, 1] predicted by the neural network, and
continuous value of the class label in the range [
the sum is taken over N events in the training data set. Similarly, AdaBoost minimizes an
ynf (xn)). These optimization criteria
exponential classiﬁcation error,
are not necessarily optimal for maximization of the signal signiﬁcance. The usual solution
is to build a neural net or an AdaBoost-based classiﬁer and then ﬁnd an optimal cut on the
continuous output of the classiﬁer to maximize the signal signiﬁcance. For decision trees,
the solution is to construct a decision tree with many terminal nodes and then combine
these nodes to maximize the signal signiﬁcance.

N
exp = P
n=1 exp(

N
n=1(yn
qua = P

−

−

−

E

E

This problem has been partially addressed in my C++ software package for pattern clas-
siﬁcation [8]. Default implementations of the decision tree and the bump hunting algorithm
include both standard ﬁgures of merit used for commercial decision trees such as the Gini
index and HEP-speciﬁc ﬁgures of merit such as the signal signiﬁcance or the signal pu-
rity, S/(S + B). The analyst can optimize an arbitrary ﬁgure of merit by providing an
implementation to the corresponding abstract interface set up in the package.

AdaBoost and the neural net, however, cannot be modiﬁed that easily. The functional
forms of the classiﬁcation error are intimately tied to implementations of these two classi-
ﬁcation algorithms. Finding a powerful method for optimization of HEP-speciﬁc ﬁgures of

2

merit is therefore an open question.

This note describes an algorithm that can be used for direct optimization of an arbitrary
ﬁgure of merit. Optimization of the signal signiﬁcance by this algorithm has shown results
comparable or better than those obtained with AdaBoost or the neural net. The training
time used by this algorithm is comparable to that used by AdaBoost with decision trees;
the algorithm is therefore faster than the neural net in high dimensions. The method has
been coded in C++ and included in the StatPatternRecognition package available for free
distribution to HEP analysts.

2. BAGGING DECISION TREES

The implementation of decision trees used for the proposed algorithm is described in detail
in Ref. [8]. The key feature of this implementation is its ability to optimize HEP-speciﬁc
ﬁgures of merit such as the signal signiﬁcance.

A decision tree, even if it directly optimizes the desired ﬁgure of merit, is rarely powerful
enough to achieve a good separation between signal and background. The tree produces
a set of signal-dominated rectangular regions. Rectangular regions, however, often fail to
capture a non-linear structure of data. The mediocre predictive power of a single decision
tree can be greatly enhanced by one of the two popular methods for combining classiﬁers —
boosting and bagging.

Both these methods work by training many classiﬁers, e.g., decision trees, on variants of
the original training data set. A boosting algorithm enhances weights of misclassiﬁed events
and reduces weights of correctly classiﬁed events and trains a new classiﬁer on the reweighted
sample. The output of the new classiﬁer is then used to re-evaluate fractions of correctly
classiﬁed and misclassiﬁed events and update the event weights accordingly. After training
is completed, events are classiﬁed by a weighted vote of the trained classiﬁers. AdaBoost, a
popular version of this approach, has been shown to produce a high-quality robust training
mechanism. Application of AdaBoost to HEP data has been explored in Refs. [7, 8].

In contrast, bagging algorithms [9] do not reweight events.

Instead, they train new
classiﬁers on bootstrap replicas of the training set. Each bootstrap replica [10] is obtained
by sampling with replacement from the original training set, with the size of each replica
equal to that of the original set. After training is completed, events are classiﬁed by the
majority vote of the trained classiﬁers. For successful application of the bagging algorithm,
the underlying classiﬁer must be sensitive to small changes in the training data. Otherwise
all trained classiﬁers will be similar, and the performance of the single classiﬁer will not be
improved. This condition is satisﬁed by a decision tree with ﬁne terminal nodes. Because
of the small node size each decision tree is signiﬁcantly overtrained; if the tree were used
just by itself, its predictive power on a test data set would be quite poor. However, because
the ﬁnal decision is made by the majority vote of all the trees, the algorithm delivers a high
predictive power.

Various kinds of boosting and bagging algorithms have been compared in the statistics
literature. Neither of these two approaches has a clear advantage over the other. On average,
boosting seems to provide a better predictive power. Bagging tends to perform better in the
presence of outliers and signiﬁcant noise [11].

For optimization of the signal signiﬁcance, however, bagging is the choice favored by
intuition. Reweighting events has an unclear impact on the eﬀectiveness of the optimization
routine with respect to the chosen ﬁgure of merit. While it may be possible to design

3

a reweighting algorithm eﬃcient for optimization of a speciﬁc ﬁgure of merit, at present
such reweighting algorithms are not known. Bagging, on the other hand, oﬀers an obvious
If the base classiﬁer directly optimizes the chosen ﬁgure of merit, bagging is
solution.
equivalent to optimization of this ﬁgure of merit integrated over bootstrap replicas.
In
eﬀect, the bagging algorithm ﬁnds a region in the space of physical variables that optimizes
the expected value of the chosen ﬁgure of merit — exactly what the analyst is looking for.
Bagging decision trees is certainly not a new item in the statistics research. The only
novelty introduced in this note is the decision tree designed for direct optimization of an
arbitrary ﬁgure of merit, e.g., the signal signiﬁcance.

coslg

ipi0

costheblg

leptonE

photonE

numLepton

Fisher

4000

acthrust

mES

deltaE

nuEP

5000

2500

5000

2500

5000

2500

5000

2500

5000

2500

0

0

0

0

0

6000
4000
2000
0

24000
16000
8000
0

5000

2500

0

130000
90000
50000
10000

10000

5000

0

0

4

FIG. 1: Separation variables for the B
γlν analysis. Signal MC is shown with a solid line
(triangles in the numLepton plot), and the overall combined background is shown with a dashed
line (squares in the numLepton plot).

→

Strain,

Svalid, and

Stest, for the B

γlν training, validation, and
TABLE I: Signal signiﬁcance,
test samples obtained with various classiﬁcation methods. The signal signiﬁcance computed for the
test sample should be used to judge the predictive power of the included classiﬁers. A branching
γeν decays. W1 and W0 represent the
fraction of 3
signal and background, respectively, expected in the signal region after the classiﬁcation criteria
have been applied; these two numbers have been estimated using the test samples. All numbers
have been normalized to the integrated luminosity of 210 fb−1. The best value of the expected
signal signiﬁcance is shown in boldface.

10−6 was assumed for both B

γµν and B

→

→

→

×

Method

B

γeν

B

γµν

Original method
Decision tree
Bump hunter with one bump
AdaBoost with binary splits
AdaBoost with decision trees

→
→
Strain Svalid Stest W1 W0
Strain Svalid Stest W1 W0
1.62 25.8 227.4
1.75
2.42 37.5 202.2
2.66
-
1.54 29.0 325.9
1.74
2.16 20.3 68.1
3.28
1.63
1.54 31.7 393.8
1.76
2.31 47.5 376.6
2.72
1.54
1.44 45.2 935.6
2.25 76.4 1077.3 1.66
2.53
1.71
1.75 41.6 523.0
2.62 58.0 432.8 11.87
13.63
1.97
1.66 55.2 1057.1
2.49 83.2 1037.2 1.84
Combiner of background subclassiﬁers 3.03
1.90
2.07 1.98 49.4 571.1
8.09
9.20

-
2.72
2.54
2.65
2.99
2.88
3.25 2.99 69.1 465.8

Bagging decision trees

signal
background

105

104

103

102

101

1

-100

4

3

2

1

0

-100

-50

0
classifier output

50

100

-50

0
Signal significance

50

100

FIG. 2: Output of the bagging algorithm with 100 trained decision trees (left) and the signal
signiﬁcance versus the lower cut on the output (right) for the B
γeν test sample. The cut
maximizing the signal signiﬁcance, obtained using the validation sample, is shown with a vertical
line.

→

5

→

Performance of the described bagging algorithm has been studied using a search for the
γlν at BABAR. Eleven variables used for classiﬁcation in
radiative leptonic decay B
this analysis are shown in Fig. 1. Several methods have been used to separate signal from
background by maximizing the signal signiﬁcance: an original method developed by the
analysts, the decision tree optimizing the signal signiﬁcance, the bump hunting algorithm,
AdaBoost with binary splits, AdaBoost with decision trees optimizing the Gini index, and
an AdaBoost-based combiner of background subclassiﬁers. I also attempted to use a feedfor-
ward backpropagation neural network with one hidden layer, but the network was unstable
and it failed to converge to an optimum. A more detailed description of this analysis and
used classiﬁers can be found in Ref. [8].

To test the bagging algorithm described in this note, I trained 100 decision trees on
bootstrap replicas of the training data. For classiﬁcation of new data, the trained trees were
combined using an algebraic sum of their outputs: if an event was accepted by a tree, the
output for this event was incremented by 1 and decremented by 1 otherwise. The minimal
γµν channels,
size of the terminal node in each tree, 100 events for both B
was chosen by comparing values of the signal signiﬁcance computed for the validation data.
The size of the trained decision trees varied from 390 to 470 terminal signal nodes in the
B
γµν channel. Jobs executing the
algorithm took several hours in a batch queue at SLAC. To assess the true performance of
the method, the signal signiﬁcance was then evaluated for the test data.

γeν channel and from 300 to 370 in the B

γeν and B

→

→

→

→

→

All attempted classiﬁers are compared in Table I. The output of the described bagging
algorithm for the B
γeν test data is shown in Fig. 2. The bagging algorithm provides the
best value of the signal signiﬁcance. It gives a 24% improvement over the original method
developed by the analysts and shown in the ﬁrst line of Table I, and a 14% improvement
over AdaBoost with decision trees shown in line 5 of Table I; both numbers are quoted for
the B

γeν channel.

→

I also used AdaBoost with decision trees optimizing the signal signiﬁcance and the bag-
ging algorithm with decision trees optimizing the Gini index. The ﬁrst method performed
quite poorly; the signal signiﬁcance obtained with this method was much worse than that
obtained by AdaBoost with decision trees optimizing the Gini index. The bagging algorithm
with decision trees optimizing the Gini index showed an 8% improvement in the B
γeν
signal signiﬁcance compared to AdaBoost with decision trees optimizing the Gini index.
But the signal signiﬁcance obtained with this method was 9% worse than that obtained by
the bagging algorithm with decision trees optimizing the signal signiﬁcance. The 14% im-
provement of the proposed bagging algorithm over AdaBoost with decision trees originated
therefore from two sources:

→

Using bagging instead of boosting.

•

•

Using the signal signiﬁcance instead of the Gini index as the ﬁgure of merit for the
decision tree optimization.

In an attempt to improve the signal signiﬁcance even further, I used the random forest
approach [12], a more generic resampling method. In addition to generating a new bootstrap
replica for each tree, I resampled the data variables used to split each node of the tree.
Because a bootstrap replica contains on average 63% of distinct entries from the original
set, only 6.9 variables out of 11 were used on average to split the tree nodes. This approach
showed only a minor 1% improvement in the B
γeν signal signiﬁcance over the bagging
algorithm without variable resampling.

→

6

As shown in Fig. 2, the described bagging algorithm does not provide a good separation
between signal and background in terms of the quadratic or exponential classiﬁcation error.
It misclassiﬁes a large fraction of signal events. However, the method does the job it was
expected to do — it ﬁnds a region in the space of physical variables that, on average,
maximizes the signal signiﬁcance.

3. SUMMARY

A bagging algorithm suitable for optimization of an arbitrary ﬁgure of merit has been
described. This algorithm has been shown to give a signiﬁcant improvement of the signal
γlν at BABAR. Included in
signiﬁcance in the search for the radiative leptonic decay B
the StatPatternRecognition package [8], this method is available to HEP analysts.

→

Acknowledgments

Thanks to Frank Porter for comments on a draft of this note.

[1] R.A. Fisher, The use of multiple measurements in taxonomic problems, Annals of Eugenics 7,

179-188 (1936).

[2] W.S. McCulloch and W. Pitts, A logical calculus of the ideas immanent in nervous activity,
Bulletin of Mathematical Biophysics 5, 115-133 (1943); F. Rosenblatt, The Perceptron: A
probabilistic model for information storage and organization in the brain, Psychological Re-
view 65, 386-408 (1958); J.J. Hopﬁeld, Neural networks and physical systems with emergent
collective computational abilities, Proceedings of the National Academy of Sciences, USA, 79,
2554-2558 (1982); D.E. Rumelhart et al., Learning internal representation by error propa-
gation, Parallel Distributed Processing: Explorations in the Microstructure of Cognition 1,
318-362 (1986); J.A. Anderson and E. Rosenfeld, Neurocomputing. Foundations of Research,
MIT Press, 1988.

[3] L. Breiman et al., Classiﬁcation and Regression Trees, Waldsworth International, 1984.
[4] J. Friedman and N. Fisher, Bump hunting in high dimensional data, Statistics and Computing

9, 123-143 (1999).

[5] Y. Freund and R.E. Schapire, A decision-theoretic generalization of on-line learning and an
application to boosting, J. of Computer and System Sciences 55, 119-139 (1997); L. Breiman,
Arcing classiﬁers, The Annals of Statistics 26, 801-849 (1998); R.E. Schapire et al., Boosting
the margin: A new explanation for the eﬀectiveness of voting methods, The Annals of Statistics
26, 1651-1686 (1998).

[6] D. Bowser-Chao and D.L. Dzialo, A Comparison of Binary Decision Trees and Neural Net-
works in Top Quark Detection, Phys. Rev. D47, 1900-1905 (1993); M. Mjahed, Multivariate
Decision Tree Designing for the Classiﬁcation of Multi-Jet Topologies in e+e− Collisions, Nucl.
Instrum. and Meth. A481, 601-614 (2002); R. Quiller, Decision Tree Technique for Particle
Identiﬁcation, SLAC-TN-03-019, 2003.

[7] B.P. Roe et al., Boosted Decision Trees, an Alternative to Artiﬁcial Neural Networks,

physics/0408124, 2004.

7

[8] I. Narsky, StatPatternRecognition: A C++ Package for Statistical Analysis of High Energy

Physics Data, physics/0507143, 2005.

[9] L. Breiman, Bagging Predictors, Machine Learning 26, 123-140 (1996); L. Lam and C.Y. Suen,
Application of majority vote to pattern recognition: An analysis of its behavior and perfor-
mance, IEEE Transactions on Systems, Man, and Cybernetics 27, 553-568 (1997).

[10] B. Efron and R.J. Tibshirani, An Introduction to the Bootstrap, Chapman & Hall/CRC, 1993.
[11] E. Bauer and R. Kohavi, An empirical comparison of voting classiﬁcation algorithms: Bagging,

boosting, and variants, Machine Learning 36, 105-142 (1999).
[12] L. Breiman, Random Forests, Machine Learning 45, 5-32 (2001).

8


PHYSTAT 2003, SLAC, Stanford, CA, September 8-11, 2003

1

Event Selection Using an Extended Fisher Discriminant Method

Byron P. Roe
University of Michigan, Ann Arbor, Michigan 48109, USA

This note discusses the problem of choosing between hypotheses in a situation with many, corre-
lated non-normal variables. A new method is introduced to shrink the many variables into a smaller
subset of variables with zero mean, unit variance, and zero correlation coeﬃcient between variables.
These new variables are well suited to use in a neural net.

Indeed, it is likely that the optimum choice depends
on the problem.

The method is also used with the original denom-
inator, [varsig + varbkg] and then with each individ-
ual variance in turn. This is done since some of the
variables may be quite narrow for signal and wide for
background or vice versa. (See Figure 1) There are
then 4 × 3 = 12 variables obtained with this method.

3
0
0
2
 
t
c
O
 
9
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
4
1
0
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

I.

INTRODUCTION

At the Durham Statistics in Physics Conference
(2002), S. Towers[1] noted some of the problems that
occur when one uses many, correlated variables in a
multivariate analysis and proposed a heuristic method
to shrink the number. In this note a semi-automatic
method is suggested help with this problem. The
MiniBooNE experiment is faced with just such a prob-
lem, distinguishing νe events from background events
given a large number of variables obtained from the
event reconstructions.

II. FISHER DISCRIMINANT METHOD AND
ITS EXTENSION

The Fisher discriminant method is a standard
method for obtaining a single variable to distinguish
hypotheses starting from a large number of variables.
If the initial variables come from a multi-normal dis-
tribution, the Fisher variable encapsulates all of the
discrimination information. However, in many prob-
lems the variables are not of this form and the Fisher
variable, although useful, is not suﬃcient.

The Fisher discriminant method [2] ﬁnds the linear
combination y of the initial variables x which maxi-
mizes

(ysig − ybkg)2/[varsig + varbkg],
where y is the mean value of the variable and var is
the variance.
If S is the correlation matrix for the
original variables corresponding to the denominator,
then the inverse of S dotted into (xsig − xbkg), gives
the combination which maximizes the preceding ex-
pression.

If the distribution is not multi-normal, there is in-
formation still to be obtained after ﬁnding the Fisher
variable. It is then useful to apply this method succes-
sively, ﬁrstly to the original variables and, afterwards,
to several non-linear transformations of the variables.
Presently, three transformations are chosen: the log-
arithms of the original variables, the exponentials of
the original variables and the cube of the original vari-
ables. Together with the original variables, this is
then four choices. The present note describes a work
in progress. It is highly likely that these are not op-
timum and that better choices can and will be found.

WEJT003

FIG. 1: Figure 1: Diﬀerent Width Normal Distributions

The procedure follows the following steps:

• 1. Start with equal Monte Carlo samples of sig-

nal and background events

• 2. Multiply and translate each variable to have
an overall mean of zero and unit variance. It is
useful to fold variables if necessary to maximize
the diﬀerence in means. A few events, very far
out on the tails of the distribution are clipped
(x > 6σ).

• 3. Order the variables according to |xsig − xbkg|
divided by the smallest of the signal and back-
ground variances. At present, the ordering of
variables is done only once.

• 4. Apply this extended Fisher method to the
appropriate transformation of the variables.

• 5. Use the Gram-Schmidt procedure to make
the other variables have zero correlation coeﬃ-
cient with the chosen linear combination.

2

PHYSTAT 2003, SLAC, Stanford, CA, September 8-11, 2003

• 6. The new variable is a linear combination of
the original n variables. One variable must be
discrded to have an independent set. Discard
the least signiﬁcant (by the criterion of step 3)
of the original variables. Using the n − 1 non-
Fisher variables, go back to step 2 to get the
next variable.

and reduced the π0/νe ratio to 1.1% of its original
value. The neural net was not hard to tune. The
reconstruction–particle identiﬁcation package is still
being improved, so these numbers will improve fur-
ther. The results obtained here are similar to those
obtained using a more elaborate neural net on a sub-
sample of 26 of the original 49 variables.

For MiniBooNE the roeﬁtter reconstruction started
with 49 particle identiﬁcation variables. Using the
steps outlined, these were reduced to 12 variables.
When νe quasi-elastic events were compared with
background neutral current π0 events, the use of
this procedure with a neural net kept 46% of the νe

These 12 variables have zero correlation coeﬃcients.
Use of the neural net is simpliﬁed and it is convenient
to look at the eﬀect of cuts using these variables.

Plots of the ﬁrst nine of the twelve variables are

shown in Figure 2.

[1] S. Tower, Beneﬁts of Minimizing the Number of Dis-
criminators Used in a Multivariate Analysis, Durham
Conference on Statistics (2002).

[2] Glen Cowan Statistical Data Analysis, Clarendon

Press, Oxford (1998).

WEJT003

PHYSTAT 2003, SLAC, Stanford, CA, September 8-11, 2003

3

FIG. 2: Figure 2: Plots of the ﬁrst nine of the variables obtained. The solid lines are the νe signal and the dashed lines
are the π0 background.

WEJT003


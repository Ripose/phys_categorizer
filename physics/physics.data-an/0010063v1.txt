0
0
0
2

 
t
c
O
5
2

 

 
 
]
n
a
-
a
t
a
d

.
s
c
i
s
y
h
p
[
 
 

1
v
3
6
0
0
1
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Optimal Recovery of Local Truth

Carlos C. Rodr´ıguez

Department of Mathematics and Statistics

University at Albany, SUNY

Albany NY 12222, USA
carlos@math.albany.edu

http://omega.albany.edu:8008/

CONTENTS

Contents

1 Introduction

1.1 Nonparametrics with the World in Mind . . . . . . . . . . . .

2 Estimating Densities from Data

2.1 The knn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 The kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Double Smoothing Estimators . . . . . . . . . . . . . . . . . .

3 The Truth as n → ∞ ?

3.1 The Natural Invariant Loss Function and Why the MSE is not

that Bad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1
2

3
3
4
5

6

6

4 Some Classic Asymptotic Results

9
4.1 Asymptotic Mean Square Errors . . . . . . . . . . . . . . . . . 12

5 Choosing the Optimal Norm

14
5.1 Generalized knn Case with Uniform Kernel . . . . . . . . . . . 17
5.2 Yet Another Proof When The Hessian is Deﬁnite
. . . . . . . 22
5.3 Best Norm With General Kernels . . . . . . . . . . . . . . . . 24

6 Asymptotic Relative Eﬃciencies

7 An Example: Radially Symmetric Distributions

8 Conclusions

9 Acknowledgments

26

27

30

31

Abstract

Probability mass curves the data space with horizons!. Let f be a mul-
tivariate probability density function with continuous second order partial
derivatives. Consider the problem of estimating the true value of f (z) > 0
at a single point z, from n independent observations.
It is shown that,
the fastest possible estimators (like the k-nearest neighbor and kernel) have
minimum asymptotic mean square errors when the space of observations is
thought as conformally curved. The optimal metric is shown to be generated
by the Hessian of f in the regions where the Hessian is deﬁnite. Thus, the
peaks and valleys of f are surrounded by singular horizons when the Hessian
changes signature from Riemannian to pseudo-Riemannian. Adaptive esti-
mators based on the optimal variable metric show considerable theoretical
and practical improvements over traditional methods. The formulas simplify
dramatically when the dimension of the data space is 4. The similarities
with General Relativity are striking but possibly illusory at this point. How-
ever, these results suggest that nonparametric density estimation may have
something new to say about current physical theory.

1 INTRODUCTION

1 Introduction

1

During the past thirty years the theory of Nonparametrics has been domi-
nating the scene in mathematical statistics. Parallel to the accelerating dis-
covery of new technical results, a consensus has been growing among some
researchers in the area, that we may be witnessing a promising solid road
towards the elusive Universal Learning Machine (see e.g. [1, 2]).

The queen of nonparametrics is density estimation. All the fundamen-
tal ideas for solving the new problems of statistical estimation in functional
spaces (smoothing, generalization, optimal minimax rates, etc.) already ap-
pear in the problem of estimating the probability density (i.e. the model)
from the observed data. More over, it is now well known that a solution
for the density estimation problem automatically implies solutions for the
problems of pattern recognition and nonparametric regression as well as for
most problems that can be expressed as a functional of the density.

In this paper I present a technical result, about optimal nonparametric
density estimation, that shows at least at a formal level, a surprising simi-
larity between nonparametrics and General Relativity. Simply put,

probability mass curves the data space with horizons.

What exactly it is meant by this is the subject of this paper but before
proceeding further a few comments are in order. First of all, let us assume
that we have a set {x1, . . . , xn} of data. Each observation xj consisting of
p measurements that are thought as the p coordinates of a vector in IRp.
To make the data space into a probability space we endow IRp with the
ﬁeld of Borelians but nothing beyond that. In particular no a priori metric
structure on the data space is assumed. The n observations are assumed to
be n independent realizations of a given probability measure P on IRp. By
the Lebesgue decomposition theorem, for every Borel set B we can write,

P (B) =ZB

f (x)λ(dx) + ν(B)

(1)

where ν is the singular part of P that assigns positive probability mass to
Borel sets of zero Lebesgue volume. Due to the existence of pathologies like
the Cantor set in one dimension and its analogies in higher dimensions, the
singular part ν cannot be empirically estimated (see e.g. [3]). Practically all
of the papers on density estimation rule out the singular part of P a priori.
The elimination of singularities by ﬁat has permitted the construction of a

1 INTRODUCTION

2

rich mathematical theory for density estimation, but it has also ruled out a
priori the study of models of mixed dimensionality (see [4]) that may be nec-
essary for understanding point masses and spacetime singularities coexisting
with absolutely continuous distributions.

We assume further that in the regions where f (x) > 0 the density f is of

class C2 i.e., it has continuous second order partial derivatives.

1.1 Nonparametrics with the World in Mind

The road from Classical Newtonian Physics to the physics of today can be
seen as a path paved by an increasing use of fundamental concepts that
are statistical in nature. This is obvious for statistical mechanics, becoming
clearer for quantum theory, and appearing almost as a shock in General
Relativity. Not surprisingly there have been several attempts to take this
trend further (see e.g. [5, 6, 7, 8]) in the direction of Physics as Inference.

Now suppose for a moment that in fact some kind of restatement of the
foundations of physics in terms of information and statistical inference will
eventually end up providing a way to advance our current understanding of
nature. As of today, that is either already a solid fact or remains a wild spec-
ulation, depending on who you ask. In any case, for the trend to take over, it
will have to be able to reproduce all the successes of current science and make
new correct predictions. In particular it would have to reproduce General
Relativity. Recall that the main lesson of General Relativity is that space
and time are not just a passive stage on top of which the universe evolves.
General Relativity is the theory that tells (through the ﬁeld equation) how to
build the stage (left hand side of the equation) from the data (right hand side
of the equation). The statistical theory that tells how to build the stage of
inference (the probabilistic model) from the observed data is: Nonparametric
Density Estimation. It is therefore reassuring to ﬁnd typical signatures of
General Relativity in density estimation as this paper does. Perhaps Physics
is not just a special case of statistical inference and all these are only coin-
cidences of no more relevance than for example the fact that multiplication
or the logarithmic function appear everywhere all the time. That may be
so, but even in that case I believe it is worth noticing the connection for
undoubtedly GR and density estimation have a common goal: The dynamic
building of the stage.

More formally. Let f be a multivariate probability density function with
continuous second order partial derivatives. Consider the problem of esti-

2 ESTIMATING DENSITIES FROM DATA

3

mating the true value of f (z) > 0 at a single point z, from n independent
observations. It is shown that, fastest possible estimators (including the k-
nearest neighbor and kernel as well as the rich class of estimators in [9, the-
orem3.1]) have minimum asymptotic mean square errors when the space of
observations is thought as conformally curved. The optimal metric is shown
to be generated by the Hessian of f in the regions where the Hessian is deﬁ-
nite. Thus, the peaks and valleys of f are surrounded by horizons where the
Hessian changes signature from Riemannian to pseudo-Riemannian.

The result for the case of generalized k-nearest neighbor estimators [9]
has circulated since 1988 in the form of a technical report [10]. Recently
I found that a special case of this theorem has been known since 1972 [11]
and undergone continuous development in the Pattern Recognition literature,
(see e.g. [12, 13, 14, 15]).

2 Estimating Densities from Data
The canonical problem of density estimation at a point z ∈ IRp can be stated
as follows: Estimate f (z) > 0 from n independent observations of a random
variable with density f .

The most successful estimators of f (z) attempt to approximate the den-
sity of probability at z by using the observations x1, . . . , xn to build both, a
small volume around z and, a weight for this volume in terms of probability
mass. The density is then computed as the ratio of the estimated mass over
the estimated volume. The two classical examples are the k-nearest neighbor
(knn) and the kernel estimators.

2.1 The knn

The simplest and historically the ﬁrst example of a nonparametric density
estimator is [16] the knn. The knn estimator of f (z) is deﬁned for k ∈
{1, 2, . . . , n} as,

hn(z) =

k/n
λk

(2)

where λk is the volume of the sphere centered at the point z ∈ IRp of radius
R(k) given by the distance from z to the kth-nearest neighbor observation.
If λ denotes the Lebesgue measure on IRp we have,

2 ESTIMATING DENSITIES FROM DATA

where,

λk = λ(S(R(k)))

S(r) = {x ∈ IRp : kx − zk ≤ r}

The sphere S(r) and the radius R(k) are deﬁned relative to a given norm,
k ·k in IRp. The stochastic behavior of the knn depends on the speciﬁc value
of the integer k chosen in (2). Clearly, in order to achieve consistency (e.g.
stochastic convergence of hn(z) as n → ∞ towards the true value of f (z) > 0)
it is necessary to choose k = k(n) as a function of n. The volumes λk must
shrink, to control the bias, and consequently we must have k/n → 0 for hn(z)
to be able to approach a strictly positive number. On the other hand, we
must have k → ∞ to make the estimator dependent on an increasing number
k of observations and in this way to control its variance. Thus, for the knn
to be consistent, we need k to increase with n but at a rate slower than n
itself.

The knn estimator depends not only on k but also on a choice of norm.
The main result of this paper follows from the characterization of the k · k
that, under some regularity conditions, produces the best asymptotic (as
n → ∞) performance for density estimators.

2.2 The kernel
If we consider only regular norms k · k, in the sense that for all suﬃciently
small values of r > 0,

λ(S(r)) = λ(S(1))rp ≡ βrp

then, the classical kernel density estimator can be written as:

gn(z) =

Mµ

λ(S(µ))

4

(3)

(4)

(5)

(6)

(7)

where,

Mµ =

1

n
 Xxj∈S(µ)

Kµ−1(xj − z)


The smoothing parameter µ = µ(n) is such that k = [nµp] satisﬁes the
conditions for consistency of the knn, Kµ−1(x) = K(µ−1x) where the kernel

2 ESTIMATING DENSITIES FROM DATA

5

function K is a non negative bounded function with support on the unit
sphere (i.e. K(x) = 0 for kxk > 1) and satisfying,

Zkxk≤1

K(x)dx = β

(8)

Notice that for the constant kernel (i.e. K(x) = 1 for kxk ≤ 1) the estimator
(6) approximates f (z) by the proportion of observations inside S(µ) over the
volume of S(µ). The general kernel function K acts as a weight function
allocating diﬀerent weights Kµ−1(xj − z) to the xj’s inside S(µ). To control
bias (see (32) below) the kernel K is usually taken as a decreasing radially
symmetric function in the metric generated by the norm k·k. Thus, Kµ−1(xj−
z) assigns a weight to xj that decreases with its distance to z. This has
intuitive appeal, for the observations that lie closer to z are less likely to fall
oﬀ the sphere S(µ), under repeated sampling, than the observations that are
close to the boundary of S(µ).

The performance of the kernel as an estimator for f (z) depends ﬁrst and
foremost on the value of the smoothness parameter µ. The numerator and
the denominator of gn(z) depend not only on µ but also on the norm k · k
chosen and the form of the kernel function K. As it is shown in theorem (8)
these three parameters are inter-related.

2.3 Double Smoothing Estimators

The knn (2) and the kernel (6) methods are two extremes of a continuum.
Both, hn(z) and gn(z) estimate f (z) as probability-mass-per-unit-volume.
The knn ﬁxes the mass to the deterministic value k/n and lets the volume λk
to be stochastic, while the kernel method ﬁxes the volume λ(S(µ)) and lets
the mass Mµ to be random. The continuum gap between (2) and (6) is ﬁlled
up by estimators that stochastically estimate mass and volume by smoothing
the contribution of each sample point with diﬀerent smoothing functions for
the numerator and denominator (see [9]).

Let b ≥ 1 and assume, without loss of generality that bk is an integer.
The double smoothing estimators with deterministic weights are deﬁned as,

fn(z) =

1

1

R(k)(cid:17)
i=1 K(cid:16) z−xi
nPn
cbPbk

i=1 ωiλi

(9)

3 THE TRUTH AS N → ∞ ?

where,

ωi = Z i/bk

(i−1)/bk

ω(u)du

and ω(·) is a probability density on [0, 1] with mean c.

6

(10)

3 The Truth as n → ∞ ?

In nonparametric statistics, in order to assess the quality of an estimator
fn(z) as an estimate for f (z), it is necessary to choose a criterion for judg-
ing how far away is the estimator from what it tries to estimate. This is
sometimes regarded as revolting and morally wrong by some Bayesian Fun-
damentalists. For once you choose a loss function and a prior, logic alone
provides you with the Bayes estimator and the criterion for judging its qual-
ity. That is desirable, but there is a problem in high dimensional spaces.
in nonparametric problems)
In inﬁnite dimensional hypothesis spaces (i.e.
almost all priors will convince you of the wrong thing!
[17, 18]
for a non-regular way out see [19]). These kind of Bayesian nonparametric
results provide a mathematical proof that: almost all fundamental religions
are wrong, (more data can only make the believers more sure that the wrong
thing is true!). An immediate corollary is that: Subjective Bayesians can’t
go to Heaven. Besides, the choice of goodness of ﬁt criterion is as ad-hoc (an
equivalent) to the choice of a loss function.

(see e.g.

3.1 The Natural Invariant Loss Function and Why the

MSE is not that Bad

The most widely studied goodness of ﬁt criterion is the Mean Square Error
(MSE) deﬁned by,

(MSE) = E|fn(z) − f (z)|2

(11)

where the expectation is over the joint distribution of the sample x1, . . . , xn .
By adding and subtracting T = Efn(z) and expanding the square, we can
express the MSE in the computationally convenient form,

(MSE) = E|fn(z) − T|2 + |T − f (z)|2

= (variance) +(bias)2

(12)

7

(13)

3 THE TRUTH AS N → ∞ ?
By integrating (11) over the z ∈ IRp and interchanging E and R (OK by
Fubbini’s theorem since the integrand ≥ 0) we obtain,
(MISE) = EZ |fn(z) − f (z)|2dz

The Mean Integrated Square Error (MISE) is just the expected L2 distance
of fn from f . Goodness of ﬁt measures based on the (MSE) have two
main advantages: They are often easy to compute and they enable the rich
Hilbertian geometry of L2. On the other hand the (MSE) is unnatural and
undesirable for two reasons: Firstly, the (MSE) is only deﬁned for densities
in L2 and this rules out a priori all the densities in L1\ L2 which is unaccept-
able. Secondly, even when the (MISE) exists, it is diﬃcult to interpret (as
a measure of distance between densities) due to its lack of invariance under
relabels of the data space. Many researchers see the expected L1 distance
between densities as the natural loss function in density estimation. The L1
distance does in fact exist for all densities and it is easy to interpret but it
lacks the rich geometry generated by the availability of the inner product in
L2. A clean way out is to use the expected L2 distance between the wave
functions ψn = √fn and ψ = √f .

Theorem 1 The L2 norm of wave functions is invariant under relabels of
the data space, i.e.,

Z |ψn(z) − ψ(z)|2dz = Z | ˜ψn(z′) − ˜ψ(z′)|2dz′

where z = h(z′) with h any one-to-one smooth function.

(14)

Proof: Just change the variables. From, the change of variables theorem
the pdf of z′ is,

from where the wave function of z′ is given by,

Thus, making the substitution z = h(z′) we get,

∂(h)

˜f (z′) = f (h(z′))(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂(z′)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
˜ψ(z′) = ψ(h(z′))(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂(z′)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∂(h)

1/2

(15)

(16)

3 THE TRUTH AS N → ∞ ?

∂(h)

Z |ψn − ψ|2dz = Z |ψn(h(z′)) − ψ(h(z′))|2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂(z′)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
− ψ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂(z′)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∂(z′)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= Z |ψn(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= Z | ˜ψn − ˜ψ|2dz′

∂(h)

∂(h)

1/2

1/2

|2dz′

dz′

Q.E.D.

8

(17)

The following theorem shows that a transformation of the MSE of a con-
sistent estimator provides an estimate for the expected L2 norm between
wave functions.

Theorem 2 Let fn(z) be a consistent estimator of f (z). Then,

EZ |ψn − ψ|2dz =
Proof: A ﬁrst order Taylor expansion of √x about x0 gives,

4Z E|fn(z) − f (z)|2

f (z)

1

dz + (smaller order terms)

√x − √x0 =

1
2

(x − x0)
√x0

+ o((x − x0)2)

(18)

(19)

Substituting x = fn(z), x0 = f (z) into (19) squaring both sides and taking
expectations we obtain,

1
4

E|fn(z) − f (z)|2

E|ψn(z) − ψ(z)|2 =

+ o(E|fn(z) − f (z)|2)
integrating over z and interchanging E and R we arrive at (18).

Q.E.D.

f (z)

Proceeding as in the proof of theorem 1 we can show that

(20)

Z |fn − f|2

f

dz = Z | ˜fn − ˜f|2

˜f

dz′

(21)

where, as before, z ↔ z′ is any one-to-one smooth transformation of the data
space and ˜f is the density of z′. Thus, it follows from (21) that the leading
term on the right hand side of (18) is also invariant under relabels of the data
space. The nice thing about the L2 norm of wave functions, unlike (21), is
that it is deﬁned even when f (z) = 0.

4 SOME CLASSIC ASYMPTOTIC RESULTS

9

4 Some Classic Asymptotic Results

We collect here the well known Central Limit Theorems (CLT) for the knn
and kernel estimators together with some remarks about nonparametric den-
sity estimation in general. The notation and the formulas introduced here
will be needed for computing the main result about optimal norms in the
next section.

Assumption 1 Let f be a pdf on IRp of class C2 with non singular Hessian,
H(z) at z ∈ IRp, and with f (z) > 0, i.e., the matrix of second order partial
derivatives of f at z exists, it is non singular and its entries are continuous
at z.

Assumption 2 Let K be a bounded non negative function deﬁned on the
unit sphere, S0 = {x ∈ IRp : kxk ≤ 1} and satisfying,

Zkxk≤1
Zkxk≤1

K(x)dx = λ(S0) ≡ β
xK(x)dx = 0 ∈ IRp

(22)

(23)

Theorem 3 (CLT for knn) Under assumption 1, if k = k(n) is taken in
the deﬁnition of the knn (2) in such a way that for some a > 0

lim
n→∞

n−4/(p+4)k = a

then, if we let Zn = √k(hn(z) − f (z)) we have,
exp −

P (Zn ≤ t) =Z t

1
√2π

lim
n→∞

−∞

2V (z) ! dy
(y − B(z))2

where,

and,

B(z) = 


a

p+4
2p

2f 2/p(z)

(β−1−2/pZkxk≤1

xT H(z)xdx)

V (z) = f 2(z)

(24)

(25)

(26)

(27)

4 SOME CLASSIC ASYMPTOTIC RESULTS

10

Proof: This is a special case of [9, theorem3.1].

Theorem 4 (CLT for kernel) Under assumptions 1, and 2 if µ = µ(n) is
taken in the deﬁnition of the kernel (6) in such a way that for some a > 0,

k = [nµp] satisﬁes (24) then, if we let Zn = √k(gn(z) − f (z)) we have (25)

where now,

and,

xT H(z)xK(x)dx)

a

p+4
2p

B(z) =
2 
(β−1Zkxk≤1

V (z) = f (z)(β−2Zkxk≤1

K 2(x)dx)

Proof: The sample x1, . . . , xn is assumed to be iid f and therefore the kernel
estimator gn(z) given by (6) and (7) is a sum of iid random variables. Thus,
the classic CLT applies and we only need to verify the rate (24) and the
asymptotic expressions for the bias (28) and variance (29). We have,

(28)

(29)

(30)

(31)

(33)

E [gn(z)] =

=

1
n

1
βµp
1

n

Xj=1Z K  xj − z

µ ! f (xj)dxj

βµp Z K(y)f (z + µy)µpdy

= Z K(y)

= f (z) +

β (f (z) + µ∇f (z) · y +
2β Z yT H(z)yK(y)dy + o(µ2)

µ2
2

µ2

yT H(z)y + o(µ2)) dy(32)

where we have changed the variables of integration to get (31), used assump-
tion 1 and Taylor’s theorem to get (32) and used assumption 2 to obtain
(33). For the variance we have,

var(gn(z)) =

=

1

nβ2µ2p var (K((X − z)/µ))
nβ2µ2p (Zkyk≤1

1

f (z + µy)K 2(y)µpdy

(34)

4 SOME CLASSIC ASYMPTOTIC RESULTS

− Zkyk≤1
nβ2µp Zkyk≤1

f (z + µy)K(y)µpdy!2

nµp!
K 2(y)dy + o  1

f (z)

=

11

(35)

(36)

where we have used var(K) = EK 2 − (EK)2 and changed the variables of
integration to get (35), used assumption 1 and (0th order) Taylor’s theorem
to get (36). Hence, the theorem follows from (33) and (36) after noticing
that (24) and k = nµp imply,

√kµ2 = k
k
k

k
nµp =

Q.E.D.

4+p

2p n−2/p = (n− 4

p+4 k)

= 1

p+4

2p −→ a

p+4
2p

(37)

(38)

Theorem 5 (CLT for double smoothers) Consider the estimator fn(z)

deﬁned in (9). Under assumptions 1, 2, and (24) if we let Zn = √k(fn(z) −

f (z)) we have (25) where now,

B(z) = 


and,

with,

a

p+4
2p

2[βf (z)]2/p

 β−1(Zkxk≤1
V (z) = f 2(z)(β−1Zkxk≤1

xT H(z)x [K(x) + λ0] dx)

K 2(x)dx − λ1)

λ0 =

b2/p

c Z 1

0

u1+ 2

λ1 = 1 − c−2b−1Z 1

p ω(u)du − 1
0 (cid:26)Z 1

ω(x)dx(cid:27)2

y

dy

(39)

(40)

(41)

(42)

Proof: See [9, theorem3.1]. Remember to substitute K by β−1K since in
the reference the Kernels are probability densities and in here we take them
as weight functions that integrate to β.

4 SOME CLASSIC ASYMPTOTIC RESULTS

12

4.1 Asymptotic Mean Square Errors

Let fn be an arbitrary density estimator and let Zn = √k(fn(z)−f (z)). Now

suppose that fn(z) is asymptotically normal, in the sense that when k = k(n)
satisﬁes (24) for some a > 0, we have (25) true. Then, all the moments of
Zn will converge to the moments of the asymptotic Gaussian. In particular
the mean and the variance of Zn will approach B(z) and V (z) respectively.
Using, (12) and (24) we can write,

lim
n→∞

n4/(p+4)E|fn(z) − f (z)|2 =

V (z)

a

+

B2(z)

a

(43)

We call the right hand side of (43) the asymptotic mean square error (AMSE)
of the estimator fn(z). The value of a can be optimized to obtain a global
minimum for the (AMSE) but it is well known in nonparametrics that the
rate n−4/(p+4) is best possible (in a minimax sense) under the smoothness
asumption 1 (see e.g. [20]). We can take care of the knn, the kernel, and the
double smoothing estimators simultaneously by noticing that in all cases,

(AMSE) = α1a−1 + α2a4/p

has a global minimum of,

(AMSE)∗ =((1 + 4/p)(cid:18)p

p+4) α
4(cid:19) 4

4

p

p+4

p+4
1 α
2

achieved at,

a∗ =(cid:18) pα1

4α2(cid:19) p

p+4

(44)

(45)

(46)

Replacing the corresponding values for α1 and α2 for the knn, for the kernel,
and for the double smoothing estimators, we obtain that in all cases,

(AMSE)∗ = (const. indep. of f )


f (z)  ∆2

p+4
f (z)! p


where,

∆ = Zkxk≤1

xT H(z)xG(x)dx

(47)

(48)

4 SOME CLASSIC ASYMPTOTIC RESULTS

=

p

Xj=1

ρj

∂2f
∂z2
j

(z)

13

(49)

with G(x) = 1 for the knn, G(x) = K(x) for the kernel, G(x) = K(x)+λ0 for
the double smoothers (see (39) and (41)) and, if ej denotes the jth canonical
basis vector (all zeroes except a 1 at position j),

ρj = Zkxk≤1

(x · ej)2G(x)dx

(50)

Notice that (49) follows from (48), (23) and the fact that H(z) is the Hes-
sian of f at z. The generality of this result shows that (47) is typical for
density estimation. Thus, when fn is either the knn, the kernel, or one of
the estimators in ([9, theorem3.1]), we have:

n4/(p+4)E|fn(z) − f (z)|2 ≥ cf (z)  ∆2

f (z)! p

p+4

lim
n→∞

(51)

The positive constant c may depend on the particular estimator but it is
independent of f . Dividing both sides of (51) by f (z), integrating over z,

∆

2p
p+4

dz

(52)

(53)

The worst case scenario is obtained by the model f = ψ2 that maximizes the
action given by the right hand side of (52),

lim
n→∞

using theorem 2 and interchanging E and R we obtain,
ψ(z)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n4/(p+4)EZ |ψn(z) − ψ(z)|2dz ≥ 4cZ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
L =Z (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(z)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∂2ψ2
∂z2
j

Xj=1

2p
p+4

dz

p

1

ψ(z)

ρj

This is a hard variational problem. However, it is worth noticing that the
simplest case is obtained when the exponent is 1, i.e. when the dimension
of the data space is p = 4. Assuming we were able to ﬁnd a solution, this
solution would still depend on the p parameters ρ1, . . . , ρp. A choice of ρj’s
is equivalent to the choice of a global metric for the data space. Notice also,
that the exponent becomes 2 for p = ∞ and that for p ≥ 3 (but not for
p = 1 or 2) there is the possibility of non trivial (i.e. diﬀerent from uniform)
super-eﬃcient models for which estimation can be done at rates higher than
n−4/(p+4). These super-eﬃcient models are characterized as the non negative

5 CHOOSING THE OPTIMAL NORM

14

solutions of the Laplace equation in the metric generated by the ρj’s, i.e.,
non negative (f (z) ≥ 0) solutions of,

p

Xj=1

ρj

∂2f
∂z2
j

(z) = 0

(54)

Recall that there are no non trivial (diﬀerent from constant) non negative
super-harmonic functions in dimensions one or two but there are plenty of
solutions in dimension three and higher. For example the Newtonian poten-
tials,

with the norm,

f (z) = ckzk−(p−2)

ρ

kzk2

ρ =

p

√ρj!2
Xj=1  zj

(55)

(56)

will do, provided the data space is compact. The existence of (hand picked)
super-eﬃcient models is what made necessary to consider best rates only in
the minimax sense. Even though we can estimate a Newtonian potential
model at faster than usual nonparametric rates, in any neighborhood of the
Newtonian model the worst case scenario is at best estimated at rate n−4/(p+4)
under second order smoothness conditions.

5 Choosing the Optimal Norm

All ﬁnite (p < ∞) dimensional Banach spaces are isomorphic (as Banach
spaces) to IRp with the euclidian norm. This means, among other things,
that in ﬁnite dimensional vector spaces all norms generate the same topology.
Hence, the spheres {x ∈ IRp : kxk ≤ r} are Borelians so they are Lebesgue
measurable and thus, estimators like the knn (2) are well deﬁned for arbitrary
norms. It is possible, in principle, to consider norms that are not coming from
inner products but in this paper we look only at Hilbert norms k · kA of the
form,

(57)
where A ∈ Λ with Λ deﬁned as the open set of real non-singular p × p
matrices. For each A ∈ Λ deﬁne the unit sphere,

A = zT AT Az

kzk2

5 CHOOSING THE OPTIMAL NORM

its volume,

SA = {x ∈ IRp : xT AT Ax ≤ 1}

βA = λ(SA) =ZSA

λ(dx)

and the A-symmetric (i.e. k · kA radially symmetric) kernel, KA,

KA(x) = (K ◦ A)(x) = K(Ax)

15

(58)

(59)

(60)

where K satisﬁes assumption 2 and it is I-symmetric, i.e., radially symmetric
in the euclidian norm. This means that K(y) depends on y only through the
euclidian length of y, i.e. there exists a function F such that,

K(y) = F (yT y)

(61)

The following simple theorem shows that all A-symmetric functions are really
of the form (60).

Theorem 6 For any A ∈ Λ, ˜K is A-symmetric if and only if we can write
(62)

˜K(x) = K(Ax) for all x ∈ IRp

for some I-symmetric K.

Proof:

˜K(x) is A-symmetric iﬀ ˜K(x) = F (kxk2

A) for some function F .

Choose K(x) = ˜K(A−1x). This K is I-symmetric since K(x) = F (cid:16)(AA−1x)T (AA−1x)(cid:17) =

F (xT x). More over, ˜K(x) = ˜K(A−1(Ax)) = K(Ax). Thus, (62) is necessary
for A-symmetry. It is also obviously suﬃcient since the assumed I-symmetry
of K in (62) implies that ˜K(x) = F ((Ax)T (Ax)) = F (kxk2
A) so it is A-
symmetric.
Q.E.D.

An important corollary of theorem 6 is,

Theorem 7 Let A, B ∈ Λ. Then, ˜K is AB-symmetric if and only if ˜KB−1
is A-symmetric.

5 CHOOSING THE OPTIMAL NORM

16

Proof: By the ﬁrst part of theorem 6 we have that ˜K = K ◦ A ◦ B with K
some I-symmetric. Thus, ˜K ◦ B−1 = K ◦ A is A-symmetric by the second
part of theorem 6.
Q.E.D.

Let us denote by β(A, K) the total volume that a kernel K assigns to the

unit A-sphere SA, i.e.,

β(A, K) =ZSA

K(x)dx

(63)

In order to understand the eﬀect of changing the metric on a density esti-
mator like the kernel (6), it is convenient to write gn explicitly as a function
of everything it depends on; The point z, the metric A, the A-symmetric
kernel function ˜K, the positive smoothness parameter µ and, the data set
{x1, . . . , xn}. Hence, we deﬁne,

gn(z; A, ˜K, µ,{x1, . . . , xn}) =

1

nPn

µ (cid:17)
˜K(cid:16) xj−z

j=1
β(A, ˜K)µp

(64)

The following result shows that kernel estimation with metric AB is equiv-
alent to estimation of a transformed problem with metric A. The explicit
form of the transformed problem indicates that a perturbation of the met-
ric should be regarded as composed of two parts: Shape and volume. The
shape is confounded with the form of the kernel and the change of volume is
equivalent to a change of the smoothness parameter.

Theorem 8 Let A, B ∈ Λ, µ > 0, and ˜K an AB-symmetric kernel. Then,
for all z ∈ IRp and all data sets {x1 . . . , xn} we have,
gn(z; AB, ˜K, µ,{x1, . . . , xn}) = gn( ˆBz; A, ˜K◦B−1,|B|−1/pµ,{ ˆBx1, . . . , ˆBxn})
(65)
where |B| denotes the determinant of B and ˆB = |B|−1/pB is the matrix B
re-scaled to have unit determinant.

Proof: To simplify the notation let us denote,

Substituting AB for A in (64) and using theorem 6 we can write the left
hand side of (65) as,

µ

|B|1/p

µB =

(66)

5 CHOOSING THE OPTIMAL NORM

17

1

nPn

µ (cid:17)(cid:17)
j=1 K(cid:16)AB(cid:16) xj−z

β(AB, ˜K)µp

=

1

nPn

j=1(K ◦ A)(cid:18) ˆBxj− ˆBz
µB (cid:19)
β(A, K ◦ A)(µB)p

where K is some I-symmetric kernel and we have made the change of vari-
ables x = B−1y in β(AB, ˜K) (see (63) ). The last expression is the right
hand side of (65). Notice that, K ◦ A = ˜KB−1 is A-symmetric.
Q.E.D.

5.1 Generalized knn Case with Uniform Kernel

In this section we ﬁnd the norm of type (57) that minimizes (47) for the
estimators of the knn type with uniform kernel which include the double
smoothers with K(x) = 1. As it is shown in theorem 8 a change in the
determinant of the matrix that deﬁnes the norm is equivalent to changing
the smoothness parameter. The quantity (57) to be minimized was obtained
by ﬁxing the value of the smoothness parameter to the best possible, i.e. the
one that minimizes the expression of the (AMSE) (43). Thus, to search for
the best norm at a ﬁx value of the smoothness parameter we need to keep
the determinant of the matrix that deﬁnes the norm constant. We have,

Theorem 9 Consider the problem,

min

|A|=1(cid:18)ZSA

xT H(z)xdx(cid:19)2

(67)

where the minimum is taken over all p × p matrices with determinant one,
SA is the unit A-ball and H(z) is the Hessian of the density f ∈ C2 at z
which is assumed to be nonsingular.
When H(z) is indeﬁnite, i.e. when H(z) has both positive and negative
eigenvalues, the objective function in (67) achieves its absolute minimum
value of zero when A is taken as,

A = c−1diag(s ξ1
p − m

, . . . ,s ξm
p − m

,sξm+1

m

, . . . ,s ξp

m

)M

(68)

where the ξj are the absolute value of the eigenvalues of H(z), m is the
number of positive eigenvalues, M is the matrix of eigenvectors and c is a

5 CHOOSING THE OPTIMAL NORM

18

normalization constant to get det A = 1 (see the proof for more detailed
deﬁnitions).

If H(z) is deﬁnite, i.e. when H(z) is either positive or negative deﬁnite,

then for all p × p real matrices A with det A = 1 we have,
p |det H(z)|1/p

2pπ
p + 3

(cid:12)(cid:12)(cid:12)(cid:12)
ZSA

xT H(z)xdx(cid:12)(cid:12)(cid:12)(cid:12) ≥

with equality if and only if,

A =

H 1/2
+ (z)
|H 1/2
+ (z)|1/p

(69)

(70)

where H 1/2
below for explicit deﬁnitions).

+ (z) denotes the positive deﬁnite square-root of H(z) (see the proof

Proof: Since f ∈ C2 the Hessian is a real symmetric matrix and we can
therefore ﬁnd an orthogonal matrix M that diagonalizes H(z), i.e. such that,

H(z) = M T DM with M T M = I

where,

D = diag (ξ1, ξ2, . . . , ξm,−ξm+1, . . . ,−ξp)

(71)

(72)

where all the ξj > 0 and we have assumed that the columns of M have been
ordered so that all the m positive eigenvalues appear ﬁrst and all the negative
eigenvalues −ξm+1, . . . ,−ξp appear last so that (71) agrees with (72). Split
D as,

D = diag (ξ1, . . . , ξm, 0, . . . , 0) − diag (0, . . . , 0, ξm+1, . . . , ξp)

= D+ − D−

and use (71) and (73) to write,

H(z) = M T D+M − M T D−M

= (cid:16)D1/2

+ M(cid:17)T (cid:16)D1/2
+Σ+ − ΣT

−Σ−

= ΣT

+ M(cid:17) −(cid:16)D1/2

− M(cid:17)T (cid:16)D1/2

− M(cid:17)

Using (74) and the substitution y = Ax we obtain,

ZSA

xT H(z)xdx = ZyT y≤1

yT (cid:16)A−1(cid:17)T (cid:16)ΣT

+Σ+ − ΣT

−Σ−(cid:17) A−1ydy

= ZyT y≤1DΣA−1y, ΣA−1yE dy

(73)

(74)

(75)

5 CHOOSING THE OPTIMAL NORM

where,

Σ = Σ+ + Σ− = (D+ + D−)1/2 M

and < ., . > denotes the pseudo-Riemannian inner product,

hx, yi =

m

Xi=1

xiyi −

p

Xi=m+1

xiyi

19

(76)

(77)

By letting G = diag(1, . . . , 1,−1, . . . ,−1) (i.e. m ones followed by p − m
negative ones) be the metric with the signature of H(z) we can also write
(77) as,

Let,

hx, yi = xT Gy

B = [b1|b2| . . .|bp] = ΣA−1

where b1, . . . , bp denote the columns of B. Substituting (79) into (75), using
the linearity of the inner product and the symmetry of the unit euclidian
sphere we obtain,

ZSA

where ρ stands for,

xT H(z)xdx = ZyT y≤1 hBy, Byi dy
= Xj Xk hbj, bkiZSI
= Xj Xk hbj, bki δjkρ

yjykdy

= ρ

p

Xj=1hbj, bji

From (79) and (81) it follows that problem (67) is equivalent to,

(78)

(79)

(80)

(81)

(82)

(83)

dy =

2pπ
p + 3

ρ = ZSI (cid:16)y1(cid:17)2
|B|=|Σ|


min

2

p

Xj=1hbj, bji


When H(z) is indeﬁnite, i.e. when m /∈ {0, p} it is possible to choose the
columns of B so that Pj hbj, bji = 0 achieving the global minimum. There
are many possible choices but the simplest one is,

B = c · diag(√p − m,√p − m, . . . ,√p − m
}

{z

|

m

,√m,√m, . . . ,√m
}
|

{z

p−m

)

(84)

5 CHOOSING THE OPTIMAL NORM

since,

p

Xj=1hbj, bji = c2m(√p − m)2 − c2(p − m)(√m)2 = 0.

20

(85)

The scalar constant c needs to be adjusted to satisfy the constraint that
det B = det Σ. From (79), (84) and (76) we obtain that the matrix for the
optimal metric can be written as,

From (86) we get,

A = B−1Σ =

c−1

√p − m

Σ+ +

c−1
√m

Σ−

AT A =

c−2
p − m

ΣT

+Σ+ +

c−2
m

ΣT

−Σ−

Finally from (74) we can rewrite (87) as,

AT A = c−2M T   1
p − m
ξ1

= c−2M T diag(

D+ +

1
m

D−! M

, . . . ,

ξm
p − m

,

ξm+1
m

, . . . ,

ξp
m

)M

p − m

(86)

(87)

(88)

(89)

Notice that when p − m = m (i.e. when the number of positive equals the
number of negative eigenvalues of H(z)) the factor 1/m can be factorized
out from the diagonal matrix in (89) and in this case the optimal A can be
expressed as,

A =

H 1/2
+ (z)
|H 1/2
+ (z)|1/p

where we have used the positive square-root of H(z) deﬁned as,

H 1/2

+ (z) = diag(qξ1, . . . ,qξp)M

(90)

(91)

In all the other cases for which H(z) is indeﬁnite, i.e. when m /∈ {0, p/2, p}
we have,

A = c−1diag(s ξ1
p − m

, . . . ,s ξm
p − m

,s ξm+1

m

, . . . ,s ξp

m

)M

(92)

5 CHOOSING THE OPTIMAL NORM

21

The normalization constant c is ﬁxed by the constraint that det A = 1 as,

c = (p − m)− m

2p m− (p−m)

2p

| det H(z)|

1
2p

(93)

This shows (68).

Let us now consider the only other remaining case when H(z) is deﬁnite,
i.e. either positive deﬁnite (m = p) or negative deﬁnite (m = 0). Introducing
λ0 as the Lagrange multiplier associated to the constraint det B = det Σ we
obtain that the problem to be solved is,

min
b1,...bp,λ0 L(b1, b2, . . . , bp, λ0)

(94)

where the Lagrangian L is written as a function of the columns of B as,
Xj=1hbj, bji
L(b1, b2, . . . , bp, λ0) = 


The −4λ0 instead of just λ0 is chosen to simplify the optimality equations
below. The optimality conditions are,

− 4λ0(det(b1, . . . , bp) − det Σ)

(95)

p

2

∂L
∂bj

= 0 for j = 1, . . . , p and

∂L
∂λ0

= 0

(96)

where the functional partial derivatives are taken in the Fr´echet sense with
respect to the column vectors bj. The Fr´echet derivatives of quadratic and
multi linear forms are standard text-book exercises. Writing the derivatives
as linear functions of the vector parameter h we have,

∂
∂bj hbj, bji (h) = 2 hbj, hi

det(b1, . . . , bp)(h) = det(b1, . . . ,

∂
∂bj

(97)

(98)

, . . . , bp)

j-th col.

h

|{z}

Thus, using (97) and (98) to compute the derivative of (95) we obtain that
for all h and all j = 1, . . . , p we must have,

∂L
∂bj

(h) = 2( p

Xk=1hbk, bki) 2 hbj, hi − 4λ0 det(b1, . . . , h, . . . , bp) = 0

(99)

5 CHOOSING THE OPTIMAL NORM

22

When Pk < bk, bk >6= 0 we can rewrite (99) as,
But now we can substitute h = bi with i 6= j into (100) and use the fact that
the determinant of a matrix with two equal columns is zero, to obtain,

0 det(b1, . . . , h, . . . , bp)

hbj, hi = c−1

(100)

hbj, bii = 0 for all i 6= j.
In a similar way, replacing h = bj into (100), we get

hbj, bji = c−1

0 det B = c

(101)

(102)

where c is a constant that needs to be ﬁxed in order to satisfy the constraint
that det B = det Σ. We have shown that the optimal matrix B must have or-
thogonal columns of the same length for the G-metric. This can be expressed
with a single matrix equation as,

BT GB = cI

Substituting (79) into (103) and re-arranging terms we obtain,

AT A = c−1ΣT GΣ

= c−1(ΣT
= c−1(ΣT

+ + ΣT
+Σ+ − ΣT

−Σ−)

−)G(Σ+ + Σ−)

AT A = c−1H(z)

From (105), (103), (82) and (81) we obtain,

ZSA

(cid:12)(cid:12)(cid:12)(cid:12)

xT H(z)x dx(cid:12)(cid:12)(cid:12)(cid:12) ≥ ρp|c|

and replacing the values of ρ and c we obtain (69).
Q.E.D.

(103)

(104)

(105)

(106)

5.2 Yet Another Proof When The Hessian is Deﬁnite

Consider the following lemma.

Lemma 1 Let A, B be two p× p non-singular matrices with the same deter-
minant. Then
(107)

ZSA kxk2

Bdx ≥ ZSB kyk2

Bdy

5 CHOOSING THE OPTIMAL NORM

Proof: Just split SA and SB as,

SA = (SASB) ∪ (SASc
B)
SB = (SBSA) ∪ (SBSc
A)

and write,

ZSA kxk2

Bdx = ZSB kxk2

Bdx −ZSc

ASB kxk2

Bdx +ZSASc

B kxk2

Bdx

Now clearly,

min
x∈SASc

B kxk2

B ≥ 1 ≥ max

y∈Sc

ASB kyk2

B

from where it follows that,

ZSASc

B kxk2

BZSASc
BZSc

ASB

B

dx

dy

x∈SASc

Bdx ≥ min
≥ max
≥ ZSc

B kxk2
ASB kyk2
ASB kyk2
Bdy

y∈Sc

23

(108)
(109)

(110)

(111)

(112)

(113)

(114)

where (112) and (114) follow from (111). To justify the middle inequality
(113) notice that from (108), (109) and the hypothesis that |A| = |B| we can
write,

ZSASc

B

dx +ZSASB

dx = ZSc

ASB

dy +ZSASB

dy

(115)

The conclusion (107) follows from inequality (114) since that makes the last
two terms in (110) non-negative.
Q.E.D.

If B is a nonsingular matrix we deﬁne,

ˆB =

B

|det B|1/p

(116)

An immediate consequence of lemma 1 is,

Theorem 10 If H(z) is deﬁnite, then for all p× p matrices with |A| = 1 we
have,

∆ = ZSA kxk2

H 1/2(z)dx ≥ ZS ˆH1/2(z) kxk2

H 1/2(z)dx

(117)

5 CHOOSING THE OPTIMAL NORM

Proof:

∆ = |H(z)|1/pZSA kxk2

ˆH 1/2(z)dx

≥ |H(z)|1/pZS ˆH1/2(z) kxk2
= ZS ˆH1/2(z) kxk2

H 1/2(z)dx

ˆH 1/2(z)dx

24

(118)

(119)

(120)

where we have used lemma 1 to deduce the middle inequality (119).
Q.E.D.

5.3 Best Norm With General Kernels

In this section we solve the problem of ﬁnding the optimal norm in the general
class of estimators (9).

Before we optimize the norm we need to state explicitly what it means to
do estimation with diﬀerent kernels and diﬀerent norms. First of all a general
kernel function is a nonnegative bounded function deﬁned on the unit sphere
generated by a given norm. Hence, the kernel only makes sense relative to
the given norm. To indicate this dependence on the norm we write KA for
the kernel associated to the norm generated by the matrix A. We let

KA = K ◦ A

(121)

where K = KI is a ﬁx mother kernel deﬁned on the euclidian unit sphere.
Equation (121) provides meaning to the notion of changing the norm without
changing the kernel. What this means is not that the kernel is invariant under
changes of A but rather equivariant in the form speciﬁed by (121). Recall also
that a proper kernel must satisfy (22). To control bias we must also require
the kernels to satisfy (23). It is easy to see (just change the variables) that
if the mother kernel K has these properties so do all its children KA with
the only proviso that |A| = 1 in order to get (22). Notice also that radial
symmetry of K is a suﬃcient but not a necessary condition for (23).
The optimization of the norm with general kernels looks more complicated
than when the kernel is uniform since the best (AMSE)∗ also depends on
A(x)dx. Consider the double smoothing estimators, which are the most

general case treated in this paper. From, (39), (40) and (45) we have,

RSA K 2
(AMSE)∗ = (const.)(cid:26)β−1ZSA

K 2

A(x)dx − λ1(cid:27) 4

p+4

f (z)   ∆2

f (z)! p

p+4

(122)

5 CHOOSING THE OPTIMAL NORM

25

where the constant depends only on the dimension of the space. Even though
the dependence of (122) on A looks much more complicated than (47) this is
only apparently so. In fact the two expressions deﬁne very similar optimiza-
tion problems as we now show.

First notice that the search for best A must be done within the class
of matrices with a ﬁx determinant. For otherwise we will be changing the
value of the smoothness parameter that was ﬁxed to the best possible value
in order to obtain (122). If we let |A| = 1 we have,

ZSA

K(x) dx = β = ZSA

dx = λ(SI)

(123)

We also have that,

ZSA

K 2

A(y) dy =ZSA

K 2(Ay) dy = ZSI

K 2(y) dx

(124)

From (123) and (124) we deduce that the term in (122) within cursive brack-
ets is the same for all matrices A and it depends only on the ﬁx kernel K.
Finally notice that the value of ∆ in (122) is given by

∆ = ZSA

xT H(z)x G(Ax) dx

(125)

where G(x) = K(x) + λ0 in the general case. By retracing again the steps
that led to (81) we can write,

yjykG(y) dy

(126)

∆ = Xj Xk hbj, bkiZSI

= Xj Xk hbj, bki δjkρk(G)

p

=

Xj=1hbj, bji ρj(G)
ρj(G) = ZSI (cid:16)xj(cid:17)2

G(x) dx

where now,

(127)

(128)

There are three cases to be considered.

1. All the ρj(G) = ρ for j = 1, . . . , p. The optimization problem reduces
to the case when the kernel is uniform and therefore it has the same
solution.

6 ASYMPTOTIC RELATIVE EFFICIENCIES

26

2. All the ρj(G) have the same sign, i.e. they are all positive or all nega-
tive. If e.g. all ρj > 0 just replace bj with √ρjbj and use the formulas
obtained for the uniform kernel case.

3. Some of the ρj(G) are positive and some are negative. This case can be
handled like the previous one after taking care of the signs for diﬀerent
indices j.

The ﬁrst case is the most important for it is the one implied when the kernels
are radially symmetric. The other two cases are only included for complete-
ness. Clearly if we do estimation with a non radially symmetric kernel the
optimal norm would have to correct for this arbitrary builtin asymmetry,
eﬀectively achieving at the end the same performance as when radially sym-
metric kernels are used. The following theorem enunciates the main result.

Theorem 11 In the general class of estimators (9) with radially symmetric
(mother) kernels, best possible asymptotic performance (under second order
smoothness conditions) is achieved when distances are measured with the best
metrics obtained when the kernel is uniform.

6 Asymptotic Relative Eﬃciencies

The practical advantage of using density estimators that adapt to the form
of the optimal metrics can be measured by computing the Asymptotic Rel-
ative Eﬃciency (ARE) of the optimal metric to the euclidian metric. Let us
denote by AMSE(I) and AMSE(H(z)) the expressions obtained from (122)
when using the euclidian norm and the optimal norm respectively. For the
Euclidean norm we get,

AMSE(I) = (const.)(cid:26)β−1ZSI

K 2(x)dx − λ1(cid:27) 4

p+4

f (z)   (ρ tr H(z))2

f (z)

p+4

! p

(129)

where tr stands for the trace since,

∆ = ZSI

xT H(z)x G(x) dx = Xi,j

hij(z)ZSI

xixjG(x) dx = ρ tr H(z)

(130)

7 AN EXAMPLE: RADIALLY SYMMETRIC DISTRIBUTIONS

27

Using (123), (124) and (69) we obtain that when H(z) is deﬁnite,

AMSE(H(z)) =

(131)

(const.)(cid:26)β−1ZSI

K 2(x)dx − λ1(cid:27) 4

p+4

f (z)  (ρ p | det H(z)|1/p)2

f (z)

p+4

! p

Hence, when H(z) is deﬁnite the ARE is,

ARE =

AMSE(I)

AMSE(H(z))

=  

tr H(z)

p | det H(z)|1/p! 2p

p+4

(132)

If ξ1, . . . , ξp are the absolute value of the eigenvalues of H(z) then we can
write,

ARE =


1

(cid:16)Qj ξj(cid:17)1/p
pPj ξj


2p
p+4

=   arith. mean of {ξj}

geom. mean of {ξj}! 2p

p+4

(133)

It can be easily shown that the arithmetic mean is always greater or equal
than the geometric mean (take logs, use the strict concavity of the logarithm
and Jensen’s inequality) with equality if and only if all the ξj’s are equal.
Thus, it follows from (133) that the only case in which the use of the optimal
metric will not increase the eﬃciency of the estimation of the density at
a point where the Hessian is deﬁnite is when all the eigenvalues of H(z)
are equal. It is also worth noticing that the eﬃciency increases with p, the
dimension of the data space. There is of course inﬁnite relative eﬃciency in
the regions where the H(z) is indeﬁnite.

7 An Example: Radially Symmetric Distri-

butions

When the true density f (z) has radial symmetry it is possible to ﬁnd the
regions where the Hessian H(z) is positive and negative deﬁnite. These
models have horizons deﬁned by the boundary between the regions where
H(z) is deﬁnite. We show also that when and only when the density is linear
in the radius of symmetry, the Hessian is singular in the interior of a solid
sphere. Thus, at the interior of these spheres it is impossible to do estimation
with the best metric.

7 AN EXAMPLE: RADIALLY SYMMETRIC DISTRIBUTIONS

28

Let us denote simply by L the log likelihood, i.e.,

f (z) = exp(L)

(134)

If we also denote simply by Lj the partial derivative of L with respect to zj
then,

∂f
∂zj

= f (z) Lj

and also,

∂2f

∂zi∂zj

=

∂f
∂zi

Lj + f (z) Lij = f (z) {LiLj + Lij}

where we have used (135) and the deﬁnition Lij = ∂Lj
. It is worth notic-
∂zi
ing, by passing, that (136) implies a notable connection with the so called
nonparametric Fisher information I(f ) matrix,

Z H(z) dz = I(f ) − I(f ) = 0

our main interest here however, is the computation of the Hessian when the
density is radially symmetric. Radial symmetry about a ﬁx point µ ∈ IRp is
obtained when L (and thus f as well) depends on z only through the norm
kz − µkV −1 for some symmetric positive deﬁnite p × p matrix V . Therefore
we assume that,

from where we obtain,

L = L(−

1
2

(z − µ)T V −1(z − µ))

(135)

(136)

(137)

(138)

(139)

(140)

Li = (cid:16)−vi·(z − µ)(cid:17) L′
Lij = L′′vi·(z − µ)vj·(z − µ) − L′vij

where vi· and vij denote the i-th row and ij-th entries of V −1 respectively.
Replacing (139) and (140) into (136), using the fact that V −1 is symmetric
and that vj·(z−µ) is a scalar and thus, equal to its own transpose (z−µ)T v·j,
we obtain

H(z) = f (z)L′( L′ +

L′′

L′! V −1(z − µ)(z − µ)T − I) V −1

(141)

We have also assumed that L′ is never zero. With the help of (141) we can
now ﬁnd the conditions for H(z) to be deﬁnite and singular. Clearly H(z)

7 AN EXAMPLE: RADIALLY SYMMETRIC DISTRIBUTIONS

29

will be singular when the determinant of the matrix within curly brackets
in (141) is zero. But that determinant being zero means that λ = 1 is an
eigenvalue of

(L′ + L′′/L′)V −1(z − µ)(z − µ)T

(142)

and since this last matrix has rank one its only nonzero eigenvalue must be
equal to its own trace. Using the cyclical property of the trace and letting

y = −

1
2

(z − µ)T V −1(z − µ)

we can write,

Theorem 12 The Hessian of a radially symmetric density is singular when
and only when either L′ = 0 or

L′ +

d
dy

log L′ = −

1
2y

(143)

Notice that theorem 12 provides an equation in y after replacing a par-
ticular function L = L(y). Theorem 12 can also be used to ﬁnd the functions
L(y) that will make the Hessian singular. Integrating (143) we obtain,

L(y) + log L′(y) = −

1
2

log(|y|) + c

and solving for L′, separating the variables and integrating we get,

L(y) = log(cid:18)aq|y| + b(cid:19)

(144)

(145)

where a and b are constants of integration. In terms of the density equation
(145) translates to,

f (z) = akz − µkV −1 + b

(146)

Hence, in the regions where the density is a straight line as a function of
r = kz − µkV −1 the Hessian is singular and estimation with best metrics is
not possible. Moreover, from (141) we can also obtain the regions of space
where the Hessian is positive and where it is negative deﬁnite. When L′ > 0,
H(z) will be negative deﬁnite provided that the matrix,

I − (L′ + L′′/L′)V −1(z − µ)(z − µ)T

(147)

8 CONCLUSIONS

30

is positive deﬁnite. But a matrix is positive deﬁnite when and only when all
its eigenvalues are positive. It is immediate to verify that ξ is an eigenvalue
for the matrix (147) if and only if (1− ξ) is an eigenvalue of the matrix (142).
The matrix (142) has rank one and therefore its only nonzero eigenvalue is
its trace so we arrive to,

Theorem 13 When,

L′ +

d
dy

log L′ < −

1
2y

(148)

H(z) is negative deﬁnite when L′ > 0 and positive deﬁnite when L′ < 0.
When,

(149)

L′ +

H(z) is indeﬁnite.

d
dy

log L′ > −

1
2y

For example when f (z) is multivariate Gaussian L(y) = y + c so that
L′ = 1 and the horizon is the surface of the V −1-sphere of radius one i.e.,
(z − µ)T V −1(z − µ) = 1. Inside this sphere the Hessian is negative deﬁnite
and outside the sphere the Hessian is indeﬁnite. The results in this section
can be applied to any other class of radially symmetric distributions, e.g.
multivariate T which includes the Cauchy.

8 Conclusions

We have shown the existence of optimal metrics in nonparametric density
estimation. The metrics are generated by the Hessian of the underlying
density and they are unique in the regions where the Hessian is deﬁnite. The
optimal metric can be expressed as a continuous function of the Hessian in
the regions where it is indeﬁnite. The Hessian varies continuously from point
to point thus, associated to the general class of density estimators (9) there is
a Riemannian manifold with the property that if the estimators are computed
based on its metric the best asymptotic mean square error is minimized. The
results are suﬃciently general to show that these are absolute bounds on the
quality of statistical inference from data.

The similarities with General Relativity are evident but so are the diﬀer-
ences. For example, since the Hessian of the underlying density is negative
deﬁnite at local maxima, it follows that there will be a horizon boundary

9 ACKNOWLEDGMENTS

31

where the Hessian becomes singular. The cross of the boundary corresponds
to a change of signature in the metric. These horizons almost always are null
sets and therefore irrelevant from a probabilistic point of view. However,
when the density is radially symmetric changing linearly with the radius we
get solid spots of singularity. There is a qualitative change in the quality of
inference that can be achieved within these dark spots. But unlike GR, not
only around local maxima but also around local minima of the density we
ﬁnd horizons. Besides, it is not necessary for the density to reach a certain
threshold for these horizons to appear. Nevertheless, I believe that the in-
fusion of new statistical ideas into the foundations of Physics, specially at
this point in history, should be embraced with optimism. Only new data will
(help to) tell.

There are many unexplored promising avenues along the lines of the sub-
ject of this paper but one that is obvious from a GR point of view. What is
missing is the connection between curvature and probability density, i.e. the
ﬁeld equation. I hope to be able to work on this in the near future.

The existence of optimal metrics in density estimation is not only of the-
oretical importance but of signiﬁcant practical value as well. By estimating
the Hessian (e.g. with kernels that can take positive and negative values, see
[21]) we can build estimators that adapt to the form of the optimal norm with
eﬃciency gains that increase with the number of dimensions. The antidote
to the curse of dimensionality!

9 Acknowledgments

I would like to thank my friends in the Maximum Entropy community spe-
cially Gary Erickson for providing a stimulating environment for this meeting.
I am also in debt to Ariel Caticha for many interesting conversations about
life, the universe, and these things.

References

[1] V. N. Vapnik, Statistical Learning Theory, John Wiley & Sons, Inc.,

1998.

[2] L. G. L. Devroye and G. Lugosi, A Probabilistic Theory of Pattern Recog-

nition, Springer, New York, 1996.

REFERENCES

32

[3] L. Devroye and L. Gy¨orﬁ, “No empirical probability measure can con-
verge in the total variation sense for all distributions,” Annals of Statis-
tics, 18, (3), pp. 1496–1499, 1990.

[4] A. R´enyi, “On the dimension and entropy of probability distributions,”

Acta Math. Acad. Sci. Hungar., 10, pp. 193–215, 1959.

[5] E. Jaynes, “Information theory and statistical mechanics,” Phys. Rev.,

106, p. 620, 1957. Part II; ibid, vol 108,171.

[6] B. R. Frieden, Physics from Fisher Information, a Uniﬁcation, Cam-

bridge University Press, 1998.

[7] C. C. Rodr´ıguez, “Are we cruising a hypothesis space?,” in Maximum
Entropy and Bayesian Methods, R. F. W. von der Linden, V. Dose and
R. Preuss, eds., vol. 18, (Netherlands), pp. 131–140, Kluwer Academic
Publishers, 1998. Also at xxx.lanl.gov/abs/physics/9808009.

[8] A. Caticha, “Change, time and information geometry,” in Maxi-
mum Entropy and Bayesian Methods, A. Mohammad-Djafari, ed.,
vol. 19, Kluwer Academic Publishers, 2000. too appear. Also at math-
ph/0008018.

[9] C. C. Rodriguez, “On a new class of multivariate density estimators,”
tech. rep., Dept. of Mathematics and Statistics, The University at Al-
bany, 1986. (http://omega.albany.edu:8008/npde.ps).

[10] C. C. Rodriguez, “The riemannian manifold induced by a density esti-
mator,” tech. rep., Dept. of Mathematics and Statistics, The University
at Albany, 1988. (http://omega.albany.edu:8008/rmide.html).

[11] K. Fukunaga and L. D. Hostetler, “Optimization of k-nearest-neighbor
density estimates,” IEEE Trans. on Information Theory, IT-19,
pp. 320–326, May 1972.

[12] R. D. Short and K. Fukunaga, “The optimal distance measure for nearest
neighbor classiﬁcation,” IEEE Trans. on Information Theory, IT-27,
pp. 622–637, September 1981.

[13] K. Fukunaga and T. Flick, “An optimal global nearest neigbor metric,”
IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-6,
pp. 314–318, May 1984.

REFERENCES

33

[14] K. Fukunaga and D. M. Hummels, “Bayes error estimation using parzen
and k-nn procedures,” IEEE Trans. on Pattern Analysis and Machine
Intelligence, PAMI-9, pp. 634–643, September 1987.

[15] J. P. Myles and D. J. Hand, “The multi-class metric problem in near-
est neighbour discrimination rules,” Pattern Recognition, 23, (11),
pp. 1291–1297, 1990.

[16] E. Fix and J. L. Hodges, “Discriminatory analysis. nonparametric dis-
crimination: Consistency properties,” Tech. Rep. 4 Project number 21-
49-004, USAF School of Aviation Medicine, Randolph Field, Tx., 1951.

[17] L. M. Le-Cam and G. Lo-Yang, Asymptotics in Statistics: Some Basic

Concepts, Springer series in statistics, Springer-Verlag, 1990.

[18] P. Diaconnis and D. Freedman, “On the consistency of bayesian esti-

mates (with discussions),” Ann. Stat., 14, (1), pp. 1–67, 1986.

[19] C. C. Rodr´ıguez, “Cv-np bayesianism by mcmc,” in Maximum Entropy
and Bayesian Methods, G. J. Erickson, ed., vol. 17, Kluwer Academic
Publishers, 1997. (physics/9712041).

[20] I. Ibragimov and R. Has’minskii, Statistical Estimation, vol. 16 of Ap-

plications of Mathematics, Springer-Verlag, 1981.

[21] R. S. Singh, “Nonparametric estimation of mixed partial derivatives of a
multivariate density,” Journal of Multivariate Analysis, 6, pp. 111–122,
1976.


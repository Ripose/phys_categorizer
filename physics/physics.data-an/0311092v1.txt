3
0
0
2
 
v
o
N
 
8
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
9
0
1
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

Statistical Issues in Particle Physics – A View from BaBar

F. C. Porter
(For the BaBar collaboration)
Lauritsen Laboratory of Physics, Caltech, Pasadena, CA 91125, USA

1

CALT 68-2463
SLAC-PUB-10243

The statistical methods used in deriving physics results in the BaBar collaboration are reviewed, with especial
emphasis on areas where practice is not uniform in particle physics.

1. Introduction

The purpose of the BaBar experiment at the PEP-
II accelerator at SLAC is to study e+e− collisions in
the 10 GeV center-of-mass region, namely the region
around B ¯B threshold.
In particular the program is
to investigate extensively CP violation and rare de-
cays of B mesons, as well as topics in charm and tau
physics.

Here, BaBar’s approach to statistical issues is sum-
marized. Emphasis is given to areas which are often
controversial.

2. BaBar Analysis Organization

∼

BaBar is a collaboration of approximately 600
physicists, from
80 institutions in a dozen coun-
tries. Thus, managing the production of physics re-
sults, from initial analysis to ﬁnal publication, while
maintaining collaboration involvement is a daunting
task. An organizational structure has been estab-
lished to facilitate this process, as illustrated in Fig. 1.
The “Statistics Working Group” was appointed by
the Publications Board in order to provide guidelines
and advice on statistical matters [1]. This group is
advisory; I’ll note how well the guidelines are actually
adopted in some cases.

3. Philosophy

The approach to choosing a statistical procedure is
to start by considering the goal. We adopt the view
that there are two broad domains in terms of goal:

•

The ﬁrst goal is that of summarizing the rel-
evant information in a measurement. This is
“descriptive” statistics. It is considered obliga-
tory to report such a description of the result
of the experiment. Inherent in this is the view
that it is actually useful to do so, a notion that
is not uniformly accepted. The use of frequency
statistics is recommended for this purpose. The

WEAT002

TALK

JOURNAL

   Paper
       or
  Analysis
Document

"Authors"

Statistics
Working
Group

Other
"Tools"
Groups

Publications Board

Analysis
Working
Group

Review
Committee

Designated
Insitutional
Reviewers

Collaboration
-Wide
Review

Figure 1: BaBar analysis organization. A detailed
analysis for some physics result is typically performed by
a subset of the collaboration, labelled “authors” here.
There are several layers of review that occur as an
analysis moves towards publication: an Analysis Working
Group interacts with the authors from the earliest stages;
once a document is produced, a Review Committee of
typically three people is assigned by the Publications
Board to critically examine the analysis; upon approval
from the Review Committee, the paper is circulated for
collaboration-wide review, including several institutions
specially designated to look closely at it. Oversight of the
process and ﬁnal review is carried out by the
Publications Board.

choice within the domain of possible frequency
statistics is driven by an emphasis on clarity and
the facility to compare and combine with other
measurements.

•

The second goal is that of interpreting the rel-
evant information in the context of making a

2

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

statement about “physics”. This is regarded as
optional, since once the relevant information is
available people are in principle able to do this
step for themselves. Because a statement about
physical reality may depend on other informa-
tion, and on theoretical input, Bayesian statis-
tics are recommended.

It may be remarked that there may be other goals,
such as making a decision concerning how to spend
money for the next experiment. This would involve,
beyond the above interpretive aspects, a consideration
of the risks and beneﬁts. We take the point of view
that this is outside the scope of the analysis and re-
porting of results, and hence do not discuss it further.

4. Statistical Practice in BaBar

We turn now to a review of the speciﬁc statistical
practices recommended or adopted in BaBar analy-
ses. Not included are the methods and tools used
for optimizing analyses, and pattern recognition, data
reduction, and simulation procedures. These matters
are crucial, but here we emphasize instead areas which
are traditionally more controversial. It should be men-
tioned that the typical products of a BaBar physics
analysis are:

1. “Best” estimates for physical parameters.

2. Interval estimates for physical parameters.

3. Signiﬁcance levels of observations (e.g., of a pos-

sible discovery).

4. Goodness-of-ﬁt of models to the data.

4.1. Blind Analysis

Many BaBar results are obtained in “blind analy-
ses”. The purpose of a blind analysis is to avoid the
introduction of bias, which could occur if the analyst
is looking at the results as the analysis is designed.
There is more than one approach to “blindness”, see
the talk by Aaron Roodman [2] for a summary of
BaBar practice. We’ll give one example here.

→

For example, consider the measurement of the rare
B decay B±
K ±e+e− [3], of interest because of
its sensitivity to possible physics beyond the standard
model. The basic idea of the analysis is to look for a
signal which peaks in the distribution of two kinematic
variables, known as “∆E” and “mES” (Fig. 2). A ﬁt
is performed to this two-dimensional distribution in
order to extract the strength of any signal present.
However, before performing the ﬁt, an event selection
is made in order to suppress backgrounds. In order to
avoid biasing the result by looking at the data while
tuning the selection, a blind analysis is performed.

WEAT002

Figure 2: Example of a blind analysis in BaBar. The
upper plot shows a Monte Carlo simulation of the signal
B± → K ±e+e− process. The outside boundaries delimit
the “large sideband region”; the intermediate box is the
“ﬁt region”, and the inner box is the region in which the
signal is concentrated (referred to as the “signal region”,
but in fact playing no special role in the analysis). The
lower plot shows the BaBar data after unblinding. Here,
the outside boundaries demarcate the ﬁt region, and the
smaller box is the “signal region”.

−

The ∆E

mES plane is divided into two regions:
a region where the ﬁt will be performed, which in-
cludes the region where a signal might appear; and
a larger (“large sideband”) region which excludes the
ﬁt region. During the tuning of the analysis, the data
may not be looked at in the ﬁt region, only in the
large sideband region. Monte Carlo and control sam-
ple data (including a type of data resembling signal)
are used to tune the analysis. Once the selection crite-
ria have been established, the ﬁt region of the data is
revealed, and the ﬁt performed to extract the result.
As BaBar is continuing to accumulate data, an issue
arises when it is desired to update a blind analysis to
include new data. In principle, one could simply add
the new data, without changing the analysis. How-
ever, this may be impractical, or undesirable. For
example, the entire dataset may be re-reconstructed
with improved constants or pattern recognition code.
Or, there may have been improvements in tools such
as particle identiﬁcation. One would like to incorpo-
rate the beneﬁt from such improvements. Addition-
ally, it might be desirable to work harder to optimize
the analysis, or to optimize on diﬀerent criteria, such

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

3

as precision instead of sensitivity. BaBar often takes
a practical compromise approach to incoporate new
data, and such improvements. We have the notion of
“re-blinding” the data, and re-optimizing. It is con-
sidered safe in this re-optimization to use variables
which have not been inspected too carefully in the
blind region in the ﬁrst dataset. Nonetheless, once we
have done this, we do not refer to the new result as
having been done with a blind analysis.

BaBar is perhaps the ﬁrst large HEP collaboration
to have embraced the blind methodology so enthusias-
tically. However, not every BaBar analysis is blind. In
particular, analyses which may be called exploratory
are generally not blinded. A recent example from
BaBar is the discovery of the D∗
sJ (2317)± [4], which
was not the result of a blind analysis. There are many
examples of people being led astray by such non-blind
exploratory analyses, so extreme caution is warranted.
The exploratory nature of such analyses makes it diﬃ-
cult to apply rigorous methodologies with well-deﬁned
statistical properties. It may not be impossible to do
better though [5].

4.2. Conﬁdence Intervals

The recommendation in BaBar is to use frequency
statistics for summarizing information (Sect. 3). The
goal is to describe what is observed, stressing simplic-
ity and coherence of interpretation, as well as facility
in combining with other results. With these crite-
ria, we think it can be counter-productive to impose
“physical” constraints. There is no reason to obscure
the observation of an “unlikely” result. Imposing con-
straints may also complicate combination of results.
Generally, the recommendation is to quote two-sided
68% conﬁdence intervals as the primary result. Where
there may be doubt, a check for frequency validity
(coverage) should be performed.

4.2.1. Example in Two Dimensions

As an example of the construction of a conﬁdence
region in a BaBar analysis, consider the measurement
of D mixing and doubly Cabibbo suppressed D de-
cays [6]. In this analysis, two parameters of interest
are to be determined, which may be expressed as x′
and y′ according to the relations:

x′

y′

≡

≡

∆m
Γ
∆Γ
2Γ

cos δ +

sin δ,

cos δ

−

sin δ,

∆Γ
2Γ
∆m
Γ

(1)

(2)

where m and Γ are the D mass and width, ∆m and
∆Γ are the (small) diﬀerences in masses and widths
between the two D mass eigenstates, and δ is an un-
known strong phase (between Cabibbo-favored and

WEAT002

doubly Cabibbo suppressed amplitudes). The mea-
surement is only sensitive to x′2 and y, and it is pos-
sible that the maximum of the likelihood will occur
at x′2 < 0 (“unphysical” region). At the current level
of sensitivity, we should ﬁnd a result consistent with
x′2 = y′ = 0, if the standard model is correct.

The construction of a conﬁdence region in the two-
dimensional (x′2, y′) plane, corresponding to 95% con-
ﬁdence level with the frequency interpretation, is per-
formed as follows (Fig. 3):
0 , y′
1. Pick a point (x′2

0) in the plane.

2. Form the “data” likelihood ratio comparing the
observed maximum likelihood with the likeli-
hood at (x′2

0 , y′

0):

λData = Lmax(Data)
0)(Data)
L(x′2

0 ,y′

.

3. Simulate many experiments with (x′2
as the true values of the parameters.

0 , y′

0) taken

4. For each Monte Carlo simulation form the “MC”

likelihood ratio:

(3)

(4)

λMC = Lmax(MC)
0)(MC)
L(x′2

0 ,y′

.

5. From the ensemble of simulations, determine the
probability P (λMC > λData). If this probability
is greater than 0.95, then the point (x′2
0) is
inside the contour; if less than 0.95, then the
point is outside the contour.

0 , y′

6. This procedure is repeated for many choices of
0) in order to map out the contour.

0 , y′

(x′2

Fig. 3 shows the result of this algorithm. The choice
was made to stop computng the contour at the bor-
der of the “physical” region. The computation could
in principle have been carried into the “unphysical”
region (up to technical diﬃculties of the sort we shall
discuss anon). It of course makes no diﬀerence to the
frequency interpretation whether it is extended into
the “unphysical” region or not.

4.2.2. Low Statistics Issues

Issues arise in applying the recommendation of al-
ways quoting a two-sided interval for a parameter
when the sampling is not from an approximate nor-
mal distribution. Most often this involves the low-
statistics regime of a counting process.

The ﬁrst issue is a technical one: it can happen that
a search in parameter space wants to go into a region
where the probability distribution is undeﬁned. This
is distinct from going into an “unphysical” region as
in the example above: we’ll call it crossing a “math
boundary”. As a simple example, consider the case of

4

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

single test point:
0.06
0.06

’
’

y
y

e

BABAR

preliminary

  

1

θ

0.04
0.04

0.02
0.02

0
0

-0.02
-0.02

-0.04
-0.04

-0.06
-0.06

-0.08
-0.08

-0.1
-0.1

   

  

  

 

  

 

θ

95% contour created
by toy MC sets in full
plane.

-1

0

1

Figure 4: Graph of the example sampling PDF for two
values of parameter θ: θ = 0.9, and “unphysical”
(negative signal) value θ = 1.1. Note that both values are
mathematically permissible.

-2
-2

-1
-1

0
0

1
1

2
2

3
3

4
4
/ 10
/ 10

5
5
-3
-3

2x’
2x’

Converged point
for fit to data.
T est point of toy
Monte Carlo set.

Figure 3: Finding a conﬁdence contour in two
dimensions [7]. The large ﬁlled dot shows the location of
the maximum likelihood for the BaBar data. The open
dot shows the value of (x′2
0 , y′
0) chosen for a simulation.
The small dots show simulated experiments for which
λMC > λData. The pluses, as well as the arrows pointing
oﬀscale, show simulated experiments for which
λMC < λData. The 95% contour resulting from the
algorithm described in the text is shown. The shaded
region is the “unphysical” region. Note that the
evaluation of the maximum likelihood is not restricted to
the “physical” region.

a normal “signal” on a ﬂat “background”, with PDF
(Fig. 4):

p(x; θ) =

+

θ
2

1
θ
−
A√2πσ

2

− x

e

2σ2 , x

1, 1).

(5)

(
−

∈

The parameter of interest is the strength of the signal,
here expressed as 1
θ, the probability of sampling a
signal event. An experiment samples N events from
this distribution, with likelihood function:

−

N

i=1
Y

(θ;

xi, i = 1 . . . , N

) =

p(xi; θ).

(6)

L

{

}

It is quite possible that the likelihood will be maxi-
mal for a value of θ for which the PDF is not deﬁned.
The function p(x; θ) may become negative in some re-
gion of x.
If there are no events in this region, the
likelihood is still “well-behaved”. However, the re-
sulting ﬁt, as a description of the data, will typically
look poor even where the PDF is positive. This is
considered unacceptable.

2.0

1.5

1.0

)
x
(
p

0.5

0.0

-1

-0.5

-1.0

-0.5

0

0.5

1

Figure 5: Example of a possible dataset generated
according to the ﬂat background plus normal signal
PDF. The data are displayed in histogram form by the
points. The curve that goes negative (and is cut oﬀ at
the plot boundary) is the result of the (unbinned)
maximum likelihood ﬁt. The other curve is the result of
the same ﬁt, except with the constraint that it cannot
become negative.

An illustration of a possible sampled dataset from
this distribution is shown in Fig. 5, displayed as a
histogram. An (unbinned) maximum likelihood ﬁt to
this data gives an estimate for θ in a region outside the
math boundary. The graph of the “PDF” curve for
this estimate does not give a good representation of
the data. On the other hand, if the ﬁt is constrained
to the math region, the graph of the PDF curve looks
like a reasonable representation of the data.

Thus, we suggest as a practical resolution to this
problem to constrain the ﬁt to remain within bounds
such that the PDF is everywhere legitimate (n.b., pa-
rameters may still be “unphysical”). Experience is
that this gives ﬁts which “look” like the data, as in the
present example, Fig. 5. This same practical recom-
mendation applies in interval evaluation (but coverage
should be checked, as always).

Another issue that arises frequently in low statis-

WEAT002

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

5

tics (Poisson) sampling may be expressed in the form
of the following example: A “cut and count” anal-
ysis for a branching fraction B ﬁnds n events. The
mean expected background contribution is estimated
as ˆb
σb events. The eﬃciency and parent sample
are estimated to give a scale factor (relating observed
signal events to B) of ˆf
σf . The problem is to deter-
mine a conﬁdence interval (at 68% conﬁdence, say), in
the frequency sense, for B.

±

±

We’ll assume that n is sampled from a Poisson dis-
= f B + b, that ˆb is sam-
n
tribution with mean µ =
h
i
pled from a normal distribution, N (b, σb), and that ˆf
is sampled from a normal distribution, N (f, σf ). Thus
the likelihood function is:

(n, ˆb, ˆf ; B, b, f ) =

µne−µ
n!

1
2πσbσf

e

− 1
2

ˆb−b
σb

2

− 1
2

ˆf −f
σf

2

.

L

(cid:17)
(7)
It should be noted that this example is realistic, aris-
ing in practice (to a good approximation). A variant
is to assume a normal distribution in

(1/f )

(cid:16)

(cid:0)

(cid:1)

Several methods have been proposed, and used, for
dealing with this problem (see Ref. [8] for further dis-
cussion of these):

d

1. Just give n, ˆb

σb, ˆf

±

σf . This provides a com-
plete summary of the relevant information, and
should be done anyway. But it isn’t a conﬁdence
interval for B.

±

2. Integrate out the nuisance parameters according

to

L

(n, ˆb, ˆf ; B) =

(8)
2

.

df

db

Z

Z

µne−µ
n!

1
2πσbσf

− 1
2

e

ˆb−b
σb

2

− 1
2

ˆf −f
σf

(cid:0)

(cid:1)

(cid:0)

(cid:1)

This is easy, and often done. It may be inter-
preted as a partially Bayesian approach, where
a uniform prior has been assumed for f and b.
The frequency properties could be investigated,
but usually aren’t.

3. A very common approach when quoting upper
limits is to do the appropriate Possion statisti-
cal analysis for n, but with the scale and back-
ground parameters ﬁxed at the estimated values
shifted by one standard deviation (in the direc-
tion to make the limit higher than with the cen-
tral values). This has the beneﬁt of being very
easy to do, but it is clearly ad hoc, and the cov-
erage is usually not investigated.

Here, I would like to comment on the possibility of
evaluating these conﬁdence intervals in another way.
The method I consider is actually a very common
method that seems to have been rather neglected as
an approach to the present problem. The algorithm is

WEAT002

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
c
n
e
u
q
e
r
f
 
e
g
a
r
e
v
o
C

B=0
B=1
B=2
B=3
B=4
Normal

0

0

1

2
3
Delta(-lnL)

4

5

Figure 6: Coverage frequency as a function of ∆ for
f = 1, σf = 0.1, b = 0.5, σb = 0.1. There are several
curves corresponding to diﬀerent numbers of expected
signal events, B. The smoothest curve is the coverage in
the high statistics (normal) limit.

as follows: First, ﬁnd the global maximum of the like-
lihood function with respect to B, f, b. Then search in
the B parameter for the point where
increases
from the minimum by a speciﬁed amount (perhaps by
∆ = 1/2 for a 68% conﬁdence interval), making sure
that the likelihood is re-maximized with respect to f
and b during this search. The resulting points Bℓ, Bu
then give an estimated interval for parameter B which
we would like to be a conﬁdence interval.

ln

−

L

The question, of course, is: Does it work? To an-
swer this, we need to investigate the frequency prop-
erty of the algorithm. For large statistics (i.e., the
normal limit) we know it works — for ∆ = 1/2 this
method produces a 68% conﬁdence interval for B. We
expect that it will fail in the extreme small statistics
limit, and the question becomes a quantitative one of
how far it can be pushed into the low statistics regime.
We answer this with Figs. 6–10.

Figure 6 shows the dependence of the coverage of
this algorithm on the value of ∆, for several values
of B and an expected background of 1/2 event. The
branching fraction scale is adjusted so that B may
be interpreted as the mean number of signal events.
It may be seen that ∆ = 1/2 gives coverage reason-
ably close to 68% for B
2. Figure 7 shows the
≥
coverage for B = 0, for several backgrounds. Even at
zero branching fraction, the ∆ = 1/2 coverage is fairly
2. Note
close to 68% for expected backgrounds b
that extending this to intervals with higher conﬁdence
may result in diﬀerent conclusions.

≥

It may be remarked that uncertainties in the back-
ground and/or scale factor help to obtain the desired
coverage (Figs. 8 and 9). This is because they smooth
out the eﬀect of the discreteness of the Poisson sam-
pling space.

One issue is when the coverage is deemed to be

6

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

0 0.5

1

1.5

3.5

4 4.5

5

2
3
2.5
Delta(- ln L)

2

4

6

8

10

f

Figure 7: Coverage frequency as a function of ∆ for
B = 0, f = 1, σf = 0, σb = 0.1. There are several curves
corresponding to diﬀerent numbers of expected
background events, b. The smoothest curve is the
coverage in the high statistics (normal) limit.

Figure 9: Dependence of coverage on scale factor f and
σf for B = 1, b = 2, σb = 0, ∆ = 1/2. There are several
curves corresponding to diﬀerent values of σf , becoming
smoother as σf increases. The horizontal line is at 68%.

1.2

1

y
c
n
e
u
q
e
r
f
 
e
g
a
r
e
v
o
C

0.8

0.6

0.4

0.2

0

0.8

0.6

0.4

0.2

y
c
n
e
u
q
e
r
f
 
e
g
a
r
e
v
o
C

0

0

b = 1
b = 2
b = 3
b = 4
b = 5
Normal

sigma_b = 0
sigma_b = 10%
sigma_b = 20%
sigma_b = 30%
sigma_b = 40%
0.683

1

0.8

0.6

0.4

0.2

y
c
n
e
u
q
e
r
f
 
e
g
a
r
e
v
o
C

0

0

1

y
c
n
e
u
q
e
r
f
 
e
g
a
r
e
v
o
C

0.8

0.6

0.4

0.2

0

0

sigma_f = 0
sigma_f = 10%
sigma_f = 20%
sigma_f = 30%
sigma_f = 40%
0.683

sigma_f = 0
sigma_f = 10%
sigma_f = 20%
sigma_f = 30%
sigma_f = 40%
0.683

5

10

b

15

20

5

10
b

15

20

Figure 8: Coverage frequency as a function of mean
background b for B = 0, f = 1, σf = 0, ∆ = 1/2. There
are several curves corresponding to diﬀerent values of σb,
becoming smoother as σb increases. The horizontal line is
at 68%.

Figure 10: Coverage as a function of expected
background for ∆ = 0.8, B = 0, f = 1, σb = 0. There are
several curves corresponding to diﬀerent values of σb,
becoming smoother as σb increases. The horizontal line is
at 68%.

“good enough”. It might be suggested that if the cov-
erage is known to be within some amount, say 5%
of 68%, that this is good enough for anything we are
going to use those numbers for. However, one could
also decide to take a “conservative” approach, and in-
sist that the coverage be at least at the quoted level.
One way to accomplish this is to shift the value of ∆.
Fig. 10 shows the coverage as a function of expected
background (in the worst-case of zero signal branch-
ing fraction and σb = 0) for a value of ∆ = 0.8. We
see that at least 68% coverage is guaranteed as long
as the mean background is greater than 1.4.

We’ll conclude this discussion with a few summary
it is a good idea to always quote
remarks: First,
n, ˆb
σb, and ˆf
σf . Second, any approach used
should be justiﬁed with a computation of the cover-

±

±

age. The likelihood analysis studied here works pretty
well even down to rather low statistics for 68% con-
ﬁdence intervals. It should be kept in mind however
that “good enough” for 68% intervals does not imply
good enough for other purposes, such as tests of signif-
icance. Finally, if σb
f this is outside the
b or σf
regime studied here; the normal assumption is likely
invalid in this case.

≈

≈

4.2.3. Interpretation Intervals

In the interpretation stage, Bayesian intervals may
be given, as deemed useful to the consumer. In BaBar
practice, this is typically done when someone wants to
give an upper limit, and is usually implemented with
the assumption of a uniform prior in the parameter of

WEAT002

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

7

interest. BaBar recognizes the issues surrounding the
choice of prior. The recommendation is to consider
it carefully, and to make checks on how sensitive the
result is to the choice. Even this recommendation is
not routinely adopted however.

4.3. Signiﬁcance

The “signiﬁcance” of an observation (e.g., of the
presence of a signal for some process) is deﬁned as the
probability of the observed deviation (or larger) from
the null (no signal) model, under the null hypothesis.
The recommended procedure in BaBar is to compute
this probability according to the frequentist method-
ology. It may be noted that knowing the 68% conﬁ-
dence interval does not always provide much insight
into the signiﬁcance. The tails of the null sampling
distribution may be non-normal. A separate analysis
is generally required, in which the tails are appropri-
ately modelled.

No recommendation is tendered for when to label
a result as “signiﬁcant”. We struggled with possible
algorithms, but eventually gave up, because such a
label implies an interpretation. No uniform prescrip-
tion seems to make sense; judgement is involved. For
example, deciding that the observation of a bizarre
new particle is signiﬁcant may involve a diﬀerent stan-
dard than the claim that an expected decay mode
It isn’t re-
of an established particle is signiﬁcant.
ally our primary role as experimenters;
it is up to
the reader ultimately to decide what they wish to be-
lieve. This is perhaps the least-accepted of the Statis-
tics Working Group’s points in BaBar: people insist
on making qualitative statements, e.g., “observation
of”, “evidence for”, “discovery of”, “not signiﬁcant”,
“consistent with”. A code exists in which “observa-
tion of” becomes quantiﬁed as > 4σ signiﬁcance, and
“evidence for” means > 3σ.

This preoccupation with qualitative interpretive
terminology is pervasive beyond BaBar. For example,
the following excerpt appeared in Physics Today [9],
(italics mine, references deleted):

“In March, back-to-back papers in Physical
Review Letters reported the measurement of
CP symmetry violation in the decay of neu-
tral B mesons by groups in Japan and Cal-
ifornia. Now the word “measurement” has
been replaced by “observation” in the titles of
two new back-to-back reports by these same
groups in the 27 August Physical Review Let-
ters. That is to say, with a lot more data and
improved event reconstruction, the BaBar col-
laboration at SLAC and the Belle collabora-
tion at KEK in Japan have at last produced
the ﬁrst compelling evidence of CP violation in
any system other than the neutral K mesons.”

For another example, some people think a measure-

WEAT002

ment should not be called a “measurement” unless
the result is signiﬁcantly diﬀerent from zero. An edi-
tor at a prominent journal has suggested that “bounds
on” might be more appropriate than “measurement”
in reference to a CP asymmetry angle which was ob-
served as consistent with zero. This can lead to amus-
0.01 would be an
ing ironies: Finding sin 2β = 0.00
exciting contradiction with the standard model. But
it isn’t a “measurement”?

±

A further issue that arises is that many people mix
the question of signiﬁcance with the choice of interval
(i.e., one-sided vs two-sided). This has a drawback,
because basing how one quotes the interval based on
the result of the measurement can introduce a bias.
The algorithm of Feldman and Cousins [10] is de-
signed to address this. However, this methodology
is not adopted in BaBar because of the constraint on
the physical region, as discussed earlier. Instead, our
recommendation is to always give a two-sided interval
(if otherwise appropriate), independent of the signiﬁ-
cance. The signiﬁcance is quoted separately. Quoting
a one-sided interval may optionally also be done, and
is usually regarded as part of the interpretation (hence
a Bayesian approach is suggested). This recommen-
dation is typically followed in BaBar, but there have
been exceptions.

Another issue that arises in the quoting of signiﬁ-
cance has to do with the tradition of quoting signiﬁ-
cance as nσ. Unfortunately, this is used to mean dif-
ferent things: Sometimes it actually means n standard
deviations. But sometimes it means the probability
content of an nσ ﬂuctuation for a normal distribu-
tion. We recommend to quote directly the probability
if the sampling distirbution is not normal. However,
this has met with very limited implementation.

4.4. Systematic Uncertainties

BaBar makes many checks in a typical analysis. For
the purpose of deﬁning systematic uncertainties, we
divide these into two broad categories:

1. “Blind checks”: This is a test for mistakes. No
correction to the data is anticipated. If the test
passes, then there is no contribution to the sys-
tematic error. An example of such a check is
dividing the data into two chronological subsets
and comparing the results.

2. “Educated checks”: This is a measurement of
biases or corrections, and may aﬀect the quoted
result. It involves a contribution to the system-
atic error. An example is the model dependence
of the eﬃciency calculation.

It is recommended that the systematic uncertainty
be quoted separately from the statistical uncertainty.
The sources of systematic uncertainty should be de-
scribed, and may contain statistical components, for

8

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

’
’
y
y

0.04
0.04

0.02
0.02

-0
-0

-0.02
-0.02

-0.04
-0.04

-0.06
-0.06

2
Physical (x’
, y’)
2
Central (x’
, y’) No CPV
95% CL CPV allowed
95% CL CPV allowed, stat only
95% CL CP conserved
95% CL CP conserved, stat only

-0.5
-0.5

0
0

0.5
0.5

1
1

1.5
1.5

2
2
2x’
2x’
 / 10
 / 10

2.5
2.5
-3
-3

Figure 11: Incorporating systematic uncertainties in the
conﬁdence contour for D mixing. The ﬁlled dot is the
location of the best ﬁt point, (ˆx′2, ˆy′), the open circle the
best ﬁt point in the physical region. The solid contour
(dotted if restricting to CP conserving models) shows the
95% conﬁdence contour according to statistical errors
only. The dot-dash contour (dash for CP conserving
models) shows how this contour becomes scaled on
incorporating systematic uncertainties.

example due to limited Monte Carlo statistics in the
eﬃciency evaluation.

We return to our earlier example (Sec. 4.2.1) of
D mixing for an example of the treatment of sys-
tematic uncertainties. The goal here is to produce a
two-dimensional conﬁdence contour in the parameter
space which incorporates the systematic uncertainites.
In this case, the statistical uncertainties are large, and
we are willing to accept an approximation in order
to keep the procedure simple. Thus, it is decided to
use a method which takes the statistics-only contour
and scales it uniformly along rays from the best ﬁt
i , where mi is
value. The scaling factor is
an estimate of systematic uncertainty i in units of the
statistical uncertainty. This estimate is obtained by
determining the eﬀect of the systematic uncertainty
on ˆx′2, ˆy′ (the position of the best ﬁt). Figure 11
shows the result of this procedure. This method is
conservative (or lazy) in the sense that scaling for a
given systematic in one (worst case) direction is ap-
plied uniformly in all directions. On the other hand,
by evaluating the error at the best ﬁt position, a linear
approximation is being made.

1 +

m2

P

p

4.5. Goodness of Fit

There appears to be no perfect general goodness-
of-ﬁt test. Given a dataset generated under the null
hypothesis, one can usually ﬁnd a test which rejects
the null hypothesis (and this may be taken as a warn-
ing that choosing the test after you see the data is

WEAT002

B0 tags
− 0 tags
B

Background

s
p
 
6
.
0
 
/
 
s
e
i
r
t
n
E

150

100

50

y 0
0.5

r
t
e
m
m
y
s
A
w
a
R

 

0

-0.5

-5

0

5

∆t (ps)

Figure 12: Measurement of CP violation (BaBar) [11].
The upper plot shows the measurement (points) of the
time distributions for B0 and ¯B0 decays to selected CP
eigenstates. The curves show the result of a maximum
likelihood ﬁt to the data. The lower plot shows the
time-dependent asymmetry between the B0 and ¯B0
decays, again with the ﬁtted curve overlaid. The
asymmetry would be zero in the absence of CP violation.

dangerous). Given a dataset generated under an al-
ternative hypothesis, one can usually ﬁnd a test for
It seems advisable to think
which the null passes.
about what one wants to test for in choosing the test.
For example, Fig. 12 shows data used in a measure-
ment of CP violation by BaBar. A likelihood ratio
(or a chi-square) test of the time distribution may be
a good test for the lifetime ﬁt to the data, but it may
have little sensitivity to testing the goodness-of-ﬁt of
the CP asymmetry, which is a low-fequency question.
So far, BaBar generally uses likelihood ratio tests
or chi-square tests if appropriate. The Kolmogorov-
Smirnov test is also used. If a test statistic such as the
likelihood ratio is used, then a Monte Carlo evaluation
of the distribution of the statistic is recommended,
rather than assuming an asymptotic property.

4.6. Consistency of Analyses

BaBar has encountered several times the question
of whether a new analysis is consistent with an old
analysis. Often, the new analysis is a combination of
additional data plus changed (improved) analysis of
original data. The stickiest issue is handling the cor-
relation in testing for consistency in the overlapping
data. People sometimes have diﬃculty understanding
that statistical diﬀerences can arise even comparing
results based on the same events, so we expound on
this.

Given a sampling ˆθ1, ˆθ2 from a bivariate normal dis-
= θ, the
=
ˆθ1 is N (0, σ)-distributed with

tribution N (θ, σ1, σ2, ρ), with
diﬀerence ∆θ

ˆθ2i
h

ˆθ1i
h

ˆθ2 −

≡

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

9

2 −

1 + σ2

σ2 = σ2
2ρσ1σ2. If the correlation is unknown,
all we can say is that the variance of the diﬀerence is
σ2)2 . . . (σ1 + σ2)2. If we at least
in the range (σ1 −
believe ρ
0 then the maximum variance of the dif-
≥
1 + σ2
ference is σ2
2.

Suppose we measure a neutrino mass, m, in a sam-
ple of n = 10 independent events. The measurements
are xi, i = 1, . . . , 10. Assume the sampling distribu-
tion for xi is N (m, σi).

We may form unbiased estimator, ˆm1, for m:

ˆm1 = 1
n

n
i=1 xi

1
n2

n
i=1 σ2
i .

(9)

±

q

P

P

±

The result (from a Monte Carlo simulation) is ˆm1 =
0.058

0.039.

Then we notice that we have some further informa-
tion which might be useful: we know the experimen-
tal resolutions, σi for each measurement. We form
another unbiased estimator, ˆm2, for m:

ˆm2 =

n
i=1

xi
σ2
i

/

n
i=1

1
σ2
i ±

1/

n
i=1

1
σ2
i

.

(10)

P

P

qP

0.016.

The result (from the same simulation, i.e., from the
same events) is ˆm1 = 0.000

±
The results are certainly correlated, so the question
of consistency arises (we know the error on the diﬀer-
ence is between 0.023 and 0.055). In this example, the
0.036, where
diﬀerence between the results is 0.058
the 0.036 error includes the correlation (ρ = 0.41).

±

Art Snyder has developed an approximate formula
for evaluating the correlation in a comparison of max-
imum likelihood analyses. Suppose we perform two
maximum likelihood analysis, with event likelihoods
L1,
L2, on the same set of N events [n.b., we may use
diﬀerent information in each analysis]. The results are
estimators ˆθ1, ˆθ2 for parameter θ (restricting to the
one-dimensional case for simplicity). The correlation
coeﬃcient ρ may be estimated according to:

ρ

≈

N
i=1 Ri

d ln L1i
dθ

d ln L2i
dθ

|θ= ˆθ1

N
P
i=1

d2 ln L1i
dθ2

θ=θ0
|

r(cid:16)P

|θ= ˆθ2
d2 ln L2i
dθ2

N
i=1

,

θ=θ0
|

(cid:17)
(11)

(cid:17) (cid:16)P

where (θ0 is an expansion reference point):

Ri =

1

(ˆθ1 −

−

θ0)

d2 ln
dθ2

L1i

"

1

"

(ˆθ2 −

−

θ0)

d2 ln
dθ2

L2i

θ=θ0
|

,

θ=θ0
|

,

d ln

L1i
dθ

d ln

L2i
dθ

θ=θ0
|

#

.

θ=θ0
|

#

If θ0 ≈

ˆθ1 ≈

ˆθ2, then

˜σθ1 ˜σθ2

ρ

≈

d ln

L1i
dθ

θ=θ0
|

d ln

L2i
dθ

θ=θ0,
|

(12)

N

i=1
X

WEAT002

where ˜σ2

1/

θk ≡

N
i=1

2

.

θ=θ0

dLki
dθ |

P

Let us look at a real example of the consistency
question in a BaBar analysis, the measurement of the
CP -violation parameter sin 2β. In August 2001, we
106 B ¯B
published a result based on a dataset of 32
pairs [12]:

×

(cid:1)

(cid:0)

sin 2β = 0.59

0.14(stat)

0.05(syst)

(13)

±

±

±

±

An updated result was produced in March 2002, based
on 62

106 B ¯B pairs [13]:

×
sin 2β = 0.75

0.09(stat)

0.04(syst)

(14)

The second result includes the earlier data,
re-
reconstructed. The analysis is not simply counting
events; it involves multivariate maximum likelihood
ﬁts, reprocessing changes, and relative likelihoods for
an event to be signal or background, for example. The
question is, are the two results statistically consistent?
If these were independent data sets, a diﬀerence of
0.16
0.17 would not be a worry. The issue is the cor-
relation. A specialized analysis deriving from Eqn. 11
is performed on the events in common between the
two analyses. A correlation of ρ = 0.87 is deduced,
yielding a diﬀerence of
2.2σ. This corresponds to
a probability of 3%, which is small enough that we
noticed, and looked hard for possible systematic prob-
lems, but not so small to be alarming, especially in an
experiment with many such tests being made.

±

∼

There has been some impression that BaBar may
be seeing more diﬀences between old vs updated re-
sults than people are used to, and the question arises
whether BaBar is making mistakes. The answer to
this seems to be, ﬁrst of all, based on studies such
as the above, there is no compelling statistical evi-
dence to support the contention that mistakes are be-
ing made. There should be diﬀerences, purely due to
statistical ﬂuctuations, among results, and BaBar sees
nothing clearly beyond what might be expected from
statistics. The second part of the answer is a specu-
lation to why the impression may exist. BaBar is dif-
ferent from most other experiments in that it makes
extensive use of the blind methodology. There is lit-
tle opportunity to react to observed diﬀerences with
further changes in analysis. Without using the blind
methodology, there is the potential for bias, tending
towards making results agree with earlier results bet-
ter than they should.

5. Reﬂections

It is my observation that statistical sophistication
in particle physics (not speciﬁc to BaBar) has grown
signiﬁcantly, not so much in the choice of methods,
which are often long-established, but in the under-
standing attached to them. People now understand
that there is a choice of approach between Bayesian

10

PHYSTAT2003, SLAC, Stanford, California, September 8-11, 2003

References

and frequency statistics, though there is yet no uni-
form agreement on adoption. There is also consider-
able awareness on the issue of biases in analyses, for
example, BaBar now relies heavily on blind method-
ology.

BaBar adopts frequency statistics for describing re-
sults, and much attention is devoted to Monte Carlo
validation and veriﬁcation of coverage. The use of
the Bayesian approach in high energy physics, includ-
ing BaBar, is still not mature: There is no established
methodology for choosing the prior distribution, other
than to default on a uniform prior. The justiﬁca-
tion for this is basically that it usually doesn’t matter
very much. There are, however, even issues still in
frequency statistics. Controversies involve such no-
tions as restricting to the “physical region”, or that
the presence of backgrounds should “always” lead to
higher upper limits. Both of these notions are not a
concern in the BaBar recommendations.

BaBar is attempting to provide a coherent, docu-
mented approach to its use of statistics in its results.
This is very much a work in progress.

Acknowledgments

I would like to thank Louis Lyons for organizing an
informative and stimulating conference. I am grateful
to my collaborators on BaBar for many interesting dis-
cussions of statistical issues. Work supported in part
by Department of Energy grant DE-FG03-92ER40701
and contract DE-AC03-76SF00515.

[1] BaBar

Statistics

Working

Group,

http://www.slac.stanford.edu/BFROOT/www/Statistics/index.html
[2] A. Roodman, http://www-conf.slac.stanford.edu/phystat2003/talks/

[5] B. Knuteson, http://www-conf.slac.stanford.edu/phystat2003/talks/

PHYSTAT2003 (2003).

[3] J. Walsh, First International Conference on Fla-
vor Physics and CP Violation (FPCP 2002),
http://www.hep.upenn.edu/FPCP/.

[4] B. Aubert et al. (BaBar collaboration), Phys.

Rev. Lett., 90, 242001 (2003).

PHYSTAT2003 (2003).

[6] B. Aubert et al. (BaBar collaboration), Phys.

Rev. Lett., 91, 171801 (2003).

[7] U. Egede, International Workshop on Frontier

Science, Frascati, October 6-11, 2002.

[8] R. Barlow, “A Calculator for Conﬁdence Inter-

vals”, MAN/HEP/2001/04.

[9] B.

Schwarzschild,

Physics

Today,

http://www.physicstoday.org/pt/vol-54/iss-9/p19.html.

[10] G. Feldman and R. Cousins, Phys. Rev. D, 57,

3873 (1998).

[11] B. Aubert et al. (BaBar collaboration), Phys.

Rev. Lett., 89, 201802 (2002).

[12] B. Aubert et al. (BaBar collaboration), Phys.

Rev. Lett., 87, 091801 (2001).

[13] B. Aubert et al. (BaBar Collaboration), SLAC-

PUB-9153 (2002).

WEAT002


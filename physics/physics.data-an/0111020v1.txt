1
0
0
2
 
v
o
N
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
2
0
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Model selection for inverse problems: Best choice
of basis functions and model order selection

A. Mohammad-Djafari

Laboratoire des Signaux et Systèmes
Supélec, Plateau de Moulon, 91192 Gif–sur–Yvette Cedex, France

Abstract. A complete solution for an inverse problem needs ﬁve main steps: choice of basis func-
tions for discretization, determination of the order of the model, estimation of the hyperparameters,
estimation of the solution, and ﬁnally, characterization of the proposed solution. Many works have
been done for the three last steps. The ﬁrst two have been neglected for a while, in part due to the
complexity of the problem. However, in many inverse problems, particularly when the number of
data is very low, a good choice of the basis functions and a good selection of the order become
primary. In this paper, we ﬁrst propose a complete solution within a Bayesian framework. Then, we
apply the proposed method to an inverse elastic electron scattering problem.

INTRODUCTION

In a very general linear inverse problem, the relation between the data y = [y1, · · · , ym]t
and the unknown function f (.) is

yi =

hi(r) f (r) dr,

i = 1, · · · , m,

ZZ

where hi(r) is the system response for the data yi. We assume here that the hi(r) are
known perfectly. The ﬁrst step for any numerical processing is the choice of a basis
function bj(r) and an order k, in such a way to be able to write

f (r) =

xj bj(r).

k

Xj=1

y = Ax + ǫ

This leads to

with y = [y1, · · · , ym]t, x = [x1, · · · , xk]t and

Ai,j =

hi(r) bj(r) dr,

i = 1, · · · , m, j = 1, · · · , k,

ZZ

where ǫ = [ǫ1, · · · , ǫm]t represents the errors (both the measurement noise and the mod-
eling and the approximation related to the numerical computation of matrix elements
Ai,j). Even when the choice of the basis functions bi(r) and the model order k is ﬁxed,
obtaining a good estimate for x needs other assumptions about the noise ǫ and about

(1)

(2)

(3)

(4)

x itself. The Bayesian approach provides a coherent and complete framework to handle
the random nature of ǫ and the a priori incomplete knowledge of x.

The ﬁrst step in a Bayesian approach is to assign the prior probability laws
p(y | x, φ, k, l) = pǫ(y − Ax | φ, k, l), p(x | ψ, k, l), p(φ | k, l) and p(ψ | k, l), where
pǫ(y − Ax|φ, k, l) is the probability law of the noise, and (φ, ψ) the hyperparameters of
the problem. Note that x represents the unknown parameters, k = dim(x) is the order of
the model, m = dim(y) is the number of the data and l is an index to a particular choice
of basis functions. Note that the elements of the matrix A depend on the choice of the
basis functions. However, to simplify the notations, we do not write this dependence
explicitly. We assume that we have to select one set l of basis functions among a ﬁnite
set (indexed by [1 : lmax]) of them. Thus, for a given l ∈ [1, lmax] and a given model
order k ∈ [1, kmax], and using the mentioned prior laws, we deﬁne the joint probability
law

p(y, x, φ, ψ | k, l) = p(y | x, φ, k, l) p(x | ψ, k, l) p(φ | k, l) p(ψ | k, l).

(5)

From this probability law, we obtain, either by integration or by summation, any
marginal law, and any a posteriori probability law using the Bayes rule.
What we propose in this paper is to consider the following problems:

• Parameter estimation:

where

b

x = arg max

p(x | y,

φ,

ψ,

k,

l)

,

x

n

o

b

b

b

b

p(x | y, φ, ψ, k, l) = p(y, x | φ, ψ, k, l) / p(y | φ, ψ, k, l),

p(y, x | φ, ψ, k, l) = p(y | x, φ, k, l) p(x | ψ, k, l)

p(y | φ, ψ, k, l) =

p(y, x | φ, ψ, k, l) dx.

• Hyperparameter estimation:

ZZ

(

φ,

ψ) = arg max

p(φ, ψ | y,

k,

l)

,

(φ,ψ) n

b

b

o

b

b

p(φ, ψ | y, k, l) = p(y, φ, ψ | k, l) / p(y | k, l)

• Model order selection:

ZZ ZZ

p(y | k, l) =

p(y, φ, ψ | k, l) dφ dψ.

and

where

and

where

and

k = arg max

p(k | y,

l)

,

k

n

b

o

b

p(k | y, l) = p(y | k, l) p(k)/ p(y | l)

p(y | l) =

p(y | k, l) p(k).

kmax

Xk=1

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

• Basis function selection:

where

and

l = arg max

{p(l | y)} ,

l

b

p(l | y) = p(y | l) p(l)/ p(y)

p(y) =

p(y | l) p(l).

lmax

Xl=1

(16)

(17)

(18)

• Joint parameter, hyperparameter, model order and basis function estimation:

(

x,

φ,

ψ,

k,

l) = arg max
(x,φ,ψ,k,l)

b

b

b

b

b

{p(y, x, φ, ψ | k, l) p(k) p(l)} .

(19)

As it can be easily seen, the ﬁrst problem is, in general, a well posed problem and the
solution can be computed, either analytically or numerically. The others (except the last)
need integrations. These integrals can be done analytically only in the case of Gaussian
laws. In other cases, one can either use a numerical integration (either deterministic or
stochastic) or to resort to approximations such as the Laplace method which allows us
to obtain a closed-form expression for the optimality criterion.

Here, we consider these problems for the particular case of Gaussian prior laws:

p(y | x, φ, k, l) = N

Ax,

= (2π/φ)

−m/2 exp

φ ky − Axk2

(20)

p(x | ψ, k, l) = N

= (2π/ψ)

−k/2 exp

−

ψ kxk2

,

(21)

1
φ

I

!

 

 

0,

1
ψ

I

!

1
2

−
(cid:20)
1
2

(cid:20)

(cid:21)

where 1

φ and 1

ψ are respectively the variance of the noise and the parameters.

PARAMETER ESTIMATION

First note that in this special case we have

p(y, x | φ, ψ, k, l) = (2π/φ)

−m/2 (2π/ψ)

−k/2 exp

−

φ ky − Axk2 −

ψ kxk2

.

(22)

1
2

(cid:20)

1
2

Integration with respect to x can be done analytically and we have:

p(y | φ, ψ, k, l) =

p(y, x | φ, ψ, k, l) dx = N (0, P y) ,

ZZ

with

ψ
φ
It is then easy to see that the a posteriori law of x is also Gaussian:

(AAt + λI) and λ =

AAt +

P y =

I =

1
ψ

1
ψ

1
φ

.

(23)

(24)

p(x | y, φ, ψ, k, l) = N

x,

P

with

P =

(AtA + λI)−1 and

x = φ

P Aty.

(25)

1
φ

(cid:16)

(cid:17)

c

b

c

b

c

(cid:21)

(cid:21)

Thus the parameter estimation in this case is straightforward:

x = arg max

{p(x | y, φ, ψ, k, l)} = arg min

{J1(x)} ,

(26)

x

x

with

b

J1(x) = ky − Axk2 + λkxk2,
which is a quadratic function of x. The solution is then a linear function of the data y
and is given by

(27)

x = K(λ) y with K(λ) = (AtA + λI)−1At.

(28)

b
HYPERPARAMETER ESTIMATION

For the hyperparameter estimation problem we note that:

p(φ, ψ | y, k, l) =

p(y | φ, ψ, k, l)

p(φ | k, l) p(ψ | k, l)
p(y | k, l)
p(φ | k, l) p(ψ | k, l)
p(y | k, l)

=

Thus, the hyperparameter estimation problem becomes:

(2π)−m/2 |P y|

−1/2 exp

−

ytP −1
y y

. (29)

1
2

(cid:20)

(cid:21)

where

(

φ,

ψ) = arg max

p(φ, ψ | y,

k,

l)

(φ,ψ) n

b

b

o

b

b

= arg min
(φ,ψ)

{J2(φ, ψ)}

J2(φ, ψ) = − ln p(φ | k, l) − ln p(ψ | k, l) +

ln |P y| +

1
2

1
2

ytP −1

y y.

(30)

(31)

Unfortunately, in general, there is not an analytical expression for the solution, but this
optimization can be done numerically. Many works have been investigated to perform
this optimization appropriately for particular choices of p(φ | k, l) and p(ψ | k, l). Among
the others, we may note the choice of improper prior laws such as Jeffreys’ prior
p(φ | k, l) ∝ 1
and p(ψ | k, l) =

ψ or proper uniform prior laws p(φ | k, l) =

ψmax−ψmin or still the proper Gamma prior laws.

φ and p(ψ | k, l) ∝ 1

1
φmax−φmin

One main issue with improper prior laws is the existence of the solution, because
p(φ, ψ | y, k, l) may not even have a maximum or its maximum can be located at the
border of the domain of variation of (φ, ψ). Here, we propose to use the following proper
Gamma priors :

1

p(φ) = G(α1, β1) ∝ φ(α1−1) exp [−β1φ] −→ E {φ} = α1/β1
p(ψ) = G(α2, β) ∝ ψ(α2−1) exp [−β2ψ] −→ E {ψ} = α2/β2.

(32)
(33)

With these priors, we have

J2(φ, ψ) = (1 − α1) ln φ + (1 − α2) ln ψ + β1φ + β2ψ +

ln |P y| +

1
2

1
2

ytP −1

y y.

(34)

The second main issue is the numerical optimization. Many works have been done
on this subject. Among the others we can mention those who try to integrate out one
of the two parameters directly or after some transformation. For example transforming
(φ, ψ) −→ (φ, λ) and using the identities

(35)

(36)

(37)

(38)

(39)

(40)

AAt + λI
(cid:12)
(cid:12)
(cid:12)
(AAt + λI)−1 =

(cid:12)
(cid:12)
(cid:12)

= λm−k

AtA + λI

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(I − AK(λ)),

1
λ

ln |P y| = −m ln φ − k ln λ + ln

AtA + λI

and

and

we have

Then, we obtain

ytP −1

y y = φ yt(I − AK(λ))y = φ yt(y − A

J2(φ, ψ) = (1 − α1 − m−k
+ 1

2 ) ln φ + (1 − α2 − k
y).

2 yt(y −

+ φ

AtA + ψ
φ I

2 ln

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
x) = φ yt(y −

y).

b

b

2 ) ln ψ + β1φ + β2ψ

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

m
2

or

J2(φ, λ) = (2 − α1 − α2 − m
AtA + λI
+ 1

2 ln

2 ) ln φ + (1 − α2 − k
2 yt(y −

+ φ

y).

b

2 ) ln λ + β1φ + β2φλ

For ﬁxed λ, equating to zero the derivative of this expression with respect to φ gives an
explicit solution which is

b

∂J2(φ, λ)
∂φ

= 0 −→ φ = (

+ α1 + α2 − 2) /

β1 + λβ2 +

yt(y −

y)

.

(41)

1
2

(cid:20)

(cid:21)

b

Putting this expression into J2 we obtain a criterion depending only on λ which can be
optimized numerically. In addition, it is possible to integrate out φ to obtain p(λ|y, k, l),
but the expression is too complex to write.

JOINT ESTIMATION

One may try to estimate all the unknowns simultaneously by

(

x,

φ,

ψ,

k,

l) = arg max
(x,φ,ψ,k,l)

{p(x, φ, ψ, k, l|y} = arg min
(x,φ,ψ,k,l)

{J3(x, φ, ψ, k, l)} ,

(42)

b

b

b

b

b

where

J3(x, φ, ψ, k, l) = − ln p(k) − ln p(l) − ( m
−( k
2 + α2 − 1) ln ψ + φ

2 + α1 − 1) ln φ

β1 + 1
(cid:16)

2ky − Axk2

+ ψ

β2 + 1

2kxk2

(cid:17)

(cid:16)

.
(43)
(cid:17)

The main advantage of this criterion is that we obtain explicit solutions for x, φ and ψ
by equating to zero the derivatives of J3(x, φ, ψ, k, l) with respect to them:

x = (AtA + λI)−1Aty,
φ = ( m
2 + α1 − 1)/
b
ψ = ( k
2 + α2 − 1)/
b

with
β1 + 1
2ky − A
β2 + 1
xk2
(cid:16)
.
2k

xk2

(cid:17)

b

λ = φ/ψ;
;

(cid:16)






b

(cid:17)
b
k and

b

b

Joint MAP estimation algorithm 1

We cannot obtain closed form expressions for
choice for p(k) and p(l). These relations suggest an iterative algorithm such as:

l which depend on the particular

(44)

for l = 1 : lmax

for k = 1 : kmax

compute the elements of the matrix A;
λ = λ0;
initialize
repeat until convergency:

λI)−1Aty;

b
x = (AtA +
φ = ( m
ψ = ( k
b

b



2 + α1 − 1)/
b
2 + α2 − 1)/

b



end
compute J(k, l) = J3(

b
ψ, k, l);

x,

φ,

end

end
choose the best model and the best order by

b

b

b

β1 + 1
2ky − A
β2 + 1
xk2
(cid:16)
2k
(cid:16)

(cid:17)

b

xk2

;

(cid:17)

−→

λ =

φ/

ψ

b

b

b

(

l,

k) = arg min(k,l) {J(k, l)}

b

b

Note however that, for ﬁxed x, φ and ψ, the criteria J3 in (43) or J5 in (47) are mainly
linear functions of k if we choose a uniform law for p(k). This means that we may not
have a minimum for these criteria as a function of k. The choice of the prior p(k) is then
important. One possible choice is the following:

p(k) =

2(kmax−k)
kmax(kmax−1) 1 ≤ k < kmax
0

k > kmax

(

(45)

which is a decreasing function of k in the range k ∈ [1, kmax] and zero elsewhere. This
choice may insure the existence of a minimum if kmax is chosen appropriately. For p(l)
we propose to choose a uniform law, because we do not want to give any favor to any
model.

Another algorithm can be obtained if we replace the expression of

x into J3 to obtain

a criterion depending only on (φ, ψ):

J4(φ, ψ, k, l) = − ln p(k) − ln p(l) − ( m
y(λ)k2
+φ

2ky −

β1 + 1
(cid:16)

2 + α1 − 1) ln φ − ( k
β2 + 1
2k
(cid:16)

+ ψ

(cid:17)

x(λ)k2

(cid:17)

b

b

b
2 + α2 − 1) ln ψ

(46)

or on (φ, λ):

J5(φ, λ, k, l) = − ln p(k) − ln p(l) − ( m+k

+φ

β1 + 1

2ky −

y(λ)k2

2 + α1 + α2 − 2) ln φ − ( k
β2 + 1
x(λ)k2
2 k
(cid:16)

+ (λφ)

(47)
(cid:17)
and then optimize it with respect to them. In the second case, we can again obtain ﬁrst
φ and put its expression

(cid:16)

(cid:17)

b

b

2 + α2 − 1) ln λ

m + k
2

 

φ =

b

1
2

!

(cid:20)

+ α1 + α2 − 2

/

(β1 +

ky −

y(λ)k2) + λ(β2 +

x(λ)k2)

k

(48)

in the criterion to obtain another criterion depending only on λ and optimize it numeri-
cally. This gives the following algorithm:

b

1
2

b

(cid:21)

Joint MAP estimation algorithm 2

for l = 1 : lmax

for k = 1 : kmax

compute the elements of the matrix A;
for λ ∈ 10[−8:1:4]
x = (AtA + λI)−1Aty and
compute
compute
φ using (eq. 48)
b
compute J(λ) = J5(
b

φ, λ, k, l) (eq. 47)

λ = arg minλ {J(λ)}

end
choose
x = (AtA +
compute
b
φ using (eq. 48);
compute
b
compute J(k, l) = J5(
b

b
φ,

b
λI)−1Aty;

λ, k, l) (eq. 47)

end

end
choose the best model and the best order by

b

b

y = A

x

b

b

(

l,

k) = arg min(k,l) {J(k, l)}

b

b

MODEL ORDER SELECTION

The model order selection

with

needs one more integration

k = arg max

{p(k | y, l)} = arg min

{J6(k)} ,

k

k

b

J6(k) = − ln p(k) − ln p(y | k, l),

p(y | k, l) =

p(y, φ, ψ | k, l) dφ dψ.

ZZ

(49)

(50)

(51)

or

p(y | k, l) =

p(y, φ, λ|k, l) dφ dλ,

(52)

ZZ

where p(y, φ, λ|k, l) ∝ exp [−J2(φ, λ)] given by (40). As we mentioned in the preceeding
section, these integrations can only be down numerically. A good approximation can be
obtained using the following:

p(y | k, l) =

p(y, φ, ψ | k, l) dφ dψ ≃

p(y|φj, ψi, k, l),

(53)

Z Z

Xi Xj

where {φj} and {ψi} are samples generated using the prior laws p(φ) and p(ψ).

BEST BASIS OR MODEL SELECTION

The model selection

with

b

l = arg max

{p(l | y)} = arg min

{J7(l)}

l

l

J7(l) = − ln p(l) − ln p(y | l)

does not need any more integration, but only one summation. Choosing p(l) uniform
and making the same previous approximations we have

(54)

(55)

(56)

J7(l) = − ln

p(y | k, l) p(k).

kmax

Xk=1

PROPOSED ALGORITHMS

Based on equations (55), (53), (50), (39) and (40), we propose the following algorithm:

Marginal MAP estimation algorithm 2

x = (AtA + λI)−1Aty and
compute λ = φ/ψ,
compute pψ(i, j, k, l) = exp [−J2(φj, ψi)] (eq. 39)

y = A

x

b

b

i pψ(i, j, k, l)

P

j pφ(j, k, l)

Generate a set of samples {φj} drawn from p(φ)
Generate a set of samples {ψi} drawn from p(ψ)
for l = 1 : lmax

for k = 1 : kmax

compute the elements of the matrix A;
for φ ∈ {φj}

for ψ ∈ {ψi}

end
normalize pψ(i, j, k, l) = pψ(i, j, k, l) /
compute pφ(j, k, l) =

i pψ(i, j, k, l)

b

end
normalize pφ(j, k, l) = pφ(j, k, l) /
compute pk(k, l) =

j pφ(j, k, l)

P

end
normalize pk(k, l) = pk(k, l) /
compute pl(l) =

k pk(k, l)

P

P

P

k pk(k, l)

P

choose the best value for φ = φ

end
normalize pl(l) = pl(l) /
choose the best model by
choose the best model order by

l p(l)
l = arg maxl {pl(l)}
P
k = arg maxk
j with
b
i with
choose the best value for ψ = ψ
b
compute
φ/
compute the elements of the matrix A for l =
compute

b
λI)−1Aty.

b
b
x = (AtA +

λ =

ψ

b

b

b

b

b

l)

pk(k,
j = arg maxj
i = arg maxi
b

n

o
pφ(j,
b
n
pψ(i,

l,

k)

j,
b

l,
b

o
k)

n
l and k =

o

b

b

b

k

b

b

APPLICATION: ELECTRON SCATTERING DATA INVERSION

Elastic electron scattering provides a means of determining the charge density of a nu-
cleus, ρ(r), from the experimentally determined charge form factor, F (q). The connec-
tion between the charge density and the cross section is well understood and in plane
wave Born approximation F (q) is just the Fourier transform of ρ(r), which for the case

(57)

(58)

(59)

(60)

(61)

of even-even nuclei, which we shall consider, is simply given by

F (q) = 4π

r2 J0(qr)ρ(r) dr,

∞

0
Z

where J0 is the spherical Bessel function of zero order and q is the absolute value of the
three momentum transfer.

We applied the proposed method with the following usual discretization procedure:

which results in

and

ρ(r) =

0
( P

k
j=1 xj bj(r) r ≤ Rc
r > Rc

F (q) = 4π

xj

r2 J0(qr) bj(r) dr

k

Rc

Xj=1

0

Z

y = Ax + ǫ,

where x is a vector containing the coefﬁcients {xj, j = 1, · · · , k}, y is a vector containing
the form factor data {F (qi), i = 1, · · · , m} and A an (m × k) matrix containing the
coefﬁcients Ai,j given by

Ai,j = 4π

r2 J0(qir) bj(r) dr.

Rc

0
Z

To compute Ai,j we deﬁne a discretization step ∆r = Rc/N, a vector r = {rn =
(n − 1)∆r, n = 1, · · · , N}, a (N × k) matrix B with elements Bn,j = bj(rn), a (m × N)
matrix C with elements Ci,n = (4π∆r)r2
nJ0(qirn) such that we have A = CB. Note
also that when the vector x is determined, we can compute ρ = {ρ(rn), n = 1, · · · , N}
by ρ = Bx.

To test the proposed methods, we used the following simulation procedure:

• Select a model type l and an order k and generate the matrices B, C and A, and

for a random set of parameters x generate the data y = Ax.

• Add some noise ǫ on y to obtain y = A x + ǫ.
• Compute the estimates

x and

y = A

x,

k,

l,
x, y = Ax and ρ = Bx.
b

b

b

b

b

b

b

We chose the following basis functions:

ρ = B

x and compare them with l, k,

• l = 1 :

bj(r) = J0(qjr)— This is a natural choice due to the integral kernel and

the orthogonality property of the Bessel functions.

• l = 2 :

bj(r) = sinc(qjr)— This choice is also natural due to the orthogonality

• l = 3 :

bj(r) = exp

and the limited support hypothesis for the function ρ(r).
2 (qjr)2
the function ρ(r) if the {xj} are constrained to be positive.
2 (qjr)2

bj(r) = exp

i

• l = 4 :

third properties.

i

− 1
h
− 1
h

— This choice can account for the positivity of

J0(qjr)— This choice combines the ﬁrst and the

• l = 5 :
one.
• l = 6 :
one.

bj(r) = 1/(cosh(qjr))— This choice has the same properties as the third

bj(r) = 1/(1 + (qjr)2)— This choice has the same properties as the third

In all these experiments we chose k = 6, m = 20, N = 100, Rc = 8 and qi = iπ/Rc.
The following ﬁgures show typical solutions. Figures 1 and 2 show the details of the
procedure for the case l = 1. Figures 3, 4 and 5 show the results for the cases l = 1 to
l = 6.

)
r
(
 
b

j

)
r
(
o
h
r

0.4

0.5

1

0

1

0.8

0.6

0.2

0

5

4

3

2

1

0

)
)
)
q
(
F
(
s
b
a
(
g
o

l

−1

−2

−3

−4

−5

0

−0.5

0

1

2

3

5

6

7

8

−0.2

0

1

2

3

5

6

7

8

4
r

4
r

bj(r)
j = 1, . . . , k = 6

k

Xj=1

ρ(r) =

xj bj(r)

with
x1 = x2 = · · · = x6 = 1

k

F (qi) =

Aij xj

Xj=1
r2J0(qir) bj(r) dr

Aij =
i = 1, . . . , m = 14
j = 1, . . . , k = 6

Z

1

2

3

5

6

7

8

4
q

Fig. 1: a) basis functions bj(r), b) ρ(r),

c) data F (qi) in a logarithmic scale.

0.1

0.05

0

1

2

3

4

5

6

model number

model order

p(k, l)

0.1

0.08

0.06

0.04

0.02

0

1

2

3

4
model number

5

6

1 2 3 4 5 6 7 8 9101112131415

model order

−0.1

0

1

2

3

4

5

6

7

8

p(l) and p(k|

l)

b

ρ(r) =

xj bj(r)

and
b

ρ(r) =

xj bj(r)

k

k

Xj=1

b

Xj=1

F (qi) =

Aij

xj

and
b

b

F (qi) =

Aij xj

k

k

Xj=1

Xj=1

0.8

0.6

0.4

0.2

0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

4

3

2

1

0

−1

−2

−3

−4

0

1

2

3

4

5

6

7

l), b) original ρ(r) and estimated

ρ(r),

Fig. 2: a) p(k, l|y), p(l|y) and p(k|y,
F (qi).
c) original F (qi) and estimated
b

b

b

)
r
(
 
b

j

1

0.8

0.6

0.4

0.2

0

−0.2

0.4

0.35

0.3

0.25

0.15

0.1

0.05

0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

5

4

3

2

1

0

−1

−2

−3

−4

0.5

1

0

)
r
(
 
b

j

−0.5

0

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

−0.1

0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

4

3

2

1

0

−1

−2

−3

−4

0

1

2

3

5

6

7

1

2

3

5

6

7

8

8

−0.4

0

)
l
(

P

)
k
(
P

0.08

)
l
(

P

0.2

1

2

3

4
Model number

5

6

1 2 3 4 5 6 7 8 9 101112131415

Model order

1

2

3

4
Model number

5

6

1 2 3 4 5 6 7 8 9 101112131415

Model order

4
r

)
k
(
P

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

4
r

0.16

0.14

0.12

0.1

0.06

0.04

0.02

0

1

2

3

4

5

6

7

1

2

3

4

5

6

7

8

0

0

8

1

2

3

4

5

6

1

2

3

4

5

6

7

−5

0

7

Fig. 3:
a) basis functions bj(r), b) p(k|y) and p(k|y,
d) F (qi) and

Left: l = 1

F (qi).

Right: l = 2

l), c) ρ(r) and

ρ(r),

b

b

b

1

0.8

0.6

0.2

0

)
r
(
 
b

j

0.4

)
l
(

P

0.5

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

4

3

2

1

0

−1

−2

−3

)
r
(
 
b

0.5

j

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

0

0

0.5

0.45

0.4

0.35

0.3

0.2

0.15

0.1

0.05

0

)
l
(

P

0.25

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

4

3

2

1

0

−1

−2

−3

−4

0

1

2

3

5

6

7

1

2

3

5

6

7

8

8

−0.2

0

4
r

)
k
(
P

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

4
r

)
k
(
P

0.06

0.12

0.1

0.08

0.04

0.02

0

1

2

3

4
Model number

5

6

1 2 3 4 5 6 7 8 9 101112131415

Model order

1

2

3

4
Model number

5

6

1 2 3 4 5 6 7 8 9 101112131415

Model order

1

2

3

4

5

6

7

1

2

3

4

5

6

7

8

8

0

0

1

2

3

4

5

6

1

2

3

4

5

6

7

−4

0

7

Fig. 4:
a) basis functions bj(r), b) p(k|y) and p(k|y,
d) F (qi) and

Left: l = 3

F (qi).

Right: l = 4

l), c) ρ(r) and

ρ(r),

b

b

b

)
r
(
 
b

0.5

j

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

0

0

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

)
l
(

P

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

5

4

3

2

1

0

−1

−2

−3

−4

−5

0

)
r
(
 
b

0.5

j

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

0

0

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

)
l
(

P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

5

4

3

2

1

0

−1

−2

0

7

1

2

3

5

6

7

8

1

2

3

5

6

7

8

4
r

0.25

0.2

0.15

)
k
(
P

0.1

0.05

0

4
r

)
k
(
P

0.4

0.8

0.7

0.6

0.5

0.3

0.2

0.1

0

1

2

3

4
Model number

5

6

1 2 3 4 5 6 7 8 9 101112131415

Model order

1

2

3

4
Model number

5

6

1 2 3 4 5 6 7 8 9 101112131415

Model order

1

2

3

4

5

6

7

1

2

3

4

5

6

7

8

8

0

0

1

2

3

4

5

6

1

2

3

4

5

6

7

Fig. 5:
a) basis functions bj(r), b) p(k|y) and p(k|y,
d) F (qi) and

Left: l = 5

F (qi).

Right: l = 6

l), c) ρ(r) and

ρ(r),

b

b

b

Note that in these tests, we know perfectly the model and generated the data according
to our hypothesis. To test the method in a more realistic case, we choose a model for
which we can have an exact analytic expression for the integrals. For example, if we
choose a symmetric Fermi distribution [4]

ρ(r) = α

cosh(R/d)
cosh(R/d) + cosh(r/d)

,

an analytical expression for the corresponding charge form factor can easily be ob-
tained [5]:

F (q) = −

4π2αd
q

cosh(R/d)
sinh(R/d) "

R cos(qR)
sinh(πqd)

−

πd sin(qR) cosh(πqd)
sinh2(πqd)

.

#

(63)

Only two of the parameters α, R and d are independent since the charge density must
fulﬁll the normalization condition

4π

r2 ρ(r) dr = Z.

Z

Figure 6 shows the theoretical charge density ρ(r) of 12C (Z=6) obtained from (62)
for r ∈ [0, 8] fm with R = 1.1 A and d = 0.626 fm and the theoretical charge form factor
F (q) obtained by (63) for q ∈ [0, 8] fm−1 and the 15 simulated data:

q = [0.001, .5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0] fm−1

(62)

(64)

which are used as inputs to the inversion method.

Charge density

y
t
i
s
n
e
d
 
e
g
r
a
h
C

1

0.8

0.6

0.4

0.2

0

−0.2

0

r
o
t
c
a
f
 

m
r
o
F

4

2

0

−2

−4

−6

−8

−10

−12

−14

0

1

2

3

5

6

7

8

1

2

6

7

8

3

4
Momentum transfer

5

4
Radius

Fig. 6: Theoretical charge density ρ(r), charge form factor log |F (q)| and the data
[stars] used for numerical experiments [right].

First note that, even with the exact data, there are an inﬁnite number of solutions

which ﬁts exactly the data. The following ﬁgure shows a few sets of these solutions.

0.06

0.04

0.02

0

1

2

3

4

5

6

model number

0.4

0.3

0.2

0.1

0

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

model order

0.06

0.05

0.04

0.03

0.02

0.01

0

4

2

0

−2

−4

−6

−8

−10

−12

0

8

1

2

3

4
model number

5

6

1 2 3 4 5 6 7 8 9101112131415

model order

1

2

3

4

5

6

7

1

2

3

4

5

6

7

Fig. 7:

a) p(k, l|y)
b) p(k|y)
d) ρ(r) and

c) p(k|y,
l)
e) F (qi) and

b

F (qi).

b

ρ(r)

b

We discussed the different steps for a complete resolution of an inverse problem and
focused on the choice of a basis function selection and the order of the model. An
algorithm based on Bayesian estimation is proposed and tested on simulated data.

CONCLUSIONS

REFERENCES

J. L. Friar and J. W. Negele, Nucl. Phys. A 212, 93 (1973).

J. Heisenberg and H. P. Blok, Ann. Rev. Nucl. Part. Sc. 33, 569 (1983).

1.
2. B. Dreher et al., Nucl. Phys. A 235, 219 (1974).
3.
4. M. E. Grypeos, G. A. Lalazissis, S. E. Massen, and C. P. Panos, J. Phys. G 17, 1093 (1991).
5. R. E. Kozak, Am. J. Phys. 59, 74 (1991).
J. Baker-Jarvis, J. Math. Phys. 30, 302 (1989).
6.
7.
J. Baker-Jarvis, M. Racine, and J. Alameddine, J. Math. Phys. 30, 1459 (1989).
8. N. Canosa, H. G. Miller, A. Plastino and R. Rossignoli, Physica A220, 611 (1995).
9. Buck and Macaulay, “Linear inversion by the method of maximum entropy,” in Maximum Entropy

and Bayesian Methods 89, (J. Skilling, ed.), Kluwer Academic Publishers, 1990.

10. A. Mohammad-Djafari, “A full Bayesian approach for inverse problems,” in Maximum Entropy and

Bayesian Methods 95, (K. Hanson and R. Silver, ed.), Kluwer Academic Publishers, 1996.

11. D.J.C. MacKay, “Hyperparameters: Optimize or integrate out?” in Maximum Entropy and Bayesian

Methods 93, (G. Heidbreder, ed.), pp. 43–59, Kluwer Academic Publishers, 1996.

12. V.A. Macaulay and B. Buck, “A fresh look at model selection in inverse scattering,” in Maximum
Entropy and Bayesian Methods 94, (J. Skilling and S. Sibisi ed.), Kluwer Academic Publishers,
1996.

13. A. Mohammad-Djafari and J. Idier, “A scale invariant Bayesian method to solve linear inverse
problems”, pp. 121–134. in Maximum Entropy and Bayesian Methods 94, (G. Heidbreder, ed.),
Kluwer Academic Publishers, 1996.

14. A. Mohammad-Djafari and J. Idier, “Maximum entropy prior laws of images and estimation of their
parameters,” pp. 285–293. in Maximum Entropy and Bayesian Methods 90, (T. Grandy, ed.), Kluwer
Academic Publishers, 1991.


8
9
9
1
 
p
e
S
 
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
0
0
9
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Information content in Gaussian noise: optimal compression rates
August Romeo, Enrique Gazta˜naga, Jose Barriga, Emilio Elizalde

Consejo Superior de Investigaciones Cient´ıﬁcas (CSIC)

Institut d’Estudis Espacials de Catalunya (IEEC)

Ediﬁci Nexus–201, Gran Capit`a 2–4, 08034, Barcelona, Spain

Abstract.

We approach the theoretical problem of compressing a signal dominated by gaussian noise. We present

accurate expressions for the compression ratio which can be reached under the light of Shannon’s noiseless

coding theorem, for a linearly quantized stochastic gaussian signal (noise). The compression ratio decreases

logarithmically with the amplitude of the frequency spectrum of the noise P (f ). Further, we show how the

entropy and the compression rate depend on the shape of this power spectrum, given diﬀerent normalizations.
The cases of white noise (w.n.), power-law noise f np –including 1/f noise–, (w.n. + 1/f ) noise, and piecewise
(w.n. + 1/f + 1/f 2) noise are discussed in detail, while quantitative behaviours and useful approximations are

provided.

Key words: Information Theory, Signal compression

1

Introduction

There are several motivations to approach the theoretical problem of compression of noise (or a stochastic

signal). In some cases the signal to be transmitted is intrinsically noisy (e.g. from scientiﬁc measurements) and

needs to be compressed in a lossless way before any reduction can be applied. This is the case for signals from

scientiﬁc instruments on-board space satellites which typically produce high rates of noisy data that have to be

sent to earth via a limited telemetry rate. Electronic instruments (e.g. detectors, ampliﬁers) show characteristic

low frequency instabilities (1/f noise) which add to the white or thermal noise. When the signal that we are

measuring with these instruments is weak, it can only be recovered from averaging many measurements. The

averaging can only be done after a careful calibration of the low frequency instabilities, which in practice

means that the whole (noisy) signal has to be transmitted (to earth). This is an example that requires lossless

compression of a signal dominated by noise. Here we would like to study in a quantitavive way how much noise

can be compressed.

This noise is usually treated as a gaussian stochastic process with an arbitrary power spectrum. As we

will see, given the properties of a Gaussian distribution, it is possible to ﬁnd some analytical approximations

for the theoretical compression factor.

In Section 2 we present a basic introduction to the problem of compression. In Section 3 we deal with

the one-dimensional case which will help to introduce the main topic of information content and compression

of noise in Section 3. In Section 4 our conclusions are presented.

1

2 The basic data compression problem

Standard lossless data compression techniques are applied successfully only to data sets with some redundacy.

This redundancy can be formally expressed using the entropy H.

It is easy to show (see below) that it is

not possible to compress a (uniformly) random distribution of measurements. If noise is discretized to a high

resolution (as compared to its variance) the resulting distribution of numbers approaches a uniform distribution.

This indicates that lossless compression might not be very eﬃcient when the data is dominated by noise, but,

as we shall see, the problem depends crucially on the digital resolution and the range of values to be stored.

Hypothetical data compression problems can be considered in the light of Shannon’s ﬁrst theorem (see

[1]-[3]). This theorem tells us that the Shannon entropy H of a source is the lower bound to the average length

of the code units or ‘words’ (In addition, we know that such a lower bound can be fairly well approached by

means of some of the available methods for coding, such as Huﬀman’s, etc.). Then, the theoretical compression

rate is deﬁned as:

cr, opt ≡

average length per code unit
Shannon entropy per code unit

Of course, for this quotient to make sense, both quantities should be referred to the same type of code divisions

(e.g. words, data values, blocks, packets, etc.) and must be written in the same length units (e.g. bits).

Thus, our problem entails the entropy of the stochastic process generating the noise under consideration.

In our case, this noise will be the result of a Gaussian proccess with a speciﬁc power spectrum. Its outcome shall

be represented by a random variable η, which can be assumed to be stationary in wide sense. The discrete set of

η(t)-values for successive t increases will be treated like the components of a multidimensional Gaussian variable

with the power spectrum in question. Most of the time, we will deal with a bandwidth-limited spectrum, i.e.,

one where the frequencies are limited by an upper and a lower limit. Examining the associated Shannon entropy,

we shall study the hypothetic chances of compressing the sort of data sequences generated by such processes. In

particular, we will consider Gaussian white noise, Gaussian noise with correlation of the 1/f -type, and Gaussian

noise with a mixed correlation of the type white-noise +1/f -noise.

In general, the compression rate cr for ﬁnite sequences of symbols that have been encoded is usually

deﬁned as the quotient between the sequence lengths before and after the encoding process —Li and Lf,
Ns) denote the initial and ﬁnal —or encoded— sets of

(j = 1, . . . ,

αj}

and

. If

{

{

Li
Lf

respectively— i.e. cr =

aj}
symbols, their average lengths are

j=1
X
where pj, L(aj) and L(αj) give the probability of the jth symbol and its length in bits before and after encoding,

respectively. When the sequences are long enough, the rate cr can be replaced with the quotient between the
initial and ﬁnal average lengths per symbol in the way cr ≃
initial data representation consists of symbols of the same length.

. We shall assume L(aj) = Li ∀

j, i.e., that the

Li
Lf

Li =

pj L(aj),

Lf =

pj L(αj),

Ns

j=1
X
Ns

2

Shannon’s ﬁrst theorem (also called noiseless coding theorem, see e.g. [4, 5]) provides theoretical lower

(and upper) bounds to the ﬁnal length per symbol in the way H

H + 1), where H is the Shannon

Lf (

≤

≤

entropy

An eﬃcient coding method will have to approach equality to the lower bound. For one dimension, the Huﬀman
scheme is known to be reasonably close 1 (see also the performance of other methods such as the Rice algorithm
Li
Lf ≤

in [7]). Thus, the compression ratio will satisfy cr ≃

, being the equality the optimal case, given by

Li
H

H =

pj log2(pj).

−

j
X

cr, opt ≡

Li
H

.

Let’s consider the case of an N -dimensional (vector) random variable. Since the probabilities must be

now referred to a multivariate distribution, (2.1) is generalized to

HN =

−

j1,...,jN
X

pj1,...,jN log2(pj1,...,jN ).

We shall suppose that each of its components is a one-dimensional random variable of the same type.

In

addition, there might exist possible correlations among these components. There is a well-known inequality for

any N -dimensional random variable ~η = (η1, . . . , ηN ) (Gaussian or not) relating the joint Shannon entropy HN

and the individual Shannon entropies of each component, H1(ηj ), j = 1, . . . , N , which reads

or, equivalently,

HN (η1, . . . , ηN )

H1(η1) + . . . + H1(ηN ),

≤

h(η1, . . . , ηN )

HN (η1, . . . , ηN )
N

1
N

≤

≡

H1(ηj ),

N

j=1
X

where h denotes the joint Shannon entropy per component. Unlike HN , h does not grow extensively by merely

increasing N . When η1, . . . , ηN are all of them of the same type, (2.5) reduces to h

H1. Deﬁning the initial

≤

length per component li as the analogue of Li for each vector component, eq. (2.2) may be rewritten as

(2.1)

(2.2)

(2.3)

(2.4)

(2.5)

(2.6)

It is essential to note that the equality in (2.5) is satisﬁed if and only if the N components η1, . . . , ηN are

independent. Therefore, for independent variables of the same type, h = HN =1, and it is enough to study the

Note that for a uniform distribution where pj = 1/

Ns, we have that h = l = log2(

Ns), so no compression

1To give an idea of this closeness, let’s quote a bound found in [6]: calling r ≡ Lf − H, and pmax = max({pj}), then

N = 1 case.

is possible (Cr,opt = 1).

r ≤ pmax + log2

2 log2(e)
e

= pmax + 0.086.

(cid:16)

(cid:17)

cr, opt ≡

li
h

.

3

3 One-dimensional Gaussian variable

We will try to ﬁnd this theoretical rate for a zero-mean Gaussian white noise η, —whose probability density will

be called f (η)— with variance equal to σ, and whose values are discretized or ‘quantized’ to a given resolution.

When discretizing, we gather results into intervals of some ﬁxed width, which shall be denoted by ∆η. If this

width is small enough, we may assume that all the values that have fallen into the same interval have, roughly,

the same probability. Thus, to each interval we assign a ‘probability’ value as follows

η in the interval around χ

χ

, χ +

∆η
2

−

(cid:18)

∆η
2

−→

(cid:19)

p(χ) ∆η =

dζ f (ζ)

χ +

χ

Z

−

∆η
2
∆η
2

f (χ) ∆η =

∆η

≃

χ2
2σ2
e−
√2πσ2






This will be done for each η(j), with η(j) = j∆η, j

Z. Each interval will be called I (j) =

η(j)

∈

∆η

2 , η(j) + ∆η

2

.

−

(cid:17)
In order to properly talk about probabilities, the set should be well normalized. Therefore, we write the
probability that η takes a value in I (j) as

(cid:16)

pj ≡

p[η

∈

I (j)] =

p(η(j))

p(η(n))

e−

=

j2 (∆η)2
2σ2
Z

,

n
X

∞

Z =

n=

X

−∞

n2 (∆η)2
2σ2

.

e−

where

constant.

This Z, introduced in order to fulﬁl the normalization condition, may be also regarded as the partition function
2πσ2
(∆η)2 , with adequate new units for the Boltzmann

of a system with energies

at temperature T =

En = πn2

{

}

In order to calculate the ideal compression rate, we need to ﬁnd the Shannon entropy (2.1). Since N = 1,

H = H1 = h, and the result (see subsec. A.1 in the appendix) is

h = log2

√2πe

(cid:20)

σ
∆η

+

(cid:21)

O

(cid:18)

2πσ2
(∆η)2 e−

2π2 σ2
(∆η)2 , . . .

(cid:19)

which depends on σ and ∆η only trhough the dimensionless quotient

Thus, the smaller λ

1/√T (the higher the temperature) the larger the entropy h. Compare this with the

result of a na¨ıve integration without discretization, which would be log2
(cid:17)
the exponentially small corrections vanish, but the logarithm of λ diverges. Thus,

(cid:16)

√2πeσ2

≡

hcont. In the λ

0 limit

→

∼

∆η
σ ≡

λ

h

hcont −

≃

log2(∆η)

4

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

Figure 1: Shannon entropy per component h (left) and associated optimal compression rate cr, opt (right) —by

formulas (2.6),(3.6)— as functions of the discretization parameter λ = ∆η/σ, for a ﬁxed li = 16 bits. The three

curves correspond to np = 0 with P (ω) = A = 1 sec. (solid line), a combination of np = 0 and np =

1 of the

form P (ω) = A(A0 + ω0/

ω
|
with the same value of ω0 (dotted line). No equal σ1

) with A0 = 1, ω0 = ωMax/10 (dashed line), and to np =
|

p-constraint has been imposed.

−

−

−

1 for P (ω) = Aω0/

ω
|

|

(see explanation in refs. [8] or [9], or in app. B, or our own comments below, after eq.(4.11)).

representation,

Let’s now write the initial mean length as li = log2(

Ns). This means that, using a suitable binary
Ns is the number of eﬀectively distinct η-values that can be considered (although li is an integer

only when

Ns is an exact power of 2, these variables will be treated as if they were real).

First, we can imagine a process in which the initial length per symbol li has been ﬁxed independently of

∆η (this could be the case when we are worried about instabilities of the signal). Then, the optimal compression

rate would just be the quotient

cr,opt ≡

li
h ≃

li
log2(√2πe/λ)

.

So that the larger we can make λ, without loss of relevat information, the larger the compression. If the ﬁnal

sensibility

S
1 as far as M

we need is obtained from some later average of M measurements of this noise η, then we can make
)2. In this extreme case the compression can be as large as cr,opt ≃

li/2.047, eg.

S

λ

≃

cr,opt = 7.8 for 16 bits symbols. Fig. 1 shows (as continuous lines) the entropy h and the compression cr,opt as

> (σ/
∼

Another possibility is to work with li as a function of R and ∆η. We suppose that the values of our

random variable η span a range R

max(η)

min(η). Assuming our discretization to be linear, it is clear that

≡

−

a function of λ.

and, therefore,

(3.6)

(3.7)

(3.8)

R =

Ns∆η

li = log2

R
∆η

.

(cid:18)

(cid:19)

5

Formulae (2.6) and (3.3)-(3.8) enable us to put cr, opt as a function of either ∆η or li. Then,

cr,opt ≃

log2(∆η)

log2(R)
−
log2(∆η)
hcont −

=

li
hcont + li −

=

log2(R)

(cid:1)
If we limit R to a given number of σ’s —say N0— around the origin, only the values in (

(cid:0)

taken into consideration. Thus, R = 2N0σ, and we can further write

li + log2

2πe

2

σ
R

li

(cid:20)q

.

(3.9)

(cid:21)
N0σ, N0σ) will be

−

(3.10)

cr,opt ≃

li + log2

=

li

√2πe
2N0 !

 

1

.

log2

log2

√2πe
2N0 !
2N0σ
∆η

(cid:19)

 

(cid:18)

1 +

√2πe

N0 crit ≡
Note that cr,opt cannot be larger than one if N0 ≤
size of the acceptable range. On the other hand, by taking larger and larger values of N0 one could achieve

2.0664. This is interpreted as a critical

2 ≃

arbitrarily high compression rates, but this would mean to collect suﬃciently meaningful amounts of data very
far from the mean. This could correspond to rare events which might not follow the Gaussian distribution.2

In general, a reasonable choice would be some N0 moderately above N0 crit, but this depends critically on the

subsequent data analysis we want to carry on with this data.

In Figure 2 we show cr,opt for the case of white noise (continuous line) with N0 = 3. The main diﬀerence

from Fig.1 is that in the former li = 16 bits while in Fig.2 we choose li according to ∆η as in Eq.[3.8] with

N0 = 3. Although the distance of three sigmas is already a long way from the mean, the compression rates found

are rather small. In Fig. 2 we also show (right panel) cr,opt as a function of li, showing how the compressibility

increases as l gets small.

4 Multidimensional case: Gaussian stochastic processes

4.1 Uncorrelated Gaussian variables: white noise

Suppose now that we have N uncorrelated Gaussian variables with diﬀerent variances σ1, . . . , σN . Although we

shall keep this general notation, we are only interested in processes where σ1 = . . . = σN , which is the case of a

gaussian stochastic process stationary in wide sense. As long as these N variables are uncorrelated, we have to

apply (2.4) as an equality, which, combined with (3.3) and equally quantizing in all dimensions, gives the joint

entropy

H

≡

HN = log2 


v
u
u
t

(cid:18)

Here we may interpret

2πe
(∆η)2

N N

(cid:19)

m=1
Y

σ2
m

+

O

N

(cid:18)

2πσ2
(∆η)2 e−

2π2 σ2
(∆η)2 , . . .

.

(cid:19)

(4.1)

(4.2)

2Note that there is no contradiction here because even in the presence of non-Gaussian rare events, the bulk of the data might

still be well descrived by a Gaussian so that our estimations could still yield a good approximation.



σ2
m = Det(C0),

N

m=1
Y

6

Figure 2: Optimal compression rate cr, opt —by formulas (2.6),(3.9)— as functions of the discretization interval

λ = ∆η/σ (left) and as functions of the initial mean length in bits li (right). The three curves have the same

parameters as in Fig. 1. Note, cr, opt has a divergence for small li, which comes from the vanishing of h.

(4.3)

(4.4)

(4.5)

where C0 is the diagonal matrix

It is usefull to deﬁne an eﬀective variance as:

C0 = diag(σ2

1, . . . , σ2

N ).

σ2
0 ≡

Det1/N (C0),

so that when σ1 = . . . = σN we will have σ0 = σ1 = . . . = σN , which also agrees with our deﬁnition of the

1-point sigma σ1p below Eq.[4.29].

Thus, the entropy per component is conveniently written as in the previous case Eq[3.3]:

h = log2

√2πe

(cid:20)

σ0
∆η

+

(cid:21)

O

(cid:18)

2πσ2
(∆η)2 e−

2π2 σ2
(∆η)2 , . . .

h0(σ0)

≡

(cid:19)

where the 0-subscript means that this is the uncorrelated case. As we shall see below, to deal with correlations

will just mean the replacement of C0 with a new correlation matrix —say C in Eq[4.4].

We have done simulations of Gaussian noise with σ1 = . . . = σN and the data has been represented

with a ﬁxed li = 8 bits. The N -dimensional variable is then compressed by the Huﬀman method, and the
compression rate cH
r
actual compression rate is then compared to the optimal one, i.e., to cr, opt = li
h . The results are presented in
Table 1. The agreement is better as N increases. The explanation is that, in practice, the compressed ﬁles take

is found as the quotient between the sizes of the initial and the compressed ﬁle. This

up some further space for storing the conversion tables between both symbol sets. Obviously, since the number
of diﬀerent symbols is ﬁxed —2li = 256— the relative contribution caused by the size of these tables decreases

as N grows.

7

N

σ0

1000

29.99

∆η/σ

λ

≡
0.033

10000

29.98

0.033

100000

29.99

0.033

100000

10.00

0.10

5.01

0.20

4.01

0.25

2.02

0.50

1.04

0.96

cH
r

0.97

1.12

1.14

1.48

1.81

1.95

2.56

3.64

cr, opt

1.15

1.15

1.15

1.49

1.83

1.97

2.61

3.80

Table 1: Comparison of optimal compression rates cr, opt with actual rates from simulated data compressed
with Huﬀman method cH
r .

4.2 Gaussian variables with correlation: coloured noise

Now, suppose that we have an N -dimensional variable ~η = (η1, . . . , ηN ) whose components are correlated
ηj )(ηk −
. In the case of zero-mean variables, it reduces to

according to the entries of some covariance matrix C . By mathematical deﬁnition, C(ηj , ηk) =

(ηj −
h

, where
i

. . .
i
h

denotes statistical average, and ηj ≡ h

ηj i

ηk)∗

C(ηj , ηk) =

ηjηk∗
h

i ≡

Cjk.

(4.6)

In practice, a discretization or shot noise ﬂuctuation could be added and the theorical correlation would be

. In general, this is of little interest as it just amounts to an constant increase

changed to Cjk =

ηj ηk∗
h

i

+

1
N
h

i

of the power spectrum. The values of ~η can correspond to continuous random variable η = η(t) sampled in N
time intervals (~η = η(~t)). For a wide sense stationary stochastic process we have that Cjk = Cj
a function of j

k, eg the covariance matrix is Toeplitz matrix.

k can only be

−

−

A sequence of a Gaussian stochastic process has a joint probability density given by

1
2

~η T C−

1 ~η∗

.

f (η)

e−

∝

In the absence of correlations C is just the C0 of (4.3) and therefore C−

N ), but now we
expect the presence of nonvanishing oﬀ-diagonal coeﬃcients. We may assume that all the ~η components are

1 = diag(1/σ2

1, . . . , 1/σ2

real. Each dimension will be discretized in the same way as for the one-dimensional case. Therefore, we will

consider the joint probabilities

pj1,...,jN ≡

IjN ]

p[η1 ∈
1
Z

e−

Ij1, . . . , ηN ∈
(∆η)2
2

=

(j1, . . . , jN )T C−

1 (j1, . . . , jN )

,

(4.7)

8

where the normalizing quantity Z is given by

(∆η)2
2

e−

(n1, . . . , nN )T C−

1 (n1, . . . , nN )

.

Z =

n1,...,nN
X

The ensuing Shannon entropy (see subsec. A.2 in the appendix) is

H

≡

HN = log2

Det

"s

(cid:18)

2πe
(∆η)2 C

+

N

O

(cid:18)

(cid:19)#

2πσ2
(∆η)2 e−

2π2 σ2
(∆η)2 , . . .

,

(cid:19)

where this σ is given in section (A.2). Note that the next-to-leading terms are, again, exponentially small, and
their typical size can be adequately expressed as a function of a dimensionless parameter ∆η

λ. Before going

σ ≡

on, some comments are in order. The previous relation can be rewritten in the form

H = log2

Det(2πe C)

N log2(∆η) + exponentially small part

(4.10)

hp

−

i

The ﬁrst term on the r.h.s. is just the result of having calculated H after replacing the multiple sum in (4.8)

with a multiple integral. Therefore, we shall call it Hcont. Further, in the continuum limit, λ

0 and the

exponential corrections should vanish. This leads to

→

(4.11)

H = Hcont −

N log2(∆η),

When an entropy associated to a discretization of width ∆η is compared with its continuous version, we realize

that we gain N times the ‘information’ leaked by mistaking a single element of unit length for an interval of

N log2(∆η). In terms of entropy per component, (4.11) becomes
size ∆η, which is N [
log2(∆η), which generalizes (3.5) as now hcont has the same expression as in (3.5) but changing
h = hcont −
σ by σe . Furthermore, there is a critical ∆η-value for which the whole h vanishes. When this happens, the

log2(∆η) + log2(1)] =

−

−

discretization is so coarse that the little resolution kept is not enough to store any eﬀective information at all.

Another convenient way of writing the entropy per component is

h = log2

√2πe

(cid:20)

σe
∆η

+

(cid:21)

O

(cid:18)

2πσ2
(∆η)2 e−

2π2 σ2
(∆η)2 , . . .

(cid:19)

were we have now that the eﬀective variance is:

σ2
e ≡

Det1/N (C),

These expressions generalize for correlated variables the result in Eq.[(4.5)] for h0 by just replacing C0 with C

and σ0 for σe. Thus for a general covariance matrix C we only need to ﬁnd σe above to obtain the corresponding

(4.8)

(4.9)

(4.12)

(4.13)

entropy.

4.2.1 Calculation of Det(C)

The next task is the calculation of the determinant of C. For convenience, we prefer to handle the Fourier-space

representation of C —which we shall denote by

C— rather than C itself (we will see that

C is simpler). A

b

9

b

vector ~η and its discrete Fourier transform

η are related by expressions of the type

η = ∆t W ~η,

b

~η =
b

∆ω
2π

W ∗

η, 



where W indicates a matrix whose coeﬃcients are given by Wmn = e

b

i

2π
N

mn

(see subsec. A.3). ∆t is a t-interval

which now has to be interpreted as the time lapse between two successive Fourier ‘samplings’. If we imagine that
ηj = η(tj), then tj −
Taking into account the usual relation between the sampling interval and the associated angular frequency (or

j. ∆ω is the corresponding interval in ‘angular frequency’ or conjugate space.

1 = ∆t,

tj

∀

−

conjugate momentum) range that can be correctly sampled in conjugate space, one has the following relation

between ∆t, ∆ω and N :

The discrete values of ω are

∆ω = 2π

1
N ∆t

,

ωj = j∆ω, j =

N/2, . . . , N/2.

−

Let ωmin and ωMax denote the minimum and maximum nonzero absolute values of ω. Then,

ωmin ≡
ωMax ≡

ω1

=

∆ω =

= 2πfmin,

ωN/2 =

∆ω =

= 2πfMax.

N
2

2π
N ∆t
π
∆t

We have here introduced frequencies —f ’s— in the way ω = 2πf , as usual.

Furthermore, by the form of its coeﬃcients and by eq.(4.14), it is clear that the W matrix satisﬁes

and, consequently, W −

1 =

∗. In other words, up to a multiplicative scalar constant, W is a unitary

operator. Taking now formula (4.6), we apply (4.14) and (4.18) to write the C matrix in terms of Fourier-space

∆t∆ω
2π

W T

objects, and quickly obtain

where

C is the above mentioned Fourier-space representation of C, i.e., it is the matrix whose coeﬃcients read

b

b

Formula (4.19) is telling us that

independently of W . In order to ﬁnd concrete results, some sort of hypothesis on

b

C has to be made. Here we

consider stationary (or homogeneous) processes, for which the the covariance matrix is a Toeplitz matrix, and

therefore

C is diagonal (see

A.3) so that

= P (ω) δDirac(ω

ω′), whose discrete version yields:

b

−

§

b

W T

= W,

W −

1 =

∆t∆ω
2π

W ∗,

C =

∆ω
2π ∆t

(cid:18)

(cid:19)

1

W −

C W,

Cjk =

ηj
h

.
η∗ki

b

b

N

b
∆ω
2π ∆t

(cid:18)

(cid:19)

Det

C

(cid:16)

(cid:17)

Det(C) =

η(ω)
h

η∗(ω′)
i

b

Cjk = P (ωj)

b

δjk
∆ω

,

b

10

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

(4.19)

(4.20)

(4.21)

(4.22)

i.e.

C is a diagonal matrix. In all these cases, the problem boils down to the properties of the P (ω) function.

If we denote by P the diagonal matrix:

b

we can write the eﬀective rms correlation σe that appears in Eq.[4.12] by:

P

diag(P (ω

N/2), . . . , P (ωN/2)).

≡

−

σ2
e ≡

Det1/N (C) =

Det1/N (P ) =

1
2π∆t

1

2π∆t 

j
Y



1/N

P (wj )

.





The white noise case corresponds to the constant power spectra P (w) = A and the matrix P is proportional to

the identity. In this case:

σ2
e = σ2

0 =

A
2π∆t

,

showing that the larger the sampling interval ∆t the smaller the variance, as expected.

We can also express the entropy as a diﬀerence from the entropy h0 of a white noise spectrum of amplitude

h = h0 +

log2

Det

"s

1
N

1
A

P

.

(cid:19)#

(cid:18)

P = A by:

are given by:

and (4.21),

introduce

In general, given two power spectra P1 and P2 with eﬀective correlations σe1 and σe2, the entropy diﬀerences

h2 −

h1 = log2

σe2
σe1 (cid:21)

(cid:20)

= log2

Det1/2N (P2)
Det1/2N (P1) #

"

=

1
N

log2

Det

1

P2P −
1

.

(cid:20)q

(cid:0)

(cid:21)
(cid:1)

Entropy comparison for equal-σ1p processes. From the expression above Eq.[4.24] it is clear that σ2

e is linearly
proportional to the amplitude of the power spectrum P (w), so that h will depend (logarithmically) on the

normalization of P (w). It is interesting to compare the entropy for diﬀerent shapes of P (w) which have been

normalized in the same way. Here we will consider the case where we normalize P (w) so that ~η has the same

1-point variance. We will see that this is equivalent to ﬁx the traces of the P matrix Eq.[4.23].

First, using (4.19) and the properties of the trace, we get Tr(C) =

Tr

C

. Combining this

Det(C) =

Det

C

.

(4.28)

N

Tr(C)

Tr

C



(cid:16)



(cid:17)

b





(cid:16)

(cid:17)

b

∆ω
2π∆t

(cid:18)

(cid:19)

(cid:16)

(cid:17)

b

1p ≡

Now, bearing in mind the usual deﬁnition of the 1-point variance: σ2, which reads σ2

C(η(t), η(t)), let’s

1
N
≡
For the case of uncorrelated variables (white noise) with equal sigma: σ1 = . . . = σN ≡
σ1p = σe = σ0 in Eq.[4.24]. In general σ1p 6

= σe when there are correlations.

1
2π∆t

σ2
1p(C)

Tr(C) =

Tr(P ).

1
N

Using this deﬁnition, we have from (4.28):

(4.29)

σ0, we have that

(4.23)

(4.24)

(4.25)

(4.26)

(4.27)

(4.30)

σ2
e ≡

Det1/N (C) = σ2
1p

Det1/N (P )
Tr(P )/N

11

Inserting this result into (4.12), we can write:

h = h1p +

log2

1
2N

Det(P )
[Tr(P )/N ]N

"

,

#

(4.31)

where

h1p = h0(σ1p) = log2

+ exponentially small part.

(4.32)

√2πe
(cid:20)

σ1p
∆η

(cid:21)

These new formulae are adequate for comparing processes with the same value of σ2

1p and diﬀerent P ’s (i.e.,
diﬀerent power spectra). The 1-point entropy h1p denotes the entropy per component of a white noise with a
variance σ2

1p, as in this case P

1 = . . . = σ2

N = σ2

I, causing the second term on the r.h.s. of (4.31) to vanish.
Det1/N (M ) holds. Both

For any square and positive semideﬁnite matrix M , the inequality 1

∝

C and P satisfy these conditions. Therefore σ2

σ2
1p and h
only for the white noise itself. In any other case, a Gaussian process with the same σ1p has smaller eﬀective

h1p. The equality is achieved when P

I, i.e.,

e ≤

≤

∝

N Tr(M )

≥

variance and lower entropy than the corresponding white noise. This is easy to understand from (2.4) or (2.5).

Asymptotic expressions. When the exact form of Det(P ) is not easy to obtain, we can resort to the following

procedure. We may assume that P (

ω) = P (ω) and that the mode with ω0 = 0 has to be removed, as often

happens (this mode is related to the correlation at t

and, if one requires that the system be ergodic, it

−

→ ∞

should vanish). Then,

log2[Det(P )] =

log2[P (ωj)] = 2

log2[P (ωj)],

(4.33)

and an application of the Euler-Maclaurin summation formula (see e.g. [11]), leads us to the approximation

log2[P (ωj)] =

dω log2[P (ω)] +

(log2[P (ωMax)] + log2[P (ωmin)])

(4.34)

N/2

j=1
X

1
∆ω

ωMax

ωmin

Z

+higher order terms in ∆ω.

The same method can be applied to the calculation of Tr(P ), in Eq.[(4.31)] i.e.,

N/2

j=1
X

1
∆ω

"

ωMax

ωmin

Z

1
2

2

P (ωj) = 2

dω P (ω) +

(P (ωMax) + P (ωmin)) + higher order terms in ∆ω

.

(4.35)

#

Filters. Quite often, stochastic processes go through what is called a ﬁlter. Formally, ﬁlters can be pictured

as multiplicative changes in the power spectrum. Therefore, everything happens as if we had a new power

spectrum function, say P ′, coming from the replacement

P (ω)

P ′(ω) = P (ω)φ(ω),

−→

where the φ function is the frequency response of the ﬁlter itself. Let h′ denote the new entropy per component.

It is immediate that the change caused by the introduction of φ will be given by

hφ =

log2[

Det(Φ)],

Φ = diag(φ(ω

N/2), . . . , φ(ωN/2)),

−

where h denotes the entropy per component for the same process when no ﬁlter is present.

h′ = h + hφ,

1
N

p

(4.36)

N/2

j=−N/2
X
j6=0

N/2

j=1
X

1
2

12

4.2.2 Simple power-law power spectrum

Here, we will consider a power spectrum of the type

(cid:18)
where A is a constant that sets the overall amplitude and w0 some characteristic scale that sets the time units.

Taking into account (4.16) we evaluate

P (ω) = A

np

,

ω
|
|
w0 (cid:19)

Det

1
A

P

(cid:18)

(cid:19)

=

P (ωj) =

j
Y

N np

2np

∆ω
ω0 (cid:19)

(cid:18)

N
2

!
(cid:21)
(cid:19)

(cid:20)(cid:18)

(where the zero mode j = 0 has been omitted). Making use of Stirling’s approximation for large N/2, and using

(4.17), we ﬁnd:

σ2
e ≡

Det1/N (C) =

A
2π∆t

π
e w0∆t

(cid:18)

(cid:19)

np

= σ2
0

np

,

ωMax
e w0 (cid:19)

(cid:18)

where σ0 corresponds to the white noise case (np = 0). If we normalize the spectrum at w0 = ωMax then for

np < 0 we have that h > h0 and the optimal compression rate has to decrease, while for np > 0 we have h < h0.

Some special values are given in table 2, and are also illustrated by Fig. 1. However, this comparison depends

on the normalization and involves noises with diﬀerent values of σ1p, as we have only changed the value of np

without doing anything to maintain the initial σ1p. In this case Eq.[4.29]:

σ2
1p =

1
2π∆t

1
N

P (ωj) =

j
X

A
πN ∆t

∆ω
w0 (cid:19)

(cid:18)

np

Snp

N
2

,

(cid:18)

(cid:19)

Snp

N
2

(cid:18)

(cid:19)

N/2

≡

j=1
X

jnp.

(4.40)

Making use of (4.31), we are led to

h = h1p +

= h1p +

np
log2
N
np + 1
2

N
2

1
log2
2
np
2

−

−

!
(cid:19)
(cid:21)
N
2

(cid:20)(cid:18)
log2

2
N

Snp

N
2

(cid:20)
log2(e)

(cid:18)
1
2

−

(cid:19)(cid:21)

log2

Snp

N
2

+

O

log2(N )
N

, . . .

,

(cid:20)
where the Stirling approximation has been applied. When np >

(cid:19)

(cid:18)

(cid:19)(cid:21)

(cid:18)
1, we apply the Euler-Maclaurin summation

(cid:18)

(cid:19)

−

formula (4.35) and obtain

σ2
1p =

1
2π∆t

A
np + 1

π
w0∆t

np

1 +

O

1
N

,

(cid:18)

(cid:19)

(cid:20)

(cid:18)

(cid:19)(cid:21)

for np >

1.

−

For the np =

1 case may be more straightforwardly estimated by using

−

where γ is Euler’s constant: γ

S

1

−

N
2

(cid:18)

≃

= Ψ

+ 1

+ γ = ln

+ γ +

N
2

(cid:19)

1
N

,

O

(cid:18)

(cid:19)

N
2

(cid:19)

(cid:19)
(cid:18)
(cid:18)
0.57721 . . . . So, σ2
1p becomes

σ2
1p =

(Aw0)
2π2

N
2

ln

+ γ +

(cid:20)

(cid:18)

(cid:19)

1
N

O

(cid:18)

(cid:19)(cid:21)

for np =

1

−

13

(4.37)

(4.38)

(4.39)

(4.41)

(4.42)

(4.43)

(4.44)

Then, by the previous formulas and by (4.31),

h1p −
h1p +

1
2

np
2
1
2

log2(e) +
1
2

log2(e)

−

log2(np + 1) +

O

(cid:18)

+ γ

+

log2(N )
N

, . . .

,

(cid:19)
log2(N )
N

log2

ln

N
2

(cid:20)

(cid:18)

(cid:19)

(cid:21)

O

(cid:18)

for np >

1,

1,

−

−

, . . .

,

for np =

(cid:19)

h = 




where h1p, given by (4.32), is the entropy per component of a white noise with the σ0 = σ1p. Note that although

it seems that h diverges with N for np =
Although σ2

−

1p diverges logarithmically with N , the information content does not as σ2

e in Eq.[4.39] is ﬁnite:

1 this is an artifact of this type of comparison with a ﬁxed σ1p.

(4.45)

(4.46)

σ2
e =

(Aw0)
2π2 e.

for np =

1

−

Some examples are illustrated by 5th column of Table 2 and Fig. 3.

Figure 3: Entropy and optimal compression rate for diﬀerent power spectra with the same σ1p, but (unlike in

Fig 2) keeping li = 16 bits ﬁxed and ω0 = ωmin. The present set of cases is: np = 0 (solid line), np =

1

−

(dashed line) and np = +1 (dotted line).

Fig. 4, shows the entropy h as a function of the spectral index np given by the above formulas. As can

be seen, h has a maximum at np = 0, as expected.

4.2.3 ‘f 0 + 1/f ’ spectrum

In practice, realistic power spectra include often combinations of several powers. This new example corresponds

to a power spectrum including two terms: one with np = 0 (white noise) and another with np =

1 (usually

called 1/f noise), which we write as

−

(4.47)

where f stands for frequency w

2πf , and fk for the so called knee frequency, where both contributions are

equal. We shall assume that w has been discretized as in the previous cases. Because a direct evaluation of

≡

P (ω)

A

1 +

= A

1 +

ωk
ω
|

| (cid:19)

fk
f
|

| (cid:19)

,

(cid:18)

≡

(cid:18)

14

Figure 4: Entropy h (continuous line) and optimal compression cr, opt (dashed line) for li = 8 bits as a function of

the spectral index np ( for a power law P (ω)

ωnp) with a ﬁxed one-point variance σ1p and λ1 ≡

∝

σ1p/∆η = 0.25

Det(P ) would not be so easy now, we shall apply the above commented approximation based on the Euler-

Maclaurin summation formula. After performing the integration (4.34) for the P (ω) of eq. (4.47) one gets

ωk
ωMax (cid:19) (cid:20)
The correspomding entropy is just given by Eq.[4.12]. When ωk << ωMax we recover the white noise case

ωMax + ωk
ωmin + ωk (cid:21)

A
2π∆t

σ2
e =

(4.48)

1 +

(cid:18)

ωk/ωMax

Eq.[4.25], while in the case ωk >> ωMax the 1/f noise dominates and we recover Eq.[4.46], as expected. We

observe that a combined power spectrum (4.47) with reasonably small A is eﬀectively equivalent to one of the

type P (ω) = A

with an intermediate np between 0 and

1. An illustration of the values of h and

−

optimal compresion for this case is shown in Fig6 and also in Fig.1 as dashed line.

np

ω
|
|
ω0 (cid:19)

(cid:18)

Typically we will have that ωmin << ωMax and also ωmin << ωk. In this case the only relevant parameter

is r

ωk/ωMax:

≡

A
2π∆t
where r = 0 reproduces the white noise case and large r reproduces the 1/f case (np =

(1 + r)1+r
rr

(1 + r)1+r
rr

σ2
e =

= σ2
0

,

(4.49)

1) with arbitrarily

−

large normalization. For r = 1 we have that the eﬀective variance of the signal is four times as large as the
white noise part σ2

0, so that the entropy will be one unit larger with the combined spectrum that with the
white noise alone. Other values for h and cr as a function of r are shown in Fig.5. In this case λ = ∆η/σ0 = 1
so that h0 ≃

3.91 (li = 8 bits) which agrees with the values at r = 0.

2.047 and cr, opt ≃

e = 4σ2

Another way to compare the two cases is to use an equal σ1p comparison with a white noise. In this case:

σ2
1p ≡

1
N

Tr(C) =

A
2π∆t

1 +

ωk
ωMax

S

1

−

N
2

,

(cid:18)

(cid:19)

(cid:21)

(4.50)

(cid:20)

15

Figure 5: Entropy h and optimal compression cr, opt (li = 8 bits) as a function of r

ωk/ωMax for a ‘f 0 + 1/f ’

≡

noise. We have choosen λ = ∆η/σ0 = 1 and symbols of li = 8 bits.

and, using (4.31),

h = h1p −

log2 


s





ωMax + ωkS

1(N/2)

−

ωMax + ωk

+

ωk
ωMax

log2

ωMax + ωk
ωmin + ωk (cid:21)

+

O

(cid:18)

log2(N )
N

,

(cid:19)

(cid:20)r

(4.51)

where h1p stands for the entropy per component of a Gaussian white noise with the same 1-point variance σ2
Of course, the ωMax

1p.
1 case of (4.45). An example of this type

0 limit of this expression yields the np =

of noise is shown in 4th column of Table 2 and Fig.2.

ωk −→

−

h

λ = ∆η/σ0

h0 = h1p

np = 0

f 0 + 1/f

σ0

6.37

4.05

3.05

2.05

σ0

σ1p = σ0

7.37

5.05

4.05

3.05

5.89

3.57

2.57

1.57

1

np =

−
σ1p = σ0

5.71

3.39

2.39

1.39

0.05

0.25

0.50

1.00

Table 2. Shannon entropy per component h for large N , and several values of λ = ∆η/σ0. The purely white-noise case

h0 for a given σ0 and λ are listed in column 2. Columns 3 and 4 gives the results for a combination P (ω) = A(1+ωk/|ω|),

with ωk = ωMax (r = 1) when the white noise is ﬁxed to the same σ0 (column 3) and when the 1-point sigma is ﬁxed to

σ1p = σ0 (column 4). In column 5 we have listed the values for a correlation of the np = −1 type P (ω) = A(w0/|ω|) and

σ1p = σ0. In the last two cases N = 1000.

16

Figure 6: Comparison between purely white noise (solid line) and two processes of the type P (f )

|
with fk =10 Hz (dashed line) and 100 Hz (dotted line), for li = 16 bits, and without imposing the equal-σ1p

1 + fk/

f
|

∝

constraint. In both cases h > h0, while in the analogous example of Fig.3, where it happenned just the opposite.

We can see there how h < h1p when we compare spectra normalized to have the same σ2

1p, while h > h1p
when we just add a term (1/f) to the (constant) white noise power spectrum. The interpretation is simple,

as shown in Eq.[4.12] the entropy is given by the eﬀective correlation. On the one hand, adding power always
increases σe (see Eq.[4.24]), and therefore h. But, on the other hand, σ2

σ2
1p so that, when σ1p is ﬁxed, any
power spectrum gives smaller h than the white noise and, as we said above, this can be easily understood in

e ≤

the light of inequality (2.3) . This change of behaviour can be seen comparing Figures 2 with Fig.6.

4.2.4 Examples of piecewise-mixed spectra

1. Here we study the piecewise-deﬁned spectrum:

The result of applying (4.26) and making asymptotic approximations for large values of ωL

∆ω , ωH

∆ω , and ωMax

∆ω is

ωL + ωH
2ωMax (cid:19)
2. Another case which can be of interest is:

h = h0 +

1
(cid:18)

−

log2(e)

log2

1
2

−

Max

ω2
ωLωH (cid:19)

(cid:18)

+ higher order terms.

(4.53)

A,

A

A

ωL
,
ω
ωLωH
ω2

ωL,

for ω

≤
for ωL < ω

,

for ωH < ω

ωMax.

ωH,

≤

≤

P (ω) = 



P (ω) =

A′,

A +

,

B
ω
|

|






ωL,

for ω

≤
for ωL < ω

ωMax.

≤

17

(4.52)

(4.54)

Taking now as reference the case in which B = 0 and A′ = A, we may write

h = h(A′ = A, B = 0)+ log2

1 +

"r
B
ωMaxA

+

B

AωMax # −

ωL
ωMax

log2

1 +

B
AωL #

log2

"r

AωMax + B
AωL + B #

+

log2

A′
A #

"r

+ . . .

"r
ωL
ωMax

(4.55)

5 Conclusion

We have studied the Shannon entropy h of a Gaussian discrete noise ηi characterized by its power spectrum P .

This is given by h

log2

√2πe σe/∆η

, where σe = σe(P ) is given by Eq.[4.24] and ∆η is the discretization

width. Finete corrections to this formula are given in the Appendix A, ie eq.((A.6)) and eq.((A.14)). The ﬁrst

(cid:0)

(cid:1)

thing to notice is that σe changes linearly with the amplitude of P , so that the entropy increases logarithmicaly

≃

with P . For a given normalization, how does the entropy depend on the shape of the power spectrum? We can

compare the entropy of two types of noises using the entropy diﬀerence ∆h = h

h0. In cases with spectra

of the type P (ω)

, ∆h can be quite sensitive to the choice of ω0, whose variations may even cause a

reversal of the sign of ∆h. This type of change is due to the already commented logarithmic dependence of h

(cid:16)

(cid:17)

np

ω
|ω0
|

∝

−

on the amplitude of P . If we ﬁx the (1-point) variance of the noise, we have seen that the maximum entropy

(minimum compression) is the one given by white noise (or constant P ), as it is expected. For a power law

spectrum P (ω)

ωnp, with a ﬁxed one-point variance, we have that the larger

the smaller the entropy for
1 (eg Eq.[4.45] and Fig.2). Notice that when ∆η > √2πe σe we have h < 0 indicating that the data has

np|
|

∝

np >

−

been discretized with such a low resolution that there is no information left.

We deﬁned the optimal compression rate as the ratio of the initial average length per code unit li over the
Ns)

Shannon entropy h per component: cr, opt ≡
bits the optimal compression rate depends on the discretization width ∆η through a simple relation:

li
h . For a linearly discretized data set with li = Nbits = log2(

cr, opt ≡

li
h ≃

Nbits

log2

√2πe σe/∆η

,

(cid:1)

(5.1)

The choice of ∆η is in principle arbitrary and depends on what we want to do in the data processing of the

(cid:0)

∆η
signal (noise). The ﬁnal compression factors will depend only on the ratio of these two quantities λ
σ and
the number of bits Nbits we choose to represent the data. Another way of writing this results is: cr, opt ≃
log2(R)
log2(∆η)
, where R is the range of the random variable and hcont is a constant depending on the
−
log2(∆η)
hcont −
type of process, which may be interpreted as the Shannon entropy per component in the continuum limit. In

≡

mathematical terms, hcont involves the determinant of the correlation matrix. If the initial length li is held

ﬁxed, independently of ∆η, the relation is just cr, opt(∆η)

≃
The purely white noise case (np = 0) oﬀers rather slight hopes, for moderate ranges R. If we choose

hcont −

li
log2(∆η)

.

R = (

N0σ, N0σ) with N0 = 3, and λ = ∆η/σ = 0.25 the compression rate is of cr, opt = 1.13 —only marginally

above one— and, yet, this happens at the expense of losing resolution to the extent that only four distinct values

−

are observed within each interval of width σ. Less resolution than that may be too little for many applications.

18

One could wonder what happens, in the opposite case, when resolution is kept at any cost. For a binning of 28

distinct intervals within the same range, λ has to take on such a value that the compression rate is a meagre

1.07. Such a thinly spaced binning means that the white noise is seen very much like a uniformly distributed

one, and has a similar uncompressibility.
On the other hand, for ﬁxed σ2

1p a negative spectral index lowers the eﬀective information and helps
compression. Moreover, the optimal compression rate increases as the sampling time interval decreases. As we

see in Fig. 2, when ∆η = 0.25 the compression rate for np =

1 with the same σ1p as for the white noise

is

∼

1.4. Moreover, the diﬀerence between np =

1 and np = 0 increases as the discretization parameter

λ = ∆η/σ grows. However, one cannot think of arbitrarily raising its value, as such a thing would imply a

−

−

widening of the discretization error, and an even greater loss in resolution for the values of our variables.

A combination of both types has also been studied by taking a ‘mixed’ power spectrum with np = 0

−

plus 1/f (i.e. np =

1) terms. If the coeﬃcient of the np = 0 part is low enough, the behaviour shown is

intermediate between purely np = 0 and purely np =

1, and can be interpreted as if it just had an eﬀective

np between both values. When P (ω)

, if A0 is set to 1, h is no too sensitive to increases in ω0

much above the knee frequency. On the contrary, if ω0 is kept constant, variations in A0 may easily change the

(cid:17)

−

A0 + ω0
ω
|

|

∝

(cid:16)

sign of ∆h. As a common feature to all possible situations, one observes an increase in compressibility as the
measured data involve more and more correlation, i.e. larger dominance of their spectral f np-parts with np 6
(see Fig. 4).

= 0

Imagine a situation of a data set that consists of a slowly varying signal (to be stored with l bits) plus

large amplitude noise that dominates over the signal on large frequencies. The signal is to be recovered by

averaging the noise after transmission (and therefore compression) and a careful calibration of instabilities in

the noise. This is a commom situation for scientiﬁc measurements on-board satellites collecting data with low

signal-to-noise ratio. In this case the noise component can be kept with a low resolution and one can choose

≃

∆η
σe which gives h
≃
compression rates cr, opt ≃
compression values in practice, an eﬃcient coding method has to be used. For one dimension, the Huﬀman

2.05 indicating that all information is contained eﬀectively in two bits. Then high
l/2 could be obtained: eg cr, opt ≃

16 bits. To achieve such a high

8 for l

≃

scheme is known to be reasonably close to the optimal value. When data (symbols) are correlated in a manifest

way, as the general case considered here, other methods have to be used in combination. One of the simplest

methods that take into account correlations is run-length encoding, where the signal is converted to a stream of

integers that indicate how many consecutive symbols are equal (see [13]). This would be quite eﬃcient in the

We would like to thank Pablo Fosalba for stimulating disscusions. This work has been supported by CSIC, and

situation we have just mentioned.

Acknowledgements

by DGES (MEC), project PB96-0925.

19

References

[1] C.E. Shannon, ’A Mathematical Theory of Communication’, Bell Syst. Tech. J., vol. 27 (1948) 379-423.

[2] C.E. Shannon, ’A Mathematical Theory of Communication’, Bell Syst. Tech. J., vol. 27 (1948) 623-656.

[3] C.E. Shannon, ’Communication in the Presence of Noise’, Proc. IRE, vol. 37 (1949) 10-21.

[4] D. Welsh, Codes and Cryptography, Clarendon Press, Oxford, 1988.

[5] V. Bhaskaran and K. Konstantinides, Image and Video Compression Standards, Kluwer Academic Press,

[6] R.G. Gallager, ‘Variations on a theme by Huﬀman’, IEEE Trans. on Info Theory, IT 24 (1978) 668-674.

[7] J. Venbrux, P.-S. Yeh and M.N. Liu, ‘A VLSI Chip Set for High-Speed Lossless Data Compression’, IEEE

Trans. on Circuits and Ssystems for Video Technology, 2 (1992) 381-391.

[8] A.N. Kolmogorov, ‘On the Shannon Theory of Information Transmission in the Case of Continuous

Signals’, IEEE Trans. Inform. Theory IT-2 (1956) 102-108.

[9] A. Papoulis, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 1984.

[10] Bateman Manuscript Project, A. Erd´elyi et al, Higher Transcendental Functions, McGraw-Hill, New York,

1995.

1953.

[11] M. Abramowitz and I.A. Stegun, Handbook of Mathematical Functions, Dover, New York, 1972.

[12] S. Ihara, Information Theory, World Sci., Singapore, 1993.

[13] R. Hunter, A.H. Robison, Proceedings in the IEEE, 68 (1980) 854-867.

A Appendix: discrete calculations

A.1 One-dimensional case

First, we rewrite the Z of (3.2) as

where

∆η
σ
is the size of the discretization interval in units of σ, and

≡

λ

n2λ2

∞

Z =

e−

2 = θ

n=

X

−∞

λ2
2π

(cid:18)

; 0

,

(cid:19)

θ(β; m)

∞

n2me−

πβn2

≡

n=

X

−∞

20

(A.1)

(A.2)

is a notation for the sort of Jacobi elliptic theta functions appearing in this calculation.

Note that the discretization has enabled us to deal with a discrete probability set —(3.1)—- thus avoiding

the well-known diﬃculties associated with H for continuous probability distributions. In our own case (calling

H

H1 all through this subsection),

≡

H =

1
ln(2)

−

λ2
2




−

ln

θ

−

λ2
2π

; 0

(cid:20)

(cid:18)

(cid:19)(cid:21)

λ2
2π
λ2
2π

; 1

; 0

(cid:19)

(cid:19)

θ

θ

(cid:18)

(cid:18)

.







θ(β; 1) =

1
π

d
dβ

−

θ(β; 0).

For m = 1, we just observe that

Using this, we arrive at

H =

1
ln(2)

−

β

d
dβ

(cid:26)

−

(cid:27)

ln [θ (β; 0)]

ln [θ (β; 0)]

,

with β =

=

(A.3)

λ2
2π

1
T

.

By (A.1), this can also be written as

d
dT
Up to the trivial change of units —or, equivalently, a conventional modiﬁcation of the Boltzmann constant—

1
ln(2)

[T ln(Z)] .

(A.4)

H =

H is the thermodynamical entropy S of a one-particle system at temperature T with partition function Z. In

the situation we are studying, this Z is Z(T ) = θ

; 0

as given by eq. (3.2). However, the validity of eq.

(A.4) is quite general: in fact, for any system with probabilities of the form

1
T

(cid:18)

(cid:19)

pJ =

EJ /T

e−

Z

, where Z =

EI /T

e−

XI

(where I, J can be single indices or multiple indices), one may check that, after applying the deﬁnition (2.1) or

(2.3), eq. (A.4) holds. Therefore, we might as well have started our calculation of H from eq. (A.4) itself (and

we will do so for the N -dimensional case). Analogously,

T ln(Z) plays the role of the Helmholtz free energy

F , satisfying the relation S =

dF
dT

.

−

A ‘ﬁnely’ or thinly spaced discretization means that λ should be small. However, the above expression

of θ(β; m) as a series is obviously inadequate when β =

1. Such a diﬃculty will be overcome by recalling

the remarkable theta function identity (see e.g. ref.[10])

−

λ2
2π ≪

θ(β; 0) =

1
√β

θ

1
β

; 0

.

(cid:18)

(cid:19)

Applying now this identity to (A.3) or (A.4), expanding each part for small λ and diﬀerentiating, one ﬁnds

H = 1

ln(2) "

1
2

+ ln

√2π

 

λ !

+ 2

1
(cid:18)

−

2π2
λ2

(cid:19)

2π2
λ2

e−

2e−

4π2
λ2 +

6π2
λ2 +

e−

−

8
3

2π
λ2 e−

8π2
λ2

O

(cid:18)

(cid:19)#

which, regarded as an expansion, is quickly convergent for 0 < λ

√2π. (One should notice that, actually, the

two expressions have a generous overlap around β

1 where both converge and any of them can be consistently

≪

(A.5)

(A.6)

used).

≃

21

There is no explicit dependence on σ, as the only relevant variable is the relative discretization size λ.

e

Even for moderately large values of λ, the next-to-leading part of H is very small: e.g.
− π
β

− 2π2
λ2
λ2 ≃
68, respectively. Neglecting such terms, we easily obtain a good approximate formula, which may

17; at λ = 1/2 these two quantities become 1.3

β = 2π e
10−
and 6.6

for λ = 1 we have

− 4π2
λ2
λ2 ≃

β = 2π e

8, e

10−

10−

10−

−2 π
β

1.7

4.5

33

·

·

·

·
be reexpressed as

H = log2

2πe
λ2

+

!

2π
λ2 e−

2π2
λ2 , . . .

,

(cid:19)

O

(cid:18)

 r

with λ given by (A.2). This yields eq. (3.3)

A.2 N-dimensional case

After looking at the Z of eq. (4.8), let’s introduce, for convenience, the new notations

σ

min(
{

σ1, . . . , σN }

),

≡

1

χ−

σ2 C−

1,

≡

∆η
σ

,

λ

≡

which enable us to write

λ2
2

e−

Z =

∞

n1,...,nN =
X

−∞

(n1, . . . , nN )T χ−

1 (n1, . . . , nN )

.

(A.9)

In terms of the multidimensional Jacobi theta function

we can put

θN (β

M )
|

≡

∞

n1,...,nN =
X

−∞

πβ

−

e

N

i,j=1
X

Mijninj

,

Z = θN

1

β

χ−
|

,

λ2
2π ≡

1
T

.

β

≡

(cid:0)
By (A.4), the joint Shannon entropy (now H
≡

(cid:1)

HN ) becomes

H =

[T log2(Z)] = log2

θN

d
dT

1

β

χ−
|

β

d
dβ

−

(cid:2)

(cid:0)

(cid:1)(cid:3)

log2

θN

1

β

χ−
|

.

(cid:2)

(cid:0)

(cid:1)(cid:3)

We are interested in approximations for small β, but the present expressions are inadequate for this situation.

The way out is to take advantage of a Jacobi identity for multidimensional theta functions, namely,

θN (β

M ) =
|

1
[Det(M )]1/2βN/2 θN

1
β

(cid:18)

M ∗

,

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

which, unlike the initial expression, may be expanded for small β. Doing so (and noting that C has to be real

when viewed in conﬁguration space),

(A.7)

(A.8)

(A.10)

(A.11)

(A.12)

(A.13)

(A.14)

H =

N
ln(2)

1
2

(cid:20)

+ ln

1
2N

+

1
√β
(cid:19)
(cid:18)
2πe
(∆η)2 C

lnDet(χ) +

O
(cid:18)
2πσ2
(∆η)2 e−

β min (C/σ2)

π

e−

1
β
2π2min C
(∆η)2

(cid:19)(cid:21)

,

(cid:19)

= log2

Det

"s

(cid:18)

+

N

O

(cid:18)

(cid:19)#

22

where min C means the minimum over the (positive) eigenvalues of the correlation matrix C, and where the

relations (A.8) and the deﬁnition of β in (A.11) have been used. More terms of this expansion can be obtained

explicitly by using Eq. (A.6). Notice, however, that each order of (A.6) gives rise here, in principle, to N diﬀerent

orders (the ﬁrst N of them corresponding to the sequence of eigenvalues of C, increasing in magnitude). The

bottom line gives us (4.9).

A.3 Discrete Fourier transforms

The continuous transforms taken as reference are

where k and x are a pair of conjugate variables. Discretizing them,

b

and calling

that

Therefore, we can write

we construct discrete transforms which, in the continuum limit, reproduce (A.15):

Taking into account (A.16) and the correct relation between sampling intervals, i.e. ∆k =

, one realizes

2π
N ∆x

η(k) =

dx e−

ikx η(x),

η(x) =
b

eikx

η(k).

dk
2π

Z

Z

,






kn = n∆k,
xn = n∆x, 


ηn ≡
ηn ≡
b

η(kn),
η(xn), 

b





ηn = ∆x

e−

iknxm ηm,

ηn =
b

eikmxn

ηm.

∆k
2π

m
X

m
X





b

2π
N

kn xm = km xn =

mn.

ηn = ∆x
∆k
2π

ηn =
b

(W ~η)n,

(W ∗

η)n. 


x

k

→

→

t,

ω,

23

where W is the symmetric matrix with coeﬃcients Wmn = e

i

2π
b
N

mn


. After renaming

this yields the expressions (4.14).

(A.15)

(A.16)

(A.17)

(A.18)

(A.19)

(A.20)

(A.21)

A.4 Power Spectrum

Recall ﬁrst the deﬁnition of covariance matrix:

Cjk =

ηjη∗ki
,
h

(A.22)

where

. . .
i
h

denotes statistical average over realizations of the stochastic process η. For a stationary stochastic

process we have that Cjk = Cj

k can only be a function of j

k, eg the covariance matrix is Toeplitz matrix.

−

−

It is a simple exercise to show that in this case the covariance matrix in Fourier space

C is always diagonal:

b

(A.23)

The power spectrum is then deﬁned as:

η∗ki ∝

δjk

ηj
Cjk ≡ h
b

b

b

P (ωj)

δjk
∆ω

,

Cjk ≡
b

in analogy with the continuous deﬁnition:

η(ω)
h

η∗(ω′)
i

= P (ω) δDirac(ω

ω′).

−

b
B Appendix: The continuous random variable case

b

As it is well-known, Shannon’s entropy was ﬁrstly designed to deal with discrete random variables

where the index runs through all possible diﬀerent countable values of the r.v.. The problem with the continuous

r.v. is that diﬀerent ηj ’s do not form a partition. To deﬁne H(η) we form ﬁrst the discrete r.v. η∆ obtained by

H(η)

p(ηj ) log2[p(ηj)],

≡ −

j
X

η∆ ≡

n∆η

, if n∆η

∆η < x

n∆η.

−

≤

P (η∆ = n∆η) = P (n∆η

∆η < η

n∆η) =

−

≤

dη f (η) = ∆η ¯f (n∆η),

(B.3)

n∆η

n∆η

∆η

−

Z

where ¯f (n∆η) is a number between the maximum and minimum of f (η) in the interval (n∆η

∆η, n∆η).

−

Applying Shannon’s deﬁnition, we have:

rounding oﬀ η

Clearly,

and, since

(A.24)

(A.25)

(B.1)

(B.2)

(B.4)

(B.5)

H(η∆) =

∆η ¯f (n∆η) log2[∆η ¯f (n∆η)]

∞

−

n=

X

−∞

∞

∆η ¯f (n∆η) =

dη f (η) = 1,

n=

X

−∞

∞

Z

−∞

24

we conclude that

H(η∆) =

log2(∆η)

−

∞

−

n=

X

−∞

∆η ¯f (n∆η) log2[ ¯f (n∆η)].

(B.6)

As ∆η

0, the r.v. η∆ tends to η; however, its entropy H(η∆) tends to

because

∞

log2 ∆η

−

→ ∞

. This is

why we deﬁne the entropy H(η) of η not as the limit of H(η∆) but as the limit of the sum H(η∆) + log2(∆η)
when ∆η

0, i.e.:

→

→

H(η∆) + log2(∆η)

dη f (η) log2[f (η)]

as ∆η

0.

→

So, the deﬁnition of ‘entropy’ for a continuous variable is:

∞

−→

Z

−∞

∞

Z

−∞

H(η) =

dη f (η) log2[f (η)],

where the integration extends only over the region where f (η)

= 0, as we have f (η) log2[f (η)] = 0 if f (η) = 0.
This ‘entropy’ is more usually called diﬀerential entropy in the literature and its deﬁnition can also be extended

to multivariate probability distributions. It is easy to see then that the above limit translates into:

H(~η∆) + log2(∆η)N

∞

−→

Z

−∞

d~η f (~η) log2[f (~η)]

as ∆η

0

→

when the N -dimensional space of ~η is latticed with ∆η-boxes. So, we could approximate H(~η∆)
H(~η). In our case we have deﬁned the compression ratio as cr, opt ≡
look just back to the approximate:

average length
h

, where h

log2(∆η)N +
H(~η∆)
. If we
N

≃ −

≡

The last summand in the previous expression is the average uncertainty per sample in a block of N consecutive

samples. The limit N

of it is what is known as diﬀerential entropy rate:

→ ∞

So, if we imagine that we have a stochastic process inﬁnitely long and ~η is a vector r.v. whose dimension

tends to inﬁnity (i.e. ηj = η(tj) and we take samples for a long time or just many samples) we could then

(B.7)

(B.8)

(B.9)

(B.10)

(B.11)

(B.12)

Regarding h(~η) as the ‘continuous part’ of the entropy per component —i.e. hcont—, this relation amounts

approximate:

to eq. (3.5).

h

≈ −

log2(∆η) + H(~η)/N.

h(~η) = lim
→∞

N

H(η1, . . . , ηN )
N

h

≃ −

log2(∆η) + h(~η).

25

6
(B.13)

(B.14)

(B.15)

B.1 Entropy in the continuous case

For the one-dimensional Gaussian distribution in Eq.[3.1] it is straight forward to show that:

hcont = log2

√2πe σ

,

h

≡

h

i

in agreement with Eq.[3.3] in the limit of small ∆η, as expected from the comments in the previous section. For

the case of N-dimensional Gaussian noise with correlations, we can use the fact that h(~η) is well-known (see eg.

[9]) for a gaussian stochastic process with power spectrum P (ω):

where ˜P (ω) refers to the discrete stochastic process derived from the continuous one P (ω) by the relation

h(~η) = log2[√2πe] +

dω log2[ ˜P (ω)].

1
4π

π

π

Z

−

˜P (ω) =

1
∆t

∞

P

m=

X

−∞

(cid:18)

ω + 2πm
∆t

,

(cid:19)

π

−

≤

ω

≤

π,

where ∆t is the sampling interval that discretizes the process. For power spectra with a bandwidth limitation

this reduces to (see [12]):

1
∆t
where ˜P (ω) refers to the process ηn = η(t = n∆t). In this case we can do a simple change of variables ω′ = ω/∆t
in Eq.[B.14] to ﬁnd:

˜P (ω) =

1
∆t

(B.16)

P (

ω)

1
4π

π

π

Z

−

dω log2[ ˜P (ω)] =

2 log2 ∆t +

−

π/∆t

∆t
2π

0
Z

dω′ log2[P (ω′)].

(B.17)

where we have used the parity of P (ω) and the fact that the range in Eq.[B.14] is symmetric . Recalling that
ωMax = π/∆t and we are using ωmin ≃
summation formula Eq.[4.34], so that the continous calculation of the entropy given by Eq.[B.14] and Eq.[B.12]

0 we see can that this calculation is equivalent to the Euler-Maclaurin

yields identical results to those of the discrete calculation Eq.[4.12] in the limit of large N .

26


a statistical tool to unfold data distributions

lots :

sP

M. Pivk a and F.R. Le Diberder b

a CERN,
CH-1211 Geneve 23, Switzerland
b Laboratoire de l’Acc´el´erateur Lin´eaire,
IN2P3-CNRS et Universit´e de Paris-Sud, F-91898 Orsay, France

Abstract

P

A novel method called s

lot, painless to implement, is presented. It projects out the signal
and background distributions from a data sample for a variable that is used or not in the original
likelihood ﬁt. In each bin of that variable, optimal use is made of the existing information present
in the whole event sample, in contrast to the case of the usual likelihood-ratio-cut projection plots.
The thus reduced uncertainties in the low statistics bins, for the variable under consideration,
makes it possible to detect small size biases such as pdf/data mismatches for a given species,
and/or presence of an unexpected background contamination, that was not taken into account in
the ﬁt and therefore was biasing it. After presenting pedagogical examples, a brief application
to Dalitz plots and measurement of branching ratios is given. A comparison with the projection
plots shows the interest of the method. Finally are given the diﬀerent steps to implement the

lot tool in an analysis.

s

P

4
0
0
2
 
b
e
F
 
7
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
8
0
2
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

Contents

1 Introduction

2 To begin easily: inP

lots, x

y

∈

3 The right tool: s

lots, x

y

P

6∈

3

4

5

7

7

8

9

12

18

19

19

20

3.1 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.1 Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.2

Statistical uncertainties . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Extended s

lots: a species is known (ﬁxed)

. . . . . . . . . . . . . . . . .

P

3.2.1 Assuming M0 to be known . . . . . . . . . . . . . . . . . . . . . . . 10

3.2.2 Assuming M0 to be unknown . . . . . . . . . . . . . . . . . . . . . 11

4 Pedagogical examples

4.1 Simple cut-and-count analysis . . . . . . . . . . . . . . . . . . . . . . . . . 12

4.2 Extended cut-and-count analysis . . . . . . . . . . . . . . . . . . . . . . . . 15

4.2.1 Generalized cut-and-count analysis: ny = Ns

. . . . . . . . . . . . . 16

4.2.2 Extended cut-and-count analysis: ny > Ns

. . . . . . . . . . . . . . 17

5 Application: Dalitz plots and BR measurement

6 Comparison with projection plots

7 How to implement s

lot technically

P

8 Conclusion

2

One of the crucial points of an analysis is to be able to use the most accurate description
of the distributions of the diﬀerent variables which enter in a measurement. The goal of
the formalism developed in this document is to provide a convenient method to unfold the
overall distribution of a sample of events in a given variable x into the sub-distributions
of the various species which compose the sample. If the sample is composed of a single
signal species and a single background species the method advocated here amounts to a
sophisticated background subtraction. The method is simple: the length of the document
should not be misleading, it results from an attempt to explain in details how and why
the method works.

1 Introduction

One considers an analysis, based on N events, which is dealing with Ns species of events:
those are various signals and backgrounds components which all together account for the
N observed events. The analysis is assumed to rely on the extended log-likelihood:

=

L

N

Ns

ln

Nifi(ye)

e=1
X

n

i=1
X

Ns

Ni

−

o

i=1
X

(1)

where fi is the pdf of the ith species with respect to a set of discriminating variables
collectively denoted y: fi(ye) denotes the value taken by the pdf fi for event e, which is
associated with a value ye for the set of discriminating variables.

Essential remark: One assumes that the pdfs entering in the likelihood deﬁnition are
exact, and that the ﬁt has already been performed. Hence, the Ni are determined, pos-
sibly with other types of parameters. The interest in this document is not about the ﬁt
itself, nor about possible systematics due to mismatch in the fi pdfs.

One is interested in the true distributions Mn of a variable x for events of the nth
species, anyone of the Ns signal and background species. The purpose of the document
is to show that one can reconstruct the Mn(x) distributions for all species separately
from the sole knowledge of the fi pdfs and of the ﬁt outputs for the Ni. To distinguish
clearly between the reconstructed distributions and the true distributions, the latter will
be denoted in boldface.

The simplest case is considered in section 2, where the variable x actually belongs
to the set of y discriminating variables. That is to say that x is assumed to be totally
correlated with y; hence, there exists a function of the y parameters which fully determines
the variable; x = x(y). In that case, while performing the ﬁt, an a priori knowledge of
the x-distributions was used, at least implicitly.

Section 3 then turns to the core of the document where the s

lots formalism is devel-
opped to treat the much more interesting case where x is a new variable, assumed to be
totally uncorrelated with y. In that case, while performing the ﬁt, no a priori knowledge
of the x-distributions was used.

P

To provide some intuitive understanding of how and why the s

lots formalism works,
in section 4 the problem of reconstructing the Mn(x) distributions in the framework of

P

3

cut-and-count analyses is reconsidered.

surements follows in section 5.

An example of application concerning Dalitz plots and branching ratios (BR) mea-

lot method is then compared, in section 6 with the projection plots method.

The s
Finally, section 7 gives indications about how to implement s

lot in a code. The path

P

to follow is indeed easy if some simple rules are respected.

P

2 To begin easily: inP

lots, x

y

∈

In this section one assumes that one is interested in a variable x which can be expressed as
a function of the variable y. A ﬁt having been performed to determine the contributions
Ni of all species, from the knowledge of the fi pdfs and the values of the Ni, one can
deﬁne, for all events, the weight

and introduce the x-distribution ˜Mn (normalized to unity) deﬁned by:

P

Pn(ye) =

Nnfn(ye)
Ns
k=1 Nkfk(ye)

Nn ˜Mn(¯x)δx

N

≡

Xe⊂δx Pn(ye)

N
e⊂δx runs over events for which xe (the value taken by the variable x for
where the sum
event number e) lies in the x-bin centered on ¯x and of total width δx. That is to say that
Nn ˜Mn(¯x)δx is the x-distribution obtained by histogramming events, using the weight of
Eq.(2). One wants to show that this simple procedure reproduces the true distribution
M(x).

P

On average, replacing the sum in Eq.(3) by the integral

N

Xe⊂δx −→ Z

Ns

Xj=1

dy

Njfj(y)δ(x(y)

¯x)δx

−

one obtains:

Nn ˜Mn(¯x) =

dy

Njfj(y)δ(x(y)

¯x)

Pn

−

Ns

j=1
X
Ns

Z

Z

j=1
X
dyδ(x(y)

= Nn

Z
NnMn(¯x)

≡

=

dy

Njfj(y)δ(x(y)

¯x)

Nnfn(y)
Ns
k=1 Nkfk(y)

−

¯x)fn(y)

−

P

Pn provides a direct estimate of the x-
Therefore, the sum over all events of the weight
distribution of events of the nth species, normalized to the number of events as determined

4

(2)

(3)

(4)

(5)

(6)

(7)

(8)

lots.

The inP

by the ﬁt. In this document, plots obtained that way (i.e., when the variable considered
is in the set of discriminating variables y) are referenced to as inP
lots suﬀer from a major drawback: x being correlated to y, it enters in the
deﬁnition of the weight and as a result the ˜Mn distributions are biased in a way diﬃcult
to grasp, when the f(y) are not accurate. For example, let us consider a situation where
some events from the nth species show up far in the tail of the Mn distribution which
is implicitely (or explicitely) used in the ﬁt. The existence of such events means that
the true distribution Mn(x) exhibits a tail which is not accounted for by the distribution
Mn(x) resulting from the shapes assumed for the f(y) distribution. These events would
enter in ˜Mn with a very small weight, and they would thus escape detection: ˜Mn would
still be close to Mn. Only a mismatch in the core of the x-distribution can be revealed
lots. Stated diﬀerently, the error bars which can be attached to each indivual
with inP
bins of ˜Mn do not account for the systematical bias inherent to the inP

lots.

3 The right tool: sP

lots, x

y

6∈

It has been shown in the previous section that if the variable x belongs to the set y of
lots are not easy to decipher because some knowledge of
discriminating variables, the inP
x enters in their construction. The more interesting case where the variable x does not
belong to y is now considered. One may still consider the above distribution ˜Mn, but this
time the trick does not work anymore, at ﬁrst sight. This is because the x-pdfs Mj(x) are
implicit in the sum over the events, while they are absent in the likelihood function. It is
shown below that a proper redeﬁnition of the weights allows to overcome this diﬃculty.
But there is a (mild) caveat: one should assume from the start that the total pdf f(x, y)
factorizes in the product M(x)f(y).

As a result:

It is assumed that x and y are not correlated.

Nn ˜Mn(¯x) =

dydx

NjMj(x)fj(y)δ(x

Ns

j=1
X

Z Z

Ns

Z

j=1
X

Ns

=

dy

NjMj(¯x)fj(y)

= Nn

Mj(¯x)

Nj

 

Z

j=1
X

= NnMn(¯x)

P

dy

P

¯x)

Pn

−

Nnfn(y)
Ns
k=1 Nkfk(y)
fn(y)fj(y)
Ns
k=1 Nkfk(y) !

Indeed, as announced, the previous trick does not work. In eﬀect, the correction term

P
is not identical to the kroenecker symbol δjn. The distribution obtained on the left hand
side Nn ˜Mn is a linear combination of the true distributions Mj. Only if the y variables
were totally discriminating would one recover the correct answer. In eﬀect, in that case,

Nj

dy

Z

fn(y)fj(y)
Ns
k=1 Nkfk(y)

5

(9)

(10)

(11)

(12)

(13)

6
(14)

(15)

(16)

(17)

(18)

(19)

(20)

the product fn(y)fj(y) being equal to f 2
if fn(y) is non zero), one gets:

n(y)δjn (for a total discrimination, fj6=n(y) vanishes

Njδjn

dy

= δjn

f 2
n(y)
Nnfn(y)

Z

But this is purely academic, because, if y were totally discriminating the obtention of
Mn(x) would be straightforward: one would just apply cuts on y to obtain a pure sample
of events of the nth species and plot them to get Mn(x) !

However, in the case of interest where y is not totally discriminating, there is a way
out. One observes that the correction term is related to the inverse of the covariance
matrix, which is given by the second derivatives of

, which the analysis minimizes:

−L

V−1

nj =

N

∂2(
)
−L
∂Nn∂Nj

=

fn(ye)fj(ye)
Ns
k=1 Nkfk(ye))2

(

e=1
X
On average, replacing the sum over events by an integral (Eq.(4)) the variance matrix
reads:

P

V−1

nj =

dydx

NlMl(x)fl(y)

Ns

Xl=1

Z Z

=

=

Ns

Xl=1

dy

dy

Nlfl(y)

(
fn(y)fj(y)
P
Ns
k=1 Nkfk(y)

Z

Z

fn(y)fj(y)
Ns
k=1 Nkfk(y))2

(

P

fn(y)fj(y)
Ns
k=1 Nkfk(y))2

Z

dxMl(x)

Therefore, Eq.(11) can be rewritten:

P

Inverting this matrix equation one recovers the distribution of interest:

˜Mn(¯x) =

Mj(¯x)NjV−1
nj

Ns

j=1
X

NnMn(¯x) =

Vnj ˜Mj(¯x)

Ns

j=1
X

lots. This result is better restated as follows.

Hence, the true distribution can still be reconstructed by a linear combination of the
inP
When x does not belong to the set y, the appropriate weight is not given by Eq.(2), but

is the covariance-weighted weight (thereafter called sWeight) deﬁned by:

sP n(ye) =

Ns
j=1 Vnjfj(ye)
Ns
k=1 Nkfk(ye)

P

(21)

P
This equation is the most important of the document.

6

With this sWeight, the marginal distribution of x can be obtained by building the s
with the following deﬁnition:

P

lot

which reproduces, on average, the true marginal distribution:

Nn s ˜Mn(¯x)δx

N

≡

Xe⊂δx

s

P n(ye)

Nn s ˜Mn(x) = NnMn(x)

(22)

(23)

Remark: The fact that the matrix Vij enters in the deﬁnition of the sWeights is enlight-
ening. Furthermore, as discussed in the next section, this confers nice properties to the
lots. But this is not the key point. The key point is that Eq.(11) is a matrix equation
s
P
which can be inverted using a numerical evaluation of the matrix based only on data,
thanks to Eq.(15). Another option to obtain the covariance matrix is to rely on Minuit,
although this option is less accurate than the direct computation. When parameters other
than the number of events per species Nj are ﬁtted together with them, in order to get
the correct matrix, one should take care to perform a second ﬁt, where parameters other
than Nj are frozen.

3.1 Properties

Beside satisfying the essential Eq.(23), s

lots bear the following properties.

P

3.1.1 Normalization
The distribution s ˜Mn deﬁned by Eq.(22) is by construction equal to Mn: in particular, it
is guaranteed to be normalized to unity. However, from expression Eq.(21), neither is it
obvious that the sum over the x-bins of Nn s ˜Mnδx is equal to Nn, nor is it obvious that in
each bin, the sum over all species of the expected numbers of events equates the number
of events actually observed. The demonstration uses the three sum rules below.

•

Maximum Likelihood Sum Rule
The likelihood Eq.(1) being extremal for Nj, one gets the ﬁrst sum rule:

N

e=1
X

fj(ye)
Ns
k=1 Nkfk(ye)

= 1

,

j
∀

(24)

•

Variance Matrix Sum Rule
From Eq.(15) and Eq.(24) one derives:

P

Ns

i=1
X

Ns

N

NiV−1

ij =

Ni

i=1
X

(

e=1
X

fi(ye)fj(ye)
Ns
k=1 Nkfk(ye))2

=

N

e=1
X

fj(ye)
Ns
k=1 Nkfk(ye)

= 1

(25)

•

Covariance Matrix Sum Rule
Multiplying both sides of Eq.(25) by Vjl and summing over j one gets the sum rule:

P

P

Ns

Xj=1

Ns

Ns

Vjl =

Vjl

NiV−1

ij =

Xj=1

Xi=1

Ns

Ns

Xi=1

Xj=1





V−1

ij Vjl


Ns

Xi=1

Ni =

δilNi = Nl

(26)

7

It follows that:

1. Each x-distribution is properly normalized:

N

e=1
X

s

P n(ye) =

X[δx]

Nn s ˜Mn(x)δx =

Ns
j=1 Vnjfj(ye)
Ns
k=1 Nkfk(ye)

N

e=1 P
X

P

Ns

j=1
X

=

Vnj = Nn

(27)

2. The contributions s

P j(ye) add up to the number of events actually observed in each

x-bin. In eﬀect, for any event:

Ns

Xl=1

s

P l(ye) =

Ns

Xl=1 P
P

Ns
j=1 Vljfj(ye)
Ns
k=1 Nkfk(ye)

=

Ns
j=1 Njfj(ye)
Ns
k=1 Nkfk(ye)

= 1

(28)

P

P

P

lot provides a consistent representation of how all events from the various
Therefore, the s
species are distributed in x, according to a ﬁt based on the variables y (x /
y). For
∈
instance, an excess of events observed for a particular nth species in a given x-bin is
eﬀectively accounted for in the number of event Nn resulting from the ﬁt: to remove these
events (for whatever reason and by whatever means) implies a corresponding decrease in
Nn. It remains to gauge how signiﬁcant is an anomaly in the x-distribution of the nth
species. This is the subject of the next section.

3.1.2 Statistical uncertainties
The statistical uncertainty on Nn s ˜Mn(x) is given in each bin by

σ[Nn s ˜Mn(x)] =

N

v
u
u
t

Xe⊂δx

(s

P n)2

The proof of Eq.(29) goes as follows:

σ2[Nn s ˜Mn] =

2

s

P ni

N

s

Xe⊂δx
s

ih

P

ih

P

s

s

ih

P

s

ih

P

h

N

h

h

h

h

N

N

N

+

P n

2
+
ni
2
ni
2
ni
2
ni

=

+

2

N

Xe⊂δx

−

i − h

h

N (N
N 2

h

(cid:16)

h
N

(cid:16)

N

Xe⊂δx

i

i − h
N
+

h
P n)2

(s

=

=

=

=

2

s

ih

1)

N

P ni
s
h

P ni
N

i
2

i

(cid:17)
− h

N

− h
2

− h
s

h

i

(cid:17)

2

i
h
N

i
2

2

s

P ni
2
s
h

2

2

P ni
N

P ni

− h

s

h

P ni

i

2

The sum in quadrature of the above uncertainties reproduces the statistical uncertainty
on Nn (σ[Nn]

√Vnn) as provided by the ﬁt. The sum over the x-bins reads:

≡

Xδx

σ2[Nn s ˜Mn] =

N

N

(s

P n)2 =

Xδx

Xe⊂δx

e=1   P
X

P

8

2

Ns
j=1 Vnjfj(ye)
Ns
k=1 Nkfk(ye) !

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

Ns

Ns

N

VnlVnj

(

e=1
X
VnlVnjV−1

P
lj =

fl(ye)fj(ye)
Ns
k=1 Nkfk(ye))2
Ns

Vnlδnl

Xl=1

=

=

Xj=1
Ns

Xl=1
Ns

j=1
X
= Vnn

Xl=1

Therefore, for the expected number of events per x-bin indicated by the s
lots, the
statistical uncertainties are straightforward to compute, and also provide a consistent
representation of how the overall uncertainty on Nn is distributed in x among the events
of the nth species.

P

More generally, the very same reasoning shows that the whole covariance matrix is

reproduced:

N

e=1
X

(s

P i)(s

P j) = Vij

As a result, two species i and j can be merged into a single species (i + j) without having
to repeat the ﬁt and recompute the sWeights. One can just add the sWeights on an
event-by-event basis to obtain the combined x-distribution:

with the proper normalization and the proper error bars (Eqs. (27) and (39)):

N(i+j) ˜M(i+j)δx =

N

Xe⊂δx

(s

P i + s

P j)

N(i+j) =

(s

P i + s

P j) = Ni + Nj

N

e=1
X
N

σ2[N(i+j)] =

(s

P i + s

P j)2
= Vii + Vjj + 2Vij = V(i+j)(i+j)

e=1
X

3.2 Extended s

lots: a species is known (ﬁxed)

P

It may happen that the contributions of some species are not derived from the data
sample, but are taken to be known from other sources. One denotes as species ’0’ the
overall component of such species which number of expected events N0, being assumed to
be known, is held ﬁxed in the ﬁt. In this section, the indices i, j... run over the Ns ﬁtted
species, the ﬁxed species ’0’ being excepted (i, j...

= 0).

One can met various instances of such a situation. Two extreme cases are:

1. the species ’0’ is fairly well known and there is no point to dig out whatever informa-
tion on it the data sample under consideration contains. Not only is N0 already pined
down by other means, but M0(x), the marginal distribution of the ﬁxed species, is
available.

2. the species ’0’ is not well known, and, worse, the sample under consideration is of
poor help to resolve its contribution. This is the case if the y variables are unable

9

6
to discriminate between species ’0’ against any one of the other Ns species. Stated
diﬀerently, if N0 is let free to vary in the ﬁt, the covariance matrix blows up for
certain species and the measurement is lost. To avoid that, one is lead to accept an
a priori value for N0, and to compute systematics associated to the choice made for
it. In that case M0(x) might be unknown as well.

lots formalism can be extended to deal with this situation,
It is shown below that the s
whether or not M0(x) is known, although in the latter case the statistical price to pay is
likely to be prohibitive.

P

3.2.1 Assuming M0 to be known

Here, it is assumed that M0(x), is taken for granted. Then, it is not diﬃcult to show that
lot which reproduces the marginal distribution of species n is now given
the Extended s
by:

P

Nn s ˜Mn(¯x)δx = cnM0(x)δx +

N

s

P n

Xe⊂δx

where:

s

•

P n is the previously deﬁned sWeight of Eq.(21):
j Vnjfj
k Nkfk + N0f0
P

P n =

s

where the covariance matrix Vij is the one resulting from the ﬁt of the Ni6=0 expected
number of events, that is to say the inverse of the matrix:

P

cn is the species dependent coeﬃcient:

•

V−1

ij =

N

e=1
X

fifj

(

k Nkfk + N0f0)2

P

cn = Nn −

Vnj

Xj

Some remarks deserve to be made:

•

Because N0 is held ﬁxed, in general, its assumed value combined with the ﬁtted
values for the Ni, does not maximize the likelihood of Eq.(1):

∂
L
∂N0

=

N

e=1
X

f0

k Nkfk + N0f0 −

1

= 0

•

It follows that the sum over the number of events per species does not equal the
total number of events in the sample:

Ni = N

N0

−

X

f0

k Nkfk + N0f0 ! 6

= N

N0

−

P

N

 

e=1
X

P
10

(43)

(44)

(45)

(46)

(47)

(48)

6
Similarly, the Variance Matrix Sum Rule Eq.(25) holds only for N0 = 0:

i
X
where the vector vj is deﬁned by1:

NiV−1

ij = 1

N0vj

−

Accordingly, Eq.(26) becomes:

Thus, as they should, the cn coeﬃcients vanish only for N0 = 0:

f0fj

(

k Nkfk + N0f0)2

vj

N

≡

e=1
X

P

Vjl = Nl + N0

Vljvj

j
X

j
X

cn =

N0

Vnjvj

−

j
X

•

•

•

•

The above deﬁned Extended s

lots share the same properties as the s

lots:

P

P

1. They reproduce the true marginal distributions, as in Eq.(23).
2. In particular, they are properly normalized, as in Eq.(27).
3. The sum of s

2
n reproduces σ2[Nn], as in Eq.(38).

P

3.2.2 Assuming M0 to be unknown

In the above treatment, because one assumes that a special species ’0’ enters in the sample
composition, the sWeights per event do not add up to unity, as in Eq.(28). Instead one
may deﬁne the sWeights for species ’0’ as:

and introduce the reconstructed s ˜M0 distribution (normalized to unity):

s

P 0 ≡

1

−

s

P i

Xi

s ˜M0(x)δx =

−1

N

N





−

i,j
X

Vij


s

P 0

Xe⊂δx

which reproduces the true distribution M0(x) if (by chance) the value assumed for N0 is
the one which maximizes the Likelihood.

Taking advantage of s ˜M0(x), one may redeﬁne the Extended s

lots by:

Nn s ˜Mn(¯x)δx = cn s ˜M0(x)δx +

P

N

N

s

P n =

es

P n

Xe⊂δx

Xe⊂δx

1This quantity is not denoted V−1

0j to avoid confusion when handling the matrix Vij which is the

inverse of V−1

ij , but ignoring such 0j components.

11

(49)

(50)

(51)

(52)

(53)

(54)

(55)

where the redeﬁned sWeight which appears on the right hand side is given by:

es

P n ≡

s

P n +

Ni
N

−
−

j Vij
i,j Vij

P

s

P 0

P

It does not rely on a priori knowledge on the true distribution M0(x). With this redeﬁ-
nition, the following properties hold:

i Ni) ˜M0
The set of reconstructed x-distributions Ni ˜Mi of Eq.(55) completed by (N
of Eq.(54) are such that they add up in each x-bin to the number of events observed.
The normalization constant of the ˜M0 distribution vanishes quadratically with N0.
It can be rewritten in the form:

−

P

•

•

(56)

(57)

N

−

Xi,j

Vij = N 2

0 

v0 −



Vijvivj 


Xi,j

where v0 is deﬁned as vj, and where the last term is regular when N0 →
lots remains correct, the
Whereas the normalization of the redeﬁned extended s
sum of the redeﬁned sWeights Eq.(56) squared is no longer equal to σ2[Nn] = Vnn.
Instead:

0.

P

•

(es

P n)2 = Vnn +

X

j Vnj)2
i,j Vij

(Nn −
N
−

P

P

= Vnn +

(58)

ij VniVnjvivj
ij Vijvivj

P

v0 −

P

Since the expression on the right hand side is regular when N0 →
0, it follows that
there is a price to pay to drop the knowledge of M0(x), even though one expects a
vanishing N0. Technically, this feature stems from

(s

P 0)2 =

X

X

s

P 0 = N

−

Vij

i,j
X

(59)

Hence, the sum in quadrature of the s ˜M0 uncertainties per bin diverges with N0 →
0.
This just express the obvious fact that no information can be extracted on species
’0’ from a sample which contains no such events.

4 Pedagogical examples

lots
The purpose of this section is to show, after all the previous mathematics, how the s
are working in simple cases of analysis and how speciﬁc events can be extracted from a
data sample using the pdfs. One begins with the simpliest existing case and move to more
diﬃcult ones.

P

4.1 Simple cut-and-count analysis

In this section, a very simple situation is considered where the proper way to obtain signal
and background distributions of a variable x is obvious. The purpose is to observe the
lots at work, when one knows what the outcome should be. One assumes that the data

s

P

12

sample consists of Ns = 2 species: species 1 is refered to as the signal and species 2 as the
background. A unique discriminating variable y
[0, 1] is used in the ﬁt. One further
assumes that:

∈

the signal distribution is the step-function:

•

•

the background distribution is uniform in the full range:

f1(y < y0) = 0
y0) = (1
f1(y

≥

y0)−1

−

f2(y) = 1

In that case, one is dealing with a cut-and-count analysis: there is a pure background
side-band (y < y0) and the shape of the signal and background distributions oﬀer no
discriminating power in the signal region (y
y0). Denoting N the total number of
events, N< the number of events located below y0, and N> the number of events located
above y0:

≥

1. the number of background and signal events can be deduced without any ﬁt:

N2 =

N<

1
y0

N1 = N>

1

y0

−
y0

−

N<

2. in passing, although not needed, N< and N> being two independent numbers of

events, the covariance matrix can be deduced directly from Eqs.(63)-(64):

2

V =

N> +

1−y0
N<
y2
0
1
N< 
y2
0

< the number of events in the x-bin, with y

1−y0
y0
1−y0
(cid:16)
y2
0

(cid:17)
N<

N<

−

−





3. Denoting δN x

distribution M2(x) is:

N2 M2(x)δx =

δN x
<
y0

y0, the background

≤

which is, here, a clumsy way to say that the x-distribution of background events
can be inferred directly from the (pure) background events populating the domain
y

y0. The signal distribution is:

≤

N1 M1(x)δx =

(1

y0)N2 M2(x) + δN x
>

−
that is to say, one can obtain the signal distribution from the (mixed) events pop-
y0, provided one subtracts the contribution of background
ulating the domain y
events, which is known from Eq.(66).

−

≥

(67)

Whereas in such a situation the use of the s
should reproduce the above obvious results, and indeed it does:

P

lots formalism would be awkward, the latter

13

(60)
(61)

(62)

(63)

(64)

(65)

(66)

1. Denoting fi(0) (resp. fi(1)) the value taken by the pdf of species i for y

y0 (resp.

≤

y > y0), Eq.(24) reads:

f1(ye)
Ns
k=1 Nkfk(ye)

= N<

f1(0)
N1f1(0) + N2f2(0)

+ N>

f1(1)
N1f1(1) + N2f2(1)

1 =

1 =

=

=

N

e=1
X

N>(1
P
N1(1
N

−

e=1
X
N<
N2

P
+

y0)−1
−
y0)−1 + N2
f2(ye)
Ns
k=1 Nkfk(ye)
N>
y0)−1 + N2

= N<

N1(1

−

The ﬁrst equation yields:

f2(0)
N1f1(0) + N2f2(0)

+ N>

f2(1)
N1f1(1) + N2f2(1)

and thus, for the second equation:

N1(1

y0)−1 + N2 = N>(1

y0)−1

−

−

1 =

N<
N2

+ 1

y0

−

which leads to Eqs. (63)-(64).

2. Similarly, Eq.(15) yields

V−1 =





1
N>
1−y0
N>

1−y0
N>

(1−y0)2
N>

+

y2
0
N<





For example, using Eq.(72):

V−1

11 =

N

(

e=1
X

f1(ye)f1(ye)
Ns
k=1 Nkfk(ye))2

= N>

(N1(1

(1

−

y0)−2
y0)−1 + N2)2 =
−

1
N>

Inverting V−1 one gets Eq.(65).

P

3. Eq.(22) then reproduces Eqs. (66)-(67). Namely:

N1 M1(x)δx =

N

V11f1(ye) + V12f2(ye)
Ns
k=1 Nkfk(ye)

+ δN x
>

V11f1(1) + V12f2(1)
N1f1(1) + N2f2(1)

y0)−1 + V12
y0)−1 + N2

y0 )2N<)(1
N>(1

y0)−1

−
y0)−1

−

−

1−y0
y2
0

N<

Xe⊂δx
= δN x
<

= δN x
<

= δN x
<

+δN x
>

P

V11f1(0) + V12f2(0)
N1f1(0) + N2f2(0)
V12
N2

V11(1
N1(1

+ δN x
>

−
−

1−y0
N<
y2
−
0
N<y−1
0
(N> + ( 1−y0

=

1

y0

−
y0

−

δN x

< + δN x
>

14

(68)

(69)

(70)

(71)

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

and:

N2 M2(x)δx =

N

V21f1(ye) + V22f2(ye)
Ns
k=1 Nkfk(ye)

+ δN x
>

V21f1(1) + V22f2(1)
N1f1(1) + N2f2(1)

y0)−1 + V22
y0)−1 + N2

Xe⊂δx
= δN x
<

= δN x
<

= δN x
<

P

+ δN x
>

V21(1
N1(1

V21f1(0) + V22f2(0)
N1f1(0) + N2f2(0)
V22
N2
1
N<
y2
0
N<y−1
0
1−y0
y2
0

N<(1

−
−

+δN x
>

−

=

δN x
<
y0

N<

y0)−1 + 1
y2
0
y0)−1

N>(1

−

−

4. it can be shown as well that Eqs. (25)-(26)-(27)-(28)-(39) hold.

Therefore, in this very simple situation where the problem of reconstructing the distribu-
lots formalism reproduces
tions of signal and background events is glaringly obvious, the s
the expected results.

P

4.2 Extended cut-and-count analysis

The above example of the previous section 4.1 is a very particular case of a more general
situation where the y-range is split into ny slices inside which one disregards the shape of
the distributions of the species, whether these distributions are the same or not. Using
greek letters to index the y-slices, this amounts to replace the fi(y) pdfs by step functions
which constant values inside each y-bin Fα
i are deﬁned by the integral over the y-bin α:

fi(y)

Fα

i =

fi(y)dy

Zα

Fα

i = 1

→
ny

α=1
X

¯Nα =

NiFα
i

Ns

i=1
X

With this notation, the number of events ¯Nα expected in the slice α is given by:

lots formalism, in the previous
To make ﬂagrant what must be the outcome of the s
section it was assumed that ny = Ns = 2 and that the signal was utterly absent in one of
the two y-slices: F1

2 = y0 and F2

1 = 0, F2

1 = 1, F1

2 = 1

y0.

P

Below one proceeds in two steps, ﬁrst considering the more general case where only
ny = Ns is assumed (section 4.2.1), then considering the extended cut-and-count analysis
where ny > Ns (section 4.2.2). Since the general case discussed in the presentation of the
, what follows amounts to a step-by-
s
P
step re-derivation of the technique.

lots formalism corresponds to the limit ny

→ ∞

−

15

(82)

(83)

(84)

(85)

(86)

(87)

(88)

(89)

(90)

4.2.1 Generalized cut-and-count analysis: ny = Ns

When the number of y-slices equals the number of species, the solution remains obvious,
is invertible (if not, the Ni cannot be determined). In that case,
if the Ns ×
one can identify the expected numbers of events ¯Nα with the observed number of events
Nα, and thus:

Ns matrix Fα
i

1. One recovers the expected number of events Ni from the observed number Nα per

slice:

2. In passing, although not needed, one obtains directly the covariance matrix:

Ni =

Nα(F−1)α
i

Ns

α=1
X

Vij =

Nα(F−1)α

i (F−1)α
j

Ns

α=1
X

Ns

i=1
X

3. Similarly to Eq.(90), the number of events δN x

α observed in the slice α and in the

bin x of width δx is given by:

δN x

α =

NiMi(x)δxFα
i

and thus, the x-distribution of species i is:

δN x

i ≡

NiMi(x)δx =

δN x

α(F−1)α
i

Ns

α=1
X

It remains to be seen that Eq.(94) is reproduced using the s
Eq.(88) and Eq.(90), one observes that:

P

lots formalism. First, using

N

e=1
X

P

fi(ye)

Ns
k=1 Nkfk(ye) →

Nα

Ns

α=1
X

Fα
i
Ns
k=1 NkFα
k

Ns

α=1
X

Fα
i
Nα

Ns

α=1
X

=

Nα

=

Fα

i = 1

(95)

which shows that the obvious solution Eq.(91) is the one which maximizes the extended
log-likelihood. Similarly:

P

V−1

ij =

N

(

e=1
X

P

fi(ye)fj(ye)
Ns
k=1 Nkfk(ye))2 →

Nα

Fα
i Fα
j
N 2
α

=

Ns

α=1
X

Ns

α=1
X

1
Nα

i Fα
Fα
j

which inverse is given by Eq.(92), and thus:

Ni s ˜Mi(x)δx

Ns

δN x
α

j VijFα
j
Nα

P

=

→

α=1
X

Ns

α=1
X

δN x

α(F−1)α
i

The s

lot formalism reproduces Eq.(94).

P

16

(91)

(92)

(93)

(94)

(96)

(97)

4.2.2 Extended cut-and-count analysis: ny > Ns

In the more general situation where the number of y-slices is larger than the number of
species, there is no intuitive solution neither for determining the Ni, nor for reconstructing
the x-distribution of each species (in particular, Eq.(91) is lost). Because of this lack of
lots, but taking
obvious solution, what follows is a rephrasing of the derivation of the s
a slightly diﬀerent point of view, and in the case where the y-distributions are binned.

P

The best determination of the Ni (here as well as in the previous simpler situations)

is provided by the likelihood method which yields (cf. Eq.(24)):

P
with a variance matrix (cf. Eq.(15)):

ny

α=1
X

NαFα
i
Ns
k=1 NkFα
k

= 1

,

i
∀

V−1

ij =

ny

α=1
X

Nα

(

i Fα
Fα
j
Ns
k=1 NkFα

k )2

P

Ni =

Nα (s

)α
i

P

ny

α=1
X

)α
i =

(s

P

Ns
j=1 VijFα
j
Ns
k=1 NkFα
k

P

from which one computes the covariance matrix Vij. Instead of Eq.(91) the number of
events Ni provided by Eq.(98) satisﬁes the equality (cf. Eq.(27)):

where the matrix s

(Eq.(21)) is deﬁned by:

P

P
The identity of Eq.(100) is exact, even for ﬁnite statistics, since the contractions with
V−1

li of both the left- and right-hand sides yield the same result. Indeed (Eq.(25)):

NiV−1

li =

Ns

Xi=1

Ns

ny

Xi=1

α=1
X

l NiFα
NαFα
i
Ns
k=1 NkFα
k )2

(

=

ny

α=1
X

NαFα
l (
Ns
k=1 NkFα
(

Ns
i=1 NiFα
i )
k )2

P

=

P

P

NαFα
l
Ns
k=1 NkFα
k

= 1

ny

α=1
X

P

(102)

which is identical to:

V−1
li

Ns

i=1
X

ny

α=1
X

Nα

Ns
j=1 VijFα
j
Ns
k=1 NkFα
k

P

=

Nα

ny

α=1
X

Ns
j=1(

li Vij)Fα
j

i=1 V−1
Ns
Ns
k=1 NkFα
k

P

P

=

ny

α=1
X

NαFα
l
Ns
k=1 NkFα
k

= 1 (103)

P

P
Since Eq.(100) holds for the complete sample of events, it must hold as well for any
sub-sample, provided the splitting into sub-samples is not correlated with the variable y.
Namely, for all x-bin, on average, one must observe the same relationship between the
numbers of events δN x

i and δN x
α:

P

δN x

i =

δN x

α (s

)α
i

P

ny

α=1
X

17

(98)

(99)

(100)

(101)

(104)

The proof follows the same line which leads to Eq.(19). On average, using successively:

δN x

α =

NlMl(x)Fα

l δx

Ns

Xl=1

Nα =

δN x

α =

Ml(x)

NkFα

k =

NkFα
k

Ns

Xk=1

Ns

Xk=1

x
X

x
X

and hence:

one gets:

ny

α=1
X

δN x

α (s

)α
i =

P

ny

Ns

α=1  
X
Ns

Xl=1

NlMl(x)Fα

l δx

= δx

NlMl(x)

Vij

Ns
j=1 VijFα
j
Ns
k=1 NkFα
k
j Fα
Fα
l
Ns
k=1 NkFα
k

j Fα
Fα
l
Ns
k=1 NkFα

P
Nα



(

! P
ny
P

α=1
X
ny

α=1
X

P

k )2 


Ns









j=1
X
Ns

j=1
X
Ns

Xl=1
Ns

Xl=1
Ns

= δx

NlMl(x)

Vij

= δx

NlMl(x)

Xl=1
= NiMi(x)δx



j=1
X
δN x

i

≡

VijV−1

jl 



which concludes the discussion of the situation where the y-distributions are step func-
tions.

5 Application: Dalitz plots and BR measurement

Beside providing a tool to cross-check the analysis by allowing x-distributions to be recon-
lots formalism can be applied also
structed and then compared with expectations, the s
to reach physics results, otherwise hard to attain. Indeed, in some instances, a variable x,
known to be discriminating against background, cannot be used in the analysis because
the construction of the signal pdf implies physical assumptions one wishes not to make.
In addition, if the selection eﬃciency for the signal depends on x, the eﬃciency corrected
yield cannot be computed.

P

To be speciﬁc, a three body decay is here considered, where the signal pdf inside the
Dalitz plot is not known because of unknown contributions of resonances, continuum,
lots of the signal component exhibits the
interference pattern. The (two-dimensional) s
Dalitz distribution without using a priori inputs for the resonance structure of the signal.
Furthermore, since the x-dependence of the selection eﬃciency ǫ(x) can be computed
without a priori knowledge of the x-distributions, one can build the eﬃciency corrected

P

lots:

s

P

1
ǫ(x)

Nn ˜Mnδx =

N

1
ǫ(x)

Xe⊂δx Pn(ye)

18

(105)

(106)

(107)

(108)

and compute the eﬃciency corrected yields to obtain the BR:

N ǫ

n =

N

e=1
X

Pn(ye)
ǫ(xe)

(109)

Analyses can then use the s
P
to measure branching ratios.

lot formalism for validation purpose, but also, using Eq.(109),

6 Comparison with projection plots

To obtain projection plots a cut is applied on the likelihood ratio in order to get sample
lots and the projections plots
enriched in signal events. Two diﬀerences between the s
lots (no cut is
deserve to be stressed: the whole signal is represented in the above s
applied) and the background pollution is taken away. Because of these two features, one
is limited only by the statistics of the full sample and one cannot misinterpret a signal
anomaly as a background ﬂuctuation.
The inconvenients of the projection plots are:

P

P

some signal events are lost,

some background events remain.

On the contrary, the s

lot method allows:

to keep all signal events,

P

to substract all background events.

•

•

•

•

while easily keeping track of the statistical uncertainties per bin.

7 How to implement sP

lot technically

The ﬁrst sections on the document tend to be a little bit tricky with lots of mathematical
lot is indeed not diﬃcult.
demonstrations. This section is then here to show that using s
P
lot tool in a code is given. The diﬀerent steps are the
A way to implement easily the s
following:

P

1. The ﬁt has to be done to obtain the Ni of each i species present in the data sample.
The variable one wants to get the distribution of must not be included in the ﬁt.

2. The sWeights s

given by Minuit (this is the easiest way to get Vij, but one can compute V−1
from Eq. 15 and inverse it).

are calculated following Eq. 21 and using the covariance matrix
ij directly

P

3. The histograms have then to be ﬁlled with the values of the variable x weighted with

the sWeights s

for each species present in the data sample.

An example of the use of s

lot can be seen in fortran code (if requested).

P

P

19

8 Conclusion

One examines a data sample using a set of y discriminating variables by building s
lots
of any variable (absent from the set y) for any species present in the sample. Although no
lot of a given species represents the whole statistics of this species)
cut is applied (the s
the distributions obtained are pure (background free) in a statistical sense. The more
lot is. The technique is straightforward to
discriminating the variables y, the clearer the s
implement and features several nice properties: both the normalizations and the statistical
uncertainties of the s

lots share the very same properties as the true distributions.

P

P

P

lot can be found in [1, 2].

An application of the use of s

It is shown how in the
BABAR B0
π+π− analysis an excess of events in the low ∆E domain was discovered
→
lots. Further studies indicated that this excess of events is due mostly to
thanks to the s
radiative events ignored in this analysis. The branching ratios are then underestimated
by about 10%. This, together with its application in Dalitz plots analyses, constitutes an
example of what the s

lot tool is able to bring to an analysis.

P

P

P

P

Acknowledgements
We would like to warmly thank Denis Bernard for his encouragements and the careful
reading of this document and Nick Danielson for his support and for having implemented

lots in RooFit.

s

P

References

[1] M. Pivk thesis (May 2003), BABAR THESIS-03/012, in french.

Can be found as well in
http://tel.ccsd.cnrs.fr/documents/archives0/00/00/29/91/index fr.html

[2] M. Pivk, proceedings of Moriond QCD session (March 2003).

20


5
0
0
2
 
n
u
J
 
5
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
3
0
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Sampling errors of correlograms with and without sample
mean removal for higher-order complex white noise with
arbitrary mean

Space Science Centre, Sussex University, Brighton BN1 9QT, United Kingdom

5 June 2005

Abstract

We derive the bias, variance, covariance, and mean square error of the standard lag windowed correl-
ogram estimator both with and without sample mean removal for complex white noise with an arbitrary
mean. We ﬁnd that the arbitrary mean introduces lag dependent covariance between different lags of the
correlogram estimates in spite of the lack of covariance in white noise for non-zeros lags. We provide a
heuristic rule for when the sample mean should be, and when it should not be, removed if the true mean
is not known. The sampling properties derived here are useful is assesing the general statistical perfor-
mance of autocovariance and autocorrelation estimators in different parameter regimes. Alternatively,
the sampling properties could be used as bounds on the detection of a weak signal in general white noise.

1 Introduction

The correlogram, an estimate of the autocorrelation function (ACF) of a time series, is one of the corner-
stone analyses in the signal processing toolbox and therefore an understanding of its statistical errors for
various random signals is of fundamental importance. Since the ﬁrst published work [2] the sampling prop-
erties of various ACF estimators for a wide variety of different processes have been investigated [6, 5, 1].
Yet it seems that one fundamental process has not draw full attention: general independent and identically
distributed (IID) processes. In particular, IID processes introduce two novel aspects to the statistical errors
of ACF estimators: a non-zero mean and non-analytic signals.

In order to investigate these novel effects and in particular the non-zero mean in particular, we compare
the standard correlogram estimator both with and without sample mean subtracted from the data samples.
These two estimators are introduced in section 2.1 and section 2.2 respectively. We conclude by discussing
how to treat the mean of a process when estimating correlation.

Our main ﬁnding is that an unknown mean introduces non-zero lag dependent covariance irrespective
of whether the sample mean is removed or not. This is surprising seeing that white noise signals are not
non-zero dependent.

2 Lag windowed correlogram estimators

In dealing with an uncertain mean when estimating the autocovariance of a give sequence of data there are
two possible procedures: either one estimates the mean from the given data or the mean can be guessed
in some way not based on the given data. These two procedures taken together with the subsequent cor-
relogram computation can be seen as two different types of autocovariance estimators. That is, there are
autocovariance estimators that include some sort of mean estimation and there correlograms that do not.
In what follows we consider one version of each, namely, we consider the standard windowed correlogram
and the standard windowed correlogram with sample mean removal.

1

(1)

(2)

(3)

(4)

2.1 Estimator without sample mean removal

The classical correlogram does not involve any explicit mean estimation. Allowing for lag windowing, we
deﬁne it for positive lags as

R(µ)

zz [l] :=

[l]

W

b

N −l

Xk=1

z∗[k]z[k + l],

l = 0, 1, 2...N

1,

−

where z[1], z[2], ...z[N ] are possibly complex data samples, N is the number of data samples,
[l] is an
arbitrary real valued lag weighting function, and z∗ denotes the complex conjugate of z. We use square
] to refer to a function of a discrete valued variable.
brackets [
·

W

There are two standard choices for

[l]. If

|
the estimators is called the traditional unbiased correlogram, while

W

[l] =

U [l] :=

W

W

1

N

l
− |

[l] =

P [l] :=

W

W

1
N

is known as the asymptotically unbiased correlogram. The latter is exactly the Fourier transform of the
periodogram. The negative lags of the correlogram are determined from the positive lags in (1) through

R(µ)

zz [l] :=

R(µ)
zz [
(cid:16)

−

l]

(cid:17)

∗

,

b

b

R(µ)

l =

1,

2, ...,

N + 1.

−

−

−

The correlogram

zz can be interpreted as an estimator of either the autocovariance sequence (ACVS)
or the autocorrelation sequence (ACS) of the discrete-time, complex-valued random variable sequence
Z[l]
}

, we mean explicitly the function

b
1, 0, 1, 2...
}
−

. By the ACS of

Z[n]; n = ...

2,

−

{

{
RZZ [l] := E

Z ∗[n]Z[n + l]
}
{

represents the expectation operator and Z ∗ denotes the complex conjugate of Z; and by the

where E
ACVS of

{·}
{

Z[n]
}

, we mean

CZZ [l] := Cov

(Z[n]

µ), (Z[n + l]

{

−

−
see [3] for details. Thus, the ACVS differs from ACS in that the mean µ has been removed from the data
sequence. In other words, ACVS is equivalent to its ACS if the mean of the data sequence is zero. For
continuously sampled processes, the ACS and ACVS are known as the autocorrelation function (ACF) and
autocovariance function (ACVF) respectively.

−

−

µ)
}

= E

(Z[n]
{

µ)∗(Z[n + l]

µ)
}

R(µ)

b

Note that

zz , for a nonzero mean µ signal, can also be interpreted as an autocovariance estimate
in which, based on other, separate information, an assumed mean µg was subtracted from the data (or
that µg was assumed 0 and nothing was done to the data) which actually had the mean µt.
In other
µg could be seen as the error in the assumed mean. If the mean µ of the sequence
words, µ = µt
z′[n]
µ
z[n]
}
}
}
zz [l] by replacing the (µ) index
z′z′ [l]. Thus we can summarize formally the relationship between the autocorrelation and

{
by subtracting the mean. We distinguish this case of the estimator
with (0), viz
autocovariance estimators mentioned above as

is known exactly the signal can be converted into a zero mean sequence

R(µ)

R(0)

z[n]

:=

−

−

b

{

{

b

R(0)

z′z′ [l] =

Czz[l]

which says that the estimator of the form (1) is an ACVS estimator if the process has a zero mean. The
R(0)
zz [l], is well documented and goes back to [2]. The
sampling properties of this special case estimator,
R(µ)
sampling properties of

zz with nonzero µ however, is what will concern us subsequently.

b

b

b

b

2

2.2 Estimator with sample mean removal

If we extend the standard correlogram introduced in the previous section to include the subtraction of the
sample mean from data samples we get the ACVS estimator

C(µ)

zz [l] :=

[l]

W

b

N −l

Xk=1

where

(z[k]

z)∗(z[k + l]

z),

l = 0, 1, 2...N

1,

−

−

−

(5)

z :=

z[k]

1
N

N

Xk=1

is the sample mean. The negative lags can be estimated through a formula analogous to (4).

There are alternatives to simply subtracting the sample mean as in (5) when estimating the ACVS if
zz as it is still commonly used and of

the mean is not known, see e.g. [6]. Here we will only consider
fundamental importance.

C(µ)

b

3 General complex higher-order white noise

The correlograms will now be applied to higher-order complex white noise with arbitrary mean. We seek
to derived the sampling properties of each correlogram up to second-order properties. As it turns out, we
only need to consider moments of the process up to fourth order. Thus, for our purposes it sufﬁces to deﬁne
] with the following properties
a test sequence ǫ[
·

Cov

Cov

ǫ′[n], ǫ′[n + l]
}
{

{

E

= Cov

ǫ[n]
}
ǫ′[n]∗, ǫ′[n + l]∗
}
{
ǫ′[n]∗, ǫ′[n + l]
Cov
}
ǫ′[n]∗ǫ′[n + l]ǫ′[n + l′]
}
{
ǫ′[n]∗ǫ′[n + l], ǫ′[n′]∗ǫ′[n′ + l′]
}

E

{

= µ
= σ2δ0,l = Cǫǫ[l]
= m2δ0,l = s2 exp(iθ2)δ0,l
(9)
= κ3δ0,lδ0,l′
= κ4δn,n′δ0,lδ0,l′ + σ4δn,n′δl,l′ + s4δn+l,n′δ−l,l′ (10)

(7)

(8)

(6)

−

{
µ is the centralised version of the process, δl,m is the Kronecker delta, µ is the mean, σ2 is
where ǫ′ := ǫ
the central variance, m2 is the second central moment, κ3 and κ4 are the third and fourth order cumulants
respectively and n, n′, l and l′ are all arbitrary integers. The quantity m2 and s2 is what we will call the
quadratic variance and the quadratic variance amplitude respectively. These names reﬂect the fact that
they are not hermitian in contrast with the ordinary variance σ2.

The ﬁrst property implies that the process has an arbitrary mean. The second is that the autocovariance
is zero except for the zero lag. The third property is the nonhermitian quadratic autocovariance of the pro-
cess which usually is either zero or equal to the autocovariance. The fourth property is a third order lagged
cumulant of the process. The last property basically implies that there is no covariance between different
autocovariance lags. Fifth orders and above are not speciﬁed and so ǫ[
] could have higher order correlation.
·
Thus ǫ[
] is more general than IID processes yet is equivalent in fourth and lower order properties. See [7]
·
for deﬁnitions of higher order lagged cumulants of random processes.

For reference, we have the following relations in special cases: for a purely real process (hence nonan-

alytic),

where

ℑ{·}

ǫ[n]
}
is the real part operator, while for an analytical process [3, sec. 13.2.3]

= 0

ℑ{

⇒

s2 = σ2

⇒
and for zero mean complex circular Gaussian white noise

] is analytic
ǫ[
·

s = 0

k, ǫ[k]

∀

0, σ

∼ N {

} ⇒

κ4 = κ3 = s = µ = 0,

3

i.e. only σ2is non-zero, while for nontrivial Poissonian real white noise

k, ǫ[k]

∀

λ

Poi
{

∼

} ⇒

κ4 = κ3 = σ2 = µ = λ, s = 0.

4 Sampling properties of the estimators to second order

We now present some of the sampling properties, namely the bias, the variance, the covariance, and the
mean-square error (MSE) for the case of the general noise process ǫ[
]. These quantities were derived
·
employing the usual techniques of estimation theory [3] and assuming that the process has the properties
given in the previous section. The details of the derivations are given in a companion paper [4].

4.1 Sampling properties of correlogram without sample mean removal

The estimator
order are as follows. The bias for all lags is found to be

R(µ) was deﬁned in (1). With respect to the ǫ[
] process, its sampling properties to second
·

b

Bias

R(µ)

ǫǫ [l]

= E

R(µ)

ǫǫ [l]

Rǫǫ[l] = ((N

o −
This shows that the so called unbiased estimator,
not zero. All other

b
[l] will result biased estimators if the mean is not zero.

W

−

o

n

n

b

l
− |

)
W
|

[l]

1)(σ2δ0l +

2)
µ
|
|

U [l], is in fact always unbiased even when the mean is

(11)

The covariance/variance of

R(µ) for any two lags l and l′ of the same sign is

W

b
[l]

[l′]

W
W
+ 2δ0l′

Cov

R(µ)

ǫǫ [l],

n

R(µ)

ǫǫ [l′]
o

=

b

b

) + δll′ σ4(N
l
− |
|
l′
, N ))
l
min(
|
|
|
(cid:1)
and when the lags have different signs the covariance is

ℜ{
µ2m∗
2}

+2

ℜ{

(N

−

+

|

δ0lδ0l′ (κ4 + s4)N + 2δ0l
(cid:0)
µκ∗
3}
(N

(N

µκ∗
l′
)+
3}
− |
|
2σ2 (N
) + 2
µ
|
|
|
ll′

0

ℜ{
l
− |
,

≥

l
max(
|

,
|

−

l′
|

))
|
(12)

Cov

R(µ)

ǫǫ [l],

n

R(µ)

ǫǫ [l′]
o

=

ℜ{

(N

µκ∗
3}
) + 2
|
ll′

l′
)+
− |
|
2σ2 (N
µ
|
|
0

[l]

W
W
+ 2δ0l′

[l′]

δ0lδ0l′ (κ4 + σ4)N + 2δ0l
(cid:0)
µκ∗
3}
ℜ{
µ2m2}
(N

(N

−

+

|

b

b

−

,
|

ℜ{

+2

l′
|

l
− |
,

l
max(
|

) + δ−l,l′ s4(N
l
− |
|
l′
, N ))
l
min(
|
|
|
(cid:1)
These expressions can broken down into all combinations of covariances and variances for zero lags and
nonzero lags. The terms without δ factors are the covariance of the nonzero lags of the estimator. They are
zero if the mean is zero but otherwise they are non-zero and have a piece-wise linear dependence on the
lags before applying the weight functions
= 0
constitute the variance of the nonzero lags. This is nonzero and linear in lag before weighting even when
the mean is zero. The terms with single δ0l or δ0l′ factors plus the terms without δ evaluated at either
l = 0 or l′ = 0, are the covariance between zero lag and non-zero lags. These are nonzero if the odd order
moments µ and µ3 are nonzero. Finally, the terms with the δ0lδ0l′ factor plus all other terms evaluated
at l = l′ = 0 is the variance of zero lag. It depends on all the moments. An example of the covariance
structure is shown in ﬁgure 1a), 1b), and 1c).

[l′]. The δll′ term plus the terms without δ for l = l′

))
|
(13)

W

W

[l]

≤

The bias and the variance given above can be combined to give the mean-square error for the zero lag

and the nonzero lag respectively

MSE

R(µ)

ǫǫ [0]

= (κ4 + s4 + 4

n

b

n

b

o

o

µκ∗
3}

ℜ{
2)2
µ
|
|
W
2) (N
µ
|
|
l
− |

)
W
|

2N (σ2 +

−
σ2(σ2 + 2
(cid:0)
−

4(N
µ
|
|

2

l
− |
[l] +

) + 2
|
4
µ
|
|

+ 2

ℜ{
[0] + (σ2 +

+

µ2m∗
4N + σ2(σ2 + 2
µ
2}
|
|
2)2
µ
|
|
µ2m∗
2}

min(2

ℜ{

(N

l
|

−

MSE

R(µ)

ǫǫ [l

= 0]

=

2)(N + 1))N
µ
|
|

W

2[0]+

, N )) +
|

4(N
µ
|
|

l
− |

)2
|

2[l]+

(14)

W
(cid:1)
(15)

4

6
6
These expressions are exact for all sample sizes N . Asymptotically, that is as N
assuming µ

= 0, as

→ ∞

, the MSE behave,

MSE

R(µ)

ǫǫ [0]

n
b
R(µ)
ǫǫ [l

= 0]

MSE

n

=

=

o

o

2

2 + σ2
µ
|
|
(cid:0)
4
µ
|
|

(N
(cid:0)

(cid:1)

l
− |

2[0]

N 2
(cid:0)
)2
|

W

W
2[l]

−

2N

[0] + 1

, N

1, µ

= 0

(16)

−
2(N

W
l
− |

)
W
|

(cid:1)
[l] + 1

≫
, N

≫

1, µ

= 0

(17)

where we have kept only the leading terms in N and l for each power of

(cid:1)
[l].
From the asymptotic expression, we see that for the unbiased lag weights

W

b

the MSE are zero. The asymptotic MSE for the nonzero lags in this case is

U [l] the leading terms in

W

MSE

R(µ)

ǫǫ [l

= 0]

=

o

n

b

σ2(σ2 + 2

2) + 2
µ
|
|

(N

µ2m∗
2}
ℜ{
l
N
|
− |
P [l], the asymptotic MSE is

−

min(2

l
|

, N ))
|

,

While for the periodogram weighting
for large lags and so it is not a consistent estimator.

W

U , N

1.

(18)

W ≡ W

≫

4l2/N 2 which does not tend to zero
µ
|
|

This is in contrast to the well known case of zero mean. If µ = 0 then the asymptotic MSE of the
2[l] for all N as expected. The MSE of the unbiased estimator in this
nonzero lags is instead σ4(N
)
W
|
case tends asymptotically to σ4/(N
) and so it does not converge for large lags, while the MSE for the
l
|
− |
periodogram estimator tends to σ4(1
/N )/N which tends to zero for large lags. It is for this reason
l
|
− |
P [l] is preferred instead of the unbiased weighting
that the periodogram weighting function
W

l
− |

U [l].

W

For the special case of zero mean see also [2].

4.2 Sampling properties of correlogram with sample mean removal

The sampling properties of the correlogram estimator
found to be as follows.

C(µ), deﬁned in (5), for the noise process ǫ[
] were
·

The bias is

b

Bias

C(µ)

ǫǫ [l]

= σ2

n

b

o

(N

[0]

(cid:18)

W

−

1)δ0l

−

(N

[l]

−

l)
N

W

.

(cid:19)

(19)

From this expression we ﬁnd, remarkably, that all nonzero lags are biased irrespective of the choice of
weights. For instance, the weights
U [l] known as unbiased in relation to the zero-mean correlogram
σ2/N for all lags. It is however possible to get an unbiased estimate for the zero
R(0), lead to a bias of
C(µ) is equivalent to the usual
lag if one chooses
b
sample variance.

1). For this choice, the zero lag of

−
[0] = 1/(N

W

W

−

The covariance/variance of

C(µ) between any two lags l and l′ of the same sign is

b

Cov

C(µ)

ǫǫ [l],

n

b

C(µ)

ǫǫ [l′]
o

=

W

b
[l]

b

[l′]

δ0,lδ0,l′ (κ4 + s4)N +
W
(cid:0)
1
δ0,lκ4 (cid:18)
κ4
N (cid:18)

1
δ0,l′ κ4 (cid:18)
l′
|

l′
|
N (cid:19) −
,
|

l
|
|
N (cid:19)
−
+
l
) + 2 min(
|
|
|
N

−
l
2 max(
|

−

1

|

−

+

+ δl,l′ σ4(N

l′
|

, N )
|

−

)+
|
+

l
− |
l
3(
|

l′
|

|

)
|

3

ll′
N 2 (cid:19)

+

−

σ4

1
(cid:18)

−

−

s4

−

1

(cid:18)

−

l
2 max(
|

,
|

l
2 min(
|

|

+

l
− |

l′
)
|
|
N
l′
, N )
|
|
N

l′
| − |

|

−
l′
| − |

l
− |

+

ll′
N 2 (cid:19)
ll′
N 2 (cid:19)(cid:19)

−

|

,

ll′

0

≥

(20)

5

6
6
6
6
6
and for lags of different signs

Cov

C(µ)

ǫǫ [l],

C(µ)

ǫǫ [l′]
o

=

[l]

W

n

b

b

∗[l′]

W
1
δ0,lκ4 (cid:18)
κ4
1
N (cid:18)

−

δ0,lδ0,l′ (κ4 + σ4)N +
(cid:0)

|

l′
|
N (cid:19) −
,
|

−
l
2 max(
|

1
δ0,l′κ4 (cid:18)
l′
|

l
|
|
N (cid:19)
−
+
l
) + 2 min(
|
|
|
N

−

+

+ δ|l|,|l′|s4(N

l′
|

, N )
|

l
3(
|

|

−

l
− |
+

)+
|
l′
|

)
|

3 |

ll′
|
N 2 (cid:19)

+

−

s4

1
(cid:18)

−

−

σ4

−

1
(cid:18)

−

l
2 max(
|

,
|

l
2 min(
|

|

+

l
− |

l′
)
|
|
N
l′
, N )
|
|
N

l′
| − |

l
− |

+

|

ll′
|
|
N 2 (cid:19)
−
l′
| − |

|

ll′
|
|
N 2 (cid:19)(cid:19)

,

−

ll′

0.

≤

(21)

Again, this expression uniﬁes all combinations of variances and covariances between zero and nonzero
lags; see the discussion in the previous subsection. The main differences with that of the estimator without
mean removal is that in this case there is no dependence on the odd order moments µ and κ3, and further,
that the lag dependence before applying the weighting functions factor is quadratic in lag. An example of
the covariance matrix is shown in ﬁgure 1d), 1e), and 1f).

C(µ) estimator can be determined for the expressions for the bias and covariance given

The MSE of the
above. The result is

MSE

C(µ)

ǫǫ [0]

σ4(N

1)N + (κ4 + s4)N

b
=

o

(cid:16)

−

n

b

s4

−

−

2κ4 +

κ4
N (cid:17) W

2[0]

2(N

1)σ4

[0] + σ4

−

−

W

MSE

C(µ)

ǫǫ [l

= 0]

=

σ4(N

o

(cid:18)

n

b

l
− |

)
|

l
2 |
1
|
N 2 (cid:19)
(cid:18)
−
, N )
l
2 min(2
|
|
N

−

+

2

κ4
1
N (cid:18)
l
|

|

−

−
l2
N 2 (cid:19)(cid:19) W

2[l]

s4

−

1
(cid:18)

−

2

l
|

|

+ 2 min(2
l
|
N

, N )
|

−

6

l
|

|

3

l2
N 2 (cid:19)

+

−

The MSE in the asymptotic case is

MSE

C(µ)

ǫǫ [0]

n
b
C(µ)
ǫǫ [l

o

= 0]

MSE

2N

[0] + 1

, N

= σ4

N 2
(cid:0)
= σ4(N

2[0]

)
W
|

W
l
− |

W
−
2[l], N

(cid:1)

1

≫

1

≫

n

o
where we have kept only the leading terms in N or l for each coefﬁcient of
sequence
σ4(1
l
− |
in both cases is different for different lags.

[l]. For the weighting
P [l], the MSE tends to zero as
); while for
l
W
|
− |
/N )/N . So both weight sequences lead to consistent estimators. However, the asymptotic MSE
|

U [l], the MSE tends to zero as σ4/(N

W

W

b

For the special case of real valued processes see [1].

(22)

(23)

(24)

(25)

5 Comparison of the correlograms with and without sample mean

removal

Now that we have the second order sampling of the correlograms with and without sample mean removal
we can compare them. Examples of the covariance of the estimators are shown in ﬁgure 1 for various kinds
of white noise both with and without zero means; and a comparison of the MSEs is shown in ﬁgure 2 for a
zero mean noise process.

Inspection of the sampling properties derived here reveals some novel features. One such feature is that
the estimators nominally have nonzero covariance, that is, the estimates at different lags are not indepen-
dent. This is down to the fact that the same data samples are reused in the evaluation of different lags of the
correlogram leading to a relationship between lag estimates. More surprising is the fact that the variance
of both the estimators are nominally lag dependent. This is despite the fact that the test sequence ǫ is white
and hence has no nonzero lag dependence, see (7). Also the covariance is lag dependent.

6

6
6
Without
sample mean removed

With
sample mean removed

10

a)

10

d)

-5

0

5

10

-5

0

5

10

-5

0

5

10

-5

0

5

10

n
a
i
s
s
u
a
G
 
x
e
l
p
m
o
C

i

n
a
s
s
u
a
G

 
l

a
e
R

i

n
a
n
o
s
s
o
P

i

 
l
a
e
R

'
l

 
g
a
L

'
l

 

g
a
L

'
l

 

g
a
L

8

6

4

2

0

-2

-4

-6

-8

8

6

4

2

0

-2

-4

-6

-8

8

6

4

2

0

-2

-4

-6

-8

-10

-10

10

b)

-10

-10

10

c)

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0.2

0.15

0.1

0.05

0

e
c
n
a
i
r
a
v
o
C

e
c
n
a
i
r
a
v
o
C

e
c
n
a
i
r
a
v
o
C

-10

-10

0
Lag l

-10

-10

0
Lag l

-5

5

10

-5

5

10

Figure 1: Covariance and variance between any two lags l and l′ of the correlogram estimator (N = 11)
with and without sample mean removal for various types of white noise signals. The ﬁrst column of panels,
R(µ)[l], deﬁned in equation (1);
namely a), b) and c), are the correlogram without sample mean removal,
while the second column of panels, namely d), e) and f), are the correlogram with sample mean removal,
C(µ)[l], deﬁned in equation (5). The signal in the ﬁrst row of panels, namely a) and d) is analytic Gaussian
white noise σ2 = 1; the second row, b) and e), is real Gaussian white noise; the third row, c) and f), is real
b
Poissonian white noise with µ = 1. In all plots, the variance is along the l = l′ diagonal while the rest,
= l′ is strictly covariance. Note that the gray-scale in the second column is different from the linear scale
l
of the ﬁrst column in order to enhance the structure of the covariance.

b

8

6

4

2

0

-2

-4

-6

-8

8

6

4

2

0

-2

-4

-6

-8

8

6

4

2

0

-2

-4

-6

-8

-10

-10

10

e)

-10

-10

10

f)

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

7

6
  1

10

r
o
r
r

 

E
e
r
a
u
q
S
n
a
e
M

 

  2

10

3
10

1

^ R (0)

εε

 ^C (µ)

εε

2

3

4

5

6

7

8

9

10

Lag

Figure 2: Comparison of the MSE of the
P [l] weights for the case of
analytic Gaussian white noise with σ2 = 1. For all lags, the autocovariance sequence estimator using the
C(µ)represented by circles, exhibits a smaller MSE than the estimator using the known
sample mean, i.e.
mean, i.e.

R(0) represented by stars.

C(µ)estimators with

R(0)and

W

b

b

b

b

8

5.1

R(0)and

C (µ) as autocovariance estimators when mean is known

b

b

In the preceding sections we have assumed that the mean of the data sequence we are trying to estimate the
autocovariance of is unknown. What can be said if the mean is known? It would seem natural that if we
R(0). Thus we would not
knew the mean we would subtract it from the data samples and use the estimator
C(µ) as it involves estimation of the mean which we already know. Surprisingly however,
use the estimator
inspection of the sampling properties presented here show that this choice of estimators is not immediately
obvious.

b

b

The MSE of the estimator

C(µ) is, in fact, smaller than that of the estimator

R(0) for general white

noise, assuming the same lag window is used in both estimators. The difference in the MSE is

b

b

2σ4 l

N 2 (N

l)

−

W

2[l]

(26)

b

C(µ) is always better than the

and is plotted in Figure 2. This difference tends asymptotically to zero as N is increased, but for ﬁnite
R(0) in the MSE sense. This is counter-intuitive as it seems
sample sizes the
to violate the principle that the more one knows about something, the better one can estimate it.
b

The solution to this conundrum is that although the error in the covariance estimate is smaller, the error
in the estimate of the mean is on the other hand larger. Speciﬁcally the difference in the MSE in the mean
estimate is σ4/N . If take, e.g., the weights
P [l] and use the inequality l < N then the MSE for lag l must
be smaller than 2σ4l/N 3 and so the total MSE of the estimator is of the order σ4/N . This is comparable
to the MSE in the sample mean. Thus the total error, autocovariance and mean estimation, is not better for
the

R(0).

W

C(µ) has a nonzero covariance between its lag estimates which

R(0)does not have. This
can be understood from the following observation: that the sum over all non-negative lags of the estimator
R(µ) with periodogram weighting is equal to sample size times the square of the sample mean, i.e.

b

b

b

C(µ) compared with
Furthermore
b

b

R(P )

zz [l] =

1
N

N −1

Xl=0

b

N −1

N −l

N −1

Xl=0

Xk=1

2
z[k](cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
R(P )
zz and the sample mean z are
zz and z enters into every lag estimate, this suggests that there is an

z∗[k]z[k + l] = N (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= N

Xk=1

2
|

z
|

for any process
{
R(P )
related. But since
interdependence between lags and explain the nonzero covariance of the lags.

. See [8] for further discussions. This shows that

z[k]
}
C(µ) is based on

b

b

b

5.2 When to use the sample mean if the mean is assumed small

Let us now look at the situation when we believe that the mean is small. The question is which estimator is
better: the one without or the one with sample mean removal. If we use the former we run the risk that the
error in the estimated mean is large than the true mean. On the other hand, the mean may not be exactly
zero so the latter estimate will also be off. There is a trade off here and we wish to ﬁnd a criterion for when
the sample mean should be removed.

Inspection of the results presented earlier suggest that as a rule of thumb the two estimators are roughly

equal when

=

µ
|
|

σ
√2N

(27)

µ
|
|

< σ/√2N ,

R(µ) is preferable to

R(µ).
and when
This condition can be understood in an intuitive way as follows. The sample mean is an estimate of the
b
population mean with a relative error of (σ/√N )/
. It makes sense to use this estimate in the correlogram
µ
|
|
> σ/√2N . This
instead of the unknown mean only if this relative error is smaller than 1, i.e. when
is because the absolute error if we do not remove anything is of course only
. Thus we arrive at the
µ
|
|
condition as the found above (27).

C(µ) is preferable to

C(µ) and when

> σ/√2N ,

µ
|
|

µ
|
|

b

b

b

Naturally, if we do not know the mean we will not be able asses the equality (27) exactly, but one could
instead get it approximately by using the sample mean and the sample variance of the given data samples.

9

•

•

•

•

•

•

•

•

6 Conclusion

We have presented expressions for the bias, variance, covariance, mean-square error of the classical correl-
ogram estimator with and without sample mean estimation for general complex white noise with arbitrary
mean. A summary of the sampling properties of the estimators for ǫ[
], i.e., complex higher order white
·
noise with arbitrary mean is as follows. The second-order sampling properties of the correlogram without
sample mean removal,

R(µ), are in summary:

] moment dependence: cumulants up to fourth order
ǫ[
·

b

Bias: unbiased for 1/(N

l) weighting, even when µ

µ, σ2, s2, κ3, κ4}
{
= 0

−

Covariance: piecewise linear lag dependent covariance before weighting

Mean square error: is asymptotically proportional to
asymptotically equal to σ4(N

2[l]

4 when µ
µ
|
|

l
− |

)
W
|

= 0 while if µ = 0 the MSE is

For the correlogram with sample mean removal,

C(µ), a summary of second-order sampling properties is:

] moment dependence: only even order cumulants
ǫ[
·

σ2, s2, κ4}

{

b

Bias: nonzero lags biased for all weighting functions, zero lag unbiased for

[0] = 1/(N

1)

W

−

Covariance: piecewise quadratic lag dependent covariance before weighting

Mean square error: asymptotically equal to σ4(N

l
− |

)
W
|

2[l]

In terms of the properties of the process, we have found that a nonzero mean can lead to covariance between
lags of the correlogram, and that complex valued processes lift the degeneracy between positive and positive
lags, (that is that the sampling properties of positive lags and negative lags are different), and distinguishes
between analytic and non-analytic processes.

For both estimators, the both the variance and the covariance are in general lag dependent. We con-
C(µ) with the standard lag weights
P [l] are optimal for

clude therefore that neither
estimating the autocorrelation/autocovariance of noise processes with an unknown mean.

R(µ) nor

U [l] and

W

W

Finally, we found that the sampling properties suggest that one should always remove the mean if it is
> σ/√2N ,

known apriori. If the mean is not known, the sample mean should be removed only when
i.e.

is preferable to

< σ/√2N .

µ
|
|

C(µ)
ǫǫ

R(µ)
ǫǫ

if

b

b

µ
|
|

b

b
Acknowledgments

References

This work was sponsored by PPARC ref: PPA/G/S/1999/00466 and PPA/G/S/2000/00058.

[1] T. W. Anderson. The Statistical Analysis of Time Series. John Wiley & Sons, Inc., 1971.

[2] M. S. Bartlett. On the theoretical speciﬁcation and sampling properties of autocorrelated time-series.

Supplement to the Journal of the Royal Statistical Society, 8(1):27–41, 1946.

[3] Juilus S. Bendat and Allan G. Piersol. Random data: analysis and measurement procedures. John

Wiley and Sons, Inc, third edition edition, 2000.

[4] T. D. Carozzi and A. M. Buckley. Deriving the sampling errors of correlograms for general white

noise. To be published in Biometrika, 2005, arXiv:physics/0505145.

[5] Gwilym M. Jenkins and Donald G. Watts. Spectral Analysis and its applications. Holden-Day, Inc,

1968.

10

6
6
[6] F. H. C. Marriott and J. A. Pope. Bias in the estimation of autocorrelations. Biometrika, 41(3/4):390–

402, 1954.

[7] Chrysostomos L. Nikias and Jerry M. Mendel. Signal processing with higher-order spectra. IEEE

Signal Processing Magazine, 10(3):10–37, July 1993.

[8] Donald B. Percival. Three curious properties of the sample variance and autocovariance for stationary

processes with unknown mean. The American Statistician, 47(4):274–276, 1993.

11


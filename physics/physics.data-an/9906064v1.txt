9
9
9
1
 
n
u
J
 
0
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
6
0
6
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

When do ﬁnite sample eﬀects signiﬁcantly aﬀect
entropy estimates ?

Centre de Physique Th´eorique, CNRS and Universit´e de Provence, Marseille, France

T. Dudok de Wit

accepted in Eur. Phys. J. B

Abstract

An expression is proposed for determining the error made by neglecting
ﬁnite sample eﬀects in entropy estimates. It is based on the Ansatz that
the ranked distribution of probabilities tends to follow a Zipf scaling.

1

Introduction

The growing interest in complexity measures and symbolic dynamics [1, 2] has
brought to the forefront various problems related to the estimation of entropic
quantities from ﬁnite sequences [3]. Such estimates are known to suﬀer from a
bias, which prevents quantities such as the metric entropy from being meaning-
fully estimated. The purpose of this letter is to provide an analytical expression
for this bias,in order in order to test for ﬁnite sample eﬀects in entropy estimates.

Consider the general case of a string of N symbols {i1i2 · · · iN }, each of which
belongs to a ﬁnite alphabet A. The average informational content of substrings
of length d taken from this sequence is expressed by the Shannon entropy [4]

Hd = −

µ ([i1i2 · · · id]) log µ ([i1i2 · · · id]) ,

(1)

i1,...,id∈A
X

where µ is the natural invariant measure with respect to the shift. Of particular
interest is the block or dynamical Shannon entropy hd = Hd+1 − Hd from which
one gets the measure-theoretic entropy of the system

h(µ) = lim
d→∞

hd ,

1

(2)

a quantity that is intimately related to the Kolmogorov-Sina¨ı entropy in case
the string represents the output of a shift dynamical system.

The main problem lies in the estimation of the empirical measure µ from a

ﬁnite string of symbols. Direct box counting yields

µ ([i1i2 · · · id]) ≈

#[i1i2 · · · id]
N − d + 1

,

(3)

where #[i1i2 · · · id] is the occurrence frequency of the block i1i2 · · · id in the
string. It is well known that statistical ﬂuctuations in the sample on average
lead to a systematic underestimation of the entropy. This problem becomes
particularly acute as the word size increases for a given string length N . Since
this deviation can easily be mistaken for the signature of a ﬁnite memory process,
it is of prime importance to determine whether its origin is physical or not.

Several authors have already addressed the problem of making corrections to
empirical entropy estimates [3, 5, 6, 7]; their expressions are valid as long as the
occurrence frequencies of the observed words are large compared to one. While
this may hold for relatively short words, it breaks down for long ones, making
it diﬃcult for a small correction to be used as a safe indication for a small
deviation. Our objective is to derive a more reliable (although less accurate)
expression of the deviation, to be used as a warning signal against the onset of
ﬁnite sample eﬀects.

As a ﬁrst guess one could require the sample to be long enough for each
word to have a chance to appear. This gives N ≫ N d
symb, where Nsymb is the
cardinality of the alphabet. This criterion, however, is generally found to be too
conservative because it does not take into account the grammar, i.e. the rules
that cause some words to be forbidden or less frequent than others.

2 The Zipf-ordered distribution

To derive our expression, we ﬁrst rank the words according to their frequency
of occurrence: let nk=1 denote the frequency of occurrence of the most probable
word, nk=2 of the next most probable one etc. Multiple instances of the same
frequency get consecutive ranks. This monotonically decreasing distribution is
called Zipf-ordered.

The Asymptotic Equipartition Property introduced by Shannon [4] states
that the ensemble of words of length d can be divided into two subsets. The
ﬁrst one consists of “typical words” that occur frequently and roughly have
the same probability of occurrence. The other subset is made of “rare words”
that belong to the tail of the distribution. According to the Shannon-Breiman-
MacMillan theorem, the entropy is related to the typical words in the limit
where N → ∞; the contribution of rare words progressively disappears as N
increases. In some sense this observation justiﬁes the procedure to be described
below.

2

It was noted by Pareto [8], Zipf [9] and others, and later interpreted by
Mandelbrot [10] that the tail of the Zipf-ordered distribution nk tends to follow
a universal scaling law

nk = αk−γ , γ > 0 .
which is found with astonishing reproducibility in economics, social sciences,
physics etc. [10]. As shown in [11, 12], many diﬀerent systems give rise to Zipf
laws, whose ubiquity is thought to be essentially a consequence of the ranking
procedure.

(4)

The physical meaning of Zipf’s law is still an unsettled question, although
it does not seem to reﬂect any particular self-organization (see for example [13,
14]). We just mention that a slow decay is an indication for a “rich vocabulary”,
in the sense that rare words occur relatively often.

The key point is that the empirical Zipf-ordered distribution has a cutoﬀ at
some ﬁnite value k = Nmax because of the ﬁnite length of the symbol string.
For the same reason, the occurrence frequencies are necessarily quantized. Our
main hypothesis is that the true distribution extends beyond Nmax, up to the
lexicon size K ≥ Nmax, following Zipf’s law with the same exponent γ. This
Ansatz has already been suggested as a way to estimate entropies from long
words [15].

3 Estimating the bias

Let ˆH be the Shannon entropy computed from the empirical distribution (using
eqs. 1 and 3) and H the entropy one would obtain from a non truncated distri-
bution, in which the frequencies are not quantized anymore and extend beyond
Nmax following Zipf’s law.

ˆH = −

H = −

Nmax

k=1
X
K

k=1
X

nk
Nmax
k=1 nk

log

nk
Nmax
k=1 nk

,

P
nk
K
k=1 nk

log

P
nk
K
k=1 nk

.

The truncation has two counteracting eﬀects. It changes the renormalization
of the occurrence frequencies and causes some of the least frequent words to be
omitted.

P

P

The diﬀerence δ between the two entropy estimates

is what we call the bias, to be used as a measure of the deviation resulting from
ﬁnite sample eﬀects. We shall assume that Nmax ≫ 1, which is equivalent to
saying that the distribution must have a suﬃciently long tail for a power law to
make sense.

δ = H − ˆH .

3

(5)

(6)

It is natural to deﬁne a small parameter 0 ≤ ε ≪ 1, which goes to zero for

a non truncated distribution

ε =

1
N

K

nk .

k=Nmax+1
X

Remember that N =

K
k=1 nk [16].

Now, assuming that Zipf’s law persists for k > Nmax, we have

ε =

1
N

P

K

k=Nmax+1
X

α
N

(cid:0)

αk−γ =

ζ(γ, Nmax + 1) − ζ(γ, K + 1)

,

(8)

(cid:1)

where ζ(γ, m) is the Hurwitz or generalized Riemann zeta function. For k > γ,
the following approximation holds [17]
m1−γ
γ − 1

m−γ−1
12

m−γ
2

ζ(γ, m) =

(9)

+

−

.

Since K, Nmax ≫ 1, we may write
α
N (γ − 1)

ε =

N 1−γ

max − K 1−γ

.

(cid:0)

(cid:1)

The value of α remains to be determined. To do so, we note that the least
frequent words in the Zipf-ordered distribution occur once or a few times only.
One may therefore reasonably set nk=Nmax ≈ 1, giving α ≈ N γ

max.

The bias δ can now be expanded in powers of ε. Keeping terms of order

O(ε) only, we have

δ = −ε ˆH + (1 + ε)

ε −

K

nk
N

log

nk
N !

.

 

k=Nmax+1
X

For the conditions stated before, the sum can be approximated by

K

nk
N

nk
N

k=Nmax+1
X

(cid:18)

log

= −ε

log N − γ log

Nmax
K

,

(cid:19)

ﬁnally giving the result of interest

(7)

(10)

(11)

(12)

(13)

δ = ε

1 + log N − ˆH − γ log

ε ≈

(cid:18)
Nmax
N (γ − 1)  

1 −

Nmax
K

(cid:18)

Nmax
K
γ−1

(cid:19)

.

(cid:19)

!

Notice that the true entropy is always underestimated; furthermore ε is contin-
uous at γ = 1 [18]. Most of the variation comes from the small parameter ε,
whose expression reveals two diﬀerent eﬀects :

4

1. the ratio Nmax/N reﬂects the uncertainty of the frequency estimates.

2. the scaling index γ, whose value is usually between 0.5 to 1.5, is indicative
of the lacunarity of the word distribution. In the case of a shift dynamical
system, γ reveals how unevenly the rare orbits ﬁll the phase space.

For the sake of comparison, the ﬁrst order approximation for ﬁnite sample eﬀects
derived in [5, 6] is

δ =

Nmax
2N
We conclude from eq. 13 that the bias is not just related to statistical ﬂuctu-
ations in the empirical occurrence frequency, but is also caused by the omission
of words that are asymptotically rare. If the true distribution of the ranked
words were exponential or ultimately ended with an exponential tail, then our
criterion would be too conservative but still reliable as such.

(14)

.

The following procedure is proposed for detecting the maximum word length
for which entropies can be meaningfully estimated : compute Zipf-ordered dis-
tributions for increasing word-lengths d. For each length, estimate the bias δ
by least-squares ﬁtting a power law to the tail of the observed distribution. As
soon as this bias exceeds a given threshold (say 10% of ˆH), then entropies com-
puted from longer words are likely to be signiﬁcantly corrupted by ﬁnite sample
eﬀects.

Equation 13 supposes that the maximum lexicon size K is known a priori,
which is seldom the case. This is not a serious handicap, however, since the
value of K has relatively little impact on the bias; a rough approximation such
as K = N d

symb may do well.

4 Two examples

To brieﬂy illustrate the results, we now consider two examples. The ﬁrst one
is based on a Bernouilli process, whose entropy and Zipf-ordered distribution
can be calculated analytically. The string of symbols is drawn from a two letter
alphabet, one with probability λ and the other with probability 1 − λ. The
block entropy of this process is independent of the word length and equals

h = −λ log λ − (1 − λ) log(1 − λ) .

(15)

Figure 1 compares the true block entropy with estimates drawn from a sam-
ple of length N = 2000 with λ = 0.15. The departure of the empirical estimate
from the true one is evident. Without knowledge of the true entropy, however,
it is very diﬃcult to tell whether the decrease of the entropy is an artifact or
just the signature of a short-time memory.

The second panel displays the true and the empirical Zipf-ordered distribu-
tions as obtained for words of length d = 9. Zipf’s law clearly holds for words

5

whose rank exceeds about 30. After this, the scaling exponent γ is estimated,
see the third panel. The decrease of this exponent with the word length d sug-
gests that the contribution of the rare words becomes increasingly important.
Finally, the bias δ, which is shown in the fourth panel, suggests that the onset
of a signiﬁcant bias occurs around d = 8; this value is indeed in agreement with
the results of the ﬁrst panel.

The validity of the bias estimate was tested on various examples and was

found to be reliable, provided that Nmax ≫ 1.

In the second example, we consider a sequence of N = 104 symbols generated
by the logistic map xi+1 = λxi(1 − xi) in a chaotic regime with λ = 3.8. The
(generating) partition P = {[0, 0.5[, [0.5, 1]} gives us a two-letter alphabet.

Figure 2 again shows that the block entropy decreases above a certain word
length. In contrast to the previous example, the measured scaling exponent γ is
small and almost constant, regardless of the word length. We believe this to be a
consequence of the intricate structure of the self-similar attractor. This low value
of γ already suggests that rare words should bring a signiﬁcant contribution to
the entropy. The bias δ ﬁnally suggests stopping at d = 12.

6

hˆ

0.6

0.4

0.2

0

0

102

k
n

100

100

3

2

1

0

γ

Hˆ

/
δ

10−1

101

100

10−2

10−3

5

15
10
word length d

20

25

101

102

103

k

0

5

20

25

15
10
word length d

0

5

20

25

15
10
word length d

Figure 1: Analysis of a Bernouilli sequence, with N = 2000 and λ = 0.15.
From top to bottom: (1) the empirical block entropy and the true one (dashed),
(2) the true (line) and observed (dots) Zipf-ordered distributions for words of
length d = 8; (3) the scaling exponent γ obtained by ﬁtting the tail of the Zipf-
ordered distribution (error bars represent ±1 standard deviation resulting from
the least-squares ﬁt), (4) the bias δ. In this case, entropies cannot be reliably
estimated for word lengths beyond d = 9. Block entropies are normalized to
7
log Nsymb, so that the maximum possible value is 1.

0.6

0.4

0.2

hˆ

0

0

102

101

k
n

100

100

1

γ

0.5

0

0

Hˆ

/
δ

10−1

101

100

10−2

10−3

5

10
15
word length d

20

25

102
k

104

5

10
15
word length d

20

25

0

5

20

25

15
10
word length d

Figure 2: Analysis of a logistic map sequence, with the same legend as the
previous ﬁgure; the string length is N = 104. The second panel shows a Zipf-
ordered distribution for d = 18. The largest word size for which the relative
bias is smaller than 10%, is d = 12.

8

5 Conclusion

Summarizing, we have derived a simple expression (eq. 13) for detecting the
onset of ﬁnite sample size eﬀects in entropy estimates. It is based on the em-
pirical evidence that rank-ordered distribution of words tend to follow Zipf’s
law. The criterion reveals that rare events can signiﬁcantly bias the empirical
entropy estimate.

References

[1] C. Beck and F. Schl¨ogl, Thermodynamics of chaotic systems (Cambridge

University Press, Cambridge, 1993).

[2] R. Badii and A. Politi, Complexity: hierarchical structures and scaling in

physics (Cambridge University Press, Cambridge, 1997).

[3] T. Sch¨urmann and P. Grassberger, Chaos 6, 414 (1996).

[4] R. E. Blahut, Principles and practice of information theory (Addison Wesley,

Reading, MS, 1987).

[5] H. Herzel, Sys. Anal. Mod. Sim. 5, 435 (1988).

[6] P. Grassberger, Phys. Lett. A 128, 369 (1988).

[7] A. O. Schmitt, H. Herzel, and W. Ebeling, Europhys. Lett. 23, 303 (1993).

[8] V. Pareto, Cours d’´economie politique (Rouge, Lausanne, 1897).

[9] G. Zipf, Human behavior and the principle of least eﬀort (Addison-Wesley,

Cambridge MA, 1949).

[10] B. Mandelbrot, Fractals and scaling in ﬁnance: discontinuity, concentra-
tion, risk (Springer, New York, 1997); B. Mandelbrot, Fractales, hasard et
ﬁnance (Flammarion, Paris, 1997).

[11] R. G¨unther, L. Levitin, B. Schapiro, and P. Wagner, Int. J. Theor. Physics

35, 395 (1996).

[12] G. Troll and P. beim Graben, Phys. Rev. E 57, 1347 (1998).

[13] G. A. Miller and E. B. Newman, Am. J. Psychology 71, 209 (1958).

[14] W. Li, Complexity 3, 10 (1998).

[15] T. P¨oschel, W. Ebeling and H. Ros´e, J. Stat. Phys. 80, 1443 (1995).

[16] To be exact N − d + 1 =

K
k=1 nk, but we use the fact that d ≪ N .

P

9

[17] J. Spanier and K. B. Oldham, An atlas of functions (Springer, Berlin, 1987),

formula 64:9:1.

[18] The term (K/Nmax)1−γ can be large when K ≫ Nmax and γ < 1 but
this divergence becomes eﬀective long after the maximum word size has been
exceeded; it is therefore not a matter of concern here.

10


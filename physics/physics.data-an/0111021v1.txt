1
0
0
2
 
v
o
N
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
2
0
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Entropie en Traitement du Signal
Entropy in Signal Processing

Ali Mohammad-Djafari
Laboratoire des Signaux et Syst`emes (cnrs-sup´elec-ups)
sup´elec
Plateau de Moulon, 91192 Gif-sur-Yvette Cedex, France.

Le principal objet de cette communication est de faire une r´etro perspective succincte de l’utilisation de
l’entropie et du principe du maximum d’entropie dans le domaine du traitement du signal. Apr`es un bref
rappel de quelques d´eﬁnitions et du principe du maximum d’entropie, nous verrons successivement comment
l’entropie est utilis´ee en s´eparation de sources, en mod´elisation de signaux, en analyse spectrale et pour la
r´esolution des probl`emes inverses lin´eaires.

Mots cl´es : Entropie, Entropie crois´ee, Distance de Kullback, Information mutuelle, Estimation spectrale,
Probl`emes inverses

R´esum´e

Abstract

The main object of this work is to give a brief overview of the diﬀerent ways the entropy has been used
in signal and image processing. After a short introduction of diﬀerent quantities related to the entropy and
the maximum entropy principle, we will study their use in diﬀerent ﬁelds of signal processing such as : source
separation, model order selection, spectral estimation and, ﬁnally, general linear inverse problems.

Keywords : Entropy, Relative entropy, Kullback distance, Mutual information, Spectral estimation, Inverse
problems

1

Introduction

En 1945, Shannon [1] a introduit la notion de l’entropie associ´ee `a une source qui est mod´elis´ee
par une variable al´atoire discr`ete X, comme la moyenne de la quantit´e d’information apport´ee par
les r´ealisations de cette variable. Depuis cette date, cette notion a eu un tr`es grand usage dans le
domaine du traitement de l’information et particuli`erement en codage et compression des donn´ees
en t´el´ecommunications.

En 1957, Jaynes [2, 3, 4] a introduit le principe du maximum d’entropie pour l’attribution d’une
loi de probabilit´e `a une variable al´eatoire lorsque la connaissance sur cette variable est incompl`ete.
En 1959, Kullback [5] a introduit une mesure de l’information relative (entropie relative) d’une
loi de probabilit´e par rapport `a une autre. Cette mesure a aussi ´et´e consid´er´ee comme une mesure
de distance entre ces deux lois.

Depuis, ces notions ont eu une inﬂuence importante et un usage ´etendu dans divers domaines
du traitement de l’information, de l’inf´erence en g´en´eral, mais aussi du traitement du signal et des
images.

Le principal objet de ce travail est de fournir une vue synth´etique et br`eve des principaux
usages de ces notions en traitement du signal. Apr`es un rappel de quelques d´eﬁnitions, des relations
importantes entre les diﬀ´erentes quantit´es et l’expos´e du principe du maximum d’entropie, nous
verrons successivement comment l’entropie est utilis´ee en s´eparation de sources, en mod´elisation
de signaux, en analyse spectrale et pour la r´esolution des probl`emes inverses lin´eaires.

2

A. Mohammad-Djafari

1.1 Rappels et d´eﬁnitions

L’entropie associ´ee `a une variable al´eatoire scalaire discr`ete X avec des r´ealisations {x1, · · · , xN }

et la distribution de probabilit´es {p1, · · · , pN } mesure son d´esordre. Elle est d´eﬁnie par

H [X] = −

pi ln pi.

N

i=1
X

H [X] = −

p(x) ln p(x) dx.

Z

Avec quelques pr´ecautions, cette d´eﬁnition peut ˆetre ´etendue au cas d’une variable al´eatoire con-
tinue X avec une densit´e de probabilit´e p(x) par

Par extension, si on consid`ere un couple de variables al´eatoires (X, Θ) avec des lois p(x), p(θ),
p(θ|x), p(x|θ) et p(x, θ), on peut d´eﬁnir les entropies respectivement associ´ees :

– Entropie de Θ :

Z
– Entropie de Θ conditionnellement `a X = x :

H [Θ] = −

p(θ) ln p(θ) dθ

Z
– Entropie de X conditionnellement `a Θ = θ :

H [Θ|x] = −

p(θ|x) ln p(θ|x) dθ

H [X|θ] = −

p(x|θ) ln p(x|θ) dx

Z

ZZ

– Entropie de (X, Θ) :

H [X, Θ] = −

p(x, θ) ln p(x, θ) dx dθ.

Avec ces d´eﬁnitions on d´eﬁnit aussi les quantit´es suivantes :

– Diﬀ´erence entropique de p1 et p2 :

δH [p1, p2] = H [p1] − H [p2]

– Entropie relative de p1 par rapport `a p2 :

Z
– Distance de Kullback de p1 par rapport `a p2 :

D [p1 : p2] = −

p1(x) ln

p1(x)
p2(x)

dx

K [p1 : p2] = −D [p1 : p2] =

p1(x) ln

p1(x)
p2(x)

dx

Z

– Information mutuelle entre Θ et X :

I [Θ, X] = EX {δH [p(θ), p(θ|x)]} = EΘ {δH [p(x), p(x|θ)]}
= H [X] − H [X|Θ] = H [Θ] − H [Θ|X]

o`u

H [Θ|X] = EX {H [Θ|x]} =

H [Θ|x] p(x) dx

H [X|Θ] = EΘ {H [X|θ]} =

H [X|θ] p(θ) dθ

Z

Z

avec les relations suivantes entre ces diﬀ´erentes quantit´es :

H [X, Θ] = H [X] + H [Θ|X] = H [Θ] + H [X|Θ] = H [X] + H [Θ] − I [Θ, X]
(14)
I [Θ, X] = D [p(x, θ) : p(x)p(θ)] = EX {D [p(θ|x) : p(θ)]} = EΘ {D [p(x|θ) : p(x)]} .(15)

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

Entropie en Traitement du Signal

3

(16)

(17)

On peut aussi remarquer les propri´et´es suivantes :

– L’information mutuelle I [Θ, X] est une fonction concave de p(θ) pour p(x|θ) ﬁx´ee et une
fonction convexe de p(x|θ) pour p(θ) ﬁx´ee, et on a I [Θ, X] ≥ 0 avec ´egalit´e si X et Θ sont
ind´ependantes. Cette propri´et´e est utilis´ee en communication pour d´eﬁnir la capacit´e d’un
canal lorsque X est transmis et Θ est re¸cu :

C = arg max
p(θ)

{I [Θ, X]}

– L’entropie relative D [p1 : p2] est invariante par changement d’´echelle mais n’est pas sym´etrique.

C’est pourquoi on introduit

J [p1, p2] = D [p1 : p2] + D [p2 : p1] ,

qui est sym´etrique et invariante par changement d’´echelle, comme une mesure de divergence
entre p1(x) et p2(x).

– La puissance entropique (PE) d’une loi p(x) est d´eﬁnie comme la variance d’une loi gaussienne
2 ln(2πeσ2), on

ayant la mˆeme entropie. En notant que l’entropie d’une loi gaussienne est 1
obtient

PE [p] = exp

2 [H [p] −

ln(2πe)]

= exp [2 δH [p, N (0, 1)]] .

(18)

1
2

(cid:20)

(cid:21)

PE [p] est une mesure de proximit´e de p `a une densit´e gaussienne r´eduite.

1.2 Lien entre entropie et vraisemblance

Consid´erons le probl`eme de l’estimation des param`etres θ d’une loi de probabilit´e p(x|θ) `a

partir d’un n-´echantillon x = {x1, · · · , xn}. La log-vraisemblance de θ est d´eﬁnie par

n

L(θ) =

ln p(x|θ).

(19)

i=1
X
Maximiser L(θ) par rapport `a θ donne l’estimation au sens du maximum du vraisemblance (MV).
Notons que L(θ) d´epend de n, c’est pourquoi on peut s’int´eresser `a 1

n L(θ) et d´eﬁnir

¯L(θ) = lim
n7→∞

1
n

L(θ) = E { ln p(x|θ)} =

p(x|θ∗) ln p(x|θ) dx,

(20)

o`u θ∗ est le pr´esum´e “vrai” vecteur des param`etres et p(x|θ∗) la loi de probabilit´e correspondante.
On peut alors noter que

D [p(x|θ∗) : p(x|θ)] = −

p(x|θ∗) ln

dx =

p(x|θ∗) ln p(x|θ∗) dx + ¯L(θ)

(21)

p(x|θ)
p(x|θ∗)

et que

Z

arg max
θ

{D [p(x|θ∗) : p(x|θ)]} = arg max

¯L(θ)

.

(cid:8)

(cid:9)

1.3 Lien entre entropie et la matrice d’information de Fisher

On consid`ere D [p(x|θ∗) : p(x|θ∗ + ∆θ)] et suppose que ln p(x|θ) est d´eveloppable en s´erie de

Taylor. En ne gardant que les termes jusqu’`a l’ordre deux, on obtient

o`u F est la matrice d’information de Fisher :

D [p(x|θ∗) : p(x|θ∗ + ∆θ)] ≃

∆θtF ∆θ.

F = E

∂2
∂θt∂θ

(

ln p(x|θ)|θ=θ∗

.

)

(22)

(23)

Z

Z

θ

1
2

4

A. Mohammad-Djafari

1.4 Cas d’un vecteur ou d’un processus al´eatoire

Toutes ces d´eﬁnitions sont facilement ´etendues au cas d’un vecteur al´eatoire ou d’un processus
al´eatoire stationnaire. Par exemple, il est facile de montrer que l’entropie d’un vecteur al´eatoire de
dimension n avec une densit´e gaussienne N (0, R) est

(24)

(25)

(26)

(27)

et que l’entropie relative entre deux lois gaussiennes N (0, R) et N (0, S) est

H =

ln(2π) +

ln(|d´et (R) |)

n
2

1
2

D = −

1
2 (cid:18)

tr

RS−1
(cid:16)

(cid:17)

− log

|d´et (R) |
|d´et (S) |

− n

.

(cid:19)

lim
n−→∞

1
n

1
2π

π

−π

Z

H(p) =

ln S(ω) dω

De mˆeme, on montre que pour un processus al´eatoire stationnaire et gaussien dont la matrice de
covariance est Toeplitz, on a

o`u S(ω) est sa densit´e spectrale de puissance (dsp), et pour deux processus stationnaires et gaussiens
de densit´es spectrales de puissance S1(ω) et S2(ω), on a

lim
n−→∞

1
n

D(p1 : p2) =

1
4π

π

−π (cid:18)

Z

S1(ω)
S2(ω)

− ln

− 1

dω

S1(ω)
S2(ω)

(cid:19)

et on retrouve la distance de Itakura-Saito [6, 7, 8] en analyse spectrale.

1.5 Principe du maximum d’entropie (PME)

Lorsqu’on doit attribuer une loi de probabilit´e `a une variable X sur laquelle on a une information
partielle, il est pr´ef´erable de choisir la loi d’entropie maximale parmi toutes les lois compatibles
avec cette information. La loi ainsi choisie est la moins compromettante au sens qu’elle ne contient
que l’information disponible (elle n’introduit pas d’information suppl´ementaire).

En termes math´ematiques, consid´erons la variable X et supposons que l’information disponible

sur X s’´ecrit

(28)
o`u φk sont des fonctions quelconques. ´Evidemment, il existe une inﬁnit´e de lois p(x) qui satisfont
ces contraintes. Alors le PME s’´ecrit

E {φk(X)} = dk,

k = 1, . . . , K.

p(x) = arg max

H [p] = −

p(x) ln p(x) dx

(29)

p∈P (cid:26)

Z

o`u

b

P =

p(x) :

φk(x)p(x) dx = dk,

k = 0, . . . , K

(cid:26)
avec φ0 = 1 et d0 = 1 pour la contrainte de normalisation.

Z

(cid:27)

(cid:27)

Sachant que H [p] est une fonction concave de p et que les contraintes (28) sont lin´eaires en p,

la solution s’´ecrit

o`u Z(λ) est la fonction de partition Z(λ) =

exp[−

b

p(x) =

exp

−

λkφk(x)

(30)

"

#

Xk=1
K
k=1 λkφk(x)] dx et λ = [λ1, . . . , λK]t v´eriﬁe

1
Z(λ)

Z

φk(x) exp

−

λkφk(x)

= dk,

k = 1, . . . , K.

(31)

1
Z(λ)

Z
K

"

Xk=1

K

P

#

5

(32)

Entropie en Traitement du Signal

La valeur maximale de l’entropie est

Hmax = ln Z(λ) + λty.

Le probl`eme d’optimisation (29) s’´etend facilement en rempla¸cant l’entropie H(p) par l’entropie
relative D[p : q] o`u q(x) est une loi a priori . Pour plus de d´eveloppements sur ce sujet on peut se
r´ef´erer `a [9, 5, 10, 11] et `a [12, 13, 14, 15].

2 Entropie en s´eparation de sources

Le mod`ele le plus simple en s´eparation de sources est x = A s o`u, s est le vecteur sources,
x est le vecteur des mesures et A est la matrice du m´elange, suppos´ee inversible en g´en´eral. Le
probl`eme est souvent pos´e comme celui de l’estimation d’une matrice de s´eparation B = A−1
ou B = Σ Λ A−1. Σ est une matrice de permutation d’indices et Λ une matrice diagonale, de
telle sorte que les composantes du vecteur y = Bx soient ind´ependantes. La notion d’entropie est
utilis´ee `a ce niveau comme un outil pour assurer cette ind´ependance. D’une mani`ere plus g´en´erale,
consid´erons un traitement de la forme yi = g([Bx]i) o`u g est une fonction monotone et croissante.
On a alors

pY (y) =

pX(x) −→ H(y) = −E {ln pY (y)} = E {ln |∂y/∂x|} − H(x).

(33)

1
|∂y/∂x|

H(y) est utilis´ee comme une mesure de l’ind´ependance des composantes du vecteur y et on estime
alors la matrice de s´eparation B en maximisant H(y) par rapport aux ´el´ements de cette matrice.
`A titre de comparaison, on note que l’estimation de B au sens du maximum de vraisemblance
s’obtient en maximisant

V (B) =

ln pi ([Bx]i) − log |d´et (B) |

(34)

lorsque les sources si sont suppos´ees ind´ependantes avec pi(si) connues.

i
X

3 Entropie en mod´elisation de signaux

L’identiﬁcation de l’ordre d’un mod`ele en traitement du signal est un sujet primordial et en-
core ouvert. Lorsque l’ordre du mod`ele (dimension du vecteur param`etre θ) est ﬁx´e, l’estimation
d’une valeur optimale (au sens du maximum du vraisemblance, du maximum a posteriori (MAP)
ou d’autres estimateurs bay´esiens) est bien ´etablie, mais la d´etermination de l’ordre du mod`ele
est encore mati`ere `a discussion. Parmi les outils utilis´es, on peut mentionner l’entropie, ou plus
exactement D [p(x|θ∗) : p(x|θ)], o`u θ∗ repr´esente le vrai vecteur des param`etres de dimension k∗
et θ le vecteur estim´e de dimension k ≤ k∗. Le fameux crit`ere d’Akaike [16, 17, 18, 19, 20] utilise
ainsi cette quantit´e pour d´eterminer l’ordre optimal du mod`ele dans le cadre sp´eciﬁque des mod`eles
lin´eaires (en les param`etres), des lois gaussiennes et de l’estimation au sens du MV [21].

4 Entropie en analyse spectrale

L’entropie est utilis´ee de multiples fa¸cons en analyse spectrale. La pr´esentation classique de

Burg [22] se r´esume ainsi :
Soit X(n) un processus al´eatoire centr´e et stationnaire, dont nous disposons d’un nombre ﬁni
d’´echantillons de la fonction d’autocorr´elation
1
2π

r(k) = E {X(n)X(n + k)} =

S(ω) exp [jkω] dω,

k = 0, . . . , K.

(35)

π

La question est d’estimer la densit´e spectrale de puissance

−π

Z

∞

Xk=−∞

S(ω) =

r(k) exp [−jkω]

6

A. Mohammad-Djafari

de ce processus. Consid´erons maintenant le probl`eme de l’attribution d’une loi de probabilit´e p(x) au
vecteur X = [X(0), . . . , X(N −1)]t. Utilisant le PME et en remarquant que les contraintes (35) sont
quadratiques en X, on obtient une loi gaussienne pour X. Pour un processus centr´e, stationnaire
et gaussien, lorsque le nombre d’´echantillons N −→ ∞, l’expression de l’entropie devient

On cherche alors `a maximiser H sous les contraintes (35). La solution est bien connue :

H =

ln S(ω) dω.

π

−π

Z

S(ω) =

K

1

2 ,

(cid:12)
(cid:12)
Xk=−K
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λk exp [jkω]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

S(ω) =

δ Γ−1δ
e Γ−1e

,

o`u λ = [λ0, · · · , λK]t, les multiplicateurs de Lagrange associ´es aux contraintes (35), sont ici ´equivalents
aux coeﬃcients d’une mod´elisation AR du processus X(n). Notons que dans ce cas particulier, il y
a une expression analytique pour λ, ce qui permet de donner une expression analytique directe de
S(ω) en fonction des donn´ees {r(k), k = 0, · · · , K} :

o`u Γ = Toeplitz(r(0), · · · , r(K)) est la matrice de corr´elation des donn´ees et δ et e sont deux
vecteurs d´eﬁnis par δ = [1, 0, · · · , 0]t et e = [1, e−jω, e−j2ω, · · · , e−jKω]t.

Notons que nous avons utilis´e le PME pour choisir une loi de probabilit´e pour le processus
X(n). Ainsi la densit´e spectrale de puissance estim´ee dans cette approche correspond `a la densit´e
spectrale de puissance du processus le plus d´esordonn´e (le plus informatif !) qui soit compatible
avec les donn´ees (35).

Une autre approche consiste `a maximiser l’entropie relative D [p(x) : p0(x)] ou minimiser la
distance de Kullback K [p(x) : p0(x)] o`u p0(x) est une loi a priori sous les mˆeme contraintes. Le
choix de cette loi est alors primordial. ´Evidemment, en choisissant p0(x) uniforme, on retrouve le
cas pr´ec´edent, mais si on choisit une loi gaussienne pour p0(x), l’expression `a maximiser devient

D [p(x) : p0(x)] =

1
4π

π

−π (cid:18)
Z

S(ω)
S0(ω)

− ln

− 1

dω

S(ω)
S0(ω)

(cid:19)

lorsque N 7→ ∞, et o`u S0(ω) correspond `a la densit´e spectrale de puissance d’un processus de
r´ef´erence avec la loi p0(x).

Une autre approche consiste `a d´ecomposer le processus X(n) sur une base de Fourier

{cos kωt, sin kωt} et consid´erer ω comme une variable al´eatoire et S(ω), une fois normalis´e, comme
une loi de probabilit´e. On d´ecrit alors le probl`eme de la d´etermination de S(ω) comme celui de la
maximisation de

sous les contraintes lin´eaires (35). La solution est de la forme

−

S(ω) ln S(ω) dω

π

−π

Z

S(ω) = exp

λk exp [jkω]

.

K





Xk=−K





La densit´e spectrale de puissance estim´ee dans cette approche correspond `a la densit´e spectrale de
puissance la plus uniforme du processus qui est compatible avec les donn´ees (35).

Une troisi`eme approche consiste `a consid´erer S(ω) (`a ω ﬁx´e) comme la moyenne d’une variable
al´eatoire Z(ω) pour laquelle nous supposons disposer d’une loi a priori µ(z). On cherche ensuite la

(36)

(37)

(38)

(39)

(40)

(41)

Entropie en Traitement du Signal

loi p(z) qui maximise D(p(z); µ(z)) sous les contraintes (35). Une fois p(z) d´etermin´ee, on d´eﬁnit
la solution par

S(ω) = E {Z(ω)} =

Z(ω)p(z) dz.

Z

Il est alors int´eressant de voir que l’expression de S(ω) d´epend du choix de la loi a priori µ(z) (voir
paragraphe 5). Lorsqu’on choisit pour µ(z) une loi gaussienne (sur IR) on obtient

H =

S2(ω) dω,

π

−π

Z

alors que si on choisit une loi de Poisson (sur IR+), on retrouve l’expression de l’entropie (40).
Finalement, si on choisit une mesure de Lebesgue sur [0, ∞], on obtient l’expression de l’entropie
(36). Voir aussi : [22, 23, 24, 25, 26, 27, 15].

5 Entropie pour la r´esolution des probl`emes inverses lin´eaires

Lorsqu’on cherche `a r´esoudre un probl`eme inverse lin´eaire num´eriquement, on est rapidement

amen´e `a chercher une solution

x pour l’´equation

b

y = Ax,

(44)

o`u A est une matrice de dimensions (M × N ), en g´en´eral singuli`ere ou tr`es mal conditionn´ee. Bien
que les cas M > N ou M = N aient les mˆemes diﬃcult´es que le cas M < N , nous consid´erons
seulement ce deuxi`eme cas pour plus de clart´e. Dans ce cas, `a l’´evidence, soit le probl`eme n’a pas
de solution, soit il en poss`ede une inﬁnit´e. Nous nous pla¸cerons dans ce dernier cas o`u la question
est de choisir une seule solution.

Parmi les diﬀ´erentes m´ethodes, on peut noter l’utilisation de la norme kxk2 pour ce choix — la

solution de norme minimale :

xN M = arg max

Ω(x) = kxk2

= At(AAt)−1y.

{x : y=Ax} n

o

b

Mais, ce choix permettant d’obtenir une unique solution `a ce probl`eme n’est pas le seul possible.
En eﬀet, tout crit`ere Ω(x) qui est convexe en x peut ˆetre utilis´e. On peut mentionner en particulier

Ω(x) = −

xj ln xj

Xj

lorsque les xj sont positifs et lorsque
xj = 1, ce qui, par analogie avec la d´eﬁnition de l’entropie,
assimile les xj `a une distribution de probabilit´e xj = P (U = uj). La variable al´eatoire U peut ou
non avoir une r´ealit´e physique. Ω(x) est alors l’entropie associ´ee `a cette variable.

P

Une autre approche consiste `a supposer xj = E {Uj} ou encore x = E {U } o`u U est un vecteur
al´eatoire, qui peut ou non avoir une r´ealit´e physique. Supposons maintenant que U admet une loi
p(u) que l’on cherche `a d´eterminer. En notant que les donn´ees y = Ax = AE {U } =
de probabilit´e
E {AU } peuvent ˆetre consid´er´ees comme des contraintes lin´eaires sur cette loi, on peut utiliser de
nouveau l’entropie pour d´eterminer la loi

p(u) :

b

p(u) =

arg max

b
Au p(u) du}

{x : y=

{D[p(u) : µ(u)]}

o`u µ(u) est une loi a priori dont nous montrerons par la suite l’importance. La solution est bien
connue :

R

b

p(u) =

µ(u) exp

1
Z(λ)

−λtAu
h

i

b

7

(42)

(43)

(45)

(46)

(47)

(48)

A. Mohammad-Djafari

8

b

mais le plus int´eressant est de voir ce que devient
Le tableau qui suit donne quelques exemples :

x = E {U }. Bien ´evidemment,

x d´epend de µ(u).

µ(u) ∝ exp[− 1
2
µ(u) ∝ exp[−
µ(u) ∝ exp[−

j u2
j ]
j |uj|]
P
j uα−1
j

P
P

exp [−βuj]],

uj > 0

b

b

AAtλ = y
x = Atλ
x = 1./(Atλ ± 1)
Ax = y
x = α1./(Atλ + β1) Ax = y
b
b
b

µ(u) exp

Dans le cas plus g´en´eral, rempla¸cant (48) dans (47) et d´eﬁnissant Z(λ) =

−λtAu

du,

µ(u) exp

ZZ
du et sa convexe conjug´ee F (x) = sups
x = E {U } peut ˆetre obtenu, soit comme une fonction de son vecteur dual

xts − G(s)

−stu

(cid:9)

(cid:8)

(cid:2)

(cid:3)

(cid:2)

(cid:3)
, on peut

λ est solution du probl`eme d’optimisation

G(s) = ln

ZZ
montrer que
x = G′(At

λ) o`u
b

b

b

soit directement comme la solution du probl`eme d’optimisation sous contraintes

b

λ = arg min

D(λ) = ln Z(λ) + λty
n

o

,

λ par

b

(49)

(50)

λ

b

x = arg min

{F (x)} .

{x : Ax=y}

D(λ) est appel´e “crit`ere dual” et F (x) “crit`ere primal”. Parfois, il est plus facile de r´esoudre le
probl`eme dual, mais il n’est pas toujours possible d’obtenir une expression explicite pour G(s) et
son gradient G′(s). Les fonctions F (x) et G(s) sont convexes conjugu´ees.

6 Conclusions

La notion d’entropie, vue comme une mesure de la quantit´e d’information dans les r´ealisations
d’une variable al´eatoire est utilis´ee de multiples fa¸cons dans diﬀ´erents domaines du traitement de
l’information. Lors de son utilisation, il est tr`es important de bien pr´eciser quelle est la variable
consid´er´ee, quelles sont les donn´ees, quelle est la relation entre les donn´ees et cette variable, et
ﬁnalement, quelle est le crit`ere optimis´e. Par exemple, en estimation spectrale, nous avons vu
comment le choix de la variable al´eatoire (X(n), S(ω) ou Z(ω)), le choix du crit`ere (entropie ou
entropie relative) et le choix de la loi a priori dans le cas de l’entropie relative, peuvent inﬂuencer
l’expression de la solution. Bien entendu, nous n’avons pas discut´e ici le probl`eme de l’estimation
des coeﬃcients de corr´elation `a partir des ´echantillons du signal. Par ailleurs, l’estimation de la
densit´e spectrale de puissance d’un processus `a partir d’une connaissance partielle de ses coeﬃcients
de corr´elation n’est qu’un cas particulier des probl`emes inverses lin´eaires.

L’auteur remercie Odile Macchi et Charles Soussen pour la relecture attentive de cet article.

Remerciements

Biographie de l’auteur

Ali Mohammad-Djafari est n´e en Iran en 1952. Il est Ing´enieur de l’´Ecole Polytechnique de
T´eh´eran (1975), Ing´enieur de l’´Ecole Sup´erieure d’´Electricit´e (1977), Docteur-Ing´enieur (1981) et
Docteur-`es-Sciences Physiques (1987) de l’Universit´e de Paris-Sud, Orsay. Il travaille depuis 1977
au Laboratoire des Signaux et Syst`emes au sein du groupe “Probl`emes Inverses en Traitement du
Signal et Imagerie”. Charg´e de Recherche au CNRS depuis 1983, il s’int´eresse `a la r´esolution des
probl`emes inverses en utilisant des m´ethodes d’inf´erence probabilistes. Parmi les applications de
ses th`emes de recherche on peut mentionner : restauration et reconstruction des signaux mono-
ou multi- variables, imagerie tomographique `a rayons X, `a ondes diﬀract´ees ou par courants de
Foucault en contrˆole non destructif (CND).

Entropie en Traitement du Signal

9

R´ef´erences

[1] C. Shannon and W. Weaver, “The mathematical theory of communication,” Bell System Technical

Journal, vol. 27, pp. 379–423, 623–656, 1948.

[2] E. T. Jaynes, “Information theory and statistical mechanics i,” Physical review, vol. 106, pp. 620–630,

[3] E. T. Jaynes, “Information theory and statistical mechanics ii,” Physical review, vol. 108, pp. 171–190,

[4] E. T. Jaynes, “Prior probabilities,” IEEE Transactions on Systems Science and Cybernetics, vol. SSC-4,

pp. 227–241, September 1968.

[5] S. Kullback, Information Theory and Statistics. New York : Wiley, 1959.
[6] Itakura and Saito, “A statistical method for estimation of speech spectral density and formant frequen-

cies,” Electron. and Commun., vol. 53-A, pp. 36–43, 1970.

[7] L. Knockaert, “A class of statistical and spectral distance measures based on Bose-Einstein statistics,”

IEEE Transactions on Signal Processing, vol. 41, no. 11, pp. 3171–3174, 1963.

[8] M. Schroeder, “Linear prediction, entropy and signal analysis,” IEEE ASSP Magazine, pp. 3–11, juillet

1957.

1957.

1984.

[9] E. T. Jaynes, “On the rationale of maximum-entropy methods,” Proceedings of the IEEE, vol. 70,

pp. 939–952, September 1982.

[10] J. Shore and R. Johnson, “Axiomatic derivation of the principle of maximum entropy and the principle
of minimum cross-entropy,” IEEE Transactions on Information Theory, vol. IT-26, pp. 26–37, January
1980.

[11] J. E. Shore and R. W. Johnson, “Properties of cross-entropy minimization,” IEEE Transactions on

Information Theory, vol. IT-27, pp. 472–482, July 1981.

[12] A. Mohammad-Djafari, “Maximum d’entropie et probl`emes inverses en imagerie,” Traitement du Signal,

pp. 87–116, 1994.

[13] J.-F. Bercher, D´eveloppement de crit`eres de nature entropique pour la r´esolution des probl`emes inverses

lin´eaires. Th`ese de Doctorat, Universit´e de Paris-Sud, Orsay, f´evrier 1995.

[14] G. Le Besnerais, M´ethode du maximum d’entropie sur la moyenne, crit`eres de reconstruction d’image et
synth`ese d’ouverture en radio-astronomie. Th`ese de Doctorat, Universit´e de Paris-Sud, Orsay, d´ecembre
1993.

[15] J. Borwein and A. Lewis, “Duality relationships for entropy-like minimization problems,” SIAM Journal

of Control, vol. 29, pp. 325–338, March 1991.

[16] H. Akaike, “Power spectrum estimation through autoregressive model ﬁtting,” Annals of Institute of

Statistical Mathematics, vol. 21, pp. 407–419, 1969. JFG.

[17] H. Akaike, “A new look at the statistical model identiﬁcation,” IEEE Transactions on Automatic and

Control, vol. AC-19, pp. 716–723, December 1974. JFG.

[18] D. Farrier, “Jaynes’ principle and maximum entropy spectral estimation,” IEEE Transactions on Acous-

tics Speech and Signal Processing, vol. ASSP-32, pp. 1176–1183, 1984.

[19] M. Wax and T. Kailath, “Detection of signals by information theoretic criteria,” IEEE Transactions on

Acoustics Speech and Signal Processing, vol. 33, pp. 387–392, avril 1985.

[20] M. Wax, “Detection and localization of multiple sources via the stochastic signals model,” IEEE Trans-

actions on Signal Processing, vol. SP-39, pp. 2450–2456, novembre 1991. FD.

[21] T. Matsuoka and U. T.J., “Information theory measures with application to model identiﬁcation,” IEEE

Transactions on Acoustics Speech and Signal Processing, vol. 34, pp. 511–517, juin 1986.

[22] J. P. Burg, “Maximum entropy spectral analysis,” in Proc. of the 37th Meeting of the Society of Explo-

ration Geophysicists, (Oklahoma City), pp. 34–41, October 1967.

[23] J. E. Shore, “Minimum cross-entropy spectral analysis,” IEEE Transactions on Acoustics Speech and

Signal Processing, vol. ASSP-29, pp. 230–237, April 1981.

[24] R. Johnson and J. Shore, “Minimum cross-entropy spectral analysis,” IEEE Transactions on Acoustics

Speech and Signal Processing, vol. ASSP-29, pp. 230–237, avril 1981.

[25] J. McClellan, “Multidimensional spectral estimation,” Proceedings of the IEEE, vol. 70, pp. 1029–1039,

septembre 1982.

10

A. Mohammad-Djafari

[26] R. Johnson and J. Shore, “Which is better entropy expression for speech processing : -slogs or logs ?,”
IEEE Transactions on Acoustics Speech and Signal Processing, vol. ASSP-32, pp. 129–137, 1984.
[27] B. Picinbono and Barret, “Nouvelle pr´esentation de la m´ethode du maximum d’entropie,” Traitement

du Signal, vol. 7, no. 2, pp. 153–158, 1990.


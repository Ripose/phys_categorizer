4
0
0
2
 
r
a

M
 
7
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
6
8
0
3
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Asymmetric Uncertainties:
Sources, Treatment and Potential Dangers

G. D’Agostini
Universit`a “La Sapienza” and INFN, Roma, Italia
(giulio.dagostini@roma1.infn.it, www.roma1.infn.it/˜dagos)

Abstract

The issue of asymmetric uncertainties resulting from ﬁts, nonlinear
propagation and systematic eﬀects is reviewed. It is shown that, in
all cases, whenever a published result is given with asymmetric uncer-
tainties, the value of the physical quantity of interest is biased with
respect to what would be obtained using at best all experimental and
theoretical information that contribute to evaluate the combined un-
certainty. The probabilistic solution to the problem is provided both
in exact and in approximated forms.

1

Introduction

We often see published results in the form

‘best value’ +∆+
−∆− .

As ﬁrst pointed out in Ref. [1] and discussed in a simpler but more com-
prehensive way in Ref. [2], this practice is far from being acceptable and,
indeed, could bias the believed value of important physics quantities. The
purpose of the present paper is, summarizing and somewhat completing the
work done in the above references, to remind where asymmetric uncertainty
stem from and to show why, as they are usually treated, they bias the value
of physical quantities, either in the published result itself or in subsequent
analyses. Once the problems are spotted, the remedy is straightforward, at
least within the Bayesian framework (see e.g. [2], or [3] and [4] for recent re-
views). In fact the Bayesian approach is conceptually based on the intuitive
idea of probability, and formally grounded on the basic rules of probability
(what are usually known as the probability ‘axioms’ and the ‘conditional

1

probability deﬁnition’) plus logic. Within this framework many methods of
‘conventional’ statistics are reobtained, as approximations of general solu-
tions, under well stated conditions of validity. Instead, in the conventional,
frequentistic approach ad hoc formulae, prescriptions and un-needed princi-
ples are used, often without understanding what is behind these methods –
before a ‘principle’ there is nothing!

The proposed Bayesian solutions to cure the troubles produced by the
usual treatment asymmetric uncertainties is to step up from approximated
methods to the more general ones (see e.g. Ref. [2], in particular the top
down approximation diagram of Fig. 2.2). In this paper we shall see, for
example, how χ2 and minus log-likelihood ﬁts can be derived from as ap-
proximates methods and what to do when the underlying conditions do not
hold. We shall encounter a similar situation regarding standard formulae to
propagate uncertainty.

Some of the issues addressed here and in Refs.

[1] and [2] have been
recently brought to our attention by Roger Barlow [5], who proposes fre-
quentistic ways out. The reader is encoraged to read also these references
to form his/her idea about the spotted problem and the proposed solutions.
In Sect. 2 the issue of propagation of uncertainty is brieﬂy reviewed at
an elementary level (just focusing on the sum of a few uncertain indepen-
dent variables – i.e. no correlations considered) but taking into account the
asymmetry probability density functions (p.d.f.) of the input quantities. In
this way one understands what ‘might have been done’ (we are rarely in the
positions to exactly know “what has been done”) by the authors who pub-
lish asymmetric results and the danger of using improperly the published
‘best value’ in subsequent analyses. Then, we shall see in Sect. 3 where
asymmetric uncertainties stem from and what to do in order to overcome
their potential troubles. This will be done in an exact way and, when-
ever is possible, in an approximated way. Some rules of thumb to roughly
recover sensible probabilistic quantities (expected value and standard devi-
ation) from results published with asymmetric uncertainties will be given in
Sect. 4. Finally, some conclusions will be drawn.

2 Propagating uncertainty

Determining the value of a physics quantity is seldom an end in itself. In
most cases the result is used, together with other experimental and the-
oretical quantities, to calculate other quantities of interest. As it is well
understood, uncertainty on the value of each ingredient is propagated into

2

uncertainty on the ﬁnal result.

If uncertainty is quantiﬁed by probability, as it is commonly done explic-
itly or implicitly1 in physics, the propagation of uncertainty is performed
using rules based on probability theory. If we indicate by X the set (‘vec-
tor’) of input quantities and by Y the ﬁnal quantity, given by the function
y = Y (x) of the input quantities, the most general propagation formula[2]
is given by (we stick to continuous variables):

f (y) =

δ[y

Y (x)]

f (x) dx ,

−

·

Z

where f (y) is the p.d.f. of Y , f (x) stands for the joint p.d.f. of X and δ is the
Dirac delta (note the use of capital letters to name variables and small letters
to indicate the values that variables may assume). The exact evaluation of
Eq. (1) is often challenging, but, as discussed in Ref. [2], this formula has
a nice simple interpretation that makes its Monte Carlo implementation
conceptually easy.

As it is also well known, often one is not needed to go through the
analytic, numerical or Monte Carlo evaluation of Eq.(1), since linearization
of Y (x) around the expected value of X (E[X]) can make the calculation of
expected value and variance of Y very easy, using the well known standard
propagation formulae, that for uncorrelated input quantities are

E[Y ]

Y (E[x])

σ2(Y )

≈

≈

Xi  

E[x]!

∂Y
∂Xi (cid:12)
(cid:12)
(cid:12)
(cid:12)

2

σ2(Xi) .

As far as the shape of f (y), a Gaussian one is usually assumed, as a result
of the central limit theorem. Holding this assumptions, E[Y ] and σ(Y ) is
all what we need. E[Y ] gives the ‘best value’, and probability intervals,
upper/lower limits and so on can be easily calculated. In particular, within
the Gaussian approximation, the most believable value (mode), the barycen-
ter of the p.d.f. (expected value) and the value that separates two adjacent
50% probability intervals (median) coincide. If f (y) is asymmetric this is
not any longer true and one needs then to clarify what ‘best value’ means,
which could be one of the above three position parameters, or something

1Perhaps the reader would be surprised to learn that in the conventional statistical
approach there is no room for probabilistic statements about the value of physics quantities
(e.g. “the top mass is between 170 and 180 GeV with such percent probability”, or “there
is 95% probability that the Higgs mass is lighter than 200 GeV”), calibration constants,
and so on, as discussed extensively in Ref. [6].

(1)

(2)

(3)

3

E(X)
σ(X)
mode
median = 0.23

= 0.17
= 0.42
= 0.5

= 0.34
E(Y )
= 0.59
σ(Y )
mode
= 0.45
median = 0.37

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

-1

1

-2

-1

1

2

2

×

=

⇒

Figure 1: Distribution of the sum of two independent quantities, each described
by an asymmetric triangular p.d.f. self-deﬁned in the left plot. The resulting p.d.f.
(right plot) has been calculated analytically making use of Eq.(1). This ﬁgure
corresponds to Fig. 4.3 of Ref. [2].

else (in the Bayesian approach ‘best value’ stands for expected value, unless
diﬀerently speciﬁed).

Anyhow, Gaussian approximation is not the main issue here and, in
most real applications, characterized by several contributions to the overall
uncertainty, this approximation is a reasonable one, even when some of the
input quantities individually contribute asymmetrically. My concerns in this
paper are more related to the evaluation of E[Y ] and σ(Y ) when

1. instead of Eqs. (2)–(3), ad hoc propagation prescriptions are used in

presence of asymmetric uncertainties;

2. linearization implicit in Eqs. (2)–(3) is not a good approximation.

Let us start with the ﬁrst point, considering, as an easy academic example,
input quantities described by the asymmetric triangular distribution shown
in the left plot of Fig. 1. The value of X can range between
1 and 1, with a
‘best value’, in the sense of maximum probability value, of 0.5. The interval
0.52, 0.84] gives a 68.3% probability interval, and the ‘result’ could be
[
−
reported as X1 = 0.50+0.34
−1.02. This is not a problem as long as we known what
this notation means and, possibly, know the shape of f (x). The problem

−

4

arises when we want to make use of this result and we do not have access to
f (x) (as it is often the case), or we make improper use of the information
[i.e. in the case we are aware of f (x)]. Let us assume, for simplicity, to have
a second independent quantity, X2, described exactly by the same p.d.f. and
reported in the same way: X2 = 0.50+0.34
−1.02. Imagine we are now interested
to the quantity Y = X1 + X2. How to report the result about Y , based on
the results about Y1 and Y2? Here are some common, but wrong ways to
give the result:

asymmetric uncertainties added in quadrature: Y = 1.00+0.48
−1.44;

asymmetric uncertainties added linearly: Y = 1.00+0.68
−2.04.

•

•

Indeed, in this simple case we can calculate the integral (1) analytically,
obtaining the curve shown in the plot on the right side of Fig. 1, where
several position and shape parameters have been reported. The ‘best value’
of Y , meant as expected value (i.e.
the barycenter of the p.d.f.) comes
out to be 0.34. Even those who like to think at the ‘best value’ as the
value of maximum probability (density) would choose 0.45 (note that in
this particular example the mode of the sum is smaller than the mode of
each addend!). Instead, a ‘best value’ of Y of 1.00 obtained by the ad hoc
rules, unfortunately often used in physics, corresponds neither to the mode,
nor to the expected value or the median.

The situation would have been much better if expected value and stan-
dard deviation of X1 and X2 had been reported (respectively 0.17 and 0.42).
Indeed, these are the quantities that matter in ‘error propagation’, because
the theorems upon which propagation formulae rely — exactly in the case Y
is a linear combination of X, or approximately in the case linearization has
been performed — speak of expected values and variances. It is easy to verify
from the numbers in Fig. 1 that exactly the correct values of E[Y ] = 0.34
and σ(Y ) = 0.59 would have been obtained. Moreover, one can see that ex-
pected value, mode and median of f (y) do not diﬀer much from each other,
and the shape of f (y) resembles a somewhat skewed Gaussian. When Y
will be combined with other quantities in a next analysis its slightly non-
Gaussian shape will not matter any longer. Note that we have achieved this
nice result already with only two input quantities. If we had a few more,
already Y would have been much Gaussian-like. Instead, performing a bad
combination of several quantities all skewed in the same side would yield
‘divergent’ results2: for n = 10 we get, using a quadratic combination of left
and right deviations, Y = 5.00+1.08

−3.23 versus the correct Y = 1.70

1.32.

2The reader might be curious to know what would happen in case of bad combinations

±

5

As conclusion from this section I would like to make some points:

•

•

in case of asymmetric uncertainty on a quantity, it should be avoided
to report only most probable value and a probability interval (be it
68.3%, 95%, or what else);

expected value, meant as barycenter of the distribution, as well as stan-
dard deviations should always be reported, providing also the shape
of the distribution (or its summary in terms of shape parameters, or
even a parameterization of the log-likelihood function in a polynomial
form, as done e.g. in Ref. [7], if the distribution is asymmetric or non
trivial).

Note that the propagation example shown here is the most elementary pos-
sible. The situation gets more complicate if also nonlinear propagation is
involved (see Sect. 3.2) or when quantities are used in ﬁts (see e.g. Sect.
12.1 of Ref. [2]).

Hoping that the reader is, at this point, at least worried about the eﬀects
of badly treated asymmetric uncertainties, let us now move to review the
sources of asymmetric uncertainties.

3 Sources of asymmetric uncertainties

3.1 Non parabolic χ2 or log-likelihood curves

−

The standard methods in physics to adjust theoretical parameters to exper-
imental data are based on maximum likelihood principle ideas. In practice,
depending on the situation, the ‘minus log-likelihood’ of the parameters
[ϕ(θ; data) =
ln L(θ; data)] or the χ2 function of the parameters [i.e. the
function χ2(θ; data)] are minimized. The notation used reminds that ϕ and
χ2 are seen as mathematical function of the parameters θ, with the data
acting as ‘parameters’ of the functions. As it is well understood, ϕ and χ2
diﬀer by just a factor two when the likelihood, seen as a joint probability
function or a p.d.f. of the data, is a (multivariate) Gaussian distribution of
the data: ϕ = χ2/2. For sake of simplicity, let us take one ﬁt parameter, θ

of input quantities with skewness of mixed signs. Clearly there will be some compensation
that lowers the risk of strong bias. As an academic exercise, let think of ﬁve independent
quantities each described by the triangular distribution of Fig. 1 and ﬁve others each
described by a p.d.f. which is its mirror reﬂexed around x = 0.5 [0 ≤ X ≤ 2, mode(X) =
0.5, E[X] = 0.83 and σ(X) = 0.42]. The correct combination gives Y = 5.00 ± 1.33,
while adding the modes and combining quadratically left and right deviations we would
get 5.00 ± 2.40.

6

(we use the usual practice of indicating the parameter by θ, though this ﬁt
parameter is just any of the input quantities X of Sect. 2).

If ϕ(θ) or χ2(θ) have a nice parabolic shape, the likelihood is, apart a
multiplicative factor, a Gaussian function3 of θ. In fact, as is well known
from calculus, any function can be approximated to a parabola in the vicin-
ity of its minimum. Let us see in detail the expansion of ϕ(θ) around its
minimum θm:

ϕ(θ)

ϕ(θm) +

≈

≈

ϕ(θm) +

(θ

θm

∂ϕ
∂θ (cid:12)
(cid:12)
1
1
(cid:12)
(cid:12)
α2 (θ
2

−

θm) +

−

θm)2 ,

∂2ϕ
∂θ2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

(θ

−

θm)2

θm

where the second term of the r.h.s. vanishes by deﬁnition of minimum and we
have indicated with α the inverse of the second derivative at the minimum.
Going back to the likelihood, we get:

L(θ; data)

≈

≈

exp [

ϕ(θm)]

−

k exp

(θ

−
2 α2

"−

exp

·
θm)2

(cid:20)−
,

#

1
2

1
α2 (θ

−

θm)2

(cid:21)

(4)

(5)

(6)

(7)

|

apart a multiplicative factor, this is Gaussian centered in θm with standard
|θm)−2. However, although this function is mathemati-
deviation (∂2ϕ/∂θ2
cally a Gaussian, it does not have yet the meaning of probability density
f (θ
data) in an inferential sense, i.e. describing our knowledge about θ in
the light of the experimental data. In order to do this, we need to process the
likelihood through the Bayes theorem, which allows probabilistic inversions
to be achieved using basic rules of probability theory and logic. Besides a
conceptually irrelevant normalization factor (that has to be calculated at
some moment) the Bayes formula is

f (θ

data)

|

∝

f (data

θ)

f0(θ) .

|

·

(8)

We can speak now about the “probability that θ is within a given interval”
and calculate it, together with expectation of θ, standard deviation and so

3But not yet a probability function! The likelihood has the probabilistic meaning of a

joined p.d.f. of the data given θ, and not the other way around.

7

on.4 If the prior f0(θ) is much vaguer that what the data can teach us (via
the likelihood), then it can be re-absorbed in the normalization constant,
giving:

f (θ

data)

f (data

θ) = L(θ; data)

|

|
i.e

or

∝

∝

∝

exp [

ϕ(θ; data)]

−

exp

χ2(θ; data)
2

#

"−

parabolic ϕ or χ2 :

f (θ

data) =

→

|

1
√2π σθ

(θ

E[θ])2

exp

"−

−
2 σ2
θ

.

#

(9)

(10)

(11)

(12)

If this is the case, it is a simple exercise to show that

a) E[θ] is equal to θm which minimizes the χ2 or ϕ.

b) σθ can be obtained by the famous conditions ∆χ2 = 1 or ∆ϕ = 1/2,

−2
θ = 1/2

(cid:12)
(cid:12)

respectively, or by the second derivative around θm: σ
−2
θ = (∂2ϕ/∂θ2)
(∂2χ2/∂θ2)
(cid:12)
(cid:12)

θm, respectively.

θm or σ

Though in the frequentistic approach language and methods are usually
more convoluted (even when the same numerical results of the Bayesian
reasoning are obtained), due to the fact that probabilistic statements about
physics quantities and ﬁt parameters are not allowed in that approach, it
is usually accepted that the above rules a and b are based on the parabolic
behavior of the minimized functions. When this approximation does not
hold, the frequentist has to replace a prescription by other prescriptions
that can handle the exception.5 The situation is simpler and clearer in the

×

4θ has not a probabilistic interpretation in the frequentistic approach, and therefore we
cannot speak consistently, in that framework, about its probability, or determine expec-
tation, standard deviation and so on. Most physicists do not even know of this problem
and think these are irrelevant semantic quibbles. However, it is exactly this contradic-
tion between intuitive thinking and cultural background[6] that yields or induces wrong
scientiﬁc conclusions, like those discussed in this paper.

5It is a matter of fact that the habit in the particle physics community of applying
uncritically the ∆χ2 = 1 or ∆ϕ = 1/2 is related to the use of the software package
MINUIT[8]. Indeed, MINUIT can calculate the parameter variances also from the χ2 or
ϕ curvature at the minimum (that relies on the same hypothesis upon which the ∆χ2 = 1
or ∆ϕ = 1/2 rules are based). But when the χ2 or ϕ are no longer parabolic, the

8

Χ2

4

3

2

1

fHΜL

0.4

0.3

0.2

0.1

2

1

Χ2min + 1

68%

2

4

6

8

2

4

6

8

Μ

10

E@ΜD=4.2
ΣHΜL=1.5

Μ

10

Figure 2: Example (Ref. [2]) of asymmetric χ2 curve (left plot) with a χ2 minimum
at µ = 5 (µ stands for the value of a generic physics quantity). The result based
on the χ2
min + 1 ‘prescription’ is compared (plot on the right side) with the p.d.f.
exp[
based on a uniform prior, i.e. f (µ

χ2/2].

data)

|

∝

−

Bayesian approach, in which the above rules a and b do hold too, but only
as approximations under well deﬁned conditions.
In case the underlying
conditions fail we know immediately what to do:

•

•

restart from Eq. (9) or from Eq. (11), depending on the other under-
lying hypotheses;

go even one step before Eq. (9), namely to the most general Eq. (8), if
priors matter (e.g. physical constraints, sensible previous knowledge,
etc.).

For example, if the χ2 description of the data was a good approximation,
e−χ2/2, properly normalized, is the solution to the problem.
then f (θ)
A non parabolic, asymmetric χ2 produces an asymmetric f (θ) (see Fig. 2),
the mode of which corresponds, indeed, to what obtained minimizing χ2,
but expected value and standard deviation diﬀer from what is obtained by

∝

standard deviation calculated from the curvature diﬀers from that of the ∆χ2 = 1 or
∆ϕ = 1/2 (in particular, when the minimized function is asymmetric the latter rules give
two values, the (in-)famous ∆± we are dealing with). People realize that the curvature at
the minimum depends from the local behavior of the minimized curve, and the ∆χ2 = 1
or ∆ϕ = 1/2 rule is typically more stable. Therefore, in particle physics the latter rule
has become de facto a standard to evaluate ‘conﬁdence intervals’ at diﬀerent ‘levels of
conﬁdence’ (depending of the value of the ∆χ2 or ∆ϕ). But, unfortunately, when those
famous curves are not parabolic, numbers obtained by these rules might loose completely
a probabilistic meaning.
[Sorry, a frequentist would object that, indeed, these numbers
do not have probabilistic meaning about θ, but they are ‘conﬁdence intervals’ at such and
such ‘conﬁdence level’, because ‘θ is a constant of unknown value’, etc. . . Good luck!]

9

y

m1

m2

x

Figure 3: Example of two-dimensional multi-spots “68% CL” and “95% CL” con-
tours obtained slicing the χ2 or the minus log-likelihood curve at some magic levels.
What do they mean?

the ‘standard rule’.
evaluated from their deﬁnitions:

In particular, expected value and variance must be

E[θ] =

θ f (θ

σ2
θ =

Z

Z

(θ

−

data) dθ

|
E[θ])2 f (θ

data) dθ .

|

(13)

(14)

Other examples of asymmetric χ2 curves, including the case with more than
one minimum, are shown in Chapter 12 of Ref. [2], and compared with the
(often missing clear) results coming from frequentist prescriptions.

Unfortunately, it is not easy to translate numbers obtained by ad hoc
rules into probabilistic results, because the dependence on the actual shape
of the χ2 or ϕ curve can be not trivial. Anyhow, some rules of thumb can be
given in next-to-simple situations where the χ2 or ϕ has only one minimum
and the χ2 or ϕ curve looks like a ‘skewed parabola’, like in Fig. 2:

the 68% ‘conﬁdence interval’ obtained by the ∆χ2 = 1, or ∆ϕ = 1/2
rule still provides a 68% probability interval for θ.

the standard deviation obtained using Eq. (14) is approximately equal
to the average between the ∆+ and ∆− values obtained by the ∆χ2 =
1, or ∆ϕ = 1/2 rule:

•

•

(15)

σθ ≈

∆+ + ∆−
2

;

10

•

the expected value is equal to the mode (θm, coinciding with the max-
imum likelihood or minimum χ2 value) plus a shift:

E[θ]

θm +

≈

(∆+ −

O

∆−) .

(16)

[This latter rule is particularly rough because E[θ] is more sensitive
than σθ on the exact shape of χ2 or ϕ curve. Equation (16) has to be
taken only to get an idea of the order of magnitude of the eﬀect. For
∆−).]
example, in the case depicted in Fig 2 the shift is 80% of (∆+ −
The remarks about misuse of ∆χ2 = 1 and ∆ϕ = 1/2 rules can be
extended to cases where several parameters are involved. I do not want be
into details (in the Bayesian approach there is nothing deeper than studying
k e−χ2/2 or k e−ϕ in function of several parameters), but I just want to get
the reader worried about the meaning of contour plots of the kind shown in
Fig. 3.

3.2 Nonlinear propagation

Another source of asymmetric uncertainties is nonlinear dependence of the
output quantity Y on some of the input X in a region a few standard
deviations around E(X). This problem has been studied with great detail in
Ref. [1], also taking into account correlations on input and output quantities,
and somewhat summarized in Ref. [2]. Let us recall only the most relevant
outcomes, in the simplest case of only one output quantity Y and neglecting
correlations.

Figure (4) shows a non linear dependence between X and Y and how
a Gaussian distribution has been distorted by the transformation [f (y) has
been evaluated analytically using Eq.(1)]. As a result of the nonlinear trans-
formation, mode, mean, median and standard deviation are transformed in
non a trivial way (in the example of Fig. 4 mode moves left and expected
value right). In general the complete calculations should be performed, ei-
ther analytically, or numerically or by Monte Carlo. Fortunately, it has been
shown in Ref. [1] that a second order expansion is often enough to take into
account small deviations from linearity and that the resulting formulae are
still compact and depend on statistical summaries of the initial distributions.
Second order propagation formulae depend on ﬁrst and second deriva-
tives. In practical cases (especially as far as the contribution from systematic

11

Y = X + X2(cid:144)10

6

4

2

-2

-4

-2

2

4

X

f

0.4

0.2

f

0.4

0.2

-4

-2

2

-4

-2

2

X1

4

Y1

4

Figure 4: Propagation of a Gaussian distribution under a nonlinear transformation.
f (Yi) were obtained analytically using Eq.(1) (part of Fig. 12.2 of Ref.[2]).

eﬀects are concerned) the derivatives are obtained numerically6 as

E[X] ≈

≈

E[X]

+

∆+
σ(X)

1
2 (cid:18)
1
σ(X) (cid:18)

=

∆−
σ(X) (cid:19)
∆−
σ(X) (cid:19)

∆+
σ(X) −

∆+ + ∆−
2 σ(X)

,

=

∆−

∆+ −
σ2(X)

,

(17)

(18)

∂Y
∂X (cid:12)
(cid:12)
∂2Y
(cid:12)
(cid:12)
∂X 2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where ∆− and ∆+ now stand for the left and right deviations of Y when
the input variable X varies by one standard deviation around E[X]. Second
order propagation formulae are conveniently given in Ref. [1] in terms of the

6Note that sometimes people do not get asymmetric uncertainty, not because the prop-
agation is approximately linear, but because asymmetry is hidden by the standard prop-
agation formula! Therefore also in this case the approximation might produce a bias in
the result (for example, the second order formula of the expected value of the ratio of two
quantities is known to experts[9]). The merit of numerical derivatives is that at least it
shows the asymmetries.

12

∆± deviations7. For Y that depends only on a single input X we get:

E(Y )
σ2(Y )

≈

Y (E[X]) + δ ,

2

∆

+ 2 ∆

δ

S(X) + δ2

(X)

[
K

1] ,

·

≈

·
where δ is the semi-diﬀerence of the two deviations and ∆ is their average:
∆+ −
2
∆+ + ∆−
2

∆ =

δ =

∆−

−

(23)

(24)

·

,

while

(X) and

(X) stand for skewness and kurtosis of the input variable.8

(21)

(22)

S

K

For many input quantities we have

E(Y )

Y (E[X]) +

δi ,

σ2(Y )

≈

≈

Xi

Xi

σ2
Xi(Y ) ,

where σ2
Xi(Y ) stands for each individual contribution to Eq. (22). The
expression of the variance gets simpler when all input quantities are Gaussian
(a Gaussian has zero skewness and kurtosis equal 3):

σ2(Y )

≈

∆

2
i + 2

δ2
i ,

Xi
and, as long as δi are much smaller that ∆i, we get the convenient approxi-
mated formulae

Xi

E(Y )

≈

σ2(Y )

Y (E[X]) +

δi ,

Xi

2
∆
i

≈
7In terms of analytically calculated derivatives, δ and ∆ are given by

Xi

δ =

∆ =

σ2(X)

1
2

∂2Y
∂X 2

E[X]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
E[X]

σ(X) .

∂Y
∂X

(cid:12)
(cid:12)
8After what we have seen in Sect. 2 we should not forget that the input quantities
(cid:12)
could have non trivial shapes. Since skewness and kurtosis are related to 3rd and 4th
moment of the distribution, Eq. (22) makes use up to the 4th moment and is deﬁnitely
better that the usual propagation formula, that uses only second moments. In Ref. [1]
approximated formulae are given also for skewness and kurtosis of the output variable,
from which it is possible to reconstruct f (y) taking into account up to 4-th order moment
of the distribution.

13

(25)

(26)

(27)

(28)

(29)

(19)

(20)

P

•

•

valid also for other symmetric input p.d.f.’s (the kurtosis is about 2 to 3 in
typical distribution and its exact value is irrelevant if the condition

i ∆

2
i holds). The resulting practical rules (28)–(29) are quite simple:
P

i δ2

i ≪

the expected value of Y is shifted by the sum of the individual shifts,
each given by half of the semi-diﬀerence of the deviations ∆±;

each input quantity contributes (in quadrature) to the uncertainty
with a term which is approximately the average between the deviations
∆±.

Moreover, if there are many contributions to the uncertainty, the ﬁnal uncer-
tainty will be symmetric and approximately Gaussian, thanks to the central
limit theorem.

3.3 Uncertainty due to systematics

Finally, and this is often the case that we see in publications, asymmetric
uncertainty results from systematic eﬀects. The Bayesian approach oﬀers a
natural and clear way to treat systematics – and I smile at the many at-
tempts9 of ‘squaring the circle’ using frequentistic prescriptions. . . – simply
because probabilistic concepts are applied to all inﬂuence quantities that
can have an eﬀect on the quantity of interest and whose value is not pre-
cisely known. Therefore we can treat them applying consistently probability
theory results. This was also recognized by metrologic organizations[11].

Indeed, there is no need to treat systematic eﬀects in a special way. They
are treated as any of the many input quantity Y discussed in Sect. 3.2, and,
in fact, their asymmetric contributions come frequently from their nonlinear
inﬂuence on the quantity of interest. The only word of caution, on which I
would like to insist, is to use expected value and standard deviation for each
systematic eﬀect. In fact, sometimes the uncertainty about the value of the
inﬂuence factors that contribute to systematics is intrinsically asymmetric.
For further details about meaning and treatment of uncertainties due
systematics and their relations to ISO Type B uncertainties, see Refs. [1]
and [2].

9It has been studied by psychologists how sometimes our eﬀorts to solve a problem are
the analogous with the moves along elements of a group structure (in the mathematical
sense). There is no way to reach a solution until we not break out of this kind of trapping
psychological or cultural cages.[10]

14

4 Some rules of thumb to unfold probabilistic sen-
sible information from results published with
asymmetric uncertainties

Having understood what one should have done to obtain expected value and
standard deviation in the situations in which people are used to report asym-
metric uncertainties, we might attempt to recover those quantities from the
published result. It is possible to do it exactly only if we know the detailed
contributions to the uncertainty, namely the χ2 or log-likelihood functions
of the so called ‘statistical terms’ and the pairs
, together to the
probabilistic model, for each ‘systematic term’. However, these pieces of
informations are not always available. But we can still make some guesses,
making some rough assumptions, lacking other information:

∆+i, ∆+i}
{

•

•

asymmetric uncertainties in the ‘statistical part’ are due to asymmetric
χ2 or log-likelihood:

apply corrections given by Eqs. (15)–(16);

asymmetric uncertainties in the ‘systematic part’ comes from nonlinear
propagation:

apply corrections given by Eqs. (28)–(29).

→

→

As a numerical example, imagine we read the following result (in arbitrary
units):

Y = 6.0 +1.0
−2.0

+0.5
−1.4 ,

(30)

(that somebody would summary as 6.0 +1.1
−2.4!). The only certainty we have,
seeing two asymmetric uncertainties with the same sign of skewness, is that
the result is deﬁnitively biased. Let us try to make our estimate of the bias
and to calculated the corrected result (that, not withstanding all uncertain-
ties about uncertainties, is closer to the ‘truth’ than the published one):

1. the ﬁrst contribution gives roughly [see. Eqs. (15)–(16)]:

2. for the second contribution we have [see. Eqs. (28)–(29)]:

1.0

δ1 ≈ −
σ1 ≈

1.0 ;

0.45

δ2 ≈ −
σ2 ≈

0.95 .

15

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

Our guessed best result would then become10

Y

4.55

4.6

1.0

±
1.4 .

±

±

≈

≈

0.95 = 4.55

1.38

±

(The exceeding number of digits in the intermediate steps are just to make
numerical comparison with the correct result that will be given in a while.)
If we had the chance to learn that the result of Eq. (30) was due to the
asymmetric χ2 ﬁt of Fig. 2 plus two systematic corrections, each described
by the triangular distribution of Fig. 1, then we could calculate exactly
expectation and variance:

E(Y ) = 4.2 + 2
×
σ2(Y ) = 1.52 + 2

0.17 = 4.54
0.422 = 1.612 ,

×

1.61, quite diﬀerent form Eq. (30) and close to the result cor-
i.e. Y = 4.54
rected by rule of thumb formulae. Indeed, knowing exactly the ingredients,
we can evaluate f (y) from Eq.(1) as

±

f (y) =

δ(y

x1 −

x2 −

−

Z

x3) f1(x1) f2(x2) f3(x3) dx1 dx2 dx3 , (39)

although by Monte Carlo. The result is given in Fig. 5, from which we
can evaluate an average of 4.54 and a standard deviation of 1.65 in perfect
agreement with the ﬁgures given in Eqs. (37)–(38).11 As we can see from the
ﬁgure, also those who like to think at ’best value’ in term of most probable
value have to realize once more that the most probable value of a sum is
not necessarily equal to the sum of most probable values of the addends (and
analogous statements for all combinations of uncertainties12). In the distri-
bution of Fig. 5, the mode of the distribution is around 5. Other statistical

10The ISO Guide [11] recommends to give the result using the standard deviation within
parenthesis, instead of using the ±xx notation.
In this example we would have Y ≈
4.55 (1.0) (0.95) = 4.55 (1.38) ⇒ Y ≈ 4.6 (1.4). Personally, I do not think this is a very
important issue as long as we know what the quantity xx means. Anyhow, I understand
the ISO rational, and perhaps the proposed notation could help to make a break with the
‘conﬁdence intervals’.

11The slight diﬀerence between the standard deviations comes from rounding, since
σ(µ) = 1.5 of Fig. 2 is the rounded value of 1.54. Replacing 1.5 by 1.54 in Eq. (38), we
get exactly the Monte Carlo value of 1.65.

12Discussing this issues with several persons I have realized, with my great surprise, that
this misconception is deeply rooted and strenuously defended by many colleagues, even
by data analysis experts (they constantly reply “yes, but. . . ”). This attitude is probably
one of the consequences of being anchored to what I call un-needed principles (namely
maximum likelihood, in this case), such that even the digits resulting from these principles
are taken with a kind of religious respect and it seems blasphemous to touch them.

16

0
0
0
5

0
0
0
4

0
0
0
3

0
0
0
2

0
0
0
1

0

y
c
n
e
u
q
e
r
F

−2

0

2

4

6

8

10

Figure 5: Monte Carlo estimate of the shape of the p.d.f. of the sum of three
independent variables, one described by the p.d.f. of Fig. 2 and the other two by
the triangular distribution of Fig. 1.

quantities that can be extracted by the distribution are the median, equal
to 4.67, and some ’quantiles’ (values at which the cumulative distribution
reaches a given percent of the maximum – the median is the 50% quantile).
Interesting quantiles are the 15.85%, 25%, 75% and 84.15%, for which the
Monte Carlo gives the following values of Y : 2.88, 3.49, 5.72 and 6.18. From
these values we can calculate the central 50% or 68.3% intervals,13 which are
[3.49, 5.72] and [2.88, 6.18], respectively. Again, the information provided
by Eq.30 is far from any reasonable way to provide the uncertainty about
Y , given the information on each component.

13I give the central 68.3% interval with some reluctance, because I know by experience

that in many minds the short circuit

“68% probability interval” ←→ “sigma”

is almost unavoidable (I have known physicists convinced – and who even taught it! –
that the standard deviation only ‘makes sense for the Gaussian’ and that it was deﬁned
via the ‘68% rule’). For this reason, recently I have started to appreciate thinking in
50% probability intervals, also because they force people to reason in terms of better
perceived ﬁfty-to-ﬁfty bets. I ﬁnd these kind of bets very enlighting to show why practical
all standard ways (including Bayesian ones!) fail to report upper/lower conﬁdence limits
in frontier case situations characterized by open likelihoods (see chapter 12 in Ref.[2]). I
like to ask “please use your method and give me a 50% C.L. upper/lower limit”, and then,
when I have got it, “are you really 50% conﬁdent that the value is below that limit and
50% conﬁdent that it is above it? Would you equally bet on either side of that limit?”.
And the supporters of ‘objective’ methods are immediately at loss.

17

Besides the lucky case14 of this example (which was not constructed on
purpose, but just recycling some material of Ref. [2]), it seems reasonable
that even results roughly corrected by rule of thumb formulae are already
better than those published directly with asymmetric result.15 But the
accurate analysis can only be done by the authors who know the details of
the individual contribution to the uncertainty.

5 Conclusions

Asymmetric uncertainties do exist and there is no way to remove them ar-
tiﬁcially. If they are not properly treated, i.e. using prescriptions that do
not have a theoretical ground but are more or less rooted in the physics
community, the published result is biased.
Instead, if they are properly
treated using probability theory, in most cases of interest the ﬁnal result is
practically symmetric and approximately Gaussian, with expected value and
standard deviations which take into account the several shifts due to indi-
vidual asymmetric contributions. Note that some of the simpliﬁed methods
to make statistical analyses had a raison d’etre many years ago, when the
computation was a serious limitation. Now there is no problem to evaluate,
analytically or numerically, integrals of the kind of those appearing e.g. in
Eqs.(1), (13) and (14).

In the case the ﬁnal uncertainty remains asymmetric, the authors should
provide detailed information about ‘shape of the uncertainty’, giving also
most probable value, probability intervals, and so on. But the best estimate
of the expected value and standard deviation should be always given (see also
the ISO Guide [11]).

To conclude, I would like to leave the ﬁnal word to preferred quotation
with whom I usually end seminars and courses on probability theory applied
to the evaluation and the expression of uncertainty in measurements:

14In the example here we have been lucky because an over-correction of the ﬁrst con-
tribution was compensated by an under-correction of the second contribution. Note also
that the hypothesis about the nonlinear propagation was not correct, because we had,
instead, a linear propagation of asymmetric p.d.f.’s. Anyhow the overall shift calculated
by the guessed hypothesis is comparable to that calculable knowing the analysis details
(and, in any case, using in subsequent analyses the roughly corrected result is deﬁnitely
better than sticking to the published ‘best value’).

15Note that even if we were told that Y was 6.0+1.1

−2.4, without further information, we
could still try to apply some shift to the result, obtaining 4.7 ± 1.8 or 5.4 ± 1.8 depending
on some guesses about the source of the asymmetry. In any case, either results are better
than 6.0+1.1
−2.4!

18

“Although this Guide provides a framework for assessing uncertainty,
it cannot substitute for critical thinking, intellectual honesty, and pro-
fessional skill. The evaluation of uncertainty is neither a routine task
nor a purely mathematical one; it depends on detailed knowledge of
the nature of the measurand and of the measurement. The quality and
utility of the uncertainty quoted for the result of a measurement there-
fore ultimately depend on the understanding, critical analysis, and
integrity of those who contribute to the assignment of its value.”[11]

It is a pleasure to thank Superfaber (Fabrizio Fabbri in hepnames) for helpful
discussions on the subject and for his supervision of the manuscript.

References

[1] G. D’Agostini and M. Raso, CERN-EP/2000-026, February 2000

[hep-ex/0002056].

[2] G. D’Agostini, Bayesian Reasoning in Data Analysis: A Crit-
ical Introduction, World Scientiﬁc, 2003 (book info,
including
list of contents and hypertexted bibliography can be found at
www.roma1.infn.it/˜dagos/WSPC/).

[3] G. D’Agostini, Rep. Prog. Phys. 66 (2003) 1383.

[4] V. Dose, Rep. Prog. Phys. 66 (2003) 1421.

[5] R. Barlow, physics/0306138 , physics/0401042, physics/0403046.

[6] G. D’Agostini, Proc. XVIII International Workshop on Maximum
Entropy and Bayesian Methods, Garching (Germany), July 1998,
V. Dose et al. eds., Kluwer Academic Publishers, Dordrecht, 1999,
157–170 [physics/9811046].

[7] ZEUS Collaboration, J. Breitweg et al., Eur. Phys. J. C14 (2000)

239.

[8] F. James and M. Roos. Comp. Phys. Comm. 10 (1975) 343.

[9] See, e.g., A.M. Mood, D.C. Boes and F.A. Graybill, Introduction to

the theory of statistics, McGraw-Hill, 1974.

19

[10] P. Waltzlawick, J.H. Weakland and R. Fisch, Change: principles of
problem formation and problem resolution, W.W. Norton, 1974.

[11] International Organization for Standardization (ISO), Guide to the
expression of uncertainty in measurement, Geneva, Switzerland,
1993.

20

0

1000

2000

3000

4000

5000

Frequency

Y

4

−
2

0

2

6

8

1
0


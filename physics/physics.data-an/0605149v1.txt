6
0
0
2
 
y
a
M
 
7
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
4
1
5
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Optimal approximations of power-laws with exponentials

Thierry Bochud
Nestec Ltd, Av. Nestl´e 55, 1800 Vevey, Switzerland∗

Damien Challet
Nomura Centre for Quantitative Finance, Mathematical Institute,
Oxford University, 24–29 St Giles’, Oxford OX1 3LB, United Kingdom†
(Dated: September 13, 2013)

We propose an explicit recursive method to approximate a power-law with a ﬁnite sum of weighted
exponentials. Applications to moving averages with long memory are discussed in relationship with
stochastic volatility models.

Exponential moving averages are widely used as tool
for computing eﬃciently averages of time-changing quan-
tities such as volatility and price. Their main advantage
resides in their recursive deﬁnition that allows for easy
numerical implementation, or remarkably simple models
of stochastic volatility, such as GARCH [1]. Their use
is however conceptually questionable when the process
in question has long memory, as the volume and volatil-
ity do [2, 3, 4]. One should rather consider a power-law
kernel; this requires however considerably more comput-
ing power as one must keep track of all the data points.
Some authors approximate a power-law with a sum of
exponentials in the literature, the record being held by
[5], which uses 600 exponentials for 2 decades but
Ref.
notices that only a few have a signiﬁcant contribution to
the ﬁnal function.

While the principle of economy should dictate to ﬁt
power-law-looking data with nothing else than a power-
law (see for instance the controversy in the June 2001
issue of Quantitative Finance), computing real-time av-
erages with a power-law kernel is much eased by the use
of a sum of exponentials. Recent stochastic volatility
models for instance use a sum of exponentials [6, 7, 8]
(5, 12 and an inﬁnity, respectively) with algebraically
decreasing weights and algebraically increasing charac-
teristic times, thereby respecting the long-memory of the
volatility, which might explain in part their forecasting
performance[16]. It is clear that only a handful of expo-
nentials are required in order to approximate a power-law
up to a given order of magnitude, as many practitioners
are aware (see for instance [2, 6]). Since ﬁnancial market
data time series do not extend over an inﬁnite period,
such approximation will be good enough for application
to ﬁnancial time-correlations. How many exponentials
should be used and with what parameters seem never
discussed in the literature. Here, we aim to derive an
explicit and new simple scheme that improves the often
used approximation; in addition we show that the usual
assumption of independent contribution from each ex-
ponential implies the existence of an optimal number of
exponentials.

Let f (x) = x−α and g(x) =

N
i=0 gi(x) where gi(x) =

P

wi exp(−λix). Assume that one would like to approxi-
mate f with g from x = 1 to x = 10k, that is, over k
decades. The standard approach (see for instance [9])
consists in deﬁning a cost function per decade that is the
integral of some measure of the diﬀerence between f and
g, i.e.

C =

[−α log x − log g(x)]2d log x

,

(1)

k

10

1
k  Z

1

1/2

!

and to minimize C with respect to wi and λi, so as
to obtain 2(N + 1) coupled non-linear equations. Ad-
hoc numerical methods have been investigated a long
time ago, that solve the resulting set of equations by
using the Gram-Schmidt orthonormalisation of exponen-
tials [9]. Our aim here is to obtain a sub-optimal (with
respect to C) but explicit set of wi and λi.

The proposed method relies on a simple ansatz for wi
and λi.
Instead of trying to solve an intricate set of
non-linear equations, one observes that the nature of a
power-law is to be scale-free, whereas an exponential has
a well deﬁned scale. Therefore, the role of each exponen-
tial is to approximate a given region of the k decades.
In particular, one wishes that the i-th exponential ap-
proximates correctly f (x) at xi = βi where β > 1 is a
constant. This already suggests that λi ∝ β−i, which is
both intuitive and well-known. Then one matches g to
f and its ﬁrst derivative g′ to f ′ at xi = βi. However,
once again, this would yield 2(N + 1) coupled non-linear
equations. The key observation is that, provided that β
is large enough (see below), only gi contributes signiﬁ-
cantly to g at xi, i.e. g(xi) ≃ gi(xi). We therefore solve
gi(xi) = f (xi) and g′(xi) = f ′(xi), which gives

−i

λi = αβ
e
βi

wi =

(cid:18)

(cid:19)

α

.

(2)

(3)

However, g(xi) > f (xi) because the contribution of the
exponentials other than the i-th cannot be totally ig-
nored. Therefore, one must correct the above over-
optimistic assumption by considering that g is a weighted

-2

y=x
N=1
N=2
N=5
N=5, recursive

10

1

0.1

C

2

k=1, uniform
k=2, uniform
k=3, uniform
k=4, uniform
k=1, recursive
k=2, recusrive
k=3, recursive
k=4, recursive

)
x
(
g
 
,
)
x
(
f

0
10

-1

10

-2

10

-3

10

-4

10

-5

10

-6

10

-7

10

-8

10

-9

10

1

10

1000

10000

100
x

0.01

0

2

4

6

8

10

N

FIG. 1: Convergence of the approximation function g(x) to
f (x) for the uniform ansatz with 2 (red line), 3 (green line)
and 6 exponentials (blue line), and for the recursive ansatz
with 6 exponentials (orange line); α = 2, β = 5

FIG. 2: Error per decade C as a function of N for various k;
α = 2; β = 10k/N for the uniform and recursive ansatz (full
and empty symbols respectively). Lines are for eye guidance
only.

sum of gi(x)

N

i=0
X

g(x) =

ciβ

−iα exp(α) exp(−α/βix),

(4)

where {ci} is a set of correction factors. The last step is
to solve g(βj) = f (βj), which is a set of N +1 linear equa-
tions with variables ci. The complexity of the problem
has been greatly reduced. One can solve numerically this
set of equations. In order to obtain explicit expressions
for ci, one has to resort to another approximation.

The simplest ansatz for ci already gives a high de-
gree of accuracy and is equivalent to the one currently
in use in the literature. Taking uniform ci = c given by
i=0 β−iα exp(α) exp(−α/βi) ensures the equality
1/c =
g(1) = f (1). With this choice the factor exp(α) disap-
pears from g(x) and

P

N

N

i=0
X

N

−1

i=0
X

g(x) =

−iα exp(−α/βi)

β

−iα exp(−α/βix)

β

(cid:0)

(cid:1)
(5)
Fig. 1 shows how the approximation works for increas-
ing N : each additional exponential extends the range
that is well approximated by a factor β. The value of β
was chosen large enough so as to emphasise the oscilla-
tions of g(x) at each βj. The uniform ansatz implies that
while g(1) = f (1) = 1, g(βj ) > f (βj) for 0 < j < N since
the contribution of each gk is asymmetric with respect to
βj; when j = N , since there are no additional exponen-
tials from i > j to contribute to g, g(βN ) < β−αN (see
Fig. 1). This problem is of course negligible when a very
large number of exponentials is used; however, since our
aim is to use as few exponentials as possible it needs to
be addressed.

The parameter β tunes how much of a decade is ap-
proximated by a single exponential. When k and N are
ﬁxed, it is sensible to take βN = 10k. The cost func-
tion C is plotted in Fig. 2 as a function of N at ﬁxed
k for several values of k. For small N , C decreases ex-
ponentially as a function N . Then, strikingly, C has a
minimum at Nm(k) and increases slightly before stabil-
ising; the smaller α, the smaller the subsequent increase.
One would have naively expected that C decreased mono-
tonically as a function of N ; however, since β decreases
when N increases at ﬁxed k, the assumption that the
exponentials give independent contributions to g is not
valid any more at N ≃ Nm, and becomes clearly in-
correct when N > Nm. The consequence is that g(x)
becomes too large except at x = 1. This is not prob-
lematic, however, since in practice, one prefers large β to
small ones, so as to use as few exponentials as possible.
As expected, Nm increases linearly with k, implying that
for α = 2, the optimal N = Nm(k) ≃ 1.7k, or equiva-
lently β ≃ 101/1.7 ≃ 3.87. Another feature of this ﬁgure
is that C(Nm(k)) decreases as function of k: this due
to the vanishing inﬂuence of the deviation caused by the
downwards shift of the last exponential.

It is possible to improve the precision of the approxi-
mation for N < Nm by modifying the scale of x, or equiv-
alently by taking into account derivatives of g of higher
α(α + 1)β−i.
orders. The second order yields λi =
From the conditions on the ﬁrst derivatives and on the
p
equality of functions, wi ∝ β−αi exp(
α(α + 1)). This
reasoning can be extended to match the derivatives up

p

3

-2

y=x
N=5, uniform
N=5, recursive

-6

10

)
x
(
g
 
,
)
x
(
f

-7

10

-8

10

1

C

0.1

N=2
N=3
N=4
N=5

0.01

0

5

10
n

15

20

FIG. 3: Error per decade C as a function of n for various
N ≤ Nm = 5; k = 3, α = 2; β = 10k/N . Dotted lines are for
eye guidance only.

g(x) =

−iα exp(−µ/βi)

β

−iα exp(−µx/βi)

β

to order n, resulting in

N

i=0
X

(cid:0)

with

N

−1

i=0
X

(cid:1)

1
n



mu =

(α + j)

=

n−1



j=0
Y

Γ(α + n)
Γ(α)

(cid:20)

1
n

(cid:21)




Since µ does not depend on i it modiﬁes the scale of x,
which can be used to adjust the position in log-space of
g relative to f . For large n, µ ≃ (n + α − 1)/e, therefore
shifting g(x) to larger x. According to Fig. 3, as long as
N < Nm, there is an optimal n. This comes from the fact
that g(βN ) < f (βN ): it is more advantageous to shift x
to larger values so as to avoid the too small value of g at
βN . It also emphasises once again the need to solve the
problem of the last exponential.

The solution comes from a close examination of Fig.
1: the ﬁrst exponentials do not contribute much to the
value of g(βN ) for N not too small. This suggest that the
contribution of gi(βj) to g(βj) can be neglected if i < j.
As a consequence, g(βN ) ≃ gN (βN ), and cN = 1. Thus

cN −1 = 1 − β

−αeα(1−1/β).

(8)

More generally,

cN −k = 1 −

cN −iβ

−α(k−i)eα(1−β

i−k

)

(9)

k−1

i=0
X

c0 is the same with both ans¨atze, since there is no ex-
ponential on the left of β0. Table I gives an example set of

1000

x

10000

FIG. 4: Zoom of Fig 1 on the last two exponentials. α = 2;
β = 4

cN −k. It is noticable that cN −k display oscillations which
are damped as k increases: since cN = 1 is large in order
to compensate for the absence of further exponentials,
cN −1 must be smaller than c0; next, cN −2 will be slightly
larger than c0 so as to satisfy g(βN −1) = f (βN −1), etc.

(6)

(7)

TABLE I: Correction coeﬃcients given by the recursive
ansatz. α = 2, N = 8, β = 4

k

0

1

2

3

4

5

6

7

8

cN−k 1.000 0.720 0.773 0.763 0.765 0.765 0.765 0.765 0.765

The recursive ansatz always gives a better result that
the uniform one, as it ensures that g(βi) is closer to f (βi)
for all i, and particularly for large i; g approximates f re-
markably well at xi = βi provided that β is not too small.
The diﬀerences are most perceptible for x ≃ βN , where
the recursive scheme gives a much better approximation
(see Fig. 4), which explains why it is most advantageous
for k ≤ 4 where it decreases C, at Nm by a factor 2 for
k = 2 and 1.5 for k = 3; larger k, hence larger Nm, will
not bring much improvement since the weight of the dis-
crepancy caused by the uniform ansatz at βN decreases.
Improving the precision further is possible by taking more
exponentials from the left hand side of βj into account
in the calculus of ci at the price of heavier and probably
non-explicit computations. Finally, if solving the full set
of linear equations for ci does not give enough precision,
the remaing possibility is to minimise numerically C [9].
The above approximation has an obvious application
to ﬁnancial markets. The measure of historical volatility
is usually done with exponential moving averages

V (t + δt) = V (t)Λ + (1 − Λ)v(t)

(10)

where v(t) is some measure of the instantaneous volatility
(e.g. daily volatility) over δt units of time, and Λ = e−λ
is the memory. RiskMetrics recommends Λ1 = 0.98 or
Λ2 = 0.94. While this is an eﬃcient way of computing
an average, it implicitely assumes a choice of a single
time scale 1/| ln Λ| ≃ 1/(1 − Λ) for Λ close to 1. Unfor-
tunately, the volatility is a process with no obvious time
scale, as its autocorrelation function decreases slowly; ﬁt-
ting it with a power-law gives an exponent ν ≃ 0.3 [2, 3].
In other words, any choice of Λ is a compromise between
smoothness and reactivity. To our knowledge, the ﬁrst
paper to use a power-law kernel for measuring volatili-
ties is from the Olsen group [10]. One possible reason of
this particular functional form of the volatility memory
is that the market is made of heteregeneous participants
[11]. For instance the variety of time-scales of people
taking part into ﬁnancial markets is obvious to any prac-
tioner, hence a choice of a single Λ selects the categories
of traders that the resulting average volatility incorpo-
rates. Direct measure on high-frequency data revealed
ﬁve characteristic time scales [8]. Fitting a stochastic
volatility model with ﬁve time-scales, this work found
them to be 0.18, 1.4, 2.8, 7, 28 business days, with re-
spective weights of 0.39, 0.20, 0.18, 0.12, 0.11; the time
scales span about 2.2 decades, and the weights decreases
algebraically as the timescale grows with an exponent of
about α = 0.3. Other work considered α = 2 [6, 10].
Generally speaking, 2α − 2 = ν, which gives α = 1.15
if ν = 0.3 (see e.g.
[7]). For α = 1.15, ﬁve expo-
nentials approximate best three decades with corrections
~c = (0.704, 0.702, 0.714, 0.647, 1). The average volatil-
ity σ2 is a weighted sum of volatilities on given time
scales corresponding to the λis, which, in principle, still
requires to keep the returns over a time horizon equal to
the longest time scale; this is barely economical and de-
feats the initial aim of the approximation. The solution
is the use of sums of nested exponential moving averages
of the last return that are a proxy for returns on larger
time scales [8, 12].

CONCLUSIONS

We have provided a simple method to use eﬃciently a
sum of weighted exponentials as a parsimonious approxi-

4

mation of a power-law with any exponent. In particular,
we have shown the existence of an optimal number of
exponentials when one neglects the contribution of some
exponentials in the determination of the coeﬃcients. The
recursive ansatz is probably precise enough for most ap-
plications.

We thank Gilles Zumbach for useful discussions.

∗ Electronic address: thierry.bochud@nestle.com
† Electronic address: challet@maths.ox.ac.uk

[1] R. Engle, Econometrica 50, 987 (1982).
[2] M. M. Dacorogna, R. Gencay, U. A. M¨uller, R. B. Olsen,
and O. V. Pictet, An Introduction to High-Frequency Fi-
nance (Academic Press, London, 2001).

[3] J.-P. Bouchaud and M. Potters, Theory of Financial
Risks (Cambridge University Press, Cambridge, 2000).
[4] P. Gopikrisnan, V. Plerou, X. Gabaix, and H. Stanley,

Phys. Rev. E p. R4493 (2000).

[5] B. Quian and R. A. Goldstein, PROTEINS: Structure,

Functions, and Genetics 45, 102 (2001).
[6] G. Zumbach, Quant. Fin. 4, 70 (2004).
[7] L. Borland and J.-P. Bouchaud (2005), preprint

[8] P. Lynch and G. Zumbach, Quantitative Finance 3, 320

physics/0507073.

(2003).

[9] T. Svensson, IEEE Trans. Circuit Theory p. 142 (1973).
[10] G. O. Zumbach, M. M. Dacorogna, J. L. Olsen, and R. B.
Olsen, nternational Journal of Theoretical and Applied
Finance 3, 347 (2000).

[11] R. B. Olsen, M. M. Dacorogna, U. A. M¨uller, and O. V.
Pictet, Tech. Rep. BRO.1992-09-07, Olsen (1992).
[12] G. Zumbach and U. M¨uller, Int. J. Th. and Appl. Fin. 4,

[13] L. Calvet and A. Fisher, J. of Econometrics 105, 27

[14] J. Muzy, J. Delour, and E. Bacry, Eur. J. Phys B 3, 537

147 (2001).

(2001).

(2000).

[15] T. Lux (2003), working paper 2003-13.
[16] Models with long memory (see also [13, 14]) appear to

be intrisically better for forecasting [15].

alpha=2 / beta=4 / n=2

k=1
k=2
k=3
k=4

r
o
r
r
e
 
g
o

l

 1

 0.1

 0.01

 0.001

 0

 2

 4

 6

 8

 12

 14

 16

 18

 20

 10

N


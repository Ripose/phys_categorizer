2
0
0
2
 
v
o
N
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
4
0
1
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Yet Another Analysis of Dice Problems. 1

Ali Mohammad-Djafari

Laboratoire des Signaux et Systèmes,
Unité mixte de recherche 8506 (CNRS-Supélec-UPS)
Supélec, Plateau de Moulon, 91192 Gif-sur-Yvette, France

Abstract. During the MaxEnt 2002 workshop in Moscow, Idaho, Tony Vignaux asked again a
few simple questions about using Maximum Entropy or Bayesian approaches for the famous Dice
problems which have been analyzed many times through this workshop and also in other places.
Here, there is another analysis of these problems. I hope that, this paper will answer a few questions
of Tony and other participants of the workshop on the situations where we can use Maximum
Entropy or Bayesian approaches or even the cases where we can actually use both of them.

keywords. Dice problems and probability theory, Maximum Likelihood, Bayesian inference,
Maximum A Posteriori, Entropy, Maximum entropy, Maximum entropy in the mean.

INTRODUCTION

Dice problems have been analyzed many times (See mainly Ed. Jaynes papers [1, 2, 3, 4]
and also [5, 6, 7, 8]), but it seems that still many questions are open. In this note, I will try
to answer some of them. Before starting, we need to set up precise notation and describe
precisely the context.

Let’s consider an imaginary die with K faces (K = 6 is the ordinary die) where on
each face there is a number. We note these numbers g = [g1, . . . , gK ′]. K is the number
of elementary states and commonly, K ′ = K and gk = k, but we may also consider the
cases where gk are any other numbers (integer or real) distinct or not.

}

g1, . . . , gK ′

and G can take values

Let’s also represent by X the variable corresponding to face number and by G the
variable corresponding to the number written on the faces. So, X may take values
. Then, we can deﬁne P (X = k) and
1, . . . , K
}
{
P (G = gk). If the gk are distinct numbers, i.e.,, K = K ′, they are equal P (X = k) =
P (G = gk) = θk, but note that E
= E
k gkθk. If gk is a monotone
k kθk
G
=
{
, but it may not always be the case.
to E
function of k, then it is easy to relate E
G
}
Note also that, in many dice problems, the main hypothesis is that they are fair.
Then assigning the probability distributions becomes a combinatorial computation. For
example, suppose we throw two dice and count the sums S of the two faces numbers.
We want to assign the probabilities pj = P (S = sj).

X
{
P

P

X

=

}

{

{

}

}

{

First, we assume gk = k and note that S can take the values in the set Ω =

2, 3, . . . , 12

{

}

1 To appear in Proceedings of American Institute of Physics: Proceedings of MaxEnt2002, the 22nd
International Workshop on Bayesian and Maximum Entropy methods (Aug. 3-9, 2002, Moscow, Idaho,
USA).

6
{

{

− |

= 11. We must be careful here because the event S = sj can occur q(sj) =
and
Q
|
|
, but S = 5 occurs 5
sj
(1, 1)
6
7
|
−
}
. Now, using the basic principle of equal weight of
times Ej =
statistical mechanics or insufﬁcient reason of Laplace, we assign pj = P (S = sj)
Ej
which gives pj = P (S = sj) = q(sj)/

times. For example, S = 2 occurs one time Ej =
(1, 4), (2, 3), (3, 2), (4, 1)
}

|Q|
j=1 q(sj).
In a more general case, we may have L dice and may want to deﬁne the events such
l xl = sj

that Ej =
}
and assign them probabilities. We may also consider the case where we throw L dice
simultaneously N times which is not the same as throwing N dice simultaneously L
times, except the case where the dice are identical. We may also consider the cases
where the number of throwing the dice are different, i.e.,, the dice l has been thrown Nl
times.

(X1 = x1, . . . , XL = xL) :

(X1 = x1, . . . , XL = xL)

P
or Ej =

∝ |

P

}

{

{

|

In some other analysis, we may not know if the die is loaded or not. This may be one
of the questions to be answered. To be able to answer to a question, we may need to
gather relevant data. These data may be of different form and thus, as we will see in the
following, the way to use them to answer a question may also differ.

Before gathering any data, we may deﬁne the question to be answered. For example, if
we want to know if the die is loaded or not, we may be interested to infer about θ. Also,
before gathering any data, we may make hypotheses and we may be able to translate the
knowledge contained in these hypotheses by an a priori probability law π(θ).

For example, we may assume that the die is not loaded and assume θ1 = θ2 = . . . =
θK = 1/K or choose a uniform prior for π(θ) over the set
.
}
Note that, even if they translate to a common-sounding hypothesis, mathematically
speaking, they are not exactly the same. The former says P (
=
1 > b > a > 0.
θl, k

∈
P
= 1) = 0 and P (θk

b) = (b
We may also be able to associate a likelihood function P (D

θ) with the data to
represent the amount of knowledge about the unknown parameters contained in the data.
We will see however that this may not be easy in some cases.

= l) = 0 and P (a < θk

k θk = 1

θ : θk

[0, 1]&

k θk

a),

P

≤

−

∀

{

|

The questions may also be different: We may want to know if the die is loaded or not
or we may want to know what is the probability that the next face be the face k, or still,
what are the numbers written on the faces of the die.

Let us start by a simple and easy problem which, here after, we call Problem 1.

PROBLEM 1

We have observed the complete data x = [x1, . . . , xN ] and we know the number of states
K (number of faces). The question is to estimate θ = [θ1, . . . , θK] where θk = P (X = k)
is the probability of the event face k up.

Here is a Matlab program which simulates this data generation:

K=6;N=100;x=round((K-1)*rand(N,1))+1;

and the following is an example (an N sample) of this data set:

6
6
6
x = [4, 2, 2, 2, 1, 5, 4, 5, 1, 4, 3, 3, 6, 6, 4, 6, 6, 4, 4, 1, 1, 2, 1, 6, 4, 2, 4, 2, 3, 2, 2, 6, 2, 2, 1, 6,
5, 5, 6, 3, 5, 4, 2, 2, 4, 4, 4, 3, 6, 6, 4, 5, 2, 5, 3, 5, 2, 5, 1, 3, 3, 4, 3, 1, 3, 3, 5, 3, 3, 2, 5, 5, 3, 4, 4,
3, 3, 3, 4, 1, 2, 4, 4, 5, 4, 5, 6, 5, 6, 5, 5, 5, 1, 1, 4, 1, 5, 2, 1, 6].

Note that, if we re-run the program, we obtain a different data set. Here are the
results of a second run:

x = [6, 2, 4, 3, 5, 5, 3, 1, 5, 3, 4, 5, 6, 5, 2, 3, 6, 6, 3, 5, 1, 3, 5, 1, 2, 2, 2, 4, 2, 2, 1, 5, 3, 6, 3, 3,
5, 4, 2, 4, 5, 1, 4, 3, 5, 4, 5, 3, 3, 2, 2, 4, 3, 4, 2, 4, 3, 5, 5, 4, 3, 5, 5, 4, 5, 4, 3, 2, 3, 4, 5, 3, 5, 4, 3,
5, 4, 3, 4, 4, 5, 6, 4, 5, 2, 6, 2, 2, 5, 5, 2, 1, 5, 2, 2, 4, 2, 3, 1, 6].

These two data sets can represent two different experiences using the same die.

Let nk denote the number of times the face k has shown up nk = #(X = k). Then we

have

k nk = N. Here is a Matlab program which computes these numbers:

P
nk=zeros(K,1);
for k=1:K

nk(k)=sum(x==k);

end

and here are the results for the two above data sets:

Data set 1: n = [13, 17, 17, 21, 19, 13] and
Data set 2: n = [07, 19, 21, 20, 25, 08].

Now, let’s start by asking about the values of θk. If all the θk are the same value, we
can say that the die is not loaded, but if they are too different from each other, we may
say that the die is loaded.

A wise man can say: This is an easy problem. If each trial has been done identically
and independently, then it is reasonable to estimate each θk by θk = nk/N and no need
for more complex mathematics. But if we ask: How conﬁdent or (how sure) are you
about these values? He may say: hum..., let’s use the probability theory.

Assume we know K and we have given x (and thus we now N) and assume that the
die has been thrown always in the same manner and independently. Here then, we can
write the complete likelihood function

P (x

θ) =

|

C nk

N θnk

k (1

θk)N −nk.

−

k
Y

(1)

Note that in the right hand side of this expression, x is present through nk and we can
write P (x

θ).

θ) = P (n
|
Then the likelihood

|

(θ) = P (x

θ) and we have

L

|

ln

(θ) =

L

k
X

[nk ln θk + (N

nk) ln(1

θk)] + c(nk, N)

(2)

−

−

where c(nk, N) =

k ln C nk

N does not depend on θ.

Knowing that each parameter θk
∈
P
this interval. However, we know that
and thus deﬁne a uniform prior on this set π(θ) = 1,
[0, 1]&
elsewhere, and thus obtain the a posteriori law

k θk = 1, then we can deﬁne the set Θ =

[0, 1], we can choose a uniform prior π(θk) = 1 on

θ : θk
∈
Θ and zero

k θk = 1

P

∈

θ

∀

}

{

P

π(θ

x) = L

|

(θ) π(θ)
m(x)

=

1
m(x)

C nk

N θnk

k (1

θk)N −nk

−

k
Y

which is deﬁned on the same set Θ and where m(x) is the marginal or evidence function:

m(x) =

(θ) π(θ) dθ =

dθ

ZZΘ L

ZZΘ

k
Y

C nk

N θnk

k (1

θk)N −nk.

−

Thus, we have

ln π(θ

x) =

[nk ln θk + (N

nk) ln(1

θk)]

ln m(x).

−

−

−

|

k
X

Now, if we are only interested by the value of

θ

we can compute it by putting the derivative of ln π(θ
θk to obtain

|

b

M AP

which has the highest probability,
x) with respect to each parameter

∂ ln π(θ

x)/∂θk =

|

nk
θk −

N
1

nk
θk

−
−

=

nk
−
θk(1

Nθk
θk)

−

= 0

−→

θM AP
k

=

nk
N

.

There is only one possible solution to this equation and there is not any ambiguity. Here
are the results for the two above data sets:

b

M AP

θ

Data set 1:

= [0.1300, 0.1700, 0.1700, 0.2100, 0.1900, 0.1300] and
= [0.0700, 0.1900, 0.2100, 0.2000, 0.2500, 0.0800].
But, we must be careful here on the interpretations that we can give to these numerical

M AP

θ
b

Data set 2:

values. We may want to answer the following questions:

b

• Do these two data sets come from the same die?
• Is this die loaded?
• What is the probability of seeing face k up based on the data set 1 or the data set 2?
• If I throw this die 100 times again, what will be the number of times I will see face

k up?

We have still too much to do before being able to give correct answers these questions.

(3)

(4)

(5)

(6)

Assume now that, in place of x, we have only access to the data n = (n1, . . . , nK) and
know the values of K and N (or if we knew that
k nk = N). It is easy to see that

PROBLEM 2

P

1

0
Z

−

(7)

(8)

(9)

(10)

we obtain exactly the same result, because (n, K,
likelihood and form sufﬁcient statistics about this problem.

K
k=1 nk = N) deﬁne perfectly the

Note however that, in both cases, the likelihood

θk = 1 and consequently, the posterior pdf π(θ
to analyze properly this point.

|

P
L

(θ) is not deﬁned for θk = 0 and
n) may not be a proper pdf. We are going

First, noting that the likelihood function in the previous section

k π(θk), we also have π(θ

π(θ) =
work hereafter only with the functions l(θ), π(θ), π(θ
[9, 10]

k π(θk

x) =

Q

Q

|

|

|

Q

x) =

(θ) =
k l(θk) and
nk). Thus, we can
x) and m(x) which is given by

L
k π(θk
|

Q

m(x) =

π(θ) θx (1

θ)(N −x) dθ.

−

With a uniform prior π(θ) = 1 we have

m(x) =

θx (1

θ)(N −x) dθ =

(x + 1, N

x + 1)

B

−

1

0

Z

where

(α, β) is the Beta probability density function (pdf)

B

α, β) =

f (x
|

1
B(α, β)

xα−1(1

x)(β−1)

−

which is deﬁned for α > 0, β > 0 and x

[0, 1] and where

B(α, β) =

xα−1(1

x)(β−1) dx

−

∈
1

0
Z

and we have:

mode
{

x
}

=

1

α
−
α + β

2

−

, E

=

x
}

{

α
α + β

and Var
{

x
}

=

αβ
(α + β)2(α + β + 1)

.

nk) =

(nk, N

alently π(θk
θM AP
k

Consequently, the posterior law, whose expression is π(θ

n) or equiv-
|
nk), is only bounded if N > nk > 0. The MAP estimators

(n, N

n) =

−

B

B

|
= nk−1

−
N −2 do not exist if nk < 1 or if nk > N

The posterior mean estimators

2.
≤
N exist if N > 0 and the posterior variances
b
(N 2(N +1) exist if 0 < nk < N and N > 0. Note also that when nk = 1 the
Var
{
1 the corresponding MAP
corresponding MAP estimator is θk = 0 and when nk = N
estimator is θk = 1. This shows a kind of bias of the estimator toward θk = 0 and θk = 1
(See Table 1).

k = (nk)(N −nk)

k = nk
θP M

1 and if N

−

−

b

}

θ

One may want to have a proper posterior law π(θ

n) for the whole range of possible
[0, 1] and the data nk = [0, 1, . . . , N]. This can be done via
values of the parameters θk
other choices for the prior law. In the two previous cases, we choose a uniform a priori
for θk. Some authors argued that this choice is too biased against extreme values 0 and
1 and proposed to use

∈

|

π(θk) = [θk(1

θk)]−1 = θ−1

k (1

θk)−1.

−

−

(11)

Note also that, again with this prior, the normalization factor or the evidence function

m(x) is given by

m(x) =

[θ(1

θ)]−1 θx (1

θ)(N −x) dθ =

(x + 1, N

x + 1)

(12)

−

−

B

(n + 1, N

n + 1) which is bounded if N

1 > nk > 0 (See

−

−

1

0
Z
n) =

B

which yields π(θ
Table 1).

|

A more general choice is

−

π(θk) = θa−1

(1

k

θk)b−1

−

(13)

which results to

1

m(x) =

θa−1(1

θ)b−1 θx (1

θ)(N −x) dθ =

(x + a, N + b

x)

(14)

−

−

B

−

0
Z
which result to π(θ
a.
Then, the mean values θk = (nk + a)/(N + b + a) have the limit value θk = nk/N when
a = b

0. The following Table summarizes these points.

n) which is bounded if N

(n + a, N + b

b > nk > 1

n) =

−

−

−

B

|

7→

nk

nk > 0

nk + 1

nk > 0

TABLE 1. Different a posteriori laws corresponding to different choices of a priori laws

α > 0

β > 0

α + β

mode

mean

variance

N

nk

−

nk < N

N

N > 0

π(θk) = 1, π(θk

(nk, N

nk) =
|
nk−1
N −2

B

nk > 0,
N > 2

−

nk)
nk
N

nk > 0,
N > 0

π(θk) = θ−1

k (1

θ)−1, π(θk
nk) =
|
nk
N −1

B

(nk + 1, N
nk+1
N +1

−
N + 1

nk)

−

N

nk

−

nk < N

−

nk

N

≤

π(θk) = θa−1

k

(1

θ)b−1, π(θk

−

nk + a

N

nk + b N + a + b

nk

0

≥

N

0

≥

N > 0

nk
0,
≥
N > 1

0,
0

nk
N

≥
≥
(nk + a, N + b
nk+a
N +a+b

nk
N

0,
0

≥
≥

nk) =
|
nk+a−1
N +a+b−2

B

0,
nk
≥
N > 0

nk(N −nk)
N 2(N −1)
nk > 0,
nk < N,
N > 1

nk+1)(N −nk)
(N +1)2(N +2)

nk
nk
N

0,
N,
0

≥
≤
≥

nk)

−

(nk+a)(N −nk+b)
(N +a+b)2(N +a+b+1)

nk
nk
N

0,
N,
0

≥
≤
≥

Note also that, when we have the expression of the a posteriori law π(θ

n), we may
deﬁne other estimators than the MAP or the posterior mean (PM). We may also answer
the questions of type P (a < θk < b).

|

Note however that, all these computed numbers depend on the data and our prior
knowledge we included. For any other data set we obtain other numbers. One may want

to study the sensitivity of the solution to a kind of variability of data. This can be done
by Monte Carlo simulations or by repeating the experience (but very often this may not
be possible).

Also, in general the sample size or, more precisely, the contrast between the sample
size and the number of parameters, is a crucial parameter. One may want to know the
convergence of the solution to the hypothetical case where the sample size goes to
inﬁnity.

Now, let’s see if we can answer some of the questions at the end of the last section.

• What is the probability of seeing face k up based on the data set 1 or the data set 2?

For each data set, we can compute, for example, the following quantities:

– The most probable values θM AP
k
– The mean values θM P
of θk;
– The variance values vk of θk;
– The lower values ak and upper values bk for which the probabilities P (ak <

of θk;

k

θk < bk) = 0.9.

• Do these two data sets come from the same die?

We can try to answer this question by comparing the probability laws π1(θk
x2) and π(θk
π2(θk
|
the relative entropy

x1),
x1, x2). But how to do this comparison? We may try to compute

|

|

KL(π1π2; π) =

π1(θk

x1) π2(θk

x2) ln

|

|

Z

π1(θk

x1) π2(θk
|
x1, x2)

|
π(θk

x2)

|

dθk.

(15)

If this value is near to zero, this means that the two data sets comes from different
dice.

• Is this die loaded?

We can answer this question by computing the probabilities of two hypotheses
H1 = (θ1 = θ2 = . . . = θK) and H0 = (θk

x).

= θl), i.e., P (H1|

x) and P (H0|

P (H1|

x) =

dθπ(θk = θ

x1)

|

k Z
Y

P (H0|

x) =

dθ1 . . .

dθK

π(θk

x1).

Z

Z

|

k
Y

(16)

(17)

• If I throw this die N ′ = 100 times again, what will be the number of times I will

see the face k up?
To answer this question, there are two methods:
i) Use the data set x =
n, N, K
previous methods (MAP, PM, ...) and then compute P (n′

to compute π(θ

}

{

|

x) and estimate

θ by one of the

θ, N ′ = 100, K).

b

|

b

6
ii) Try to ﬁnd the expression of P (n′

n, N, K, N ′) by following

|

k
Y

k
Y

k
Y

P (n
|

θ, N, K) =

N θnk
C nk

k (1

θk)N −nk,

−

P (n′

θ, N ′, K) =

|

P (n, n′

θ, N, K, N ′) =

θk)N ′−n′
k,

n′
N ′ θ
k

n′
k
k (1

C

−
nk+n′
nk+n′
N +N ′ θ
k
k
k

C

θk)N +N ′−nk−n′
k,

(1

−

|

|

P (n′

n, θ, N, K, N ′) = P (n, n′

θ, N, K, N ′)/P (n′

θ, N ′, K)

|

and then integrate out θ to obtain P (n′

n, N, K, N ′).

|

|

PROBLEM 3

Now, consider the case where, the observer has given to us only a subset (n1, . . . , nK ′)
of the whole data n = (n1, . . . , nK) with K ′ < K. (He just has forgotten to count and
, but he is sure that the die has K faces. In
report the numbers
}
this case we can only obtain an expression for the likelihood function if we know the
K
total number of the observations N =
k=1 nk

nk, k = K ′ + 1, . . . , K
{

K ′
k=1 nk which is

N ′ =

≥

P
θk)N −nk.

P (x

θ)

|

∝

θnk
k (1

−

K ′
P

k=1
Y

Note that this likelihood expression does not depend on the parameters
θk, k = K ′ + 1, . . . , K
. Thus, the maximum likelihood (ML) estimation approach is
{
unable to propose any values for them, while the Bayesian approach and in particular
the MAP estimation can propose a solution which depends on the choice of a priori. For
example, with a uniform prior, we have:

}

k = 1, . . . , K ′

nk
N
(N −N ′)
(K−K ′)N k = K ′ + 1, . . . , K

θk =

(cid:26)

where the ﬁrst row is common with ML and the second row is due to the uniform prior
and the normalization.

It is important to note that, while in the two previous cases, the prior law π(θ) has a
less important role, here the classical ML approach cannot give any answer the problem
and the role of prior information is crucial.

(18)

(19)

PROBLEM 4

Another interesting case is the one where we do not know the number of states (faces of
the die). For example, we have observed the following data:

, 4,

x = [4, 2, 2, 2, 1,
,
,

∗
,
, 4, 2, 2, 4, 4, 4, 3,
∗
, 3, 4, 4, 3, 3, 3, 4, 1, 2, 4, 4,

,
, 1, 4, 3, 3,
∗
∗
, 4,
,
∗
∗
,
, 4,
∗

, 3,

∗
∗

∗
∗

∗

∗

∗

∗

,

, 4,

∗
, 2,
∗
,
,
∗

∗

,

, 4, 4, 1, 1, 2, 1,
∗
, 3,
∗
,
,
∗

, 2,
∗
,
∗

∗
, 1, 1, 4, 1,

∗

∗

∗

∗

, 2, 1,

]

∗

, 1, 3, 3, 4, 3, 1, 3, 3,

∗
, 3, 3, 2,

∗

, 4, 2, 4, 2, 3, 2, 2,

, 2, 2, 1,

∗

where
may mean anything else greater than 4 or do not know. Note that these
two cases are different. In the following, we ﬁrst consider the ﬁrst case which is, in fact,
very close to the Problem 3 in the previous section, because we know exactly the nk for
k = 1, . . . , K ′ but we do not know other nk, k > K ′ nor the the true value of K > K ′
itself. However, N is given. We can only give an expression for the likelihood if we ﬁx
the value of K. Then, we can consider K = 5, 6, 7, ... and for each case compute the
results using (19):

For K = 5 we obtain: θ = [0.1300, 0.1700, 0.1700, 0.2100, 0.03200]
For K = 6 we obtain: θ = [0.1300, 0.1700, 0.1700, 0.2100, 0.01600, 0.01600]
For K = 7 we obtain: θ = [0.1300, 0.1700, 0.1700, 0.2100, 0.01067, , 0.01067, 0.01067]
and so on.

A difﬁcult question remains: How to ﬁx K? We may try to compare π(θ

x, K) for
different values of K through their entropies. We may also choose a prior for it and
compute π(θ, K
x) from which we can estimate
K.

x) or still integrate out θ to obtain π(K

|

|

|

The case where, the

in the data means do not know is more complex. If at least we
know K, then it may still be possible to write the expression of the likelihood. Let’s note
[nk, nk + n∗], k = 1, . . . , K ′
the true values of nk by Nνk. Then, we know that Nνk =
and Nνk =

[0, n∗], k = K ′, . . . , K with n∗ = N

K ′
k=1 nk. Then, we may write

∈

∗

∈

P (x

θ, ν, K, K ′) =

N −n∗ θnk
C nk

k (1

θk)N −n∗−nk

C νk

n∗ θN νk

k

(1

θk)n∗−N νk

−

−

|

or

K ′

k=1
Y

−

P

K

k=1
Y

P (x

θ, ν, K) =

|

C N νk

N θN νk

k

(1

θk)n∗−N νk.

−

K

k=1
Y

We can then try to integrate out θ from this expression to obtain P (x
out ν to obtain P (x
|
out K by summing over all values of K?

ν, K) or integrate
θ, K). But, what to do if we do not know K? Can we also integrate

|

Another question that may arise in this problem and the previous ones, is to estimate
the frequencies νk = nk/N which is not exactly the same question of estimating θk. In
the following, we consider this problem.

First consider the case of complete data

of problems 1 and 2. We may note
)
that, if we assume that the die is fair, the knowledge of the past experience (
}
{
does not change anything on the results of the future experience. But, if we do not know

n, N, K

n, N, K

{

}

if the die is loaded, then from the past experience, we can estimate θ and use it to
compute the probability of observing any event.

The situation becomes more complex if we do not know K or N or if some data are
missing as is the case in problems 3 or 4, or more generally the cases where we cannot
write easily the exact expression of the likelihood.

Consider the incomplete data problem 4 where we know N, nk and n∗, but we do
not know K and assume that the
are a priori distributed uniformly between 1 and K
(or between K ′ and K) and compute the numbers dk = (nk + n∗/K)/N (or dk = (nk +
K ′))/N). We can then say that these computed dk are good approximations to
n∗/(K
the true unobserved νk. The question is how to model this approximation. Two models
can then be used:

−

∗

i) Assume dk as the mean values of the unknown frequencies νk

dk = E

νk

=

νkp(νk) dνk

{

}

Z

or

ii) Assume each dk to be the sum of the true νk and a random error ǫk:

dk = νk + ǫk

where ǫk is assumed to be centered with unknown pdf. In both cases, we are
interested in ﬁnding p(νk

dk) or p(ν

d).

|

|

But, before going further, it is important to note that, in the following, we are not going
to analyze the original data x but the pre-processed data d. We changed the problem to
a new one: Given d can we assign or compute p(ν
Two approaches can then be used.

d).

|

Information Theory or Maximum Entropy approach:

This approach is based on the ﬁrst equation between dk and νk. It is obvious that, there
are an inﬁnite number of possible solutions to this equation. Let us denote by
this
ensemble:

P

The Maximum Entropy principle chooses the one pM E(νk) with the highest entropy

=

p : E
{

{

νk

}

=

P

νkp(νk) dνk = dk

.

}

Z

where

or, more generally, if we assume a reference (prior?) distribution q(νk), the one
pM KL(νk) which has minimum Cross Entropy or Kullback-Leibler (KL) divergence
[7, 11, 12], of p with respect to to q:

pM E(νk) = arg max
p∈P {

H(p)

}

H(p) =

p(x) ln p(x) dx,

−

Z

pM KL(νk) = arg min

KL(p, q)

p∈P {

}

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

KL(p, q) =

p(x) ln(p(x)/q(x)) dx.

We note that when q is uniform KL(p, q) =
The unique solution, if exists, is given by

−

Z

H(p) and thus pM KL(νk) = pM E(νk).

pM KL(νk) =

1
Z(λk)

q(νk) exp

λkνk

{−

}

where

where

}
and it can be shown that λk is the solution of the equation

{−

Z

Z(λk) =

q(νk) exp

λkνk

dνk,

∂ ln Z(λk)/∂λk = dk

−

which can be computed numerically. It is evident that the expressions of pM KL(νk),
Z(λk), and consequently any numerical values for the estimate

νM KL
k

= E

νk

=

{

}

Z

νkpM KL(νk) dνk

depend on the choice of q.

As a matter of algorithmic and computation of

λ (solution of the equation (29)) and

ν deﬁned in (30), it is interesting to know that they can be computed through:

b

b
D(λ) = ln Z(λ) + λtd

,

H(ν, ν(0))

(cid:9)

(cid:26)

λ = arg minλ
ν = arg minν∈C
(cid:8)
b
b

(cid:8)

where D(λ) is called the dual criterion and H(ν, ν (0)) is called the primal criterion and
where ν(0)

{
example, when q is uniform on

The expressions of dual and primal criteria also depends on the expression of q. For
, p is exponential we have

νkq(νk) dνk.

k = Eq

νk

=

}

R

(cid:9)

C

Z(λ) =

(1/λk), ln Z(λ) =

ln λk, D(λ) =

ln λk +

λkdk

−

k
X

−

k
X

k
X

k
Y

and

P (ν, ν(0)) =

ln(νk/ν(0)

k ) +

−

k
X

ν(0)
k ).

(νk

−

k
X

For other choices of q and more details on these relations refer to [13, 14, 15, 16, 17, 18,
19, 20, 21, 22].

Bayesian approach:

The Bayesian approach is based on the second equation, i.e.,, dk = νk + ǫk and we have
ν) and assign a prior q(νk) or
to ﬁnd an expression for the likelihood

(ν) = P (d

L

|

q(ν). When this done we can give an expression for the posterior πB(ν
d). Note that,
in both cases, we have to choose q(ν). The ﬁrst step, which is to ﬁnd an expression for

|

(ν) = P (d

ν), is not easy. Here are a few approaches:

L
Assuming θ = ν:

|

The ﬁrst approach consists in assuming θ = ν. Then, if we are also given N, the problem
becomes equivalent to the Problem 2 and we have:

(32)

(33)

(34)

(35)

(36)

Then, again choosing a uniform prior q(ν) = 1

k νk), we obtain

P (d

ν, N) =

|

C N dk

N νN dk

k

(1

νk)N (1−dk).

−

k
Y

π(νk

d, N) =

(Ndk

|

B

−

dk)

P
−

−

1)

Z0 δ(1

−
1, N(1

E

νk

d, N

1
−
2
−
dk when N goes to inﬁnity.

Ndk
N

=

}

{

|

.

and then we have

We see that E

νk

d, N

{

|

} 7→

not go further in this direction.

Frequentist point of view:

But, if we do not know N, we can try to integrate out N. Can we do it easily? I did

(d

ν, N) using the following arguments:

Here, we assume a priori that the die is fair and try to obtain an expression for the
likelihood
Given N and K and assuming that each through of the die is independent of all others,
we may argue on the number of possible outcomes resulting to a particular data set using
the multinomial coefﬁcient

L

|

W (n, N, K) =

N!
n1! . . . nk!

=

N!
K
k=1(nk!)

.

W (n, N, K) is the number of possible outcomes x such that the face k appears nk times
between the total possible outcomes which is K N . Thus, we may assign

Q

N, K) = W (n, N, K)/(K N ) =

P (n
|

N!
K
k=1(nk!)

.

(K N )

It is known that, using the Stirling approximation 2 the expression of this probability,

Q

when N is large, converges to

lim
N 7→∞

ln P (n
|

N, K) = H(ν) =

νk ln νk

(37)

K

−

k=1
X

2 Stirling (1692-1770) showed that xn = n!en
. This means
that, for large n we get the approximation ln(n!) = 1
2 ln(2πn) + n ln n. However, even if this is usu-
ally called Stirling’s formula, in fact, it may have been known earlier to Abraham de Moivre (see
http://www-gap.dcs.st-and.ac.uk/˜history/Mathematicians/De_Moivre.html).

nn+1/2 converges to √2πn when n goes to

∞

where νk = limN 7→∞

nk
N .

This explanation and this approximation have also been used to justify the choice
of an expression for entropy H(ν) =
k νk ln νk and a prior law for nb which is
π(n)
, so that, given a set of constraints on νk, ﬁnding the most probable
}
P
(sampling argument or maximum likelihood approach) value of n subject to those
constraints become equivalent to maximizing H(ν) subject to those constraints:

αH(ν)

exp

∝

−

{

nk = arg max

ln π(n)

= arg max

H(n)

.

n {

}

n {

}

(38)

But, we do not know either N or K. We may however try to use these expressions to

b

ﬁnd approximations to the likelihood function we need. First, we may assign

P (d

ν, N, K) = P (Nd

N, K)(1

P (Nν

N, K)),

(39)

|
and replacing for P (Nd
we may ﬁnd an expression which may be independent of N.

|
N, K) and P (Nν

−

|

|

|

N, K) and using again the Stirling formula

Integration of nuisance parameter θ:

Again here, we start by assuming N known. Then, we know the expressions of
P (d

θ, N) and P (ν

θ, N):

P (d

θ, N) =

C N dk

N θN dk

k

(1

θk)N (1−dk)

|

|

k
Y

k
Y

−

−

P (ν

θ, N) =

C N νk

N θN νk

k

(1

θk)N (1−νk).

(40)

(41)

|

and

Then we can write

|

|

P (d

ν, θ, N) = (1

P (ν

θ, N)) P (d

θ, N)

=

N θN νk

k

(1

θk)N (1−νk)

!

|

−

|
C N νk

k
Y
N θN dk
C N dk

k

−

1

−

k
Y

 

×

θk)N (1−dk ).

(1

−

(42)

ν, N). Can we
Then, we have to integrate out θ to obtain the likelihood
obtain simple expressions? Can we integrate out N too? I did not go farther in this
direction.

(ν) = P (d

L

|

Ad hoc empirical approach:

Another approach is to assign the two pdfs p(ǫ) = p(dk
which we can compute

−

νk) and the prior q(νk) from

πB(νk

dk) = p(dk

νk) q(νk) / m(dk).

|

−

(43)

Here too, the expression of the posterior pdf π(νk
depends on the choice of p(ǫ) and q(νk).

|

A question may arise here:

dk) and thus any inference about νk

Can we ﬁrst ﬁx q(νk) and compute pM KL(νk) and use it again as a prior in this Bayesian
approach?
The answer is "No", because pM KL(νk) is in fact pM KL(νk
used two times the same data dk.

dk) and doing so, we have

|

Another question is how to compare and how to use pM KL(νk

dk) and πB(νk

dk)?

My answer is that πB contains more information than that of pM KL, because to obtain
πB, we combined information about both ǫk through p(ǫ) and νk through q(νk) while to
obtain pM KL we used only q(νk). Indeed, it seems that the only consistent point estimator
of νk from pM KL is its posterior mean, while, there is not any such restriction on πB.

|

|

PROBLEM 5

gk θk = d

k
X

An important case is the one where we have only given the mean value of the face
numbers
k k θk = d0 or the more general case of the mean value of the numbers
written on the faces
k gk θk = d without any other knowledge and, in particular, without
knowing N. We need however to know K.
P

P

k gk θk are not the same. They

k k θk and E

Remember also that E
X
}
become equivalent if gk = k.

{

=

Thus, we consider the case:

P

G

=

{

}

P

and we assume to know the number of states K. The objective is to ﬁnd θk.

MaxEnt solution:

The classical answer this problem is MaxEnt which can be described as follows:
It is obvious that, there are inﬁnite number of possible solutions to the equation (44).
The Maximum Entropy principle chooses the one with the highest entropy

H(θ) =

θk ln θk.

−

k
X

The solution has the form

where

θk(λ) =

exp

λ gk

= exp

(ln Z(λ) + λ gk)

,

{−

}

{−

}

1
Z(λ)

and λ is the solution of the following equation

Z(λ) =

exp

λ gk

,

}

{−

k
X

∂ ln Z(λ)/∂λ = d

−

(44)

(45)

(46)

(47)

(48)

which can be computed numerically.

It is also easy to show that the maximum value of the entropy is

Hmax(θ) =

θk ln θk = ln Z(λ) + λd = max

ln θk(λ)

(49)

λ

which can also be written

−

k
X

max
λ

θ(λ) = exp

Hmax(θ)

.

{

}

(50)

Bayesian solution:

If we knew N, we could write the expression of the likelihood P (D = d
d =

k gknk and

k nk = N:

|

θ, N) with

P (n
|

θ)δ(N

−

k
X

−

k
X

nk)δ(d

gknk).

(51)

P

P

N

P (D = d

θ, N) =

nk=0
Y
We can also try to integrate out N:

|

|

P (D = d

θ) =

P (n
|

θ)δ(N

−

nk)δ(d

gknk).

(52)

k
X

−

k
X

∞

N

N =0
X

nk=0
Y

These computations seem to me intractable. In the following, I propose another ap-
proach:

The main idea here is that, we may account for uncertainty of this data (in particular,

because we do not know the value of N) by assuming

and by arguing on the additivity and positivity of θ we choose

p(d

θ) =

|

d

N  

−

gk θk, σ2

,

!

k
X

π(θ) = exp

H(θ)

.

{−

}

Then, the posterior is

and the MAP solution is

π(θ

d) = exp

|

1
2σ2 (d

−

(−

gk θk)2

H(θ)

,

−

)

k
X

θ = arg min

(d

θ (

−

gk θk)2

αH(θ)

−

)

k
X

b

(53)

(54)

(55)

(56)

with α = 2σ2.

Now, if we choose H(θ) =

k θk ln θk the numerical results obtained by this approach
and those obtained by using the MaxEnt solution become almost identical. However, if
we can ﬁx the value of α, we have access to the π(θ
d) which contains more information
than only one point estimator.

P

|

Combined data fusion solution:

Assume now that, not only we have the data x or n, but also d from previous section.
How to combine them? Here is my solution.

Follow the Bayesian approach of the sections 1 or 2 to write down the expression of

ln π(θ

n) =

[nk ln θk + (N

nk) ln(1

θk)] + ln π(θ) + c

(57)

−

−

the a posteriori law

|

k
X
and use the expression of π(θ

|

d) in equation (55) as the prior π(θ) here.

PROBLEM 6

Assume now that, our observer has repeated the experience L times, and before each
experience, he has changed the numbers written on each face. For example, the ﬁrst
time, he has written gk = k and for the second experience gk = k2. This is also equivalent
to the experiment of using L similar dice with different colors and different labeling on
each faces simultaneously. Then, he computed the numbers nkl.

But, assume now that, ﬁnally, he gives us only the mean values ¯nl = (1/N)

k nkl or
k gkl. The problem is similar to the previous case, but here we have L data:

dl = (1/N)

P

gkl θk = dl,

l = 1, . . . , L,

k
X

which can be written Gθ = d where G is the matrix with elements gkl. Thus, we have a
linear system of equations with K unknowns and L data. Note that here we know exactly
the values gkl.

If the experimenter has made good choices for gkl and if L = K, then we may only
try to solve that system of equations and obtain an exact solution to the problem. But,
what if L < K or if the experimenter has not made a good choice for gkl, for example,
if he has naively written gkl = kl. In both cases, the system of equations has an inﬁnite
number of solutions.

MaxEnt solution:

The MaxEnt approach is again straightforward and the solution has the form

P

(58)

θk =

1
Z(λ)

exp

λl gkl

= exp

(ln Z(λ) +

λl gkl)

,

(59)

(−

l
X

)

(−

l
X

)

(60)

(61)

(63)

(64)

(65)

(66)

(67)

where

and λ = [λ1, . . . , λL] is the solution of the following equation

Z(λ) =

exp

k
X

(−

l
X

λl gkl

,

)

∂ ln Z(λ)/∂λl = dl

−

which can be computed numerically. It is also easy to show that the maximum value of
the entropy is

Hmax(θ) =

θk ln θk = ln Z(λ) + λtd = max

ln θ(λ)

(62)

λ

which can also be written

−

k
X

max
λ

θ(λ) = exp

Hmax(θ)

.

{

}

Bayesian solution:

Following the steps of the section 5, we have

and by arguing on the additivity and positivity of θ we choose

(cid:1)

(cid:0)

p(d

θ) =

|

d

−

N

Gθ, σ2

π(θ) = exp

H(θ)

.

{−

}

Then, the posterior is

and the MAP solution is

π(θ

d) = exp

1
2σ2 k

d

−

Gθ

2

H(θ)

k

−

(cid:27)

−

(cid:26)

θ = arg min

θ

d

Gθ

2

−

k

−

αH(θ)

k
(cid:8)

(cid:9)

|

b

with α = 2σ2.

Combined data fusion solution:

Assume now that, not only we have the data x or n, but also d from the previous section.
How to combine them. Here again we can follow the Bayesian approach of the sections
1 or 2 to write down the expression of the a posteriori law

ln π(θ

n) =

[nk ln θk + (N

nk) ln(1

θk)] + ln π(θ) + c

(68)

−

−

|

k
X
and use the expression of π(θ

|

d) in equation (66) as the prior π(θ) here.

PROBLEM 7

Consider the same previous experiment, but this time, the experimenter is sure that all
dice were absolutely identical and unloaded, but he has forgotten to note the numbers he
has written on the dice faces.

However, he has also noted the mean values (1/L)

l gkl = dk. Can we be of any help

for him to ﬁnd them?

P
Thus, this time, θk = 1/K, k = 1, . . . , K and we have

gkl θk = (1/K)

gkl = dl,

l = 1, . . . , L,

(69)

and also (1/L)

k
X
l gkl = dk,

k
X
k = 1, . . . , K.

The problem becomes an interesting one, we want to compute the elements of a
matrix from its row and column sums. This mathematical problem arises in many other
applications such as computed tomography where we want to recover the pixel values
of an image from its horizontal and vertical projections.

P

Except the case of K = L = 2, we have always less data than unknowns and the
problem has an inﬁnite number of solutions. Even in the case K = L = 2 where the
number of unknowns and data are equal, the problem is still under-determined and has
inﬁnite number of solutions.

We need to question our experimenter to see if he can remember of any other infor-
mation about those numbers (prior information or constraints?) which can be helpful to
give reasonable answers about this question.

To go further in details of this problem, let’s change slightly the notation. We want to
l gkl and its

L) matrix G from its row sums rk =

estimate the elements gkl of a (K
column sums cl =

k gkl.

×

We may also note r = [r1, . . . , rK], c = [c1, . . . , cL], d = [r; c] and g a vector containing
all the elements of the matrix G concatenated column by column. Then, it is easy to see
that we can also write c = A1g, r = A2g and thus d = Ag where A1, A2 and A are,

P

P

respectively, a (K

KL), a (L

KL) and a ((K + L)

KL) matrix with A =

×

×

×

and whose elements are composed of zeros and ones.

Now, we consider two sets of answers of our experimenter: those who put determin-

istic constraints on gkl and those who put probabilistic constraints.

A2
A2

(cid:20)

(cid:21)

Deterministic constraints:

to the condition that

• gkl = gk. Then, we have rk = Lgk and we have a unique solution gk = rk/L subject
k rk = cl,
• gkl = gl. Then, we have cl = Kgl and we have a unique solution gl = cl/K subject

l = 1, . . . , L.

k gk = K

L

to the condition that

P

l gl = L

K

P

l cl = rk,

k = 1, . . . , K.

• gkl = g1k g2l. Then, we have rk = g1k
P
cl. There still remains two unknowns
and g2l ∝
and g2l are normalized, then we have a unique solution.

l g2l and cl = g2l

l g2l and
P

P

P

k g1k and we have g1k ∝

rk
k g1k. However, if g1k

P

P

P

P

• gkl are normalized as they represent a probability distribution:

l gkl =
l gkl = 1. This information is not enough to ﬁnd a unique solution. That

k gkl =

k

becomes true if gkl is separable as in the previous case.
P

P

Atλ
}

{

• gkl are normalized as they represent a probability distribution:

l gkl =
l gkl = 1 and and are distributed as uniformly as possible over the
(k, l), k = 1, . . . , K, l = 1, . . . , L
}

k
grid
P
This information may be enough to ﬁnd a solution if it exists, by maximizing
H(g) =
j gj ln gj subject to the data constraint Ag = d and the normalization
−
j gj = 1. Then the solution is given by g = 1
where
constraint

k gkl =

Z(λ) exp

{
P

P

P

.

P
P
−
D(λ) = ln Z(λ) + λtd

.

b

(cid:8)

∝

exp

λ =

(cid:9)
[Atλ]j
{

∂ ln Z(λ)/∂λj = dj which can also be computed by

λ is the solution of
arg minλ
Unfortunately, there is not an explicit expression for this solution, but it is by
construction positive (gj
) and satisﬁes the data and normalization
}
constraints for any correct data sets. Note also that this solution is not a linear
function of the data.
There is only one question remaining: Is there any other criteria H(g) which can
give these satisfactions?
To give a partial answer to this question, we may say that any convex criterion
2 which
can be used to ﬁnd a unique solution. For example, H(g) =
gives the minimum norm (generalized inverse) solution g = A+d which becomes
g = At(AAt)−1d if AAt was invertible. Note that this solution is a linear function
of the data, but, this criterion does not guarantee the positivity of the solution.
Another example is H(g) =

j ln gj which gives the solution of the form gj =
but, this criterion does not guarantee neither the positivity or the boundedness

1
[Atλ]j
of the solution. One can ﬁnd other convex criteria (see next section).

j g2

j =

P

P

g

k

k

Probabilistic constraints:

• We know that g

and that we generated g according to a reference measure q(g)
∈ C
= g0. Now, again, we can use the ME tool and search for
g
over
such that Eq
}
{
C
= d and minimizes KL(p, q). We know that the solution
g
p(g) such that AEp
}
{
λtAtg
is p(g) = 1
∂ ln Z(λ)/∂λj = dj
Z(λq(g) exp
which can also be computed by
and ﬁnally,
g = Ep
the solution
. However,
as we discussed it before, the expression of H depends on the choice q(g):
For

a closed set of real numbers and q(g) Gaussian, we have

λ = arg minλ
(cid:8)
(cid:9)
can be computed by

g = arg minAg=d
(cid:8)

D(λ) = ln Z(λ) + λtd

where λ is the solution of

H(g, g0}
(cid:9)

−

b

g

}

{

{

b

C

b

H(g, g(0)) =

g

2.

g0k

k

−

For for

a closed set of real numbers and q(g) a Lebesgue measure on

, we have

C

C

H(g, g(0)) =

ln(gj/g0j ) + (gj

g0j ),

−

−

j
X

and, ﬁnally,
For

C

a closed set of integer numbers and q(g) Poissonian, we have

H(g, g(0)) = KL(g, g0) =

gj ln(gj/g0j ) + (gj

g0j ).

−

j
X

This discussion shows a relation between the classical ME approach of the last
section and the ME in the mean as is presented here. Even if here, we have a tool to
derive the expression of the needed convex criterion, still an arbitrary remains on
the choice of

and the reference measure q(g).

• Each element gkl has been generated independently using a Gaussian random

C

• Each element gkl has been generated independently using a Gaussian random

number generator: gkl

(k, λ).

number generator: gkl

(l, λ).

∼ N

∼ N

• Two sets of numbers g1k and g2l have been generated using a Gaussian random
(l, λ2), then normalized and point-
number generator g1k ∼ N
wise multiplied: gkl = g1kg2l.

(k, λ1) and g2l ∼ N

• Each element gkl has been generated independently using a random number gener-

ator. We consider two interesting cases: gkl

(µ, λ) and gkl

(λ).

∼ N

∼ P

• The elements g1l, gk1, g1L, gK1 have been generated independently using a random
(¯gkl, 1) where ¯gkl =
(0, 1), but others are generated by gkl

number generator
1
4[gk−1,l + gk,l−1 + gk+1,l + gk,l+1].

N

Let’s consider only the case of independent Gaussian gkl

(µ, λ) and gkl

where we may be able to do all the computations.

(λ)

∼ P

∼ N

∼ N

Gaussian case:

We have:

gkl

(µ, λ)

p(gkl) = (

∼ N

−→

1
2πλ

1

)

2 exp

1
2λ

−

(cid:26)

(gkl

µ)2

.

−

(cid:27)

Then, the column sums cl and rows sums rk are also Gaussian:

rk =

gkl

(Lµ, Lλ),

cl =

gkl

(Kµ, Kλ),

∼ N

l
X

∼ N

k
X

and thus:

p(r) = (

1
2πL

K

)

2 exp

1
2Lλ

(−

(rk

Lµ)2

−

)

p(c) = (

1
2πK

L

)

2 exp

1
2Kλ

(−

(cl

−

Kµ)2

.

)

k
X

l
X

Then, we can write the expression of the posterior law:

p(gkl

r, c, λ)

|

P (gkl, r, c
|

λ) = exp

∝

×

1
2Lλ

exp

(−
with rk =

k
X

l
X

1
2λ

(gkl

µ)2

−

(cid:27)

−

(cid:26)

(rk

Lµ)2

−

exp

) ×

(−

1
2Kλ

gkl and cl =

gkl.

k
X

Kµ)2

(cl

−

)

l
X

It is then easily seen that

p(gkl

r, c, λ)

exp

J(gkl)

|

∝

1
2λ

−

with J(gkl) = (gkl

µ)2 +

(cid:26)

−

(cid:27)
(

1
K

k
X

l
X

Lµ)2 +

gkl

−

1
L

(

gkl

Kµ)2

−

l
X

k
X

is Gaussian and we can easily compute its mean and variance. To obtain the mean values,
we can compute the derivative of

J = (gkl

µ)2 +

−

1
K

(rk

Lµ)2 +

−

1
L

Kµ)2

(cl

−

k
X

l
X

which is

∂J/∂gkl = 2(gkl

µ) +

−

(rk

Lµ) +

−

(cl

Kµ)

−

2
K

k
X

and equate it to zero to obtain

gkl =

KL
KL + 2L + 2K  

µ +

1
K

rk +

cl

!

(70)

k
X
l cl is what is called the back-projection

l
X

This result is interesting, because 1
K
in computed tomography.

k rk + 1
L

P

P

and d = [

]g = Ag. Then, we have:

r
c] = [

A1
A2

We can generalize these results, if we work with the vectors g, r = A1g, c = A2g

(g0, Rg), d

(Ag0, ARgAt),

∼ N

g
d

(cid:20)

(cid:21)

g0
Ag0

,

Rg
ARg
(cid:20)

(cid:21)

RgAt
ARgAt

(cid:21)(cid:19)

∼ N

(cid:18)(cid:20)

g

∼ N

and thus

g

d

|

∼ N

(

g,

Rg), with

b

b

(cid:26)

g = g0 + RgAt(ARgAt)+(d
Rg = Rg
b
b

−

RgAt(ARgAt)+ARg

−

Ag0)

,

l
X

2
L

1
L

where (ARgAt)+ is the generalized inverse of ARgAt.

Note that when ARgAt is invertible, we have
For the particular case of Rg = λI we have

g = A−1d and

Rg = 0.

b
g = g0 + At(AAt)+(d
Rg = λ(I
b
b
For the particular case of A =

we have

−

(cid:26)

At(AAt)+A)

−

A1
A2
(cid:20)

(cid:21)
A1At
1
A2At
1

(cid:20)

AAt =

A1At
2
A2At
2(cid:21)

=

(cid:20)

KI
1

1
LI

,

(cid:21)

b

Ag0)

.

(71)

where 1 is a matrix with all its elements equal to 1. We may note that AAt is singular
Rg. Note also
1. We can however compute numerically
and its rank is K + L
that, even if a priori gkl were independent, a posteriori they are correlated.
b

g and

−

b

Poisson case: gkl

(λ):

∼ P

Here, we have:

P (gkl) = λgkl exp

λ

/(gkl!)

ln P (gkl) = (ln λ)gkl

ln(gkl!)

λ

{−

}

−→

−

−

and

Then, we can write

k
Y

and

gkl

(λ),

rk =

gkl

(Lλ),

cl =

gkl

(Kλ).

∼ P

∼ P

l
X

∼ P

k
X

P (r) =

(Lλ)rk exp

Lλ

/(rk!), P (c) =

(Kλ)cl exp

{−

}

Kλ

/(cl!)

{−

}

l
Y

P (gkl

r, c, λ)

|

∝

(λ)gkl/(gkl!)

(Lλ)rk/(rk!)

(Kλ)cl/(cl!)

k
Y

l
Y

with rk =

gkl and cl =

gkl.

k
X
r, c, λ) is also a Poisson law, but it is not easy to
It is then possible to show that P (gkl
ﬁnd an explicit expression for its mean value. However, using again the Striling formula
when working with ln P (gkl

r, c, λ) one can obtain an approximate expression for it

l
X

|

|
, r, c, λ) =

P

(λ(1 + L exp

+ K exp

rk

)),

cl

{

}

{

}

P (gkl

gk′6=k,l′6=l

|{
and thus we have

E

gkl

{

|{

gk′6=k,l′6=l

, r, c, λ

}

}

}

= λ(1 + L exp
rk
{
{
= KLλ(1/(KL) + (1/K) exp

+ K exp

cl

}

)
}
cl
{

}

+ (1/L) exp

rk

{

).
}
(72)

corresponds again to the
This is interesting, because (1/K) exp
}
famous back-projection operation in computed tomography, but here, in place of back-
projecting cl and rk themselves, their exponential values exp
are back-
projected.

+ (1/L) exp

and exp

rk

rk

cl

cl

}

}

{

}

{

{

{

CONCLUSIONS

This paper was another analysis of dice problems trying to answer some of the ques-
tions about the situations where we can use the Bayesian or the Maximum Entropy
approaches. Through this paper, we distinguished three approaches: Bayesian, classical
MaxEnt and MaxEnt on the mean. I showed some of the situations where we can use
these approaches.

The Bayesian approach can be used when we can write explicitly a probabilistic
model relating the data to the unknown parameters from which we can deduce the
expression of the likelihood and can assign an a priori law to those parameters, we
can then use the Bayesian approach to compute the a posteriori from which we can
infer about the parameters.

The classical MaxEnt approach can be used in cases where we have a set of data which
can be considered as linear constraints on a set of parameters which are themselves
a probability distribution. Then the classical MaxEnt gives the possibility of ﬁnding a
unique solution to the under-determined problem.

The MaxEnt on the mean approach can be used in cases where we have a set of
data which can be considered as linear constraints on the expected values of a set of
parameters which are the elements of a convex set on which we can deﬁne a reference
measure. Then, we can use the MaxEnt on the mean approach to compute a probability
law on that set such that the expected values of the parameters satisfy exactly the
data. We can then compute those expected values which depend on the choice of the
reference measure. We showed also that there are strong relation between the two
MaxEnt approaches.

In some cases, it may happens that we have both the moment data and the sampling
data. Then we can ﬁrst use the MaxEnt approach to assign the prior law using the
moment data and then use it with the likelihood to compute the a posteriori law of
the parameters from which we can infer about them.

Finally, even if I tried to answer to some of the questions, I also asked more questions
to be answered. We thus still have a lot to do with all the three approaches. However, it
seems that for practical applications the Bayesian approach seems to be the right and the
easiest one.

REFERENCES

1.

2.

E. T. Jaynes, “Prior probabilities,” IEEE Trans. Systems Science and Cybernetics, vol. SSC-4,
pp. 227–241, Sept. 1968.
E. T. Jaynes, “Where do we stand on maximum entropy ?,” in The Maximum Entropy Formalism
(R. D. Levine and M. Tribus, eds.), Cambridge (MA): M.I.T. Press, 1978.

4.

3.

E. T. Jaynes, “On the rationale of maximum-entropy methods,” Proc. IEEE, vol. 70, pp. 939–952,
Sept. 1982.
E. T. Jaynes, “Where do we go from here?,” in Maximum-Entropy and Bayesian Methods in Inverse
Problems (C. R. Smith and W. T. J. Grandy, eds.), pp. 21–58, 1985.
Frieden, “Dice, entropy and likelihood,” J. Opt. Soc. Amer., vol. 73, no. 12, pp. 1764–1770, 1985.

5.
6. B. R. Frieden, “Maximum-probable restoration of photon-limited images,” Applied Optics, vol. 26,

7.

8.

no. 9, pp. 1755–1764, 1987.
J. Shore and R. Johnson, “Axiomatic derivation of the principle of maximum entropy and the
principle of minimum cross-entropy,” IEEE Trans. Inf. Theory, vol. 26, pp. 26–37, Jan. 1980.
J. M. Van Campenhout and T. M. Cover, “Maximum entropy and conditional probability,” IEEE
Trans. Inf. Theory, vol. 27, pp. 483–489, July 1981.

9. C. Robert, L’analyse statistique Bayésienne. Economica, 1992.
10. C. P. Robert, The Bayesian Choice. A Decision-Theoretic Motivation. Springer Texts in Statistics,

11. S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The Annals of Mathematical

New York: Springer-Verlag, 1997.

Statistics, vol. 22, pp. 79–86, 1951.

12. S. Kullback, Information Theory and Statistics. New York: Wiley, 1959.
13. R. T. Rockafellar, Convex Analysis. Princeton University Press, 1970.
14. J. M. Borwein and A. S. Lewis, “Duality relationships for entropy-like minimization problems,”

SIAM J. Control and Optimization, vol. 29, pp. 325–338, Mar. 1991.

15. A. Mohammad-Djafari, A Matlab Program to Calculate the Maximum Entropy Distributions,

pp. 221–233. Laramie, WY: Kluwer Academic Publ., T.W. Grandy ed., 1991.

16. R. T. Rockafellar, “Lagrange multipliers and optimality,” SIAM Review, vol. 35, pp. 183–238, June

1993.

17. G. Le Besnerais, Méthode du maximum d’entropie sur la moyenne, critères de reconstruction
d’image et synthèse d’ouverture en radio-astronomie. Phd thesis, Université de Paris-Sud, Orsay,
Dec. 1993.

18. A. Mohammad-Djafari, “Maximum d’entropie et problèmes inverses en imagerie,” Traitement du

Signal, pp. 87–116, 1994.

19. J.-F. Bercher, Développement de critères de nature entropique pour la résolution des problèmes

inverses linéaires. Phd thesis, Université de Paris–Sud, Orsay, Feb. 1995.

20. A. Mohammad-Djafari, “A comparison of two approaches: Maximum entropy on the mean (MEM)
and Bayesian estimation (BAYES) for inverse problems,” in Maximum Entropy and Bayesian Meth-
ods, (Berg–en–Dal,), Kluwer Academic Publ., Aug. 1996.

21. G. Le Besnerais, J.-F. Bercher, and G. Demoment, “A new look at entropy for solving linear inverse

problems,” IEEE Trans. Inf. Theory, vol. 45, pp. 1565–1578, July 1999.

22. A. Mohammad-Djafari, “Entropie en traitement du signal,” Traitement du signal, vol. Num. spécial,

volume 15, no. 6, pp. 545–551, 1999.


5
0
0
2
 
c
e
D
 
9
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
6
1
2
1
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Perfect Tempering

M. Daghofer∗ and W. von der Linden∗

∗Institute for Theoretical and Computational Physics, TU Graz, Austria

Abstract. Multimodal structures in the probability density can be a serious problem for traditional
Markov Chain Monte Carlo (MCMC), because correct sampling of the different structures can only
be guaranteed for inﬁnite sampling time. Samples may not decouple from the initial conﬁguration
for a long time and autocorrelation times may be hard to determine.

We present a suitable modiﬁcation of the simulated tempering idea [1], which has orders of
magnitude smaller autocorrelation times for multimodal probability densities and which samples all
peaks of multimodal structures according to their weight. The method generates exact, i. e. uncorre-
lated, samples and thus gives access to reliable error estimates. Perfect tempering is applicable to
arbitrary (continuous or discrete) sampling densities and moreover presents a possibility to calculate
evidences.

EXACT SAMPLING WITH SIMULATED TEMPERING

Simulated Tempering was introduced in Ref. [1], parallel tempering, also known as
Replica Exchange Monte Carlo, in Ref. [2, 3] and both have been widely used (see
e. g. Refs. [4, 5]) to make Markov chain Monte Carlo faster. For an introduction to both
methods see Ref. [6].

Besides speeding simulations up, Simulated Tempering provides a way to obtain exact
samples from arbitrary probability density functions, see also Ref. [7]. Fig. 1 shows the
principle for a multi-modal distribution p1(X ) consisting of two Gaussians, but it does
not depend on the speciﬁed example and can thus be applied to a variety of probability
distributions. We want to draw Exact samples from the distribution p1(X ), which we
can not sample directly, where X can be a discrete or continuous quantity of arbitrary
dimension. In order to do so, we introduce an additional parameter b
and the joint
probability p(X , b ) = p(X |b )p(b ). We have large freedom in choosing p(X , b ), for
the simulation depicted in ﬁg. 1, we chose:

p(X , b m) =

p1(X )

b m p0(X )1−b m ,

1
Z

1
Zm

(1)

where Z is the overall normalization, Zm is a constant depending on b m, which deter-
mines p(b m). The additional variable b was allowed to take M + 1 discrete values b m
with b 0 = 0 and b M = 1. p0(X ) should be chosen in a way to allow generating Exact
samples easily; in our example, it was a single broad Gaussian peak. Furthermore, its
range in X -space should be broad enough to cover all structures of p1(X ).

We then do Markov chain Monte Carlo in the {X , b }-space, where we alternate
a couple of sweeps in X -space with moves in b -direction. In b -direction, b m′ with

m′ = m ± 1 is proposed with equal probability. It is accepted with probability

pacc(b m → b m′|X ) = min

p(b m′|X )
p(b m|X )
p1(X )
p0(X )

(cid:18)

(cid:18)(cid:16)

, 1

= min

(cid:19)
m′ −b m

b

(cid:18)
, 1

·

Zm
Zm′

.

(cid:19)

p(b m′, X )
p(b m, X )

p(X )
p(X )

, 1

=

(cid:19)

(2)

= min

(cid:17)
In X -space and for b m 6= 0, usual Metropolis updates are employed. A special case arises
for X -moves at b m = 0. In this case p(X |b = 0) (cid:181)
p0(X ) and we are able to draw a new
exact sample X ′ distributed according to p0(X ), which gives us a sample X ′ uncorrelated
from X .

An example of the resulting random walk is depicted on the ’ﬂoor’ of Fig. 1. When-
ever this random walk reaches b = 0, a new exact sample from p0 is drawn independent
from the current state of the Markov chain so that the walk forgets its past. The MC time
needed for one exact sample is thus given by the time needed by the Markov chain to
travel from b 0 = 0 to b M = 1 and back again.

)

,b
x
(
p

12

10

8

6

4

2

0

1

b =b

=1

M

b  

 

e
m

i
t
 

n
o

i
t

l

a
u
m
S

i

b =0 

4th bin 

3rd bin 

2nd bin 

1st bin 

0.5

0

−0.5

x

0

−1

1

0.5

FIGURE 1. Example for a Simulated Tempering run. On the ‘ﬂoor’, the Markov chain travels through
the {x, b }-space, the larger dots are the obtained samples, the dotted lines show the way the Markov
process has taken. Via b 0 = 0, the walk reaches both peaks at b M = 1, although no direct tunneling between
them occurs. The peaks (solid lines) are the probabilities p(x|b ) for the various discrete b -values. The
samples drawn at a certain temperature obey this distribution.On the right hand ‘wall’, the vertical axis is
the time axis of the simulation; one sees the wandering of the random walk through the temperatures. The
thick lines are inserted where the walk reaches b 0 = 0, i. e. where an independent exact sample is drawn
from p0 = p(x|b = 0) (chosen as a single broad Gaussian peak). At these points, the walk forgets its past
and a new uncorrelated bin starts.

A plain MCMC run would instead be trapped in one of the two peaks and rarely tunnel
to the other. Repeating several plain MCMC runs and taking their average would give
the wrong expectation value ¯x = 0, because the different weight of the peaks would not
be accounted for.

b
EXPECTATION VALUES AND ERROR ESTIMATES

As the {X , b }-samples obtained by the simulation obey p(X , b ), the X drawn at a given
temperature b m obeys p(X |b m). Expectation values for b M = 1 are therefore calculated
from all (correlated and uncorrelated) samples obtained at a given temperature:

¯X(b M = 1) =

X j =

1
NM

NM,ind

Ni

Xi,n

i=1

n =1

h ¯X(b M = 1)i =

hX ji = hX i

1
NM
1
NM

NM(cid:229)

j=1

j

(3)

(4)

The X j are the measurements obtained at the desired temperature b M = 1, their index
j was broken into i and n with i denoting the independent and uncorrelated bins and n
labeling the correlated measurements within one bin, see Fig. 1. NM,ind is the number
of independent bins which contain at least one sample drawn at b M, and Ni the number
of measurements within the i-th bin. NM = (cid:229) NM,ind
i=1 Ni is the total number of times the
simulation has visited the desired temperature b M = 1. A bar denotes the sample mean
obtained in the Monte Carlo run, h. . .i denotes an expectation value over all samples.
The sample mean is obviously unbiased.

It is worth noting that measuring the bin averages does not give the same result,
because the probability for a move in b -direction, and thus the number of measurements
(Ni) taken in a bin before the walk returns to b = 0, is a random variable and depends
on the current sample X :

Nind

(cid:229) Ni

h

1
Nind

i=1

n =1 Xi,n
Ni

i 6= h

1
Nind

i=1

Nind

(cid:229) Ni

n =1 Xi,n
¯Nbin

i = h

i = hX i

(5)

i,n Xi,n
Nind ¯Nbin
=N

(cid:229) Nind

Here, ¯Nbin = 1
i=1 Ni is the average number of measurements per bin. For the same
Nind
reason, taking only the ﬁrst sample of each bin does not give correct results. For a multi-
modal p1(X ) with a different height (and/or width) of the peaks as in Fig. 1, the Markov
Chain may visit the smaller peak very often, but it will stay at the larger one longer.

| {z }

The independent samples provide a way to analyze correlations and to calculate
reliable error estimates [7]. When calculating the variance of the estimate ¯X , the new
labels i and n become useful as it is now important to distinguish between correlated
and uncorrelated samples:

h ¯X 2i =

Xi,n X j,m i =

(6)

1

N2 (cid:229)

1

N2 (cid:229)

Ni

N j

h

h

i, j

n =1

m =1
N j

Ni

i, j

n =1

m =1

=

D Xi,n

D X j,m i +

2hX i

N2 h(cid:229)

N j

j

i,n

D Xi,n i +

hX i2
N2 h(cid:229)

i, j

NiN j

i =

=N

| {z }

=N2

| {z }

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
=

1

N2 (cid:229)

Ni

h
n ,m =1

i

D Xi,n

D Xi,m i +

D Xi,n i

h

D X j,m i

+

D Xi,n i

+hX i2 ,

1

N2 (cid:229)

Ni

h

i6= j

n =1

N j

m =1

2hX i
N

h(cid:229)

i,n

=0

=0

=0

n ,m

D X j,m i = h(cid:229)
D Xi,n
where h(cid:229)
are from different bins, h(cid:229) Ni
equivalent. From Eq. 6, it follows for the variance

D Xi,n ih(cid:229)
|
D Xi,n

n ,m =1

|

n

}

D X j,m i for i 6= j, because the measurements
{z
m
D Xi,m i is independent of i, because all bins are

{z

{z

}

|

}

var( ¯X) = h ¯X 2i − h ¯Xi2 = h ¯X 2i − hX i2 =

D Xi,n

D Xi,m i .

(7)

Ni

Nind
N2 h

n ,m =1

The unknown expectation value h(cid:229) Ni
D Xi,m i is estimated from the Monte Carlo
D Xi,n
(cid:229) Nind
(cid:229) Ni
run, thus h(cid:229) Ni
D Xi,m . However, the variance
D Xi,m iest ≈ 1
i=1
Nind
depends on the determination of the above expactation value, so it can only be correct,
if all modes of p1 have been sampled sufﬁciently.

D Xi,n

D Xi,n

n ,m =1

n ,m =1

n ,m =1

BEHAVIOR IN ONE DIMENSION

Although nobody would think of using Monte Carlo simulation for one dimensional
problems, as much more efﬁcient approaches are available, it is interesting to examine
the Markov matrix for a Simulated Tempering simulation in the two-dimensional X -b -
space with discretized X . The probability density p1(x) for b = 1 was chosen to consist
of two Gaussians well separated from each other and p0(x) was chosen to be constant.
For Simulated Tempering, the number of b -slices was varied from two (just b = 0 with
p(X |b = 0) = p0 and b = 1 with p(X |b = 1) = p1) to ﬁve. The intermediate b -values
were chosen so as to give approximately the same transition rate between all pairs of
adjacent b -values. Autocorrelation and thermalization are largely determined by the
second largest eigenvalue (e2) of the Markov matrix, i. e. the one with magnitude closest
to one. The autocorrelation time was approximately calculated as t AC ≈ 1/(1 − |e2|).

Fig. 2 shows this autocorrelation time as a function of the distance of the two peaks.
One sees that more b -slices become necessary as the distance increases. For plain
Markov chain Monte Carlo in the one-dimensional discrete X -space, the autocorrelation
time far exceeded the range plotted in Fig. 2 even for a distance of d = 12 (t AC ≈
2.6499e + 03) and its calculation is numerically instable for larger distances.

The columns of the Tempering Markov matrix which correspond to b = 0 are iden-
tical, which means just that whenever the current state of the chain is at b = 0, the
outcome of the next move will not depend on the current position in X -space.

PARALLEL TEMPERING

Another method similar to Simulated Tempering Parallel Tempering, also called Ex-
change Monte Carlo, see Refs. [2, 6]. In this method, we have M copies of X at the M

(cid:229)
(cid:229)
(cid:229)
(cid:229)
Nb  = 2 (b =0 and b =1 only)
Nb  = 3
Nb  = 4
Nb  = 5

C
A

 t

 »
)

V
E
d
n

 

2
−
1
(
/
1

300 

200 

100 

40 

40 

400 
distance between peaks

1600 

FIGURE 2. Autocorrelation time for the Simulated Tempering algorithm in 1D depending on the
distance between the two peaks for two to ﬁve b -slices. The distance is measured in multiples of the
width s of the Gaussian.

values for b

.

Instead of the space {X , b m} as in Simulated Tempering, we now consider the product
space {X0, X1, . . . , Xm, . . . , XM} where the conﬁguration Xm is at the temperature b m. At
every b m, there is exactly one conﬁguration X , denoted by Xm. As Xm is at b m, it obeys
the distribution p(X |b m). The probability of ﬁnding a certain X1 at b 1, X2 at b 2 and so
on is given by the product of the individual probabilities:

p

(X0, b 0), (X1, b 1), . . ., (XM, b M)

=

p(Xm|b m) =

p1(Xm)

b m p0(Xm)1−b m

M

m=0

M

m=0

1
Zm

(cid:0)

(cid:1)

(8)
We now do Markov chain Monte Carlo again with this product probability. The prod-
uct of the Zm gives just a constant, and they therefore do not affect the simulation.
In X -space, Metropolis Monte Carlo updates are performed for all b s independently.
New conﬁgurations X ′
m are accepted with probability pacc = min
=
( p1(X ′)
p1(X) )b m( p0(X)

min
, as for a usual Metropolis random walk, because all the
other factors cancel out. For b = 0 a new sample is drawn directly from p0(X ). Alter-
nated with the updates in X -space, it is proposed to swap conﬁgurations Xm and Xm+1 at
adjacent b -values:

p(...,(Xm,b m),...)
p(...,(X ′
m,b m),...)

p0(X ′))b m, 1

, 1

(cid:0)

(cid:1)

(cid:0)

(cid:1)

{(X0, b 0), (X1, b 1), . . . ,

{(X0, b 0), (X1, b 1), . . .,

(Xm, b m), (Xm+1, b m+1)
↓
(Xm+1, b m), (Xm, b m+1)

, . . . , (XM, b M)}

, . . . , (XM, b M)} ,

and these moves are accepted with probability

pacc = min

p
p

. . . , (Xm+1, b m), (Xm, b m+1), . . .
. . . , (Xm, b m), (Xm+1, b m+1), . . .
(cid:1)
(cid:1)

(cid:0)
(cid:0)

 

, 1

=

!

(cid:213)
(cid:213)
= min

p1(Xm)
p0(Xm)

 (cid:18)

(cid:19)

(cid:18)

p1(Xm+1)
p0(Xm+1)

(cid:19)

b m+1−b m

b m−b m+1

, 1

.

!

The conﬁguration currently at b = 1 obeys our desired distribution as it does for
Simulated Tempering. During the Monte Carlo run, it will eventually get swapped
to b = 0, where a new sample is drawn. This time, however, the random walk does
not completely forget its past, which can be inferred from the Markov matrix for a
similar toy situation as for Simulated Tempering above. Suppose, we have three b -
values b 0 = 0, b 1, b 2 = 1, and the following temperature swaps occur in the Markov
chain Monte Carlo:

˜0
2
˜1
where a tilde means that an exact sample is drawn from p0. All Conﬁgurations have
now been at b = 0, but the columns of the matrix corresponding to the above sequence
of swaps are still not equal, which means that the current state of the Markov chain
still depends on its initial state. However, these correlations are small after an initial
thermalization and autocorrelation times are short.

˜0
˜1
˜2

˜0
˜1
2

2
˜0
1

2
1
0

→

→

→

→

,

NEEDED PARAMETERS

In order to do Simulated or Parallel Tempering, we have to adjust the values for the b m
and the Zm, see eq.(1). The b m-values have to be dense enough to give a considerable
overlap of p(X |b m) and p(X |b m±1). To see this, we have a look at the probability to go
from b m to b m′:

p(b m → b m′) =

dX min

p(X |b m) =

ZX

ZX

=

=

dX min

1
p(b m)

ZX

, 1

p(b m′|X )
p(b m|X )
p(X |b m′)p(b m′)
p(X |b m)p(b m)

(cid:19)

(cid:18)

(cid:18)

, 1

p(X |b m) =

(cid:19)

dX min

p(X |b m′)p(b m′), p(X |b m)p(b m)

(cid:0)

(cid:1)

So, unless this minimum value is large enough for some X , the walk will not move from
b m to b m′. On the other hand, we want to have as few b -values as possible between
b = 1 and b = 0. The b -values can be adjusted in a Parallel Tempering prerun, where a
new value is inserted whenever the swapping rate between adjacent b s is too low.

The ideal Zm needed for Simulated Tempering would make all b -values equally likely.
This prevents the Markov chain from spending too much time at on single temperature
and thus speeds travel from b = 0 to b = 1 and back again. This leads to:

= p(b m) =

dX p(X , b m) =

p1(X )

b m p0(X )1−b m

(9)

1
M

ZX

1
Z

ZX

dX

1
Zm

Zm (cid:181)

dX p1(X )

b m p0(X )1−b m

⇓

ZX

The weight Z(b = 1) gives the model evidence, which can only be determined in terms
of Z(b = 0). The weights can be obtained from the visiting frequency for the b -values
in Simulated Tempering preruns, but this is rather difﬁcult, because they may differ
by orders of magnitude. They are not needed for Parallel Tempering, where they cancel
out, but the evidence can still be calculated with a procedure similar to thermodynamical
integration, see Ref. [5]. With the random samples produced at b m, we can estimate Zm+1
for b m+1:

Zm+1 =

dX p1(X )

b m+1 p0(X )1−b m+1 =

ZX

ZX

=

dX

p1(X )b m+1 p0(X )1−b m+1
p1(X )b m p0(X )1−b m

Zm+1
Zm

=

dX

ZX

(cid:18)

(cid:19)

⇓
p1(X )
p0(X )

b m+1−b m

|
p(X |b m)

· p1(X )

b m p0(X )1−b m

=Zm·p(X|b m)

{z

}

(10)

(11)

The evidence Z(b = 1) is the product of all the measured ratios:

Z(b = 1) = ZM = Z0 ·

M−1

m=0

Zm+1
Zm

.

Care must be taken in evaluating this quantity, because the conﬁgurations are inter-
changed between b -values and the measurements obtained for the different b -values
are therefore heavily correlated.

BEHAVIOR IN HIGHER DIMENSIONS

In this section, we examine the behavior of the Tempering algorithm in higher dimen-
sions. We chose p0 as one single broad Gaussian with width s 0 = 1 centered at X = 0
and the wanted probability p1 consisted of two Gaussians of width s = 0.04 centered at
X = (0.3, 0.3, . . .) and X = (0.8, 0.8, . . .), which were multiplied by 5000, so as to yield a
norm n = 10 000. Figure 3, left panel, shows the number of MC updates needed for one
independent sample. One sees that the increase in needed samples with the dimension of
the problem approximately obeys a power law. For all presented dimensions, the results
for the norm were consistent with the errorbars (see Fig. 3, right panel) and likewise
the average for X , i. e. the simulation found both peaks. 100 sweeps were performed
between b -moves, the b m and Zm were adjusted in a parallel tempering prerun.

(cid:213)
.
s
a
e
m

 
.

d
n

i
 
/
 
s
e
v
o
m
C
M

 

5.e6

2.e6

1.e6

5.e5

2.e5

1.e5

3.e4

1

)

 æ

Z

 

(Æ
g
o

l

9.5

9.4

9.3

9.2

9.1

9

8.9
0

 2

10

15

2

4

6

10

12

14

16

8

D

 5

D

FIGURE 3. Number of needed MC updates per independent sample (left panel) and obtained evidence
(right panel) for various dimensions D.

CONCLUSIONS

Simulated Tempering provides a way to draw exact, i.e. completely uncorrelated samples
from arbitrary distributions in high dimensions. The peaks of multimodal densities
are sampled with their respective weights. The parameters b m and Zm needed for the
Simulated Tempering run can be adjusted in a Parallel Tempering prerun. While the
Parallel Tempering algorithm itself does not provide perfectly uncorrelated samples, its
autocorrelation time is small. For practical purposes, it is a robust alternative, because it
does not need the parameters Zm. Both methods allow to calculate model evidences.

This work has been supported by the Austrian Science Fund (FWF), project no. P15834.

ACKNOWLEDGMENTS

REFERENCES

1. E. Marinari, and G. Parisi, Europhys. Lett., 19, 451–458 (1992).
2. K. Hukushima, and K. Nemoto, J. Phys. Soc. Japan, 65, 1604 (1996).
3. K. Hukushima, H. Takayama, and K. Nemoto, Int. J. Mod. Phys. C, 7, 337 (1996).
4. W. Kerler, and P. Rehberg, Phys. Rev. E, 50, 4220–4225 (1994).
5. K. Pinn, and C. Wieczerkovski, cond-mat/9804109 (1998).
6. E. Marinari, “Optimized Monte Carlo Methods,” in Advances in Computer Simulation, edited by J.

Kertesz and I. Kondor, Springer, Berlin, 1997, URL cond-mat/9612010.

7. C. J. Geyer, and E. A. Thompson, J. Amer. Statist. Assoc., 90, 909 (1995).


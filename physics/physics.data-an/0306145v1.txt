Macrostate Data Clustering

Daniel Korenblum∗ and David Shalloway†

Biophysics Program

Dept. of Molecular Biology and Genetics

Cornell University

Ithaca, NY 14853

Abstract

We develop an eﬀective non-hierarchical data clustering method using an analogy to the dynamic

coarse-graining of a stochastic system. Analyzing the eigensystem of an inter-item transition matrix

identiﬁes fuzzy clusters corresponding to the metastable macroscopic states (macrostates) of a

diﬀusive system. A novel “minimum uncertainty criterion” determines the linear transformation

from eigenvectors to cluster-deﬁning window functions. Eigenspectrum gap and cluster certainty

conditions identify the proper number of clusters. The physically-motivated fuzzy representation

and associated uncertainty analysis distinguishes macrostate clustering from spectral partitioning

methods. Macrostate data clustering solves a variety of test cases that challenge other methods.

PACS numbers: 02.70.-c, 02.70.Hm, 02.50.Fz, 89.75.Kd

3
0
0
2
 
n
u
J
 
9
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
4
1
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗Current address: Gene Network Sciences, Ithaca, NY 14850
†Electronic address: dis2@cornell.edu

1

I.

INTRODUCTION

Finding subgroups or clusters within large sets of items is a problem that occurs in many

contexts from astronomy to integrated chip design and pattern recognition ([1, 2, 3, 4] for

reviews). DNA microarray gene expression analysis and bioinformatic sequence comparisons

are recent and important applications in molecular biology [3, 5].

The clustering problem may be posed in two ways: In the ﬁrst case (e.g., DNA microar-

rays), NM measurements are made on each of the N items. The N

NM measurement

matrix X is then used in a problem-speciﬁc manner to compute a symmetric N

×

N dis-

×

similarity matrix D. Each oﬀ-diagonal element Dij provides an inverse indicator of the

correlations between the measurements of items i and j. A straightforward method is to set

Dij =

(Xia −

Xja)gab(Xib −

Xjb)

(1)

1/2

,

#

NM

"
Xa,b=1

where g is a problem-speciﬁc Euclidean metric tensor. This allows preconditioning of the

scales of the diﬀerent measurements and, by using non-diagonal g, adjustment for measure-

ment correlations (particularly important if some measurements are replicates). Statistical

noise and complexity can be reduced by using singular value decomposition to pre-identify

principal components of X that span much of the variation in the measurement space. This

facilitates identiﬁcation of clusters “by eye” or with various heuristics (e.g., [6, 7, 8]).

In the second case (e.g., pairwise gene sequence comparisons), the primary data are

measures of dissimilarities between pairs of items: In this case D is deﬁned, but there is no

measurement matrix X and the elements of D may not satisfy the triangle inequality.

Early clustering methods were “hierarchical,” generating open binary trees which can (de-

pending on the selected cross-section) dissect the items into any number of clusters between

1 and N. In these methods, the choice of the optimal number of clusters is an independent

problem [2, 9, 10]. “Agglomerative” hierarchical methods iteratively join items together

to form a decreasing number of larger clusters; “divisive” hierarchical methods use succes-

sive subdivision to generate an increasing number of smaller clusters. While agglomerative

methods can be inexpensive, they usually use only local and not global information, which

weakens performance [2]. While divisive methods can use global information, they can have

high complexity in N and thus can be too expensive for large problems. A weakness of

2

both types of hierarchical methods is that they cannot repair defects from previous stages

of analysis.

Some clustering methods are based on analogies to physical systems in which macroscopic

structure emerges from microscopically-deﬁned interactions. A number have used analogies

to statistical mechanical phase transitions [11, 12, 13, 14, 15], while others have used chaotic

[16] or quantum mechanical [17] systems as analogs. Most of these have the advantage of

being “fuzzy”—in addition to assigning items to clusters, they provide a continuous measure

of the probability or strength of the assignment of each item.

Clustering can also be performed by objective function optimization.

If there is an a

priori model for the structure of the clusters in the measurement space (e.g., as a collection

of Gaussians), then a corresponding parametric objective function can be used. Otherwise

a non-parametric objective function may be useful. For example, graph theory clustering

methods treat the items as nodes of a graph whose interconnecting edges have “aﬃnities”

or “weights” determined from D ([18, 19], for review). They typically use “min-cut” or

“normalized-cut” objective functions and search for the (sometimes “balanced”) graph par-

titioning that minimizes the (sometimes normalized) sum of the weights of the cut edges.

“Spectral graph theory” [20] methods use the eigenvectors of the aﬃnity matrix (or the

closely related generalized Laplacian matrix) to assist the process. Spectral bipartitioning

([21], for history and review), which uses one eigenvector, can be applied recursively for hier-

archical dissection [22]; and the development of non-hierarchical methods for the concurrent

use of multiple eigenvectors is an active topic of research ([19, 23], for reviews).

We present here a novel, non-hierarchical, fuzzy clustering method which uses an anal-

ogy between data clustering and the coarse-graining of a stochastic dynamical system. The

items are regarded as microstates that interact via a dynamical transition matrix Γ, which

is derived from D. Clusters are identiﬁed as the slowly-relaxing metastable macroscopic

states (macrostates) of the system. These are computed by concurrently using multiple

eigenvectors of Γ in the same way that macrostates of a continuous diﬀusive system are

identiﬁed from the eigenfunctions of the Smoluchowski operator [24]. The number of clus-

ters is algorithmically determined by the spectral properties and cluster overlap criteria.

We demonstrate that the method can solve diﬃcult problems, including ones in which the

clusters are irregularly shaped and separated by tortuous boundaries.

3

II. METHOD

A. Macrostates and stochastic coarse-graining; a brief overview

Coarse-graining is used in nonequilibrium statistical physics to reduce the dimensional-

ity and complexity of the dynamical description [25].

In the usual situation, the system

is initially described microscopically by a ﬁne-grained ﬁrst-order equation speciﬁed over a

conﬁguration space of microscopic states (microstates). Microscopic degrees-of-freedom cor-

responding to very rapid motions are removed by (possibly non-linear) projection. This

generates a coarse-grained master equation with fewer degrees-of-freedom that describes the

slower dynamics of the system’s macrostates. Each macrostate corresponds to a subregion of

conﬁguration space that has been projected onto one value of the macroscopic parameters.

For example, to describe Brownian motion of pollen in water, the (fast) water molecule

degrees-of-freedom are projected out leaving only the (slow) coordinate of the pollen to

parameterize the macrostates. In this example the macrostates are continuously parame-

terized, but they may also be discrete. For example, to describe chemical reactions, each

macrostate is a chemical state, a subregion of conformation space which includes all vibra-

tions, translations, and rotations of a speciﬁc metastable, covalently-bonded arrangement of

atoms.

Coarse-graining projections are not arbitrary: the utility of the resultant macroscopic

description depends upon the existence of a suﬃcient disparity between τ local, the time-

scale of the fast (projected-out) motions (which generate ergodicity within the macrostate),

and τ global, the time-scale of the remaining slow motions (which are required for ergodicity

between macrostates). Appropriate projections can sometimes be chosen heuristically when

the disparity between τ local and τ global is large and subjectively obvious. When this is not

so, projections into discrete macrostates can be selected by analyzing the eigenspectrum of

the microscopic stochastic dynamics. This procedure is described in detail in Refs. 24 and

26. We summarize the salient points here.

Consider the example illustrated in Fig. 1A of a thermal ensemble of systems having

microscopic parameter x and potential energy V (x). The bimodal equilibrium probability

density is given by the Gibbs-Boltzmann distribution peq(x)

exp[

βV (x)], where β is

∝

−

the inverse temperature in inverse energy units. If system dynamics are overdamped (i.e.,

4

diﬀusive), then the nonequilibrium probability distribution p(x; t) evolves in time according

to the ﬁrst-order Smoluchowski equation

where Γ is the kernel of an operator determined by V , the temperature, and the diﬀusion

constant. Once the eigenfunctions and eigenvalues of Γ have been determined, the general

solution to Eq. (2) can be expanded as

∂p(x; t)
∂t

Z

=

Γ(x, x′) p(x′; t) dx′ ,

p(x; t) =

cn e−γnt ϕn(x) ,

∞

n=0
X

(2)

(3)

where the eigenvalues and right-eigenfunctions of Γ are

γn and ϕn(x), respectively, and

−

the expansion coeﬃcients cn are determined by the initial conditions p(x; 0). (Without loss

of generality we normalize ϕ0 so that c0 = 1, and assume that eigenfunctions are ordered

according to the magnitudes of their eigenvalues.)

We always have γ0 = 0 and ϕ0(x) = peq(x), corresponding to the stability of the Gibbs-

Boltzmann distribution. The other γn are non-negative and determine the rates of relaxation

towards this equilibrium state. While ϕ0 is non-negative everywhere, the other eigenfunc-

tions take both positive and negative values, and the exponential decays of their amplitudes

generate probability ﬂuxes. For illustration, Fig. 1A, panel b displays (for a selected tem-

perature) ϕ0 and the “slow” right-eigenfunction ϕ1. If c1 > 0, p(x; 0) will have a probability

excess (relative to peq) for x < 0 and a deﬁciency for x > 0. These overall deviations from

equilibrium will decay away as exp(

0. The “fast” eigenfunctions ϕn>1 will have

γ1t)

−

→

more nodes than ϕ1 and their more rapid decays will transport probability over smaller

regions.

Suﬃciently large potential energy barriers can separate conﬁguration space into localized,

dynamically metastable macrostate regions, each having the property that τ local, the time

scale for relaxation of p(x; t) towards peq(x) within the region, is much less than τ global, the

time-scale for probability to enter or leave the region. τ local and τ global are determined by the

eigenvalues, and a disparity between them will correspond to a gap in the eigenspectrum.

If there are m macrostates, a gap will occur between γm−1 and γm: there will be m slow

modes generating inter-macrostate probability equilibration, and the remaining fast modes

will generate intra-macrostate relaxations.

5

For example, in Fig. 1A the energy barrier centered at x = 0 separates conﬁguration space

into two macrostates a and b (roughly containing the regions x < 0 or x > 0, respectively).
Correspondingly, there is a spectral gap γ1 ≪
transfer of probability between a and b that is generated by the slow decay of the amplitude

γ2; so m = 2. γ1 is the rate of the slow

of ϕ1. Thus, τ global

γ−1
1 . The larger values of the γn>1 correspond to the fast decays of

∼

the more-rapidly varying ϕn>1, corresponding to intra-macrostate probability relaxations.

The slowest of these rates, γ2, determines the time needed for local equilibration. Thus,

τ local

γ−1
2 .

∼

In this simple case, it is tempting to “crisply” deﬁne the macrostates by inspection

as the regions x > 0 and x < 0. However, this is inapt for two reasons: (1) A sharp

boundary at x = 0 introduces high-frequency dynamical modes and thus is incompatible

with a consistent low-frequency dynamical description; and (2) Subjective inspection and

barrier identiﬁcation are not possible in multidimensional problems. Instead, we use this

example to show how the correct “fuzzy” macrostates can be identiﬁed (without subjective

inspection) by a generalizable algorithm:

The starting point is the recognition of the spectral gap γ1 ≪

γ2. When t > γ−1

2 , the

values of p(x; t) for all x < 0 or all x > 0 will be highly correlated, and relative equilibrium

within (but not between) these regions will have been achieved. Therefore, in this temporal

regime p(x; t) can be well-approximated by an expansion within the rank-2 (in general,

rank-m) macrostate subspace spanned by ϕ0 and ϕ1, and only the ﬁrst two terms in the

summation in Eq. (3) need be kept. To obtain a probabilistic description, we replace this

truncated eigenfunction expansion by an expansion in the alternative basis provided by the

non-negative macrostate distributions ϑa(x) and ϑb(x) (to be deﬁned precisely below) shown

in Fig. 1A, panels c and d. ϑa (or ϑb) is approximately proportional to ϕ0 for x < 0 (or

x > 0) and is approximately 0 for x > 0 (or x < 0). Thus, Eq. (3) can be replaced by the

macrostate expansion

p(x; t)

pα(t) ϑα(x) ,

≈

α
X

(4)

where Greek letters index macrostates and sums over Greek letters indicate sums over all

macrostates. (We assume the normalization

ϑα(x) dx = 1.) Since ϑa and ϑb have sig-

niﬁcant support only for x < 0 and x > 0, respectively, pa(t) and pb(t) specify the time-
R

dependent amounts of probability in these regions. Their dynamics are described by the

6

coarse-grained macrostate master equation

where Γβα is the macrostate transition matrix. As t

, Eq. (4) reduces to

dpα(t)
dt

=

Xβ

pβ(t) Γβα ,

lim
t→∞

p(x; t) = ϕ0 =

→ ∞
peq
α ϑα(x) .

α
X

where peq

α is the total probability contained in the macrostate region α at equilibrium.
The ϑα implicitly deﬁne the macrostate regions. To make this explicit, we deﬁne

macrostate window functions

peq
α ϑα(x)
ϕ0(x)
Eq. (6) and the non-negativity of the ϑα imply that

wα(x)

≡

.

wα(x)

0 ,

≥

wα(x) = 1 ,

α, x

∀

x .

∀

α
X

wα(x) speciﬁes the probability of assignment of microstate x to macrostate α. The window

functions corresponding to ϑa and ϑb are shown in Fig. 1A. They deﬁne a fuzzy dissection

of conﬁguration space into the macrostate regions x < 0 and x > 0.

Now we can address the precise deﬁnition of the ϑα themselves. Because they span the

macrostate subspace, they must be linear combinations of the slow eigenfunctions:

ϑα(x) =

Mαn ϕn(x) .

(9)

Since the ϕn are smooth, the ϑα, and hence the wα, must also be smooth. This induces an

unavoidable uncertainty in microstate assignment. For example, in Fig. 1 the assignments

are almost certain for

x

|

| ≫

0 where wα ≈

1, but are highly uncertain for x

0 where

≈

wα(x)

0.5. The essential point is to choose M, and hence the ϑα and wα, so as to

≈

maximize certainty: We deﬁne Υα, the uncertainty of macrostate α, as the sum of its

equilibrium probability-weighted overlaps with other the other macrostates, relative to its

total probability:1

wα(x)wβ(x)peq(x) dx
wα(x)peq(x) dx

.

β6=α

Υα ≡ P

R

R

1 This deﬁnition is motivated by analysis of the experimental macrostate preparation and measurement

process [24].

m−1

n=0
X

7

(5)

(6)

(7)

(8a)

(8b)

(10)

Using Eqs. (6), (7), and (8b), the macrostate certainty Υα is

Υα ≡

1

−

Υα = (peq

α )−1

w2

α(x)peq(x) dx .

Z

(11)

We choose M so as to maximize the geometric mean of the Υα subject to the constraints

of Eq. (8). This minimum uncertainty criterion minimizes macrostate overlap and, in the

example of Fig. 1A, results in the ϑα and wα shown in panels c and d. The amount of

overlap of these optimized macrostate functions depends on the magnitude of the spectral

B. Adapting macrostate coarse-graining to data clustering

To adapt the physical coarse-graining procedure to data clustering, we make the compu-

tational analogy

microstates, macrostates, Γ

items, clusters, f (D−1)

. In this analogy,

{

} ↔ {

}

the continuous conﬁguration space of microstates x is replaced by a discrete space of items

i : 1

i

≤

≤

N, and the probability distribution p(x, t) is replaced by p(t), the vector of

individual item probabilities pi(t) (e.g., see the simple example in Fig. 1B). Since p(t) is a

probability vector, it must satisfy

gap.

where

By analogy with Eq. (2), we assume that the dynamics in the item-space are described

by the microscopic master equation

where Γ is a ﬁrst-order transition matrix. Non-negativity of each pi(t) under time evolution

requires that

and conservation of probability requires that

0 ,

i, t,

pi(t)

≥
p(t) = 1 ,

∀

∀

t,

1

·

1i = 1 ,

i .

∀

dp(t)
dt

= Γ

p(t) ,

·

Γij ≥

0 ,

i

= j ,

1

Γ = 0 .

·

8

(12a)

(12b)

(13)

(14)

(15)

6
The heart of the analogy is to assume that Γij (i

= j) depends on Dij, the dissimilarity

between items i and j. If D were embeddable as a distance matrix in a metric space (e.g., as

when it is computed from a measurement matrix X), then Γ could, in principle, be computed

by solving a multidimensional diﬀusion equation in the continuous space. However, this

would be extremely expensive. Instead, we model Γ from D using the following heuristic

argument: A starting point is to set Γij = (Dij)−2 by analogy to the rate of diﬀusion

between two isolated microstates in one-dimension. However, this does not account for the

interception of probability ﬂux by intervening items. To model interception, we use an

exponential cutoﬀ scaled to the mean nearest-neighbor squared-distance

:

d2
0i

h

Γij =

,

i

= j ,

e−(Dij )2/2hd2
0i
(Dij)2
N

d2
0i

h

= N −1

(Di<)2 ,

i=1
X
where Di< is the smallest element in the ith row of D. The diagonal elements of Γ are ﬁxed

by Eq. (15).

fore, Eq. (15) implies that

Γ deﬁned by Eq. (16) is symmetric, so its left- and right-eigenvectors are identical. There-

and the equilibrium probability vector peq is

Eq. (14) and the symmetry of Γ imply that all its eigenvectors ϕn are orthogonal and that

all its eigenvalues

γn are non-positive (see Appendix B). It is convenient to use bra-ket

notation to indicate the renormalized inner product,

−

and to normalize the eigenvectors so that

Then,

Γ

1 = 0 ,

·

peq = N −1 1 .

x

y

h

|

i ≡

N −1 x

y ,

·

ϕn|

ϕmi

h

= δnm .

ϕ0 = 1 .

9

(16a)

(16b)

(17)

(18)

(19)

(20)

(21)

6
6
Fig. 1B illustrates ϕ0 and ϕ1 computed in this way for a simple case of N = 20 items in a

1-dimensional measurement space.

Because all the elements of ϕ0 are identical, the vector analog of Eq. (7) is trivial and

the macrostate distributions and window functions are directly proportional to each other.

Therefore, we simplify by expressing the m cluster window vectors directly in terms of the

m slow eigenvectors (for now we assume that m has been speciﬁed):

wα =

Mαn ϕn .

(22)

m−1

n=0
X

Analogously to Eqs. (8), the wα satisfy the positivity and summation constraints required

for a probabilistic interpretation:

(23a)

(23b)

(24)

(25)

(26)

(wα)i ≥

0 ,

α, i ,

∀

wα = 1 .

α
X

Mαn = δn0 .

α
X

wα|
Υα(M) = h
1
|
h

wαi
wαi

.

Eqs. (21) and (23b) and the orthonormality of the eigenvectors implies the m summation

constraints on M

By analogy to Eq. (11), the certainty of cluster α is

As in the continuous case, 0

1. Maximizing the geometric mean of the Υα is

Υα ≤

≤

equivalent to minimizing the objective function

Φ(M)

log Υα(M) .

≡ −

α
X

Minimization of Φ consistent with the linear constraints of Eq. (24) and the linear in-

equality constraints of Eq. (23a) ﬁxes M, and hence the wα, for a speciﬁed value of m. The

solution of this global optimization problem is discussed in Appendix A. There we show

that the resultant wα are linearly independent, so they provide a complete basis for the

macrostate subspace. Once the wα have been computed we complete the clustering proce-

dure for m by assigning each item i to the cluster α having the maximal value of (wα)i. We

say that the assignment is “strong” or “weak” depending on how close this maximal value,

10

the item assignment strength, is to 1. The assignments are extremely strong for the example

shown in Fig. 1B (re panels c and d) because of the relatively large separation between the

two clusters.

In some cases, the procedure may deﬁne a cluster with only a single item. In this case

τ local is undeﬁned and there is no meaningful dissection of dynamics into slow and fast

processes. Therefore we treat such outliers by a special procedure: When one is identiﬁed,

it is removed from the dataset and the pruned dataset is reanalyzed. The pruning procedure

is repeated if new outliers appear. We designate the ﬁnal clustering as

(m).

C

C. Determining the number of clusters

We use two conditions to determine if

(m) is an acceptable clustering: As motivated

above, we examine the eigenspectrum of Γ for spectral gaps, which are deﬁned as

where ργ is the minimum gap parameter. However, Eq. (27) alone may accept excessively

fuzzy clusters having weak item assignment vectors. To eliminate these, we supplement Eq.

(27) with the minimum macrostate certainty conditions:

C

γm/γm−1 > ργ ,

Υα > ρΥ ,

α .

∀

(27)

(28)

We have found that choosing ργ = 3 and ρΥ = 0.68 (the fraction of the normal distribution

contained within one standard-deviation of the mean) works well for all the problems that

we have tested (see Results).

The complete algorithm is to sequentially compute

(m) for m = 2, 3, . . . and to test

C

these clusterings for acceptability according to Eq. (28). If multiple clusterings are accept-

able, we will usually be most interested in the one of largest m, since it provides the ﬁnest

resolution. As a practical matter, if

(m) is not acceptable for three consecutive m’s we

C

assume that it will not be acceptable for higher m’s and terminate the analysis.

D. Computational implementation

Only two steps in the procedure are potentially expensive: computing the slow eigen-

vectors and eigenvalues of Γ and the global minimization of Φ(M). Since we only use a

11

relatively small number (typically m < 10) of slow eigenvectors, it suﬃces to compute only

these via the Lanczos method [27] at cost

O(N 2). The global minimization of Φ(M) is a

linearly constrained global optimization problem in m(m

1) dimensions. The number of

vertices of the feasible region-bounding polytope increases with N, formally as a polynomial

∼

−

dependent on m. However, at least for the problems tested here, a simple solver is adequate

(see Appendix A).

III. RESULTS

We tested the method on a number of problems that have challenged other clustering

methods. Bivariate problems in which the data set can be graphically displayed in two di-

mensions were used to enable subjective evaluation of the quality of the results. In addition,

to check that performance did not depend on low dimensionality of the data space, we tested

problems where the items were embedded in a 20-dimensional space.

A. Bivariate test-cases

The algorithm was evaluated on four previously described diﬃcult test-cases. In each case,

the dataset consisted of NM = 2 measurements on each of N items. These can be represented

as N points in a 2-dimensional space. For example, the “crescentric” clustering problem

shown in Fig. 2a consists of 52 items, each represented as a point in the 2-dimensional

measurement space. The two clusters are closely juxtaposed crescents, which makes the

problem diﬃcult [2, 28]. The D matrix was computed from the coordinates using Eq.

(1) with gab = δab, and Γ was computed from D according to Eqs. (16). The slowest

eigenvectors, ϕ0, ϕ1, and ϕ2, are graphically displayed in panels b, c and d, respectively. As

per Eq. (18), all components of ϕ0 are identical. It is gratifying to see that ϕ1 clearly reﬂects

the two-cluster structure: the components of all the items in the bottom-right crescent are

positive, while the components of all the items in the other crescent are negative. The next

eigenvector, ϕ2, has three localized regions of same-sign components. Subjectively, it is

clear that separating into these regions would overdissect the space. As predicted by the

discussion above, these eigenvector properties in the spatial domain correspond in the time

domain to an eigenspectrum gap between γ1 and γ2 (Fig. 2 and Table I). In contrast, there

12

is no gap between γ2 and γ3 (Fig. 2). This suggests that the m = 2, but not the m = 3

clustering will be acceptable.

The task for the algorithm is to recognize that the correct clustering is embedded in the

structure of ϕ1, and to deﬁne the proper clustering. Applying it for m = 2, 3, . . . yields

the clusters shown in the top panels of Fig. 3. (For illustration, we display clusterings that

do not satisfy the spectral gap condition, even though these would not be computed by an

eﬃcient algorithm.) The cluster certainties Υα are listed in Table I. The m = 2 clustering

satisﬁes both Eqs. (27) and (28), and all clusterings with m > 2 fail both criteria. Therefore,

the algorithm correctly selects m = 2 clusters. The individual item assignment strengths for

this clustering are displayed in Fig. 4; most are in the range of 0.7

0.9, indicating that there

−

is signiﬁcant fuzziness resulting from the close juxtaposition of the clusters. Nonetheless, all

the item assignments are made correctly.

Three other test problems were analyzed in the same way: (1) The “intersecting” problem

consists of two barely-contacting sets of items having highly anisotropic Gaussian distribu-

tions.

It has previously been used to demonstrate the weakness of non-parametric opti-

mization clustering for clusters of greatly diﬀerent shapes and sizes [2]. (2) The “parallel”

problem consists of two highly extended, anisotropic sets of items whose separation along

the vertical axis is much smaller than their horizontal extent. It has previously been used

to demonstrate the failure of agglomerative hierarchical methods [2]. (3) The “horseshoe”

problem [3] consists of a central cluster of items surrounded by a horseshoe-shaped cluster

of items. The center-of-mass of the outer cluster lies within the inner cluster, increasing dif-

ﬁculty. In addition, a “random” test set, in which points were randomly distributed within

a square two-dimensional region, was analyzed as a control.

The results obtained for m = 2, 3, 4, and 5 are illustrated in Fig. 3. The acceptable

clusterings that satisfy Eqs. (27) and (28) are outlined by dark boxes. Only a single clustering

is acceptable in each case (although this need not be so in general). None of the random

control clusterings are acceptable, correctly indicating that it should not be clustered.

As with the crescentric problem, the clustering solution for the “horseshoe” test-case

(fourth row, Fig. 3) is straightforward, with m = 2. Cluster certainties (Table II) and

item assignment strengths (Fig. 4) are extremely strong (> 0.99). The “parallel” problem

is slightly more challenging in that two of the items (located at the extreme left and right

sides of the item distributions) are identiﬁed as outliers. Nonetheless, the algorithm correctly

13

identiﬁes the m = 2 clustering of the non-outlying items. As expected, the item assignment

strengths are lower for the items in the central overlapping region, and higher for the non-

overlapping items near the left and right edges (Fig. 4).

The solution to the “intersecting” problem is more elaborate: While the m = 2 solution

is subjectively acceptable, the assignment strengths of some of the items in the vicinity

of the intersection have weak item assignment strengths. Because of this, the m = 2 and

m = 3 clusterings do not satisfy the required assignment certainty condition Eq. (28) and

are rejected by the algorithm. The acceptable m = 4 clustering resolves this diﬃculty

by segregating these uncertain items into a separate small cluster. It also segregates two

outliers (in the top-right corner) while assigning most of the items to two major clusters, as

desired. The individual item assignment strengths are strong, except for one item near the

intersection of the three clusters (Fig. 4).

None of the

(m) are acceptable for the “random” distribution of items because all of the

C

γm/γm−1 were < 2.5 for m > 1. Thus, the algorithm is not fooled into spurious clustering.

B. Gaussians with varying overlap in two and twenty dimensions

We systematically tested the performance of the algorithm as a function of the relative

distance between clusters. For this purpose, four pseudo-random groups of 50 items were

generated with Gaussian kernels having variance λ2

themselves pseudo-randomly selected from a Gaussian kernel having variance λ2

ℓ . The centers-of-mass of the groups were
g (see Fig.
5). The corresponding ratio of the expected root-mean-square (rms) inter cluster item-item

separations to the rms intracluster item separations is

(∆R)2
h
(∆R)2
h

iinter
iintra

s

q

=

(λ2

ℓ + λ2

g)/λ2
ℓ ,

(29)

Tests in a bivariate measurement space were conducted for λg/λℓ varying from 16 (where the

clusters were highly separated) down to 2 (where the clusters were completely overlapping).

The algorithm dissects the items into four clusters when λg/λℓ = 16. When λg/λℓ = 8

and λg/λℓ = 4, the top two groups partially merge (see Fig. 5), and the algorithm accord-

ingly reports m = 3 clusters. The clusters are not subjectively separable for λg/λℓ = 2;

correspondingly, the algorithm reports m = 1 cluster. The assignment strengths for these

clusterings are displayed in Fig. 5.

14

The same test was performed with four groups generated as described above using Gaus-

sian kernels in a 20-dimensional space. The increased dimensionality does not alter Eq. (29).

However, the distributions of the inter- and intra-group squared-distances are narrower: the

standard deviations of the inter- and intra-group (∆R)2 normalized by their means are both

smaller by factors of

20/2 = √10. Therefore, for a given value of λg/λℓ, clustering is

actually easier in higher dimensionality. To compensate and make the 20-dimensional test

p

more challenging, the range of λg/λℓ was reduced by a factor of 4 (roughly matching √10);

i.e., λg/λℓ was varied from 4 down to 0.5. The algorithm correctly identiﬁes the four clusters

for λg/λℓ = 4 and λg/λℓ = 2. The individual item assignment strengths of these clusterings

are displayed in Fig. 6. These are all close to one for λg/λℓ = 4 and λg/λℓ = 2, indicating

unambiguous clustering. At smaller values of λg/λℓ, the only clustering satisfying both the

minimum gap and minimum certainty conditions has one cluster containing all the items.

Even so, for λg/λℓ = 1, the (formally unacceptable) m = 3 clustering correctly reﬂects some

of the group structure (Fig. 6).

IV. DISCUSSION

We have shown that macrostate clustering performs well on a variety of test problems

that have challenged other methods. The method only needs a dissimilarity matrix D

(not a data matrix X) and has the advantage of being non-hierarchical,2 which should

improve performance in general. Beyond identifying potential clusterings, it uses internal

criteria—the eigenspectrum gaps γm/γm−1 and the cluster certainties Υα—to determine the

appropriate number of clusters. The corresponding acceptance parameters, ργ and ρΥ, were

empirically determined and gave robust performance—a single choice worked well for all the

problems tested.

Eigenvectors have previously been used for clustering by many diﬀerent spectral graph

theory (SGT) partitioning methods: SGT bipartitioning methods use the values of ϕ1 to

deﬁne a one-dimensional ordering of the items which can then be divided by a heuristic. A

2 For example, the m = 5 “crescentric” clustering can not be obtained by subdividing its m = 4 clustering
and the m = 4 “horseshoe” clustering is not hierarchically related to its m = 3 clustering. Nevertheless,
inherent hierarchical structure can still emerge, and some was evident in all the problems. For example,
all the clusterings for 2
5 for the “intersecting” and “parallel” problems are hierarchically related
(Fig. 3).

m

≤

≤

15

variety of diﬀerent approaches have been developed to extend this to multiple eigenvectors

and clusters ([18, 19, 21, 23], for review). For example, recursive spectral bipartitioning

generates a hierarchical binary tree [22]; some methods use k eigenvectors to deﬁne 2k clusters

[29]; and many methods project the items into the subspace spanned by k eigenvectors and

then use a partitioning heuristic to identify clusters within the subspace (e.g., [8, 19, 23, 30,

31, 32] and references therein).

Macrostate clustering diﬀers in that it computes continuous (fuzzy) assignment window

vectors rather than partitionings.3 This has important ramiﬁcations: It permits the window

vectors to be expressed as linear combinations of the eigenvectors [see Eq. (22)]; this neces-

sarily results in window function overlap and cluster uncertainty. Combining these concepts

with the principle of uncertainty minimization provides a simple prescription for the con-

current use of multiple eigenvectors in clustering. A related diﬀerence is that the number of

clusters is internally determined in macrostate clustering, while it is usually ﬁxed a priori

or determined by eigensystem-independent heuristics in SGT methods (e.g., [18, 19, 23] and

references therein). It is perhaps surprising that the spectral gap condition has not been

used for this purpose in SGT approaches.4 This may reﬂect the fact that it does not work

well by itself, and the companion minimum cluster certainty condition is not available when

(crisply) partitioning. Macrostate and SGT clustering also diﬀer in the manner in which Γ

(or the SGT analog) is computed from the dissimilarity matrix D. SGT methods typically

use a weight matrix equivalent to Γij = exp(

Dij/σ), i

= j, where σ is an empirically-

−

determined scale constant.

In contrast, motivated by the analogy to a diﬀusive system,

we used Eqs. (16). While this diﬀerence not of fundamental signiﬁcance, the relationship

between Γ and D can aﬀect performance. Thus, it may be helpful to test the use of Eqs.

(16) in SGT methods or the SGT relationship in macrostate clustering.

The use of a linear transformation from indeﬁnite, orthogonal eigenvectors to semideﬁ-

nite, non-orthogonal window vectors is fundamental, but some freedom remains in the choice

of the objective function used to determine the optimal transformation and in the condi-

tions used to determine acceptable clusterings. An uncertainty minimization criterion is a

3 Drineas et al. [6] consider real-valued “generalized clusters” within a SGT context, but these are indeﬁnite

and do not have a probabilistic interpretation.

4 However, spectral gaps have been used heuristically to determine the appropriate dimensionality of singular

subspaces in data mining [33].

16

6
natural choice, since it is (in an information-theoretic sense) the entropic counterpart to the

(implicit) “energy” minimization criterion that focuses attention on the slow eigenvectors

(see Sec. II.C of [26]). On-the-other-hand, the deﬁnition of uncertainty could be modiﬁed

and tested for improved performance. Similarly, while we believe that it is advantageous to

combine energetic (spectral gap) and entropic (cluster certainty) conditions in determining

the number of clusters, it may be possible to improve upon the speciﬁc criteria used here.

Other improvements and extensions merit attention: (1) While we accepted or rejected

each clustering in toto, it may be useful in some cases to examine incomplete clusterings in

which only some of the clusters satisfy the cluster certainty condition. This modiﬁcation

would enable the algorithm to resolve all four clusters for the case of λg/λℓ = 8 in Fig. 5.5 (2)

The individual item assignment strengths (wα)i measure the certainty of each assignment,

but their precise statistical signiﬁcance is not known. It would be helpful to have a model

for assessing this. (3) The cluster transition matrix γβα =

can be used to assess

wβ|

h

Γ

wαi

|

the strength of the relationship between the clusters and may be useful in setting the cluster

acceptance criteria.

(4) We have deﬁned Γ as a symmetric matrix, which implies that

peq

∝

1. However, this restriction is not required: The generalization to asymmetric Γ is

straightforward [24] and it could be used to incorporate additional experimental information.

For example, if item i is known a priori to be partially redundant with other items (e.g.,

when analyzing expression levels of members of gene families), it may be given reduced
weight in the analysis by setting peq

i < 1.

Our main goal has been a proof-of-principle demonstration of the high quality of the

clusterings provided by the dynamical macrostate approach. The current implementation is

suﬃciently eﬃcient for problems where N

O(102), but we have not examined performance

∼

for very large problems. The continuous formulation replaces the NP-hard combinatoric SGT

partitioning problem with a global minimization problem having polynomial complexity in

N. However, the order of the polynomial can be very large for large m (Appendix A) so,

formally, this is not much of an improvement. Nonetheless, as discussed in Appendix A,

because the objective function is smooth and the constraints are highly degenerate, a simple

solver has worked well and we believe that it will be possibly to obtain adequate approximate

5 The m = 5 solution identiﬁes the four major clusters with strong certainty, but also groups three items
(located near the boundary between the two top clusters) into a ﬁfth cluster which has Υα < ρΥ. In an
incomplete clustering, all but these three items would be unambiguously assigned.

17

solutions eﬃciently even for very large problems. This remains to be examined.

Acknowledgments

We thank Bruce Church, Jason Gans, Ron Elber, Jon Kleinberg, and Golan Yona

for helpful conversations and the NSF (grant CCR9988519) and the NIH (training grant

T32GM08267) for support.

APPENDIX A: MINIMIZING Φ(M )

Φ(M) is to be minimized as a function of the m2 elements of Mαn within the feasible re-

gion speciﬁed by the m

N linear inequality constraints of Eq. (23a). The rows of M can be

×

regarded as the coordinates of m particles in the m-dimensional space of the slow eigenvec-

tors. Designating the coordinate row vector of particle α as −→M α = (Mα0, Mα1, . . . Mα(m−1)),

M is the outer product of the −→M α’s:

α
O
The equality constraints of Eq. (24) imply that the center-of-mass of the m particles is at

position

where ˆε0 is the unit vector in the 0th direction:

M =

−→M α .

1
m

−→M α =

ˆε0
m

,

α
X

ˆε0 = (1, 0, . . . , 0) .

[Eq. (A3) must be modiﬁed when there is more that one stationary eigenvector; see Appendix

B.] The feasible region is a polytope in the m(m

1) dimensional subspace where Eq. (A2)

−

The minimum of Φ(M) must lie at a vertex of this polytope. Proof : The gradient of Φ

is satisﬁed.

with respect to −→M α is

and the Hessian is

∇ αΦ
−→

≡

δΦ

δ−→M α

=

2
−

−→M α
−→M α|

|

2

+

ˆε0
−→M α ◦

,

ˆε0

−→
∇ α ⊗

∇ βΦ
−→

≡

δ2Φ

δ−→M α δ−→M β

=

δαβ

−

2 −

4

−→M α
4

−→M α ⊗
−→M α|
|

+

ˆε0 ⊗
(−→M α ◦

ˆε0
ˆε0)2 #

,

"

2I
−→M α|
18

|

(A1)

(A2)

(A3)

(A4)

(A5)

where I is the m

m identity matrix and

denotes the inner product over the eigenvector

indices,

×

◦

m−1

~x

~y

◦

≡

xn yn .

n=0
X
The gradient does not vanish anywhere, so Φ has no minimum in the absence of constraints.

In fact, a minimum will occur only when all m2 degrees-of-freedom are constrained by

the m equality constraints and m(m

1) inequality constraints. To see this, consider the

−

situation without the equality constraints, but with some number c

m(m

1) of active

≤

−

inequality constraints. Each active inequality constraint acts on a single wα, so by Eq. (22)

it acts on a single −→M α to enforce

−→M α ◦

(−→ϕ )i = 0 ,

(A6)

where −→ϕ is the supervector having components (ϕ0, ϕ1, . . . , ϕm−1). Therefore, the inequal-
ity constraints are separable and, similarly to Eq. (A1), the space of inequality-constrained

M’s can be expressed as the outer-product of the subspaces of inequality-constrained −→M α’s.

Thus, if M c =

α M c

respect to independent variations of each of the inequality-constrained M c

N

α is an inequality-constrained minimizer of Φ, it must be stable with
α. However, this
α + ~δα, the existence of a minimum would

−→M c

is not possible: For any such variation −→M c

require that

and

However, Eqs. (A4) and (A7) imply that

α →

~δα ◦

∇ αΦ = 0
−→

~δα ◦

(−→
∇ α ⊗

∇ α)Φ
−→

◦

~δα > 0 .

~δα
−→M α ◦
2
−→M α|
|

=

~δα ◦
ˆε0
2 −→M α ◦

ˆε0

,

(A7)

(A8)

and combining this with Eq. (A5) implies that

~δα ◦

(−→
∇ α ⊗

∇ α)Φ
−→

◦

~δα =

−

2

2

~δα|
2
|
−→M α|

|

< 0 .

Thus, Eqs. (A7) and (A8) cannot both be true. Therefore, a minimum can occur only if all

variations of the Mα are prevented by a combination of inequality- and equality-constraints.

Since there are only m equality constraints, we must have c = m(m

1) active inequality

−

constraints. This corresponds to a vertex of the feasible region.

19

Note also that the minimizing

must be linearly independent within the m-

dimensional slow eigenvector space. This implies that the associated

must span the

wα}

{

macrostate subspace. Proof : If the

are not independent, there would exist a linear

−→M c

α}

{

−→M c

α}

{

α
X

ξα −→M c

α = 0 .

combination of vectors such that

Then, the combined variation

−→M c

α →

−→M c

α + δ ξα−→M c

α ,

α,

∀

where δ is a small number, will not aﬀect the equality constraint Eq. (A2). As proven

above, all the components of the constrained minimum must be ﬁxed by constraints, so this

variation must be excluded by an inequality constraint. However, this variation only rescales

each −→M c

α and hence each wα. Therefore it also will not aﬀect the inequality constraints and

is permitted, contrary to assumption. Reductio ad absurdum.

To ﬁnd the vertex with the lowest value of Φ, we used a simple minimizer that operates

in the m(m

1)-dimensional subspace that remains after one of the Mα has been explicitly

eliminated using Eq. (A2). The minimizer starts from −→M α = ˆε0/m,

α, chooses a random

∀

direction in the m(m

1)-dimensional space, proceeds to the nearest inequality constraint,

−

−

and then proceeds along faces of the feasible region (of decreasing dimensionality) until a

vertex is reached. This process was repeated until the same extremal minima was found

three times or for a minimum of 500,000 trials, whichever was greater.

Accounting for the separability of the inequality constraints and assuming no degeneracies

between the values of the ϕn (the usual case), the number of vertices of the constraining

polytope might grow as rapidly as O(N m). However, we expect that most of the inequality

constraints of Eq. (23a) will be almost degenerate because of the relatively small diﬀerences

between the components of the eigenvectors at diﬀerent items within a cluster. Moreover,

the objective function Φ is smooth, so we expect that the variation of Φ over nearby vertices

will be small. Therefore, it will not much aﬀect the wα if a neighbor, rather than the global

minimizer itself, is found. Thus, we anticipate that the practical growth in computational

cost with N will be much less than the worst-case bound. These considerations also suggest

that it will always be advantageous to use solvers that move through the [m(m

1)

1]-

−

−

dimensional space of search-space directions rather than between vertices of the constraining

polytope.

20

APPENDIX B: DEGENERATE “ZERO” EIGENVALUES

Because Γ is a symmetric matrix which satisﬁes Eqs. (15) and (17),

x

Γ

x =

−

·

·

Γij (xi −

xj)2 ,

j>i
X
i

(B1)

for any vector x. The right-hand-side (rhs) can be viewed as the potential energy of N par-

ticles having pairwise quadratic interactions in one-dimension. Because all the oﬀ-diagonal

elements of Γ are positive, the rhs must be non-negative. The implied non-positivity of

x

Γ

x for all x implies that all the eigenvalues of Γ must be non-positive. Furthermore,

·

·

the isomorphism makes it evident that x = 1 is the only stationary eigenvector (up to a

multiplicative constant) unless the dataset contains an isolated subset

, which has Γij = 0

S

if i

and j

. In this case, Γ will have multiple zero eigenvalues, and there will be

∈ S

6∈ S

one stationary eigenvector corresponding to each isolated subset. This degeneracy can be

removed by analyzing each isolated subset independently.

It is more common to encounter approximate isolation in which none of the Γij is exactly

zero but in which there are multiple small eigenvalues that are 0 on the scale of numerical

accuracy. (This occurs in the Gaussian clustering problem shown in Fig. 5 when λg/λℓ is

large.) This can cause numerical problems: the ϕ0 returned by a numerical eigensystem

solver will not necessarily satisfy Eq. (21), but instead will be a linear combination of the

approximately degenerate eigenvectors. Because of this, Eq. (21), and hence Eq. (24), may

not be true.

and to replace Eq. (A3) with

The simplest resolution of this numerical problem is to replace Eq. (24) with Eq. (A2)

This does not require the numerical validity of Eq. (21).

ˆε0 =

1
|−→ϕ
i

h

.

(B2)

[1] B. Mirkin, Mathematical classiﬁcation and clustering (Kluwer Academic Pubishers, Boston,

[2] B. Everitt, S. Landau, and M. Leese, Cluster Analysis (E. Arnold, 2001), pp. 1–10, 64–67, 76,

1996).

94–99, 144–145, 4th ed.

21

[3] Z. Szallasi, in 2nd International Conference on Systems Biology (2001), URL http://www.

icsb2001.org/SzallasiTutorial.pdf.

[4] A. Jain and R. Dubes, Algorithms for Clustering Data (Prentice-Hall, Englewood Cliﬀs, NJ,

1981).

[5] R. B. Altman and S. Raychaudhuri, Curr. Opin. Struct. Biol. 11, 340 (2001).

[6] P. Drineas, R. Kannan, A. Frieze, S. Vempala, and V. Vinay, in Proceedings of the Tenth

Annual ACM-SIAM Symposium on Discrete Algorithms (ACM Press, New York, 1999), URL

http://doi.acm.org/10.1145/314500.314576.

[7] O. Alter, P. O. Brown, and D. Botstein, Proc. Natl. Acad. Sci. USA 97, 10101 (2000).

[8] R. Kannan, S. Vempala, and A. Vetta,

in 41st Annual Symposium on Foundations of

Computer Science (IEEE Computer Society, Los Alamitos, CA, 2001), pp. 367–377, URL

http://citeseer.nj.nec.com/495691.html.

[9] G. Milligan and M. Cooper, Psychometrika 50, 159 (1985).

[10] A. D. Gordon, Data Science, Classiﬁcation and Related Methods (Springer-Verlag, Tokyo,

1998), chap. Cluster Validation, pp. 22–39.

[11] K.Rose, E. Gurewitz, and G. C. Fox, Phys. Rev. Lett. 65, 945 (1990).

[12] M. Blatt, S. Wiseman, and E. Domany, Phys. Rev. Lett. 76, 3251 (1996).

[13] S. Wiseman, M. Blatt, and E. Domany, Phys. Rev. E 57, 3767 (1998).

[14] L. Kullmann, J. Kertesz, and R. N. Mantegna, Physica A pp. 412–419 (2000).

[15] L. Giada and M. Marsili, Phys. Rev. E 63, 061101 (2001).

[16] L. Angelini, F. DeCarlo, C. Marangi, M. Pellicoro, and S. Stramaglia, Phys. Rev. Lett. 85,

[17] D. Horn and A. Gottlieb, Phys. Rev. Lett. 88, 18702 (2002).

[18] A. J. Seary and W. D. Richards, in Proceedings of the International Conference on Social

Networks (1995), vol. 1, pp. 47–58, URL http://www.sfu.ca/~richards/Pdf-ZipFiles/

554 (2000).

london98.pdf.

[19] Y. Weiss, in Proceedings of the Seventh IEEE International Conference on Computer Vi-

sion (IEEE Computer Society, Los Alamitos, CA, 1999), vol. II, pp. 975–982, URL http:

//citeseer.nj.nec.com/weiss99segmentation.html.

[20] F. R. K. Chung, Spectral Graph Theory, no. 92 in CBMS Regional Conference Series in Math-

ematics (Amer. Math. Soc., Providence, R.I., 1997).

22

[21] D. A. Spielman and S.-H. Teng,

in 37th Annual Symposium on Foundations of Com-

puter Science (IEEE Computer Society, Los Alamitos, CA, 1996), pp. 96–105, URL http:

//citeseer.nj.nec.com/spielman96spectral.html.

[22] S. T. Barnard and H. D. Simon, Concurrency: pract. ex. 6, 101 (1994).

[23] C. J. Alpert, A. B. Kahng, and S.-Z. Yao, Disc. Appl. Math. 90, 3 (1999).

[24] D. Shalloway, J. Chem. Phys. 105, 9986 (1996).

[25] R. Kubo, M. Toda, and N. Hashitsume, Statistical Physics II: Nonequilibrium Statistical Me-

chanics (Springer-Verlag, New York, 1985).

[26] A. Ulitsky and D. Shalloway, J. Chem. Phys. 109, 1670 (1998).

[27] G. H. Golub and C. F. VanLoan, Matrix Computations (John Hopkins U. Press, Baltimore,

Md., 1989), 2nd ed.

[28] M. Wong and T. Lane, J. Royal Statist. Soc. B 45, 362 (1983).

[29] B. Hendrickson and R. Leland, in Proceedings of the 6th SIAM Conference on Parallel Pro-

cessing for Scientiﬁc Computing (SIAM, 1993), pp. 953–961, URL http://citeseer.nj.

nec.com/hendrickson93multilevel.html.

[30] A. Pothen, H. D. Simon, and K. P. Liou, SIAM J. Matrix Anal. Appl. 11, 430 (1990).

[31] M. Meila and J. Shi, in Advances in Neural Information Processing Systems 13 (MIT Press,

Cambridge, MA, 2001), pp. 873–879, URL http://citeseer.nj.nec.com/meil01learning.

html.

[32] A. Ng, M. Jordan, and Y. Weiss, in Advances in Neural Information Processing Systems 14

(MIT Press, Cambridge, MA, 2001), URL citeseer.nj.nec.com/ng01spectral.html.

[33] Y. Azar, A. Fiat, A. R. Karlin, F. McSherry, and J. Saia, in ACM Symposium on Theory

of Computing (ACM Press, New York, 2001), pp. 619–626, URL http://citeseer.nj.nec.

com/azar00spectral.html.

23

FIGURES

FIG. 1: Heuristic examples. (A) Identifying the macrostates of a continuous stochastic system in

one-dimension. a: The potential V (x) and eigenvalue spectrum. b: The zeroth and ﬁrst excited

right-eigenfunctions of the corresponding diﬀusive dynamical (Smoluchowski) equation. c and d:

The two macrostate distribution and window functions. (B) Macrostate clustering of items in a

one-dimensional space. a: The positions of the items in the univariate measurement space. b:

Graphical representation of the zeroth and ﬁrst eigenvectors of Γ; the height of the bar at the

position of item i corresponds to its component within the indicated eigenfunction. c and d: The

components of the two window vectors corresponding to the left (wa) and right (wb) clusters.

FIG. 2: “Crescentric” clustering problem and its slow eigenvectors. a: The x and y coordinates

of each point correspond to two measurement values of the corresponding item. b, c and d: ϕ0,

ϕ1, and ϕ2, respectively. For illustration, the amplitude of the ith component of each ϕn is

represented by the height (if positive) or depth (if negative) of a cone centered at position i. The

relative magnitudes of the corresponding eigenvalues are indicated.

FIG. 3: Bivariate test-cases. The algorithmically-determined clusterings

≤
5 are displayed for four bivariate examples in which the items are points in a two-dimensional

(m) for 2

m

≤

C

measurement space. Clusters are distinguished by diﬀerent symbols, except that unﬁlled squares

identify items that were designated as outliers by the algorithm. The acceptable clusterings, which

satisfy Eqs. (27) and (28), are outlined by dark boxes.

FIG. 4:

Item assignment strengths for the acceptable clusterings. The acceptable clusterings for

each of the problems in Fig. 3 are shown. The height of the dark section of the bar relative to its

total height at the position of an item indicates its assignment strength.

24

FIG. 5: Clustering of Gaussian-distributed items in two dimensions for various cluster separations.

Top: The unique acceptable clustering for each value of λg/λℓ is indicated. Bottom: The height

of the dark section of the bar at the position of an item indicates its assignment strength. (Most

of the strengths are

1).

≈

FIG. 6:

Item assignment strengths for cluster solutions for various group separations in 20 dimen-

sions. Items were pseudo-randomly distributed into four groups in a 20-dimensional measurement

space for diﬀerent values of λg/λℓ as described in the text. The items within each group have

consecutive serial numbers (i.e., items 1–50 are in the ﬁrst group, 51–100 are in the second group,

etc.). Their assignment strengths for the indicated

(m) clusterings are displayed in the each case.

C

(Item 171 is an outlier for both clusterings shown in the bottom row; hence it is not assigned to

any cluster.) However, only the m = 4 clusterings for λg/λℓ = 4 and λg/λℓ = 2 are acceptable; the

(3) and

(2) shown in the bottom panels fail the acceptability conditions of Eqs. (27) and (28)

C
because of their low cluster certainties.

C

25

TABLE I: Crescentric cluster analysis

TABLES

m γm

γm−1 Υα(m)

2

3.52

3

1.12

4

2.73

5

1.03

0.71

0.70

0.67

0.41

0.53

0.83

0.81

0.51

0.53

0.71

0.47

0.55

0.38

0.38

26

TABLE II: Bivariate test case analyses

Problem

γm
γm−1 Υα(m)

Crescentric

3.52 0.71

Intersecting

4

3.82 0.91

Parallel

10.68 0.93

Horseshoe

60.73 0.998

Random

–

–

0.70

0.95

0.84

0.94

0.93

0.99

m

2

2

2

1

27

This figure "fig1.jpg" is available in "jpg"(cid:10) format from:

http://arXiv.org/ps/physics/0306145v1

This figure "fig2.jpg" is available in "jpg"(cid:10) format from:

http://arXiv.org/ps/physics/0306145v1

This figure "fig3.jpg" is available in "jpg"(cid:10) format from:

http://arXiv.org/ps/physics/0306145v1

This figure "fig4.jpg" is available in "jpg"(cid:10) format from:

http://arXiv.org/ps/physics/0306145v1

This figure "fig5.jpg" is available in "jpg"(cid:10) format from:

http://arXiv.org/ps/physics/0306145v1

This figure "fig6.jpg" is available in "jpg"(cid:10) format from:

http://arXiv.org/ps/physics/0306145v1


Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

1

Information-Theoretic Approach to the Study of
Control Systems

Hugo Touchette, Student member, IEEE, and Seth Lloyd

1
0
0
2
 
r
p
A
 
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
0
0
4
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Abstract—We propose an information-theoretic framework for studying
control systems, based on a model of controllers analogous to communi-
cation channels. Given the initial state of a system to be controlled, the
dynamics of a controller is described as applying a transmission channel,
called the actuation channel, to that state in order to redirect it towards
another target state. In this process, two different control strategies can be
adopted: (i) the controller applies an actuation dynamics independently of
the state of the system to be controlled (open-loop control); or (ii) the con-
troller enacts an actuation dynamics based on some information about the
state of the controlled system (closed-loop control). In the context of this
model, we provide necessary and sufﬁcient entropic conditions for a system
to be perfectly controllable and perfectly observable. Also, using the fact
that the information gathered by a controller is quantiﬁed by mutual in-
formation, we prove a limiting result expressing the trade-off between the
availability of information in a closed-loop control process and its perfor-
mance over open-loop control in stabilizing a system. This work completes
a ﬁrst paper on the subject [1] by providing new proofs of the results, and
by proposing an information-based optimality criterion for control systems.
New applications of this approach pertaining to proportional controllers,
and the control of chaotic maps are also presented.

Keywords—Control theory, stochastic systems, entropy, mutual informa-
tion, communication channel, control channel, stochastic stability, propor-
tional controller, chaotic control.

I. INTRODUCTION

CONVENTIONAL control systems are constructed from

two fundamental and usually distinct physical components:
sensors and actuators. On the one hand, sensors are devices
whose task, as the word plainly suggests, is to sense, observe or
estimate the state of a system intended to be controlled. Actua-
tors, on the other hand, are devices which act directly on the con-
trolled system by augmenting its natural dynamics so as to redi-
rect its evolution toward a desired response, thereby achieving
purposeful control. The actual control or actuation dynamics ap-
plied can be prescribed either by the outcome of a sensor, or by
general characteristics of the controlled system not expressly re-
lated to the instant variations of its state. In the control-theoretic
jargon, the former control strategy corresponds to what is known
as closed-loop or feedback control, whereas the latter is referred
to as open-loop control [2], [3].

Intuitively, the functioning of sensors and actuators in a con-
trol unit is often described by having recourse to the comple-
mentary concepts of uncertainty and information. Sensors can
be thought of as gathering information from the system to be
controlled in the form of data relative to its state (estimation
step); this information is processed according to a determined

The work of H. Touchette has been supported in part by the d’Arbeloff Lab-
oratory for Information Systems and Technology at MIT, and by the Natural
Sciences and Engineering Research Council of Canada (NSERC).

H. Touchette is with the Department of Physics and the School of Com-
puter Science, McGill University, Montr´eal, Qu´ebec, Canada H3A 2A7. (email:
htouchet@alum.mit.edu).

S. Lloyd is with the d’Arbeloff Laboratory for Information Systems and
Technology, Department of Mechanical Engineering, Massachusetts Institute of
Technology, Cambridge, MA 02139 USA (email: slloyd@mit.edu).

control strategy (decision step), and then transferred to actua-
tors which feed this information back to the controlled system
by modifying its dynamics, with the goal of decreasing the un-
certainty about the value of the system’s variables (actuation
step) [3]. In this spirit, it can be said that an open-loop con-
troller distinguishes itself from a closed-loop controller in that
it does not need a continual input of ‘selective’ information [4]
to work: like the throttle of a gas pipe or a blindfolded driver,
to take simple examples, it implements a control action inde-
pendently of the state of the controlled system. In this respect,
open-loop control techniques then merely represent a subclass
of closed-loop controls restricted by the fact that information
made available by estimation is neglected.

In view of this compelling information-based description of
control units, it is perhaps surprising to note that few efforts have
been made to go beyond the intuitive and qualitative aspects of
it to develop a quantitative theory of controllers focused explic-
itly on information. Indeed, although controllers have been de-
scribed by numerous authors as information gathering and using
systems (e.g., [5]-[8]), and despite some results related to this
problem (see [9]-[20] and most notably [21]-[24]), there exists
at present no general information-theoretic formalism charac-
terizing the exchange of information between a controlled sys-
tem and a controller, and more importantly, which allows for
the assignation of a deﬁnite value of information in control pro-
cesses. To address this deﬁciency, we proceed in this paper with
a detailed study of an attempt for such a formalism elaborated
ﬁrst in [1]. The basis of the results presented here draws upon
the work of several of the papers cited above by bringing to-
gether some aspects of dynamical systems, information theory,
in addition to probabilistic networks to construct control models
in the context of which quantities analogous to entropy can be
deﬁned.

Central to our approach is the notion of a communication
channel, and its extension to the idea of control channels. As
originally proposed by Shannon [25], a (memoryless) commu-
nication channel can be represented mathematically by a prob-
ability transition matrix, say p(y|x), relating the two random
variables X and Y which are interpreted, respectively, as the in-
put and the output of the channel. In the next two sections of
the present work, we adapt this common probabilistic picture of
communication engineering to describe the operation of a ba-
sic control setup, composed of a sensor linked to an actuator, in
terms of two channels: one coupling the initial state of the sys-
tem to be controlled and the state of the sensor (sensor channel),
and another one describing the state evolution of the controlled
system as inﬂuenced by the sensor-actuator’s states (actuation
channel).

In Sections IV and V, we use this model in conjunction with
the properties of entropy-like quantities to exhibit fundamental

2

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

results pertaining to control systems. As a ﬁrst of these results,
we show that the classical deﬁnition of controllability, a concept
well-known to the ﬁeld of control theory, can be rephrased in
an information-theoretic fashion. This new deﬁnition is used, in
turn, to show that a necessary and sufﬁcient condition for a sys-
tem to be perfectly controllable is that the target state of that sys-
tem, upon the application of controls, is statistically independent
of any other external systems playing the role of noise sources.
A similar result is also proven for the complementary concept of
observability. Moreover, we prove that the information a feed-
back controller must gather in order to stabilize the state of an
arbitrary system by decreasing its entropy must be bounded be-
low by the difference ∆Hclosed − ∆H max
open , where ∆Hclosed is
the closed-loop entropy reduction that results from utilizing in-
formation in the control process, and ∆H max
open is the maximum
decrease of entropy attainable when restricted to open-loop con-
trol techniques. This last result, as we will see, may be used to
deﬁne an information-based optimality criterion for control sys-
tems.

The idea of reducing the entropy of a system using informa-
tion gathered from estimating its state is not novel by itself. It
has, in fact, been treated abundantly in the physics literature
in the context of thermodynamics, particularly in connection
with the so-called Maxwell’s demon paradox. (See [26] for a
description of this paradox and a guide to the original litera-
ture.) However, it is an unfortunate fact that familiarity with the
Maxwell’s demon paradox is not widespread among engineers
working in control theory, and therefore discussions of this sub-
ject in physics had very limited impact, if none, on the ﬁeld of
control.

To the authors’ knowledge, the only published works con-
taining control-oriented ﬁndings which exploit on a quantita-
tive level the idea of reducing the entropy of a dynamical sys-
tem have been reported by Poplavski˘ı [12], [13] and by Weide-
mann [21]. Although the content of these references is similar in
essence to that which is presented here, the conceptual approach
adopted by their respective authors fails to address the problem
of information and control in its full generality. For instance,
most of the results obtained by Poplavski˘ı concerning the infor-
mation gathered by sensoring devices are based on Brillouin’s
notion of negentropy, a quantity which proved with time to be
very misleading as it gave rise to a number of misconceptions
related to the reversibility of information processing. In addi-
tion, his study focuses almost entirely on the sensor part of con-
trollers, leaving completely aside the actuation process which,
as will be shown, can be also treated in an information-theoretic
fashion. In the same vein, the results derived by Weidemann
lack a certain generality due to the fact that he only considers
a restricted class of linear control systems having measure pre-
serving sensors.

In the present paper, we go beyond these limitations by pre-
senting results which apply equally to linear and nonlinear sys-
tems, and can be generalized with the aid of a few modiﬁcations
to encompass continuous-space systems as well as continuous-
time dynamics. To illustrate this scope of applications, we study
in Section VI speciﬁc examples of control systems. Among
these, we consider two variants of proportional controllers,
which play a predominant role in the design of present-day con-

(a)

0

(c)

0

X

X

X

X

S

A

C

C

(b)

0

(d)

0

X

X

X

X

C

C = c

Fig. 1. Directed acyclic graphs representing a basic control process. (a) Full
control system with a sensor S and an actuator A.
(b) Reduced closed-
loop diagram obtained by merging the sensor and the actuator into a single
(c) Reduced open-loop control diagram.
controller device, the controller.
(d) Single actuation channel enacted by the controller’s state C = c.

trollers, in addition to complete our numerical investigation of
noise-perturbed chaotic controllers initiated in [1]. Finally, we
propose, by way of conclusion, a general discussion of the rela-
tionship of our framework with thermodynamics, optimal con-
trol theory, and rate distortion theory.

II. CHANNEL-LIKE MODELS OF CONTROL

In this section, we introduce a model that allows investigation
of the general control problem in its simplest but nontrivial ver-
sion. It appears to us that this model, while focusing only on a
few components of controllers, nonetheless captures the essence
of what a control process is about: that is, a dynamical interplay
between a sensor and an actuator aimed at enforcing the dy-
namics of a system from one initial state to a ﬁnal target state.
In this sense, the proposed model is arguably the best possible
compromise between, on the one hand, the desire to address the
problem at a level amenable to formalization, and, on the other
hand, the need to deduce results from it which are relevant for
the study of realistic control systems.

The basic control models that we are interested in are depicted
schematically in Figure 1 in the form of directed acyclic graphs,
also known as Bayesian networks [27], [28]. The vertices of
these graphs correspond to random variables representing the
state of a particular (classical) system, whereas the arrows give
the probabilistic dependencies among the random variables ac-
cording to the general decomposition

p(x1, x2, . . . , xN ) =

p(xi|π[Xi]),

(1)

N

i=1
Y

where π[Xi] is the set of random variables which are direct par-
ents of Xi, i = 1, 2, . . . , N , (π[X1] = ∅). The acyclic condition
of the graphs ensures that no vertex is a descendant or an ances-
tor of itself, in which case we can order the vertices chronolog-
ically, i.e., from ancestors to descendants. This deﬁnes a causal
ordering, and, consequently, a time line directed on the graphs
from left to right.

In the control graph of Figure 1a, the random variable X rep-
resents the initial state of the system to be controlled, and whose
values x ∈ X are drawn according to a ﬁxed probability distri-
bution pX(x). In conformity with our introductory description

H. Touchette and S. Lloyd

3

of controllers, this initial state is controlled to a ﬁnal state X ′
with state values x′ ∈ X by means of a sensor, of state vari-
able S, and an actuator whose state variable A inﬂuences the
transition from X to X ′. For simplicity, all the random vari-
ables describing the different systems are taken to be discrete
random variables with ﬁnite sets of outcomes. The extension
to continuous-state systems is discussed in Section IV. Also, to
further simplify the analysis of this model, we assume through-
out this paper that the sensor and the actuator are merged into a
single device, called the controller, which fulﬁlls both the roles
of estimation and actuation (see Figure 1b). The state of the
controller is denoted by C, and assumes values from the set
C of admissible controls. From the viewpoint of information,
this simpliﬁcation amounts to a situation whereby the sensor is
connected to the actuator by a noiseless communication channel
describing a one-to-one mapping between the input S and the
output A of the controller [29].

Using these notations, and the decomposition of Eq.(1), the
joint distribution p(x, x′, c) describing the causal dependencies
between the states of the control graphs can now be constructed.
For instance, the complete joint distribution corresponding to
the closed-loop graph of Figure 1b is written as

p(x, x′, c)closed = pX (x)p(c|x)p(x′|x, c),

(2)

whereas the open-loop version of this graph, depicted in Figure
1c, is characterized by a joint distribution of the form

p(x, x′, c)open = pX (x)pC (c)p(x′|x, c).

(3)

Following the deﬁnition of closed- and open-loop control given
before, what distinguishes probabilistically and graphically both
control strategies is the presence, for closed-loop control, of a
direct correlation link between X and C represented by the con-
ditional probability p(c|x). This correlation can be thought of
as a (possibly noisy) communication channel, referred here to as
the sensor or measurement channel, that enables the controller
to gather an amount of information identiﬁed formally with the
mutual information

I(X; C) =

pXC (x, c) log

x∈X ,c∈C
X

pXC (x, c)
pX (x)pC (c)

,

(4)

where pX,C (x, c) = pX (x)p(c|x).
(All logarithms are as-
sumed to the base 2, except where explicitly noted.) Recall that
I(X; C) ≥ 0 with equality if and only if the random variables X
and C are statistically independent [30], so that in view of this
quantity we are naturally led to deﬁne open-loop control with
the requirement that I(X; C) = 0. Closed-loop control, on the
other hand, must be such that I(X; C) 6= 0.

As for the actuation part of the control process, the joint dis-
tributions of Eqs.(2)-(3) show that it is accounted for by the
channel-like probability transition matrix p(x′|x, c). The entries
of this actuation matrix give the probability that the controlled
system in state X = x is actuated to X ′ = x′ given that the
controller’s state is C = c. Henceforth, it will be convenient to
think of the control actions indexed by each value of C as a set
of actuation channels, with memoryless transition matrices

p(x′|x)c = p(x′|x, c),

(5)

governing the transmission of the random variable X to a target
state X ′. In terms of the control graphs, such channels are rep-
resented similarly as in Figure 1d in order to evidence the fact
that the ﬁxed value C = c (ﬁlled circle in the graph) enacts a
transformation of the random variable X (open circle) to a yet
unspeciﬁed value associated with the random variable X ′ (open
circle as well). Guided by this graphical representation, we will
show in the next section that the overall action of a controller
can be decomposed into a series of single conditional actuation
actions, or subdynamics, triggered by the internal state of C.

Our main concern, in this study, is precisely to characterize
the effect of the subdynamics available to a controller on the
entropy of the initial state X:

H(X) = −

pX (x) log pX (x).

(6)

x∈X
X

In theory, this effect is completely determined by the choice of
the initial state X, and the form of the actuation matrices, and
can be categorized according to the three following classes of
dynamics:

• One-to-one transitions. A given control subdynamics spec-
iﬁed by C = c conserves the entropy of the initial state X
if the corresponding probability matrix p(x′|x)c is that of
a noiseless channel. Permutations or translations of X are
examples of this sort of dynamics;

• Many-to-one transitions. A control channel p(x′|x)c may
cause a subset of the state space X to be mapped onto a
smaller subset of values for X ′.
In this case, the corre-
sponding subdynamics is said to be dissipative or volume-
contracting as it decreases entropy of most typical states,
for instance, states characterized by non-singular or non-
uniform probability distributions;

• One-to-many transitions. A channel p(x′|x)c can also lead
H(X) to increase, again in a typical sense, if it is non-
deterministic, i.e., if it speciﬁes the image of one or more
values of X only up to a certain probability different than
zero or one. This will be the case, for example, if the ac-
tuator is unable to accurately manipulate the dynamics of
the controlled system, or if any part of the control system
is affected by external and non-controllable systems.

From a strict mathematical point of view, let us note that
any non-deterministic channel modeling a source of noise at the
level of actuation or estimation can be represented abstractly as
a randomly selected deterministic channel with transition ma-
trix containing only zeros and ones. The outcome of a random
variable undisclosed to the controller can be thought of as be-
ing responsible for the choice of the channel to use. Figure 2
shows speciﬁcally how this can be done by supplementing our
original control graphs of Figure 1 with an exogenous and non-
controllable random variable Z in order to ‘purify’ the channel
considered (actuation or estimation). For the actuation channel,
as for instance, the puriﬁcation condition simply refers to the
following properties:

• The mapping from X to X ′ conditioned on the values
c and z, as described by the extended transition matrix
p(x′|x, c, z), is deterministic for all c ∈ C and z ∈ Z;

4

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

(a)

(b)

Z

where

X

X

0

X

0

X

Z

C

C

Fig. 2. Control diagrams illustrating the puriﬁcation procedure for (a) the actu-
ation channel, and (b) the sensor channel. Purifying a channel, for instance
the sensor channel, simply means that knowing the value of X and Z en-
ables one to know with probability one the value of C. However, discarding
(viz, tracing out) any information concerning Z leaves us with some uncer-
tainty as to which C is reached from a given value for X.

• When traced out of Z, p(x′|x, c, z) reproduces the dynam-

ics of p(x′|x, c), i.e.,

p(x′|x, c) =

p(x′|x, c, z)pZ(z),

(7)

for all x′, x ∈ X , and all c ∈ C.

z∈Z
X

To complement the material introduced in the previous sec-
tion, we now present a technique for analyzing the control
graphs that emphasizes further the conceptual importance of the
actuation channel and its graphical representation. The tech-
nique is based on a useful symmetry of Figure 1c that enables
us to separate the effect of the random variable X in the actua-
tion matrix from the effect of the control variable C. From one
perspective, the open-loop decomposition

pX ′(x′)open =

pC (c)

c
X

"

x
X

p(x′|x, c)pX (x)
#

(8)

suggests that an open-loop control process can be decomposed
into an ensemble of actuations, each one indexed by a particular
value c that takes the initial distribution pX (x) to a conditional
distribution (ﬁrst sum in parentheses)

H(X ′|c) = −

p(x′|c) log p(x′|c).

(13)

x′∈X
X

(Subscripts of H indicate from which distribution the entropy is
to be calculated.) In the latter perspective, the entropy reduction
associated with the unconditional transition from X to X ′ is
simply the open-loop entropy reduction

∆Hopen = H(X) − H(X ′)open

(14)

which characterizes the control process as a whole, without re-
gard to any knowledge of the controller’s state.

For closed-loop control, the decomposition of the control ac-
tion into a set of conditional actuations seems a priori inapplica-
ble, for the controller’s state itself depends on the initial state of
the controlled system, and thus cannot be ﬁxed at will. Despite
this fact, one can use the Bayesian rule of statistical inference

p(x|c) =

p(c|x)pX (x)
pC(c)

,

pC(c) =

p(c|x)pX (x),

x∈X
X

(15)

(16)

to invert the dependency between X and C in the sensor channel
so as to rewrite the closed-loop decomposition in the following
form:

pX ′(x′)closed =

pC(c)

c
X

"

x
X

p(x′|x, c)p(x|c)
#

.

(17)

By comparing this last equation with Eq.(8), we see that a
closed-loop controller is essentially an open-loop controller act-
ing on the basis of p(x|c) instead of pX (x) [31]. Thus, given that
c is ﬁxed, a closed-loop equivalent of Eq.(12) can be calculated
simply by substituting pX (x) with p(x|c), thereby obtaining

∆H c

closed = H(X|c) − H(X ′|c)

(18)

III. CONDITIONAL ANALYSIS

where

p(x′|c)open =

p(x′|x, c)pX (x).

(9)

for all c.

The ﬁnal marginal distribution pX ′(x′)open is then obtained by
evaluating the second sum in Eq.(8), thus averaging p(x′|c)open
over the control variable. From another perspective, Eq.(8), re-
ordered as

pX ′(x′)open =

pX (x)

x
X

"

c
X

p(x′|x, c)pC (c)
#

,

(10)

indicates that the overall action of a controller can be seen as
transmitting X through an ‘averaged’ channel (sum in paren-
theses) whose transition matrix is given by

p(x′|x) =

p(x′|x, c)pC (c).

(11)

x∈X
X

c∈C
X

In the former perspective, each actuation subdynamics repre-
sented by the control graph of Figure 1d can be characterized by
a conditional open-loop entropy reduction deﬁned by

∆H c

open = H(X) − H(X ′|c)open

(12)

The rationale for decomposing a closed-loop control action
into a set of conditional actuations can be justiﬁed by observ-
ing naively that a closed-loop controller, after the estimation
step, can be thought of as an ensemble of open-loop controllers
acting on a set of estimated states. In other words, what differ-
entiates open-loop and closed-loop control from the viewpoint
of the actuator is the fact that, for the former strategy, a given
control action selected by C = c transforms all the values x
contained in the support of X, i.e., the set

supp(X) = {x ∈ X : pX (x) > 0},

(19)

whereas for the latter strategy, namely closed-loop control, the
same actuation only affects the support of the posterior distri-
bution p(x|c) associated with X|c, the random variable X con-
ditioned on the outcome c . This is so because the decision as
to which control value is used has been determined according
to the observation of speciﬁc values of X which are in turn af-
fected by the chosen control value. By combining the inﬂuence
of all the control values, we thus have that information gathered

H. Touchette and S. Lloyd

5

(a)

(b)

(c)

S

A

1

C

0

C

0

1

X

X

X

0

0

1

0

1

1

0

X

0

X

0

1

0

0

X

0

0

C

0

1

(d)

(e)

(f )

(g)

p(x)

p(xjc)

p(x jc)

p(x )

0

0

1

1

1

1

0

1

0

1

0

1

0

1

x

1

0

0

1

0

x

c

x

c

0

x

Fig. 3. Controlled-NOT controller. (a) Boolean circuit illustrating the effect of the controller’s state C = 0 on the input states 0, 1 of the controlled system X
(identity in this case). (b) Control action triggered by C = 1 (swapping). (c) Complete control system with sensor S and actuator A. Note that the sensor itself
is modeled by a CNOT gate. (d)-(g) State of the controlled system at different stages of the control depicted in the spirit of conditional analysis. (d) A uniformly
distributed input state X is measured by the sensor in such a way that the conditional random variable X|c is deterministic (e). (f) The control action triggered
by C has the effect of swapping the values x for which C = 1. (g) Deterministic probability distribution for the ﬁnal state X′ upon averaging over C.

by the sensor affects the entire control process by inducing a
covering of the support space

supp(X) =

supp(X|c),

(20)

c∈C
[

in such a way that values x ∈ supp(X|c1), for a ﬁxed
c1 ∈ C, are controlled by the corresponding actuation channel
p(x′|x, C = c1), while other values in supp(X|c2) are con-
trolled using p(x′|x, C = c2), and so on for all ci ∈ C. This is
manifest if one compares Eqs.(8) and (17). Note that a particular
value x included in supp(X) may be actuated by many differ-
ent control values if it is part of more than one ‘conditional’
support supp(X|c). Hence the fact that Eq.(20) only speciﬁes a
covering, and not necessarily a partition constructed from non-
overlapping sets. Whenever this occurs, we say that the control
is mixing.

To illustrate the above ideas about subdynamics applied to
conditional subsets of X in a more concrete setting, we proceed
in the next paragraph with a basic example involving the con-
trol of a binary state system using a controller restricted to use
permutations as actuation rules [1]. This example will be used
throughout the article as a test situation for other concepts.

Example 1: Let C be a binary state controller acting on a bit
X by means of a so-called controlled-NOT (CNOT) logical gate.
As shown in the circuits of Figures 3a-b, the state X, under the
action of the gate, is left intact or is negated depending on the
control value:

if c = 0
x,
x ⊕ 1, if c = 1.

x′ =

(cid:26)

(21)

(⊕ stands for modulo 2 addition.) Furthermore, assume that
the controller’s state is determined by the outcome of a ‘per-
fect’ sensor which can be modeled by another CNOT gate such
that C = X when C is initially set to 0 (Figure 3c). As a re-
sult of these actuation rules, it can be veriﬁed that ∆H c
open =

∆H c
closed = 0 , and so the application of a single open- or closed-
loop control action cannot increase the uncertainty H(X). In
fact, whether the subdynamics is applied in an open- or closed-
loop fashion is irrelevant here: a permutation is just a permu-
tation in either cases. Now, since C = X, we have that the
random variable X conditioned on C = c must be equal to c
with probability one. For closed-loop control, this implies that
the value X = 0, which is the only element of supp(X|C = 0),
is kept constant during actuation, whereas the value X = 1 in
supp(X|C = 1) is negated to 0 in accordance with the con-
troller’s state C = 1 (Figure 3e). Under this control action,
the conditional random variable X ′|c is forced to assume the
same deterministic value for all c, implying that X ′ must be
deterministic as well, regardless of the statistics of C (Figures
3f-g). Therefore, H(X ′)closed = 0.
In contrast, the applica-
tion of the same actuation rules in an open-loop fashion trans-
form the state X to a ﬁnal state having, at best, no less uncer-
tainty than what is initially speciﬁed by the statistics of X, i.e.,
(cid:3)
H(X ′)open ≥ H(X).

IV. ENTROPIC FORMULATION OF CONTROLLABILITY AND

OBSERVABILITY

The ﬁrst instance of the general control problem that we now
proceed to study involves the dual concepts of controllability
and observability. In control theory, the importance of these con-
cepts arises from the fact that they characterize mathematically
the input-output structure of a system intended to be controlled,
and thereby determine whether a given control task is realizable
or not [2], [3]. In short, controllability is concerned with the
possibilities and limitations of the actuation channel or, in other
words, the class of control dynamics that can be effected by a
controller. Observability, on the other hand, is concerned with
the set of states which are accessible to estimation given that
a particular sensor channel is used. In this section, prompted
by preliminary results obtained by Lloyd and Slotine [22], we
deﬁne entropic analogs of the widely held control-theoretic def-

6

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

initions of controllability and observability, and explore the con-
sequences of these new deﬁnitions.

A. Controllability

In its simplest expression, we deﬁne a system to be control-
lable at X = x if, for any ﬁnal speciﬁed state X ′ = x′, there
exists at least one control input for C driving the controlled sys-
tem from x to x′ [2], [3]. In the case of stochastic systems, like
those of interest here, we will say that a state x is approximately
controllable if there exists an actuation subdynamics connect-
ing x to any values of X ′ with non-vanishing probability. As an
extension of these deﬁnitions, some authors also deﬁne a sys-
tem to be completely controllable whenever the controllability
conditions are veriﬁed for all initial states x ∈ X .

In terms of the actuation matrix, controllability for x, or more
precisely perfect controllability as opposed to approximate con-
trollability, must correspond to the case for which there exists
at least one c such that every value x′ is reachable from x with
probability 1. Let Cx denote the set of all control values c for
which p(x′|x, c) = 1 over all x′ ∈ X and for x ﬁxed. If we
restrict the controller’s admissible states to values in Cx, i.e., if
supp(C) = supp(Cx), then as a necessary and sufﬁcient con-
dition for perfect controllability we have the following result.
(The result was originally put forward in [22] without a com-
plete proof.)

Theorem 1: A system is perfectly controllable at x if and only
if p(x′|x) 6= 0 for all x′ and there exists a non-empty set C of
control values such that

H(X ′|x, C) =

H(X ′|x, c)pC (c) = 0,

(22)

where

H(X ′|x, c) = −

p(x′|x, c) log p(x′|x, c).

(23)

Proof: If x is controllable, then for each x′ there exists at
least one control value c ∈ Cx such that p(x′|x, c) = 1, and thus
H(X ′|x, c) = 0. As this holds true for all c ∈ Cx, we must
also have that the average conditional entropy over C vanishes.
Moreover, the condition p(x′|x, c) = 1 for all x′, and for at least
one c such that pC(c) 6= 0, implies

p(x′|x) =

p(x′|x, c)pC (c) 6= 0,

(24)

c∈C
X

x′∈X
X

c∈C
X

so that the direct part of the result is proved. To prove the con-
verse, note that if p(x′|x) 6= 0 for a given x′, then there is at
least one value c for which p(x′|x, c) 6= 0. In fact, to be more
precise, p(x′|x, c) = 1 for these particular values x′, x and c.
Indeed, the condition H(X ′|x, C) = 0 implies that the random
variable X ′ conditioned on x and c must assume only one value
with probability one for all c ∈ supp(C). This is veriﬁed for
any state value x′, so that for all x′ ∈ X there exists a c such
that p(x′|x, c) = 1.

From a perspective centered on information, H(X ′|x, C) has
the desirable feature of being interpretable as the residual uncer-
tainty, or uncontrolled variation, left in the output X ′ when the

controller’s state C is chosen with respect to the initial value x
[22]. If one regards C as an input to a communication chan-
nel and X ′ as the channel output, then the degree to which
the ﬁnal state X ′ is controlled by manipulating the controller’s
state can be identiﬁed with the conditional mutual information
I(X ′; C|x). This latter quantity can be expressed either using a
formula similar to Eq.( 4), or by using the expression

I(X ′; C|x) = H(X ′|x) − H(X ′|x, C),

(25)

which is a conditional version of the chain rule

I(X; Y ) = H(X) − H(X|Y ),

(26)

valid for any random variables X and Y .

It is interesting to remark that the two above equations al-
low for another interpretation of H(X ′|x, C). The conditional
entropy H(X|Y ), entering in (26), is often interpreted in com-
munication theory as representing an information loss (the so-
called equivocation of Shannon [25]), which results from sub-
stracting the maximum noiseless capacity I(X; X) = H(X) of
a communication channel with input X and output Y from the
actual capacity of that channel as measured by I(X; Y ). In our
case, we can apply the same reasoning to Eq.(25), and interpret
the quantity H(X ′|x, C) as a control loss which appears as a
negative contribution in the expression of I(X ′; C|x), the num-
ber of bits of accuracy to which specifying the control variable
speciﬁes the output state of the controlled system. This means
that higher is the quantity H(X ′|x, C), then higher is the uncer-
tainty or imprecision associated with the outcome of X ′ upon
application of the control action.

In order to characterize the complete controllability of a sys-
tem, one may also look at the control loss over the entire state
space of X, and, in that respect, deﬁne

LC = min
pC (c)

H(X ′|X, C)

(27)

as the average control loss over all input states x. In the above
equation, the conditional entropy H(X ′|X, C) is obtained by
averaging H(X ′|x, C) over X. Also, for the quantity LC to
be meaningful, we must now assume that the set of admissible
controls contains all the subsets Cx ⊆ C used to assess the con-
trollability properties of each value x, so that

C =

Cx.

(28)

x∈X
[
with supp(Cx) 6= ∅ for all x ∈ X . In terms of the average con-
trol loss we have that a system is perfectly controllable over the
support of X if LC = 0 and p(x′|x) 6= 0 for all x′. In any other
cases, it is approximately controllable for at least one x. The
proof of this result follows essentially by noting that, since dis-
crete entropy is positive deﬁnite, the condition H(X ′|X, C) = 0
necessarily implies H(X ′|x, C) = 0 for all x ∈ supp(X).

The next two results relate the average control loss with other
quantities of interest. Control graphs containing the puriﬁca-
tion of the actuation channel, as depicted in Figure 2, are used
throughout the rest of this section.

H. Touchette and S. Lloyd

7

Theorem 2: Under the assumption that X ′ is a deterministic
random variable conditioned on the values x, c, and z (puriﬁ-
cation assumption), we have LC ≤ H(Z) with equality if, and
only if, H(Z|X ′, X, C) = 0.

Proof: Using the general inequality H(X) ≤ H(X, Y ),

and the chain rule for joint entropies, one may write

H(X ′|X, C) ≤ H(X ′, Z|X, C)

= H(Z|X, C) + H(X ′|X, C, Z).

(29)

However, H(X ′|X, C, Z) = 0, since the knowledge of the
triplet (x, c, z) is sufﬁcient to infer the value of X ′ (see the con-
ditions in Section II). Hence,

H(X ′|X, C) ≤ H(Z|X, C)

= H(Z),

(30)

where the last equality follows from the fact that Z is chosen
independently of X and C as illustrated in the control graph of
Figure 2a. Now, from the chain rule

H(X ′, Z|X, C) = H(X ′|X, C) + H(Z|X ′, X, C),

(31)

it is clear that equality in the ﬁrst line of expression (29) is
achieved if and only if H(Z|X ′, X, C) = 0.

The result of Theorem 2 demonstrates that the uncertainty as-
sociated with the control of the state X is upper bounded by the
noise level of the actuation channel measured by the entropy of
Z. This agrees well with the fact that one goal of controllers is
to protect a system against the effects of its environment so as to
ensure that it is minimally affected by noise. In the limit where
the control loss vanishes, the state X ′ of the controlled system
should show no variability given that we know the initial state
and the control action, even in the presence of actuation noise,
and should thus be independent of the random variable Z. This
is the essence of the next two results which hold for the same
conditions as Theorem 2.

Theorem 3: LC = I(X ′; Z|X, C).

Proof: From the chain rule of mutual information, we can

easily derive

I(X ′; Z|X, C) = H(X ′|X, C) − H(X ′|X, C, Z).

(32)

Thus, I(X ′; Z|X, C) = H(X ′|X, C) if we use again the deter-
ministic property of the random variable X ′|x, c, z upon puriﬁ-
cation of p(x′|x, c).

Theorem 4: LC = I(X ′; X, C, Z) − I(X ′; X, C).

Proof: Using the chain rule of mutual information, we

write

I(X ′; X, C, Z) = H(X ′) − H(X ′|X, C, Z)
= H(X ′) − H(X ′|X, C, Z)

+H(X ′|X, C) − H(X ′|X, C)
= I(X ′; X, C) + I(X ′; Z|X, C).

(33)

For the last equality, we have used Eq.(32). Now, by substituting
LC = I(X ′; Z|X, C) from the previous theorem, we obtain the
desired result.

As a direct corollary of these two results, we have that a
system is completely and perfectly controllable if, and only if,
I(X ′; Z|X, C) is equal to zero or equivalently if, and only if,

I(X ′; X, C, Z) = I(X ′; X, C).

(34)

Hence, a necessary and sufﬁcient entropic condition for per-
fect controllability is that the ﬁnal state of the controlled sys-
tem, after the actuation step, is statistically independent of the
noise variable Z given X and C. In that case, the ‘information’
I(X ′; Z|X, C) conveyed in the form of noise from Z to the con-
trolled system is zero. Another ‘common sense’ interpretation
of this result can be given if the quantity I(X ′; Z|X, C) is in-
stead viewed as representing the ‘information’ about X ′ that has
been transferred to the non-controllable state Z in the form of
‘lost’ correlations.

Interestingly, such a perspective on control systems focus-
ing on noise and information protection reminds us that error-
correcting codes are designed just like control systems: the in-
formation duplicated by a code, when corrupted by noise, is
used to detect errors (sensor step) which are then corrected by
enacting speciﬁc correcting or erasure actions (actuation step)
[25], [32], [33]. This somewhat overlooked aspects of error-
correcting codes can be strengthened even further if probabili-
ties accounting for undetected and uncorrected errors are mod-
eled by means of communication channels similar to the sensor
and actuation channels. In this context, the issue of determining
whether or not a prescribed set of erasure actions is sufﬁcient to
correct for errors known to occur is determined by the control
loss.

B. Observability

The concept of observability is concerned with the issue of
inferring the state X of the controlled system based on some
knowledge or data of the state provided by a measurement ap-
paratus, taken here to correspond to C. More precisely, a con-
trolled system is termed perfectly observable if the sensor’s tran-
sition matrix p(c|x) maps no two values of X to a single obser-
vational output value c, or in other words if for all c ∈ C there
exists only one value x such that p(x|c) = 1. As a consequence,
we have the following result [22]. (We omit the proof which
readily follows from well-known properties of entropy.)

Theorem 5: A system with state variable X is perfectly ob-
servable, with respect to all observed value c ∈ supp(C), if and
only if

H(X|C) =

H(X|c)pC(c) = 0.

(35)

c∈C
X

The information-theoretic analog of a perfectly observable
system is a lossless communication channel X → Y charac-
terized by H(X|Y ) = 0 for all input distributions [29]. As a
consequence of this association, we interpret the conditional en-
tropy H(X|C) as the information loss, or sensor loss, of the
sensor channel, henceforth denoted by LS. This quantity being
deﬁned, we now consider the problem of extending the results
on controllability into the domain of observability. Speciﬁcally,
given the similarity between the average control loss LC and the
sensor loss, do we obtain true results for observability by merely
substituting LC by LS in Theorems 2 and 3?

8

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

The answer, rather deceptively, is no for a simple reason: the
fact that a communication channel is lossless has nothing to do
with the fact that it can be non-deterministic. A convincing ex-
ample of this is a communication channel that maps the single-
ton input set X = {0} to multiple instances of the output set
C with equal probabilities. This is clearly a non-deterministic
channel according to our deﬁnition, and, yet, since there is only
one possible value for X, the conditional entropy H(X|c) must
be equal to zero for all c ∈ C. Hence, contrary to Theorem
2, there can be no result stating that the observation loss LS is
bounded above by the entropy of the random variable respon-
sible for the non-deterministic properties of the sensor channel.
However, we are not far from a similar result: by analyzing the
meaning of the sensor loss a bit further, one can realize that
the generalization of Theorem 2 for observability may in fact
be derived using the ‘backward’ version of the sensor channel.
More precisely, LS ≤ H(ZB) where ZB is now the random
variable associated with the puriﬁcation of the transition matrix
p(x|c). To prove this result, the reader may revise the proof
of Theorem 2, and replace the forward puriﬁcation condition
H(C|X, Z) = 0 for the sensor channel by its backward analog
H(X|C, ZB) = 0.

To close this section, we present next what is left to general-
ization of the results on controllability. One example aimed at
illustrating the interplay between the controllability and observ-
ability properties of a system is also given.

Theorem 6: If the state X is perfectly observable,

then
(The random variable Z stands for the pu-

I(X; Z|C) = 0.
riﬁcation variable of the ‘forward’ sensor channel p(c|x).)

Proof: The proof

H(X|C) ≥ H(X|C, Z),
H(X|C, Z) = 0. Thus by the chain rule

is rather straightforward.

Since
the condition LS = 0 implies

I(X; Z|C) = H(X|C) − H(X|C, Z),

(36)

we conclude with I(X; Z|C) = 0.

Corollary 7: If LS = 0, then I(X; C, Z) = I(X; C).

The interpretations of the two above results follow closely
those given for controllability. We will thus not discuss these
results furthermore except to mention that, contrary to the case
of controllability, I(X; Z|C) = 0 is not a sufﬁcient condition
for a system to be observable. This follows simply from the
fact that I(X; Z|C) = 0 implies H(X|C) = H(X|C, Z), and
at this point the puriﬁcation condition H(C|X, Z) = 0 for the
sensor channel is of no help to obtain H(X|C) = 0.

open = ∆H c

Example 2: Consider again the control system of Figure 3.
Given the actuation rules described by the CNOT logical gate, it
can be veriﬁed easily that for X = 0 or 1, H(X ′|x, C) = 0 and
p(x′|x) 6= 0 for all x′. Therefore, the controlled system is com-
pletely and perfectly controllable. This implies, in particular,
that ∆H c
closed = 0, and that the ﬁnal state of the con-
trolled system may be actuated to a single value with probability
1, as noted before. For the latter observation, note that X ′ = x′
with probability 1 so long as the initial state X is known with
probability 1 (perfectly observable). In general, if a system is
perfectly controllable (actuation property) and perfectly observ-
able (sensor property), then it is possible to perfectly control its

state to any desired value with vanishing probability of error. In
such a case, we can say that the system is closed-loop control-
(cid:3)
lable.

C. The case of continuous random variables

The concept of a deterministic continuous random variable
is somewhat ill-deﬁned, and, in any case, cannot be associ-
ated with the condition H(X) = 0 formally. (Consider, e.g.,
the peaked distribution p(x) = δ(x − x0) which is such that
H(X) = −∞.) To circumvent this difﬁculty, controllability and
observability for continuous random variables may be extended
via a quantization or coarse-graining of the relevant state spaces
[30]. For example, a continuous-state system can be deﬁned
to be perfectly controllable at x if for every ﬁnal destination x′
there exists at least one control value c which forces the system
to reach a small neighborhood of radius ∆ > 0 around x′ with
probability 1. Equivalently, x can be termed perfectly control-
lable to accuracy ∆ if the variable x∆ obtained by quantizing X
at a scale ∆ is perfectly controllable. Similar deﬁnitions involv-
ing quantized random variables can also be given for observ-
ability. The recourse to the quantized description of continuous
variables has the virtue that H(X ∆) and H(X ∆|C∆) are well-
deﬁned functions which cannot be inﬁnite. It is also the natural
representation used for representing continuous-state models on
computers.

V. STABILITY AND ENTROPY REDUCTION

The emphasis in the previous section was on proving upper
limits for the control and the observation loss, and on ﬁnding
conditions for which these losses vanish.
In this section, we
depart from these quantities to focus our attention on other mea-
sures which are interesting in view of the stability properties of
a controlled system. How can a system be stabilized to a tar-
get state or a target subset (attractor) of states? Also, how much
information does a controller need to gather in order to achieve
successfully a stabilization procedure? To answer these ques-
tions, we ﬁrst propose an entropic criterion of stability, and try
to justify its usefulness for problems of control. In a second step,
we investigate the quantitative relationship between the closed-
loop mutual information I(X; C) and the gain in stability which
results from using information in a control process.

A. Stochastic stability

Intuitively, a stable system is a system which, when activated
in the proximity of a desired operating point, stays relatively
close to that point indeﬁnitely in time, even in the presence of
small perturbations. In the ﬁeld of control engineering, there
exist several formalizations of this intuition, some less stringent
than others, whose range of applications depend on theoretical
as well as practical considerations. In the next paragraphs, we
present a selection of three important criteria of stability which
will be discussed thereafter in the light of information theory.

• Bounded input-bounded output stability (BIBO) [34] . A
system is BIBO stable if any bounded input signals feeding
that system, such as control inputs or environment distur-
bances, cause an always bounded response for the system’s
observables. The limitations on the signals can be in the

H. Touchette and S. Lloyd

9

form of a bound on the distance between the actual and
desired response, a limitation of the signals’ variances, a
power limitation, etc.

• Stability in the sense of Lyapunov [3], [34] . A state x∗ is
stable if any discrete-time trajectory {xn}∞
n=0 initiated by
a point x0, chosen in a small neighborhood of x∗, stays
arbitrarily close to that state at all time steps n. Mathe-
matically, this translates into the following. The state x∗
is stable if for every ε > 0, one can ﬁnd δ(ε) > 0 such
that kx0 − x∗k ≤ δ implies kxn − x∗k ≤ ε, for all n > 0.
(k·k is an arbitrary norm to be speciﬁed.) The ball of radius
ε around x∗ if often called the Lyapunov stability region.
Also, if

lim
n→∞

kxn − x∗k = 0,

(37)

then x∗ is said to be asymptotically stable. This criteria,
obviously, can be generalized to continuous-time trajecto-
ries.

• Relative entropy convergence [35], [36]. The relative en-
tropy or Kullback-Leibler distance between two probabil-
ity distributions p(x) and q(x), as deﬁned by

D(p||q) =

p(x) log

(38)

p(x)
q(x)

,

x∈X
X

is a quantity that is always positive, and vanishes only when
p(x) = q(x) for all x ∈ X . For ﬁxed q(x) is it also a con-
vex function of p(x). This means that in the interior of
a closed region of the simplex deﬁned by D(p||q) ≤ d,
where q(x) is ﬁxed, there exist only distributions p(x)
whose ‘distance’ to q(x) is smaller than d [30]. Using this
property, we can deﬁne a distribution-analog of the Lya-
punov stability region by requiring that the probability dis-
tribution pXn (xn) associated with the state Xn approaches
a stable target or limiting distribution p∗(x) within a dis-
tance d, i.e.,

D(pXn+1(xn+1)||p∗(x)) ≤ D(pXn (xn)||p∗(x)),

(39)

and D(pXn (xn)||p∗(x)) ≤ d for all n > 0. The gener-
alization to continuous-time dynamics is straightforward
here again.

In view of the above deﬁnitions, it appears logical to reduce
the problem of stabilizing a dynamical system to that of decreas-
ing its entropy, or at least immunizing it from sources of entropy
like those associated with environment noise, motion instabili-
ties, and incomplete speciﬁcation of control conditions. This
entropic aspect of stabilization is implicit in all of the above cri-
teria insofar as a probabilistic description of systems focusing
on sets of responses, rather than on individual response one at
a time, is adopted [37]-[39]. For example, a system is BIBO
stable if the uncertainty (viz, entropy) associated with the con-
trol inputs is not ampliﬁed arbitrarily at the output. Similarly,
stability in the sense of Lyapunov implies that a set of initial
conditions with entropy proportional to log δ is constrained to
evolve into states which are generally of lower entropy, espe-
cially when the system of interest is asymptotically stable. The

same argument applies essentially for the relative entropy crite-
rion: in this case, the initial and ﬁnal entropy of the controlled
system, as measured approximately by the logarithm of the sup-
port of p(x0) and p(xn), respectively, is most likely to be such
that H(X0) ≥ H(Xn) for sufﬁciently large n. For, again, what
is usually sought in controlling a system is to conﬁne its possi-
ble responses to lie in a set as small as possible, starting from a
wide range of initial states.

Many other evidences can be invoked to support the point that
stabilizing a system is fundamentally a problem of entropy re-
duction. The following is only a partial list:

• Least-squares controllers aimed at minimizing the average
squared distance between a Gaussian distributed state X
and a target state x∗ are minimum entropy designs [21];

• Linear unstable systems, with eigenvalues located in the
left half part of the complex plane, in addition to unstable
nonlinear systems, having positive Lyapunov exponents,
are all characterized by positive entropy rates [40];

• From the standpoint of nonequilibrium thermodynamics a
system is termed stable if there is more overall entropy dis-
sipation in the system than there is entropy generation [9],
[35], [39].

In the light of all these points, we propose to study the two
following problems. First, given the initial state X and its en-
tropy H(X), a set of actuation subdynamics, and the type of
controller (open- or closed-loop), what is the maximum entropy
reduction achievable during the controlled transition from X to
X ′? Second, what is the quantitative relationship, if there exists
one, between the maximal open-loop entropy reduction and the
closed-loop entropy reduction?

Note in relation to these questions that, for control purposes,
it does not sufﬁce to reduce the entropy of X ′ conditionally on
the state of another system (the controller in particular). For
instance, the fact that H(X ′|C) vanishes for a given controller
acting on a system does not imply by itself that H(X ′) must
vanish as well, or that X ′ is stabilized. What is required for
control is that actuators modify the dynamics of the system in-
tended to be controlled by acting directly on it, so as to reduce
the marginal entropy H(X ′). This unconditional aspect of sta-
bility has been discussed in more details in [1].

B. Open-loop control optimality

Using the concavity property of entropy, and the fact that
∆H open is upper bounded by the maximum of ∆H c
open over all
control values c, we show in this section that the maximum de-
crease of entropy achieved by a particular subdynamics of con-
trol variable

∆H c

ˆc = arg max
c∈C
is open-loop optimal in the sense that no random (i.e., non-
deterministic) choice of the controller’s state can improve upon
that decrease. More precisely, we have the following results.
(Theorem 9 was originally stated without a proof in [1].)

open

(40)

Lemma 8: For any initial state X, the open-loop entropy re-

duction ∆H open satisﬁes

∆Hopen ≤ ∆H C

open,

(41)

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

10

where

∆H C

open =

pC(c)∆H c

open

c∈C
X

= H(X) − H(X ′|C)open

(42)

with ∆H c
and only if I(X ′; C) = 0.

open deﬁned as in Eq.(12). The equality is achieved if

Proof: Using the inequality H(X ′) ≤ H(X ′|C), we write

directly

∆Hopen = H(X) − H(X ′)open

≤ H(X) − H(X ′|C)open.

(43)

Now, let us prove the equality part. If C is statistically indepen-
dent of X ′, then H(X ′|C) = H(X ′), and

∆Hopen = ∆H C

open.

(44)

Conversely, the above equality implies H(X ′|C) = H(X ′), and
thus we must have that C is independent of X ′.

Theorem 9: The entropy reduction achieved by a set of ac-
tuation subdynamics used in open-loop control is always such
that

∆Hopen ≤ max
c∈C

∆H c

open,

(45)

for all pX (x). The equality can always be achieved for the de-
terministic controller C = ˆc, with ˆc deﬁned as in Eq.(40).

Proof: The average conditional entropy H(X ′|C) is al-

ways such that

H(X ′|c) ≤

pC (c)H(X ′|c).

(46)

min
c∈C

c∈C
X

Therefore, making use of the previous lemma, we obtain

∆Hopen ≤ ∆H C

open
≤ H(X) − min
c∈C
open.

∆H c

= max
c∈C

H(X ′|c)

(47)

Also, note that if C = ˆc with probability 1, then the two above
inequalities are saturated since in this case I(X ′; C) = 0 and
∆H C

open = ∆H ˆc

open.

An open-loop controller or a control strategy is called pure if
the control random variable C is deterministic, i.e., if it assumes
only one value with probability 1. An open-loop controller that
is not pure is called mixed. (We also say that a mixed controller
activates a mixture of control actions.) In view of these deﬁ-
nitions, what we have just proved is that a pure controller with
C = ˆc is necessarily optimal; any mixture of the control vari-
able either achieves the maximum entropy decrease prescribed
by Eq.(45) or yields a smaller value. As shown in the next ex-
ample, this is so even if the actuation subdynamics used in the
control process are deterministic.

Example 3: For the CNOT controller of Example 1, we noted
that H(X ′)open = H(X), or equivalently that ∆Hopen = 0, only

at best. To be more precise, ∆Hopen = 0 only if a pure controller
is used or if H(X) = 1 bit (already at maximum entropy). If
the control is mixed, and if H(X) < 1 bit, then ∆Hopen must
necessarily be negative. This is so because uncertainty as to
which actuation rule is used must imply uncertainty as to which
(cid:3)
state the controlled system is actuated to.

Note that purity alone is not a sufﬁcient condition for open-
loop optimality, nor it is a necessary one in fact. To see this,
note on the one hand that a pure controller having

C = arg min
c∈C

∆H c

open

(48)

with probability one is surely not optimal, unless all entropy
reductions ∆H c
open have the same value. On the other hand,
to prove that a mixed controller can be optimal, note that if
any subset Co ⊆ C of actuation subdynamics is such that
p(x′|c) = pX ′(x′), and ∆H c
open assumes a constant value for
all c ∈ Co, then one can build an optimal controller by choosing
a non-deterministic distribution p(c) with supp(C) = Co.

C. Closed-loop control optimality

The distinguishing characteristic of an open-loop controller is
that it usually fails to operate efﬁciently when faced with uncer-
tainty and noise. An open-loop controller acting independently
of the state of the controlled system, or solely based on the sta-
tistical information provided by the distribution pX (x), cannot
reliably determine which control subdynamics is to be applied
in order for the initial (a priori unknown) state X to be propa-
gated to a given target state. Furthermore, an open-loop control
system cannot compensate actively in time for any disturbances
that add to the actuator’s driving state (actuation noise). To over-
come these difﬁculties, the controller must be adaptive; that is to
say, it must be capable of estimating the unpredictable features
of the controlled system during the control process, and must be
able to use the information provided by estimation to decide of
speciﬁc control actions, just as in closed-loop control.

A basic closed-loop controller was presented in Example 1.
For this example, we noted that the perfect knowledge of the
initial state’s value (X = 0 or 1) enabled the controller to de-
cide which actuation subdynamics (identity or permutation) is
to be used in order to actuate the system to X ′ = 0 with proba-
bility 1. The fact that the sensor gathers I(X; C) = H(X) bits
of information during estimation is a necessary condition for
this speciﬁc controller to achieve ∆Hclosed = 0, since having
I(X; C) < H(X) may result in generating the value X ′ = 1
with non-vanishing probability. In general, just as a subdynam-
ics mapping the input states {0, 1} to the single value {0} would
require no information to force X ′ to assume the value 0, we
expect that the closed-loop entropy reduction should not only
depend on I(X; C), the effective information available to the
controller, but should also depend on the reduction of entropy
attainable by open-loop control. The next theorem, which con-
stitutes the main result of this work, embodies exactly this state-
ment by showing that one bit of information gathered by the
controller has a maximum value of one bit in the improvement
of entropy reduction that closed-loop gives over open-loop con-
trol.

H. Touchette and S. Lloyd

11

Theorem 10: The amount of entropy

∆Hclosed = H(X) − H(X ′)closed

(49)

that can be extracted from a system with given initial state X by
using a closed-loop controller with ﬁxed set of actuation subdy-
namics satisﬁes

where

∆Hclosed ≤ ∆H max

open + I(X; C).

∆H max

open =

max
pX (x)∈P,c∈C

∆H c

open

(50)

(51)

is the maximum entropy decrease that can be obtained by (pure)
open-loop control over any input distribution chosen in the set
P of all probability distributions.

A proof of the result, based on the conservation of entropy for
closed systems, was given in [1] following results found in [41],
[42]. Here, we present an alternative proof based on conditional
analysis which has the advantage over our previous work to give
some indications about the conditions for equality in (50). Some
of these conditions are derived in the next section.

Proof: Given that ∆H max

open is the optimal entropy reduction
for open-loop control over any input distribution, we can write

H(X ′)open ≥ H(X) − ∆H max
open .

(52)

Now, using the fact that a closed-loop controller is formally
equivalent to an ensemble of open-loop controllers acting on the
conditional supports supp(X|c) instead of supp(X), we also
have for all c ∈ C

H(X ′|c)closed ≥ H(X|c) − ∆H max
open ,

(53)

and, on average,

H(X ′|C)closed ≥ H(X|C) − ∆H max
open .

(54)

That ∆H max
open must enter in the lower bounds of H(X ′)open and
H(X ′)closed can be explained in other words by saying that each
conditional distribution p(x|c) is as a legitimate input distri-
bution for the initial state of the controlled system.
It is, in
any cases, an element of P. This being said, notice now that
H(X ′) ≥ H(X ′|C) implies

H(X ′)closed ≥ H(X|C) − ∆H max
open .

(55)

Hence, we obtain

∆Hclosed ≤ H(X) − H(X|C) + ∆H max
closed
= I(X; C) + ∆H max
closed,

The above theorem enables us to ﬁnally understand all the
results of Example 1. As noted already, since the actuation sub-
dynamics consist of permutations, we have ∆H max
open = 0 for any
distribution pX (x). Thus, we should have ∆Hclosed ≤ I(X; C).
For the particular case studied where C = X, the controller is
found to be optimal, i.e., it achieves the maximum possible en-
tropy reduction ∆Hclosed = I(X; C). This proves, incidentally,
that the bound of inequality (50) is tight. In general, we may
deﬁne a control system to be optimal in terms of information
if the gain in stability obtained by substracting H(X ′)open from
H(X ′)closed is exactly equal to the sensor mutual information
I(X; C). Equivalently, a closed-loop control system is optimal
if its efﬁciency η, deﬁned by

η =

H(X ′)open − H(X ′)closed
I(X; C)

,

(57)

is equal to 1.

Having determined that optimal controllers do exist, we now
turn to the problem of ﬁnding general conditions under which
a given controller is found to be either optimal (η = 1) or
sub-optimal (η < 1). By analyzing thoroughly the details of
the proof of Theorem 10, one can note that the assessment
of the condition I(X ′; C) = 0, which was the necessary and
sufﬁcient condition for open-loop optimality, is not sufﬁcient
here to conclude that a closed-loop controller is optimal. This
comes as a result of the fact that not all control subdynam-
ics applied in a closed-loop fashion are such that ∆H c
closed =
∆H max
open in general. Therefore the average ﬁnal condition en-
tropy H(X ′|C)closed need not necessarily be equal to the bound
imposed by inequality (54). However, in a scenario where the
open and ∆H c
entropy reductions ∆H c
closed are both equal to a
constant for all control subdynamics, then we effectively re-
cover an analog of the open-loop optimality condition, namely
that a zero mutual information between the controller and the
controlled system after actuation is a necessary and sufﬁcient
condition for optimality.

Theorem 11: Under the condition that, for all c ∈ C,

∆H c

open = ∆H c

closed = ∆H,

(58)

where ∆H is a constant, then a closed-loop controller is optimal
if and only if I(X ′; C) = 0.

Proof: To prove the sufﬁciency part of the theorem, note
that the constancy condition (58) implies that the minimum for
H(X ′) open equals H(X) − ∆H. Similarly, closed-loop control
must be such that

H(X ′|C)closed = H(X|C) − ∆H.

(59)

which is the desired upper bound. To close the proof, note that
∆H max
open cannot be evaluated using the initial distribution pX (x)
alone because the maximum reduction of entropy in open-loop
control starting from pX (x) may differ from the reduction of
entropy obtained when some actuation channel is applied in
closed-loop to p(x|c). See [43] for a speciﬁc example of this.

we obtain

(56)

Combining these results with the fact that I(X ′; C) = 0, or
equivalently that

H(X ′)closed = H(X ′|C)closed,

(60)

H(X ′)min

open − H(X ′)closed = H(X) − H(X|C)

= I(X; C).

(61)

12

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

To prove the converse, namely that optimality under condition
(58) implies I(X ′; C) = 0, notice that Eq.(59) leads to

H(X ′)min

open − H(X ′|C)closed = H(X) − H(X|C)

= I(X; C).

(62)

Hence, given that we have optimality, i.e., given Eq.(61), then
X ′ must effectively be independent of C.

Example 4: Consider again the now familiar CNOT con-
troller. Let us assume that instead of the perfect sensor chan-
nel C = X, we have a binary symmetric channel such that
p(c = x|x) = 1 − e and p(c = x ⊕ 1|x) = e where 0 ≤ e ≤ 1,
i.e., an error in the transmission occurs with probability e [30].
The mutual information for this channel is readily calculated to
be

I(X(t); C(t)) 6= 0 if purposeful control is to take place. Fi-
nally, are we simply legitimized to extend a result derived in
the context of a Markovian or memoryless model of controllers
to sampled continuous-time processes, even if the sampled ver-
sion of such processes has a memoryless structure? Surely, the
answer is no.

To overcome these problems, we suggest the following con-
ditional version of the optimality theorem. Let X(t − ∆t), X(t)
and X(t + ∆t) be three consecutive sampled points of a con-
trolled trajectory X(t). Also, let C(t − ∆t) and C(t) be the
states of the controller during the time interval in which the state
of the controlled system is estimated. (The actuation step is as-
sumed to take place between the time instants t and t + ∆t.)
Then, by redeﬁning the entropy reductions as conditional en-
tropy reductions following

I(X; C) = H(C) −

p(x)H(C|x)

∆H t = H(X(t)|Ct−∆t) − H(X(t + ∆t)|Ct−∆t),

(67)

Xx∈{0,1}
= H(C) − H(e),

where

H(e) = −e log e − (1 − e) log(1 − e)

where Ct represents the control history up to time t, we must
have

∆H t

closed ≤ ∆H t

open + I(X(t); C(t)|Ct−∆t).

(68)

(63)

(64)

is the binary entropy function. By proceeding similarly as in
Example 1, the distribution of the ﬁnal controlled state can be
calculated. The solution is pX ′(0) = 1 − e and pX ′(1) = e, so
that H(X ′) = H(e) and

Note that by thus conditioning all quantities with Ct−∆t, we
extend the applicability of the closed-loop optimality theorem
to any class of control processes, be they memoryless or not.
Now, since

∆Hclosed = H(X) − H(e).

(65)

I(X(t − ∆t); C(t − ∆t)|Ct−∆t) = 0

(69)

By comparing the value of ∆Hclosed with the mutual information
I(X; C) (recall that ∆H max
open = 0), we arrive at the conclusion
that the controller is optimal for e = 0, e = 1 (perfect sensor
channel), and for H(X) = 1 (maximum entropy state). In going
through more calculations, it can be shown that these cases of
(cid:3)
optimality are all such that I(X ′; C) = 0.

D. Continuous-time limit

In an attempt to derive a differential analog of the closed-loop
optimality theorem for systems evolving continuously in time,
one may be inclined to proceed as follows: sample the state, say
X(t), of a controlled system at two time instants separated by
some (inﬁnitesimal) interval ∆t, and from there directly apply
inequality (50) to the open- and closed-loop entropy reductions
associated with the two end-points X(t) and X(t + ∆t) using
I(X(t); C(t)) as the information gathered at time t . However
sound this approach might appear, it unfortunately proves to
be inconsistent for many reasons. First, although one may ob-
tain well-deﬁned rates for H(X(t)) in the open- or closed-loop
regime, the quantity

lim
∆t→0

I(X(t); C(t))
∆t

(66)

does not constitute a rate, for I(X(t); C(t)) is not a differ-
ential element which vanishes as ∆t approaches 0. Second,
our very deﬁnition of open-loop control, namely the require-
ment that I(X; C) be equal to 0 prior to actuation, fails to
Indeed, open-loop con-
apply for continuous-time dynamics.
trollers operating continuously in time must always be such that

by deﬁnition of the mutual information, we also have

∆H t

closed ≤ ∆H t

open + I(X(t); C(t)|Ct−∆t)
−I(X(t − ∆t); C(t − ∆t)|Ct−∆t).

(70)

Hence, by dividing both sides of the inequality by ∆t, and by
taking the limit ∆t → 0, we obtain the rate equation

˙Hclosed ≤ ˙Hopen + ˙I.

(71)

This equation relates the rate at which the conditional entropy
H(X(t)|Ct−∆t) is dissipated in time with the rate at which the
conditional mutual information I(X(t); C(t)|Ct−∆t) is gath-
ered upon estimation. The difference between the above infor-
mation rate and the previous pseudo-rate reported in Eq.(66) lies
in the fact that I(X(t); C(t)|Ct−∆t) represents the differential
information gathered during the latest estimation stage of the
control process. It does not include past correlations induced by
the control history Ct−∆t. This sort of conditioning allows, in
passing, a perfectly meaningful re-deﬁnition of open-loop con-
trol in continuous-time, namely ˙I = 0, since the only corre-
lations between X(t) and C(t) which can be accounted for in
the absence of direct estimation are those due to the past control
history.

VI. APPLICATIONS

A. Proportional controllers

There are several controllers, including automatic ﬂight guid-
ance systems, which have the character of applying a control

H. Touchette and S. Lloyd

signal with amplitude proportional to the distance or error be-
tween some estimate ˆX of the state X, and a desired target point
x∗.
In the control engineering literature, such controllers are
designated simply by the term proportional controllers [34]. As
a simple version of a controller of this type, we study in this
section the following system:

and

X ′ = X − C
C = ˆX − x∗,

(72)

(73)

(74)

with all random variables assuming values on the real line. For
simplicity, we set x∗ = 0 and consider two different estimation
or sensor channels deﬁned mathematically by

and

C∆ = ˆX =

X
∆

+

∆,

1
2

(cid:18)(cid:22)

(cid:23)

(cid:19)

CZ = ˆX = X + Z,

where Z ∼ N (0, N ) (Gaussian distribution with zero mean and
variance N ). The ﬁrst kind of estimation, Eq.(73), is a coarse-
grained measurement of X with a grid of size ∆; it basically
allows the controller to ‘see’ X within a precision ∆, and selects
the middle coordinate of each cell of the grid as the control value
for C∆. The other sensor channel represented by the control
state CZ is simply the Gaussian channel with noise variance N .
Let us start our study of the proportional controller by consid-
ering the coarse-grained sensor channel ﬁrst. If we assume that
X ∼ U(0, ε) (uniform distribution over an interval ε centered
around 0), and pose that ε/∆ is an integer, then we must have

I(X; C∆) = log(ε/∆).

(75)

Now, to obtain pX ′ (x′)closed, note that the conditional random
variables X|c deﬁned by conditional analysis are all uniformly
distributed over non-overlapping intervals of width ε/∆, and
that, moreover, all of these intervals must be moved under
the control law around X ′ = 0 without deformation. Hence,
X ′ ∼ U(0, ∆), and

13

(79)

(80)

(81)

I(X; CZ) =

P
N
open = 0 (recall that ∆H max

1 +

log

1
2

(cid:18)

.

(cid:19)

Again, ∆H max
open does not depend on
the choice of the sensor channel), and so we conclude that op-
timality is achieved only in the limit where the signal-to-noise
ratio goes to inﬁnity. Non-optimality, for this control setup, can
be traced back to the presence of some overlap between the dif-
ferent conditional distributions p(x|c) which is responsible for
the mixing upon application of the control. As P/N → ∞,
the ‘area’ covered by the overlapping regions decreases, and so
is I(X ′; C). Based on this observation, we have attempted to
change the control law slightly so as to minimize the mixing in
the control while keeping the overlap constant and found that
complete optimality for the Gaussian channel controller can be
achieved if the control law is modiﬁed to

with a gain parameter γ set to

X ′ = X − γC,

γ =

P
P + N

.

The veriﬁcation of optimality for this controller is left to the
reader.

B. Noisy control of chaotic maps

The second application is aimed at illustrating the closed-loop
optimality theorem in the context of a controller restricted to
use entropy-increasing actuation dynamics, as is often the case
in the control of chaotic systems. To this end, we consider the
feedback control scheme proposed by Ott, Grebogi and Yorke
(OGY) [44] as applied to the logistic map

xn+1 = f (rn, xn) = rnxn(1 − xn),

(82)

where xn ∈ [0, 1], and rn ∈ [0, 4], n = 0, 1, 2, . . .. In a nut-
shell, the OGY control method consists in setting the control
parameter rn at each time step n according to

∆Hclosed = log ε − log ∆

= log(ε/∆).

(76)

rn = r + δrn
δrn = −γ(cn − x∗)

(83)

These results, combined with the fact that ∆H max
open = 0 , prove
that the coarse-grained controller is always optimal, at least pro-
vided again that ε is a multiple of ∆.

In the case of the Gaussian channel, the situation for optimal-
ity is different. Under the application of the estimation law (74),
the ﬁnal state of the controlled system is

X ′ = X − C = X − (X + Z) = −Z,

(77)

so that X ′ ∼ Z. This means that if we start with X ∼ N (0, P ),
then

∆Hclosed =

log(2πeP ) −

log(2πeN )

1
2
1
2

=

log

P
N

,

1
2

(78)

whenever the estimated state cn = ˆxn falls into a small con-
trol region D in the vicinity of a target point x∗. This target
state is usually taken to be an unstable ﬁxed point satisfying the
equation f (r, x∗) = x∗, where f (r, x∗) is the unperturbed map
having rn = r as a constant control parameter. Moreover, the
gain γ is ﬁxed so as to ensure that the trajectory {xn}∞
n=0 is sta-
ble under the control action. (See [45], [46] for a derivation of
the stability conditions for γ based on linear analysis, and [47],
[48] for a review of the ﬁeld of chaotic control.)

Figure 4 illustrates the effect of OGY controller when ap-
plied to the logistic map. The plot of Figure 4a shows a typical
chaotic trajectory obtained by iterating the dynamical equation
(82) with rn = r = 3.7825. Note on this plot the presence
of non-recurring oscillations around the unstable ﬁxed point
x∗(r) = (r − 1)/r ≃ 0.7355. Figure 4b shows the orbit of

14

(cid:9)(cid:16)(cid:11)

(cid:4)

(cid:0)

(cid:5)

(cid:1)

(cid:2)(cid:3)

(cid:9)

(cid:11)

(cid:17)

(cid:4)

(cid:0)

(cid:5)

(cid:1)

(cid:2)(cid:3)

(cid:1)

(cid:1)

(cid:9)

(cid:11)

(cid:18)

(cid:8)(cid:9)(cid:10)

(cid:11)

(cid:5)

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

system has been controlled down to a given residual en-
tropy which speciﬁes the size of the basin of control, i.e.,
the average distance from x∗ to which xn has been con-
trolled.

It is the size of the basin of control, and, more precisely, its
dependence on the amount of information provided by the sen-
sor channel which is of interest to us here. In order to study this
dependence, we have simulated the OGY controller, and have
compared the value of the residual entropy H(Xn) for two types
of sensor channel: the coarse-grained channel Cn = C∆(Xn),
and the Gaussian channel Cn = CZ (Xn).

In the case of the coarse-grained channel, we have found that
the distribution of Xn in the controlled regime was well approx-
imated by a uniform distribution of width ε centered around the
target point x∗. Thus, the indicator value for the size of the basin
of control is taken to correspond to

(85)

(86)

ε = eH(Xn),

ε ≥ eλ∗

εm,

which, according to the closed-loop optimality theorem, must
be such that

where λ∗ is the Lyapunov exponent associated with the r value
of the unperturbed logistic map, and where εm is the coarse-
grained measurement interval or precision of the sensor chan-
nel. (All logarithms are in natural base in this section.) To un-
derstand the above inequality, note that a uniform distribution
for Xn covering an interval of size δ must stretch by a factor
eλ(r) after one iteration of the map with parameter r. This fol-
lows from the fact that λ(r) corresponds to an entropy rate of
the dynamical system [49]-[54] (see also [37]-[40]), and holds
in an average sense inasmuch as the support of Xn is not too
small or does not cover the entire unit interval. Now, for open-
loop control, it can be seen that if λ(r) > 0 for all admissible
control values r, then no control of the state Xn is possible, and
the optimal control strategy must consist in using the smallest
Lyapunov exponent λmin available in order to achieve

∆H max

open = H(Xn) − H(Xn+1)open

= ln δ − ln eλminδ
= −λmin < 0.

(87)

In the course of the simulations, we noticed that only a very
narrow range of r values were actually used in the controlled
regime, which means that ∆H max
open can be taken for all purposes
to be equal to −λ∗. At this point, then, we need only to use
expression (75) for the mutual information of the coarse-grained
channel, substituting ∆ with εm, to obtain

∆Hclosed ≤ −λ∗ + ln(ε/εm).

(88)

This expression yields the aforementioned inequality by posing
∆Hclosed = 0 (controlled regime).

The plots of Figure 5 present our numerical calculations of
ε as a function of εm. Each of these plots has been obtained
by calculating Eq.(85) using the entropy of the normalized his-
togram of the positions of about 104 different controlled trajec-
tories. Other details about the simulations may be found in the

(cid:12)(cid:13)(cid:13)(cid:14)

(cid:12)(cid:13)(cid:13)(cid:14)

(cid:12)(cid:13)(cid:13)(cid:13)(cid:14)

(cid:12)(cid:13)(cid:14)

(cid:12)(cid:13)(cid:14)

(cid:12)(cid:13)(cid:15)(cid:14)

(cid:1)

(cid:3)

(cid:1)

(cid:4)(cid:1)(cid:1)

(cid:4)

(cid:1)

(cid:3)

(cid:6)(cid:1)(cid:1)

(cid:7)

Fig. 4. (a) Typical uncontrolled trajectory of the logistic map with r = 3.7825.
(b) Controlled trajectory which results from applying the OGY feedback
control at time n = 50. Note the instant resurgence of instability as the
control is switched off at n = 150. The gain for this simulation was set
to γ = −7.0, and D = [0.725, 0.745]. (c) Entropy H(Xn) (in arbitrary
units) associated with the position of the controlled system versus time (see
text).

the same initial point x0 now stabilized by the OGY controller
around x∗ for n ∈ [50, 150]. For this latter simulation, and more
generally for any initial points in the unit interval, the controller
is able to stabilize the state of the logistic map in some region
surrounding x∗, provided that γ is a stable gain, and that the
sensor channel is not too noisy. To evidence the stability proper-
ties of the controller, we have calculated the entropy H(Xn) by
constructing a normalized histogram pXn (xn) of the positions
of a large ensemble of trajectories (∼ 104) starting at different
initial points. The result of this numerical computation is shown
in Figure 4c. On this graph, one can clearly distinguish four dif-
ferent regimes in the evolution of H(Xn), numbered from (i) to
(iv), which mark four different regimes of dynamics:

• Chaotic motion with constant r (i). Exponential divergence
of nearby trajectories initially located in a very small re-
gion of the state space. The slope of the linear growth of
entropy, the signature of chaos, is probed by the value of
the Lyapunov exponent

λ(r) = lim
N→∞

1
N

N −1

n=0
X

ln

∂f (r, x)
∂x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

xn (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

;

(84)

• Saturation (ii). At this point, the distribution of positions
pXn (xn) for the chaotic system has reached a limiting or
equilibrium distribution which nearly ﬁlls all the unit inter-
val;

• Transient stabilization (iii). When the controller is acti-
vated, the set of trajectories used in the calculation of
H(Xn) is compressed around x∗ exponentially rapidly in
time;

• Controlled regime (iv). An equilibrium situation is reached
whereby H(Xn) stays nearly constant. In this regime, the

H. Touchette and S. Lloyd

15

(cid:29)

(cid:30)

(cid:31)

 

(cid:28)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:23)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:27)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:26)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:22)

(cid:19)

(cid:19)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:19)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:23)

(cid:19)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:19)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:23)

(cid:19)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:19)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:23)

(cid:19)

(cid:19)(cid:20)(cid:19)(cid:19)(cid:19)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22) (cid:19)(cid:20)(cid:19)(cid:19)(cid:22)(cid:21) (cid:19)(cid:20)(cid:19)(cid:19)(cid:23)

(cid:24)

(cid:24)

(cid:24)

(cid:24)

(cid:25)

(cid:25)

(cid:25)

(cid:25)

Fig. 5. (Data points) Control interval ε as a function of the effective coarse-grained interval of measurement εm for four different target points. (Solid line) Optimal
linear relationship predicted by the closed-loop optimality theorem. The values of r and the Lyapunov exponents λ∗ associated with the target points are listed
in Table 1 and displayed in Figure 6.

TABLE I. Characteristics of the four target points.

Target point
1
2
3
4

x∗
0.7218
0.7284
0.7356
0.7455

r
3.5950
3.6825
3.7825
3.9290

λ∗ (base e)
0.1745
0.3461
0.4088
0.5488

’

(

&’

(cid:2)(cid:1)(cid:1)(cid:2)

)*+

)

+

,

/

/

.

.

0

0

-

-

caption. What differentiates the four plots is the ﬁxed point to
which the ensemble of trajectories have been stabilized, and, ac-
cordingly, the value of the Lyapunov exponent λ∗ associated
to x∗(r). These are listed in Table 1 and illustrated in Fig-
ure 6. One can verify on the plots of Figure 5 that the points
of ε versus εm all lie above the critical line (solid line in the
graphs) which corresponds to the optimality prediction of in-
equality (86). Also, the relatively small departure of the numer-
ical data from the optimal prediction shows that the OGY con-
troller with the coarse-grained channel is nearly optimal with
respect to the entropy criterion. This may be explained by notic-
ing that this sort of controller complies with all the requirements
of the ﬁrst class of linear proportional controllers studied previ-
ously. Hence, we expect it to be optimal for all precision εm,
although the fact must be considered that ∆H max
open = −λ∗ is
only an approximation. In reality, not all points are controlled
with the same parameter r for a given value of εm, as shown
in Figure 6. Moreover, how ε is calculated explicitly relies on
the assumption that the distribution for Xn is uniform. This
assumption has been veriﬁed numerically; yet, it must also be
regarded as an approximation. Taken together, these two ap-
proximations may explain the observed deviations of ε from its
optimal value.

For what concerns the Gaussian channel, the situation of opti-
mality is also very related to our results about proportional con-
trollers. The results of our simulations, for this type of channel,
indicated that the normalized histogram of the controlled posi-
tions for Xn is very close to a normal distribution with mean x∗
and variance P . As a consequence, we now consider the vari-
ance P , which for Gaussian random variables is given by

P =

e2H(Xn)
2πe

,

(89)

!"#

!"$

!"%

#

!"#

!"$

!"%

#

(cid:1)

(cid:1)

Fig. 6.

(a) Lyapunov spectrum (r, λ(r)) of the logistic map. The positive
Lyapunov exponents associated with the four target points listed in Table
1 are located by the circles. The set of r values used during the control
spans approximately the diameter of the circles. Note that the few negative
values of λ(r) close to the λ∗’s are effectively suppressed by the noise in the
sensor channel. This is evidenced by the graph of (b) which was obtained
by computing the sum (84) up to N = 2 × 104 with an additive noise
component of very small amplitude. See [55], [56] for more details on this
point.

as the correlate of the size of the basin of control. For this
quantity, the closed-loop optimality theorem with ∆Hclosed = 0
yields

P ≥ (e2λ∗

− 1)N,

(90)

where N is the variance of the zero-mean Gaussian noise per-
turbing the sensor channel.

In Figure 7, we have displayed our numerical data for P as
a function of the noise power N . The solid line gives the opti-
mal relationship which results from taking equality in the above
expression, and from substituting the Lyapunov exponent asso-
ciated with one of the four stabilized points listed in Table 1.
From the plots of this ﬁgure, we verify again that P is lower
bounded by the optimal value predicted analytically. However,
now it can be seen that P deviates signiﬁcantly from its optimal
value, making clear that the OGY controller driven by the Gaus-
sian noisy sensor channel is not optimal (except in the trivial
limit where N → 0). This is in agreement with our proof that
linear proportional controllers with Gaussian sensor channel are
not optimal in general. On the plots of Fig. 7, it is quite remark-
able to see that the data points all converge to straight lines. This

;

>

<

=

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

16

:

41

9

8

7

5

1

1

123

4

423

5

523

1

123

4

423

5

523

1

123

4

423

5

523

1

123

4

423

5

523

6

6

6

6

Fig. 7.

(Data points) Dispersion P characterizing the basin of attraction of the controlled system as a function of the noise power N introduced in the Gaussian

sensor channel. The horizontal and vertical axes are to be rescaled by a factor 10−5. (Solid line) Optimal lower bound.

suggests that the mixing induced by the controller, the source of
non-optimality, can be accounted for simply by modifying our
inequality for P so as to obtain

and units of temperature (Kelvin); the constant ln 2 arises be-
cause entropy, in physics, is customary deﬁned in base e. From
the closed-loop optimality theorem, we then write

P = (e2λ′

− 1)N.

(91)

The new exponent λ′ can be interpreted as an effective Lya-
punov exponent; its value is necessarily greater than λ∗, since
the chaoticity properties of the controlled system are enhanced
by the mixing effect of the controller.

VII. CONCLUDING REMARKS

A. Control and thermodynamics

The reader familiar with thermodynamics may have noted
some similarity between the functioning of a controllers, when
viewed as a device aimed at reducing the entropy of a system,
and the so-called Maxwell demon [26]. Such a similarity is not
fortuitous: as he wondered about the validity of the second law
of thermodynamics, the physicist James Clerk Maxwell imag-
ined the ﬁrst example of a system whose task, in effect, is to
reduce the entropy of another system by putting to use informa-
tion, and, for this reason, has been the original impetus for this
work. In the case of Maxwell’s demon, the system to be con-
trolled or ‘cooled’ is a volume of gas; the entropy to be reduced
is the equilibrium thermodynamic entropy of the gas; and the
pieces of information gathered by the controller (the demon) are
the velocities of the atoms or molecules constituting the gas.

When applied to this scheme, our result on closed-loop opti-
mality can be translated into an absolute limit to the ability of
the demon, or any control devices, to convert heat to work. In-
deed, consider a feedback controller operating in a cyclic fash-
ion on a system in contact with a heat reservoir at tempera-
ture T . According to Clausius law of thermodynamic [57], the
amount of heat ∆Qclosed extracted by the controller upon re-
ducing the entropy of the controlled system by a concomitant
amount ∆Hclosed must be such that

∆Qclosed = (kBT ln 2)∆Hclosed.

(92)

In the above equation, kB is the Boltzmann constant which pro-
vides the necessary conversion between units of energy (Joule)

∆Qclosed ≤ (kBT ln 2)[∆H max
= ∆Qmax

open + I(X; C)]
open + (kBT ln 2)I(X; C),

(93)

open = (kBT ln 2)∆H max

where ∆Qmax
open . This limit should be com-
pared with analogous results found by other authors on the sub-
ject of thermodynamic demons. Consult, as for example, some
of the articles reprinted in [26], and especially Szilard’s analysis
of Maxwell’s demon. This classic paper, originally published in
[58], contains many premonitory insights about the use of infor-
mation in control.

It should be remarked that the connection between the prob-
lem of Maxwell’s demon, thermodynamics, and control is effec-
tive only to the extent that Clausius law provides a link between
entropy and the physically measurable quantity that is energy.
But, of course, the notion of entropy is a more general notion
than what is implied by Clausius law; it can be deﬁned in rela-
tion to several situations which have no direct relationship what-
soever with physics (e.g., coding or decision theory). This ver-
satility of entropy is implicit here. Our results do not rely on
thermodynamic principles, or even physical principles for that
matter, to be true. They constitute valid results derived in the
context of a general model of control processes whose precise
nature is yet to be speciﬁed.

B. Entropy and optimal control theory

Consideration of entropy as a measure of dispersion and un-
certainty led us to choose this quantity as a control function
of interest, but other information-theoretic quantities may well
have been chosen instead if different control applications require
so. From the point of view of optimal control theory, all that is
required is to minimize a desired performance criterion (a cost
or a Lyapunov function), such as the distance to a target point
or the energy consumption, while achieving some desired dy-
namic performance (stability) using a set of permissible controls
[34], [17]. For example, one may be interested to maximize
∆Hclosed instead of minimizing this quantity if destabilization
(anti-control) or mixing is an issue [59]. As other examples,

H. Touchette and S. Lloyd

17

let us mention the minimization of the relative entropy distance
between the distribution of the state of a controlled system and
some target distribution [36], the problem of coding [60], as well
as the minimization of rate-like functions in decision or game
theory [61]-[63], [30].

C. Control and rate distortion theory

The conceptual closeness of optimal control theory and the
theory of rate distortion [25], [30] can serve as another basis for
an information-theoretic formulation of control. This possibil-
ity has not been considered explicitly here, but should surely be
investigated in more details. It consists, speciﬁcally, in consid-
ering a ﬁdelity criterion, say a real-valued function d(X, X ′) of
the initial and ﬁnal states X and X ′, and to seek for the least
amount of information R(D) needed for a controller to achieve
some upper bound D on d(X, X ′) using a ﬁxed set of actua-
tion dynamics. The function R(D) so deﬁned is known as the
rate distortion function. A distortion rate function may also be
deﬁned if what is sought is the maximum D(R) of the perfor-
mance function which can be attained under a communication
constraint I(X; C) ≤ R. (See [21], [64] for an analogous ap-
proach to sensor ﬁlters and linear controllers.)

Evidently, different deﬁnitions of these quantities may be
given along these lines if the performance criterion is to be max-
imized instead of being minimized. From a formal point of view,
minimizing a criterion functional simply amounts to maximiz-
ing the same functional with a minus sign. Thus, in this case,
the functions R(D) and D(R) should properly be re-deﬁned as
follows:

R(D) =

min
p(c|x):d(X,X ′)≥D

I(X; C)

D(R) =

max
p(c|x):I(X;C)≤R

d(X, X ′).

R(D)

attainable

Fig. 8. Rate distortion function R(D) for the general control systems studied

in this paper.

max

D

¢Hopen

what we have studied can be compared with the memoryless
channel of information theory; what is needed in the future is
something like a control analog of network information theory.
Work is ongoing along this direction.

ACKNOWLEDGMENTS

H.T. would like to thank P. Dumais for correcting a pre-
liminary version of the manuscript, S. Patagonia for inspiring
thoughts, and especially V. Poulin for her always critical com-
ments. Many thanks are also due to A.-M. Tremblay for the
permission to access the supercomputing facilities of the CER-
PEMA at the Universit´e de Sherbrooke.

REFERENCES

[1] H. Touchette, S. Lloyd, “Information-theoretic limits of control,” Phys.

[2]

Rev. Lett., vol. 84, pp. 1156-1159, 2000.
J.J. D’Azzo, C.H. Houpis, Linear Control Systems Analysis and Design,
3rd ed., New York: McGraw-Hill, 1988.

[3] M.G. Singh (ed.), Systems and Control Encyclopedia . Oxford: Pergamon

(94)

[4] D.M. Mackay, Information, Mechanism, and Meaning . Cambridge, MA:

Press, 1987.

MIT Press, 1969.

What we have shown in Section V is that, if the performance
criterion for control is taken to correspond to the closed-loop
entropy reduction, i.e., d(X, X ′) = H(X) − H(X ′)closed, then

R(D) = max(0, ∆H max

open − D)

and

D(R) = ∆H max

open − R.

(95)

(96)

These two equations are illustrated in Figure 8. Note that
∆H max
open is a constant of the problem, since what is varied above
is the sensor channel. Similar relations hold if R(D) and D(R)
are deﬁned by requiring that the sensor channel is ﬁxed and that
an optimal design for the controller is to be found by selecting
an appropriate actuation channel.

D. Beyond Markovian models

Many questions pertaining to issues of information and con-
trol remain at present unanswered. We have considered in
this paper only but the ﬁrst level of investigation of a much
broader and deﬁnitive program of research aimed at providing
information-theoretic tools for the study of general control sys-
tems, such as those involving many interacting components, as
well as controllers exploiting non-Markovian features of dy-
In a sense,
namics (e.g., memory, learning, and adaptation).

[5] N. Wiener, Cybernetics: or Control and Communication in the Animal and

the Machine. Cambridge, MA: MIT Press, 1948.

[6] E. Scano, “Mod`ele de m´ecanisme informationnel,” Cybernetica, vol. VIII,

pp. 188-223, 1965.

[7] W.R. Ashby, An Introduction to Cybernetics. New York: Wiley, 1956.
[8] W.R. Ashby, “Measuring the internal information exchange in a system,”
Cybernetica, vol. VIII, pp. 5-22, 1965. Reprinted in R. Conant (ed.), Mech-
anisms of Intelligence: Ross Ashby’s Writings on Cybernetics. Seaside,
CA: Intersystems Publications, 1981.

[9] D. Sankoff, “Entropy and the control of stochastic processes,” M.Sc. The-

sis, McGill University, 1965.

[10] M. Belis, S. Guiasu, “Quantitative-qualitative measure of information in
cybernetic systems,” IEEE Trans. Inform. Theory, vol. 14, pp. 593-594,
1968.

[11] R.C. Conant, “Laws of information which govern systems,” IEEE Trans.

Syst. Man, and Cybern., vol. 6, pp. 240-255, 1976.

[12] R. Poplavski˘ı, “Thermodynamic models of information processes,” Sov.
Phys. Usp., vol. 18, pp. 222-241, 1975. Original Russian version in Usp.
Fiz. Nauk, vol. 115, pp. 465-501, 1975.

[13] R. Poplavski˘ı, “Information and entropy,” Sov. Phys. Usp., vol. 22, pp.
371-380, 1979. Original Russian version in Usp. Fiz. Nauk, vol. 128, pp.
165-176, 1979.

[14] A.M. Weinberg, “On the relation between information and energy systems:
a family of Maxwell’s demons,” Interdisciplinary Sci. Rev., vol. 7, pp. 47-
52, 1982.

[15] K.P. Valavanis, G.N. Saridis, “Information-theoretic modeling of intelli-
gent robotic systems,” IEEE Trans. Syst. Man, and Cybern., vol. 18, pp.
852-872, 1988.

[16] G.N. Saridis, “Entropy formulation of optimal and adaptive control,” IEEE

Trans. Automat. Contr., vol. 33, pp. 713-721, 1988.

[17] G.N. Saridis, Stochastic Processes, Estimation, and Control: the Entropy

Approach, John Wiley, 1995.

[18] J.C. Musto, G.N. Saridis, “Entropy-based reliability analysis for intelligent

18

Submitted to IEEE TRANSACTIONS ON INFORMATION THEORY. Version of November 29, 2012

[53] L.-S. Young, “Entropy, Lyapunov exponents, and Hausdorff dimension in
differentiable dynamical systems,” IEEE Circ. and Syst., vol. 30, pp. 599-
607, 1983.

[54] V. Latora, M. Baranger, “Kolmogorov-Sinai entropy rate versus physical

entropy,” Phys. Rev. Lett., vol. 82, pp. 520-523, 1999.

[55] J.P. Crutchﬁeld, J.D. Farmer, B.A. Huberman, “Fluctuations and simple

chaotic dynamics,” Phys. Report, vol. 92, pp. 54-82, 1982.

[56] J.P. Crutchﬁeld, N.H. Packard, “Symbolic dynamics of one-dimensional
maps: entropies, ﬁnite precision, and noise,” Int. J. Theor. Phys., vol. 21,
pp. 433-466, 1982.

[57] R. Reif, Statistical and Thermal Physics, New York: McGraw-Hill, 1965.
[58] L. Szilard, “On the decrease of entropy in a thermodynamic system by the
intervention of intelligent beings,” Behavioral Science, vol. 9, pp. 301-310,
1964. Original version in Z. f. Physik, vol. 53, pp. 840-856, 1929.

[59] D. D’Alessandro, M. Dahleh, I. Mezi´c, “Control of mixing in a ﬂow: a
maximum entropy approach,” IEEE Trans. Automat. Contr., vol. 44, pp.
1852-1863, 1999.

[60] R. Ahlswede, N. Cai, “Information and control: matching channels,” IEEE

Trans. Inform. Theory, vol. 44, pp. 542-563, 1998.

[61] W. Jianhua, The Theory of Games. Oxford: Clarendon Press, 1988.
[62] J. Kelly, “A new interpretation of information rate,” Bell Sys. Tech. J., vol.

35, pp. 917-926, 1956.

[63] D. Middleton, An Introduction to Statistical Communication Theory, New

York: IEEE Press, 1996. See Sections 23.3-23.5.

[64] H.L. Weidemann, “Entropy analysis of parameter estimation,” Inform. and

Contr., vol. 14, pp. 493-506, 1969.

Hugo Touchette received his Bachelor degree (B.Sc.
Physics) from the Universit´e de Sherbrooke, Sher-
brooke, Canada in 1997. In the beginning of 2000, he
received a M.Sc. degree from the Department of Me-
chanical Engineering at MIT, where he was a mem-
ber of the d’Arbeloff Laboratory for Information Sys-
tems and Technology. After graduating from MIT, he
joined the Department of Physics at McGill Univer-
sity, Montr´eal, Canada, where he is currently pursuing
his Ph.D. studies as an NSERC graduate fellow on the
subject of non-conventional measures of entropy. He
is also involved with the Cryptography and Quantum Information Laboratory of
the School of Computer Science at McGill University.

Seth Lloyd was educated at Harvard University (B.Sc.
Physics, 1982), and at Cambridge University, where
he received in 1984 a Master in Philosophy of Science.
In 1988, he completed his Ph.D. studies in Physics at
the Rockefeller University, New York. After work-
ing as a postdoctoral fellow at Caltech and the Los
Alamos National Laboratory, S Lloyd joined in 1994
the Department of Mechanical Engineering at MIT
where he is currently appointed Associate Professor.
In this department, he is working with the d’Arbeloff
Laboratory for Information Systems and Technology.
He is also an associate member of the Laboratory for Information and Decision
Systems at MIT, and serves as an external faculty for the Santa Fe Institute in
New Mexico. His research interests focus primarily on the theory of quantum
information processing, and on the experimental realization of quantum comput-
ers and quantum communication systems. In addition to these topics, S. Lloyd
works on the theory of control systems, both classical and quantum, on the the-
ory of complex systems, and on the physical limitations of computation.

machines,” IEEE Trans. Syst. Man, and Cybern. , vol. 27, pp. 239-244,
1997.

[19] D.F. Delchamps, “Controlling the ﬂow of information in feedback systems
with measurement quantization,” Proc. 28th Conf. Decision and Control,
1989.

[20] I. Pandelidis, “Generalized entropy and minimum system complexity,”
IEEE Trans. Syst. Man, and Cybern., vol. 20, pp. 1234-1238, 1990.
[21] H.L. Weidemann, “Entropy analysis of feedback control systems,” in C.
Leonodes (ed.), Advances in Control Systems, vol. 7. New York: Academic
Press, 1969.

[22] S. Lloyd, J.-J.E. Slotine, “Information theoretic tools for stable adaptation
and learning,” Int. J. Adapt. Contr. Sig. Proc., vol. 10, pp. 499-530, 1996.
[23] V.S. Borkar, S.K. Mitter, “LQG control with communication constraints,”
in A. Paulraj, V. Roychowdhury, C.D. Schaper (eds.), Communications,
Computation, Control and Signal Processing. Boston: Kluwer, 1997.
[24] S. Tatikonda, A. Sahai, S. Mitter, “Control of LQG systems under com-

munication constraints,” Proc. Amer. Contr. Conf., 1999.

[25] C.E. Shannon, “A mathematical theory of communication,” Bell Sys. Tech.
J., vol. 27, pp. 379-423, 623-656, 1948. Reprinted in C.E. Shannon, W.
Weaver, The Mathematical Theory of Communication. Urbana, Ill.: Uni-
versity of Illinois Press, 1963.

[26] H.S. Leff, A.F. Rex (eds.), Maxwell’s Demon, Entropy, Information, Com-

puting. New Jersey: Princeton University Press, 1990.

[27] J. Pearl, Probabilistic Reasoning in Intelligent Systems. San Mateo: Mor-

[28] M.I. Jordan (ed.), Learning in Graphical Models. Cambridge, MA: MIT

gan Kaufmann, 1988.

Press, 1999.

[29] R.B. Ash, Information Theory. New York: Interscience Publishers, 1965;

reprinted by New York: Dover, 1990.

[30] T.M. Cover, J.A. Thomas, Elements of Information Theory. New York:

Wiley, 1991.

[31] Y.C. Ho, R.C.K. Lee, “A Bayesian approach to problems in stochastic es-
timation and control,” IEEE Trans. Automat. Contr., vol. 9, pp. 333-339,
1964.

[32] S. Roman, Coding and Information Theory. New York: Springer, 1992.
[33] N.J. Cerf, R. Cleve, “Information-theoretic interpretation of quantum
error-correcting codes,” Phys. Rev. A, vol. 56, pp. 1721-1732, 1997.
[34] R.F. Stengel, Optimal Control and Estimation. New York: Dover, 1994.
[35] F. Schl¨ogl, “Stochastic measures in nonequilibrium thermodynamics,”

Phys. Report, vol.62, pp.267-380, 1980.

[36] A. Beghi, “On the relative entropy of discrete-time Markov processes with
given end-point densities,” IEEE Inform. Theory , vol. 42, pp.1529-1535,
1996.

[37] R. Shaw, “Strange attractors, chaotic behavior, and information ﬂow,” Z.

Naturforsch., vol. 36a, pp. 80-112, 1981.

[38] J.D. Farmer, “Information dimension and the probabilistic structure of

chaos,” Z. Naturforsch., vol. 37a, pp. 1304-1325, 1982.

[39] G. Nicolis, D. Daems, “Probabilistic and thermodynamic aspects of dy-

namical systems,” Chaos, vol. 8, pp. 311-320, 1998.

[40] C. Beck, F. Schl¨ogl, Thermodynamics of Chaotic Systems. Cambridge:

Cambridge Univ. Press, 1993.

[41] S. Lloyd, “Use of mutual information to decrease entropy: implications for
the second law of thermodynamics,” Phys. Rev. A, vol. 39, pp. 5378-5386
(1989).

[42] R. Schack, C.M. Caves, “Chaos for Liouville densities,” Phys. Rev. E, vol.

53, pp. 3387-3401, 1996.

[43] H. Touchette, “Information-theoretic aspects in the control of dynamical
systems,” M.Sc. Thesis, Dept. of Mechanical Engineering, MIT, 2000.
[44] E. Ott, C. Grebogi, J.A. Yorke, “Controlling chaos,” Phys. Rev. Lett., vol.

64, pp. 1196-1199, 1990.

[45] T. Shinbrot, C. Grebogi, E. Ott, J.A. Yorke, “Using small perturbations to

control chaos,” Nature, vol. 363, pp. 411- ,1993.

[46] H.G. Schuster, Deterministic Chaos, 3rd ed. Weinheim: VCH, 1995.
[47] T. Shinbrot, “Progress in the control of chaos,” Advances in Physics, vol.

44, pp. 73-111, 1995.

[48] S. Boccaletti, C. Grebogi, Y.-C. Lai, H. Mancini, D. Maza, “The control
of chaos: theory and applications,” Phys. Reports , vol. 329, pp. 103-197,
2000.

[49] A.N. Kolmogorov, “A new metric invariant for transitive dynamical sys-

tems,” Dokl. Akad. Nauk. SSSR, vol. 119, pp. 861-864, 1958.

[50] A.N. Kolmogorov, “Entropy per unit time as a metric invariant of auto-

morphisms,” Dokl. Akad. Nauk. SSSR, vol. 124, p. 754, 1959.

[51] Ya. G. Sinai, “On the notion of entropy of a dynamical system,” Dokl.

Akad. Nauk. SSSR, vol. 124, pp. 768-771, 1959.

[52] G. Gy¨orgyi, P. Sz´epfalusy, “Calculation of the entropy in chaotic systems,”

Phys. Rev. A, vol. 31, pp. 3477-3479, 1985.


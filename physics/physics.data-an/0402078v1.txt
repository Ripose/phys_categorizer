4
0
0
2
 
b
e
F
 
7
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
7
0
2
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Self-generated Self-similar Traﬃc

P´eter H´aga, P´eter Pollner, G´abor Simon, Istv´an Csabai, G´abor Vattay
Communication Networks Laboratory
E¨otv¨os Lor´and University,
H-1117 Budapest, P´azm´any P.s. 1/A.
E-mail: {haga, pollner, gaba, csabai, vattay}@complex.elte.hu

Self-similarity in the network traﬃc has been studied from several aspects: both at the user
side and at the network side there are many sources of the long range dependence. Recently some
dynamical origins are also identiﬁed: the TCP adaptive congestion avoidance algorithm itself can
produce chaotic and long range dependent throughput behavior, if the loss rate is very high. In
this paper we show that there is a close connection between the static and dynamic origins of self-
similarity: parallel TCPs can generate the self-similarity themselves, they can introduce heavily
ﬂuctuations into the background traﬃc and produce high eﬀective loss rate causing a long range
dependent TCP ﬂow, however, the dropped packet ratio is low.

INTRODUCTION

REAL AND EFFECTIVE LOSS

In a large number of internet traﬃc measurements
many authors detected self-similarity[2, 3, 7, 13]. Self-
similarity is usually attributed with heavy-tailed dis-
sizes
tributions of objects at the traﬃc sources, e.g.
in user
of
interactions[3].

transfered ﬁles[13, 15] or

time delays

Recently the dynamical origins of self-similarity also
attract increasing attention. Chaotic maps are known
to generate fractal properties[5], and it has been shown,
that the exponential backoﬀ algorithm of the TCP proto-
col can produce long range dependent traﬃc under some
special circumstances. Veres[18] emphasized the chaotic
nature of the transport, and Fekete[6] gave an analyt-
ical explanation that this chaoticity is in close relation
with the loss rate at highly utilized buﬀers due to packet
drops. Guo[9] gave a Markovian analysis of the backoﬀ
mechanism, and showed, that the self-similarity can be
observed only at very high loss rates.

In this paper we study a lossless network, where the
traﬃc sources are not heavy-tailed distributed. Even
though we observe self-similar traﬃc, which is generated
by the communicating TCPs themselves. We show, that
the random switching between the destinations of the
ﬂows together with the complex dynamics of interacting
TCPs can lead to long range dependency. The interac-
tion between TCPs at the buﬀers leads to backoﬀ phases
in the individual TCPs causing fractal traﬃc in an en-
vironment where the loss rate is much below the lower
bound of self-similarity.

The outline of the paper is the following: First we
introduce the concept of real and eﬀective loss. Then
we present a simple lossless model network, where self-
similar traﬃc is observed, however the necessary condi-
tions discusssed in the literature cited above, are not sat-
isﬁed. Next we show that similar scenario can be found
in real networks as well. Finally we conclude our results.

In the internet traﬃc many individuals use a common,
ﬁnite resource to transmit information. If the resources
are exhausted (e.g. routers are congested), data through-
put is not possible. Therefore data transmitters should
avoid congestion on shared information routes. Most
of today’s computer programs use similar algorithm to
avoid congestion: they apply basicly the same TCP pro-
tocol with slight diﬀerences. The common concept in ev-
ery TCP is, that the data sending rate must be adapted
to the actually available resources[10, 11]. Every TCP
starts with a blind measuring phase (slow start), which
exponentially reaches the maximum throughput rate. If
the route, where TCP sends its data is stationary utilized,
the algorithm works in a high throughput – slow adaption
phase (congestion avoidance). The sending rate is var-
ied around a high value by slowly increasing and rapidly
decreasing it. Since every sent packet and its received
acknowledgement continuously measure the transmission
possibilities, this phase is very stable and can adapt to
slowly varying situations[19].

If the route gets highly loaded, the TCP tries to clear
If the
the congestion by decreasing the sending rate.
congestion is so high, that the TCP cannot guess the
proper sending rate (acknowledgements do not arrive in
the timeout period), the algorithm enters a very slow
sending phase (exponential backoﬀ). In this phase due
to the lack of information an exponentially slowing al-
gorithm is applied to try to ﬁnd a new possible sending
rate: the last package is resent after exponential increas-
ing time intervals until an acknowledgement received or
a maximum time interval is reached.

In this paper we concentrate on the backoﬀ phase of
the TCP. We will show, that due to its blind nature, in
this phase the TCP can feel higher loss rates as it really
is.

By the blindness of the TCP we mean the consequence
of Karn’s algorithm[12], which governs the backoﬀ phase.
Under normal transmission conditions TCP operates in

slow start or in congestion avoidance mode.
In these
modes the TCP estimates the optimal sending rate from
the arrival time of the acknowledgements (ACK) by cal-
culating the average round trip time (SRTT) and its av-
erage deviation from the mean (MDEV). After each re-
ceived ACK the TCP estimates the retransmission time-
out (RTO). If this timeout is exceeded between sending
a packet and receiving an acknowledgement for it, the
TCP retransmits the packet assumed to be lost (by real
loss or timeout).
In this situation TCP applies the so
called Karn’s algorithm.

The Karn’s algorithm speciﬁes that the acknowledg-
ments for retransmitted data packets cannot be used to
approximate the sending rate. Since for a received ACK
packet one cannot decide if it is the ACK of the origi-
nal or of the retransmitted packet, the round trip time
(RTT) and so the sending rate cannot be estimated. The
RTT can be calculated only for those packets, which are
not retransmitted. So the TCP retransmits the packet
and doubles the RTO calculated from the previous ﬂow-
informations (backoﬀ phase). If the retransmitted packet
timeouts again, the RTO is doubled and the packet is
retransmitted again. The RTO is increased up to a max-
imal value deﬁned in the protocol. The TCP leaves the
backoﬀ phase only if the RTT can be estimated without
ambiguity: the TCP must receive the acknowledgements
of two consecutive sent packets. We will show a situation
where this method reports reasonably higher loss rate for
the TCP as it really is.

We distinguish the loss into real or virtual. Real loss is
referred to dropped packets which either are not arrived
to the destination or the acknowledgment for it do not
arrive to the sending TCP. We call a loss to be virtual
if the acknowledgment arrives after the retransmission
timeout (RTO) period, so the packet is retransmitted
due to a spurious timeout. The eﬃctive loss is assembled
from the real and virtual losses.

This distinction is important, since real loss emerges
at highly congested buﬀers or at low quality lines (e.g.
radio connections). These situations can be solved by
improving the hardware conditions.
In contrast, high
virtual loss can evolve also under very good hardware
conditions from heavily ﬂuctuating background traﬃc.
On a route with several routers, where the packets can
stay in a long queue, round trip times can change in a
wide range depending on the queuing time. The queu-
ing time depends on the saturation of the buﬀers on the
route. If the background traﬃc ﬁlls the buﬀers at a vary-
ing rate, the queueing time, and so the round trip time
varies also. Bursty background traﬃc can ﬁll the buﬀers
rapidly to a high value, and after that can leave it to be
cleared out.

If the round trip time increases to such a high value
due to a rapid ﬁlling up, that it becomes larger than the
retransmission timeout value, a virtual loss occurs. After
a burst which caused the virtual loss the clearing out of

2

the buﬀer will lead to a shorter round trip time, which
decreases the RTO value also. So for the next burst event
the RTO is not large enough that the TCP can receive
the ACK packet. So another virtual loss occurs without
really loosing the sent packets.

We will show in a model network and in a real measure-
ment, that long range dependent traﬃc can emerge from
the virtual losses due to the bursty background, however,
real packet loss rate is so low, that one would expect a
scalable simple traﬃc rate.

CASE STUDY: SIMULATION

In this section we present a simple model network,
which shows self-similar traﬃc. Our model diﬀers in sev-
eral aspects from previous studies in the literature ex-
cluding the known reasons of self-similarity.

In our model three hosts transfer ﬁxed sized ﬁles to
each other through a router. All hosts transfer ﬁles with
the same size. The topology of the model is shown in
Fig. 1. From each numbered sites of the network a TCP
ﬂow is initiated to one of the other numbered sites. Each
TCP ﬂow passes through the router R using full duplex
connections, so the ﬂow of the acknowledgements do not
interfere with the corresponding TCP data ﬂow. However
data from other TCPs must share the same buﬀers and
lines with acknowledgements.

2

R

1

3

FIG. 1: The topology of the network model. The numbered
circles represent hosts with TCP agents, and R denotes the
router.

We have chosen the network to be practically lossless:
the buﬀer length in the router was set so large, that it
is very improbable that TCP ﬂows ﬁll them. All the six
buﬀers for the three full duplex lines are large enough
to store all the ﬁles transfered between the hosts at a
moment. There is no external packet loss on the lines as
well.

We will study the traﬃc properties on a line connecting
a chosen host with the router. So the packet ﬂows we

want to analyze are initiated from a ﬁxed chosen host
and they are built up from several successive TCP ﬂows.
In this topology the traﬃc is not always self-similar.
The throughput of packets on a line can be regular if the
destination of the individual TCP ﬂows is chosen on a
regular way. An example is shown in Fig. 2a, where the
TCP ﬂows has been generated with the following sim-
ple rule: from the host numbered by i (i = 1 . . . 3) the
TCP sends packets to host (i mod 3) + 1. After a ﬁle
has been transmitted, the host starts a new TCP ﬂow
immediately, there is no external random delay between
the ﬂows as it would be if we took the user behavior
into account[4, 16]. Under such regular sending rules the
TCPs can utilize the available bandwidth and the traﬃc
has a scalable periodic pattern. In Fig. 2a we show the
congestion window for a host.

 1000

 100

 10

 1

 100

 10

d
n
w
c

d
n
w
c

random

periodic

 1
 1100

 1150

 1250

 1300

 1200
time (sec)

FIG. 2: The congestion window for one of the hosts. a) regular
case b) random case

We have implemented this simple network in the ns-2
simulator (version number: 2. 1b7a)[14]. The link pa-
link rate 1Mbps, delay 1ms. File size of
rameters are:
a TCP ﬂow was 1000 packet. The receiver window was
much larger than the ﬁle sizes. We have used the Reno
version of TCP.

If we introduce stochasticity in the sending rules we
arrive at a non-scalable, long range dependent traﬃc.
We applied the following rules to generate self-similar
packet transport. All hosts send ﬁxed size ﬁles to each
other. Each host uses the same ﬁle size. If a host starts a
new TCP, it randomly chooses to which host to send the
ﬁle. After a transmission is completed, the host chooses
the next destination immediately. The next destination
is chosen randomly again without silent period between
In Fig. 2b we show, that the
consecutive TCP ﬂows.
stochasticity inhibits the TCPs to synchronize and the
packet transport becomes irregular. The size of the trans-
fered ﬁles was chosen not too large to hinder the TCPs
to adapt to each other.

We investigate now the irregular packet traﬃc if
it shows self-similarity or not. Self-similarity can be
tested by investigating the second order statistics of the

3

m P

traﬃc[13]. Consider a weakly stationary process X,
with constant mean and autocorrelation function c(k).
km
Let X m(k) = 1
i=(k−1)m+1 X(i) denote the m ag-
gregated series of X. The process X is self-similar if
X ≈ m1−H X m, and X is second order self-similar if
m1−H X m has the same variance and autocorrelation as
X. The sign ≈ expresses the fact that the equality can
be satisﬁed only in a stochastic sense, exact equation can
only be used for abstract mathematical objects.

We have performed self-similarity test by using the
In Fig. 3 we plot the variance
variance time method.
of the aggregated time series of the packets which scales
as

V ar(δX) = (X m(k) − hX mik)2 ∼ m2H .

The ﬁtted line in the ﬁgure indicates Hurst exponent
H = 0.89 showing that the time series is self-similar since
H > 0.5. We emphasize again, that the time series under
consideration is built up from several consecutive TCP
ﬂows.

t
 
r
a
V
g
o
l

 

20

15

10

5

0

-5

-8

-6

-4

-2

0

2

4

6

log t

FIG. 3: Variance time plot of the aggregated traﬃc on a host-
router link. The ﬁtted line indicates H = 0.89.

If a traﬃc is self-similar it shows properties which dif-
fers from ones of memory-less (Markovian) processes: the
dynamics of the system shows a strong history depen-
dence. This long range dependence in the time evolution
manifests itself typically in heavy-tailed distributions. A
distribution is heavy-tailed if asymptotic decay of the dis-
tribution is slow: a power-law with exponent less than
two. Due to the always existing upper bounds of the
measured data it is enough if the decay holds in the last
decades below the maximum value:

P (S > s) ∼ s

−(1+α), as s ∈ [10

−nsmax, smax], and n > 2.5 .

Such distributions are called heavy-tailed, since occur-
rence of values much larger than the mean value of the
observable s is not negligible in contrast to commonly
used distributions as Gaussian or exponential. However
in measured time series it can happen, that from the
tail we cannot detect so many events as it is needed
In these cases
to plot a smooth distribution function.

it is favorably to work with the cumulative distribution
s P (s′)ds′, which has an asymptotic behavior
Pc(s) = R
as Pc(s) ∼ 1 − s−α. Therefore one should use the inverse
cumulative function 1 − Pc(s) to ﬁt the parameter α on
the logarithmic plot.

Now we want to investigate if the long range depen-
dency shows up in the traﬃc. We consider only the case
when a destinations of the TCPs were chosen randomly.
In Fig. 4 we plot the inverse cumulative distribution of
the packet inter arrival time on a down link. The distri-
bution shows a slow decay with α = 1.18 which indicates
that the ﬂuctuating traﬃc has long silent periods. A
power law decaying ﬂuctuation can be characterized by
the Hurst exponent if the traﬃc is viewed as an ON-OFF
process. The silent periods are the OFF intervals of the
process. The Hurst parameter is known for this type of
process[9]: H = 3−α
2 , which gives similar result as calcu-
lated from the variance time plot in Fig. 3.

)
.

.

b
o
r
p
m
u
c
.

v
n
i
(
 

g
o
l

0

-2

-4

-6

-8

-10

-12

-14

4

cessive packet losses. Let’s denote the packet retransmis-
sion probability (eﬀective loss) with p. The probability
of b consecutive packet retransmission is pb. Hence the
probability of a silent period t due to backoﬀs, decays as
plog2(t/TRT T ) ∼ t1−α, where α = log2(1/2p).

Next we repeat the main idea of a Markovian chain
model for backoﬀ states[6, 9] and show, that the statis-
tics of backoﬀ states delivers the average eﬀective loss
probability p.

Let denote the probability Pb that the TCP is in a
In a simpliﬁed Markovian framework
b deep backoﬀ.
one can estimate the Pb by the transition probabilities
between backoﬀ states as follows (for a detailed ma-
trix representation see [8]. The RTO value is doubled
if one of two successive packets do not receive ACK
and is retransmitted.
If the retransmission probabil-
ity is p the transition probability to a deeper backoﬀ is
1−(1−p)2 = 2p−p2. This yields a backoﬀ probability de-
cay to be Pb ∼ (2p−p2)b and one can read oﬀ the average
loss probability from the gradient of the semilogarithmic
plot of Pb versus b. We emphasize here, that the loss
probability measured by the probability of backoﬀ states
is the eﬀective loss felt by the TCP. This probability can
be much larger as the real loss. This is the case in our
model, since the real loss is below 0.1%, however, the ef-
fective loss is about 21%. A typical backoﬀ distribution
for our stochastic model is shown in Fig. 5.

b
_
P
g
o
l

 

-3

-4

-5

-6

-7

-8

-9

-10

-8

-6

-2

0

2

-4
log dt

FIG. 4: The distribution of the packet inter arrival times.
The ﬁtted line indicates H = 0.91.

In the following we look for the origin of the long range
dependence found above. In our model the topology and
the TCP ﬂow generating rules are chosen such a way,
that the link/source conditions for self-similarity[3, 13]
are excluded. In network-side models the adaptive na-
ture of TCP, namely the TCP backoﬀ state mechanism
is pointed out as a main origin of such behavior[6, 9].
We investigate now if there is a close relation between
the self-similarity of the traﬃc, and backing oﬀ of the
TCP.

In the backoﬀ phase TCP retransmits the lost packet
and doubles the RTO. TCP keeps track of the doubling
times by a backoﬀ variable b. In the non-backoﬀ phases
b = 0, and in backoﬀ b shows how many times the RTO
counter has been doubled. Due to Karn’s algorithm the
RTO is doubled until two consecutive packet receives its
acknowledgement.

First we recall shortly, that a TCP ﬂow in the backoﬀ
phase produces heavy-tailed statistics in the packet in-
ter arrival time[9]. A TCP in a given backoﬀ state waits
for a period t = 2bTRT T between two packet sending
attempts. The b-th backoﬀ state occurs only after b suc-

1

2

4

5

3
b

FIG. 5: Logarithmic plot of the probability distribution of
backoﬀ states (Pb) as the function of the backoﬀ variable b
The ﬁtted line indicates p = 21% loss probability which gives
a Hurst parameter H = 0.89.

This gives us the possibility to demonstrate the con-
nection between long range dependency and the backoﬀ
distribution. One compares the probability p calculated
from the backoﬀ statistics and the inter packet arrival
time decay factor α calculated from the packet traﬃc
time series. The two value agrees as α ≈ log2(1/2p),
hence the long range dependency is caused mainly by
the backoﬀ mode of the TCP (and not by other external
reasons as e.g. long range distributed user interaction).
We have demonstrated the connection between the
long range dependency, self-similarity and backoﬀ mech-

anism. Finally we search for the origins of backing oﬀ
the TCP.

Our model by construction excludes the known ori-
gins of self-similarity: the TCP ﬂows follow each other
immediately and transfer data at a given rate without
external silent periods as e.g. would be the case with
user-generated interactive ﬂow. The transfered ﬁle sizes
are constant. The buﬀer lengths are much above the crit-
ical chaotic threshold value[6]. The only stochasticity is
in the switching between the destinations of the TCP
ﬂows. This irregularity introduces some unpredictability
in the data ﬂow. If this unpredictability is high, the TCP
estimation for the available sending rate is false. The
consequences of unpredictability has been studied from
many aspects[3, 18], however all the previous studies re-
quire a case when the high real loss probability (due to
small buﬀers or external loss) hinders the TCP to make
suﬃcient predictions.

Here we presented a model, where the stochastic choos-
ing of destination itself pushes TCP into the backoﬀ
phase and generates self-similar traﬃc. How can this
happen?

TCP operates in backoﬀ, if the ACK packet arrive af-
ter the limit set by RTO. The RTO value is calculated
from the traﬃc continuously, using an average over some
time-window. If the traﬃc is bursty, with silent periods
comparable with size of the averaging window, the TCP
cannot adapt to the rapid changes in the traﬃc. In our
model we detect heavy bursts in the queue lengths in the
router. Since TCPs changes the destination randomly, it
can happen, that after a silent period a buﬀer will be fed
by one or two TCP. If these TCPs are in slow start, the
feeding of a buﬀer can be exponential fast. The queue
lengths can hence grow very rapidly.
If a queue gets
longer, packets arriving in this queue must wait longer.
A rapid change in the queue length can cause a so rapid
change in the round trip time of a packet, that the ACK
for this packet arrives after the RTO expires. So large
ﬂuctuations in the queue length (background traﬃc) can
cause a series of virtual losses and backing oﬀ the TCP.

In Fig. 6 we show a typical queue length – time plot,
where the large ﬂuctuations cause backoﬀ phase in a
TCP. There is a clear relation between the increasing
queue length and the evolution of backoﬀ states.

Since in our model only the heavily ﬂuctuating back-
ground traﬃc can back oﬀ a TCP, we can conclude to
identify the ﬂuctuating background as a source of self-
similarity. This self-similarity is a self-generated one,
originating from the improper synchronization of hosts,
which continuously send data to each other by using
many successive TCP ﬂows.

 1200

 1000

 800

 600

 400

 200

)
s
t
e
k
c
a
p
(
 
l

queue length (l)
backoff variable (b)

5

 
e
l
b
a
i
r
a
v
 
f
f
o
k
c
a
b

6

5

4

3

2

1

 0

0
 1160  1180  1200  1220  1240  1260  1280  1300

t (sec)

FIG. 6: Queue length in one of the buﬀers in the router R
and the backoﬀ variable of the TCPs in a host. The time
intervals where the queue length changes drastically overlap
clearly with the backoﬀ phases of the TCP. The inset shows
a magniﬁcation of part of the data.

CASE STUDY: MEASUREMENT

In this section we present active measurement results
which show similar results in a real network environ-
ment as found in the previous section in a small model
scenario. The time evolution of a long TCP ﬂow on a
transcontinental connection was followed on the IP level
by tcpdump[17] and on the kernel level by a slightly mod-
iﬁed linux kernel from the 2.2.x series. The modiﬁed ker-
nel gave us the possibility to follow directly the internal
TCP variables in situ for a real network environment.

On the transcontinental line chosen for the measure-
ment typically many TCP connections share the same
route giving a highly ﬂuctuating background traﬃc. Ad-
ditionally on the long line with many routers it is pos-
sible that the packets of our TCP ﬂow stacks in ﬁlled
queues. So the round trip time can ﬂuctuate in a very
wide range resulting many backoﬀ states. Figure 7 shows
a very congested time interval, where many backoﬀ states
were observed. Here we mention, that in contrast to the
TCP implementations of ns-2, the backoﬀ variable b of
the linux kernel can have larger values than 6.

As described in the previous section the self-similarity
is characterized by the Hurst parameter, if the stochastic
process under consideration is weakly stationary. To sat-
isfy this condition we restrict our analysis only for some
parts (time intervals) of the whole measurement.

In the time range under study the highly congested
traﬃc showed self-similar nature. The variance time plot
for the aggregated time series of packet arrivals is plotted
in Figure 8, from which we can read oﬀ the Hurst param-
eter 0.69. In Fig. 9 we show the statistical distribution
of packet inter arrival times, which show an α = 1.505
decay giving a similar value for the Hurst parameter as
calculated from the variance time plot.

Since we do not have total control over the whole in-
ternet, we cannot prove rigorously that the observed self-

 0

 50000  100000  150000  200000  250000  300000  350000

-10

-8

-6

-4

0

2

4

6

-2
log(dt)

t (sec)

FIG. 7: Backoﬀ states of a TCP, which sends data through a
congested transcontinental line.

FIG. 9: Statistics of inter arrival times. The ﬁtted line on
the slow decaying tail with α = 1.505 indicates long range
dependent traﬃc.

 12

 10

 8

 6

 4

 2

 0

20

18

16

14

12

10

8

6

4

2

0

b

t
 
r
a
V
g
o
l

 

0

2

4

6

8

10

12

14

log t

FIG. 8: Variance time plot of measured data in the longest
stationary time interval. The ﬁtted line indicates H = 0.69.

similarity is the consequence exclusively of the ﬂuctua-
tions in the background traﬃc as it is in the simulation
scenario presented in the previous section. However it is
possible to show, that — as in the simulation — there is
a close relation between the inter packet time statistics
and the backoﬀ statistics under such conditions where
the real packet loss is low, indicating self-generated self-
similarity.

Here we investigate ﬁrst, what was the loss rate at
the line.
In end-to-end measurements packet loss can
be easily detected by analyzing tcpdump data. But to
gain this direct information about the traﬃc, one needs
special rights on the origin of the TCP ﬂow and on the
destination as well. This ideal condition is given usually
only for a very restricted number of routes. In most cases
one can monitor the network traﬃc only on one side as
it was the case in our measurement. We applied the
algorithm of Benko et.al.[1] with some improvement to
detect packet losses from tcpdump data, and to decide if
the packet is lost really or timeout occurred.

The algorithm is the following. An eﬀective loss occurs,
if a packet is resent. A resent packet begins with the same
serial number as the original packet, so we have to count
the number of packets, whose sequence number occurred

6

)
.
b
o
r
p
.
m
u
c
.
v
n
i
(
g
o
l

0

-2

-4

-6

-8

-10

-12

-14

more than once in the TCP ﬂow. We used timestamps
to avoid the wrapped sequence number problem.

Detecting real loss events is a bit more tricky. A sent
packet is probably lost if the TCP receives duplicate ac-
knowledgement. Duplicate ACKs are sent by the receiv-
ing TCP if a packet with higher sequence number has
arrived. However this can happen due to changes in the
packet order also, therefore the TCP waits a little and
retransmits the packet only if packet order switching is
very improbable. Hence for detecting real loss events we
have to count the number of resent packets, which are
sent due to receiving of duplicate ACKs.

Previously we mentioned that the background traﬃc
during the whole measurement cannot be approximated
by weakly stationary stochastic processes and for anal-
ysis one has to consider only parts of the data. In this
parts the ﬂow can be characterized by static parameters
e.g. the loss ratio is constant in time. These intervals
cannot be too short to have enough data for statistical
considerations.
In Fig. 10 we plot the loss probability
versus time for the whole measurement. One can see
long plateaus however there are non-stationary regimes
as well. In the following we restrict ourself only for the
longest stationary range.

We investigated the statistics of the backoﬀ states for
this time regime from the data logged by the modiﬁed
linux kernel. We found, that the distribution shows an
exponential decay as it follows from the Markovian de-
scription presented in the previous section. The Fig. 11
shows the decay of the probability of the backoﬀ states.
The slope of the ﬁtted line indicates a loss probability
p = 16.5% felt by the TCP. This loss rate is consis-
tent with the asymptotic decay of the packet inter-arrival
times (Fig. 9) and with the Hurst parameter of the ag-
gregated traﬃc (Fig. 8).

So the close relation between the backoﬀ states and the
self-similarity of the traﬃc holds. The next question is,
if the TCP is backed oﬀ due to real packet losses or the
packets where only delayed and timed out. In Fig. 12 we

real loss
effective loss

real loss
effective loss
loss from backoff

7

 22

 20

 18

 16

 14

 12

 10

 8

 6

)

%

(
 
s
s
o
l
 
l
a
e
r
 
 
d
n
a
 
s
s
o
l
 
e
v
i
t
c
e
f
f
e

 0

 0

 50000  100000  150000  200000  250000  300000  350000

 4
 55000

t (sec)

 75000

 95000

t (sec)

FIG. 10: Real loss and eﬀective loss during the measurement.
The loss rate changes in time, therefore one has to restrict
the analysis for the weakly stationary intervals.

FIG. 12: Eﬀective loss and real loss rate for a time interval.
The horizontal line shows the loss probability calculated from
the backoﬀ states.

self-generated burstiness evolves to self-similarity how-
ever the network properties indicate simple, easily pre-
dictable traﬃc.

In the future we focus on the self-generation of the
burstiness, what are the exact conditions for emergence
of self-similarity in perfect network.

 25

 20

 15

 10

 5

)

%

(
 
s
s
o
l
 
l
a
e
r
 
d
n
a
 
s
s
o
l
 
e
v
i
t
c
e
f
f
e

 

b
P
g
o
l

 0

-2

-4

-6

-8

-10

-12

-14

-16

 0

 2

 4

 8

 10

 12

 6

b

FIG. 11: Logarithmic plot of the backoﬀ value probabilities
observed in a transcontinental TCP data ﬂow. The ﬁtted line
indicates a p = 16.5% loss rate felt by the TCP.

Acknowledgements

G. V. thanks the support of the Hungarian Science
Fund OTKA T037903 and T 032437. P. P. thanks the
support of the Hungarian Science Fund OTKA D37788,
T34832, T42981.

compare the loss ratio from the backoﬀ statistics (16.5%)
with the loss probability calculated from the tcpdump
output. We ﬁnd, that the average loss probability felt
by the TCP equals with the real plus virtual (eﬀective)
loss and not with the real loss alone. Here the diﬀerence
between the two type of losses is crucial, since the real loss
is smaller than 12.5%, the lower bound of loss probability,
which gives self-similar traﬃc, but the eﬀective loss is
higher.

CONCLUSION

We have demonstrated in a model network and in a
real measurement how TCP can generate self-similar traf-
ﬁc itself. We showed that at very low packet loss rate
the congestion control mechanism detects false packet
losses if the background traﬃc is bursty. On this ﬂuc-
tuating background traﬃc TCP responds with backoﬀ
states. The switching between backoﬀ and congestion
avoidance phases introduces further ﬂuctuations into the
overall traﬃc, which results additional backoﬀs. This

[1] P.Benko and A.Veres, “A Passive Method for Estimating
End-to-End TCP Packet Loss”, Globecom 2002, Taipei
[2] I.Csabai, “1/f noise in computer network traﬃc”, J.

Phys. A: Math. Gen.,27, L417-L421, 1994.

[3] M.Crovella and A.Bestavros, “Self-similarity in world
wide web traﬃc:
Evidence and possible causes”,
IEEE/ACM Transactions on Networking, pp. 835-846,
December 1997.

[4] P.Danzig, S.Jamin, R. C´aceres, D.Mitzel and D.Estrin,
for driving wide-area
Internetworking: Re-

“An empirical workload model
TCP/IP network simulations”,
search and Experience 3:1-26, 1992.

[5] A.Erramilli, R.P. Singh and P. Pruthi, “Chaotic Maps
As Modesl of Packet Traﬃc”, Proc. of the 14th ITC,
329-338, June, 1994.

[6] A.Fekete and G.Vattay, “Self-Similarity in Bottleneck

Buﬀers” Proceedings of Globecom, 2001

[7] A.Feldmann, A.C.Gilbert, W.Willinger and T.G.Kurtz,
“The changing nature of network traﬃc: Scaling phe-
nomena”, ACM Computer Communication Review, pp.
5-29, April 1998.

[8] D.R.Figueiredo, B.Liu, V.Mishra and D.Towsley, “On
the autocorrelation structure of TCP traﬃc”, Tech. Rep.

8

00-55, Dep. of Computer Science, University of Mas-
sachusetts, Amherst, November 2000.

[9] L.Guo, M.Crovella and I.Matta, “TCP congestion con-
trol and heavy tails”, Tech. Rep. BUCS-TR-2000-017,
Computer Science Dep., Boston University, 2000.

[10] V.Jacobson, “Congestion avoidance and control,” in

ACM SIGCOMM, 314, 1998.

[11] V.Jacobson, “Modiﬁed TCP congestion avoidance algo-
rithm,” Tech. Rep., end2end-interest mailing list, April
1990.

[12] P.Karn and C.Partridge, “Improving round-trip time es-
timates in reliable transport protocols”, ACM Transac-
tions on Computer Systems (TOCS), vol. 9, pp 365-373,
1991.

[13] W.E.Leland, M.S.Taqqu, W.Willinger and D.V. Wilson,
“On the self-similar nature of ethernet traﬃc (extended
version)”, IEEE/ACM Transactions on Networking, pp.

1-15, February 1994.

[14] UCB, LBNL and VINT, “Network simulator - ns (version

2)”, http://www-mash.cs.berkeley.edu/ns, 2002.

[15] K.Park, G.Kim and M.Crovella, “On the relationship be-
tween ﬁle sizes, transport protocols, and self-similar net-
work traﬃc”, In Proceedings of the International Confer-
ence on Network Protocols, pp. 171-180, Oktober 1996

[16] V.Paxson and S.Floyd, “Wide-area traﬃc: the failure of
Poisson modeling. IEEE/ACM Transactions on Network-
ing 3:226-244, 1995.

[17] LBL tcpdump, http://www.tcpdump.org.
[18] A.Veres and M.Boda, “The chaotic nature of TCP con-
gestion control”, in IEEE INFOCOM 2000, March 2000.
[19] A. Veres, Zs. Kenesi, S. Molnar, and G. Vattay, “On the
Propagation of Long-Range Dependence in the Internet,”
Computer Communication Review 30, No 4, pp. 243-254,
2000


6
0
0
2
 
b
e
F
 
8
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
0
0
3
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Dissipative functional microarrays for classiﬁcation
of crystallization patterns

D. Napoletani ∗, D. C. Struppa†, T. Sauer †
and
V. Morozov‡, N. Vsevolodov‡, C. Bailey ‡

Abstract

In this article, we describe a new method of extracting information
from signals, called functional dissipation, that proves to be surpris-
ingly eﬀective for enhancing classiﬁcation of high resolution, texture-
rich data as in our case study of crystallization patterns. Our algorithm
bypasses to some extent the need to have very specialized feature ex-
traction techniques, and can potentially be used as an intermediate,
feature enhancement step in any classiﬁcation scheme.

Functional dissipation is based on signal transforms, but uses the
transforms recursively to uncover important features. We generate ran-
dom masking functions and ‘extract’ features with several generalized
greedy regression iterations. In each iteration, the recursive process
modiﬁes several coeﬃcients of the transformed signal with the largest
absolute values according to the speciﬁc masking function; in this way
the greedy regression is turned into a slow, controlled, dissipation of
the structure of the signal. The idea is that some unknown statistical
features of the original signal can be detected in the dissipation process
at least for some of the masking functions.

Keywords: Features enhancement, classiﬁcation, greedy regression, non-
linear iterative maps.

∗Center for Biomedical Genomics, George Mason University, Manassas, VA 20110,

email: dnapolet@gmu.edu

20110, Manassas, VA

†Department of Mathematical Sciences,George Mason University, Fairfax, VA 22030
‡National Center for Biodefense and Infectious Diseases George Mason University

1

1

Introduction

Pattern classiﬁcation normally consists of two somewhat distinct phases:
the identiﬁcation of a set of signiﬁcant quantitative features in the pattern,
and the application of a learning algorithm to the features. If the feature
extraction process is not robust, the success of the learning algorithm is lim-
ited. On the other hand, after the feature space is properly identiﬁed, there
is a wide choice of standard learning algorithms that can be applied, such as
support vector machines, neural networks, k-neighborhood algorithms( see
[6]).

In this paper we introduce a type of feature enhancement algorithm,
which we call dissipative functional microarray scheme, and is directly in-
spired by the microarray methodology. In general terms, the idea behind the
use of microarrays is that it is often possible to answer many questions on a
phenomenon, if one can collect a large and diverse data set about it. This
may be feasible even when no speciﬁc interpretation for the data is known
(see for example [1]). The algorithm we describe here seems particularly
suitable for high resolution, texture-rich data and bypasses the need to have
very specialized feature extraction algorithms.

The case study of this paper is the classiﬁcation of crystallization pat-
terns of amino acids solutions aﬀected by addition of small quantities of
proteins. The goal is to recognize whether an unknown solution contains
one of several proteins in a database. One diﬃculty is that crystallization
patterns may be signiﬁcantly aﬀected by laboratory conditions, such as tem-
perature and humidity, so the degree of similarity of patterns belonging to
a same protein is subject to some variations.

Our basic approach is to derive long feature vectors for each amino acid
reporter by an unconventional use of recursive greedy regression. Since the
crystallization patterns are generated by stochastic processes, there is a great
local variability in each droplet and any signiﬁcant feature must encode a
statistical information about the image to be part of a robust classiﬁer.

In section 2 we introduce a features enhancement method, functional
dissipation, to explore the feature space. In sections 3 and 4 we apply the
method to the crystallization data to show how a suﬃciently large number of
diﬀerent functional dissipations can greatly improve the classiﬁcation error
rates even when a single amino acid is used, these collections of functional
dissipations are akin to microarrays in scope and limitations.

2

2 Functional dissipation for classiﬁcation

In this section we introduce a classiﬁcation algorithm that is designed for
cases where feature identiﬁcation is complicated or diﬃcult. We ﬁrst outline
the major steps of the algorithm, and then discuss speciﬁc implementations.
In the following sections we apply one suggested implementation to droplet
classiﬁcation, and describe the results.

We begin with input data divided into a training set and a test set. We

then follow four steps (to be discussed in detail later on) as follows:

A Choose a classiﬁer and a ﬁgure of merit that quantiﬁes the classiﬁcation

quality.

B Choose a basic method of generating features from the input data, and
then enhance the features by a recursive process of structure dissipa-
tion (see below for a clariﬁcation of this key step of the algorithm).

C For a ﬁxed integer k, search the feature space deﬁned in B for the k
features which maximize the ﬁgure of merit in A on the training set.

D Apply the classiﬁer from A, using the optimal k features from C, to

classify the test set.

In step A, for example, multivariate linear discrimination can be used
as a classiﬁer. This method comes with a built-in ﬁgure of merit, the ra-
tio of the between-group variance to the within-group variance. More so-
phisticated classiﬁers often have closely associated evaluation parameters.
Cross-validation or leave-one-out error rates can be used.

The key of the algorithm, however, lies in step B. Methods for generating
features depend on the input data. For two-dimensional images, orthogo-
nal or over-complete image transforms can be used. In this article, we will
describe a new method of extracting information from images, called func-
tional dissipation, that proves to be surprisingly eﬀective for our case study
of classiﬁcation of crystallization patterns. The method of functional dis-
sipation is a way to leverage a feature extraction method, such as image
transforms, to generate more interesting and varied features. This method
is based on some given image transforms, but uses the transforms recursively
to uncover important features.

Consider a single input datum X and several invertible transforms Ti,
i = 1, . . . , K, that can be applied to X. (In the case study to be shown
later, X represents a 256 × 256 gray scale image and Ti represent Discrete
Wavelet Transforms.)

3

Let A(x) be a real-valued function deﬁned on IR, which we call a mask
or a masking function. At each iteration we extract several coeﬃcients
from the input datum X. Begin by setting the initial collection of coeﬃcients
C selected by the algorithm to be the empty set, then apply the following
functional dissipation steps (E1)-(E3) K times. Fix positive integers
K, M and set X1 = X.
For i = 1, . . . , K:
(E1): Compute the transform TiXi
(E2): Collect the M coeﬃcients C(m), m = 1, ..., M , of TiXi with

largest absolute value. Add this set of coeﬃcients to the set C

(E3): Apply the mask: Set C ′(m) = A(|C(m)|)C(m), and modify the
(TiXi)′

corresponding coeﬃcients of TiXi in the same fashion. Set Xi+1 = T −1
to be the inverse of the modiﬁed coeﬃcients (TiXi)′

i

At the conclusion of the K steps, features are generated by computing
statistics that describe the probability distribution of the set C. For example,
one could use m(h), h = 2, 3, 4, the second, third and fourth moments of
the set. These statistics are used as features, delivered by the means of
functional dissipation. If we carry out these steps for N diﬀerent masks An,
we obtain a 3N -dimensional feature vector for each data input.

One way to view our approach is as a greedy regression strategy such
as, for example, matching pursuit (see [7] chapter 9), but used in a new and
unusual way. In general matching pursuit is used to ﬁnd good suboptimal
approximations to a signal. The way this is done is by expanding a function
f in some dictionary D = {g1, ..., gP } and by choosing the element in the
dictionary gi for which | < f, gi > | is maximum. Given an initial approxi-
mation ˜f = 0 of f , and an initial residue Rf = f , we set ˜f = ˜f + < f, gi > gi
and Rf = Rf − < f, gi > gi. The process is repeated on the residue several
times to extract successively diﬀerent relevant structures from the signal.

In our algorithm instead, we generate random maskings functions and
‘extract’ features with several generalized greedy regression iterations. In
each iteration, the recursive process modiﬁes several of the largest coeﬃ-
cients of the transformed signal according to the masking function; in this
way the greedy regression is actually not a regression at all, but rather a
slow, controlled, dissipation of the structure of the signal. The idea is that
some unknown statistical feature of the original signal may be detected in
the dissipation process at least for some of the random maskings.

This process is striking in that, individually, each feature extraction with
masking becomes unintelligible, because of the added randomness and dis-
sipation, and only a string of such feature extractions can be ‘blindly’ used
to some eﬀect. There is some similarity in spirit between our approach and

4

the beautiful results on random projections reconstructions of [2], [3] with
the diﬀerence that we do not focus on reconstruction issues, but on classiﬁ-
cation, and we use several distinct randomization and dissipation processes
to our beneﬁt.

There are several results that shows how the structure of the matching
pursuit algorithm has potentially more applications than just approximat-
ing functions.
It is possible to prove interesting dynamical properties for
the residue ([4]) that are in principle not limited to greedy regression, and
we could substitute < f, gi > in the iteration with some suitable function of
it and still expect interesting dynamics for the behavior of the residue. If
we generalize matching pursuit in this way, then the whole iterative process
of extracting coeﬃcients becomes simply an instance of non-linear iterative
map, disjoined from approximation purposes. Moreover, it turns out that
some of the most powerful data analysis algorithms, so called boosting meth-
ods, can be interpreted essentially as an attenuated greedy regression when
the choice of ‘best’ coeﬃcient at each iteration is made with respect to more
sophisticated criteria than the absolute value of the coeﬃcients (see [6] chap-
ter 10 and [5]).

The masking functions that we use in (E1)-(E3) for our case study are some-
what arbitrary. We generate several realizations of a low variance Gaussian
white noise and we convolve them with low-pass ﬁlters of diﬀerent frequency
supports. We repeat this process N times to generate curves of the type
shown in Figure 1.

Even though the speciﬁc process of generation of the random masks
is arbitrary, the guiding idea is to have suﬃcient variability in the masks
themselves while preserving some of the structure of the original signal from
one iteration to the next. To this extent, the masking functions are designed
to assume relatively small values so that the image is only slightly aﬀected at
each iteration by the change of of a subset of its coeﬃcients. A preliminary
analysis shows that our results hold even when the maskings are allowed to
have larger variations, as long as not too many coeﬃcients are altered at
each iteration. Indeed we expect a wide range of possible maskings to be
suitable for this method.

3 Case Study: Droplet Classiﬁcation

In this section we describe a case study that motivated this research, the
classiﬁcation of proteins by their eﬀect on crystallization patterns of amino

5

5

10

5

10

−1

−2

0

2

1

0

2

1

0

−1

−2

0

1

0

1

0

−1

−2

0

−1

−2

0

5

10

5

10

Figure 1: Examples of masking functions used in the algorithm (E1)-(E3).

acids used as reporter substances. We restrict ourselves to a database of 4
proteins: albumin from chicken egg white, hemoglobin from bovine blood,
lysozyme, albumin from bovine serum, which we will denote as P 1,P 2,P 3,
and P 4, respectively. The control solution without protein will be denoted
as W ater. These four proteins were added to solutions of 4 amino acids:
leucine, lysine, phenylalanine and methionine, denoted by A1, A2, A3, and
A4, respectively. Both the number of proteins to classify and the number
of reporters can increase in practice, but we restrict our analysis to this
smaller set for our purposes. A crucial advance in the experimental study of
droplets has been the ability to generate quickly a large number of droplets
to which diﬀerent amino acids have been added. Remarkably, the addition
of proteins to the amino acid reporters can have very diﬀerent eﬀects on the
crystallization patterns. In some cases there is no visible diﬀerence between
droplets of amino acid plus W ater (control) and droplets of amino acid plus
protein; for some other proteins the resulting crystallization patterns are
very diﬀerent from control [8].

Remark 1: An alternative approach to classifying droplet patterns (see
[8]) was based on a binary code that can be attached to each protein by la-
belling with a “1” a crystallization pattern of the amino acid reporters if the
reporter is noticeably aﬀected by the addition of the protein, and by labelling
the crystallization pattern with a “0” otherwise. This approach was very

6

Figure 2: Each row of this ﬁgure shows (from left to right) representative
patters of amino acids A1,A2, A3, A4 to which proteins P 1, P 2,P 3,P 4 and
W ater were added.

eﬀective in visual inspection classiﬁcation. We show in this paper that the
binary code is not necessary for a computational classiﬁcation algorithm.
The original motivation for this work was the attempt to design an instru-
ment to detect contaminants in droplets of water suspended in the air. The
results described in this paper, together with the results in [8], indicate that
such an approach is a feasible one.

In Figure 2, each row shows (from left to right) representative patters of
four amino acid (A1, A2, A3, A4) solutions with four proteins P 1, P 2,P 3,P 4
added. In Figures 3 to 6 we show full size images of some representative
droplets to fully appreciate their complexity. Note that the structures in
Figure 4, and 5 (which are distinct in spatial distribution and type) are
very ﬁne, and only quite sensitive tools may be able to detect and use such
information.

7

Figure 3: Typical crystallization pattern of A2 + W ater.

Figure 4: Typical crystallization pattern of A3 + W ater.

Figure 5: Typical crystallization pattern of A3 + P 2.

8

Figure 6: Typical crystallization pattern of A4 + P 1.

4 Results and Discussion

In this section we discuss a particular implementation of the pattern classiﬁ-
cation algorithm of Section 2, and the results of applying it to the droplets.
We used for our analysis 20 gray-scale images of crystallization patterns for
each combination of amino acids A1, A2,A3, A4 with proteins P 1,P 2,P 3,P 4
and with W ater. For each droplet Xija, i.e. the i-th instance of protein j
with addition of amino acid a, we reduce the size of the image to a matrix of
256 by 256 pixels and we normalize the image so that its mean as an array
is zero and its variance is one.

For purposes of comparison, we will apply the algorithm with and with-
out the advantage of function dissipation. This corresponds to setting the
parameter K, the number of iterations of steps (E1)-(E3), to one, or greater
than one, respectively.

The transforms Ti in steps (E1)-(E3) are set to be discrete wavelet trans-
forms with Haar mother wavelet for odd values of i and Daubechies mother
wavelet with 10 vanishing moments for even values of i; we compute the
wavelet transforms up to coeﬃcients of level d = 5.

The masking functions we use are the Gaussian processes described at
the end of section 2. In practice we restrict the domain of the masks to be
some ﬁxed interval [0, R]. As long as R is suﬃciently large, we veriﬁed that
its speciﬁc value does not aﬀect the quality of the classiﬁcation in any signif-
icant way; in the following implementation we take R = 10. In the cases in
which the largest wavelet coeﬃcients generated by the algorithm are outside
the speciﬁed domain, the mask is periodically extended as needed.

9

Remark 2: In this speciﬁc implementation of steps (E1)-(E3) we alter-
nate between two diﬀerent wavelet transforms to assure that, even though
we use orthonormal transforms, there is a non-linear eﬀect of the masking
procedure on the coeﬃcients generated in subsequent iterations. If T is cho-
sen to be an over-complete transform, then it is suﬃcient to take Ti = T
for all i, since matching pursuit with over-complete transforms has already
a non-linear eﬀect on the coeﬃcients of each subsequent iteration.

First, with K = 1 no dissipation occurs, moreover only N = 1 masks are
used. For each droplet a three dimensional feature vector is extracted with
(E1)-(E3) by gathering the largest M = 5000 wavelet coeﬃcients in (E2)
and by computing the ﬁrst three moments of these coeﬃcients.

We apply now a general purpose classiﬁcation algorithm such as a linear
discriminant analysis classiﬁer to the output of (E1)-(E3). We tested the
algorithm by dividing the 20 instances of feature vectors for each class ran-
domly into a training set of 15 instances and a test set of 5 instances. We
trained the classiﬁer on the training set and then tested it on the remaining
5 instances of each class. Note that we are interested in identifying each
distinct protein and control reporters as well, so the total number of classes
that we try to discriminate are 5, i.e. the four proteins and W ater.

We repeated the classiﬁcation scheme 10000 times, using diﬀerent di-
visions into training and testing sets to obtain estimated of the misclas-
siﬁcation error rates for each class. The algorithm is unable to achieve
classiﬁcation with any reasonable degree of accuracy if only one amino acid
is used: the average misclassiﬁcation error for the four test proteins and
W ater is 0.1288 if we use only A2, 0.1629 for A7, 0.2118 if we use A3 and
0.2650 if we use A4. Note that no strong classiﬁcation results are obtained
if we don’t allow dissipation ( i.e. for K = 1) even when we use N > 1 masks.

Remark 3: Interestingly, suppose we use all four amino acids in the classi-
ﬁcation scheme by randomly stringing together moment vectors for droplets
of a given protein and for all available amino acids. Since we are using
4 amino acids, we obtain in this way twenty 12N -dimensional moments
vectors. Set again N = 1, then under these conditions the average misclas-
siﬁcation error is 0.0055 with a maximum misclassiﬁcation error (achieved
for P 3) of 0.0078. We improve the performance of the algorithm by at least
an order of magnitude with respect to the case in which only single amino
acids are used. Moreover, P 2, P 4 are never misclassiﬁed as W ater, while
P 1 and P 3 are misclassiﬁed as W ater only with frequency below 0.0018.

10

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

1

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

1

1.5

2

2.5

3

1.5

2

2.5

3

Figure 7: The ﬁrst 3 moments associated to a mask for: (left plot) the
20 instances of A1 plus P 4 (in red) and the 20 instances of A1 plus P 2
(in green); (right plot) the 20 instances of A1 plus P 4 (in red) and the 20
instances of A1 plus W ater (in blue).

Second, we now turn on the function dissipation technique by setting K = 5
in the algorithm. In our speciﬁc implementation we take N = 40 random
functions An, n = 1, ..., N deﬁned on IR then for each An we can apply steps
(E1)-(E3) with the set number of iterations. At each iteration we extract
M = 1000 coeﬃcients (to have the set C of same size as in the case K = 1).
If we repeat (E1)-(E3) for each masking An, we obtain a 3N -dimensional
feature vector for each image droplet with the addition of a single amino
acid.

In Figure 7(left) we show the 3 moments associated to one speciﬁc mask
for the 20 instances of A1 plus P 4 (in red) and the 20 instances of A1 plus
P 2 (in green), note that there is only a marginal separation between these
classes for this speciﬁc mask; Figure 7(right) shows the 3 moments for the
20 instances of A1 plus P 4 (in red) and the 20 instances of A1 plus W ater
(in blue), note that in this case we observe full separation between the two
classes for all three moments.

The use of many masks allow to look at data from many diﬀerent (albeit
unstructured) view points, in line with microarray approach. We sugges-
tively call each of the elements of the feature vector a dissipative gene, when
we display the resulting random genes in several columns, each column rep-

11

resenting the dissipative genes for one instance of a protein or W ater, we
have what we can properly call a dissipative functional microarray (see Fig-
ure 8).

Again, we tested the algorithm by dividing the 20 instances of feature
vectors for each class randomly into a training set of 15 instances and a test
set of 5 instances and repeating the the classiﬁcation scheme 10000 times
for each random splitting of the instances.

Remarkably, we observe in our experiments that a subset of random
genes can bring great improvement in misclassiﬁcation error rates even when
the feature vectors are derived by the crystallization patterns of a single
amino acid reporter. More particularly, given each amino acid reporter, we
select the ‘best’ k dissipative genes (say for example k = 12) for which the
ratio of between-class variance over within-class variance for the training
sets were maximal (see [6] page 94 for more on such notion).
If we use
only these 12 dissipative genes in the classiﬁcation of the test sets, then the
average misclassiﬁcation error for the 5 classes is: < 10−4 using only amino
acid A1, 0.0013 using A2, 0.0009 using A3 and 0.0027 using A4. This is
extremely surprising as, even by eye inspection, some of the proteins do not
show signiﬁcant diﬀerences when a single amino acid is considered.

For linear discriminant analysis classiﬁers, the quality of the classiﬁca-
tion results does not seem to be aﬀected signiﬁcantly by the choice of the
number k of dissipative genes, as long as care is taken not to make the linear
discriminant classiﬁer ill-conditioned by taking k too large.

In Figure 8 we show from left to right 10 instances of gene column vec-
tors for A1 plus P 1 (of length 120 i.e. 3N with N = 40) and 10 instances
for A1 plus W ater. Note how this speciﬁc dissipative functional microar-
ray implemented with wavelet transforms shows a distinct behavior for P 1
droplets and W ater droplets for many genes.

In is interesting to note that, supposedly, one of the weaknesses of match-
ing pursuit is its inability, as a greedy algorithm, to ﬁnd an optimal repre-
sentation for a given signal; the use of randomization and dissipation turns
this weakness in a strength, at least in the setting of classiﬁcation problems.
This change of perspective is in line with the idea that greedy regression
has a greater potential than simply being an approximation to optimal so-
lutions. Note moreover that the dissipative functional microarrays based on
the greedy regression technique of (E1)-(E3) can be used as a preprocessing
for any classiﬁcation scheme, and not just the linear discriminant analysis

12

20

40

60

80

100

120

5

10

15

20

Figure 8: Dissipative functional microarray for, respectively from the left
column to the right one, 10 instances of A1 plus P 1 and 10 of A1 plus
W ater. We highlight with an arrow one example of gene for which A1 plus
P 1 and A1 plus W ater show clearly diﬀerent behavior.

13

scheme used in this section 1.

Our dissipative functional microarray algorithm seems to show that in-
deed ideas from biological data analysis can be successfully turned into func-
tional data analysis tools in line with the methodological principles of [10].
This is only one possible way in which controlled dissipation and, possibly,
other non-linear functional schemes can be used to extend the impact of the
microarray methodology in computational science; functional microarrays
have the potential of bringing to data analysis many of the beneﬁts that
they brought to biology, but, again, at the same price and under the same
conditions: a lack of structural understanding, a need for quick approxima-
tion procedures to generate microarrays.

Acknowledgments

The authors gratefully acknowledge support from DOE grant, DE-F C52-
04NA25455.

1

A patent application has been ﬁled on the method described in this paper with U.S.

Provisional Patent Application Number 60/687,868, ﬁle date 6/7/2005.

14

References

[1] P. Baldi, G. W. Hatﬁeld, W. G. Hatﬁeld, DNA Microarrays and Gene
Expression : From Experiments to Data Analysis and Modeling. Cam-
bridge University Press, 2002

[2] E. J. Candes and J. Romberg (2004). Practical Signal Re-
at

from Random Projections.

covery
www.acm.caltech.edu/ emmanuel/publications.html

Submitted. Available

[3] E. J. Candes and T. Tao (2004). Near Optimal Signal Recovery From
Submitted.

Random Projections: Universal Encoding Strategies?
Available at www.acm.caltech.edu/ emmanuel/publications.html

[4] G. Davis, S. Mallat and M. Avelaneda, Adaptive Greedy Approxima-
tions, Jour. of Constructive Approximation, vol. 13, No. 1, pp. 57−98,
1997

[5] Y. Freund, R. Schapire, A short introduction to boosting, J. Japan.
Soc. for Artifcial Intelligence, vol. 14 n. 5 (1999), pp. 771 − 780.

[6] T. Hastie, R. Tibshirani, J. Friedman, The elements of Statistical

Learning, Springer 2001.

[7] S. Mallat, A Wavelet Tour of Signal Processing, Academic Press, 1998.

[8] V. N. Morozov, N. N. Vsevolodov, A. Elliott, C.Bailey, Recognition of
Proteins by Crystallization Patterns in an Array of Reporter Solution
Microdroplets, Anal. Chem. vol. 78 (2006), pp. 258 − 264.

[9] S. Noh, K. Bae, Y. Park, J. Kim, A Novel Method to Extract Features
for Iris Recognition System. AVBPA 2003, LNCS 2688 (2003), pp.
862 − 868.

[10] J.O. Ramsay, B.W. Silverman, Functional Data Analysis, Springer

1997.

15


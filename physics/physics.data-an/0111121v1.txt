1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
2
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

NEW ADVANCES IN BAYESIAN CALCULATION FOR LINEAR
AND NONLINEAR INVERSE PROBLEMS

A. Mohammad-Djafari, H. Carfantan and M. Nikolova
Laboratoire des Signaux et Syst`emes (CNRS-ESE-UPS)
´Ecole Sup´erieure d’ ´Electricit´e,
Plateau de Moulon, 91192 Gif-sur-Yvette, France.
E-mail: djafari@lss.supelec.fr

Abstract. The Bayesian approach has proved to be a coherent approach to handle ill
posed Inverse problems. However, the Bayesian calculations need either an optimization
or an integral calculation. The maximum a posteriori (MAP) estimation requires the
minimization of a compound criterion which, in general, has two parts: a data ﬁtting
part and a prior part.
In many situations the criterion to be minimized becomes multimodal. The cost of
the Simulated Annealing (SA) based techniques is in general huge for inverse problems.
Recently a deterministic optimization technique, based on Graduated Non Convexity
(GNC), have been proposed to overcome this diﬃculty.
The objective of this paper is to show two speciﬁc implementations of this technique for
the following situations:
– Linear inverse problems where the solution is modeled as a piecewise continuous func-
tion. The non convexity of the criterion is then due to the special choice of the prior;
– A nonlinear inverse problem which arises in inverse scattering where the non convexity
of the criterion is due to the likelihood part.

Key words: Inverse problems, Regularization, Bayesian calculation, Global optimiza-
tion, Graduated Non Convexity

1.

Introduction

We consider the case of general inverse problems:

y = A(x) + b,

where x is the vector of unknown variables, y is the data, A is a linear or non
linear operator and b represents the errors which are assumed, hereafter, additive,
zero-mean, white and Gaussian.

The Bayesian approach has proved to be a coherent approach to handle these
problems. However, the Bayesian calculations need either an optimization or an
integral calculation. The maximum a posteriori (MAP) estimation needs the min-
imization of a compound criterion:

J(x) = − log p(x|y) = Q(x) + λΩ(x)

which, in general, has two parts: a data ﬁtting part:

Q(x) = − log p(y|x) = ky − A(x)k2

(1)

(2)

(3)

2

and a prior part:

A. Mohammad–Djafari et al.

Ω(x) = − log p(x) =

φ(xr − xs).

(4)

Xr Xs∈Nr

This last expression is due to general Markov modeling where r and s are two site
indexes and Nr means neighbor sites of r.

In many situations the criterion J(x) becomes multimodal. We consider here
two cases: The ﬁrst is the case of general linear inverse problems with markovian
priors with non convex energies, and the second is, the case of non-linear inverse
problem.

In both cases we need a global optimization technique to determine the solution.
In the ﬁrst case the non convexity is due to the second termed and in the second
case the non convexity is more due to the ﬁrst termed.

The cost of the Simulated Annealing (SA) based techniques is mainly depen-
dent to the neighborhood size of the posterior marginal probability distribution
p(xj |x, y) which is directly related to the neighborhood size of the prior marginal
probability distribution p(xj |x) and the support of the operator A. When A is
a local operator with very small support, i.e.; when the data element yi depends
only to a few number of the unknown variables xj , then SA can be implemented
eﬃciently [1, 2, 3]. But, unfortunately, this is not the case of many inverse prob-
lems, where the support of the operator is not small. The cost of SA is then, in
general, huge for these problems.

Recently a deterministic relaxation algorithm, inspired by the Graduated Non
Convexity (GNC) principle, has been proposed by Blake and Zisserman in [4, 5] for
the optimization of the multimodal MAP criteria. They have shown its eﬃciency in
practical applications for noise cancellation and segmentation. This algorithm has
been extended to the general linear ill-posed inverse problem by Nikolva et al. in [6].

The object of this presentation is to show two speciﬁc implementations of this

technique for two speciﬁc cases of the two aforementioned situations, i.e.;
– The linear inverse problems where the solution is modeled as a piecewise contin-
uous function using a compound markov modeling (for example the intensity and
the line process in image reconstruction), where the non convexity of the criterion
is due to the markovian priors; and
– A special non-linear inverse problem which arises in inverse scattering and diﬀrac-
tion tomography imaging applications where the non convexity of the criterion is
due to the likelihood part.

The paper is organized as follows: The next section presents the main idea of
the GNC principle. Sections 3 and 4 will consider the two aforementioned speciﬁc
cases and, ﬁnally, some simulation results will illustrate the performances of the
proposed method in two special applications.

NEW ADVANCES IN BAYESIAN CALCULATION. . .

3

2. Graduated Non Convexity scheme

The principle of this algorithm is very simple. It consists of approximating the
non convex criterion J(x) with a sequence of continuously derivable criteria Jck (x)
such that:
• the ﬁrst one Jc0(x) be convex;
• the ﬁnal one (the limit) Jck (x) converges to J(x):

lim
k7→∞

Jck (x) = J(x),

∀x,

where ck > 0 are increasing relaxation parameters; and then
• for each k, a relaxed solution is calculated by minimizing locally, initialized by
the previous solution, as follows:

xck = arg

min

{Jck (x)}

x∈V (

xck−1 )

b

(5)

xck converges to
Fig. 2. illustrates such a scheme. The hope is that the sequence
the global minimizer of J(x). Note that there is no theoretical ground for this
hope, however, in many practical applications it seems to be realist.

b

b

Final step
multimodale Criterion

Intermediate step

global minimum

Initial step
convex criterion

initial minimum

Figure 1: GNC scheme.

4

A. Mohammad–Djafari et al.

3. Linear inverse problems with a piecewise Gaussian prior

Let ﬁrst consider a very simple noise ﬁltering problem:

y = x + b,

where we know that x represents the samples of a piecewise continuous function
x(t). Blake and Zisserman in [4] proposed to estimate x by searching the global
minimum of the following criterion:

with

and

J(x) = ky − xk2 + Ω(x)

Ω(x) =

φ(tj),

tj = xj − xj−1

Xj

φ(t) =

(λt)2
α = (λT )2

(cid:26)

if
if

|t| < T
|t| > T

✻

φ(t)

α = (λT )2

−T

T

✲
t

Figure 2: Truncated quadratic function.

Note that, for φ(t) = t2 the criterion J(x) can be considered either as the
MAP criterion with Gaussian prior or as the Tikhonov regularization one. The
choice (9) for φ(t) is done to preserve the discontinuities in x(t). With this choice,
obviously, J(x) is multimodal. The GNC idea was then to construct:

Jc(x) = ky − xk2 + Ωc(x)

Ωc(x) =

φc(tj),

Xj

(λt)2
α − 1
α = (λT )2

2 c(|t| − rc)2

if
if
if

|t| < qc
qc > |t| > rc
|t| > rc

,

qc = T (1 + 2λ2/c)−1/2
rc = T (1 + 2λ2/c)1/2

(cid:26)

with

and

φc(t) = 




(6)

(7)

(8)

(9)

(10)

(11)

(12)

NEW ADVANCES IN BAYESIAN CALCULATION. . .

and ﬁnd a c0 such that Jc for c < c0 be convex and then for a given sequence of
relaxations parameters {c0, c1, . . . , ck} do:

xck = arg

min

{Jck (x)}

x∈V (

xck−1 )

b

b
✻

φ(t)

α = (λT )2

−rc

−T

−qc

qc

T

rc

✲
t

Figure 3: Truncated quadratic function and it’s relaxation scheme.

Blake and Zisserman in [4] showed the existence of c0 such that Jc0 (x) be
convex and global convergence of this algorithm. However, for the case of inverse
problems y = Ax + b where A is singular or ill-conditioned, the existence of c0 is
no more insured. Nikolova et al. [6, 7] extended this work by proposing a doubly
relaxed criterion:

with

Jc(x) = ky − xk2 + Ωa,c(x)

Ωa,c(x) =

φc(tj) + at2
j

Xj (cid:2)

(cid:3)

and the following double relaxation scheme:

for ﬁxed c = c0

for ﬁxed a = 0

and for a = a0, . . . , 0 do:
xak = arg

min
xak−1 )

x∈V (

{Jak,c0(x)}

b
and for c = c0, . . . , ∞ do:
xck = arg

b
min

{Jck,0(x)}

x∈V (

xck−1 )

b

The initial convexity of the criterion is insured for a0 > c0
sions and more extensions, specially for 2D case, are given in [7, 8, 9].
3.1. Link with compound Markov models

b

2 . Many details, discus-

Compound Markov modeling in image processing became popular after the
works of Geman & Geman [1] and Besag [10, 11]. To see the link with these model
brieﬂy, let consider a 1-D case. When x(t) is assumed piecewise continuous or
piecewise Gaussian, it can be modeled with a set of coupled variables (x, l), where

5

(13)

(14)

(15)

(16)

(17)

A. Mohammad–Djafari et al.

6

where

l is a vector of binary-valued variables and x is Gaussian. Now, consider the MAP
estimate of (x, l):

x,
(

l) = arg min
(x,l)

b

b

{J(x, l) = − log p(x, l|y)}

(18)

− log p(x, l|y) = − log p(y|x) − log p(x|l) − log p(l) + cte,

(19)

with the following prior laws:

− log p(y|x) = ky − Axk2

− log p(l) = α

lj

Xj

Xj

− log p(x|l) =

φ(tj)(1 − lj),

tj = xj − xj−1, φ(t) = (λt)2

we obtain:

with

x,
(

l) = arg min
(x,l)

{J(x, l)}

b

b

J(x, l) = ky − Axk2 +

λ2(xj − xj−1)2(1 − lj) + αlj

.

(21)

Xj (cid:2)

(cid:3)

Note that the line variables lj are assumed non-interacting (mutually independent).
l) obtained by (20)
With this hypothesis, it is easy to show that the solution (
is equivalent to the solution obtained by

x,

{J(x)} with J(x) = ky − Axk2 +

x = arg min
x

b

with φ(t) deﬁned by (9) and

b

b
φ(xj − xj−1)2

(22)

Xj

(20)

(23)

1 if
0 if

lj =

(cid:26)

b

|
xj −
xj −
|
b
b

xj−1| > T
xj−1| < T
b
b

4. A non linear inverse problem

To show how the GNC principle can be used for nonlinear inverse problems we
consider the case of inverse scattering and more speciﬁcally the diﬀraction tomog-
raphy. To be short in presentation of the application, we give here an abstract
presentation of the problem. For more details on derivation of the inverse scat-
tering and diﬀraction tomography application see [12, 13, 14]. To summarize,
considering the geometry of Fig. 4., we have the following relations:

y(ri) =

Gm(ri, r′)φ(r′)x(r′) dr′,

ri ∈ S,

ZZD

φ(r) = φ0(r) +

Go(r, r′)φ(r′)x(r′) dr′,

r ∈ D.

ZZD

(24)

(25)

NEW ADVANCES IN BAYESIAN CALCULATION. . .

The discretized version of these two equations can be written with the following
compact notations:

y = GmXφ + b,
φ = φ0 + GoXφ,

where φ0 is the incident ﬁeld, Gm, Go are matrices related to the Green functions,
X is a diagonal matrix with the components of the vector x (a n length vector) as
its diagonal elements and y, φ are respectively m and n length vectors representing
the measured data (scattered ﬁeld) and the total ﬁeld on the object. Note that n
may be greater than m.

7

(26)

(27)

✲

φ0

D
(φ, x)

S
(y)

Figure 4: Diﬀraction tomography geometry conﬁguration

These two equations can be combined to obtain a symbolic explicit relation

between the data y and the unknowns x:

y = GmX (I − GoX)

−1 φ0 + b = A(x) + b,

(28)

where the considered matrix is assumed to be invertible. Now, the inverse problem
we are faced is to ﬁnd x given y. Note that, φ0, Go and Gm are known and the
relation between x and y is non linear. In fact, given φ, y is linear in x (26), but
φ depends on x through the second equation (27).

Here also, using the Bayesian approach, the MAP estimate is deﬁned as the

minimizer of

J(x) = ky − A(x)k2 + Ω(x)
and, even when Ω(x) is chosen to be convex, J(x) may not due to the fact that
ky − A(x)k2 is no more quadratic in x. Carfantan et al. in [12, 13, 14] proposed
to use the GNC idea in this case.

(29)

To introduce the GNC technique, they considered the following relaxation se-

quence:

Ack (x) = GmX(I − ckGoX)−1φ0,

(30)

with c0 = 0, and limk→∞ ck = 1.

Note that the ﬁrst term (c0 = 0) corresponds to a linearized model for the
problem named the Born approximation which consists in neglecting partially the
diﬀraction eﬀects. This results to the following convex criterion:

J0(x) = ky − GmXφ0k2 + Ω(x).

8

A. Mohammad–Djafari et al.

Note also that for ck = 1 the criterion J1(x) = J(x). The main practical problem
is then the choice of sequences {ck = 0, · · · , 1} which is done by experiment.

5. Some simulation results and applications
5.1. 1-D noise filtering

Fig. 5.1. shows an example of results obtained in noise ﬁltering. In this ﬁgure
we see the original signal, noisy data, restoration using a Gaussian model (φ(t)
quadratic) and restoration obtained by GNC when φ(t) is chosen to be truncated
quadratic.

Original & Data

Original & GNC reconstruction

4.5

3.5

2.5

4

3

2

1

1.5

0.5

0

4.5

3.5

2.5

4

3

2

1

1.5

0.5

0

20

40

80

100

120

20

40

80

100

120

60

a

60

b

Figure 5: Noise ﬁltering:
a) Original and data, b) Original, Gaussian restoration and GNC restoration

5.2. 1-D signal deconvolution

Fig. 5.2. shows an example of results in signal deconvolution. In this ﬁgure
we see the original signal, noisy data, restoration using a Gaussian model and
restoration obtained by GNC.

5.3.

Image restoration

Fig. 5.3. shows an example of results in Image restoration. In this ﬁgure a)
is the original image, b) is the blurred and noisy data, c) is the restoration using
a Gaussian model and d) is the restoration obtained by GNC with truncated
quadratic regularization.

NEW ADVANCES IN BAYESIAN CALCULATION. . .

9

Original & Data

Original & GNC reconstruction

4.5

3.5

4

3

2

2.5

1.5

1

0

4.5

3.5

2.5

4

3

2

1

1.5

0.5

0

20

40

80

100

120

20

40

80

100

120

60

a

60

b

Figure 6: Deconvolution:
a) Original and data, b) Original, Gaussian restoration and GNC restoration

5.4.

Image reconstruction in X-ray tomography

Fig. 5.4. shows results obtained in X-ray tomography image reconstruction. In
this ﬁgure a) shows the original image, b) shows the projections (data), c) shows
the backprojection reconstruction, d) shows a reconstruction using a Gaussian
prior, e) shows a reconstruction using a Gamma prior, and f) shows a reconstruc-
tion using GNC with truncated quadratic regularization.

5.5.

Inverse scattering and diffraction tomography

Fig. 5.5. shows an example of results in non linear diﬀraction tomography image
reconstruction. In this ﬁgure a) is the original image, b) is the measured scattered
ﬁeld data, d) is a reconstruction using the linear Born approximation, and e) is a
reconstruction using GNC.

6. Conclusions

The Bayesian maximum a posteriori (MAP) estimates requires the minimization
of a compound criterion which, in general, has two parts: a data ﬁtting part and
a prior part. In many situations in inverse problems the criterion to be minimized
is multimodal. The cost of the Simulated Annealing (SA) based techniques is in
general huge for these problems.

We reported here recently proposed new techniques, based on Graduated Non
Convexity (GNC), to overcome this diﬃculty and showed two speciﬁc implemen-
tations of this technique for:
– The linear inverse problems such as: noise ﬁltering, deconvolution, image restora-
tion and tomographic image reconstruction, where the solution is modeled as a
piecewise continuous function and where, the non convexity of the criterion is due
to the special choice of the prior; and
– A nonlinear inverse inverse scattering and diﬀraction tomography where the non
convexity of the criterion is due to the likelihood part.

10

References

A. Mohammad–Djafari et al.

1. S. Geman and D. Geman, “Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images,” IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, vol. PAMI-6, pp. 721–741, Nov. 1984.

2. L. Youn`es, “Estimation and annealing for Gibbsian ﬁelds,” Annales de l’institut

Henri Poincar´e, vol. 24, pp. 269–294, Feb. 1988.

3. F. Jeng and J. Woods, “Simulated annealing in compound Gaussian random ﬁelds,”

IEEE Transactions on Information Theory, vol. IT-36, pp. 94–107, Jan. 1990.

4. A. Blake and A. Zisserman, Visual reconstruction. Cambridge: The MIT Press,

1987.

5. A. Blake, “Comparison of the eﬃciency of deterministic and stochastic algorithms
for visual reconstruction,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. PAMI-11, pp. 2–12, January 1989.

6. M. Nikolova, A. Mohammad-Djafari, and J. Idier, “Inversion of large-support ill-
conditioned linear operators using a Markov model with a line process,” in ICASSP,
vol. V, (Adelaide, Australia), pp. 357–360, 1994.

7. M. Nikolova and A. Mohammad-Djafari, “Discontinuity reconstruction from linear
attenuating operators using the weak-string model,” in Proceedings of European Sig-
nal Processing. Conf., vol. 2, pp. 1062–1066, 1994.

8. M. Nikolova, Inversion markovienne de probl`emes lin´eaires mal pos´es. application `a
l’imagerie tomographique. PhD thesis, Universit´e de Paris-Sud, Orsay, Feb. 1995.
9. M. Nikolova, J. Idier, and A. Mohammad-Djafari, “Inversion of large-support ill-
posed linear operators using a piecewise Gaussian mrf,” tech. rep., gpi–lss, sub-
mitted to IEEE Transactions on Image Processing, Gif-sur-Yvette, France, 1995.
10. J. E. Besag, “Spatial interaction and the statistical analysis of lattice systems (with
discussion),” Journal of the Royal Statistical Society B, vol. 36, no. 2, pp. 192–236,
1974.

11. J. E. Besag, “On the statistical analysis of dirty pictures (with discussion),” Journal

of the Royal Statistical Society B, vol. 48, no. 3, pp. 259–302, 1986.

12. H. Carfantan and A. Mohammad-Djafari, “A Bayesian approach for nonlinear in-
verse scattering tomographic imaging,” in ICASSP, vol. IV, (Detroit, U.S.A.),
pp. 2311–2314, May 1995. HC.

13. H. Carfantan and A. Mohammad-Djafari, “Approche bay´esienne et algorithme mul-
tir´esolution pour un probl`eme inverse non lin´eaire en tomographie de diﬀraction,” in
Actes du 15e Colloque GRETSI, vol. 2, (Juan-les-pins, France), pp. 849–852, Sept.
1995.

14. H. Carfantan and A. Mohammad-Djafari, “Beyond the Born approximation in in-
verse scattering with a Bayesian approach,” in 2nd International Conference on
Inverse Problems in Engineering, (Le Croisic, France), June 1996.

NEW ADVANCES IN BAYESIAN CALCULATION. . .

11

10

20

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

30
a

c

10

20

30

40

50

60

10

20

30

40

50

60

b

d

10

20

30

40

50

60

10

20

30

40

50

60

Figure 7: Image restoration:
a) original, b) data, c) Gaussian restoration, d) GNC restoration

12

10

20

30

40

50

60

80

60

40

20

0

−20

−40

−60

−80

A. Mohammad–Djafari et al.

Original

Backprojection

ME: log x

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

c

RegQ1

e

GNC: Trun. Quad

10

20

30

40

50

60

10

20

30

40

50

60

a

Projections

y

f(x,y)

x

b

a

d

b

10

20

30

40

50

60

10

20

30

40

50

60

f

c

−80

−60

−40

−20

0

20

40

60

80

10

20

30

40

50

60

10

20

30

40

50

60

Image reconstruction in X-ray tomography:

Figure 8:
a) original, b) projections (data), c) Backprojection, d) Gaussian reconstruction,
d) Gamma prior reconstruction, and e) GNC reconstruction

Figure 9: Diﬀraction tomography image reconstruction:
a) original, b) linear Born approximation reconstruction, c) GNC based recon-
struction


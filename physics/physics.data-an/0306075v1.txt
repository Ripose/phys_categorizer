3
0
0
2
 
n
u
J
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
7
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Combined ﬁrst–principles calculation and neural–network correction approach as a
powerful tool in computational physics and chemistry

LiHong Hu, XiuJun Wang, LaiHo Wong and GuanHua Chen∗
Department of Chemistry, The University of Hong Kong, Hong Kong, China
(Dated: January 12, 2014; submitted to Phys. Rev. Lett.)

Despite of their success, the results of ﬁrst-principles quantum mechanical calculations contain
inherent numerical errors caused by various approximations. We propose here a neural-network
algorithm to greatly reduce these inherent errors. As a demonstration, this combined quantum me-
chanical calculation and neural-network correction approach is applied to the evaluation of standard
and standard Gibbs energy of formation ∆fG(cid:10)
heat of formation ∆fH (cid:10)
for 180 organic molecules
at 298 K. A dramatic reduction of numerical errors is clearly shown with systematic deviations
being eliminated. For examples, the root–mean–square deviation of the calculated ∆fH (cid:10)
(∆fG(cid:10)
)
−1 for B3LYP/6-
for the 180 molecules is reduced from 21.4 (22.3) kcal·mol
311+G(d,p) and from 12.0 (12.9) kcal·mol−1 to 3.3 (3.4) kcal·mol−1 for B3LYP/6-311+G(3df,2p)
before and after the neural-network correction.

−1 to 3.1 (3.3) kcal·mol

PACS numbers: 31.15.Ew, 31.30.-i, 31,15,-p, 31.15.Ar

One of the Holy Grails of computational science is
to quantitatively predict properties of matters prior to
experiments. Despite the facts that the ﬁrst-principles
quantum mechanical calculation [1, 2] has become an in-
dispensable research tool and experimentalists have been
increasingly relying on computational results to inter-
pret their experimental ﬁndings, the practically used nu-
merical methods by far are often not accurate enough,
in particular, for complex systems. This limitation is
caused by the inherent approximations adopted in the
ﬁrst-principles methods. Because of computational cost,
electron correlation has always been a diﬃcult obstacle
for ﬁrst-principles calculations. Finite basis sets cho-
sen in practical computations are not able to cover en-
tire physical space and this inadequacy introduces fur-
ther inherent computational errors. Eﬀective core po-
tential is frequently used to approximate the relativistic
eﬀects, resulting inevitably in errors for systems that con-
tain heavy atoms. The accuracy of a density-functional
theory (DFT) calculation is mainly determined by the
exchange-correlation (XC) functional being employed [1],
whose exact form is however unknown. Nevertheless, the
results of ﬁrst-principles quantum mechanical calculation
can capture the essence of physics. For instance, the cal-
culated results, despite that their absolute values may
poorly agree with measurements, are usually of the same
tendency among diﬀerent molecules as their experimen-
tal counterpart. The quantitative discrepancy between
the calculated and experimental results depends predom-
inantly on the property of primary interest and, to a less
extent, also on other related properties, of the material.
There exists thus a sort of quantitative relation between
the calculated and experimental results, as the aforemen-
tioned approximations, to a large extent, contribute to

∗Electronic
URL: http://yangtze.hku.hk

address:

ghc@everest.hku.hk;

the systematic errors of speciﬁed ﬁrst-principles methods.
Can we develop general ways to eliminate the systematic
computational errors and further to quantify the accu-
racies of numerical methods used? It has been proven
an extremely diﬃcult task to determine the calculation
errors from the ﬁrst-principles. Alternatives must be
sought.

We propose here a neural–network algorithm to deter-
mine the quantitative relationship between the experi-
mental data and the ﬁrst-principles calculation results.
The determined relation will subsequently be used to
eliminate the systematic deviations of the calculated re-
sults, and thus, reduce the numerical uncertainties. Since
its beginning in the late ﬁfties, Neural Networks has been
applied to various engineering problems, such as robotics,
pattern recognition, speech, and etc. [3, 4] As the ﬁrst
application of Neural Networks to quantum mechanical
calculations of molecules, we choose the standard heat of
formation ∆fH (cid:10) and standard Gibbs energy of formation
∆fG(cid:10) at 298.15 K as the properties of interest.

A total of 180 small- or medium-sized organic
molecules, whose ∆fH (cid:10) and ∆fG(cid:10) values are well doc-
umented in Refs. 5, 6, 7, are selected to test our pro-
posed approach. The tabulated values of ∆fH (cid:10) and ∆fG(cid:10)
in the three references diﬀer less than 1.0 kcal·mol−1
for same molecule. The uncertainties of all ∆fH (cid:10) val-
ues are less than 1.0 kcal·mol−1, while those of ∆fG(cid:10)s
are not reported in Refs. 5, 6, 7. These selected
molecules contain elements such as H, C, N, O, F, Si,
S, Cl and Br. The heaviest molecule contains 14 heavy
atoms, and the largest has 32 atoms. We divide these
molecules randomly into the training set (150 molecules)
and the testing set (30 molecules). The geometries of 180
molecules are optimized via B3LYP/6-311+G(d,p) [8]
calculations and the zero point energies (ZPEs) are cal-
culated at the same level. The enthalpy and Gibbs en-
ergy of each molecule are calculated at both B3LYP/6-
311+G(d,p) and B3LYP/6-311+G(3df,2p). [8] B3LYP/6-
311+G(3df,2p) employs a larger basis set than B3LYP/6-

311+G(d,p). The unscaled B3LYP/6-311+G(d,p) ZPE
is employed in the ∆fH (cid:10) and ∆fG(cid:10) calculations. The
strategies in reference 9 are adopted to calculate ∆fH (cid:10)
and ∆fG(cid:10). The calculated ∆fH (cid:10) and ∆fG(cid:10) for B3LYP/6-
311+G(d,p) and B3LYP/6-311+G(3df,2p) are compared
to their experimental counterparts in Figs. 1 and 2, re-
spectively. The horizontal coordinates are the raw cal-
culated data, and the vertical coordinates are the exper-
imental values. The dashed lines are where the verti-
cal and horizontal coordinates are equal, i.e., where the
B3LYP calculations and experiments would have the per-
fect match. The raw calculation values are mostly below
the dashed line, i.e., most raw ∆fH (cid:10) and ∆fG(cid:10) are larger
than the experimental data.
In another word, there
are systematic deviations for both B3LYP ∆fH (cid:10) and
∆fG(cid:10). Compared to the experimental measurements, the
root–mean–square (RMS) deviations for ∆fH (cid:10) (∆fG(cid:10))
are 21.4 (22.3) and 12.0 (12.9) kcal·mol−1 for B3LYP/6-
311+G(d,p) and B3LYP/6-311+G(3df,2p) calculations,
In Table I we compare the B3LYP and
respectively.
experimental ∆fH (cid:10)s for 10 of 180 molecules. Overall,
B3LYP/6-311+G(3df,2p) calculations yield better agree-
ments with the experiments than B3LYP/6-311+G(d,p).
In particular, for small molecules with few heavy el-
ements B3LYP/6-311+G(3df,2p) calculations result in
very small deviations from the experiments. For in-
stance, the ∆fH (cid:10) deviations for CH4 and CS2 are only
-0.5 and 0.6 kcal·mol−1, respectively. Our B3LYP/6-
311+G(3df,2p) calculation results are also in good agree-
ments with those of reference 9 which employed a similar
calculation strategy except that their ZPEs were scaled
by a factor of 0.98 or 0.96 and their geometries were op-
timized at B3LYP/6-31+G(d). For large molecules, both
B3LYP/6-311+G(d,p) and B3LYP/6-311+G(3df,2p) cal-
culations yield quite large deviations from their experi-
mental counterparts.

Our neural network adopts a three-layer architecture
which has an input layer consisted of input from the phys-
ical descriptors and a bias, a hidden layer containing a
number of hidden neurons, and an output layer that out-
puts the corrected values for ∆fH (cid:10) or ∆fG(cid:10) (see Fig. 3).
The number of hidden neurons is to be determined. The
most important issue is to select the proper physical de-
scriptors of our molecules, which are to be used as the
input for our neural network. The calculated ∆fH (cid:10) and
∆fG(cid:10) contain the essence of exact ∆fH (cid:10) and ∆fG(cid:10), re-
spectively, and are thus obvious choices of the primary
descriptor for correcting ∆fH (cid:10) and ∆fG(cid:10), respectively.
We observe that the size of a molecule aﬀects the accu-
racies of calculations. The more atoms a molecule has,
the worse the calculated ∆fH (cid:10) and ∆fG(cid:10) are. This is
consistent with the general observations in the ﬁeld. [9]
The total number of atoms Nt in a molecule is thus cho-
sen as the second descriptor for the molecule. ZPE is
an important parameter in calculating ∆fH (cid:10) and ∆fG(cid:10).
Its calculated value is often scaled in evaluating ∆fH (cid:10)
and ∆fG(cid:10), [9] and it is thus taken as the third physical
descriptor. Finally, the number of double bonds, Ndb, is

2

TABLE I: Experimental and calculated ∆fH (cid:10)(298 K) for ten
selected compounds (all data are in the units of kcal·mol−1)

Deviations (Theory-Expt.)

Molecules
CF2O
CH2Cl2
CH2F2
CH4
CS2
C5H12
C5H12O
C6H14
C8H10
C9H12

Expt.a DFT1b DFT1-NNc DFT2d DFT2-NNe DFT3f
9.1
4.6
0.0
-1.6
0.2
–
–
–
–
–

-152.9±0.4
-22.8±0.3
-108.1±0.2
-17.8±0.1
27.9±0.2
-35.1±0.2
-75.3±0.3
-41.1 ±0.2
4.6 ±0.3
-2.3 ±0.3

8.7
5.0
0.6
-0.5
0.6
9.9
14.0
17.0
13.3
17.6

20.0
10.6
8.0
1.1
8.7
16.7
23.2
25.1
25.7
31.3

6.9
3.6
0.9
1.1
3.3
-2.1
0.2
1.4
0.5
0.9

6.8
4.9
0.6
1.0
3.2
-2.2
0.1
1.5
1.0
1.8

aThe experimental values were taken from reference [7].
bThe deviations of calculated ∆fH (cid:10) by using B3LPY/6-
311+G(d,p) geometries, zero point energies and

enthalpies.
cThe deviations of calculated ∆fH (cid:10) by B3LYP/6-311+G(d,p)-
Neural Networks approach.
dThe deviations of calculated ∆fH (cid:10) by using the 6-311+G(d,p)
geometries and zero point energies, and

the calculated enthalpies with 6-311+G(3df,2p) basis.
eThe deviations of calculated ∆fH (cid:10) by B3LYP/6-311+G(3df,2p)-
Neural Networks approach.

fThe deviations were taken from [9], where the zero point energies

were corrected by a scale factor.

selected as the fourth and last descriptor to reﬂect the
chemical structure of the molecule.

To ensure the quality of our neural network, a cross-
validation procedure is employed to determine our neural
network. [10] We divide further randomly 150 training
molecules into ﬁve subsets of equal size. Four of them
are used to train the neural network, and the ﬁfth to val-
idate its predictions. This procedure is repeated 5 times
in rotation. The number of neurons in the hidden layer is
varied from 2 to 10 to decide the optimal structure of our
neural network. We ﬁnd that the hidden layer contain-
ing two neurons yields best overall results. Therefore, the
5-2-1 structure is adopted for our neural network as de-
picted in Fig. 3. The input values at the input layer, x1,
x2, x3, x4 and x5, are scaled ∆fH (cid:10) (or ∆fG(cid:10)), Nt, ZPE,
Ndb and bias, respectively. The bias x5 is set to 1. The
weights {W xij}s connect the input layer {xi} and the
hidden neurons y1 and y2, and {W yj}s connect the hid-
den neurons and the output Z which is the scaled ∆fH (cid:10)
or ∆fG(cid:10) upon neural-network correction. The output Z
is related to the input {xi} as

Z = X
j=1,2

W yj Sig( X
i=1,5

W xij xi),

(1)

1

where Sig(v) =
1+exp(−αv) and α is a parameter
that controls the switch steepness of Sigmoidal function
Sig(v). An error back-propagation learning procedure [4]
is used to optimize the values of W xij and W yj(i =
1, 2, 3, 4, 5 and j = 1, 2). In Figs. 1c, 1d, 2c and 2d, the
triangles belong to the training set and the crosses to the
testing set. Compared to the raw calculated results, the
neural-network corrected values are much closer to the

-150

 0

 20
 40
Deviation

 0

 60

 0

 10  20  30
Deviation

(a) B3LYP/6-311+G(d,p)

Training set
Testing set

(b) B3LYP/6-311+G(3df,2p)
Training set
Testing set

3

 30

 25

 20

 15

 10

 5

 0

 45

 30

 15

 0

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

 20

 15

 10

 5

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

 45

 30

 15

 0

1
-

l

o
m

⋅
l

a
c
k
 
/
 
)

K
8
9
2
(

o
H
∆
 
l
a
t
n
e
m

i
r
e
p
x
E

 100

f

 100

 50

 0

-50

-100

-200

-250

 50

 0

-50

-100

-150

-200

-250

-5

 0
Deviation

 5

-5

 0
Deviation

 5

(c)
Training set
Testing set

(d)
Training set
Testing set

-250 -200 -150 -100

-50

 0
 50
Calculated ∆

-250 -200 -150 -100

 100
fHo(298K) / kcal⋅mol-1

-50

 0

 50

 100

FIG. 1: Experimental ∆fH (cid:10) versus calculated ∆fH (cid:10) for all 180 compounds. (a) and (b) are for raw B3LYP/6-311+G(d,p)
and B3LYP/6-311+G(3df,2p) results, respectively. (c) and (d) are for neural-network corrected B3LYP/6-311+G(d,p) and
B3LYP/6-311+G(3df,2p) ∆fH (cid:10)
s, respectively. In (c) and (d), triangles are for the training set and crosses for the testing set.
Inserts are the histograms for the diﬀerences between the experimental and calculated ∆fH (cid:10)
s. All values are in the units of
kcal·mol

−1.

experimental values for both training and testing sets.
More importantly, the systematic deviations for ∆fH (cid:10)
and ∆fG(cid:10) in Figs. 1a, 1b, 2a and 2b are eliminated, and
the resulting numerical deviations are reduced substan-
tially. This can be further demonstrated by the error
analysis performed for the raw and neural-network cor-
rected ∆fH (cid:10)s and ∆fG(cid:10)s of all 180 molecules. In the in-
serts of Figs. 1 and 2, we plot the histograms for the devi-
ations (from the experiments) of the raw B3LYP ∆fH (cid:10)s
and ∆fG(cid:10)s and their neural–network corrected values.
Obviously, the raw calculated ∆fH (cid:10)s and ∆fG(cid:10)s have
large systematic deviations while the neural–network cor-
rected ∆fH (cid:10)s and ∆fG(cid:10)s have virtually no systematic de-
viations. Moreover, the remaining numerical deviations
are much smaller. Upon the neural-network corrections,
the RMS deviations of ∆fH (cid:10)s (∆fG(cid:10)s) are reduced from
21.4 (22.3) kcal·mol−1 to 3.1 (3.3) kcal·mol−1 and 12.0
(12.9) kcal·mol−1 to 3.3 (3.4) kcal·mol−1 for B3LYP/6-
311+G(d,p) and B3LYP/6-311+G(3df,2p), respectively.
Note that the error distributions after the neural–
network correction are of approximate Gaussian distribu-
tions (see Figs. 2c and 2d). Although the raw B3LYP/6-
311+G(d,p) results have much larger deviations than
those of B3LYP/6-311+G(3df, 2p), the neural–network

corrected values of both calculations have deviations of
the same magnitude. This implies that it is suﬃcient to
employ the smaller basis set 6-311+G(d,p) in our com-
bined DFT calculation and neural–network correction (or
DFT-NEURON) approach. The neural–network algo-
rithm can correct easily the deﬁciency of a small basis
set. Therefore, the DFT-NEURON approach can po-
tentially be applied to much larger systems. In Table I
we also list the neural–network corrected ∆fH (cid:10)s of the
10 molecules. The deviations of large molecules are of
the same magnitude as those of small molecules. Unlike
other quantum mechanical calculations that usually yield
worse results for larger molecules than for small ones, the
DFT-NEURON approach does not discriminate against
the large molecules.

Analysis of our neural network reveals that the weights
connecting the input for ∆fH (cid:10) or ∆fG(cid:10) have the domi-
nant contribution in all cases. This conﬁrms our fun-
damental assumption that the calculated ∆fH (cid:10) (∆fG(cid:10))
captures the essential values of exact ∆fH (cid:10) (∆fG(cid:10)). The
input for the second physical descriptor, Nt, has quite
large weights in all cases. In particular, when the smaller
basis set 6-311+G(d,p) is adopted in the B3LYP calcu-
lations, Nt has the second largest weights.
It is found

4

 25

 20

 15

 10

 5

 0

 45

 30

 15

 0

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

 20

 15

 10

 5

 0

 45

 30

 15

 0

y
c
n
e
u
q
e
r
F

y
c
n
e
u
q
e
r
F

1
-

l

o
m

⋅
l

a
c
k
 
/
 
)

K
8
9
2
(

o
G
∆
 
l
a
t
n
e
m

i
r
e
p
x
E

 100

f

 100

 50

 0

-50

-100

-200

-250

 50

 0

-50

-100

-150

-200

-250

Wx11

Wx12

Wx21

Wx22

Wx31

Wx32

Wx41

Wx42

Wx51

Wx52

Nt

x2

ZPE

x3

Ndb

x4

Bias

x5

-150

 0

 20
 40
Deviation

 60

 0

 10  20  30
Deviation

(a) B3LYP/6-311+G(d,p)

Training set
Testing set

(b) B3LYP/6-311+G(3df,2p)
Training set
Testing set

-5

 0
Deviation

 5

-5

 0
Deviation

 5

(c)
Training set
Testing set

(d)
Training set
Testing set

-250 -200 -150 -100

-50

 0
 50
Calculated ∆

-250 -200 -150 -100

 100
fGo(298K) / kcal⋅mol-1

-50

 0

 50

 100

FIG. 2: Experimental ∆fG(cid:10) versus calculated ∆fG(cid:10) for all 180 compounds. (a) and (b) are for raw B3LYP/6-311+G(d,p)
and B3LYP/6-311+G(3df,2p) results, respectively. (c) and (d) are for neural-network corrected B3LYP/6-311+G(d,p) and
B3LYP/6-311+G(3df,2p) ∆fG(cid:10)

s, respectively. Legends, units and inserts are similar to those of Fig. 1.

Input layer

∆fH

0

/∆fG

0

x1

Hidden layer

Output layer

y1

Wy1

y2

Wy2

Z

FIG. 3: Structure of our neural network.

that the raw ∆fH (cid:10) and ∆fG(cid:10) deviations are roughly pro-
portional to Nt, which conﬁrms the importance of Nt
as a signiﬁcant descriptor of our neural network. The
bias contributes to the correction of systematic devia-
tions in the raw calculated data, and has thus signiﬁcant
weights. When the larger basis set 6-311+G(3df,2p) is

used, the bias has the second largest weights for all cases.
ZPE has been often scaled to account for the discrepan-
cies of ∆fH (cid:10)s or ∆fG(cid:10)s between calculations and experi-
ments, [9] and it is thus expected to have large weights.
This is indeed the case, especially when the smaller basis
set 6-311+G(d,p) is adopted in calculations. In all cases
the number of double bonds, Ndb, has the smallest but
non-negligible weights. In Table II we list the values of
{W xij} and {W yj} of the two neural networks for cor-
recting ∆fG(cid:10)s of B3LYP/6-311+G(d,p) and B3LYP/6-
311+G(3df,2p) calculations.

Our DFT-NEURON approach has a RMS deviation of
∼3 kcal·mol−1 for the 180 small- to medium-sized organic
molecules. This is slightly larger than their experimental
uncertainties. [5, 6, 7] The physical descriptors adopted
in our neural network, the raw calculated ∆fH (cid:10) or ∆fG(cid:10),
the number of atoms Nt, the number of double bonds
Ndb and the ZPE are quite general, and are not limited
to special properties of organic molecules. The DFT-
NEURON approach developed here is expected to yield
a RMS deviation of ∼3 kcal·mol−1 for ∆fH (cid:10)s and ∆fG(cid:10)s
of any small- to medium-sized organic molecules. G2
method [9] results are more accurate for small molecules.
However, our approach is much more eﬃcient and can be
applied to much larger systems. To improve the accu-

TABLE II: Weights of DFT-Neural Networks for ∆fG(cid:10)

Weights
Wx1j
Wx2j
Wx3j
Wx4j
Wx5j
Wyj

DFT1-NNa
y1
0.78
-0.60
0.44
0.07
-0.42
1.48

y2
-0.72
0.02
0.02
0.24
-0.04
-0.57

DFT2-NNb
y1
0.83
-0.30
0.18
0.05
-0.46
1.44

y2
-0.73
0.02
0.02
0.17
0.01
-0.47

aDFT1-NN refers B3LYP/6-311+G(d,p)-Neural Networks ap-
proach.
bDFT2-NN refers B3LYP/6-311+G(3df,2p)-Neural Networks ap-
proach.

racy of the DFT-NEURON approach, we need more and
better experimental data, and possibly, more and bet-
ter physical descriptors for the molecules. Besides ∆fH (cid:10)
and ∆fG(cid:10), the DFT-NEURON approach can be gener-
alized to calculate other properties such as ionization
energy, dissociation energy, absorption frequency, band
gap and etc. The raw ﬁrst-principles calculation prop-
erty of interest contains its essential value, and is thus
always the primary descriptor. Since the raw calcula-
tion error accumulates as the molecular size increases,
the number of atoms Nt should thus be selected as a de-
scriptor for any DFT-NEURON calculations. Additional
physical descriptors should be chosen according to their
relations to the property of interest and to the physical
and chemical structures of the compounds. Others have
used Neural Networks to determine the quantitative rela-
tionship between the experimental thermodynamic prop-

5

erties and the structure parameters of the molecules. [10]
We distinct our work from others by utilizing speciﬁ-
cally the ﬁrst-principles methods and with the objective
to improve quantum mechanical results. Since the ﬁrst-
principles calculations capture readily the essences of the
properties of interest, our approach is more reliable and
covers much a wider range of molecules or compounds.

To summarize, we have developed a promising new
approach to improve the results of ﬁrst-principles quan-
tum mechanical calculations and to calibrate their un-
certainties. The accuracy of DFT-NEURON approach
can be systematically improved as more and better ex-
perimental data are available. As the systematic de-
viations caused by small basis sets and less sophisti-
cated methods adopted in the calculations can be eas-
ily corrected by Neural Networks, the requirements on
ﬁrst-principles calculations are modest. Our approach
is thus highly eﬃcient compared to much more sophisti-
cated ﬁrst-principles methods of similar accuracy, and
is expected to be applied to much
more importantly,
larger systems. The combined ﬁrst-principles calcula-
tion and neural-network correction approach developed
in this work is potentially a powerful tool in computa-
tional physics and chemistry, and may open the possibil-
ity for ﬁrst-principles methods to be employed practically
as predictive tools in materials research and design.

We thank Prof. YiJing Yan for extensive discussion on
the subject and generous help in manuscript preparation.
Support from the Hong Kong Research Grant Council
(RGC) and the Committee for Research and Conference
Grants (CRCG) of the University of Hong Kong is grate-
fully acknowledged.

[1] R. G. Parr and W. Yang, Density-Functional The-
ory of Atoms and Molecules (Oxford University Press,
New York, 1989), and references therein.

[2] H. F. Schaefer III, Methods of electronic structure the-
ory (Plenum press, New York and London, 1977), and
references therein.

[3] B. D. Ripley, Pattern recognition and neural networks,

[6] D. R. Lide, CRC Handbook of Chemistry and Physics
3rd Electronic ed (CRC Press, Boca Raton, FL 2000).
[7] J. B. Pedley, R. D. Naylor, and S. P. Kirby, Thermo-
chemical data of organic compounds 2nd ed (Chapman
and Hall, New York, 1986).

[8] M. J. Frisch et al. Gaussian 98, Revision A.11.3 Gaussian,

Inc., Pittsburgh PA, 2002.

(New York : Cambridge University Press, 1996).

[9] L. A. Curtiss, K. Raghavachari, P. C. Redfern, J. A.

[4] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Nature,

Pople, J. Chem. Phys., 106, 1063 (1997).

[5] C. L. Yaws, Chemical properties Handbook (McGraw-Hill,

[10] X. Yao, X. Zhang, R. Zhang, M. Liu, Z. Hu, B. Fan,

Computers & Chemistry, 25, 475 (2001).

323, 533(1986).

New York, 1999).


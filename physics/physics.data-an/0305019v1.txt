3
0
0
2
 
y
a
M
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
1
0
5
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Fitting a Sum of Exponentials to Numerical Data

Bernhard Kaufmann∗

Abstract

A ﬁnite sum of exponential functions may be expressed by a linear
combination of powers of the independent variable and by successive in-
tegrals of the sum. This is proved for the general case and the connection
between the parameters in the sum and the coeﬃcients in the linear com-
bination is highlighted. The ﬁtting of exponential functions to a given
data- set is therefore reduced to a multilinear approximation procedure.
The results of this approximation do not only provide the necessary in-
formation to compute the factors in the exponents and the weights of the
exponential terms but also they are used to estimate the errors in the
factors.

1 Introduction

From time to time the need arises to ﬁt a sum of exponentials to numerical
data. That means to approximate a given data- set consisting of pairs of real
numbers (xj , yj) by the following expression:

y(x) = a0 +

aie−bix

N

Xi=1

(1)

where x, xj ∈ R+, yj ∈ R, N ∈ N+ and ai, bi are unknown real numbers which
have to be chosen so that the ﬁt becomes optimal.

If the bi were known, the task usually would be a well posed linear problem,
but if the bi are unknown too, it turns out to be ill conditioned. The hopelessness
of eﬀorts dealing with this kind of problem has been described drastically by
F.S. Acton [1] in a chapter entitled ”What not to compute”.

At ﬁrst sight, ﬁtting equation 1 to a given data- set inevitably seems to be
a nonlinear problem. However it has been noted [2], [3] that equation 1 may
be expressed as a linear combination of powers of x and successive integrals of
y(x), reducing the problem to a multilinear ﬁtting procedure. This method is
based on the fact that y(x) can be shown to satisfy an ordinary linear diﬀerential
equation of N-th order with constant coeﬃcients. The roots of the characteristic

∗Metallurgical research department, voestalpine, 4031 Linz, Austria.

e-mail: bernhard.kaufmann@voestalpine.com

1

polynomial of this equation give the bi and the ai are identiﬁed as solutions to
linear equations involving the bi and the derivatives of y(x) at x = 0. However,
derivatives of experimental data- sets enhance the errors in the data, therefore
it is desirable to eliminate them. Actually, [3] shows already the way to do this,
but only for the case N = 2 and a0 = 0, the general case not really being obvious.
In the present paper, this method of linearizing the ﬁtting procedure is revived
and without referring to diﬀerential equations and without using derivatives,
the general case is derived. Additionally, a method for estimating the errors in
the exponential factors is presented. Of course, the problem remains ill posed,
but linear ﬁtting oﬀers computational advantages over nonlinear approximation
and also supplies estimates of the errors in the computed coeﬃcients, which may
be used to predict the errors in the exponential coeﬃcients.

2 Results

The functions used to construct a linear approximation problem are powers of x
and successive integrals of y(x). Before the announced relation can be asserted,
some deﬁnitions are required.

Deﬁnition: The k-th integral of y(x) is deﬁned recursively:

I0(x) = y(x)
x

Z

0

Ik(x) =

Ik−1(t)dt

k > 0

(2)

Deﬁnition: βN ij, αN ij. Given the set B = {b1, · · · , bN } of N exponential
factors bk in equation 1 we consider the products of i diﬀerent elements of B.
Each of these products corresponds to a combination of i elements out of B, the
number of these products therefore is

C(N, i) =

N !
(N − i)!i!

=

N
i (cid:19)

(cid:18)

(3)

as is proved in combinatorics. We assume that the products are ordered in some
way. βN ij then is the j-th of these products. Additionally, we deﬁne βN 01 = 1.
αN ij is the sum of all ak in equation 1 excluding those whose index is equal
N
k=0 ak. Obviously,

to that of one of the b’s in βN ij. By deﬁnition, αN 01 =
each αN ij contains at least a0.
Example: N=3

P

2

β301 = 1
β311 = b1
β321 = b1b2
β331 = b1b2b3

α301 = a0 + a1 + a2 + a3
α311 = a0 + a2 + a3
α321 = a0 + a3
α331 = a0

β312 = b2
β322 = b1b3

β313 = b3
β323 = b2b3

α312 = a0 + a1 + a3 α313 = a0 + a1 + a2
α322 = a0 + a2

α323 = a0 + a1

With these deﬁnitions, the central statement of this article now may be

asserted:

a0 +

aie−bix = −

Ii(x)

βN ij +

βN ijαN ij

(4)

N

C(N,i)

N

C(N,i)

xi
i!

Xi=1

Xj=1

Xi=0

Xj=1

N

Xi=1

Assuming the validity of equation 4 the task now consists in approximating
the data- set {(xj, yj)} by a linear combination of the 2N functions (I1, . . . , IN ,
x, . . . , xN ) plus a constant. By standard linear approximation techniques the
coeﬃcients (c1, . . . , cN , d1, . . . , dN ) and the intercept d0 may be determined
together with their errors (∆c1, . . . , ∆cN , ∆d1, . . . , ∆dN ) and ∆d0. It follows
that

ci = −

βN ij

i = 1, . . . , N

C(N,i)

Xj=1
C(N,i)

Xj=1

βN ij αN ij
i!

d0 = αN 01 =

ak

di =

i = 1, . . . , N

Given the ci in 5 Vieta’s root theorem asserts that the bi are the N roots of

the polynomial

P (x) = xN +

(−1)i+1cixN −i

Xi=1
As soon as the bi are known, the expressions 6 and 7 represent a system of N+1
linear equations for the N+1 coeﬃcients ai.

If the ∆ci are small, the relation between the errors may be approximated

by the linear terms of the Taylor- series for P(x).

∆P (x) =

∆x +

∂P (x)
∂x

N

Xi=1

∂P (x)
∂ci

∆ci

N

Xk=0

N

3

(5)

(6)

(7)

(8)

(9)

As the bk are roots of P, ∆P should be zero and therefore, inserting bk for

x, we get:

∆bk = −

1
∂P (bk)
∂x

N

Xi=1

∂P (bk)
∂ci

∆ci

(10)

Treating the ci and bk as probability variables with standard deviations sci and
sbk, the standard deviation and therefore the estimated error of bk is given by

sbk =

1
| ∂P (bk)
∂x

N

|

v
u
u
t

Xi=1

b2(N −i)

k

s2
ci + 2

(−1)i+jb2N −i−j

k

Cov(ci, cj)

(11)

N

N

Xi=1

Xj=i+1

As usual, Cov(ci, cj) means the covariance between ci and cj.
It remains to show that equation 4 is valid. For this purpose it is useful to

state some properties of the coeﬃcients β.

For any l with 1 ≤ l < N and any i ≤ N the sum of all βN lm may be divided

into the sum of all βN lm containing bi and those not containing bi:

C(N,l)

Xm=1

βN lm = bi

C(N −1,l−1)

C(N −1,l)

β(−i)
(N −1)(l−1)j +

β(−i)
N −1,l,j.

(12)

Xj=1

Xj=1

With β(−i)
(N −1)lj we denote the products not containing bi that is, which are
chosen from the set {b1, . . . , bi−1, bi+1, . . . , bN } containing N-1 elements and
not containing bi. For l ≤ 0 we deﬁne β(−i)

(N −1)lm = 1.

An important special case of 12 results if i = N. Then β(−N )

(N −1)lm = β(N −1)lm

and the following expression results:

C(N,l)

C(N −1,l)

C(N −1,l−1)

βN ljm =

β(N −1)lj + bN

β(N −1)(l−1)j

(13)

Xm=1

Xj=1

Xj=1

For a proof of equation 4 consider the following system of equations:

I0(x) = a0 +

aie−bix

N

Xi=1

Ik(x) = a0

+

xk
k!

k

−1
bi (cid:19)

(cid:18)

N

Xi=1

ai 



e−bix −

k−1

(cid:18)

Xj=0

−1
bi (cid:19)

k−j xj
j!





(14)

The validity of 14 is easily seen by performing the integrals in equation 2

Now consider the following linear transformations deﬁned recursively on the

analytically.

set of equations 14:

4

I (1)
k
I (h)
k

= Ik + b1Ik+1
= I (h−1)

k

+ bhI (h−1)

k+1

h > 1

(15)

For this kind of transformation a rather general relationship holds:

I (h)
k = Ik +

βhlmIk+l

h

C(h,l)

Xl=1

Xm=1

(16)

Proof: Induction for h. For h = 1, proposition 16 just repeats the deﬁnition
k . Now assume that 16 holds for I (h)

k . Then

of I (1)

Ik +

βhlmIk+l + bh+1Ik+1 +

bh+1βhlmIk+l+1 =

I (h+1)
k

= I (h)

k + bh+1I (h)

k+1 =

h

C(h,l)

Xl=1

Xm=1

h+1

C(h,l−1)

Xl=2

Xm=1

h

C(h,l)

Xl=1

Xm=1

h

C(h,l)

Xl=1

Xm=1

C(h+1,1)

Ik +

βhlmIk+l + bh+1Ik+1 +

bh+1βh(l−1)mIk+l =

h

C(h+1,l)

Ik +

β(h+1)1mIk+1 +

β(h+1)lmIk+l + bh+1βhh1Ik+h+1

Xm=1

Xl=2

Xm=1

where 13 has been used in order to obtain the last line. Obviously, this result
may be converted into

I (h+1)
k

= Ik +

β(h+1)lmIk+l

h+1

C(h+1,l)

Xl=1

Xm=1

whereby the proof of 16 is completed.

Consider now I (N )

. By 16

0

I (N )
0 = y +

N

C(N,l)

Xl=1

Xm=1

βN lmIl

(17)

Inserting 14 this expands into

y +

βN lmIl =

N

C(N,l)

Xl=1

Xm=1

N

C(N,l)

a0 +

aie−bix 

1 +

N

Xi=1

Xl=1

Xm=1



βN lm

l

−1
bi (cid:19)

(cid:18)



+



5

N

C(N,l)

N

l−1

βN lm 

a0

−

ai

Xl=1

Xm=1

Xi=1

Xj=0

xl
l!



(−bi)j−l xj
j!





(18)

The motive for applying transform 15 to I0 was to get rid of the exponential
terms. The following proposition asserts that equation 18 is actually free of
exponential terms:

h

C(h,l)

Xl=1

Xm=1

βhlm

l

−1
bi (cid:19)

(cid:18)

= −1

1 ≤ i ≤ h

(19)

Proof:

For h = 1 the assertion is trivial. Now assume that 19 is valid for h. Then

the following calculations prove the truth for h+1 and therefore for all h:

h+1

C(h+1,l)

Xl=1

Xm=1

β(h+1)lm (cid:18)

l

−1
bi (cid:19)

=

h

l

C(h,l)

−1
bi (cid:19)

(cid:18)

Xl=1





Xm=1

βhlm + bh+1

βh(l−1)m

+

β(h+1)(h+1)1 =

C(h,l−1)

Xm=1

h+1

−1
bi (cid:19)

(cid:18)



−1 −

+ bh+1

bh+1
bi

h−1

C(h,l)

Xl=1

Xm=1

l+1

h+1

βhlm

−1
bi (cid:19)

(cid:18)

+

−1
bi (cid:19)

(cid:18)

β(h+1)(h+1)1 =

−1 −

bh+1
bi

−

bh+1
bi

βhlm

l

−1
bi (cid:19)

(cid:18)

+

bh+1

bi (cid:18)

h

−1
bi (cid:19)

βhh1+

h

C(h,l)

Xl=1

Xm=1

h+1

−1
bi (cid:19)

(cid:18)

β(h+1)(h+1)1

Using equation 19 for and collecting all terms the last expression evaluates to
-1.

To complete the proof of equation 4 some more transformations on formula

18 are required:

N

C(N,l)

N

C(N,l)

y +

βN lmIl = a0 +

βN lm 

a0

−

ai

Xl=1

Xm=1

Xl=1

Xm=1

xl
l!

N

l−1

Xi=1

Xj=0

(−bi)j−l xj
j!



=





N

N

C(N,l)

N

C(N,j)

N

ai +

βN lma0 −

βN jm

ai(−bi)l−j 

=

Xi=0

Xl=1

Xm=1

Xj=l+1

Xm=1

Xi=1

xl
l!







6

N

N

C(N,l)

l

C(N,j)

N

ai+

βN lma0 +

ai(−bi)l +

βN jm

ai(−bi)l−j 

=

Xi=0

Xl=1

Xm=1

Xj=1

Xm=1

Xi=1

N

Xi=1

xl
l!







(20)

N

N

C(N,l)

N

N

ai +

βN lm

ai +

aiSil

Xi=0

Xl=1

Xm=1

Xi=0

Xi=1

xl
l!







Sil =

βN (l−p)m(−bi)p + (−bi)l

for l > 1

l−1

C(N,l−p)

Xp=1

Xm=1

where

and

Using 12, for l > 1 Sil transforms into

Si1 = −bi

for l = 1

Sil =

C(N −1,l−p−1)

Xm=1

l−1

C(N −1,l−p)

Xp=1

Xm=1





β(−i)
(N −1)(l−p)m + bi

β(−i)
(N −1)(l−p−1)m

(−bi)p+(−bi)l

Substituting in the second part of this sum q for p+1 this expression transforms
into



C(N −1,l−1)

l−1

C(N −1,l−p)

Sil = −

β(−i)
(N −1)(l−1)mbi +

Xm=1

Xp=2

Xm=1

β(−i)
(N −1)(l−p)m(−bi)p

l−1

C(N −1,l−q)

−

Xq=2

Xm=1

β(−i)
(N −1)(l−q)m(−bi)q − (−bi)l + (−bi)l =

C(N −1,l−1)

−

Xm=1

β(−i)
(N −1)(l−1)mbi

Therefore Sil is the negative sum of all βN lm which contain bi. Consequently, in
expression 20 only those ai are not cancelled for which βN lm does not contain
bi. For that, 20 may be written as

N

C(N,l)

N

C(N,l)

y +

βN lmIl =

βN lmαN lm

Xl=1

Xm=1

Xl=0

Xm=1

xl
l!

which proves equation 4.
Example: Consider this sum of two exponentials and a constant:

y(x) = 0.3 + exp(−0.7x) + 0.4exp(−0.3x)

7

Id
1
2
3
4
5

Id
1
2
3
4
5

The function is evaluated in in the interval 0 ≤ x ≤ 6 at Np equally spaced
points. The discrete function values are multiplied by one plus a gaussian dis-
tributed random variable so that the relative error has the standard deviation
σ. I1 and I2 are calculated using the trapezoidal method. For diﬀerent settings
of Np and σ the coeﬃcients c1 and c2, d0 (the intercept), d1 and d2 and the
corresponding errors of c1 and c2 as well as the covariance between these two
factors are determined by the commercial statistics program STATISTICAr
and are listed in table 1. The parameters b1, b2, a0, a1, a2 and the errors ∆b1
and ∆b2 are calculated from these coeﬃcients as described above and are listed
in table 2.

Table 1: Statistically determined coeﬃcients
c2
-0.2100
-0.2130
-0.2300
-0.2615
-0.1603

Cov
0
3.10−6
0.000343
0.0502
0.00815

c1
-1.0000
-1.0054
-1.0358
-1.0842
-0.9155

∆c1
10−6
0.0024
0.0253
0.2954
0.1190

∆c2
10−6
0.0014
0.0146
0.1700
0.0685

σ
0.0000
0.0001
0.001
0.01
0.01

Np
601
601
601
601
2001

d0
1.7000
1.7001
1.7008
1.7006
1.6999

d1
0.8800
0.8890
0.9396
1.0249
0.7369

d2
0.0315
0.0320
0.0349
0.0408
0.0228

Table 2: Parameter estimates based on the coeﬃcients in table 1
a0
b1
0.30
0.7000
0.30
0.7020
0.30
0.7134
0.31
0.7220
0.28
0.6796

∆b2
3.10−6
0.0021
0.0196
0.1754
0.0911

∆b1
3.10−6
0.0019
0.0180
0.1211
0.0281

b2
0.3000
0.3034
0.3224
0.3622
0.2359

a2
0.40
0.41
0.45
0.51
0.32

a1
1.00
0.99
0.95
0.88
1.09

The results show that for small errors in the coeﬃcients the estimated vari-
ance of b1 and b2 is also small and the estimate is realistic. The ﬁrst case was
computed without artiﬁcial noise, in this case the accuracy seems to be deter-
mined mainly by the statistics program. Adding noise deteriorates the accuracy
of the results rapidly. While a relative error of 0.0001 (case 2) still leads to
a reasonable result, the tenfold relative error (case 3) already means that the
calculated uncertainty of b2 is about 7%. A one- percent inaccuracy in the data
(case 4) gives a result even with the ﬁrst digit uncertain. As case 5 where the
number of data- points is raised to 2001 shows, increasing the size of the data-
set may at least partially compensate for noise.

References

[1] F. S. Acton, Numerical Methods that Work, The Mathematical Association

of America, Washington (1990)

[2] W. Squire, A Simple Integral Method for System Identiﬁcation, Mathemat-

ical Biosciences 10, 145-148 (1971)

8

[3] J. E. Diamessis, Least- Square- Exponential Approximation, Electronics

Letters 8, 454-455 (1972)

9


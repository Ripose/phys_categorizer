5
0
0
2
 
g
u
A
 
8
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
4
0
8
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Studies of Boosted Decision Trees
for MiniBooNE Particle Identiﬁcation

Hai-Jun Yanga,c,1, Byron P. Roea, Ji Zhub

a Department of Physics, University of Michigan, Ann Arbor, MI 48109, USA
b Department of Statistics, University of Michigan, Ann Arbor, MI 48109, USA
c Los Alamos National Laboratory, Los Alamos, NM 87545, USA

Abstract

Boosted decision trees are applied to particle identiﬁcation in the MiniBooNE
experiment operated at Fermi National Accelerator Laboratory (Fermilab) for
neutrino oscillations. Numerous attempts are made to tune the boosted decision
trees, to compare performance of various boosting algorithms, and to select input
variables for optimal performance.

1 Introduction

In High Energy Physics (HEP) experiments, people usually need to select some events
with speciﬁc interest, so called signal events, out of numerous background events for
study. In order to increase the ratio of signal to background, one needs to suppress
background events while keeping high signal eﬃciency. To this end, some advanced
techniques, such as AdaBoost[1], ǫ-Boost[2], ǫ-LogitBoost[2], ǫ-HingeBoost, Random
Forests [3] etc., from Statistics and Computer Sciences were introduced for signal and
background event separation in the MiniBooNE experiment[4] at Fermilab. The Mini-
BooNE experiment is designed to conﬁrm or refute the evidence for νµ → νe oscillations
at ∆m2 ≃ 1 eV 2/c4 found by the LSND experiment[5]. It is a crucial experiment which
will imply new physics beyond the standard model if the LSND signal is conﬁrmed.
These techniques are tuned with one sample of Monte Carlo (MC) events, the training
Ini-
sample, and then tested with an independent MC sample, the testing sample.
tial comparisons of these techniques with artiﬁcial neural networks (ANN) using the
MiniBooNE MC samples were described previously[6]. This work indicated that the
method of boosted decision trees is superior to the ANNs for Particle Identiﬁcation
(PID) using the MiniBooNE MC samples. Further studies show that the boosted de-
cision tree method has not only better event separation, but is also more stable and
robust than ANNs when using MC samples with varying input parameters.

The boosting algorithm is one of the most powerful learning techniques introduced
in the past decade[7, 8, 9, 10]. The motivation for the boosting algorithm is to design a
procedure that combines many “weak” classiﬁers to achieve a ﬁnal powerful classiﬁer.

1E-mail address: yhj@umich.edu

1

In the present work numerous trials are made to tune the boosted decision trees, and
comparisons are made for various algorithms. For a large number of discriminant
variables, several techniques are described to select a set of powerful input variables in
order to obtain optimal event separation using boosted decision trees. Furthermore,
post-ﬁtting of weights for the trained boosting trees is also investigated to attempt
further possible improvement.

This paper is focussed on the boosting tuning. All results appearing in this pa-
per are relative numbers. They do not represent the MiniBooNE PID performance;
that performance is continually improving with further algorithm and PID study. The
description of the MiniBooNE reconstruction packages[11, 12], the reconstructed vari-
ables, the overall and absolute performance of the boosting PID[13, 14], the validation
of the input variables and the boosting PID variables by comparing various MC and
real data samples[15] will be described in future articles.

2 Decision Trees

Boosting algorithms can be applied to any classiﬁer, Here they are applied to decision
trees. A schematic of a simple decision tree is shown in Figure 1, S means signal, B
means background, terminal nodes called leaves are shown in boxes. The key issue
is to deﬁne a criterion that describes the goodness of separation between signal and
background in the tree split. Assume the events are weighted with each event having
weight Wi. Deﬁne the purity of the sample in a node by

P =

Ps Ws
Ps Ws + Pb Wb

,

where Ps is the sum over signal events and Pb is the sum over background events.
Note that P (1 − P ) is 0 if the sample is pure signal or pure background. For a given
node let

n

X
i=1
where n is the number of events on that node. The criterion chosen is to minimize

Gini = (

Wi)P (1 − P ),

Ginilef t child + Giniright child.

To determine the increase in quality when a node is split into two nodes, one

maximizes

Criterion = Ginif ather − Ginilef t child − Giniright child.

At the end, if a leaf has purity greater than 1/2 (or whatever is set), then it is called
a signal leaf, otherwise, a background leaf. Events are classiﬁed signal (have score of
1) if they land on a signal leaf and background (have score of -1) if they land on a
background leaf. The resulting tree is a decision tree.

Decision trees have been available for some time[7]. They are known to be powerful
but unstable, i.e., a small change in the training sample can produce a large change in

2

S/B
52/48

< 100

PMT Hits?

≥ 100

< 0.2 GeV

≥ 0.2 GeV

S/B
9/10

Radius?

< 500 cm

≥ 500 cm

S
39/1

S/B
48/11

Energy?

B
2/9

B
4/37

S
7/1

Figure 1: Schematic of a decision tree.

the tree and the results. Combining many decision trees to make a “majority vote”, as
in the random forests method, can improve the stability somewhat. However, as will be
discussed in Section 6, the performance of the random forests method is signiﬁcantly
worse than the performance of the boosted decision tree method in which the weights
of misclassiﬁed events are boosted for succeeding trees.

3 Some Boosting Algorithms

If there are N total events in the sample, the weight of each event is initially taken as
1/N. Suppose that there are M trees and m is the index of an individual tree. Let

• xi = the set of PID variables for the ith event.

• yi = 1 if the ith event is a signal event and yi = −1 if the event is a background

event.

• wi = the weight of the ith event.

• Tm(xi) = 1 if the set of variables for the ith event lands that event on a signal leaf
and Tm(xi) = −1 if the set of variables for that event lands it on a background
leaf.

• I(yi 6= Tm(xi)) = 1 if yi 6= Tm(xi) and 0 if yi = Tm(xi).

There are several commonly used algorithms for boosting the weights of the misclas-
siﬁed events in the training sample. The boosting performance is quite diﬀerent using
various ways to update the event weights.

3

3.1 AdaBoost

The ﬁrst boosting method is called “AdaBoost”[1] or sometimes discrete AdaBoost.
Deﬁne for the mth tree:

Calculate:

N
i=1 wiI(yi 6= Tm(xi))
errm = P
N
i=1 wi

P

.

αm = β × ln((1 − errm)/errm).

β = 1 is the value used in the standard AdaBoost method. Change the weight of each
event i, i = 1, ..., N.

Renormalize the weights.

The score for a given event is

wi → wi × eαmI(yi6=Tm(xi)).

wi →

wi
N
i=1 wi

.

P

T (x) =

αmTm(x),

M

X
m=1

which is just the weighted sum of the scores of the individual trees.

3.2

ǫ-Boost

A second boosting method is called “ǫ-Boost” [2], or sometimes “shrinkage”. After the
mth tree, change the weight of each event i, i = 1, ..., N.

where ǫ is a constant of the order of 0.01. Renormalize the weights.

The score for a given event is

which is the renormalized, but unweighted, sum of the scores over individual trees.

3.3

ǫ-LogitBoost

A third boosting method is called “ǫ-LogitBoost”. This method is quite similar to
ǫ-Boost, but the weights are updated according to:

where T (x) = T (x) + ǫ × Tm(x) for the mth tree iteration.

wi → wie2ǫI(yi6=Tm(xi)),

wi →

wi
N
i=1 wi

.

P

T (x) =

ǫTm(x),

M

X
m=1

wi →

e−yiT (xi)
1 + e−yiT (xi) .

4

3.4

ǫ-HingeBoost

A fourth boosting method is called “ǫ-HingeBoost”. Again this method is quite similar
to ǫ-Boost, but here the weights are updated according to:

wi = 1 if yiT (xi) < 1; wi = 0 if yiT (xi) ≥ 1

where T (x) = T (x) + ǫ × Tm(x) for the mth tree iterations.

3.5 LogitBoost

A ﬁfth boosting method is called “LogitBoost”[2]. Let y∗
i = 1 for signal events and
y∗
i = 0 for background events. Initial probability estimates are set to p(xi) = 0.5 for
event i, where x is the set of PID variables. Let:

where wi is the weight of event i. Let z be the weighted average of z over some set
of events. Instead of the Gini criterion, the splitting variable and point to divide the
events at a node into two nodes L and R is determined to minimize

X
L

wi(zi − zL)2 + X
R

wi(zi − zR)2.

The output for tree m is T ∗
output score is:

m(xi) = z for the node onto which event i falls. The total

The probability is updated according to:

3.6 Gentle AdaBoost

A sixth boosting method is called “Gentle AdaBoost”[2].
It uses same criterion as
described for the LogitBoost. Here zi is same as yi. The weights are updated according
to:

for signal events and

for background events, where pm(xi) is the weighted purity of the leaf on which event
i falls.

zi =

y∗
i − p(xi)
p(xi)(1 − p(xi))

;

wi = p(xi)(1 − p(xi)),

T (x) =

M

X
m=1

1
2

T ∗
m(x).

p(x) =

eT (x)
eT (x) + e−T (x) .

wi → wie−(2pm(xi)−1)

wi → wie+(2pm(xi)−1)

5

AdaBoost(b =0.3, 45 leaves)
AdaBoost(b =0.5, 45 leaves)
AdaBoost(b =0.8, 45 leaves)
AdaBoost(b =1.0, 45 leaves)

Ntrees = 1000

AdaBoost(b =0.5, 8 leaves)
AdaBoost(b =0.5, 20 leaves)
AdaBoost(b =0.5, 45 leaves)
AdaBoost(b =0.5, 100 leaves)

Ntrees = 1000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

20

30

40

50
Signal Efficiency (%)

60

70

80

20

30

40

50
Signal Efficiency (%)

60

70

80

Figure 2: Tuning β (left) and decision tree size (right) using AdaBoost.

3.7 Real AdaBoost

A seventh boosting method is called “Real AdaBoost”[2]. It is similar to the discrete
version of AdaBoost described in Section 3.1, but the weights and event scores are
calculated in diﬀerent ways. The event score for event i in tree m is given by:

Tm(xi) = 0.5 × ln(pm(xi)/(1. − pm(xi)))

where pm(xi) is the weighted purity of the leaf on which event i falls. The event weights
are updated according to:

wi → wi × e(−yi×Tm(xi))

and then renormalized so that the total weight is one. The total output score including
all of the trees is given by T (x) = P

M
m=1 Tm(x)

4 Tuning Parameters for the Boosted Decision Trees

MiniBooNE MC samples from the February 2004 Baseline MC were used to tune some
parameters of the boosted decision trees. There are 88233 intrinsic νe signal events
and 162657 νµ background events. 20000 signal events and 30000 background events
were selected randomly for the training sample and the rest of the events were the test
sample. The number of input variables for boosting training is 52. The relative ratio
is deﬁned as the background eﬃciency divided by the corresponding signal eﬃciency
and rescaled by a constant value.

The left plot of Figure 2 shows the relative ratio versus the signal eﬃciency for
AdaBoost with 45 leaves per decision tree and various β values for 1000 tree iterations.
The boosting performances slightly depend on the β values. AdaBoost with β = 1
works slightly better in the high signal eﬃciency region (Eﬀ > 65%) but worse in the

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

6

low signal eﬃciency region (Eﬀ < 60%) than AdaBoost with smaller β values, 0.8, 0.5
or 0.3. To balance the overall performance, β = 0.5 is selected to replace the standard
value 1 for the AdaBoost training.

The right plot of Figure 2 shows the relative ratio versus the signal eﬃciency for
AdaBoost with β = 0.5 and 1000 tree iterations for various decision tree sizes ranging
from 8 leaves to 100 leaves. AdaBoost with a large tree size worked signiﬁcantly
better than AdaBoost with a small tree size, 8 leaves; the latter number has been
recommended in some statistics literature[16]. Typically, it takes more tree iterations
for the smaller tree size to reach optimal performance. For this application, even with
more tree iterations (10000 trees), results from boosting with small tree size (8 leaves)
are still signiﬁcantly worse (∼10%-20%) than results obtained with large tree size (45
leaves). Here, 45 leaves per decision tree is selected (this number is quite close to the
number of input variables, 52, for the boosting training.)

How many decision trees are suﬃcient? It depends on the MC samples for boosting
training and testing. For the given set of boosting parameters selected above, we
ran boosting with 1000 tree iterations. The left plot of Figure 3 shows the relative
ratio versus the signal eﬃciency for AdaBoost with tree iterations of 100, 200, 500,
800 and 1000, respectively. The boosting performance becomes better with more tree
iterations. The right plot of Figure 3 shows the relative ratio versus the number of
decision trees for signal eﬃciencies of 50%, 60% and 70% which cover the regions of
greatest interest for the MiniBooNE experiment. Typically, the boosting performance
for low signal eﬃciencies converges after few hundred tree iterations and is then stable.
For high signal eﬃciency, boosting performance continues to improve as the number of
decision trees is increased. For these particular MC samples, the boosting performance
is close to optimal after 1000 tree iterations. For the sake of comparison, the AdaBoost
performance of the boosting training MC samples is also shown in the right plot of
Figure 3. The relative ratios drop quickly down to zero (zero means no background
events left after selection for a given signal eﬃciency) within 100 tree iterations for
50%-70% signal eﬃciencies. The AdaBoost outputs for the training MC sample and
for the testing MC sample for Ntree = 1, 100, 500 and 1000 are shown in Figure 4.
The signal and background separation for the training sample becomes better as the
number of tree iterations increases. The signal and background events are completely
distinguished after about 500 tree iterations. For the testing samples, however, the
signal and background separations are quite stable after a few hundred tree iterations.
The corresponding relative ratios are stable for given signal eﬃciencies as shown in
right plot of Figure 3.

The tuning parameter for ǫ-Boost is ǫ. The left plot of Figure 5 shows the relative
ratio versus the signal eﬃciency for ǫ-Boost with ǫ values of 0.005, 0.01, 0.02, 0.04,
respectively. ǫ-Boost with fairly large ǫ values for 45 leaves per decision tree and 1000
tree iterations has better performance for the high signal eﬃciency region (Eﬀ > 50%).
The results from AdaBoost with β=0.5 are comparable to those from ǫ-Boost. ǫ-Boost
with ǫ > 0.01 works slightly better because ǫ-Boost converges more quickly with larger
ǫ values. However, with more tree iterations, the ﬁnal performances for diﬀerent ǫ
values are very comparable. Here ǫ = 0.01 is chosen for further comparisons.

7

o
i
t
a
R
 
e
v
i
t
a
l
e
R

2.2

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

AdaBoost(b =0.5, 45 leaves, 100 trees)
AdaBoost(b =0.5, 45 leaves, 200 trees)
AdaBoost(b =0.5, 45 leaves, 500 trees)
AdaBoost(b =0.5, 45 leaves, 800 trees)
AdaBoost(b =0.5, 45 leaves, 1000 trees)

10

1

o
i
t
a
R
 
e
v
i
t
a
l
e
R

AdaBoost(eff=50%,testing MC)
AdaBoost(eff=60%,testing MC)
AdaBoost(eff=70%,testing MC)

AdaBoost(eff=50%,training MC)
AdaBoost(eff=60%,training MC)
AdaBoost(eff=70%,training MC)

-1

10

20

30

40

50
Signal Efficiency (%)

60

70

80

0

200

400

600

800

1000

Number of Tree Iterations

Figure 3: Tuning decision tree iterations using AdaBoost (left) and the relative ratio
versus the number of tree iterations for signal eﬃciencies ranging from 50% to 70%
(right). β = 0.5 and there are 45 leaves per decision tree.

The right plot of Figure 5 shows the relative ratio versus the signal eﬃciency for
AdaBoost and ǫ-Boost using two diﬀerent ways to split tree nodes. One way is to
maximize the criterion based on the Gini index to select the next tree split, the other
way is to split the left tree node ﬁrst. For AdaBoost, the performance for the “left
node ﬁrst” method gets worse for signal eﬃciency less than about 65%. At about the
same signal eﬃciency, the performance for the two ǫ-Boosts are quite comparable and
are comparable with AdaBoost based on the Gini index. However, the ǫ-Boost method
based on the Gini index becomes worse than the others for high signal eﬃciency.

Larger ǫ makes ǫ-Boost converge more quickly, but increasing the the size of deci-
sion trees also makes ǫ-Boost converge more quickly. The performance comparison of
AdaBoost with diﬀerent tree sizes shown in the right plot of Figure 2 is for the same
number of tree iterations (1000). To make a fair comparison for the boosting perfor-
mance with diﬀerent tree sizes, it is better to let them have a similar number of total
tree leaves. The top left plot of the Figure 6 shows the relative ratio versus the signal
eﬃciency for AdaBoost and ǫ-Boost with similar numbers of the total tree leaves, 1800
tree iterations for 45 leaves per tree and 10000 tree iterations for 8 leaves per tree. For
a small decision tree size of 8 leaves, the performance of the ǫ-Boost is better than
that of AdaBoost for 10000 trees. For a large decision tree size of 45 leaves, ǫ-Boost
has slightly better performance than AdaBoost at low νe signal eﬃciency (<65%), but
worse at high νe signal eﬃciency (>70%). The comparison between small tree size
(8 leaves) and large tree size (45 leaves) with comparable overall decision tree leaves
indicates that large tree size with 45 leaves yields ∼10%-20% better performance for
the MiniBooNE Monte Carlo samples.

The other ﬁve plots in Figure 6 show the relative ratio versus the number of tree
iterations for AdaBoost and ǫ-Boost with 45 leaves and 8 leaves assuming signal ef-

8

Training MC Samples   .VS.    Testing MC Samples

Ntree = 1

Ntree = 1

-1

0

1

Ntree = 100

-1

0

1

2

Ntree = 100

-20

-10

0

10

20

-20

-10

0

10

20

Ntree = 500

Ntree = 500

-20

0

20

-20

0

20

Ntree = 1000

Ntree = 1000

30000

20000

10000

0

-2

2000

1000

0

3000

2000

1000

0

2000

1500

1000

500

0

-40

-20

0

20

-40

-20

0

20

Boosting Outputs

Boosting Outputs

Figure 4: The AdaBoost outputs for training MC samples (plots in left column) and
for testing MC samples (plots in right column) for Ntree = 1, 100, 500 and 1000,
respectively. Dotted histograms represent background events and solid histograms
represent signal events. β = 0.5 and there are 45 leaves per decision tree.

x 10 2

1500

1000

500

0

-2

2

10000

10000

8000

6000

4000

2000

0

7500

5000

2500

0

8000

6000

4000

2000

0

9

e -Boost(e =0.005, 45 leaves)
e -Boost(e =0.01, 45 leaves)
e -Boost(e =0.02, 45 leaves)
e -Boost(e =0.04, 45 leaves)
AdaBoost(b =0.5, 45 leaves)

Ntrees = 1000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

AdaBoost(b =0.5)
AdaBoost(b =0.5, left node first)
e -Boost(e =0.01)
e -Boost(e =0.01,left node first)

Ntrees = 1000, 45 leaves

20

30

40

50
Signal Efficiency (%)

60

70

80

20

30

40

50
Signal Efficiency (%)

60

70

80

Figure 5: Tuning ǫ-Boost (left) and comparing two ways to split the tree branches
(right).

ﬁciencies of 40%, 50%, 60% ,70% ,80%, respectively. The maximum number of tree
iterations is 5000 for the large tree size of 45 leaves and 10000 for the small tree size of
8 leaves. Usually, the performance of the boosting method becomes better with more
tree iterations in the beginning; then at some point, it may reach an optimal value
and gradually get worse with increasing number of trees, especially in the low signal
eﬃciency region. The turning point of the boosting performance depends on the signal
eﬃciency and MC samples used for training and test.

Generally, if the number of weighted signal events is larger than the number of
weighted background events in a given leaf, it is called a signal leaf, otherwise, a
background leaf. Here, the threshold value for signal purity is 50% for the leaf to be
called a signal leaf. This threshold value can be modiﬁed, say, to 30%, 40%, 45%, 60%
or 70%.
It is seen in Figure 7 that the performance of boosted decision trees with
Adaboost degrades for threshold values away from the central value of 50%. Especially
for threshold values away from 50%, the errm of mth tree often converges to 0.5 within
about 100 tree iterations; after that the weights of the misclassiﬁed events do not
successfully update because αm ≡ β × ln((1 − errm)/errm) = 0 if errm = 0.5. Then
wi = wi × eαm×I(yi6=Tm(xi)) remains the same as for the previous tree. Typically, the
errm value increases for the ﬁrst 100-200 tree iterations and then remains stable for
further tree iterations, causing the weight of mth tree, αm, to decrease for the ﬁrst
100-200 tree iterations and then remain stable. For practical use of the AdaBoost
algorithm, a lower limit, say, 0.01, on αm will avoid the impotence of the succeeding
boosted decision trees.

This problem is unlikely to happen for ǫ-Boost because the weights of misclassi-
ﬁed events are always updated by the same factor, e2ǫ×I(yi6=Tm(xi)). If diﬀering purity
threshold values are applied to boosted decision trees with ǫ-Boost, the performance
peaks around 50% and slightly worsens, typically within 5%, for other values ranging

10

3

2

1

0

0.9

0.8

0.7

0.6

1.8

1.6

1.4

1.2

1

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

AdaBoost(45)
e -Boost(45)
AdaBoost(8)
e -Boost(8)

Signal Eff = 40%

20

40

60

80

0

2000 4000 6000 8000 10000

Signal Efficiency(%)

Number of Tree Iterations

Signal Eff = 50%

Signal Eff = 60%

0

2000 4000 6000 8000 10000

0

2000 4000 6000 8000 10000

Number of Tree Iterations

Number of Tree Iterations

Signal Eff = 70%

Signal Eff = 80%

0

2000 4000 6000 8000 10000

0

2000 4000 6000 8000 10000

Number of Tree Iterations

Number of Tree Iterations

Figure 6: Boostings with diﬀerent tree sizes.

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

0.8

0.7

0.6

0.5

0.4

1.2

1

0.8

0.6

3.5

2.5

3

2

1.5

11

AdaBoost, 45 leaves, Ntrees = 1000
Purity > 0.3
Purity > 0.4
Purity > 0.45
Purity > 0.5
Purity > 0.6
Purity > 0.7

2.5

2.25

2

1.75

1.5

1.25

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1

0.75

0.5

0.25

20

30

40

50
Signal Efficiency (%)

60

70

80

Figure 7: Performance of AdaBoost with β = 0.5, 45 leaves per tree and 1000 tree
iterations for various threshold values for signal purity to determine whether the tree
leaf is signal leaf or background leaf.

from 30% to 70%.

The unweighted misclassiﬁed event rate, weighted misclassiﬁed event rate errm and
αm for the boosted decision trees with the AdaBoost algorithm versus the number of
tree iterations are shown in Figure 8, for a signal purity threshold value of 50%. From
this plot, it is clear that, after a few hundred tree iterations, an individual boosted
decision tree has a very weak discriminant power (i.e., is a “weak” classiﬁer). The
errm is about 0.4-0.45, corresponding to αm of around 0.2-0.1. The unweighted event
discrimination of an individual tree is even worse, as is also seen in Figure 8. Boosted
decision trees focus on the misclassiﬁed events which usually have high weights after
hundreds of tree iterations. The advantage of the boosted decision trees is that the
method combines all decision trees, ”weak” classiﬁers, to make a powerful classiﬁer as
stated in the Introduction section.

When the weights of misclassiﬁed events are increased (boosted), some events which
are very diﬃcult correctly classify obtain large event weights. In principle, some outliers
which have large event weights may degrade the boosting performance. To avoid this
eﬀect, it might be useful to set an upper limit for the event weights to trim some outliers.
It is found that setting a weight limit doesn’t improve the boosting performance, and, in
fact, may degrade the boosting performance slightly. However, the eﬀect was observed
to be within one standard deviation for the statistical error. One might also trim events

12

un-weighted misclassified event rate
weighted misclassified event rate, errm
m = b *ln((1-errm)/errm), b =0.5

m

 
,

m
r
r
e

1

0.8

0.6

0.4

0.2

0

0

200

400

600

800

1000

Number of Tree Iterations

Figure 8: The unweighted, weighted misclassiﬁed event rate (errm), αm versus the
number of tree iterations for AdaBoost with β = 0.5 and signal purity threshold value
of 50%.

with very low weights which can be correctly classiﬁed easily to provide a better chance
for diﬃcult events. No apparent improvement or degradation was observed considering
the statistical error. These results may indicate that the boosted decision trees have
the ability to deal with outliers quite well and to focus on the events located around the
boundary regions where it is diﬃcult to correctly distinguish signal and background
events.

5 Comparison Among Boosting Algorithms

Besides AdaBoost and ǫ-Boost, there are other algorithms such as ǫ-LogitBoost, and ǫ-
HingeBoost which use diﬀerent ways of updating the event weights for the misclassiﬁed
events. The four plots of Figure 9 show the relative ratio versus the signal eﬃciency
for various boostings with diﬀerent tree sizes. The top left, top right, bottom left,
and bottom right plots are for 500, 1000, 2000, and 3000 tree iterations, respectively.
Boosting with a large tree size of 45 leaves is seen to work better than boosting with
a small tree size of 8 leaves as noted above. AdaBoost and ǫ-Boost have comparable
performance, slightly better than that of ǫ-LogitBoost.
ǫ-HingeBoost is the worst
among these four boosting algorithms, especially for the low signal eﬃciency region.

The top left, top right, bottom left and bottom right plots of Figure 10 show the rel-
ative ratio versus the signal eﬃciency with 45 leaves of ǫ-Boost, AdaBoost, ǫ-LogitBoost
and ǫ-HingeBoost for varying numbers of tree iterations. Generally, boosting perfor-
mance continuously improves with an increase in the number of tree iterations until
an optimum point is reached. From the two top plots, it is apparent that ǫ-Boost
converges more slowly than does AdaBoost; however, with about 1000 tree iterations,
their performances are very comparable. There is only marginal improvement beyond

13

a
a
e -Boost(45)
AdaBoost(45)
e -logitBoost(45)
e -hingeBoost(30)
e -Boost(8)
AdaBoost(8)
e -logitBoost(8)
e -hingeBoost(8)

e -Boost(45)
AdaBoost(45)
e -logitBoost(45)
e -hingeBoost(30)
e -Boost(8)
AdaBoost(8)
e -logitBoost(8)
e -hingeBoost(8)

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

2.5

1.5

0.5

2.5

1.5

0.5

3

2

1

0

3

2

1

0

Ntree = 500

Ntree = 1000

20

40

60

80

Signal Efficiency (%)

20

40

60

80

Signal Efficiency (%)

Ntree = 2000

Ntree = 3000

20

40

60

80

Signal Efficiency (%)

20

40

60

80

Signal Efficiency (%)

Figure 9: Performance comparison of various Boostings.

e -Boost(45)
AdaBoost(45)
e -logitBoost(45)
e -hingeBoost(30)
e -Boost(8)
AdaBoost(8)
e -logitBoost(8)
e -hingeBoost(8)

e -Boost(45)
AdaBoost(45)
e -logitBoost(45)
e -hingeBoost(30)
e -Boost(8)
AdaBoost(8)
e -logitBoost(8)
e -hingeBoost(8)

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

2.5

1.5

0.5

2.5

1.5

0.5

3

2

1

0

3

2

1

0

14

e -Boost(45)

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

5
4.5

4
3.5

3
2.5

2

1.5
1

0.5

0

10

9
8

7

6
5

4

3
2
1

0

20

40

60

80

20

40

60

80

Signal Efficiency (%)

Signal Efficiency (%)

e -logitBoost(45)

e -hingeBoost(30)

20

40

60

80

20

40

60

80

Signal Efficiency (%)

Signal Efficiency (%)

Figure 10: Performance comparison of various Boostings with large tree size.

AdaBoost(45)

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

5
4.5

4
3.5

3
2.5

2

1.5
1

0.5

0

10

9
8

7

6
5

4

3
2
1

0

15

20

40

60

80

20

40

60

80

Signal Efficiency (%)

Signal Efficiency (%)

e -Boost(8)

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000
ntree = 10000

e -logitBoost(8)

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000
ntree = 10000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

5

4.5

4
3.5

3
2.5

2

1.5
1

0.5

0

10

9
8

7

6
5

4
3

2
1

0

AdaBoost(8)

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000
ntree = 10000

e -hingeBoost(8)

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000
ntree = 10000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

5

4.5

4
3.5

3
2.5

2

1.5
1

0.5

0

10

9
8

7

6
5

4
3

2
1

0

16

20

40

60

80

20

40

60

80

Signal Efficiency (%)

Signal Efficiency (%)

Figure 11: Performance comparison of various Boostings with small tree size of 8 leaves
per tree.

1000 tree iterations for high signal eﬃciency, and the performance may get worse for
the low signal eﬃciency region if the boosting is over-trained (goes beyond the optimal
performance range). Similar plots for the four boosting algorithms with 8 leaves per
decision tree are shown in the Figure 11. Results for ǫ-HingeBoost with 30 and 8 tree
leaves are shown in the bottom right plots of Figures 10 and 11. The performance for
200 tree iterations seems worse than that for 100 tree iterations. This may indicate
that its performance is unstable in the ﬁrst few hundred tree iterations, but works well
after about 500 tree iterations. However, the overall performance of ǫ-Hinge boost is
the worst among the four boosting algorithms described above.

For some purposes, LogitBoost has been found to be superior to other algorithms[17].
For the MiniBooNE data, it was found to have about 10%-20% worse background con-
tamination for a ﬁxed signal eﬃciency than the regular AdaBoost. LogitBoost con-
verged very rapidly after less than 200 trees and the contamination ratio got worse
past that point. A modiﬁcation of LogitBoost was tried in which the convergence was
slowed by taking T (x) = P
2Tm(x), the extra factor of ǫ = 0.1 slowing the
weighting update rate. This indeed improved the performance considerably, but the
results were still slightly worse than obtained with AdaBoost or ǫ-Boost for a tree size
of 45 leaves. The convergence to an optimum point still took fewer than 300 trees,
which was less than the number needed with AdaBoost or ǫ-Boost.

M
m=1 ǫ × 1

Gentle AdaBoost and Real AdaBoost were also tried; both of them were found
slightly worse than the discrete AdaBoost. Relative error ratio versus signal eﬃciency
for various boosting algorithms are listed in Table.1.

6 Comparison of AdaBoost and Random Forests

The random forests is another algorithm which uses a ”majority vote” to improve
the stability of the decision trees. The training events are selected randomly with or
without replacement. Typically, one half or one third of the training events are selected
for each decision tree training. The input variables can also be selected randomly for
determining the tree splitters. There is no event weight update for the misclassiﬁed
events. For the AdaBoost algorithm, each tree is built using the results of the previous
tree; for the random forests algorithm, each tree is independent of the other trees.

Figure 12 shows a comparison between random forests of diﬀerent tree sizes and
Adaboost, both with 1000 tree iterations. Large tree size is preferred for the random
forests (The original random forests method lets each tree develop fully until all tree
leaves are pure signal or background). In this study a ﬁxed number of tree leaves were
used. The performance of the random forests algorithm with 200 or 400 tree leaves is
about equal. Compared with AdaBoost, the performance of the random forests method
is signiﬁcantly worse. The main reason for the ineﬃciency is that there is no event
weight update for the misclassiﬁed events. One of main advantages for the boosting
algorithm is that the weights of misclassiﬁed events are boosted which makes it possible
for them to be correctly classiﬁed in succeeding tree iterations.

Considering this advantage, an event weight update algorithm (AdaBoost) was used

17

Relative ratios for given signal eﬃciencies

Boosting
Algorithms
AdaBoost
AdaBoost
AdaBoost
AdaBoost
AdaBoost
AdaBoost
AdaBoost
AdaBoost
ǫ-Boost
ǫ-Boost
ǫ-Boost
ǫ-Boost
ǫ-Boost
ǫ-Boost
AdaBoost (b=0.5)
ǫ-Boost (b=0.5)
ǫ-Boost (b=0.5)
ǫ-Boost (b=0.5)
AdaBoost
AdaBoost
ǫ-Boost
ǫ-Boost
ǫ-LogitBoost
ǫ-LogitBoost
ǫ-HingeBoost
ǫ-HingeBoost
ǫ-LogitBoost
ǫ-HingeBoost
LogitBoost
LogitBoost
Real AdaBoost
Gentle AdaBoost
Random Forests(RF)
AdaBoosted RF

Parameters
β,ǫ (Nleaves, Ntrees)
0.3 (45,1000)
0.5 (45,1000)
0.8 (45,1000)
1.0 (45,1000)
0.5 (8,1000)
0.5 (20,1000)
0.5 (45,1000)
0.5 (100,1000)
0.005 (45,1000)
0.01 (45,1000)
0.02 (45,1000)
0.03 (45,1000)
0.04 (45,1000)
0.05 (45,1000)
0.5 (45,1000)
0.01 (45,1000)
0.03 (45,1000)
0.05 (45,1000)
0.5 (8,1000)
0.5 (8,5000)
0.01 (8,1000)
0.01 (8,5000)
0.01 (8,1000)
0.01 (8,5000)
0.01 (8,1000)
0.01 (8,5000)
0.01 (45,1000)
0.01 (30,1000)
1.0 (45,130)
0.1 (45,150)
(45,1000)
(45,1000)
(400,1000)
0.5 (100,1000)

30%
0.39
0.38
0.45
0.48
0.53
0.43
0.38
0.43
0.38
0.41
0.40
0.38
0.40
0.40
0.39
0.36
0.38
0.37
0.53
0.50
0.49
0.51
0.49
0.52
0.58
0.61
0.39
0.77
0.41
0.44
0.47
0.47
0.49
0.48

40%
0.49
0.50
0.54
0.55
0.63
0.58
0.50
0.51
0.47
0.50
0.48
0.48
0.50
0.47
0.47
0.46
0.45
0.44
0.63
0.60
0.55
0.55
0.59
0.57
0.66
0.69
0.50
0.80
0.55
0.52
0.57
0.54
0.63
0.56

50%
0.63
0.62
0.62
0.67
0.81
0.71
0.62
0.61
0.62
0.60
0.62
0.58
0.60
0.60
0.60
0.62
0.58
0.58
0.81
0.74
0.71
0.66
0.79
0.68
0.83
0.82
0.61
0.86
0.73
0.62
0.69
0.67
0.85
0.66

60%
0.80
0.78
0.82
0.81
1.11
0.93
0.78
0.76
0.84
0.80
0.77
0.75
0.75
0.79
0.76
0.83
0.76
0.74
1.11
0.98
0.93
0.86
1.07
0.89
1.09
1.05
0.82
0.96
0.98
0.82
0.82
0.83
1.29
0.81

70%
1.12
1.06
1.07
1.07
1.78
1.31
1.06
1.00
1.26
1.14
1.08
1.03
1.02
1.07
1.06
1.23
1.06
1.03
1.78
1.40
1.40
1.17
1.58
1.22
1.68
1.48
1.11
1.20
1.43
1.23
1.10
1.05
1.92
1.04

80%
1.73
1.63
1.60
1.60
3.21
2.20
1.63
1.45
2.23
1.87
1.71
1.62
1.57
1.61
1.58
2.00
1.65
1.58
3.21
2.52
2.44
1.82
2.95
2.01
2.88
2.49
1.84
1.80
2.40
2.00
1.60
1.56
3.50
1.58

Table 1: Relative error ratio versus signal eﬃciency for various boosting algorithms for
MiniBooNE data. Diﬀerences up to about 0.03 are largely statistical. b=0.5 means
smooth scoring function described in Section 9.

18

RForest(1/2,100 leaves)

RForest(1/2,200 leaves)

2.5

RForest(1/2,400 leaves)

RForest+Boost(1/3,100 leaves)

RForest+Boost(1/2,100 leaves)

1.5

AdaBoost(100 leaves)

3.5

3

2

1

0.5

o
i
t
a
R
 
e
v
i
t
a
l
e
R

Ntree = 1000

0

20

30

50

70
40
Signal Efficiency (%)

60

80

90

Figure 12: Performance comparison of AdaBoost, random forests and boosted random
forests with 1000 tree iterations.

to boost the random forests. The performances of the boosted random forests algorithm
are then signiﬁcantly better than those of the original random forests as can be seen
in Figure 12. The performance of the AdaBoost with 100 leaves per decision tree is
slightly better than that of the boosted random forests. Other tests were made using
one half training events selected randomly for each tree together with 30%, 50%, 80%
or 100% of the input variables selected randomly for each tree split. The performances
of the boosted random forests method using the AdaBoost algorithm are very stable.
The boosted random forests only uses one half or one third of the training events
selected randomly for each tree and also only a fraction of the input variables for each
tree split, selected randomly; This method has the advantage that it can run faster
than regular AdaBoost while providing similar performance. In addition, it may also
help to avoid over-training since the training events are selected partly and randomly
for each decision tree.

7 Post-ﬁtting of the Boosted Decision Trees

Some recent papers [18, 19, 20] indicate that post-ﬁtting of the trained boosted decision
trees may help to make further improvement. One possibility is that a selected ensemble
of many decision trees could be better than the ensemble of all trees. Here post-ﬁtting
of the weights of decision trees was tried. The basic idea is to optimize the boosting

19

performance by retuning the weights of the decision trees or even removing some of
them by setting them to have 0 weight. A Genetic Algorithm [21, 22] is used to optimize
the weights of all trained decision trees.

A new MC sample is used for this purpose. The MC sample is split into three
subsamples, mc1, mc2 and mc3, each subsample having about 26700 signal events and
21000 background events. Mc1 is used to train AdaBoost with 1000 decision trees. The
background eﬃciency for mc1, mc2 and mc3 for a signal eﬃciency of 60% are 0.12%,
5.15% and 4.94%, respectively. If mc1 is used for post-ﬁtting, then the corresponding
background eﬃciency can be driven down to 0.05%, but the background eﬃciency for
test sample mc3 is about 5.5%. It has become worse after post-ﬁtting. It seems that
it is not good to use same sample for the boosting training and post-ﬁtting. If mc2 is
used for post-ﬁtting, then the background eﬃciency goes down to 4.21% for the mc2,
and 4.76% for the testing sample mc3. The relative improvement is about 3.6% and the
statistical error for the background events is about 3.2%. Suppose the MC samples for
post-ﬁtting and testing are exchanged, mc3 is used for post-ﬁtting while mc2 is used
for testing. The background eﬃciency is 4.38% for training sample mc3 and 5.06% for
the testing sample mc2. The relative improvement is about 1.5%.

A second post-ﬁtting program was tried, the Pathseeker program of J.H. Fried-
man and B.E. Popescu[19, 20], a robust regularized linear regression and classiﬁcation
method. This program produced no overall improvement, with perhaps a marginal 4%
improvement for 50% signal eﬃciency. It seems that post-ﬁtting makes only a marginal
improvement based on our studies.

8 How to Select Input Variables

One of the major advantages of the boosted decision tree algorithm is that it can
handle large numbers of input variables as was pointed out previously[6]. Generally
speaking, more input variables cover more information which may help to improve
signal and background event separation. Often one can reconstruct several hundreds
or even thousands of variables which have some discriminant power to separate signal
and background events. Some of them are superior to others, and some variables may
have correlations with others. Too many variables, some of which are “noise” variables,
won’t improve but may degrade the boosting performance. It is useful to select the most
useful variables for boosting training to maximize the performance. New MC samples
were generated with 182 reconstructed variables. In order to select the most powerful
variables, all 182 variables were used as input to boosted decision trees running 150
tree iterations. Then the eﬀectiveness of the input variables was rated based on how
many times each variable was used as a tree splitter. The ﬁrst variable in the sorted
list was regarded as the most useful variable for boosting training. The ﬁrst 100 sorted
input variables were selected to train AdaBoost with β = 0.5, 45 leaves per decision
tree and 1000 tree iterations. The dependence of the number of times a variable is
used as a tree splitter versus the number of tree iterations is shown for some selected
input variables (Variables number 1, 5, 10, 20, 50, 80 and 100) in the top left plot with

20

Var-1
Var-5
Var-10
Var-20
Var-50
Var-80
Var-100

Var-1
Var-5
Var-10
Var-20
Var-50
Var-80
Var-100

0

0

250

500
Number of Tree Iterations

750

1000

10

2

10

3

10

Number of Tree Iterations

Ntree = 1000

Ntree = 1000

1200

1000

800

600

400

200

s
r
e
t
t
i
l
p
S
 
e
e
r
T
 
s
a
 
r
e
b
m
u
N

s
r
e
t
t
i
l
p
S
 
e
e
r
T
 
s
a
 
r
e
b
m
u
N

800

700

600

500

400

300

0

25

50
Number of Input Variables

75

100

0

25

50
Number of Input Variables

75

100

Figure 13: Top plots show the dependence of the number of times a variable is used as
a tree splitter versus the number tree iterations for some selected input variables. The
top left plot is linear scale and the top right plot is log scale. The number of times a
variable is used as a tree splitters versus all input variables with 1000 tree iterations
is shown in the bottom left plot. The bottom right plot shows the corresponding Gini
index contribution for all input variables.

s
r
e
t
t
i
l
p
S
 
e
e
r
T
 
s
a
 
r
e
b
m
u
N

10 3

10 2

10

1

1

14000

12000

10000

8000

6000

4000

n
o
i
t
u
b
i
r
t
n
o
C
 
x
e
d
n
i
 
i
n
i
G

21

linear scale and in the top right plot with log scale.

In this way, the ﬁrst 30, 40, 60, 80, 100, 120 and 140 input variables were selected
from the sorted list to train boosted decision trees with 1000 tree iterations. A com-
parison of their performance is shown in the left plot of Figure 14. The boosting per-
formance steadily improves with more input variables until about 100 to 120. Adding
further input variables doesn’t improve and may degrade the boosting performance.
The main reason for the degradation is that there is no further useful information in
the additional input variables and these variables can be treated as “noise” variables
for the boosting training. However, if the additional variables include some new infor-
mation which is not included in the other variables, they should help to improve the
boosting performance.

So far only one way to sort the input variables has been described. Some other ways
can also be used and work reasonably well as shown in the right plot of Figure 14. List1
means the input variables are sorted based on the how many times they were used as
tree splitters for 150 tree iterations, list2 means the input variables are sorted based
on their Gini index contributions for 150 tree iterations, and list3 means the variables
are sorted according to which variables are used earlier than others as tree splitters for
150 tree iterations. List1001, list1002 and list1003 are similar to list1, list2 and list3,
but use 1000 tree iterations. The ﬁrst 100 input variables from the sorted lists are used
for boosting training with 1000 tree iterations. The performances are comparable for
100 input variables sorted in diﬀerent ways. However, the boosting performances for
list1 and list3 are slightly better than the others.

If an equal number of input variables of 100 are selected from each list, the number
of variables which overlap typically varies from about 70 to 90 for the diﬀerent lists. In
other words, about 10 to 30 input variables are diﬀerent among the various lists. In spite
of these diﬀerences, the boosting performances are still comparable and stable. Further
studies with MC samples generated using varied MC input parameters corresponding
to systematic errors show that the boosting outputs are very stable even though some
input variables vary quite a lot. If these same varied MC samples are applied to the
ANNs, it turns out that boosted decision trees work signiﬁcantly better than the ANNs
for both event separation performance and for stability.

9 Tests of the Scoring Function

In the standard boost, the score for an event from an individual tree is a simple square
wave depending on the purity of the leaf on which the event lands. If the purity is
greater than 0.5, the score is 1 and otherwise it is −1.

One can ask whether a smoother function of the purity might be more appropriate.
If the purity of a leaf is 0.51, should the score be the same as if the purity were 0.99?
Two possible alternative scores were tested. Let z = 2×purity−1.

A. score = −1 +

2
e−az + 1
B. score = sign(z) × |z|b,

,

22

2.5

1.5

3

2

1

0.5

o
i
t
a
R
 
e
v
i
t
a
l
e
R

30 PID Vars

40 PID Vars

60 PID Vars

80 PID Vars

100 PID Vars

120 PID Vars

140 PID Vars

list1

list2

list3

list1001

list1002

list1003

AdaBoost, 45 leaves, Ntree = 1000

AdaBoost, 45 leaves, Ntree = 1000

0

20

30

40

50
Signal Efficiency (%)

60

70

80

0

20

30

40

50
Signal Efficiency (%)

60

70

80

Figure 14: Left: boosting performances with diﬀerent number of input variables. Right:
boosting performances with the 100 input variables selected by using diﬀerent ways.
Totally, 1000 tree iterations are used.

AdaBoost, 45 leaves, Ntrees = 500
Score = -1 + 2/(1 + e-a(2p-1))

Step Function (standard)

AdaBoost, 45 leaves, Ntrees = 1000
Score =   |2p-1|b, if p > 0.5, Signal leaf
Score = - |2p-1|b, if p ≤ 0.5, Bkgd leaf

b=0 (standard value)

b=0.2

b=0.5

b=2.0

b=0.33

b=1.0

b=3.0

o
i
t
a
R
 
e
v
i
t
a
l
e
R

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

a=1

a=3

a=5

20

30

40

50
Signal Efficiency (%)

60

70

80

20

30

40

50
Signal Efficiency (%)

60

70

80

Figure 15: Performance of AdaBoost with β = 0.5, 45 leaves per tree, and purity
threshold value 0.5 for various parameters a (left) and b (right) values.

2.5

2.25

2

1.75

1.5

1.25

1

0.75

0.5

0.25

o
i
t
a
R
 
e
v
i
t
a
l
e
R

2.5

2.25

2

1.75

1.5

1.25

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1

0.75

0.5

0.25

23

AdaBoost, 45 leaves, eff=60%

b=0 (step function - standard)

b=0.5 (smooth function)

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1.2

1.15

1.1

1.05

1

0.95

0.9

0.85

0.8

0.75

0

200

400

600

800

1000

Number of Tree Iterations

Figure 16: Performance of AdaBoost with b = 0 (step function) and b = 0.5 (smooth
function), β = 0.5, 45 leaves per tree, versus tree iterations.

where a and b are parameters.

Tests were run for various parameter values for scores A and B and compared
with the standard step function. Performance comparisons of AdaBoost for various
parameters a (left) and b (right) values are shown in Figure 15.

For a smooth function with b = 0.5, boosting performance converges faster than
the original AdaBoost algorithm for the ﬁrst few hundred decision trees, as shown in
Figure 16. However, no evidence was found that the optimum was reached any sooner
by the smooth function. The reason is that the smooth function of the purity describes
the probability of a given event to be signal or background in more detail than the step
function used in the original AdaBoost algorithm. With an increase in the number
of tree iterations, however, the “majority vote” plays the most important role for the
event separation. The ultimate performance of the smooth function with b = 0.5 is
comparable to the performance of the standard AdaBoost.

10 Some miscellaneous tests

In MiniBooNE, one is trying to improve the signal to background ratio by more than
a factor of 100. One might expect that one should start by giving the background a
greater total weight than the signal. In fact, giving the background two to ﬁve times
the weight of the signal slightly degraded the performance. Giving the background 0.5
to 0.2 of the weight of the signal gave the same performance as equal initial weights.

For one set of Monte Carlo runs the PID variables were carefully modiﬁed to be

24

AdaBoost, 45 leaves, Ntrees = 1000
Training events, Signal = 20k

bg=10k

bg=20k

bg=30k

bg=40k

bg=50k

bg=60k

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

o
i
t
a
R
 
e
v
i
t
a
l
e
R

30

35

40

50

45
65
55
Signal Efficiency (%)

60

70

75

80

Figure 17: Performance of AdaBoost with β = 0.5, 45 leaves per tree and 1000 tree
iterations, using various number of background events for training. 20,000 signal events
were used for the training.

ﬂat as functions of the energy of the event and the event location within the detector.
This decreased the correlations between the PID variables. The performance of these
corrected variables was compared with the performance of the uncorrected variables.
As expected, the convergence was much faster at ﬁrst for the corrected variable boost.
However, as the number of trees increased, the performance of the uncorrected variable
boost caught up with the other. For 1000 trees, the performance of the two boost tests
was about the same. Over the long run, boost is able to compensate for correlations and
dependencies, but the number of trees for convergence can be considerably shortened
by making the PID variables independent.

The number of MC events used to train the boosting eﬀectively is also an impor-
tant issue we have investigated. Generally, more training events are preferred, but
it is impractical to generate unlimited MC events for training. The performance of
AdaBoost with 1000 tree iterations, 45 tree leaves per tree using various number of
background events ranging from 10000 to 60000 for training are shown in Figure 17,
where the number of signal events is ﬁxed 20000. For the MiniBooNE data, the use
of 30000 or more background events works fairly well; fewer background events for
training degrades the performance.

25

11 Conclusions

PID input variables obtained using the event reconstruction programs for the Mini-
BooNE experiment were used to train boosted decision trees for signal and background
event separation. Numerous trials were made to tune the boosted decision trees. Based
on the performance comparison of various algorithms, decision trees with the AdaBoost
or the ǫ-Boost algorithms are superior to the others. The major advantages of boosted
decision trees include their stability, their ability to handle large number of input vari-
ables, and their use of boosted weights for misclassiﬁed events to give these events a
better chance to be correctly classiﬁed in succeeding trees.

Boosting is a rugged classiﬁcation method. If one provides suﬃcient training vari-
ables and suﬃcient leaves for the tree, it appears that it will, eventually, converge to
close to an optimum value. This assumes that ǫ for ǫ-Boost or β for Adaboost are not
set too large. There are modiﬁcations of the basic boosting procedure which can speed
up the convergence. Use of a smooth scoring function improves initial convergence. In
the last section, it was seen that removing correlations of the input PID variables im-
proved convergence speed. For some applications, the use of a boosted natural forests
technique may also speed the convergence.

For a large set of discriminant variables, several techniques can be used to select a
set of powerful input variables to use for training boosted decision trees. Post-ﬁtting of
the boosted decision trees makes only a marginal improvement in the tests presented
here.

12 Acknowledgments

We wish to express our gratitude to the MiniBooNE collaboration for the excellent work
on the Monte Carlo simulation and the software package for physics analysis. This work
is supported by the Department of Energy and the National Science Foundation of the
United States.

References

[1] Y. Freund and R.E. Schapire (1996), Experiments with a new boosting algorithm,

Proc COLT, 209–217. ACM Press, New York (1996).

[2] J. Friedman, Greedy function approximation: a gradient boosting machine, Annals
of Statistics, 29(5), 1189-1232(2001); J. Friedman, T. Hastie, R. Tibshirani, Additive
Logistic Regression: a Statistical View of Boosting, Annals of Statistics, 28(2), 337-
407(2000)

[3] L. Breiman, Random forests, University of California, Berkeley, technical report,

January 2001.

[4] E. Church et al., BooNE Proposal, FERMILAB-P-0898(1997).

26

[5] A. Aguilar et al., Phys. Rev. D 64(2001) 112007.

[6] B.P. Roe, H.J. Yang, J. Zhu, Y. Liu, I. Stancu, G. McGregor, Boosted decision trees
as an alternative to artiﬁcial neural network for particle identiﬁcation, Nuclear In-
struments and Methods in Physics Research A, 543 (2005) 577-584, physics/0408124.

[7] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, Classiﬁcation and Re-

gression Trees, Wadsworth International Group, Belmont, California (1984).

[8] R.E. Schapire, The boosting approach to machine learning: An overview, MSRI

Workshop on Nonlinear Estimation and Classiﬁcation, (2002).

[9] Y. Freund and R.E. Schapire, A short introduction to boosting, Journal of Japanese
Society for Artiﬁcial Intelligence, 14(5), 771-780, (September, 1999). (Appearing in
Japanese, translation by Naoki Abe.)

[10] J. Friedman, Recent Advances in Predictive (Machine) Learning, Proceedings of

Phystat2003, Stanford University, (September 2003).

[11] B.P. Roe et al., Reconstruction and Particle Identiﬁcation Algorithms for the Mini-
BooNE Experiment, BooNE-TN-117, March 18, 2004. (Journal paper under prepa-
ration.)

[12] I. Stancu et al., BooNE-TN-36, September 15, 2001; BooNE-TN-50, February 18,
2002; BooNE-TN-100, September 19, 2003. (Journal paper under preparation.)

[13] Y. Liu and I. Stancu, The Performance of the S-Fitter Particle Identiﬁcation,

BooNE-TN-141, August 25, 2004.

[14] H.J. Yang and B.P. Roe, The Performance of the R-Fitter Particle Identiﬁcation,

BooNE-TN-147, November 18, 2004.

[15] H. Ray, Validation of the PID Algorithm, BooNE-TN-142, September 9, 2004;

BooNE-TN-143, September 12, 2004.

H. Ray, W.C. Louis, Unisim Variations, BooNE-TN-165, July 27, 2005.

[16] T. Hastie, R. Tibshirani, J. Friedman, Elements of Statistical Learning, Data Min-
ing, Inference and Prediction, Chapter 10, Section 11, Page 324, Springer, (2001).

[17] M. Dettling, P. Buhlmann, Boosting for tumor classiﬁcation with gene expression

data, Bioinformatics, Vol.19, No.9, pp1061-1069, (2003)

[18] Z.H. Zhou, J.X. Wu, W. Tang, ensembling neural networks: many could be bet-
ter than all, Artiﬁcial Intelligence, 137(1-2):239-263,(2002); Z.H. Zhou, W. Tang,
Selective Ensemble of Decision Trees, NanJing University, (2003).

[19] J.H. Friedman, B.E. Popescu, Importance Sampled Learning Ensembles, Stanford

University, technical report, (2003).

27

[20] J.H. Friedman, B.E. Popescu, Gradient Directed Regularization, Stanford Univer-

sity, (2004).

[21] J.H. Holland, Adaptation in natural and artiﬁcial system, Ann Arbor, The Uni-

versity of Michigan Press, (1975).

[22] D. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning,

Addison-Wesley, (1988).

28


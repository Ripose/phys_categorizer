Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003

1

Revison Control in the Grid Era - the unmet challenge

Arthur Kreymer
Fermilab, Batavia, IL 60510, USA

3
0
0
2
 
n
u
J
 
4
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
6
3
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

As we move to distribute High Energy Physics computing tasks throughout the global Grid, we are encountering
ever more severe diﬃculties installing and selecting appropriate versions of the supporting products. Problems
show up at every level: the base operating systems and tools, general purpose utilities like root, and speciﬁc
releases of application code. I will discuss some speciﬁc examples, including what we’ve learned in commissioning
the SAM software for use by CDF. I will show why revision control can be a truly diﬃcult problem, and will
discuss what we’ve been doing to measure and control the situation.

1. Concerns

I’m here to talk about revision control because this
has emerged lately as an issue of major concern within
CDF, especially as we start to integrate our own code
with the SAM data handing system previously used
only at the Fermilab D0 experiment, and as we start
using Grid tools to submit jobs at remote sites.

We have considerable experience distributing the
CDF oﬄine analysis code on a wide variety of hard-
ware and software platforms, and in a number of run-
time environments ( online Level 3 triggers, online
monitoring, interactive and batch systems. )

When a user comes to us saying ”It’s Broken !”, we
need to know just what it is that they are running.
There can be a surprisingly large number of possibili-
ties if you stop to count them carefully. It is important
to do so occasionally, and to take steps to keep this
under some level of control. I will show several speciﬁc
examples.

Point releases are numbered like 4.8.1, and are
otherwise just like base releases. They have spe-
ciﬁc corrections for operations, and are usually
not on the main line of development. For exam-
ple, 4.8.4 for Production track restruction for
the 2003 winter conferences, and 4.9.1hpt2, for
physics analysis for the same conferences.

• Integration

Integration releases are (bi)weekly frozen re-
leases which have much less testing than base
release. We only require that the code compile
and link for integration releases. They provide
a stable base against which major code develop-
ment is conducted.

• Development

We build a release using the latest source code
from the head of the CVS code repository each
night, using the code present at midnight. We
also perform rudementary code execution validi-
tion nightly. This gives early feedback in case of
broken code or global problems.

2. CDF Releases

• Patch

First, let me outline what kinds of oﬄine software
releases we support, just so that the terminology is
understood.

Then let’s count how many varieties of code we
might be dealing with for a single given line of source
code, in the development release.

This is a worst case, but it’s what got me worrying
about this problem, and it is interesting to look at the
numbers.

Inevitably there are slight administrative code
changes need in the Level3, online monitoring,
and production farm operations which do not
merit a formal code release, but which need to
be reproducible from CVS tags. Patch releases
are a formalization of the usual developers tem-
porary working area, using a standard creation
script and named patch list.

You can ﬁnd a current list of base, point and inte-

gration releases on the web 1 .

2.1. Types of Releases

2.2. Development 2001

These are fully tested for speciﬁed purposes
(Production, Analysis ) Version numbers are for
example 4.8.0 for general purpose use.

Here we count the diﬀerent ways in which a given
line of code might have been compiled in the nightly
development release, in 2001 .

1http://cdfkits.fnal.gov/DIST/logs/rel/Releases.html

• Base

• Point

TUJT003

2

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003

• 5 - OS

We were building with Linux 2.0, Linux 2.2,
IRIX , OSF/1 and SunOS

• 3 - Optimization

We built variously with default, minopt (K1)
and maxtop (K3)-

• 3 - History

Some systems do full clean rebuilds daily. Some
systems force a full clean rebuild every Sunday.
Most do daily incremental builds, and rarely if
ever do full rebuilds. These three diﬀerent his-
tories could leave stale code in some libraries,
as gmake does not know how to clean up when
source code is removed. We have ways of forcing
global rebuilds when we think it is needed. This
is not usually a problem, but we have to worry
about it nevertheless.

interface, with static libraries and using the stan-
dard distributed libraries built in parallel. So the
IRIX/Linux choice was the most commmon issue.

But remember that the team of people support-
ing the code do have to be aware of all 2160 varia-
tions. There were people exercising all the options
listed above, in various combinations. It is essential
to track down just what a developer is doing before
starting to investigate a problem.

2.3. Development 2003

The situation was not just a relic of rapid transi-

tions during 2001.

Fewer people work out of the development release,
now that Integration releases are available. So these
choices are exercised less often by regular developers.

• 3 - Operating System

Linux 2.2, Linux 2.4, IRIX

You might ﬁnd code built with with the KAI
KCC 3, KCC 4 or gcc compilers.

We have dropped OSF1 and SunOS, and will
probably drop IRIX soon.

• 3 - Languages

• 4 - database

There was automatically generated code for con-
necting to text, mSQL, Oracle OCT or Oracle
OTL. So any calibration database code would
exist in for diﬀerent forms.

• 2 - Library mode

The libraries are mostly built for static linking.
But when for rapid development, developers of-
ten exercise an option to make local shared ob-
ject libraries.

• 2 - build style

Through early 2001, we had only built libraries
one at a time, using a single processor. Then
we transitioned, on the large central systems, to
doing parallel rebuilds. This is not trivial, as
part of the gmake process involved generating
headers and code which need to be subsequently
used by other packages. We think we have this
correctly set up now, but this was less obvious
in 2001.

The net result of these possibilities is that a given
single line of code in the development release could be
built 2160 diﬀerent ways.

It is clear impossible to exercise and test all of the

possibilities with our ﬁnite resources.

We survived because very few of the options were
exercised by normal users. Most developers ran under
Linux 2.2 or IRIX, with default optimization, with
weekly clean rebuilds, KAI 3, using the Oracle OCI

• 3 - Optimization

• 5 - Database

Debug (K0), default (K1) and maxopt

text, mSQL, Oracle OTL MySQL, ODBC

It looks like we have added more options here.
We’d like to cut back to text plus one ODBC
type API to all the back end databases, reducing
this to 2 varieties of code.

clean, weekly clean, stale just as before.

• 3 - History

• 2 - Languages

KAI KCC 4, gcc

• 2 - Codegen

We expect to drop KCC, moving to gcc 3.2+
before 2004.

We are testing a new simpliﬁed code generation
structure at the moment. Eventually this will
allow the single database API, making things
better. A this moment, though, it creates one
more option to track.

There are still a net 810 varieties possible for each
line of code. Again, most are not exercised. Almost
all development is now on Linux 2.4, default optimiza-
tion, Oracle OTL, using the Integration base release,
with KCC 4, and the old codegen. The parallel versus
serial builds are now considered equivalent.

TUJT003

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003

3

Table I Product versions

3. SAM versions

Product
cdfdab
cern
csl oﬄine
dcap
dddebugger
gdb
gsl
gtools
herwig
isajet
java
kai
lund
level3 oﬄine
msql
oracle client
pdf
perl
perl dbd oracle
root
stdhep
tcl
cdfdb data
geant
qq

version
v1 0
2000
v3 1
v2 24 f0203
v3 3 1
v5 0b external
v1 2
v2 4
v6 4a
v7 51a
v1 2 2b
v4 0f a
v6 203a
v2 1
v2 0 10
v8 1 7 lite
v8 04
v5 005
v5 005 817
v3 05 00a -q KC 4 0
v4 09
v8 3 1a
v0 external
v3 21 13 external
v9 1b external

2.4. Product versions

So far we have just discussed how many ways a sin-
gle line of code could get built within the CDF oﬄine
release. That release itself depends on many external
products, which we do not build as part of the release.
We try to take strong control of these versions, and
support only a single speciﬁc version of each for a
given CDF oﬄine code release.

A current snapshot is given in Table I
There are also a small number of products for which
a ﬁxed version is not appropriate, where we need to
use the ’current version.

These products provide overall infrastructure sup-
port (UPS, UPD), or provide services not tied to any
single release (diskcache i, kai key).

The development and current product versions are
updated by requiring each system to run a nightly
update of a ’development lite’ release of the CDF
code, containing a few standard maintenance and op-
erations scripts, database access conﬁgurations, other
small operations data ﬁles.

TUJT003

Having this experince in mind, it was natural when
we started to integrate the SAM data handling soft-
ware with the CDF oﬄine code to look at similar is-
sues.

Just as we did when looking at varieties of code
in the CDF development releases, we can look at va-
rieties of operating environments for the SAM code.
We encountered about 60 conﬁguration ﬁles, used by a
variety of programs, tailored for each of the following
:

• 2 - Experiments
D0 or CDF

• 3 - Databases

development, integration, or production

• 5 - Operating systems

IRIX, Linux, OSF/1, SunOS and generic

• - usage

User, Station, Stager, CORBA server

This makes 120 varieties to consider, clearly exces-

sive.

After some investigation, we found that the D0 ex-
periment does not use these conﬁguration ﬁles. CDF
runs SAM clients only under Linux, and does not want
the option of diﬀerent product versions for any of the
options listed above. So in this case, we can reduce the
complexity by simply removing the options entirely.

I have mentioned this small example mainly because
this is what immediately motivated the presentation
of this talk.

4. Java

There is the temptation to look to portable virtual
environments such as Java for a solution to portability
and revision control problems. So I’ll deliver a little
word of warning that things are not so simple.

At this time there are at least ﬁve versions of java
used in the CDF oﬄine environment, each somewhat
incompatible with the other in function or licensing.
I’m sure that even more versions are in use in the
online system.

• v1.1.6 on IRIX

• v1.2.2b on Linux

retired them

velopment

• v1.1.7 was used on SunOS and OSF1 before we

• v1.3 used for dcache data handling software de-

4

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003

Table II CDF Desktop Fermi Linux deployment

systems Fermi Linux

Table IV CDF oﬀsite kernels
systems kernel
1 2.2.16-3
1 2.2.16-3-ide
4 2.2.16-3smp

Table III CDF oﬀ site Linux systems

systems

type Linux

64
33
121
67

6.1.1
6.1.2
7.1.1
7.3.1a

1 RedHat 6.1
6 Fermi 6.1.1
2 RedHat 6.2
7 RedHat 7.1
11 Fermi 7.1.1
5 RedHat 7.2
3 Fermi 7.3.1

• v1.4 used for dcache deployment in production

at Fermilab

Even in the very simple oﬄine application,
where we use Java to convert data struction de-
scriptions into C++ calibration database inter-
face code (reading and writing plain text), we
have had severe portability problems, particu-
larly on SMP systems. The java application will
die with strange and inappropriate diagnostics,
something we’ve never really been able to ﬁnd a
cause for. At the moment we seem to be able to
work around this by setting environment vari-
able LD ASSUME KERNEL=2.2.5 ,

5. Operating systems

It is tempting when faced with operating system
release issues to demand that everyone run the same
system. But CDF is a worldwide collaboration. Most
of the systems are not under Fermilab control, and are
subject to legitemate local constraints.

Here is a little snapshot of RedHat releases, and

kernels in use within CDF on 22 March 2003.

The hundreds of centrally managed systems in the
Production farms and the Central Analysis Farm
batch system all run Fermi Linux 7.3 .

The oﬃce desktop systems in Table II and oﬀsite
systems in Table III are a very mixed bag. Note that
the oﬀsite survery only lists systems I can log into, and
does not include any of the large oﬀsite farm systems.
There are 21 diﬀerent Linux kernels in use oﬀsite,

each on 1 to 4 nodes, as shown in Table IV

TUJT003

1 2.2.17-14

1 2.2.19-6.2.1
1 2.2.19-6.2.16enterprise

3 2.4.3-12
1 2.4.3-12smp

2 2.4.5
1 2.4.5-4G-rtc

1 2.4.7-10
2 2.4.7-10smp

1 2.4.9-12smp
1 2.4.9-21smp
2 2.4.9-34

3 2.4.18
2 2.4.18-10smp
3 2.4.18-19.7.xsmp
1 2.4.18-5bigmem
1 2.4.18RUI2S3

2 2.4.19

Therefore, we try very hard, and change the code if
necessary, to keep the CDF oﬄine code independent
of the kernel.

6. root

Even for a single release of a single product, such as
root, there can be many varieties when faced with the
requirements of various experiments just at Fermilab.
We presently build 15 varieties of each release of root,
listed in Table V . We are working hard to reduce
this work load, by moving to a common gcc compiler,
and moving to enable C++ exception handling in the
CDF code.

Note that the UPS/UPD tools for selecting and us-
ing just the desired version of a product is essential for
survival If a given system can only have one version
of root in use, shared by all projects, then most users
will be unable to use it.

7. File systems

Likewise, there are many ﬁlesystems in use, and one
hopes that none of the oﬄine physics code depends on
these systems. This is not a trivial task.

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, California, March, 2003

5

are in negotiations right now over issues of mu-
tual trust and subsequent use of these certiﬁ-
cates for authorization.

• Authorization What is the user allowed to do.
The classic issues are ﬁle access (as in afs), job
submission/login, and ﬁle copying. In the Grid
this is generalized to a broader range of services,
hopefully with a single integrated set of autho-
rization tools.

• Batch systems The primary initial use of Grid
tools seems to be to access batch computing fa-
cilities. There are a large number of local batch
system, (lsf, pbs, fbsng, etc.),and eﬀort is under-
way to make this transparent to the end user.

• Database access It take more to run a job these
days than just handline input, output, and the
executeable environment. Database access to a
central server is ofter requires, raising issues of
network performance, or cacheing when multiple
tier servers are involved.

• Multi experiments It is quite nontrivial to man-
age the environment for even one experiment.
In the Grid world we are expected to share the
same environment. This will require careful at-
tention to detail to see that undesired couplings
do not arise.

9. Conclusions

A few simple items of advice seem to have emerged

• Count the varieties or conﬁgurations, and pay

Table V Fermilab varieties for root 3.05.03b

Flavor Variety

Linux+2.2 GCC 3 1 opt
Linux+2.2 GCC 3 1
Linux+2.2 KCC 4 0 opt
Linux+2.2 KCC 4 0

Linux+2.4 GCC 3 1 exception opt thread
Linux+2.4 GCC 3 1 opt
Linux+2.4 GCC 3 1
Linux+2.4 KCC 4 0 exception opt thread
Linux+2.4 KCC 4 0 opt
Linux+2.4 KCC 4 0

IRIX+6.5 GCC 3 1 exception opt thread
IRIX+6.5 GCC 3 1 opt
IRIX+6.5 GCC 3 1
IRIX+6.5 KCC 4 0 exception opt thread
IRIX+6.5 KCC 4 0 opt

Under Linux, one can use ext2, ext3, xfs and reiserfs

transparently.

There is also limited use of afs, but without Fer-
milab support. So we try to avoid doing things that
would break thisafs usage.

On the other hand, ﬁle systems like NTFS which
do not distinguish upper and lower case characters in
ﬁle names cannot be used safely, and we will not try
to ’ﬁx’ this in the CDF code.

8. Grid issues

It is hard to quantify the complexity of the Grid
space, as the tools to be used are just emerging. Some
of the choices will involve :

Who is the user requesting services ? There are
a variety of methods in use outside the Grid (
kerberos 4 and 5, ssh, etc.) Various Grid certiﬁ-
cate authorities have been commissioned, and

• Authentication

• Reduce cross-project couplings

• Reduce sensitivity to revisions (ﬁle systems, ker-

attention.

nels, etc)

TUJT003


6
0
0
2
 
v
o
N
 
7
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
6
2
1
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Cascade Training Technique for Particle
Identiﬁcation

Yong Liu ∗, Ion Stancu

Department of Physics and Astronomy
University of Alabama, Tuscaloosa, AL 35487

Abstract

The cascade training technique which was developed during our work on the Mini-
BooNE particle identiﬁcation has been found to be a very eﬃcient way to improve
the selection performance, especially when very low background contamination lev-
els are desired. The detailed description of this technique is presented here based
on the MiniBooNE detector Monte Carlo simulations, using both artiﬁcal neural
networks and boosted decision trees as examples.

Key words: Neural networks, Computer data analysis, Neutrino oscillations
PACS: 07.05.Mh, 29.85.+c, 14.60.Pq

1 Introduction

Particle identiﬁcation (PID) is the procedure of selecting signal events while
rejecting background events, and is a key step in the data analysis for essen-
tially all particle physics experiments. For some experiments, the search for a
possibly very small signal within a large amount of data, such as MiniBooNE,
requires an extremely powerful separation of signal from background events.

MiniBooNE [1] is a short baseline accelerator neutrino experiment currently
running at the Fermi National Accelerator Laboratory. Its primary goal is to
deﬁnitely conﬁrm or rule out the potential ¯νµ → ¯νe oscillation signal claimed
by the LSND experiment [2], by looking in the CP-conjugate channel for the νe
appearance in a νµ beam. The νe appearance is identiﬁed by the presence of an
isolated electron from the charged-current νeC → e−X reaction on the carbon
∗ Corresponding author.

Email address: yongliu@fnal.gov (Yong Liu).

Preprint submitted to Elsevier

17 January 2014

atoms of the mineral oil active medium. In addition to the intrinsic νe contam-
ination in the beam, the main backgrounds to the oscillations analysis come
from misidentiﬁed muons and neutral pions from the νµ quasielastic scattering
and neutral-current π0 production, respectively. In order to reach the design
sensitivity, the MiniBooNE PID algorithms must achieve an electron selection
eﬃciency of better than 50%, for an overall background contamination level
of approximately 1%.

The crucial points for any PID-based analysis are both the input variables and
the underlying algorithm. Each variable must (obviously) have a relatively
good separation between signal and background events, while in addition it
must yield a good agreement between data and Monte Carlo (MC) in both
distributions and correlations. Once the set of PID variables has been iden-
tiﬁed, the next step is to chose the PID algorithm. In the case in which the
signal event statistics are large enough and some few variables already show
a very clear separation between signal and background, the simple, straight
cuts method may be preferred. Otherwise, the linear or nonlinear combination
techniques of the underlying variables, such as Fisher discriminants [3], arti-
ﬁcial neural networks (ANN) [4], or boosted decision trees (BDT) [5] should
be applied. While ANNs have been relatively widely accepted and success-
fully used in high energy experimental data analysis in the past decades [6],
BDTs as a novel and powerful classiﬁcation technique have been ﬁrst intro-
duced to the particle physics community within the MiniBooNE collaboration
only recently [7]. Since then it has also been applied to the radiative leptonic
decay identiﬁcation of B-mesons in the BaBar experiment [8], as well as to
supersymmetry searches at the LHC [9].

Generally, given a particular set of PID variables and a particular selection
algorithm, the maximum PID performance is essentially ﬁxed, up to relatively
small variations induced by adjusting some internal settings in the PID code
itself, e.g., the learning rate, number of hidden nodes, etc., in the case of neural
networks, or the number of leaves, minimum number of events in a leaf, etc.,
in the case of decision trees. In addition to the PID variables and the algo-
rithm, the training event sets also play a crucial role in the PID performance.
Therefore, a careful selection of the training event sample may signiﬁcantly
improve the overall PID performance, as developed within the cascade training
technique (CTT) [10].

In this paper, both ANNs and BDTs are taken as examples to describe the
cascade training procedure, as based on the MiniBooNE detector MC simula-
tions. In Section 2 we describe brieﬂy the MiniBooNE PID variables used in
this study, as well as a systematic procedure for the variable selection. Sec-
tion 3 describes the cascade training technique, while the results are discussed
in Section 4. Our conclusions are summarized in Section 5.

2

2 PID variable construction and selection

The variable construction and selection is naturally a ﬁrst concern for an
eﬃcient PID. The diﬀerence in the information content between signal and
background events based on which the separation is made has to be extracted
from the variables via some classiﬁcation algorithm, such as Fisher discrimi-
nants, neural networks, decision trees, etc. The variable set used for diﬀerent
experiments and diﬀerent analysis goals will naturally diﬀer from each other.
However, as already mentioned before, some fundamental requirements have
to be satisﬁed, namely: (i) the variable distributions must show some separa-
tion between signal and background events, and (ii) the variable distributions
and their correlations must show good data/MC agreement. The ﬁrst require-
ment is directly connected with the maximum eﬃciency of the PID, while the
second one guarantees the relability of PID output.

The MiniBooNE detector is a spherical steel tank of 610-cm radius, ﬁlled with
800 metric tons of ultra-pure mineral oil. An optical barrier divides the detec-
tor into an inner region of 575-cm radius, viewed by 1280 photomultiplier tubes
(PMT), while the 35-cm-thick outer volume, viewed by 240 PMTs, serves as
an active veto shield. Neutrino interactions in the mineral oil are detected via
both Cherenkov and scintillation light with a ratio of 3:1 for highly relativis-
tic particles. The particle identiﬁcation is essentially based on the time and
charge distribution recorded at the PMTs.

Every event in the detector is subject to three diﬀerent maximum-likelihood
reconstructions, assuming that the underlying event was an electron, a muon,
or a π0. Ideally, just the e/µ and e/π likelihood ratios should be enough to
achieve a powerful separation of the electron signal from the muon and neutral
pion backgrounds. However, this is not the case. Therefore, in addition to
these two variables, a large set of variables is deﬁned, based on the corrected
time at the PMTs and the charge angular distribution with respect to the
reconstructed event direction. These variables have been designed to exploit
the diﬀerent topologies of the signal and background events, such as short and
fuzzy tracks for electrons, long tracks and sharp rings for muons, two tracks
for pions, etc. For any given PMT, at a distance ri from the event vertex,
the corrected time t(i)
corr is the measured PMT time, ti, corrected for the event
time, t0, and the time of ﬂight:

t(i)
corr = ti − t0 −

ri
cn

,

where cn is the speed of light in oil, while cos θi mesures the cosine of the an-
gle between the PMT location and the event direction. By binning the sets of
(t(i)
corr) for the hit PMTs and (cos θi) for all PMTs (including the no-hit ones)

3

and recording the hits, charge, time, likelihoods information, etc., in each
bin, one can construct several hundreds of potential PID variables. In addi-
tion, some reconstructed physical observables, such as the ﬁtted Cherenkov-
to-scintillation ﬂux ratio, the event track length, the invariant π0 mass, etc.
can also serve as PID variables, and they are found to be quite powerful.

In principle, all variables which pass the data/MC comparison test can be
used as PID input variables. However, it is rather well-known that more input
variables does not necessarily imply a better separation in the output, as the
PID performance may saturate or even degrade after the number of input
variables exceeds a certain limit that depends on the problem itself, as well as
the PID algorithm. In the particular case of MiniBooNE, we have found that
BDTs can easily handle large numbers of inputs, whereas ANNs appear to
be limited to several tens of variables [7]. Therefore, in order to demonstrate
the advantage of the cascade training technique, we have decided to limit the
number of input variables to some arbitrary small number, e.g., Nvar = 20,
which can be easily handled by the ANNs. However, the 20 variables used by
the ANNs may be diﬀerent than those used by the BDTs, as we want to utilize
a set of inputs that maximizes the performance of that particular algorithm.
In the following paragraph we brieﬂy discuss the algorithms for chosing a small
subset of input variables from a large pool of input variables.

Boosted decision trees oﬀer several natural ways of ordering the variables
and identifying a subset of Nvar inputs for maximum performance. Once the
decision trees have been built with all available variables, the criteria by which
the input variables can be easily ordered are, for example: (a) the order in
which they are used, (b) the frequency with which they are used, or (c) the
number of events they split. Unfortunately, the neural networks do not allow
for such a classiﬁcation. However, an alternative, systematic procedure for
ordering the input variables can be deﬁned as follows: starting from the i-th
variable, scan all other variables and build a neural network or a decision tree
using only these two inputs, with the j-th variable yielding the best separation.
Using variables (i, j), scan again all other variables and search for a third
variable, say the k-th one, which gives a maximum performance for either a
new network or a new decision tree. The procedure is then repeated to ﬁnd the
4th, 5th, etc., variable, until the desired subset size, Nvar, is reached. Note that
starting from diﬀerent variables may result in a diﬀerent set of variables for a
given ﬁnal size Nvar, so an additional loop over the ﬁrst variable is needed.

A direct comparison of the diﬀerent ordering procedures for BDTs is illus-
trated in Fig. 1. The three natural variable ordering schemes for BDTs appear
to have similar performance for Nvar > 15, and they appear to be quite close
to saturation at Nvar = 50, the maximum value plotted here. The systematic
ordering appears to have a slightly better performance than the natural order-
ing for Nvar < 20, in particular when using a relatively low number of inputs.

4

The systematic ordering has been carried out rigurously only to Nvar = 20
for computational reasons, as the CPU time increases dramatically with Nvar.
The eﬃciencies displayed here for 20 < Nvar ≤ 25 have been obtained by
ﬁxing the ﬁrst 20 variables and simply searching for additional new variables,
which may not necessarily yield the best eﬃciencies.

(a)

(b)

(c)

y
c
n
e
c
i
f
f

i

e

 
l

a
n
g
S

i

i

y
c
n
e
c
i
f
f
e
 
l
a
n
g
S

i

i

y
c
n
e
c
i
f
f
e
 
l
a
n
g
S

i

0.6

0.4

0.2

0

0.6

0.4

0.2

0

0.6

0.4

0.2

0

0

10

20

30

40

50

Number of input variables

Fig. 1. Signal eﬃciency at 1% background contamination versus number of input
variables for boosted decision trees. The variable ordering is by (a) usage ordering,
(b) usage frequency, (c) number of events they split, while the systematic ordering
– represented by the empty circles – has been superimposed on all of them.

For consistency, we apply this systematic procedure for the variable search to
both networks and boosting, using Nvar = 20, and turn now to describing the
cascade training.

3 The Cascade Training Technique

The idea of the CTT came originally from the combination of the ANN and
BDT outputs [11]. In general, the ANN output is not 100% correlated with

5

the BDT output, and hence one can expect some improvement in the overall
performance of the PID selection algorithm by combining them together.

Figure 2 below shows the BDT versus the ANN output for the same number
of signal and background events. For a 1% background contamination, an
ANN cut of about OAN N > 0.98, or a BDT cut of about OBDT > 1.15 is
necessary. By direct observation it is easy to see that the combined OR region
(OAN N > 0.98 or OBDT > 1.15) helps improve the signal eﬃciency relative
to a single cut based on either one of the two outputs: the eﬃciency yields
now 62.5%, for a slightly higher contamination level of 1.26%. Insisting on the
nominal background contamination level, a signal eﬃciency of 54.7% can be
reached for OAN N > 0.99 or OBDT > 1.22, or even 55.9% for OAN N > 0.92
and OBDT > 1.03 by optimizing the cuts.

(a)

(b)

t
u
p
t
u
o
 
T
D
B

t
u
p
t
u
o
 
T
D
B

3

2

1

0

-1

-2

-3

-4

3

2

1

0

-1

-2

-3

-4

0

0.2

0.4

0.6

0.8

1

ANN output

Fig. 2. BDT versus ANN output for (a) signal and (b) background events. The
vertical and horizontal dashed lines indicate the ANN and BDT cuts for a 1%
background contamination, respectively.

There are three ways to combine the ANN and the BDT outputs in a relatively
straightforward manner. The ﬁrst is to optimize the cuts on both of them –
as discussed in the previous paragraph. The second is to take them as inputs

6

to another ANN or BDT. The third is to use the output of a single algorithm
(either ANN or BDT) as a cut to remove a large portion of the background
events, and then force another algorithm to focus on separating the remaining
signal and background events – which are now harder to separate. We have
found that this latter technique proves to be the most eﬃcient one, especially
in the very low background contamination region.

The CTT procedure can be formulated as follows:

(a) Prepare three independent sets of samples A, B, and C, where the num-
ber of background events in B is several times larger than that in A.

(b) Train the PID algorithm (ANN or BDT) with sample A, where the input

variables may be selected as described in Section 2, if needed.

(c) Examine the PID output distribution using sample C in order to deter-
mine the PID cut value, which is set by the point where the signal and
background distributions cross each other, as shown in Fig. 3. The cross-
ing point obviously depends on the relative number of signal and back-
ground events in the sample; in this study we have used equal numbers
of signal and background events, for both the training and test samples.

(d) Select the training events from sample B using the PID cut determined

by the procedure in step (c) above.

(e) Train another PID algorithm (ANN or BDT) with the training events ob-
tained in step (d) above, where another variable selection (as described
in Section 2) may be applied.

(f) Test the performance of the resulting PID algorithm with sample C.

Thus, the ﬁrst ANN or BDT algorithm as built in step (b) only serves to de-
termine the cut used to select the training events for the second PID algorithm
training in step (e), and hence the name of the cascade training technique.

4 Results

Figure 3 shows the ANN/BDT output distributions from sample C after the
ﬁrst step training of the two algorithms on sample A. In this particular case
the signal/background crossing occurs at OAN N = 0.36 for neural networks
and OBDT = 0 for the boosted decision trees. Therefore, the events with
OAN N > 0.36 or OBDT > 0 are selected to form the restricted sample of
events, B’ and B”, respectively, used for the second step training. In our

7

case approximately 90% of the signal events pass either one of these cuts,
while about 90% of the background events are rejected. Note that the two
peaks seen in the BDT output distribution of background events in Fig. 3(b)
(dashed histogram) represent the two diﬀerent bakgrounds in MiniBooNE:
the left-most peak around OBDT = −4 corresponds to muon events, which
are relatively easy to identify, while the right peak at about OBDT = −1
corresponds to neutral pion events, which are harder to identify.

(a)

Second step training

Background

Signal

0

0.2

0.4

0.6

0.8

1

ANN output

(b)

Second step training

Background

Signal

s
t
n
e
v
E

10 4

10 3

10 2

s
t
n
e
v
E

4500

4000

3500

3000

2500

2000

1500

1000

500

0

-5

-4

-3

-2

-1

0

1

2

3

BDT output

Fig. 3. PID output distributions of the test events after the ﬁrst step training for (a)
ANNs and (b) BDTs. The vertical dashed lines indicate the cuts applied to select
the training events for the second step training.

The eﬃciencies of the new ANN and BDT algorithms, trained on the new
samples, B’ and B”, respectively, are illustrated in Fig. 4 as a function of the
background contamination level, along with the eﬃciencies of the correspond-
ing algorithms trained on the common sample A. The variable sets used in
the ﬁrst and second step training are identical (although they may be diﬀerent
between ANNs and BDTs).

At 1% background contamination, the ANN-based signal eﬃciency increases

8

(a)

(b)

i

y
c
n
e
c
i
f
f
e
 
l
a
n
g
S

i

i

y
c
n
e
c
i
f
f
e
 
l
a
n
g
S

i

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

-3

10

-2

10

10
Background contamination

-1

Fig. 4. Signal eﬃciency versus background contamination for (a) ANNs and (b)
BDTs for the ﬁrst and second step training (red circles and blue stars, respectively).

from about 39% to 60% after the cascade training, which represents a 50%
improvement. This improvement is even more signiﬁcant at lower background
contamination levels, where the signal eﬃciency can more than double. For
boosting however, the improvement by the cascade training appears to be less
signiﬁcant, as it raises the signal eﬃciency from about 54% to 63% (an im-
provement of 17%) at 1% background contamination. As in the ANN case, the
relative improvement is also more signiﬁcant at lower levels of contamination.

The lower gain in improvement seen when using boosted decision trees may
indicate that the boosting algorithm is already powerful enough to exhaust to
a large extent the diﬀerent information content between signal and background
events after the ﬁrst training step. Nonetheless, while the BDTs may provide
a superior PID performance than the conventional ANNs (54% versus 39%),
the diﬀerence after the CTT is much reduced (63% versus 60%). This implies
that the CTT can signiﬁcantly help the classiﬁcation algorithms exhaust any
useful signal-to-background diﬀerence, and push the algorithms to do their
best in separating signal from background.

9

The eﬃciencies at the 1% contamination level are summarized in Table 1. In
addition, the table also gives the eﬃciencies at a lower contamination level,
0.5%, which shows that the relative gains obtained by using the CTT are more
signiﬁcant at lower levels of background contamination.

First

Second

Eﬃciency (%) Eﬃciency (%)

algorithm algorithm at 0.5% BCL

at 1.0% BCL

ANN

ANN

ANN

BDT

BDT

BDT

ANN

BDT

–

–

BDT

ANN

8.4

32.5

39.8

25.0

37.0

34.2

38.7

60.3

67.6

54.0

62.8

55.3

Table 1
Signal eﬃciencies at two diﬀerent background contamination levels (BCL) for the
cascade training after initial selection based on ANNs (top half), and on BDTs
(bottom half).

5 Conclusions

In conclusion, a very eﬃcient way to improve ANN- or BDT-based particle
identiﬁcation is introduced, namely the cascade training technique. The pro-
cedure is described in detail, as well as the relevant variable construction and
selection method.

Our study shows that the CTT can help both ANN and BDT algorithms ex-
haust the available information contained in the input variables, while signif-
icantly improving the PID performance in the low background contamination
regions. However, the results reported here are based only on the MiniBooNE
detector MC. The relative improvement obtained by the CTT should depend
on the concrete experiment/application, variable set, and analysis goal. It
could be less, or even more than the numbers presented in this paper.

The intuitive explanation of the high eﬃciency of CTT may be that if the
algorithm can separate hard-to-identify events, it may be able to identify eas-
ily separable events quite naturally. Therefore, the strategy is to force the
algorithm to focus on learning the diﬀerent information content from the very
signal-like background events and true signal events, while disregarding some
number of background-like signal events. Based on our experience, we believe

10

that in general, in multi-component background composition case, the CTT
should be a very eﬃcient way to improve the PID eﬃciency.

Finally, this technique reveals that in addition to the search of good input
variables and application of a powerful classiﬁcation algorithm, an apropriate
manipulation of the training event selection opens another way to improve the
eﬃciency of particle identiﬁcation.

Acknowledgements

We are grateful to the entire MiniBooNE Collaboration for their excellent
work on the Monte Carlo simulations and the software packages for physics
analysis. It is a great pleasure for Y. Liu to dedicate this paper to his Ph.D.
supervisor, Prof. Mo-Lin Ge, on the occasion of his seventieth birthday.

This work has been supported by the US-DoE grant numbers DE-FG02-
03ER41261 and DE-FG02-04ER46112.

References

[1] E. Church et al., A proposal for an experiment to measure νµ → νe oscillations
and νµ disappearance at FermiLab Booster: BooNE. FERMILAB-P-0898, 1997.

[2] A. Aguilar et al., Phys. Rev. D64, 112007 (2001).

[3] G. Cowan, Statistical Data Analysis. Clarendon Press, Oxford (1998). B. Roe,
Event Selection Using an Extended Fisher Discriminant Method, PHYSTAT-
2003, SLAC, Stanford, CA, September 8–11, 2003.

[4] http://www.thep.lu.se/ftp/pub/LundPrograms/Jetnet/

[5] Y. Freund and R. E. Schapire, A Short Introduction to Boosting, Journal of
Japanese Society for Artiﬁcial Intelligence, 14, 771 (1999). R. E. Schapire,
A brief Introduction to Boosting, Proceedings of the Sixteenth International
Joint Conference on Artiﬁcial Intelligence, 1999. R. Meir and G. Ratsch, An
introduction to boosting and leveraging, In S. Mendelson and A. Smola, editors,
Advanced Lectures on Machine Learning, LNCS, pages 119-184. Springer, 2003.

[6] H. Abramowicz, A. Caldwell and R. Sinkus, Nucl. Instrum. Meth. A365, 508
(1995). H. Abramowicz, D. Horn, U. Naftaly and C. Sahar-Pikielny, Nucl.
Instrum. Meth. A378, 305 (1996). M. Justice, Nucl. Instrum. Meth. A400,
463 (1997). B. Berg and J. Riedler, Comput. Phys. Commun. 107, 39 (1997).
T. Maggipinto, G. Nardulli, S. Dusini, F. Ferrari, I. Lazzizzera, A. Sidoti, A.
Sartori and G. P. Tecchiolli, Phys. Lett. B409, 517 (1997). S. Chattopadhyaya,

11

Z. Ahammed, Y. P. Viyogi, Nucl. Instrum. Meth. A421 558 (1999). D0
Collaboration, B. Abbott, et al., Neural Networks for Analysis of Top Quark
Production, Fermi-Conf-99-206-E. D0 Collaboration, V. M. Abazov et al., Phys.
Lett. B517, 282 (2001). D. V. Bandourin and N. B. Skachkov, JHEP 0404, 007
(2004). S. Forte, L. Garrido, J. I. Latorre and A. Piccione, JHEP 0205, 062
(2002). J. Rojo, JHEP 0605, 040 (2006).

[7] J. Zhu, H.-J. Yang and B. P. Roe, νe and π0 Separation in the MiniBooNE
Experiment by Using the Boosting Algorithm, MiniBooNE-TN-112, January 9,
2004. B. P. Roe, H.-J. Yang, J. Zhu, Y. Liu, I. Stancu and G. McGregor, Nucl.
Instrum. Meth. A 543, 577 (2005).

[8]

I. Narsky, Optimization of Signal Signiﬁcance by Bagging Decision Trees,
physics/0507157.

[9] J. Conrad and F. Tegenfeldt, Applying Rule Ensembles to the Search for Super-

Symmetry at the Large Hadron Collider, JHEP 0607, 040 (2006).

[10] Y. Liu and I. Stancu, Energy and Geometry Dependence of ParticleID and
Cascade Artiﬁcal Neural Network and Boosting Training, MiniBooNE-TN-178,
March 1, 2006.

[11] Y. Liu and I. Stancu, The Performance of the S-Fitter Particle Identiﬁcation,

MiniBooNE-TN-141, August 25, 2004.

12


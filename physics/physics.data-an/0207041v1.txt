2
0
0
2
 
l
u
J
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
4
0
7
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

HAUSDORFF MOMENT PROBLEM VIA FRACTIONAL MOMENTS

1. Introduction
In Applied Sciences a variety of problems, formulated in terms of linear boundary values or
integral equations, leads to a Hausdorﬀ moment problem. Such a problem arises when a given
sequence of real numbers may be represented as the moments around the origin of non-negative
measure, deﬁned on a ﬁnite interval, typically [0, 1]. The underlying density f (x) is unknown,
0 xjf (x)dx, j = 0, 1, 2, ...,with µ0 = 1, are known. Next, through
while its moments µj =
a variety of techniques, for practical purposes f (x) is recovered by taking into account only a
R
M
j=0. Such a process implies that f (x) is well-characterized by its ﬁrst few
ﬁnite sequence
moments. On the other hand, it is well known that the moment problem becomes ill-conditioned
when the number of moments involved in the reconstruction increases [1,2]. In Hausdorﬀ case,
once ﬁxed (µ0, ..., µM −1), the moment µM may assume values within the interval [µ
M ],
where [3]

M , µ+

µj}

{

−

1

If one considers the approximating density fM (x) = exp(
tion, constrained by the ﬁrst M moments [4], then its entropy H[fM ] =
satisﬁes

M
j=0 λjxj) by entropy maximiza-
1
0 fM (x) ln fM (x)dx
R

P

−

−

µ+

M −

µ

−
M ≤

2

−2(M −1)

lim
µM →µ

±
M

H[fM ] =

−∞

Such a relationship is satisﬁed by any other distribution constrained by the same ﬁrst M mo-
ments, since fM (x) has maximum entropy. On the other hand f (x) and fM (x) have the same
ﬁrst M moments and as a consequence, as we illustrate in section 3, the following relationship
holds

I(f, fM ) =:

f (x) ln

dx = H[fM ]

H[f ].

(1.3)

1

Z
0

f (x)
fM (x)

−

Here H[f ] is the entropy of f (x), while I(f, fM ) is the Kullback-Leibler distance between f (x)
and fM (x).
Equations (1.1)-(1.3) underline once more the ill-conditioned nature of the moment problem.
The ill-conditioning may be even enlightened by considering the estimation of the parameters λj
of fM (x). The λj calculation leads to minimize a proper potential function Γ(λ1, ..., λM )[Kesa
4], with

min
λ1,...,λM

Γ(λ1, ..., λM ) = min

ln

exp(

λ1,...,λMh

(cid:16)Z

0

λjxj)dx
(cid:17)

+

λjµj

.
i

Xj=1

−

Xj=1

(1.4)

1

M

M

fM (x) satisﬁes the constraints

µj =

xj exp(

λkxk1)dx,

j = 0, ..., M

1

Z

0

M

−

Xk=0

Letting µ = (µ0, ..., µM ) and λ = (λ0, ..., λM ), (1.5) may be written as the map

(1.1)

(1.2)

(1.5)

(1.6)

Then the corresponding Jacobian matrix, which is up to sign a Hankel matrix, has conditioning
(1 + √2)4M /√M [5]. All the previous remarks lead to the conclusion that f (x) may
number
be eﬃciently recovered from moments only if few moments are requested. In other terms, f (x)
may be recovered from moments if its information content is spread among ﬁrst few moments.

≃

µ = φ(λ)

1

In this paper we are looking for a way to overcome the above-quoted diﬃculties in recovering
∞
j=0 to be
f (x) from moments. First of all, we assume the inﬁnite sequence of moments
known. Then, from such a sequence, we calculate fractional moments

µj}

{

E(X αj ) =:

xαj f (x)dx =

bn(αj)µn, αj > 0

(1.7)

∞

Xn=0

where the explicit analytic espression of bn(αj) is given by (2.5). Finally, from a ﬁnite num-
M
j=0 λjxαj ) by entropy
ber of fractional moments
maximization [4]. The exponents

M
j=1, we recover fM (x) = exp(
M
j=1 are chosen as follows

E(X αj )

P

−

{

M
j=1 : H[fM ] = minimum

(1.8)

M
j=1, according to (1.8), leads to a density fM (x) having minimum distance

The choice of
αj}
{
from f (x), as stressed by (1.3).
Remark. If the information content of f (x) is shared among ﬁrst moments, so that ME ap-
proximant fM (x) represents an accurate approximation of f (x), then fractional moments may
be accurately calculated by replacing f (x) with fM (x). As a consequence, function fM (x) con-
norm to f (x) [6], and the error obtained replacing f (x) with
verges in entropy and then in L1−
fM (x)

1

Z

0

}
αj}
{
αj}

{

Ef (X αj )

EfM (X αj )

|

−

1

1

xαj

|≤ Z

0

f (x)

fM (x)

dx

|

−

|

≤

≤ Z

0 |

−

f (x)

fM (x)

dx

2(H[fM ]

H[f ])

|

≤

p

−

may be rendered arbitrarily small by increasing M (inequalities in (1.9) are proved in section
3).

2. Fractional moments from moments
Let X a continuous random variable with density f (x) on the support [0, 1], with moments of
order s, centered in c, c

IR

∈

µs(c) := IE [(X

c)s] =

−

1

(x

Z

0

−

c)s f (x) dx,

s

∗

IN

= IN

∈

0

.

∪ {

}

and moments from the origin µs =: µs(0) related to moments generically centered in c through
the relationship

µs =

s

Xh=0

s
h(cid:19)

(cid:18)

cs−h µh(c),

∗

.

IN

s

∈

It is well known the relationship similar to (2.2) which permits to calculate the (fractional)
IR+ (which replaces αj for notational convenience as in (1.7) and (3.2))
moment of order s
involving all the central moments of a given distribution about the point c.
Firstly, by deﬁnition of noncentral moment of order s, we can write IE(X s) =
then, by Taylor expansion of xs around c, where c

0 xsf (x)dx and
R

(0, 1), we have

∈

1

(1.9)

(2.1)

(2.2)

(2.3)

xs =

[xs](n)
x=c

(x

−
n!

∈
c)n

∞

Xn=0
∞

Xn=0
∞

Xn=0

=

=

s
n(cid:19)

(cid:20)(cid:18)

n! xs−n

(cid:21)x=c

(x

c)n

−
n!

s
n(cid:19)

(cid:18)

cs−n(x

c)n

−

2

x=c indicates the n-th derivative of the function k(x) wrt x, evaluated at c.

where [k(x)](n)
Taking the expectation on both sides of the last equation in (2.3), we get the required relation-
ship

IE (X s) =

s
n(cid:19)

(cid:18)

cs−nIE [(X

c)n]

−

∞

Xn=0
∞

Xn=0

=

bn µn(c)

bn =

s
n(cid:19)

(cid:18)

cs−n, n

∗

IN

∈

where

represents the coeﬃcient of the integral n-order moment of X centered at c.
The formulation of the s-order fractional moments as in (2.4) shows some numerical instabilities
which depend on the structure of the relationship between µn(c) and IE(X s); these instabilities
are related to the value of the center c and increase as the order of the central moments becomes
high. In particular,

(a) the numerical error ∆IE(X

−
integral moments IE(X h), h

c)n due to the evaluation of IE(X

c)n in terms of noncentral

n, becomes bigger as c and n increase. In fact,

−

where eps corresponds to the error machine.

(b) the numerical error ∆ IE(X s) due to the evaluation of IE(X s) involving the ﬁrst Mmax

central moments IE(X

c)n, is given by

−

≤

∆IE(X
|

−

c)n

|

n

(
= (cid:12)
(cid:12)
Xh=0
(cid:12)
(cid:12)
n
(cid:12)

≤

Xh=0

1)h

−

n
h(cid:19)

(cid:18)

n
h(cid:19)

(cid:18)

cn−h

cn−h ∆IE(X h)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆IE(X h)

(cid:12)
(cid:12)

n

(cid:12)
(cid:12)
cn−h =

n
h(cid:19)

(cid:18)

=
k

=
k

∆ IE(X h)

∞

∆ IE(X h)

Xh=0
∞ (1 + c)n

k

k

eps (1 + c)n,

≃

∆ IE(X s)
|

|

=

s
n(cid:19)

(cid:18)

cs−n∆ IE(X

c)n

−

Mmax

Xn=0
Mmax

s
n(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xn=0

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
∆ IE(X

≤

≤ k

cs−n

∆ IE(X

|

c)n

|

−

c)n

k∞ cs max
n (cid:18)

−

s
n(cid:19)

=

∆ IE(X

c)n

∞cs max

k

−

k

s
n(cid:19) (cid:0)

n (cid:18)

Mmax

n

1
c (cid:19)

(cid:18)

Xn=0
1
c

(cid:1)

Mmax+1

1
c −

1

1

,

−

(2.4)

(2.5)

(2.6)

(2.7)

s
n
(cid:0)

=

with maxn
if [s] is odd, where [x]
represents the integer part of x. The product of ﬁrst two factors of the right hand side of
(2.7) is an increasing function of c, whilst the last factor gives a function which decreases
with c.

if [s] is even and maxn

=

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

s
[s/2]
(cid:0)

s
[s/2]+1

s
n
(cid:1)

3

Hence, taking in account both (a) and (b), a reasonable choice of c could be c = 1
rewriting the last inequality in (2.7) as

2 . Further,

∆ IE(X)s

|

| ≤ k

∆ IE(X

c)n

∞ cs max
k

n (cid:18)

−

1
c

s
n(cid:19) (cid:0)

(cid:1)

Mmax+1

1
c −

1

1

−

< ε

we can reconstruct the s-order fractional moment with a preﬁxed level of accuracy ε, ε > 0,
just involving a number of central moments equal to the value Mmax.

3. Recovering f (x) from fractional moments
Let be X a positive r.v. on [0, 1] with density f (x), Shannon-entropy H[f ] =
∞
j=0, from which positive fractional moments E(X αj ) =
and moments
{
be obtained, as in (2.4)-(2.5).
From [4], we know that the Shannon-entropy maximizing density function fM (x), which has
the same M fractional moments E(X αj ), of f (x), j = 0, ..., M , is

1
0 f (x) ln f (x)dx
−
∞
n=0 bn(αj)µn may
R

µj}

P

fM (x) = exp(

λjxαj ).

M

−

Xj=0

(3.1)

Z

0

1

1

− Z

0

Here (λ0, ..., λM ) are Lagrangean multipliers, which must be supplemented by the condition
that the ﬁrst M fractional moments of fM (x) coincide with E(X αj ), i.e,

E(X αj ) =

xαj fM (x)dx, j = 0, ..., M, α0 = 1

(3.2)

The Shannon entropy H[fM ] of fM (x) is given as

H[fM ] =

fM (x) ln fM (x)dx =

λjE(X αj ).

(3.3)

M

Xj=0

Given two probability densities f (x) and fM (x), there are two well-known measures of the dis-
0 f (x) ln f (x)
tance between f (x) and fM (x). Namely the divergence measure I(f, fM ) =
fM (x) dx
R
dx. If f (x) and fM (x) have the same

fM (x)

f (x)

1

and the variation measure V (f, fM ) =
fractional moments E(X αj ), j = 1, ..., M then

−

|

1
0 |
R

I(f, fM ) = H[fM ]

H[f ]

−

holds. In fact I(f, fM ) =
M
j=0 λjE(X αj ) = H[fM ]

1

0 f (x) ln f (x)
R
H[f ].
−

fM (x) dx =

H[f ] +

−

M
j=0 λj

1

0 xαj fM (x)dx =
R

−

P

H[f ] +

In literature, several lower bounds for the divergence measure I based on the variation measure
P
V are available. We shall however use the following bound [7]

(3.4)

(3.5)

If g(x) denotes a bounded function, such that
and (3.5), we have

|

|≤

g(x)

K, K > 0, by taking into account (3.4)

Ef (g)

EfM (g)

|

−

|≤ Z

0 |

| · |

1

g(x)

f (x)

fM (x)

dx

K

2(H[fM ]

H[f ])

(3.6)

|

≤

p

−

V 2
2

.

I

≥

−

4

. Equation (3.6) suggests us what fractional moments have to be chosen

M
j=1 : H[fM ] = minimum

αj}

{

(3.7)

The use of fractional moments in the framework of ME relies on the following two theoretical
results. The ﬁrst is a theorem [8, Th. 2] which guarantees the existence of a probability density
from the knowledge of an inﬁnite sequence of fractional moments

Theorem 3.1 [8, Th. 2] If X is a r.v. assuming values from a bounded interval [0, 1] and
αj = 0 and
αj}
∞
j=0 αj = +

∞
j=0 is an inﬁnite sequence of positive and distinct numbers satisfying lim
j→∞

, then the sequence of moments

∞
j=0 characterizes X.

E(X αj )

{

P
The second concerns the convergence in entropy of fM (x), where entropy-convergence means
lim
M →∞

H[fM ] = H[f ]. More precisely,

{

}

∞

Theorem 3.2. If
αj}
the ME approximant converges in entropy to f (x).
Proof. See Appendix.

{

M

j=0 are equispaced within [0, 1), with αM −j+1 = j

M +1 , j = 0, ..., M then

We just point out that the choice of equispaced points αM −j+1 = j
both conditions of Theorem 3.1, i.e.

M +1 , j = 0, ..., M satisﬁes

lim
M →∞

αM = 0 and

lim
M →∞

αj = lim
M →∞

1
M + 1

M
2

(M + 1) = +

.
∞

M

Xj=0

As a consequence, if the choice of equispaced αM −j+1 guarantees entropy-convergence, then the
choice (3.7) guarantees entropy-convergence too.
From a computational point of view, Lagrangean multipliers (λ1, ..., λM ) are obtained by (1.4),
and the normalizing constant λ0 is obtained by imposing that the density integrates to 1. Then
the optimal

M
j=1 exponents are obtained as

αj}

{

M
j=1 :

αj}

{

min
α1,...,αM h

min
λ1,...,λM

Γ(λ1, ..., λM )

.
i

(3.8)

4. Numerical results
We compare fractional and ordinary moments by choosing some probability densities on [0, 1].
Example 1. Let be

f (x) =

sin(πx)

π
2

with H[f ]
relationship

≃ −

0.144729886. From f (x) we have ordinary moments satisfying the recursive

n(n

µn =

1
2 −
∞
n=0 we calculate E(X αj ) =

−
π2

1)

µn−2,

n = 2, 3, ..., µ0 = 1, µ1 =

1
2

.

∞
E(X αj )
M
n=0 bn(αj)µn, as in (2.4)-(2.5). From
j=0
M
j=1 satisfy (3.7).

}

{

{

µn}

From
we obtain the ME approximant fM (x) for increasing values of M , where
In Table 1 are reported
a) H[fM ]
using fractional moments.
b) H[fM ]
Inspection of Table 1 allows us to conclude that:

αj}
H[f ] = I(f, fM ), where H[fM ] is obtained using ordinary moments.

H[f ] = I(f, fM ) and exponents

αj}

P

−

−

{

{

M
j=1 satisfying (3.7), where H[fM ] is obtained

5

1) Entropy decrease is fast, so that practically 4-5 fractional moments determine f (x).
2) On the converse an high number of ordinary moments are requested for a satisfactory char-
acterization of f (x).
3) Approximately 12 ordinary moments have an eﬀect comparable to 3 fractional moments.
f (x) and fM (x), obtained by 4-5 fractional moments, are practically indistinguishable.

Table 1
Optimal fractional moments and entropy diﬀerence of distributions having an
increasing number of common a) fractional moments b) ordinary moments

b)

M H[fM ]
2

−
0.9510E

H[f ]
2

4

6

8

10

12

0.2098E

0.7058E

0.4442E

0.3357E

0.3288E

−

−

−

−

−

−

2

3

3

3

3

a)

M
1

αj}
{
13.4181

M
j=1 H[fM ]

−
0.8716E

H[f ]
1

−

0.2938E

2

0.3038E

3

0.3276E

4

−

−

−

0.1016E

4

−

2

3

4

5

0.00289
4.69275

0.04680
1.84212
13.2143

0.00220
2.76784
13.7293
20.5183

0.0024
2.7000
13.700
20.500
25.200

Example 2. This example is borrowed from [9]. Here the authors attempt to recover a non-
negative decreasing diﬀerentiable function f (x) from the frequency moments ωn, with

ωn =

[f (x)]ndx,

n = 1, 2, ...

1

Z
0

The authors of [9] realize that other density reconstruction procedures, alternative to ordinary
moments, would be desirable. We propose fractional moments density reconstruction procedure.
Here

f (x) = 2

+

ln(

1
2

h

1
10

1

Ax + B −

1)

i

1

B =

1 + e5 , A =

1
1 + e−5 −

1
1 + e5

with H[f ]
0.06118227 (f (x), compared to [9], contains the normalizing constant 2). From
≃ −
∞
n=0 we calculate
f (x) we have ordinary moments µn through a numerical procedure. From
∞
M
E(X αj )
E(X αj ) =
j=0 we obtain the ME
n=0 bn(αj)µn, as in (2.4)-(2.5). Finally, from
M
j=1 satisfy (3.7).
approximant fM (x) for increasing values of M , where
αj}
Table 2 reports:
a) H[fM ]
using fractional moments.

M
j=1 satisfying (3.7), where H[fM ] is obtained

H[f ] = I(f, fM ) and exponents

µn}

αj}

P

−

{

{

{

{

}

6

b) H[fM ]

H[f ] = I(f, fM ), where H[fM ] is obtained using ordinary moments.

−

Inspection of Table 2 allows us to conclude that:
1) Entropy decrease is fast, so that practically 4 fractional moments determine f (x).
2) An high number of ordinary moments is requested for a satisfactory characterization of f (x).
3) Approximately 14 ordinary moments have an eﬀect comparable to 4 fractional moments.
Functions f (x) and fM (x), obtained by 4 fractional moments, are practically indistinguishable.
As a consequence, we argue that the use of 4 fractional moments is as eﬀective as that of
8 frequency moments (as in [9]). The former ones, indeed, provide an approximant fM (x)
practically indistinguishable from f (x) (see ﬁgure 1 of [9]).

Table 2
Optimal fractional moments and entropy diﬀerence of distributions having an
increasing number of common a) fractional moments b) ordinary moments

a)

M
1

αj}
{
1.56280

M
j=1 H[fM ]

−
0.6278E

H[f ]
2

−

b)

M H[fM ]
2

−
0.5718E

H[f ]
2

0.3152E

2

0.1169E

2

0.1025E

3

−

−

−

2

3

4

0.52500
3.90000

1.05000
3.00000
7.87500

0.44062
7.65470
12.5262
63.9093

4

6

8

10

12

14

0.1776E

0.1320E

0.6744E

0.3509E

0.2648E

0.1914E

−

−

−

−

−

−

−

2

2

3

3

3

3

5. Conclusions
In this paper we have faced up the Hausdorﬀ moment problem and we have solved it using a low
number of fractional moments, calculated explicitly in terms of given ordinary moments. The
approximating density, constrained by few fractional moments, has been obtained by maximum-
entropy method. Fractional moments have been chosen by minimizing the entropy of the ap-
proximating density. The strategy proposed in the present paper, for recovering a given density
function, consists in accelerating the convergence by a proper choice of fractional moments, so
obtaining an approximating density by the use of low order moments, as (1.1) suggests.

7

6. References

[1] D. Fasino, Spectral properties of Hankel matrices and numerical solutions of ﬁnite moment
problems, J. Comput. Applied Math., 65, 145-155, (1995).

[2] G. Talenti, Recovering a function from a ﬁnite number of moments, Inverse Problems, 3,
501-517, (1987).

[3] S. Karlin, L.S. Shapley, Geometry of moment spaces, AMS Memoirs 12, Providence RI
(1953).

[4] H.K. Kesavan, J.N. Kapur, Entropy Optimization Principles with Applications, Academic
Press, (1992).

[5] B. Beckermann, The condition number of real Vandermonde, Krylov and positive deﬁnite
Hankel matrices, Numerische Mathematik, 85, 553-577, (2000).

[6] J.M. Borwein, A.S. Lewis, Convergence of best entropy estimates, SIAM J. Optimization, 1,
191-205, (1991).

[7] S. Kullback, A lower bound for discrimination information in terms of variation, IEEE
Transaction on Information Theory, IT-13, 126-127, 1967.

[8] G.D. Lin, Characterizations of Distributions via moments, Sankhya: The Indian Journal of
Statistics, 54, Series A, 128-132, 1992.

[9] E. Romera, J.C. Angulo, J.S. Dehesa, The Hausdorﬀ entropic moment problem, J. of Math.
Physics, 42, 2309-2314, (2001).

[10] J.A. Shohat, J.D. Tamarkin, The problem of moments, AMS Mathematical Survey, 1,
Providence RI, (1963).

8

Appendix: Entropy convergence

A.1 Some background
Let’s consider a sequence of equispaced points αj = j

M +1 , j = 0, ..., M and

µj =: E(X αj ) =

tαj fM (t)dt,

j = 0, ..., M

(A.1)

1

Z

0

M
j=0 λjtαj ). With a simple change of variable x = t

1

M +1 , from (A.1) we

with fM (t) = exp(
have

−

P

1

Z
0

µj = E(X αj ) =

xj exp

(λ0 −

h−

ln(M + 1))

λjxj + M ln x
dx, j = 0, ..., M (A.2)
i

M

−

Xj=1

which is a reduced Hausdorﬀ moment problem for each ﬁxed M value and a determinate Haus-
. Referring to (A.2) the following symmetric deﬁnite
dorﬀ moment problem when M
→ ∞
positive Hankel matrices are considered

∆0 = µ0, ∆2 =

µ0 µ1
µ1 µ2 (cid:21)

(cid:20)

, ..., ∆2M = 



· · ·

µ0
...
· · ·
µM · · ·

µM
...
µ2M






(A.3)

whose (i, j)-th entry i, j = 0, 1, ... holds

µi+j =

xi+jfM (x)dx,

1

Z

0

where fM (x) = exp
is determinate and the underlying distribution has a continuous distribution function F (x),
with density f (x). Then the massimal mass ρ(x) which can be concentrated at any real point
x is equal to zero ([10], Corollary (2.8)). In particular, at x = 0 we have

. The Hausdorﬀ moment problem

M
j=1 λjxj + M ln x
i

ln(M + 1))

(λ0 −

h−

P

−

0 = ρ(0) = lim
i→∞

ρ(0)
i =:

= lim
i→∞

(µ0 −

−(i)
µ
0

)

(A.4)

indicates the largest mass which can be concentrated at a given point x = 0 by any
indicates the minimum value of

−(i)
i and µ
0

i

where ρ(0)
solution of a reduced moment problem of order
µ0 once assigned the ﬁrst 2i moments.
Let’s ﬁx
we have

µ0, ..., µi−1, µi+1, ..., µM }

{

while only µi, i = 0, ..., M varies continuously. From (A.2)

(A.5)

where ei+1 is the canonical unit vector

0 <

, ...,

dλ0
dµi

h

dλM
dµi i·

∆2M ·

dλ0
dµi

=

−h

, ...,

dλM
dµi i

ei+1 =

dλi
dµi

−

i

∀

(A.6)

µ2
...
µi+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µi+1
...
µ2i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|

∆2i |
· · ·

· · ·
· · ·

≥

∆2M ·

dλ0/dµi
...
dλM /dµi








=

ei+1

−



IRM +1, from which

∈
dλ0/dµi
...
dλM /dµi











9

A.2 Entropy convergence
The following theorem holds.
Theorem A.1 If αj = j

lim
M →∞

H[fM ] =:

− Z

0

1

Proof. From (A.1) and (A.7) we have

M +1 , j = 0, ..., M and fM (x) = exp(

M
j=0 λjxαj ) then

fM (x) ln fM (x)dx = H[f ] =:

f (x) ln f (x)dx.

(A.7)

−

P

1

− Z

0

H[fM ] =

λjµj

M

Xj=0

(A.8)

Let’s consider (A.8). When only µ0 varies continuously, taking into account (A.3)-(A.6) and
(A.8) we have

d
dµ0

H[fM ] =

M

Xj=0

µj

dλj
dµ0

+ λ0 = λ0 −

1

µ2
...
µM +1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µM +1
...
µ2M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

· · ·

· · ·
· · ·
∆2M |

d2
dµ2
0

H[fM ] =

dλ0
dµ0

=

−

=

−

1

−(M )
µ
0

< 0.

µ0 −
−(M )
,
µ
Thus H[fM ] is a concave diﬀerentiable function of µ0. When µ0 →
0
→ −∞
whilst at µ0 it holds H[fM ] > H[f ], being fM (x) the maximum entropy density once assigned
(µ0, ..., µM ). Besides, when M

µ0. So the theorem is proved.

then H[fM ]

−(M )
then µ
0

|

→ ∞

→

10

HAUSDORFF MOMENT PROBLEM VIA FRACTIONAL MOMENTS

Pierluigi Novi Inverardi(1), Alberto Petri(2), Giorgio Pontuale(2), Aldo Tagliani(1)(∗)

(1) Faculty of Economics, Trento University, 38100 Trento, Italy.
(2) CNR, Istituto di Acustica ”O.M. Corbino”, 00133 Roma, Italy.
(∗) Corresponding author:
Phone: +39-0461-882116, Fax:+39-0461-882124, E-mail: ataglian@cs.unitn.it

Abstract
We outline an eﬃcient method for the reconstruction of a probability density function from the
knowledge of its inﬁnite sequence of ordinary moments. The approximate density is obtained
resorting to maximum entropy technique, under the constraint of some fractional moments. The
latter ones are obtained explicitly in terms of the inﬁnite sequence of given ordinary moments.
It is proved that the approximate density converges in entropy to the underlying density, so
that it demonstrates to be useful for calculating expected values.

Key Words: Entropy, Fractional moments, Hankel matrix, Maximum Entropy, Moments.

11


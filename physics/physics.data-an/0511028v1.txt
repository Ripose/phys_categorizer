5
0
0
2
 
v
o
N
 
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
2
0
1
1
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

STATISTICAL CHALLENGES FOR SEARCHES FOR NEW PHYSICS AT THE LHC

KYLE CRANMER
Brookhaven National Laboratory, Upton, NY 11973, USA
e-mail: Kyle.Cranmer@cern.ch

Because the emphasis of the LHC is on 5σ discoveries and the LHC environment induces high systematic errors, many
of the common statistical procedures used in High Energy Physics are not adequate. I review the basic ingredients
of LHC searches, the sources of systematics, and the performance of several methods. Finally, I indicate the methods
that seem most promising for the LHC and areas that are in need of further study.

1 Introduction

The Large Hadron Collider (LHC) at CERN and the
two multipurpose detectors, Atlas and CMS, have
been built in order to discover the Higgs boson, if it
exists, and explore the theoretical landscape beyond
the Standard Model.1,2 The LHC will collide protons
with unprecedented center-of-mass energy (√s = 14
TeV) and luminosity (1034 cm−2s−1); the Atlas and
CMS detectors will record these interactions with
108 individual electronic readouts per event. Be-
∼
cause the emphasis of the physics program is on dis-
covery and the experimental environment is so com-
plex, the LHC poses new challenges to our statistical
methods – challenges we must meet with the same
vigor that led to the theoretical and experimental
advancements of the last decade.

In the remainder of this Section, I introduce the
physics goals of the LHC and most pertinent factors
that complicate data analysis. I also review the for-
mal link and the practical diﬀerences between conﬁ-
dence intervals and hypothesis testing.

In Sec. 2, the primary ingredients to new particle
searches are discussed. Practical and toy examples
are presented in Sec. 3, which will be used to assess
the most common methods in Sec. 4. The remainder
of this paper is devoted to discussion on the most
promising methods for the LHC.

1.1 Physics Goals of the LHC

Currently, our best experimentally justiﬁed model
for fundamental particles and their interactions is
the standard model.
In short, the physics goals of
the LHC come in two types: those that improve our
understanding of the standard model, and those that
go beyond it.

The only particle of the standard model that has
not been observed is the Higgs boson, which is key for
the standard model’s description of the electroweak
interactions. The mass of the Higgs boson, mH , is
a free parameter in the standard model, but there
exist direct experimental lower bounds and more in-
direct upper bounds. Once mH is ﬁxed, the standard
model is a completely predictive theory. There are
numerous particle-level Monte Carlo generators that
can be interfaced with simulations of the detectors
to predict the rate and distribution of all experimen-
tal observables. Because of this predictive power,
searches for the Higgs boson are highly tuned and of-
ten employ multivariate discrimination methods like
neural networks, boosted decision trees, support vec-
tor machines, and genetic programming.3,4,5

i.e.

While the Higgs boson is key for understand-
ing the electroweak interactions, it introduces a new
problem:
the hierarchy problem. There are
several proposed solutions to the problem, one of
which is to introduce a new fundamental symmetry,
called supersymmetry (SUSY), between bosons and
fermions. In practice, the minimal supersymmetric
extension to the standard model (MSSM), with its
105 parameters, is not so much a theory as a theo-
retical framework.

They key diﬀerence between SUSY and Higgs
searches is that, in most cases, discovering SUSY
will not be the diﬃcult part. Searches for SUSY
often rely on robust signatures that will show a de-
viation from the standard model for most regions of
the SUSY parameter space.
It will be much more
challenging to demonstrate that the deviation from
the standard model is SUSY and to measure the fun-
damental parameters of the theory.6 In order to re-
strict the scope of these proceedings, I shall focus
LHC Higgs searches, where the issues of hypothesis
testing are more relevant.

1

2

1.2 The Challenges of LHC Environment

The challenges of the LHC environment are mani-
fold. The ﬁrst and most obvious challenge is due
to the enormous rate of uninteresting background
events from QCD processes. The total interaction
rate for the LHC is of order 109 interactions per sec-
ond; the rate of Higgs production is about ten orders
of magnitude smaller. Thus, to understand the back-
ground of a Higgs search, one must understand the
extreme tails of the QCD processes.

Compounding the diﬃculties due to the extreme
rate is the complexity of the detectors. The full-
ﬂedged simulation of the detectors is extremely com-
putationally intensive, with samples of 107 events
taking about a month to produce with computing
resources distributed around the globe. This compu-
tational limitation constrains the problems that can
been addressed with Monte Carlo techniques.

→

blν ¯bjj jj),

Theoretical uncertainties also contribute to the
challenge. The background to many searches re-
quires calculations at, or just beyond, the state-
of-the-art in particle physics. The most common
situation requires a ﬁnal state with several well-
separated high transverse momentum objects (e.g.
t¯t jj
in which the regions of phys-
ical interest are not reliably described by leading-
order perturbative calculations (due to infra-red
and collinear divergences), are too complex for the
requisite next-to-next-to-leading order calculations,
and are not properly described by the parton-
shower models alone. Enormous eﬀort has gone
into improving the situation with next-to-leading or-
der calculations and matrix-element–parton-shower
matching.7,8 While these new tools are a vast im-
provement, the residual uncertainties are still often
dominant.

Uncertainties from non-perturbative eﬀects are
also important. For some processes, the relevant
regions of the parton distribution functions are not
well-measured (and probably will not be in the ﬁrst
few years of LHC running), which lead to uncertain-
ties in rate as well as the shape of distributions. Fur-
thermore, the various underlying-event and multiple-
interaction models used to describe data from pre-
vious colliders show large deviations when extrap-
olated to the LHC.9 This soft physics has a large
impact on the performance of observables such as
missing transverse energy.

In order to augment the simulated data chain,
most searches introduce auxiliary measurements to
estimate their backgrounds from the data itself. In
some cases, the background estimation is a simple
sideband, but in others the link between the auxiliary
measurement to the quantity of interest is based on
simulation. This hybrid approach is of particular
importance at the LHC.

While many of the issues discussed above are not
unique to the LHC, they are often more severe. At
LEP, it was possible to generate Monte Carlo sam-
ples of larger size than the collected data, QCD back-
grounds were more tame, and most searches were not
systematics-limited. The Tevatron has much more in
common with the LHC; however, at this point dis-
covery is less likely, and most of the emphasis is on
measurements and limit setting.

1.3 Conﬁdence Intervals & Hypothesis Testing

The last several conferences in the vein of PhyStat
2005 have concentrated heavily on conﬁdence inter-
vals. In particular, 95% conﬁdence intervals for some
physics parameter in an experiment that typically
has few events. More recently, there has been a large
eﬀort in understanding how to include systematic er-
rors and nuisance parameters into these calculations.
LHC searches, in contrast, are primarily inter-
ested in 5σ discovery. The 5σ discovery criterion is
somewhat vague, but usually interpreted in a fre-
quentist sense as a hypothesis test with a rate of
10−7.
Type I error α = 2.85

There is a formal link between conﬁdence inter-
vals and hypothesis testing:
frequentist conﬁdence
intervals from the Neyman construction are formally
inverted hypothesis tests. It is this equivalence that
links the Neyman-Pearson lemmaa to the ordering
rule used in the uniﬁed method of Feldman and
Cousins.10 Furthermore, this equivalence will be very
useful in translating our understanding of conﬁdence
intervals to the searches at the LHC.

·

In some cases, this formal link can be mislead-
ing. In particular, there is not always a continuous
parameter that links the fully speciﬁed null hypoth-
esis H0 to the fully speciﬁed alternate H1 in any

aThe lemma states that, for a simple hypothesis test of size
α between a null H0 and an alternate H1, the most powerful
critical region in the observable x is given by a contour of the
likelihood ratio L(x|H0)/L(x|H1).

e
c
n
a
c
i
f
i

n
g
i
s
 
l
a
n
g
i
S

 

10 2

  ∫ L dt = 30 fb-1
 (no K-factors)
ATLAS

 H  →  g g 
 ttH (H  →  bb)
 H   →  ZZ(*)   →  4 l
 H   →  WW(*)   →  ln ln
 qqH   →  qq WW(*)
 qqH   →  qq t

Total significance

10

1

100

120

140

160

180

200

 mH (GeV/c2)

Figure 1. Expected signiﬁcance as a function of Higgs mass
for the Atlas detector with 30 fb−1 of data.

physically interesting or justiﬁed way. Furthermore,
the performance of a method for a 95% conﬁdence
interval and a 5σ discovery can be quite diﬀerent.

2 The Ingredients of an LHC Search

In order to assess the statistical methods that are
available and develop new ones suited for the LHC, it
is necessary to be familiar with the basic ingredients
of the search. In this section, the basic ingredients,
terminology, and nomenclature are established.

2.1 Multiple Channels & Processes

Almost all new particle searches do not observe the
particle directly, but through the signatures left by
the decay products of the particle. For instance, the
Higgs boson will decay long before it interacts with
the detector, but its decay products will be detected.
In many cases, the particle can be produced and de-
cay in many diﬀerent conﬁgurations, each of which is
called a search channel (see Tab. 1). There are may
be multiple signal and background processes which
γγ,
contribute to each channel. For example, in H
the signal could come from any Higgs production
mechanism and the background from either contin-
uum γγ production or QCD backgrounds where jets
fake photons. Each of these processes have their own
rates, distributions for observables, and uncertain-
ties. Furthermore, the uncertainties between pro-
cesses may be correlated.

→

3

In general the theoretical model for a new parti-
cle has some free parameters. In the case of the stan-
dard model Higgs, only the mass mH is unknown.
For SUSY scenarios, the Higgs model is parametrized
by two parameters: mA and tan β. Typically, the un-
known variables are scanned and a hypothesis test is
performed for each value of these parameters. The
results from each of the search channels can be com-
bined to enhance the power of the search, but one
must take care of correlations among channels and
ensure consistency.

The fact that one scans over the parameters and
performs many hypothesis tests increases the chance
that one ﬁnds at least one large ﬂuctuation from the
null-hypothesis. Some approaches incorporate the
number of trials explicitly,11 some approaches only
focus on the most interesting ﬂuctuation,12 and some
see this heightened rate of Type I error as the moti-
vation for the stringent 5σ requirement.13

2.2 Discriminating Variables & Test Statistics

Typically, new particles are known to decay with cer-
tain characteristics that distinguish the signal events
from those produced by background processes. Much
of the work of a search is to identify those observ-
ables and to construct new discriminating variables
(generically denoted as m). Examples include an-
gles between particles, invariant masses, and parti-
cle identiﬁcation criterion. Discriminating variables
are used in two diﬀerent ways: to deﬁne a signal-like
region and to weight events.

The usage of discriminating variables is related
to the test statistic: the real-valued quantity used
to summarize the experiment. The test statistic is
thought of as being ordered such that either large or
small values indicate growing disagreement with the
null hypothesis.

A simple “cut analysis” consists of deﬁning a
signal-like region bounded by upper- and lower-
values of these discriminating variables and counting
events in that region. In that case, the test statistic
is simply the number of events observed in the signal
like region. One expects b background events and s
signal events, so the experimental sensitivity is op-
timized by adjusting the cut values. More sophisti-
cated techniques use multivariate algorithms, such as
neural networks, to deﬁne more complicated signal-
like regions, but the test statistic remains unchanged.

t
4

In these number counting analyses, the likelihood of
observing n events is simply given by the Poisson
model.

In particular,

There are extensions to this number-counting
if one knows the dis-
technique.
tribution of
the discriminating variable x for
the background-only (null) hypothesis, fb(m), and
the signal-plus-background (alternate) hypothesis,
fs+b(m) = [sfs(m) + bfb(m)]/(s + b), then there is
a more powerful test statistic than simply counting
events. This is intuitive, a well measured ’golden
event’ is often more convincing than a few messy
ones. Following the Neyman-Pearson lemma, the
most powerful test statistic is

Q =

L(m
L(m

H1)
|
H0)
|
Nchan
i

=

Q

or equivalently
Q

q = ln Q =

stot +

−

P ois(ni|

Nchan
i

si + bi)
P ois(ni|

ni
j
bi)
Q

sifs(mij )+bifb(mij )
si+bi
ni
j fb(mij)

Q

ln

1 +

Nchan

ni

i
X

j
X

(cid:18)

sifs(mij )
bifb(mij )

.

(cid:19)

(2)
The test statistic in Eq. 2 was used by the LEP
Higgs Working Group (LHWG) in their ﬁnal results
on the search for the Standard Model Higgs.14

At this point, there are two loose ends: how does
one determine the distribution of the discriminating
variables f (m), and how does one go from Eq. 2 to
the distribution of q for H0 and H1. These are the
topics of the next subsections.

2.3 Parametric and Non-Parametric Methods

In some cases, the distribution of a discriminat-
ing variable f (m) can be parametrized and this
parametrization can be justiﬁed either by physics ar-
guments or by goodness-of-ﬁt. However, there are
many cases in which f (m) has a complicated shape
not easily parametrized. For instance, Fig. 2 shows
the distribution of a neural network output for signal
events. In that case kernel estimation techniques can
be used to estimate f (m) in a non-parametric way
.15 The technique that
from a sample of events
mi}
{
was used by the LHWG14 was based on an adaptive
kernel estimation given by:

ˆf1(m) =

n

i
X

1
nh(mi)

K

m
mi
−
h(mi)

(cid:18)

,

(cid:19)

y
t
i
s
n
e
D
 
y
t
i
l
i
b
a
b
o
r
P

Neural Network Output

Figure 2. The distribution of a neural network output for sig-
nal events. The histogram is shown together with ˆf1(m).

where

(1)

h(mi) =

1/5

4
3

−1/5,

n

(4)

σ
ˆf0(mi)
xi}

(cid:18)

(cid:19)

r
σ is the standard deviation of
, K(x) is some ker-
nel function (usually the normal distribution), and
ˆf0(x) is the ﬁxed kernel estimate given by the same
equation but with a ﬁxed h(mi)

{

∗

h

=

1/5

4
3

(cid:18)

(cid:19)

σn

−1/5.

(5)

The solid line in Fig. 2 shows that the method
(with modiﬁed-boundary kernels) works very well for
shapes with complicated structure at many scales.

2.4 The Fourier Transform Technique

Given, fs(m) and fb(m) the distribution of q(x) can
be constructed. For the background-only hypothe-
sis, fb(m) provides the probability of corresponding
values of q needed to deﬁne the single-event pdf ρ1.b

ρ1,b(q0) =

fb(m) δ(q(m)

q0)dm

(6)

−

Z

For multiple events, the distribution of the log-
likelihood ratio must be obtained from repeated con-
volutions of the single event distribution. This con-
volution can either be performed implicitly with ap-
proximate Monte Carlo techniques,16 or analytically
with a Fourier transform technique.17 In the Fourier
domain, denoted with a bar, the distribution of the
log-likelihood for n events is

ρn = ρ1

n

(7)

Thus the expected log-likelihood distribution for
background with Poisson ﬂuctuations in the number

(3)

bThe integral is necessary because the map q(m) : m → q may
be many-to-one.

of events takes the form

ρb(q) =

ρn,b(q)

(8)

e−bbn
n!

∞

n=0
X

which in the Fourier domain is simply

ρb(q) = eb[ρ1,b(q)−1].

(9)

For the signal-plus-background hypothesis we expect
s events from the ρ1,s distribution and b events from
the ρ1,b distribution, which leads to the expression
for ρs+b in the Fourier domainc

ρs+b(q) = eb[ρ1,b(q)−1]+s[ρ1,s(q)−1].

(10)

This equation generalizes, in a somewhat obvious
way, to include many processes and channels.

Numerically these computations are carried out
with the Fast Fourier Transform (FFT). The FFT
is performed on a ﬁnite and discrete array, beyond
which the function is considered to be periodic. Thus
the range of the ρ1 distributions must be suﬃciently
large to hold the resulting ρb and ρs+b distributions.
If they are not, the “spill over” beyond the maxi-
mum log-likelihood ratio qmax will “wrap around”
leading to unphysical ρ distributions. Because the
range of ρb is much larger than ρ1,b it requires a
very large number of samples to describe both distri-
butions simultaneously. The implementation of this
method requires some approximate asymptotic tech-
niques that describe the scaling from ρ1,b to ρb.18

The nature of the FFT results in a number of
round-oﬀ errors and limit the numerical precision to
about 10−16 – which limit the method to signiﬁcance
levels below about 8σ. Extrapolation techniques and
arbitrary precision calculations can overcome these
diﬃculties,18 but such small p-values are of practical
little interest.

From the log-likelihood distribution of the two
hypotheses we can calculate a number of useful quan-
tities. Given some experiment with an observed log-
likelihood ratio, q∗, we can calculate the background-
only conﬁdence level, CLb :

∗

CLb(q

) =

′

′

ρb(q

)dq

(11)

∞

q∗

Z

cPerhaps it is worth noting that ρ(q) is a complex valued
function of the Fourier conjugate variable of q. Thus nu-
merically the exponentiation in Eq. 9 requires Euler’s formula
eiθ = cos θ + i sin θ.

5

In the absence of an observation we can calculate the
expected CLb given the signal-plus-background hy-
pothesis is true. To do this we ﬁrst must ﬁnd the me-
dian of the signal-plus-background distribution qs+b.
From these we can calculate the expected CLb by
using Eq. 11 evaluated at q∗ = qs+b.

Finally, we can convert the expected background
conﬁdence level into an expected Gaussian signiﬁ-
cance, Zσ, by ﬁnding the value of Z which satisﬁes

CLb(qs+b) =

erf(Z/√2)

1

−

2

.

(12)

Z
y2)dy is a function
0 exp(
where erf(Z) = (2/π)
readily available in most numerical libraries. For Z >
R
1.5, the relationship can be approximated19 as

−

√u

Z

≈

−

ln u with u =

2 ln(CLb√2π)

(13)

−

2.5 Systematic Errors, Nuisance Parameters &

Auxiliary Measurements

Sections 2.3 and 2.4 represent the state of the art for
HEP in frequentist hypothesis testing in the absence
of uncertainties on rates and shapes of distributions.
In practice, the true rate of background is not known
exactly, and the shapes of distributions are sensitive
to experimental quantities, such as calibration coef-
ﬁcients and particle identiﬁcation eﬃciencies (which
are also not known exactly). What one would call a
systematic error in HEP, usually corresponds to what
a statistician would refer to as a nuisance parameter.
Dealing with nuisance parameters in searches is
not a new problem, but perhaps it has never been as
essential as it is for the LHC. In these proceedings,
Cousins reviews the diﬀerent approaches to nuisance
parameters in HEP and the professional statistical
literature.20 Also of interest is the classiﬁcation of
systematic errors provided by Sinervo.21 In Sec. 4,
the a few techniques for incorporating nuisance pa-
rameters are reviewed.

From an experimental point of view, the miss-
ing ingredient is some set of auxiliary measurements
that will constrain the value of the nuisance param-
eters. The most common example would be a side-
band measurement to ﬁx the background rate, or
some control sample used to assess particle identi-
ﬁcation eﬃciency. Previously, I used the variable
M to denote this auxiliary measurement22; while
Linnemann,19 Cousins,20 and Rolke, Lopez, and
Conrad23,24 used y. Additionally, one needs to know

6

M
d

 
/
 

ds
 
)

g
i
s

/s

1
(

0.6

0.5

0.4

0.3

0.2

0.1

ds

 / dMg

 = N e-a Mg

a=-0.048

a=-0.030

and later combined to assess correlations. The fac-
torization of the likelihood and the number of nui-
sance parameters included impact the diﬃculty of
implementing the various scenarios considered below.

t  = s

SB / s

sig

3 Practical and Toy Examples

SB

sig

SB

0
100

105

110

115

120

125

130

135

140

145

150

Mg

Figure 3. The signal-like region and sideband for H → γγ in
which τ is correlated to b via the model parameter a.

the likelihood function that provides the connection
between the nuisance parameter(s) and the auxiliary
measurements.

The most common choices for the likelihood of
τ b)
b) = P ois(y
the auxiliary measurement are L(y
|
|
τ b, σy), where τ is a constant that
and L(y
|
speciﬁes the ratio of the number of events one expects
in the sideband region to the number expected in the
signal-like region.d

b) = G(y
|

A constant τ is appropriate when one simply
counts the number of events y in an “oﬀ-source” mea-
surement. In a more typical case, one uses the distri-
bution of some other variable, call it m, to estimate
the number of background events inside a range of
m (see Fig. 3). In special cases the ratio τ is inde-
pendent of the model parameters. However, in many
e−am), the ratio τ depends on the
cases (e.g. f (m)
model parameters. Moreover, sometimes the side-
band is contaminated with signal events, thus the
background and signal estimates can be correlated.
These complications are not a problem as long as
they are incorporated into the likelihood.

∝

The number of nuisance parameters and aux-
iliary measurements can grow quite large. For in-
stance, the standard practice at Ba ¯Bar is to form
very large likelihood functions that incorporate ev-
erything from the parameters of the unitarity tri-
angle to branching fractions and detector response.
These likelihoods are typically factorized into multi-
ple pieces, which are studied independently at ﬁrst

dNote that Linnemann19 used α = 1/τ instead, but in this
paper α is reserved for the rate of Type I error.

In this Section, a few practical and toy examples are
introduced. The toy examples are meant to provide
simple scenarios where results for diﬀerent methods
can be easily obtained in order to expedite their com-
parison. The practical examples are meant to ex-
clude methods that provide nice solutions to the toy
examples, but do not generalize to the realistic situ-
ation.

3.1 The Canonical Example

Consider a number-counting experiment that mea-
sures x events in the signal-like region and y events
in some sideband. For a given background rate b in
the signal-like region, say one can expect τ b events
in the sideband. Additionally, let the rate of signal
events in the signal-like regions – the parameter of in-
terest – be denoted µ. The corresponding likelihood
function is

·

(14)

P ois(y

τ b).
|

LP (x, y

µ + b)
µ, b) = P ois(x
|
|
This is the same case that was considered in
Refs. 20,22,23,24 for x, y =
(10) and α = 5%.
O
For LHC searches, we will be more interested in
10−7. Furthermore, the
x, y =
auxiliary measurement will rarely be a pure number
counting sideband measurement, but instead the re-
sult of some ﬁt. So let us also consider the likelihood
function

(100) and α = 2.85

O

·

LG(x, y

µ + b)
µ, b) = P ois(x
|
|

·

G(y

τ b, √τ b).
|

(15)

As a concrete example in the remaining sections,
let us consider the case b = 100 and τ = 1. Opera-
tionally, one would measure y and then ﬁnd the value
xcrit(y) necessary for discovery. In the language of
conﬁdence intervals, xcrit(y) is the value of x nec-
essary for the 100(1
α)% conﬁdence interval in µ
to exclude µ0 = 0. In Sec. 4 we check the coverage
(Type I error or false-discovery rate) for both LP and
LG.

−

Linnemann reviewed thirteen methods and
eleven published examples of this scenario.19 Of the

g
g
g
s
s
s
g
g
7

ttH(120)
ttjj
ttbb (QCD)
ttbb (EW)

)

V
e
G
b
p
(
 

/

M
d

/
 
 
 

d

Box
Born

V
V
e
e
G
G
 
 
0
0
1
1
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
E
E

100
100

80
80

60
60

40
40

20
20

0
0
0
0

50
50

100
100

150
150

200
200

250
250

400
400
350
350
300
300
 (GeV)
 (GeV)

bbm
bbm

Figure 4. The bb invariant mass spectrum for t¯tH signal and
background processes at Atlas.

published examples, only three (the one from his ref-
erence 18 and the two from 19) are near the range of
x,y, and α relevant for LHC searches. Linnemann’s
review asks an equivalent question posed in this pa-
per, but in a diﬀerent way: what is the signiﬁcance
(Z in Eq. 12) of a given observation x, y.

3.2 The LHC Standard Model Higgs Search

The search for the standard model Higgs boson is
by no means the only interesting search to be per-
formed at the LHC, but it is one of the most studied
and oﬀers a particularly challenging set of channels
to combine with a single method. Figure 1 shows
the expected signiﬁcance versus the Higgs mass, mH ,
for several channels individually and in combination
for the Atlas experiment.25 Two mass points are
considered in more detail in Tab. 1, including re-
sults from Refs.1,25,26. Some of these channels will
most likely use a discriminating variable distribu-
tion, f (m), to improve the sensitivity as described
in Sec. 2.3. I have indicated the channels that I sus-
pect will use this technique. Rough estimates on the
uncertainty in the background rate have also been
tabulated, without regard to the classiﬁcation pro-
posed by Sinervo.

The background uncertainties for the t¯tH chan-
nel have been studied in some detail and separated
into various sources.26 Figure 4 shows the mbb mass
spectrum for this channel.e Clearly, the shape of
the background-only distribution is quite similar to

Figure 5. Two plausible shapes for the continuum γγ mass
spectrum at the LHC.

Mass (GeV)

the shape of the signal-plus-background distribution.
Furthermore, theoretical uncertainties and b-tagging
uncertainties aﬀect the shape of the background-only
spectrum. In this case the incorporation of system-
atic error on the background rate most likely pre-
cludes the expected signiﬁcance of this channel from
ever reaching 5σ.

→

Similarly, the H

γγ channel has uncertainty
in the shape of the mγγ spectrum from background
processes. One contribution to this uncertainty
comes from the electromagnetic energy scale of the
calorimeter (an experimental nuisance parameter),
while another contribution comes from the theoreti-
cal uncertainty in the continuum γγ production. Fig-
ure 5 shows two plausible shapes for the mγγ spec-
trum from “Born” and “Box” predictions.

4 Review of Methods

Based on the practical example of the standard
model Higgs search at the LHC and the discussion
in Sec. 2, the list of admissible methods is quite
short. Of the thirteen methods reviewed by Linne-
mann, only ﬁve are considered as reasonable or rec-
ommended. These can be divided into three classes:
Bayesian methods based on the Cousins-Highland
technique, frequentist methods based on the Like-
lihood principle, and frequentist methods based on
the Neyman construction.

4.1 The Cousins-Highland Method

eIt is not clear if this result is in agreement with the equivalent
CMS result.27

The class of methods frequently used in HEP and
commonly referred to as the Cousins-Highland tech-

s
8

Table 1. Number of signal and background events for representative Higgs search channels with 30 fb−1 of data. A rough
uncertainty on the background rate is denoted as δb/b, without reference to the type of systematic uncertainty. The table also
indicates if the channels are expected to use a weight f (m) as in Eq. 2.

t¯tbb
γγ

channel

t¯tH
→
H
→
qqτ τ
→
→
qqτ τ
→
→
qqW W ∗
qqW W ∗
ZZ
→
W W

qqll /ET
qqlh /ET

qqll /ET
qqll /ET
4l
ll /ET

→
→
→
→

→
→
H

H

→

qqH
qqH

qqH
qqH

s
42
357
17
16
28.5
262.5
7.6
337

b
219
11820
14
8
47.4
89.1
3.1
484

∼

δb/b
10%
∼
0.1%
10%
10%
10%
10%
1%
5%

∼
∼
∼
∼
∼
∼

dominant backgrounds
t¯tjj, t¯tbb
γγ, jγ, jj
τ τ , t¯t
Z
τ τ , t¯t
Z

→
→
t¯t, W W
t¯t, W W
4l
ZZ
→
τ τ , t¯t
Z

→

Yes
No
Yes
Yes
Yes
Yes
No
Yes

120
120
120
120
120
170
170
170

use f (m) mH (GeV)

nique (or secondarily Bayes in statistical literature)
are based on a Bayesian average of frequentist p-
values as found in the ﬁrst equation of Ref.28. The
Bayesian average is over the nuisance parameters and
y). Thus the p-value
weighted by the posterior P (b
|
of the observation (x0, y0) evaluated at µ is given by

∞

x0

Z

∞

p(x0, y0|

µ) =

db p(x0|

∞

0
Z

∞

µ, b)P (b

y0)
|

µ, b)P (b
dx P (x
|

y0)(17)
|

db

0
Z

∞

=

=

x0

Z
µ, y0)
dx P (x
|

(16)

(18)

where

µ, y0) =
P (x
|

0
Z

µ, b)
db P (x
|

P (y0|

b) P (b)

P (y)

(19)

The form in Eq. 16, an average over p-values, is sim-
ilar to the form written in Cousins & Highland’s ar-
ticle. The double-integral is expanded in Eq. 17 and
re-written in Eq. 18 to the form that is more familiar
to those from LEP Higgs searches.16,17 Actually, the
dependence on y0 and the Bayesian prior P (b) shown
explicitly in Eq. 19 is often not appreciated by those
that use this method.

The speciﬁc methods that Linnemann considers
correspond to diﬀerent choices of Bayesian priors.
The most common in HEP is to ignore the prior and
use a truncated Gaussian for the posterior P (b
y0),
|
which Linnemann calls ZN . For the case in which
the likelihood L(y
b) is known to be Poisson, Linne-
|
man prefers to use a ﬂat prior, which gives rise to a
Gamma-distributed posterior and Linnemann’s sec-
ond preferred method ZΓ. The method Linnemann
calls Z5′ can be seen as an approximation of ZN for

large signals and is what Atlas used to assess its
physics potential.1 The method not recommended by
Linnemann and was critically reviewed in Ref.29.

′

x5
crit(y) = y/τ + Z

y/τ (1 + 1/τ )

(20)

4.2 Likelihood Intervals

p

As Cousins points out, the professional statistics
literature seems less concerned with providing cor-
rect coverage by construction, in favor of likelihood-
based and Bayesian methods. The likelihood princi-
ple states that given a measurement x all inference
about µ should be based on the likelihood function
L(x
µ). When nuisance parameters are included,
|
things get considerably more complicated.

The proﬁle likelihood function is an attempt to
eliminate the nuisance parameters from the likeli-
hood function by replacing them with their condi-
tional maximum likelihood estimates (denoted with
a ˆˆ). For example, the proﬁle likelihood for LP in
µ0,
Eq. 14 is given by L(x, y
|
(1 + τ )µ0

ˆˆb(µ0)), with

x + y

ˆˆb(µ0) =

−
2(1 + τ )

(x + y

(1 + τ )µ0)2 + 4(1 + τ )yµ0

+

p

−

2(1 + τ )

The relevant likelihood ratio is then

x, y) =
λP (µ
|

L(x, y

µ0,
|
L(x, y

ˆˆb(µ0))
ˆµ, ˆb)
|

,

where ˆµ and ˆb are the unconditional maximum like-
lihood estimates.

One of the standard results from statistics is that
2 ln λ converges to the χ2 dis-
the distribution of
tribution with k degrees of freedom, where k is the

−

(21)

.

(22)

 l

g
o

l
 
2
-

30

25

20

15

10

5

X=185, Y=100, t =1

0

0

20

40

60

80

100

120

140

160

Figure 6. The proﬁle likelihood ratio −2 ln λ versus the signal
strength µ for y = 100, τ = 1, and x = xcrit(y) = 185.

number of parameters of interest.
In our example
k = 1, so a 5σ conﬁdence interval is deﬁned by the
set of µ with
x, y) < 25. Figure 6 shows
2 ln λ(µ
|
−
x, y) for y = 100 at the criti-
2 ln λ(µ
the graph of
|
−
cal value of x for a 5σ discovery.

At PhyStat2003, Nancy Reid presented var-
ious adjustments and improvements to the pro-
ﬁle likelihood which speed asymptotic convergence
properties.30 Cousins considers these methods in
more detail from a physicist perspective.20

Only recently was it generally appreciated that
the method of Minuit31 commonly used in HEP cor-
responds to the proﬁle likelihood intervals. The cov-
erage of these methods is not guaranteed, but has
been studied in simple cases.23,24 These likelihood-
based techniques are quite promising for searches at
the LHC, but their coverage properties must be as-
sessed in the more complicated context of the LHC
with weighted events and several channels. In par-
ticular, the distribution of q in Eq. 10 is often highly
non-Gaussian.

4.3 The Neyman Construction with Nuisance

Parameters

Linnemann’s preferred method, ZBi, is related to
the familiar result on the ratio of Poisson means.32
Unfortunately, the form of ZBi is tightly coupled
to the form of Eq. 14, and can not be directly ap-
plied to the more complicated cases described above.

9

However, the standard result on the ratio of Pois-
son means32 and Cousins’ improvement33 are actu-
ally special cases of the Neyman construction with
nuisance parameters (without and with conditioning,
respectively).

Of course, the Neyman construction does gener-
alize to the more complicated cases discussed above.
Two particular types of constructions have been pre-
sented, both of which are related to the proﬁle like-
lihood ratio discussed in Kendall’s chapter on likeli-
hood ratio tests & test eﬃciency.34 This relationship
often leads to confusion with the proﬁle likelihood
intervals discussed in Sec. 4.2.

The ﬁrst method is a full Neyman construction
over both the parameters of interest and the nui-
sance parameters, using the proﬁle likelihood ratio
as an ordering rule. Using this method, the nuisance
parameter is “projected out”, leaving only an inter-
I presented this
val in the parameters of interest.
method at PhyStat2003 in the context of hypothesis
testing,f and similar work was presented by Punzi
at this conference.22,35 This method provides cover-
age by construction, independent of the ordering rule
used.

The motivation for using the proﬁle likelihood
ratio as a test statistic is twofold. First, it is inspired
by the Neyman-Pearson lemma in the same way as
the Feldman-Cousins ordering rule. Secondly, it is
independent of the nuisance parameters; providing
some hope of obtaining similar tests.g Both Punzi
and myself found a need to perform some “clipping”
to the acceptance regions to protect from irrelevant
values of the nuisance parameters spoiling the pro-
jection. For this technique to be broadly applica-
ble, some generalization of this clipping procedure is
needed and the scalability with the number of pa-
rameters must be addressed.h

The second method, presented by Feldman at
involves a Ney-
the Fermilab conference in 2000,
man construction over the parameters of interest, but
the nuisance parameters are ﬁxed to the conditional
maximum likelihood estimate: a method I will call
the proﬁle construction. The proﬁle construction is

f In simple hypothesis testing µ is not a continuous parameter,
but only takes on the values µ0 = 0 or µ1 = s.
gSimilar tests are those in which the critical regions of size α
are independent of the nuisance parameters. Similar tests do
not exist in general.
hA Monte Carlo sampling of the nuisance parameter space
could be used to curb the curse of dimensionality.22

m
10

an approximation of the full construction, that does
not necessarily cover. To the extent that the use of
the proﬁle likelihood ratio as a test statistic provides
similar tests, the proﬁle construction has good cover-
age properties. The main motivation for the proﬁle
construction is that it scales well with the number of
nuisance parameters and that the “clipping” is built
in (only one value of the nuisance parameters is con-
sidered).

It appears that the chooz experiment actually
performed both the full construction (called “FC cor-
rect syst.”) and the proﬁle construction (called “FC
proﬁle”) in order to compare with the strong conﬁ-
dence technique.36

Another perceived problem with the full con-
struction is that bad over-coverage can result from
the projection onto the parameters of interest.
It
should be made very clear that the coverage proba-
bility is a function of both the parameters of interest
and the nuisance parameters. If the data are con-
sistent with the null hypothesis for any value of the
nuisance parameters, then one should probably not
reject it. This argument is stronger for nuisance pa-
rameters directly related to the background hypoth-
esis, and less strong for those that account for instru-
mentation eﬀects. In fact, there is a family of meth-
ods that lie between the full construction and the
proﬁle construction. Perhaps we should pursue a hy-
brid approach in which the construction is formed for
those parameters directly linked to the background
hypothesis, the additional nuisance parameters take
on their proﬁle values, and the ﬁnal interval is pro-
jected onto the parameters of interest.

5 Results with the Canonical Example

Consider the case btrue = 100, τ = 1 (i.e. 10% sys-
tematic uncertainty). For each of the methods we
ﬁnd the critical boundary, xcrit(y), which is neces-
sary to reject the null hypothesis µ0 = 0 at 5σ when
y is measured in the auxiliary measurement. Figure 7
shows the contours of LG, from Eq. 15, and the criti-
cal boundary for several methods. The far left curve
shows the simple s/√b curve neglecting systematics.
The far right curve shows a critical region with the
correct coverage. With the exception of the proﬁle
likelihood, λP , all of the other methods lie between
these two curves (ie. all of them under-cover). The
rate of Type I error for these methods was evaluated

contours for btrue

=100, critical regions for t  = 1

No Systematics
Z5¢
ZN=Cousins-Highland
G profile
P profile
ad hoc

correct coverage

130

y

120

110

100

90

80

70

60

50

40

60

80

100

120

140

160

180

200
x

Figure 7. A comparison of the various methods critical bound-
ary xcrit(y) (see text). The concentric ovals represent con-
tours of LG from Eq. 15.

for LG and LP and presented in Table 2.

The result of the full Neyman construction and
the proﬁle construction are not presented. The full
Neyman construction covers by construction, and
it was previously demonstrated for a similar case
(b = 100, τ = 4) that the proﬁle construction gives
similar results.22 Furthermore, if the λP were used as
an ordering rule in the full construction, the critical
region for b = 100 would be identical to the curve
labeled “λP proﬁle” (since λP actually covers).

It should be noted that if one knows the likeli-
µ, b), then one should use
hood is given by LG(x, y
|
µ),
the corresponding proﬁle likelihood ratio, λG(x, y
|
for the hypothesis test. However, knowledge of the
correct likelihood is not always available (Sinervo’s
Class II systematic), so it is informative to check
µ) and
the coverage of tests based on both λG(x, y
|
µ) by generating Monte Carlo according to
λP (x, y
|
LG(x, y
µ, b). In a similar way, this
µ, b) and LP (x, y
|
|
decoupling of true likelihood and the assumed likeli-
hood (used to ﬁnd the critical region) can break the
“guaranteed” coverage of the Neyman construction.
It is quite signiﬁcant that the Cousins-Highland
method, ZN under-covers, since it is so commonly
used in HEP. The degree to which the method under-
covers depends on the truncation of the Gaussian
posterior P (b
y). Linnemann’s table also shows sig-
|
niﬁcant under-coverage (over estimate of the signif-

l
l
icance Z). In order to obtain a critical region with
the correct coverage, the author modiﬁed the region
xcrit(y) = xCH
crit(y) + C and found C = 16 provided
the correct coverage. A discrepancy of 16 events is
not trivial!

Table 2. Rate of Type I error interpreted equivalent Zσ for
various methods designed for a 5σ test. Monte Carlo events
are generated via either LG or LP . The critical x for y = 100
is also listed for easy comparison.

LG (Zσ) LP (Zσ)

Method
No Syst
Z5′
ZN C.-H.
ad hoc
proﬁle λP
proﬁle λG

3.0
4.1
4.2
4.6
5.0
4.7

3.1
4.1
4.2
4.7
5.0
4.7

xcrit(y = 100)
150
171
178
188
185
∼182

Notice that for large x, y the Cousins-Highland
result ZN approaches Z5′, where the the critical re-
y/τ . Because
gion is of the form xcrit(y) = y/τ + n
the boundary is very nearly linear around y0, one
can ﬁnd the value of n that gives the proper cover-
age with a little geometry. In particular, the number
n needed to get a Zσ test gives

p

xcrit(y) = y/τ + Z

1 + 1/τ m2

y/τ

(23)

where

p

p

−1

Z
y/τ !

m =

1 +

(24)

 

p

2
The m2 factor can be seen as a correction to the Z5′
and ZN (Cousins-Highland) results. Notice that the
correction is larger for higher signiﬁcance tests. As
an ad hoc method, I experimented with the Cousins-
Highland method replacing τ with τ m2 in the pos-
y). The coverage of this ad hoc method is
terior P (b
|
better than ZN , but not exact because x, y are not
suﬃciently large.

6 Conclusions

I have presented the statistical challenges of searches
at the LHC and the current state of the statistical
methods commonly used in HEP. I have attempted
to accurately portray the complexity of the searches,
explain their key ingredients, and provide a practical
example for future studies. Three classes of methods,
which are able to incorporate all the ingredients, have
been identiﬁed: Bayesian methods, frequentist meth-
ods based on the likelihood principle, and frequentist
methods based on the Neyman construction.

11

The Cousins-Highland method, based on a
Bayesian average over the nuisance parameters,
shows signiﬁcant under-coverage in the toy example
considered when pushed to the 5σ regime. While
Bayesian might not care about coverage, signiﬁcant
under-coverage is undesirable in HEP. Further study
is needed to determine if a more careful choice of
prior distributions can remedy this situation – espe-
cially in more complex situations.

The methods based on the likelihood principle
have gained a great deal of attention from HEP in
recent years. While the methods appear to do well in
the toy example, it requires further study to deter-
mine their properties in the more realistic situation
with weighted events.

Slowly, the HEP community is coming to grips
with how to incorporate nuisance parameters into the
Neyman construction. Several ideas for reducing the
over-coverage induced by projecting out the nuisance
parameters and reducing the computational burden
have been presented. A hybrid approach between the
full construction and the proﬁle construction should
be investigated in more detail.

Finally, it seems that the HEP community is
approaching a point where we appreciate the fun-
damental statistical issues, the limitations of some
methods, and the beneﬁts of others. Clearly, the
philosophical debate has not ended, but there seems
to be more emphasis on practical solutions to our
very challenging problems.

Acknowledgments

This

I would like to thank the many people that helped in
preparing this review.
In particular, Bob Cousins,
Gary Feldman, Jan Conrad, Fredrik Tegenfeldt,
Wolfgang Rolke, Nancy Reid, Gary Hill, and Stathes
Paganis. I would also like to thank Louis Lyons for
his past advice and the invitation to speak at such
an enjoyable and productive conference.
been

authored
by Brookhaven Science Associates, LLC under Con-
tract No. DE-AC02-98CH1-886 with the U.S. De-
partment of Energy. The U.S. Government retains,
and the publisher, by accepting the article for publi-
cation, acknowledges, a world-wide license to publish
or reproduce the published form of this manuscript,
or allow others to do so, for the U.S. Government
purposes.

manuscript

has

5. K. Cranmer and R. S. Bowman, Comp. Phys.

Meth. A458, 745 (2001).

12

References

1. ATLAS Collaboration, Detector and physics
performance technical design report (volume ii)
CERN-LHCC/99-15 (1999).

2. CMS Collaboration, Technical proposal CERN-

LHCC/94-38 (1994).

3. H. B. Prosper Advanced Statistical Techniques
in Particle Physics, Durham, England, 18-22
Mar 2002.

4. J. H. Friedman, Recent advances in (machine)

learning PhyStat2003 (2003).

Commun. 167, 165 (2005).

6. I. Hinchliﬀe, F. E. Paige, M. D. Shapiro,
J. Soderqvist and W. Yao, Phys. Rev. D55,
5520 (1997).

7. S. Frixione and B. R. Webber, The mc@nlo

event generator hep-ph/0207182, (2002).

8. A. Schalicke and F. Krauss, JHEP 07, p. 018

(2005).

9. C. Buttar, D. Clements, I. Dawson and A.
Moraes, Acta Phys. Polon. B35, p. 433 (2004).
10. G. J. Feldman and R. D. Cousins, Phys. Rev.

11. B. Abbott et al., Phys. Rev. D62, p. 092004

D57, 3873 (1998).

(2000).

12. Y. Gao, L. Lu and X. Wang, Signiﬁcance calcu-
lation and a new analysis method in searching
for new physics at the LHC physics/0509174,
(2005).

13. G. Feldman, Concluding talk PhyStat2005

(2003).

20. R. Cousins, Treatment of nuisance parameters
in high energy physics, and possible justiﬁca-
tions and improvements in the statistical litera-
ture. PhyStat2005, (2005).

21. P. Sinervo, Deﬁnition and treatment of system-
atic uncertainties in high energy physics and as-
trophysics PhyStat2003, (2003).

22. K. Cranmer, Frequentist hypothesis

test-
ing with background uncertainty PhyStat2003
physics/0310108 (2003).

23. W. A. Rolke and A. M. Lopez, Nucl. Instrum.

24. W. A. Rolke, A. M. Lopez and J. Conrad, Nucl.

Instrum. Meth. A551, 493 (2005).

25. S. Asai et al., Eur. Phys. J. C3252, 19 (2004).
26. J. Cammin and M. Schumacher, The ATLAS
discovery potential for the channel ttH, (H
→
bb) ATLAS Note ATL-PHYS-2003-024 (2003).
27. V. Drollinger, Th. M¨uller, and D. Denegri,
Searching for Higgs Bosons in Association with
Top Quark Pairs in the H
bb Decay Mode
→
CMS NOTE-2001/054 (2001).

28. R. Cousins and V. Highland, Nucl.

Instrum.

Meth. A320, 331 (1992).

29. K. Cranmer, P. McNamara, B. Mellado,
W. Quayle, and Sau Lan Wu, Conﬁdence level
l+l− /pT for
calculations for H
115 < MH < 130 GeV using vector boson fu-
sion ATL-PHYS-2003-008 (2002).

W +W −

→

→

30. N. Reid, Likelihood inference in the presence of
nuisance parameters PhyStat2003, (2003).
31. F.James and M. Roos, Comput.Phys.Commun.

32. F. James and M. Roos, Nucl. Phys. B 172, 475

33. R. Cousins, Nucl. Instrum. and Meth. in Phys.

34. J. Stuart, A. Ord and S. Arnold, Kendall’s Ad-
vanced Theory of Statistics, Vol 2A (6th Ed.)
(Oxford University Press, New York, 1994).
35. G. Punzi, Ordering algorithms and conﬁdence
intervals in the presence of nuisance parameters
PhyStat2005 (2005).

36. D. Nicolo and G. Signorelli Prepared for Con-
ference on Advanced Statistical Techniques in
Particle Physics, Durham, England, 18-22 Mar
2002.

14. LEP Higgs Working Group, Phys. Lett. B565,

10, 343 (1975).

15. K. Cranmer, Comput. Phys. Commun. 136,

(1980).

Instrum. Meth. A434, 435

Res. A 417, 391 (1998).

(2005).

61 (2003).

198 (2001).
16. T. Junk, Nucl.

(1999).

17. H. Hu and J. Nielsen, Analytic Conﬁdence
Level Calculations Using the Likelihood Ra-
tio and Fourier Transform CERN 2000-005
physics/9906010, (2000).

18. K. Cranmer, B. Mellado, W. Quayle and
Sau Lan Wu, Challenges of Moving the LEP
to the LHC. PhyStat2003
Higgs Statistics
physics/0312050 (2003).

19. J. Linnemann, Comparison of measures of
signiﬁcance in PhyStat2003 physics/0312059,


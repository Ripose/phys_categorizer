6
0
0
2
 
r
a

M
 
1
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
7
1
3
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Additive N-Step Markov Chains as Prototype Model of Symbolic Stochastic
Dynamical Systems with Long-Range Correlations

Z.A. Mayzelis
Department of Physics, Kharkov National University, 4 Svoboda Sq., Kharkov 61077, Ukraine

S. S. Mel’nyk, O. V. Usatenko ∗, V. A. Yampol’skii
A. Ya. Usikov Institute for Radiophysics and Electronics
Ukrainian Academy of Science, 12 Proskura Street, 61085 Kharkov, Ukraine
(Dated: February 21, 2014)

A theory of symbolic dynamic systems with long-range correlations based on the consideration
of the binary N -step Markov chains developed earlier in Phys. Rev. Lett. 90, 110601 (2003)
is generalized to the biased case (non equal numbers of zeros and unities in the chain).
In the
model, the conditional probability that the i-th symbol in the chain equals zero (or unity) is a linear
function of the number of unities (zeros) among the preceding N symbols. The correlation and
distribution functions as well as the variance of number of symbols in the words of arbitrary length
L are obtained analytically and veriﬁed by numerical simulations. A self-similarity of the studied
stochastic process is revealed and the similarity group transformation of the chain parameters is
presented. The diﬀusion Fokker-Planck equation governing the distribution function of the L-words
is explored. If the persistent correlations are not extremely strong, the distribution function is shown
to be the Gaussian with the variance being nonlinearly dependent on L. An equation connecting the
memory and correlation function of the additive Markov chain is presented. This equation allows
reconstructing a memory function using a correlation function of the system. Eﬀectiveness and
robustness of the proposed method is demonstrated by simple model examples. Memory functions
of concrete coarse-grained literary texts are found and their universal power-law behavior at long
distances is revealed.

PACS numbers: 05.40.-a, 02.50.Ga, 87.10.+e

INTRODUCTION

The problem of systems with long-range spatial and/or temporal correlations (LRCS) is one of the topics of intensive
research in modern physics, as well as in the theory of dynamic systems and the theory of probability. The LRC systems
are usually characterized by a complex structure and contain a number of hierarchic objects as their subsystems.
The LRCS are the subject of study in physics, biology, economics, linguistics, sociology, geography, psychology,
etc. [1, 2, 3, 4].

One of the eﬃcient methods to investigate the correlated systems is based on a decomposition of the space of
states into a ﬁnite number of parts labelled by deﬁnite symbols. This procedure referred to as coarse graining can be
accompanied by the loss of short-range memory between states of system but does not aﬀect and does not damage
its robust invariant statistical properties on the large scales. The most frequently used method of the decomposition
is based on the introduction of two parts of the phase space. In other words, it consists in mapping the two parts of
states onto two symbols, say 0 and 1. Thus, the problem is reduced to investigating the statistical properties of the
symbolic binary sequences. This method is applicable for the examination of both discrete and continuous systems.
One of the ways to get a correct insight into the nature of correlations consists in an ability of constructing a
mathematical object (for example, a correlated sequence of symbols) possessing the same statistical properties as
the initial system. There are many algorithms to generate long-range correlated sequences: the inverse Fourier
transform [5], the expansion-modiﬁcation Li method [6], the Voss procedure of consequent random addition [7], the
correlated Levy walks [8], etc. [5]. We believe that, among the above-mentioned methods, using the Markov chains is
one of the most important. This was demonstrated in Ref. [9], where the Markov chains with the step-like memory
function (MF) were studied. It was shown that there exist some dynamical systems (coarse-grained sequences of the
Eukarya’s DNA and dictionaries) with correlation properties that can be properly described by this model.

The many-step Markov chain is the sequence of symbols of some alphabet constructed using a conditional probability
function, which determines the probability of occurring some deﬁnite symbol of sequence depending on N previous

∗ usatenko@ire.kharkov.ua

ones. The property of additivity of Markov chain means the independent inﬂuence of diﬀerent previous symbols on
generated one. The concept of additivity, primarily introduced in paper [10], was later generalized for the case of
binary non-stationary Markov chains [13]. Another generalization was based on consideration of Markov sequences
with a many-valued alphabet [14, 15]. Here we generalize the results of paper [9] to the biased case where the numbers
of zeros and unities are not supposed to be equal.

In the present work, we also continue investigating into additive Markov chains with more complex memory func-
tions. An equation connecting mutually-complementary characteristics of a random sequence, i.e. the memory and
correlation functions, is obtained. Upon ﬁnding the memory function of the original random sequence on the basis
of the analysis of its statistical properties, namely, its correlation function, we can build the corresponding Markov
chain, which possesses the same statistical properties as the initial sequence.

FORMULATION OF THE PROBLEM

Conditional probability of the many-step additive Markov chain

Let us consider a stationary binary sequence of symbols ai, ai = {0, 1}, i ∈ Z = ..., −1, −2, 0, 1, 2, .... To determine
the N -step Markov chain we have to introduce the conditional probability P (ai | ai−N , ai−N +1, . . . , ai−1) of occurring
the deﬁnite symbol ai (for example, ai = 1) after N -word TN,i, where TN,i stands for the sequence of symbols
ai−N , ai−N +1, . . . , ai−1. Thus, it is necessary to deﬁne 2N values of the P -function corresponding to each possible
conﬁguration of the symbols ai in the N -word TN,i.

We suppose that the conditional probability P (ai | TN,i) diﬀers from zero and unity for any word TN,i that provides
the metrical transitivity of the Markov chain (see Appendix). In turn, according the Markov theorem, this property
leads to the ergodicity of the symbolic system under consideration.

Since we suppose to apply our theory to the sequences with long memory lengths of the order of 106, some special

restrictions to the class of P -functions should be imposed. We consider the memory function of the additive form,

P (ai = 1 | TN,i) =

f (ai−r, r).

(1)

1
N

N

r=1
X

Here the function f (ai−k, k)/N describes the additive contribution of the symbol ai−r to the conditional probability of
occurring the symbol unity, ai = 1, at the ith site. The homogeneity of the Markov chain is provided by independence
of the conditional probability Eq. (1) of the index i. It is possible to consider Eq. (1) as the ﬁrst term in expansion
of conditional probability in the formal series, where each term corresponds to the additive (unary), binary, ternary,
and so on functions up to the N -ary one.

It is reasonable to assume the function f to be decreasing with an increase of the distance r between the symbols
ai−r and ai in the Markov chain. However, for the sake of simplicity we consider a step-like memory function f (ai−r, r)
independent of the second argument r. As a result, the model is characterized by three parameters only, speciﬁcally
by f (0), f (1), and N :

Note that the probability P in Eq. (2) depends on the numbers of symbols 0 and 1 in the N -word but is independent
of the arrangement of the elements ai−k. Instead of two parameters f (0) and f (1) it is convenient to introduce new
independent parameters ν and µ (see below Eq. (6)),

P (ai = 1 | TN,i) =

f (ai−r).

1
N

N

r=1
X

f (0) + f (1) = 1 + 2ν,

|ν| < 1/2.

Parameter ν provides the statistical inequality of the numbers of symbols zero and unity in the Markov chain under
consideration. In other words, the chain is biased. Indeed, taking into account Eqs. (2) and (3) and the sequence of
equations,

P (ai = 1|TN,i) =

f (˜ai−r) − 2ν = P (ai = 0 | ˜TN,i) − 2ν,

1
N

N

r=1
X

2

(2)

(3)

(4)

3

(5)

(6)

(7)

(8)

(9)

(10)

one can see the lack of symmetry with respect to interchange ˜ai ↔ ai in the Markov chain if ν 6= 0. Here ˜ai is the
symbol ”opposite” to ai, ˜ai = 1 − ai, and ˜TN,i is the word ”opposite” to TN,i. Therefore, the probabilities of occurring
the words TL,i and ˜TL,i are not equal to each other for any word of the length L. At L = 1 this yields nonequal
average probabilities that symbols 0 and 1 occur in the chain. Particularly, probability of occurring symbol 0 is grater
by 2ν than that of symbol 1. If ν = 0 one has non-biased case.

Taking into account the symmetry of the conditional probability P with respect to a permutation of symbols ai
(see Eq. (2)), we can simplify the notations and introduce the conditional probability pk of occurring the symbol zero
after the N -word containing k unities, e.g., after the word (11...1

),

00...0
N −k

k

| {z }

00 . . . 0
N −k

)

k

pk = P (aN +1 = 0 | 11 . . . 1

| {z }

=

+ ν + µ

1
2

| {z }

| {z }
2k
1 −
N

(cid:18)

,

(cid:19)

µ =

f (0) − f (1)
2

= f (0) −

− ν.

1
2

with the correlation parameter µ being deﬁned by the relation

We focus mainly our attention on the region of µ determined by the persistence inequality 0 < µ. In this case, each
of the symbols unity in the preceding N -word promotes the birth of new symbol unity. Nevertheless, the major part
of our results is valid for the anti-persistent region µ < 0 as well. Note that inequalities |ν| < 1/2 and |µ + ν| < 1/2
follow from Eq. (5)). Without loss of generality, we consider a case ν > 0 only.

Statistical characteristics of the chain

In order to investigate the statistical properties of the Markov chain, we consider the distribution WL(k) of the

words of deﬁnite length L by the number k of unities in them,

and the variance of k,

where

If µ = 0, one arrives at the known result for the non-correlated Brownian diﬀusion,

(cid:18)
We will show that the distribution function WL(k) for the sequence determined by Eq. (5) (with nonzero but not
extremely close to 1/2 − ν parameter µ) is the Gaussian with the variance D(L) nonlinearly dependent on L. However,
at µ → 1/2 − ν the distribution function can diﬀer strongly from the Gaussian.

(cid:19)

For the stationary Markov chain, the probability b(a1a2 . . . aN ) of occurring a certain word (a1, a2, . . . , aN ) satisﬁes

the condition of compatibility for the Chapman-Kolmogorov equation (see, for example, Ref. [16]):

ki(L) =

ai+l,

L

Xl=1

D(L) = k2 − k

2

,

g(k) =

g(k)WL(k).

L

Xk=0

D(L) = L

− ν2

.

1
4

Main equation

b(a1 . . . aN ) =

=

b(aa1 . . . aN −1)P (aN | a, a1, . . . , aN −1).

a=0,1
X

Thus, we have 2N homogeneous algebraic equations for the 2N probabilities b of occurring the N -words and the
normalization equation
b = 1. This set of equations is equivalent to that of Eq. (77). In the case under consideration,
the set of Eqs. (11) can be substantially simpliﬁed owing to the following statement:

Proposition 1: The probability b(a1a2 . . . aN ) depends on the number k of unities in the N -word only, i. e., it is

P

independent of the arrangement of symbols in the word (a1, a2, . . . , aN ).

0.012

0.010

0.008

)
z
(
b

0.006

0.004

0.002

0

50

100

150

200

250

z

FIG. 1: The probability b of occurring a word (a1, a2, . . . , aN ) vs its number z expressed in the binary code, z =
for N = 8, µ = 0.1, ν = 0.03.

N
i=1 ai · 2

i−1,

P

This statement illustrated by Fig. 1 is valid owing to the chosen simple model (2), (5) of the Markov chain. It
can be easily veriﬁed directly by substituting the obtained below solution (15) into the set of Eqs. (11). Note that
according to the Markov theorem, Eqs. (11) do not have other solutions [17].

Proposition 1 evidently leads to the very important property of isotropy: any word (a1, a2, . . . , aL) appears with

the same probability as the inverted one, (aL, aL−1, . . . , a1).
Let us apply the set of Eqs. (11) to the word (11 . . . 1

):

00 . . . 0
N −k

k

This yields the recursion relation for b(k) = b(11...1

b(11 . . . 1

| {z }

| {z }
) = b(0 11 . . . 1

00 . . . 0
N −k

k

)pk+

00 . . . 0
N −k−1

k

| {z }

| {z }
+ b(1 11 . . . 1

| {z }

| {z }

)pk+1.

00 . . . 0
N −k−1

k

| {z }
00...0
N −k

k

| {z }
),

| {z }
b(k) =

| {z }
1 − pk−1
pk

b(k − 1) =

=

N − 2νN − 2µ(N − 2k + 2)
N + 2νN + 2µ(N − 2k)

b(k − 1).

The probabilities b(k) for µ > 0 satisfy the sequence of inequalities,

1 +

< b

1 +

− 1

< ... < b(0),

(cid:18)

(cid:18)

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:19)

(cid:19)

b

b

N
2

N
2

ν
µ

ν
µ

N
2

N
2

ν
µ

ν
µ

(cid:18)

(cid:18)

(cid:19)(cid:19)

(cid:18)

(cid:18)

(cid:19)

(cid:19)

1 +

< b

1 +

+ 1

< ... < b(N ),

(14)

4

(11)

(12)

(13)

which is the reﬂection of persistent properties for the chain.

The solution of Eq. (11) is

with the parameters n1 and n2 deﬁned by

b(k) = A · Γ(n1 + k)Γ(n2 + N − k)

The constant A will be found below by normalizing the distribution function. Its value is,

n1 =

N (1 − 2(µ + ν))
4µ

, n2 =

N (1 − 2(µ − ν))
4µ

.

A =

Γ(n1 + n2)
Γ(n1)Γ(n2)Γ(n1 + n2 + N )

.

DISTRIBUTION FUNCTION OF L-WORDS

In this section we investigate into the statistical properties of the Markov chain, speciﬁcally, the distribution of the
words of deﬁnite length L by the number k of unities. The length L can also be interpreted as the number of jumps
of some particle over an integer-valued 1D lattice or as the time of the diﬀusion imposed by the Markov chain under
consideration. The form of the distribution function WL(k) depends, to a large extent, on the relation between the
word length L and the memory length N . Therefore, the ﬁrst thing we will do is to examine the simplest case L = N .

Statistics of N -words

The value b(k) is the probability that an N -word contains k unities with a deﬁnite order of symbols ai. Therefore,
the probability WN (k) that an N -word contains k unities with arbitrary order of symbols ai is b(k) multiplied by the
number Ck

N = N !/k!(N − k)! of diﬀerent permutations of k unities in the N -word,

Combining Eqs. (15) and (18), we ﬁnd the distribution function,

WN (k) = Ck

N b(k).

The normalization constant WN (0) can be obtained from the equality

WN (k) = 1,

Comparing Eqs. (15), (18)-(20), one can get Eq. (17) for the constant A in Eq. (15).

WN (k) = WN (0)Ck
N

Γ(n1 + k)Γ(n2 + N − k)
Γ(n1)Γ(n2 + N )

.

N

k=0
P
Γ(n1 + n2)Γ(n2 + N )
Γ(n2)Γ(n1 + n2 + N )

.

WN (0) =

Limiting case of weak persistence, n1, n2 ≫ 1

This inequality can be rewritten via the f -function (see Eqs. (2)—(6)),

1 − 2(µ + ν)
4µ

≫

1
N

.

f (1)
f (0) − f (1)

≫

1
N

.

In terms of the correlation parameter µ, this limiting case corresponds to the values of µ not very close to 1/2,

5

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

In the absence of correlations, n2 → ∞, Eq. (19) and the Stirling formula yield the Gaussian distribution at

k, N n1/n2, N − k ≫ 1, k − k0 ≪ N . Given the persistence is not too strong,

one can also obtain the Gaussian form for the distribution function,

n2 ≫ 1,

with the µ-dependent variance,

p

WN (k) =

1

2πD(N )

exp

−

(cid:26)

(k − k0)2
2D(N )

,

(cid:27)

D(N ) =

N (N + n1 + n2)n1n2
(n1 + n2)3

=

=

N
4(1 − 2µ)

4ν2
(1 − 2µ)2

,

(cid:21)

1 −

(cid:20)

k0 =

n1
n1 + n2

N =

N
2(1 − 2µ)

1 −

(cid:20)

2ν
1 − 2µ

.

(cid:21)

It is followed from Eq. (24) that N -words containing k0 unties are the most probable. It is interesting to note, that
the persistence leads to a decrease of the variance D(N, µ > 0) with respect to D(N, µ = 0) = N

1/4 − ν2

if

ν >

1 − 2µ

.

2

3 − 6µ + 4µ2

(cid:0)

(cid:1)

(27)

In other case, for instance, at ν = 0, the persistence results in an increase of the variance D(N, µ). To put it diﬀerently,
the persistence is conductive to the intensiﬁcation of the diﬀusion under conditions opposite to inequality (27).

Inequality n2 ≫ 1 gives D(N ) ≪ N 2. Therefore, despite the increase of D(N ), the ﬂuctuations of (k − k0) of the

p

order of N are exponentially small.

Intermediate case, n2 >

∼ 1

WN (k) =

1
N + 1

.

If the parameters n1 and n2 are integers of the order of unity, the distribution function WN (k) is a polynomial of

degree n1 + n2 − 2. In particular, at n1 = n2 = 1, the function WN (k) is constant,

At n1 6= 1, WN (k) has a maximum within the interval [0, N ]. At n1 = 1 and n2 > 1, WN (k) decreases monotonously
with an increase of k.

Limiting case of strong persistence

If the parameter n2 satisﬁes the inequality,

n2 ≪ ln

−1 N,

or

1 − 2(µ − ν) ≪ 1/N ln(N ),

f (1) ≪ 1/N ln(N ),

then one can neglect the parameters n1 and n2 in the arguments of the functions Γ(n1+k), Γ(n2+N ), and Γ(n2+N −k)
in Eq. (19). In this case, the distribution function WN (k) assumes its maximal values at k = 0 and k = N ,

WN (1) = WN (0)

≪ WN (0).

n1N
N − 1

6

(23)

(24)

(25)

(26)

(28)

(29)

(30)

(31)

7

(32)

(33)

(34)

(35)

(36)

Formula (31) describes the sharply decreasing WN (k) as k varies from 0 to 1 (and from N to N − 1). Then, at
1 < k < N/2, the function WN (k) decreases more slowly with an increase in k,

At k = N/2, the probability WN (k) achieves its minimal value,

It follows from normalization (20) that the values WN (0) and WN (N ) are approximatively equal to n2/(n1 + n2)

and n1/(n1 + n2) respectively. Neglecting the terms of the order of n2

2, one gets

WN (k) = WN (0)

n1N
k(N − k)

.

WN

= WN (0)

N
2

(cid:18)

(cid:19)

4n1
N

.

WN (0) =

(1 − n1 ln N ),

WN (N ) =

(1 − n2 ln N ).

n2
n1 + n2

n1
n1 + n2

D(N ) =

n1n2N 2
(n1 + n2)2 −

n1n2N (N − 1)
n1 + n2

.

In the straightforward calculation using Eqs. (8) and (32) the variance D is

Thus, the variance D(N ) is equal to n1n2N 2/(n1 + n2) in the leading approximation in the parameter n. This fact
has a simple explanation. The probability of occurrence the N -word containing N unities is approximatively equal
1N 2/(n1 + n2)2 give (36). The case of strong
to n1/(n1 + n2). So, the relations k2 ≈ n1N 2/(n1 + n2) and k
if we chose randomly some symbol ai in the
persistence corresponds to the so-called ballistic regime of diﬀusion:
sequence, it will be surrounded by the same symbols with the probability close to unity.

= n2

2

The evolution of the distribution function WN (k) from the Gaussian form to the inverse one with a decrease of
−1 N < n2 < 1 the curve WN (k) is concave and the
the parameters n1 and n2 is shown in Fig. 2. In the interval ln
−1 N < n2 < 1, the curve remains a smooth
maximum of function WN (k) inverts into minimum. At N ≫ 1 and ln
function of its argument k as shown by curve with n = 0.5 in Fig. 2. Below, we will not consider this relatively narrow
region of the change in the parameter n2.

Formulas (24), (25), (32) and (34) — (36) describe the statistical properties of L-words for the ﬁxed ”diﬀusion

time” L = N . Below, we examine the distribution function WL(k) for more general situation, L < N .

0.20

 

(2, 10)

0.15

(30, 40)

)
k
(

W

0
2

0.05

0.00

0.10

 

(0.4, 0.8)

(0.5, 1.5)

0

5

10

15

20

k

FIG. 2: The distribution function WN (k) for N =20 and diﬀerent values of the parameters n1 and n2 shown near the curves.

The distribution function WL(k) at L < N can be given as

This equation follows from the consideration of N -words consisting of two parts,

Statistics of L-words with L < N

Distribution function WL(k)

WL(k) =

b(i)Ck

LCi−k

N −L.

k+N −L

Xi=k

(a1, . . . , aN −L,

aN −L+1, . . . , aN

).

i−k unities

k unities

|
The total number of unities in this word is i. The right-hand part of the word (L-sub-word) contains k unities. The
remaining (i − k) unities are situated within the left-hand part of the word (within (N − L)-sub-word). The multiplier
LCi−k
Ck
N −L in Eq. (37) takes into account all possible permutations of the symbols ”1” within the N -word on condition
that the L-sub-word always contains k unities. Then we perform the summation over all possible values of the number
i. Note that Eq. (37) is a direct consequence of the proposition 1 formulated in Subsec. C of the previous section.

{z

{z

|

}

}

The straightforward summation in Eq. (37) yields the following formula that is valid at any value of the parameters

n1 and n2:

where

and

as

WL(k) = WL(0)Ck
L

Γ(n1 + k)Γ(n2 + L − k)
Γ(n1)Γ(n2 + L)

WL(0) =

Γ(n1 + n2)Γ(n2 + L)
Γ(n2)Γ(n1 + n2 + L)

.

It is of interest to note that the parameters µ, ν and the memory length N are presented in Eqs. (39), (40) via
the parameters n1 and n2 only. This means that the statistical properties of the L-words with L < N are deﬁned by
these ”combined” parameters.

In the limiting case of weak persistence, n2 ≫ 1, at k, Ln1/n2, L − k ≫ 1, Eq. (39) along with the Stirling formula

give the Gaussian distribution function,

with the variance D(L)

p

WL(k) =

1

2πD(L)

exp

−

(cid:26)

(k − k0)2
2D(L)

(cid:27)

In the case of strong persistence (29), the asymptotic expression for the distribution function Eq. (39) can be written

D(L) =

n1n2L
(n1 + n2)2

1 +

(cid:18)

L
n1 + n2 (cid:19)

=

1 +

L
4

(cid:20)

2µL
N (1 − 2µ)

(cid:21) (cid:20)

1 −

4ν2
(1 − 2µ)2

(cid:21)

k0 =

n1L
n1 + n2

=

1 −

L
2

(cid:20)

2ν
1 − 2µ

.

(cid:21)

WL(k) = WL(0)

, k 6= 0, k 6= L,

n1L
k(L − k)

WL(0) =

(1 − n1 ln L), WL(L) =

(1 − n2 ln L).

n2
n1 + n2

n1
n1 + n2

Both the distribution WL(k) (44) and the function WN (k) (32) have concave forms. The former assumes the

maximal values (45) at the edges of the interval [0, L] and has a minimum at k = L/2.

8

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

Variance D(L)

Using the deﬁnition Eq. (8) and the distribution function Eq. (39) one can obtain a very simple formula for the

variance D(L),

D(L) =

Ln1n2
(n1 + n2)2

1 +

(cid:20)

(L − 1)
1 + n1 + n2 (cid:21)

=

1 +

L
4

(cid:20)

2µ(L − 1)
N − 2µ(N − 1)

1 −

4ν2
(1 − 2µ)2

(cid:21) (cid:20)

.

(cid:21)

Eq. (46) shows that the variance D(L) obeys the parabolic law independently of the correlation strength in the Markov
chain.

In the case of weak persistence, at n2 ≫ 1, we obtain the asymptotics Eq. (42).

It allows one to analyze the
behavior of the variance D(L) with an increase in the “diﬀusion time” L. At small µ, the dependence D(L) follows
the classical law of the Brownian diﬀusion, D(L) ≈ L(1/4 − ν2).

For the case of strong persistence, n2 ≪ 1, Eq. (46) gives the asymptotics,

D(L) =

n1n2L2
(n1 + n2)2 −

n1n2L(L − 1)
n1 + n2

.

The ballistic regime of diﬀusion leads to the quadratic law of the D(L) dependence in the zero approximation in the
parameter n2 ≪ 1.

The unusual behavior of the variance D(L) raises an issue as to what particular type of the diﬀusion equation
corresponds to the nonlinear dependence D(L) in Eq. (42). In the following subsection, when solving this problem,
we will obtain the conditional probability p(0) of occurring the symbol zero after a given L-word with L < N . The
ability to ﬁnd p(0), with some reduced information about the preceding symbols being available, is very important for
the study of the self-similarity of the Markov chain (see Subsubsec. 4 of this Subsection).

Generalized diﬀusion equation at L < N , n2 ≫ 1

It is quite obvious that the distribution WL(k) satisﬁes the equation

WL+1(k) = WL(k)p(0)(k) + WL(k − 1)p(1)(k − 1).

Here p(0)(k) is the probability of occurring ”0” after an average-statistical L-word containing k unities and p(1)(k − 1)
is the probability of occurring ”1” after an L-word containing (k − 1) unities. At L < N , the probability p(0)(k) can
be written as

p(0)(k) =

1
WL(k)

k+N −L

Xi=k

pib(i)Ck

LCi−k

N −L.

LCi−k

The product b(i)Ck
i unities, the right-hand part of which, the L-sub-word, contains k unities (compare with Eqs. (37), (38)).

N −L in this formula represents the conditional probability of occurring the N -word containing

The product b(i)Ci−k

N −L in Eq. (49) is a sharp function of i with a maximum at some point i = i0 whereas pi obeys
the linear law (5). This implies that pi can be factored out of the summation sign being taken at point i = i0. The
asymptotical calculation shows that point i0 is described by the equation,

i0 =

1 −

N
2

(cid:18)

2ν
1 − 2µ

−

L/2
1 − 2µ(1 − L/N )

1 −

−

2k
L

(cid:19)

(cid:18)

2ν
1 − 2µ

.

(cid:19)

Expression (5) taken at point i0 gives the desired formula for p(0) because

is obviously equal to WL(k). Thus, we have

k+N −L

Xi=k

b(i)Ck

LCi−k
N −L

9

(46)

(47)

(48)

(49)

(50)

(51)

p(0)(k) =

1 +

1
2

(cid:18)

2ν
1 − 2µ

+

µL
N − 2µ(N − L)

1 −

−

2k
L

(cid:19)

(cid:18)

2ν
1 − 2µ

.

(cid:19)

Let us consider a very important point relating to Eq. (50). If the concentration of unities in the right-hand part
of the word (38) is higher than 1/2 − ν/(1 − 2µ), k/L > 1/2 − ν/(1 − 2µ), then the most probable concentration
(i0 − k)/(N − L) of unities in the left-hand part of this word is likewise increased, (i0 − k)/(N − L) > 1/2 − ν/(1 − 2µ).
At the same time, the concentration (i0 − k)/(N − L) is less than k/L,

1
2

1 −

2ν
1 − 2µ

<

i0 − k
N − L

<

k
L

.

(cid:19)
This implies that the increased concentration of unities in the L-words is necessarily accompanied by the existence
of a certain tail with an increased concentration of unities as well. Such a phenomenon is referred by us as the
macro-persistence. An analysis performed in the following section will indicate that the correlation length lc of this
tail is γN with γ ≥ 1 dependent on the parameters µ and ν only. It is evident from the above-mentioned property of
the isotropy of the Markov chain that there are two correlation tails from both sides of the L-word.

(cid:18)

Note that the distribution WL(k) is a smooth function of arguments k and L near its maximum in the case of weak
persistence and k, L − k, Ln1/n2 ≫ 1. By going over to the continuous limit in Eq. (48) and using Eq. (52) with
the relation p(1)(k − 1) = 1 − p(0)(k − 1), we obtain the diﬀusion Fokker-Planck equation for the correlated Markov
process,

∂W
∂L

=

1
8

∂2W
∂κ2

1 −

(cid:18)

4ν2
(1 − 2µ)2

(cid:19)

−

2µ
(1 − 2µ)N + 2µL

∂
∂κ

(κW ),

where κ = k − L/2. Equation (54) has a solution of the Gaussian form Eq. (41) with the variance D(L) satisfying the
ordinary diﬀerential equation,

dD
dL

=

1 −

1
4

(cid:18)

4ν2
(1 − 2µ)2

+

4µ
(1 − 2µ)N + 2µL

D.

(cid:19)

Its solution, given the boundary condition D(0) = 0, coincides with (42).

Self-similarity of the persistent Brownian diﬀusion

In this subsection, we point to one of the most interesting properties of the Markov chain being considered, namely,
its self-similarity. Let us reduce the N -step Markov sequence by regularly (or randomly) removing some symbols and
introduce the decimation parameter λ,

Here N ∗ is a renormalized memory length for the reduced N ∗-step Markov chain. According to Eq. (52), the
conditional probability p∗
k of occurring the symbol zero after k unities among the preceding N ∗ symbols is described
by the formula,

∗

λ = N

/N ≤ 1.

∗
k =

p

1
2

+ ν

∗

∗
+ µ

1 −

2k
N ∗

,

(cid:19)

(cid:18)

with

∗

N

= λN, ν

= ν

∗

1
1 − 2µ(1 − λ)

∗
, µ

= µ

λ
1 − 2µ(1 − λ)

.

The comparison between Eqs. (5) and (57) shows that the reduced chain possesses the same statistical properties
as the initial one but it is characterized by the renormalized parameters (N ∗, ν∗, µ∗) instead of (N , ν, µ). Thus,

10

(52)

(53)

(54)

(55)

(56)

(57)

(58)

11

(59)

Eqs. (56) and (58) determine the one-parametrical renormalization of the parameters of the stochastic process deﬁned
by Eq. (5).

The astonishing property of the reduced sequence consists in that the variance D∗(L) is invariant with respect to
the one-parametric decimation transformation (56), (58). In other words, it coincides with the function D(L) for the
initial Markov chain:

∗

D

(L) =

Ln∗
1
1 + n∗
2)

1 +

(L − 1)

= D(L), L < N

∗

.

(cid:20)

(n∗

1 + n∗

1 + n∗
2 (cid:21)
Indeed, according to Eqs. (56), (58), the renormalized parameters n∗
2 = N ∗(1 −
2(µ∗ − ν∗))/4µ∗ of the reduced sequence coincides exactly with the parameters n1 and n2 of the initial Markov chain.
Since the shape of the function WL(k) Eq. (39) is deﬁned by the invariant parameters n1 = n∗
2, the
distribution WL(k) is also invariant with respect to the decimation transformation.

1 = N ∗(1 − 2(µ∗ + ν∗))/4µ∗ and n∗

1 and n2 = n∗

The transformation (N , ν, µ) → (N ∗, ν∗, µ∗) (56), (58) possesses the properties of semi-group, i. e., the composition
of transformations (N , ν, µ) → (N ∗, ν∗, µ∗) and (N ∗, ν∗, µ∗) → (N ∗∗, ν∗∗, µ∗∗) with transformation parameters
λ1 and λ2 is likewise the transformation from the same semi-group, (N , ν, µ) → (N ∗∗, ν∗∗, µ∗∗), with parameter
λ = λ1λ2.

The invariance of the function D(L) at L < N was referred to by us as the phenomenon of self-similarity. It is

demonstrated in Fig. 3.

It is interesting to note that the property of self-similarity is valid for any strength of the persistency. Indeed, the
result Eq. (52) can be obtained directly from Eqs. (15)-(17), and (49) not only for n2 ≫ 1 but also for the arbitrary
value of n2.

10

D

1

0.1

 

L

 

1

10

100

FIG. 3: The dependence of the variance D on the tuple length L for the generated sequence with N = 100, µ = 0.4 and
ν = 0.08 (dotted line) and for the decimated sequences (the parameter of decimation λ = 0.5). Squares and circles correspond
to the stochastic and deterministic reduction, respectively. The solid line describes the non-correlated Brownian diﬀusion,
D(L) = L(1/4 − ν 2).

MEMORY FUNCTION AND ITS CONNECTION WITH CORRELATION FUNCTION

Typically, the correlation function and other moments are employed as the input characteristics for the description
of the correlated random sequences. However, the correlation function describes not only the direct interconnection
of the elements ai and ai+r, but also takes into account their indirect interaction via all other intermediate elements.
Our approach operates with the ”origin” characteristics of the system, speciﬁcally, with the memory function. The
correlation and memory functions are mutual-complementary characteristics of a random sequence in the following
sense. The numerical analysis of a given random sequence enables one to directly determine the correlation function
rather than the memory function. On the other hand, it is possible to construct a random sequence using the memory
function, but not the correlation one. Therefore, we believe that the investigation of memory function of the correlated
systems will permit one to disclose their intrinsic properties which provide the correlations between the elements.

12

(60)

(61)

(62)

(63)

The memory function used in Refs. [9, 10] was characterized by the step-like behavior and deﬁned by two parameters
only: the memory depth N and the strength of symbol’s correlations. Such a memory function describes only one
type of correlations in a given system, the persistent or anti-persistent one, which results in the super- or sub-linear
dependence D(L) [22]. Obviously, both types of correlations can be observed at diﬀerent scales in the same system.
Thus, one needs to use more complex memory functions for detailed description of the systems with both type of
correlations. Besides, we have to ﬁnd out a relation connecting the mutually-complementary characteristics of random
sequence, the memory and correlation functions.

Let us rewrite Eq. (1) in an equivalent form,

with

P (ai = 1 | TN,i) = b +

F (r)(ai−r − b),

Main equation

N

r=1
X

1
N

N

r=1
P
1 −

f (0, r)/N

F (r)

N

r=1
P

b =

,

F (r) =

[f (1, r) − f (0, r)].

¯a = lim

M→∞

1
2M + 1

M

ai.

Xi=−M

¯a = P r(ai = 1) =

P (ai = 1 | TN,i)P r(TN,i).

XTN,i

The constant b is the value of ai averaged over the whole sequence, b = ¯a:

Indeed, according to the ergodicity of the Markov chain, ¯a coincides with the value of ai averaged over the ensemble
of realizations of the Markov chain. So, we can write

Here P r(ai = 1) is the probability of occurring the symbol ai equal to unity and P r(TN,i) is the probability of
occurring the deﬁnite word TN,i in the considering ensemble of sequences. Substituting P (ai = 1 | TN,i) from Eq. (60)
into Eq. (63) and taking into account the obvious relation

P r(TN,i) = 1, one gets,

TN,i
P

N

N

The sum

XTN,i
P r(TN,i)ai−r does not depend on the subscript r and obviously coincides with ¯a. So, we have ¯a =

r=1
X

r=1
X

¯a = b − b

F (r) +

F (r)

P r(TN,i)ai−r.

(64)

b + (¯a − b)

F (r). From this equation we conclude that b = ¯a. Thus, we can rewrite Eq. (60) as

TN,i
P
r
P

P (ai = 1 | TN,i) = ¯a +

F (r)(ai−r − ¯a).

(65)

N

r=1
X

We refer to F (r) as the memory function (MF). It describes the strength of inﬂuence of previous symbol ai−r upon
a generated one, ai. To the best of our knowledge, the concept of memory function for many-step Markov chains was
introduced in Ref. [9]. The function P (. | .) contains the complete information about correlation properties of the
Markov chain.

We suggest below two methods for ﬁnding the memory function F (r) of a random binary sequence with a known
correlation function. The ﬁrst one is based on the minimization of a ”distance” Dist between the Markov chain

generated by means of a sought-for MF and the initial sequence of symbols. This distance is determined by the
formula,

Dist = (ai − P (ai = 1 | TN,i))2 = lim
M→∞

(ai − P (ai = 1 | TN,i))2,

(66)

1
2M + 1

M

Xi=−M

with the conditional probability P deﬁned by Eq. (65).

Let us express distance (66) in terms of the correlation function,

K(r) = aiai+r − ¯a2, K(0) = ¯a(1 − ¯a), K(−r) = K(r).

From Eqs. (65), (66), one obtains

Xr,r′

The minimization equation,

Dist =

(ai−r − ¯a)(ai−r′ − ¯a)F (r)F (r

) − 2

(ai − ¯a)(ai−r − ¯a)F (r) + (ai − ¯a)2

=

K(r − r

)F (r)F (r

) − 2

K(r)F (r) + K(0).

′

′

′

r
X

r
X

= 2

K(r − r

)F (r

) − 2K(r) = 0,

′

′

yields the relationship between the correlation and memory functions,

K(r) =

F (r

)K(r − r

),

r ≥ 1.

′

′

Xr′

N

Xr′=1

Equation (70) can also be derived by straightforward calculation of the average aiai+r in Eq. (67) using deﬁnition (65)
of the memory function.

0.050

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

0.045

0.040

K

0.035

0.030

0

5

10

15

20

25

30

r

Xr,r′

δDist
δF (r)

0.08

0.06

0.04

F

0.02

0.00

0

5

10

15

20

25

30

r

FIG. 4: The initial memory function Eq. (72) (solid line) and the reconstructed one (dots) vs the distance r. In inset, the
correlation function K(r) obtained by a numerical analysis of the sequence constructed by means of the memory function
Eq. (72).

The second method resulting from the ﬁrst one, establishes a relationship between the memory function F (r) and

the variance D(L),

M (r, 0) =

′

F (r

)M (r, r

),

′

N

Xr′=1

13

(67)

(68)

(69)

(70)

(71)

0.08

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

0.06

               

               

               

               

               

               

               

               

               

               

0.0

               

               

               

               

               

               

               

               

               

               

0

5

10

15

20

25

30

0.02

r

0.04

K

0.00

-0.02

0.3

0.2

0.1

F

-0.1

r

0

5

10

15

20

25

30

FIG. 5: The model correlation function K(r) described by Eq. (73) (solid line). The dots correspond to the reconstructed
correlation function. In inset, the memory function F (r) obtained by numerical solution of Eq. (70) with correlation function
Eq. (73).

M (r, r

) = D(r − r

) − (D(−r

) + r[D(−r

+ 1) − D(−r

)]).

′

′

′

′

′

It is a set of linear equations for F (r) with coeﬃcients M (r, r′) determined by D(r). The relations, K(r) = [D(r −
1) − 2D(r) + D(r + 1)]/2 obtained in Ref. [9] and D(−r) = D(r) are used here.

Let us verify the robustness of our method by numerical simulations. We consider a model ”triangle” memory

function,

F (r) = 0.008

r,
20 − r,
0,

(

1 ≤ r < 10,
10 ≤ r < 20,
r ≥ 20,

presented in Fig. 4 by solid line. Using Eq. (65), we construct a random non-biased, ¯a = 1/2, sequence of symbols
{0, 1}. Then, with the aid of the constructed binary sequence of the length 106, we calculate numerically the correlation
function K(r). The result of these calculations is presented in inset Fig. 4. One can see that the correlation function
K(r) mimics roughly the memory function F (r) over the region 1 ≤ r ≤ 20. In the region r > 20, the memory function
is equal to zero but the correlation function does not vanish [23]. Then, using the obtained correlation function K(r),
we solve numerically Eq. (70). The result is shown in Fig. 4 by dots. One can see a good agrement of initial, Eq. (72),
and reconstructed memory functions F (r).

The main and very nontrivial result of our paper consists in the ability to construct a binary sequence with an
arbitrary prescribed correlation function by means of Eq. (70). As an example, let us consider the model correlation
function,

Numerical simulations

K(r) = 0.1

sin(r)
r

,

presented by the solid line in Fig. 5. We solve Eq. (70) numerically to ﬁnd the memory function F (r) using this
correlation function. The result is presented in inset Fig. 5. Then we construct the binary Markov chain using the
obtained memory function F (r). To check up a robustness of the method, we calculate the correlation function K(r)
of the constructed chain (the dots in Fig. 5) and compare it with Eq. (73). One can see an excellent agreement
between the initial and reconstructed correlation functions.

Let us demonstrate the eﬀectiveness of our concept of the additive Markov chains when investigating the correlation
properties of coarse grained literary texts. First, we use the coarse-graining procedure and map the letters of the text
of Bible [24] onto the symbols zero and unity (here, (a − m) 7→ 0, (n − z) 7→ 1). Then we examine the correlation
properties of the constructed sequence and calculate numerically the variance D(L). The result of simulation of the
normalized variance Dn(L) = D(L)/4¯a(1 − ¯a) is presented by the solid line in Fig. 6. The dominator 4¯a(1 − ¯a) in the

14

(72)

(73)

7

6

5

4

3

2

1

0

n

D

10

10

10

10

-1

10

0.04

0.00

F

15

10

10

10

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

10

               

               

               

               

               

               

               

               

               

               

2

1

0

n

 

 

D

/

D

1

2

3

10

10

10

L

0

1

2

3

4

5

6

10

10

10

10

10

10

10

L

FIG. 6: The normalized variance Dn(L) for the coarse-grained text of Bible (solid line) and for the sequence generated by
means of the reconstructed memory function F (r) (dots). The dotted straight line describes the non-biased non-correlated
Brownian diﬀusion, D0(L) = L/4. The inset demonstrates the anti-persistent dependence of ratio Dn(L)/D0(L) on L at short
distances.

equation for the normalized variance Dn(L) is inserted in order to take into account the inequality of the numbers
of zeros and unities in the coarse-grained literary texts. The straight dotted line in this ﬁgure describes the variance
D0(L) = L/4, which corresponds to the non-biased non-correlated Brownian diﬀusion. The deviation of the solid
line from the dotted one demonstrates the existence of correlations in the text. It is clearly seen that the diﬀusion is
anti-persistent at small distances, L <

∼ 300, (see inset Fig. 6) whereas it is persistent at long distances.

The memory function F (r) for the coarse-grained text of Bible at r < 300 obtained by numerical solution of Eq. (71)
is shown in Fig. 7. At long distances, r > 300, the memory function can be nicely approximated by the power function
F (r) = 0.25r−1.1, which is presented by the dash-dotted line in inset Fig. 7.

-0.04

               

               

               

               

               

r

 

2

3

4

10

10

10

               

               

               

               

               

-3

10

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

-4

1x10

F

-5

1x10

r

-0.08

               

               

               

               

               

1

10

100

FIG. 7: The memory function F (r) for the coarse-grained text of Bible at short distances. In inset, the power-law decreasing
portions of the F (r) plots for several texts. The dots correspond to ”Pygmalion” by B. Shaw. The solid line corresponds to
power-law ﬁtting of this function. The dash dotted and dashed lines correspond to Bible in English and Russian, respectively.

Note that the region r <

∼ 40 of negative anti-persistent memory function provides much longer distances L ∼ 300

of anti-persistent behavior of the variance D(L).

Our study reveals the existence of two characteristic regions with diﬀerent behavior of the memory function and,
correspondingly, of persistent and anti-persistent portions in the D(L) dependence. This appears to be a prominent
feature of all texts written in any language. The positive persistent portions of the memory functions are given in
inset Fig. 7 for the coarse-grained English- and Russian-worded texts of Bible (dash-dotted and dashed lines, Refs. [24]
and [25], correspondingly). Besides, for comparison, the memory function of the coarse-grained text of ”Pygmalion”
by B. Shaw [26] is presented in the same inset (dots), the power-law ﬁtting is shown by solid line.

It is interesting to note that the memory function of any text mimics the correlation function, as it was found for
the model example Eq. (73). This fact is conﬁrmed by Fig. 8 where the correlation function of the coarse-grained
text of Bible is shown. One can see that its behavior at both short and long scales is similar to the memory function
presented in Fig. 7. However, the exponents in the power-law approximations of K(r) and F (r) functions diﬀer
essentially.

16

0.01

0.00

K

-0.01

-0.02

2

3

4

 

r

10

10

10

-3

10

K

-4

10

r

1

10

100

FIG. 8: The correlation function K(r) for the coarse-grained text of Bible at short distances. In inset, the power-law decreasing
portions of the K(r) plot for the same text. The solid line corresponds to power-law ﬁtting of this function.

CONCLUSION

Thus, the simple, exactly solvable model of the uniform binary N -step Markov chain is presented. The memory
length N , the parameter µ of the persistent correlations and the biased parameter ν are three parameters in our theory.
The correlation function K(r) is usually employed as the input characteristics for the description of the correlated
random systems. Yet, the function K(r) describes not only the direct interconnection of the elements ai and ai+r, but
also takes into account their indirect interaction via other elements. Since our approach operates with the ”original”
parameters N , µ and ν, we believe that it allows us to reveal the intrinsic properties of the system which provide the
correlations between the elements.

We have demonstrated the eﬃciency of description of the symbolic sequences with long-range correlations in terms
of the memory function. An equation connecting the memory and correlation functions of the system under study is
obtained. This equation allows reconstructing a memory function using a correlation function of the system. Actually,
the memory function appears to be a suitable informative ”visiting card” of any symbolic stochastic process. The
eﬀectiveness and robustness of the proposed method is demonstrated by simple model examples. Memory functions
for some concrete examples of the coarse-grained literary texts are constructed and their power-law behavior at long
distances is revealed. Thus, we have shown the complexity of organization of the literary texts in contrast to a
previously discussed simple power-law decrease of correlations [4].

If the memory length N of the system under consideration is of order of the very system length then the Markov
chain, modeling the system, could be non-stationary. In this case the proposed method does not allow to describe the
system precisely, as distinct from the method proposed in [27, 28].

APPENDIX. MATRIX OF THE CONDITIONAL PROBABILITY

In this Appendix, we prove the property of metrical transitivity of the N -step Markov chains.
It is possible to look at the Markov chain from the other point of view and consider it as a 1-step vector Markov

chain. To this end, we introduce the N -component vector-function Xl,

Xl = (al+1, al+2, ..., al+N ),

l = ..., −2, −1, 0, 1, 2, ...

(74)

17

(75)

(76)

The number of diﬀerent sets of symbols (ai+1, ai+2, ..., ai+N ) is equal to Q = 2N . We number the diﬀerent states of
the vector Xl by their binary representation,

D(aN , aN −1, ..., a1) = aN 20 + aN −121 + ... + a12N −1,

0 ≤ D ≤ 2N − 1.

the probabilities of transition of the vector X =
The matrix elements Mik of the probability matrix M , i.e.
(a1, a2, ..., aN ) into the vector Y = (a′
N ) can be expressed via the function of conditional probability
P (ai | TN,i). The subscripts i and k of the matrix Mik are determined by the binary representations of the sequences
(a1, a2, ..., aN ) and (a′
1).
Every matrix row contains only two non-zero elements since the vector X1 can take up two values only, namely,
(a2, a3, ..., aN , 0) and (a2, a3, ..., aN , 1). For k ≤ Q/2, let us denote the probability of occurring of aN +1 = 0 as 1 − Pk,
where the index k is equal to k = 1 + D(aN , aN −1, ..., a1) in the binary representation.

N ), correspondingly: i = 1 + D(aN , aN −1, ..., a1) and k = 1 + D(a′

N −1, ..., a′

2, ..., a′

2, ..., a′

N , a′

1, a′

1, a′

For the index k being in the range from Q/2 + 1 to Q, we denote the probability of occurring of symbol aN +1 = 0
after the word a1, a2, ..., aN as Pk. Then, 1 − Pk is the probability of occurring of the symbol unity. Taking into
account that aN = 0 for k ≤ Q/2 and obvious relations,

D(aN −1, ..., a1, 0) = 2D(aN , aN −1, ..., a1),

D(aN −1, ..., a1, 1) = 2D(aN , aN −1, ..., a1) + 1,

we get the transition probabilities matrix M :

P1
0
...
0

0
...
0

Q

Xk=1

M =

1 − PQ/2+1 PQ/2+1

1 − P1
0
...
0

0
...
0















0
1 − P2
...
...
0

0
P2
...
...
0

...
1 − PQ/2+2 PQ/2+2 0 ...
...
...
0 0

...
...

...
...

0
0
...

...
...
0
0 ...
0
...
...
...
0 0 1 − PQ/2 PQ/2
0
...
0
...
1 − PQ

0
0
...
PQ

.















Thus, to determine the vectors b of probability distribution of N -words in the stationary Markov chain we need to
solve the system of equations,

bi =

bkMki,

bk = 1.

(77)

Q

Xk=1

In other words, one needs to obtain the normalized eigenvector corresponding to the eigenvalue λ = 1 of the matrix
M (N ) of the order Q = 2N . It is clear that if the vector b satisﬁes to the condition bM = b then for every integer k the
condition bM k = b is also true, here M k is the power k of the matrix M . Let us consider the matrix M N and show
that all matrix elements are positive. In this case, following the Markov theorem we can conclude that the matrix M
determines uniquely the probability of the words distribution.

Let us suggest that for any k < N the matrix M k satisﬁes to the next conditions: in the ﬁrst row the elements M1i
for i = 1, . . . , 2k are positive, in the second row the positive elements are M2i with i = 2k + 1, . . . 2 × 2k, ...
in the
2N −k-th row — i = (2N −k − 1)2k, . . . , 2N . In the next rows this order is repeated. Let us demonstrate that if the
matrix M k obeys to this rules, then it is true for the matrix M k+1 also.

After multiplication of matrixes M k and M the elements of obtained matrix are deﬁned by the expression:

M k+1(i, j) =

M k(i, l)M (l, j).

(78)

Xl
Let us consider the ﬁrst row of the matrix M k+1 — i = 1. In each column of the matrix M only two elements are
non-zero. After multiplication of the ﬁrst row of the matrix M k to some column of the matrix M the result is non-zero
(positive) for j ≤ 2 ∗ 2k only, because positive elements of the matrix M corresponds to the positive zone (i < 2k)
of the ﬁrst row of matrix M k only for this j. So the described rule remains for the ﬁrst row of the matrix M K+1.
Similarly this fact can be proved for other rows.

The matrix M 1 obeys to this rule, consequently, by induction, it is true for all M k. In according to this rule, if

power k = N , then all elements of the matrix M N are positive.

Therefore, from the Markov theorem, there is the unique solution of the system bM N = b (or bM = b). This

solution can be obtained by the method of successive approximations,

bk+1
i = bk

j M (j, i),

k = 0, 1, 2, ...,

if we start from the arbitrary initial distribution b0
probability vector b.

j . In the limit k → ∞ we get to the stationary distribution of the

Taking into account the explicit form of the matrix M , the equation (77) comes to the next equations:

18

(79)

(80)

For Q = 2 we get the well known result [21]:

And in the case Q = 4 we obtain the next result:

bi(1 − Pi) + bi+Q/2Pi+Q/2 = b2i−1,

biPi + bi+Q/2(1 − Pi+Q/2) = b2i.

M =

1 − P1
P2

P1
1 − P2 (cid:19)

,

(cid:18)

b1 =

P2
P1 + P2

,

b2 =

P1
P1 + P2

.

1 − P1
0
P3
0

P1
0
1 − P3
0

0
1 − P2
0
P4

0
P2
0
1 − P4



,





M = 





b1 =

P3p4
P1P2 + 2P1P4 + P3P4

,

b2 = b3 =

P1P4
P1P2 + 2P1P4 + P3P4

,

b4 =

P1P2
P1P2 + P1P4 + P3P4

.

[1] H. E. Stanley et. al., Physica A 224,302 (1996).
[2] A. Provata and Y. Almirantis, Physica A 247, 482 (1997).
[3] R. N. Mantegna, H. E. Stanley, Nature (London) 376, 46 (1995).
[4] I. Kanter and D. F. Kessler, Phys. Rev. Lett. 74, 4559 (1995).
[5] A. Czirok, R. N. Mantegna, S. Havlin, and H. E. Stanley, Phys. Rev. E 52, 446 (1995).
[6] W. Li, Europhys. Let. 10, 395 (1989).
[7] R. F. Voss, in: Fundamental Algorithms in Computer Graphics, ed. R.A. Earnshaw (Springer, Berlin, 1985) p. 805.
[8] M. F. Shlesinger, G. M. Zaslavsky, and J. Klafter, Nature (London) 363, 31 (1993).
[9] O.V. Usatenko, V.A. Yampol’skii, K.E. Kechedzhy, and S.S. Mel’nyk, Phys. Rev. E 68, 061107 (2003).
[10] O. V. Usatenko and V. A. Yampol’skii, Phys. Rev. Lett. 90, 110601 (2003).
[11] C. V. Nagaev, Theor. Probab. & Appl., 2, 389 (1957) (In Russian).
[12] M. I. Tribelsky, Phys. Rev. Lett. 87, 070201 (2002).
[13] S. Hod and U. Keshet, Phys. Rev. E 70, 015104(R) (2004).
[14] S. L. Narasimhan, J. A. Nathan, and K. P. N. Murthy, Europhys. Lett. 69 (1), 22 (2005).
[15] S. L. Narasimhan, J. A. Nathan, P. S. R. Krishna, and K. P. N. Murthy, arXiv:cond-mat/0409053.
[16] C. W. Gardiner: Handbook of Stochastic Methods for Physics, Chemistry, and the Natural Sciences, (Springer Series in

[17] A. Katok, B. Hasselblat: Introduction to the Modern Theory of Dynamical Systems, (Cambridge University Press; Factorial,

Synergetics, Vol. 13) Springer-Verlag, 1985).

Moscow, 1999) p. 768.

[18] I. A. Ibragimov, Yu. V. Linnik: Independent and stationary connected variables, (Nauka, Moscow, 1965) p. 524.
[19] A. Schenkel, J. Zhang, and Y. C. Zhang, Fractals 1, 47 (1993).
[20] M. T. Madigan, J. M. Martinko, J. Parker: Brock Biology of Microorganisms, (Prentice Hall, 2002) p. 1104.
[21] B.V. Gnedenko: The Theory of Probability, (Chelsea Publ. Co., 1962).
[22] Note that we here discuss the dependence of the variance D upon the length L that describes the persistent (or antipersis-
tent) correlations in the words of diﬀerent lengths L. This length does not coincides with the memory length N , L 6= N .
The dependence D(L) is completely diﬀerent from the dependence D on the memory length N discussed in Ref. [14, 15].

[23] The existence of the ”additional tail” in the correlation function is in agreement with Ref. [9] and corresponds to the well

known fact that the correlation length is always larger then the region of memory function action.

[24] The Old Testament of the King James Version of the Bible, http://www.writersbbs.com/bible/.
[25] Russian Synodal LiO 31/7/91, http: //lib.ru/ hristian/bibliya/nowyj zawet.txt.
[26] http://eserver.org/drama/pygmalion/default.html.
[27] P. Allegrini, M. Buiatti, P. Grigolini, and B.J. West, Phys. Rev. E 58, 3640 (1998).
[28] P. Allegrini, P. Grigolini, and L. Palatella, Chaos, Solitons and Fractals 20, 95 (2004).

19


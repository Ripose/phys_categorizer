6
0
0
2
 
g
u
A
 
1
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
0
0
9
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

VARIATIONAL METHOD FOR ESTIMATING THE RATE OF
CONVERGENCE OF MARKOV CHAIN MONTE CARLO
ALGORITHMS

FERGAL P. CASEY† , JOSHUA J. WATERFALL‡ , RYAN N. GUTENKUNST‡,
CHRISTOPHER R. MYERS§, AND JAMES P. SETHNA‡

Abstract. We demonstrate the use of a variational method to determine a quantitative lower
bound on the rate of convergence of Markov Chain Monte Carlo (MCMC) algorithms as a function
of the target density and proposal density. The bound relies on approximating the second largest
eigenvalue in the spectrum of the MCMC operator using a variational principle and the approach
is applicable to problems with continuous state spaces. We apply the method to one dimensional
examples with Gaussian and quartic target densities, and we contrast the performance of the basic
Metropolis-Hastings algorithms with a “smart” variant that incorporates gradient information into
the trial moves. We ﬁnd that the variational method agrees quite closely with numerical simulations.
We also see that the smart MCMC algorithm often fails to converge geometrically in the tails of the
target density except in the simplest case we examine, and even then care must be taken to choose
the appropriate scaling of the deterministic and random parts of the proposed moves. We apply the
same method to approximate the rate of convergence in multidimensional Gaussian problems with
and without importance sampling. Thus we demonstrate the necessity of importance sampling for
target densities which depend on variables with a wide range of scales.

Key words. Markov Chain Monte Carlo, convergence rate, variational method

AMS subject classiﬁcations. 65C05, 65C40, 65C60, 65K10

1. Introduction. Markov Chain Monte Carlo (MCMC) methods are important
tools in parametric modeling [?, ?] where the goal is to determine a posterior distri-
bution of parameters given a particular dataset. Since these algorithms tend to be
computationally intensive, the challenge is to produce algorithms that have better
convergence rates and are therefore more eﬃcient [?, ?]. Of particular concern are
situations where there is a large range of scales associated with the target density,
which we ﬁnd is widespread in models from many diﬀerent ﬁelds [?, ?, ?, ?, ?].

There are a number of techniques to either determine exactly or bound the con-
vergence rate for MCMC algorithms on discrete state spaces [?], but there has been
little discussion on ﬁnding quantitative eigenvalue bounds for continuous state spaces.
Where work has been done in that area [?, ?], upper bounds on the convergence rate
can be derived but the techniques are rather involved and the bounds may not be
very useful. Therefore, in this work, we show that a conceptually straightforward
variational method can provide convergence rate estimates for continuous state space
applications. Even though we provide only lower bounds on the convergence rate we
show these bounds can be remarkably tight. Furthermore, lower bounds allow us to
discover conditions for which the MCMC method fails to converge.

We have been able to obtain explicit formulas for one dimensional example prob-
lems but the method may be more generally applicable, when applied in an approxi-
mate way, as we demonstrate for a multidimensional problem.

2. Markov Chain Monte Carlo. Typically, one wishes to obtain a sample
x1, x2... from a probability distribution π(x) which is sometimes called the target dis-
tribution. An MCMC algorithm works by creating a Markov chain that has π(x) as

†Center for Applied Mathematics, Cornell University, Ithaca, NY 14853 (ferg@cam.cornell.edu).
‡Laboratory of Atomic and Solid State Physics, Cornell University, Ithaca, NY 14853
§Cornell Theory Center, Cornell University, Ithaca, NY 14853, USA

1

2 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

its stationary distribution, i.e. after many steps of the chain any initial distribution
converges to π(x). A suﬃcient condition to establish π(x) as the stationary distribu-
tion is that the chain be ergodic and that the transition density, t(x, y), of the chain
satisfy detailed balance:

π(x) t(x, y) = π(y) t(y, x).

Given a proposal density q(x, y) for generating moves, one way to construct the re-
quired transition density [?, ?] is to deﬁne t(x, y) = α(x, y) q(x, y) where

α(x, y) = min

q(y, x)π(y)
q(x, y)π(x)

, 1

(cid:19)

(cid:18)

(2.1)

is the acceptance probability of the step x
y. Obtaining the sample from the
stationary distribution then involves letting the chain run past the transient (burn-
in) time and taking uncorrelated samples from the late time trajectory. How long it
takes to reach the stationary distribution determines the eﬃciency of the algorithm
and for a given target distribution, clearly it depends on the choice of the proposal
density. We can write down the one-step evolution of a probability density p(x) as a
linear operator:

→

p)(y) =

t(x, y)p(x) dx +

t(y, x) dx

p(y)

(
L

1
(cid:18)

−

Z

(cid:19)
t(y, x)p(y)) dx + p(y)

Z

Z

=

(t(x, y)p(x)

−

where dx = dx1 . . . dxn, dy = dy1 . . . dyn, n is the dimension of the state space and
all integrals are from
here and elsewhere in this manuscript. The second
form makes it explicit that p(y) = π(y) is the stationary distribution by the detailed
balance relation.

−∞

∞

to

Now, if the linear operator has a discrete set of eigenfunctions and eigenvalues,
it holds that the asymptotic convergence rate is determined by the second largest
eigenvalue in absolute value (the largest being one) [?, ?]. We will write this eigenvalue
as λ∗, and will refer to it as the second eigenvalue meaning the second largest in
absolute value. Assuming geometric convergence of the chain [?], the discrepancy
between the density at the mth iterate of the chain and the target density decreases
as (λ∗)m for large m. Therefore we would like λ∗ to be as small as possible.

The variational calculation allows us to obtain an estimate for λ∗, but before we
can do this we need to convert our operator into a self-adjoint form which ensures
that the eigenfunctions are orthogonal. This is easily accomplished by a standard
technique [?] of deﬁning a new transition density by s(x, y) = t(x, y)
π(y)
and our self-adjoint operator is then given by

π(x)/

p

p

p)(y) =

s(x, y)p(x) dx +

t(y, x) dx

p(y)

(
S

1
(cid:18)

−

Z

(cid:19)
t(y, x)p(y)) dx + p(y)

=

(s(x, y)p(x)

−

(2.2)

(2.3)

where the “diagonal” part of the old operator (multiplying p(y)) need not be trans-
is self-adjoint. Note
formed using s(x, y). It is easy to show that deﬁned as above,
that if u(x) is an eigenfunction of the operator
π(x)u(x) is an eigenfunction
of the original operator

with the same eigenvalue.

, then

S

S

p

Z

Z

L

VARIATIONAL METHOD FOR MCMC

3

(y

x)T L(y

L
|
p

/(2π) exp
|

2.1. Metropolis-Hastings and smart Monte Carlo. MCMC algorithms es-
sentially diﬀer only in the choice of proposal density and acceptance probability that
is used in selecting steps. We will refer to the standard Metropolis-Hastings (MH)
algorithm as that which uses a symmetric proposal density to determine the next
move; for example, a Gaussian centred at the current point:
q(x, y) =
where L is an inverse covariance ma-
trix that needs to be chosen appropriately for the given problem (importance sam-
pling). In other words, the proposed move from x to y is given by y = x + R where
N(0, L−1) is a normal random variable, mean 0 and covariance L−1. Thus the
R
update on the current state is purely random. We will see that when the target den-
sity is not spherically symmetric, a naive implementation of the Metropolis-Hastings
algorithm where the step scales are all chosen to be equal leads to very poor per-
formance of the algorithm. As would be expected the convergence deteriorates as a
function of the ratio of the true scales of the target density to the scale chosen for the
proposal density.

−
(cid:0)

x)/2

∼

−

−

(cid:1)

One variant used to accelerate the standard algorithm is a smart Monte Carlo
method [?] that uses the gradient of the negative of the log target density at every
step, G(x) =

log(π(x)) to give

q(x, y) =

exp

−∇

L
|
|
√2π
p

1
2

−

(cid:18)

(y

(x

−

−

−1G(x))T L(y

H

−1G(x))

H

(2.4)

(x

−

−

(cid:19)

−

∼

N(0, L−1) and a deterministic component

and H can be considered either as a constant scaling of the gradient part of the
step or, if it is the Hessian of
log(π(x)), as producing a Newton step. The move
−
H −1G(x) + R, so now we have a random component
to y is generated as y = x
H −1G(x). Viewed like this, moves
R
can be considered to be steps in an optimization algorithm (moving to maximize the
probability of the target density) with random noise added. We will see that with an
optimal choice of H and for Gaussian target densities, the smart Monte Carlo method
can converge in one step to the stationary distribution. We will also see that for a
one dimensional non-Gaussian distribution it actually fails to converge geometrically,
independent of the values of the scaling parameters.

−

from Eqn. 2.3, and we know the eigenfunction with eigenvalue λ1 = 1,

2.2. Variational method. Once we have the self-adjoint operator for the chain,
π(x), we
S
can look for a candidate second eigenfunction in the function space orthogonal to the
ﬁrst eigenfunction where the inner product is deﬁned by (p1, p2) =
p1(x)p2(x) dx.
Given a family of normalized candidate functions in this space, va(x), with variational
parameter a, the variational principle [?, ?] states
∗
λ

(2.5)

p

1

R

(va,

va)

maxa|

S

| ≤

≤

and depending on how accurately our family of candidate functions captures the true
second eigenfunction, this can give quite a close approximation to the second dominant
eigenvalue. In the problems we examine in the following sections the target densities
have an even symmetry which makes it straightforward to select a variational trial
function: any function with odd symmetry will naturally lie in the orthogonal space.
For more complicated problems with known symmetries this general principle may be
useful in selecting variational families for the purposes of algorithm comparison.

Writing out explicitly for

in (va,

va) we have

S

S

(va,

va) =

S

Z Z

−

Z Z

va(x)s(x, y)va(y) dxdy

t(y, x) (va(y))2 dxdy + 1 .

(2.6)

4 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

As we will see in the following section, the lower bound in Eqn. 2.5 can be arbitrarily
close to 1 and therefore equality holds. In these situations we see the chain does not
converge geometrically. We will also see that there can be eigenvalues in the spectrum
that are close to
λn|
|
where λn < 0. Interestingly, for this situation there is oscillatory behaviour of the
Markov chain state space density.

1 which determine the asymptotic convergence rate, i.e. λ∗ =

−

3. Examples.

3.1. Gaussian target density. Consider the simplest case of a one dimensional
kx2/2) with variance 1/k. Under
k/(2π) exp(

Gaussian target distribution π(x) =
−
the standard MH algorithm, the proposal density is

q(x, y) =

exp

(3.1)

p

l
2π

r

1
2

−

(cid:18)

l(y

x)2

.

−

(cid:19)

The issue is to determine l optimally; a ﬁrst guess would be that l = k is the best
choice. We will see that this is not actually correct.
To begin, deﬁne a variational function va(x)

ax2/2), orthogonal to the
v2
a dx = 1. We can motivate this choice
target density and normalized such that
by recognizing that any initial distribution that is asymmetric will most likely have a
R
component of this test function, and a convergence rate estimate based on it roughly
corresponds to how fast probability “equilibrates” between the tails. (More commonly,
variational calculations will use linear combinations of many basis functions with the
coeﬃcients as variational parameters. We ﬁnd here that including higher order terms
in the test function is unnecessary as we obtain tight enough bounds just retaining
the lowest order term.)

x exp(

−

∝

≤

−

−

x, y

≤ ∞

1 and

−∞ ≤

1
−
(va,

log(π(y)) + log(π(x)) = k(y2

We proceed by evaluating Eqn. 2.6 noting that because of the form of the ac-
ceptance probability, Eqn. 2.1, there are two functional forms for the kernels t(x, y)
and s(x, y) delineated by the equation y2 = x2, i.e. whether the “energy” change,
x2)/2, is positive or negative. (It is then
∆E(x, y) =
convenient to deﬁne the coordinate change y = rx, x = x or x = ry, y = y where
to evaluate the integrals.) An explicit expression for

r
va) can be obtained for this case of a Gaussian target density.

≤
S
Next, we use a numerical optimization method to maximize the bound deﬁned
by Eqn. 2.5 with respect to a. The result of this analysis is shown in Fig. 3.1 along
with an empirically determined convergence rate for comparison. (To obtain the rate
empirically, we run the MCMC algorithm for many iterates on an initial distribution
and observe the long time diﬀerences from the target distribution. These diﬀerences
are either ﬁt using Hermite polynomial functions or by looking for the multiplicative
factor by which the density changes from one iterate to the next.) The variational
bound tightly matches the asymptotic convergence rates in this case, and an optimum
step size l can be ascertained. Clearly our l = 1 initial guess for the best scaling is
far from optimal.

Moving to the one dimensional smart Monte Carlo, we have a Gaussian proposal

density of the form :

q(x, y) =

exp

l
2π

r

1
2

l

y
(cid:18)

 −

(x

−

−

k
h

x)

2

!

(cid:19)

(3.2)

where 1/l is the variance of the random part of the step and 1/h is the scale of the

VARIATIONAL METHOD FOR MCMC

5

1

0.95

0.9

0.85

λ*

0.8

0.75

0.7

0.65

0.6
0

0.2

0.4

0.8

1

1.2

0.6
l

Fig. 3.1. Variational estimate on the second eigenvalue for the one dimensional Gaussian
problem using the standard MH method, with k = 1.0. The variational estimate is the solid line and
the empirically determined values are marked with stars. Some of the empirical convergence rates
seem to be less than the lower bound, but this is due to inaccuracies in their estimation

deterministic part. (Letting h
3.1.)

→ ∞

we recover the standard MH algorithm of Eqn.

Taking h = k corresponds to performing a Newton step at every iterate of the
algorithm. Thus, since the log of the target density is purely quadratic, the current
point will always be returned to the extremum at 0 by the deterministic component
of the smart Monte Carlo step and the random component will give a combined
, which has the form of an
l/(2π) exp
move drawn from q(x, y) = q(y) =
independence sampler [?]. If we then also choose l = k, we see immediately that we
are generating moves from the target distribution from the beginning, i.e. we have
convergence in one step starting from any initial distribution.

−
(cid:0)

ly2/2

p

(cid:1)

−

In real problems, however,

log(π(x)) will not be quadratic. We may obtain
an estimate for l and h by considering its quadratic approximation or curvature but
in many cases those estimates will have to be adjusted.
If the curvature is very
small (or in multidimensional problems if the quadratic approximations are close to
singular), the parameters will have to be increased to provide a step size control to
prevent wildly unconstrained moves (analogous to the application of a trust region
in optimization methods [?]). If the curvature is large but we believe that the target
density is multimodal, we need to decrease the parameters to allow larger steps to
escape the local extrema. Therefore we examine in the following the dependence of
the convergence rate as we vary both of the parameters l and h.

The acceptance probability Eqn. 2.1 has two functional forms separated by a

boundary in the (x, y) plane given by

k
h

−

(cid:18)

k
h

(cid:19)(cid:19)

−

(cid:18)

k + l

2 +

(y2

x2) = b(k, h, l)(y2

x2) = 0 .

(3.3)

−

6 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

In particular, the acceptance probability is

α(x, y) = min

exp

b(k, h, l)(y2

x2)

, 1

.

(3.4)

1
2

−

(cid:18)

(cid:18)

−

(cid:19)

(cid:19)

Now we have a complication over the standard MH method because depending on the
sign of the coeﬃcient function b(k, h, l) in Eqn. 3.3, we ﬁnd that either α(x, y) < 1
on

or vice versa. This is shown in Fig. 3.2.

<

y
|

, α(x, y) = 1 on
x
|

y
|

|

| ≥ |

x
|
|

α(x, y) < 1

y

α(x, y) = 1

y

b(1, h, l)

0

≥

α(x, y) = 1

α(x, y) = 1

x

α(x, y) < 1

α(x, y) < 1

x

α(x, y) < 1

α(x, y) = 1

b(1, h, l) < 0

h = 0.5

0.5

1

1.5

2

l

4

3
h
2

1

(a)

(b)

(c)

Fig. 3.2. Regions in xy plane where acceptance probability α(x, y) < 1 or α(x, y) = 1, when (a)
0 and (b) b(1, h, l) < 0. The equation for the boundary is shown in (c), see Eqn. 3.3 with

b(1, h, l)
k = 1.0. (The standard MH algorithm will only have regions described by (a). )

≥

As before, for a given value of h and l, we need to break up the double integrals
va), Eqn. 2.6, into the appropriate regions. Our choice
of the scalar product (va,
of variational function is the same as before (since the target density is the same)
and we again can get an explicit (but complicated) expression for Eqn. 2.6 which we
maximize with respect to a. The results of this analysis are shown in Fig. 3.3 (a),
where we ﬁx k = 1.0 and vary h, l. We have conﬁrmed that these lower bounds are
quite accurate as shown in Fig. 3.3 (b).

S

The remarkable feature of these results is that even for this simple Gaussian
problem, the selection of step scale parameters h, l is critical to achieve convergence.
As already mentioned, there is a trivial choice of optimum with h = l = k = 1
that gives one step convergence from any initial distribution (and therefore λ∗ = 0).
However, if we change parameters inﬁnitesimally such that l = 1 + ǫ, h = 1 (ǫ > 0) we
go through a discontinuous transition where we see no convergence from any initial
distribution. This can be understood by recognizing that after one step we will have
(1 + ǫ)x2/2) which has a factor
a proposal density (before accept/reject)
ǫx2/2) less probability in its tails than the target density. Suppose there is an
exp(
initial distribution or point mass concentrated at x = 20/√ǫ. The proposed step of
the smart Monte Carlo algorithm, starting at x, will revisit x too infrequently by a
0 to be accepted
factor exp(

100). Thus detailed balance will force the transition x

exp(

−

∝

−

−

→

VARIATIONAL METHOD FOR MCMC

7

3.5

4

3

2.5

h

2

1.5

1

0.5

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

λ*

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

0.5

1.5

2

0.5

1

2

2.5

3

1

l

(a)

1.5

h

(b)

Fig. 3.3. Estimate of second eigenvalue for the symmetrized smart Monte Carlo operator. (a)
k = 1 is ﬁxed and h, l are allowed to vary. (h = 1.0, l = 1.0 is the optimal scaling for deterministic
and random parts of the step.) (b) We take a slice through this surface at l = 1.5 and empirically
determine the second eigenvalue at points along this curve (stars). The error bars are too small to
be seen. Dashed lines are discontinuities.

with a probability of only exp(
exponentially long time to converge to the target density.

−

100), and thus the initial distribution will take an

|

L

→

y
|

0. This corresponds to a perturbation on the target density of x

In fact this is one of the two disconnected regions where no geometric convergence
is observed in Fig. 3.3. The largest of the two (with h > 1/2) is deﬁned exactly
by the equation b(1, h, l) < 0 (compare Fig. 3.2 (c) with Fig. 3.3 (a)).
In this
region the bound on the second eigenvalue approaches 1 as the variational parameter,
a
π(x) for the
unsymmetrized MCMC operator
. In other words, we have a test distribution that
has exponentially more probability in its tails than the target density. For initial
states x arbitrarily far away from the origin, the acceptance probability α(x, y) in the
is arbitrarily small. To see this, note that Eqn. 3.4 is an exponentially
region
<
x
|
|
x2 in this region, and given the form of the proposal density
decaying function of y2
−
x2 is arbitrarily large and negative.
Eqn. 3.2, we see that the expected value of y2
Thus states far out will never be “allowed back” and the fat tails of
π(x) will never
shrink back down those of π(x). Furthermore, moves x
are
y where
p
always accepted (because α(x, y) = 1 on
) which simultaneously prevents
x
y
|
|
|
convergence. The situation is analogous to that described for l = 1 + ǫ and h = k = 1,
except now there is a cutoﬀ both on the deterministic step and the random step.
A typical example of this is shown in Fig. 3.4. Once we cross to the b(1, h, l)
0
region, moves x
are always accepted by Eqn. 3.4 (Fig. 3.2 (a)).
Therefore excess probability in the tails is allowed to ﬂow back into the central part
of the distribution and the convergence is not blocked.

y where

| ≥ |

x
|
|

x
|

y
|

y
|

→

p

→

−

≥

>

<

|

|

In the second region where no convergence is observed, (h < 1/2 in Fig. 3.3),
) leads to the
1)th to nth
βx(n−1) where β > 2. The trial variational function for this
0, again implying that the tails are not

we have a situation where the deterministic step alone (taking l
→ ∞
proposed moves being generated by an unstable mapping, from the (n
iterate: x(n) = x(n−1)
situation also maximizes the bound as a

−

−

→

8 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

target density

)
)
x
(
p
(
g
o

l

0

-1

-2

-3

-4

-5

-6

-7

-8

-20

-10

10

20

0
x

Fig. 3.4. Forty iterates of the smart Monte Carlo algorithm (solid lines), Eqn. 3.2, when the
initial distribution is normal with standard deviation ﬁve times the Gaussian target density (dashed
line). Parameters are chosen to be in the region of no convergence (h = 2.0, l = 1.5), see Fig. 3.3
(a). We see that the tails of the initial distribution are essentially unchanging after many iterates
and have failed to converge to the target density

decaying to the stationary distribution. The reason is that, even when l <
, we
have a situation in which the expected or mean position of a state x after one step is
. Thus excessive probability in the tails cannot be shifted inward to
x
y where
|
| ≥ |
match the target density.

y
|

∞

The h = 1/2 “trough” is a special case where we have oscillatory behaviour. That
is, the second eigenvalue is negative but greater than
1 and in fact convergence does
occur. Interestingly setting h = k/2 means that b(k, h, l) = k and the acceptance
probability of Eqn. 3.4 looks again like that of the standard MH algorithm, but the
convergence is actually faster. In a sense, given that the deterministic part of the
step moves x
x and the target distribution is symmetric, the oscillatory behavior
allows the chain to sample the distribution twice as fast.

→ −

−

3.2. Quartic target density. In scientiﬁc or statistical applications where
MCMC is used, the log of the target density will ordinarily have higher order terms
beyond the quadratic order we studied in the previous section. For example, in a
Bayesian inference problem the posterior distribution will rarely have a simple Gaus-
sian form. Both ﬁnding the maximum a posteriori parameter estimates and sampling
from the posterior are made more diﬃcult in the presence of these higher order terms.
Therefore, we wish to extend the previous example by studying a target dis-
tribution of the form π(x) = 2(3/4)k(1/4)/Γ(1/4) exp
Here, the log of the
target density is quartic and the proposal density (Gaussian) no longer has the same
form as the target density. We would like to understand the performance of the
Monte Carlo algorithms in this circumstance. (The test distribution is taken to be

kx4/2

−

(cid:0)

(cid:1)

x exp

ax4/2

, i.e. in the orthogonal space to the stationary distribution).

∝

−
(cid:0)

(cid:1)

VARIATIONAL METHOD FOR MCMC

9

The goal is to estimate the optimal value of l, as before. We can argue approxi-
mately that the step scale should be such that kx4/2
1 for a typically move x, i.e.
1).
the change in energy is about 1 and the acceptance probability is therefore exp(
This gives a typical value for x2 = √2/√k. Since the proposal density is Gaussian
with variance 1/l, we therefore would naively predict l = √k/√2. Applying the vari-
ational method, we were unable to ﬁnd a closed form solution to Eqn. 2.6 so we had
to resort to numerical integrals in determining the bound in Eqn. 2.5. The results are
shown in Fig. 3.5 for the standard MH method; it suggests an optimal choice for the
step size parameter, l, which is an improvement over our initial guess of 1/√2 (when
k = 1).

≈

−

0.85

0.8

0.75

λ*

0.7

0.65

0.6

0.55
0

0.5

1

2

2.5

3

1.5
l

Fig. 3.5. Second eigenvalue estimate from the variational method (solid line) and data points
(stars), for the quartic target density (k = 1) using the standard MH method, Eqn. 3.1. The
numerical values for λ∗ are now estimated by taking the ratio of the discrepancy from the target
density in subsequent iterates and ﬁnding a single multiplicative factor which describes the decay.
This is done rather than using functional forms analogous to Hermite polynomials to ﬁt the decay,
because it appears that there may be more signiﬁcant contributions from higher order terms. This
also explains why the lower bound shown diﬀers more than in Fig. 3.1 and Fig. 3.3 (b). The data
point shown at 1/√2

.71 (see text) does not appear to be optimal.

≈

Turning to the smart Monte Carlo algorithm, if we wish to make the deterministic
log(π(x)) at x = 0
part of the proposed move a Newton step using the Hessian of
we are left with a singular Hessian and an inﬁnite deterministic step, reinforcing the
need for the step length control parameter, h.

−

Surprisingly, we ﬁnd that, independent of the value of h and l, (k ﬁxed at 1), the
0. Thus there are no choices of scaling parameters
scalar product (va,
which will lead to convergence. This is borne out by numerical simulation, see Fig.
3.6 for the changes in an initial density under many iterates of the algorithm with an
arbitrary choice for s, h.

1 as a

va)

→

→

S

The failure of the smart Monte Carlo method for the quartic problem is clearly
due to non-convergence of the tails of the distribution, and can be seen by analyzing

10 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

target density

0

-1

-2

-3

-4

-5

-6

-7

)
)
x
(
p
(
g
o

l

-8
-10

-5

0
x

5

10

Fig. 3.6. Forty iterates of the smart Monte Carlo algorithm (solid lines), Eqn. 3.2, when the
target density is quartic (dashed line). The initial distribution of points is normal with standard
deviation about ﬁve times that of target density (dashed line). Parameters are arbitrarily chosen as
(h = 1.0, l = 1.0), and we see that the tails of the initial distribution are unchanged for every iterate
of the algorithm. Other parameter sets tested lead to the same behaviour.

the integrals deﬁning the operator, Eqn. 2.6, and noting that they all tend to zero as
the variational parameter tends to zero, independent of the choice for k, h and l.

The quartic problem is a representative example containing higher order terms
beyond quadratic in the log of the target density. Almost all real world applications
will involve higher order nonlinearities and we would expect to see a similar inability
of the smart Monte Carlo method to converge geometrically.
It may well be that
in applications where the method is extensively used (e.g. [?, ?, ?]) the convergence
criteria are less precise than ours.
(For example it may be acceptable to merely
monitor the variance of some function of the state space variables and conclude that
convergence has been achieved when it ceases to change appreciably.)

4. Multidimensional target densities. For multidimensional problems, it is
quite common that there is a large range of scales associated with the target density [?,
?, ?]. That is, the curvature of the probability density along some directions in
the parameter space is much larger than in other directions. Clearly, if an MCMC
method is not designed to take these diﬀerent scales into account through importance
sampling, the algorithm will perform very poorly. If the curvature is very high in
a particular direction and we try to take a moderately sized step, it will almost
certainly be rejected but if we take small steps in directions that are essentially ﬂat
the MCMC algorithm will be very slow to equilibrate. We would like to show explicitly
here what happens to the convergence rate when the scale of the problem has been
underestimated or overestimated.

The variational calculations for the one dimensional examples of the previous
section either yielded explicit formulas or gave integrals that were relatively fast to

VARIATIONAL METHOD FOR MCMC

11

S

≥

compute numerically. However as we go to multiple dimensions neither of these fea-
va) will not
tures are present, in general. Typically the integrals describing (va,
factor into one dimensional integrals. For Gaussian target densities the full space
is broken into regions analogous to those in Fig. 3.2, described by an equation like
ytAy
xtAx where A is a symmetric n by n matrix which is not necessarily positive
deﬁnite. For the standard MH algorithm applied to a multivariate Gaussian target
density with inverse covariance matrix K, we have A = K, and therefore all the di-
mensions are coupled through the energy change, ∆E = ytKy
xtKx. We would still
like to be able to get a lower bound on λ∗, and to this end note that any test function
orthogonal to the target density will work in Eqn. 2.5; we do not explicitly need to
introduce a variational parameter. It is still necessary to make choices that are both
v) and are “diﬃcult” functions for the given algorithm to
tractable in computing (v,
converge from.

−

S

As an example, take the multivariate Gaussian distribution of the form

π(x) =

exp

xtKx

K
|
(2π)
p

|

n
2

1
2

−

(cid:18)

(cid:19)

(4.1)

with x = (x1, ..., xn), and consider using the MH algorithm with importance sampling,
i.e.

q(x, y) =

exp

L
|
n
(2π)
2
p

|

1
2

−

(y

−

x)tL(y

x)

−

(cid:18)
where again L is the inverse covariance matrix/step size control term and to simplify
we assume that both K and L are diagonal. Without any analysis we might guess
that the optimum choice for L is K.

(cid:19)

First we construct a test function that will provide a useful bound when the
proposed steps are too large for the natural scale of the problem. For simplicity,
consider putting a delta function distribution at the origin. If we take large steps
the acceptance probability should be low and there will be a large overlap between
the initial state and the ﬁnal state. In the limit that the proposed steps have inﬁnite
length, the initial state will not be changed at all and the bound on the second
eigenvalue will approach one. To do this more carefully we deﬁne a test function
which is a Gaussian whose variance will ultimately be taken to zero to represent
the delta function. However, we also need to add another term to ensure the test
function is orthogonal to the target density, in order to apply the variational bound.
Therefore, for the unsymmetric operator we write the test function as : uσ(x) =
Aπ(x) + Bwσ(x) where wσ(x) is the probability density for a multivariate Gaussian
−
with covariance matrix σ2I and A and B are constants. For the symmetrized operator
π(x). A and B
the trial function is transformed to vσ(x) =
are constrained to satisfy the orthogonality relation (vσ, π) = 0 and a normalization
(vσ, vσ) = 1. These lead to the conditions

π(x) + Bwσ(x)/

p

p

−

A

A = B and B2

dx = 1 + B2 .

2

wσ(x)

Z  

π(x) !

p

Then it can be seen that

(vσ,

vσ) =

B2 + B2

S

−

wσ(x)
π(x)

,

 S

wσ(x)

π(x) !

p

p

12 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

where we have used the orthogonality condition, the fact that wσ(x) integrates to 1
and that

is self-adjoint. Writing out the operator

explicitly we get

S

wσ(x)
π(x)

,

 S

wσ(x)

=

π(x) !

p

p

s(x, y)

dxdy

t(x, y)

dxdy

S

−

Z Z

2

wσ(x)

 

π(x) !

p

wσ(y)
π(y)

2

p
dx .

wσ(x)
π(x)

p

wσ(x)

Z Z

+

Z  

π(x) !

p

The last term on the right hand side is (1 + B2)/B2, making use of the normalization
condition, so we are left with

s(x, y)

dxdy

B2

t(x, y)

dxdy+1 .

2

wσ(x)

 

π(x) !

a delta function) we can make

p

(vσ,

vσ) = B2

S

Z Z

wσ(x)
π(x)

wσ(y)
π(y)

p
Since we are ultimately taking a limit as σ
→
approximations to these integrals as follows :

p

−

Z Z
0 (wσ →

Z Z

wσ(x)
π(x)

p

wσ(y)
π(y)

p

s(x, y)

dxdy

s(0, 0)

≈

Z Z

and

t(x, y)

dxdy

t(0, y) dy

Z Z

≈

Z

Finally by taking σ

0 we have the expression

2

wσ(x)

 

π(x) !

p

→

v0, v0) = 1

t(0, y) dy .

(
S

−

Z

wσ(x)
π(x)

wσ(y)
π(y)

dxdy

p

p

2

wσ(x)

dx .

Z  

π(x) !

p

As already mentioned, for the multidimensional problem we expect diﬀerent functional
forms for the kernels s(x, y) and t(x, y) depending on the initial and ﬁnal state (x, y)
and this is what makes decoupling the integrals diﬃcult. However for this choice
of test function the equation for the boundary (with x = 0) is given by ytKy = 0
and since K is positive semideﬁnite we always stay on one side of the boundary (the
energy never decreases from the initial distribution placed at x = 0). Then

v0, v0) = 1

(
S

L
|
|
n
− p
(2π)
2
n

Z

1
2

−

(cid:18)

= 1

−

i=1 r
Y

li
li + ki

.

exp

yt(K + L)y

dy

(cid:19)

(4.2)

(4.3)

where li and ki are the diagonal elements of the diagonal matrices L and K, respec-
tively. With no importance sampling we would have L = kI where k would be chosen
to make suﬃciently large steps to enable it to sample π(x). A rough argument as
follows can give some insight into the form of Eqn. 4.3 : 1/√li is a measure of the
scale in the ith coordinate direction of the proposal density, 1/√ki is the scale in the
ith coordinate direction of the target density. Suppose that li ≪
ki for each i, that is
the scales of the proposal density are too large in all directions. Then the ratio of the

VARIATIONAL METHOD FOR MCMC

13

mean volume of moves generated by q(0, y) to the volume occupied by π(y) is exactly
Intuitively, this ratio is proportional to the acceptance probability,
ki the acceptance probability determines the convergence

n
i=1 √li/√ki.

and in the regime li ≪
Q
properties.

We want to use Eqn. 4.3 to show how choosing step sizes too large even in one
direction will result in a very ineﬃcient algorithm. Suppose that for all but one of
1 which would be roughly the correct
the directions we make li = ki, i = 1, . . . , n
scaling in those directions. Then the bound on the second eigenvalue is

−

v0, v0) = 1

(
S

n−1

1
2

− s(cid:18)

(cid:19)

1
1 + kn/ln

.

s

(4.4)

From this we can see that as we go to larger and larger step sizes relative to the scale
), the bound on λ∗ increases to 1. Conversely we can
in the last direction (kn/ln → ∞
argue that if one of the directions of the target density has a scale that is considerably
smaller than the step scales being used in the proposal density, we will get very few
acceptances and the convergence rate will be close to 0. Hence we see explicitly the
need for importance sampling to accelerate convergence.

We would also like to address what happens in the other limit as the step size
becomes excessively small compared to the natural scale of the problem.
(In fact
Eqn. 4.3 gives a lower bound of zero in that case which is not surprising as it is based
essentially on the term in the operator equation which gives the probability of staying
at the current state. If we take inﬁnitesimally small steps, the acceptance probability
will be one and we will never stay at the current state). When the step scales are
inﬁnitesimally small we expect intuitively that the bound on the second eigenvalue
will also approach one; even though the acceptance ratio is close to one, very small
steps will never be able to “explore” the target distribution suﬃciently. To compute
this limit, we propose a test function which has components of the target density in
all directions except the last, where it has an antisymmetric form to make sure it is
orthogonal to the target density. With respect to the symmetrized operator
this
means

S

v(x)

xn

∝

πi(xi) .

n

i=1
Y

(4.5)

p
πi(xi) is the one dimensional Gaussian density which is the ith factor in a
Here
diagonalized multivariate Gaussian density. We still have the problem of decoupling
the n-dimensional multivariate problem into n one dimensional problems. To manage
this we use a device to re-express the operator equation , Eqn. 2.6, explicitly in terms
log π(y)
of the change 1

π(x) ), which we denote by ∆E. That is

xtKx). (i.e.

2 (ytKy

p

−

−

(v,

v) =

v(x)s(x, y)v(y) dxdy

t(x, y) (v(x))2 dxdy + 1

xnπ(x)q(x, y)

min(e

∆E

−

Z Z
−∆E, 1) δ

S

=

Z Z

Z Z

Z Z

 Z

 Z

1
2

1
2

−

−

n

i=1
X
n

i=1
X

 

 

ki(y2

i −

x2
i )

d∆E

dxdy

−

!

!

!

!

ki(y2

i −

x2
i )

d∆E

dxdy

x2
nπ(x)q(x, y)

min(e

−∆E, 1) δ

∆E

14 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

Then we use the integral representation of the delta function δ(x) = 1
2π
factor q(x, y) =

−
n
i=1 qi(xi, yi), and rearrange the order of integration to give :

exp(

iwx) dw,

min(exp(

∆E), 1)

A(w) exp(

iw∆E) dw

d∆E

(4.6)

−

(cid:18)Z

−

where A(w) contains the integration over the now decoupled (x, y) coordinates :

(v,

v) =

S

1
Q
2π

Z

n−1

A(w) =

πi(xi)qi(xi, yi) exp

 

i=1 Z Z
Y

1
2

(cid:18)

iwki(y2

x2
i )

i −

dxi dyi

! ×

(cid:19)

R

(cid:19)

(xnyn −

x2
n)πn(xn)qn(xn, yn) exp

(1 + ki

i + w))

1
2

1
li w(

−

i kn
ln w

(1 + kn

ln w(

−

i + w))

3
2

=

Z Z
n−1

i=1
Y

1
2

(cid:18)

iwki(y2

x2
n)

n −

(cid:19)

dxn dyn (4.8)

(4.7)

(4.9)

3

−

ln w(

i + w))

Note that the complex integral with respect to dw has a branch point at the roots
of (1 + kn
2 which lie on the imaginary axis at r1 and r2. It simpliﬁes
1) and assume that
the analysis to consider the situation ki = li for i = 1, . . . , (n
n
1)/2
order poles and not branch points, also on the imaginary axis. If we now also assume
that kn < sn, then we can take a contour as shown in Fig. 4.1 when ∆E < 0 and
a similar one in the lower imaginary plane when ∆E > 0. Thus Eqn. 4.9 is reduced

1 is even. This way, the roots of (1 + w(

2 , r1,0 and r2,0, are (n

i + w))

−

−

−

−

n−1

Im

r
1

r
1,0

Re

Fig. 4.1. Contour used to evaluate Eqn. 4.9 when ∆E < 0. r1 is a branch point and r1,0 is
1)/2. The contour is the same for ∆E < 0 except restricted to the negative

a pole of order (n
imaginary plane.

−

to a residue term and a real integral which needs to be evaluated numerically. The
result is plotted for n = 11 in Fig. 4.2 along with the bound that came from Eqn.
4.4. Thus we see the trade oﬀ between taking large steps that potentially can explore
the space quickly but have a higher chance of being rejected and taking small steps
which will have a high acceptance probability but will be unable to sample the space
quickly. As we saw when doing the full variational calculation for the one dimensional
problems, the best step scale to use is not what we may have guessed; the natural
choice ln = kn = 1 here does not appear to minimize the second eigenvalue. We
believe this kind of “approximate” variational approach may be a useful way to deal
with problems which are diﬃcult to analyze otherwise.

VARIATIONAL METHOD FOR MCMC

15

1.00

0.98

0.96

∗λ

0.94

0.92

0.90
0

1

2

3

4

5

step scale

Fig. 4.2. Lower bound on second eigenvalue for the multivariate Gaussian problem, Eqn. 4.1,
1/ln. kn = 1 sets the scale of the target density in the last direction.
with n = 11. Step scale =
The test function is chosen as the negative of the target density perturbed by a delta function (solid
line) or as the target density itself in all directions but the last (dashed line). The estimate for the
lower bound is a maximum of the two curves.

p

5. Conclusion. By applying a variational method, it is possible to obtain an
accurate (lower bound) estimate for the second eigenvalue of an MCMC operator and
thus the asymptotic convergence rate of the chain to the target distribution. Given
such an estimate we can optimally tune the parameters in the proposal distribution to
improve the performance of the algorithm. The procedure has a role to play between
the various numerical algorithms that perform convergence diagnostics before the full
simulations are run, to allow the user to manually tune parameters, and the adaptive
schemes [?, ?] that require no preliminary exploration.

In addition, the variational method allows us to discover weaknesses in variants
of the basic Metropolis-Hastings algorithm which on the surface appear to be reason-
able prescriptions for sampling the target density. This is most dramatically seen in
the smart Monte Carlo method discussed above which apparently has serious ﬂaws
for even the simplest of one dimensional target densities. Although the smart MC
method has been widely used in molecular dynamics applications [?, ?, ?] the scales
are often chosen by physical considerations (for example, to not exceed signiﬁcantly
the step sizes needed to accurately describe the dynamical evolution of the system)
and furthermore, the diagnostics of convergence are not as rigorous as ours; typically
a physical quantity is monitored till it appears to reach an equilibrium value, the rare
events which correspond to the tails of the target distribution are possibly of lesser
importance in those studies. Therefore the convergence problems we have discussed
here, to our knowledge, have not been previously examined.

It would be interesting to apply the same technique to the more broadly used
gradient based hybrid MC algorithms [?] and other non-adaptive accelerated methods
(e.g. parallel tempering [?]). More generally, the variational analysis could be a useful

16 F.P. CASEY, J.J. WATERFALL, R.N. GUTENKUNST, C.R. MYERS AND J.P. SETHNA

tool in making comparisons between the convergence properties of the latest MCMC
algorithms without extensive numerical simulation.

Acknowledgments. The authors wish to thank Cyrus Umrigar for discussions
and the USDA-ARS plant pathogen systems biology group at Cornell University for
computing resources. CRM acknowledges support from USDA-ARS project 1907-
21000-017-05.


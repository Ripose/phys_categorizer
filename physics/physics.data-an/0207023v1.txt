2
0
0
2
 
l
u
J
 
5
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
2
0
7
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Data Compression and Entropy Estimates by Non-sequential Recursive Pair
Substitution

Peter Grassberger
John-von-Neumann Institute for Computing, Forschungszentrum J¨ulich, D-52425 J¨ulich, Germany
(Dated: February 2, 2008)

We argue that Non-sequential Recursive Pair Substitution (NSRPS) as suggested by Jim´enez-
Monta˜no and Ebeling can indeed be used as a basis for an optimal data compression algorithm.
In particular, we prove for Markov sequences that NSRPS together with suitable codings of the
substitutions and of the substitute series does not lead to a code length increase, in the limit of
inﬁnite sequence length. When applied to written English, NSRPS gives entropy estimates which
are very close to those obtained by other methods. Using ca. 135 GB of input data from the project
Gutenberg, we estimate the eﬀective entropy to be ≈ 1.82 bit/character. Extrapolating to inﬁnitely
long input, the true value of the entropy is estimated as ≈ 0.8 bit/character.

PACS numbers: 02.50.-r, 05.10.-a, 05.45.Tp

I.

INTRODUCTION

The discovery that the amount of information in a mes-
sage (or in any other structure) can be objectively mea-
sured was certainly one of the major scientiﬁc achieve-
ments of the 20th century. On the theoretical side, this
quantity – the information theoretic entropy – is of inter-
est mainly because of its close relationship to thermody-
namic entropy, its importance for chaotic systems, and
its role in Bayesian inference (maximum entropy princi-
ple). Practically, estimating the entropy of a message
(text document, picture, piece of music, etc.)
is im-
portant because it measures its compressibility, i.e. the
optimal achievement for any possible compression algo-
rithm.
In the following, we shall always deal with se-
quences (s0, s1, . . .) built from the characters of a ﬁnite
alphabet A = {a0, . . . , am−1} of size m. In the simplest
case the alphabet consists just of 2 characters, in which
case the maximum entropy is 1 bit per character.

Indeed, information entropy as introduced by Shan-
non [1] is a probabilistic concept. It requires a measure
(probability distribution) to be deﬁned on the set of all
possible sequences. In particular, the probability for st
to be given by ak, given all characters s0, s1, . . . , st−1, is
given by

′

′′

pt(k|k

, k

, . . .) =

prob(st = ak | st−1 = ak′ , st−2 = ak′′ , . . .).

In case of a stationary measure with ﬁnite range cor-
relations, pt(k|k′, k′′, . . .) becomes independent of t for
t → ∞. Then Shannon’s famous formula,

(1)

(2)

h = lim
i→∞

h(i)

with

h(i) = − X

k1...ki

p(k1 . . . ki) log2 p(k1|k2 . . . ki) ,

(3)

gives the average information per character. The gen-
eralization to non-stationary measures is straightforward
but will not be discussed here.

In contrast to this approach are attempts to deﬁne
the exact information content of a single ﬁnite sequence.
Theoretically, the basic concept here is the algorithmic
complexity AC (or algorithmic randomness) [2, 3]. For
any given universal computer U , the AC of a sequence S
relative to U is given by the length of the shortest pro-
gram which, when input to U , prints S and then makes
U to stop, so that the next sequence can be read. If S
is randomly drawn from a stationary ensemble with en-
tropy h, then one can show that the AC per character
tends towards h, for almost all S and all U , as the length
of S tends towards inﬁnity [4]. Thus, except for rare se-
quences which do not contribute to averages, h sets the
limit for the compressibility.

Practically, the usefulness of AC is limited by the fact
that there cannot exist any algorithm which ﬁnds for
each S its shortest code (such an algorithm could be used
to solve Turing’s halting problem, which is known to be
impossible) [4]. But one can give algorithms which are
often quite eﬃcient. Huﬀman, arithmetic, and Lempel-
Ziv coding are just three well known examples [5]. Any
such algorithm can be used to give an upper bound to
h (modulo ﬂuctuations from the ﬁnite sequence length)
while, inversely, knowledge of h sets a lower limit to the
average code lengths possible with these codes.

A data compression scheme is called optimal, if it does
not do much worse than the best possible for typical ran-
dom strings. More precisely, let {S} be a set of sequences
with entropy h(S), and let the code string C(S) be built
from an alphabet of mC characters. Then we call the
coding scheme C : S → C(S) optimal, if

length[C(S)]
length[S]

→

h
log2 mC

for length[S] → ∞ (4)

and for nearly all S. While Huﬀman coding is not opti-
mal, arithmetic and Lempel-Ziv codings are [5].

In several papers, Jim´enez-Monta˜no, Ebeling, and
others [6, 7] have suggested coding schemes by non-
sequential recursive pair substitution (NSRPS) [8]. Call
the original sequence S0. We count the numbers njk
of non-overlapping successive pairs of characters in S0

where st = aj and st+1 = ak, and ﬁnd their maximum,
nmax = maxj,k<m njk. The corresponding index pair is
(j0, k0). Then we introduce a new character by concate-
nation

am = (aj0 ak0 )

(5)

and form the sequence S1 by replacing everywhere the
pair aj0 ak0 by am. For the special case of j0 = k0, any
string of 2r + 1 characters aj0 is replaced by r characters
am, followed by one aj0 .

This is then repeated recursively: The sequence Si+1
is obtained from Si by replacing the most frequent pair
aji aki by a new character am+i. The procedure stops if
one can argue that further replacements would not pos-
sibly be of any use. Typically this will happen if the
code length consisting of both a description of Si+1 and
a description of the pair (ji, ki) is deﬁnitely longer than
a description of Si, for the present and all subsequent i.
Thus one sees that eﬃcient encodings (which must also
be uniquely decodable!) of the sequences Si and of the
type of substituted pairs become crucial for the analysis
of NSRPS. Unfortunately, the “codings” given in [6, 7]
are neither eﬃcient nor uniquely decodable [9]. Thus
their “complexities” have no direct relationship to h or
to algorithmic complexity (in contrast to their claim),
and it is not clear from their work whether NSRPS can
be made into an optimal coding scheme at all.

It is the purpose of the present paper to give at least
partial answers to this. More precisely, we shall only be
concerned with the limit of inﬁnitely long strings, where
the information encoded in the pairs (ji, ki) can be ne-
glected in comparison with the information stored in Si,
at least for any ﬁnite i. We will ﬁrst show analytically
that a coding scheme for Si exists which satisﬁes a neces-
sary condition for optimality (Sec.2). We then apply this
to written English (Sec.3), where we shall also compare
our estimates of h to those obtained with other methods.

II. NSRPS FOR MARKOV SEQUENCES

Let us for the moment assume that S0 is binary (the
two characters are “0” and “1”), and that it is completely
random, i.e.
identically and independently distributed
(iid) with the same probability for each character. Thus
p(0| . . .) = p(1| . . .) = 1/2, and h = 1 bit. The length of
S0 is N0, thus the total average information stored in S0
is N0 bits.

No coding scheme can reduce the length of C(S0) to
less than N0 bits on average.
Indeed, all schemes will
have length[C(S0)] > N0 bits (strict inequality!), unless
the “coding” is a verbatim copy. For a coding scheme to
be optimal, a necessary (but not suﬃcient) condition is
that

length[C(S0)]/N0 → 1 bit

(6)

for N0 → ∞, i.e. the overhead in the code must be less
than extensive in the sequence length. This is what we

2

want to show here, together with its generalization to
arbitrary (ﬁrst order) Markov sequences.

For this, we need two lemmata:
Lemma 1: For any Markov sequence S0 (not necessar-
ily binary, and not necessarily iid) built from m letters,
the sequence S1 is again Markov.

Lemma 2: If a word w = (k, k′, k′′, . . .) appears sev-
eral times in S0, and if one of these instances is substi-
tuted in Si by a string of characters not straddling its
boundaries, then all other instances of w in S0 are also
substituted in Si by the same string.

Lemma 1 tells us that NSRPS might make the struc-
ture of Si more complex than that of S0, but not much
so. Being a Markov chain, its entropy can be estimated if
the transition probabilities p(k|k1) are known. Thus es-
timating the entropy of Si reduces to estimating di-block
entropies h(2), which is straightforward (at least in the
limit N0 → ∞).

Lemma 2 tells us that there cannot be any ambiguity in
Si. In particular, it cannot happen that more information
is needed to specify Si than there is needed to specify
S0, since the mapping S0 → Si is bijective, once the
substitution rules are ﬁxed.

The proofs of the lemmata are easy. Let us denote
by pj(. . .) the probability distributions after j pair sub-
stitutions. For lemma 1 we just have to show that
p1(k|k′, k′′) is independent of k′′ for each pair (k, k′),
provided the same holds also for p0. This follows ba-
sically from the fact that any substitution makes the se-
quence shorter. But the detailed proof is somewhat te-
dious, because p1(k|k′, k′′) 6= p0(k|k′, k′′), even if all k’s
are less than m, k 6= k0, k′′
6= j0, and neither (k, k′)
nor (k′, k′′) are equal to the pair (j0, k0). In that case,
(N0 − nmax)p1(k|k′, k′′) = N0p0(k|k′, k′′), and indepen-
dence of k′′ follows immediately. All other cases have
to be dealt with similarly. For instance, if either (k, k′)
or (k′, k′′) is the pair (j0, k0), then p1(k, k′, k′′) = 0.
Else, if k′′ = m 6= k, k′, then p1(k|k′, k′′) = N0/(N0 −
nmax)p0(k|k′, j0, k0) = N0/(N0−nmax)p0(k|k′). We leave
the other cases as exercises to the reader.

For proving lemma 2 we proceed indirectly. We as-
sume that there is a word in S0 which is encoded dif-
ferently in diﬀerent locations. Let us assume that this
diﬀerence happened for the ﬁrst time after i substitu-
tions. Since only one type of pair is exchanged in each
step, this means that a substitution is skipped in one of
the locations, at this step. But this is impossible, since
all possible substitutions are made at each step.

From the two lemmata we obtain immediately our cen-

tral

Theorem:

If S0 is drawn from a (ﬁrst order)
Markov process with length N0 and entropy h0 =
− Pk,k′ p0(k, k′) log2 p0(k|k′),
is also
then every Si
Markovian in the limit N0 → ∞, with entropy

hi = h(2)

i = − X
k,k′

′

pi(k, k

) log2 pi(k|k

′

)

(7)

and with length Ni satisfying Ni/N0 = h0/hi.

Thus the total amount of information needed to specify
Si is the same as that for S0, for inﬁnitely long sequences.
Since the overhead needed to specify the pairs (ji, ki) can
be neglected in this limit, we see that we do not loose
code length eﬃciency by pair substitution, provided we
take pair probabilities correctly into account during the
coding. The actual encoding can be done by means of an
arithmetic code based on the probabilities pi(k|k′) [5],
but we shall not work out the details. It is enough to
know that the code length then becomes equal to the
information (both measured in bits), for N0 → ∞.

Let us see in detail how all this works for completely
random iid binary sequences. The original sequence
S0 = 00101001111010011011 . . . has p0(00) = p0(01) =
p0(10) = p0(11) = 1/4 and therefore h0 = 1 bit. Thus
we can, without loss of generality, assume that the new
character is 2 = (01), so that S1 = 02202111202121 . . ..
The 3 characters are now equiprobable, p1(0) = p1(1) =
p1(2) = 1/3, but they are not independent since of
course p1(01) = 0. Indeed, one ﬁnds p1(00) = p1(02) =
p1(11) = p1(21) = 1/6, p1(10) = p1(12) = p1(20) =
p1(22) = 1/12. The order-2 entropy of S1 is easily cal-
culated as h(2)
1 = 4/3 log2 2. On the other hand, since
N0/4 pairs have been replaced by single characters, the
length of S1 is N1 = 3N0/4. Thus, if S1 is Markov, then
the total information needed to specify it is N1h(2)
1 = N0
bits, the same as for S0. If it were not Markov, its in-
formation would be smaller. But this cannot be, because
the map S0 → S1 was invertible. Thus S1 must indeed
be Markov, as can also be checked explicitly.

In the next step, we can either replace (21) → 3 or
(02) → 3, since both have the same probability. If we do
the former, the sequence becomes S2 = 02203112033 . . ..
Now the letters are no longer equiprobable, p2(1) =
p2(2) = p2(3) = 1/5, p2(0) = 2/5. Calculating
N2, p2(kk′), and h(2)
is straightforward, and one ﬁnds
2
again N2h(2)
2 = N0 bits. Thus one concludes that S2
must also be Markov. For the next few steps one can
still verify

Nih(2)

i = . . . N0 bits,

(8)

by hand, but this becomes increasingly tedious as i in-
creases.

Thus we have veriﬁed Eq.(8) by extensive simulations,
where we found that it is exact, within the expected ﬂuc-
tuations, up to several thousand substitutions (Fig.1).
The distribution of the probabilities pi(k) becomes very
wide for large i, i.e. the sequences Si are far from uniform
for large i, but they are Markov and their entropies h(2)
are exactly (within the expected systematic ﬁnite sample
corrections [11, 12]) equal to N0/Ni bits. Notice that if
we would encode the last Si without taking the correla-
tions into account (as seems suggested in [6, 7]), then the
code length for it would be larger and the coding scheme
would not be optimal.

i

We have also made some simulations where we started
with non-trivial Markov processes for S0, or even with

3

1.4

1.3

1.2

1.1

1

0
N

 
 
/
 
 
h
t
g
n
e
l
 
 
e
d
o
c

y
t
i
l
i

b
a
b
o
r
p

0.01

0.001

0.0001

1

10

100

1000

10000

size  of  extended  alphabet

FIG. 1: Results for a completely random (iid, uniformly dis-
tributed) binary initial sequence of N0 = 8 × 108 bits, plotted
against the size of the extended alphabet. Uppermost curve:
code length needed to encode Si, divided by N0, if log2(i + 2)
bits are used for each character. Middle curve: code length
based on h(1)
, i.e. the single-character distributions pi(k) are
used in the encoding. Lowest curve, indistinguishable on this
scale from a horizontal straight line: code length based on
h(2)
i

, using the two-character distributions pi(k, k′

).

i

iid, p0(0) = 0.50
iid, p0(0) = 0.29
CA rule 150
English

1

10

1000

10000

100

rank

FIG. 2: Ranked single character probability distributions
pi(k) of strings after i = 2298 pair substitutions. The dif-
ferent curves are for a completely random iid initial string
S0 (solid line), iid string S0 with p0(0) = 0.29 (long dashed),
S0 obtained by applying two times CA rule 150 to an iid se-
quence with p(0) = 0.09 (dashed), and to written English
with a reduced (46 character) alphabet (dotted).

non-Markov sequences with known entropy. The latter
were generated by creating initially a binary iid sequence
with p(0) 6= p(1), and then using this as an input con-
ﬁguration for a few iterations of the bijective cellular au-
tomaton R150 (in Wolfram’s notation) [17].

From these simulations it seems that Nih(2)

always
tends towards N0. Also, the probability distributions
pi(k) seem to tend (very slowly, see Fig.2) to the same
scaling limit as for iid and uniform S0. This suggests
that indeed Si tends to a Markov process for arbitrary

i

4

S0.
In this case an optimal coding would be obtained
if one would use, e.g., an arithmetic code to encode Si
by using approximate values of the observed pi(k|k′) for
large i.

Thus we have given strong (but still incomplete) ar-
guments that NSRPS combined with eﬃcient coding of
Si gives indeed an optimal coding scheme. In practice, it
would of course be extremely ineﬃcient in terms of speed,
and thus of no practical relevance. But it could well be
that it might lead to more stringent entropy estimates
than other methods. To test this we shall now turn to
one of the most complex and interesting system, written
natural language.

i

r
e
t
c
a
r
a
h
c
 
 
l
a
n
g
i
r
o
 
 
r
e
p
 
 
h
t
g
n
e
l
 
 
e
d
o
c

3.8
3.6
3.4
3.2
3
2.8
2.6
2.4
2.2
2
1.8

III. THE ENTROPY OF WRITTEN ENGLISH

The data used for the application of NSRPS to en-
tropy estimation of written English consisted of ca. 150
MB of text taken from the Project Gutenberg homepage
[10].
It includes mainly English and American novels
from the 19th and early 20th century (Austen, Dick-
ens, Galsworthy, Melville, Stevenson, etc.), but also some
technical reports (e.g. Darwin, historical and sociologi-
cal texts, etc.), Shakespeares collected works, the King
James Bible, and some novels translated from French and
Russian (Verne, Tolstoy, Dostoevsky, etc.).

From these texts we removed ﬁrst editorial and legal
remarks added by the editors of Project Gutenberg. We
also removed end-of-line, end-of-page, and carriage re-
turn characters. All runs of consecutive blanks were re-
placed by a single blank. Finally, we also removed all
characters not in the 7-bit ASCII alphabet (ca. 4200 in
total). These cleaned texts were then concatenated to
form one big input string of 148,214,028 characters.

Entropies were estimated both from this string (which
still contained upper and lower case letters, numbers, all
kinds of brackets and interpunctation marks, 95 diﬀer-
ent characters in total), and from a version with reduced
alphabet. In the latter, we changed all letters to upper
case; all brackets to either ( or ); the symbols $,#,&,*,%,
@ to one single symbol; colons, exclamation and ques-
tion marks to points; quotation marks to apostrophes;
and semicolons to commas. This reduced alphabet had
then 46 letters (including, of course, the blank “⊔”).

The most frequent pair of letters in English is “e⊔”.
After replacing it by a new “letter”, the next pair to
substitute is “⊔t”, then “⊔a”, “⊔th”, etc. Very soon also
longer strings are substituted, e.g. after 92 steps appears
the ﬁrst two-word combination, “of⊔the⊔”.

As long as the number of new symbols is still small, it
is easy to estimate the pair probabilities, and from this
an upper bound ˆhi = h(2)
i Ni/N0 on the entropy. This
becomes more and more diﬃcult as the alphabet size in-
creases, as the sampling becomes insuﬃcient even with
our very long input ﬁle, and we can no longer approx-
imate the pi(k, k′) by the observed relative frequencies.
As long as the number of diﬀerent subsequent pairs is

100

1000

size  of  extended  alphabet

FIG. 3: Entropy estimates ˆh from pair probabilities plotted
against the size of the extended alphabet. Upper curve is
for the initial 7 bit alphabet, including upper and lower case
letters. The lower curve is for the reduced (46 letter) initial
alphabet. The smooth dotted line passing through the lower
data set is a ﬁt with Eq.(9).

much smaller than the sequence length (i.e., most pairs
are observed many times), we can still get reliable esti-
mates of ˆhi by using the leading correction term discussed
in [12, 13]. But ﬁnally, when many pairs are seen only
once in the entire text, we have to stop since any estimate
of h(2)
i

becomes unreliable.

We went up to 6000 substitutions.

The longest
substrings substituted by a single new symbol had
length 13 in the original (95 letter) alphabet, and
length 16 in the reduced (46 letter) one (the latter
was “would⊔have⊔been⊔”). The entropies ˆh per (orig-
inal) character are plotted in Fig.3. We see that they
are very similar for both alphabets. We ﬁnd ˆh ≈ 1.8
bits/character after 6000 substitutions. This number is
very close to the value obtained from most other methods
(with the exception of [14], where ≈ 1.5 bits/character
were obtained), if one uses 10 − 100 MB of input text
[15, 17]. This is surprising in view of two facts. First
of all, the methods applied in [15, 17] are very diﬀerent,
and one might have thought a priori that they are able
to use diﬀerent structures of the language to achieve high
compression rates. Apparently they do not.

Secondly, it is clear that ˆh ≈ 1.8 bits/character is not a
realistic estimate of the true entropy of written English.
Even though we can not, with our present text lengths
and our computational resources, go to much larger al-
phabet sizes (i.e. to more substitutions), it is clear from
Fig.3 that both curves would continue to decrease. Let us
denote by i the number of substitutions. Then empirical
ﬁts to both curves in Fig.3 are given by

ˆhi = h +

c
(i + i0)α .

(9)

Such a ﬁt to the 46 letter data, with h = 0.7, i0 = 34, c =
4.99, and α = 0.1745, is also shown in Fig.3. One should

of course not take it too serious in view of the very slow
convergence with i and the very long extrapolation, but
it suggests that the true entropy of written English is
0.7 ± 0.2 bits/character.

This estimate is somewhat lower than estimate of [16]
and the extrapolations given in [17].
It is comparable
with that of [18] and with Shannon’s original estimate
[19]. It seems deﬁnitely to exclude the possibility h = 0
which was proposed in [20, 22].

IV. CONCLUSIONS

We have shown how a strategy of non-sequential re-
placements of pairs of characters can yield eﬃcient data
compression and entropy estimates. A similar strategy
was ﬁrst proposed by Jim´enez-Monta˜no and others, but
details and the actual coding done in the present paper
are quite diﬀerent from those proposed in [6, 7]. Indeed,
this strategy was never used in [6, 7] for actual codings,
and it was also not used for realistic entropy estimates.
Compared to conventional sequential codes (such as
Lempel-Ziv or arithmetic codes [5], just to mention two),
the present method would be much slower.
Instead of
a single pass through the data as in sequential coding
schemes, we had gone up to 6000 times through the data
ﬁle, in order to achieve a high compression rate. We
could do of course with much less passes, if we would be
content with compression rates comparable to those of
commercial packages such as “zip” or “compress”. For
written English these achieve typically compression fac-
tors ≈ 2.6, i.e. ca. 3 bits/character. As seen from Fig.1,
this can be achieved by NSRPS very easily with very

5

few passes, but even then the overhead and the compu-
tational complexity of NSRPS is much too high to make
it a practical alternative.

NSRPS can be seen as a greedy and extremely simple
version of oﬀ-line textual substitution [21]. In combina-
tion with other sophisticated techniques, similar substi-
tutions can give excellent results [14]. But without these
techniques, it is in general believed that only much more
sophisticated versions of oﬀ-line textual substitution are
of any interest [21]. Again this is presumably true as far
as practical coding schemes are concerned. But things
seem to be diﬀerent if one is interested in entropy esti-
mation. Here the present method is much simpler (even
though computationally more demanding) than the tree-
based gambling algorithms [15, 17] that had given the
best results up to now. Without extrapolation, it gives
the same (upper bound) estimates as these methods. But
it seems that it allows a more reliable extrapolation to
inﬁnite text length and inﬁnite substitution depth, and
thus a more reliable estimate of the true asymptotic en-
tropy.

From the mathematical point of view, we should how-
ever stress that we have only partial results. While we
have proven that the Markov structure is a ﬁxed point of
the substitution, we have not proven that it is attractive.
We thus cannot prove that the present strategy is indeed
universally optimal, although we believe that our numer-
ical results strongly support this conjecture. A rigorous
proof would of course be extremely welcome.

I thank Ralf Andrzejak, Hsiao-Ping Hsu, and Walter
Nadler for carefully reading the manuscript and for useful
discussions.

[1] C.E. Shannon and W. Weaver, The Mathematical The-
ory of Communications (Univ. of Illinois Press, Urbana
1949).

[2] A.N. Kolmogorov, IEEE Trans. Inf. Theory IT 14, 662

(1965).

[3] G.J. Chaitin, Algorithmic Information Theory (Cam-

bridge Univ. Press, New York 1987).

[4] M. Li and P. Vit´anyi, An Introduction to Kolmogorov
Complexity and its Applications (Springer, New York
1997).

[5] T.M. Cover and J.A. Thomas, Elements of Information

Theory (Wiley Interscience, 1991).

[6] W. Ebeling and M.A. Jim´enez-Monta˜no, Math. Biosc.
52, 53 (1980); M.A. Jim´enez-Monta˜no, Bull. Math. Biol.
46, 641 (1984); P.E. Rapp, I.D. Zimmermann, E.P.
Vining, N. Cohen, A.M. Albano, and M.A. Jim´enez-
Monta˜no, Phys. Lett. A 192, 27 (1994);

[7] M.A. Jim´enez-Monta˜no, W. Ebeling, and T. P¨oschel,

preprint arXiv:cond-mat/0204134 (2002).

[8] Actually, Jim´enez-Monta˜no et al. use somewhat diﬀer-
ent schemes. Also, we found the names given in [6, 7]
to their algorithms somewhat misleading, since they re-
fer to grammatical categories, while we are dealing with

probability measures.

[9] In [7], e.g., it is assumed that a character from a two-
letter alphabet can still be encoded by one bit, after the
ﬁrst pair has been replaced by a “non-terminal node”,
in their notation. This is not true, since encoding this
character now must ﬁx a choice between three (instead of
two) possibilities.
[10] http://promo.net/pg/.
[11] B. Harris, Colloquia Mathematica Societatis Janos
Bolya, 1975, p. 323; H. Herzel, Syst. Anal. Model Sim.
5, 435 (1988).

[12] P. Grassberger, Phys. Lett. A 128, 369 (1988).
[13] We use Eq.(13) of [12], but with a misprint corrected:
The denominator of the last term should be (ni + 1)ni
instead of ni + 1.

[14] W.J. Teahan and J.G. Cleary, The entropy of English us-
ing PPM-based models, Proc. of Data Compression Conf.,
Los Alamos (1996)

[15] T.C. Bell, J.G. Cleary, and I.H. Witten, Text Compres-

sion (Prentice-Hall, Englewood Cliﬀs, NJ, 1990).

[16] T. Cover and R. King, IEEE Trans. Inf. Theory IT-24,

413 (1978)

[17] T. Sch¨urmann and P. Grassberger, CHAOS 6, 414

[18] P. Grassberger, IEEE Trans. Inf. Theory IT-35, 669

Rockville, MD, 1988).

[21] J.A. Storer, Data Compression (Computer Science Press,

[22] W. Ebeling and T. P¨oschel, Europhys. Lett. 26, 241

(1996).

(1989).

[19] C.E. Shannon, Bell Syst. Technol. J. 30, 50 (1951).
[20] W. Hilberg, Frequenz 44, 243 (1990).

(1994).

6


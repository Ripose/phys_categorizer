3
0
0
2
 
g
u
A
 
5
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
6
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Sensitivity of searches for new signals and its
optimization

Giovanni Punzi
Scuola Normale Superiore and INFN, Pisa

November 11, 2012

Abstract

A frequentist deﬁnition of sensitivity of a search for new phenom-
It is based on
ena is discussed, that has several useful properties.
completely standard concepts, is generally applicable, and has a very
clear interpretation. It is particularly suitable for optimization, being
independent of a-priori expectations about the presence of a signal,
thus allowing the determination of a single set of cuts that is optimal
both for setting limits and for making a discovery. Simple approxi-
mate formulas are given for the common problem of Poisson counts
with background.

1 Introduction

The question of the sensitivity of a search for new phenomena is a very com-
mon one. The need may arise either by the wish to predict the outcome
of an experiment and compare several possible experiments or diﬀerent con-
ﬁgurations of the same experiment. Several diﬀerent ways have been used
to quantify the sensitivity of a search, which makes it sometimes diﬃcult
to compare them.
In particular, two diﬀerent sensitivity ﬁgures are often
quoted, one that is relative to the potential for actually making a discovery,
and another to characterize how strong a constraint is imposed on the un-
known phenomena if no evidence is found for a deviation from the standard

1

theory. This situation makes it diﬃcult to optimize the design of an exper-
iment to be performed, because it is not clear what should be maximized.
I describe here a deﬁnition of sensitivity which is unique and well-deﬁned
for any experiment. This is based on purely frequentist ideas, which avoids
the issue of the choice of an a-priori distribution for a new and unknown
phenomena.

2 Statement of the problem

The problem of searches for new phenomena can be stated formally in classi-
cal statistics as one of “Hypothesis testing”. We have a “default hypothesis”
H0, that is our current best theory, and as a result of the experiment we wish
to either conﬁrm or disprove the theory H0, in favor of an alternative theory
Hm, where m indicates the free parameters of the new theory (mass or set of
masses of new particles, coupling constants, production cross sections, etc.).
The experiment consists of measuring the value of a set of observables X
(possibly a large number) whose distribution depends on the true state of
nature being H0 or Hm. In a simple counting experiment, the observable X
is the number of observed counts, and hypothesis H0 is deﬁned as the distri-
bution of X being a Poisson with the mean equal to the number of expected
background events B. Hypothesis Hm is that the distribution is instead a
Poisson with a larger mean B + Sm, where Sm is the expected contribution
of the “new signal”, which is a function of the unknown free parameters of
the new theory, m. A test of H0 is speciﬁed by deﬁning the set of values of
X that will make us decide that H0 must be rejected (this is named “critical
region”); the signiﬁcance level of the test, indicated by α, is the probability
of rejecting H0 when it is indeed true, that is to say, α is the probability for
X to fall within the critical region, calculated under the assumption that H0
is true. There are many possible choices of the critical region, therefore many
possible diﬀerent tests at the given signiﬁcance level α, and we will not be
concerned here with the way the choice is made; all of the present discussion
is independent of the way the test was chosen.

What about the value of α ? This is a “small number”, common practice
for really new physics discovery being to require α to correspond to the 5σ
single tail of a gaussian distribution.

The other element to be considered in a test is the probability that a

2

−

discovery is made. The classical way to express this is by the power function
β(m), that is, the probability that X will fall in the critical region (=the
1
probability that a discovery will be claimed), assuming Hm is true, as a
function of the parameters m.
It is clearly desirable to have the greatest
possible power. However, it is well known that only in very few special
problems it is possible to maximize the power simultaneously for every m.
For this reason, trying to optimize the power is subject to a judgement about
what values of the parameters are more important to be sensitive to; in the
next section we will attack this problem from a diﬀerent angle.

After a measurement is performed, if no discovery is made the experi-
menter will usually produce an additional piece of information: a conﬁdence
region for the unknown parameters m. This part is in principle completely
independent from the “testing” part, and interesting issues arise when one
tries to make sure the two kinds of information are coherent. For instance,
limits are often desired at a conﬁdence level lower than the level of signif-
icance required for claiming a discovery; this can lead easily to situations
where no discovery is claimed, and yet limits are quoted that do not include
the H0 hypothesis. For the purpose of the present discussion we don’t need
to deal with such diﬃcult issues and we will avoid them by making only min-
imal assumptions about the relationship between the test and the algorithm
adopted for setting limits. We will only assume that the conﬁdence band
for m be built in such a way to exclude, whenever possible, all values of X
falling within the acceptance region for H0; (this can be done for every m
β(m) > CL, where CL is the desired Conﬁdence Level). This
such that 1
is quite natural, and usually happens spontaneously, because it makes for
tighter conﬁdence regions when no discovery is made, at no expense.

−

If a discovery is indeed made, the most interesting piece of information
in the result will be the discovery itself, and maybe an estimate of the pa-
rameters m, so we will not be concerned with limits in case of discovery, only
with the probability that it happens.

3

3 Deﬁnition of Sensitivity of a search exper-

iment

Many deﬁnitions of sensitivity for a search have to do with either the ”av-
erage limit” produced if H0 is true (deﬁned in various ways), or with the
signiﬁcance of an observed signal, assuming the observation is exactly equal
to the expected value in presence of a signal at m.

We suggest to characterize the sensitivity of an experiment in the follow-
ing way. Correct statistical practice requires to decide before the experiment
the values of α and CL, so we assume their values are given. Then one can
proceed by quoting the region of the parameters m for which the power of the
chosen test is greater or equal to the Conﬁdence Level chosen for the limits
in case there is no discovery:

1

βα(m) > CL

−

(1)

This region of m can be thought of as a region of parameters to which
the experiment is “suﬃciently sensitive”. While it is always possible to pro-
vide additional information by plotting contours of constant power in the m
space for values diﬀerent from the CL, the speciﬁc region deﬁned by eq. (1)
is particularly informative because it has a very simple and clear-cut inter-
pretation. In fact, it is easy to verify that the following two statements hold
simultaneously:

•

•

If the true value of m satisﬁes (1), then there is a probability at least
CL that performing the experiment will lead to discovery (with the
chosen signiﬁcance α).

If performing the experiment does not lead to discovery, the resulting
limits will exclude (at least) the entire region deﬁned by (1), at the
chosen CL. (N.B. this relies on the minimal assumption of a “reason-
able algorithm” for setting limits made in previous section, and holds
independently of the true value of m.)

In short, eq. (1) deﬁnes the region in the parameter space for which the
experiment will certainly give an answer: that region will be excluded, or a
discovery will be claimed, with no possible in-between. This double discov-
ery/exclusion interpretation suggests that it deserves to be named sensitivity

4

region for the experiment and to be quoted as the single most useful informa-
tion to characterize its potential and optimize it. Note explicitly that there
is no possibility for an experimental ﬂuctuation to jeopardize the result; it
is possible for a ﬂuctuation to increase the region of exclusion, but not to
diminish it. In particular, if the parameter region covers the whole range of
physically interesting values for m, the experiment can very well been said to
be conclusive. This sensitivity region appears to be a more useful information
than others commonly quoted, that have a more vague meaning, like:

the “average” excluded region, if H0 is true (tells you nothing certain
about the actual limits that will be quoted; tells you nothing about
what will happen if the signal exist but it is small)

an ”average number of sigmas”, for given values of m, or the number
of sigmas you would get in case exactly the expected number of signal
events is observed (tells you nothing about the limits in case there is
no observation; tells you little about how likely actually is that a signal
wil be observed, due to the eﬀect of statistical ﬂuctuations)

Comparison between two experiments or experimental settings should be
made on the basis of whether one sensitivity region includes the other; it is
still possible for two experiments to be non-comparable, by having none of
the two region completely include the other. In this case, the issue of which
is preferable cannot be resolved on a statistical basis, but it is a question of
strategy. If the sensitivity regions are very diﬀerent, the actual conclusion
is that the two experiments are somehow ‘complementary’, probing diﬀerent
regions of the parameters space.

There are a few other arguments in favor of quoting this quantity to

characterize the sensitivity of an experiment:

The deﬁnition is independent of the choice of metric (in both observable
and parameter space).

It does not require a choice of priors

It is straightforward (and meaningful) to apply it to complex situations.
For instance:

•

•

•

•

•

5

– 1-D problems with a “non monotonic” structure. Example: search
for a CP violation eﬀect, where one measures the sine of an angle,
1, 1]. In this case H0 is in the middle, and it
with the range [
makes no sense to quote “average upper limit”.

−

– multidimensional parameter problems. Examples of this kind are
neutrino oscillation searches, where the space is 2-D. Even more
complex examples are found in CP-violation measurements in neu-
tral B mesons oscillations, where both a direct and a mixed com-
ponent are possible; in this case the allowed region for the pa-
rameters is circle of unit radius, H0 being at the center, and it
is impossible to use concepts like “average upper limit”, or even
“median of the limit”.

•

•

It is independent of the expectations for a signal to be present, thus
allowing an unbiased optimization.

It allows you to optimize what you really want for a search, without
being distracted by other elements. For instance, if one had to concen-
trate on getting the maximum possible power (e.g. by looking at its
average it over a chosen region), one can easily be fooled into preferring
an experiment that has a very high power in a region where the power
is pretty high anywyay, over one that has a more even distribution of
power, that is actually much more likely to provide useful information,
since in a discovery measurement the power counts the most where it
is “intermediate”. Considering the region rather than power in itself
takes this correctly into account.

4 Optimization of a counting experiment

We will now apply the ideas discussed in the previous section to the very
common problem of a counting experiment in presence of background. In this
case, we have the discrete observable n, the number of events observed, which
is Poisson-distributed with a mean determined by B, the expected number of
background events (supposed known), and the possible contribution of signal
events Sm:

H0) =

p(n
|

e−

BBn/n!

6

(2)

Hm) = e−

B

Sm(B + Sm)n/n!

−

p(n
|

(3)

For this problem, the only sensible deﬁnition of a critical region for the
presence of non-zero signal Sm takes the form of a condition like

n > nmin

Therefore, the test is completely deﬁned once the desired signiﬁcance
level α is chosen. Figure 1 shows the value of nmin as a function of B,
for given values of α, obtained by numerical calculation of sums of Poisson
probabilities.

Having completely deﬁned the test, we can now evaluate its power as a
function of m, and determine the set of values for m such that eq. (1) holds.
Since the power of a test of the form n > nmin grows monotonically with Sm,
it is easy to see that eq. (1) leads to simple inequalities of the form:

Sm > Smin

Therefore, all is needed to completely characterize the solution of our
problem is the value of Smin, that is in general a function of α, β, and B.
Plots of Smin obtained by numerical calculations are shown in Figure 2.

Tabulated data like these can in principle be used to compare diﬀerent
experimental settings, by determining for each of them the set of values of
m such that Sm > Smin, and choosing the one with the largest set. However,
it is much easier to perform such optimizations tasks on the basis of an
analytical parametrization of the plots. For the purpose of optimization, an
approximation of the exact result is usually suﬃcient; in particular, there is
no need to account for the discretization eﬀects.

A simple parametrization of our result can be obtained by means of Gaus-
sian approximation of the Poisson. It is easy to see that in this approxima-
tion, condition (1) translates into the following equation for Smin:

where a and b are the number of sigmas corresponding to one-sided Gaus-

Smin

a√B = b

B + Smin

−

q

(4)

sian tests at signiﬁcance α and β respectively.
Solving eq. (4) for Smin yields the solution:

Smin =

+ a √B +

b2 + 4 a √B + 4 B

(5)

b2
2

b
2 q

7

The above expression holds for one speciﬁc set of data selection criteria.
Now consider the common situation where one has to decide on the set of
cuts to be used in the analysis. This means that both the background B
and the number of expected signal events Sm will depend on the cuts (let’s
indicate the whole set of cuts with the symbol t). In a completely general
case, in order to decide which set of cuts t is best, you need to determine for
every t the set of values ˜m to which the experiment is sensitive, by solving
for ˜m the inequality:

S ˜m(t)

+ a

B(t) +

B(t) + 4 B(t)

b2
2

≥

q

b
2 rb2 + 4 a

q

and then choose the cuts t yielding the most extended region. The situ-
ation is much simpler when the eﬃciency ǫ of the chosen cuts on the signal
is indipendent of m, that is when one can write:

Sm(t) = ǫ(t)

L

σm

·

·

where L is the integrated luminosity and σm is the cross section of the

new process.

In this case one can very simply invert the above equation to write down

the minimum “detectable” (according to our criteria) cross section:

b2
2 + a

B(t) + b

q

σmin =

B(t) + 4 B(t)

2 rb2 + 4 a
ǫ(t)

L

q

·

Obviously, the maximum sensitivity is attained when σmin is smallest, i.e.

when the quantity:

b2 + 2 a

B(t) + b rb2 + 4 a

q

q

B(t) + 4 B(t)

reaches its maximum value. Note explicitly that, in the given assumption
of the eﬃciency being independent of m, the optimal choice of cuts does not
depend on the assumed cross section for the new process σ(m). This is a very
useful feature, since this parameter is often unknown in practice, and it is a
direct consequence of the chosen approach, that focuses on maximizing the
power where it is really necessary, namely at the threshold of visibility.

(6)

ǫ(t)

8

Expression (6) becomes even simpler when the choice b = a is made:

ǫ(t)

a/2 +

B(t)

(7)

q
This simple expression is adequate in most problems of search optimiza-
tion; also, it is readily compared with some “signiﬁcance-like” expressions
that are commonly used for optimization purposes:

a)

b)

S
√B

S
√B+S

Note that expression b) cannot be maximized without knowing explicitly
the cross section for the searched signal. Also, it does not quite represent
what one wants to maximize for a search, being more directly related to
the relative uncertainty in the measurement of the yield of a new process,
if found, than to signiﬁcance. Expression a), being linear in S, shares with
expression (7) the good property of being independent of the cross section
of the new process, but it has the important problem of breaking down at
small values of B. Imposing maximization of a) may push the experiment
eﬃciency down to very small values. In order to see the failure of expression
a), it is suﬃcient to consider, for instance, that it would prefer an expectation
5 over a situation with 10 signal
of 0.1 signal events with a background of 10−
events expected over a background of 1 event.

It should be clear that expression (7), or its slightly more sophisticated
form (6), besides being based on more solid motivations, is unambigously
better than both a) and b) from the point of view of practical application.

These features are more easily seen by plotting the factor 1/Smin from
the exact calculation, that is proportional to the quantity that needs to be
maximized (as in eq. (6)), together with the two signiﬁcance–like expressions
discussed above: they all behave as 1/√B at large B, and it is therefore
possible to normalize them to converge as B
. Expression b) is not
simply proportional to S, so it has been necessary to set
√B+S = a (which
is in agreement with the spirit of our current approach, of focusing on the
point where signiﬁcance is at the threshold) and solve for 1/S in order to get
an expression proportional to a function of B only.

→ ∞

S

9

The comparison is shown in Fig. 5, where it appears that our suggested
solution lies between a) and b), where a) largely overestimates the “sensitiv-
ity” at low backgrounds, as expected, and conversely b) underestimates it,
expecially for high signiﬁcance settings.

≈

The Gaussian approximation to the exact solution is shown instead in
ﬁg. 3, and its special case for b
a in ﬁg. 4. It can be seen that the ap-
proximate formulas work well at moderate values of a and b, but become
less accurate when high signiﬁcance/CL are desired, due to the larger devi-
ations from Gaussian behavior that occur in the Poisson far tails. However,
the Gaussian approximation can easily be improved, without losing the good
features of the solutions. For instance, it is possible to obtain a more accurate
expression by accounting for diﬀerences between Gaussian and Poisson tail
integrals at the next order in a and b, simply by performing a semi-empirical
ﬁt. This results in the following improved expression for Smin:

Smin =

+

+ a √B +

b2 + 4 a √B + 4 B

(8)

a2
8

9 b2
13

b
2 q

Fig. 6 shows this slightly modiﬁed expression to be considerably accurate
even at high signiﬁcances, which makes it suitable also for searches of “really
new” eﬀects, where a signiﬁcance level of 5σ is a customary requirements.

Acknowledgments

The author wishes to thank Louis Lyons for many helpful comments.

10

30

20

10

30

25

20

15

10

5

2

4

6

8

10

12

14

Figure 1: Minimum number of observed events needed to claim discovery
with 95%, 3σ, 5σ, vs expected background.

2

4

6

8

10

12

14

Figure 2: The lower limit of the sensitivity region Smin, for a search experi-
ment with (signiﬁcance, CL) respectively of (95%,95%), (3σ,95%), (5σ,90%)

11

0.25

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

2

4

6

8

10

12

14

Figure 3: Gaussian approximation of the “Sensitivity factor” 1/Smin (eq. (6))
for a search experiment with (signiﬁcance, CL) respectively of (95%,95%),
(3σ,95%), (5σ,90%)

2

4

6

8

10

12

14

a approximation
Figure 4: Gaussian approximation of 1/Smin in the b
(eq. (6)), for a search experiment with (signiﬁcance, CL) respectively of
(95%,95%), (3σ,95%), (5σ,90%). Curves are normalized to the asymptotic
limit.

≈

12

0.25

0.2

0.15

0.1

0.05

0.25

0.2

0.15

0.1

0.05

2

4

6

8

10

12

14

Figure 5: Comparison of 1/Smin with the corresponding sensitivity factor
given by S/√B (dotted) and S/√S + B (dashed), for a search experiment
with (signiﬁcance, CL) respectively of (95%,95%), (3σ,95%), (5σ,90%)

2

4

6

8

10

12

14

Improved Gaussian approximation of the “Sensitivity factor”
Figure 6:
1/Smin (eq. (8) for a search experiment with (signiﬁcance, CL) respectively
of (95%,95%), (3σ,95%), (5σ,90%)

13


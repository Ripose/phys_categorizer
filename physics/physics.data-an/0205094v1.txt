2
0
0
2
 
y
a
M
 
1
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
9
0
5
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Generalized Singular Spectrum Time Series Analysis

Martin Nilsson
EES-6, MS T003
Los Alamos National Laboratory
nilsson@lanl.gov

January 17, 2014

Abstract

This paper is a study of continuous time
Singular Spectrum Analysis (SSA). We
show that the principal eigenfunctions
are solutions to a set of linear ODEs
with constant coeﬃcients. We also in-
troduce a natural generalization of SSA,
constructed using local (Lie-) transfor-
mation groups. The time translations
used in standard SSA is a special case.
The eigenfunctions then satisfy a sim-
ple type of linear ODE with time de-
pendent coeﬃcient, determined by the
inﬁnitesimal generator of the transfor-
mation group. Finally, more general one
parameter mappings are considered.

Singular Spectrum Analysis (SSA) is a rela-
tively recent method for nonlinear time series
analysis. The original idea behind SSA was
ﬁrst presented by Broomhead and King [1], in
the context of time series embedding. Dur-
ing the last decade this technique has been
very successful and has become a standard tool
in many diﬀerent scientiﬁc ﬁelds, such as cli-
matic [2], meteorological [3], and astronomi-
cal [4] time series analysis. For introductions
to the SSA technique, see e.g., [5, 6].

fore discrete. This paper is a theoretical study
of SSA, and it is therefore more natural to con-
sider the general case of continuous time. We
start the paper by a (semi) formal expansion
of the SSA procedure to continuous time.

Let f (t) be a function representing a con-
tinuous time signal on an interval ΩT = [0, T ].
We assume that f (t)
L2(ΩT )
is a Hilbert space with an inner product de-
dtf (t)g(t) and a norm
ﬁned as (f, g)ΩT =
(f, f )Ω. We deﬁne a trajectory func-
kΩ =
f
k
tion X(ξ, t)
p

∈ L2(ΩT ), where

ΩT
R

X(ξ, t) = f (ξ + t)

(1)

∈

−

ΩW = [0, W ] and ξ

where t
∈
ΩT −W = [0, T
W ]. The parameter W is
ﬁxed and referred to as the window length.
By construction X(ξ, t)
L2(ΩW ) and the norm is deﬁned as
ΩT −W
R
A Schmidt decomposition (the continuous
equivalent to a Singular Value Decomposition
of a matrix) of the trajectory function is deﬁed
as

∈ L2(ΩT −W )
k

dtX(ξ, t)X(ξ, t).

×
2 =

dξ

ΩW

X

k

R

X(ξ, t) =

λkvk(ξ)uk(t)

(2)

d

Xk=1 p

In practical applications, a time series is a
result of a sampled measurement, and is there-

where d is referred to as the rank (which is
often inﬁnite). All partial sums, k = 1 to ˜d, are

1

optimal ˜d-rank
i.e., the functions vk(ξ) and uk(t) fulﬁlls

L2-approximations of X(ξ, t),

˜d

1

1

min
u,v k

X(ξ, t)

−

Xk=1 p

λkvk(ξ)uk(t)
k

(3)

Without lost of generality, the functions vk(ξ)
and uk(t) can be assumed to be normalized,
= 1. Eq. 3 implies
uk

vk

=

k

k

k

k

vk(ξ) =

dtX(ξ, t)uk(t)

√λk ZΩW

uk(t) =

dξX(ξ, t)vk(ξ)

(4)

√λk ZΩT −W

together with the orthogonality conditions
(vk, vl)ΩT −W = δkl and (uk, ul)ΩT = δkl. The
two relations in Eq. 4 can be combined into
eigenvalue problems for the functions uk(t) re-
spective vk(ξ)

dt

S(t, t

)uk(t

) = λkuk(t)

′

′

′

′

′

′

dξ

R(ξ, ξ

)vk(ξ

) = λkvk(ξ) (5)

ZΩW

ZΩT −W

where the t-covariance function S(t, t′) and
the ξ-covariance function R(ξ, ξ′) are deﬁned
as

R(ξ, ξ

) =

dtX(ξ, t)X(ξ′, t)

S(t, t

) =

dξX(ξ, t)X(ξ, t′)

′

′

ZΩW

ZΩT −W

From Eq. 5 it
that vk(ξ) and
is clear
uk(t) are eigenfunctions to two compact, lin-
ear and symmetric integral operators R and
S with kernels R(ξ, ξ′) and S(t, t′). The
spectral theorem then guarantees vk(ξ) and
to form complete orthogonal bases
uk(t)
L2(ΩT −W )/
in
(S)
) denotes a null-space). The eigen-
(
(where
·
N
values λk are real and non-negative, and mea-
sure the variance of X(ξ, t) in the “direction”

L2(ΩT )/

(R) respective

N

N

deﬁned by vk(ξ) and uk(t). Note the close cor-
respondence to principal component analysis.
By construction, SSA decomposes the orig-
inal time series into orthogonal components.
Usually, the components also represent intu-
itive contributions to the time series, such as
a trend, various oscillation modes and noise.

We start our analysis of the SSA technique
by investigating signal separation into orthog-
onal subspaces. From now on, due to space
limitations, we will
focus our attention to
the “right eigenmodes”, uk(t). By symmetry,
equivalent results are valid for the “left eigen-
modes”, vk(ξ). Assume that the time series
i fi(t), where
can be decomposed as f (t) =

dξfi(t + ξ)fj (t

ZΩT −W

′

P
+ ξ) = C1 ·

δij

dtfi(t + ξ)fj (t + ξ

ZΩW

′

) = C2 ·

δij (6)

∈

∈

⊥

∈ N

= i, λ

ΩW , ξ, ξ′

uj when λi

t, t′
ΩT −W . Then the t-
∀
covariance kernel also decompose as S =
Since
i Si, where Si corresponds to fi.
dtSi (t, t′) Sj (t′, t′′) = 0 if i
= j, it fol-
P
ΩW
(Sj),
lows that Siu = λu implies u
R
j
= 0. An eigenfunction of Si is there-
fore also an eigenfunction of S, i.e., Siu = λu
implies Su = λu. Since S a symmetric opera-
tor, ui
= λj, which guarantees
that a Schmidt decomposition is unique up to
a rotation of eigenfunctions with identical sin-
i fi(t),
gular value. It follows that if f (t) =
Eq. 6 is fulﬁlled and fi(t) and fj(t) have dis-
joint spectra, then the SSA decomposition in
Eq. 2 is a direct sum of the decompositions of
the individual time series.
In this case fi(t)
and fj(t) are called strongly separable. If the
spectra are not disjoint, but Eq. 6 is fulﬁlled,
fi(t) and fj(t) are called weakly separable (us-
ing the same notation as in [6]).

P

An important implication of the above anal-
ysis is that, for periodic functions, when the
total time frame T and the time window W is

2

6
6
6
6
∈

−

W = nτ and W = n′τ
chosen such that T
(where τ is the period and n, n′
Z+), the sin-
gular spectrum √λk is identical to the Fourier
coeﬃcients of f (t). Furthermore, if the Fourier
coeﬃcients are distinct, then the eigenfunc-
tions uk(t) are identical to the basis functions
in the Fourier expansion. Similar results hold
asymptotically in the inﬁnite time frame limit.
To understand the SSA procedure, it is es-
sential to further analyze the characteristics of
the orthogonal eigenfunctions uk(t). We start
by the following trivial observation:

f ((ξ + ξ

) + t) = f (ξ + (ξ

+ t)) . (7)

′

′

Using this relation in Eq. 2 gives

d

Xk=1 p
d

Xk=1 p

λkvk(ξ)uk(ξ

+ t) =

′

λkvk(ξ + ξ

)uk(t).

′

R

ΩT −W

dξvl(ξ),
Apply the projection operator
use the orthogonality of vk(ξ), subtract ul(t),
divide by ξ′, and ﬁnally let ξ′
0. Tech-
nically, a problem appears when integrating
vk(ξ + ξ′) over ξ
ΩT −W . The function is not
deﬁned when ξ + ξ′ /
ΩT −W . However, since
∈
ξ′
0, using any smooth continuation of vk
gives equivalent results. Alternatively, Eq. 7
could be replaced by ∂tf (ξ + t) = ∂ξf (ξ + t).
Either way, we ﬁnd:

→

→

∈

∂tuk(t) =

Aklul(t),

(8)

d

Xl=1

where

Projecting Eq. 8 onto uk(t) also gives:

Akl =

dtul(t)∂tuk(t)

ZΩW

which clearly shows the symmetric relation be-
tween the left and right principal eigenfunc-
tions.

From Eq. 8 it is clear that polynomial, ex-
ponential and harmonic functions show espe-
cially simple (ﬁnite) spectra during SSA. In
the more general case, the SSA procedure de-
composes the time series into an optimal (in-
ﬁnite) linear combination of polynomial, ex-
ponential and harmonic functions. This has
previously been discovered by others, but the
approach taken in this paper is quite diﬀer-
ent and arguably more straight forward. For
a discussion on related work see [4, 6].

We now show a connection between the ma-
trix A and an underlying dynamical system
from which the time series is generated. Let
f (t) be a solution to a linear system of diﬀer-
ential equations

∂tf (t) = Mf (t),

and let a scalar time series f (t) be deﬁned by
some linear projection of f (t), f = pT f , where
p is non-degenerate in the sense that all os-
cillation modes that appears in f (t) are also
present in f (t). We shall now prove that M
and A have the same spectra. The vector func-
tion f (t) can be decomposed as:

f (t) =

λkwk

uk(t),

(9)

d

q

Xk=1

e

e

where t is restricted to the interval ΩW . Since
f (ξ + t) = exp(Mξ)f (t), we have

Akl =

dξvk(ξ)∂ξvl(ξ)

f (ξ + t) =

λl
λk ZΩT −W

r

d

q

Xk=1

λk

e

pT exp
(cid:0)

Mξ
{

}

wk

uk(t).

(cid:1)

e

does not depend on t. Note the connection
between A and the cross-correlation function.

Comparing this expression to Eq. 2 and use
the uniqueness of the SVD expansion, gives

3

·

}

P

wl

l Rkl

Mξ
{

pT exp
(cid:0)

, where
vk(ξ) = Ck
Ck is a normalization constant and R is some
(cid:1)
rotation matrix which rotates elements within
the equivalence classes deﬁned by identical sin-
u(t) =
gular values. This further shows that
Ru(t), and therefore the time derivative of
Eq. 9 gives:

e

together with the existence of an identity ele-
ment e and an inverse g−1 for all g

G.

∈

In this paper, the elements of the transforma-
tion group is spanned by a parameter ξ (we
have a one-parameter group). We will some-
times use the compact notation:

λkMwkRklul(t) =

λkwkRklAlmum(t).

q

Xkl

e

Xklm q

e

Using the orthogonality of uk(t) and wk, gives

Ψ (g (ξ) , x)

Ψξx.

≡

We also chose the parametrization such that
g(0) = e.

A one-parameter transformation group de-

ﬁnes a vector ﬁeld V =

i ζi(xi)∂xi :

A = RT

−1WT MW

Λ

ΛR,

(10)

V

|x =

Ψ(ξ, x),

P

ξ=0

d
dξ (cid:12)
(cid:12)
(cid:12)
(cid:12)

where we use the matrix notation:

Λ =

e

e

diag

λ1,

λ2, . . .

, W = (w1, w2, . . .).

e

(cid:18)q

q
ΛR

e
W

(cid:19)
Λ−1WT

RT

e
(cid:17) (cid:16)

e

(cid:17)

(cid:16)

Since
plies that A and M have identical spectra.
e

= 1, Eq. 10 im-

Note that this line of argument is similar to
the local linear analysis of dynamical systems,
used as part of the proof delay coordinate em-
bedding theorems [7]. It also shows a straight
forward connection between PCA of a set of
time series from a system of ODEs, and a SSA
analysis of a projection from the system.

Though natural in the discrete case, the def-
inition of the trajectory function as X(ξ, t) =
f (ξ + t) is somewhat arbitrary in our analy-
sis. Why not X(ξ, t) = f (ξt), for example?
We can use this arbitrariness both to gener-
alize SSA and to gain better theoretical un-
derstanding of the procedure. Recall the def-
inition of a (local) transformation group (see
e.g., [8]:

Deﬁnition 1 A transformation group is a
Rn
continuous Lie group G and a set M
along with a smooth map Ψ : G
M
M
which satisﬁes, for every g, h

⊂
→
M ,

×
G, x

∈
Ψ (g, Ψ (h, x)) = Ψ (g

∈
h, x) ,

(11)

◦

which formally is “solved” as

Ψ(ξ, x) = exp

ξV
{

}

x.

The vector ﬁeld is therefore called the in-
ﬁnitesimal generator of the one-parameter
transformation group. Note that the exponen-
g(ξ′) = g(ξ + ξ′).
tial map also implies g(ξ)
The transformation group also generates or-
bits through every point x
M , deﬁned as
φ(ξ) = Ψ(ξ, x). The orbits are solutions to a
system of ordinary diﬀerential equations:

∈

◦

∂ξφi(ξ) = ζi(φ(ξ))

φ(0) = x,

(12)

where the explicit representation of the vector
ﬁeld is used. Eq. 12 can be used to derive ex-
plicit expressions for the transformation group
corresponding to a vector ﬁeld.

Using this framework, the SSA procedure
may be generalized in the following way. Con-
sider a trajectory function XG, constructed
from a one-parameter transformation group
Ψξ acting on M = ΩW , with the continuous
Lie group G:

XG(ξ, t) = f (Ψξt) ,

(13)

4

where Ψ : ΩT −W
a relation equivalent to Eq. 7:

ΩW

→

×

ΩT . Eq 11 provides

f (Ψξ (Ψξ′t)) = f (Ψξ+ξ′ t) ,

(14)

Eq. 14 can be used to ﬁnd a system of diﬀer-
ential equations, equivalent to Eq. 8, in terms
of the inﬁnitesimal generator of the transfor-
mation group:

[uk(t)] =

Aklul(t),

(15)

L

d

Xl=1

L

where
] expresses a Lie derivative with re-
[
·
spect to the transformation group. Since uk(t)
is a scalar function (the index k is ﬁxed),
[uk] = ζ(t)∂tuk(t), where ζ(t)∂t is the vec-
L
tor ﬁeld generating the one-parameter trans-
formation group. The matrix A is now deﬁned
as

Akl =

dξvk(ξ)∂ξvl(ξ).

λl
λk ZΩT −W

r

Again, A is independent of t. If the time series
is deﬁned as a linear projection f = pT f and
[f (t)] = Mf , an equivalent analysis as above

L
shows that M and A have identical spectra.

Lie groups was originally developed to an-
alyze symmetries in diﬀerential equations, see
e.g., [8]. To make a direct connection between
this theory and the analysis above, assume
that f (t) is a solution to some linear diﬀer-
ential equation, i.e., Dtf (t) = 0 where Dt is
a linear operator. Assume further that Ψξ is
a symmetry group of the diﬀerential operator
Dt, then Dtf (t) = 0 implies Dt (Ψξf (t)) = 0.
If the transformation group Ψξ is used to con-
struct the trajectory function, then the prin-
cipal eigenfunctions uk(t) will also satisfy the
same diﬀerential equation, Dtuk(t) = 0. This
can be seen by noting that the t-covariance
kernel can be written explicitly as

′

S (t, t

) =

dξf (Ψξt) f (Ψξt

) . (16)

′

ZΩT −W

Since DtS (t, t′) = 0, it follows from the eigen-
value problem in Eq. 5 that Dtuk(t) = 0, if
λk

= 0.
The analysis also shows how the global
transformation group, used in the construction
of the trajectory function, aﬀects the eigen-
modes via the inﬁnitesimal generator. Let the
vector ﬁeld be deﬁned by V = ζ(t)∂t. The re-
lation between the eigenmodes in Eq. 15 and
Eq. 8 is then given by a variable transforma-
ζ(s) . The general solutions is on
tion, τ =
the form

t ds

R

u(t) = exp

A

u0.

(17)

(cid:26)

Z

ζ(s) (cid:27)

t ds

Eq. 17 reﬂects the fact that up to an iso-
morphism, there are only two connected one-
parameter Lie groups, R and SO(2), corre-
sponding to real respective imaginary eigen-
values of A, see [8] for details.

In the standard SSA, time translations are
used to construct the trajectory function, cor-
responding to Ψ(ξ, t) = ξ + t, V = ∂t and
u(t) = exp
u0. Using scale transforma-
tions, Ψ(ξ, t) = eξt, corresponds to V = t∂t
and u(t) = exp
u0, which contain
A ln(t)
}
functions on the form tα (scaling functions)
and sin (ω ln(t)).

At

{

{

}

In fact, any smooth vector ﬁeld generates
a transformation group, which may only be
locally deﬁned. For example, consider ζ(t) =
tα for some constant α
R, α
= 1. Using
Eq. 12 and Eq. 17, we then have:

∈

V = tα∂t

Ψ (ξ, t) =

1
β

βξ + tβ
(cid:0)

u(t) = exp

−1tβ
(cid:1)
Aβ

u0,

(cid:8)

(cid:9)

where β = 1

α.

−

For completeness, we ﬁnish this paper by a
further generalization of the SSA. Consider a
smooth function ψ : ΩW
ΩT , and
construct the trajectory function as X(ξ, t) =

ΩT −W

→

×

5

6
6
f (ψ(ξ, t)). We assume the mapping to be of
maximal rank, i.e., have a non-vanishing Jaco-
bian. The implicit function theorem then en-
sures that in the neighborhood of each point
on a curve deﬁned by ψ(ξ, t) = const., ξ can
be expressed as a function of t and vice versa,
i.e., the mapping ψ deﬁnes an implicit one-
dimensional submanifold in R2. We assume
ψ (0, t) = t. For small ξ′, we then use the
smoothness of ψ to write

′

ψ (ξ, ψ (ξ
ψ (ξ

, t)) = ψ (ξ + h(t)g(ξ, t)(ξ
′
, t) = t + h(t)(ξ

)κ,

′

′

)κ, t)
(18)

1 in ξ′ for some
which is valid to order κ
functions h and g. The derivative of the ﬁrst
relation in Eq. 18 gives an explicit expression
for g(ξ, t):

≥

g(ξ, t) =

∂tψ(ξ, t)
∂ξψ(ξ, t)

Note that if h(t)g (ξ, t) = 1 and κ = 1, then
ψ is a transformation group according to Def-
inition 1. Given a function g(ξ, t) we can also
ﬁnd the global mapping ψ by solving the linear
ﬁrst order PDE (using for example the method
of characteristics):

∂tψ(ξ, t)

−

g(ξ, t)∂ξψ(ξ, t) = 0
ψ(0, t) = t,

(19)

corresponding to Eq. 12 above. Using Eq. 18
we derive the structural diﬀerential equations
for the principal eigenfunctions

∂tuk(t) =

Akl(t)ul(t)

(20)

Xl

where

Akl(t) =

dξg (ξ, t) vk(ξ)∂ξvl(ξ),

λl
λk ZΩT −W

r

which in this case dependent on t. The
time dependence of A in Eq. 20 makes the

equations hard to analyze in general. Again
it worth noting that an equivalent approach
would be to use the relation

∂ξψ(ξ, t)∂ξf (ψ(ξ, t)) = ∂tψ(ξ, t)∂tf (ψ(ξ, t)) ,

which is valid for all smooth functions f (es-
pecially f (x) = x which gives Eq. 19).

There are a number of diﬀerent ways to
further generalize SSA along the lines of this
paper. The perhaps most natural is to con-
sider multi-dimensional transformation groups
and/or multi-dimensional data ﬁelds. This
would require an extension of the SVD to ten-
sor decomposition. This is not straightfor-
ward [9], but generalized versions of the SVD
does exist [10].

Finally, the author would like to thank
Steen Rasmussen, for support as well as valu-
able discussions and perspectives. The au-
thor would also like to acknolege grant support
from U.S. departmet of Energy.

References

[1] D.S. Broomhead and G.P. King, Physica

D, 20:217–236, 1986.

[2] R. Vautard and M. Ghil, Physica D,

35:395–424, 1989.

[3] M. Ghil et. al., Reviews in Geophysics, in

press, 2001.

1061, 1999.

[4] Varadi et. al., Astrophys. J., 526, 1052–

[5] J.B. Elsner and A.A. Tsonis, Plenum

Press, New Tork, 1996.

[6] N. Golyandina, V. Nekrutkin and A.
Zhigljavsky, Analysis og Time Series
Structure, Chapman & Hall/CRC, New
York, 2001.

6

[7] F. Takens, in Proceedings of the Sympo-
sion on Dynamical Systems and Turbu-
lence, University of Warwick, 1979-1980,
edited by D.A. Rand and L.S. Young,
Springer, Berlin, 1981.

[8] P. Olver, Applications of Lie Groups to
Diﬀerential Equations, Second Edition,
Springer-Verlag, 1993.

[9] T.G. Kolda, SIAM J. Matrix Anal. Appl.,

23 (1): 243–255, 2001.

[10] L.D. Lathauwer, B.D. Moor and J. Van-
dewalle, SIAM J. Matrix Anal. Appl., 21
(4): 1253–1278, 2000.

7


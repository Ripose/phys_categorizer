Advances in Data Analysis and Classiﬁcation manuscript No.
(will be inserted by the editor)

A Global Algorithm for Clustering Univariate
Observations

Nicolas Paul · Michel Terre · Luc Fety

Received: date / Accepted: date

Abstract This paper deals with the clustering of univariate observations:
given a set of observations coming from K possible clusters, one has to esti-
mate the cluster means. We propose an algorithm based on the minimization
of the ”KP” criterion we introduced in a previous work. In this paper, we
show that the global minimum of this criterion can be reached by ﬁrst solv-
ing a linear system then calculating the roots of some polynomial of order K.
The KP global minimum provides a ﬁrst raw estimate of the cluster means,
and a ﬁnal clustering step enables to recover the cluster means. Our method’s
relevance and superiority to the Expectation-Maximization algorithm is illus-
trated through simulations of various Gaussian mixtures.

Keywords unsupervised clustering · non-iterative algorithm · optimization
criterion · univariate observations

1 Introduction

In this paper we focus on the clustering of univariate observations coming
from K possible clusters, when the number of clusters is known. One method
consists in estimating the observation pdf (mixture of K pdf), by associating
a kernel to each observation and adding the contribution of all the kernels
(Parzen 1962). A search of the pdf modes then leads to the cluster means.
The drawback of such method is that it requires the conﬁguration of extra-
parameters (kernel design, intervals for the mode search). Alternately, the
Expectation-Maximization (EM) (Dempster et al. 1977) algorithm is the most

N. Paul, M. Terre, L. Fety
Conservatoire National des Arts et Metiers, Electronic and Communications,
292 rue Saint-Martin, 75003 PARIS, FRANCE
Tel.: 33 1 40 27 25 67
Fax: 33 1 40 27 24 81
E-mail: nicolas.paul@cnam.fr

7
0
0
2
 
r
a

M
 
0
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
8
2
3
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

2

(cid:1)

(cid:0)

1
1

−
−

N
K

commonly used method when the mixture densities belong to the same known
parameterized family. It is an iterative algorithm that look for the mixture pa-
rameters that maximize the likelihood of the observations. Each EM iteration
consists of two steps. The Expectation step estimates the probability for each
observation to come from each mixture component. Then, during the Maxi-
mization step, these estimated probabilities are used to update the estimation
of the mixture parameters. One can show that this procedure converges to one
maximum (local or global) of the likelihood (Dempster et al. 1977). If the mix-
ture components do not belong to a common and known parameterized family,
the EM algorithm does not directly apply. Yet, if the component densities do
not overlap too much, some clustering methods can be used to cluster the data
and calculate the cluster means: in (Fisher 1958) an algorithm is proposed to
compute the K-partition of the N sorted observations which minimize the
sum of the squares within clusters. Instead of testing the
possible parti-
tions, some relationships between k-partitions and (k + 1)-partitions are used
to recursively compute the optimal K-partition. The main drawbacks of this
method are a high sensitivity to potential diﬀerences between the cluster vari-
ances and a complexity in O(KN 2) (Fitzgibbon 2000). Among the clustering
methods, the K-Means algorithm (Hartigan 1977) is one of the most popular
method. It is an iterative algorithm which groups the data into K clusters in
order to minimize an objective function such as the sum of point to cluster
mean square Euclidean distance. The main drawback of K-Means or EM is the
potential convergence to some local extrema of the criterion they use. Some
solutions consist for instance in using smart initializations (see (McLachlan
and Peel 2000) and (Lindsay and Furman 1994) for EM, (Bradley and Fayyad
1998) for k-means) or stochastic optimization, to become less sensitive in the
initialization (see (Celeux et al. 1995) and (Pernkopf and Bouchaﬀra 2005)
for EM, (Krishna and Murty 1999) for K-means). Another drawback of these
methods is the convergence speed, which can be very slow when the number
of observations is high. A survey of the clustering techniques can be found in
(Berkhin 2006) and (Xu and Wunsch 2005). In this contribution, we propose
a non-iterative algorithm which mainly consists in calculating the minimum
of the ”K-Product” (KP) criterion we ﬁrst introduced in (Paul et al. 2006):
is a set of N observations, K the known number of clusters
if {zn}n
1
∈{
any vector of RK, we deﬁne the KP criterion as the sum
and {xk}k
···
K
k=1(zn − xk)2. The main motivation for us-
of all the K-terms products
ing such criterion is that, though it provides a slightly biased estimation of
the cluster means, its global minimum can be reached by ﬁrst solving a lin-
ear system then calculating the roots of some polynomial of order K. Once
these K roots have been obtained, a ﬁnal clustering step assigns each obser-
vation to the closest root and calculates the resulting cluster means. Another
advantage of the proposed method is that it does not require the conﬁgura-
tion of any extra-parameters. The rest of the paper is organized as follow: In
section 2 the observation model is presented and the criterion is deﬁned. In
section 3 the criterion global minimum is theoretically calculated. In section

···
1

}
K

Q

∈{

N

}

3

4 the clusters estimation algorithm is described. Section 5 presents simulation
results which illustrate the algorithm performances on diﬀerent Gaussian mix-
tures: mixtures of three, six and nine components have been simulated with
various conﬁgurations (common/diﬀerent mixing weights, common/diﬀerent
variances). Conclusions are ﬁnally given in Section 6.

2 Observation model and criterion deﬁnition

1

}

∈{

be a set of K diﬀerent values of RK, let a be the vector of
be a set of K

Let {ak}k
K
···
cluster means deﬁned by a ∆
mixing weights (prior probabilities) that sum up to one and let {gk}k
K
}
be a set of K zeros-mean densities. The probability density function of the
multimodal observation z is a ﬁnite mixture given by:

= (a1, a2 · · · aK)t, let {πk}k

∈{

∈{

···

···

K

}

1

1

f (z) =

πkgk(z − ak)

K

k=1
X

Note that the form of the densities gk are usually not known by the estimator
and that the gk do not necessarily belong to the same parameterized family.
be a set of N observations in RN . In all the following we
Now let {zn}n
assume that N is greater than K and that the number of diﬀerent observations
is greater than K − 1. The KP criterion J(x) is deﬁned by:

∈{

···

N

}

1

J : RK → R+ : x →

(zn − xk)2

(1)

N

K

n=1
X

k=1
Y

Note the diﬀerence with the K-means criterion which can be written (for the
square Euclidean distance):

K-means : RK → R+ : x →

N

n=1
X

min
1
···

∈{

k

K

(zn − xk)2
}

The KP criterion (1) is clearly positive for any vector x. The ﬁrst intuitive
motivation for deﬁning this criterion is its asymptotic behavior when all the gk
variances are null. In this case, all the observations are equal to one of the ak
and therefore J(a) = 0. J(x) is then minimal when x is equal to a or any of its
K! permutations. The second motivation is that, in the general case, J have
K! minima that are the K! permutations of one single vector which can be
reached by solving a linear system then ﬁnding the roots of some polynomial
of order K. This is shown in section 3.

3 KP global minimum

We ﬁrst give in section 3.1 some useful deﬁnitions which are needed in section
3.2 to reach the global minimum of J.

4

3.1 Some useful deﬁnitions

To any observation zn we associate the vector zn deﬁned by:

zn

∆
= (zK
n

1

, zK
n

−

−

2

· · · , 1)t, zn ∈ RK

The vector z and the Hankel matrix Z are then respectively deﬁned by:

N

z ∆
=

n zn, z ∈ RK
zK

Z ∆
=

znzt

n, Z ∈ RK

×

K

n=1
X
N

n=1
X

The matrix Z is regular if the number of diﬀerent observations is greater than
K − 1 (one explanation is detailed in Appendix A).

Now let y = (y1, · · · , yK)t be a vector of RK. We deﬁne the polynomial of
order K qy(α) as:

qy(α)

∆
= αK −

αK

kyk

−

if r = (r1, · · · , rK )t is a vector of CK containing the K roots of qy(α) the
factorial form of qy(α) is:

qy(α) =

(α − rk)

qy(α) = αK − (r1 + · · · + rK)αK

1 + ... + (−1)K(r1 × r2 · · · × rK)
K

qy(α) = αK −

αK

kwk(r)

−

where wk(r) is the Elementary Symmetric Polynomial (ESP) in the variables
r1, · · · , rK deﬁned by:
∆
= (−1)k+1

rj1 .rj2 · · · .rjk

wk(r)

(6)

K

k=1
X

K

k=1
Y
−

k=1
X

For instance, for K = 3, we have:

,jk

j1,
1
X{
}∈{
···
···
<jk6K
j1<

K

}

k

···

w1(r) = r1 + r2 + r3

w2(r) = −(r1r2 + r2r3 + r1r3)
w3(r) = r1r2r3

w(r)

∆
= (w1(r), · · · , wK(r))t

If we call w(r) the vector of ESP of r deﬁned by:

the relationship between the roots and coeﬃcients of qy(α) becomes:

y = w(r) ⇔ ∀k ∈ {1 · · · K} qy(rk) = 0

(2)

(3)

(4)

(5)

(7)

(8)

5

Table 1 KP algorithm steps and complexities

step 1: calculate a minimum of J

calculate Z and z: O(N K)
calculate ymin by solving Z.ymin = z: O(K 2)
calculate the roots (x1,min, · · · , xK,min) of qymin (α): O(K 2)
step 2: clustering and cluster means estimation

assign each zn to the closest xk,min: O(N K)
calculate the K means of the resulting clusters: O(N )

3.2 The KP minimum

The global minimum of J is given by theorem 1:

Theorem 1 if ymin is the solution of Z.ymin = z (where z and Z have been
deﬁned in (3) and (4)) and if xmin is a vector containing, in any order, the
K roots of qymin (α) (deﬁned in (5)), then xmin belongs to RK and xmin is the
global minimum of J.

The proof is given in appendix B.

4 Clusters estimation algorithm

The clusters estimation algorithm consists of two steps. In the ﬁrst step, the
minimum of J, xmin = (x1,min, ..., xK,min)t, is calculated, giving a ﬁrst raw
estimation of the set of cluster means. This ﬁrst estimate is slighly biased: for
instance, for a Gaussian mixture with two balanced components centred on
−a and a and a common standard deviation σ the asymptotical solution of
Z.ymin = z is ymin = (0, a2 + σ2)t and the roots of qymin (α) are:

xmin =

−a

1 +

, a

1 +

 

r

r

σ2
a2

σ2
a2

!

Therefore, in a second step, each observation zn is assigned to the nearest
xk,min, K clusters are formed, and the ﬁnal estimated cluster means are cal-
culated. The algorithm steps and their complexities are illustrated in table 1.
The total complexity is in O(N K + K 2), which is equivalent to O(N K) since
N is greater than K.

5 Simulations

Several types of Gaussian mixture have been considered. The number of com-
ponents (clusters) is equal to three (scenario A), six (scenario B) and nine
(scenario C). In scenario A, the distance between two successive cluster cen-
ters (component means) is equal to one. In scenario B and in scenario C, the

6

Table 2 simulation scenario A

scenario A.1

scenario A.2

scenario A.3

scenario A.4

means

variances

prior

variances

prior

variances

prior

variances

prior

Table 3 simulation scenario B

scenario B.1

scenario B.2

scenario B.3

scenario B.4

means

variances

prior

variances

prior

variances

prior

variances

prior

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

1
3

1
3

1
3

1
6

1
6

1
6

1
6

1
6

1
6

σ2

σ2
2
σ2

σ2

σ2
2
σ2

σ2
2
σ2

σ2
2

1
3

1
3

1
3

1
6

1
6

1
6

1
6

1
6

1
6

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

0.4

0.4

0.2

0.2

0.2

0.1

0.2

0.2

0.1

σ2

σ2
2
σ2

σ2

σ2
2
σ2

σ2
2
σ2

σ2
2

0.4

0.4

0.2

0.2

0.2

0.1

0.2

0.2

0.1

0

1

2

0

1

2

4

5

6

distance between two successive centers is equal to one or two. For each sce-
nario ”X”, four cases have been studied: common variance and common mixing
weight (scenario X.1), diﬀerent variances and common mixing weight (scenario
X.2), common variance and diﬀerent mixing weights (scenario X.3) and dif-
ferent variances and diﬀerent mixing weights (scenario X.4). A summary of
all the scenari is given in Tables 2, 3 and 4. The number of observations (N )
per simulation run is equal to 100 in scenario A, 200 in scenario B and 300 in
scenario C.

To evaluate the performances of our proposal we compare it to the classical
EM algorithm. To estimate the parameters of a Gaussian mixtures, the EM
algorithm proceeds as follows (Dempster et al. 1977): if ˆβ(ite)
is the estimed
n,k
probability that zn comes from cluster k and if ˆπ(ite)
and ˆσ(ite)
, ˆa(ite)
are
k
respectively the estimated prior, mean and standard deviation of cluster k at
iteration ite, then the estimations at iteration ite + 1 are given by:

k

k

Table 4 simulation scenario C

scenario C.1

scenario C.2

scenario C.3

scenario C.4

means

variances

prior

variances

prior

variances

prior

variances

prior

0

1

2

4

5

6

8

9

10

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

1
9

1
9

1
9

1
9

1
9

1
9

1
9

1
9

1
9

σ2

σ2
2
σ2

σ2

σ2
2
σ2

σ2

σ2
2
σ2

1
9

1
9

1
9

1
9

1
9

1
9

1
9

1
9

1
9

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

σ2

2
15

2
15

1
15

1
15

3
15

1
15

2
15

2
15

1
15

σ2

σ2
2
σ2

σ2

σ2
2
σ2

σ2

σ2
2
σ2

7

2
15

2
15

1
15

1
15

3
15

1
15

2
15

2
15

1
15

Expectation step:

Maximization step:

ˆβ(ite+1)
n,k

=

ˆπ(ite)
√2πˆσ(ite)

k

k

exp

− 1
2

 

(cid:18)

2

zn

ˆa(ite)

k

−
ˆσ(ite)
k

!
2

(cid:19)
ˆa(ite)

k

K

k=1
P

k

ˆπ(ite)
√2π ˆσ(ite)

k

exp

− 1
2

 

(cid:18)

zn

−
ˆσ(ite)
k

!

(cid:19)

ˆπ(ite+1)
k

=

ˆa(ite+1)
k

=

N
n=1

P
N
n=1
N
n=1

P

ˆβ(ite+1)
n,k
N
ˆβ(ite+1)
zn
n,k
ˆβ(ite+1)
n,k

ˆσ(ite+1)
k

=

P

N
n=1

P
ˆβ(ite+1)
n,k

2

zn − ˆa(ite+1)
k
(cid:16)
ˆβ(ite+1)
n,k

(cid:17)

N
n=1

This iterative procedure converges to one maximum of the likelihood function
P
N

1

···

∈{

P (zn| {ˆak ˆσk ˆπk}k

). To initialize the EM algorithm in our simula-

K
}
n=1
tions, K cluster means ˆa(0)
Q
are randomly chosen with a uniform draw in the
k
observation zone [min(zn) max(zn)]. For each n, ˆβ(1)
is
the closest cluster means to the observation zn and ˆβ(1)
n,k is set to zero oth-
erwise. This initialization is repeated until each cluster contains at least one
observation. Then the EM starts with a maximization step. The algorithm is

n,k is set to one if ˆa(0)

k

8

stopped if all the estimated parameters do not change between two EM steps
or if a maximal number of 100 iterations is reached.

The clustering performances are evaluated as follows: to get rid of the
permutation ambiguity, for each simulation run r and estimation ˆar, the per-
formance criterion er is deﬁned as the maximal absolute distance between the
true and estimated sorted vector of cluster means:

er

∆
= N (sort(a) − sort(ˆar))

where N(x)

∆
= max
K
···
∈{

k

1

|xk|.
}

The distribution of er is given in ﬁgure 1 for the scenario A.1 with σ = 0.25
and 10000 simulation run. The KP minimum is a biased estimation: er is
greater than 0.1 for 90% of the run. Yet, er remains less than 0.2 for 80% of the
run. Then the ”full KP” algorithm (calculation of the KP minimum followed
by a clustering) always provides an accurate set of estimates: er remains less
than 0.1 (resp 0.2) for 80% (resp. 100%) of the run. With the EM algorithm, er
is less than 0.1 (resp. 0.2) for 45% (resp. 65%) of the run but er is greater than
0.5 for 30% of the run. In this case, the EM gets stuck at a local maximum of
the likelihood. Typically one estimated cluster mean is located in the middle
of two true cluster means (assuming a too high variance), while two other
estimated cluster means are closed to the same true cluster mean. In ﬁgure
2, 3 and 4 we present the EM and KP performances for all the scenari with
diﬀerent values of σ. In scenario A (scenari A.1 to A.4) the KP algorithm
estimation is perfect when σ is less than 0.2 (er is less than 0.1 for 95% of
the run) and remains correct for σ < 0.3 (er is less than 0.2 for 95% of the
run). On the contrary, EM can provide a wrong set of estimated clusters as
soon as σ is not null: for instance, when σ = 0.1, er is greater than 0.5 for
25% of the run. When the mixture components strongly overlap (σ > 0.5)
the two methods lead to wrong estimations, with a slight superiority of EM
when σ > 0.8. The KP algorithm remains superior to EM in scenario B and
in scenario C. In scenario B (6 clusters), KP is robust for any σ less than 0.15,
while, for σ = 0.1, EM converges to a wrong set of cluster means for 75% of
the run. in scenario C (9 clusters), KP is robust for any σ less than 0.05, while,
for σ = 0.02, EM converges to a wrong set of cluster means for 70% of the run.
In each scenario, the mixing weights conﬁguration (balanced/unbalanced) has
a slight inﬂuence on the KP algorithm: the performances on sub-scenari X.3
and X.4 (diﬀerent mixing weights) are weaker than the performances on sub-
scenari X.1 and X.2 (common mixing weights). Yet the KP performances on
the unbalanced mixtures remain strongly greater than the EM performances.

6 Conclusion

We have proposed a clusters estimation algorithm for univariate observations
when the number of clusters is known. It is based on the minimization of the
”KP” criterion we ﬁrst introduced in (Paul et al. 2006). We have shown that

performances on scenario A.1 with sigma=0.25

KP minimum
KP minimum + final clustering
EM

9

n
o
i
t
u
b
i
r
t
s
d

i

5500

5000

4500

4000

3500

3000

2500

2000

1500

1000

500

0

0

0.2

0.4

0.8

1

1.2

0.6
e
r

Fig. 1 Estimation performances on scenario A.1 with σ = 0.25. 10000 simulation run have
been performed. For each simulation run, 100 observations have been generated. er is the
maximal distance between the sorted vector of true cluster means and the sorted vector of
estimated cluster means

the global minimum of this criterion can be reached with a linear least square
minimization followed by a roots ﬁnding algorithm. This minimum is used
to get a ﬁrst raw estimation of the cluster means, and a ﬁnal clustering step
enables to recover the cluster means. The proposed method is not iterative, its
complexity is in O(N K + K 2) and it does not require the conﬁguration of any
extra parameter. Simulations have illustrated the KP algorithm performances
and superiority to the Expectation-Maximization algorithm which can get
stuck at a local maximum of the likelihood. We focused on the univariate case
and our current researchs deal with the multivariate case. If the observations
is any set of K vectors of Rd, the KP criterion
zn belong to Rd, if {xk}k
1
∈{
is now deﬁned as the sum of all the K-terms products
Rd. The
minima of such criterion and some algorithms to reach them are currently
being studied.

K
k=1 ||zn − xk||2

Q

···

K

}

Acknowledgements The authors want to thank B. Scherrer, M. Bellanger, P. Tortelier,
G. Saporta and J.P.Nakache for their constructive comments that helped in improving this
manuscript.

References

1. Berkin P (2006) A Survey of clustering data mining techniques. Grouping Multidimensional
Data: Recent Advances in Clustering, Ed. J. Kogan and C. Nicholas and M. Teboulle,
Springer, pp. 25-71

10

2. Bradley P S, Fayyad U M (1998) Reﬁning initial points for K-means clustering. Proc. of the
15th Int. Conf. on Machine Learning, San-Fransisco, Morgan Kaufmann, pp. 91-99
3. Celeux G, Chauveau D, Diebolt J (1995) On stochastic version of the EM algorithm. INRIA

research report no 2514, available: http://www.inria.fr/rrrt/rr-2514.html

4. Dempster A, Laird N, Rubin D (1977) Maximum likelihood from incomplete data via the

EM algorithm. Journal of the Royal Statistical Society, B. 39, pp. 1-38

5. Fisher W D (1958) On grouping for maximum homogeneity. Journal of the American Sta-

tistical Association, Vol. 53, No. 284, pp. 789-798

6. Fitzgibbon L J, Allison L, Dowe D L (2000) Minimum message length grouping of ordered
data. Algorithmic Learning Theory, 11th International Conference, ALT 2000, Sydney,
Australia

7. Hartigan J, Wong M (1979) A k-means clustering algorithm, Journal of Applied Statistics,

vol 28, pp. 100-108

8. Krishna K, Narasimha Murty M (1999) Genetic K-Means Algorithm, IEEE Transactions on

Systems, Man, and Cybernetics - Part B: Cybernetics, Vol. 29, No. 3

9. Lindsay B, Furman D (1994) Measuring the relative eﬀectiveness of moment estimators as
starting values in maximizing likelihoods. Computational Statistics and Data Analysis,
Volume 17, Issue 5, pp. 493-507

10. McLachlan G, Peel D (2000) Finite Mixture Models. Wiley Series in probability and statis-

tics, John Wiley and Sons

11. Parzen E (1962) On estimation of a probability density function and mode. Annals of

Mathematicals Statistics 33, pp. 1065-1076

12. Paul N, Terre M, Fety L (2006) The k-product criterion for gaussian mixture estimation.

7th Nordic Signal Processing Symposium, Reykjavik, Iceland

13. Pernkopf F, Bouchaﬀra D (2005) Genetic-based EM algorithm for learning gaussian mixture
models, IEEE Transactions On Pattern Analysis and Machine Intelligence, Vol. 27, No. 8
14. Xu R, Wunsch II D (2005) Survey of Clustering Algorithms. IEEE Transactions On Neural

Networks, vol. 16, No. 3, pp. 645-676

A : non-singularity of Z

In appendix A we explain why the matrix Z of size K × K, deﬁned in (4) is regular if the
number of diﬀerent observations is greater than K − 1. Z can be written as the following
matrix product:

where V is a K × N Vandermonde Matrix deﬁned by:

Z = VVt

V ∆

= (z1, z2, · · · zN )

and zn has been deﬁned in (2). Let us assume that the K ﬁrst observations are diﬀerent. The
(zj − zi),
determinant of the K×K Vandermonde matrix (z1, z2, · · · zK ) is equal to

which is diﬀerent from zero. The rank of V is then equal to K, so the rank of Z is equal to
K and Z is regular.

1≤i<j≤K
Q

B : proof of theorem 1

In appendix B we prove theorem 1. Let F be the function deﬁned by:

F : CK → R+ : x →

||zn − xk||2
C

N

K

n=1
X

Yk=1

The restriction of F to RK is the function J since the observations zn are real:

∀x ∈ RK :

F (x) = J(x)

(9)

11

(11)

(12)

The function H applied to the ESP of a vector x in Ck is equal to the function F applied
to x:

∀x ∈ CK :

F (x) =

(zn − xk)

(10)

Now let H be the function deﬁned by:

H : CK → R+ : y →

zK
n − zt

ny

N

n=1 ‚
X
‚
‚

N

K

n=1 ‚
Yk=1
‚
X
‚
‚
‚

N

K

zK
n −
n=1 ‚
‚
X
‚
‚
‚

Xk=1

N

n=1 ‚
X
‚
‚

2

C

‚
‚
‚

2

C

‚
‚
‚
‚
‚

2

C

‚
‚
‚

2

C

‚
‚
‚
‚
‚

∀x ∈ CK :

F (x) =

zK
n − zt

nw(x)

∀x ∈ CK :

F (x) = H(w(x))

developping (10) using deﬁnition (6) leads to:

∀x ∈ CK :

F (x) =

zK−k
n

wk(x)

including deﬁnitions (2) and (7):

The global minimum of H is the linear least square solution ymin given by:

n=1 ‚
X
‚
‚
developping (12) using deﬁnitions (3) and (4) and remembering that the coeﬃcients of Z
and z are real:

ymin = argmin

y∈CK (

N

zK
n − zt

2

C)

ny
‚
‚
‚

ymin = argmin
y∈CK

n

yH Zy − 2Re{yH }z

o

Z.ymin = z, ymin ∈ RK
The Hankel matrix Z is regular since the number of diﬀerent observations is greater than
K − 1 (appendix A). System (13) therefore has exactly one solution. Since Z belongs to
RK×K and z belongs to RK , ymin belongs to RK . Now let xmin=(x1,min, · · · , xK,min)t
be a vector containing, in any order, the K (potentially complex) roots of qymin (α). One
can show that the following holds:

(13)

(i) xmin is a global minimum of F
(ii) xmin ∈ RK
(iii) xmin is a global minimum of J
Property (i) is a direct consequence of (11):

According to (8), ymin = w(xmin) and we have:

∀x ∈ CK :

F (x) = H(w(x))

∀x ∈ CK :
∀x ∈ CK :

F (x) ≥ min {H}
F (x) ≥ H(ymin)

∀x ∈ CK :

F (x) ≥ H(w(xmin))

∀x ∈ CK :

F (x) ≥ F (xmin)

which proves (i). Property (ii) can be shown by contradiction: if xmin does not belong to
RK , then for one of the xk,min we have xk,min 6= Re{xk,min} and, since all the observations
zn are real:

∀n ∈ {1, · · · , N } :

zn − xk,min

C >

zn − Re{xk,min}

C

‚
‚

‚
‚

‚
‚

‚
‚

12

which leads to:

This is impossible since xmin is a global minimum of F . This proves property (ii). We ﬁnally
have to prove (iii): since xmin ∈ RK we have, using (9):

F (xmin) > F (Re{xmin})

F (xmin) = J(xmin)

(14)

Furthermore, according to (9):

then, according to property (i):

using (14):

∀x ∈ RK :

J(x) = F (x)

∀x ∈ RK :

J(x) ≥ min{F }

∀x ∈ RK :

J(x) ≥ F (xmin)

∀x ∈ RK :

J(x) ≥ J(xmin)

which proves (iii). Properties (ii) and (iii) directly lead to theorem 1.

13

EM and KP performances on scenario A

KP − scenario A.1
EM − scenario A.1
KP − scenario A.2
EM − scenario A.2
KP − scenario A.3
EM − scenario A.3
KP − scenario A.4
EM − scenario A.4

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
sigma

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
sigma

)
1
.
0
<
e
(
P

r

1

0.8

0.6

0.4

0.2

0

0

)
2
.
0
<
e
(
P

r

)
5
.
0
>
e
(
P

r

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

1

0.5
sigma

Fig. 2 performances of the EM and KP algorithms on scenario A for diﬀerent values of σ.
For each value of σ and for each sub-scenario 10000 simulation run have been performed.
For each simulation run, 100 observations have been generated. er is the maximal distance
between the sorted vector of true cluster means and the sorted vector of estimated cluster
means. The performance criteria are the probabilities for er to be smaller than 0.1 (top),
smaller than 0.2 (middle) and greater than 0.5 (bottom).

14

EM and KP performances on scenario B

KP − scenario B.1
EM − scenario B.1
KP − scenario B.2
EM − scenario B.2
KP − scenario B.3
EM − scenario B.3
KP − scenario B.4
EM − scenario B.4

0.05

0.1

0.15

0.25

0.3

0.35

0.4

0.2
sigma

0.05

0.1

0.15

0.25

0.3

0.35

0.4

0.2
sigma

)
1
.
0
<
e
(
P

r

1

0.8

0.6

0.4

0.2

0

0

)
2
.
0
<
e
(
P

r

1

0.8

0.6

0.4

0.2

0

0

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

0

0

.

)
5
0
>
e
(
P

0.5

r

0.05

0.1

0.15

0.25

0.3

0.35

0.4

0.2
sigma

Fig. 3 performances of the EM and KP algorithms on scenario B for diﬀerent values of σ.
For each value of σ and for each sub-scenario 10000 simulation run have been performed.
For each simulation run, 200 observations have been generated. er is the maximal distance
between the sorted vector of true cluster means and the sorted vector of estimated cluster
means. The performance criteria are the probabilities for er to be smaller than 0.1 (top),
smaller than 0.2 (middle) and greater than 0.5 (bottom).

15

EM and KP performances on scenario C

KP − scenario C.1
EM − scenario C.1
KP − scenario C.2
EM − scenario C.2
KP − scenario C.3
EM − scenario C.3
KP − scenario C.4
EM − scenario C.4

0.05

0.1

0.15

0.25

0.3

0.35

0.4

0.2
sigma

0.05

0.1

0.15

0.25

0.3

0.35

0.4

0.2
sigma

)
1
.
0
<
e
(
P

r

1

0.8

0.6

0.4

0.2

0

0

)
2
.
0
<
e
(
P

r

)
5
.
0
>
e
(
P

r

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.05

0.1

0.15

0.25

0.3

0.35

0.4

0.2
sigma

Fig. 4 performances of the EM and KP algorithms on scenario C for diﬀerent values of
σ. For each value of σ and for each sub-scenario 1000 simulation run have been performed.
For each simulation run, 300 observations have been generated. er is the maximal distance
between the sorted vector of true cluster means and the sorted vector of estimated cluster
means. The performance criteria are the probabilities for er to be smaller than 0.1 (top),
smaller than 0.2 (middle) and greater than 0.5 (bottom).


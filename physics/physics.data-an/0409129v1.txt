4
0
0
2
 
p
e
S
 
7
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
2
1
9
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

CDF/MEMO/STATISTICS/PUBLIC/7117

Interval estimation in the presence of nuisance parameters.
1. Bayesian approach.

Joel Heinricha, Craig Blockerb, John Conwayc, Luc Demortierd,
Louis Lyonse, Giovanni Punzif, Pekka K. Sinervog
aUniversity of Pennsylvania, Philadelphia, Pennsylvania 19104

f Istituto Nazionale di Fisica Nucleare, University and Scuola Normale Superiore of Pisa, I-56100 Pisa, Italy

bBrandeis University, Waltham, Massachusetts 02254

cRutgers University, Piscataway, New Jersey 08855

dRockefeller University, New York, New York 10021

eUniversity of Oxford, Oxford OX1 3RH, United Kingdom

gUniversity of Toronto, Toronto M5S 1A7, Canada

September 27, 2004

Abstract

We address the common problem of calculating intervals in the presence of
systematic uncertainties. We aim to investigate several approaches, but here
describe just a Bayesian technique for setting upper limits. The particular ex-
ample we study is that of inferring the rate of a Poisson process when there are
uncertainties on the acceptance and the background. Limit calculating software
associated with this work is available in the form of C functions.

1 The problem

A very common statistical procedure is obtaining a conﬁdence interval for a physics pa-
rameter of interest, when there are uncertainties in quantities such as the acceptance of
the detector and/or the analysis procedure, the beam intensity, and the estimated back-
ground. These are known in statistics as nuisance parameters, or in Particle Physics
as sources of systematic uncertainty. We assume that estimates of these quantities are
available from subsidiary measurements.1 A variant of this procedure which is particu-

1There are other possibilities. Thus it may be that all that is known is that a nuisance parameter
µu; that is not enough information for a Bayesian
is contained within a certain range: µl ≤
approach. Alternatively the data relevant for the physics and nuisance parameters could be bound up
in the main measurement, and not require a subsidiary one.

≤

µ

1

larly relevant for Particle Physics is the extraction of an upper limit on the rate of some
hypothesized process or on a physical parameter, again with systematic uncertainties.
To specify the problem in more detail, we assume that we are performing a counting
experiment in which we observe n counts, and that the acceptance has been estimated
σb. For a signal rate s, n is Poisson distributed
as ǫ0 ±
with mean sǫ + b. Here ǫ contains factors like the intensity of the accelerator beam(s),
the running time, and various eﬃciencies. It is constrained to be non-negative, but can
be larger than unity.

σǫ and the background as b0 ±

We aim to study and compare diﬀerent approaches for determining conﬁdence in-

tervals for this problem. In general we are interested in pathologies in these areas:

•

Coverage. This is a measure of how often the limits that we deduce would
in fact include the true value of the parameter. This requires consideration of
an ensemble of experiments like the one we actually performed, and hence is
an essentially frequentist concept. Nevertheless, it can be applied to a Bayesian
technique.

Coverage is a property of the technique, and not of the particular limit deduced
from a given measurement. It can, however, be a function of the true value of
the parameter, which is in general unknown in a real measurement.

Undercoverage (i.e. the probability of containing the true value is less than the
stated conﬁdence level) is regarded by frequentists as a serious defect. Usually
coverage is required for all possible values of the physical parameter.2 In contrast,
overcoverage is permissible, but the larger intervals result in less stringent tests
of models that predict the value of the parameter. For measurements involving
quantised data (e.g. Poisson counting), most methods have coverage which varies
with the true value of the parameter of interest, and hence if undercoverage is to
be avoided, overcoverage is inevitable.

Frequentist methods by construction will not undercover for any values of the
parameters. This is not guaranteed for other approaches. For example, even
though the Bayesian intervals shown here do not undercover, in other problems
Bayesian 95% credible intervals could even have zero coverage for some values of
the parameter of interest.[2] It should also be remarked that, although coverage
is a very important property for frequentists, on its own exact coverage does not
guarantee that intervals have desirable properties (for many examples, see Refs.
[3] and [4]).

2The argument is that the parameter is unknown, and so we wish to have coverage, whatever its
value. This ensures that, if we repeat our speciﬁc experiment many times, we should include the
true value within our conﬁdence ranges in (at least) the stated fraction of cases. This argument may,
however, be over-cautious. The location of the dips in a coverage plot like that of Fig. 1 occur at values
which are not ﬁxed in s, but which depend on the details of our experiment (such as the values of ǫ and
b). These details vary from experiment to experiment. Thus we could achieve ‘no undercoverage for
the ensemble of experiments measuring the parameter s’, even if the individual coverage plots did fall
below the nominal coverage occasionally. Thus in some sense ‘average coverage’ would be suﬃcient
(see for example reference [1]), although it is hard to quantify the exact meaning of ‘average’. It
should be stated that this is not the accepted position of most High Energy Physics frequentists.

2

•

Interval length. This is sometimes used as a criterion of accuracy of intervals,
in the sense that shorter intervals have less probability of covering false values
of the parameter of interest. However, one should keep in mind that short in-
tervals are only desirable if they contain the true value of the parameter. Thus
empty intervals, which do occur in some frequentist constructions, are generally
undesirable, even when their construction formally enjoys frequentist coverage.

Intervals that ﬁll the entire physically allowed range of the parameter of interest
may also occur in some situations. Examples of this behavior are given in [3]
and [5]. An experimenter who requests a 68% conﬁdence interval, but receives
what appears to be a 100% conﬁdence interval instead, may not be satisﬁed
with the explanation that he is performing a useful service in helping to keep
the coverage probability—averaged over his measurement and his competitor’s
measurements—from dropping below 68%.

•

Bayesian credibility. In some situations it may be relevant to calculate the
Bayesian credibility of an interval, even when the latter was constructed by fre-
quentist methods. This would of course require one to choose a prior for all
the unknown parameters. The question is one of plausibility: given the type of
measurement we are making, the resolution of the apparatus, etc., how likely is
it that the true value of the parameter we are interested in lies in the calculated
interval? Does this diﬀer dramatically from the nominal coverage probability of
the interval? In fact for diﬀerent values of the observable(s), frequentist ranges
are very likely to have diﬀerent credibilities. Some examples of this behavior are
noted in Ref. [6].

When calculating the Bayesian credibility of frequentist intervals, “uninforma-
tive” priors appear advisable. Note that severe interval length pathologies will
automatically produce a large inconsistency between the nominal coverage and
the Bayesian credibility of an interval. Except in a handful of very special cases,
it is not possible to construct an interval scheme that has simultaneously con-
stant Bayesian credibility and constant frequentist coverage, even if one has total
freedom in choosing the prior(s). Although it is not at all clear exactly how large
a level of disagreement is pathological, nevertheless it may be instructive to know
how severely an interval scheme deviates from constant Bayesian credibility (and
how sensitive this is to the choice of prior).

•

Bias. In the context of interval selection, this means having a larger coverage
B(s′, strue) for an incorrect value s′ of the parameter than for the true value strue.
This requires plots of coverage versus s′ for diﬀerent values of strue. For upper
limits, B(s′
2, so methods are necessarily
biassed for low s′. Bias thus is not very interesting for upper limits. It will be
discussed in later notes dealing with two-sided intervals.

1 is less than s′

2, strue) if s′

1, strue)

B(s′

≥

•

Transformation under
that are not
reparametrisation.
transformation-respecting can be problematic. For example, it is possible for
the predicted value of the lifetime of a particle to be contained within the 90%

Intervals

3

interval determined from the data, but for the corresponding predicted value of
the decay rate (equal to the reciprocal of the predicted lifetime) to be outside the
90% interval when the data is analysed by the same procedure, but in terms of
decay rate. This would result in unwanted ambiguities about the compatibility
of the data with the prediction.

•

•

Unphysical ranges. The question here is whether the interval construction
procedure can be made to respect the physical boundaries of the problem. For
example, branching fractions should be in the range zero to one, masses should not
be negative, etc. Statements about the true value of a parameter should respect
In contrast, some methods give estimates of parameters
any physical bounds.
which can themselves be unphysical, or which include unphysical values when
the errors are taken into account. We do not recommend truncating ranges of
estimates of parameters to obey such bounds. Thus the fact that a branching
fraction is estimated as 1.1
0.2 conveys more information about the experimental
result than does the statement that it lies in the range 0.9 to 1.

±

Behavior with respect to nuisance parameter. We would normally expect
that the limits on a physical parameter would tighten as the uncertainty on a
nuisance parameter decreases; and that as this uncertainty tends to zero, the
limits should agree with those obtained under the assumption that the “nuisance
parameter” was exactly known. (Otherwise we could sometimes obtain a tighter
limit simply by pretending that we knew less about the nuisance parameter than
in fact is the case.) These desiderata are not always satisﬁed by non-Bayesian
methods (see [7] and [8]).

Although we are ultimately interested in comparing diﬀerent approaches to this
problem, in this note we investigate a Bayesian technique for determining upper limits.
Our purpose is to spell out in some detail how this approach is used, and to discuss
some of the properties of the resulting limits in this speciﬁc example. We believe that,
for variants of this problem (e.g. diﬀerent choice of prior for s; alternative assumptions
about the information on the nuisance parameters; etc.), the reader could readily adapt
the techniques described here (and the associated software) to their particular situation.
We will report on two-sided intervals and also compare with other methods (e.g.

Cousins–Highland, pure frequentist, proﬁled frequentist) in later notes.

2 Reminder of Bayesian approach

Before dealing with the problem of extracting and studying the limits on s as deduced
from observing n events from a Poisson distribution with mean sǫ + b in the presence
of an uncertainty on ǫ, we recall the way the Bayesian approach works for the simpler
problem of a counting experiment with no background and with ǫ exactly known. Then

4

(1)

(2)

(3)

(4)

(5)

gives

Then5

n is Poisson distributed with mean sǫ, and Bayes’ Theorem3

P (B

C) = P (C

B)P (B)/P (C)

|

|

p(s

n) =

|

s)π(s)
P (n
|
s)π(s) ds
P (n
|

where π(s) is the prior probability density for s; p(s
density function (p.d.f.) for s, given the observed n; and P (n
|
observing n, given s.

R

|

n) is the posterior probability
s) is the probability of

We assume a constant prior for s,4 and that P (n
|
s) = e−sǫ(sǫ)n/n!

P (n
|

s) is given by the Poisson

p(s

n) = ǫe−sǫ(sǫ)n/n!

|

su

The limit is now obtained by integrating this posterior p.d.f. for s until we achieve the
required fraction β of the total integral from zero to inﬁnity. If β is 90%, the upper
limit su is given by

0
Z
β is termed the credible or Bayesian conﬁdence level for the limit.

p(s

n) ds = 0.9

|

20 + 1.28√20

For diﬀerent observed n, the upper limits are shown in the last two columns of
Table 1, for b = 0 and for b = 3 respectively. The Gaussian approximation for the case
b = 0, n = 20, would yield su ≃
25.7, which is roughly comparable to
the corresponding su = 27.0451 of the Table. For b = 0, it coincidentally turns out that,
for this particular example, the Bayesian upper limits are identical with those obtained
in a frequentist calculation with the Neyman construction and a simple ordering rule
(see later note on the frequentist approach to this problem). In general this is not so.
Other priors sometimes used for s are 1/√s [9] or 1/s [10]. Having a prior peaked at
smaller values of s in general results in tighter limits for a given observed n.

≃

If the whole procedure is now repeated with a background b and a ﬂat prior, the
upper limits not surprisingly decrease for increasing b at ﬁxed n (except for the case
n = 0 where the limits can trivially be seen to be independent of b). This is not
inconsistent with the fact that the mean limit for a series of measurements increases
with b, i.e. experiments with larger expected backgrounds have poorer sensitivity.

3We follow the common convention whereby lower case π’s denote prior p.d.f.’s, lower case p’s
denote other p.d.f.’s, upper case Π’s denote prior probabilities, and upper case P ’s denote other
probabilities. Equation (1) is true for probabilities, p.d.f.’s, or mixtures depending on whether B
and/or C are discrete or continuous variables.

4This is an assumption, not a necessity, and is in some ways unsatisfactory. (It is implausible,
cannot be normalised, and creates divergences for the posterior if used with a (truncated) Gaussian
prior for the acceptance ǫ.)

5It turns out that the sum over n of the discrete distribution (3) and the integral over s of the
s) and the
|

continuous distribution (4) are both equal to unity. This means that the probability P (n
probability density p(s

n) are correctly normalised.
|

5

n
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

b = 0
2.3531
3.9868
5.4669
6.8745
8.2380
9.5714
10.8826
12.1766
13.4570
14.7261
15.9858
17.2375
18.4823
19.7210
20.9545
22.1832
23.4078
24.6286
25.8459
27.0601
28.2715

1
2.3531
3.3470
4.5520
5.8618
7.1964
8.5213
9.8288
11.1203
12.3984
13.6655
14.9233
16.1732
17.4163
18.6535
19.8854
21.1127
22.3359
23.5553
24.7714
25.9844
27.1946

2
2.3531
3.0620
3.9676
5.0463
6.2451
7.5063
8.7885
10.0703
11.3441
12.6085
13.8641
15.1121
16.3533
17.5887
18.8191
20.0448
21.2665
22.4845
23.6992
24.9109
26.1199

ǫ = 1.0 ± 0.1
4
2.3531
2.8000
3.3623
4.0571
4.8914
5.8579
6.9344
8.0904
9.2952
10.5247
11.7630
13.0017
14.2371
15.4682
16.6946
17.9169
19.1353
20.3502
21.5619
22.7708
23.9770

5
2.3531
2.7297
3.1953
3.7666
4.4569
5.2719
6.2066
7.2450
8.3635
9.5365
10.7415
11.9621
13.1881
14.4139
15.6373
16.8572
18.0737
19.2868
20.4969
21.7042
22.9090

3
2.3531
2.9019
3.6026
4.4644
5.4751
6.6022
7.8047
9.0460
10.3014
11.5575
12.8090
14.0542
15.2934
16.5269
17.7554
18.9795
20.1996
21.4161
22.6294
23.8397
25.0474

6
2.3531
2.6783
3.0736
3.5534
4.1313
4.8180
5.6184
6.5289
7.5374
8.6250
9.7701
10.9525
12.1560
13.3692
14.5856
15.8014
17.0151
18.2261
19.4344
20.6400
21.8432

7
2.3531
2.6391
2.9816
3.3922
3.8832
4.4660
5.1499
5.9387
6.8300
7.8142
8.8758
9.9966
11.1582
12.3452
13.5459
14.7528
15.9612
17.1689
18.3747
19.5784
20.7799

8
2.3531
2.6083
2.9099
3.2671
3.6904
4.1904
4.7772
5.4586
6.2380
7.1136
8.0775
9.1170
10.2162
11.3588
12.5302
13.7187
14.9161
16.1172
17.3189
18.5198
19.7191

ǫ = 1 ± 0

b = 0
2.3026
3.8897
5.3223
6.6808
7.9936
9.2747
10.5321
11.7709
12.9947
14.2060
15.4066
16.5981
17.7816
18.9580
20.1280
21.2924
22.4516
23.6061
24.7563
25.9025
27.0451

3

2.3026
2.8389
3.5228
4.3624
5.3447
6.4371
7.5993
8.7958
10.0030
11.2085
12.4073
13.5983
14.7816
15.9580
17.1280
18.2924
19.4516
20.6061
21.7563
22.9025
24.0451

Table 1: Upper 90% limits for n observed events with b background and ǫ = 1.0
0.1
(κ = 100 and m = 99, as deﬁned in section 3.2). Also shown are limits for b = 0 and
b = 3 with ﬁxed ǫ = 1.

±

2.1 Coverage

≥

(6)

Next we can investigate the frequentist coverage C(strue)6 of this Bayesian approach.
That is, we can ask what the probability is, for a given value of strue, of our upper limit
being larger than strue, and hence being consistent with it. This is equivalent to adding
strue i.e.
up the Poisson probabilities of eqn. (3) for those values of n for which su(n)

C(strue) =

e−strueǫ(strueǫ)n/n!

Xrelevant n

As strue increases through any of the values of su of the last two columns of Table 1,
the coverage drops sharply. For example, for the case of zero background and eﬃciency
known to be unity, the 90% Bayesian upper limits will include strue = 3.8896 for n = 1
or larger. But strue = 3.8898 is no longer below the upper limit for n = 1. Thus
one term drops out of the summation of eqn. (6) for the calculation of the coverage at
strue = 3.8898, while the remaining terms change but little for the small change in strue;
this produces the abrupt fall in coverage. The coverage is plotted in Fig. 1, where the
drop at strue = 3.8897 can be seen.

The calculation of C(strue) can be done as follows: The identity

f ′(x) = e−x

n−1

n

xk
k! −

"

k=0
X

k=0
X

xk
k! #

=

e−x xn
n!

−

for

f (x) = e−x

(7)

xk
k!

n

k=0
X

6This is the coverage at s = strue when the Poisson variable is generated with s = strue. This

diﬀers from B(s′, strue) where the coverage is checked at s = s′ when the generation value is strue.

6

allows us to write (integrating

f ′(x))
strueǫ

−

strue

p(s

n) ds =
|

0
Z

0
Z

e−x xn
n!

dx = 1

e−strueǫ

−

(strueǫ)k
k!

n

k=0
X

From this, it follows that “relevant n” is equivalent to any one of these inequalities:

su(n)

strue ⇔

≥

p(s

n) ds
|

≥

p(s

n) ds
|

β

1

⇔

≥

−

su

0
Z

strue

0
Z

e−strueǫ

(strueǫ)k
k!

n

k=0
X

and our expression for the coverage becomes

C(strue) = 1

−

′

e−strueǫ (strueǫ)n

n!

where
result proves that C(strue)

n=0
X
means “sum until the next term would cause the sum to exceed 1
β for all values of strue in this simple example.

′

−

P

≥

(8)

(9)

(10)

β”. This

It is seen that the coverage starts at 100% for small strue. This is because even for
n = 0 the Bayesian upper limit will include strue, and this is even more so for larger n.
Bayesian methods can be shown to achieve average coverage. By this we mean that
when the coverage is averaged over the parameter s, weighted by the prior in s, the
result will agree with the nominal value β, i.e.

C(s) π(s) ds
π(s) ds

= β

R

R

(11)

A proof of this theorem is given in the second appendix, section 7 of this note.

For a constant prior, the region at large s tends to dominate the average, while in
general we will be interested in the coverage at small s. Thus the “average coverage”
result is of academic rather than practical interest, especially for the case of a ﬂat prior.
Indeed it is possible to have a situation where the average coverage is, say, 90%, while
the coverage as a function of s is always larger than or equal to 90%.

3 The actual problem

Our actual problem diﬀers from the simple case of Section 2 in that
(a) we have a background b, assumed for the time being to be accurately known; and
(b) we have an acceptance ǫ estimated in a subsidiary experiment as ǫ0 ±
express p(s, ǫ
|

What we are going to do is to use a multidimensional version of Bayes’ Theorem to

s, ǫ) and the priors for s and ǫ. The relationship is7

n) in terms of P (n
|

σǫ.

p(s, ǫ
|

n) =

s, ǫ)π(s)π(ǫ)

P (n
|
s, ǫ)π(s)π(ǫ) ds dǫ

P (n
|

(12)

7For the case where the probabilities have a frequency ratio interpretation, this is seen from the

mathematical identities

RR

P (X and Y and Z) = N (X and Y and Z)

N (Z)

N (Z)
Ntot = P (X, Y

P (X and Y and Z) = N (X and Y and Z)

N (X and Y )

N (X and Y )
Ntot

= P (Z

Z) P (Z) and
|

X, Y ) P (X, Y ).
|

So with X, Y and Z identiﬁed with s, ǫ and n respectively, and with the prior for s and ǫ
factorising into two separate priors for s and for ǫ, we obtain p(s, ǫ

n) P (n) = P (n
|

s, ǫ) π(s) π(ǫ).
|

7

To obtain the posterior p.d.f. for s, we now integrate this over ǫ:

and ﬁnally we use this to set a limit on s as in eqn. (5).

The coverage for this procedure needs to be calculated as a function of strue and

ǫtrue. The average coverage theorem of the previous section must be generalized to

p(s

n) =

|

p(s, ǫ
|

n) dǫ,

∞

0

Z

C(s, ǫ)π(s)π(ǫ) ds dǫ
π(s)π(ǫ) ds dǫ

= β

RR

RR

3.1 Priors

To implement the above procedure we need priors for s and ǫ. As in the simple
example of Section 2, for simplicity we assume that the prior for s is constant. It will
be interesting to look at the way the properties of this method change as other priors
for s are used.

We assume that the prior for ǫ is extracted from some subsidiary measurement
σǫ. We do not assume that this implies that our belief about ǫtrue is represented
ǫ0 ±
by a Gaussian distribution centred on ǫ0, as this would give trouble with the lower
end of the Gaussian extending to negative ǫ. Instead, we specify some particular form
of the subsidiary experiment that provides information about ǫ, and then assume that
a Bayesian analysis of this yields a posterior p.d.f. for ǫ. Slightly confusingly, this
posterior from the subsidiary experiment is used as the prior for the application of
Bayes’ Theorem to extract the limit on s (see eqns. (12) and (13)).

3.2 The subsidiary measurement

Somewhat arbitrarily, we assume that, for a true acceptance ǫtrue, the probability for
the measured value ǫ0 in the subsidiary experiment is given by a Poisson distribution

ǫtrue) = e−κǫtrueκmǫm

true/m!

P (ǫ0|

where ǫ0 = (m + 1)/κ, σ2
ǫ = (m + 1)/κ2 and κ is a scaling constant8. We interpret
this as the probability for ǫ0. This is discrete because the observable m is discrete, but
the allowed values become closely spaced for large κ. For small σǫ/ǫ (i.e. for large m),
these probabilities approximate to a narrow Gaussian (see Fig. 2).

Given our choice of probability in eqn. (15), the likelihood for the parameter ǫ,

given measured ǫ0, is

ǫ0) = e−κǫκmǫm/m!

(ǫ
|

L

This is the same function of ǫ and ǫ0 as eqn. (15), but now m is regarded as ﬁxed, and
ǫ is the variable. The likelihood is a continuous function of ǫ. It is compared with a
Gaussian in Fig. 3.

8Here we deﬁne ǫ0 and σ2

ǫ as the mean and variance of the posterior p.d.f. of eqn. (17).

8

(13)

(14)

(15)

(16)

Finally in the Bayes approach, with the choice of a constant prior for ǫ, the posterior

probability density for ǫ after our subsidiary measurement is

m)

p(ǫ
|

∝

e−κǫκmǫm/m!

(17)

which is obtained by multiplying the right-hand side of eqn. (16) by unity. This pos-
terior probability density for ǫ will be used as our prior for ǫ in the next step of
deducing the limit for s.

4 Results

The details of the necessary analytical calculations9 are presented in the Appendix of
this note.
In this section we investigate the behavior of the Bayesian limits in this
example, especially the shape of the frequentist coverage probability as a function of
strue.

4.1 Shape of the posterior

The posterior p.d.f. for s has the form

p(s

b, n)ds

|

∝

0

(cid:20)Z

∞

e−(ǫs+b)(ǫs + b)n
n!

κ(κǫ)me−κǫ
Γ(m + 1)

dǫ

1 ds

(cid:21)

(18)

where the likelihood, the prior for ǫ, the (constant) prior for s, and the marginalization
integral over ǫ are all prominently displayed.

The posterior probability density for s gives the complete summary of the outcome
of the measurement in the Bayesian approach. It is therefore important to understand
its shape before proceeding to use it to compute a limit (or extract a central value and
error-bars).

Figure 4 illustrates the shape of the posterior for s (i.e. marginalized over ǫ) in the
case of a nominal 10% uncertainty on ǫ, and an expectation of 3 background events.
Plots are shown for 1, 3, 5, and 10 observed events. The posterior evolves gracefully
from being strongly peaked at s = 0 to a roughly Gaussian shape that excludes the
neighborhood near s = 0 with high probability. Technically, the posterior would be
described as a mixture of n + 1 Beta distributions of the 2nd kind10, giving it a tail at
high s that is heavier than that of a Gaussian.

4.2 Upper limits

In this note, our main goal is to obtain a Bayesian upper limit su from our observation
of n events. It is by integrating the posterior p.d.f. out to s = su that an upper limit is
calculated: a β = 90% upper limit is deﬁned so that the integral of the posterior from

9This example can be handled analytically. More complicated cases might require numerical inte-

gration, which can be done via numerical quadrature or Monte Carlo methods.

10The 2nd Beta distribution is also known as “Beta′” (i.e. “Beta prime”), “inverted Beta”, “Pearson

Type VI”, “Variance-Ratio”, “Gamma-Gamma”, “F”, “Snedecor”, “Fisher-Snedecor”. . . .

9

s = 0 to s = su is 0.9. The probability (in the Bayesian sense) of strue < su is then
exactly β.

Table 1 shows the upper limits (β = 0.9) for n = 0–20 observed events with b = 0–8
0.1. (Integer values of b are chosen for illustration purposes only; b can,

and ǫ = 1.0
of course, take any real value

±

0.)

≥

One notices that when n = 0, the limit is independent of the expected background b.
This is required in the Bayesian approach: we know that exactly zero background events
were produced (when no events at all were produced), and this knowledge of what did
happen makes what might have happened superﬂuous. An interesting corollary is, in
the case of no events observed, uncertainties in estimating the background rate are
of no consequence in the Bayesian approach, and must not contribute any systematic
uncertainty to the limit. This reasoning does not hold in the frequentist framework,
where what might have happened deﬁnitely does inﬂuence the limit.

For comparison, limits for ﬁxed ǫ = 1 with b = 0 or b = 3 are also shown in Table 1.
It is interesting that these two columns start out equal at n = 0 and diﬀer by almost
exactly 3 for n > 11. In contrast, the diﬀerence between the b = 0 and b = 3 columns
0.1 is already greater than 3 at n = 6, and continues to grow as n
for ǫ = 1.0
increases; it is not clear whether the diﬀerence approaches a ﬁnite value as n
. In
any case, the limits for ǫ = 1 exactly are all smaller than the corresponding limits for
ǫ = 1.0

0.1, as expected.

→ ∞

±

±

4.3 Coverage

The main quantity of interest in this subsection is the frequentist coverage probability
C as a function of strue (for ﬁxed ǫtrue and b). Because both the main and the subsidiary
measurements involve observing a discrete number of events, the function C(strue) will
have many discontinuities. On the other hand, C(ǫtrue) will be continuous (for ﬁxed
strue). The explanation of this eﬀect is as follows:

The measured data are n events in the main measurement and m events in the
subsidiary measurement. For each observed outcome (n, m) there is a limit su(n, m).
This limit includes the eﬀect of marginalization over ǫ.

All (n, m) with n

0 are possible, and the probability P of observing
(n, m) can be calculated as the product of two Poissons. (It will depend on strue, ǫtrue,
. . . ) If we look at all the possible limits we can obtain,

0 and m

≥

≥

n
su(n, m)
|

0 and m

0

≥
and sort them in increasing su, the su are countably inﬁnite in number and dense in the
same way that rational numbers are dense in the reals.

≥

{

}

To compute the coverage as a function of strue, we simply add up all the probabilities

(19)

of obtaining (n, m) with su(n, m)

strue:

≥

C =

P (n, m)

X(n,m)∈A

=

strue ≤
(n, m)
|

{

A

su(n, m) and n

0 and m

≥

0

}

≥

(20)

This sum is over a countably inﬁnite number of terms. If we increase strue slightly to
strue + ds and recalculate the coverage, we have to drop all the terms

strue ≤
(n, m)
|

{

su(n, m)

strue + ds

≤

}

(21)

10

from the previous sum (the P (n, m) for each term also changes continuously with strue,
but this is no problem). If there are M > 0 such terms, there are M discontinuities in
the coverage in the interval [strue, strue + ds], since P (n, m) for each of these is ﬁnite,
and we lose them one by one as we sweep across the interval [strue, strue + ds].
But it seems that, in general, we can always ﬁnd a solution to strue ≤

≤
strue + ds for ﬁnite ds by going out to larger and larger n and m. So, although the
discontinuity may be tiny, we can always ﬁnd a ﬁnite discontinuity in any ﬁnite interval
of strue.

su(n, m)

On the other hand, if we keep strue ﬁxed and vary ǫtrue, we always sum over the same
does not involve ǫtrue, and P (n, m) is continuous

set of (n, m), since the deﬁnition of
A
in ǫtrue. So the coverage is continuous as a function of ǫtrue for strue ﬁxed.

Plotting a curve that is discontinuous at every point is somewhat problematical.
The solution adopted here is to plot the coverage as straight line segments between
< 10−4. Figure 5 shows
the discontinuities, ignoring any discontinuities with
C(strue) for the case β = 90%, ǫtrue = 1, nominal 10% uncertainty of the subsidiary
measurement of ǫ, and b = 3. We observe that C(strue) > β in this range, and it is
not clear numerically whether C(strue)
. The same conclusions hold
for Fig. 6, which illustrates the same situation with a 20% nominal uncertainty for the
ǫ-measurement.

β as strue → ∞

∆C

→

|

|

Figure 7 shows C(ǫtrue) for β = 90%, strue = 10, κ = 100, and b = 3—continuous
as advertised. The shape of the curve is quite similar to that of Figs. 5 and 6, so it
seems that the coverage probability (with b ﬁxed) is approximately a function of just
the product of ǫtrue and strue. This approximate rule is likely to fail in the limit as
, for example, but it seems to hold when ǫtrue and strue are at
ǫtrue →
least of the same order of magnitude.

0 and strue → ∞

When ǫtruestrue is small, of order 1 or less, the coverage is

100%, as in the simple
case of Fig. 1. Otherwise, the behavior of coverage in Figs. 5–7 is superior to that of
Fig. 1, which has a much larger amplitude of oscillation.

∼

Another frequentist quantity that characterizes the performance of a limit scheme
is the sensitivity, deﬁned as the mean of su. Figure 8 shows the sensitivity as a function
is observed to be nearly linearly dependent on strue.
of strue for the case of Fig. 5;
There is one complication here: when the subsidiary measurement observes m = 0
events, and the prior for s is ﬂat, su =
. Since the Poisson probability of obtaining
is consequently inﬁnite. So we must exclude the m = 0
sui
m = 0 is always ﬁnite,
(In Fig. 8 the probability of obtaining m = 0 is
sui
case from the deﬁnition of
e−100

10−44.)

sui

∞

4

h

h

h

.

≃

×

4.4 Other priors for s

A weakness of the Bayesian approach is that there is no universally accepted method
to obtain a unique “non-informative” or “objective” prior p.d.f. Reference [9], for
example, states:

Put bluntly: data cannot ever speak entirely for themselves; every prior
speciﬁcation has some informative posterior or predictive implications; and
“vague” is itself much too vague an idea to be useful. There is no “objective”
prior that represents ignorance.

11

Nevertheless, Ref. [9] does derive a 1/√s “reference prior” for the simple Poisson case,
which is claimed to have “a minimal eﬀect, relative to the data, on the ﬁnal inference”.
This is to be considered a “default option when there are insuﬃcient resources for
detailed elicitation of actual prior knowledge”.

Reference [10] attempts to discover the optimal form for prior ignorance by con-
sidering the behavior of the prior under reparameterizations. For the case in question,
the form 1/s clearly has the best properties in this respect.

We are using an ﬂat (s0) prior for this study, which seems to be the most popular
choice, but the Appendix works out the form of the posterior using an sα−1 prior, so
we can brieﬂy here summarize the results for the 1/s and 1/√s cases:

The 1/s prior leads to an unnormalizable posterior for all observed n when b > 0.
The posterior becomes a δ-function at s = 0, su = 0 for any β, and the coverage is
consequently zero for all strue > 0. This clearly is a disaster.

The 1/√s prior results in a posterior p.d.f. qualitatively similar in shape to those
of Fig 4, except that the p.d.f. is always inﬁnite at s = 0. For n
b, this produces an
extremely thin “spike” at s = 0, which has a negligible contribution to the integral of
the posterior p.d.f. A more signiﬁcant diﬀerence (for frequentists) between the 1/√s
and the s0 case is that the coverage probability is signiﬁcantly reduced: for the case
0.87. So the 1/√s
of Fig. 5 the 1/√s prior pushes the minimum coverage down to
prior leads to violation of the frequentist coverage requirement; it undercovers for some
values of strue.

≫

∼

One might also seek to further improve the coverage properties by adopting an
intermediate prior. For example, an s−0.25 prior would reduce the level of overcoverage
obtained with the s0 prior. How acceptable this approach would be within the Bayesian
Statistical community is an interesting question.

It should be noted that all the prior p.d.f.’s considered in this note are “improper
priors”—they cannot be correctly normalized: In the case of the s0 and 1/√s priors,
the integral from 0 to any value s0 is ﬁnite, while the integral from s0 to inﬁnity is
inﬁnite. The corresponding integrals of the 1/s prior are inﬁnite on both sides for all
s0 > 0.
Improper priors are dangerous but often useful; “improper posteriors” are
generally pathological. Extra care must be taken when employing improper priors to
verify the normalizability of the resulting posterior—when using a numerical method
to obtain the posterior, it is very easy to miss the fact that its integral is inﬁnite.

4.5 Restrictions

We summarize here the restrictions forced on the priors for s and ǫ—see the Appendix
for the analytical causes. The discussion below assumes b > 0. The prior for s being
of the form sα−1, we must require α > 0, as discussed above.

As speciﬁed in this note, the prior for ǫ, being taken from the posterior from the
subsidiary measurement with a ﬂat prior, has been given no freedom. Should the
subsidiary measurement observe m = 0 events, the posterior for s is not normalizable
when α

1.
This behavior is due to a well known eﬀect: the ǫ prior becomes κe−κǫ when m = 0,

when m = 0 and α

1: su =

∞

≥

≥

12

0. All such cases11 yield su =
which remains ﬁnite as ǫ
1; any positive
α < 1 cuts oﬀ the posterior at large s suﬃciently rapidly to render it normalizable.
From this point of view, a 1/√s prior may seem preferable, but on the other hand,
having su =
for m

when m = 0 seems intuitively reasonable. (In general, we have su =

2 are not popular choices.)

∞
1, but α

when α

→

∞

∞

≥

α

There is another approach possible to the gamma prior for ǫ: we may simply specify

≤

−

≥

by ﬁat the form of the prior as

µ)dǫ =

p(ǫ
|

κ(κǫ)µ−1e−κǫ
Γ(µ)

dǫ

(22)

where µ is no longer required to be an integer.
In practice, one then might obtain
µ and κ from a subsidiary measurement whose result is approximated by the gamma
distribution. In such cases, one must require µ > α to keep the posterior normalizable.
1)/κ is the mode, and µ/κ2
Note that in this form, µ/κ is the mean of the ǫ prior, (µ
is the variance. The subsidiary measurement is often analysed by other experimenters,
who chose statistics to quote for their central value and uncertainty (omitting additional
likelihood information). It is then important to obtain µ and κ in a consistent way
from the information supplied by the subsidiary measurement. If ǫ, for example, were
estimated by a maximum likelihood method, one would identify the estimate with
(µ

1)/κ rather than µ/κ.

−

−

5 Conclusions

Results have been presented on the performance of a purely Bayesian approach to the
issue of setting upper limits on the rate of a process, when n events have been observed
in a situation where the expected background is b and where the eﬃciency/acceptance
factor ǫ
σǫ has been determined in a subsidiary experiment. We ﬁnd that this
approach, when using a ﬂat prior for the rate, results in modest overcoverage. Plots of
the expected sensitivity of such a measurement and of the coverage of the upper limits
are given. It will be interesting to compare these with the corresponding plots for other
methods of extracting upper limits, to be given in future notes. Reference [11] provides
the limit calculating software associated with this study in the form of C functions.

±

6 Appendix A—Analytical Details

Here we present the details of the analytical calculation of the posterior p.d.f. for s.
For generality, we work through the calculation with a sα−1 prior; a ﬂat prior is then
the special case α = 1.

6.1 Posterior for s with ǫ and b ﬁxed

We measure n events from a process with Poisson rate ǫs+b, and we want the Bayesian
posterior for s, given improper prior sα−1. We compute the posterior for ﬁxed ǫ and b

11A Gaussian truncated at ǫ = 0 is the standard example.

13

in this subsection; the calculation with our prior for ǫ follows in the next subsection.
We have

posterior: p(s

ǫ, b, n)ds =

e−ǫs(ǫs + b)nsα−1ds

|

1

Ns

where all factors not depending on s have already been absorbed into the normalization
constant

Ns, which is deﬁned by

∞

Ns =

0

Z

∞

bn+α
ǫα

0
Z

e−ǫs(ǫs + b)nsα−1ds =

e−buuα−1(1 + u)ndu

(u = sǫ/b)

where we have performed the indicated change of variable.

Expanding (1 + u)n in powers of u using the binomial theorem, we get

(1 + u)n = n!

n

un−k

(n

k)!k!

−

Xk=0

⇒

Ns = n!ǫ−α

n

Xk=0

Γ(α + n
(n

−
k)!k!

k)bk

−

Recognizing this as of the general hypergeometric form, we write it as

Ns = ǫ−αΓ(α + n)

1 +

(cid:20)

n
α + n

b
1!

1

+

−

(α + n

n(n

1)
1)(α + n

−

−

2)

−

b2
2!

+

· · ·

(cid:21)

to make the hypergeometric nature more explicit. Using the modern notation[12] for
the falling factorial

zk

Γ(z + 1)

≡

Γ(z

k + 1)

−

= z(z

1)(z

2)

(z

k + 1)

−

−

· · ·

−

this is expressed as

Ns = ǫ−αΓ(α + n)

n

Xk=0

nk
(α + n

bk
k!

1)k

−

= ǫ−αΓ(α + n)M(

n, 1

n

α, b)

−

−

−

n, 1

where M is the notation of [13]. (M, a conﬂuent hypergeometric function, is often
written 1F1, and the relation given here is only valid for integer n
0.) Note that
α, b) is a polynomial of order n in b (for n a non-negative integer), and
M(
n, b), which is
is related to the Laguerre polynomials. When α = 1, we get M(
−
related to the Incomplete Gamma Function. When α = 0, we get M(
n, b),
which is inﬁnite, so we require that α > 0.

n, 1

n,

−

−

−

−

−

−

≥

n

Our posterior probability density for ﬁxed ǫ is then given by

p(s

ǫ, b, n)ds =

|

ǫαe−ǫs(ǫs + b)nsα−1
n, 1

n

Γ(α + n)M(

−

−

−

ds

α, b)

6.2 Posterior for ǫ of the subsidiary measurement

The subsidiary measurement observes an integer number of events m, Poisson dis-
tributed as:

P (m
|

ǫ) =

e−κǫ(κǫ)m
m!

14

where κ is a real number (connecting the subsidiary measurement to the main measure-
ment) whose uncertainty is negligible, so κ can safely be treated as a ﬁxed constant. κ
might be thought of, for example, as based on a cross section that is exactly calculable
by theory. There is negligible (i.e. zero) background in the subsidiary measurement.
The prior for ǫ is speciﬁed to be ﬂat. The Bayesian posterior p.d.f. for ǫ is then

m) =

p(ǫ
|

κ(κǫ)me−κǫ
m!

(or Γ(m+1) instead of m! in the denominator if you prefer). This is known as a gamma
distribution.

The mean and rms of this posterior p.d.f. summarize the result of the subsidiary

measurement as:

m + 1

ǫ =

κ ±

√m + 1
κ

= ǫ0 ±

σǫ

Note that the observed data quantity in the subsidiary measurement is an integer m,
while the quantity being measured by the subsidiary measurement is a positive real
number ǫ.

6.3 Posterior for s with gamma prior for ǫ (b ﬁxed)

b, n)dsdǫ using the sα−1 prior for s and our
Next we compute the joint posterior p(s, ǫ
|
gamma distribution prior (i.e. the posterior derived from the subsidiary measurement)
for ǫ

0

≥

prior for ǫ: π(ǫ)dǫ =

µ = m + 1 = (ǫ0/σǫ)2

κ = ǫ0/σǫ

2

(κǫ)µe−κǫ
Γ(µ)

dǫ
ǫ

where it is convenient to write µ for m + 1. We have for the joint posterior p.d.f.

where

We calculated

p(s, ǫ
|

b, n)dsdǫ =

1

Ns,ǫ

π(ǫ)e−ǫs(ǫs + b)nsα−1dsdǫ

∞

∞
π(ǫ)e−ǫs(ǫs + b)nsα−1dsdǫ =

∞

Ns,ǫ =
0 Z
0
Z
Ns above, so we have
Ns,ǫ = Γ(α + n)M(
Ns,ǫ = καΓ(µ
b, n)dsdǫ =

−

p(s, ǫ
|

π(ǫ)

Nsdǫ

0

Z

∞
ǫ−απ(ǫ)dǫ

n, 1

n

α, b)

−

−

−

0
Z
n

α)Γ(α + n)M(

n, 1
κµ−αǫµ−1sα−1(ǫs + b)ne−(s+κ)ǫ

−

−

−

α, b)/Γ(µ)

Γ(µ

α)Γ(α + n)M(

n, 1

n

α, b)

−

−

−

dsdǫ

The marginalized posterior for s can then be expressed as

p(s

b, n)ds =

|

∞

p(s, ǫ
|

0
(cid:20)Z

b, n)dǫ

ds =

sα−1κµ−α
α)Γ(α + n)M(

Iǫ
n, 1
−

ds

n

α, b)

−

−

−

(cid:21)

Γ(µ

−

15

where the integral

Iǫ is given by
Iǫ =

∞

0

Z

ǫµ−1e−(s+κ)ǫ(ǫs + b)ndǫ

The same procedure that was used for the normalization integral can be applied here,
producing

∞

Iǫ =

bµ+n
sµ
snn!
(s + κ)µ+n

Z

0

Iǫ =

sn

uµ−1e−b(1+κ/s)u(1 + u)ndu

n

Γ(µ + n

k)

−
k)!k!

(n

−

(cid:20)

b(s + κ)
s

k

(cid:21)

Xk=0

Iǫ =

(s + κ)µ+n Γ(µ + n)M(

−

n, 1

µ, b(s + κ)/s)

−
M(

n

−
n, 1
M(

−

p(s

b, n)ds =

|

Γ(µ + n)

Γ(µ

α)Γ(α + n)

−

sα+n−1κµ−α
(s + κ)µ+n

n
−
n, 1

−
−

−

µ, b(s + κ)/s)
α, b)
n

ds

−

which has a particularly simple form when the background term is zero:

p(s

b = 0, n)ds =

|

Γ(µ + n)

Γ(µ

α)Γ(α + n)

−

sα+n−1κµ−α
(s + κ)µ+n ds

a Beta distribution of the 2nd kind. Note that we must require µ > α > 0 to obtain a
normalizable posterior.

Our posterior p.d.f. for s with ǫ (and b) ﬁxed is recovered exactly by taking the
limit of p(s
0 is identical to
the value of su when ǫ is known exactly. This property may seem obvious, but it is
violated by some frequentist methods of setting limits, so it is worth mentioning.

0. This means that the limit of su as σǫ →

b, n) as σǫ →

|

6.4 Calculating the limit

We need to integrate p(s
follows.
su

|

Γ(µ + n)

p(s

b, n)ds =

0

Γ(µ

α)Γ(α + n)

Z
where the substitution t = s
and integrating term by term yields

−

0
Z

b, n) up to some limit su, which can be done analytically as

su
su+κ

tα+n−1(1

t)µ−α−1 M(
−
M(
−

n, 1
n, 1

n
−
n
−

−
−

µ, b/t)
α, b)

dt

−

s+κ has been performed. Re-expanding the polynomial M

su

p(s

b, n)ds =

n

Ix(α + n

k, µ

−
(α + n

−
1)k

α)nk

n

bk
k!,

nk
(α + n

bk
k!

1)k

0
Z
−
where Ix is the standard notation for the Incomplete Beta Function

Xk=0

Xk=0

−

x =

(cid:18)

su
su + κ

(cid:19)

|

|

Ix(q, r)

Γ(q + r)
Γ(q)Γ(r)

≡

x

0

Z

tq−1(1

t)r−1dt

−

which also satisﬁes the following recursion:

Ix(q, r) =

Γ(q + r)
Γ(q + 1)Γ(r)

xq(1

x)r + Ix(q + 1, r)

−

16

6.5

Integer moments of the marginalized posterior

Using the same technique as above, we can calculate the jth moment of the posterior
p.d.f. as

∞

sj

h

i

=

0
Z

sjp(s

b, n)ds =

|

(α + n)jκj
1)j
α
(µ

−

−

M(

n, 1

n

α

j, b)

−
M(

−
n, 1

−
n

−
α, b)

−

−

−

where we utilize the rising factorial notation[12]

zk

≡

Γ(z + k)
Γ(z)

= z(z + 1)(z + 2)

(z + k

1)

· · ·

−

The expression for the mean of the posterior when α = 1 can be simpliﬁed using

the identity

obtaining

M(

n,

n

−

−

−

1, b) =

1
(cid:18)

−

b
n + 1

(cid:19)

M(

n,

n, b) +

−

−

bn+1
(n + 1)!

mean(α = 1) =

s

i|α=1 =

h

κ(n + 1
µ

−
2

b)

+

−
Note that the 2nd term is very small when n

b.

≫

The recurrence relation[13]

κbn+1

(µ

2)n!M(

n,

n, b)

−

−

−

r(r

1)M(q, r

1, z) + r(1

r

z)M(q, r, z) + z(r

q)M(q, r + 1, z) = 0

−

−

−

−

−

leads to a recurrence relation between moments

sj

h

i

=

κ(α + n + j
α
µ

−

b)

−

1
−
j
−

sj−1

+

h

i

(µ

κ2b(α + j
α

−
j + 1)(µ

2)

sj−2

i

α

−

−

j) h

−

−

The special case α = 1 then yields

s2

h

i|α=1 =

(µ

κ2
2)(µ

(cid:20)
which leads to this approximation for the variance of the posterior

−

−

(2 + n

b)(1 + n

b) + b +

3)

−

−

(2 + n
n!M(

b)bn+1
n, b)

−
n,

−

−

(cid:21)

variance(α = 1)

κ2(1 + n)
2)(µ

−

3)

−

+

κ2(1 + n
(µ

−
2)2(µ

b)2
3)

−

−

≃

(µ

(n

b)

≫

6.6 Posterior for s with gamma priors for ǫ and b

Here we very brieﬂy consider the case where the background parameter b also acquires
an uncertainty. This case is more general than the ﬁxed b case that is the main subject
of this note: the ﬁxed b case will be the subject of additional studies employing various
popular frequentist techniques, with the goal of comparing their performance. We

17

judge the more general case considered in this subsection to be more complicated than
necessary for the purpose of comparing the various methods, but it is instructive to
document the fact that the Bayesian method can easily handle the more general case.
We assume a 2nd subsidiary measurement observing r events (Poisson, as was the
case for ǫ), which, when combined with a ﬂat prior for b, results in a gamma posterior
for b of the form

r)db =

p(b
|

ω(ωb)re−ωb
r!

db

where ω is a calibration constant (analogous to κ in the subsidiary measurement for ǫ).
The posterior for b becomes the prior for b in the measurement of s. After deter-
n) by using our priors for s, ǫ and b, we marginalize

mining the joint posterior p(s, ǫ, b
|
with respect to ǫ and b, resulting in

p(s

n)ds =

|

Γ(µ + n)

Γ(µ

α)Γ(α + n)

−

sα+n−1κµ−α
(s + κ)µ+n

F (

−

n, ρ; 1
F (

n
−
n, ρ; 1

−

µ; (s + κ)/(sω))
α; 1/ω)
n

ds

−
−

−

where we write ρ = r + 1 for convenience, and F is the hypergeometric function[14].
As long as n is a non-negative integer and α > 0, F (
α; x) is a polynomial
of order n in x (closely related to Jacobi polynomials).

n, ρ; 1

−

−

−

n

This marginalized posterior for s can then be integrated, with the result

su

0
Z

p(s

n)ds =

|

n

Xk=0

Ix(α + n

k, µ

α)nkρk

−
(α + n

−
1)k

−

ω−k
k! ,

n

Xk=0

nkρk

(α + n

1)k

−

ω−k
k!

x =

(cid:18)

su
su + κ

(cid:19)

These two equations closely resemble the main results of sections 6.3 and 6.4: to
.

recover the ﬁxed b results, simply substitute bω for ρ above, and take the limit ω

→ ∞

7 Appendix B—Average Coverage Theorem

In this appendix we prove that Bayesian credible intervals have average frequentist
coverage, where the average is calculated with respect to the prior density. We start
from the Bayesian posterior density:

p(s

n) =

|

P (n
∞
0 P (n

|
|

s) π(s)
s) π(s) ds

.

For a given observed value of n, a credibility-β Bayesian interval for s is any interval
[sL(n), sU(n)] that encloses a fraction β of the total area under the posterior density.
Such an interval must therefore satisfy:

R

(23)

(24)

β =

sU(n)

sL(n)

Z

p(s

n) ds,

|

or, using the deﬁnition of the posterior density:

P (n

s) π(s) ds = β

P (n

s) π(s) ds.

(25)

sU(n)

sL(n)

Z

|

∞

0

Z

|

18

Now for coverage. Given a true value st of s, the coverage C(st) of [sL(n), sU(n)] is the
frequentist probability that st is included in that interval. We can write this as:

C(st) =

P (n

st).

|

Xall n such that:
sL(n)≤s≤sU(n)

(26)

Next we calculate the average coverage C, weighted by the prior π(s):

C =

C(s) π(s) ds,

P (n

s) π(s) ds,

using equation (26),

Xall n such that:
sL(n)≤s≤sU(n)

|

P (n

s) π(s) ds,

interchanging integral and sum,12

= β

P (n

s) π(s) ds,

using equation (25),

= β

P (n

s) π(s) ds,

interchanging sum and integral,

∞

∞

0

Z

0

Z

=

=

|

|

|

∞

sU(n)

sL(n)

n=0 Z
X

∞

∞

0

n=0 Z
X

∞

∞

0

Z

n=0
X

∞

0

Z

= β

π(s) ds,

by the normalization of P (n

s),

|

= β,

by the normalization of π(s).

This completes the proof. We have assumed here that the prior π(s) is proper and
normalized to 1, but the proof can be generalized to improper priors such as those we
considered in this note. A constant prior for example, can be regarded as the limit for
smax → ∞

of the proper prior:

π(s

smax) =

|

ϑ(smax −
smax

s)

,

where ϑ(x) is 0 if x < 0 and 1 otherwise. We then deﬁne the average coverage for the
constant prior as the limit:

C =

lim
smax→ +∞

C(s) π(s

smax) ds.

|

12The best way to understand this step is to draw a diagram of s versus n: one is integrating and
summing over the area between the curves sL(n) and sU(n). The limits on the sum and integral
depend on the order in which one does these operations and can be derived from the diagram.

(27)

(28)

∞

0
Z

19

The previous proof can now be applied to the argument of the limit and leads to the
same result.

The average coverage theorem remains valid when s is multidimensional, for exam-
ple when it consists of a parameter of interest and one or more nuisance parameters.
In that case one needs to average the coverage over all the parameters.

20

References

[1] M. J. Bayarri and J. O. Berger, “The Interplay of Bayesian and Frequentist Anal-

ysis”, Statistical Science 19, p 58 (2004),
projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1089808273,
www.isds.duke.edu/~berger/papers/interplay.html.

[2] Giovanni Punzi, “Example of Bayesian intervals with zero coverage”, CDF Internal

Note 6689 (2001),
www-cdf.fnal.gov/publications/cdf6689_Bayes_zero_coverage.pdf.

[3] Peter Cliﬀord, “Interval estimation as viewed from the world of mathematical
statistics”, CERN Yellow Report CERN 2000-005, p 157 (2000), Proceedings of
the Workshop on Conﬁdence Limits at CERN, 17–18 January 2000, edited by
L. Lyons, Y. Perrin, and F. James,
doc.cern.ch/yellowrep/2000/2000-005/p157.pdf.

[4] Giovanni Punzi,

“A stronger classical deﬁnition of Conﬁdence Limits”,

hep-ex/9912048, www.arxiv.org/abs/hep-ex/9912048.

[5] G¨unter Zech, “Confronting classical and Bayesian conﬁdence limits to examples”,
CERN Yellow Report CERN 2000-005, p 141 (2000), Proceedings of the Work-
shop on Conﬁdence Limits at CERN, 17–18 January 2000, edited by L. Lyons,
Y. Perrin, and F. James, doc.cern.ch/yellowrep/2000/2000-005/p141.pdf.

[6] D. Karlen, “Credibility of conﬁdence intervals”, in Proceedings of the Conference
on Advanced Techniques in Particle Physics, Durham, 18–22 March 2002, edited
by M. Whalley and L. Lyons, p 53, (2002),
www.ippp.dur.ac.uk/Workshops/02/statistics/proceedings/karlen.pdf.

[7] R. D. Cousins and V. L. Highland, “Incorporating systematic uncertainties into

an upper limit”, Nucl. Instrum. Meth. A 320, p 331 (1992).

[8] Gary Feldman, “Multiple measurements and parameters in the uniﬁed approach”,

Fermilab Workshop on Conﬁdence Limits 27–28 March, 2000, p 11,
conferences.fnal.gov/cl2k/copies/feldman2.pdf,
huhepl.harvard.edu/~feldman/CL2k.pdf.

[9] J. M. Bernardo and A. F. M. Smith, “Bayesian Theory”, (John Wiley and Sons,

[10] Harold Jeﬀreys, “Theory of Probability”, 3rd ed., (Oxford University Press, Ox-

Chichester, UK, 1993),

5.4 and
§

A.2.
§

ford, 1961),

3.1.
§

[11] Joel Heinrich, “User Guide to Bayesian-Limit Software Package”, CDF Internal

Note 7232, (2004),
www-cdf.fnal.gov/publications/cdf7232_blimitguide.pdf;
CDF Statistics Committee Software Page,
www-cdf.fnal.gov/physics/statistics/statistics_software.html.

21

[12] R. L. Graham, D. E. Knuth, and O. Patashnik, “Concrete Mathematics: A Foun-
dation for Computer Science”, 2nd ed., (Addison-Wesley, Reading, MA, 1994);
PlanetMath Mathematics Encyclopedia,
planetmath.org/encyclopedia/FallingFactorial.html.

[13] M. Abramowitz and I.A. Stegun, editors, “Handbook of Mathematical Functions”,
(United States Department of Commerce, National Bureau of Standards, Wash-
ington, D.C. 1964; and Dover Publications, New York, 1968), chapter 13.

[14] M. Abramowitz and I.A. Stegun, ibid., chapter 15; William H. Press, et al., “Nu-
merical Recipes”, 2nd edition, (Cambridge University Press, Cambridge, 1992),
6.12, lib-www.lanl.gov/numerical/bookcpdf/c5-14.pdf,
5.14 and
§
§
lib-www.lanl.gov/numerical/bookcpdf/c6-12.pdf.

22

1

C

0

0

strue

20

Figure 1: Coverage as a function of the true signal rate s for Bayes 90% limits, for the
simple case of no background and no uncertainty on ǫ = 1. The dotted line at C = 0.9
is given to show that the coverage never falls below 90% (in this simple case).

23

0.7

0.8

0.9

1.0

1.1

1.2

1.3

ε0

Figure 2: Comparison of our discrete probability for ǫ0 (shown as a histogram, see
eqn. (15)) and Gaussian (continuous curve) for the case ǫ = 1.0

0.1.

±

24

0.7

0.8

0.9

1.0

1.1

1.2

1.3

ε

Figure 3: Comparison of our likelihood (dashed, see eqn. (16)) and Gaussian (solid)
for the case ǫ = 1.0

0.1.

±

25

n=1     ε=1.0±0.1     b=3     

n=3     ε=1.0±0.1     b=3     

n=5     ε=1.0±0.1     b=3     

n=10     ε=1.0±0.1     b=3     

0

0

s

s

s

s

20

20

Figure 4: Posterior densities p(s
ǫ = 1.0

0.1 (i.e. κ = 100 and m=99).

|

±

b, n) vs s for n = 1, 3, 5, 10. In each case, b = 3 and

20

0

20

0

26

β=0.90     εtrue=1     κ=100     b=3     

1.00

C

0.90
0

strue

20

Figure 5: Coverage of 90% upper limits as a function of strue for ǫtrue = 1, nominal 10%
uncertainty of the subsidiary measurement of ǫ, and expected background b = 3.

27

β=0.90     εtrue=1     κ=25     b=3     

1.00

C

0.90
0

strue

20

Figure 6: Coverage of 90% upper limits as a function of strue for ǫtrue = 1, nominal 20%
uncertainty of the subsidiary measurement of ǫ, and expected background b = 3.

28

β=0.90     strue=10     κ=100     b=3     

1.00

C

0.90
0

εtrue

2

Figure 7: Coverage of 90% upper limits as a function of ǫtrue for strue = 10, nominal
10% uncertainty of the subsidiary measurement of ǫ, and expected background b = 3.

29

β=0.90     εtrue=1     κ=100     b=3     

50

〈su〉

 0

0

strue

20

Figure 8: Sensitivity of 90% upper limits as a function of strue for ǫtrue = 1, nominal
10% uncertainty of the subsidiary measurement of ǫ, and expected background b = 3.
For reference, the sensitivity for σǫ = 0 is also given (dashed).

30


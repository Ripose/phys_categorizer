5
0
0
2
 
y
a
M
 
0
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
4
1
5
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Deriving the sampling errors of correlograms for general
white noise

T. D. Carozzi and A. M. Buckley
Space Science Centre, Sussex University, Brighton BN1 9QT, United Kingdom
T.Carozzi@sussex.ac.uk

20 May 2005

Abstract

We derive the second-order sampling properties of certain autocovariance and autocorrelation esti-
mators for sequences of independent and identically distributed samples. Speciﬁcally, the estimators we
consider are the classic lag windowed correlogram, the correlogram with subtracted sample mean, and
the ﬁxed-length summation correlogram. For each correlogram we derive explicit formulas for the bias,
covariance, mean square error and consistency for generalised higher-order white noise sequences. In
particular, this class of sequences may have non-zero means, be complexed valued and also includes
non-analytical noise signals. We ﬁnd that these commonly used correlograms exhibit lag dependent
covariance despite the fact that these processes are white and hence by deﬁnition do not depend on lag.

1 Introduction

Serial correlation, although not as popular as its Wiener-Khintchine theorem equivalent, spectral analysis,
is still of fundamental importance in the analysis of time series, and is used in a wide range of applications
ranging from radio astronomy [12] and radar scatter from random media [11] to DNA sequencing [4] and
wave-particle interaction instruments for space plasma research [8].

One of the most fundamental problems in time series analysis is to discern if the samples in a given
series are independent. Under stationary conditions, this problem is most naturally dealt with using the
autocorrelation sequence (ACS) of the series since it measures the correlation between samples in the
series which are separated by some interval of time known as the lag. In practice, we can only estimate
an ACS based on a ﬁnite number of data samples. So if we are to successfully detect the independence
of the samples we must know the sampling errors of the ACS estimator for independent and identically
distributed (IID) sample sequences, which are by deﬁnition independent. For our purposes of deriving the
sampling errors it turns out that we can in fact broaden slightly the class of sequences from IID sequences
to higher-order white noise or, as we will sometimes call it, general white noise.

It is of course the sampling errors or sampling properties that limit how well we can determine sample
independence from the estimated ACS and it is therefore an important issue to explicitly provide them for
general white noise sequences. General sampling properties of ACSs were ﬁrst studied in [2]. Despite
considerable subsequent attention, it seems the important special case of general IID sample sequences has
not been fully exhausted. For instance, the sampling properties of autocovariance estimates of complex
valued noise has not been published and neither has the case of autocorrelation estimates of nonzero mean
noise series. To be clear, the difference between what we call autocorrelation and autocovariance is that in
autocovariance the mean is explicitly subtracted from data samples while in autocorrelation it is not. Our
use of these terms conforms with [3]. We will use the term correlogram as a collective term for estimators
of autocovariance or autocorrelation functions when there is no need to distinguish them.

In this paper we derive the sampling properties of correlograms for complex valued higher-order white
noise with arbitrary mean. The speciﬁc correlograms we consider are the well known lag windowed au-
tocorrelation estimator, and the lag windowed autocovariance estimator in which the sample mean is sub-

1

tracted from the data. To contrast with the ubiquitous lag windowed correlograms, we also consider the
ﬁxed-length summation autocorrelation estimator.

The sampling properties of the estimators presented here are derived to second-order. This includes the

bias, the covariance and the mean-square-error (MSE) and also the variance and consistency.

2 Deﬁnitions and conventions

In this section we introduce some basic deﬁnitions and notational conventions.

In practice most correlograms are based on sequences of data samples Z[n] where n

Here we denote the sequence by putting a square bracket around the independent variable to distinguish
R where the independent variable is continuous. The correlogram of
them from functions such as Z(t) t
sequences will also be a sequence so we favor the terms autocorrelation sequence (ACS), denoted R[l] and
autocovariance sequence (ACVS), denoted C[l], over the often used terms autocorrelation function (ACF),
denoted R(l) and autocovariance function (ACVF), denoted C(l).

∈

Since we will be dealing with statistical estimators we will need to distinguish between ensemble, or
population, quantities and the estimates, or samples, of the quantities. We will put a hat accent on the
symbol used for the estimators of a quantity so, for instance, ˆR is an estimate of R.

The distinction between the ACS and the ACVS is as follows. We deﬁne the ACS as

Z =

∈

0,

1,

.

2, ...
}

±

±

{

while the ACVS is deﬁned

RZZ [l] := E

Z ∗[k]Z[k + l]
}
{

,

l, k

Z

∈

:= E

CZZ [l] := Cov

Z[k], Z[k + l]
}
{
is the expectation operator, Cov

Z ∗[k]Z[k + l]
{
is the covariance operator, l is the lag, and Z ∗[k] is the
where E
complex conjugate of Z[k]. Thus the difference between the ACS and ACVS is that, in the case of the
ACVS, the mean of the sequence is explicitly subtracted out, or removed, while it is not removed in ACS.
Note that C··, R··, and, Cov
, are all deﬁned so that their ﬁrst argument becomes complex conjugated,
this so that the default product between two unconjugated arguments will be a hermitian form.

Z[k + l]
}
{

Z ∗[k]
}
{

,
{·

,
{·

} −

{·}

·}

·}

E

E

The deﬁnition of the ACS varies in the literature with regards to the placement of the complex conjuga-
tion. Here, it is deﬁned in such a way that if Z[k] was the complex harmonic sequence exp(if k), the phase
if k) exp(if (k + l)) = exp(if l).
of its corresponding ACS would increase with increasing lag, viz. exp(

−

3 Generalised white noise

General white noise sequences, in the sense we will use here, are a less restrictive version of IID distributed
samples in which the cross-moments of any two different samples in the sequence are zero in the lowest
orders but may be non-zero at a higher order. This is different from an IID since all their cross-moments
are zero. It is also slightly more general that higher-order white noise since we allow for a non-zero mean
and we also allow for a broader structure of complex moments. The reason for considering these process is
driven entirely by the sampling error formulas themselves: the generalised white noise is the least restrictive
class of sequences with a ﬂat ACVS.

3.1 Deﬁning conditions of fourth-order general white noise

For the purposes of determining the second-order sampling properties of second-order lagged-moment
In what
estimates we need to know the statistical lagged-moments of the signal only to fourth-order.
follows, we introduce a general white noise sequence, which we denote ǫ[n], with the following deﬁning
properties: the ﬁrst-order lagged moment is

(1)

where µ is the mean.

E

ǫ[n]
}

{

= µ

2

The ACVS of ǫ[n] is zero for all non-zero lags,

Cǫǫ[l] = Cov

ǫ

[k], ǫ

[k + l]

= Cov

′

′

′

ǫ

′

∗[k], ǫ

∗[k + l]

= E

′

′

ǫ

∗[k]ǫ

[k + l]

= σ2δ0,l,

k, l

o

n

n

(2)
where σ2 is the (hermitian) variance and δa,b is the Kronecker delta function. δa,b is equal to one if a = b
and zero otherwise, hence δ0,l represents the zero lag, l = 0. Equation (2) expresses an important property
of white noise, that its autocovariance function is zero for all nonzero lags. It is precisely this property
which can be exploited of to test if a sequence is independent or not. Unfortunately as we shall see,
all autocovariance estimates will in practice be distributed around zero and this distribution is speciﬁed
according to the estimators sampling properties.

n

o

o

The ordinary ACVS for complex sequences is a hermitian bilinear form. Since we are dealing with
possibly complex valued sequences, the full description of the second-order properties requires also another
lagged bilinear form, namely the lagged quadratic form covariance

Z

∈

Cov

′

ǫ

∗[k], ǫ

′

[k + l]

= E

ǫ

[k]ǫ

[k + l]

= m2δ0,l = s2 exp(iθ2)δ0,l

′

′

(3)

n

o

n

o

where m2 is what we will call the quadratic variance. We call the left-hand side of (3) the quadratic ACVS
in contrast to the hermitian ACVS of the left-hand side of (2). An important property of the quadratic
ACVS is that it depends on whether the signal is analytic or not. It is identically zero for random stationary
analytical signals [3]. The quadratic ACVF has, to the best of the authors knowledge, not been explicitly
investigated in the literature. This is because it is usually argued that only analytical signals are of practical
importance. This is, however, does not mean the quadratic ACVS is without interest. In terms of the
second-order sampling properties derived here, the quadratic ACVF is precisely what distinguishes purely
real (non-imaginary) signals from complex signals. For purely real signals the quadratic ACVS is not zero,
it is equivalent to the hermitian ACVS.
The third-order lagged moment is

E

′

′

ǫ

∗[k]ǫ

[k + l]ǫ

′

[k′]

= κ3δkk′ δ0l

n

o

where κ3 is the third-order cumulant of the zero lag and is related to the skewness of distribution. Finally,
the fourth-order lagged moments of the white noise is

Cov

′

′

ǫ

∗[k]ǫ

′

′

[k + l], ǫ

∗[k′]ǫ

[k′ + l′]

σ4)δkk′ δ0lδ0l′ + σ4δkk′ δll′ (1
2σ4

n
=(µ4 −
=(µ4 −
=κ4δkk′ δ0lδ0l′ + σ4δkk′ δll′ + s4δk′,k+lδ−l,l′

2)δkk′ δ0lδ0l′ + σ4δkk′ δll′ +

m2|

o
−

− |

δ0lδ0l′ ) +

2δl,k′−kδ−l,l′ (1

m2|
|
2δl,k′−kδ−l,l′
m2|
|

−

δ0lδ0l′ )

2σ4
where µ4is the fourth-order central moment andκ4 = µ4 −
Fourth-order moments are related to the kurtosis of the distribution.

m2|

− |

2 is the fourth-order cumulant.

] which is given by a minimum of 5 low order cumu-
To summarise, we specify a noise sequence ǫ[
·
lants, namely, the mean, variance, complex variance, third- and fourth-order cumulants. This is the least
restrictive white noise speciﬁcation relevant to the second order sampling properties of correlograms.

3.2 Examples of higher-order white noise signals

] deﬁned in the previous section, we give now some explicit
To clarify the general white noise sequence ǫ[
·
examples. The ultimate example of a white noise signal is of course the Gaussian process with no memory.
This is because the central limit theorem shows that most processes under general conditions will tend to
Gaussian, and the no-memory aspect implies that the signal is white. The ﬁrst four cumulants of a complex

(4)

(5)

3

Gaussian random variable are

µ = µg
σ2 = σ2
g
s2 = 0
κ3 = 0
κ4 = 0






µ = λ
σ2 = λ
s2 = λ, θ2 = 0
κ3 = λ
κ4 = λ






complex Gaussian white noise ǫ

N(µg, σ2
g)

∼

(6)

Gaussian white noise is, however, not the only type of white noise. Other kinds of white noise are based
on non-Gaussian distributions. Such situations come about in practice if, for example, the values of the
physical quantity being measured are not continuous as assumed in the Gaussian case. Examples of this
are Poissonian and Bernoullian white noise.

A Poissonian white noise model is appropriate when the signal values are limited to non-negative
integers as, for instance, when the signal is a series of independent count values. The ﬁrst four cumulants
of the real Poissonian distribution, Po(λ), which specify the Poissonian white noise sequence, are

real Poissonian white noise ǫ

Po(λ)

(7)

∼

Notice how the third and fourth order cumulants κ3 and κ4 are nonzero in contrast to Gaussian white noise.
The reality of the signal comes from the fact that s2 = σ2 and θ2 = 0.

The Bernoullian white noise model is appropriate when the signal values are binary, for instance, a
0, 1
}
p). The ﬁrst four cumulants

sequence of independent yes-no decisions. We take the range of a Bernoulli random variable to be
and the probability of getting a 1 is p (so the probability of getting a 0 is 1
of the Bernoulli distribution, Be(p), are

−

{

−

p2 + p

µ = p
σ2 =
s2 = σ2, θ2 = 0
κ3 = 2p3
κ4 =

3p2 + p
6p4 + 12p3

−

−

7p2 + p

−






real Bernoullian white noise ǫ

Be(p)

(8)

∼

Again, the cumulants of order greater than 2 are not zero as opposed to Gaussian white noise and the signal
is real because the two second-order cumulants are equal. The estimation of ACF of one-bit sequences and
their sampling properties was discussed in [13].

A feature of both the Poissonian and Bernoullian noise is that they inherently have non-zero means.
Ultimately, for large enough mean values both the Poisson and Bernoulli distributions can be shown to
tend to Gaussian. But for ﬁnite mean values their cumulants are clearly incompatible with their Gaussian
counterparts.

4 Sampling properties of various correlograms for general white noise

There are many different types of correlograms. These can be subdivided, see [3], according to whether
the correlograms 1) explicitly involve an attempt to remove the mean of the signal, 2) correlate two dis-
tinct signals or the signal with itself, and 3) normalisation based on the data samples is applied. The ﬁrst
dichotomy can also be seen as whether the intention is to estimate correlation functions or covariance func-
tions. The second dichotomy is the distinction between cross- and auto-correlation respectively. The last
one includes correlation functions which are normalised by an estimate of the covariance of the sequence
and are sometimes known as coherence functions.

We will now look in detail at various correlograms which are either estimates of autocorrelation se-
quences or of autocovariance sequences. In particular we will examine the classical correlogram, the ﬁxed-
length correlogram, and the classical correlogram in which the sample mean is subtracted. Most standard

4

treatments allow the correlograms to be windowed arbitrarily according to a lag window w[l]. We however
use a slightly different quantity for weighting the correlogram lag estimates which we will simply call the
[l] which includes all normalisation factors. It is related to the lag window, which for
weight function
w[l] = 1 gives an unbiased correlogram estimate, as
). As we allow the signal to be
[l] = w[l]/(N
|
W
complex we use accordingly complexiﬁed correlograms throughout. As for the signal to be analysed, we
], as deﬁned in the
will assume that it consists of a ﬁnite number, N , of samples of general white noise, ǫ[
·
previous section.

l
− |

W

Our goal in this section is to derive the sampling properties of these correlograms up to second-order,
which fundamentally are the expectation and the covariance of the estimators. We also provide some related
sampling properties, such as the bias, the mean square error (MSE), asymptotic MSE for large number of
samples.

4.1 Lag window autocorrelation estimator

We start by considering the most fundamental and simplest of all correlograms, the lag window correlogram
R(lw)
Z ; see [10]. It was popularised by Blackman and Tukey [5]. Although it was originally expressed for
real signals, it is easily generalised to complex signals by requiring it to be a hermitian form. Thus, for
non-negative lags, we deﬁne the complex lag windowed correlogram as
b

R(lw)

Z [l] :=

[l]

W

Z ∗[k]Z[k + l]

l

0

≥

N −l

k=1
X

R(lw)

Z [l] :=

∗

R(lw)
l
Z [
|
(cid:16)

]
|

(cid:17)

l < 0

and the negative lags are just the complex conjugate of the correlogram with the opposite (positive) sign

b

R(lw)
Z

b

b

Usually

b
Z
{
R(lw)
Z

is used as an autocovariance estimator. This implies that the signal being analysed, Z, is
= 0. However, in what follows, we will not assume that the mean
assumed to have a zero mean, i.e. E
}
is zero, in other words, we see
as an autocorrelation estimator. Whether the data samples have a
non-zero mean intentionally or through an oversight is irrelevant. If the mean is not known apriori and one
wishes to remove it the usual method is to estimate the mean and subtract it from the data before estimating
the correlogram. In this paper we regard this process as a distinct correlogram and the most commonly
used example, the lag windowed autocovariance estimator, will be treated in section 4.3.

b

4.1.1 Expectation and bias

The expectation of

R(lw)
ǫ

[l] is

E

R(lw)
ǫ

[l]

=

b
[l]

W

n

b

o

=

[l]

W

N −l

k=1 (cid:16)
X
N −l

k=1 (cid:16)
X
N −l

n

n

E

′

′

ǫ

∗[k]ǫ

[k + l]

+ µE

′

ǫ

∗[k]

+ µ∗E

ǫ

[k + l]

+

o

o

n

n

o

o

′

′

n

n

2
µ
|
|

=

(cid:17)

o

o(cid:17)

E

′

′

ǫ

∗[k]ǫ

[k + l]

+ µE

′

ǫ

∗[k]

+ µ∗E

ǫ

[k + l]

+ (N

l)

−

W

2 =
µ
[l]
|
|

σ2δ0l + (N

l)

−

W

2 =
µ
[l]
|
|

=

[l]

W

= (N

k=1
X
l)

−

W

[l](σ2δ0l +

2)
µ
|
|

The bias is just the difference between the expectation and the true, population value. In this case it is
therefore, for all l,

Bias

R(lw)
ǫ

[l]

= E

R(lw)
ǫ

[l]

n

b

Rǫǫ[l] =

−
o
[l](σ2δ0l +
[l]

2)
µ
|
|
1)(σ2δ0l +

o

n
= (N

= ((N

b
−

l)

W
l)

−

W

−

(σ2δ0,l +
−
2),
µ
|
|

2) =
µ
|
|

5

(9)

(10)

(11)

(12)

Bias

R(lw)
ǫ

[l]

= ((N

l)

[l]

−

W

−

1)(σ2δ0l +

2),
µ
|
|

l =

N, ...

1, 0 + 1, ..., N.

(13)

−

−

n

b

o

that is,

4.1.2 Covariance

1
[l]
W
N −l′

W
N −l

[l′]

n

b
′
Z

n
Z

′

=

Cov

k=1
X
+

k′=1 (cid:16)
X
2
µ
|
|
+ Cov

(cid:16)

+ Cov

n
2
2,
µ
µ
|
|
|
|
(cid:8)
∗[k]Z
Z

′

′

The covariance of a correlogram is just the covariance of the correlogram estimate between any two lags,
say l and l′. The derivation of the covariance is as follows. For ll′

0,

≥

Cov

R(lw)
ǫ

[l],

R(lw)
ǫ

[l′]

=

∗[k]Z

′

b
[k + l], Z

o
′
∗[k′]Z

′

Cov

′

∗[k], Z

∗[k′]

+ Cov

Z

[k′ + l′]

+

[k′ + l′]

+

′

o
[k + l], Z

′

+ Cov

′

o
Z

∗[k]Z

′

n
[k + l], µZ

′

∗[k′]

o(cid:17)

+ Cov

′

′

′

µZ

∗[k], Z

∗[k′]Z

[k′ + l′]

+

n
(cid:9)
[k + l], µ∗Z

′

[k′ + l′]

+ Cov

′

o
µ∗Z

n
[k + l], Z

′

′

∗[k′]Z

[k′ + l′]

+

o

n

′

+Cov

µZ

∗[k], µ∗Z

[k′ + l′]

′

o
+ Cov

µ∗Z

′

n
[k + l], µZ

′

∗[k′]

o

N −|l|

n
N −|l′

|

=

Cov

′

′

ǫ

∗[k]ǫ

o
[k + l], ǫ

′

n
∗[k′]ǫ

′

[k′ + l′]

+

o(cid:17)

k′=1 (cid:16)
k=1
X
X
2
Cov
µ
+
|
|
+ 0 + µE

(cid:16)

n
∗[k], ǫ

′

′

ǫ

∗[k′]

′

+ Cov

ǫ

[k + l], ǫ

′

o
[k′ + l′]

+

′

′

n
[k]ǫ
ǫ

∗[k + l]ǫ

′

o
∗[k′]

n
+ µ∗E

′

′

ǫ

[k]ǫ

∗[k′]ǫ

′

o(cid:17)
[k′ + l′]

+

+ µ∗E

′

′

n
[k]ǫ

ǫ

∗[k + l]ǫ

′

o
[k′ + l′]

+ µE

′

n
ǫ

′

∗[k + l]ǫ

∗[k′]ǫ

′

o
[k′ + l′]

+

+(µ∗)2Cov

ǫ

∗[k], ǫ

[k′ + l′]

′

′

n

o
n
+ µ2Cov

′

ǫ

[k + l], ǫ

′

∗[k′]

o

N −|l|

N −|l′

|

n
(µ4 −
(cid:0)

=

2σ4

− |

m2|
k′=1
X
3δkk′ δ0l + µ∗κ3δkk′ δ0l′ + µ∗κ3δ0lδk−k′,l′ + µκ∗
N −|l′

|

k=1
X
+µκ∗

N −|l|

o
2)δkk′ δ0lδ0l′ + σ4δkk′ δll′ + s4δk,k′ δ0,lδ0,l′ +

o(cid:17)

n

2
µ
|
|

(cid:0)

3δ0l′ δk′−k,l + (µ∗)2m2δk,k′+l′ + µ2m∗

2δk′,k+l

σ2δkk′ + σ2δk+l,k′+l′

+

=

(κ4 + s4)δkk′ δ0lδ0l′ + (µκ∗

3δkk′ + µ∗κ3δk,k′+l′ )δ0l + (µ∗κ3δkk′ + µκ∗

3δk′,k+l)δ0l′

(cid:1)

(cid:1)

k′=1
k=1
X
X
+σ4δkk′ δll′ +

(cid:0)

=δ0lδ0l′ (κ4 + s4)N + δ0l(µκ∗

2σ2 (δkk′ + δk+l,k′+l′ ) + (µ∗)2m2δk,k′+l′ + µ2m∗
µ
|
|

2δk′,k+l

3(N
max(l, l′) + N

l′) + µ∗κ3(N
−
max(l, l′)) + (µ∗)2m2(N

−

l′)) + δ0l′ (µ∗κ3(N

−

+

2σ2 (N
µ
|
|

−
=δ0lδ0l′ (κ4 + s4)N + δ0l(µκ∗

−

+ 2

2σ2 (N
µ
|
|
=δ0lδ0l′ (κ4 + s4)N + 2δ0l
2σ2 (N
µ
|
|

+ 2

−

−

3 + µ∗κ3)(N
max(l, l′)) + ((µ∗)2m2 + µ2m∗
l′) + 2δ0l′
3)(N
2)(N

(µκ∗
ℜ
max(l, l′)) + 2

−
(µ2m∗

−

ℜ

l′) + δ0l′ (µ∗κ3 + µκ∗

2)(N

min(l + l′, N )) =
l) + δll′ σ4(N

−
(µκ∗
3)(N

−
ℜ
min(l + l′, N ))

l)+

−

l) + µκ∗

3(N

−

(cid:1)
−
min(l + l′, N )) + µ2m∗
2(N
l) + δll′ σ4(N

3)(N

−

−

−
l)+

−

l)) + δll′ σ4(N

l)+
min(l + l′, N ))

−

) is the real-part
where we have used the summation formulas (42), (43), (44), (45), and (46). Here,
(
·
operator, min(a, b) is equal to the argument, either a or b, which is less than or equal to the other argument,
and conversely max(a, b) is equal to the argument which is more than or equal to the other argument.

ℜ

6

From the above derivation we have thus found that the covariance is

1
[l]

W

W

[l′]

Cov

R(lw)
ǫ

[l],

R(lw)
ǫ

[l′]

= δ0lδ0l′ (κ4 + s4)N + 2δ0l

n
+ δll′ σ4(N
b

) + 2
l
b
|
− |

o
2σ2 (N
µ
|
|

l
max(
|

,
|

−

l′
|

)) + 2
|

ℜ

l′
− |

(µκ∗

3)(N

ℜ
(µ2m∗

2)(N

−

) + 2δ0l′
|
l
min(
|

+

|

ℜ
l′
|

, N )),
|

(µκ∗

3)(N

l
− |

)+
|

ll′

≥

0 (14)

Cov

R(lw)
ǫ

[l],

R(lw)
ǫ

[l′]

=

=

(Cov

b
′
[k]Z

′

Z

b

∗[k + l], Z

o
′
∗[k′]Z

′

0,

≤

For ll′
1
[l]
W
N −l′

W
N −l

[l′]

n

k′=1
k=1
X
X
+ Cov

+ Cov

n
2
µ
|
|
[k]Z

2,
µ
|
|
(cid:8)
Z

′

′

[k′ + l′]

+ (µ∗)2Cov

′

Z

[k], Z

′

∗[k′]

+ µ2Cov

Z

∗[k + l], Z

[k′ + l′]

+

′

′

+ Cov

Z

[k]Z

∗[k + l], µZ

′

′

′

o
∗[k′]

n
µ∗Z

′

′

o
∗[k′]Z

′

[k], Z

n

[k′ + l′]

+

+ Cov

n
(cid:9)
∗[k + l], µ∗Z

′

[k′ + l′]

+ Cov

µZ

′

o
n
∗[k + l], Z

′

′

∗[k′]Z

[k′ + l′]

+ Cov

′

o
µ∗Z

′

[k], µ∗Z

[k′ + l′]

+

o

o

n

′

+ Cov

µZ

∗[k + l], µZ

∗[k′]

)

′

o

n

N −|l|

n
N −|l′

|

o

o

n

=

(Cov

′

′

ǫ

∗[k]ǫ

′

′

[k + l], ǫ

∗[k′]ǫ

[k′ + l′]

+ (µ∗)2Cov

′

ǫ

[k], ǫ

′

∗[k′]

+ µ2Cov

ǫ

∗[k + l], ǫ

[k′ + l′]

+

′

′

o

n

o

2δkk′ + µ2m2δk+l,k′+l′ +

k′=1
k=1
X
X
+ 0 + µE

n
∗[k]ǫ

′

′

ǫ

[k + l]ǫ

′

∗[k′]

+ µE

o
∗[k′]ǫ
∗[k]ǫ

′

′

n
[k′ + l′]

′

ǫ

+

+ µ∗E

′

′

n
∗[k]ǫ

ǫ

[k + l]ǫ

′

o
[k′ + l′]

n
+ µ∗E

′

ǫ

[k + l]ǫ

′

∗[k′]ǫ

′

o
[k′ + l′]

+

+

n
2Cov
µ
|
|
N −|l|
N −|l

|

′

′

′

ǫ

[k], ǫ

[k′ + l′]

+

o
2Cov
µ
|
|

′

n
ǫ

∗[k + l], ǫ

∗[k′]

)

′

o

n

2σ4

k=1
X
+ µκ∗

o
2)δkk′ δ0lδ0l′ + s4δkk′ δll′ + σ4δk,k′ δ0,lδ0,l′ + (µ∗)2m∗
m2|

n
((µ4 −
k′=1
X
3δkk′ δ0l + µκ∗
3δkk′ δ0l′ + µ∗κ3δ0lδk−k′,l′ + µ∗κ3δ0l′ δk′−k,l+
2σ2δk′,k+l)
2σ2δk,k′+l′ +
µ
µ
|
|
|
|
N −|l′
N −|l|

− |

+

o

|

=

=

((κ4 + σ4)δkk′ δ0lδ0l′ + (µκ∗

3δkk′ + µ∗κ3δk,k′+l′ )δ0l + (µκ∗

3δkk′ + µ∗κ3δk′,k+l)δ0l′ + s4δkk′ δll′ +

2δk,k′ + µ2m2δk+l,k′+l′ )

3(N
max(l, l′) + N

l′) + µ∗κ3(N
−
max(l, l′)) + (µ∗)2m∗

−

l′)) + δ0l′ (µκ∗

3(N

l) + µ∗κ3(N

−
min(l + l′, N )) + µ2m2(N

−

l)) + δll′ s4(N

l)+
min(l + l′, N ))

−

2(N

−

l′) + δ0l′ (µ∗κ3 + µκ∗

3)(N

l) + δll′ s4(N

min(l + l′, N )) =

−

−
l)+

−

2 + µ2m2)(N
l′) + 2δ0l′

−
(µκ∗
3)(N

ℜ

−
min(l + l′, N ))

−

l) + δll′ s4(N

l)+

−

k′=1
k=1
X
X
2σ2 (δk,k′+l′ + δk′,k+l) + (µ∗)2m∗
µ
+
|
|

=δ0lδ0l′ (κ4 + σ4)N + δ0l(µκ∗

−

+

2σ2 (N
µ
|
|

−
=δ0lδ0l′ (κ4 + σ4)N + δ0l(µκ∗

−

+ 2

2σ2 (N
µ
|
|
=δ0lδ0l′ (κ4 + σ4)N + 2δ0l
2σ2 (N
µ
|
|

+ 2

−

3 + µ∗κ3)(N
max(l, l′)) + ((µ∗)2m∗
(µκ∗
max(l, l′)) + 2

3)(N

ℜ

−
(µ2m2)(N

−

so

1
[l]

W

W

[l′]

ℜ

o

Cov

R(lw)
ǫ

[l],

R(lw)
ǫ

[l′]

=δ0lδ0l′ (κ4 + σ4)N + 2δ0l

(µκ∗

3)(N

+ δll′ s4(N
+ 2

) + 2
l
− |
|
(µ2m2)(N

ℜ

ℜ
2σ2 (N
µ
|
|
l′
l
min(
|
|

+

|

−

l′
) + 2δ0l′
|
− |
l
max(
|

,
|

l′
))
|
|
ll′

0

≤

−
, N )),
|

(µκ∗

3)(N

ℜ

l
− |

)+
|

(15)

n

b

b

7

4.1.3 Mean square error

Now the mean square error (MSE) is the variance (l = l′) plus the bias squared. The derivation of the MSE
in this case starts from this deﬁnition and proceeds as follows:

2

MSE

W
+ ((N
(cid:0)

W
+

=

=

=δ0l

+

=δ0l

1

o

l)

−

+

−

=

[l]

[l]

[l]

W

Bias

n
2[l]

= Var

R(lw)
ǫǫ

R(lw)
ǫǫ

R(lw)
ǫǫ
(cid:12)
o(cid:12)
n
o
n
δ0l((κ4 + s4) + 4
3))N + (σ4 + 2
2σ2) (N
(µµ∗
µ
(cid:12)
(cid:12)
b
b
b
ℜ
|
|
(cid:12)
(cid:12)
2(σ2δ0l +
2)2 =
µ
[l]
(N
(cid:0)
|
|
|
|
−
3))N + (σ4 + 2
δ0l((κ4 + s4) + 4
2[l]
(µµ∗
ℜ
l)2
2[l]
2(N
l)
((κ4 + s4) + 4
(µµ∗
σ2(σ2 + 2
(cid:0)
((κ4 + s4) + 4
(cid:0)
σ2(σ2 + 2
(cid:0)

[l] + 1)((σ4 + 2σ2
−
2[0] + σ2(σ2 + 2
3))N
W
(µ2m∗
l) + 2
2)(N
ℜ
−
−
3) + σ2(σ2 + 2
2)N )N
(µµ∗
µ
|
|
(µ2m∗
2)(N
l) + 2

ℜ
2) (N
µ
|
|

2σ2) (N
µ
|
|

−
2)δ0,l +
µ
|
|
2)(N 2
µ
|
|

W
min(2l, N )) +
2[0]

min(2l, N )) +

W

W

W

−

−

−

ℜ

−

+

ℜ
2) (N
µ
|
|
(cid:0)
Thus we ﬁnd that the MSE is

−

l) + 2

(µ2m∗

2)(N

ℜ

(µ2m∗

2)(N

l) + 2

ℜ
4) =
µ
|
|
2[0]

2N

min(2l, N ))

+

(cid:1)
min(2l, N ))

+

−

−

(cid:1)

−
4(N
µ
|
|
−
2N σ2(σ2 + 2
4(N
µ
|
|

−

+

[0] + 1)
W
4(N
2[l]
l)2
µ
(cid:1)
−
|
|
[0] + σ2(σ2 + 2
W
2[l]

W
2)
µ
(cid:1)
|
|
l)2

−

2

2

4(N
µ
|
|

−

W

−

l)

W
2))
µ
|
|
l)

W

[l] +

4 =
µ
|
|

+

[l] +
(cid:1)

4
µ
|
|

(cid:1)

MSE

R(lw)
ǫǫ

[0]

=

n

b
R(lw)
ǫǫ

n

b

κ4 + s4 + σ4 + 2
(cid:16)
−
σ2(σ2 + 2
(cid:0)

σ2 +

2N

2
µ
|
|
2) (N
(cid:1)
µ
|
|
l)

2

4(N
µ
|
|

(cid:0)
−

W

−

W

−
[l] +

(µκ∗

2σ2 + 4
µ
|
|
2

[0] +

ℜ
σ2 +

3) + 2
2

ℜ

2
µ
|
|
(µ2m∗
(cid:1)
2)(N

(cid:0)
l) + 2
ℜ

4
µ
|
|

o

o

(µ2m∗

2) +

σ2 +

2
µ
|
|

2

N

N

2[0]+

W

(cid:0)

(cid:1)

(cid:17)

min(2l, N )) +

−

4(N
µ
|
|

−

l)2

2[l]+

(16)

(17)

W

(cid:1)

MSE

[l

= 0]

=

Asymptotically, i.e. N

, keeping only the leading terms of in N or l for each coefﬁcient of each

power of

[l], we ﬁnd

W

→ ∞

where we assume that µ

b

= 0.

n

o

o

MSE

R(lw)
ǫ

[0]

MSE

n
R(lw)
[l
b
ǫ

= 0]

2[0]

2N (σ2 +

2 + σ2)2N 2
µ
= (
|
|
4(N
µ
|
|

l)2

W

−

=

W
2[l]

−
4(N
µ
2
|
|

−

2)2
µ
|
|
[l] +

W

4
µ
|
|

l)

−

W

[0] + (σ2 +

2)2
µ
|
|

(18)

(19)

4.2 Fixed-length summation autocorrelation estimator

As an example of a correlogram that does not have form of the classic lag windowed correlogram, as given
Z . It is
by equation (9), we present what we will call the ﬁxed-length summation correlogram denoted
deﬁned as

R(f l)

R(f l)

Z [l] :=

[l]

W

Z ∗[k]Z[k + l] 0

l

N

L = M,

≤

≤

−

L

k=1
X

b

(20)

b

This estimator is deﬁned in [3], equation (8.96). It is implemented on the DWP electron counts correlator
experiment [6] onboard the CLUSTER-II space mission. Its deﬁning characteristic is that the same, ﬁxed
number of terms are summed over for all lags. This in contrast with the classic correlogram for which the
number of terms decrements with increasing lag. This implies that the lags of the classical correlogram
all have different statistics, which is an unwanted property. With a ﬁxed length summation however, one
might hope that each of the estimated lags will have the same statistics giving more equitable lag estimates.
As we will now see, this is in fact true but only when the mean is zero.

8

6
6
6
4.2.1 Expectation and bias

The bias is for a general signal

The bias is then

E

Z [l]

R(f l)
n

o

b

=

[l]

W

E

Z ∗[k]Z[k + l]
}

{

=

L

k=1
X
L

k=1
X

=

[l]

W

RZ[l] =

L

k=1
X

=

[l]RZ[l]

1 =

W

= L

[l]RZ[l]

W

Bias

Z [l]

R(f l)
n

o

b

= E

= L
= (L

−

Z [l]

R(f l)
n
W
W

−

o
[l]RZ[l]
b
−
1) RZ [l]
[l]

RZ [l] =

RZ [l] =

Thus with a window choice of
[l] = 1/L the ﬁxed-length correlogram is unbiased irrespective of the
signal. For the special case of the higher-order white noise signal, ǫ, with which we are mainly interested
in here, the bias is therefore

W

Bias

R(f l)
ǫ

[l]

= (L

[l]

W

−

1)

σ2δ0,l +

2
µ
|
|

(cid:1)

(cid:0)

n

b

o

(21)

4.2.2 Covariance

The covariance of the ﬁxed-length correlogram can be derived accordingly,

1
[l]

[l′]

W
L

W
L

=

k=1
X

k′=1 (cid:16)
X

Cov

R(f l)
ǫ

[l],

R(f l)
ǫ

[l′]

=

n

Cov

b
′
∗[k]Z

′

b
[k + l], Z

Z

o
′
∗[k′]Z

′

[k′ + l′]

+

+

n
2
µ
|
|
+ Cov

′

′

Cov

Z

∗[k], Z

∗[k′]

+ Cov

[k + l], Z

[k′ + l′]

′

+ Cov

(cid:16)

′

n
∗[k]Z

′

Z

[k + l], µZ

o
′
∗[k′]

n
+ Cov

′

µZ

∗[k], Z

′

o(cid:17)
′
∗[k′]Z

[k′ + l′]

+ Cov

′

′

Z

∗[k]Z

[k + l], µ∗Z

′

o
[k′ + l′]

n
+ Cov

′

µ∗Z

[k + l], Z

′

∗[k′]Z

o
′
[k′ + l′]

+Cov

µZ

∗[k], µ∗Z

[k′ + l′]

′

o
+ Cov

µ∗Z

′

n
[k + l], µZ

′

∗[k′]

=

2
µ
|
|

+

2,
µ
|
|
(cid:8)
+

(cid:9)

+

o

o(cid:17)

′

′

L

L

=

Cov

n
′
∗[k]ǫ
ǫ

′

o

′

[k + l], ǫ

∗[k′]ǫ

[k′ + l′]

k=1
X

k′=1 (cid:16)
X

n
+ (µ∗)2Cov

′

ǫ

[k], ǫ

′

∗[k′]

o
+ µ2Cov

+ µE

′

′

n
∗[k]ǫ

ǫ

[k + l]ǫ

′

o
∗[k′]

+ µE

∗[k]ǫ

∗[k′]ǫ

[k′ + l′]

′

n
ǫ

o

+

n
+ µ∗E

′

′

ǫ

∗[k]ǫ

[k + l]ǫ

′

o
[k′ + l′]

n
+ µ∗E

ǫ

[k + l]ǫ

′

∗[k′]ǫ

o
′
[k′ + l′]

+

ǫ

∗[k + l], ǫ

[k′ + l′]

+ 0+

′

′

′

′

o

n

o

n

n

n

o
Z

′

n
+

′

9

′

′

ǫ

[k], ǫ

[k′ + l′]

′

ǫ

∗[k + l], ǫ

∗[k′]

′

=

+

2Cov
µ
|
|

o

n

o(cid:17)

2)δkk′ δ0lδ0l′ + s4δkk′ δll′ + σ4δk,k′ δ0,lδ0,l′ + (µ∗)2m∗

2δkk′ + µ2m2δk+l,k′+l′ +

3δkk′ δ0l′ + µ∗κ3δ0lδk−k′,l′ + µ∗κ3δ0l′ δk′−k,l +

2σ2δk,k′+l′ +
µ
|
|

2σ2δk′,k+l
µ
|
|

=

(κ4 + s4)δkk′ δ0lδ0l′ + (µκ∗

3δkk′ + µ∗κ3δk,k′+l′ )δ0l + (µ∗κ3δkk′ + µκ∗

3δk′,k+l)δ0l′ +

(cid:1)

+

2Cov
µ
|
|

n

2σ4

(µ4 −
(cid:0)
+µκ∗

m2|

− |
3δkk′ δ0l + µκ∗

L

L

k=1
X

k′=1
X

L

L

=

=

k=1
X

k′=1
X

(cid:0)

+ δll′ σ4L +

+σ4δkk′ δll′ +

2σ2 (δk,k′+l′ + δk′,k+l) + (µ∗)2m2δk,k′ + µ2m∗
µ
|
|
=δ0lδ0l′ (κ4 + s4)L + δ0l (µκ∗
3L + µ∗κ3(L
l′
, L))) + δ0l′ (µ∗κ3L + µκ∗
min(
|
|
, L)) + (µ∗)2m2L + µ2m∗
2σ2 (L
min(
l
, L) + L
µ
|
−
|
|
|
|
l′
µ∗κ3 min(
, L)) + δ0l′ (2
3)L
ℜ
ℜ
|
|
(µ2m∗
l′
2)L
, L)) + 2
min(
l
min(
|
|
|

−
=δ0lδ0l′ (κ4 + s4)L + δ0l (2

2σ2 (2L
µ
|
|

l′
min(
|
(µκ∗

+ δll′ σ4L +

−
, L)
|

2δk+l,k′+l′
3(L
2 (L
µκ∗

−
3)L
l
3 min(
−
|
µ2m∗
l
2 min(
||

(µκ∗

l
min(
(cid:1)
|
min(

, L))) +
|
l′
l
||
| − |
, L)) +
|
l′
| − |

, L)

−

=

−

−

−

−

ℜ

||

||

, L)) =

where we have used the summation formulas (47), (48), (49), (50), and (51).

So ﬁnally we can write the covariance for all l and l′ as

1
[l]

W

W

[l′]

Cov

R(f l)
ǫ

[l],

R(f l)
ǫ

[l′]

= δ0lδ0l′ (κ4 + s4)L + δ0l (2

(µκ∗

3)L

l′
µ∗κ3 min(
|

, L)) +
|

−

ℜ

n

b

o
+ δ0l′ (2

b
2σ2 (2L
µ
|
|

−

+

(µκ∗

−

3)L
, L)
|

−

µκ∗

l
3 min(
|
l′
min(
|

, L)) + δll′ σ4L+
|
(µ2m∗
, L)) + 2
|

2)L

ℜ

−

ℜ
l
min(
|

µ2m∗

l
2 min(
||

l′
| − |

||

, L)

(22)

For convenience, we also provide the covariance for the case of real-valued variates

1
[l]

W

W

[l′]

n

b

o

b

Cov

R(f l)
ǫ

[l],

R(f l)
ǫ

[l′]

=δ0lδ0l′ (κ4 + s4)L + δ0lµκ3(2L

l′
min(
|

, L))+
|

−

+ δ0l′ µκ3(2L
+ µ2σ2 (4L

, L)) + δ|l|,|l′|σ4L+
l
min(
|
|
l′
min(
, L)
|
|

−
l
min(
|

, L)
|

−

−

−

min(

l
||

l′
| − |

||

, L))

(23)

and if in addition

1
[l]

W

W

Cov

[l′]

L , then

l
| ≤ |

l′
|
R(f l)
ǫ

[l],

| ≤
R(f l)
ǫ

n

b

o

b

[l′]

=δ0lδ0l′ (κ4 + s4)L + δ0l2µκ3(L

l′
− |

/2) + δ0l′ 2µκ3(L
|

l
− |

/2)+
|

+ δll′ σ4L + 4µ2σ2(L

l
− |

)/2)
|

The results shown above are expressed for both zero and nonzero lags. Formulas can also be given

which are explicit in the zero and nonzero lags. They are as follows:

Var

R(f l)
ǫ

[0]

2[0]L

κ4 + s4 + σ4 + 2

Var

n
R(f l)
[l
b
ǫ

= 0]

2[l]

(cid:0)
Lσ4 + 2

2σ2(L
µ
|
|

−

(µ2m∗

2) + 4

(µκ∗
3)

2σ2 + 2
µ
|
|
l
min(
|

ℜ

, L)) + 2L
|

ℜ

ℜ
(µ2m∗
2)

(cid:1)

=

=

W

W

o

o

(cid:1)

n

b
R(f l)
ǫ

n

b

1
[0]

W

W

Cov

[l]

1
[l]

W

W

[l′]

[0],

R(f l)
ǫ

[l]

= 2L

(µκ∗

3) +

(µ2m∗

2) +

ℜ

2σ2
µ
|
|

2σ2 + µ∗κ3 + µ2m∗
µ
2
|
|
(cid:0)

−
(cid:1)

(27)
(cid:1)

l
min(
|

, L),
|

l

= 0

o

b

Cov

R(f l)
ǫ

[l],

R(f l)
ǫ

[l′]

=

n

b

o

b

2σ2(2L
µ
|
|
+ 2

−
(µ2m∗

l
min(
|

2)L

−

l′
, L)
min(
−
|
|
µ2m∗
l
2 min(
||

ℜ

, L))
|

l′
| − |

||

, L),

l

= 0, l′

= 0

(cid:0)

ℜ
(cid:0)

10

(24)

(25)

(26)

6
6
6
6
4.2.3 Mean square error

The mean square error of the ﬁxed length correlogram for complex white noise with arbitrary mean can
now be derived. The derivation proceeds from the variance and bias results derived above as follows

2

=

=

1

b

n

n

o

n

(cid:12)
(cid:12)
(cid:12)

−

−

+

=

[l]

[l]

[l]

2[l]

Bias

W
+

MSE

= Var

R(f l)
ǫ

l
min(
|

o(cid:12)
2σ2 (L
µ
(cid:12)
|
|
(cid:12)

R(f l)
ǫ
o
3)L + σ4L + 2
(µκ∗
2

R(f l)
ǫ
δ0l(κ4 + s4)L + δ0l4
b
b
ℜ
2
σ2δ0l +
2
µ
[l]
(cid:0)
|
|
W
|
3)) + σ4L + 2
δ0lL(κ4 + s4 + 4
2σ2 (L
(µκ∗
µ
(cid:1)
(cid:0)
|
|
−
σ4δ0l + 2σ2
2δ0l +
4
µ
2L
µ
|
|
|
|
2)(L2
+ σ2(σ2 + 2
(µκ∗
3)
µ
(cid:1)
(cid:0)
W
|
|
(µ2m∗
+
2)L
, L)) + 2
l
min(
(cid:1)
W
ℜ
|
|
+ σ2(σ2 + 2
3) + Lσ2(σ2 + 2
2)(1
2)
µ
µ
(cid:0)
(cid:1)
|
|
|
|
4
2)L + L2
(µ2m∗
+
µ
, L)) + 2
l
min(
(cid:1)
|
|
|
|

L
|
2[l]
W
2[l]
+ (L2
(cid:0)
W
2[l]L
W
2[l]
(cid:0)
W
2[l]L
(cid:0)
W
σ4L + 2
2[l]
(cid:0)
W

−
W
κ4 + s4 + 4
ℜ
2σ2 (L
µ
(cid:0)
|
|
κ4 + s4 + 4
ℜ
2σ2 (L
µ
|
|

2L
−
4
µ
|
|

, L)) + 2
|

ℜ
[l] + 1)

l
min(
|

σ4L + 2

−
(µκ∗

W
L2

2[l]

+

+

ℜ

(cid:0)

=δ0l

=δ0l

, L)) + 2
|

ℜ

(µ2m∗

2)L

(cid:1)

(µ2m∗

2)L

ℜ

(cid:1)

[l] + 1)
2[l]

(cid:1)
−
2L
−
4 (1
µ
|
|

2L

[l] + 1

W
[l])

W

2L

[l])

(cid:1)
W

−

(cid:1)

−
Thus, the ﬁnal formula for the MSE is

(cid:0)

(cid:1)

MSE

R(f l)
ǫ

[0]

=L

κ4 + s4 + 4

(µκ∗

3) + L(σ2 +

2)2 + σ4 + 2
µ
|
|

2σ2 + 2
µ
|
|

ℜ

(µ2m∗
2)

2[0]

MSE

[l

= 0]

=

n

b
R(f l)
ǫ

n

b

o

o

ℜ
2
(1

(cid:0)
+

σ2 +

2
µ
−
|
|
2σ2 (L
Lσ4 + 2
(cid:1)
(cid:0)
µ
|
|
2L

−
[l])

4 (1
µ
|
|

W

−

(cid:0)
+

2L

[0])

W
l
min(
|

, L)) + 2L
|

ℜ

(µ2m∗

2) + L2

4
µ
|
|

2[l]

W

(cid:1)

W

(cid:1)

(28)

(29)

Asymptotically as L

and assuming µ

= 0 the MSE becomes

→ ∞
MSE

R(f l)
ǫ

[0]

=

σ2 +

2

L2

2[0]

2L

[0] + 1

2
µ
|
|
L2

W
2L

W

−
[l] + 1

n
R(f l)
[l
b
ǫ

o

MSE

W
n
where we have kept leading terms in L for each power of
b

(cid:0)
(cid:1)
2[l]
−
. This proves that for the weight window choice
[l] = 1/L the ﬁxed-length correlogram, as an autocorrelation estimate, is statistically consistent since

W
the MSE is zero at all lags. If instead µ = 0 then the asymptotic MSE of the nonzero lags is Lσ4

(cid:0)
4
µ
=
|
|

= 0]

W

W

o

(cid:1)

(cid:1)

(cid:0)

2.
[l]
|

|W

4.3 Lag windowed autocovariance estimator using sample mean

The two previous correlograms cannot be used as autocovariance estimators if the signal has an apriori
unknown mean. As this is a common state of affairs one usually resorts to estimating the mean and sub-
tracting this estimate from the signal. If the mean estimate is based on the same data samples as those on
which the autocovariance estimate is to be based it is natural to see this scheme as a distinct estimator. In
fact the mean need not even be explicitly calculated. There are a variety of autocovariance estimators, see
[9]. Here we only consider the simplest: the classic correlogram in which the ordinary sample mean is
subtracted from the data samples. We denote it

and deﬁned it for an arbitrary complex signal as

where

is the sample mean.

(Zk

Z)(Zk+l

Z)

−

−

C(lw)

Z [l] :=

b

C(lw)
Z

W

[l]
b
X
1
N

Z =

N

Zk

k=1
X

11

(30)

(31)

6
6
6
4.3.1 Properties of the sample mean

C(lw)
In order to derive the sampling properties of
estimator (31). These well know properties are that the sample mean estimator is unbiased,
b

Z , we will need some of the properties of the sample mean

E

Z

= E

N

1
N

(

Zk

=

)

N

1
N

E

Zk
{

}

=

N

1
N

µ = µ

k=1
X
that the standard deviation for independent samples goes is proportional to 1/√N ,

k=1
X

k=1
X

(cid:9)

(cid:8)

Var

=

ǫ
{

}

E

ǫ[k]ǫ[k′]∗
{

} −

2

E

ǫ[k]

=

1
N 2

(σ2δkk′ + µ2)

µ2

−

N

N

k=1
X

k′=1
X

δkk′ + µ2

µ2 =

−

o

n
σ2
N 2 N

N

N

k=1
X
N

k′=1
X
N

k=1
X

k′=1
X

1
N 2

σ2
N 2

σ2
N

=

=

where we have specialised to the generalised noise signal ǫ[k].

4.3.2 Expectation and bias

The expectation of

C(lw)
Z

is

so the bias is

b

E

ǫ;W [l]

C(lw)
n

o

b

= (N

l)

−

W

[l]σ2(δ0l

1/N )

−

Bias

ǫ;W [l]

C(lw)
n

o

b

= σ2

(cid:18)

(N

[0]

W

−

1)δ0,l

−

(N

[l]

−

l)
N

W

(cid:19)

(35)

4.3.3 Covariance

We start by deriving an expression for the covariance of the correlogram for an arbitrary signal Z

(32)

(33)

(34)

1
[l]
W
N −l′

W
N −l

[l′]

Cov

C(lw)

Z [l],

C(lw)∗
Z

[l′]

=

n
′

Z

b
[k]Z

′

b
∗[k + l]

o
Z ′ ∗

=

Cov

k=1
X
′
Z

k′=1
X
∗[k′]Z

′

n
[k′ + l′]

−

N −l

N −l′

=

′

Z

[k]

′

Z ′Z

∗[k + l] +

−

Z ′
|

2,
|

′

Z ′Z

∗[k′]

′

Z ′ ∗

Z

[k′ + l′] +

−

−

Z ′
|

2
|

=

o
Cov

−

n
′
[k]Z

′

k=1
X

k′=1 (cid:16)
X
Cov
Z
n
Z ′ ∗
n

−
+ Cov

Cov

−

−

−

Cov

n

Z ′ Z

Cov

n
Z ′
|
n

′

′

∗[k + l], Z ′ ∗

Z

′

[k′ + l′]

+ Cov

[k]Z

′

Z

′

[k], Z ′Z

∗[k′]

+ Cov

′

o
Z ′ ∗

Z

Z ′ Z

∗[k + l], Z

′

o
∗[k′]Z

′

n
[k′ + l′]

+ Cov

Z ′Z

∗[k + l],
2, Z ′ ∗
|

Z

′

Z ′
|

2
|
o
[k′ + l′]

+ Cov

o
Z ′
|
n
+ Cov

2, Z
|
Z ′
|
n

2,
|

o

Z ′
|

2
|

o(cid:17)

12

′

′

′

′

Cov

Z

[k]Z

∗[k + l], Z

∗[k′]Z

[k′ + l′]

′

′

Z

[k]Z

∗[k + l], Z ′Z

∗[k′]

+

′

′

o
Z
n
[k], Z ′ ∗

′

n
∗[k + l],

′

Z

[k′ + l′]

′

−
∗[k + l], Z ′Z

o

′

n
∗[k′]Z

′

[k′ + l′]

Cov

Cov

Z ′
|

2
|
o
Cov

−
Z ′ ∗
n
∗[k′]

′

′

n
[k],

Z

+ Cov

2, Z ′Z
|

o
Z ′
|
n

−

o

o
′
Z

Z ′ ∗

′

′

[k], Z

∗[k′]Z

[k′ + l′]

+

Z ′
2
|
|
Z ′Z

+
o
∗[k + l], Z ′ ∗
′

′

Z

[k′ + l′]

+

o

o

n
′
∗[k′]

+

o

where Z ′ := Z

µ.

−

The last expression contains 16 terms involving various covariances of a general signal Z. To progress
further we specialise Z to be the higher-order white noise signal ǫ. Working out each of the 16 covariance
terms individually in the order the appear above for this case we have: ﬁrst covariance term

o

σ4) + (1

= δ0lδ0l′ δkk′ (µ4 −
2σ4
= δ0lδ0l′ δkk′ (µ4 −
= δ0lδ0l′ δkk′ κ4 + δkk′ δll′ σ4 + δk′,k+lδ−l,l′

−
m2|

− |

δ0l)δkk′ δll′ σ4 + (1

2) + δkk′ δll′ σ4 + δk,k′+l′ δk′,k+l
2
m2|
|

−

δ0lδ0l′ )δk,k′+l′ δk′,k+l
m2|
|

2 =

2 =

m2|
|

′

′

′

′

Cov

ǫ

[k]ǫ

∗[k + l], ǫ

∗[k′]ǫ

[k′ + l′]

n

n

second covariance term

Cov

′

′

ǫ

[k]ǫ

∗[k + l], ǫ′ǫ

∗[k′]

′

n

third covariance term

′

Cov

ǫ

[k]ǫ

′

∗[k + l], ǫ′ ∗

ǫ

′

[k′ + l′]

fourth covariance term

Cov

′

′

ǫ

[k]ǫ

∗[k + l],

n

ﬁfth covariance term

Cov

′

ǫ′ ∗

ǫ

′

′

[k], ǫ

∗[k′]ǫ

[k′ + l′]

n

sixth covariance term

seventh

Cov

′

ǫ′ ∗

ǫ

[k], ǫ′ ∗

ǫ

′

[k′ + l′]

n

o

=

=

=

1
N
1
N
1
N

o

=

=

=

1
N
1
N
1
N

ǫ′
|

2
|

o

σ4) + (1

(δ0lδkk′ (µ4 −
(δ0lδkk′ (µ4 −
(δ0lδkk′ κ4 + δkk′ σ4 + δk′,k+l

−
m2|

2σ4

− |

δ0l)(δkk′ σ4 + δk′,k+l

2)) =

2) + δkk′ σ4 + δl,k′−k

2) =

m2|
|
m2|
|

2)
m2|
|

σ4) + (1

(δ0lδk,k′+l′ (µ4 −
(δ0lδk,k′+l′ (µ4 −
(δ0lδk,k′+l′ κ4 + δk+l,k′+l′ σ4 + δk,k′+l′

−
m2|

2σ4

− |

δ0l)(δk+l,k′+l′ σ4 + δk,k′+l′

2)) =

2) + δk+l,k′+l′ σ4 + δk,k′+l′

2) =

m2|
|
m2|
|

2)
m2|
|

m2|
|
m2|
|

σ4) + (1

δ0l)(σ4 +

2)) =

2) + σ4 +

2) =

=

=

=

1
N 2 (δ0l(µ4 −
1
N 2 (δ0l(µ4 −
1
N 2 (δ0lκ4 + σ4 +

2σ4

−
m2|
− |
2)
m2|
|

o

=

=

=

1
N
1
N
1
N

σ4) + (1

(δ0l′ δkk′ (µ4 −
(δ0l′ δkk′ (µ4 −
(δ0l′ δkk′ κ4 + δkk′ σ4 + δk,k′+l′

−
m2|

2σ4

− |

δ0l′ )(δkk′ σ4 + δk,k′+l′

m2|
|
2) + (δkk′ σ4 + δk,k′+l′
m2|
|

2)) =

2)) =

2)
m2|
|

Cov

′

ǫ

′

[k], ǫ′ǫ

∗[k′]

ǫ′ ∗
n

=

=

o

σ4 + (N

1
N 2 (δkk′ (µ4 −
1
N 2 (δkk′ (κ4 + N σ4) +

−
2)
m2|
|

1)σ4) + (1

m2|
δkk′ )
|

−

2) =

o

=

=

=

1
N 2 (δk,k′+l′ (µ4 −
1
N 2 (δk,k′+l′ (µ4 −
1
N 2 (δk,k′+l′ (κ4 + N

m2|
|

−

m2|
1)
|
m2|
1)
−
|
2) + σ4)

σ4 + (N

2) + (1

δk,k′+l′ )σ4) =

−

2σ4 + (N

2) + σ4) =

13

eighth

ninth

n

tenth

11-th

12-th

13-th

14-th

Cov

′

ǫ

[k],

ǫ′
|

2
|

ǫ′ ∗
n

o

σ4) + (N

=

=

=

1
N 3 ((µ4 −
1
N 3 ((µ4 −
σ4 +
κ4
N 3 +

2σ4

− |
2
m2|
|
N 2

2)) =

1)(σ4 +

m2|
−
|
2) + N (σ4 +
m2|

2)) =

m2|
|

Cov

′

ǫ′ ǫ

∗[k + l], ǫ

∗[k′]ǫ

[k′ + l′]

′

′

δ0l′ δk′,k+l)(δk+l,k′+l′ σ4 + δk′,k+l

2)) =

σ4) + (1

(δk′,k+lδ0,l′ (µ4 −
(δk′,k+lδ0,l′ (µ4 −
(δk′,k+lδ0,l′ κ4 + δk+l,k′+l′ σ4 + δk′,k+l

−
m2|

2σ4

− |

2)
m2|
|

2) + δk+l,k′+l′ σ4 + δk′,k+l

m2|
|
2) =

m2|
|

1
N
1
N
1
N

o

o

=

=

=

=

=

=

Cov

ǫ′ǫ

∗[k + l], ǫ′ǫ

∗[k′]

′

′

n

σ4 + (N

1)s4) + (1

δk′,k+l)σ4) =

−

−

2σ4 + (N

1)s4) + σ4) =

1
N 2 (δk′,k+l(µ4 −
1
N 2 (δk′,k+l(µ4 −
1
N 2 (δk′,k+l(κ4 + N

−
2) + σ4)

m2|
|

Cov

′

ǫ

′

[k], ǫ′ǫ

∗[k′]

ǫ′ ∗
n

o

=

=

=

σ4 + (N

1
N 2 (δkk′ (µ4 −
−
1
2 + (N
N 2 (δkk′ (µ4 −
m2|
− |
1
N 2 (δkk′ (κ4 + N σ4) +

2)
m2|
|

σ4

1)σ4) + (1

2) =

−
1)σ4) +

m2|
δkk′ )
|
m2|
|

2) =

−

Cov

′

ǫ′ǫ

∗[k + l],

n

ǫ′
|

2
|

o

σ4) + (N

=

=

=

1
N 3 ((µ4 −
1
N 3 ((µ4 −
σ4 +
κ4
N 3 +

2σ4

− |
2
m2|
|
N 2

2)) =

1)(σ4 +

m2|
−
|
2) + N (σ4 +
m2|

2)) =

m2|
|

Cov

′

2, ǫ
|

ǫ′
|
n

′

∗[k′]ǫ

[k′ + l′]

o

=

=

=

1
N 2 (δ0l′ (µ4 −
1
N 2 (δ0l′ (µ4 −
1
N 2 (δ0l′ κ4 + σ2 +

2σ4

−
m2|
− |
2)
m2|
|

σ4) + (1

δ0l′ )(σ2 +

2)) =

2) + (σ2 +

2)) =

m2|
|
m2|
|

Cov

ǫ′
|
n

′

∗[k′]

2, ǫ′ǫ
|

o

2)) =

1)(σ4 +

m2|
−
|
2) + N (σ4 +
m2|

2)) =

m2|
|

σ4) + (N

=

=

=

1
N 3 ((µ4 −
1
N 3 ((µ4 −
σ4 +
κ4
N 3 +

2σ4

− |
2
m2|
|
N 2

14

15-th

Cov

ǫ′
|

2, ǫ′ ∗
|

′

ǫ

[k′ + l′]

n

o

σ4) + (N

=

=

=

1
N 3 ((µ4 −
1
N 3 ((µ4 −
σ4 +
κ4
N 3 +

2σ4

− |
2
m2|
|
N 2

2)) =

1)(σ4 +

m2|
−
|
2) + N (σ4 +
m2|

2)) =

m2|
|

and ﬁnally the last term

Cov

2,
|

ǫ′
|

2
|

ǫ′
|
n

o

= E

2
|

−

E

−

4
|

ǫ′
|

ǫ′
|
n
o
n
1
N 4 (N µ4 + N (N
1
N 3 (µ4 + (N
σ4 +
κ4
N 3 +

−
m2|
|
N 2

2

=

=

=

=

E

ǫ′
|

2
|

o
n
1)(2σ4 +

o
m2|
|
2)
m2|
|

−

σ4
N 2 =

2))

−
N σ4) =

1)(2σ4 +

0, l′

for l
functions[δ0lδ0l′, (δ0l + δ0l′ ), δl,l′ , 1]. The term containing δ0lδ0l′ is

0. Adding up all the terms we can now collect all terms containing the lag dependent delta

≥

≥

2

κ4 + N σ4
N

−

δk,k′ +

+δ0lδ0l′ (κ4 + s4)δk,k′

the terms containing δ0l and δ0l′ are

κ4
N

−

(cid:18)
the terms containing δl,l′ is

and all the other terms are

(δk,k′ + δk,k′+l′

)δ0l + (δk,k′ + δk′,k+l

)δ0l′

1
N

−

(cid:19)

1
N

−

+δl,l′(σ4δk,k′ )

2δk+l,k′+l′ σ4 + (δk,k′+l′ + δk′,k+l)s4 + 2δk,k′ σ4 + (δk,k′+l′ + δk′,k+l)s4

(δk,k′+l′ + δk′,k+l)

κ4 + N s4
N

2

−

σ4 + s4

2

σ4 + s4
N

+ 3

σ4 + s4
N

+ 3

κ4
N 2

(cid:19)

=

−

(cid:18)

(2δk,k′ + 2δk+l,k′+l′

2δkk′ + −

−

N −
2

2 + 3

−
N

)σ4+

−

(cid:18)

1
N

−
1
N

1
N

+(2δk,k′+l′ + 2δk′,k+l

δk,k′+l′

δk′,k+l + −

−

−

2

2 + 3

)s4

κ4
N

−

(2δk,k′ + δk,k′+l′ + δk′,k+l

1
N

(cid:18)

−

=

−

)σ4 + (δk,k′+l′ + δk′,k+l

(2δk+l,k′+l′

)
(cid:19)
Now we perform the double sum over all k, k′ each of the lag dependent delta terms individually and make
use of the summation formulas (42), (43), (44), (45), and (46). First we have a contribution to the zero lag
variance, δ0lδ0l′ , which becomes

(2δk,k′ + δk,k′+l′ + δk′,k+l

)s4

−

−

−

−
N
1
N

κ4
N

3
N

)
(cid:19)

−

3
N

then a contribution to the zero lag versus any lag covariance terms, δ0l and δ0l′ , which become

+δ0lδ0l′ (κ4 + s4)(N

max(l, l′))

−

κ4
N

−

(cid:18)

(2N

max(l, l′)

l

l′

−

(N

−

l)(N
N

−

−

l′)

−

)δ0l + (2N

max(l, l′)

l

l′

−

−

−

−

(N

−

l)(N
N

l′)

−

)δ0l′

=

(cid:19)

−
κ4
N

=

−

((N

l′)δ0l + (N

l)δ0l′ ) =

κ4

−

1
(cid:18)(cid:18)

−

l′
N

(cid:19)

δ0l +

l
N

1
(cid:18)

−

δ0l′

(cid:19)

(cid:19)

−

−

15

then a contribution to the any lag variance term, δl,l′ , which becomes

while all other terms become

+δl,l′ σ4(N

max(l, l′))

−

(2(N

max(l, l′))

)σ4 + 2(N

min(l + l′, N ))

(N

−

l)(N
N

−

l′)

−

(N

−

l)(N
N

l′)

−

)s4 +

(2(N

max(l, l′)) + 2(N

min(l + l′, N ))

−

ll′
N

−

(N

2 max(l, l′) + l + l′

)σ4 + (N

−

(N

2 max(l, l′)

2 min(l + l′, N ) + 3(l + l′

−

ll′
N

))

−

(cid:19)

−
(N

l′)

l)(N
N

3

−

−

−

)
(cid:19)
2 min(l + l′, N )) + l + l′

−

=

ll′
N

−

)s4 +

Finally, adding up the individual terms, we arrive at the ﬁnal expression for the covariance. For ll′

0,

≥

1
N

−
1
N

−

(cid:18)
κ4
N

(cid:18)
κ4
N

−

=

−

it is

−

−

−

−

1
[l]

W

W

[l′]

n

b
+ δl,l′ σ4(N

Cov

C(lw)
ǫ

[l],

C(lw)
ǫ

[l′]

= δ0,lδ0,l′ (κ4 + s4)N

δ0,lκ4

−

b
) + κ4
l
|
− |
l
2 max(
|

,
|

(cid:18)
l′
)
|
|
N

o
1
N −
l
− |

l
2 max(
|

,
|

l′
|

l′
| − |

|

ll′
N 2

−

+

l
) + 2 min(
|
|
|
N 2
l
2 min(
|

s4

|

−

(cid:19)

1
(cid:18)

−

σ4

−

1
(cid:18)

−

l′
|
|
N
−
, N )
|

1
(cid:18)
l′
|

−
(cid:19)
l
3(
|

|

−

δ0,l′ κ4

+

l′
|

+

l′
, N )
|
|
N

l
− |

l
|
|
N
ll′
N 3

−

3

1
(cid:18)
)
|

−
l′
| − |

|

−

(cid:19)

+

(cid:19)
ll′
N 2
ll′

,

(cid:19)

≥

0.

(36)

These results are comparable to [1] Theorem 8.2.6 for the case of real white noise.

For ll′

0, the analogous expression for the covariance is

≤

[l′]

1
[l]

W

W
+ δ|l||l′|s4(N

n

σ4

−

1
(cid:18)

−

Cov

C(lw)
ǫ

[l],

C(lw)
ǫ

[l′]

= δ0,lδ0,l′ (κ4 + s4)N

δ0,lκ4

o
l
2 max(
|

,
|

l′
|

b
b
) + κ4
l
− |
|
l
2 max(
|

,
|

l′
)
|
|
N

1
N −
l
− |

(cid:18)

l′
| − |

|

ll′
N 2

−

+

−
l
) + 2 min(
|
|
|
N 2
l
2 min(
|

l′
|

s4

−

(cid:19)

1
(cid:18)

−

1
(cid:18)
, N )
|

−

−

l′
|
|
N
(cid:19)
l
3(
|

|

−
+

+

|

l′
, N )
|
|
N

l
− |

δ0,l′ κ4

l′
|

)
|

−
l′
| − |

|

l
|
|
N

+

(cid:19)

−

1
(cid:18)
ll′
N 3

3

(cid:19)
ll′
N 2
0.

,

(cid:19)

(37)

−

ll′

≤

4.3.4 Mean squared error

From the bias and the covariances one can determine the mean square error. From the deﬁnition of the
MSE we ﬁnd that

MSE

C(lw)
ǫ

[l]

= Var

C(lw)
ǫ

[l]

+

Bias

C(lw)
ǫ

[l]

=

W

n
2[l]

b
δ0,l
(cid:18)

o

n
(κ4 + s4)N

2κ4

b
−

o
+ σ4(N

(cid:12)
(cid:12)
(cid:12)

n
b
l) + κ4

σ4

−

(cid:0)
l2
N 2

(cid:18)

1

s4

−

−

1
(cid:18)
(cid:19)
δ0,l((κ4 + s4)N

−

2[l]

=

W

(cid:18)

−

−

(cid:1)

2 min(2l, N )

2l

N

−

2κ4) + σ4(N

−

l

1 +

−

−

2

=

o(cid:12)
1
(cid:12)
(cid:12)
N −

(cid:18)
l2
N 2
(cid:19)(cid:19)
l2
N 2 ) + κ4

2l + 2 min(2l, N )

6l

−

3

l2
N 3

−

+

N 2

+ σ4

(N

[0]

W

−

1)δ0,l

−

(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
N −
(cid:18)

2l + 2 min(2l, N )

N 2

(N

2

[l]

−

W

=

(cid:19)
l)
N
6l

−

(cid:12)
(cid:12)
l2
(cid:12)
(cid:12)
N 3

3

−

+

(cid:19)

16

=δ0l

(κ4 + s4)N

2κ4

2 min(2l, N )

2l

−

−

1
(cid:18)

−

l2
N 2
l
N

(cid:19)

l2
N 2

+ κ4

(cid:19)
2l

−

(cid:18)
l2
N 2

−

1
N −

+

(cid:19)

N

−

N

N

l

1 +

−
−
2 min(2l, N )

(cid:18)

s4

−

1
(cid:18)

−

(cid:18)(cid:18)
σ4

+

(cid:18)
s4

−

1
(cid:18)

−

thus

+ σ4

(N

2)N

−

2[0]

2(N

W

−

1)

−

W

[0] + 1

δ0,l +

(cid:19)(cid:19)
+ σ4(N

(cid:18)
(cid:0)
2)N

−

2[l]

2(N

1)σ4

[0] + σ4

2l + 2 min(2l, N )

6l

W

(cid:19)

N 2

−

−

−

3

l2
N 3

−

W

+

(cid:19)

(cid:1)
+

(cid:19)

(N

l)2σ4

−
N 2

2[l]

W

(cid:19)

(N

[l]2

−

l)2
N 2

W

=

(cid:19)

MSE

C(lw)
ǫ

[0]

= (σ4(N

1)N + (κ4 + s4)N

s4

2κ4 +

2[0]

2(N

1)σ4

[0] + σ4

(38)

MSE

n
C(lw)
b
[l
ǫ

= 0]

=

σ4(N

n

b

o

o

−

−

(cid:18)

−

s4

1
(cid:18)

−

l)(1

2l
N 2 ) +
2 min(2l, N )

−

κ4
N
2l

−

N

−

1

(cid:18)

−

−
l2
N 2

N

2[l]

W

(cid:19)(cid:19)

κ4
N

)
W

−
2l + 2 min(2l, N )

−
6l

−

W

−
l2
N 2

3

−

(cid:19)

(39)

Asymptotically i.e. N

, keeping only terms of order N or lwe ﬁnd

→ ∞

MSE

C(lw)
ǫ

[0]

= σ4N 2

2[0]

2N σ4

[0] + σ4

MSE

n
C(lw)
[l
b
ǫ

= 0]

= σ4(N

o

o

n

b

W

W

l)

−

W

−
2[l]

5 Conclusion

R(lw),
We have derived the sampling properties up to second-order of the ACS and ACVS estimators
C(lw) for a general white noise sequence ǫ[
R(f l) and
]. An interesting result we have found is that the
·
covariances of the correlograms in general have a lag dependence. This is despite the fact that the noise
] for which these covariances were derived, is not itself lag dependent.
sequence,ǫ[
b
b
·

Further conclusions based on the results derived here will be given in a following paper [7].

b

A Summation formulas

In deriving the second order sampling properties we have used the following summation formulas for the
lag windowed correlograms (with and without mean removal) in sections 4.1 and 4.3,

N −|l|

N −|l′

|

k=1
X
N −|l

k′=1
X
′
|

N −|l|

k=1
X
N −|l|

k′=1
X
N −|l′
|

k=1
X
N −|l′

k′=1
X
|

N −|l|

k=1
X

k′=1
X

δk,k′ = N

l
max(
|

,
|

−

l′
|

)
|

0

l
≤ |

| ≤

N, 0

l′
≤ |

| ≤

N

δk′,k+|l| = N

l
min(
|

|

+

−

l′
|

, N ) 0
|

l
≤ |

| ≤

N, 0

l′
≤ |

| ≤

N

δk,k′+|l′| = N

l
min(
|

|

+

−

l′
|

, N ) 0
|

l
≤ |

| ≤

N, 0

l′
≤ |

| ≤

N

δk+|l|,k′+|l′| = N

l
max(
|

,
|

−

l′
|

)
|

0

l
≤ |

| ≤

N, 0

l′
≤ |

| ≤

N

17

(40)

(41)

(42)

(43)

(44)

(45)

6
6
N −|l|

N −|l′

|

k=1
X

k′=1
X

1 = (N

l
− |

)(N
|

l′
− |

) 0
|

l
≤ |

| ≤

N, 0

l′
≤ |

| ≤

N

The following sums were used with the ﬁxed-length summation estimator in section 4.2,

L

L

k′=1
X

k=1
X
L

L

k=1
X
L

k′=1
X
L

k′=1
X

k=1
X
L

L

k=1
X

k′=1
X

δk,k′ = L 0

l
≤ |

| ≤

M, 0

l′
≤ |

| ≤

M

δk′,k+|l| = L

l
min(
|

, L) 0
|

l
≤ |

−

| ≤

M, 0

l′
≤ |

| ≤

M

δk,k′+|l′| = L

l′
min(
|

−

, L) 0
|

l
≤ |

| ≤

M, 0

l′
≤ |

| ≤

M

δk+|l|,k′+|l′| = L

min (

−

l
||

l′
| − |

||

, L)

0

l
≤ |

| ≤

M, 0

l′
≤ |

| ≤

M

L

L

k=1
X

k′=1
X

1 = L2

0

l
≤ |

| ≤

M, 0

l′
≤ |

| ≤

M

(46)

(47)

(48)

(49)

(50)

(51)

where M := N

L.

−

Acknowledgments

References

This work was sponsored by PPARC ref: PPA/G/S/1999/00466 and PPA/G/S/2000/00058.

[1] T. W. Anderson. The Statistical Analysis of Time Series. John Wiley & Sons, Inc., 1971.

[2] M. S. Bartlett. On the theoretical speciﬁcation and sampling properties of autocorrelated time-series.

Supplement to the Journal of the Royal Statistical Society, 8(1):27–41, 1946.

[3] Juilus S. Bendat and Allan G. Piersol. Random data: analysis and measurement procedures. John

Wiley and Sons, Inc, third edition edition, 2000.

[4] Giorgio Bertorelle and Guido Barbujani. Analysis of dna diversity by spatial autocorrelation. Genet-

ics, 140:811–819, 1995.

[5] R. B. Blackman and J. W. Tukey. The measurement of power spectra: from the point of view of

communications engineering. Dover Publications, Inc, 1958.

[6] A. M. Buckley, M. P. Gough, H. Alleyne, K. Yearby, and I. Willis. Measurement of wave-paricle
interaction in the magnetosphere using the particle correlator experiments on cluster. European Space
Agency, ESA SP-449:303–306, 2000.

[7] T. D. Carozzi and A. M. Buckley. Sampling errors of correlograms with and without sample mean
removal for higher-order complex white noise with arbitrary mean. To be published in Journal of
Time Series Analysis, 2005.

[8] M. P. Gough, A. M. Buckley, T. Carozzi, and N. Beloff. Experimental studies of wave-particle
interactions in space using particle correlators: results and future developments. Adv. Space Res.,
32(3):407–416, 2003.

18

[9] F. H. C. Marriott and J. A. Pope. Bias in the estimation of autocorrelations. Biometrika, 41(3/4):390–

402, 1954.

[10] Donald B. Percival and Andrew T. Walden. Spectral Analysis for Physical Applications: multitaper

and conventional univariate techniques. Cambridge University Press, 1993.

[11] Andreas Schifﬂer. Superdarn measurements of double-peaked velocity spectra. PhD thesis, University

of Saskatchewan, Canada, 1996.

[12] A. B. Smolders and M. P. van Haarlem, editors. Perspectives on Radio Astronomy: Technologies for

Large Antenna Arrays, Dwingeloo, The Netherlands, April 1999. ASTRON.

[13] Sander Weinreb. A Digital Spectral Analysis Technique and Its Application to Radio Astronomy. PhD

thesis, Massachusetts institute of technology, 1963.

19


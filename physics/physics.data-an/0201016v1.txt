2
0
0
2
 
n
a
J
 
9
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
6
1
0
1
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

ENTROPIC PRIORS FOR DISCRETE PROBABILISTIC NETWORKS
AND FOR MIXTURES OF GAUSSIANS MODELS

C. C. RODRIGUEZ

Department of Mathematics and Statistics
University at Albany, SUNY
Albany NY 12222, USA
carlos@math.albany.edu
http://omega.albany.edu:8008/

Abstract. The ongoing unprecedented exponential explosion of available comput-
ing power, has radically transformed the methods of statistical inference. What
used to be a small minority of statisticians advocating for the use of priors and a
strict adherence to bayes theorem, it is now becoming the norm across disciplines.
The evolutionary direction is now clear. The trend is towards more realistic, ﬂexible
and complex likelihoods characterized by an ever increasing number of parameters.
This makes the old question of: What should the prior be? to acquire a new central
importance in the modern bayesian theory of inference. Entropic priors provide
one answer to the problem of prior selection. The general deﬁnition of an entropic
prior has existed since 1988 [1], but it was not until 1998 [2] that it was found that
they provide a new notion of complete ignorance. This paper re-introduces the
family of entropic priors as minimizers of mutual information between the data
and the parameters, as in [2], but with a small change and a correction. The general
formalism is then applied to two large classes of models: Discrete probabilistic net-
works and univariate ﬁnite mixtures of gaussians. It is also shown how to perform
inference by eﬃciently sampling the corresponding posterior distributions.

Key words: Bayesian Belief Networks, Mixture Models, Entropic Priors, Markov
Chain Monte Carlo, MCMC, Generalized Inverse Gaussian distribution, Gamma
Approximation to GIG

1. Introduction

Entropic Priors [1,3,4,5] minimize a type of mutual information between the data
and the parameters [2]. Hence, Entropic Priors are the prior models that are most
ignorant about the data. As Jaynes used to say: they are maximally noncommit-
tal with respect to missing information. Entropic Priors (as opposed to other prior
assignments of probability) come with a guarantee: They include only the informa-

2

C. C. RODRIGUEZ

tion in the likelihood, the initial guess, the hyper-parameter and the possible side
conditions that are explicitly imposed, and nothing else. Entropic Priors provide
a general recipe for prior probabilities that allow the enjoyment of the bayesian
omelet even in high dimensional parameter spaces.

This paper presents the explicit computation of Entropic Priors for two classes
of models: General Discrete Probabilistic Networks (a.k.a. Belief Nets, Bayesian
Nets, BBNs) and for Mixtures of Gaussians Models. These models constitute the
core of the probabilistic treatment of uncertainty in AI.

The paper is divided into 5 parts. Section 2, repeats the derivation in [2] (but
with a small change and a correction) that Entropic Priors minimize mutual infor-
mation between the data and the parameters. Section 3, presents the computation
for discrete BBNs. Section 4 shows an application for classiﬁcation. Section 5 com-
putes the priors for the Mixture of Gaussians case. Finally some general remarks
and conclusions are included in Section 6.

2. Entropic Priors are Most Ignorant Priors

Given a regular parametric hypothesis space, i.e. a Riemannian manifold of domi-
nated probability distributions with volume element g1/2(θ)dθ. Where g(θ) is the
θ) the density (with
determinant of the Fisher information at θ. We denote by f (x
|
respect to either Lebesgue or counting measure) of the distribution indexed by θ
and by π(θ) a prior density on the parameters θ. The entropic prior is the π that
makes the joint distribution

(1)

(2)

f (x1, . . . , xα, θ) = π(θ)

α

j=1
Y

f (xj|

θ)

hardest to discriminate (in the sense of minimizing the Kullback number) from
the independent model,

h(x1, . . . , xα)cg1/2(θ)

g1/2(θ)

h(xj)

∝

α

j=1
Y











for a given ﬁx density h(x) on the data space. Where c is a normalization constant
independent of θ and the xjs. Notice that c > 0 when the parameter space has ﬁnite
volume. However, the solution to the optimization problem (5) (and hence, the
entropic prior) does not depend on c and still makes sense for models with inﬁnite
volume. Notice further that the setting is coherent in the sense that the rhs of (2) is
in fact proportional to the density of the model that assigns probabilities to the xs
according to h and independently of the θ which, according with (2), is uniform
over the surface area of the model. This is true since Fisher information in the
hypothesis space of α independent observations is α times the Fisher information
in the hypothesis space of one observation and thus the volume element in the space
of α observations is αk/2g1/2(θ). i.e., the two volume elements are proportional and
we assume the proportionality constant is included in c.

3

(3)

(4)

(5)

(6)

ENTROPIC PRIORS FOR BBNS AND MIXTURES

To simplify the notation let xα = (x1, . . . , xα) and write,

I(θ : h) =

Z

θ) log
f (x
|

θ)
f (x
|
h(x)

dx

I(f π : hg1/2) =

f (xα

Z

θ)π(θ) log
|

f (xα
θ)π(θ)
|
h(xα)cg1/2(θ)

dxαdθ

and

We have,
Theorem 1

where the minimum is taken over all the proper priors on the parameter space, is
given by the entropic prior:

π∗ = argmin

I(f π : hg1/2)

π

π∗(θ

α, h)
|

∝

e−αI(θ:h)g1/2(θ)

Proof Using Fubbini’s theorem, (1),(2) and the fact that π integrates to one, we
can write

I(f π : hg1/2) = α

π(θ)I(θ : h)dθ +

π(θ) log

dθ

log c.

(7)

π(θ)
g1/2(θ)

−

Z

Z

Therefore using a Lagrange multiplier to enforce the normalization constraint
(

π = 1) we can ﬁnd π∗ by solving:

R

argmin
π

Z (cid:26)

απ(θ)I(θ : h) + π(θ) log

+ λπ(θ)

dθ

(8)

π(θ)
g1/2(θ)

(cid:27)

(π, λ) denote the expression inside the curly brackets in (8). The Euler-

Let
Lagrange equation for the optimal π∗ is ∂

L

L∂π = 0 given by,

αI + log π∗

log g1/2 + λ + 1 = 0.

(9)

−
From where we obtain the expression for the entropic prior given by (6).
Q.E.D.

2.1. BUT WHAT DOES IT MEAN?

First of all it needs to be clear that the above analysis is logically a priori. By
this I mean that the actual numerical values of the observed data are not used,
nor is the actual sample size number n of observed i.i.d. data vectors used. The
parameters α and h of the entropic prior are the carriers of prior information.
Notice also that, since the derivation was done on a virtual and not actual space
of α observations, it makes sense to allow α to take non integer values as long as
α > 0. In fact an irrational α′ is immediately obtained if we decide to change (in
the ﬁnal formula for the entropic prior) the entropy scale to bits by changing the
original base of the logarithm in I(θ : h) from e to 2 so that α′ = α log 2. It is

4

C. C. RODRIGUEZ

however incorrect to claim that by starting the derivation with another base for
the logarithm one will end up with a non integer α′ as it was wrongly claimed in
[2]. In fact the objective functions are proportional and they obviously produce
the same π∗. To see the source of the mistake one just needs to notice that when
the base of the log in (9) is 2 say, one has to exponentiate 2, and not e, in order
to solve for π∗. This was ﬁrst pointed out to me by Ariel Caticha, who then tried
to build a justiﬁcation for an entropic prior with ﬁx α = 1 in [6].

2.1.1. Imaginary α

Allowing α to be not just a real number but a Cliﬀord number, in particular
to be a pseudo scalar, opens up a garden of unexplored possibilities. This may
not be as insane as it ﬁrst appears to be, if one thinks of the resulting prior
as the density of a Cliﬀord valued probability measure (see [7]). Moreover, if I
(entropy) could be justiﬁed as S (action) then the resulting prior eiS/¯h (relative to
local ignorance) would take a familiar form. Going with the ﬂow of this (for now)
applied numerology this would point to current physical theory to be based on
the order of 1066 equivalent a priori observations! (i.e. expressing ¯h in geometrized
units).

2.2. RECIPES FOR CHOOSING α AND h

The values of the hyperparameters α and h of the entropic prior need to be ﬁxed
in order to obtain numerical assignments of probabilities. To ﬁx h we need to
specify a function (i.e. an a priori density h(x) for the data) which involves, in
principle, the speciﬁcation of an inﬁnite number of parameters. Nevertheless, the
importance of the a priori biases introduced by h are modulated by the value of
the real positive parameter α. Take α suﬃciently close to 0 and the prior will
be blind to the speciﬁc form of h and controlled by the volume element g1/2dθ
(i.e. uniform over the model surface, see [8]). There is a close similarity with the
problem of choosing a kernel and a bandwidth in density estimation. As it is the
case in density estimation, the speciﬁc form of the kernel is not as critical as the
choice of the smoothness parameter. A natural choice for h is to use h(x) = f (x
θ0)
|
where θ0 is the best current guess for the value of θ. If we assume the value of
θ0 to be unknown then we can consider the entropic prior model, which is now
indexed by the 1 + k parameters (α, θ0), to be another regular hypothesis space
that needs a prior on its parameters. The entropic prior on the entropic prior, on
the entropic prior,. . ., etc is, in principle, computable. The possibility of a chain
of entropic priors for α was ﬁrst given to ﬁrst level in [1] and for all levels in [4].
Another general alternative is to use the empirical bayes approach (see [5]). Finally,
1) and using ˆθ0 the mle (maximum
just ﬁxing α to an arbitrary small value (
likelihood estimator) or MAP (Maximum A posteriori Probability), with an easy
to handle conjugate prior, for θ has been shown to perform well in simulation
experiments.

≈

ENTROPIC PRIORS FOR BBNS AND MIXTURES

5

Figure 1. DAG for the Sprinkler Problem

3. The Entropic Prior of a Discrete Probabilistic Network

An understanding of Cox’s [9] argument should be suﬃcient to impose the rules
of probability to the treatment of uncertainty in AI. But it has taken, however,
a long heated debate (see [10] and [11]), the invention of new eﬃcient methods
of computation (e.g. the junction tree algorithm, see [12]) and the publication of
Pearl’s text [13], to arrive at today’s dominant view of a complete probabilistic
approach.

3.1. DAGS

The current recipe for the thinking machine consists of a fully bayesian probabilis-
tic treatment of a long vector of facts (the data). The main approach for encoding
prior information about an speciﬁc domain of application, is not the prior, but
the likelihood. An a priori network of conditional independence assumptions is
typically provided by means of a Directed Acyclic Graph (DAG) that is supposed
to encode an expert’s knowledge of causal relations among observable facts.

The canonical textbook example is displayed in ﬁg 1. The arrows indicate
causality. Thus, the presence of the arrow from Cloudy to Rain represents the fact
that the sky being cloudy is a possible cause for rain. More important is the absence
of arrows which indicate independence. Thus, the picture shows that conditionally
on the values of Sprinkler and Rain, Cloudy is independent of WetGrass. The
entries of the tables of conditional probabilities constitute the parameters of the

6

C. C. RODRIGUEZ

2

3

1

4

5

Figure 2. Example of a DAG

DAG. In the case of ﬁg 1 there are 9 independent parameters. We can think of a
DAG as a convenient way to specify a high dimensional submanifold of the space
of all joint distributions of the variables under consideration. For example, the
pictured DAG (with unspeciﬁed tables) represents a 9 dimensional submanifold
of the 15 dimensional simplex of all the assignments of probability on the 24 =
16 possible observations of the binary variables (C, S, R, W ). The DAG in ﬁg 1
speciﬁes the joint distribution of all the variables (C, S, R, W ) in terms of the
parameters θ (i.e. table entries) as,

P (C, R, S, W ) = P (C)P (R

C)P (S
|

R)P (W
|

R, S).
|

(10)

Each of the factors on the right of equation (10) can be read oﬀ the tables provided
in ﬁg 1. For example,

P (C = T, R = T, S = F, W = F ) = (0.5)(0.8)(0.5)(0.1) = 0.02

(11)

In order to provide general formulas for DAGs we number the vector of variables
by x = (x1, x2, x3, x4) = (C, R, S, W ) and parameterized the joint distribution
with a vector θ of parameters as in,

θ4w(r, s) = P (W = w

R = r, S = s)
|

Thus, labeling F = 1 and T = 2, (11) becomes,

P (2, 2, 1, 1

θ) = θ12θ22(2)θ31(2)θ41(2, 1)
|

(12)

(13)

3.2. WHO IS WHO ON A DAG: GENERAL NOTATION

This section provides some deﬁnitions and notations that are needed for writing
the entropic prior on a general DAG. All the examples refer to ﬁg 2.

ENTROPIC PRIORS FOR BBNS AND MIXTURES

7

Directed Graph: An ordered pair (V, E) where V is a set of vertices (e.g. V =

1, 2, 3, 4, 5

) and E

V

V is a set of directed edges. e.g.,

{

}

⊂

×

E =

(1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 5)
}

{

DAG: A Directed Acyclic Graph is a directed graph without cycles. (e.g. ﬁg 2).
Parents: pa(k) denotes the set of parents for the vertices k
V . (e.g. pa(1) =

φ, pa(5) =

4

, pa(4) =

1, 2, 3

).

Ancestors: an(k) denotes the set of ancestors of k
, an(1) = φ). Clearly,

1, 2, 3, 4

∈

{

}

{

}

{

}

∈

V . (e.g. an(2) =

1

, an(5) =

{

}

an(k) = pa(k)

an(j)

[j∈pa(k)

Ancestors that are not Parents: Denoted by ap(k)

(e.g. ap(5) =

1, 2, 3

{

}

Notation:

e.g.

ap(k) = an(k)

pa(k)

\
, ap(4) = φ, ap(2) = φ).

xpa(k) ≡ {

xj : j

pa(k)
}

∈

xpa(1) = φ, xpa(4) =

x1, x2, x3}

{

(14)

(15)

(16)

Notation:

denotes the multiple sum over all the possible values of the

xpa(k)
X

variables that are parents of vertice k

V . e.g.

∈

≡

xpa(4)
X

x1 X
X

x2 X
x3

The notation introduced with equation (13) generalizes naturally for any num-

ber of discrete variables. Given a DAG with set of vertices V we let x =
V

. Hence, the joint distribution of the variables of a given DAG is given by,

xk : k

{

∈

}

θ) =
p(x
|

p(xk|

xpa(k), θ)

Yk∈V

Yk∈V

=

θkxk (xpa(k))

(17)

We are now ready to compute.

3.3. ENTROPY OF A DAG

Given a DAG, the Kullback number between two sets of parameters θ and µ is,

I(θ : µ) = Eθ

log

θ)
p(x
|
µ)
p(x
|

(cid:21)

(cid:20)

(18)

8

C. C. RODRIGUEZ

Using (17) and interchanging expectation with summation we obtain,

I(θ : µ) =

Eθ

log

Xk∈V

(cid:20)

θkxk (xpa(k))
µkxk (xpa(k))

(cid:21)

(19)

Now for each k
tioning on the values of xpa(k) to obtain,

∈

V compute the unconditional expectation in (19) by ﬁrst condi-

θkxk (xpa(k))
µkxk (xpa(k))

Eθ

log

(cid:20)

xpa(k)

=

θkj (xpa(k)) log

θkj (xpa(k))
µkj(xpa(k))

rk

j=1
X

= I(θk(xpa(k)) : µk(xpa(k)))

(20)

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last equality is a deﬁnition and it was assumed that xk can take rk
discrete values. Taking expectations over the xpa(k) and replacing in (19) we obtain,

I(θ : µ) =

p(xpa(k)|

xpa(k)
Xk∈V X

(cid:0)

θ) I

θk(xpa(k)) : µk(xpa(k))

.

(21)

(cid:1)

Finally, using the fact that,

p(xpa(k)|

θ) =

p(xap(k), xpa(k)|

θ)

=

=

xap(k)
X

xap(k) Yj∈an(k)
X

xap(k) Yj∈an(k)
X

p(xj|

xpa(j), θ)

θjxj (xpa(j))

(22)

we obtain the expression for the entropy,

I(θ : µ) =

θjxj (xpa(j))

I(θk(xpa(k)) : µk(xpa(k))).

xpa(k)
Xk∈V X

xap(k) Yj∈an(k)
X









(23)
Thus, formula (21) shows that the total entropy for a DAG is obtained by adding
the entropies for each node. The entropy of a node is computed as an average of
all the possible entropies obtained for the diﬀerent values of the parents of that
node. In practice formula (23) may be too expensive to compute and it may be
necessary to use a Monte Carlo estimate.



3.4. VOLUME ELEMENT OF A DAG

To compute the Fisher metric, write θ as a long vector and use the fact (see [5])
that,

I(θ : θ + ǫv) =

gij(θ)vivj + o(ǫ2)

(24)

ǫ2
2

i,j
X

ENTROPIC PRIORS FOR BBNS AND MIXTURES

9

It then follows immediately from (23) that the Fisher matrix is block diagonal.
1) (Fisher matrix Gk(θk(xpa(k)))
Each block corresponds to the (rk −
θ). The determinant,
associated to the kth node, multiplied by the scalar p(xpa(k)|
g(θ), of the Fisher matrix is then given by the product of the determinants of each
of the blocks. We have

(rk −

1)

×

g(θ) =

θjxj (xpa(j))

det Gk

θk(xpa(k))

(25)

xpa(k)
Yk∈V Y

xap(k) Yj∈an(k)
X




(cid:0)

(cid:1)


Finally using the fact that Gk is the Fisher matrix of a multinomial with param-
eters θk1(xpa(k)), . . . , θkrk (xpa(k)) we have,



rk−1




det Gk

θk(xpa(k))

=

rk

1

(cid:0)

(cid:1)

θkj (xpa(k))

j=1
Y

(26)

replacing (26) in (25) and taking square root we obtain the expression for the
volume element,

g1/2(θ) dθ =

dθ

(27)

(rk−1)/2

xap(k) Yj∈an(k)
X

rk






xpa(k)
Yk∈V Y

θjxj (xpa(j))






θ1/2
kj (xpa(k))

j=1
Y

3.5. THE ENTROPIC PRIOR FOR A DAG

To obtain (6) we use (23), (22) and (27) to get,

π(θ

α, µ)
|

∝

1/2





p(rk−1)(xpa(k)|
θ)
θkj (xpa(k))

rk



θ)I(θk(xpa(k)) : µk(xpa(k)))


j=1
Y

αp(xpa(k)|

xpa(k)
Yk∈V Y

exp

−

(cid:8)

(28)

(cid:9)

3.6. POSTERIOR

Let us assume that there is available a set of N independent observations

D =

x(1), x(2), . . . , x(N )

{

}

(29)

10

C. C. RODRIGUEZ

where each x(t) = (x(t)
= n dimensional vector containing the
V
|
|
observed values of the nodes of a general DAG (V, E). As usual the posterior is
given by Bayes theorem as,

1 , . . . , x(t)

n ) is an

where the likelihood is given by,

π(θ

D, α, µ)
|

∝

f (D

θ) π(θ
|

α, µ)
|

(30)

f (D

θ) =
|

f (x(t)

θ)
|

N

t=1
Y
N

n

=

=

t=1
Y
N

Yk=1
n

f (x(t)
k |

x(t)
pa(k), θ)

θkx(t)

k

(x(t)

pa(k))

(31)

(32)

(33)

(34)

Yk=1
Let us partition the set of vertices into two groups, those with parents and those
without (orphans). For the orphan nodes, i.e. for k
V such that pa(k) = φ and
for i = 1, 2, . . . , rk deﬁne

∈

t=1
Y

and for k

V with pa(k)

= φ and i = 1, 2, . . . , rk

∈

nki(φ) =

t : x(t)

1 = i

{

(cid:12)
(cid:12)
(cid:12)

}

(cid:12)
(cid:12)
(cid:12)

nki(xpa(k)) =

t : x(t)

k = i and x(t)

pa(k) = xpa(k)}

Replacing these counts into (31) we obtain,

{
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

f (D

θ) =
|

n

rk

xpa(k)
Yk=1 Y

i=1
Y

θki(xpa(k))

nki(xpa(k))

(cid:8)
To simplify the notation let us write simply by pk the expression (22) which is
always a probability that depends only on the ancestors of the node k. Let us also
just write θki, nki, µki instead of θki(xpa(k)), . . . and keep implicit their dependence
on given values of the parents. With this notation the posterior becomes,

(cid:9)

π(θ

D, α, µ)
|

∝

p(rk−1)/2

k

nki− 1
2
ki

rk

θ
i=1 (cid:26)
Y

xpa(k)
Yk∈V Y

−

(cid:18)

exp

αpkθki log

(35)

θki
µki (cid:19)(cid:27)

were we have used (20) to write the exponential in (28) as a product of rk factors.

4. Example: Na¨ive Bayes

When the DAG has the form shown in ﬁg 3 the general formulas have simpler
forms. This case is known as na¨ive bayes and it is often used as an approximation

6
ENTROPIC PRIORS FOR BBNS AND MIXTURES

11

1

2

3

4

...

n

Figure 3. DAG for Na¨ive Bayes

in discrimination problems. For this case, V =
we have pa(k) =

, an(k) =

, ap(k) = φ and,

1

1

{

}

1, . . . , n

, pa(1) = φ, and for k

= 1

{

}

{
}
p(xpa(k)|

θ) = p(x1|
The expression for the entropy (23) becomes,

θ1) = θ1x1

(36)

I(θ : µ) = I(θ1, µ1) +

θ1j I(θk(j) : µk(j))

(37)

n

r1

Xk=2
and the volume element (27) reduces to,

j=1
X

g1/2(θ) dθ =

1/2 dθ

(38)

(

n

k=2

rk−1)/2

r1





j=1
Y
r1

P

θ1j


n

rk





j=1
Y

Yk=2

i=1
Y

θki(j)





The entropic prior is then easily computed by multiplying exp(
tained from (37)) by (38).

−

αI(θ : µ)) (ob-

4.1. POSTERIOR

For na¨ive bayes the likelihood is given by,

N

n

f (D

θ) =
|

θ1x(i)

1

i=1
Y

Yk=2

θkx(i)

k

(x(i)
1 )

(39)

6
12

C. C. RODRIGUEZ

Replacing the counts into (39) we obtain,

Letting,

f (D

θ) =
|

θn1j
1j 

r1





j=1
Y

r1

n

rk

j=1
Y

Yk=2

i=1
Y

(θki(j))nki(j)








n



1
2  

Xk=2

m =

rk −

n

!

we can write the posterior as,

r1

∝ 


j=1
Y

π(θ

D, α, µ)
|

r1

n

rk

j=1
Y

Yk=2

i=1
Y






θm+n1j −αθ1j

1j

exp

(α log

)θ1j

1
µ1j

−

(cid:18)




(cid:19)



(θki(j))nki(j)− 1

2

−αθ1j θki(j) exp

(αθ1j log

)θki(j)

1

µki(j)

−

(cid:18)

4.2. THE ENTROPIC SAMPLER

(40)

(41)

(42)




(cid:19)



A combination of Gibbs and Metropolis can be used for sampling the posterior
(42). The parameters are naturally grouped in blocks θk, where,

θk = θk(xpa(k))

= (θk1, . . . , θkrk ) with

θki = 1

(43)

rk

i=1
X

1. It can be readily seen from
are distributed over the simplex of dimension rk −
(42) that the marginal joint distributions of the θk blocks are all of the generic
form,

f (y1, y2, . . . , yr−1)

r

∝

j=1 n
Y

yαj −1

j

e−βj yj

o

0 and yr = 1

with yj ≥
parent node and for the children nodes. For the parent,
P

−

r−1
j=1 yj. The parameters αj and βj are diﬀerent for the

For the children blocks the parameters are,

αj = 1 + m + n1j −
βj = α
+

log

1
µ1j

 

αθ1i ≈

n

1 + m + n1j

I(θk(j) : µk(j))

Xk=2

αj = nki(j) +

αθ1j θki(j)

nki(j) +

≈

βj = αθ1j log

1
2 −
1
µki(j)

!

1
2

(44)

(45)

(46)

(47)

(48)

Excellent initial distributions for Metropolis are obtained by using the following,

ENTROPIC PRIORS FOR BBNS AND MIXTURES

Lemma 1 Let y1, y2, . . . , yr be independent with yj following a Gamma distribu-
tion with parameters (αj, βj). Let,

zj =

y1 +

+ yr

for j = 1, . . . , r

1

−

yj

· · ·

then the joint density of the zj’s is given by,

f (z1, . . . , zr−1)

∝

r

zαj−1

j

r

j=1
Y

α1+α2+···+αr





j=1
X

βjzj


1

−

zr−1.

z1 −

z2 − · · · −

where zr ≡
Proof Notice that (50) is a generalization of the classic result for the Dirichlet
distribution obtained when all the βj’s are equal, in which case the denominator
becomes proportional to 1. To prove (50) just condition on yr = y so that the
transformation (49) from the yj’s to the zj’s for j = 1, 2, . . . , r
1 is one to one
with inverse,

−

To show (51) just notice that,

yj =

for j = 1, . . . , r

1

−

yzj
zr

y zj =

yj y

y +

r−1
i=1 yi

= yj

P
1
−

 

r−1
i=1 yi
r−1
i=1 yi !

y +
P
r−1

P
zi

!

1

= yj

 
= yj zr

−

i=1
X

13

(49)

(50)

(51)

(52)

(53)

(54)

(55)

where we have used (49) and the deﬁnition of zr. The probability density of ob-
serving z1, . . . , zr−1 is then,

f (z1, . . . , zr−1) =

f (z1, . . . , zr−1|

yr = y)gr(y) dy

(56)

∞

0
Z

where gj for j = 1, . . . , r are the gamma densities of the yj. Using the deﬁnition
of the zj’s given in (49), the assumed independence of the yj’s, and the change of
variables theorem together with (62), we have,

f (z1, . . . , zr−1|

yr = y) =

r−1





j=1
Y

gj

y zj
zr (cid:19)

(cid:18)

1
zr (cid:18)

y
zr (cid:19)

r−1





(57)

14

C. C. RODRIGUEZ

The expression outside the product is the determinant of the Jacobian of the
transformation (51). This can be seen by noticing that the Jacobian matrix is,

and compute its determinant by subtracting from each column the column that
follows, to obtain,

z1 + zr

z1

z2

z2 + zr

z1

z2

zr−1

zr−1

zr−1 + zr

J =

y
z2
r











. . .

. . .
. . .

. . .

. . .

. . .
. . .

. . .

zr

zr

−

0

zr

0

0

z1

z2

0

0

zr

−

zr−1 + zr











(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

det J =

y
z2
r (cid:19)

(cid:18)

r−1 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r−1

r−1

(cid:18)

y
z2
r (cid:19)
y
z2
r (cid:19)
y
zr (cid:19)

(cid:18)
1
zr (cid:18)

(cid:0)
zr−2
r

r−1

=

=

and expanding along the last column,

det J =

z1zr−2

r + z2zr−2

r + . . . + zr−2zr−2

r + (zr−1 + zr)zr−2

r

This proves (57). Replacing (57) into (56) and using the expressions for the gamma
densities we obtain,

r

j=1

αj −1

r

∞

zαj−1
j 

1
zr Z

y
zr (cid:19)P

∝ 

f (z1, . . . , zr−1)

βjzj
(61)

this is a simple gamma integral. Integrating out and simplifying the zr’s we obtain
the desired result (50).
Q.E.D.

0 (cid:18)

j=1
X

j=1
Y

− 




exp









y






To generate approximate samples from (44) we use the Lemma but with ˜βj

r

1
zr

dy

chosen so that,

(58)

(59)

(cid:1)

(60)

(62)

C

α1+···+αr ≈

exp

r



−



j=1
X

βjzj


r

˜βjzj






j=1
X

ENTROPIC PRIORS FOR BBNS AND MIXTURES

15

where the constant C does not depend on the zj. To ﬁnd the ˜βj just write the left
side of (62) in exponential form and use,

log( ˜β1z1 +

+ ˜βrzr) = log( ˜βr) + log

1 +

· · ·

˜βr

˜β1 −
˜βr

z1 +

+

· · ·

˜βr

˜βr−1 −
˜βr

zr−1

 

we obtain, that in order for (62) to be true, we must have,

together with,

we can then use,

!
(63)

(64)

(65)

(66)

(67)

Metropolis corrections are needed to correct for the approximations introduced in
(45), (47) and (64).

4.3. TEST: CREDIT CARD CLASSIFICATION EXAMPLE

We tested the performance of the MCMC sampler on a standard set of 10000 data
records containing the 13 variables in table 1. Most of the node names are self

log(1 + z) = z + o(z)

r

˜βr

˜βj −
˜βr

i=1
X

αi = βj −

βr

r

˜βr =

αi

i=1
X
βr + ˜βr

˜βi = βi −

Nodes

Sizes

2(4)
Card = C
2
Gender = G
3
Country = Y
9
Age = A
13
State = S
5
Education = E
2
Marital = M
5
Occupation = O
6
Total children = T
8
Income = I
2
House owner = H
Cars owned = R
5
Children home = N 6

TABLE 1. Data Records in Ex-
ample

16

C. C. RODRIGUEZ

explanatory. Card, originally contained the type of credit card owned by the indi-
vidual with categories: no card, regular, gold, platinum. These were later reduced
. The data con-
to only two categories:
tains individuals from the three north american countries: Mexico, US, Canada.
However, the majority of records are from the US. The Children home variable
contains information about the actual number of children living at home with the
individual.

no card, regular
}

gold, platinum

and

{

}

{

4.3.1. The Bayes Classiﬁer
To test the performance of the entropic sampler we chose at random 100 individuals
to be used as the observed data and 1000 to test the bayes classiﬁer. The bayes
classiﬁer simply assigns the category with highest posterior probability.

Let D be the observed N = 100 records and let x2, . . . , xn (here n = 13) be
the values of all the nodes except the ﬁrst (i.e. Card) for an individual that we
want to classify. The bayes classiﬁer allocates x1 = 1 if,

P (x1 = 1

x2, . . . , xn, D) > P (x1 = 2
|

x2, . . . , xn, D)
|

(68)

we compute both sides with,

P (x1 = j

x2, . . . , xn, D) =
|

P (x1 = j, θ

x2, . . . , xn, D) dθ
|

Z

Z

Z

∝

=

P (x1 = j, x2, . . . , xn, θ

D) dθ
|

P (x1 = j, x2, . . . , xn|

θ) π(θ

D) dθ
|

(69)

where we have assumed that the values of the individual to be classiﬁed are inde-
pendent of the observed data D. We use the MCMC sampler to estimate (69) for
j = 1 and j = 2. Thus, if the sampler produces θ(1), . . . , θ(M) samples from the
posterior π(θ

D) we classify x1 = 1 if,
|

M

t=1
X

M

t=1
X

p(1, x2, . . . , xn|

θ(t)) >

p(2, x2, . . . , xn|

θ(t))

(70)

To avoid underﬂows it is better to use only ratios. A more stable rule is then:
assign x1 = 1 if,

M

1
t=1 (cid:18)
X

−

p(2, x2, . . . , xn|
p(1, x2, . . . , xn|

θ(t))
θ(t))

(cid:19)

p(1, x2, . . . , xn|
p(1, x2, . . . , xn|

θ(t))
θ(1))

> 0

(71)

4.3.2. Preliminary Results
Table 2 shows the results of running the sampler with diﬀerent parameter values.
The burn column contains the number of complete sweeps performed and discarded
before collecting samples. The other columns are: M the number of thetas sampled,

ENTROPIC PRIORS FOR BBNS AND MIXTURES

17

burn M

N

inter Met

% succ.

100
200
1000
1000
100

100
100
200
200
100

100
100
100
100
50

50
100
50
100
50

[30 15]
[5 2]
[2 2]
[1 1]
[1 1]

α

10
0.1
1.0
1.0
1.0

82.7
81.2
78.4
79.0
76.3

TABLE 2. Summary of Simulations

N the observed sample size, inter the number of discarded sweeps between samples,
Met is the number of metropolis step corrections for the root node and for the
children nodes, α is the parameter of the entropic prior and ﬁnally, % succ. is the
percentage of correct classiﬁcations on 1000 random tests.

Notice that the metropolis corrections seem to help but they slow down the

sampler. Notice also the drop in performance when the sample size becomes 50.

These results show the adequacy of the entropic sampler for the classiﬁcation
task. However, the na¨ive bayes DAG is not competitive with DAGs containing
more realistic structure for this problem. A simulated annealing search over the
space of DAGs produces structures showing over 84% success rate in the more
diﬃcult task of classiﬁcation with 4 (not just 2) categories of credit card.

5. Entropic Prior for Mixtures of Gaussians

The need for ﬂexible, informative, proper priors for mixtures has been in the
statistician’s wish list for a long time (e.g. see [14]). In this section we derive, from
ﬁrst principles, the entropic prior for a ﬁnite mixture of gaussians. This seems to
be the ﬁrst informative prior for mixtures, derivable from an objective principle.
The straight forward application of (6) produces a prior that on the one hand
is remarkably close to the conjugate prior that has been shown most successful
in simulations, and on the other hand, departs from it in a way that has always
thought to be desirable but for which there was no known way to implement.

5.1. THE MODEL

We consider a ﬁnite mixture of k univariate gaussians with vector of parameters
IRk is the vector of k means, σ
+ is the vector of k
θ = (µ, σ, ω) where µ
standard deviations and ω
1)-
dimensional simplex ∆k−1. We use the standard missing data model for mixtures,
1, 2, . . . , k
i.e., we assume the data is (x, z) has joint density, for x
given by,

∆k−1 is the mixing probability vector in the (k

IR and z

IRk

∈ {

−

∈

∈

∈

∈

}

f (x, z

θ) = ωzN (x; µz, σz)
|

(72)

where N (x; a, b) denotes the density of the normal distribution with mean a and
standard deviation b. The label z is assumed to be missing from the data so that

18

C. C. RODRIGUEZ

the marginal density of x has the desired mixture form,

θ) =
f (x
|

ωjN (x; µj , σj)

(73)

k

j=1
X

The trick is to compute the prior on the complete (x, z) likelihood to disentangle
the expression for the entropy.

5.2. ENTROPY

Let θo = (m, s, ωo) be the initial guess for θ. The Kullback number between two
distributions (72) with parameters θ and θo is,

I(θ : θo) = Eθ

log

ωzN (x; µz, σz)
ωo
zN (x; mz, sz)

(cid:19)

(cid:18)

(74)

Computing the expectation by ﬁrst conditioning on z we obtain,

I(θ : θo) =

ωj

I(N (µj , σ2

j ) : N (mj, s2

j )) + log

ωj
ωo

j )

=

ωj

log

+

sj
σj

mj)2

(µj −
2s2
j

+

σ2
j
2s2
j −

1
2

+ log

(75)

ωj
ωo

j )

k

j=1
X
k

j=1
X

(

(

Notice that since
absorbed into the proportionality constant for the entropic prior.

k
j=1 ωj = 1 we can take the 1/2 outside the sum and it will get

P

5.3. VOLUME ELEMENT

Using (24) we can immediately obtain from (75) the entries of the Fisher matrix.
The matrix is clearly block diagonal with gaussian blocks for the (µ, σ) parameters
and a multinomial block for the ω parameters. From the standard volume elements
for gaussians and multinomials we can write the full volume element as,

g1/2(θ) dθ =

dµ dσ dω

k
j=1 σ2
j

k
j=1 ωj

1/2

(cid:16)Q
where we are abusing the notation a bit since dω must be understood as
so that ω

∆k−1.

(cid:17) (cid:16)Q

(cid:17)

(76)

k−1
j=1 dωj

Q

∈

5.4. ENTROPIC PRIOR

Just multiply e−αI(θ:θo) with (76) to get,

π(θ

α, θo)
|

∝

exp

αωj

(−

mj)2

(µj −
2s2
j

) ·

k

j=1
Y

ENTROPIC PRIORS FOR BBNS AND MIXTURES

19

k

σ2
j

αωj

2

−1

αωj

j=1
Y
k

(cid:0)

j=1 (cid:18)
Y

(cid:1)
ωo
j
sj (cid:19)

exp

αωj
2s2
j

(−

σ2
j

) ·

−αωj −1/2

ωj

ω ; N

mj,

 

ω ; Gamma

µj|

σ2
j |

s2
j
αωj !
αωj −
2

 

1

,

αωj
2s2

j !

This is a remarkable result. Equation (77) says that conditional on ω all the com-
ponents of µ and σ are independent and independent of each other. Moreover,

where to obtain (79) we have used the change of variables v = σ2
j that produces
the jacobian v−1/2. The joint marginal density of ω is obtained by integrating (77)
over µ and σ coordinates obtaining, up to a proportionality constant that,

1)/2)
j )(αωj −1)/2 ·

ωo
j
sj (cid:19)

(cid:18)

αωj

−αωj −1/2

ωj

)

ω ;

k

j=1 (
Y
k

Γ((αωj −
sj
1/2 ·
(αωj/s2
ωj
j )αωj Γ((αωj −
(ωo
(3αωj +1)/2

ωj

1)/2)

;

j=1
Y

5.5. POSTERIOR

Let xn = (x1, . . . , xn) be the observed data and let zn be the missing labels. As
usual we shake the bayesian wand to obtain,

π(θ, zn

xn, α, θo)
|

∝

f (xn
n

θ, zn)f (zn
|

∝  

i=1
Y
1, 2, . . . , n
For j = 1, . . . , k deﬁne kj ∈ {

α, θo)
θ)π(θ
|
|
xi)2
(µzi −
2σ2
zi

−

1
σzi

exp

(cid:26)

n

(cid:27)!  

i=1
Y

ωzi

π(θ

!

α, θo)
|

by,

}
i : zi = j

}|

kj =

|{

and replacing these counts into (81) we have,

π(θ, zn

xn, α, θo)
|

∝

(µj −

mj)2

π(θ

α, θo)
|

(83)

k

j=1
Y






kj

ωj
σkj
j

exp

1
−
2σ2
j

i:zi=j
X
















(77)

(78)

(79)

(80)

(81)

(82)

20

C. C. RODRIGUEZ

5.6. GIBBS SAMPLER

Inference is done by sampling (θ, zn) vectors from the posterior (83). To sample
from (83) we use Gibbs sampling, i.e. we cycle over the full conditionals for each of
the parameters. Let us use the notation
. . . to mean given all the other parameters
and the data. Here are the distributions for each of the terms:

|

5.6.1. Conditional for zn
When the vector of mixing probabilities ω is given the joint distribution of zn are
independent multinomials with ω as the parameter and independent of everything
else. Thus, for i = 1, 2, . . . , n

. . . ; Multi(ω1, ω2, . . . , ωk)

zi|

5.6.2. Conditional for µ
Here again we have the classic problem of computing the posterior distribution for
the mean of a gaussian given kj independent gaussian observations when the prior
is the conjugate gaussian. Looking at the ﬁrst term of (77) and the right hand side
of (83) we get,

where,

and

. . . ; N (aj, b2
j )

µj|

aj =

1
σ2
j

xi +

mj

αωj
s2
j

i:zi=j
X
kj
σ2
j

+

αωj
s2
j

1
b2
j

=

+

kj
σ2
j

αωj
s2
j

5.6.3. Conditional for σ
Collecting all the factors with σj from (83) and the second term from (77) we
obtain,

. . . ; (σ2
j )

1

2 (αωj −kj )−1 exp

σj|

(µj −

mj)2

αωj
2s2
j

−

1
−
2σ2
j






i:zi=j
X

v−1/2

1
2

fv(v) = fσj (√v)

σ2
j 




Now let v = σ2

j , then

Using (89) with (88) we get,

v = σ2
j |

. . . ; v−a−1 exp

c
−
v −

bv

(cid:27)

(cid:26)

(84)

(85)

(86)

(87)

(88)

(89)

(90)

21

(91)

(92)

(93)

(94)

ENTROPIC PRIORS FOR BBNS AND MIXTURES

where,

a =

(kj + 1

αωj)

−

1
2
αωj
s2
j
1
2

b =

c =

(µj −

xi)2

i:zi=j
X

fu(u) = fv(u−1)u−2

We can obtain a useful alternative to (90) by doing u = 1/v so that

and we get,

u = σ−2

j

|

. . . ; ua−1 exp

b
−
u −

cu

(cid:27)
where a, b and c are given by (91), (92), and (93) as before.

(cid:26)

The distributions (90) and (94) are instances of the so called Generalized In-
verse Gaussian (or GIG for short, see [15]) distribution. The GIG distribution was
ﬁrst introduced in relation to hyperbolic distributions in [16]. It can be shown
that,

∞

ua−1 exp

b
−
u −

(cid:26)

(cid:27)

0

Z

a
2

b
c

(cid:18)

(cid:19)

cu

du = 2

BesselK(a, 2√bc)

(95)

where the BesselK(a, x) is the modiﬁed Bessel function of the third kind. It is the
solution to the diﬀerential equation,
x2y′′ + xy′

(x2 + a2)y = 0

(96)

−

Thus, (90) and (94) are proper provided that b > 0 and c > 0. When either b = 0
or c = 0 (but not both) one of the two becomes a Gamma. As it is indicated in
[15] the good news about GIGs is that they are log concave and there are universal
algorithms for generating them. The problem is that the standard oﬀ the shelve
algorithm for log concave densities requires the evaluation of the normalization
constant, which in this case is too expensive, since it involves evaluating BesselK.
The following Gamma approximation provides a solution to this problem.

5.6.4. Gamma Approximation to GIG
By computer algebra it is possible to ﬁnd the parameters of a Gamma that best
ﬁt a given GIG. Let us use the notation, for α > 0 and β > 0,

Γ(x; α, β) =

xα−1 e−βx for x > 0

(97)

βα
Γ(α)

and let, for a > 0, b > 0 and c > 0,

G(x; a, b, c) =

xa−1 exp

cx

for x > 0

(98)

1
Z

b
−
x −

(cid:26)

(cid:27)

where Z is the normalization constant given by the right hand side of (95). We
summarize the ﬁndings in the next theorem.

22

C. C. RODRIGUEZ

Theorem 2 The best second order Γ(x; α∗, β∗) approximation to G(x; a, b, c) is
when,

α∗ = a

1 +

β∗ = c

1 +

(cid:20)

(cid:20)

4bc
λ
(cid:21)
4bc
ρ

(cid:21)

λ = a

1 + E

ρ = (a

1)λ

−

−
(a

E =

1)2 + 4bc

x∗ =

λ
2c

A1(α, β) = 0
A2(α, β) = B2(a, b, c)

−
Proof Here is a summary of what was found with MAPLE. The function G(x; a, b, c)
has a single global maximum at

p

Expanding both log likelihoods in Taylor series about x∗ we get,

log Γ(x; α, β) = A0 + A1(x
(x

log G(x; a, b, c) = B0 + 0

−
−

·

x∗) + A2(x
x∗) + B2(x

x∗)2 + o((x
x∗)2 + o((x

−
−

x∗)2)
x∗)2)

−
−

The optimal parameters α∗ and β∗ are the solution to the system of equations,

The Gamma approximation provided by theorem 2 ﬁts the bulk of the GIG
very well but the tails of the GIG are always heavier. A few metropolis iterations
starting from the gamma approximation should be used to correct for the light
tails.

5.6.5. Conditional for ω
Collecting all the factors with ωj from (83) and all the terms from (77) we obtain,

. . . ;

ω

|

k

j=1
Y

αj −1 e−βjωj

ωj

αj = kj −

αωj + 1/2

kj + 1/2

≈

βj =

mj

α
2 "(cid:18)

µj −
sj

2

+

(cid:19)

(cid:18)

2

σj
sj (cid:19)

log

−

2

σj
sj (cid:19)

(cid:18)

+ 2 log

(111)

1
ωo

j #

where,

Q.E.D.

where,

and,

(99)

(100)

(101)

(102)

(103)

(104)

(105)
(106)

(107)

(108)

(109)

(110)

ENTROPIC PRIORS FOR BBNS AND MIXTURES

23

Notice that βj > 0 and we can use Lemma 1 again to ﬁnd good starting approxi-
mations to be corrected with a small number of metropolis iterations.

6. Conclusions and Future Work

We have provided explicit formulas for adding objective prior information in two
general classes of hypothesis spaces: Discrete probabilistic networks and mixtures
of gaussians models. Many highly successful models are special cases of BBNs. A
partial list lifted from [17] include, linkage analysis in genetics, Hidden Markov
Models for speech recognition, Kalman ﬁltering for tracking missiles, and density
estimation for data compression and coding with turbocodes. It is only natural
to expect improvements in the performance of these methods if there is available
cogent prior information that has not been used. This is specially true in high
dimensional parametric models.

I am currently investigating alternative/complementary methods to MCMC
for performing approximate inference with entropic piors. These include, the vari-
ational bayes approach (see [18]), and the Expectation Propagation (EP) method
of Minka (see [19]).

7. Acknowledgments

This paper was conceived during the summer of 2000 while I was visiting the
data analysis group at the Center for Interdisciplinary Plasma Science (CIPS)
[20]. I would like to thank Volker Dose, Rainner Fischer, Roland Preuss, Udo von
Toussaint and Silvio Gori for many stimulating conversations.

References

1. C. Rodr´ıguez, “The metrics induced by the kullback number,” in Maximum Entropy and

Bayesian Methods, J. Skilling, ed., Kluwer Academic Publishers, 1989.

2. C. C. Rodr´ıguez, “Are we cruising a hypothesis space?,” in Maximum Entropy and Bayesian
Methods, W. von der Linden, V. Dose, R. Fisher, and R. Preuss, eds., vol. 18, (Netherlands),
pp. 131–140, Kluwer Academic Publishers, 1998. http://arXiv.org/abs/physics/9808009.

3. J. Skilling, “Classical Max Ent data analysis,” in Maximum Entropy and Bayesian Methods,

J. Skilling, ed., Kluwer Academic Publishers, 1989.

4. C. Rodr´ıguez, “Objective bayesianism and geometry,” in Maximum Entropy and Bayesian

Methods, P. F. Foug`ere, ed., Kluwer Academic Publishers, 1990.

5. C. Rodr´ıguez, “Entropic priors,” tech. rep., omega.albany.edu:8008/entpriors.ps, Oct. 1991.
6. A. Caticha, “Maximum entropy ﬂuctuations and priors,” in Bayesian Inference and
Maximum Entropy Methods in Science and Engineering: 20th International Workshop,
A. Mohammad-Djafari, ed., vol. CP568, (Melville, New York), American Institute of
Physics, 2001. (math-ph/0008017).

7. C. C. Rodr´ıguez, “Unreal probabilities: partial truth with cliﬀord numbers,” in Maximum
Entropy and Bayesian Methods, W. von der Linden, V. Dose, R. Fisher, and R. Preuss,
eds., vol. 18, (Netherlands), pp. 247–270, Kluwer Academic Publishers, 1998. (to appear).
Preprint at (physics/9808010).

8. C. Rodr´ıguez, “Bayesian robustness: A new look from geometry,” in Maximum Entropy
and Bayesian Methods, G. Heidbreder, ed., pp. 87–96, Kluwer Academic Publishers, 1996.
(since Nov. 1993) in omega.albany.edu:8008/robust.ps.

24

C. C. RODRIGUEZ

9. R. T. Cox, “Probability, frequency and reasonable expectation,” American Journal of

Physics, 14, pp. 1–13, 1946.

10. P. Cheeseman, “In defense of probability,” in Proc. 8th Intl. Joint Conf. on AI, vol. (IJCAI-

85), (Los Angeles), pp. 1002–9, 1985.

11. P. Cheeseman, “An inquiry into computer understanding,” Computational Intelligence, 4,

(1), pp. 58–66, 1988.

12. R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Siegelhalter, Probabilistic Networks
and Expert Systems, Statistics for Engineering and Information Science, Springer-Verlag,
New York, 1999.

13. J. Pearl, Probibilistic Inference in Intelligent Systems, Morgan Kaufmann, San Mateo,

California, 1988.

14. M.

Stephens,

Mixtures
of Normal Distributions. PhD thesis, Dept. of Statistics, Oxford University U.K., 1999.
http://www.stat.washington.edu/stephens/papers/corrected.ps.gz.

Bayesian

Methods

for

15. L. Devroye, Non-Uniform Random Variate Generation, no. ISBN number 0-387-96305-7,

Springer-Verlag, New York, 1986. (http://www-cgrl.cs.mcgill.ca/˜luc/rng.html).

16. O. Barndorﬀ-Nielsen and C. Halgreen, “Inﬁnite divisivility of the hyperbolic and generalized
inverse gaussian distributions,” Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte
Gebiete, 38, pp. 309–311, 1977.

17. K. Murphy, “Homepage.” http://www.cs.berkeley.edu/˜murphy/.
18. M. Jordan, “Homepage.” http://www.cs.berkeley.edu/˜jordan/.
19. T. Minka, “Homepage.” http://www-2.cs.cmu.edu/˜minka/.
20.

“Center for interdisciplinary plasma science (cips): Bayesian data analysis group.”
http://www.ipp.mpg.de/OP/Datenanalyse/.


2
0
0
2
 
v
o
N
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
5
0
1
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

MCMC JOINT SEPARATION AND

SEGMENTATION OF HIDDEN MARKOV

FIELDS

Hihem Snoussi and Ali Mohammad(cid:21)Djafari

Laboratoire des Signaux et Systèmes (nrs (cid:21) supéle (cid:21) ups).

supéle, Plateau de Moulon, 91192 Gif(cid:21)sur(cid:21)Yvette Cedex, Frane.

E-mail: snoussilss.supele.fr,

djafarilss.supele.fr

Abstrat.

In this ontribution, we onsider the problem of the

blind separation of noisy instantaneously mixed images. The im-

ages are modelized by hidden Markov (cid:28)elds with unknown param-

eters. Given the observed images, we give a Bayesian formulation

and we propose to solve the resulting data augmentation problem

by implementing a Monte Carlo Markov Chaîn (MCMC) proe-

dure. We separate the unknown variables into two ategories:

1. The parameters of interest whih are the mixing matrix, the

noise ovariane and the parameters of the soures distributions.

2. The hidden variables whih are the unobserved soures and the

unobserved pixels lassi(cid:28)ation labels.

The proposed algorithm provides in the stationary regime sam-

ples drawn from the posterior distributions of all the variables in-

volved in the problem leading to a (cid:29)exibility in the ost funtion

hoie.

We disuss and haraterize some problems of non identi(cid:28)ability

and degeneraies of the parameters likelihood and the behavior of

the MCMC algorithm in this ase.

Finally, we show the results for both syntheti and real data to

illustrate the feasibility of the proposed solution.

I. INTRODUCTION AND MODEL ASSUMPTIONS

The observations are m images (X i)i=1..m , eah image X i
(cid:28)nite set of sites, S , orresponding to the pixels of the image: X i = (xi
r)r∈S .
The observations are noisy linear instantaneous mixture of n soure images
(Sj)j=1..n de(cid:28)ned on the same set S :

is de(cid:28)ned on a

xi
r =

aijsj

r + ni

r, r ∈ S, i = 1..m

n

j=1
X

where A = (aij) is the unknown mixing matrix, N i = (ni
mean white Gaussian noise with variane σǫ

r)r∈S is a zero-
2
i . At eah site r ∈ S , the matrix

notation is:

x = A s + n

(1)

The noise and soure omponents (N i)1..m and (Sj)j=1..n are supposed to

be independent. Eah soure is modelized by a double stohasti proess

(Sj, Z j). Sj
is a (cid:28)eld of values in a ontinuous set R and represents the
real observed image in the absene of noise and mixing deformation. Z j

is the hidden Markov (cid:28)eld representing the unobserved pixels lassi(cid:28)ation

whose omponents are in a disrete set, Z j
distribution of Z j

satis(cid:28)es the following properties,

r ∈ {1..K j}. The joint probability

∀ Z j, PM (zj

r | Z j

S\{r}) = PM (zj

r | Z j

N (r))

∀ Z j, PM (Z j) > 0






where Z j
S\{r} denotes the (cid:28)eld restrited to S\{r} = {l ∈ S, l 6= r} and N (r)
denotes the set of neighbors of r, aording to the neighborhood system
de(cid:28)ned on S for eah soure omponent. Aording to the Hammersley-

Cli(cid:27)ord theorem, there is an equivalene between a Markov random (cid:28)eld and

a Gibbs distribution,

PM (Z j ) = [W (αj)]−1 exp{−Hαj (Z j)}

where Hαj is the energy funtion and αj is a parameter weighting the spatial

dependenies supposed to be known. Conditionally to the hidden disrete

(cid:28)eld Z j

, the soure pixels Sj

r , r ∈ S are supposed to be independent and

have the following onditional distribution:

p(Sj | Z j, ηj) =

pr(sj

r | zj

r, ηj)

r∈S
Y

where the positive onditional distributions depend on the parameter ηj ∈
Rd
. We assume in the following that pr(. | z) is a Gaussian distribution with
parameters ηj = (µjz, σ2

jz)z=1..K .

We note that we have a two-level inversion problem:

1. The problem desribed by (1) when the mixing matrix A is unkown is

the soure separation problem [1, 2, 3℄.

2. Given the soure omponent Sj

, the estimation of the parameter ηj

and the reovering of the hidden lassi(cid:28)ation Z j

is known as the un-

supervised segmentation [4℄.

In this ontribution, given the observations X i, i = 1..m, we propose a so-
lution to jointly separate the n unknown soures and perform their unsu-

pervised segmentations. In setion II, we give a Bayesian formulation of the

problem. In setion III, an MCMC algorithm based on the data augmen-

tation modelization is proposed. In setion IV, we fous on the problem of

the non identi(cid:28)ability and the degeneraies ourring in the soure separa-

tion problem and their e(cid:27)ets on the MCMC implementation. In setion V,

numerial simulations are shown to illustrate the feasibility of the solution.

II. BAYESIAN FORMULATION

Given the observed data X = (X 1, ..., X m), our ob jetive is the estimation
2
of the mixing matrix A, the noise ovariane Rǫ = diag(σǫ
m), the
means and varianes (µjz, σ2
jz)j=1..n,z=1..K of the onditional Gaussians of

2
1, ..., σǫ

the prior distribution of the soures. The a posteriori distribution of the

whole parameter θ = (A, Rǫ, µjz, σ2

jz) ontains all the information that we

an extrat from the data. Aording to the Bayesian rule, we have

p(θ | X) ∝ p(X | θ)p(θ)

In the setion III, we will disuss the attribution of appropriate prior distri-

bution p(θ). Conerning the likelihood, it has the following expression,

p(X | θ) =

p(X, S, Z | θ)d S

S

Z Z
X

Z (
X

r∈S
Y

=

N (xr ; Aµzr , ARzr A∗ + Rǫ)

PM (Z)

)

(2)

where N denotes the Gaussian distribution, xr the (m × 1) vetor of observa-
tions on the site r, zr is the vetor label, µzr = [µ1z1, ..., µnzn ]t
and Rzr the
diagonal matrix diag[σ2
nzn ]. We note that the expression (2) hasn't
a tratable form with respet to the parameter θ beause of the integration
of the hidden variables S and Z . This remark leads us to onsider the data
augmentation algorithm [5℄ where we omplete the observations X by the
hidden variables (Z, S), the omplete data are then (X, S, Z). In a previous

, ..., σ2

1z1

work [6℄, we implemented restoration maximization algorithms in the one

dimensional ase to estimate the maximum a posteriori estimate of θ . We

extend this work in two diretions: (i) (cid:28)rst, the soures are two-dimensional

signals, (ii) seond, we implement an MCMC algorithm to obtain samples of

θ drawn from its a posteriori distribution. This gives the possibility of not

being restrited to estimate the parameter by its maximum a posteriori , we

an onsider another ost funtion and ompute the orresponding estimate.

III. MCMC IMPLEMENTATION

We divide the vetor of unknown variables into two sub-vetors: The hidden

variables (Z, S) and the parameter θ and we onsider a Gibbs sampler:

repeat until onvergene,

1. draw ( ˜Z (k), ˜S(k)) ∼ p(Z, S | X, ˜θ

(k−1)

)

2. draw

˜θ

(k) ∼ p(θ | X, ˜Z (k), ˜S(k))

This Bayesian sampling [7℄ produes a Markov hain (˜θ

), ergodi with
stationary distribution p(θ | X). After k0 iterations (warming up), the sam-
ples (˜θ
) an be onsidered to be drawn approximately from their a
posteriori distribution p(θ | X). Then, by the ergodi theorem, we an ap-

(k0+h)

(k)

proximate a posteriori expetations by empirial expetations:

E

h(θ) | X

≈

(cid:2)

(cid:3)

h(˜θ

(k)

)

1
K

K

k=1
X

(3)

Sampling (Z, S): The sampling of the hidden (cid:28)elds (Z, S) from p(Z, S | X, θ)

is obtained by,

1. draw

˜Z from

p(Z | X, θ) ∝ p(X | Z, θ) PM (Z)
In this expression, we have two kinds of dependenies: (i) Z are inde-
n
j=1 p(Z j) but eah disrete im-
pendent aross omponents, p(Z) =
age Z j ∼ PM (Z j) has a Markovian struture. (ii) Given Z , the (cid:28)elds
r∈S p(xr | zr, θ)
X are independent through the set S , p(X | Z, θ) =

Q

but dependent through the omponents beause of the mixing operation

p(xr | zr, θ) = N (xr ; Aµzr , ARzr A∗ + Rǫ).

Q

2. draw

˜S | ˜Z from

where the a posteriori mean and ovariane are easily omputed [8℄,

p(S | X, Z, θ) =

N (sr ; mapost

r

, V apost
r

)

r∈S
Y

V apost
r

=

A∗R−1

ǫ A + R−1
zr

−1

mapost
r

(cid:2)
= V apost
r

A∗R−1

(cid:3)
ǫ xr + R−1

zr µzr

Sampling θ: Given the observations X and the samples (Z, S), the sam-
pling of the parameter θ beomes an easy task (this represents the prini-

(cid:0)

(cid:1)

pal reason of introduing the hidden soures). The onditional distribution

p(θ | X, Z, S) is fatorized into two onditional distributions,

p(θ | X, Z, S) ∝ p(A, Rǫ | X, S) p(µ, σ | S, Z)

leading to a separate sampling of (A, Rǫ) and (µ, σ). The hoie of the a
priori distributions is not an easy task [9℄. The omplete likelihood of (A, Rǫ)

belongs to the loation sale family [10℄ and applying the Je(cid:27)rey's rule we

have,

p(A, Rǫ) ∝ |F(Rǫ)|

2 = |Rǫ|

1

−(m+1)
2

where p(A) is loally uniform and F is the Fisher information matrix. We

obtain an inverse Wishart distribution for the noise ovariane and a Gaussian

distribution for the mixing matrix,

R−1
ǫ

∼ W im(αǫ, βǫ), αǫ = |S|−n

, βǫ = |S|

2

2 (Rxx − RxsR−1

ss R∗

xs)




p(A | Rǫ) ∼ N (µa, Ra), µa = vec(RxsR−1

ss ), Ra = 1

|S| R−1

ss ⊗ Rǫ



where we de(cid:28)ne the empirial statistis Rxx = 1
|S|
and Rss = 1
|S|

r srs∗

r . We note that the ovariane matrix of A is pro-
P

P

r xrx∗

r , Rxs = 1
|S|

r xrs∗
r

portional to the noise to signal ratio. This explains the fat noted in [11℄

P

onerning the slow onvergene of the EM algorithm. For the parameters

(µ, σ), we hoose onjugate priors [7℄. The reason of this hoie is the elim-
ination of degeneraies ourring when estimating the varianes σjz . This

point is eluidated in setion IV. The a posteriori distribution remains in the

same family as the likelihood funtion, Gaussian for the means and Inverse

Gamma for the varianes. The expressions are the same as in [7℄.

IV. IDENTIFIABILITY AND DEGENERACIES

It is well known that in the soure separation problem there exist sale and

permutation indeterminations. This an be seen when multiplying the matrix

A by a sale permutation matrix ΛP and the soures by P T Λ−1

. The permu-

tation indetermination doesn't degrade the performane of the algorithm. In

fat, in image proessing the size of data |S| is su(cid:30)iently large to avoid the
Markov haîn ( ˜A(k)) produed by the algorithm permuting its olumns, the

probability that the Markov hain hanges the a posteriori mode is very low.

However, the sale indetermination must be eliminated.

In pratie, after

eah iteration of the MCMC algorithm, the olumns of A are normalized.

There is another kind of indetermination whih is the transfer of varianes

between the ovarianes Rz and the noise ovariane Rǫ :

p(X | A, Rǫ + ǫAΛA∗, Rz − ǫΛ, µz) = p(X | A, Rǫ, Rz, µz), ǫ = ±1

When we study the partiular ase of diagonal noise ovariane and the mix-

ing matrix A is unitary (A A∗ = I ), we note that an obvious transfer of
varianes ours when Λ = αI . A retained solution in this paper is the pe-

nalization of the likelihood by a prior on the varianes whih eliminates this

variane transfer. This solution is more robust than the fat of (cid:28)xing either

Rz or Rǫ . However, a simultaneous penalization of noise and signal variane

an indue a transfer between modes. In suh situations, the Markov hains

(k)

( ˜Rǫ

) and ( ˜Rz

(k)

) seem to onverge to a stationary distribution even after

a great number of iterations but suddenly a transfer ours (see the setion

V for numerial illustration). This indetermination is noted in [11℄ and was

used to aelerate the onvergene of the EM algorithm by foring the noise

ovariane to be maximized. In the MCMC algorithm, we note that the a

posteriori ovariane of A is Ra = 1

|S| R−1

ss ⊗ Rǫ . Consequently, as the sig-
˜A is more

nal to noise ratio inreases, ovariane dereases and the sample

onentrated on its mean value.

It is obvious, under the form 2, that degeneray happens when one of the

terms onstituting the sum approahes to in(cid:28)nity and this is independent of

the law PM .

Consider now the matries

is produed when, among matries

Γz = ARzA∗ + Rǫ . It's lear that degeneray
Γz , at least one is singular and one is

regular. We show in the following that this situation an our.

We reall that the matries Rz and Rǫ belong to a losed subset of the set

of the non negative de(cid:28)nite matries. Constraining matries to be positive

de(cid:28)nite leads to ompliated solutions. The main origin of this ompliation

is the fat that the set of positive de(cid:28)nite matries is not losed. For the

same reason, we don't onstrain the mixing matrix A to be of full rank.

Proposition 1: ∀ A non null, ∃ matries {Γz = ARzA∗ + Rǫ for z =

1..K } suh that {z | Γz is singular} 6= ∅ and {z | Γz is regular} 6= ∅.
Rǫ is neessarily a singular NND matrix and Card ({z | Rz is regular}) <
K .

For a detailed proof see [12℄

One possible way to eliminate this degeneray onsists in penalizing the

likelihood by an Inverse Wishart a priori for ovariane matries. In fat, we

know that the origin of degeneray is that the ovariane matries Rz and
Rǫ approah the boundary of singularity (in a non arbitrary way). Thus, if

we penalize the likelihood suh that when one of the ovariane matries ap-

proahes the boundary, the a posteriori distribution goes to zero, eliminating

the in(cid:28)nity value at the boundary and even foring it to zero.

Proposition 2: ∀ X ∈ (Rm)|S|

, the likelihood p(X | θ) penalized by an
a priori Inverse Wishart for the noise ovariane matrix Rǫ or by an a priori
Inverse Wishart for the matries Rz is bounded and goes to 0 when one of

the ovariane matries approahes the boundary of singularity.

For a detailed proof see [12℄.

V. SIMULATION RESULTS

To illustrate the feasibility of the algorithm, we generate two disrete (cid:28)elds

of 64 × 64 pixels from the Ising model,

PM (Z j) = [W (αj)]−1 exp{αj

Izr=zs}, α1 = 2, α2 = 0.8

r∼s
X

−2 2
−3 3

(cid:20)

(cid:21)

α1 > α2 implies that the (cid:28)rst soure is more homogeneous than the seond
soure. Conditionally to Z , the ontinuous soures are generated from Gaus-

sian distributions of means µjz =

and varianes σjz =

The soures are then mixed with the matrix A =

white Gaussian noise with ovariane σ2
ǫ = 5) is added. The signal to
noise ratio is 1 to 3 dB. Figure-1 shows the mixed signals obtained on the

ǫ I (σ2

detetors.

X1 

X2 

(cid:20)
0.85 0.44
0.51 0.89

(cid:20)

1 2
1 2

.

(cid:21)

and a

(cid:21)

10

20

30

40

50

60

10

20

30

40

50

60

10

30

40

20
60
10
(cid:28)gure-1. The noisy mixed images X 1

50

20
30
and X 2

40

50

60

We apply the MCMC algorithm desribed in setion III to obtain the

Markov haîns A(k)
. Figures 2 and 3 show the his-
tograms of the element samples of A and their empirial expetations (3).

jz and σ2
jz

, R(k)
ǫ

, µ(k)

(k)

We note the onentration of the histograms representing approximately the

marginal distributions around the true values and the onvergene of the em-

pirial expetations after about 1000 iterations. Figures 4 and 5 show the

onvergene of the empirial expetations. We note that the onvergene of

the varianes is slower that the mixing elements and the means. Figure 6

shows the transfer of varianes when the matrix A

0.89 −0.44
0.44 0.89

(cid:20)

is uni-

(cid:21)

tary. We note that this transfer ourred after a great number of iterations

(80000 iterations) and that the sum of the varianes remains onstant.

0
0.82

350

300

250

200

150

100

50

350

300

250

200

150

100

50

0
0.4

−0.5

−1

−1.5

−2

−2.5

0

−0.5

−1

−1.5

−2

−2.5

−3

0

a11

a12

a11

a12

0.84

0.86

0.88

0.9

0.92

0.4

0.45

0.5

1000

2000

3000

1000

2000

3000

a21

a22

a21

a22

0.45

0.5

0.55

0.6

0.9

1000

2000

3000

1000

2000

3000

Figure-2 The histograms of the
samples of mixing elements aij

Figure-3 Convergene of the empirial
expetations of aij after 1000 iterations

mu11

mu12

s11

s12

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

mu21

mu22

s21

s22

0
0.35

350

300

250

200

150

100

50

350

300

250

200

150

100

50

0

1

0

2

1.8

1.6

1.4

1.2

2.5

3

2

1.5

1

0

1

0.95

0.9

0.85

0.8

0

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

0.5

1

0

1

−0.5

0

0.95

0.9

0.85

0.8

0

1

0

2.5

3

2

1.5

2.5

3

2

1.5

1

0

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

Figure-4 Convergene of the empirial
expetations of the means mij

Figure-5 Convergene of the empirial
expetations of the variane σ2
ij

Evolution of Noise Variance and Signal Variance 

7

6

5

4

3

2

1

0

0

Signal variance 

Transfer 

Noise variance 

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

2
4

x 10

Figure-6. The transfer of varianes between Rǫ(1, 1)

and the variane σ2

11 after 80000 iterations

We test our algorithm on real data. The (cid:28)rst soure is a satellite image

of an earth region and the seond soure represents the louds (First olumn

of (cid:28)gure 7). The mixed images are shown in the seond olumn of (cid:28)gure 7.
The results of the algorithm are illustrated in the third olumn of (cid:28)gure 7
where the soures are suessfully separated. The (cid:28)gure 8 illustrate the joint

segmentation of the soures. We note that the results of the two segmen-

tations are the same as the results whih an be obtained if we apply the

segmentation on the original soures.

 

(a)

(b)

(c)

Figure.7: (a) Original soures, (b) Mixed soures and () Estimated soures

Figure 8: Segmented images

VI. CONCLUSION

In this ontribution, we propose an MCMC algorithm to jointly estimate the mixing

matrix and the parameters of the hidden Markov (cid:28)elds. The problem has an inter-

esting natural hidden variable struture leading to a two-level data augmentation

proedure. The observed images are embedded in a wider spae omposed of the

observed images, the original unknown images and hidden disrete (cid:28)elds modeliz-

ing a seond attribute of the images and allowing to take into aount a Markovian

struture. The problems of identi(cid:28)ability and degeneraies are mentioned and dis-

ussed. In this work the number of soures and the number of the disrete values of

the hidden Markov (cid:28)eld are assumed to be known. However, the implementation of

the algorithm ould be extended to involve the reversible jump proedure on whih

we are working.

REFERENCES

[1℄ J. F. Cardoso, (cid:16)Infomax and maximum likelihood for soure separation(cid:17), IEEE

Letters on Signal Proessing, vol. 4, pp. 112(cid:21)114, avril 1997.

[2℄ K. Knuth,

(cid:16)A Bayesian approah to soure separation(cid:17),

in Proeedings of

Independent Component Analysis Workshop, 1999, pp. 283(cid:21)288.

[3℄ A. Mohammad-Djafari,

(cid:16)A Bayesian approah to soure separation(cid:17),

in

Bayesian Inferene and Maximum Entropy Methods, J. R. G. Erikson and

C. Smith, Eds., Boise, ih, July 1999, MaxEnt Workshops, Amer. Inst. Physis.

[4℄ N. Peyrard, (cid:16)Convergene of MCEM and related algorithms for hidden markov

random (cid:28)eld(cid:17), Researh Report 4146, INRIA, 2001.

[5℄ M. A. Tanner and W. H. Wong, (cid:16)The alulation of posterior distributions by

data augmentation(cid:17), J. Amer. Statist. Asso., vol. 82, no. 398, pp. 528(cid:21)540,

June 1987.

[6℄ H. Snoussi and A. Mohammad-Djafari, (cid:16)Bayesian separation of HMM soures(cid:17),

in Bayesian Inferene and Maximum Entropy Methods, R. L. Fry, Ed. MaxEnt

Workshops, August 2002, pp. 77(cid:21)88, Amer. Inst. Physis.

[7℄ C. Robert, Méthodes de Monte-Carlo par haînes de Markov, Eonomia,

Paris, Frane, 1996.

[8℄ H. Snoussi and A. Mohammad-Djafari, (cid:16)Bayesian soure separation with mix-

ture of gaussians prior for soures and gaussian prior for mixture oe(cid:30)ients(cid:17),

in Bayesian Inferene and Maximum Entropy Methods, A. Mohammad-Djafari,

Ed., Gif-sur-Yvette, Frane, July 2000, Pro. of MaxEnt, pp. 388(cid:21)406, Amer.

Inst. Physis.

[9℄ R. E. Kass and L. Wasserman, (cid:16)Formal rules for seleting prior distributions:

A review and annotated bibliography(cid:17), Tehnial report no. 583, Department

of Statistis, Carnegie Mellon University, 1994.

[10℄ G. E. P. Box and G. C. Tiao, Bayesian inferene in statistial analysis,

Addison-Wesley publishing, 1972.

[11℄ O. Bermond, Méthodes statistiques pour la séparation de soures, Phd thesis,

Eole Nationale Supérieure des Téléommuniations, 2000.

[12℄ H. Snoussi and A. Mohammad-Djafari,

(cid:16)Penalized maximum likelihood for

multivariate gaussian mixture(cid:17), in Bayesian Inferene and Maximum Entropy

Methods, R. L. Fry, Ed. MaxEnt Workshops, August 2002, pp. 36(cid:21)46, Amer.

Inst. Physis.


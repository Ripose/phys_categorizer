1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
2
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A COMPARISON OF TWO APPROACHES:
MAXIMUM ENTROPY ON THE MEAN (MEM) AND
BAYESIAN ESTIMATION (BAYES) FOR INVERSE PROBLEMS

Ali Mohammad-Djafari
Laboratoire des Signaux et Syst`emes (CNRS-ESE-UPS)
´Ecole Sup´erieure d’ ´Electricit´e,
Plateau de Moulon, 91192 Gif-sur-Yvette, France.
E-mail: djafari@lss.supelec.fr

Abstract. To handle with inverse problems, two probabilistic approaches have been
proposed: the maximum entropy on the mean (MEM) and the Bayesian estimation
(BAYES). The main object of this presentation is to compare these two approaches
which are in fact two diﬀerent inference procedures to deﬁne the solution of an inverse
problem as the optimizer of a compound criterion.

Key words: Inverse problems, Maximum Entropy on the Mean, Bayesian inference,
Convex analysis

Introduction

1.
Inverse problems arises in many areas of science and engineering. In fact, rarely,
we can measure directly a quantity x and, in general, the unobserved interested x
is related to the measured quantity y via a model. In many area this model can
be written in the general form g = A(x) + n or in the discrete case:

y = A(x) + n,

where y stands for the data, x for the unknown variables and n for the errors
(modeling and noise). Since Newton and Gauss, one tries to deﬁne a solution to
this problem as the optimizer of a criterion, for example the Least Squares (LS):

x = arg min
x

ky − A(x)k2

.

(cid:8)

(cid:9)

But the inverse problems are, in general, ill-posed and the LS criterion may not
have a unique optimum or this solution may be very sensitive to noise. Since
Tikhonov [1], the regularization theory became the main approach to give a satis-
factory solution by deﬁning it as the optimizer of a compound criterion:

b

x = arg min
x

{J(x)} with J(x) = Q(x)+ λΩ(x) = ky − A(x)k2 + λkDxk2 (3)

(1)

(2)

or in its more general forms [2]:

b

x = arg min
x

b

{J(x)} with J(x) = Q(y − A(x)) + λΩ(x, m).

(4)

2

A. Mohammad–Djafari and al.

The questions then raised on how to choose the functionals Q and Ω and the
regularization parameter λ and the default solution m.

The probabilistic approaches started to give partial answers to this request.
In particular in the Bayesian estimation approach and the maximum a posteriori
(MAP) estimate:

x = arg max

{p(x|y)} = arg min
x

{− log p(x|y)} = arg min
x

x

{− log p(y|x) − log p(x)} ,

(5)
b
this choice is: Q = − log p(y|x) and λΩ(x) = − log p(x). This approach just
pushed a little farther the questions which became how to translate our prior
knowledge into a probability law and how to determine their parameters. Even,
nowadays, there are many tools for the estimation of the hyperparameters [3], the
main question on how to translate some knowledge about x into a probability dis-
tribution stays without a complete answer. The maximum entropy (ME) principle
gave partial answers [4, 5, 3]. See also [6] for an extensive discussed bibliography.
At the same time, many authors used the ME principle to ﬁnd unique solutions
to linear inverse problems by considering x as a distribution and the data y as
linear constraints on them. Then, assuming that the data constraints are satisﬁed
by a non empty set of solutions, a unique solution is chosen by maximizing the
entropy:

xj
mj

−

xj log xj

or −

xj log

− (xj − mj)

,

(6)

j
X

j
X
where m is default solution. See for example [7, 8] and the cited references.
However, even if in these methods, thanks to convex analysis and Lagrangian
techniques, the constrained optimization of 6 can be replaced by an equivalent un-
constrained optimization, the obtained solutions satisfy the uniqueness condition
of well-posedness but not the stability one [9, 10, 11].

(cid:20)

(cid:21)

Recently, some authors [12, 13, 14, 15] used the ME principle in a diﬀerent
way by considering x not as a distribution but as the mean value of a random
vector X and the data as the constraints on its distribution dP (x). Then, the
x is deﬁned as
ME principle is used to deﬁne it uniquely and ﬁnally the solution
the expected value of this ME distribution.

Following these authors, some others used, commented and analyzed exten-
sively these ideas [16, 17, 18, 19, 20, 21], [22, 23, 24, 25, 26] and [27, 28, 29, 21].
However, in all these works, the data y were considered as exact constraints and
the errors on the data were either neglected or partially token account of. (See
however new developments in [30].)

b

More recently, some authors who were more faced with real applications, [31,
32, 33, 34] followed the same idea, but by ﬁxing themselves as the objective to
use these ideas for describing the solution as the optimizer of a combined convex
criteria such as (4) and more on a constructive way to determine these functionals.
The objective of this paper is to make a comparison of the Bayesian approach
which we call hereafter BAYES and the maximum entropy in the mean which we
refer to as MEM. This comparison is done very pragmatically and is based on the

A COMPARISON OF TWO APPROACHES: MEM and BAYES. . .

3

understanding of the author who does not have pretension to know all the details
of the both approaches and will be happy to discuss all the following discussions
with the pro of the approaches.

2. Maximum entropy on the mean approach
2.1. Basics

The main references to the basics of this approach are [12, 13]. The original
idea and ﬁrst applications in crystallography are given in [14, 15]. More details
and extensions are given in [31, 32, 33, 34]. The mathematical aspects of convex
analysis and duality theorems are given in [35, 36, 27, 28, 29, 21].
The following resumes the diﬀerent steps of the approach:

− Consider a set C, assume that x ∈ C and deﬁne a reference measure µ(x):

x ∈ C,

m =

x dµ(x),

where m is the mean value of x under this reference measure.

− Consider x as the mean value of a random vector X for which you assume a

probability distribution P :

x = EP {X} =

x dP (x)

and the data y as exact equality constraints on it:

y = Ax = AEP {X} =

Ax dP (x).

ZZC

− Determine the distribution P by:

dP (x)
dµ(x)

ZZC
The solution is calculated via Lagrangian:

maximize −

log

dP (x)

s.t. y = Ax = AEP {X} .

(10)

ZZC

ZZC

L(x, λ) =

log

dP (x)
dµ(x)

−

M

i=1
X

λi(yi − [Ax]i)

dP (x)

#

dP (x)
dµ(x)

− λt(y − Ax)
(cid:21)

dP (x)

ZZC "

ZZC (cid:20)

=

log

and is given by:

where

dP (x, λ) = exp

λt[Ax] − log Z(λ)

dµ(x),

Z(λ) =

exp

λt[Ax]

dµ(x).

(cid:3)

(cid:2)

ZZC

(cid:2)

(cid:3)

(7)

(8)

(9)

(11)

(12)

(13)

4

A. Mohammad–Djafari and al.

The Lagrange parameters are calculated by searching the unique solution (if
exists) of the following system of non linear equations:

∂log Z(λ)
∂λi

= yi,

i = 1, · · · , M.

− The solution to the inverse problem is then deﬁned as the expected value of

this distribution:

These steps are very formal. In fact, it is possible to determine

x(λ) in a more

direct manner. Using the following notations:

x(λ) =

x dP (x, λ).

ZZ

b

s = Atλ, G∗(s) = log Z(s) = log

exp

stx

b
dµ(x),

ZZC

(cid:2)

(cid:3)

H(x) = max

{stx − G∗(s)}, D(λ) = λty − G∗(Atλ)

s

and

it is shown that:

x = arg min
x∈C

b

b

λ = arg max

{D(λ)}

λ
{H(x)} s.t. y = Ax

(Dual criterion)

(Primal criterion),

x(s) =

dG∗(s)
ds

(Explicit relation),

where:

b
− Functions G and H depend on the reference measure µ(x);
− D(λ) is the dual criterion which is function of the data and the function G;
− H(x) = H(x, m) is the primal criterion which is a distance measure between

x and m which means:
– H(x, m) ≥ 0,
– H(x, m)
– H(x, m) = ∞ if x 6∈ C.

and H(x, m) = 0 iﬀ x = m;

is diﬀerentiable and convex on C;

Now, to be able to go a little more in details, let assume that the reference measure
is separable:

µ(x) =

µj(xj )

N

j=1
Y

N

j=1
Y

dP (x, λ) =

dPj(xj , λ)

then, we have:

and

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

G(s) =

gj (sj) , H(x, m) =

hj(xj , mj),

xj = g′

j(sj).

(23)

j
X

j
X

b

A COMPARISON OF TWO APPROACHES: MEM and BAYES. . .

5

Replacing s = Atλ we obtain:

G(λ) =

gj

[Atλ]j

, H(x, m) =

hj(xj, mj),

xj = g′
j

[At

λ]j

, (24)

j
X

(cid:0)

(cid:1)

j
X

(cid:16)

(cid:17)

b

b

where hj and gj depend on the reference measure µj:

− gj is the log Laplace transform (Cramer transform) of µj:

g(s) = log

exp [sx] dµ(x);

− hj is the convex conjugate of gj:

h(x) = max

{sx − g(s)}.

Let give some examples:

Z

s

gj(s)
(s − m)2

1
2

Gaussian:

exp

Poisson:

Gamma:

µj(x)
1
2

(x − m)2

−
(cid:20)
mx
x!
xα−1 exp

exp [−m]
x
m

−

h

i

(cid:21)

hj(x, m)
1
(x − m)2
2

x
m
x
m

x
m

exp [m − s] −x log

+ m − x

log(s − m) − log

+

− 1

When µ(x) is not separable it is very diﬃcult to do the calculation more in details,
excepted the Gaussian case µ(x) = N (m, Rx), where we have:

1
2

H(x, m) = −

(x − m)tR−1

x (x − m), G(λ) = −

kλk2.
(25)
(See however a new presentation of the method in [30] trying to extend the method
for taking account of the correlations.)
2.2. Extensions

kλk2, D(λ) = λty +

1
2

1
2

How to account for the noise: Two approaches have been developed in [31, 32,

33, 34]:

− Replacing the exact equality constraints y = Ax by the following inequalities:

|yi − [Ax]i| < ǫ,

or ky − Axk2 < σ2

(26)

and using the duality relations they showed:

{H(x)} s.t. |yi − [Ax]i| < ǫ, with H(x) =

hj(xj )

˜D(λ)

with ˜D(λ) = D(λ) + αkλk2

j
X

x = arg min
x

λ = arg max
b

λ






b

where α depends on ǫ or on σ2.

n

o

(27)

6

A. Mohammad–Djafari and al.

− Replacing y = Ax by y = Ax + n and rewriting it as follows:

y = [A|I]

−→ y = ˜A ˜x

x
n

(cid:20)

(cid:21)

and assuming µ(˜x) = µx(x)µn(n) they showed:

x = arg min
x∈C

{Q(y − Ax) + αH(x)}

(28)

(29)

with

hj(xj),

and

Q(z) =

qi(zj).

(30)

N

b
H(x) =

j=1
X

M

i=1
X

Here also hj(xj ) and qi(zi) depend on the reference measures µx(x) and µn(x).
The determination of α is not discussed.

3. Bayesian approach
3.1. Basics

The diﬀerent steps of this approach are now well-known:

− From the observation model and the hypothesis (prior knowledge) on the noise

derive the likelihood p(y|x; β);

− From the hypothesis (prior knowledge) on x derive the prior law p(x|θ);
− Apply the Bayes rule to obtain p(x|y; β, θ) = p(y|x; β) p(x|θ)/p(y; β, θ);
− Deﬁne an estimation rule via a cost function c(x,

x) by:

x(y; β, θ) = arg min

b
c(x, z)p(x|y; β, θ) dx

.

(31)

z

(cid:26)ZZ

(cid:27)

Diﬀerent cost functions give diﬀerent estimators:

b
− Maximum a posteriori (MAP):

C(x,

x) = 1 − δ(x −

x) −→

x = arg max

{p(x|y; θ, β)} .

(32)

x

− Posterior mean (PM):

b

b

b

C(x,

x) = [x −

x]tQ[x −

x]t −→

x = Ex|y {X} =

x p(x|y; θ, β) dx. (33)

− Maximum of the Marginal a posteriori (MMAP):

b

b

b

b

C(x,

x) =

1 − δ(xj −

xj) −→

xj = arg max

{p(xj|y; θ)} ,

(34)

j
Y

b

where

b

b

p(xj|y; θ) =

p(x|y; θ) dx1 · · · dxj−1 · · · dxj+1 · · · dxn.

(35)

ZZ

ZZ

xj

A COMPARISON OF TWO APPROACHES: MEM and BAYES. . .

7

To illustrate this, let consider the case of linear inverse problems y = Ax + n with
the following hypothesis:

− n is zero-mean, white and Gaussian: n ∼ N (0, (1/β)I) which leads to:

(cid:20)
− x is Gaussian: x ∼ N (x0, (1/θ)P 0):

p(y|x, β) ∝ exp

−

βky − Axk2

.

1
2

(cid:21)

p(x|θ) ∝ exp

−

θ[x − x0]tP −1

0 [x − x0]
(cid:21)

.

1
2

(cid:20)

(36)

(37)

Then, using the Bayes rule it is easy to show that

x|y ∼ N (

x, P ) with

x = P At(y − Ax0), P =

AtA + λP −1
0

−1

.

(38)

The MAP solution is:

b

x = arg max

{p(x|y)} = arg min
x

b

x

(cid:0)

(cid:1)

{J(x)} , with J(x) = Q(x) + λφ(x),

(39)

where

b

Q(x) = ky − Axk2, φ(x) = xtP −1

0 x = kDxk, λ =

(40)

θ
β

Now, relaxing the second hypothesis, i.e; choosing other prior laws for x we obtain
other MAP criteria. Let just note some special interesting cases:

− A Generalized Gaussian law for x:

p(xj) ∝ exp [−(xj − mj)α] .

(41)

The related MAP criterion becomes:

J(x) = Q(x) + φ(x) with

φ(x) =

(xj − mj)α.

(42)

j
X

− A Gamma law for x:

xj ∼ G(α, mj ) −→ p(xj ) ∝ (xj/mj)−α exp [−xj/mj] .

(43)

The related MAP criterion becomes:

J(x) = Q(x) + φ(x) with

φ(x) =

α log

xj
mj

+

xj
mj

.

j
X

− A Beta law for x:

xj ∼ B(α, β) −→ p(xj) ∝ xα

j (1 − xj )β.

The related MAP criterion becomes:

(44)

(45)

J(x) = Q(x) + φ(x) with

φ(x) = α

log xj + β

log(1 − xj ). (46)

j
X

j
X

xj
mj

j
X

j
X

i∈Nj
X

8

A. Mohammad–Djafari and al.

− A Poisson law for x:

p(xj ) ∝

exp [−mj] .

(47)

mxj
j
xj!

The related MAP criterion becomes:

J(x) = Q(x) + φ(x) with

φ(x) = −

xj log

+ (xj − mj).

(48)

− Markovian models for x:

3.2. Extensions

J(x) = Q(x) + φ(x) with

φ(x) = α

V (xj , xi).

(49)

The Bayesian approach can be exactly applied when all the direct (prior) prob-
ability laws (p(y|x, β) and p(x|θ)) are assigned. Even, choosing an appropriate
law is done in general by hand, another diﬃculty is to determine their parameters
(β, θ). This problem has been addressed by many authors and the subject is an
active area in statistics. See [37, 38, 39, 40], [41, 42, 43, 44, 45] and also [46, 47, 48].

All these methods can mainly be divided in three main families:

− Generalized MAP: In this approach one tries to estimate both the hyperpa-
rameters and the unknown variables x directly from the data by deﬁning:

x,
(

θ,

β) = arg max

{p(x, θ, β|y)}

(50)

(x,θ,β)

where

b

b

b

p(x, θ, β|y) ∝ p(y|x, β) p(x|θ) p(θ) p(β)
(51)
and where p(θ) and p(β) are appropriate prior laws. Many authors used the
non informative prior law for them.

− Marginalization: In this approach one tries to estimate ﬁrst the hyperparam-

eters by marginalizing over the unknown variables x:

p(θ, β|y) ∝ p(β) p(θ)

p(y|x, β) p(x|θ) dx

and then, using them in the estimation of the unknown variables x:

ZZ

θ,
(

β) = arg max
(θ,β)

{p(θ, β|y)}

− Nuisance parameters: In this approach the hyperparameters are considered

b

b

as the nuisance parameters, so, marginalized:

p(x|y) =

p(y, x, θ, β) dθ dβ

and the unknown variables x are estimated by:

ZZ

x
To see some more discussions and diﬀerent possible implementations of these
approaches see [48].

b

x = arg max

{p(x|y)}

(52)

(53)

(54)

(55)

A COMPARISON OF TWO APPROACHES: MEM and BAYES. . .

9

4. Discussed points in each approach
4.1. Mem:

− Choice of C and µ(x):

– C must be a convex set, such as:
– Up to now, the whole analysis can be done for separable measures.
– The only reference measures µj which permits to go through all the steps
are those for which we have analytical expression for the Laplace transform
of their logarithms.

IRN ,

IRN
+ ,

[a, b]N

− Accounting for the noise:

In the ﬁrst approach only the support and the energy of the noise is used. In
the second approach we have more choices via the reference measures µn(n),
but determination of α stays adhoc. In fact, in general, when the reference
measures µn(n) and µx(x) depend on any parameters, this approach lacks
any tool to determine them.

− Eﬀective calculation of the solution:

No problem, and more, this is probably the main interest of the approach
which deﬁnes the solution, by construction, as the optimizer of a convex cri-
terion.

− Characterization of the solution:

A sensitivity analysis has been proposed by [32], but, in my opinion, this is
not enough to characterize a solution to an inverse problem.
It is not easy to use the notions of variance or covariance of the solution,
because this approach does not deﬁne a posterior distribution for the solution.

− Possibility of the extension of the approach:

I have not yet seen any extension of this approach to non linear inverse prob-
lems, or linear inverse problems in which the operator depend on unknown
parameters, such as blind deconvolution or antenna array processing;
The fact that we have to choose a convex set C on which the solution is de-
ﬁned excludes the inverse problems in which we know a priori that the solution
is discrete-valued (binary, nary images for example). This excludes the use
of this approach in image segmentation or communication inverse problems
(canal equalization, blind deconvolution, etc.).

4.2. Bayes:

− Choice of p(x|β):

p(x|β) can be chosen separable or not. Evidently, separable p(x|β) (Entropic
prior laws) simpliﬁes the calculations. Accounting for correlations is easily
done via Markovian models; In both cases (Entropic or Markovian prior laws),
there are some tools for choosing them either by physical considerations, or
by scale invariance arguments [49, 50, 51, 52, 53].

− Choice of the cost function or equivalently of an estimator MAP, PM, MMAP:
This choice is done more on the basis of cost calculation. MAP calculation
needs, in general, global optimization, but does not need any integration.
MP or MMAP needs multidimensional integration, so in general, greater cost.
However, there are approximate calculation techniques based on Monte Carlo

10

A. Mohammad–Djafari and al.

methods and Gibbs sampling.

− Eﬀective calculation of diﬀerent solutions:

For MAP estimate, when the posterior law is unimodal, we can use any gradi-
ent descent based method, but if this is not the case, there are two categories
of methods: Simulated Annealing or Deterministic relaxation (GNC). For
more discussions on Bayesian calculations see [54] in this volume.

5. Comparisons and discussions
The following main items are discussed:

− In MEM, the unknowns x are considered as the mean values of a random

vector X for which a prior probability measure dµ(x) is deﬁned.

− In BAYES, the unknowns x are considered as a sample of a random vector

X for which a prior probability measure p(x) is deﬁned.

− In MEM, a probability distribution p(x) is deﬁned as the minimizer of the
Kullback distance K(p, µ) subject to the data constraints, and the solution
is deﬁned to be Ep(X). What is interesting here is that this solution can
equivalently be obtained as the minimizer of a convex criterion J(x) subject
to the data constraints, and what is more attractive is that, thanks to the
convex analysis, this solution can also be obtained as the stationary point of
a dual criterion which can easily be calculated numerically.

− In BAYES, the posterior law p(x|y) is calculated using the Bayes’ rule. In fact,
the data y are considered as a sample of a random vector Y for which we can
deﬁne a conditional probability law p(y|x) which, when used in conjunction
with the prior p(x) in the Bayes’ rule will give us the posterior law, from
which we can deﬁne an estimator. One of these estimators is the posterior
mean Ep(X), but others can also be deﬁned. This posterior law is used
not only to deﬁne an estimate (a solution), but also to calculate any other
probabilistic information about that solution.

− In MEM, in its original version, the data are considered as the exact linear con-
straints. The uncertainty on the data are not considered, and consequently,
the uncertainty on the solution is not handled. However, some extensions are
recently presented to take account of the errors on the data and to calculate
the sensitivity of the solution to these errors.

− In BAYES, the errors are naturally considered through p(y|x) and the un-
certainty of the solution through the posterior probability p(x|y). Naturally
then, we can compare the information content of the data and the prior model
using their entropies. We can also measure the relative information content
of the posterior to prior model by K(p(x|y), p(x)).

− In MEM, even in their extended versions, it is not easy to handle with the
hyperparameters. In BAYES, there are the necessary tools to handle them.
− In MEM, one can not yet handle with non linear problems. This is not the

case of the BAYES.

As a ﬁnal conclusion, we have to mention that, even if the two approaches are
diﬀerent, they can, in some cases result to the same deﬁnition of the solution as

A COMPARISON OF TWO APPROACHES: MEM and BAYES. . .

11

the minimizer of the same criterion, and consequently, to give exactly the same
numerical solutions to a given inverse problem. However, we can give diﬀerent
interpretations to the obtained result depending on the approach used to reach it.
The main objective of this paper was to give a succinct presentation of the two
approaches for the resolution of the inverse problems.

It is important to note that the two approaches give diﬀerent views and inter-
pretations which can be used advantageously for any application. Also, even the
Bayesian approach is now really mature, the MEM approach is more recent. So,
many of the conclusions I made today may be altered in future. In particular, new
presentation of the method by Heinrich et al [30] in this volume will probably give
new possibilities to the MEM method and will push the limits of the method to
greatest generality.

References

Winston, 1977.

1. A. Tikhonov and V. Arsenin, Solutions of Ill-Posed Problems. Washington DC:

2. G. Demoment, “Image reconstruction and restoration : Overview of common es-
timation structure and problems,” ieeeASSP, vol. ASSP-37, pp. 2024–2036, Dec.
1989.

3. A. Mohammad-Djafari and J. Idier, A scale invariant Bayesian method to solve
linear inverse problems, pp. 121–134. Maximum entropy and Bayesian methods,
Santa Barbara, U.S.A.: Kluwer Academic Publ., g. heidbreder ed., 1996.

4. A. Mohammad-Djafari and J. Idier, Maximum entropy prior laws of images and esti-
mation of their parameters, pp. 285–293. Maximum entropy and Bayesian methods,
Laramie: Kluwer Academic Publ., t.w. grandy ed., 1990.

5. A. Mohammad-Djafari, Maximum Entropy and Linear Inverse Problems; A Short
Review, pp. 253–264. Maximum entropy and Bayesian methods, Paris, France:
Kluwer Academic Publ., mohammad-djafari, ali and demoment, guy ed., 1992.
6. R. E. Kass and L. Wasserman, “Formal rules for selecting prior distributions: A re-
view and annotated bibliography,” technical report no. 583, department of statistics,
carnegie mellon university, pa, submitted to jnl. of american statistical association,
1994.

7. Skilling, “Theory of maximum entropy image reconstruction,” in Maximum Entropy
and Bayesian Methods in Applied Statistics, Proc. of the Fourth Max. Ent. Workshop
(J. Justice, ed.), (Calgary), Cambridge Univ. Presse, 1984.

8. J. Skilling, “J. skilling,” in Maximum Entropy and Bayesian Methods (J. Skilling,
ed.), vol. Classical maximum entropy, pp. 45–52, Dordrecht, The Netherlands:
Kluwer Academic Publishers, 1989.

9. M. Nashed and G. Wahba, “Generalized inverses in reproducing kernel spaces: An
approach to regularization of linear operators equations,” siamMA, vol. 5, pp. 974–
987, 1974.

10. M. Nashed, “Operator-theoretic and computational approaches to ill-posed prob-
lems with applications to antenna theory,” IEEE Transactions on Antennas and
Propagation, vol. 29, pp. 220–231, 1981.

11. J. M. Borwein, “On the failure of maximum entropy reconstruction for fredholm
equations and other inﬁnite systems,” Mathematical Programming, vol. 9, pp. 509–
523, 1993. AMD.

12

A. Mohammad–Djafari and al.

12. D. Dacunha-Castelle and F. Gamboa, “Maximum d’entropie et probl`eme des mo-
ments,” Annales de l’Institut Henri Poincar´e, vol. 26, no. 4, pp. 567–596, 1990.
13. F. Gamboa, M´ethode du maximum d’entropie sur la moyenne et applications. PhD

thesis, Universit´e de Paris-Sud, Orsay, 1989.

14. J. Navaza, “On the maximum entropy estimate of electron density function,” Acta

Crystallographica, vol. A–41, pp. 232–244, 1985.

15. J. Navaza, “The use of non-local constraints in maximum-entropy electron density

reconstruction,” Acta Crystallographica, vol. A–42, pp. 212–223, 1986.

16. J. Shore and R. Johnson, “Axiomatic derivation of the principle of maximum entropy
and the principle of minimum cross-entropy,” IEEE Transactions on Information
Theory, vol. IT-26, pp. 26–37, Jan. 1980.

17. J. E. Shore and R. W. Johnson, “Properties of cross-entropy minimization,” IEEE

Transactions on Information Theory, vol. IT-27, pp. 472–482, 1981.

18. J. M. Van Campenhout and T. M. Cover, “Maximum entropy and conditional prob-
ability,” IEEE Transactions on Information Theory, vol. IT-27, pp. 483–489, July
1981.

19. J. Borwein and A. Lewis, “Duality relationships for entropy-like minimization prob-
lems,” SIAM Journal of Control and Optimization, vol. 29, pp. 325–338, March
1991.

20. I. Csisz´ar, “Why least-squares and maximum entropy ? an axiomatic approach
to inference for linear inverse problems,” The Annals of Statistics, vol. 19, no. 4,
pp. 2032–2066, 1991.

21. A. Decarreau, D. Hilhorst, C. Lemar´echal, and J. Navaza, “Dual methods in entropy
maximization. application to some problems in crystallography,” siamJO, vol. 2,
pp. 173–197, May 1992.

22. D. Mukherjee and D. Hurst, “Maximum entropy revisited,” Statistica Neerlandica,

vol. 38, 1984.

23. J. A. O’Sullivan, “Divergence penalty for image regularization,” in ICASSP, vol. V,

(Adelaide, Australia), pp. 541–544, Apr. 1994.

24. C. Smith, “A dual method for maximum entropy restoration,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. PAMI-1, pp. 411–414, 1979.
25. C. Michelot and M. L. Bougeard, “Duality results and proximal solutions of the huber
m-estimator problem,” Applied Mathematics and Optimization, vol. 30, pp. 203–221,
1994. AMD.

26. A. Ben-Tal, J. M. Borwein, and M. Teboulle, “A dual approach to multidimension-
nal lp spectral estimation problems,” SIAM Journal of Control and Optimization,
vol. 26, pp. 985–996, July 1988.

27. J. M. Borwein and A. S. Lewis, “Partially ﬁnite convex programming, part i : Quasi
relative interiors and duality theory,” Mathematical Programming, vol. 57, pp. 15–48,
1992.

28. J. M. Borwein and A. S. Lewis, “Partially ﬁnite convex programming, part ii :
Explicit lattice models,” Mathematical Programming, vol. 57, pp. 49–83, 1992.
29. J. M. Borwein and A. S. Lewis, “Partially-ﬁnite programming in l1 and the existence
of maximum entropy estimates,” SIAM Journal of Optimization, vol. 3, pp. 248–267,
May 1993.

30. C. Heinrich, J. Bercher, and G. Demoment, “The maximum entropy on the
mean method: correlations and implementation issues,” in Maximum Entropy and
Bayesian Methods, (Dordrecht, The Netherlands), MaxEnt 96, Kluwer Academic
Publishers, Aug. 1996.

A COMPARISON OF TWO APPROACHES: MEM and BAYES. . .

13

31. G. Le Besnerais, M´ethode du maximum d’entropie sur la moyenne, crit`eres de re-
construction d’image, et synth`ese d’ouverture en radio-astronomie. PhD thesis, Uni-
versit´e de Paris-Sud, Orsay, Dec. 1993.

32. J.-F. Bercher, G. Le Besnerais, and G. Demoment, The maximum entropy on the
mean method, noise and sensitivity. Maximum entropy and Bayesian methods, Cam-
bridge, U.K.: J. Skilling (ed.), Kluwer Academic Publishers, 1995.

33. J.-F. Bercher, G. Le Besnerais, and G. Demoment, “Building convex criteria for
solving linear inverse problems,” in Proc. Intern. Workshop on Inverse Problems,
(Ho-Chi-Minh City, Vietnam), pp. 33–44, Jan. 1995.

34. J.-F. Bercher, D´eveloppement de crit`eres de nature entropique pour la r´esolution des
probl`emes inverses lin´eaires. PhD thesis, Universit´e de Paris-Sud, Orsay, Feb. 1995.

35. R. T. Rockafellar, Convex Analysis. Princeton University Press, 1970.
36. R. T. Rockafellar, “Lagrange multipliers and optimality,” SIAM Review, vol. 35,

pp. 183–238, June 1993.

37. P. Hall and D. M. Titterington, “Common structure of techniques for choosing
smoothing parameter in regression problems,” JRSSB, vol. 49, no. 2, pp. 184–198,
1987. JFG.

38. T. J. Hebert and R. Leahy, “Statistic-based map image reconstruction from pois-
son data using Gibbs prior,” IEEE Transactions on Signal Processing, vol. SP-40,
pp. 2290–2303, Sept. 1992.

39. V. Johnson, W. Wong, X. Hu, and C.-T. Chen, “Image restoration using Gibbs pri-
ors: Boundary modeling, treatement of blurring, and selection of hyperparameter,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-13,
no. 5, pp. 413–425, 1984.

40. D. M. Titterington, “Common structure of smoothing techniques in statistics,” ISR,

vol. 53, no. 2, pp. 141–170, 1985. JFG,Mila.

41. L. Youn`es, “Estimation and annealing for Gibbsian ﬁelds,” Annales de l’institut

Henri Poincar´e, vol. 24, pp. 269–294, Feb. 1988.

42. L. Younes, “Parametric inference for imperfectly observed Gibbsian ﬁelds,” Prob.

Th. Rel. Fields, vol. 82, pp. 625–645, 1989.

43. C. Bouman and K. Sauer, “Maximum likelihood scale estimation for a class of
Markov random ﬁelds penalty for image regularization,” in Proceedings of IEEE
ICASSP, vol. V, pp. 537–540, 1994.

44. J. A. Fessler and A. O. Hero, “Complete data spaces and generalized em algorithms,”
in Proc. IEEE Int. Conf. ASSP, (Minneapolis, Minnesota), pp. IV 1–4, 1993.
45. K.-Y. Liang and D. Tsou, “Empirical Bayes and conditional inference with many

nuisance parameters,” BMK, vol. 79, no. 2, pp. 261–270, 1992. AMD.

46. N. Fortier, G. Demoment, and Y. Goussard, “gcv and ml methods of determining
parameters in image restoration by regularization: Fast computation in the spatial
domain and experimental comparison,” JVCIR, vol. 4, pp. 157–170, June 1993.
47. A. Mohammad-Djafari, “On the estimation of hyperparameters in Bayesian ap-
proach of solving inverse problems,” in ICASSP, (Minneapolis, U.S.A.), pp. 567–571,
IEEE, Apr. 1993.

48. A. Mohammad-Djafari, “Bayesian approach with maximum entropy priors to imag-
ing inverse problems, part II : Applications,” submitted to : Applied Optics, 1995.
49. A. Mohammad-Djafari and J. Idier, Maximum Likelihood Estimation of the Lagrange
Parameters of the Maximum Entropy Distributions, pp. 131–140. Maximum entropy
and Bayesian methods, Seattle, USA: Kluwer Academic Publ., smith, c.r. and erik-
son, g.j. and neudorfer, p.o. ed., 1991.

14

A. Mohammad–Djafari and al.

50. A. Mohammad-Djafari and J. Idier, A scale invariant Bayesian method to solve
linear inverse problems, pp. 121–134. Maximum entropy and Bayesian methods,
Santa Barbara, U.S.A.: Kluwer Academic Publ., g. heidbreder ed., 1993.

51. A. Mohammad-Djafari and J. Idier, “A scale invariant Bayesian method to solve
inverse problems,” in Proc. of the Section on Bayesian Statistical Sciences, (San
Francisco, U.S.A.), pp. –, American Statistical Association, 1993.

52. S. Brette, J. Idier, and A. Mohammad-Djafari, “Scale invariant Markov models for
linear inverse problems,” in Proc. of the Section on Bayesian Statistical Sciences,
(Alicante, Spain), pp. 266–270, American Statistical Association, 1994.

53. S. Brette, J. Idier, and A. Mohammad-Djafari, “Scale invariant Bayesian estimator
for inversion of noisy linear system,” (Spain), Fifth Valencia Int. Meeting on Bayesian
Statistics, June 1994.

54. A. Mohammad-Djafari, H. Carfantan, and M. Nikolova, “New advances in Bayesian
calculation for linear and non linear inverses problems,” in Maximum Entropy and
Bayesian Methods, (South Africa), Kluwer Academic Publ., Aug. 1996.


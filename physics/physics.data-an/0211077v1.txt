2
0
0
2
 
v
o
N
 
8
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
7
0
1
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A modiﬁed Least Squares Lattice ﬁlter
to identify non stationary process

Elena Cuoco

Istituto Nazionale di Fisica Nucleare Firenze,
Via G. Sansone 1, 50019 Sesto F. Firenze, Italy

Abstract

In this paper the author proposes to use the Least Squares Lattice ﬁlter with forget-
ting factor to estimate time-varying parameters of the model for noise processes. We
simulated an Auto-Regressive (AR) noise process in which we let the parameters of
the AR vary in time. We investigate a new way of implementation of Least Squares
Lattice ﬁlter in following the non stationary time series for stochastic process. More-
over we introduce a modiﬁed Least Squares Lattice ﬁlter to whiten the time-series
and to remove the non stationarity. We apply this algorithm to the identiﬁcation of
real times series data produced by recorded voice.

Key words: System identiﬁcation; Gaussian time-varying AR model; Whitening;
Adaptive Least Squares Lattice algorithms; Forgetting factor

1 Introduction

In the application of optimal ﬁltering for the detection of a signal buried in
noise, often it is useful the procedure of whitening [1,2]. If the noise in which
the signal is hidden is non stationary or non Gaussian noise we cannot apply
anymore the optimal ﬁlter for stationary and Gaussian noise. We focused
this work on the application of Least Squares Lattice [4,5,13,14] algorithm to
the problem of identiﬁcation of non stationary noise in stochastic process, to
the procedure of whitening and to the possibility of making stationary a non
stationary process.

To this aim we apply this algorithm to some toy models we built using an
autoregressive non stationary model (see section 5) [3,12,10]. In section 2 we

Email address: cuoco@fi.infn.it (Elena Cuoco).

Preprint submitted to Elsevier Science

2 February 2008

show the whitening techniques based on a lattice structure, in section 3 we
introduce the adaptive Least Squares methods and its application to simulated
non stationary data. In sections 5 and 6 we show how it is possible to whiten
the data and to eliminate the non stationarity present in the noise. We apply
this algorithm to simulated and real data.

2 The autoregressive model and the whitening

An Auto-Regressive process x[n] of order P with parameter ak, from here after
AR(P ), is characterized by the relation

x[n] =

akx[n − k] + σw[n] ,

P

X
k=1

being w[n] a white Normal process.

The problem of determining the AR parameters is the same of that of ﬁnding
the optimal “weights vector” w = wk, for k = 1, ...P for the problem of linear
prediction [3]. In the linear prediction we would predict the sample x[n] using
the P previous observed data x[n] = {x[n − 1], x[n − 2] . . . x[n − P ]} building
the estimate as a transversal ﬁlter:

ˆx[n] =

wkx[n − k] .

P

X
k=1

e[n] = x[n] − ˆx[n]

We choose the coeﬃcients of the linear predictor by minimizing a cost function
that is the mean squares error ǫ = E[e[n]2] (E is the operator of average on
the ensemble), being

the error we make in this prediction and obtaining the so called Normal or
Wiener-Hopf equations

ǫmin = rxx[0] −

wkrxx[−k] ,

P

X
k=1

which are identical to the Yule–Walker equations [3] used to estimated the
parameters ak from autocorrelation function with wk = −ak and ǫmin = σ2

2

(1)

(2)

(3)

(4)

w[n]

+

Σ

-

-

-

x[n]

  -1

z

z-1

...

z-1

x[n-1]

x[n-2]

x[n-p]

a1

a2

ap

Fig. 1. Whitening ﬁlter and AR ﬁlter.

This relationship between AR model and linear prediction assures us to obtain
a ﬁlter which is stable and causal [3], so we can use the AR model to reproduce
stable processes in time-domain. It is this relation between AR process and
linear predictor that becomes important in the building of whitening ﬁlter.

The tight relation between the AR ﬁlter and the whitening ﬁlter is clear in
the ﬁgure 1. The ﬁgure describes how an AR process colors a white process at
the input of the ﬁlter if you look at the picture from left to right. If you read
the picture from right to left you see a colored process at the input that pass
through the AR inverse ﬁlter coming out as a white process.

When we ﬁnd the P parameters that ﬁt a PSD of a noise process, what we are
doing is to ﬁnd the optimal vector of weights that let us reproduce the process
at the time n knowing the process at the P previous times. All the methods
that involve this estimation try to make the error signal (see equation (3) ) a
white process in such a way to throw out all the correlation between the data
(which we use for the estimation of the parameters).

3 LSL ﬁlter

The Least Squares based methods build their cost function using all the in-
formation contained in the error function at each step, writing it as the sum
of the error at each step up to the iteration n :

ǫ[n] =

λn−ie2(i|n) ,

n

X
i=1

(5)

3

LSL

b
k
p+1

f
k
p+1

+

Σ

 +

Single stage

f
e
p+1

[n]

e

b
p+1

[n]

f
e
p

[n]

b
e
[n]
p

z −1

x[n]

Stage 

e

f
1

[n]

Stage

1

[n]

e

b
1

2

e

f
p 

[n]

be
p 

[n]

Stage

P

+

Σ

+

e

f
2

[n]
...

b
e
[n]
2
...

Fig. 2. Lattice structure for LS ﬁlter.

being

e(i|n) = d[i] −

xi−kwk[n],

N

X
k=1

(6)

where d is the signal to be estimated, x are the data of the process and w the
weights of the ﬁlter. The forgetting factor λ lets us tune the learning rate of
the algorithm. This coeﬃcient can help when there are non stationary data
in the time series and we want the algorithm has a short memory. If we have
stationary data we ﬁx λ = 1, otherwise we choose 0 < λ < 1

There are two ways to implement the Least Squares methods for the spectral
estimation: in a recursive way (Recursive Least Squares or Kalman Filters)
or in a Lattice Filters using fast techniques [5]. The ﬁrst kind of algorithm,
examined in [1], has a computational cost proportional to the square of the
order of ﬁlter, while the cost of the second one is linear in the order P.

The computational cost of RLS is prohibitive for an on line implementation.
Moreover its structure is not modular, thus forcing the choice of the order P
once for all. The algorithm with a modular structure like that of the lattice
oﬀers the advantages of giving an output of the ﬁlter at each stage p, so in
principle we can change the order of the ﬁlter by imposing some criteria on its
output. The Least Square Lattice ﬁlter is a modular ﬁlter with a computational
cost proportional to the order P .

In the least squares methods the linear prediction is made for a vector of data
ˆx[n], so the natural space where developing these methods are the vectorial

4

spaces (a detailed insight in these techniques is reported in [5]).

Let X be a Hilbert p−dimensional space, to which the vectors x[n] of acquired
data belong. The p vectors xj[n] of length n obtained as time translation of
length p of the vector x[n]

x1[n] = z−1x[n] = (0, 0, x[1], ..., x[n − 1]) ,
x2[n] = z−2x[n] = (0, 0, 0, x[1], ..., x[n − 2]) ,

... =

...

xp[n] = z−px[n] = (0, 0, 0, . . . , x[1], ..., x[n − p])

form a base of this space. A vector u which belongs to this space can be
written as

u[n] =

akxk[n] .

p

X
k=1

(7)

(8)

(9)

The vector xp+1[n] with last component x[n] does not belong to this space, but
to a vectorial p + 1-dimensional space D. In the problem of linear prediction
the best estimation of desired signal d[n], that is xp+1[n], is obtained using
the vector lying in the space X.

Therefore the least squares methods look for a vector ˆx[n] which is the closest
to the vector d[n], by minimizing the norm of the distance between ˆx[n] and
d[n]. It can be shown that this operation corresponds to the projection of the
vector d[n] from the p + 1-dimensional space D in the p-dimensional sub-space
X by a projector P. We can decompose the vector d[n] as sum of the vector
ˆx[n] and a vector which has null component only along the vector orthogonal to
the space X. This vector is the vector e(n|n) which, by deﬁnition, is orthogonal
to the data vector x[n]. In fact the orthogonal vector to the space X can be
obtained as

(I − P)d[n] = d[n] − Pd[n] = d[n] − ˆx[n] = e(n|n) .

Therefore the vector d[n] belongs to the vectorial space D, direct sum of the
sub-space X and of sub-space E deﬁned by the vector e(n|n)

D = X ⊕ E .

For the LS adaptive algorithm we want to write the quantities we need for the
estimation of ˆx[n] at the iteration n by means of the quantities at the iteration
n − 1 and if we use a modular structure the same quantities at the stage p in
terms of the ones at the stage p − 1.

5

Using the described techniques, if we augment the order of the ﬁlter from p to
p + 1, we must write the new projector Pp+1 as function of the operator Pp.
The new vectorial space will be the direct sum of X of dimension p and of the
1-dimensional sub-space orthogonal to X along which there is e(n|n) and the
projector will be

Pp+1 = Pp + P1 ,

where we wrote P1 to point the projector on the one-dimensional space ⊥ X.

If we add a new data x[n] to the space X(n − 1), we introduce the vector π[n]
orthogonal to the space X(n − 1) and the new projection of the signal d[n]
along X(n) will be

P[n]d[n] = P[n − 1]d[n − 1] + Pπ[n]d[n] =
ˆx[n − 1] + Pπ[n]d[n] .

Then we can write in a matrix form the relation (11)



P[n − 1] 0



P[n] =




0

.

1




A useful parameter which is introduced is the angle γp[n − 1] between the two
sub-spaces X[n − 1] and X[n] which can be obtained from the relation

γp[n − 1] =< π[n], P⊥

p [n]π[n] > ,

where we introduced the scalar product <, > between the two vectors a[n]
b[n] deﬁned as

< a[n], b[n] >=

λn−ku[k]v[k] ,

n

X
k=1

p [n] = I − Pp[n]. Let us remember that λ is the forgetting factor; if we

and P⊥
limit ourselves to λ = 1 the scalar product <, > is simply aTb.

We can write an adapting relation for the projector in term of the vector γ

P⊥

p [n − 1] 0






0

= P⊥

p [n] −

P⊥

p [n]Pπ[n]p⊥
γp[n]

p [n]

.






0

6

(10)

(11)

(12)

(13)

(14)

(15)

It is important to note that, thanks to the properties of the projector, the
number of operation per iteration is now proportional to the order P and not
to P 2 as for the RLS algorithm.

The LSL ﬁlter is a lattice ﬁlter characterized by recursive relation between the
forward error (FPE) and the backward one (BPE). With the new notation we
can write

ef
p[n] = x[n] − ˆx[n] = [I − Pp[n]] x[n] .

The scalar error ef
π[n] of the vector perpendicular to the sub-space X[n − 1]

p[n] can be written as the component along the direction

ef
p =< π[n], ef

p[n] > .

In a similar way we can write the backward errors. For the backward errors
the space where we make the prediction is diﬀerent from the sub-space X[n]
because the base is now given by z0x[n], . . . , z−(p−1)x[n]. If we introduce the
projector Pp−1 on this new base we can write

p[n] = [I − Pp−1[n]] z−px[n] .
eb

The scalar backward error is given by

eb
p[n] =< π[n], eb

p[n] > .

ǫf
p [n] = < ef
ǫb
p[n] = < eb

p[n], ef
p[n], eb

p[n] >
p[n] >

ef
p+1[n] = ef
p+1[n] = eb
eb

p+1[n]eb

p[n] + kb
p[n − 1] + kf

p[n − 1] ,
p [n] ,

p+1[n]ef

kb
p+1[n] = −

p[n], ef
< z−1eb
ǫb
p[n − 1]

p[n] >

,

7

The square sum for the forward and backward errors can be written as

Now we can write the recursive relations for the projectors in the equations
(16) and (18)

where we introduced the forward kf
deﬁned by

p and backward kb

p reﬂection coeﬃcients

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

kf
p+1[n] = −

p[n], ef
< z−1eb
ǫf
p [n]

p[n] >

.

The adaptive implementation of the LSL ﬁlter (22) (23) requires the updating
with order p and time n of the reﬂection coeﬃcients.

So we must write the recursive relation for the quantities ǫf
∆p+1[n] =< z−1eb

p[n] >. This can be done using the updating formula

p [n], ǫb

p[n], ef

p[n] and

∆p+1[n] = λ∆p+1[n − 1] +

p[n − 1]ef
eb
γp[n − 1]

p[n]

,

ǫf
p+1[n] = λǫf

p[n] −

p+1[n] = λǫb
ǫb

p[n − 1] −

,

∆2
p+1[n]
ǫb
p[n − 1]
∆2
p+1[n]
ǫf
p [n]
[eb
p[n − 1]]2
ǫb
p[n − 1]

,

γp+1[n − 1] = γp[n − 1] −

.

The error ef
p at the last stage is the whitened sequence of the input data. So
at the output of LSL ﬁlter we ﬁnd the reﬂection coeﬃcients we can use for the
estimation of the AR parameters for ﬁt to PSD of the time series. Moreover
one of the output of this ﬁlter is the whitened sequence of data.

The procedure described for the implementation of LSL ﬁlter is the so called
aposteriori procedure [4]. Since this algorithm involves division by updated
parameters, we must be careful in avoiding division by values too small. In
the aposteriori procedure, the reﬂection coeﬃcients are estimated indirectly
by the estimation of ǫf and ǫb.

In the apriori implementation, the reﬂection coeﬃcients are estimated directly
by the forward and backward errors.

In the apriori implementation the recursive relation for the parameters are
given by

ǫf
p−1[n] = λǫf
p−1[n] = λǫb
ǫb
p[n] = ef
ef
p[n] = eb
eb

p−1[n − 1] + γp−1ef
p−1[n − 1] + γp−1eb
p−1[n] + kf
p [n − 1]eb
p−1[n − 1] + kb

p−1[n]ef
p−1[n − 1]eb
p−1[n − 1] ,
p−1[n] ,

p[n − 1]ef

p−1[n] ,

p−1[n − 1] ,

8

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

p [n] = kf
kf

p [n − 1] −

p[l] = kb
kb

p[n − 1] −

γp = γp−1 −

p [n]

p[n]

,

,

p−1[n]ef
γp−1eb
ǫb
p−1[n]
γp−1ef
p−1[n]eb
ǫf
p−1[n]
p−1|eb
p−1[n − 1]|2
γ2
ǫb
p−1[n]

.

(34)

(35)

(36)

(37)

Since the apriori implementation is more stable with respect the aposteriori
one, we choose to use the apriori recursive relation for the LSL ﬁlter to perform
tests on non stationary data.

4 Modiﬁcation of LSL ﬁlter output to remove non stationarity in

the data

In the above relations no one of the parameters gives a direct estimation of the
σ of the guiding white noise process for the AR model. We have to estimate
it by using the quantities ǫf or ǫb. In particular if we suspect to have non
stationary data in which the overall RMS of the process changes in time we
have to estimate it step by step. The relation we used to estimate σ in LSL
ﬁlter is:

σ[n] = qǫf

P [n]/P ,

(38)

being P the order we choose for the ﬁlter and consequently for the AR ﬁt to
the PSD.

Moreover we have to normalize this quantity with respect to the number of
iterations we used to estimated it. If λ = 1 we used all the data to achieve the
converging value for σ[n], so we have to divide this value at the step N, by
the length N of the time series. If we used λ < 1 we have to normalize by the
window of data we used that is equal to 1
1−λ, called the memory of the ﬁlter.

If the σ varies in time we well ﬁnd a σ that follows the changes, choosing a
good value for the parameter λ.

The novelty in our algorithm is the introduction of a normalization of the
output of the whitening ﬁlter in such a way to make the process white and
stationary. We accomplish this task by estimating the σ[n] at each step n and
the by diving the output ef [n], which is our whitened time series, by σ[n].

9

Table 1
Modiﬁed LSL algorithm

Parameter and variable descriptions

l: time index

p: index on ﬁlter stage

x[l]: input data sequence

y[l]: whitened data sequence

Main Loop

for l = 1, 2, ...N
0[l] = ef
eb
ǫf
0 [l] = ǫb
γ[0] = 1.0

0 [l] = x[l]
0[l] = λǫf

0 [0] + x2[l]

p−1[n − 1] + γp−1ef
p−1[n − 1] + γp−1eb

p−1[n]ef
p−1[n]
p−1[n − 1]eb

p−1[n − 1]

p−1[n] + kf
p−1[n − 1] + kb

p [n − 1] −

p [n − 1]eb

p−1[n − 1]

p[n − 1]ef
p−1[n]
p−1[n]ef
γp−1eb
p [n]
ǫb
p−1[n]
γp−1ef
p−1[n]eb
ǫf
p−1[n]

p[n]

p[n − 1] −
p−1[n−1]|2
γ2
p−1|eb
ǫb
p−1[n]

for p = 1, 2, P
ǫf
p−1[n] = λǫf
ǫb
p−1[n] = λǫb
p [n] = ef
ef
eb
p[n] = eb
p [n] = kf
kf

kb
p[l] = kb

γp = γp−1 −

end
σ[l] = qǫf
y[l] = ef
P [l]

P [l]/P

y[l] = y[l]/σ[l]

end

Normalization for output

Initializations at l = 0:

for p = 1, 2, ...P
p[0] = 0 kf
eb
ǫf
p [0] = δ ǫb

p[0] = δ

p [0] = 0 kb

p[0] = 0 γp = 1.0

being δ a value close to the average amplitude of the process.

10

If we do not normalize the output of the whitening ﬁlter by the varying σ we
will have white non stationary data, but if we divide the output by σ we will
ﬁnd white stationary data, that are what we need in applying optimal signal
search ﬁlter for Gaussian and stationary data. In the next section we show
the results of the application of this modiﬁed version of LSL algorithms to
simulated non stationary data.

5 LSL: application to non stationary noise data

5.1 Toy model I: varying the parameter

We build an AR process of order P = 2 simulating a power spectral density in
which one resonance is present and we let varying the frequency of the peak
changing in time the value of one of the two parameter. The value we use are

Fig. 3. Simulated a1(t) and LSL estimate

the following:

A(1) = 1.3 A(2) = −0.9 σ = 1.0

(39)

choosing a sampling frequency of 200 Hz.

11

Fig. 4. Time series x[n] and LSL whitened one

Moreover we let A(1) vary in time with the following law:

A(1, t) = A(1) exp(a sin(2πωt + φ)) ,

(40)

with the values φ = 0 ω = 0.1 Hz and a = 0.2.

We ﬁt this process using LSL ﬁlter and whiten the data, using λ = 0.99. In
ﬁgure 3 we show the simulated time varying parameter and the estimated
one. In ﬁgure 4 we show the simulated time series and the output of the LSL
whitening ﬁlter.

In these data it is one of the parameters of the AR model which changes its
value in time, so when we estimate the reﬂection coeﬃcients from the data
we ﬁnd also this variation in time if we use this forgetting factor < 1. So the
division of the output of the process by the estimated σ doesn’t inﬂuence the
whitening of the data, since the values of the σ is constant in time.

We check these results also by plotting the PSD at the input and at the output
of the modiﬁed LSL ﬁlter and we plot the in ﬁgure 5. In this ﬁgure the peak
of the PSD results broadened due to the moving of the main resonance, while
after the application of the LSL algorithm with forgetting factor < 1 the PSD
becomes ﬂat, and also the non stationarity disappears.

12

Fig. 5. Power spectral density of the process x[n] and LSL whitened one

5.2 Toy model II:varying the σ

(41)

(42)

We simulated an AR noise process in which the σ of the guiding white normal
noise changes in time with the following function:

σ(t) = σ(1.0 + (a sin(2πωt)) ,

We use an AR(2) model with the same initial values of previous toy model,and
the following values for the modulation

a = 0.4 ω = 0.005Hz .

In this case it is crucial the division by the estimated σ of the process if we
want to have at the output of whitening ﬁlter stationary data. In fact if we
plot the estimated σ of the LSL ﬁlter with λ < 1, we ﬁnd that, even if in a
noisy way, the estimation follows the variations in time of the simulated σ (see
ﬁgure 6).

If we plot the output of the standard implementation of the LSL whitening
ﬁlter in ﬁgure 7, it is evident the this ﬁlter has reduced the total RMS of the
data, but it has not removed the modulation of the sigma of the process.

If we apply the modiﬁed LSL ﬁlter, as it evident in ﬁgure 8, we succeed in
removing also the modulation of the data and in having a stationary whitened

13

Fig. 6. Simulated time varying σ[n] and LSL estimated σ with λ = 0.99

Fig. 7. Simulated time series and not modiﬁed LSL whitening output

time series.

As it is clear in ﬁgure 9 the whitening obtained with the modiﬁed LSL algo-
rithm is good and the whitened PSD for non stationary data results ﬂat.

14

Fig. 8. Time series x[n] and modiﬁed LSL whitened one

Fig. 9. PSD of simulated non stationary AR(2) and modiﬁed LSL whitened one

6 A realistic case: whitening the voice

In order to see the application of this algorithm on real data we perform a
test on a recorded speech sound, that we convert in a time series. This is
surely a non stationary time series, and we want to test if our algorithm is

15

Fig. 10. Voice time series in time domain and whitened ones. The time series was
ﬁtted as an AR(100) model and we use forgetting factor λ = 0.92 in the LSL
algorithm

able to identify the speech and to remove all the features present in it. This
will also mean that we can be able to reconstruct the speech from the learned
parameters [9]. In ﬁgure 11 we report the voice time series in time domain
and the outputs of the standard and LSL algorithm, using an order P = 100
for the ﬁlter and a value λ = 0.92 for the forgetting factor. If we do not apply
the modiﬁcation of the LSL ﬁlter, we succeeded in whitening the PSD (see
ﬁgure 12), but non in removing the non stationarity, as is clear in ﬁgure 10.
In ﬁgure 11 we reports the results of the modiﬁed algorithm. The whitening
results good and the variation of the RMS in time has disappeared.

In ﬁgure 13 we superimpose the estimated σ[n] on the voice time series to show
that most of the variation in time of the voice are due not to the variation of
the AR parameters but to the variation of the σ of the AR process.

These tests show that we have a powerful method to identify non stationary
process and to whiten them. If we have the estimation of the parameters in
time we can reconstruct step by step the original time series.

7 Conclusion

We build a whitening ﬁlter using a modiﬁed LSL algorithm to remove the non
stationarity present in the RMS of the driving white noise for a simulated AR

16

Fig. 11. Voice time series in time domain and whitened ones with modiﬁed algo-
rithm. The time series was ﬁtted as an AR(100) model and we use forgetting factor
λ = 0.92 in the LSL algorithm

Fig. 12. PSD for recorded voice, standard and modiﬁed LSL whitened ones with
order P = 100 and forgetting factor λ = 0.92.

17

Fig. 13. Voice time series in time domain and estimated σ[n].

process. We ﬁnd that with this algorithm is able to follow the non stationarity
either coming from the AR parameters or from the σ parameter and to remove
them from the original data set.

This kind of implementation could be useful if we want to deal with stationary
and white data and we have to apply an optimal ﬁlter for signal detection.

Moreover we test this algorithm on speech signal, ﬁnding encouraging results
on the identiﬁcation of speech, so this application could be useful also in the
reconstruction of speech phoneme.

References

[1] E. Cuoco et al ” On-line power spectra identiﬁcation and whitening for the
noise in interferometric gravitational wave detectors” Class. Quantum Grav.
18(2001) 1727-1751

[2] E. Cuoco et al ” Noise parametric estimation and whitening for LIGO 40-m

interferometer data”, Phys. Rev. D,64,122002

[3] Kay S, Modern spectral estimation:Theory and Application, 1988 Prentice Hall

Englewood Cliﬀs

[4] Haykin S, Adaptive Filter Theory, 1996 Prentice Hall

[5] Alexander S T, Adaptive Signal Processing, 1986 Springer-Verlag

18

[6] Hayes M H, Statistical Digital Signal Processing and Modeling, 1996 Wiley

[7] Widrow B and Stearns S D, Adaptive Signal Processing, 1985 Prentice Hall

[8] Orfanidis S J, Introduction to Signal Processing, 1996 Prentice-Hall

[9] E. Cuoco, in preparation

[10] James R Dickie and Asoke K Nandi, ” A comparative study of AR order

selection methods”, Signal Processing 40 (2-3) 1994 pp. 239-255

[11] Ki Yong Lee, Souhwan Jung and JaeYeal Rheem, ” Smoothing approach using
forward-backward Kalman ﬁlter with Markov switching parameters for speech
enhancement”, Signal Processing 80 (12) 2000 pp. 2579-2588

[12] Yuanjin Zheng, David B.H. Tay and Zhiping Lin, ” Modeling general distributed
nonstationary process and identifying time-varying autoregressive system by
wavelets: theory and application”, Signal Processing 81 (9) 2001 pp. 1823-1848

[13] Martin Bouchard,” Numerically stable

least-squares
algorithms for multichannel active sound cancellation systems and sound
deconvolution systems”, Signal Processing 82 (5) 2002 pp. 721-736

convergence

fast

[14] Dong Kyoo Kim and PooGyeon Park,” The normalized least-squares order-

recursive lattice smoother “, Signal Processing 82 (6) 2002 pp. 895-905

19


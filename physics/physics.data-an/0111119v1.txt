1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
1
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A Bayesian Approach for the Determination of the Charge Density
from Elastic Electron Scattering Data

A. Mohammad-Djafari
Laboratoire des Signaux et Syst`emes (CNRS-ESE-UPS),
Plateau de Moulon, 91192 Gif-sur-Yvette, France
and
H. G. Miller
Department of Physics, University of Pretoria,
Pretoria 0002, South Africa

Abstract. The problem of the determination of the charge density from limited in-
formation about the charge form factor is an ill-posed inverse problem. A Bayesian
probabilistic approach to this problem which permits to take into account both errors
and prior information about the solution is presented. We will show that many classical
methods can be considered as special cases of the proposed approach. We address also
the problem of the basis function choice for the discretization and the uncertainty of
the solution. Some numerical results for an analytical model are presented to show the
performance of the proposed method.

1.

Introduction

Elastic electron scattering provides a mean of determining the charge density of a
nucleus, ρ(r), from the experimentally determined charge form factor, F (q). The
connection between the charge density and the cross section is well understood
and in plane wave Born approximation F (q) is just the Fourier transform of ρ(r)
which for the case of even-even nuclei, which we shall consider, is simply given by

F (q) = 4π

r2 J0(qr)ρ(r) dr

(1)

where J0 is the spherical Bessel function of zero order and q is the absolute value
of the three momentum transfer. Given that the experimental measurements are
performed over a limited range at a ﬁnite number of values of the momentum
transfer q, a unique determination of ρ(r) is not possible since the resulting inverse
problem is ill posed.

One of the generally accepted procedures for determining ρ(r) is to expand it
in a basis and then determine the expansion coeﬃcients from a least squares (LS)
ﬁt to the experimentally measured values of F (q) [1, 2, 3, 14, 18]. The following
questions then arise: how to choose a basis and how to determine the order of the
expansion? Another problem with the LS methods is that increasing the number
of terms in the expansion generally leads to non physical oscillations in the charge
density in spite of the fact that the charge form factor is well reproduced at the
experimentally determined values of q [4, 13]. Finally, due to the fact that the

∞

0
Z

1

problem is inherently ill posed, a small error in the data (experimental errors
or measurement noise) will produce large variations in the solution which is not
acceptable in practical situations.

What we are going to do is to show how a Bayesian approach can be helpful to
give both correct and reasonable answers to the aforementioned questions and to
propose new methods which are more stable with respect to the errors and ﬁnally
to give procedures to put the correct error bars on the proposed solutions.

2. Fundamentals of the Bayesian approach

Let us start by discretizing the problem in the usual manner by expanding ρ(r) in
a basis φn(r):

ρ(r) =

0
(cid:26) P

N
n=1 anφn(r)

r
Rc
≤
r > Rc

and substituting it in (1) yields

F (q) =

4π

r2 J0(qr)

anφn(r) dr

Rc

0
Z
N

N

n=1
X

Rc

=

4π

an

r2 J0(qr)φn(r) dr

Am,n = 4π

r2 J0(qmr)φn(r) dr

n=1
X

0
Z

Rc

0
Z

F c = Aa + ǫ

Now, deﬁning

we obtain

where a is a vector containing the coeﬃcients
, F c is a vector
{
N ) matrix
Fc(qm), m = 1,
containing the form factor data
· · ·
containing the coeﬃcients Am,n given by (4). Note also that when the vector a is
determined, we can calculate ρ =
ρ(rk), k = 1,

, N
and A an (M

an, n = 1,
, M

, K

· · ·

by

×

{

}

}

{

· · ·

}

ρ = Φa

×

where Φ is a K

N matrix with the elements Φkn = φn(rk).

The vector ǫ is added to take account of the errors in both measurement noise

and due to discretization. We assume that the components
}
are additive, zero mean (no systematic error), mutually independent (no correla-
tion) and independent of a, and they can only be characterized by their common
variance σ2
ǫ . This hypothesis is reasonable unless we know more about its charac-
teristics.

ǫm, m = 1,

, M

· · ·

{

Note that we have not yet discussed the choice of the basis functions φn and
the determination of the expansion order, N . We will come back to these questions
later. Let us now see how the Bayesian estimation approach works.

2

(2)

(3)

(4)

(5)

(6)

The main idea behind the Bayesian probabilistic approach is to represent the
uncertainty or any lack of knowledge or any diﬀuse prior knowledge about a quan-
tity by a probability law. For example, the knowledge (or the hypothesis) that
ǫm, m = 1,
are zero mean, mutually independent and that they are only
{
characterized by their common variance σ2
ǫ can be described by choosing a Gaus-
sian probability distribution for them. One may also use the Maximum Entropy
Principle to enforce this choice. This means that we can write

, M

· · ·

}

1

p(ǫm) =

(2πσ2)1/2 exp

1
2σ2
ǫ

ǫ2
m

(cid:27)

−

(cid:26)

or p(ǫm) =

(0, σ2

ǫ ) and

N

M

p(ǫ) =

p(ǫm) =

1

(2πσ2)M/2 exp

1
2σ2

ǫ k

2

ǫ
k

(cid:27)

−

(cid:26)

m=1
Y
ǫ I) where I is the (M

or simply p(ǫ) =
ǫ is the
common variance of ǫm for all m. Now, using this model (5), we can deﬁne the
conditional probability law

M ) unitary matrix and σ2

(0, σ2

N

×

p(F c|

a) =

1
ǫ )M/2 exp
(2πσ2
ǫ I). It is usual to call p(F c|

1
2σ2

−

(cid:26)

ǫ k

or p(F c|
as a function of a, the Likelihood.

(Aa, σ2

a) =

N

F c −

Aa
k

2

(cid:27)

a) or its logarithm, considered

One can stop here and deﬁne the solution of the problem (5) as the vector

a,

which maximizes the likelihood (ML):

or equivalently

b

a = arg max

a {

p(F c|

a)
}

a = arg min

a {−

ln p(F c|

a)
}

which, in the case of a Gaussian distribution (9) becomes
b
a = arg min
a

2)

F c −

Aa
k

k
(cid:8)

(cid:9)

and we ﬁnd here the LS solutions given by:

b

(AtA)

a = AtF c

The main problem with these solutions is that, very often, the matrix AtA is
b
either singular or at least ill-conditioned, so that the solutions are very sensitive
to the errors in the data or even on the round-oﬀ errors during the numerical
calculation.

The Bayesian approach can do better. In fact, before looking at the data, we
IRN and

may have some prior knowledge about a. For example, we know that a

∈

3

(7)

(8)

(9)

b
(10)

(11)

(12)

(13)

. To translate
that we might prefer those vectors which have a minimal norm
this prior knowledge and this preference, we may assign a Gaussian probability
distribution to the vector a:

a
k

k

p(a) =

1
a)M/2 exp
(2πσ2

1
2σ2

a k

2

a
k

−

(cid:26)
a gives an idea about the scale of the norm of the vector a. Now, using

where σ2
the Bayes rule we can calculate the posterior probability law

(cid:27)

where the denominator

F c) =
p(a
|

p(F c|

a) p(a)

p(F c)

p(F c) =

p(F c|

a) p(a) da

ZZ

is the normalization constant (called sometimes the Evidence) [15].

This posterior law contains all the information we may wish about the solution.
a. This

For example, we may want to know what is the probability that a < a
can be calculated by

≤

a

a

ZZ

an

an

Z

P (a < a

a) =

≤

p(a

F c) da
|

Or, we may be interested only in one of these parameters an and want to know

what is the probability that an < an ≤

an. This can be calculated by

P (an < an ≤

an) =

p(an|

F c) dan

where the marginal posterior law p(an|

F c) can be calculated by

p(an|

F c) =

· · ·

Z

Z

F c) da1 · · ·
p(a
|

dan−1 dan+1 · · ·

daN .

(19)

We can also simply deﬁne as the solution the vector
mean value of the posterior law –called Posterior mean (PM) estimator:

a which corresponds to the

b

a =

a p(a

F c) da
|

ZZ

or the vector
posteriori (MAP) estimator:

a which maximizes this posterior distribution –called Maximum a

b

a = arg max

p(a

a {

F c)
}
|

= arg min

a {−

ln p(a

F c)
}
|

b

b

4

(14)

(15)

(16)

(17)

(18)

(20)

(21)

a whose components
or even the vector
marginal posterior law (19) –called Marginal MAP estimator:
ln p(an|
p(an|

b
an = arg max

= arg min

F c)
}

an {−

an {

b

F c)
}

.

an correspond to the maximizer of the

(22)

In the following we consider only the MAP estimator (21). Using the proba-

bility distributions (9) and (14) in (15), the MAP solution is given by:

b

F c −
where λ = (σǫ/σa)2 and we ﬁnd the minimum norm least squares (MNLS) solution
which is given explicitly by

a = arg min
a

k
(cid:8)

(23)

a

a

(cid:9)

b

k

k

2 + λ
k

2)

a = (AtA + λI)

−1AtF c.

(24)

Comparing (13) and (24) we see that, for a given N , the matrix (AtA + λI) is
always better conditioned than the matrix (AtA) and so the solution (24) always
is more stable than the solution (13).

b

We may also want some information about the uncertainty of this solution. For
F c). For example, using the likelihood (9)
this we can use the posterior law p(a
|
and the prior law (14), it is easy to show that the posterior law is Gaussian, i.e.
P = (AtA + λI)−1. We can then
p(a
P to calculate the
use the diagonal elements of the posterior covariance matrix
Pnn and so put the error bars
posterior variances of the estimates, i.e. Var(an) =
b
on the solution. When the posterior law is not Gaussian, we can always calculate

a given by (24) and

F c) =
|

P ) with

a,
(

N

b

b

b

b

E(an) =

an p(an|

Z

b
F c) dan

Var(an) =

(an −

E(an))2 p(an|

F c) dan

Z

but in general we may not have explicit expressions for these integrals. We can
however do numerical calculation either by approximating the posterior law by a
Gauusian law or by a stochastic integral calculation.

One question still remains: how to determine λ and N ? Three approaches are

and

possible:

1. Assign them experimentally from the data using some knowledge on the
physics of the problem. For example, the Parseval-type relation between ρ(r)
and Fc(q):

4πr2ρ2(r) dr =

4πq2F c(q) dq

can be used to estimate σ2

a by:

Z

σ2
a =

1
N

N

M

a2
n =

F 2
cm

m=1
X
and having an estimate of the noise variance σ2

n=1
X

ǫ we can determine λ.

(25)

(26)

(27)

(28)

1
(2π)3

Z

1
M

5

(29)

(30)

(31)

(32)

2. Consider λ and N as two extra parameters (hyper parameters) to estimate
jointly with the unknown parameter a. We can then assign a prior law for
them. For example Jeﬀrey’s priors p(λ) = 1
λ for λ and a uniform p(N ) =
1/Nmax for N . (Other choices are possible, for example a Gamma prior λ which
and a binomial prior for N which eliminates
eliminates λ = 0 and λ =
N = 0 and N = Nmax.)
Finally, we can estimate them jointly with a by

∞

a,
(

λ,

N ) = arg max

p(a, λ, N

(a,λ,N ) {

F c)
}
|

where

b
p(a, λ, N

b
b
F c)
|

p(F c|

a, λ, N ) p(a
λ, N ) p(λ) p(N ).
|

∝

We must however be careful to verify that this joint criterion has at least a
local optimum.

3. Consider λ and N as two extra parameters as in the precedent case but not
on the same level. This means that we can try to estimate them ﬁrst by

where

b

b

λ,
(

N ) = arg max

p(λ, N

(λ,N ) {

F c)
}
|

p(λ, N

F c) =
|

p(a, λ, N

F c) da
|

ZZ

and then use them in (21). Note, however that ﬁnding an analytical expression
F c) is not always possible and its numerical calculation may be very
for p(λ, N
|
costly.

4. Consider λ and N as two nuisance parameters, integrate them out and esti-

mate a directly by

a = arg max

p(a

= arg max

a {

F c)
}
|

p(a, λ, n

F c) dλ
|

)

(33)

N

a (

n=1 ZZ
X

b

(For more details on these methods, their relative characteristics, their practical
implementations and their relatives performances see [20, 17, 19, 16]).

Still one question remains: the choice of the basis-functions.

3. Choice of the basis-functions
Two approaches are used to select the basis functions. We call them the operator
based parametric approach and the non parametric approach and we will discuss
both in detail in the following sections. We propose then a new third approach
which tries to eliminate the limitations and to keep the advantages of the previous
approaches. We call this third approach physically based parametric.

6

3.1. Operator based parametric approach

The ﬁrst approach is to choose special purpose basis functions based on the
properties of the operator linking the data to the unknowns. For example in our
case, due to the fact that the kernel of the integral operator of the direct problem
is a Bessel function, we may also use the Bessel functions as the basis functions
for ρ(r)

ρ(r) =

0
(cid:26) P

N
n=1 anj0(qnr)

r
Rc
≤
r > Rc

This will permit us, using the orthogonality relation

Rc

0
Z

r2 Jl(qnr) Jl(qmr) dr =

J 2
l+1(qnRc) δn,m,

R3
c
2

to ﬁnd an explicit expression for the charge form factor as a function of the coef-
ﬁcients an :

F (q) =

4πR2
c
q

N

n=1
X

an

(
−
(qRc)2

1)n

−

(nπ)2 sin(qR).

With this choice, note also that, if the form factor F (q) was known exactly at

qn = nπ

Rc then the coeﬃcients an could be calculated analytically by

(34)

(35)

(36)

(37)

an =

F (qn)
c [J1(qnR)]2 .

2πR3

In general, however, the cross section is measured at momentum transfers dif-

ferent from qn = nπ
Rc .

fers q =
a =

q1, q2, . . . , qM }

{

a1, a2, . . . , aN }

{

Now, assume that we are given M measurements at arbitrary momentum trans-
and we wish to determine the N expansion coeﬃcients

. In this case Eq. (36) leads to

F c = Aa + ǫ

(38)

as in (5).

The main advantage of this approach is the fact that a is a small dimension
vector and so is the matrix A and we have an explicit analytical expression for
calculating its elements.

But at least one main disadvantage to such a choice is that our prior knowledge
on a may be limited. For example, if we know that ρ(r) is a positive function we
cannot easily incorporate this information in the parameters, a.
3.2. Non parametric approach

The second approach is to choose the basis-functions as general as possible and

independently of the direct problem operator, for example, either:

φn(r) = δ(r

n∆)

−

(39)

7

or

φn(r) =

(cid:26)

1 if (n
0 elsewhere

−

1)∆ < r

n∆

≤

(40)

with ∆ chosen appropriately small (maximum needed resolution) to be certain
we are able to approximate any function ρ(r) as precisely as desired. But, this
means that N will probably be large. This is a disadvantage, but this can be
compensated, as we will see further, by the fact that the coeﬃcients an now have
a direct physical meaning: the samples of ρ(r) in the ﬁrst case and the mean values
n∆ in the second case. This means, for
of ρ(r) in the intervals (n
example, that the prior knowledge such as the smoothness or the positivity of the
function ρ(r) can be transmitted to the coeﬃcients an easily.

1)∆ < r

≤

−

Let us choose (39) and go further into the details. Replacing (2) with the

basis-functions (39) in (1) we obtain:

F (q) =

an

dr 4πr2 J0(qr)δ(r

n∆) = 4π

an∆ (n∆)2 J0(n∆q)

(41)

N

Rc

n=1
X
Denoting by

0
Z

−

N

n=1
X

Am,n = 4π ∆ (n∆)2 J0(n∆qm)
we obtain F c = Aa + ǫ as in (38). If we use (40) in place of (39), the only change
will be in the expression of the Matrix elements Am,n which become

(42)

Am,n = 4π

r2 J0(qmr) dr.

n∆

(n−1)∆

Z

(43)

(44)

(45)

To make a distinction between this approach and the preceding one, let us

denote a by ρ and A by B:

F c = Bρ + ǫ

Let us now compare (44) and (38): a in (38) is a vector of small dimension

while ρ in (44) is a vector of much larger dimension.
Here we can also deﬁne either the LS solution:

ρ = arg min

ρ

F c −

Bρ

2

k

= (BtB)

−1BtF c

k
(cid:8)

or the MNLS solution:

b

ρ = arg min

ρ

F c −

Bρ
k

2 + λ
k

ρ
k

2

= (BtB + λI)

−1BtF c

(46)

k
(cid:8)

but neither of these solutions may be satisfactory.

b

In (44) it is possible to incorporate the smoothness and the positivity of the
function ρ(r) into a more appropriate prior distribution for the components ρn.
For example, to enforce the smoothness of ρ(r) we can assign

(cid:9)

(cid:9)

p(ρ) = p(ρ1)

p(ρn|

ρn−1) =

N

(ρn−1, σ2
0)

(47)

N

n=2
Y

8

(48)

(49)

(50)

(51)

(52)

(53)

(54)

(55)

with

p(ρ1) =

(ρ0, σ2
0)

N

exp

∝

(cid:20)
(ρn−1, σ2
0)

1
2σ2
0

−

exp

∝

ρ0)2

(ρ1 −
1
2σ2
0

−

(cid:20)

(cid:21)
(ρn −

ρn−1)2

(cid:21)

p(ρn|
which leads to

ρ) = p(ρn|

ρn−1) =

N

p(ρ)

exp

∝

1
2σ2
0

"−

N

n=1
X

(ρn −

ρn−1)2

#

Using this prior distribution in (15), the MAP estimator becomes

ρ = arg max
ρ {

F c)
p(ρ
}
|

= arg min
ρ {

J(ρ)
}

J(ρ) =

F c −

k

Bρ

2 + λ

k

(ρn −

ρn−1)2

N

n=1
X

with

b

Deﬁning the matrix

1

1
−
1

1
−
. . .

. . .

D = 














1

1

−

it is easy to show that

J(ρ) =

F c −

k

Bρ

2 + λ

k

(Dρ
k

2 + λ (ρ1 −

k

ρ0)2

Let us temporarily assume that ρ1 = ρ0. We then have an explicit solution for the
minimizer of (54) which is given by:

ρ = (BtB + λDtD)

−1BtF c

b

Comparing this solution with the MNLS solution (46) gives us the possibility to
see the diﬀerence in which the term DtD is used in place of I.
Indeed, due
to the fact that D corresponds to a ﬁrst order derivative, we may designate the
MNLS solution as the zero order regularized solution in contrast to the ﬁrst order
regularized solution.
It is possible to extend this to more general regularized
solutions by an appropriate choice of the matrix D.

Now, let us go back to (54). ρ0 is now a new extra hyper-parameter which
may play a great role in the solution of our inverse problem where the data do not
contain information about the DC level of the function ρ(r).

One way to enforce the positivity of the solution is to choose a prior distribution

such as :

βα
Γ(α)

ρα−1
n

exp[

βρn]

−

(56)

p(ρ) =

p(ρn) =

N

n=1
X

N

n=1
X

9

which can also be written as:

p(ρ)

exp

∝

(1

α) ln ρn + βρn

#

N

"−

n=1
X

−

and which is called an Entropic prior in [20]. Using this prior law in (15), the
MAP estimator becomes

ρ = arg max
ρ {

F c)
p(ρ
}
|

= arg min
ρ {

J(ρ)
}

with

b

J(ρ) =

F c −

k

Bρ

2 + λ1

k

N

N

ln ρn + λ2

ρn

where λ1 and λ2 are related to α, β and σ2

n=1
n=1
X
X
ǫ . Other choices are possible [20].
To enforce both positivity and the smoothness we propose here to choose

p(ρ)

exp

∝

λ1

"−

(ρn −

ρn−1)2

λ2

−

ln ρn −

λ3

N

n=1
X

N

n=1
X

ρn

#

N

n=1
X

which leads to

with

ρ = arg min

ρ>0 {

J(ρ)
}

N

b
2 +

n=1
X

J(ρ) =

F c −

k

Bρ

k

λ1(ρn −

ρn−1)2 + λ2 ln ρn + λ3ρn

(62)

3.3. Physically based parametric approach

In this approach we choose special purpose basis functions based on the physics
of the problem. For example, in our case, since the charge density is a single-valued
function deﬁned in a ﬁnite domain, the Fourier-Bessel (FB) basis functions which
satisfy both the orthogonality and the concentration property conditions, can be
used for expansion:

ρ(r) =

0
(cid:26) P

N
n=1 anj0(qnr)

Rc
r
≤
r > Rc

where qn = nπ
Rc . Excepted the original motivation, this choice is exactly the same
as in the ﬁrst approach and all the relations developed and discussed there can be
used.

We may choose other basis functions which are more appropriate to translate
our prior knowledge on the desired solution. For example, in our case, we know a
priori that, the solution is smooth, positive and a decreasing function. Then we
can choose the following function :

(57)

(58)

(59)

(60)

(61)

(63)

(64)

ρ(r) =

0
(cid:26) P

N
n=1 an exp(

qnr2)

−

r
Rc
≤
r > Rc

10

Using this expansion in eq. (1) we ﬁnd F c = Aa + ǫ as in (5) or in (38), where

Am,n = 4π

dr r2 J0(qmr) exp(

qnr2).

(65)

−

Rc

0
Z

With this choice we keep the main advantage of the ﬁrst approach which is the
small dimension of the vector a and the main advantages of the second approach
which is the translation of our prior knowledge of the positivity of the function
ρ(r). This is due to the fact that if we impose the positivity constraint on the
coeﬃcients an we insure that the solution remains always positive.

In the next section we will illustrate the performance of these diﬀerent solutions

for the estimation of the charge density from elastic electron scattering data.

4.

Issues on the uncertainty of the solution

In any scientiﬁc problem solving, a proposed solution should be given in any way
with a measure of its uncertainty or conﬁdence. In Bayesian approach, the poste-
rior probability gives us naturally the necessary tool. To see this, let come back
to our problem and make a summary. We have a set of data F and we want
to estimate ρ(r) or more precisely ρ for some locations ri. Let assume that we
have chosen a constant discretization step and so, we want to estimate a vector
ρ =

ρ(ri) i = 1,

{
We presented two approaches: parametric and non-parametric.

In the ﬁrst

, K

· · ·

}

.

approach, we have

and in the second

F = Aa + ǫ
ρ = Φa

F = Bρ + ǫ

(66)

(67)

(68)

a), calculated the posterior p(a
|

In both cases, we are interested to ρ. In the ﬁrst approach, we assigned p(a) and
p(F
a for the parameters a,
a for ρ. In the second, we assigned directly p(ρ) and
and ﬁnally, a solution
p(F
ρ. In both
cases, we can use the posterior laws to quantify the uncertainty of the solutions.
There are, at least, three approaches:

F ), and ﬁnally deﬁned a solution
|

ρ), calculated the posterior p(ρ
|

F ), deﬁned a solution
|

ρ = Φ

b

b

b

b

−

−

−

F ) using for example a
Simply generate samples from the posterior law p(ρ
|
monte carlo method, and show all these samples to see the distribution of the
proposed solution.
Calculate the posterior mean and the posterior variance of the solution at each
point either analytically (when possible) or numerically using for example the
samples generated by a monte carlo method.
Calculate the posterior mean and covariance of the solution either analytically
(when possible) or approximate it numerically by any quadrature algorithm.

11

To illustrate this, let consider the cases where all the probability laws are
Gaussian. Then, all the calculations can be done analytically. The following
summarizes all the steps for the calculation of the solutions and their posterior
covariances in the above-mentioned two cases:

Non-parametric

Parametric

F

= Bρ + ǫ

p(ρ)

= N (ρ0, σ2

ρP 0)

p(F |ρ) = N (Bρ, σ2

ǫ I)

ρ

P ρ
b

= [B tB + λP −1

0 ]−1

with λ = σ2

ǫ /σ2

ρ

F

ρ

= Aa + ǫ

= Φa

p(a)

= N (a0, σ2

aI)

p(F |a) = N (Aa, σ2

ǫ I)

P a
b

= [AtA + λP −1
0 ]

−1

with λ = σ2

ǫ /σ2

a

p(ρ|F ) = N (Φ

a, ΦP aΦt)

p(ρ|F ) = N (

ρ, P ρ)

p(a|F ) = N (

a, P a)

= [B tB + λP −1
0 ]

−1B t(F − Bρ0)

a

b

= [AtA + λI]
b

−1At(F − Aa0)

Special cases

ρ = [B tB]

−1B tF

b

Special cases

ρ = Φ[AtA]

−1AtF

σǫ −→ 0

σρ −→ 0

(

P ρ = 0
b

ρ = ρ0

(

P ρ = 0
b

ρ = ρ0

σǫ −→ 0

σa −→ 0

(

P ρ = 0
b

ρ = Φa0

(

P ρ = 0
b
ρ = Φa0

σǫ −→ ∞

σǫ −→ ∞

(

P ρ = σ2
b

ρP 0

ρ = [B tB]

−1B tF

(

P ρ = σ2
b

ǫ [B tB]

−1

(

aΦΦt

P ρ = σ2
b
ρ = Φ[AtA]

−1AtF

(

P ρ = σ2
b

ǫ Φ[AtA]

−1Φt

σρ −→ ∞

σa −→ ∞

Table 1: A comparison between parametric and non-parametric approaches.

12

When the posterior covariance matrix P ρ is calculated, we can use it to give
some information about the uncertainty of the solution. For example, we can
use its diagonal elements to calculate σk = √Pkk and use it to error bars on the
solution.

5. Numerical experiments

In order to demonstrate the preceding considerations we make use of the following
analytical model. For a charge density given by a symmetric Fermi distribution [5]

ρ(r) = α

cosh(R/d)
cosh(R/d) + cosh(r/d)

(69)

an analytical expression for the corresponding charge form factor can easily be
obtained [6, 7]:

F (q) =

4π2αd
q

−

cosh(R/d)
sinh(R/d)

R cos(qR)
sinh(πqd) −

πd sin(qR) cosh(πqd)
sinh2(πqd)

(70)

.

(cid:21)

Only two of the parameters α, R and d are independent since the charge density
must fulﬁll the normalization condition

4π

r2 ρ(r) dr = Z.

(71)

Figure 1 shows the theoretical charge density ρ(r) of 12C (Z=6) obtained from
1
3 and d = 0.626 fm and the theoretical charge
[0, 8] fm−1 and the nine simulated

(69) for r
form factor Fc(q) obtained by (70) for q
experimental data:

[0, 0.8] with R = 1.1 A

∈

∈

q = [0.001, .5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0] fm

−1

1

which are used as inputs to all the inversion methods.

4

(cid:20)

Z

)
r
(
o
h
r

0.4

0.8

0.6

0.2

0

−0.2

0

)
)
)
q
(
F
(
s
b
a
(
g
o

l

2

0

−2

−4

−6

−8

−10

−12

−14

0

8

13

1

2

3

5

6

7

1

2

3

5

6

7

8

4
q

4
r

Figure 1. Theoretical charge density ρ(r) [left], charge form factor log kFc(q)k and the data
[stars] used for numerical experiments [right].

5.1. Experiments with operator based parametric models

We use these data in the parametric model (34) with Rc = 8 fm and estimate

the coeﬃcients a by

LS :
MNLS :

MAP :

−1AtF c

a = (AtA)
a = (AtA + λI)
b
a = (AtA + λDtD)
b

−1AtF c

−1AtF c

Then using these coeﬃcients we calculate ρ(r) by (34) and Fc(q) by (36).

Figure 2 and Figure 3 show the reconstructed charge densities
sponding charge form factors
N = 10.

ρ and the corre-
F c obtained by LS and by MNLS for N = 5 and

b

Figure 4 shows the reconstructed charge densities by LS and by MNLS for diﬀerent
expansion order N from 5 to 10. Note that the LS solutions are very sensitive and
vary greatly with N , but the MNLS solution stays more stable with respect to N .

b

b

Figure 5 shows the reconstructed charge densities and the corresponding charge
form factors obtained by MNLS and MAP for N = 30.

Charge density

y
t
i
s
n
e
d
 
e
g
r
a
h
C

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

−0.1

0

4

2

0

−2

−4

−6

−8

−10

−12

r
o
t
c
a
f
 

m
r
o
F

14

1

2

3

5

6

7

4
Radius

−14

0

8

1

2

3

4
Momentum transfer

5

6

7

8

ρ(r) obtained by LS (point) and by MNLS (dotted) for
Figure 2. Parametric reconstruction of
Fc(q)k [right]. Two
N = 5 [left] and the corresponding reconstructed charge form factors log k
solutions are practically indistinguishable and both not very satisfactory due to a large bias of
the solution for small radius r. Note also that both solutions ﬁt well the data.
b

b

1

2

3

5

6

7

4
Radius

−14

0

8

1

2

3

4
Momentum transfer

5

6

7

8

ρ(r) obtained by LS (point) and by MNLS (dotted) for
Figure 3. Parametric reconstruction of
N = 10 [left] and the corresponding reconstructed charge form factors log kFc(q)k [right]. Note
that the LS solution ﬁts very well the data but is very unstable but the MNLS solution, which

b

does not ﬁt perfectly the data, is at least more stable.

Charge density

Charge density

Charge density

y
t
i
s
n
e
d

 

e
g
r
a
h
C

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

0

y
t
i
s
n
e
d
 
e
g
r
a
h
C

3

2

1

0

−1

−2

−3

−4

0

4

2

0

−2

−4

−6

−8

−10

−12

r
o
t
c
a
f
 

m
r
o
F

1.2

1

0.8

0.6

0.4

0.2

0

y
t
i
s
n
e
d
 
e
g
r
a
h
C

15

1

2

3

5

6

7

1

2

3

5

6

7

8

−0.2

0

8

4
Radius

4
Radius

ρ(r) obtained by LS (left) and by MNLS (right) for
Figure 4. Parametric reconstruction of
diﬀerent values of N := 5 : 1 : 10. Note that the LS solutions vary greatly with N , but the
MNLS solutions stay more stable with respect to N .

b

Charge density

y
t
i
s
n
e
d

 

e
g
r
a
h
C

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

−0.1

0

1

2

3

5

6

7

4
Radius

−14

0

8

1

2

3

4
Momentum transfer

5

6

7

8

ρ(r) obtained by MNLS (point) and by MAP (dotted) for
Figure 5. Parametric reconstruction of
Fc(q)k [right]. In this
N = 30 [left] and the corresponding reconstructed charge form factors log k
case the LS solution is completely unrealistic and is not presented. The MNLS solution has a

b

large bias for small raduis. The MAP solution is very satisfactory. Note also that both solutions
satisfy the data constraint practically in the same way.

b

5.2. Experiments with non parametric models

The same data are then used with the non parametric model (44) with N = 100

and Rc = 8 fm and

ρ is calculated by

LS :

−1BtF c

b
MNLS :
MAP1 :

ρ = (BtB)
ρ = (BtB + λI)
b
ρ = (BtB + λDtD)
b
b
Figure 6 shows the estimated

−1BtF c

−1BtF c

MAP.

Figure 7 shows two solutions obtained by a parametric and a non-parametric

b

b

method and their associated error bars.

ρ and the corresponding

F c by MNLS and by

4

2

0

−2

−4

−6

−8

−10

−12

r
o
t
c
a
f
 

m
r
o
F

16

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0.8

0.6

0.4

0.2

0

y
t
i
s
n
e
d

 

e
g
r
a
h
C

y
t
i
s
n
e
d
 
e
g
r
a
h
C

Charge density

4

2

0

−2

−4

−6

−8

−10

−12

r
o
t
c
a
f
 

m
r
o
F

y
t
i
s
n
e
d
 
e
g
r
a
h
C

0.8

0.6

0.4

0.2

0

−0.2

0

8

b

b

17

−0.1

0

1

2

3

5

6

7

4
Radius

−14

0

8

1

2

3

4
Momentum transfer

5

6

7

8

Figure 6. Non parametric reconstruction of
for N = 100 [left] and the corresponding reconstructed charge form factors log k

ρ(r) obtained by MNLS (point) and MAP1 (dotted)

Fc(q)k [right].

b

Charge density

Charge density

b

−0.2

0

4
Radius

1

2

3

5

6

7

1

2

3

5

6

7

8

4
Radius

Figure 8. Uncertainty in parametric and non parametric methods:
Left: Parametric reconstruction of

ρ(r) obtained by MAP1

Right: Non-parametric reconstruction of

ρ(r) obtained by MAP1

6. Conclusion

We considered the problem of the determination of the charge density from a
limited number of charge form factor measures as an ill-posed inverse problem. We
proposed a Bayesian probabilistic approach to this problem and showed how many
classical methods can be considered as special cases of the proposed approach. We
addressed also the problem of the basis function choice for the discretization and
the uncertainty of the solution. We illustrated the performances of the proposed
methods by some numerical results.

References

1. J. L. Friar and J. W. Negele, Nucl. Phys. A 212, 93 (1973).
2. B. Dreher et al., Nucl. Phys. A 235, 219 (1974).
3. J. Heisenberg and H. P. Blok, Ann. Rev. Nucl. Part. Sc. 33, 569 (1983).
4. D. S. Watkins, Fundamentals of Matrix Computations (Wiley, New York, 1991).
5. M. E. Grypeos, G. A. Lalazissis, S. E. Massen, and C. P. Panos, J. Phys. G 17, 1093

(1991).

6. R. E. Kozak, Am. J. Phys. 59, 74 (1991).
7. R. Anni, G. Co’, and P. Pellegrino, preprint (1994).
8. C. R. Rao and S. K. Mitra, Generalized Inverse of Matrices and its Applications

(Wiley, New York, 1971).

9. J. Baker-Jarvis, J. Math. Phys. 30, 302 (1989).
10. J. Baker-Jarvis, M. Racine, and J. Alameddine, J. Math. Phys. 30, 1459 (1989).
11. N. Canosa, H. G. Miller, A. Plastino and R. Rossignoli, Physica A220, 611 (1995).
12. S. F. Gull and G.J. Daniell, Nature 272, 686 (1978).
13. H.G. Miller, Y. Tzeng, G.D. Yen, N. Canosa, R. Rossignoli and A. Plastino, to be

published.

14. Buck and Macaulay, “Linear inversion by the method of maximum entropy,” in
Maximum Entropy and Bayesian Methods 89, (J. Skilling, ed.), Kluwer Academic
Publishers, 1990.

15. J. Skilling, “Classical maximum entropy” in Maximum Entropy and Bayesian Meth-

ods 89, (J. Skilling, ed.), pp. 45–52, Kluwer Academic Publishers, 1989.

16. A. Mohammad-Djafari, “A full Bayesian approach for inverse problems,” in Max-
imum Entropy and Bayesian Methods 95, (K. Hanson and R. Silver, ed.), Kluwer
Academic Publishers, 1996.

17. D.J.C. MacKay, “Hyperparameters: Optimize or integrate out?” in Maximum En-
tropy and Bayesian Methods 93, (G. Heidbreder, ed.), pp. 43–59, Kluwer Academic
Publishers, 1996.

18. V.A. Macaulay and B. Buck, “A fresh look at model selection in inverse scattering,”
in Maximum Entropy and Bayesian Methods 94, (J. Skilling and S. Sibisi ed.), Kluwer
Academic Publishers, 1996.

19. A. Mohammad-Djafari and J. Idier, “A scale invariant Bayesian method to solve
linear inverse problems”, pp. 121–134. in Maximum Entropy and Bayesian Methods
94, (G. Heidbreder, ed.), Kluwer Academic Publishers, 1996.

20. A. Mohammad-Djafari and J. Idier, “Maximum entropy prior laws of images and
estimation of their parameters,” pp. 285–293. in Maximum Entropy and Bayesian
Methods 90, (T. Grandy, ed.), Kluwer Academic Publishers, 1991.

18


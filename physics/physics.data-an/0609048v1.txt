Knowledge Network Approach to Noise Reduction

Arturo Berrones

1

6
0
0
2
 
p
e
S
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
4
0
9
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Abstract— Previous preliminary results on the application of
knowledge networks to noise reduction in stationary harmonic
and weakly chaotic signals are extended to more general cases.
The formalism gives a novel algorithm from which statistical tests
for the identiﬁcation of deterministic behavior in noisy stationary
time series can be constructed.

The estimation of L is closely related to the separation of
the signal from the noise. In order to see this, consider the case
in which y(t) is stationary and
= 0, where the brackets
stand for statistical average. The variance of y is in this case
written as

y

h

i

Index Terms— Noise reduction, knowledge networks, signal

processing, time series analysis.

I. INTRODUCTION

N OISE reduction and identiﬁcation of underlying deter-

ministic behavior in signals are fundamental questions in
ﬁelds like communication [13], [16] and time series analysis
[7]. A classical model setup relative to the measurement of
such signals [13], [16], considers that each observation in a
sequence y1, y2, ..., yi, ..., yT can be decomposed as a sum of
a deterministic component and a random perturbation,

yi = y(ti) = f (ti) + ε(ti).

(1)

The random terms ε(ti) are statistically independent from
measurement to measurement and independent of f . Consider
a clean signal that can be adequately modeled by a linear
combination of the form

f (ti) =

alϕ(blti + cl)

(2)

L

Xl=1

where the ϕ’s are members of an orthogonal basis of functions.
The meaning of adequately modeled in the present context
refers to the consistency of f with Eq. (1), in the following
sense: if f is approximated through the optimization of some
suitable risk or likelihood function in a ﬁnite sample, then the
residuals should behave like independent random variables.
Additionally, if the resulting form of f is expected to be
used in a fruitful way for prediction purposes, then f should
have the same consistency also outside the original sample,
satisfying a suitable goodness criterion as well. In general,
the speciﬁc nature of the functional basis for f is hidden.
For instance, the number of components needed to describe
the signal, L, is usually unknown beforehand. Previous to any
attempt of ﬁtting the data to f , the model complexity should
be deﬁned. For the setup given by Equations (1) and (2) L
gives a quantity that measures the model complexity.

This work was partially supported by CONACYT under project J45702–
A, SEP–PROMEP under project PROMEP/103.5/05/372 and UANL–PAICYT
under project CA1275–06.

A. Berrones is with Posgrado en Ingenier´ıa de Sistemas, Facultad de
Ingenier´ıa Mec´anica y El´ectrica, Universidad Aut´onoma de Nuevo Le´on, AP–
111, Cd. Universitaria, San Nicol´as de los Garza, NL, M´exico 66450 (e–mail:
arturo@yalma.ﬁme.uanl.mx)

y2

=

(cid:10)

(cid:11)

L

L

*

Xl1=1

Xl2=1

al1al2 ϕl1(ti)ϕl2 (ti)

(3)

+

+

ε2

.

The model complexity L could be estimated from the
knowledge of the noise amplitude and some statistical aspects
of the components of the basis.

(cid:10)

(cid:11)

The purpose of the present contribution is to give a novel
method for the estimation of the complexity of signal models,
which in turn introduces a new framework to deal with
noise reduction. The main concern regarding the application
of the formalism is on cases in which the noise is strong,
that is, with a variance comparable with the corresponding
variance of the clean signal. The proposed approach is valuable
to the characterization of deterministic signals under strong
stochasticity. In many important ﬁelds of application, like
analysis of geophysical data, voice recognition, time series of
economic, ecological or clinic origin, etc., the identiﬁcation of
deterministic behavior is difﬁcult due to the presence of strong
additive noise or insufﬁcient sample size. These difﬁculties are
particularily evident for the identiﬁcation and characterization
of low dimensional chaotic behavior in noisy time series. The
algorithm introduced here tackle these questions for several
important cases. The procedure is linear, yet it is able to
perform signal analysis tasks that are beyond the capabilities
of traditional linear noise reduction techniques.

A. Knowledge Networks

The proposed method relies on the notion of a knowledge
network [1], [8]. Knowledge networks have been originally
motivated from the study of some particular structures that
arise in economy and biology, like interactions between con-
sumers and products in a market or protein – substrate inter-
actions [8], [9]. A knowledge network is deﬁned as a network
in which the nodes are characterized by L internal degrees
of freedom, while their edges carry scalar products of vectors
on two nodes they connect [8]. In order to ﬁx ideas, consider
the following knowledge network model of opinion formation
[1], [8]: suppose that there exists a database of opinions given
by agents on a given set of products. This database can be
seen as a sparse matrix, with holes corresponding to missing
opinions (say, agents that have never been exposed to a given
product). In geometrical words, the preferences of an agent are
represented as a vector in an hypothetical taste space, whose

dimension and base vectors are generally unknown. A product
is represented by a similar vector of qualities. An agent’s
opinion on a given product is assumed to be proportional
to the overlap between preferences and qualities, which can
be expressed by the scalar product between corresponding
vectors. Therefore, products act like a basis, and opinions as
agent’s coordinates on such a basis. Consider a population of
M agents interacting with N products. The two sets of vectors
lie in a L-dimensional space, an = (a1, a2, ..., aL) and bm =
(b1, b2, ..., bL), where n = 1, 2, ..., N and m = 1, 2, ..., M . In
this way the overlap ym,n = bm ·
an represents the opinion of
an
agent bm on product an. Only the overlaps ym,n = bm ·
can be directly observable. The issue is then to reconstruct the
hidden quantities from a known fraction of the scalar products.
For the case in which L is known, Maslov and Zhang [8] have
shown the existence of thresholds for the fraction p of known
overlaps, above which is possible to reconstruct at different
extents the missing information. Bagnoli, Berrones and Franci
[1], have generalized the study of Maslov and Zhang to the
case in which the dimensionality L is unknown. The present
work mainly relies on this last approach, so a brief summary of
the results of Bagnoli, Berrones and Franci is now presented.
Suposse that the components of bm and an are random

variables distributed according to

P (al

n, bl

m) = Pn,l(a)Pm,l(b),

(4)

h

i

h

as the average, computed in the thermodynamic
and deﬁne
limit, over P (al
n, bl
m). For
a set of hidden components distributed according to Eq. (4),
the y’s are uncorrelated in the thermodynamic limit. However,
correlations arise because L is ﬁnite.

m) of an arbitrary function h(al

n, bl

In order to kept the expressions simple, it is assumed that
al
= 0. Averaging over the distribution (4) the
n
variance of the overlaps is written as
(cid:10)

bl
n

=

(cid:10)

(cid:11)

(cid:11)

y2

= L

a2

b2

.

(5)

For this model setup, Bagnoli, Berrones and Franci [1]
(cid:11) (cid:10)
have shown that any overlap can be expressed in terms of
a weighted average of other overlaps,

(cid:11)

(cid:10)

(cid:11)

(cid:10)

ym,n =

Cm,iyi,n + ǫL,M,N ,

i

= m,

(6)

M

L

M

1

−

i=1
X

where Ci,j is the correlation among yi and yj, speciﬁcally, the
correlation calculated over the expressed opinions of agents i
and j on different products. This correlation asymptotically
goes to the overlap between the corresponding vectors of
agents tastes. The hidden quantity L can be extracted by ﬁtting
the proportionality factor

L
M−1 .

The error term ǫ is at ﬁrst order given by

nents

where

2

opinion matrix. The results are extended to sparse datasets
simply by the redeﬁnition of the parameters M and N like
functions of the pair (m, n). In this way Mn represents the
available number of opinions over product n given by any
agent and Nm is the number of opinions expressed by agent
m regarding any product [1].

B. Knowledge Networks and Signal Models

As already pointed out in [3], a knowledge network frame-
work for signals as those described by Eqs. (1) and (2) can
be built for certain classes of stationary signals. The essential
point is the assumption that a distribution for the components
of the signal model exists, analogous to distribution (4). If
N time ordered subsamples of size M are extracted from the
observed sequence y1, y2, ..., yi, ..., yT , we refer to ym,n as the
measured value at time m in subsample n, with n = 1, 2, ..., N
and m = 1, 2, ..., M . The distribution of the components of
ym,n is assumed to be

P (an,l, ϕm,n,l) = Pn,l(a)Pm,n,l(ϕ).

(8)

In order to see how a distribution P (an,l, ϕm,n,l) can arise
for the problem in hands, note that from Equations (1) and (2)
follows that

L

Xl=1

ym,n =

an,lϕ(mbn,l + cn,l) + εm,n.

(9)

in the given sample with respect

For ﬁxed L, the parameters an,l, bn,l and cn,l are chosen
to be optimal
to some
suitable risk or likelihood function [4]. Due to the noise and
to the ﬁnite sample size, the chosen parameters ﬂuctuate from
sample to sample, giving rise to a distribution of the form
P (an,l, ϕm,n,l).

In the next Section a formalism for noise reduction in
signals is built under the assumption (8). The close connection
between the problem of noise reduction and estimation of
model complexity is shown, leading to a new technique for
model complexity estimation in stationary signals. In Section
III the resulting algorithm is numerically tested on several
examples, that are relevant to important potential applications.
Final remarks and a brief discussion of future work is given
in Section IV.

II. NOISE REDUCTION BY KNOWLEDGE NETWORKS
Consider the following linear transformation of the compo-

an,l − h

an,l →
ϕm,n,l − h

ali
,
ϕm,li

ϕm,n,l →

(10)

ǫ

∼

a2
h
p

b2

L

i h

i

3/2 √M + √N

,

√M N

An aspect of this formalism that is important for applica-
tions is that there is no necessity to have a fully connected

(7)

an,lPn,l(a)

(11)

=

ali
h

n
X

ϕm,li
h

=

n
X

ϕm,n,lPm,n,l(ϕ)

6
3

Introducing the deﬁnitions

a1,1
.
.
.

...

aN,1
.
.
.

a1,L ... aN,L

ϕ1,1,n
.
.
.
ϕ1,L,n









, Φn =









... ϕM,1,n

.
.
.

(12)









... ϕM,L,n









A =

and

Y =

(18)

ˆY =

k
M

Y Y τ
y2
h

k
M

i
[Φτ A + Γ] [Aτ Φ + Γτ ]
ϕ2
]
N [L
i

ih
[Φτ AAτ ΦΦτ A + ΓΓτ Φτ A]
ϕ2

N [L

+
i

a2

a2

ε2

ε2

h

h

.

+
i

]
i

k
M

Y =

ih
Introducing the results (15) and (16) into Eq. (18)

h

h

ε1,1
.
.
.
εM,1

...

...

ε1,N
.
.
.
εM,N

,









Γ = 






ˆY =

a2
a2
(cid:10)
h
The factor k must therefore be chosen as

ε2
ε2
(cid:10)
h

+
+
(cid:11)
i

ϕ2
ϕ2

M
L

k
M

(cid:11) (cid:10)
i h

(cid:11)
i

Φτ A.

(13)

(19)

ϕ2
+
ϕ2
+
(cid:11)
i
The ﬂuctuations of the observable y(t) can be decomposed

ε2
ε2
(cid:10)
h

a2
a2
(cid:10)
h

M [L
M

(20)

k =

(cid:11) (cid:10)
i h

(cid:11)

i

]

the model setup given by Eq. (9) can be written in matricial
form as

Y = Φτ

nA + Γ.

as

(14)

In the limit N

the operation AAτ goes to

→ ∞



(cid:10)

(cid:11)

AAτ = N








In the same way, in the limit M

(cid:10)
→ ∞

(cid:11)

a2
1
0
.
.
.
0

ϕ2
1
0
.
.
.
0

0
a2
2
.
.
.
0

(cid:10)

(cid:11)

0
ϕ2
2
.
.
.
0

(cid:10)

(cid:11)

...
...

...

...
...

...

0
0
.
.
.
a2
L

0
0
.
.
.
ϕ2
L

.











.













(cid:10)

(cid:11)

ΦΦτ = M









The form of the diagonal elments in Ec. (16) follows from
an additional ergodicity assumption: the average
can be
equivalently taken over inﬁnitely many ﬁnite samples or over
a single sample of inﬁnite length. For stationary signals the
validity of this assumption is straightforward.

ϕ2
l

(cid:10)

(cid:11)

(cid:10)

(cid:11)

Consider the operation

ˆY =

CY,

k
M

=

a2

a2
l

and

ϕ2
l

where C is the correlation matrix of the y’s. It is now shown
that if
, that is, if the variabilty
=
due to ﬁnite sample size, discrete sampling and noise affect in
(cid:10)
the same way all of the components, then ˆY = Y
Γ in the
, using a suitable value for the factor
limit N
k. The formula (17) is expanded as

→ ∞

→ ∞

, M

ϕ2

−

(cid:11)

(cid:10)

(cid:11)

(cid:10)

(cid:10)

(cid:11)

(cid:11)

(15)

(16)

(17)

2

y

= L

2

a

2
ϕ

+

2

ε

.

(21)

Introducing Eq. (21) into Eq. (20), an expression for L in

(cid:11) (cid:10)

(cid:10)

(cid:11)

(cid:10)

(cid:11)

(cid:10)

(cid:11)

terms of measurable quantities is found

L =

y2

ε2
αM [
−
ε2
y2
α
(cid:10)
(cid:11)
(cid:10)
h
i −
h

]

,

(cid:11)
i
where α = k
M . In order to see how the terms appearing at
the right in Eq. (22) are measured, consider the following
algorithm for noise reduction and estimation of the optimum
complexity in models for stationary signals. The anticipation
formula in this case reads

(22)

ˆy(ti) =

k
M

M

Xh=1,h6=i

C(th)y(ti −

th).

(23)

The signal is processed performing the following steps:
i) Calculate the autocorrelation function C(t).
ii) Perform mean squares over a sample of M consecutive

points to estimate the factor α = k

M in Eq. (23).

The mean squares problem can be solved exactly, giving

α =

M
j=1

P

P

M

M
i=1 y(ti)
τ1=1 C(tτ1 )y(tj −
P

P

M

τ =1 C(tτ )y(ti −

M

tτ )
τ2=1 C(tτ2 )y(tj −
= τ1, j
i

= τ, j

tτ2)
= τ2,

tτ1 )

P

(24)

ε2

with M less than or equal to one half of the total lenght
of the signal. The term
is estimated after the ﬁltering,
using the ﬁltered data as an approximation of the underlying
(cid:10)
deterministic signal and performing the substraction
=
y2
iii) By the use of Eq. (22), calculate L in terms of observable
(cid:10)
(cid:11)
quantities.

f 2

ε2

−

(cid:10)

(cid:11)

(cid:11)

(cid:11)

(cid:10)

The steps i) – iii) deﬁne what hereafter is called the

Knowledge Network Noise Reduction (KNNR) algorithm.

6
6
6
III. EXAMPLES

The KNNR algorithm is tested on data generated numeri-
cally, adding at each time step a Gaussian white noise term
ε(t) to a deterministic function f (t). The simulation of the
noise is based on the L’Ecuyer algorithm, which is known
to accomplish adecuate performance with respect to the main
statistical tests, and to produce sequences of random numbers
1018 [11]. The noisy data y(t) = f (t) + ε(t)
with lenght
enters as input for the KNNR algorithm. By the use of the
Fast Fourier Transform of the input [11], the autocorrelation
function is calculated for a maximum lag equal to one half
of the total length of the signal. The steps ii) and iii) of the
KNNR algorithm are then performed over the second half of
the input.

∼

The capabilities for noise reduction in harmonic and weakly
chaotic time series of the proposed method have already been
discussed in [3].

The KNNR framework provides a characterization of the
signal model complexity in terms of L, the number of member
functions of a certain orthogonal basis needed to describe the
signal, if it is indeed separable into a deterministic component
and a white noise term. If the necessary assumptions are met,
L should converge to a ﬁnite value as the sample size grows.
This fact can be used to identify underliyng deterministic
behavior.

In the next examples the KNNR approach is tested on
several chaotic systems, with and without additive noise, and
for camparision purposes, on purely stochastic systems as
well. The mean value of the signals is substracted before they
enter as input in the KNNR algorithm. The examples with a
deterministic part are therefore constructed by

y
yi = si + εi − h

i

,

(25)

is the input and si

where yi
a nonlinear discrete map. Each of the noise terms εi,
independently drawn from a Gaussian distribution.

is given by the iteration of
is

The KNNR algorithm is capable to perform tasks that
are beyond the scope of traditional linear signal processing
techniques. For instance, with large enough sample size, the
KNNR algorithm is able to identify nonlinear behavior in
signals whose power spectrum is consistent with a correlated
stochastic process. This identiﬁcation is not possible by clas-
sical approaches like the Wiener ﬁlter [16], which relies in a
clear separation between oscillatory and noise components in
the spectrum. More recent methods, like surrogate data [15]
or nonlinear techiques [2], on the other hand, do not give a
comprehensive framework to deal with noise reduction and
identiﬁcation of determinism in a common ground.

A. The Logistic Map

An archetypal example of a simple nonlinear system capable

of chaotic behavior is given by the logistic map [10]

si = rsi−1(1

si−1).

−

4

With a parameter value of r = 3.6 and initial conditions in
the interval (0, 1), the map (26) displays a weakly chaotic be-
havior, close to quasi – periodic motion. As already discussed
in [3], in this case L
2, indicating that with this low model
complexity is possible to accomplish the separation dictated
by Eqs. (1) and (2).

∼

A case with r = 3.7 and the initial condition in the interval
(0, 1) is analyzed with the KNNR algorithm. The map is
perturbed by a Gaussian white noise with a variance of 0.2,
essentially the same variance of the clean signal.

The power spectrum taken from a sample of 16384 points
of the input signal is presented in Fig. 1. Besides the presence
of some relevant peaks at high frequencies, the spectrum is
basically a white noise.

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

1,5

0,5

1

0

-0,5

)
 
t
 
(
 
y

-1
9700

-4

10

-2

10

f

0

10

Fig. 1.

Log–log plot of the power spectrum of the perturbed logistic map.

Segments of the noisy, clean and ﬁltered time series are
shown in Fig. 2. In order to present all the data in the same
graph, suitable constants have been added to the mean values
of the signals.

Noisy signal

Clean signal

9720

9740

9760

9780

9800

t

Filtered signal

Fig. 2.
logistic map.

Noise reduction by the KNNR algorithm for a strongly perturbed

In Fig. 3 the values of L for increasing sample size are
plotted. A mean squares ﬁt of the resulting data is performed
with respect to the formula

LM = L

−

aM − 1
2 ,

a > 0.

(27)

The type of convergence given in Eq. (27) is suggested by
the ﬁrst order error term Eq. (7), in the anticipation formula of
the original Bagnoli, Berrones and Franci setup. This behavior
of errors is obtained for the case in which the basis components

(26)

5

2

10

)
 

M

 
(
 
L

0

-1

a
h
p
l
a
 
g
o
l

-2

-3

-4

-5
4

1
10

2

10

3
10

4

10

M

5

10

6

10

5

6

8

9

10

7
log M

Fig. 3.

Convergence of L for the perturbed logistic map.

Fig. 4.

Behavior of α with increasing M for the noisy logistic map.

∼

are independent random variables. The fundamental point in
the derivation of Eq. (7) is that the ﬂuctuations of these
components sum in accordance to the Central Limit Theorem
[1]. The numerical results suggest that for strongly chaotic
systems this condition holds. In this example the number of
hidden components converge to a value of order L

102.

The convergence of L constitute a basis for a novel tech-
nique of identiﬁcation of chaos and other types of deterministic
behavior in time series. In real world problems, the availability
of arbitrarily large samples is a rare luxury. The convergence
of L can be however assesed indirectly, through the parameter
α that appears in formula (22). As the sample size grows,
the variance terms of Eq. (22) tend to be constant. In order
to have a ﬁnite value for L, α must be decreasing with M .
M −1. The particular way in
Of course, assimptotically α
∝
which this assimptotic behavior is attained is unknown. By
a smootness assumption a decreasing behavior of α can be
however expected for a range of sample sizes. Note that this
claim is consistent with the curve shown in Fig. 3. On the other
hand, according to the evidence presented in Subsection III-D,
L diverges assimptotically in a linear way with sample size
for linear stochastic processes with ﬁnite correlation lengths.
On these grounds, the proposed test for determinism is a
standard F –test applied to log[α(M )]. Consider the model
βlog(M ) + c, where β is a positive number and c
log(α) =
is a real. These parameters are given by ﬁtting the linear model
to data. The null hypothesis is that β = 0. Numerical results
indicate that the proposed test gives a reliable identiﬁcation
with input signals of moderate length. In this and all of the
following examples the F -test is performed over a set of values
of the parameter α calculated through the KNNR algorithm
for noisy signals with sample sizes of 64, 128, 256, 512, 1024,
2048, 4096, 8192 and 16384.

−

In Fig. 4 is presented log(α) vs log(M ), where log stands
for the natural logarithm. It turns out that F = 42 >>
F0.05(1, 7) = 5.59, so the null hypothesis is clearly rejected
at a 95% conﬁdence level.

B. The H´enon Map

A famous two dimensional extension of the Logistic Map

is the system introduced by H´enon [5],

si = a

−

s2
i−1 + bxi−1,
si = xi−1.

(28)

The canonical values a = 1.4, b = 0.3 are taken. The
iteration of the map (28) is done starting from the initial
conditions s0 = 0.5, x0 = 0.5. The KNNR algorithm is
applied to a case in presence of a noise with variance 1.2
(the variance of the clean signal is 1). The knowledge network
algorithm performs a satisfactory noise reduction of the input
signal. Fig. 5 shows the power spectrum of the clean, noisy
and ﬁltered signals in semilog scale. The input has a length
of 16384 points. The ﬁltered signal captures the overall shape
of the clean spectrum.

For the noisy H´enon system the F –test again indicates
convergence in L at a 95% conﬁdence level. It is found that
F = 7.2 > F0.05(1, 7) = 5.59.

C. The Intermittency Map

In this example the deterministic part of the input

is

generated by the iteration of the intermittency map,

si = β + si−1 + csm

i−1,
d

,

−
d

=

si−1
1

−

0 < si−1

d

≤

d < si−1 < 1

c =

1

−

β
dm

−

d

.

(29)

The map (29) is related to several models that arise in the
study of the phenomenon of intermittency found in turbulent
ﬂuids [14]. Recently, the map (29) has been proposed as
a model for the long term dynamics of packet
trafﬁc in
telecommunication networks [6].

Depending on the parameters, the system (29) can display
spectral properties that range from white noise to 1/f noise.
The values for the parameteres m and d considered here are
m = 2, d = 0.7. The initial condition is taken as 0.01. Two
different cases are studied:

i) β = 0.05.
With this choice of the parameters the map generates a
signal with rapidly decaying correlations. The short – term
correlations are reﬂected in the fact that the spectrum is a

( a )

( a )

6

0,10

0,20

0,30

0,40

0,50

-5

10

-4

10

-3

10

-2

10

-1

10

0
10

0,10

0,20

0,30

0,40

0,50

10

0
10

-5

-4

10

-3

10

-2

10

-1

10

0
10

6

10

)
 
f
 
(
 
P

4

10

6

10

4

10

2

10

)
 
f
 
(
 
P

6

10

4

10

2

10

)
 
f
 
(
 
P

( b )

( c )

f

f

f

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

( b )

( c )

f

f

f

0,10

0,20

0,30

0,40

0,50

10

0
10

-5

-4

10

-3

10

-2

10

-1

10

0
10

Fig. 5.
H´enon map: (a) noisy case, (b) clean signal, (c) ﬁltered signal.

Semilog plots of the power spectra of a signal generated by the

Fig. 6.
Log–log plots of the power spectra of a signal generated by the
intermittency map (β = 0.05): (a) noisy case, (b) clean signal, (c) ﬁltered
signal.

∼

0.1, as shown
white noise for frequencies smaller than
in Fig. 6a. The same chaotic system in the presence of
additive noise is considered in Figures 6b and 6c. The noise
values are independently drawn from a Gaussian distribution
with standard deviation of 0.4 (the standard deviation of the
clean signal is 0.26). The perturbed chaotic signal enters as
input in the KNNR algorithm. In Fig. 7 is shown how the
KNNR algorithm is capable to reduce considerabily the noise.
Morover, the ﬁltered signal has similar spectral properties that
the clean signal, despite the fact that the noisy data displays
an almost ﬂat spectrum at all frequencies.

The behavior of α calculated from samples of the noisy
signal with increasing sample size is shown in Fig. 8. The
F –test gives F = 12.8 > F0.05(1, 7) = 5.59.

ii) β = 0.0005
In this case the correlations decay much more slowly. The
mean squares ﬁt of the power spectrum of the clean signal
f −1.15, with a crossover to
to a power law indicates P (f )
white noise at frequencies

∝
0.001.
Noise reduction is performed to this map in the presence of
independent Gaussian perturbations, taken from a distribution
with standard deviation of 0.5, a value that almost doubles
the standard deviation of the clean signal, which is 0.26. The
Fig. 9 makes clear how the KNNR algorithm is in this case
capable to extract the essentially correct spectral properties
from a very noisy input signal. While the noisy signal has a
f −0.3, which is close
power spectrum described by P (f )

∼

∝

7

Fig. 7.
0.05).

Noise reduction for a strongly perturbed intermittency map (β =

10

-2
10

-5

-4

10

-3

10

-2

10

-1

10

0
10

Noisy signal

Clean signal

Filtered signal

38550

38600
t

38650

38700

2,5

1,5

3

2

1

0

-0,5

-1

-1,5

)
 
t
 
(
 
y

0,5

-2
38500

-0,5

a
h
p
l
a
 
g
o
l

-1

-1,5

-2
4

Fig. 8.
(β = 0.05).

Behavior of α with increasing M for the noisy intermittency map

to the spectrum of a white noise, the ﬁtting of the spectrum
f −1.11.
of the ﬁltred signal to a power law indicates P (f )
The application of the F –test to succesive values of α
calculated by the KNNR algorithm with the noisy signal as
input, gives evidence of the convergence to a ﬁnite L. It is
found F = 13 > F0.05(1, 7) = 5.59.

∝

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

6

10

4

10

)
 
f
 
(
 
P

2

10

0

10

( a )

( b )

( c )

f

f

f

5

6

8

9

10

7
log M

10

-2
10

-5

-4

10

-3

10

-2

10

-1

10

0
10

D. White Noise and Ornstein–Uhlenbeck Processes

10

-2
10

-5

-4

10

-3

10

-2

10

-1

10

0
10

In contrast to deterministic systems, even in the case that
these were chaotic, stochastic systems do not display a conver-
gence in L with increasing observation time. The numerical
experiments indicate that for signals generated by stochastic
processes with a ﬁnite correlation length, L asimptotically
grows linearly with sample size.

The KNNR algorithm is applied to signals generated by
discrete analogous of the white noise and Ornstein–Uhlenbeck
processes: sequences of independent random numbers and the
AR(1) process, respectively.

A sequence of independent Gaussian deviates is generated
by the already mentioned L’Ecuyer algorithm. In Fig. 10 is
presented the behavior of model complexity for a signal in
which the random numbers are drawn from a distribution with
standard deviation of 0.23. The number L diverge linearly.
Performing an F -test in the same way as before (Fig. 11)
gives F = 0.25 << F0.05(1, 7) = 5.59, which indicates that
the hypothesis of a constant α can’t be rejected at the 95%
conﬁdence level.

Log–log plots of the power spectra of a signal generated by the
Fig. 9.
intermittency map (β = 0.0005): (a) noisy case, (b) clean signal, (c) ﬁltered
signal. The clean and ﬁltered signals display very similar spectral properties,
while the noisy signal is close to a white noise.

In Fig. 12 the values of L for increasing sample size are
plotted for three different realizations of an AR(1) process of
the form

yt = yt−1

λyt + εt,

0 < λ < 1.

(30)

−

The term εt is again a Gaussian deviate generated by the
L’Ecuyer algorithm with a standard deviation of 0.23. In the
limit
in which the parameter λ goes to zero the process
(30) tends to a random walk. For other values of λ the
correlations decay exponentially, with a characteristic time
1/λ. The examples considered in Fig. 12 have the parameter
values λ = 0.5, λ = 0.05 and λ = 0.005. Note that L

1
10

2

10

3

10

M

4

10

5

10

2

10

3

10

5

10

6

10

4

10

M

Fig. 10.
diverges linearly with sample size.

L vs. M for a sequence of independent random numbers. L

Fig. 12. Behavior of L with increasing M for AR(1) processes with different
correlation lengths.

4

10

3

10

2

10

)
 

M

 
(
 
L

a
h
p
l
a
 
g
o
l

-0,5

-0,55

-0,6

-0,65

-0,7

-0,75

-0,8
4

5

6

8

9

10

7
log M

Fig. 11.
random numbers.

Behavior of α with increasing M for a sequence of independent

eventually diverges linearly for all cases. The point at which
this divergent regime is attained depends on the correlation
length. In fact, the F –test performed for a maximum sample
size of 16384 rejects the null hypothesis at a 95% conﬁdence
level only in the ﬁrst two cases. This implies that for small
enough samples, linear autocorrelated stochastic processes are
indistinguishable by the KNNR algorithm from chaotic sistems
with similar autocorrelations. This is quite natural taking into
account that
the KNNR algorithm is based on the linear
autocorrelation structure. It must be pointed out however, that
if the stochastic signal at hands has ﬁnite correlation length,
the numerical experiments suggest that the identiﬁcation of
determinism is always possible with large enough sample
sizes.

It’s worth mention that

in all of the examples in this
Subsection, the ﬁltered signals display the same correlation
lengths than the original signals.

IV. CONCLUSION

The proposed formalism constitute a basis for a novel
technique of identiﬁcation of deterministic behavior in time
series. A careful study of the convergence of L as the sample
size grows, may be used to improve the introduced statistical
test. The question of the deﬁnition of the most adequate
statistic and test to be used, e. g. parametric or non–parametric,
deserves further research. In the same direction, statistical tests
could also be made on the basis of a comparision between the

8

)
 

M

 
(
 
L

5
10

4

10

3
10

2

10

1

10

0

10

λ = 0.5

λ = 0.05

λ = 0.005

spectral properties of a signal before and after its ﬁltering by
the KNNR algorithm.

The presented results, on the other hand, give a linear
ﬁlter for noise reduction capable to extract features otherwise
difﬁcult to deduce from traditional linear approches. Further
research should be done on the use of the KNNR algorithm
for noise reduction and forecasting in important ﬁelds of
application.

The presented approach treats the time series in a very
direct manner. The generalization of the KNNR algorithm
to the case in which y depends on more than one variable
could be used to allow delay representations of data. This
may give a more powerful algorithm, capable to identify
deterministic behavior in smaller data sets, and to connect the
presented theory with the important problem of the calculation
of embedding dimensions. This generalization of the study to
higher dimensional data sets could also ﬁnd application in
questions such like the estimation of the optimal number of
hidden neurons in models of artiﬁcial neural networks.

ACKNOWLEDGMENT

The author would like to thank to CONACYT, SEP–
PROMEP and UANL–PAICYT for partial ﬁnancial support.

REFERENCES

[1] F. Bagnoli, A. Berrones and F. Franci, “Degustibus Disputandum
(Forecasting Opinions by Knowledge Networks)”, Physica A 332, 2004,
pp. 509-518.

[2] M. Barahona and C. Poon, “ Detection of Nonlinear Dynamics in Short,

Noisy Time Series: ”, Nature 381, 1996, pp. 215-217.

[3] A. Berrones, “Filtering by Sparsely Connected Networks Under the
Presence of Strong Additive Noise”, to be published in Proc. Seventh
Mexican International Conference on Computer Science (ENC06).
[4] S. Haykin, Neural Networks: a Comprehensive Foundation, Prentice Hall,

[5] M. H´enon, “A Two–Dimensional Mapping with a Strange Attractor”,

Commun. Math. Phys. 50, 1976, 69.

[6] A. Herramilli, R. Singh and P. Pruthi “Chaotic Maps as Models of
Packet Trafﬁc”, Proc. 14th Int. Teletrafﬁc Cong. 35, Elsevier, 1994.
[7] H. Kantz and T. Schreiber, Nonlinear Time Series Analysis. Cambridge

University Press, 2004.

[8] S. Maslov and Y. Zhang, “Extracting Hidden Information from Knowl-

edge Networks”, Physical Review Letters 87, 2001, 248701.

[9] S. Maslov and K. Sneppen, “Speciﬁcity and Stability in Topology of

Protein Networks”, Science 296, 2002, 910.

[10] E. Ott, Chaos in Dynamical Systems.

Cambridge University Press,

1999.

1993.

9

[11] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,
Numerical Recipes in C++. The Art of Scientiﬁc Computing, 2nd ed.
Cambridge University Press, 2002.

[12] O. Renaud, J. Starck and F. Murtagh, “Wavelet–Based Combined
Signal Filtering and Prediction”, IEEE Transactions on Systems, Man
and Cybernetics–Part B 35, 2005, pp. 1241-1251.

[13] C. E. Shannon and W. Weaver, The Mathematical Theory of Informa-

tion, University of Illinois Press, 1949.

[14] H. G. Schuster Deterministic Chaos: An Introduction, 2nd revised ed.

VCH, 1988.

[15] J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, and J. D. Farmer,
“Testing for Nonlinearity in Time Series: the Method of Surrogate Data”,
Physica D 58, 1992, pp. 77-94.

[16] N. Wiener, Cybernetics: Or the Control and Communication in the

Animal and the Machine, 2nd ed. MIT Press, 1965.

( a )
( b )
( c )

)
)
)
 
 
 
t
t
t
 
 
 
(
(
(
 
 
 
y
y
y

3
3
3

2
2
2

1
1
1

0
0
0

-1
-1
-1

-2
-2
-2
0
0
0

10000 20000 30000 40000 50000 60000
10000 20000 30000 40000 50000 60000
10000 20000 30000 40000 50000 60000
t
t
t


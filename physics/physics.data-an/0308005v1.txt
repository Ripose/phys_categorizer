3
0
0
2
 
g
u
A
 
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
0
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Maximum Probability and Maximum Entropy
methods: Bayesian interpretation

M. Grendar, Jr. and M. Grendar

Institute of Mathematics and Computer Science of Mathematical Institute of Slovak Academy of
Sciences (SAS) and of Matej Bel University, Severn´a ulica 5, 974 00 Bansk´a Bystrica, Slovakia &
Institute of Measurement Science of SAS, D´ubravsk´a cesta 9, 841 04 Bratislava, Slovakia.
umergren@savba.sk

Abstract. (Jaynes’) Method of (Shannon-Kullback’s) Relative Entropy Maximization (REM or MaxEnt) can be
- at least in the discrete case - according to the Maximum Probability Theorem (MPT) viewed as an asymptotic
instance of the Maximum Probability method (MaxProb). A simple bayesian interpretation of MaxProb is given
here. MPT carries the interpretation over into REM.

INTRODUCTION

Relationship of the Method of (Shannon-Kullback’s)
Relative Entropy Maximization (REM or MaxEnt)
and Bayesian Method is notoriously peculiar. The
two methods of induction are viewed as unrelated at
all, or opposed, or identical in some circumstances,
or one as a special case of the other one (see [9]).

As it was noted, a ﬁnding that REM can be
viewed as an asymptotic instance of Maximum
Probability method (MaxProb, cf. [4]) implies that
MaxProb/REM/MaxEnt cannot be in conﬂict with
Bayes’ Theorem (cf. [5]).

A beautiful, simple (yet in some extent over-
looked) bayesian interpretation of REM which oper-
ates on the level of samples and employs Conditioned
Weak Law of Large Numbers (CWLLN) was sug-
gested and elaborated at [2]. Csisz´ar’s original argu-
ment together with the Maximum Probability The-
orem (MPT, see [4], Thm 1), inspired a bayesian in-
terpretation of MaxProb and REM methods, which
we intend to present here.

TERMINOLOGY AND NOTATION

Let X , {x1, x2, . . . , xm} be a discrete ﬁnite set called
support, with m elements and let {Xl, l = 1, 2, . . . , n}
be a sequence of size n of identically and indepen-
dently drawn random variables taking values in X.
A type νn , [n1, n2, . . . , nm]/n is an empirical
probability mass function which can be based on se-

quence {Xl, l = 1, 2, . . . , n}. Thus, ni denotes number
of occurrences of i-th element of X in the sequence.
Let P(X) be a set of all probability mass functions
(pmf’s) on X. Let Πn ⊆ P(X) be a set of all types
νn, and let Hn ⊆ Πn.

Let the supposed source of the sequences (and

hence also of types) be q ∈ P(X).

Let π(νn) denote the probability that q will gen-

erate type νn, ie. π(νn) ,

n1! n2! ... nm! Qm

i=1 qni
i .

n!

BAYESIAN INTERPRETATION OF
MAXIMUM PROBABILITY
METHOD

Bayesian recipe prescribes to update prior distribu-
tion (information) by an evidence via Bayes’ The-
orem (BT) to get a posterior distribution. Usually
bayesians use BT to update prior distribution of a
parameter by evidence which has form of random
sample and obtain posterior distribution of the pa-
rameter, given the sample. Then it is customary to
select the value of parameter at which the posterior
distribution attains its maximum (i.e. mode) and
perform further inference.

The bayesian recipe and [2] will be followed here
on a diﬀerent level. A prior distribution of types will
be updated via BT by data of special form. Then
the maximum aposteriori type will be searched out.
The bayesian updating will be carried out in four

steps:

Step 1: Select a probability mass function q which

could be the best guess of source of types νn. It
will specify a prior probability P(νn) of type by
the following simple scheme: P(νn) ≡ π(νn). Thus,
π(νn) is the apriori distribution of types, which is
going to be updated once an evidence (data) will
become available.

Step 2: The data arrive in rather special form: they
specify a set Hn of types νn (which were observed,
or ’feasible’ in some general way). In other words,
the evidence is that types which do not belong to
Hn cannot be observed, or are ’not feasible’.

Step 3: Use Bayes’ Theorem to update the prior
probability of type π(type = νn) by the evidence
”type ∈ Hn” to obtain the posterior probability
P(type = νn|type ∈ Hn) that type is equal to νn
given that it conforms with the evidence (i.e. belongs
to Hn).

P(type = νn|type ∈ Hn) =
P(type ∈ Hn|type = νn) π(type = νn)
P(type ∈ Hn)

Note that P(type ∈ Hn|type = νn) is 0 if νn /∈ Hn
and 1 otherwise. Thus, for νn ∈ Hn the aposteriori
probability is

P(type = νn|type ∈ Hn) =

π(type = νn)
P(type ∈ Hn)

Obviously, P(type ∈ Hn) is given as a ratio of the
number of types in Hn to the number of all types in
Πn.

Step 4: The type(s) with the highest value of the
posterior probability (MAP type) is to be searched
out. Since types which do not belong to Hn have zero
posterior probability, a search for the MAP type can
be restricted to types which belong to Hn. So, the
MAP type ^νn is

^νn , arg max
νn∈Hn

P(type = νn|type ∈ Hn)

Since, for ﬁxed n and any νn, P(type ∈ Hn) is a
constant, the MAP type turns to be

^νn = arg max
νn∈Hn

π(type = νn)

(1)

Thus the MAP type ^νn is just the type in Hn
which has the highest value of the prior probability.

Here it stops.
Observe that (1) is identical with prescription of
the Maximum Probability (MaxProb) method (cf.
[4]). Thus the above reasoning provides its bayesian
interpretation.

HOW DOES IT RELATE TO
REM/MAXENT?

Via Maximum Probability Theorem (MPT, see [4],
Thm 1 and [7]).

Before stating MPT, I-projection has to be
deﬁned. I-projection ^p of q on set Π ⊆ P(X)
I(^pkq) = inf p∈Π I(pkq),
such ^p ∈ Π that
is
where1 I(pkq) , PX pi log pi
is the I-divergence.
qi
I-divergence is known under various other names:
Kullback-Leibler’s distance, KL number, Kullback’s
directed divergence, etc. When taken with minus
sign it is known as (Shannon-Kullback’s) relative
entropy.

(MPT)2 Let diﬀerentiable constraint F(νn) = 0
deﬁne feasible set of types Hn and let H , {p : F(p) =
0} be the corresponding feasible set of probability
mass functions. Let ^νn , arg maxνn∈Hn π(νn). Let
^p be I-projection of q on H. And let n
. Then
^νn = ^p.

→ ∞

MPT shows that REM is an asymptotic instance
of MaxProb method. Thus MPT carries the bayesian
interpretation of MaxProb over into REM/MaxEnt.
Hence, I-projection is just the MAP type which
results from the bayesian updating which was de-
scribed at the previous Section, in the case of suﬃ-
ciently large n.

To sum up: Whenever n is suﬃciently large and
prior will be assigned to types νn as in the Step 1,
and new data will take form as in the Step 2, and
the prior will be updated by the data via BT as in
the Step 3, and MAP type will be searched out as in
the Step 4, then the MAP type will be nothing but
the REM I-projection of q on H.

DISCUSSION

Two questions. The ﬁrst one: Why MAP? Why
not say median aposteriori type? The MAP type
just the I-projection. If
becomes when n
the I-projection is unique then Conditioned Weak
Law of Large Numbers (CWLLN, cf.
[13],
[8], [1], [12], [10], [11]) can be invoked. If read in
the above bayesian manner, it says that any other
type/distribution than I-projection has asymptoti-

→ ∞

[14],

∞

∞

, log b

, 0· (±

0 = +

1 There, log 0 = −
) = 0, conventions
are assumed. Throughout the paper log denotes the natural
logarithm.
2 Originally MPT was stated with unique I-projection case in
mind. Its proof however readily allows to state it in general
form (see [7]). Since the issue of uniqueness is at this Section
irrelevant the MPT will be stated at its original form.

∞

REFERENCES

1. Csisz´ar, I., Ann. Prob., 12, No. 3, 768-793, (1984).
2. Csisz´ar I., “An extended Maximum Entropy

principle and a bayesian justiﬁcation,” in Bayesian
Statistics 2, J. M. Bernardo, M. H. DeGroot, D.
V. Lindley, A. F. M. Smith (eds.), Elsevier, 83-98,
North-Holland, 1985.

3. Grend´ar, M., Jr. and Grend´ar, M., “On the prob-
abilistic rationale of I-divergence and J-divergence
minimization,” arxiv.org/abs/math.PR/0008037,
Aug 2000.

4. Grend´ar, M., Jr. and Grend´ar, M., “What is the

question that MaxEnt answers? A probabilistic
interpretation,” in Bayesian inference and
Maximum Entropy methods, A. Mohammad-
Djafari (ed.), 83-94, AIP, Melville (NY), 2001. Also
arxiv.org/abs/math-ph/0009020, Sep 2000.
5. Grend´ar, M., Jr. and Grend´ar, M., Entropy, 3,

58-63, (2001).

6. Grend´ar, M., Jr. and Grend´ar, M., Acta U. Belii

Ser. Math., 10, 3–8, (2003).

7. Grend´ar, M., Jr. and Grend´ar, M., “Maximum
Entropy method with non-linear moment
constraints: challenges,” Tech Rep IMS SAS,
Bratislava, July 2003. To be presented at MaxEnt
2003.

8. Grooneboom, P., Oosterhoﬀ, J. and Ruymgaart, F.

H., Ann. Prob., 7, 553-586, (1979).
Jaynes, E. T., “The relation of bayesian and
Maximum Entropy methods,” in Maximum Entropy
and Bayesian Methods in Science and Engineering
(Vol. I), G. J. Erickson and C. R. Smith (eds.),
Kluwer, pp. 25-29, 1988.

9.

10. La Cour, B. R. and Schieve, W. C., Jour. Stat.

Phys., 107, 3/4, 729-755, (2002).

11. Leonard, Ch. and Najim, J., Bernoulli, 8, 6,

721–743, (2002).

12. Lewis, J. T., Pﬁster, C.-E. and Sullivan, W. G.,
Markov Proc. Rel. Field., 1, 319-386, (1995).
13. van Campenhout, J. M. and Cover, T. M., IEEE

IT, 27, 483-489, (1981).

14. Vasicek, O. A., Ann. Prob., 8, 142-147, (1980).

Tech Rep of IMS SAS, Bratislava, July 2003. To

be presented at MaxEnt 23.

cally zero posterior probability. So, this is why MAP
and not median. However, what if there are multi-
ple I-projections? Obviously, the bayesian interpre-
tation of MaxProb is valid regardless of the num-
ber of MAP types. MPT in its general form (cf.
[7]) covers also the case of multiple MaxProb types
and claims that they converge to I-projections. Then
one can either recall Entropy Concentration Theo-
rem (cf. [7]) or invoke an extension of CWLLN which
covers also the case of multiple I-projections (cf. [6])
– to answer the ”Why MAP” question in the general
case.

n!

The second one: Why not some other scheme for
assigning prior probability P(νn) to types? If at the
Step 1 the apriori probability was assigned to types
by the following scheme: P(νn) = π(νn)π(nq) where
(n1q1)! (n2q2)! ... (nmqm)! Qm
π(nq) ,
i=1(ni/n)nqi ;
and the remaining Steps were performed then the
reasoning would provide bayesian interpretation of
Jeﬀreys’ Maximum Probability method (cf. [3]). A
Theorem (cf. [3], Thm 3) similar to MPT shows
that the Jeﬀreys’ Maximum Probability type con-
to J-projection of q on H. And
verges as n
Jeﬀreys’ analogue of CWLLN can be easily proved.
It provides answer to the ﬁrst question in this case.

→ ∞

CONCLUDING NOTE

Originally (cf.
[4]), MaxProb was presented as
a method which looks in Hn for a type ^νn =
arg maxνn∈Hn π(νn) which the ’prior’ generator q
can generate with the highest probability. The word
’prior’ was used merely to mean that the generator is
selected before the data arrive. Alternatively, since
unconstrained maximization of the conditional prob-
ability P(type = νn|type ∈ Hn) reduces to maxi-
mization of π(νn) constrained to νn ∈ Hn, Max-
Prob could be interpreted as search for the type with
the highest value of the conditional probability. The
third, bayesian interpretation of MaxProb – inspired
by [2] – was given here. Obviously, MPT stands re-
gardless of what is the preferred interpretation of
MaxProb.

ACKNOWLEDGEMENTS

Supported by the grant VEGA 1/0264/03 from the
Scientiﬁc Grant Agency of the Slovak Republic. It
is a pleasure to thank Viktor Witkovsk´y for valu-
able discussions and comments on earlier version of
this work. The thanks extend to Aleˇs Gottvald and
George Judge. Lapses are mine.


8
9
9
1
 
l
u
J
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
1
0
7
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

On the determination of probability density
functions by using Neural Networks

Llu´ıs Garrido1,2, Aurelio Juste2

1) Dept. d’Estructura i Constituents de la Mat`eria,
Facultat de F´ısica, Universitat de Barcelona,
Diagonal 647, E-08028 Barcelona, Spain.
Phone: +34 93 402 11 91 Fax: +34 93 402 11 98
e-mail: garrido@ecm.ub.es

2) Institut de F´ısica d’Altes Energies,
Universitat Aut`onoma de Barcelona,
E-08193 Bellaterra (Barcelona), Spain.
Phone: +34 93 581 28 34 Fax: +34 93 581 19 38
e-mail: juste@ifae.es

Abstract

It is well known that the output of a Neural Network trained to disen-
tangle between two classes has a probabilistic interpretation in terms of the
a-posteriori Bayesian probability, provided that a unary representation is
taken for the output patterns. This fact is used to make Neural Networks
approximate probability density functions from examples in an unbinned
way, giving a better performace than “standard binned procedures”. In
addition, the mapped p.d.f. has an analytical expression.

PACS’96: 02.50.Ph, 07.05.Kf, 07.05.Mh

(Submitted to Comput. Phys. Commun.)

1

1

Introduction

Estimating a probability density function (p.d.f.) in a n-dimensional space is
a necessity which one may easily encounter in Physics and other ﬁelds. The
standard procedure is to bin the space and approximate the p.d.f. by the ratio
between the number of events falling inside each bin over the total and nor-
malised to the bin volume. The fact of binning not only leads to a loss of
information (which might be important unless the function is smoothly varying
inside each bin) but is intrinsically arbitrary: no strong arguments for a deﬁned
binning strategy, e.g. constant bin size versus constant density per bin, exists.
More sophisticated approaches imply for instance the deﬁnition of an “intel-
ligent” binning, with smaller bins in the regions of rapid function variation.
However, the main drawback still remains: even for a low number of bins per
dimension, large amounts of data are necessary since the number of data points
needed to ﬁll the bins with enough statistical signiﬁcance grows exponentially
with the number of variables. As it will be shown, Neural Networks (NN)
turn out to be useful tools for building up analytical n-dimensional probability
density functions in an unbinned way from examples.

This manuscript is organised as follows: in Sect. 2 the proposed method to
construct unbinned p.d.f.s from examples is described. After a brief introduc-
tion to the statistical interpretation of the output of a Neural Network applied
to pattern recognition in the case of only two classes, an expression for the
mapped p.d.f.
is obtained. Then, a method to quantify the goodness of the
mapped p.d.f. is described. In order to illustrate the concept, an artiﬁcial ex-
ample is discussed in Sect. 3, whereas Sect. 4 is devoted to the discussion of
an example of practical application in High Energy Physics. Finally, in Sect.
5, the conclusions are given.

2 Method

Let us assume that we have a sample of N events distributed among 2 diﬀerent
classes of patterns (
C2), each event e being characterised by a set of
n variables x(e). Each class of patterns has a proportion αi and is generated
by the normalised probability density function Pi(x), i = 1, 2 (in probability
terms, Pi(x) = P (x
i)).

C1 and

i) and αi = P (
C

| C

By minimising over this sample the quadratic output-error E:

E [o] =

o(x(e))

N

1
2N

e=1 h
X

2

.

−

d(x(e))
i

with respect to the unconstrained function o(x), where d(x) takes the value
C1 and 0 for the events belonging to class
1 for the events belonging to class
C2, it can be shown [3, 4, 5, 6] that the minimum is achieved when o(x) is the
a-posteriori Bayesian probability to belong to class

C1:

(2.1)

(2.2)

o(min)(x) =

(
C1 |

P

x).

2

The above procedure is usually done by using layered feed-forward Neural
Networks (see e.g. [1, 2] for an introduction). In this paper we have considered
No, where Ni (No = 1) are
Neural Networks with topologies Ni
the number of input (ouput) neurons and Nh1, Nh2 are the number of neurons
in two hidden layers.

Nh2 ×

Nh1 ×

×

The input of neuron i in layer ℓ is given by,

I ℓ
i =

(

x(e)
i
ijSℓ−1
j + Bℓ
i

j wℓ

ℓ = 1
ℓ = 2, 3, 4

(2.3)

P

i

1) and Bℓ

1), Sℓ−1
j

where x(e)
is the set of n variables describing a physical event e, the sum is
is the state of
extended over the neurons of the preceding layer (ℓ
i is a bias input to neuron i at layer ℓ. The
neuron j at layer (ℓ
−
state of a neuron is a function of its input Sℓ
j ), where F is the neuron
response function. In general the “sigmoid function”, F (I ℓ
j ), is
j ) = 1/(1 + e
chosen since it oﬀers a more sensitive modeling of real data than a linear one,
being able to handle existing non-linear correlations. However, depending on
the particular problem faced, a diﬀerent neuron response function may be more
convenient. For instance, in the artiﬁcial example described below, a sinusoidal
j ) = (1 + sin(I ℓ
neuron response function, F (I ℓ

j ))/2, has been adopted.

j = F (I ℓ

−I ℓ

−

Back-propagation [7, 8, 9] is used as the learning algorithm. Its main ob-
jective is to minimise the above quadratic output-error E by adjusting the wij
and Bi parameters.

Let us now consider the situation we are concerned in this paper: we have
data(x),
a large amount of events (“data”) distributed according to the p.d.f.
whose analytical expression is unknown and which we want precisely to approx-
imate. If a Neural Network is trained to disentangle between those events and
ref (x) (not vanishing in
other ones generated according to any kwown p.d.f.,
data(x) is non-zero), the Neural Network output will approxi-
a region where
mate, after training, the conditional probability for a given event to be of the
“data” type:

P

P

P

o(min)(x)

(data

x)

|

≃ P

data(x)
αdata
P
data(x) + αref

,

ref (x)

P

≡

αdata

P

(2.4)

where αdata and αref are the proportions of each class of events used for training,
satisfying αdata + αref = 1.

From the above expression it is straightforward to extract the NN approxi-

mation to

data(x) as given by:

P

(N N )
data (x) =
P

P

ref (x)

αref
αdata

o(min)(x)

o(min)(x)

.

1

−

(2.5)

As a result, the desired p.d.f.

is determined in an unbinned way from
(N N )
data (x) has an analytical expression since we indeed
P
ref (x) and o(min)(x) is known once we have determined the network

examples. In addition,
have it for
parameters (weights and bias inputs).

P

3

For what the reference p.d.f. is concerned, a good choice would be a p.d.f.
built from the product of normalised good approximations to each 1-dimensional
projection of the data p.d.f., thus making easier the learning of the existing
ref (x) is a normalised p.d.f.
correlations in the n-dimensional space. Since
(N N )
data (x) will depend on the goodness
by construction, the normalisation of
P
of the Neural Network approximation to the conditional probability, so that
in general it must be normalised a-posteriori.
In the artiﬁcial (High Energy
Physics) example shown below, the normalisation of the obtained p.d.f.s was
consistent with 1 at the 1% (3%) level.

P

On the other hand, one would like to test the goodness of the approximation
of the mapped p.d.f. to the true one. Given a data sample containing Ndata
events, it is possible to perform a test of the hypothesis of the data sample under
consideration being consistent with coming from the mapped p.d.f. For that,
one can compute the distribution of some test statistics like the log-likelihood
function of Eq.(2.6), which can be obtained by generating Monte Carlo samples
containing Ndata events generated using the mapped p.d.f.

= log(L) =

L

log(

(N N )
data (x(e)))
P

Ndata

e=1
X

Being

data the value of the log-likelihood for the original data sample, the
conﬁdence level (CL) associated to the hypothesis of the data sample coming
from the mapped p.d.f. is given by:

L

(2.6)

(2.7)

CL =

Ldata

−∞

Z

d

)

(
L

L P

which in practice can be obtained as the fraction of generated Monte Carlo
samples of the data size having a value of the log-likelihood equal or below the
one for the data sample. If the mapped p.d.f. is a good approximation to
data,
the expected distribution for CL evaluated for diﬀerent data samples should
have a ﬂat distribution as it corresponds to a cumulative distribution.

P

3 Artiﬁcial example

In this section we propose a purely artiﬁcial example in order to illustrate how a
Neural Network can perform a mapping of a 5-dimensional p.d.f. in an unbinned
way from examples.

In this example our ”data” will consist in a sample of 100000 events gener-

ated in the cube [0, π]5

R5 according to the following p.d.f.:

∈

data(x) =

P

1
C

(sin(x1 + x2 + x3) + 1)

(3.1)

4 + x2
sin(x2
5)
4 + x2
x2
5

(cid:18)

+ 1

,

(cid:19)

which we want to estimate from the generated events. In the above expression,
data(x) has unit integral. The above
C is a normalisation factor such that
P
p.d.f. has a rather intrincate structure of maxima and minima in both, the

4

3-dimensional space of the ﬁrst three variables and the 2-dimensional space of
the two last variables.

In order to map the above p.d.f., we need to train a Neural Network to dis-
data(x) and events generated
entangle between events generated according to
data(x) is diﬀerent
ref (x) non-vanishing in any region where
according to any
from zero. In order to make easier the learning of the existing correlations in
ref (x) is chosen as the product
the 5-dimensional space, as explained before,
data(x), properly
of good approximations to the 1-dimensional projections of
normalised to have unit integral.

P

P

P

P

P

In the case of data p.d.f., it turns out that the 1-dimensional projections of
the three ﬁrst variables are equal and essentially ﬂat, whereas the 1-dimensional
projections for the two last variables can be parametrised as a 4th degree poli-
nomial (P4). Therefore, we choose as reference p.d.f.:

·

P

P4(x5)

ref (x) =

1
C ′ P4(x4)
and generate a number of 100000 events according to it. As before, C ′ is a
normalisation factor so that

ref (x) has unit integral.
After the training and normalisation, the p.d.f. given by Eq.(2.5) constitutes
data(x), as it is indeed observed in Fig. 1,
a reasonably good approximation to
where both are compared for diﬀerent slices in the 5-dimensional space with
respect to the variable x1. For comparison, it is also shown the reference p.d.f.
which, as expected, is unable to reproduce the complicated structure of maxima
and minima in the 5-dimensional space.

(3.2)

P

P

As explained in previous section, it is posible to perform a test of the good-
ness of the mapped p.d.f. For that, a number of 10000 Monte Carlo samples
have been generated with the mapped p.d.f., each one containing 100000 events,
which is the same number of events of the ”data” sample. The log-likelihood
is computed for each MC sample and its distribution is shown in Fig. 2a), in
which the arrow indicates the value of the log-likelihood for the original data
data we have found a
sample (
conﬁdence level of 5.5% associated to the hypothesis of the data sample coming
from the mapped p.d.f. This seems a low CL and needs further comments,
but as we know the true p.d.f given by Eq.(2.5), we can do much better than
performing a single measurement for CL and is to ﬁnd out its distribution.

data). From this distribution and the value of

L

L

Very often in High Energy Physics and other ﬁelds the problem consist
on estimating a p.d.f.
from a sample of simulated Monte Carlo events which
is much larger (typically a factor 100 times larger) than the experimental data
sample over which we should use this p.d.f (see the High Energy Physics example
of Sect. 4). For this reason we have obtained the CL distribution in three
diﬀerent scenarios: when the number of experimental data events (Nexp ) has
the same number of events as the data sample used to obtain the mapped p.d.f.
(Ndata = 100000), and two with smaller statistics, one with Nexp = 10000 and
another with Nexp = 1000.

A number of 10000 Monte Carlo samples have been generated with the
mapped p.d.f., each containing Nexp events, for the three diﬀerent values of
Nexp and the log-likelihood is computed for each sample in all three scenarios.

5

On the other hand, a number of 1000 data samples are generated with the true
in the three scenarios and the conﬁdence level is computed according
p.d.f.
to Eq.(2.7). The distribution of CL is shown in Fig. 2b) for Nexp = 1000
(dotted line), 10000 (dashed line) and 100000 (solid line). It can be observed
that for Nexp = 1000 the distribution of CL is to a good approximation a
ﬂat distribution whereas for Nexp = 10000 it starts deviating from being ﬂat,
which indicates that the statistics of the data sample is high enough to start
“detecting” systematic deviations in the mapped p.d.f. with respect to the true
one.

In the case of Nexp = 1000 which, as mentioned above illustrates a common
situation in High Energy Physics, the mapped p.d.f. turns out to be a good
enough approximation when used for the smaller experimental data sample. In
the other extreme, Nexp = 100000, which illustrates the situation in which there
is a unique data sample from which one wants to estimate the underlying p.d.f.,
it can be observed in Fig. 2b) (solid line) the existence of enough resolution to
detect systematic deviations in the mapped p.d.f. with respect to the true one.
It should be stressed the very complicated structure of the true p.d.f., which
makes extremely diﬃcult its accurate mapping and nevertheless the diﬀerence
between both distributions are the ones observed in Fig. 1 between the solid
and the dashed lines. In such situations we can not use the mapped p.d.f. for
ﬁne probability studies but it is clear that it is still very useful for other kind
of studies like classiﬁcation or discrimination.

4 High Energy Physics example

In order to illustrate the practical interest of p.d.f. mapping, the following High
Energy Physics example is considered.

One of the major goals of LEP200 is the precise measurement of the mass
of the W boson. At energies above the WW production threshold (√s > 161
GeV) W bosons are produced in pairs and with suﬃcient boost to allow a
competitive measurement of the W mass by direct reconstruction of its product
decays. Almost half of the times (45.6%) both W bosons decay hadronically,
so that four jets of particles are observed in the ﬁnal state.

Most of the information about the W mass is contained in the reconstructed
di-jet invariant mass distribution, so that MW can be estimated by perform-
ing a likelihood ﬁt to this 2-dimensional distribution. Therefore, the W mass
estimator, ˆMW , is obtained by maximising the log-likelihood function:

N

(MW ) =

L

log

P

(s

′(e)
1

, s

′(e)
2

MW )

|

(4.1)

e=1
X
′(e)
(s
with respect to MW , where
1
event e, characterised by the two measured invariant masses (s
MW which, accounting for the existing background, can be expressed as:

MW ) represents the probability of
), given

′(e)
2

′(e)
2

′(e)
1

, s

, s

P

|

(s

′
1, s

′
2 |

P

MW ) = ρww

ww(s

MW ) + (1

ρww)

bckg(s

−

P

′
1, s

′
2 |

P

′
1, s

′
2).

(4.2)

6

P

ww and

bckg are respectively the p.d.f.

In the above expression ρww is the expected signal purity in the sample and
for signal (W-pair production) and
P
background in terms of the reconstructed di-jet invariant masses. For a typical
selection procedure above threshold at LEP200, signal eﬃciencies in excess of
80% with a purity at the level 80% can be obtained in the fully hadronic decay
channel.

Therefore, in order to determine MW , we need to obtain both p.d.f.s, for

signal and background, in terms of the reconstructed di-jet invariant masses.

At √s = 172 GeV and after selection, most of the background comes from
for the background, a 2-5-2-1 Neural Network was
QCD. To map the p.d.f.
trained with
6000 selected q ¯q Monte Carlo events generated with full detector
simulation (“data”) and the same number of “reference” Monte Carlo events
generated according to the 1-dimensional projections of the “data” sample.

∼

As far as the signal p.d.f.

is concerned, it depends on the parameter we
want to estimate: MW .
It can be obtained by a folding procedure of the
theoretical prediction for the 3-fold diﬀerential cross-section in terms of the 2
di-quark invariant masses (s1 and s2) and x (the fraction of energy radiated in
the form of initial state photons), with a transfer function T , which accounts
for distortions in the kinematics of the signal events due to fragmentation,
detector resolution eﬀects and biases in the reconstruction procedure. This
transfer function represents the conditional probability of the reconstructed
invariant masses given some invariant masses at the parton level and initial
state radiation (ISR). The ISR is most of the times lost along the beam pipe
and therefore unknown, reason for which it must be integrated over. This
conditional probability is given by:

T (s

′
1, s

′
2 |

s1, s2, x) =

f (s′

2, s1, s2, x)

1, s′
g(s1, s2, x)

,

(4.3)

where s′
i stands for each reconstructed invariant mass and g(s1, s2, x) is theo-
retically known and has a compact expression, reason for which there is no need
to map it.

Then, the goal is to map the 5-dimensional p.d.f. f (s′

2, s1, s2, x). To do it,
a 5-11-5-1 Neural Network was trained with 40000 hadronic WW Monte Carlo
events generated with full detector simulation (“data”) and the same number
of “reference” events generated according to the 1-dimensional projections of
the “data” sample.

1, s′

In order to test that the event-by-event p.d.f. is meaningful, the predicted
1-dimensional projection of the average invariant mass distribution is compared
to Monte Carlo in Figs. 3a) and b) for both signal and background by using the
obtained
bckg, respectively. Note the overall good agreement between
ww and
the distributions.

P

P

The unbiasedness of the obtained estimator is checked by computing the
calibration curve with respect the true parameter by performing a large number
of ﬁts to Monte Carlo samples generated with diﬀerent values of MW .

The performance of the NN in mapping a n-dimensional p.d.f. has been
compared to the “box method” [10], a standard procedure to build up binned
p.d.f.s. In the case of the background p.d.f., which is only 2-dimensional, the

7

“box method” yielded reasonable results as shown in Fig. 3b), while in the
it showed strong limitations which made im-
case of the 5-dimensional p.d.f.
possible its application. The main reason is the time required to compute the
ﬁnal p.d.f which needs an integration on top of the adjustement of the “box
method” parameters (initial box size, minimum number of MC points inside
each box, etc) in a space of high dimensionality and limited statistics. Is in this
environment where the mapping of p.d.f.s by means of NNs may be superior to
“standard binned procedures” in terms of accuracy (the p.d.f. is determined in
an unbinned way from examples) and speed (the resulting p.d.f. is an analytic
function).

5 Conclusions

We have shown that Neural Networks are useful tools for building up n-dimen-
sional p.d.f.s from examples in an unbinned way. The method takes advantage
of the interpretation of the Neural Network output, after training, in terms
of a-posteriori Bayesian probability when a unary representation is taken for
the output patterns. A purely artiﬁcial example and an example from High
Energy Physics, in which the mapped p.d.f.s are used to determine a parameter
through a maximum likelihood ﬁt, have also been discussed. In a situation of
high dimensionality of the space to be mapped and limited available statistics,
the method is superior to “standard binned procedures”.

6 Acknowledgements

This research has been partly supported by CICYT under contract number
AEN97-1697.

References

[1] J.A. Hertz, A. Krogh and R.G. Palmer, Introduction to the theory
of neural computation, Addison-Wesley, Redwood City, California (1991).

[2] B. M¨uller and J. Reinhardt, Neural networks: an introduction,

Springer-Verlag, Berlin (1991).

[3] Ll. Garrido and S. G´omez, Analytical interpretation of feed-forward

nets outputs after training, Int. J. of Neural Systems 7 (1996) 19.

[4] A. Papoulis, Probability, random variables and stochastic processes,

McGraw-Hill, New York (1965).

[5] D.W. Ruck, S.K. Rogers, M. Kabriski, M.E. Oxley and B.W.
Suter, The multilayer perceptron as an approximation to a Bayes optimal
discriminant function, IEEE Trans. Neural Networks 1 (1990) 296.

[6] E.A. Wan, Neural network classiﬁcation: a Bayesian interpretation, IEEE

Trans. Neural Networks 1 (1990) 303.

8

[7] D.E. Rumelhart, G.E. Hinton and R.J. Williams, Learning repre-

sentations by back-propagating errors, Nature 323 (1986) 533.

[8] D.E. Rumelhart, G.E. Hinton and R.J. Williams, Learning internal
representations by error propagation. In Parallel Distributed Processing,
D.E. Rumelhart and J.L. McClelland (eds.), MIT Press, Vol. 1, Cambridge,
MA (1986) 318.

[9] P. Werbos, Beyond regression: new tools for prediction and analysis in

the behavioral sciences, Ph.D. thesis, Harvard University (1974).

[10] D.M. Schmidt, R.J. Morrison and M.S. Witherell, A general
method of estimating physical parameters from a distribution with ac-
ceptance and smearing eﬀects, Nucl. Inst. and Meth. A328 (1993) 547.

9

Figure captions

•

•

•

Figure 1: Comparison between the true (solid line) and the mapped
(dashed line) and the reference (dotted line) p.d.f. versus x1 for diﬀerent
slices in the 5-dimensional space:
(a) x2 = x3 = x4 = x5 = 0, (b)
x2 = x1, x3 = x4 = x5 = 0, (c) x3 = x2 = x1, x4 = x5 = 0 and (d)
x4 = x3 = x2 = x1, x5 = 0.

Figure 2: (a) Distribution of the log-likelihood computed for Monte
Carlo samples of 100000 events generated according to the mapped p.d.f.
The arrow indicates the value of the log-likelihood for the original data
sample. (b) Distribution of the conﬁdence level for data samples contain-
ing 1000 (dotted line), 10000 (dashed line) and 100000 (solid line) events
respectively, generated with the true p.d.f., of being consistent with the
hypothesis of coming from the mapped p.d.f.

Figure 3: Comparison between NN (solid line) and Monte Carlo (points
with error bars) prediction for the average di-jet invariant mass p.d.f. for
a) signal and b) background.
In b), the p.d.f. as obtained by a box
method (dashed line) is also shown.

10

(a)

(b)

0

1

2

0

1

2

)
x
(

a
t
a
d
P

)
x
(

a
t
a
d
P

0.02
0.018
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0

0.02
0.018
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0

3
x1
(d)

3
x1

0

1

2

0

1

2

Figure 1: Comparison between the true (solid line), the mapped (dashed line) and the
reference (dotted line) p.d.f. versus x1 for diﬀerent slices in the 5-dimensional space: (a)
x2 = x3 = x4 = x5 = 0, (b) x2 = x1, x3 = x4 = x5 = 0, (c) x3 = x2 = x1, x4 = x5 = 0 and
(d) x4 = x3 = x2 = x1, x5 = 0.

0.02
0.018
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0

0.02
0.018
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0

3
x1
(c)

3
x1

11

ﬁ
ﬁ
Nexp=105

(a)

0
-5280

-5275

-5270

-5265

-5260

-5255
10-2 Log(L)

-5250

(b)

0
0
1
/
s
e
i
r
t
n
E

0
1
/
s
e
i
r
t
n
E

120

100

80

60

40

20

350

300

250

200

150

100

50

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1
CL

Figure 2: (a) Distribution of the log-likelihood computed for Monte Carlo samples of 100000
events generated according to the mapped p.d.f. The arrow indicates the value of the log-
likelihood for the original data sample. (b) Distribution of the conﬁdence level for data samples
containing 1000 (dotted line), 10000 (dashed line) and 100000 (solid line) events respectively,
generated with the true p.d.f., of being consistent with the hypothesis of coming from the
mapped p.d.f.

12

y
t
i
s
n
e
D
 
y
t
i
l
i

b
a
b
o
r
P

y
t
i
s
n
e
D
 
y
t
i
l
i

b
a
b
o
r
P

0.2

0.175

0.15

0.125

0.1

0.075

0.05

0.025

0

0.0225
0.02
0.0175
0.015
0.0125
0.01
0.0075
0.005
0.0025
0

67.5

70

72.5

75

77.5

80

82.5

85

(M1+M2)/2 [GeV/c2]

20

30

40

50

60

70

80

(M1+M2)/2 [GeV/c2]

Figure 3: Comparison between NN (solid line) and Monte Carlo (points with error bars)
prediction for the average di-jet invariant mass p.d.f. for a) signal and b) background. In b),
the p.d.f. as obtained by a box method (dashed line) is also shown.

13


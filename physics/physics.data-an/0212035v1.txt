Fisher Information With Respect to Cumulants

S. Prasad and N. C. Menicucci

Center for Advanced Studies and Department of Physics and Astronomy
University of New Mexico
Albuquerque, New Mexico 87131
(Dated: February 21, 2014)
Abstract
Fisher information is a measure of the best precision with which a parameter can be estimated
from statistical data. It can also be deﬁned for a continuous random variable without reference
to any parameters, in which case it has a physically compelling interpretation of representing
the highest precision with which the ﬁrst cumulant of the random variable, i.e.
its mean, can
be estimated from its statistical realizations. We construct a complete hierarchy of information
measures that determine the best precision with which all of the cumulants of a continuous or
discrete random variable can be estimated from its statistical realizations. Many properties of
these information measures and their generating functions are discussed.

2
0
0
2
 
c
e
D
 
8
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
3
0
2
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

I.

INTRODUCTION

Statistical estimation theory furnishes a variety of useful estimates of deterministic pa-
rameters from statistical observations typical of a physical experiment [1]. Fisher information
constitutes a central concept in this theory [2]. Its inverse yields the best lower bound, also
called the Cramer-Rao lower bound (CRLB), on the variance of any unbiased estimator
of a continuous parameter, and thus the best precision with which the parameter can be
extracted from statistical measurements [1, 2].

Fisher information is always deﬁned with respect to one or more parameters of a distribu-
tion. Even without an explicit parameterization, a continuous probability density function
(pdf) may be regarded as being implicitly parameterized in terms of a translational location
parameter, e.g. its mean, median, or mode. For a pdf speciﬁed on the inﬁnite interval, the
Fisher information relative to such a purely translational parameter is easily seen to be inde-
pendent of it, and is called the Fisher information of the random variable itself [2]. However,
we must not lose sight of the implicit location-parameterization behind this nomenclature.
The notion of Fisher information of a random variable has been applied to the case in
which the random variable is the sample based mean [3]. In the limit of large sample size,
asymptotic estimates have been obtained in terms of the cumulants of the underlying pdf
and their derivatives.

In this paper, we present a further generalization of the Fisher information of a contin-
uous random variable. Instead of using only implicit parameterizations, we will explicitly
parameterize all smooth, well-behaved pdf’s in terms of their cumulants [4, 5]. Since the
ﬁrst cumulant is the mean of the pdf, the Fisher information with respect to the ﬁrst cumu-
lant should be equivalent, as we shall also explicitly prove, to the Fisher information of the
random variable, with the mean serving as the ﬁducial location parameter. More generally,
since the set of all cumulants of a pdf uniquely and completely speciﬁes the pdf, the Fisher
information matrix relative to all the cumulants should represent, in eﬀect, the ﬁdelity of
estimation of the full pdf from data. The choice of cumulants to parameterize a pdf is a
particularly convenient one since, as we shall see, it leads to a simple analytical form for the
Fisher information matrix.

These concepts can be generalized further to the case of a discrete, integer-valued random
variable. Any such distribution can similarly be completely parameterized in terms of its
cumulants, and the same interpretation of these information measures may be applied.
We shall present some useful properties of these information measures and discuss their
generating functions and illustrate our considerations with the help of examples of physically
interesting distributions.

II. FISHER INFORMATION OF A CONTINUOUS RANDOM VARIABLE

Given a continuous random variable X distributed according to the pdf p(x) (with x a
θ) =
θ) with respect to

statistical realization of X), we may deﬁne a parameterized version of this pdf, p(x
|
p(x

θ) = p(x) if θ = 0. The Fisher information of p(x
|

θ). Note that p(x
|

−

2

θ is

Jθ =

∂ ln p(x
|

θ)

∂θ

2

+

(cid:19)

dx p(x
|

θ)

∂ ln p(x
|

θ)

2

∂θ

(cid:19)
∂ ln p(x
∂x

−

(cid:18)

θ)

dx p(x

dx
p(x)

−

dp
dx

−

(cid:18)
2

≡

J(X) ,

*(cid:18)
∞

Z

−∞
∞

Z

−∞
∞

−∞

=

=

=

2

θ)

(cid:19)

Z

(cid:18)

(cid:19)
where the angled brackets in the ﬁrst line indicate expectation value, and J(X) is the Fisher
information of the random variable X [2]. Note that because the integration is over the
inﬁnite interval, Jθ is independent of θ. Thus, any location parameter may be used for θ,
and J(X) is a functional only of the shape of the distribution, independent of its absolute
location on any axis.

The Fisher information of a random variable J(X) has two well-known interpretations
[1, 2]. First, J(X) quantiﬁes the statistical precision with which the location parameter
θ of a pdf on which the pdf depends translationally can be estimated from data drawn
according to the pdf p(x
θ). On the other hand, because J(X) measures the mean squared
slope of the log-likelihood function ln p(x), it typically correlates with the narrowness of
the pdf or, equivalently, with the degree of statistical reproducibility of the values assumed
by the variable X.
In the Bayesian context, this narrowness is related to the extent of
prior knowledge about X. These two interpretations are related in that a narrower pdf will
provide higher “resolution” when used as a measurement tool for determining the location
parameter θ.

−

A question of central interest in this paper is the following: What information measures
characterize the ﬁdelity of a statistical determination of the full pdf, not just its location
parameter? We shall see presently that a particularly simple answer to this question can
be obtained in terms of the Fisher information matrix elements relative to the cumulants of
the pdf of the random variable.

III. CUMULANTS OF A PDF AND ASSOCIATED FISHER INFORMATION

Every pdf p(x) has a unique characteristic function associated with it,

M(ν)

eiνx

=

≡ h

i

dx p(x)eiνx ,

∞

−∞

Z
which, in most cases, may be expressed in a power series in ν in terms of the moments of
the pdf:

(1)

(2)

(3)

where µ′
is the nth moment of the pdf (about 0). Writing the logarithm of the char-
acteristic function in a similar series form deﬁnes the cumulants κn of the pdf as coeﬃcients

n ≡ h

xn

i

M(ν) =

(iν)n
n!

µ′
n ,

∞

n=0
X

3

in the series expansion [4, 5]

(4)

(5)

(6)

(7)

(8)

L(ν)

ln M(ν)

≡

∞

≡

n=1
X

(iν)n
n!

κn .

κn =

1
in

dnL
dνn

.

ν=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Note that the function L(ν) may be regarded as the generating function for the cumulants,
since

The cumulants κn are related to the mean and central moments of the pdf, the ﬁrst few of
them taking the form

κ1 = µ′
1,

κ2 = µ2,

κ3 = µ3,

κ4 = µ4 −

2µ2
2,

. . .

,

where µ′
1 and µ2 are the mean and variance of the pdf, while its third and fourth central
moments, µ3, µ4, are related to its skewness and kurtosis. By exponentiating Eq. (4), we
may write the characteristic function in terms of the cumulants,

M(ν) = exp

∞

(iν)n
n!

κn .

n=1
X
Since a pdf is related to its characteristic function by a Fourier transform, we may thus
parameterize p(x) in terms of the entire collection of its cumulants (indicated by κ) as

p(x)

κ)

p(x
|

≡

→

1
2π

∞

−∞

Z

dν e−iνx exp

∞

(

n=1
X

(iν)n
n!

κn

.

)

Not all pdf’s may be parameterized in terms of their cumulants. A necessary condition
for such a parameterization is that the characteristic function M(ν) must be analytic at
ν = 0. Otherwise, the cumulant generating function L(ν) is not analytic and cannot be
expanded in a power series in ν. Such is the case for the Lorentzian pdf, which is given by

p(x) =

1
π

(x

1
2 Γ
m)2 + ( 1

,

2Γ)2

−
where m is the median of the distribution and Γ is the full width at half maximum. Neither
its characteristic function, M(ν) = exp(imν
/2), nor its cumulant generating function,
Γ
/2, is analytic at ν = 0.
L(ν) = imν

−

Γ

ν

ν

|

|

The Fisher information with respect to a set of real estimation parameters θ

−

|

|

is deﬁned as a positive semi-deﬁnite matrix with elements [1, 2]

θ1, θ2, . . .
}

≡ {

(9)

J (θ)
mn ≡

(cid:28)

θ)

∂ ln p(x
|
∂θm

∂ ln p(x
|
∂θn

θ)

,

(cid:29)

θ), of the statistical data from which
where the expectation value is taken over the pdf, p(x
|
the parameters are estimated. The superscript (θ) indicates the parameter set being used,

4

and both n and m range over the indices of the parameters in θ. For a continuous pdf over
the inﬁnite interval, this becomes

J (θ)
mn =

∞

−∞

Z

dx
p(x
|

θ)

θ)

∂p(x
|
∂θm

∂p(x
|
∂θn

θ)

.

(10)

Based on this deﬁnition, the Fisher information matrix relative to the cumulants treated as
estimation parameters follows readily from Eq. (8). First, we note that a partial derivative of
that equation relative to κn generates an extra factor of (iν)n/n! in the integrand, but that
κ)
can equally well be regarded as (
with respect to x. This observation yields the following Fisher information matrix elements
relative to the cumulants:

1)n/n! times the nth order partial derivative of p(x
|

−

J (κ)
mn =

(

1)m+n
−
m!n!

∞

−∞

Z

dx
p(x
|

κ)

κ)

∂mp(x
|
∂xm

∂np(x
|
∂xn

κ)

,

m, n = 0, 1, 2, . . . .

(11)

While κ0 is not a true cumulant, including the possibility that n and/or m = 0 in Eq. (11)
leads formally to a particularly convenient generating function for these elements, as we
shall see in Sec. IV.

Equation (11) is the most important result of this paper. It deﬁnes a complete hierarchy
matrix of information measures, which we may call the cumulant information matrix (CIM).
The diagonal elements of the inverse of the CIM yield the full hierarchy of CRLB’s on how
κ) may be estimated from the statistical realizations
precisely the various cumulants of p(x
|
of X [1]. For m = n = 1, the CIM element J (κ)
reduces to J(X) deﬁned in Eq. (1) as the
11
Fisher information of the random variable. This result conﬁrms our earlier interpretation of
J(X) as the precision with which a ﬁducial location parameter of the pdf may be estimated.
After all, without a knowledge of any higher order cumulants, it is the ﬁrst cumulant, namely
the mean, that furnishes the most useful location parameter of a pdf.

IV. A GENERATING FUNCTION FOR THE CUMULANT INFORMATION MA-
TRIX

A useful technique for evaluating the CIM elements is the method of generating functions.
Upon multiplying both sides of Eq. (11) by λmµn, then summing over all non-negative
integral values of m, n, and ﬁnally interchanging the order of integration and summations,
we obtain the following result:

J (κ)(λ, µ)

J (κ)
mn λmµn

∞

m,n=0
X
∞

−∞

Z

≡

=

dx
p(x
|

κ) "

∞

m=0
X

(

λ)m
−
m!

∂mp(x
|
∂xm

κ)

∞

# "

n=0
X

(

µ)n
−
n!

∂np(x
|
∂xn

κ)

.

#

(12)

(13)

5

The two pairs of square brackets in Eq. (13) enclose the Taylor expansions of p(x
p(x

κ) and
κ), respectively.1 We thus arrive at a rather simple form of the function J (κ)(λ, µ):

λ
|

−

µ

−

|

J (κ)(λ, µ) =

∞

p(x

−

dx

−∞

Z

κ)

µ
|

−

.

λ

κ)p(x
|
κ)
p(x
|

The function J (κ)(λ, µ) given by Eq. (14) is a generating function for the CIM elements,

since from Eq. (12) an arbitrary element J (κ)

mn may be expressed as its partial derivative

J (κ)
mn =

1
m!n!

∂m+n
∂λm∂µn J (κ)(λ, µ)
0n = J (κ)

λ=µ=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

As mentioned earlier, the elements J (κ)
n0 do not correspond to any information relative
to any cumulants, but they are included to complete the Taylor expansions in Eq. (13) in
order to derive the simple generating function (14). It is easy to see from the deﬁnition (11)
that these extraneous elements vanish for n

1, for

0n = J (κ)
J (κ)

n0 =

∞

−∞

dx

∂np(x
|
∂xn

=

∂n−1p(x
|
∂xn−1

∞

κ)

= 0 ,

Z
00 = 1, the pdf normalization. In other words, J (κ)

0n = J (κ)

−∞
(cid:12)
(cid:12)
(cid:12)
n0 = δn0, with δab being the
(cid:12)

≥
κ)

while J (κ)
Kronecker delta function.

V. EXAMPLE: GAUSSIAN PDF

The Gaussian pdf provides a useful illustration of our results. Its cumulants are easily

evaluated. Speciﬁcally, if the pdf has the form

p(x) =

1
√2πσ2

e−(x−x0)2/2σ2

,

then its characteristic function is also Gaussian:

M(ν)

iνx0 −
(cid:0)
Thus, for the Gaussian pdf, the ﬁrst two cumulants are, as expected, its mean x0 and variance
σ2, while all higher order cumulants vanish identically. Parameterizing the Gaussian in terms
of its cumulants κ, we obtain, very simply,

= exp

(17)

≡ h

(cid:1)

i

.

ν2σ2/2

eiνx

The CIM generating function can be easily evaluated for the Gaussian pdf (18), for which

the integrand in Eq. (14) is also Gaussian and easily integrated, with the result

1 The uniform convergence of these Taylor expansions for λ, µ lying within the radius of convergence of

p(x) justiﬁes a posteriori the interchange of integration and summations used in deriving Eq. (13).

κ) =

p(x
|

1
√2πκ2

e−(x−κ1)2/2κ2 .

J (κ)(λ, µ) = eλµ/κ2 = eλµ/σ2

.

6

(14)

(15)

(16)

(18)

(19)

The individual CIM elements then follow from a use of Eq. (15):

vanishing whenever n
variance of any unbiased estimators

= m. The diagonal nature of the CIM implies simple CRLB’s on the
of the cumulants of the pdf:

For general, biased estimators, the right-hand side of Eq. (21) must be replaced by the
diagonal element of the matrix BT IB, where the matrix I is the inverse of the CIM,
nn
element equal to n!σ2n, B is the bias
which for the Gaussian case is diagonal with the nn
matrix, with elements

−

−

J (κ)
mn =

δmn
n!σ2n ,

var(ˆκn)

κn)2

n!σ2n .

{

≡

ˆκn}
(ˆκn −
(cid:10)

≥

(cid:11)

Bmn =

∂
ˆκmi
h
∂κn

,

and BT is its transpose [1].

The k-statistics are minimum-variance unbiased estimators of the cumulants of a dis-
tribution based on a ﬁnite sample drawn from that distribution [7, 8, 9]. The ﬁrst few
k-statistics are given by

k1 = µ

k2 =

k3 =

N

−

N

m2

1

−

N 2
m3
(N
1)(N
2)
N 2[(N + 1)m4 −
−
−

−

3(N
2)(N

1)m2
2]
3)

(N

k4 =

1)(N

−
−
N
j=1 xj is the mean of that sample,
µ)n is the nth central moment of that sample. The k-statistics are
P
= κn, but they are also the best
kni
h
var(ˆκn),

where N is the number of points in a sample, µ = 1
N
and mn = 1
N
not just unbiased estimators for the cumulants in that
sample-based estimators of the cumulants in the sense that var(kn)
where ˆκn is any other unbiased estimator of the cumulants [9].

j=1(xj −

κn)2

(22d)

P

≤

≡

N

,

We should be able to check our CRLB’s for the Gaussian cumulants by evaluating the

(kn−
(cid:10)

(cid:11)

variance of the k-statistics. For a general distribution, these variances are given by

var(k1) =

var(k2) =

var(k3) =

var(k4) =

κ2
N
κ4
N
κ6
N
κ8
N

+

+

+

+

(N

2κ2
2

+

N
1
−
9κ2κ4
N
1
−
16κ2κ6
N
1
−
72Nκ2
2κ4
1)(N

−

−

9κ2
3

1

N
−
48κ3κ5
N
1

+

−
+

2)

(N

+

(N

6Nκ3
2
1)(N

−
34κ2
4
N
1

+

−
144Nκ2κ2
3
1)(N

−

−

7

2)

−

+

2)

(N

24(N + 1)Nκ4
2
2)(N
1)(N

−

−

3)

−

.

(23d)

(20)

(21)

(22a)

(22b)

(22c)

(23a)

(23b)

(23c)

6
These simplify greatly for the Gaussian pdf for which κ =

x0, σ2, 0, 0, . . .
}

:

{

For N independent, identically distributed points drawn according to a distribution, the
total Fisher information is just N times the Fisher information for a single point [2]. Thus,
the CRLB’s for N trials are simply decreased by a factor of N from the bounds for one trial.
These latter bounds for the k-statistics are given as the rightmost expression in each of the
following equations:

var(k1) =

var(k2) =

var(k3) =

var(k4) =

κ2
N
2κ2
2

N

−

(N

(N

−

1
6Nκ3
2
1)(N
−
−
24(N + 1)Nκ4
2
2)(N
1)(N

2)

−

−

.

3)

var(k1) =

var(k2) =

var(k3) =

var(k4) =

=

κ2
N
2κ2
2

N

σ2
N

>

2σ4
N

−

1
6Nκ3
2
1)(N
−
−
24(N + 1)Nκ4
2
2)(N
1)(N

2)

>

(N

(N

6σ6
N

>

3)

24σ8
N

.

(24a)

(24b)

(24c)

(24d)

(25a)

(25b)

(25c)

(25d)

−

−

→ ∞

−
Notice that k1 is an eﬃcient estimator, since var(k1) equals the CRLB, and the others are
asymptotically eﬃcient in the limit N
. This is really the best one can do. While one
data point does give information about the mean, it gives no information about the variance
or higher cumulants. At least two points are needed to estimate the variance. In general,
a sample of j data points carries no information about the cumulants of order (j + 1) or
greater. The CIM does not take this peculiar dependence of information on the sample size
into account, each data point having the same information as any other about a particular
cumulant. In the present Gaussian case, this information per data point about κj is 1/j!σ2j.
However, in the inﬁnite-N limit, this sample-size-dependent loss of information about the
higher order cumulants becomes negligible, and the bound is asymptotically achieved for
the estimation of any cumulant.

Two remarks are in order here. First, although any unbiased estimator of a third or higher
order cumulant is, on average, zero for the Gaussian pdf, there is a ﬁnite statistical scatter
in the data from which the cumulants, regardless of their order, are estimated. Second, the
sharp increase of the CRLB’s (21) with increasing n is a reﬂection of the sharply decreased
probability of occurrence of values of a Gaussian variate in the wings of the pdf to which the
higher order cumulants are increasingly sensitive as a function of their order. This is in fact
, because
a general result for any localized distribution; the CIM elements
of the factor of n!m! in the denominator of Eq. (11), resulting in CRLB’s that increase
without bound as the order of the cumulant being estimated increases.

0 as n, m

→ ∞

→

Besides the Gaussian, other useful pdf’s include the supergaussian distributions, i.e.,

those that are (up to a normalization constant) exponentials of even polynomials in (x

−

8

x0), since their CIM elements become simply functions of the moments (or cumulants) of
the distribution, which often have a simple form. Their generating functions are much
more involved than for the Gaussian, and the CIM in general has oﬀ-diagonal elements.
Nevertheless, evaluating the CIM elements is often analytically tractable.

VI. GENERALIZATION TO A DISCRETE RANDOM VARIABLE

While the Fisher information of a continuously random variable over the inﬁnite in-
terval may be deﬁned through the use of an arbitrary location parameterization, discrete
distributions do not readily permit an analogous deﬁnition. Although one cannot treat a
discrete random variable as a continuously-valued parameter, the class of discrete distribu-
tions deﬁned over the entirety of the integers can always be parameterized in terms of their
cumulants and a corresponding CIM matrix deﬁned.

Given a discrete random variable X distributed according to P (x) with x ranging over

all integers, we may deﬁne its characteristic function as

M(ν)

eiνx

=

≡ h

i

∞

∞

P (x)eiνx

(iν)n
n!

µ′
n ,

≡

n=0
X

x=−∞
X

where µ′
characteristic function, when expanded in a power series in ν, deﬁnes the cumulants:

is the nth moment of the distribution about 0. The logarithm of the

n ≡ h

xn

i

L(ν)

ln M(ν)

≡

∞

≡

n=1
X

(iν)n
n!

κn .

While the deﬁnitions for M(ν), L(ν), moments, and cumulants carry over directly from the
continuous case, the inversion to get to a parameterized distribution requires a Fourier series
analysis since M(ν) is guaranteed to be periodic over any 2π-interval in ν due to the discrete
integral sum in Eq. (26). Thus, we may parameterize this distribution as

P (x)

P (x
|

→

κ)

≡

1
2π

π

−π

Z

dν e−iνx exp

∞

(

(iν)n
n!

κn

.

)

−

n=1
X
π, π]. The parameterized distribu-
Notice that the integral is now restricted to the interval [
κ) coincides with P (x) for all integral values of x, but it is also deﬁned continuously
tion P (x
|
for intermediate values of x as well. These intermediate values are not probabilities of any-
thing, and they can range outside the interval [0, 1]. Still, because of its coincidence with
κ) may be substituted for P (x) when calculating an expectation
P (x) at integral x, P (x
|
value of any function of X. Most importantly, though, we may now also take derivatives
of this parameterized distribution with respect to x. While it is possible to “connect the
dots” of the original discrete distribution in any number of ways, resulting in many diﬀerent
possible derivatives, the particular choice (28) is the “most natural choice” for two reasons.
κ) is simply the
First, if P (x) can be written in terms of analytic functions of x, then P (x
|
analytic continuation of P (x) to non-integral values of x. Thus, the parameterized distribu-
tion has the same functional form as the original. Second, and perhaps more importantly,
this choice of parameterization makes
κ)

κ)

(

∂P (x
|
∂κn

=

1)n
−
n!

∂nP (x
|
∂xn

,

9

(26)

(27)

(28)

which allows us to deﬁne the discrete CIM elements exactly as in the continuous case, with
the inﬁnite integral replaced by an inﬁnite sum:

J (κ)
mn =

(

1)m+n
−
m!n!

∞

x=−∞
X

1
P (x
|

∂mP (x
|
∂xm

κ)

κ)

∂nP (x
|
∂xn

κ)

,

m, n = 0, 1, 2, . . . .

(29)

The possibility that n or m = 0 is also included in this deﬁnition for completeness and will
be useful in deﬁning a generating function for these matrix elements (see below).

This is a powerful result. While it is not possible to directly deﬁne the Fisher information
of a discrete random variable as in the continuous case, we can deﬁne an analogous quantity:
the Fisher information with respect to the mean of the cumulant-parameterized distribution.
This is J (κ)
11 = J(X), we
can deﬁne J(X)
for a discrete distribution. The CIM matrix provides a complete
hierarchy of information measures with respect to the cumulants, and the inverse of the CIM
gives the CRLB on any unbiased estimators of the cumulants.

11 of the discrete CIM matrix. Since in the continuous case J (κ)

J (κ)

≡

11

In Sec. IV, we derived a generating function for the CIM of a continuous random variable
κ) given by Eq. (8). A generating function for the discrete
with a parameterized pdf p(x
|
case is trivially obtained by making the following substitutions in that section: all inﬁnite
∞
κ)
integrals (
x=−∞ · · ·
κ) deﬁned in Eq. (28). These substitutions
are replaced with the discrete distribution P (
·|
P
give the following generating function,

), and all instances of the pdf p(

) become inﬁnite sums (

∞
−∞ dx

· · ·

·|

R

(30)

(31)

from which the CIM elements may be obtained exactly as in the continuous case:

The only result that does not generalize directly from Sec. IV is the claim that J (κ)
n0 =
δn0, since this was derived from the Fundamental Theorem of Calculus using integrals. To
verify its validity in the discrete case, we note that

0n = J (κ)

J (κ)(λ, µ) =

∞

x=−∞
X

P (x

−

κ)P (x
λ
|
κ)
P (x
|

κ)

µ
|

−

,

J (κ)
jk =

1
j!k!

∂j+k
∂λj∂µk J (κ)(λ, µ)

.

λ=µ=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0n = J (κ)
J (κ)

n0 =

∞

κ)

∂nP (x
|
∂xn

x=−∞
X
1
2π

π

−π

Z

=

dν (

iν)n

−

e−iνxM(ν)

∞

x=−∞
X

= (

iν)n M(ν)

−

= δn0 ,

ν=0

(cid:12)
(cid:12)
(cid:12)

where in the penultimate step we have used the fact that
2πr), which with the given limits [

π, π] picks out the integrand value at ν = 0.
P

P

−

∞

x=−∞ e−iνx = 2π

∞
r=−∞ δ(ν

−

10

The Poisson distribution provides an example of this generalization to discrete distribu-

tions and addresses an important issue about the support set of a distribution. Let

P (x) =

mxe−m
x!

,

x = 0, 1, . . . ,

(32)

In addition, for the Poisson distribution, all

where m is the mean of the distribution.
cumulants are equal to m.

Our discussion of discrete distributions has focused on those that are deﬁned over all
integers. Still, we may deﬁne the CIM for this distribution by restricting all expectation
values to sums over the support set (i.e., all nonnegative integers). While one could simply
assign P (x) = 0 ∀x < 0, this creates problems in evaluating the CIM elements because of
the 1/P (x) factors in the denominator. Restricting our attention to nonnegative integers is
equivalent to applying constraints on the parameter space, since we know a priori that the
probability for those values is 0. Speciﬁcally, all moments about 0 must be positive (unless
n = 0 ∀n). Care must be taken when calculating Fisher information
m = 0, in which case µ′
with respect to a constrained parameter space. Gorman and Hero [6] showed that when
calculating the Fisher information for a distribution within a parameter space mappable to
Rn where only inequality constraints are active, the constrained Fisher information equals
If, however, one or more equality constraints are
the unconstrained Fisher information.
active on the parameters, one must project the Fisher information matrix onto the tangent
hyperplane of the constraint set at the point in question before using it to derive CRLB’s.
In our case, assuming m > 0, we are free to use the unconstrained CIM and sum only over
nonnegative integers.

Since P (x) is written in terms of analytic functions of x, we may simply write

P (x)

P (x
|

→

κ) =

mxe−m
x!

,

(33)

where the factorial is understood to extend to non-integral x by the relation x! = Γ(x + 1),
where Γ(

) is the gamma-function. The generating function then becomes

·

J (κ)(λ, µ) =

∞

x=0
X

P (x

−

µ

κ)

−

|

κ)P (x
λ
|
κ)
P (x
|
∞

= m−λ−µe−m

(x

−
x=0
X
= e−m m−λ−µ
µ)! 2F2
(
−
−

λ)!(

1

(cid:20)

mxx!
λ)!(x

−
1 ,

µ)!

1

n1,n2
d1,d2 ;

−
where 2F2
is the generalized hypergeometric function (see, e.g., [10] for an intro-
duction to and a list of references for this function). While this generating function does
not appear to have a simpler analytical form, it could be used as the basis for numerical
calculations.

−

i

h

·

λ, 1

µ

; m

,

(cid:21)

(34)

VII. AN INVERSE CIM GENERATING FUNCTION

The CIM elements are of value only insofar as they give a general sense of how much
information the data contain about the cumulants of the pdf from which the data are

11

drawn. It can be shown from the non-negativity of covariance matrices that the variance of
any unbiased estimator of the nth cumulant must exceed the reciprocal of the nn
diagonal
element of the CIM [11]. Such a bound, though easy to write down, is not optimal, however,
since the CRLB involving the nn
diagonal element of the inverse CIM represents in general
a greater lower bound. Finding the inverse of the CIM is therefore a very useful exercise.
Here we derive two relations between the generating functions for the CIM and inverse CIM
elements.

−

−

Let us deﬁne the generating function I (κ)(ν, η) for the matrix elements I (κ)

nm of the inverse

of the CIM:

r,s=0
X
That the two generating functions (12) and (35) are related can be shown in two ways. The
ﬁrst uses complex analysis. Consider the integral

I (κ)(ν, η)

≡

∞

I (κ)
rs νrηs .

dµ
2πiµ

I

J (κ)(λ, µ)I (κ)(µ∗, η)

= 1. By substituting the deﬁning relations (12) and (35) for the two
over the unit circle
generating functions into Eq. (36) and noting that µnµ∗r = µn−r on the unit circle, as well
as the integral identity

µ

|

|

dµ
2πiµ

I

µp = δp0 ,

∞

m,n,s=0
X

we have the following intermediate result for the integral (36):

dµ
2πiµ

I

J (κ)(λ, µ)I (κ)(µ∗, η) =

mn I (κ)
J (κ)

ns λmηs .

(37)

This result reduces further since the matrices J (κ) and I (κ) are inverses of each other and
therefore the summation over the index n is simply δms. The remaining double sum collapses
into a single geometric series in powers of λη, that can be easily summed provided
< 1,
and the following integral equation results:

λη

|

|

dµ
2πiµ

I

J (κ)(λ, µ)I (κ)(µ∗, η) =

1

,

1

λη

−

λη

< 1 .

|

|

A second relation between J (κ) and I (κ) involves deﬁning two “marginal” generating

functions:

Note also that

∞

≡

m=0
X

J (κ)
n (λ)

λmJ (κ)
nm

and

I (κ)
n (η)

ηmI (κ)
nm .

∞

≡

m=0
X

(35)

(36)

(38)

(39)

(40)

(41)

J (κ)(λ, µ) =

µnJ (κ)

n (λ) ,

I (κ)(µ∗, η) =

µ∗nI (κ)

n (η) .

∞

n=0
X
∞

n=0
X

12

These relations, when substituted into the left-hand side of Eq. (38) give

∞

n=0
X

n (λ)I (κ)
J (κ)

n (η) =

1

,

1

λη

−

λη

< 1 .

|

|

It is in general not possible to solve either the integral equation (38) or the marginal
equation (42) analytically to obtain the inverse generating function I (κ)(ν, η) or its marginal
I (κ)
n (η) from a knowledge of the CIM generating function J (κ)(λ, µ). It is hoped, however,
that these equations can nevertheless serve as a useful starting point for discussing further
properties of the inverse CIM and perhaps for establishing general results involving its matrix
elements. They may also serve to motivate useful approximations.

Returning to the Gaussian example, knowing the form of the inverse CIM elements,

I (κ)
nm = δnmn!σ2n, we can explicitly construct the inverse generating function (35),

∞

I (κ)(ν, η) =

r!σ2rνrηr ,

r=0
X
which does not converge except when ν = η = 0. This problem is not limited to the
Gaussian pdf, because as mentioned in Sec. V, the CRLB’s on successively higher cumulants
will always increase without bound due to the n!m! factor in the denominator of the CIM
elements. This problem suggests that an alternative deﬁnition of an inverse generating
function could be more useful.

VIII. A DIFFERENT INVERSE CIM GENERATING FUNCTION

An alternative deﬁnition of a generating function for the inverse CIM is

from which the inverse CIM elements may be obtained as follows:

Note that because of the inclusion of the factorials r! and s! into the deﬁnition (44), such
factorials are absent from Eq. (45). Compare Eq. (15).

With this new deﬁnition, the marginal relation (42) becomes

˜I (κ)(ν, η)

∞

≡

r,s=0
X

I (κ)
rs

νrηs
r!s!

,

I (κ)
rs =

∂r+s
∂νr∂ηs

˜I (κ)(ν, η)

.

ν=η=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n (λ) ˜I (κ)
J (κ)

n (η) = eλη ,

∞

n=0
X

˜I (κ)
n (η)

≡

ηm
m!

I (κ)
nm .

∞

m=0
X

13

(42)

(43)

(44)

(45)

(46)

(47)

where the “marginal”

Note that Eq. (46) is valid for all λ and η. The integral relation (38) does not translate
directly under the redeﬁnition (44). Still, from the marginal relation above and the additional
relations

(48)

(49)

(50)

(51)

J (κ)
n (λ) =

˜I (κ)
n (η) =

1
n!

∂n
∂νn J (κ)(λ, µ)
∂n
˜I (κ)(ν, η)
∂νn

,

,

λ=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ν=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a complex Fourier analysis gives the integral relation

1
2π

∞

∞

−∞

Z

−∞

Z

dµ

dν e−iµνJ (κ)(λ, µ) ˜I (κ)(iν, η) = eλη .

This can be considered the analog of the relation (38). Unlike its counterpart, this relation
is deﬁned for all values of λ and η.

In the case of the Gaussian pdf, the solution ˜I (κ)(ν, η) to these relations is well-behaved

and easily obtained:

˜I (κ)(ν, η) = eνη σ2

.

That the generating function ˜I (κ)(ν, η) deﬁned in Eq. (44) is convergent in at least one
case (the Gaussian) leads us to believe that it will prove more useful than its counterpart,
I (κ)(ν, η) deﬁned in Eq. (35). In addition, the relations (50) and (46) are valid for all values
of λ and η, which is a second advantage over the corresponding relations in the previous
section.

IX. CONCLUSION

The Fisher information of a continuous random variable can be interpreted as the ﬁdelity
with which a ﬁducial location parameter of a pdf (such as the mean) may be estimated from
statistical data drawn according to that distribution.
In this paper, we have introduced
a more robust information measure in the CIM whose inverse bounds the variance of any
estimates of the cumulants of a pdf and, consequently, the ﬁdelity with which the entire
pdf may be estimated. The Fisher information of the random variable is included in this
measure. We have also extended the CIM concept to discrete random variables, for which the
notion of Fisher information of the random variable is ill deﬁned. Finally, we have derived a
generating function for the CIM and given several relations between this generating function
and two alternative generating functions for the inverse CIM. The latter generating functions
are desirable because they directly generate the bounds on the ﬁdelity of estimation of the
cumulants.

Acknowledgments

The work reported here was supported in part by the US Air Force Oﬃce of Scientiﬁc
Research under grants F49620-00-1-0155 and F49620-01-1-0321. We are also pleased to

14

receive from Dr. Chris Lloyd a reprint of his paper cited here.

[1] H. L. Van Trees, Detection, Estimation, and Modulation Theory, Part I. New York: John

[2] T. M. Cover and J. A. Thomas, Elements of Information Theory. New York: John Wiley,

Wiley, 1968.

1991, Sec. 12.11.

[3] C. J. Lloyd, “Asymptotic expansions of the Fisher information in a sample mean,” Statistics

[4] E.

& Probability Lett., vol. 11, pp. 133-137 (1991).
Weisstein.
http://mathworld.wolfram.com/Cumulant.html

(1999).

W.

Cumulant.

[Online].

Available:

[5] J. F. Kenney and E. S. Keeping, “Cumulants and the Cumulant-Generating Function,” “Ad-
ditive Property of Cumulants,” and “Sheppard’s Correction,” in Mathematics of Statistics,
2nd ed., Pt. 2. Princeton, NJ: Van Nostrand, 1951,

[6] J. D. Gorman and A. O. Hero, “Lower bounds for parameer estimation with constraints,”

4.10–4.12, pp. 77–82.
§

IEEE Trans. Inf. Theory, vol. 26, pp. 1285-1301 (1990).

[7] E. W. Weisstein. (1999). k-statistic. [Online]. Available: http://mathworld.wolfram.com/k-

[8] J. F. Kenney and E. S. Keeping, “The k-Statistics,” in Mathematics of Statistics, 3rd ed., Pt.

1. Princeton, NJ: Van Nostrand, 1962,

7.9, pp. 99–100.
§

[9] P. R. Halmos, “The theory of unbiased estimation,” Ann. Math. Stat., vol. 17, pp. 34–43

Statistic.html

(1946).

[10] E. W. Weisstein.

(1999). Generalized Hypergeometric Function.

[Online]. Available:

http://mathworld.wolfram.com/GeneralizedHypergeometricFunction.html

[11] H. H. Barrett, J. L. Denny, R. F. Wagner, and K. J. Myers, “Objective assessment of image
quality. II. Fisher information, Fourier crosstalk, and ﬁgures of merit for task performance,”
J. Opt. Soc. Am. A, vol. 12, 834-852 (1995).

15


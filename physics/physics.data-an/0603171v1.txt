6
0
0
2
 
r
a

M
 
1
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
7
1
3
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Memory functions and Correlations in Additive Binary Markov

Chains

S. S. Melnyk, O. V. Usatenko, V. A. Yampol’skii ∗

A. Ya. Usikov Institute for Radiophysics and Electronics

Ukrainian Academy of Science, 12 Proskura Street, 61085 Kharkov, Ukraine

S. S. Apostolov, Z.A. Maiselis

V. N. Karazin Kharkov National University,

4 Svoboda Sq., Kharkov 61077, Ukraine

A theory of additive Markov chains with long-range memory, proposed earlier in

Phys. Rev. E 68, 06117 (2003), is developed and used to describe statistical proper-

ties of long-range correlated systems. The convenient characteristics of such systems,

a memory function, and its relation to the correlation properties of the systems are

examined. Various methods for ﬁnding the memory function via the correlation

function are proposed. The inverse problem (calculation of the correlation function

by means of the prescribed memory function) is also solved. This is demonstrated

for the analytically solvable model of the system with a step-wise memory function.

PACS numbers: 05.40.-a, 02.50.Ga, 87.10.+e

I.

INTRODUCTION

The problem of long-range correlated dynamic systems (LRCS) has been under study for

a long time in many areas of contemporary physics [1, 2, 3, 4, 5, 6], biology [7, 8, 9, 10,

11, 12], economics [8, 13, 14], literature [15, 16, 17, 18, 19], etc. [8, 20]. One of the ways

to get a correct insight into the nature of correlations in a system consists in constructing

a mathematical object (for example, a correlated sequence of symbols) possessing the same

statistical properties as the initial system. There exist many algorithms for generating

long-range correlated sequences: the inverse Fourier transformation [20, 21], the expansion-

∗ yam@ire.kharkov.ua

2

modiﬁcation Li method [22], the Voss procedure of consequent random additions [23], the

correlated Levy walks [24], etc. [20]. The use of the multi-step Markov chains is one of the

most important among them because they oﬀer a possibility to construct a random sequence

with necessary correlated properties in the most natural way. This was demonstrated in

Ref. [25], where the concept of Markov chain with the step-wise memory function was

introduced. The correlation properties of some dynamical systems (coarse-grained sequences

of the Eukarya’s DNA and dictionaries) can be well described by this model [25].

A sequence of symbols in the Markov chain can be thought of as the sequence of states

of certain particle, which participates in a correlated Brownian motion. Thus, every L-word

(the portion of the length L in the sequence) can be considered as one of the realizations

of the ensemble of correlated Brownian trajectories in the ”time” interval L. This point

gives an opportunity to use the statistical methods for examining the correlation properties

of the dynamic systems. Another important reason for the study of Markov chains is its

application to the various physical objects [26, 27, 28], e.g., to the Ising chains of spins. The

problem of thermodynamics description of the Ising chains with long-range spin interaction

is still unresolved even for the 1D case. However, the association of such systems with the

Markov chains can shed light on the non-extensive thermodynamics of the LRCS.

In this paper, we ascertain the relation between the memory function of the additive

Markov chains and the correlation properties of the systems under consideration. We exam-

ine the simplest variant of the random sequences, dichotomic (binary) ones, although the

presented theory can be applied to arbitrary additive Markov processes with ﬁnite or inﬁnite

number of states.

The paper is organized as follows. In the ﬁrst Section, we introduce the general relations

for the Markov chains, derive an equation connecting the correlation and memory functions

of additive Markov chains, and verify the robustness of our method by numerical simulations.

The second part is devoted to the study of the correlation function for the Markov chain

with the step-wise memory function. In Subsec. (III B) we reveal a band structure of the

correlation function and obtain its explicit expression. Subsec. (III C) contains the results

of asymptotic study of correlation function.

3

II. GENERAL PROPERTIES OF ADDITIVE MARKOV CHAINS

A. Basic notions

Let us consider a homogeneous binary sequence of symbols, ai =

0, 1

, i

Z =

{

}

∈

2,

−

−

1, 0, 1, 2, .... To determine the N-step Markov chain we have to introduce the con-

...,
ditional probability P (ai |
ai = 1 or ai = 0 ) occurring after the N-word TN,i, where TN,i denotes the sequence of
symbols ai−N , ai−N +1, . . . , ai−1. Thus, it is necessary to deﬁne 2N values of the P -function

ai−N , ai−N +1, . . . , ai−1) of the deﬁnite symbol ai (for example,

corresponding to each possible conﬁguration of the symbols in the N-word TN,i. Since we

intend to deal with the sequences possessing the memory length of the order of 106, we need

to make some simpliﬁcations. We suppose that the P -function has the additive form,

P (ai = 1

TN,i) =

f (ai−k, k).

|

(1)

Here the value f (ai−k, k) is the additive contribution of the symbol ai−k to the conditional

probability of the symbol unity occurring at the ith site. Equation (1) corresponds to the

additive inﬂuence of the previous symbols on the generated one. Such Markov chain is

referred to as additive Markov chain, Ref. [29]. The homogeneity of the Markov chain is

provided by the independence of the conditional probability Eq. (1) of the index i. It is

possible to consider Eq. (1) as the ﬁrst term in expansion of conditional probability in the

formal series of terms that correspond to the additive (or unary), binary, ternary, and so on

functions up to N-ary one.

Let us rewrite Eq. (1) in an equivalent form,

P (ai = 1

TN,i) = ¯a +

|

F (r)(ai−r −

¯a).

(2)

N

Xk=1

N

r=1
X

Here

N

¯a =

f (0, r)/[1

(f (1, r)

f (0, r))]

N

−

−

r=1
X
is the average number of unities in the sequence, Ref. [29], and

r=1
X

F (r) = f (1, r)

f (0, r).

−

We refer to F (r) as the memory function (MF). It describes the strength of impact of

previous symbol ai−r upon a generated one, ai. Evidently, this function has to satisfy

|

|

L

l=1
P

condition 0 6 P (ai = 1

TN,i) 6 1. To the best of our knowledge, the concept of the

memory function for multi-step Markov chains was introduced in papers [19, 25] where it

is shown that it is convenient to use it in describing the correlated properties of complex

dynamical systems with long-range correlations.

The function P (ai = 1

TN,i) contains complete information about correlation properties

of the Markov chain. In general, the correlation function and other moments are employed

as the input characteristics for the description of the correlated random systems. Yet, the

correlation function takes account of not only the direct interconnection of the elements ai

and ai+r, but also their indirect interaction via other elements. Our approach operates with

the “origin” characteristics of the system, speciﬁcally with the memory function.

The positive values of the MF result in persistent diﬀusion where previous displacements

of the Brownian particle in some direction provoke its consequent displacement in the same

direction. The negative values of the MF correspond to the antipersistent diﬀusion where

the changes in the direction of motion are more probable. In terms of the Ising long-range

particles interaction model, which could be naturally associated with the Markov chains,

the positive values of the MF correspond to the attraction of particles and negative ones

conform to the repulsion.

We consider the distribution WL(k) of the words of deﬁnite length L by the number k of

unities in them, ki(L) =

ai+l, and the variance D(L) of ki(L),

D(L) = (k

¯k)2,

−

L

k=0
P

¯a2.

where the deﬁnition of average value of g(k) is g(k) =

g(k)WL(k).

Another statistical characteristics of random sequences is the correlation function,

K(r) = aiai+r −
By deﬁnition, the correlation function is even, K(

variance of the random variable ai. The correlation function is connected with the above

r) = K(r), and K(0) = ¯a(1

¯a) is the

−

−

mentioned variance by the equation

K(r) =

(D(r

1)

2D(r) + D(r + 1)),

1
2

−

−

or

in the continuous limit.

K(r) =

1
2

d2D(r)
dr2

4

(3)

(4)

(5)

(6)

B. Derivation of main equation

In this subsection we obtain a very important relation connacting the memory and corre-

lation functions of the additive Markov chain. Let us introduce the function φ(r) = p(ai =

1

ai−r = 1), which is the probability of symbol ai = 1 occurring under condition that

|

previous symbol ai−r is also equal to unity. This function is obviously connected to the

correlation function K(r), see Eq. (4) since the quantity aiai−r is the probability of simul-

taneous equality to unity of both symbols, ai and ai−r. It can be expressed in terms of the

conditional probability φ(r),

Substituting Eq. (2) into Eq. (4) we get:

aiai−r = ¯aφ(r).

K(r) = ¯aφ(r)

¯a2.

−

For the N-step Markov chain, the probability of the symbol ai = 1 occurring depends

only on the previous N-word. Therefore, to obtain the value of φ(r) one needs to average

the conditional probability P Eq. (2) over all realizations of the N-words with the condition

ai−r = 1,

φ(r) = p(ai = 1

ai−r = 1)

|
TN,i)p(TN,i |

|

=

P (ai = 1

ai−r = 1).

XTN,i

If the value of r is less or equal to N, then ai−r in Eq. (9) is one of the symbols

ai−1, ai−2, . . . , ai−N in the word TN,i.

In this case, the sum in Eq. (9) allows not all N-

words, but only the words that contain the symbol unity at the (i

r)th position. If r > N,

−

the memory function F (r) equals zero in this region and, therefore, the sum in Eq. (9)

contains all terms corresponding to all diﬀerent N-words.

Substituting Eq. (2) into Eq. (9), we obtain:

5

(7)

(8)

(9)

(10)

φ(r) = ¯a

P (TN,i |

ai−r = 1)

XTN,i

+

F (r′)

N

Xr′=1

TN,i
X

(ai−r′

¯a)P (TN,i |

−

ai−r = 1).

6

(11)

(12)

(13)

(14)

According to the normalization condition, the ﬁrst sum in Eq. (10) is equal to unity.

Consider the sum

in the second term in RHS of Eq. (10). The symbol ai−r′ is contained within the word TN,i.

Therefore, Eq. (11) represents the average value of ai−r′ under condition ai−r = 1. In other
r′) of ai−r′ = 1 occurring under the condition ai−r = 1:

words, it equals the probability φ(r

ai−r′P (TN,i |

ai−r = 1)

TN,i
X

−

Substituting this equation into Eq. (10) we obtain:

ai−r′P (TN,i |

ai−r = 1) = φ(r

r′).

−

w
X

φ(r) = ¯a +

F (r′)(φ(r

r′)

¯a).

−

−

N

Xr′=1

K(r) =

F (r′)K(r

r′),

r > 1.

−

N

Xr′=1

Taking into account Eq. (8), we arrive at the relation between the memory function and the

correlation function:

This equation was ﬁrst derived by the variation method in Ref. [29].

Another equation resulting from Eq. (14) by double summation over index r establishes

a relationship between the memory function F (r) and the variance D(L),

M(r, 0) =

F (r′)M(r, r′),

(15)

N

Xr′=1

M(r, r′) = D(r

r′)

(D(

r′) + r[D(

r′ + 1)

D(

r′)]).

−

−

−

−

−

−

Equation (5) and parity of the function D(r) are used here.

The last equation shows convenience of using the variance D(L) instead of the correlation

function K(r). The function K(r), being a second derivative of D(r) in continuous approx-

imation, is less robust in computer simulations.

It is the main reason why we prefer to

use Eq. (15) for the long-range memory sequences. This is our tool for ﬁnding the memory

function F (r) of a sequence using the variance D(L).

C. Numerical reconstruction of the memory function

Let us verify the robustness of our method by numerical simulations. We consider a

model memory function,

F (r) = 0.1

1

r/10,

−

(

0,

1 6 r < 10,

r > 10.

shown in the inset in Fig. 1 by solid line. Using Eq. (2), we construct a random unbiased

7

(16)

0.08

F

0.04

0.00

0.03

0.02

K

0.00

0.01

0

2

4

6

8

10

r

0

20

40

60

r

Figure 1: Calculated correlation function K(r) of the Markov chain constructed with the model

memory function F (r), Eq. (16), shown by the solid line in the inset. Dots in the inset correspond

to the memory function reconstructed by solving Eq. (14) with correlation function K(r) presented

in the main panel.

¯a = 1/2, Markov chain. Then, with the help of the constructed binary sequence of the

length 3

106, we calculate numerically the correlation function K(r) by solving the set of

×

N linear equations (14). The result of these calculations is given in Fig. 1. One can see that

the correlation function K(r) mimics roughly the memory function F (r) over the region

1 6 r 6 10. In the region r > 10, the memory function is equal to zero but the correlation

function does not vanish [30]. Then, using the obtained correlation function K(r), we solve

numerically Eq. (14). The result is shown in the inset in Fig. 1 by dots. One can see an

excellent agreement of initial, Eq. (16), and reconstructed memory functions F (r).

The ability of constructing a binary sequence with an arbitrary prescribed correlation

function by means of Eq. (14) is the very nontrivial result of this paper.

Yet another approach to numerical ﬁnding the memory function is an iteration procedure.

8

(17)

(18)

For its realization let us rewrite Eq. (14) in the form,

F (r) =

K(r)
K(0) −

N

K(r

r′)

−
K(0)

F (r′).

Xr′=1, r′6=r

Using Eq. (17) with starting iteration F0(r) = 0, we obtain the formula,

Fn+1(r) =

K(r)
K(0) −

N

K(r

r′)

−
K(0)

Xr′=1, r′6=r

Fn(r′), n > 0.

Thus, the memory function can be presented as the series,

F (r) =

K(r)
K(0) −

K(r

r′)K(r′)

−
K 2(0)

+

K(r

r′)K(r′

r′′)K(r′′)

−

−
K 3(0)

+ . . .

(19)

Xr′6=r Xr′′6=r′
Note, that the Markov chain with the deﬁnite correlation function K(r) exists if the se-

Xr′6=r

ries (19) is convergent and the obtained function implies the probability Eq. (2) satisfy-

ing the requirement 0 6 P (ai = 1

TN,i) 6 1 for arbitrary word TN,i.

If ¯a = 1/2 we

|

F (r)

6 1. The suﬃcient, but not necessary, requirement is

obtain the restriction,
N −1

K(r)

6 1/12

|

|

|
K(N)
P

/3.

|

− |

r=1 |
P

III. CORRELATION FUNCTION OF THE CHAIN WITH THE STEP-WISE

MEMORY FUNCTION

In the previous section, we obtained the relationship (14) between two characteristics of

the Markov chain, the memory and correlation functions, and used this equation to solve the

problem of ﬁnding the memory function via the known correlation function. Here we present

the solution of the inverse problem. We suppose the memory function to be known and ﬁnd

the correlation function of the corespondent Markov chain. To simplify our consideration,

we examine the step-wise memory function,

F (r) =

α,

0,

(cid:26)

r 6 N,

r > N.

(20)

The restriction imposed on the parameter α can be obtained from Eq. (2):

α

< 1/N. Note

|

|

that each of the symbols unity in the preceding N-word promotes the emergence of new

symbol unity if 0 < α < 1/N. This corresponds to the persistent diﬀusion. The region of

parameter α, determined by inequality

1/N < α < 0, corresponds to the anti-persistent

−
diﬀusion. If α = 0, one arrives at the case of the non-correlated Brownian motion.

9

(21)

(22)

A. Main equation for the correlation function

Substituting Eq. (20) into Eq. (14) we obtain the relation,

K(r) = α

K(r

r′),

r > 1.

−

N

Xr′=1

Here, the correlation function is assumed to be even, K(

r) = K(r). Equation (21) is

−

the linear recurrence of the order of N for r > N + 1, so we stand in need of N initial

conditions. For the unbiased sequence, ¯a = 1/2, we have K(0) = 1/4. The solution of

Eqs. (21) written for r = 1, . . . , N yields the constant value of the correlation function,

K(r) = K0 at r = 1, . . . , N

1,

−

K0 =

α
α(N

.

1))

−

4(1

−

Subtracting Eq. (21) from the same equation written for r + 1, we derive another, more

convenient, form of the recurrence:

K(r + 1)

(1 + α) K(r) + α K(r

N) = 0.

(23)

−

−

This equation is of the order of N + 1, thus we need an additional initial condition. It can

be derived from Eq. (21): K(N) = K0. Note that the possibility to rewrite Eq. (21) in the

form of Eq. (23) is the result of the simple structure of the memory function. We solve the

obtained recurrent equations by the most natural method, by means of step-by-step ﬁnding

the sequent values of the correlation function. Such an approach is very suitable for the

analysis of correlation function at r & N.

B. Correlation function at r & N

1. Band structure of the correlation function

Equation (21) allows one to ﬁnd numerically the unknown correlation function K(r). The

result of this step-by-step calculation is presented in Fig. 2 by solid line. One can easily see

the discontinuity of K(r) at the point L = N = 100, the breakpoint of the curve is observed

at L = 2N. Such behavior of the correlation function is the result of using the memory

function of the step-wise form. To clarify this fact it is convenient to change the variable r

10

0

1

2 3

4 5 6 7 8 9

 

0.012

s

 

0.008

K

0.004

0.000

0

200 400 600 800 1000

r

Figure 2: Correlation function K(r) (solid line) obtained by two diﬀerent methods: the numerical

simulation of Eq. (14) and exact solution Eq. (41). The dotted line is for the contribution to K(r)

of the ﬁrst root of Eq. (32). The dashed line refers to the correlation function obtained in [19].

The vertical lines indicate the limits of the bands numbered by s.

by the band number s and the intra-band number ρ:

K(r) = Ks(ρ),

r = sN + ρ + 1,

ρ = 0, 1, . . . , N

1, s = 0, 1, . . . .

(24)

−

Within sth band, Eq. (23) is the recurrence of the second order with the term α Ks−1(ρ) that

is determined at the previous step, while ﬁnding the correlation function for the (s

1)th

−

band.

2. General expression for the correlation function

In the zeroth band (s = 0, 1 6 r 6 N), as it was shown above, the correlation function

is constant,

we have

K0(ρ) = K0.

For the ﬁrst band (s = 1, N + 1 6 r 6 2N), taking into account that K(r

N

1) = K0(ρ),

−

−

K1(ρ) = K0 (1

(1

−

−

αN)(1 + α)ρ).

The correlation function decreases quasi-continuously within the ﬁrst band. However, as it

was mentioned above, there exists a discontinuity in the K(r) dependence at r = N. This

discontinuity disappears in the limiting case of the strong persistence, α

1/N.

→

(25)

(26)

11

Substituting the obtained formula (26) in Eq. (23), we ﬁnd the solution K2(ρ) for the

second band (s = 2, 2N + 1 6 r 6 3N),

K2(ρ) = K0 (1

(1

−

−

−

αN)((1 + α)ρ+N

ρ α(1 + α)ρ−1).

(27)

The correlation function K(r) is continuous at the interface between the ﬁrst and second

bands, K1(N) = K2(0). However, its ﬁrst derivative of K(r) is discontinuous here (see

Fig. 2). Using the induction method, one can easily derive the formula for Ks(ρ) in the sth

band (sN + 1 6 r 6 (s + 1)N):

Ks(ρ) = K0

1

(1

αN)

−

−

 

α)i−1(1 + α)(s−i)N +ρ−i+1Ci−1

(s−i)N +ρ

(28)

,

!

C k

n =

Γ(n + 1)

Γ(k + 1)Γ(n

k + 1)

−

.

s

(

−

i=1
X

−

−

It follows from Eq. (28), that the ﬁrst (s

2) derivatives of the correlation function K(r)

are continuous at the border between the (s

1)th and sth bands, but the derivative of the

(s

1)th order changes discontinuously. Under the condition αN

1, Eq. (28) takes a

≪

−

simpler form,

Ks(ρ) = K0αsCs

s+N −1−ρ.

(29)

It is seen that the correlation function decreases proportionally to αs with an increase of the

It is not easy to analyze the asymptotical behavior of the function K(r) at large s because

the number of summands in Eq. (28) increases being proportional to s. It is the reason to

propose another approach for the asymptotical study of the correlation function K(r) at

band number s.

1.

s

≫

C. Asymptotical study of the correlation function

1. Derivation of the characteristic equation

The general solution of linear recursion equations (21) can be represented as the linear

combination of N diﬀerent exponential functions,

K(r) =

aiξr
i .

N

i=1
X

(30)

To ﬁnd the values of ξi, we substitute the fundamental solution,

K(r) = ξr,

into Eq. (21) and obtain the characteristic polynomial equation of the order of N. Constant

multipliers ai are to be determined by initial conditions.

It is more convenient to use Eq. (23) instead of Eq. (21), that implies the characteristic

equation of the order of N + 1,

ξN +1

(1 + α)ξN + α = 0.

−

The extra root of this equation, ξ = 1, appears as a consequence of passing on to the

equation of order of N + 1 from that of the order of N . The corresponding coeﬃcient, ai,

in Eq. (30) is equal to zero because the correlation function should decrease at r

.

→ ∞

Our study shows that Eq. (32) has one real positive root less than unity in the case of

odd N. In the case of even N, there are two real roots, one positive and one negative. The

remaining roots are complex. All absolute values of roots are less than unity, that is in

agreement with the ﬁniteness of memory function F (r). In the case of large N, the absolute

magnitudes of all roots are close to unity for nearly all values of α satisfying the inequality,

12

(31)

(32)

(33)

Distribution of the roots in the complex plane ξ is shown in Fig. 3.

1
N

ln

1
α ≪

1.

x

1,0

0,5

- 0,5

- 1,0

 

 

 

0,0

 

- 1,0

- 0,5

0,0

0,5

1,0

Figure 3: The dots are the roots of the characteristic equation (32) for N = 100 and α = 0.008.

The solid line is the circle

= 1.

ξ
|

|

In the simplest case, N = 2, Eq. (32) has two real roots,

ξ1, 2 =

α
2 ± r

α2
4

+ α.

(34)

13

(35)

≪

Taking into account the initial conditions, we ﬁnd the solution of Eq. (21) in the form,

K(r) =

α

4(1

α)√α2 + 4α

−

(ξr−1
1

(1

ξ2)

−

−

ξr−1
2

(1

ξ1)).

−

This expression can be simpliﬁed at small and large values of parameter α. For α

1, one

obtains

1
4
with square brackets standing for the integer part. The correlation function in the sequent

α[r/2]+1

K(r) =

(36)

odd and even points are equal to each other. In accordance with Eq. (29), K(r) decreases

being proportional to αs. In the opposite limiting case of the strong persistency,

at r

α

→

→ ∞
1/2, we have two diﬀerent roots,

1
2
−
α. The coeﬃcient corresponding to the second root is much less than that

ξ1 = 1

ξ2 =

(37)

1
3

4
3

φ,

φ,

−

+

with φ = 1/2

−

corresponding to the ﬁrst one. Besides, the second term in Eq. (35) decreases more rapidly.

Therefore, the approximate solution in this case is

K(r) =

exp

4φ(r

1)/3.

−

−

1
4

2. Correlation function at small α

Let us return to the case of arbitrary value of N. If α is very small, i.e. at
1
N

1
α ≫

ln

1,

Eq. (32) has N roots with small absolute magnitudes:

ξk = α1/N (cos(2π

) + i sin(2π

)),

k = 0, . . . , N

1.

k
N

k
N

−

The correlation function, being a linear combination of the power functions with these roots

as their exponents, decreases proportionally to αs, which agrees with Eq. (29).

The coeﬃcients ai in the linear combination Eq. (30) can be found in a general case,

without any restrictions imposed on the value of α. The solution of Eq. (21) written for

1 along with K(0) = 1/4 can be expressed with the help of the Vandermond

1 6 r 6 N

−
determinants:

with ξN +1 = 1.

K(r) = K0(αN

1)

−

N

Xk=1

ξr−1
k

,

(ξk −

ξj)

N +1

j=1,j6=k
Q

(38)

(39)

(40)

(41)

14

(42)

(43)

(44)

(45)

3. Correlation function at not too small α

In the case (33) of not too small α, the absolute magnitudes of all roots are close to unity.

It is convenient to rewrite Eq. (32), introducing two new real variables γ and ϕ instead of

complex x according to

Equation (32) takes the form,

x = (1

1
γN

−

)eiϕ.

For the real root, Eq. (32) yields,

1
γ −iN ϕ

α(e

1) = (1

−

1
γN

−

)eiϕ

1.

−

αNγ(e1/γ

1) = 1.

−

This expression along with Eq. (31) determines the asymptotical behavior of correlation

function. It was ﬁrst obtained in Ref. [19]. The qualitative approach of this paper did not

allow one to take into account the contribution of other roots as it is done in the present

paper.

Equation (43) yields all remaining complex roots with the values of ϕ, which are quite

uniformly distributed over the circle [0, 2π] and

1
γ ∼

ln

1
α

.

The roots of Eq. (32) located in the vicinity of point ξ = 1 are shown in Fig. 4. The single
real root is much closer to the line Re ξ = 1 than the other ones. Besides, the coeﬃcients

ai in Eq. (41) (see also Eq. (30)) for all terms containing the complex exponents are much

less than those for the term with the real exponent. Therefore, the behavior of correlation

function K(r) is determined generally by the term with the real exponent.

The exact correlation function K(r) resulting from the numerical simulation of Eq. (41)

and its approximation determined by the contribution of the real root alone are shown in

Fig. 2. These curves are compared with that obtained in Ref. [19] by a qualitative method.

The obtained correlation function can be used to calculate one of the most important

characteristics of the random binary sequences, the variance of number of unities in the

L-word. The results of the numerical simulations are shown in Fig. 5. One can see a good

agreement of curves plotted using both of these methods.

0,997

0,998

0,999

1,000

Figure 4: The roots of characteristic polynomial equation close to the point ξ = 1 for N = 4000,

α = 2

10−4. The solid line is Re ξ = 1.

·

15

x

 

0,00

 

 

 

 

0,06

0,03

- 0,03

- 0,06

10000

1000

100

D

10

1

 

1

10

100

1000

  L

Figure 5: Variance D(L) for the Markov chain with N = 100, α = 0.008 obtained by means of

exact Eqs. (41), (5) (solid line) and using only one root of Eq. (32) (dashed line). The thin solid

line describes the non-correlated Brownian diﬀusion, D(L) = L/4.

4. Conclusion

Thus, we have demonstrated the eﬃciency of describing the symbolic sequences with

long-range correlations in terms of the many-step Markov chains with the additive memory

function. Actually, the memory function appears to be a suitable informative ”visiting card”

of any symbolic stochastic process. Various methods for ﬁnding the memory function via the

correlation function of the system are proposed. Our preliminary consideration shows the

possibility to generalize our concept of the Markov chains on larger class of random processes

where the random variable can take on arbitrary, ﬁnite or inﬁnite number of values.

The suggested approach can be used for the analysis of diﬀerent correlated systems in

various ﬁelds of science. For example, the application of the Markov sequences to the theory

of spin chains with long-range interaction makes it possible to estimate some thermodynamic

characteristics of these non-extensive systems.

16

[1] U. Balucani, M. H. Lee, V. Tognetti, Phys. Rep. 373, 409 (2003).

[2] I. M. Sokolov, Phys. Rev. Lett. 90, 080601 (2003).

[3] A. Bunde, S. Havlin, E. Koscienly-Bunde, H.-J. Schellenhuber, Physica A 302, 255 (2001).

[4] H. N. Yang, Y.-P. Zhao, A. Chan, T.-M. Lu, and G. C. Wang, Phys. Rev. B 56, 4224 (1997).

[5] S. N. Majumdar, A. J. Bray, S. J. Cornell, and C. Sire, Phys. Rev. Lett. 77, 3704 (1996).

[6] S. Halvin, R. Selinger, M. Schwartz, H. E. Stanley, and A. Bunde, Phys. Rev. Lett. 61, 1438

(1988).

[7] R. F. Voss, Phys. Rev. Lett. 68, 3805 (1992).

[8] H. E. Stanley et. al., Physica A 224,302 (1996).

[9] S. V. Buldyrev, A. L. Goldberger, S. Havlin, R. N. Mantegna, M. E. Matsa, C.-K. Peng, M.

Simons, H. E. Stanley, Phys. Rev. E 51, 5084 (1995).

[10] A. Provata and Y. Almirantis, Physica A 247, 482 (1997).

[11] R. M. Yulmetyev, N. Emelyanova, P. H¨anggi, and F. Gafarov, A. Prohorov, Phycica A 316,

671 (2002).

[12] B. Hao, J. Qi, Mod. Phys. Lett., 17, 1 (2003).

[13] R. N. Mantegna, H. E. Stanley, Nature (London) 376, 46 (1995).

[14] Y. C. Zhang, Europhys. News, 29, 51 (1998).

[15] A. Schenkel, J. Zhang, and Y. C. Zhang, Fractals 1, 47 (1993).

[16] I. Kanter and D. A. Kessler, Phys. Rev. Lett. 74, 4559 (1995).

[17] P. Kokol, V. Podgorelec, Complexity International, 7, 1 (2000).

[18] W. Ebeling, A. Neiman, T. Poschel, arXiv:cond-mat/0204076.

[19] O. V. Usatenko, V. A. Yampol’skii, K. E. Kechedzhy and S. S. Mel’nyk, Phys. Rev. E 68,

06117 (2003).

[20] A. Czirok, R. N. Mantegna, S. Havlin, and H. E. Stanley, Phys. Rev. E 52, 446 (1995).

[21] H. A. Makse, S. Havlin, M. Schwartz, and H. E. Stanley, Phys. Rev. E 53, 5445 (1995).

[22] W. Li, Europhys. Let. 10, 395 (1989).

17

[23] R. F. Voss, in: Fundamental Algorithms in Computer Graphics, ed. R. A. Earnshaw (Springer,

Berlin, 1985) p. 805.

[24] M. F. Shlesinger, G. M. Zaslavsky, and J. Klafter, Nature (London) 363, 31 (1993).

[25] O. V. Usatenko and V. A. Yampol’skii, Phys. Rev. Lett. 90, 110601 (2003).

[26] C. Tsalis, J. Stat. Phis. 52, 479 (1988).

[27] Nonextensive Statistical Mechanics and Its Applications, eds. S. Abe and Yu. Okamoto

(Springer, Berlin, 2001).

[28] S. Denisov, Phys. Lett. A, 235, 447 (1997).

[29] S. S. Melnyk, O. V. Usatenko, and V. A. Yampol’skii, Physica A, 361, 405 (2006).

[30] The existence of the ”additional tail” in the correlation function is in agreement with Ref. [19]

and corresponds to the well known fact that the correlation length is always larger then the

region of memory function action.


6
0
0
2
 
c
e
D
 
7
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
7
0
2
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

a non-iterative algorithm to estimate the modes of
univariate mixtures with well-separated components

Nicolas Paul, Student Member, IEEE, Luc Fety and Michel Terre, Member, IEEE

Electronic and communication department, Conservatoire National des Arts et Metiers
292 rue saint martin, 75003 PARIS, FRANCE
E-mail: nicolas.paul@cnam.fr, Tel: (33) 1 40 27 25 67, Fax: (33) 1 40 27 24 81

Abstract— This paper deals with the estimation of the modes
of an univariate mixture when the number of components is
known and when the component density are well separated.
We propose an algorithm based on the minimization of the
”kp” criterion we introduced in a previous work. In this paper
we show that the global minimum of this criterion can be
reached with a linear least square minimization followed by a
roots ﬁnding algorithm. This is a major advantage compared
to classical iterative algorithms such as K-means or EM which
suffer from the potential convergence to some local extrema of the
cost function they use. Our algorithm performances are ﬁnally
illustrated through simulations of a ﬁve components mixture.

Index Terms—

univariate mixture, separated mixture components, multimodal
estimation, non-iterative algorithm

EDICS Category: SAS-STAT

I. INTRODUCTION

In this paper we focus on the estimation of the modes of
an univariate mixture with a known number of components.
When the mixture component belongs to a parameterized
family known by the estimator (gaussian mixture case for
instance), the observation estimated moments can be mapped
to the mixture parameters [1]. Yet, when the number of
components is high, the relationships between the moments
and the mixture parameters are usually too complicated to be
analytically solved. Alternately, the Expectation-Maximization
(EM) [2] algorithm is the most commonly used method when
the mixture densities belong to a parameterized family. It is
an iterative algorithm that look for the mixture parameters
that maximize the likelihood of the observations. The EM
iteration consists of two steps. The Expectation step estimates
the probability for each observation to come from each mixture
component. During the Maximization step, these estimated
probabilities are used to update the estimation of the mixture
parameters. If the mixture components do not belong to any
parameterized family, or if the parameterized family is not
known by the estimator, the moment method and the EM
algorithm do not directly apply. Yet, if the mixture compo-
nents density are identical and quite separated, any clustering
methods can be used to cluster the data and calculate the
clusters means to reach the mixture modes. A survey of the
clustering techniques can be found in [3]. Among them, the
K-means algorithm [4] is one of the most popular method.

It is an iterative algorithm which groups the data into K
clusters in order to minimize an objective function such as
the sum of point to cluster mean square Euclidean distance.
K-means alternately assign each data to the closest cluster
center, compute the new clusters centers and calculate the
resulting cost function. A data assignment is validated only
if it decreases the overall cost function. The main drawback
of K-means or EM is the potential convergence to some local
extrema of the criterion they use. Some solutions consist for
instance in using smart initializations ( [5] [6] for EM, [7] for
k-means) or stochastic optimization, to become less sensitive
in the initialization ( [8] [9] for EM, [10] for K-means).
Another drawback of these methods is the convergence speed,
which can be very slow when the number of observations is
high. In this contribution, we propose a non-iterative algorithm
which mainly consists in calculating the minimum of the ”k-
product” (kp) criterion we ﬁrst introduced in [11]. The main
motivation for using such criterion is that its minimization
has a global solution which can be reached by a least square
optimization followed by a roots ﬁnding algorithm. The paper
is organized as follows: In section 2 the observation model is
presented and the criterion is deﬁned. In section 3 the criterion
global minimum is theoretically calculated. In section 4 the
mode estimation algorithm is described. Section 5 presents
some simulations which illustrate the algorithm performances
for a 5 components mixture and conclusions are given in
Section 6.

II. OBSERVATION MODEL AND CRITERION DEFINITION

Let δ be a discrete random variable taking its values in the
set {ak}k∈{1···K} of RK with probabilities {πk}k∈{1···K} and
let v be a random zero-mean variable with probability density
function g(v). The multimodal observation z is given by:

z = δ + v

(1)

∆
We call a the vector of
=
(a1, a2 · · · aK)t. We suppose that the ak are all distincts: the
probability density of z, f (z), is then a ﬁnite mixture of K
identical densities with the mixing weights {πk}k∈{1···K}:

the modes deﬁned by a

f (z) =

πkg(z − ak)

(2)

K

k=1
X

Let {zn}n∈{1···N } be a set of N observations in RN . In all
the following we assume that N is superior to K and that the
number of different observations is superior to K − 1. The kp
criteria J(x) is deﬁned by:

J : RK → R+; x →

2
(zn − xk)

(3)

N

K

n=1
X

k=1
Y

This criterion has been introduced in [11]. It is clearly positive
for any vector x. The ﬁrst intuitive motivation for deﬁning this
criterion is its asymptotic behavior when v is null. In this case,
all the observations are equal to one of the ak and therefore
J(a) = 0. J(x) is then minimal when x is equal to a or any
of its K! permutations. The second motivation is that, in the
general case, J have K! minima that are the K! permutations
of one single vector which can be reached with a linear least
square solution followed by a roots ﬁnding algorithm. This is
shown in section 3.

III. KP GLOBAL MINIMUM

We ﬁrst provide in section III-A some useful deﬁnitions
which are needed in section III-B to reach the global minimum
of J.

A. Some Useful Deﬁnitions

To any observation zn we associate the vector zn deﬁned

by:

zn

∆
= (zK−1
n

, zK−2
n

· · · , 1)t, zn ∈ RK

The vector z and the Hankel matrix Z are then respectively
deﬁned by:

N

∆
=

z

n zn, z ∈ RK
zK

∆
=

Z

znzt

n, Z ∈ RK×K

n=1
X
N

n=1
X

qy(α)

∆
= αK −

αK−kyk

K

k=1
X

qy(α) =

(α − rk)

K

i=1
Y

K

k=1
X

if r = (r1, · · · , rK)t is a vector of CK containing the K roots
of qy(α) the factorial form of qy(α) is:

qy(α) = αK−(r1+· · ·+rK)αk−1+...+(−1)K(r1×r2 · · ·×rK)
(9)

qy(α) = αK −

αK−kwk(r)

(10)

2

where wk(r) is the Elementary Symmetric Polynomial (ESP)
( [12]) in the variables r1, · · · , rK deﬁned by:

wk(r)

∆
= (−1)k+1

rj1 .rj2 · · · .rjk

(11)

X{j1,··· ,jk}∈{1···K}
j1<···<jk6K

k

If we call w(r) the vector of ESP of r deﬁned by:

w(r)

∆
= (w1(r), · · · , wK (r))t

(12)

the relationship between the roots and coefﬁcients of qy(α)
becomes:

y = w(r) ⇔ ∀k ∈ {1 · · · K} qy(rk) = 0

(13)

B. The KP Minimum

2

N
n=1

nw(x)

n − zt
zK

The main idea is to express J(x) as a function of w(x):
using deﬁnitions (4) and (12), the development of each term of
the sum in J leads to J(x) =
. There-
fore, the minization of J becomes a least square minimization
P
in the variable w(x). The vector ymin which minimizes
can be easily obtained. Now if xmin is
a vector such as ymin = w(xmin) and xmin ∈ RK, then xmin
P
is clearly a minimum of J. According to (13), xmin has to
contain the K roots of qymin (α) to have ymin = w(xmin).
The difﬁcult part is to show that these K roots of qymin (α)
are always real:

n − zt
zK

N
n=1

ny

(cid:1)

(cid:0)

(cid:0)

(cid:1)

2

Theorem 1: if ymin is the solution of Z.ymin = z (where
Z and z have been deﬁned in (5) and (6)) and if xmin is a
vector containing, in any order, the K roots of qymin (α), then
xmin belongs to RK and xmin is the global minimum of J.

Proof: Let F be the function deﬁned by:

F : CK → R+ : x →

||zn − xk||2
C

(14)

The restriction of F to RK is the function J since the
observations zn are real:

∀x ∈ RK :

F (x) = J(x)

(15)

H : CK → R+ : y →

n − zt
zK

ny

2
C

(16)

(cid:13)
(cid:13)
The function H applied to the ESP of a vector x in Ck is
equal to the function F applied to x:

(cid:13)
(cid:13)

2

C

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∀x ∈ CK :

F (x) =

(zn − xk)

(17)

developping (17) using deﬁnition (11) leads to:

∀x ∈ CK :

F (x) =

zK
n −

N

K

n=1 (cid:13)
(cid:13)
X
(cid:13)
(cid:13)
(cid:13)

k=1
X

zK−k
n wk(x)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

C

(18)

including deﬁnitions (4) and (12):

N

K

n=1
X

k=1
Y

N

n=1
X

N

K

k=1
Y

n=1 (cid:13)
(cid:13)
X
(cid:13)
(cid:13)
(cid:13)

(4)

(5)

(6)

(7)

(8)

Let y = (y1, · · · , yK)t be a vector of RK. We deﬁne the
polynomial of order K qy(α) as:

Now let H be the function deﬁned by:

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

∀x ∈ CK :

F (x) =

n − zt
zK

nw(x)

N

n=1
X

(cid:13)
(cid:13)

2
C

(cid:13)
(cid:13)

∀x ∈ CK :

F (x) = H(w(x))

The global minimum of H is the linear least square solution
ymin given by:

ymin = argmin

y∈CK (

N

n − zt
zK

ny

2
C

)

n=1
X
developping (21) using deﬁnitions (5) and (6) and remember-
ing that the coefﬁcients of Z and z are real:

(cid:13)
(cid:13)

(cid:13)
(cid:13)

ymin = argmin
y∈CK

yH Zy − 2Re{yH }z

(cid:8)
Z.ymin = z,

ymin ∈ RK

(cid:9)

The Hankel matrix Z is regular since the number of dif-
ferent observations is superior to K − 1 [13]. Sytem (23)
therefore have exactly one solution. Since Z belongs to
RK×K and z belongs to RK, ymin belongs to RK. Now let
xmin=(x1,min, · · · , xK,min)t be a vector containing, in any
order, the K (potentially complex) roots of qymin (α). One can
show that the following holds:

(i) xmin is a global minimum of F
(ii) xmin ∈ RK
(iii) xmin is a global minimum of J
Property (i) is a direct consequence of (20):

∀x ∈ CK :

∀x ∈ CK :

∀x ∈ CK :

F (x) = H(w(x))

F (x) ≥ min {H}

F (x) ≥ H(ymin)

According to (13), ymin = w(xmin) and we have:

∀x ∈ CK :

F (x) ≥ H(w(xmin))

∀x ∈ CK :

F (x) ≥ F (xmin)

which proves (i). Property (ii) can be shown by contradiction:
if xmin does not belong to RK, then for one of the xk,min we
have xk,min 6= Re{xk,min} and, since all the observations zn
are real:

∀n ∈ {1, · · · , N } : kzn − xk,minkC > kzn − Re{xk,min}kC
(29)

which leads to:

F (xmin) > F (Re{xmin})

(30)

This is impossible since xmin is a global minimum of F .
This proves property (ii). We ﬁnally have to prove (iii): since
xmin ∈ RK we have, using (15):

F (xmin) = J(xmin)

(31)

Furthermore, according to (15):

3

(34)

(35)

TABLE I

KP ALGORITHM STEPS AND COMPLEXITIES

step 1: calculate a minimum of J
calculate Z and z: o(N K)
calculate ymin by solving (23): o(K 2)
calculate the roots (x1,min, · · · , xK,min) of qymin (α): o(K 2)
step 2: clustering and mode estimation
assign each zn to the closest xk,min: o(N K)
calculate the K means of the resulting clusters: o(N )

then, according to property (i):

∀x ∈ RK :

J(x) ≥ F (xmin)

using (31):

∀x ∈ RK :

J(x) ≥ J(xmin)

which proves (iii). Properties (ii) and (iii) directly lead to
theorem 1.

IV. MODES ESTIMATION ALGORITHM

The mode estimation algorithm consists of two steps. In
the ﬁrst step, the minimum of J, {x1,min, ..., xK,min}, is
calculated, giving a ﬁrst raw estimation of the set of modes. In
the second step, each observation zn is assigned to the nearest
estimated mode, K clusters are formed, and the ﬁnal set of
estimated modes is given by the means of the K clusters.
The algorithm steps and their complexities are illustrated in
table I. It appears from table I that the global complexity is
in o(N K + K 2), which is equivalent to o(N K) since N is
superior to K.

e−λ|v| with a variance

V. SIMULATION
For each simulation run, a set of N = 100 observa-
tions is generated from the mixture described in (1) with
K = 5 modes, a = (0, 1, 2, 3, 4)t and πk = 0.2 for all
k in {1, · · · , K}. The density of v is a zero-mean Laplace
λ
2
distribution given by g(v) =
λ2 =
2
10−2. This leads to well separated mixture component; the
observation multimodal pdf is shown in ﬁgure 1. We suppose
that the form of g(v) is not known by the estimator. Therefore
a moment matching method or the EM algorithms would
not directly apply. The kp algorithm is then compared to
the K-means algorithm. The K-means algorithm is randomly
initialized and the used cost function is the point to cluster
mean square Euclidean distance. K-means is stopped when
the cost function no longer decreases. The number of modes
is supposed to be known in each method. 10000 runs have been
performed. To get rid of the permutation ambiguity, for each
run r, the estimated mode (ˆar) accuracy is characterized by
the maximal absolute distance Dr between the sorted vector
of mode and the sorted vector of estimated mode:

Dr

∆
= N (sort(a) − sort(ˆar))

(36)

∀x ∈ RK :

J(x) = F (x)

∀x ∈ RK :

J(x) ≥ min{F }

(32)

where N (x)

∆
= max

k∈{1···K}

|xk|. The distribution of Dr is given

(33)

in ﬁgure 2. With the K-means algorithm, Dr is inferior to

observation pdf

 for 10000 runs of 100 observations
distribution of D
r

4

kp
k−means

2.5

2

1

1.5

f
d
p

0.5

0
−1

3000

2500

2000

1500

n
o
i
t
u
b
i
r
t
s
d

i

1000

500

0

0

1

3

4

5

2
observation

0

0.2

0.4

0.6

0.8

1

D
r

Fig. 1. probability density function of a ﬁve components univariate mixture
with components means {0,1,2,3,4}, mixing weight {0.2,0.2,0.2,0.2,0.2} and
λ
λ2 = 10−2.
2

common component density g(v) =

e−λ|v| with variance

2

Fig. 2.
comparison of kp (*) and k-means (o) algorithm applied to the ﬁve-
component mixture described in ﬁgure 1; for each method, 10000 runs of 100
observations have been generated; for each run, the performance criteria, Dr,
is the maximal absolute distance between the sorted vector of modes and the
sorted vector of estimated modes.

0.1 for 41.3% of the run and inferior to 0.2 for 41.4% of
the run. Yet, for 58.6% of the run, Dr is superior to 0.7,
which corresponds to a poor estimation of the set of modes.
In this case the K-means method has converged to a local
minimum of its cost function. Typically, one estimated mode
is located in the middle of two true modes (gathering two true
clusters) while two other estimated modes are closed to the
same true mode. In this conﬁguration of estimated modes, Dr
is around 1. On the contrary, the kp algorithm always provides
an accurate set of estimated modes: Dr remains inferior to 0.1
for 98.7% of the run and Dr remains inferior to 0.2 for 99.6%
of the run.

VI. CONCLUSION

We have provided a global minimum of the new ”kp” crite-
rion we ﬁrst introduced in [11] and used it for the estimation of
the modes of univariate mixture whose component density are
common and well separated. The form of the mixture densities
does not have to be known by the mode estimator and does not
have to belong to any particular parameterized family. Simu-
lations have illustrated the kp algorithm good performances in
the case of an univariate mixture of ﬁve Laplace distributions.
In particular, the simulations have shown the superiority of our
algorithm to the K-means algorithm which often converge to
local minima of the used cost function. The generalization to
multivariate mixture is now being studied, as well as the use
of the kp criteria for estimating the number of components in
a mixture.

REFERENCES

[1] B. Lindsay, ”Moment Matrices: Application in Mixture”, The Annals of

Statistics, Vol. 17, No. 2 (June 1989) pp. 722-740

[2] A. Dempster, N. Laird and D. Rubin, ”Maximum likelihood from
incomplete data via the EM algorithm”, journal of the Royal Statistical
Society, B. 39, pp. 1-38, 1977

[3] P. Berkin, ”A Survey of clustering data mining techniques”, in Grouping
Multidimensional Data: Recent Advances in Clustering, Ed. J. Kogan
and C. Nicholas and M. Teboulle. Page(s) 25-71.Springer, 2006.
[4] J. Hartigan and M. Wong ”A k-means clustering algorithm”, journal of

Applied Statistics, 1979, vol 28, pp. 100-108

[5] G. McLachlan and D. Peel ”Finite Mixture Models”, Wiley Series in

probability and statistics, John Wiley and Sons, 2000

[6] B. Lindsay and D. Furman, ”Measuring the relative effectiveness of
moment estimators as starting values in maximizing likelihoods”, Com-
putational Statistics and Data Analysis, Volume 17 , Issue 5 (June 1994)
pp. 493 - 507

[7] P. S. Bradley and U.M Fayyad, ”Reﬁning initial points for K-means
clustering” in Proc. of the 15th Int. Conf. on Machine Learning, pp.
91-99, San-Fransisco, 1998. Morgan Kaufmann.

[8] G. Celeux, D. Chauveau and J. Diebolt, ”On stochastic version of
the EM algorithm”, INRIA research report no 2514, 1995, available:
http://www.inria.fr/rrrt/rr-2514.html

[9] F. Pernkopf and D. Bouchaffra, ”Genetic-Based EM Algorithm for
Learning Gaussian Mixture Models”, IEEE Transactions On Pattern
Analysis and Machine Intelligence, Vol. 27, No. 8, August 2005
[10] K. Krishna and M. Narasimha Murty ”Genetic K-Means Algorithm”,
IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cyber-
netics, Vol. 29, No. 3, June 1999

[11] N. Paul, M. Terre and L. Fety ”the k-product criterion for gaussian
mixture estimation”’ 7th Nordic Signal Processing Symposium, June 7-
9, 2006 Reykjavik, Iceland

[12] S. Lang, ”Algebra”, Springer-Verlag, 2004
[13] J.A. Shohat ”the problem of moments”, american mathematical society,

New York 1943


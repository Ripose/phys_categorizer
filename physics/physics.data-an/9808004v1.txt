8
9
9
1
 
g
u
A
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
0
0
8
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Joint Bayesian Treatment of Poisson and Gaussian Experiments in a Chi-squared
Statistic

Bayesian Poisson probability distributions for ¯n can be converted into equivalent chi-squared
distributions. These can then be combined with other Gaussian or Bayesian Poisson distributions
to make a total chi-squared distribution.

Dennis Silverman
Department of Physics and Astronomy
University of California, Irvine
4129 Frederick Reines Hall
Irvine, CA 92697-4575
(September 23, 2013)

I. INTRODUCTION

In analyzing the joint probability for mutual experimental results or for parameters, often a number of Poisson
statistics experiments with a low number of events may be mixed with Gaussian experiments with high numbers of
events. It is desirable to combine both types in a way to maintain the simplicity of a chi-squared distribution for all
of the experiments. In this paper we show a simple mathematical identity between the Bayesian Poisson distribution
for the average and an associated chi-squared distribution that allows us to accomplish this. In section 2 we restate
the method for joining two chi-squared distributions into a joint chi-squared distribution. In section 3 we review using
Bayes’ theorem to ﬁnd the Bayesian Poisson distribution for the average. In section 4 we show the equivalence of the
Bayesian Poisson distribution for the average to a chi-squared distribution. In section 5 we derive the joint probability
distribution for combining a single Bayesian Poisson distribution for the average with a chi-squared distribution. In
section 6 we then use the results of section 2 to combine in general the Bayesian Poisson distributions for averages
with chi-squared distributions from Gaussian distributions. Appendix A reviews the comparison of the integrated
probability of the Bayesian Poisson distribution for the average with the classical Poisson sum which is often used.
Appendix B gives a table of two-sided conﬁdence level limits for the Bayesian Poisson average for a single experiment.
Appendix C gives a table of chi-squared conﬁdence levels which are useful for the joint distribution. Appendix D
gives the solution for the minimum chi-squared for the case that the means only depend linearly on the parameters
in both the Poisson and Gaussian distributions. Appendix E gives the most probable value and limits for a single
linear parameter in the combination of one Poisson experiment with one Gaussian experiment. Appendix F examines
the consistency of converting Poisson to chi-squared distributions in the case of combining two Poisson distributions
whose averages depend on one linear parameter.

II. METHOD OF JOINING TWO CHI-SQUARED DISTRIBUTIONS

With xi = (X expt

i

X th

−

i )/σi being the scaled deviation for each experiment, chi-squared is deﬁned by

The basic chi-squared distribution with N experiments is

The chi-squared distribution arises from combining independent Gaussian distributions

(1)

(2)

(3)

by integrating over all xi subject to a delta-function constraint of holding the “radius squared”, χ2, ﬁxed while
integrating over the relative “angles” in the N dimensional space. We do this by inserting in the xi integrals

χ2 =

N

Xi=1

x2
i .

fN (χ2) =

2

(χ2)
2

2

N

2 −1e− χ
2 Γ( N
2 )

N

g(x) =

2
i
2

e− x

1

(2π)

p

1

We break these integrations into two parts consisting of

dχ2δ(χ2

Z

x2
i ).

− X

χ2

1 =

x2
i ,

N1

Xi=1
N

and χ2

2 =

x2
i

Xi=N1+1

−

1 −
∞

1 =

Z
0

dχ2fN (χ2),

with N2 ≡

N

−

N1 terms in χ2

2. We then insert two integrals over delta functions which are identically equal to 1

N1

N

dχ2

1δ(χ2

Z

1 −

Xi=1

χ2
i )

Z

dχ2

2δ(χ2

2 −

Xi=N1+1

χ2

i ).

The angular integrals over the subsets for N1 and N2 subject to the delta functions are now performed. Observing
that the original delta function now reads δ(χ2

2) we can use it to integrate out χ2
χ2

2. This leaves

χ2

and the convolution integral for combining two chi-squared distributions for N1 and N2, which is the standard formula
for producing the joint chi-squared distribution

fN (χ2) =

dχ2

1fN1(χ2

1)fN2(χ2

χ2

1).

−

2
χ

Z
0

1/χ2, and using the formula for the
By substituting chi-squared distributions in the above, changing variable to t = χ2
resulting beta function integral, one sees more directly that the result fN (χ2) is the chi-squared distribution function
for N = N1 + N2. (The analogous formula for joining two Poisson distributions, with averages ¯n1 and ¯n2 to produce
nt total events is

P (nt; ¯nt) =

P (n1; ¯n1)P (nt −

n1; ¯n2),

nt

Xn1=0

where ¯nt = ¯n1 + ¯n2.)

III. POISSON DISTRIBUTION AND BAYES THEOREM FOR LIMITING ¯N

According to Bayes’ Theorem [1–3], the probability for a given “theoretical parameter average” ¯n given an observed
number of events n, P (¯n; n), is proportional to the probability of observing n events from a Poisson distribution with
an average number of events ¯n, or P (n; ¯n) [4]. The latter is

The probability distribution for ¯n, P (¯n; n), is proportional to this [5], subject to the normalization condition that the
probability for all possible ¯n should integrate to one

This is satisﬁed by the formula for P (n; ¯n) without further renormalization, since the integral is seen to be the form
for Γ(n + 1)/n! = 1. Thus we have the normalized distribution for ¯n which we call the Bayesian Poisson distribution
for the average [6].

P (n; ¯n) =

¯nne−¯n
n!

.

∞

Z

0

d¯nP (¯n; n) = 1.

P (¯n; n) =

¯nne−¯n
n!

.

2

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

IV. CONNECTION OF THE BAYESIAN POISSON DISTRIBUTION FOR THE AVERAGE TO A
CHI-SQUARED DISTRIBUTION

We will show a mapping of the variables (¯n, n) from a Bayesian Poisson distribution for the average to (χ2, N ) for
a chi-squared distribution that keeps the identical probability distribution and integration of the Poisson distribution,
but now in a chis-squared form. This may be used by itself using usual chi-squared probabilities and contours, or
included with other chi-squared joined experiments by the convolution integral in section 2.

The chi-squared distribution to be integrated over dχ2 for N degrees of freedom is

This is identical to the ¯n distribution to be integrated over ¯n

with the identiﬁcation of

and

fN (χ2) =

1
2Γ(N/2)

e

2
−χ

/2

χ2
2 (cid:19)

(cid:18)

N/2−1

.

P (¯n; n) =

e−¯n¯nn

1
n!

¯n =

χ2
2

,

or χ2 = 2¯n,

n = N/2

1,

or N = 2(n + 1).

−

d¯n =

dχ2

1
2

P (¯n; nT ) =

(¯nS + ¯B)nT e−(¯nS+ ¯B)
nT !

.

The equivalency of the two forms is noted in the Particle Data Group article on statistics [7], but they do not use it
to merge experiments into a chi-squared distribution. The identity includes the integrals over ranges of probabilities
in ¯n or equivalently in χ2 using

Thus a Poisson with n events now counts mathematically as a chi-squared distribution with N = 2n + 2 degrees of
freedom.

For cases with an unknown mean signal number of events ¯nS plus an exact known background average ¯B, the

Bayesian Poisson distribution for the mean (¯nS + ¯B) when nT events are observed is [8]

The considerations of this paper apply to this case as well with n = nT and ¯n = ¯nT = ¯nS + ¯B.

If the prior probability is of a logarithmic, power law preserving form preferred by statisticians, P (¯n) = 1/¯n, then
the normalized Bayesian Poisson distribution for the average is directly seen to be the same as that for the uniform
prior for n
1) [1]. Since the Poisson form was the only requirement for the above connection
between Bayesian Poisson and chi-squared distributions, the results still hold for the logarithmic prior, but with n
replaced by n

1 events, P (¯n; n

1, so that Nlog = 2n.

−

−

−

V. DERIVATION OF JOINT PROBABILITY FOR A BAYESIAN POISSON DISTRIBUTION FOR THE
AVERAGE AND A CHI-SQUARED DISTRIBUTION

Here we demonstrate the derivation of the product probability for the case of one Poisson distribution for the
G with NG degrees of freedom formed either from Gaussians or from

average with a chi-squared distribution for χ2
joint Gaussian and Poisson distributions. The integrated product probability is

We convert the integral over the average ¯n to the variable χ2

P = 2¯n and rewrite using Eqs.(15-17)

1 =

d¯nP (¯n; n)

dχ2

GfNG(χ2
G)

∞

Z
0

∞

Z
0

3

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

with NP = 2n + 2. Into the new integral we now introduce the total χ2 by inserting
this to do the dχ2

G integral, which limits χ2

χ2

∞

0 dχ2δ(χ2
R

χ2

P −

−

χ2

G) and use

d¯nP (¯n; n) = fNP (χ2

P )dχ2
P

∞

P ≤

dχ2

2
χ

Z
0

1 =

Z
0

dχ2

P fNP (χ2

P )fNG(χ2

χ2

P ).

−

By the chi-squared convolution integral, the second integral is fNP +NG(χ2), which is the resultant probability distri-
bution for this case, with χ2 = χ2

P + χ2
G.

VI. MERGING BAYESIAN POISSON AND CHI-SQUARED DISTRIBUTIONS

Now that we have a fN (χ2) distribution Eq.(14) that is equivalent to a Bayesian Poisson parameter distribution in
value and in its probability integral, we can merge this (independent of its origin) with other chi-squared distributions
using Eq.(9), the convolution, to obtain the ﬁnal χ2 distribution.

The results can now be used, for example, in ﬁnding χ2 contours corresponding to various conﬁdence levels. We
must remember that a single Poisson experiment now counts as N = 2(n + 1) degrees of freedom, where n is the
number of observed events in the Poisson distribution. While this sounds counter-intuitive, we recall that the form
of the χ2 distribution that we are using also has χ2 replaced by 2¯n, and with the above replacements, χ2 per degree
of freedom N or χ2/N = 2¯n/(2(n + 1)), approaches 1 at large n since n is within √¯n of ¯n.

If MP is the number of Poisson experiments with ni events in the i’th experiment, we associate with each Ni = 2ni+2
degrees of freedom. We call the associated theoretical Poisson averages ¯ni. The total Poisson degrees of freedom
becomes

MP
i=1 2ni. We now convolute the Poisson distributions
With the alternate choice of a logarithmic prior, NP −log =
for the average in the chi-squared forms, Eqs.(13-22) with the chi-squared distribution of NG Gaussian experimental
degrees of freedom which have a chi-squared χ2

G. The result will use the joint chi-square

P

From successive convolutions in Eq.(9), the combined chi-squared distribution for the Poisson plus Gaussian distri-
butions is ﬁnally

Npar.

We emphasize that these results are an exact treatment, not involving large n or other approximations. As in the
standard treatment, if Npar is the number of parameters that are being ﬁtted, then the number of degrees of freedom
is dof = NG + NP −
This has been applied in analyzing the constraints of many experiments on new ﬂavor changing neutral current
models of CP violation in B meson decay asymmetries [9]. There, all experiments have a Gaussian distribution, except
for an experiment [10] where one event has been seen in K +
P = 2¯n and
adding four degrees of freedom. In that case, ¯n is a function of the down quark mixing matrix elements as are the
other experiments. That analysis also provides an example of the sensitivity to the choice of a uniform or logarithmic
prior probability distribution. With the uniform prior, the total number of degrees of freedom is seven, and the
chi-squared limits are at 8.2, 12.0, and 14.3 for 1-σ, 90% (1.64-σ), and 2-σ conﬁdence levels, respectively. With the
logarithmic prior, the total number of degrees of freedom decreases by two to ﬁve, and the chi-squared limits are at
5.89, 9.24, and 11.3, for the 1-σ, 90%, and 2-σ conﬁdence levels, respectively. The chi-squared per degree of freedom
ratios stay withing 10% of each other between the two cases. However, use of the logarithmic prior does move the
contours in by two to three units or about 1/2 of a standard deviation, and thus gives tighter bounds.

π+ν ¯ν and is treated with an additional χ2

→

In the limit of large n and ¯n, just as the Poisson distribution becomes a Gaussian, so does the equivalent chi-squared

distribution. The chi-squared distribution in Eq. (14) becomes

4

MP

MP

NP =

Ni =

(2ni + 2).

Xi=1

Xi=1

χ2

P G = 2

¯ni + χ2
G.

MP

Xi=1

f(NG+NP )(χ2

P G).

G(ξ, σ) =

2

2

/2σ

e−ξ
√2πσ

,

where in our variables σ = ¯n1/2 = (χ2/2)1/2, ξ = n
the order of σ for large n and ¯n, the diﬀerence
N/2
|
N and χ2.

−
−

¯n = (N/2
χ2/2
1

−

1)

χ2/2, and dχ2 =

−

−
−
is conﬁned to the order of
|

2dξ. Since
ξ
|
χ2/2 or

|

is conﬁned to
N/2 for large

p

p

In conclusion, we have shown how the simplicity of the Bayesian χ2 analysis can be extended to include experiments

with a small number of events which are described by a Bayesian Poisson distribution for the average.

The author thanks Peter Meyers, Bill Molzon, and Jonas Schultz for helpful discussions. This research was supported

in part by the U. S. Department of Energy under Contract No. DE-FG0391ER40679.

ACKNOWLEDGMENTS

APPENDIX A: COMPARISON WITH OTHER FORMULAS USED FOR POISSON PARAMETER
LIMITS

For completeness we include here some properties of and a comparison between the classical (or frequentist) and
Bayesian Poisson limits on ¯n. The methods are given full discussion by R. D. Cousins in Ref. 1. The classical Poisson
parameter distribution used for the upper ¯n limit is to sum the Poisson distributions P (n; ¯n) from n + 1 events to
inﬁnity, when the number of observed events is n, and use it as the probability for ¯n when ¯n is greater than n. We
show that the Bayesian Poisson parameter distribution Eq.(13) integrated from zero to a cutoﬀ nc agrees with the
above formulation [1]. First we do the integrated probability for ¯n from nc to inﬁnity by integrating e−¯n by parts

Continued integration by parts shows that the integral over a semi-inﬁnite interval beginning at nc of the Bayesian
Poisson parameter distribution is [1,11]

The two methods are now seen to be equivalent using ¯n = nc and the fact that the Poisson terms sum [1] to 1

I(nc; n) = P (n; nc) + P (n

1; nc) + . . . + P (0; nc).

−

I(nc; n) =

d¯n

¯nn
n!

e−¯n

∞

Z

nc
¯nn
n!

∞

∞

+

d¯n

=

(
−

e−¯n)
(cid:21)

(cid:20)

Z
= P (n; nc) + I(nc; n

nc

nc

1).

¯nn−1
(n

1)!

−

e−¯n

−

∞

Xn′=n+1

P (n′; ¯n) = 1

P (n′; ¯n)

n

−

−
nc

Xn′=0
I(¯n; n)
¯nn
n!

d¯n

= 1

=

Z
0

e−¯n.

∞

Xn′=n+1

P (n′; n+

c ) = 1

I(n+

c , n) = 0.84

−

5

from Eqs.(12) and (A3).

For n the number of observed events, the rule for the “1-σ” upper limit on nc is to ﬁnd n+

c such that 84% of the
time there would be greater than n events. Since “1-σ” means 32% is outside the central region, 16% should occur
on one side. Thus the sum from n + 1 to inﬁnity is set equal to 0.84

from Eq.(A6). So for the upper “1-σ” limit, n+
distribution for the average in Eq.(A7) equal to 0.84 and the sum of higher n agree.

c , both the Bayesian result of setting the integral of the Poisson

(26)

(A1)

(A2)

(A3)

(A4)

(A5)

(A6)

(A7)

(A8)

For the lower 1-σ, the classical rule of setting the sum from 0 to n

from n to inf set to 0.16) gives

1 equal to 0.84 to determine n−

c (or the sum

−

n−1

Xn′=0

P (n′; n−

c ) = I(n−

c ; n

1) = 0.84.

−

This is not the same as setting the integral of the Bayesian Poisson distribution for the average from 0 to n−
to 0.16

c equal

I(n−

c ; n) =

1

−

nc

d¯n

¯nn
n!

Z
0

e−¯n = 0.16 or

I(n−

c ; n) = 0.84

from Eq.(A7). To see the diﬀerence, we note from Eq.(A3)

With the prior chosen to be 1/¯n, the lower limits agree but not the upper [1].

I(n−

c ; n) = P (n; n−

c ) + I(n−

c ; n

1).

−

(A9)

(A10)

(A11)

6

APPENDIX B: TABLE OF BAYESIAN POISSON CENTRAL LIMITS FOR THE AVERAGE

The Bayesian Poisson average central interval limits with uniform prior are the upper or lower n±
limits as in
c
Eq.(A8) or Eq.(A10) beyond which the conﬁdence level is below a given value. This is in analogy with the ¯x
σ one
σ limits in a single Gaussian distribution, where half of the excluded intervals on each side are used in the integral
limits (0.16 on each side for 1σ). The following table covers lower and upper limits out to 3σ, and for n = 0 to n = 24.
It was produced from the following Mathematica program (except for the n column), which can be used to extend
the table as needed

±

<< Statistics‘ContinuousDistributions‘

cl =

0.0013499, 0.01, 0.0227501, 0.1, 0.158655, 0.5, 0.841345, 0.9, 0.9772500, 0.99, 0.9996500
}

{

navgtable := N[Table[0.5

Quantile[ChiSquareDistribution[k], cl[[i]]],

k, 4, 50, 2

{

,

i, 1, 11
}

{

}

], 4]

TeXForm[navgtable//TableForm].

∗

For n = 0 events observed, the one-sided conﬁdence interval upper bounds are meaningful as opposed to two-sided
intervals. The upper limits of intervals starting from zero which contain 0.6827, 0.90, 0.95, 0.9545, 0.99, and 0.9973
probability are 1.15, 2.30, 3.00, 3.09, 4.61, and 5.9, respectively. G. J. Feldman and R. D. Cousins use an approach
which carefully covers both one and double sided cases [3].

Table I: Bayesian Poisson Central Limits for the Average

3σ

2σ

1σ

0.9

0.5

0.1

-1σ

-2σ

-3σ

0.99

0.01

n
1 0.05288 0.1486 0.2301 0.5318 0.7082 1.678 3.300 3.890 5.683 6.638 10.39
0.2117 0.4360 0.5963 1.102 1.367 2.674 4.638 5.322 7.348 8.406 12.47
2
0.4653 0.8232 1.058 1.745 2.086 3.672 5.918 6.681 8.902 10.05 14.38
3
0.7919 1.279 1.583 2.433 2.840 4.671 7.163 7.994 10.39 11.60 16.18
4
1.785 2.153 3.152 3.620 5.670 8.382 9.275 11.82 13.11 17.90
1.175
5
2.330 2.758 3.895 4.419 6.670 9.584 10.53 13.22 14.57 19.56
1.603
6
2.906 3.391 4.656 5.232 7.669 10.77 11.77 14.59 16.00 21.17
2.068
7
3.507 4.046 5.432 6.057 8.669 11.95 12.99 15.94 17.40 22.75
2.563
8
4.130 4.719 6.221 6.891 9.669 13.11 14.21 17.27 18.78 24.30
3.084
9
4.771 5.409 7.021 7.734 10.67 14.27 15.41 18.58 20.14 25.82
3.628
10
5.428 6.113 7.829 8.585 11.67 15.42 16.60 19.87 21.49 27.32
4.191
11
6.099 6.828 8.646 9.441 12.67 16.56 17.78 21.16 22.82 28.80
4.772
12
6.782 7.555 9.470 10.30 13.67 17.70 18.96 22.43 24.14 30.26
5.367
13
7.477 8.291 10.30 11.17 14.67 18.83 20.13 23.70 25.45 31.70
5.977
14
8.181 9.036 11.14 12.04 15.67 19.96 21.29 24.95 26.74 33.13
6.599
15
8.895 9.789 11.98 12.92 16.67 21.08 22.45 26.20 28.03 34.55
7.233
16
9.616 10.55 12.82 13.80 17.67 22.20 23.61 27.44 29.31 35.95
7.877
17
10.35 11.32 13.67 14.68 18.67 23.32 24.76 28.68 30.58 37.34
8.530
18
11.08 12.09 14.53 15.57 19.67 24.44 25.90 29.90 31.85 38.72
9.193
19
11.83 12.87 15.38 16.45 20.67 25.55 27.05 31.13 33.10 40.10
9.863
20
12.57 13.65 16.24 17.35 21.67 26.66 28.18 32.34 34.35 41.46
10.54
21
13.33 14.44 17.11 18.24 22.67 27.76 29.32 33.55 35.60 42.82
11.23
22
14.09 15.23 17.97 19.14 23.67 28.87 30.45 34.76 36.84 44.17
11.92
23
14.85 16.03 18.84 20.03 24.67 29.97 31.58 35.96 38.08 45.51
12.62
24

7

APPENDIX C: TABLE FOR CHI-SQUARED LIMITS

Since the joint method for n events requires χ2 for N = 2(n + 1) + NG which can be large, we give here a table of
chi-squared limits for large N up to 25, and a program with which one can generate further limits. In the following
table, N is the number of degrees of freedom, and 1, 2, and 3 σ correspond to 1-CL of 0.682689, 0.954500, and
0.997300, respectively. The Mathematica program used to generate the table is

<< Statistics‘ContinuousDistributions‘
0.682689, 0.9, 0.954500, 0.99, 0.997300
cstable := N[Table[Quantile[ChiSquareDistribution[k], cl[[i]]],

cl =

{

}

TeXForm[cstable//TableForm].

k, 1, 25

,

i, 1, 5

], 4]

}

{

}

{

2σ

0.90

0.99

Table II: Chi-squared Limits
3σ

N 1σ
1 1.000 2.706 4.000 6.635 9.000
2 2.296 4.605 6.180 9.210 11.83
3 3.527 6.251 8.025 11.34 14.16
4 4.719 7.779 9.716 13.28 16.25
5 5.888 9.236 11.31 15.09 18.21
6 7.038 10.64 12.85 16.81 20.06
7 8.176 12.02 14.34 18.48 21.85
8 9.304 13.36 15.79 20.09 23.57
9 10.42 14.68 17.21 21.67 25.26
10 11.54 15.99 18.61 23.21 26.90
11 12.64 17.28 19.99 24.72 28.51
12 13.74 18.55 21.35 26.22 30.10
13 14.84 19.81 22.69 27.69 31.66
14 15.94 21.06 24.03 29.14 33.20
15 17.03 22.31 25.34 30.58 34.71
16 18.11 23.54 26.65 32.00 36.22
17 19.20 24.77 27.95 33.41 37.70
18 20.28 25.99 29.24 34.81 39.17
19 21.36 27.20 30.52 36.19 40.63
20 22.44 28.41 31.80 37.57 42.08
21 23.51 29.62 33.07 38.93 43.52
22 24.59 30.81 34.33 40.29 44.94
23 25.66 32.01 35.58 41.64 46.36
24 26.73 33.20 36.83 42.98 47.76
25 27.80 34.38 38.07 44.31 49.16

APPENDIX D: SOLUTION FOR MINIMUM CHI-SQUARED FOR THE LINEAR PARAMETER
DEPENDENCE CASE

The case where the theoretical values for the mean in the Gaussian and Poisson distributions is linear in parameters
to be ﬁtted can be solved analytically using the same method as for pure Gaussian distributions [12,13]. In the method
of expressing Poisson distributions for the average as χ2 distributions in this paper, the ﬁnal χ2

GP is

where α is the set of k parameters αm. The experiments described by (yi, Fi) can even be totally diﬀerent, and the
Fi and ¯nℓ are assumed to be linearly expandable in the parameters αm

χ2

G =

NG

Xi=1

(yi −

Fi(α))2
σ2
i

,

and

GP = χ2
χ2

G + 2

¯nℓ(α),

NP

Xℓ=1

8

(D1)

(D2)

Fi(α) =

αnfin,

and

¯nℓ(α) =

nℓjαj.

k

Xn=1
k

Xj=1

nℓm,

and

gm =

V −1
mn =

NG

Xi=1
NG

Xi=1

yi

fim
σ2
i −

NP

Xℓ=1

finfim
σ2
i

.

ˆα = V g,

Minimizing χ2

GP with respect to each αm gives rise to the vector g and matrix V −1 with components

Using the inverse matrix V , the values of the parameters that give the minimum χ2

GP are given by

with the eﬀect of the ¯nk terms entering through g. The minimum value of χ2
can then be rewritten in terms of α away from the minimum values as

GP is Eq.(D2) evaluated at α = ˆα. χ2

GP

GP = χ2
χ2

GP −min + (α

ˆα)T V −1(α

−

ˆα).

−

APPENDIX E: SOLUTION OF ONE BAYESIAN POISSON DISTRIBUTION WITH ONE GAUSSIAN
DISTRIBUTION AND ONE LINEAR PARAMETER

We present here the solution for the single linear parameter case with one Bayesian Poisson and one Gaussian
distribution. For the unknown parameter a, we have the theoretical relations ¯n = acP for the Poisson average, and
¯x = ac with known standard deviation σ for the Gaussian average, where coeﬃcients cP and c are given, and n and
x are the results of the respective experiments. Then

With one parameter to be ﬁtted, the number of joint degrees of freedom with the equivalent chi-squared method with
1 = 2n + 2 where one degree of freedom is cancelled by the one parameter. For
a uniform prior is N = 2n + 2 + 1
1 = 2n, which gives tighter χ2 limits.
the logarithmic prior, N = 2n + 1
P G occurs at

The minimum of χ2

−
−

giving the minimum chi-squared

When χ2

P G is set equal to a certain upper limit boundary at χ2

lim, there are bounds on the range of a given by

For physical reasons we may want ¯a to be positive when cP and c are positive. Looking at ¯x = ¯ac above, we see
that ¯x and ¯a are positive when x¯x/σ2
¯n. In order to use a Gaussian, we expect at least a 3-σ separation of the
peak from zero, or x/σ
3 and
we can start using a Gaussian instead of a Poisson for the ¯n experiment. The same reasoning follows through if for
example we require a 5-σ separation from zero to use a Gaussian.

9, this method works and ¯a

≥
3. Thus for ¯n

3 and ¯x/σ

9, ¯n/√¯n

0. For ¯n

≤

≥

≥

≥

≥

≥

χ2

P G = 2¯n + (x

¯x)2/σ2

−

¯ac = x

σ2cP /c

−

χ2

min = 2x(cP /c)

σ2(cP /c)2.

−

a

±
lim = ¯a

σ
c q

±

χ2

lim −

χ2

min.

9

(D3)

(D4)

(D5)

(D6)

(D7)

(D8)

(E1)

(E2)

(E3)

(E4)

APPENDIX F: TWO POISSON DISTRIBUTIONS WITH ONE LINEAR PARAMETER

We approach this problem both from Bayes theorem directly, and from converting the Bayesian Poisson distributions
to chi-squared distributions as proposed in this paper. For the latter we then merge the chi-squared distributions to
a single chi-square distribution for the linear parameter and then convert that back to a joint Poisson distribution, to
compare to the direct approach. For the case of the logarithmic prior we ﬁnd consistency.

The averages of the experiments are theoretically given by the parameter a with respective known coeﬃcients
¯n1 = ac1 and ¯n2 = ac2. The direct Bayesian result is proportional to the probability for observing the experimental
values n1 and n2 given a value of a

Prob(a; n1, n2) = P (n1; ac1)P (n2; ac2)P (a)/(P (n1)P (n2))

(ac1)n1 (ac2)n2 e−ac1e−ac2P (a)
(a(c1 + c2))(n1+n2)e−a(c1+c2)P (a)
P (a(c1 + c2); n1 + n2)P (a).

∝
∝
∝

(F1)

1), integrating over da(c1 + c2).

For the uniform prior, P (a) = 1, the normalized result is P (a(c1 + c2); n1 + n2), integrating over da(c1 + c2). For the
logarithmic prior with P (a) = 1/a, the normalized result is the same as the uniform prior with total n lowered by 1,
or P (a(c1 + c2); n1 + n2 −
If we now start with the method in this paper, we take the joint Bayesian result as the product of the Bayesian
Poisson for each experiment as if they were independent, P (ac1; n1)P (ac2; n2) times either the uniform prior d¯n1d¯n2
1) with
or the logarithmic prior d¯n1d¯n2/(¯n1¯n2). The logarithmic prior is equivalent to P (ac1; n1 −
a uniform prior. Converting the uniform case to chi-squared distributions gives the convolution of the product
f2n1+2(2ac1)f2n2+2(2ac2) leading to f2n1+2n2+4(2ac1 + 2ac2). Converting this back to a Poisson distribution for the
average gives P (ac1+ac2; n1+n2+1) for the uniform prior, which is inconsistent with the direct uniform Bayesian result
in the previous paragraph. For the logarithmic prior, converting to chi-squared distributions gives the convolution of
the product f2n1(2ac1)f2n2(2ac2) which is f2n1+2n2 (2ac1 + 2ac2). Converting this back to a Poisson distribution for
the average gives

1)P (ac2; n2 −

P (ac1 + ac2; n1 + n2 −

1)

∝

P (ac1 + ac2; n1 + n2)

da(c1 + c2)
a(c1 + c2)

,

(F2)

which is consistent with the direct Bayesian result for the logarithmic prior in the previous paragraph.

In the combined form as a single Bayesian Poisson distribution for the average, both upper and lower limits on a
for a given central conﬁdence interval can be found using the table in appendix B. The case where no events were
observed in either experiment can also be dealt with using one-sided bounds, which are also given in appendix B.

[1] An excellent discussion of classical and Bayesian methods, including the Bayesian Poisson distribution for the average,

which also contains many references is R. D. Cousins, Am. J. Phys. 63, 398 (1985).

[2] Introductory lectures on the web: “Probability and Measurement Uncertainty in Physics - a Bayesian Primer”, G.

D’Agostini, hep-ph/9512295 (1995).

[3] G. J. Feldman and R. D. Cousins, Phys. Rev. D 57, 3873 (1998).
[4] This is similar to the approach used by S. Baker and R. D. Cousins, Nucl. Inst. and Meth. 221 437, (1984), except that
they use a likelihood function method which is the ratio of the Poisson distribution to the Poisson distribution with the
true mean.

[5] Bayes’ Theorem also contains the ratio of the a priori probabilities of P (¯n)/P (n). We take both as unity, or P (¯n) = 1,

which is called a uniform prior.

[6] L. J. Rainwater and C. S. Wu, Nucleonics 1, 60 (1947).
[7] Particle Data Group, Phys. Rev. D 54 Part I, (1996) p. 164, beneath Fig. 28.4.
[8] Nucl. Inst. and Meth. 228, 120 (1984).
[9] D. Silverman, “The Full Range of Predictions for B Physics From Iso-singlet Down Quark Mixing”, UC Irvine TR 98-14,

hep-ph/9806489, Phys. Rev. D, to be published.

[10] BNL E787, S. Adler, et al., Phys. Rev. Lett. 79, 2204 (1997).
[11] This formula is in M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions, NBS, formula 26.4.2, for

Q(2nc|2n + 2) with c − 1 = n using our replacements, and in Ref.1, Eq.(18).

10

[12] For the pure Gaussian case, see for example Jon Mathews and R. L. Walker, Mathematical Methods of Physics, Second

Edition, Section 14-7, Addison-Wesley (1970).

[13] Here we use notation similiar to the Particle Data Group, Phys. Rev. D 54 Part I, (1996) p. 160, Sec. 28.5.

11


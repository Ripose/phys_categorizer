1
0
0
2
 
v
o
N
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
1
0
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Bayesian source separation with mixture of
Gaussians prior for sources and Gaussian prior
for mixture coefﬁcients

Hichem Snoussi∗ and Ali Mohammad-Djafari∗

∗Laboratoire des Signaux et Systèmes (L2S),
Supélec, Plateau de Moulon, 91192 Gif-sur-Yvette Cedex, France

Abstract. In this contribution, we present new algorithms to source separation for the case of noisy
instantaneous linear mixture, within the Bayesian statistical framework. The source distribution
prior is modeled by a mixture of Gaussians [1] and the mixing matrix elements distributions by
a Gaussian [2]. We model the mixture of Gaussians hierarchically by mean of hidden variables
representing the labels of the mixture. Then, we consider the joint a posteriori distribution of
sources, mixing matrix elements, labels of the mixture and other parameters of the mixture with
appropriate prior probability laws to eliminate degeneracy of the likelihood function of variance
parameters and we propose two iterative algorithms to estimate jointly sources, mixing matrix and
hyperparameters: Joint MAP (Maximum a posteriori) algorithm and penalized EM algorithm. The
illustrative example is taken in [3] to compare with other algorithms proposed in literature.

PROBLEM DESCRIPTION

We consider a linear instantaneous mixture of n sources. Observations could be cor-
rupted by an additive noise. This noise may represent measurement errors or model
incertainty:

x(t) = As(t) + ǫ(t),

t = 1, .., T

(1)

where x(t) is the (m × 1) measurement vector, s(t) is the (n × 1) source vector which
components have to be separated, A is the mixing matrix of dimension (m × n) and ǫ(t)
represents noise affecting the measurements. We assume that the (m × T ) noise matrix
ǫ(t) is statistically independant of sources, centered, white and Gaussian with known
variance σ2
ǫ I. We note s1..T the matrix n × T of sources and x1..T the matrix m × T of
data.

Source separation problem consists of two sub-problems: Sources restoration and
mixing matrix identiﬁcation. Therefore, three directions can be followed:

1. Supervised learning: Identify A knowing a training sequence of sources s, then use

2. Unsupervised learning: Identify A directly from a part or the whole observations

it to reconstruct the sources.

and then use it to recover s.

3. Unsupervised joint estimation: Estimate jointly s and A

In the following, we investigate the third direction. This choice is motivated by practical
cases where sources and mixing matrix are unknown.

This paper is organised as follows: We begin in section II by proposing a Bayesian
approach to source separation. We set up the notations, present the prior laws of the
sources and the mixing matrix elements and present the joint MAP estimation algorithm
assuming known hyperparameters. We introduce, in section III, a hierarchical modelisa-
tion of the sources by mean of hidden variables representing the labels of the mixture of
Gaussians in the prior modeling and present a version of JMAP using the estimation of
these hidden variables (classiﬁcation) as an intermediate step. In both algorithms, we as-
sumed known the hyperparameters which is not realistic in applications. That is why, in
section IV, we present an original method for the estimation of hyperparameters which
takes advantages of using this hierarchical modeling. Finally, since EM algorithm has
been used extensively in source separation [4], we considered this algorithm and pro-
pose, in section V, a penalized version of the EM algorithm for source separation. This
penalization of the likelihood function is necessary to eliminate its degeneracy when
some variances of Gaussian mixture approche zero [5]. Each section is supported by
one typical simulation result and partial conclusion. At the end, we compare the two last
algorithms.

BAYESIAN APPROACH TO SOURCE SEPARATION

Given the observations x1..T , the joint a posteriori distribution of unknown variables
s1..T and A is:

p (A, s1..T |x1..T ) ∝ p (x1..T |A, s1..T ) p(A) p(s1..T )

(2)

where p(A) and p(s1..T ) are the prior distributions through which we modelise our a
priori information about sources s and mixing matrix A. p (x1..T |A, s1..T ) is the joint
likelihood distribution. We have, now, three directions:

1. First, integrate (2) with respect to s1..T to obtain the marginal in A and then estimate

A by:

A = arg max

{J(A) = ln p (A|x1..T )}

A

2. Second, integrate (2) with respect to A to obtain the marginal in s1..T and then

b

estimate s1..T by:

(3)

(4)

s1..T = arg max

{J(s1..T ) = ln p (s1..T |x1..T )}

s1..T

3. Third, estimate jointly s1..T and A:

b

(

A,

s1..T ) = arg max
(A,s1..T )

b

b

{J(A, s1..T ) = ln p (A, s1..T |x1..T )}

(5)

Choice of a priori distributions

The a priori distribution reﬂects our knowledge concerning the parameter to be
estimated. Therefore, it must be neither very speciﬁc to a particular problem nor too
general (uniform) and non informative. A parametric model for these distributions seems
to ﬁt this goal: Its stucture expresses the particularity of the problem and its parameters
allow a certain ﬂexibility.
Sources a priori: For sources s, we choose a mixture of Gaussians [1]:

p(sj) =

αjiN (mji, σ2

ji),

j = 1..n

(6)

qj

i=1
X

Hyperparameters qj are supposed to be known.
This choice was motivated by the following points:

• It represents a general class of distributions and is convenient in many digital

communications and image processing applications.

• For a Gaussian likelihood p (x1..T |s1..T , A) (considered as a function of s1..T ), the
a posteriori law remains in the same class (conjugate prior). We then have only to
update the parameters of the mixture with the data.

Mixing matrix a priori: To account for some model uncertainty, we assign a Gaussian
prior law to each element of the mixing matrix A:

p(Aij) = N (Mji, σ2

a,ij)

(7)

which can be interpreted as knowing every element (Mji) with some uncertainty (σ2
a,ij).
We underline here the advantage of estimating the mixing matrix A and not a separating
matrix B (inverse of A) which is the case of almost all the existing methods for source
separation (see for example [6]). This approach has at least two advantages: (i) A does
not need to be invertible (n 6= m), (ii) naturally, we have some a priori information on
the mixing matrix not on its inverse which may not exist.

JMAP algorithm

We propose an alternating iterative algorithm to estimate jointly s1..T and A by

extremizing the log-posterior distribution:

s(k)
A(k−1), s1..T |x1..T
1..T = arg maxs1..T ln p
(cid:16)
A(k) =
arg maxA ln p
b

s(k)
1..T |x1..T

(cid:17)




A,
b
(cid:16)

(cid:17)

(8)

In the following, we suppose that sources are white and spatially independant. This
assumption is not necessary in our approach but we start from here to be able to compare
later with other classical methods in which this hypothesis is fundamental.



b

b

With this hypothesis, in step (k + 1), the criterion to optimize with respect to s1..T is:

n

(cid:17)

j=1
X

n

j=1
X

T

t=1 "
X

(cid:16)

s(t)

J(s1..T ) =

ln p

x(t)|

A(k), s(t)

+

ln pj (sj(t))

(9)

#

Therefore, the optimisation is done independantly at each time t:

b

s(t)(k+1) = arg max

{ln p

x(t)|

A(k), s

+

ln pj (sj(t))}

(10)

b

(cid:17)
n
The a posteriori distribution of s is a mixture of
j=1 qj Gaussians. This leads to a
high computational cost. To obtain a more reasonable algorithm, we propose an iterative
scalar algorithm. The ﬁrst step consists in estimating each source component knowing
the other components estimated in the previous iteration:

Q

(cid:16)

b

sj(t)(k+1) = arg max

{ln p

sj(t)|x(t),

A(k),

sl6=j(t)(k)

}

(11)

sj(t)

(cid:16)

The a posteriori distribution of sj is a mixture of qj Gaussians:
with:

b

b

′

qj
z=1 α

′

jzN (m

jz, σ

2

′
jz

),

P

′

α

jz = αjz

1
jz + σ2
σ2
j

s

exp

−1
2

1
jz + σ2
σ2
j

(mj − mjz)2

where

b






′

m

jz =

j mjz + σ2
σ2
j + σ2
σ2
jz

jzmj

2

′
jz

σ

=

j σ2
σ2
jz
j + σ2
σ2
jz

(cid:2)

σ2
j =

σ2
ǫ
m
i=1 A2
ij






P

n
i=1 Aij (xi −
m
i=1 A2
ij

xi)

mj =

P

b

xi =

Ail sl

P

l6=j
X

b

(cid:17)

(cid:3)

(12)

(13)

If the means m

distribution. The algorithm to estimate sj is to ﬁrst compute

jz aren’t close to each other, we are in the case of a multi-modal
xi, i = 1, . . . , m, mj and

′

b

σ2
j by (13) and then α
is the greatest one.

′

jz, σ

′
jz

2 and m

′

jz by (12), and select the m

jz for which the ratio

′

′
α
jz
′
jz

σ

After a full update of all sources s1..T , the estimate of A is obtained by optimizing:

J(A) =

T
t=1 ln p

x(t)|A,

sk+1(t)

+ ln p (A(t)) + cte

(14)

which is quadratic in elements of A. The gradient has then a simple expression:

P

(cid:1)

(cid:0)

b

∂J(A)
∂Ai,j

=

1
σ2
ǫ

T

t=1
X

sk+1
j

(t)

xi(t) −

A

sk+1(t)

−

i

(Ai,j − Mi,j)

(15)

1
σ2

a;i,j

Cancelling the gradient to zero and deﬁning Λi,j = σ2
σ2
relation:

ǫ

a;i,j

(cid:0)

b

(cid:2)

b

(cid:3)

(cid:1)

, we obtain the following

x(t) − A

sk+1(t)

sk+1(t)T

− Λi,j (Ai,j − Mi,j) = 0

(16)

T

"

t=1
X

(cid:0)

#i,j

b

(cid:1)

b

We deﬁne the operator Vect transforming a matrix to a vector by the concatenation of
the transposed rows. Operator Mat is the inverse of Vect. Applying operator Vect to
relation (16), we obtain the following expression:

V ect

x1..T (

sk+1
1..T )T

+ µV ect(M) = (µ + S∗) V ectA

(17)

where µ is a diagonal matrix (nm×nm) which diagonal vector is V ect((Λi,j)i=1..m,j=1..n)
sT
and S∗ the matrix (nm × nm) with block diagonals
1..T estimated at iteration
(k + 1). We have ﬁnally the explicit estimation of A:

s1..T

b

(cid:0)

(cid:1)

Ak+1 = Mat

[µ + S∗]−1

b
µV ect(M ) + V ect

b
x1..T (

sk+1
1..T )T

(18)

To show the faisability of this algorithm, we consider in the following a telecom-
munication example. For this, we simulated synthetic data with sources described by a
mixture of 4 Gaussians centered at −3, −1, 1 and 3, with the same variance 0.01 and

b

b

(cid:0)

(cid:2)

(cid:1)(cid:3)(cid:1)

(cid:0)

weighted by 0.3, 0.1, 0.4 and 0.2. The unknown mixing matrix is A =

1 0
0 1

We ﬁxed the a priori parameters of A to: M =

and Λ =

(cid:18)

(cid:19)
meaning that we are nearly sure of diagonal values but we are very uncertain about the
other elements of A. Noise of variance σ2
ǫ = 1 was added to the data. The ﬁgure 1 il-
lustrates the ability of the algorithm to perform the separation. However, we note that
estimated sources are very centered arround the means. This is because we ﬁxed very
low values for the a priori variances of Gaussian mixture. Thus, the algorithm is sensi-
tive to the a priori parameters and exploitation of data is useful. We will see in section
IV how to deal with this issue.

(cid:19)

(cid:18)

1 −0.6
0.6

1
0.009
150

(cid:18)
150
0.009

(cid:19)

.

,

5

2
x

0

5

2
h
S

0

5

2
s

0

−5

−5

5

−5

−5

5

−5

−5

5

0
s1
(a)

0
x1
(b)

0
Sh1
(c)

Figure 1- Results of separation with QAM-16 (Quadratic Amplitude Modulation)
using JMAP algorithm: (a) phase space distribution of sources,
(b) mixed signals, and (c) separated sources

Now, we are going to re-examine closely the expression for the a posteriori distri-
bution of sources. It’s a multi-modal distribution if the Gaussian means aren’t too close.
The maximum of this distribution doesn’t correspond, in general, to the maximum of
the most probable Gaussian. So, we intend to estimate ﬁrst, at each time t, the a priori
Gaussian law according to which the source s(t) is generated (classiﬁcation) and then
estimate s(t) as the mean of the a posteriori Gaussian. This leads us to the introduction
of hidden variables and hierarchical modelization.

HIDDEN VARIABLES

ji). We
The a priori distribution of the component sj is p(sj) =
consider now the hidden variable zj taking its values in the discrete set Zj = (1, . . . , qj)
so each source can belong to one of the qj sources, with αji = p (zj = i). Given zj = i,
sj is normal N (mji, σ2
ji). We can extend this notion to vectorial case by considering the
vector z = [z1, . . . , zn] taking its values in the set Z = Πn
j=1Zj. The s distribution given
z is a normal law p(s|z) = N (mz, Γz) with:

P

qj
i=1 αjiN (mji, σ2

The marginal a priori law of s is the mixture of Πn

j=1qj Gaussians:

mz = [m1z1, m2z2, . . . , mnzn]

Γz = diag(σ2

1z1, σ2

2z2, . . . , σ2

nzn)

p(s) =

p(z)p(s|z)

z∈Z
X

(19)

(20)

(21)

We can re-interpret this mixture by considering it as a discrete set of couples (Nz, p(z))
(see Figure 2). Sources which belong to this class of distributions are generated as
follows: First, generate the hidden variable z ∈ Z according p(z) and then, given this z,

generate s according to Nz. This model can be extended to include continuous values of
z (also continuous distribution f (z)) and then to take account of inﬁnity of distributions
in only one class (see Figure 2).

(N2, p2)

(N1, p1)

(N3, p3)

F

generalize

p(z)

1 2 3

R

p(z)

R

Figure 2- Hierarchical modelization with hidden variables

a posteriori distribution of sources

In the following, we suppose that mixing matrix is known. The joint law of s, z
and x can be factorized in two forms: p(s, z, x) = p(x|s)p(s|z)p(z) or p(s, z, x) =
p(s|x, z)p(z|x)p(x). Thus, the marginal a posteriori law has two forms:

or

p(s|x) =

p(z) p(x|s) p(s|z)
p(x)

p(s|x) =

p(z|x) p(s|x, z)

z∈Z
X

z∈Z
X

We note in the second form that the a posteriori is in the same class that of the a priori
(same expressions but conditionally to x). This is due to the fact that mixture of Gaus-
sians is a conjugate prior for Gaussian likelihood. Our strategy of estimation is based on
this remark: The sources are modeled hierarchically, we estimate them hierarchically;
we begin by estimating the hidden variable using p(z|x) and then estimate sources us-
ing p(s|x, z) which is Gaussian of mean θxz:

θxz = mz + ΓzAtRz(x − Amz)

and variance Vxz:

where,

Vxz = Γz − ΓzAtRzAΓz

Rz = (AΓzAt + Rn)−1

(22)

(23)

(24)

(25)

(26)

and Rn represent the noise covariance.

a posteriori of z and s with respect to s:

Now we have to estimate z by using p(z|x) which is obtained by integrating the joint

p(z|x) =

p(z, s|x)ds ∝ p(z)

p(x|s) p(s|z)ds

(27)

The expression to integrate is Gaussian in s. The result is immediate:

Z

Z

p(z|x) ∝ p(z) | Γz |− 1

2 | Vxz |

2 exp

Kzx

1

(cid:2)

(cid:3)

where:

(cid:26)

Kzx =
Qxz = (I − RzAΓzAt)R−1

− 1

2(Amz − x)tQxz(Amz − x)

n (I − AΓzAtRz) + RzAΓzAtRz

If now we consider the whole observations, the law of z1..T is:

(28)

(29)

p(z1..T |x1..T ) ∝ p(z1..T )

p(x1..T |s1..T ) p(s1..T |z1..T ) ds1..T

(30)

Supposing that z(t) are a priori independant, the last relation becomes:

Z

p(z1..T |x1..T ) ∝ ΠT

t=1

p(z(t))

p(x(t)|s(t)) p(s(t)|z(t)) ds(t)

(31)

(cid:26)
Estimation of z1..T is then performed observation by observation:

Z

(cid:27)

(cid:27)

arg max
z1..T

p(z1..T |x1..T ) =

arg max
z(t)

 

p(z(t)|x(t))

!t=1..T

(32)

Hierarchical JMAP algorithm

Taking into account of this hierarchical model, the JMAP algorithm is implemented

in three steps. At iteration (k):

1. First, estimate the hidden variable

zM AP (combinatary estimation) given observa-

tions and mixing matrix estimated in the previous iteration:

z(k)
M AP (t) = arg max

{p

z(t)|x(t),

A(k−1)

}

(33)

(cid:16)

(cid:17)

2. Second, given the estimated
, Vx

b
) and then the source estimate is θx

z(k)
M AP , source vector s follows Gaussian law

N (θx

z(k)

z(k)

z(k)

.

M AP

M AP

M AP

3. Third, given the estimated sources
b

b

b

rithm of section II.

sk, mixing matrix is evaluated as in the algo-

b

b

b

z(t)

b

We evaluated this algorithm using the same synthetic data as in section 2. Separation
was robust as shown in Figure 3:

2
s

0

2
x

0

2
h
S

5

4

3

2

1

−1

−2

−3

−4

5

4

3

2

1

0

−1

−2

−3

−4

5

4

3

2

1

−1

−2

−3

−4

−5

−5

5

−5

−5

5

−5

−5

5

0
s1

(a)

0
x1

(b)

0
Sh1

(c)

Figure 3- Results of separation with QAM-16
using Hierarchical JMAP algorithm: (a) phase space distribution of sources,
(b) mixed signals, and (c) separated sources

The Bayesian approach allows us to express our a priori information via paramet-
ric prior models. However, in general, we may not know the parameters of the a priori
distributions. This is the task of the next section where we estimate the unknown
hyperparameters always in a Bayesian framework.

HYPERPARAMETERS ESTIMATION

The hyperparameters considered here are the means and the variances of Gaussian
mixture prior of sources: sj ∼
, j = 1, . . . , n. We develop, in
the following, a novel method to extract the hyperparameters from the observations
x1..T . The main idea is: conditioned on the hidden variables (zj)1..T = [zj(1), . . . , zj(T )],
hyperparameters mjz and ψjz for z ∈ Zj = (1, . . . , qj) are means and variances of
a Gaussian distribution. Thus, given the vector (zj)1..T = [zj(1), . . . , zj(T )], we can
perform a partition of the set T = [1, . . . , T ] into sub-sets Tz as:

qj
z=1 ΠjzN

mjz, 1
ψjz

P

(cid:17)

(cid:16)

Tz = { t | zj(t) = z} , z ∈ Zj

(34)

This is the classiﬁcation step.

Suppose now that mixing matrix A and components sl6=j are ﬁxed and we are

interested in the estimation of mjz and ψjz. Let θjz = (mjz , ψjz).

The joint a posteriori law of sj and θjz given zj at time t is:

p(sj, θjz | x, zj) ∝ p(x | s) p(sj | θjz, zj) p(θjz | zj)

(35)

p(sj | θjz, zj) is Gaussian of mean mjz and inverted variance ψjz.
p(θjz | zj) = p(θjz) = p(mjz) p(ψjz) is hyperparameters a priori. The marginal a poste-

riori distribution of θjz is obtained from previous relation by integration over sj:

p(θjz | x, zj) ∝ p(θjz)

p(x | s) p(sj | θjz, zj) dsj.

(36)

sj

Z

The expression inside the integral is proportional to the joint a posteriori distribution of
(sj , zj) given x and θjz, thus:

p(θjz | x, zj) ∝ p(θjz) p(zj | x, θjz).

(37)

where p(zj | x, θjz) is proportional to α
1 / σ2

j and ψjz = 1 / σ2

jz, we have:

′

jz as deﬁned in expression (12). Noting φj =

p(θjz | x, zj) ∝ p(θjz)

φj ψjz
φj + ψjz

s

exp

−

1
2

φj ψjz
φj + ψjz

(mjz − mj)2

(38)

the likelihood is normal

Note that
(φjψjz) / (φj + ψjz).
Choosing a uniform a priori for the means, the estimate of mjz is:

for means mjz and Gamma for λjz =

(cid:2)

(cid:3)

mM AP

jz =

t∈Tz mj(t)
Tz

b

P
For variances, we can choose (i) an inverted Gamma prior G (α, β) after developing the
expression for λjz knowing the relative order of ψjz and φj (to make λjz linear in ψjz) or
(ii) an a prior which is Gamma in λjz. These choices are motivated by two points: First, it
is a proper prior which eliminate degenaracy of some variances at zero (It is shown in [5]
that hyperparameter likelihood (noiseless case without mixing) is unbounded causing a
variance degeneracy at zero). Second, it is a conjugate prior so estimation expressions
remain simple to implement. The estimate of inverted variance (ﬁrst choice when ψjz is
the same order of φj) is:

(39)

(40)

ψM AP

jz =

αposteriori − 1
βposteriori

with αposteriori = α + Tz

b
2 and βposteriori = β +

t∈Tz (mj (t)−

mM AP
jz

)2

.

P

4

b

Hierarchical JMAP including estimation of hyperparameters

Including the estimation of hyperparameters, the proposed hierarchical JMAP algo-

rithm is composed of ﬁve steps:

1. Estimate hidden variables (

zj)M AP

1..T by:

(

zj)M AP

1..T = (arg max

p(zj | x(t), mjz , ψjz, A, sl6=j))1..T

(41)

b

zj

b

which permits to estimate partitions:

Tz =

t | (

zj)M AP (t) = z

(42)

(cid:8)
This corresponds to the classiﬁcation step in the previous algorithm

b

b

2. Given the estimate of partitions, hyperparameters

are updated
according to equations (39) and (25). The following steps are the same as those in
the previous proposed algorithm
3. Re-estimation of hidden variables (
s)M AP
4. Estimation of sources (
1..T .
5. Estimation of mixing matrix (

1..T given the estimated hyperparameters.

A)M AP .

zj)M AP

and

b

b

b

mM AP
jz

(cid:9)
ψM AP
jz

b

b

Simulation results

1 −0.6
0.4

1
(cid:18)
observations is 1000.
Parameters: M =

(cid:19)

and β = 2.

To be able to compare the results obtained by this algorithm and the Penalized
likelihood algorithm developed in the next section with the results obtained by some
other classical methods, we generated data according to the example described in [3].
Data generation: 2-D sources, every component a priori is mixture of two Gaussians
(±1), ψ = 100 for all Gaussians. Original sources are mixed with mixing matrix A =

. A noise of variance σ2

ǫ = 0.03 is added (SNR = 15 dB). Number of

1 0
0 1

, Λ =

(cid:18)

(cid:19)

(cid:18)

150
0.009

0.009
150

, Π =

(cid:19)

(cid:18)

0.5 0.5
0.5 0.5

(cid:19)

, α = 200

Initial conditions: A(0) =

generated according to s(0)

(cid:18)
j ∼

, ψ(0) =

1 0
0 1
qj
z=1 ΠjzN (m(0)

(cid:19)

(cid:18)
jz , 1
ψ(0)
jz

1 1
1 1
).

(cid:19)

, m(0) =

and s(0)

0 0
0 0

(cid:18)

(cid:19)

Sources are recovered with negligible mean quadratic error: MEQ(s1) = 0.0094 and

P

MEQ(s2) = 0.0097. The following ﬁgures illustrate separation results:

The non-negative performance index of [7] is used to chacarterize mixing matrix

identiﬁcation achievement:

ind(S =

A−1 A) =

b

1
2 "

i  

X

j
X

|Sij|2
maxl|Sil|2 − 1

+

!

j  

X

i
X

|Sij|2
maxl|Slj|2 − 1

!#

Figure 7a represents the index evolution through iterations. Note the convergence of
JMAP algorithm since iteration 30 to a satisfactory value of −45 dB. For the same
SNR, algorithms PWS, NS [3] and EASI [6] reach a value greater than −35 dB after
6000 observations. Figures 7b and 7c illustrate the identiﬁcation of hyperparameters. We
note the algorithm convergence to the original values (−1 for m11 and 100 for ψ11).
In order to validate the idea of data classiﬁcation before estimating hyperparameters,

we can visualize the evolution of classiﬁcation error (number of data badly classiﬁed).
Figure 7d shows that this error converges to zero at iteration 15. Then, after this iteration,
hyperparameters identiﬁcation is performed on the true classiﬁed data. Estimation of
mjz and ψjz takes into account only data which belong to this class and then it is not
corrupted by other data which bring erroneous information on these hyperparameters.

s1(t)











s2(t)

x1(t)

x2(t)

s1(t)




b
s2(t)


s1(t) − s1(t)

b

b
s2(t) − s2(t)






b

Figure 4- Separation results with SNR = 15 dB

2

1

2
s

0

−1

−2

−2

2

1

2
x

0

−1

−2

−2

2
h
S

2

1

0

−1

−2

−2

0
s1

2

0
x1

2

0
Sh1

2

Figure 5- Separation results with SNR = 15 dB: Phase space distribution of sources,
mixed signals and separated sources.

300

200

100

200

150

100

50

600

400

200

0
−2

x
e
d
n

i

−25

0

−5

−10

−15

−20

−30

−35

−40

−45

−50

0

120

100

80

1

1

i

s
p

60

40

20

0

0

s1

0
x1

s2

0
x2

0
−2

−1

1

2

0
−2

−1

1

2

0
−4

−2

0
Sh1

2

4

0
−4

−2

0
Sh2

2

4

−1

0

1

2

−1

0

1

2

Figure 6- Separation results with SNR = 15 dB: Histograms of sources,
mixed signals and separated sources.

10

20

40

50

60

10

20

40

50

60

30
iteration

−1

0

30
iteration

Figure 7-a- Evolution of index through iterations

Figure 7-b- Identiﬁcation of m11

10

20

40

50

60

10

20

40

50

60

30
iteration

30
iteration

Figure 7-c- Identiﬁcation of ψ11

Figure 7-d- Evolution of classiﬁcation error

Thus, a joint estimation of sources, mixing matrix and hyperparameters is performed
successfully with a JMAP algorithm. The EM algorithm was used in [4] to solve source

400

300

200

100

200

150

100

50

600

400

200

0
−2

0

1

m

1

−0.5

n
o
i
t
i
t
r
a
P
r
u
e
r
r

E

1000

900

800

700

600

500

400

300

200

100

0

0

separation problem in a maximum likelihood context. We now use the EM algorithm
in a Bayesian approach to take into account of our a priori information on the mixing
matrix.

PENALIZED EM

The EM algorithm has been used extensively in data analysis to ﬁnd the maximum
likelihood estimation of a set of parameters from given data [8]. Considering both the
mixing matrix A and hyperparameters θ, at the same level, being unknown parameters
and complete data x1..T and s1..T . Complete data means jointly observed data x1..T
and unobserved data s1..T . The EM algorithm is executed in two steps: (i) E-step
(expectation) consists in forming the logarithm of the joint distribution of observed
data x and hidden data s conditionally to parameters A and θ and then compute
its expectation conditionally to x and estimated parameters A
(evaluated in
the previous iteration), (ii) M-step (maximization) consists of the maximization of the
obtained functional with respect to the parameters A and θ:

and θ

′

′

1. E-step :

2. M-step :

Q (A, θ | A′, θ′) = Ex,s [log p(x, s | A, θ) | x, A′, θ′]

= arg max
(A, θ)

{Q (A, θ | A′, θ′)}

A,

θ

(cid:16)

(cid:17)

b

b

Recently, in [4], an EM algorithm has been used in source separation with mixture of
Gaussians as sources prior. In this work, we show that:

1. This algorithm fails in estimating variances of Gaussian mixture. We proved that

this is because the degeneracy of the estimated variance to zero.

2. The computational cost of this algorithm is very high.
3. The algorithm is very sensitive to initial conditions.
4. In [4], there’s neither an a priori distribution on the mixing matrix A or on the

hyperparameters θ.

Here, we propose to extend this algorithm in two ways by:

1. Introducing an a priori distribution for θ to eliminate degeneracy and an a priori

distribution for A to express our previous knowledge on the mixing matrix.

2. Taking advantage of our hierarchical model and the idea of classiﬁcation to reduce

the computational cost.

To distinguish the proposed algorithm from the one proposed in [4], we call this algo-
rithm the Penalized EM. The two steps become:

1. E-step :

Q (A, θ | A′, θ′) = Ex,s [log p(x, s | A, θ) + log p(A) + log p(θ) | x, A′, θ′] (45)

(43)

(44)

2. M-step :

= arg max
(A, θ)

Q (A, θ | A′, θ′)

A,

θ

(cid:16)

(cid:17)

b

b

The joint distribution is factorized as: p(x, s, A, θ) = p(x | A, s) p(A) p(s | θ) p(θ).
We can remark that p(x, s, A, θ) as a function of (A, θ) is separable in A and θ. Con-
sequently, the functional is separated into two factors: one representing an A functional
and the other representing a θ functional:

Q (A, θ | A′, θ′) = Qa (A | A′, θ′) + Qh (θ | A′, θ′)

with:

(46)

(47)

(48)

Qa (A | A′, θ′) = E [log p(x | A, s) + log p(A) | x, A′, θ′]
Qh (θ | A′, θ′) =

E [log p(s | θ) + log p(θ) | x, A′, θ′]

(cid:26)

- Maximisation with respect to A

The functional Qa is:

Qa =

−1
2 σ2
ǫ

T

t=1
X

E

(x(t) − A s(t))T (x(t) − A s(t)) | x, A′, θ′
h

i

+ log p(A).

(49)

The gradient of this expression with respect to the elements of A is:

∂Qa
∂Ai,j

=

T
σ2
ǫ

−

1
σ2
aij

(cid:16)

b

i,j

(cid:17)

b

where:

Rxs − A

Rss

(Ai,j − Mi,j) .

(50)

T
t=1 E
T
t=1 E

x(t) s(t)T | x, A′, θ′
s(t) s(t)T | x, A′, θ′

(51)

Evaluation of

(cid:2)
(cid:2)
Rss requires the computation of the expectations of x(t) s(t)T
and s(t) s(t)T . The main computational cost is due to the fact that the expectation of any
function f (s) is given by:
b

P
P

(cid:3)
(cid:3)

b

Rxs = 1
T
Rss = 1
T
b
Rxs and
b

(cid:26)

E [ f (s) | x, A′, θ′] =

E [ f (s) | x, z = z′, A′, θ′] p(z′ | x, A′, θ′).

(52)

z′ ∈

n
i=1 Zi

X
Q
n
j=1 q (j) terms corresponding to the whole combinations of
which involves a sum of
labels. One way to obtain an approximate but fast estimate of this expression is to limit
the summation to only one term corresponding to the MAP estimate of z:

Q

E [ f (s) | x, A′, θ′] = E

f (s) | x, z =

zM AP , A′, θ′

.

(53)

(cid:2)

b

(cid:3)

Then, given estimated labels z1..T , the source s(t) a posteriori law is Normal with mean
θxz and variance Vxz given by (24) and (40).

The source estimate is then θxz.

Rxs and

Rss become:

and

:

(54)

(55)

(57)

(58)

b
x(t)

s(t)T

b
Rxs =

1
T

b

T

t=1
X

b

1
T

T

t=1
X

1
T

T

t=1
X

Rss =

s(t)

s(t)T +

(AtR−1

n A + Γ−1

z )−1

When S1..T estimated and using the matrix operations deﬁned in section II and

cancelling the gradient (50) to zero, we obtain the expression of the estimate of A:

b

b

b

Ak+1 = Mat

Λ + T

R∗
ss

ΛV ect(M) + T V ect

Rxs

(56)

(cid:18)h

b

−1

i

h

b

(cid:17)i(cid:19)

(cid:16)

b

- Maximisation with respect to θ

With a uniform a priori for the means, maximisation of Qh with respect to mjz gives

mjz =

T

t=1 θjz(t) p(z(t) | x, A′, θ′)
t=1 p(z(t) | x, A′, θ′)

T

P

With an Inverted Gamma prior G (α, β) (α > 0 et β > 1) for the variances, the

P

b

maximisation of Qh with respect to σjz gives:

2 β +

T
t=1

P

P

σjz =

b

jz − 2

Vjz + θ2
mjzθjz +
T
t=1 p(z(t) | x, A′, θ′) + 2 (α − 1)

m2
jz

(cid:1)

(cid:0)

b

b

p(z(t) | x, A′, θ′)

Summary of the Penalized EM algorithm

Based on the preceeding equations, we propose the following algorithm to estimate

sources and parameters using the following ﬁve steps:

1. Estimate the hyperparameters according to (57) and (58).
2. Update of data classiﬁcation by estimating
3. Given this classiﬁcation, sources estimate is the mean of the Gaussian a posteriori

zM AP
1..T .

law (39).

b

4. Update of data classiﬁcation.
5. Estimate the mixing matrix A according to the re-estimation equation (56).

COMPARISON WITH JMAP ALGORITHM AND ITS
SENSITIVITY TO INITIAL CONDITIONS

The Penalized EM algorithm has an optimization cost approximately 2 times higher,
per sample, than the JMAP algorithm. However, both algorithms have a reasonable
computational complexity, linearly increasing with the number of samples. Sensitivity
to initial conditions is inherent to the EM-algorithm even to the penalized version. In
order to illustrate this fact, we simulated the algorithm with the same parameters as

in section IV. Note that initial conditions for hyperparameters are ψ(0) =

1 1
1 1

(cid:18)

(cid:19)

and

. However, the Penalized EM algorithm fails in separating sources (see

m(0) =

0 0
0 0

(cid:18)

(cid:19)

ﬁgure 11). We note then that JMAP algorithm is more robust to initial conditions.

2
s

0

2
x

0

0
s1

(a)

0
x1

(b)

0
Sh1

(c)

2

−2

−2

2

−2

−2

2

Figure 11- Results of separation with the Penalized EM algorithm:
(a) Phase space distribution of sources,
(b) mixed signals and (c) separated sources

We modiﬁed the initial condition to have means: m(0) =

. We noted, in

this case, the convergence of the Penalized EM algorithm to the correct solution. Figures
12-16 illustrate the separation results:

−0.5 0.5
−0.5 0.5

(cid:18)

(cid:19)

2
s

0

2
x

0

0
s1

(a)

0
x1

(b)

0
Sh1

(c)

2

−2

−2

2

−2

−2

2

Figure 12- Results of separation with the Penalized EM algorithm:
(a) Phase space distribution of sources,
(b) mixed signals and (c) separated sources

1.5

2

1

0.5

2
h
S

0

−0.5

−1

−1.5

1.5

2

1

0.5

2
h
S

0

−0.5

−1

−1.5

1.5

2

1

0.5

−0.5

−1

−1.5

−2

−2

1.5

2

1

0.5

−0.5

−1

−1.5

−2

−2

1.5

2

1

0.5

−0.5

−1

−1.5

1.5

2

1

0.5

−0.5

−1

−1.5

10

20

40

50

10

20

40

50

60

60

−60

0

30
iteration

30
iteration

Figure 13- Evolution of classiﬁcation error

Figure 14- Evolution of index

n
o
i
t
i
t
r
a
P
r
u
e
r
r

E

18

16

14

12

10

8

6

4

2

0

0

−0.6

−0.65

−0.7

−0.75

1

m

1

−0.8

−0.85

−0.9

−0.95

−1

0

0

−10

−20

−40

−50

x
e
d
n

i

−30

1

1

i

s
p

60

100

90

80

70

50

40

30

20

0

10

20

40

50

60

10

20

40

50

60

30
iteration

30
iteration

Figure 15- Identiﬁcation of m11

Figure 16- Identiﬁcation of ψ11

CONCLUSION

We have proposed solutions to source separation problem using a Bayesian framework.
Speciﬁc aspects of the described approach include:

• Taking account of errors on model and measurements.
• Introduction of a priori distribution for the mixing matrix and hyperparameters.
This was motivated by two different reasons: Mixing matrix prior should exploit
previous information and variances prior should regularize the log-posterior objec-
tive function.

We then consider the problem in terms of a mixture of Gaussian priors to develop a
hierarchical strategy for source estimation. This same interpretation leads us to classify
data before estimating hyperparameters and to reduce computational cost in the case of
the proposed Penalized EM algorithm.

REFERENCES

1. E. Moulines, J. Cardoso, and E. Gassiat, “Maximum likelihood for blind separation and deconvolution

of noisy signals using mixture models”, in ICASSP-97, München, Germany, April 1997.

2. A. Mohammad-Djafari, “A Bayesian approach to source separation”, in MaxEnt99 Proceedings. 1999,

3. O. Macchi and E. Moreau, “Adaptative unsupervised separation of discrete sources”,

in Signal

Kluwer.

Processing 73, 1999, pp. 49–66.

4. O. Bermond, Méthodes statistiques pour la séparation de sources, PhD thesis, Ecole Nationale

Supérieure des Télécommunications, January 2000.

5. A. Ridolﬁ and J. Idier, “Penalized maximum likelihood estimation for univariate normal mixture
distributions”, in Actes du 17e colloque GRETSI, Vannes, France, September 1999, pp. 259–262.
6. J. Cardoso and B. Labeld, “Equivariant adaptative source separation”, Signal Processing, vol. 44, pp.

7. E. Moreau and O. Macchi, “High-order contrasts for self-adaptative source separation”, in Adaptative

8. R. A. Redner and H. F. Walker, “Mixture densities, maximum likelihood and the EM algorithm”, SIAM

3017–3030, 1996.

Control Signal Process. 10, 1996, pp. 19–46.

Rev., vol. 26, no. 2, pp. 195–239, April 1984.


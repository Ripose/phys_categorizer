December 13, 2013 2:27 WSPC/INSTRUCTION FILE

d0sar-grid

5
0
0
2
 
n
a
J
 
1
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
6
1
1
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

International Journal of Modern Physics A
c(cid:13) World Scientiﬁc Publishing Company

Performance of an Operating High Energy Physics Data Grid:
DØSAR-Grid

B. Abbott,a P. Baringer,b T. Bolton,c Z. Greenwood,d E. Gregores,e H. Kim,f C. Leangsuksun,d
D. Meyer,f N. Mondal,g S. Novaes,e B. Quinn,h H. Severini∗,,a P. Skubic,a J. Snow,i
M. Sosebee,f J. Yu f
aUniversity of Oklahoma, Norman, OK 73019, USA
bUniversity of Kansas, Lawrence, KS 66045, USA
cKansas State University, Manhattan, KS 66506, USA
dLouisiana Tech University, Ruston, LA 71272, USA
eUniversidade Estadual Paulista, Sao Paulo, Brazil
f University of Texas at Arlington, Arlington, TX 76019, USA
gTata Institute of Fundamental Research, Bombay, India
hThe University of Mississippi, University, MS 38677, USA
iLangston University, Langston, OK 73050, USA

Received (Day Month Year)
Revised (Day Month Year)

The DØ experiment at Fermilab’s Tevatron will record several petabytes of data over
the next ﬁve years in pursuing the goals of understanding nature and searching for the
origin of mass. Computing resources required to analyze these data far exceed capabilities
of any one institution. Moreover, the widely scattered geographical distribution of DØ
collaborators poses further serious diﬃculties for optimal use of human and computing
resources. These diﬃculties will exacerbate in future high energy physics experiments, like
the LHC. The computing grid has long been recognized as a solution to these problems.
This technology is being made a more immediate reality to end users in DØ by developing
a grid in the DØ Southern Analysis Region (DØSAR), DØSAR-Grid, using all available
resources within it and a home-grown local task manager, McFarm. We will present
the architecture in which the DØSAR-Grid is implemented, the use of technology and
the functionality of the grid, and the experience from operating the grid in simulation,
reprocessing and data analyses for a currently running HEP experiment.

1. Introduction

Particle physicists employ high energy particle accelerators and complex detectors
to probe ever smaller distance scales in an attempt to understand the nature and
origin of the universe. The Tevatron proton-anti proton collider located in the De-
partment of Energy’s Fermi National Accelerator Laboratory,1 Batavia, Illinois,
currently operates at the ”energy frontier” and has an opportunity of providing new

∗Corresponding
severini@ou.edu.

author. Tel.: +1-405-325-3961x36415; Fax: +1-405-325-7557; E-mail:

1

December 13, 2013 2:27 WSPC/INSTRUCTION FILE

d0sar-grid

2 DØSAR

discoveries through the DØ2 and CDF3 high energy physics (HEP) experiments.
Future experiments, such as ATLAS4 at the Large Hadron Collider (LHC)5 in the
European Organization for Nuclear Research (CERN)6 and a proposed electron-
positron linear collider (LC)7 have the prospects of either building on discoveries
made at Fermilab, or making the discoveries themselves if nature has placed these
new processes beyond the energy reach of the Tevatron.

2. DØSAR

As part of the DØ experiment’s eﬀort to utilize grid technology for the expeditious
analysis of data, twelve universities have formed a regional virtual organization
(VO), the DØ Southern Analysis Region (DØSAR).8 The centerpiece of DØSAR is a
data and resource hub called a Regional Analysis Center (RAC),9 constructed at the
University of Texas at Arlington with the support of NSF MRI funds. Each DØSAR
member institution constructs an Institutional Analysis Center (IAC), which acts as
a gateway to other RACs and to the grid for the users within that institution. These
IACs combine dedicated rack-mounted servers and personal desktop computers into
a local physics analysis cluster. The data access system for DØ oﬄine analyses is
managed by a database and cataloging system called Sequential Data Access via
Metadata, or SAM.10 The MC Runjob package11 provides a low-level MC manager
that coordinates data ﬁles used by and produced with the executables of the MC
chain, via scripts produced for the job. This package is combined with MC task
management software called McFarm12, developed at UTA, for automated, highly
eﬃcient MC production.

3. DØSAR-Grid

In order to pursue full utilization of grid concepts, we are establishing an oper-
ational regional grid called DØSAR-Grid using all available resources, including
personal desktop computers and large dedicated computer clusters. We have con-
structed and are operating DØSAR-Grid utilizing a framework called SAM-Grid13
being developed at Fermilab. DØSAR-Grid will subsequently be made interopera-
ble under other grid frameworks such as LCG,14 TeraGrid,15 Grid3,16 and Open
Science Grid.17 Wherever possible, we will exploit existing software and technolog-
ical advances made by these other grid projects. We plan to develop and implement
tools to support easy and eﬃcient user access to the grid and to ensure its robust-
ness. Tools to transfer binary executables and libraries eﬃciently across the grid
for environment-independent execution will be developed. DØSAR will implement
the Grid for critical physics data analyses, while at the same time subjecting grid
computing concepts to a stress test to its true conceptual level, down to personal
desktop computers. Many of the proponents of this project are members of the LHC
ATLAS4 experiment and a future experiment under study by the American Linear
Collider Physics Group,18 and will apply the experience gained from DØSAR to
these projects.

December 13, 2013 2:27 WSPC/INSTRUCTION FILE

d0sar-grid

Performance of an Operating High Energy Physics Data Grid: DØSAR-Grid 3

4. Conclusions

Experimental particle physics has always pushed information technology in direc-
tions that have had profound broader impact, with the most spectacular example
being CERN’s development of the World Wide Web, originally intended to fa-
cilitate communication among collaborating physicists spread around the globe.
DØSAR’s goal of eﬃciently using computing resources scattered across the south-
central United States for tasks such as complex data reduction and simulation will
produce grid computing solutions with broad applicability. This project will also
likely aﬀect how we disseminate and analyze data from future experiments at fa-
cilities such as the LHC. DØSAR will help improve the cyber-infrastructure within
the region; ﬁve participating institutions are located in states that are traditionally
under-funded for R&D activities. In addition to technology development, there will
also be signiﬁcant development of human resources in the region, delivering inter-
disciplinary training in physics and computer science. Information for the broader
public will be disseminated on the web. Finally, DØSAR will realize the conceptual
grid to the level of personal desktop computers, demonstrating the performance of
a grid at its fullest level. The DØSAR Grid will be exploited in critical physics
analyses of real and simulated data from the DØ experiment, and later from LHC
and other future experiments. These data will serve to advance our understanding
of nature at the smallest distance scales.

References

1. Fermilab National Accelerator Laboratory, http://www.fnal.gov/.
2. The DØ Experiment, http://www-d0.fnal.gov/.
3. The CDF Experiment, http://www-cdf.fnal.gov/.
4. The ATLAS (A Toroidal LHC ApparatuS) Experiment, http://www.cern.ch/Atlas/.
5. The Large Hadron Collider (LHC) project, http://www.cern.ch/lhc/.
6. European Organization for Nuclear Research (CERN), http://www.cern.ch/.
7. The

Development Working

Research

Collider

Linear

and

Group,

http://www.hep.uiuc.edu/LCRD/.
Southern

DØ

8. The

http://www-hep.uta.edu/d0-sar/d0-sar.html.

Analysis

Region

(DØSAR),

9. I. Bertram, R. Brock, F. Filthaut, L. Lueking, P. Mattig, M. Narain, P. Lebrun, B.
Thooris,
Zeit-
njitz, ”Proposal for DØ Regional Analysis Centers,” DØ Note #3984, unpublished
(2002), http://www-d0.fnal.gov/computing/D0Race/d0rac-wg/d0rac-ﬁnal.pdf.

Yu,

C.

J.

10. The

Sequential

Data

Access

via

Metadata

(SAM)

project,

http://www-d0.fnal.gov/computing/sam/.

11. MC Runjob Home Page, http://www-clued0.fnal.gov/runjob/.
12. McFarm Home Page, http://hepfm000.uta.edu/documentation.html.
13. The DØ Grid Group, http://www-d0.fnal.gov/computing/grid/.
14. The LHC Computing Grid Project, http://www.cern.ch/LCG/.
15. The TeraGrid, http://www.teragrid.org/.
16. The Grid3 project, http://www.ivdgl.org/grid2003/.
17. Open Science Grid, http://www.opensciencegrid.org/.
18. American Linear Collider Group (ALCPG), http://blueox.uoregon.edu/∼lc/alcpg/.


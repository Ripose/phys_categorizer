9
9
9
1
 
v
o
N
 
0
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
7
0
1
1
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Mixtures of Gaussian process priors∗

J¨org C. Lemm
Institut f¨ur Theoretische Physik I, Universit¨at M¨unster
D–48149 M¨unster, Germany
E-mail: lemm@uni-muenster.de
http://pauli.uni-muenster.de/∼lemm
Publication No.: MS-TP1-99-5

Abstract

Nonparametric Bayesian approaches based on Gaussian processes have recently become
popular in the empirical learning community. They encompass many classical methods
of statistics, like Radial Basis Functions or various splines, and are technically convenient
because Gaussian integrals can be calculated analytically. Restricting to Gaussian processes,
however, forbids for example the implemention of genuine nonconcave priors. Mixtures of
Gaussian process priors, on the other hand, allow the ﬂexible implementation of complex
and situation speciﬁc, also nonconcave a priori information. This is essential for tasks
with, compared to their complexity, a small number of available training data. The paper
concentrates on the formalism for Gaussian regression problems where prior mixture models
provide a generalisation of classical quadratic, typically smoothness related, regularisation
approaches being more ﬂexible without having a much larger computational complexity.

Contents

1 Introduction

2 The Bayesian model

3 Gaussian regression

4 Prior mixtures

5 A numerical example

6 Conclusions

1

Introduction

4.1 General formalism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Maximum a posteriori approximation . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Analytical solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 High and low temperature limits
. . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Equal covariances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

The generalisation behaviour of statistical learning algorithms relies essentially on the correctness
of the implemented a priori information. While Gaussian processes and the related regularisation
approaches have, on one hand, the very important advantage of being able to formulate a priori
∗This is an extended version of a contribution to the Ninth International Conference on Artiﬁcial Neural

Networks (ICANN 99), 7–10 September 1999, Edinburgh, UK.

1

1

2

3

4
4
5
6
7
7

9

9

information explicitly in terms of the function of interest (mainly in the form of smoothness
priors which have a long tradition in density estimation and regression problems [18, 17, 5]) they
implement, on the other hand, only simple concave prior densities corresponding to quadratic
errors. Especially complex tasks would require typically more general prior densities. Choosing
mixtures of Gaussian process priors combines the advantage of an explicit formulation of priors
with the possibility of constructing general non-concave prior densities.

While mixtures of Gaussian processes are technically a relatively straightforward extension
of Gaussian processes, which turns out to be a computational advantage, practically they are
much more ﬂexible and are able to produce in principle, i.e., in the limit of inﬁnite number of
components, any arbitrary prior density.

As example, consider an image completion task, where an image have to be completed,
given a subset of pixels (‘training data’). Simply requiring smoothness of grey level values
would obviously not be suﬃcient if we expect, say, the image of a face. In that case the prior
density should reﬂect that a face has speciﬁc constituents (e.g., eyes, mouth, nose) and relations
(e.g., typical distances between eyes) which may appear in various variations (scaled, translated,
deformed, varying lightening conditions).

While ways how prior mixtures can be used in such situations have already been outlined in
[6, 7, 8, 9, 10] this paper concentrates on the general formalism and technical aspects of mixture
models and aims in showing their computational feasibility. Sections 2–4 provide the necessary
formulae while Section 5 exempliﬁes the approach for an image completion task.

Finally, we remark that mixtures of Gaussian process priors do usually not result in a (ﬁnite)
mixture of Gaussians [3] for the function of interest. Indeed, in density estimation, for example,
arbitrary densities not restricted to a (ﬁnite) mixture of Gaussians can be produced by a mixture
of Gaussian prior processes.

2 The Bayesian model

Let us consider the following random variables:

1. x, representing (a vector of) independent, visible variables (‘measurement situations’),

2. y, being (a vector of) dependent, visible variables (‘measurement results’), and

3. h, being the hidden variables (‘possible states of Nature’).

A Bayesian approach is based on two model inputs [1, 11, 4, 12]:

1. A likelihood model p(y|x, h), describing the density of observing y given x and h. Regarded
as function of h, for ﬁxed y and x, the density p(y|x, h) is also known as the (x–conditional)
likelihood of h.

2. A prior model p(h|D0), specifying the a priori density of h given some a priori information

denoted by D0 (but before training data DT have been taken into account).

Furthermore, to decompose a possibly complicated prior density into simpler components,
we introduce continuous hyperparameters θ and discrete hyperparameters j (extending the set of
hidden variables to ˜h = (h, θ, j)),

p(h|D0) =

dθ

p(h, θ, j|D0).

Z

j
X

In the following, the summation over j will be treated exactly, while the θ–integral will be
approximated. A Bayesian approach aims in calculating the predictive density for outcomes y in
test situations x

p(y|x, D) =

dh p(y|x, h) p(h|D),

given data D = {DT , D0} consisting of a priori data D0 and i.i.d. training data DT = {(xi, yi)|1 ≤
i ≤ n}. The vector of all xi (yi) will be denoted xT (yT ). Fig.1 shows a graphical representation
of the considered probabilistic model.

(1)

(2)

Z

2

test data

training data DT
(cid:15)(cid:12)· · ·
x1
(cid:14)(cid:13)
?
(cid:15)(cid:12)· · ·
y1
(cid:14)(cid:13)

XXXXXXy

(cid:15)(cid:12) x
xn
(cid:14)(cid:13)
?
(cid:15)(cid:12) y
yn
(cid:14)(cid:13)
@@I (cid:0)(cid:0)(cid:18)
(β), h

?

prior data
(cid:15)(cid:12)
D0
(cid:14)(cid:13)

-

6
θ, j

˜h

Figure 1: Graphical representation of the considered probabilistic model, factorising accord-
ing to p(xT , yT , x, y, h, θ, j, (β)|D) = p(xT ) p(x) p(yT |xT , h, (β)) p(y|x, h, (β)) p(h|θ, j, D0, (β))
p(θ, j, (β)|D0). (The variable β is introduced in Section 3.) Circles indicate visible variables.

In saddle point approximation (maximum a posteriori approximation) the h–integral becomes

p(y|x, D) ≈ p(y|x, h∗),

h∗ = argmaxh∈Hp(h|D),
assuming p(y|x, h) to be slowly varying at the stationary point. The posterior density is related
to (xT –conditional) likelihood and prior according to Bayes’ theorem

(4)

p(h|D) =

p(yT |xT , h) p(h|D0)
p(yT |xT , D0)

,

where the h–independent denominator (evidence) can be skipped when maximising with respect
to h. Treating the θ–integral within p(h|D) also in saddle point approximation the posterior
must be maximised with respect to h and θ simultaneously .

3 Gaussian regression

In general density estimation problems p(yi|xi, h) is not restricted to a special form, provided
it is non–negative and normalised [9, 10]. In this paper we concentrate on Gaussian regression
where the single data likelihoods are assumed to be Gaussians

p(yi|xi, h) =

e− β

2 (h(xi)−yi)

2

.

β
2π

r

In that case the unknown regression function h(x) represents the hidden variables and h–
integration means functional integration

dh →

x dh(x).

As simple building blocks for mixture priors we choose Gaussian (process) prior components

[2, 17, 14],

R

R Q

d
2

p(h|β, θ, j, D0) =

β
2π
2 (h − tj(θ), Kj(θ)(h − tj(θ)))

(cid:18)

(cid:19)

(det Kj(θ))

1
2

×e− β

(3)

(5)

(6)

(7)

the scalar product notation (·, ·) standing for x–integration. The mean tj(θ)(x) will in the
following also be called an (adaptive) template function. Covariances K−1
j /β are real, symmetric,
positive (semi–)deﬁnite (for positive semideﬁnite covariances the null space has to be projected
out). The dimension d of the h–integral becomes inﬁnite for an inﬁnite number of x–values (e.g.

3

continuous x). The inﬁnite factors appearing thus in numerator and denominator of (5) however
cancel. Common smoothness priors have tj(θ) = 0 and as Kj a diﬀerential operator, e.g., the
negative Laplacian.

Analogously to simulated annealing it will appear to be very useful to vary the ‘inverse
temperature’ β simultaneously in (6) (for training but not necessarily for test data) and (7).
Treating β not as a ﬁxed variable, but including it explicitly as hidden variable, the formulae of
Sect. 2 remain valid, provided the replacement h → (h, β) is made, e.g. p(yi|xi, h) → p(yi|xi, h, β)
(see also Fig.1).

Typically, inverse prior covariances can be related to approximate symmetries. For example,
assume we expect the regression function to be approximately invariant under a permutation of
its arguments h(x) ≈ h(σ(x)) with σ denoting a permutation. Deﬁning an operator S acting on
h according to Sh(x) = h(σ(x)), we can deﬁne a prior process with inverse covariance

K = (I − S)T (I − S),

with identity I and the superscript T denoting the transpose of an operator. The corresponding
prior energy

E0 =

(h, K h) =

(h − S)h, (h − S)h

,

1
2

(cid:16)
is a measure of the deviation of h from an exact symmetry under S. Similarly, we can consider
a Lie group S = eθs with s being the generator of the inﬁnitesimal symmetry transformation.
In that case a covariance

(cid:17)

with prior energy

K =

1
θ2 (I − Sinf )T (I − Sinf ) = sT s,

E0 =

(sh, sh) ,

1
2

1
2

can be used to implement approximate invariance under the inﬁnitesimal symmetry transfor-
mation Sinf = I + θs. For appropriate boundary conditions, a negative Laplacian K can thus
be interpreted as enforcing approximate invariance under inﬁnitesimal translations, i.e., for s =
∂/∂x.

4 Prior mixtures

4.1 General formalism

Decomposed into components the posterior density becomes

p(h, β|D) ∝

dθ

p(yT |xT , h, β)

(12)

m

Z

j
X

× p(h|β, θ, j, D0) p(β, θ, j|D0).

Writing probabilities in terms of energies, including parameter dependent normalisation factors
and skipping parameter independent factors yields

(8)

(9)

(10)

(11)

(13)

p(yT |xT , h, β) ∝ e−βET + n
p(h|β, θ, j, D0) = e−βE0,j + d

2 ln β

2 ln β

1
2 ln det Kj (θ)

×e

p(β, θ, j|D0) ∝ e−Eθ,β,j .

This deﬁnes hyperprior energies Eθ,β,j, prior energies E0,j (‘quadratic concepts’)

E0,j =

h − tj(θ), Kj(θ)(h − tj(θ, j))

,

(14)

1
2

(cid:16)

(cid:17)

4

(the generalisation to a sum of quadratic terms E0,j =
or likelihood energy (training error)

k E0,k,j is straightforward) and training

1
2  
(cid:16)
The second line is a ‘bias–variance’ decomposition where

h − tT , KT(h − tT )

+

=

(cid:17)

n

i
X

VT (xi)

.

!

is the mean of the nxi training data available for xi, and

P

ET =

(h(xi) − yi)2

1
2

n

i
X

tT (xi) =

nxi

k
X

yk(x)
nxi

,

VT (xi) =

− t2

T (xi),

nxi

k
X

y2
k(x)
nxi

is the variance of yi values at xi. (Vi vanishes if every xi appears only once.) The diagonal
matrix KT is restricted to the space of x for which training data are available and has matrix
elements nx.

4.2 Maximum a posteriori approximation

In general density estimation the predictive density can only be calculated approximately, e.g.
in maximum a posteriori approximation or by Monte Carlo methods. For Gaussian regression,
however the predictive density of mixture models can be calculated exactly for given θ (and
β). This provides us with the opportunity to compare the simultaneous maximum posterior
approximation with respect to h and θ with an analytical h–integration followed by a maximum
posterior approximation with respect to θ.

Maximising the posterior (with respect to h, θ, and possibly β) is equivalent to minimising

the mixture energy (regularised error functional [13, 17, 15, 16])

E = − ln

e−Ej+cj ,

m

j
X

with component energies

and

Ej = βEh,j + Eθ,β,j, Eh,j = ET + E0,j,

cj(θ, β) =

ln det Kj(θ) +

ln β.

1
2

d + n
2

In a direct saddle point approximation with respect to h and θ stationarity equations are

obtained by setting the (functional) derivatives with respect to h and θ to zero,

where the derivatives with respect to θ are matrices if θ is a vector,

m

j
X
m

(cid:16)

0 =

aj

KT (h − tT ) +Kj(h − tj)

,

0 =

j
X

∂Ej
∂θ

aj

 

− Tr

K−1
j

∂Kj
∂θ

(cid:18)

(cid:17)

,
(cid:19) !

aj = p(j|h, θ, D0)

=

e−βE0,j −Eθ,β,j + 1
k e−βE0,k−Eθ,β,k+ 1
m

2 ln det Kj

2 ln det Kk

,

P

5

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

and

with

and

where

Eq.(21) can be rewritten

∂Ej
∂θ

=

∂Eθ,β,j
∂θ

+ β

, Kj(tj − h)

(cid:19)

(h − tj),

(h − tj)

.

+

β
2

(cid:16)

h = K−1
a

KT tT +

ajKjtj

,

∂tj
∂θ
(cid:18)
∂Kj
∂θ

m

l
X

m

j
X

(cid:17)

!





Ka =

KT +

ajKj

.

 





Due to the presence of h–dependent factors aj, Eq.(25) is still a nonlinear equation for h(x).
For the sake of simplicity we assumed a ﬁxed β; it is no problem however to solve (21) and (22)
simultaneously with an analogous stationarity equation for β.

4.3 Analytical solution

The optimal regression function under squared–error loss — for Gaussian regression identical to
the log–loss of density estimation — is the predictive mean. For mixture model (12) one ﬁnds,
say for ﬁxed β,

with mixture coeﬃcients

¯y =

dy y p(y|x, D) =

dθ bj(θ) ¯tj(θ),

Z

j Z

X

bj(θ) = p(θ, j|D)

=

p(θ, j) p(yT |xT , D0, θ, j)

dθp(θ, j) p(yT |xT , D0, θ, j)

j

.

¯tj = (KT + Kj)
= tj + K−1

j

Kj(tT − tj),

−1 (KT tT + Kjtj)

p(yT |xT , D0, θ, j) = e−β

E0,j+ 1

2 ln det( β
2π

Kj ),

e

The component means ¯tj and the likelihood of θ can be calculated analytically [17, 14]

P

R

e

1
2

e

,

E0,j(θ) =
tT − tj,
Kj(tT − tj)
T + K−1
Kj(θ) = (K−1
j,T T )−1,
e
e
j,T T is the projection of the covariance K−1
e

(cid:0)

j

(cid:1)

and K−1
training data are available. (˜n ≤ n is the number of data with distinct x–values.)

into the ˜n–dimensional space for which

The stationarity equation for a maximum a posteriori approximation with respect to θ is at

this stage found from (28,30)

0 =

bj

j
X

∂
Ej
∂θ
e

 

− Tr

K−1
j

 

e

∂
Kj
∂θ !!
e

,

Ej = β
where
K. The coeﬃcient b∗
with the ˜n × ˜n–matrix
is of form (23) with the replacements Kj →

E0,j + Eθ,β,j. Notice that Eq.(33) diﬀers from Eq.(22) and requires only to deal
j = bj(θ∗) for θ set to its maximum posterior value
Kj, Ej →

Ej .

e

e

e

e

6

e

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

4.4 High and low temperature limits

Low and high temperature limits are extremely useful because in both cases the stationarity
Eq.(21) becomes linear, corresponding thus to classical quadratic regularisation approaches.
In the high temperature limit β → 0 the exponential factors aj become h–independent

β→0
−→ a0

j =

aj

e−Eθ,β,j + 1
k e−Eθ,β,k+ 1
m

2 ln det Kj

2 ln det Kk

,

e
¯t = K−1
a0

KTtT +

 

a0
j Kjtj

,

!

Ka0 = KT +

a0
j Kj.

m

l
X

j
X

j → b0,∗

j

(for b∗
average’

replace Kj by

Kj). The solution h = ¯t is a (generalised) ‘complete template

P

with

P

This high temperature solution corresponds to the minimum of the quadratic functional ET +

j Eh,j,

m
j a0
In the low temperature limit β → ∞ only the maximal component contributes, i.e.,

β→∞
−→ a∞

j =

aj

1 :
0 :

j = argminjEh,j
j 6= argminjEh,j

,

(cid:26)

j replace Eh,j by

(for b∗
Ej) assuming Eβ,θ,j = Eβ + Eθ,j or Eβ,θ,j = Eβ + Ej +βEθ. Hence,
low temperature solutions h = ¯tj, are all (generalised) ‘component averages’ ¯tj provided they
fulﬁl the stability condition

e

Eh,j(h = ¯tj) < Eh,j′ (h = ¯tj),

∀j′ 6= j,

or, after performing a (generalised) ‘bias–variance’ decomposition, 2Vj < Bj′ (j, j) + 2Vj′ , with
m × m matrices

¯tk − ¯tj, (KD + Kj) (¯tl − ¯tj)
(cid:16)
(cid:17)
and (generalised) ‘template variances’

Bj(k, l) =

Vj =

tT , KT tT

+

tj, Kj tj

(cid:17)

(cid:16)

(cid:17)

−

¯tj, (KT + Kj) ¯tj

=

E0,j.

1
2  

(cid:16)

(cid:16)

!

(cid:17)

e

That means single component averages ¯tj (which minimise Eh,j and thus −βEj + cj) become so-
lutions at zero temperature 1/β in case their (generalised) variance Vj measuring the discrepancy
between data and prior term is small enough.

4.5 Equal covariances

Especially interesting are j–independent Kj(θ) = K0(θ) with θ–independent determinants so
det Kj or det

Kj, respectively, do not have to be calculated.

e

Notice that this still allows completely arbitrary parameterisations of tj(θ). Thus, the tem-
plate function can for example be a parameterised model, e.g., a neural network or decision tree,
and maximising the posterior with respect to θ corresponds to training that model.
In such
cases the prior term forces the maximum posterior solution h to be similar (as deﬁned by K0)
to this trained parameterised reference model.

The condition of invariant det K0(θ) does not exclude adaption of covariances. For example,
transformations for real, symmetric positive deﬁnite K0(θ) leaving determinant and eigenval-
ues (but not eigenvectors) invariant are of the form K(θ0) → K(θ) = O(θ)KO−1(θ) with real,

7

Figure 2: Left: Example of a solution space for m = 3. Shown are three low temperature
solutions ¯tj, high temperature solution ¯t, and a possible solution h at ﬁnite β. Right: Exact b1
vs. (dominant) a1 (dashed) for m = 2, b = 2,

E1 = 0.405,

E2 = 0.605.

β

2

4

6

8

10

12

14

¯t1
r
(cid:0)
@
¯t
h
(cid:0)(cid:0)
r
b
r¯t2

@@
r¯t3

0.9

0.8

0.7

0.6

0.5

a1

b1

e

e

1

0.75

0.5

0.25

0

0
0

4

3

β

2

0.25
0.25

0.5
0.5

a1

0.75
0.75

1

0

1
1

Figure 3: Shown are the plots of f1(a1) = a1 and f2(a1) = 1
temperature range 0 ≤ β ≤ 4 (for b = 2,
stable solution at low temperatures.

E2 −

2 (tanh ∆ + 1) within the inverse
E1 = 0.1β). Notice the appearance of a second

e
orthogonal O−1 = OT . This allows for example to adapt the sensible directions of multidimen-
sional Gaussians. A second kind of transformations changing eigenvalues but not eigenvectors
and determinant is of the form K(θ0) = OD(θ0)OT → K(θ) = OD(θ)OT if the product of
eigenvalues of the real, diagonal D(θ0) and D(θ) are equal.

e

Eqs.(29,35) show that the high temperature solution becomes a linear combination of the

(potential) low temperature solutions

Similarly, Eq.(21) simpliﬁes to

and Eq.(23) to

m

m

¯t =

a0
j ¯tj =

b0,∗
j

¯tj.

j
X

j
X

h =

aj ¯tj = ¯t +

(aj − a0

j ) ¯tj ,

m

j
X

aj =

2 aBj a−

Ej

e− β
k e− β

2 aBka−
e

Ek

2 aBj a

bj e− β
k bk e− β

2 aBka

,

m

j
X

=

introducing vector a with components aj, m × m matrices Bj deﬁned in (39). Eq.(42) is still a
e
nonlinear equation for h, it shows however that the solutions must be convex combinations of the
h–independent ¯tj (see Fig. 2). Thus, it is suﬃcient to solve Eq.(43) for m mixture coeﬃcients
aj instead of Eq.(21) for the function h.

P

P

For two prior components, i.e., m = 2, Eq.(42) becomes

with

h =

¯t1 + ¯t2
2

+ (tanh ∆)

¯t1 − ¯t2
2

,

∆ =

E2 − E1
2

β
4

=

b(2a1 − 1) +

E2 −
2

E1

,

e

e

8

(41)

(42)

(43)

(44)

(45)

Figure 4: Top row, from left to right: Data points sampled with Gaussian noise, two template
functions t1, t2. Bottom row, from left to right: Original, reconstructed solutions (regression
function h, 180×240 pixels) at low and at high temperature.

because the matrices Bj are in this case zero except B1(2, 2) = B2(1, 1) = b. For Eθ,β,j uniform
in j we have (¯t1 + ¯t2)/2 = ¯t so that a0
j = 0.5. The stationarity Eq.(43), being analogous to the
celebrated mean ﬁeld equation of a ferromagnet, can be solved graphically (see Fig.3 and Fig.2
for a comparison with bj), the solution is given by the point where

a1 =

(tanh ∆ + 1) .

1
2

(46)

5 A numerical example

As numerical example we study a two component mixture model for image completion. Assume
we expect an only partially known image (corresponding to pixel-wise training data drawn
with Gaussian noise from the original image) to be similar to one of the two template images
shown in Fig.4. Next, we include hyperparameters parameterising deformations of templates. In
particular, we have chosen translations (θ1, θ2) a scaling factor θ3, and a rotation angle (around
template center) θ4.

Interestingly, it turned out that due to the large number of data (˜n ≈ 1000) it was easier to
solve Eq.(21) for the full discretized image than to invert (32) in the space of training data. A
prior operator K0 has been implemented as a 3×3 negative Laplacian ﬁlter. (Notice that using a
Laplacian kernel, or another smoothness measure, instead of a straight template matching using
simply the squared error between image and template, leads to a smooth interpolation between
data and templates.) Completed images h for diﬀerent β have been found by iterating according
to

hk+1 = hk + ηA−1

KT (tT − hk) + K0
h

j
(cid:16) X

j tj − hk
ak

,

(cid:17)i

(47)

performed alternating with θ–minimisation. A Gaussian learning matrix A−1 (implemented by
a 5 × 5 binomial ﬁlter) proved to be successful. Typically, the relaxation factor η has been set
to 0.05.

Being a mixture model with m = 2 the situation is that of Fig.3. Typical solutions for large

and small β are shown in Fig.4.

6 Conclusions

Prior mixture models are capable to build complex prior densities from simple, e.g., Gaussian
components. Going beyond classical quadratic regularisation approaches, they still can use

9

the nice analytical features of Gaussians, and allow to control the degree of the resulting non-
convexity explicitly. Combined with parameterised component mean functions and covariances
they seem to provide a powerful tool.

Acknowledgements The author was supported by a Postdoctoral Fellowship (Le 1014/1–1) from
the Deutsche Forschungsgemeinschaft and a NSF/CISE Postdoctoral Fellowship at the Massachusetts
Institute of Technology. Part of the work was done during the seminar ‘Statistical Physics of Neural
Networks’ at the Max–Planck–Institut f¨ur Physik komplexer Systeme, Dresden. The author also wants
to thank Federico Girosi, Tomaso Poggio, J¨org Uhlig, and Achim Weiguny for discussions.

References

1980.

[1] Berger, J.O.: Statistical Decision Theory and Bayesian Analysis. New York: Springer Verlag,

[2] Doob, J.L.: Stochastic Processes. New York: Wiley, 1953 (New edition 1990).

[3] Everitt, B.S. & Hand, D.J.: Finite Mixture Distributions. Chapman & Hall, 1981.

[4] Gelman A., Carlin, J.B., Stern, H.S., & Rubin, D.B.: Bayesian Data Analysis. New York:

Chapman & Hall, 1995.

[5] Girosi, F., Jones, M., & Poggio, T.: Regularization Theory and Neural Networks Architec-

tures. Neural Computation 7 (2), 219–269, 1995.

[6] Lemm, J.C.: Prior Information and Generalized Questions. A.I.Memo No. 1598, C.B.C.L.
Paper No. 141, Massachusetts Institute of Technology, 1996. (available at http://pauli.uni–
muenster.de/∼lemm)

[7] Lemm, J.C.: How to Implement A Priori Information: A Statistical Mechanics Approach.
Technical Report MS-TP1-98-12, Universit¨at M¨unster, 1998. (cond-mat/9808039, also
available at http://pauli.uni–muenster.de/∼lemm.)

[8] Lemm, J.C.: Quadratic Concepts. In Niklasson, L, Boden, M, Ziemke, T.(eds.): Proceedings
of the 8th International Conference on Artiﬁcial Neural Networks (ICANN 98), Sk¨ovde,
Sweden, September 2-4, 1998, Springer Verlag, 1998.

[9] Lemm, J.C.: Bayesian Field Theory. Technical Report MS-TP1-99-1, Universit¨at M¨unster,

1999. (available at http://pauli.uni–muenster.de/∼lemm.)

[10] Lemm, J.C., Uhlig, J., & Weiguny, A.: A Bayesian Approach to Inverse Quantum Statis-
tics. Technical Report MS-TP1-99-6, Universit¨at M¨unster, 1999. (cond-mat/9907013, also
available at http://pauli.uni–muenster.de/∼lemm.)

[11] Robert, C.P.: The Bayesian Choice. New York: Springer Verlag, 1994.

[12] Sivia, D.S.: Data Analysis: A Bayesian Tutorial. Oxford: Oxford University Press, 1996.

[13] Tikhonov A.N. & Arsenin V.: Solution of Ill–posed Problems. New York: Wiley, 1977.

[14] Williams, C.K.I. & Rasmussen, C.E.: Gaussian processes for regression. In Proc. NIPS8,

[15] Vapnik, V.N.: Estimation of dependencies based on empirical data. New York: Springer

MIT Press, 1996.

Verlag, 1982.

[16] Vapnik, V.N.: Statistical Learning Theory. New York: Wiley, 1998.

[17] Wahba, G.: Spline Models for Observational Data. Philadelphia: SIAM, 1990.

[18] Whittaker, E.T., On a new method of graduation. Proc. Edinborough Math. Assoc., 78,

81-89, 1923.

10


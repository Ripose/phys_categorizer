1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
2
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

SCALE INVARIANT MARKOV MODELS FOR BAYESIAN
INVERSION OF LINEAR INVERSE PROBLEMS

St´ephane Brette, J´erˆome Idier and Ali Mohammad-Djafari
Laboratoire des Signaux et Syst`emes (CNRS-ESE-UPS)
´Ecole Sup´erieure d’´Electricit´e,
Plateau de Moulon, 91192 Gif-sur-Yvette Cedex, France

ABSTRACT.
In a Bayesian approach for solving linear inverse problems one needs to specify the prior
laws for calculation of the posterior law. A cost function can also be deﬁned in order to have a common tool
for various Bayesian estimators which depend on the data and the hyperparameters. The Gaussian case
excepted, these estimators are not linear and so depend on the scale of the measurements. In this paper a
weaker property than linearity is imposed on the Bayesian estimator, namely the scale invariance property
(SIP).

First, we state some results on linear estimation and then we introduce and justify a scale invariance
axiom. We show that arbitrary choice of scale measurement can be avoided if the estimator has this SIP.
Some examples of classical regularization procedures are shown to be scale invariant. Then we investigate
general conditions on classes of Bayesian estimators which satisfy this SIP, as well as their consequences
on the cost function and prior laws. We also show that classical methods for hyperparameters estimation
(i.e., Maximum Likelihood and Generalized Maximum Likelihood) can be introduced for hyperparameters
estimation, and we verify the SIP property for them.

Finally we discuss how to choose the prior laws to obtain scale invariant Bayesian estimators. For this,
we consider two cases of prior laws : entropic prior laws and ﬁrst-order Markov models. In related preceding
works [1, 2], the SIP constraints have been studied for the case of entropic prior laws. In this paper extension
to the case of ﬁrst-order Markov models is provided.
KEY WORDS : Bayesian estimation, Scale invariance, Markov modelling, Inverse Problems, Image re-
construction, Prior model selection

1.

Introduction

Linear inverse problem is a common framework for many diﬀerent objectives, such as reconstruction,
restoration, or deconvolution of images arising in various applied areas [3]. The problem is to
estimate an object x which is indirectly observed through a linear operator A, and is therefore
noisy. We choose explicitly this linear model because its simplicity captures many of interesting
features of more complex models without their computational complexity. Such a degradation
models allows the following description:

where b includes both the modeling errors and unavoidable noise of any physical observation system,
and A represents the indirect observing system and depends on a particular application. For exam-
ple, A can be diagonal or block-diagonal in deblurring, Toeplitz or bloc-Toeplitz in deconvolution,
or have no special interesting form as in X-ray tomography.

In order to solve these problems, one may choose to minimize the quadratic residual error

ky − Axk2. That leads to the classical linear system

y = Ax + b,

AtA

x = Aty.

(1)

(2)

When mathematically exact solutions exist, they are too sensitive to unavoidable noise and so are
not of practical interest. This fact is due to a very high condition number of A [3]. In order to have
a solution of interest, we must mathematically qualify admissible solutions.

b

2

S. Brette, J. Idier & A. Mohammad-Djafari

The Bayesian framework is well suited for this kind of problem because it could combine infor-
mation from data y and prior knowledge on the solution. One needs then to specify the prior laws
px(x; λ) and pb(y − Ax; ψ) for calculation of the posterior px|y(x|y) ∝ px(x) pb(y − Ax) with the
Bayes rules. Most of the classical Bayesian estimators, e.g., Maximum a posteriori (MAP), Posterior
Mean (PM) and Marginal MAP (MMAP), can be studied using the common tool of deﬁning a cost
function C(x∗, x) for each of them. It leads to the classical Bayesian estimator

x(y, θ) = arg min
x

Ex∗|y {C(x∗, x)|y}

(3)

depending both on data y and hyperparameters θ.

(cid:8)

(cid:9)

b

Choosing a prior model is a diﬃcult task. This prior model would include our prior knowledge.
Some criteria based on information theory and maximum entropy principle, have been used for that.
For example, when our prior knowledge are the moments of the image to be restored, application of
maximum entropy principle leads Djafari & Demoment [4] to exact determination of the prior,
including its parameters. Knowledge of the bounds (a gabarit) and the choice of a reference measure
leads LeBesnerais [5, 6] to the construction of a model accounting for human shaped prior in the
context of astronomic deconvolution.

We consider the case when there is no important and quantitative prior information such as
the knowledge of moment or bounds of the solution. Then we propose to reduce the arbitrariness
of the choice of prior model by application of constraint to the resulting Bayesian estimator. The
major constraint for the estimator is to be scale invariant, that is, whichever the scale or physi-
cal unit we choose, estimation results must be identical. This desirable property will reduce the
possible choice for prior models and make it independent of the unavoidable scale choice. In this
sense, related works of Jaynes [7] or Box & Tiao [8] on non-informative prior are close to our
statement, although in these works the ignorance is not limited to the measurement scale. In our
work, qualitative information only is supposed to be known (positivity excepted), so we think of
choosing a parametric family of probability laws as a usual and natural way in accounting for the
prior. The parameters estimation in the chosen family of laws will be done according to the data,
with a Maximun Likelihood (ML) or the Generalized Maximum Likelihood (GML) approach. These
approaches are shown in this paper to be scale invariant.

One can criticize choosing the prior law from a desired property of the ﬁnal estimator rather
than from the available prior knowledge. We do not maintain having exactly chosen a model but
just restricting the available choice. Then Gaussian or convex prior popularity is due likely to the
tractability of the associated estimator rather than Gaussianity or convexity of the modeling process.
Lastly, good as the model is, its use depends on the tradeoﬀ between the good behavior of the ﬁnal
estimator and the quality of estimation.

The paper is organized as follows. First, we state some known results on Gaussian estimators as
well as introduce and justify the imposition of scale invariance property (SIP) onto the estimator.
This will be done in section 2 with various examples of scale invariant models. In section 3 we prove
a general theorem for a Bayesian estimator to be scale invariant. This theorem states a suﬃcient
condition on the prior laws which can be used for reducing the choice to admissible priors. For this,
we consider two cases of prior laws : entropic prior laws and ﬁrst-order Markov models. In related
preceding works [1, 2], the SIP constraints has been studied for the case of entropic prior laws. In
this paper we extend that work to the case of ﬁrst-order Markov models.

2. Linearity and scale invariance property

In order to better understand the scale invariance property (SIP), in the next subsection we consider
in detail the classical case of linear estimators. First, let us deﬁne linearity as combination of
additivity:

∀y1, y2,

=⇒ y1 + y2 7→

x1 +

x2,

(4)

y1 7→
y2 7→

(cid:26)

x1
x2
b
b

b

b

3

(5)

(6)

(7)

Scale invariant Markov models...

and the scale invariance property (SIP):

∀y, y 7→

x =⇒ ∀k,

ky 7→ k

x.

Linearity includes the SIP and so is a stronger property. We show a particular case how the SIP is
satisﬁed in these linear models.
2.1. Linearity and Gaussian assumptions

b

b

Linear estimators under Gaussian assumptions have been (and probably still are) the most studied
Bayesian estimators because they lead to an explicit estimation formula.
In a similar way their
practical interest is due to their easy implementation, such as Kalman ﬁltering. In all these cases,
prior laws have the following form:

px(x) ∝ exp

−

(x − mx)tΣ−1

x (x − mx)

,

(cid:19)

1
2

(cid:18)

whereas the conditional additive noise is often a zero mean Gaussian process N (0, Σb).

Minimization of the posterior likelihood for all the three classical cost functions MAP, PM and

MMAP is the same as those of a quadratic form. It leads to the general form of the solution:

x = (AtΣ−1

b A + Σ−1

x I)−1(AtΣ−1

b y + Σ−1

x mx)

which is a linear estimator.
b

Some particular cases follow:

• Case where Σ−1

x = 0 and Σb = σ2

b I. This can be interpreted as degenerated uniform prior of
the solution. The solution is the minimum variance one and is rarely suitable due to the high
condition number of A.

• Case where Σb = σ2

b I and Σx = σ2

xI. This leads to the classical Gaussian inversion formula:

x = (AtA + µI)−1(Aty + µmx), with µ = σ2

b /σ2
x,

(8)

The Signal-to-noise ratio (SNR) µ = σ2
parameter. It plays therefore the meaningful role of a hyperparameter.

x/σ2

b appears explicitly and serves as a scale invariant

b

• The Gauss-Markov regularization case, which considers a smooth prior of the solution, is speciﬁed

by setting Σ−1

x = µDtD + σ−2

x I, with D a discrete diﬀerence matrix.

For all these cases, estimate

x depends on a scale. Let us look at the dependence. For that
matter, suppose that we change the measurement scale. For example, if both x and y are optic
images where each pixel represents the illumination (in Lumen) onto the surface of an optical device,
we measure the number of photons coming into this device. (This could be of practical interest for
X-ray tomography.) Then we convert y into the new chosen scale and simultaneously update our
parameters Σx, Σb and mx. Estimation formula is then given by

b

xk = (Atk−2Σ−1

b A + k−2Σ−1

x I)−1(Atk−2Σ−1

b ky + k−2Σ−1

x kmx),

or, canceling the scale factor k:

b

xk = k(AtΣ−1

b A + Σ−1

x I)−1(AtΣ−1

b y + Σ−1

b mx).

(9)

(10)

Thus, if we take care of hyperparameters, the two restored images are physically the same.

This property is rarely stated in the Gaussian case, which can be explained by the use of SNR

b

as a major tool of reasoning. Thus if we set the SNR, then

xk and k

x are equal.

In many cases Gaussian assumptions are fulﬁlled, often leading to fast algorithms for calculating
the resulting linear estimator. We focus on the case where Gaussian assumptions are too strong.
It is the case when Gauss-Markov models are used, leading to smoother restoration than wanted.
It might be explained by the short probability distribution tails which make discontinuity rare and
which prevent appearing of wide homogeneous areas into the restored image.

b

b

4

S. Brette, J. Idier & A. Mohammad-Djafari

2.2. Scale invariance basics

Although the particular case considered above may appear obvious, it is at the base of the scale
invariance axiom. In order to estimate or to compare physical parameters, we must choose a scale
measurement. This can have a physical meaningful unit or only a grey-level scale in computerized
optics. Anyway we have to keep in mind that a physical unit or scale is just a practical but arbitrary
tool, both common and convenient. As a consequence of this remark we state the following axiom
of scale invariance:

Estimation results must not depend on the arbitrary choice of the scale measurement.

This is true when scale measurement depends on time exposure (astronomic observations, Positron
emission tomography, X-ray tomography, etc.). Estimation results with two diﬀerent values of time
exposure must be coherent. SIP is also of practical interest when exhaustive tests are required for
the validation.

Let us have a look on some regularized criteria for Bayesian estimation. In all the cases, the

MAP criterion is used, and the estimators take the following form:

x(y; ψ, λ) = arg min
x

{− log pb(y − Ax; ψ) − log px(x; λ)} .

(11)

Lp–norm estimators: General form of those criteria involves an Lp–norm rather than a quadratic
norm. Then, the noise models and prior models take the following form:

b

pb(y − Ax; ψ) ∝ exp [ψky − Axkp]

(12)

px(x; λ) ∝ exp [λkM xkq] ,
where M can be a diﬀerence matrix as used by Bouman & Sauer and Besag on the Generalized
Gauss-Markov Models [9], and L1–Markov models [10]. Finally, with q = 1 and M an identity
matrix it leads to a L1–deconvolution algorithm in the context of seismic deconvolution [11].

(13)

According to the scale transformation x 7→ kx and y 7→ ky, the models change in the following

pb(ky − Akx; ψ) ∝ exp [kpψky − Axkp]

px(kx; λ) ∝ exp [kqλkM xkq] .

If we set (ψk, λk) = (kpψ, kqλ), the two estimates are scale invariant. Moreover, if p = q, we can
drop the scale k in the MAP criteria (eq. 11) which becomes scale invariant. This is done in [9] [11],
but it makes the choice of the prior and the noise models mutually dependent. We can also remark
that ψq/λp is scale invariant and can be interpreted as a generalized SNR.

Maximum Entropy methods: Maximum Entropy reconstruction methods have been extensively
used in the last decade. A helpful property of these methods is positivity of the restored image.
In these methods, the noise is considered zero-mean Gaussian N (0, Σb), while the Log-prior take
diﬀerent forms which look like an “Entropy measure” of Burg or Shannon. Three diﬀerent forms
which have been used in practical problems are considered below.

• First, in a Fourier synthesis problem, Wernecke & D’Addario [12] used the following form:

and

way:

and

(14)

(15)

(16)

px(x; λ) ∝ exp

−λ

log xi

.

"

i
X

#

Scale invariant Markov models...

Changing the scale in this context just modiﬁes the partition function which is not important
in the MAP criterion (eq. 11). As the noise is considered Gaussian, these authors show that if
we update the λ parameter in a proper way (λk = k2λ), then the ME reconstruction maintain
linearity with respect to the measurement scale k. Thus, this ME solution is scale invariant,
although nonlinear.

• In image restoration, Burch & al. [13] , consider a prior law of the form

Applying our scale changing yields:

px(x; λ) ∝ exp

−λ

xi log xi

.

"

i
X

#

px(kx; λ) ∝ exp

−λ

k xi log xi + k log k

"

i
X

xi

,

#

i
X

which does not satisfy the scale invariance property due to the k log k
i xi term. It appears from
their later papers that they introduced a data pre-scaling before the reconstruction. Then, the
modiﬁed version of their entropy becomes

P

px(x; λ) ∝ exp

−λ

xi
s

log

xi
s

,

#

(cid:17)

(cid:16)

"

i
X

where s is the pre-scaling parameter.

• Modiﬁcation of the above expression with natural parameters for exponential family leads to the

”entropic laws” used later by Gull & Skilling. [14] and Djafari [15]:

px(x; λ) ∝ exp

−λ1

xi log xi − λ2

"

i
X

xi

.

#

i
X

The resulting estimator is scale invariant for the reasons stated above.

Markovian models: A new Markovian model [16] has appeared from I-divergence considerations
on small translation of an image in the context of astronomic deconvolution. This model can be
rewritten as Gibbs distribution in the following form:

If we change the scale of the measurement, the scale factor k vanishes in the logarithm, and

px(x; λ) ∝ exp

−λ

(xs − xr) log

X(s,r)∈C







.

xs
xr (cid:19)



(cid:18)

.

xs
xr (cid:19)



(cid:18)

px(kx; λ) ∝ exp

−kλ

(xs − xr) log

X(s,r)∈C
Thus this particular Markov random ﬁeld leads to a scale invariant estimator if we update the
parameter λ according to λσb constant (the noise is assumed Gaussian-independent). In the same
way as in the Lp norm example, λσb can be considered as a generalized SNR.



These examples show that the family of scale invariant laws is not a duck-billed platypus family.
It includes many already employed priors on the context of image estimation. We have shown in a
related work that other scale invariant prior laws exist, both in the Markovian prior family [17] and
in the uncorrelated prior [2] family.

5

(17)

(18)

(19)

(20)

(21)

(22)

6

S. Brette, J. Idier & A. Mohammad-Djafari

b

Ax

y=Ax+b

k

^
x

1

1)
Argmin J(x

^
θ

1

Est.

^
x

1

=?

kx^

1
k

Argmin J(x k )

^
x

k

1
k

^
θ

k

Est.

Scheme 1: Global scale invariance property for an estimator

3. Scale invariant Bayesian estimator

Before further developing the scale invariance constraint for the estimator, we want to emphasize the
role of the hyperparameters θ (i.e., parameters of the prior laws) and to sketch their estimation from
the data which is very important in real-world applications. The estimation problem is considered
globally. By globally we mean that, although we are interested on the estimation of x we want
also to take into account the estimation of the hyperparameters θ. To summarize the SIP of an
estimator, we illustrate it by the following scheme:

For more detail, let us deﬁne a scale invariant estimator in the following way:

Deﬁnition 1 An estimator
such that

x(y; θ) is said to be scale invariant if there exists function θk = f k(θ)

or in short

b

∀(y, θ, k > 0),

x(ky, θk) = k

x(y, θ)

y 7→

x =⇒ ∀k > 0,

b

b
ky 7→ k

x.

(23)

(24)

In this paper, we focus only on priors which admit density laws. We deﬁne then the scale

b

b

invariant property for those laws as follows:

Deﬁnition 2 A probability density function pu(u; θ) [resp., a conditional density pu|v(u|v; θ),] is
said to be scale invariant if there exists function θk = f k(θ) such that
pu(ku; θk) = k−N pu(u; θ),
∀(u, θ, k > 0),

(25)

∀(u, θ, k > 0),

[resp.,
where N = dim(u).
If f k = Id, i.e.; if θk = θ then pu(u; θ) is said to be strictly scale invariant.

pu|v(ku|kv; θk) = k−N pu|v(u|v; θ), ]

The above property for density laws speciﬁes that these laws are a part of a family of the laws
which is closed relative to scale transformation. Thus, in this class, a set of pertinent parameters
exists for each chosen scale.

We need also to set two properties for scale invariant density laws. Both concern the conservation

of the SIP, one after marginalization, the other after application of the Bayes rules.

Scale invariant Markov models...

Lemma 1 If px,y(x, y; θ) is scale invariant, then the marginalized py(y; θ) is also scale invariant.

Lemma 2 If px(x; λ) and py|x(y|x; ψ) are scale invariant, then the joint law px,y(x, y; λ, ψ) is
also scale invariant.

Proofs are straightforward and are found in Appendix A.

Using these two deﬁnitions, we prove the following theorem which summarizes suﬃcient condi-

tions for an estimator to be scale invariant:

Theorem 1 If the cost function C(x∗, x) of a Bayesian estimator satisﬁes the condition:

∀k > 0, ∃(ak ∈ IR, bk > 0) such that ∀(x∗, x), C(x∗

k, xk) = ak + bkC(x∗, x),

and if the posterior law is scale invariant, i.e., there exists function θk = f k(θ) such that:

∀k > 0, ∀(x, y),

p(kx|ky; θk) = k−dim(x)p(x|y; θ),

then, the resulting Bayesian estimator is scale invariant, i.e.,

x(ky, θk) = k

x(y, θ).

See the appendix B for the proof. It is also shown there that the cost functions of the three

b

b

classical Bayesian estimators, i.e.; MAP, PM and the MMAP, satisfy the ﬁrst constraint.

Remark: In this theorem, the SIP is applied to the posterior law p(x|y; θ). However, we can sepa-
rate the hyperparameters θ in two sets λ and ψ, where λ and ψ are the parameters of the prior laws
px(x; λ) and pb(y − Ax; ψ). In what follows, we want to make the choice of px and pb independent.
From the lemma 1 and 2, if px and pb satisfy the SIP then the posterior p(x|y; θ) satisﬁes the SIP.
As a consequence θk must be separated according to θk = [λk, ψk] = [gk(λ), hk(ψ)].

4. Hyperparameters estimation
In the above theorem, we assumed that the hyperparameters θ are given. Thus, given the data y
x. Now, if the scale factor k of the data has been
and the hyperparameters θ, we can calculate
changed, we have ﬁrst to update the hyperparameters [18] according to θk = f k(θ), and then we
can use the SIP:

b

x(ky, θk) = k

x(y, θ).

Now, let us see what happens if we have to estimate both x and θ, either by Maximum or Generalized
Maximum Likelihood.

b

b

• Maximum likelihood (ML) method estimates ﬁrst θ by

where

θ = arg max

{L(θ)} ,

θ

b

L(θ) = p(y; θ)

and then

θ is used to estimate x. At a scale k,

b

θk = arg max
θk

{Lk(θk)} .

Application of lemma 1 implies that

b

Lk(θk) = kdim(y)L(θ),

7

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

8

S. Brette, J. Idier & A. Mohammad-Djafari

thus, the Maximum Likelihood estimator satisﬁes the condition

θk = f k(

θ).

The likelihood function (eq. 31) has rarely an explicit form, and a common algorithm for its
locally maximization is the EM algorithm which is an iterative algorithm described brieﬂy as
follows:

b

b

At a scale k,





b

(i)

Q(θ;

θ

) = E

(i) {ln p(y|x; θ)}

x|y;

θ

(i+1)
b

θ

b

= arg max

Q(θ;

θ

b

θ

(cid:26)

(i)

.

)
(cid:27)

b

Qk(θk;

θ

(i)
k ) = E

(i)

{ln p(ky|kx; θk)}

kx|ky;
= −M ln k + E

θ

k

(i)

{ln p(y|x; θ)}

kx|ky;

θ

k

b

= −M ln k + k−dim(y)Q(θ;

θ

(i)

).

(1···l)
k

θ

= f k

(1···l)

b

b

.

b

(cid:19)

(cid:18)

θ

b

Thus, if we initialize this iterative algorithm with the value

(0)
θ
k = fk(
b

θ

(0)

), then we have

Then the scale invariance coherence of hyperparameters is ensured during the optimization steps.

b

• In Generalized Maximum Likelihood (GML) method, one estimates both θ and x by

θ,

x

(cid:16)

(cid:17)

= arg max
(θ,x)

{p(x, y; θ)} .

Applying the same demonstration as above to the joint laws rather than to the marginalized one
leads to

b

b

f k(
(cid:16)
However, this holds if and only if the GML has a maximum. This may not be always the case
and this is a major drawback in GML. Also, in GML method, direct resolution is rarely possible
and sub-optimal techniques lead to the classical two-step estimation scheme:

θ), k

(39)

θk,

xk

=

x

(cid:16)

(cid:17)

(cid:17)

b

b

b

b

.

x(i) = arg max
x

(i)

p(x, y;

θ

= arg max

p(

x(i), y; θ)
b

(cid:26)

n

θ

,

)
(cid:27)
.

o

(i+1)
b

θ

b

We see that, in each iteration, the θ estimation step may be considered as the ML estimation
of θ if x(i) is supposed to be a realization of the prior law. Thus the coherence of estimated
hyperparameters at diﬀerent scales is fulﬁlled during the both optimization steps, and

b

(1···l)
k

,

θ

x(1···l)
k

=

θ
f k(

(1···l)

), k

x(1···l)

.

(cid:18)

b

(cid:19)

(cid:18)

b

(cid:19)

Thus, if we consider the whole estimation problem (with a ML or GML approach), the SIP of
the estimator is assured in both cases. It is also ensured during the iterative optimization schemes
of ML or GML.

b

b

(34)

(35)

(36)

(37)

(38)

(40)

(41)

(42)

9

(43)

(44)

Scale invariant Markov models...

5. Markovian invariant distributions

Markovian distributions as priors in image processing allow to introduce local characteristics and
inter-pixels correlations. They are widely used but there exist many diﬀerent Markovian models
and very few model selection guidelines exist. In this section we apply the above scale invariance
considerations to the prior model selection in the case of ﬁrst order homogeneous MRFs.

Let X ∈ Ω be a homogeneous Markov random ﬁeld deﬁned on the subset [1 . . . N ] × [1 . . . M ] of

ZZ2. The Markov characteristic property is:

where δi is the neighbourhood of site i, and S is the set of pixels. Hammersley-Cliﬀord theorem for
the ﬁrst order neighbourhood reads:

pX (xi|xS−i) = pX (xi|xδi),

pX (x; λ) ∝ exp

−λ

φ(xs, xr)

,





X{r,s}∈C





where C is the clique set, and φ(x, y) the clique potential. In most works [9, 19, 20, 21] a simpliﬁed
model is introduced under the form φ(x, y) = φ(x − y). Here we keep a general point of view.
Application of the scale invariance condition to the Markovian prior laws pX (x, λ) leads to the two
following theorems:

Theorem 2 A familly of Markovian distribution is scale invariant if and only if there exist two
functions f (k, λ) and β(k) such that clique potential φ(xs, xr) satisﬁes:

f (k, λ) φ(kxs, kxr) = λφ(xs, xr) + β(k).

(45)

Theorem 3 A necessary and suﬃcient condition for a Markov random ﬁelds to be scale invariant is
that exists a triplet (a, b, c) such as the clique potential φ(xs, xr) veriﬁes the linear partial diﬀerential
equation (PDE) :

aφ(xs, xr) + b

xs

∂φ(xs, xr)
∂xs

+ xr

∂φ(xs, xr)
∂xr

= c.

(cid:19)

(cid:18)

Finally, enforcing symmetry of the clique potentials φ(xs, xr) = φ(xr, xs) the following theorem

provides the set of scale invariant clique potentials:

Theorem 4 pX (x, λ) is scale invariant if and only if φ(xs, xr) is chosen from one of the following
vector spaces:

(cid:26)

V0 =

φ(xs, xr) | ∃ψ(.) even and p ∈ IR, φ(xs, xr) = ψ

xs
xr (cid:12)
(cid:12)
(cid:12)
(cid:12)
xs
(cid:12)
(cid:12)
(cid:12)
(cid:12)
xr (cid:12)
(cid:12)
(cid:19)
(cid:12)
(cid:12)
Moreover, V0 is the subspace of strictly scale invariant clique potentials.
(cid:12)
(cid:12)
(cid:12)
(cid:12)

φ(xs, xr) | ∃ψ(.) even , φ(xs, xr) = ψ

(cid:18)
log

V1(p) =

log

(cid:26)

(cid:18)

(cid:19)

− p log |xsxr|

(cid:27)

|xsxr|p

(cid:27)

(46)

(47)

For the proof of these theorems see [22].

Among the most common models in use for image processing purposes, only few clique potentials

fall into the above set. Let us give two examples:

First, the GGMRFs proposed by Bouman & Sauer [9] were built by a similar approach of scale
invariance but under the restricted assumption that φ(xs, xr) = φ(xs − xr). The yielded expression
φ(xs, xr) = |xs − xr|p can be factored according to φ(xs, xr) = |xs xr|p/2|2sh (log(xs/xr)/2)|p which
shows that it falls in V1(p).

The second example of potential does not reduce to the single variable function

φ(xs − xr): φ(xs, xr) = (xs − xr) log (xs/xr). It has recently been introduced from I-divergence

10

S. Brette, J. Idier & A. Mohammad-Djafari

penalty considerations in the ﬁeld of image estimation problem (optic deconvolution) by O’Sullivan
[16]. Factoring |xsxr|

2 leads to:

1

φ(xs, xr) = |xsxr|

2 ψ (log(xs/xr)) ,

1

(48)

where ψ(X) = 2Xsh(X/2) is even. It shows that φ(xs, xr) is in V1(1/2) and is scale invariant. As
φ(xs, xr) is deﬁned only on IR2
∗+ it applies to positive quantities. This feature is very useful in image
processing where prior positivity applies to many physical quantities.

6. Conclusions

In this paper we have outlined and justiﬁed a weaker property than linearity that is desired for the
Bayesian estimators to have. We have shown that this scale invariance property (SIP) helps to avoid
an arbitrary choice for the scale of the measurement. Some models already employed in Bayesian
estimation, including Markov prior Models [9, 16], Entropic prior [23, 2] and Generalized Gaussian
models [11], have demonstrated the existence and usefulness of scale invariant models. Then we
have given general conditions for a Bayesian estimator to be scale invariant. This property holds
for most Bayesian estimators such as MAP, PM, MMAP under the condition that the prior laws
are also scale invariant. Thus, imposition of the SIP can assist in the model selection. We have also
shown that classical hyperparameters estimation methods satisfy the SIP property for estimated
laws.

Finally we discussed how to choose the prior laws to obtain scale invariant Bayesian estimators.
For this, we considered two cases: entropic prior laws and ﬁrst-order Markov models. In related
preceding works [1, 2, 24], the SIP constraints have been studied for the case of entropic prior laws.
In this paper we extended that work to the case of ﬁrst-order Markov models and showed that many
common Markov models used in image processing are special cases.

1. SIP property inheritance
• Proof of the Lemma 1:

Let px,y(x, y; θ) have the scale invariance property, then if there exists θk = f k(θ) such that

px,y(kx, ky; θk) = k−(M+N )px,y(x, y; θ),

where N = dim(x) and M = dim(y), then, marginalizing with respect to x, we obtain

py(ky; θk) = k−(M+N )

px,y(x, y; θ)k−N dx = k−M py(y; θ),

ZZ

which completes the proof.

• Proof of the Lemma 2:

The deﬁnition of SIP for density laws and direct application of the Bayes rule lead to

px,y(kx, ky; θk) = k−N px(x; λ) k−M py|x(y|x; ψ) = k−(M+N )px,y(x, y; θ),

which concludes the proof.

2. SIP conditions for Bayesian estimator
• Proof of the Theorem 1:

Since a Bayesian estimator is deﬁned by

x = arg min
x

(cid:26)Z

b

C(x∗, x) p(x∗|y; θ) dx∗

,

(cid:27)

Scale invariant Markov models...

11

then

xk = arg min
xk

C(x∗

k, xk) p(x∗

k|ky; θk)d(x∗
k)

b

= k arg min

C(kx∗, kx) p(kx∗|ky; θk) kN dx∗

(cid:27)

(cid:27)

= k arg min

[ak + bkC(x∗, x)]k−N p(x∗|y; θ) kN dx∗

= k

x,

(cid:26)Z

x

x

(cid:26)Z

(cid:26)Z

(cid:27)

b

which proves the Theorem 1.

• Conditions for cost functions:

The three classical Bayesian estimators, MAP, PM and MMAP, satisfy the condition of the cost
function:

– Posterior mean (PM): C(x∗

k, xk) = (x∗

– Maximum a posteriori (MAP): C(x∗

k − xk)tQ (x∗
k, xk) = 1 − δ(x∗

k − xk) = k2 C(x∗, x).
k − xk) = C(x∗, x).

– Marginal Maximum a Posteriori (MMAP):

C(x∗

k, xk) =

(1 − δ([x∗

k]i − [xk]i)) = C(x∗, x).

i
X

References

[1] A. Mohammad-Djafari and J. Idier, “Maximum entropy prior laws of images and estimation
of their parameters,” in Maximum Entropy and Bayesian Methods in Science and Engineer-
ing (T. Grandy, ed.), (Dordrecht, The Netherlands), MaxEnt Workshops, Kluwer Academic
Publishers, 1990.

[2] A. Mohammad-Djafari and J. Idier, “Scale invariant Bayesian estimators for linear inverse

problems,” in Proc. of the First ISBA meeting, (San Fransisco, USA), Aug. 1993.

[3] G. Demoment, “Image reconstruction and restoration : Overview of common estimation struc-
ture and problems,” IEEE Transactions on Acoustics Speech and Signal Processing, vol. 37,
pp. 2024–2036, Dec. 1989.

[4] A. Mohammad-Djafari and G. Demoment, “Estimating priors in maximum entropy image pro-

cessing,” in Proceedings of IEEE ICASSP, pp. 2069–2072, IEEE, 1990.

[5] G. Le Besnerais, J. Navaza, and G. Demoment, “Aperture synthesis in astronomical radio-
interferometry using maximum entropy on the mean,” in SPIE Conf., Stochastic and Neural
Methods in Signal Processing, Image Processing and Computer Vision (S. Chen, ed.), (San
Diego), p. 11, July 1991.

[6] G. Le Besnerais, J. Navaza, and G. Demoment, “Synth`ese d’ouverture en radio-astronomie par
maximum d’entropie sur la moyenne,” in Actes du 13`eme colloque GRETSI, (Juan-les-Pins,
France), pp. 217–220, Sept. 1991.

[7] E. Jaynes, “Prior probabilities,” IEEE Transactions on Systems Science and Cybernetics,

vol. SSC-4, pp. 227–241, Sept. 1968.

[8] G. Box and T. G.C., Bayesian inference in statistical analysis. Addison-Wesley publishing,

1972.

[9] C. Bouman and K. Sauer, “A generalized Gaussian image model for edge-preserving MAP
estimation,” IEEE Transactions on Medical Imaging, vol. MI-2, no. 3, pp. 296–310, 1993.

12

S. Brette, J. Idier & A. Mohammad-Djafari

[10] J. Besag, “Digital image processing : Towards Bayesian image analysis,” Journal of Applied

Statistics, vol. 16, no. 3, pp. 395–407, 1989.

[11] D. Oldenburg, S. Levy, and K. Stinson, “Inversion of band-limited reﬂection seismograms:

theory and practise,” Procedings of IEEE, vol. 74, p. 3, 1986.

[12] S. Wernecke and L. D’Addario, “Maximum entropy image reconstruction,” IEEE Transactions

on Computers, vol. C-26, pp. 351–364, Apr. 1977.

[13] S. Burch, S. Gull, and J. Skilling, “Image restoration by a powerful maximum entropy method,”

Computer Vision and Graphics and Image Processing, vol. 23, pp. 113–128, 1983.

[14] S. Gull and J. Skilling, “Maximum entropy method in image processing,” Proceedings of the

IEE, vol. 131-F, pp. 646–659, 1984.

[15] A. Mohammad-Djafari and G. Demoment, “Maximum entropy reconstruction in X ray and
diﬀraction tomography,” IEEE Transactions on Medical Imaging, vol. 7, no. 4, pp. 345–354,
1988.

[16] J. A. O’Sullivan, “Divergence penalty for image regularization,” in Proceedings of IEEE

ICASSP, vol. V, (Adelaide), pp. 541–544, Apr. 1994.

[17] S. Brette, J. Idier, and A. Mohammad-Djafari, “Scale invariant Markov models for linear inverse
problems,” in Fifth Valencia Int. Meeting on Bayesian Statistics, (Alicante, Spain), June 1994.

[18] J. Marroquin, “Deterministic interactive particle models for image processing and computer
graphics,” Computer Vision and Graphics and Image Processing, vol. 55, no. 5, pp. 408–417,
1993.

[19] S. Geman and D. Geman, “Stochastic relaxation, Gibbs distributions, and the Bayesian restora-
tion of images,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-6,
p. 2, 1984.

[20] S. Geman and G. Reynolds, “Constrained restoration and recovery of discontinuities,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-14, pp. 367–383, 1992.

[21] J. Besag, “On the statistical analysis of dirty pictures,” Journal of the Royal Statistical Society

B, vol. 48, p. 1, 1986.

[22] S. Brette, J. Idier, and A. Mohammad-Djafari, “Scale invariant Markov models for linear inverse
problems,” in Second ISBA meeting, vol. Bayesian Statistics, (Alicante, Spain), ISBA, American
Statistical Association, June 1994.

[23] S. F. Gull, “Developments in maximum entropy data analysis,” in Maximum Entropy and
Bayesian Methods (J. Skilling, ed.), pp. 53–71, Dordrecht, The Netherlands: Kluwer Academic
Publishers, 1989.

[24] A. Mohammad-Djafari and J. Idier, “A scale invariant Bayesian method to solve linear inverse
problems,” in Maximum Entropy and Bayesian Methods (G. Heidbreder, ed.), (Dordrecht, The
Netherlands), The 13th Int. MaxEnt Workshops, Santa Barbara, USA, Kluwer Academic Pub-
lishers, 1993.


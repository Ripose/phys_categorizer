2
0
0
2
 
l
u
J
 
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
0
0
7
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Inference of Entropies of Discrete Random
Variables with Unknown Cardinalities

Ilya Nemenman
Kavli Institute for Theoretical Physics
University of California
Santa Barbara, CA 93106
nemenman@kitp.ucsb.edu

Abstract

We examine the recently introduced NSB estimator of entropies of
severely undersampled discrete variables and devise a procedure for cal-
culating the involved integrals. We discover that the output of the estima-
tor has a well deﬁned limit for large cardinalities of the variables being
studied. Thus one can estimate entropies with no a priori assumptions
about these cardinalities, and a closed form solution for such estimates is
given.

1 Introduction

Estimation of functions of a discrete random variable with an unknown probability distri-
bution using independent samples of this variable seems to be an almost trivial problem
known to many yet from the high school [1]. However, this simplicity vanishes if one con-
siders an extremely undersampled regime, where K, the cardinality or the alphabet size
of the random variable, is much larger than N , the number of independent samples of the
variable. In this case the average number of samples per possible outcome (also called bin
in this paper) is less than one, relative uncertainty in the underlying probability distribution
is large, and the usual formulas for estimation of various statistics fail miserably. Then one
has to use the power of Bayesian statistics to a priori constraint the set of allowable distri-
butions and thus decrease the posterior error. Unfortunately, due to the usual bias–variance
tradeoff, decreasing the variance this way may lead to an increased bias, i.e., the estimator
becomes a function of the prior, rather than of the experimental data.

The situation is particularly bad for inferring the Boltzmann–Shannon entropy, S, one of
the most important characteristics of a discrete variable. Its frequentist as well as common
Bayesian estimators have low variances, but high biases that are very difﬁcult to calculate
(see Ref. [2] for a review). However, recently ideas from Bayesian model selection [3, 4,
5, 6] were used by Nemenman, Shafee, and Bialek to suggest a solution to the problem [7].
Their method, hereafter called NSB, is robust and unbiased even for severely undersampled
problems. We will review it and point out that it is equivalent to ﬁnding the number of yet
unseen bins with nonzero probability given K, the maximum cardinality of the variable.
While estimation of K by model selection techniques will not work, we will show that the
method has a proper limit as K
. Thus one should be able to calculate entropies of
discrete random variables even without knowing their cardinality.

→ ∞

2 Summary of the NSB method

In Bayesian statistics, one uses Bayes rule to expresses posterior probability of a probability
distribution q
, i = 1 . . . K, of a discrete random variable with a help of its a priori
≡ {
(q). Thus if ni identical and independent samples from q are observed in bin
probability,
K
n), is
i=1 ni = N , then the posterior, P (q
i, such that
|

qi

P

}

P

n) =
P (q
|

(q)

q)
P (n
|
P
P (n)

=

K

i=1 qni

(q)
i P
K
i=1 qni

i P

.

(q)

1
0 dK q
Q
R

Q

Following Ref. [7], we will focus on popular Dirichlet family of priors, indexed by a (hy-
per)parameter β:

β(q) =

P

1
Z(β)

δ

1
 

−

K

K

qi

i=1
X

!

i=1
Y

qβ−1

i

, Z(β) =

ΓK(β)
Γ(Kβ)

.

Here δ–function and Z(β) enforce normalizations of q and
β(q) respectively, and Γ
stands for Euler’s Γ–function. These priors are common in applications [8] since they,
as well as the data term, P (n
q), are of a multinomial structure, which is analytically
|
tractable. For example, in Ref. [9] Wolpert and Wolf calculated posterior averages, here
denoted as

β , of many interesting quantities, including the distribution itself,
i

. . .
h

P

(1)

(2)

(3)

qi
h
and the moments of its entropy, which we will not reprint here.

β =
i

Kβ ,

≡

κ

,

ni + β
N + κ

≫

As suggested by Eq. (3), Dirichlet priors add extra β sample points to each possible bin.
n) is dominated by the distributions
N/K the data is unimportant, and P (q
Thus for β
|
close to the uniform one, q
1/K. The posterior mean of the entropy is then strongly
biased upwards to its maximum possible value of Smax = ln K.1 Similarly, for β
N/K,
distributions in the vicinity of the frequentist’s maximum likelihood estimate, q = n/N ,
are important, and

≪

≈

S
h

β has a strong downward bias [2].
i

In Ref. [7], Nemenman et al. traced this problem to the properties of the Dirichlet family:
its members encode reasonable a priori assumptions about q, but not about S(q). Indeed, it
turns out that a priori assumptions about the entropy are extremely biased, as may be seen
from its following a priori moments.

ξ(β)

S

≡ h
(δS)2

N =0 i
N =0 i

|

|

σ2(β)

≡ h

β =

β = ψ0(κ + 1)

ψ0(β + 1) ,

−
ψ1(β + 1)

β + 1
κ + 1

ψ1(κ + 1) ,

−

(4)

(5)

where ψm(x) = (d/dx)m+1 ln Γ(x) are the polygamma functions. ξ(β) varies smoothly
from 0 for β = 0, through 1 for β
. σ(β) scales as
1/√K for almost all β (see Ref. [7] for details). This is negligibly small for large K. Thus
q that is typical in
β(q) usually has its entropy extremely close to some predetermined
β–dependent value. It is not surprising then that this bias persists even after N < K data
are collected.

1/K, and to ln K for β

→ ∞

≈

P

The NSB method suggests that to estimate entropy with a small bias one should not look
for priors that seem reasonable on the space of q, but rather the a priori distribution of
(S(q)), should be ﬂattened. This can be done approximately by noting that
entropy,
(S) is almost a δ–function. Thus a prior that enforces
Eqs. (4, 5) ensure that, for large K,

P

P
1In this paper the unit of entropy is nat. Thus all logarithms are natural.

(6)

(8)

integration over all non–negative values of β, which correspond to all a priori expected
entropies between 0 and ln K, should do the job of eliminating the bias in the entropy
estimation even for N
K. While there are probably other options, Ref. [7] centered on
the following prior, which is a generalization of Dirichlet mixture priors [10] to an inﬁnite
mixture:

≪

(q; β) =

P

1
Z

δ

1

 

−

K

K

qi

i=1
X

!

i=1
Y

qβ−1

i

dξ(β)

dβ P

(β) .

Here Z is again the normalizing coefﬁcient, and the term dξ/dβ ensures uniformity for the
(β), may be
a priori expected entropy, ξ, rather than for β. A non–constant prior on β,
used if sufﬁcient reasons for this exist, but we will set it to one in all further developments.

P

Inference with the prior, Eq. (6), involves additional averaging over β (or, equivalently, ξ),
but is nevertheless straightforward. The a posteriori moments of the entropy are

Sm =

ρ(ξ

c
n) =
|

R

P

ln K
0

dξ ρ(ξ, n)
h
ln K
dξ ρ(ξ
0

Sm

n)
|

iβ(ξ)

R
(β (ξ))

Γ(κ(ξ))
Γ(N + κ(ξ))

Γ(ni + β(ξ))
Γ(β(ξ))

.

K

i=1
Y

, where the posterior density is

(7)

Nemenman et al. explain why this method should work using the theory of Bayesian model
selection [3, 4, 5, 6]. All possible probability distributions, even those that ﬁt the data
extremely badly, should be included in the posterior averaging. For models with a larger
volume in q space, the number of such bad q’s is greater, thus the probability of the model
decreases. Correspondingly, such contributions from the phase space factors are usually
termed Occam razor because they automatically discriminate against bigger, more complex
models. If the maximum likelihood solution of a complex model explains the data better
than that of a simpler one,2 then the total probability, a certain combination of the maximum
likelihood and the Occam factors, has a maximum for some non–trivial model, and the
sharpness of the maximum grows with N . In other words, the data selects a model which
is simple, yet explains it well.

In the case of Eq. (6), we can view different values of β as different models. The smaller
β is, the closer it brings us to the frequentist’s maximum likelihood solution, so the data
gets explained better. However, as there is less smoothing [cf. Eq. (3)], smaller β results
in the larger phase space. Thus, according to Ref. [7], one may expect that the integrals in
Eq. (7) will be dominated by some β∗, appropriate smoothing will be sharply selected, and
β∗. In the current paper we will investigate whether a maximum of the integrand
· · · ≈ h· · ·i
in Eq. (7), indeed, exists and will study its properties. The results of the analysis will lead
us to an extension and a simpliﬁcation of the NSB method.
c

3 Calculation of the NSB integrals

We will calculate integrals in Eq. (7) using the saddle point method. Since the moments
of S do not have N dependence, when N is large only the Γ–terms in ρ are important for
estimating the position of the saddle and the curvature around it. We write

(β(ξ)) exp [

(n, β, K)] ,

−L

ρ(ξ

n) =
|
(n, β, K) =

L

P

−

i
X

ln Γ(β + ni) + K ln Γ(β)

ln Γ(κ) + ln Γ(κ + N ) .

(10)

−

2This is usually achieved by requiring that models are nested, that is, all q’s possible in the simpler

model are possible in the complex one, but not vice versa.

(9)

Then the saddle point (equivalently, the maximum likelihood) value, κ∗ = Kβ∗, solves the
following equation obtained by differentiating Eq. (10).

1
K

ni>0

i
X

ψ0(ni + β∗)

ψ0(β∗) + ψ0(κ∗)

ψ0(κ∗ + N ) = 0 ,

(11)

−

K1
K

−

where we use Km to denote the number of bins that have, at least, m counts. Note that
N > K1 > K2 > . . . .

≫

We notice that if K
N , and if there are at least a few bins that have more that one datum
in them, i.e., K1 < N , then the distribution the data is taken from is highly non–uniform.
Thus the entropy should be much smaller than its maximum value of Smax. Since for any
β = O(1) the entropy is extremely close to Smax (cf. Ref. [7]), small entropy may be
achievable only if β∗

. Thus we will look for

0 as K

→

→ ∞
κ∗ = κ0 +

1
K

κ1 +

1
K 2 κ2 + . . . ,

where none of κj depends on K. Plugging Eq. (12) into Eq. (11), after a little algebra we
get the ﬁrst few terms in the expansion of κ∗:
ni>1

κ1 =

ψ0(ni)

ψ0(1)
ψ1(κ0) + ψ1(κ0 + N )

−

,

0 −

K1/κ2
+ ψ2(κ0)−ψ2(κ0+N )

i
X
K1
κ3
0

ni>1
i
i
ψ1(κ0) + ψ1(κ0 + N )
and the zeroth order term solves the following algebraic equation

2
K1/κ2

κ2 =

κ2
1 +

0 −

P

h

K1
κ0

= ψ0(κ0 + N )

ψ0(κ0) .

−

κ0 [ψ1(ni)

ψ1(1)]

−

,

(14)

If required, more terms in the expansion can be calculated, but for common applications K
is so big that none are usually needed.
0 and N > 0, the r. h. s. of the equation is
We now focus on solving Eq. (15). For κ0 →
, it is close to N/κ0. Thus if
approximately 1/κ0 [11]. On the other hand, for κ0 → ∞
N = K1, that is, the number of coincidences among different data, ∆
K1, is zero,
N
then the l. h. s. always majorates the r. h. s., and the equation has no solution. If there are
coincidences, a unique solution exists, and the smaller ∆ is, the bigger κ0 is. Thus we may
want to search for κ0 ∼
Now it is useful to introduce the following notation:

1/∆ + O(∆0).

−

≡

fN (j)

≡

N −1

mj
N j+1 ,

m=0
X
where each of fN ’s scales as N 0. Using standard results for polygamma functions [11], we
rewrite Eq. (15) as

Here we introduced the relative number of coincidences, δ
previous observation, Eq. (17) suggests that we look for κ0 of the form

≡

∆/N . Combined with the

δ
1
−
κ0/N

=

1)j fN (j)

(κ0/N )j .

(
−

∞

j=0
X

κ0 = N

+ b0 + b1δ + . . .

,

(cid:19)

b−1
δ

(cid:18)

(12)

(13)

(15)

(16)

(17)

(18)

where each of bj’s is independent of δ and scales as N 0.

Substituting this expansion for κ0 into Eq. (17), we see that it is self–consistent, and
N

1

b−1 = fN (1) =

−
2N

,

b0 =

b1 =

fN (2)
fN (1)
f 2
N (2)
f 3
N (1)

−

−

= −

,

2N + 1
3N
fN (3)
f 2
N (1)

=

+

N 2
−
9(N 2

2
−
N )

.

N

−

(19)

(20)

(21)

Again, more terms can be calculated if needed.
This expresses the saddle point value β∗ (or κ∗, or ξ∗) as a power series in 1/K and δ.
In order to complete the evaluation of integrals in Eq. (7), we now need to calculate the
curvature at this saddle point. Simple algebra results in

= ∆ + N O(δ2) .

(22)

ξ(β∗)

=

(cid:20)

∂2
L
∂β2

1
(dξ/dβ)2

(cid:21)β∗

∂2
L
∂ξ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Notice that the curvature does not scale as a power of N as was suggested in Ref. [7]. Our
uncertainty in the value of ξ∗ is determined to the ﬁrst order only by coincidences. One
can understand this by considering a very large K with most of the bins having negligible
probabilities. Then counts of ni = 1 are not informative for entropy estimation, as they can
correspond to massive bins, as well as to some random bins from the sea of negligible ones.
However, coinciding counts necessarily signify an important bin, which should inﬂuence
the entropy estimator. Note also that to the ﬁrst order in 1/K the exact positioning of
coincidences does not matter: a few coincidences in many bins or many coincidences in a
single one produce the same saddle point and the same curvature around it, provided that
β(q) and
∆ stays the same. While this is an artifact of our choice of the underlying prior
may change in a different realization of the NSB method, this behavior parallels famous
Ma’s entropy estimator, which is also coincidence based [12].

P

In conclusion, if the number of coincidences, not N , is large, then a proper value for β is
selected, and the variance of entropy is small. Then the results of this section transform cal-
culations of complicated integrals in Eq. (7) to pure algebraic operations. This analysis has
been used to write a general purpose software library for estimating entropies of discrete
variables. The library is available from the author.

4 Choosing a value for K?

If N

≪
K1)β

A question is in order now.
K, the regime we are mostly interested in, then
the number of extra counts in occupied bins, K1β, is negligible compared to the number of
extra counts in empty bins, (K
Kβ. Then Eqs. (3, 8) tell us that selecting β (that
is, integrating over it) means balancing N , the number of actual counts versus κ = Kβ,
the number of pseudocounts, or, equivalently, the scaled number of unoccupied bins. Why
do we vary the pseudocounts by varying β? Can we instead use Bayesian model selection
methods to set K? Indeed, not having a good handle on the value of K is usually one of
the main reasons why entropy estimation is difﬁcult. Can we circumvent this problem?

≈

−

To answer this, note that smaller K leads to a higher maximum likelihood value since the
total number of pseudocounts is less. Unfortunately, smaller K also means smaller volume
in the distribution space since there are fewer bins, fewer degrees of freedom, available.
As a result, Bayesian averaging over K will be trivial: the smallest possible number of
bins, that is no empty bins, will dominate. This is very easy to see from Eq. (8): only the
ﬁrst ratio of Γ–functions in the posterior density depends on K, and it is maximized for

K = K1. Thus straight–forward selection of the value of K is not an option. However, in
the next Section we will suggest a way around this hurdle.

5 Unknown or inﬁnite K

When one is not sure about the value of K, it is usually because its simple estimate is
intolerably large. For example, consider measuring entropy of ℓ–gramms in printed English
[13] using an alphabet with 29 characters: 26 different letters, one symbol for digits, one
space, and one punctuation mark. Then even for ℓ as low as 7, a naive value for K is
1010. Obviously, only a miniscule fraction of all possible 7–gramms may ever
297
happen, but one does not know how many exactly. Thus one is forced to work in the space
of full cardinality, which is ridiculously undersampled.

∼

A remarkable property of the NSB method, as follows from the saddle point solution in
Sec. 3, is that it works even for ﬁnite N and extremely big K (provided, of course, that there
, the method simpliﬁes since then one should only
are coincidences). Moreover, if K
keep the ﬁrst term in the expansion, Eq. (12). Even more interestingly, for every β
1/K
the a priori distribution of entropy becomes an exact delta function since the variance of
entropy drops to zero as 1/K, see Eq. (5). Thus the NSB technique becomes more precise
as K increases. So the solution to the problem of unknown cardinality is to use an upper
bound estimate for K: it is much better to overestimate K than to underestimate it. If
desired, one may even assume that K

to simplify the calculations.

→ ∞

≫

→ ∞

It is important to understand which additional assumptions are used to come to this con-
clusion. How can a few data points specify entropy of a variable with potentially inﬁnite
cardinality? As explained in Ref. [7], a typical distribution in the Dirichlet family has a
very particular rank ordered (Zipf) plot: the number of bins with the probability mass less
than some q is given by an incomplete B–function, I,

ν(q) = KI(q; β, κ

β)

K

−

≡

q

0 dxxβ−1(1
B(β, κ
R

−
−

x)κ−β−1
β)

(23)

where B stand for the usual complete B–function. NSB ﬁts for a proper value of β (and
κ = Kβ) using bins with coincidences, the head of the rank ordered plot. But knowing
β immediately deﬁnes the tails, where no data has been observed yet, and the entropy can
be calculated. Thus if the Zipf plot for the distribution being studied has a substantially
longer tail than allowed by Eq. (23), then one should suspect the results of the method. For
example, NSB will produce wrong estimates for a distribution with q1 = 0.5, q2, . . . qK =
0.5/(K

1), and K

−

.
→ ∞

With this caution in mind, we may now try to calculate the estimates of the entropy and its
variance for extremely large K. We want them to be valid even if the saddle point analysis
0, but κ∗ = Kβ∗ is
of Sec. 3 fails because ∆ is not large enough. In this case β∗
, so the prior
some ordinary number. The range of entropies now is 0
→ ∞
(q; β) is (almost) uniform over a semi–inﬁnite range and thus is non–
on S produced by
β(q), Eq. (2). However, as is
normalizable. Similarly, there is a problem normalizing
common in Bayesian statistics, these problems can be easily removed by an appropriate
limiting procedure, and we will not pay attention to them in the future.

→
ln K

≤

≤

P

P

S

When doing integrals in Eq. (7), we need to ﬁnd out how
vicinity of the maximum of ρ, using the formula for

S(n)
β depends on ξ(β). In the
i
h
S(n)
β from Ref. [9] we get
i
h

S(n)
[
κ
i
h

−

ξ(β)]

κ≈κ∗

(cid:12)
(cid:12)
N K1 −
(cid:12)
(N + κ)κ −

N

=

ni>1

i
X

niψ0(ni)

niψ0(1)

−
N + κ

+ O

= O(δ,

) .

(24)

1
K

(cid:18)

(cid:19)

1
K

, δ = ∆/N

The expression for the second moment is similar, but complicated enough so that we chose
not to write it here . The main point is that for K
0, and κ in the vicin-
ity of κ∗, the posterior averages of the entropy and its square are almost indistinguishable
from ξ and ξ2, the a priori averages. Since now we are interested in small ∆ (otherwise we
can use the saddle point analysis), we will use ξm instead of
β in Eq. (7). The error
i
δ, 1
of such approximation is O
K

N , 1
K
Now we need to slightly transform the Lagrangian, Eq. (10). First, we drop terms that do
not depend on κ since they appear in the numerator and denominator of Eq. (7) and thus
cancel. Second, we expand around 1/K = 0. This gives

Sm
h

→ ∞

= O

→

(cid:1)

(cid:1)

(cid:0)

(cid:0)

1

.

(n, κ, K) =

L

ln Γ(ni)

K1 ln κ

ln Γ(κ) + ln Γ(κ + N ) + O(

) .

(25)

−

−

1
K

ni>1

−

i
X

We note that κ is large in the vicinity of the saddle if δ is small and N is large, cf. Eq. (18).
N ψ0(κ) + N 2ψ1(κ)/2.
Thus, by deﬁnition of ψ–functions, ln Γ(κ + N )
Further, ψ0(κ)
Cγ, where Cγ is
ln κ, and ψ1(κ)
the Euler’s constant, Eq. (4) says that ξ

≈
ln κ. Combining all this, we get

1/κ [11]. Finally, since ψ0(1) =

ln Γ(κ)

−

≈

≈

−

Cγ

−

≈

(n, κ, K)

L

≈ −

ln Γ(ni) + ∆(ξ

Cγ) +

exp(Cγ

ξ) ,

(26)

−

−

ni>1

i
X

where the

sign means that we are working with precision O

≈

Now we can write:

N 2
2

1

N , 1

K

.

(cid:0)

(cid:1)

S

Cγ

b
(δS)2

≈

≈

ln K

e−Ldξ ,

ln

∂
∂∆
2

0
Z
ln K

e−Ldξ .

ln

0
Z

−

∂
∂∆

(cid:18)

d

(cid:19)
The integral involved in these expressions can be easily calculated by substituting exp(Cγ
exp(Cγ) by 0
ξ) = τ and replacing the limits of integration 1/K exp(Cγ)
τ
≤ ∞
δ2 exp(
there is, at least, one coincidence. Thus

−
≤
. Such replacement introduces errors of the order (1/K)∆ at the lower limit and
1/δ2) at the upper limit. Both errors are within our approximation precision if

≤

≤

−

τ

Finally, substituting Eq. (29) into Eqs. (27, 28) we get for the moments of the entropy

ln K

0
Z

e−Ldξ

Γ(∆)

≈

−∆

.

N 2
2

(cid:18)

(cid:19)

S

(Cγ

ln 2) + 2 ln N

ψ0(∆) ,

−

(δS)2
b

−

ψ1(∆) .

≈

≈

These equations are valid to zeroth order in 1/K and 1/N . They provide a simple, yet
nontrivial, estimate of the entropy that can be used even if the cardinality of the variable
is unknown. Note that Eq. (31) agrees with Eq. (22) since, for large ∆, ψ1(∆)
1/∆.
Interestingly, Eqs. (30, 31) carry a remarkable resemblance to Ma’s method [12].

d

≈

6 Conclusion

We have further developed the NSB method for estimating entropies of discrete random
variables. The saddle point of the posterior integrals has been found in terms of a power

(27)

(28)

(29)

(30)

(31)

series in 1/K and δ. It is now clear that validity of the saddle point approximation depends
not on the total number of samples, but only on the coinciding ones. Further, we have
extended the method to the case of inﬁnitely many or unknown number of bins and very
few coincidences. We obtained closed form solutions for the estimates of entropy and its
variance. Moreover, we speciﬁed an easily veriﬁable condition (extremely long tails), under
which the estimator is not to be trusted. To our knowledge, this is the ﬁrst estimator that can
boast all of these features simultaneously. This brings us one more step closer to a reliable,
model independent estimation of statistics of undersampled probability distributions.

Acknowledgments

I thank William Bialek, the co–creator of the original NSB method, whose thoughtful ad-
vices helped me in this work. I am also grateful to Jonathan Miller, Naftali Tishby, and
Chris Wiggins, with whom I had many stimulating discussions. This work was supported
by NSF Grant No. PHY99-07949 to Kavli Institute for Theoretical Physics.

References

[1] R. J. Larsen and M. L. Marx. An introduction to mathematical statistics and its ap-

plications. Prentice Hall, Englewood Cliffs, NJ, 1981.

[2] I. Nemenman. Estimating entropy of a discrete variable. In preparation, 2002.
[3] G. Schwartz. Estimating the dimension of a model. Ann. Stat., 6:461–464, 1978.

[4] D. J. C. MacKay. Bayesian interpolation. Neural Comp., 4:415–447, 1992.
[5] V. Balasubramanian. Statistical inference, Occam’s razor, and statistical mechanics

on the space of probability distributions. Neural Comp., 9:349–368, 1997.

[6] I. Nemenman and W. Bialek. Occam factors and model independent Bayesian learn-

ing of continuous distributions. Phys. Rev. E, 65, 2002.

[7] I. Nemenman, F. Shafee, and W. Bialek. Entropy and inference, revisited. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information
Processing Systems 14, Cambridge, MA, 2002. MIT Press.

[8] K. Karplus. Regularizers for estimating distributions of aminoacids from small sam-
ples. Technical report, UC Santa Cruz, Computer Science Department, March 1995.
UCSC-CRL-95-11.

[9] D. Wolpert and D. Wolf. Estimating functions of probability distributions from a ﬁnite

set of samples. Phys. Rev. E, 52:6841–6854, 1995.

[10] K. Sj¨olander, K. Karplus, M. Brown, R. Hughey, A. Krogh, I. S. Mian, and D. Haus-
sler. Dirichlet mixtures: A method for mproving detection of weak but signiﬁcant
protein sequence homology. In Computer Applications in the Biosciences (CABIOS),
volume 12, pages 327–345, 1996.

[11] I. S. Gradshteyn and I. M. Ryzhik. Tables of integrals, series and products. Academic

Press, Burlington, MA, 6 edition, 2000.

[12] S. Ma. Calculation of entropy from data of motion. J. Stat. Phys., 26:221–240, 1981.
[13] T. Schurmann and P. Grassberger. Entropy estimation of symbol sequences. Chaos,

6:414–427, 1996.


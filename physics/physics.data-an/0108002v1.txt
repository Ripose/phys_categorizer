1
0
0
2
 
g
u
A
 
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
0
0
8
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

~αPDE: A New Multivariate Technique for Parameter Estimation

B. Knuteson1, H. E. Miettinen2, L. Holmstr¨om3

1Enrico Fermi Institute, University of Chicago
2Department of Physics and Astronomy, Rice University
3Rolf Nevanlinna Institute, University of Helsinki, Finland

Abstract

We present ~αPDE, a new multivariate analysis technique for parameter estimation. The method is based on a direct
construction of joint probability densities of known variables and the parameters to be estimated. We show how
posterior densities and best-value estimates are then obtained for the parameters of interest by a straightforward
manipulation of these densities. The method is essentially non-parametric and allows for an intuitive graphical
interpretation. We illustrate the method by outlining how it can be used to estimate the mass of the top quark,
and we explain how the method is applied to an ensemble of events containing background.

I. INTRODUCTION

In an earlier paper [1] we introduced the PDE (Prob-
ability Density Estimation) method, an essentially non-
parametric and multivariate method designed for iden-
tifying small signals among large backgrounds. The
method makes use of kernel density estimates for sig-
nal and background probability densities, and a simple
discriminant function is then used to classify candidate
events. The PDE method was applied successfully to the
search for the top quark at the Fermilab Tevatron, and
it is an integral part of a general search strategy [2] for
analyzing data from high-energy physics experiments.

In this paper we present ~αPDE, an extension of the
PDE method designed for parameter estimation, where
~α represents a vector of parameters to be estimated. In
many applications ~α is a single parameter, such as the
mass of an unstable particle detected through its decay
products. This non-parametric and multivariate method
may be particularly applicable to problems such as de-
termining the mass of the top quark in the upcoming
collider run (Run II) of the Fermilab Tevatron.

Multivariate methods are now widely recognized as be-
ing more powerful than univariate methods, and a non-
parametric method has the advantage that one need not
make assumptions about the forms of probability distri-
butions. Those who feel uneasy about the “black-box”
quality of neural networks should welcome the straight-
forward manipulation of probability densities used in this
method, and the intuitive graphical interpretation that
results. Because probability densities are constructed
and manipulated directly, obtaining any additional sta-
tistical information – conﬁdence intervals, for example –
is a straightforward exercise.

A typical parameter estimation problem is described
in Sec. II; our recipe for solving it is provided in Sec. III.
The salient features of the method and its potential ad-
vantages are summarized in Sec. IV.

II. THE PROBLEM

The next decade of high energy collider physics will
emphasize measurements and searches for new phenom-
ena at the scale of several hundred GeV. The existence of
a new particle at this scale can be convincingly demon-
strated by observing a peak in an invariant mass dis-
tribution, but the signature may be such that more in-
direct methods of establishing the particle’s existence,
and subsequently measuring parameters such as its mass
and couplings, are required. We introduce ~αPDE with
an example of this nature: the determination of the top
quark mass. Top quarks are pair-produced at the Fer-
milab Tevatron, each decaying promptly to a W boson
and a b quark. Each W boson in turn decays either to a
charged lepton and a neutrino, or to two quarks. Quarks
hadronize, appearing in the detector as collimated ﬂows
of energy (jets). The characteristic experimental signa-
ture for a top quark event is therefore a ﬁnal state con-
taining either an energetic lepton, missing transverse en-
ergy, and several energetic jets, or a ﬁnal state containing
two energetic leptons, missing transverse energy, and a
pair of jets; decays to six jets are diﬃcult to distinguish
from events in which no top quark was produced. The
application of selection criteria favoring events with jets
originating from b quarks enhances the fraction of top
quark events in the sample.

For the sake of simplicity we assume that two variables

1

~x = (x, y) have been identiﬁed for this analysis. This
pair might be the transverse energies of the lepton and
the leading jet; it might be the invariant mass of the
sub-leading jets and the transverse momentum of the W
boson; it might be the scalar sum of all jet transverse
energies and the output of a neural network built with
event-shape variables. No special assumptions about the
nature of these variables need be made.

III. THE RECIPE

of y, and the top quark mass m. The Monte Carlo events
are labeled with the index i (i = 1, .., N ); the three num-
bers corresponding to the ith event are then xi, yi, and
mi. Deﬁne the event vector ~vi for the ith Monte Carlo
event by

and deﬁne the training array T for the entire set of Monte
Carlo events by

~vi = (~xi, mi),

Tij = (~vi)j.

The goal is to construct a method that performs as
well as (or better than) such popular algorithms as neural
networks, but to keep the method suﬃciently simple that
it reads like a recipe. The recipe follows.

Here and below i ranges from 1 to N and indexes the
Monte Carlo events; j ranges from 1 to 3 and indexes the
components of the event vector ~v.

A. Specify p(m)

D. Calculate the covariance matrix

Having deﬁned the event vector ~v, calculate the mean

event vector

|I

This method has its roots in Bayesian statistics, and as
a result it has the advantage (disadvantage) of enabling
(requiring) the speciﬁcation of a function p(m
) that
represents the prior probability that nature has chosen
the top quark mass to be m.
here is used in standard
Bayesian notation to represent all assumptions implicit
in our speciﬁcation of this prior probability. The basic
assumptions contained in
will not change, so we drop it
from here on, writing simply p(m). A natural choice for
p(m), used when there is strong belief that the true mass
must lie somewhere between a and b but no reason to
prefer any value within that range over any other, is the
ﬂat prior: p(m) = 1

b−a for a < m < b, and 0 elsewhere.

I

I

=

~v
h

i

1
N

N

Xi=1

~vi

and construct the training covariance matrix

Σkl =

1
N

N

Xi=1

~v
((~vi)k − h

~v
ik)((~vi)l − h

il)

(4)

in the standard way. Σ is a 3 by 3 symmetric matrix,
Σ21 = Cov(x, y), and so on.

(1)

(2)

(3)

B. Generate Monte Carlo events

E. Estimate the joint density p(~v)

Monte Carlo events are generated with top quark
masses m pulled from the distribution p(m) speciﬁed
above. That is, the probability that an event with a
top quark mass between m and m + δm is generated is
p(m) δm. For each Monte Carlo event we calculate the
two variables ~x = (x, y).

A histogram in (x, y, m) ﬁlled with the generated
events approximates the joint density p(x, y, m). This
function has the property that, given an event in which
a top quark is produced and decays to the observed ﬁnal
state, the probability that the top quark mass was be-
tween m and m + δm, the ﬁrst variable between x and
x + δx, and the second variable between y and y + δy, is
simply p(x, y, m) δx δy δm.

C. Construct a training array T

In Sec. III B we imagined ﬁlling a three-dimensional
histogram in ~v with Monte Carlo events, and recognized
that the resulting histogram represents an estimation of
a probability density. A well-known technique in multi-
variate statistics involves estimating a probability density
not by ﬁlling a histogram, but rather by summing ker-
nels of probability placed around each point. A favorite
kernel choice is the multivariate gaussian:

K(~v) =

1
(√2πh)3

det(Σ)

p

exp

−

(cid:18)

~vT Σ−1~v
2h2

.
(cid:19)

(5)

The vector ~v is the same three-component vector deﬁned
above, and Σ−1 is the inverse of the training array co-
variance matrix Σ. The parameter h is known in the
language of density estimation as a smoothing parame-
ter; it controls the width of the kernels placed around
each point. Theoretical arguments suggest an optimal
N −1/(d+4) as a function of the number of
choice of h

Each of the N Monte Carlo events just generated is
characterized by three numbers: the value of x, the value

≈

2

data points N and the dimensionality d of the variable
space.∗

An estimate of the joint probability density p(~v) is then
obtained simply by summing kernels centered about each
of the N data points ~vi, so that

the variables ~x correlate with the true mass m. Given
variables appropriate for the problem at hand, ~αPDE
oﬀers a natural and intuitive way of estimating unknown
parameters.

p(~v) =

1
N

N

Xi=1

K(~v

~vi).

−

F. Compute ˆm

(6)

(7)

(8)

(9)

IV. CONCLUSIONS

The analysis method described here is quite general,
and can be used in the context of any parameter esti-
mation problem. The non-parametric approach used to
estimate probability densities is helpful when the distri-
butions under consideration do not lend themselves to
an obvious parameterization. ~αPDE allows the use of
several measured variables, and enables the simultane-
ous estimation of several parameters. The generalization
to arbitrary dimension is provided in Appendix A. Con-
ﬁdence intervals and moments are easily obtained from
simple manipulations of the joint probability density.

APPENDIX A: THE GENERAL MULTIVARIATE
CASE

For pedagogical reasons, ~αPDE has been introduced
through a speciﬁc example — determining the mass m of
the top quark from two measured quantities x and y —
and the expressions in the text are therefore speciﬁc to
that example. In this appendix we provide the formulae
for the general case.

In the general case, let each event be characterized by
d1 known variables ~x and d2 unknown parameters ~α. Let
d = d1 + d2, and let the d-dimensional event vector be
~v = (~x, ~α).

The ith Monte Carlo event is now described by the

event vector

(10)

and the entire Monte Carlo sample is described by the
training array

where j now ranges from 1 to d. The mean event vector
is

~vi = (~xi, ~αi),

Tij = (~vi)j,

=

~v
h

i

1
N

N

Xi=1

~vi

(A1)

(A2)

(A3)

A physicist attempting to measure the top quark mass
is interested in the posterior density p(m
In
~x) is the probability that the top quark mass
words, p(m
|
is m given that we have observed an event with variable
values ~x. This posterior density is easily obtained. The
probability of obtaining both ~x and m is equal to the
probability of obtaining ~x multiplied by the probability
of obtaining m given that you have obtained ~x:

~x) for m.
|

p(~x, m) = p(~x)p(m

~x),
|

and the probability of obtaining ~x is given by integrating
the probability of obtaining both ~x and m over all values
of m:

p(~x) =

′

′

p(~x, m

) dm

.

Z

Thus the posterior density p(m
density p(~x, m) simply by

~x) is related to the joint
|

p(m

~x) =
|

p(~x, m)
p(~x, m′) dm′ .

R

The best estimate ˆm for the mass of the top quark in a
single event is then the value of the mass that maximizes
this posterior density. The equation

p( ˆm

~x) = max
|

m

p(m

~x)
|

may be solved numerically for ˆm. Note that since the
denominator of Eq. 9 is independent of m, maximizing
the posterior density p(m
~x) is equivalent to maximiz-
|
ing the joint density p(~x, m), which we have constructed
explicitly.

The extent to which the posterior density p(m

~x) peaks
|
around the value ˆm depends, of course, on how strongly

and the training covariance matrix is

∗This expression for h∗ depends on assumptions about the
probability density that we have not made explicit, and is
not exact [3,4]. In practice, h may be optimized for any set
of Monte Carlo events by constructing and minimizing some
appropriate error estimate χ(h). For N = 105 and d = 3, the
optimal choice for h is roughly 0.20.

Σkl =

1
N

N

Xi=1

~v
((~vi)k − h

~v
ik)((~vi)l − h

il),

(A4)

as before, and the general multivariate gaussian is given
by

3

0.03

0.02

0.01

0
-2
-2

0.01
0.008
0.006
0.004
0.002
0
-2
-2

60

40

m

20

60

40

m

20

0
0

2
2

x
x

4
4

6
6

0

8

0
0

2
2

x
x

4
4

6
6

0

8

FIG. 1. A sample function ξ(x, m) that might be con-
structed from Monte Carlo events at masses m = 10, 20, 30,
40, and 50. Notice the ridges in this function, due to the fact
that it is constructed from events at speciﬁc masses.

FIG. 2. The density p(x, m) formed by rescaling the func-
tion ξ(x, m) shown in Fig. 1. Notice how this rescaling cor-
rects for the fact that only events at speciﬁc masses were used
in the construction of ξ(x, m).

K(~v) =

1
(√2πh)d

det(Σ)

p

exp

−

(cid:18)

~vT Σ−1~v
2h2

.
(cid:19)

(A5)

Finally, in Eqs. 9 and 10, m should be replaced by the
vector ~α.

APPENDIX B: ALTERNATIVE TO
GENERATING A RANDOM SAMPLE OF
MONTE CARLO EVENTS

In this appendix we describe a modiﬁcation to the pro-
cedure described in the text if practical constraints pre-
vent the generation of events pulled from a continuous
prior p(m), but allow the generation of events at q dis-
crete values mj, where j = 1, .., q.

Two changes are required in the ﬁrst ﬁve steps of the
recipe (Secs. III A–III E). First, it is assumed that prac-
tical constraints require Monte Carlo events to be gener-
ated at the discrete masses mj, rather than as described
in Sec. III B. Second, the function calculated in Eq. 6,
which may no longer be interpreted as a joint density,
should be re-labeled. For lack of a better alternative,
call it ξ(~v).

We now add a step 5 1

2 between Secs. III E and III F.
The function ξ(~v) is clearly not an appropriate density. If
events have been generated assuming ﬁve diﬀerent masses
mj, a graph of ξ(~v) might appear as shown in Fig. 1. We
see that the density has ridges along the values of m for
which events have been generated, with corresponding
valleys in the regions between these values.

An appropriately rescaled probability density p(~x, m)
can be generated by multiplying ξ(~v) by a normalizing
m-dependent factor s(m):

This normalizing factor will correct for the fact that val-
leys have been introduced into the density by only gen-
erating events at speciﬁc masses mj. The requirement
that

p(~x, m) d~x = p(m)

(B2)

Z

determines this normalizing factor uniquely. The desired
joint probability density p(~v) is then given by

p(~v) =

ξ(~v)p(m)
d~x′ ξ(~x′, m)

,

(B3)

R

and the ﬁnal step (Sec. III F) is exactly as before. The
rescaled density of Fig. 1 is shown in Fig. 2.

We mention brieﬂy a useful shortcut when calculating
integrals such as that appearing in the denominator of
Eq. B3. Multidimensional integrals are diﬃcult to cal-
culate in general, but this integral can be handled ana-
lytically provided one uses gaussian kernels. Assume as
in Appendix A that the vector of known variables ~x is
of d1 dimensions, that the vector of unknown variables
~α is of d2 dimensions, and that the Monte Carlo has a
covariance matrix Σ. Then the relevant formula is

K(~x, ~α) d~x =

Z

1
(√2πh)d2

det(Σ′)

exp

−

(cid:18)

~αT Σ′−1~α
2h2

,
(cid:19)

p

(B4)
where Σ′ is the d2 by d2 sub-matrix of Σ formed by re-
taining elements with row and column numbers larger
than d1.

APPENDIX C: BACKGROUND EVENTS

p(~x, m) = ξ(~x, m)s(m).

(B1)

In the text we considered the problem of determining
the top quark mass m for one candidate event. In a real

4

[1] L. Holmstr¨om, S. R. Sain, and H. E. Miettinen, Comp.

Phys. Comm. 88:195 (1995).

[2] DØ Collaboration, V. Abazov et al., submitted to Phys.

Rev. Lett., hep-ex/0106039 (2001).

[3] D. Scott. Multivariate Density Estimation. John Wiley &

[4] M. Wand and M. Jones. Kernel Smoothing. Chapman &

Sons, 1992.

Hall, 1995.

analysis there will be n such events, and of those some
fraction b are expected to be background events – events
that do not contain a top quark at all. This appendix
shows how to apply ~αPDE to a complete analysis.

Signal and background Monte Carlo events are gener-
ated and used to construct the signal and background
probability densities ps(~x, ~α) and pb(~x, ~α) as described in
Secs. III A–III E. From a careful analysis of background
eﬃciencies we determine the probability p(b) that a frac-
tion b of our events are background events. We label the
n data events that we observe by ~x1, .., ~xn.

is

The goal

to compute

the posterior density
~x1, .., ~xn). Since the observations ~x1, .., ~xn are as-
p(~α
|
~α, b) factors into a
sumed to be independent, p(~x1, .., ~xn|
product:

p(~x1, .., ~xn|

~α, b) =

~α, b).

p(~xi|

(C1)

n

Yi=1

~α, b) for the ith data event can be
The probability p(~xi|
written in terms of the signal and background probability
densities as

~α, b) = (1

p(~xi|
b) ps(~xi|
−
where p(~x
~α) = p(~x, ~α)/p(~α).
|
sance parameter b in Eq. C1 leaves

~α),

~α) + b pb(~xi|
Integrating out the nui-

(C2)

n
i=1 [(1

~x1, .., ~xn) = N p(~α)
p(~α
|

1
0 (
R
Q
b pb(~xi|
where N is a normalization factor ensuring that
~x1, .., ~xn) d~α = 1, and p(~α, b) = p(~α)p(b) is as-
p(~α
|
sumed.
R

−
~α)]) p(b) db,

b) ps(~xi|

~α)+

(C3)

The most likely values of the parameters ~α are then
those for which p(~α
~x1, .., ~xn) achieves its maximum; the
|
uncertainty on these values can be estimated from the
width of the peak. The expectation value

=

~α
h

i

Z

~x1, .., ~xn) d~α,
~α p(~α
|

(C4)

and covariance

σrs =

~α
ir)(αs − h
~α
(αr − h

~x1, .., ~xn) d~α (C5)
is) p(~α
|

Z

are easily computed, if desired — the ease with which
quantities such as these may be calculated is one of many
advantages of dealing directly with probability densities.
Perhaps the greatest advantage of constructing proba-
bility densities directly is the ability to use a graphing
package to visualize them. This method, even when ap-
plied in several dimensions, lends itself to an intuitive,
graphical interpretation.

5


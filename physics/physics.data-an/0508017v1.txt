5
0
0
2
 
g
u
A
 
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
1
0
8
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

On Consistent and Calibrated Inference about
the Parameters of Sampling Distributions

Tomaˇz Podobnik1), 2),

∗ and Tomi ˇZivko2),

†

1)Physics Department, University of Ljubljana, Slovenia
2)”Joˇzef Stefan” Institute, Ljubljana, Slovenia

Abstract

The theory of probability, based on very general rules referred to as the Cox-
P´olya-Jaynes Desiderata, can be used both as a theory of random mass phenomena
and as a quantitative theory of plausible inference about the parameters of sampling
distributions. The existing applications of the Desiderata must be extended in order
to allow for consistent inferences in the limit of complete a priori ignorance about
the values of the parameters. Since the limits of consistent quantitative inference
from incomplete information can clearly be established, the developed theory is nec-
essarily an effective one. It is interesting to note that when applying the Desiderata
strictly, we ﬁnd no contradictions between the so-called Bayesian and frequentist
schools of inductive reasoning.

Ljubljana, August 2005

∗e-mail: Tomaz.Podobnik@ijs.si
†e-mail: Tomi.Zivko@ijs.si

As for prophecies, they will pass away; as for tongues, they will
cease; as for knowledge, it will pass away. For we know in part and
we prophesy in part.

1 Corinthians 13, 8-9.

1 Introduction

The term inference ([1], p. 436) stands for two kinds of reasoning ([2], p. xix): deduc-
tive or demonstrative reasoning whenever enough information is at hand to permit it, and
inductive or plausible reasoning when not all of the necessary information is available.
“The difference between the two kinds of reasoning is great and manifold. Demonstrative
reasoning is safe, beyond controversy, and ﬁnal. Plausible reasoning is hazardous, con-
troversial, and provisional. Demonstrative reasoning penetrates the science just as far as
mathematics does, but is in itself (as mathematics is in itself) incapable of yielding essen-
tially new knowledge about the world around us. Anything new that we learn about the
world involves plausible reasoning, which is the only kind of reasoning for which we care
in everyday affairs. Demonstrative reasoning has rigid standards, codiﬁed and clariﬁed
by logic (formal or demonstrative logic1), which is the theory of demonstrative reasoning.
The standards of plausible reasoning are ﬂuid, and there is no theory of such reasoning
that could be compared to demonstrative logic in clarity or would command comparable
consensus.” So George P´olya in the Preface to his Mathematics and Plausible Reasoning
([3], p. v).

In the second volume [4] of the work he collects patterns of plausible reasoning and
dissects our intuitive common sense into a set of elementary qualitative desiderata that
represent basic rules of inductive reasoning. When formulating his views in mathemati-
cal terms ([4], Chapter XV), he recognizes his rules to be in a close agreement with the
calculus of probability as developed by Laplace in the late 18th century [5], but P´olya
advances a thesis that when applying the calculus of probability to plausible reasoning, it
should be applied only qualitatively (see, for example, [4], pp. 136-139), i.e. numerical
values should be strictly avoided.

In the present paper we formulate a quantitative theory of inductive reasoning, in
particular a consistent theory of quantitative inference about the parameters of sampling
distributions. In Section 2 we adopt the basic qualitative rules of plausible reasoning, the
so-called Cox-P´olya-Jaynes Desiderata, and review some of the well known results of
their direct applications, such as Cox’s and Bayes’ Theorems. In addition, we clearly
establish the lack of such applications in the limit of complete prior ignorance about
the inferred parameter, with such ignorance representing the natural starting point of an
inference. In other words, Bayes’ Theorem that can be used for updating probabilities,
cannot directly be used in the step of probability assignment.

We carefully deﬁne the state of complete prior ignorance about the inferred parameter

1A branch of mathematics, also referred to as deductive logic.

1

in Section 3. In particular, throughout the present paper we never question the speciﬁed
model, i.e. the form of the sampling probability distribution is always known beyond the
required precision. What we are completely ignorant about at the beginning of reasoning
is the value of the inferred parameter.

In Section 4 we deﬁne location, dispersion and scale parameters and brieﬂy review
some of the properties of sampling distributions determined by these parameters. A sep-
arate section is devoted to the invariance of such distributions, a property that turns out
to be of decisive importance when constructing a consistent theory of inductive reason-
ing. We also show that invariance of both the form of the sampling distribution and its
domain, under a continuous (or Lie) group, is found only in problems of inference about
parameters that can be reduced to inference about location parameters.

In Section 6 we extend the applications of the Desiderata in order to allow for a con-
sistent assignment and not just for updating of probability distributions for the inferred
parameters. The so-called Consistency Theorem is obtained by making use of Bayes’
Theorem and by requiring that if a conclusion can be reasoned out in more than one way,
then every possible way must lead to the same result, in particular by requiring logically
independent pieces of information to be commutative. The form of the Consistency The-
orem is very similar to that of Bayes’ Theorem, but there is also a fundamental difference
between the two since in the former a consistency factor is used instead of the prior
probability distribution. Hence, the consistency factor cannot be subject to any of the
requirements such as normalization or invariance with respect to a one-to-one parameter
transformation, that are perfectly legitimate for well deﬁned probability distributions.

Instead, the form of the consistency factor is determined in a way that preserves the
logical consistency of our reasoning. By consistency we mean that, among other things,
if in two problems of inference our state of knowledge is the same, then we must assign
the same probabilities in both. In Section 7 we ﬁnd that the basic Desiderata uniquely
determine the form of consistency factors for sampling distributions whose form and do-
main are invariant under a Lie group
of transformations. It is therefore only for those
problems reducible to inference about location parameters that we can give an assurance
of consistency to our parameter inference. The form of consistency factors for such dis-
tributions is then determined throughout Sections 8-10, while in Section 11 we discuss
under what circumstances the present theory is guaranteed to be consistent in the case of
pre-constrained parameters.

G

In Section 12 we make veriﬁable predictions that are based on the presented theory,
thus elevating its status above the level of a mere speculation. The predictions are made in
terms of long run relative frequencies. We show that a consistent inference is necessarily
also a calibrated one, i.e. that consistently predicted frequencies always coincide with (a
one-to-one function of) actual frequencies of occurrence. This important result speaks
in favour of a complete reconciliation between the so-called Bayesian and frequentist
schools of plausible reasoning.

In counting experiments the invariance of sampling distributions is clearly missing and
so the consistency factors cannot be uniquely determined by following the basic Desider-
ata. The remedy is to collect enough data so that the sampling distribution approaches its

2

dense limit. Until then, our reasoning is necessarily based on some ad hoc prescriptions
that can (and very often will) lead to logically unacceptable results, as is demonstrated
in Section 14. In such cases it might therefore be the best to refrain from quantitative
inferences, i.e. to remain on a qualitative level.

In Section 15 we brieﬂy review conceptual and practical difﬁculties and paradoxes
caused by using Bayes’ Theorem instead of the Consistency Theorem in the limit of
complete ignorance about the inferred parameters. The problem is contained in the self-
contradicting non-informative prior probability distributions. Long-lasting arguments
over this subject led, inter alia, to a split in the theory of inductive reasoning and it is in
this way that the Bayesian and the frequentist schools emerged. In our view, the splitting
into (at ﬁrst glance) almost diametrically opposed schools is highly artiﬁcial, provided
that the two schools strictly obey their basic rules, i.e.
that they refrain from using ad
hoc shortcuts on the course of inference. For regardless how close to our intuitive reason-
ing these ad hoc procedures may be, how well they may have performed in some other
previous inferences, and how respectable their names may sound (e.g.
the principle of
insufﬁcient reason or its sophisticated version - the principle of maximum entropy, the
principle of group invariance, the principle of maximum likelihood, and the principle of
reduction), they will in general inevitably lead not only to contradictions between the two
schools of thought, but also to inferences that are neither consistent nor calibrated.

There are also two appendices to the present paper. The ﬁrst one contains, for the
sake of completeness, a proof of Cox’s Theorem, while in the second one, the so-called
marginalization paradox, is extensively discussed.

2 Basic rules and their applications

Let n hypothesis or an event be an unambiguous proposition A , i.e. a statement that
can be either true or false. As we are in general not certain about either of the two
possibilities, the classical logic of deductive reasoning [6] is to be extended in order to
allow for plausible or inductive inferences based on incomplete information.

A

Let (a state of ) information I summarize the information that we have about some
of propositions Ai, called the basis of I, and their relations to each other. The
set
domain of I is the logical closure of
. A state of information
is not restricted to containing only deductive information; it can also contain imprecise
or insufﬁcient information that says nothing with certainty, but still affects one’s opinion
about a certain proposition. Such kind of information can also be updated: we write
I ′ = BI for a state of information obtained from I by adding additional information (
evidence) that proposition B is true.

, that is, the union of

A

A

Now, let I be a state of information of a given person and A be a proposition in the
domain of I. Then, we introduce the (degree of ) plausibility (A
I) as a degree of belief
|
of the person that A is true given the information in I. We say that I is the knowledge
base for the assigned plausibility (A
I). In the present paper we assume all considered
|
plausibilities to be subject to very general requirements that can be listed in the following

3

three basic Cox-P´olya-Jaynes Desiderata ([2],

1.7, pp. 17-19):

§

I. Degrees of plausibilities are represented by real numbers.

That is, plausibilities are numerically encoded states of knowledge about propositions.
Formally, an assigned plausibility can be regarded as a function:

(

) :

|

A × I −−−→

R ,

where

is the set of possible states of information about some set

of propositions.

In addition to the ﬁrst Desideratum, we adopt two natural but nonessential conven-

A

I

tions:

a greater degree of belief shall correspond to a greater number;

the plausibility of a hypothesis that we are certain about (e.g. the plausibility of a
tautology) equals 1.

By referring to the conventions as being nonessential we mean that we could have equally
well adopted a convention that the plausibility of a tautology equals a different positive
constant, or that a greater degree of probability should correspond to a smaller number.
Nevertheless, according to the above Desideratum and the two conventions, the assigned
plausibilities can range within an interval [F, 1], where F < 1 is plausibility of the false
proposition.

We say that a state of information I is consistent if there is no proposition A for which
I) for A being true and ( ¯A
plausibilities (A
I) for A being false can both equal unity.
|
|
That is, based on consistent information both a proposition and its denial cannot be true.
In order to avoid ambiguities, we restrict ourselves to considering only plausibilities that
are assigned upon consistent states of information.

II. Assignment of plausibilities must be in qualitative correspondence with common sense.

In our case, the concept of common sense stands for the following conditions:

Since plausible reasoning is a generalization of deductive logic, it must be consis-
tent with the results of Boolean algebra [7] - the algebra of deductive logic.

Microscopic changes in the knowledge base should not cause macroscopic changes
in the plausibilities assigned. In addition, for every considered proposition A there
I), with
exists some set of possible consistent states of knowledge
[F, 1]
I
(continuity requirement).

such that (A
|
, can take any of the values within a continuous interval (a, b)
⊆

∈ I

I

We assume that the degree of belief ( ¯A
|
the plausibility (A
|
into I ′ in such a way that the plausibility of A is increased, (A
|

I) that A is false depends in some way on
I) that A is true. In addition, when old information I is updated
I), it

I ′) > (A
|

4

•

•

•

•

•

must produce a decrease in the plausibility that A is false, ( ¯A
I). That is,
|
we assume that there exists a continuous, twice differentiable, strictly decreasing
function S of plausibility (A
|

I ′) < ( ¯A
|

I), such that

( ¯A
|

I) = S

I)

(A
|

.

•

|

(cid:2)

(cid:3)
Plausibility (AB
I), assigned to a hypothesis AB that two non-contradictory hy-
potheses, A and B , are simultaneously true, is assumed to be completely deter-
mined by the values of (A
BI). Then it can be shown
|
(see Lemma 1 in Appendix A) that evident inconsistencies are avoided only if (AB
depends solely on (A
|

AI), i.e. if there exists a function H such that

I) and (A
|

I) and (B

AI), (B

I), (B

|

|

|

I)

|
I) = H

(AB

|

I), (B

AI)] .

|

(A
|

(cid:2)

I ′) > (A
|

We further require for the function H to be strictly increasing and twice differen-
tiable in both of its arguments. By strictly increasing we mean that if the knowl-
edge base I is updated to I ′ in such a way that the plausibility of A is increased,
AI),
(A
|
this can only produce an increase in the plausibility that both A and B are true,
I), in which the equality can hold only if B is impossible given
(AB
A and I. Likewise, given information I ′′ such that (A
AI ′′) >
|
(B

AI) remains the same, (B

I), but the plausibility (B

AI), we require that (AB

I ′′) = (A
|

AI ′) = (B

I) and (B

(AB

(AB

I).

I ′)

≥

I ′′)

|

|

|

|

|

|

|

|

≥

|

III. Assignment of plausibilities must be a consistent procedure:

a) If a conclusion can be reasoned out in more than one way, then every possible

way must lead to the same result.

b) When assigning plausibilities, we must always take into account all of the evi-
dence we have relevant to a hypothesis. We do not arbitrarily ignore some of
the information and base our conclusion only on what remains.

c) Equivalent states of knowledge must be always represented by equivalent plau-
sibility assignments. For example, if in two problems our state of knowledge is
the same (except perhaps for the labelling of the propositions), then we must
assign the same plausibilities in both.

The requirement of consistency plays a special rˆole among various requirements which
a theoretical system, or an axiomatic system, must satisfy. It can be regarded as the ﬁrst
of the requirements to be satisﬁed by every theoretical system, be it empirical or non-
empirical. As for an empirical system, however, besides being consistent, it should satisfy
a further criterion: it must be falsiﬁable ([8],
24, pp. 91-92). According to this criterion,
§
statements, or systems of statements, convey information about the empirical world only
if they are capable of clashing with experience; or more precisely, only if they can be
systematically tested, that is to say, if they can be subjected (in accordance with a method-
ological decision) to tests which might result in their refutation. In so far as a scientiﬁc

5

statement speaks about reality, it must be falsiﬁable: and in so far as it is not falsiﬁable,
it does not speak about reality [9]. Since every degree of belief is to be assigned on the
basis of available evidence, our aim is clearly to formulate an empirical theory of plau-
sible reasoning, i.e. a theory that speaks about reality. We therefore add an additional
requirement, an operational Desideratum, to the original Cox-P´olya-Jaynes Desiderata:

IV. A theory of plausible inference must specify operations that ensure the falsiﬁability of

every assigned degree of plausibility.

Richard Cox showed [10] that the following can be deduced when plausibilities satisfy

Desiderata I.-III.b:

1. Suppose that plausibilities (A
|

I) can be assigned.
|
Then there exists a continuous strictly increasing function P of each of these plau-
sibilities,

BI) and (AB

I), (A
|

AI), (B

I), (B

|

|

such that

and

P (AB

I) = P (A
|

|

|

I) P (B

AI) = P (B

I) P (A
|

|

BI)

(1)

P : [F, 1]

[0, 1] ,

−−−→

P (F) = 0 .

Every compositum of function P and a plausibility assignment (A
,
|
or P (A
I) in a simpliﬁed notation, is referred to as the probability for A to be
(cid:3)
|
true given available information I, and the above equation (1) is referred to as the
product rule. Note that every probability is at the same time also a plausibility, i.e.
it is consistent with the basic Desiderata.

(A
|

I), P

I)

(cid:2)

2. Probabilities P (A
|

up to unity:

I) and P ( ¯A
|

I) sum up to the probability of a certain event, i.e. sum

P (A
|
which is referred to as the sum rule.

I) + P ( ¯A
|

I) = 1 ,

(2)

The above results are usually referred to as Cox’s Theorem (for a proof of the Theorem
see Appendix A). Note that the product and the sum rule, being only relations between
probabilities, do not of themselves assign numerical values to any of the probabilities
arising in a speciﬁc problem. The only numerical values, considered thus far, are those
corresponding to certainty and impossibility, one and zero, respectively, of which the
former is a mere consequence of a convention, adopted along with Desideratum I, rather
than required by the rules of the Theorem. Moreover, it is hardly to be supposed that
every reasonable expectation should have a precise numerical value [10], nor is there any
guarantee that every state of information about a particular proposition A will meet the
continuity requirement of the common sense Desideratum.

The product and the sum rule are unique in the sense that any set of rules for manip-
ulating our degrees of belief, represented by real numbers, is either isomorphic to (1) and

6

(2), i.e. different from (1) and (2) only in form but not in content, or inconsistent. Thus,
we could have chosen any other set of plausibilities
P that are one-to-one functions of the
corresponding probabilities P , and then adequately adapt the product and the sum rule.
For example, if we choose

I) such that

e

P (A
|

P (A
|
with a being an arbitrary positive number, the corresponding product rule for
e
the same while the sum rule reads:

P (A
|

p

I)

I)

≡

e

a

P remains

e

P a(A
|

I) +

P a( ¯A
|

I) = 1 .

As another example, we could have chosen zero to represent the plausibility of a proposi-
e
tion that we are certain about, and a plausibility

P such that:

e

Then, the appropriate product and sum rules for

P (A
|

I)

ln P (A
|

I) .

≡

e
P would have read:

P (AB

I) =

|

P (A
|

P (B

AI) =
e

|

P (B

I) +

|

P (A
|

BI)

e
I) +

and

e

e

exp

P (A
|

I)

e
+ exp

e
P ( ¯A
I)
|

e

= 1 .

e

e

(cid:8)

(cid:9)

(cid:9)

The freedom to choose an arbitrary plausibility function to represent our degree of
(cid:8)
belief is analogous to gauge invariance in ﬁeld theories where potentials (i.e. functions
that the ﬁelds are expressed by) are not rigidly ﬁxed. The predictions of ﬁeld theories
if the
are unchanged if the potentials are transformed according to speciﬁc rules, i.e.
potentials are subjects to gauge transformations. Then we choose one particular form of
potential, i.e. we choose a particular gauge, not because it is more correct than any other,
but because it is more convenient for the particular problem that we are solving within a
ﬁled theory. For the same reason we choose probabilities and not any other plausibilities
to represent our degrees of belief: not because they are more correct, but because it is
for probabilities that the product and the sum rule take the simplest forms. We comment
on the choice of probability, that we adhere to throughout the present paper, again in
Section 12 when we discuss the relation between probability and frequency.

Once the probabilities are chosen from all possible plausibility functions, i.e once the
gauge is ﬁxed, the incompleteness of the concept of plausibility is removed: the product
and the sum rules, (1) and (2), are the fundamental equations of probability theory, while
all other equations for manipulating probabilities follow from their repeated applications.
For example, it is in this way that we obtain the general sum rule that either A or B is
true:

P (A + B

I) + P (B
Suppose now that propositions A1, A2, ..., An form an exhaustive set of mutually ex-
clusive propositions. The propositions are mutually exclusive if the evidence I implies
that no two of them can be true simultaneously,

I) = P (A
|

P (AB

I) .

I)

−

|

|

|

(3)

P (AiAj|

I) = 0 for i

= j ,

7

6
and exhaustive if one of them must be true,

n

n

P

I

Ai|

=

P (Ai|

I) = 1 .

i=1
X
A classical textbook example of such a set would be six hypotheses Ai arising from toss-
ing a die, where index i corresponds to the particular number thrown.

i=1
X

(cid:17)

(cid:16)

The hypotheses of a set are unambiguously classiﬁed by assigning one or more nu-
merical indices. Deciding between the hypotheses Ai and estimating the index i are prac-
tically the same thing:

(4)

(5)

(6)

with the corresponding normalization

P (Ai|

I) = p(i
|

I) ,

n

i=1
X

I) = 1 .

p(i
|

We denote probabilities by capital P when arguments are propositions, and by small p
when arguments are numerical values. By assigning probability to every possible value
of the index i we specify how our degree of belief is distributed among the hypotheses of
the set Ai, i.e. we specify the (sampling) probability distribution for i.

The distribution of one’s degree of belief in hypotheses labelled by different values
of index i may be equivalently represented by the cumulative distribution function (cdf),
deﬁned as

F (i)

≡

p(j

I) = P (j

|

I) ,

i
|

≤

i

j=ia
X

where permissible values of the index i range from ia to ib. The probability for i taking a
value between i1 and i2 can thus be expressed by cdf’s simply as:

P (i1 < i

I) =

i2|

≤

I) =

p(i
|

I)

p(i
|

−

p(i
|

I) = F (i2)

F (i1) .

−

i2

i=i1
X

i1

i=ia
X

In addition, for an exhaustive sets of hypotheses the normalization condition (5) implies:

F (b) =

I) = 1 .

p(i
|

In many cases of practical importance the hypotheses of a set become very numerous
and dense. For example, when predicting the decay time of an unstable particle, we start
with a countable set of hypotheses Ai that the decay time of that speciﬁc particle would
be, for instance, i seconds. But such a set is not an exhaustive one since the decay time
could also be i + 1
4 seconds. Further reﬁnement of the original propositions leads
to a dense set of hypotheses where neighbouring hypotheses, i.e. hypotheses with nearly

2 or i + 1

i2

i=ia
X

ib

i=ia
X

8

the same index value, become barely distinguishable. In such cases there cannot be a
sharply deﬁned hypothesis that is strongly favoured over all others. Instead, it only makes
sense to consider probabilities for index i in a certain interval of its permissible range.
In this way index i transforms into a continuous variable xi with a continuous sampling
probability distribution:

p(xi|

I) = P

x

(xi, xi + dx)

I

.

∈
Every continuous distribution can be expressed by a probability density function (pdf),
f (x
|

I):

(cid:1)

(cid:0)

|

Using pdf’s we can now rewrite the product rule (1) as

I)

p(x
|

≡

f (x
|

I) dx .

f (x1x2|

I) = f (x1|

I) f (x2|

x1I) = f (x2|

I) f (x1|

x2I) ,

and the normalization (5) by replacing summation over discrete indices i by integration
over a dense domain x:

f (x′

I) dx′ = 1 .

|

Zx

x

The sum in the cdf (6) for a discrete variable i is replaced by an integral for a contin-

uous variable x:

F (x)

f (x′

I) dx′ = P (x′

≡

xa

|

I) ,

x
|

≤

Z
where x ranges from xa to xb. Since the probability P (x1 < x < x2|
by the cdf’s as:

I) can be expressed

(7)

(8)

(9)

(10)

(11)

P (x1 < x < x2|

I) =

I) dx = F (x2)

F (x1) ,

−

the normalization (10) implies:

x2

f (x
|

x1

Z

xb

xa

Z

|

F (xb) =

f (x′

I) dx′ = 1 .

Suppose we have a continuous variable y that is related functionally to a variable x by

a one-to-one relation:

x = x(y) ; y = y(x) ,

y being differentiable in x and vice versa. Let y1 = y(x1) and y2 = y(x2). Since the
inferences about x and y are based on equivalent pieces of information I and I ′ (the
transformations of the variables correspond only to relabellings of hypotheses), Desider-
atum III.c implies the following equality:

P

x

(x1, x2)

I

=

∈

(cid:0)

|

(cid:1)

P
P

(

y
y
(cid:0)
(cid:0)

9

(y1, y2)
(y2, y1)

I ′
I ′

|
|

∈
∈

;
;

dx
dy > 0
dx
dy < 0

(cid:1)
(cid:1)

,

(12)

where

P

x

(x1, x2)

I

I) dx ,

f (x
|

(cid:0)
y

P

(cid:0)
y

P

∈

∈

∈

=

=

=

(cid:1)

(cid:1)

|

|

|

x2

x1

Z

y2

y1

Z

y1

y2

Z

|

|

dx
dy
dx
dy

(y2, y1)

I ′

f (y

I ′) dy ;

< 0 ,

(y1, y2)

I ′

f (y

I ′) dy ;

> 0 ,

(13)

(cid:0)
I ′) being the pdf’s for x and y, respectively. That is, assigned prob-
I) and f (y

with f (x
|
abilities must be invariant under variate transformations.

(cid:1)

|

As an example that illustrates the above reasoning, imagine two scientists, say Mr. A and
Mr. B, measuring decay times of unstable particles. Each time they start their clocks at
the moment when a particle is produced, but the clocks run at different speeds, so Mr. A
measures a decay time ti of the i-th particle, and Mr. B

t′i = ati ,

(14)

where a is an arbitrary positive constant.

I) and P (t = t′i|

Since t is a continuous random variable, there is not much point in considering prob-
abilities P (t = ti|
I), since both, t = ti and t = t′i are events with zero
probability (probability measure). Instead, we should consider probabilities for measuring
t in certain intervals (ti, ti+dt) and (t′i, t′i +dt′), where dt and dt′ are the widths of the two
intervals. Due to the different speeds of the two clocks, equivalent events, i.e. equivalent
time intervals, are not labelled equally by the two observers: the interval (ti, ti + dt) of
Mr. A corresponds to the interval (t′i, ti + dt′) = (ati, ati + adt) of Mr. B. For example, in
the case of a = 5, the interval no. 10 of Mr. A is split into intervals 46-50 by Mr. B. That
is, the variate transformation (14) implies

Then, since the two propositions, t
(ti, ti + dt) and t′ ∈
in labelling, Desideratum III.c implies the two probabilities P
P
, assigned by Mr. A and Mr. B, respectively, to be equal:

(t′i, t′i + dt′) differ only
I
(ti, ti + dt)
t
and
|

∈

∈

t′ ∈

I ′
(t′i, t′i + dt′)
|

(cid:0)

P

(cid:1)
t

(cid:0)

I
(ti, ti + dt)
|

∈

= P

t′

(cid:0)
I ′
(t′i, t′i + dt′)
|

∈

.

(cid:1)

Note that the logic behind such a reasoning is very similar to the logic of Poincar´e’s
relativity principle [11] of the special theory of relativity stating that no preferred inertial
frame (or no absolute time scale) exists.

(cid:1)

(cid:0)

The variate transformation (14) is linear, but it need not be so, as long as it remains

one-to-one. Suppose that Mr. B considers a probability distribution of a variate

(cid:1)

(15)

If Mr. A divides a range (0, 1] of his variate t into n intervals of equal widths

dt′ = adt .

y

ln t .

≡

dt =

1
n

,

10

Mr. B’s n corresponding intervals dy of highly non-uniform widths

dy =

dt = e−

y dt

1
t

, 0] of y. Despite the non-uniformity of the
cover the inﬁnite corresponding range (
interval widths, the transformation (15) still represents a mere relabelling of the hypothe-
(yi, yi + dy) (with
(ti, ti + dt) and y
ses: t and y are equivalent variates, and t
yi dt) are equivalent propositions. Imagine that because of using
yi = ln ti and dy = e−
the transformed variate y instead of t, Mr. B considers himself inferior to Mr. A. But since
the transformation (15) is one-to-one, the inverse transformation

−∞

∈

∈

t = ey

always exists: Mr. B can always obtain ti from yi and then make his inference from ti
instead of from yi.

To sum up, the logic behind Desideratum III.c implies equivalence of all variates, con-
nected via one-to-one transformations: while the speciﬁed models (i.e. forms of pdf’s, see
below) together with the range of the variates may be changed (this is indicated by using
symbol I ′ instead of I), the probability content must be invariant under such transforma-
tions.

The equality (12) is assured for all intervals (x1, x2) and
pp. 20-28)

y(x1), y(x2)

only if ([12],

(cid:0)

(cid:1)

(16)

f (y

I ′) = f (x
|

|

I)

dx
dy

.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Indeed:

P

y

(y1, y2)

I ′

=

f (y

I ′) dy =

y2

y1

Z

x2

x1

Z

|

f (x
|

(cid:1)

=

Z
I) dx = P

y2

I)

f (x
|

dx
dy

dy

(cid:12)
(cid:12)
(x1, x2)
(cid:12)

(cid:12)
(cid:12)
I
(cid:12)

y1

(cid:0)

x

∈

|

(cid:1)

for variate transformations with positive dx/dy, and

P

y

(y2, y1)

I ′

=

f (y

I ′) dy =

y1

|

y2

Z

x1

−

x2

Z

(cid:1)

=

y1

I)

f (x
|

dx
dy

dy

y2

Z

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(x1, x2)
(cid:12)
(cid:12)

I) dx = P

x

f (x
|

∈

(cid:0)

I

|

(cid:1)

|

in the case of negative dx/dy. Inversely, f (x
|

I) is expressed in terms of f (y

I) as

∈

(cid:0)

∈

(cid:0)

|

|

(17)

where

dy/dx
|

|

is the reciprocal of

dx/dy

dy
dx

,

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
1

f (x
|

I) = f (y

I ′)

|

,

|

=

|

dy
dx

dx
dy

−

,

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
11

(cid:12)
(cid:12)
(cid:12)

and

and

since any one-to-one transformation from x to y and then from y to x must restore the
original distribution, and hence

dy
dx
Note that equal information implies equal probabilities, while in general it does not imply
equality of pdf’s.

dx
dy

= 1 .

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

In the bivariate case where

y(1) = y(1)(x(1), x(2)) , y(2) = y(2)(x(1), x(2)) ;
x(1) = x(1)(y(1), y(2)) , x(2) = x(2)(y(1), y(2)) ;

the relations between the pdf’s for (x(1), x(2)) and (y(1), y(2)), f (x(1), x(2)
are the following:

|

I) and f (y(1), y(2)

I),

|

f (y(1), y(2)

I ′) = f (x(1), x(2)

I)

J

,

|

|

|

f (x(1), x(2)

I) = f (y(1), y(2)

I ′)

J ∗

,

|

|

(18)

(19)

with the absolute values of the derivatives,
absolute values of the corresponding Jacobians,

|

dx/dy

|

|
and

|
|
dy/dx
|

|

, being replaced by the

J =

∂(x(1), x(2))
∂(y(1), y(2)) ≡

J ∗ = J −

1 =

,

∂y(1)x(1) ∂y(2)x(1)
∂y(1)x(2) ∂y(2)x(2)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂(y(1), y(2))
∂(x(1), x(2))

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

respectively.

Special attention is needed if the derivatives in a univariate case, or the Jacobians in a
multivariate case, change sign within the domain of the pdf since in that case the variate
transformations x
x are not one-to-one any more. We will meet such a
difﬁculty in Section 4 where it will be overcome on account of the special symmetry of
the speciﬁc transformation.

y and y

→

→

θI) = f (x
|

In the present paper we consider sampling probability distributions, either discrete
θI) dx, that can be speciﬁed by a mathematical
θI) or continuous p(x
p(i
|
|
function, determined by the values of its parameters θ2.
In such cases assignment of
probabilities to hypotheses from a given set reduces to estimation of the parameters θ of
the distribution: what we try to achieve is to assign probabilities to different values of
trhe parameters, i.e. to specify the probability distribution for θ. Note that the probability
distributions for parameters are subjects to the same Desiderata as the distributions for
sampling variates. In this paper we will focus on the parameters with dense domains,
where the corresponding probability distributions are continuous:

2We adhere to the common and useful convention of using Greek letters θ, µ, σ, τ , ν and λ for parame-

p(θ

x1I) = f (θ

x1I) dθ .

|

|

(20)

ters throughout the paper.

12

The probability for θ (20) is assigned upon information that we explicitly split in our
notation into evidence x1 from the measurement of the quantity x whose probability dis-
tribution is determined by θ, and the additional relevant information I. The reason for
such splitting of the information will become evident below when we derive Bayes’ The-
orem.

The pdf for θ (20) is subject to the usual normalization,

f (θ′

x1I) dθ′ = 1 ,

|

Zθ

where integration is performed over the complete range of θ. In addition, in the case of
assigning probabilities for two parameters, θ and ν, simultaneously, the product rule (1)
can be applied:

|

f (θν

x1I) = f (θ

x1I) f (ν

θx1I) = f (ν

x1I) f (θ

νx1I) .

(22)

|
θx1I) and f (θ

|
|
νx1I) being properly normalized according to

|

Then, with the factors f (ν
(21), it is easy to see that the marginalization procedure yields

|

|

f (θ′ν

x1I) dθ′ = f (ν

x1I) ,

f (θν′

x1I) dν′ = f (θ

x1I) .

|

|

Zθ

Zν

|

|

f (θ

x1I) =

f (θ

θI)

,

I) p(x1|
|
I)
p(x1|

f (θ

x1I) =

f (θ

θI)

,

I) f (x1|
|
I)
f (x1|

|

|

The product rule can also be applied for assigning probabilities to θ and x1:

|
The above equation can be rewritten into Bayes’ Theorem [13, 14],

|

p(θx1|

I) = f (θ

I) dθ p(x1|

θI) = p(x1|

I) f (θ

x1I) dθ .

also referred to as the principle of inverse probability (see [15],
domain of x is also dense, the theorem can be written in terms of pdf’s only:

§

1.22, p. 28). When the

We interpret the theorem in the following way ([16],

1.3, p.2). We are interested in
the probability distribution for θ and begin with the initial or prior probability, also re-
I). It is based on any additional
ferred to as the probability a priori, whose pdf reads f (θ
information I that we possess beyond the immediate data x1. Thus, p(θ
I) dθ
is the probability for θ prior to taking evidence x1 into account. The posterior probabil-
x1I) dθ, is the probability for θ
ity, also referred to as the probability a posteriori, f (θ

I) = f (θ

§

|

|

|

|

13

(21)

(23)

(24)

(25)

(26)

|

θI),
posterior to adding evidence x1 to our previous information. The likelihood, p(x1|
tells us how likely x1 is observed, given the value θ of the parameter that determines the
probability distribution for x. According to Bayes’ Theorem (25), the only consistent
x1I), is by multiplying the prior pdf by the likeli-
way of obtaining the posterior pdf, f (θ
hood, which is usually referred to as the likelihood principle (see, for example, [2],
8.5,
2.0, p. 57): ”Consequently the whole of the
p. 250). Or, in the words of Jeffreys ([15],
§
information contained in the observations that is relevant to the posterior probabilities of
different hypotheses3, is summed up in the values that they give to the likelihood.” Note
that with the basic Desiderata being adopted, the term “likelihood principle” becomes in-
appropriate and might even be misleading, since the fact that all of the information that
can be extracted from the datum x1 is contained in the value of the appropriate likelihood,
is a mere consequence of application of the basic Desiderata, rather than an additional
I) can be
principle (i.e. a Desideratum) on its own. The denominators p(x1|
obtained by the normalization requirement (21) as:

I) and f (x1|

§

and

I) =

p(x1|

f (θ′

I) p(x1|

|

θ′I) dθ′

f (x1|

I) =

f (θ′

I) f (x1|

|

θ′I) dθ′ .

Zθ

Zθ

(27)

(28)

Bayes’ Theorem is thus a rule for updating the information that an inference is based
upon. Formally, it is just a special case of the product rule of the Cox theorem. The
latter also ensures that (25) is the only consistent way of updating the information and,
consequently, our probability distribution for θ.

Suppose that after x1 we learn a new piece of information, x2, that we would like to
x1I) serves as a prior pdf, i.e. pdf for θ prior

include in our inference about θ. Then, f (θ
to taking x2 into account. According to (25), the posterior pdf then reads:

|

f (θ

x1x2I) =

|

f (θ

|

x1I) p(x2|
I)
p(x1x2|

θx1I)

f (θ

=

|

I) p(x1|
p(x1|

θI) p(x2|
θx1I)
I)
I) p(x1x2|

,

(29)

θx1I) is the likelihood for x2 given θ, x1 and the additional information I.

where p(x2|
In the limit of our complete ignorance about the value of a parameter θ prior to the ﬁrst
evidence x1, when I merely stands for our admission that we possess no prior informa-
tion relevant to θ apart from the speciﬁed form of the sampling distribution, the complete
procedure for manipulating probabilities by using the product rule (1), or Bayes’ Theo-
rem (25), breaks down. This is a direct consequence of the fact that we can only assign
probabilities for hypotheses on the basis of available relevant information: ignorance I
thus allows for no probability assignment f (θ
I). In this event, both the product rule (1)
and its derivative, Bayes’ Theorem (25), lack their vital components and cannot be used.
In other words, Bayes’ Theorem only allows for updating probabilities that were already

|

3i.e. of different values of the inferred parameter(s)

14

assigned prior to their updating, and therefore need be amended for the limit of complete
prior ignorance, which is a natural starting point for every sequential updating of infor-
mation. Our goal in the following sections is to make such an amendment in a consistent
way, i.e. to establish when and how probabilities can consistently be assigned.

3 Complete ignorance about parameters

As a starting point, we would like to specify precisely what we mean and what we do
not mean by the limit of complete ignorance about the value of a parameter of a given
probability distribution for x. First of all, ignorance about a distribution parameter is not
a synonym for absolute ignorance in every possible respect. For example, throughout the
paper we assume as a working hypothesis that the probability for x is distributed in a form
that is completely known but for the value of its parameter(s) θ, i.e. the chosen form of
the probability distribution for x together with its domain - the ranges of the sampling
variate(s) and the inferred parameter(s), (xa, xb) and (θa, θb), is always assumed to be
appropriate beyond the required precision. This assumption is explicitly indicated by the
symbol I that every probability (or probability density) is conditioned upon.

What we are completely ignorant about is the value of θ. There is no information at
our disposal that would enable us to assign a probability distribution for θ: it can take any
value within its permissible range (θa, θb). Since the value of θ is completely unknown,
then the distribution for x becomes undetermined. This is where we then start collecting
data that we would like to use for a consistent inference about θ.

The situation, described above, is an ideal limiting case that can serve as a reasonable
approximation for many real-life situations. For example, even before the ﬁrst measure-
ment of a decay time of an unknown unstable particle, there is not much room for doubt
about the form of the decay time distribution. Due to past experiences with all other unsta-
ble particles we feel almost completely certain that the distribution would be exponential
(we come back to this point in Section 16 where the possibility of assigning probabilities
to speciﬁed models is considered).

But before the ﬁrst measurement, the parameter of the distribution, the average decay
time τ , is completely unknown, so we do not know what value of the ﬁrst measure decay-
time, t1, to expect: it could be anything between zero and inﬁnity. Inversely, before the
ﬁrst measurement of t, the parameter τ can take any value in the same interval. Note that
a hint of a symmetry between the collected data and the inferred parameters is present in
the foregoing reasoning. The concept will be extensively exploited during the following
sections.

With I representing only knowledge about the type of sampling distribution, different
θI can be enumerated according to the values of x
Iθ of possible different states of knowledge
Ix and
xI) can both
θI) and f (θ

xI and Iθ ≡
states of knowledge Ix ≡
and θ, respectively. In this way, the sets
become subsets of real numbers,
Ix,
be formally expressed as functions

R, and the pdf’s f (x
|

Iθ ⊆

|

f (x
|

θI), f (θ

xI) : R

R

|

×

−−−→

R .

15

Many of the derivations in the present article automatically require dense ranges for
both x and θ, as well as differentiability of pdf’s f (x
x1I). The problems
|
accompanying discrete sets of hypotheses are extensively discussed in Section 14, where
inferences about parameters of counting experiments are considered.

θI) and f (θ

|

4 Location, scale and dispersion parameters

We pay special attention to the so called location, scale, and dispersion parameters of
probability distributions. A parameter µ of a sampling distribution is a location parameter,
and a parameter σ is a dispersion parameter, if the pdf for x takes the form

µσI) =

φ

f (x
|

1
σ

x

µ

−
σ

,

(cid:17)

(cid:16)

with the range for x stretching over the whole real axis, with the range of µ being an
interval (µa, µb) on the real axis and with the permissible range of σ being an interval
(σa, σb) on the positive half of the real axis. For the time being, let the permissible range
of µ coincide with the entire real axis, (µa, µb) = (
), and the range of σ with
its entire positive half, (σa, σb) = (0,
), while we postpone a discussion about pre-
constrained parameters until Section 11.

−∞

∞

∞

,

A bivariate pdf for two independent variates, x(1) and x(2), both being subject to the
same pdf of the form (30), according to the product rule (9) equals the product of univari-
ate pdf’s, f (x(1)

µσI) and f (x(2)

µσI):

|

|

f (x(1)x(2)

|

µσI) = f (x(1)
= f (x(1)
1
σ2 φ

=

|

µσI) f (x(2)
µσI) f (x(2)
|
x(1)
µ
−
σ

φ

|

|

x(1)µσI)

µσI)
x(2)
−
σ

(cid:16)

(cid:17)

(cid:16)

µ

.

(cid:17)

The pdf for transformed variates x(1) and x(2), ¯x and s, where

(30)

(31)

x(1) + x(2)
2

,

¯x

s

≡

≡

x(1)

|

x(2)

|

=

−
2

(x(1)
(x(2)

(

x(2))/2
x(1))/2

; x(1) > x(2)
; x(2) > x(1) ,

−
−

or, inversely,

x(1,2) =

¯x
¯x

(

s
s

±
∓

; x(1) > x(2)
; x(2) > x(1) ,

16

can be calculated according to (18):

f (¯xs

µσI) = 4 f

|

(32)

(33)

µσI

x(1)(¯x, s) x(2)(¯x, s)
|
µ
µ

¯x+s
σ
s
σ

−

¯x

−

−

µ

φ
φ

¯x

s
−
σ
¯x+s
σ

−

−

µ

(cid:0)

(

¯x

¯x

µ

−

(cid:1)
σ + s
(cid:1)
σ
s
(cid:1)
σ
s
(cid:1)
σ

µ
−
σ −
µ
+
−
σ

(cid:0)
φ
(cid:0)
φ
(cid:0)
(cid:0)
φ

¯x

¯x

s
σ

(cid:1)
µ
(cid:1)
−
σ −
µ
σ + s
−
(cid:1)
σ
µ
¯x
(cid:1)
−
σ −

φ
φ

(cid:0)
(cid:0)

φ
φ
(cid:0)
¯x
(cid:0)

4
σ2

4
σ2

=

=

=

(

4
σ2 φ
(cid:16)
1
φ
σ2

; x(1) > x(2)
(cid:1)
; x(2) > x(1)

; x(1) > x(2)
; x(2) > x(1)
s
σ

s
σ
(cid:17)
The factor 4 in (32) that ensures appropriate normalization, arises as a product of the
absolute value of the Jacobian,

(cid:16)
e

−
σ

=

µ

¯x

,

(cid:17)
.

(cid:16)

(cid:17)

J =

∂(x(1), x(2))
∂(¯x, s)

=

2
−
+2

(

; x(1) > x(2)
; x(2) > x(1) ,

−

µσI

x(1)(¯x, s) x(2)(¯x, s)

and an additional factor of 2, the latter being a consequence of the symmetry of the pdf
x(2). With the
for x(1) and x(2) with respect to the change of sign of the difference x(1)
sign of the difference inverted, the determinant (33) inverts its sign, too, but its absolute
value, as well as the value of the pdf f
(see (32)), remain un-
changed. Therefore both the range with the positive and the range with the negative sign
of the Jacobian can be simultaneously taken into account simply by multiplying the pdf
f

x(1)(¯x, s) x(2)(¯x, s)
When inferring the parameters of a sampling distribution of the form (30) it may
happen that the value of one of the two parameters is known to a high precision. In such
cases the parameter with the precisely determined value is ﬁxed and we only make an
inference about the remaining one. Let ﬁrst the dispersion parameter σ be ﬁxed to σ0, e.g.
to 1, so that the pdf for x, given the possible value of µ and the ﬁxed value σ0,
1
σ0

by an additional factor of 2.

µσ0I) =

f (x
|

= φ(x

(34)

µσI

−
σ0

µ) ,

−

µ

φ

x

(cid:0)

(cid:1)

(cid:0)

(cid:1)

|

|

is a function of x and µ only. The ﬁxed parameter (σ0 in the present case) is usually
(though not always) omitted from explicit expressions.
According to (34) and (11), the cdf for x reads:

(cid:16)

(cid:17)

F (x, µ, σ0) =

f (x′

µσ0I) dx′ =

φ(x′

µ) dx′ =

φ(u) du = Φ(x

µ) .

x

Z

−∞

|

x

Z

−∞

−

x

µ

−

Z

−∞

(35)
Note that the form Φ(x
µ) of the above cdf implies the corresponding pdf to be of the
form (34), i.e. implies µ to be a location parameter of a sampling probability distribution
for x. Indeed:

−

−

f (x
|

∂
∂x

∂
∂x

d
du

∂u
∂x

d
du

µI) =

F (x, µ) =

Φ(x

µ) =

Φ(u)

=

Φ(u) = φ(u) ,

−

17

(36)

(37)

where

u

x

µ .

≡

−

Since the integrand in (35), i.e. the pdf for x, is a positive function, and the upper bounds
of the integral are strictly decreasing with the increase in the parameter, the cdf is evi-
dently strictly decreasing in µ.

With the location parameter being ﬁxed to µ0, say to 0, the pdf (30) for x reduces to:

σµ0I) =

f (x
|

1
σ

φ

x

µ0

−
σ

=

φ

1
σ

x
σ

.

(cid:16)
When the range of the random variate is bound to the positive half of the real axis,

(cid:16)

(cid:17)

(cid:17)

the corresponding parameter τ of the sampling distribution,

t

∈

(0,

) ,

∞

τ µ0I) =

f (t
|

1
τ

φ

t
τ

(cid:16)

(cid:17)

,

is usually referred to as the scale parameter. Note that in symmetric cases when

f (x

σµ0I) = f

(x

µ0)

σµ0I

,

−
(cid:0)
the pdf (36) for x with ﬁxed µ0 can be reduced to a pdf (37) without any loss of either
generality or information. Namely, the pdf for a transformed variate

−

−

(cid:1)

|

µ0|

t =

x

|

−

,

µ0|

reads:

σµ0I) = 2 f (x

f (t
|

µ0|

−

σµ0I)

dx
dt

=

φ

2
σ

t
σ

=

φ

1
σ

(cid:16)

(cid:17)

t
σ

,

(cid:17)

(cid:16)
e

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

where the factor of 2 arises due to shrinkage of the sample space. In such cases dispersion
parameters are evidently equivalent to scale parameters.

Any sampling probability distribution determined by a scale parameter, τ , and a ﬁxed
location parameter µ0, can be further transformed into a probability distribution deter-
3.2.2,
mined by a location parameter ν (see, for example, [17],
pp. 22-23):

4.4, p. 144 or [18],

§

§

and a ﬁxed dispersion parameter λ0, say λ0 = 1. Namely, a substitution

yields:

f (z

νλ0I) = f (t
|

|

τ µ0I)

= ez

ν φ

−

ν

ez

−

φ(z

ν) .

−

≡

(38)

(cid:0)

(cid:1)

e

ν = ln τ ,

z = ln t

dt
dz

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
18

As was the case with cdf (35), the cdf for t,

F (t, τ, µ0) =

f (t′

τ µ0I) dt′ =

dt′ =

φ(u) du ,

(39)

t

0
Z

|

t

1
τ

φ

t′
τ

(cid:16)

(cid:17)

0
Z

t
τ

0
Z

is also monotonically decreasing with increasing value of the parameter.

Some of the most important continuous sampling distributions are determined by one
or more parameters of the above mentioned types (see [19],
4.2, pp. 58-83). In addition,
all distributions determined by either location, scale or dispersion parameters share a very
important property: they all belong to the invariant families of distributions.

§

5 Invariant distributions

Let

θI) = φ(x, θ)

f (x
|

be a pdf for a random variable from a dense sample space X that is determined by the
value of parameter θ from the parameter space Θ. Let there exist a group
of transfor-
mations ga ∈ G

of the sample space into itself:

G

X ,

ga : X

ga : x

−−−→

−−−→

ga(x)

y ,

≡

is a group, it is closed
where index a denotes the particular element of the group. Since
under composition of transformation, i.e. a composition gc of every pair of transforma-
tions ga, gb ∈ G

, gc = gbga, such that

G

gc(x) = gbga(x) = gb

ga(x)

,

is also contained in

(cid:0)
. In addition, the group also contains an identity ge such that

(cid:1)

G

and the inverse transformation g−
a

1

for any ga such that

ge(x) = x ,

x

∀

∈

X ,

1

a ga = gag−
g−

a = ge .

1

As a consequence, the transformations ga are one-to-one, i.e. ga(x1) = ga(x2) implies
X such that ga(x2) = x1
x1 = x2, and onto X, i.e. for every x1 ∈
(see, for example, [17],

X there exists an x2 ∈

4.1, p. 143).

Since the transformation y = ga(x) is one-to-one, the pdf for the transformed variate

§

according to (16) and (17) reads:

f (y

θI ′) = f

ga(x)

|

(cid:0)
= φ(x, θ)

θI ′

|
(cid:1)
g′a(x)

= f (x
|
= φ

−

1

θI)

1

−

dy
dx
(cid:12)
(cid:12)
1
a (y), θ
g−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g′a(x)

1

−

.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

19

(cid:0)

(cid:1)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(40)

In addition to
G
space Θ into itself,

, let there exist also a set ¯
G

of transformations ¯ga of the parameter

Θ ,

¯ga : Θ

¯ga : θ

−−−→

−−−→

¯ga(θ)

ν .

≡

Then:

f (y

νI ′) = f

|

ga(x)
1

|
a (y), ¯g−
g−
(cid:0)

¯ga(θ)I ′
1
a (ν)
(cid:1)

= φ

= φ

x(y), θ(ν)
1

g′a(x)
(cid:0)

−

φ

≡

1

−

g′a(x)
ga(x), ¯ga(θ)
(cid:1)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

If for all x

X, and θ

∈

∈

(cid:0)
Θ, and for every ga ∈ G
ga(x), ¯ga(θ)) = φ
φ

(cid:1)(cid:12)
(cid:12)

ga(x), ¯ga(θ)

,

(cid:12)
(cid:0)
there exists ¯ga ∈
e
(cid:12)

¯
G

(cid:1)

such that

(41)

(42)

the family of distributions f (x
|
p. 144 and [20],

(cid:0)
e
23.10, pp. 300-301).

θI) is said to be invariant under the group

([17],

4.1,

(cid:0)

(cid:1)

G

§

§

If a family of distributions is invariant under

of transformations of Θ
into itself is also a group, usually referred to as the induced group ([20],
23.10, p. 300).
Namely, according to the deﬁnition of invariance, if the pdf for x is given by φ(x, θ), the
pdf for ga(x) is given by φ
= gbga(x) is given
. From the equality of the two it
by both φ
follows that
(cid:0)

. Hence, the pdf for gb
gbga)(x), gbga(θ)
(cid:1)

ga(x), ¯ga(θ)
and φ

gb(ga(x)), ¯gb(¯ga(θ))

, then the set ¯
G

ga(x)

G

(cid:1)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

§

(cid:0)
gbga = ¯gb¯ga .

This shows that ¯
G
inverses if we let gb = g−

is closed under composition.

It also shows that ¯
G
a and note that ¯ge is the identity in ¯
.
G

1

For example, a sampling distribution for ¯x and s (32), determined by the values of a
location parameter µ and a dispersion parameter σ, is invariant under the group of simul-
taneous location and scale transformations:

is closed under

where

By ﬁxing the dispersion parameter, the symmetry of the pdf with respect to the scale trans-
formation is broken, leaving only the symmetry with respect to a simultaneous translation
of x and µ by an arbitrary real number b:

¯x

s
µ

ga,b :

(

¯ga,b :

−−−→

−−−→
−−−→

−−−→

ga,b(¯x) = a¯x + b

ga,b(s) = as
¯ga,b(µ) = aµ + b

,

,

(

σ

¯ga,b(σ) = aσ

a

(0,

) and b

(

,

) .

∈

∞

∈

−∞

∞

gb : x

¯gb : µ

−−−→

−−−→

gb(x) = x + b ,

¯gb(µ) = µ + b .

20

(43)

(44)

When, on the other hand, the location parameter is ﬁxed to µ0 = 0 and the dispersion (or
scale) of the distribution is unknown, the appropriate pdf f (x
σµoI) (37) is still invariant
|
under the scale transformation:

ga : x

¯ga : σ

−−−→

−−−→

ga(x) = ax ,

¯ga(σ) = aσ .

(45)

Let now f (x
|

that

θI) be an invariant sampling distribution and let F (x, θ) be its cdf such

F (x, θ) =

f (x′

θI) dx′ =

φ(x′, θ) dx′ ,

where xa is the lower bound of the sample space. Then the cdf for y = ga(x), given
ν = ¯ga(θ), reads:

x

xa

Z

y

ya

Z

|

|

x

xa

Z

ga(x)

ya

Z

(cid:0)

F (y, ν) = F

ga(x), ¯ga(θ)

=

f (y′

νI ′) dy′ =

φ

ga(x′), ¯ga(θ)

d

ga(x′)

,

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

where ya is the lower bound of the range of y. It is easy to show that: the lower and the
upper bound of the range of x, xa and xb, become transformed into the bounds of y, ya
and yb:

ya =

ga(xa) ; g′a(x) > 0
ga(xb) ; g′a(x) < 0

(

and

yb =

ga(xb) ; g′a(x) > 0
ga(xa) ; g′a(x) < 0

(

,

and that the cdf for y, given ν, is related to the cdf for x, given θ, as:

F (y, ν) = F

ga(x), ¯ga(θ)

=

(cid:0)

(cid:1)

Indeed:

ga(x)
ga(xa) φ
ga(x)
ga(xb) φ
(R
R

(cid:0)

y′, ¯ga(θ)
y′, ¯ga(θ)
(cid:0)

dy′ = F (x, θ)

; g′a(x) > 0
F (x, θ) ; g′a(x) < 0

.

dy′ = 1

−

(cid:1)

(cid:1)

ga(x)

F

ga(x), ¯ga(θ)

F

ga(xa), ¯ga(θ)

=

f

y′

¯ga(θ)I ′

dy′

(cid:0)

−

(cid:1)

(cid:0)

(cid:0)
x′, θ)

φ

(cid:1)
g′a(x′)

1 d

−

ga(x′)

(cid:12)
(cid:12)

(cid:0)

(cid:1)

Z

ga(xa)
ga(x)

(cid:1)

|

=

=

=

Z

ga(xa)
x

(cid:0)

(cid:12)
(cid:12)

φ(x′, θ) dx′

xa

Z
F (x, θ) ,

±

±

where the positive and the negative sign correspond to g′a(x) > 0 and to g′a(x) < 0,
respectively. Setting x to the upper bound xb of its range, the above equation reads:

F

ga(xb), ¯ga(θ)

F

ga(xa), ¯ga(θ)

=

F (xb, θ) =

1 .

±

±

(cid:0)

−

(cid:1)

(cid:0)

(cid:1)

21

Since the cdf’s are limited within [0, 1], this completes the proof by implying

F

ga(xa), ¯ga(θ)

=

and F

ga(xb), ¯ga(θ)

=

0 ; g′a(x) > 0
1 ; g′a(x) < 0

(

1 ; g′a(x) > 0
0 ; g′a(x) < 0

(

(cid:1)
(cid:0)
and ¯ga ∈
for every ga ∈ G
relations. Let a probability distribution for x be invariant under
continuous groups such that partial derivatives

Θ.
A very important corollary - the Existence Theorem - can be deduced from the above
be

¯
, and for all θ
G

and ¯
G

, and let

∈

G

G

(cid:1)

(cid:0)

ga(x)

∂
∂a
¯
, with both derivatives always being different from zero
exist for every ga ∈ G
G
and ¯
and ﬁnite. In other words,
7.1-7.2,
G
pp. 126-130). In addition, let the range (xa, xb) of the sampling variate also be invariant
under

are to be Lie groups (see, for example, [21],

and ¯ga ∈
G

∂
∂a

¯ga(θ)

, i.e.

and

§

G

ga(xa) =

xa
xb

(

; g′a(x) > 0
; g′a(x) < 0

and

ga(xb) =

xb
xa

(

; g′a(x) > 0
; g′a(x) < 0

Then the cdf for y = ga(x), given ν = ¯ga(θ), can be rewritten as:

F

=

ga(x), ¯ga(θ)

ga(x)
ga(xa) φ
ga(x)
ga(xb) φ
(R
R
which permits the following conclusions: the cdf F
parameter a of the transformations, i.e.

y′, ¯ga(θ)
y′, ¯ga(θ)
(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

dy′ = 1

dy′ = F (x, θ)

; g′a(x) > 0
F (x, θ) ; g′a(x) < 0

−
ga(x), ¯ga(θ)

(cid:0)

(cid:1)

is independent of the

.

,

d
da

F

ga(x), ¯ga(θ)

= 0 ,

and the parameter a enters F

ga(x), ¯ga(θ)

only through ga(x) and ¯ga(θ), i.e.

(cid:0)

(cid:1)

F

ga(x), ¯ga(θ)

(cid:0)
= F1

(cid:1)
ga(x), ¯ga(θ)

d
da

∂
∂a

ga(x) + F2

ga(x), ¯ga(θ)

¯ga(θ) ,

∂
∂a

(cid:0)

(cid:1)
where Fi denotes differentiation with respect to the i-th argument of F (we adhere to this
notation throughout the present paper, whatever the function and the arguments may be).
Then, by combining the two conclusions and by setting a = e we obtain:

(cid:0)

(cid:0)

(cid:1)

(cid:1)

where the derivatives h′(x) and k′(θ) of functions h(x) and k(θ) are deﬁned as reciprocals
of the corresponding inﬁnitesimal operators of the Lie groups

F1(x, θ) k′(θ) + F2(x, θ) h′(x) = 0 ,

and ¯
:
G

G

1

−

h′(x)

dh(x)

≡

dx ≡

∂
∂a

ga(x)

(cid:20)

22

(cid:21)

a=e
(cid:12)
(cid:12)
(cid:12)

(46)

(47)

and

and

k′(θ)

dk(θ)

≡

dθ ≡

∂
∂a

(cid:20)

¯ga(θ)

1

−

.

(cid:21)

a=e
(cid:12)
(cid:12)
(cid:12)

G(x, θ)

h(x)

k(θ) ,

≡

−

By deﬁning a function G(x, θ),

(46) can be further reduced to

F1(x, θ) G2(x, θ)

F2(x, θ) G1(x, θ) = 0 ,

−

or to a functional determinant (see [22],

7.2.1, p. 325),

§
F1(x, θ) F2(x, θ)
G1(x, θ) G2(x, θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0 .

F1(x, θ) = α(x, θ) G1(x, θ)

F2(x, θ) = β(x, θ) G2(x, θ) ,

With G1(x, θ) = h′(x) being different from zero, F1(x, θ) and F2(x, θ) can be expressed
as

(48)

(49)

(50)

(51)

(52)

which, inserted in (50), yield:

G1(x, θ) G2(x, θ)

α(x, θ)

β(x, θ)

= 0 .

−

Since this is to be true for any θ and x, α(x, θ) and β(x, θ) must be the same functions:

(cid:0)

(cid:1)

α(x, θ) = β(x, θ) .

Taking this into account, we multiply equations (51) and (52) by dx and dθ, respectively,
so that their sum reads:

dF (x, θ) = α(x, θ) dG(x, θ) ,

implying the distribution function F (x, θ) to be a function of a single variable G(x, θ)
(49),

F (x, θ) = Φ

G(x, θ)

= Φ

h(x)

k(θ)

.

(53)

By choosing

−

(cid:0)
h(x) and µ

(cid:1)

(cid:0)

k(θ) ,

z

(cid:1)

≡
the cdf F (x, θ) of an invariant sampling distribution thus reduces to

≡

which is a cdf of the variate z and a location parameter µ (c.f. eq. (35)). The above
reasoning can be summarized as

F (x, θ) = Φ(z

µ) ,

−

23

Theorem 1 A sampling distribution for a continuous variate x, given a continuous pa-
rameter θ, with both its form and its domain being invariant under a Lie group
, is
µ) to a sampling
z and θ
necessarily reducible (by separate transformations x
distribution for z with the parameter µ being a location parameter.

→

→

G

For example, a pdf f (t
|

τ µ0I) for t (37) with τ being a scale parameter and with the
location parameter µ0 being ﬁxed to zero, is invariant under the group of scale transfor-
mations (45). Then, according to (47) and (48), h′(t) and k′(τ ) read:

h′(t) =

and k′(τ ) =

1
t

1
τ

,

which, in order to reduce the example to the problem inference about a location parameter
µ, implies the appropriate transformations of the variate t and the parameter τ :

z = h(t) = ln t and µ = k(τ ) = ln τ .

Indeed, this is in perfect agreement with equation (38) of Section 4.

In the following two sections we will see that the invariance of sampling distribu-
tions is indispensable when constructing a logically consistent theory of inference about
parameters.

6 Consistency Theorem

Suppose that before we received the ﬁrst evidence, x1, we had been completely ignorant
about the value of the parameter θ that determines the probability distribution for x. We
had only known the type of sampling distribution for x and the permissible range (θa, θb)
of the parameter. Let the probability for x taking the value x1 in a discrete case, or taking
the value in the interval (x1, x1 + dx) in a continuous case, be denoted by Φ(x1, θ):

p(x1|

θI) = Φ(x1, θ) .

In this section we prove the following proposition, henceforth referred to as the Consis-
tency Theorem:

Theorem 2 In order to meet the consistency Desideratum, the pdf for θ based on x1 only,
must be directly proportional to the likelihood (54).

Proof. After having made the ﬁrst observation, we know the type of sampling distribution
and the values of x1. Therefore, the pdf for θ given evidence x1 will be proportional to a
function

Φ(x1, θ),

e

whose form we would like to determine. The denominator ζ(x1) is just a normalization
constant due to (21),

(54)

(55)

(56)

f (θ

x1I) =

|

Φ(x1, θ)
ζ(x1)
e

,

ζ(x1) =

Φ(x1, θ′) dθ′ ,

Zθ

e
24

and contains no information about θ.

Let now x2 be another piece of evidence, independent of x1, that we would like to
include in our inference about θ. Since x2 is independent of x1, and subject to the same
probability distribution (54) as x1, its likelihood reads:

p(x2|

θx1I) = p(x2|

θI) = Φ(x2, θ) .

In the ﬁrst section we saw that the only consistent way of updating pdf for θ is the one in
accordance with Bayes’ Theorem (29). With f (θ
x1I) taking the role of the prior pdf for
θ, the pdf posterior to including x2 into our reasoning about θ is written as:

|

f (θ

x1x2I) =

|

Φ(x1, θ) Φ(x2, θ)
ζ(x1, x2)

,

with the normalization constant ζ(x1, x2) being

ζ(x1, x2) =

Φ(x1, θ′) Φ(x2, θ′) dθ′ .

Nothing prevents us from reversing the order of taking the two pieces of information,

x1 and x2, into account, which results in the following pdf for θ:

f (θ

x2x1I) =

|

Φ(x2, θ) Φ(x1, θ)
ζ(x2, x1)

,

with the appropriate normalization constant ζ(x2, x1),

ζ(x2, x1) =

Φ(x2, θ) Φ(x1, θ) dθ .

e

e

Zθ

e

Zθ

e

Moreover, the consistency Desideratum III.a requires equality of the two results, (57) and
(58):

or

f (θ

x1x2I) = f (θ

x2x1I) ,

|

|

Φ(x1, θ) Φ(x2, θ)
ζ(x1, x2)

=

Φ(x2, θ) Φ(x1, θ)
ζ(x2, x1)

.

The ratio of eq. (59) and its derivative with respect to θ yields

e

e

Φ′(x1, θ)
Φ(x1, θ) −
e

Φ′(x1, θ)
Φ(x1, θ)

=

Φ′(x2, θ)
Φ(x2, θ) −
e

Φ′(x2, θ)
Φ(x2, θ)

,

where we use the notation

e

Φ′(x, θ)

e
d
Φ(x, θ) .
dθ

≡

25

(57)

(58)

(59)

(60)

(61)

(62)

Evidently, in order to ensure equality in (60) for all possible values of x1 and x2, the
left and the right side of the equation must be independent of x1 and x2, respectively, but
can depend on the value of the parameter θ.

Note that at this point, the two sides of equation (60) can, in principle, also depend on the
values xa, xb, θa and θb, determining the admissible ranges of the sampling variate and
of the parameter. However, in the following sections we will see that in all problems of
parameter inference that can be consistently solved, there is no such explicit dependence.

Taking this dependence into account by introducing a function h(θ) we obtain

d
dθ

d
dθ

ln

Φ(x, θ) =

ln Φ(x, θ) + h(θ) ,

and, after integration of the latter,
e

where π(θ) is a consistency factor,

e

Φ(x, θ) = k π(θ) Φ(x, θ) ,

π(θ)

exp

h(θ) dθ

,

≡

(cid:26)Z
and k an arbitrary integration constant. That is, the consistency factor is determined only
up to an arbitrary constant factor.

(cid:27)

The consistency factor is differentiable by construction. From the form of π(θ) (62)
it is also obvious that if k is chosen to be positive, k π(θ) is positive for every θ deﬁned.
Consequently, the normalization factor ζ(x),

ζ(x) = k

π(θ′) Φ(x, θ′) dθ′ ,

Zθ

being an integral of a product of positive factors (61), is also a strictly positive quantity
for every x deﬁned.

By inserting the solution (61) into (55), we can ﬁnally write:

f (θ

xI) =

|

k π(θ)
ζ(x)

Φ(x, θ) =

k π(θ)
ζ(x)

θI)

p(x
|

(63)

which completes the proof of the Consistency Theorem.

The result is valid for p(x
θI) being the likelihood of either a discrete or dense variable x.
|
For the latter, the Consistency Theorem can be rewritten by replacing the likelihood with
the appropriate likelihood density, f (x
|

θI),

f (θ

xI) =

|

k π(θ)
ζ(x)

θI) =

p(x
|

k π(θ)
k η(x)

θI) =

f (x
|

π(θ)
η(x)

θI) ,

f (x
|

(64)

26

(65)

(66)

where η(x) is the corresponding normalization factor,

η(x)

≡

Zθ

π(θ′) f (x1|

θ′I) dθ′ .

Evidently, the normalization constant is also determined up to a constant factor k, i.e., the
consistency factor π(θ) and the normalization factor η(x) are determined up to the same
factor.

Similarly, in terms of posterior probabilities and likelihoods instead of the correspond-

ing densities, the Theorem reads:

p(θ

xI) =

|

k π(θ) dθ
k η(x) dx

θI) =

p(x
|

π(θ) dθ
η(x) dx

θI) .

p(x
|

The form of the Consistency Theorem (66) reminds very much of that of Bayes’ The-
orem (25). In both cases, within a speciﬁed model, the complete information about the
inferred parameter θ of the model that can be extracted from a measurement x, is con-
θI). But there is also a fundamental
tained in the value of the appropriate likelihood, p(x
|
and very important difference between the two Theorems: while f (θ
I) in Bayes’ The-
orem represents the pdf for θ prior to including evidence x in our inference about θ, the
consistency factor π(θ) in the Consistency Theorem is just a proportionality coefﬁcient
between the pdf for θ and the likelihood function. The form of the factor depends on the
only relevant information I that we possess before the ﬁrst datum x1 is collected, i.e. it
depends on the speciﬁed sampling model.

|

In Section 15 we comment on how overlooking this difference led to a long-lasting
confusion in plausible reasoning. Before that we show under what conditions the factors
π(θ) can be consistently determined, and uniquely determine them for such cases by
following the basic Desiderata.

7 Objective inference and equivalence of information

According to the deﬁnition of probability adopted in the ﬁrst section, every assigned prob-
ability is necessarily subjective: no probability distribution can be assigned independently
of the experience of the person who is expressing his or her degree of belief. In the words
of Bruno de Finetti ([23], Preface, p. x): “Probability does not exist”, meaning that there
is no probability per se. For example, when there is not enough relevant information at
our disposal, we are simply not in a position to make any probabilistic inferences. That
is, even when lacking, information should never be confused with our hopes, fears, value
judgments, etc. Since no matter how carefully these are speciﬁed, they still represent
mere personal biases, prejudices and speculations.

The general Desiderata represent the rules that we have to obey in order to preserve
consistency of inference, so it is evident that the adjective subjective does not stand for
arbitrary. In fact, in accordance with Desideratum III.c, our goal is that inferences are to

27

be completely objective in the sense that if in two problems the state of knowledge of a
person making the inference is the same, then he or she must assign the same probabilities
in both. The goal of the present and the following sections is to show when and how in-
formation I about the speciﬁed model and its domain, within the framework of our basic
Desiderata (i.e without any additional ad hoc assumptions), uniquely determine the form
of consistency factors, the latter being indispensable at the starting point of any param-
eter inference. Within the Desiderata, only III.a and III.c directly consider equalities of
probabilities and can thus provide equations that could determine the form of consistency
factors. Since the requirements of Desideratum III.a were extensively exploited already
throughout the previous section when the Consistency Theorem was derived, it is only
III.c that is left at our disposal to obtain the desired functional equation for π(θ).

Let

θI) = φ(x, θ)

f (x
|

be a sampling pdf for x whose parameter θ we would like to infer. We saw in the preceding
section that in the case when this can be done in a consistent way, the pdf for θ must take
the form:

f (θ

xI) =

|

π(θ)
η(x)

θI) =

f (x
|

π(θ)
η(x)

φ(x, θ) ,

(67)

where η(x) is the usual normalization factor

η(x) =

π(θ′) f (x
|

Zθ

Zθ

θ′I) dθ′ =

π(θ′) φ(x, θ′) dθ′ .

(68)

Equation (67) with the unknown pdf f (θ
xI) clearly does not determine uniquely the form
of the consistency factors: as long as the normalization integral (68) exists, π(θ) can be
any positive and differentiable function of θ. Additional constraints (functional equations)
are therefore needed to reduce all these functions to a single consistent function, i.e. to
the only function that is consistent with our Desiderata.

|

In the case there exists a group

of transformations ga of the sample space such that

y = ga(x), the above pdf for θ can be expressed as

G

f

θ

ga(x)I

=

|

(cid:0)

(cid:1)

g′a(x)
η(x)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

where

π(θ) φ(x, θ)

g′a(x)

1

−

=

π(θ) f (y

θI) ,

|

g′a(x)
η(x)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1 .

(cid:12)
(cid:12)
φ(x, θ)

(cid:12)
(cid:12)
g′a(x)

f

θI ′

y
Let there also exist a group ¯
(cid:0)
G

|
of transformations ¯ga of the parameter space, ν = ¯ga(θ).
We saw already in Section 2 that, according to objectivity Desideratum III.c, the assigned
probabilities must be invariant under variate transformations. This is assured if the pdf’s
of the original and the transformed variate, θ and ν, are related via (16) and (17), so that
the pdf for ν, given a measured x, reads:

≡

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)

−

f (ν

xI ′) = f (θ

xI)

¯g′a(θ)

1 .

−

|

|

28

(cid:12)
(cid:12)

(cid:12)
(cid:12)

That is, in order to assign equal probabilities in states of equal knowledge, the pdf for
¯ga(θ) must take the form:

f

¯ga(θ)

x(y)I ′

=

|

(cid:0)

g′a(x)
η(x)

(cid:12)
(cid:12)
g′a(x)
η(x)

(cid:12)
(cid:12)

π(θ)
¯g′a(θ)
π(θ)
(cid:12)
(cid:12)
¯g′a(θ)

(cid:1)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
¯ga(θ)
π
ga(x)
η
(cid:0)
e
(cid:0)
e
ga(x), ¯ga(θ)

φ

(cid:12)
(cid:12)
(cid:1)
(cid:0)
(cid:1) e

≡

φ(x, θ)

g′a(x)

1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ga(x), ¯ga(θ)

(cid:12)
(cid:12)

φ

(cid:0)
e
ga(x), ¯ga(θ)

(cid:12)
(cid:12)

,

(cid:1)

φ(x, θ)

g′a(x)

1 ,

−

(cid:1)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)
k(a)

(cid:0)
e
π

(cid:0)

e

¯ga(θ)

≡

(cid:1)

π(θ)
¯g′a(θ)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

where:

f (y

νI ′) =

φ

|

and, for ¯g′a(θ) > 0,

(69)

(70)

η

ga(x)

k(a)

≡

η(x)
g′a(x)

=

¯ga(θb)

¯ga(θa)

Z

π

¯ga(θ′)

f

ga(x)

¯ga(θ′)I

d

¯g(θ′)

,

(71)

|

(cid:1)

(cid:0)
e

(cid:0)
while for ¯g′a(θ) < 0 the limits of the above integral are to be interchanged4. Note that
in general the value of the multiplication constant k, up to which the consistency and the
normalization factors, (62) and (65), are to be uniquely determined (recall the preceding
section), may depend on the value of the transformation parameter a.

e

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)

(cid:0)

(cid:1)

(cid:1)

(cid:1)

Equation (69) represents a constraint on consistency factors that is additional to (67),
. For invariant
but it also introduces an additional unknown variable, function
sampling distributions, however, it is easy to demonstrate that the form of the consistency
factor must also be invariant under the induced group ¯
π must be the same
G
depend on infor-
functions. Namely, the forms of consistency factors π(θ) and
mation I and I ′ that we possess about θ and ¯ga(θ), respectively, prior to collecting datum
x: on the forms and domains of sampling distributions, φ(x, θ) and
. In
ga(x), ¯ga(θ)
the case where all of these are invariant under particular transformations ga and ¯ga:
(cid:1)

, i.e. that π and
e
¯ga(θ)
π
e
(cid:1)
φ

¯ga(θ)

e

π

(cid:0)

(cid:0)

(cid:1)

(cid:0)
e

φ

ga(x), ¯ga(θ)

= φ

ga(x), ¯ga(θ)

,

ga(xa) =

,

ga(xb) =

(cid:1)

(cid:0)

(cid:0)
e
; g′a(x) > 0
; g′a(x) < 0

xa
xb

(

(cid:1)
xb
xa

(

; g′a(x) > 0
; g′a(x) < 0

,

a)

b)

and

4Index a in ga and ¯ga denotes particular elements of transformation groups, while in xa and θa it

indicates the lower bounds of the sample and parameter space, respectively.

29

π

¯ga(θ)

,

(cid:0)

e

(cid:1)
(72)

c)

¯ga(θa) =

θa
θb

(

; ¯g′a(θ) > 0
; ¯g′a(θ) < 0

,

¯ga(θb) =

θb
θa

(

; ¯g′a(θ) > 0
; ¯g′a(θ) < 0

,

the information I equals information I ′ and the two consistency factors, π(θ) and
must be the same functions:

π

¯ga(θ)

= π

¯ga(θ)

.

When combined with (70), this implies:

(cid:1)

(cid:0)

(cid:1)

(cid:0)

e

k(a) π(θ) = π

¯ga(θ)

¯g′a(θ)

.

(73)

(cid:0)
The above functional equation for π(θ) is the cornerstone of the entire theory of consistent
assignment of probabilities to parameters of sampling distributions:

(cid:1) (cid:12)
(cid:12)

(cid:12)
(cid:12)

Conjecture 1 Equation (73) is the only functional equation within the basic Desiderata
that can be used for determination of consistency factors.

The proof of this conjecture represents an open problem that is still to be solved. It is a
serious problem, though, since any additional functional equations for π(θ), independent
of (73), with solutions different from solutions of (73), would most seriously jeopardize
the consistency of the entire probabilistic approach to inferences about the parameters of
sampling distributions.

Be that as it may, equation (73) is the only functional equation that we know of that
can be used for determination of consistency factors if we want to rely exclusively on our
basic Desiderata. All other procedures for determination of π(θ) (of non-informative prior
’probability’ distributions; see Section 15) that we are aware of involve applications of
some additional ad hoc assumptions so that there is absolutely no guarantee that reason-
ings of such a kind be consistent. Further arguments and examples, supporting the above
conjecture by exhibiting the decisive role of the invariance of sampling distributions un-
der group transformations in the process of determination of the consistency factors, are
presented in Sections 11, 12, 14 and 15, and in Appendix B.

In case of a two-parametric induced group ¯
G

of parameter transformations ¯ga,b, the
functional equation for the consistency factor π(θ(1), θ(2)) for the inferred parameters θ(1)
and θ(2) reads:

(cid:0)
where J stands for the appropriate Jacobian

k(a, b) π(θ(1), θ(2)) = π

¯ga,b(θ(1)), ¯ga,b(θ(2))

(74)

J

1 ,

−

|

|

(cid:1)

(cid:0)
We will come across functional equation (74) in Section 10, during a simultaneous infer-
ence about a location and a scale parameter.

(cid:1)

J

≡

∂

(cid:0)

¯ga,b(θ(1)), ¯ga,b(θ(2))
θ(1), θ(2)

∂

.

(cid:1)

30

When equation (72) holds the normalization factor

η

ga(x)

equals η

ga(x)

,

η

ga(x)

=

π

¯ga(θ′)

φ

ga(x), ¯ga(θ′)

(cid:1)
(cid:0)
e
¯ga(θ′)
d

= η

(cid:0)
ga(x)

,

(cid:1)

θb

θa

Z

so that

(cid:0)
e

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

k(a) η(x) = η

ga(x)

g′a(x)

.

Under what circumstances does a unique solution of the functional equation (73) ex-
(cid:0)
ist? Let us consider the problem with a sampling distribution being invariant under a
discrete group of transformations

(cid:1) (cid:12)
(cid:12)

(cid:12)
(cid:12)

where a can only take two values,

ga : x

¯ga : θ

−−−→

−−−→

ga(x) = ax ,

¯ga(θ) = aθ ,

a =

1,

{

1
−

}

for both groups,
simultaneous inversion of sampling and parameter space coordinates. Then, for a =
functional equation (73) reads:

. That is, the considered distribution possesses parity under
1,

−

G

and ¯
G

or, after an inversion θ

(

←→

−

Multiplying the two equations yields:

θ) = k(a =

1) π(θ) ,

−

π(

−
θ),

π(θ) = k(a =

1) π(

θ) .

−

−

which, when the convention about π being positive is invoked, further implies

k2(a =

1) = 1 ,

−

π(

θ) = π(θ) .

−

That is, the consistency factor that corresponds to a sampling distribution being invariant
under simultaneous inversions of sampling and parameter space coordinates, must itself
possess parity under inversion of parameter space coordinates. But apart from this it can
take any form and so in this case the solution of (73) is clearly not unique.
It is not
difﬁcult to understand that this is a common feature of all solutions based on invariance
of the sampling distributions under discrete groups. If the symmetry group is discrete,
the sample and the parameter spaces break up in intervals with no connections in terms of
group transformations within the points of the same interval. We are then free to choose
the form of π(θ) in one of these intervals (e.g. we can choose π(θ) for the positive values

31

(75)

(76)

(77)

of θ in the above example), so it is evident that it is impossible to determine uniquely the
form of consistency factors for problems that are invariant only under discrete groups of
transformations.

It turns out, however, that for sampling distributions that are invariant under Lie
groups, functional equation (73) uniquely determines the form of the corresponding con-
sistency factors. But according to the Existence Theorem of Section 5, the invariance of
a sampling distribution under a Lie group is found only when the parameter of the dis-
tribution is reducible to a location parameter by one-to-one transformations of both the
parameter and the sampling variate. It is therefore sufﬁcient to determine the form of
consistency factors for location parameters, which is accomplished in the following three
sections.

8 Location parameters

We saw in Section 5 that a sampling distribution, parameterized by a location parameter
µ and by a ﬁxed dispersion parameter σ0, is invariant under the group of translations (44).
In such a case the functional equation (73) for π(µ) reads:

π(µ + b) = k(b) π(µ) .

(78)

After its differentiation with respect to b,

π′(µ + b) = k′(b) π(µ) ,

we set b = 0 and obtain a simple differential equation with separable variables π and µ,

with the constant q being deﬁned as

The general solution of (79) reads

where Cπ is an integration constant. Since all multiplication constants can be put into
η, we can assume without any loss of generality that Cπ = 1, obtaining in this way the
general form of the consistency factor for location parameters:

For sampling distributions, symmetric under simultaneous inversions of the sampling and
the parameter space, equation (77) implies q = 0, i.e. implies uniform consistency fac-
tors for location parameters. By invoking the symmetry of the problems of simultaneous

dπ
π

=

q dµ ,

−

q

≡ −

k′(0) .

π(µ) = Cπ exp

q µ

,

−

(cid:8)

(cid:9)

π(µ) = exp

q µ

.

−

(cid:8)

(cid:9)

32

(79)

(80)

inference about a location and a scale parameter in Section 10, we show that further appli-
cations of the basic Desiderata and their direct implications also require q = 0 in the case
of problems without space-inversion symmetry.

Based on a measured value x1, the pdf for a location parameter µ therefore reads:

f (µ

x1σ0I) =

|

f (x1|

µσ0I) =

qµ

e−
η(x1)

qµ

e−
η(x1)

1
σ0

φ

µ

x1 −
σ0

(cid:16)

.

(cid:17)

Now, as an example, we want to update our inference about the parameter µ by including
additional information x2 in our inference, where x2 is a result of a measurement of x that
is also subject to the same sampling distribution and independent of x1. We can write the
likelihood density for x2,

f (x2|
and the updated pdf for µ,

µσ0x1I) = f (x2|

µσ0I) =

1
σ0

φ

µ

x2 −
σ0

(cid:16)

,

(cid:17)

f (µ

x1x2σ0I) =

|

f (µ

|

x1σ0I) f (x2|
η(x1, x2)

µσ0I)

=

=

=

π(µ)
η(x1, x2)
qµ
e−
η(x1, x2)
qµ
e−
η(x1, x2)

f (x1|
1
σ2
0

φ

µσ0I) f (x2|
µ
x1 −
σ0

φ

(cid:16)
f (x1x2|

(cid:17)
µσ0I) ,

(cid:16)

µσ0I)

µ

x2 −
σ0

(cid:17)

with the appropriate normalization constant, η(x1, x2), being:

η(x1, x2) =

∞

qµ′

e−

f (x1x2|

µ′σ0I) dµ′ .

Z

−∞

The update (83) is made in accordance with Bayes’ theorem (29) with the purpose of
ensuring our reasoning be consistent.

The product of the likelihood densities f (x1|

µσ0I) and f (x2|
µσ0I) in (83) is equal
to the combined likelihood density for the two independent events, x1 and x2, due to the
µσ0I) can equivalently
product rule (9). According to (32), the likelihood density f (x1x2|
be represented by the density for ¯x and s, f (¯xs

Written in terms of f (µ

σ0¯xsI) the pdf for µ (83) thus reads:

x1 + x2
2

¯x

≡

, s

|

µσ0I), where
x1 −
2

x2|

|

.

≡

f (µ

σ0¯xsI) =

f (¯xs

µσ0I) =

|

|
qµ

e−
η(¯x, s)

4
σ2
0

φ

(cid:16)

qµ

e−
η(¯x, s)
µ
¯x

−
σ0

|

φ

(cid:17)

(cid:16)

+

s
σ0

33

¯x

µ
−
σ0 −

s
σ0

,

(cid:17)

(81)

(82)

(83)

(84)

(85)

(86)

with the appropriate normalization constant η(¯x, s),

η(¯x, s) =

∞

qµ′

e−

f (¯xs

µ′σ0I) dµ′ .

Z

−∞

|

The ﬁndings of the present example will become of particular importance in Sec-
tion 10 where we determine the form of the consistency factor π(µ, σ) for simultaneous
estimation of a location and a dispersion parameter.

9 Inference about scale and dispersion parameters

When, contrary to the preceding section, the inferred dispersion (or scale) parameter is
unknown and the location parameter is ﬁxed to µ = µ0, the problem is invariant under the
group (45) of scale transformations. In such a case the functional equation (73) for π(σ)
reads:

π(aσ) = h(a) π(σ) ,

≡
Equation (88) determines the form of the consistency factor for dispersion and scale pa-
rameters to be

(87)

(88)

(89)

(90)

where the value of the constant r,

h(a)

k(a)
a

.

π(σ) = σ−

r

π(τ ) = σ−

r ,

r

≡ −

h′(1) ,

is yet to be determined in Section 10.

We stressed in Section 4 (c.f. equation (38)) that an assignment of a pdf to a scale
parameter τ (or, equivalently, to a dispersion parameter σ of a symmetric distribution)
can be reduced to an assignment of a pdf, f (ν

zλ0I), to a location parameter, ν:

where

and

with

and with λ0 being a ﬁxed dispersion parameter. According to the ﬁndings of the previous
section (see eq. (80)), we can immediately write the appropriate consistency factor:

|
π(ν)
η(z)
e
e
and

ν

f (ν

zλ0I) =

|

f (z

νλ0I)

|

z

ln t

≡

ln τ ,

≡

π(ν) = e−

qν .

e

34

Making use of eq. (17), the pdf for ν can be transformed into the pdf for τ :

f (τ

tµ0I) = f (ν

zλ0I)

|

|

dν
dτ

,

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
1 .

where

dν
dτ
(cid:12)
On the other hand, in order to make a consistent inference, the pdf for τ , f (τ
(cid:12)
(cid:12)
on t only, must be proportional to the likelihood density f (t
|

ν = τ −

= e−

(cid:12)
(cid:12)
(cid:12)

|

τ µ0I) (see eq. (64)):

tµ0I), based

f (τ

tµ0I) =

|

π(τ )
η(t)

f (t
|

τ µ0I) .

Then, due to Desideratum III.a, the two pdf’s, (91) and (92), must be equal, which implies
the form of the consistency factors for scale parameters,

π(τ ) = τ −

(q+1) ,

π(σ) = σ−

(q+1) .

r = q + 1

as well as for dispersion parameters,

The same Desideratum implies equality of the factors (89) and (94), as well as equality of
the factors (90) and (93), i.e. implies the relation

between the parameters q and r of the consistency factors of the location and scale pa-
rameters. Evidently, if q is determined to be zero, this would immediately imply r = 1.

In the limit of complete prior ignorance about its value, the pdf for σ given a measured

value x1 and the ﬁxed value of µ = µ0 therefore reads:

f (σ

µ0x1I) =

|

π(σ)
η(x1)

f (x1|

µ0σI) .

Following the steps of the example of the preceding section, we update the pdf for σ by
including result x2 of an additional measurement in our inference. The updated value of
the pdf reads:

(91)

(92)

(93)

(94)

(95)

f (σ

µ0x1x2I) =

|

f (x1x2|

µ0σI)

r

π(σ)
η(x1, x2)
σ−
η(x1, x2)
σ−
η(x1, x2)

r

=

=

µ0σI)

f (x1x2|
1
σ2 φ

(cid:16)

35

µ0

x1 −
σ

φ

µ0

x2 −
σ

(cid:17)

(cid:16)

,

(cid:17)

(96)

(97)

(98)

with the normalization constant η(x1, x2),

η(x1, x2) =

∞

(σ′)−

r f (x1x2|

µ0σ′I) dσ′ .

0
Z

In an analogy with (86) and (87), both the updated pdf for σ and the corresponding nor-
malization constant can be expressed in terms of ¯x and s (85), instead of x1 and x2:

f (σ

µ0¯xsI) =

f (¯xs

µ0σI) =

r

|
σ−
η(¯x, s)

4
σ2 φ

(cid:16)

+

s
σ

|

φ

(cid:17)

(cid:16)

¯x

µ0
−
σ −

s
σ

,

(cid:17)

η(¯x, s) =

(σ′)−

r f (¯xs

µ0σ′I) dσ′ .

|

r

σ−
η(¯x, s)
µ0
¯x

−
σ

∞

0
Z

and

10 Simultaneous inference about a location and a disper-

sion parameter

By ﬁxing neither the location nor the dispersion parameter, an inference about the two
parameters is invariant under a simultaneous location and scale transformation (43). The
symmetry of the problem implies the following form of the functional equation (74) for
the appropriate consistency and normalization factors:

π(aµ + b, aσ) = h(a, b) π(µ, σ) ,

(99)

where

h(a, b)

k(a, b)

≡

∂(aµ + b, aσ)
∂(µ, σ)

1

−

=

k(a, b)
a2

.

In order to solve it, we differentiate equation (99) separately with respect to a and b,

set afterward a = 1 and b = 0, and obtain:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ π1(µ, σ) + σ π2(µ, σ) =
π1(µ, σ) =

˜r π(µ, σ) ,
˜q π(µ, σ) ,

(100)
(101)

with the constants ˜q and ˜r being deﬁned as

˜q

≡ −

h2(1, 0)

and

˜r

h1(1, 0) .

≡ −

The general solution of differential functional equation (101) is a function π(µ, σ) of

the form

π(µ, σ) = Ω(σ) exp

˜qµ

,

(102)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−
−

−

(cid:8)

(cid:9)

36

where Ω(σ) is a non-negative function of σ. When inserted in (100), (102) yields:

which, if it is to be true for all µ and σ, further implies ˜q = 0 and

so that the general form of the consistency factor π(µ, σ) reads:

˜qµ = ˜r + σ

Ω′(σ)
Ω(σ)

,

Ω(σ)

σ−

˜r ,

∝

π(µ, σ) = σ−

˜r ,

where we put all possible multiplication constants in the normalization factor:

η(¯x, s) =

∞

dµ′

∞

dσ′ (σ′)−

˜rf (¯xs

µ′σ′I) .

|

Z

−∞

0

Z

Now there is only one step dividing us from a complete determination of consistency
factors for location, scale and dispersion parameters. Having established the form (103)
of the consistency factor π(µ, σ), we can write down a pdf for µ and σ given ¯x and s:

f (µσ

¯xsI) =

|

π(µ, σ)
η(¯x, s)

f (¯xs

µσI) =

|

˜r

σ−
η(¯x, s)

f (¯xs

µσI) .

|

According to the product rule (9), the pdf can also be written as

f (µσ

¯xsI) = f (µ

σ¯xsI) f (σ

¯xsI) ,

|
¯xsI) is a marginal pdf (see equation (23)):

|

|

where f (σ

|

f (σ

¯xsI) =

f (µ′σ

¯xsI) dµ′ =

f (¯xs

µ′σI) dµ′ .

(107)

|

∞

Z

−∞

|

˜r

σ−
η(¯x, s)

∞

Z

−∞

|

Then, combination of equations (86-87) and (105-107) leads to:

eqµ =

∞
−∞
e−
∞
R
−∞
R

µ′σI) dµ′

f (¯xs
qµ′ f (¯xs

|

µ′σI) dµ′

|

,

solvable for any value of µ if and only if

where q is the constant of the consistency factor for location parameters (80). Due to
the simple relation (95) between the constant q of the consistency factor for the location
parameters and the constant r of the factors for the scale and dispersion parameters, the
above solution also implies

(103)

(104)

(105)

(106)

(108)

(109)

q = 0 ,

r = 1 .

37

These are highly nontrivial results since they uniquely determine the consistency factors
for the location, dispersion and scale parameters (recall eqns. (80), (89) and (90)):

π(µ) = 1 ,
π(σ) = σ−
π(τ ) = τ −

1 ,
1 .

(110)

In addition, in order to determine also the value of the constant ˜r of the consistency
factor (103), we recall that apart from (106), the product rule (9) also allows for the pdf
(105) to be written as:

f (µσ

¯xsI) = f (σ

µ¯xsI) f (µ

¯xsI) ,

(111)

where the marginal distribution f (µ

|

|
¯xsI) now stands for

|

f (µ

¯xsI) =

f (µσ′

¯xsI) dσ′ =

|

∞

0
Z

1
η(¯x, s)

0
Z

∞

(σ′)−

˜rf (¯xs

µσ′I) dσ′ ,

(112)

|

while the pdf for σ given µ, f (σ

µ¯xsI), equals the pdf (97):

f (σ

µ¯xsI) =

|

r

σ−
rf (¯xs

0 (σ′)−
∞

f (¯xs

µσI) .

µσ′I) dσ′

|

(113)

|
Equations (105) and (111-113) combined yield:

R

|

|

|

with r being determined (109) to be 1. Evidently, the solution of the above equation reads:

σr

˜r =

−

˜r f (¯xs
r f (¯xs

µσ′I) dσ′
µσ′I) dσ′

,

|
|

0 (σ′)−
∞
0 (σ′)−
∞
R
R

˜r = r = 1 ,

which ﬁnally determines the consistency factor π(µ, σ) for a symmetric sampling distri-
bution (see eq. (103)),

π(µ, σ) = σ−

1 .

(114)

Now we would like to make use of results obtained in this and preceding sections and
address the so-called problem of two means (i.e. two location parameters), also referred to
as the Fisher-Behrens problem (see refs. [24] and [25], and
19.48,
26.28-26.29, pp. 441-442 in ref. [20]). Imagine x and y being independent
p. 164 and
quantities, both being subject to Gaussian sampling distributions,

19.47, pp. 160-162,

§

§

§

and

µ1σ1I) =
f (x
|

1
√2πσ1

exp

(x

µ1)2

−
2σ2

1 (cid:27)

−

(cid:26)

f (y

µ2σ2I) =
|

1
√2πσ2

exp

(y

µ2)2

−
2σ2

2 (cid:27)

,

−

(cid:26)

38

with parameters of both distributions being unknown and unconstrained. After collecting
two events, (x1, x2) and (y1, y2), from each of the two samples, we are in a position to
make a probabilistic inference about the unknown parameters. The pdf’s for (µ1, σ1) and
(µ2, σ2) read:

f (µ1σ1|

¯xsxI) =

π(µ1, σ1)
η(¯x, sx)
sx
4
σ3
2π
1
sx
4
σ3
2π
1

exp

exp

f (¯xsx|
(¯x

µ1σ1I)

µ1)2

−
σ2
1
(¯x

−

(cid:26)

s2
x
σ2

−

(cid:26)

1 (cid:20)

−
s2
x

=

=

s2
x
σ2
−
µ1)2

1 (cid:27)

+ 1

(cid:21)(cid:27)

f (µ2σ2|

¯ysyI) =

µ2σ2I)

π(µ2, σ2)
η(¯y, sy)
sy
4
σ3
2π
2
sy
σ3
2

4
2π

exp

exp

f (¯ysy|
(¯y

−

(cid:26)

s2
y
σ2

−

(cid:26)

2 (cid:20)

=

=

−
σ2
2
(¯y

µ2)2

s2
y
σ2
−
µ2)2
−
s2
y

2 (cid:27)

+ 1

,

(cid:21)(cid:27)

¯x =

x1 + x2
2

, sx = |

x1 −
2

x2|

,

¯y =

y1 + y2
2

, sy = |

y1 −
2

y2|

.

and

where

and

(115)

(116)

Since the two pdf’s are independent, we apply the product rule (9) and write down a

pdf for µ1, µ2, σ1 and σ2 as a product of pdf’s (115) and (116):
¯xsxI) f (µ2σ2|
¯x¯ysxsyI) = f (µ1σ1|
µ2)2
(¯y
exp
−
−
σ2
σ2
1
2
(¯x

¯ysyI) =
s2
x
σ2
1 −
(¯y

−
µ1)2

f (µ1µ2σ1σ2|
sx
sy
4
exp
σ3
σ3
π2
2
1
sx
sy
σ3
σ3
2
1

−
(cid:26)
s2
y
σ2

s2
x
σ2

−
s2
x

4
π2

µ1)2

exp

exp

+ 1

(¯x

−

−

−

(cid:27)

(cid:26)

(cid:21)(cid:27)

(cid:26)

2 (cid:20)

(cid:26)

1 (cid:20)

=

s2
y
σ2
2 (cid:27)
µ2)2
−
s2
y

+ 1

.

(cid:21)(cid:27)

By integrating out parameters σ1 and σ2 we ﬁnd the marginal pdf for µ1 and µ2 to be of
the form:

f (µ1µ2|

¯x¯ysxsyI) =

∞

dσ1

∞

0
Z
(¯x

0

Z

=

1
π2sxsy (cid:18)

−
s2
x

dσ2 f (µ1µ2σ1σ2|
µ1)2

−

1

+ 1

(cid:19)

(cid:18)

¯x¯ysxsyI)

(¯y

µ2)2
−
s2
y

1

−

.

+ 1

(cid:19)

Note that the above results were all obtained simply by using some of the applications of
basic Desiderata without making any additional assumptions or requiring any new postu-
26.29, p. 442). We refer to the Fisher-Behrens
lates (compare to references [26] and [20],

§

39

problem again in Section 15 when we comment on difﬁculties with inferences about pa-
rameters outside the framework of probability.

11 On uniqueness of consistency factors and on consis-

tency of basic rules

In previous sections we saw that the general rules for plausible reasoning - the Cox-
P´olya-Jaynes Desiderata - uniquely determine the consistency factors for location, scale
and dispersion parameters. In other words, in the limit of complete prior ignorance, there
is only one possible way of making a consistent inference about the three types of pa-
rameters. According to (63), the pdf, assigned to one or to several of these parameters
simultaneously, is to be proportional to the likelihood function, containing the informa-
tion from one or several measured events xi that are subject to the distribution determined
by the inferred parameter(s), and to the appropriate consistency factor as determined at
the end of the preceding section.

Anyone who possesses the same information, but assigns a different probability dis-
tribution to a given parameter , e.g. by choosing a ’consistency factor’ of a different form,
thus necessarily violates at least one of our basic Desiderata. It is certainly true that no-
body has the authority to forbid such violations, but, at the same time, it is also true that
anyone coming to the conclusions by violating such basic and general rules being would
surely have difﬁculties in persuading anyone else, who was aware of these violations, to
accept his conclusions.

,

|

(cid:1)

(cid:0)

∞

−∞

ga(x)

¯ga(θ)I ′

) and (σa, σb) = (τa, τb) = (0,

In Section 7 we stressed that a prior information I in a problem of inference about a
parameter θ of a sampling distribution, f (x
θI), is equal to corresponding information
|
I ′ about an inferred parameter ν = ¯ga(θ) of the sampling distribution for y = ga(x),
, only if three requirements are simultaneously met: a) if the sampling
f
distribution is invariant under transformations ga and ¯ga, b) if the permissible range of
sampling variate x, (xa, xb) is invariant under ga, and c) if the permissible range of the in-
ferred parameter θ, (θa, θb) is invariant under transformation ¯ga. Evidently, for the ranges
), and the corre-
of parameters (µa, µb) = (
sponding transformations (43-45), the condition c) is well fulﬁlled for all a
) and
(0,
b

(
But, in practice, we usually face problems with pre-constrained inferred parameters:
we possess some additional information that narrows the admissible range. As a simple
example, when we are estimating the average lifetime of a newly discovered particle,
produced in an experiment with highly energetic protons from an accelerator hitting a
ﬁxed target, it is easy to imagine that τ cannot really be inﬁnite, for in that case there
should be many of these particles around as remnants of the Big Bang. In fact, it might
well be reasoned that τ is necessarily even much smaller than billions of years, since
in case of τ being sufﬁciently large, e.g. of the order of a millisecond, we should have
noticed the particles as products of cosmic protons hitting nuclei in the upper layers of
the Earth’s atmosphere. The listed two arguments, as well as any possible additional

−∞

∞

∞

∞

∈

∈

).

,

40

ones, thus lead to a ﬁnite interval (τa, τb) in the positive half of the real axis. But such
an interval is clearly not invariant under parameter transformation ¯ga(τ ) = aτ and so the
above condition c) for the equality of information, I = I ′, is not fulﬁlled. How shall we
proceed in such cases?

Suppose for a moment that we ignore the fact that the condition c) is not fulﬁlled.
Effectively this is equivalent to a prescription that can often be found in textbooks on the
so-called Bayesian inference: to use the same function π(τ ) as in the case of no constraints
on the parameter range, and afterwards to chop-off the unconstrained pdf for the inferred
parameter, τ , outside the interval (τa, τb) and renormalize the truncated distribution (see,
3.17, pp. 72-73). It is easy to show that in general such an ad hoc
for example, [16],
prescription inevitably leads to inconsistencies.
Namely, without the constraints on τ ,

π(aτ ) in equation (70) is to equal π(aτ ), and

§

consequently π(τ ) is to equal 1/τ . Then, (70) implies

k(a) = 1 ,

a

(0,

) ,

∈

∞

e
∀

so the functional equation (75) for the normalization factor η(t1) reads:

By setting a = 1 we realize that

η(t) and η(t) are to be the same functions,

or, equivalently,

When inserted in (117), (119) yields
e

η(at1) =

η(t1)
a

.

e

e

η(t1) = η(t1) ,

e

η(at1) = η(at1) .

η(at1) =

η(t1)
a

,

η(t1) = η(1)

1
t1

.

which must be true for any a

(0,

). For a = t−
1

1

(120) thus reads:

∈

∞

On the other hand, by deﬁnition (71),

η(t1) should ensure normalization of the pdf for aτ :

η(t1) =

π(τ ′) f (t1|

τ ′ I) dτ ′ =

τb

τa

Z

e

τb

τa

Z

1
2 e−
τ ′

t1
τ ′ dτ ′ =

1
t1

e

e

Evidently, the equality (118) is assured for all t1 ∈
and τb =

τa = 0

(0,

∞
,

t1/τb

e−

t1/τa

e−

.

(122)

−

(cid:16)
) only if

(cid:1)

∞

41

(117)

(118)

(119)

(120)

(121)

0.12

|

0.09

)
I
1
t

τ
(
f

0.06

0.03

a)

b)

0.3

|

0.2

)
I
n
t

τ
(
f

ǫ1

ǫ2

0

0

3

6

9

12
15
τ [ps]

0.1

0

0

ǫ1

☛

3

6

ǫ2

☛

9

12
15
τ [ps]

Figure 1: a) Unconstrained probability density function for τ based on a recorded decay
time t1 = 5 ps. The hatched areas represent integrals, ǫ1 = 0.189 and ǫ2 = 0.465, of
), respectively. b) Pdf for τ
the pdf in the intervals (0, τa = 3 ps) and (τb = 8 ps,
based on an average ¯t = 5 ps of n = 10 recorded decay times with the integrals of
the unconstrained pdf outside the allowed region for τ being reduced to ǫ1 = 0.031 and
ǫ2 = 0.102.

∞

i.e. only if the invariance of the parameter range under the particular group ¯
G
mations ¯ga is exact.

of transfor-

In practice, our reasoning would still be sufﬁciently consistent if

t1
τa ≫

1

and

t1
τb ≪

1 ,

i.e. if the integrals ǫ1 and ǫ2 of the unconstrained pdf for τ outside the allowed region (see
Figure 1),

ǫ1 =

f (τ ′

t1I) dτ ′ =

ǫ2 =

f (τ ′

t1I) dτ ′ =

τa

0
Z

∞

τb

Z

|

|

τa

0
Z

∞

τb

Z

t1
2 e−
τ ′
t1
2 e−
τ ′

t1/τ ′

dτ ′ ,

t1/τ ′

dτ ′ ,

are sufﬁciently small. That is, if the conditions (123) are fulﬁlled, normalization factors
(121) and (122) are equal beyond the precision required for the particular inference. As
admitted above, the invariance of the admissible parameter range, and consequently also
the consistency of our reasoning, is not exact in such a case since for extremely large or
extremely small values of measured decay times t the equality (117) would not hold. But
once t1 has been recorded, it is very likely that the value of the inferred τ would also be of
the same order of magnitude, i.e. it would be extremely unlikely for τ to be much smaller
or much larger than t1. Then, with τ
t1, it is also very unlikely that we would ever
observe an event t orders of magnitude different from t1.

∼

42

(123)

(124)

On the other hand, if the conditions (123) are not fulﬁlled (see, for example, Fig-
ure 1.a), our reasoning is clearly not consistent: by applying the two equally valid normal-
ization factors, (121) and (122), we are able to arrive at signiﬁcantly different probabilities
for the same proposition, e.g. P
, which is in direct contradiction with
the consistency Desideratum III.a. The additional information that narrows the interval
(τa, τb) may still be very useful, but it is just that consistent probabilistic reasoning is
impossible in such a situation.

(τ1, τ2)

at1 I

aτ

∈

(cid:1)

(cid:0)

|

In order to avoid such an inconsistency, we could have ignored the information, ad-
ditional to t1, and stretched (τa, τb) to the whole positive half of the real axis. But this is
not a solution either, since in this way we would have been arbitrarily ignoring some of
the available information and basing our conclusions on what remains. Acting in explicit
contradiction with Desideratum III.b, we would have allowed ideology to break into our
inference, which is inadmissible for any scientiﬁcally respectable reasoning.

Thus, the only consistent solution to our problem of inference about the pre-con-
strained parameter τ would be provided by recording additional independent decay times,
t2, t3, ..., tn, of particles of the same type. Then, the unconstrained probability distribution
for τ , based on the recorded data, is described by the following pdf:

f (τ

t1t2...tnI) = f (τ

|

tnI) =

|

(n¯t)n
(n

1)!

1
τ n+1 e−

n¯t/τ ,

−

where ¯t is an average of the recorded decay times:

¯t =

1
n

n

ti .

i=1
X

The distribution narrows as n increases (see Figure 1.b). Therefore, by collecting enough
data, the inconsistency is diminished beyond the required level: by diminishing the inte-
grals ǫ1 and ǫ2 of the unconstrained pdf for τ outside the constrained domain, the limits
τa and τb that caused the inconsistencies become irrelevant.

Our theory of consistent inference about parameters is therefore valid only if certain
conditions are fulﬁlled, i.e. in the limit ǫ
0. Such a theory is referred to as an effective
theory, with the term coming from physics where all theories are effective. The precision
of predictions of an effective theory is estimated by the proximity of the actual conditions
to the ideal limit. The values of integrals ǫ
(124), compared to zero, can thus serve
as an estimate of the precision of our probabilistic inference about the pre-constrained
parameters.

± →

±

12 Calibration

The most striking achievement of the physical sciences is prediction.

Georg P´olya ([4], Chapter XIV,

4, p. 64)

§

43

Thus far our theory of plausible inference about parameters has been developed by
following Desiderata I.-III. only, while the implications of the operational Desideratum
have not yet been considered. According to the latter Desideratum, in order to exceed the
level of a mere speculation, our theory of inference about parameters must be exposed,
i.e. must be able to make predictions that can be veriﬁed by experiments. The magni-
tudes considered by physicists such as mass, electric charge or reaction velocity have an
operational deﬁnition: the physicist knows very well which operations he or she has to
perform if he or she wishes to ascertain the magnitude of an electric charge, for example
([4], Chapter XV,
4, p. 117). Is there a way for probability as a measure of a personal
degree of belief to become operational, too?

§

Imagine we were given several numbers xi, all produced by a random number gen-
θiI). While the form of the distribution is known
erator according to a distribution f (xi|
to us, we do not know much about the values θi of the parameter: in general they can be
different for each of the numbers xi generated and can be any number in the range (θa, θb).
In fact, what we are asked for is to make probabilistic inferences about the unknown
values θi on the basis of each datum xi separately. Based on our probability distribution
xiI), we should specify our conﬁdence intervals (θi,1, θi,2) such
for θi, p(θi|
that

xiI) = f (θi|

P

θi ∈

(cid:0)

θi,2

θi,1

Z

(cid:1)

(θi,1, θi,2)

xiI

=

|

f (θi|

xiI) dθi = δ ,

(125)

with δ being the same for all inferences. Note that the interval for the inference of a
particular value θi is not unique: it can be the shortest of all possible intervals, the central
δ)/2, the lower-most interval
interval with P (θi ≤
θi,1|
with θi,1 = θa, the upper-most interval with θi,2 = θb, or any other interval as long as the
probability (125) equals δ.

xiI) = P (θi > θi,2|

xiI) = (1

−

After the inference has been made, we learn the values θi of the parameter used in the
random number generator. Then, our probability judgments are said to be calibrated if
they agree with the actual frequencies of occurrence ([16],
6.4, p. 142), i.e. if the fraction
of inferences with the speciﬁed intervals containing the actual value of the parameter for
the particular example, coincides with δ.

§

The deﬁnition of long range relative frequency, although in some way less distinct than
that of an electric charge, is still operational: it suggests deﬁnite operations that we can
undertake to obtain an approximate numerical value of such a frequency ([4], Chapter XV,
4,p. 117). Our theory thus cannot be expected to give a correct prediction each time, but
§
it can be veriﬁable from its long range consequences, i.e. it can reasonably be expected
to give the right answer in an assignable percentage of cases in the long run.

Under what conditions is a calibrated inference achieved? To answer this question we
9.2.1, pp. 200-201 in
refer to the construction of classical conﬁdence intervals (see [27],
19.44-19-47,
[19], and Chapter 19 in [20]) and to ﬁducial theory (see refs. [25] and [20],
26.26-26.29, pp. 440-442). Let α and α + δ be probabilities for x to take
pp. 156-162,
values less or equal to x1 and x2, respectively, given the value θ of the parameter of the

§

§

§

44

✻

∆θ

❄

θ

✻

θi,2

θi

θi,1

θ

a)

b)

x1(θ; α)

θ

✻

x2(θ; α + δ)

✻

∆θ

❄

θi,2

θi
θi,2
e
θi,1

θi,1
e

x1(θ) x2(θ)

x1(θi) xi

x2(θi)

✲
x

x2(θ; α + δ)

x1(θ; α)

✲
x

x1(θi) xi x2(θi)

xi
e

Figure 2: Conﬁdence belts in the case of distribution function F (x, θ) being a) strictly
decreasing and b) strictly increasing in θ.

(continuous) sampling distribution for x:

(126)

(127)

α = P

x

α + δ = P

x
(cid:0)

x1(θ)
x2(θ)

θI

θI

|

|

(cid:1)

≤

≤

= F

= F

x1(θ), θ
x2(θ), θ
(cid:0)

(cid:1)

,

.

(cid:0)
Then, the probability for x to take a value in the interval

(cid:1)

(cid:0)

(cid:1)
x1(θ), x2(θ)

, equals δ:

P

x1(θ)

x < x2(θ)

= F

x2(θ), θ

≤

(cid:0)
F

−

x1(θ), θ

(cid:1)
= δ ,

(cid:0)

(cid:1)

(cid:1)
(cid:0)
regardless of the value of α. Let unique values x1(θ) and x2(θ), satisfying (126), exist
It is easy to see that
for every θ in the range (θa, θb), and for every α
unconstrained location and scale parameters with µ
) meet
(0,
such a requirement . The curves x1(θ; α) and x2(θ; α + δ) are formed by varying θ in
x1(θ) and x2(θ) but at ﬁxed α and δ (see Figure 2) and the region between the two curves
is known as a conﬁdence belt.

(cid:0)
(0, 1
(

−
,
−∞

) and τ

δ).

∞

∞

∈

∈

∈

(cid:1)

Having the conﬁdence belts deﬁned, we return to the estimation of unknown values θi
of the parameter. Given the measured value xi, the required conﬁdence interval (θi,1, θi,2)
is obtained by the intersection of the vertical line x = xi with the curves x1(θ; α) and
x2(θ; α + δ). By inspecting Figure 2 it becomes evident that the proposition

θi ∈

(θi,1, θi,2)

(128)

x1(θi), x2(θi)

xi and the
is true if xi ∈
corresponding conﬁdence interval (
θi,1) in Figure 2.b). This means that, according to
(cid:0)
(127), the probability for (128) to be true is exactly δ for every θi, i.e. in the long run our
inferences will be correct in δ per cent regardless the distribution of the values θi.
e

and is wrong otherwise (see, for example,

θi,1,

e

e

(cid:1)

45

In the case there exist unique values θ1,2 ∈
−

(θa, θb) for any α
∈
(xa, xb), that solve equations (126), the cdf for x is either
(0, 1
strictly decreasing or strictly increasing in θ. Then, by construction of conﬁdence belts,
the relations

α) and for any x1,2 ∈

(0, 1), any δ

∈

are always true for cdf’s strictly decreasing in θ (Figure 2.a), as is the case for functions
with θ being a location or a scale parameter (see Section 4), while for strictly increasing
distribution functions (Figure 2.b), the relations read:

F (xi, θi,1) = α + δ ,
F (xi, θi,2) = α ,

F (xi, θi,1) = α ,
F (xi, θi,2) = α + δ .

Accordingly, the fraction δ of correct inferences can be expressed as:

δ =

F (xi, θi,1)

F (xi, θi,2) =

F (xi, θi,1)

F (xi, θi,1 + ∆θ) ,

±

±
where the upper (lower) sign corresponds to cumulative distribution functions strictly
decreasing (increasing) in θ, while

∓

∓

(129)

∆θ = θi,2 −

θi,1 .

In the limit of inﬁnitesimally small fractions δ and differences ∆θ = dθ, equation (129)
can be rewritten in terms of a differential of the distribution function with respect to θ:

δ =

F (xi, θ)

dθ =

F2(xi, θ) dθ .

(130)

∂
∂θ

∓

(cid:16)

∓

(cid:17)

But recall now our probabilistic inference about θi: given the pdf f (θ

xiI), the prob-

|

ability for θi being in the interval (θ, θ + dθ) reads:

p(θ

xiI) = f (θ

xiI) dθ .

|

|

Thus, we ﬁnally came to the point when we are able to answer: our inference will be
calibrated, i.e.
the assigned probability will coincide with the fraction δ of conﬁdence
intervals containing the true values of the parameter, if and only if

∓
For location parameters with the cumulative distribution function (35) for the sam-

|

f (θ

xiI) =

F2(xi, θ) .

(131)

pling variable x, the condition (131) implies the calibrated pdf for θ be:

f (µ

xiI) =

|

∂
∂µ

−

xi

µ

−

Z

−∞

φ(u) du = φ(xi −

µ) = f (xi|

µI) .

46

The above pdf for θ coincides with the one obtained by following the basic Desiderata of
consistent inference, implying the consistency factor π(µ) to be constant (110). There-
fore, the only consistent way of inference about location parameters is also the only one
that is calibrated.

The latter is also true for inferences about scale parameters with the appropriate dis-

tribution function (39). Namely,

f (τ

tiI) =

|

∂
∂τ

−

φ(u) du =

ti
τ 2 φ

ti
τ

=

ti
τ

τ I)

f (ti|

ti
τ

0
Z

corresponds to π(τ ) = τ −

(cid:17)
1 as already determined in (110).

(cid:16)

The probability distributions for location, scale or dispersion parameters that were
assigned in consistent way, passed an important test: they are all calibrated. The question
can be raised whether there are any other types of parameters that are also in accordance
with the calibration requirement (131)? We restrict the answer only to parameters whose
pdf can be written in the form of the Consistency Theorem (64), obtained by requiring
logically independent pieces of information to be commutative. By combining the two
equations we obtain:

π(θ) F1(x, θ)

η(x) F2(x, θ) = 0 ,

±

(132)

where the upper (lower) sign stands for cdf’s which are strictly decreasing (increasing) in
θ. By deﬁning function G(x, θ) as a difference (sum),

with h(x) and k(θ) being related to π(θ) and η(x1) as

G(x, θ)

h(x)

k(θ) ,

≡

∓

h′(x) = η(x) and k′(θ) = π(θ) ,

equation (132) can be rewritten as

F1(x, θ) G2(x, θ)

F2(x, θ) G1(x, θ) = 0 ,

−

with G1(x, θ) = η(x) and G2(x, θ) = π(θ) being strictly positive functions (see Sec-
tion 6). But as we saw in Section 5, the general solution of such a differential functional
equation is a distribution function F (x, θ) of the form

where

F (x, θ) = Φ(z

µ) ,

−

z

≡

h(x) and µ

k(θ) ,

≡ ±

i.e. a distribution function that corresponds to µ being a location parameter (35). There-
fore, in the limit of complete prior ignorance, an inference about a parameter θ that is
subject to the calibration condition (131), is necessarily reducible to an inference about
a location parameter. Note that this result was ﬁrst obtained by Dennis Lindley [28] by

47

combining the calibration condition (131) and the Bayes’ theorem with a prior pdf f (θ
which is independent of data x.

|

I)

It is interesting to note that the above result is identical to the result obtained at the end
of Section 5 and discussed in Section 7, despite the fact that the requirements of logical
consistency and that of calibration may appear, at least at ﬁrst sight, as almost diamet-
rically opposed starting points. As a very important consequence, every probabilistic
inference about a parameter of a sampling distribution that we are sure is consistent will
thus at the same time also be calibrated and, vice versa, every calibrated inference, based
on a posterior probability distribution that is factorized according to the Consistency
Theorem, will simultaneously be logically consistent, too.

Recall Section 2 where we chose a set of functions, called probabilities, out of all
possible plausibility functions, suitable for representing our degree of belief according to
the basic Desiderata. The main reason for such a choice was that for the probabilities the
basic equations for manipulating plausibilities - the product and the sum rule, (1) and (2)-
are of especially simple forms. But now we realize another advantage of probabilities
over other plausibility functions: it is only for probabilities that the assigned degrees of
belief exactly coincide with the long term relative frequencies. Other plausibilities are
one-to-one functions of probabilities, so their values correspond to (the same) one-to-one
functions of the relative frequencies. Again, this does not imply that predictions in terms
of probabilities are more reliable than predictions in terms of any other set of plausibility
functions: they are just the easiest to interpret.

The predictions of the systems of plausible reasoning, however, that are not isomor-
phisms of the probability system, are, apart from being inconsistent, also necessarily un-
calibrated. This means that in general their predictions in terms of long-range frequencies
are not correct.

Take, as an example, the power-law distribution with the pdf

the ranges of x and θ

and the corresponding cdf

θI) = (θ + 1) xθ ,
f (x
|

x

(0, 1) and θ

∈

1,

(
−

∈

∞

) ,

F (x, θ) =

(1 + θ) x′

θ dx′ = xθ+1 .

x

0

Z

Due to the range of x, the cdf is strictly decreasing in θ, therefore the calibration condition
for the inference about the parameter reads:

xI) =
|
The ratio of the pdf’s for θ and x, (133) and (135),

f (θ

−

x ln x)
F2(x, θ) = (θ + 1) xθ (
−
θ + 1

.

(133)

(134)

(135)

implies consistency and normalization factors of the form

xI)
f (θ
|
θI)
f (x
|

= −

x ln x
θ + 1

,

π(θ) =

1
θ + 1

48

Their integrals,

and

and

η(x) =

1
x ln x

.

−

z

h(x) =

η(x) dx =

ln (

ln x)

−

−

µ

k(θ) =

π(θ) dθ = ln (1 + θ) ,

≡

≡

Z

Z

with the domains of both being the complete real axis (c.f. equation (134)),

(
−∞
allow for a reduction of the inference about parameter θ to an inference about a location
parameter µ of the sampling distribution of the variate z with the corresponding cdf:

∞

∈

z, µ

) ,

,

Indeed:

F (z, µ) = Φ

ln (

ln x)

ln (1 + θ)

= Φ(z

µ) .

−

−

−

f (z

µI) = f
|

θ(µ)I
x(z)
|

(cid:1)
µ) exp

dx
dz

= e−

(z

−

e−

(z

µ)

−

.

−

−
(cid:0)

(cid:12)
As a counter-example, i.e. as an example with the cdf of the sampling distribution
(cid:12)
(cid:12)
being neither strictly increasing nor decreasing with respect to all of its parameters, we
refer to the Weibull distribution ([12],

5.33, pp. 189-190) with the pdf of the form:

(cid:1) (cid:12)
(cid:12)
(cid:12)

(cid:8)

(cid:9)

(cid:0)

f (t

θ τ I) =
|

§
θ
τ

t
τ

(cid:16)

(cid:17)

θ

1

−

exp

θ

.

t
τ

−

(cid:26)

(cid:16)

(cid:17)

(cid:27)

The range of the variate t, as well as the ranges of both parameters θ and τ , coincide with
the positive half of the real axis:

t, τ, θ

(0,

) .

∞

∈

x =

t
τ

, x

(0,

) ,

∞

∈

Let the value of the scale parameter τ of the distribution be known and let x be a normal-
ized variate:

with the appropriate pdf,

and cdf:

θI) = f
f (x
|

θ τ I
t(x)
|

dt
dx

= θxθ

1 exp

−

xθ

,

(136)

(cid:1) (cid:12)
(cid:0)
(cid:12)
(cid:12)
F (x, θ) = 1
−

(cid:12)
(cid:12)
(cid:12)
exp

xθ

.

−
(cid:0)

−
(cid:0)

(cid:1)

Clearly, the cdf is decreasing in θ for x < 1, but increasing in θ for x > 1.

(cid:1)

49

13 Reduction to inference about location parameters un-

der more general conditions

We can provide a consistent and calibrated parameter inference only when the problem
is reducible to estimation of a location parameter. But, as we saw in the case of the
Weibull distribution, reduction to a location parameter estimation is not always possible.
Nevertheless, under very general conditions, there is a neat way out of such difﬁcult
situations.

When lacking any additional prior knowledge, all information about an inferred pa-
rameter θ of a speciﬁed sampling distribution that can be extracted from the measured
events, is contained in the value of the likelihood. After collecting n independent events
xi, the likelihood density for x reads:

f

x = (x1, x2, ..., xn)

θI) =

|

θI) .

f (xi|

(cid:0)

n

i=1
Y

Let ˆθ be the value of the parameter that maximizes the value of the likelihood density,
given a particular set of collected events x:

Suppose also that the integral

ˆθ = ˆθ(x1, x2, ..., xn) = ˆθ(x)

:

f (x
|

θI)

θ=ˆθ = max.
(cid:12)
(cid:12)

R2(θ) =

∂
∂θ

Zx (cid:16)

ln f (x′

θI)

f (x′

θI) dx′

|

|

2

(cid:17)

exists for every θ in its permissible range. Then, as the number n of measurements xi
increases, the sampling distribution for ˆθ(x) converges to a Gaussian distribution with
1 (see, for example, [20],
µ = θ and σ(θ) =
18.16,
pp. 57-58). The width of the distribution decreases with increasing n. For a sufﬁciently
large number of measurements the width can be approximated by σ(θ)

18.10, pp. 52-53 and

R(θ)√n

σ(ˆθ),

(cid:1)

(cid:0)

§

§

−

≃

σ(ˆθ) =

1
R(ˆθ)√n

,

leaving θ as a pure location parameter of the distribution for ˆθ(x),

f (ˆθ
|

θI) =

1
√2πσ(ˆθ)

exp

(ˆθ
θ)2
2σ2(ˆθ) )

−

,

(−

thus allowing to make a consistent inference about θ. The required number of collected
events depends on the precision required for the inference about the parameter and on
the form of the sampling distribution (see Figure 13 with two examples for the Weibull
distribution).

50

0.25

|

0.2

(cid:1)
I
θ

)
x
(
ˆθ
(cid:0)
f

0.15

0.1

0.05

a)

b)

|

(cid:1)
I
θ

)
x
(
ˆθ
(cid:0)
f

1

0.8

0.6

0.4

0.2

20
ˆθ(x)

0

0

5

10

15

0

0

2

4

6

8

ˆθ(x)
Figure 3: Sampling distribution for ˆθ(x) resulting from maximizations of the likelihood
density f (x
θI) that is a product of Weibull pdf’s (136) (continuous
i=1 f (xi|
|
1
line) and the limiting Gaussian or normal distribution N
(dashed line) for θ = 5 and for n being a) 5 and b) 100.

µ = θ, σ = (R(θ)√n)−

θI) =

Q

n

(cid:0)

(cid:1)

14 Inferring parameters of counting experiments

Let an urn contain balls that are identical in every respect except that some of the balls are
coloured white and the remaining ones are coloured red. Such an urn is usually referred
to as the Bernoulli urn and drawing from the urn is referred to as a Bernoulli trial. We
draw a ball from the urn blindfolded, observe and record its colour, put it back into the
urn and thoroughly shake the urn in order to minimize any possible correlations between
successive draws. Then we repeat the process until n0 balls have been drawn, out of which
n have been recorded to be white (0
n0). If correlations between the draws can be
neglected, the probability of recording n white balls in n0 draws is given by the binomial
distribution ([12],

5.2-5.7, pp. 163-168):

≤

≤

n

§

θn0I) =

p(n
|

n0
n

θn (1

θ)n0

n ,

−

−

(cid:19)
where the parameter θ of the distribution coincides with the fraction of the white balls in
the urn.

(cid:18)

On the basis of known n and n0 we would like to make a probabilistic inference about
the value of the parameter θ. Lacking any additional prior information, we write the pdf
for θ in accordance with (63):

f (θ

nn0I) =

|

π(θ) p(n
θn0I)
|
1
θ′n0I) dθ′
0 π(θ′) p(n
|

=

π(θ)
η(n, n0)

θn0I) ,

p(n
|

where the form of the consistency factor π(θ) is yet to be determined.

R

51

We showed that we are able to uniquely determine the form of a consistency fac-
tor only in presence of invariance of the sampling distribution under a continuous (Lie)
group of transformations. But in the case of counting experiments, with a discrete variate
n of the sampling distribution, such an invariance is evidently absent. In this way we
loose ground to uniquely determine the form of the consistency factor simply by follow-
ing Cox-P´olya-Jaynes Desiderata. If we insist on making a ’probabilistic’ inference, we
are therefore restricted to using ’consistency factors’, also referred to as non-informative
priors, whose forms are chosen on the basis of some ad hoc criteria. But no matter how
carefully these criteria and non-informative priors are speciﬁed, there is no guarantee
that in this way our reasoning remains consistent. Then, without being protected by the
Desiderata, we stand at the mercy of all kinds of paradoxes, stemming from inconsisten-
cies that we may unintentionally have committed.

As an example, imagine a large number of urns, each containing an unknown fraction
of white balls. We make a series of Bernoulli trials by drawing a single ball from each of
the urns, i.e. n0 = 1 for each of the draws. The outcome of drawing from the i-th urn can
be a white (n = 1) or a red (n = 0) ball and the corresponding likelihood reads:

n0θiI) =

p(n
|

1
(

θi

; n = 1
θi ; n = 0

,

−
where θi is the (unknown) fraction of white balls in the i-th urn. Let us try to make a
’probabilistic’ inference about the parameter by using a uniform non-informative prior
distribution of ’probability’ for θi:

Then, a ’pdf’ can be assigned to θi simply by using Bayes’ theorem (25):

’f (θ

I)’ = 1 .

|

’f (θi|

nn0I)’ =

’f (θi|
0 ’f (θi|
R
If (137) is truly a pdf for θ, then a ’probability’

I)’ p(n
n0θiI)
|
I)’ p(n
n0θiI) dθi
|

1

=

; n = 1
θi) ; n = 0

2θi

−

2(1

(

.

(137)

(θ1, θ2)

nn0I)’ =

|

’f (θi|

nn0I)’ dθi

θ2

θ1

Z

’P

θi ∈
(cid:0)

nn0I)’ per cent of the inferences,
should cover the true value of θi in 100 ’P
θi ∈
but it is easy to see that such an inference is in general not calibrated. Let us choose the
(cid:0)
shortest intervals with the ’probability’ of containing θi being 50% (see Figure 14):

(θ1, θ2)

|

(θ1, θ2) =

; n = 1
; n = 0

.

−
If, for example, the fraction of white balls in each of the urns were exactly one-half, our

(cid:1)

√2
2 , 1

√2
(cid:1)
2

0, 1

( (cid:0)
(cid:0)

52

’
(cid:1)
I
0
n
n

θ
(cid:0)
f
’

|

2

1

a)

b)

’
(cid:1)
I
0
n
n

θ
(cid:0)
f
’

|

2

1

’P ’

’P ’

0

0

0.5

θ1

1

θ

0

0

θ2

0.5

1

θ

Figure 4: ’Probability density functions’ assigned to the parameter of a binomial distribu-
tion when the number of draws is n0 = 1 and the result of drawing is either a) favourable
(n = 1) or b) non-favourable (n = 0). The distributions are assigned by choosing a uni-
form non-informative prior. The integrals of the ’densities’, ’probabilities’ ’P ’ (hatched
regions), on the intervals a) (θ1 = √2/2, θ2 = 1) and b) (θ1 = 0, θ2 = 1
√2/2) amount
to 0.5

−

intervals would never cover the true value, i.e. our inference is manifestly non-calibrated.
Non-informative priors of different forms, for example

’f (θ

I)’ =

|

θ(1

θ)

or ’f (θ

I)’ =

|

1

−

1
θ(1

θ)

−

(see [2],
to such kind of problems, either.

12.4.3, p. 384, eq. (12.50), and [18],

§

§

11.3, p. 106, eq. (11.63)), are not immune

p

As on two previous occasions, such an obstacle in the way of consistent parameter

inference can be overcome simply by collecting more data, such that

(138)

When the above condition is fulﬁlled, the sampling distribution for xn,

converges to a Gaussian distribution N(µ, σ) ([12],

4.15, pp. 138-140) with

n0 −

n
n

≫
≫

1 ,
1 .

xn ≡

n
n0

,

µ(θ) = θ and σ(θ) =

§

θ(1

θ)

−
n0

s

53

in the sense that

n2

n0θI) =

p(n
|

n=n1
X

xn2 + 1
2n0

≃

xn1 −
Z

1
2n0

µσI) dx =

f (x
|

n0θI)

n2

p(xn|
xn2 + 1
2n0

n=n1
X

xn1 −
Z

1
2n0

1
√2πσ

exp

−

(cid:26)

(x

µ)2

−
2σ2

dx .

(cid:27)

The dispersion parameter σ(θ) of the limiting distribution decreases with increasing num-
ber of draws from the urn and for sufﬁciently large n0 it can be approximated by

σ(θ)

σ(xn) =

≃

s

xn(1

xn)

.

−
n0

In this way, θ becomes a pure location parameter of a Gaussian distribution with the
corresponding uniform consistency factor and with the corresponding pdf:

f (θ

xnn0I) =

|

π(θ)
η(xn)

p(xn|

n0θI) =

1
√2πσ(xn)

exp

−

(cid:26)

θ)2
(xn −
2σ2(xn)

.

(cid:27)

|

At this point we ﬁnd it appropriate, for the sake of completeness, to make a comment
on the density of information nn0I that probability p(θ
nn0I) is conditional upon. Re-
call that a continuity requirement concerning sets of different states of knowledge about
inferred propositions was listed within the common sense Desideratum II. But with I
merely representing information about the type of counting sampling distribution, and
with n0 = 1 being a ﬁxed number of experiments performed, the possible information
about θ after the ﬁrst trial consists two atoms, n = 0 and n = 1, that do not allow for
such a requirement to be met. In this way the proof of Cox’s Theorem (see, for example,
[10], or [2],
2.1-2.2, pp. 24-45), stating that the basic Desiderata necessarily imply (up to
an isomorphic transformation) the product rule (1), the sum rule (2), and their corollary,
Bayes’ Theorem (25), misses an indispensable fact. That is, in a situation like that there is
no explicit reason why an inference about a parameter of a sampling distribution should be
made in accordance with the Consistency Theorem that is deduced by assuming, among
other things, Bayes’ Theorem to be the only consistent way to update the probability dis-
tribution for the inferred parameter (recall Section 6). However, in the dense limit (138),
the continuity of information (i.e. of n/n0) is recovered and the procedure of inference
about θ becomes uniquely determined by the adopted Desiderata.

§

Note that for a consistent and calibrated inference both conditions of (138) must be

met. Suppose for a moment that only

holds so that the binomial sampling distribution can be approximated by the Poisson limit
([12],

5.8-5.9, pp. 168-171):

§

n0 −

n

≫

1

µI) =

p(n
|

µn
n!

e−

µ ,

54

where the parameter µ of the distribution represents the expected number of white balls
drawn. Suppose that only red balls are drawn, i.e. n = 0, and that we want to make
a ’probabilistic’ inference about µ by choosing a uniform non-informative prior. The
corresponding ’pdf’ for µ then reads:

’f (µ

n = 0 I)’ =

|

which implies a ’probability’ for µ
R
≤

I)’ p(n = 0
|
I)’ p(n = 0
|

µI)
µ′I) dµ′

= e−

µ ,

∞
0

’f (µ
|
’f (µ′|
µ0,
µ0

µ0|
or, equivalently, implies a ’probability’ for µ > µ0,

nI)’ =

’P (µ

’f (µ

0
Z

≤

|

n = 0 I)’ dµ = 1

e−

µ0 ,

−

’P (µ > µ0|

nI)’ =

∞

µ0

Z

|

’f (µ

n = 0 I)’ dµ = e−

µ0 ,

∼

∼

where µ0 takes an arbitrary positive value. For example, for µ0 = 3, the two ’probabilities’
equal

0.05, respectively.

0.95 and

Imagine now such draws from a number of urns N, all ending up with ni = 0. We can
make use of the general sum rule (3) and calculate a ’probability’, ’PN ’, for at least one
out of N parameters µi being greater than µ0. Since the draws from different urns are not
correlated, the ’probability’ equals:

’PN ’ = 1

1

’P (µ > µ0|

−

nI)’

−

(cid:16)

N

.

(cid:17)

With increasing N, the value of ’PN ’ approaches unity: for a sufﬁciently large number
of urns we can claim with ’certainty’ that at least for one of the urns the parameter µi is
greater than µ0, regardless of the chosen value of the latter. But claiming the existence
of white balls in the urns on the basis of observing only red ones, is clearly a logically
unacceptable result, pointing to serious ﬂaws of this kind of inference. Note that it was
not the choice of the uniform non-informative prior that was decisive for the above result
since every ’f (µ

I)’ with the existing integral

|

∞

0

Z

’f (µ′

I)’ p(n = 0

µ′I) dµ′

|

|

would lead to the same kind of a logically unacceptable conclusion.

If an urn contains only red balls, the requirement

n

1

≫
can never be met. Is there some kind of inference that could be made in such cases? Let
us perform two Bernoulli trials by drawing n0 and 2n0 balls from an urn, with all of the
drawn balls in both trials being red. The evidence against the presence of white balls in
the urn that was obtained by the second trial may be reasonably held to be stronger than
the evidence from the ﬁrst trial. Yet how much stronger? It seems to us that in such cases
the degrees of belief, although still comparable, cannot be expressed quantitatively (see
also [4], Chapter XV, p. 137), i.e. in order to avoid all sorts of paradoxes we should remain
on a qualitative level.

55

15 Historical digression

In Section 6 we stressed that, contrary to the prior probability distribution in Bayes’ The-
orem (25 – 26), the consistency factor in equations (63 – 66) does not represent any kind
of probability distribution. Overlooking this fact led to perpetual philosophical argu-
ment throughout the history of probabilistic reasoning, with far-reaching practical conse-
quences.

A natural starting point for every sequential updating of information is a state of com-
plete ignorance, e.g. complete prior ignorance about the value of an inferred parameter θ
of a sampling distribution for x. The original sin is then committed in an attempt to make
use of Bayes’ Theorem, (25) or (26), in such a situation. According to the Theorem, in
order to obtain a probability distribution for the inferred parameter after observing x1 both
the likelihood, i.e. the probability for observing x1 given a particular value of the param-
eter, and a prior probability distribution for θ, i.e. a distribution of our belief in different
values of the parameter prior to observing x1, must be provided. With the known form of
the sampling distribution, the calculation of the likelihood is a rather straightforward task.
The problem arises when we try to formulate the distribution of our belief prior to the
ﬁrst recorded event, for up untill then we had been completely ignorant about the possible
values of θ. What we are trying to do is to assign a prior probability distribution based
on ignorance, i.e. we are trying to establish the so-called ignorance or non-informative
probability distributions. But according to the deﬁnition of probability adopted as a de-
gree of reasonable belief that is based on relevant information at hand (see Section 2),
a probability assigned on grounds of ignorance is simply a contradiction in terms. A
prior probability distribution that is based only on ignorance thus cannot be the realm of
a consistent probability theory. We will see that apart from being self-contradicting on
the conceptual level, the concept of ignorance or non-informative priors inevitably also
produces many practical inconsistencies and paradoxes.

This delusion about probability distributions based on ignorance has been present for
more than two centuries. In a scholium to his essay [13] Bayes suggested that in the
absence of all prior knowledge it is reasonable to assume a uniform distribution for p,
where p stands for, for example, an unknown fraction of white balls in an urn experiment
as described in Section 14. Laplace ([5], p. XVII) was also very explicit on the same
subject:“When the probability of a single event is unknown we may suppose it equal
to any value from zero to unity.” The above assumption is usually referred to as the
8.19, p. 298), the Laplace principle of insufﬁcient reason [31] or
Bayes principle ([12],
the principle of indifference ([2],
2.4, p. 40). When applied to estimation of a general
unknown parameter, the assumption would read: in the limit of complete prior ignorance
about the value of an inferred parameter, the prior distribution should be uniform.

§

§

We strongly disagree with such a rule and propose a very simple rule to replace it:
when there is no (prior) information, no (prior) probabilities are to be assigned whatso-
ever. Or to paraphrase de Finetti: prior probability does not exist. Knowing that a uniform
prior probability distribution in the range (θa, θb) has been assigned to the value of a pa-
rameter θ as a result of positive knowledge, and not knowing anything about θ with the

56

exception of its admissible range, are two fundamentally different states of knowledge.

In order to illustrate the difference, we return to drawing coloured balls from urns.
Imagine having a good number of urns, each containing red and white balls, with the
fractions of white balls, θi, in each of the urns being unknown. According to the ﬁrst
scenario, we possess information that the fractions are distributed uniformly in the range
(0,1), while according to the second scenario we know nothing about the distribution of
the values of θi. It is easy to see that the two states of knowledge are completely different.
In the ﬁrst case it is possible to assign a probability for θi of the i-th urn which is in an
[0, 1] even before drawing the ﬁrst ball from the urn simply

arbitrary interval (θ1, θ2)
by integrating the (uniform) prior probability distribution:

⊂

P

θi ∈
(cid:0)

(θ1, θ2)

I

=

f (θ

I) dθ =

θ2

θ1

Z

|

|

(cid:1)

θ2

θ1

Z

dθ = θ2 −

θ1 .

(139)

On making the ﬁrst draw from the i-th urn, it can be either a white (ni = 1) or a red
(ni = 0), implying the corresponding likelihood to be:

p(ni|

θiI) =

(1

(

; ni = 1
θi) ; ni = 0 ,

θi

−

The updated posterior probability therefore reads:

P

θi ∈

(cid:0)

(θ1, θ2)

niI

=

|

(1

(

−

(cid:1)

θ2
2 −
θ1)2
−

θ2
1
(1

−

; ni = 1
θ2)2 ; ni = 0 ,

(140)

|

θi ∈
(cid:0)

where the update was made by using Bayes’ Theorem (25). Note that both probabilities
are calibrated: when repeating the above assessments for all of the urns, intervals (θ1, θ2)
contain the true fractions of white balls in the each of the urns in 100P
I
percent of cases before any of the balls were drawn, while after drawing the coverage is
(cid:1)
exactly P

θi ∈
(cid:0)

(θ1, θ2)

(θ1, θ2)

niI

|

.

(cid:1)

Within the second scenario, with complete prior ignorance about the distribution of θi
within the urns, we cannot make a probabilistic inference about fraction of white balls in
a particular urn before any of the balls is drawn. The statement that the probability for
θi to be in the interval (θ1, θ2) equals the length of the interval (see eq. (139)) would in
general be uncalibrated unless the distribution of θi within the urns is truly uniform - but
this we do not know and need not be true. The same holds for the probability statement
(140) after drawing one ball from each of the urns. We saw in the previous section that
in the limit of complete prior ignorance, consistent and calibrated probability statements
about the parameters θ can be made only after drawing enough balls from each of the urns
so that the Gaussian limit of the binomial sampling distribution can be applied.

Many failed to recognize the fundamental difference between knowing the prior prob-
ability distribution of an inferred parameter to be uniform and not knowing anything about
the value of the parameter. Harold Jeffreys, for example, wrote ([15],
1.22, p. 29): “If
there is originally no ground to believe one of a set of alternatives rather than another,

§

57

§

their prior probabilities are equal.” The same standpoint was persistently advocated also
by Edwin Jaynes ([2],
18.11.1, p. 573): “Before we can use the principle of indifference
to assign numerical values of probabilities, there are two different conditions that must
be satisﬁed: (1) we must be able to analyze the situation into mutually exclusive, exhaus-
tive possibilities; (2) having done this, we must then ﬁnd available information gives us
no reason to prefer any of the possibilities to any other. In practice, these conditions are
hardly ever met unless there is some evident element of symmetry in the problem. But
there are two entirely different ways in which condition (2) might be satisﬁed. It might be
satisﬁed as a result of ignorance, or it might be satisﬁed as a result of positive knowledge
about the situation.”5

We could not agree more with Jaynes if it were not for the last sentence. Take, for
example, two different samples of radionuclides. For the ﬁrst sample we do not know
anything about the isotopes except that the permissible range of their expected decay
times is in an interval (τa, τb), while for the second sample the isotopes were chosen in
such a way that the distribution of their expected decay times can be well approximated
by a uniform distribution in the same interval. As a result of ignorance, we cannot make
inferences about the expected decay times τi of the isotopes in the ﬁrst sample before mea-
suring their actual decay times ti. After the decay time ti of the i-th isotope is measured,
however, we can make a probabilistic statement about τi according to (64):

f (τi|

tiI) =

π(τi)
η(ti)

τiI) =

f (ti|

τiI) =

f (ti|

ti
τi

ti
τ 2
i

e−

ti/τi ,

where I stands for prior ignorance about the expected decay time of a particular radionu-
clide, while the consistency factor π(τi) was chosen according (110). From Section 11
we recall that our inference is consistent if the integral of the above pdf outside the ad-
missible range for τi is small compared to the precision required. For the second sample
of isotopes, as a result of positive knowledge, there is a pdf for the expected decay time
of each of the isotopes at our disposal even prior to measuring the actual decay times:

I ′) =

f (τi|

(τb −
0
(

τa)−

1 ; τa < τi < τb
; otherwise ,

where I ′ stands for positive prior knowledge. After ti is measured (and not knowing
the results of other possible measurements of decay times from the same sample), the
distribution of our belief can be updated by means of Bayes’ theorem (29):

tiI ′) =

f (τi|

f (τi|
f (τ ′i|
Note that when appropriate prior pdf’s exist, the inferences are always consistent and cali-
brated. Figure 5 shows examples of inferences with and without existing prior probability
distributions.

I ′) f (ti|
I ′) f (ti|

τiI ′)
τ ′iI ′) dτ ′i

τb
τa

R

.

5A very similar idea expressed in very similar words by Anthony O’Hagan can be found in [16],

4.39,

§

p. 112.

58

|

0.5

(cid:1)

′

I

i
τ
(cid:0)
f

0.4

0.3

0.2

0.1

0

0

a)

b)

0.5

|

0.4

(cid:1)
)
I
(
′

I
i
t

i
τ
(cid:0)
f

0.3

0.2

0.1

0

0

2

4

6

8

2

4

6

8

10
τi

10
τi

Figure 5: Probability density functions for the expected decay time τi of the i-th isotope
from a sample of isotopes with the expected decay times distributed approximately uni-
formly in an admissible range (τa, τb) = (0, 10) (hatched areas), and the corresponding
pdf for an isotope from a sample with the expected decay times in the same interval but
with an unknown distribution of their actual values (shaded area). Figure a) displays the
pdf before any of the actual decay times are measured, while Figure b) shows appropri-
ate pdf’s based on a measured value ti = 1 in both cases. All decay times are given in
arbitrary but equal units. While the hatched histograms are limited within the permissible
range, the integral of the shaded pdf in the range (10,

) is approximately ǫ2 = 0.095.

∞

Apart from the problems with calibration, the Bayes postulate contains also some
insurmountable inconsistencies. Suppose we are estimating a parameter θ with no prior
information available. According to the postulate, the prior ’probability’ distribution in
such a case is given by a uniform ’pdf’:

’f (θ

I)’ = 1.

|

θ

λ

But, instead of θ, we could have equally well chosen different parameterization, say λ(θ),
where

−−−→
is a bijective parameter transformation. Then, due to the consistency Desideratum III.c,
the appropriate prior ’pdf’ for the transformed variate λ, obtained according to (16), reads:

’f (λ

I)’ = ’f

θ(λ)

I

’

=

dθ
dλ

dθ
dλ

.

|

|
In the case of a non-linear transformation (141), the absolute value of the derivative in
(142) is not a constant, i.e. the prior ’pdf’ for λ is not uniform. But since a one-to-one
mathematical transformation like (141) does not change the state of knowledge about the
inferred parameters, we also remain completely ignorant about λ. The Bayes postulate
would therefore imply a uniform prior distribution for λ, which obviously contradicts
(142). That is, Bayes postulate in general contradicts consistency Desideratum III.c).

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:1)

(cid:0)

(141)

(142)

59

A sophisticated version of the principle of insufﬁcient reason is referred to as the
principle of maximum entropy. The term information entropy was introduced by Claude
Shannon ([29],

6) as

§

S

≡ −

pi ln pi ,

i
X
where pi is the probability assigned to inferred parameter θ of taking a value within an
interval (θi, θi + dθi), and the sum covers the whole admissible range of θ. In order for it
to be used for determination of non-informative prior distributions, pi is interpreted as

pi ≡

’p(θi|

I)’ = ’f (θi|

I)’ dθi ,

I)’ stands for the non-informative prior ’pdf’ for θ. The principle of maxi-
where ’f (θi|
mum entropy then states ([2],
I)’ which maximizes
11.3, pp. 350) that the function ’p(θi|
entropy represents the most honest description of what we know about the value of the
inferred parameter.

§

Suppose, for example, that we know only that θ

(θa, θb) is a parameter of a speciﬁed
sampling distribution, and that the prior ’probability’ distribution for θ is subject to the
usual constraint

∈

’p(θi|

I)’ =

pi = 1 .

i
X

i
X

(143)

Then, the principle of maximum entropy and the above constraint are taken simultane-
ously into account if pj maximizes the function H,

that is, if

H

S

α

≡

−

1

pi −

=

−

pi ln pi −

α

i
X

1

pi −

,

(cid:17)

i
(cid:16)X

i
(cid:16)X

∂H
∂pj

(cid:17)

−

=

(ln pj + 1 + α) = 0 ,

where α is a Lagrange multiplier. It is therefore the constant non-informative prior ’prob-
ability’ distribution

pj = ’f (θj|

I)’ dθj = e−

(1+α) = constant

that maximizes the information entropy, subject to the usual normalization condition
(143). If we choose intervals of uniform width

the principle of maximum entropy yields a uniform non-informative prior ’pdf’. But a
non-linear one-to-one re-parameterization

implies widths of intervals

dθi = constant ,

θ

−−−→

λ(θ)

dλi =

dθi

∂λ
∂θ

60

(cid:12)
(cid:12)
(cid:12)

θ=θi
(cid:12)
(cid:12)
(cid:12)

and the corresponding ’pdf’

’f (λi|

I)’ = ’f (θi|

I)’

∂λ
∂θ

1

−

θ=θi

,

∂λ
∂θ

1

−
θ=θi ∝
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
neither of the two being uniform. And so an immediate question can be raised which
(cid:12)
(and why) is the distinguished parameterization of the sampling distribution with both the
interval widths and the non-informative prior ’pdf’ being uniform. In other words, this
means that the principle of maximum entropy cannot solve the ancient ambiguity of how
to ﬁnd the elusive non-informative ’probability’ distributions.

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

§

Jeffreys ([15],

3.1) proposed the following solution to the problem of non-informative
priors. He suggested that for parameters with the admissible range coinciding with the
whole real axis we should keep to Bayes postulate, i.e. to the uniform priors, while for
parameters known to be positive the proper way to express complete ignorance is to as-
sign uniform prior probability to its logarithm, i.e. in the latter case the prior ’pdf’ should
be:

’f (θ

I)’

|

∝

; θ

(0,

) .

∈

∞

1
θ

(144)

He tried to justify this on the grounds of invariance of the former prior under translation,

and on the grounds on invariance of the latter prior on raising θ to a power of n or to
scaling it by a positive constant a:

λ = θ + b ; b

(

,

) ,

∈

−∞

∞

λ = θn and λ = aθ .

Why invariance of the non-informative priors under these particular transformations?
For the location, scale and dispersion parameters the answer was extensively searched for
by utilizing related transformation groups (see refs. [30], [31], [18] and Chapter 12 in
[2]).

For example, when we are completely ignorant about location µ of a distribution for

x prior to starting to collect data, a mere shift of location,

µ

−−−→

µ′ = µ + b ,

could not change our state of knowledge. That is, according to the principle of group
invariance, the non-informative prior ’probability’ distribution for µ should be invariant
under a group of transformations (145). It is easy to see that the uniform prior is the only
one that satisﬁes this condition.

Similarly, when we are completely ignorant about the scale τ of a distribution for t,
the prior ’probability’ distribution for τ should be invariant under a scale transformation,

(145)

(146)

τ

−−−→

τ ′ = aτ .

61

The only non-informative prior ’probability’ distribution satisfying the above condition,
is expressed by Jeffreys’ prior (144). To see that, we should realize that the required
invariance implies equality for prior ’probabilities’

and

or, equivalently,

|

’p(τ ′

I)’ = ’f (τ ′

I)’ dτ ′ =

φ(τ ′) dτ ′ =

φ(aτ ) a dτ ,

’p(τ

I)’ = ’f (τ

I)’ dτ = φ(τ ) dτ

|

|

|

|

φ(τ ) =

e
φ(aτ ) a .

e

∈

∞

e

’f (τ ′

I)’ = φ(τ ) =

φ(1)

,

1
τ

If this is to be true for any a

(0,

), it must also be true for a = τ −

1. Hence:

and the above statement about Jeffreys’ prior is proved.

e

In the same spirit, a non-informative prior ’probability’ distribution representing com-
plete ignorance about both location µ and scale σ should be invariant under simultaneous
location and scale transformations:

Following the idea of the proof from the preceding example leads to a non-informative
’pdf’ of the form:

µ

σ

−−−→

−−−→

µ′ = aµ + b ,

σ′ = aσ .

’f (µσ

I)’ =

|

1
σ2 .

(147)

(148)

However, strong objections can be raised against the form of (148). For example, non-
informative prior (148) leads to a marginalization paradox (see, for example, references
[32] and [33], and Chapter 10, pp. 81-94 of reference [18]). Let µ be a location parameter
with known ﬁxed value, i.e. we are inferring only a dispersion parameter σ. By inserting
the non-informative Jeffreys’ prior (144) in the Bayes’ Theorem (26) we obtain:

’f (σ

µ¯xsI)’ =

|

’f (σ
|
’f (σ′|

I)’ f (¯xs
|
I)’ f (¯xs
|

∞
0

µσI)
µσ′I) dσ′ ∝

1
σ

f (¯xs

µσI) .

|

(149)

Note that the result is identical to the one obtained by inserting the appropriate consistency
factor (110) into the Consistency Theorem (64). But we can also obtain a ’pdf’ for σ
given µ, ¯x and s in another way. We start with a simultaneous inference about unknown
parameters µ and σ:

R

’f (µσ

¯xsI)’ =

’f (µσ
I)’ f (¯xs
|
0 dσ′ ’f (µ′σ′|
R
Then we make use of the product rule (9) and rewrite the above expression as:

µσI)
|
I)’ f (¯xs

1
σ2 f (¯xs

∞
−∞
R

µ′σ′I) ∝

dµ′

∞

|

|

|

µσI) .

’f (µσ

¯xsI)’ = ’f (σ

µ¯xsI)’ ’f (µ

¯xsI)’ ,

|

|

|

62

where ’f (µ

¯xsI)’ is marginalized ’pdf’ (see eq. (23)):

|

’f (µ

¯xsI)’ =

’f (µσ′

¯xsI)’ dσ′ .

|

|

∞

0

Z

By combining the last three equations we can therefore write down the ’pdf’ for σ given
µ and observed ¯x and s also as:

’f (σ

µ¯xsI)’ =

|

’f (µσ
’f (µ

¯xsI)’
|
¯xsI)’ ∝

1
σ2 f (¯xs

|

µσI) .

|

(150)

The pdf’s (149) and (150) are in an obvious contradiction, i.e. the non-informative priors
(144) and (148) are inconsistent. For further discussion about the marginalization paradox
see Appendix B.

§

§

marginalization (23) to be illegitimate (see, for example, ref. [18],

When recognizing this fundamental difﬁculty, some authors claim the procedure of
10, pp. 81-94 and
17.2, pp. 163-164), despite the fact that - according to Cox’s Theorem - the procedure
§
is implied by the basic Desiderata. Others apply transformations different from (147) in
attempts to solve the problem (see, for example, [2],
12.4, p. 378, equation (12.18)).
But for the latter, since the modiﬁed transformations do not correspond to a simultaneous
translation and scale transformation, the original motivation to relate the complete prior
ignorance about location, scale and dispersion parameters to invariance of the correspond-
ing non-informative priors with respect to the transformations (145-147), is deﬁnitely lost.
Note that the formalism related to the principle of group invariance of non-informative
priors is remarkably similar to the formalism applied for determination of consistency
factors. But no matter how strong this similarity may appear at ﬁrst glance, there is a fun-
damental difference between the two methods, leading to substantially different results.
Group invariance of non-informative prior probabilities is imposed as a principle, addi-
tional to the basic Desiderata, while we made use of invariance of likelihoods (being well
deﬁned probabilities) as a necessary condition for equivalence of two states of knowl-
edge that led to functional equation (73) for consistency factors. In the latter case it is
the ratio of π(θ) dθ and η(x) dx that is to be invariant under simultaneous transformations
of the inferred parameter θ and the sampling variate x, while
¯ga(θ)
π(θ) dθ itself need not be invariant under ¯
since it is, by deﬁnition, uniquely determined
G
only up to a multiplicative constant; and it is due to this degree of freedom that marginal-
ization paradoxes, stemming from strict applications of the principle of group invariance,
are avoided.

and ga(x)

∈ G

¯
G

∈

In practice, the difference between the probability distribution assigned simultane-
ously to a location and a dispersion parameter by multiplying the appropriate likelihood
by the corresponding consistency factor (114), and the ’probability’ distribution assigned
by utilizing the non-informative prior (148), will fade away with increasing number of
collected events. So, from a pragmatic standpoint, arguments about which function cor-
rectly expresses a state of complete prior ignorance might amount to quibbling over pretty
6.15, p. 183). But, from a standpoint of principle, this is deﬁnitely
small peanuts ([2],

§

63

not true, for the convergence of the limiting distributions by itself certainly does not guar-
antee either of the two to be correct, i.e. we might have been completely wrong in both
cases. Fundamental difﬁculties with non-informative priors introduced very serious con-
sequences for inductive reasoning. For example, applications of the Laplace principle of
insufﬁcient reason that led to logically unacceptable results, provided P´olya ([4], Chap-
6, pp. 133-136) with the main reason to persist in a qualitative level when linking
ter XV,
induction with probability. Others (see, for example, references [34] and [35]) refused
even a qualitative correspondence. As noted already by Jeffreys ([15],
3.1, p. 120), “a
succession of authors have said that the prior probability is nonsense and therefore that
the principle of inverse probability, which cannot work without it, is nonsense too.”

§

§

In this way two, at ﬁrst glance fundamentally distinct, schools of inductive reason-
ing emerged. The ﬁrst one, usually referred to as the Bayesian school due to the central
role of the Bayes’ theorem in the process of inference, recognizes probability as a degree
of reasonable belief and applies probability theory in the course of inductive reasoning.
The second one, usually referred to as the frequentist school due to its strict frequency
interpretation of probability, advocates the usage of the calculus of probability only for
treatment of so-called random phenomena. The aim of the frequentist school is to avoid
the supposed mistakes and inconsistencies of the probabilistic inductive inference, so they
relegate the problems of inductive inference, e.g. estimations of distribution parameters,
to a new ﬁeld, statistical inference. Lacking applications of probability theory may, how-
ever, represent a serious drawback when making inductive inferences. For example, the
Fisher-Behrens problem, introduced in Section 10, may become an insurmountable ob-
19.47, Ex-
stacle outside the probabilistic parameter inference (see, for example, [20],
ample 19.10, pp. 160-162 and
26.28-26.29, pp. 441-442). In particular, difﬁculties stem
from the illegitimacy of the marginalization procedure within the theory of statistical in-
ference.

§

§

Some of the substitutes for the calculus of probability that are proposed within the
framework of statistical inference, are put forward as solutions of speciﬁc problems, such
as the principles of least squares and minimum chi squared. The principle of Maximum
Likelihood [34], however, is usually advocated as one of general application (see [12],
8.22-8.27, pp. 300-304, and [20], Chapter 18, pp. 46-104). The principle states that,
§
(θa, θb), we choose
when confronted with a choice of the values of a parameter θ
the particular value ˆθ which maximizes the corresponding likelihood, p(x
θI), for the
|
observed data x. In general, the principle contradicts our basic Desiderata. Imagine a
problem of inference when there is, apart from x, some additional information I at hand
that would allow for an assignment of probability distribution for θ on its own. Neglecting
this information directly contradicts Desideratum III.b. For special cases, when the pdf
I), is uniform in a wide interval around ˆθ, the
for θ based on prior information I, f (θ
contradiction is removed and the principle can no longer be disputed.

∈

|

When there is no prior knowledge about the value of a particular inferred parameter,
according to the consistency Desiderata, the inference should be made on the basis of both
the likelihood containing the information about θ from the measurement x, and the consis-
tency factor containing information about θ coming from the known form of the sampling

64

distribution for x. Ignoring the latter in general implies our reasoning to be inconsistent,
i.e. to be in a direct contradiction with Desideratum III.c. Imagine, for example, two per-
sons inferring average decay times τi of unknown particles on the basis of single decay
time measurements ti. Mr. A proceeds according to the Maximum Likelihood principle
by extracting values

ˆτ A
i = ti
of the parameter that maximizes the likelihoods

p(ti|

τiI) = f (ti|

τiI) dt =

1
τi

e−

ti/τi dt

for observing measured decay times in an interval (ti, ti + dt), given particular τi’s. In
accordance with the adhered principle, ˆτ A
should be the value of the parameter being
i
indicated by the datum ti as the strongest candidate, i.e. in the long term the fraction of
Mr. A’s conﬁdence intervals, (ˆτ A
i + dτ ), covering the true values of inferred param-
eters, should be larger than the corresponding fractions of any other interval of the same
width, dτ . Mr. B, on the other hand, chooses the consistent procedure: he extracts the
values

i , ˆτ A

ˆτ B
i =

ti
2

of the parameters that maximize the pdf’s for τi (see equations (64) and (110)),

f (τi|

tiI) =

π(τi)
η(ti)

τiI) =

f (ti|

ti
τ 2
i

e−

ti/τi ,

taking this way into account both information from the immediate data ti, and information
I about the form of the sampling distribution that is contained in the consistency factor
It is easy to see that, contrary to the claims of Mr. A, the coverage of Mr. B’s
π(τi).
conﬁdence intervals for τi, (ˆτ B
i +dτ ),
by a factor of 4 e−

i +dτ ), surpasses that of Mr. A’s intervals, (ˆτ A

i , ˆτ A

i , ˆτ B

1.47.

1

Again, for special cases when inferring explicit location parameters, the consistency

≃

of the principle of Maximum Likelihood is restored.

§

Last but not least, in order to avoid unnecessary though frequent misunderstandings,
we would like to clarify the following. Most of the advocates of the so-called subjective
Bayesian school of thought ﬁnd both the existence of complete prior ignorance about
the inferred parameter ([16],
4.15, p. 102), and the existence of an exact equivalence
of information possessed by two different persons inferring the same parameter ([16],
1.16, p. 11), impossible and thus irrelevant to a theory of inductive reasoning. We believe
§
that such statements are as poorly grounded as it would have been absurd, for example,
rejecting the use of right angled and similar triangles when constructing images within
geometrical optics just due to the fact that no real triangle is exactly right angled and that
no two real triangles are exactly similar. In Section 2 we demonstrated that a particular
state of knowledge is exactly the same as the state after an arbitrary one-to-one variate
transformation, while in Section 7 we stressed that ignorance is just a limiting state of

65

knowledge and the natural starting point in any actual inference, just as zero is the natural
starting point in adding a column of numbers.

Similarly, it might also have been argued for experiments like drawing balls from
urns, tossing dice, or collecting the decay times of unstable particles, are oversimpli-
ﬁed and therefore not adequate for calibration of methods derived for the real scientiﬁc
inferences about unknown parameters. But such simple (not oversimpliﬁed) experiments
usually serve as a paradigm for all experiments with well known sampling distributions of
the collected data, and with well controlled experimental conditions, both features being
among the basic assumptions of our theory (see Sections 3 and 7).

16 On the probability of general hypotheses

Theories are nets: only he who casts will catch.

Friedrich von Hardenburg (Novalis)

Inference about a parameter, say θ, of a distribution for a certain sampling variate,
say x, is, by deﬁnition, always conditional on a speciﬁed model (recall Section 3). The
value of a parameter is always estimated under the assumption that within the speciﬁed
family of distributions, with each member of the family being completely determined
by the value of the inferred parameter, there is a distribution, speciﬁed by the so-called
true value of θ, that corresponds to the actual sampling distribution of x. How well can
such an assumption be justiﬁed: what is the probability for the speciﬁed model to be
true? Or, putting it in a wider framework, what is the probability for a general hypothesis
I) of Newton’s law of universal
A to be true? For example: what is the probability p(A2|
gravitation [36], here denoted by A2, judged in the light of the facts I collected in the ﬁrst
edition of the Principia?

In order to answer such a question, we must be able to analyze the situation into
mutually exclusive, exhaustive possibilities A1, A2, ... in order to allow for the usual nor-
malization

i
X

I) = 1 .

P (Ai|
Recall that the unity in the above expression is only a matter of convention within Desider-
atum I, but a normalization of the probability for an exhaustive set to an arbitrary (positive)
constant value is necessary in order to deﬁne the scale of the assigned probabilities. For
example, without such a normalization it would have been impossible to say whether a
I) = 0.75, is either high or low.
certain probability, say p(A2|
Unfortunately, the normalization represents a task that cannot be consistently accom-
plished. First of all, the absolute status of a hypothesis embedded in the universe of all
conceivable theories cannot be stated. That is, its probability within the class of all con-
ceivable theories is neither large or small; it is simply undeﬁned because the class of all
conceivable theories is undeﬁned ([2],

9.16.1, p.310).

Consequently, it would only be possible to express the plausibility of a hypothesis

§

66

within a class of well–deﬁned alternatives. But what is the criterion that a hypothesis
should satisfy in order to take a place within such a class? Let us, for example, choose a
class of alternatives An to Newton’s law of universal gravitation such that the gravitational
attraction between two planets could be inversely proportional to the n-th power of the
distance r between the planets, where n is limited to positive integers. Since we are
not able to justify the restriction of n to positive integers only – these have been chosen
completely arbitrarily – we extend the class by allowing n to take any real value. Then,
since there is no obvious reason that the alternatives should be limited to power–laws, we
add hypotheses from the exponential family to our class. And so on and so forth, into
an inﬁnite regress that brings us to the conclusion that there is no such thing as a well–
deﬁned class of alternatives that would allow for expressing the quantitative plausibility
of a hypothesis.

Scientiﬁc theories can never be justiﬁed, or veriﬁed. Under certain circumstances a
hypothesis A can be trusted more than a hypothesis B – perhaps because B is contra-
dicted by certain results of observations, and therefore falsiﬁed by them, whereas A is not
falsiﬁed; or perhaps because a greater number of predictions can be derived with the help
of A than with the help of B [37]; or because the likelihood for observing measured data
x, given A is correct, p(x
BI), con-
|
ditional on B being true. The best we can say about an hypothesis is that up to now it has
been able to show its worth, and that it has been more successful than other hypotheses
although, in principle, it can never be justiﬁed or veriﬁed; we saw that we cannot even
state its probability for being true.

AI), is higher than the corresponding likelihood p(x
|

Some inductive arguments are stronger than others, and some are very strong. But how
much stronger or how strong we cannot express [38]. If predictions made by a theory are
borne out by future observations, then we become more conﬁdent of the hypotheses that
led to them; and if the predictions never fail in vast number of tests, we come eventually
to call them physical laws. On the other hand, if the predictions prove to be wrong, we
have learned that our hypotheses are wrong or incomplete, and from the nature of the
error we may get a clue as to how they might be improved ([2],
9.16.1, p.311). But there
§
is absolutely no guarantee for the corrected theory to be correct.

After all has been said and done, it becomes evident that there is nothing absolute
about the theory of consistent inference about parameters of sampling distributions. The
theory does not rest upon a rock–bottom: an inference about a parameter is necessarily
conditional on a speciﬁed model (i.e. on a speciﬁed family of sampling distributions)
whose truth we are never able to prove. The whole structure of the theory rises, as it
were, above a swamp. It is like a building erected on piles. The piles are driven down
from above into the swamp, but not down to any natural or given base; and when we
cease our attempts to drive our piles into a deeper layer, it is not because we have reached
ﬁrm ground. We simply stop when we are satisﬁed that they are ﬁrm enough to carry the
structure, at least for the time being ([8],

30, p. 111).

§

67

17 Conclusions

This article presents an attempt to formulate a consistent theory for inferring parameters
of sampling probability distributions. The theory is developed by following general rules,
referred to as the Cox-P´olya-Jaynes Desiderata. We extended the existing applications of
the Desiderata in order to allow for consistent inferences in the limit of complete prior
ignorance about the values of the parameters.

|

Starting from that limit, the Consistency Theorem (63, 66) is to be used for assigning
probabilities on the basis of the collected data. The form of the Theorem is very similar
to the form of Bayes’ Theorem (25) that is used for updating the assigned probability
distributions, but we stressed an important difference between the two. While in Bayes’
Theorem the prior probability f (θ
I) represents a distribution of credibility of different
values of the parameter θ that is based on information I prior to the inclusion of data x in
our inference, π(θ) in Consistency Theorem is just a consistency factor that depends on
the form of the sampling distribution and is determined in a way that ensures consistent
reasoning. Contrary to prior and posterior probabilities for an inferred parameter θ, p(θ
I)
xI), and to the likelihood for an
and p(θ
|
observed x given θ, p(x
θI), the consistency factors by no means represent any kind of
|
probability distributions and should not be confused with the ill-deﬁned non-informative
priors. That is, no probabilistic inference is ever to be made on the basis of the form of
the consistency factor alone. When this is recognized, there is absolutely no need for the
factors to possess (or not to possess) any of the properties of the pdf’s: we ﬁnd arguments
3.27-
for the factors to be either normalizable (see, for example, references [33], [16],
15.12, p. 488) or non-normalizable (see, for example, [15],
3.29, pp. 77-78, and [2],

xI), to the corresponding pdf’s, f (θ

I) and f (θ

§

|

|

|

3.1, p. 121, or [2],

15.10, p. 485) completely irrelevant.

§

The developed theory is only an effective one. We met several examples where the
prior information and the collected data were too meagre to permit a consistent param-
eter inference. We saw that, under very general conditions, the remedy is just to collect
more data relevant to the estimated parameter. Probability theory does not guarantee in
advance that it will lead us to a consistent answer to every conceivable question. But, on
second thoughts, this shuld not be too big a surprise, since we are accustomed to effective
theories in all branches of science outside pure mathematics. Formulating an absolute
theory of inductive reasoning, based on imperfect information, might just turn out to be
an overambitious task.

By giving up the idea of the existence of (self-contradicting) non-informative prob-
ability distributions, and the illusion of an absolute theory, all paradoxes and inconsis-
tencies, extensively discussed in the preceding sections, are solved and we arrive at a
position where we can write down a logically consistent quantitative theory of inference
about parameters.

The theory is operational in the sense that it is veriﬁable from long range conse-
quences. We saw that all the predictions were automatically calibrated, i.e. that the pre-
dicted long range relative frequencies coincided with the actual frequencies of occurrence.
This is a very important feature that allows for a reconciliation between the frequentist

§

§

68

and the Bayesian approaches to statistics, probably the same kind of reconciliation that
Maurice Kendal [39] had in mind: “Neither party can avoid ideas of the other in order to
set up and justify a comprehensive theory.” In this way the distinction between the theory
of probability and that of statistical inference might be removed, leaving a logical unity
and simplicity.

Acknowledgment

Prof. Gabrijel Kernel, dr. Igor Mandi´c, prof. Aleˇs Stanovnik and prof. Peter ˇSemrl read
a preliminary draft of this paper. We wish to thank them for their kindness and for their
help in correcting several inaccuracies. They are not responsible, of course, for any er-
rors that remain as to the opinion expressed concerning the nature of consistent plausible
reasoning.

69

References

University Press (1974).

(2003).

[1] A. S. Hornby, Oxford Advanced Learner’s Dictionary of Current English, Oxford

[2] E. T. Jaynes, Probability Theory - The Logic of Science, Cambridge University Press

[3] G. P´olya, Mathematics and Plausible reasoning, Vol. 1 - Induction and Analogy in

Mathematics, Princeton University Press, Princeton (1954).

[4] G. P´olya, Mathematics and Plausible reasoning, Vol. 2 - Patterns of Plausible Infer-

ence, Princeton University Press, Princeton (1954).

[5] P. S. Laplace, Œvres Compl`etes - Tome Septi`eme: Th´eorie Analitique des Proba-

bilit´es, Gauthier-Villars, Paris (1886).

[6] Aristotle, Organon (4th century BC).

[7] G. Boole, An Investigation of The Laws of Thought, Macmillan, London (1854);

reprinted by Dover Publications, Inc., New York (1954).

[8] K. R. Popper, The Logic of Scientiﬁc Discovery, Hutchinson & Co. Publishers, Lon-

don (1959).

[9] K. R. Popper, Erkenntnis 3 (1933) 426.

[10] R. T. Cox, Am. J. Phys. 14 (1946) 1.

[11] J. H. Poincar´e, Bull. Sci. Math. 28 (1904) 302.

[12] A. Stuart, K. Ord, Kendall’s Advanced Theory of Statistics, Vol. 1 - Distribution The-

ory, Arnold, London (1994).

[13] Rev. T. Bayes, Phil. Trans. Roy. Soc. (1763) 370.

[14] P. S. Laplace, Mem. Acad. Roy. 6 (1774) 621.

[15] H. Jeffreys, Theory of Probability, Clarendon Press, Oxford (1961).

[16] A. O’Hagan, Kendall’s Advanced Theory of Statistics, Vol. 2B - Bayesian Inference,

Arnold, London (1994).

demic Press (1967).

[17] T. S. Ferguson, Mathematical Statistics - A Decision Theoretical Approach, Aca-

[18] H. L. Harney, Bayesian Inference, Springer (2003).

70

[19] W. T. Eadie, D. Drijard, F. E. James, M. Roos, B. Sadoulet, Statistical Methods in Ex-

perimental Physics, North-Holland (1971).

[20] A. Stuart, K. Ord, S. Arnold, Kendall’s Advanced Theory of Statistics, Vol. 2A - Clas-

sical Inference and the Linear Model, Arnold, London (1999).

[21] J. P. Elliot, P. G. Dawber, Symmetry in Physics, Vol. 1 - Principles and Simple Appli-

cations, Macmillan, London (1986).

[22] J. Acz´el, Lectures on Functional Equations and Their Applications, Academic Press

(1966).

[23] B. de Finetti, Theory of Probability, Vols. 1 & 2, John Wiley & Sons (1974).

[24] V. W. Behrens, Landswirtsch. Jb. 68 (1929) 807.

[25] R. A. Fisher, Ann. Eugen. 6 (1935) 391.
R. A. Fisher, Ann. Eugen. 9 (1939) 174.

[26] F. Yates, Proc. Camb. Phil. Soc. 35 (1939) 579.

[27] J. Neyman, Phil. Trans. A 236 (1937) 333.

[28] D. V. Lindley, J. Roy. Stat. Soc. B 20 (1958) 102.

(July, October 1948) pp. 379-423 , 623-656.

[30] C. Villegas, J. Am. Stat. Assoc. 72 (1977) 453.

C. Villegas, Ann. Stat. 9 (1981) 768.

[29] C. E. Shannon, The Mathematical Theory of Communication, Bell. Sys. Tech. J. 27

[31] R. E. Kass, L. W. Wasserman, J. Am. Stat. Assoc. 91 (1996) 1343.

[32] M. Stone, A. P. Dawid, Biometrika 59 (1972) 369.

[33] A. P. Dawid, M. Stone, J. V. Zidek, J. Roy. Stat. Soc. B 35 (1973) 189.

[34] R. A. Fisher, Phil. Trans. Roy. Soc. A 222 (1922) 309.

[35] A. W. F. Edwards, Nature 352 (1991) 386.

[36] I. Newton, PhilosophiæNaturalis Principia Mathematica - Liber Tertius: Mundi

Systemate (1687).

[37] K. R. Popper, Erkenntnis 5 (1935) 170.

[38] J. M. Keynes, A Treatise on Probability, Macmillan, London (1921).

[39] M. G. Kendall, Biometrika 36 (1949) 101.

71

[40] K. S. Van Horn, Int. J. Appr. Reas. 34 (2003) 3.

[41] A. P. Dawid,

“Conformity of

inference patterns” in Recent Developments
J. R. Barra, B. van Cutsen, F. Brodeau and G. Romier (eds.), North-

in Statistics,
Holland, Amsterdam (1977) 245.

72

Appendix

A Cox’s Theorem

In what follows we present, for the sake of completeness, a proof of Cox’s Theorem,
borrowing mainly from the original proof of Richard Cox [10] and from the proofs of
Edwin Jaynes ([2],
2.1-2.2, pp. 24-35) and Kevin Van Horn [40], but ﬁrst we prove the
following Lemma:

§

Lemma 1 Suppose that plausibilities (A
|
signed6 and that they completely and uniquely determine the plausibility (AB
logical product AB to be true. Then (AB
I) and (A
(B
|

I) is to be a function either of (A
|

I) and (A
|

BI) can be as-
I) for the
B) and

AI) or of (B

BI) only.

AI), (B

B), (B

|

|

|

|

|

|

Proof. According to the above assumptions, there are ﬁfteen different combinations
BI), corresponding to ﬁfteen different
of plausibilities (A
|
|
subsets of arguments of a function H from which we might compute (AB

I) and (A
|

AI), (B

B), (B

I):

|

|

(151)

(152)

(153)

(154)

(155)

(156)

(157)

(158)

t = H(x) = H(u) ,
t = H(y) = H(v) ,

t = H(x, u) ,

t = H(y, v) ,

t = H(x, y) = H(u, v) ,

t = H(x, v) = H(u, y) ,

t = H(x, y, v) = H(u, v, y) ,

t = H(x, y, u) = H(u, v, x)

t = H(x, y, u, v) ,

73

and

where we used abbreviations

x

(A
|

≡

I), y

(B

AI), u

(B

I), v

BI) and t

(AB

I) .

≡

|

≡

|

(A
|

≡

≡

|

6Throughout the proof of Cox’s Theorem it is always assumed that all considered degrees of plausibility

can be assigned.

Interchangeability of x and u, as well as of y and v, in the above expressions is implied
by commutativity of the logical product AB, AB = BA. We always assume that H is
continuous, as well as differentiable and non-decreasing in all of its arguments. More-
over, in the case where none of the arguments equals F, H should be strictly increasing in
all of its arguments.

The Lemma is then proved by the method of trying out all possible subsets of ar-
guments and demonstrating that all but two of them inevitably lead to conclusions that
contradict the basic Desiderata.

a) In four out of the ﬁfteen possible cases, H is a function of a single variable. Let, for
example, in the case of t = H(x), A being true, e.g. a tautology. Then AB is
equivalent to B (i.e. t is equivalent to u), so we have

(B

I) = H(1) = constant ,

|

regardless the proposition B and the state of information I. By letting B be a tautol-
ogy and a false proposition, respectively, the above equation implies 1 = F, which
evidently contradicts the requirements of Desideratum I. Similarly, an assumption
A = B in the case of t = H(y) leads to the same kind of contradiction, so we
I) cannot be expressed as a function of a single
conclude that the plausibility (AB
variable.

|

b) In a very similar way we can also rule out the possibility (153). Namely, by choosing

A = B we obtain

t = H(1, 1) = constant .

In the case of (155), identical contradictions are obtained by choosing either A or
B to be tautologies.

c) According to the ﬁrst one of the possibilities (156),

the plausibility

A(BC)

I

is expressible as

t = H(x, y, v) ,

(159)

(160)

|
(cid:1)
A(BC)

I

|

(cid:0)

(cid:1)

|

(cid:0)

|

= H

x, (BC

AI), (A
|

|

BCI)

,

where C, as long as it is consistent with both I and B, is a completely arbitrary
proposition. We can therefore choose C = A, implying

(cid:3)

(cid:2)

A(BC)

I

= t, (BC

AI) = y, (BC

I) = t and (A
|

|

BCI) = 1 ,

so that (160) reduces to

(cid:0)

(cid:1)

t = H(x, y, 1) .

which is incompatible with the original assumption (159) about the value of t being
dependent on three independent variables x, y and v.

74

d) In the case (158) where H is to be a function of four independent variables x, y, u and
is expressible as

v, the plausibility

A(BC)

I

(cid:0)

A(BC)

I

(cid:1)
= H

x, (BC

AI), (BC

|

I), (A
|

|

BCI)

.

By setting C = A, (161) reduces to

(cid:0)

(cid:2)

(cid:3)

|

|

(cid:1)

(161)

(162)

t = H(x, y, t, 1) ,

or, equivalently, to

H(x, y, u, v) = H

x, y, H(x, y, u, v), 1

.

(163)

Note that due to the interchangeability of x and u and of y and v, (162) can be
rewritten as

(cid:2)

(cid:3)

t = H(t, 1, x, y) .

(164)

Differentiation of (163) with respect to x, y, u and v, respectively, yields a system
of four differential equations:

H1(x, y, u, v) = H3
H2(x, y, u, v) = H3
H3(x, y, u, v) = H3
H4(x, y, u, v) = H3

x, y, H(x, y, u, v), 1

x, y, H(x, y, u, v), 1
(cid:2)
x, y, H(x, y, u, v), 1
(cid:2)
x, y, H(x, y, u, v), 1
(cid:2)

(cid:3)

(cid:3)

(cid:3)

H1(x, y, u, v) + H1
H2(x, y, u, v) + H2
H3(x, y, u, v) ,
H4(x, y, u, v) .

(cid:2)

x, y, H(x, y, u, v), 1

x, y, H(x, y, u, v), 1
(cid:2)

(cid:3)

,

,

(cid:3)

(165)

(cid:2)

(cid:3)

Since H(x, y, u, v) is to be increasing in all of its arguments, the derivatives
H3(x, y, u, v) and H4(x, y, u, v) must be positive (i.e. different from zero) in the
case of x, y, u and v all being different from F. Then, according to the latter two of
the four equations, the derivative H3

x, y, H(x, y, u, v), 1

should equal unity,

H3

(cid:2)
x, y, t, 1) = 1 .

(cid:3)

(166)

This, when inserted to the ﬁrst equation of (165), further implies

(cid:0)

H1

x, y, H(x, y, u, v), 1

= H1(x, y, t, 1) = 0 .

(167)

(cid:3)
(cid:2)
Since both (166) and (167) must hold for arbitrary propositions A and B , they must
also hold for B being a tautology, implying

t = (AB

I) = (A
|

|

I) = x and y = (B

AI) = 1 .

|

In this case (166) and (167) read

H1(x, 1, x, 1) = 0 and H3(x, 1, x, 1) = 1 .

(168)

75

The same kind of reasoning as above, however, applied to equation (164), leads to

H1(x, 1, x, 1) = 1 and H3(x, 1, x, 1) = 0 ,

which, when compared to (168), is an evident inconsistency. In this way the pos-
sibility for t to be a function of four independent variables x, y, u and v, is ﬁnally
rejected.

Note that three additional possibilities for H, (157) and (152), can be excluded by
following the same patterns of reasoning as in the case of H(x, y, u, v).

After trying out thirteen different possibilities we thus end up with only two admissi-
ble subsets of arguments for H: x and y, or u and v, the latter being a consequence
of the aforementioned interchangeability of variables. All other subsets, (151), (152),
(153), (155), (156), (157) and (158), have been ruled out as incompatible with the basic
I) is truly to be uniquely and completely determined by
Desiderata. Therefore, if (AB
the plausibilities (A
BI), there must exist a function H (154),
|
such that

I) and (A
|

|
AI), (B

B), (B

|

|

|
which completes the proof of the Lemma.

|

(A
|

(cid:2)

(AB

I) = H

I), (B

AI)

= H

(B

I), (A
|

|

BI)

,

(cid:3)

(cid:2)

(cid:3)

In summary, existence of the function H (154) thus represents the starting point of the
proof of Cox’s Theorem. Recall that Desideratum II. requires H to be strictly increasing
and twice differentiable in both of its arguments, so we have

≥
with equalities if and only if y and x represent impossibilities, respectively.

≥

H1(x, y)

0 and H2(x, y)

0 ,

Suppose now we try to ﬁnd the plausibility (ABC

I) that three propositions, A ,
B and C, would be true simultaneously. Because of the fact that Boolean algebra is asso-
ciative, ABC = (AB)C = A(BC), we can express the plausibility that we are searching
for in two different ways,

|

and

(cid:0)

(AB)C

I

= H

(AB

I), (C

ABI)

= H

H(x, y), z

|

(cid:1)

(cid:2)

|

|

(cid:3)

(cid:2)

(cid:3)

I

= H

A(BC)

(A
|
where another abbreviation, z
ABI), was used. According to Desideratum III.a,
the two ways must lead to the same result, i.e. if our reasoning is to be consistent, function
H must solve the Associativity Equation:

x, H(y, z)] ,

I), (BC

= H

AI)

(C
(cid:2)

≡

(cid:0)

(cid:1)

(cid:2)

(cid:3)

|

|

|

H

H(x, y), z

= H

x, H(y, z)

(169)

(see, for example, reference [22],
ences quoted therein).

§

(cid:2)

6.2, pp. 253-273, and

(cid:3)

(cid:2)

(cid:3)
7.2.2, pp. 327-330, and refer-

§

76

In order to solve it, we ﬁrst differentiate the Associativity Equation with respect to x

and y, obtaining in this way:

H1

H(x, y), z

H1(x, y) = H1

x, H(y, z)

(cid:2)
H(x, y), z

H1

(cid:3)

H2(x, y) = H2

(cid:2)
x, H(y, z)

(cid:3)
H1(y, z) .

Dividing (171) by (170) yields:

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(170)

(171)

(172)

and

where

K(x, y) = K

x, H(y, z)

H1(y, z) ,

(cid:2)

(cid:3)

H2(x, y)
H1(x, y)

.

≡

K(x, y)

Equation (172) can also be rewritten as:

K(x, y) K(y, z) = K

x, H(y, z)

H2(y, z) .

(173)

(cid:2)
The right side of (172) is independent of z, therefore when differentiated with respect to
z, it vanishes for all x and y and z:

(cid:3)

K2

x, H(y, z)

H1(y, z) H2(y, z) + K

x, H(y, z)

H1,2(y, z) = 0 .

(174)

(cid:2)

d
dy

The right side of (173), when differentiated with respect to y, equals (174), and must
therefore vanish for all x, y and z, too. It is then evident that both the left and the right
side of eq. (173) must be independent of y, i.e. this means that

(cid:2)

(cid:3)

(cid:3)

ln

K(x, y) K(y, z)

=

ln K(x, y) +

ln K(y, z) = 0

d
dy

(cid:1)

d
dy

or, equivalently,

(cid:0)

−
This further implies both the right and the left side of the above equation to be independent
of either x or z:

ln K(x, y) =

ln K(y, z) .

d
dy

d
dy

ln K(x, y) =

ln K(y, z)

ln h′(y) ,

(175)

d
dy

−

d
dy

≡

where

≡
is a strictly positive function of y. Permutation of the variables results in an expression
very much like (175):

h′(y)

h(y)

d
dy

ln K(z, x) =

ln K(x, y)

ln h′(x) .

(176)

d
dx

−

77

d
dx

≡

d
dy

d
dx

By subtracting equation (176), multiplied by dx, from equation (175), multiplied by dy,
we obtain:

or, when integrated,

d ln K(x, y) = d ln

h′(y)
h′(x)

,

(cid:27)

(cid:26)

H2(x, y)
H1(x, y)

=

1
a

h′(x)
h′(y)

,

G(x, y)

ah(x) + h(y) ,

where a is an arbitrary integration constant. By introducing

equation (177) can be rewritten as:

H1(x, y)G2(x, y)

H2(x, y)G1(x, y) = 0 .

≡

−

(177)

We saw on two previous occasions (recall Section 5, eqns. (50-53), and the repeated situ-
ation in Section 12), that the general solution of such a functional equation reads:

H(x, y) = k

G(x, y)

= k

ah(x) + h(y)

,

(178)

(cid:1)
where k is an arbitrary function of a single variable G(x, y). When inserted into the
Associativity Equation, the solution (178) yields:

(cid:0)

(cid:1)

(cid:0)

k

ah

k

ah(x) + h(y)

+ h(z)

= k

ah(x) + h

k

ah(y) + h(z)

.

(179)

(cid:26)

h

(cid:0)

(cid:1)i

(cid:27)

(cid:26)

h

(cid:0)

If the above equality is to be true for every z, then the function h
take the form:

(cid:1)i(cid:27)
ah(y) + h(z)

k

h

(cid:0)

must

(cid:1)i

h

k

ah(y) + h(z)

= l(y) + h(z) ,

or, equivalently, then the function k

ah(y) + h(z)

must take the form:

h

(cid:0)

(cid:1)i

k

ah(y) + h(z)

(cid:0)

= h−

1

(cid:1)
l(y) + h(z)

,

(180)

(cid:0)
1 is an inverse function of h, while l(y) is a function of y whose form we are
where h−
about to determine. In order to do that, we ﬁrst differentiate equation (180) with respect
to z and obtain:

(cid:0)

(cid:1)

(cid:1)

k′

ah(y) + h(z)

h′(z) = (h−

l(y) + h(z)

h′(z) ,

1)′

(cid:0)
thus implying equality between k′ and (h−

(cid:1)

1)′:

k′

ah(y) + h(z)

= (h−

1)′

l(y) + h(z)

.

(cid:0)

(cid:0)

(cid:1)

(cid:1)

Taking this equality into account, we also differentiate (180) with respect to y and obtain
a differential equation:

(cid:0)

(cid:1)

ah′(y) = l′(y) ,

78

1

(cid:0)

1

(cid:0)

(A
|

(181)

(182)

(183)

(184)

whose integral reads:

l(y) = ah(y) + b ,

where b is an integration constant. This then implies the form of H(x, y) to be:

H(x, y) = h−

ah(x) + h(y) + b

.

To determine the value of the constant a, let us insert the solution (182) into (179),

(cid:1)

obtaining in this way an equation:

a

ah(x) + h(y) + b

+ h(z) = ah(x) + ah(y) + h(z) + b ,

or

(cid:0)

(cid:1)

ah(x) + b

(a

1) = 0 ,

−

with two possible solutions: a = b = 0 or a = 1. Since the former violates the re-
(cid:1)
quirement for monotonicity of H(x, y), H1(x, y) > 0, the only acceptable solution of the
Associativity Equation, reads:

(cid:0)

H(x, y) = h−

h(x) + h(y) + b

,

or, written in terms of plausibilities:

h

(AB

I)

= h

I)

+ h

(B

AI)

+ b .

|

(cid:3)
By exponentiation, the solution takes the form

(cid:2)

(cid:3)

(cid:2)

(cid:2)

(cid:1)

(cid:3)

|

with

w(AB

I) = w(A
|

|

I) w(B

AI) eb ,

|

w(A
|

I) = w

I)

(A
|

≡

exp

h

I)

(A
|

being a function of plausibilities that is by construction both positive and strictly increas-
(cid:3)
ing with respect to its argument.

(cid:3)(cid:9)

(cid:8)

(cid:2)

(cid:2)

Suppose now that given information I, a proposition A is certain, i.e.

true beyond
any reasonable doubt, and that B is another proposition. Then, the state of knowledge
about the propositions A and B being simultaneously true, AB, is the same as the state
of knowledge about only B being true, which can be expressed by a simple equation of
Boolean algebra as:

Therefore, by Desideratum III.c, we must assign equal plausibilities for AB and B,

and we also will have

AB = B .

(AB

I) = (B

I) ,

|

|

(A
|

BI) = (A
|

I)

79

because if A is already certain given I, then given any other information B not contra-
dicting I, it remains certain. In this case the product rule reads:

|
so our function w(x) must have a property that for certain events A

|

w(B

I) = w(A
|

I) w(B

I) eb ,

I) = e−

b .

w(A
|

With no loss of generality we may choose the value of the constant b to be zero, i.e. the
I) for a certain event A to be one. In other words, any continuous
value of function w(A
|
b can be renormalized by being
positive strictly increasing function w with upper bound e−
multiplied with eb so that its upper bound equals unity. Note that the compositum of
I) meets all the
plausibility assignment (A
|
requirements for plausibilities, i.e. renormalized functions w(A
I) are also plausibilities
|
by themselves. Then, our general product rule takes the form:

I) and the renormalized functions w, w(A
|

w(AB

I) = w(A
|
where in the case that, apart from (A
BI)
|
can be assigned, the last equality is due to Desideratum III.a. Evidently, the product rule
can also be rewritten as

AI), also plausibilities (B

I) and (A
|

I) w(A
|

AI) = w(B

I) and (B

I) w(B

BI) ,

(185)

|

|

|

|

|

wa(AB

I) = wa(A
|

|

I) wa(B

AI) = wa(B

|

I) wa(A
|

|

BI) ,

(186)

where a is a non-zero but otherwise arbitrary constant.

Now suppose that A is impossible, given I. Then, the proposition AB is also impos-

sible given I:

and if A is already impossible given I, then, given any further information B which does
not contradict I, A would still be impossible:

w(AB

I) = w(A
|

|

I) ,

w(A
|
In this case, the product rule reduces to

BI) = w(A
|

I) .

w(A
|

I) = w(A
|

I) w(B

I) ,

|

I) that could satisfy this condition: it could be either

which must hold regardless of plausibility for B , given I. There are three possible values
or zero. The choice
of w(A
|
is ruled out due to the requirement for plausibilities to take non-negative values,
contradicts the requirement for plausibilities to be monotonically increasing,

−∞
while
both thus implying the plausibility for impossible events uniquely to be zero.

−∞

∞

∞

,

Now, in order to derive the sum rule, we suppose that the plausibility for proposition
I), must depend in some way on the plausibility

A to be false, given information I, w( ¯A
|
w(A
|

I) that it is true, i.e. there must exist some functional relation

w( ¯A
|

I) = S

w(A
|

I)

.

(cid:2)
80

(cid:3)

Qualitative correspondence with common sense requires that S
I)
twice differentiable, strictly decreasing function with the extreme values
(cid:1)

w(A
|

(cid:0)

be a continuous,

S(0) = 1 and S(1) = 0 .

(187)

But it cannot be just any function with these properties, for it must be fully consistent
with the product rule. We make use of the latter and express the plausibility for A and
B simultaneously to be true as:

w(AB

I) = w(A
|

|

I) w(B

AI) = w(A
|

|

I) S

w( ¯B

AI)

|

= w(A
|

I) S

We also invoke consistency of the product rule, so that:

(cid:2)

(cid:3)

w(A ¯B
AI)
|
I)
w(A
|
Since this must hold for every A and B, given I, it must also hold when

w( ¯AB
w(B

AI)
I)

w(A
|

= w(B

I) S

I) S

|
|

(cid:20)

(cid:21)

(cid:21)

(cid:20)

|

.

w(A ¯B
AI)
|
I)
w(A
|

(cid:20)

(cid:21)

.

where C is any new proposition. But then, according to simple results of Boolean algebra:

i.e. when

B = ¯A + ¯C ,

¯B = AC ,

A ¯B = ¯B and

¯AB = ¯A ,

and by using the abbreviations

(188) becomes a functional equation

x

w(A
|

≡

I) and y

w(B

I) ,

≡

|

By deﬁning new variables,

the functional equation is further reduced to

x S

S(y)
x

(cid:20)

(cid:21)

= y S

S(x)
y

.

(cid:21)

(cid:20)

S(y)
x

u

≡

and v

S(x)
y

,

≡

x S(u) = y S(v) .

S(u)

u S′(u) = S′(u) S′(x) ,

−

81

(188)

(189)

(190)

(191)

On the way to solution, we differentiate (190) with respect to x, y, and x and y, respec-
tively, obtaining in this way:

v
y

−

.

(cid:17)

(192)

(193)

(194)

(195)

and

S′(u) S′(y) = S(v)

v S′(v) ,

−

S′′(u) S′(y)

= S′′(v) S′(x)

.

u
x

In order to eliminate x and y, we multiply equations (190) and (193),

S′′(u) S(u) S′(y) u = S′′(v) S(v) S′(x) v ,

and express S′(x) and S′(y) from (191) and (192), arriving in this way to:

S′′(u) S(u) u

S′′(v) S(v) v

=

S′(u)

S(u)

u S′(u)

S′(v)

S(v)

v S′(v)

.

−

Then, evidently both sides of the above equation must equal a constant, say a
above equation splits into two identical differential equations of the form:

−

(cid:2)

(cid:3)

(cid:3)

(cid:2)

1, and the

dS′
S′

= (a

1)

−

du
u

+

dS
S

(cid:16)

The solution that is obtained by two successive integrations and that satisﬁes the boundary
conditions (187), reads:

−
In this way we obtained the so-called sum rule:

(cid:0)

(cid:1)

S(u) =

1

ua

1

a .

wa( ¯A
|

I) + wa(A
|

I) = 1 .

Since our derivation of the functional equation (190) used the special choice (189) for
B , (194) is a necessary condition to satisfy the general consistency requirement (188).
To check for its sufﬁciency, we substitute (194) in (188) and obtain an evident equality
(c.f. eq. (185)):

w(A
|

|

I) w(B

AI) = w(B

I) w(A
|

|

BI) .

Therefore, equation (194) is the necessary and sufﬁcient condition on S(x) for consis-
tency in the sense (188).

Out of all possible plausibility functions w(A
|
wa(A
|

P (A
|

I)

≡

I) ,

I) we then choose the probability P (A
|

I),

for which the product and the sum rule evidently take the forms:

P (AB

I) = P (A
|

|

|

I) P (B

AI) = P (B

I) P (A
|

|

BI)

and

This completes the proof of Cox’s Theorem.

P (A
|

I) + P ( ¯A
|

I) = 1 .

82

B More on the marginalization paradox

In 1972, Stone and Dawid published an article [32] containing two examples of the so-
called marginalization paradox. The examples refer to inferences that could be made in
two different ways leading to two different results, despite the two possible ways being
completely equivalent. Since at least one of the two chosen ways involves the procedure
of marginalization (23), such inconsistencies are usually referred to as the marginaliza-
tion paradox. Stone and Dawid judged the usage of improper (i.e. non-integrable) non-
informative priors as the cause of the paradox.

If correct, these arguments would seriously threaten the consistency of probability
theory, for we saw that our consistency factors for location, scale and dispersion param-
eters, (110) and (114), despite being conceptually different from non-informative priors,
are all non-integrable over their particular (inﬁnite) domains. But since the form of the
consistency factors is uniquely determined by the Cox-P´olya-Jaynes Desiderata (see Sec-
tion 11), the arguments of Stone and Dawid, if correct, would imply that it is impossible
to construct a consistent and calibrated theory of qualitative inference about parameters
of sampling distributions.

If the appropriate consistency factors (110) and (114) are used, there is no paradox in
the ﬁrst of the two aforementioned examples. The origin of the paradox in Example #2,
however, is better camouﬂaged and is discussed in the following.

Let the data x = (x1, x2, ..., xn) consist of n

2 observations from the normal
sampling distribution N(µ, σ). Suppose that before the data x were collected we had been
completely ignorant about the values of the parameters and that we are not interested in
either of the two parameters separately but only in their ratio θ:

≥

Let the inference be made by two persons, say Mr. A and Mr. B, each choosing his
own way to obtain the probability distribution for θ. Mr. A strictly obeys the rules of
probability as developed in the present paper and makes use of Consistency Theorem
(64), of the consistency factor (114), and of the sequential use of Bayes’ Theorem (29),
in order to obtain ﬁrst a two-dimensional pdf for µ and σ:

f (µσ

xI)

|

π(µ, σ) f (x
|

∝

µσI) =

µσI)

f (xi|

1
σ

n

i=1
Y

−

µ2 +

1
σn+1 exp
1
σn+1 exp
1
σn+1 exp

∝

∝

∝

(cid:0)

n
2σ2
n
2σ2
nθ2
2 −

(cid:16)

−

(cid:26)

−

(cid:26)

−

(cid:26)

(¯x

µ)2 + s2

R2
n −

(cid:27)
(cid:1)
2rRµ
n

(cid:17)(cid:27)

R2
2σ2 +

rRθ
σ

,

(cid:27)

(196)

µ
σ

.

θ

≡

83

where:

and

1
n

¯x

≡

xi , s2

1
n

≡

n

i=1
X

¯x)2 ,

(xi −

R2

n(¯x2 + s2) , r

n

i=1
X

≡

n¯x
R

.

≡

−

The admissible range of variables ¯x is the entire real axis, for s2 and R = √R2 it is only
its positive half, while the range of r is the interval (

√n, √n).

By multiplying the above pdf by the appropriate Jacobian,

Mr. A obtains the pdf for θ and σ,

=

J

|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
σn exp

∂(µ, σ)
∂(θ, σ)

= σ ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
nθ2
2 −

−

(cid:26)

R2
2σ2 +

rRθ
σ

.

(cid:27)

f (θσ

xI)

|

∝

Since he is interested only in θ but not in σ, he integrates out the latter in order to obtain
the marginal pdf for θ,

f (θ

xI) =

∞

f (θσ

xI) dσ

|

∝

e−

nθ2/2 Hn

2(r, θ) ,

−

|

0
Z

where function Hn(r, θ) is introduced as:

Evidently, Mr. A’s pdf for θ is a function of statistics r alone and, when also appropriately
normalized, it reads:

Hn(r, θ)

∞

un exp

+ rθu

du .

u2
2

−

n

o

≡

0
Z

f (θ

xI) =

|

nθ2/2

e−

η(r)

Hn

2(r, θ) ,

−

(197)

(198)

where η(r) is the usual normalization factor.

Seeing Mr. A’s result, Mr. B decides to simplify the calculation. His intuitive judgment
is that he ought to be able equivalently but more easily derive the (marginal) pdf for θ by
a direct application of Consistency Theorem in a reduced model, i.e.
that θ should be
inferred directly from the sampling distribution for r only, following in this way the so-
called reduction principle [33]. He starts with the appropriate pdf for ¯x and s2,

f (¯xs2

θσI ′)

σ−

n sn

−

3 exp

|

∝

n
2σ2

−

(cid:26)

(cid:2)

(¯x

µ)2 + s2

,

−

(cid:27)

(cid:3)

transforms it into the pdf for r and R,

f (rR

θσI ′) = f (¯xs2

θσI ′)

|

1

Rn
−
σn

∝

n

−

|

(cid:0)

1

−

∂(r, R)
∂(¯x, s2)
n−3
2

(cid:12)
(cid:12)
(cid:12)
exp
(cid:12)

(cid:12)
(cid:12)
(cid:12)
r2
(cid:12)

(cid:1)

84

nθ2
2 −

R2
2σ2 +

rRθ
σ

,

(cid:27)

−

(cid:26)

(199)

and then reduces it by means of marginalization to a pdf for r only:

f (r

θσI ′) =

f (rR

θσI ′) dR

|

∞

0
Z

|

r2

n

−

∝

(cid:0)

(cid:1)

n−3
2 e−

nθ2/2 Hn

1(r, θ) .

−

(200)

Note that the pdf for r is independent of parameter σ, i.e. f (r
according to Consistency Theorem, Mr. B’s pdf for θ reads:

|

θσI ′) = f (r

θI ′). Then,

|

f (θ

rI ′) =

f (r

θI ′) =

|

|

e−

nθ2/2 Hn

1(r, θ) ,

−

(201)

π(θ) and

with
also containing the factor (n
e

e

η(r) being the appropriate consistency and normalization factors, the latter

n−3
2

from equation (200).

Now, if the two marginal pdf’s for θ of Mr. A and Mr. B, (198) and (201), are to be

equal, there must exist a consistency factor

π(θ) such that

π(θ)
η(r)
e
e
−

r2)

π(θ)
η(r)
e
e

h(r)

π(θ) Hn

1(r, θ) = Hn

2(r, θ)

−

e

(202)

for all real r, θ

(

,

) and for every integer n

2, where

∈

−∞

∞

e

−

≥

h(r)

η(r)
η(r)

.

≡

But it is easy to demonstrate by reductio ad absurdum that a general solution
π of equation
(202) does not exist. For a moment we therefore suppose that such a solution indeed
exists. By differentiating logarithm of (202) with respect to r (with respect to θ) we
obtain

e

e

and

h′(r)
h(r)

= θ

Hn
Hn

−

−

(cid:18)

1(r, θ)
2(r, θ) −

Hn(r, θ)
1(r, θ)
Hn

−

= r

Hn
Hn

−

1(r, θ)
2(r, θ) −

Hn(r, θ)
1(r, θ)
Hn

(cid:19)

,

π′(θ)
π(θ)
e
e

(cid:19)
where we made use of the following properties of functions Hn:

(cid:18)

−

−

∂
∂r

Hn(r, θ) = θ Hn+1(r, θ) and

Hn(r, θ) = r Hn+1(r, θ) .

∂
∂θ

When divided, equations (203) and (204) yield a functional equation:

(203)

(204)

π′(θ)
π(θ)
e
π(θ) of the form
whose general solution are functions h(r) and
e

h′(r)
h(r)

r =

θ ,

h(r)

rc and

∝

π(θ)
e

∝

θc ,

e

85

with c being a real constant that could be either positive, negative or exactly zero. When
inserted in equation (202), functions h(r) and

π(θ) imply:

Hn
Hn
Hn

−

−

−

2(r, θ) = C rc θc Hn
e
−
1(r, θ) = C r|
| Hn
−
1(r, θ)
2(r, θ) = C Hn

| θ|

c

c

1(r, θ)

; c > 0 ,
2(r, θ) ; c < 0 ,
; c = 0 ,

−

(205)
(206)
(207)

where C is an arbitrary constant.

If this is to be true for any value of θ within its permissible range, it must also be
2(r, θ = 0) = 0, which, when
true for θ = 0. But then, for positive c, this implies Hn
that follows directly from the deﬁnition of
compared to Hn
Hn (197), is a clear contradiction. For negative c we derive a contradiction in an identical
(cid:1)
= 0, the result of differentiation of equation (207) that corresponds to the
way. For r
value of c being exactly zero, reads:

2(r, θ = 0) = 2

n−3
2 Γ

−
2

(cid:0)

−

−

n

1

Hn

1(r

= 0, θ) = C Hn(r

= 0, θ) .

−

(208)

Dividing (207) with (208) and again setting θ = 0 yields:

n

1

−
2

=

Γ2

,

(cid:16)
n

(cid:17)
1

Γ2

n
2

−
2

(cid:16)
which is, since it is to be valid for any integer n
proof is completed.

≥

(cid:17)

2, an evident contradiction. Hence, the

Stone and Dawid in [32], as well as Dawid, Stone and Zidek in [33], considered it
obvious that Mr. A and Mr. B made their inferences about θ from the same information
(i.e. from the measured value of r) and should therefore indeed come to the same conclu-
sions: the pdf’s for θ, (198) and (201) ought to be identical. The fact that Mr. B cannot
reproduce Mr. A’s result whatever consistency factor
π(θ) he chooses, served them as a
proof that either Mr. A of Mr. B must be guilty of some transgression. According to their
reasoning, the blame is to be put on Mr. A for using a non-integrable consistency factor
π(µ, σ) (114).

e

From the standpoint that we advocate in the present paper, it is evident that such rea-
soning is unjustiﬁed. First, as we have stressed in many places and especially in Sections 5
and 7, in the limit of complete prior ignorance about the value of an inferred parameter,
information about the parameter of a sampling distribution after recording events from
that particular distribution consists of two pieces: of the value of the likelihood and of
the form of the sampling distribution (i.e. of the speciﬁed model). But the model I (196)
of Mr. A is different from the model I ′ (200) of Mr. B. In particular, Mr. B obtained the
reduced model (200) by marginalization of the sampling distribution (199). Since every
marginalization represents an irreversible process (a reverse transformation from the re-
duced model to the original one does not exist), Mr. B is deliberately throwing away avail-
able information, acting in this way against the consistency Desideratum III.b. Therefore,

86

6
6
6
Mr. A and Mr. B do not infer parameter θ from the same information so it is natural that
they come to different conclusions (see also ref. [2],
15.8, pp. 472-473). The fact that
the inferences of Mr. A and Mr. B will never coincide is thus an unequivocal proof that
some of the cogent information was lost during Mr. B’s reduction of his model. More-
over, since there is one and only one logically consistent procedure (i.e. a procedure that
is in a complete accordance with the basic Desiderata) to infer θ and this is the proce-
dure applied by Mr. A, and since the reduction of the sampling distribution put Mr. B in
a position where he cannot ﬁnd the appropriate consistency factor that would reproduce
Mr. A’s result, Mr. B is no longer able to make any kind of consistent inference about the
parameter θ.

§

Second, Mr. A is perfectly capable of making predictions about the value of the in-
ferred parameter that are veriﬁable at long-range consequences. We performed numerous
Monte Carlo experiments and inferred the value θ of the ratio of parameters µ and σ of
the generator of normally distributed random numbers. As expected, conﬁdence intervals
based on Mr. A’s marginal distribution (198) for θ covered the true value (i.e. the ratio
of the parameters that were actually used by the generator) exactly in the percentage δ
of cases that was predicted according to (125). The coverage was exact regardless of the
number n of observations that we based our inference upon each time, the chosen value
δ, and the type of chosen conﬁdence interval (it could have equally well been the shortest
of all possible intervals, the central interval, the lower-most or the upper-most interval, or
any other interval with the chosen probability (125) that equals δ). That is, the predictions
of Mr. A are calibrated. On the other hand, the predictions of Mr. B who is not able to
reproduce the predictions of Mr. A, will thus necessarily be non-calibrated.

Third, the inference of Mr. A was based only on steps that are all (including the use
xI))
of the consistency factor (114) and the procedure of marginalization of the pdf f (θσ
deduced directly (i.e. without any other assumptions) from the basic Desiderata. In par-
ticular, absolutely no assumption was ever made about the existence of an integral of the
consistency factor over its domain: it need not exist, neither is it forbidden to exist. That
is, for Mr. A the existence of the integral is completely irrelevant and we therefore see
no reason whatsoever why the eventual non-integrability of the consistency factor (114)
could be any kind of transgression.

|

After all is said and done, there are only two possibilities left at Mr. B’s disposal.
He can either correct his intuitive reasoning and abandon the principle of reduction, or
he can develop a completely new theory of inference on his own by incorporating the
reduction principle in his set of basic rules. But, as exhibited above, such a theory would
necessarily be logically inconsistent (e.g. it would allow, among other things, some of the
available information that is relevant to a particular inference to be ignored), as well as
non-operational, and thus of no practical importance.

The basic Desiderata and their direct applications such as the Cox, Bayes and Consis-
tency Theorems, are adequate for conducting inference so they must always take prece-
dence over intuitive ad hoc devices like the above principle of reduction. We agree with
Edwin Jaynes ([2],
15.7, p. 469) that in order to avoid inconsistencies, the rules of in-
ference must be obeyed strictly, in every detail. Intuitive shortcuts that violate those rules

§

87

might, by a coincidence, lead to correct results in some very special cases, but will in
general lead to inconsistent and non-calibrated inferences.

88


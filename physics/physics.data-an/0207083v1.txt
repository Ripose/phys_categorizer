A Measure of the Goodness of Fit in Unbinned Likelihood Fits

Rajendran Raja∗

Fermi National Accelerator laboratory

Batavia, IL 60510

(Dated: February 21, 2014)

Abstract

Maximum likelihood ﬁts to data can be done using binned data (histograms) and unbinned data.

With binned data, one gets not only the ﬁtted parameters but also a measure of the goodness of ﬁt.

With unbinned data, currently, the ﬁtted parameters are obtained but no measure of goodness of

ﬁt is available. This remains, to date, an unsolved problem in statistics. Using Bayes theorem and

likelihood ratios, we provide a method by which both the ﬁtted quantities and a measure of the

goodness of ﬁt are obtained for unbinned likelihood ﬁts, as well as errors in the ﬁtted quantities.

We provide an ansatz for determining Bayesian a priori probabilities.

PACS numbers:

2
0
0
2
 
l
u
J
 
2
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
8
0
7
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗Electronic address: raja@fnal.gov

1

I.

INTRODUCTION

We outline a method by which goodness of ﬁt measures can be calculated in an unbinned

likelihood analysis. We are able to also calculate the probability density function of the ﬁtted

variables and hence their errors in a rigorous manner. We brieﬂy describe the currently used

method of “maximum likelihood”, originally due to R.A.Fisher [1]. Let s denote a set

parameters deﬁning our theoretical model used to describe data. Example of s are the mass

of the top quark or the lifetime of a particle. The symbol s (for signal) can in general denote

a discrete or continuous set of variables. Let c denote a set of observations describing a

high energy physics event and there are n events in our dataset. In general, for each event,

c can be a vector of dimension d. Let P (c

s)dc describe the probability of observing the

conﬁguration c in the d-dimensional phase space volume dc given the theoretical parameter

set s. Thus P (c

s) is a probability density function (pdf ) in the variable c and obeys

|

(1)

(2)

(3)

Then one can deﬁne a likelihood

of observing the dataset as

P (c

s)dc = 1

Z

L

=

L

P (ci|

s)

i=n

Yi=1

The maximum likelihood point can be found of observing by minimizing the negative log-

likelihood

logeL
−

deﬁned as

Xi=1
while varying the parameters s either analytically or numerically to obtain the best values

i=n

logeL
−

=

−

loge(P (ci|

s))

s∗ of s that ﬁt the data.

At the maximum likelihood point, s∗, the best ﬁt values of s, are obtained. There is

however no measure of the goodness of ﬁt, since the likelihood at the optimal value is not

normalized to anything. There is strictly no measure for the error on the ﬁtted parameters,

since

L
by treating the

is not a probability density function of s, though people have calculated errors
2χ2. Such error

at the minimum as though it were equivalent to 1

logeL
−

calculations are not hitherto considered rigorously justiﬁable.

Unbinned likelihood ﬁts, despite these disadvantages, are extremely useful in ﬁnding s∗

since one does not have to treat bins with small populations in a special manner as would

be the case for binned ﬁts.

|

|

2

In this paper we use Bayes theorem to rectify the above disadvantages. In the process,

we obtain a measure for the goodness of ﬁt and also P (s

c), the posterior pdf of s, enabling

|

us to calculate the errors of the ﬁtted values in a rigorous way.

II. BAYES THEOREM

We derive Bayes theorem here for the sake of completeness and to illustrate the main

ideas.

In the Bayesian approach [2], the theoretical parameters s can have a probability

distribution both a priori and a posteriori. The a priori distribution refers to the knowledge

of s before the given set of observations are made. The a posteriori probability distribution

refers to the distribution of s, given the set of observations c.

A. Joint and Conditional Probabilities

We deﬁne a joint probability density for the theory parameters s and the observables c

as

dPjoint = Pjoint(s, c)dsdc

which is the probability that s occurs in interval s and s + ds and c occurs in a volume

element dc centered around c.

We deﬁne the conditional probability density

dPconditional = P (s

c)ds

a volume element dc centered around c .

Similarly, the conditional probability density

dPconditional = P (c

s)dc

|

|

as the probability density of observing s in the interval s and s + ds given that c occurs in

(4)

(5)

(6)

is deﬁned as the probability density of observing c in a volume element dc centered around

c, given that s occurs between s and s + ds . Then, by the laws of probability , we can write

the joint probability

dPjoint = Pjoint(s, c)dsdc = P (c

s)dc

P (s)ds

(7)

|

×

3

Where P (s) is the a priori probability of observing s in interval s and s + ds , and P (c

s)dc

is the probability of observing c given s . One can also obtain the same joint probability, by

ﬁrst observing c with a priori probability P (c) and then using the conditional probability

P (s

c) , i.e.

|

dPjoint = Pjoint(s, c)dsdc = P (s

c)ds

P (c)dc

|

×

By equating 7 and 8, one gets the fundamental relation leading up to Bayes Theorem.

dPjoint = P (c

s)dc

P (s)ds = P (s

c)ds

P (c)dc

|

×

|

×

Expressed in terms of densities, dropping ds and dc terms, this yields

One is interested in evaluating P (s

c), the probability of the theory parameters, given a set

of observations c. This becomes,

P (c

s)

P (s) = P (s

c)

P (c)

|

×

|

×

|

P (s

c) =

|

P (s)

P (c

|

s)
×
P (c)

The a priori probability P (c) is not an independent quantity, given the a priori probability

P (s) which represents the knowledge of s before the set of observations c . The reason for

this is that P (s

c) integrated over s must add up to unity.

|

B. Some Normalization Formulae

Integrating over one of the variables in the joint probability yields, using equation 7, the

following relations.

P (c)

Pjoint(s, c)ds =

P (c

s)

P (s)ds

≡ Z

Z

|

×

where the

sign is the deﬁnition of the a priori probability P (c), since one integrates the

≡

joint probability Pjoint(s, c) over all values of s . This then yields

also, integrating the joint probability over c, one gets

P (c) =

P (c

s)

P (s)ds

Z

|

×

P (s)

Pjoint(s, c)dc =

P (c

s)

P (s)dc

≡ Z

Z

|

×

4

|

(8)

(9)

(10)

(11)

(12)

(13)

(14)

i.e.

Similarly, using equation 8, one gets relations similar to the above with c and s interchanged.

Summarizing, one gets the following normalization relations.

P (s) =

P (c

s)

P (s)dc

Z

|

×

or

P (c

s)dc = 1

Z

|

P (c) =

P (c

s)

P (s)ds

Z

Z

|

|

×

×

P (s) =

P (s

c)

P (c)dc

P (s)ds = 1

P (c)dc = 1

Z

Z

P (c

s)dc = 1

P (s

c)ds = 1

|

|

Z

Z

R

P (s

c) =

|

P (c
P (c

s)
|
s)
|

×
×

P (s)
P (s)ds

Substituting 17 in 11, one gets the derivation of Bayes Theorem.

The above equation normalizes to unity as per equation 22. This is the central expression

of Bayes’ theorem.

C. Observation of Many Conﬁgurations

Now we come to one of the more beautiful properties of formula 23, namely it is recursive.

Let us observe two separate conﬁgurations say, c1 and c2 . Then equation 23 yields for c1 ,
s)
P (c1|
s)
P (c1|
|
We can then replace the a priori probability for s, P (s) in equation 23 by the probability

Now we observe c2. We wish to compute P (s

c1, c2) , the probability of s given c1 and c2 .

P (s)
P (s)ds

c1) =

×
×

(24)

P (s

R

|

of s after observing c1 (i.e. P (s

c1)) to calculate the probability of s given c1 and c2 . This

yields,

|

|

P (s

c1, c2) =

s)
P (c2|
s)
P (c2|

×
×

P (s
P (s

c1)
|
c1)ds
|

R

5

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(25)

Substituting for P (s

c1) from equation 24, we get

|

P (s

c1, c2) =

|

P (c2|
P (c2|
R
yielding P (s

s)P (c1|
s)P (c1|
c1, c2) =

s)P (s)ds

P (c1|

s)P (s)/
R
P (c1|
s)P (s)ds/
R
s)P (c1|
P (c2|
s)P (c1|
P (c2|
R
s)P (c1|
s)...P (c2|
s)P (c1|
s)...P (c2|

s)P (s)ds
s)P (s)
s)P (s)ds
s)P (s)
s)P (s)ds

P (cn|
P (cn|

generalizing, P (s

c1, c2...cn) =

|

R
Another way to think about equation 28 is to think of the n conﬁgurations as one massive
super conﬁguration cn , which also obeys the Bayes theorem equation 23

P (s

cn) =

|

R
s) = P (cn|

|

P (cn
s)
P (s)
|
×
P (cn
s)
P (s)ds
×
|
s)
s)P (c1|
s)...P (c2|

where P (cn

It should be noted that the probability P (cn

s) obeys the normalization condition 21. Equa-

tion 30 is just the law of multiplication of independent probabilities. This implies that it

is possible to chain probabilities in Bayes theorem as in equation 25 if and only if the con-

ﬁgurations are statistically independent. This is certainly true in the case of high energy

physics events.

The expression for a posteriori probability P (s

cn) in equation 29 cannot be used as is

|

unless one knows P(s), the a priori probability of s. In the “Bayesian approach”, people use

various guesses for P(s) and a lot of care and energy are expended in arriving at “reasonable”

(26)

(27)

(28)

(29)

(30)

functions for P(s).

D. Likelihood Ratios

We now recast the Bayes theorem equation 29 as a set of likelihood ratios

LR.

P (s

cn)

|
P (s)

=

P (cn
s)
|
P (cn)

LR =

(31)

using equation 13. The likelihood ratio

where we have substituted the function P (cn) for the normalizing integral in the denominator
LR has a very important invariant property. It is
s′ where c′ and s′ are
→

invariant under the transformations of variable sets c

c′ and s

→

functions of the variable sets c and s. It is possible to ask what exact variables one uses

to form the vector c. For instance,when a jet is measured experimentally, does one use the

|

|

6

energy, pseudo-rapidity and azimuth of the jet or the three components of the energy three

vector as components of c? Clearly, the probability density function P (c

s) will depend on

|

the choice of the variable set c since,

P (c′

s) =

|

dc
dc′ |

|

P (c

s)

|

(32)

dc
dc′

|

|

where,

denotes a Jacobian of transformation to go from the set of variables c to c′.
LR, hence the likelihood ratio
is unaﬀected by the transformation. The same argument can be made with respect to

However, the same Jacobian ocurs in the denominator of the

transformations of the variable set s

henceforth work with the likelihood ratio

→

s′. These are extremely important properties, so we
LR and not the likelihoods

which do not possess

L

these properties.

III. THE PRINCIPLE OF MAXIMUM LIKELIHOOD RATIOS

The equation 31 for

LR can be expanded as follows.
s)
P (c2|
s)
P (c1|
P (c2) · · · ×
P (c1) ×

P (cn
s)
|
P (cn)

=

LR =

s)
P (cn|
P (cn)

(33)

(34)

(35)

where we have used the independence of a priori probabilities for P (ci), i = 1, n. Similarly,

one gets expressions,

P (s

cn)

|
P (s)

=

P (s

c1)
P (s) ×

|

P (s

c2)

|
P (s)

LR =

P (s

cn)

|
P (s)

· · · ×

where we have derived equation 34 from equation 33 by applying equation 31 to the likelihood

ratios of the individual events in the product. In order to ﬁnd the optimal set of parameters
LR in equation 33 with respect to s. This is equivalent

s, we maximize the likelihood ratio
to minimizing the negative log likelihood ratio logeLR.

∂logeLR
∂s

−

=

i=n

−

Xi=1

∂logeP (ci|
∂s

s)

= 0

Notice that this is the same set of equations that one gets when maximizing the likelihood

as in equation 3, since the a priori probabilities P (ci) are constant with respect to variations

in s. So one gets the same set of optimal variables s∗ whether one maximizes the likelihood
LR. However, at the optimum, the likelihood ratio can be used
L
to obtain a goodness of ﬁt parameter as we show below, whereas the likelihood method

or the likelihood ratio

7

of

would be unable to provide this information. One can now ask what the minimum value
LR is with respect to variations in the event conﬁguration, for a ﬁxed value of theory;
i.e. what event conﬁgurations produce the minimum value of the negative log likelihood?

Diﬀerentiating equation 33 with respect to ci, one gets,
∂logeLR
∂ci

∂logeP (ci|
∂ci

s)

−

−

=

+

∂logeP (ci)
∂ci

= 0

i.e

∂logeP (ci|
∂ci

s)

=

∂logeP (ci)
∂ci
s) = P (ci)

P (ci|
The equation 38 implies that the lowest value of the likelihood ratio occurs when the ex-

(38)

perimental probability density P (c) and the theory probability density P (c

s) are the same

|

at the observed events. The negative log likelihood is zero at this point, yielding the best

possible ﬁt.

(36)

(37)

IV. EVALUATING THE FUNCTION P (c) AND THE GOODNESS OF FIT

The key point to note is that just as P (s) is the a priori probability of the theoretical

ratio

parameter s, P (c) is the a priori probability of the data. In order to evaluate the likelihood
LR at the maximum likelihood point, one needs to evaluate the function P (c) at
the observed event conﬁgurations c1, c2 . . . cn. So the problem to solve is this: given the

event conﬁgurations c1, c2 . . . cn, what is their probability density? Well known methods

exist to estimate the pdf ′s given discrete event distributions. These are collectively titled

probability density estimators (P DE), which have recently found application in high energy

physics analyses [3].

As noted above, the probability density function P (c) is the a priori pdf of the data. In

previous applications, to the author’s best knowledge, the function P (c) was subsumed into

the equation 13 and expressed in terms of an unknown P (s). This resulted in the theory pdf

P (c

s) being evaluated at the data points c1, c2 . . . cn, but not the data pdf ! It is precisely

|

this failure to evaluate P (c) given c that has led to the absence of goodness of ﬁt criteria in

unbinned likelihood ﬁts.

In binned likelihood ﬁts, one ﬁts a theoretical curve to a binned set of data points. Two

distributions, those of theory and data, are involved in providing a goodness of ﬁt measure

8

such as χ2 in the binned approach. In the unbinned method, however, one ﬁnds the maximum

likelihood point by evaluating the theoretical function P (c

s) at the data points ci, i = 1, n.

|

There is only one distribution involved, namely theory! One has hitherto ignored P (c), by

subsuming it into a normalization constant. We rectify this lapse here and restore P (c) to

its proper role, namely, the pdf of the data.

A. Probability Density Estimators

Let cα

i , α = 1, d denote the components of the d-dimensional vector c for the ith event.

Then we can deﬁne the mean vector < cα > as

< cα >=

1
n

i=n

Xi=1

cα
i

The covariance (or error) matrix E of c is deﬁned as

Eα,β =< cαcβ >

< cα >< cβ >

−

where the <> implies average over the n events. The Hessian matrix H is deﬁned as the

inverse of E. One can deﬁne a multivariate Gaussian Kernel

(c) as

where the repeated indices imply summing over and the parameter h is a “smoothing pa-

rameter”, which has[4] a suggested optimal value h

n−1/(d+4). The pdf of c is then given

by

(c) =

G

(√2πh)d

(det(H)

exp( −

1

p

G
H αβcαcβ
2h2

)

≈

1
n

i=n

Xi=1

P (c)

P DE(c) =

≈

(c

G

−

ci)

Simply put, one takes an arbitray point c in conﬁguration space, calculates the separation

from this point to all the measured points and sums up the values (at c) of the Gaussians

that are centered at the measured points. This sum is divided by the number of Gaussians,

which equals n. Since the Gaussians are all normalized to unity, P (c) obeys equation 20.

One can feed in any value of c and the P DE will provide a probability density at that value

of c.

It is clear that the P DE method is generalizable to arbitrary dimensions and will

provide us with P (c). One should note that the Gaussian Kernel function depends on n,

9

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

the number of events in the sample. This dependence is through the smoothing parameter,

which goes to zero as n

. In this limit, equation 42 becomes

→ ∞

P (c) =

P (c)G∞(c

ci)dci

Z

−

This implies that

G∞(c

ci)

−

≡

lim
n→∞

G(c

−

ci) = δ(c

ci)

−

There exist generalizations [5] of the above scheme where the covariance matrix is made

locally variable that can estimate pdf ′s with greater complexity albeit at a cost in computing

speed. In what follows, we introduce a method by which the smoothing factor can be made

a function of the conﬁguration variables c in an iterative fashion, which may be equivalent

to varying the covariance matrix locally.

V. AN ILLUSTRATIVE EXAMPLE

We illustrate the ideas discussed above with a simple one-dimensional example of events

in which the observable c consists of decay times distributed exponentially with a decay

constant s=1.0 in arbitrary units. The conditional probability P (c

s) deﬁnes our theoretical

|

model and is given by

The P DE one dimensional Gaussian kernel for this simple case would be given by

P (c

s) =

exp(

|

1
s

c
s

)

−

(c) =

G

1
(√2πsh)

exp(

c2
2s2h2 )

−

We generate a thousand events for which the smoothing parameter h is calculated to be 0.125

as per equation[6] h = 0.5n−1/(d+4). Figure 1 shows the generated events, the theoretical

curve P (c

s) and the P DE curve P (c) normalized to the number of events.

|

The P DE curve is a poor estimator of the data near the cutoﬀ at c=0. This is because

the Gaussians centered at values of negative c would have contributed to the curve near c=0.

Also, for large values of c, data are sparse and the Gaussian approximation with constant

smoothing factor h ﬁnds it diﬃcult to approximate the data. We choose to restrict our

ﬁtting to a ﬁducial interval in time t1 < c < t2 = 1 < c < 5. Both the theoretical model

P (c

s) and the P DE likelihood curves are renormalized so that they integrate to unity in

|

the ﬁducial interval.

10

FIG. 1: Figure shows the histogram (with errors) of generated events. Superimposed is the theo-

s) and the P DE estimator (solid) histogram with no errors.
retical curve P (c
|

A.

Iterative Determination of the Smoothing Factor

The expression h

n−1/(d+4) clearly is meant to give a smoothing factor that decreases

≈

slowly with increased statistics n.

It is expected to be true on average over the whole

distribution. However, the exponential distribution under consideration has event densities

that vary by orders of magnitude as a function of the time variable c. In order to obtain a

function h(c) that takes into account this variation, we ﬁrst work out a P DE with constant

h and then use the number densities obtained thus [7] to obtain h(c) as per the equation

n P DE(c)
(t2 −
The equation is motivated by the consideration that a uniform distribution of events

h(c) =

t1) (cid:19)

(47)

(cid:18)

−0.6

between t1 and t2 has a pdf = 1/(t2 −
The function h(c) thus obtained is used to work out a better P DE(c). This process is

t1) whereas the real pdf is approximated by P DE.

iterated three times to give the best smoothing function.

We generate n=1000 events in the ﬁducial interval. If now we were to superimpose a

11

Gaussian with 500 events centered at c=2.0 and width=0.2 on the data, the P DE estimator

will follow the data as shown in Figure 2. This shows that the P DE estimator we have is

adequate to reproduce the data, once the smoothing parameter is made to vary with the

number density appropriately.

FIG. 2: Figure shows the histogram (with errors) of 1000 events in the ﬁducial interval 1.0 < c < 5.0

generated as an exponential with decay constant s=1.0. with a superimposed Gaussian of 500

events centered at c=2.0 and width=0.2. The P DE estimator is the (solid) histogram with no

errors.

h

≈

The smoothing function h(c) for the events in Figure 2 is shown in Figure 3. It can be

seen that the value of h increases for regions of low statistics and decreases for regions of

high statistics. Superimposed is the constant smoothing factor obtained by the equation

0.5n−1/(d+4) = 0.5n−0.2, with n being the total number of events generated, including

those outside the ﬁducial volume.

12

FIG. 3: The variation of h as a function of c for the example shown in Figure 2. The variation

of the smoothing parameter is obtained iteratively as explained in the text. The ﬂat curve is a

smoothing factor resulting from the formula h

0.5n−1/(d+4).

≈

B. An Empirical Measure of the Goodness of Fit

The negative log-likelihood ratio

logeLR at the maximum likelihood point
now provides a measure of the goodness of ﬁt. In order to use this eﬀectively, one needs

N LLR ≡ −

an analytic theory of the sampling distribution of this ratio. This is diﬃcult to arrive at,

since this distribution is sensitive to the smoothing function used. If adequate smoothing is

absent in the tail of the exponential, larger and broader sampling distributions of

will

N LLR

result. One can however determine the distribution of

empirically, by generating the

N LLR

events distributed according to the theoretical model many times and determining

N LLR
at the maximum likelihood point for each such distribution. The solid histogram in ﬁgure 4

shows the distribution of

for 500 such ﬁts. This has a mean of 2.8 and an rms

N LLR

of 1.8. The dotted histogram shows the corresponding value of

for the constant

N LLR

value of smoothing factor shown in ﬁgure 3. This distribution is clearly broader (rms=2.63)

with a higher mean(=9.1) and thus has less discrimination power in judging the goodness

13

of ﬁt than the solid curve. We now ﬁt the same exponential background distribution with

FIG. 4: The solid curve shows the distribution of the negative log likelihood ratio

at the

N LLR

maximum likelihood point for 500 distributions, using the iterative smoothing function mechanism.

The dashed curve shows the corresponding distribution in the case of a constant smoothing function.

diﬀerent numbers of Gaussian events superimposed on an exponential background. Table I

shows the results of the ﬁt. When a Gaussian of 500 events with width 0.2 and mean 2.0

is superimposed on the exponential distribution of 1000 events, a value of

=189 is

N LLR

obtained while trying to ﬁt for the exponential using the unbinned maximum likelihood

method. This is 103σ away from the mean of the

distribution shown in ﬁgure 4 with

N LLR

the iterated smoothing function. A 3σ eﬀect is observed when the number of events in the

Gaussian is 85. Figure 5 shows the generated events, the PDE and the ﬁtted curve for this

case.

Let us note that it is possible to make a cumulative function from the solid histogram

in ﬁgure 4 and estimate the probability that

exceeds the observed value, just as we

do with χ2 tests. We have also performed a binned χ2 ﬁt to an exponential over the same

N LLR

14

histograms, with the data in the ﬁducial region binned over 41 bins. The resulting value

of χ2 for 39 degrees of freedom are shown in the last column in table I. At the 3σ point

for the unbinned method, the binned method yields a χ2 of 42 over 39 degrees of freedom,

which may be considered a good ﬁt. This implies that the unbinned method may have more

discriminating power against bad ﬁts than the binned one. It is worth noting however that

the binned ﬁt is over two parameters (the number of events and the slope) whereas the

unbinned ﬁt being considered here is only over a single parameter, namely the slope.

TABLE I:

Number of

Unbinned ﬁt Unbinned ﬁt Binned ﬁt χ2

Gaussian events

39 d.o.f.

N LLR
189.

58.6

11.6

8.2

6.3

2.55

0.44

N σ

103

31

4.9

3.0

1.9

-0.14

-1.33

304

125

48

42

38

30

24

500

250

100

85

75

50

0

We now can ﬁt the exponential data (with no superimposed Gaussian bumps) and com-

pute the value of the likelihood ratio

P (c) as a function of the parameter s about the
maximum likelihood point. Figure 6 shows this function, which has the maximum value at
LR, a dimensionless quantity, is not the likelihood distribu-

s = 1.019. Note, however, that

LR = P (c|s)

tion of s, which has to have the dimensions of 1/s.

C. Determination of the a priori Likelihood P(s)

In order to obtain the likelihood distribution of s, P (s

cn), we need to understand better

|

P (s), the a priori distribution of s. We are in a position to do this, since we have identiﬁed

P (c) to be the distribution of data, and P(s) and P(c) are linked by the equation

P (c) =

P (c

s)

P (s)ds

Z

|

×

(48)

15

FIG. 5: Figure shows the histogram (with errors) of 1000 events in the ﬁducial interval 1.0 < c < 5.0

generated as an exponential with decay constant s=1.0. with a superimposed Gaussian of 85 events

centered at c=2.0 and width=0.2. The P DE estimator is the (solid) histogram with no errors.

The data are ﬁtted with a goodness of ﬁt that is 3σ away from the average value of

. The

N LLR

continuous curve shows the ﬁt to an exponential.

Before we use equation 48 to calculate P(s), let us make the following observations. Using

equation 31, we can write

|

|

P (s

cn) = P (s)

× LR = P (s)

×

P (cn
s)
|
P (cn)

(49)

As we increase n, the number of events sampled, in the limit n
cn) to tend towards the delta function δ(s

a posteriori probability P (s

→ ∞

we expect the

s∗) where s∗ is the
cn) is the likelihood distribution of s and we expect

−

true value of s. This is because P (s

|

to determine the true value of s with inﬁnite precision in this limit. However, the ratio
P (cn|s)
P (cn) will tend towards unity in this limit (for a good ﬁt), since for each data point ck, the
s) and the data pdf P (ck) will be close to each other. The only way out
theoretical pdf P (ck|
of this is to allow P (s) to depend discretely on n and let the distribution P (s)

s∗)

δ(s

as n

. We can see the need for this further, using equation 48. In the limit n

→ ∞

→

−

,

→ ∞

16

FIG. 6: Figure shows the likelihood ratio

P (c) as a function of the ﬁtted parameter s. The

LR = P (c|s)

maximum likelihood point is at s = 1.019.

the data pdf P(c) will have the form P (c

s∗), where s∗ is the true value of s, if it ﬁts the

|

theoretical model. Then the only solution for equation 48 is again P (s) = δ(s

s∗).

−

To repeat, the only way out of this dilemma, is for the a priori probability distribution

P(s) to be dependent on n and tend towards a delta function as n

. If we are solving a

→ ∞

Bayes theorem problem for n data points, then the a priori function P(s) for that problem

will be written as Pn(s) indicating that it comes from a discrete familiy of probability

distributions that depend on n. Nothing further is known about Pn(s) a priori except that

it is a pdf in s and that the pdf depends discretely on n.

1. Rewriting the Bayes Theorem Equations

The Bayes theorem equations have to be re-written to take into account this change.

Equation 31 now becomes

17

LR,n =

cn)
P (s
|
Pn(s)

=

P (cn
s)
|
P (cn)

Equation 33 remains as is and equation 34 becomes

LR,n =

cn)
P (s
|
Pn(s)

=

c1)
P (s
P1(s) ×

|

c2)
P (s
P1(s) · · · ×

|

cn)
P (s
|
P1(s)

where we have also added the subscript n to the likelihood ratio

LR to indicate its dependence

on n. The recursive chain rule can now be rewritten as

LR,k =

LR,l =

i=k

Yi=1
i=l

ck)
P (s
|
Pk(s)

cl)
P (s
|
Pl(s)

=

=

s)
P (ci|
P (ci)

s)
P (ci|
P (ci)

s)
P (ci|
P (ci)

Yi=1
i=k+l

Yi=1

LR,k+l =

LR,k × LR,l =

ck)
P (s
Pk(s) ×

|

cl)
P (s
|
Pl(s)

=

ck+l)
P (s
|
Pk+l(s)

=

where we have two sub-samples of k and l events which are being combined to form a total

number of k + l events.

2. An Ansatz for Pn(s)

The expression for P (c) in equation 48 can be thought of as the theoretical prediction

for P (c) and the P DE estimator is the experimental measurement of P (c). Then, one can

write,

P pred(cn)
P exp(cn)

=

Z

s)

P (cn
|
P P DE(cn) ×

Pn(s)ds

=

Z LR,n(s)

×

Pn(s)ds =

P (s

cn)ds = 1

Z

|

with the last expression following from Bayes theorem. There are two ways the last equation

P (s

cn)ds = 1 can be satisﬁed. Either the likelihood ratio

LR,n(s) = 1 or if

|

R

It is very diﬃcult for

LR,n(s) to equal unity even at the maximum likelihood value, since
the experimental P DE estimator in the denominator is subject to statistical ﬂuctuations.

Pn(s) =

1

LR,n(s)ds ≡

R

1
2λ

18

(50)

(51)

(52)

(53)

(54)

(55)

(56)

(57)

Equation 57, however, gives us an expression for the a priori likelihood Pn(s). Pn(s) is the

value of the a priori probability distribution at the true value of s. Since

Pn(s)ds = 1,

R
we can satisfy this with a functional form for Pn(s) being a step function θ(s
|

µ) such that

(58)

(59)

(60)

(61)

(62)

(63)

with s1 = µ

λ

−

and s2 = µ + λ

Pn(s)

θ(s

µ) = 0 if s < s1 or if s > s2

|

≡
Pn(s)

θ(s

µ) =

≡

|

if s1 ≤

s

≤

s2

1
2λ

and the value of µ, the mean of the distribution, is totally unknown. Let us note that it is

to compute a dimensionless quantity such as the likelihood ratio

possible to write down an equation such as equation 57 only due to the fact that we are able
LR,n(s) and this becomes
possible only due to the concept of the P DE estimator for P (c) introduced in this paper.

The integral in equation 57 has thus the dimensions of s giving Pn(s) the dimensions of s−1

as required.

As n increases, the value of λ decreases, since the distribution P (cn

s) sharpens. This

|

has the eﬀect of narrowing Pn(s) in accordance with the discussion above. It is important

to realize that there is only one true value of s, and the Bayesian a priori probability Pn(s)
refers to the value of Pn(s) at that true value, which, according to our ansatz, equals 1
2λ.
The data does not result from an admixture of probable values of s, but from a single true

value of s. So Bayes theorem becomes,

P (s

cn)

P (cn) = P (cn

s)

Pn(s) = P (cn

s)

|

×

|

×

1
2λ

|

×

yielding

P (s

cn) =

|

P (cn
s)
|
P (cn) ×

1
2λ

= LR(s)

=

LR(s)ds

R

R

P (cn
P (cn

s)
|
s)ds
|

The last equation in 63 results from the fact that the P DE estimator for P (c) cancels both
cn), one can proceed to calculate

in the numerator and denominator. Having obtained P (s

the statistical quantities associated with s, namely the mean, mode, median, variance, errors
cn) is obtainable only with the use of

and limits, in a rigorous fashion. We note here that P (s

of Bayes theorem, and our ansatz for the Bayesian a priori likelihood P (s). The expression
cn), the a posteriori likelihood for s does not depend on the P DE estimator of data,

for P (s
but only on the theoretical function P (cn

|

s) evaluated at the data points. The evaluation

|

|

|

19

of Pn(s) and the goodness of ﬁt criteria both require the usage of the P DE estimator for

the data pdf .

The ansatz for the a priori distribution for Pn(s) assumes a ﬂat distribution in Pn(s).

This ﬂatness may not be invariant under change of variables and the consequences of this

needs further investigation. Let us note, however, that we use the value of the function Pn(s)

at the true value of s only and so may not be sensitive to the shape of Pn(s) and that we do

not use the a priori distribution for any calculations, since the information about the error
LR(s). Combining data from diﬀerent datasets may be

of s is contained in the normalized

done by multiplying likelihood ratios as shown in equation 54.

We note in passing that the the values of λ are not large enough to span the width of

the likelihood distribution. Figure 7 shows the correlation between λ and the ratio (3σ/λ),
where σ is the rms of the likelihood ratio distribution, for 500 conﬁgurations cn of 1000

events per conﬁguration. At no point does the ratio fall below unity, indicating that the

likelihood curve is always broader than the step function θ(s

µ). We may not blindly use the

step function as the a priori distribution, centered at the maximum likelihood value and do

|

the integral in equation 56, since it will chop oﬀ the likelihood ratio curve in the tails. The

step function can only be used after we feed it with a mean value µ. The function in the

integral in equation 56 is in fact a constant which equals the value of Pn(s) at the true value

of s. This value does not change as we change s in the integral in equation 56. It is possible

that the true value of s is at the maximum likelihood point. It is also possible that it is at a

value 3σ away, albeit with a reduced probability. The key point is that the true value of s is

either at the maximum likelihood point or at any of the other values at which the likelihood

is non-zero. They do not simultaneously have to be true. Hence we can integrate over the
whole likelihood distribution with Pn(s) = 1

2λ without worrying about falling oﬀ the edge
of the step function. As one varies s, one is testing mutually exclusive hypotheses that the

value of s under consideration is the true value of s.

It is still instructive to see what happens when one supplies a distribution for the mean

value of the step function. The following section deals with the self-consistency of our
cn) for the mean value µ of

expressions, when one feeds in the a posteriori distribution P (s

|

the step function.

20

FIG. 7: Figure shows a scatter plot of λ, half the integral under the likelihood curve vs. 3σ/λ,

where σ is the width of the likelihood distribution for 500 conﬁgurations.

3. The Bootstrap

If the mean value µ of the step function distribution has a probability distribution P (µ),

then one can write an expression for the joint probability density of µ and s as

inside the shaded region in ﬁgure 8 and is zero outside. Integrating the above equation

along the s axis ﬁrst (ﬁxed µ), followed by integration along the µ axis yields

We can now reverse the order of integraton, doing the µ integration ﬁrst, which yields

P (µ)

θ(s

µ)dµds =

dµds

×

|

P (µ)
2λ

P (µ)

dµ

θ(s

µ)ds = 1

Zµ

×

Zs

|

s+λ

1
2λ

ds

Zs

Z

s−λ

P (µ)dµ = 1

1
2λ

Zs

g(s)ds = 1

21

(64)

(65)

(66)

(67)

This can be re-written as

FIG. 8: The abscissa shows the variable s and the ordinate the variable µ, the mean value of the

θ function distribution. The hatched region shows the area over which the probability distribution

for s is non zero as a function of µ.

where

g(s) =

P (µ)dµ

s+λ

Z

s−λ
These equations are true for any P (µ). We need to supply a P (µ), which is the probability

distribution of the mean value of the θ function. The obvious candidate for P (µ) is clearly

P (s

cn), the a posteriori distribution for the true value of s. Since

g(s)ds = 2λ, g(s) can

|

be identiﬁed with

LR(s), yielding
g(s)

s+λ

≡ LR(s) =

Z

s−λ

P (s

cn)ds

|

R

For small λ, we can Taylor expand the above integral yielding,

yielding the desired result

LR(s)

≈

2λ

×

P (s

cn)

|

P (s

cn)

|

≈

LR(s)
2λ

= LR(s)

LR(s)ds

R

22

(68)

(69)

(70)

(71)

Also, from equation 67, we can identify P (s

2λ is the projection of the
probability density for s in ﬁgure 8 along the s axis, computed after knowing cn. This again

≡

|

g(s)

2λ , since g(s)

cn)

yields equation 71 and completes the bootstrap.

To summarize the arguments so far,

•

We have made the ansatz that the a priori distribution for Pn(s) is

LR(s)ds . Such an
ansatz gives us a distribution Pn(s) whose mean value is unknown but whose width

R

1

the a priori distribution. This step requires a dimensionless

decreases with increased statistics. Both these properties qualify it as a candidate for
LR and is only possible
by the use of the experimental P DE′s for the goodness of ﬁt test, introduced in this

paper.

of s.

•

This results in

•

We then supply it with a probability distribution for the mean value, which is only
known after we have analyzed cn. The candidate for the probability distribution for
cn), which is the a posteriori distribution for the true value of s, and

the mean µ is P (s

|

is the object of our quest. This is then used to calculate the probabilty distribution

This yields the expression for P (s

cn) of equation 71 as well as a probability density

a posteriori for s that is consistent with the same equation.

|

In multi-dimensional parameter space, with α being the dimension of the parameter

P (s

|

cn) = LR,n(s)

=

LR,n(s)ds

R

R

P (cn
P (cn

s)
|
s)ds
|

for the a posteriori likelihood for s.

vector s. the above equations are generalized as follows

with integrals over s being carried out over α dimensions.

Pn(s) =

1

LR, n(s)ds ≡

R

1
(2λ)α

(72)

(73)

23

VI. TOWARDS AN ANALYTIC THEORY OF UNBINNED LIKELIHOOD

GOODNESS OF FIT

Because the likelihood ratio

LR(s) is invariant under transformation c

→

′

c

, one can use

variables c

such that

′

This leads to probability distributions P (c

s) such that

′

c

(c) =

c

P (c

s)dc

′′

′′

|

Z
0

′

|

P (c

s) = P (c

s)

′

|

′

dc
dc′ |

= 1

|

× |

′

(74)

(75)

and with the limits of the variable c

being 0 < c

< 1. These sets of transformations in

multi-dimensions is known as the hypercube transformation. The number density is constant

in the hypercube which implies that we are not sensitive to systematics associated with the

smoothing parameter.

The theoretical curve is a constant =1 in this scheme. The experimental P DE will also

be close to being ﬂat. The question to answer is “ What is the distribution of the negative

log likelihood ratio

that results from the statistical ﬂuctuation of the P DE in the

hypercube”? We leave this question to a subsequent paper.

N LLR

VII. CONCLUSIONS

We have introduced a technique for estimating goodness of ﬁt in unbinned likelihood ﬁts

by the use of probability density estimators to obtain the a priori likelihood distribution of

the data. In additions to provide a measure of the goodness of ﬁt in unbinned likelihood ﬁts

for the ﬁrst time, this approach enables us to obtain expressions for the a priori likelihood

distribution of the theoretical parameters and hence to derive expressions for the a posteriori

likelihood distributions of the theoretical parameters. We have shown that the a priori

likelihood of the theoretical parameters depends on the number n of events being employed in

the problem. We have emphasized that the a priori likelihood is the value of the probability

distribution at the true value of s and this does not change as we change s, a posteriori, to

calculate the likelihood that s is the true value.

The approach outlined in this paper permits the rigorous calculation of errors in the

ﬁtted quantites.

It makes unnecessary the practice of “guessing” the a priori likelihood

24

distributions of parameters, a practice titled “Bayesianism”. For the type of problems

considered here, the a priori likelihood distributions can be computed.

The techniques detailed here are extensible to arbitrary dimensions, even though we have

used a one-dimensional problem for illustrative purposes. In the process of using probability

density estimators, we have developed an algorithm for iteratively improving the smoothing

parameter as a function of local number density.

VIII. ACKNOWLEDGEMENTS

The author would like to thank British Rail for a particularly peaceful and appropriately

long journey between Durham and London during which the main ideas in this paper were

worked out.

IX. REFERENCES

[1] R. A. Fisher,“On the mathematical foundations of theoretical statistics”, Philos. Trans. R. Soc.

London Ser. A 222, 309-368(1922);

R. A. Fisher,“Theory of statistical estimation”, Proc. Cambridge Philos. Soc. 22, 700-725

(1925).

Comm.

[2] “An essay towards solving a problem in the doctrine of chances”, Rev. Thomas Bayes,

Biometrika,45 293-315 (Reprint of 1763) (1958).

[3] B. Knuteson et al , FERMILAB-PUB-01-226-E, physics/0108002, submitted to Comp. Phys.

L. Holmstr¨om, S. R. Sain, and H. E. Miettinen, Comp. Phys. Comm. 88:195 (1995).

[4] D. Scott. Multivariate Density Estimation. John Wiley & Sons, 1992.

M. Wand and M. Jones, Kernel Smoothing. Chapman & Hall, 1995.

[5] S.Towers, Kernel Probability Density Estimation Methods, paper submitted to the Confer-

ence on Advanced Statistical Techniques in Particle Physics, Durham, England, March 2002,

http://www.ippp.dur.ac.uk/statistics/papers/towers pde.ps.gz

25

[6] After a modest amount of experimentation with smoothing factors, we settled on the factor of

0.5 as being adequate to reproduce the exponential during this ﬁrst phase of analysis. Later the

smoothing function was introduced to improve the goodness of ﬁt capability of the technique.

[7] Empirically we found that a power of the order of -0.6 is needed to provide suﬃcently large

smoothing factors for large values of time. We can in principle optimize this smoothing function

further, but have not done so.

[8] MINUIT is a general minimization program written by F. James.

26


6
0
0
2
 
l
u
J
 
5
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
2
2
7
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Isotropy Properties of the Multi-Step Markov Symbolic Sequences

S. S. Apostolov, Z. A. Mayzelis
V. N. Karazin Kharkov National University, 4 Svoboda Sq., Kharkov 61077, Ukraine

O. V. Usatenko∗, V. A. Yampol’skii
A. Ya. Usikov Institute for Radiophysics and Electronics
Ukrainian Academy of Science, 12 Proskura Street, 61085 Kharkov, Ukraine

A new object of the probability theory, the two-sided chain of symbols (introduced in
Ref. arXiv:physics/0306170) is used to study isotropy properties of binary multi-step Markov chains
with the long-range correlations. Established statistical correspondence between the Markov chains
and certain two-sided sequences allows us to prove the isotropy properties of three classes of the
Markov chains. One of them is the important class of weakly correlated additive Markov chains,
which turned out to be equivalent to the additive two-sided sequences.

PACS numbers: 05.40.-a, 02.50.Ga, 87.10.+e

I.

INTRODUCTION

The problem of long-range correlated random symbolic systems (LRCS) has been under study for a long time in

many areas of contemporary physics [1–6], biology [7–12], economics [8, 13, 14], linguistics [15–19], etc.

Among the ways to get a correct insight into the nature of correlations of complex dynamic systems, the use of
the multi-step Markov chains is one of the most important because it allows constructing a random sequence with
the prescribed correlated properties in the most natural way. The N -step Markov chains are characterized by the
conditional probability that each symbol of the sequence takes on some deﬁnite value depending on N previous
symbols. These chains can be easily constructed by sequential generation using the prescribed conditional probability
function. The binary correlation functions of the Markov chains can be explicitly calculated in some simple cases.
The concept of additive chains turned out to be very useful because it is possibile to evaluate the binary correlation
function of the chain via the memory function (see for the details Refs. [20–22]).

Another important reason for the study of Markov chains is their application to the various physical objects [23–
25], e.g., to the Ising chains of classical spins. The problem of thermodynamical description of the Ising chains with
long-range spin interaction is open even for the 1D case. However, the association of such systems with the Markov
chains can shed light on the non-extensive thermodynamics of the LRCS.

The LRCS can also be modeled by another class of correlated sequences, the so-called two-sided chains introduced
in Sec. II. They are characterized by the conditional probability that each symbol of the sequence takes on the deﬁnite
value depending on the neighboring symbols at both sides of the considered symbol. An example of systems with such
a property is the above-mentioned Ising chain. In Ref. [26], we proved that the two-sided sequences were statistically
equivalent to the Markov chains. In this paper this equivalence is used to study the isotropy properties of the Markov
chains.

The paper is organized as follows. In the ﬁrst Section, we give the deﬁnitions of the Markov and two-sided chains
and formulate the problem of this study. The next Section is devoted to the examination of the anisotropy properties
of three classes of Markov chains: (a) the class of Markov chains that are equivalent to the two-sided sequences with
symmetric conditional probability function; (b) the Markov chains with permutative conditional probability function;
(c) the additive Markov chains, which are shown to be equivalent to the additive two-sided sequences.

II. GENERAL DEFINITIONS

Let us determine the N -step Markov chain.

i ∈ Z =
{. . . , −2, −1, 0, 1, 2, . . . }, possessing the following property: the probability of symbol ai to have a certain value,
under the condition that the values of all previous symbols are ﬁxed, depends on the values of N previous symbols
only,

This is a sequence of random symbols, ai,

P (ai = a| . . . , ai−2, ai−1) = PN (ai = a|ai−N , . . . , ai−2, ai−1).

(1)

∗ usatenko@ire.kharkov.ua

2

(3)

(4)

The Markov chain is a homogeneous sequence, because the conditional probability Eq. (1) does not depend explicitly
on i, i.e., is independent of the position of symbols ai−N , . . . , ai−1, ai in the chain. It only depends on the values of
ai−N , . . . , ai−1, ai and their positional relationship.

An important class of the random sequences is the binary chains where each symbol ai can take on only two values,

say, 0 and 1.

A very important subclass of the Markov chains is the additive binary ones. The conditional probability functions

for these chains are described by the following formula:

PN (ai = 1|ai−N , . . . , ai−2, ai−1) = ¯a +

F (r)(ai−r − ¯a).

(2)

N

r=1
X

Here F (r), r = 1, . . . , N , is the memory function and ¯a is the average number of unities in the sequence (see, e.g.,
Ref. [21]).

A permutative binary N -step Markov chain is determined by the conditional probability that is independent of the

order of symbols within the memory length N and depends on the number of unities among them only.

We deﬁne an isotropic chain as a sequence for which the probability of an arbitrary set of L sequential symbols
occurring, referred to as the L-word, does not depend on the direction of ”reading” the symbols. For example, the
probabilities of 5-words 01100 and 00110 occurring are equal.

In diﬀerent mathematical and physical problems, we are confronted with the sequences for which the conditional

probability of symbol ai to have a certain value depends on values of N previous and N next symbols only,

P (ai = a| . . . , ai−2, ai−1, ai+1, ai+2, . . .) =

= P2,N (ai = a|ai−N , . . . , ai−1, ai+1, . . . , ai+N ).

We refer to these chains as N -two-sided sequences. One can deﬁne additive binary two-sided chains similarly to the
Markov ones,

P2,N (ai = 1|ai−N , . . . , ai−1, ai+1, . . . , ai+N ) = ¯a +

G(r)(ai+r − ¯a).

N

r=−N
X
r6=0

Here G(r), r = ±1, . . . , ±N , is the memory function of the two-sided chain. For the same reason as above, see Eq. (1),
this chain is homogeneous.

In paper [26], the equivalence of the two-sided and the Markov chains was proved. We have derived the equation

describing the correspondence between their conditional probabilities,

P2,N (ai = 1|T −

i , T +

i ) =

.

(5)

PN (ai+r|T −

i+r)

N

r=0
ai =1
Q

N

r=0
ai=0
Q

N

r=0
ai=1
Q

PN (ai+r|T −

i+r) +

PN (ai+r|T −

i+r)

Here T −
symbol aj.

j = (aj−N , . . . , aj−1) and T +

j = (aj+1, . . . , aj+N ) are the previous and the following N -words with respect to

Thus, any Markov chain can be characterized not only by the conditional probability PN (ai = a|T −

i ), but by the

two-sided conditional probability function, P2,N (ai = a|T −

i , T +

i ), as well.

III.

ISOTROPY OF THE MARKOV CHAINS

A. Two-sided chains with symmetric probability function

The deﬁnition of isotropic N -step Markov chain given in Sec. II is equivalent to the following statement: the Markov

chain is isotropic if its two-sided conditional probability function is symmetrical,

P2,N (ai = a|T −

→ , T +

→ ) = P2,N (ai = a|T +

← , T −

← ).

(6)

3

Here T − and T + are the previous and the following N -words with respect to symbol a. The subscript “→” indicates
that word T is read in the direct order, from left to right, and the subscript “←” shows that word T is read in the
inverse order. For example, if the binary 3-two-sided chain is isotropic, then

P2,3(ai = 1|011, 001) = P2,3(ai = 1|100, 110).

Equation (6) can serve as the second deﬁnition of the isotropy for the Markov chains.

Below we prove the equivalence of these two deﬁnitions of the isotropy. This equivalence is the main result of our

paper.

Suppose that the ﬁrst deﬁnition is valid and, therefore, the probability P (T −

→ ) of an arbitrary (2N + 1)-
word occurring in the Markov chain is independent of the direction in which it is read. Then, the formula for two-sided
probabilities can be rewritten as

→ , aj = a, T +

P2,N (aj = a|T −

→ , T +

→ ) =

P (T −

→ , aj = a, T +
→ )
→ , aj = b, T +
→ )

P (T −

=

P (T +

← , aj = a, T −
← )
← , aj = b, T −
← )

P (T +

= P2,N (aj = a|T +

← , T −

← ).

b
P

b
P

Thus, the two-sided conditional probability function of the Markov chain is symmetrical and the chain is isotropic in
accordance with the second deﬁnition.

Now, let us suppose that the second deﬁnition of the isotropy is valid, i.e., the two-sided probability of the Markov

chain is symmetrical.

Then, this chain, being read in the direct and inverse orders, is statistically identical.
In other words, the probability of symbol ai = a occurring under condition that previous N -word T→ is ﬁxed is
equal to that of the same symbol occurring under the condition that the following N -word T← is ﬁxed. It means that
the original chain and its copy written in the inverse order have the same Markov conditional probabilities. However,
the probabilities of words of arbitrary length L are determined completely by the conditional probabilities. Hence,
these probabilities are the same for both chains.

Thus, the probability of an arbitrary word of length L occurring does not depend on the direction in which it is

read.

According to Eq. (5) this chain, being read in the direct and inverse orders, has the same Markov conditional prob-
abilities. However, the probabilities of words of arbitrary length L are fully governed by the conditional probabilities.
Hence, this chain is isotropic according the ﬁrst deﬁnition.

In the general case, the Markov chains are anisotropic. Nevertheless, our analysis of Eq. (5) shows that all 1-step
and 2-step additive Markov chains are isotropic. All non-biased 3-step additive Markov chains are also isotropic.
The additive chains can be anisotropic for N > 3. The 3-step biased Markov chains with conditional probability

P (ai = 1|ai−3, ai−2, ai−1) = ¯a +

fr(ai−r − ¯a)

(7)

3

r=1
X

and ¯a 6= 1/2 are isotropic in three exceptional cases only:

1. f1 = f2. This condition is fulﬁlled, e.g., for the chains with the step-wise memory function.

2. f1 + f2 + f3 = 1. It is the degenerated case. This memory function determines the Markov chain consisting

completely of unities or zeroes, since P (ai = 1|111) = P (ai = 0|000) = 1.

3. f3 = 0. Actually, it is the 2-step additive Markov chain.

All additive Markov chains with small memory functions, F (r) ∝ ε ≪ 1, are isotropic in the main approximation

with respect to ε. This fact will be clariﬁed in Subsec. III C 1.

It should be noted that the two-sided chain with asymmetrical conditional probability function can be considered
as the Markov chain being read in the direct order and as another Markov chain being read in the inverse order. The
conditional probability functions of these chains are diﬀerent. From the foregoing two conclusions can be made:
1. There are, at least, two diﬀerent asymmetrical N -steps Markov chains with equal correlation functions,

2. The additive anisotropic Markov chain being read in the inverse order is the non-additive Markov chain. Other-
wise we would have two additive Markov chains with diﬀerent memory functions having the same correlation function.
But as shown in Ref. [22] it is not feasible.

K(r) = (ai − ¯a)(ai+r − ¯a).

(8)

B. Permutative Markov chains

1.

Isotropy of permutative Markov chains

Here we prove that the permutative binary N -step Markov chain is isotropic. Let us consider two chains, M and
M′, where M is the given Markov chain, {ai}, and M′ is the chain written in the inverse order, {a′
i = a−i. We
−→
P (1|T (k)), as the probability of symbol ai in chain M to be equal to unity given the
refer to
previous N -word T (k) contains k unities. The probability of the N -word occurring satisﬁes the set of linear equations,

−→
P (ai = 1|T (k)) =

−→
P k, or

i}, a′

−→
P (aN |a0, a1, . . . , aN −1)P (a0, a1, . . . , aN −1),

P (a1, a2, . . . , aN ) =

. . .

P (a1, a2, . . . , aN ) = 1.

a0
P




a1
P

aN
P

The probability P (a1, a2, . . . , aN ) of N -word occurring is determined uniquely by Eq. (9). The solution of this set of
equations depends on the total number k = a1 + a2 + . . . + aN of the unities in the N -word (a1, a2, . . . , aN ) and can
be presented in the form P (a1, a2, . . . , aN ) = Pk. One can easily ﬁnd the following expression for Pk,



Pk = P0

k

−→
P r−1
−→
P r

1 −

.

r=1
Y
←−
P (1|T ) as the probability of symbol a′

We also refer to

i to be equal to unity under condition that the previous
N -word in chain M′, T , is ﬁxed. Now let us prove that the conditional probability functions of chains M and M′
−→
are equal to each other. In other words,
P k for arbitrary N -word T containing exactly k unities
(k = 1, . . . , N ) and is not dependent on the order of symbols in this word. With this purpose in mind one needs to
prove that

←−
P (1|T ) equals to

←−
P (1|[k], 0) =

←−
P (1|[k − 1], 1) =

−→
P k.

Here [j] means (N − 1)-word containing exactly j unities, so that (1, [k − 1]) and ([k − 1], 1) are the N -words containing
k unities. Using the deﬁnition of the conditional probability we ﬁnd:

←−
P (1|[k − 1], 1) =

←−
P (1|[k − 1], 1)

Pk
Pk

=

P (1, [k − 1], 1)
Pk

−→
P (1|1, [k − 1]) =

−→
P k,

=

From set (9), one gets the relation

So, we have proved that

←−
P (1|[k], 0) =

P (1, [k], 0)
Pk

=

1 −

−→
P k+1

Pk+1
Pk

.

(cid:0)

(cid:1)

Pk+1 =

−→
P k+1Pk+1 +

−→
P kPk.

←−
P (1|[k − 1], 1) =

←−
P (1|[k], 0) =

−→
P k.

←−
P k as the probability of symbol in chain M′ to be equal to unity under condition that the
Thus we can refer to
previous N -word containing exactly k unities is ﬁxed, and the conditional probability functions for the chains M and
M′ are equal to each other,

Hence,

since

−→
P k =

←−
P k, k > 1.

−→
P 0 =

←−
P 0

Equations (10) and (11) do imply that the permutative binary N -step Markov chains are isotropic.

−→
P (1|T )P (T ) =

←−
P (1|T )P (T ) = 1.

T
X

T
X

4

(9)

(10)

(11)

5

(12)

(13)

(14)

2. Two-sided probability functions of permutative Markov chains

As was mentioned above (see Eq. (5)), every Markov chain can be regarded as two-sided one. Below we examine
the properties of the two-sided conditional probability function of the Markov chains with one-sided conditional
probability functions possessing the property of permutability. An essential point is that this property does not
provide the permutability of the two-sided conditional probability function. To demonstrate this fact we will show
that in the general case the two-sided conditional probability function changes its value when two neighboring symbols
1 and 0 are transposed.

i , T +

i ) takes on the same value for two variants of the word T −

Let us prove this statement by contradiction and suppose that the two-sided conditional probability function
P (ai = 1|T −
i only diﬀer
in the values of the neighboring symbols aj and aj+1: aj = 1, aj+1 = 0 in the ﬁrst variant and aj = 0, aj+1 = 1 in
the second one. Consider the structure of Eq. (5). Taking into account the permutability of the Markov conditional
probability function one can see that N factors in all products in Eq. (5) coincide for two symbols sets under study.
In the general case, only one factor, PN (aj+N +1|T −
j+N +1), changes its value. If aj+N +1 = 1, the coincidence of the
values of P (ai = 1|T −
i ) for two sets of symbols under the consideration yields the following relation between the
values of one-sided conditional probability function:

i . These variants of the word T −

i , T +

for k = aj+2 + aj+3 . . . + aj+N , 1 6 k 6 N − 1. In the opposite case, at aj+N +1 = 0, the similar requirement is

−→
P 2

k =

−→
P k+1

−→
P k−1

(1 −

−→
P k)2 = (1 −

−→
P k+1)(1 −

−→
P k−1).

These two relations are compatible for the non-correlated chain only where
the case of one-step binary Markov chain that always has permutative two-sided conditional probability function.

−→
P k is k-independent. The exception is

Thus, any correlated multi-step permutative Markov chain possesses the non-permutative two-sided probability

function.

C. Additive weakly correlated Markov chains

The third class of the isotropic sequences represents the additive weakly correlated Markov chains. For these chains,

we suppose that the memory function is small,

|F (r)| ≪ 1.

N

r=1
X

Their asymptotical isotropy is proved in Subsection III C 1.

Every function of N variables, f (a1, . . . , aN ), satisfying the evident restriction, 0 6 f (a1, . . . , aN ) 6 1, can be
thought of the conditional probability function P (ai = 1|ai−N , . . . , ai−2, ai−1) of some Markov chain. Yet not every
function of 2N variables, even if restricted by the similar condition, is the conditional probability function of some two-
sided chain. It follows from Eq. (5), that an arbitrary binary Markov chain is determined by 2N parameters, i.e. the
number of all possible sets of arguments in the conditional probability function. Hence, a two-sided chain equivalent to
this Markov one, is also determined by 2N parameters. Nevertheless, the two-sided conditional probability function
P (ai = 1|T −
i , T +
i , and
following, T +
i , N -words. So, not every function of 2N arguments can play a role of some two-sided conditional
probability function. The example is an additive two-sided chain with small memory function G(r) in Eq. (4). In this
case, G(r) has to be asymptotically even. This fact is proven in the Subsection III C 2.

i ) is formally governed by 22N parameters, i.e.

the number of all possible previous, T −

1.

Isotropy of additive weakly correlated Markov chains

In order to ﬁnd the conditional probability function, P (ai = 1|T −

i ), of the weakly correlated additive Markov
chain one has to substitute the probability Eq. (2) into Eq. (5), and retain the terms of the zeroth and ﬁrst orders in
F (r). The obtained two-sided conditional probability function takes the form of Eq. (4) with even memory function:
G(r) = G(−r) = F (r). So, the additive weakly correlated Markov chains are asymptotically isotropic.

i , T +

6

(16)

(17)

(18)

(19)

2. Restriction on the class of the memory functions of weakly correlated additive two-sided chains

In this subsection, we show that the weakly correlated additive two-sided chain is asymptotically isotropic, i.e. the
two-sided memory function G(r) is necessarily even. To this end we consider arbitrary two-sided additive chain and
ﬁnd its one-sided conditional probability function, P (ai = 1|T −
i ). We will prove that this function is reduced to the
additive form, Eq. (2), with the memory function F (r) = G(−r), and, therefore, the chain under consideration is
asymptotically isotropic.

Let us examine the additive two-sided chain (not obligatory isotropic) and ﬁnd its one-sided conditional probability
function. In the general case the problem is reduced to solving the set of 2N non-linear equations, Eq. (5), written
for diﬀerent sequences of symbols in word T −
i . We consider weakly correlated additive two-sided chain subjected to

the restriction

|G(−r)| + |G(r)|

≪ 1. Its one-sided conditional probability function can be presented in more

(cid:1)
P (ai|T −

¯a + ϕ(T −
i )
i ) = 1 − P (ai = 0|T −
i ) to be determined. The evident equation P (ai = 1|T −
(cid:1)

with function ϕ(T −
i ) is fulﬁlled for
Eq. (15). If G(r) tends to zero, the probability P (ai = 1|T −
i ) → 0.
Now, substituting the conditional probability function in the form of Eq. (15) into Eq. (5) and retaining only terms of
the zeroth and ﬁrst orders in ϕ(T −
i ), we obtain the approximate expression for the two-sided conditional probability
function:

i ) goes to ¯a and function ϕ tends to zero: ϕ(T −

i ) = 1 − ai + (2ai − 1)

(15)

(cid:0)

,

N

r=1
P

(cid:0)

convenient form:

P (ai = 1|T −

i , T +

i ) ≃ ¯a

1 + (1 − ¯a)

ψ(ai+r)ϕ(T −

i+r) −

ψ(ai+r)ϕ(T −

i+r)

.

N

(cid:16)

r=0
X
ai=1

N

r=0
X
ai=0

(cid:17)(cid:17)

Here we introduce a new function ψ,

The two-sided conditional probability function of the chain under consideration is given by Eq. (4). So, ﬁnally, we
obtain

(cid:16)

N

r=0
X
ai=1

N −1

r=0
X
ai=1

ψ(ai) =

ai − ¯a
¯a(1 − ¯a)

=

1/(¯a − 1), ai = 0
ai = 1.
1/¯a,

(

ψ(ai+r)ϕ(T −

i+r) −

ψ(ai+r)ϕ(T −

i+r)

≃

1
¯a(1 − ¯a)

N

r=1
X

(cid:0)

G(r)(ai+r − ¯a) + G(−r)(ai−r − ¯a)

.

(cid:1)

ϕ(T −

i+N )

ai=1 − ϕ(T −

i+N )

ai=0 ≃ G(N ).

ψ(ai+r)ϕ(T −

i+r) −

ψ(ai+r)ϕ(T −

i+r)

N

r=0
X
ai=0

(cid:12)
(cid:12)
N −1

r=0
X
ai=0

Calculating the diﬀerence between two expressions presented by Eq. (18) written for ai+N = 1 and ai+N = 0, we get

Substitution of Eq. (19) in Eq. (18) yields,

(cid:12)
(cid:12)

≃

1
¯a(1 − ¯a)

N −1

(cid:16)

r=1
X

(cid:0)

G(r)(ai+r − ¯a) + G(−r)(ai−r − ¯a)

+ G(−N )(ai−N − ¯a)

.

(20)

Now we repeat this procedure N − 1 times. At the ﬁrst repeat we calculate the diﬀerence between two expressions (20)
written for ai+N −1 = 1 and ai+N −1 = 0 and substitute the obtained result in Eq. (20), and so on. At the last repeat
we obtain,

(cid:1)

(cid:17)

ϕ(T −

i ) ≃

G(−r)(ai−r − ¯a).

(21)

N

r=1
X

So the one-sided conditional probability function is

P (ai = 1|T −

i ) ≃ ¯a +

G(−r)(ai−r − ¯a).

N

r=1
X

As it follows from previous Subsection, Markov chains with such conditional probability functions are isotropic. Thus,
we have found that the additive weakly correlated two-sided chain is asymptotically isotropic. In other words, the
memory function of additive weakly correlated chain can be only even, G(−r) = G(r).

7

(22)

IV. CONCLUSION

Thus, using the equivalence of the Markov and two-sided chains, we studied the important property of the Markov
chains, their isotropy. The results of this study are shown in the scheme. Here, A −st→ B means, that the chains
from class A, restricted by the statement st, are the members of class B. The most evident fact is that the Markov
chains, that possess symmetric two-sided conditional probability function, are isotropic. Another important class of
the isotropic Markov chains are the sequences with the permutative conditional probability functions. One of the
examples of such chains are the additive Markov chains with the step-wise memory functions, examined in details
in Refs. [19, 22]. The additive weakly correlated chains are also isotropic. Such chains play a key role in the non-
extensive thermodynamics of Ising chains of classical spins with long-range interaction, as well as in the literary texts
and sequences of nucleotides in DNA molecules.

M arkov k

T wo − sided

{{{{{{{{

permutative PN

F (r)≪1

symmetric P2,N

}{{{{{{{{

Isotropic

[1] U. Balucani, M. H. Lee, V. Tognetti, Phys. Rep. 373, 409 (2003).
[2] I. M. Sokolov, Phys. Rev. Lett. 90, 080601 (2003).
[3] A. Bunde, S. Havlin, E. Koscienly-Bunde, H.-J. Schellenhuber, Physica A 302, 255 (2001).
[4] H. N. Yang, Y.-P. Zhao, A. Chan, T.-M. Lu, and G. C. Wang, Phys. Rev. B 56, 4224 (1997).
[5] S. N. Majumdar, A. J. Bray, S. J. Cornell, and C. Sire, Phys. Rev. Lett. 77, 3704 (1996).
[6] S. Halvin, R. Selinger, M. Schwartz, H. E. Stanley, and A. Bunde, Phys. Rev. Lett. 61, 1438 (1988).
[7] R. F. Voss, Phys. Rev. Lett. 68, 3805 (1992).
[8] H. E. Stanley et. al., Physica A 224,302 (1996).
[9] S. V. Buldyrev, A. L. Goldberger, S. Havlin, R. N. Mantegna, M. E. Matsa, C.-K. Peng, M. Simons, H. E. Stanley, Phys.

Rev. E 51, 5084 (1995).

[10] A. Provata and Y. Almirantis, Physica A 247, 482 (1997).
[11] R. M. Yulmetyev, N. Emelyanova, P. H¨anggi, and F. Gafarov, A. Prohorov, Phycica A 316, 671 (2002).
[12] B. Hao, J. Qi, Mod. Phys. Lett., 17, 1 (2003).
[13] R. N. Mantegna, H. E. Stanley, Nature (London) 376, 46 (1995).
[14] Y. C. Zhang, Europhys. News, 29, 51 (1998).
[15] A. Schenkel, J. Zhang, and Y. C. Zhang, Fractals 1, 47 (1993).
[16] I. Kanter and D. A. Kessler, Phys. Rev. Lett. 74, 4559 (1995).
[17] P. Kokol, V. Podgorelec, Complexity International, 7, 1 (2000).
[18] W. Ebeling, A. Neiman, T. Poschel, arXiv:cond-mat/0204076.
[19] O. V. Usatenko, V. A. Yampol’skii, K. E. Kechedzhy and S. S. Mel’nyk, Phys. Rev. E 68, 06117 (2003).
[20] O. V. Usatenko and V. A. Yampol’skii, Phys. Rev. Lett. 90, 110601 (2003).
[21] S. S. Melnyk, O. V. Usatenko, and V. A. Yampol’skii, Physica A, 361, 405 (2006); arXiv:physics/0412169.
[22] S. S. Melnyk, O. V. Usatenko, V. A. Yampol’skii, S. S. Apostolov, and Z. A. Mayzelis, arXiv:physics/0306170.
[23] C. Tsalis, J. Stat. Phis. 52, 479 (1988).
[24] Nonextensive Statistical Mechanics and Its Applications, eds. S. Abe and Yu. Okamoto (Springer, Berlin, 2001).
[25] S. Denisov, Phys. Lett. A, 235, 447 (1997).
[26] S. S. Apostolov, and Z.A. Mayzelis, O. V. Usatenko, and V. A. Yampol’skii, arXiv:physics/0306170.

s
+
3
(cid:20)
(cid:20)
(
(
}

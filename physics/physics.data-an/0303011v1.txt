3
0
0
2
 
r
a

M
 
4
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
1
0
3
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

How many clusters? An information theoretic perspective

Susanne Still and William Bialek∗
Physics Department, Princeton University, Princeton, New Jersey 08544, USA
(Dated: Draft of October 29, 2012)

Clustering provides a common means of identifying structure in complex data, and there is renewed
interest in clustering as a tool for analysis of modern genomic data. A natural question is how
many clusters are appropriate for the description of a given system. Traditional approaches to
this problem are based either on heuristic cross–validation methods or on a framework in which
clusters of a particular shape are assumed as a model of the system.
In a statistical mechanics
approach, clustering can be seen as a trade oﬀ between energy– and entropy–like terms, with lower
temperature driving the proliferation of clusters to provide a more detailed description of the data.
We show that, in a general information theoretic framework, the ﬁnite size of a data set determines
an optimal temperature and, in the hard clustering limit, an optimal number of clusters.

Much of our intuition about the world around us in-
volves the idea of clustering: many diﬀerent acoustic
waveforms correspond to the same syllable, many diﬀer-
ent images correspond to the same object, and so on. It is
plausible that a mathematically precise notion of cluster-
ing in the space of sense data may approximate the prob-
lems solved by our brains. Clustering methods also are
used as a practical tool to evaluate structure in complex
data, one recent example is the classiﬁcation of galaxy
shapes [9]. Interest in clustering has increased recently
because of new areas of application, such as data min-
ing, image– and speech–processing and bioinformatics.
In particular, many groups have used clustering methods
to analyze the results of genome–wide expression exper-
iments, hoping to discover genes with related functions
as members of the same cluster, e.g.
[5]. A central is-
sue in these and other applications of clustering is how
many clusters provide an appropriate description of the
data. This has been recognized as one of the most diﬃ-
cult problems in cluster analysis in [3] where a review is
given on some methods that address the issue.

The goal of clustering is to group data in a meaning-
ful way. For this, a criterion has to be deﬁned with re-
spect to which the data are clustered. Many clustering
methods optimize some heuristic, intuitively reasonable
criterion (agglomerative methods such as [20], iterative
relocation methods, such as k-means [10, 11]). More re-
cently, physically inspired criteria have been introduced
[2, 8]. The number of clusters often is found by similarly
[17]. In contrast, probabilistic
heuristic means, see e.g.
mixture models assume that the data can be described by
a mixture of k multivariate distributions with some pa-
rameters that determine their shape. Now the problem
of ﬁnding the number of clusters is a statistical model
choice problem. There is a trading between complex-
ity of the model and goodness of ﬁt. One approach to
model selection is to compute the total probability that
models with Nc clusters can give rise to the data, and
then one ﬁnds that phase space factors associated with

∗Electronic address: susanna,wbialek@princeton.edu

the integration over model parameters serve to discrim-
inate against more complex models [1]. This Bayesian
approach has been used to determine the number of clus-
ters [6]. Cross–validation [16] and related methods also
have been used to estimate the prediction error and to
ﬁnd the number of clusters that minimizes this error, e.g.
[15]. Alternatively, the number of clusters can be derived
from a bound on the probability of a large deviation be-
tween the error made on the training data and the ex-
pected error [4]. Unfortunately, model based clustering
lacks generality, and it would be attractive to ﬁnd the ap-
propriate number of clusters without having to assume a
family of models that describe the underlying probability
distribution.

From an information theoretical point of view, cluster-
ing is most fundamentally a strategy for lossy data com-
pression: the data are compressed as much as possible
while a distortion function is minimized [14]. The choice
of the distortion function provides an implicit distinction
between relevant and irrelevant information in the raw
data. The notion of relevance was deﬁned explicitly in
[18] as the information which the data provide about an
auxiliary variable. In this article, we show that the re-
sulting formulation of the clustering problem [18] leads
naturally to a principled method of ﬁnding the number
of clusters consistent with a ﬁnite data set.

Rate distortion theory and the Information Bottleneck
Method.
If data x ∈ X are chosen from a probability
distribution P (x), then a complete description of a single
data point requires an average code length equal to the
entropy of the distribution, S(x) = −
x P (x) log2 P (x)
bits. On the other hand, if we assign points to clus-
ters c ∈ {1, 2, · · · , Nc}, then we need at most log2 Nc
bits. For Nc << |X| we have log2 Nc << S(x), and
our intuition is that many problems will allow substan-
tial compression at little cost if we assign each x to a
cluster and approximate x by a representative xc. Rate
distortion theory formalizes the cost of approximating x
by xc as the expected value of some distortion function,
d(x, xc) [14]. This distortion measure can, but need not
be a metric. Lossy compression means assigning the data
to clusters such that the mutual information I(c; x) =

P

xc P (c|x)P (x) log2

P (c|x)
P (c)

pected distortion hd(x, xc)i =
(cid:16)
(cid:17)
P
ﬁxed. This leads to the variational problem

P
[hd(x, xc)i + T I(c; x)] .

min
P (c|x)

is minimized while the ex-
xc P (c|x)P (x)d(x, xc) is

The (formal) solution is a Boltzmann distribution

P (c|x) =

exp

−

d(x, xc)

,

(1)

P (c)
Z(x; T )

1
T

(cid:18)

(cid:19)

with the distortion playing the role of energy, and
c P (c) exp (−d(x, xc)/T )
the normalization Z(x, T ) =
playing the role of a partition function [12]. The repre-
sentatives, xc, often simply called cluster centers, are de-
termined by the condition that all of the “forces” within
each cluster balance for a test point located at the cluster
center,

P

P (x|c)

d(x, xc) = 0.

∂
∂xc

x
X

The Lagrange parameter T regulates the trade oﬀ be-
tween the detail we keep and the bit cost we are willing
to pay; in analogy with statistical mechanics, T often
is referred to as the temperature [12]. As we lower T
there are phase transitions among solutions with diﬀer-
ent numbers of clusters, and if we follow these transitions
we can trace out a curve of the minimum hdi vs. I(c; x),
which is called the rate–distortion curve and is analo-
gous to plotting energy vs. (negative) entropy with tem-
perature varying parametrically along the curve. Cru-
cially, there is no optimal temperature which provides
the unique best clustering, and thus there is no optimal
number of clusters: more clusters always provide a more
detailed description of the original data and hence al-
low us to achieve smaller average values of the distortion
d(x, xc), while the cost of the encoding increases.

The formulation above assumes that we know the prob-
ability distribution underlying the data. In practice we
have access only to a ﬁnite number of samples drawn from
this distribution, and so there are errors in the estimation
of P (x). Random errors in P (x) produce a systematic
error in the computation of the information cost I(c; x)
but there is no systematic error in the computation of
hdi. Furthermore, the Boltzmann distribution in Eq. (1)
does not depend explicitly on the estimate of P (x) and so
there is no obvious quenched disorder that might cause
glassiness. The result is that, even with a ﬁnite data
set, more clusters will provide a description that allows
for smaller average distortion. We might mis–estimate
the bit cost of these extra clusters, but there remains a
monotonic trading of bits for distortion.

The distortion function implicitly selects the features
that are relevant for the compression. However, for many
problems, we know explicitly what it is that we want to
keep information about while compressing the data, but
one can not always construct the distortion function that

2

selects for these relevant features. Examples are words in
the case of speech and objects in the case of images. In
the Information Bottleneck method [18] the relevant in-
formation in the data is deﬁned as information about an-
other variable, y ∈ Y . The data is compressed such that
the relevant information is maximally preserved. That
leads to the optimization problem

max
P (c|x)

[I(c; y) − T I(c; x)] .

(2)

One obtains a solution similar to Eq. (1) in which the
Kullback–Leibler divergence

DKL[P (y|x)kP (y|c)] =

P (y|x) log2

y
X
emerges in the place of the distortion function [18]. When
we plot the maximum I(c; y) as a function of I(c; x), we
obtain a curve similar to the Rate Distortion Curve with

(cid:19)

(cid:18)

P (y|x)
P (y|c)

δI(c; y)
δI(c; x)

= T.

(3)

How many clusters? In contrast to Rate Distortion
theory, in the Information Bottleneck formulation both
terms in the objective function (2) involve systematic
errors from ﬁnite sampling of the underlying distribu-
tion P (x, y). In particular, with simple frequentist esti-
mates of P (x, y), we overestimate the relevant informa-
tion I(c; y). This opens the possibility of a solution to
our problem: if we could correct for this error, we might
ﬁnd an optimal temperature and, in the deterministic
(T → 0), hard clustering limit, a number of clusters for
which relevant information is kept maximally.

When we compute the solution to the compression
problem expressed in Eq. (2), we really want to evaluate
the functional (2) at the true distribution P (x, y), but in
practice we have to use our estimate ˆP (x, y). The idea
here is to use perturbation theory to compute the sys-
tematic error in I(c; y). We assume that P (x) is known
and then there is no bias in I(c; x) [21].

We assume for simplicity that y is discrete. The fre-
quentist estimate ˆP (y|x) = n(y|x)/N , where n(y|x) is
the number of counts of y, when x is given, converges to
the true distribution in the limit of large data set size
N → ∞. However, for ﬁnite N , the estimated distri-
bution will diﬀer from the true distribution and there is
a regime in which N is large enough such that we can
approximate (compare [19]) ˆP (y|x) = P (y|x) + δP (y|x),
where δP (y|x) is some small perturbation; and its av-
erage over all possible realizations of the data is zero,
hδP (y|x)i = 0. Expansion of I(c; y) around P (y|x) leads
to a systematic error:

∞

(−1)n
n(n − 1)

(P (x))n
(P (y))n−1

n=2
X
×h(δP (y|x))ni/ ln(2).

xy
X

 

c
X

(P (c|x))n
(P (c|y))n−1 − 1

!

(4)

Note that the term with n = 1 vanishes, because
hδP (y|x)i = 0. For counting statistics, if the number
of bins, Ky, used to estimate P (y|x) is suﬃciently large
and thus P (y|x) is a small number ∀ (x, y), we can ap-
proximate

h(δP (y|x))ni ≃

P (y|x)
N n−1(P (x))n−1

(5)

and the error becomes

N
ln(2)

−

N
ln(2)

xyc
X

y
X

P (x, y)P (c, y)

(P (y))2

∞

n=2
X

∞

(−1)n
n(n − 1)

1
N

P (c|x)
P (c, y)

n=2
X
(−1)n
n(n − 1)

(cid:18)
1
P (y)

n

.

(cid:19)

1
N

(cid:18)

n

(cid:19)

The error converges if P (x, y) > 1/N ∀(x, y), because
x P (c|x) ≥ P (c|x) (the ﬁrst inequal-
then N P (c, y) >
x P (c|x)P (x, y) [18], while the last
ity uses P (c, y) =
P
one follows by deﬁnition), and N P (y) > 1. For x < 1 we
can use the equality

P

(−1)n
n(n − 1)

∞

n=2
X

xn = (x + 1) ln (x + 1) − x.

Altogether, we have an error of

P (x, y)

[P (c|x) + N P (c, y)]log2

1 +

xy
X

c
X

where Λ does not depend on P (c|x)

P (c|x)
N P (c, y)

−Λ,

(6)

(cid:19)

1
N P (y)

.
(cid:19)

(cid:18)

(cid:18)

Λ =

P (y) [N P (y) + 1] log2

1 +

y
X

We should subtract this error from the objective function
(2) and recompute the distribution that maximizes the
corrected objective function. This is not straightforward,
but instead we can approximate the error with the term
of leading order in δP (y|x). This term is (ignoring the
constant Λ) the n = 2 term in eq. (4) (together with the
approximation (5)):

1
2 ln(2)N

x(P (c|x))2P (x|y)
x P (c|x)P (x|y)

,

(7)

yc P
X

P

x P (c|x)P (x|y) [18]. This
where we used P (c|y) =
term is bounded from above by KyNc/2 ln(2)N , which is
P
the value it assumes in the deterministic limit (T → 0),
in which assignments P (c|x) are either 1 or 0 and thus
(P (c|x))2 = P (c|x). The term is bounded from below
by 2I(c;x)/2 ln(2)N [22], and hence the corrected infor-
mation curve is bounded from above by I corr
UB (c; y) =
I(c; y)− 2I(c;x)/2 ln(2)N with slope T − 2I(c;x)/2N (using
eq. (3)), and a maximum at T ∗ = 2I(c;x)/2N .

If the hard clustering assigns equal numbers of data
to each cluster the upper bound on the error can be

3

)
y
;
x
(
I
 
/
 
)
y
;
c
(
I

1

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84
3

4

5

6

7

8

9

10

N
c

FIG. 1: Synthetic data with P (y|x) = N (0, α(x)) with 6 pos-
sible values for α. Dashed lines: maximum value of I(c; y)
(scaled by I(x; y)), for T → 0, assuming that the observed dis-
tribution is the true distribution. Solid lines: maximum value
of I(c; y), corrected by the estimated error (and scaled by
I(x; y)) – all curves are maximal at Nc = 6. Ny/Ky equals 5
(triangles), 10 (circles), 15 (stars) and 50 (squares). Nx = 60
and Ky = 100 for all curves.

rewritten as Ky2I(c;x)/2 ln(2)N . Thus, the error is
proportional to 2I(c;x), with proportionality constant
γ/2 ln(2)N , where 1 < γ < Ky, and therefore the cor-
rected information curve must have a maximum at some
critical temperature the value of which is conﬁned by the
bounds on γ. For deterministic assignments, in general,
the information we gain by adding another cluster sat-
urates for large Nc, and it is reasonable to assume that
it grows sub-linearly in the number of clusters; then the
lower bound on I corr(c; y) has a maximum (or at least
a plateau), ensuring that I corr(c; y) has a maximum (or
plateau). We have veriﬁed this numerically for the ex-
amples given below.

To ask for the number of clusters that are consistent
with the uncertainty in our estimation of P (y|x) makes
sense only for deterministic assignments. This number,
N ∗
c , is optimal in the sense that using more clusters, we
would “overﬁt” the data, and although in principle we
could always use fewer clusters, this comes at the cost of
keeping less I(c; y). From the above discussion, we know
the leading order error term in the deterministic limit.
To ﬁnd N ∗
c , we ﬁx the number of clusters and compute
the corrected relevant information in the limit T → 0:

I corr
T →0(c; y) = I(c; y) −

Ky
2 ln(2)N

Nc.

While I(c; y) increases monotonically with Nc, we expect
I corr
T →0(c; y) to have a maximum (or at least a plateau) at
N ∗
c .
Numeric results. For Fig. 1, we created synthetic
data with P (x) = 1/Nx and P (y|x) = N (0, α(x)) where
α(x) ∈ A, and |A| = NA, with P (α) = 1/NA; and
Nx/NA is constant. We estimate ˆP (y|x), using Ky = 100

0
10

α
 
d

−1

10

0
10

95%

1
10

N
y

 / K
y

FIG. 2: Synthetic data set of size N = NxNy (Nx = 20) with
P (y|x) = N (α(x), 0) with either 2, 5 or 10 possible values
for α, spaced dα apart. We indicate values of dα and the
resolution Ny/Ky (Ky = 100) at which the correct number
of clusters is resolved: for 2, 5 and 10 clusters (squares); only
for 2 and 5 clusters (stars); only for 2 clusters (circles). The
classiﬁcation error on the test set is 0 for all points except for
the one that is labeled with 95% correct.

bins. We compare how I(c; y) and I corr
T →0(c; y) behave
as a function of the number of clusters. The number
of observations of y, given x, is Ny = N/Nx. For a
large range of resolutions Ny/Ky, I corr
T →0(c; y) has a max-
imum at N ∗
c = NA. As Ny/Ky becomes very large,
I corr
T →0(c; y) approaches I(c; y), as expected. However, for
small enough Ny/Ky we expect to lose the ability to

4

resolve all the clusters in the data set. To investigate
in which regime our approximation works as we on one
hand use less data and on the other hand make the classi-
ﬁcation problem intrinsically harder, we create synthetic
data with P (y|x) = N (α(x), 0) with equidistant means
α, which are dα apart. The problem becomes harder
as dα becomes smaller. Fig. 2 shows the values of dα
at which we ﬁnd the correct number of clusters, as a
function of Ny/Ky. Results for 2, 5 and 10 clusters are
summarized. For small sample sizes, the correct number
of clusters is resolved only if the clusters are well sepa-
rated, but as we accumulate more data, we can recover
the correct number of classes for more and more overlap-
ping clusters. Since we only need to solve the problem
once for each number of clusters, our method takes less
computational time than cross–validation type methods.
Furthermore, our method detects when only one cluster
is present, a case in which many methods fail [7]. We ver-
iﬁed this for data drawn from one Gaussian distribution
and for data drawn from the uniform distribution.

In summary, we have kept the generality of the infor-
mation theoretic formulation while answering the ques-
tion posed in the title. We have shown, using pertur-
bation theory, that the Information Bottleneck method
allows us to ﬁnd the optimal number of clusters through
correction of the bias in the relevant information which
arises due to the ﬁnite size of the data set.

We thank M. Berciu, L. Bottou and N. Slonim for help-
ful discussions. S. Still was supported in part by the
German Research Foundation (DFG), grant no. Sti197.

[1] V. Balasubramanian, Neural Comp. 9 (1997) 349.
[2] M. Blatt, S. Wiseman and E. Domany, Phys. Rev. Lett.

(1967) University of California Press, pp. 281-297 (Vol.
I)

76 (1996) 3251-3254, cond-mat/9702072

[12] K. Rose, E. Gurewitz and G. C. Fox, Phys. Rev. Lett. 65

[3] H.-H. Bock in Clustering and Classiﬁcation Eds.: P. Ara-
bie, L.J. Hubert and G. De Soete (1996) World Scientiﬁc
pp. 378-453.

[4] J. M. Buhmann and M. Held in Adv. Neural Inf. Proc.
Sys. (NIPS) 12 Eds.: S. A. Solla, T. K. Leen and K.-R.
M¨uller (2000) MIT Press

[5] M. Eisen, P. T. Spellman, P. O. Brown and D. Botstein

Proc. Nat. Acad. Sci. (PNAS) 95 (1998) 14863.

[6] C. Fraley and A. Raftery, J. Am. Stat. Assoc. 97 (2002)

611.

[7] A. D. Gordon, Classiﬁcation, (1999) Chapmann and

Hall/CRC Press, London

[8] D. Horn and A. Gottlieb, Phys. Rev. Lett. 88 (2002)

018702, extended version: physics/0107063

[9] S. Kieshner, I. Cadez, P. Smyth and C. Kamath in Adv.
Neural Inf. Proc. Sys. (NIPS) 15 Eds.: S. Becker, S.
Thrun and K. Obermayer (2003) MIT Press

[10] S. Lloyd, Technical Report (1957) Bell Laboratories.

IEEE Trans. Inf. Th., vol. IT-28 (1982) 129.

[11] J. MacQueen in Proc. 5th Berkeley Symp. Math. Statis-
tics and Probability Eds.: L.M.L Cam and J. Neyman

(1990) 945.

[13] G. Schwarz, Ann. Stat. 6 (1978) 461.
[14] C. Shannon and W. Weaver, The Mathematical Theory
of Communication (1963) University of Illinois Press

[15] P. Smyth, Statistics and Computing 10 (2000) 63.
[16] M. Stone, J. R. Stat. Soc. 36 (1974) 111.
[17] R. Tibshirani, G. Walther and T. Hastie, J. R. Stat. Soc.

B 63 (2001) 411.

[18] N. Tishby, F. Pereira and W. Bialek in Proc. 37th Annual
Allerton Conf. Eds.: B. Hajek and R. S. Sreenivas (1999)
University of Illinois, physics/0004057

[19] A. Treves and S. Panzeri, Neural Comp. 7 (1995) 399.
[20] J. H. Ward, J. Am. Stat. Assoc. 58 (1963) 236.
[21] In many practical problems x is just an index to the
identity of samples, and hence P (x) is constant, and the
real challenge is to estimate P (y|x).

[22] Proof:

(P (c|x))2P (x|y)
P (c|y)

xyc

xc P (x, c) P (c|x)

>
Note, however, that this is not a tight bound.

P (c) =

P

P

P

P

xcP (x, c)P (c|x)

=
xc P (x, c)2log2(

y
P (c)
P (c|x)
P (c) ) ≥ 2I(c;x)

P

P (y|x)
P (y|c)


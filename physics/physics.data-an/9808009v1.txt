8
9
9
1
 
g
u
A
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
0
0
8
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

ARE WE CRUISING A HYPOTHESIS SPACE?

C. C. RODRIGUEZ

Department of Mathematics and Statistics
University at Albany, SUNY
Albany NY 12222, USA
carlos@math.albany.edu

Abstract.

This paper is about Information Geometry, a relatively new subject within
mathematical statistics that attempts to study the problem of inference by using
tools from modern diﬀerential geometry. This paper provides an overview of some
of the achievements and possible future applications of this subject to physics.

Key words: Information Geometry, Entropic Priors, Riemannian Geometry, En-
tropic Connections, Information Metric

1. Introduction

It is not surprising that geometry should play a fundamental role in the theory of
inference. The very idea of what constitutes a good model cannot be stated clearly
without reference to geometric concepts such as size and form of the model as well
as distance between probability distributions. Recall that a statistical model (hy-
pothesis space) is a collection of probability distributions for the data. Therefore,
a good model should be big enough to include a close approximation to the true
distribution of the data, but small enough to facilitate the task of identifying this
approximation. As Willy said: as simple as possible but not too simple.

But there is more. Regular statistical models have a natural Riemannian struc-
ture. Parameterizations correspond to choices of coordinate systems and the Fisher
information matrix in a given parameterization provides the metric in the corre-
sponding coordinate system [1,2]. By thinking of statistical models as manifolds,
hypothesis spaces become places and it doesn’t take much to imagine some of these
places as models for the only physical place there is out there, namely: spacetime.
In section 2 of the paper we apply the techniques of information geometry to show
that the space of radially symmetric distributions admits a foliation into pseudo-
spheres of increasing radius. If we think of a radially symmetric distribution as
describing an uncertain physical position we discover a hypothesis space that, in
many ways resembles an expanding spacetime. An isotropic, homogeneous space

2

C. C. RODRIGUEZ

with pseudo-spherical symmetries and with time increasing with decreasing curva-
ture radius. This admittedly simple toy model of reality already suggests a number
of truly remarkable consequences for the nature of spacetime. Here are three of
them:

1. The appearance of time is a consequence of uncertainty.
2. Space is inﬁnite dimensional and only on the average appears as four dimen-

3. Spin is a property of space and not of a particle so that all truly fundamental

sional.

particles must have spin.

I must emphasize that, at the time of writing, there is no direct experimental
evidence in favor of any of the above statements. Nevertheless there is indirect
evidence that they should not be too quickly dismissed as nonsense.

With respect to the ﬁrst statement. Recall that the appearance of the axis of
time, in standard general relativity, is a consequence of specifying an initial and
a ﬁnal 3-geometry on two spacelike hypersurfaces plus evolution according to the
ﬁeld equation [3]. Time is therefore a consequence of 3-space geometry and the ﬁeld
equation of general relativity, which in turn seems to be of an statistical nature
(see [4] and section 4 below). These, I believe, are facts that support, at least in
spirit, the ﬁrst claim above.

There is absolutely no evidence that space has inﬁnitely many dimensions, but
had this be true, it would explain why we observe only four of them. It also seems
a priori desirable to have a model that produces observed space as a macroscopic
object not unlike pressure or temperature.

With respect to the third statement. Hestenes[5] shows that many of the rules
for manipulating spin have nothing to do with quantum mechanics but are just
general expressions for the geometry of space. It is also worth noticing that the
standard model allows the existence of elementary particles without spin but these
have not yet been observed.

But there is still more. Think of the diﬀerent roles that the concepts of, en-
tropy, curvature, and, local hyperbolicity, play in statistics and in physics and you
will realize that the link is a useful bridge for transporting ideas from physics to
statistics and vice-versa. The following sections (3, 4, 5) of this paper do exactly
that. That is, they examine the meaning of each of these concepts (entropy, cur-
vature and local hyperbolicity) keeping the proposed link between inference and
physics in mind.

The link between information geometry and general relativity promises big re-
wards for both, statistical inference and physics. For example, statisticians may
look at the ﬁeld equations of general relativity as a procedure for generating sta-
tistical models from prior information encoded in the distribution of energy and
matter ﬁelds. On the other hand, physicists may see information geometry as a
possible language for the elusive theory of quantum gravity since it is a language
already made out of the right ingredients: uncertainty and diﬀerential geometry.

INFORMATION GEOMETRY

2. The Hypothesis Space of Radially Symmetric Distributions

3 be the collection of all radially symmetric distributions of three dimensional

Let
euclidean space. The probability of an inﬁnitesimal region around the point x
3
IR

R
, of volume d3x, that is assigned by a general element of

3 is given by,

∈

3
P (d

ψ, θ, σ) =
x
|

1
σ3

ψ

2

θ

x

−
σ

R

2

3
d

x

(cid:12)
(cid:12)
(cid:12)
where, θ
is a location parameter, σ > 0 is a scale parameter and ψ is an
(cid:12)
(cid:12)
arbitrary diﬀerentiable function of r2 > 0 satisfying the normalization condition:

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

3
IR

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈

∞

0

Z

r2

2dr =
ψ(r2)
|
|

1
4π

′

Equation (2) assures that the probability assigned to the whole space by (1) is
, must also decrease to 0 suﬃciently fast so that
in fact 1. The derivative, ψ
the integrals (3) exist. Since ψ is an inﬁnite dimensional parameter,
3 is also
an inﬁnite dimensional manifold but the space ,
3 (ψ), of radially symmetric
distributions for a given function ψ is a four dimensional submanifold of
3
R
3 (ψ) is given by the
parameterized by (θ0, θ1, θ2, θ3) = (σ, θ). The metric in
4

R
4 Fisher information matrix (see [6] p. 63) with entries:

R

R

×

where µ, ν = 0, . . . , 3, the function f is the square root of the density given in (1)
i.e.,

and ∂µ denotes the derivative with respect to θµ. Let us separate the computation
of the metric tensor terms into three parts. The entries gij, the entries g0i for
i, j = 1, 2, 3 and the element g00. Replacing (4) into (3), doing the change of
variables x = θ + σy and using the fact that

gµν = 4

(∂µf )(∂ν f )d3x

Z

θ, σ) = σ
f (x
|

−3/2ψ

x

θ

−
σ

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

!

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

∂iψ(y2) =

2yiψ

′

(y2)/σ

−

gij =

yiyj

16
σ2

Z

2

d3y

′

ψ

(y2)
(cid:17)

(cid:16)

gij = 0 for i

= j

gii =

64π
3σ2

Z

r4

′

ψ
|

2dr
(r2)
|

where y2 =
integration in spherical coordinates we obtain,

2 is the Cliﬀord product of the vector y by itself. Carrying out the
|

y
|

3

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

we get,

and,

6
4

C. C. RODRIGUEZ

The derivative with respect to σ of the function given in (4) is,

and therefore from this and (5) we have,

∂0f =

σ−5/2
2

−

3ψ + 4y2ψ

′

h

i

(9)

(10)

(11)

(12)

(13)

(14)

g0i = 4

(∂0f )(∂if )d3x

[3ψ + 4y2ψ

′

]yiψ

′

d3y = 0

∝

Z

Z
where the value of 0 for the last integral follows by performing the integration in
spherical coordinates or simply by symmetry, after noticing that the integrand is
odd. Finally from (9) we get,

Expanding the square and integrating the cross term by parts to show that,

g00 =

2

′

2

[3ψ(r

) + 4r

ψ

(r

2

2

2
)]

r

dr

4π
σ2

Z

r4ψ(r2)ψ

(r2)dr =

′

3
4

(

1
4π

)

−

Z

′

′

where we took u = ψr3/2 and v
used (2). We obtain,

= 2rψ

for the integration by parts and we have

g00 =

4π
σ2

9
−
4π

(cid:20)

+ 16

Z

r6

′

ψ
|

2dr
(r2)
|

(cid:21)

The full matrix tensor looks like this,

(g) =

1
σ2

J (ψ)

0

0

0










K (ψ)

0

0

0

0

0

0

K (ψ)

0

0

0

K (ψ)










where J(ψ) and K(ψ) are just short hand notations for the factors of 1
σ2 in (13)
and (8). These functions are always positive and they depend only on ψ. Straight
forward calculations, best done with a symbolic manipulator like MAPLE, show
that a space with this metric has constant negative scalar curvature given by
1/J(ψ). It follows that for a ﬁx value of the function ψ the hypothesis space of
−
3 (ψ) is the pseudo-sphere of radius J 1/2(ψ).
radially symmetric distributions
We have therefore shown that the space of radially symmetric distributions has a
foliation (i.e. a partition of submanifolds) of pseudo-spheres of increasing radius.
This is a mathematical theorem. There can be nothing controversial about it.
What it may be disputed, however, is my belief that the hypothesis space of radially
symmetric distributions may be telling us something new about the nature of real
physical spacetime. What I ﬁnd interesting is the fact that if we think of position
subject to radially symmetric uncertainty then the mathematical object describing
the positions (i.e. the space of its distributions) has all the symmetries of space

R

INFORMATION GEOMETRY

5

(15)

(16)

(17)

(18)

plus time. It seems that time, or something like time, pops out automatically when
we have uncertain positions. I like to state this hypothesis with the phrase:

there is no time, only uncertainty

2.1. UNCERTAIN SPINNING SPACE?

The hypothesis space of radially symmetric distributions is the space of distribu-
of the form,
tions for a random vector y

3
IR

∈

y = x + ǫ

3

3

∈

IR

is a non random location vector, and ǫ

is a random vector with
where x
a distribution radially symmetric about the origin and with standard deviation
σ > 0 in all directions. It turns out that exactly the same hypothesis space is
obtained if instead of (15) we use,

IR

∈

y = x + i ǫ

3
where i is the constant unit pseudo scalar of the Cliﬀord algebra of IR
. The pseudo
scalar i has unit magnitude, commutes with all the elements of the algebra, squares
[7]. By taking expectations
to
with the probability measure indexed by (x, σ, ψ) we obtain that,

3
1 and it represents the oriented unit volume of IR

−

and,

E(y

x, σ, ψ) = x
|

E(y2

x, σ, ψ) = x2
|

−

σ2

Equation (18) shows that, even though the space of radially symmetric distri-
butions is inﬁnite dimensional, on the average the intervals look like the usual
spacetime intervals.

We may think of y in (16) as encoding a position in 3-space together with an
uncertain degree of orientation given by the bivector part of y, i.e. iǫ. In other words
.
we assign to the point x and intrinsic orientation of direction ˆǫ and magnitude
|
In this model the uncertainty is not directly about the location x (as in (15)) but
about its postulated degree of orientation (or spinning).

ǫ
|

3. Entropy and Ignorance

The notion of statistical entropy is not only related to the corresponding notion
in physics it is exactly the same thing as demonstrated long ago by Jaynes [8].
Entropy appears indisputable as the central quantity of information geometry. In
particular, from the Kullback number (relative entropy) between two distributions
in the model we obtain the metric, the volume element, a large class of connections
[2], and a notion of ignorance within the model given by the so called entropic priors
[9]. In this section I present a simple argument, inspired by the work of Zellner on
MDIP priors [10], showing that entropic priors are the statistical representation of
the vacuum of information in a given hypothesis space.

6

C. C. RODRIGUEZ

{

H

Let

θ) : θ
f (x
|

=
be a general regular hypothesis space of probability
density functions f (x
θ) for a vector of observations x conditional on a vector of
|
parameters θ = (θµ). Let us denote by f (x, θ), the joint density of x and θ and by
f (x) and π(θ) the marginal density of x and the prior on θ respectively. We have,

Θ

∈

}

θ)π(θ)
f (x, θ) = f (x
|

Since

is regular, the Fisher information matrix,

H

gµν (θ) = 4

(∂µf 1/2)(∂ν f 1/2)dx

(19)

(20)

Z

∝

p

exists and it is continuous and positive deﬁnite (thus non singular) at every θ. As
in (3), ∂µ denotes the partial derivative with respect to θµ. The space
with
the metric g = (gµν) given in (20) forms a Riemannian manifold. Therefore, the
invariant element of volume is given by,

H

η(dθ)

det g(θ)dθ

(21)

H

This is in fact a diﬀerential form [11, p. 166] that provides a notion of surface area
and it is naturally interpreted as the uniform distribution over
for the manifold
. Formula (21), known as Jeﬀeys rule, is often used as a universal method for
H
building total ignorance priors. However, (21) does not take into account the fact
that a truly ignorant prior for θ should contain as little information as possible
about the data x. The entropic prior in
demands that the joint distribution of
H
x and θ, f (x, θ), be as diﬃcult as possible to discriminate from the independent
det g(θ), where h(x) is an initial guess for f (x). That is, we are
model h(x)
looking for the prior that minimizes the Kullback number between f (x, θ) and the
independent model, or in other words, the prior that makes the joint distribution
of x and θ to have maximum entropy relative to the measure h(x)
det g(θ)dxdθ.
Thus, the entropic prior is the density π(θ) that solves the variational problem,

p

f (x, θ) log

min
π

Z

f (x, θ)

h(x)

det g(θ)

dx dθ

p

(22)

Replacing (19) into (22), simplifying, and using a lagrange multiplier, λ, for the
normalization constraint, that

π(θ)dθ = 1, we ﬁnd that π must minimize,

p

R
π(θ)I(θ : h) dθ +

π(θ) log

Z

Z

π(θ)

det g(θ)

Z

dθ + λ

π(θ) dθ

(23)

θ) and h(x), i.e.,
where, I(θ : h) denotes the Kullback number between f (x
|

p

I(θ : h) =

Z

θ) log
f (x
|

f (x
θ)
|
h(x)

dθ

(24)

The Lagrangian
Lagrange equation is then,

L

is given by the sum of the integrands in (23) and the Euler-

= I(θ : h) + log

+ 1 + λ = 0

(25)

∂
L
∂π

π(θ)
det g(θ)

p

7

(26)

(27)

(28)

(29)

(30)

from where we obtain that,

INFORMATION GEOMETRY

π(θ)

e−

∝

I(θ : h)

det g(θ)

The numerical values of the probabilities obtained with the formula (26) depend on
the basis for the logarithm used in (22). However, the basis for the logarithm that
appears in the deﬁnition of the Kullback number is arbitrary (entropy is deﬁned
only up to a proportionality constant). Thus, (26) is not just one density, but a
family of densities,

p

π(θ

α, h)
|

∝

e−

αI(θ : h)

det g(θ)

p

indexed by the parameter α > 0 and the function h. Equation (27) is the family
of entropic priors introduced in [2] and studied in more detail in [12],[9] and [13].
It was shown in [9] that the parameter α should be interpreted as the number
of virtual observations supporting h(x) as a guess for the distribution of x. Large
values of α should go with reliable guesses for h(x) but, as it was shown in [13], the
inferences are less robust. This indicates that ignorant priors should be entropic
priors with the smallest possible value for α, i.e., with,

∗
α

= inf

α > 0 :

{

αI(θ, h) η(dθ) <

e−

∞}

Z

Here is the canonical example.

3.1. EXAMPLE: THE GAUSSIANS

Consider the hypothesis space of one dimensional gaussians parameterized by the
mean µ and the standard deviation σ. When h is an arbitrary gaussian with
parameters µ0 and σ0 straight forward computations show that the entropic prior
is given by,

π(µ, σ

α, µ0, σ0) =
|

σα−2 exp

−

1
Z

α

(µ

(cid:16)

2
µ0)
−
2σ2
0

+ σ2





(cid:17)





where the normalization constant Z is deﬁned for α > 1 and is given by,

Z =

2
√π

α
2

(cid:16)

(cid:17)

α/2

Γ(

α

1

−
2

−1σ0
)

−α

Thus, in this case α∗ = 1 and the most ignorant prior is obtained by taking the
limit α
in (29) obtaining, in the limit, an improper density
proportional to 1/σ, which makes every body happy, frequentists and bayesians
alike.

1 and σ0

→ ∞

→

4. Curvature and Information

Curvature seems to be well understood only in physics, specially from the point
of view of gauge theories where the curvature form associated to a connection has

8

C. C. RODRIGUEZ

been shown to encode ﬁeld strengths for all the four fundamental forces of nature
[14]. In statistics, on the other hand, the only thing we know (so far) about the
role of curvature is that the higher the scalar curvature is at a given point of the
model, the more diﬃcult it is to do estimation at that point. This already agrees
nicely with the idea of black holes , for if in a given model there is a curvature
R0 beyond which estimation is essentially impossible then the space is partitioned
into three regions with curvatures, R < R0, R = R0 and R > R0 that correspond
to regular points, horizon points and points inside black holes. No body has found
an example of a hypothesis space with this kind of inferential black holes yet,
but no body has tried to look for one either. Before rushing into a hunt it seems
necessary to clarify what exactly it is meant by the words: estimation is essentially
impossible at a point.

I believe that one of the most promising areas for research in the ﬁeld of
information geometry is the clariﬁcation of the role of curvature in statistical
inference. If indeed physical spacetime can be best modeled as a hypothesis space
then, what is to be learned from the research on statistical curvature will have
direct implications for the nature of physical space. On the other hand, it also
seems promising to re-evaluate what is already known in physics about curvature
under the light of the proposed link with inference. Even a naive ﬁrst look will
show indications of what to expect for the role of curvature in inference. Here is
an attempt at that ﬁrst look.
From the classic statement: Mass-energy is the source of gravity and the strength

of the gravity ﬁeld is measured by the curvature of spacetime

We guess: Information is the source of the curvature of hypothesis spaces. That

is, prior information is the source of the form of the model

From: The dynamics of how mass-energy curves spacetime are controlled by the

where G is the Einstein tensor, T is the stress-energy tensor and κ is a
proportionality factor

guess: The ﬁeld equation controls the dynamics of how prior information produces

G = κT

(31)

ﬁeld equation:

models

From: The ﬁeld equation for empty space is the Euler-Lagrange equation that
characterizes the extremum of the Hilbert action, with respect to the choice of
geometry. That is it extremizes

Sg =

RdΩ,

dΩ =

det g

(32)

Z

d4x,
|

|
p

where the integral is taken over the interior of a four-dimensional region Ω,
R is the scalar curvature and g is the metric

guess1: The form of hypothesis spaces based on no prior information must satisfy

Rij −

1
2

Rgij = 0

(33)

where gij is the Fisher information matrix, Rij is the Ricci tensor and R is
the scalar curvature as above.

INFORMATION GEOMETRY

9

guess2: Given a hypothesis space with Fisher information matrix g(θ), the Ein-
stein tensor, G, i.e. the left hand side of (33), quantiﬁes the amount of prior
information locally contained in the model at each point θ.

5. Hyperbolicity

What it seems most intriguing with respect to the link between information ge-
ometry and general relativity is the role of hyperbolicity. We know from general
relativity that physical spacetimes are Riemannian manifolds which are locally
Lorentzian. That is, at each point, the space looks locally like Minkowski space.
Or, in other words, the symmetries of the tangent space at each point are those
of hyperbolic space. On the other hand, in information geometry, hyperbolicity
appears at two very basic levels. First, hyperbolicity appears connected to the
notion of regularity through the property of local asymptotic normality (LAN for
short see [6]). This is in close agreement with what happens in physics. The LAN
property says that the manifold of distributions of n independent and identically
regularly distributed observations can be locally approximated by gaussians for
large n, and since the gaussians are known to form hyperbolic spaces, the cor-
respondence with physics is perfect. Second, in statistical inference hyperbolicity
also appears mysteriously connected to entropy and Bayes’ theorem! (see my From
Euclid to Entropy[15]) and by following the link back to general relativity we ob-
tain a completely new and unexpected result: entropy and Bayes theorem are the
source of the local hyperbolicity of spacetime!. That entropy and thermodynamics
are related to general relativity may have seem outrageous in the past, but not
today. It does not seem outrageous at all when we consider that, Bekenstein found
that the entropy of a black hole is proportional to its surface area [16], when we
consider that Hawking discovered that black holes have a temperature [17] and
specially when we consider that Jacobson showed that the ﬁeld equation is like an
equation of state in thermodynamics [4].

References

1. S.-i. Amari, Diﬀerential-Geometrical Methods in Statistics, vol. 28 of Lecture Notes in

Statistics, Springer-Verlag, 1985.

2. C. Rodr´ıguez, “The metrics induced by the kullback number,” in Maximum Entropy and

Bayesian Methods, J. Skilling, ed., Kluwer Academic Publishers, 1989.

3. D. S. R.F. Baierlein and J. Wheeler, “Three-dimensional geometry as carrier of information

about time,” Phys. Rev., pp. 1864–1865, 1962.

4. T. Jacobson, “Thermodynamics of spacetime: The einstein equation of state,” tech. rep.,

xxx.lanl.gov/abs/gr-qc/9504004, 1995.

5. D. Hestenes, Space-Time Algebra, Gordon and Breach, N.Y., 1966.
6.

I. Ibragimov and R. Has’minskii, Statistical Estimation, vol. 16 of Applications of Mathe-
matics, Springer-Verlag, 1981.

7. D. Hestenes and G. Sobczyk, Cliﬀord Algebra to Geometric Calculus, D. Reidel, 1984. For
links to online resources on Cliﬀord algebra check, http://www.mrao.cam.ac.uk/˜cliﬀord/.
8. E. Jaynes, “Information theory and statistical mechanics,” Phys. Rev., 106, p. 620, 1957.

Part II; ibid, vol 108,171.

9. C. Rodr´ıguez, “Entropic priors,” tech. rep., omega.albany.edu:8008/entpriors.ps, Oct. 1991.

10

C. C. RODRIGUEZ

10. A. Zellner, “Past and reacent results on maximal data information priors,” tech. rep., H.G.B.
Alexander Reseach Foundation. Graduate Shool of Business, University of Chicago, 1995.
11. B. Dubrovin, A. Fomenko, and S. Novikov, Modern Geometry–Methods and Applications,

Part-I, vol. GTM 93 of Graduate Texts in Mathematics, Springer-Verlag, 1984.

12. C. Rodr´ıguez, “Objective bayesianism and geometry,” in Maximum Entropy and Bayesian

Methods, P. F. Foug`ere, ed., Kluwer Academic Publishers, 1990.

13. C. Rodr´ıguez, “Bayesian robustness: A new look from geometry,” in Maximum Entropy
and Bayesian Methods, G. Heidbreder, ed., pp. 87–96, Kluwer Academic Publishers, 1996.
(since Nov. 1993) in omega.albany.edu:8008/robust.ps.

14. D. Bleecker, Gauge Theory and Variational Principles, Addison-Wesley, 1981.
15. C. Rodr´ıguez, “From euclid to entropy,” in Maximum Entropy and Bayesian Methods,
W. T. Grandy, Jr., ed., Kluwer Academic Publishers. omega.albany.edu:8008/euclid.ps,
1991. omega.albany.edu:8008/euclid.ps.
16. J. Bekenstein Phys. Rev. D, 7, p. 2333, 1973.
17. S. Hawking Comm. Math. Phys., 43, p. 199, 1975.


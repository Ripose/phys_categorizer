2
0
0
2
 
v
o
N
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
5
0
1
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Binary N-Step Markov Chain as an Exactly Solvable Model of Long-Range Correlated
Systems

O.V. Usatenko and V.A. Yampol’skii
A.Ya. Usikov Institute for Radiophysics and Electronics
Ukrainian Academy of Science, 12 Proskura Street, 61085 Kharkov, Ukraine
(Dated: December 8, 2013)

A theory of systems with long-range correlations based on the consideration of binary N-step
Markov chains is developed. In our model, the conditional probability that the i-th symbol in the
chain equals zero (or unity) is a linear function of the number of unities among the preceding N
symbols. The model allows exact analytical treatment. The correlation and distribution functions
as well as the variance of number of symbols in the words of arbitrary length L are obtained
analytically and numerically. A self-similarity of the studied stochastic process is revealed and the
similarity transformation of the chain parameters is presented. The diﬀusion equation governing
the distribution function of the L-words is explored. If the persistent correlations are not extremely
strong, the distribution function is shown to be the Gaussian with the variance being nonlinearly
dependent on L. The applicability of the developed theory to the coarse-grained written and DNA
texts is discussed.

PACS numbers: 05.40.-a, 02.50.Ga, 87.10.+e

I.

INTRODUCTION

The problem of long-range correlations is one of the topics of current research in the statistical mechanics. The
stochastic processes with strong correlations have been observed in numerous systems. Examples include the natural
languages, the DNA sequences, etc. [1, 2].

One of the eﬃcient methods to investigate the correlated systems is based on a decomposition of the space of states
into a ﬁnite number of parts labelled by deﬁnite symbols. This procedure referred to as coarse graining is accompanied
by the loss of short-range memory between symbols but does not aﬀect and does not damage the robust invariant
statistical properties of the long-range correlated sequences.
In terms of the power spectrum, one loses only the
short-wave part of the spectrum. The most frequently used method of the decomposition is based on the introduction
of two parts of the phase space. In other words, it consists in mapping the two parts of states onto two symbols, say 0
and 1. Thus, the problem is reduced to the investigation into the statistical properties of the binary sequences. This
method is applicable for the examination of the both discrete and continuous systems.

One of the ways to get a correct insight into the nature of correlations in a system consists in an ability of
constructing a mathematical object (for example, a correlated sequence of symbols) possessing the same statistical
properties as the initial system. There are many algorithms to generate long-range correlated sequences: the inverse
Fourier transform [3], the expansion-modiﬁcation Li method [4], the Voss procedure of consequent random addition [5],
the correlated Levy walks [6], etc. [3]. We believe that from among the above-mentioned methods, using the Markov
chains is one of the most important. We would like to demonstrate this statement in the present paper.
In the next sections, the statistical properties of the binary many-steps Markov chain is examined.

In spite of
the long-time history of studying the Markov sequences (see, for example, [7, 8, 9]), the concrete expressions for
the variance of sums of random variables in such strings have not been obtained yet. Our model operates with
two parameters governing the conditional probability of the discrete Markov process, speciﬁcally with the memory
length N and the correlation parameter µ. The correlation and distribution functions as well as the variance D being
nonlinearly dependent on the length L of a word are derived analytically and calculated numerically. The nonlinearity
of the D(L) function reﬂects the strong correlations in the system. The evolved theory is applied to the coarse-grained
written texts and dictionaries, and to DNA strings as well.

II. FORMULATION OF THE PROBLEM

A. Markov Processes

Let us consider a stationary binary sequence of symbols, ai = {0, 1}. To determine the N -step Markov chain we
have to introduce the conditional probability P (ai = 0 | ai−N , ai−N +1, . . . , ai−1) of following the deﬁnite symbol
(for example, zero) after symbols ai−N , ai−N +1, . . . , ai−1. Thus, it is necessary to deﬁne 2N values of the P -function

corresponding to each possible conﬁguration of the symbols ai−N , ai−N +1, . . . , ai−1. We suppose that the P -function
has the form,

P (ai = 0 | ai−N , ai−N +1, . . . , ai−1) =

f (ai−k, k).

It is reasonable to assume the function f to be decreasing with an increase of the distance k between the symbols
ai−k and ai in the Markov chain. However, for the sake of simplicity we consider here a step-like memory function
f (ai−k, k) independent of the second argument k. As a result, our model is characterized by three parameters only,
speciﬁcally by f (0), f (1), and N :

P (ai = 0 | ai−N , ai−N +1, . . . , ai−1) =

f (ai−k).

Note that the probability P in Eq. (2) depends on the numbers of symbols 0 and 1 in the N -word but is independent
of the arrangement of the elements ai−k. We also suppose that

f (0) + f (1) = 1.

This relation provides the statistical equality of the numbers of symbols zero and unity in the Markov chain under
consideration. In other words, the chain is non-biased. Indeed, taking into account Eqs. (2) and (3) and the sequence
of equations,

P (ai = 1|ai−N , . . . , ai−1) = 1 − P (ai = 0|ai−N , . . . , ai−1) =

f (˜ai−N ) = P (ai = 0 | ˜ai−N , . . . , ˜ai−1),

(4)

one can see the symmetry with respect to interchange ˜ai ↔ ai in the Markov chain. Here ˜ai is the symbol opposite
to ai, ˜ai = 1 − ai. Therefore, the probabilities of occurring the words (a1, . . . , aL) and (˜a1, . . . , ˜aL) are equal to each
other for any word length L. At L = 1 this yields equal average probabilities of occurring 0 and 1 in the chain.

Taking into account the symmetry of a conditional probability P with respect to a permutation of symbols ai (see
Eq. (2)), we can simplify the notations and introduce the conditional probability pk of occurring the symbol zero after
the N -word containing k unities, e.g., after the word (11...1

00...0

),

1
N

N

Xk=1

1
N

N

Xk=1

1
N

N

Xk=1

pk = P (aN +1 = 0 | 11 . . . 1

k

N −k

| {z }

| {z }
00 . . . 0

) =

k

N −k

1
2

+ µ(1 −

2k
N

),

with the correlation parameter µ being deﬁned by the relation

| {z }

| {z }

µ = f (0) −

1
2

.

We focus our attention on the region of µ determined by the persistence inequality 0 ≤ µ < 1/2. Nevertheless, the

major part of our results is valid for the anti-persistent region −1/2 < µ < 0 as well.

A similar rule for the production of an N -word (a1, . . . , aN ) following after a word (a0, a1, . . . , aN −1) was suggested
in Ref. [8]. However, the conditional probability pk of occurring the symbols aN does not depend on the previous
ones in the model [8].

B. Statistical characteristics of the chain

In order to investigate the statistical properties of the Markov chain, we consider the distribution WL(k) of the

words of deﬁnite length L by the number k of unities in them,

2

(1)

(2)

(3)

(5)

(6)

(7)

ki(L) =

ai+l,

L

Xl=1

3

(8)

(9)

(10)

and the variance of k,

where

D(L) = k2 − k

2

,

f (k) =

f (k)WL(k).

L

Xk=0

D(L) = L/4.

C. Main equation

If µ = 0, one arrives at the known result for the non-correlated Brownian diﬀusion,

We will show that the distribution function WL(k) for the sequence determined by Eq. (5) (with nonzero but not
extremely close to 1/2 parameter µ) is the Gaussian with the variance D(L) nonlinearly dependent on L.

For the stationary Markov chain, the probability b(a1a2 . . . aN ) of occurring a certain word (a1, a2, . . . , aN ) satisﬁes

the equation,

b(a1 . . . aN ) =

b(aa1 . . . aN −1)P (aN | a, a1, . . . , aN −1).

(11)

a=0,1
X

Thus, we have 2N homogeneous algebraic equations for 2N words and the normalization equation
of equations can be essentially simpliﬁed owing to the following statement.

b = 1. The set

Proposition ♠: The probability b(a1a2 . . . aN ) depends on the number k of unities in the N -word only but is

P

independent of the arrangement of symbols in the word (a1, a2, . . . , aN ).

This statement illustrated by Fig. 1 is valid owing to the chosen simple model (2), (5) of the Markov chain. It can
be easily veriﬁed directly by the substitution of the obtained below solution Eq. (13) into set (11). Proposition ♠
leads to the very important property of isotropy: any word (a1, a2, . . . , aL) appears with the same probability as the
inverted one, (aL, aL−1, . . . , a1).

0,008

0,007

0,006

)
z
(
P

0,005

0,004

0,003

0

50

100

150

200

250

z

FIG. 1: The probability of occurring a word (a1, a2, . . . , aN ) vs its number z in the binary code, z =
µ = 0.4.

N
i=1 ai · 2

i−1 for N = 8,

P

Let us apply set of Eqs. (11) to the word (11 . . . 1

00 . . . 0

):

k

N −k

b(11 . . . 1

00 . . . 0

| {z }
) = b(0 11 . . . 1

| {z }

00 . . . 0

)pk + b(1 11 . . . 1

00 . . . 0

)pk+1.

(12)

k

N −k

k

N −k−1

k

N −k−1

| {z }

| {z }

| {z }

| {z }

| {z }

| {z }

This yields the recursion relation for b(k) = b(11...1

00...0

),

k

N −k

b(k) =

1 − pk−1
pk

| {z }
b(k − 1) =

| {z }

N − 2µ(N − 2k − 2)
N + 2µ(N − 2k)

b(k − 1).

III. DISTRIBUTION FUNCTION OF L-WORDS

In this section we investigate the statistical properties of the Markov chain, speciﬁcally, the distribution of the
words of deﬁnite length L by the number k of unities in them. The length L can also be interpreted as the number
of jumps of some particle over an integer-valued 1-D lattice or as the time of the diﬀusion imposed by the Markov
chain under consideration. The form of the distribution function WL(k) depends essentially on the relation between
the word length L and the memory length N . Therefore, we start our study with the simplest case, L = N .

A. Statistics of N -words

The value b(k) is the probability that an N -word contains k unities with a deﬁnite order of symbols ai. Therefore,
the probability WN (k) that an N -word contains k unities with arbitrary order of symbols ai is b(k) multiplied by the
number Ck

N = N !/k!(N − k)! of diﬀerent permutations of k unities in the N -word,

Combining Eqs. (13) and (14), we get

with the parameter n deﬁned by

WN (k) = Ck

N b(k).

WN (k) = WN (0)Ck
N

Γ(n + k)Γ(n + N − k)
Γ(n)Γ(n + N )

The normalization constant WN (0) can be obtained from the equality,

n =

N (1 − 2µ)
4µ

.

Ck

N b(k) = 1.

N

Xk=0

WN (N − k) = WN (k).

Note that the distribution WN (k) is an even function of the variable κ = k − N/2,

This fact is a direct consequence of the above-mentioned statistical equivalency of zeros and unities in the Markov chain
under consideration. Let us analyze the distribution function WN (k) for diﬀerent relations between the parameters
N and µ.

1. Limiting case of weak persistence, n ≫ 1

In the absence of correlations, µ → 0, Eq. (15) and the Stirling formula yield the Gaussian distribution at k, N, N −
k ≫ 1. In the most interesting case of not too strong persistence, n ≫ 1, one can also obtain the Gaussian form for
the distribution function,

with the µ-dependent variance,

p

WN (k) =

1

2πD(N )

exp

−

(cid:26)

(k − N/2)2
2D(N )

,

(cid:27)

D(N ) =

N
4(1 − 2µ)

.

4

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

Equation (19) says that the N -words with equal numbers of zeros and unities, k = N/2, are mostly probable. Note
that the persistence results in an increase of the variance D(N ) with respect to its value N/4 at µ = 0. In other
words, the persistence is conductive to the intensiﬁcation of the diﬀusion. Inequality n ≫ 1 provides D(N ) ≪ N 2.
Therefore, despite the increase of D(N ), the ﬂuctuations of (k − N/2) of the order of N are exponentially small.

If the parameter n is an integer of the order of unity, the distribution function WN (k) is a polynomial of degree

2(n − 1). In particular, at n = 1, the function WN (k) is constant,

2.

Intermediate case, n ∼ 1

WN (k) =

1
N + 1

.

At n 6= 1, WN (k) has a maximum at the middle of the interval [0, N ].

3. Limiting case of strong persistence, n ≪ 1

In this situation, WN (k) assumes the maximal values at k = 0 and k = N ,

WN (1) = WN (0)

≪ WN (0).

nN
N − 1

At k = N/2, the probability WN (k) achieves its minimal value,

WN (k) = WN (0)

nN
k(N − k)

.

WN

= WN (0)

N
2

(cid:18)

(cid:19)

4n
N

.

Formula (22) describes the sharply decreasing WN (k) as k changes from 0 to 1 (and from N to N − 1). Then, at
1 < k < N/2, the function WN (k) decreases more slowly with an increase in k,

The values WN (0) = WN (N ) are nearly 1/2 to a logarithmic accuracy.
The evolution of the distribution function WN (k) from the Gaussian form to the inverse one with a decrease of the

parameter n is shown in Fig. 2. Below we restrict ourselves to the case of weak persistence, n ≫ 1.

Formulas (19) and (20) describe the statistical properties of L-words for the ﬁxed ”diﬀusion time” L = N . It is
necessary to look into the distribution function WL(k) for the general situation, L 6= N . We start the analysis with
L < N .

The distribution function WL(k) at L < N can be given as

This equation follows from the consideration of N -words consisting of two parts,

B. Statistics of L-words with L < N

1. Distribution function WL(k)

WL(k) =

b(i)Ck

LCi−k

N −L.

k+N −L

Xi=k

(a1, . . . , aN −L,

aN −L+1, . . . , aN

).

i−k unities

k unities

|
}
The total number of unities in this word is i. The right-hand part of the word (L-sub-word) contains k unities. The
LCi−k
remaining i − k unities are situated within the left-hand part of the word ((N − L)-sub-word). The multiplier Ck
N −L

{z

{z

|

}

5

(21)

(22)

(23)

(24)

(25)

(26)

0,15

0,10

)
k
(

W

0
2

0,05

0,00

40

3

1

k

0.5

0

5

10

15

20

FIG. 2: The distribution function WN (k) for N =20 and diﬀerent values of the parameter n shown near the curves.

in Eq. (25) takes into account all possible permutations of the symbols ”1” within the N -word on condition that the
L-sub-word always contains k unities. Then we perform the summation over all possible values of the number i. Note
that Eq. (25) is valid due to the main proposition ♠ formulated in Subsec. C of the previous section.

In this subsection, we focus our attention on the most important limiting case n, k, L − k ≫ 1. The straightforward

calculations with using the Stirling formula give the result,

with

WL(k) =

1

2πD(L)

exp

−

(cid:26)

(k − L/2)2
2D(L)

(cid:27)

p

L
4

D(L) =

(1 + mL), m =

=

1
2n

2µ
N (1 − 2µ)

.

The last equation allows one to analyze the behavior of the variance D(L) with an increase in the “diﬀusion time”
L. At small mL ≪ 1 the dependence D(L) follows the classical law of the Brownian diﬀusion, D(L) ≈ L/4. Then, at
mL ∼ 1, the function D(L) becomes super-linear and meets the value (20) at L = N .

Such an unusual behavior of the variance D(L) raises an issue as to what particular type of the diﬀusion equation
corresponds to the nonlinear dependence D(L) in Eq. (28). In the following subsection, when solving this problem,
we will obtain the conditional probability p(0) of occurring the symbol zero after a given L-word with L < N . The
ability to ﬁnd p(0), with some reduced information about preceding symbols being available, is very important for the
study of the self-similarity of the Markov chain (see Subsubsec. 3 of this Subsection).

2. Generalized diﬀusion equation

It is quite obvious that the distribution WL(k) satisﬁes the equation

WL+1(k) = WL(k)p(0)(k) + WL(k − 1)p(1)(k − 1).

Here p(0) is the probability of occurring ”0” after an average-statistical L-word containing k unities and p(1) is the
probability of occurring ”1” after an L-word containing k − 1 unities. At L < N , the probability p(0) can be written
as

p(0) =

1
WL(k)

k+N −L

Xi=k

pib(i)Ck

LCi−k

N −L.

The product b(i)Ck
i unities, the right-hand part of which, the L-sub-word, contains k unities (compare with Eqs. (25), (26)).

N −L in this formula represents the conditional probability of occurring the N -word containing

LCi−k

6

(27)

(28)

(29)

(30)

The product b(i)Ci−k

N −L in Eq. (30) is a sharp function of i with a maximum at some point i = i0 whereas pi obeys
linear law (5). This implies that pi can be factored out of the summation sign being taken at point i = i0. The
asymptotical calculation shows that point i0 is described by the equation,

Expression (5) taken at point k = i0 gives the desired formula for p(0) because

i0 =

−

N
2

L/2
1 − 2µ(1 − L/N )

1 −

(cid:18)

2k
L

.

(cid:19)

is obviously equal to WL(k). Thus, we have

k+N −L

Xi=k

b(i)Ck

LCi−k
N −L

p(0)(k) =

+

1
2

µL
N − 2µ(N − L)

1 −

(cid:18)

2k
L

.

(cid:19)

Let us consider a very important fact following from Eq. (31). If the concentration of unities in the right-hand part
of the word (26) is higher than 1/2, k/L > 1/2, then the most probable concentration (i0 − k)/(N − L) of unities in
the left-hand part of this word is likewise increased, (i0 − k)/(N − L) > 1/2. At the same time, the concentration
(i0 − k)/(N − L) is less than k/L,

1
2

<

i0 − k
N − L

<

k
L

.

This means that the increased concentration of unities in the L-words is necessarily accompanied by the existence of a
certain tail with an increased concentration of unities as well. We name such a phenomenon as the macro-persistence.
An analysis performed in the next section will indicate that the correlation length lc of this tail is γN with γ ≥ 1
dependent on the parameter µ only. It is evident from the above-mentioned property of the isotropy of the Markov
chain that there are two correlation tails from the both sides of the L-word.

By going over to the continuous limit in Eq. (29) and using Eq. (33) and the relation p(1)(k − 1) = 1 − p(0)(k − 1),

we obtain the diﬀusion equation generalized to the case of the correlated Markov process,

with κ = k − L/2 and

∂W
∂L

=

1
8

∂2W
∂κ2 − η(L)

κ

∂W
∂κ

(cid:18)

+ W

,

(cid:19)

η(L) =

2µ
(1 − 2µ)N + 2µL

.

Equation (35) has a solution of the Gaussian form Eq. (27) with the variance D(L) satisfying the ordinary diﬀerential
equation,

Its solution with the boundary condition D(0) = 0 coincides with (28).

dD
dL

1
4

=

+ 2η(L)D.

3. Self-similarity of the persistent Brownian diﬀusion

In this subsection we point out one of the most important properties of the Markov chain being considered, namely,
its self-similarity. Let us reduce the N -step Markov sequence by regularly (or randomly) removing some symbols and
introduce the decimation parameter λ,

Here N ∗ is a renormalized memory length for the reduced N ∗-step Markov chain. According to Eq. (33), the
k of occurring the symbol zero after k unities among the preceding N ∗ symbols is described
conditional probability p∗
by the formula,

λ = N ∗/N ≤ 1.

p∗
k =

+ µ∗

1 −

1
2

(cid:18)

2k
N ∗

,

(cid:19)

7

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

with

µ∗ = µ

λ
1 − 2µ(1 − λ)

.

The comparison of Eqs. (5) and (39) shows that the reduced chain possesses the same statistical properties as the
initial one but is characterized by the renormalized parameters (N ∗, µ∗) instead of (N , µ). Thus, Eqs. (38) and (40)
determine the one-parametrical renormalization of the parameters of the stochastic process deﬁned by Eq. (5).

The astonishing property of the reduced sequence consists in that the variance D∗(L) is invariant with respect
to the one-parametric decimation transformation (38), (40). Therefore, it coincides with the function D(L) for the
initial Markov chain:

D∗(L) =

1 +

L
4

(cid:18)

2µ∗
1 − 2µ∗

L
N ∗

=

L
4

1 +

2µ
1 − 2µ

L
N

(cid:19)

(cid:18)

(cid:19)

= D(L).

The invariance of the function D(L) at L < N was referred by us to as the phenomenon of self-similarity.
demonstrated in Fig. 3 and is also discussed in Sec. IV A.

8

(40)

(41)

It is

100

10

D

1

1

10

100

L

FIG. 3: The dependence of the variance D on the tuple length L for the generated sequence with N = 100 and µ = 0.4 (solid
line) and for the decimated sequences (the parameter of decimation λ = 0.5). Squares and circles correspond to the stochastic
and deterministic reduction, respectively. The thin solid line describes the non-correlated Brownian diﬀusion, D(L) = L/4.

C. Long-range diﬀusion, L > N

Unfortunately, the very useful proposition ♠ is valid for the words of the length L ≤ N only and cannot be applied
to the analysis of the long words with L > N . Therefore, investigating the statistical properties of the long words
represents a rather challenging combinatorial problem and requires new physical approaches for its simpliﬁcation.
Thus, we start this subsection by analyzing the correlation properties of the long words.

Let us rewrite Eq. (5) in the form,

1. Correlation length

< ai+1 >=

+ µ

1
2

2
N

 

i

Xk=i−N +1

< ak > −1

.

!

(42)

The angle brackets denote the averaging of the density of unities in some region of the Markov chain for its deﬁnite
realization. The averaging is performed over distances much greater than unity but far less than the memory length

N . Note that this averaging diﬀers from the statistical averaging over the ensemble of realizations of the Markov
chain denoted by the bar in Eqs. (8) and (9). Equation (42) is a relationship between the average densities of unities
in two diﬀerent macroscopic regions of the Markov chain, namely, in the vicinity of (i + 1)-th element and in the
region (i − N, i). Such an approach is similar to the mean ﬁeld approximation in the theory of the phase transitions
and is asymptotically exact under the condition N → ∞. In the continuous limit, Eq. (42) can be rewritten in the
integral form,

< a(i) >=

+ µ

< a(k) > dk − 1

.

(cid:19)

1
2

1
2

i

2
N

(cid:18)

i−N

Z

1
2

< a(i) −

>=< a(0) −

> exp (i/γN ) ,

It has the obvious solution,

where γ is determined by the relation,

The last equation has a unique solution γ(µ) for any value of µ ∈ (0, 1/2).

Formula (44) shows that any ﬂuctuation (the diﬀerence between < a(i) > and the equilibrium value of ai = 1/2) is

exponentially damped at distances of the order of the correlation length lc,

Law (44) describes the phenomenon of the persistent macroscopic correlations discussed in the previous subsection.
This phenomenon is governed by both of the parameters, the memory length N and the persistence parameter µ.
According to Eqs. (45), (46), the correlation length lc goes logarithmically to inﬁnity with an increase in µ, at µ → 1/2.
At µ → 0, the macro-persistence is broken and the correlation length tends to zero.

Using the already studied correlation properties of the the Markov sequence and some heuristic reasons, one can

obtain the correlation function K(r),

and then the variance D(L),

(cid:17)
Comparing Eqs. (47) and (48) and taking into account the unbiased property of the sequence, ai = 1/2, it is easy to
derive the general relationship between the functions K(r) and D(L),

(cid:16)X

γ

exp

− 1

=

(cid:18)

(cid:18)

(cid:19)

(cid:19)

1
γ

1
2µ

.

lc = γN.

2. Correlation function

K(r) = aiai+r − ai

2,

D(L) =

2

−

ai

2

.

ai

(cid:18)X

(cid:19)

D(L) =

+ 4

K(r).

L2
4

L−1

L−i

i=1
X

r=1
X

Considering (49) as an equation with respect to K(r), one can ﬁnd its solution given as

K(1) =

D(2) −

, K(2) =

D(3) − D(2) +

1
4

1
2

1
8

,

K(r) =

[D(r + 1) − 2D(r) + D(r − 1)] ,

r ≥ 3.

1
2

1
2

This solution has a very simple form in the continuous limit,

K(r) =

1
2

d2D(r)
dr2

.

9

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

10

(52)

(53)

(54)

(55)

(56)

(57)

Equations (50) and (28) give the correlation function at r < N ,

K(r) = Cr

= Crm,

C1 = 1/2, C2 = 1/8, C3≤r≤N = 1/4,

2µ
1 − 2µ

1
N

in the continuous approximation. The independence of the correlation function of r at r < N results from our choice
of the conditional probability in the simplest form (5). At r > N , the function K(r) should decrease because of loss
of the memory. Therefore, based on Eqs. (44) and (46), let us prolongate the correlator K(r) as the exponentially
decreasing function at r > N ,

K(r) =

r ≤ N

m
4

,

K(r) =

1,
exp

m
4 (

r ≤ N,
, r > N.

− r−N
lc
(cid:16)

(cid:17)

D(L) =

(1 + mF (L)) ,

L
4

Correspondingly, the variance D(L) becomes

or

with

F (L) =

(

L,
2(1 + γ)N − (1 + 2γ) N

2

L − 2γ2 N

L

2

1 − exp

− L−N
lc

,

h

(cid:16)

(cid:17)i

L < N,
L > N.

The plot of Eq. (55) for N = 100 and µ = 0.4 is shown by the solid line in Fig. 4. For comparison, the straight line
in the ﬁgure corresponds to the dependence D(L) = L/4 for the usual Brownian diﬀusion without correlations (for
µ = 0). It is clearly seen that the plot of variance (55) contains two qualitatively diﬀerent portions. One of them,
at L ∼ N , is the super-linear curve that moves away from the line D = L/4 with an increase of L as a result of the
persistence. For L ≫ N , the plot D(L) achieves the linear asymptotics,

D(L) ∼=

(1 + 2(1 + γ)mN ).

L
4

This phenomenon can be explained as a result of the diﬀusion where each practically independent step ∼ D1/2(N + lc)
of wandering represents a path traversed during the characteristic “ﬂuctuating time” ∆L ∼ (N + lc). Since these
steps of wandering are quasi-independent, the distribution function WL(k) is the Gaussian not only at L < N (see
Eq. (27)) but also in the case L > N, lc.

Note that the above-mentioned phenomenon of the self-similarity relates only to the portion L < N of the curve
D(L). Since the decimation procedure leads to the decrease of the parameter µ (see Eq. (40)), the plot of asymptotics
(57) for the reduced sequence at L ≫ N ∗ goes below the plot for the initial chain.

IV. RESULTS OF NUMERICAL SIMULATIONS AND APPLICATIONS

In this section, we support the obtained above analytical results by numerical simulations of the Markov chain with
the conditional probability Eq. (5). Besides, the properties of the studied binary N -step Markov chain are compared
with ones for the natural objects, speciﬁcally for the coarse-grained written and DNA texts.

A. Numerical simulations of the Markov chain

The ﬁrst stage of the construction of the N -step Markov chain was a generation of the initial non-correlated N
symbols, zeros and unities, identically distributed with equal probabilities 1/2. Each consequent symbol was then
added to the chain with the conditional probability determined by the previous N symbols in accordance with Eq. (5).
Than we numerically calculated the variance D(L) by means of Eq. (8). The circles in Fig. 4 represent the calculated
variance D(L) for the 100-step Markov chain generated at µ = 0.4. A very good agreement between the analytical
result in Eq. (55) and the numerical simulation can be observed.

11

5

4

3

2

1

0

10

10

10

D

10

10

10

0

1

2

3

4

10

10

10

10

10

L

FIG. 4: The numerical simulation of the dependence D(L) for the generated sequence with N = 100 and µ = 0.4 (circles). The
solid line is the plot of function Eq. (55) with the same values of N and µ.

The numerical simulation was also used for the demonstration of the proposition ♠ (Fig. 1) and the self-similarity
property of the Markov sequence (Fig. 3). The squares in Fig. 3 represent the variance D(L) for the sequence
obtained by the stochastic decimation of the initial Markov chain (solid line) where each symbol was omitted with
the probability 1/2. The circles in this ﬁgure correspond to the regular reduction of the sequence by removing each
second symbol.

And ﬁnally, the numerical simulations have allowed us to make sure that we are able to determine the parameters
N and µ of a given binary sequence. We generated Markov sequences with diﬀerent parameters N and µ and
deﬁned numerically the corresponding curves D(L). Then we solved the inverse problem of the reconstruction of the
parameters N and µ by analyzing the curves D(L). The reconstructed parameters were always in a good agreement
with their prescribed values. In the next subsections we apply this ability to the treatment of the statistical properties
of literary and DNA texts.

It is well-known that the statistical properties of the coarse-grained texts written in any language show a remarkable
deviation from random sequences [8, 10]. In order to check the applicability of the theory of the binary N -step Markov
chains to literary texts we resorted to the procedure of coarse graining by the random mapping of all characters of
the text onto binary set of symbols, zeros and unities. The statistical properties of the coarse-grained texts depend,
but not signiﬁcantly, on the kind of mapping. This is illustrated by the curves in Fig. 5 where the variance D(L) for
ﬁve diﬀerent kinds of the mapping of Bible is presented. Usually, the random mapping leads to nonequal numbers of
unities and zeros, k1 and k0, in the coarse-grained sequence. The particular analysis shows that the variance D(L)
(28) gets the additional multiplier,

B. Literary texts

4k0k1
(k0 + k1)2 ,

in this biased case. In order to derive the function D(L) for the non-biased sequence, we divided the numerically
calculated value of the variance by this multiplier.

The study of diﬀerent written texts has shown that all of them are featured by the pronounced persistent correlations.
It is demonstrated by Fig. 6 where ﬁve variance curves go signiﬁcantly higher than the straight line D = L/4. However,
it should be emphasized that regardless of the kind of mapping the initial portions, L < 80, of the curves correspond
to a slight anti-persistent behavior (see insert to Fig. 7). Moreover, for some inappropriate kinds of mapping (e.g.,
when all vowels are mapped onto the same symbol) the anti-persistent portions can reach the values of L ∼ 1000. In
order to avoid this problem, all the curves in Fig. 6 are obtained for the deﬁnite representative mapping: (a-m) → 0;
(n-z) → 1.

12

7

6

5

4

3

2

1

0

7

6

5

4

3

2

1

0

10

10

10

10

D

10

10

10

10

-1

10

10

10

10

10

D

10

10

10

10

-1

10

L

L

1

2

3

4

5

10

10

10

10

10

FIG. 5: The dependence D(L) for the coarse-grained text of the Bible obtained by means of ﬁve diﬀerent kinds of random
mapping.

1

2

3

4

5

10

10

10

10

10

−3, solid
FIG. 6: The dependence D(L) for the coarse-grained texts of collection of works on the computer science (m = 2.2 · 10
−3, dotted line), “History of Russians in the
−3, dashed line), Bible in English (m = 1.5 · 10
line), Bible in Russian (m = 1.9 · 10
20-th Century” by Oleg Platonov (m = 6.4 · 10−4, dash-dotted line), and “Alice’s Adventures in Wonderland” by Lewis Carroll
(m = 2.7 · 10−4, dash-dot-dotted line).

Thus, the persistence is the common property of the binary N -step Markov chains and the coarse-grained written
texts at large scales. Moreover, the written texts as well as the Markov sequences possess the property of the
self-similarity. Indeed, the curves in Fig. 7 obtained from the text of Bible with diﬀerent levels of the deterministic
decimation demonstrate the self-similarity. Presumably, this property is the mathematical reﬂection of the well-known
hierarchy in the linguistics: letters → syllables → words → sentences → paragraphs → chapters → books → collected
works.

All the above-mentioned circumstances allow us to suppose that our theory of the binary N -step Markov chains can
be applied to the description of the statistical properties of the texts of natural languages. However, in contrast to the
generated Markov sequence (see Fig. 4) where the full length M of the chain is far greater than the memory length N ,
the coarse-grained texts described by Fig. 6 are of relatively short length M < N . In other words, the coarse-grained
texts are similar not to the Markov chains but rather to some non-stationary short fragments. This implies that each
of the written texts is correlated throughout the whole of its length. Therefore, for the written texts, it is impossible

13

D

10(cid:13)2(cid:13)

20(cid:13)

40(cid:13)

60(cid:13)

80(cid:13)

100(cid:13)

L(cid:13)

10(cid:13)5(cid:13)

10(cid:13)4(cid:13)

D

10(cid:13)3(cid:13)

25(cid:13)

20(cid:13)

15(cid:13)

10(cid:13)

5(cid:13)

10(cid:13)1(cid:13)

10(cid:13)0(cid:13)

10(cid:13)-1(cid:13)

10(cid:13)0(cid:13)

10(cid:13)1(cid:13)

10(cid:13)2(cid:13)

10(cid:13)3(cid:13)

10(cid:13)4(cid:13)

L(cid:13)

FIG. 7: The dependence of the variance D on the tuple length L for the coarse-grained text of Bible (solid line) and for the
decimated sequences with diﬀerent parameters λ: λ = 3/4 (squares), λ = 1/2 (stars), and λ = 1/256 (triangles). The insert
demonstrate the anti-persistent portion of the D(L) plot for Bible.

to observe the second portion of the curve D(L) parallel (in the log-log scale) to the line D(L) = L/4, similar to that
shown in Fig. 4. As a result, one cannot deﬁne the values of the both parameters N and µ for the coarse-grained
texts. The analysis of the curves in Fig. 6 can give the combination m = 2µ/N (1 − 2µ) only (see Eq. (28)). Perhaps,
this particular combination is the real parameter governing the persistent properties of the literary texts.

We would like to note that the origin of the long-range correlations in the literary texts is hardly related to the
grammatical rules as is claimed in Ref. [8]. At short scales L ≤ 80 where the grammatical rules are in fact applicable
the character of correlations is anti-persistent whereas semantic correlations lead to the global persistent behavior of
the variance D(L) throughout the whole of literary text.

The numerical estimations of the persistent parameter m and the characterization of the languages and diﬀerent
authors using this parameter can be regarded as a new intriguing problem of the linguistics. For instance, the
unprecedented low value of m for the very inventive work by Lewis Carroll as well as the closeness of m for the texts
of English and Russian versions of Bible are of certain interest.

It should be noted that there exist special kinds of short-range correlated texts which can be speciﬁed by both of
the parameters, N and µ. For example, all dictionaries consist of the families of words where some preferable letters
are repeated more frequently than in their other parts. Yet another example of the shortly correlated texts is any
lexicographically ordered list of words. The analysis of written texts of this kind is given below.

C. Dictionaries

As an example, we have investigated the statistical properties of the coarse-grained alphabetical list of the most
frequently used 15462 English words.
In contrast to other texts, the statistical properties of the coarse-grained
dictionaries are very sensitive to the kind of mapping. If one uses the above-mentioned mapping, (a-m) → 0; (n-z) →
1, the behavior of the variance D(L) similar to that shown in Fig. 6 would be obtained. The particular construction
of the dictionary manifests itself if the preferable letters in the neighboring families of words are mapped onto the
diﬀerent symbols. The variance D(L) for the dictionary coarse-grained by means of such mapping is shown by circles
in Fig. 8. It is clearly seen that the graph of the function D(L) consists of two portions similarly to the curve in Fig. 4
obtained for the generated N -step Markov sequence. The ﬁtting of the curve in Fig. 8 by the function (55) (solid line
in Fig. 8) yielded the values of the parameters N = 180 and µ = 0.44. The parameter γ given by Eq. (45) is around
3.9. Note that the characteristic ﬂuctuation length N (1 + γ) for these N and γ is nearly 900. This value corresponds
qualitatively to the length of the family of words in the dictionary.

(cid:13)
(cid:13)
14

D(L)=L(1+0.04L)/4

D(L)=L(1+72)/4

0

1

2

3

4

10

10

10

10

10

FIG. 8: The dependence D(L) for the coarse-grained alphabetical list of 15462 English words (circles). The solid line is the
plot of function Eq. (55) with the ﬁtting parameters N = 180 and µ = 0.44.

D. DNA texts

It is known that any DNA text is written by four “characters”, speciﬁcally by adenine (A), cytosine (C), guanine
(G), and thymine (T). Therefore, there are three nonequivalent types of the DNA text mapping onto one-dimensional
binary sequences of zeros and unities. The ﬁrst of them is the so-called purine-pyrimidine rule, {A,G} → 0, {C,T} →
1. The second one is the hydrogen-bond rule, {A,T} → 0, {C,G} → 1. And, ﬁnally, the third is {A,C} → 0, {G,T}
→ 1.

6

5

4

3

2

1

0

10

10

10

10

10

10

10

-1

10

D

10

10

10

10

10

10

10

10

9

8

7

6

5

4

3

2

1

0

10

-1

10

D

10

L

L

0

1

2

3

4

5

6

10

10

10

10

10

10

10

FIG. 9: The dependence D(L) for the coarse-grained DNA text of Bacillus subtilis, complete genome, for three nonequivalent
kinds of the mapping. Solid, dashed, and dash-dotted lines correspond to the mappings {A,G} → 0, {C,T} → 1 (the parameter
m = 4.1 · 10−2), {A,T} → 0, {C,G} → 1 (m = 2.5 · 10−2), and {A,C} → 0, {G,T} → 1 (m = 1.5 · 10−2), respectively.

By way of example, the variance D(L) for the coarse-grained text of Bacillus subtilis, complete genome
(ftp://ftp.ncbi.nih.gov/genomes/bacteria/bacillus subtilis/NC 000964.gbk) is displayed in Fig. 9 for all possible types
of mapping. One can see that the persistent properties of DNA are more pronounced than for the written texts and,
contrary to the written texts, the D(L) dependence for DNA does not exhibit the anti-persistent behavior at small
values of L. However, as well as for the written texts, the D(L) curve for DNA does not contain the linear portion

15

given by Eq. (57). This suggests that the DNA chain is not a stationary sequence. In this connection, we would like
to point out that the DNA texts represent the collection of extended non-coding regions interrupted by small coding
regions (see, for example, [2]). According to Fig. 9, the coding regions do not interrupt the correlation between the
non-coding areas, and the DNA system is fully correlated throughout its whole length.

The noticeable deviation of diﬀerent curves in Fig. 9 from each other demonstrate, in our opinion, that the DNA
texts are much more complex objects in comparison with the written ones. Indeed, the diﬀerent kinds of mapping
reveal and emphasize various types of physical attractive correlations between the nucleotides in DNA, such as the
strong purine-purine and pyrimidine-pyrimidine persistent correlations (the upper curve), and the correlations caused
by a weaker attraction A↔T and C↔G (the middle curve).

V. CONCLUSION

Thus, we have developed a new approach for the description of the strongly correlated one-dimensional systems.
The simple exactly solvable model of the uniform binary N -step Markov chain is presented. The memory length
N and the parameter µ of the persistent correlations are two parameters in our theory. Usually, the correlation
function K(r) is employed as the input characteristics for the description of the correlated random systems. Yet, the
function K(r) describes not only the direct interconnection of the elements ai and ai+r, but also takes into account
their indirect interaction via other elements. Since our approach operates with the “origin” parameters N and µ, we
believe that it allows us to disclose the intrinsic properties of the system which provide the correlations between the
elements.

We have demonstrated the applicability of the suggested theory to the diﬀerent kinds of correlated stochastic
systems. However, there exist some aspects which cannot be interpreted in terms of our two-parameter model.
Obviously, more complex models should be developed for the adequate description of real correlated systems.

We acknowledge to Dr. S.V. Denisov who drew our attention to the exposed problem, Yu.L. Rybalko for consulta-

tions and kind assistance in the numerical simulations, S.S. Mel’nik and M.E. Serbin for the helpful discussions.

[1] H.E. Stanley et. al., Physica A 224,302 (1996).
[2] A. Provata and Y. Almirantis, Physica A 247, 482 (1997).
[3] A. Czirok, R.N. Mantegna, S. Havlin, and H.E. Stanley, Phys. Rev. E 52, 446 (1995).
[4] W. Li, Europhys. Let. 10, 395 (1989).
[5] R.F. Voss, in: Fundamental Algorithms in Computer Graphics, ed. R.A. Earnshaw (Springer, Berlin, 1985) p. 805.
[6] M.F. Shlesinger, G.M. Zaslavsky, and J. Klafter, Nature (London) 363, 31 (1993).
[7] C.V. Nagaev, Theor. Probab. & Appl., 2, 389 (1957) (In Russian).
[8] I. Kanter and D.F. Kessler, Phys. Rev. Lett. 74, 4559 (1995).
[9] M.I. Tribelsky, Phys. Rev. Lett. 87, 070201 (2002).
[10] A. Schenkel, J. Zhang, and Y.C. Zhang, Fractals 1, 47 (1993).


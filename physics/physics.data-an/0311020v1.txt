3
0
0
2
 
v
o
N
 
5
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
2
0
1
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Peak ﬁnding through Scan Statistics

Francesco Terranova a,∗

aLaboratori Nazionali di Frascati dell’INFN, Frascati (Roma), Italy

Abstract

We discuss the conditions under which Scan Statistics can be fruitfully implemented
to signal a departure from the underlying probability model that describes the
experimental data. It is shown that local perturbations (“bumps” or “excesses” of
events) are better dealt within this framework and, in general, tests based on these
statistics provide a powerful and unbiased alternative to the traditional techniques
related with the χ2 and Kolmogorov distributions. Approximate formulas for the
computation of Scan Statistics in the range of interest for high energy and nuclear
physics applications are also presented.

Key words: scan statistics, hypothesis testing
PACS: 06.20.Dk

1 Introduction

In the last two decades, the properties of “scan statistics” [1,2] have been
extensively investigated and this subject still represents a rapidly developing
branch of applied probability. Nowadays, scan statistics are used in several
areas of science and engineering to analyze the occurrence of cluster of events
and to assess their statistical signiﬁcance. Applications range from control
theory to molecular biology while, at present, their use in physics is mainly
limited to the analysis of time series, especially in x and γ-ray astronomy [3].
In fact, a common problem in nuclear and particle physics is to determine
whether an observed cluster of events has occurred by chance or if it signals a
departure from the underlying probability model (“null hypothesis”) for the
data. Peaks or local excesses of events can appear during energy scans at
the colliders, in the distributions of kinematical variables as invariant masses
or four-momentum transfer, in the interpretation of Dalitz plots, etc. The

∗ Corresponding author.

Preprint submitted to Elsevier Science

2 February 2008

traditional test statistics that are employed to challenge the null hypothesis
after a data taking can be divided into two broad families. Binned goodness-
of-ﬁt tests are connected to the χ2 distribution. If the region where the excess
is expected is known a priori because it has already been observed by another
experiment, i.e. a conﬁrmatory experiment against a claim of discovery has
been performed, the level of agreement between the null hypothesis and the
data can be evaluated in a straightforward manner through a Pearson χ2
test [4]. For one-dimensional distributions of the variable x (x ∈ [A, B]), let
the number of observations in the candidate signal region [a, b] be kab. The
best estimate of the background in the region [a, b] is given by

ˆBab =

B(x, ˆθ) dx

b

Za

Tab ≡

(kab − ˆBab)2
V (kab − ˆBab)

Tab ≃

(kab − ˆBab)2
ˆBab

where θ is the set of parameters describing the background distribution and
ˆθ is its best estimate based on events outside the interval [a, b], so that
cov(kab, ˆBab) = 0. Hence, the test variable can be deﬁned as:

V (kab− ˆBab) being the variance of kab− ˆBab. Under the null hypothesis V (kab) =
ˆBab and V (kab − ˆBab) = ˆBab + ˆσ2
ab, i.e. the variance is the quadratic sum of
the estimated background rate in the signal region and its error. Finally, if the
error on the background is negligible

and, in the asymptotic limit, the test variable behaves as χ2(1). Clearly, if the
bin number Nbin and the bin size w ≡ (B − A)/Nbin are speciﬁed in advance
but no information on the position, size and width of the signal are available,
the corresponding test is given by

T ≡

Nbin

Xi=1

(ki − bi)2
bi

2

(1)

(2)

(3)

(4)

which behaves as χ2(N) in the asymptotic limit. The power 1 of this test
depends on the position and width of the signal compared to the binning
since a cluster shared among several bins becomes harder to be detected while
clusters appearing in too large bins are swamped by background. If the size
and position of the bin is chosen after the inspection of the data, the power
of the test is increased but the estimate of the p-value 2 of the null hypothesis
inferred from χ2(N) is unreliable; hence, the signiﬁcance level of the hypothesis
test is biased. The binning problem can be overcome employing a second
family of test statistics connected to the Kolmogorov distribution. The most
common test is the Kolmogorov-Smirnov test [4] which corresponds to the
largest distance between the cumulative distribution of the data and the one of
the null hypothesis. This distance has a characteristic distribution that can be
computed analytically and, hence, provides the p-value of the null hypothesis
for a given data taking. These tests are well-suited to detect global distortions
of the x distribution but have limited power for strong local deviation from the
null hypothesis (cluster of events). Moreover, the power depends signiﬁcantly
on the position of the signal peak.

The procedure used to seek for event clusters after the data taking suggests
a possible alternative to those methods, but which does not suﬀer from the
problems of choosing the region a posteriori. The search is performed scanning
the [A, B] interval to identify the region where an anomalous accumulation of
events appears. Given N events distributed along the [A, B] range, we call
S(w) the largest number of events in a window of ﬁxed length w. If the dis-
tribution of S(w) is known, it will be possible to compute the probability
P rob(S(w) ≥ k) for the null hypothesis to produce a cluster S(w) greater
or equal than the one actually observed. Hence, the p-value of the null hy-
pothesis can be assessed. In this context, an a priori binning similar to the
one of the Pearson χ2 test is no more needed. Moreover, the test statistics
S(w) (“scan statistics”) is not aﬀected by the drawbacks of the Kolmogorov-

1 The power of an hypothesis test against a speciﬁc alternative hypothesis is the
chance that the test correctly rejects the null hypothesis when that alternative
hypothesis is true; that is, the power is 100% minus the chance of a Type II error
when that alternative hypothesis is true.
2 For a null hypothesis H0 and a test statistic T (e.g. the one of Eq. (4) ) we deﬁne
g(T |H0) as the p.d.f. of T in the occurrence of H0. The p-value is deﬁned as the
probability to ﬁnd T in the region of equal or less compatibility with H0 than the
level of compatibility observed with the actual data [5]. For example, if T is deﬁned
such that large values correspond to poor agreement with the hypothesis, then the
p-value will be

Tobs being the value of the test statistic obtained in the actual experiment.

+∞

ZTobs

g(T |H0)dT

3

Smirnov (K-S) tests (see Sec. 3). Sometimes this approach is followed, at least
qualitatively, in literature. For instance, the OPAL [6] data accumulated at
LEP during the high energy run beyond the Z 0 resonance (LEP2) were used
to falsify the ALEPH [7] claim of a peak in the dijet invariant mass M of
the e+e− → four jets ﬁnal state at M ≃ 105 GeV. Clearly, this refutation
was carried out using the test statistics of Eq. (3). Moreover, to test for a
peak in the dijet mass sum distribution for arbitrary mass M and indepen-
dent of histogram binning, the positions of the mass windows were scanned
over the full range of M. However, no quantitative statement was drawn due
to the strong correlations of the contents of nearby bins. In fact, accounting
for this correlation is possible once the properties of the scan statistics S(w)
are determined. In Sec. 2 these properties are discussed and the formulas to
compute P rob(S(w) ≥ k) are presented. The power and signiﬁcance of the
test statistics S(w) is computed in Sec. 3 and compared with the Pearson χ2
and K-S approach for one-dimensional distributions. Extensions of the tests
based on S(w) and further applications in particle physics data analyses are
discussed in Sec. 4.

2 Scan statistics

Let us consider an interval [A, B] of a continuous variable x and a Poisson
process (“background”) whose mean value per unit interval is denoted with
λ. Hence, the probability of ﬁnding Yx(w) events in an interval [x, x + w] is

P rob(Yx(w) = k) = e−λw (λw)k
k!

;

k = 0, 1, 2, . . .

(5)

The number of events in any disjoint non-overlapping intervals are indepen-
dently distributed. We call “scan statistic” (SS) the largest number of events
to be found in any subinterval of [A, B] of length w 3 , i.e.

S(w) ≡ max

{Yx(w)}

A≤x≤B−w

The probability that the number of events in a scanning window never reaches
k will be denoted, following [1], as

Q∗(k, λ∆, w/∆) ≡ 1 − P rob(S(w) ≥ k)

3 The case of non-uniform background can be dealt with by allowing for a window
of variable width w(x) that always contains w/(B − A) percent of the expected
events under the null hypothesis [8].

(6)

(7)

4

where ∆ ≡ B − A and the suﬃx “*” indicates that unconditional probabilities
are considered, i.e. that the overall number of events N in the interval [A, B]
is not ﬁxed but it ﬂuctuates according to Eq. (5) with w = ∆. The exact
form of Eq. (7) can be expressed in terms of the sum of products of two
determinants [9]. The summation is over the set V of all the partitions of N
into 2H+1 non-negative integers mi satisfying mi+mi+1 < k for i = 1, . . . , 2H,
where H is the largest integer in ∆/w. The determinants are computed starting
from the (H + 1) × (H + 1) matrix {h}ij and the H × H matrix {v}ij whose
entries are:

hij =

ms − (i − j)k

1 ≤ j ≤ i ≤ H + 1

= −

ms + (j − i)k

1 ≤ i < j ≤ H + 1

vij =

ms − (i − j)k

1 ≤ j ≤ i ≤ H

= −

ms + (j − i)k

1 ≤ i < j ≤ H

2i−1

Xs=2j−1
2j−2

Xs=2i
2i

Xs=2j
2j−1

Xs=2i+1

Using these deﬁnitions for V , hij and vij, we have for k ≥ 2 and w < ∆:

Q∗(k, λ∆, w/∆) =

R∗ det|1/hij!| det|1/vij!|

XV

In formula (8)

R∗ = N! dM (

− d)N −M p(N, λ∆)

w
∆

M =

m2j+1

H

Xj=0

being d ≡ 1 − wH/∆ and p(N, λ∆) is the Poisson probability of having N
events from an average rate λ∆.

A very useful approximation of Eq. (8) has been derived by Naus in 1982 [10],
based on the exact values of the probabilities Q2 ≡ Q∗(k, 2ψ, 1/2) and Q3 ≡
Q∗(k, 3ψ, 1/3)) 4 . It can be shown that

Q∗(k; ψL, 1/L) ≃ Q∗

2 [Q∗
4 For later convenience we deﬁne ψ ≡ λw and L = ∆/w.

3/Q∗

2]L−2

5

(8)

(9)

(10)

(11)

(12)

(13)

(14)

where

and

Q∗

2 = [F (k − 1, ψ)]2 − (k − 1) p(k, ψ)p(k − 2, ψ)

− (k − 1 − ψ) p(k, ψ)F (k − 3, ψ)
3 = [F (k − 1, ψ)]3 − A1 + A2 + A3 − A4

Q∗

A1 = 2 p(k, ψ)F (k − 1, ψ) {(k − 1) F (k − 2, ψ) − ψF (k − 3, ψ)}
A2 = 0.5 [p(k, ψ)]2 {(k − 1) (k − 2) F (k − 3, ψ)
−2 (k − 2) ψF (k − 4, ψ) + ψ2F (k − 5, ψ)
k−1

o

A3 =

p(2k − r, ψ) [F (r − 1, ψ)]2

A4 =

p(2k − r, ψ)p(r, ψ) {(r − 1) F (r − 2, ψ) − ψF (r − 3, ψ)}

Xr=1
k−1

Xr=2

In the above formulas F (k, ψ) denotes the cumulative distribution

F (k, ψ) =

p(i, ψ) ; p(i, ψ) = e−ψ ψi
i!

k

Xi=0

and F (k, ψ) = 0 for k < 0. For large values of ∆/w an even simpler approxi-
mation due to Alm [11] can be implemented:

Q∗(k, λ∆, w/∆) ≃

F (k − 1, λw) exp

−

λ(∆ − w) p(k − 1, λw)

(15)

)

k − wλ
k

(

Eq. (15) is often used in astrophysics applications and in many time series
problems but it is of limited use in the present case where the condition w/∆ ≪
1 is rarely fulﬁlled. In the following, the test statistics based on SS will be
studied relying on the approximation (11). For a systematic comparison of
the various approximations of Eq. (8) we refer to [1].

6

3 Power and signiﬁcance for one-dimensional distributions

Sec. 2 dealt with the distribution of the scan statistics under the null hypoth-
esis. The class of alternative hypotheses considered hereafter describe a local
perturbation of the uniform distribution which leads to the appearance of a
“excess” of events. Long-range distortions like anomalous angular distribu-
tions are better dealt with global K-S tests and will not be further considered
here. The alternative functions are Poisson processes of mean S. The signal
events are spread along [A, B] according to a normal distribution of mean xS
5 . In the following we reject the null hypothesis if its p-value is
and sigma σS
smaller than 5%. The actual signiﬁcance of the test statistics, i.e. the number
of experiments where the null hypothesis was rejected albeit true, has been
computed by Monte Carlo experimentation. Similarly the rate of Type II er-
rors was computed to estimate the power of the test. In general, some prior
assumptions are made before the inspection of a distribution. The domain
[A, B] of the variable x accessible to the experiment depends on the particular
apparatus and, in most of the cases, it is selected a priori; so it is not expected
to be a source of biases. In fact, in many applications the ﬁnal distribution
of x is the result of sequential cuts on other kinematic variables which can
severely bias the sample [12]. For narrow resonances, whose width does not
exceed the (known) instrumental resolution of the detector, the scanning win-
dow w of S(w) can be ﬁxed a priori. In particular, for a gaussian perturbation
of variance σ2

S a nearly optimal choice of w is w ≃ 4σS [13].

Fig. 1 shows the power of the K-S, SS and χ2 tests as a function of the
signal position xS. Here, [A, B] = [0, 1], σS = 0.05, B ≡ λ∆ = 100 and
S = 20. The optimal bin size for the χ2 test has been computed following the
prescription [14] Nbin = 2(λ∆)2/5, where λ∆ is the expected sample size in
case of null hypothesis 6 . In Fig. 1 signal events generated beyond the interval
[0, 1] are ignored (out of the sensitivity region [A, B] ). The power averaged
over the peak positions is shown in Fig. 2 as a function of S. A few comments
are in order. As anticipated in Sec. 1 the K-S test is not appropriate for
local perturbations. The power is limited compared to other statistics and
depends on the peak position, having the highest sensitivity at the border of
the distribution. The Pearson χ2 test has a much higher power but in general
the peak detection eﬃciency is reduced when the peak is shared between

5 This is the case, for instance, of a narrow resonance whose intrinsic width is
smaller than the detector resolution. For resonances broader than the instrumental
precision a relativistic Breit-Wigner or a Jacobian-peak would be more appropriate.
However, for the present purposes the details of the alternative function are not
critical.
6 Other choices of the binning for the χ2 test, based on the knowledge of σS, have
been tested by Monte Carlo experimentation. The corresponding powers do not
exceed the one shown in Fig. 1.

7

r
e
w
o
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

K-S
χ2

SS

0.9
xS

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Fig. 1. The power of the test statistics versus the peak positions for S = 20 and
B ≡ λ∆ = 100.

two adjacent bins. On average the χ2 test underperforms w.r.t. SS since the
correlations among the bins are ignored 7 . However, the bin prescription for
χ2 is independent of the a priori knowledge of σS while SS makes use of this
additional information. This is a drawback for SS if the cluster width is broader
than the instrumental resolution because the scanning window is no more
optimized. Fig. 3 shows the average power versus σS assuming w = ∆/Nbin
and S = 20. The vertical line corresponds to w = 4σS. In fact, it is possible
to compute the scan statistics for an a posteriori choice of w [15] but, clearly,
this additional degree of freedom implies a deterioration of the power.

On the other hand, SS has a relevant feature which is not manifest in Figs. 1,2.
The test statistics (4) behaves as χ2(N) only in the asymptotic limit. This im-
plies that the prescription Nbin = 2(λ∆)2/5 is appropriate only if the number
of expected events per bin is such that the normal limit is justiﬁed. If this is
not the case, the extraction of the p-value for the null hypothesis under the
assumption T ∼ χ2(N) is biased and the proper behavior has to be restored
increasing the bin size or computing the correct p-values by MC experimen-
tation [16]. This fact is immaterial for SS, since the derivation of Eqs. (8)
and (11) does not invoke the Central Limit theorem. The unbiaseness of the
p-value for the null hypothesis even for few events expected in the scanning

7 This is the reason why the χ2 test and the Run Test [4] are complementary.

8

r
e
w
o
P

1
0.9
0.8
0.7

0.6

0.5

0.4

0.3

0.2

0.1

K-S
χ2

SS

0

5

10

15

20

25

30
S (events)

Fig. 2. The power of the test statistics averaged over the peak positions versus the
mean expected signal S.

window has been checked by Monte Carlo experimentation. Fig. 4 shows the
probability P rob(S(w) ≥ k) − P rob(S(w) ≥ k + 1) of ﬁnding exactly k events
after a scan, computed by Monte Carlo (crosses) and by Eq. (11). The up-
per plot shows the region with highest probability assuming B = 10, S = 0,
w = 0.2; in this case the corresponding χ2 test with optimal binning would
have no more than 2 events per bin. The number of trials is 107 so the MC
error in the upper plot is negligible. The lower plot indicates the tail of the
distribution. Note that the exact formula on which the Naus approximation
is based holds for k > 1. Biases in the p-value will appear only when the
approximation

P rob(S(w) = 0) ≃ 0 =⇒ P rob(S(w) = 1) ≃ 1 − P rob(S(w) ≥ 2)

(16)

does not hold, that is when the probability of having zero events after a full
scan is non-negligible as in Fig. 5 where λ∆ = 2 and the ﬁrst empty dot
indicates P rob(S(w) = 0 or S(w) = 1) = 1 − P rob(S(w) ≥ 2).

9

r
e
w
o
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

K-S

χ2

SS

σ

S

0

0.02

0.04

0.06

0.08

0.1

Fig. 3. The power of the test statistics averaged over the peak positions versus
the signal width σS for B = 100 and S = 20. For the Pearson χ2 test, optimal
binning Nbin is assumed; for SS w = ∆/Nbin = 0.08. The vertical line corresponds
to w = 4σS.

4 Extensions of the Scan Statistics

In Sec. 3 the alternative hypotheses (normal distributions centered at xS ∈
[A, B]) were such that events generated outside the sensitivity region [A, B]
have been ignored. This implies a loss of power for the SS in the case of xS
lying near the border of the sensitivity region (see Fig. 1), i.e. when xS − A or
B − xS is comparable or smaller than the scanning window w. Clearly, these
results can be extended in a straightforward manner to a bounded variable x
on a range [A, B] where the signal accumulates at the border. In this case the
probability of ﬁnding a signal event between x and x + dx is

P rob(ˆx ∈ [x, x + dx]) =

G(x, xS, σS)

: x ∈ (A, B)

(17)

I (−∞,A) δ(x − A)

: x = A

I (B,∞) δ(B − x)

: x = B






10

)
k
=
)
w
(
S
(
P

0.35

0.3

0.25

0.2

0.15

0.1

0.05

)
k
=
)
w
(
S
(
P

-2

-3

-4

-5

-6

-7

-8

10

10

10

10

10

10

10

2

3

4

5

6

7

8

+  MC

Naus

+  MC

Naus

k

k

9

10

11

12

13

14

15

Fig. 4. The probability of seeing k events after a scan with B = 10, w = 0.2 under
the null hypothesis, computed by MC (crosses) and Eq. (11) (empty dots). The
high-k tail of the distribution is shown in the lower plot.

and zero for x outside [A, B]; G(x, xS, σS) is the normal distribution with mean
xS and variance σ2

S and

I (a,b) ≡

G(x, xS, σS) dx

b

Za

(18)

On the other hand, in many physics applications the alternative hypothe-
sis (17) does not describe our signal expectation. For instance, if the bounded
variable is connected with an angular distribution, a signal excess will mani-
fest as a local perturbation of an uniform distribution of events along a unit

11

)
k
=
)
w
(
S
(
P

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

2

3

4

5

k

Fig. 5. The probability of seeing k events after a scan with B = 2, w = 0.2 under
the null hypothesis, computed by MC (crosses) and Eq. (11) (empty dots). The
ﬁrst empty dot indicates the probability of having k < 2 events after a full scan:
1 − P rob(S(w) = k ≥ 2).

circle. Event positions are described by the bounded variable θ ∈ [0, 1]. In this
case Sc(w) is deﬁned as the maximum number of points in any arc of length
w and, following the notation of Sec. 2 with ∆ = 1, we have [10]:

P rob(Sc(w) ≥ k) ≃ 1 − Q∗(k, 4ψ, 1/4)

[Q∗(k, 3ψ, 1/3)]L−2
[Q∗(k, 2ψ, 1/2)]L−1

(19)

where Q∗(k, 3ψ, 1/3) and Q∗(k, 2ψ, 1/2) are given by Eqs. (12) and (13) and
Q∗(k, 4ψ, 1/4) can be derived from Eq. (8) with L = 4.

Note also that in Sec. 3 we considered the parameters describing the back-
ground known with high precision, so that it is possible to assume the null
hypothesis to be fully speciﬁed. This is not the case if the parameters θ of
Eq. (1) have to be estimated after the data taking. In this case SS should be
extended to devise the optimal estimate of the underlying background density
B(x, θ) that is unbiased and consistent under both the null hypothesis and the
occurrence of a local excess of width σS. This problem is still unsolved [1] for
a generic function B(x, θ). Unbiased estimators have been obtained for simple
functional dependences as in the case of the linear regression: for a discussion
we refer to [1].

12

Finally, it is worth mentioning that, even if we focused on 1-dim distributions,
SS has been extended to multivariate problems [17,18,19]. In particular, 2-dim
applications are quite common e.g. in space analysis of arrival direction data of
high energy cosmic rays, x and γ-ray bursts [3]. A description of multivariate
unconditional scan statistics can be found in [1,18].

5 Conclusions

In this paper, we considered the conditions under which Scan Statistics can be
implemented to signal a departure from the underlying probability model that
describes the experimental data. In fact, local perturbations (“bumps” or “ex-
cesses” of events) are better dealt within this framework and, in general, tests
based on S(w) provide a powerful and unbiased alternative to the traditional
techniques related with the χ2 and Kolmogorov distributions. This holds in
particular if the widths of the resonances are known a priori, e.g. when the
event distribution is dominated by the instrumental resolution. Approximate
formulas for the computation of SS in the range of interest for high energy
and nuclear physics applications have been provided. Possible extensions to
bounded variables and multivariate problems were also discussed.

I’m greatly indebted with L. Lyons, F. Ronga and T. Tabarelli de Fatis for
useful discussions and careful reading of the manuscript.

Acknowledgements

References

[1] J. Glaz, J. Naus and S. Wallenstein, Scan Statistics, Springer, New York, 2001.

[2] J. Glaz and N. Balakrishnan, Scan Statistics and applications, Birkhauser,

Boston, 1999.

[3] K.J. Orford, J. Phys. G26 (2000) R1.

[4] W.T. Eadie, D. Drijard, F. James, M. Roos, B. Sadoulet, Statistical methods in

experimental physics, North-Holland, Amsterdam, 1971.

[5] K. Hagiwara et al., Phys. Rev. D66 (2002) 010001.

[6] K. Ackerstaﬀ et al. [OPAL Collaboration], Phys. Lett. B429 (1998) 399.

[7] D. Buskulic et al. [ALEPH Collaboration], Z. Phys. C71 (1996) 179.

13

[8] M. Weinstock, Int. J. Epidem. 10 (1981) 289.

[9] R.J. Huntington and J. I. Naus, Ann. Prob. 3 (1975) 898.
S. Wallestein and J. I. Naus, Ann. Prob. 1 (1973) 188.

[10] J.I. Naus, J. Amer. Stat. Ass. 77 (1982) 177.

[11] S.E. Alm, On the distribution of scan statistics in a Poisson process,

in
Probability and mathematical statistics, A. Gut and L. Helst eds., Upsalla
University Press, Upsalla, 1983.

[12] P.F. Harrison, J. Phys. G28 (2002) 2679.

[13] N. Cressie, J. App. Prob. 14 (1977) 272.

[15] N. Nargawalla, Stat. Med. 15 (1996) 845.

[16] J. Heinrich, CDF note 6438 (2003).

[14] D. Moore, Test of Chi Squared type,

in Goodness of ﬁt

techniques,

R.B. D’Agostino and M.A. Stephens eds., Dekker, New York, 1986.

[17] C. Loader, AT&T Bell Labs, Technical Memorandum 11214-901025-15TM.

C. Loader, Adv. Appl. Prob. 23 (1991) 751

[18] S.E. Alm, Approximations of the distributions of scan statistics of Poisson

processes, in [2].

[19] P. Auer, K. Hornik and P. Revesz, Stat. Prob. Lett. 12 (1991) 91.

14


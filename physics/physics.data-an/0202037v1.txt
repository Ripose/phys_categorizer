2
0
0
2
 
b
e
F
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
3
0
2
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Products of Random Matrices

A. D. Jackson and B. Lautrup
The Niels Bohr Institute
Copenhagen, Denmark
P. Johansen
The Institute of Computer Science
University of Copenhagen, Denmark
and
M. Nielsen
The IT-university of Copenhagen, Denmark

February 2, 2008

Abstract

We derive analytic expressions for inﬁnite products of random 2

2 matrices. The
determinant of the target matrix is log-normally distributed, whereas the remainder is a
surprisingly complicated function of a parameter characterizing the norm of the matrix
and a parameter characterizing its skewness. The distribution may have importance
as an uncommitted prior in statistical image analysis.

×

1. Introduction: Considerable eﬀort has been invested over the last half century
in determining the spectral properties of ensembles of matrices with randomly chosen
elements and in discovering the remarkably broad applicability of these results to
systems of physical interest. In spite of a similarly rich set of potential applications
(e.g. in the statistical theory of Markov processes and in various chaotic dynamical
systems in classical physics), the properties of products of random matrices have
received considerably less attention. See ref. [1] for a survey of products of random
matrices in statistics and ref. [2] for a review of physics applications.

The purpose of the present manuscript is to consider in some detail the limit for

N

→ ∞

of the ensemble of matrices

(1)

Y =

1 +

X1

1 +

τ
N

τ
N

X2

1 +

· · ·

τ
N

XN

(cid:18)

(cid:19)

(cid:19)

(cid:18)

×

r

r

(cid:19) (cid:18)

r
where τ > 0 is a real parameter and the Xn are real d
d matrices with all elements
drawn at random on a distribution of zero mean and unit variance. If this distribution
has compact support, the probability that the matrix Y should become non-positive
deﬁnite vanishes for N
In one dimension, d = 1, it is well-known from the
law of large numbers that log Y has a Gaussian distribution, but because of the non-
commutativity of matrix products, the distribution is much more complicated for
d

2.
In this paper we derive some general properties for the limiting distribution

(Y )
and determine it explicitly for d = 2. In section 2 we establish a compact diﬀusion
equation for the distribution valid for any d. In section 3 we derive a simple expression

.
→ ∞

≥

P

1

for any average over the distribution, and we show that the determinant det [Y ] has
a log-normal distribution. Sections 4 and 5 will be devoted to the determination of
the explicit form of
for d = 2. We shall ﬁrst write the diﬀusion equation using
an appropriate parameterization of Y . The resulting partial diﬀerential equation will
(Y ) supports only the identity
then be solved subject to the boundary condition that
matrix in the limit of τ
0. This explicit solution will require new integrals involving
Jacobi functions. The derivation of these integrals will be given in the Appendix.

→

P

P

2. The diﬀusion equation: The normalized probability distribution is (for
given N and variable τ )

PN (Y, τ ) =

Y

δ
*

"

−

N

1 +

n=1 (cid:18)
Y

r

τ
N

Xn

(cid:19)#+X1,... ,XN

where the integrand is a product of δ-functions for each matrix element of Y and the
average runs over all the random matrices. Pealing oﬀ the N th factor in the product
and using only that the Xn are statistically independent, we derive the following exact
recursion relation

PN (Y, τ ) =

*

det

1 +

(cid:20)

r

(cid:21)

−d

τ
N

X

Y

1 +

PN −1

"

(cid:18)

r

(cid:19)

τ
N

X

N

, τ

1
−
N #+X

−1

where the average is over the N th random matrix, here renamed X. The determinantal
prefactor of

PN −1 is the Jacobi determinant arising from the general matrix rule
δ[Y M −1

Z]

δ[Y

ZM ] =

−

−
∂(ZM)
∂Z

det

h

i

with M = 1 +

τ /N X. Since

p

∂(ZM )ij
∂Zkℓ

= δikMℓj ,

(2)

(3)

(4)

(5)

the Jacobian is block diagonal with d identical blocks, and the prefactor follows.

The recursion relation (3) is of the Markovian type with the initial distribution
under very general conditions (which
(y, τ ) = limN→∞ PN (y, τ ).
(1/N ) and using the fact that all the matrix

1]. It converges for N
P0(Y, τ ) = δ[Y
we shall not discuss here) towards a limiting distribution
Expanding the recursion relation to
O
elements of X are statistically independent with zero mean and unit variance,

→ ∞

−

P

Xij iX = 0
h

XijXkliX = δikδjl ,
h

(6)

we obtain to leading order

PN =

PN −1 +

τ
N

∂

PN −1
∂τ

1
2

+

d2(d + 1)

PN −1

+(d + 1)Yij

+

YikYjk

∂2
PN −1
∂Yiℓ∂Yjℓ (cid:19)

−

(cid:18)
∂

PN −1
∂Yij

1
2

2

with implicit summation over all repeated indices. The assumed convergence towards
a limiting distribution requires the expression in the parenthesis to vanish in the limit,
so that

∂
P
∂τ

1
2

=

d2(d + 1)

+ (d + 1)Yij

+

YikYjk

∂
P
∂Yij

1
2

∂2
P
∂Yiℓ∂Yjℓ

.

P

(7)

This is a diﬀusion equation of the Fokker-Planck type with τ playing the role of time.
It must be solved subject to the initial condition that

(y, 0) = δ[Y

1].

Both the diﬀusion equation and the initial condition are invariant with respect to an
M ⊤Y M , where M is an orthogonal matrix satisfying
orthogonal transformation Y
M ⊤M = 1. Since the number of free parameters in an orthogonal transformation is
1
1) =
2 d(d
1
2 d(d + 1). Since the distribution only has support for det [Y ] > 0, this number
consists of d independent eigenvalues and 1
1) rotation angles in a singular value
decomposition.

1), the number of “dynamic” variables in the distribution is d2

1
2 d(d

2 d(d

→

−

−

−

−

P

−

For d = 1 the solution to (7) which approaches δ[Y

1] for τ

0 is

−

→

Pd=1(Y ) =

1
Y √2πτ

exp

(log Y + τ /2)2
2τ

.

(cid:21)

−

(cid:20)

As expected, it is a log-normal distribution.

3. Averages: Remarkably, equation (7) may be written in the much simper form

without any explicit reference to d. Deﬁning the average of a function f (Y ) by

with dY =

ij dYij , we obtain from (9)

Q

f
∂
h
∂τ

1
2

(cid:28)

i

=

YikYjk

∂2f
∂Yiℓ∂Yjℓ (cid:29)

.

This equation permits in principle the determination of the moment of any product
of matrix elements. The ﬁrst two are found to be

(8)

(9)

(10)

(11)

(12)

(13)

The exponential growth of the averages with “time” τ is a consequence of the multi-
plicative nature of the problem.

The determinant D = det [Y ] is, according to the deﬁnition of the product (1),
an inﬁnite product of random real numbers that converge towards unity, and log D
must have a Gaussian distribution according to the law of large numbers. Its mean

∂
P
∂τ

=

1
2

∂2(YikYjkP
∂Yiℓ∂Yjℓ

)

=

f
h

i

Z

f (Y )

(Y ) dY

P

Yiji
h
Yij Ykli
h

= δij
= eτ dδikδjl

3

and variance are, however, diﬀerent from those of the one-dimensional distribution
(8). The distribution of the determinant is also an average

Using the fact that

F (D) =

δ

D

det [Y ]

.

−

(cid:10)

(cid:0)

(cid:1)(cid:11)

∂det [Y ]
∂Yij

= det [Y ] Y −1

,

ji

we obtain the following equation for F

∂F
∂τ

=

1
2

d

∂2(D2F )

∂D2 = d

F + 2D

(cid:18)

∂F
∂D

+

1
2

D2 ∂2F
∂D2

.

(cid:19)

Apart from the factor d in front, this is identical to the diﬀusion equation (9) in one
dimension. Consequently the determinant has a log-normal distribution

F (D) =

1
D√2πτ d

exp

(log D + τ d/2)2
2τ d

−

,

(cid:20)
which is obtained from (8) by replacing τ by τ d. The distribution has support only
It can be shown in general (and we shall demonstrate it
for positive values of D.
explicitly for d = 2 below) that the distribution of the determinant factorizes in

(cid:21)

.

4. The case d = 2: The ﬁrst non-trivial case is d = 2 where the general matrix
is ﬁrst parameterized using a quaternion or 4-vector notation

Y0 + Y3 Y1 −
Y1 + Y2 Y0 −
(cid:18)
In this representation the determinant becomes a metric with two “space” and two
“time” dimensions

Y2
Y3(cid:19)

Y =

(18)

.

D = Y 2

1 + Y 2
Y 2

Y 2
3 .

2 −

0 −

The structure of this expression and the positivity of D suggest the following param-
eterization in terms of one imaginary and two real angles

The Jacobi determinant of the transformation from
simply

Y0, Y1, Y2, Y3}

{

to

D, ψ, θ, φ
}

{

is

(14)

(15)

(16)

(17)

P

(19)

(20a)

(20b)

(20c)

(20d)

(21)

Y0 = √D cosh ψ cos θ
Y1 = √D sinh ψ cos φ
Y2 = √D cosh ψ sin θ
Y3 = √D sinh ψ sin φ .

J

D sinh ψ cosh ψ .

∼

4

Orthogonal 2
2 matrices are generated by the matrix
with Y2. Thus, an orthogonal transformation rotates the angle φ, and
be independent of φ as indicated above.

×

(cid:1)

(cid:0)

, which is associated
(Y, τ ) must

P

0 −1
1 0

In these variables the diﬀusion equation (7) simpliﬁes to

∂
P
∂τ

= 6

+ 6D

P
1
4

∂
P
∂D

+ D2 ∂2
P
∂D2
∂2
1
∂θ2 +
P
4

+

(1 + tanh2 ψ)

(tanh ψ + coth ψ)

+

(22)

∂
P
∂ψ

1
4

∂2
∂ψ2 .
P

Taking into account the factor of D in the Jacobi determinant, we replace the original
distribution
with the product of the determinant distribution F (D) given in (17)
and an as yet unknown function of ψ and θ,

P

1
D

P

=

F (D)G(ψ, θ) ,

(23)

and ﬁnd that G satisﬁes the diﬀusion equation

∂G
∂τ

1
4

=

(1 + tanh2 ψ)

∂2G
∂θ2 +

1
4

(tanh ψ + coth ψ)

+

(24)

∂G
∂ψ

1
4

∂2G
∂ψ2 .

The corresponding normalization integral is found from the Jacobi determinant,

2π

∞

0
Z

0
Z

dθ

dψ 2 sinh 2ψ G(ψ, θ) = 1 .

(25)

This normalization integrals (25) suggests that it is more convenient to employ still
another variable

z = cosh 2ψ =

1 + Y 2
0 + Y 2
Y 2
1 + Y 2
Y 2
Y 2
0 −

2 + Y 2
3
Y 2
3

2 −

.

With this variable the normalization integral takes the form

2π

∞

dθ

dz G(z, θ) = 1 ,

0
Z

1
Z

and the diﬀusion equation (24) becomes

∂G
∂τ

=

1
4

2z
z + 1

∂2G
∂θ2 + 2z

∂G
∂z

+ (z2

∂2G
∂z2 .

1)

−

(26)

(27)

(28)

→

(Y ) in the limit
0 reduces to a product of delta functions which select only the identity matrix.
1, and

This equation must be solved with the boundary condition that
τ
This evidently requires Y0 →
θ
−
→
G(z, θ)

0 and, consequently, D
1 and Y1,2,3 →
1), the initial condition takes the form

0. Since F (D)

1)δ(θ)

−
The limiting distribution should be approached from above (i.e. from z > 1).

(29)

δ(D

1, z

0) .

δ(z

→

→

→

→

→

(τ

P

5

The form of the diﬀusion equation (28) reveals that G may naturally be expanded

in a Fourier series

G(z, θ) =

Gn(z)einθ

∞

1
2π

n=−∞
X

with coeﬃcients that obey

∂Gn
∂τ

=

1
4

n2 2z
z + 1

−

Gn + 2z

∂Gn
∂z

+ (z2

∂2Gn
∂z2

.

1)

−

For the special case n = 0, we recognize Legendre’s diﬀerential operator on the right.
The normalization condition only aﬀects G0 and becomes

∞

0

Z

dz G0(z) = 1 .

Gn(z)

δ(z

1)

−

→

(τ

0)

→

The initial condition (29) implies that

for all n.

5. Explicit solution: All that remains is to determine the angular functions
Gn(z). One relatively simple way is to use Sturm-Liouville theory, and we now outline
the main steps in this procedure.

The diﬀerential operator (“Hamiltonian”) appearing on the right hand side of

eqn. (31) may be written

which shows that it is Hermitean. Let the spectral variable (which denumerates the
eigenvalues and may be both discrete and continuous) be denoted r, and let g(r)
n (z)
be the eigenfunction corresponding to the eigenvalue λ(r)
n ,

The Hermiticity of
tions are both orthogonal and complete on the interval 1

H

guarantees that the eigenvalues are real and that the eigenfunc-

z <

≤

,
∞

∂
∂z

(z2

1)

−

∂
∂z −

n2
4

2z
z + 1

,

=

H

g(r)
n (z) = λ(r)

n g(r)

n (z) .

H

∞

1

Z

dz g(r)

n (z)g(r

n g(r)
µ(r)

n (z)g(r)

′

)

n (z) =

δr,r′
µ(r)
n
n (z′) = δ(z

z′)

−

r
X
with a suitable measure, µ(r)
n .

form

The solution of the diﬀusion equation (31) with initial condition (33) takes the

Gn(z, τ ) =

n g(r)
µ(r)

n (1)g(r)

n (z) exp

λ(r)
n τ

.

(38)

r
X

(cid:16)

(cid:17)

6

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

In view of the completeness (37), these functions indeed satisfy the initial conditions
at τ = 0. The appearance of g(r)
n (1) in this expression requires the eigenfunctions to
be regular at z = 1.

We now present the complete solution of the eigenvalue problem. (Further details
are given in the Appendix.) The eigenvalue spectrum contains discrete values (for
n

2) as well as a continuum

≥

λ(r)
n =

1

2 n2
2 n2

1

1
4 +
1
4 −

−
−

−
−

(

t2
(cid:0)

n+1
2 −

k

2

(cid:1)

k = 1, 2, . . . ,
.
t <
0

≤

∞

,

n
2 ⌋

⌊

(n

2)

≥

(39)

The properly normalized discrete eigenfunctions are Jacobi polynomials

g(k)
n =

n + 1

2 −

r

1 + z
2

k

(cid:18)

(cid:19)

n/2

P (0,n)
−k

(z) ,

while the eigenfunctions in the continuum are Jacobi functions of complex index

with the measure obtained from the integral (36) as

g(t)
n =

n/2

1 + z
2

(cid:18)

(cid:19)

P (0,n)
−(n+1)/2+it(z)

µ(t)

n =

t tanh πt
t coth πt

(

n even
n odd .

The special case n = 0 was stated without proof by Mehler in 1881 [3]. The general
case is proven in the Appendix.

Since g(t)

n (1) = 1, the ﬁnal solution becomes a simple superposition of the discrete

and continuous contributions

Gn = Gdisc

n + Gcont

n

where the discrete contribution (for n

2) is

≥

Gdisc

n (z, τ ) =

1 + z
2

(cid:18)

(cid:19)

n/2 ⌊n/2⌋

n + 1

2 −

k

(cid:19)

Xk=1 (cid:18)

P (0,n)
−k

2

(z)e−(n

/2+1/4−((n+1)/2−k)

dt µn(t) P (0,n)

−(n+1)/2+it(z) e−(n

2

2
/2+1/4+t

)τ

(45)

The continuous contribution is

Gcont
n

(z, τ ) =

1 + z
2

n/2

∞

(cid:19)

0

Z

(cid:18)

×

with µn(t) given by (42). Thus, we arrive at the ﬁnal result. The probability for
drawing a given 2

2 matrix Y is

(Y, τ ) =

P

F (D)
2πD  

G0(z, τ ) + 2

Gn(z, τ ) cos nθ

(46)

!

∞

n=1
X

7

(40)

(41)

(42)

(43)

2)τ .

(44)

1

0.5

0
1
1

2

0

q

2
2

3
3

z
z

4
4

-2

5

Figure 1: Plot of G(z, θ) for τ = 1. Notice the characteristic log-normal tapering of the ridge as
a function of z, and the nearly Gaussian distribution in θ around θ = 0.

with F (D) given by eqn. (17) and Gn(z, τ ) given by eqns. (43–45). As noted previously,
the Gn(z, τ ) are independent of the sign of n so that
is manifestly real. In ﬁg. 1
the function G(z, θ) (the expression in parenthesis) is plotted for τ = 1.

P

6. Conclusions: We have analytically derived the distribution of an inﬁnite
product of random 2
2 matrices. In statistical image analysis, it may be used as an
uncommitted prior for morphing and warping [4], with desirable properties not shared
by the usual priors based on elastic membranes. The distribution of such matrices
may be evaluated numerically at a moderate cost in computer time and converges
reasonably fast because of the strong exponential damping.

×

8

7. Appendix: The Jacobi functions are related to the hypergeometric functions,

P (0,n)
−n/2−1/2+it(z) = 2F1

n + 1

2 −

it,

n + 1
2

+ it; 1;

1

z

−
2

(cid:19)

(cid:18)

with t real, and obey the orthogonality relation

∞

1 (cid:18)

Z

1 + z
2

n

(cid:19)

dz P (0,n)

−n/2−1/2+it(z) P (0,n)

−n/2−1/2+it′ (z) =

δ(t

t′)

−
µn(t)

(47)

(48)

In order to ﬁnd µn(t) for arbitrary n, it is helpful to consider the asymptotic form of
these functions by using the standard relation for hypergeometric functions

F (a, b; c; z) =

F

a, c

b; a

b + 1;

−

−

(1

−

(1

−

z)−a Γ(c)Γ(b
−
Γ(b)Γ(c
−
z)−b Γ(c)Γ(a
Γ(a)Γ(c

a)
a)
b)
b)

−
−

(cid:18)

(cid:18)

1

+

1

z
−
1

(cid:19)

1

z

(cid:19)

−

F

b, c

a; b

a + 1;

−

−

.

(49)

This form allows us to see that

P (0,n)
−n/2−1/2+it′ (z)

2

A(t)
|
|

→

z−n/2−1/2 cos (φt + t ln z)

(50)

as z

. Here,

→ ∞

A(t) =

Γ(2it)

2n/2+1/2−it ,

(51)

Γ(n/2 + 1/2 + it)Γ(

n/2 + 1/2 + it)

−
and φt is the phase of A(t). Using this asymptotic form, we can perform the integral
in eqn. (48) by using the variable u = log z, adding a convergence factor of exp (
µu),
and ﬁnally taking the limit µ

0. The result is simply

−

→

2

A(t)
|
|

(cid:20)

2µ
µ2 + (t

t′)2

(cid:21)

−

2−n .

(52)

The factor in brackets is a familiar representation of 2πδ(t
0.
Standard relations for the gamma function immediately yield eqn. (36). This conﬁrms
the results of Mehler [3] for the special case n = 0. The extension to n > 0 would
appear to be new.

t′) in the limit µ

→

−

References

[1] Richard D. Gill and Søren Johansen, Ann. Statist. 18 (1990) 1501.

[2] A. Crisanti, G. Paladin, and A. Vulpiani, Products of Random Matrices in Sta-

tistical Physics, Springer-Verlag, Berlin, 1993.

[3] F. G. Mehler, Math. Ann. XVIII (1881) 161.

[4] Manuscript in preparation.

9


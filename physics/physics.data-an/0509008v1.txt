5
0
0
2
 
p
e
S
 
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
0
0
9
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A General Theory of Goodness of Fit in

Likelihood Fits

Rajendran Raja

Fermi National Accelerator Laboratory

Batavia, IL 60510

Abstract

Maximum likelihood ﬁts to data can be performed using binned data and unbinned

data. The likelihood ﬁts in either case result in only the ﬁtted quantities but not

the goodness of ﬁt. With binned data, one can obtain a measure of the goodness

of ﬁt by using the χ2 method, after the maximum likelihood ﬁtting is performed.

With unbinned data, currently, the ﬁtted parameters are obtained but no measure of

goodness of ﬁt is available. This remains, to date, an unsolved problem in statistics.

By considering the transformation properties of likelihood functions with respect

to change of variable, we conclude that the likelihood ratio of the theoretically

predicted probability density to that of the data density is invariant under change

of variable and provides the goodness of ﬁt. We show how to apply this likelihood

ratio for binned as well as unbinned likelihoods and show that even the χ2 test

is a special case of this general theory. In order to calculate errors in the ﬁtted

quantities, we use Bayes’ theorem which then yields the surprising result that the

quantity generally considered the Bayesian prior is an uninteresting constant and

the resulting statistics is consistent with frequentist ideas.

Preprint submitted to Elsevier Science

30 March 2013

Contents

1

Introduction

1.1 To show that

cannot be used as a goodness of ﬁt variable

L

2

Likelihood ratios

2.1 The concept of “data likelihood” derived from the pdf of the data

2.2 Historical use of Likelihood Ratios

Normalizing the theoretical curve to the data

Binned Goodness of Fit

4.1 The multinomial distribution

4.2 Degeneracy of the distribution

3

4

5

6

4.3 To Show that the Binned Negative Log-Likelihood Ratio Approaches a χ2 Distribution for Large n

4.4 Normalizing theory and experiment and the problem of Goodness of ﬁt for the Poisson distribution

4.5 The Gaussian limit of the binomial

4.6 To show that χ2 is also the negative logarithm of a likelihood ratio

Unbinned Goodness of Fit

An illustrative example

6.1

Improving the P DE

6.2 Periodic Boundary Conditions

Email address: raja@fnal.gov (Rajendran Raja).

2

5

6

8

8

11

11

12

12

12

18

19

20

22

25

27

The distribution of the goodness of ﬁt variable

7

8

Calculation of ﬁtted errors

8.1 The concept of the pdf of a ﬁxed parameter

8.2 The true value of the parameter s

8.3 The unknowability of

Pn(s)

8.4 The posterior density P (s

~cn)
|

8.5 Derivation of Bayes’ theorem equations

8.6 Determination of the Posterior Density P (s

~cn)
|

8.7 The error bootstrap

8.8 New form of equations

8.9 The dependence of αn on n

9

Combining Results of Experiments

10

Interpreting the results of one experiment

11 Another Illustrative Example

11.1 One more iteration

12 The distribution of s∗ and the function

Pn(s)

13 Co-ordinate transformations s′ = s′(s)

14 Comparison with the Bayesian approach

15 Conclusions

3

30

31

31

32

32

33

33

37

38

40

43

43

44

44

47

48

51

52

53

16 Acknowledgments

17 Appendix

17.1 An extreme goodness of ﬁt example

17.2 An extreme problem

17.3 Goodness of ﬁt for the above problem

17.4 Comments

References

55

55

55

55

55

57

59

4

1 Introduction

In particle physics as well as other branches of science, ﬁtting theoretical mod-

els to data is a crucial end stage to the performance of experiments. Minimizing

the χ2 between theory and experiment is perhaps the most commonly used

form of ﬁtting, with data binned in histograms. Such ﬁts yield not only the

ﬁtted parameters and errors on the ﬁtted parameters but also a measure of

the goodness of ﬁt. Another common ﬁtting method is the maximum likeli-

hood method which can be performed on binned and unbinned data to obtain

the best values of theoretical parameters. In the case of unbinned likelihood

ﬁtting, there is currently no measure of the goodness of ﬁt. In this paper, we

propose a solution to the problem, which by its nature works generally for

both binned and unbinned likelihood ﬁts. A general theory of goodness of ﬁt

in likelihood ﬁts results.

In what follows, we will denote by the vector s, the theoretical parameters

(s for “signal”) and the vector c, the experimentally measured quantities or

“conﬁgurations”. For simplicity, we will illustrate the method where both s

and c are one dimensional, though either or both can be multi-dimensional in

practice. We thus deﬁne the theoretical model by the conditional probability

density P (c

s), deﬁned as the probability of observing c given a value of s.

|

The theoretical probability function obeys the normalization condition

P (c

s)dc = 1

Z

|

(1)

Then an unbinned maximum likelihood ﬁt to data is obtained by maximizing

5

the likelihood [1],

=

L

P (ci|

s)

i=n

Yi=1

where the likelihood is evaluated at the n observed data points ci, i = 1, n.

Such a ﬁt will determine the maximum likelihood value s∗ of the theoretical

parameters, but will not tell us how good the ﬁt is.

1.1 To show that

cannot be used as a goodness of ﬁt variable

L

The goodness of ﬁt variable must be invariant under a change of variable

c

→

c′. The value of the likelihood

at the maximum likelihood point does

L

not furnish a goodness of ﬁt, since the likelihood is not invariant under change

of variable. This can be seen by observing that one can transform the variable

set c to a variable set c′ such that P (c′

s∗) is uniformly distributed between 0

|
and 1. In one dimension, this is trivially done by the transformation function

c′(c) such that

c′(c) =

s∗)dt

P (t
|

c

Zc1

The variable c ranges from c1 to c2 and the probability function P (c

s∗) nor-

|

malizes to unity in this range. This implies that c′ ranges from 0 to 1. Such a

transformation is known as a hypercube transformation, in multi-dimensions.

The transformed probability distribution in the variable c′ is unity in this

interval as can be seen by examining the Jacobian of the transformation

(2)

(3)

∂c′
∂c |

|

(4)

(5)

∂c′
∂c |

|

= P (c

s∗)

|

∂c
∂c′ |

|

|

P (c′

s∗) = P (c

s∗)

= 1

|

6

Other datasets will yield diﬀerent values of likelihood in the variable space

c when the likelihood is computed with the original function P (c

s∗). How-

|

ever, in hypercube space, the value of the likelihood is unity regardless of the

dataset c′i, i = 1, n, thus the likelihood

cannot furnish a goodness of ﬁt by

L

itself, since neither the likelihood, nor ratios of likelihoods computed using

the same distribution P (c

s∗) is invariant under variable transformations. The

|

fundamental reason for this non-invariance is that only a single distribution,

namely, P (c

s∗) is being used to compute the goodness of ﬁt.

|

To illustrate further, we use a concrete example of ﬁtting a dataset using the

maximum likelihood method as shown in Figure 1(a). The ﬁtting is done in

the range c1 < c < c2, where c1 = 1.0 and c2 = 5.0. The ﬁtting function is

P (c

s) =

|

s(exp(

exp(
c1/s)

c/s)
exp(

−
−

−

−

c2/s))

(6)

which normalizes to unity in the range c1 < c < c2. The ﬁtted dataset is shown

as a full histogram. The dashed histogram shows a dataset that is a poor ﬁt

to the data and will produce a smaller value of

when ﬁtted as a function of

L

c. Figure 1(b) shows the same data in the hypercube space where the ﬁtted

function is ﬂat as per the transformation given in equation 3. Both the datasets

will produce a value of unity for

in this space implying an equally good ﬁt

in either case, which is obviously false. This clearly demonstrates that the

likelihood by itself cannot provide a goodness of ﬁt variable.

L

7

Fig. 1. (a) shows the ﬁtting in the dataset space. The curve shows the ﬁtted function.

Superimposed is the ﬁtted data, (full histogram, normalized to unity). The dashed

histogram shows the diﬀerent dataset which obviously does not ﬁt to the ﬁtted

curve. (b) The same plot in hyperspace. the ﬁtted function is ﬂat by construction.

Both the ﬁtted data set (full histogram) and the dashed histogram will have the

same value of likelihood

in this space which implies that

cannot be used as a

L

L

goodness of ﬁt variable.

2 Likelihood ratios

2.1 The concept of “data likelihood” derived from the pdf of the data

It is interesting to note that while using χ2 as the goodness of ﬁt technique for

binned histograms, we use two distribution functions, namely the theoretical

curve and the data. By binning the data, we are in eﬀect estimating the

probability density function of the data as the second distribution, in addition

to the theoretical distribution speciﬁed by the theoretical curve. In likelihood

language we deﬁne the probability density function (pdf ) of the data as

P data(c) = lim
→∞

n

1
n

dn
dc

8

(7)

which obeys the normalization condition

P data(c)dc = 1

Z

(8)

(9)

(10)

(11)

When one is using binned likelihoods, the pdf of the data would be estimated

by binning the events in a histogram and normalizing the sum of contents of

all bins to unity. In the unbinned case, we will describe below a technique [2]

on estimating P data(c) using Probability Density Estimators (P DE).

We can now deﬁne a likelihood ratio

such that

LR

=

LR

i=n

s)

i=1 P (ci|
i=n
i=1 P data(ci) ≡
Q

s)
P ( ~cn|
P data( ~cn)

Q

where we have used the notation ~cn to denote the dataset ci, i = 1, n.

Since the n events ci, i = 1, n are independent, the probability of obtaining

the dataset ~cn is given by

P data( ~cn) =

P data(ci)

i=n

Yi=1

The quantity P data( ~cn) we name the “data likelihood” of the dataset ~cn and

the quantity P ( ~cn|
that the “data likelihood” P data( ~cn) may also be thought of as the probability

s) as the “theory likelihood” of the dataset ~cn. We note

density of of the“ n

object” ~cn which obeys the normalization condition

−

P data( ~cn) d ~cn = 1

Z

Let us now note that

is invariant under a general variable transformation

LR
(not restricted to hypercube transformation) c

c′, since

→

9

P (c′

s) =

|

∂c
∂c′ |

P (c

s)

|

P data(c′) =

P data(c)

|
∂c
∂c′ |

|

=

′
R

L

LR

(12)

(13)

(14)

(15)

and the Jacobian of the transformation

cancels in the numerator and

∂c
∂c′

|

|

denominator in the ratio. This is an extremely important property of the

likelihood ratio

that qualiﬁes it to be a goodness of ﬁt variable. Since the

LR

denominator P data( ~cn) is independent of the theoretical parameters s, both

the likelihood ratio and the likelihood maximize at the same point s∗. The

likelihood ratios for two diﬀerent data sets ~cm and ~cn can be combined by

multiplication as per

m+n =

m

n

LR

× LR

LR

This rule follows from the deﬁnition of

in equation 9. In practice, we will

LR

use the negative log-likelihood ratio

logeLR as the goodness of
−
ﬁt variable and minimize it. The multiplication rule of equation 15 results

N LLR

=

in an addition rule for

. The problem of ﬁnding the distribution of

N LLR

for a good ﬁt then reduces to ﬁnding the distribution of

N LLR
hyper-cube space for a variable that is uniformly distributed between zero

N LLR

in

and one, as in Figure 1(b). This is because

is invariant under the

N LLR

transformation of variable. So all goodness of ﬁt problems using likelihood

ratios can be reduced to ﬁnding the distribution of

for a variable that

N LLR

is uniformly distributed in hypercube space.

10

2.2 Historical use of Likelihood Ratios

The Neyman-Pearson lemma [3] states that if one is trying to choose between

two hypotheses H0 and H1, then the cut on the likelihood ratio

LR =

P ( ~cn|
P ( ~cn|

H0)
H1)

> ǫ

(16)

will have the optimum power in diﬀerentiating between the hypotheses H0

and H1, where ǫ is a constant adjusted to obtain the desired purity in favor

of hypothesis H0. Notice that this likelihood ratio is between the likelihood

computed for two diﬀerent hypotheses H0 and H1. Our likelihood ratio diﬀers

fundamentally from this in that the denominator we use P ( ~Cn) is the “data

likelihood” that is computed from the distribution of the data and is not tied

to any hypothesis as such.

3 Normalizing the theoretical curve to the data

The method of maximum likelihood ﬁts the shape of the theoretical distribu-

tion to the data distribution. The theoretical model obeys the normalization

condition in equation 1 and the likelihood is evaluated at the number of ob-

served data events n. There is no explicit mention of the theoretically expected

number of events, which we denote by nt. Later we will show how to incor-

porate a goodness of ﬁt in the absolute normalization by making use of the

binomial distribution and its limiting cases the Poisson and the normal distri-

butions. We will begin by obtaining goodness of ﬁt formulae for the case where

we bin the data and ﬁt the theoretical shape to the experimental distribution.

11

4 Binned Goodness of Fit

When one bins data in histograms and ﬁts the theory shape to the data, one

can ﬁt by using either maximum likelihood or by minimizing χ2. In either

case, the goodness of ﬁt is usually evaluated using χ2. We now illustrate how

the likelihood ratio deﬁned in section 2 can be used to obtain a goodness of ﬁt

after the maximum likelihood ﬁtting is done. In order to evaluate the likelihood

ratio, one needs to evaluate the theory likelihood and the data likelihood for

each value of ci. For the binned histogram, we make the approximation of

assuming that both these quantities are constant for all values of ci in a given

bin and evaluating each at the bin center. Let there be nb bins and let the kth

The probability of obtaining the histogram is given by the multinomial distri-

bin contain nk entries.

4.1 The multinomial distribution

bution

P (histogram) =

k=nb

n!
k=nb
k=1 nk!

s)nk

P (ck|

Yk=1

k=nb

Q

nk = n

Xk=1

4.2 Degeneracy of the distribution

(17)

(18)

The factor

denotes the number of ways n events can be partitioned

to form the observed histogram, which we term the degeneracy

of the his-

Q

D

togram. Each of the

histograms is identical to each other and possesses the

n!
k=nb
k=1 nk!

D

12

same goodness of ﬁt. We can then evaluate the goodness of ﬁt for any one of

the

degenerate histograms, the likelihood for which is given by

D

=

L

k=nb

Yk=1

s)nk

P (ck|

and the likelihood ratio can be written as

LR =

k=nb

Yk=1  

nk

s)

P (ck|
P data(ck) !

The value of P (ck

s)

P data(ck)) is raised to the power nk in equation 20 results from the

|

fact that there are nk conﬁgurations ci in the kth bin and we are multiplying

a constant ratio (at the bin center) over nk conﬁgurations. If ∆ck is the bin

width for the kth bin, then the data likelihood can be approximated by

P data(ck)

nk
n∆ck

≈

This obeys the normalization condition

P data(ck)dck ≈

Z

k=nb

Xk=1

nk
n∆ck

∆ck = 1.

The theoretical likelihood can be integrated over the bin to yield

P bin average(ck|

s) =

1
∆ck

c=ck+∆ck/2

c=ck

Z
∆ck/2
−

P (c

s)dc

|

This obeys the normalization condition

k=nb

Xk=1

P bin average(ck|

s)∆ck = 1

Then the likelihood ratio can be written

=

LR

k=nb

Yk=1  

n∆ckP bin average(ck|
nk

s)

nk

k=nb

!

≡

Yk=1 (cid:18)

nk

Tk
nk (cid:19)

13

(19)

(20)

(21)

(22)

(23)

(24)

(25)

where Tk ≡
events in the kth bin obeying the normalization condition

n∆ckP bin average(ck|

s) is the theoretically expected number of

k Tk = n, as per

equation 24. This likelihood ratio may be used to obtain a maximum likeli-

P

hood ﬁt as well as to obtain a goodness of ﬁt. Note that the likelihood ratio

is well-behaved even for empty bins where nk = 0, since nnk
k

is unity for such

Note that the negative log-likelihood ratio

resulting from equation 25

N LLR

cases.

yields

=

nk loge (

N LLR

nk
Tk

)

k=nb

Xk=1

which is the same result as derived by Baker and Cousins [4] for the multino-

mial case where normalization is preserved between theory and experiment.

We have derived the result using very diﬀerent arguments (than Baker and

Cousins) for the denominator of the likelihood ratio, namely it is the value of

the data pdf at the bin center as a result of the general theory developed here.

If we are reluctant to work out (for reasons of computing speed) the integral

in equation 23 for each bin at each step of the ﬁtting process, then we can

(26)

(27)

approximate it by the bin center values

P bin average(ck|

s)

≈

P (ck|
k P (ck|

s)
s) ∆ck

P

tion 26 for

can be used generally.

N LLR

14

This then obeys the normalization equation 24 and the expression in equa-

4.3 To Show that the Binned Negative Log-Likelihood Ratio Approaches a χ2

Distribution for Large n

Let the diﬀerence between nk, the observed number of events and Tk the

theoretical number of events be denoted by λk = nk −
virtue of the normalization conditions. Then the binned negative log likelihood

k λk = 0, by

Tk. Then

P

ratio

can be written

N LLR

N LLR

=

loge LR =
−

−

k=nb

Xk=1

nk loge

1
 

−

λk
nk !

(28)

This can be expanded in powers of λk/nk as

N LLR

=

loge LR =
−

k=nb

Xk=1

nk

λk
nk

 

+

(

1
2

λk
nk
k=nb

Xk=1

)2 +

1
2

(

λ2
k
nk

=

1
3

(

λk
nk

)3 +

1
4

(

λk
nk

)4

(29)

· · ·!

) +

) +

1
3

(

λ3
k
n2
k

1
4

(

λ4
k
n3
k

)

(30)

· · ·

As n

, the individual bin contents become normally distributed about

→ ∞

their expected value Tk with variance σ2

k = nk(1

nk/n)

nk for nk << n.

−

≈

This is true for all cases (named the null hypothesis) where the data and theory

ﬁt each other. Then we can write χ2

k = λk/nk and

For large n, λk ≈

√nk and the higher order terms may be neglected yielding

N LLR

=

k=nb

Xk=1

1
2

χ2

k +

1
3

λ3
k
σ4
k

+

1
4

λ4
k
σ6
k · · ·

N LLR →

χ2

k when n

→ ∞

k=nb

Xk=1

1
2

15

(31)

(32)

This is an example of the likelihood ratio theorem [5]. The expected value of

the

N LLR

can then be written

E(

N LLR

) =

k=nb

Xk=1

1
2

E(χ2

k) +

1
3

µ3
σ4
k

+

1
4

µ4
σ6
k

+

1
5

µ5
σ8
k

+

1
6

µ6
σ10
k · · ·

(33)

where µ3, µ4,

are the 3rd, 4th

moments of the normal distribution about

· · ·

· · ·

the mean. Since the normal distribution is symmetric about the mean, all the

odd moments (µ3, µ5 · · ·
tion (for integer l) are given by the formula

) are zero. The even moments of the normal distribu-

(34)

(35)

(36)

(37)

µ2l = 1.3.5

(2l

· · ·

−

1)σ2l

This yields

E(

N LLR

) =

k=nb

Xk=1

1
2

E(χ2

k) +

3
4

σ4
k
σ6
k

+

15
6

σ8
k
σ10
k · · ·

All the remaining terms tend to zero as 1/nk(= 1/σ2

k) as nk → ∞

leading to

E(

N LLR

) =

E(

k=nb

E(χ2

1
2
Xk=1
LR) = exp(
−

k) =

nb
2
nb/2)

The number of degrees of freedom for

N LLR

would be nb −

1, due to the

normalization condition

k nk = n.

P

4.4 Normalizing theory and experiment and the problem of Goodness of ﬁt

for the Poisson distribution

As we have pointed out, maximum likelihood ﬁtting only ﬁts the shape of the

theoretical distribution to the experimental data. This is due to the normal-

16

ization condition of equation 1. However, if we employ a binomial distribution

and deﬁne the ﬁrst bin as containing the number of observed events n with the-

oretical expectation of nt events, and the second bin to contain the number of

unobserved events in N tries, then one can employ the formula in equation 25

with nb = 2 to obtain the likelihood ratio.

LR =

n

nt
n (cid:19)

(cid:18)

N
N

nt
n (cid:19)

−
−

(cid:18)

N

n

−

=

n

nt
n (cid:19)

(cid:18)

1
1

−
−

 

nt/N
n/N !

N

n

−

We now take the Poissonian limit of N

with nt and n ﬁnite and the

→ ∞

above likelihood ratio becomes

LR = e−

(nt

n)

−

n

nt
n (cid:19)

(cid:18)

where we have employed the relations (N

n)

N and (1

−

→

x/N)N

x

e−

→

−

as N

.

→ ∞

Equation 39 provides the goodness of ﬁt likelihood ratio for all Poissonian

problems where nt events are expected and n are observed. We can now mul-

tiply this Poissonian

LR with equation 25 to produce the likelihood ratio for
a general binned likelihood problem where the normalization for theory and

experiment vary.

LR = e−

(nt

n)

−

nt
n (cid:19)

(cid:18)

n k=nb

nk

Tk
nk (cid:19)

Yk=1 (cid:18)

= e−

(nt

n)

−

k=nb

Yk=1  

nk

T ′k
nk !

where we have deﬁned T ′k = ntTk/n and

T ′k = nt. With this redeﬁnition, we

obtain the

for the multinomial with theoretical normalization diﬀering

P

(38)

(39)

(40)

(41)

N LLR
from the experimental one as

N LLR

=

k=nb

Xk=1

T ′k −

nk + nk loge(

nk
T ′k

)

17

This is same as the “Poissonian result” of Baker and Cousins [4] again derived

using very diﬀerent arguments for the denominator of the likelihood ratio.

4.5 The Gaussian limit of the binomial

The Poissonian result is useful when nt and n are relatively small numbers

(<

25). When we have larger number of events, then the Gaussian approxi-

≈

mation is more relevant. We have already shown that (equation 30) that in a

multinomial, the negative log likelihood ratio can be approximated by

We apply this to the binomial with nb = 2, n1 = n, and n2 = N

n and

−

N LLR

=

k=nb

Xk=1

1
2  

λ2
k
nk !

λ1 =

λ2 = n

nt. Then

−

−

=

λ2
2
2 (cid:18)

1
n1

+

1
n2 (cid:19)

=

N LLR

(1

λ2
2
2  
λ2
2
2  

≈

1

=

n/N)(n/N)N !
nt)2
(n

−
1
Npq !

−
2σ2

(42)

(43)

(44)

where p = nt/N

n/N is the probability of an event appearing in the ﬁrst

bin and q = 1

p and σ2 = Npq is the variance of the bin contents of

≈

−

the ﬁrst bin. We now let N

, n

and N >> n. In this case,

→ ∞

→ ∞

the variance can be approximated by n and we have the Gaussian case with

= (n

nt)2/2n). This

can be added to the one resulting from

N LLR
the maximum likelihood shape ﬁtting to get an overall goodness of ﬁt.

N LLR

−

We must emphasize once again that the method of maximum likelihood always

ﬁts theoretical shapes to experimental data. We have been able to circumvent

this restriction by using the device of the binomial distribution where the

18

observed events n are in the ﬁrst bin and the total number of events in the

distribution N refer to the “number of tries” and the second bin consists of the

N

n events that failed to appear in the experiment. The binomial distribution

−

is special in this regard since once we specify the properties of the ﬁrst bin,

the second bin is completely speciﬁed and anti-correlated with the ﬁrst bin.

The number of tries is unknown, but we set it to inﬁnity in two diﬀerent limits

as discussed resulting in the Poisson and the Gaussian likelihood ratios.

4.6 To show that χ2 is also the negative logarithm of a likelihood ratio

The most commonly used method for goodness of ﬁt is the χ2 test of Karl

Pearson, which is used even when the quantities being ﬁtted are not events

but measurements with error bars. We show here that the χ2 measure is also

twice the negative logarithm of a Gaussian likelihood ratio rather than the

negative logarithm of a Gaussian likelihood, as is the popular misconception.

Consider a binned histogram where the contents in the kth bin is noted by

ck and the theoretical expectation of this bin is sk. The standard error of the

observed variable ck is known to be σk. Then, one can write

P (ck|

sk) =

1
√2πσk

exp

 −

sk)2

(ck −
2σ2
k

=

!

1
√2πσk

exp

χ2
k
2 !

 −

This leads to

loge (P (ck|

−

sk)) =

χ2
k
2

+ loge(√2πσk)

(45)

(46)

From the above expression, people are mistakenly led to conclude that χ2 is

equivalent to twice the negative log-likelihood. This ignores the term loge(√2πσk)

in the above equation, which varies from bin to bin. In order to work out the

19

likelihood ratio, we need to estimate the data density P (ck) at each measure-

ment. The data points are distributed as a Gaussian with standard deviation

σk. The best estimate of the mean of the Gaussian from the data alone is ck.

This leads to

P (ck) =

1
√2πσk

exp

 −

ck)2

(ck −
2σ2
k

=

!

1
√2πσk

yielding the likelihood ratio

k =

LR

sk)

P (ck|
P (ck)

= exp

ck)2

(sk −
2σ2
k

!

 −

= exp(

χ2
k
2

)

−

The overall likelihood ratio is given by

LR =

k=nb

k
R

Yk=1 L

leading to

χ2 = 2 loge (

LR) =

k=nb

Xk=1

χ2
k

(47)

(48)

(49)

(50)

i.e. χ2 is equal to twice the negative log-likelihood ratio and not the negative

log-likelihood!.

5 Unbinned Goodness of Fit

Very often the data are not plentiful enough to bin adequately and it is more

eﬃcient to perform an unbinned likelihood ﬁt. Presently, a goodness of ﬁt

method does not exist for unbinned likelihood ﬁts. Using the formalism de-

veloped above, we present a solution. After the unbinned likelihood ﬁt is per-

formed by maximizing the likelihood in equation 2 one needs to work out

20

the data likelihood P data( ~cn) in order to evaluate the likelihood ratio and the

goodness of ﬁt. We employ the technique of Probability Density Estimators

(P DE′s), also known as Kernel Density Estimators [2] (KDE′s) to do this.

The pdf P data(c) is approximated by

P data(c)

P DE(c) =

≈

1
n

i=n

Xi=1 G

(c

ci)

−

where a Kernel function

(c

ci) is centered around each data point ci, is so

G

−

deﬁned that it normalizes to unity. The choice of the Kernel function can vary

depending on the problem. A popular kernel is the Gaussian deﬁned in the

multi-dimensional case as

(c) =

G

(√2πh)d

(det(E))

exp( −

H αβcαcβ
2h2

)

where E is the error matrix of the data deﬁned as

Eα,β =< cαcβ >

< cα >< cβ >

1

q

−

and the <> implies average over the n events, and d is the number of dimen-

sions. The Hessian matrix H is deﬁned as the inverse of E and the repeated

indices imply summing over. The parameter h is a “smoothing parameter”,

which has[7] a suggested optimal value h

n−

1/(d+4), that satisﬁes the asymp-

∝

totic condition

(c

ci)

G∞

−

lim
→∞ G
n

(c

−

≡

ci) = δ(c

ci)

−

The parameter h will depend on the local number density and will have to be

adjusted as a function of the local density to obtain good representation of the

data by the P DE. Our proposal for the goodness of ﬁt in unbinned likelihood

21

(51)

(52)

(53)

(54)

ﬁts is thus the likelihood ratio

=

s)

P ( ~cn|
P data( ~cn) ≈

s)
P ( ~cn|
P P DE( ~cn)

LR

evaluated at the maximum likelihood point s∗.

6 An illustrative example

tion is given by

P (c

s) =

exp(

|

1
s

c
s

)

−

Kernel for the P DE would be given by

(c) =

G

1
(√2πσh)

exp(

c2
2σ2h2 )

−

We consider a simple one-dimensional case where the data is an exponential

distribution, say decay times of a radioactive isotope. The theoretical predic-

We have chosen an exponential with s = 1.0 for this example. The Gaussian

(55)

(56)

(57)

where the variance σ of the exponential is numerically equal to s. To be-

gin with, we chose a constant value for the smoothing parameter, which for

1000 events generated is calculated to be 0.125. Figure 2 shows the generated

events, the theoretical curve P (c

s) and the P DE curve P (c) normalized to

|

the number of events. The P DE fails to reproduce the data near the origin

due to the boundary eﬀect, whereby the Gaussian probabilities for events close

to the origin spill over to negative values of c. This lost probability would be

compensated by events on the exponential distribution with negative c if they

existed. In our case, this presents a drawback for the P DE method, which we

22

Fig. 2. Figure shows the histogram (with errors) of generated events. Superimposed

s) and the P DE estimator (solid) histogram with no
is the theoretical curve P (c
|

errors.

will remedy later in the paper using P DE deﬁnitions on the hypercube and

periodic boundary conditions. For the time being, we will conﬁne our example

to values of c > 1.0 to avoid the boundary eﬀect.

In order to test the goodness of ﬁt capabilities of the likelihood ratio

LR
we superimpose a Gaussian on the exponential and try and ﬁt the data by a

,

simple exponential. Figure 3 shows the “data” with 1000 events generated as

23

an exponential in the ﬁducial range 1.0 < c < 5.0. Superimposed on it is a

Gaussian of 500 events. More events in the exponential are generated in the

interval 0.0 < c < 1.0 to avoid the boundary eﬀect at the ﬁducial boundary at

c=1.0. Since the number density varies signiﬁcantly, we have had to introduce

a method of iteratively determining the smoothing factor as a function of

c as described in [6]. With this modiﬁcation in the P DE, one gets a good

description of the behavior of the data by the P DE as shown in Figure 3.

We now vary the number of events in the Gaussian and obtain the value of

the negative log likelihood ratio

as a function of the strength of the

N LLR

Gaussian. Table 1 summarizes the results. The number of standard deviations

the unbinned likelihood ﬁt is from what is expected is determined empirically

by plotting the value of

for a large number of ﬁts where no Gaussian is

N LLR

superimposed (i.e. the null hypothesis) and determining the mean and RMS

of this distribution and using these to estimate the number of σ’s the observed

is from the null case. Table 1 also gives the results of a binned ﬁt on the

N LLR
same “data”. It can be seen that the unbinned ﬁt gives a 3σ discrimination

when the number of Gaussian events is 85, where as the binned ﬁt gives a

χ2/ndf of 42/39 for the same case.

Figure 4 shows the variation of -log P ( ~cn|
of 500 experiments each with the number of events n = 1000 in the exponential

s) and -log P P DE( ~cn) for an ensemble

and no events in the Gaussian (null hypothesis). It can be seen that -log

s) and -log P P DE( ~cn) are correlated with each other and the diﬀerence

P ( ~cn|
between the two (-log

) is a much narrower distribution than either

N LLR

and provides the goodness of ﬁt discrimination.

24

Fig. 3. Figure shows the histogram (with errors) of 1000 events in the ﬁducial

interval 1.0 < c < 5.0 generated as an exponential with decay constant s=1.0 with

a superimposed Gaussian of 500 events centered at c=2.0 and width=0.2. The P DE

estimator is the (solid) histogram with no errors.

6.1 Improving the P DE

The P DE technique we have used so far suﬀers from two drawbacks; ﬁrstly,

the smoothing parameter has to be iteratively adjusted signiﬁcantly over the

full range of the variable c, since the distribution P (c

s) changes signiﬁcantly

|

over that range; and secondly, there are boundary eﬀects at c=0 as shown in

25

Table 1

Goodness of ﬁt results from unbinned likelihood and binned likelihood ﬁts for

various data samples. The negative values for the number of standard deviations in

some of the examples is due to statistical ﬂuctuation.

Number of

Unbinned ﬁt Unbinned ﬁt Binned ﬁt χ2

Gaussian events

N LLR

39 d.o.f.

500

250

100

85

75

50

0

189.

58.6

11.6

8.2

6.3

2.55

0.44

N σ

103

31

4.9

3.0

1.9

-0.14

-1.33

304

125

48

42

38

30

24

ﬁgure 2. Both these ﬂaws are remedied if we deﬁne the P DE in hypercube

space. After we ﬁnd the maximum likelihood point s∗, for which the P DE

is not needed, we transform the variable c

c′, such that the distribution

→

P (c′

s∗) is ﬂat and 0 < c′ < 1. The hypercube transformation can be made

|

even if c is multi-dimensional by initially going to a set of variables that are

uncorrelated and then making the hypercube transformation. The transfor-

mation can be such that any interval in c space maps on to the interval (0, 1)

in hypercube space.

26

Fig. 4. (a) shows the distribution of the negative log-likelihood -loge(P ( ~cn|
an ensemble of experiments where data and experiment are expected to ﬁt. (b)

s)) for

Shows the negative log P DE likelihood -loge(P ( ~cn)) for the same data (c) Shows

the correlation between the two and (d) Shows the negative log-likelihood ratio

that is obtained by subtracting (b) from (a) on an event by event basis.

N LLR

6.2 Periodic Boundary Conditions

We solve the boundary problem by imposing periodicity in the hypercube.

In the one dimensional case, we imagine three “hypercubes”, each identical

to the other on the real axis in the intervals (

1, 0), (0, 1) and (1, 2). The

−

27

hypercube of interest is the one in the interval (0, 1). When the probability

from an event kernel leaks outside the boundary (0, 1), we continue the kernel

to the next hypercube. Since the hypercubes are identical, this implies the

kernel re-appearing in the middle hypercube but from the opposite boundary.

Put mathematically, the kernel is deﬁned such that

(c′
(c′

G
G

−
−

c′i) =
c′i) =

(c′
(c′

G
G

−
−

c′i −
1); c′ > 1
c′i + 1); c′ < 0

Although a Gaussian Kernel will work on the hypercube, the natural kernel

to use considering the shape of the distribution in hypercube space (it is ﬂat

for a good ﬁt), would be the “boxcar function”

(c′).

G

(58)
(59)

(60)

(61)

(c′) =

c′

<

1
h

;

|

|

|

|

h
2
h
2

(c′) = 0;

c′

>

G

G

This kernel would be subject to the periodic boundary conditions given above,

which further ensure that every conﬁguration in hypercube space is treated

exactly as every other conﬁguration irrespective of its co-ordinate. The pa-

rameter h is a smoothing parameter which needs to be chosen with some care.

However, since the theory distribution is ﬂat in hypercube space, the smooth-

ing parameter may not need to be iteratively determined over hypercube space

to the extent that data distribution is similar to the theory distribution. Even

if iteration is used, the variation in h in hypercube space is likely to be much

smaller.

Figure 5 shows the distribution of the

for the null hypothesis for

N LLR

an ensemble of 500 experiments each with 1000 events as a function of the

smoothing factor h. It can be seen that the distribution narrows considerably

28

Fig. 5. The distribution of the negative log likelihood ratio

for the null

N LLR

hypothesis for an ensemble of 500 experiments each with 1000 events, as a function

of the smoothing factor h=0.1, 0.2 and 0.3

as the smoothing factor increases. We choose an operating value of 0.2 for h

and study the dependence of the

as a function of the number of events

N LLR

ranging from 100 to 1000 events, as shown in ﬁgure 6. The dependence on the

number of events is seen to be weak, indicating good behavior. The P DE thus

arrived computed with h=0.2 can be transformed from the hypercube space

to c space and will reproduce data smoothly and with no edge eﬀects. We note

that it is also easier to arrive at an analytic theory of

with the choice

N LLR

29

of this simple kernel.

Fig. 6. The distribution of the negative log likelihood ratio

for the null hy-

N LLR

pothesis for an ensemble of 500 experiments each with the smoothing factor h=0.2,

as a function of the number of events

7 The distribution of the goodness of ﬁt variable

Of all the goodness of ﬁt variables we have studied above, for both binned

and unbinned likelihood ﬁts, the χ2 variable is the most studied and has an

30

analytic theory associated with its distribution. This is used to set a p-value

for the goodness of ﬁt, deﬁned as the probability to exceed the observed value

χ2 based on its analytic distribution. In the absence of an analytic theory,

it is possible to use Monte Carlo methods to obtain the distribution of the

goodness of ﬁt variable for the hypothesis being tested and to numerically

obtain the p-value.

8 Calculation of ﬁtted errors

After the ﬁtting is done and the goodness of ﬁt is evaluated, one needs to

work out the errors on the ﬁtted quantities. One needs to calculate the poste-

rior density P (s

~cn), which carries information not only about the maximum

|

likelihood point s∗, from a single experiment, but how such a measurement is

likely to ﬂuctuate if we repeat the experiment.

8.1 The concept of the pdf of a ﬁxed parameter

Before we begin the error calculation, we would like to deﬁne precisely a few

concepts. The theoretical parameter s is a ﬁxed but unknown constant. What

do we mean by its probability density function? We give the following oper-

ational deﬁnition. First, determine the maximum likelihood value s∗ from a

single dataset ~cn. Repeat this procedure for an ensemble N of such datasets.

We deﬁne

Pn(s) as the probability density function of the parameter s, the dis-
)

tribution of s∗ that we would obtain from such an inﬁnite ensemble (N

→ ∞
of datasets. We employ the subscript n to note the expected dependence of

this pdf on the number of elements n in each of the datasets in the ensem-

31

ble. For instance, in the illustrative example of section 11, with individual

measurement error σ, we expect

Pn(s) to be a Gaussian of width σ/√n.

Let us note that we could also denote the pdf as

Pn(s∗) , but since, whenever
we talk of a distribution of s, we mean a distribution of s∗, we employ the

notation

Pn(s).

8.2 The true value of the parameter s

The true value sT of the parameter s is deﬁned to be that value of s at which

the maximum of the pdf

Pn(s) has an
inﬁnite number of similar datasets ~cn contributing to it and hence this is just a

Pn(s) occurs. Let us remember that

statement of the experiments being unbiased. Let us note that in the Gaussian

illustrative example of section 11,

Pn(sT ), the value of the pdf at the true value

sT is equal to √n

√2πσ which goes to

as n

.

→ ∞

∞

8.3 The unknowability of

Pn(s)

Since the true value of s can never be determined to inﬁnite precision, and the

that the function

true value is the abscissa for which the pdf

Pn(s) is the maximum, it follows
Pn(s) is unknowable. We cannot associate an abscissa to the
Pn(s) and hence the function cannot be “anchored” to the s axis. We
thus call this function the “unknown concomitant”, to distinguish it from a

function

Bayesian prior.

32

8.4 The posterior density P (s

~cn)

|

|

In order to determine the error on the ﬁtted parameter s, we need to de-

termine the posterior density P (s

~cn). The maximum likelihood ﬁt yields the

maximum likelihood value s∗ given ~cn. We postulate that there is additional

information in a single dataset ~cn to yield an estimate of the distribution of

s∗ from an ensemble of such datasets. That information is expressed in the

posterior density P (s

~cn).

|

We would like to determine this function P (s

~cn) using Bayes’ theorem. Since

|

Bayes’ theorem is central to the argument, we give a simple and intuitively

compelling derivation of it for two continuous variables c, s.

8.5 Derivation of Bayes’ theorem equations

Consider a joint probability distribution P (s, c) in variables s, c. For the sake

of simplicity, we will take both s and c to be one-dimensional. The arguments

being made are general enough to easily change them into multi-dimensional

variables. Figure 7 shows geometrically the two dimensional space of s and c.

We plot s as the ordinate and c as the abscissa. At this stage s and c are two

general variables. Then,

P (s, c)dsdc = 1

Z Z

We deﬁne the single variable probabilities P (c) and P (s) as

(62)

(63)

(64)

P (c) =

P (s, c)ds

P (s) =

P (s, c)dc

Z

Z

33

P (c) is the probability density of c irrespective of the value of s and P (s)

is the probability density of s irrespective of the value of c. It follows from

equation 62 that

P (s)ds = 1

and

Z

Z

P (c)dc = 1

(65)

(66)

Fig. 7. Joint probability distribution in the variables s, c. Conditional probabilities

are computed along the slices AB( s=constant) and CD(c= constant).

We deﬁne a conditional probability P (c

s) as the probability of observing c

given s. It is thus, the joint probability P (s, c) along the slice AB (s=constant)

|

34

in ﬁgure 7, appropriately normalized to unity. i.e,

P (c

s) =

|

P (s, c)
P (s, c)dc

R

Therefore, (using equation 64)

P (c

s) =

|

P (s, c)
P (s)

where the denominator in the above equation ensures that

P (c

s)dc = 1.

|

R

By symmetrical arguments (integrations along the slice CD), we show that

the conditional probability P (s

c) is given by

|

P (s

c) =

|

P (s, c)
P (c)

leading to the joint probability equation

P (s, c) = P (c

s)P (s) = P (s

c)P (c)

|

|

rem [8] as

P (s

c) =

|

P (c

s)P (s)
|
P (c)

which is sometimes written in a more familiar form known as Bayes’ theo-

It is a general theorem in statistics, which we have derived using intuitive

geometrically explicit arguments. By substituting the expression for P (s, c) in

equation 68 in equation 63 we get the equation

P (c) =

P (c

s)P (s)ds

Z

|

35

(67)

(68)

(69)

(70)

(71)

(72)

and by substituting the expression for P (s, c) in equation 69 in equation 64

we get the equation

P (s) =

P (s

c)P (c)dc

Z

|

These complete the Bayes’ theorem equations. Note also that the joint prob-

ability equation 70 can be written in a form a likelihood ratio

LR

LR =

P (s
c)
|
P (s)

=

P (c
s)
|
P (c)

The quantity

LR equation 74 is invariant under change of variables c

→

c′

and s

s′, since the Jacobian of the transformation

divides out in the

∂c′
∂c |

|

→

numerator and the denominator for the right hand side of the equation 74 for

the ratio of probability densities in P (c

s)

P (c) . Similarly the ratio is invariant under

|

the transformation variable s in the LHS of the equation. These invariances

are essential in the use of the ratio

LR as a goodness-of-ﬁt variable.

We can then extend the derivation given above to derive Bayes’ theorem equa-

tions for the dataset ~cn.

P (s, ~cn) = P ( ~cn|

s)

Pn(s) = P (s

~cn)P data( ~cn)
|
Pn(s)ds
P ( ~cn|
~cn)P data( ~cn)d ~cn

s)

P data( ~cn) =

Pn(s) =

Z

Z
P (s

|

(73)

(74)

(75)

(76)

(77)

Let us note that the above derivation of Bayes’ theorem treats the variables

c and s symmetrically. P (c) and P (s) are projections of the joint probability

P (s, c) on the c and s axes respectively. Neither P (c) nor P (s) is a prior in

the Bayesian sense.

36

8.6 Determination of the Posterior Density P (s

~cn)

|

The joint probability density P (s, ~cn) of the parameter s and the data ~cn is

given by

Pn(s).

P data(s, ~cn) = P (s

~cn)P data( ~cn)

|

(78)

where we use the superscript data to distinguish the joint probability P data(s, ~cn)

as having come from using the data pdf . If we now integrate the above equa-

tion over all possible datasets ~cn, we get the expression for (using equation 77)

Pn(s) =

Z

Z

|

P data(s, ~cn)d ~cn =

P (s

~cn)P data( ~cn)d ~cn

(79)

Equation 79 states that in order to obtain the pdf of the parameter s, one

needs to add together the conditional probabilities P (s

~cn) over an ensemble

|

of events, each such distribution weighted by the “data likelihood” P data( ~cn).

At this stage of the discussion, the function P data(s

~cn) is unknown. However,

it is important to note that equation 79 enables us to write down an expression

for the pdf of s, given the posterior density P (s

~cn) and the key concept of

the “data likelihood” P data( ~cn) we have introduced, motivated by goodness of

|

|

If the ensemble consists of N elements denoted by the index k, k = 1, N, then

ﬁt considerations.

as N

,

→ ∞

dN
N →

P data( ~cn)d ~cn

(80)

37

The equation 79 can be written

Pn(s) =

Z

|

P (s

~cn)P data( ~cn)d ~cn =

P (s

~cn)

|

dN
N ≈

1
N

Z

k=N

Xk=1

P (s

~cn)

|

(81)

i.e.

Pn(s) is the ensemble average of the posterior densities P (s

|

~cn). Equation 81

highlights the diﬀerence between our theory and standard Bayesian theory. In

Bayesian statistics,the data likelihood is deﬁned as [9]

(82)

(83)

(84)

P Bayesian( ~cn) =

P (s)P ( ~cn|

s)ds

Z

since

P (s

~cn) =

|

s)P (s)
P ( ~cn|
P Bayesian( ~cn)

=

s)P (s)

P ( ~cn|
P (s)P ( ~cn|

s)ds

R

where P(s) is the Bayesian prior. i.e. the Bayesian data likelihood is a purely

theoretical quantity and is handled as an uninteresting normalization constant.

As a result, Bayesian statistics is devoid of the concept of goodness of ﬁt. When

Bayesians use equation 79, they will obtain the Bayesian prior, which has no

n dependence, since

P (s) =

(P (s

~cn)P Bayesian( ~cn)d ~cn

Z

|

Thus, if we restore goodness of ﬁt using the data likelihood as derived from

data, the Bayes theorem equations are incompatible with a Bayesian prior.

8.7 The error bootstrap

We now need to compute the function P (s

~cn). We employ Bayes’ theorem to

do this. The error on the ﬁtted parameter s∗ will be related to the width of

|

38

the posterior density P (s

~cn) that we are trying to compute. It is also related

|

to our ignorance of the value of sT and our inability to anchor the distribution

Pn(s). At this stage, we have worked out
evaluated the maximum likelihood value s∗ of s. We can choose an arbitrary

(s) as a function of s and have

LR

value of s and evaluate the goodness of ﬁt at that value using the likelihood

ratio. When we do this, we are in fact hypothesizing that sT , the true value,

is at this value of s. The function

LR(s) then gives us a way of evaluating the
goodness of ﬁt of the hypothesis as we change s. Let us now take an arbitrary

value of s and hypothesize that that is the true value. Then, consistent with

our hypothesis, we must insist that the distribution

Pn(s) is moved so that
the maximum value of the distribution (i.e. sT ) is at the current value of s.

Then the theoretical estimate for the joint probability P theory(s, ~cn) is given

by the product of the probability density of the pdf of s at the true value of

s, namely

Pn(sT ), and the theoretical likelihood P (cn|

s) evaluated at the true

value, which by our hypothesis is s.

P theor(s, ~cn) = P theor( ~cn|

s)

Pn(sT )

The joint probability P (s, ~cn) is a joint distribution of the theoretical param-

eter s and data ~cn. The two ways of evaluating this (from the theoretical

end and the data end) must yield the same result, for consistency. This is

equivalent to equating P data(s, ~cn) and P theor(s, ~cn). This gives the equation

P (s

~cn)P data( ~cn) = P theor( ~cn|

|

s)

Pn(sT )

which is a form of Bayes’ theorem, but with two pdf ′s (theory and data).

39

(85)

(86)

Rearranging equation 86, one gets

P (s

~cn) =

|

(s)

Pn(sT ) =

LR

P theor( ~cn|
s)
P data( ~cn) Pn(sT )

(87)

To reiterate, when one varies s in equation 87, one makes the hypothesis that

s = sT . As one changes s, a new hypothesis is being tested that is mutually

exclusive from the previous one, since the true value can only be at one loca-

is at the value of s being tested. This implies that

tion. So as one changes s, one is free to move the distribution

Pn(s) so that sT
Pn(sT ) does not change as
one changes s and is a constant wrt s, which we can now write as αn. Figure 8

illustrates these points graphically. Thus

Pn(sT ) in our equations is a number,
not a function. We have thus “bootstrapped” the error. On the one hand,

P (s

~cn) gives us an estimate of the spread in the measurements of s∗ from

|

an ensemble of datasets ~cn, based on one such data set. From the theoretical

end, the error in s∗ is expressed in the uncertainty on where to put sT . We

have connected these two uncertainties using Bayes’ theorem and hypothesis

testing. We can now solve for P (s

~cn) as shown below.

|

8.8 New form of equations

Equation 87 can now be re-written

P (s

~cn) =

|

s)αn
P ( ~cn|
P data( ~cn)

Since P (s

~cn) must normalize to unity, one gets for αn,

|

R

αn =

P data( ~cn)
s)ds
P ( ~cn|

=

1
(s) ds

LR

R

40

(88)

(89)

Fig. 8. Comparison of the usage of Bayesian priors with the new method. In the

upper ﬁgure, illustrating the Bayesian method, an unknown distribution is guessed

at by the user based on “degrees of belief” and the value of the Bayesian prior

changes as the variable s changes. In the lower ﬁgure, an “unknown concomitant”

distribution is used whose shape depends on the statistics. In the case of no bias, this

distribution peaks at the true value of s. As we change s, we change our hypothesis

as to where the true value of s lies, and the distribution shifts with s as explained

in the text. The value of the distribution at the true value is thus independent of s.

We have thus determined αn, the value of the “unknown concomitant” at

the true value sT using our data set cn. This is our measurement of αn and

41

diﬀerent datasets will give diﬀerent values of αn, in other words αn will have

a sampling distribution with an expected value and standard deviation.

Note that it is only possible to write down an expression for αn dimensionally

when a likelihood ratio

is available. This then leads to

LR

P (s

|

~cn) = LR
LR

ds

=

R

R

P ( ~cn|
P ( ~cn|

s)
s)ds

The last equality in equation 90 is the same expression that “frequentists” use

for calculating their errors after ﬁtting, namely the likelihood curve normalized

to unity gives the parameter errors. If the likelihood curve is Gaussian shaped,

then this justiﬁes a change of negative log-likelihood of 1

2 from the optimum

point to get the 1σ errors. Even if it is not Gaussian, as we show in section

(10), we may use the expression for P (s

~cn) as a pdf of the parameter s to

evaluate the errors.

Note also that the expression for P (s

~cn) in equation 90 is invariant under

the co-ordinate transformation c

c′(c), since the Jacobian cancels in the

|

|

→

numerator and denominator.

The normalization condition (using equation 76)

P data( ~cn) =

P (s, ~cn)ds =

Z

P (cn|

s)

Pn(sT )ds

Z

is obeyed by our solution, since

P ( ~cn|

s)

Pn(sT ) ds =

αnP ( ~cn|

s) ds

≡

Z

Z

P data( ~cn)

(90)

(91)

(92)

The expression

s) ds in the above equation may be thought of as

being due to an “unknown concomitant” whose peak position is distributed

αnP ( ~cn|

R

42

uniformly in s space. The likelihoods of the theoretical prediction P ( ~cn|
contribute with equal probability each with a weight αn, to sum up to form

s)

the data likelihood P data( ~cn). i.e. the data, due to its statistical inaccuracy

will entertain a range of theoretical parameters. However, equation 92 does

not give us any further information, since it is obeyed identically.

8.9 The dependence of αn on n

For binned likelihood ﬁtting, as n

, the likelihood ratio at s = sT will

→ ∞

tend to exp(

nb/2) where nb is denotes the number of bins (see equation 37).

−

We do not currently have an analytic theory for unbinned likelihood ﬁtting.

However, we can perhaps assume that the limit of the binned likelihood ratio

approaches that of the unbinned likelihood ratio as nb → ∞
LR(sT ) approaches a ﬁnite number (exp(
In either case then
−
δ(s

However, P (s

sT ) as n

~cn)

.

and n

→ ∞
nb/2) or 0).

|

→

−

in this limit, implying a dependence on n for

→ ∞

. This must imply that αn → ∞
Pn(s). This is another way
Pn(s) and the Bayesian prior, which is

of illustrating the diﬀerence between

supposed to be a constant function, independent of n.

9 Combining Results of Experiments

Each experiment should publish a likelihood curve for its ﬁt as well as a number

for the data likelihood P data( ~cn). Combining the results of two experiments

with m and n experiments each, involves multiplying the likelihood ratios.

m+n(s) =

m(s)

n(s) =

LR

× LR

LR

s)

P ( ~cm|
P data( ~cm) ×

s)
P ( ~cn|
P data( ~cn)

(93)

43

Posterior densities and goodness of ﬁt can be deduced from the combined

likelihood ratio.

10 Interpreting the results of one experiment

After performing a single experiment with n events, we now can calculate

at

P (s

~cn), using equation 90. Equation 79 gives the prescription for arriving

|
Pn(s), given an ensemble of such experiments. The ensemble is a purely
theoretical abstraction. In practice, one only has a single dataset ~cn. If there

were two such datasets, they would combined to form a single dataset ~c2n. One

thus has to come to grips with interpreting the results of a single experiment.

However, we have shown (equation 81) that

Pn(s) = lim

N

→∞

1
N

k=N

Xk=1

P (s

~cn)

|

(94)

Thus given a single experiment, the unbiased estimator for

Pn(s), the pdf of
~cn) as though it is the pdf of s and deduce

s, is P (s

~cn). We can thus use P (s

|

|

limits and errors from it. The proviso is of course that these limits and errors

as well as s∗ come from a single experiment of ﬁnite statistics and as such are

subject to statistical ﬂuctuations.

11 Another Illustrative Example

We now apply the theory developed here to a practical example. The problem

is to determine the weight of an object using an apparatus whose standard

error is known to be 5 gm. The weight is a ﬁxed constant of nature for the

44

duration of the experiment. We obtain a dataset of 100 measurements, i.e.

n = 100. Then P (c

s) is a Gaussian of unknown mean s and width σ =

|

s) for the 100 events by multiplying the individual

5 gm. We compute P ( ~cn|
P (ci|
using unbinned likelihoods. We then transform the measurements ci to the

s) together and maximize the likelihood to determine s∗ for the dataset

hypercube space using equation 3. We use the improved P DE in hypercube

space with h = 0.2 and determine the goodness of ﬁt and the negative log-

likelihood ratio

. We repeat this for an ensemble of 1000 experiments.

N LLR

Figure 9(a) shows the distribution of s∗ for this ensemble. The mean value of

s∗ over this ensemble is 49.98 gm and the RMS is 0.495 gm which is consistent

with the expected σ/

(100) value of 0.5 gm. Figure 9(b) shows the distri-

bution of

for the 1000 members of the ensemble. Figure 9(c) shows

q

N LLR
the likelihood ratio functions

LR(s) for the ﬁrst 10 ﬁts in the ensemble. The
LR(s∗), the negative
. The ﬂuctuation in s∗ for the ﬁts in the

value of s∗ ﬂuctuates as expected, as well as the value of

logarithm of which gives the

N LLR

ensemble essentially expresses our lack of knowledge of the position of the true

value sT . The width of the likelihood distribution also contains information

on the same lack of knowledge.

We now take each function

LR(s) and hypothesize that the true value is at
a given value of s and apply Bayes’ theorem as per equation 86. This set of

inﬁnite mutually exclusive hypotheses also expresses the same ignorance of

the position of sT . Bayes’ theorem allows us to connect the two approaches

(theoretical and data) to provide a calculation the posterior density P (s

~cn)

|

for each member of the ensemble. These functions are shown in Figure 9(d).

The maximum likelihood value moves around with the expected spread of

45

Fig. 9. (a)The distribution of s∗, the maximum likelihood value of s for a 1000

member ensemble of datasets of n = 100. (b)The goodness of ﬁt variable

for the ﬁts (c)The likelihood ratio

of the ensemble (d) The function P (s

N LLR
LR(s) as a function of s for the ﬁrst 10 members
~cn) for the ﬁrst 10 members of the ensemble
|

0.5 gm. The average standard deviation of these curves is 0.5 gm with an rms

of 0.65 E-3 gm. The average of these functions on an inﬁnite ensemble yields

the true pdf

Pn(s).

46

11.1 One more iteration

In practice, if one has a dataset with n = 100 and N = 1000 similar instances

of them, the easiest way to analyze the data is to combine them all into a

dataset with n′ = Nn = 100, 000. However, we are interested in studying the

function

Pn(s) which is estimated by the ensemble average of the functions
~cn). This function tells us the behavior of the distribution of the maximum

P (s

|

likelihood values s∗ over similar datasets each with n=100.

After we do the average and obtain our best estimate of

Pn(s) on the ensemble,
we have more information (from the whole ensemble) on the position of the

true value than we possessed while evaluating P (s

~cn) for an element of the

|

ensemble. We should use this additional information by re-introducing it into

the Bayes’ theorem equations 75 and 76 to re-work the individual P (s

~cn).

|

(95)

P (s

~cn) =

|

Pn(s)
s)
P ( ~cn|
Pn(s)ds
s)
P ( ~cn|

R

where we approximate

Pn(s) by the ensemble average. The resulting P (s

|

~cn)

are used to recompute the ensemble average to yield a better (iterated) esti-

mate for

estimate of

Pn(s) as per equation 81. Figure 10(a) shows the ensemble average
Pn(s) for n=100 and N=1000 before and after iteration. The mean
value of the un-iterated and iterated functions are the same at 49.977 gm (The

Gaussians were generated with a true value of 50 gm). The r.m.s values of the

function before and after iteration are 0.701 gm and 0.522 gm respectively.

The iterated function thus has the correct width and mean value. Figure 10(b)

shows the individual P (s

~cn) functions for two members of the ensemble before

|

and after iteration. The iterations pull these functions towards the true value,

since we are inputing additional information on the true value.

47

Fig. 10. (a) The function

Pn(s) computed on the ensemble for n=100 and N=1000.
The two iterations are shown, with the numbers (1,2) indicating the iteration num-

ber. (b) The function P (s

~cn) for two elements on the ensemble for the two iterations.
|

12 The distribution of s∗ and the function

Pn(s)

Using Bayes’ theorem, we have shown that the function

Pn(s) as estimated on
the ensemble using equation 81 yields the pdf of s∗, the maximum likelihood

values measured for each dataset on the ensemble. Here we show the same

fact another way. The functions P (s

~cn) are functions of s and depend on the

individual dataset ~cn. Each dataset k in the ensemble yields two quantities

|

48

after ﬁtting and iteration; the maximum likelihood value s∗k and the poste-

rior density function P (s

~cn). Without loss of generality, we can express the

|

posterior density function as a function of s

s∗k such that

−

P (s

~cn)

|

≡ Gk(s

−

s∗k)

Then equation 81 can be re-expressed

Pn(s) = lim

N

→∞

k=N

1
N

Xk=1 Gk(s

−

s∗k)

(96)

(97)

But this is just the P DE equation for the distribution of s∗, with the functions

Gk(t)dt =
Gk serving as the kernels!. They satisfy the normalization condition
1 as required. This should be compared with equation 51 for the deﬁnition of

R

P DE′s. Thus

Pn(s) represents a P DE of the distribution of s∗ and will yield

the same distribution as s∗.

In the limit N

, we can represent the distribution of the maximum

→ ∞

likelihood values s∗ on the ensemble as a continuous pdf g(s∗). In this limit,

one can write

Pn(s) =

Z

g(s∗)

(s∗, s

s∗)ds∗ = g(s)

G

−

(98)

where we have used the notation

(s∗, s

s∗) to emphasize the variation of the

G

−

kernel as a function of s∗ (i.e. ensemble element). The latter half of the above

equation is an integral equation with kernel

(s∗, s

s∗) whose eigenfunction

G

−

is g(s). Figure 11(a) shows the values of s∗ histogrammed for our illustrative

example for an ensemble of N=1000 and n=100. The superimposed curve is

the iterated function

Pn(s) calculated for this ensemble normalized to a 1000
element ensemble. It can be seen that the function describes the distribution

49

of s∗ well. Figure 11(b) shows the iterated function

Pn(s) for n = 100 and
n = 200 respectively. As expected, the n = 200 function is narrower and its

value at the maximum is larger, illustrating that αn ≡ Pn(sT ) increases with
n.

Fig. 11. (a)The distribution of s∗ (solid histogram) for an ensemble with N=1000

elements each consisting of a dataset n=100. The curve is the estimate for the

iterated function

Pn(s) for this ensemble normalized to the 1000 observations. (b)
Pn(s) on the ensemble for n=100 and n=200. This illustrates that the ensemble
averaged function, depends on n, the size of the dataset. As n increases, the function

narrows and the value of the function at its maximum increases.

50

13 Co-ordinate transformations s′ = s′(s)

We have shown that the posterior densities P (s

~cn) are invariant under the

co-ordinate transformations c′ = c′(c), as they should be. How do they behave

under transformations s′ = s′(s)? The function P (s

~cn) represents our estimate

using one member of the ensemble of the pdf of s. So if P (s

~cn) represents a

|

pdf , we would expect it to behave like a pdf , namely

|

|

P (s′

~cn) = P (s

~cn)

|

∂s
∂s′ |

|

|

This is how pdf ′s transform (via the Jacobian). This can be shown patently

not to be so, since P ( ~cn|
s′) = P ( ~cn|
s′)
P ( ~cn|
s′)ds′
P ( ~cn|

~cn) =

P (s′

|

= λ( ~cn)P (s

~cn)

|

s) and

R

where the s independent constant λ( ~cn) is given by

λ( ~cn) =

P ( ~cn|
P ( ~cn|

s)ds
s′)ds′

R

R

(99)

(100)

(101)

i.e. the posterior densities do not transform in a way that is expected of pdf ′s.

This was perhaps a naive expectation. As we have just demonstrated, the

posterior densities serve the purpose of kernels on the ensemble, the ensemble

average of which gives the pdf

Pn(s). There is no need for the kernel from a
member of the ensemble to transform to the kernel from the same member

under these transformations. The properties of the ensemble average deduced

from the individual kernels will ﬂuctuate from kernel to kernel. Similarly, when

one analyzes in transformed variables, the same kernel will give diﬀerent results

which may be thought of as being part of the ﬂuctuation.

51

The distributions of the maximum likelihoods g(s∗) however will transform as

pdf ′s, since g(s) represents the probability density of the maximum likelihood

value and s′∗ = s′(s∗). i.e.

g′(s′) =

g(s)

∂s
∂s′ |

|

′n(s′) =

P

∂s
∂s′ |Pn(s)

|

Since we have demonstrated using equation 98 that

tical distributions, we can similarly assert that

Pn(s) and g(s) are iden-
′n(s′) and g′(s′) are identical

P

distributions. And due to equation 102, we conclude that

(102)

(103)

i.e the true pdf ′s on an inﬁnite ensemble will transform correctly. The indi-

vidual kernels will not transform on to each other as pdf ′s.

14 Comparison with the Bayesian approach

In the Bayesian approach, an unknown Bayesian prior P (s) is assumed for the

distribution of the parameter s in the absence of any data. The shape of the

prior is guessed at, based on subjective criteria or using other objective pieces

of information. However, such a shape is not invariant under transformation

of variables. For example, if we assume that the prior P (s) is ﬂat in s, then

if we analyze the problem in s2, it will not be ﬂat in s2. This feature of the

Bayesian approach has caused controversy. Also, the notion of a pdf of the

data does not exist and P (c) is taken to be a normalization constant. As such,

no goodness of ﬁt criteria exist. In the method outlined here, we have used

Bayes’ theorem to calculate posterior densities of the ﬁtted parameters while

52

being able to compute the goodness of ﬁt. The formalism developed here shows

that what is conventionally thought of as a Bayesian prior distribution is in

fact a normalization constant and what Bayesians think of as a normalization

constant is in fact the pdf of the data. Table 2 outlines the major diﬀerences

between the Bayesian approach and the new one.

15 Conclusions

To conclude, we have proposed a general theory for obtaining the goodness

of ﬁt in likelihood ﬁts for both binned and unbinned likelihood ﬁts. In order

to obtain a goodness of ﬁt measure, one needs two likelihoods:- one derived

from theory and the other derived from the data alone. In order to compute

the errors on ﬁtted quantities, posterior densities need to be worked out and

Bayes’ theorem needs to be employed. The usage of data likelihood using

data alone does away the need for the Bayesian prior which is shown to be

a number and not a distribution. This number is the value of the pdf of the

parameter, which we call the “unknown concomitant” at the true value of

the parameter. This number is calculated from a combination of data and

theory and is seen to be an irrelevant parameter. If this viewpoint is accepted,

the controversial practice of guessing distributions for the “Bayesian Prior”

can now be abandoned, as can be the terms “Bayesian” and “frequentist”.

We investigate the transformation properties of the posterior density of ﬁtted

parameters under change of variable.

53

Table 2

The key points of diﬀerence between the Bayesian method and the new method.

Item

Bayesian Method

New Method

Goodness Absent

Now available

of ﬁt

in both binned

and unbinned ﬁts

Data

Used in evaluating

Used in evaluating

theory pdf

theory pdf

at data points

at data points

as well as evaluating

data pdf at data points

Prior

Is a distribution

No prior needed.

that is guessed based One calculates a

on “degrees of belief”

constant from data

Independent of data,

monolithic

αn = P data( ~cn)

P ( ~cn

s)ds

|

R
as n

→ ∞

→ ∞

Posterior Depends on Prior.

Independent of prior.

same as frequentists use

density

P (s

~cn)
|

P ( ~cn
P ( ~cn

s)P (s)
|
s)P (s) ds
|

R

P ( ~cn
P ( ~cn

s)
|
s) ds
|

R

54

16 Acknowledgments

This work is supported by Department of Energy.

17 Appendix

17.1 An extreme goodness of ﬁt example

In order to demonstrate the capabilities of the unbinned goodness of ﬁt method,

we illustrate its power with the following example.

17.2 An extreme problem

We now attempt to solve a problem with three observed data points, made

extreme due to the sparsity of data. The problem is stated as follows.

“Three data points are observed [10] in three dimensional co-ordinate space

x,y,z with (x,y,z) = (0.1,0.2,0.3), (0.2,0.4,0.1), and (0.05,0.6,0.21). What is

the goodness of ﬁt to the hypothesis that the observed number of events is

distributed according to p(x, y, z) = e−

(x+y+z) ? “

17.3 Goodness of ﬁt for the above problem

We note that the likelihood function for the problem is

=

L

1
s

i=3

Yi=1

exp

((xi + yi + zi)/s)

−

55

(104)

where we assume a maximum likelihood ﬁt has been done and the lifetime

parameter s has been determined to be s∗ = 1 at the maximum. Since the

three co-ordinates x,y, and z are uncorrelated (as per the above likelihood

function), we can reformulate the problem as a single dimensional problem as

follows.

=

L

1
s

i=9

Yi=1

exp (

ci/s)

−

(105)

where the n=9 vector ~cn = 0.1 0.2 0.3 0.2 0.4 0.1 0.05 0.6 0.21

We transform the co-ordinates to the hypercube space (s∗ = 1), with the limits

of c assumed to be 0.0 and 10.0 1 .

Figure 12 shows the transformed co-ordinates in hypercube space. We then

proceed to work out the negative log-likelihood ratio

for this conﬁgura-

N LLR

tion with the “smoothing parameter h” set to three diﬀerent values h = 0.2, 0.3

and 0.4. We study the behavior of the

for the null hypothesis (i.e. n=9

N LLR

events distributed uniformly in hypercube space) for a 1000 such experiments.

We repeat this for a dataset of n = 100 as well to study the eﬀect of the small

data sample on our goodness of ﬁt measure. Figure 13 shows the distribution

of the

for the three diﬀerent values of h for a data set size n = 9.

N LLR

Figure 14 shows the distribution of the

for the three diﬀerent values

N LLR

of h for a data set size n = 100. Table 3 summarizes the observed

for

N LLR

our dataset as a function of h. The mean and sigma of the null hypothesis

histograms are also shown as well as the probability that the observed

N LLR
is exceeded for both the n = 9 null hypothesis and an n = 100 null hypothesis.

1 Since the program expects a ﬁnite upper limit, the high value of c=10 is deemed

to be suﬃciently large to be inﬁnite for this problem.

56

Fig. 12. Transformed co-ordinates in hypercube space.

The latter is run to test the sensitivity of the results to the small data sample.

17.4 Comments

The observed data is a bad ﬁt to the model. We have managed not only

obtain a goodness of ﬁt for the problem (made extreme by the sparsity of

data), but also to show that the method gives reliable results for a variety of

smoothing parmaters. The method is also robust with respect to the data size

57

Fig. 13. The distrbution of

as a function of the smoothing parameter

N LLR

h = 0.2, 0.3, 0.4 for a dataset n = 9 generated to be uniform in the hypercube.

n. We see that as we increase the smoothing parameter to 0.4, we begin to

increase the chance of ﬁtting. When h = 1.0, everything will ﬁt. A smoothing

parameter of h = 0.2 or 0.3 gives reliable results. The probability to exceed

the observed

is estimated from the histograms with 1000 experiments.

N LLR

We can improve the accuracy of this by running more Monte Carlo statistics.

58

Fig. 14. The distrbution of

as a function of the smoothing parameter

N LLR

h = 0.2, 0.3, 0.4 for a dataset n = 100 generated to be uniform in the hypercube.

References

[1] R. A. Fisher,“On the mathematical foundations of theoretical statistics”, Philos.

Trans. R. Soc. London Ser. A 222, 309-368(1922);

R. A. Fisher,“Theory of statistical estimation”, Proc. Cambridge Philos. Soc.

22, 700-725 (1925).

[2] E. Parzen, “On estimation of a probability density function and mode”

Ann.Math.Statis. 32, 1065-1072 (1962).

59

Table 3

Summary of results

Smoothing Dataset

n=9

n=9

n=9 Prob.

n=100

n=100

n=100 Prob.

parameter h

µ

σ

to exceed

µ

σ

to exceed

N LLR

0.2

0.3

0.4

5.36

0.82

1.26

0.5%

0.77

1.255

0.3%

5.84

0.34

0.96

< 0.1%

0.30

0.91

< 0.1%

1.77

0.12

0.67

1.0%

0.083

0.697

1.1%

[3] J. Neyman and E. S. Pearson, Biometrika 20A (1928) 263. See also S. Brandt,

“Statistical and Computational Methods in Data Analysis”, Springer, New York,

(1997) for a proof.

[4] S. Baker, R. D. Cousins, Nucl. Instrum. Meth. A221 (1984).

[5] See P. G. Hoel, “Introduction to Mathematical Statistics”, 4th ed., Wiley, New

York 1971, p211 for a derivation of the likelihood ratio theorem.

[6] “A measure of the goodness of ﬁt in unbinned likelihood ﬁts”, R .Raja, Fermilab-

PUB-02-152-E, physics/0207083

[7] D. Scott. Multivariate Density Estimation. John Wiley & Sons, 1992.

M. Wand and M. Jones, Kernel Smoothing. Chapman & Hall, 1995.

[8] “An essay towards

solving a problem in the doctrine of

chances”,

Rev. Thomas Bayes, Biometrika,45 293-315 (Reprint of 1763) (1958).

[9] All Bayesians make this substitution. See for example, Bruno De Finetti, “Theory

of Probability, A Critical Introductory treatment”, John Wiley & Sons,(1974) P.

142.

[10] The author is grateful to Bruce Knuteson for posing this problem.

60


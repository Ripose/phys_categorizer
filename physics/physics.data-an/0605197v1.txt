6
0
0
2
 
y
a
M
 
3
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
7
9
1
5
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Optimal Data-Based Binning for Histograms

Kevin H. Knuth

Department of Physics,
University at Albany, State University of New York,
Albany, NY, 12222, USA

Summary. Histograms are convenient non-parametric density estimators, which continue to
be used ubiquitously. Summary quantities estimated from histogram-based probability density
models depend on the choice of the number of bins.
In this paper we introduce a straight-
forward data-based method of determining the optimal number of bins in a uniform bin-width
histogram. Using the Bayesian framework, we derive the posterior probability for the number of
bins in a piecewise-constant density model given the data. The most probable solution is deter-
mined naturally by a balance between the likelihood function, which increases with increasing
number of bins, and the prior probability of the model, which decreases with increasing number
of bins. We demonstrate how these results outperform several well-accepted rules for choosing
bin sizes. In addition, we examine the effects of small sample sizes and digitized data. Last,
we demonstrate that these results can be applied directly to multi-dimensional histograms.

1.

Introduction

Histograms are used extensively as nonparametric density estimators both to visualize data
and to obtain summary quantities, such as the entropy, of the underlying density. However
in practice, the values of such summary quantities depend on the number of bins chosen for
the histogram, which given the range of the data dictates the bin width. The idea is to choose
a number of bins suﬃciently large to capture the major features in the data while ignoring
ﬁne details due to ‘random sampling ﬂuctuations’. Several rules of thumb exist for deter-
mining the number of bins, such as the belief that between 5-20 bins is usually adequate (for
example, Matlab uses 10 bins as a default). Scott (1979) and Freedman and Diaconis (1981)
derived formulas for the optimal bin width by minimizing the integrated mean squared error
of the histogram model h(x) of the true underlying density f (x),

L(h(x), f (x)) =

Z

h(x)
(cid:0)

−

f (x)
(cid:1)

2

.

For N data points, the optimal bin width v goes as αN −1/3, where α is a constant that
depends on the form of the underlying distribution. Assuming that the data are normally
distributed with a sample variance s gives α = 3.49s (Scott, 1979), and

Given a ﬁxed range R for the data, the number of bins M then goes as

vscott = 3.49sN

−1/3.

Mscott =

R
3.49s

⌈

N 1/3

.

⌉

(1)

(2)

(3)

2

K.H. Knuth

Freedman and Diaconis report similar results, however they suggest choosing α to be twice
the interquartile range of the data. While these appear to be useful estimates for unimodal
densities similar to a Gaussian distribution, they are known to be suboptimal for multimodal
densities. This is because they were derived assuming particular characteristics of the
underlying density. In particular, the result by Freedman and Diaconis is not valid for some
densities, such as the uniform density, since it derives from the assumption that the density
f satisﬁes

f ′2 > 0.

Stone (1984) chooses to minimize L(h, f )

R

f 2 and obtains a rule where one chooses

the bin width v to minimize

−

R

K(v, M ) =

2

1
v (cid:18)

N

N + 1
1
N

1 −

M

Xm=1

π2
i (cid:19)

(4)

−

−
where M is the number of bins and πi are the bin probabilities. Rudemo (1982) obtains a
similar rule by applying cross-validation techniques with a Kullback-Leibler risk function.
We approach this problem from a diﬀerent perspective. Since the underlying density
is not known, it is not reasonable to use an optimization criterion that relies on the error
between our density model and the true density. Instead, we consider the histogram to be a
piecewise-constant model of the underlying probability density. Using Bayesian probability
theory we derive a straightforward algorithm that computes the posterior probability of
the number of bins for a given data set. This enables one to objectively select an optimal
piecewise-constant model describing the density function from which the data were sampled.

2. The Piecewise-Constant Density Model

We begin by considering the histogram as a piecewise-constant model of the probability
density function from which N data points were sampled. This model has M bins with
each bin having width vk, where k is used to index the bins. We further assume that
the bins have equal width v = vk for all k, and together they encompass an entire width
V = M v.
Each bin has a “height” hk, which is the constant probability density over the
†
region of the bin. Integrating this constant probability density hk over the width of the bin
vk leads to a total probability mass of πk = hkvk for the bin. This leads to the following
piecewise-constant model h(x) of the unknown probability density function f (x)

where hk is the probability density of the kth bin with edges deﬁned by xk−1 and xk, and
Π(xk−1, x, xk) is the boxcar function where

h(x) =

hk Π(xk−1, x, xk),

M

Xk=1

Π(xa, x, xb) = 


0
1
0

if x < xa
if xa ≤
if xb ≤

x < xb
x



M

Xk=1

M
V

h(x) =

πk Π(xk−1, x, xk).

Our density model can be re-written in terms of the bin probabilities πk as

†For a one-dimensional histogram, vk is the width of the kth bin.

In the case of a multi-

dimensional histogram, this will be a multi-dimensional volume.

(5)

(6)

(7)

Optimal Binning

3

It is important to keep in mind that the piecewise-constant density model is not a histogram
in the usual sense. We use the bin heights to represent the probability density and not
the probability mass of the bin. Furthermore, we will show that the mean value of the
probability mass of a bin is computed in a slightly diﬀerent way.

Given M bins and the normalization condition that the integral of the probability density
equals unity, we are left with M
1 bin probabilities: π1, π2, . . . , πM−1, each describing
−
the probability that samples will be drawn from each of the M bins. The normalization
M−1
k=1 πk. For simplicity, we assume that the bin alignment
condition requires that πM = 1
is ﬁxed so that extreme data points lie precisely at the center of the extreme bins of the
histogram (that is, the smallest sample is at the center of the leftmost bin, and similarly for
the largest sample). As we will show, this technique is easily extended to multi-dimensional
densities of arbitrarily high dimension.

P

−

2.1. The Likelihood of the Piecewise-Constant Model

The likelihood function is a probability density that when multiplied by dx describes prob-
ability that a datum point dn is found to have a value in the inﬁnitesimal range between x
and x + dx. For dn to have a value of x occurring in the kth bin, the likelihood is simply
the probability density in the region deﬁned by the bin

where I represents our prior knowledge about the problem, which includes the range of the
data and the bin alignment. For equal width bins, the likelihood density reduces to

p(dn|

πk, M, I) = hk =

πk
vk

p(dn|

πk, M, I) =

πk.

M
V

For N independently sampled data points, the joint likelihood is given by

p(d
π, M, I) =
|

(cid:18)

N

M
V (cid:19)

πn1
1 πn2

2 . . . πnM −1

M−1 πnM

M

{

and π =

d1, d2, . . . , dN }

where d =
. Equation (10) is data-dependent
π1, π2, . . . , πM−1}
and describes the likelihood that the hypothesized piecewise-constant model accounts for
the data. Individuals who recognize this as having the form of the multinomial distribution
may be tempted to include its familiar normalization factor. However, it is important to
note that this likelihood function is properly normalized as is, which we now demonstrate.
For a single datum point d, the likelihood that it will take the value x is

{

π, M, I) =
p(d = x
|

1
v

M

Xk=1

πk Π(xk−1, x, xk),

(11)

(8)

(9)

(10)

(12)

(13)

(14)

(15)

(16)

(17)

4

K.H. Knuth

where we have written v = V
probability and integrating over all possible values of x we have

M . Multiplying the probability density by dx to get the

∞

π, M, I) =
dx p(d = x
|

Z

−∞

Z

−∞

dx

πk Π(xk−1, x, xk)

dx πk Π(xk−1, x, xk)

∞

M

1
v

Xk=1

1
v

1
v

M

∞

Z

−∞

πk v

Xk=1
M

Xk=1

=

=

=

M

Xk=1

πk

= 1.

2.2. The Prior Probabilities
For the prior probability of the number of bins, we assign a uniform density

p(M

I) =
|

(cid:26)

C−1
0

if 1
M
≤
otherwise

C

≤

where C is the maximum number of bins to be considered. This could reasonably be set to
the range of the data divided by smallest non-zero distance between any two data points.
We assign a non-informative prior for the bin parameters π1, π2, . . . , πM−1, the possible
values of which lie within a simplex deﬁned by the corners of an M -dimensional hypercube
with unit side lengths

p(π

M, I) =
|

π1π2 · · ·

πM−1(cid:18)

1

(cid:1)
M (cid:20)

−

πi(cid:19)(cid:21)

.

Xi=1

(18)

M−1

−1/2

Γ

M
2

Γ

(cid:0)
1
2

(cid:0)

(cid:1)

Equation (18) is the Jeﬀreys’s prior for the multinomial likelihood (10) (Jeﬀreys, 1961;
Box and Tiao, 1992; Berger and Bernardo, 1992), and has the advantage in that it is also
the conjugate prior to the multinomial likelihood.

2.3. The Posterior Probability
Using Bayes’ Theorem, the posterior probability of the histogram model is proportional to
the product of the priors and the likelihood

p(π, M

d, I)
|

∝

p(π

I) p(M
|

π, M, I).
I) p(d
|
|

(19)

Substituting (10), (17), and (18) gives the joint posterior probability for the piecewise-
constant density model

p(π, M

d, I)
|

∝ (cid:18)

M
V (cid:19)

N Γ

M
2

Γ

(cid:0)
1
2

(cid:0)

(cid:1)

n1− 1
2
M π
1
(cid:1)

n2− 1
2
π
2

. . . π

nM −1− 1
2
M−1

M−1

nM − 1
2

1

(cid:18)

−

πi(cid:19)

Xi=1

,

(20)

Optimal Binning

5

where p(M
that we will only consider a reasonable range of bin numbers.

I) is absorbed into the implicit proportionality constant with the understanding
|

The goal is to obtain the posterior probability for the number of bins M . To do this we
integrate the joint posterior over all possible values of π1, π2, . . . , πM−1 in the simplex. The
1 dimensional
expression we desire is written as a series of nested integrals over the M
parameter space of bin probabilities

−

p(M

d, I)
|

∝ (cid:18)

N Γ

M
2

M
V (cid:19)

Γ

(cid:0)
1
2

(cid:0)

(cid:1)
M Z
0
(cid:1)

1

n1− 1
2
dπ1 π
1

1−π1

Z

0

n2− 1
2
dπ2 π
2

. . .

(1−

M −2
i=1 πi)

. . .

Z
0

P

dπM−1 π

nM −1− 1
2
M−1

M−1

nM − 1
2

1
(cid:18)

−

πi(cid:19)

Xi=1

.

In order to write this more compactly, we ﬁrst deﬁne

a1 = 1
a2 = 1
a3 = 1
...

π1
π1 −

−

−

π2

M−2

−

Xk=1

aM−1 = 1

πk

ak = ak−1 −

πk−1.

and note the recursion relation

These deﬁnitions greatly simplify the sum in the last term as well as the limits of integration

p(M

d, I)
|

∝ (cid:18)

N Γ

M
2

M
V (cid:19)

Γ

(cid:0)
1
2

(cid:0)

(cid:1)
M Z
0
(cid:1)

a1

n1− 1
2
dπ1 π
1

a2

Z
0

n2− 1
2
dπ2 π
2

. . .

aM −1

(aM−1 −
To solve the set of nested integrals in (21), consider the general integral

dπM−1 π

. . .

Z
0

nM −1− 1
2
M−1

πM−1)nM − 1
2 .

where bk ∈

Ik =

ak

nk− 1
2
dπk π
k

(ak −

πk)bk

Z
0
R+ and bk > 1/2. This integral can be re-written as
bk
ak

Ik = abk

k Z
0

nk− 1
2
dπk π
k

1
(cid:18)

−

πk
ak (cid:19)

.

Setting uk =

we have

πk
ak

Ik = abk

1

du a

nk+ 1
2
k

unk− 1

2 (1

u)bk

k Z
0
bk+nk+ 1
2
k

1

Z
0

= a

du unk− 1

2 (1

u)bk ,

−

−

= a

bk+nk+ 1
2
k

B(nk +

, bk + 1)

1
2

(21)

(22)

(23)

(24)

(25)

(26)

(27)

6

K.H. Knuth

) is the Beta function with
where B(
·

To solve all of the integrals we rewrite ak in (27) using the recursion formula (23)

B(nk +

, bk + 1) =

1
2

Γ

(cid:18)

Γ

(cid:18)

nk + 1

Γ(bk + 1)

2 (cid:19)

nk + 1

2 + bk + 1

.

(cid:19)

Ik = (ak−1 −

πk−1)bk+nk+ 1

2 B(nk +

, bk + 1).

1
2

By deﬁning

we ﬁnd

bM−1 = nM −
bk−1 = bk + nk +

1
2

1
2

b1 = N

n1 +

−

M
2 −

3
2

.

Finally, integrating (24) gives

p(M

d, I)
|

∝ (cid:18)

M
V (cid:19)

N Γ

M
2

Γ

(cid:0)
1
2

(cid:1)
M

(cid:0)

(cid:1)

M−1

Yk=1

1
2

B(nk +

, bk + 1),

(32)

which can be simpliﬁed further by expanding the Beta functions using (28)

p(M

d, I)
|

∝ (cid:18)

M
V (cid:19)

N Γ

M
2

Γ

(cid:0)
1
2

(cid:1)
M

(cid:0)

(cid:1)

Γ(n1 + 1
Γ(n1 + 1

2 )Γ(b1 + 1)
2 + b1 + 1) ·

Γ(n2 + 1
Γ(n2 + 1
Γ(nM−1 + 1
Γ(nM−1 + 1

2 )Γ(b2 + 1)
2 + b2 + 1) · · · ·
2 )Γ(bM−1 + 1)
2 + bM−1 + 1)

· · · ·

Using the recursion relation (30) for the bk, we see that the general term Γ(bk + 1) in each
numerator, except the last, cancels with the denominator in the following term. This leaves

p(M

d, I)
|

N Γ

M
2

M
V (cid:19)

M
k=1 Γ(nk + 1
2 )
Γ(n1 + b1 + 3
2 )

,

(cid:0)
1
2
(cid:0)
where we have used (30) to observe that Γ(bM−1 + 1) = Γ(nM + 1/2). Last, again using the
3
2 , which results in our marginal
recursion relation in (30) we ﬁnd that b1 = N
posterior probability

n1 + M

(cid:1)
M Q

2 −

∝ (cid:18)

−

Γ

(cid:1)

p(M

d, I)
|

∝ (cid:18)

N Γ

M
2

M
V (cid:19)

Γ

(cid:0)
1
2

(cid:1)
M Q

M
k=1 Γ(nk + 1
2 )
Γ(N + M
2 )

.

(cid:0)
The normalization of this posterior probability density depends on the actual data used.
For this reason, we will work with the unnormalized posterior, and shall refer to it as the
relative posterior.

(cid:1)

(28)

(29)

(30)

(31)

(33)

(34)

(35)

In optimization problems, it is often easier to maximize the logarithm of the posterior

Optimal Binning

7

log p(M

d, I) = N log M + log Γ
|

(cid:18)

M
2 (cid:19) −

M log Γ

(cid:18)

1
2 (cid:19) −
M

log Γ

N +

(cid:18)

M
2 (cid:19)

+

(36)

+

log Γ

nk +

+ K,

Xk=1

(cid:18)

1
2 (cid:19)

where K represents the sum of the volume term and the logarithm of the implicit propor-
tionality constant. The optimal number of bins ˆM is found by identifying the mode of the
logarithm of the marginal posterior

ˆM = arg max
M {

log p(M

d, I)
|
}

.

Such a result is reassuring, since it is independent of the order in which the bins are counted.
Many software packages are equipped to quickly compute the log of the gamma function.
However, for more basic implementations, the following deﬁnitions from Abramowitz and Stegun (1972)
can be used for integer m .

log Γ(m) =

log k

m−1

Xk=1

1
2 (cid:19)

log Γ

m +

=

log π

n log 2 +

log (2k

1)

(cid:18)

1
2

−

m

Xk=1

−

Equation (36) allows one to easily identify the number of bins M which optimize the poste-
rior. We call this technique the optBINS algorithm and provide a Matlab code implemen-
tation in Appendix 1.

3. The Posterior Probability for the Bin Height

In order to obtain the posterior probability for the probability mass of a particular bin, we
begin with the joint posterior (65) and integrate over all the other bin probability masses.
Since we can consider the bins in any order, the resulting expression is similar to the multiple
1 bins is not performed.
nested integral in (21) except that the integral for one of the M
Treating the number of bins as a given, we can use the product rule to get

−

p(π

d, M, I) =
|

p(π, M
p(M

d, I)
|
d, I)
|

where the numerator is given by (65) and the denominator by (35). Since the bins can be
treated in any order, we derive the marginal posterior for the ﬁrst bin and generalize the
result for the kth bin. The marginal posterior is

p(π1|

d, M, I) =

M
2

V )N Γ
( M
(cid:0)
1
Γ
2
(cid:0)
(cid:1)
d, I)
|

p(M

M
(cid:1)

n1− 1
2
π
1

a2

n2− 1
2
dπ2 π
2

a3

Z
0

n3− 1
2
dπ3 π
3

. . .

Z

0

. . .

Z
0

aM −1

dπM−1 π

nM −1− 1
2
M−1

(aM−1 −

πM−1)nM − 1
2 .

(37)

(38)

(39)

(40)

(41)

8

K.H. Knuth

Evaluating the integrals and substituting (35) into the denominator we get

p(π1|

d, M, I) =

Q

Q

M
k=2 B(nk + 1
M
k=1 B(nk + 1

2 , bk + 1)
2 , bk + 1)

n1− 1
2
π
1

(1

π1)b1

−

(42)

Cancelling terms and explicitly writing b1, the marginal posterior for π1 is

p(π1|

d, M, I) =

Γ(n1 + 1

Γ(N + M
2 )
n1 + M−1
2 )Γ(N

2

)

−

n1− 1
2
π
1

(1

−

π1)N −n1+ M −3

2

,

(43)

which can easily be veriﬁed to be normalized by integrating π1 over its entire possible range
from 0 to 1. Since the bins can be considered in any order, this is a general result for the
kth bin

p(πk|

d, M, I) =

Γ(nk + 1

Γ(N + M
2 )
nk + M−1
2 )Γ(N

2

)

−

nk− 1
2
π
k

(1

−

πk)N −nk+ M −3

2

.

(44)

The mean bin probability mass can be found from its expectation

1

=

πki
h

Z
0

dπk πk p(πk|

d, M, I),

which substituting (44) gives

=

πki
h

Γ(nk + 1

Γ(N + M
2 )
nk + M−1
2 )Γ(N

2

−

) Z
0

1

nk+ 1
2
dπk π
k

(1

−

πk)N −nk+ M −3

2

.

(46)

The integral again gives a Beta function, which when written in terms of Gamma functions
is

=

πki
h

Γ(nk + 1

Γ(N + M
2 )
nk + M−1
2 )Γ(N

2

) ·

−

Γ(nk + 3

2 )Γ(N
−
Γ(N + M
2 + 1)

nk + M−1

)

2

.

Using the fact that Γ(x + 1) = xΓ(x) and cancelling like terms, we ﬁnd that

=

πki
h

nk + 1
2
N + M
2

.

The mean probability density for bin k (the bin height) is simply

µk =

hki
h

= h

πki
vk

=

M
V (cid:19)(cid:18)

(cid:18)

nk + 1
2
.
N + M
2 (cid:19)

It is an interesting result that bins with no counts still have a non-zero probability. This
makes sense since no lack of evidence can ever prove conclusively that an event occurring in
a given bin is impossible—just less probable. The Jeﬀrey’s prior eﬀectively places one-half
of a datum point in each bin. The variance of the height of the kth bin is found similarly
by

σ2
k =

2

M
V (cid:19)

(cid:18)

π2
ki − h
h
(cid:0)

2

πki

,
(cid:1)

(45)

(47)

(48)

(49)

(50)

Optimal Binning

9

which gives

σ2
k =

2

M
V (cid:19)

(cid:18)

(cid:18)

(nk + 1
(N + M

nk + M−1

2 )(N
2 + 1)(N + M

)
2
.
2 )2 (cid:19)

−

(51)

Thus, given the optimal number of bins found by maximizing (36), the mean and variance
of the bin heights are found from (49) and (51), which allow us to construct an explicit
histogram model of the probability density and perform computations and error analysis.
Note that in the case where there is one bin (51) gives a zero variance.

4. Results

N

4.1. Demonstration using One-dimensional Histograms
In this section we demonstrate the utility of this method for determining the optimal number
of bins in a histogram model of several diﬀerent data sets. Here we consider 1000 data points
sampled from four diﬀerent probability density functions. The optimal histogram for 1000
data points drawn from a Gaussian distribution
(0, 1) is shown in Figure 1A, where it is
superimposed over a 100-bin histogram showing the density of the sampled points. Figure
1B shows that the logarithm of the posterior probability (36) peaks at 14 bins. Figure 1C
shows the optimal binning for data sampled from a 4-step piecewise-constant density. The
logarithm of the posterior (Figure 1D) peaks at 4 bins, which indicates that the algorithm
can correctly detect the 4-step structure. In ﬁgures 1E and F, we see that samples drawn
from a uniform density were best described by a single bin. This result is signiﬁcant,
since entropy estimates computed from these data would be biased if multiple bins were
used. Last, we consider a density function that consists of a mixture of three sharply-peaked
Gaussians with a uniform background (Figure 1G). The posterior peaks at 52 bins indicating
that the data warrant a detailed model (Figure 1H). The spikes in the log posterior are due
to the fact that the bin edges are ﬁxed. The log posterior is large at values of M where the
bins happen to line up with the Gaussians, and small when they are misaligned. This last
example demonstrates one of the weaknesses of the equal bin-width model, as many bins
are needed to describe the uniform density between the three narrow peaks.

4.2. Comparison to Other Techniques
We now compare our results with those obtained using Scott’s Rule, Stone’s Rule, and the
Akaike model selection criterion (Akaike (1974)). Since Freedman and Diaconis’ (F&D)
method has the same functional form as Scott’s Rule, the results using F&D are not pre-
sented here. Their technique leads to a greater number of bins than Scott’s Rule, which,
as we will show, already prescribes more bins than are warranted by the data. Akaike’s
method, applied to histograms in Hartigan (1996), balances the logarithm of the likelihood
of the model against the number of model parameters. The number of bins is chosen to
maximize

AIC(M ) = log p(dn|

πk, M, I)

M.

−

(52)

Since Scott’s Rule was derived to be asymptotically optimal for Gaussian distributions,
we limited our comparison to Gaussian-distributed data. We tested data sets with 12
diﬀerent numbers of samples including N =
. For each N
we tested 50 diﬀerent histograms, each with N samples drawn from a Gaussian distribution
(0, 1), for a total of 700 histograms in this analysis. The optimal number of bins was
N
found using Scott’s Rule (3), Stone’s Rule (4), Akaike’s AIC (52) and the present optBINS

200, 500, 1000, 2000,

, 1000000

· · ·

}

{

10

K.H. Knuth

Fig. 1. To demonstrate the technique, 1000 samples were drawn from four different probability den-
sity functions (pdf) above.
(A) The optimal histogram for 1000 samples drawn from a Gaussian
density function is superimposed over a 100-bin histogram that shows the distribution of data sam-
ples. (B) The log posterior probability of the number of bins peaks at 14 bins for these 1000 data
sampled from the Gaussian density. (C) Samples shown are from a 4-step piecewise-constant den-
sity function. The optimal binning describes this density accurately since (D) the log posterior peaks
at four bins. (E) These data were sampled from a uniform density as veriﬁed by the log posterior
probability (F). (G) shows a more complex example—three Gaussian peaks plus a uniform back-
ground. (H) The posterior, which peaks at 52 bins, demonstrates clearly that the data themselves
support this detailed picture of the pdf.

Optimal Binning

11

Fig. 2. Four techniques are compared: optBINS (solid), AIC (dot-dash), Stone (dash), and Scott
(dot). For each number of data points N , 50 separate sets of samples were drawn from a Gaussian
distribution N (0, 1). With each set of samples, each method was used to determine the optimal
number of bins M and the Integrated Square Error (ISE) between each of the 50 modelled density
functions and the original Gaussian density. Mean and standard deviations of these results were
computed for the 50 samples. (A) This pair of ﬁgures (top: inset, bottom: complete ﬁgure) shows
the mean and standard deviation of the number of bins M determined by each algorithm. For large
numbers of data points N > 5000, optBINS consistently chooses a smaller number of bins than
the other techniques. (B) The ﬁgures on the right compare the ISE between the modelled density
function and the true density function. The present algorithm optBINS consistently outperforms the
other methods, despite the fact that Scott’s Rule is derived to asymptotically optimize the ISE when
the sampling distribution is a Gaussian.

12

K.H. Knuth

algorithm (36). The quality of ﬁt was quantiﬁed using the Integrated Square Error (ISE),
which is the criterion for which Scott’s Rule is asymptotically optimized. This is

∞

ISE =

dx

h(x, M )

Z

−∞

(cid:18)

1
√2π

−

x2
2

2

,

(cid:19)
(cid:1)

exp
−
(cid:0)

(53)

where h(x, M ) is the piecewise-constant histogram model with M bins.

Figure 2A shows the mean number of bins M found using each of four methods for the
values of the number of samples N tested. The present algorithm optBINS tends to choose
the least number of bins, and does so consistently for N > 5000. More importantly, Figure
2B shows the mean ISE between the modelled density function and the correct Gaussian
distribution
(0, 1). Since Scott’s Rule was derived to asymptotically optimize the ISE, it
may be surprising to see that optBINS outperforms each of these methods with respect to
, optBINS, which is based on
this measure. While Scott’s Rule may be optimal as N
a Bayesian solution, is optimal for ﬁnite N . Furthermore if a log probability (or log-odds
ratio) criterion were to be used, optBINS would be optimal by design.

→ ∞

N

5. Effects of Small Sample Size

5.1. Small Samples and Asymptotic Behavior
It is instructive to observe how this algorithm behaves in situations involving small sample
sizes. We begin by considering the extreme case of two data points N = 2. In the case of a
single bin, M = 1, the posterior probability reduces to

so that the log posterior is zero. For M > 1, the two data points lie in separate bins,
resulting in

p(M = 1

d1, d2, I)
|

∝

∝

M N Γ
Γ
(cid:0)
1
2
(cid:0)
1
2

12 Γ
Γ

M
2

(cid:0)
1
2

(cid:1)
Γ
Γ

(cid:1)
1

(cid:0)

(cid:1)

M
k=1 Γ(nk + 1
2 )
Γ(N + M
2 )

(cid:1)
M Q

2 + 1
2
(cid:0)
2 + 1
2
(cid:0)

(cid:1)
(cid:1)

= 1,

p(M

d1, d2, I)
|

(cid:1)
M Q

M
k=1 Γ(nk + 1
2 )
Γ(N + M
2 )
2 )M−2
2 )2Γ( 1
Γ(2 + M
2 )

Γ(1 + 1

Γ

M
2

(cid:1)
(cid:0)
Γ(2 + M
2 )

(cid:1)
M

M
2

(cid:0)

(cid:0)
1
2
(cid:1)
M
2

M N Γ
Γ
M 2 Γ
(cid:0)
1
Γ
2
(cid:1)
(cid:0)
M 2 Γ( 3
2 )2
Γ

2

1
2
(cid:1)
(cid:0)
M
1 + M
2

1
2 ·

.

∝

∝

∝

∝

(54)

(55)

Figure 3A shows the log posterior which starts at zero for a single bin, drops to log( 1
2 ) for
M = 2 and then increases monotonically approaching zero in the limit as M goes to inﬁnity.
The result is that a single bin is the most probable solution for two data points.

Optimal Binning

13

Fig. 3. These ﬁgures demonstrate the behavior of the relative log posterior for small numbers of
samples. (A) With only N = 2 samples, the log posterior is maximum when the samples are in the
same bin M = 1. For M > 1, the log posterior follows the function described in (55) in the text. (B)
The relative log posterior is slightly more complicated for N = 3. For M = 1 all three points lie in
the same bin. As M increases, two data points are in one bin and the remaining datum point is in
another bin. The functional form is described by (56). Eventually, all three data points lie in separate
bins and the relative log posterior is given by (57). (C) The situation is more complicated still for
N = 5 data points. As M increases, a point is reached when, depending on the particular value
of M , the points will be in separate bins. As M changes value, two points may again fall into the
same bin. This gives rise to this oscillation in the log posterior. Once all points are in separate bins,
the behavior follows a well-deﬁned functional form (58). (D) This plot shows the behavior for a large
number of data points N = 200. The log posterior now displays a more well-deﬁned mode indicating
that there is a well-deﬁned optimal number of bins. As M approaches 10000 to 100000 bins, one
can see some of the oscillatory behavior demonstrated in the small N cases.

14

K.H. Knuth

For three data points in a single bin (N = 3 and M = 1), the posterior probability is
one, resulting in a log posterior of zero. In the M > 1 case where there are two data points
in one bin and one datum point in another, the posterior probability is

(56)

(57)

(58)

and for each point in a separate bin we have

p(M

d1, d2, d3, I)
|

∝

3
4 ·

M 2
2 )(1 + M
2 )

,

(2 + M

p(M

d1, d2, d3, I)
|

∝

1
4 ·

M 2
2 )(1 + M
2 )

.

(2 + M

While the logarithm of the posterior in (56) can be greater than zero, as M increases, the
data points eventually fall into separate bins. This causes the posterior to change from
(56) to (57) resulting in a dramatic decrease in the logarithm of the posterior, which then
asymptotically increases to zero as M
. This behavior is shown in Figure 3B. The
result is that either one or two bins will be optimal depending on the relative positions of
the data points.

→ ∞

More rich behavior can be seen in the case of N = 5 data points. The results again
(Figure 3C) depend on the relative positions of the data points with respect to one another.
In this case the posterior probability switches between two types of behavior as the number
of bins increase depending on whether the bin positions force two data points together in
the same bin or separate them into two bins. The ultimate result is a ridiculous maximum a
posteriori solution of 57 bins. Clearly, for a small number of data points, the mode depends
sensitively on the relative positions of the samples in a way that is not meaningful. In these
cases there are too few data points to model a density function.

With a larger number of samples, the posterior probability shows a well-deﬁned mode
indicating a well-determined optimal number of bins. In the general case of M > N where
each of the N data points is in a separate bin, we have

p(M

d, I)
|

∝ (cid:18)

N

M
2 (cid:19)

M
Γ
2
(cid:1)
N + M
2
(cid:0)

(cid:0)

Γ

,

(cid:1)

which again results in a log posterior that asymptotically approaches zero as M
.
→ ∞
Figure 3D demonstrates these two eﬀects for N = 200. This also can be compared to the
log posterior for 1000 Gaussian samples in Figure 1B.

5.2. Sufﬁcient Data
This investigation on the eﬀects of small sample size raises the question as to how many
data points are needed to estimate the probability density function. The general shape of a
healthy log posterior reﬂects a sharp initial rise to a well-deﬁned peak, and a gradual fall-oﬀ
as the number of bins M increases from one (eg. Fig. 1B, Fig. 3D. With small sample sizes,
σi,
however, one ﬁnds that the bin heights have large error bars (Figure 4A) so that µi ≃
and that the log posterior is multi-modal (Figure 4B) with no clear peak.

We tested our algorithm on data sets with 199 diﬀerent sample sizes from N = 2 to
N = 200. One thousand data sets were drawn from a Gaussian distribution for each value
of N . The standard deviation of the number of bins obtained for these 1000 data sets at a
given value if N was used as an indicator of the stability of the solution.

Optimal Binning

15

Fig. 4. (A) An optimal density model (M = 19) for N = 30 data points sampled from a Gaussian
distribution. The fact that the error bars on the bin probabilities are as large as the probabilities
themselves indicates that this is a poor estimate. (B) The log posterior probability for the number of
bins possesses no well-deﬁned peak, and is instead reminiscent of noise. (C) This plot shows the
standard deviation of the estimated number of bins M for 1000 data sets of N points, ranging from
2 to 200, sampled from a Gaussian distribution. The standard deviation stabilizes around σM = 2
bins for N > 150 indicating the inherent level of uncertainty in the problem. This suggests that one
requires at least 150 data points to consistently perform such probability density estimates, and can
perhaps get by with as few as 100 data points in some cases.

Figure 4C shows a plot of the standard deviation of the number of bins selected for the
1000 data sets at each value of N . As we found above, with two data points, the optimal
solution is always one bin giving a standard deviation of zero. This increases dramatically
as the number of data points increases, as we saw in our example with N = 5 and M = 57.
This peaks around N = 15 and slowly decreases as N increases further. The standard
2
deviation of the number of bins decreased to σM < 5 for N > 100, and stabilized to σM ≃
for N > 150.
While 30 samples may be suﬃcient for estimating the mean and variance of a density
function known to be Gaussian, it is clear that more samples are needed to reliably estimate
the shape of an unknown density function. In the case where the data are described by a
Gaussian, it would appear that at least 100 samples, and preferentially 150 samples, are
required to accurately and consistently infer the shape of the density function. By examining
the shape of the log posterior, one can easily determine whether one has suﬃcient data to
estimate the density function. In the event that there are too few samples to perform such
estimates, one can either incorporate additional prior information or collect more data.

16

K.H. Knuth

Fig. 5. N = 1000 data points were sampled from a Gaussian distribution N (0, 1). The top plots
show (A) the estimated density function using optimal binning and (B) the relative log posterior, which
exhibits a well-deﬁned peak at M = 11 bins. The bottom plots reﬂect the results using the same data
set after it has been rounded with ∆x = 0.1 to keep only the ﬁrst decimal place. (C) There is no
optimal binning as the algorithm identiﬁes the discrete structure as being a more salient feature than
the overall Gaussian shape of the density function. (D) The relative log posterior displays no well-
deﬁned peak, and in addition, for large numbers of M displays a monotonically increase given by
(58) that asymptotes to a positive value. This indicates that the data have been severely rounded.

6. Digitized Data

Due to the way that computers represent data, all data are essentially represented by
integers (Bayman and Broadhurst, 1979). In some cases, the data samples have been in-
tentionally rounded or truncated, often to save storage space or transmission time. It is
well-known that any non-invertible transformation, such as rounding, destroys information.
Here we investigate how severe losses of information due to rounding or truncation aﬀects
the optBINS algorithm.

When data are digitized via truncation or rounding, the digitization is performed so as
to maintain a resolution that we will denote by ∆x. That is, if the data set has values that
range from 0 to 1, and we represent these numbers with an 8 bits, the minimum resolution
we can maintain is ∆x = 1/28 = 1/256. For a suﬃciently large data set (in this example
N > 256) it will be impossible for every datum point to be in its own bin when the number
of bins is greater than a critical number, M > M∆x, where

M∆x =

V
∆x

,

(59)

(60)

(61)

Optimal Binning

17

and V is the range of the data considered. Once M > M∆x the number of populated bins
P will remain unchanged since the bin width w for M > M∆x will be smaller than the
digitization resolution, w < ∆x.

For all bin numbers M > M∆x, there will be P populated bins with populations
n1, n2, . . . , nP . This leads to a form for the marginal posterior probability for M (35)
that depends only on the number of instances of each discrete value that was recorded,
n1, n2, . . . , nP . Since these values do not vary for M > M∆x, the marginal posterior can be
viewed solely as a function of M

p(M

d, I)
|

∝ (cid:18)

N

M
2 (cid:19)

M
Γ
2
(cid:1)
N + M
2
(cid:0)

(cid:0)

Γ

(cid:1)

2N

·

Q

P
p=1 Γ(np + 1
2 )
1
2

Γ

P

,

(cid:0)

(cid:1)

where the product over p is over populated bins only. Comparing this to (58), the function
on the right-hand side asymptotically approaches a value greater than one—so that its
logarithm increases asymptotically to a value greater than zero.

As the number of bins M increases, the point is reached where the data can not be
further separated; call this point Mcrit. In this situation, there are np data points in the
pth bin and the posterior probability can be written as

p(M

d, I)
|

∝ (cid:18)

N

M
2 (cid:19)

M
Γ
2
(cid:1)
N + M
2
(cid:0)

(cid:0)

Γ

(cid:1)

P

·

Yp=1

(2np −

1)!!,

where !! denotes the double factorial. For M > Mcrit, as M
asymptotes to

→ ∞
1)!!), which can be further simpliﬁed to

P

, the log posterior

p=1 log((2np −

P

P

log((2np −

Xp=1

1)!!) = (P

N ) log(2) +

log s.

(62)

−

P

2np−1

Xp=1

Xs=np

To test for excessive rounding or truncation, the mode of log p(M

d, I) for M < Mcrit
|
should be compared to (62) above. If the latter is larger, than the discrete nature of the data
is a more signiﬁcant feature than the general shape of the underlying probability density
function. When this is the case, a reasonable histogram model of the density function can
still be obtained by adding a uniformly-distributed random number, with a range deﬁned
by the resolution ∆x, to each datum point (Bayman and Broadhurst, 1979). While this will
produce the best histogram possible given the data, this will not recover the lost information.

7. Multi-Dimensional Histograms

In this section, we demonstrate that our method can be extended naturally to multi-
dimensional histograms. We begin by describing the method for a two-dimensional his-
togram. The constant-piecewise model h(x, y) of the two-dimensional density function
f (x, y) is

h(x, y; Mx, My) =

πj,k Π(xj−1, x, xj)Π(yk−1, y, yk),

(63)

M
V

Mx

My

Xj=1

Xk=1

where M = MxMy, V is the total area of the histogram, j indexes the bin labels along x,
1 bin probability
and k indexes them along y. Since the πj,k all sum to unity, we have M

−

18

K.H. Knuth

Fig. 6. 10000 samples were drawn from a two-dimensional Gaussian density to demonstrate the
optimization of a two-dimensional histogram. (A) The relative logarithm of the posterior probability is
plotted as a function of the number of bins in each dimension. The normalization constant has been
neglected in this plot, resulting in positive values of the log posterior. (B) This plot shows the relative
log posterior as a contour plot. The optimal number of bins is found to be 12 × 14. (C) The optimal
histogram for this data set. (D) The histogram determined using Stone’s method has 27 × 28 bins.
This histogram is clearly sub-optimal since it highlights random variations that are not representative
of the density function from which the data were sampled.

(64)

(65)

(66)

(67)

density parameters as before, where M is the total number of bins. The likelihood of
obtaining a datum point dn from bin (j, k) is still simply

Optimal Binning

19

p(dn|
The previous prior assignments result in the posterior probability

πj,k, Mx, My, I) =

πj,k.

M
V

p(π, Mx, My|

d, I)

∝ (cid:18)

M
V (cid:19)

N Γ

M
2

Γ

(cid:0)
1
2

(cid:1)
M

(cid:0)

(cid:1)

Mx

My

Yj=1

Yk=1

nj,k− 1
2
j,k

π

,

where πMx,My is 1 minus the sum of all the other bin probabilities. The order of the
bins in the marginalization does not matter, which gives a result similar in form to the
one-dimensional case

p(Mx, My|

d, I)

∝ (cid:18)

M
V (cid:19)

N Γ

M
2

Γ

(cid:0)
1
2

(cid:0)

(cid:1)

Mx
j=1

My
k=1 Γ(nj,k + 1
2 )
Q
Γ(N + M
2 )

,

M Q
(cid:1)

where M = MxMy.

For a D-dimensional histogram, the general result is

p(M1,

, MD|

d, I)

· · ·

∝ (cid:18)

M
V (cid:19)

N Γ

M
2

Γ

(cid:0)
1
2

M1
i1=1 · · ·

MD
iD =1 Γ(ni1,...,iD + 1
2 )
Q
Γ(N + M
2 )

,

(cid:1)
M Q

(cid:1)
where Mi is the number of bins along the ith dimension, M is the total number of bins, V
is the D-dimensional volume of the histogram, and ni1,...,iD indicates the number of counts
in the bin indexed by the coordinates (i1, . . . , iD). Note that the result in (36) can be used
directly for a multi-dimensional histogram simply by relabelling the multi-dimensional bins
with a single index.

(cid:0)

Figure 6 demonstrates the procedure on a data set sampled from a two-dimensional
Gaussian. In this example, 10000 samples were drawn from a two-dimensional Gaussian
density. Figure 6A shows the relative logarithm of the posterior probability plotted as a
function of the number of bins in each dimension. The same surface is displayed as contour
14. Figure 6C
plot in Figure 6B, where we ﬁnd the optimal number of bins to be 12
shows the optimal two-dimensional histogram model. Note that modelled density function
is displayed in terms of the number of counts rather than the probability density, which can
be easily computed using (49) with error bars computed using (51). In Figure 6D, we show
the histogram obtained using Stone’s method, which results in a 27
28 array of bins. This
is clearly a sub-optimal model since random sampling variations are easily visible.

×

×

8. Discussion

The optimal binning algorithm presented in this paper, optBINS, relies on ﬁnding the mode
of the marginal posterior probability of the number of bins in a piecewise-constant density
function. This posterior probability originates as a product of the likelihood of the density
parameters given the data and the prior probability of those same parameter values. As
the number of bins increases, the model can better ﬁt the data, which leads to an increase
in the likelihood function. However, by introducing additional bins, the prior probability,

20

K.H. Knuth

which must sum to unity, is now spread out over a parameter space of greater dimensionality
resulting in a decrease of the prior probability. Thus a density model with more bins will
result in a greater likelihood, but also in a smaller prior probability. Since the posterior
is a product of these two functions, the maximum of the posterior probability occurs at a
point where these two opposing factors are balanced. This interplay between the likelihood
and the prior probability eﬀectively implements Occam’s razor by selecting the most simple
model that best describes the data.

The quality of this algorithm was demonstrated by testing it against several other pop-
ular bin selection techniques. The optBINS algorithm outperformed all techniques inves-
tigated in the sense that it consistently resulted in the minimum integrated square error
between the estimated density model and the true density function. This was true even in
the case of Scott’s technique, which is designed to asymptotically minimize this error for
Gaussian-distributed data. Our algorithm also can be readily applied to multi-dimensional
data sets, which we demonstrated with a two-dimensional data set. In practice, we have
been applying optBINS to three-dimensional data sets with comparable results.

It should be noted that we are working with a piecewise-constant model of the density
function, and not a histogram. The distinction is subtle, but important. Given the full pos-
terior probability for the model parameters and a selected number of bins, one can estimate
the mean bin probabilities and their associated standard deviations. This is extremely use-
ful in that it quantiﬁes uncertainties in the density model, which can be used in subsequent
calculations. In this paper, we demonstrated that with small numbers of data points the
magnitude of the error bars on the bin heights is on the order of the bin heights themselves.
Such a situation indicates that too few data exist to infer a density function. This can
also be determined by examining the marginal posterior probability for the number of bins.
In cases where there are too few data points, the posterior will not possess a well-deﬁned
mode. In our experiments with Gaussian-distributed data, we found that approximately
150 data points are needed to accurately estimate the density model when the functional
form of the density is unknown. This approach has an additional advantage in that we
can use the optBINS algorithm to identify data sets where the data have been excessively
truncated. This will be explored further in future papers (Knuth et al., 2006).

As always, there is room for improvement. The optBINS algorithm, as currently imple-
mented, performs a brute force search of the number of bins. This can be slow for large data
sets that require large numbers of bins, or multi-dimensional data sets that have multiple
bin dimensions. We have also made some simplifying assumptions in designing the algo-
rithm. First, the endpoints of the density model are deﬁned by the data, and are not allowed
to vary during the analysis. Second, we use the marginal posterior to select the optimal
number of bins and then use this value to estimate the mean bin heights. This neglects the
fact that we actually possess uncertainty about the number of bins. Thus the uncertainty
in the number of bins is not quantiﬁed. Last, equally-spaced bins can be very ineﬃcient in
describing multi-modal density functions (as in Fig. 1G.) In such cases, variable bin-width
models such as Bayesian Blocks (Jackson et al., 2005) or the Markov chain Monte Carlo
implementations that we have experimented with may prove to be more useful.

For most applications, optBINS eﬃciently delivers an appropriate number of bins that
both maximizes the depiction of the shape of the density function while minimizing the
appearance of random ﬂuctuations.

Optimal Binning

21

Appendix 1: Matlab code for the optimal number of uniform bins in a histogram

optM = optBINS(data,minM,maxM);

% optBINS computes the optimal number of bins for a given one-dimensional
% data set. This optimization is based on the posterior probability for
% the number of bins
%
% Usage:
%
%
% Where:
%
%
%
%
% This algorithm uses a brute-force search trying every possible bin number
% in the given range. This can of course be improved.
% Generalization to multidimensional data sets is straightforward.
%
% Created by Kevin H. Knuth on 17 April 2003
% Modified by Kevin H. Knuth on 15 March 2006

data is a (1,N) vector of data points
minM is the minimum number of bins to consider
maxM is the maximum number of bins to consider

function optM = optBINS(data,minM,maxM)

if size(data)>2 | size(data,1)>1

error(’data dimensions must be (1,N)’);

end
N = size(data,2);

% Simply loop through the different numbers of bins
% and compute the posterior probability for each.
logp = zeros(1,maxM);
for M = minM:maxM

n = hist(data,M); % Bin the data (equal width bins here)
part1 = N*log(M) + gammaln(M/2) - gammaln(N+M/2);
part2 = - M*gammaln(1/2) + sum(gammaln(n+0.5));
logp(M) = part1 + part2;

end

[maximum, optM] = max(logp);
return

References

Abramowitz, M. and Stegun, I. A. (1972). Handbook of Mathematical Functions. New York:

Dover Publications, Inc., p. 255.

Akaike, H. (1974). A new look at the statistical identiﬁcation model. IEEE Trans. Automatic

Control. 19, 716–723.

22

K.H. Knuth

Bayman, B. F. and Broadhurst, J. B. (1979). A simple solution to a problem arising from the
processing of ﬁnite accuracy digital data using integer arithmetic. Nuclear Instruments
and Methods. 167, 475–478.

Berger, J. O. and Bernardo, J. M. (1992). Ordered group reference priors with application

to the multinomial problem. Biometrika. 79, 25–37.

Box, G. E. P. and Tiao, G. C. (1992). Bayesian Inference in Statistical Analysis. New York:

John Wiley & Sons, p. 55.

Freedman, D. and Diaconis, P. (1981). On the histogram as a density estimator: L2 theory.

Zeitschrift f¨ur Wahrscheinlichkeitstheorie verw. Gebiete. 57, 453–476.

Hartigan, J. A. (1996). Bayesian histograms. Bayesian Statistics. (eds. J.M. Bernardo, J.O.
Berger, A.P. Dawid, A.F.M. Smith) vol. 5, pp. 211–222. Oxford: Oxford Univ. Press.

Jackson, B., Scargle, J., Barnes, D., Arabhi, S., Alt, A., Gioumousis, P., Gwin, E., Sang-
trakulcharoen, P., Tan, L., and Tsai, T. T. (2005). An algorithm for optimal partitioning
of data on an interval. IEEE Signal Processing Letters. 12, pp. 105–108.

Jeﬀreys, H. (1961). Theory of Probability 3rd. ed. Oxford: Oxford University Press.

Knuth, K. H., Castle, J. P., Wheeler, K. R. (2006).

Identifying excessively rounded or
truncated data. (Invited paper) Proc. of the 17th meeting of the Int. Assoc. for Statisti-
cal Computing-European Regional Section: Computational Statistics (COMPSTAT 2006)
Florence, Italy, Aug 2006.

Rudemo, M. (1982). Empirical choice of histograms and kernel density estimators. Scand.

J. Statist.. 9, 65–78.

Scott, D. W. (1979). On optimal and data-based histograms. Biometrika. 66, 605–610.

Stone, C. J. (1984). An asymptotically histogram selection rule. Proc. Second Berkeley

Symp (ed. J. Neyman) pp. 513–520. Berkeley: Univ. California Press.


Entropy, Information Matrix and order statistics of

Multivariate Pareto, Burr and related distributions

1 Iran University of Science and Technology, Narmak, Tehran 16844, Iran.

2 Laboratoire des Signaux et Syst`emes (Cnrs,Sup´elec,Ups),

Sup´elec, Plateau de Moulon, 3 rue Joliot-Curie, 91192 Gif-sur-Yvette, France.

Gholamhossein Yari 1,2

Ali Mohammad-Djafari 2

February 2, 2008

Abstract

In this paper we derive the exact analytical expressions for the information and

covariance matrices of the multivariate Burr and related distributions. These distribu-

tions arise as tractable parametric models in reliability, actuarial science, economics,

ﬁnance and telecommunications. We show that all the calculations can be obtained

from one main moment multi dimensional integral whose expression is obtained through

some particular change of variables.

keywords: Gamma and Beta functions; Polygamma functions ; Information matrix;

Covariance matrix; Multivariate Burr models.

4
0
0
2
 
r
p
A
 
3
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
6
0
4
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

1 Introduction

In this paper the exact form of Fisher information matrix for multivariate Pareto (IV) and

related distributions is determined. It is well-known that the information matrix is a valu-

able tool for derivation of covariance matrix in the asymptotic distribution of maximum

likelihood estimations (MLE). In the univariate case of the above distributions, the Fisher

information matrix is found by Brazauskas [4]. As discussed in Serﬂing [16], section 4, un-

der suitable regularity conditions, the determinant of the asymptotic covariance matrix of

(MLE) reaches an optimal lower bound for the volume of the spread ellipsoid of joint estima-

tors. In the univariate case of the Pareto (IV), this optimality property of (MLE) is widely

used in the robustness versus eﬃciency studies as a quantitative benchmark for eﬃciency

considerations (Brazauskas and Serﬂing [6, 5], Brazauskas [3], Hampel et al [9], Huber [10],

Klugman [14], Kimber [13, 12] and Lehmann [15], Chapter 5). These distributions are suit-

able for situations involving relatively high probability in the upper tails. More speciﬁcally,

such models have been formulated in the context of actuarial science, reliability, economics,

ﬁnance and teletraﬁc. These models arise whenever we need to infer the distributions of

variables such as sizes of insurance claims, sizes of ﬁrms, income in a population of people,

stock price ﬂuctuations and length of telephone calls. For a broad discussion of Pareto mod-

els and diverse applications see Arnold [2], Johnson, Kotz and Balakrishnan [11], Chapter

19. Gomes, Selman and Crato [8] have recently discovered Pareto (IV) tail behavior in the

cost distributions of combinatorial search algorithms.

This paper is organized as follows: Multivariate Pareto and Burr distribution are intro-

duced and presented in section 2. Elements of the information and covariance matrix for

multivariate Pareto (IV) distribution is derived in section 3. Elements of the information

matrices for Multivariate Burr, Pareto (III), and Pareto (II) distributions are derived in

section 4. Conclusion is presented in section 5. Derivation of ﬁrst and second derivatives of

the log density and the main moment integral calculation are given in Appendices A and B

.

2

2 Multivariate Pareto and Burr distributions

As discussed in Arnold [2] Chapter 3, a hierarchy of Pareto distribution is established by

starting with the classical Pareto (I) distribution and subsequently introducing additional

parameters related to location, scale, shape and inequality (Gini index). Such an approach

leads to a very general family of distributions, called the Pareto (IV) family, with the cumu-

lative distribution function

FX(x) = 1 −

1 + (

,

x > µ,

(1)

−α

x − µ
θ

1
γ

)

(cid:19)

(cid:18)

where −∞ < µ < +∞ is the location parameter, θ > 0 is the scale parameter, γ > 0 is the

inequality parameter and α > 0 is the shape parameter which characterizes the tail of the

distribution. We note this distribution by Pareto (IV) (µ, θ, γ, α). Parameter γ is called the

inequality parameter because of its interpretation in the economics context. That is, if we

choose α = 1 and µ = 0 in expression (1), the parameter (γ ≤ 1) is precisely the Gini index

of inequality. For the Pareto (IV) (µ, θ, γ, α) distribution, we have the density function

The density of the n-dimensional Pareto (IV) distribution is

(cid:17)

1
γ

−1

x−µ
θ

α
1 + ( x−µ
(cid:1)
θ )

(cid:0)

1
γ

θγ

(cid:16)

fX(x) =

α+1 ,

x > µ.

(2)

fn(x) =

1 +

 

n

j=1
X

(

xj − µj
θj

1
γj

)

!

−(α+n) n

i=1
Y

α + i − 1
θiγi

(

xi − µi
θi

1
γi

−1,

)

xi > µi,

(3)

where x = [x1, · · · , xn], xi > µi, α > 0, γi > 0, θi > 0 for i = 1, · · · , n. One of the main

properties of this distribution is that, the joint density of any subset of the components of a

Pareto random vector is again of the form (3) [2].

The n-dimensional Burr distribution has the density

fn(x) =

1 +

 

n

(

xj − µj
θj

)cj

!

−(α+n) n

(α + i − 1)ci
θi

(

xi − µi
θi

)ci−1,

xi > µi,

(4)

i=1
Y
where xi > µi, α > 0, ci > 0, θi > 0 for i = 1, · · · , n. We note that the multivariate Burr

j=1
X

distribution is equivalent to the multivariate Pareto distribution with 1
γi

= ci.

3

3 Information Matrix for Multivariate Pareto (IV)

Suppose X is a random vector with the probability density function fΘ(.) where Θ =

(θ1, θ2, ..., θK). The information matrix I(Θ) is the K × K matrix with elements

Iij(Θ) = −EΘ

,

i, j = 1, · · · K.

(5)

∂2ln fΘ(X)
∂θi∂θj

(cid:21)

(cid:20)

For the multivariate Pareto (IV), we have Θ = (µ1, ..., µn, θ1, ..., θn, γ1, ..., γn, α). In order

to make the multivariate Pareto (IV) distribution a regular family (in terms of maximum

likelihood estimation), we assume that µ is known and, without loss of generality, equal to

0. In this case information matrix is (2n + 1) × (2n + 1). Thus, further treatment is based

on the following multivariate density function

n

n

fn(x) =

1 +

 

j=1 (cid:18)
X

1
γj

xj
θj (cid:19)

!

−(α+n) n

−1

1
γi

α + i − 1
θiγi

xi
θi (cid:19)

(cid:18)

i=1
Y

,

xi > 0.

(6)

The log-density is:

ln fn(x) =

ln(α + i − 1) − ln θi +

− 1

ln

− ln γi

xi
θi (cid:19)

(cid:19)

(cid:18)

(cid:21)

i=1 (cid:20)
X

n

−(α + n) ln

1 +

 

1
γi

(cid:18)

1
γj

xj
θj (cid:19)

.

!

j=1 (cid:18)
X
Since the information matrix I(Θ) is symmetric it is enough to ﬁnd elements Iij(Θ), where

1 ≤ i ≤ j ≤ 2n + 1. The required ﬁrst and second partial derivatives of the above expression

(7)

are given in the Appendix A. Looking at these expressions, we see that to determine the

expression of the information matrix and score functions, we need to ﬁnd the expressions of:

E

ln

1 +

"

 

n

j=1 (cid:18)
X

1
γj

Xj
θj (cid:19)

, E

!#

rl
γl

Xl
θl (cid:19)

#

"(cid:18)

, E

l
γl

Xl
θl (cid:19)

,

#

"(cid:18)

E

ln

(cid:20)

(cid:18)

Xl
θl (cid:19)(cid:21)

, E

rl
γl

Xl
θl (cid:19)

"(cid:18)

ln

Xl
θl (cid:19)#

(cid:18)

1
γl

Xl
θl

, E 

1 +





(cid:18)

(cid:16)
(cid:17)
n
j=1

Xj
θj

1
γj

P

(cid:16)

(cid:17)



,





(cid:19)

4

and the general terms

n1
γl

Xl
θl

Xk
θk

n2

γk lnn4

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
Xj
θj

E 

(cid:16)

(cid:17)

lnn3
n5

Xk
θk

1
(cid:17)
γj

Xl
θl

,


(cid:17)

(cid:16)

(n1, n2 > −1) ∈ IR, n3, n4 ∈ NN+ and n5 ∈ IR+.








3.1 Main strategy to obtain expressions of the expectations

P

(cid:18)

(cid:19)

(cid:17)

(cid:16)

Derivation of these expressions are based on the following strategy: ﬁrst, we derive an

analytical expression for the following integral

n

E

"

i=1 (cid:18)
Y

ri
γi

Xi
θi (cid:19)

+∞

+∞

n

=

#

0
Z

· · ·

0
Z

ri
γi

xi
θi (cid:19)

i=1 (cid:18)
Y

fn(x) dx,

(8)

and then, we show that all the other expressions can be found easily from it. We consider

this derivation as one of the main contributions of this work. This derivation is given in the

Appendix B. The result is the following:

n

E

"

i=1 (cid:18)
Y

ri
γi

Xi
θi (cid:19)

#

+∞

+∞

n

=

· · ·

0
Z
Γ(α −

0
Z
n
i=1 ri)

ri
γi

xi
θi (cid:19)
i=1 (cid:18)
Y
n
i=1 Γ(ri + 1)

,

fn(x) dx =

P

Γ(α)
Q

n

i=1
X

ri < α,

ri > −1, ri ∈ IR,

(9)

where Γ is the usual Gamma function,

Γrlrk

α −

 

ri

=

!

n

i=1
X

∂2Γ (α −

n
i=1 ri)

∂rk∂rl
P

,

1 ≤ l, k ≤ n,

Ψ(n)(z) =

,

z > 0,

dn
dzn

Γ′(z)
Γ(z)

(cid:18)

(cid:19)

∂(m+n)
l ∂rn
∂rm

k (cid:18)

Γrlrk (z)
Γ(z)

(cid:19)

= Ψ(m+n)(z),

z > 0

and integers n, m ≥ 0 (Abramowitz and Stegun [1]). Speciﬁcally, we use digamma Ψ(z) =
Ψ(.)(z), trigamma Ψ′(z) and Ψrlrk(z) functions. To conﬁrm the regularity of ln fn(x) and
evaluation the expected Fisher information matrix, we take expectations of ﬁrst and second

order partial derivatives of (7). All the other expressions can be derived from this main

5

result. Taking of derivative with respect to α, from the both sides of the relation

+∞

fn(x) dx,

1 =

0
Z

n

leads to

Xj
θj (cid:19)
From relation (9), for a pair of (l, k) we have

j=1 (cid:18)
X

1 +

ln

 

E

"

1
γj

=

!#

n

i=1
X

1
α + i − 1

.

ϕ(rl, rk) = E

rk
γk

rl
γl

Xl
θl (cid:19)

Xk
θk (cid:19)

(cid:18)

=

#

"(cid:18)

Γ(α − rl − rk)Γ(rl + 1)Γ(rk + 1)
Γ(α)

,

and

∂(n3+n4)
l ∂rn4
∂rn3

k

ϕ(rl = n1, rk = n2) = E

n1
γl

Xl
θl (cid:19)

"(cid:18)

Xk
θk (cid:19)

(cid:18)

n2
γk

lnn4

Xk
θk (cid:19)

(cid:18)

lnn3

Xl
θl (cid:19)#

.

(cid:18)

From relation (11), at rk = 0 we obtain

rl
γl

Xl
θl (cid:19)

E

"(cid:18)

=

#

Γ(α − rl)Γ(rl + 1)
Γ(α)

,

and evaluating this expectation at rl = 1, we obtain

Writing the expression of the expectation

l
γl

Xl
θl (cid:19)

E

"(cid:18)

=

#

1
α − 1

.

1
γl

Xl
θl

E 

(cid:16)
(cid:17)
n
j=1

Xj
θj

1
γj

(cid:16)

(cid:17)

(cid:19)







1 +





(cid:18)

P

as Eα to emphasis the role of the parameter α in (6), it can easily be shown that

1
γl

Xl
θl

Eα 

1 +





(cid:18)

(cid:17)
(cid:16)
n
j=1

Xj
θj

1
γj

P

(cid:16)

(cid:17)





(cid:19)



=

α
α + n

Eα+1

1
γl

Xl
θl (cid:19)

.

#

"(cid:18)

6

(10)

(11)

(12)

(13)

(14)

(15)

Using (14) with α replaced by α + 1, we now obtain an expression for the last expectation

as

and

(16)

(17)

(18)

Diﬀerentiating (13) with respect to rl, and replacing for rl = 0 and rl = 1, we obtain the

following relations:

1
γl

Xl
θl

Eα 

1 +





(cid:18)

(cid:16)
(cid:17)
n
j=1

Xj
θj

1
γj

P

(cid:16)

(cid:17)





(cid:19)



=

1
α + n

.

E

ln

(cid:20)
1
γl

(cid:18)

ln

Xl
θl (cid:19)(cid:21)
Xl
θl (cid:19)#

(cid:18)

E

"(cid:18)

Xl
θl (cid:19)

= γl [Γ′(1) − Ψ(α)] ,

Γ′(2) − Ψ(α − 1)
α − 1

,

(cid:21)

= γl

(cid:20)

1

γl ln

Xl
θl

Xl
θl

Eα 

(cid:16)
1 +

(cid:17)

n
j=1

(cid:16)
Xj
θj

1
(cid:17)
γj

P

(cid:16)

(cid:17)





(cid:18)





(cid:19)



=

α
α + n

Eα+1

1
γl

Xl
θl (cid:19)

"(cid:18)

ln

Xl
θl (cid:19)#

.

(cid:18)

3.2 Expectations of the score functions

The expectations of the ﬁrst three partial derivations of the ﬁrst order follow immediately

from the corresponding results for their three corresponding parameters and we obtain:

∂ln fn(X)
∂α

E

(cid:20)

n

=

(cid:21)

i=1
X

1
α + i − 1

− E

ln

1 +

"

 

n

1
γj

Xj
θj (cid:19)

= 0,

!#

∂ln fn(X)
∂θl

E

(cid:20)

= −

(cid:21)

1
θlγl

+

α + n
θlγl (cid:19)

(cid:18)

E 

∂ln fn(X)
∂γl

E

(cid:20)

= −

−

E

ln

1
γl

1
γ2
l

(cid:20)

(cid:18)

Xl
θl (cid:19)(cid:21)

+

α + n
γ2
l (cid:19)

(cid:18)

(cid:21)

j=1 (cid:18)
X

1
γl

Xl
θl

(cid:16)
(cid:17)
n
j=1

Xj
θj

1
γj

= 0,







1

(cid:19)
(cid:17)
γl ln

(cid:16)

Xl
θl

Xl
θl

1 +





(cid:18)

P

E 

(cid:16)
1 +

(cid:17)

n
j=1

(cid:16)
Xj
θj

1
(cid:17)
γj

(cid:16)

(cid:17)

(cid:19)

P





(cid:18)







= 0.

7

3.3 The expected Fisher information matrix

Main strategy is again based d on the integral (9) which is presented in the Appendix B.

However, derivation of the following expressions can be obtained mecanically but after some

tedious algebraic simpliﬁcations :

Ix(α) =

Ix(θl, α) = −

n

i=1
X

1
(α + i − 1)2 ,

1
θlγl (α + n)

,

Ix(γl, α) = −

1
γl (α + n)

[Γ′(2) − Ψ(α)] ,

l = 1, · · · , n,

Ix(θl) =

α + n − 1
l (α + n + 1)

,

l γ2
θ2

l = 1, · · · , n,

Ix(γl) =

Γ′′(α)
Γ(α)

+ Γ′′(1) + 1

(cid:21)

[Γ′(1) − Ψ(α)]

+

α + n − 1
γ2
l (α + n + 1)
(cid:20)
2(α + n − 2)
γ2
l (α + n + 1)
2(α + n − 1)
γ2
l (α + n + 1)

−

[Γ′(1)Ψ(α)] ,

l = 1, · · · , n,

Ix(θl, θk) = −

1
θlγlγkθk (α + n + 1)

,

k 6= l,

Ix(γl, γk) =

−1
γlγk (α + n + 1)

(Γ′(2))2 − Γ′(2) (Ψrl(α) + Ψrk(α)) + Ψrl rk(α))
h

i

, k 6= l, (25)

Ix(θl, γk) = −

1
θlγlγk (α + n + 1)

[Γ′(2) − Ψrk(α)] ,

k 6= l,

Ix(θl, γl) =

α + n − 1
l (α + n + 1)

θlγ2

[Γ′(2) − Ψ(α)] −

1

θlγ2

l (α + n + 1)

(cid:21)

(cid:20)

,

l = 1, · · · , n.

(27)

Thus the information matrix, I MP(IV)(Θ), for the multivariate Pareto (IV) (0, θ, γ, α)
distribution is

I MP(IV)(Θ) = 





I(θl, θk)

I(θl, γk)

I(θl, α)

I(θl, γk)

I(γl, γk)

I(γl, α)

I(θl, α)

I(γl, α)

I(α)



.






8

(19)

(20)

(21)

(22)

(23)

(24)

(26)

(28)

3.4 Covariance matrix for multivariate Pareto (IV)

Since the joint density of any subset of the components of a Pareto (IV) random vector is

again a multivariate Pareto (IV), Arnold [2], we can calculate the expectation

E

(cid:20)(cid:18)
∞

Xl − µl

ml

Xk − µk

mk

θl (cid:19)

(cid:18)
xl − µl

∞

θk (cid:19)

(cid:21)
xk − µk

ml

=

mk

θl (cid:19)

0 (cid:18)

0 Z
(cid:18)
Z
Γ(α − mlγl − mkγk)Γ(mlγl + 1)Γ(mkγk + 1)
Γ(α)

,

θk (cid:19)

fXl,Xk(xl, xk) dxl dxk =

ml, mk ∈ IR, mlγl, mkγk > −1, α − mlγl − mkγk > 0.

(29)

Evaluating this expectation at (ml = 1, mk = 0), (ml = 0, mk = 1) and (ml = 1, mk = 1),

we obtain

E [Xl] = µl +

[Γ(α − γl)Γ(γl + 1)],

γl < α,

γl > −1,

E [Xk] = µk +

[Γ(α − γk)Γ(γk + 1)],

γk < α,

γk > −1,

E [XlXk] = µkE [Xl] + µlE [Xk] − µlµk

[Γ(α − γl − γk)Γ(γl + 1)Γ(γk + 1)],

γl + γk < α,

(32)

[Γ(α − mlγl)Γ(mlγl + 1)],

γlml < α,

σ2
Xl =

θ2
l
Γ2(α)

(cid:2)

Γ(α − 2γl)Γ(2γl + 1)Γ(α) − Γ2(γl + 1)Γ2(α − γl)

,

2γl < α,

Cov [X, Y ] =

[Γ(α − γl − γk)Γ(α)

θlθkΓ(γl + 1)Γ(γk + 1)
Γ2(α)
−Γ(α − γk)Γ(α − γl)],

(cid:3)

1 ≤ l ≤ k ≤ n,

k = 2, · · · , n.

(35)

θl
Γ(α)
θk
Γ(α)

+

θlθk
Γ(α)

E [X m

l ] =

θm
l
Γ(α)

(30)

(31)

(33)

(34)

9

4 Special Cases

4.1 Burr(θ, γ, α) distribution

The Burr family of distributions is also suﬃciently ﬂexible and enjoy long popularity in the

actuarial science literature (Daykin, Pentik¨ainen, and Pesonen [7] and Klugman, Panjer, and

Willmot [14]). However, this family can be treated as a special case of Pareto (IV): Burr

(θ, γ, α) = Pareto (IV) (0, θ, 1

γ , α) (Klugman, Panjer, and Willmot [14], p. 574).

Since the Burr distribution is a reparametrization of Pareto (IV) (0, θ, γ, α), it follows

from Lehmann (8), Section 2.7, that its information matrix I B(Θ) can be derived from
I P(IV)(Θ) by JI P(IV)(Θ)J ′, where J is the Jacobian matrix of the transformation of
variables. Thus, the information matrix of multivariate Burr distribution, I MB(Θ) is then
given by JI MP(IV)(Θ)J ′, where

I

0

1

Iγ2

0

0

J = 





1

0

1








which is obtained by noting that J is the Jacobian matrix of the transformation (θ, γ, α) →

(θ, 1

γ , α).

4.2 Pareto (III) (0, θ, γ) distribution

This is a special case of Pareto (IV) with α = 1. Therefore, last row and last column of

I MP(IV)(Θ) vanish (these represent information about parameter α) and we obtain

I MP(III)(Θ) =

I(θl, θk)

I(θl, γk)





,

I(θl, γk)
I(γl, γk) 


where we have to substitute α = 1 in all the remaining expressions.

(36)

(37)

10

4.3 Pareto (II) (0, θ, α) distribution

This is a special case of Pareto (IV) with γ = 1. Therefore I(θl, γk), I(γl, γk) and I(γl, α)

in I MP(IV)(Θ) vanish and we obtain

I MP(II)(Θ) =

I(θl, θk)

I(θl, α)

I(θl, α)

I(α)





,





where we have to substitute γ = 1 in all the remaining expressions.

(38)

5 Conclusion

In this paper we obtained the exact form of Fisher information and covariance matrix for

multivariate Pareto (IV) distribution. We showed that all the calculations can be obtained

from one main moment multi dimensional integral which has been considered and whose

expression is obtained through some particular change of variables. A short method of

obtaining some of the expectations as a function of α is used. To conﬁrm the regularity
of the ln fn(x), we showed that the expectations of the score functions are equal to 0.

Information matrices of multivariate Burr, Pareto (III) and Pareto (II) distributions are

derived as special cases of multivariate Pareto (IV) distribution.

11

A Expressions of the derivatives

In this Appendix, we give detailed expressions of all the ﬁrst and second derivatives of

ln fn(x) which are needed for obtaining the expression of the information matrix:

∂ln fn(x)
∂α

=

1
α + i − 1

n

i=1
X

n

j=1 (cid:18)
X

1
γj

xj
θj (cid:19)

,

!

− ln

1 +

 

xl
θl

1
γl

∂ln fn(x)
∂θl

= −

1
θlγl

+

α + n
θlγl (cid:19)

(cid:18)

,

l = 1, · · · , n,

(2)

∂ln fn(x)
∂γl

= −

−

ln

1
γl

1
γ2
l

xl
θl (cid:19)

(cid:18)

+

α + n
γ2
l (cid:19)

(cid:18)

,

l = 1, · · · , n,

(3)

1 +

(cid:18)

P

(cid:16)
(cid:17)
n
j=1

xj
θj

1
γj

(cid:16)

(cid:17)

1

γl ln

xl
θl

(cid:19)

xl
θl

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

1
(cid:17)
γj

(cid:16)

(cid:17)

(cid:19)

(cid:18)

n

P
1
(α + i − 1)2

,

∂2ln fn(x)
∂α2

= −

∂2ln fn(x)
∂θk∂α

=

1
θkγk (cid:19)

(cid:18)

∂2ln fn(x)
∂γk∂α

=

1
γ2
k (cid:19)

(cid:18)

i=1
X

xk
θk

1
γk

1
γj

(cid:18)

1 +

(cid:16)

(cid:17)
n
j=1

1

P
γk ln

xk
θk

xj
θj

xk
θk

(cid:16)

(cid:17)

(cid:19)

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

1
(cid:17)
γj

(cid:18)

P

(cid:16)

(cid:17)

(cid:19)

∂2ln fn(x)
2
∂θl

=

1
θ2
l γl

−

α + n
θ2
l γl (cid:19) (cid:18)

(cid:18)

1 +

1
γl (cid:19)

,

k = 1, · · · , n,

,

k = 1, · · · , n,

1
γl

xl
θl

1 +

(cid:16)
(cid:17)
n
j=1

xj
θj

1
γj

(cid:16)

(cid:17)

(cid:19)

+

α + n
l γ2
θ2

l (cid:19)

(cid:18)

(cid:18)

2
γl

xl
θl

P

2 ,

(cid:17)

(cid:16)
n
j=1

xj
θj

1
γj

(cid:16)

(cid:17)

(cid:19)

1 +

(cid:18)

P

12

l = 1, · · · , n,

(7)

(1)

(4)

(5)

(6)

∂2ln fn(x)
∂γl

2

=

1
γ2
l

+

2
γ3 ln

xl
θl (cid:19)

(cid:18)

− 2

α + n
γ3
l (cid:19)

(cid:18)

1

γl ln

xl
θl

xl
θl

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

1
(cid:17)
γj

(cid:18)

P

(cid:16)

(cid:17)

(cid:19)

−

α + n
γ4
l (cid:19)

(cid:18)

+

α + n
γ4
l (cid:19)

(cid:18)

1

γl ln2

xl
θl

xl
θl

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

1
(cid:17)
γj

(cid:18)







(cid:18)

P
xl
θl

1

(cid:16)
γl ln

(cid:17)
xl
θl

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

1
(cid:17)
γj

P
xl
θl

(cid:16)
1
γl

(cid:17)
xk
θk

(cid:19)

2







(cid:19)

1
γk

∂2ln fn(x)
∂θk∂θl

=

α + n
γkγkθlθk (cid:19)

(cid:18)

∂2ln fn(x)
∂γk∂γl

=

α + n
l γ2
γ2

k (cid:19)

(cid:18)

xl
θl

(cid:16)

(cid:17)

2 ,

k 6= l,

,

(cid:17)

k 6= l,

(cid:16)
1 +

(cid:16)

(cid:17)
n
j=1

(cid:17)
xj
θj

1
γj

(cid:18)
1
γl

1

P
γk ln

xk
θk

(cid:16)

(cid:17)

(cid:19)

xl
θl

ln

xk
θk

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

(cid:17)
1
γj

(cid:16)
2

P
1
γl

xk
θk

1

(cid:16)
(cid:17)
γk ln

(cid:19)
xk
θk

∂2ln fn(x)
∂γk∂θl

=

α + n
γlθlγ2

(cid:18)

k (cid:19)

(cid:16)

(cid:17)
1 +

(cid:16)

(cid:17)
n
j=1

2 ,
(cid:17)

(cid:16)
1
γj

k 6= l,

(9)

(10)

(11)

,

l = 1, · · · , n,

(8)

∂2ln fn(x)
∂γk∂θl

=

1
θlγ2
l

−

α + n
θlγ2

(cid:18)

l (cid:19)

1 +

(cid:18)
xl
θl

(cid:18)

(cid:16)

(cid:17)

(cid:19)

P

xj
θj

1
γl

xl
θl

(cid:17)
(cid:16)
n
j=1

xj
θj

1
γj

(cid:16)

(cid:17)

(cid:19)

(cid:18)
xl
θl

1

P
γl ln

xl
θl

(cid:17)

(cid:16)
1 +

n
j=1

(cid:16)
xj
θj

1
(cid:17)
γj

(cid:18)







P

(cid:17)

1
γl

(cid:16)
xl
θl

(cid:17)
(cid:16)
n
j=1

xj
θj

(cid:19)

1
γj

(cid:16)

(cid:17)

(cid:19)

2







1 +

(cid:18)

P

13

−

α + n
θlγ3

(cid:18)

l (cid:19)

+

α + n
θlγ3

(cid:18)

l (cid:19)

ln

,

xl
θl (cid:19)

(cid:18)

k 6= l.

(12)

B Expression of the main integral

This Appendix gives one of the main results of this paper which is the derivation of the

expression of the following integral

n

E

"

ri
γi

Xi
θi (cid:19)

=

#

+∞

+∞

n

· · ·

ri
γi

xi
θi (cid:19)

0
Z
where, fn(x) is the multivariate Pareto (IV) density function (3). This derivation is done in

i=1 (cid:18)
Y

i=1 (cid:18)
Y

0
Z

fn(x) dx,

(1)

the following steps:

First consider the following one dimensional integral:

C1 =

+∞

r1
γ1

α
θ1γ1 (cid:18)
+∞
α
θ1γ1 (cid:18)

x1
θ1 (cid:19)
x1
θ1 (cid:19)

(cid:18)
r1
γ1

1
γ1

−1

x1
θ1 (cid:19)
x1
θ1 (cid:19)
−(α+n)

1
γ1

(cid:18)

 

−1

n

1 +

1
γj

xj
θj (cid:19)
xj
θj (cid:19)

!

1
γj

!

j=1 (cid:18)
X
n

1 +

 

j=2 (cid:18)
X

−(α+n)

dx1

−(α+n)

0
Z

=






0
Z

1 +

1
γ1

x1
θ1

1 +

(cid:16)

(cid:17)
n
j=2

xj
θj

P

(cid:16)

(cid:17)

1
γj






dx1.

Note that, goings from ﬁrst line to second line is just a factorizing and rewriting the last term

of the integral. After many reﬂections on the links between Pareto (IV) and Burr families

and Gamma and Beta functions, we found that the following change of variable

1
γ1

x1
θ1

1 +






1 +

(cid:16)

(cid:17)
n
j=2

xj
θj

P

(cid:16)

(cid:17)

1
γj






=

1
1 − t

,

0 < t < 1,

(2)

simpliﬁes this integral and guides us to the following result

C1 =

αΓ(r1 + 1)Γ(α + n − r1 − 1)
Γ(α + n)

1 +

 

n

j=2 (cid:18)
X

1
γj

xj
θj (cid:19)

!

−(α+n)+r1+1

.

(3)

Then we consider the following similar expression:

C2 =

+∞

0

Z

α(α + 1)
θ2γ2

Γ(r1 + 1)Γ(α + n − r1 − 1)
Γ(α + n)

r2
γ2

x2
θ2 (cid:19)

(cid:18)

14

(cid:18)

=

x2
θ2 (cid:19)
+∞

0
Z

n

1 +

 

−(α+n)+r1+1

1
γ2

−1

n

1 +

 

1
γj

xj
θj (cid:19)

!

j=2 (cid:18)
X
Γ(r1 + 1)Γ(α + n − r1 − 1)
Γ(α + n)

α(α + 1)
θ2γ2

dx2

−(α+n)+r1+1

1
γj

xj
θj (cid:19)

!

j=3 (cid:18)
X

r2
γ2

1
γ2

−1

x2
θ2 (cid:19)
−(α+n)+r1+1

(cid:18)

x2
θ2 (cid:19)

1
γ2

(cid:18)

x2
θ2

dx2,

1 +

(cid:16)

(cid:17)
n
j=3

xj
θj

P

(cid:16)

(cid:17)

1
γj






1 +






1
γ2

x2
θ2

1 +






1 +

(cid:16)

(cid:17)
n
j=3

xj
θj

P

(cid:16)

(cid:17)

1
γj






=

1
1 − t

,

and again using the following change of variable:

we obtain:

C2 =

α(α + 1)Γ(r1 + 1)Γ(r2 + 1)Γ(α + n − r1 − r2 − 2)
Γ(α + n)
−(α+n)+r1+r2+2

n

1
γj

xj
θj (cid:19)

!

1 +

 

j=3 (cid:18)
X

.

Continuing this method, ﬁnally, we obtain the general expression:

(4)

(5)

Cn = E

n

"

i=1 (cid:18)
Y

ri
γi

Xi
θi (cid:19)

=

#

Γ(α −

n
i=1 ri)

n
i=1 Γ(ri + 1)

n

P

Γ(α)
Q

i=1
X

,

ri < α,

ri > −1.

(6)

We may note that to simplify the lecture of the paper we did not give all the details of these

calculations.

15

References

[1] M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions. National Bureau

of Standards, Applied Mathematics Series (1972), no. 55.

[2] B. C. Arnold, Pareto distributions, International Cooperative Publishing House, Fair-

land, Maryland, 1983.

[3] V. Brazauskas, Fisher information matrix for the Feller-Pareto distribution, Statist.

Probab. Lett. 59 (2002), no. 2, 159–167.

[4]

, Information matrix for Pareto (IV), Burr, and related distributions, Comm.

Statist. Theory and Methods 32 (2003), no. 2, 315–325.

[5] V. Brazauskas and R. Serﬂing, Robust and eﬃcient estimation of the tail index of a

single-parameter Pareto distribution, North American Actuarial Journal 4 (2000), no. 4,

12–27.

[6]

, Robust estimation of tail parameters for two-parameter Pareto and exponential

models via generalized quantile statistics, Extremes 3 (2001), no. (3), 231–249.

[7] C. D. Daykin, T. Pentik¨ainen, and M. Pesonen, Practical risk theory for actuaries,

Chapman and Hall, London, 1994.

[8] C. P. Gomes, B. Selman, and N. Crato, Heavy-tailed distributions in combinatorial

search, In Principles and Practice of Constraint Programming CP-97 and Smolka, G.

Ed. Lecture Notes in Computer Science 1330 (1997), 121–135.

[9] F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel, Robust statistics:

The approch based on inﬂuence functions., Wiley, New York, 1986.

[10] P. J. Huber, Robust statistics, Wiley, New York, 1981.

[11] N. L. Johnson, S. Kotz, and N. Balakrishnan, Continuous univariate distributions, 2nd

edition, vol. 1, Wiley, New York, 1994.

16

[12] A. C. Kimber, Comparision of some robust estimators of scale in gamma samples with

known shape., Journal of Statistical Computation and Simulation 18 (1983), 273–286.

[13]

, Trimming in gamma samples, Applied Statistics 32 (1983), no. 1, 7–14.

[14] S. A. Klugman, H. H. Panjer, and G. E. Willmot, Loss models: From data to decisions,

Wiley, New York, 1998.

[15] E. L. Lehmann, Theory of point estimation, Wiley, New York, 1983.

[16] R. J. Serﬂing, Approximation theorems of mathematical statistics, Wiley, New York,

1980.

17


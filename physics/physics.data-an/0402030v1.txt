4
0
0
2
 
b
e
F
 
5
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
3
0
2
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

PhysicsGP: A Genetic Programming Approach
to Event Selection

Kyle Cranmer a R. Sean Bowman b

aCERN, CH-1211 Geveva, Switzerland
bOpen Software Services, LLC, Little Rock, Arkansas, USA

Abstract

We present a novel multivariate classiﬁcation technique based on Genetic Program-
ming. The technique is distinct from Genetic Algorithms and oﬀers several advan-
tages compared to Neural Networks and Support Vector Machines. The technique
optimizes a set of human-readable classiﬁers with respect to some user-deﬁned per-
formance measure. We calculate the Vapnik-Chervonenkis dimension of this class
of learning machines and consider a practical example: the search for the Stan-
dard Model Higgs Boson at the LHC. The resulting classiﬁer is very fast to eval-
uate, human-readable, and easily portable. The software may be downloaded at:
http://cern.ch/

cranmer/PhysicsGP.html

∼

Key words: Genetic Programming, Triggering, Classiﬁcation, VC Dimension,
Genetic Algorithms, Neural Networks, Support Vector Machines

1 Introduction

The use of multivariate algorithms
in the search for particles in High
Energy Physics has become quite
common. Traditionally, a search can
be viewed from a classiﬁcation point
of view: from a tuple of physical mea-
surements (i.e., momenta, energy,
etc.) we wish to classify an event as
signal or background. Typically, this
classiﬁcation is realized through a
Boolean expression or cut designed
by hand. The high dimensionality of
the data makes this problem diﬃ-
cult in general and favors more so-

phisticated multivariate algorithms
such as Neural Networks, Fisher Dis-
criminants, Kernel Estimation Tech-
niques, or Support Vector Machines.
This paper focuses on a Genetic Pro-
gramming approach and considers a
speciﬁc example: the search for the
Higgs Boson at the LHC.

The use of Genetic Programming for
classiﬁcation is fairly limited; how-
ever, it can be traced to the early
works on the subject by Koza [1].
More recently, Kishore et al. ex-
tended Koza’s work to the multicat-
egory problem [2]. To the best of the
authors’ knowledge, the work pre-

Preprint submitted to Elsevier Science

2 January 2014

sented in this paper is the ﬁrst use of
Genetic Programming within High
Energy Physics.

In Section 2 we provide a brief his-
tory of evolutionary computation
and distinguish between Genetic Al-
gorithms (GAs) and Genetic Pro-
gramming (GP). We describe our al-
gorithm in detail for an abstract per-
formance measure in Section 3 and
discuss several speciﬁc performance
measures in Section 4.

Close attention is paid to the perfor-
mance measure in order to leverage
recent work applying the various re-
sults of statistical learning theory in
the context of new particle searches.
This recent work consists of two com-
ponents. In the ﬁrst, the Neyman-
Pearson theory is translated into the
Risk formalism [3, 4]. The second
component requires calculating the
Vapnik-Chervonenkis dimension for
the learning machine of interest. In
Section 5, we calculate the Vapnik-
Chervonenkis dimension for our Ge-
netic Programming approach.

Because evolution is an operation on
a population, GP has some statisti-
cal considerations not found in other
multivariate algorithms. In Section 6
we consider the main statistical is-
sues and present some guiding princi-
ples for population size based on the
user-deﬁned performance measure.

Finally, in Section 7 we examine the
application of our algorithm to the
search for the Higgs Boson at the
LHC.

2 Evolutionary Computation

In Genetic Programming (GP), a
group of “individuals” evolve and
compete against each other with re-
spect to some performance measure.
The individuals represent potential
solutions to the problem at hand,
and evolution is the mechanism by
which the algorithm optimizes the
population. The performance mea-
sure is a mapping that assigns a
ﬁtness value to each individual. GP
can be thought of as a Monte Carlo
sampling of a very high dimensional
search space, where the sampling is
related to the ﬁtness evaluated in the
previous generation. The sampling
is not ergodic – each generation is
related to the previous generations
– and intrinsically takes advantage
of stochastic perturbations to avoid
local extrema 1 .

Genetic Programming is similar to,
but distinct
from Genetic Algo-
rithms (GAs), though both methods
are based on a similar evolutionary
metaphor. GAs evolve a bit string
which typically encodes parameters
to a pre-existing program, function,
or class of cuts, while GP directly
evolves the programs or functions.
For example, Field and Kanev [5]
used Genetic Algorithms to optimize
the lower- and upper-bounds for six
1-dimensional cuts on Modiﬁed Fox-
Wolfram “shape” variables. In that
case, the phase-space region was a
pre-deﬁned 6-cube and the GA was

1 These are the properties that give
power to Markov Chain Monte Carlo
techniques.

2

simply evolving the parameters for
the upper and lower bounds. On the
other hand, our algorithm is not con-
strained to a pre-deﬁned shape or
parametric form. Instead, our GP
approach is concerned directly with
the construction and optimization of
a nontrivial phase space region with
respect to some user-deﬁned perfor-
mance measure.

In this framework, particular at-
tention is given to the performance
measure. The primary interest in the
search for a new particle is hypoth-
esis testing, and the most relevant
measures of performance are the ex-
pected statistical signiﬁcance (usu-
ally reported in Gaussian sigmas) or
limit setting potential. The diﬀerent
performance measures will be dis-
cussed in Section 4, but consider a
concrete example: s/√b, where s and
b are the number of signal and back-
ground events satisfying the event
selection, respectively.

3 The Genetic Programming

Approach

While the literature is replete with
uses of Genetic Programming and
Genetic Algorithms, direct evolu-
tion of cuts appears to be novel.
In the case at hand, the individu-
als are composed of simple arith-
metic expressions, f , on the input
variables ~v. Without loss of gen-
erality, the cuts are always of the
1 < f (~v) < 1. By scal-
form
−
af (~v), and translation,
ing, f (~v)
f (~v)
f (~v)+b, of these expressions,
single- and double-sided cuts can be

→

→

3

Signal

 Background

y
t
i
s
n
e
D
 
y
t
i
l
i
b
a
b
o
r
P

0

2

4

6

8

10

−1

0

1

2

3

4

−1

0

1

2

3

4

−8

−7

−6

−5

−4

−3

−2

−1

0

1

2

Evaluated Expression

Fig. 1. Signal and Background his-
tograms for an expression.

produced. An individual may consist
of one or more such cuts combined by
the Boolean conjunction AND. Fig. 1
shows the signal and background
distributions of four expressions that
make up the most ﬁt individual in a
development trial.

several

structural

Due to computational considera-
tions,
changes
have been made to the na¨ıve imple-
mentation. First, an Island Model
of parallelization has been imple-
mented (see Section 3.5). Secondly,
individuals’ ﬁtness can be evaluated
on a randomly chosen sub-sample of
the training data, thus reducing the
computational requirements at the
cost of statistical variability. There
are several statistical considerations
which are discussed in Section 6.

3.1 Individual Structure, Mutation,

and Crossover

The genotype of an individual is a
collection of expression trees similar

(a)

(b)

(c)

(d)

Fig. 2. An example of crossover. At
some given generation, two parents (a)
and (b) are chosen for a crossover mu-
tation. Two subtrees, shown in bold,
are selected at random from the par-
ents and are swapped to produce two
children (c) and (d) in the subsequent
generation.

|

|

4.2v1 + v2/1.5

to abstract syntax trees that might
be generated by a compiler as an in-
termediate representation of a com-
puter program. An example of such a
tree is shown in Fig. 2a which corre-
< 1.
sponds to a cut
Leafs are either constants or one of
the input variables. Nodes are simple
arithmetic operators: addition, sub-
traction, multiplication, and safe di-
vision 2 .When an individual is pre-
sented with an event, each expression
tree is evaluated to produce a num-
ber. If all these numbers lie within the
1, 1), the event is considered
range (
signal. Otherwise the event is classi-
ﬁed as background.

−

Initial trees are built using the PTC1
algorithm described in [6]. After each
generation, the trees are modiﬁed by

2 Safe division is used to avoid division
by zero.

4

mutation and crossover. Mutation
comes in two ﬂavors. In the ﬁrst, a
randomly chosen expression in an in-
dividual is scaled or translated by a
random amount. In the second kind
of mutation, a randomly chosen sub-
tree of a randomly chosen expression
is replaced with a randomly gener-
ated expression tree using the same
algorithm that is used to build the
initial trees.

While mutation plays an important
rˆole in maintaining genetic diversity
in the population, most new individ-
uals in a particular generation result
from crossover. The crossover opera-
tion takes two individuals, selects a
random subtree from a random ex-
pression from each, and exchanges
the two. This process is illustrated in
Fig. 2.

3.2 Recentering

Some expression trees, having been
generated randomly, may prove to
be useless since the range of their
expressions over the domain of their
inputs lies well outside the interval
1, 1) for every input event. When
(
−
an individual classiﬁes all events
in the same way (signal or back-
ground), each of its expressions is
translated to the origin for some ran-
domly chosen event exemplar ~v0, viz.
f (~v)
f ( ~v0). This modiﬁ-
cation is similar to, and thus reduces
the need for, normalizing input vari-
ables.

f (~v)

→

−

3.3 Fitness Evaluation

e
v
i
t
a
l
u
m
u
C

Fitness evaluation consumes the ma-
jority of time in the execution of the
algorithm. So, for speed, the ﬁtness
evaluation is done in C. Each individ-
ual is capable of expressing itself as a
fragment of C code. These fragments
are pieced together by the Python
program, written to a ﬁle, and com-
piled. After linking with the train-
ing vectors, the program is run and
the results communicated back to the
Python program using standard out-
put.

The component that serializes the
population to C and reads the results
back from the generated C program
is conﬁgurable, so that a user-deﬁned
performance measure may be imple-
mented.

3.4 Evolution & Selection Pressure

After a given generation of individ-
uals has been constructed and the
individuals’ ﬁtnesses evaluated, a
new generation must be constructed.
Some individuals survive into the
new generation, and some new indi-
viduals are created by mutation or
crossover. In both cases, the popu-
lation must be sampled randomly.
To mimic evolution, some selection
pressure must be placed on the indi-
viduals for them to improve. This se-
lection pressure is implemented with
a simple Monte Carlo algorithm and
controlled by a parameter α > 1.
The procedure is illustrated in Fig. 3.
In a standard Monte Carlo algo-

1

x1/α

1

0

Performance

(Uniform Variate)

0

Fig. 3. Monte Carlo sampling of indi-
viduals based on their ﬁtness. A uni-
form variate x is transformed by a sim-
ple power to produce selection pressure:
a bias toward individuals with higher
ﬁtness.

∈

rithm, a uniform variate x
[0, 1]
is generated and transformed into
the variable of interest by the in-
verse of its cumulative distribution.
Using the cumulative distribution of
the ﬁtness will exactly reproduce the
population without selection pres-
sure; however, this sampling can be
biased with a simple transformation.
The right plot of Fig. 3 shows a uni-
form variate x being transformed
into x1/α, which is then inverted (left
plot) to select an individual with a
given ﬁtness. As the parameter α
grows, the individuals with high ﬁt-
ness are selected increasingly often.

While the selection pressure mech-
anism helps the system evolve,
it
comes at the expense of genetic di-
versity. If the selection pressure is
too high, the population will quickly
converge on the most ﬁt individual.
The lack of genetic diversity slows
evolutionary progress. This behavior
can be identiﬁed easily by looking at
plots such as Fig. 4. We have found
that a moderate selection pressure
α

[1, 3] has the best results.

∈

5

3.5 Parallelization and the Island

Model

e
c
n
a
c
i
f
i
n
g
i
S

GP is highly concurrent, since diﬀer-
ent individuals’ ﬁtness evaluations
are unrelated to each other, and di-
viding the total population into a
number of sub-populations is a sim-
ple way to parallelize a GP problem.
Even though this is a trivial modi-
ﬁcation to the program, it has been
shown that such coarse grained par-
allelization can yield greater-than-
linear speedup [7]. Our system uses
a number of Islands connected to a
Monitor in a star topology. CORBA
is used to allow the Islands, which are
distributed over multiple processors,
to communicate with the Monitor.

Islands use the Monitor to exchange
particularly ﬁt individuals each gen-
eration. Since a separate monitor
process exists, a synchronous ex-
change of individuals is not neces-
sary. The islands are virtually con-
nected to each other (via the Moni-
tor) in a ring topology.

4 Performance Measures

The Genetic Programming approach
outlined in the previous section is a
very general algorithm for producing
individuals with high ﬁtness, and it
allows one to factorize the deﬁnition
of ﬁtness from the algorithm. In this
section we examine the function(al)
which assigns each individual its ﬁt-
ness: the performance measure.

Before proceeding, it is worthwhile

6

Generation

Fig. 4. The ﬁtness of the population as a
function of time. This plot is analogous
to a neural network error vs. epoch plot,
with the notable exception that it de-
scribes a population and not an individ-
ual. In particular, the neural network
graph is a 1-dimensional curve, but this
is a two dimensional distribution.

|

P

i |

f (~vi)

yi −

to compare GP to popular multivari-
ate algorithms such as Support Vec-
tor Machines and Neural Networks.
Support Vector Machines typically
try to minimize the risk of misclas-
, where yi is
siﬁcation
the target output (usually 0 or -1
for background and 1 for signal) and
f (~vi) is the classiﬁcation of the ith in-
put. This is slightly diﬀerent than the
error function that most Neural Net-
works with backpropagation attempt
2 [8, 9]. In
yi −
to minimize:
both cases, this performance measure
is usually hard-coded into a highly
optimized algorithm and cannot be
easily replaced. Furthermore, these
two choices are not always the most
appropriate for High Energy Physics,
as will be discussed in Section 4.1.

f (~vi)

i |

P

|

common performance
The most
measure for a particle search is the
Gaussian signiﬁcance, s/√b, which

measures the statistical signiﬁcance
(in “sigmas”) of the presence of a
new signal. The performance mea-
sure s/√b is calculated by determin-
ing how many signal events, s, and
background events, b, a given indi-
vidual will select in a given amount
of data (usually measured in fb−1).

The s/√b is actually an approxi-
mation of the Poisson signiﬁcance,
σP , the probability that an expected
background rate b will ﬂuctuate to
s + b. The key diﬀerence between the
two is that as s, b
0, the Poisson
signiﬁcance will always approach 0,
but the Gaussian signiﬁcance may
diverge. Hence, the Gaussian signiﬁ-
cance may lead to highly ﬁt individ-
uals that accept almost no signal or
background events.

→

The next level of sophistication in
signiﬁcance calculation is to include
systematic error in the background
only prediction b. These calcula-
tions tend to be more diﬃcult and
the ﬁeld has not adopted a stan-
dard [10, 11, 12]. It is also quite com-
mon to improve the statistical signif-
icance of an analysis by including a
discriminating variable [13].

In contrast, one may be more inter-
ested in excluding some proposed
particle. In that case, one may wish
to optimize the exclusion potential.
The exclusion potential and discov-
ery potential of a search are related,
and G. Punzi has suggested a perfor-
mance measure which takes this into
account quite naturally [14].

Ideally, one would use as a perfor-
mance measure the same procedure

that will be used to quote the results
of the experiment. For instance, there
is no reason (other than speed) that
one could not include discriminating
variables and systematic error in the
optimization procedure (in fact, the
authors have done both).

4.1 Direct vs. Indirect Methods

Certain approaches to multivariate
analysis leverage the many powerful
theorems of statistics, assuming one
can explicitly refer to the joint prob-
ability density of the input variables
and target values p(~v, y). This de-
pendence places a great deal of stress
on the asymptotic ability to estimate
p(~v, y) from a ﬁnite set of samples
(~v, y)i}
. There are many such tech-
{
niques for estimating a multivariate
density function p(~v, y) given the
samples [15, 16]. Unfortunately, for
high dimensional domains, the num-
ber of samples needed to enjoy the
asymptotic properties grows very
rapidly; this is known as the curse of
dimensionality.

Formally, the statistical goal of a
new particle search is to minimize
the rate of Type II error. This is
logically distinct from, but asymp-
totically equivalent to, approximat-
ing the likelihood ratio. In the case
of no interference between the sig-
nal and background, this is logically
from, but asymptotically
distinct
equivalent
to, approximating the
signal-to-background ratio. In fact,
most multivariate algorithms are
concerned with approximating an
auxiliary function that is one-to-one

7

with the likelihood ratio. Because the
methods are not directly concerned
with minimizing the rate of Type
II error, they should be considered
indirect methods. Furthermore, the
asymptotic equivalence breaks down
in most applications, and the indi-
rect methods are no longer optimal.
Neural Networks, Kernel Estimation
techniques, and Support Vector Ma-
chines all represent indirect solutions
to the search for new particles. The
Genetic Programming approach is a
direct method concerned with opti-
mizing a user-deﬁned performance
measure.

5 Statistical Learning Theory

In 1979, Vapnik provided a re-
markable family of bounds relating
the performance of a learning ma-
chine and its generalization capac-
ity [17]. The capacity, or Vapnik-
Chervonenkis dimension (VCD) is
a property of a set of functions, or
, where
learning machines,
α is a set of parameters for the learn-
ing machine [18].

f (~v; α)

}

{

}

In the two-class pattern recogni-
tion case considered in this paper,
an event x is classiﬁed by a learn-
ing machine such that f (~v; α)

∈
signal, background
. Given a set of
{
l events each represented by ~vi, there
are 2l possible permutations of them
belonging to the class signal or back-
ground. If for each permutation there
f (~v; α)
exists a member of the set
}
which correctly classiﬁes each event,
then we say the set of points is shat-
tered by that set of functions. The

{

8

Fig. 5. The VCD for a line in R2 is 3.

}

{

f (~v; α)

f (~v; α)
VCD for a set of functions
}
is deﬁned as the maximum number
of points which can be shattered by
. If the VCD is h, it does
{
not mean that every set of h points
can be shattered, but that there ex-
ists some set of h points which can
be shattered. For example, a hyper-
plane in Rn can shatter n + 1 points
(see Fig. 5 for n = 2).

In the modern theory of machine
learning, the performance of a learn-
ing machine is usually cast in the
more pessimistic setting of risk. In
general, the risk, R, of a learning
machine is written as

R(α) =

Q(~v, y; α) p(~v, y) d~vdy

Z

(1)
where Q measures some notion of
loss between f (~v; α) and the tar-
get value y. For example, when
classifying events, the risk of mis-
classiﬁcation is given by Eq. 1 with
Q(~v, y; α) =
. Simi-
larly, for regression tasks one takes
f (~v; α))2. Most of
Q(~v, y; α) = (y
the classic applications of learning
machines can be cast into this for-
malism; however, searches for new
particles place some strain on the
notion of risk [3, 4].

f (~v; α)

−

−

y

|

|

The starting point for statistical
learning theory is to accept that we

{

might not know p(~v, y) in any an-
alytic or numerical
form. This is,
indeed, the case for particle physics,
(~v, y)i}
can be ob-
because only
tained from the Monte Carlo con-
volution of a well-known theoretical
prediction and complex numerical
description of the detector. In this
case, the learning problem is based
entirely on the training samples
with l elements. The risk
(~v, y)i}
{
functional
is thus replaced by the
empirical risk functional

Remp(α) =

Q(~vi, yi; α).

(2)

1
l

l

Xi=1

There is a surprising result that the
true risk R(α) can be bounded inde-
pendent of the distribution p(~v, y). In
particular, for 0

Q(~v, y; α)

1

≤

≤

R(α)

Remp(α)

+

≤

 

v
u
u
t

h(log(2l/h) + 1)

log(η/4)

−

l

(3)

,

!

→

where h is the VC dimension and η is
the probability that the bound is vi-
olated. As η
0, h
0
the bound becomes trivial. The sec-
ond term of the right hand side is of-
ten referred to as the VC conﬁdence
– for h = 200, l = 105, and η = 95%
the VC conﬁdence is about 12%.

→ ∞

, or l

→

While the existence of the bounds
found in Eq. 3 are impressive, they
are frequently irrelevant. In partic-
ular, for Support Vector Machines
with radial basis functions for ker-
nels the VCD is formally inﬁnite
and there is no bound on the true
risk. Similarly, for Support Vector

9

Machines with polynomial kernels
of degree p and data embedded in d
dimensions, the VCD is
+ 1
which grows very quickly.
(cid:16)

p+d−1
p

(cid:17)

This motivates a calculation of the
VCD of the GP approach.

5.1 VCD for Genetic Programming

The VC dimension, h,
is a prop-
erty of a fully speciﬁed learning
machine. It is meaningless to cal-
culate the VCD for GP in general;
however, it is sensible if we pick a
particular genotype. For the slightly
simpliﬁed genotype which only uses
the binary operations of addition,
subtraction, and multiplication, all
expressions are polynomials on the
input variables. It has been shown
that for learning machines which
form a vector space over their pa-
rameters, 3 the VCD is given by the
dimensionality of the span of their
parameters [19]. Because the Genetic
Programming approach mentioned is
actually a conjunction of many such
cuts, one also must use the theorem
that the VCD for Boolean conjunc-
tions, b, among learning machines
is given by VCD(b(f1, . . . , fk))
≤
ck maxi VCD(fi), where ck is a con-
stant [19].

If we placed no bound on the size of
the program, arbitrarily large poly-
nomials could be formed and the

3 A learning machine,
space if for any two functions f, g

, is a vector

∈
and real numbers a, b the function
. Polynomials satisfy these

F

F
af + bg
conditions.

∈ F

f (x, y; α) = a1
+ a4 ·
+ a7 ·

x
x

x
x

·
·

y

·

x
+a2 ·
x
+a5 ·
x
+a8 ·

y
·
y
·

y

·

+ a3 ·
+ a6 ·
+ a9 ·

y
y
x

y
x

·
·

y

y

·

·

Fig. 6. An explicit example of the largest polynomial on two variables with degree
two. In total, 53 nodes are necessary for this expression which has only 9 independent
parameters.

VCD would be inﬁnite. However, by
placing a bound on either the size
of the program or the degree of the
polynomial, we can calculate a sen-
sible VCD. The remaining step nec-
essary to calculate the VCD of the
polynomial Genetic Programming
approach is a combinatorial prob-
lem: for programs of length L, what
is the maximum number of linearly
independent polynomial coeﬃcients?
Fig. 6 illustrates that the smallest
program with nine linearly inde-
pendent coeﬃcients requires eight
additions, eighteen multiplications,
eighteen variable leafs, and nine con-
stant leafs for a total of 53 nodes. A
small Python script was written to
generalize this calculation.

The Genetic Programming approach
with polynomial expressions has a
relatively small VCDs (in our tests
with seven variables nothing more
than h = 100 was found) which
aﬀords the relevance of the upper-
bound proposed by Vapnik.

5.2 VCD of Neural Networks

aided by algebraic techniques. Ed-
uardo Sontag has an excellent re-
view of these techniques and shows
that the VCD of neural networks
can, thus far, only be bounded fairly
weakly [19]. In particular, if we de-
ﬁne ρ as the number of weights and
biases in the network, then the best
bounds are ρ2 < h < ρ4. In a typical
particle physics neural network one
can expect 100 < ρ < 1000, which
translates into a VCD as high as 1012,
which implies l > 1013 for reasonable
bounds on the risk. These bounds
imply enormous numbers of training
samples when compared to a typical
training sample of 105. Sontag goes
on to show that these shattered sets
are incredibly special and that the
set of all shattered sets of cardinality
greater than µ = 2ρ + 1 is mea-
sure zero in general. Thus, perhaps a
more relevant notion of the VCD of
a neural network is given by µ.

6 Statistical Fluctuations
the Fitness Evaluation

in

In order to apply Eq. 3, one must de-
termine the VC dimension of Neural
Networks. This is a diﬃcult prob-
lem in combinatorics and geometry

In this section we examine the trade-
oﬀ between the time necessary to
evaluate the ﬁtness of an individ-
ual and the accuracy of the ﬁtness
when evaluated on a ﬁnite sample of

10

events. Holding computing resources
ﬁxed, the two limiting cases are:

1 With very large sample sizes, one
can expect excellent estimation
of the ﬁtness of individuals and
a clear “winner” at the expense
of very little mutation and poorly
optimized individuals.

2 With very small sample sizes, one
can expect many mutations lead-
ing to individuals with very high
ﬁtness which do not perform as re-
ported on larger samples.

Illustrated in Fig. 7 is the distribu-
tion of ﬁtness for a given “winning”
individual generated with a large en-
semble of training samples each with
400 events. The ﬁtness reported in
the last generation of the training
phase (indicated with an arrow) is
much higher than the mean of this
distribution. In fact, the probability
to have randomly chosen a sample
of 400 events which would produce
such a high empirical signiﬁcance is
about 0.1%.

While the chance that an arbitrary
individual’s ﬁtness evaluates several
standard deviations from the mean is
quite small, with thousands, maybe
millions, of individual programs the
chance that one will ﬂuctuate signif-
icantly can be quite large. Further-
more, the winning individual has a
much higher chance of a signiﬁcant
upward ﬂuctuation, because individ-
uals with upward ﬂuctuations have a
higher chance of being the winner.

y
t
i
s
n
e
D
 
y
t
i
l
i
b
a
b
o
r
P

−1

10

−2

10

−3

10

n
o
i
t
a
r
e
n
e
g
 
t
s
a
l
 

m
o
r
f
 
e
c
n
a
m
r
o
f
r
e
P

Performance

Fig. 7. The distribution of ﬁtness eval-
uated on a single individual with an
ensemble of testing samples (each with
400 events). The dashed vertical arrow
indicates the ﬁtness evaluated with a
speciﬁc testing sample in the last gener-
3σ) up-
ation of training. The large (
ward ﬂuctuation in the evaluated ﬁtness
largely enhanced the chance this indi-
vidual would be chosen as the winner.

≈

oped a few guiding principles for
reliable use of the algorithm.

•

•

For training, the standard devia-
tion of the ﬁtness distribution eval-
uated on N events should be on the
order of a noticeable and marginal
improvement in the ﬁtness based
on the users performance measure.
Select the winning individual with
a large testing sample.

If we take as our measure of perfor-
mance s/√b, then it is possible to cal-
culate the variance due to the ﬂuctu-
ations in s and b. The expected error
is given by the standard propagation
of errors formula. In particular,

Having recognized that statistical
ﬂuctuations in the ﬁtness evaluation
complicate matters, we have devel-

s
√b !

∆

 

2

=

∆s2
b

+

∆b2s2
4b3

(4)

11

−

−

50% and ǫb ≈

where s = Lσsǫs and b = Lσbǫb
via standard rate calculations and
∆s2 = ǫs(1
ǫs)L2σ2
s /Ns and sim-
ǫb)L2σ2
ilarly ∆b2 = ǫb(1
b /Nb via
binomial statistics. For the analysis
presented in Section 7, the selec-
tion eﬃciency for signal and back-
ground are ǫs ≈
5%,
respectively. The predicted rate of
signal and background events are
Lσs ≈
560, respec-
tively. Using these values, one can
expect a 10% (5%) relative error on
the ﬁtness evaluation with a sample
of Ns = Nb = 100 (400) events. Anal-
ogous calculations can be made for
any performance measure (though
they may require numerical studies)
to determine a reasonable sample
size. The rule of thumb that relative
errors scale as 1/√N is probably
reasonable in most cases.

100, and Lσb ≈

7 Case Study: The Search for
the Higgs Boson at the LHC

Finally we consider a practical ex-
ample: the search for the Higgs bo-
son at the LHC. While there are
many channels available, the recent
Vector Boson Fusion analyses oﬀer
a suﬃciently complicated ﬁnal state
to warrant the use of multivariate
algorithms.

The Higgs at the LHC is produced
predominantly via gluon-gluon fu-
sion. For Higgs masses such that
MH > 100 GeV the second domi-
nant process is Vector Boson Fu-
sion. The lowest order Feynman dia-
gram of the production of Higgs via
VBF is depicted in Fig 8. The decay

W

W

q

H

q

W +

W −

ν

¯ν

l+

l−

Fig. 8. Tree-level diagram of Vector
Boson Fusion Higgs production with
l+l−νν
H

W +W −

→

→

W +W −

channel chosen is H
→
e±µ∓νν, e+e−νν, µ+µ−νν. These
channels will also be referred to as
eµ, ee, and µµ, respectively.

→

These analyses were performed at
the parton level and indicated that
this process could be the most pow-
erful discovery mode at the LHC in
the range of the Higgs mass, MH ,
115 < MH < 200 GeV [20]. These
analyses were studied speciﬁcally
in the ATLAS environment using a
fast simulation of the detector [21].
Two traditional cut analyses, one
for a broad mass range and one op-
timized for a low-mass Higgs, were
developed and documented in refer-
ences [22] and [23]. We present re-
sults from previous studies without
systematic errors on the dominant t¯t
background included.

In order to demonstrate the potential
for multivariate algorithms, a Neural
Network analysis was performed [24].
The approach in the Neural Network
study was to present a multivariate
analysis comparable to the cut anal-
ysis presented in [22]. Thus, the anal-
ysis was restricted to kinematic vari-
ables which were used or can be de-
rived from the variables used in the
cut analysis.

12

The variables used were:

•

•

•

•

•

•

•

•

•

•

∆ηll - the pseudorapidity diﬀer-
ence between the two leptons,
∆φll - the azimuthal angle diﬀer-
ence between the two leptons,
Mll - the invariant mass of the two
leptons,
∆ηjj - the pseudorapidity diﬀer-
ence between the two tagging jets,
∆φjj - the azimuthal angle diﬀer-
ence between the two tagging jets,
Mjj - the invariant mass of the two
tagging jets, and
MT - the transverse mass.

In total, three multivariate analyses
were performed:

a Neural Network analysis using
backpropagation with momentum,
a Support Vector Regression anal-
ysis using Radial Basis Functions,
and
a Genetic Programming analysis
using the software described in
this Communication.

The Neural Network (NN) analysis
is well documented in reference [24].
The analysis were performed with
both the Stutgart Neural Network
Simulator (SNNS) 4 and MLPﬁt 5
with a 7-10-10-1 architecture.

For the Support Vector Regression
(SVR) analysis, the BSVM-2.0 6
li-
brary was used.

4 SNNS can be found here:
www-ra.informatik.uni-tuebingen.de
5 MLPﬁt can be found here:
cern.ch/~schwind/MLPfit.html
6 BSVM can be found here:
www.csie.ntu.edu/~cjlin/bsvm

13

The only parameters are the cost
parameter, set to C = 1000, and
the kernel function. BSVM does not
support weighted events, so an “un-
weighted” signal and background
sample was used for training.

Because the trained machine only de-
pends on a small subset of “Support
Vectors”, performance is fairly sta-
ble after only a thousand or so train-
ing samples. In this case, 2000 signal
and 2000 background training events
were used.

Both NN and SVR methods produce
a function which characterizes the
signal-likeness of an event. A sep-
arate procedure is used to ﬁnd the
optimal cut on this function which
optimizes the performance measure
(in this case the Poisson signﬁcance,
σP ). Fig. 9 shows the distribution of
the SVR (left) and NN (right) out-
put values. The optimal cut for the
SVR technique is shown as a vertical
arrow.

Tab. 1 compares the Poisson signiﬁ-
cance, σP , for a set of reference cuts,
a set of cuts speciﬁcally optimized
for low-mass Higgs, Neural Networks,
Genetic Programming, and Support
Vector Regression. It is very pleas-
ing to see that the multivariate tech-
niques achieve similar results. Each of
the methods has its own set of advan-
tages and disadvantages, but taken
together the methods are quite com-
plementary.

y
t
i
s
n
e
D
 
y
t
i
l
i
b
a
b
o
r
P

 

y
t
i
s
n
e
D
y
t
i
l
i
b
a
b
o
r
P

Signal

Background

−2

−1

0

1

2

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

SVR output

Neural Network output

Fig. 9. Support Vector Regression and Neural Network output distributions for signal
and background for 130 GeV Higgs boson in the eµ channel.

Ref. Cuts

low-mH Opt. Cuts NN GP SVR

120 ee

120 eµ

120 µµ

Combined

130 eµ

0.87

2.30

1.16

2.97

4.94

1.25

2.97

1.71

3.91

6.14

1.72

1.66

1.44

3.92

3.60

3.33

2.28

2.26

2.08

4.98

4.57

4.26

7.55

7.22

6.59

Table 1
Expected signiﬁcance for two cut analyses and three multivariate analyses for dif-
ferent Higgs masses and ﬁnal state topologies. Signiﬁcance is expressed in terms of
Gaussian Sigmas, but calculated with Poisson statistics.

8 Conclusions

recentering algorithm to dramati-
cally improve performance. We have
emphasized the importance of the
performance measure and decoupled
ﬁtness evaluation from the optimiza-
tion component of the algorithm.
We have touched on the relationship
of Statistical Learning Theory and
VC dimension to the search for new
particles and multivariate analysis
in general. Finally, we have demon-
strated that this method has simi-
lar performance to Neural Networks
(the de facto multivariate analysis
of High Energy Physics) and Sup-
port Vector Regression. We believe

We have presented an implemen-
tation of a Genetic Programming
system speciﬁcally applied to the
search for new particles. In our ap-
proach a group of individuals com-
petes with respect to a user-deﬁned
performance measure. The genotype
we have chosen consists of Boolean
conjunctions of simple arithmetic
expressions of the input variables re-
1, 1).
quired to lie in the interval (
Our implementation includes an is-
land model of parallelization and a

−

14

that this technique’s most relevant
advantages are

•

•

•

•

the ability to provide a user-
performance measure
deﬁned
speciﬁcally suited to the problem
at hand,
the speed with which the resulting
individual / cut can be evaluated,
the fundamentally important abil-
ity to inspect the resulting cut, and
the relatively low VC dimension
which implies the method needs
only a relatively small training
sample.

9 Acknowledgments

This work was supported by a grad-
uate research fellowship from the
National Science Foundation and US
Department of Energy Grant DE-
FG0295-ER40896.

References

[1] J.R. Koza. Genetic Program-
ming: On the Programming of
Computers by Means of Natu-
ral Selection. MIT Press, Cam-
bridge, MA, 1992.
[2] J.K. Kishore et. al.

Appli-
cation of genetic programming
for multicategory pattern clas-
IEEE Transactions
siﬁcation.
on Evolutionary Computation, 4
no.3, 2000.

[3] K. Cranmer. Multivariate anal-
ysis and the search for new par-
ticles. Acta Physica Polonica B,
34:6049–6069, 2003.

[4] K. Cranmer. Multivariate anal-
ysis from a statistical point of
In PhyStat2003, 2003.
view.
physics/0310110.

[5] R. D. Field and Y. A. Kanev.
Using collider event topology in
the search for the six-jet decay of
top quark antiquark pairs. hep-
ph/9801318, 1997.

[6] S. Luke.

Two fast

tree-
creation algorithms for genetic
IEEE Transac-
programming.
tions on Evolutionary Computa-
tion, 2000.

[7] D. Andre and J.R. Koza. Par-
allel genetic programming on a
network of transputers. In Jus-
tinian P. Rosca, editor, Proceed-
ings of the Workshop on Genetic
Programming: From Theory to
Real-World Applications, pages
111–120, Tahoe City, California,
USA, 9 1995.
[8] P.J. Werbos.

The Roots of
Backpropagation. John Wiley &
Sons., New York, 1974.

[9] D.E. Rumelhart et. al. Par-
allel Distributed Processing Ex-
plorations in the Microstructure
of Cognition. The MIT Press,
Cambridge, 1986.

[10] R.D. Cousins and V.L. High-
land.
Incorporating system-
atic uncertainties into an up-
per limit. Nucl. Instrum. Meth.,
A320:331–335, 1992.

[11] K. Cranmer. Frequentist hy-
testing with back-
pothesis
In PhyS-
ground uncertainty.
tat2003, 2003. physics/0310108.
[12] J. T. Linnemann. Measures of
signiﬁcance in HEP and astro-
physics. In PhyStat2003, 2003.
physics/0312059.

[13] Search for the standard model

15

[23] S. Asai et. al. Prospects for
the search of a standard model
Higgs boson in ATLAS using
vector boson fusion. to appear
in EPJ. ATLAS Scientiﬁc Note
ATL-PHYS-2003-005 (2002).

[24] K. Cranmer et. al.
based
network
bosons
Higgs

Neu-
search
ral
decay
for
l+l−/pT
H
for 115 < MH < 130 GeV. AT-
LAS note ATL-PHYS-2003-007
(2002).

W +W −

→

→

Higgs boson at LEP. Phys. Lett.,
B565:61–75, 2003.

[14] G. Punzi. Sensitivity of searches
for new signals and its opti-
mization. In PhyStat2003, 2003.
physics/0308063.

[15] D. Scott. Multivariate Den-
sity Estimation: Theory, Prac-
tice, and Visualization.
John
Wiley and Sons Inc., 1992.
[16] K. Cranmer. Kernel estimation
in high-energy physics. Com-
put. Phys. Commun., 136:198–
207, 2001.

[17] V. Vapnik. Estimation of depen-
dences based on empirical data.
Nauka, 1979. in Russian.
[18] V. Vapnik and A.J. Chervo-
nenkis. The uniform conver-
gence of frequencies of the ap-
pearance of events to their prob-
Dokl. Akad. Nauk
abilities.
SSSR, 1968. in Russian.

[19] E. Sontag. VC dimension of neu-
In C.M. Bishop,
ral networks.
editor, Neural Networks and Ma-
chine Learning, pages 69–95,
Berlin, 1998. Springer-Verlag.

[20] D. Rainwater and D. Zep-

Observing H

penfeld.
→
e±µ±/pT in weak
W (⋆)W (⋆)
boson fusion with dual forward
jet tagging at the CERN LHC.
D60:113004, 1999.

→

[21] D. Froidevaux E. Richter-Was
and L. Poggioli. Atlfast2.0 a
fast simulation package for at-
las. ATLAS Internal Note ATL-
PHYS-98-131.
[22] K. Cranmer et. al.

Search

for Higgs bosons decay H
→
l+l−/pT for 115 <
W +W −
MH < 130 GeV using vector bo-
son fusion. ATLAS note ATL-
PHYS-2003-002 (2002).

→

16


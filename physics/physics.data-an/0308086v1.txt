3
0
0
2
 
g
u
A
 
3
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
6
8
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Remarks on statistical aspects of safety
analysis of complex systems

L. P´al∗and M. Makai†
KFKI Atomic Energy Research Institute H-1525 Budapest 114, POB 49 Hungary

August 23, 2002

Abstract

We analyze safety problems of complex systems using the methods of
mathematical statistics for testing the output variables of a code simulating
the operation of the system under consideration when the input variables
are uncertain. We have deﬁned a black box model of the code and derived
formulas to calculate the number of runs needed for a given conﬁdence level
to achieve a preassigned measure of safety. In order to show the capabilities of
diﬀerent statistical methods, ﬁrstly we have investigated one output variable
with unknown and known distribution functions. The general conclusion
has been that the diﬀerent methods do not bring about large diﬀerences in
the number of runs needed to ensure a given level of safety. Analyzing the
case of several statistically dependent output variables we have arrived at
the conclusion that the testing of the variables separately may lead to false,
safety related decisions with unforseen consequences. We have advised two
methods: the sign test and the tolerance interval methods for testing more
then one mutually dependent output variables.

List of key words: safety analysis, black box model, best estimate,
Bayesian method, quantile test, conﬁdence interval, sign test, tolerance in-
terval.

∗e-mail: lpal@rmki.kfki.hu
†e-mail: makai@sunserv.kfki.hu

1

1 Introduction

There are two approaches to safety analysis of large complex systems. Since
the analysis has to demonstrate safety of the operation under the investigated
circumstances, we may scrutinize a not too realistic but rather unfavorable
situation saying that if that situation is safe then any real situation must be
on the safe side. This approach we call conservatism.

An alternative approach may attempt to investigate the real situation
and show that no limit violation can occur. In this case the calculated values
should be increased by the possible error when compared with the safety
limit [1]. That approach is called best estimate which is not a very fortunate
but generally accepted name.

In conservative analysis, the ﬁrst problem is in the selection of the case
to be studied. It identiﬁes an overt attempt to bound the actual expected
state hence it should estimate also the consequences of model uncertainties.
How do we know if a given situation is more conservative than is the other?
It is often impossible to foresee the outcome of a non-linear process. Another
problem may be the interplay between approximations. It may happen that
either of two approximations leads to conservatism but their simultaneous
presence does not. The conservative approach has been prevalent for a long
time, although today rather the best estimate methods are in the focus.

The main diﬃculty with best estimate calculation is in the complexity of
the phenomena involved. (A new material phase may appear, at a given tem-
perature chemical reactions may take place producing new material proper-
ties, and also producing or removing heat, the process dynamics is nonlinear
etc.) In spite of the problem’s complexity, a best estimate method attempts
to solve the equations describing the involved physical processes as accurately
as our knowledge permits. From licensing viewpoint, several key parameters
should be selected and compared to the acceptance criteria.

Best estimate methods are accompanied by an uncertainty analysis to
learn the uncertainty band of the response [2]. The purpose of the uncertainly
evaluation is to provide assurance that the selected parameters at least with
probability 95% or more will be in the acceptance region or will not exceed
their acceptance level.

The present work is dealing with the code uncertainty only, which is rather
important constituent of the total uncertainty. We assume the modeled pro-
cedure to start from a known initial state. All the physical quantities in the
model we sort as input, output, and latent data. By deﬁnition, a datum is

2

input if its domain is known along with a distribution function associating
a probability with any admitted value. In a model, there are several con-
stants, which are considered either as input or latent data. Input, when the
given constant is looked upon as a variable in a given range and a probabil-
ity is allotted to every possible value. Distinction between input and latent
data is a matter of engineering judgement. The nature of the distribution
may depend on the determination of the constant. Latent, when we refrain
from analyzing the uncertainties of the constant, temporarily we take it as a
ﬁxed number. A datum not falling into the input or latent category is called
output.

The paper is organized as follows. In Section 2 we deﬁne a simple black
box model linking the output variables to input variables, while in Section 3
we analyze possibilities and limitations of several well-known statistical tests
for one output variable with unknown and known cumulative distribution
function. Special attention is paid to the application of a slightly new variant
of the tolerance interval method. In Section 4 we deal with the case of several
not independent output variables by using the advantages of order statistics,
and, ﬁnally the conclusions are summarized in Sections 5 and 6.

The present work focuses on deriving criteria for safe operation when the
output variables are ﬂuctuating as a result of randomness of input variables,
and intends to give some help in practical applications. In the sequel we fol-
low the notation used in the classical handbook of statistics by M.G. Kendall
and A. Stuart [3].

2 Black box model

Let us consider a system as complex as a nuclear power plant, or an oil
reﬁnery plant for instance. Assume we have a model describing that system,
and that model enables us to calculate physical parameters characterizing
the system at arbitrary instant t. Let n be the number of technologically
important variables. In the frame of the model, the operation of the system
is considered safe if all calculated variables belong to a given set of intervals

determined by the technology.

L(j)
T , U (j)

T

,

j = 1, . . . , n

VT =

nh

o

In order not to be set back by the complexity of the problem, we sug-
gest a simple black box model, in which output variables are linked to input

i

3

Y

variables. That link can be a computer code that transforms vector ~x
∈ X
the input variables, into a vector ~y(t)
X
∈ Y
are sets of all possible values of ~x and ~y(t), respectively. In general,
and
the dimension of ~x, i.e. the number of input variables is not the same as
the dimension of ~y(t), i.e. the number of output variables. Every data that
enters into the model is treated as an input variable, hence we do not dis-
tinguish parameters. The model is an explicit relationship between input ~x
and output ~y:

, the output variables. Here

,

(1)

where ˆ
C

(t) is a nonlinear operator that maps

~y(t)

ˆ
(t)~x,
C

⇐

x1
x2
...
xh








~x = 





into

y1(t)
y2(t)
...
yn(t)



.






~y(t) = 





In practical cases the link between input and output is very complex hence
there is no reason to anticipate an analytical relationship like ~y(t) = ~f(~x, t).
In the sequel ˆ
(t) is assumed to be deterministic, in other words once the
C
input has been ﬁxed, we obtain the same output within the computation
accuracy for each run. At the same time, if the input vector ﬂuctuates ac-
cording to distribution laws simulating possible variations of the technology,
or, reﬂecting uncertainty of some parameters of the model then the output
parameters also ﬂuctuate in repeated runs.

We present an illustration of how random input may inﬂuence an output
variable, see Fig. 1. We used the thermohydraulic code ATHLET [4] to
generate several output variables for a simple experimental setup, but in
Fig. 1 we presented only one output variable as function of time for three
independent runs. It is obvious that in this case the above given criterion
for safe operation of the system needs to be changed because there is no
guarantee that a new run after a successful run will also be successful.

We call a state ~x0 nominal, if all the input parameters take their respective
expectation value, i.e. ~x0 = E
. We can perform a calculation in the
nominal state to get the corresponding output ~y0 = ˆ
(t)~x0. Usually the
C
state ~x0 is called safe if ~y0 is in the safety envelope
VT . However, we need a
more stringent deﬁnition: state ~x is called safe if ~y is in the safety envelope
VT for every ~x

~x
}
{

∈ X

.

4

900

800

700

600

500

400

s
t
i
n
u

.

b
r
a

n

i

t
u
p
t
u
o

1
2
3

5

0

200

400
time in arb. units

600

800

Figure 1: Inﬂuence of the random input on the time dependence of one output variable
in three independent runs.

X

Here we should make three remarks. (i)

may be an inﬁnite interval
when at least one of the input variables is of normal distribution. In practi-
cal calculations such variables are conﬁned to a ﬁnite interval by engineering
judgement. (ii) We check that statement by a given, ﬁnite number of calcula-
tions [5] with input from
VT
the state ~x is unsafe independently of the fact that the nominal state ~x0 may
be safe. (iii) Even if every calculated output is safe, there is a probability
that the state is actually unsafe.

. If there is a value outside the safety envelop

X

Fixing time t after N runs we obtain N randomly varying output vectors
~y1(t), ~y2(t), . . . , ~yN (t)
which carries information on the ﬂuctuating input
{
In the next Section we are considering only one
and the code properties.
output variable with continuous cumulative distribution function G(y) =
y
−∞ g(u) du, and the time t is taken as ﬁxed and its notation is omitted.

}

R
3 One output variable

3.1 Old Bayesian method

If we carry out N runs with ﬂuctuating input, then we obtain a sample
of the random variable y at a ﬁxed time point. Through
SN =

y1, y2, . . . , yN }
{

technological considerations, we deﬁne a ﬁx acceptance and a ﬁx rejection
interval to variable y. Let the acceptance interval be
Ha = [LT , UT ], and
Hr = [LT , UT ] = (
The probability

) the rejection interval. 1

(UT , +

, LT )

−∞

∞

∪

y

P{

∈ Ha}

=

g(u) du = w

ZHa
that an observed y lays in the acceptance interval is not known. Knowing
however that k elements of the sample
Ha,
SN are in the acceptance interval
then utilizing Bayes’ theorem, without knowing g(u), we can claim that

β(ω

N, k) =

|

=

1

ω uk (1
1
0 uk (1
R
R

−

−

k

u)N −k du
u)N −k du

=

N + 1
j

(1

−

(cid:19)

j=0 (cid:18)
X

ω)j ωN +1−j = β(ω

N, k)

(2)

|

is the probability that the unknown acceptance probability w is greater than
a prescribed ω. The proof of the mentioned theorem is available in textbooks2
hence we omit it here. We wish to point out the expression

β(ω

N, 0) = 1

ωN +1,

(3)

−

−

|
which shows convincingly that even when the whole sample
elements to be accepted, we can state only that w
1
we have

ωN +1. If one element in the sample

SN consists of
ω with probability
SN is in the rejection interval, then
ω)ωN.
(N + 1) (1
(4)

−
Using (2), one can easily determine the allowed number of rejections in a
sample of N elements so that the unknown probability of the acceptance w to
be larger than the prescribed limit ω with a given probability β(ω
α.
N, k)
It can be agreed on that a system is safe if it is almost certain (0 << α
1)
that the unknown probability of the acceptance w is larger than a prescribed
ω.

N, 1) = 1

ωN +1

≥
≤

β(ω

−

−

≥

|

|

1In many practically important cases LT =

(UT , +

−∞
2P´al. L.: Fundamentals of probability Theory and Statistics, vol.

−∞

∞

).

, and so

Ha = (

, UT ] and

Hr =
I.-II., 109-113,

Budapest, Akad´emiai Kiad´o, Budapest (1995), in Hungarian.

6

Table I. Number of observations N at which w

for several values of α, ω, and the number of rejected values N

≥

ω with probability β(ω
k.

N, k)
|

≥

α

−

−

α

0.90

0.95

0.99

ω
0.90
0.95
0.99
0.90
0.95
0.99
0.90
0.95
0.99

N

−

k = 0 N
21
44
228
27
57
297
43
89
457

−

k = 1 N
31
75
387
45
92
472
63
129
660

k = 2
51
104
530
60
123
626
80
164
836

For example, we read out from Table I that if all the 297 observed values
were acceptable, i.e. there was not a single value to be rejected, then, larger
0.99, i.e. the proportion of rejected
than 95% is the probability that w
≥
observations in any sample will be not larger than 0.01. The more observa-
tions we have, with the higher probability we can state that the investigated
system is safe, and the higher is the lower level ω for the unknown acceptance
probability w.

3.2 Distribution free conﬁdence interval for quantile

Assume again the cumulative distribution function G(y) of the output vari-
able y to be unknown but continuous and strictly increasing. Denote by Qγ
the γ-quantile of G(y), i.e the value satisfying the equation

G(Qγ) =

dG(y) = γ.

Qγ

−∞

Z

Qγ = G−1(γ).

Clearly, the interval (
G(y). Since G(y) is continuous and strictly increasing 3 one can write

, Qγ] covers the proportion γ of the distribution

−∞

It is to mention that the point estimate of Qγ is that element of the ordered
sample the index k of which is the nearest integer to Nγ.

3If G(y) is a continuous and not decreasing function, then Qγ = inf

y : G(y)

{

γ

.

}

≥

7

3.2.1 Two-tailed test

Carrying out N independent runs, we get a sample
. Ar-
range the sample elements in increasing order, 4 and denote by y(k) the kth
of ordered elements; hence we have

y1, . . . , yN }

SN =

{

y(1) < y(2) <

< y(r) <

< y(s) <

< y(N),

and by deﬁnition y(0) =
density function of random variables

−∞

· · ·

· · ·
, while y(N + 1) = +

· · ·

∞

. As known the joint

where r and s > r are positive integers from

1, 2, . . . , N

is given by

z(r) = G[y(r)]

and

z(s) = G[y(s)],

gr,s(u, v) =

{

}
v)N −s
s + 1)

,

ur−1 (v
B(r, s

u)s−r−1 (1
r) B(s, N

−
−

−
−

0

u

v

1.

≤
Here B(j, k) is the Euler beta function.

≤

≤

Theorem 1 If r and s positive integers satisfying the inequality 0 < r <
(N + 1)γ < s
N, then the random interval [y(r), y(s)] covers the unknown
≤
γ-quantile Qγ with probability

β =

y(r)

P{

Qγ ≤

≤

y(s)

=

}

= I(1

γ, N

s + 1, s)

I(1

γ, N

r + 1, r)

−

−

−

−

−

(5)

where

I(c, j, k) =

B(c, j, k)
B(j, k)

is the regularized incomplete beta function for non-singular cases.

The proof of the theorem is simple and it can be ﬁnd in the Appendix I.
One can see that the conﬁdence level β for the the random interval [y(r), y(s)]
does not depend on G(y), in other words, the conﬁdence interval for the
unknown Qγ is distribution free.

Clearly, there are many diﬀerent conﬁdence intervals covering Qγ with a
prescribed probability β. We have to chose the shortest interval by using the
following procedure:

4The probability that equal values occur is zero.

8

•

•

from the ordered sample determine the integer q = [(N + 1)γ] due to
the point estimate ˜Qγ = y(q) of the γ-quantile Qγ,

calculate the conﬁdence level β step by step for intervals deﬁned by
integer pairs [rj, sk] where rj = q
1 and sk =
j = 1, 2, . . . q
q + k, k = 1, 2, . . . , N
k, respectively, until the prescribed value of
β is reached provided that it is possible at the sample size N that we
have,

−

−

−

j,

•

if the prescribed β could not be reached, then the sample size should
have been increased.

When the conﬁdence interval [y(rj), y(sk)] covering the γ-quantile of the
unknown distribution G(y) at prescribed conﬁdence level β is a part of the
interval [LT , UT ] deﬁned by technology, then the system can be qualiﬁed safe
at level (β

γ).

|

Table II. Conﬁdence levels β for conﬁdence intervals covering the unknown quantile
Q0.9 in the case of sample size N = 100. (The point estimate of Q0.9 is equal to ˜Q0.9 =
y(90)).

r
s
\
89
88
87
86
85
84
83
82
81
80

95
0.6455
0.7442
0.8185
0.8699
0.9025
0.9218
0.9324
0.9378
0.9404
0.9416

96
0.6793
0.7781
0.8524
0.9037
0.9364
0.9557
0.9663
0.9717
0.9743
0.9755

97
0.6952
0.7940
0.8683
0.9196
0.9523
0.9716
0.9822
0.9876
0.9902
0.9914

98
0.7011
0.7999
0.8742
0.9255
0.9582
0.9775
0.9880
0.9935
0.9961
0.9972

99
0.7027
0.8015
0.8758
0.9271
0.9598
0.9791
0.9897
0.9951
0.9977
0.9989

100
0.7030
0.8018
0.8761
0.9274
0.9601
0.9794
0.9900
0.9954
0.9980
0.9992

In Table II we see that, for example, the conﬁdence interval [y(85), y(97)]
deﬁned by elements y(85) and y(97) of the ordered sample of size N = 100
covers the quantile Q0.9 of the unknown distribution of the output variable
y with probability (on conﬁdence level) β = 0.9523. In other words, having
N = 100 observations for the output variable y we can state with probability
β = 0.9523 that y[85] < Q0.9 < y[97], i.e. the upper limit of the interval
, Q0.9] containing 90% of the unknown distribution G(y) is covered by
(

−∞

9

|

[y(85), y(97)] on conﬁdence level β = 0.9523.
then the system is safe, but only on the level (0.9523

If [y(85), y(97)]
0.9).

⊆

[LT , UT ],

When we need stronger criteria of safety, then we have to ﬁnd conﬁdence
intervals covering quantiles Q0.95 or Q0.99 with probability near the unity. As
seen in Tables III and IV the sample size N should be greatly increased. For
example, if we would like to construct a conﬁdence interval for the quantile
Q0.99 at the level of β = 0.9467 we need sample with N
700 elements. The
production of such a large sample for even one output variable of complex
systems is very expensive, and at the same time, there is no guarantee that
the relation [y(r), y(s)]
[LT , UT ] will be always satisﬁed, especially when
the distribution is asymmetric.

⊆

≈

Table III. Conﬁdence levels β for conﬁdence intervals covering the Q0.95 unknown
quantile in the case of sample size N = 150. (The point estimate of Q0.95 is equal to
˜Q0.95 = y(143)).

r
s
\
142
141
140
139
138
137
136

144
0.2909
0.4080
0.4949
0.5531
0.5886
0.6084
0.6186

1456
0.4293
0.5464
0.6333
0.6916
0.7270
0.7469
0.7571

146
0.5382
0.6553
0.7422
0.8004
0.8359
0.8557
0.8659

147
0.6090
0.7261
0.8130
0.8712
0.9067
0.9265
0.9368

148
0.6456
0.7627
0.8496
0.9078
0.9433
0.9632
0.9734

149
0.6597
0.7768
0.8637
0.9219
0.9574
0.9773
0.9875

150
0.6633
0.7804
0.8673
0.9255
0.9610
0.9809
0.9911

Table IV. Conﬁdence levels β for conﬁdence intervals covering the Q0.99 unknown
quantile in the case of sample size N = 700. (The point estimate of Q0.99 is equal to
˜Q0.99 = y(694)).

r
s
\
692
691
690
689
688
687
686

694
0.2808
0.3826
0.4536
0.4986
0.5247
0.5387
0.5456

695
0.4303
0.5321
0.6031
0.6481
0.6742
0.6882
0.6951

696
0.5581
0.6599
0.7309
0.7759
0.8020
0.8160
0.8229

697
0.6490
0.7508
0.8218
0.8668
0.8929
0.9069
0.9138

698
0.7007
0.8024
0.8735
0.9185
0.9446
0.9585
0.9655

699
0.7226
0.8244
0.8954
0.9405
0.9666
0.9805
0.9874

700
0.7289
0.8306
0.9017
0.9467
0.9728
0.9867
0.9936

10

3.2.2 One-tailed test

In order to declare that a system is operating safely on a given level, in many
practical cases it seems to be enough to know that the value of a properly
selected output variable y with probability near 1 is smaller than the value UT
prescribed by technology. In this case we should determine that element y(s)
of the ordered sample which, with probability β, is larger than the quantile
Qγ of the unknown distribution G(y) of the output variable y. It means that
, y(s)] covers the proportion larger than γ of the
the random interval (
unknown distribution G(y) of output variable y with probability

−∞

β =

y(s) > Qγ}

.

P{

s−1

−∞

N
j

j=0 (cid:18)
X

(cid:19)

In order to determine this probability we should substitute r = 0 into Eq.
(5), since according to our deﬁnition y(0) =

. We obtain that

β = I(1

γ, N

s + 1, s) =

−

−

γj (1

γ)N −j,

−

(6)

where I(c, j, k) is the regularized incomplete beta function for non-singular
cases. 5 If y(s) is smaller than UT , then we can state: the system is safe at
the level (β

γ).

If s = N, i.e. if the largest element of the sample is chosen as upper limit

|

of the random interval, then one obtains the well-known formula:

5This equation can be easily derived directly. It is obvious that

β = 1

γN .

−

(7)

β =

y(s) > Qγ}

P{

=

y(s) > G−1(γ)
}

P{

=

P{

G[y(s)] > γ

,

}

and since the probability density function of the random variable z(s) = G[y(s)] is nothing
else than

so we can write immediately that

gs(u) =

us−1 (1
B(s, N

u)N −s
s + 1)

,

−
−

β =

B(s, N

s + 1)

−

−

−

us−1 (1

u)N −s du = I(1

γ, N

s + 1, s) =

1

−

1

γ
Z

and this nothing else than (6).

= 1

I(γ, s, N

s + 1),

−

−

11

Since in the engineering practice one can ﬁnd misinterpretations it is not
superﬂuous to underline the just proven notion of this formula: β is the
probability that the largest value y(N) of a sample consisting of N obser-
vations is greater than the γ quantile of the unknown distribution of the
output variable y. This statement can be formulated also as follows: β is the
, y(N)] covers the proportion larger than γ
probability that the interval (
of the unknown distribution G(y) of the output variable y.

−∞

If s = N

1, i.e. if the (N

1)-th element of the ordered sample is chosen

−
as upper limit, then we get from (6) the following formula:

−

β = 1

γN

−

−

N(1

γ) γN −1,

−

(8)

the notion of which is obvious. Clearly, when β and γ are ﬁxed, and the
second largest element of the sample is chosen for upper limit, then the
γ) is obviously greater than if the
sample size N needed to reach the level (β
largest element would have been chosen. For example, let the certainty level
(0.95
0.95), then if the largest element is chosen, the sample size should be
N0 = 58, 6 while if the second largest one is applied, the sample size has to
be N1 = 93. However, it is at all not certain that y(93)(92)
y(58)(58). (The
superscript denotes the sample size.)

≤

|

|

y
t
i
l
i

b
a
b
o
r
p

1

0.9

0.8

0.7

0.6

N=100

s=100
s=99
s=98

0.92

0.94

0.96

0.98

percentile level

Figure 2: Dependence of the probability β on γ = G (Qp) at three values of s.

6The root of Eq. 0.95N

0.05 = 0 is N

58.404, and we are using the rounded value

N = 58. In engineering practice the value N = 59 is accepted.

−

≈

12

Figure 2 shows the dependence of the probability β on γ when N = 100
and s = 100, 99 and 98. One can see the sharp decrease of β when the
quantile-level γ approaches the unity.

Table V. Sample sizes N0, N1, . . . , N6 for ﬁnding elements y(s),

s = N0, N1 −
6 to be larger than quantiles Q0.90, Q0.95 and Q0.99 of the unknown distribution
1, . . . , N6 −
of the output variable y with prescribed probabilities β = 0.90, 0.95 and 0.99, respectively.

γ

β

\

0.90

0.95

0.99

0.90
22
37
52
65
78
91
103
45
76
105
132
158
183
206
229
388
531
666
797
925
1051

0.95
28
46
61
75
89
102
115
58
93
124
153
180
207
234
298
473
627
773
913
1049
1181

s
0.99
N0 −
44
N1 −
64
N2 −
81
N3 −
97
112 N4 −
127 N5 −
141 N6 −
N0 −
90
130 N1 −
165 N2 −
197 N3 −
228 N4 −
258 N5 −
287 N6 −
458 N0 −
661 N1 −
837 N2 −
1001 N3 −
1157 N4 −
1307 N5 −
1453 N6 −

0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
1
2
3
4
5
6

By ﬁxing the values β and γ we may calculate sample sizes N0, N1, . . . , Nk
k such
which are needed for ﬁnding elements y(s), s = N0, N1 −
to be larger than the γ-quantile of the unknown distribution of the output
variable y with prescribed probability β. We can see in Table V that for
example the largest element in a sample of size N = 58 with probability
β = 0.95 is greater than the quantile Q0.95 of the unknown distribution. If
N = 234, then this statement is true for the element y(227).

1, . . . , Nk −

3.2.3 Illustrations

In order to get a deeper insight into the properties of the just outlined
method, we choose the lognormal distribution with parameters m and d

13

as the ”unknown” distribution G(y). We note that this distribution arises
when many independent random variables are combined in a multiplicative
fashion. The density function

g(y) =

1
√2π dy

exp

1
2

(−

log y
−
d

(cid:18)

2

m

)

(cid:19)

,

y

0

≥

0.12

0.1

0.08

0.06

0.04

0.02

n
o
i
t
c
n
u
f

y
t
i
s
n
e
d

0

0

Q0.95

16.8

27.7

d=0.5

m=2.0
m=2.5

10

20
30
y variable

40

50

Figure 3: Lognormal density function with parameter values m = 2.0, 2.5 and d = 0.5.
The vertical arrows indicate the quantile Q0.95.

27.7 (m = 2.5).

16.8 (m = 2) and Q0.95 ≈

can be seen in Fig. 3 when m = 2.0, 2.5 and d = 0.5. The arrows show the
quantiles Q0.95 ≈
By using Monte Carlo simulation let us generate now four sample of size
N = 100 corresponding to lognormal distribution with parameters (m =
2.5, d = 0.5), and denote by A, B, C and D these samples. Calculate the
point estimates of 0.95-quantiles for each of the samples, and determine the
shortest two-tailed conﬁdence intervals which cover with probability 0.95 the
”unknown” quantile Q0.95. In the present case we know that Q0.95 ≈
16.8
(m = 2) and Q0.95 ≈
In Fig. 4 the conﬁdence intervals are shown by vertical straight lines.
Obviously, these intervals are random variables, hence ﬂuctuate from sample
to sample. In the presented example the sample D is the most unfavorable,
because in this case we can state only that the ”unknown” quantile Q0.95 is
covered by the interval [23.29, 53.05] with probability larger than β = 0.95.

27.7 (m = 2.5).

14

Table VI. Conﬁdence intervals [y(r), y(s)] covering the ”unknown” quantile Q0.95

with probability 0.95.

A
22.66
27.73
33.25
(91, 100)

B
25.21
27.73
38.28
(91, 100)

C
22.48
27.73
35.88
(91, 100)

D
23.29
27.73
53.05
(91, 100)

y(r)
Q0.95
y(s)
(r, s)

g

g

0.06
0.04
0.02

0.06
0.04
0.02

10 20 30 40 50 60

10 20 30 40 50 60

g

g

0.06
0.04
0.02

0.06
0.04
0.02

A

C

y

y

B

D

y

y

10 20 30 40 50 60

10 20 30 40 50 60

Figure 4: Two-sided conﬁdence intervals denoted by vertical straight lines for samples
A, B, C and D. The intervals are calculated to be covered the true value of the quantile
Q0.95 with probability larger than β = 0.95. The density function is lognormal with
parameters m = 2.5 and d = 0.5. The vertical dashed lines are indicating the true value
of the quantile Q0.95.

If the upper limit UT determined by technology would be UT = 40, then only
three (A, B, C) of four samples could be regarded safe at the level (0.95
0.95),
however, sample D, which is certainly a ”rare event”, would decrease the
weight of our statement.

|

As mentioned,

y(s), s = N, N

in many cases it is enough to know only the element
k of the ordered sample of size N for which the

1, . . . , N

−

−

15

equation

}

=

P{

= β

−∞

Qγ < y(s)

y(s) > Qγ}
P{−∞ ≤
, y(s)] is called one tailed test.
is valid. The test based on the interval (
First, determine the sample size N at which the largest element of the sam-
ple y(N) with probability β is greater than the quantile Qγ of the unknown
distribution G(y) of the output variable y. If β = 0.95 and γ = 0.95, then
the largest element has to be chosen out of a sample containing N = 58
elements. Produce a sample of size N = 58 simulating the lognormal distri-
bution with parameters m = 2.5, d = 0.5, and call it basic sample, denoted
by y(b). Then, repeat randomly the sample production n-times, and denote
by y(1), y(2), . . . , y(n) the series of samples. We are interested in the largest
elements y(j)(58), j = 1, . . . , n of samples y(j), j = 1, . . . , n.

140

120

100

80

60

40

20

t

n
e
m
e
e

l

.
x
a
m

200

400

600

800

1000

number of sample

Figure 5: Largest elements of 1000 samples of size N = 58. The horizontal line corre-
sponds to the largest element of the basic sample of size N = 58. This element is equal to
y(b)(58)

44.99.

≈

Fig. 5 shows the largest elements of n = 1000 randomly produced, inde-
pendent samples of size N = 58. The minimal value of the largest elements
is 22.62, while the maximal value is 132.27. One can observe that 224 largest
elements exceed the value y(b)(58)
44.99 which is the largest element of

≈

16

the basic sample. However, this surprisingly great number is in full agree-
ment with the statement that the interval [0, y(b)(58)] covers the ”unknown”
0.95-quantile with probability at least 0.95.

In order to show this, let us introduce the random variable ξn(Qγ) which
gives the number of largest elements being greater than the quantile Qγ in
y(j), j = 1, . . . , n independent samples of size N. Since the probability that
the largest element in a given sample is greater than Qγ is nothing else than
1

γN , hence, we conclude that

−

ξn(Qγ) = k

=

}

P{

n
k

(cid:18)

(cid:19)

(1

−

γN )k γN (n−k).

From this we obtain immediately that

E

ξn(Qγ)

= n(1

{

}

γN )

−

and D

ξn(Qγ)

=

n γN (1

γN ).

{

}

−

As known, if n and k are suﬃciently large, then the distribution of the random
variable

p

χn(Qγ) =

ξn(Qγ)
D

ξn(Qγ)

E
{
−
ξn(Qγ)
{

}

}

is approximately standard normal, hence we can write that

w =

χn(Qγ)

λ

=

P {|

| ≤

}
E

=

E

ξn(Qγ)

λ D

ξn(Qγ)

P {

{
where λ is the root of Eq.

} −

{

} ≤

ξn(Qγ)

ξn(Qγ)

+ λ D

ξn(Qγ)

≤

{

}

{

,

}}

1
√2π

λ

−∞

Z

e−u2/2 du =

1 + w
2

.

It means that the inequality

E

ξn(Qγ)

λ D

ξn(Qγ)

ξn(Qγ)

E

ξn(Qγ)

+ λ D

ξn(Qγ)

{

} −

{

} ≤

≤

{

}

{

}

is valid with probability w.

If n = 1000, N = 58, γ = 0.95 and w = 0.95, then we obtain the values
1.96, hence we can state
ξn(Qγ)

6.96 and λ

ξn(Qγ)

} ≈

≈

E
with probability 0.95 that

= 950, D

{

}

{

936 < ξ1000(Q0.95) < 964.

17

If we count the number of largest elements y(j)(58), j = 1, . . . , 1000 exceed-
27, 728), we obtain the value
ing Q0.95 that we know in this example (Q0.95 ≈
949 that is indeed inside of the interval [936, 964].

In spite of this ”nice” agreement we have to underline that the require-
ment of safety, for instance, at the level (0.95
0.95) does not exclude the
appearance of ”rare events” such as exceeding the technological limit UT .
Therefore, we advice stronger requirements of safety the fulﬁllment of which,
of course, is much more expensive.

|

3.3 Method based on sign test

Assume again the cumulative distribution function G(y) of the output vari-
able y to be continuous but unknown. Let
be a sample
containing the values of N observations. Deﬁne the function

y1, . . . , yN }

SN =

{

∆(x) =

1,

if x > 0,

0,

if x < 0,




N

j=1
X

and introduce the statistical function



zN =

∆(UT −

yj).

(9)

which gives the number of sample elements smaller than UT . Criteria based
on this statistical function are used to be named sign criteria because zN
yj, j = 1, . . . , N. Since we assumed
counts only the positive diﬀerences UT −
that G(y) is continuous, hence the probability of the event
is
zero.

UT −

y = 0

{

}

Obvious that zN has binomial distribution since zN is nothing else than
the sum of N independent random variables with values either 0 or 1. By
using the notation

we can write

∆(UT −

P{

y) = 1

=

y

}

P{

≤

UT }

= p,

zN = j

=

}

P{

N
j

(cid:18)

(cid:19)

pj (1

p)N −j,

−

j = 0, 1, . . . , N.

18

(10)

(11)

≤

The task is very simple. Assume that we have a sample of size N and
for this sample zN = k
N. We should determine a conﬁdence interval
[γL(k), γU (k)] which covers the value p with a prescribed probability β. The
unknown p deﬁned by (10) is nothing else than the probability that the
output variable y is not larger than the technological limit UT . When the
lower conﬁdence limit γL(k) is near the unity, then, since γL(k) < p, we
can state at least with probability β that the chance of ﬁnding the output
variable y smaller than UT is also near the unity, and so the system operation
can be regarded safe at the level [β

γL(k)].

|

3.3.1 Approximate calculation

If the sample size N > 50, then the random variable

k

Np

−
Np (1

p)

= ζk

−
has approximately standard normal distribution, where k is the number of
sample elements not larger than UT . Let β be the conﬁdence level, then we
can write that

p

=

P{|

uβ}

ζk| ≤

|
−
where Φ(x) is the standard normal distribution function. This equation can
be rewritten 7 in the following form:

= 2Φ(uβ)

1 = β,

p) ≤

P (

uβ

p

)

−

Np

k
|
−
Np (1

ζk| ≤

uβ}

=

P{|

ζ 2
k ≤

u2
β}

P{

=

=

P{

(N + u2

β)(p

γL)(p

γU )

−

0

}

≤

= β,

−

(12)

γL = γL(k, uβ) =

k(1

k/N) + u2

(13)

1
N + u2

β (cid:20)

k +

u2
β −

uβ

γU = γU (k, uβ) =

k +

u2
β + uβ

k(1

k/N) + u2

(14)

7The following elementary considerations can be found in any textbook for statistics,

1
N + u2

β (cid:20)

−

−

q

q

,

β/4
(cid:21)

.

β/4
(cid:21)

where

and

e.g. [6].

1
2

1
2

19

It is obvious that [p

γL(k, uβ)][p

γU (k, uβ)]

0 is fulﬁlled only, if

−

≤

−

≤

γL(k, uβ)

γU (k, uβ),

p

≤

and therefore

uβ}
where uβ is the root of Eq.

ζk| ≤

P{|

=

γL(k, uβ)

p

≤

≤

P{

γU (k, uβ)

= β

}

(15)

Φ(uβ) =

(1 + β).

1
2

This equation shows clearly that the interval [γL(k, uβ), γU (k, uβ)] covers the
unknown p with probability β.

In many cases we do not need the restriction due to the upper conﬁdence
p
.
}
and

limit. We want to know only the probability of the event
{
Since ζk at ﬁxed k is a decreasing function of p, the events
γL(k, vβ)

are equivalent, and so we can write

γL(k, vβ)
≤
vβ}
ζk ≤

{

p

{

≤

}

ζk ≤

vβ}

P{

=

P{

γL(k, vβ)

p

}

≤

= Φ(vβ) = β.

(16)

Consequently, the operation of a system can be regarded safe if the param-
eter p for all output variables is covered by [γL(k, vβ), 1] with a prescribed
probability β, provided that γL(k, vβ) is near the unity. 8 For the sake of
simpler notation in the sequel γL(k, vβ) and γU (k, uβ) will be denoted by γL
and γU , respectively.

y

{

UT }

The event

belonging to the acceptance region of the sample
space will be called success. Now, let us calculate the number of successes k
needed in a sample of size N to ensure a ﬁxed conﬁdence level β and a given
lower conﬁdence limit γL.

≤

Table VII. Numbers of sample elements k in samples of size N = 100(10)200
needed for the acceptance on level β = γL = 0.95. (The approximate formula (13) has
been used for calculations.)

k
99
N 100

108
110

118
120

128
130

137
140

147
150

157
160

166
170

176
180

185
190

195
200

8It is obvious that γL(k, vβ )

γL(k, uβ).

≥

20

In Table VII we see the numbers of successes needed in samples of size
N = 100(10)200 in order to reach the level β = γL = 0.95. The requirement
is quite sever: if the sample size N = 100 one should have k = 99 successes!
For illustration of the method the approximate γL < p values have been
calculated at conﬁdence levels β = 0.90(0.01)0.99 when the sample size N =
100 and the number of successes k = 90(1)100. The results are shown in Table
occurs only once,
VIII. It can be seen, for example, that if the event
then it can be stated with probability β = 0.95 that γL = 0.9564 < p. It
means that the appearance of ”dangerous” events
is not excluded
even if the level of acceptance is better than (0.95

≥

{

y

UT }
UT }
y
{
≥
0.9564).
|

Table VIII. Approximate γL < p values calculated at conﬁdence levels β =

0.90(0.01)0.99 for numbers of success k = 90(1)100. Sample size N = 100.

0.95
0.8396
0.8515
0.8635
0.8757
0.8882
0.9008
0.9138
0.9273
0.9414
0.9564
0.9737

0.93
0.8469
0.8586
0.8704
0.8825
0.8947
0.9072
0.9200
0.9331
0.9469
0.9617
0.9787

0.982
0.8213
0.8335
0.8458
0.8584
0.8712
0.8843
0.8978
0.9117
0.9264
0.9420
0.9505

0.94
0.8435
0.8553
0.8672
0.8794
0.8917
0.9043
0.9171
0.9304
0.9444
0.9593
0.9764

0.99
0.8085
0.8208
0.8333
0.8460
0.8591
0.8724
0.8861
0.9003
0.9152
0.9311
0.9487

k

β

\
90
91
92
93
94
95
96
97
98
99
100

0.90
0.8549
0.8664
0.8781
0.8899
0.9019
0.9141
0.9266
0.9394
0.9528
0.9672
0.9838

0.91
0.85245
0.8640
0.8758
0.8877
0.8997
0.9120
0.9246
0.9376
0.9511
0.9655
0.9823

0.96
0.8350
0.8470
0.8591
0.8714
0.8839
0.8967
0.9099
0.9235
0.9377
0.9529
0.9703

k

β

\
90
91
92
93
94
95
96
97
98
99
100

0.92
0.8498
0.8615
0.8733
0.8852
0.8974
0.9097
0.9224
0.9355
0.9491
0.9637
0.9806

0.97
0.8292
0.8413
0.8535
0.8659
0.8786
0.8915
0.9048
0.9186
0.9330
0.9484
0.9658

21

3.3.2 Exact calculation

When the sample size N is smaller than 50 we cannot apply the asymp-
totically valid normal distribution. For the exact calculation of conﬁdence
limits we used a slightly new version of the method proposed by Clopper and
Pearson [7].

The probability of ﬁnding at least k successes from N observations is

nothing else than

k

S(N )
k

(p) =

N
j

pj (1

p)N −j,

−

j=0 (cid:18)
X

(cid:19)

(17)

where

As known, this formula can be written in the form:

p =

y

P{

≤

.

UT }

S(N )
k

(p) =

k! (N

1

uk (1

u)N −k−1 du =

−

=

k! (N

N!
k

−
and it is obvious, that S(N )
of p, since

k

v)k vN −k−1 dv,

(1

−

(18)

(p) is a continuous monotone decreasing function

dS(N )
k
dp

(p)

=

−

k! (N

N!
k

−

1)!

−

pk (1

p)N −k−1 < 0.

−

Taking into account that

N!
k

−

1)!

−

p

Z

1−p

1)!

−

0
Z

it is evident that S(N )
Consequently, a p = pδ value can be determined so that

(p) assumes any values in the interval [0, 1] only once.

k



S(N )
k

(p) =

1,

if p = 0,




0,

if p = 1,

S(N )
k

(pδ) = δ,

0 < δ < 1.

∀

22

Since S(N )

k

(p) is a monotone decreasing function, if p > pδ, then

S(N )
k

(p) < S(N )

k

(pδ) = δ.

1

0.95

0.9

0.85

0.8

0.75

0.7

s
t
i

m

i
l

e
c
n
e
d
i
f
n
o
c

s
t
i

m

i
l

e
c
n
e
d
i
f
n
o
c

1

0.975

0.95

0.925

0.9

0.875

0.85

0.825

cl = 0.95, N = 50

40

42

44

46

48

number of successes

cl = 0.95, N = 100

90

92

94

96

98

number of successes

Figure 6: Dependence of the upper and the lower conﬁdence limits on the number of
successes k at conﬁdence level β = cl = 0.95 in cases of sample size N = 50 and 100,
respectively.

23

Clearly, the function

R(N )
k

(p) = 1

S(N )
k−1(p) =

−

N

N
j

pj (1

p)N −j,

−

Xj=k (cid:18)

(cid:19)

(19)

will satisfy the inequality

R(N )
k

(p) < R(N )

k

(pδ) = δ,

if

p < pδ.

Fixing the conﬁdence level β one can obtain the upper conﬁdence limit γU
for the unknown parameter p from S(N )
β), while the lower
conﬁdence limit γL is determined by R(N )
β). Now one can
formulate the statement that the random interval [γL, γU ] covers the un-
known parameter p with probability β.

1
2(1
−
1
2 (1

≤
(γL)

(γU )

≤

−

k

k

N=100

cl=0.90
cl=0.95
cl=0.99

0.975

0.95

0.925

0.9

0.875

0.85

0.825

t
i

m

i
l

e
c
n
e
d
i
f
n
o
c

r
e
w
o

l

90

92

94

96

98

100

number of successes

Figure 7: Dependence of the the lower conﬁdence limit on the number of successes k on
three conﬁdence levels β = cl = 0.90, 0.95, 0.99 when the sample size N = 100.

For the sake of illustration Fig. 6 shows the dependence of the upper
and the lower conﬁdence limits on the number of successes k on conﬁdence
level β = 0.95 in cases of sample size N = 50 and 100, respectively. For
example, if k = 98, i.e. two observations out of N = 100 are failed, then we
can state with probability 0.95 that the unknown p is covered by the interval
[0.9296, 0.9975].

As mentioned already in many practical situations it suﬃces to know that
the interval [γL, 1] calculated from the sample of N observations covers the

24

with prescribed probability β. Fig. 7
chance of success p =
shows the dependence of the the lower conﬁdence limit γL on the number
of successes k at three conﬁdence levels β = cl = 0.90, 0.95, 0.99 when the
sample size N = 100.

UT }

P{

≤

y

Table IX. Lower conﬁdence limits at three levels when the number of successes

k = 90(1)100. Sample size N = 100.

β
k
\
0.90
0.95
0.99

90
0.8501
0.8362
0.8086

91
0.8616
0.8482
0.8212

92
0.8733
0.9602
0.8340

93
0.8850
0.9725
0.8471

94
0.8970
0.8850
0.8604

95
0.9092
0.8977
0.8741

β
k
\
0.90
0.95
0.99

96
0.9216
0.9108
0.8882

97
0.9344
0.9242
0.9030

98
0.9476
0.9383
0.9185

99
0.9616
0.9534
0.9354

100
0.9772
0.9704
0.9549

Table IX contains the γL values plotted in Fig. 7 for the mostly used
conﬁdence levels provided that the sample size N = 100. It is remarkable
that even in that case when k = 100, i.e. when all elements of a sample can be
found in the acceptance interval we can state with probability β = 0.95 only
that the unknown p value is covered by the interval [0.9704, 1], or simply,
but not precisely: the p is larger than 0.97 with probability 0.95 One can
imagine a number of cases where this statement is not enough to declare: the
operation of the analyzed system can be regarded safe.

3.4 Tolerance interval method

Assume again that we have N independent values y1, . . . , yN of the output
variable y. Let γ and β be positive numbers not larger than 1. Now, we wish
y1, . . . , yN }
to answer the following question: On the basis of a sample
can we state that a fraction larger than γ of the distribution G(y) lays with
probability β in an interval [L, U]

⊆
In order to answer this question, let us construct from the sample

SN two
random functions L = L(y1, . . . , yN ) and U = U(y1, . . . , yN ), called tolerance
limits, such that

[LT , UT ]?

SN =

{

(20)

U

P{

L

Z

dG(y) > γ

= β.

}

25

We remark that

U

dG(y) =

(y1, . . . , yN )

(21)

L

Z

A
is a random variable, sometimes called probability content, which measures
the proportion of the distribution included in the random interval [L, U].
Probability β bears the name conﬁdence level. For safe operation it is advis-
able to specify the probability content γ and the conﬁdence level β as large
as possible in the interval (0, 1).

Having ﬁxed β and γ, from deﬁnitions of L(y1, . . . , yN ) and U(y1, . . . , yN )
it becomes possible to determine the number of runs N. Carrying out N
, from which we can calculate an appro-
runs, we get a sample
priate tolerance interval [L, U].
If that interval lies in [LT , UT ] we declare
the operation safe. 9 This program can be easily realized when the distribu-
tion G(y) is known and normal, however, in subsection 3.4.1 the problem of
distribution free tolerance interval will be discussed.

y1, . . . , yN }

{

3.4.1 Distribution free tolerance limits

To solve the problem of setting tolerance limits when nothing is known about
the cumulative distribution function G(y) except that it is continuous, seems
to be not an easy task. Exploiting advantages of the order statistics, Wilks [8]
was the ﬁrst who found a satisfactory solution to the problem and somewhat
later Robbins [10] published a nice proof that distribution free tolerance limits
can be given only by means of order statistics.

It is evident that in the order statistics we are unable to exploit the total
amount of information which is present in the sample when the distribution
function G(y) is unknown. Consequently, with γ and β given, we anticipate
either a wider tolerance interval around the sample mean or a larger sample
size to achieve the same tolerance interval as in the case of known G(y).
Not going into details, we give here a well-known theorem, which is useful in
uncertainty and sensitivity analysis of codes.

Theorem 2 Let y1, . . . , yN be N independent observations of the random
output y. Suppose that nothing is known about the distribution function G(y)
except that it is continuous. 10 Arrange the values of y1, . . . , yN in increas-
9Many authors have discussed the problem of setting tolerance limits for a distribution
on the basis of an observed sample. The pioneering work was done by S. S. Wilks [8] and
by A. Wald [9].

10It can be shown that the one-sided continuity only is needed.

26

ing order, 11 and denote by y(k) the k-th of these ordered values; hence in
particular

y(1) = min
1≤k≤N

yk,

y(N) = max
1≤k≤N

yk,

and by deﬁnition y(0) =
. In this case for some
positive γ < 1 and β < 1 there can be constructed two random function
L(y1, . . . , yN ) and U(y1, . . . , yN ), called tolerance limit, such that the proba-
bility that

, while y(N + 1) = +

−∞

∞

U

L

Z

dG(y) > γ

holds is equal to

where

β = 1

I(γ, s

r, N

s + r + 1) =

−

−

−

γj (1

γ)N −j,

(22)

−

s−r−1

N
j

j=0 (cid:18)
X

(cid:19)

I(γ, j, k) =

du,

B(j, k) =

γ

uj−1 (1

u)k−1

−
B(j, k)

0

Z
0

≤

r < s

N,

and

L = y(r),

≤

1)!

,

−
1)!

(23)

(j

1)! (k

−
(j + k

−
U = y(s).

The proof of Theorem 2, which is a simpliﬁed version of Wald’s proof, is

given in Appendix II.

The selection of tolerance limits L = y(1) and U = y(N) appears to be
expedient in many cases. Substituting r = 1 and s = N in Eq. (22), we get
for the two-sided tolerance interval the expression

β = 1

γN

−

−

N(1

γ) γN −1.

−

Often we are interested solely in the upper tolerance limit U = y(N) and
we call the interval [y(0), y(N)] one-sided tolerance interval. Now r = 0 and
s = N, therefore

β = 1

γN .

−
When the lower limit is of interest, we select [y(1), y(N + 1)] and this is also
a one-sided tolerance interval. Substituting r=1 and s=N+1 into expression
(22), we obtain (25) again.

11The probability that equal values occur is zero.

(24)

(25)

27

Finally, we make two remarks. Two outputs are considered the same if
their diﬀerence is smaller than the round-oﬀ error. Therefore the probability
that two runs yield the same output is very small but not zero. The second
remark is that expressions (24)-(25) may appear as a relationship between
two probabilities β and γ. However, γ is not a probability, which can be seen
from the nonsensical interpretation for γ from any of the mentioned expres-
sions. In Table X. we compiled the probability content γ of the tolerance
interval [y(1), y(N)] for β = 0.9, 0.95, 0.99 and N = 10(10)100(25)300.

If we are interested in a tolerance interval [L, U] which includes larger
than γ = 0.953 proportion of the distribution of the output with probability
β = 0.95, then we should make 100 runs, see Table X. and select the lowest
output as L and the largest as U. If U is smaller than the technological limit
UT , then the system is safe at the level γ = 0.953, β = 0.95. This means
that additional runs may produce an output exceeding U but this portion of
runs is not larger than 4.7% of the total number of runs. However, these rare
output values may be greater than the technological limit UT . Evidently, if
U is larger than UT , the system must be declared unsafe.

Table X. γ values of tolerance interval [y(1), y(N )] for β = 0.9, 0.95, 0.99 and

N = 10(10)100(25)300.

N

10
20
30
40
50
60
70
80
90
100
125
150
175
200
225
250
275
300

β = 0.90
0.66315
0.81904
0.87643
0.90620
0.92443
0.93671
0.94557
0.95225
0.95747
0.96166
0.96924
0.97432
0.97796
0.98069
0.98282
0.98453
0.98593
0.98710

β = 0.99
0.49565
0.71127
0.79845
0.84528
0.87448
0.89442
0.90890
0.91989
0.92851
0.93554
0.94813
0.95658
0.96268
0.96736
0.97087
0.97375
0.97618
0.97809

γ values
β = 0.95
0.60584
0.78389
0.85141
0.88682
0.90860
0.92336
0.93402
0.94207
0.94837
0.95344
0.96262
0.96877
0.97318
0.97650
0.97909
0.98118
0.98287
0.98429

28

pr
1
0.8
0.6
0.4
0.2

pr

0.8
0.6
0.4
0.2

pr
1
0.8
0.6
0.4
0.2

pr

0.8
0.6
0.4
0.2

pr

0.25
0.2
0.15
0.1
0.05

pr
1
0.8
0.6
0.4
0.2

pc= 0.80

pc= 0.90

10 20 30 40 50

N

10 20 30 40 50

N

pc= 0.95

pc= 0.98

10 20 30 40 50

N

10 20 30 40 50

N

pc= 0.98

pc= 0.99

200 400 600 800

N

200 400 600 800

N

Figure 8: The dependence of the probability β = pr on the the number of runs N at
probability contents γ = pc = 0.8, 0.9, 0.95, 0.98 0.99.

To get some insight into relation (24) we present the probabilities β ver-
sus N for six γ values, see Fig. 8. With increasing number of runs, each
interpolated curve reaches saturation, and β tends to unity as N tends to
inﬁnity. The smaller is the γ value the sooner comes the saturation, because
small γ means that only a small fraction of the calculated output is required
to fall into the given interval.

Small γ value is not acceptable in safety analysis for small γ means that a
large portion of output values may fall outside the tolerance interval. Practi-
cally we need γ > 0.95. For example, if we wish the tolerance interval [L, U]
to include larger than γ = 0.98 proportion of the output values with proba-
bility β = 0.95, we need approximately 235 runs in order to get the proper
L and U. In spite of the large number runs the probability content γ = 0.98
is far from being completely satisfactory. To achieve a better probability

29

content, say γ = 0.99 with probability β = 0.95, we need 473 runs, which is
practically hard to realize.

3.4.2 Known cumulative distribution function

Let us assume the cumulative distribution function G(y) to be known. How-
ever, one should emphasize that there are situations where it would be par-
ticulary dangerous to make unwarranted assumptions about the exact shape
of distribution G(y). In general, the attempt to get an explicit expression
for β by means of expression (20) would fail. There is however one excep-
tion, when G(y) is of normal distribution N(m, σ) then exact formula can be
obtained for β. 12

We shall denote by ˜yN the sample estimate of the expectation value m

and by ˜σ2

N that of the variance σ2, i.e.

N

1
N

˜yN =

yk,

and

˜σ2
N =

Xk=1
Let us construct two random variables, viz.

1

−

N

1

N

Xk=1

(yk −

˜yN )2.

(26)

λ ˜σN

and U = U(y1, . . . , yN ; λ) = ˜yN +λ ˜σN ,

L = L(y1, . . . , yN ; λ) = ˜yN −
where the parameter λ scales the length of the interval [L, U]. Denote by
(˜yN , λ˜σN ) the proportion of the output distribution included between the
A
limits L(y1, . . . , yN ; λ) = ˜yN −

λ˜σN and U(y1, . . . , yN ; λ) = ˜yN + λ˜σN , i.e.

(˜yN , λ˜σN ) =

g(y) dy =

A

U

L
Z

Introducing new variable z = (y

−

1
√2πσ

U

(y

m)2

exp[

−

−
2σ2

L
Z
m)/σ we obtain

] dy.

(27)

(m + σ˜zN , λ˜σN ) = ρ(˜zN , ˜sN ) =

A

1
√2π

uN

e−z2/2 dz,

(28)

where

˜zN =

m

˜yN −
σ

and

˜sN =

ℓN

Z

˜σN
σ

,

12It is worth mentioning that if output variable y is a sum of a large number of small,
statistically independent random variable, then its distribution is almost normal. Now we
discuss the case when the output variable y is of normal distribution.

30

while

ℓN = ˜zN −

λ ˜sN

and

uN = ˜zN + λ ˜sN .

We stress again that ρ(˜zN , ˜sN ) is a random variable because in expression
(28) the limits of the integral are random variables.

Theorem 3 For any given positive value of λ the probability that ρ > γ,
where 0 << γ < 1 is expressed by

W (λ, γ, N) = 1

N
2π

− r

+∞

−∞

Z

KN −1

(N

1)

"

−

q(µ, γ)
λ

2

#

(cid:19)

(cid:18)

e−N µ2/2 dµ,

where KN −1[
q(µ, γ) is the solution of the equation

] is the χ2 distribution with (N

· · ·

−

(29)
1)-degrees of freedom and

1
√2π

µ+q

µ−q

Z

e−x2/2 dx = γ.

(30)

The value λ determining the tolerance interval 13 at a preassigned probability
content γ and a preassigned signiﬁcance level β in the case of N runs can be
calculated from the equation

W (λ, γ, N) = β,

(31)

and it is independent of unknown parameters m and σ of the distribution
function G(y). The equation (31) has exactly one root in λ, since W (λ, γ, N)
is a strictly increasing function of λ.

Proof of Theorem 3 is given in Appendix III, since the mathematical
details are not relevant to the aim of the present work. However, it is worth
mentioning that an approximate tolerance interval can be derived when N
is large (e.g. N > 50).

13If one-sided tolerance interval with upper limit is needed, then Eq. (30) should be

replaced by

1
√2π

µ+q

−∞

Z

2

e−x

/2 dx = γ.

31

Theorem 4 The approximate two-sided tolerance interval is given by

[˜yN −

λa(γ, β) ˜σN , ˜yN + λa(γ, β) ˜σN ],

where

λa(γ, β) =

N
−
QN −1(1

s

1

−

β)

q(1/√N , γ).

(32)

Here QN −1(1
degree of freedom and q(1/√N , γ) is the root of the equation

β)-percentile of the χ2 distribution with (N

β) is (1

−

−

1)

−

1
√2π

1
√N

+q

1
√N

−q

Z

e−z2/2 dz = γ.

(33)

The λa for the approximate one-sided tolerance interval with upper limit can be calculated
in the same way, but Eq. (33) has to be replaced by

1
√N +q

1
√2π

−∞

Z

2

e−z

/2 dz = γ.

Proof of Theorem 4 is given in Appendix IV.

Table XI. λ values of two-sided tolerance intervals for the number of runs N =50(5)100

β = 0.90
0.95
2.284
2.265
2.248
2.234
2.222
2.211
2.202
2.193
2.185
2.178
2.172

0.90
1.916
1.901
1.887
1.875
1.865
1.856
1.848
1.841
1.834
1.828
1.822

0.99
3.001
2.976
2.956
2.936
2.920
2.906
2.894
2.882
2.872
2.862
2.854

β = 0.95
0.95
2.379
2.354
2.333
2.315
2.299
2.285
2.272
2.261
2.251
2.241
2.233

0.90
1.996
1.976
1.958
1.943
1.929
1.917
1.907
1.897
1.889
1.881
1.874

0.99
3.126
3.093
3.066
3.042
3.021
3.002
2.986
2.971
2.958
2.945
2.934

β = 0.99
0.95
2.576
2.538
2.506
2.478
2.454
2.433
2.414
2.397
2.382
2.368
2.355

0.99
3.385
3.335
3.293
3.257
3.225
3.197
3.173
3.150
3.130
3.112
3.096

0.90
2.162
2.130
2.103
2.080
2.060
2.042
2.026
2.012
1.999
1.987
1.977

N
γ
\
50
55
60
65
70
75
80
85
90
95
100

In order to give an impression of λ values (i.e. of the tolerance interval
around the sample mean of the output variable), Table XI. contains the λ

32

l
e
v
e
l

e
c
n
e
d
i
f
n
o
c

1

0.9

0.8

0.7

0.6

0.5

ip=2.5

pc=0.95
pc=0.97
pc=0.98

20

40

60

80

100

sample size

Figure 9: Dependence of the conﬁdence level β on the sample size N at probability
contents γ = pc = 0.95, 0.97, 0.98 when the interval parameter λ = ip = 2.5.

values 14 associated with often used γ and β for the sample sizes N=50(5)100.
One can see that at N=100 the tolerance interval which includes 95% of the
distribution with 95% probability is given by

[˜y100 −

2.23˜σ100, ˜y100 + 2.23˜σ100] .

If that interval 15 lies within [LT , UT ] then the system is safe on level γ = 0.95
and β = 0.95.

Fig. 9 shows convincingly the interrelations between the basic character-
istics of the tolerance intervals for a normal distribution. As expected the
conﬁdence level β increases with increasing sample size N provided that the
coverage pc = γ and the interval parameter ip = λ are ﬁxed.

However, if the ﬁxed coverage γ exceeds a critical value γcrt ≈

0.98758
when λ = ip = 2.5, then one can observe an ”anomalous” behavior of the
dependence β on N, as shown in Fig. 10.
It is seen that the probability
β of ﬁnding the proportion γ > γcrt of the distribution G(y) in the interval
λ ˜sN , ˜zN + λ ˜sN ) decreases with increasing sample size N > Ncrt,
(˜zN −
where Ncrt depends on both λ and γ. The explanation is straightforward:

14More detailed tables can be found in [11].
15If one-sided tolerance interval with upper limit is needed, then λ = 2.23 has to be

replaced by λ = 1.75!

33

ip=2.5

pc=0.985
pc=0.990
pc=0.995

0.6

0.5

0.4

0.3

0.2

0.1

l
e
v
e
l

e
c
n
e
d
i
f
n
o
c

l
e
v
e
l

e
c
n
e
d
i
f
n
o
c

1

0.9

0.8

0.7

0.6

0.5

20

40

60

80

100

sample size

Figure 10: Dependence of the conﬁdence level β on the sample size N at probability
0.98758 and provided the interval parameter
contents larger than the critical value γcrt ≈
λ = ip = 2.5 is ﬁxed.

2

2.2

2.4

2.6

2.8

3

interval parameter

Figure 11: Dependence of the conﬁdence level β on the interval parameter λ = ip at
probability content γ = pc = 0.95 for three sample sizes N = 40, 50, 60.

since

lim
N→∞

˜zN

p
= 0

and

lim
N→∞

˜sN

p
= 1,

pc=0.95
N=40
N=50
N=60

34

pc=0.99
N=40

N=50

N=60

0.8

0.6

0.4

0.2

l

e
v
e

l

e
c
n
e
d
i
f
n
o
c

0

2

2.2

2.4

2.6

2.8

3

interval parameter

Figure 12: Dependence of the conﬁdence level β on the interval parameter λ = ip at
probability content γ = pc = 0.99 higher than the critical value for three sample sizes
N = 40, 50, 60.

it is evident that

ρ(˜zN , ˜sN )

p
= γcrt,

lim
N→∞

where

γcrt =

+λ

e−x2/2 dx,

1
√2π

−λ

Z

consequently, if γ = γcrt + δ, where 0 < δ < 1

γcrt, then

ρ(˜zN , ˜sN )

γcrt|

−

= 0,

}

−
> δ

lim
N→∞ P{|
γcrt|

i.e.
N > Ncrt. It easy to show that 16

ρ(˜zN , ˜sN )

> δ

P{|

−

}

is a monotonously decreasing function of

ρ(˜zN , ˜sN )

γcrt|

−

> δ

>

}

P{

P{|

ρ(˜zN , ˜sN ) > γcrt + δ

= β,

}

16Introducing the notations:

and

and taking into account that

ρ(˜zN , ˜sN )

{

γcrt −

δ

}

≤

=

A

(−)
N

{

ρ(˜zN , ˜sN ) > γcrt + δ
(−)
N =

}

A
, we can write that

=

(+)
N ,

(+)
N ∩ A
=

A

> δ

ρ(˜zN , ˜sN )

γcrt|

−

P{|

}

P{A

(+)
N ∪ A

(−)
N }

>

P{

ρ(˜zN , ˜sN ) > γcrt + δ

.

}

∅

35

and so one can state that β decreases with increasing N > Ncrt if γ > γcrt
provided λ is ﬁxed.

It is not superﬂuous to know how does the conﬁdence level β depend on
the interval parameter λ at a ﬁxed probability content (coverage) γ and at
a given sample size N. Fig. 11 shows this dependence at γ = pc = 0.95 for
three sample sizes N = 40, 50, 60. What we see completely corresponds to
our expectations, however, as seen in Fig. 12, the character of β vs. λ curves
is radically changing. The explanation is the same as in the case of Fig. 10.

4 Several output variables

Now we assume the output to comprise n variables. Let these variables be
y1, . . . , yn. If they are statistically completely independent 17 we can apply
the results of previous Sections, otherwise we need new considerations. Let
G(y1, . . . , yn) be the unknown joint cumulative distribution function of the
output variables, furthermore, let

y11 y12
y21 y22
...
...
yn1 yn2

. . . y1N
. . . y2N
...
. . .
. . . ynN








S N = 





(34)

be the sample matrix obtained in N >> 2n independent observations (runs).
Introducing the n-components vector

the sample matrix can be written in the form:

By using proper statistical methods for testing the sample matrix we can
make useful probabilistic statement about the safety of system operation.

17There are many fairly good statistical tests to prove the independence of random

variables.

y1k
y2k
...
ynk



,






~yk = 





S N =

~y1, . . . , ~yN }
{

.

36

First, we will show how to generalize the method of sign test for several
output variables, and then we will deal with the problem of setting tolerance
limits for more than one random variable.

4.1 Sign test

For the sake of simplicity we are going to deal with two output variables
y1 and y2 provided their joint distribution function G(y1, y2) is unknown,
but continuous at least from right (or from left) in both variables. Let us
accept that the system operation can be declared safe if the requirement
y1 < U (1)

is realized with probability

{

T , y2 < U (2)
T }

p12 =

y1 < U (1)

T , y2 < U (2)
T }

P{

(35)

T and U (2)

near the unity. Here U (1)
T are the limit values deﬁned by technology,
and they deﬁne the acceptance region of the (y1, y2) plane. Since the p12
is unknown, the task is to construct from the sample a conﬁdence interval
[γ(1,2)
] which covers the p12 with probability β12. In most of the cases
L
it is suﬃcient to calculate the γ(1,2)
, 1] as
conﬁdence interval. Let the 2-components vectors

only and to use the interval [γ(1,2)

, γ(1,2)
U

L

L

~yk =

y1k
,
y2k(cid:19)
(cid:18)

k = 1, . . . , N

be elements of a sample
should emphasize that ~yj and ~yk are independent if j
of a given sample vector are not.

S N obtained by N independent observations. One
= k, but the components

In order to use a terminology as simple as possible, the event

y1 <

{

will be called success. Deﬁne now the function

U (1)
T , y2 < U (2)
T }

1,

if y1k < U (1)

T and y2k < U (2)
T ,

∆

U (1)

T −

(cid:16)

y1k

∆

(cid:17)

U (2)
(cid:16)

T −

y2k

=

(cid:17)

0, otherweise,

and introduce the statistical function

z(1,2)
N =

N

Xk=1

∆

U (1)
(cid:16)

T −

y1k

∆

(cid:17)

(cid:16)

U (2)

T −

y2k

,

(cid:17)

(36)






37

6
which gives the number of successes in a sample of size N. Since z(1,2)
is
the sum of N independent random variables with values either 1 or 0, it is
obvious that z(1,2)

is of binomial distribution. By using the notation

N

N

∆

P{

U (1)
(cid:16)

∆

y1

T −

U (2)
(cid:17)
(cid:16)
y1 < U (1)
T , y2 < U (2)
T }

T −

P{

y2

=

= 1

=

}

(cid:17)
= p12,

we can write

z(1,2)
N = k

=

}

P{

pk
12 (1

p12)N −k,

−

∀

k = 0, 1, . . . , N,

N
k

(cid:18)

(cid:19)

and this is the point where we can use from the results of Subsection 3.3.

Now, we would like to make a trivial but important amendment. Deﬁne

two statistical functions:

N

z(1)
N =

i=1
(cid:16)
X
N and z(2)

∆

U (1)

T −

y1i

(cid:17)

and

z(2)
N =

N

j=1
X

∆

U (2)
(cid:16)

T −

y2j

.

(cid:17)

Clearly, z(1)
N are not independent, but both of them are sum of N
independent random variables with values either 1 or 0, consequently one
can write

z(1)
N = i
}

P{

=

pi
1(1

−

p1)N −i

(cid:18)

(cid:19)

z(2)
N = j

=

}

P{

pj
2(1

p2)N −j,

−

(cid:18)

(cid:19)

i, j = 1, . . . , N,

and

where

pℓ =

yℓ < U (ℓ)
T }

P{

=

∆

P{
ℓ = 1, 2,

U (ℓ)
(cid:16)

T −

yℓ

= 1

,

}

(cid:17)

(2)
N =

are unknown probabilities. By using the samples
and
}
scribed in Subsection 3.3 we can construct two random intervals [γ(1)
[γ(2)

}
separately with help of the method de-
L , 1] and
L , 1] covering p1 as well as p2 with probabilities β1 and β2, respectively.

j = 1, . . . , N

i = 1, . . . , N

y2j,

y1i,

S

S

{

{

(1)
N =

N
i

N
j

38

S

S

γ(1)
L ) and (β2|

γ(2)
Obviously it could be occurred that the levels (β1|
L ) support
(1)
(2)
the statement that the samples
N and
N separately do not contradict to
the requirement of safe operation, however, from this one cannot conclude
that the operation of the system is safe on a preassigned level for variables
y1 and y2 tested jointly. The reason is clear: the output variables y1 and
y2 are not independent, and in this case we have to know weather the value
p12 =
, 1] with a
L
preassigned probability β12. Clearly, γ(1,2)
, therefore γ(1)
L
and γ(2)
L do not contain suﬃcient information to declare that the operation
of the system is safe. The procedure should be as follows: ﬁrstly test the
hypothesis that the output variables y1 and y2 are dependent, and if this is
the case, estimate the probability of the event
, and
not the events

is covered by the interval [γ(1,2)
min
{

T , y2 < U (2)
T }

T , y2 < U (2)
T }

γ(1)
L , γ(2)
L }

{
separately.

y1 < U (1)

y1 < U (1)

L ≤

and

P{

y1 < U (1)
T }

{

y2 < U (2)
T }

{

Finally, we would like to note that the generalization of the sign test for
n > 2 output variables is straightforward: we have to use the statistical
function

N

n

z(1,...,n)
N

=

∆

U (j)

T −

yjk

,

j=1
Y
in order to obtain the sum of N independent random variables, and then the
further steps will be the same as they were in Subsection 3.3.

Xk=1

(cid:17)

(cid:16)

4.1.1 Illustration

Now we want to present an example to show how the sign test method is
working. By using Monte Carlo simulation we have generated two samples a
and b. Both are consisting of N = 100 value pairs due to the population of a
bivariate normal distribution with parameters m1 = m2 = 0 and σ1 = σ2 = 1,
but the correlation coeﬃcient is C = 0.1 in a, while C = 0.7 in b.

One can see in Fig. 13 that in the sample a four, while in the b two

observations out of N = 100 can be found in rejection region.

From Table IX. one can read that in the case of sample a the inter-
val [0.9108, 1] covers the parameter p12 with probability β12 = 0.95, at
the same time both p1 and p2 are covered by the interval [0.9383, 1] with
0.95) is not ”very good”, but better than
β1 = β2 = 0.95. The level (0.9383
(0.9108
0.95), however, in the decision about the safety one should take evi-
dently into account the level calculated for the parameter p12, and not those
calculated separately for p1 and p2.

|

|

39

acceptance region

- 2

- 1

0

1

2

output Y1

acceptance region

2

1

0

- 1

- 2

2
Y

t
u
p
t
u
o

2
Y

t
u
p
t
u
o

2

1

0

- 1

- 2

- 2

- 1

0
output Y1

1

2

Figure 13: Sample vectors denoted by points in the sample plane (y1, y2) and the ac-
ceptance region deﬁned by technological requirements. Upper ﬁgure (sample a) and lower
ﬁgure (sample b) refer to correlation coeﬃcients C = 0.1 and C = 0.7, respectively.

Testing the sample b which shows a strong correlation between the vari-
ables y1 and y2, we ﬁnd that the conﬁdence interval [0.9383, 1] covers the
parameter p12 with probability β12 = 0.95. Consequently, we can state with
probability 0.95 that the chance of the event
is higher

y1 < U (1)

{

T , y2 < U (2)
T }

40

than the value 0.9383, i.e. we are able to declare that the operation of the
system is safe on the level (0.95
0.9383) only. The parameters p1 and p2
are covered by intervals [0.9534, 1] and [0.9383, 1], respectively, with the pre-
scribed probability β1 = β2 = 0.95, however, these values are not informative
for the safety of the system.

|

This simple example shows convincingly that the tests performed sepa-
rately on output variables which are depending on one another could bring
about false decision concerning the safety of the system operation.

4.2 Tolerance region

The problem of setting tolerance limits for output variables y1, . . . , yn can
be formulated as follows. Assume that the unknown joint distribution func-
tion G(y1, . . . , yn) is absolute continuous, i.e. it has a joint density function
g(y1, . . . , yn). For some given positive values γ < 1 and β < 1 we have to
construct n pairs of random variables Lj(y1, . . . , yn) and Uj(y1, . . . , yn) j =
1, . . . , n such that the probability that

U1

Un

L1

Z

· · ·

Ln

Z

g(y1, . . . , yn) dy1 · · ·

dyn > γ,

(37)

holds is equal to β. A natural extension of the procedure applied previously
to the one variable case would seem the right selection. Unfortunately that
choice does not provide the required solution since the probability of the
inequality (37) depends on the unknown joint density function g(y1, . . . , yn).
Our task is to ﬁnd a reasonable procedure such that the probability β is
independent of g(y1, . . . , yn). It can be shown that such a procedure exists
but its uniqueness has not been proven yet.

that no two elements of the sample matrix
rows in the sample matrix
number the output variables arbitrarily.

Since the distribution function G(y1, . . . , yn) is continuous, we can state
S N are equal. The sequence of
S N can be arbitrary, reﬂecting the fact that we
Let us choose the ﬁrst row of the sample matrix, and arrange its elements
in order of increasing magnitude y1(1), y1(2), . . . , y1(N). Select from these
y1(r1) as L1 and y1(s1) > y1(r1) as U1. Let i1, i2, . . . , is1−r1−1 stand for the
original column indices of elements y1(r1 + 1), y1(r1 + 2), . . . , y1(s1 −
1). In
the next step, choose the second row, the N observed values of the output
variable y2 and arrange the part y2i1, y2i2, . . . , y2is1
1 of its elements in

r1

−

−

41

increasing order to obtain y2(1) < y2(2) <
< y2(s1 −
1). From among
these, y2(r2) and y2(s2) > y2(r2) are selected for L2 and U2 and evidently
r2 ≥
1. We continue this imbedding procedure to the last
row of the sample matrix and deﬁne a n-dimensional volume 18

r1, s2 ≤

s1 −

r1 −

r1 −

· · ·

Vn =

{

[L1, U1]

[L2, U2]

×

× · · · ×

[Ln, Un]
}

,

Lj = yj(rj),

Uj = yj(sj),

rj ≥

rj−1 ≥ · · · ≥

r1,

where

and

while

1,

sj−1 −

rj < sj ≤

rj−1 −
Theorem 5 In the case of n
2 dependent output variables with continuous
joint distribution function G(y1, . . . , yn) it is possible to construct n-pairs of
j = 1, . . . , n such that the probability of the
random intervals [Lj, Uj],
inequality

j = 2, . . . , n.

≥

∀

U1

Un

· · ·

L1

Ln

g(y1, . . . , yn) dy1 · · ·

dyn > γ,

Z
is free of g(y1, . . . , yn) and is given by

Z

U1

Un

P

L1

(cid:26)Z

· · ·

Ln

Z

g(y1, . . . , yn) dy1 · · ·

dyn > γ

=

(cid:27)

= 1

I (γ, sn −

−

−

rn, N

sn + rn + 1) = β.

(38)

Here function I(

) is the regularized incomplete beta-function and

· · ·

sn ≤

sn−1 −

rn−1 −

1

s1 −

≤

(rj + 1)

and

rn ≥

rn−1 ≥ · · · ≥

r1. (39)

Proof of Theorem 5 is given in Appendix V.

18This n-dimensional volume is the tolerance region which is nothing else than a subspace

of an n-dimensional Euclidian space.

n−1

j=1
X

42

4.2.1 Illustrations

In several practical applications the choice r1 = r2 =
sn = N
−
sided tolerance region is given by

= rn = 1 and
1) can be advised, hence the conﬁdence level β for a two-

2(n

· · ·

−

β = 1

I (γ, N

2n + 1, 2n) =

−

−

γj (1

γ)N −j.

(40)

−

N −2n

N
j

j=0 (cid:18)
X

(cid:19)

The structure of expression (40) is remarkably similar to that of expression
(22), which refers to the one output variable case. Furthermore, if the lower
limits Lj =

j = 1, . . . , n, i.e if

,

−∞

∀
r1 = r2 =

· · ·

= rn = 0

and

sn = N

n + 1,

−

then one obtains the conﬁdence level

β = 1

I (γ, N

n + 1, n + 2) =

−

−

γj (1

γ)N −j

(41)

−

N −n

N
j

j=0 (cid:18)
X

(cid:19)

for one-sided tolerance region.

In many practical cases it is suﬃcient to use one-sided tolerance regions
if two mutually dependent output

(limited from above).
variables y1 and y2 are tested, then from (41) one obtains

If n = 2, i.e.

β = 1

γN

−

−

N(1

γ)γN −1

−

(42)

which is exactly the same as (24) derived for the two-sided tolerance in-
terval for one output variable. Here, it is worthwhile to cite two sentences
from [13]. ”There are several ways to interpret even a simple mathematical
formula. The problem under consideration decides which interpretation we
need. Notwithstanding, we should carefully prove the appropriateness of the
interpretation chosen.”

Perhaps, it is not superﬂuous to show how to determine the two-dimen-
sional one-sided tolerance region for output variables y1 and y2. First, cal-
culate from (42) the number of observations N needed for the preassigned
safety level (β

γ). Secondly, create the the sample

|

S N =

y11, y12, . . . , y1N
y21, y22, . . . , y2N (cid:19)

,

(cid:18)

43

and arrange the elements of the ﬁrst row in increasing order. We obtain the
matrix

y1(1), y1(2),
y2i2,
y2i1,

. . . , y1(N)
. . . ,

y2iN (cid:19)

,

(cid:18)

and choose the element y1(N) as upper limit for y1, i.e. U1 = y1(N). Thirdly,
search the largest element in the series y2i1, y2i2, . . . , y2iN
1 that gives the
upper limit for y2, i.e. U2 = max1≤j≤N −1 y2ij , and ﬁnally, construct the
, U2] which is tolerance region of variables (y1 and y2).
region [
[
−∞
−∞
×
Clearly, if U1 < U (1)
and U2 < U (2)
T , then we can state that the operation of
T
the system is safe on the level (β
γ) for the jointly tested two variables (y1
and y2).

, U1]

|

−

|

In order to compare the number of runs needed to determine two-sided
tolerance regions at a given (β
γ) level for n = 1, 2, and 3 mutually depen-
dent output variables with unknown distributions, we compiled Table XII. In
order to achieve the usual safety level (0.95
0.95) we need N = 153 observa-
tions in the case of two and N = 207 observations in the case of three output
variables. The number of observations (runs) needed to meet stringent re-
β = 0.98)
quirement, e.g. with three output variables the level (γ = 0.98
we need N = 598 runs. Therefore it seems to be inevitable to seek methods
with lower computational demands.

|

|

Table XII. Number of runs needed to determine the two-sided tolerance region for
n = 1, 2, 3 output variables at listed γ, β values.

β

γ

\

0.95

0.96

0.97

0.98

0.99

0.95
93
153
207
98
159
215
105
167
224
114
179
237
130
197
258

0.96
117
191
260
123
200
269
132
210
281
143
224
297
163
248
324

0.97
156
256
348
165
267
360
176
281
376
192
300
397
218
331
433

44

0.98
235
385
523
249
402
542
266
422
565
289
451
598
329
499
651

0.99
473
773
1049
499
806
1086
533
848
1134
581
905
1199
661
1001
1307

n
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3

In order to provide some insight, let us consider the following example.
We have two output variables y1 and y2, their the joint distribution function
is known:

g(y1, y2) =

1
2π√1

exp

(cid:20)

C 2

−

1

−

−

2(1

C 2)

(y2

1 −

2Cy1y2 + y2
2)

,

(43)

(cid:21)

| ≤

C
|

1 is the correlation coeﬃcient of variables y1 and y2. We are
where
interested in the relationship between the signiﬁcance level β and probability
content of a given two dimensional region [L, U] = [L1, U1]
[L2, U2] at the
number of runs N = 50(50)200.

×

Table XIII. Levels of signiﬁcance β of two-sided tolerance regions for two output
variables at listed γ and N values.

N

γ

\

50

100

150

200

0.95
0.8831
0.9433
0.2396
0.9109
0.9911
0.7422
0.9933
0.9981
0.9452
0.9986
0.9998
0.9910

0.96
0.7547
0.8775
0.1391
0.8836
0.9590
0.5705
0.9443
0.9869
0.8542
0.9683
0.9955
0.9605

0.97
0.5351
0.7442
0.0628
0.6297
0.8488
0.3528
0.6871
0.9044
0.6616
0.7380
0.9414
0.8528

DF

0.98
0.2376 C = 0.1
0.4970 C = 0.9
0.0178
0.2121 C = 0.1
0.4970 C = 0.9
0.1410
0.1894 C = 0.1
0.5554 C = 0.9
0.3528
0.1612 C = 0.1
0.5779 C = 0.9
0.5685

DF

DF

DF

Now we can proceed in two ways. The ﬁrst way is to ﬁx the fraction of the
samples to fall into the given interval [L, U], and to determine the associated
probability β, from Eq.
(41), these numbers are in row DF (referring to
Distribution Free). The second way is to use the known joint distribution
function, calculate the estimates of variances ˜σi for i = 1, 2 from N runs
2.5˜σi, +2.5˜σi]. From 105 random cases
and deﬁne the interval [Li, Ui] = [
we estimated the β value, see Table XIV. These values are given for two
correlation coeﬃcient in rows C = 0.1 and C = 0.9.

−

As we see in Table XIII. the order statistics gives lower β values, in
most of the cases, compared to those obtained by using the density function

45

1

0.8

0.6

0.4

0.2

l
e
v
e
l

e
c
n
e
d
i
f
n
o
c

N=100
ip=2.5
DF

C=0.1

C=0.9

0.95

0.955

0.96
0.97
0.965
probability content

0.975

0.98

Figure 14: Two output variables.

g(y1, y2, C). 19 This indicates a considerable gain from a known distribution
function of output variables.

In order to visualize the dependence of the conﬁdence level β on proba-
bility content γ < γcr three curves are shown in Fig. 14 when N = 100. The
two upper curves correspond to the known bivariate normal distribution of y1
and y2 with λ = 2.5, while the curve denoted by DF refers to the distribution
free case.

5 Safety Inference

The purpose of performing safety analysis is to assure that the designed
It is a self-understanding premise that
equipment can be operated safely.
by altering input data randomly within their prescribed distribution all the
states will be either safe or unsafe.
If both safe and unsafe states would
occur, the entire range under consideration should be regarded as unsafe.

19Without going into details, we mention only there exists a critical value γcr(C) such
that for γ > γcr(C) for all N1 < N2 we have β(γ, N1) > β(γ, N2). The critical value
λ
γcr(C) is deﬁned by the integral
−λ g(y1, y2, C) dy1 dy2, similarly to that proved in
one-dimensional case. See sub-subsection 3.4.2! The decrease of β with increasing N can
be so large for N > 100 that the β becomes smaller than the value obtained from the order
statistics. This is the case for some β values with C = 0.1 in Table XIII. when N > 100
and γ > γcr(0.1) = 0.9753.

λ
−λ

R

R

46

Our approach has severe consequences on every statement concerning
safety. The present section assesses those consequences. The ﬁrst conse-
quence is that we can not speak of safety of a given state, rather we can
speak of the probability of a given state to be safe. Assume for the sake of
simplicity that the model is not chaotic around the nominal state (~x0, ~y0)
where ~y0 = ˆC~x0. The input variable(s) may take values in a given range,
that range is mapped into a range of the output variables. From some other
considerations, which thus far have not been regarded as part of safety anal-
ysis, we get information on the probability distribution of the input variable.
And, we select a range into which a large portion, say more than 90%, of the
possible area lies with a given high, say 95%, probability. Consequently, we
conclude that:

1. It is insuﬃcient to show that the nominal state is safe because there may
be probable inputs, which are unsafe. Therefore, when the calculations
are carried out exclusively in the nominal state, safety analysis should
demonstrate the estimated error to be realistic.

2. Another possibility is that safety analysis should show that images of
all x points in the vicinity of x0 are safe. In this case we get rid of the
uncertainty caused by input uncertainty.

3. Assumptions or knowledge of probability distributions of the input do
have an impact on safety issues, therefore they must not be treated
separately. Here two problems occur. Engineering input data are usu-
ally not accompanied by probability distributions and input variables,
which are actually internal in the given calculational model, may inﬂu-
ence the output error decisively. Such internal inputs are usually ob-
tained from a ﬁtting but that procedure usually gives no information
on the probability distribution of ﬁtted parameters (although theory
and technique are known).

4. Even if every ˆ
C

~x is safe for a given interval, there is a slight chance that
some input(s) may be associated with unsafe output(s). Those chances
can be read out from Table XI. for normally distributed output and
from Tables IX. and X. for a single arbitrarily distributed output vari-
able, as well as from Table XII. for two and three arbitrarily distributed
output variables.

47

5. Safety is described by random variables therefore we can make only
statistical assertions. Any claim concerning safety is associated with
(β, γ) and the assumptions on the probability distribution(s) of the
input variables. An alarming example is given in sub-subsection 3.2.3
where we see that 22.4% of the rejected calculations result in larger
maxima than in the basic sample.

6. Biased probability density functions seem to be extremely dangerous.
The simple problem of determining a quantile (see Fig. 4) may lead to
large diﬀerences. Based on the presented examples it seems desirable to
treat certain class of distributions of the output variables with special
care.

7. Safety analysis should make it clear that every output interval lies in-
side the safety envelope. The safety is not unconditional but the values
β and γ characterize that ”level” of safety. When any of inequalities
yk(N)
T , k = 1, . . . , n would be observed, then the system opera-
tion could hardly be declared safe. This clearly indicates that safety is
not deterministic, as treated by many, but random.

U (k)

≥

Since the general consideration results in loss of a large amount of infor-
mation, eﬀorts should be made to chose a reasonable test for the estimation
of the probability distribution of the output variable(s). To this end speciﬁc
safety analysis models should be analyzed individually. All these caveats
necessitate a reconsideration of safety issues.

6 Conclusions

The object of our investigation has been a complex system (e.g. a computer
code) that we treated as a black box: From a well-deﬁned input set the system
(code) produces a well-deﬁned output set. Both sets have metrics; we can
speak of distance between two input sets or between two output sets. The
computer code simulating the complex system is a map ˆ
X
C
is the input set and
is the output set. When analyzing given equipment,
we have a nominal input x0 but the actual input might as well be anywhere in
, hence the input is a random vector. The probability distribution of input
X
components is usually derived from diverse engineering considerations.
In
that setting, we have to predict the statistical behavior of the output vector;

X → Y

, where

Y

:

48

and, we have to specify a safety envelope into which the actual output falls
with high probability. 20 The exact statements are formulated as theorems
in Sections 3 and 4, while the conclusions are summarized as follows.

1. The nominal state

is determined from the expectation value
of the input and from the associated output. The investigation of the
nominal state alone is insuﬃcient to declare the system operation to
be safe.

~x0

(cid:17)

~x0, ˆ
C
(cid:16)

2. When the distribution of output is not known, four methods, namely
the Bayesian, the percentile and the sign test as well as the tolerance
interval methods are proposed for testing the output data. The statis-
tical statements which can be obtained by these methods do not diﬀer
signiﬁcantly from one another. As expected, since the distribution is
unknown, only a fraction of the information present in the output can
be utilized. As a result, more runs are needed or lower probabilities
are achieved.

3. When the output is of normal distribution, Theorem 3 determines an
interval [L, U] around the estimated mean value of the output into
which a larger than a prescribed fraction γ of the distribution falls
with preassigned probability β. The limits L and U are determined by
the sample estimate of the standard deviation and by a positive factor
λ. For the mostly used N, β, γ values the λ factors are given in Table
XI. Our results are in accordance with Ref. [12] and [11].

4. When the output consists of more then one statistically not indepen-
dent quantities, the portion of information content that can be utilized
rapidly decreases with the number of simultaneously tested output vari-
ables. That manifests again in a larger number of runs or in lower
probabilities. This is true for both the sign test and tolerance inter-
val methods. The results are given in Tables IX. and XII. It is worth
noting that our results comply with the results given in Ref. [12] only
when the output variables are independent. To achieve identical (β, γ)
level for statistically dependent outputs we need a larger number of
runs than given in Ref. [12].

20The present work was initiated by a remark stating that a limited number of runs

suﬃces to determine the safety envelop.

49

All these observations may inﬂuence, in safety analysis, the application of
best estimate methods, and underline the opinion that any realistic modeling
and simulation of complex systems must include the probabilistic features of
the system and the environment.

I Appendix. Proof of Theorem 1.

Obvious, if G(y) is continuous strictly increasing function of y, then

y(r)

Qγ ≤

≤

P{

y(s)

=

y(r)

G−1(γ)

}
z(r)

P{
γ

≤
z(s),

P{

≤

≤

}

y(s)

=

}

≤

which is nothing else than

z(r)

γ

z(s),

=

z(r)

γ, z(s)

γ,

.

P{

≤

≤

}
z(s)

P{
γ

≤

}

≤

and

=

A

{

≥
z(s)

}
γ

≥

}

Introducing the notations
that

=

A

{

, we can write

z(r)

γ

=

z(r)

≤
}
γ, z(s)

P{
γ

+

γ,

≤
z(r)

P{
z(r)

=

+

A}
γ, z(s)

=

≤

}

P{

A

≤

> γ

,

}

≥

≤
and from this we obtain

P{

y(r)

Qγ ≤

≤

P{

y(s)

=

z(r)

γ, z(s)

}

P{

≤

γ

}

≥

=

=

z(r)

γ

z(r)

γ, z(s)

} − P{
P{
By using the well known expression

≤

≤

γ

.

}

≤

(I-a)

u

z(r)

u + du, v

z(s)

v + dv

= gr,s(u, v) du dv =

P{

≤

≤

≤

≤

}

=

ur−1 (v
B(r, s

−
−

u)s−r−1 (1
r) B(s, N
v
u
0

≤

≤

−
−
≤

v)N −s
s + 1)
1,

du dv,

(I-b)

we obtain that

γ

1

0 Z
Z

0

γ

γ

−

0 Z
0

Z

50

β =

gr,s(u, v) du dv

gr,s(u, v) du dv.

(I-c)

The ﬁrst integral:

Y1 =

gr,s(u, v) du dv =

gr(u) du,

γ

1

0 Z
Z

0

γ

0
Z

Taking into account that v

u the second integral is nothing else than

where

hence

where

gr(u) =

ur−1 (1
B(r, N

u)N −r
r + 1)

,

Y1 = I(γ, r, N

r + 1).

−
−

−

gr,s(u, v) du dv =

≥

γ

γ

0 Z
0
Z
v

Y2 =

γ

dv

0
Z

0
Z

Cr,s =

B(r, s

= Cr,s

ur−1 (v

u)s−r−1 (1

v)N −s du,

−

−

1
r) B(s, N

.

s + 1)

−

(I-d)

−
By performing the transformations

u = t1t2

and

v = t2

the integral Y2 can be easily calculated. Since

= t2,

∂u
∂t1

∂u
∂t2

J =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂v
∂t1

∂v
∂t2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(t1t2)r−1 (t2 −

1

we ﬁnd that

Y2 = Cr,s

t2 dt2

t1t2)s−r−1 (1

t2)N −s dt1 =

= Cr,s

tr−1
1

(1

t1)s−r−1 dt1

ts−1
2

(1

t2)N −s dt2.

0
Z
Replacing Cr,s by (I-c) one obtains that

−

−

γ

0
Z

γ

0

Z

1

0

Z

−

Y2 = I(γ, s, N

s + 1).

−

51

By using the well known identity I(c, a, b) = 1

I(1

c, b, a), ﬁnally we have

β =

y(r)

P{
γ, N

≤

Qγ ≤
s + 1, s)

y(s)

}
I(1

= I(1

−

−

−

−

−

−
−
Y2 =
= Y1 −
γ, N

r + 1, r),

and so the Theorem 1 is proven.

II Appendix. Proof of Theorem 2.

The derivation of Eq. (22) is based on the following observation. Let

G(y) =

g(t) dt

y

−∞

Z
be the unknown but continuous cumulative distribution function of the out-
put variable y. Let y1, . . . , yN be its independently observed values. Arrange
the sample elements yk, k = 1, . . . , N in increasing order, and denote by
y(k) the kth element of the ordered sample. Introduce the random variables
z(k) = G[y(k)], k = 1, . . . , N which are are not independent [6]. According
to (I-b) the bivariate density function of z(s) and z(r), r < s is given by

g(N )
(r,s)(u, v) =

(r

1)! (s

−

−

N!
r + 1)! (N

s)!

−

0

u

v

≤

≤

≤

1.

In order to determine the probability

ur−1 (v

u)r−s+1 vN −s,

−

y(s)

y(r)

P (Z
G[y(r)] > γ

dG(y) > γ

=

)

=

G[y(s)]

P {

−

=

}

P {

z(s)

z(r)] > γ

,

}

we need the probability

t

z(s)

z(r)

P {

≤

−

t + dt
}

≤

= w(N )

(r,s)(t) dt =

g(N )
(r,s)(u, u + t) du dt.

(II-a)

−

1−t

0

Z

52

Substituting g(N )
out:

r,s (u, v) into (II-a), the integration in (II-a) can be carried

w(N )

(r,s)(t) =

1
r) B(s, N

ts−r+1

1−t

0
Z

s + 1)

−

B(r, s

−

ur−1 (1

t

−

−

u)N −s du,

(II-b)
where B(j, k) is the Euler beta function. Taking this expression into account,
we get

z(s)

z(r) > γ

=

P {

−

w(N )

(r,s)(t) dt,

1

γ

Z

}

γ

z(s)

z(r) > γ

= 1

P {

−

ts−r−1 (1
r, N

B(s

−

−
−

t)N −s−r
s + r + 1)

dt.

−

0

Z

after integration we obtain

In other words,

as stated. Q.E.D.

}

−

β = 1

I(γ, s

r, N

s + r + 1)

−

−

(II-c)

III Appendix. Proof of Theorem 3.

The proof of the Theorem 3 is based on a few well known relations of math-
ematical statistics. By the deﬁnition of conditional probability,

+∞

ρ(˜z, ˜s) > γ

=

P{

}

−∞ P{

Z

ρ(˜z, ˜s) > γ

˜z = µ

d

˜z

|

}

P{

≤

µ

,

}

(III-a)

where

Since

we have

d

˜z

µ

= d

P{

≤

}

1
N

P (

N

n=1
X

m

yn −
σ

≤

µ

=

)

r

N
2π

e−N µ2/2 dµ.

ρ(˜z, ˜s) > γ

˜z = µ

=

ρ(µ, ˜s) > γ

,

P{

|

}

P {

}

ρ(˜z, ˜s) > γ

=

P{

+∞

N
2π

}

r

−∞ P {

Z

ρ(µ, ˜s) > γ

e−N µ2/2 dµ.

(III-b)

}

53

where

is random variable. Let us deﬁne the function
µ+λs

ρ(µ, ˜s) =

µ+λ˜s

e−z2/2 dz

µ−λ˜s

Z

r(µ, λs) =

e−z2/2 dz

µ−λs

Z

1
√2π

1
√2π

for real s. If µ and λ are ﬁxed, then r(µ, λs) is strictly monotonously in-
creasing function of s, therefore the equation

has only one root in s. It is clear that λs is independent of λ, hence we may
write λs = q(µ, γ) and q is obtained from

1
√2π

µ+λs

µ−λs

Z

e−z2/2 dz = γ

1
√2π

µ+q

µ−q

Z

e−z2/2 dz = γ.

It follows from the property of r(µ, λs) that the probability of ρ(µ, ˜s) > γ
equals to the probability of ˜s > sr, i.e.

˜s > sr}
By using this relations we can write that

ρ(µ, ˜s) > γ

P {

P {

=

}

=

˜s >

P

(cid:26)

q(µ, γ)
λ

.

(cid:27)

(III-c)

˜s >

q(µ, γ)
λ

=

˜s2 >

(cid:27)

P

(cid:26)

[q(µ, γ)]2
λ2

=

(cid:27)

P

(cid:26)

˜σ2
σ2 >

[q(µ, γ)]2
λ2

,

(cid:27)

P

(cid:26)

and taking into account that the random variable

(N

1)

−

N

˜σ2
σ2 =

˜y

yn −
σ

2

(cid:19)

n=0 (cid:18)
X

is of χ2 distribution with (N

1) degree of freedom [6], we get

ρ(µ, ˜s) > γ

= 1

KN −1

(N

1)

P {

}

(III-d)

where

KN (x) =

1
2Γ(N/2)

Substituting (III-d) into (III-b) we get the theorem proven. Q.E.D.

−

−

[q(µ, γ)]2
λ2

,

(cid:27)

(N −2)/2

e−u/2 du.

−

(cid:26)

x

u
2

0
Z

(cid:16)

(cid:17)

54

IV Appendix. Proof of Theorem 4.

Before setting out for the proof of Theorem 4, we set forth the following
notation. Let

H(λ, γ

µ) =

ρ(µ, ˜s) > γ

.

|

P {

}

Lemma 1 It can be shown that

We need

where

H(λ, γ

1/√N)

W (λ, γ, N) = O(N −2).

(IV-a)

|

−

W (λ, γ, N) =

H(λ, γ

µ) e−µ2/2 dµ.

(IV-b)

+∞

N
2π

r

−∞

Z

|

Proof of Lemma 1. The expression

H(λ, γ

µ) =

|

P

1
√2π

µ+λ˜s

µ−λ˜s

e−z2/2 dz > γ

(IV-c)

Z
is an even function of µ and can be developed into Taylor series around µ = 0
as

(cid:26)

(cid:27)

H(λ, γ

µ) =

|

∞

n=0 (cid:20)
X

∂2nH(λ, γ
∂µ2n

|

µ)

µ2n
(2n)!

.

(cid:21)µ=0

(IV-d)

Substituting (IV-d) into (IV-c) we obtain

W (λ, γ, µ) =

∞

n=0 (cid:20)
X

∂2nH(λ, γ
∂µ2n

|

µ)

(2n

1)!!

−
(2n)!

1
N n =

(cid:21)µ=0

= H(λ, γ

0) +

|

∂2H(λ, γ
∂µ2

|

µ)

1
2N

(cid:21)µ=0
and replacing µ by 1/√N in (IV-d), we have

(cid:20)

+ O(N −1/2),

(IV-e)

H(λ, γ

1/√N ) = H(λ, γ

0) +

|

∂2H(λ, γ
∂µ2

|

µ)

1
2N

(cid:21)µ=0

|

(cid:20)

+ O(N −1/2).

(IV-f)

55

From Eqs. (IV-f) and (IV-e) follows that (IV-a) is true. Consequently , we
have the following approximate equation

H(λ, γ

1/√N) = 1

KN −1

(N

1)

−

−

|

[q(N −1/2, γ)]2
λ2

β,

≈

where the argument of KN −1[
of χ2 distribution with (N

(cid:20)

(cid:21)
] is nothing else than the (1

β) percentile
· · ·
1) degree of freedom. Introducing the notation

−

QN −1(1

β) = (N

1)

[q(N −1/2, γ)]2
λ2

,

−

−

−

we ﬁnd that

λa(γ, β) =

λ

≈

N
−
QN −1(1

s

1

−

β)

q(1/√N , γ).

(IV-g)

This completes the proof of Theorem 2. Q.E.D.

V Appendix. Proof of Theorem 5.

The proof of Theorem 5 is given in two steps. In the ﬁrst step we show that
the Theorem holds for n = 2, and then we generalize the claim for n > 2.

Step 1. We assume that the unknown joint distribution function of two

output variables y1 and y2 is given by

and denote by

G(y1, y2) =

g(t1, t2) dt1 dt2,

y1

y2

−∞ Z
Z

−∞

+∞

−∞

Z

g1(y1) =

g(y1, t2) dt2

the density function of the output variable y1. Let us consider the following
random variable

A2 =

A2(L1, U1, L2, U2) =

Z
where the boundaries of the integration are random variables. The limits
were discussed in Section 4.2.2.

L1 Z

L2

U1

U2

g(y1, y2) dy1dy2,

(V-a)

A2(L1, U1, L2, U2) =

A2 can be expressed almost surely as
C2(L2, U2|
56

A1(L1, U1).

L1, U1)

(V-b)

(V-c)

(V-d)

(V-e)

Here

and

where

A1(L1, U1) =

U1

g1(y1) dy1

L1

Z

U2

L2

Z

C2(L2, U2|

L1, U1) =

φ2(y2|

L1, U1) dy2.

φ2(y2|

L1, U1) =

U1
L1
+∞
−∞ dy2
R

g(y1, y2) dy1

U1
L1

g(y1, y2) dy1

U1
L1

g(y1, y2) dy1
A1(L1, U1)

=

R

is the random density of variable y2 under the condition that y1 lies in [L1, U1].
G1[y1(r1)], using relation (IV-b), we ﬁnd that
Since

R

R

A1(L1, U1) = G1[y1(s1)]

−

t1 ≤ A1(L1, U1)

≤

t1 + dt1}

=

P {

ts1−r1−1
1

(1
r1, N

t1)N −s1+r1
s1 + r1 + 1)

−
−

dt1 =

L1, U1), we deﬁne the random

(V-f)

= k(N )

B(s1 −
(r1,s1)(t1) dt1.
C2(L2, U2|

To obtain the density function of
probability measure

L1, U1) =

G(t
|

φ2(y2|

L1, U1) dy2,

t

−∞

Z

with which we can express

C2(L2, U2|
r2 <

· · ·

where r1 ≤

C2 as
L1, U1) = G[y2(s2)
< s2 ≤
t2 ≤ C2(L2, U2|

|

P {

s1. Finally we get

L1, U1)

t2 + dt2}

≤

=

L1, U1]

G[y2(r2)

L1, U1],

−

|

=

ts2−r2−1
2
B(s2 −

(1
−
r2, s1 −

t2)s1−r1−1−s2+r2
s2 + r2)

r1 −

dt2 = ℓ(s1−r1−1)

(r2,s2)

(t2) dt2.

(V-g)

Note that expression (V-g) contains neither L1 nor U1, therefore, distri-
C2 is independent of L1 and U1. Consequently,
C2 is the product of (V-f) and (V-g).
A2. Exploiting the

bution of random variable
the joint density distribution of
We still need the density function of the random variable
A1, we get
independence of

A1 and

C2 and

57

t
P {

≤ A2(L1, U1, L2, U2)

t + dt
}

=

≤

1

1
x

=

t

Z

(r1,s1)(x) ℓ(s1−r1−1)
k(N )

(r2,s2)

(t/x) dx dt = wA2(t) dt.

(V-h)

Substituting here (V-f) and (V-g) and performing the indicated calculations
we obtain:

wA2(t) =

ts2−r2−1 (1
r2, N

B(s2 −

−
−

t)N −s2+r2
s2 + r2 + 1)

.

(V-i)

From this, immediately follows

= 1

r2, N
B(γ, s2 −
r2, N
B(s2 −
This completes Step 1.

−

−

P {A2(L1, U1, L2, U2) > γ

s2 + r2 + 1)
−
s2 + r2 + 1)

= 1

−

=

}
I(γ, s2 −

r2, N

s2 + r2 + 1).

−

Step 2. Now we generalize the above result for n > 2. Let us assume that
the unknown joint probability distribution of the output variables y1, . . . , yn
is given by

yn

Y1

G(y1, . . . , yn) =

−∞ · · ·

−inf ty

g(v1, . . . , vn) dv1 · · ·

dvn.

Z
Our task is to derive the probability distribution of the random variable

Z

Ap (L1, U1, . . . , Ln, Un) =

g(y1, . . . , yn) dy1 · · ·

dyn,

Un

U1

Ln · · ·

Z

L1

Z

which is an n-fold integral over the n output variables. We introduce an
intermediate term, in which an i-fold deﬁnite integral over the ﬁrst i variables
is involved, and the rest of the variables are integrated over the [
]
range:

−∞

, +

∞

+∞

=

−∞

Z

and

Ai (L1, U1, . . . , Li, Ui) =

Ui

U1

+∞

dyn · · ·

dyi+1

dyi · · ·

L1

Z

Li

Z

−∞

Z

dy1 g(y1, . . . , yn),

φi (yi|

L1, U1, . . . , Li−1, Ui−1) =

dy1 g(y1, . . . , yn),

Ui

1

−

Li

1

−

1

Ai−1 Z
58

dyi−1 · · ·

U1

L1

Z

which is the random density of the variable yi under the condition that
Lj ≤
1. As we did in (V-d), we introduce a random
probability measure associated with the condition that the ﬁrst (i
1) output
variables lie in the interval assigned to them by [Lj, Uj], j = 1, . . . , i

Uj, j = 1, . . . , i

yj ≤

−

−

1:

−

L1, U1, . . . , Li−1, Ui−1) =

L1, U1, . . . , Li−1, Ui−1) dyi.

Ci =

Ci (Li, Ui|

The above deﬁned

Z
Ai’s obey the recursion

Li

Ui

φi (yi|

Ai+1 =
Lemma 2 The probability of ﬁnding
by

Ci+1 Ai.
Ai in the interval [ti, ti + dti] is given

(V-j)

ti ≤ Ai ≤

ti + dti}

=

P {

tsi−ri−1
i

(1
ri, N

B(si −

−
−

ti)N −si+ri
si + ri + 1)

dti.

(V-k)

Proof of Lemma 2. Eq. (V-k) is certainly true for i = 1, 2 because

A1 =

A0 C1 =

C1 =

g(y) dy

U1

L1

Z

and

A2 =

C2 A1 =

C2(L2, U2|

L1, U1)

A1(L1, U1) =

g(y1, y2) dy1 dy2.

U2

U1

L2 Z
Z

L1

Now we assume that (V-k) is true for i = j and show that it is true also for
i = j + 1. Note that

Cj+1 are statistically independent because

Aj and

=

=

j+1

P {
tsj+1−rj+1−1
B(sj+1 −

tj+1 ≤ Cj+1 ≤
(1
−
rj+1, sj −
does not involve the quantities Lj, Uj, . . . , L1, U1, which occur in
joint density function of
function of

tj+1 + dtj+1}
tj+1)sj−rj −1−sj+1+rj+1
sj+1 + rj+1)

Aj. The
Cj+1 takes the form of the joint density

C2 in the case of n = 2. Hence the density function of

Aj and

A1 and

rj −

dtj+1

Aj Cj+1 =
59

Aj+1

is obtainable from Eq. (V-i) by substituting rj+1 for r2 and sj+1 for s2, i.e.

Hence, Eq.
Furthermore, the density function of

=

=

j+1

P {
tsj+1−rj+1−1
B(sj+1 −

tj+1 ≤ Aj+1 ≤
(1
−
rj+1, N

tj+1 + dtj+1}
tj+1)N −sj+1+rj+1
sj+1 + rj+1 + 1)
(V-k) is proven for i = 1, 2, . . . , n. This completes Step 2.
An is given by
tsn−rn−1 (1
rn, N

t)N −sn+rn
sn + rn + 1)

= wAn(t) dt =

t + dt
}

dtj+1.

dt.

−

t
P {

≤ An ≤

−
−
It is interesting to note that the density function of
An does not depend on
the integers r1, s1, . . . , rn−1, sn−1. This completes the proof of Theorem 4.
Q.E.D.

B(sn −

References

[1] A. Gandini: Uncertainty Analysis and Experimental Data Transposi-
tions Methods Based on Perturbation Theory, in: Y Ronen (Ed.): Hand-
book of Uncertainty Analysis, CRC Press, Boca Raton (FL), 1988

[2] A. Guba, M. Makai and L. P´al: Reliab. Engng. Sys. Safety, 80, 217

(2003)

[3] M. Kendall and A. Stuart, The Advenced Theory of Statistics, vol. 2,

London, Charles Griﬃn, 1979

[4] M.J. Burwell et al.: The Thermohydraulic Code ATHLET for Anal-
ysis of PWR and BWT Systems, NURETH-4, Karlsruhe, 1989 ; H.
Austregesilio, H. Dellenbeck: ATHLET Mod 12 Cycle A, Programmers
Manual, vol. 1., GRS, March 1998

[5] H. G. Glaeser et al.: Uncertainty Analysis of a Post-Experiment Calcu-
lation in Thermal Hydraulics, Reliability Engineering and System Safety,
45, 19 (1994)

[6] L. P´al: Fundamentals of Probability Theory and Statistics, vol. I-II,

Akad´emiai Kiad´o, Budapest, 1995, in Hungarian

60

[7] C.J. Clopper and E.S. Pearson: Biometrica, 26, 404 (1934)

[8] S.S. Wilks: Annals of Math. Stat. 12, 91 (1941) and Annals of Math.

Stat. 13, 400 (1942)

Stat. 17, 208 (1946)

[9] A. Wald: Annals of Math. Stat. 14, 45 (1943) and Annals of Math.

[10] H. Robbins: Annals of Math. Stat. 15, 214 (1944)

[11] R.R. Odeh and D.B. Owen: Tables for Normal Tolerance Limits. Sam-

pling Plans, and Screening, Marcel Dekker , New York, (1980)

[12] B. Krzykacz, EQUUS – A Computer Program for the Derivation of Em-
pirical Uncertainty Statements of Results from Large Computer Models,
GRS-A-1720, 1990

[13] M. Makai and L. P´al: Reliab. Engng. Sys. Safety, 80, 313 (2003)

61


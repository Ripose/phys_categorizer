2
0
0
2
 
r
a

M
 
3
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
3
0
3
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Time series analysis for minority game simulations of ﬁnancial markets

Fernando F. Ferreira, Gerson Francisco, Birajara S. Machado, and Paulsamy Muruganandam
Instituto de F´ısica Te´orica, Universidade Estadual Paulista, 01.405-900 S˜ao Paulo, S˜ao Paulo, Brazil
(Dated:)

The minority game model introduced recently provides promising insights into the understanding
of the evolution of prices, indices and rates in the ﬁnancial markets. In this paper we perform a time
series analysis of the model employing tools from statistics, dynamical systems theory and stochastic
processes. Using benchmark systems and a ﬁnancial index for comparison, we draw conclusions
about the generating mechanism for this kind of evolution. The trajectories of the model are found
to be similar to that of the ﬁrst diﬀerences of the SP500 index: stochastic, nonlinear and (unit root)
stationary.

PACS numbers: 05.45.Tp, 89.65.Gh, 05.10.-a

I.

INTRODUCTION

The pricing of contingent claims contracts in ﬁnancial
economics is often based on very restrictive assumptions
about the time evolution of the underlying instrument
[1].
In recent years researchers have endeavored to re-
move some of these restrictions by proposing more re-
alistic models which would incorporate features found
in real markets [2, 3, 4]. There is however a trade-oﬀ
between analytic tractability and adherence to stylized
facts observed from empirical ﬁnancial time series. The
most attractive features of the usual Black-Scholes type
of models is the possibility of obtaining closed, exact for-
mulas for the premium of derivative securities, and to
build a risk free replicating strategy. Such qualities are
amply used in ﬁnancial institutions which require fast
calculation and tools to hedge risky assets.

However, the geometric Brownian motion assumption
of the Black-Scholes models ignores several empirically
observed features of the real markets such as, volatility
clusters, fat tails, scaling, occurrence of crashes, etc. It
is as yet unknown which stochastic process is responsible
for the motion of risky assets, but physicists have taken
some important steps in the right direction[3, 5, 6, 7].
In this work we implement a microscopic agents-based
model of market dynamics which gives rise to a quite
complex and rich behaviour[8] and whose output are
macroscopic quantities such as price returns. By varying
its parameters the model exhibits market crashes, Gaus-
sian statistics and short ranged correlation, fat tailed
returns and long range correlation. This model retains
the nontrivial opinion formation structure of the grand
canonical minority game[9, 10] because it incorporates
two new features. The ﬁrst one is to allow two categories
of agents, producers (who use the market for exchang-
ing goods), and speculators (whose aim is to proﬁt from
price ﬂuctuations). The second feature is that specu-
lators might choose not to trade, and in this sense the
model it is similar to the grand canonical ensemble of
statistical physics since the number of active traders is
not constant.

We perform an analysis of the time series generated by
this model in order to classify its dynamical behaviour.

We ﬁrst test the data for unit roots and remove a sim-
ple kind of nonstationarity by taking diﬀerences wherever
necessary. The BDS statistic [11], originated in the chaos
literature, uses the correlation integral [12] as the basis
of a test for the hypothesis that the data is indepen-
dent and identically distributed (i.i.d.). We apply this
statistic as a model speciﬁcation test by applying it to
the residue of an ARIMA, autoregressive integrated mov-
ing average, process (although this might not remove all
kinds of linearity)[13]. If the null hypothesis is rejected
then this is indication that the data is nonlinear. Since
the alternative hypothesis is not speciﬁed other tests have
to be applied in order to determine whether the nonlin-
earity comes from a stochastic or deterministic mecha-
nism. Such distinction is subtle and here we approach
this question by computing more parameters, the cor-
relation dimension and two other procedures which do
not require embeddings: recurrence plots [14, 15], and
the Lempel-Ziv complexity [16, 17].
In complement to
the above procedures we implement a Bayesian approach
called cluster weighted modelling (CWM) [18, 19] in or-
der to ﬁnd further indication of determinism.

The paper is organized as follows. The market model
and its trajectories are analysed in Section II. The time
series analysis, including all statistical tests is discussed
in Section III. A succinct presentation of the CWM and
its results is found in Section IV. Analysis of the results
obtained and a classiﬁcation of the model evolution given
in Section V while in the last section we summarize the
main results and comment on future work.

II. MARKET MODEL

Inspired by the El Farol Problem proposed by Arthur
[9], the so called Minority Game (MG) model introduced
introduced by Challet and Zhang[10, 20] represents a fas-
cinating toy-model for ﬁnancial market. Now it is becom-
ing a paradigm for complex adaptive systems, in which
individual members or traders repeatedly compete to be
in the winning group. The game consists of N agents that
participate in the market buying or selling some kind of
asset, e.g. stocks. At any given time agent i can take

0.1

n
r
u
e
r

t

0.0

-0.1

0.1

n
r
u
e
r

t

0.0

-0.1

0

2

two possible actions ai = ±1, meaning buy or sell. Those
players whose bets fall in the minority group are the win-
ners, i.e., the sellers win if there is an excess of buyers,
and vice versa. To determine the minority group we just
i ai, so
consider the sign of the global action A(t) =
that if A(t) is positive the minority is the group of sell-
ers; in this case the majority of players expect asset prices
to go up.

P

In other words, this dynamics follows the law of de-
mand and supply. In the end of each turn, the output
is a single label, 1 or −1, denoting the winning group
at each time step. This output is made available to all
traders, and it is the only information they can use to
make decisions in subsequent turns. Indeed, they store
the m most recent output of winners set µ(t). In this way
a limited memory of length m is assigned to the traders
corresponding to the most recent history bit-string that
traders use to make decisions for the next step. In order
to decide what action to take, agents use strategies. A
strategy is an object that processes the outcomes of the
winning sets in the last m bets and from this information
it determines whether a given agent should buy or sell for
the next turn. The memory deﬁnes D = 2m possible past
histories so the strategy of one agent can be viewed as
a D-dimensional vector, whose elements can be 1 or -1.
The space Γ of strategies is an hypercube of dimension
D, and the total number of strategies in this space is
2D. At the beginning of the game each agent draws ran-
domly a number S of strategies from the space Γ and
keeps them forever. After each turn, the traders assigns
one (virtual) point to each of the strategies which would
have predicted the correct outcome. Along the game the
traders always choose the strategy with the highest score.
The MG is a very simple model, capable of complex be-
haviour, like phase transitions between an information-
eﬃcient phase and information-ineﬃcient phase, by just
varying a control parameter α = 2.2
N S , e.g., the ratio be-
tween information complexity and number of strategies
present in the game.

m

More realistic features are reached with the grand
canonical minority game[3]. In this model we deﬁne the
price process in terms of excess demand A(t)

(a)

(b)

2000

6000

8000

4000
time

FIG. 1: Figure showing the returns for (a) Minority game
model with Np = 1500, Ns = 510, m = 4 and ǫ = 0.003 and
(b) SP500 data

tive agents with bounded rationality. The number
of speculators is Ns.

In this version of the game, the number of speculators
can change anytime, because the agent may decide not
to trade in which case ai = 0. Since the strategy chosen
by speculators is the one with highest score, it is very im-
portant to update the scores of each strategy at each time
step. The updating of the scoring Ui,s of each strategy s
belonging to agent i is given by the following equations

Ui,s(t + 1) = Ui,s(t) − aµ(t)
Ui,0(t + 1) = Ui,0(t) + ǫ

i,s A(t) for s > 0

(2)

where ǫ is a threshold parameter. A sample of the tr-
jectory generated by this model is shown in Fig. 1(a)
together eith the SP500 index in Fig. 1(b).

This work is based upon the grand canonical MG

log p(t + 1) = log p(t) + A(t)

(1)

model, henceforth called MG model.

and introduce two kinds of agents.

• The ﬁrst kind is called producers, who go to the
market only for the purpose of exchanging goods;
they only have one strategy and in this sense they
behave in a deterministic way with respect to µ(t).
The number of producers is Np.

• The other kind of agents are the speculators who
go to the market to make proﬁt from price ﬂuctu-
ations. Since they are endowed with at least two
strategies during the game they need to use the
best strategy; in this sense speculators are adapta-

III. TIME SERIES ANALYSIS

In this section we discuss the tools employed in time
series analysis starting with the BDS statistics. We gen-
erate 10000 points for each of the benchmark data de-
scribed in the Appendix; another benchmark is the set
comprising 9998 closing prices for the index SP500 from
January 01, 1965 to January 01, 1995.

The BDS test uses the correlation integral [12] as the
basis of a statistic to test whether a series of data is
i.i.d.
In the chaos literature the correlation integral is

Data set
Lorenz system

a

Epsilon (ε)
0.070

Henon map

Random

ARMA(2,1)

NLMA(2)

SP500

0.300

0.500

0.500

0.600

0.003

Minority Game

2.800

TABLE I: BDS statistic

BDS-statistic (Embedding dimension: 2-8)
610.2067, 1031.9777, 1955.1017, 4071.2547, 9083.3612,
21286.7083, 51751.7514
101.1437, 183.6664, 368.5771, 716.3989, 1474.7676,
3129.6945, 6832.0170
-0.5487, -0.4410, -0.3585, -0.6511, -0.8622,
-0.2150, 0.3504
-0.0270, -0.5267, -0.6978, -1.0838, -2.0321,
-2.6126, -2.5441
6.1692, 11.6768, 13.3927, 14.3183, 14.5538,
14.8573 15.6130
15.4909, 19.8118, 23.6680, 27.3441, 33.1618,
40.8782, 50.6347
14.3499, 18.2687, 22.3293, 26.6340, 31.2736,
36.8712, 43.6115

3

Decision
strongly reject linearity

strongly reject linearity

accept linearity

accept linearity

reject linearity

reject linearity

reject linearity

aThe value of ε is taken as one-half the deviation of the data set

part of an eﬃcient tool to compute the fractal dimen-
sion of objects called attractors (for a formal deﬁnition
of attractor see, e.g. Ref.
[21]). Given a sample of
empirical data {xt}N
t=1, the theory of state-space recon-
struction [22] requires that the d-histories of the data be
constructed, where d is called the embedding dimension.
Under certain conditions it is possible to reproduce in this
space the dynamics of the system for a correct choice of
d. The correlation integral is a function deﬁned on the
trajectories in this space and from it one can compute
the correlation dimension. A simple test for determinism
consists of increasing the embedding dimension and ob-
serving the occurrence of a corresponding increase in the
correlation dimension. Some conditions have to be met
in order to apply this method, mainly stationarity and
suﬃcient number of data points [23, 24, 25]. Our results
show that the increase of the correlation dimension for
the MG model with respect to the embedding dimension
is practically identical to the stochastic benchmark se-
ries, including the index SP500. In this work we resort
to better ways of analysing the occurrence of stochastic
behaviour in complex time series evolution.

The BDS statistic will be applied to the residue of
ARIMA processes in order to detect nonlinearity in the
data. In this sense the statistic is used as a speciﬁcation
test. The asymptotic distribution of the statistic under
the null of pure whiteness, is the standard normal distri-
bution. The alternative hypothesis is not speciﬁed[11].
The code implemented here is taken from Ref. [26]. From
Table I, the null hypothesis for the MG model and for
the SP500 index are rejected more strongly than for the
nonlinear stochastic model N LM A(2).

In the remaining of this Section we discuss methods

which do not require embeddings.

Another measure of randomness that provides fur-
ther insight into time series dynamics is the Lempel-Ziv
complexity[16, 17]. No embedding is necessary and the
data is interpreted as a binary signal generated by some
kind of source. This idea is ever present in communi-

cation theory where one wishes to determine the min-
imum alphabet required to code a source whose signal
is to be sent through a noisy channel. Let us consider
the length L(N ) of the minimal program that repro-
duces a sequence with N symbols. The Lempel-Ziv al-
gorithm is constructed by a special parsing which splits
the sequence into words of the shortest length and that
has not appeared previously. For example, the sequence
0011101001011011 is parsed as 0.01.1.10.100.101.1011.
One can show that L(N ) ≈ Nw(log(Nw(N ) + 1) where
Nw is the number of distinct words in a parsing and N the
size of the sequence. From this one can see that L(N )
contains a measure of randomness where a source that
produces a greater number of new words is more ran-
dom than a source producing a more repetitive pattern.
In analogy with dynamical evolution, those systems that
are composed of well deﬁned cycles are predictable while
chaotic motion and stochastic processes are always pro-
ducing new kinds of trajectories that never repeat them-
selves. A comparison between chaos and stochasticity
can now be obtained. In the former case the Lempel-Ziv
complexity is well below 1 while in the latter it is close to
one. More speciﬁcally, if we consider an oscillatory sys-
tem such as the well known van der Pol oscillator, then
C = 0.049. For the Lorenz attractor with 2000 points
C = 0.181. The discrepancy between this value and that
in Table II is commented in Section V The complexity for
the MG dynamics is found to be 0.82 while the ﬁnancial
index has C ≈ 1.

The implementation of recurrence plots [14] used here
is taken from [15] since it provides a clear distinction of
the systems we intend to classify. The idea is to detect
regions of “close returns” in a data set. The construction
of the plots is simple: just compute the absolute values
| xi − xi+n | for all the points in the data base. If the hor-
izontal axis is designated by i, corresponding to xi, and
the vertical axis by i+n, corresponding to xi+n, then plot
a black dot at the site (i, i + n), whenever the absolute
value diﬀerence is lower than δ ∈ (0, 1), otherwise plot

4

FIG. 2: Recurrence plot (a) Lorenz model, (b) Henon system, (c) random numbers (d) ARMA(2,1), (e) NLMA(2), (f) SP500
and (g) minority game model.

fit
Data

fit
Data

)
t
(
x

20

0

-20

80

60

40

it white. The black/white pattern can be used to detect
determinism in the data. There is a striking diﬀerence
amongst plots generated by diﬀerential equations, maps,
random data and stochastic processes as shown in Fig. 2.

IV. CLUSTER-WEIGHTED MODELLING

An interesting probability density estimation approach
to characterize and forecast time series developed by Ger-
shenfeld, Schoner and Metois [18] is the so called cluster-
weighted modelling. This seems to be a powerful tech-
nique as it characterizes extremely well the time series of
nonlinear, nonstationary, non-gaussian and discontinu-
ous systems using probabilistic dependence of local mod-
els. The cluster-weighted modelling technique estimates
the functional dependence of time series in terms of delay
coordinates. The main task of this approach is to ﬁnd
the conditional forecast by estimating the joint probabil-
ity density.

Let {yn, ~xn}N

n=1 be the N observations in which ~xn
are known inputs and yn are the corresponding outputs.
By knowing the joint probability density p(y, ~x), we can
derive the conditional forecast, the expectation value of y
given ~x, hy|~xi. We can also deduce other quantities such
as the variance of the above estimation. Actually, the
joint density p(y, ~x) is expanded terms of clusters which
describe the local models. Each cluster contains three
terms namely, the weight p(cm), the domain of inﬂuence
in the input space p(~x|cm), and ﬁnally the dependence in

(a)

(b)

900

1000

1100

1200

1300

1000

1050

1150

1200

1100
time

FIG. 3: Fitting of data using cluster weighted analysis (a)
Lorenz system and (b) minority game model

the output space p(y|~x, cm). Thus the joint density can
be written as[19],

p(y, ~x) =

p(y|~x, cm) p(~x|cm) p(cm)

(3)

M

Xm=1

5

TABLE II: Summary of test results

ARIMA(i,j,k)
2, 0, 0
4, 0, 3
0, 0, 0
2, 0, 0
0, 0, 2
1, 0, 1
1, 0, 5

a

BDS
nonlinear
nonlinear
linear
linear
nonlinear
nonlinear
nonlinear

LZ Complexity (C)
0.0677
0.5754
≈ 1
0.8451
≈ 1
≈ 1
0.82

Recurrence plot
structures
structures
no structures
structures
structures
structures
structures

Data set
Lorenz system
Henon map
Random
ARMA(2,1)
NLMA(2)
SP500
Minority Game

Unit Root
stationary
stationary
stationary
stationary
stationary
non-stationary
stationary

aSee details in Table I

Once the joint density is known the other quantities can
be derived from p(y, ~x). For example, the conditional
forecast is given by,

hy|~xi =

y p(y|~x) dy,

Z

=

P

M
m=1 f (~x, βm) p(~x|cm) p(cm)
M
m=1 p(~x|cm) p(cm)

(4)

P

Here f (~x, βm) describes the local relationship between ~x
and y. The parameters βm are found by maximizing the
cluster-weighted log-likelihood. The simplest approxima-
tion for the local model is with linear coeﬃcients of the
form,

f (~x, βm) =

βm,ifi(~x)

(5)

I

Xi=1

The method just described is capable of modelling a wide
range of deterministic time series. Here we use cluster
weighted modelling to distinguish between deterministic
and stochastic time series. In deterministic systems we
observe that the variances of the diﬀerent clusters con-
verge to values lower than the variance of the original
time series and one can verify that this property is ro-
bust under changes of the number of clusters. On the
other hand stochastic systems do not have this property.
Fig. 3 illustrates the comparison between the ﬁtting of
a deterministic system (Lorenz) and the minority game
data using cluster weighted modelling.

V. ANALYSIS AND CLASSIFICATION

The main objective is to understand the minority
game mode of evolution and other similar time series
behaviour. Although this system is not generated by
any kind of diﬀerentiable dynamics or even stochastic
diﬀerentiable equations, we use in its analysis methods
from dynamical systems, stochastic processes and com-
plex systems theory. Tables I & II and Fig. 2 summarize
the main results. We will make frequent reference to the
SP500 index since the minority game model is supposed
to reproduce the dynamical evolution of ﬁnancial markets
and this index is used as a benchmark for comparison.

The BDS statistic provides clear evidence that the
minority game is nonlinear. Another test that could
be used to make a similar inference is the Kaplan test
statistic[27]. However since these methods have been
shown to lead to equivalent conclusions[28], we have not
implemented them both. The benchmark systems de-
scribed in the Appendix reproduce the expected results
for the BDS in Table II and we can clearly see that the
null hypothesis is strongly rejected for known determin-
istic nonlinear systems. As for the nonlinear stochastic
system, the index and the MG model, the rejection oc-
curs at all dimensions and the probability of type I er-
ror is practically zero. In particular, the BDS was used
in other instances of the MG model, using diﬀerent pa-
rameters and time intervals with the same result. These
ﬁndings support the conclusion that there is a nonlinear
mechanism in operation which drives the MG dynam-
ics and that this property is robust to the extent tested
herein.

We focus now on the stochastic behaviour of the model
generated during the time evolution. Once the initial
random selection of strategies is made, the probabilistic
aspects develop through iteration.

The Lempel-Ziv complexity is an important parameter
that can be used in the analysis of complex systems. Its
advantage is that it does not require embeddings and can
be easily employed in conjunction with other methods.
The results in Table I reveal that there is a stochastic
mechanism in operation driving the MG model. There is
a higher degree of indeterminacy in the SP500 and this
is perhaps due to the fact that in this index, in addition
to dynamical noise there is a certain amount of mea-
surement noise. The only remark that should be made
regarding the other benchmark series is the Lorenz sys-
tem.
Its surprisingly low complexity is comparable to
that of limit cycles, e.g. Van der Pol. The explanation
for this comes about when we compute its complexity for
shorter time series. For example at 2000 points the com-
plexity is about 2.5 times higher than the complexity for
a 10000 length series as reported in Table II . This phe-
nomenon does not occur for the Henon attractor. Due to
the fact that the Lorenz attractor contains a dense set of
unstable periodic orbits, long time evolution aﬀects the
computation of the complexity and reveals some resem-
blance with periodic systems. Eﬀects of this magnitude

6

did not appear in the other time series analysed.

The recurrence plot is a visual method which helps in
the identiﬁcation of similarities and diﬀerences amongst
diverse modes of evolution. Several tests with diﬀeren-
tial equations and maps, represented here by the Lorenz
and Henon systems, show that it is unlikely that low di-
mensional chaos can describe the kind of evolution found
in the markets and in the MG model. In particular, re-
currences are clearly identiﬁed in the Lorenz system and
nothing of the kind will ever appear in stochastic mod-
els. In this sense stochastic processes are better suited to
describe the ﬁnancial index and the MG model.

Cluster weighted analysis reveals another aspect of the
minority game behaviour. Its use in the modelling of gen-
eral deterministic systems produces clusters whose vari-
ances are always smaller than the variance of the data.
In contrast to this, when applied to a stochastic system
the variances of the clusters are comparable to the vari-
ances of the data. Such distinction is presented when we
vary the number of clusters.

VI. CONCLUSIONS AND FURTHER WORK

The issue of nonstationarity is a subtle one. We lim-
ited ourselves in this study to unit root stationarity, but
more sophisticated methods need to be used for the sev-
eral brands of MG models and ﬁnancial indices. Using
the complexity parameter we found that long time evolu-
tion reveals some intrinsic features of chaotic attractors
described by diﬀerential equations. Also, this parame-
ter associated a higher complexity to the SP500 index as
compared to the MG model and a possible explanation
for this was given above. The recurrence plots conﬁrm
the stochastic behaviour of the MG but there are diﬀer-
ences in the pattern generated by the several benchmark
systems including the index. The nonlinear character of
the model and the index are clearly indicated by the BDS
test.

The tests used herein show clearly that the minority
game model of ﬁnancial markets possesses a time evolu-
tion mechanism that cannot be represented by low di-
mensional chaotic systems. However the precise stochas-
tic mechanism that drives this model has not been iden-
tiﬁed. The next steps in this study would be to identify
a class of stochastic processes that resemble the motion
for various kinds of tests and statistics, and to compute
derivative instruments using the several brands of grand
canonical MG model trajectories.

model. We acknowledge Funda¸c˜ao de Amparo `a Pesquisa
do Estado de S˜ao Paulo of Brazil for fully supporting this
research (F.F.F. and P.M.)

APPENDIX A: DESCRIPTION OF THE DATA
SETS

The time series used as benchmarks were chosen to
represent the kind of behaviour we intend to identify in
the evolution of the minority game model. The Lorenz
system and the Henon mapping are prototypes of deter-
ministic behavior generated by diﬀerential equations and
diﬀerentiable mappings. ARMA models and the NLMA
are examples of linear and nonlinear stochastic processes.
In the following we describe brieﬂy the models used in the
present study.

a. Lorenz system: Nonlinear diﬀerential equations

= σ(y − x)

= −xz + rx − y

dx
dt
dy
dt
dz
dt

= xy − βz

(A1)

The parameter values are chosen as σ = 16, β = 4 and
r = 40. The data series is obtained by solving Eqs. (A1)
numerically using fourth order Runge-Kutta method.
b. Henon map: Nonlinear diﬀerentiable mapping

xt = 1 − ax2
yt = bxt−1

t−1 + yt−1

(A2)

where a = 1.4 and b = 0.3.

c. Random (Noise): The data set is taken by draw-
ing random points from a uniform distribution in the unit
interval xi ∈ rand(0, 1).

d. ARMA(2,1): Linear stochastic process

xt = 0.8xt−1 − 0.5xt−2 + 0.4ǫt−1 + ǫt

(A3)

where ǫt ∼ i.i.d., a normal distribution with mean zero
and unit variance N (0, 1).

e. NLMA(2): Nonlinear stochastic process

Acknowledgments

yt = ǫt − 0.3ǫt−1 + 0.2ǫt−2 + 0.4ǫt−1ǫt−2

−0.25ǫ2

t−2

(A4)

The authors acknowledge Damien Challet for useful e-
mail on the implementation of the grand canonical MG

where ǫt ∼ i.i.d. N (0, 1).

7

[1] F. Black and M. Scholes, J. Polit. Econ. 81, 637 (1973).
[2] I. Giardina, J.-P. Bouchaud, and M. Mezard, Physica A

[3] D. Challet, M. Marsili, and Y.-C. Zhang, Physica A 294,

(1976).

299, 28 (2001).

514 (2001).

[4] T. Lux and M. Marchesi, Nature 498, 498 (1999).
[5] H. S. Stanley and R. N. Mantegna, An introduction
to econophysics: correlations and complexity in ﬁnance
(Cambridge University Press, Cambridge, 2000).

[6] J.-P. Bouchaud and M. Potters, Theory of ﬁnancial risks:
from statistical physics to risk management (Cambridge
University Press, Cambridge, 2000).

[7] J. D. Farmer, IEEE J. Comp. Sci. Eng. 1, 26 (1999).
[8] D. Challet and Y. C. Zhang, Physica A 256, 514 (1998).
[9] W. B. Arthur, Am. Econ. Rev. 84, 406 (1994).
[10] Y.-C. Zhang, Europhys. News 29, 51 (1998).
[11] W. A. Brock, W. Dechert, and J. Sheinkman, Economet-

ric Reviews 15, 197 (1996).

[12] P. Grassberger and I. Procaccia, Phys. Rev. Lett. 50, 346

(1983).

[13] G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. Jenk-
ins, Time series analysis for forecasing and control, 2nd
ed. (Holden-Day, San Francisco, 1976).

[14] J. P. Eckmann, S. O. Kamphorst, and D. Ruelle, Euro-

phys. Lett. 4, 973 (1987).

[15] C. G. Gilmore, J. Econ. Behav. Organ. 22, 209 (1993).
[16] A. Lempel and J. Ziv, IEEE Trans. Inform. Theo. 22, 75

[17] R. Badii and A. Politi, Complexity: hierarchical stru-
tures and scaling in physics (Cambridge University Press,
Cambridge, 1999).

[18] N. Gershenfeld, B. Schoner, and E. Metois, Nature 397,

329 (1999).

[19] N. Gershenfeld, Nature of Mathematical Modeling (Cam-

bridge University Press, Cambridge, 1999).

[20] D. Challet and Y.-C. Zhang, Physica A 246, 407 (1997).
[21] J. Milnor, Comm. Math. Phys. 99, 177 (1985).
[22] F. Takens, in Detecting strange attractors in turbulence

(Springer-Verlag, Berlin, 1981), pp. 366–381.

[23] T. Schreiber, Phys. Rev. Lett. 78, 843 (1997).
[24] J. B. Gao, Phys. Rev. Lett. 83, 3178 (1999).
[25] J. P. Eckmann and D. Ruelle, Physica D 56, 185 (1992).
[26] B. LeBaron, Stu. Non. Econ. Econometrics 2, 53 (1997).
[27] D. T. Kaplan, Physica D 73, 38 (1994).
[28] W. A. Barnett, A. R. Gallent, M. J. Hinich, J. A.
Jungeilges, D. T. Kaplan, and M. J. Jensen, J. Econo-
metrics. 82, 157 (1994).


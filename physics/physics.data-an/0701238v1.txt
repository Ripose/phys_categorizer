7
0
0
2
 
n
a
J
 
2
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
3
2
1
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Turbulence Time Series Data Hole Filling using
Karhunen-Lo`eve and ARIMA methods

M P J L Chang1, H Nazari2, C O Font, G C Gilbreath and E Oh3
1 Physics Department, University of Puerto Rico, Mayag¨uez, Puerto Rico 00680
2 234 Calle Bellas Lomas, Mayag¨uez, Puerto Rico, 00682
3 U.S. Naval Research Laboratory, Washington D.C. 20375

E-mail: mark@charma.uprm.edu

Abstract. Measurements of optical turbulence time series data using unattended instruments
over long time intervals inevitably lead to data drop-outs or degraded signals. We present
a comparison of methods using both Principal Component Analysis, which is also known
as the Karhunen–Lo`eve decomposition, and ARIMA that seek to correct for these event-
induced and mechanically-induced signal drop-outs and degradations. We report on the
quality of the correction by examining the Intrinsic Mode Functions generated by Empirical
Mode Decomposition. The data studied are optical turbulence parameter time series from a
commercial long path length optical anemometer/scintillometer, measured over several hundred
metres in outdoor environments.

1. Introduction
How many data points can be missed oﬀ (set to zero) from a 1-D discrete, regularly spaced time
series record and still be recoverable? Such a question is prompted by many situations, often
associated to the automated collection of data. We are motivated to address this question by our
experimental ﬁeld work to record the behaviour of the strength of optical turbulence parameter,
C 2

n, over time intervals of several weeks, under diﬀerent climate conditions [1, 2, 3, 4, 5].
The data of interest to us are path integrated measures of C 2

n measured across the visible to
near infrared region. The measurement path is a 600 m stretch of free space, approximately 1.5
m altitude above sea water in the Caribbean. The probe beam originated from an LED centered
on 0.9 µm; the instrumentation are two identical Optical Scientiﬁc Inc. LOA-0041 systems,
which employ aperture averaging to estimate the value of C 2
n. The LOA-004s are generally
left unattended over the course of up to a few days during ﬁeld operation, which mean that
they are susceptible to spurious events which result in gaps in the data record. Although for
some types of data analysis techniques, data gaps of a limited size can be tolerated, this is not
universal. Moreover, there exists a new class of very powerful techniques based on Empirical
Mode Decomposition [6, 4] that are exceedingly sensitive to lossy datasets. It is for this reason
we are exploring diﬀerent methodologies to synthetically ﬁll the data holes; we describe the
results from our studies using Principal Component Analysis in this paper.

1 see http://www.opticalscientiﬁc.com or email info@opticalscientiﬁc.com.

2. Karhunen–Lo`eve or Principal Component Analysis
The principal components of any ensemble can be used to identify the members of that ensemble.
This idea forms the foundation of face recognition and tracking through eigenfaces (see, for
example, Turk and Pentland 1991 [8]). We may extend this method to reconstruct the missing
data for any data record under given restrictions. The key point is that the gappy data record
must have the same, or similar, salient features as all the members of the ensemble.

The principal components are the eigenvectors of the covariance matrix of the data and
represent the features of the dataset. Provided that a reference library can be created, each
member of the library will contribute to each eigenvector, more or less. As such, each member
can be exactly represented by a linear combination of eigenvectors. Any similar data record
external to the library will also be represented by a linear combination of eigenvectors, within a
margin of error.

The ﬁrst step for the ﬁlling procedure must therefore be to deﬁne the reference library.
We may do so by collecting a family of turbulence data series that share certain speciﬁc
characteristics; a diﬃcult task since the deﬁnition of such is an open question. Moreover,
since the mean value of the family of reference data plays a key part, all the members of the
library would require normalisation. Again how to do this is an open question. Alternatively we
may use the neighbouring record around a data hole, sectioning this information to provide the
ensemble members. We opt for the latter technique since the record pre and post the data hole
(within a certain time interval) ought to be similar in nature to the missing data. The mean
value of this type of library would probably not diﬀer greatly from the mean of the missing data,
so normalisation would not be so crucial.

How does one determine the principal components of a reference library? Following Sirovich
and Kirby [9], let the M members (each of length N ) of the reference ensemble be {ϕn}. Thus,
the average data record of this ensemble will be

It is very reasonable to assume that departures from the mean record will provide an eﬃcient
procedure for extracting the primary features of the data. Therefore, we deﬁne

Now, if we consider the dyadic matrix

where each term of the sum signiﬁes a second rank tensor product, we can recognize this as the
ensemble average of the two point correlation of the deviations from the mean. Here, AT is the
transpose of A.

We require eigenvectors un of the matrix AAT . For ensembles whose members have a large
number of points N > M , matrix AAT issingular and its order cannot exceed M . To ﬁnd those
eigenvectors of AAT corresponding to nonzero eigen values, Turk and Pentland used a standard
singular value decomposition technique, as described below.

ϕ =< ϕ >=

1
M

M

X
n=1

ϕn

φn = ϕn − ϕ

C =

φnφT

n = AAT

M

X
n=1

AT Avn = µvn
AAT Avn = µnAvn
CAvn = µAvn

(1)

(2)

(3)

(4)

Cun = µnun

′
φ

≈

anun

M

X
n=1

an = (φ′, un)

where µn are the eigenvalues. This deduction can be equated to

where un = Avn. Thus AAT and AT A have the same eigenvalues and their eigenvectors are
related through un = Avn, provided that ||un|| = 1. The treatment described is recognizable as
the Karhunen–Lo´eve (KL) method [10].

The implication is that a dataset φ′ can be obtained from a limited summation

where the coeﬃcients an are obtained through the inner product

We emphasise that φ′ is not considered to be part of the {φn}, although it is similar in features.

3. Proof of Concept
To demonstrate the validity of the assertion of the previous section, we take a perfect data
record of C 2
n measurements over a 7 hour period starting from midnight, smoothed by a forward
moving rolling average of 5 minutes’ interval (60 data points). The data contain 2492 points in
total. We split up the test data into 21 sections, where all but 1 are members of the reference

−15

x 10

Data split into 21 divisions of 119 points each

(5)

(6)

(7)

13

12

11

10

9

8

7

6

5

1

2

3

4

5

6

7

8

9

10 11

12 13 14 15 16 17 18 19 20 21

Figure 1. The test data, split into 21 sections. The data are padded before division with a set
of points taken from the tail of the time series and mirrored outward.

library, as shown in Fig. 1. The exclusive section is set to zero, and the algorithm described
in Sec. 2 is employed on the 20 library elements. The reconstructions are created from the KL
coeﬃecients of the eigenvectors equivalent to the sections adjacent to the missing data. Hence
we will talk of a prior and posterior reconstruction meaning e.g. for omitted section 5, we use for
the prior the KL coeﬃcient equivalent to section 4 and for the posterior reconstruction we use
the coeﬃcient equivalent to section 6 of the test data set. Note that this does not reconstruct
those sections, since the eigenvectors are generated from the entire reference library.

The reconstructions shown in the left hand set of Fig. 2 represent the best level of error,
while the right hand set shows the worst. It is clear that the greater the diﬀerence between
the (masked oﬀ) original data section and its neighbours, the poorer the reconstruction will be.
Nevertheless, the reconstruction errors for the full set of test data are all 1 order of magnitude
less than the mean value of the reconstruction and the original data segment. Evidently the end
points have not been synthesised to be continuous with the adjacent segments of the reference
library signal, as can be seen from the error. A continuity condition in terms of the both the
function and its derivative has to be imposed on both ends of the reconstructed segment in
order to achieve smoothness. The most eﬀective way to do so is currently being investigated.
The eigenvalues determined through this method show a similar distribution in both cases,
with minor variations only at the upper end of the spectrum, implying that the KL diﬀerences
between best and worst section for reconstruction is not very large.

3.1. KL and EMD
In the absence of a workable continuity condition, we present here the eﬀects of crudely patching
the data hole with reconstructions determined from the prior and posterior terms with respect
to the gap.

We apply the Empirical Mode Decomposition (EMD) algorithm [14] to the reconstructions.
EMD is a novel adaptive method for separating a nonlinear time series into components based
on instantaneous frequency. Basically it acts as a dyadic ﬁlter bank [7]; the set for the best
reconstruction case are illustrated in Figs. 3. We refer to these sets as EM DL and EM DR.
The original data’s intrinsic mode functions (IMFs) and residuals (set EM DO) are also shown
and for comparison, we present the eﬀect of a simple minded linear interpolation between the
endpoints of the known data on the IMFs.

Upon visual inspection, we see that the original data generates 9 components: 8 intrinsic
modes and 1 residual (the stopping criterion for our EMD implementation is the same as in
Huang et al [6]). On the other hand, the interpolated data have only 8 components. Numbering
the IMFs from highest instantaneous frequency to lowest, starting from IMF 1, we can see by
inspection that both EM DL and EM DR are strongly similar to EM DO in IMFs 4,5,6 and 7.
The diﬀerences appear in the higher frequency components, due to the discontinuity between
the inserted segment and the unadulterated data. The endpoint discontinuities evidently modify
the variances of IMFs 1,2 and 3, although it seems that they are only aﬀected in the area local
to the discontinuity, per IMF.

By way of comparison, a linear interpolant between the edges of the known data show that
there is contamination all through the IMFs. It is so strong that IMF 7, which in the other sets
clearly distinguishes the baseline rise and fall of the turbulence over the interval under study, is
unable to pick out a clean pedestal.

4. ARIMA
Instead of decomposing the data sequence into KL components, an alternative method studied is
that of ARIMA (Auto–Regressive Integrated Moving Average) [15]. This technique’s principle is
founded on taking advantage of the past (ﬁrst moment) behaviour of stochastic data to predict
the future characteristics.
It is a well known technique used in econometrix, and we have
employed it here to investigate its eﬀects on EMD.

An ARIMA(p, d, q) process is the solution of the following equation

φ(B)(1 − B)

dXt = ψ(B)ǫt

(8)

where BXt = Xt−1 is the backshift operator, ǫt represents a white noise process, and φ, ψ are
polynomials of degree p and q respectively. For mathematical convenience we take p and q to

−14 Segment − 1

x 10

−14 Original

x 10

−14 Segment + 1

x 10

20 40 60 80 100
−14

x 10

20 40 60 80 100
−14 Recon sec−1

x 10

20 40 60 80 100

20 40 60 80 100
−14

x 10

20 40 60 80 100
−14 Recon sec+1

x 10

Eigenvalues

(a)

20 40 60 80 100

20 40 60 80 100

10

20

−14 Segment − 1

x 10

−14 Original

x 10

−14 Segment + 1

x 10

20 40 60 80 100
−14

x 10

20 40 60 80 100
−14 Recon sec−1

x 10

20 40 60 80 100

20 40 60 80 100
−14

x 10

20 40 60 80 100
−14 Recon sec+1

x 10

Eigenvalues

0.5

0.5

0.5

0.5

Diff/pix=
4.64949e−16

Diff/pix=
5.43152e−16

0.5

0.5

0.5

0.5

0.5

0.5

Diff/pix=
2.51099e−15

Diff/pix=
1.96782e−15

0.5

0.5

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

0.5

1

0

−26

10

−28

10

−30

10

−32

10

0

0.5

1

0

−26

10

−28

10

−30

10

−32

10

0

(b)

20 40 60 80 100

20 40 60 80 100

10

20

Figure 2. Best and worst reconstruction result using PCA for (a) section 5 and (b) section
13 respectively. The layout in each set is : (Top Left) Segment prior to the selection. (Top
Middle) The original selected data. (Top Right) Segment after selection. (Centre Left) The
diﬀerence between the reconstruction using the coeﬃcient of eigenvector related to the prior
segment and the original. The value shown is the mean absolute diﬀerence per pixel. (Centre
Right) The (prior) reconstruction.
(Bottom Left) The diﬀerence between the reconstruction
using the coeﬃcient related to the posterior segment and the original. (Bottom Middle) The
(posterior) reconstruction. (Bottom Right) The eigenvalue spectrum.

−16

x 10

−16

−1
2

x 10
0

0

−2

−15

x 10
0

−15

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
5

x 10
0

−15

−5
9

x 10
0

1

0

0

2

0

0

0

0

0

8

7

0

−15

x 10

−16

x 10
0

0

2

0

0

2

0

0

0

0

−15

−2

x 10
0

−15

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−14

−2
2

x 10
0

1

0

0

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

7

7

7

7

7

7

7

7

2

0

0

0

2

0

0

0

−16

x 10

−15

−2

x 10
0

−15

x 10
0

−15

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
5

x 10
0

−15

x 10
0

0

−5

8.5

0

−15

x 10

−16

x 10
0

0

2

0

0

2

0

0

0

0

−15

−2

x 10
0

−15

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−14

−2
2

x 10
0

1

0

0

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

8

8

8

8

8

8

8

8

8

8

8

8

8

8

8

8

8

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

7

7

7

7

7

7

7

8

8

8

8

8

8

8

8

8

8

8

8

8

8

8

8

EM DO

EM DLinear

EM DL

EM DR

Figure 3. (Top Left) The IMFs 1 to 8 (top to bottom) and the residual trend line of EM DO,
generated by applying Empirical Mode Decomposition to the test data. (Top Right) The IMFs
1 to 7 and the residue of EM DL, generated from data using the PCA reconstruction method
with the coeﬃcient of the prior section. (Bottom Left) The IMFs 1 to 7 and the residue of
EM DR, generated from data using the PCA reconstruction method of the posterior section.
(Bottom Right) The IMFs 1 to 7 (top to bottom) and the residue, generated from data with a
linear interpolant across section 5 of the test data.

be zero. Fractional d generalises the diﬀerencing parameter between data points to obtain the
long range dependence. In general, 0 ≤ d ≤ 0.5, where longer memory is represented by higher
values of d.

4.1. ARIMA and EMD
We applied an ARIMA(0,d,0) model to predict the behaviour of the ﬁnal section of the split
test data. The length of each section is 119 points long; by basing the data behaviour on the
previous 2373 points and by setting the diﬀerence operator to be d = 0.5 we generated the IMF
results shown in Fig. 4.

1

0

0

0

2

0

0

0

0

−16

x 10

−15

−1

x 10
0

−15

x 10
0

−15

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
2

x 10
0

−15

−2
5

x 10
0

−15

x 10
0

0

−5

8

0

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

5

5

5

5

5

5

5

5

5

6

6

6

6

6

6

6

6

6

7

7

7

7

7

7

7

7

7

8

8

8

8

8

8

8

8

8

Figure 4. The IMFs and the residual trend line of EM DO, generated by applying Empirical
Mode Decomposition to ARIMA(0,d = 0.5,0).

As can be seen, the eﬀect of the ARIMA(0, d, 0) extrapolation is to ﬁll in the higher order
IMFs in a smooth fashion (apparently, by visual inspection , IMFs 5 to 8 are smoothly ﬁlled at
the tail). It is only untrustworthy when considering the higher frequency components, because
ARIMA considers only the conditional ﬁrst moment. Note that the number of IMFs and residual
are the same as the unadulterated signal EM DO.

5. CONCLUSIONS
We have discussed a data hole ﬁlling method for stochastic data, a necessary step to be able to use
the new technique of Empirical Mode Decomposition upon a time series record. The Karhunen–
Lo`eve eigenvectors from an ensemble of neighbouring sections of complete data around a data
hole can be used to reconstruct the missing segment to a reasonable degree of accuracy, at
least for the purposes of applying EMD. We have shown that the edge continuity is important,
although the eﬀect of discontinuities is not universal through all the intrinsic modes of the data.
The quality of the reconstruction is much better using PCA than a simplistic linear interpolant
(or merely ignoring the data gap). We compared this to a simpliﬁed ARIMA(0,d,0) model,
which performed better than the linear interpolant but less eﬀectively than the KL algorithm,
disregarding edge eﬀects.

In summary, when a data hole is present, merely ignoring the data hole or ﬁlling it with a
linear interpolant is a poor technique from the point of view of EMD. The result of doing so
leads to leakage of spurious eﬀects both laterally in time (per IMF) and longitudinally across
all the IMFs. By reconstructing the datagap with ARIMA(0, d = 0.5, 0) one can limit the
leakage laterally and longitudinally, although the lowest IMFs remain strongly contaminated
with artiﬁcial structure. KL reconstruction limits the leakage best, and even without considering
the continuity between the adjacent sections and the reconstructed datagap, it promises to
provide the best reconstruction of the three methods described in this paper.

[1] F. Santiago, M. P. J. L. Chang, C. O. Font, E. A. Roura, C. Wilcox, and S. R. Restaino, “Low altitude
horizontal scintillation measurements of atmospheric turbulence over the sea: Experimental results,” Proc.
SPIE 6014, 2005.

[2] C. O. Font, M. P. J. L. Chang, E. Oh, and G. C. Gilbreath, “Humidity contribution to the refractive index
n,” in Atmospheric Propagation III, C. Y. Young and G. C. Gilbreath, eds., Proc.

structure function C 2
SPIE 6215, 2006.

[3] M. P. J. L. Chang, C. O. Font, G. C. Gilbreath, and E. Oh, “Humidity contribution to C 2

n over a 600m

pathlength in a tropical marine environment,” Proc. SPIE 6457, 2007.

[4] M. P. J. L. Chang, C. O. Font, G. C. Gilbreath, and E. Oh, “Humidity’s inﬂuence on visible region refractive
n,” Applied Optics (accepted), ArXiv Physics e-prints physics/0606075,

index structure parameter C 2
June 2006.

[5] C. O. Font, “Understanding the atmospheric turbulence structure parameter C 2

n in the littoral regime,”

Master’s thesis, University of Puerto Rico at Mayag¨uez, 2006.

[6] N. E. Huang, Z. Shen, S. R. Long, M. C. Wu, H. H. Shih, Q. Zheng, N.-C. Yen, C. C. Tung, and H. H.
Liu, “The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time
series analysis,” Proc. R. Soc. Lond. Ser. A 454, pp. 903–995, 1998.

[7] P. Flandrin, G. Rilling and P. Gon calves, “Empirical mode decomposition as a ﬁlter bank,” Signal Processing

[8] M. Turk and A. Pentland, “Eigenfaces for recognition,” Journal of Cognitive Neuroscience 3, pp. 71–86,

Letters, IEEE 22, pp. 112–114, 2004.

1991.

[9] L. Sirovich and M. Kirby, “Low–dimensional procedure for the characterization of human faces,” Journal of

the Optical Society of America A 4, pp. 519–524, 1987.

[10] J. W. Goodman, Statistical Optics, Wiley–Interscience, New York, 1996.
[11] B. Mandelbrot, The Fractal Geometry of Nature, W. H. Freeman and Co., New York, 1983.
[12] A. J. Roberts and A. Cronin, “Unbiased estimation of multi–fractal dimensions of ﬁnite data sets,” Physica

A 233, pp. 867–878, 1996.

[13] H. E. Stanley, L. A. N. Amaral, A. L. Goldberger, S. Havlin, P. C. Ivanov, and C.-K. Peng, “Statistical

physics and physiology: Monofractal and multifractal approaches,” Physica A 270, pp. 309–324, 1999.

[14] M. P. J. L. Chang, E. A. Roura, C. O. Font, E. Oh, and C. Gilbreath, “Applying the Hilbert-Huang
n data,” in Advances in Stellar Interferometry, these

Decomposition to horizontal light propagation C 2
proceedings, Proc. SPIE 6268, 2006.

[15] J. Beran, Statistics for Long–Memory Processes, Chapman and Hall, New York, 1994.


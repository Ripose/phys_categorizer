6
0
0
2
 
c
e
D
 
0
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
9
1
2
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Mutual Information as a Tool for Identifying Phase Transitions in Dynamical
Complex Systems With Limited Data

R. T. Wicks, S. C. Chapman, and R. O. Dendy∗
Centre for Fusion, Space and Astrophysics, University of Warwick, Coventry, CV4 7AL, UK.
(Dated: February 21, 2014)

We use a well known model (T. Vicsek et al. Phys Rev Lett 15, 1226 (1995)) for ﬂocking
to test mutual information as a tool for detecting order-disorder transitions, in particular when
observations of the system are limited. We show that mutual information is a sensitive indicator of
the phase transition location, in terms of the natural dimensionless parameters of the system which
we have identiﬁed. When only a few particles are tracked, and when only a subset of the positional
and velocity components are available, mutual information provides a better measure of the phase
transition location than the susceptibility of the data.

PACS numbers: 05.45.Tp, 05.70.Fh, 89.70.+c, 89.75.-k

I.

INTRODUCTION

Order-disorder transitions are often found in complex
systems. They have been identiﬁed in physical systems
such as Bose-Einstein condensates and ferromagnets and
in biological, chemical and ﬁnancial systems. Phase tran-
sitions are found, for example, in the behaviour of bac-
teria [1], locusts [2], voting games and utilisation of re-
source in markets [3]. These systems have in common the
property that there is competition between ﬂuctuations
driving the system towards disorder, and inter-element
Insight
interactions driving the system towards order.
into such systems can be gained using simple models.
Although the dynamics of individual elements are diﬃ-
cult to predict, one can identify macroscopic parameters
that characterise the behaviour of the system. These can
be approached through dimensional analysis, e.g. Buck-
ingham’s Π theorem [4].

A generic challenge in real world measurements of
physical, chemical, biological or economic systems is that
they yield datasets that are, in essence, sparse. Single el-
ements such as tracer particles in turbulent ﬂow, tagged
birds or dolphins in a group or a constituent of a ﬁnan-
cial index may, or may not, adequately sample the full
In consequence, the be-
underlying system behaviour.
haviour of a ﬁnite number of individual elements may, or
may not, provide a proxy for the behaviour of the entire
system. If the system behaviour is known to exhibit a
phase transition, the question arises how this can best
be captured from analysis of the dynamics of individual
elements. Previously, for example, both mutual infor-
mation (MI) [5] [6] [7] [8] and susceptibility have been
shown to be sensitive to the phase transition in the Ising
spin model of ferromagnetism [9]. MI can also extract
correlation, or dependence, between causally linked but
spatiotemporally separated observed parameters: for ex-
ample, between in-situ plasma measurements in the solar

∗Also at UKAEA Culham Division, Culham Science Centre, Abing-
don, Oxfordshire, OX14 3DB, UK.

wind, and the ionospheric response detected by ground
based measurements on Earth [10]; and within the brains
of Alzheimer’s disease patients [11].

Here we compare the use of MI and susceptibility to
quantify the location of the phase transition in the dimen-
sionless parameter space of the Vicsek model [12]. We
ﬁnd that, when full knowledge of the system is available,
that is when all the particles are tracked, the suscepti-
bility is an accurate method for estimating the position
of the phase transition in the Vicsek model. However, if
the data is limited to a sample of just a few particles out
of a large number or a subset of the complete data, this
method is less accurate.

We show that the mutual information of only a few
particles or of limited data from the whole system, can
successfully locate the phase transition in dimensionless
parameter space. For example we ﬁnd that the MI of a
timeseries of components of particle position or velocity is
suﬃcient. We thus show that MI can provide a practical
method to detect order-disorder transitions when only a
few particles, or elements, of the system are observed.

II. THE VICSEK MODEL

In 1995 Vicsek et al. [12] introduced the self propelled
particle model in which particles have a constant speed
|v| = v0 and a varying direction of motion θ.
In the
discrete time interval δt = tn+1 − tn an isolated particle
increments its vector position xn → xn+1 by moving with
constant speed v0 in a direction θn which is in turn incre-
mented at each timestep θn+1 = θn + δθn. The random
ﬂuctuation δθn is an independent identically distributed
angle in the range −η ≤ δθn ≤ η; thus η characterises
the strength of the noise for the system.

In the model, particles interact when they are within
distance R of each other, such that the direction of their
motion tends to become oriented with that of their neigh-
bours. This interaction is modelled, as shown in Fig 1,
by replacing the particle’s direction of motion by the av-
erage of those particles NR within distance R, so that
θn+1 = hθNR
n i with a random angle δθn also added. Thus

2

0
0

10

20

30

40

50

0
0

10

20

30

40

50

50

40

30

20

10

50

40

30

20

10

50

40

30

20

10

0
0

10

20

30

40

50

FIG. 2: The eﬀect of increasing noise on a typical Vicsek
system from ordered dynamics (top: η = 0) to disordered
dynamics (bottom: η = 4π/5), and in the vicinity of the phase
transition (middle: η = f 2π/5). Particle velocity vectors are
plotted as arrows at the position of each particle in the x-y
plane. The system has parameters N = 3000, |v| = 0.15,
R = 0.5. This corresponds to Π2 = 0.3, Π3 = 0.94 and
Π1 = η, see equations 6-8.

FIG. 1: Multiple particles interact if within a radius R of
each other. Each of the NR particles within R (here NR = 4)
NR
contributes its angle of propagation to the average hθ
n i,
which is assigned to the particle at the centre of R.

for the ith particle in the system, after n timesteps:

xi
n+1 = xi
n+1 = hθNR
θi
vi
n = v0

n + vi
n δt
n i + δθi
n
cos θi
(cid:0)

n ˆx + sin θi

n ˆy
(cid:1)

(1)

(2)

(3)

Here, direction is deﬁned by the angle from the x axis
(ˆx), and η is such that η = ηδt, that is normalised to the
time step δt.

There are two limiting cases for the system dynam-
ics: disorder, where all particles execute random walks;
and order, where all particles move together with the
same velocity. Figure 2 shows snapshots of the system
dynamics for η = 0, η = 2π/5 ≃ 1 and η = 4π/5 > 1. We
can see that η ≪ 1 is highly ordered and η ≫ 1 is highly
disordered, and around η ≃ 1 there is a phase transition
[13]. As with other critical systems it is possible to de-
ﬁne an order parameter φ and a susceptibility χ [12] [14]
[15] [16]. For the Vicsek model, the magnitude of the av-
erage velocity of all the particles in the system provides
a macroscopic order parameter and the variance of this
speed is the susceptibility:

N

φ =

1
N v0 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1
χ = σ2(φ) =

vi

1
N

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

hφ2i − hφi2
(cid:0)

(cid:1)

(4)

(5)

Here N denotes the total number of particles in an im-
plementation of the model of Vicsek et al..

We plot φ and χ as a function of η in Fig 3. In the
thermodynamic limit (N → ∞, l → ∞) where l is the
system size, the susceptibility would tend to inﬁnity at
the critical noise ηc, where the phase transition occurs. In
a ﬁnite sized realisation of system, the susceptibility has
a sharp but ﬁnite maximum at the critical noise at which
the phase transition occurs. Finite size eﬀects make the
peak location uncertain, but it is still possible to obtain
an estimate of the critical noise ηc.

φ

0.7

r
e
t
e
m
a
r
a
P
r
e
d
r
O

1

0.9

0.8

0.6

0.5

0.4

0.3

0.2

0.1

0
0
0

3

−4

x 10

2.5

1.5

2

1

χ
y
t
i
l
i

b
i
t
p
e
c
s
u
S

0.5

0
4
4

III. SYSTEM PHASE SPACE

For given values of Π2 and Π3, we run simulations of
the Vicsek system for a range of values of Π1 to deter-
mine the value Π1 = Πc
1 at which the susceptibility χ
peaks and thus the phase transition occurs. By repeat-
ing this operation for a set of parameter values of Π2
and Π3, we obtain the full set of coordinates at which
the phase transition is located for a region of the phase
space around Π3 = 1. We show this graphically in Fig 4
where we plot contours of Πc
3(Π1, Π2) in the upper panel,
and Πc
2(Π1, Π3), in the lower panel. These plots conﬁrm
that there is a smooth, well deﬁned surface of Πc
2, Πc
3;
they can be used to inform the choice of Π1, Π2 and Π3
for the next section.

1, Πc

0.5

0.4

0.3

0.2

0.1

1.5

2

1

0.5

2
Π

3
Π

0.5
0.5

1
1

1.5
1.5

2.5
2.5

3
3

3.5
3.5

2
2
Noise η

FIG. 3: An example of a typical Vicsek system. The order
parameter φ (line) is maximum for zero noise and falls to
a constant small value at high noise. The susceptibility χ
(crosses) peaks at the critical point ηc ≈ 1.33 for the system.
The system parameters are: Π2 = 0.3 and Π3 = 0.94, with
N = 3000.

The system can be analysed using Buckingham’s Π
theorem [4], and three independent dimensionless quan-
tities can be formed that characterise its behaviour. The
ﬁrst of these (Π1) is the amplitude of the normalised noise
η, the second (Π2) is the ratio of the distance travelled
in one timestep v0δt to the interaction radius R, and the
third (Π3) is the average number of particles contained
within a circle of one interaction radius R:

Π1 = η = ηδt
Π2 = v0δt/R
Π3 = πR2ρ

(6)
(7)
(8)

These three parameters determine the behaviour of the
system in the thermodynamic limit (N → ∞, l → ∞,
R and ρ ﬁnite) where ρ denotes the number density of
particles over the whole system.

The system size l aﬀects the number of interactions
that occur. If l is ﬁnite and the system is periodic as here,
the ﬁnite system size increases the chance of two ran-
domly chosen particles interacting, compared to the limit
of inﬁnite l. The system only approaches the thermody-
namic limit when the ﬁnite interaction radius R ≪ l.
Conversely, for example, if the interaction radius is half
the diagonal size of the system, then all the particles
interact with each other at any given moment. This im-
plies a fourth parameter reﬂecting the ﬁnite size of any
computer based realisation of this model:

Π4 = R/l

(9)

0

0.5

1

1.5

2

2.5

Π1

Π1

0

0.5

1

1.5

2

2.5

FIG. 4: Phase transition diagram contours for the Vicsek
model around Π3 = 1. Top panel: the eﬀect of changing
Π3, from Π3 = 0.2 (dark blue contours, left hand side) to
Π3 = 2.0 (dark red contours, right hand side) in steps of 0.2,
on the position of the phase transition in the Π1, Π2 plane.
Bottom panel: the eﬀect of changing Π2, from Π2 = 0.05
(dark blue, left hand side) to Π2 = 0.5 (dark red, right hand
side) in steps of 0.05, on the position of the phase transition
in the Π1, Π3 plane. Colour plots available online

IV. MUTUAL INFORMATION

Mutual information (MI) quantiﬁes the information
content shared by two signals A and B. For discrete
signals we can write the MI as

In the thermodynamic limit we have N → ∞, l → ∞,
whilst R and ρ = N/l2 are ﬁnite, so that Π4 → 0 and
Π1−3 are ﬁnite and specify the system.

I(A, B) =

m

Xi,j

P (ai, bj) log2 (cid:18)

P (ai, bj)
P (ai)P (bj) (cid:19)

(10)

Here the signal A has been partitioned into an al-
phabet (a library of possible values the signal can take)
A = {a1, . . . , ai, . . . am} where a1 and am are the extrema
of A found in all data considered. The discretized signal
takes value ai with probability P (ai) and similarly for
bi we have P (bi), while P (ai, bj) is the joint probability
of ai and bj occurring together. The chosen base of the
logarithm deﬁnes the units in which the mutual informa-
tion is measured. Normally base two is used, so that the
mutual information is measured in bits. If we deﬁne the
entropy of a signal as

H(A) = −

P (ai) log2(P (ai))

(11)

m

Xi

then MI can be written as a combination of entropies

H(A) [5]:

I(A, B) = H(A) + H(B) − H(A, B)

(12)

The calculation of the entropies needed to form the
MI is not trivial, as there is some freedom in the method
of discretization of the signals and in the method used
to estimate the probabilities P (ai), P (bj) and P (ai, bj).
There are many diﬀerent methods currently used, sum-
marised and compared by Cellucci et al. [6] and Kraskov
et al. [7].

MI has been used in the analysis of the two dimen-
sional Ising model by Matsuda et al.
[9]. Importantly
the critical temperature for the Ising model is picked out
precisely by the peak in the mutual information of the
whole system. This peak survives the coarse graining
of the system very well, which raises the possibility of
mutual information being useful in the study of other
complex systems.

V.

IDENTIFYING THE PHASE TRANSITION

A. Full System Mutual Information

In the 2D Vicsek system there are three variables for
each of the N particles: their positions (xi, yi) and the
orientation of their velocities θi, giving three signals X,
Y and Θ each containing N measurements at every time
step. The simplest discretization of these signals xi, yi,
θi is to cover the range of the signals with equally spaced
bins, so for position coordinate X we have m bins Xi
with width δX. Then if n particles are in the range
(Xi − Xi + δX) we have probabilities:

P (Xi) =

n(Xi)
N δX
P (Xi) = 1

Xi

(13)

(14)

The
single and joint probabilities P (Yj ), P (Θk),
P (Xi, Θk) and P (Yj , Θk) are calculated in a similar man-
ner.

4

The key factor governing the accuracy with which
MI is measured is to optimise the size of the bins used
in the above procedure. If the bins are too large then
resolution is lost, and the exact level of small scale struc-
ture and clustering cannot be identiﬁed. If the bins are
too small then at high noise the probability of ﬁnding a
particle at a given point does not become smoothed over
the whole system because individual particles can be re-
solved, giving P (xi, yj) 6= P (xi)P (yj) even though the
system is in a well mixed random state.

There is no ideal bin structure yet determined for
this method of MI calculation [6] [7], the Vicsek model
has two natural length scales, R the interaction radius
and l the box size. Therefore the only sensible length
scale to choose for discretization, when a snapshot of the
whole system is being used, is the interaction radius R.
Thus all mutual information calculations made on the
whole system are made with a bin size of 2R, the diame-
ter of the circle of interaction and the bins are therefore
squares of size 4R in the (x, y) plane. When θ is dis-
cretized the same number of bins are used as for x or y
as there is no natural size for bins in θ.

Given full knowledge of xi, yi, θi for all N particles
in the system over a large number of timesteps there are
several diﬀerent calculations of mutual information that
can be made. It was found that the most accurate form
of mutual information for the whole system is that cal-
culated between x or y position and θ. Thus we perform
the following calculation at each time step n once the
system has reached a stable state:

I(X, Θ) =

P (Xi, Θj) log2 (cid:18)

Xi,j

I(Y, Θ) =

P (Yi, Θj) log2 (cid:18)

Xi,j
I(X, Θ) + I(Y, Θ)
2

I =

P (Xi, Θj)
P (Xi)P (Θj) (cid:19)

P (Yi, Θj)
P (Yi)P (Θj) (cid:19)

(15)

(16)

(17)

and average over all timesteps for which MI is mea-

sured.

We compare the MI as calculated using the above
method and the susceptibility as a function of normalised
noise η in Fig 5. We see that at large η the MI falls to
zero as X, Y and Θ tend to uncorrelated noise (see also
[9]). We would also expect the MI to fall to zero at suf-
ﬁciently low η as the system becomes ordered and this
behaviour is also seen within the errors. The errors on
our measurements of MI are calculated from the standard
deviation of measurements of MI calculated over 50 sim-
ulations at each noise η. The error on the susceptibility
is calculated in the same manner.

The error bars become larger at low η because the
mutual
information includes the signatures of spatial
clustering as well as velocity clustering in the measure-
ment. Thus at low η, when extended clusters form, the
mutual information will give a higher value for the more
spatially extended axis of the cluster and a lower value

5

(18)

(19)

(20)

ηc

0.12

0.1

0.08

0.06

0.04

0.02

)
s
t
i
b
(

I

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M

−4

x 10

χ
y
t
i
l
i

b
i
t
p
e
c
s
u
S

2.5

1.5

2

1

0.5

0
4
4

into S sections, labelled s = 1, . . . , S of length Ns = τ /S
steps. This gives us nS pseudo-systems, relying on the
assumption that one particle over Ns steps is equivalent
to Ns particles at one step. This is a reasonable assump-
tion to make for the Vicsek model as it is ergodic while
η remains constant.

To calculate the susceptibility, we need to estimate
the variance of the average velocity of each of these nS
pseudo-systems. We therefore cut each section s into S′
further subsections s′, calculate the average velocity φi
s′
of these subsections and ﬁnd their variance, giving χi
s
the pseudo-system susceptibility. This is done for each
pseudo-system individually to give χi
s, and averaged over
all nS pseudo-systems to give χ, the average variance of
the average velocity for all pseudo-systems:

0
0
0

0.5
0.5

1
1

1.5
1.5

2.5
2.5

3
3

3.5
3.5

2
2
Noise η

FIG. 5: The mutual information, I (circles) peaks at approx-
imately the same point as the susceptibility, χ (crosses) and
with the critical noise ηc ≈ 1.33 marked. The system param-
eters are Π2 = 0.15 and Π3 = 0.98, with N = 3000 particles.

for the less extended axis of the cluster. This implies the
shape and orientation of the (usually single) large cluster
formed at low noise inﬂuences the mutual information.
Diﬀerent measurements of MI thus arise for each imple-
mentation of the model, giving rise to the error seen at
low η. This could be corrected by using other approaches
to computing MI, for example recurrence plotting [8] [10],
or a diﬀerent distribution of bins, these are more compu-
tationally intensive however.

When estimated as the standard deviation over 50
repeated runs of the simulation, the error is found to
be considerably larger, as a fraction of the overall mea-
surement, for the susceptibility than for the MI. This is
because the susceptibility is simply an average ﬂuctua-
tion over all the velocity vectors of the system; whereas
the MI also directly reﬂects the level of spatial ’clumpi-
ness’ (that is, spatial correlation) of the particles. The
detailed spatial distribution varies from one simulation
to the next, but at ﬁxed Π1−4 the degree of ’clumpiness’
does not. Mutual information is able to quantify cluster-
ing (correlation in space as well as velocity) in a simple
dynamical complex system, in a manner that identiﬁes
the order-disorder phase transition.

B. Mutual Information from Limited Data

Observations of many real world systems typically pro-
vide only a subset of the full system information, in the
present case the dynamics of all N interacting particles.
We now consider results from the Vicsek model using
only very limited amounts of data. The mutual informa-
tion and susceptibility are now calculated on a τ = 5000
step timeseries of positional and velocity data for n = 10
particles out of the N = 3000 simulated. To optimise
both methods, the data for each particle timeseries is cut

φi
s′ =

χi

s =

χ =

SS′
τ v0

1
S′
1
nS

′

Xk=1

τ /SS
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
hφ2
(cid:12)
(cid:0)

vi
k(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s′ i − hφs′ i2
(cid:12)

S

n

(cid:1)

χi
s

Xs=1

Xi=1

The result is shown in Fig 6 where we also plot the
mutual information calculated between the same vari-
ables as previously, I(X, Θ) but now as nS timeseries;
the parameters used are n = 10, S = 10, S′ = 10. The
error bars are calculated as the standard deviation of
the 100 measurements made using the diﬀerent pseudo-
systems of length τ /S = 500 timesteps. These values
for n, S and S′ are chosen so as to limit the data in a
realistic way. n = 10 is a suitably small subset of the
N = 3000 particles, S = 10 cuts the data into segments
suﬃciently long (500 timesteps) to be treated indepen-
dently. S′ = 10 is chosen so that each section s′ is still
long enough (50 timesteps) to make as good an estimate
of the average velocities φi
s′ as possible, but allows enough
of these measurements to be made to reduce the error on
the measurement of χi
s.

The system is identical to that shown in Fig 5 and
the phase transition is at the same noise, ηc ≈ 1.33. Near
their respective peaks, the error in the mutual informa-
tion remains smaller than that in the susceptibility and
so MI better identiﬁes the peak. The peak in the sus-
ceptibility no longer coincides with ηc and is shifted to
the higher noise side of the phase transition. This occurs
because the susceptibility is now measured on too small
a sample of data, 50 angles θi
t are averaged to ﬁnd the
average velocity φi
s′ . Such a small ensemble average re-
sults in a large deviation in the average velocity from the
expected value.

For comparison with a linear measure, we calculate
the cross correlation for our 10 trajectories. We choose
one of the particles at random and compute its cross
correlation with each of the remaining 9. The average

ηc

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

)
s
t
i
b
(

I

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M

1

0.75

0.5

0.25

θ

n

i

n
o
i
t
a
l
e
r
r
o
C

0

0

0.9

0.8

x

n

i

n
o
i
t
a
l
e
r
r
o
C

0.7

0.6

0.5
0

−0.2
0
0

0.5
0.5

1
1

1.5
1.5

2.5
2.5

3
3

3.5
3.5

−0.04
4
4

2
2
Noise η

FIG. 6: The mutual information I (circles) calculated using
timeseries from only ten particles for 5000 time steps, with
S = 10, compared to the average susceptibility χ (crosses) for
the same data using S′ = 10 subsections to calculate χ and
with the critical noise ηc ≈ 1.33 marked. System parameters
are Π2 = 0.3 and Π3 = 0.94, with N N = 3000 particles.

0.5

1

1.5

2.5

3

3.5

4

2
Noise η

0.5

1

1.5

2.5

3

3.5

4

2
Noise η

FIG. 7: The cross correlation between a randomly chosen
particle and nine others, calculated using a timeseries with
5000 steps. The top panel shows the average cross correlation
between θ1 and θn, 2 ≤ n ≤ 10. The bottom panel shows
the average cross correlation between x1 and xn, 2 ≤ n ≤ 10.
System parameters are Π2 = 0.3 and Π3 = 0.94, with N =
3000 particles.

of these is plotted in Fig 7. The average cross corre-
lation between angles θ1 and θN , 2 ≤ N ≤ 10 in the
top panel shows strong correlation at low noise, as ex-
pected. This cross correlation declines as noise increases,
but not smoothly, because the correlation depends on

6

(21)

(22)

(23)

(24)

(25)

0.36

0.32

0.28

0.24

0.2

0.16

0.12

0.08

0.04

0

χ
y
t
i
l
i

b
i
t
p
e
c
s
u
S

the exact dynamics of the particles considered. Angular
cross correlation reaches zero around the phase transi-
tion, but does not provide an accurate location for the
critical noise.
In the bottom panel of Fig 7 the cross
correlation between x1 and xN , 2 ≤ N ≤ 10 provides no
reliable indication of the position of the phase transition
either. The cross correlation does become more variable
on the higher noise side of the graph but this eﬀect can-
not be used to accurately ﬁnd the critical noise ηc.

The value of using mutual information can be seen
when the available data is restricted still further. Let
us consider signals derived from one component of the
particle trajectory only, equivalent to a line of sight mea-
surement. We then have one of the position coordinates
xi
k, and the instantaneous x component of the velocity,
∆xi
k). The susceptibility is calculated as in
equations (18) - (20) but using the average one dimen-
sional velocities ∆xi
k:

k = v0cos(θi

φi
s′ =

χi

s =

χ =

SS′
τ v0

1
S′
1
nS

′

Xk=1

τ /SS
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
hφ2
(cid:12)
(cid:0)

∆xi
k(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s′ i − hφs′ i2
(cid:12)

S

n

(cid:1)

χi
s

Xs=1

Xi=1

The mutual information is calculated for each section of
the x only components of the timeseries for each particle
using a suitable binning:

I(X, ∆X) =

P (Xi, ∆Xj) log2 (cid:18)

Xi,j

P (Xi, ∆Xj)
P (Xi)P (∆Xj ) (cid:19)

I(Y, ∆Y ) =

P (Yi, ∆Yj) log2 (cid:18)

Xi,j

P (Yi, ∆Yj)
P (Yi)P (∆Yj) (cid:19)

Figure 8 shows the mutual information calculated
from the data in this manner with S = 10. The peak in
the mutual information is at approximately the correct
value of η (ηc ≈ 1.33). Figure 9 shows for comparison the
susceptibility calculated over the X data as in equations
(21)-(23). We see that although there is a peak, it no
longer identiﬁes η → ηc accurately. The peak is broader
and has larger error bars than in Fig 8 giving a large
uncertainty in identifying ηc.

The peak in Fig 8 is shifted to the low noise side of
the phase transition and shows some scatter. This can be
understood by looking at the same data using a diﬀerent
value of the interval S. In Fig 10 we show the same data
analysed using S = 1, that is we consider one timeseries
of length 5000 time steps for each of ten particles and ob-
tain MI averaged over these ten. We plot I(X, ∆X) (cir-
cles) and I(Y, ∆Y ) (squares). The measurements overlap
within errors on the high noise side of the phase transi-
tion but separate into two distinct branches, containing
both I(X, ∆X) and I(Y, ∆Y ), on the low noise side.

−0.2
0

0.5

1

1.5

2.5

3

3.5

4

2
Noise η

FIG. 8: The mutual information I(X, ∆X) (circles) calcu-
lated using a timeseries from only ten particles for 5000
time steps, with S = 10 and the critical noise ηc ≈ 1.33
marked. System parameters are Π2 = 0.3 and Π3 = 0.94,
with N = 3000 particles.

ηc

ηc

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

)
s
t
i
b
(

)

X
∆

,

X

(
I

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M

0.22

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

y
t
i
l
i

b
i
t
p
e
c
s
u
S

7

high peaks on the low noise side of the phase transition,
making it appear to be shifted towards η = 0.

Finally in Fig 11 we plot the minimum of I(X, ∆X)
and I(Y, ∆Y ) from Fig 10 and see that a clear peak
emerges at η = etac, where the error bars are smallest.
This outcome obviates the diﬃculty that arises if we only
allow knowledge of {X, ∆X} for example, when it would
be necessary to exclude high measurements of MI at low
noise, as discussed above.

ηc

0.5

1

1.5

2.5

3

3.5

4

2
Noise η

FIG. 10: The mutual information I(X, ∆X) (circles) and
I(Y, ∆Y ) (squares) calculated using a timeseries from only
ten particles for 5000 steps, with S = 1 and the critical noise
ηc ≈ 1.33 marked. System parameters are Π2 = 0.3 and
Π3 = 0.94, with N = 3000 particles.

)
s
t
i
b
(

)
Y
∆

,

Y
(
I

d
n
a

)

X
∆

,

X

(
I

n
o
i
t
a
m
r
o
f
n
I

l
a
u
t
u
M

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

)
s
t
i
b
(

)
)
Y
∆

,

Y
(
I
,
)

X
∆

,

X

(
I
(
n
M

i

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
0

−0.02
0

0.5

1

1.5

2.5

3

3.5

4

ηc

2
Noise η

FIG. 9: The susceptibility χ (crosses) calculated using a time-
series of one dimensional data {X, ∆X} from only ten parti-
cles for 5000 time steps, with S = 10 and the critical noise
ηc ≈ 1.33 marked. System parameters are Π2 = 0.3 and
Π3 = 0.94, with N = 3000 particles.

One potential source of this behaviour is that, as the
system becomes ordered at η < ηc, the particles clump
together and take on a preferred direction of motion. The
eﬀectiveness of MI will then depend on whether our single
component (line of sight) data is aligned along, or perpen-
dicular to, the preferred direction. Along the preferred
direction the mutual information is reduced by the low
dispersion of particle positions and velocities, whereas
along the perpendicular direction the MI and positional
dispersion are increased because the particles have higher
velocity and positional dispersion relative to this direc-
tion. The anomalously high MI measurements that result
from these large relative velocities lead to anomalously

0.5

1

1.5

2

2.5

3

3.5

4

Noise η

FIG. 11: The minimum results from mutual information mea-
surements I(X, ∆X) and I(Y, ∆Y ) calculated using a time-
series from only ten particles for 5000 steps, with S = 1 and
the critical noise ηc ≈ 1.33 marked. System parameters are
Π2 = 0.3 and Π3 = 0.94, with N = 3000 particles.

VI. CONCLUSIONS

The Vicsek model is used here to test the potential of
measurements of order and clustering that exploit mutual
information in dynamic complex systems. It is found that
when complete knowledge of the system is available, the
mutual information has a smaller error than the suscepti-
bility (Fig 5). Using Buckingham’s Π theorem the set of
dimensionless parameters that capture the phase space
of the Vicsek model have been presented as a complete
set for the ﬁrst time.

When data is limited to observations of only ten par-
ticles out of 3000 the error in the mutual information re-
mains comparatively small, and the mutual information
thus provides a better measurement than susceptibility
of the position of the order-disorder phase transition (Fig
6). When data is limited still further, such that only one
line of sight component of the particle motion is avail-
able, the mutual information measurement remains sen-
sitive enough to calculate the critical noise of the phase
transition, while the susceptibility does not (Fig 8 - 11).
In this case the mutual information also provides an
indication of the axial direction of clumped particle mo-
tion at low noise. Anomalously high mutual information
estimates in this ordered phase indicate that the par-
ticles measured are mostly moving along the dimension

8

being measured; low estimates indicate that the particles
are moving perpendicular. This is remarkable given that
susceptibility does not contain this information and that
the MI is a probabilistic measurement.

Real world data are often in the form of the ﬁnal
data studied here; a limited sample from a larger set,
measured in fewer dimensions than those of the original
system: for example, line of sight measurements of wind
speed measured by an anemometer at a weather station,
or satellite measurements of the solar wind. It has been
information can provide an
shown here that mutual
eﬀective measure of the onset of order, and may provide
a viable technique for real world data with its inherent
constraints.

Acknowledgements

This work was supported in part by the EPSRC. RW
acknowledges a PPARC CASE PhD studentship in asso-
ciation with UKAEA. The authors would like to thank
Khurom Kiyani for valuable discussions and the Centre
for Scientiﬁc Computing at the University of Warwick for
providing the computing facilities.

[1] A. Czir´ok, E. Ben-Jacob, I. Cohen, and T. Vicsek. For-
mation of complex bacterial colonies via self-generated
vortices, Phys. Rev. E. 54, 1791 (1996).

[2] J. Buhl, D. J. T. Sumpter, I. D. Couzin, J. J. Hale, E.
Despland, E. R. Miller and S. J. Simpson. From disor-
der to order in marching locusts, Science, 312, 1402-1406
(2006).

[3] R. Savit, R. Manuca, R Riolo. Adaptive competition,
market eﬃciency, and phase transitions, Phys. Rev. Lett.
82, 2203 (1999).

[4] Malcolm Longair. Theoretical Concepts in Physics; an
alternative view of theoretical reasoning in physics, 2nd
Ed. Cambridge Univ. Press (2003).

[5] C. E. Shannon. A Mathematical Theory of Communica-
tion Bell System Tech. Journal 27 379-423 (1948).
[6] C. J. Cellucci, A. M. Albano, P. E. Rapp. Statistical
validation of mutual information calculations: Compari-
son of alternative numerical algorithms Phys. Rev. E 71
066208 (2005)

[7] A. Kraskov, H. St¨ogbauer, P. Grassberger. Estimating
mutual information Phys. Rev. E 69 066138 (2004)
[8] T. K. March, S. C. Chapman, and R. O. Dendy. Recur-
rence plot statistics and the eﬀect of embedding Physica
D, 200, 171-184 (2005)

[9] H. Matsuda, K. Kudo, R. Nakamura, O. Yamakawa, T.
Murata. Mutual information of Ising systems, Int. Jour-
nal of Theor. Phys. 35, 839 (1996).

[10] T. K. March, S. C. Chapman, R. O. Dendy. Mutual infor-
mation between geomagnetic indices and the solar wind
as seen by WIND: Implications for propagation time es-
timates, Geophys. Res. Lett. 32, L04101 (2005).

[11] J. Jeong, J C. Gore, B. S. Peterson. Mutual information
analysis of the EEG in patients with Alzheimer’s disease,
Clinical Neurophysiology 112 827-835 (2001).

[12] T. Vicsek, A. Czir´ok, E. Ben-Jacob, I. Cohen and O.
Shochet. Novel type of phase transition in a system of
self-driven particles, Phys. Rev. Lett. 75, 1226 (1995).

[13] G. Gr´egoire and H. Chat´e. Onset of collective and cohe-

sive motion, Phys. Rev. Lett.92 025702 (2004).

[14] G. Gr´egoire, H. Chat´e and Y. Tu. Moving and staying
together without a leader, Physica. D,181, no3-4, 157-170
(2003).

[15] A. Czir´ok, H. E. Stanley and T. Vicsek. Spontaneously or-
dered motion of self-propelled partices, J. Phys. A: Math.
Gen. 30 1375-1385 (1997).

[16] A. Czir´ok and T. Vicsek. Collective behaviour of interact-
ing self-propelled particles Physica A 281 17-29 (2000).


3
0
0
2
 
g
u
A
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
5
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

APS preprint

On Statistical Methods of Parameter Estimation
for Deterministically Chaotic Time-Series

V.F. Pisarenko1 and D. Sornette2, 3, 4
1International Institute of Earthquake Prediction Theory and Mathematical Geophysics,
Russian Ac. Sci. Warshavskoye sh., 79, kor. 2, Moscow 113556, Russia
2Institute of Geophysics and Planetary Physics, University of California, Los Angeles, CA 90095
3Department of Earth and Space Sciences, University of California, Los Angeles, CA 90095
4Laboratoire de Physique de la Mati`ere Condens´ee,
CNRS UMR 6622 and Universit´e de Nice-Sophia Antipolis, 06108 Nice Cedex 2, France∗
(Dated: February 2, 2008)

We discuss the possibility of applying some standard statistical methods (the least square method,
the maximum likelihood method, the method of statistical moments for estimation of parameters)
to deterministically chaotic low-dimensional dynamic system (the logistic map) containing an obser-
vational noise. A “pure” Maximum Likelihood (ML) method is suggested to estimate the structural
parameter of the logistic map along with the initial value x1 considered as an additional unknown
parameter. Comparisons with previously proposed techniques on simulated numerical examples
give favorable results (at least, for the investigated combinations of sample size N and noise level).
Besides, unlike some suggested techniques, our method does not require the a priori knowledge of
the noise variance. We also clarify the nature of the inherent diﬃculties in the statistical analysis
of deterministically chaotic time series and the status of previously proposed Bayesian approaches.
We note the trade-oﬀ between the need of using a large number of data points in the ML analysis to
decrease the bias (to guarantee consistency of the estimation) and the unstable nature of dynamical
trajectories with exponentially fast loss of memory of the initial condition. The method of statistical
moments for the estimation of the parameter of the logistic map is discussed. This method seems
to be the unique method whose consistency for deterministically chaotic time series is proved so far
theoretically (not only numerically).

PACS numbers:

The problem of characterizing and quantifying a noisy
nonlinear dynamical chaotic system from a ﬁnite realiza-
tion of a time series of measurements is full of diﬃculties.
The ﬁrst one is that one rarely has the luxury of know-
ing the underlying dynamics, i.e., one does not in general
know the underlying equations of evolution. Techniques
to reconstruct a parametric representation of the time
series then may lead to so-called model errors.

Even in the rare situations where one can ascertain
that the measurements correspond to a known set of
equations with additive noise, the chaotic nature of the
dynamics makes the estimation of the model parameters
from time series surprisingly diﬃcult. This is true even
for low-dimensional systems, another even rarer instance
in naturally occurring time series.

Here, we revisit the problem proposed by McSharry
and Smith [1], who introduced an improved method over
standard least-square ﬁts to estimate the structural pa-
rameter of a low-dimensional deterministically chaotic
system (the logistic map). We discuss the caveats un-
derlying this problem, propose a “pure” Maximum Like-
lihood method that we compare with previously proposed
methods. Our conclusion stresses the inherent diﬃculties
in formulating a bona ﬁde statistical theory of structural
parameter estimations for noisy deterministic chaos.

I. DEFINITION AND NATURE OF THE
PROBLEM

Let us consider the supposedly simple problem consid-
ered by McSharry and Smith [1], in which one measures
the sample s1, ..., sN with

si = xi + ηi

(1)

(2)

where the underlying dynamical one-dimensional discrete
recurrence equation

xi+1 = F (xi, a) ≡ 1 − ax2
i

is known and the ηi’s are Gaussian N (0, ǫ) iid random
variables with zero mean and standard deviation ǫ. The
problem is to determine the model parameter a from the
measurements s1, ..., sN , knowing that (2) is the true dy-
namics.

At ﬁrst sight, this problem looks like a statistical es-
timation of an unknown structural parameter, given ob-
servational data. However, strictly speaking, this prob-
lem cannot be (even formally) refered to as a bona ﬁde
statistical problem in which the maximum likelihood
(ML) method can be proved to be asymptotically opti-
mal or even consistent. Indeed, the Likelihood Function
L(a, x1|s1, ..., sN ) reads

∗Electronic address: sornette@moho.ess.ucla.edu

ln L(a, x1|s1, ..., sN ) ∝ −N ln(ǫ)−

1
2ǫ2

si − F (i)(x1, a)
(cid:17)

(3)

i
X

(cid:16)

2

,

where F (i)(x1, a) is the i-th iteration of the logistic map
(2) with parameter a and initial value x1. The key point
of diﬃculty is that F (i)(x1, a) is a non-stationary function
(despite the fact that the dynamical system (2) has an in-
variant measure µ(x)). Standard statistical ML methods
are applicable either to functions not depending on i, or
depending on i in a periodic manner. For non-stationary
and non-periodic dependence of the function on i, no sta-
tistical theorem on optimal properties of MLE is a priori
applicable. Then, numerical simulations of examples are
not enough and should be complemented with proofs of
results stating what known mathematical statistics prop-
erties of ML or of Bayesian methods continue to apply to
(3). A ﬁrst taste of the diﬃculty of the problem is given
by an analysis of the behavior of the “one-step least-
square (LS) estimation” and of the “total least-square”
method, given in Appendix A. Appendix A shows that
least-square methods are biased and should be corrected
before comparing these to other methods, as done in [1].
In particular, Appendix A shows that it was a priori un-
fair or inappropriate to compare any estimate obtained
with a given method (such as the one advocated by Mc-
Sharry and Smith [1]) to uncorrected ML-estimates due
to the non-stationarity of the function; the appropriate
corrections can be obtained from the standard statistical
theory of conﬂuence analysis [2–4].

II. A “PURE” MAXIMUM LIKELIHOOD
APPROACH IN TERMS OF (a, x1)

Putting aside the question of a rigorous demonstration
of the consistency and asymptotic optimality of the MLE
method, let us come back to expression (3), which is the
straightforward translation of the iid Gaussian N (0, ǫ)
properties of the random variables ηi’s. It suggests that
the problem of estimating the structural parameter a can-
not actually be separated from estimating simultaneously
the initial value x1.

The MLE of (a, x1) amounts in this case to the mini-

mization of the sum:

si − F (i)(x1, a)
(cid:17)

2

,

i
X

(cid:16)

(4)

which looks superﬁcially as a standard non-linear least-
square sum. There is however one very important dis-
tinction, as we already pointed out above: the non-linear
function depends on the index i whereas, in the standard
least-square method, one has a sum of the type

(si − F (xi, a))2 ,

(5)

i
X

where the x1, ..., xN are assumed to be known.

For the parameters a for which the logistic map ex-
hibits the phenomenon of sensitivity upon the initial con-
dition, the direct minimization of (4) is not feasible di-
rectly. Indeed, if we disturb x1 by a small number δ, then

2

the ith iterations F (i)(x1, a) and F (i)(x1 + δ, a) diverge
asymptotically exponentially fast with i:
for instance,
with an accuracy δ = 10−15 and for a = 1.85, the diﬀer-
ence F (i)(x1, a) − F (i)(x1 + δ, a) becomes of order 1 for
i > 20. This implies that, in practice, we cannot calcu-
late with the necessary accuracy xi+1 = F (i)(x1, a) for
i > 20. To address this fundamental limitation, we pro-
pose to cut the sample s1, ..., sN into n1 portions of size
no more than n2 = 20, and to treat each portion sepa-
rately. This amounts to re-estimating a diﬀerent initial
condition for each such sub-series, which is a natural step
since the sensitivity upon initial conditions amounts to
losing the information on the speciﬁc value of the initial
condition.

Our numerical tests show that our MLE works well
(see below) by considering sub-series of size in the range
n2 = 4 − 25 (for the true value of a equal to the value
1.85 considered by by McSharry and Smith [1] that we
take as our benchmark for the sake of comparison). For
larger samples (say, N = 100), we recommend to cut
this sample into n1 subsamples of size n2 = 4 − 25, and
treat them separately. It is possible that we lose some
eﬃciency in treating subsamples separately, but a joint
estimation would require the maximization of the likeli-
hood with the common parameter a and several diﬀerent
initial value parameters. This procedure would lead to
a very diﬃcult numerical multivariate search problem as
any gradient method would fail due to the very irregular
structure of the likelihood function (see below and ﬁgure
1).

The procedure we propose is thus to cut the initial
time-series into n1 independent subsamples of size n2
in the range 4 − 25, and to average the resulting n1
a-estimates.
In order to determine the optimal value
of n1 for a ﬁxed N (say N = 100) and for the value
a = 1.85 investigated here, we calculate the standard de-
viation sdt(a) over the n1 subsamples as a function of n1.
We ﬁnd that, basically independently of the noise level
ǫ, the pair n1 = 25, n2 = 4 gives the smallest standard
deviation sdt(a).

We have implemented this approach and compared it
with the results obtained by the method proposed by
McSharry and Smith [1], as discussed in the next section.

III. ML VERSION OF MCSHARRY AND
SMITH [1] AND COMPARISONS

The main result of McSharry and Smith’s paper [1]
consists in their formulae (13,14) for their proposed ML
cost function. Their idea is to substitute in the ML cost
function the unknown invariant measure µa(x) of the dy-
namical system (2), for a given value of the parameter a,
for what should be a realization of the latent variables
xi’s. Notice that a should be varied in order to deter-
mine the maximum likelihood. In practice, the integral
over the unknown invariant measure µa(x) is replaced
by a sum over a model trajectory (which can be calcu-

lated since the model is assumed to be known) of length
τ ≫ N . Unfortunately, this most important step is not
conﬁrmed by any numerical results (see below).

Before continuing,

let us note that there is a mis-
take in the probability density function (pdf) and like-
lihood given by their equations (7-9). Using the intu-
ition that pairs (si, si+1) should be used in their equation
(5, 6) to track the deterministic relation between xi and
xi+1 = F (xi, a), we see that a single latent variable xi is
associated with each pair (si, si+1) since si is compared
with xi and si+1 with F (xi, a). Thus, each xi is used
only once when scanning all possible pairs (si, si+1), for
i = 1, ..., N −1 and in their ML cost function (13,14). Ac-
tually, the correct likelihood should use only once each
observed random variable si, not the latent variable xi.
Therefore, using pairs (si, si+1), McSharry and Smith
take into account each si, i = 2, ..., N − 1 twice, and the
end values s1, sN once. For N ≫ 2, their expression
(7) is approximately equals (up to the end terms) to the
square of the correct likelihood. Taking the logarithm in
their equation (13) gives approximately twice the correct
likelihood, which gives almost the same estimate as the
exact likelihood.

While this mistake has no serious consequences for the
numerical accuracy of their calculation for long time se-
ries N ≫ 2, it illustrates the diﬀerence between their con-
struction of the likelihood and our direct approach pre-
sented in the previous section. By writing the conditional
likelihood for a pair (si, si+1) under a latent variable xi,
and by averaging this conditional likelihood weighted by
the invariant measure µ(x|a), McSharry and Smith sug-
gest that, by doing so, they incorporate additional in-
formation on the system in question. If we had a usual
probability space, then such averaging would provide the
unconditional likelihood of the pair (si, si+1) but, for de-
terministically chaotic time series, the exact meaning of
this averaging is not clear. Another questionable step of
McSharry and Smith is to multiply these pairwise likeli-
hoods as if the pairs (si, si+1) were independent. If this
was so, this would indeed give the unconditional likeli-
hood for the data sample s1, ..., sN .

But, we deal here with “deterministic chaos” which
generates not truly random variables (see for instance
[5, 6] for discussions on the pseudo-randomness nature
of such time series). Besides, we have some more in-
formation about the structure of the system in ques-
tion. Namely, we suppose known the generating relation
(2). This relation contains everything and is, in princi-
ple, much more informative than the stationary invariant
measure µ(x|a) (which is akin to a one-point statistics
while (2) contains information on all higher-order point
statistics). Concretely, it is clear that the product of
pdf’s for each pair (si, si+1) and the resulting likelihood
depends solely on the ﬁrst initial value x1 since all sub-
sequent xi are deterministically determined recurrently.
This remark gives the likelihood function (3) in terms of
two unknown parameters (a, x1) to be estimated. This
leads indeed to consider the initial state variable x1 as an

3

mean(a) std(a)

noise
std 0.5

std 1

Ref.[1]

1.816
“pure” ML 1.841
1.764
“pure” ML 1.885

Ref.[1]

ˆǫ

q2

q1

q2 − q1
0.0714 1.630 1.925 0.295
0.0390 1.762 1.913 0.151 0.459
0.123 1.510 1.975 0.465
0.0467 1.781 1.959 0.178 0.766

TABLE I: Comparison between McSharry and Smith’s ML
method [1] and our “pure” ML method described in section
II over 1000 realizations of the system (2) with true value
a = 1.85 giving 1000 time series of length N = 100, each
them decorated with Gaussian noise with two diﬀerent stan-
dard deviations (0.5 and 1). q1 and q2 are the sample quan-
tiles at the 2.5% and 97.5% probability level, so that q2 − q1
gives the width of the 95% conﬁdence intervals. Our “pure”
ML method provides us with an estimation ˆǫ of the standard
deviation of the noise given in the last column.

unknown parameter to be estimated (along with a) from
the sample s1, ..., sN . The likelihood (3) provides a more
detailed form than obtained by averaging over the in-
variant measure µ(x|a). We can hope that our approach
would lead to a more eﬃcient estimate of a. McSharry
and Smith avoid the maximization with respect to x1 in
their likelihood (13,14) and replace it by an averaging
over a proxy of the invariant measure. It is doubtful that
such a step is warranted, not speaking of optimality, in
view of our numerical tests presented below.

We now compare our “pure” Maximum Likelihood ap-
proach in terms of (a, x1) proposed in section II with Mc-
Sharry and Smith’s ML method, using numerical tests.
We consider 1000 time series with N = 100 data points
and subdivide each of them into n1 = 25 sub-series of
n2 = 4 data points. We ﬁx the true a equal to 1.85 as
in [1] and study noises with standard deviations equal
to 0.5 and 1.0. Table I shows a signiﬁcant improvement
oﬀered by our “pure” ML method over McSharry and
Smith’s average ML, as least for the set of parameters
studied here.
It is not possible to guarantee that this
will be the case for all possible parameter values but we
believe our method can not be worse that McSharry and
Smith’s average ML. A diﬃculty that should be men-
tioned is that the chaotic nature of the dynamics and in
particular the sensitivity of the invariant measure with
respect to the control parameter a is reﬂected into an
ugly-looking log-Likelihood landscape shown in Figure 1,
with many competing valleys. Standard numerical meth-
ods like gradient or simplex are unapplicable. We have
used a systematic 2D-grid search. Other methods in the
ﬁeld of computational intelligence, such as stimulated an-
nealing and genetic algorithms, could also be used. The
sensitivity of the invariant measure with respect to the
control parameter a means that the invariant distribution
can bifurcate from an almost uniform distribution on the
interval [−a, 1] to a distribution consisting of three delta-
functions (this happens around a ≈ 1.75).

In addition to performing better, our “pure” ML ap-
proach does not depend on the noise level, in contrast

),N=20,a
Contour lines of Likelihood L(a,x
1

max

,
=1.85;x
max
1

=.9;

1

0.98

0.96

0.94

0.92

0.88

0.86

0.84

0.82

1

x

0.9

0.8

1.5

1.55

1.6

1.65

1.7

1.8

1.85

1.9

1.95

2

1.75
a

FIG. 1: Contour lines of the “pure” log-Likelihood given by
expression (3) for a given realization of N = 20 data points
generated with a starting value x1 = 0.9, a = 1.85 and noise
std equal to 1. The log-Likelihood landscape is similar to a
2D Brownian sheet (2D generalization of a random walk).

with the ML cost function (13,14) proposed by McSharry
and Smith [1]. This is an important advantage when the
true level of noise is not known (noise error). Our method
is insensitive to such noise error while we have found
examples where the optimal estimation of the structure
parameter a with McSharry and Smith’s method is ob-
tained for a value of the noise standard deviation diﬀerent
from the true value. In general, the true noise level is not
known and McSharry and Smith’s method does not ap-
ply in such situation. Our “pure” ML method actually
provides us with an estimation ˆǫ of the standard devi-
ation of the noise given in the last column of Table I.
These estimates have a small bias down (two ﬁtted pa-
rameters were taken into account), which may be due to
the fact that n1 is not suﬃciently large (n1 = 25; n2 = 4;
N = n1 × n2 = 100).

IV. DISCUSSION OF OTHER APPROACHES

Meyer and Christensen [7] have proposed to replace the
ad hoc construction of McSharry and Smith’s ML cost
function by a Bayesian approach, assuming noninforma-
tive priors for the structural parameter a, for the ini-
tial value x1 and for the standard deviation of the noise.
Their approach improves signiﬁcantly on McSharry and
Smith [1] by recognizing the role of x1 but turns out to be
incorrect, as shown by Judd [8], because their approach
amounts to assuming a stochastic model, thus refering to
quite another problem.

Based on the formulation of [9], Judd [8] develops a
formulation which is almost identical to our “pure” ML
(3) but there are important distinctions. Similarly to us,
Judd introduces x1 but he does not employ it. He prefers

4

to eliminate the dependency on x1 by averaging this pa-
rameter with a ﬁducial distribution (see e.g. [4], Chapter
21, Interval Estimation, Fiducial Intervals). Judd incor-
rectly calls the method based on his equations (4,5) a ML
method. In fact, his equations (4,5) gives a a hybrid of
ML, Bayesian and so-called ﬁducial methods. It is a ML
method with respect to the structural parameter a. It is
Bayesian with respect to the initial value x1. It is ﬁducial
since it does not assume any a-priori density for x1, but
uses a prior density function ρ(s1−w) (using the notation
above) that is in fact a Gaussian density of the noise with
mean value equal to the unknown initial value s1. Using
such density is equivalent to weighting a two-parameter
likelihood by weights corresponding to diﬀerent values of
noise disturbances. Thus, the averaged likelihood (5) in
[8] describes an ensemble of diﬀerent noise disturbances
of an unknown initial value s1. This provides a (rea-
sonable but not optimal) method of elimination of the
second parameter x1 from the maximization procedure.
It is neither a pure Bayesian method (that would assume
explicitly some a-priori density for s1 which could be ar-
bitrary, and not necessarily equal to ρ(s1 − w)), nor a
ML method for two unknown parameters as we suggested
above in section II.

In this context in view of the emphasis on Bayesian
methods to solve this problem [7, 8], it is perhaps use-
ful to stress that the probability theory rule P {A, B} =
P {A|B} P {B} is often freely called “the Bayes rule.”
This is why the averaging of likelihoods over conditional
state variables can be called Bayesian approaches, al-
though this is not quite correct since the latent (state)
variables are not random values in the standard meaning
of this notion (as it is assumed by McSharry and Smith),
although the state variables have a limit invariant mea-
sure, as we said above. The Bayesian approach assumes
that parameters are random values. For instance, Mc-
Sharry and Smith assume that the latent (state) variables
are random variables, which is not quite so, although the
state variables have a limit invariant measure, as we said
above. We stressed already that the series of state vari-
ables can be considered as a degenerate set of random
values that are determined by one single random vari-
able, namely x1. What is more natural? To consider x1
as a random variable with a distribution determined by
the invariant measure, or to consider x1 as an unknown
parameter to be estimated? The answer, in our opinion,
is dictated by consideration of eﬃciency: the diﬀerent
examples that we have explored suggest that the latter is
as a rule more eﬃcient (has smaller mean square error),
at least for some combinations of sample size N and noise
level.

As all the above has shown, the major obstacle is the
loss of information on the initial value x1 by the unstable
logistic map beyond 10 − 25 time steps. We proposed the
simple recipe of cutting the time series in short pieces
and of averaging the estimations. Judd proposes a shad-
owing method [8]. It is not obvious that this will result
in a consistent estimation and that this will overcome the

intrinsic diﬃculty in treating long realizations (which is
a necessary condition for unbiased estimations).

In sum, there is no analytical proof of consistency for
all the estimation methods discussed until now (including
the suggestions performed by the most convincing work
to date [8] and our “pure” ML). It is useful to analyze the
only method to our knowledge for which one can derive
a proof of consistency in the present context, that is, the
method of statistical moments.

V. THE METHOD OF STATISTICAL
MOMENTS

The method of statistical moments provides a consis-
tent estimate of the parameters for non-linear maps with
ergodic properties. The method of statistical moments
is the unique theoretically proven consistent estimator
among all methods suggested so far by other authors.
Although the moment estimates are known to have little
eﬃciency, they are consistent! Consistency of all esti-
mates suggested earlier including ours above were con-
ﬁrmed only numerically, which is very dangerous for in-
stable non-linear maps.

We consider four moment of the observed time se-
ries: hsiN , hs2iN , hs3iN and hsisi+1iN , where the brack-
ets stand for time averaging over some time interval N .
Building on the knowledge that the series {xi} is ergodic
[10] and using (1,2), we obtain the following relations

hsiN → hxi∞ ,
hs2iN → hx2i∞ ,
hs3iN → hx3i∞ + 3hxi∞ǫ2 ,

hsisi+1iN → hxi∞ − ahx3i∞ .

(6)
(7)
(8)
(9)

5

sample size Noise std

q1

q2

q2 − q1

Estimate
(a)± std

N

100
1000
10000
100000
100
1000
10000
100000
100
1000
10000
100000

ǫ
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.5
0.5
0.5
0.5

1.8768 ± 0.0926 1.684 2.000 0.316
1.8544 ± 0.0418 1.774 1.936 0.162
1.8503 ± 0.0136 1.824 1.878 0.054
1.8499 ± 0.0044 1.842 1.858 0.016
1.8456 ± 0.1546 1.499 2.000 0.501
1.8532 ± 0.0815 1.693 2.000 0.307
1.8505 ± 0.0279 1.795 1.908 0.113
1.8497 ± 0.0089 1.833 1.867 0.034
1.2411 ± 0.7331
2.000 2.000
1.6907 ± 0.3496 0.903 2.000 1.097
1.8244 ± 0.1659 1.467 2.000 0.533
1.8554 ± 0.0741 1.715 2.000 0.285

0

TABLE II: Estimation of the structural parameter a by the
method of statistical moments (expression (11)) for the lo-
gistic map xi+1 = 1 − ax2
i , a = 1.85; the observations are
si = xi + ηi; ηi is a Gaussian random variable N (0, ǫ). As in
table I, q1 and q2 are the sample quantiles at the 2.5% and
97.5% probability level, so that q2 − q1 gives the width of the
95% conﬁdence intervals. Each estimate for a and std are
based on 1000 simulated samples.

We present in table II the estimates of the parameter a
given by expression (11). The consistency of the method
of statistical moments is clearly suggested by the numer-
ical results, as seen from the bracketing of the true value
by (a)± std and by q1 and q2. However, as we already
pointed out, the method of statistical moments is rather
ineﬃcient: the ratio of its standard deviation for a to that
of the “pure” ML is about 4 for N = 100 and ǫ = 0.1 for
instance.

Besides, averaging equation (2), we get

VI. CONCLUDING REMARKS

hxi∞ = 1 − ahx2i∞ .

(10)

This provides us with ﬁve limit relations (6-10) with ﬁve
unknown parameters: a, hxi∞, hx2i∞, hx3i∞ and ǫ. Solv-
ing these ﬁve relations with respect to the unknown pa-
rameters, we get the so-called estimates of the method of
moments:

ˆa =

hsisi+1iN + 2hsiN − 3(hsiN )2
3hsiN hs2iN − hs3iN

,

(11)

hˆx3i∞ =

hˆxi∞ = hsiN ,
hˆx2i∞ = hs2iN − ˆǫ2 ,
1
ˆa
hs3iN − hx3i∞
3hsiN

ˆǫ2 =

.

(hsiN − hsisi+1iN ) ,

(12)
(13)

(14)

(15)

Because of the limit relations (6-9) (which are valid be-
cause of the ergodicity of the time series {xi} [10]), the
estimates (11-15) are consistent if N → ∞.

We have proposed a “pure” Maximum Likelihood (ML)
method to estimate the structural parameter of a deter-
ministically chaotic low-dimensional system (the logistic
map), which adds the initial value x1 to the structural
parameter to be determined. We have compared quan-
titatively this method with the ML method proposed by
McSharry and Smith [1] based on an averaging over the
unknown invariant measure of the dynamical system. A
key aspect of the implementation of our approach lies in
the compromise between the need to use a large number
of data points for the ML to become consistent and the
unstable nature of dynamical trajectories which loses ex-
ponentially fast the memory of the initial condition. This
second aspect prevents using our “pure” ML for systems
larger than 10 − 25 data points. For larger time series,
we have found convenient to devide them into subsys-
tems of very small lengths and then to average over their
estimations. Numerical tests suggest that this direct ML
method provides often signiﬁcantly better estimates than
previously proposed approaches.

The diﬀerence between McSharry and Smith’s aver-
aging over the invariant measure and our “pure” ML is
reminiscent of the distinction between “annealed” versus
“quenched” averaging in the statistical physics of ran-
dom systems, such as spin glasses [11, 12]. It has indeed
been shown that the correct theory of strongly hetero-
geneous media is obtained by performing the thermal
Gibbs-Boltzmann averaging over ﬁxed structural disor-
der realizations, similarly to our use of a speciﬁc trajec-
tory of the latent variables xi’s. In constrast, performing
the thermal Gibbs-Boltzmann averaging together with
an averaging over diﬀerent realization of the structural
disorder describes another type of physics, which is not
that of ﬁxed heterogeneity. This second incorrect type of
averaging is similar to the averaging of the ML over the
invariant measure performed by McSharry and Smith.

There are several ways to improve our approach. One
simple implementation is to use overlapping running win-
dows. Another method is to re-estimate the realized
trajectory by using the extended Kalman ﬁlter method
(however, diﬃculties may arise due to the existence of a
maximum in the logistic map). Using shadowing meth-
ods as proposed in [8] in our context would also be inter-
esting to investigate.

Let us end with a cautionary note. As we just said,
the ML approach for two parameters (a, x1) that we sug-
gest here evidently works only for a limited sample size
N (perhaps, N < 25 or so) due to the sensitivity upon
initial conditions of the chaotic logistic map. As is well-
known in classical statistics, ML-estimates have a bias
that can be considerable if N is not large (say, N < 100
or so). The ML-estimates are usually only asymptoti-
cally unbiased. Thus, for N = 25 (and all the more for
N = 4), ML-estimates can exhibit a considerable bias.
Thus, averaging biased estimates as we proposed many
not result in a consistent estimation. Therefore, we can-
not assert that our ML method (as well as any other
suggested methods) is consistent. We can only observe,
for particular combinations of the considered parame-
ters, the numerically determined mean square error of
our suggested estimates with respect to the true parame-
ter value. We are pleased if these errors are not too high,
although our estimates can be biased (though, with small
bias). But we are not able to make such bias arbitrarily
small by increasing the sample size N , due to the insta-
bility under the iterations of the logistic map which leads
to a loss of information about the initial value x1. Thus,
the situation is rather hopeless for the establishment of a
meaningful statistical theory of estimation using the con-
tinuous theory of classical statistics to such discontinuous
objects as the invariant measures of chaotic dynamical
systems.

Acknowledgments

6

and by the James S. Mc Donnell Foundation 21st century
scientist award/studying complex system.

Appendix A: One-step and total least-square
estimations

McSharry and Smith noticed that the one-step leasts-
square method gives strongly biased results for the esti-
mation of a [1]. Indeed, the method of estimation of the
parameter a by the one-step least square method is evi-
dently inconsistent, since the deviations (of the random
variables) to be minimized in a least-square sense are

si+1 − F (si, a) = xi+1 + ηi+1 − F (xi + ηi, a)

= ηi+1 + 2axiηi + aη2

i ,

(16)

which has non-zero expectation equal to aǫ2. But, the
fundamental least-square principle consists in the min-
imization of deviations with zero mean. There are no
least-square schemes that would suggest to minimize ran-
dom deviations with non-zero mean depending on an un-
known parameter. Thus, it is not reasonable to include
the least-square method in any reasonable comparison.

The method called by McSharry and Smith as “to-
tal least-squares” (TLS) is applied in situation when the
variables xi are known only with some errors ηi. This
situation is called in statistics a “Conﬂuence analysis,”
or “Estimation of a structural relation between two (or
more) variables in the presence of errors on both vari-
ables” [2–4]. In such a situation of conﬂuence analysis,
since the xi’s are in fact unknown (nuisance) parameters
whose number grows with sample size, there is no guar-
antee of consistency of the ML estimates of the structural
parameter a.

As an example, let us consider the very simple conﬂu-

ent scheme:

Yi = Xi + ηi ,
Zi = Xi + ζi .

(17)
(18)

Suppose we observe a sample of N pairs (Yi, Zi), i =
1, ..., N , where Xi are unknown arbitrary values and ηi, ζi
are iid Gaussian random variables with standard devia-
tion ǫ. The problem consists in estimating the parameter
ǫ. Similarly to the situation with (1) and (2) studied in
[1], no restrictions are placed on the Xi’s. The likelihood
L(ǫ, X1, ..., XN |(Yi, Zi), i = 1, ..., N ) is

L(ǫ, X1, ..., XN |(Yi, Zi), i = 1, ..., N ) ∝

−2N

ǫ

exp

−(1/2ǫ2)

(Yi − Xi)2 − (1/2ǫ2)

(Zi − Xi)2

.

"

N

i=1
X

N

i=1
X

#

(19)
The MLE ˆXi’s of the Xi’s (that coincide in this case with
the least-square estimates) are:

We are grateful to K. Ide for useful discussions. This
work is partially supported by a LDRD-Los Alamos grant

ˆXi =

Yi + Zi
2

.

(20)

Inserting (20) into (19), we get

ˆL(ǫ|(Yi, Zi),

i = 1, ..., N ) ∝

−2N

ǫ

exp

−(1/4ǫ2)

(Yi − Zi)2

.(21)

#

N

i=1
X

Thus, the MLE of the parameter ǫ obtained from (21)
satisﬁes

ǫ2 =

1
4N

(Yi − Zi)2 .

(22)

Since E
tent. A consistent (“corrected”) estimate is

= 2ǫ2, the estimate (22) is inconsis-

(Yi − Zi)2

(cid:2)

(cid:3)

ǫ2 =

1
2N

(Yi − Zi)2 .

(23)

"

N

i=1
X

N

i=1
X

7

Thus, we see that the MLE of the structural parameter ǫ
is inconsistent due to the increasing number of nuisance
parameters. Thus, the direct use of the least-square (or
total least-square) in the conﬂuent situation is not jus-
tiﬁed, and was not recommended in any statistical text-
book. Instead, standard statistical works recommend a
“corrected” ML estimates (see for instance [3, 4]).

We should stress in addition that there is a signiﬁ-
cant diﬀerence between the standard conﬂuent analysis
and the problem addressed in [1]. Conﬂuent analysis
deals with arbitrary unknown (distorted) arguments xi,
whereas in [1], the latent variables xi are related by the
non-linear map (2). The information on the structure of
the xi’s is not used in Conﬂuence Analysis while it can
really help in the estimation procedure as shown in [1]
and in the present work.

[1] P.E. Mcsharry and L.A. Smith, Phys. Rev. Lett. 83, 4285

[7] R. Meyer and N. Christensen, Phys. Rev. E 62, 3535

(1999)

(2000).

[2] Frisch R. Statistical Conﬂuence Analysis by Means of

Complete Regression Systems, Oslo, 1934.

[3] Geary R.C. Non-linear functional relationship between
two variables when one variable is controlled, J. Amer.
Statist. Ass. 48, 94 (1953).

[4] M. Kendall and A. Stuart, The advanced theory of statis-
tics, Curvilinear Dependencies, 2d ed. (New York, Hafner
Publ. Co., 1961), Chapter 29, Section 29.50.

[5] D. Sornette and A. Arn´eodo, J. Phys. (Paris) 45, 1843

[6] S.C. Phatak and S.S. Rao, Phys. Rev. A. 51 (4 Part B),

(1984).

3670 (1995).

[8] CK. Judd, Phys. Rev. E 67 (2), 026212 (2003).
[9] M.L. Berliner, J. Am. Stat. Assoc. 86, 939 (1991).

[10] P. Collet and J.-P. Eckmann, Iterated maps on the in-
terval as dynamical systems (Basel; Boston: Birkhauser,
1980).

[11] M., M´ezard, M., Parisi, G. and Virasoro, M., Spin Glass
Theory and Beyond (World Scientiﬁc, Singapore, 1987).
[12] D. Sornette, Critical Phenomena in Natural Sciences
(Springer Series in Synergetics, Heidelberg, 2000), see
chapter 16.


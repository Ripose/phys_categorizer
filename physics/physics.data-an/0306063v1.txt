3
0
0
2
 
n
u
J
 
7
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
6
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Entropy and information in neural spike trains:
Progress on the sampling problem

Ilya Nemenman, 1 William Bialek,2 and Rob de Ruyter van Steveninck3

1Kavli Institute for Theoretical Physics
University of California at Santa Barbara, Santa Barbara, California 93106
2Departments of Physics and 3Molecular Biology, and
the 2Lewis–Sigler Institute for Integrative Genomics
Princeton University, Princeton, New Jersey 08544
nemenman@kitp.ucsb.edu, wbialek@princeton.edu, deruyter@princeton.edu

Abstract

The major problem in information theoretic analysis of neural responses
is the reliable estimation of entropy–like quantities from small samples.
We review a Bayesian estimator of entropies introduced recently [1] to
solve this problem, and study its performance on synthetic and experi-
mental spike trains. The estimator performs admirably even very deep in
the undersampled regime, where other techniques fail. This opens new
possibilities for the information theoretic analysis of experiments, and
may be of general interest as an example of learning from limited data.

1 Introduction

There has been considerable progress in using information theoretic methods to sharpen
and to answer many questions about the structure of the neural code [2, 3, 4, 5, 6, 7, 8, 9].
Where classical experimental approaches to the characterization of neurons have focused
on their mean response to relatively simple stimuli, information theoretic methods have
the power to quantify the responses to arbitrarily complex and even fully natural stimuli
[10, 11], taking account of both the mean response and its variability in a rigorous way,
independent of detailed modeling assumptions. Measurements of entropy and information
in spike trains also allow us to test directly the hypothesis that the neural code adapts to the
distribution of sensory inputs, optimizing the rate or efﬁciency of information transmission
[12, 13, 14].

The difﬁculty of information theoretic approaches is that entropy and information depend
explicitly on the full distribution of neural responses, and experiments give us only limited
samples from this distribution. In particular, we need to know the distribution of responses
to each stimulus in our ensemble, and the number of samples from this distribution is lim-
ited by the number of times the full set of stimuli can be repeated. For more natural stimuli
with long correlation times, however, the time required to present a useful “full set of stim-
uli” is much longer than for simple noise–like stimuli, limiting the number of independent
samples we can obtain in a ﬁxed time window of stable neural recordings. Furthermore,
natural stimuli also seem to generate neural responses of higher timing precision, which

means that the meaningful space of responses itself is larger [4, 11, 15, 16]. These factors
conspire to make the sampling problem more serious as we move to more interesting and
natural stimuli.

Once we admit that we will not be able to do experiments so large as to make observed
frequencies of responses virtually identical with their probabilities, one natural response to
the sampling problem is to give up on the generality of a completely model independent
information theoretic approach. Certainly we need some explicit help from models to reg-
ularize the problem of learning the underlying probability distributions from the samples
given by experiments. The question is whether we can maintain the generality of our anal-
ysis by introducing the most gentle of regularizations for the abstract learning problem, or
if we need stronger assumptions about the structure of the neural code itself (for exam-
ple, introducing a metric on the space of responses [17]), making many of our conclusions
crucially dependent on the validity of such assumptions. Here we argue that a simple and
abstract Bayesian prior, introduced in Ref. [1], is sufﬁcient to generate reliable estimates of
entropy and information well into a classically undersampled regime.

2 An estimation strategy

Consider the problem of estimating the entropy S of a probability distribution {pi},

S = −

pi log2 pi,

K

i=1
X

Snaive = −

fi log2 fi.

K

i=1
X

where the index i runs over K possibilities (e.g., K possible neural responses).
In an
experiment we observe that in N examples each possibility i occurred ni times. When
N is large, in particular if N ≫ K, it makes sense to approximate the probabilities by
frequencies, pi ≈ fi ≡ ni/N , and then to construct a naive estimate of the entropy,

This is also a maximum likelihood estimator, since the maximum likelihood estimate of
the probabilities is given by the frequencies. Thus we will replace Snaive by SML in what
follows. It is well known that SML underestimates the entropy (see, for example, Ref. [18]
for a recent review). For large N this systematic error is proportional to 1/N , and one can
detect this behavior and make appropriate corrections as in Ref. [5]; see also Ref. [19]. This
approach is workable only when the sampling errors are in some sense a small perturbation.
Can we make progress outside the asymptotically large N regime?

Maximum likelihood estimation is a limiting case of Bayesian estimation with Dirichlet
priors. Formally, we consider that the probability distributions p ≡ {pi} are themselves
drawn from a distribution Pβ(p) of the form

Pβ(p) =

1
Z(β; K) "

K

i=1
Y

p(β−1)
i

δ

#

K

i=1
X

pi − 1

,

(cid:17)

(cid:16)
where the delta function enforces normalization of distributions p and the partition func-
tion Z(β; K) normalizes the prior Pβ(p). Maximum likelihood estimation is Bayesian
estimation with this prior in the limit β → 0, while the natural “uniform” prior is β = 1.
The key observation of Ref. [1] is that while these priors are quite smooth on the space
of p, the distributions drawn at random from Pβ all have very similar entropies, with a
variance that vanishes as K becomes large. Fundamentally this is the origin of the sample
size dependent bias in entropy estimation, and one might thus hope to correct the bias at its

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

source. The goal then is to construct a prior on the space of probability distributions which
generates a nearly uniform distribution of entropies. Because the entropy of distributions
chosen from Pβ is sharply deﬁned and monotonically dependent on the parameter β, we
can come close to this goal by an average over β,

PNSB(p) ∝

dβ

d ¯S(β; K)
dβ

Pβ(p),

where ¯S(β; K) is the average entropy of distributions chosen from Pβ; analytic expressions
for ¯S(β; K) are available [1, 20]. Given this prior we proceed in standard Bayesian fashion.
We recall that the probability of observing the data n ≡ {ni} given the distribution p is

Z

K

Yi=1

P (n|p) ∝

pni
i

,

and then

P (p|n) = P (n|p)PNSB(p) ·

1
P (n)

P (n) =

dpi

P (n|p)PNSB(p)

K

Z "

i=1
Y
K

#

Z "

i=1
Y

K

i=1
X

SNSB =

dpi

−

#  

pi log2 pi

P (p|n).

!

The Dirichlet priors have the nice property that all the (K dimensional) integrals over p can
be done analytically, so that the computation of SNSB and its error reduces to three one–
dimensional integrals which must be done numerically; details can be found in Refs. [1,
21]. We draw attention to several points:

1. Since we are doing a Bayesian analysis, we obtain not only an estimate but also

its a posteriori standard deviation, δSNSB—an error bar on our estimate.

2. In detail, the estimation procedure relies on ∆, the number of coincidences in the

data set to make its estimates; this is in the spirit of Ref. [22].

3. SNSB is unbiased if the rank ordered (Zipf) plot of the probability distribution be-
ing learned is well behaved. If the Zipf plot has tails that are too short (too long),
then the estimator should over (under) estimate. Interestingly, while underestima-
tion may be severe (though always strictly smaller than that for SML), overesti-
mation is very mild, if present at all, in the most interesting regime 1 ≪ ∆ ≪ N .
4. For N → ∞ and N/K → 0 the estimator admits asymptotic analysis if ∆/N →
const or ∆/N → 0. The second asymptotic is particularly interesting since SNSB
happens to have a ﬁnite limit for K → ∞, thus allowing entropy estimation even
for inﬁnite (or unknown) cardinalities.

Before proceeding to results, it is worth asking what we hope to accomplish. Any rea-
sonable estimator will converge to the right answer in the limit of large N . In particular,
this is true for SNSB, which is a consistent Bayesian estimator. The central problem of
entropy estimation is systematic bias, which will cause us to (perhaps signiﬁcantly) under
or overestimate the information content of spike trains or the efﬁciency of the neural code.
The bias, which vanishes for N → ∞, will manifest itself as a systematic drift in plots of
the estimated value versus the sample size. A successful estimator would remove this bias
as much as possible. Ideally we thus hope to see an estimate which for all values of N is
within its error bars from the correct answer. As N increases the error bars should narrow,
with relatively little variation of the (mean) estimate itself. We will see that our procedure
comes close to this ideal over a wide range of N ≪ K, and even N ≪ 2S.

3 A model problem

S

5
 

0
 

0
1
 

0
3
 

5
1
 

0
2
 

5
2
 

s
t
i
b
 
,

K=230
K=216
ML
S
true

Refractory spikes, T = 15 ms, τ = 0.5 ms

It is important to test our tech-
niques on a problem which cap-
tures some aspects of real world
data but nonetheless is sufﬁ-
ciently well deﬁned that we
know the correct answer. To
this end, we constructed syn-
thetic spike trains where inter-
vals between successive spikes
were independent and chosen
from an exponential distribu-
tion with a dead time or refrac-
tory period of 1.8 ms; the mean
spike rate was 0.26 spikes/ms.
These parameters are typical of
the high spike rate, noisy re-
gions of the experiment dis-
cussed below, which provide
the greatest challenge for en-
tropy estimation. Following the
scheme outlined in Ref. [5], we
examine the spike train in windows of duration T = 15 ms and discretize the response
with a time resolution τ = 0.5 ms. Because of the refractory period each bin of size τ can
contain at most one spike, and hence the neural response is a binary word with T /τ = 30
letters. The space of responses has K = 230 ≈ 109 possibilities. Of course, most of
these have probability exactly zero because of refractoriness, and the number of possible
responses consistent with this constraint is bounded by ∼ 216 ≈ 105. The entropy of the
distribution, calculated analytically, is S = 13.57 bits.

Figure 1: Entropy estimation for a model problem.
Notice that the estimator reaches the true value within
the error bars as soon as N 2 ∼ 2S, at which point co-
incidences start to occur with high probability. Slight
overestimation for N > 103 is expected (see text)
since this distribution is atypical in PNSB.

 2.5
log10

 3
N

 4.5

 1.5

 0.5

 3.5

 4

 5

 2

 1

In Fig. 1 we show the results of entropy estimation for this model problem. As expected,
the naive estimate SML approximates the right answer only when N > 2S (though extrapo-
lation techniques may become successful at N ∼ 104). In contrast, we see that SNSB gives
the right answer within errors at N ∼ 100. We can improve convergence by providing the
estimator with the “hint” that the number of possible responses K is much smaller than the
upper limit of 230, but even without this hint we have excellent entropy estimates already at
N ∼ (2S)1/2. This is in accord with expectations from Ma’s analysis of (microcanonical)
entropy estimation from trajectories in classical mechanics [22], but here we achieve these
results even for a nonuniform distribution.

4 An experiment with natural signals

We test our methods on realistic neurophysiological data, recorded from a wide ﬁeld mo-
tion sensitive neuron (H1) in the visual system of the blowﬂy Calliphora vicina. While
action potentials from H1 were recorded, the ﬂy rotated on a stepper motor outside among
the bushes, with time dependent angular velocity representative of natural ﬂight. Figure 2
presents a sample of raw data from such an experiment. Details can be found in Ref. [11],
but conditions were slightly different in the present experiment. First, the velocity trace
repeated with a period of 10 s, as opposed to 5 s, to accommodate a larger number of inde-
pendent samples across time. Further, the velocity stimulus had a nonzero DC component,
so that the ﬂy was subject to the same velocity with the scene at different retinal positions
in different trials. The natural environment in the experiment was inhomogeneous, with

strong variations in light intensity and texture, and the ﬂy’s brain computes a velocity esti-
mate from the raw spatiotemporal data presented by the moving scene. These inputs have a
random component due to photon shot noise, which means that the velocity estimate itself
will be biased [23]. This bias is noticeable in the form of ﬂuctuations in spike timing, cor-
related to the ﬂy’s position. These ﬂuctuations effectively act as an extra source of noise,
presenting a more stringent test of our analysis method.

5 Analyzing real data: Noise entropy in individual time windows

0

°

20

10

)
s
/

2000

l
a
i
r
t

0
1500

(
 
y
t
i
c
o
l
e
v

−2000
30

The central difﬁculty of ana-
lyzing the information content
of spike trains is in estimat-
ing the entropy of neural re-
sponses to repeated presenta-
tions of the same stimulus. As
in Ref. [5], we call this the noise
entropy Sn, since it measures
response variations that are un-
correlated with the sensory in-
put. The noise in neurons de-
pends on the stimulus itself—
there are, for example, stimuli
which generate with certainty
zero spikes in a given window
of time—and so we write Sn|t
to mark the dependence on the
time t at which we take a slice
through the raster of responses.
In our experiment the full stim-
ulus was repeated 196 times,
which actually is a relatively
large number by the standards
of neurophysiology. The ﬂy makes behavioral decisions based on ∼ 10 − 30 ms windows
of its visual input [24], and under natural conditions the time resolution of the neural re-
sponses is of order 1 ms or even less [11], so that a meaningful analysis of neural responses
must deal with binary words of length 10 − 30 or more. Refractoriness limits the number
of these words which can occur with nonzero probability (as in our model problem), but
nonetheless we easily reach the limit where the number of samples is substantially smaller
than the number of possible responses.

Figure 2: Data from a ﬂy motion sensitive neuron in
a natural stimulus setting. Top: a 500 ms section of a
10 s angular velocity trace that was repeated 196 times
in a continuous loop. Bottom: raster plot showing the
response to 30 consecutive trials induced by repetitions
of the velocity trace; each dot marks the occurrence of
a spike. The experiment was done around 2 PM on a
sunny day, at an outside temperature of 18 ◦C.

time (ms)

1700

1600

1800

1900

2000

Let us start by looking at a single moment in time, t = 1800 ms from the start of the
repeated stimulus, as in Fig. 2. If we consider a window of duration T = 16 ms at time
resolution τ = 2 ms, we obtain the entropy estimates shown in the left panel of Fig. 3.
Notice that in this case we actually have a total number of samples which is comparable to
or larger than 2Sn|t, and so the maximum likelihood estimate of the entropy is converging
with the expected 1/N behavior. The NSB estimate agrees with this extrapolation. The
crucial result is that the NSB estimate is correct within error bars across the whole range of
N ; there is a slight variation in the mean estimate, but the main effect as we add samples is
that the error bars narrow around the correct answer. In this case our estimation procedure
has removed essentially all of the sample size dependent bias.

Encouraged by these results, consider what happens as we open our window to T = 30 ms.
Now the number of possible responses (even considering refractoriness) is vastly larger
than the number of samples, and as we see in the right panel of Fig. 3 any attempt to

Slice at 1800 ms, τ = 2 ms, T = 16 ms

Slice at 1800 ms, τ = 2 ms, T = 30 ms

NSB
ML

NSB
ML

s
t
i
b
 
,
)
τ
,
T
(
t
n
S

|

8
 

6
 

4
 

2
 

0
 

s
t
i
b
 
,
)
τ
,
T
(
t
n
S

|

6
1
 

2
1
 

8
 

4
 

0
 

 0

 0.04

 0.08

 0.12

 0.16

 0.2

 0

 0.04

 0.08

 0.12

 0.16

 0.2

1/N

1/N

N =Nmax

Figure 3: Slice entropy vs. sample size. Dashed line on both plots is drawn at the value of
SNSB
to show that the estimator is stable within its error bars even for very low
N . Triangle corresponds to the value of SML extrapolated to N → ∞ from the four largest
values of N . Left and right panels show examples of word lengths for which SML can or
cannot be reliably extrapolated. SNSB is stable in both cases, shows no N dependent drift,
and agrees with SML where the latter is reliable.

(cid:12)
(cid:12)

extrapolate the ML estimate of entropy requires some wishful thinking. Nonetheless, in
parallel with our results for the model problem, we ﬁnd that the NSB estimate is stable
within error bars across the full range of available N .

 N = 75

ε

3

2

1

0

−1

−2

−3

0

For small T we can compare
the results of our Bayesian esti-
mation with an extrapolation of
the ML estimate; each moment
in time relative to the repeated
stimulus provides an example.
We have found that the results
in the left panel of Fig. 3 are
typical: in the regime where ex-
trapolation of the ML estimator
is reliable, our estimator agrees
within error bars over a broad
range of sample sizes. More
precisely, if we take the extrap-
olated ML estimate as the cor-
rect answer, and measure the
deviation of SNSB from this an-
swer in units of the predicted er-
ror bar, we ﬁnd that the mean
square value of this normalized
error is about one. This is as ex-
pected if our estimation errors
are close to being random rather
than systematic.

5

10

 S NSB

15

Figure 4: Distribution of the normalized entropy er-
ror conditional on SNSB(Nmax) for N = 75 and
τ = 0.75 ms. Darker patches correspond to higher
probability. The band in the right part of the plot is
the normal distribution around zero with the standard
deviation of 1 (the standard deviation of plotted con-
ditional distributions averaged over SNSB is about 0.7.
For values of SNSB, up to about 12 bits, the estimator
performs remarkably well. For yet larger entropies,
where the number of coincidence is just a few, the dis-
crete nature of the estimated values is evident, and this
puts a bound on reliability of SNSB.

For larger T we do not have a
calibration against the (extrapo-
lated) ML estimator but we can
still ask if the estimator is stable, within error bars, over a wide range of N . To check
this stability we treat the value of SNSB at N = Nmax = 196 as our best guess for the

(cid:2)

SNSB(N ) − SNSB(Nmax)

entropy and compute the normalized deviation of the estimates at smaller values of N from
/δSNSB(N ). Again, each moment in time
this guess, ε =
provides an example. Figure 4 shows the distribution of these normalized deviations con-
(cid:3)
ditional on the entropy estimate itself, with N = 75; this analysis is done for τ = 0.75 ms,
with T in the range between 1.5 ms and 22.5 ms. Since the different time slices span a
range of entropies, over some range we have Nmax > 2S, and in this regime we know
that the entropy estimate must be accurate (as in the analysis of small T above). Through-
out this range, the normalized deviations fall in a narrow band with mean close to zero
and a variance of order one, as expected if the only variations with sample size were ran-
dom. Remarkably this pattern continues almost unchanged as we look at larger entropies,
S > log2 Nmax = 7.5 bits, demonstrating that our estimator is stable even deep into the
undersampled regime. This is consistent with the results obtained in our model problem,
but here we ﬁnd the same answer for the real data.

Note that Fig. 4 illustrates results with N less than one half the total number of samples,
so we really are testing for stability over a large range in N . This emphasizes that our
estimation procedure moves smoothly from the well sampled into the undersampled regime
without accumulating any clear signs of systematic error. The procedure collapses only
when the entropy is so large that the probability of observing the same response more than
once (a coincidence) becomes negligible.

6 Discussion

While one might expect that a meaningful estimate of entropy S requires many more than
2S samples, we know from Ma [22] that for uniform distributions of unknown size (as in the
microcanonical ensemble) we can make reliable estimates of the entropy when N 2 ∼ 2S.
This is related to the fact that we will encounter two people who have the same birthday
once we have chosen at random a group of N ∼ 23 << 365 people—conversely, testing
for coincidences tells us something about the entropy of the distribution of birthdays even
when we are far from having seen all the possibilities. The challenge is to convert these
ideas about coincidences into a reliable estimator for the entropy of nonuniform distribu-
tions.

The estimator we have explored here is constructed from a nearly uniform prior not on
the space of probability distributions, but on the entropy itself. It is plausible that such a
uniform prior would largely remove sample size dependent biases in entropy estimation,
but it is crucial to test this experimentally. We have found that the bias is removed in
model problems (Fig. 1), and that for real data in a regime where sampling problems can
be beaten down by data the bias is removed to yield agreement with the extrapolated ML
estimator even at very small sample sizes (Fig. 3, left panel). Finally and most crucially, our
estimation procedure continues to perform smoothly and stably past the nominal sampling
limit of N ∼ 2S, all the way out to the Ma cutoff of at N 2 ∼ 2S (Fig. 4). This opens the
opportunity for rigorous analysis of entropy and information in spike trains under a much
wider set of experimental conditions.

Acknowledgments

We thank J Miller for important discussions and GD Lewen for his help with the experi-
ments, which were supported by the NEC Research Institute. I. N. was supported by NSF
Grant No. PHY99-07949 to the Kavli Institute for Theoretical Physics. I. N. is also very
thankful to the developers of the following Open Source software: GNU Emacs, GNU
Octave, GNUplot, and teTEX.

References

[1] I Nemenman, F Shafee, and W Bialek. Entropy and inference, revisited.

In TG Dietterich,
S Becker, and Z Ghahramani, editors, Advances in Neural Information Processing Systems 14,
Cambridge, MA, 2002. MIT Press.

[2] W Bialek, F Rieke, RR de Ruyter van Steveninck, and D Warland. Reading a neural code.

Science, 252:1854–1857, 1991.

[3] F Theunissen and JP Miller. Representation of sensory information in the cricket cercal sen-
sory system. II: Information theoretic calculation of system accuracy and optimal tuning curve
widths for four primary interneurons. J. Neurosphys., 66:1690–1703, 1991.

[4] MJ Berry, DK Warland, and M Meister. The structure and precision of retinal spike trains. Proc.

Nat. Acad. Sci. (USA), 94:5411–5416, 1997.

[5] SP Strong, R Koberle, RR de Ruyter van Steveninck, and W Bialek. Entropy and information

in neural spike train. Phys. Rev. Lett., 80:197–200, 1998.

[6] A Borst and FE Theunissen. Information theory and neural coding. Nature Neurosci., 2:947–

[7] N Brenner, SP Strong, R Koberle, W Bialek, and RR de Ruyter van Steveninck. Synergy in a

neural code. Neural Comp., 12:1531–1552, 2000.

[8] P Reinagel and RC Reid. Temporal coding of visual information in the thalamus. J. Neurosci,

957, 1999.

20:5392–5400, 2000.

[9] DS Reich, F Mechler, and JD Victor. Temporal coding of contrast in primary visual cortex:

when, what, and why? J. Neurophysiol., 85:1039–1050, 2001.

[10] F Rieke, D Warland, R de Ruyter van Steveninck, and W Bialek. Spikes: Exploring the Neural

Code. MIT Press, Cambridge, MA, 1997.

[11] GD Lewen, W Bialek, and RR de Ruyter van Steveninck. Neural coding of naturalistic motion

stimuli. Network: Comput. In Neural Syst., 12:312–329, 2001.

[12] SB Laughlin. A simple coding procedure enhances a neuron’s information capacity. Z. Natur-

forsch., 36c:910–912, 1981.

[13] N Brenner, W Bialek, and RR de Ruyter van Steveninck. Adaptive rescaling maximizes infor-

mation transmission. Neuron, 26:695–702, 2000.

[14] AL Fairhall, GD Lewen, W Bialek, and RR de Ruyter van Steveninck. Efﬁciency and ambiguity

in an adaptive neural code. Nature, 412:787–792, 2001.

[15] ZF Mainen and TJ Sejnowski. Reliability of spike timing in neocortical neurons. Science, 268:

[16] RR de Ruyter van Steveninck, GD Lewen, SP Strong, R Koberle, and W Bialek. Reproducibility

and variability in neural spike trains. Science, 275:1805–1808, 1997.

[17] JD Victor. Binless strategies for estimation of information from neural data. Phys. Rev. E, 66:

1503–1506, 1995.

51903–51918, 2002.

[18] L Paninski. Estimation of entropy and mutual information. Neur. Comp., 15:1191–1253, 2003.
[19] S Panzeri and A Treves. Analytical estimates of limited sampling biases in different information

measures. Network: Comput. in Neural Syst., 7:87–107, 1996.

[20] D Wolpert and D Wolf. Estimating functions of probability distributions from a ﬁnite set of

[21] I Nemenman. Inference of entropies of discrete random variables with unknown cardinalities.

samples. Phys. Rev. E, 52:6841–6854, 1995.

Available at physics/0207009, May 2002.

[22] S Ma. Calculation of entropy from data of motion. J. Stat. Phys., 26:221–240, 1981.
[23] M Potters and W Bialek. Statistical mechanics and visual signal processing. J. Phys. I France,

4:1755–1775, 1994.

[24] MF Land and TS Collett. Chasing behavior of houseﬂies (fannia canicularis). A description

and analysis. J. Comp. Physiol., 89:331–357, 1974.


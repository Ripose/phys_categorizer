2
0
0
2
 
v
o
N
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
5
0
1
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Wavelet Domain Image Separation 1

Ali Mohammad-Djafari∗ and Mahieddine Ichir∗

∗Laboratoire des Signaux et Systèmes,
Supélec, Plateau de Moulon, 91192 Gif-sur-Yvette, France

Abstract. In this paper, we consider the problem of blind signal and image separation using a
sparse representation of the images in the wavelet domain. We consider the problem in a Bayesian
estimation framework using the fact that the distribution of the wavelet coefﬁcients of real world im-
ages can naturally be modeled by an exponential power probability density function. The Bayesian
approach which has been used with success in blind source separation gives also the possibility of
including any prior information we may have on the mixing matrix elements as well as on the hy-
perparameters (parameters of the prior laws of the noise and the sources). We consider two cases:
ﬁrst the case where the wavelet coefﬁcients are assumed to be i.i.d. and second the case where we
model the correlation between the coefﬁcients of two adjacent scales by a ﬁrst order Markov chain.
This paper only reports on the ﬁrst case, the second case results will be reported in a near future
The estimation computations are done via a Monte Carlo Markov Chain (MCMC) procedure. Some
simulations show the performances of the proposed method.

Keywords. Blind source separation, wavelets, Bayesian estimation, MCMC Hasting-Metropolis
algorithm.

INTRODUCTION

Blind source separation (BSS) is an active area of research in signal and image pro-
cessing. Different approaches have been proposed: Principal component analysis (PCA)
[41], Independent factor analysis (IFA) [3, 25, 26], Independent component analysis
(ICA) [6, 7, 8], Maximum likelihood estimation [45, 40, 43, 5, 15, 24, 19, 4] and
Bayesian estimation [27, 12, 14, 16, 28, 28, 13, 17]. All these methods use in general
independence, sparsity and diversity of the sources either in time or in Fourier domain.
Wavelets, as being a powerful tool of signal processing, have been largely used in
many signal processing domains and particularly in signal denoising: [1, 30, 10, 2, 23,
31]. They have been also used in inverse problems: [29, 42, 9]. The authors in these
papers take advantage of the properties of the wavelet coefﬁcients [29]: locality, multi-
resolution, singularity detection, energy compaction and decorrelation. These outlined
properties were said to be primary properties and give rise to what was described to be
secondary properties: non-Gaussianity and persistency.

Zibulevsky and Pearlmutter in [44] considered the problem of blind source separa-
tion within a Bayesian framework using an over-complete sparse representation of the
sources. They have, then, minimized an objective function assuming a known noise vari-

1 Presented at MaxEnt2002, the 22nd International Workshop on Bayesian and Maximum Entropy meth-
ods (Aug. 3-9, 2002, Moscow, Idaho, USA). To appear in Proceedings of American Institute of Physics

ance and an empirical estimation of the sources variances.

In this paper, thanks to the unitary property of the wavelet transform, we transport
the problem of BSS to the wavelet domain and propose to use the Bayesian estimation
framework.

According to the properties [29]: decorrelation (the wavelet coefﬁcients of real
world signals (images) tend to be approximately decorrelated) and non-Gaussianity
(the wavelet coefﬁcients have peaky, heavy tailed marginal distributions), we propose
to model the distribution of the wavelet coefﬁcients by a generalized exponential (GE)
probability density function (pdf). Thus, independence and sparsity which are the main
hypotheses of all the source separation technics are not required for the sources them-
selves, but rather for their wavelet coefﬁcients.

The Bayesian approach which has been used with success in blind source separation
gives also the possibility of including any prior information we may have on the mixing
matrix elements as well as on the hyperparameters (parameters of the prior laws of the
noise and the sources) of the problem.

In this work, we make use of the fast wavelet transform developed by Mallat [20] to
have a non-redundant multi-scale representation. This paper is organized as follows: In
section 2, we ﬁrst present the general source separation problem using notation which
can be used either in the 1D, 2D or the m-D case. Then, we write the same problem in the
wavelet domain and explicit our hypotheses about the prior distributions of the noise and
wavelet coefﬁcients. In section 3, we present the Bayesian approach and give the main
expressions of the prior and posterior probability density functions. In section 4, ﬁrst we
give the basics of the MCMC algorithm and then apply it to our case. In section 5, we
present a few simulation results to show the performances of the proposed method and
give some comparison with other known and classical approaches. Finally, in section 6,
we present our conclusions and perspectives.

PROBLEM FORMULATION

Blind image separation consists of estimating sources from a set of their linear mixtures.
The observations consist of m images
which are instantaneous linear
mixtures of n unknown sources
, possibly corrupted by additive noise
ξi, i = 1, . . . , m
}

Xi, i = 1, . . . , m
}

Sj, j = 1, . . . , n
}

{

{

{

:

X = AS + ξ

(1)

×

where A(m×n) is the mixing matrix. To be able to consider 1D, 2D or even m-D signals,
we assume that Xi, Sj and ξi contain each T samples representing either T samples of
time series or T pixels of an image or, more generally, T voxels of an m-D signal. Thus,
S is a (n

T ) matrix and X and ξ are (m

T ) matrices.

The blind source separation problem is to estimate both the mixing matrix A and the
sources S from the data X and some assumptions about noise distribution and some
prior knowledge of sources distributions. Different approaches have been proposed:
Principal component analysis (PCA) [11, 21] mainly assumes the problem without
noise and Gaussian distribution for sources, Independent component analysis (ICA)
[11, 18] and Maximum likelihood estimation [21] assume again the problem without

×

noise but different non-Gaussian distributions for sources, Factor analysis (FA) methods
take account of the noise, but assume Gaussian priors both for the noise and the sources.
The Bayesian approach is a generalization of FA with the possibility of any non-
Gaussian priors for noise and sources as well as the possibility of accounting for any
prior knowledge on the elements of the mixing matrix and the hyperparameters of the
problem. In addition, it allows us to jointly estimate the sources S, the mixing matrix A
and even the hyperparameters θ of the problem through the posterior:

p(S, A, θ

X)

p(X

S, A, θ) p(S

θ) p(A

θ) p(θ)

|

∝

|
θ) such as Gaussian [22]
We have used this approach before with different priors p(S
and mixture of Gaussians [34, 33]. We also used this approach in multi-spectral image
separation in astronomy for separating the cosmological microwave background (CMB)
from other cosmological microwave activities [35, 36, 39, 38, 32, 37].

|

|

|

In this paper, we are going to use the same Bayesian approach, but doing the sepa-
ration taking the advantage of the independence and diversity properties of the wavelet
domain coefﬁcients of the sources. Noting by the vector s the T samples of one of the
sources, by H the discrete wavelet transform matrix, and by w the complete wavelet
coefﬁcients of the 1-D signal we have

(2)

Now, using the fact that the complete discrete wavelet transform is a linear and unitary
operator (H tH = HH t = I), the problem of source separation can be easily trans-
ported to the wavelet domain and written as:

s = Hw

Wx = AWs + Wξ

The main advantage of using this last equation in place of the original source separation
problem is that we can more easily assign simple prior laws for Ws than for S itself. For
example, when S contains discontinuity or non-stationary, still its wavelet coefﬁcients
distribution can be modeled by a simple generalized exponential (GE) probability den-
sity function (pdf) while it is harder to model appropriately signal samples distribution
by a simple pdf. Indeed, it has been reported by many authors that the distribution of the
wavelet coefﬁcients of real world images are well modeled by a GE pdf:

p(w

α, β) =

(α, β) =

|

GE

β
2αΓ(1/β)

exp

w/α

β

|

−|

(cid:8)

(cid:9)

Note that β = 1 gives an exponential pdf and β = 2 corresponds to a Gaussian pdf. We
are going to use this prior probability law in our Bayesian estimation framework.

This is shown in the following ﬁgures. Figure (1) shows two images (Lena and the
cameraman) which we will use later in our simulations. Figure (2) shows their respective
histograms while Figure (3) shows their wavelet coefﬁcients and Figure (4) shows the
corresponding histograms of their wavelet coefﬁcients. We can remark that even if the
histograms of the image pixels are very different, the corresponding wavelet coefﬁcients
are similar and can be modeled easily by GE pdf, with different α and β. For a given

(3)

(4)

(5)

signal or image, these two parameters can be estimated using either the Maximum
Likelihood (ML) method:

( ˆα, ˆβ) = arg min

n ln α + n ln

(α,β) (cid:18)

n

Γ( 1
β )
β

+

1
αβ

β

xi|

|

(cid:19)

i=1
X

or a moments based method by noting that the moments of the GE pdf are given by:

Γ( n+1
β )
β ) αn
Γ( 1

if n is even

0

if n is odd

E(xn) = 




50

100

150

200

250

350

300

250

200

150

100

50

0

0

50

100

150

200

250

350

300

250

200

150

100

50

50

100

150

200

250

50

100

150

200

250

FIGURE 1. Lena and the cameraman images

50

100

150

200

50

100

150

200

250

250

0

0

FIGURE 2. Histograms of Lena and the cameraman images

BAYESIAN FORMULATION

In a ﬁrst step, we assume that the sources and the noise wavelet coefﬁcients are i.i.d. .
Thus, to simplify the notation, we denote, respectively, by x(k), s(k) and ξ(k) the

50

100

150

200

250

2500

2000

1500

1000

500

0
−500

50

100

150

200

250

2500

2000

1500

1000

500

50

100

150

200

250

50

100

150

200

250

FIGURE 3. Wavelet coefﬁcients of Lena and the cameraman images

−250

0

250

−250

0

250

500

500

0
−500

FIGURE 4. Histograms of the wavelet coefﬁcients of Lena and the cameraman images

vectors containing the wavelet coefﬁcients of the data, the sources and the noise for
a given index k. Thus, we have x(k) = As(k) + ξ(k). Hereafter, we omit the index k
and note it only when needed. To proceed with the Bayesian approach, we have to assign
the prior laws. In the following we assume:

• The noise wavelet coefﬁcients ξ are assumed independent and p(ξi) =

(αǫi, β).

GE

Then

p(x

A, s,

αǫi, β

) =

|

{

}

m

i=1 (cid:18)
Y

β
2αǫiΓ(1/β)

exp

(−

(cid:19)

m

i=1
X

(cid:0)

xi −

|

[As]i|

/αǫi

(6)

β

)

(cid:1)

• The wavelet coefﬁcients s of the sources are also assumed independent and p(sj) =

(αsj , βs). Then

GE

n

p(s

|

αsj , βs

) =

(cid:8)

(cid:9)

j=1 (cid:18)
Y

βs
2αsj Γ(1/βs)

exp

(−

(cid:19)

n

j=1
X

(cid:0)

/αsj

sj|

|

βs

)

(cid:1)

(7)

• The elements aij of the mixing matrix A are assumed i.i.d. and Gaussian with mean

values µij and variances σ2
ij:

p(aij) = (2πσ2

aij )−1/2exp

1
2σ2
aij

(−

(aij −

µij)2

)

(8)

Therefore, we may note by

p(A

|

M , Ra) = (2π)−mn/2
1
2

exp

−

−1/2

Ra|
|
Vect(A

−

(cid:26)

(cid:1)
, Vect(M ) means a vector containing the elements of the matrix

(cid:0)

(cid:0)

M )

tR−1
a

Vect(A

M )

(9)

−

(cid:27)
(cid:1)

where M =
M and

µij}
{

Ra (mn×mn) = diag(σ2

a11, σ2

a12, . . . , σ2

amn)

• All the hyperparameters ( 1
αβ
ǫi

, 1
αβs
sj

Gamma prior distributions p(x) =

(2, 1), where:

) are assumed independent and assigned standard

G

a, b) =

(x
|

G

xa−1
baΓ(a)

exp(

x
b

)

−

The joint a posteriori law of the sources coefﬁcients s, the mixing matrix A and the
hyperparameters θ is then given by:

p(s, A, θ

x)

p(x

s, A, θ) p(s

θ) p(A

θ) p(θ)

|

∝

|

|

|

where we noted all the hyperparameters

1
αβ
ǫi

, 1
αβs
sj (cid:19)

(cid:18)

by θ.

The conditional a posteriori laws of s, A and θ are then given by :

p(s

x, A, θ)

|

∝

β
2αǫiΓ(1/β)

m

m

i=1 (cid:18)
Y

exp

(−

|
(cid:0)

i=1
X

n

βs
2αsj Γ(1/βs)

(cid:19)

n

(cid:19)

j=1 (cid:18)
Y
[As]i|

xi −

β

/αǫi

−

(cid:1)

j=1
X

(cid:0)

/αsj

sj|

|

βs

)

(cid:1)

p(A

x, s, θ)

|

∝

(2π)−mn/2

−1/2

Ra|

|

β
2αǫiΓ(1/β)

(cid:19)

m

m

i=1 (cid:18)
Y

exp

(−

−

(cid:26)

(cid:0)

i=1
X
1
2

(cid:0)

xi −

|

[As]i|

/αǫi

β

)

−

(cid:1)
tR−1
a

(cid:1)

(cid:0)

exp

Vect(A

M )

Vect(A

M )

(13)

−

(cid:27)
(cid:1)

(10)

(11)

(12)

p

θi =

x, s, A

1
αβ
ǫi |

(cid:18)

∝

(cid:19)

p

θj =

x, s, A

1
αβs
sj |

(cid:18)

∝

(cid:19)

(cid:18)

K

K
β +1

1
αβ

ǫi (cid:19)

β
2Γ(1/β)

(cid:18)

exp

(cid:19)

(cid:18)
K

1
αβ
ǫi  −

(

Xk=1

xi(k)

|

[As(k)]i|

−

β + 1

!)

(14)

K
βs

+1

βs
2Γ(1/βs)

K

(cid:19)

 
K

1
αβs

sj !

exp

1
αβs
sj  −

(

sj(k)

|

βs + 1
|

!)

Xk=1

(15)

MCMC IMPLEMENTATION

x) of all the unknowns has
Once the expression of the joint a posteriori law p(s, A, θ
been derived, we can use it to infer them. However, in general, the computation of the
normalization factor needs a huge dimensional integration. When the MAP estimation is
chosen, this normalization factor is not needed, but it is formally needed for other esti-
mation rules such as the posterior mean. The MCMC algorithms are then the basic tools
to generate samples from the posterior law. The main idea is to generate successively
s(k), θ(k), x)
the samples from the posterior laws s(k)
and θ(k)
s(k), A(k), x) and then estimate their expected values by averaging these
samples.

A(k), θ(k), x), A(k)

p(A

p(θ

p(s

∼

∼

∼

|

|

|

|

We use the Hasting-Metropolis algorithm combined to a Gibbs sampler to obtain an
ergodic chain, and then approximate the ensemble expectation of any quantity Z by its
empirical mean:

E (Z)

≈

(N

−

1
T + 1)

N

Xt=T

h(Z (t))

where

Z (t)

are samples from p(z

.).

{

}

|

Noting that, when β = 2 and βs = 2, the posterior laws for the sources and for the
elements of the mixing matrix are Gaussian, we can use these Gaussian as the trial (or
instrumental) pdf. Thus, to simplify the presentation of the proposed algorithm, we give
here the expressions of these Gaussian posterior laws:

• The trial posterior pdf of the sources is Gaussian g(s

θ, x) =

(

s,

Rs) with

|

N

and

s = 2

RsAtR−1

αǫ x

b

b

Rs =

b
(AtR−1

b
1
2

αǫ A + R−1

αs )−1

b

(16)

(17)

where

N

and

where

Rαs (n×n) = diag(α2
Rαǫ (m×m) = diag(α2

s1, α2
ǫ1, α2

s2, . . . , α2
ǫ2, . . . , α2

sn)
ǫm)

• The trial posterior pdf of the mixing matrix elements is Gaussian g(Vect(A)

θ, x) =

(Vect(

M ),

Ra) with

c

b

Vect(

M ) =

Ra

2Vect(sxtR−1

αǫ ) + R−1

a Vect(M )

(cid:1)

c
Ra =

b
2

(cid:0)
EtR−1

αǫ E

(cid:0)

b

(cid:1)
(cid:0)
E(m×mn) = blockdiag
C(mn×mn) = blockdiag

C + R−1
a

−1

.

∗

(cid:1)

[1, . . . , 1](n×1), m
sst

(n×n), m

(cid:1)

(cid:0)

|

(18)

(19)

(cid:1)
where blockdiag(M , m) stands for a m block-diagonal matrix with matrix M
as the block elements, and A.
B stands for a point-wise multiplication of two
matrices, i.e. C = A.

B means Cij = AijBij.

∗

(cid:0)

∗
The proposed MCMC algorithm is then the following:

• Initialize s, A, θ to s0, A0, θ0 and repeat the following steps until convergence
• Sampling s(k), for k = 1 . . . K:

where

s and

Rs are given, respectively by eq. (16) and eq. (17) and

z   g(z

θ, x) =

(

s,

Rs)

|

N

b

b
with probability ρ
s(t)(k) with probability 1

z

ρ

−

b

b

s(t+1)(k) =

(cid:26)

with

ρ = min

1,

x(k), A, θ)

p(z
p(s(t)(k)

|

x(k), A, θ)

(cid:30)

g(z)
g(s(t)(k))

(cid:19)

(cid:18)
x(k), A, θ) is given by eq.(12).

|

where p(z
• Sampling A:

|

where

M and

z   g(z

θ, x) =

(Vect(

M ),

Ra)

N
Ra are given, respectively by eq. (18) and eq. (19) and

|

c

b

A(t+1) =

(cid:26)

c

b

Mat(z) with probability ρ
with probability 1

A(t)

ρ

−

with

ρ = min

1,

(cid:18)

x, s, θ)

p(z
p(Vect(A(t))

|

x, s, θ)

g(z)
g(Vect(A(t)))

|

(cid:30)

(cid:19)

where p(z

x(k), A, θ) is given by eq. (13).

|

• Sampling θi = 1
αβ
ǫi

, for i = 1 . . . m:

θ(t+1)  

(a, b)

G

a =

+ 2 and b =

K
β

K

|

 

Xk=1

−1

xi(k)

[As(k)]i|

−

β + 1

!

• Sampling θj = 1
αβs
sj

, for j = 1 . . . n:

θ(t+1)
j

 

(a, b)

G

a =

+ 2 and b =

K
βs

K

|

(cid:16)

Xk=1

sj(k)

βs + 1

|

−1

.

(cid:17)

SIMULATION RESULTS

To illustrate the performances of the proposed method, we consider two cases: a favor-
able case where we have 2 unknown sources with 3 measured data, and a more difﬁcult
64 pixel
case where we have only two measured data. In the ﬁrst case, we consider 64
images of the two images of Figure (1) with the following rectangular mixing matrix:

×

to generate the mixed images and added a white Gaussian noise of zero mean to obtain
the data with a SNR = 30dB, where SNR is deﬁned as being the ratio of the mixed signal
. Figure (5) shows the mixed
energy to that of the noise in dB: SNRdB = 10 log10
images obtained.

kxk2
kǫk2

(cid:16)

(cid:17)

A =

0.8211 0.4053
0.3769 0.7997
0.4287 0.4428 






10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

FIGURE 5. The mixed images in the rectangular case

with

with

10

20

30

40

50

60

We applied the proposed method directly on the mixed images where we assumed
noise to be i.i.d. and original images to be independent and Gaussian. Then, we ac-
counted for the local correlation between neighboring pixels through a Markovian mod-
eling of the original images. Finally, we applied the method in the wavelet domain.
Figure (6) shows the separated images obtained for each case.

(a)

(b)

(c)

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

PSNR = 0.0910

PSNR = 0.0651

PSNR = 0.1144

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

PSNR = 0.1712

PSNR = 0.1049

PSNR = 0.1306

FIGURE 6. Estimated source images : (a) Sources assumed independent, (b) Accounting for local
correlation in the sources, (c) Estimated sources obtained in the wavelet domain.

We may note that in this case which is an extremely favorable case the three different
methods give satisfactory results and it is not easy to really distinguish between these
three methods as it can also be noted from the PSNR’s of the reconstructed images
compared to the original images. We can, however, speculate that accounting for local
correlation of the image pixels outperforms the other two methods.

We have also considered a second case where we have an equal number of mea-
surements and sources (square case). The original source images where mixed with the
following matrix:

A =

(cid:20)

0.9088 0.4928
0.4172 0.8702

(cid:21)

and the same type of noise was added to obtained the data with a SNR = 30dB shown in
Figure (7).

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

FIGURE 7. The mixed images for the case of a square mixing matrix

Figure (8) shows the reconstructed images by the three methods of modeling the
source images, i.e. Gaussian i.i.d. , Gauss-Markov on pixels and GE on their wavelet
coefﬁcients.

(a)

(b)

(c)

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

PSNR = 0.2560

PSNR = 0.0740

PSNR = 0.1260

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

10

20

30

40

50

60

PSNR = 0.3150

PSNR = 0.0998

PSNR = 0.1660

FIGURE 8. Estimated source images : (a) Sources assumed independent, (b) Accounting for local
correlation in the sources, (c) Estimated sources obtained in the wavelet domain.

We should point out that we have used the following values for the initialization of

the algorithm:

A(0) =

1.0 0.0
0.0 1.0

(cid:20)

(cid:21)

, σ2(0)

= σ2(0)

1 = σ2(0)

2 = 1

α(0)
ǫi = √2 , β = 2
α(0)
sj = (2)1/βs , βs = 1.9

−→ (

The ﬁnal estimated values obtained by averaging the last 10% samples after 5000

iterations are the following:

A =

(cid:20)

0.8604 0.4681
0.5096 0.8837

ˆαǫ1 = 24.0966, ˆαǫ2 = 24.2096
ˆαs1 = 91.4272, ˆαs2 = 83.5939

(cid:21)

We may also note that the estimated values of αǫ1, αǫ2, αs1 and αs2 directly from the

b

original images are:

αǫ1 = 7.6457, αǫ2 = 7.2784
αs1 = 96.5342, αs2 = 107.9316
We notice that neither the noise variances nor the variance of the second image (the
cameraman) were well estimated. We clearly notice that in Figure (8). However, the sep-
aration of the images in the wavelet domain outperforms the separation applied directly
to the images assuming sources to be independent and this is due to the decorrelation
property of the wavelet transform. In fact, the wavelet transform nearly decorrelates a
signal, thus assuming independent wavelet coefﬁcients is more realistic than assuming
independent signal samples.

Figure (9) shows the rate of acceptance of the generated samples from the Gaussian

to approximate the posterior law of the wavelet coefﬁcients for βs = 1.9.

FIGURE 9. Rate of acceptance of the samples for the wavelet coefﬁcients along the iterations

We also noticed that this rate of acceptance is a function of the parameter βs:

ρ

0

ց

as βs ց

1

and

as βs ր
Figure (10) shows the convergence of the elements of the matrix A and Figure (11)

ր

ρ

2

1

shows the convergence of the hyperparameters.
Figure (12) shows the histograms of the original and estimated images while Figure (13)
shows the histograms of the wavelet coefﬁcients of the original images superposed with
the Exponential pdf with parameter α estimated with the algorithm.

1

0.75

0.5

0.25

1

0.75

0.5

0.25

200

0

1

200

0

1

1

0.75

0.5

0.25

0

1

1

0.75

0.5

0.25

0

1

a11

a21

αǫ

200

200

a12

a22

αs

FIGURE 10. Convergence of the elements of A during the ﬁrst 200 iterations

FIGURE 11. Convergence of the hyperparameters θ: Left: αǫ1 and αǫ2 Right: αs1 and αs2.

CONCLUSIONS AND PERSPECTIVES

In this contribution we proposed an approach to jointly estimate the mixing matrix
and the original source images. We transported the problem to the wavelet domain
using a Bayesian approach where the wavelet coefﬁcients of real world images are
naturally modeled by generalized exponential distributions. Independence of the wavelet
coefﬁcients of signals is more realistic than the independence of the signals themselves.
In a ﬁrst step, we assumed all the wavelet coefﬁcients to be independent and iden-
tically distributed and follow a GE pdf with a ﬁxed value for its parameter βs while
its second parameter is estimated during the iterations. Even if this gives satisfactory
results, it will be better to estimate βs too during the iterations.

A second point is that the choice of a Gaussian trial pdf is good when βs is not far

from 2, but it seems that this choice is no more efﬁcient when βs approaches 1.

Finally, since the wavelet coefﬁcients of real world signals (images) tend to propagate
through scales, a future work is to put a Markovian model on the wavelet coefﬁcients
taking into account inter-scale correlation of the coefﬁcients.

350

300

250

200

150

100

50

350

300

250

200

150

100

50

0

0

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
−1000

350

300

250

200

150

100

50

350

300

250

200

150

100

50

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
−1000

50

100

150

200

50

100

150

200

250

250

0

0

50

100

150

200

50

100

150

200

250

250

0

0

FIGURE 12. The histogram of: (a) Original source images, (b) The estimated images (top: Lena image,
bottom: The cameraman image)

(a)

(a)

(b)

(b)

−800

−600

−400

−200

0

200

400

600

800

1000

−800

−600

−400

−200

0

200

400

600

800

1000

FIGURE 13. The histogram of the wavelet coefﬁcients of the original source images superposed with
the pdf of the estimated images of: (a) Lena image, (b) The cameraman image

REFERENCES

1.

Felix Abramovich and Yoav Benjamini. Thresholding of wavelets coefﬁcients as multiples hypothe-
ses testing procedure. Wavelets and Statistics, A. Antoniadis and G. Oppenheim Eds, Lecture Notes
in Statistics, pages 5–14, 1995.

2. A. Antoniadis, D. Leporini, and J.C. Pesquet. Wavelet Thresholding for some classes of Non-

Gaussian Noise. Satistica Neerlandica, 2000.

3. H. Attias. Independent factor analysis. Neural Computation, 11:803–851, 1999.
4. O. Bermond and J.-F. Cardoso. Approximate likelihood for noisy mixtures. In Proc. First Interna-
tional Conference on Independent Component Analysis and Blind Source Separation ICA’99, pages
325–330, Aussois, France, January 11–15, 1999.
Jean-François Cardoso. Infomax and maximum likelihood for source separation. IEEE Letters on
Signal Processing, 4(4):112–114, avril 1997.
Jean-François Cardoso. Blind signal separation: statistical principles. Proceedings of the IEEE.
Special issue on blind identiﬁcation and estimation, (10):2009–2025, octobre 1998.

5.

6.

7.

8.

Jean-François Cardoso. High-order contrasts for independent component analysis. Neural Compu-
tation, 11(1):157–192, janvier 1999.
Jean-François Cardoso and Pierre Comon.
algebraic methods. In Proc. ISCAS’96, volume 2, pages 93–96, 1996.

Independent component analysis, a survey of some

9. David L. Donoho. Nonliner Solution of Linear Inverse Problems by Wavelet-Vaguelette Decompo-

sition. IEEE Transactions on Signal Processing, April 1992.

10. David L. Donoho. De-noising by soft-thresholding. IEEE Trans. Inf. Theory, 1995.
11. Aapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independant Component Analysis. John Wiley &

Sons Inc., 2001.

12. K. Knuth. Bayesian source separation and localization. In A. Mohammad-Djafari, editor, SPIE’98

Proceedings: Bayesian Inference for Inverse Problems, San Diego, CA, pages 147–158, July 1998.

13. K. Knuth. A Bayesian approach to source separation. In C. Jutten J.-F. Cardoso and P. Loubaton,
editors, Proceedings of the First International Workshop on Independent Component Analysis and
Signal Separation: ICA’99, Aussios, France, pages 283–288, 1999.

14. K. Knuth and H.G. Vaughan JR. Convergent Bayesian formulation of blind source separation and and
electromagnetic source estimation. In Fischer R. von der Linden W., Dose W. and Preuss R., editors,
MaxEnt 98 Proceedings: Int. Workshop on Maximum Entropy and Bayesian methods, Garching,
Germany, page in press, 1998.

15. J.-L. Lacoume. A survey of source separation. In Proc. First International Conference on Indepen-
dent Component Analysis and Blind Source Separation ICA’99, pages 1–6, Aussois, France, January
11–15, 1999.

16. S. E. Lee and S. J. Press. Robustness of Bayesian factor analysis estimates. Communications in

Statistics – Theory And Methods, 27(8), 1998.

17. T. Lee, M.S. Lewicki, M. Girolami, and T. Sejnowski. Blind source separation of more sources than

mixtures using overcomplete representation. IEEE Signal Processing Letters, page in press, 1999.

18. Te-Won Lee.

Independant Component Analysis "Theory and Applications". Kluwer Academic

Publishers, Boston, Dordrecht, London, 1998.

19. R. B. MacLeod and D. W. Tufts. Fast maximum likelihood estimation for independent component
In Proc. First International Conference on Independent Component Analysis and Blind

analysis.
Source Separation ICA’99, pages 319–324, Aussois, France, January 11–15, 1999.

20. Stephane Mallat. a Wavelet Tour of Signal Processing. Academic Press, 1999.
21. A. Mohammad-Djafari. A Bayesian Approach to Source Sepration. Boise, Idaho, USA, 1999. 19th

Int. workshop on Bayesian and Maximum Entropy methods, MaxEnt.

22. Ali Mohammad-Djafari. Bayesian inference and maximum entropy methods. In Ali Mohammad-
Djafari, editor, Bayesian Inference and Maximum Entropy Methods, Gif-sur-Yvette, juillet 2000.
MaxEnt Workshops, à paraître dans Amer. Inst. Physics.

23. Pierre Moulin and Juan Liu. Analysis of multiresolution image denoising schemes using generalized-

gaussian and complexity priors. IEEE Transactions on Information Theory, April 1999.

24. E. Oja. Nonlinear PCA criterion and maximum likelihood in independent component analysis.
In Proc. First International Conference on Independent Component Analysis and Blind Source
Separation ICA’99, pages 143–148, Aussois, France, January 11–15, 1999.

25. S. J. Press. Applied Multivariate Analysis: Using Bayesian and Frequentist Methods of Inference.

Robert E. Krieger Publishing Company, Malabar, Florida, 1982.

26. S. J. Press and K. Shigemasu. Bayesian inference in factor analysis. In Contributions to Probability

and Statistics, chapter 15. Springer-Verlag, 1989.

27. J. J. Rajan and P. J. W. Rayner. Decomposition and the discrete karhunen-loeve transformation using
IEE Proceedings - Vision, Image, and Signal Processing, 144(2):116–123,

a bayesian approach.
1997.

28. S. J. Roberts.

Independent component analysis: Source assessment, and separation, a Bayesian

approach. IEE Proceedings - Vision, Image, and Signal Processing, 145(3), 1998.

29. Justin K. Romberg, Hueokho Choi, and Richard G. Baraniuk. Bayesian Tree-Structured Image
Modeling using Wavelet-domain Hidden Markov Models. IEEE Transactions on Image Processing,
March 2000.

30. F. Abramovich T. Sapatinas and B.W. Silverman. Wavelet thresholding via a bayesian approach. The

Royal Statiscal Society B, (60):725–749, 1998.

31. Eero P. Simoncelli. Bayesian Denoising of Visual Images in the Wavelet Domain. Lecture Notes in

Statistics, 141:291–308, March, 30 1999.

32. H. Snoussi, G. Patanchon, J.F. Macías-Pérez, A. Mohammad-Djafari, and J. Delabrouille. Bayesian
blind component separation for cosmic microwave background observations. In Robert L. Fry, editor,
Bayesian Inference and Maximum Entropy Methods, pages 125–140. MaxEnt Workshops, Amer.
Inst. Physics, août 2002.

33. Hichem Snoussi and Ali Mohammad-Djafari. Approche bayésienne pour la séparation de sources.

Rapport de stage de DEA-ATS, GPI–L2S, 2000.

34. Hichem Snoussi and Ali Mohammad-Djafari. Bayesian source separation with mixture of gaussians
In Ali Mohammad-Djafari, editor,
prior for sources and gaussian prior for mixture coefﬁcients.
Bayesian Inference and Maximum Entropy Methods, pages 388–406, Gif-sur-Yvette, juillet 2000.
Proc. of MaxEnt, Amer. Inst. Physics.

35. Hichem Snoussi and Ali Mohammad-Djafari. Dégénérescences des estimateurs MV en séparation

de sources. Technical report ri-s0010, GPI–L2S, 2001.

36. Hichem Snoussi and Ali Mohammad-Djafari. Séparation de sources par une approche bayésienne

hiérarchique. In Actes 18e coll. GRETSI, Toulouse, septembre 2001.

37. Hichem Snoussi and Ali Mohammad-Djafari. Unsupervised learning for source separation with
mixture of Gaussians prior for sources and Gaussian prior for mixture coefﬁcients. In David J.Miller,
editor, Neural Networks for Signal Processing XI, pages 293–302. IEEE workshop, septembre 2001.
38. Hichem Snoussi and Ali Mohammad-Djafari. Bayesian separation of HMM sources. In Robert L.
Fry, editor, Bayesian Inference and Maximum Entropy Methods, pages 77–88. MaxEnt Workshops,
Amer. Inst. Physics, août 2002.

39. Hichem Snoussi and Ali Mohammad-Djafari. Penalized maximum likelihood for multivariate gaus-
sian mixture. In Robert L. Fry, editor, Bayesian Inference and Maximum Entropy Methods, pages
36–46. MaxEnt Workshops, Amer. Inst. Physics, août 2002.

40. Petre Stoica, Björn Ottersten, Mats Viberg, and Randolph L. Moses. Maximum likelihood array

processing for stochastic coherent sources. Signal Processing, 44(1):96–105, janvier 1996.

41. M. E. Tipping and C. M. Bishop. Mixtures of probabilistic principal components analysis. Neural

Computation, 11:443–482, 1999.

42. Yi Wan and Robert D. Nowak. A Multiscale Bayesian Framework for Linear Inverse Problems and
Its Application to Image Restoration. IEEE Transactions on Image Processing, January 2001.
43. M. Wax. Detection and localization of multiple sources via the stochastic signals model. IEEE Trans.

Signal Processing, 39(11):2450–2456, novembre 1991.

44. Michael Zibulevsky and Barak A. Pearlmutter. Blind Source Separation by Sparse Decomposition.
Technical report, Computer Science Dept, FEC 313, University of Mexico, Albuquerque, NM 87131
USA, July 19 1999.
I. Ziskind and M. Wax. Maximum likelihood localization of multiple sources by alternating projec-
tion. IEEE Trans. Acoust. Speech, Signal Processing, ASSP-36(10):1553–1560, octobre 1988.

45.


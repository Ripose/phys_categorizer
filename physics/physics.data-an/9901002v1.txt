9
9
9
1
 
n
a
J
 
5
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
0
0
1
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Anomalous jumping in a double-well potential

P. D. Ditlevsen
The Niels Bohr Institute, Department for Geophysics,
University of Copenhagen, Juliane Maries Vej 30,
DK-2100 Copenhagen O, Denmark.
(February 2, 2008)

Noise induced jumping between meta-stable states in a potential depends on the structure of the
noise. For an α-stable noise, jumping triggered by single extreme events contributes to the transition
probability. This is also called Levy ﬂights and might be of importance in triggering sudden changes
in geophysical ﬂow and perhaps even climatic changes. The steady state statistics is also inﬂuenced
by the noise structure leading to a non-Gibbs distribution for an α-stable noise.

I. INTRODUCTION

Noise induced jumping between meta-stable states separated by potential barriers is common in physical systems.
The time scale for the barrier penetration depends on the structure of the noise. Most often the noise is Gaussian.
However, non-Gaussian noises distributed with power-function tails, Levy ﬂights, are observed in many diﬀerent
physical systems [1] such as turbulent diﬀusion [2,3] and vortex dynamics [4]. Levy ﬂights also seems to be a common
feature in dynamical models [5] and critical phenomena [6].

r(t)

The Levy ﬂights can result from a Langevin equation driven by α-stable noise and give rise to anomalous diﬀusion
Dt2/α where D is a constant and 0 < α < 2 [16].
2
of a random walker with position r(t) such that
r(0)
|
The case α = 2 corresponds to normal diﬀusion where D is the diﬀusion constant. The exponent α is related to the
scaling of the tail of the probability distribution for the increments of the random walker, P (X > r)
2
the second moment exists and by the central limit theorem the random walker reduces in the continuum limit to a
Gaussian random walker unless the diﬀusion takes place on a fractal set like in a quenched random medium [7]. In
this case the random walk can be sub-diﬀusive. Another example of a process which can be sub-diﬀusive is the Levy
walk where a random walker has a constant speed in between discrete stochastic time points (a renewal process) with
a power-function tail distribution. Note that since the time process is discrete for a Levy walk it cannot result from
a Langevin equation.

r−α. For α

i ∝

−

∝

≥

h|

Anomalous diﬀusion was ﬁrst observed in hydrological time-series [9]. Recently evidence for α-stable statistics in
atmospheric circulation data has been reported [10]. In a long paleoclimatic time-series an α-stable noise induced
jumping in a double-well potential was found [11]. In both cases α was found to be around 1.7. The latter describes
a jumping, in glacial times, between two climatic states governed by the oceanic ﬂow forced by random ﬂuctuations
from the atmosphere. Understanding the role of extreme events and the time-scales for these climatic shifts is the
main motivation for this study.

In this paper we will interchangeably use the physics jargon,

, and the mathematics jargon, E[x], for the
x
i
h
expectation value for x. The latter will be used in the case of conditional expectations. We use the usual convention
that probability distribution functions, P , are capitalized and probability density functions, p = dP/dx, are in small
letters.

II. THE α-STABLE DISTRIBUTIONS

≥

For distributions with power-function tails, P (X > x)
for β

=
γ). For 0 < γ < 2 a generalized version of the central limit theorem applies, namely that the average of n

x−γ, only moments of order less than γ exists (

∞
independent stochastic variables from the distribution P asymptotically will have an α-stable distribution as n
→ ∞
α/α).
with α = γ. The α-stable distributions are deﬁned by their characteristic functions,
k
|
|
n
The α-stable distributions are stable with respect to averaging, Yn = n−1/α
i=1 Xi, meaning that Yn has the same
distribution as Xi where the Xi’s are i.i.d. (independent identically distributed) α-stable, thus the phrase ’α-stable’.
As for the case of Gaussian noise, the dynamics of a noise driven system with power-function tail distributions for
x−α, 0 < α < 2, will reduce to a system with an α-stable noise in the continuum
the noise increments, P (X > x)
limit, described by a Langevin equation [8],

exp(ikX)
i
h

= exp(

β
x
|

σα

P

∝

∝

−

h|

i

1

A random walker with α-stable noise increments will be super-diﬀusive due to the large jumps from the tails of the

distribution surviving the averaging in the continuum limit. See Appendix A for a further short description.

dX = f (X)dt + σ(X)dLα.

(2.1)

III. THE FOKKER-PLANCK EQUATION

The probability density for X in (2.1) is determined from the Fokker-Planck equation (FPE), see Appendix B for

a derivation,

p(k).
The second term on the right hand side is expressed in terms of the Fourier transformed probability density,
This term reduces to the ordinary diﬀusion term, ∂x2[σ2(x)p(x)]/2. when α = 2. In this case the solution for the
stationary probability density function can be expressed explicitly in the well-known form,

b

∂tp(x) =

∂x[f (x)p(x)]

1
α Z Z

−

e−ikx

σα(k

−
α
k
k1)
|
|

−

p(k1)dkdk1.

c

b

p(x)

1
σ2(x)

∝

2

exp
{

Z
0

x

f (y)
σ2(y)

dy

,

}

b

b

For α < 2 the FPE (3.1) is non-local in spectral space. This is a reﬂection of the super-diﬀusivity of the process
(2.1). Besides the Gaussian case we can only solve the FPE explicitly for α = 1. This is the case of a system driven
by Cauchy distributed noise having the probability density, q(x) = 1/[π(1 + x2)], see Appendix C for further details
on the Cauchy distribution. We are using p(x) for the probability density for X in (2.1) and q(x) for the probability
density of the noise. Then the stationary FPE becomes,

where the noise intensity σ is taken to be constant. From taking the derivative with respect to k on both sides of
(3.3) and performing a partial integration on the left-hand side it follows that

k)

p(k1)dk1 = sgn(k)σ

p(k)

i

Z

f (k1 −
b

b

for any m. The solution is

p(k) = e−λ|k|, where λ is determined by

k)

p(m)(k1)dk1 = sgn(k)σ

p(m)(k)

i

Z

f (k1 −
b

b

b

i

Z

f (k1 −
b

k)e−λ(k1−k)dk1 = if (iλ) = sgn(k)σ.

Thus the solution is determined by the analytic continuation of f (x) into the complex plane, provided it exists. Note
that the solution also apply for k = 0 where the r.h.s of (3.4) jumps, since from the deﬁnition of the Fourier transform
of the probability density we have ˆp(0) = E[1] = 1. By complex conjugation of (3.5) we get if (
k)σ,
λ∗, where λ solves (3.5) for k > 0. With λ = β + iδ the characteristic function
so for k < 0 the solution is given as
p(k) = e−β|k|e−iδk. For
p(k) to be a characteristic function we must have β > 0, and the stationary
is given as
distribution is
b

iλ∗) = sgn(

−

−

−

b

p(x) =

N

Xi=1

pi

1
π

βi
β2
i + (x + δi)2

where the sum is over the N zero points of the complex function if (iλ)
this solution of the stationary FPE there is an indeterminacy since any p(x) with
that satisﬁes (3.1).

−

sgn(k)σ in the upper half-plane (β > 0). In
i pi = 1 is a probability density

P

The indeterminacy might be related to the problem of conservation of probability. If there is a ﬁnite probability
for the random walker to escape to inﬁnity, it must be reinserted into the system for a stationary probability density
to be conserved. Then the indeterminacy in the reinsertion could result in the indeterminacy in the coeﬃcients pi in
0. When the intensity of the noise becomes small the
(3.6). However, the indeterminacy can be lifted in the limit σ

→

2

(3.1)

(3.2)

(3.3)

(3.4)

(3.5)

(3.6)

Cauchy distribution approaches a δ-distribution (when acting on functions that are bounded by
as x
equation (3.3) is approximated by N transition (Master) equations for the weights pi, i = 1, ...N ,

β for some β < α
x
|
|
). Then we can approximate the system by a system with discrete states and the stationary Fokker-Planck

→ ∞

where i, j represents the N minima deﬁned in (3.6). The transition probabilities p(i
waiting times which will be deﬁned in the following.

→

j) are related to the transition

pi = Σjpjp(j

i),

→

IV. THE POTENTIAL

Before proceeding we will deﬁne the drift term as resulting from a potential. The governing equation then describes
dU/dx. As an example for study we deﬁne the potential

a massless, viscous particle moving in a potential, f (x) =
as

−

U (x) = 4(x/∆)4 + h(x/∆)3

8(x/∆)2

3h(x/∆).

−

−

∆

U (x) is a double-well potential for
x =
a and x = ∆
potential values are [U (a), U (b), U (c)] = [
are readily generalized to other forms of the potential U (x).

c. The local potential maximum between the two minima is at x =

16/3 < h < 16/3. 4h is the level diﬀerence between two potential minima at
b, and the
4(1 + h/2)]. See ﬁgure 1. The results

h/2), (3h/16)2(8

3h2/64),

3h∆/16

4(1

−

−

≡

−

≡

−

−

−

≡

−

(3.7)

(4.1)

FIG. 1. The potential (4.1). 4h = U (a) − U (c) is the potential diﬀerence between the two minima, w = b − a is the ’left

half-width’. Units are arbitrary.

V. WAITING TIME

The waiting time for jumping between the two potential minima (from a to c) of U (x) deﬁned above is exponentially
distributed. With pac(τ > t) being the probability of staying in minimum a longer than t we have pac(τ > t) =
t/Tac) with a mean waiting time Tac. This follows from the Markov property of the Langevin equation in the
exp(
discrete state limit, since we have

−

P (t < τ < t + ∆t)/∆t = (1

λac∆t)t/∆tλac
λact),

−
λac exp(

−

→

(5.1)

as ∆t
0, where λac = 1/Tac is the transition probability intensity. In the non-discrete case, a little more rigorous
treatment is needed [12]. However, the result holds, if the potential wells are substituted for the minima, and the
waiting time is deﬁned as the time between consecutive crossings of a and c.

→

3

A. Gaussian noise and Arrhenius formula

Tac ≈

b

2
σ2 Z

−∞

dxe−2U(x)/σ

dye2U(y)/σ

,

2

c

2

Z
a

In the case of Gaussian noise in (2.1) Tac can be calculated from the backward Fokker-Planck equation [12],

and correspondingly for Tca. By using the saddle-point approximation on (5.2) we obtain the Arrhenius formula,

Tac ∝

exp(2[U (b)

U (a)]/σ2).

−
For comparison with the case of α-stable noise ﬁgure 2 displays the standard result of a numerical simulation in
the case of Gaussian noise. Figure 2 (a) shows the simulated process with the potential in ﬁgure 1. 2 (b) shows
the simulated probability density function and the right-hand side of (3.2). Figure 2 (c) shows the time-scale for
jumping as a function of the parameter h. The time-scale is calculated from the exponential distribution of times
between consecutive crossings of the levels a and c. Figure 3 shows the number of crossings (from a to c and from
c to a respectively) with a waiting-time larger than each waiting-time measured, normalized by the total number of
crossings. These points are situated on straight lines in the semi-logarithmic plot where Tac and Tca are the slopes
of the lines. Figure 2 (c) shows the time-scales for seven simulations with diﬀerent h. The curves are the time-scales
calculated from (5.2).

(5.2)

(5.3)

FIG. 2. A simulation of (2.1) with Gaussian noise and the potential shown in ﬁgure 1. (a) shows a realization and (b) the
probability density function. The actual simulation is 1000 times longer than what is shown in (a). The smooth curve in (b) is
the pdf calculated from (3.2). (c) shows the mean waiting times, Tac (diamons) and Tca (triangles), for seven simulations with
varying h. The curves are the waiting times calculated from (5.2). Units are arbitrary.

FIG. 3. The probability for waiting longer than t before jumping to the other well as a function of t obtained from the

simulation. The slope of the upper curve gives Tca and the slope of the lower curve gives Tac. Units are arbitrary.

In the case α < 2 the situation is radically diﬀerent. The sample curves of the process are no longer continuous
and the ﬁnite jumps or extreme events will contribute to the probability of jumping between the potential wells. The

B. α-stable noise

4

probability, (λac∆t + o(∆t)), for jumping from the left well, x < b to any y > b in a single jump in a time interval ∆t
(x/σ)−(α+1)∆t/σ. This is seen by observing that the process (2.1)
is governed by the tail of the distribution, p(x)
∝
can be obtained from the discrete process, X(t + ∆t) = X(t) + f (X(t))∆t + [σ∆t1/α]η(t), for ∆t
0, where η(t) has
an α-stable distribution with unit intensity. Thus we have

→

λac∆t
∞

P (X(t + ∆t) > b
∞

≈

b

X(t) < b)/P (X(t) < b)
|

[
Z

Z

−∞

b−x

p(u)du]˜p(x)dx

≈ Z

b−a

p(u)du

[(b

≈

−

∝
a)/σ]−α∆t

The inner integral is the probability of jumping from x < b to any y > b, and ˜p(x) is the stationary probability
density. The outer integral is dominated by the central part of the probability distribution. This result is exact in
the ∆t

0 limit where p(x)

a). Thus we have

0, σ

δ(x

→

→

→

−

Tac = c(α)[(b

a)/σ]α

−
where c(α) is some constant. So in this case we see that the waiting time scales with the ’left half-width’ of the
barrier, b
w, to the power α. The height of the barrier has no inﬂuence on the transition probability. The
results are conﬁrmed by numerical simulation. Figure 4 displays the numerical simulation using Cauchy noise, α = 1,
and the same potential as in the case displayed in ﬁgure 2. Note the linear scale in ﬁgure 4 (c) showing the scaling of
the time-scale with w.

−

≡

a

FIG. 4. The same as ﬁgure 2 but with Cauchy noise, α = 1. Note the linear axis in (c). The curves are obtained from (5.5).

VI. THE STATIONARY DISTRIBUTION

For the Cauchy noise driven system the indeterminacy in (3.6) can now be resolved by use of the Master equation
(3.7).
0 the system can be approximated as a discrete two state system, with the two states
corresponding to the two potential minima, at a and c. In this limit the system fulﬁll the stationary Master equation,

In the limit σ

→

The transition probabilities are now p(a

a)

(c

b)/σ and we get,

→

∝

−

0 = pap(a

c)

pcp(c

a).

→
−
1/Tac ∝
pc = (b

−

(b

−
a)/(c

c)

→
pa = 1

∝

−

a).

−

→
a)/σ and p(c

U (c)]/σ2), which in the Gaussian case corresponds to the Gibbs
Note that this is independent of exp(
distribution. Figure 5 shows the distribution pa, which is diﬀerent from the Gibbs distribution, as a function of w.
Figure 4 (b) shows the probability density function from the simulation over-plotted the one calculated from (3.6)
and (6.2).

2[U (a)

−

−

5

(5.4)

(5.5)

(6.1)

(6.2)

FIG. 5. The probability, pa = 1 − pc, for ﬁnding the particle in the left well as a function of w in the simulation with Cauchy

noise. The curve is obtained using (6.2), The distribution is deviating strongly from a Gibbs distribution.

VII. BARRIER PENETRATION

When α is close to 2 we should expect the ’single jump penetration’ of the barrier to become more and more unlikely
and the continuous penetration dominating. The Levy decomposition theorem [8] states that the α-stable process
can be decomposed in a Brownian process and a compound Poisson process. The ’continuous’ barrier penetration can
be estimated by considering the distribution to be truncated so that there are no jumps larger than the half-width
of the barrier, w. The truncated probability for the noise, pt(x), is then deﬁned by pt(x)
< w and
pt(x) = 0 for
w. This part of the noise now has ﬁnite second order moment and we can estimate the variance as
w2−α asymptotically for large w or small noise intensity σ. The waiting time can be estimated
σ2
eﬀ ∝ R
as,

x
| ≥
|
x2 ˜p(x)dx

p(x) for

x
|
|

∝

∝

where c denotes ’continuous’. Note that this part of the process is not strictly continuous, since it contains jumps
smaller than w. The time-scale for single jump penetration can be estimated from (5.5),

T c

∝

σ−2
eﬀ exp(2[U (b)

U (a)]/σ2

eﬀ),

−

where d denotes ’discontinuous’ and we have,

T d

wα,

∝

T d
T c ∝

w2 exp(

˜c[U (b)

U (a)]wα−2),

−

−

where ˜c is a constant. So the relative importance of extremal jumping depends both on the height and the width
of the barrier. To illustrate the relative importance of two jumping processes a simulation of (2.1) with an α-stable
noise [13] with α = 1.7 and a potential (4.1) with h = 3, was performed. Figure 6 shows part of a realization of
this process. Here it is seen that the jumping from the deep to the shallow well is governed by the discontinuous
part, T d(c
a), while the jumping from the shallow to the deep well is dominated by the ’continuous’
part, T d(a
c). For proportioning the continuous and discontinuous processes in a given situation the
prefactor and the constant, ˜c, in (7.3) must be calculated or estimated.

T c(c
T c(a

→
→

→
→

≪
≫

a)
c)

(7.1)

(7.2)

(7.3)

6

FIG. 6. A realization of the process with α = 1.7. The potential used is shown in the insert. The jumping from the left
(shallow) well to the right (deep) well is triggered by the - almost - normal diﬀusion. The jumping the other way is driven by
the tail of the α-stable distribution, the extreme events.

VIII. SUMMARY

We have seen that the statistics of noise induced jumping between meta-stable states in a potential is diﬀerent for
α-stable noise from the usual Gaussian noise case. The stationary probability distribution deviates from the Gibbs
distribution, and the waiting time for jumping depends in some cases more on the width than on the height of the
barrier. This is the case where a single extreme event triggers the jumping. These observations might be of importance
for understanding the triggering mechanisms of climatic changes, where the ﬂow state of the ocean is trapped in a
potential minimum, a stable climatic state. This ﬂow is stochastically forced by the atmospheric ﬂow. There are
some evidence that this stochastic forcing is α-stable rather than Gaussian such that climatic shifts from one state to
another could be triggered by single extreme events. This would perhaps explain why the climate models at present
are not capable of reproducing the climatic changes observed in the geological records. The models are too coarse
grained and contains to much diﬀusive smoothening to allow for extreme events.

I would like to thank O. Ditlevsen for valuable discussions. The work was funded by the Carlsberg foundation.

IX. ACKNOWLEDGEMENT

APPENDIX A: THE ADDITION OF α-STABLE RANDOM VARIABLES.

Textbooks on α-stable processes are now available [14,13], but for those readers not familiar with the α-stable

distributions and processes a few notes are added in the following.

When

Xi, i = 1, ..., n

is a series of i.i.d. random variables, the distribution of the variable Y = c(n)

be determined from the characteristic function,

{

}

n
j=1 Xj can

P

If the distribution for Y is the same as for X the equation (A1) for the characteristic function, f (k) =

(A1)

is,

exp[ikY ]
i
h
(A2)

exp[ikY ]
i
h

=

exp[ikc(n)
h

Πn
h

j=1 exp[ikc(n)Xj]
i

=

n

Xj]
i

Xj=1
eikc(n)X
h

=

n.
i

f (k) = f (c(n)k)n

7

with the solution

f (k) = exp[

σα

α/α],
k
−
|
|
c(n) = n−1/α.

The constant, σα/α is chosen so that it coinsides with the usual notation in the gaussian case, α = 2. Only for α > 0
does (A3) represent a characteristic function. It can be shown that the characteristic function (A3) corresponds to
x−α [15,14]. For α > 2 the second moment of the distribution
distributions with power-function tails, P (X > x)
exists and sums of i.i.d. variables converges by the central limit theorem to the gaussian distribution, α = 2. For
0 < α < 2 the distributions has a domain of attraction in the sense that sums of i.i.d. random variables with tail
x−γ, under rather general conditions converges to an α-stable distribution with α = γ.
distributions, P (X > x)
This is the generalization of the central limit theorem for α-stable distributions. The proof of this is similar to the
proof of the central limit theorem for the normal distribution. It basically substitudes a limit, ˜f (c(n)k)n
f (k) for
(A2). The proof can be found in Fellers book, pp 574 – 581 [15].

→

∼

∼

Now we can intuitively understand the noise term, dLα, in the Langevin equation (2.1) as the continuum limit of

addition of small increments,

∆Lα(∆t) =

X(j∆t/m)

1
m1/α

m

Xj=1

x−α, and unit intensity. In the limit,
, ∆Lα will be an α-stable noise. It follows from (A4) that dLα = dt1/α, which in the gaussian case is the

where X(t) is a random process with power-function tails, P (X(t) > x)
m
well-known relation, dB2 = dt.

→ ∞

∼

For α < 2 the α-stable variables have inﬁnite variance. This concept can be diﬃcult to comprehend when considering
measurements from a given physical system. In the case a sample is taken, say of n measurements of the variable
X, where X has an α-stable distribution with stability index α, then of course any of the measurements, x1, ..., xn
n)/n, is some ﬁnite number. The variable Y = X 2 will
of X is ﬁnite so that the sample variance, (x2
have a tail distribution given by P (Y > x2) = P (X > x)
x−α = y−α/2, so that, asymptotically for large n,
n−2/α(Y1 +...+Yn) will have an α-stable distribution with stability index α/2. Imagine now that we estimate the
Zn ≡
n)/n = n2/α−1Zn.
(inﬁnite) variance of variable X by taking samples of length n, estimating the variance as (X 2
Then the estimate itself will be an α-stable process with stability index α/2 and intensity n2/α−1. This estimate will
be ﬂuctuating with an intensity growing with n for α < 2.

1 + ... + x2

1 +...+X 2

∼

APPENDIX B: THE FOKKER-PLANCK EQUATION

In the following the Fokker-Planck equation (3.1) corresponding to the Langevin equation (2.1) will be derived.
The Fokker-Planck equation will be derived in spectral form using that the α-stable processes are deﬁned by their
characteristics functions. Following the lines of Stratonovich [17] we deﬁne the functional

where

I =

R(y)∂tp(x0|

y, t)dy = lim
∆t→0

I∆t

Z

I∆t =

1
∆t Z

R(y)[p(x0|

y, t + ∆t)

p(xo|

−

y, t)]dy

R(y) is an arbitrary generator function, and p(x0|
to passing from x0 to x1 during time t. Assuming stationarity we suppress the ﬁrst temporal index, p(x0|
x1, τ +t). For simplicity of writing we make the convention that
p(x0, 0
|

x1, t) is the conditional probability density at x1 corresponding

is to be read as (1/√2π)

x, t) being a probability density in x we trivially have

x1, t)
≡
∞
−∞.
R

R

x1, t) = p(x0, τ
|
With p(x0|

(A3)

(A4)

(A5)

(B1)

(B2)

(B3)

and the Chapman-Kolmogorov equation

∞

Z

−∞

p(x0|

x, t)dx = 1

8

For the functional we then get

We now deﬁne the Fourier transforms

p(x0|

x1, t) =

p(x0|

x1, t
x, τ )p(x
|

−

τ )dx.

∞

Z

−∞

1
∆t Z

R(y)[

∞

Z

−∞

=

√2π
∆t Z

y, ∆t)p(x0|
p(x
|

x, t)dx

p(x0|

−

y, t)]dy

p(x0|

x, t)

{Z

y, ∆t)[R(y)
p(x
|

−

R(x)]dy

dx

}

I∆t =

R(x) =

ˆR(k)eikxdk, ˆR(k) =

R(x)e−ikxdx,

Z

Z

p(x0|

x, t) =

ˆp(x0|

Z

k, t)e−ikxdk,

ˆp(x0|

k, t) =

p(x0|

Z

x, t)eikxdx

similarly for f (x) in (2.1), and for σα(x) to be introduced below. However, for the probability density p(x0|
deﬁne

x, t) we

consistent with the standard deﬁnition of characteristic function except for the factor √2π. With these deﬁnitions it
is easy to derive the formula

f (x)p(x0|

Z

x, t)eikxdx =

ˆf (k1 −

k)ˆp(x0|

Z

k1, t)dk1

from which it directly follows that

f (x)p(x0|

x, t) =

[
Z

Z

ˆf (k1 −

k)ˆp(x0|

k1, t)dk1]e−ikxdk

Using the spectral representation for the generator function we get

1
√2π

I∆t =

=

Z Z

x, t)

p(x0|
Z
{Z
eikx ˆR(k)p(x0|

x, t)

1
∆t

y, ∆t)
p(x
|

1
∆t Z

ˆR(k)(eiky

eikx)dk

dy

−

}

E[eik[X(t+∆t)−x]

1

X(t) = x]dkdx
|

−

The conditional expectation is evaluated using the Langevin equation (2.1) and the characteristic function of the of
the alpha-stable Levy noise increment dLα,

We get

E

exp[ikdLα]
}

{

= exp[

σα∆t

−

k
|

α/α]
|

1
∆t

=

1
∆t

=

E[eik[X(t+∆t)−x]

1

X(t) = x]
|

−

E[eik[f (x)∆t+o(∆t)+dLα]

1
∆t
eik[f (x)∆t+o(∆t)]−σ

α

α

∆t|k|

/α

ikf (x)

σα

k
|

−

→

1]

−

1

−
α/α
|

9

as ∆t

0. Substitution of (B13) in (B11) and combining with (B2) then gives

→

(B4)

(B5)

(B6)

(B7)

(B8)

(B9)

(B10)

(B11)

(B12)

(B13)

=

Z Z

ˆR(k)eikx[ikf (x)

Z Z

ˆR(k)eikx∂tp(x0|
α/α] p(x0|
|

σα(x)
k
|

−

x, t)dxdk

x, t)dxdk

Here we have permitted the scaling factor σα/α, corresponding to the variance σ2dt of the noise increment in the case
α = 2, to depend on the variable x. By eliminating the x by use of (B9) we get, suppressing x0,

ˆR(k)∂t ˆp(k, t)dk =

Z Z

ˆR(k)[ik ˆf (k1 −

k)

ˆσα(k1 −

k
k)
|

α/α)ˆp(k1, t)]dkdk1
|

−

Z

and ﬁnally since ˆR(k) is arbitrary we get the spectral Fokker-Planck equation for the integrand, suppressing the t
index,

(ik ˆf (k1 −
Multiplying by e−ikx and using (B10) gives the Fokker-Planck equation in the usual form

α/α)ˆp(k1)dk1
|

ˆσα(k1 −

∂t ˆp(k) =

k
k)
|

k)

−

Z

∂tp(x) =

∂x[f (x)p(x)]

−

1
α Z Z

−

e−ikx ˆσα(k

k
k1)
|

α ˆp(k1)dkdk1
|

−

For the stationary Fokker-Planck equation the l.h.s of (B17) vanishes and the partial derivatives become total deriva-
tives. The last term on the r.h.s is a generalized diﬀusion which formally can be written

1
α Z Z

≡ −

e−ikx ˆσα(k

1
α

dα
dxα [σα(x)p(x)]
α ˆp(k1)dkdk1
k
k1)
|
|

−

In the case α = 2 this is the usual diﬀusion term corresponding to Gaussian white noise excitation of intensity σ2(x).
For α < 2 the diﬀusion is non-local. The physical meaning of this term is that for α-stable processes there will, due
to the fat tails of the distributions, be ﬁnite size jumps in the process.

APPENDIX C: THE CAUCHY DISTRIBUTION

The probability density can only be expressed explicitely for α = 1, and α = 1/2. For α = 1, Cauchy noise, the

characteristic function is c(k) = exp(

σ

k
|

) and its Fourier transform is,
|

−

p(x) =

1
πσ[1 + (x/σ)2]

x) = p(x), this does not imply that

For this distribution even the mean does not exist. Note that even though the density distribution (C1) is symmetric,
p(
= 0. For a data sampling this manifests itself in the fact that the average
of n data points, Zn = (X1 + ... + Xn)/n is Cauchy distributed with the same intensity as Xi, so that there is no
convergence for the series Zn, n = 1, 2, ..., it ﬂuctuates exactly as the data Xi itself.

x
i
h

−

A classical example of this characteristic of the Cauchy distribution is seen by considering the distribution of light

on a line, L, from a point source, see ﬁgure 7. Since the light is uniformly distributed over the angles, p(θ) = 1/π, θ
∈
[0, π], the distribution on the line will be, p(x) = p(θ)(dθ/dx) = 1/[δπ(1 + (x/δ)2], where X = δ tan(θ) is a stochastic
variable representing the point where a foton released from S at the (stochastic) angle θ crosses L. Now inserting n
1
lines, Li, parallel to L, between the light source, S, and L, we can apply Huygens principle, saying that Li will act as
a line of point sources, where the light follows the path S
X, where X = X1 + ... + Xn. The
→
variables Xi are independent and Cauchy distributed with scale parameter δ/n. Thus Huygens principle is consistent
with the fact that X = (

n
i=1 Xi)/n has the same distribution as Xi.

X1 + X2 →

X1 →

→

...

−

P

(B14)

(B15)

(B16)

(B17)

(B18)

(C1)

10

FIG. 7. Huygens principle applied to the light from a point source on a line. This illustrates the behavior of the averaging

of Cauchy distributed stochastic variables.

APPENDIX D: SIMULATIONS

The simulations performed in this work only involves Cauchy noise, which is easily obtained from a random variable,
π/2, π/2], as Y = tan(X). More generally a random variable with an α-

X, uniformly distributed in the interval [
stable distribution [13] is obtained from,

−

Y = [sin(αX)/ cos(X)(1/α)]

cos([1

α]X)/ log(W )](1−α)/α

[
−

×

−

(D1)

where X is deﬁned as above and W is another random variable uniformly distributed on the interval [0, 1]. When
simulating (2.1) by a discrete numerical time stepping the (ﬁxed size) time steps usually needs to be much smaller
than would be expected from numerical integration of the drift term alone. This is due to the large excursions from
the tails of the distribution of the noise. It is thus important to use a stable integration routine for the drift term. A
simple durable routine, which is the one used in these simulations, is Heun’s integration scheme. The simulation is
performed as x(t + ∆t) = x(t) + (f [x(t)] + f [x(t) + f [x(t)∆t])∆t/2 + σ∆t1/αη(t), where η(t) is generated by (D1).

[1] M. F. Shlesinger, G. M. Zaslavsky and U. Frisch, ”L´evy Flights and Related Topics in Physics”, Springer (1994).
[2] M. Shlesinger, B. West, J. Klafter, Phys. Rew. Lett. 58, 1100 (1987).
[3] G. Zimbardo, P. Veltri, G. Basile and S. Pricipato, Phys. Plasmas 2, 2653 (1995).
[4] J. Viecelli, Phys. Fluids A 5, 2484 (1993).
[5] J. Klafter, G. Zumofen and M. F. Shlesinger, Fractals 1, 389 (1993).
[6] M. Paczuski, S. Maslov and P. Bak, Phys. Rev. E 53, 414 (1996).
[7] H. Fogedby, Phys. Rev. Lett. 73, 2517 (1994).
[8] P. Protter, Stochastic Integration and Diﬀerential Equations, Springer (1995).
[9] H. E. Hurst, Trans. Amer. Soc. Civil Eng. 116 770 (1951).
[10] J. A. Viecelli, Journ. Atmos. Sci. 55, 677 (1998).
[11] P. D. Ditlevsen, to appear in Geophys. Res. Lett., 1999.
[12] C. W. Gardiner, Handbook of Stochastic Methods, 2. ed., Springer (1985).
[13] A. Janicki and A. Weron, Simulations and Chaotic Behavior of α-stable stochastic processes, Marcel Dekker, Inc. (1994).
[14] G. Samorodnitsky and M. S. Taqqu, Stable non- gaussian random processes, Chapman & Hall, N.Y. (1994).
[15] Feller W., An Introduction to Probability Theory and Its Applications, Volume II, Wiley, New York, 1971.

11

[16] J.-P. Bouchaud and A. Georges, Anomalous Diﬀusion in Disordered Media: Statistical Mechanisms, Models and Physical

Applications, Phys. Rep., 195, 128 (1990).

[17] Stratonovich, Topics in the theory of random noise, Vol.1+2, Gordon and Breach, New York, 1963+1967.

12


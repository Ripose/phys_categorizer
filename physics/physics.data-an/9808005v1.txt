Continuous Probability Distributions from Finite Data

LA-UR-98-3087

David M. Schmidt
Biophysics Group, Los Alamos National Laboratory, Los Alamos, New Mexico 87545
(August 5, 1998)

Abstract

Recent approaches to the problem of inferring a continuous probability dis-
tribution from a ﬁnite set of data have used a scalar ﬁeld theory for the form
of the prior probability distribution. This letter presents a more general form
for the prior distribution that has a geometrical interpretation which is useful
for tailoring prior distributions to the needs of each application. Examples are
presented that demonstrate some of the capabilities of this approach, includ-
ing the applicability of this approach to problems of more than one dimension.

02.50.Wp, 02.50.-r

8
9
9
1
 
g
u
A
 
7
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
0
0
8
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Typeset using REVTEX

1

Inferring the continuous probability distribution, or target distribution, from which a
ﬁnite number of data samples were drawn is an example of an ill-posed inverse problem:
there are many diﬀerent distributions that could have produced the given ﬁnite data. Often
one has prior information, separate from the data itself, that can reduce the range of pos-
sible target distributions. More generally, one can assign a prior probability to each target
distribution based on the prior information. Combining this prior probability distribution
with the likelihood of the data given any particular target distribution, using Bayes’ rule
of probability, produces a posterior probability over the space of target distributions. This
posterior distribution encapsulates all the information available, both from the data and
from the prior information, and can be used to make probabilistic inferences.

x1, . . . , xN ] denote the posterior probability that the target distribution Q(x)

Let P [Q
|

describes the data x1, . . . , xN . By Bayes’ rule,

P [Q
|

x1, . . . , xN ] =

P [x1, . . . , xN

Q]P [Q]

|

P [x1, . . . , xN ]
Q(x1)
· · ·
Q Q(x1)

· · ·

Q(xN )P [Q]

Q(xN )P [Q]

,

=

D

R

where P [Q] is the prior probability of the target distribution Q.

The form for P [Q] should incorporate the available prior information. For example, by
setting Q(x) = ψ2(x) [1], where ψ may take any value in (
), we may insure that Q
is non-negative. ψ is referred to as the amplitude by analogy with quantum mechanics [2].
A particular form for P [Q], or rather P [ψ], that has been presented in order to, the authors
say, incorporate a bias that Q be “smooth” is [2–4]

−∞

∞

,

P [ψ] =

exp

dx

(∂xψ)2

1
Z

"− Z

ℓ2
2

δ

1
(cid:18)

#

− Z

dx ψ2

,

(cid:19)

where Z is the normalization factor and ℓ is a constant which controls the penalty applied
to gradients. The delta function enforces normalization of the distribution Q.

Because this particular prior distribution is not very eﬀective at generating smooth dis-
tributions (as will be shown) and because the prior information available for each problem
will vary, it is useful to consider a more general form for the prior distribution. A more
general approach is to deﬁne the prior distribution as

P [ψ] =

exp

1
Z

V −1

ψ

ψ

1
2 h

|

|

i(cid:21)

(cid:20)−

δ (1

ψ

ψ

) ,

− h

|

i

where V is a positive, symmetric (Hermitian) operator within whatever Hilbert space is cho-
sen for ψ. This distribution is a generalization of a multi-dimensional Gaussian distribution
with V acting as the covariance operator. Continuing this analogy, we write

V (x, y) = σ(x)σ(y)ρ(x, y)

where σ2(x) is the variance at x and ρ is the correlation function.
Information about
smoothness is encoded in the correlation function. For example, if the distribution from
were drawn is expected to be smooth over distances smaller than a certain
which the

xi

{

}

2

(1)

(2)

(3)

(4)

(5)

−

spatial scale then the correlation function should be near unity over distances smaller than
this scale. The prior distribution used in [2,3] is equivalent to the one presented here in one
dimension with V −1 =

x, assuming ψ goes to zero at

ℓ2∂2

.

It is useful to consider this prior probability distribution in geometrical terms. The
eigenfunctions of the the operator V form a basis for the space of ψ. The normalization
constraint restricts ψ to lie on a hyper-spherical surface of radius one. Those eigenfunctions
with larger eigenvalues are more likely, a priori. If V has any eigenvalues that are zero then
the corresponding eigenfunctions form a basis for a subspace that is orthogonal to ψ; that is
the prior distribution prevents ψ from having any components along these eigenfunctions.
x1, . . . , xN ] of a distribution

With this form for the prior distribution the probability P [Q
|

±∞

Q given the data is

where the eﬀective action S is

P [ψ

x1, . . . , xN ]

|

exp

×

(cid:20)−
= e−S[ψ]δ (1

ψ2(x1)
V −1

ψ

∝
1
2 h

ψ

|

i

− h

|
ψ

· · ·
ψ

|
) ,

i(cid:21)

ψ2(xN )

δ (1

ψ

ψ

)

− h

|

i

S[ψ] =

1
2 h

V −1

ψ

ψ

|

|

i −

2

ln (

xi

ψ

) .

h

|

i

i
X

The most likely distribution given the data is that function ψcl which minimizes the ef-
fective action subject to the normalization constraint. To enforce this constraint a Lagrange
multiplier term λ(1
)/2 is subtracted from the action. Variational methods then lead
− h
to the following equations for ψcl and λ:

ψ

ψ

i

|

ψcli

|

= 2

(V −1 + λI)−1
ψcli
xi

h

|

xi

|

i

i
X

ψcl|

ψcli

h

= 1.

The solution to these equations may be written

Xi
where U(λ) = (V −1 + λI)−1. Eqs. (9) imply

ψcli

|

=

aiU(λ)

xi

,

|

i

ai

aj

xi

U(λ)

xj

= 2,

h

|

|

i

j
X

i = 1, . . . , N

aiaj

xi

U 2(λ)

xj

= 1.

h

|

|

i

Xi,j

3

These N + 1 non-linear equations determine λ and the ai and may be solved using Newton’s
method [2].

(6)

(7)

(8)

(9a)

(9b)

(10)

(11a)

(11b)

FIG. 1. The most likely distributions from an inverse Laplacian prior distribution with ℓ = 6
and from N = 20 (dashed line) and N = 1000 (dotted line) data drawn randomly from a target
distribution consisting of the sum of two Normal distributions (solid curve).

The covariance operator V in the prior distribution should be chosen for each diﬀerent
probability distribution that one is estimating. A few examples with three diﬀerent forms
for V are described below in order to illustrate the eﬀects that diﬀerent choices of V can
have. First consider the case used in [2,3] in which the prior covariance operator is an inverse
Laplacian in one dimension, V −1 =

ℓ2∂2

which is the Green’s function of the modiﬁed Helmholtz equation. The solutions of this
equation are well known, even for dimensions larger than one [5].
In particular, in one
dimension the most likely solution ψcl(x) is, from Eq. 10

−

U(λ) =

x. In this case
−1

ℓ2∂2

x + λI

−
(cid:16)

(cid:17)

ψcl(x) =

Xi

ai

1
2kℓ2 exp (

k

x

−

|

xi

)

|

−

where k = √λ/ℓ. Examples of the most likely probability distributions for this case with
ℓ = 6 are shown in Fig. 1. For these examples the data were drawn from a target distribution
consisting of the sum of two Normal distributions, shown as the solid curve in the ﬁgure.
The most likely distributions are not very smooth, as would be expected from the functional
form of Eq. 13.

For the second example consider the case in which the prior covariance operator has a

correlation function which is a Gaussian,

(12)

(13)

(14)

V (x, y; r) = σ exp

y)2

(x

−

"

−
r

.

#

4

FIG. 2. The most likely distributions from a Sinc function prior distribution with k0 = 3.33
and from the same N = 20 (dashed line) and N = 1000 (dotted line) data used for the examples in
Fig. 1, which were drawn randomly from a target distribution consisting of the sum of two Normal
distributions (solid curve).

Here σ2 is the prior variance for the magnitude of the target probability distribution and r is
a correlation scale below which the target probability distribution is believed to be smooth.
In this case it is useful to expand U in an operator product expansion in V ,

(15)

1
(cid:16)

U(λ) = V

λ V + λ2 V

V

λ3 V

V

V +

.

·

·

V

−

−

∝

·
V (x, y; √r) for this particular V , Eq. 15 generates a multi-resolution
Because V
expansion, analogous to a wavelet expansion, for U and therefore also for ψcl consisting of
Gaussians of ever increasing width, increasing each step by a factor of √2 with the ﬁnest
scale being represented by the original V (x, y; r). This functional form for V therefore
generates a most likely probability distribution that has ﬁnite derivatives to all orders and
is generally more smooth than that from the ﬁrst example.

· · ·
(cid:17)

·

For the ﬁnal example, consider the case in which the prior covariance operator is a
projection operator that projects onto the subspace formed by functions having only Fourier
wavenumbers smaller than a particular wavenumber k0. In one dimension this covariance
operator is the Sinc function,

V (x, y; k0) =

sin [k0(x
π(x

−
y)

y)]

.

−

(16)

V = V and from Eq. 15 U for this case is simply
Because this is a projection operator, V
U(λ) = V /(1 + λ). The most likely amplitude therefore consists of sums of Sinc functions
centered at each data point. Examples of the most likely probability distribution using

·

5

FIG. 3. The Fourier spectra of the three types of covariance operators shown in the legend.
Free parameters in each case have been set to correspond roughly to a cutoﬀ at wavenumber
k0 = 3.33.

this prior distribution with k0 = 3.33 are shown in Fig. 2. The same data used for the
examples in Fig. 1 were used here. Even with only 20 data points the most likely solution
indicates a doubly peaked distribution. Both of the examples here are more smooth than
those generated by the prior distribution discussed above in the ﬁrst case and shown in
Fig. 1.

It is useful to examine the Fourier spectrum of the prior covariance operator in order
to understand some of the properties of the resulting most likely distribution. The Fourier
spectra of the three covariance operators considered in the above examples are shown in
Fig. 3. Because of the form of the prior distribution (Eq. 4) those wavenumbers with larger
Fourier amplitudes are more likely, a priori. However, in order to maximize the likelihood
of the given data, the most likely amplitude will tend to consist of the largest possible
wavenumber components. Because the Sinc function covariance operator has the sharpest
high wavenumber cutoﬀ it will tend to generate the smoothest most likely distribution.
Conversely, the inverse Laplacian covariance operator will tend to produce the least smooth
most likely distribution. If the Sinc function covariance operator is used, however, the cutoﬀ
k0 should be chosen with great care because this prior forbids any solutions containing
wavenumbers higher than the cutoﬀ. Thus if the chosen cutoﬀ wavenumber is lower than the
maximum wavenumber component of the target distribution then the most likely distribution
will not converge to the target distribution as the number of data points increases.

6

ACKNOWLEDGMENTS

Supported by Los Alamos National Laboratory and by NIDA/NIMH Grant

DA/MH09972, J.S. George, Principal Investigator.

7

REFERENCES

[1] I. J. Good and R. A. Gaskins, Biometrika 58, 255 (1971).
[2] T. E. Holy, Phys. Rev. Lett. 79, 3545 (1997).
[3] W. Bialek, C. G. Callan, and S. P. Strong, Phys. Rev. Lett. 77, 4693 (1996).
[4] For a reparametrization invariant geometrical formulation see: V. Periwal, Phys. Rev.

[5] For example see: G. Arfken, Mathematical Methods for Physicists (Academic Press,

Lett. 78, 4671 (1997).

Orlando, 1985).

8


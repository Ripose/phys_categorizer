3
0
0
2
 
l
u
J
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
5
0
7
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Maximum Entropy and Bayesian Data Analysis:
Entropic Priors

Ariel Caticha∗ and Roland Preuss†
∗Department of Physics, University at Albany-SUNY,
Albany, NY 12222, USA.
†Center for Interdisciplinary Plasma Science,
Max-Planck-Institut f¨ur Plasmaphysik, EURATOM Association,

Boltzmannstrasse 2, D-85748 Garching bei M¨unchen, Germany

Abstract

The problem of assigning probability distributions which objectively
reﬂect the prior information available about experiments is one of the ma-
jor stumbling blocks in the use of Bayesian methods of data analysis. In
this paper the method of Maximum (relative) Entropy (ME) is used to
translate the information contained in the known form of the likelihood
into a prior distribution for Bayesian inference. The argument is inspired
and guided by intuition gained from the successful use of ME methods
in statistical mechanics. For experiments that cannot be repeated the re-
sulting “entropic prior” is formally identical with the Einstein ﬂuctuation
formula. For repeatable experiments, however, the expected value of the
entropy of the likelihood turns out to be relevant information that must
be included in the analysis. The important case of a Gaussian likelihood
is treated in detail.

1 Introduction

The inference of physical quantities from data generated either by experiment or
by numerical simulation is a ubiquitous and often cumbersome task. Whether
the data is corrupted by noise, hampered by ﬁnite resolution or tied up in cor-
relations, in principle it should always be possible to improve the analysis by
taking into account, in addition to the information contained in the data, what-
ever other knowledge one might have about the physical quantities to be inferred
or about how the data was generated. The way to link this prior information
with the new information in the data is found in Bayesian probability theory.

Bayesian methods are increasingly popular in physics [1]. They are essential
whenever repeating the experiment many times in order to reduce the measure-
ment uncertainty is either too expensive or time consuming. This is a common
situation in astronomy and astrophysics [2], and also in large laboratory exper-

1

iments as in fusion [3] and in high energy physics [4]. Other typical uses in
physics arise in spectrum restoration, in ill-posed inversion problems [5, 6, 7]
and when separating a signal from an unknown background [8]. Applications
include mass spectrometry [9], Rutherford backscattering [10] and nuclear mag-
netic resonance [11].

From a general point of view the problem of inductive inference is to update
from a prior probability distribution to a posterior distribution when new infor-
mation becomes available. The challenge is to develop updating methods that
are systematic and objective. Two methods have been found which are of very
broad applicability: one is based on Bayes’ theorem and the other is based on
the maximization of entropy. The choice between these two updating methods
is dictated by the nature of the information being processed.

When we want to update our beliefs about the values of certain quantities θ
on the basis of the observed values of other quantities y – the data – and of some
known relation between θ and y we must use Bayes’ theorem. The updated or
posterior distribution is p(θ
θ); the relation between y and θ is
π(θ)p(y
y)
∝
|
|
supplied by a known model p(y
θ); the previous knowledge about θ is codiﬁed
|
both into the “prior” probability π(θ) and also in the “likelihood” distribution
p(y

θ).
|
The selection of the prior π(θ) is a controversial issue which has generated
an enormous literature [12]. The diﬃculty is that it is not clear how to carry out
an objective translation of our previous beliefs about θ into a distribution π(θ).
One reasonable attitude is to admit subjectivity and recognize that diﬀerent
individuals may start from the same information and legitimately end with
diﬀerent translations. In simple cases experience and physical intuition have
led to a considerable measure of success, but we are often confronted with new
complex situations involving perhaps parameter spaces of high dimensionality
where we have neither a previous experience nor a reliable intuition.

On the other hand, there are special cases where some degree of objectivity
can be attained. For example, requirements of invariance can go a long way
towards the complete speciﬁcation of a prior. Considerable eﬀort has been
spent seeking an objective characterization of that elusive state of knowledge
that presumably reﬂects complete ignorance. Although there are convincing
arguments against the existence of such non-informative priors [13], the search
has had the merit of suggesting connections with the notion of entropy [14]
including two proposals for “entropic priors” [15, 16]. This brings us to the
second method of processing information.

Bayes’ theorem follows from the product rule for joint probabilities, p(y, θ) =
θ), and therefore its applicability is restricted to situations where as-
π(θ)p(y
|
sertions concerning the joint values of the data y and the parameters θ are
meaningful. But there are situations where the available information is of a dif-
ferent nature and involves assertions about the probabilities themselves. Such
information, which includes but is not limited to assertions about expected val-
ues, cannot be processed using Bayes’ theorem.

The method of Maximum Entropy (ME) is designed for updating from a prior
probability distribution to a posterior distribution when the information to be

2

processed takes the form of a constraint on the family of acceptable posterior
distributions [17]. The early and less satisfactory justiﬁcation of the ME method
followed from interpreting entropy, through the Shannon axioms, as a measure
of the amount of uncertainty in a probability distribution [18, 19]. Objections
to this approach are that the Shannon axioms refer to probabilities of discrete
variables, the entropy of continuous variables is not deﬁned, and that the use
of entropy as the unique measure of uncertainty remained questionable. Other
so-called entropies could and, indeed, were introduced. Ultimately, the real
problem is that Shannon was not concerned with inductive inference. He was
not trying to update probability distributions but was instead analyzing the
capacity of communication channels. Shannon’s entropy makes no reference to
prior distributions.

Considerations such as these motivated several attempts to justify the ME
method directly as a method of inductive inference without invoking question-
able measures of uncertainty [20, 21]. The concept of relative entropy is then
introduced as a tool for consistent reasoning which, in the special case of uni-
form priors, reduces to the usual entropy. There is no need for an interpretation
in terms of heat, disorder, or uncertainty, or even in terms of an amount of
information. Perhaps this is the explanation of why the search for the meaning
of entropy has turned out to be so elusive: strictly, entropy needs no interpre-
tation. In section 2, as background for the rest of the paper, we present a brief
outline of one such ‘no-interpretation’ approach inspired by [21].

In this paper we use entropic arguments to translate prior information into
a prior distribution. Rather than seeking a totally non-informative prior, we
make use of information that we do in fact have. Remarkably, it turns out
that the very conditions that allow us to contemplate using Bayes’ theorem –
namely, knowledge of a likelihood function, p(y
θ) – already constitute valuable
|
prior information. In this sense one can assert that the search for completely
non-informative priors is misplaced: if we do not know the likelihood, then prior
distributions are not needed anyway. The prior thus obtained is an “entropic
prior.” The name and the ﬁrst proposal of a prior of this kind is due to Skilling
[15] for the case of discrete distributions. The generalization to the continuous
case and further elaborations by Rodr´ıguez [16, 23] constitute a second proposal.
It is essential for the successful use of any prior, and of entropic priors in
particular, to be aware of what information they contain and, crucially, what
information they do not contain. No prior can be expected to succeed unless all
the information relevant to the problem at hand has been taken into account. It
is quite likely that most practical problems that were encountered with entropic
priors in the past can be traced to a failure to identify and incorporate all the
relevant information.

The information that has, in this paper, been translated into the entropic
prior is that contained in the likelihood. The bare entropic priors discussed here
apply to a situation where all we know about the quantities θ is that they appear
θ), and nothing else. Generalizations are,
as parameters in the likelihood p(y
|
of course, possible. Sometimes we are aware of additional relevant information
beyond what is contained in the likelihood and it can easily be incorporated

3

into a modiﬁed entropic prior. Other times we might be guilty of overlooking
additional information we already have.
Indeed, we would not be willing to
spend valuable eﬀort in the determination of a parameter θ unless we suspected
that knowledge of θ has important implications elsewhere. Typically we know
something about the physical signiﬁcance and the physical meaning of θ. It is
clear that in these cases we know considerably more than just that θ is a pa-
rameter appearing in the likelihood. We might even conceive of several diﬀerent
experiments, e = 1, 2, . . ., each yielding diﬀerent sets of data ye related to θ
θ). It is sometimes objected that one’s
by diﬀerent likelihood functions pe(ye|
prior knowledge about θ should not depend on which experiment one decides
to use to measure it, but this objection is misplaced: the mere fact that θ is
measurable through one or another experiment is additional information which,
if relevant, should be taken into account.

Another family of problems that can be tackled as a rather straightforward
extension of the ideas described here involve choosing which likelihood distri-
bution from among several competing candidates is responsible for generating
the data. Indeed, it is clear that any systematic approach to model selection
requires as a prerequisite the capability to process in an objective way the in-
formation implicit in each of those likelihoods. Except for some brief remarks
in the ﬁnal section, all these further developments, valuable as they might be,
will be addressed elsewhere.

Our contribution includes a derivation of an entropic prior (section 3) follow-
ing the same principles of ME inference that have been successful in statistical
mechanics. In fact, our whole approach is guided by intuition gained from ap-
plications of ME to statistical mechanics. Preliminary steps along this direction
were taken in [24] where a problem with the important case of experiments that
can be indeﬁnitely repeated had already been identiﬁed but not fully resolved.
This problem, re-examined in section 4, is interpreted as a symptom that impor-
tant relevant information has been overlooked. The complete resolution, which
hinges on identifying and incorporating this additional information, is given in
sections 5 and 6. The actual way in which ME is used in the derivation, in
analogy to standard applications in statistical mechanics, turns out to be im-
portant because it clariﬁes what it is that has been derived and how to use it:
ours is, in eﬀect, a third proposal for an entropic prior. In section 7 we discuss
in detail the important example of a Gaussian likelihood and ﬁnally, in section
8, we summarize and comment on the diﬀerences among the three versions of
entropic prior and on possible further developments.

2 The logic behind the ME method

The goal is to update beliefs about y
Y which are codiﬁed in the prior prob-
∈
ability distribution m(y) to a posterior distribution p(y) when new information
in the form of a constraint becomes available. (The constraints can, but need
not, be linear.) The selection is carried out by ranking the probability distribu-
tions according to increasing preference. One feature we impose on the ranking

4

scheme is transitivity: if distribution p1 is preferred over distribution p2, and p2
is preferred over p3, then p1 is preferred over p3. Such transitive rankings are
implemented by assigning to each p(x) a real number S[p] called the entropy of
p in such a way that if p1 is preferred over p2, then S[p1] > S[p2]. The selected
p will be that which maximizes S[p]. Thus the method involves entropies which
are real numbers and entropies that should be maximized. These are features
imposed by design; they are dictated by the function that the ME method is
supposed to perform .

Next we determine the functional form of S[p]. This is the rule that deﬁnes
the ranking scheme. The purpose of the rule is to do induction. We want to
extrapolate, to generalize from those special cases where we know what the
preferred distribution should be to the much larger number of cases where we
do not. Thus, in order to be an inductive rule S[p] must have wide applicability;
we will assume that the same rule applies to all cases. There is no justiﬁcation
for this universality except for the usual pragmatic justiﬁcation of induction: we
must be inclined to generalize lest we become paralyzed into not generalizing
at all. But then, we should remain cautious and keep in mind that in many
instances induction just fails.

The argument goes as follows [21]. If a general theory exists, then it must
apply to special cases. Furthermore, if in a certain special case the preferred
distribution is known, then this knowledge can be used to constrain the form
of S[p]. Finally, if enough special cases are known, then S[p] will be completely
determined. The known special cases are called the “axioms” of ME. As we
will see below the axioms reﬂect the conviction that one should not change
one’s mind frivolously, that whatever was learned in the past is important. The
chosen posterior distribution should coincide with the prior as closely as possible
and one should only update those aspects of one’s beliefs for which corrective
new evidence has been supplied. The three axioms are listed below.

⊂

Axiom 1: Locality. Local information has local eﬀects. We do not re-
vise the relative probabilities p(y′)/p(y) with y and y′ within a certain domain
D
Y unless the newly provided information refers explicitly to the domain
D. The power of this axiom stems from the arbitrariness in the choice of D.
The consequence of the axiom is that non-overlapping domains of y contribute
dy F (p(y)) where F is some unknown func-
additively to the entropy: S[p] =
tion.

Axiom 2: Coordinate invariance. The ranking should not depend on
the system of coordinates. The coordinates that label the points y are ar-
bitrary; they carry no information. The consequence of this axiom is that
dy p(y)f (p(y)/m(y)) involves coordinate invariants such as dy p(y) and
S[p] =
p(y)/m(y), where the density m(y) and the function f are, at this point, un-
R
known.

R

Next we make a second use of the locality axiom to enforce objectivity. We
allow domain D to extend over the whole space Y and assert that when there
is no new information there is no reason to change one’s mind. When there
are no constraints the selected posterior distribution should coincide with the
prior distribution. This eliminates the arbitrariness in the density m(y): up to

5

normalization m(y) is the prior distribution.

Axiom 3: Consistency for independent subsystems. When a system is
composed of independent subsystems it should not matter whether the inference
procedure treats them separately or jointly. If y = (y1, y2)
Y2, and
the subsystem priors m1(y1) and m2(y2) are respectively upgraded to p1(y1) and
p2(y2), then the prior for the whole system m1(y1)m2(y2) should be upgraded
to p1(y1)p2(y2). This axiom restricts the function f to be a logarithm. (The
fact that the logarithm applies also when the subsystems are not independent
follows from our inductive hypothesis that the ranking scheme has universal
applicability.)

Y = Y1 ×

∈

The overall consequence of these axioms [25] is that probability distributions
p(y) should be ranked relative to the prior m(y) according to their (relative)
entropy [17],

S[p, m] =

dy p(y) log

(1)

−

Z

p(y)
m(y)

.

The derivation has singled out S[p, m] as the unique entropy to be used in induc-
tive inference. Other expressions, such as S[m, p], or S[p, m] + S[m, p], or even
expressions that do not involve the logarithm, may be useful for other purposes,
but they do not constitute an induction: they are not a generalization from the
simple cases described in the axioms.

We end this section with two comments on the prior density m(y). First,
S[p, m] may be inﬁnitely negative when m(y) vanishes within some region D.
In other words, the ME method confers an overwhelming preference on those
distributions p(y) that vanish whenever m(y) does.
Is this a problem? Not
really. A similar “problem” also arises in the context of Bayes’ theorem. A
vanishing prior represents a tremendously serious prejudice because no amount
of data to the contrary would allow us to revise it. The solution in both cases
is to recognize that unless we are absolutely certain that y could not possibly
lie within D then we should not have assigned m(y) = 0 in the ﬁrst place.
Assigning a very low but non zero prior represents a safer and less prejudiced
representation of one’s beliefs and/or doubts both in the context of Bayesian
and of ME inference.

Second, choosing the prior density m(y) can be tricky. When there is no
information leading us to prefer one microstate of a physical system over an-
other we might as well assign equal prior probability to each state. Thus it is
reasonable to identify m(y) with the density of states and the invariant m(y)dy
is the number of microstates in dy. This is the basis for statistical mechanics.
Other examples of relevance to physics arise when there is no reason to prefer
one region of the space Y over another. Then we should assign the same prior
R dy m(y) to be
probability to regions of the same “volume,” and we can choose
the volume of a region R in the space Y . Notice that because of the presence of
R
the prior m(y) not all subjectivity has been eliminated and Laplace’s principle
of insuﬃcient reason still plays an important role, albeit in a somewhat modi-
ﬁed form. Just as with Bayes’ theorem, what is objective here is the manner in
which information is processed, not the initial probability assignments.

6

3 Entropic priors: the basic idea

In this section we follow [24] closely. We use the ME method to derive a prior
π(θ) for use in Bayes’ theorem,

p(θ

y)
|

∝

p(y, θ) = π(θ)p(y

θ) .
|

(2)

The selection of a preferred distribution using the ME method demands that the
space in which the search will be conducted be speciﬁed. Being a consequence of
the product rule for joint probabilities, Bayes’ theorem requires that assertions
such as ‘y and θ’ be meaningful and that the ‘probability of y and θ’ be well
deﬁned. Therefore we must focus our attention on p(y, θ) rather than π(θ); the
relevant universe of discourse is neither Θ, the space of all θs, nor the data space
Y . This point, ﬁrst made by Rodr´ıguez [22], is central
Y , but the product Θ
to the argument. Our derivation and the ﬁnal result, however, diﬀer from his
in several respects [22, 23].

×

To rank distributions in the space Θ

Y we must decide on a prior m(y, θ).
At this starting point absolutely nothing is known about the variables θ, in
particular, they have no physical meaning, and no relation between y and θ is
known. The θs are totally arbitrary. Therefore the prior must be a product
m(y)µ(θ) of the separate priors in the spaces Y and Θ. Indeed, the distribution
that maximizes the relative entropy

×

σ[p] =

dy dθ p(y, θ) log

(3)

p(y, θ)
m(y)µ(θ)

,

−

Z

when no constraints are imposed is p(y, θ)
about y tells us absolutely nothing about θ.

∝

m(y)µ(θ); it is such that data

In what follows we assume that m(y) is known. We consider this an impor-
tant part of understanding what data it is that has been collected. In section 7
we will suggest a reasonable m(y) for the special case of a Gaussian likelihood.
The prior µ(θ) remains unspeciﬁed.

Next we incorporate the crucial piece of information from which the param-
eters θ derive their physical meaning and which establishes the relation between
θ) is known. This has two consequences:
θ and y: the likelihood function p(y
|
First, the joint distribution p(y, θ) is constrained to be of the form π(θ)p(y
θ).
|
Notice that this constraint is not in the form that is most usual for applications
of the ME method:
it is not an expectation value. Note also that the only
information we are using about the quantities θ is that they appear as param-
θ), nothing else. In many situations of experimental
eters in the likelihood p(y
|
interest there exists additional relevant information beyond what is contained
in the likelihood; such information should be included as additional constraints
in the maximization of the relative entropy σ.

Second, now that a bare minimum is known about θ, namely that each θ
represents a probability distribution, there is a natural but still subjective choice
for µ(θ). As discussed in [26], except for an overall multiplicative constant, there
is a unique Riemannian metric that adequately reﬂects the fact that the points

7

(4)

(5)

(6)

in a space of probability distributions are not ‘structureless’, but happen to
be probability distributions; this is the Fisher-Rao metric. Within the ﬁnite-
dimensional subspace deﬁned by the constraint – the known p(y
θ) – the natural
|
metric on Θ is dℓ2 = gijdθidθj, where the unique gij induced by the family of
distributions p(y

θ) is
|

gij =

dy p(y

θ)
|

Z

∂ log p(y

∂ log p(y

θ)
|

∂θi

∂θj

θ)
|

.

Accordingly we choose µ(θ) = g1/2(θ), where g(θ) is the determinant of gij. Hav-
ing identiﬁed the prior measure and the constraints, we allow the ME method
to take over.

The preferred distribution p(y, θ) is chosen by varying π(θ) to maximize

Z

R

−

σ[π] =

dy dθ π(θ)p(y

θ)
π(θ)p(y
|
g1/2(θ)m(y)

θ) log
|
π(θ)
g1/2(θ)

=

dθ π(θ) log

+

dθ π(θ)S(θ) ,

−

−

Z

Z

where S(θ) is the entropy of the likelihood,

S(θ) =

−

Z

dy p(y

θ) log
|

p(y
θ)
|
m(y)

.

Writing the Lagrange multiplier that enforces
θ) is normalized yields
|

assuming p(y

dθ π(θ) = 1 as 1

log ζ, and

−

0 =

dθ

log

Z

−

(cid:18)

π(θ)
g1/2(θ)

+ S(θ)

log ζ

δπ(θ) ,

(7)

(cid:19)

Z

Therefore the probability that the value of θ should lie within the small volume
g1/2(θ)dθ is

π(θ)dθ =

eS(θ)g1/2(θ)dθ with ζ =

dθ g1/2(θ) eS(θ).

(8)

1
ζ

This entropic prior is our ﬁrst main result. It tells us that the preferred value of
θ is that which maximizes the entropy S(θ) because this maximizes the scalar
It also tells us the degree to which values of θ
probability density exp S(θ).
away from the maximum are ruled out; in many cases the preference for the
ME distribution can be overwhelming. Note also that the density exp S(θ) is
a scalar function and the presence of the Jacobian factor g1/2(θ) makes eq.(8)
manifestly invariant under changes of the coordinates θ in the space Θ.

We can claim a partial success. The ingredients that have been used are
precisely those that led us to consider using Bayes’ theorem in the ﬁrst place.
The information contained in the model – by which we mean that the data
space Y , its measure m(y), and the conditional distribution p(y
θ) – has been
|
translated into a prior π(θ). The success is partial because it has been achieved
for the special case of the ﬁxed data space Y of those experiments which cannot
conceivably be repeated. A more complete treatment requires that we address
the important case of experiments that can be repeated indeﬁnitely.

8

4 Repeatable experiments

Experiments need not be repeatable but sometimes they are. Let us assume
that successive repetitions are possible and that they happen to be independent.
Suppose, to be speciﬁc, that the experiment is performed twice so that the space
Y = Y 2 consists of the possible outcomes y1 and y2. Suppose further
of data Y
that θ is not a “random” variable; the value of θ is ﬁxed but unknown. Then
the joint distribution in the space Θ

Y 2 is

×

×
p(y1, y2, θ) = π(2)(θ) p(y1, y2|

θ) = π(2)(θ)p(y1|

θ)p(y2|

θ),

(9)

and the appropriate σ entropy is

σ(2)[π] =

dy1 dy2 dθ p(y1, y2, θ) log

−

Z

p(y1, y2, θ)

g(2)(θ)

1/2

m(y1)m(y2)

,

(10)

where g(2)(θ) is the determinant of the Fisher-Rao metric for p(y1, y2|
Eq.(4) it follows that g(2)
of θ. Maximizing σ(2)[π] subject to

θ). From
ij = 2gij so that g(2)(θ) = 2dg(θ), d being the dimension

dθ π(2)(θ) = 1 we get

(cid:2)

(cid:3)

π(2)(θ) =

1

Z (2) g1/2(θ) eS(2)(θ) =

R

1
Z (2) g1/2(θ) e2S(θ),

(11)

θ), and S(θ) def= S(1)(θ). The
where S(2)(θ) = 2S(θ) is the entropy of p(y1, y2|
generalization to N repetitions of the experiment, with data space Y N , is im-
mediate,

π(N )(θ) =

1

Z (N ) g1/2(θ) eS(N )(θ) =

1
Z (N ) g1/2(θ) eN S(θ).

(12)

This is clearly wrong: the dependence of π(N ) on the amount N of data would
lead us to a perpetual revision of the prior as more data is collected. The
absurdity of this situation becomes manifest when we consider the case of large
N . Then the exponential preference for the value of θ that maximizes S(θ)
becomes so pronounced that no amount of data to the contrary can successfully
overcome its eﬀect. The data becomes irrelevant, and the more data we have,
the more irrelevant it becomes.

Repeatable experiments present us with a problem. One possible attitude
is to blame the ME method: it gives nonsense and cannot be trusted. As with
all inductive methods this is, of course, a logical possibility. A second, more
constructive approach, is to always be prepared to question the results of ME
calculations on the basis that there is no guarantee that all the information
relevant to the situation at hand has been taken into account. The problem
is not a failure of the ME method but a failure to include all the relevant
information.

That this is indeed the case can be seen as follows: When we say an ex-
periment can be repeated twice, N = 2, we actually know more than just

9

θ) = p(y1|

θ). We also know that forgetting or discarding the
θ)p(y2|
p(y1, y2|
value of say y2, yields an experiment that is totally indistinguishable from the
single, N = 1, experiment. This additional
information is quantitatively ex-
pressed by

dy2 p(y1, y2, θ) = p(y1, θ), or equivalently

R

dy2 π(2)(θ)p(y1|
Z
which leads to π(2)(θ) = π(1)(θ).
reasonable result

θ)p(y2|

θ) = π(1)(θ)p(y1|

θ) ,

In the general case we get the manifestly

π(N )(θ) = π(N −1)(θ) = . . . = π(1)(θ) .

(13)

(14)

The challenge then is to identify a constraint that codiﬁes this information
within each space Θ

Y N .

×

5 More information: the Lagrange multiplier α

The problem with the prior π(N )(θ) in eq.(12) is that it expresses an overwhelm-
ing preference for the value θmax of θ that maximizes the entropy S(θ). Indeed,
as N

we have π(N )(θ)

θmax) leading to

δ(θ

→ ∞

−

→
dθ π(N )(θ)S(θ) N→∞
−→

=

S
h

i

Z

S(θmax) ,

(15)

×

which is manifestly incorrect. This suggests that a better prior would be ob-
tained by maximizing the entropy σ(N ) of distributions on the space space
Y N subject to an additional constraint on the numerical value ¯S of the
Θ
. It is not that we happen to know the numerical value ¯S
expected entropy
i
of
. In fact we do not. It is rather that we recognize that information about
i
¯S is relevant in the sense that if ¯S were known the problem above would not
arise. Naturally, additional eﬀort will be required to obtain the needed value of
¯S.

S
h

S
h

The logic of the previous paragraph may sound unfamiliar and further com-
ments may be helpful. When justifying the use of the ME method to obtain, say,
e−βEq ) it has been common
the canonical Boltzmann-Gibbs distribution (Pq ∝
to say something like “we seek the minimally biased (i.e. maximum entropy)
distribution that codiﬁes the information we do possess (the expected energy)
and nothing else”. Many authors ﬁnd this justiﬁcation objectionable. Indeed,
they might argue, for example, that the spectrum of black body radiation is
what it is independently of whatever information happens to be available to us.
We prefer to phrase our objection diﬀerently:
in most realistic situations the
expected value of the energy is not a quantity we happen to know. Nevertheless,
it is still true that maximizing entropy subject to a constraint on this (unknown)
expected energy leads to correct predictions. Therefore, the justiﬁcation behind
imposing a constraint on the expected energy cannot be that this is a quantity
that happens to be known – because it is not – but rather that the expected

10

energy is the quantity that should be known. Even if unknown, we recognize it
as the crucial relevant information without which no successful predictions can
be made. Therefore we proceed as if this crucial information were available and
produce a formalism that contains the temperature as a free parameter that
will later have to be obtained from the experiment itself. In other words, the
temperature (or expected energy) is one additional parameter to be inferred
from the data.

The entropy on the space Θ

Y N is

×

σ(N )[π] =

dθ dy(N ) π(θ)p(y(N )

θ) log
|

π(θ)p(y(N )
θ)
|
g1/2(θ) m(y(N ))

=

dθ π(θ) log

+ N

dθ π(θ)S(θ)

(16)

π(θ)
g1/2(θ)

Z

where S(θ) given by eq.(6). (A constant factor of N d/2 associated to the Fisher-
Rao measure g(N )(θ) has been omitted. It would eventually be absorbed into
the normalization of π(θ).) To obtain the prior π(θ) we maximize σ(N ) subject
to constraints on

and that π be normalized,

−

−

Z

Z

S
h

i

δ

σ(N ) + (1

log ζ)

−

(cid:20)

(cid:18)Z

This gives,

dθ π(θ)

1

+ λN

dθ π(θ)S(θ)

= 0 . (17)

−

(cid:19)

(cid:18)Z

¯S

−

(cid:19)(cid:21)

dθ

log

−

(cid:18)

Z

π(θ)
g1/2(θ)

+ (N + λN )S(θ)

log ζ

δπ(θ) = 0 .

(18)

−

(cid:19)

Therefore,

π(θ) =

g1/2(θ) exp [(N + λN )S(θ)] .

(19)

1
ζ

Y N the La-
The undesired dependence on N is eliminated if in each space Θ
grange multipliers λN are chosen so that N + λN = α is a constant independent
of N . The resulting entropic prior,

×

satisﬁes eq.(14). This is our second main result. The prior π(θ
α) codiﬁes
|
information contained in the likelihood function, plus information about the
expected value of the entropy of the likelihood implicit in the hyper-parameter
α,

with ζ(α) is given by

π(θ

α) =
|

1
ζ(α)

g1/2(θ)eαS(θ) ,

¯S(α) =

log ζ(α) ,

d
dα

ζ(α) =

dθ g1/2(θ)eαS(θ) .

Z

11

(20)

(21)

(22)

The next and ﬁnal step is ﬁgure out which α applies to the particular exper-
imental situation under consideration. The natural way to proceed is to invoke
Bayes’ theorem

p(α, θ

yN ) = π(α)π(θ
|

α)
|

p(yN
θ)
|
p(yN )

.

The choice of a prior π(α) for α itself is addressed in the next section. If we
were truly interested in the actual α, we could marginalize over θ to obtain

yN ) =
p(α
|

Z

dθ p(α, θ

yN ) =
|

π(α)
p(yN )

dθπ(θ

α)p(yN
|

θ) .
|

Z

But our interest in the value of α is only indirect; α is a necessary but annoy-
ing technical complication along the way to the real goal which is inferring θ.
Marginalizing over α, we get

where

p(θ

yN ) =
|

dα p(α, θ

yN ) = ¯π(θ)
|

Z

p(yN
θ)
|
p(yN )

¯π(θ) =

dα π(α)π(θ

α).
|

Z

This is the answer we sought: the eﬀective prior for θ, the averaged ¯π(θ), is
independent of the actual data yN , as it should. The last step is the assignment
of π(α).

6 An entropic prior for α

To remain consistent with the spirit of this paper, namely using ME to obtain
priors, the prior for α must itself be an entropic prior. The motivation behind
discussing entropic priors is that we wish to consider information included in
θ) refers to θ but makes no reference to any
the likelihood function. Since p(y
|
hyper-parameters it is quite clear that α should not be treated like the other
θs. The relation between α and the data y is indirect: α is related to θ, and θ
is related to y. Once θ is given, the data y becomes irrelevant, it contains no
further information about α. The whole signiﬁcance of α is derived purely from
its appearance in π(θ
α), eq.(20). Therefore, the relevant universe of discourse
|
Θ with α
is A

A. We focus our attention on the joint distribution

×

∈

α) .
|
and we obtain π(α) by maximizing the entropy

π(α, θ) = π(α)π(θ

Σ[π] =

dα dθ π(α, θ) log

−

Z

π(α, θ)
γ1/2(α) g1/2(θ)

where γ1/2(α) is determined below. Since no reference is made to repeatable
experiments in Y N there is no need for any further constraints except for nor-
malization.

(23)

(24)

(25)

(26)

(27)

(28)

12

The Fisher-Rao measure γ1/2(α) in eq.(28) is

γ(α) =

dθ π(θ

log π(θ

(29)

d
dα

α)
|

(cid:20)

Z

2

.

α)
|

(cid:21)

Using eqs.(20),(21) and (22) we get

γ(α) =

dθ π(θ

S(θ)

α)
|

(cid:20)

Z

d log ζ(α)
dα

−

2

(cid:21)

= (∆S)2,

(30)

but

Therefore,

d ¯S(α)
dα

=

d
dα

1
ζ(α)

dζ(α)
dα

=

1
ζ(α)

d2ζ(α)

dα2 −

(cid:20)

1
ζ(α)

dζ(α)
dα

2

(cid:21)

= (∆S)2.

(31)

γ(α) =

d2 log ζ(α)
dα2

.

The interpretation is straightforward: the distance between π(θ
dα) is given by

α) and π(θ
|

α +
|

γ1/2(α)dα = ∆S (α)dα ,

or, in words, the local entropy uncertainty ∆S is the distance per unit change
in α.

To maximize Σ rewrite it as

Σ[π] =

dα π(α) log

dα π(α) s(α),

(34)

−

Z

π(α)
γ1/2 +

Z

where s(α) is given by

Then, varying with respect to π(α) gives

s(α) =

−

Z
= log ζ(α)

dθ π(θ

α) log
|

α)
π(θ
|
g1/2(θ)

α

d log ζ(α)
dα

.

−

π(α) =

γ1/2(α)es(α) .

1
z

This is our third main result. It completes our derivation of the actual prior for
θ: the averaged ¯π(θ) in eq.(26) codiﬁes information contained in the likelihood
function, plus the insight that for repeatable experiments, information about
the expected likelihood entropy, even if unavailable, is relevant.

We argued above that the hyper-parameter α should not be treated in the
same way as the other parameters θ because the likelihood π(y
θ) refers only to
|
θs and not to α. Nonetheless, it may still be worthwhile to discuss brieﬂy what

13

(32)

(33)

(35)

(36)

would happen if α were treated as one of the θs. In this case, the entropic prior
π(α) would be determined by focusing our attention on the joint distribution

p(α, θ, yN ) = π(α)π(θ

α)p(yN
|

θ) ,
|

(37)

×

Θ

×

where the last two factors on the right are assumed known. The assumed uni-
Y N . A straightforward application of the
verse of discourse would be A
ME method would, as before, run into trouble with an unwanted N dependence
which would require the introduction of a new constraint on the appropriate
expected entropy. Thus, the entropic prior for α would involve a second hyper-
parameter α2. The unknown α2 would itself require its own entropic prior,
involving yet a third hyper-parameter α3, and so on. There would be an endless
chain of hyper-parameters [16]. In any practical calculation, the chain would
have to be truncated. Whether the predictions about θ depend on where and
how the truncation is carried out remains to be studied. But, fortunately, this
is not necessary: α is not like the other θs.

7 Example: a Gaussian model

Consider data yN =
µ,

y1, . . . , yN }

{

that are scattered around an unknown value

y = µ + ν

i

ν
h

= 0 and

= σ2. The goal is to estimate the parameters θ =
with
(θ1, θ2) = (µ, σ) on the basis of the data yN and the information implicit in
the model: the data space Y , the measure m(y) (discussed below), and the
Gaussian likelihood,

ν2
h

i

p(y

µ, σ) =
|

1
(2πσ2)1/2 exp

(y

µ)2

−
2σ2

.

(cid:21)

−

(cid:20)

In section 3 we asserted that knowing the measure m(y) is part of know-
ing what data has been collected. Therefore, nothing can be said about m(y)
without further speciﬁcation of the experimental situation. It turns out, how-
ever, that in many physical situations where the data happen to be distributed
according to eq.(39) the underlying space Y is suﬃciently symmetric, i.e., in-
variant under translations, that we can assume m(y) = m = constant . This is
physically reasonable. Gaussian distributions arise when the measured value of
y is the sum of a large number of “microscopic” contributions and the details
of how the individual contributions are themselves distributed are washed out
in the “macroscopic” sum. The macroscopically relevant features are just those
that distinguish one Gaussian from another, namely, the mean µ and the vari-
ance σ2. This is the physical basis behind the Central Limit Theorem. But if
microscopic details are irrelevant it should be possible to understand the situ-
ation from a purely macroscopic point of view: it should be possible to obtain
the Gaussian distribution as the preferred one among all those with the given µ

(38)

(39)

14

and σ2, and this is, indeed, the case: setting m(y) = constant in S[p, m], eq.(1),
and maximizing subject to constraints on the mean and variance yields eq.(39).

From eqs. (6) and (39) the entropy of the likelihood is

S(µ, σ) = log

where σ0

def=

e
2π

1/2 1
m

,

σ
σ0 (cid:21)

(cid:20)
and the corresponding Fisher-Rao measure, from eq.(4) is

(cid:16)

(cid:17)

g(µ, σ) = det

1/σ2
0

0
2/σ2

=

2
σ4 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Note that both S(µ, σ) and g(µ, σ) are independent of µ. This means that
if we were concerned with the simpler problem of estimating µ in a situation
where σ happens to be known, then the entropic prior, in any of the versions
eq.(8), (20), or (26), is a constant independent of µ. In other words, when σ
is known, the Bayesian estimate of µ using entropic priors coincides with the
maximum likelihood estimate, i.e., by the popular procedure of minimizing

χ2 =

1
µ

N

i=1
X

µ)2 .

(yi −

π(µ, σ

α) =
|

21/2
ζ(α)

σα−2
σα
0

.

Returning to the more interesting case of unknown σ, the α-dependent en-

tropic prior, eq.(20) is

π(µ, σ
α) is improper in both µ and σ; normalization requires the introduction
|
of high and low cutoﬀs for both µ and σ. The fact that without cutoﬀs the
model is not well deﬁned is an indication that more relevant information is
being requested: the cutoﬀs constitute relevant information that must be taken
into account. (The logic parallels that which led to the introduction of α in
section 5.) The case of unknown cutoﬀ values is important and we intend to
explore it in detail in future work. The basic idea is that specifying cutoﬀs is
an integral part of deﬁning the model, and therefore the choice of cutoﬀs can
be tackled as a problem of model selection. In the remainder of this section,
however, we will assume that the information about cutoﬀs is already available.
µL and to deﬁne
the σ cutoﬀs in terms of dimensionless quantities εL and εH ; σ extends from
σL = σ0εL to σH = σ0/εH. Then ζ(α) and π(µ, σ

It is convenient to write the range of µ as ∆µ = µH −

and

ζ(α) =

21/2∆µ
σ0

ε1−α
H −
α
−

α) are given by
|
εα−1
L
1

.

π(µ, σ

α) =
|

1
∆µσ0

α
−
ε1−α
H −

1
εα−1
L

σ
σ0 (cid:19)

(cid:18)

α−2

.

15

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

(49)

Notice that in the special case of α = 1, the prior over σ reduces to dσ/σ
which is called the Jeﬀreys prior and is usually introduced by the requirement
of invariance under scale transformations, σ

λσ.

Writing ε def= (εLεH )1/2, the prior for α can be obtained from eq.(32),

→

γ(α) =

(α

1)2 −

ε1−α

εα−1

2 log ε

−

2

(cid:19)

1

−

(cid:18)

1

and from eqs.(26) and (35),

γ1/2(α)
z

ε1−α
α

εα−1
1

π(α) =

−
−
where the normalization z has been suitably redeﬁned.

+ α

exp

−

α

(cid:20)

1

−

ε1−α + εα−1
ε1−α

εα−1 log ε

,

(cid:21)

Eqs.(46) and (47) simplify considerably when we take the limit ε

Clearly the same result is obtained whether we let εH →
ﬁxed, or letting εL →
0 simultaneously. The resulting γ(α) and π(α) are
εL →

0 while keeping εH ﬁxed, or even allowing εH →

0.
0 while keeping εL
0 and

→

and

γ(α) =

(α

1)2 ,

1

−

π(α) =

1

(1−α)2 exp
0

h

1
α−1

i

(

for α < 1
1
for α

≥

where π(α) is normalized. This is shown in Fig. 1.

π(α) reaches its maximum value at α = 1/2. Since π(α)

→ −∞
the expected value of α and all higher moments diverge. This suggests that
replacing the unknown α in the prior π(θ
α) by any given numerical value ˆα is
|
probably not a good approximation.

∼

α−2 for α

As explained in section 5, since α is unknown, the eﬀective prior for θ = (µ, σ)
α)π(α) over α, eq.(26). Since
|
.
0 or σH → ∞
0 we can safely take the limit εH →
0. The

is obtained marginalizing π(µ, σ, α) = π(µ, σ
π(α) = 0 for α
Conversely, since π(α)
limit σH → ∞

→
= 0 for α < 1 we cannot take εL →

while keeping σL ﬁxed gives,

0 or σL →

1 as ε

≥

π(µ, σ, α) =

1
∆µσL

α−1 ]

exp[ 1
1−α
0

σ
σL

(cid:16)

(cid:17)

(

α−2

for α < 1
1.
for α

≥

(50)

The averaged prior for µ and σ is

¯π(µ, σ) =

2

1

exp

1
∆µσL

σL
σ

−∞

(cid:16)

Z

(cid:17)

1
α−1
α

i

α

σ
σL (cid:19)

(cid:18)

1

h
−

dα ,

(51)

which integrates to

¯π(µ, σ) =

2
∆µσ

K0

log

2
(cid:18)

r

σ
σL (cid:19)

,

(52)

16

6
p(a)

0.5

0.4

0.3

0.2

0.1

e=10 -1

e=10 -2

e=10 -3

e=10 -4

2

-2

-1
Exp[   ]/(1-a )
1
a -1

2

1
e=10 -6

Figure 1: The prior π(α) for various values of the cutoﬀ parameter ε, as ε

0.

→

(53)

where K0 is a modiﬁed Bessel function of the second kind. This is the entropic
prior for the Gaussian model. The function

P (x) =

K0

2

log x

2
x

p
is shown in Fig. 2 as a function of x = σ/σL.
P (x) has an integrable singularity as x

(cid:16)

(cid:17)

1 where it behaves as

P (x)

2
x

≈

−

(cid:16)

p

→

−

(cid:17)

log

log x

γ

for x

1 .

(54)

≈

Since σL is a lower cutoﬀ the region of large x is more relevant. The leading
asymptotic behavior is given by

P (x)

√π
x (log x)1/4 exp

≈

log x

for x

1.

(55)

≫

2

−

(cid:16)

p

(cid:17)

Finally, we turn to Bayes’ theorem, eq.(25), with the prior (52) to obtain
estimators for µ and σ. For large N the results are independent of the prior and
the estimators coincide with the standard maximum likelihood results. The case
when N is not so large is the more interesting one. As estimators we can take
the expected values
over the posterior (25). The integrations can
be performed numerically and are not particularly illuminating. Alternatively,
one can follow standard practice and marginalize eq.(25) over σ to obtain the
yN ) and calculate the estimator ˆµ from
distribution p(µ
|

σ2
h

µ
i
h

and

i

= 0 ,

(56)

d
dµ

yN )
log p(µ
|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆµ

17

a
P(x)
2

1.5

1

0.5

1

2

3

4

5

6
x

Figure 2: The eﬀective ¯π(µ, σ) is shown as P (x) = 2
x = σ/σL.

x K0

2

log(x)

where

(cid:16)

p

(cid:17)

and its error bar ˆσ from

d2
yN )
dµ2 log p(µ
|
(cid:12)
(cid:12)
(cid:12)
yN ) happens to be a Gaussian these estimators coincide with the
(cid:12)
When p(µ
|
expected values
. The ﬁnal result for ˆµ is very simple. For any
µ
i
i
h
value of N we have

1
ˆσ2 .

σ2
h

(57)

and

−

=

ˆµ

ˆµ =

1
N

yi = ¯y ,

(58)

i
X
the estimator ˆµ is the sample average. The result for ˆσ is not as elegant but, of
course, for large N it asymptotically reduces to ˆσ2

¯y2)/N.

(y2

≈

−

8 Final remarks

In this paper the method of maximum relative entropy has been used to trans-
late the information contained in the known form of the likelihood into a prior
distribution for Bayesian inference. The argument follows closely the analo-
gous ME methods that have been so successful in statistical mechanics. For
experiments that cannot be repeated the resulting “entropic prior” is formally
identical with the Einstein ﬂuctuation formula. For repeatable experiments,
however, the expected value of the entropy of the likelihood – represented in
terms of a Lagrange multiplier α – turns out to be relevant information that

18

must be included in the analysis. As an illustration the important case of a
Gaussian likelihood was treated in detail.

It may be useful to comment brieﬂy on the diﬀerences between our entropic
prior and the versions previously proposed by Skilling and by Rodr´ıguez. Per-
haps the main diﬀerence with Skilling’s prior is that, unlike ours, its use is not
restricted to probability distributions but is intended for generic “positive ad-
ditive distributions” including, for example, the distributions of intensities in
images [15]. One problem here is that of justifying the applicability of the ME
method in such a general context. Our impulse to generalize is a dangerous
one; we may get away with indulging it occasionally but overindulgence will
certainly lead to error. In any case, our argument in section 3, which consists
in maximizing the entropy σ subject to a constraint p(y, θ) = π(θ)p(y
θ), makes
|
no sense in the case of generic positive additive distributions for which there
is no available product rule. A more speciﬁc problem arises from the fact that
Skilling’s entropy is not, in general, dimensionless and the hyper-parameter α is
vaguely interpreted some sort of cutoﬀ carrying the appropriate corrective units.
Some of the diﬃculties, which led Skilling to seek an alternative approach, were
identiﬁed in [27].

Rodr´ıguez’s approach is closer to ours. His prior applies to probability distri-
butions and appears to be derived from a ME principle [23]. One diﬀerence, per-
haps a minor one, is his treatment of the underlying measure m(y). For us m(y)
is not arbitrary; knowing m(y) is part of knowing what data has been collected.
For him m(y) is just an initial guess and he suggests setting m(y) = p(y
θ0) for
|
some value θ0. The more important diﬀerence, however, is that the number of
Y n
observed data n is deliberately and explicitly left unspeciﬁed. The space Θ
over which distributions are deﬁned, and therefore the distributions themselves,
also remain unspeciﬁed. It is not clear what the maximization of an entropy
over such unspeciﬁed spaces could possibly mean but a hyper-parameter α is
eventually introduced and it is interpreted as a “virtual number of observations
supporting the initial guess θ0.” He proposes that α be considered as one more
among the parameters θ to be inferred. As mentioned earlier this leads to the
introduction of an endless chain of additional hyper-parameters.

×

There are several directions in which the ideas of this paper can be further
extended. First, we emphasize once again that the entropic priors discussed
here apply to a situation where all we know about the quantities θ is that
θ), and nothing else. In many
they appear as parameters in the likelihood p(y
|
situations of experimental interest there exists additional relevant information
beyond what is contained in the likelihood. Such information should be included
as additional constraints in the maximization of the relative entropy σ in eq.(17).
The resulting modiﬁed entropic prior would provide a better representation of
our state of knowledge prior to the acquisition of the data. Indeed, the advantage
of the Bayesian approach over the usual method of maximum likelihood is the
possibility of including additional relevant information by replacing a ﬂat prior
by an appropriately more informative prior. There is nothing to prevent us from
performing a similar improvement and going beyond the “bare” entropic priors
discussed in this paper. Two kinds of additional information that are easy to

19

include are restrictions on the range of the parameters θ and information about
the known expected values of some variables a(θ). Steps in this direction were
taken in section 5, where a(θ) is the likelihood entropy, and in section 7 where
high and low cutoﬀs on the range of the Gaussian parameters were introduced.
Second, in the introduction we mentioned the interesting possibility of ana-
lyzing data ye from diﬀerent experiments, e = 1, 2, . . ., related to θ by diﬀerent
θ). Clearly this can be analyzed as a single combined
likelihood functions pe(ye|
experiment with likelihood p(y1, y2, . . .
θ) . . . to which all our
θ)p2(y2|
θ) = p1(y1|
|
previous results apply. As we stated earlier, the mere fact that θ is measurable
through one or another experiment is additional relevant information that can
be taken into account.

Third, we also mentioned that problems of model selection can be tackled
as an extension of the ideas described in this paper. On the basis of data y
we want to select one model among several competing candidates labeled by
m, θm). The answer, i.e.,
m = 1, 2, . . . with likelihood distributions given by p(y
|
the probability of model m given the data y, is given by Bayes’ theorem,

p(m

y) =
|

p(y

m) =
|

dθm p(y, θm|

m)

π(m)
p(y)

Z

=

dθm π(m, θm) p(y

m, θm) .
|

(59)

π(m)
p(y)
1
p(y)

Z

This is exact. The problem is solved, at least in principle, once an entropic prior
for π(m, θm) is assigned. However, the remaining practical problems associated
with carrying out the actual numerical calculations could, of course, still be
quite formidable.

Finally, we end with a word of caution. As in all instances of inductive
inference there is the possibility that predictions based on the ME method could
be wrong because not all the information relevant to the problem at hand was
taken into account. This potential problem is not peculiar to the ME method, it
is a problem shared by all methods of induction. Nevertheless, we are conﬁdent
that the rewards of extending the beneﬁts of an inductive method singled out
by requirements of objectivity, the ME method, beyond its traditional territory
of statistical mechanics and into that of data analysis will be enormous.
Acknowledgments- Many of our comments and arguments have been inspired
by Carlos C. Rodr´ıguez, Volker Dose, and Rainer Fisher through insightful ques-
tions and discussions which we gratefully acknowledge. A. C. also acknowledges
the hospitality of the Max-Planck-Institut f¨ur Plasmaphysik during the two ex-
tended visits when most of this work was carried out.

References

[1] For a recent review see V. Dose, “Bayesian inference in physics: case stud-
ies”, Rep. Prog. Phys., accepted for publication (2003); for a pedagogical
introduction see D. S. Sivia, “Data Analysis, A Bayesian Tutorial” (Oxford
University Press, Oxford, 1996).

20

[2] “Statistical Challenges in Modern Astronomy I–III”, series ed. by E. D.

Feigelson and G. J. Babu, Springer, New York (1992, 1997, 2002).

[3] V. Dose, R. Preuss and W. von der Linden, Phys. Rev. Lett. 81, 3407

[4] “Bayesian Reasoning in high energy physics: principles and applications”,

G. D’Agostini, CERN Yellow Report 99-03 (1999).

[5] W. von der Linden, M. Donath, and V. Dose, Phys. Rev. Lett. 71, 899

(1998).

(1993).

[6] R. Preuss et al., Phys. Rev. Lett. 73, 732 (1994); R. Preuss, W. Hanke
and W. von der Linden, Phys. Rev. Lett. 75, 1344 (1995); R. Preuss, W.
Hanke, C. Gr¨ober, and H.G. Evertz, Phys. Rev. Lett. 79, 1122 (1997).

[7] R. Fischer, M. Mayer, W. von der Linden, and V. Dose, Phys. Rev. E 56,

6667 (1997).

[8] W. von der Linden, V. Dose, J. Padayachee, and V. Prozesky, Phys. Rev.
E 59, 6527 (1999); R. Fischer, K. M. Hanson, V. Dose, and W. von der
Linden. Phys. Rev. E 61, 1152 (2000).

[9] T. Schwarz-Selinger, R. Preuss, V. Dose and W. von der Linden, J. Mass

Spect. 36, 866 (2001).

(1999).

[10] U. V. Toussaint, R. Fischer, K. Krieger, and V. Dose. New J. Phys. 1, 11

[11] G. L. Bretthorst, Bayesian Spectrum Analysis and Parameter Estimation
(Lect. Notes in Physics, Vol. 48, Springer-Verlag, Berlin, 1988); J. Magn.
Reson. 88, 552 (1990).

[12] For a review with annotated bibliography see e.g., R. E. Kass and L.

Wasserman, J. Am. Stat. Assoc. 91, 1343 (1996).

[13] J. M. Bernardo, T. Z. Irony, N. D. Singpurwalla, J. Stat. Plan. Inf. 65, 159

(1997).

[14] E. T. Jaynes, IEEE Trans. Syst. Sci. Cybern. Vol. SSC-4, 227 (1968); J.
M. Bernardo, J. Roy. Stat. Soc. B 41, 113 (1979); A. Zellner, “Bayesian
methods and entropy in economics and econometrics” in Maximum En-
tropy and Bayesian Methods, edited by W. T. Grandy Jr. and L. H. Schick
(Kluwer, Dordrecht, 1991).

[15] J. Skilling, “Classic Maximum Entropy” in Maximum Entropy and
Bayesian Methods, J. Skilling (ed.) (Kluwer, Dordrecht, 1989); “Quanti-
ﬁed Maximum Entropy” in Maximum Entropy and Bayesian Methods, P.
F. Foug`ere (ed.) (Kluwer, Dordrecht, 1990).

21

[16] C. C. Rodr´ıguez, “The metrics generated by the Kullback number” in Max-
imum Entropy and Bayesian Methods, J. Skilling (ed.) (Kluwer, Dordrecht,
1989); “Objective Bayesianism and geometry” in Maximum Entropy and
Bayesian Methods, P. F. Foug`ere (ed.) (Kluwer, Dordrecht, 1990); “En-
tropic priors” in Maximum Entropy and Bayesian Methods, edited by W.
T. Grandy Jr. and L. H. Schick (Kluwer, Dordrecht, 1991); “Bayesian ro-
bustness: a new look from geometry” in Maximum Entropy and Bayesian
Methods, G. R. Heidbreder (ed.) (Kluwer, Dordrecht, 1996).

[17] On terminology: The terms ‘prior’ and ‘posterior’ are normally used in the
context of Bayes’ theorem; we retain the same terminology when using ME
because we are concerned with the similar goal of processing information
to update from a prior to a posterior. The “method of ME” is usually un-
derstood in the restricted sense that one updates from a prior distribution
that happens to be uniform. Here we adopt a broader meaning that in-
cludes updates from arbitrary priors and which involves the maximization
of relative entropy. Indeed since all entropies are relative to some prior the
qualiﬁer ‘relative’ is not needed and will henceforth be omitted.

[18] C. E. Shannon, Bell Systems Tech. Journal 27, 379, 623 (1948); C. E. Shan-
non and W. Weaver, The Mathematical Theory of Communication (Univ.
of Illinois Press, Urbana, 1949); N. Wiener, Cybernetics (MIT Press, Cam-
bridge, 1948); L. Brillouin, Science and Information Theory, (Academic
Press, New York, 1956); S. Kullback, Information Theory and Statistics
(Wiley, New York, 1959).

[19] E. T. Jaynes, “Information Theory and Statistical Mechanics” Phys. Rev.
106, 620 and 108, 171 (1957); R. D. Rosenkrantz (ed.), E. T. Jaynes:
Papers on Probability, Statistics and Statistical Physics (Reidel, Dordrecht,
1983); E. T. Jaynes, Probability Theory: The Logic of Science (Cambridge
University Press, Cambridge, 2003).

[20] J. E. Shore and R. W. Johnson, “Axiomatic derivation of the Principle of
Maximum Entropy and the Principle of Minimum Cross-Entropy,” IEEE
Trans. Inf. Theory IT-26, 26 (1980); Y. Tikochinsky, N. Z. Tishby and R.
D. Levine, Phys. Rev. Lett. 52, 1357 (1984) and Phys. Rev. A30, 2638
(1984); I. Csiszar, Ann. Stat. 19, 2032 (1991).

[21] J. Skilling, “The Axioms of Maximum Entropy” in Maximum-Entropy and
Bayesian Methods in Science and Engineering, G. J. Erickson and C. R.
Smith (eds.) (Kluwer, Dordrecht, 1988).

[22] C. C. Rodr´ıguez, see section 3 of “Are we cruising a hypothesis space?” in
Maximum Entropy and Bayesian Methods, ed. by W. von der Linden, V.
Dose, R. Fischer and R. Preuss (Kluwer, Dordrecht, 1999).

[23] C. C. Rodr´ıguez: ‘Entropic Priors for Discrete Probabilistic Networks and
for Mixtures of Gaussian Models’. In: Bayesian Inference and Maximum

22

Entropy Methods in Science and Engineering, ed. by R. L. Fry, AIP Conf.
Proc. 617, 410 (2002) (online at arXiv.org/abs/physics/0201016).

[24] A. Caticha,

in Bayesian
‘Maximum entropy, ﬂuctuations and priors’,
Methods and Maximum Entropy in Science and Engineering, ed. by
A. Mohammad-Djafari, AIP Conf. Proc. 568, 94 (2001) (online at
arXiv.org/abs/math-ph/0008017).

[25] The number and the wording of our axioms diﬀers from Skilling’s because
we concentrate on the speciﬁc problem of ranking probability distributions
while he was concerned with ranking general positive additive distributions.
Proofs, which are easily constructed following Shore and Johnson [20] and
Skilling [21], will be presented elsewhere.

[26] S. Amari, Diﬀerential-Geometrical Methods in Statistics (Springer-Verlag,
1985); for a brief derivation see A. Caticha, “Change, Time and Information
Geometry,” in Bayesian Methods and Maximum Entropy in Science and
Engineering, ed. by A. Mohammad-Djafari, AIP Conf. Proc. 568, 72 (2001)
(online at arXiv.org/abs/math-ph/0008018).

[27] J. Skilling and S. Sibisi, “Priors on Measures” in Maximum-Entropy and
Bayesian Methods, K. M. Hanson and R. N. Silver (eds.) (Kluwer, Dor-
drecht, 1996); J. Skilling, “Massive Inference and Maximum Entropy” in
Maximum-Entropy and Bayesian Methods, G. J. Erickson, J. T. Ryckert
and C. R. Smith (eds.) (Kluwer, Dordrecht, 1998).

23


PHYSTAT2003, Stanford Linear Accelerator Center, September 2003

1

Asymmetric Errors

Roger Barlow
Manchester University, UK and Stanford University, USA

4
0
0
2
 
n
a
J
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
2
4
0
1
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Errors quoted on results are often given in asymmetric form. An account is given of the two ways these can
arise in an analysis, and the combination of asymmetric errors is discussed. It is shown that the usual method
has no basis and is indeed wrong. For asymmetric systematic errors, a consistent method is given, with detailed
examples. For asymmetric statistical errors a general approach is outlined.

1. Asymmetric Errors

2. Asymmetric Systematic Errors

In the reporting of results from particle physics ex-
periments it is common to see values given with er-
rors with diﬀerent positive and negative numbers, to
denote a 68% central conﬁdence region which is not
symmetric about the central estimate. For example
(one of many) the Particle Data Group[1] quote

B.R.(f2(1270)

ππ) = (84.7+2.4

1.3)%.

−

→

The purpose of this note is to describe how such er-
rors arise and how they can properly be handled, par-
ticularly when two contributions are combined. Cur-
rent practice is to combine such errors separately, i.e.
to add the σ+ values together in quadrature, and then
do the same thing for the σ− values. This is not, to
my knowledge, documented anywhere and, as will be
shown, is certainly wrong.

There are two separate sources of asymmetry, which
unfortunately require diﬀerent treatments. We call
these ‘statistical’ and ‘systematic’; the label is fairly
accurate though not entirely so, and they could
equally well be called ‘frequentist’ and ‘Bayesian’.

Asymmetric statistical errors arise when the log
likelihood curve is not well described by a parabola [2].
The one sigma values (or, equivalently, the 68% cen-
tral conﬁdence level interval limits) are read oﬀ the
points at which ln L falls from its peak by 1
2 –
or, equivalently, when χ2 rises by 1. This is not
strictly accurate, and corrections should be made us-
ing Bartlett functions[3], but that lies beyond the
scope of this note.

Asymmetric systematic errors arise when the de-
pendence of a result on a ‘nuisance parameter’ is
non-linear. Because the dependence on such parame-
ters – theoretical values, experimental calibration con-
stants, and so forth – is generally complicated, involv-
ing Monte Carlo simulation, this study generally has
σ
to be performed by evaluating the result x at the
and +σ values of the nuisance parameter a (see [4] for
a fuller account) giving σ−x and σ+
σ gives σ±x
or σ∓x according to the sign of dx
da .)

x . (a

This note summarises a full account of the proce-
dure for asymmetric systematic errors which can be
found in [5] and describes what has subsequently been
achieved for asymmetric statistical errors. For another
critical account see [6].

−

±

WEMT002

If σ−x and σ+

x are diﬀerent then this is a sign that the
dependence of x on a is non-linear and the symmetric
distribution in a gives an asymmetric distribution in x.
In practice, if the diﬀerence is not large, one might be
well advised to assume a straight line dependence and
take the error as symmetric, however we will assume
that this is not a case where this is appropriate. We
consider cases where a non-linear eﬀect is not small
enough to be ignored entirely, but not large enough to
justify a long and intensive investigation. Such cases
are common enough in practice.

2.1. Models

For simplicity we transform a to the variable u de-
scribed by a unit Gaussian, and work with X(u) =
x(u)
It is useful to deﬁne the mean σ, the
diﬀerence α, and the asymmetry A:

x(0).

−

σ =

σ+ + σ−
2

α =

σ+

σ−

−
2

A =

σ+
σ−
−
σ+ + σ−

(1)
There are inﬁnitely many non-linear relationships be-
tween u and X that will go through the three deter-
mined points. We consider two. We make no claim
that either of these is ‘correct’. But working with
asymmetric errors must involve some model of the
non-linearity. Practitioners must select one of these
two models, or some other (to which the same formal-
ism can be applied), on the basis of their knowledge
of the problem, their preference and experience.

•

•

Model 1: Two straight lines

Two straight lines are drawn, meeting at the
central value

X = σ+u
= σ−u

0
0.

u
u

≥
≤

(2)

Model 2: A quadratic function

The parabola through the three points is

X = σu + αu2 = σu + Aσu2.

(3)

2

PHYSTAT2003, Stanford Linear Accelerator Center, September 2003

These forms are shown in Figure 1 for a small asym-

metry of 0.1, and a larger asymmetry of 0.4.

It can be seen that the Model 1 dimidated Gaussian
and Model 2 distorted Gaussian are not dissimilar if
the asymmetry is small, but are very diﬀerent if the
asymmetry is large.

2.2. Bias

If a nuisance parameter u is distributed with a
Gaussian probability distribution, and the quantity
X(u) is a nonlinear function of u, then the expecta-
tion

X
h

).
i
For model 1 one has

u
is not X(
h

i

For model 2 one has

< X >=

σ+

σ−

−
√2π

< X >=

σ+

σ−

−
2

= α

(4)

(5)

Hence in these models, (or any others), if the result
quoted is X(0), it is not the mean.
It diﬀers from
it by an amount of the order of the diﬀerence in the
positive and negative errors. It is perhaps defensible
as a number to quote as the result as it is still the
median - there is a 50% chance that the true value is
below it and a 50% chance that it is above.

2.3. Adding Errors

If a derived quantity z contains parts from two
quantities x and y, so that z = x + y, the distribution
in z is given by the convolution:

fz(z) =

dxfx(x)fy(z

x)

(6)

−

Z

Figure 1: Some nonlinear dependencies

Model 1 is shown as a solid line, and Model 2 is
dashed. Both go through the 3 speciﬁed points. The
diﬀerences between them within the range
1
are not large; outside that range they diverge consid-
erably.

≤

≤

−

u

1

The distribution in u is a unit Gaussian, G(u),
and the distribution in X is obtained from P (X) =
G(u)
. Examples are shown in Figure 2. For Model
dX/du
|
|
1 (again a solid line) this gives a dimidated Gaus-
sian - two Gaussians with diﬀerent standard devia-
tion for X > 0 and X < 0. This is sometimes called a
‘Bifur-
‘bifurcated Gaussian’, but this is inaccurate.
cated’ means ‘split’ in the sense of forked. ‘Dimidated’
means ‘cut in half’, with the subsidiary meaning of
‘having one part much smaller than the other’ [7].
For Model 2 (dashed) with small asymmetries the
curve is a distorted Gaussian, given by G(u)
with
σ+2αu
|
u = √σ2+4αX
larger
X
|
sidered.

. For larger asymmetries and/or
2α
values, the second root also has to be con-

−

σ

|

|

Figure 2: Probability Density Functions from Figure 1

With Model 1 the convolution can be done ana-
lytically. Some results for typical cases are shown in

Figure 3: Examples of the distributions from combined
asymmetric errors using Model 1.

WEMT002

PHYSTAT2003, Stanford Linear Accelerator Center, September 2003

3

Figure 3. The solid line shows the convolution, the
dashed line is obtained by adding the positive and
negative standard deviations separately in quadrature
(the ‘usual procedure’). The dotted line is described
later.

The solid and dashed curves disagree markedly. The
‘usual procedure’ curve has a larger skew than the con-
volution. This is obvious. If two distributions with the
same asymmetry are added the ‘usual procedure’ will
give a distribution just scaled by √2, with the same
asymmetry. This violates the Central Limit Theo-
rem, which says that convoluting identical distribu-
tions must result in a combined distribution which is
more Gaussian, and therefore more symmetric, than
its components. This shows that the ‘usual procedure’
for adding asymmetric errors is inconsistent.

2.4. A consistent addition technique

If a distribution for x is described by some function,
f (x; x0, σ+, σ−), which is a Gaussian transformed ac-
cording to Model 1 or Model 2 or anything else, then
‘combination of errors’ involves a convolution of two
such functions according to Equation 6. This com-
bined function is not necessarily a function of the same
form: it is a special property of the Gaussian that the
convolution of two Gaussians gives a third. The (solid
line) convolution of two dimidated Gaussians is not
itself a dimidated Gaussian. Figure 3 is a demonstra-
tion of this.

Although the form of the function is changed by
a convolution, some things are preserved. The semi-
invariant cumulants of Thi`ele (the coeﬃcients of the
power series expansion of the log of the Fourier Trans-
form) add under convolution. The ﬁrst two of these
are the usual mean and variance. The third is the
unnormalised skew:

γ =< x3 >

3 < x >< x2 > +2 < x >3

(7)

−

Within the context of any model, a consistent ap-
proach to the combination of errors is to ﬁnd the
mean, variance and skew: µ, V and γ, for each con-
tributing function separately. Adding these up gives
the mean, variance and skew of the combined func-
tion. Working within the model one then determines
the values of σ
, σ+, and x0 that give this mean, vari-
−
ance and skew.

2.5. Model 1

For Model 1, for which

x3
h

i

= 2
√2π

(σ3

+ −

σ3
−

) we

have

(σ+

σ−)

µ = x0 + 1
√2π
V = σ2 + α2
3)

σ−

3

−

−

1
−
2 (σ+
(cid:0)

−

2
π

−

σ−)(σ+2 + σ−
(cid:1)

2)

γ = 1
√2π

2(σ+3

(cid:2)

WEMT002

+ 1

π (σ+

σ−)3

−

(cid:3)

(8)

Given several error contributions the Equations 8 give
the cumulants µ, V and γ of each. Adding these up
gives the ﬁrst three cumulants of the combined dis-
tribution. Then one can ﬁnd the set of parameters
σ−, σ+, x0 which give these values by using Equa-
tions 8 in the other sense.

It is convenient to work with ∆, where ∆ is the
diﬀerence between the ﬁnal x0 and the sum of the in-
dividual ones. The parameter is needed because of
the bias mentioned earlier. Even though each contri-
bution may have x0 = 0, i.e.
it describes a spread
about the quoted result, it has non-zero µi through
the bias eﬀect (c.f. Equations 4 and 5 ). The σ+
and σ− of the combined distribution, obtained from
the total V and γ, will in general not give the right µ
unless a location shift ∆ is added. The value of the
quoted result will shift.

Recalling section B, for the original distribution one
could defend quoting the central value as it was the
median, even though it was not the mean. The con-
voluted distribution not only has a non-zero mean, it
also (as can be seen in Figure 3 ) has non-zero me-
dian. If you want to combine asymmetric errors then
you have to accept that the quoted value will shift. To
make this correction requires a real belief in the asym-
metry of the error values. At this point practitioners,
unless they are sure that their errors really do have a
signiﬁcant asymmetry, may be persuaded to revert to
quoting symmetric errors.

Solving the Equations 8 for σ−, σ+ and x0
to be done numer-
given µ, V and γ has
is available on
ically.
this
A program for
http://www.slac.stanford.edu/
barlow.
Some
results are shown in the dotted curve of Figure 3 and
Table 1.

∼

y σ+

x σ−

Table I Adding errors in Model 1
y σ− σ+ ∆
σ−
x σ+
1.0 1.0 0.8 1.2 1.32 1.52 0.08
0.8 1.2 0.8 1.2 1.22 1.61 0.16
0.5 1.5 0.8 1.2 1.09 1.78 0.28
0.5 1.5 0.5 1.5 0.97 1.93 0.41

It is apparent that the dotted curve agrees much
better with the solid one than the ‘usual procedure’
dashed curve does.
It is not an exact match, but
does an acceptable job given that there are only 3
If the shape
adjustable parameters in the function.
of the solid curve is to be represented by a dimidated
Gaussian, then it is plausible that the dotted curve is
the ‘best’ such representation.

4

PHYSTAT2003, Stanford Linear Accelerator Center, September 2003

2.6. Model 2

For Model 2 one has

The equivalent of Equations 8 are

µ = x0 + α
V = σ2 + 2α2
γ = 6σ2α + 8α3

(9)

As with Method 1, these are used to ﬁnd the cu-
mulants of each contributing distribution, which are
summed to give the three totals, and then Equation 9
is used again to ﬁnd the parameters of the distorted
Gaussian with this mean, variance and skew. The web
program will also do these calculations

Some results are shown in Figure 4 and Table II.
The true convolution cannot be done analytically but
can be done by a Monte Carlo calculation.

y σ+

x σ−

Table II Adding errors in Model 2
y σ− σ+ ∆
σ−
x σ+
1.0 1.0 0.8 1.2 1.33 1.54 0.10
0.8 1.2 0.8 1.2 1.25 1.64 0.20
0.5 1.5 0.8 1.2 1.12 1.88 0.35
0.5 1.5 0.5 1.5 1.13 2.07 0.53

δ = σu + Aσu2.

(10)

This can be considered as a quadratic for u with
solution which when squared gives u2, the χ2 contri-
bution, as

1
2

(11)

σ −

u2 =

2 + 4A δ

2(1 + 4A δ
σ )
4A2
This is not really exact,
in that it only takes one
branch of the solution, the one approximating to the
straight line, and does not consider the extra possi-
bility that the δ value could come from an improb-
able u value the other side of the turning point of
the parabola. Given this imperfection it makes sense
to expand the square root as a Taylor series, which,
neglecting correction terms above the second power,
leads to

χ2 = (

)2

2A(

) + 5A2(

)2

.

(12)

δ
σ

1
(cid:18)

−

δ
σ

δ
σ

(cid:19)

This provides a sensible form for χ2 from asym-
It is important to keep the δ4 term
metric errors.
rather than stopping at δ3 to ensure χ2 stays posi-
tive! Adding higher orders does not have a great ef-
fect. We recommend it for consideration when it is
required (e.g. in ﬁtting parton distribution functions)
to form a χ2 from asymmetric errors

2.8. Weighted means

The ‘best’ estimate (i.e. unbiassed and with small-
est variance) from several measurements xi with dif-
ferent (symmetric) errors σi is given by a weighted
sum with wi = 1/σ2
i . We wish to ﬁnd the equivalent
for asymmetric errors.

As noted earlier, when sampling from an asymmet-
ric distribution the result is biassed towards the tail.
The expectation value
is not the location param-
x
i
h
eter x. So for an unbiassed estimator one must take
wi(xi −

(13)

bi)/

ˆx =

wi

b =

σ+

σ−

−
√2π

(Model 1)

b = α (Model 2)

The variance of this is given by

V =

w2
i Vi
wi)2

(
P
where Vi is the variance of the ith measurement about
P
its mean. Diﬀerentiating with respect to wi to ﬁnd
the minimum gives

(14)

(15)

2wiVi

2

(

wj )2 −

w2
j Vj
wj )3 = 0

i
∀

(16)

(

P

P

P

Figure 4: Examples of combined errors using Model 2.

where

X

X

Again the true curves (solid) are not well repro-
duced by the ‘usual procedure’ (dashed) but the
curves with the correct cumulants (dotted) do a good
job. (The sharp behaviour at the edge of the curves
is due to the turning point of the parabola.)

2.7. Evaluating χ2

For Model 1 the χ2 contribution from a discrepancy
δ is just δ2/σ+2 or δ2/σ−
2 as appropriate. This is
manifestly inelegant, especially for minimisation pro-
cedures as the value goes through zero.

WEMT002

PHYSTAT2003, Stanford Linear Accelerator Center, September 2003

5

which is satisﬁed by wi = 1/Vi. This is the equivalent
of the familiar weighting by 1/σ2. The weights are
given, depending on the Model, by (see Equations 8
and 9)

V = σ2 + (1

or

V = σ2 + 2α2

(17)

2
π

−

)α2

Note that this is not the Maximum Liklelihood es-
timator - writing down the likelihood in terms of the
χ2 and diﬀerentiating does not give a nice form - so
in principle there may be better estimators, but they
will not have the simple form of a weighted sum.

3. Asymmetric Statistical Errors

As explained earlier, (log) likelihood curves are used
to obtain the maximum likelihood estimate for a pa-
rameter and also the 68% central interval – taken as
the values at which ln L falls by 1
2 from its peak. For
large N this curve is a parabola, but for ﬁnite N it
is generally asymmetric, and the two points are not
equidistant about the peak.

The bias, if any, is not connected to the form of the
curve, which is a likelihood and not a pdf. Evaluat-
ing a bias is done by integrating over the measured
value not the theoretical parameter. We will assume
for simplicity that these estimates are bias free. This
means that when combining errors there will be no
shift of the quoted value.

3.1. Combining asymmetric statistical
errors

Suppose estimates ˆa and ˆb are obtained by this
method for variables a and b. a could typically be
an estimate of the total number of events in a sig-
nal region, and b the (scaled and negated) estimate of
background, obtained from a sideband. We are inter-
ested in u = a + b, taking ˆu = ˆa + ˆb. What are the
errors to be quoted on ˆu?

3.2. Likelihood functions known

We ﬁrst consider the case where the likelihood func-

tions La(~x

a) and Lb(~x
|

b) are given.
|

For the symmetric Gaussian case, the answer is well
known. Suppose that the likelihoods are both Gaus-
sian, and further that σa = σb = σ. The log likelihood
term

ˆa

a

−
σ

(cid:18)

(cid:19)

2

2

+

ˆb

b
−
σ !

 

(18)

WEMT002

can be rewritten

ˆa + ˆb

(a + b)

−
σ

1
2  

2

!

+

ˆa

1
2  

ˆb

−

−
σ

(a

b)

−

2

!

(19)

−

so the likelihood is the product of Gaussians for u =
a + b and v = a

b, with standard deviations √2σ.

−

Picking a particular value of v, one can then triv-
ially construct the 68% conﬁdence region for u as
√2σ, ˆu + √2σ]. Picking another value of v, in-
[ˆu
deed any other value of v, one obtains the same region
for u. We can therefore say with 68% conﬁdence that
these limits enclose the true value of u, whatever the
value of v. The uninteresting part of a and b has been
‘parametrised away’. This is, of course, the standard
result from the combination of errors formula, but de-
rived in a frequentist way using Neyman-style conﬁ-
dence intervals. We could construct the limits on u by
ﬁnding ˆu+σ+
u such that the integrated probability of a
result as small as or smaller than the data be 16%, and
1
similarly for σ−u , rather than taking the ∆ ln L =
2
shortcut, and it would not aﬀect the argument.

−

The question now is how to generalise this. For this

to be possible the likelihood must factorise

L(~x

a, b) = Lu(~x
|

v)
u)Lv(~x
|
|

(20)

with a suitable choice of the parameter v and the func-
tions Lu and Lv. Then we can use the same argument:
for any value of v the limits on u are the same, de-
pending only on Lu(~x
u). Because they are true for
|
any v they are true for all v, and thus in general.

There are cases where this can clearly be done. For
= σb the result is the same
two Gaussians with σa 6
as above but with v = aσ2
a. For two Poisson
distributions v is a/b. There are cases (with multiple
peaks) where it cannot be done, but let us hope that
these are artiﬁcially pathological.

b −

bσ2

−

On the basis that if it cannot be done, the question
is unanswerable, let us assume that it is possible in the
case being studied, and see how far we can proceed.
Finding the form of v is liable to be diﬃcult, and as
it is not actually used in the answer we would like to
avoid doing so. The limits on u are read oﬀ from the
1
∆ ln L(~x
u, v) =
2 points where v can have any value
|
provided it is ﬁxed. Let us choose v = ˆv, the value
at the peak. This is the value of v at which Lv(v)
is a maximum. Hence when we consider any other
value of u, we can ﬁnd v = ˆv by ﬁnding the point at
b, or
which the likelihood is a maximum, varying a
a, or b, or any other combination, always keeping a+ b
ﬁxed. We can read the limits oﬀ a 1 dimensional plot
of ln Lmax(~x
u), where the ‘max’ suﬃx denotes that
|
at each value of u we search the subspace to pick out
the maximum value.

−

This generalises to more complicated situations. If
u = a + b + c we again scan the ln Lmax(~x
u) function,
|
where the subspace is now 2 dimensional.

6

PHYSTAT2003, Stanford Linear Accelerator Center, September 2003

3.3. Likelihood functions not completely
known

In many cases the likelihood functions for a and b
will not be given, merely estimates ˆa and ˆb and their
a , σ−a , σ+
asymmetric errors σ+
b and σ−b . All we can do
a)
is to use these to provide best guess functions La(~x
|
b). A parametrisation of suitable shapes,
and Lb(~x
|
which for σ+
σ− approximate to a parabola, must
be provided. Choosing a suitable parametrisation is
not trivial. The obvious choice of introducing small
higher-order terms fails as these dominate far from the
peak. A likely candidate is:

∼

ln L(a) =

1
2

−

ln (1 + a/γ)
ln β

2

(21)

(cid:18)
and γ = σ+σ−
where β = σ+/σ
σ− . This describes the
usual parabola, but with the x-axis stretched by an
amount that changes linearly with distance. Figure 5
shows two illustrative results. The ﬁrst is the Poisson

(cid:19)

σ+

−

−

errors). Here a and b could be results from diﬀer-
ent channels or diﬀerent experiments. This can be
regarded as a special case, constrained to a = b, i.e.
v = 0, but this is rather contrived. It is more direct
just to say that one uses the log likelihood which is
the sum of the two separate functions, and determines
1
the peak and the ∆ ln L =
2 points from that. If the
−
functions are known this is unproblematic, if only the
errors are given then the same parametrisation tech-
nique can be used.

4. Conclusions

If asymmetric errrors cannot be avoided they need

careful handling.

A method is suggested and a program provided for
It is not
combining asymmetric systematic errors.
‘rigorously correct’ but such perfection is impossible.
Unlike the usual method, it is at least open about its
assumptions and mathematically consistent.

Formulæ for χ2 and weighted sums are given.
A method is proposed for combining asymmetric
statistical errors if the likelihood functions are known.
Work is in progress to enable it to be used given only
the results and their errors.

Acknowledgments

The author a gratefully acknowledges the support

of the Fulbright Foundation.

Figure 5: Approximations using Equation 21

References

likelihood from 5 observed events (solid line) for which
2 points is µ = 5+2.58
the estimate using the ∆ ln L = 1
1.92,
as shown. The dashed line is that obtained inserting
these numbers into Equation 21. The second considers
a measurement of x = 100
10, of which the logarithm
has been taken, to give a value 4.605+0.095
0.105. Again,
the solid line is the true curve and the dashed line
the parametrisation. In both cases the agreement is
excellent over the range
1σ and reasonable over
the range

≈ ±

3σ.

±

−

−

To check the correctness of the method we can use
the combination of two Poisson numbers, for which
the result is known. First indications are that the
errors obtained from the parametrisation are indeed
closer to the true Poisson errors than those obtained
from the usual technique.
3.4. Combination of Results

≈ ±

A related problem is to ﬁnd the combined estimate
ˆu given estimates ˆa and ˆb (which have asymmetric

WEMT002

[1] D.E. Groom et al., Eur. Phys. J. C15 1 (2000).
[2] W. T. Eadie et al, “Statistical Methods in Exper-

imental Physics”, North Holland, 1971.

[3] A.G. Frodesen et al. “Probablity and Statistics
in Particle Physics”, Universitetsforlaget Bergen-
Oslo-Tromso (1979), pp 236-239.

[4] R. J. Barlow “Systematic Errors: Facts and Fic-
tions” in Proc. Durham conference on Advanced
Statistical Techniques in Particle Physics, M. R.
Whalley and L. Lyons (Eds). IPPP/02/39. 2002.
System-
preprint MAN/HEP/03/02,

“Asymmetric

Barlow,

[5] R.

J.

atic
Errors”
ArXiv:physics/030613.

[6] G. D’Agostini “Bayesian Reasoning in Data Anal-
ysis: a Critical Guide”, World Scientiﬁc (2003).
[7] The Shorter Oxford English Dictionary, Vol I (A-
M) p 190 and p 551 of the 3rd edition (1977).


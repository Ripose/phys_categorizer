BARI-TH 482/04

Extending Granger causality to nonlinear systems

Nicola Ancona1, Daniele Marinazzo2,3, Sebastiano Stramaglia2,3,4
1 Istituto di Studi sui Sistemi Intelligenti per l’Automazione,
C.N.R., Bari, Italy,
2TIRES-Center of Innovative Technologies for Signal Detection and Processing,
Universit`a di Bari, Italy
3 Dipartimento Interateneo di Fisica, Bari, Italy
4Istituto Nazionale di Fisica Nucleare,
Sezione di Bari, Italy
(Dated: December 6, 2013)

We consider extension of Granger causality to nonlinear bivariate time series. In this frame, if
the prediction error of the ﬁrst time series is reduced by including measurements from the second
time series, then the second time series is said to have a causal inﬂuence on the ﬁrst one. Not all
the nonlinear prediction schemes are suitable to evaluate causality, indeed not all of them allow
to quantify how much the knowledge of the other time series counts to improve prediction error.
We present a novel approach with bivariate time series modelled by a generalization of radial basis
functions and show its application to a pair of unidirectionally coupled chaotic maps and to a
physiological example.

PACS numbers: 05.10.-a,87.10.+e,89.70.+c

I.

INTRODUCTION

Identifying causal relations among simultaneously acquired signals is an important problem in computational time
series analysis and has applications in economy [1-2], EEG analysis [3], human cardiorespiratory system [4], interaction
between heart rate and systolic arterial pressure [5], and many others. Several papers dealt with this problem relating
it to the identiﬁcation of interdependence in nonlinear dynamical systems [6], or to estimates of information rates
[7, 8]. Some approaches modelled data by oscillators and concentrated on the phases of the signals [9]. One major
approach to analyze causality between two time series is to examine if the prediction of one series could be improved
by incorporating information of the other, as proposed by Granger [1] in the context of linear regression models of
stochastic processes. In particular, if the prediction error of the ﬁrst time series is reduced by including measurements
from the second time series in the linear regression model, then the second time series is said to have a causal inﬂuence
on the ﬁrst time series. By exchanging roles of the two time series, one can address the question of causal inﬂuence
in the opposite direction. It is worth stressing that, within this deﬁnition of causality, ﬂow of time plays a major role
in making inference, from time series data, depending on direction. Since Granger causality was formulated for linear
models, its application to nonlinear systems may not be appropriate. The question we address in this paper is: how
is it possible to extend Granger causality deﬁnition to nonlinear problems?

In the next section we review the original approach by Granger while describing our point of view about its nonlinear
extension; we also propose a method, exploiting radial basis functions, which fulﬁlls the requirements a prediction
scheme should satisfy to analyze causality. In section (III) we show application of the proposed method to simulated
and real examples. Some conclusions are drawn in Section (IV).

II. GRANGER CAUSALITY

A. Linear modelling of bivariate time series.

We brieﬂy recall the Vector AutoRegressive (VAR) model which is used to deﬁne linear Granger causality [1]. Let
In the following we will
{¯xi}i=1,.,N and {¯yi}i=1,.,N be two time series of N simultaneously measured quantities.
assume that time series are stationary. For k = 1 to M (where M = N − m, m being the order of the model), we
denote xk = ¯xk+m, yk = ¯yk+m, Xk = (¯xk+m−1, ¯xk+m−2, ..., ¯xk), Yk = (¯yk+m−1, ¯yk+m−2, ..., ¯yk) and we treat these
quantities as M realizations of the stochastic variables (x, y, X, Y). The following model is then considered [10]:

x = W11 · X + W12 · Y,
y = W21 · X + W22 · Y,

(1)

4
0
0
2
 
y
a
M
 
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
0
0
5
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

2

(2)

(3)

(4)

(5)

{W} being four m-dimensional real vectors to be estimated from data. Application of least squares techniques yields
the solutions:

and

where Σ matrices and T vectors are the estimates, based on the data set at hand, of the following average values:

W11
W12 (cid:19)

(cid:18)

=

(cid:18)

Σxx Σxy
Σyx Σyy (cid:19)

T11
T12 (cid:19)

,

(cid:18)

W21
W22 (cid:19)

(cid:18)

=

(cid:18)

Σxx Σxy
Σyx Σyy (cid:19)

T21
T22 (cid:19)

,

(cid:18)

−1

−1

M

M

[Σxx]αβ = hXαXβi = 1
[Σxy]αβ = hXαYβi = 1
[Σyx]αβ = hYαXβi = 1
[Σyy]αβ = hYαYβi = 1
M
= 1
[T11]α = hxXαi
M
= 1
[T12]α = hxYαi
M
= 1
[T21]α = hyXαi
M
= 1
[T22]α = hyYαi
M

M

M
αX k
k=1 X k
M
αY k
k=1 X k
M
α X k
k=1 Y k
M
k=1 Y k
α Y k
M
k=1 xkX k
M
k=1 xkY k
M
k=1 ykX k
M
k=1 ykY k

β α, β = 1, ..., m
β α, β = 1, ..., m
β α, β = 1, ..., m
β α, β = 1, ..., m
α α = 1, ..., m
α α = 1, ..., m
α α = 1, ..., m
α α = 1, ..., m

P
P
P
P
P
P
P
P

Let us call ǫxy and ǫyx the prediction errors of this model, deﬁned as the estimated variances of x − W11 · X − W12 · Y
and y − W21 · X − W22 · Y respectively. In particular

P
P
We also consider AutoRegressive (AR) predictions of the two time series, i.e. the model

ǫxy = 1
M
ǫyx = 1
M

M

k=1(xk − W11 · Xk − W12 · Yk)2;
k=1(yk − W21 · Xk − W22 · Yk)2.

M

x = V1 · X,
y = V2 · Y.

D =

c2 − c1
c1 + c2

.

In this case the least squares approach provides V1 = Σ−1
yy T22. The estimate of the variance of
x − V1 · X is called ǫx (the prediction error when x is predicted solely on the basis of the knowledge of its past values);
similarly ǫy is the variance of y − V2 · Y. If the prediction of x improves by incorporating the past values of {yi}, i.e.
ǫxy is smaller than ǫx, then y has a causal inﬂuence on x. Analogously, if ǫyx is smaller than ǫy, then x has a causal
inﬂuence on y. Calling c1 = ǫx − ǫxy and c2 = ǫy − ǫyx, a directionality index can be introduced:

xx T11 and V2 = Σ−1

if Y is uncorrelated with X and x, then ǫx = ǫxy.

The index D varies from 1 in the case of unidirectional inﬂuence (x → y) to −1 in the opposite case (y → x), with
intermediate values corresponding to bidirectional inﬂuence. According to this deﬁnition of causality, the following
property holds for N suﬃciently large:
Indeed in this case
Σxy = Σyx = 0 and T12 = 0, therefore W12 = 0. This means that VAR and AR modelling of the {xi} time
series coincide. Analogously if X is uncorrelated with Y and y, then ǫy = ǫyx. It is clear that these properties are
fundamental and make the linear prediction approach suitable to evaluate causality. On the other hand, for nonlinear
systems higher order correlations may be relevant. Therefore, we propose that any prediction scheme providing a
nonlinear extension of Granger causality should satisfy the following property: (P1) if Y is statistically independent
of X and x, then ǫx = ǫxy; if X is statistically independent of Y and y, then ǫy = ǫyx.
In a recent paper [11],
use of a locally linear prediction scheme [12] has been proposed to evaluate nonlinear causality. In this scheme, the
joint dynamics of the two time series is reconstructed by delay vectors embedded in an Euclidean space; in the delay
embedding space a locally linear model is ﬁtted to data. The approach described in [11] satisﬁes property P1 only
if the number of points in the neighborhood of each reference point, where linear ﬁt is done, is suﬃciently high to
establish good statistics; however linearization is valid only for small neighborhoods. It follows that this approach to
nonlinear causality requires very long time series to satisfy P1. In order to construct methods working eﬀectively with
moderately long time series, in the next subsection we will characterize the problem of extending Granger causality
as the one of ﬁnding classes of nonlinear models satisfying property P1.

What is the most general class of nonlinear models which satisfy P1? The complete answer to this question is

matter for further study. Here we only give a partial answer, i.e. the following family of models:

B. Nonlinear models.

x = w11 · Φ (X) + w12 · Ψ (Y) ,
y = w21 · Φ (X) + w22 · Ψ (Y) ,

where {w} are four n-dimensional real vectors, Φ = (ϕ1, ..., ϕn) are n given nonlinear real functions of m variables,
and Ψ = (ψ1, ..., ψn) are n other real functions of m variables. Given Φ and Ψ, model (6) is a linear function in
the space of features ϕ and ψ; it depends on 4n variables, the vectors {w}, which must be ﬁxed to minimize the
prediction errors

ǫxy = 1
M
ǫyx = 1
M

M
k=1(xk − w11 · Φ
M
k=1(yk − w21 · Φ

P
P

− w12 · Ψ
− w22 · Ψ

Xk
Xk
(cid:0)
(cid:0)

(cid:1)
(cid:1)

Yk
Yk
(cid:0)
(cid:0)

)2;
)2.
(cid:1)
(cid:1)

We also consider the model:

x = v1 · Φ (X) ,
y = v2 · Ψ (Y) ,

and the corresponding prediction errors ǫx and ǫy.

Now we prove that model (6) satisﬁes P1. Let us suppose that Y is statistically independent of X and x. Then,

for each µ = 1, .., n and for each λ = 1, .., n: ψµ (Y) is uncorrelated with x and with ϕλ (X). It follows that

variance [x − w11 · Φ (X) − w12 · Ψ (Y)] = variance [x − w11 · Φ (X)] + variance [w12 · Ψ (Y)] .

(9)

As a consequence, for large N , at the minimum of ǫxy one has w12 = 0. The same argument may be used exchanging
x and y. This proves that P1 holds.

The solution of least squares ﬁtting of model (6) to data may be written in the following form:

where † denotes the pseudo-inverse matrix [13]; S matrices and t vectors are given by:

Solution of model (8) is given by v1 = S1

†t1 and v2 = S2

†t2.

w11
w12 (cid:19)

(cid:18)

w21
w22 (cid:19)

(cid:18)

= (S1 S2)† t1,

= (S2 S1)† t2,

Xk
(cid:0)

[S1]kρ = ϕρ
k = 1, ..., M, ρ = 1, ..., n
[S2]kρ = ψρ(Yk) k = 1, ..., M, ρ = 1, ..., n
[t1]k = xk
[t2]k = yk

k = 1, ..., M
k = 1, ..., M

(cid:1)

C. Radial basis functions.

Radial basis functions (RBF) methods were initially proposed to perform exact interpolation of a set of data points
in a multidimensional space (see, e.g., [14]); subsequently an alternative motivation for RBF methods was found
within regularization theory [15]. RBF models have been used to model ﬁnancial time series [16].

In this subsection we propose a strategy to choose the functions Φ and Ψ, in model (6), in the frame of RBF
ρ=1, in the space of X vectors, are determined by a clustering procedure
ρ=1, in the space of Y vectors, are determined by a clustering

k=1. Analogously n centers { ˜Yρ}n

methods. Fixed n ≪ M , n centers { ˜Xρ}n
applied to data {Xk}M
procedure applied to data {Yk}M

k=1. We then make the following choice:

ϕρ (X) = exp

ψρ (Y) = exp

−kX − ˜Xρk2/2σ2
(cid:17)
−kY − ˜Yρk2/2σ2

ρ = 1, ..., n,

ρ = 1, ..., n,

(cid:16)

(cid:16)

(cid:17)

3

(6)

(7)

(8)

(10)

(11)

4

(12)

(13)

(14)

σ being a ﬁxed parameter, whose order of magnitude is the average spacing between the centers. Centers { ˜Xρ} are the
prototypes of X variables, hence ϕ functions measure the similarity to these typical patterns. Analogously, ψ functions
measure the similarity to typical patterns of Y. Many clustering algorithm may be applied to ﬁnd prototypes, for
example in our experiments we use fuzzy c-means [17].

Some remarks are in order. First, we observe that the models described above may trivially be adapted to handle
the case of reconstruction embedding of the two time series in a delay coordinate space, as described in [11]. Second,
we stress that in (6) x and y are modelled as the sum of two contributions, one depending solely on X and the other
dependent on Y. Obviously better prediction models for x and y exists, but they would not be useful to evaluate
causality unless they would satisfy P1. This requirement poses a limit to the level of detail at which the two time
series may be described, if one is looking at causality relationships. The justiﬁcation of the model we propose here,
based on regularization theory, is sketched in the Appendix.

D. Empirical risk and generalization error.

In the previous subsections the prediction error has been identiﬁed as the empirical risk, although there is a diﬀerence
between these two quantities as Statistical Learning Theory (SLT) [18] shows. The deep connection between empirical
risk and generalization error deserves a comment here. First of all we want to point out that the ultimate goal of
a predictor and in general of any supervised machine x = f (X) [19] is to generalize, that is to correctly predict
the output values x corresponding to never seen before input patterns X (for deﬁniteness we consider the case of
predicting x on the basis of the knowledge of X). A measure of the generalization error of such a machine f is the
risk R[f ] deﬁned as the expected value of the loss function V (x, f (X)):

R[f ] =

dx dX V (x, f (X)) P (x, X),

Z

2

where P (x, X) is the probability density function underlying the data. A typical example of loss function is
V (x, f (X)) = (x − f (X))
and in this case the function minimizing R[f ] is called the regression function. In gen-
eral P is unknown and so we can not minimize the risk. The only data we have are M observations (examples)
S = {(xk, Xk)}M
k=1 of the random variables x and X drawn according to P (x, X). Statistical learning theory [18] as
well as regularization theory [15] provide upper bounds of the generalization error of a learning machine f . Inequalities
of the following type may be proven:

where

R[f ] ≤ ǫx + C,

ǫx =

M

1
M

2

xk − f (Xk)
(cid:1)

Xk=1 (cid:0)

is the empirical risk, that measures the error on the training data. C is a measure of the complexity of machine f
and it is related to the so-called Vapnik-Chervonenkis (VC) dimension. Predictors with low complexity guarantee low
generalization error because they avoid overﬁtting to occur. When the complexity of the functional space where our
predictor lives is small, then the empirical risk is a good approximation of the generalization error. The models we
deal with in this work verify such constraint. In fact, linear predictors have a ﬁnite VC-dimension, equal to the size of
the space where the input patterns live, and predictors expressed as linear combinations of radial basis functions are
smooth. In conclusion empirical risk is a good measure of the generalization error for the predictors we are considering
here and so it can be used to construct measures of causality between time series [20].

In order to demonstrate the use of the proposed approach, in this section we study two examples, a pair of

unidirectionally coupled chaotic maps and a bivariate physiological time series.

III. EXPERIMENTS.

Let us consider the following pair of noisy logistic maps:

A. Chaotic maps.

xn+1 = a xn (1 − xn) + sηn+1,
yn+1 = e a yn (1 − yn) + (1 − e) a xn (1 − xn) + sξn+1;

{η} and {ξ} are unit variance Gaussianly distributed noise terms; parameter s determines their relevance. We ﬁx
a = 3.8, and e ∈ [0, 1] represents the coupling x → y. In the noise-free case (s = 0), a transition to synchronization
(xn = yn) occurs at e = 0.37. We evaluate the Lyapunov exponents by the method described in [21]: the ﬁrst
exponent is 0.43, the second exponent depends on e and is depicted in Fig. 1 for e < 0.37 (it becomes negative for
e > 0.37). For several values of e, we have considered runs of 105 iterations, after 105 transient, and evaluated the
prediction errors by (6) and (8), with m = 1, n = 100 and σ = 0.05. In ﬁg. 2a we depict, in the noise free case, the
curves representing c1 and c2 versus coupling e. In ﬁgures 2b, 2c and 2d we depict the directionality index D versus e,
in the noise free case and for s = 0.01 and s = 0.07 respectively. In the noise free case we ﬁnd D = 1, i.e. our method
revealed unidirectional inﬂuence. As the noise increases, also the minimum value of e, which renders unidirectional
coupling detectable, increases.

B. Physiological data.

As a real example, we consider time series of heart rate and breath rate of a sleeping human suﬀering from sleep
apnea (ten minutes from data set B of the Santa Fe Institute time series contest held in 1991, available in the Physionet
data bank [22]). There is a growing evidence that suggests a causal link between sleep apnea and cardiovascular disease
[23], although the exact mechanisms that underlie this relationship remain unresolved [24]. Figure 3 clearly shows
that bursts of the patient breath and cyclical ﬂuctuations of heart rate are interdependent. We ﬁx m = 1 and n = 50;
varying σ we ﬁnd that both ǫx (x representing heart rate) and ǫy (y representing breath) have a minimum at σ close
to 0.5. In ﬁg. 4 we depict the directionality index D vs σ, around σ = 0.5. Since we ﬁnd D positive, we may conclude
that the causal inﬂuence of heart rate on breath is stronger than the reverse [25]. This data have been already
analyzed in [7], measuring the rate of information ﬂow (transfer entropy), and a stronger ﬂow of information from the
heart rate to the breath rate was found. In this example, the rate of information ﬂow entropy and Granger nonlinear
causality give consistent results: both these quantities, in the end, measure the departure from the generalized Markov
property [7]

5

(15)

(16)

P (x | X) = P (x | X, Y),
P (y | Y) = P (y | X, Y).

IV. CONCLUSIONS.

The components of complex systems in nature rarely display a linear interdependence of their parts: identiﬁcation
of their causal relationships provides important insights on the underlying mechanisms. Among the variety of methods
which have been proposed to handle this important task, a major approach was proposed by Granger [1]. It is based
on the improvement of predictability of one time series due to the knowledge of the second time series: it is appealing
for its general applicability, but is restricted to linear models. While extending Granger approach to the nonlinear
case, on one hand one would like to have the most accurate modelling of the bivariate time series, on the other hand
the goal is to quantify how much the knowledge of the other time series counts to reach this accuracy. Our analysis is
rooted on the fact that any nonlinear modelling of data, suitable to study causality, should satisfy the property P1,
described in Section (II). It is clear that this property sets a limit on the accuracy of the model; we have proposed a
class of nonlinear models which satisfy P1 and constructed an RBF like approach to nonlinear Granger causality. Its
performances, in a simulated case and a real physiological application, have been presented. We conclude remarking
that use of this deﬁnition of nonlinear causality may lead to discover genuine causal structures via data analysis, but
to validate the results the analysis has to be accompanied by substantive theory.

Acknoledgements. The authors thank Giuseppe Nardulli and Mario Pellicoro for useful discussions about causality.

We show how the choice of functions (11) arise in the frame of regularization theory. Let z be a function of X and
Y. We assume that z is the sum of a term depending solely on X and one depending on Y: z(X, Y) = f (X) + g(Y).
We also assume the knowledge of the values of f and g at points { ˜Xρ, ˜Yρ}ρ=1,..,n:

Let us denote ˆK(~ω) the Fourier transform of K(~r) = exp(−r2/2σ2). The following functional is a measure of the
smoothness of z(X,Y):

V. APPENDIX.

f ( ˜Xρ) = f ρ ρ = 1, ..., n,
g( ˜Yρ) = gρ ρ = 1, ..., n.

S[z] =

d~ω

Z

| ˆf (~ω)|2 + |ˆg(~ω)|2
ˆK(~ω)

.

Indeed it penalizes functions with relevant contributions from high frequency modes. Variational calculus shows that
the function that minimize S under the constraints (17) is given by:

n

Xρ=1

z =

µρ K

X − ˜Xρ

+

λρ K

Y − ˜Yρ

;

(cid:16)

(cid:17)

Xρ=1

(cid:16)

(cid:17)

n

where {µ} and {λ} are tunable Lagrange multipliers to solve (17). Hence model (6)-(11) corresponds to the class of
the smoothest functions, sum of a term depending on X and a term depending on Y, with assigned values on a set
of n points.

6

(17)

(18)

(19)

[1] C.W.J. Granger, Econometrica 37, 424 (1969).
[2] J.J. Ting, Physica A 324, 285 (2003).
[3] P. Tass et al., Phys. Rev. Lett. 81, 3291 (1998); M. Le van Quyen et al., Brain Res. 792, 24 (1998); E. Rodriguez et al.,

[4] C. Ludwig, Arch. Anat. Physiol. 13, 242 (1847); C. Schafer et al., Phys. Rev. E 60, 857 (1999); M. G. Rosemblum et al.,

[5] S. Akselrod et al., Am. J. Physiol. Heart. Circ. Physiol. 249, H867 (1985); G. Nollo et al., Am. J. Physiol. Heart. Circ.

Nature (London) 397, 430 (1999).

Phys. Rev. E 65, 41909 (2002).

Physiol. 283, H1200 (2002).

[6] S. J. Schiﬀ et al., Phys. Rev. E 56, 6708 (1996); J. Arnhold et al., Physica D 134, 419 (1999); R. Quian Quiroga et al.,

Phys. Rev. E 61, 5142 (2000); R. Quian Quiroga et al., Phys. Rev. E 65, 41903 (2002).

[7] T. Schreiber, Phys. Rev. Lett. 85, 461 (2000).
[8] M. Palus et al., Phys. Rev. E 63, 46211 (2001).
[9] F. R. Drepper, Phys. Rev. E 62, 6376 (2000); M. G. Rosemblum et al., Phys. Rev. E 64, 45202R (2001).
[10] Usually both times series are normalized in the preprocessing stage, i.e. they are linearly transformed to have zero mean

and unit variance.

[11] Y. Chen et al., Phys. Lett. A 324, 26 (2004).
[12] J.D. Farmer and J.J. Sidorowich, Phys. Rev. Lett. 59, 845 (1987).
[13] C. R. Rao and S.K. Mitra, Generalized Inverse of Matrices and Its Applications (John Wiley, New York, 1971).
[14] C. M. Bishop, Neural networks for pattern recognition (Oxford University Press, New York, 1995).
[15] T. Poggio, F. Girosi, Science 247, 978 (1990).
[16] J. Hutchinson, A Radial Basis Function Approach to Financial Time Series Analysis, Ph.D. Thesis, Massachusetts Institute

of Technology, Department of Electrical Engineering and Computer Science (1994).

[17] J. C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms (Plenum Press, New York, 1981).
[18] V. Vapnik, Statistical Learning Theory (John Wiley & Sons, INC., 1998).
[19] Machine means ’algorithm which learns from data’ in the machine learning community.
[20] In the general case, Leave-one-out (Loo) error Eloo[f ] provides a better estimate of the generalization error of f (Luntz
and Brailovsky theorem) than the empirical risk, given a ﬁnite number of training data. Loo-error is deﬁned as the error
variance when the prediction for the k-th pattern is made using the model trained on the M-1 other patterns; it needs M
predictors to be trained, where M is the cardinality of the data set. Hence, Loo-error estimation is unfeasible to compute
for large training sets. For linear predictors, like the ones we consider in this paper, the empirical risk is already a good
estimate, due to the low complexity of these machines.

[21] H. F. von Bremen et al., Physica D 101, 1 (1997).
[22] http://www.physionet.org/

[23] F. Roux et al., Am. J. Med. 108, 396 (2000).
[24] H. W. Duchna et al., Somnologie 7, 101 (2003).
[25] These results may also be due to coupling of the two signals to a common external driver.

7

0.1

0.2
e

0.3

FIG. 1: The second Lyapunov exponent of the coupled maps (15) is plotted versus coupling e.

0.4

λ

0

0

−3

x 10

a) 

4

2

0

2
c
,
1
c

−2
0

1.5

c) 

1

D

0.5

0.1

0.2
e

0.3

0

0.1

0.2
e

0.3

1.5

b) 

1

D

0.5

1.5

d) 

1

D

0.5

0

0.1

0.2
e

0.3

0

0.1

0.2
e

0.3

FIG. 2: (a) For the noise free case of coupled maps (15), c1 = ǫx − ǫxy (dashed line) and c2 = ǫy − ǫyx (solid line) are plotted
versus coupling e. (b) The directionality index D (see the text) is plotted versus e in the noise free case. (c) The directionality
index D is plotted versus e, s = 0.01. (d) D is plotted versus e, s = 0.07.

8

)

m
p
b
(
 
e
t
a
r
 
t
r
a
e
h

)
a
P

(
 
e
r
u
s
s
e
r
p
 
g
n
u

l

100

75

50

4

2

0

−2
0

4
x 10

0.2

D

0.1

0

200

400

600

time (s)

FIG. 3: Time series of the heart RR (upper) and breath signal (lower) of a patient suﬀering sleep apnea. Data sampled at 2
Hz.

0.3

0.5

0.7

0.9

σ

FIG. 4: The directionality index D is plotted versus σ for the physiological application, around σ = 0.5. Solid line is the
3th-polynomial best ﬁt of points, here shown only for illustrative purposes.


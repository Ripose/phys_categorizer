3
0
0
2
 
c
e
D
 
1
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
9
7
0
2
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

PHYSTAT2003, SLAC, September 8-11, 2003

1

Likelihood Inference in the Presence of Nuisance Parameters

N. Reid, D.A.S. Fraser
Department of Statistics, University of Toronto, Toronto Canada M5S 3G3

We describe some recent approaches to likelihood based inference in the presence of nuisance parameters.
Our approach is based on plotting the likelihood function and the p-value function, using recently developed
third order approximations. Orthogonal parameters and adjustments to proﬁle likelihood are also discussed.
Connections to classical approaches of conditional and marginal inference are outlined.

1. INTRODUCTION

We take the view that the most eﬀective form of
inference is provided by the observed likelihood func-
tion along with the associated p-value function.
In
the case of a scalar parameter the likelihood func-
tion is simply proportional to the density function.
The p-value function can be obtained exactly if there
is a one-dimensional statistic that measures the pa-
rameter.
If not, the p-value can be obtained to a
high order of approximation using recently developed
methods of likelihood asymptotics.
In the presence
of nuisance parameters, the likelihood function for a
(one-dimensional) parameter of interest is obtained
via an adjustment to the proﬁle likelihood function.
The p-value function is obtained from quantities com-
puted from the likelihood function using a canonical
parametrization ϕ = ϕ(θ), which is computed locally
at the data point. This generalizes the method of
eliminating nuisance parameters by conditioning or
In Section
marginalizing to more general contexts.
2 we give some background notation and introduce
the notion of orthogonal parameters. In Section 3 we
illustrate the p-value function approach in a simple
model with no nuisance parameters. Proﬁle likelihood
and adjustments to proﬁle likelihood are described in
Section 4. Third order p-values for problems with nui-
sance parameters are described in Section 5. Section
6 describes the classical conditional and marginal like-
lihood approach.

2. NOTATION AND ORTHOGONAL
PARAMETERS

We assume our measurement(s) y can be modelled
as coming from a probability distribution with density
or mass function f (y; θ), where θ = (ψ, λ) takes values
in Rd. We assume ψ is a one-dimensional parameter
of interest, and λ is a vector of nuisance parameters.
If there is interest in more than one component of
θ, the methods described here can be applied to each
component of interest in turn. The likelihood function
is

and

L(θ) = L(θ; y) = c(y)f (y; θ);

(1)

THAT001

it is deﬁned only up to arbitrary multiples which may
depend on y but not on θ. This ensures in particu-
lar that the likelihood function is invariant to one-to-
one transformations of the measurement(s) y. In the
context of independent, identically distributed sam-
pling, where y = (y1, . . . , yn) and each yi follows the
model f (y; θ) the likelihood function is proportional
to Πf (yi; θ) and the log-likelihood function becomes a
sum of independent and identically distributed com-
ponents:

ℓ(θ) = ℓ(θ; y) = Σ log f (yi; θ) + a(y).

(2)

The maximum likelihood estimate ˆθ is the value of
θ at which the likelihood takes its maximum, and in
regular models is deﬁned by the score equation

′

ℓ

(ˆθ; y) = 0.

The observed Fisher information function j(θ) is the
curvature of the log-likelihood:

j(θ) =

′′

ℓ

(θ)

−
and the expected Fisher information is the model
quantity

i(θ) = E

′′

ℓ

=

(θ)
}

{−

′′

ℓ

−

Z

(θ; y)f (y; θ)dy.

(5)

If y is a sample of size n then i(θ) = O(n).

In accord with the partitioning of θ we partition the
observed and expected information matrices and use
the notation

i(θ) =

iψψ iψλ
iλψ iλλ

(cid:18)

(cid:19)

−1(θ) =

i

iψψ iψλ
iλψ iλλ

.

(cid:19)

(cid:18)

We say ψ is orthogonal to λ (with respect to expected
Fisher information) if iψλ(θ) = 0. When ψ is scalar
a transformation from (ψ, λ) to (ψ, η(ψ, λ)) such that
ψ is orthogonal to η can always be found (Cox and
Reid, [1]). The most directly interpreted consequence

(3)

(4)

(6)

(7)

2

PHYSTAT2003, SLAC, September 8-11, 2003

of parameter orthogonality is that the maximum like-
lihood estimates of orthogonal components are asymp-
totically independent.

Example 1: ratio of Poisson means Suppose
y1 and y2 are independent counts modelled as Poisson
with mean λ and ψλ, respectively. Then the likelihood
function is

L(ψ, λ; y1, y2) = e

−λ(1+ψ)ψy2λy1+y2

and ψ is orthogonal to η(ψ, λ) = λ(ψ + 1).
In
fact in this example the likelihood function factors as
L1(ψ)L2(η), which is a stronger property than param-
eter orthogonality. The ﬁrst factor is the likelihood for
a binomial distribution with index y1 + y2 and proba-
bility of success ψ/(1 + ψ), and the second is that for
a Poisson distribution with mean η.

Example 2:

exponential regression Sup-
pose yi, i = 1, . . . , n are independent observations,
each from an exponential distribution with mean
ψxi), where xi is known. The log-likelihood
λ exp(
function is

−

ℓ(ψ, λ; y) =

n log λ + ψΣxi −

−

−1Σyi exp(ψxi) (8)
λ

and iψλ(θ) = 0 if and only if Σxi = 0. The stronger
property of factorization of the likelihood does not
hold.

3. LIKELIHOOD INFERENCE WITH NO
NUISANCE PARAMETERS

We assume now that θ is one-dimensional. A plot
of the log-likelihood function as a function of θ can
quickly reveal irregularities in the model, such as a
non-unique maximum, or a maximum on the bound-
ary, and can also provide a visual guide to deviance
from normality, as the log-likelihood function for a
normal distribution is a parabola and hence symmet-
ric about the maximum.
In order to calibrate the
log-likelihood function we can use the approximation

r(θ) = sign(ˆθ

θ)[2

ℓ(ˆθ)

ℓ(θ)
}

]1/2 ·
∼

N (0, 1),

(9)

{

−

−
which is equivalent to the result that twice the log like-
lihood ratio is approximately χ2
1. This will typically
provide a better approximation than the asymptoti-
cally equivalent result that

ˆθ

−

θ

·

∼

N (0, i

−1(θ))

(10)

as it partially accommodates the potential asymme-
try in the log-likelihood function. These two approx-
imations are sometimes called ﬁrst order approxima-
tions because in the context where the log-likelihood
is O(n), we have (under regularity conditions) results
such as

r(θ; y)

Pr
{

r(θ; y0)
}

≤

= Pr
{

Z

≤
1 + O(n

r(θ; y0)
}
−1/2)
}

{

(11)

Table I The p-values for testing µ = 0, i.e. that the
number of observed events is consistent with the
background.

upper p-value 0.0005993
lower p-value
0.0002170
mid p-value
0.0004081
Φ(r∗
0.0003779
)
Φ(r)
0.0004416
Φ{(ˆθ − θ)ˆj1/2} 0.0062427

where Z follows a standard normal distribution.
It
is relatively simple to improve the approximation to
third order, i.e. with relative error O(n−3/2), using
the so-called r∗ approximation

∗

r

(θ) = r(θ) +

1/r(θ)
}

{

{

log

q(θ)/r(θ)

N (0, 1)

} ∼

(12)
where q(θ) is a likelihood-based statistic and a gener-
alization of the Wald statistic (ˆθ
θ)j1/2(ˆθ); see Fraser
[2].
Example 3: truncated Poisson

−

Suppose that y follows a Poisson distribution with
mean θ = b + µ, where b is a background rate that
is assumed known. In this model the p-value function
can be computed exactly simply by summing the Pois-
son probabilities. Because the Poisson distribution is
discrete, the p-value could reasonably be deﬁned as
either

or

Pr(y

y0; θ)

≤

Pr(y < y0; θ),

(13)

(14)

sometimes called the upper and lower p-values, respec-
tively.

For the values y0 = 17, b = 6.7, Figure 1 shows
the likelihood function as a function of µ and the p-
value function p(µ) computed using both the upper
In Figure 2 we plot the mid p-
and lower p-values.
value, which is

Pr(y < y0) + (1/2)Pr(y = y0).

(15)

The approximation based on r∗ is nearly identical to
the mid-p-value; the diﬀerence cannot be seen on Fig-
ure 2. Table 1 compares the p-values at µ = 0. This
example is taken from Fraser, Reid and Wong [3].

4. PROFILE AND ADJUSTED PROFILE
LIKELIHOOD FUNCTIONS

We now assume θ = (ψ, λ) and denote by ˆλψ the
restricted maximum likelihood estimate obtained by

THAT001

PHYSTAT2003, SLAC, September 8-11, 2003

3

maximizing the likelihood function over the nuisance
parameter λ with ψ ﬁxed. The proﬁle likelihood func-
tion is

Lp(ψ) = L(ψ, ˆλψ);

(16)

also sometimes called the concentrated likelihood or
the peak likelihood. The approximations of the pre-
vious section generalize to

r(ψ) = sign( ˆψ

ψ)[2

ℓp( ˆψ)

−

{

ℓp(ψ)
}

−

]1/2 ·
∼

N (0, 1),

(17)

and

ˆψ

−

·

∼

ψ

N (0,

iψψ(θ)
}

{

−1).

(18)

→ ∞

These approximations, like the ones in Section 3, are
derived from asymptotic results which assume that
n
, that we have a vector y of independent, iden-
tically distributed observations, and that the dimen-
sion of the nuisance parameter does not increase with
n. Further regularity conditions are required on the
model, such as are outlined in textbook treatments
of the asymptotic theory of maximum likelihood. In
ﬁnite samples these approximations can be mislead-
ing: proﬁle likelihood is too concentrated, and can be
maximized at the ‘wrong’ value.

Example 4: normal theory regression Suppose
yi = x′
iβ + ǫi, where xi = (xi1, . . . , xip) is a vector of
known covariate values, β is an unknown parameter
of length p, and ǫi is assumed to follow a N (0, ψ)
distribution. The maximum likelihood estimate of ψ
is

ˆψ =

1
n

Σ(yi −

′
x
i

ˆβ)2

(19)

which tends to be too small, as it does not allow for
the fact that p unknown parameters (the components
of β) have been estimated. In this example there is
a simple improvement, based on the result that the
likelihood function for (β, ψ) factors into

L1(β, ψ; ¯y)L2{

ψ; Σ(yi −

′
x
i

ˆβ)2

}

(20)

where L2(ψ) is proportional to the marginal distri-
ˆβ)2. Figure 3 shows the proﬁle
bution of Σ(yi −
likelihood and the marginal likelihood; it is easy to
verify that the latter is maximized at

x′
i

ˆψm =

1

−

n

Σ(yi −

p

′
x
i

ˆβ)2

(21)

which in fact is an unbiased estimate of ψ.

Example 5: product of exponential means
Suppose we have independent pairs of observations
Exp(ψ/λi), i =
y1i, y2i, where y1i ∼
1, . . . , n. The limiting normal theory for proﬁle likeli-
hood does not apply in this context, as the dimension

Exp(ψλi)

y2i ∼

0
3

0
2

0
1

0

d
o
o
h

i
l

e
k

i
l

l

e
u
a
v
-
p

6
0
0

.

4
0

.

0

2
0

.

0

0
0

.

0
1

.

8

.

0

6

.

0

4
.
0

2
.
0

0
.
0

0

10

30

40

20

mu

20

mu

.....................................
....
.....
....
....
.....
.....
......
......
......
......
.......
.......
......
.......
.......
.......
.......
.......
........
........
........
........
........
........
.......
.......
.......
.......
........
.......
.......
.......
......
.......
......
......
......
......
......
.....
.....
......
.....
.....
.....
.....
.....
.....
....
....
....
........................................... .....

0

10

30

40

Figure 1: The likelihood function (top) and p-value
function (bottom) for the Poisson model, with b = 6.7
and y0 = 17. For µ = 0 the p-value interval is
(0.99940, 0.99978).

1.0

0.8

0.6

0.4

0.2

0.0

p−value

Figure 2: The upper and lower p-value functions and the
mid-p-value function for the Poisson model, with b = 6.7
and y0 = 17. The approximation based on Φ(r∗
) is
identical to the mid-p-value function to the drawing
accuracy.

THAT001

m
4

PHYSTAT2003, SLAC, September 8-11, 2003

Figure 3: Proﬁle likelihood and marginal likelihood for
the variance parameter in a normal theory regression
with 21 observations and three covariates (the ”Stack
Loss” data included in the Splus distribution). The
proﬁle likelihood is maximized at a smaller value of ψ,
and is narrower; in this case both the estimate and its
estimated standard error are too small.

l

i

a
n
g
r
a
m

e

l
i
f
o
r
p

6

5

4

3

2

1

1.0

0.8

0.6

0.4

0.2

0.0

likelihood

and

of the parameter is not ﬁxed but increasing with the
sample size, and it can be shown that

ˆψ

→

π
4

ψ

(22)

as n

(Cox and Reid [4]).

→ ∞

The theory of higher order approximations can be
used to derive a general improvement to the proﬁle
likelihood or log-likelihood function, which takes the
form

ℓa(ψ) = ℓp(ψ) +

log

1
2

jλλ(ψ, ˆλψ)
|
|

+ B(ψ)

(23)

where

where jλλ is deﬁned by the partitioning of the ob-
served information function, and B(ψ) is a further
adjustment function that is Op(1). Several versions of
B(ψ) have been suggested in the statistical literature:
we use the one deﬁned in Fraser [5] given by

B(ψ) =

1
2

−

log

′
λ(ψ, ˆλψ)jϕϕ( ˆψ, ˆλ)ϕ
ϕ
|

′

λ(ψ, ˆλψ)
|

. (24)

This depends on a so-called canonical parametrization
ϕ = ϕ(θ) = ℓ;V (θ; y0) which is discussed in Fraser,
Reid and Wu [6] and Reid [7].

In the special case that ψ is orthogonal to the nui-
sance parameter λ a simpliﬁcation of ℓa(ψ) is available
as

ℓCR(ψ) = ℓp(ψ)

1
2

−

log

jλλ(ψ, ˆλψ)
|
|

(25)

THAT001

j
|
|
In i.i.d.

which was ﬁrst introduced in Cox and Reid (1987).
The change of sign on log
comes from the or-
sampling, ℓp(ψ) is
thogonality equations.
Op(n), i.e. is the sum of n bounded random variables,
whereas log
is Op(1). A drawback of ℓCR is that
it is not invariant to one-to-one reparametrizations
In contrast
of λ, all of which are orthogonal to ψ.
ℓa(ψ) is invariant to transformations θ = (ψ, λ) to
θ′ = (ψ, η(ψ, λ)), sometimes called interest-respecting
transformations.

j
|

|

Example 5 continued In this example ψ is or-

thogonal to λ = (λ1, . . . , λn), and

ℓCR(ψ) =

(3n/2) log ψ

(2/ψ)Σ√(y1iy2i).

(26)

−

−

The value that maximizes ℓCR is ’more nearly con-
sistent’ than the maximum likelihood estimate as
ˆψCR −→

(π/3)ψ.

5. P -VALUES FROM PROFILE
LIKELIHOOD

The limiting theory for proﬁle likelihood gives ﬁrst

order approximations to p-values, such as

p(ψ)

.
= Φ(rp)

p(ψ)

.
= Φ

( ˆψ

ψ)j1/2
p

( ˆψ)
}

{

−
although the discussion in the previous section sug-
gests these may not provide very accurate approxima-
tions. As in the scalar parameter case, though, a much
better approximation is available using Φ(r∗) where

∗

r

(ψ) = rp(ψ) + 1/

rp(ψ)
}

{

log

Q(ψ)/rp(ψ)
}

{

(29)

where Q can also be derived from the likelihood func-
tion and a function ϕ(θ, y0) as

(27)

(28)

Q = (ˆν

−

−1/2
ˆνψ)ˆσ
ν

ν(θ) = eT

ψϕ(θ) ,
eψ = ψϕ′(ˆθψ)/
ψϕ′ (ˆθψ)
,
|
|
j(θθ)(ˆθ)
j(λλ)(ˆθψ)
ˆσ2
/
ν =
|
|
|
|
−2 ,
ϕθ′ (ˆθ)
jθθ(ˆθ)
j(θθ)(ˆθ)
=
|
|
|
||
|
−2 .
ϕλ′ (ˆθψ)
jλλ(ˆθψ)
j(λλ)(ˆθψ)
|
||
|
|
|

=

,

The derivation is described in Fraser, Reid and
Wu [6] and Reid [7]. The key ingredients are the
log-likelihood function ℓ(θ) and a reparametrization
ϕ(θ) = ϕ(θ; y0), which is deﬁned by using an approx-
imating model at the observed data point y0; this ap-
proximation in turn is based on a conditioning argu-
ment. A closely related approach is due to Barndorﬀ-
Nielsen; see Barndorﬀ-Nielsen and Cox [8, Ch. 7], and
the two approaches are compared in [7].

s
PHYSTAT2003, SLAC, September 8-11, 2003

5

Table II Employment of men and women at the Space
Telescope Science Institute, 1998–2002 (from Science
magazine, Volume 299, page 993, 14 February 2003).

Left Stayed Total
18
2
20

19
7
26

Men
1
Women 5
6
Total

0

2
−

4
−

6
−

8
−

Example 7: Poisson with estimated back-
ground Suppose in the context of Example 3 that
we allow for imprecision in the background, replacing
b by an unknown parameter β with estimated value ˆβ.
We assume that the background estimate is obtained
from a Poisson count x, which has mean kβ, and the
signal measurement is an independent Poisson count,
y, with mean β+µ. We have ˆβ = x/k and var ˆβ = β/k,
so the estimated precision of the background gives us
a value for k. For example, if the background is es-
2.1 this implies a value for k of
timated to be 6.7
6.7/(2.1)2 .
= 1.5. Uncertainty in the standard error
of the background is ignored here. We now outline
the steps in the computation of the r∗ approximation
(29).

±

The log-likelihood function based on the two inde-

pendent observations x and y is

ℓ(β, µ) = x log(kβ)
with canonical parameter ϕ = (log β, log(β + µ))′.

kβ + y log(β + µ)

−

−

−

β

µ (31)

Then

ϕθ′(θ) =

∂ϕ(θ)
∂θ′ =

0

1/β

 

1/(β + µ) 1/(β + µ) !

,

(32)

from which

Then we have

χ(ˆθ) = −

χ(ˆθψ) = −

ϕ

−1
θ′ =

β β + µ
β

0 !

−
−

 

ψϕ′ = (

β, β + µ).

−

(33)

(34)

√

ˆβµ log( ˆβ) + ( ˆβµ + µ) log( ˆβ + ˆµ)
µ + ( ˆβµ + µ)2
ˆβ2
ˆβµ log( ˆβµ) + ( ˆβµ + µ) log( ˆβµ + µ)
µ + ( ˆβµ + µ)2
ˆβ2

{

}

(35)

,(36)

√
{

}

j(θθ)(ˆθ)
|
|
j(λλ)(ˆθψ)
|
|

= y1y2 = k/ ˆβ( ˆβ + ˆµ)
y1( ˆβµ + µ)2 + y2 ˆβ2
µ
( ˆβµ + µ)2 + ˆβ2
µ

=

(37)

(38)

and ﬁnally

Q =

( ˆβµ + µ) log

ˆβ + ˆµ
ˆβµ + µ ! −

ˆβµ log

ˆβ
ˆβµ )

(

 
k ˆβ( ˆβ + ˆµ)
}
k ˆβ( ˆβµ + µ)2 + ( ˆβ + ˆµ) ˆβ2
µ}

1/2

{

{

.

1/2

(39)

The likelihood root is

r = sign(Q)√[2

ℓ( ˆβ, ˆµ)
= sign(Q)√(2[k ˆβ log

ℓ( ˆβµ, µ)
]
}
) + ( ˆβ + ˆµ)

{

−
ˆβ/ ˆβµ}
( ˆβ + ˆµ)/( ˆβµ + µ)
}
( ˆβµ + µ)
}

ˆβ + ˆµ

ˆβµ)

− {

−

{

−

log
{
k( ˆβ

−

]).

(41)

(40)

1.0

0.8

0.6

0.4

0.2

0.0

p−value

Figure 4: The p-value function for the log-odds ratio, ψ,
for the data of Table II. The value ψ = 0 corresponds to
the hypothesis that the probabilities of leaving are equal
for men and women.

Example 6: comparing two binomials Table 2
shows the employment history of men and women at
the Space Telescope Science Institute, as reported in
Science Feb 14 2003. We denote by y1 the number
of males who left and model this as a Binomial with
sample size 19 and probability p1; similarly the num-
ber of females who left, y2, is modelled as Binomial
with sample size 7 and probability p2. We write the
parameter of interest

ψ = log

p1(1
p2(1

p2)
p1)

.

−
−

(30)

The hypothesis of interest is p1 = p2, or ψ = 0. The p-
value function for ψ is plotted in Figure 4. The p-value
at ψ = 0 is 0.00028 using the normal approximation
to rp, and is 0.00048 using the normal approximation
to r∗. Using Fisher’s exact test gives a mid p-value
of 0.00090, so the approximations are anticonservative
in this case.

THAT001

y
6

PHYSTAT2003, SLAC, September 8-11, 2003

The third order approximation to the p-value function
is 1

Φ(r∗), where

log fcond(s

t; ψ), and that

|

−

p(ψ) = Φ(r

)

∗

(45)

∗

r

= r + (1/r) log(Q/r).

(42)

where

Figure 5 shows the p-value function for µ using the
mid-p-value function from the Poisson with no adjust-
ment for the error in the background, and the p-value
Φ(r∗). The p-value for testing µ = 0
function from 1
is 0.00464, allowing for the uncertainty in the back-
ground, whereas it is 0.000408 ignoring this uncer-
tainty.

−

The hypothesis Ey = β could also be tested by
modelling the mean of y as νβ, say, and testing the
value ν = 1.
In this formulation we can eliminate
the nuisance parameter exactly by using the binomial
distribution of y conditioned on the total x + y, as
described in example 1. This gives a mid-p-value of
0.00521. The computation is much easier than that
outlined above, and seems quite appropriate for test-
ing the equality of the two means. However if infer-
ence about the mean of the signal is needed, in the
form of a point estimate or conﬁdence bounds, then
the formulation as a ratio seems less natural at least
in the context of HEP experiments. A more complete
comparison of methods for this problem is given in
Linnemann [8].

6. CONDITIONAL AND MARGINAL
LIKELIHOOD

In special model classes,

it is possible to elimi-
nate nuisance parameters by either conditioning or
marginalizing. The conditional or marginal likelihood
then gives essentially exact inference for the parame-
ter of interest, if this likelihood can itself be computed
exactly. In Example 1 above, L1 is the density for y2
conditional on y1 + y2, so is a conditional likelihood
for ψ. This is an example of the more general class of
linear exponential families:

f (y; ψ, λ) = exp
{

′
ψs(y)+λ

t(y)

c(ψ, λ)

d(y)
}

−

; (43)

−

in which

fcond(s

t; ψ) = exp
{

|

ψs

−

Ct(ψ)

Dt(s)
}

−

(44)

deﬁnes the conditional likelihood. The comparison of
two binomials in Example 6 is in this class, with ψ
. The
as deﬁned at (30) and λ = log
diﬀerence of two Poisson means, in Example 7, can-
not be formulated this way, however, even though the
Poisson distribution is an exponential family, because
the parameter of interest ψ is not a component of the
canonical parameter.

p2)
}

p2/(1

−

{

It can be shown that in models of the form (43)
the log-likelihood ℓa(ψ) = ℓp(ψ) + (1/2) log
ap-
proximates the conditional log-likelihood ℓcond(ψ) =

jλλ|
|

∗

)

r

log(

Q
ra

= ra +

ra =
[2
±
{
Q = ( ˆψa −

1
ra
ℓa( ˆψa)
ψ)
{
approximates the p-value function with relative error
O(n−3/2) in i.i.d. sampling. An asymptotically equiv-
alent approximation based on the proﬁle log-likelihood
is

−
ja( ˆψ)
}

ℓa(ψ)
}
1/2

]1/2

p(ψ) = Φ(r

)

∗

(46)

where

∗

r

= rp +

Q
rp

)

log(

1
rp
ℓp( ˆψ)

[2

rp =

±
Q = ( ˆψ

{

ψ)
{

−

−
jp( ˆψ)
}

]1/2
ℓp(ψ)
}
jλλ(ψ, ˆλψ)
1/2
1/2 |
|
jλλ( ˆψ, ˆλ)
1/2
|
|

.

In the latter approximation an adjustment for nui-
sance parameters is made to Q, whereas in the former
the adjustment is built into the likelihood function.
Approximation (46) was used in Figure 3.

A similar discussion applies to the class of transfor-
mation models, using marginal approximations. Both
classes are reviewed in Reid [9].

Acknowledgments

The authors wish to thank Anthony Davison and
Augustine Wong for helpful discussion. This research
was partially supported by the Natural Sciences and
Engineering Research Council.

References

[1] D.R. Cox and N. Reid, “Parameter Orthogonal-
ity and Approximate Conditional Inference”, J. R.
Statist. Soc. B, 47, 1, 1987.

[2] D.A.S. Fraser, “Statistical Inference: Likelihood to
Signiﬁcance”, J. Am. Statist. Assoc. 86 258, 1991.
[3] D.A.S. Fraser, N. Reid and A. Wong, “On
arXiv:
Inference
for Bounded Parameters”,
physics/0303111, v1, 27 Mar 2003. to appear in
Phys. Rev. D.

[4] D.R. Cox and N. Reid, “A Note on the Diﬀerence
between Proﬁle and Modiﬁed Proﬁle Likelihood”,
Biometrika 79, 408, 1992.

THAT001

PHYSTAT2003, SLAC, September 8-11, 2003

7

d
n
u
o
r
g
k
c
a
b
 
d
e
t
a
m

i
t
s
e

d
n
u
o
r
g
k
c
a
b
 
n
w
o
n
k

u
m

0
2

0
4

0
3

0
1

0

1.0

0.8

0.6

0.4

0.2

0.0

p−value

Figure 5: Comparison of the p-value functions computed assuming the background is known and using the mid-p-value
with the third order approximation allowing a background error of ±1.75.

[5] D.A.S. Fraser, “Likelihood for Component Param-

1999.

eters”, Biometrika 90, 327, (2003).

[6] D.A.S. Fraser, N. Reid and J. Wu, “A Simple Gen-
eral Formula for Tail Probabilities for Frequen-
tist and Bayesian Inference”, Biometrika 86, 246,

[7] N. Reid, “Asymptotics and the Theory of Infer-

ence”, Ann. Statist., to appear, 2004.

[9] J. T. Linnemann, “Measures of signiﬁcance in HEP
and astrophysics”, Phystat 2003, to appear, 2004.

THAT001

8

PHYSTAT2003, SLAC, September 8-11, 2003

[9] N. Reid, “Likelihood and Higher-Order Approxi-
mations to Tail Areas: a Review and Annotated

Bibliography”, Canad. J. Statist. 24, 141, 1996.

THAT001


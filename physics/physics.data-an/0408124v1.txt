4
0
0
2
 
g
u
A
 
0
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
4
2
1
8
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Boosted Decision Trees, an Alternative to Artiﬁcial Neural Networks

Byron P. Roe, Hai-Jun Yang∗
Department of Physics, University of Michigan, Ann Arbor, MI 48109, USA

Ji Zhu
Department of Statistics, University of Michigan, Ann Arbor, MI 48109, USA

Yong Liu, Ion Stancu
Department of Physics and Astronomy, University of Alabama, Tuscaloosa, AL 35487, USA

Gordon McGregor
Los Alamos National Laboratory, Los Alamos, NM 87545, USA
(Dated: February 17, 2014)

The eﬃcacy of particle identiﬁcation is compared using artiﬁcial neutral networks and boosted
decision trees. The comparison is performed in the context of the MiniBooNE, an experiment at
Fermilab searching for neutrino oscillations. Based on studies of Monte Carlo samples of simulated
data, particle identiﬁcation with boosting algorithms has better performance than that with artiﬁcial
neural networks for the MiniBooNE experiment. Although the tests in this paper were for one
experiment, it is expected that boosting algorithms will ﬁnd wide application in physics.

PACS numbers: 07.05.Mh, 07.05.Kf, 14.60.Lm

I.

INTRODUCTION

The artiﬁcial neural network (ANN) technique has
been widely used in data analysis of High Energy Physics
experiments in the last decade. The use of the ANN
technique usually gives better results than the traditional
simple-cut techniques. In this paper, another data classi-
ﬁcation technique, boosting, is introduced for data anal-
ysis in the MiniBooNE experiment[1] at Fermi National
Accelerator Laboratory. The MiniBooNE experiment is
designed to conﬁrm or refute the evidence for νµ → νe
oscillations at ∆m2 ≃ 1 eV 2/c4 found by the LSND
experiment[2]. It is a crucial experiment which will im-
ply new physics beyond the standard model if the LSND
signal is conﬁrmed. Based on our studies, particle identi-
ﬁcation (PID) with the boosting algorithm is 20 to 80%
better than that with our standard ANN PID technique,
the boosting performance relative to that of ANN de-
pends on the Monte Carlo samples and PID variables.
Although the boosting algorithm was tested in only one
experiment, it’s anticipated to have wide application in
physics, especially in data analysis of particle physics ex-
periments for signal and background events separation.
The boosting algorithm is one of the most powerful
learning techniques introduced during the past decade.
The boosting algorithm is a procedure that combines
many “weak” classiﬁers to achieve a ﬁnal powerful clas-
siﬁer. Boosting can be applied to any classiﬁcation
method.
In this paper, it is applied to decision trees.
Two boosting algorithms, AdaBoost[3] and ǫ-Boost[4],
are considered. A brief description of boosting algorithms

∗Corresponding author, e-mail address: yhj@umich.edu

is given in the next section. Our results are presented in
Section III, while we summarize our conclusions in Sec-
tion IV.

II. BRIEF DESCRIPTION OF BOOSTING

A. Decision Tree

Suppose one is trying to divide events into signal and
background and suppose Monte Carlo samples of each
are available. Divide each Monte Carlo sample into two
parts. The ﬁrst part, the training sample, will be used
to train the decision tree, and the second part, the test
sample, to test the ﬁnal classiﬁer after training.

For each event, suppose there are a number of PID vari-
ables useful for distinguishing between signal and back-
ground. Firstly, for each PID variable, order the events
by the value of the variable. Then pick variable one and
for each event value see what happens if the training sam-
ple is split into two parts, left and right, depending on
the value of that variable. Pick the splitting value which
gives the best separation into one side having mostly sig-
nal and the other mostly background. Then repeat this
for each variable in turn. Select the variable and split-
ting value which gives the best separation. Initially there
was a sample of events at a “node”. Now there are two
samples called “branches”. For each branch, repeat the
process, i.e., again try each value of each variable for the
events within that branch to ﬁnd the best variable and
splitting point for that branch. One keeps splitting un-
til a given number of ﬁnal branches, called leaves, are
obtained, or until each leaf is pure signal or pure back-
ground, or has too few events to continue. This descrip-
tion is a little oversimpliﬁed. In fact at each stage one

picks as the next branch to split, the branch which will
give the best increase in the quality of the separation. A
schematic of a decision tree is shown in Fig.1, in which
3 variables are used for signal/background separation:
event hit multiplicity, energy, and reconstructed radial
position.

What criterion is used to deﬁne the quality of separa-
tion between signal and background in the split? Imagine
the events are weighted with each event having weight
Wi. Deﬁne the purity of the sample in a branch by

P =

Ps Ws
Ps Ws + Pb Wb

,

where Ps is the sum over signal events and Pb is the
sum over background events. Note that P (1 − P ) is 0
if the sample is pure signal or pure background. For a
given branch let

2

S/B
52/48

< 100

PMT Hits?

≥ 100

< 0.2 GeV

≥ 0.2 GeV

S/B
9/10

Radius?

< 500 cm

≥ 500 cm

S
39/1

S/B
48/11

Energy?

B
2/9

B
4/37

S
7/1

Gini = (

Wi)P (1 − P ),

n

X
i=1

FIG. 1: Schematic of a decision tree. S for signal, B for back-
ground. Terminal nodes(called leaves) are shown in boxes.
If signal events are dominant in one leave, then this leave is
signal leave; otherwise, background leave.

where n is the number of events on that branch. The
criterion chosen is to minimize

Ginilef t son + Giniright son.

To determine the increase in quality when a node is

split into two branches, one maximizes

Criterion = Ginif ather − Ginilef t son − Giniright son.

At the end, if a leaf has purity greater than 1/2 (or
whatever is set), then it is called a signal leaf and if the
purity is less than 1/2, it is a background leaf. Events
are classiﬁed signal if they land on a signal leaf and back-
ground if they land on a background leaf. The resulting
tree is a decision tree.

Decision trees have been available for some time[5].
They are known to be powerful but unstable, i.e., a small
change in the training sample can give a large change in
the tree and the results.

B. Boosting

Within the last few years a great improvement has
been made[6, 7, 8]. Start with unweighted events and
build a tree as above. If a training event is misclassiﬁed,
i.e, a signal event lands on a background leaf or a back-
ground event lands on a signal leaf, then the weight of
that event is increased (boosted).

A second tree is built using the new weights, no longer
equal. Again misclassiﬁed events have their weights
boosted and the procedure is repeated. Typically, one
may build 1000 or 2000 trees this way.

A score is now assigned to an event as follows. The
event is followed through each tree in turn. If it lands
on a signal leaf it is given a score of 1 and if it lands on

a background leaf it is given a score of -1. The renor-
malized sum of all the scores, possibly weighted, is the
ﬁnal score of the event. High scores mean the event is
most likely signal and low scores that it is most likely
background. By choosing a particular value of the score
on which to cut, one can select a desired fraction of the
signal or a desired ratio of signal to background. For
those familiar with ANNs, the use of this score is the
same as the use of the ANN value for a given event. For
the MiniBooNE experiment, boosting has been found to
be superior to ANNs. Statisticians and computer scien-
tists have found that this method of classiﬁcation is very
eﬃcient and robust. Furthermore, the amount of tuning
needed is rather modest compared with ANNs. It works
well with many PID variables. If one makes a monotonic
transformation of a variable, so that if x1 > x2 then
f (x1) > f (x2), the boosting method gives exactly the
same results. It depends only on the ordering according
to the variable, not on the value of the variable.

In articles on boosting within the statistics and com-
puter science communities, it is often recommended that
short trees with eight leaves or so be used. For the Mini-
BooNE Monte Carlo samples it was found that large trees
with 45 leaves worked signiﬁcantly better.

C. Some Boosting Algorithms

If there are N total events in the sample, the weight of
each event is initially taken as 1/N . Suppose that there
are Ntree trees and m is the index of an individual tree.
Let

• xi = the set of PID variables for the ith event.

• yi = 1 if the ith event is a signal event and yi = −1

if the event is a background event.

• wi = the weight of the ith event.

• Tm(xi) = 1 if the set of variables for the ith event
lands that event on a signal leaf and Tm(xi) = −1
if the set of variables for that event lands it on a
background leaf.

• I(yi 6= Tm(xi)) = 1 if yi 6= Tm(xi) and 0 if yi =

Tm(xi).

There are at least two commonly used methods for boost-
ing the weights of the misclassiﬁed events in the training
sample.

The ﬁrst boosting method is called AdaBoost[3]. De-

ﬁne for the mth tree:

errm = P

N
i=1 wiI(yi 6= Tm(xi))
N
i=1 wi

P

.

αm = β × ln((1 − errm)/errm).

β = 1 is the value used in the standard AdaBoost
method. For the MiniBooNE Monte Carlo samples,
β = 0.5 has been found to give better results. Change
the weight of each event i, i = 1, ..., N :

wi → wi × eαmI(yi6=Tm(xi)).

Each classiﬁer Tm is required to be better than random
guessing with respect to the weighted distribution upon
which the classiﬁer is trained. Thus, errm is required to
be less than 0.5, since, otherwise, the weights would be
updated in the wrong direction. Next, renormalize the
N
weights, wi → wi/ P
i=1 wi. The score for a given event
is

T (x) =

αmTm(x),

Ntree

X
m=1

which is just the weighted sum of the scores over the
individual trees, see Fig.2.

The second boosting method is called ǫ-Boost[4], or
sometimes “shrinkage”. After the mth tree, change the
weight of each event i, i = 1, ..., N :

wi → wie2ǫI(yi6=Tm(xi)),

where ǫ is a constant of the order of 0.01. Renormalize
N
the weights, wi → wi/ P
i=1 wi. The score for a given
event is

T (x) =

ǫTm(x),

Ntree

X
m=1

which is the renormalized, but unweighted, sum of the
scores over individual trees.

3

 a

mTm(x)

TM(x)

T3(x)

T2(x)

T1(x)

Weighted Sample

Weighted Sample

Weighted Sample

Training Sample

FIG. 2: Schematic of a boosting procedure.

III. RESULTS

For the νµ → νe oscillation search in the MiniBooNE
experiment[1], the main backgrounds come from intrinsic
νe contamination in the beam, mis-identiﬁed νµ quasi-
elastic scattering and mis-identiﬁed neutral current π0
production. Since intrinsic νe events are real νe events,
the PID variables cannot distinguish them from oscilla-
tion νe events. This report concentrates on separating
the non-νe events from the νe events. Good sensitiv-
ity for the νe appearance search requires low background
contamination from all kinds of backgrounds. Here, the
ANN and the two boosting algorithms are used to sepa-
rate νe charged current quasi-elastic (CCQE) events from
non-νe background events.

500000 Monte Carlo νµ events distributed among
the many possible ﬁnal states and 200000 intrinsic νe
CCQE events were fed into the reconstruction package R-
ﬁtter[9]. Among these events, 88233 intrinsic νe CCQE
and 162657 background events passed reconstruction and
pre-selection cuts.

The signature of each event is given by 52 variables
for the R-ﬁtter. All variables are used in the boosting
algorithms for training and testing. Since the recontruc-
tion and PID algorithms are still undergoing continuous
modiﬁcations, relative results rather than absolute per-
centages are presented in the following plots.

For the AdaBoost algorithm, the parameter β = 0.5,
the number of leaves Nleaves = 45 and the number of
tree iterations Ntree = 1000 were used. The relative
ratio(deﬁned as the number of background events kept
divided by the number kept for 50% intrinsic νe selection
eﬃciency and Ntree = 1000) as a function of νe selec-
tion eﬃciency for various tree iterations is shown in the
top plot of Fig.3 and the AdaBoost output distributions
are shown in the bottom plot. 20000 intrinsic νe CCQE
signal and 30000 background events were used for train-
ing, 68233 νe and 132657 background events were used
for testing. All results shown in the paper are for testing

S
4

a)

80

b)

80

c)

80

o
i
t
a
R
 
e
v
i
t
a
l
e
R

2.5

1.5

2

1

2

1.5

1

1.75

1.5

1.25

1

0.75

ntree = 200

ntree = 500

ntree = 800

ntree = 1000

o
i
t
a
R
 
e
v
i
t
a
l
e
R

5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

s
t
n
e
v
E
 
f
o
 
r
e
b
m
u
N

8000
7000
6000
5000
4000
3000
2000
1000
0

30

40

50

60
e selection efficiency (%)

70

80

90

30

40

50

60

70

Backgrounds

Signal

30

40

50

60

70

-40

-30

-20

-10

0

10

20

30

AdaBoost Output

FIG. 3: Top: the number of background events kept divided
by the number kept for 50% intrinsic νe selection eﬃciency
and Ntree = 1000 versus the intrinsic νe CCQE selection eﬃ-
ciency. Bottom: AdaBoost output, All kinds of backgrounds
are combined for the boosting training.

samples.

In order to quantify the performance of the boosting
algorithm, the AdaBoost results for a particular set of
PID variables were compared with ANN results. The re-
sults, compared as a function of the intrinsic νe CCQE
selection eﬃciency, are shown in Fig.4. For the intrinsic
νe signal eﬃciency ranging from 40% to 60%, the per-
formances of AdaBoost were improved by a factor of ap-
proximately 1.5 and 1.8 over the ANN if trained by the
signal and all kinds of backgrounds with 21 (red dots)
and 52 (black boxes) input variables respectively, shown
in Fig.4.a. If AdaBoost and ANN were trained by the sig-
nal and neutral current π0 background, the performances
of AdaBoost were improved by a factor of approximately
1.3 and 1.6 over the ANN for 22 (red dots) and 52 (black
boxes) training variables respectively, shown in Fig.4.b.
The best results for the ANN were found with 22 vari-
ables, while the best results for boosting were found with
52 variables. Comparison of the best ANN results and
the best boosting results indicates that, when trained by
the signal and neutral current π0 background, the ANN
results kept approximately 1.5 times more background
events than were kept by the boosting algorithms for
about 50% νe CCQE eﬃciencies.

In Fig.4.c, the ratio of the background kept for a 52
variable AdaBoost to that for a 21(red dots - results for
AdaBoost trained by the signal and all kinds of back-
grounds) / 22(black boxes - results for AdaBoost trained
by the signal and neutral current π0 background) vari-
ables is shown as a function of νe eﬃciency. It can be

30

40

50

70
e selection efficiency (%)

60

FIG. 4: Comparison of ANN and AdaBoost performance for
test samples. Relative ratio(deﬁned as the number of back-
ground events kept for ANN divided by the events kept for
AdaBoost) versus the intrinsic νe CCQE selection eﬃciency.
a) all kinds of backgrounds are combined for the training
against the signal. b) trained by signal and neutral current π0
background. c) relative ratio is re-deﬁned as the number of
background events kept for AdaBoost with 21(red)/22(black)
training variables divided by that for AdaBoost with 52 train-
ing variables. All error bars shown in the ﬁgures are for Monte
Carlo statistical errors only.

seen that the AdaBoost performance is improved by the
use of more training variables.

The above ANN and AdaBoost performance compar-
ison with diﬀerent input variables indicates that Ad-
aBoost can improve the PID performance signiﬁcantly by
using more input variables, even though many of them
have weak discriminant power; ANN, however, seems un-
likely to make full use of all input variables because it is
more diﬃcult to optimize all the weights between ANN
nodes, given more nodes in both the input and the hidden
layers.

Further evidence of this eﬀect comes from the S-
ﬁtter[10], a second reconstruction–PID program set for
the MiniBooNE. A systematic attempt was made to ﬁnd
the optimum sets of variables for ANN and for boost-
ing classiﬁers by using νe CCQE signal and π0 back-
ground (which includes 25 NUANCE reaction channels).
It is found that, for S-ﬁtter, the optimum ANN result is
achieved by a selected set of 22 variables, while for boost-
ing, no obvious improvement is seen after a selected opti-
mum set of 50 variables are used. Comparison of the best

n
n
a)

80

b)

o
i
t
a
R
 
e
v
i
t
a
l
e
R

o
i
t
a
R
 
e
v
i
t
a
l
e
R

1.6

1.5

1.4

1.3

1.2

1.1

1

0.9

0.8
1.3

1.2

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

30

40

50

60

70

ntree = 100
ntree = 200
ntree = 500
ntree = 1000
ntree = 2000
ntree = 5000
40

30

50

70
e selection efficiency (%)

60

80

FIG. 5: Comparison of AdaBoost and ǫ-Boost performance
with diﬀerent decision tree sizes (8 and 45 leaves per decision
tree) versus the intrinsic νe CCQE selection eﬃciency. a)
Relative ratio is deﬁned as the number of background events
kept for decision tree of 8 leaves divided by that for deci-
sion tree of 45 leaves, red dots with error bars represent re-
sults from AdaBoost and black boxes with error bars for ǫ-
Boost. The tree iterations were 10000 for 8 leaves/tree and
1800 for 45 leaves/tree, respectively. b) Relative ratio here
is the number of background kept for AdaBoost divided by
that for ǫ-Boost with Nleaves = 45. The performance compar-
isons of AdaBoost and ǫ-Boost with diﬀerent tree iterations
are shown in diﬀerent colors, Ntree = 100(black), 200(cyan),
500(magenta), 1000(yellow), 2000(blue), 5000(red).

ANN results and the best boosting results indicates that,
for a given fraction of νe CCQE events kept, the ANN
results kept about 1.2 times more π0 background events
than were kept by the boosting algorithms within target
range of keeping close to 50% of the νe CCQE events.

As noted in the introduction, two boosting algorithms
are considered in the present paper. The comparison of
AdaBoost and ǫ-Boost performance is shown in Fig.5,
where parameters β = 0.5 and ǫ = 0.01 were selected for
AdaBoost and ǫ-Boost training, respectively. The com-
parison between small tree size (8 leaves) and large tree
size (45 leaves) with a comparable overall number of de-
cision leaves, indicates that large tree size with 45 leaves
yields 10 ∼ 20 % better performance for the MiniBooNE
Monte Carlo samples shown in Fig.5.a.
Increasing the
tree size past 45 leaves did not produce appreciable im-
provement

Comparison of AdaBoost and ǫ-Boost performance for
the background contamination versus the intrinsic νe

5

CCQE selection eﬃciency as a function of the number
of decision tree iterations is shown in Fig.5.b. A smaller
relative ratio implies a better performance for AdaBoost.
The performance of AdaBoost is better than that of ǫ-
Boost if the relative ratio is less than 1. Boosting perfor-
mance in the high signal eﬃciency region is continuously
improved for more tree iterations. AdaBoost has better
performance than ǫ-Boost for less than about 200 tree
iterations, but becomes slightly worse than ǫ-Boost for
a large number of tree iterations, especially for νe signal
eﬃciency below ∼ 60%. For higher νe signal eﬃciency(>
70%), AdaBoost works slightly better than ǫ-Boost.

IV. CONCLUSIONS

PID variables obtained using the R-ﬁtter and the S-
ﬁtter event reconstruction programs for the MiniBooNE
experiment were used to separate signal events from
background events. The ANN and the boosting algo-
rithms were compared for PID. Based on these studies
with the MiniBooNE Monte Carlo samples, the boosting
algorithms, AdaBoost and ǫ-Boost, improved PID per-
formance signiﬁcantly compared with the artiﬁcial neu-
ral network technique. This improvement manifested
itself when a large number of PID variables was used.
For a small number of variables, the ANN classiﬁcation
was competitive, but as the number of variables was in-
creased, the boosting results proved more eﬃcient and
superior to the ANN technique.
If more variables are
needed, boosting will use them as necessary.

It was also found that boosting with a large tree size of
45 leaves worked signiﬁcantly better than boosting with a
small tree size, 8 leaves, as recommended in some statis-
tics literature.

The boosting technique proved to be quite robust. If a
transformation of variables from x to y = f (x) is made,
then as long as the ordering is preserved, that is if x2 >
x1, then y2 > y1, the boosting results are unchanged.
ANNs must be tuned for temperature, learning rate and
other variables, while for boosting, there is much less to
vary and it is quite straightforward.

There are certainly applications where ANNs prove
better than boosting. However,
for this application
boosting appears superior and seems to be exceptionally
robust and simple to use. It is anticipated that boosting
techniques will have wide application in physics.

V. ACKNOWLEDGMENTS

We wish to express our gratitude to the MiniBooNE
collaboration for their excellent work on the Monte Carlo
simulation and the software package for physics analysis.
This work is supported by the Department of Energy
and by the National Science Foundation of the United
States.

n
6

[1] E. Church et al., BooNE Proposal, FERMILAB-P-

0898(1997).

[2] A. Aguilar et al., Phys. Rev. D 64(2001) 112007.
[3] Y. Freund and R.E. Schapire (1996), “Experiments with
a new boosting algorithm.” Proc COLT, 209–217. ACM
Press, New York (1996).

[4] J. Friedman et al., Annals of Statistics, 29(5), 1189

(2001); 28(2), 337 (2000)

[5] L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone,
Classiﬁcation and Regression Trees, Wadsworth Interna-
tional Group, Belmont, California (1984).

[6] Robert E. Schapire, The boosting approach to machine
learning: An overview, MSRI Workshop on Nonlinear
Estimation and Classiﬁcation, (2002).

[7] Yoav Freund and Robert E. Schapire, A short introduc-
tion to boosting, Journal of Japanese Society for Arti-
ﬁcial Intelligence, 14(5), 771-780, (September, 1999).
(Appearing in Japanese, translation by Naoki Abe.)
[8] J. Friedman, Recent Advances in Predictive (Machine)
Learning, Proceedings of Phystat2003, Stanford U.,
(Sept. 2003).

[9] Byron P. Roe et al., BooNE-TN-117, Mar. 18, 2004.

(Journal paper under preparation.)

[10] Ion Stancu et al., BooNE-TN-36, Sept. 15, 2001; BooNE-
TN-50, Feb. 18, 2002; BooNE-TN-100, Sept. 19, 2003.
(Journal paper under preparation.)


5
0
0
2
 
r
p
A
 
1
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
5
1
4
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Delay-Coordinates Embeddings as a Data Mining
Tool for Denoising Speech Signals.

D. Napoletani1, C.A. Berenstein2, T. Sauer3a, D.C. Struppa3b and D. Walnut3c.

1

Abstract— In this paper we utilize techniques from the theory
of non-linear dynamical systems to deﬁne a notion of embedding
threshold estimators. More speciﬁcally we use delay-coordinates
embeddings of sets of coefﬁcients of the measured signal (in
some chosen frame) as a data mining tool to separate structures
that are likely to be generated by signals belonging to some
predetermined data set. We describe a particular variation of
the embedding threshold estimator implemented in a windowed
Fourier frame, and we apply it to speech signals heavily cor-
rupted with the addition of several types of white noise. Our
experimental work seems to suggest that, after training on the
data sets of interest, these estimators perform well for a variety
of white noise processes and noise intensity levels. The method
is compared, for the case of Gaussian white noise, to a block
thresholding estimator.

Index Terms— Threshold estimators, delay-coordinates embed-

dings, nonlinear systems, data-driven denoising.

I. INTRODUCTION

In this paper we explore the performance of a method of
denoising that is designed to be efﬁcient for a variety of white
noise contaminations and noise intensities, while keeping a
ﬁxed choice of parameters of the algorithm itself (adapted to
the class of signals to denoise). The method is based on a
loose distinction between the geometry of delay-coordinates
embeddings of, respectively, deterministic time series and
non-deterministic ones. Delay-coordinates embeddings are
the basis of many applications of the theory of non-linear
dynamical systems, see for example [ASY] or [KS], our work
stands apart from previous applications of embeddings in that
no exact modelization of the underlyning signals (through the
delay-coordinates embeddings) is needed nor attempted here.
Instead, we measure the overall ‘squeezing’ of the dynamics
along the principal direction of the embedding image by
computing the quotient of the largest and smallest singular
values.

We deﬁne ﬁrst of all
in which we look for
the context
signal estimators. Let F [n], n = 1, ..., N , be a discrete signal
of length N , and let X[n] = F [n] + W [n], n = 1, ..., N ,
be a contaminated measurement of F [n], where W [n] are
realizations of a white noise process W ,
throughout this
) to denote the expected value
paper we use the notation E(
∗

1 School of Computational Sciences, George Mason University, Fairfax,

VA 22030, email:dnapolet@gmu.edu

2 Institute for Systems Research, University of Maryland, College Park,

MD 20742,email:carlos@glue.umd.edu

3a,b,c Department of Mathematical Sciences,George Mason Univer-
sity,Fairfax, VA 22030,emails: 3a tsauer@gmu.edu; 3b dstruppa@gmu.edu;
3c dwalnut@gmu.edu

of a quantity

.

∗

}

F

−

gm

2
|

f
{|

Generally we are interested in estimators F such that
the expected mean square error E
is as small as
possible. For a given discrete orthonormal basis B =
}
of the N dimensional space of discrete signals, we can write:
N −1
m=0 XB[m]gm where XB[m] =< X, gm > is the
X = P
inner product of X and gm. Given such notation, we can
deﬁne a class of estimators that is amenable to theoretical
analysis, namely the class of diagonal estimators of the form
N −1
˜F = P
m=0 dm(XB[m])gm where dm(XB[m]) is a function
that depends only on the value of XB[m]. One particular
kind of diagonal estimator is the hard thresholding estimator
˜FT (for T some positive real number) deﬁned by the choice

{

˜FT =

dm(XB[m])gm

(1)

N −1

X
m=0

where

and

dm(XB[m]) = XB[m] if

XB[m]
|
|

> T

dm(XB[m]) = 0 otherwise.

If W [n] are realizations of a white normal distribution
with variance σ2, then it is shown in [DJ] that ˜FT , with
T = σ√2logN , achieves almost minimax risk (when
implemented in a wavelet basis) for the class of signals
f [n] of bounded variation. The possibility of proving such a
striking result is based, in part, on the fact that the coefﬁcients
WB[n] are realizations of a Gaussian white noise process in
any basis B.

techniques have been developed to deal with
Several
the non-Gaussian case, some of the most successful are the
Efromovich-Pinsker (EP) estimator (see for example [ELPT]
and references threin) and the block threshold estimators
of Cai and collaborators (see [CS],[C] and the more recent
[CL]). In these methods, the variance of the white process
needs to be estimated from the data, moreover, since the
threshold is designed to evaluate intensities (or
relative
intensities) of the coefﬁcients in blocks of multiwavelets,
low intensity details may be ﬁltered out as it is the case for
simpler denoising methods (see also remark 3 on the issue of
low intensity non-noisy features).

The method we describe in this paper does not need
the knowledge of the noise intensity level (thanks to the use

2

of quotients of singular values), and it is remarkably robust
to changes in the type of noise distribution.
This strenght is achieved at a price, the inner parameters of
the algorithm need to be adjusted to the data, this is true to
some extent for the EP and block thresholding algorithms
as well (see again [ELPT] and [CL]), but the number and
type of parameters that need to be trained in our approach is
increased by the need of choosing a ‘good’ delay-coordinates
embedding suitable for the data we would like to denoise.
In section V we will explore possible ways to make the
training on the data automatic, but it is yet to be seen at
this stage which data sets are amenable to the analysis
we propose. This paper is meant as a mostly experimental
analysis that suggest the method is sound at least for one
choice of data sets (namely, speech signals).

Because of
the choice of applying our algorithm to a
database of speech signals, we decided to use windowed
Fourier frames as a basic analytical tool. This is an obvious
way in which we are already adapting to the data, but more
could be used, or even collection of frames
general frames
and bases, therefore we prefer to label
as a dictionary of
analysis.
Note that any discrete periodic signal X[n], n
ZZ with
period N can be represented in a discrete windowed Fourier
frame. The atoms in this frame are of the form

D

D

∈

gm,l[n] = g[n

m]exp(

−

i2πln
N

−

), n

ZZ.

∈

(2)

We choose the window g to be a symmetric N -periodic
function of norm 1 and support q. Speciﬁcally we can choose
g to be the characteristic function of the [0, 1] interval; we
realize that this may not be the most robust choice in many
cases, but we have deliberately selected this function to avoid
excessive smoothing which was found to adversely affect our
algorithm.

the previous

Under
reconstructed from the inner products
X, gm,l >, i.e.,

conditions x can be

F

completely
X[m, l] =<

X =

1
N

N −1

N −1

X
m=0

X
l=0

F

X[m, l]˜gm,l

(3)

where

i2πln
N

(4)

˜gm,l[n] = g[n

m] exp(

), n

ZZ

{

∈

by

< X, gm,l >

−
We denote the collection
X. For ﬁnite
discrete signals of length N the reconstruction has boundary
errors. However, the region affected by such boundary effects
is limited by the size q of the support of g and we can
therefore have perfect reconstruction if we ﬁrst extend X
suitably at the boundaries of its support and then compute
the inner products
X. More details can be found in [S] and
references therein.

F

F

}

that are oriented in time direction,
double-indexed paths

the collection Cp of

γ ¯m,¯l =

{

gm,l such that l = ¯l, ¯m

m

¯m + p

,

(5)

≤

≤

}

where p is some positive integer, will be relatively sensitive
to local time changes of such ridges, since each path is a
short line in the time frequency domain oriented in the time
direction.

The choice of p is very important as different structure
in speech signals (our main case study) is evident at different
time scales. Let I = I(γ ¯m,¯l) = I(
Xγ ¯m,¯l) be a function
p. We deﬁne now a semi-local
deﬁned for each path γ ¯m,¯l ∈ C
thresholding estimator in the window Fourier
frame as
follows:

F

˜F =

1
N

N −1

N −1

X
m=0

X
l=0

dI,T (

X[m, l])˜gm,l

(6)

F

X[m, l] if I(
where dI,T (
X[m, l]) =
some γ ¯m,¯l containing (m, l), and dI,T (
I(

Xγ ¯m,¯l) < T for all γ ¯m,¯l containing (m, l).

T for
Xγ ¯m,¯l)
F
≥
X[m, l]) = 0 if
F

F

F

F

F

this threshold estimator is build to mirror the
Note that
diagonal estimators in (1), but that the ‘semilocal’ quality of
˜F is evident from the fact that all coefﬁcients in several
Xγ
are used to decide the action of the thresholding on each
coefﬁcient. This procedure is similar to block thresholding
estimators, with the additional ﬂexibility of choosing the
index function I. We propose in the next section a novel
use of embedding techniques from non-linear dynamical
systems theory to choose a speciﬁc form for I. We ﬁnd
in this way a variance independent estimator that does not
depend signiﬁcantly on the probability distribution of the
random variable W and such that we can adapt to the data in
a ﬂexible way.

II. DELAY-COORDINATES EMBEDDING IMAGES OF TIME
SERIES

We ﬁrst recall a fundamental result about reconstruction of
the state space realization of a dynamical system from its time
series measurements. Suppose S is a dynamical system, with
state space IRk and let h : IRk
IR be a measurement, i.e.,
a continuous function of the state variables. Deﬁne moreover
a function F of the state variables X as

→

F (X) = [h(X), h(S−τ (X)), ..., h(S−(d−1)τ (X))]

(7)

where by S−jτ (X) we denote the state of the system with
initial condition X at jτ time units earlier.

We say that A
to S if X
following theorem is true (see [ASY], [SYC] and [KS]):

IRk is an invariant set with respect
A for all t. Then the

A implies St(X)

⊂

∈

∈

the structure in the
Since for speech signals much of
time frequency domain is contained in localized ‘ridges’

Theorem: Let A be an m-dimensional submanifold of

IRk which is invariant under the dynamical system S. If
d > 2m, then for generic measuring functions h and generic
delays τ , the function F deﬁned in (7) is one-to-one on A.

signiﬁcant
Keeping in mind that generally the most
information about g is the knowledge of
the attractive
invariant subsets, we can say that delay maps allow to have
a faithful description of the underlining ﬁnite dimensional
dynamics, if any. The previous theorem can be extended to
invariant sets A that are not topological manifolds; in that
case more sophisticated notions of dimension are used (see
[SYC]).
Generally the identiﬁcation of
the ‘best’ τ and d that
allows for a faithful representation of the invariant subset
in practical applications (as
is considered very important
discussed in depth in [KS]), as it allows to make transparent
the properties of the invariant set itself, more particularly
we want to deduce from the data itself the dimension m of
the invariant set (if any) so that we can choose a d that is
large enough for the theorem to apply. Moreover the size
of τ has to be large enough to resolve the image far from
the diagonal, but small enough to avoid decorrelation of the
delay coordinates point.
We apply the structure of the embedding in such a way that
the identiﬁcation of the most suitable τ and d is not so crucial
, even though we will see that we do need to train such
parameters on the available data, but in a much simpler and
straightforward way. The technical reason for such robustness
in the choice of parameters will be clariﬁed later on, but
essentially we use time delay embeddings as data mining
tools rather than modelization tools as usually is the case.

To understand how such data mining is possible, we
start by applying the delay-coordinate procedure to the time
series W [n], n = 1, ..., N , for W an uncorrelated random
process; let the measuring function h be the identity function
and assume from now on that τ is an integer delay so that
1)τ ]]. For
F (W [n]) = [W [n], W [n
any embedding dimension d, the state space will be ﬁlled
according to a spherically symmetric probability distribution.
Let now ¯Z =
be the embedding
image in IRd of a time series Z for any given time delay τ .
Then we have the following very simple, but fertile lemma
relates spherical distributions to their associated to
that
principal directions

F (Z[n]), n = 1, ..., N

τ ], ..., W [n

(d

−

−

−

}

{

Lemma 1: Let σ1, σd be the variance of ¯W along the
ﬁrst principal direction (of largest extent) and the last one
(smallest) respectively. Then the expected value E
converges to 1 as N goes to inﬁnity.
Proof: Because W is a white noise process, each coordinate
of F (W [n]) is a realization of a same random variable with
some given probability density function g, therefore ¯W is a
realization of a multivariate random variable of dimension d
and symmetric probability distribution. If the expected value
of σ1
= Q > 1, then a point at a distance from the origin of
σd
σ1 has a greater probability to lie along the principal direction
associated to σ1 contradicting the fact that the probability

σ1
σd }

{

distribution of ¯W was symmetric.

3

Remark 1: Even when X is a pure white noise process,
the windowed Fourier frame will enforce a certain degree of
smoothness along each path γ since consecutive points in γ
are inner products of frame atoms with partially overlapping
segments of X. So there will be some correlation in
Xγ
even when X is an uncorrelated time series, therefore it is
possible in general that I(
Xγ) >> 1 even when X is a
white noise process.

F

F

Remark 2: Similarly, the length p of γ cannot be chosen
very large in practice, while E( σ1
) converges to 1 for any
σd
uncorrelated processes only asymptotically for very long time
series and again for small length p we may have E( σ1
) >> 1.
σd

Even with the limitations explained in the previous two
remarks, it is still meaningful to set I(Xγ) = I svd(Xγ) = σ1
,
σd
and therefore we deﬁne an embedding threshold estimator to
be a semilocal estimator ˜F (as in (2)) with the choice of index
I = I svd, what we call an embedding index. The question
1, given a choice
is now to ﬁnd a speciﬁc choice of T
of (
, Cp, d, τ ), that allows to discriminate a given data set
(speech signals in this paper) from white noise processes.

≥

D

We need therefore to study the value distribution of
I svd for our speciﬁc choice of
, and assuming X is
either an uncorrelated random process or a signal belonging
to our class of speech signals.

p and

D

C

issue
In the next section we explore numerically this
for the windowed Fourier frames and the collection of paths
Cp in (5).

III. EMBEDDING INDEX OF SPEECH SIGNALS AND
RANDOM PROCESSES

For a given times series X and choice of parameters
(p, τ, d) we can compute the collection of embedding indexes
I svd(
, Deﬁne now the index
X) =
F
cumulative function as

Xγ), γ

I svd(

Cp

F

∈

}

{

QX (t) =

#
{

γ such that I svd(

Xγ) > t

F

}

,

(8)

γ

#
{

}

i.e. for a given t, QX (t) is the fraction of paths that have
index above t.

A simple property of QX will be crucial in the following
discussion:

Lemma 2: If X is a white noise process and X ′ = aX is
another random process that component by component is a
rescaling of X by a positive number a, then the expected
function QX and QX ′ are equal.
Proof: Each set of embedding points generated by one speciﬁc
path γ is, coordinate by coordinate, a linear combination

4

F

¯
X ′

γ = a ¯
F

of some set of points in the original time series. Therefore
if X ′ = aX,
Xγ, but the quotient of singular
values of a set of points is not affected by rescaling of all
coordinates,
X) and
I svd(
X ′) are equal, but QX ′ and QX are deﬁned in terms
of I svd so they are equal as well.

therefore the distributions of I svd(

F

F

Remark 3: We see the use of embedding index as
a possible generalization of methods like the coherent
structures extraction of [M] section 10.5 (more details can
be found in [DMA]), where it
is explored the notion of
correlation of a signal X of length N with a basis B, deﬁned
as

(X) =

C

sup0≤m<N
X
|

XB[m]
|
|
|

.

It turns out that in the limit N
Gaussian white process converges to

→ ∞

the correlation of any

N =

C

√2logeN
√N

|X| >

independently of the speciﬁc variance and therefore estimation
of a signal X is performed by retaining a coefﬁcient XB[m] if
|XB [m]|
N . In this paper the embedding index determines
the coherence of a coefﬁcient with respect to a neighbourhood
of the signal and it is independent of the variance of the noise
process as well.

C

Remark 4: As we said in section II,
the choice of p
in Cp is very important in practice. The speech signals that
we consider are sampled at a sampling frequency of about
8100 pt/s, we choose supprt of the window q = 64 and length
of the paths p = 28, since these values seem to assure that
each path will be signiﬁcantly shorter than most stationary
vocal emissions, a point to take into consideration when we
gauge the relevance of our results.
Given this lenght p for γ, we have some signiﬁcant restrictions
on the maximum embedding dimension d and time delay
τ that we can choose if we want to have for each path a
sufﬁciently large number of points in the embedding image to
be statistically signiﬁcant, which we can obtain if p >> dτ .
Because of these restrictions we choose d = 4 and τ = 4
that give dτ = 24 << p = 28, we generate in this way
240 points for each path. We heuristically tried to adjust the
embedding parameters d and τ and the lenght p of the paths
so that the qualitative behaviour of speech signals and white
noise processes was as distinct as possible, see the discussion
in section IV for a possible way to make the choice of
parameters automatic.

−

R)4N2/pr + 16(1

1) Gaussian probability density function.
2) Uniform probability density function.
3) Tukey probability density function, that is, a sum of two
normal distributions with uneven weight (used in [ELPT] as
well), each point of the time series is a realization of the
r),
random variable W = RN1 + (1
where N1 and N2 are Gaussian random variables, and R
is a Bernoulli random variable with P (R = 1) = 0.9 and
r = P (R = 1).
4)discrete uniform pdf with values in
positive Q.
All probability density functions are set to have mean zero.
and variance 1, since by Lemma 2 we know Q∗ will not be
affected by changes of the variance. One of the pdf has heavy
tail (Tukey pdf) and one of them is discrete (discrete uniform
pdf). The kurtosis is respectively from pdf in 1) to pdf in 4):
3, about 1.8, about 13,and about 1.2

for some

Q, Q

{−

−

}

In Figure 1a we plot QX (t) for the white noise processes
generated with pdfs in 1)-4), averaged over 10 repetitions for
each random distribution.

Remark 5: To speed up the computation, we sampled
the indexes ( ¯m, ¯l) of the paths in (5), more particularly we
selected a sampling length of S ¯m = 1 for the frequency index
¯m and a sampling length of S¯l = p for the time index.

Note that the qualitative behaviour of QX is very similar for
all chosen distributions, in particular they all exhibit a very
fast decay for larger values of t. The maximum L2 distance
between any two QX in the interval [0, 40] is
0.54 (or
some 6% of the average L2 norm of the QX ) , we found that
even for distribution with kurtosis up to 50 the maximum
distance was less that 0.8 (about 8.5% of the average L2
norm of QX), irrispective of the speciﬁc pdf, moreover most
of the error is concentrated in regions of high intensity of the
derivative and it does not affect much the behaviour of the
right tail of the curves QX .

≈

for our choice of

and Cp,
Therefore it seems that,
D
a
exibit
reasonably heavy tail distributions will not
signiﬁcantly different behaviour
in QX with respect
to the Gaussian distribution, supporting our claim that QX
is robust with respect to the choice of white noise distribution.

For each probability density function,
the shape of QX
is affected by the correlation introduced by the length of q
(the window support of the windowed Fourier Frame): if
τ < q some coordinates in each embedding point will be
correlated and this will cause the decay of QX to be slower
when τ is smaller.

We now expand some uncorrelated zero mean random
processes of length N = 211 on the windowed Fourier frame
with the set values q = 64, p = 28, d = 4 and τ = 8. And
we compute the embedding index QX.
The speciﬁc random processes we use here are time series
with each point a realization of a random variables with:

When QX is computed (with the same choice of parameters)
for a collection of 10 randomly selected segments of speech
signals of length 211,
the rate of decay of the functions
QX is signiﬁcantly different, and the tail of the functions
is still considerably thick by the time the rate of decay of
QX for most random processes is almost zero (see Figure 1b).

5

5

10

15

20

25

30

35

40

5

10

15

20

25

30

35

40

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

0

5

10

15

20

25

30

35

40

5

10

15

20

25

30

35

40

From top to bottom, this ﬁgure shows Q∗, as deﬁned in equation
Fig. 1.
(7) for: a) uncorrelated random processes 1)to 4); b) ten randomly selected
segment of speech signal from the TIMIT database.

From top to bottom, this ﬁgure shows E∗, as deﬁned in equation
Fig. 2.
(8) for: a) the uncorrelated random processes in Figure 1a; b) the segments
of speech signals in Figure 1b.

Since we want
to have a signiﬁcantly larger fraction of
paths retained for speech signals rather than noise, we can
select the threshold T in the following way:

,
D
Sj
{

(A) Determination of Threshold Given a choice of
parameters (
training speech
p, p, τ, d), a collection of
C
time series
, and a selection of white noise processes
}
, choose T0 to be the smallest t so that the mean of
Wi
{
QSj (T0) is one order of magnitute (10 times) larger than the
mean of QWi(T0).

}

This heuristic rule gives, for the parameters in this section,
28.2. (A) gives us as experimental way to determine
T0 ≈
a threshold T = T0 for the index I svd that removes most
of
the time frequency structure of some predetermined
fraction
noise distributions, while it preserves a larger
of the time frequency structure of speech signals. Since
moreover ‘reasonable’ distributions exibited a QX similar to
the one of Gaussian distributions, we can in practice train
the threshold only on Gaussian noise and be assured that
it will be a meaningful value for a larger class of distributions.

Note that even very low energy paths could have in principle
high embedding index, still, the energy concentration in paths
that have very high index tends to be large for speech signals,
to see that, for a given signal X, let

EX (t) = P{|F

Xγ

|2 such that I svd(
F
P |F

Xγ

|2

Xγ) > t

}

,

(9)

be the fraction of the total energy contained in paths with
index above x. We can see in Figure 2 that the amount of
energy contained in paths with high index value is signiﬁcantly
larger for speech signals than for noise distributions.

More particularly,
the total energy of
the fraction of
the paths carried by paths with I svd > T0 is on average 0.005
for the noise distributions and 0.15 for the speech signals, or
an increase by a factor of 30.

It seems therefore that I svd, with our speciﬁc choice
of parameters, is quite effective in separating a subset of
paths that are likely to be generated by speech signals, note

F

F

∞

moreover that similar results can be obtained with local
changes of p, τ and d, which suggests an intrinsic robustness
of the separation with respect of the parameters.
This separation ability could be due, in principle, only to the
very nice properties of speech signals. Note that if, for some
Xγ, I svd =
, then the state realization of the time series
F
Xγ is embedded in a subspace of Rd and therefore each
F
point of
Xγ must be described as a linear function of the
delay coordinates. This condition is very restrictive on the
dynamics of
Xγ, but vocal emissions are locally periodic
signals, and so they do fall, at least locally, into the class of
linearly predictable discrete models, i.e., processes for which
Xk = r(Xk−1, ..., Xk−d) for some linear function r and for
some integer d.
The complexity of
increases with
increasing values of the embedding dimension d. But this
is not fully satisfactory as we would like to be able to
use the embedding index I svd to denoise more complex
dynamics that cannot be described by simple linear predictive
models. Moreover for small τ we are measuring in many
cases smoothness of the path and local correlation with the
embedding index, yet,
if we try to choose τ as large as
possible with still a clear separation of the training sets,
we can see differences that are not accounted for by local
correlation, indeed the embedding image is squeezed along
the diagonal for paths with high local smoothness, but in
principle for complex dynamics the principal direction could
be oriented in any direction and therefore the embedding index
is much more than simply a measure of local smoothness.

these linear models

There is a large literature on possible ways to distinguish
complex dynamical systems from random behaviour (see for
example the articles collected in [Me]), as we underlined
in the previous section, much of this work stresses the
identiﬁcation of the proper embedding parameters τ and d;
the contribution of this paper to this ongoing discussion is the
use of embedding techniques in the context of computational
harmonic analysis. This context frees us from the need to
use embedding techniques to ﬁnd an effective modelization
of the signals, such ‘blind’ use of the embedding theorem is,
we believe, fertile from a practical point of view, as well as
a theoretical one.

6

Note in any case that
if the dimension of the invariant
set A is dA = 0, then for any white noise process W , X + W
has spherically symmetric embedding image and σ1
1 for
any embedding dimension d as in the case of pure white
noise. This means that an estimator based on I svd is not able
to estimate noisy constant time series on a given path γ.
This restriction can be eased by allowing information on the
distance of the center of the embedding image to be included
in the deﬁnition of the embedding threshold estimator. In this
paper for simplicity we assumed dA > 0 for all paths in Cp.
That seems to be sufﬁcient in analyzing speech signals.

σd ≈

IV. ATTENUATED EMBEDDING ESTIMATORS

In this section we develop an algorithm based on these
ideas. The notion of semilocal estimator is slightly expanded
to improve the actual performance of the estimator itself. To
this extent, deﬁne tubular neighborhoods for each atom in the
windowed Fourier frame, i.e.:

l′
|

1,

m′
|

(gm,l) =

gm′,l′ s.t.

l

m

1

,

(10)

{

O

| ≤

−
Such neighborhoods are used in the algorithm as a way to
make a decision on the value of the coefﬁcients in a two
dimensional neighborhood of
Xγ based on the the analysis
of the one dimensional time series

Xγ itself.

| ≤

−

F

}

F

(C1) Set ˜F = 0.

(C2) Given X, choose q > 0 and expand X in a windowed
Fourier frame with window size q.

(C3) Choose sampling intervals S¯l
for time coordinate
and S ¯m for the frequency coordinate. Choose the path length
p. Build a collection of paths

p as in (5).

C

F

p. Use (A) to ﬁnd the threshold level T .

(C4) Choose embedding dimension d and delay τ along
the path. Compute the index I svd(
for each
Xγ ¯m,¯l ∈ C
(C5) Choose attenuation coefﬁcient α. Set
α
X[m, l]
F
gm′,l′ , gm′,l′
I svd(

Y [m, l] =
T for some γ containing
Y [m, l] = 0 if
(gm,l).

Xγ) < T for all γ containing gm′,l′ , gm′,l′

F
≥
(gm,l), otherwise set

if I svd(

Xγ ¯m,¯l)

∈ O

Xγ)

F

F

F

F

∈ O

Y . Set ˜F = ˜F + Y

(C6) Let Y be the inversion of
Y .
and X = X

F

−

(C7) Choose a paramenter ǫ > 0,
step (C2).

if

Y
|

|

> ǫ go to

the details of

Note that
the implementation (C1)-(C7)
are in line with the general strategy of matching pursuit.
The window length q in step (C2) could change from one
iteration to the next to ‘extract’ possible structure belonging
to the underlining signal at several different scales. In the
experiments performed in the following section we alternate
between two window sizes q1 and q2.

The attenuation introduced in (C5) has some additional ad hoc
parameters in the deﬁnition of the neighborhoods in (10) and
in the choice of the attenuation parameter α. By the double
process of increasing the number of nonzero coefﬁcients
chosen at each step and decreasing their contribution we
are allowing more information to be taken at each iteration
of the projection pursuit algorithm, but in a slow learning
framework that in principle (and in practice as we found
out) should increase the sharpness of the distinct features
of the estimate, on the general issue of attenuated learning
processes see the discussion in [HTF] chapter 10. Note that
the attenuation coefﬁcient
leads to improved results only
when it is part of a recursive algorithm, otherwise it gives
only a rescaled version of the estimate.

D

D

that

the algorithm we described is

One drawback of
the
need to choose several parameters: we choose a dictionary of
analysis
, a collection of discrete paths Cp, the embedding
parameters τ (time delay) and d (embedding dimension), and
the learning parameters T (threshold level), α (attenuation
coefﬁcient) and ǫ. Again we stress that all such choices
are context dependent, and are the price to pay to have
is relatively intensity independent and
an estimator
applicable to wide classes of noise distributions.
The choice of
is dependent on the type of signals we
analyze and we do not see a serious need to make such
choice automatic.
Since we analyze speech signals, we choose the dictionary
to be the set of atoms of the windowed Fourier frames; the
algorithm is not very sensitive to the choice of the length q
of the window in the Fourier frame, while the use of several
windows is found to be always beneﬁcial.
The choice of Cp is also dependent on the type of signals
analyzed, speech signals have speciﬁc frequencies that change
in time, so a set of paths parallel to the time axis was natural
in this case. Let us explore now the relation of parameters
associated with Cp, embedding parameters τ and d and
threshold T . Recall that for the collection Cp we have as
parameters the time and frequency sampling rates ¯l and ¯m
and the length p of the paths. The frequency sampling rates ¯l
and ¯m are necessary only to speed up the algorithm, ideally
we would like a dense sampling. Same considerations apply
to the ‘thickening’ of the paths in (10), we basically try
to speed up the algorithm by collecting more data at each
iteration.
So the only essential parameters are the path length p, the
embedding parameters and the threshold T
Essentially we want
the
number of paths that have index I svd > T is sizeable for
a training set of speech signals and marginal for the white
noise time series of interest.
Our experience is that such choice is possible and robust, we
gave a simple rule to ﬁnd the threshold T in step (A) in the
previous section given a choice of (p, τ, d).
A learning algorithm could be built to ﬁnd T , the paths’
length p, and the embedding parameters, namely let ¯QS(x)
be the mean of the functions QSi(x) for a training set of
speech signals Si and ¯QW (x) be the mean of the functions

these parameters so that

to set

QWi (x) for a set of white noise time sieries Wi
We can ﬁrst ﬁnd d, τ and p such that the distance of the
functions ¯QW (x) and ¯QS(x) is maximum in the L2 norm.
After ﬁnding these parameters, we can ﬁnd a value of T
such that T is the smallest positive number with ¯QS(T ) one
order of magnitude larger than ¯QW (T ), as we did in (A) in
the previous section, to make our algorithm automatically
applicable to data sets of interest different from speech signals
it will be necessary to formalize this optimization procedure.

in
Finally the choice of α and ǫ is completely practical
nature, ideally we want α and ǫ as close to zero as possible,
but,
to avoid making the algorithm unreasonably slow,
we must set values that are found to give good quality
reconstructions on some training set of speech signals while
they require a number of iterations of the algorithm that is
compatible with the computing and time requirements of
the speciﬁc problem. For longer time series, as the ones
in the next section, we segment the data in several shorter
pieces, and we iterate the algorithm a ﬁxed number of times
k rather than using ǫ in (C7) to decide the number of iterations.

Note:The algorithm described in this paper is being patented,
with provisional patent application number 60/562,534 ﬁled
on April 16, 2004.

V. DENOISING

C

In this section we explore the quality of the attenuated
embedding threshold as implemented in the windowed Fourier
frame and with our class of paths
p. We apply the algorithm
to 10 speech signals from the TIMIT database contaminated
by different types of white noise with several intensity levels.
We show that the attenuated embedding threshold estimator
performs well for all white noise contaminations we consider.
The delay along the paths is chosen as τ = 4, the length of
the paths is p = 28 and the window length of the windowed
Fourier transform alternates between q = 100 and q = 25
(to detect both features with good time localization and those
with good frequency localization), the embedding dimension
d = 4. For these parameters and for the set of speech signals
that we used as training, we have T
26.8 when q = 100
and T
27.4 when q = 25 using the procedure (A) of section
III.
The sampling interval of the paths in the frequency direction
is S ¯m = 3 and along the time direction is S¯l = p/2 We
select α = 0.1, as small values of α seem to work best (see
discussion in the previous section). The algorithm is applied to
short consecutive speech segments to reduce the computational
cost of computing the windowed Fourier transform on very
long time series, therefore, to keep the running time uniformly
constant for all such segments, we decided to iterate the
algorithm (C1)-(C6) a ﬁxed number of times (say 6 times)
instead of choosing a parameter ǫ in (C7).
As we already said, the window size q in (C2) alternates
between q = 100 and q = 25. It is moreover important
to note that the attenuated embedding threshold is able to
extract only a small fraction of the total energy of the signal

≈

≈

7

0

5

10

15

0

5

10

15

6

4

2

0

6

4

2

0

−2

−4

−5

−2

−4

−5

6

4

2

0

6

4

2

0

−2

−4

−5

−2

−4

−5

0

5

10

15

0

5

10

15

Fig. 3.
Scaled SNR gain in decibel of the attenuated embedding estimates
plotted against the scaled SNR of the corresponding measurements. From top
left in clockwise order we consider the case of: a)Gaussian white noise; b)
uniform noise; c)Tukey white noise; d)discrete bimodal distribution .

f , exactly because of the attenuation process, therefore the
Signal-to-Noise Ratio (SN R) computations are done on scaled
measurements X, estimates ˜F , and signals F set to be all
of norm 1. We call such estimations scaled SN R, and we
explicitely write, for a given signal F and estimation Z,

SN Rs(Z) = 10log10

F/
E(
|
compute SN Rs(X)

1

F
|

| −

then

by
and
We
approximating the expected values E(
)
F/
|
|
˜F
˜F /
and E(
) with an average over several
F/
|
|
|
realizations for each white noise contamination.

X
|

| −

F
|

Z/

Z
|

)
|
SN Rs( ˜F )
X/
F
|

| −

In Figure 3 we show the gains of
the scales SNR of
the reconstructions (with the attenuated embedding threshold
estimator) plotted against
the corresponding scaled SNR
of the measurements. Each curve correspond to one of 10
speech signals of approximately one second used to test
the algorithm. From top left in clockwise direction we have
measuremets contaminated by random processes with pdfs
1) to 4) as deﬁned in section III and with several choices of
variance. Note that the overall shape of the scaled SNR gain is
similar for all distributions (notwithstanding that the discrete
plots do not have exactly the same domain). The maximum
gain seems to happen for measurements with scaled SNR
around 1 decibel. Note that the right tail of the SNR gains
takes often negative values; this is due to the attenuation
effect of the estimator that is pronunced for the high intensity
speech features, but it is not necessarily indicative of worse
perceptual quality with respect to the measurements, some of
the ﬁgures in the following will clarify this point.

In the ﬁrst case of Gaussian white noise, we compared
our algorithm to the block thresholding algorithm described
in [CS], we used the matlab code implemented by [ABS],
made available at www.jstatsof t.org/v06/i06/codes/ as a
part of their thourogh comparison of denoising methods. As
the block thresholding estimator is implemented in a symmlet
wavelet basis that is not well adapted to the structure of

8

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

0

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

0

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

0

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

Fig. 4. Signal ‘SPEECH10’ scaled to have norm 1.

Fig. 7. Signal ‘SPEECH5’ scaled to have norm 1.

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

Fig. 5. Noisy scaled measurement of SPEECH10 with Gaussian white noise
and scaled SNR of about 1db.

Fig. 8. Noisy measurement of SPEECH5 with Tukey white noise and scaled
SNR of about 1db.

speech signals, a more compelling comparison would require
the development of an embedding threshold estimator in a
wavelet basis, we plan to do so in a future work. In Figure
10 we show the scaled SNR gain for all tested speech signals
using the block threshold estimator (right plot) and attenuated
embedding estimator (left plot). In Figure 4 we show one
original speech signal, Figure 5 shows the measurement in
the presence of Gaussian noise corresponding to the ‘peak’
of the SN Rs gain curve (measurement SN Rs
1), Figure
6 shows the corresponding reconstruction with attenuated
embedding threshold estimator. Similarly Figure 7 shows
another speech signal, while Figure 8 shows the measurement
with Tukey noise corresponding to the ‘peak’ of the Tukey
noise SN Rs gain curve (measurement SN Rs
1), Figure
9 shows the reconstruction. In both cases the perceptual
quality is better than the noisy measurements, which is not
necessarily the case for estimators in general.
Note moreover that even though T was found using only
Gaussian white noise as the training distribution, none of the
parameters of the algorithm were changed as we went from
Gaussian white noise contaminations to more general white
noise processes, and yet the SN Rs gain was similar, it must

≈

≈

be noted though that the estimates for bimodal and uniform
noise were not intelligible at the peak of the SN Rs gain
curve (just as the measurements were not).

the embedding estimator

is
Since the performance of
not well represented by the scaled SNR for low intensity
noise (measurements appear to be better than the estimates),
in Figures 10 to 21 we show two more instances of
speech signals contaminated by lower variance Tukey noise,
Gaussian noise and discrete bimodal noise (uniform noise
leads to reconstructions very similar to the discrete bimodal
distribution), for one case of
low Gaussian white noise
we show a block thresholding estimate, note how the low
intensity details are lost, this inability to preserve low intensity
details worsens when higher variance noise is added, but then
again, it must be tempered by the fact that a standard wavelet
basis is not well adapted to the structure of speech signals.

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

Fig. 6. Attenuated embedding estimate of SPEECH10 from the measurement
in Figure 6, scaled to have norm 1.

Fig. 9. Attenuated embedding estimate of SPEECH5 from the measurement
in Figure 9, scaled to have norm 1.

9

6

5

4

3

2

1

0

−1

−2

−3

−4

−5

0

5

10

15

0

5

10

15

2000

4000

6000

8000

10000

Fig. 11. Signal ‘SPEECH2’ scaled to have norm 1.

Fig. 10. SN Rs gain for the estimates of 10 speech signals and Gaussian
additive noise using: the block thresholding estimator of [CS](right),
the
embedding threshold estimator(left).

2000

4000

6000

8000

10000

Fig. 15. Attenuated embedding estimate of SPEECH2 from the measurement
in Figure 14, scaled to have norm 1, SN Rs is ≈ 8.1db.

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

Fig. 12. Noisy measurement of SPEECH2 with Tukey white noise and scaled
SNR of about 4.4db.

Fig. 16. Signal ‘SPEECH7’ scaled to have norm 1.

2000

4000

6000

8000

10000

Fig. 13. Attenuated embedding estimate of SPEECH2 from the measurement
in Figure 12, scaled to have norm 1, SN Rs is ≈ 8.1db.

2000

4000

6000

8000

10000

Fig. 17. Noisy measurement of SPEECH7 with Tukey white noise and scaled
SNR of about 7.3db.

2000

4000

6000

8000

10000

Fig. 14. Noisy measurement of SPEECH2 with bimodal white noise and
scaled SNR of about 4.5db.

6

5

4

3

2

1

0

−1

−2

−3

−4

−5

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

0

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

0

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

0.05

0.04

0.03

0.02

0.01

0

−0.01

−0.02

−0.03

−0.04

−0.05

0

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

0.12

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

0.12

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

10

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

−0.1

0

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

−0.1

0

0.12

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06

−0.08

0

2000

4000

6000

8000

10000

2000

4000

6000

8000

10000

Fig. 18. Attenuated embedding estimate of SPEECH7 from the measurement
in Figure 17, scaled to have norm 1, SN Rs is ≈ 6.

Fig. 21. Block thresholding estimate of SPEECH7 from the measurement in
Figure 19, scaled to have norm 1,SN Rs is ≈ 7.6, note low intensity details
are removed by the estimator.

2000

4000

6000

8000

10000

Fig. 19. Noisy measurement of SPEECH7 with Gaussian white noise and
scaled SNR of about 11.1db.

Data ﬁles for the signal, measurement and reconstructions
used to compute the quantities in all the ﬁgures are available
upon request for direct evaluation of the perceptual quality.

VI. FURTHER DEVELOPMENTS

{

D

=

g1, ..., gP

Given that the embedding threshold ideas were implemented
with the speciﬁc goal of denoising speech signals, it may be
worth emphasizing that in principle the construction of classes
of paths can be applied to other dictionaries well adapted to
other classes of signals, more paricularly, let
}
be a generic frame dictionary of P > N elements so that
P
m=1 XD[m]˜gm, XD[m] =< X, gm >, where ˜gm
X = P
are dual frame vectors (see [M] ch.5). Given such a general
, Q > P , be
representation for X, let
p =
{
C
of length p, that is,
a collection of ordered subsets of
D
and the cardinality
, so that S γi =
gi1, ..., gip}
γi =
D
is constant for every
of the set
γi
}
{
1 (this ensures that the discrete covering of
j = 0, ..., P
the frame atoms is locally uniform). Note that
p needs not
be the entire set of ordered subsets of
. We call each γi a
for reasons that will be clear in the following.
‘path’ in
D
be an ordered
Let Xγi =
XD[m] =< X, gm >, gm

γi such that gj

γ1, ..., γQ

γi

−

D

∈

C

}

{

{

∈

}

2000

4000

6000

8000

10000

Fig. 20. Attenuated embedding estimate of SPEECH7 from the measurement
in Figure 19, scaled to have norm 1,SN Rs is ≈ 7.7.

collection of coefﬁcients of X in the dictionary

.

D

Then a a semi-local estimator in

can be deﬁned as:

D

˜F =

P −1

X
m=0

dI,T (XD[m])˜gm

(11)

where dI,T (XD[m]) = XD[m] if I(Xγ)
T for some γ
containing m, and dI,T (XD[m]) = 0 if I(Xγ) < T for all γ
containing m.

≥

signiﬁcant

sets of paths

The construction of
p will
depend from the application, we are currently exploring
even the possibility of using random walks along the atoms
of the dictionary
. In any case, after Cp is selected, our
specifc choice of index I svd can be used and the attenuated
embedding estimator can certainly be applied and tested,
soft threshold embedding estimators are an interesting open
possibility as well.

D

C

REFERENCES

[ABS] A. Antoniadis, J. Bigot, T. Sapatinas, Wavelet
Estimators
in Nonparametric Regression: A
Comparative Simulation Study, 2001, available
http://www.jstatsoft.org/v06/i06/

[C]

[ASY] K. T. Alligood, T. D. Sauer, J. A. Yorke, Chaos. An
introduction to Dynamical systems, Springer, 1996.
T. Cai, Adaptive wavelet estimation: a block thresh-
olding and oracle inequality approach. The Annals
of Statistics 27 (1999), 898-924.

[CL] T. Cai, M. Low, Nonparametric function estimation
over shrinking neighborhoods: Superefﬁciency and
adaptation. The Annals of Statistics 33 (2005)., in
press.

[CS] T. Cai, B. W. Silverman, Incorporating information
on neighboring coefﬁcients into wavelet estimation,
Sankhya 63 (2001), 127-148.

[DMA]G. Davis, S. Mallat and M. Avelaneda, Adaptive
Greedy Approximations, Jour. of Constructive Ap-
proximation, vol. 13, No. 1, pp. 57-98, 1997
[DJ] D. Donoho, I. Johnstone, Minimax estimation via
wavelet shrinkage. Annals of Statistics26 : 879-
921,1998.

11

[ELPT]S. Efromovich, J. Lakey, M.C. Pereyra, N. Tymes,
Data-driven and optimal denoising of a signal and
recovery of its derivative using multiwavelets, IEEE
transaction on Signal Processing, 52 (2004) ,628-
635.

[KS] H. Kantz, TSchreiber ,Nonlinear Time Series Analy-

sis, Cambridge University Press, 2003.

[HTF] T. Hastie, R. Tibshirani, J. Friedman, The Elements

of Statistical Learning, Springer, 2001.

[LE] E. N. Lorenz, K. A. Emanuel, Optimal Sites for Sup-
plementary Weather Observations: Simulation with a
Small Model. Journal of the Atmospheric Sciences
55, 3 (1998), 399414.
S. Mallat, A Wavelet Tour of Signal Processing,
Academic Press, 1998.

[M]

[Me] A. Mees (Ed.), Nonlinear Dynamics and Statistics,

[S]

Birkhauser, Boston, 2001.
T. Strohmer, Numerical Algorithms for Discrete
in Gabor Analysis and Algo-
Gabor Expansions,
rithms. Theory and Applications, H. G. Feichtinger,
T. Strohmer editors. Birkhauser, 1998.

[SYC] T. Sauer, J. A. Yorke, M. Casdagli, Embedology,

Journal of Statistical Physics,65 (1991), 579-616.


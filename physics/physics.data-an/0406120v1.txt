4
0
0
2
 
n
u
J
 
4
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
2
1
6
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Asymmetric Statistical Errors

MAN/HEP/04/02
24/6/2004

Roger Barlow

Department of Physics
Manchester University
England

Abstract

Asymmetric statistical errors arise for experimental results obtained by Maximum
Likelihood estimation, in cases where the number of results is ﬁnite and the log likelihood
function is not a symmetric parabola. This note discusses how separate asymmetric errors
on a single result should be combined, and how several results with asymmetric errors
should be combined to give an overall measurement. In the process it considers several
methods for parametrising curves that are approximately parabolic.

1.

Introduction
When an experimental result is presented as x+σ+

±

−

−σ− this signiﬁes, just as with the usual
form x
σ, that x is the value given by a ‘best’ estimate (i.e. one with good properties
of consistency, eﬃciency, and lack of bias) and that the 68% central conﬁdence region is
[x

σ−, x + σ+].
Such asymmetric errors arise through two common causes. The ﬁrst is when a nuisance
parameter a has a conventional symmetric (even Gaussian) probability distribution, but
produces a non-linear eﬀect on the desired result x. These errors are generally systematic
rather than statistical, and their probability distribution is generally best considered from
a Bayesian viewpoint. Their treatment has been considered in a previous note [1].

The second cause of asymmetry is the extraction of a result x through the maximisa-
tion of a likelihood function L(x) which is not a symmetric parabola. This occurs because
the function is in general only parabolic in the limit when the number of results N , the
number of terms contributing to the sum which makes up the log likelihood, is large, and
for many results this is not the case. For such a function the errors are conventionally read
oﬀ the points at which the log likelihood falls by 1
2 from its peak, though this is not exact
[2] and it may be better to obtain the errors from a toy Monte Carlo computation.

Although such asymmmetric errors are frequently used in the reporting of particle

physics results, constructive analyses of their use are scarce in the literature [3].

2. Two Combination Problems

The two most signiﬁcant questions on the manipulation of asymmetric errors are the

Combination of Results and the Combination of Errors.

2.1 Combination of Results

The ﬁrst occurs when one has two results x1

and x2

of the same quantity.

This arises when two diﬀerent experiments measure the same quantity. Assuming that
they are compatible (according to some criterion), one wants the ppropriate value (and
errors) that combines the two. This is the equivalent of the well-known expression for
symmetric errors

+σ+
1
−
−σ
1

+σ+
2
−
−σ
2

x1/σ2
1/σ2

1 + x2/σ2
2
1 + 1/σ2
2 ± s

1

1/σ2

1 + 1/σ2
2

(1)

1
2 points

−

If the log likelihood functions L1(x1) and L2(x2) are known, then the combined log likeli-
hood is just the sum of the two. The maximum can then be found and the errors read oﬀ
the ∆lnL =

The question naturally extends to more than two results, and it is clearly a desir-
ably property that the operation be associative: if results are combined pairwise till only
one remains, then the pairing strategy should not eﬀect the result. For the addition of
likelihoods this obviously holds.

2.2 Combination of Errors

The second question arises when a particular result (taken, without loss of generality,
as zero) is subject to several separate (asymmetric) uncertainties, and one needs to quote

1

the overall uncertainty. An obvious example would be the uncertainty due to background
subtraction where the background has several diﬀerent components, each with asymmetric
uncertainties. This is the equivalent of the well-known expression for symmetric errors

If x = x1 + x2

then

σ2 = σ2

1 + σ2
2

(2)

Again, it is desirable that the operation be associative.

If the likelihood functions are known then the joint function L1(x1)L2(x2) is deﬁned
on the (x1, x2) plane with its peak at (0,0). The uncertainty on the sum x1 + x2 is found
by the proﬁling technique: we ﬁnd ˆL(x1 + x2), the peak value of the likelihood anywhere
1
2 errors can be read oﬀ from this [4].
on the line x1 + x2 = constant, and the ∆logL =
To explain why this works (and when it doesn’t), consider ﬁrst a case where the answer
is easily found: suppose x1 and x2 are both Gaussian, with the same mean σ. The log
likelihood can then be rewritten using u = x1 + x2 and v = x1 −
u2
4σ2 −

(x1 + x2)2
4σ2

(x1 −
4σ2

x2
2
2σ2 =

x2
1
2σ2 −

v2
4σ2

x2)2

x2:

(3)

=

−

−

−

−

−

The likelihood is the product of two Gaussians (of width √2σ), one in the combination of
interest u, the other in the ignorable combination v.

Now for some ﬁxed value of v, the likelihood for u is a Gaussian of mean zero, and the
68% central conﬁdence region for u is given by its standard deviation and is of half-width
√2σ. If v is ﬁxed at some other value, the likelihood for u, and the deductions that can
be drawn from it, are the same, Thus one can say ‘There is a 68% probability that u lies
√2σ, √2σ], whatever value of v is chosen’, and this can legitimately be
in the region [
shortened by striking out the ﬁnal condition. And the problem is solved.

−

To apply this technique in some less transparent case we need to factorise the likelihood
into the form L1(x1)L2(x2) = Lu(u)Lv(v) where we have freedom to choose the functions
Lu, Lv, and the form v(x1, x2). In some instances this is clearly possible: a double Gaussian
σ1x2. There are also instances, such as
with σ1 6
a volcano-crater shaped function, which are manifestly impossible to factorise. These
can readily be proposed as counterexamples, but appear somewhat contrived and it is
reasonable to hope that they might not occur in practical experience, except for very small
N .

= σ2 can be factorised using v = σ2x1 −

On the grounds that if this factorisation is impossible we can get nowhere, let us
assume it to be true and see where that leads us. Finding the explicit forms of v and Lv
is complicated and one would like to avoid it. This can be done by noting that:

1: For ﬁxed v the shape of the total likelihood as a function of u is the same
2: For ﬁxed u the shape of the total likelihood as a function of v is the same
(1) tells us that we can study the properties of Lu(u) by ﬁxing on any value of v. (2)
tells us that we can ﬁx the value of v by ﬁnding the maximum, the likelihood (as a function
of v, with u ﬁxed) will always peak at the same value of v. Thus for a given u = x1 + x2
x2 at which L is greatest, as that is always the same value of v.
one ﬁnds the value of x1 −

2

Figure 1: 2-D likelihood functions with lines of constant u and constant v

Figure 1 gives an illustration. The left hand plot shows the standard double Gaussian
(shown as a linear function rather than the logarithm, for presentational reasons) as a
function of x1 and x2. The lines of constant u = x1 + x2 run diagonally, from top left
x2 are orthogonal to them, running
to bottom right, and the lines of constant v = x1 −
from bottom left to top right. For any chosen value of v, the proﬁle of the likelihood as
a function of u is the same Gaussian shape, from which 68% limits can be read oﬀ, the
same in each case. There is a line of constant v = 0 running through the maximum, which
follows the maximum for any chosen u.

The right hand plot shows a more interesting function. The lines of constant u =
x1 + x2 are as before. The lines of constant v are such that the likelihood as a function of
u along them is the same, up to a constant factor. There is a line of constant v through
the maximum which follows the maximum for any chosen u.

This construction shows the limits of the technique. For some given u we plot L as a
x2 and compare it with the same curve for u = 0. Then we map the values
function of x1 −
x2 onto the corresponding values at u = 0 at which the log likelihood falls oﬀ from
of x1 −
the peak by the same amount, and these give the lines of constant v. If both curves are
single peaks then this is readily done and the mapping is continuous. If there are multiple
peaks then this continuous mapping is not possible. Thus for a simple peak the technique
will work, but not if there are secondary peaks or valleys.

This generalises readily to the case of several variables. The proﬁle likelihood is a
xi and ˆL is the maximum value of the likelihood in the

function ˆL(u) where u =
u = constant hyperplane.

P

3. Parametrisation of the likelihood function

Thus both questions can be answered if the likelihood functions are known. In general
they are not: a quoted result will only give the value and the positive and negative error.
We therefore need a way to reconstruct, as best we can, the log likelihood function from
them, using a parametrised curve.

This curve must go through the three points, having a maximum at the middle one.
This gives four equations, and hence the curve will have four parameters, obtainable from
the quoted values of the peak and the positive and negative errors. (The fourth parameter
is an additive constant which controls the value of the function at its maximum, which is
in fact irrelevant for our purposes.) It must also behave in a ‘reasonable’ fashion elsewhere.

3

Various possibilities have been tried, and tested against the log likelihood curves
where the true value is known, such as the Poisson and the log of a Gaussian variable. For
simplicity in what follows we take the quoted value as zero, and work with just σ+ and σ−
as input parameters.

3.1 Form 1: a cubic

Adding a cubic term is the obvious step

f (x) =

1

2 (αx2 + βx3)

−

(4)

(5)

(6)

(7)

with the coeﬃcients readily obtained as α =

σ2
−−σ2
+
−(σ−+σ+) . Exten-
+σ2
σ2
sion to several values has some consistency, as adding cubics will give another cubic, but
associativity is not guaranteed.

σ3
−+σ3
+
−(σ−+σ+) β =
+σ2
σ2

This gives curves which will behave sensibly in the [x

σ−, x + σ+] range, but outside

that the x3 term produces an unwanted turning point and the curve does not go to
for large positive and negative x.

−∞

−

3.2 Form 2: A constrained quartic

derivative a perfect square:

A quartic curve can be constrained to give only one maximum by making the second

′′

f

(x) =

1

2 (α + βx)2

−

f (x) =

1
2

−

(cid:18)

α2x2
2

+

αβx3
3

+

β2x4
12

(cid:19)

The parameters are given by

β =

1
σ+σ− v
u
u
t

6(σ− + σ+)2

±

12

3σ2

4σ+σ3

− + 4σ−σ3
+ −
q
− + 2σ−σ+ + 3σ2
+

2σ4
−

2σ4
+

−

Here the negative sign in the expression for β should be chosen to give a quartic term which
is small. In very asymmetric cases (σ− and σ+ diﬀering by more than about a factor of 2)
the inner square root is negative, indicating that there is no solution of the desired form.

Then one solves for α

α = (

)
−

βσ
3 ± p

36

2β2σ4

−
6σ

for both σ = σ+ and σ = σ−, where the (
the solution which is common to both.

−

) minus sign is used for the σ− case, and selects

Combination again gives closure, in that the sum of two quartics (with second deriva-

tive everywhere negative) is a quartic (with second derivative everywhere negative.)

This form gives rather better large x behaviour but is not always satisfactory in the

range between σ− and σ+.

4

3.3 Form 3: Logarithmic

One can also use a logarithimc approximation

where

f (x) =

log(1 + γx)
logβ

1
2

−

(cid:18)

2

(cid:19)

β = σ+/σ

−

γ =

σ−

σ+ −
σ+σ−

This is easy to write down and work with, and has some motivation, as it describes the
expansion/contraction of the abscissa variable at a constant rate. Its unpleasant features
are that it is undeﬁned for values of x beyond some point in the direction of the smaller
error, as 1 + γx goes negative, and that it does not give a parabola in the σ+ = σ− limit.

3.4 Form 4: Generalised Poisson

Starting from the Posson likelihood L(x) =

x + N ln x

ln N ! one can generalise to

−

−

f (x) =

α(x + β) + ν ln α(x + β) + const

(10)

−

using ν, a continuous variable, to give skew to the function, and then scaling and shifting
using α and β. Putting the maximum at the right place requires ν = αβ and thus, adjusting
the constant for convenience to make the peak value zero:

Writing γ = α/ν the equations at σ− and σ+ lead to

f (x) =

αx + ν ln (1 +

−

αx
ν

)

γσ−
1
1 + γσ+

−

= exp

−γ(σ−+σ+)

This has to be solved numerically. It has a solution between γ = 0 and γ = 1/σ− which
can be found by bifurcation. (Attempts to use more sophisticated algorithms failed.)

Given the value of γ, ν is then found from

This form did fairly well with many of the tests, but the extraction of the function

parameters from σ− and σ+ is inelegantly numerical.

ν =

1

2(γσ+ −

ln(1 + γσ+))

5

(8)

(9)

(10a)

(11)

(12)

3.5 Form 5: Variable Gaussian (1)

Another function is motivated by the Bartlett technique for maximum likelihood errors
[2,5]. This assumes (and indeed justiﬁes) that the likelihood function for a result ˆx from a
true value x is described with good accuracy by a Gaussian whose width depends on the
value of x.

lnL(ˆx; x) =

1
2

−

(cid:18)

ˆx
x
−
σ(x)

2

(cid:19)

This does not include the
it turns out [2] that omitting this term actually improves the accuracy of the ∆ ln L =
errors, bringing them into line with the Bartlett form.

ln σ(x) term from the denominator of the Gaussian. However
1
2

−

−

We make the further assumption that in the neighbourhood of interest this variation

in standard devation is linear

σ(x) = σ + σ

(x

ˆx)

′

−

lnL(ˆx; x) =

x
ˆx
−
σ + σ′(x

1
2

−

(cid:18)

ˆx)

(cid:19)

−

2

1
2 points gives

−

σ =

2σ+σ−
σ+ + σ−

′

σ

=

σ−
σ+ −
σ+ + σ−

the requirement that this go through the

Thus the parameters are easy to ﬁnd, and when σ− = σ+ the symmetric case is smoothly
incorporated.

3.6 Form 6: Variable Gaussian (2)

Still using the Bartlett-inspired form, we could alternatively take the variance as linear

and

and the parameters are again easy to ﬁnd, and sensible if σ− = σ+

V (x) = V + V

(x

ˆx)

′

−

lnL(ˆx; x) =

x)2

1
2

(ˆx
V + V ′(x

−

−

ˆx)

−

V = σ−σ+

′

V

= σ+ −

σ−

6

(13)

(14)

(15)

(16)

(17)

(18)

(19).

3.7 Example: Approximating a Poisson likelihood

Figure 2: Approximations to a Poisson likelihood

Figure 2 shows in black the likelihood function for Poisson measurement of 5 events.
1
In red are the approximations, constrained to peak at x = 5 and to go through the
2
points, indicated by the horizontal line. They all do well interpolationg in that region,
but outside it their behavour is very diﬀerent. The polynomial forms diverge signiﬁcantly
from the truth. The logarithmic form does fairly well, and the generalised Poisson does
perfectly (as it should for a Poisson likelihood). The variable width Gaussian models both
do quite well, but the one with linear variance does noticably better than the form linear
in the standard deviation

−

7

3.8 Example: Approximating a Logarithmic measurement.

Figure 3: Approximations to the likelihood of the log of a Gaussian measuremnet

is a Gaussian measurement with the value 8

Figure 3 shows the same approximations, ﬁtting a measurement of x = ln y, where y
3.
Again, all perform well in the central region, and the polynomial forms diverge badly
outside that region, though the quartic does adequately on the positive side and down to
2σ− from the peak. The logarithmic curve does fairly well, but the generalised
about
Poisson is not so good. The variable width Gaussians both do well, but in this case the
linear σ form does markedly better than the linear variance form.

−

±

8

We can conclude that the variable width Gaussians are the best approximation for our
purpose, having good descriptive power together with parameters that are readily obtained
from Equations 16 or 19, but that the choice between the linear σ or linear V form is one
that the user has to make on a case by case basis. Likelihood functions based on a Poisson
measurement will be better represented by the linear V form.

4. Procedure for combination of results

Working with a variable-width Gaussian parametrisation the likelihood function for a

set of measurements xi is

lnL =

1
2

−

xi
ˆx
−
σi(ˆx)

2

.

(cid:19)

X (cid:18)

(20)

(21)

For the linear σ form, the position of the maximum is given by the equation

ˆx

wi =

xiwi

with

wi =

i
X

i
X

(σi + σ′

σi
i(ˆx

−

xi))3 .

For the linear V form the corresponding equation is

ˆx

wi =

wi(xi

i
X

i
X

V ′
i
2Vi

−

xi)2)

(ˆx

−

with

wi =

(Vi + V ′

Vi
i (ˆx

−

xi))2 .

(22)

The algebra is simple, and has been implemented in a Java applet, obtainable under

http://www.slac.stanford.edu/

barlow/statistics.html.

∼

Equations 21 and 22 are nonlinear for ˆx, and the solution is found by iteration:
1
i xi is taken as a ﬁrst guess for ˆx, and this is used in the right hand side of the
N
equation to give an improved value. The implementation deems it to have converged if the
step size is less that 10−6 of the total range of interest, deﬁned as from
3σ− below the
lowest point to +3σ+ above the highest. In practice such convergence occurs after a few
iterations.

P

−

−

The ∆logL =

1
2 points of the function of Equation 20 are also found numerically.
The function is reasonably linear over the region where the iteration is performed, and
again convergence is rapid: an initial value is taken, inspired by Equation (1), as the
inverse root sum of the inverse squares of the positive or negative, as appropriate, errors.
1
A small step is taken, until the
2 line is crossed, and successive linear interpolation is
then done until the value is within 10−7 of 0.5. Again, only a few iterations are required
for a typical case.

−

The value of the function at the peak gives the χ2 for the result, and this can be used
to judge the compatibility of the diﬀerent results. (The number of degrees of freedom is
just one less that the number of values being combined.)

9

Figure 4: Three parametrised likelihood curves and their sum

Figure 4 shows the graphical result of combining 1.9+0.7

−0.4. The
upper black line shows the peak value (which, as mentioned earlier, is not relevant and
1
2 The 3 blue curves are the
therefore set to zero). The lower black line shows ln L =
three parametrised likelihood curves (using linear σ). It can be seen that they do indeed
each go through their 3 known values correctly. Otherwise we have no precise knowledge
of what they should look like, but they are apparently well behaved.

−0.5 with 2.4+0.6

−0.8 and 3.1+0.5

−

The red curve is the sum of the three blue curves (again, adjusted to have a peak value
of zero.) The position of the peak, found as described above, is indicated by the short
vertical red line, and the horizontal red line indicates the 68% conﬁdence interval, again
obtained as described above. One can thus verify by eye that the numerical techniques are
giving sensible answers.

Results are also given numerically, as shown in Figure 5. Values and errors are given,
and each measurement may be speciﬁed as being linear in σ or V using the right hand
button. On pressing the bottom left button, the graph above is drawn and the numerical
values displayed. There are also facilities to add more values (up to a limit of 10).

10

Figure 5: The user interface, showing input values, output values and options

4.1 Example of combination of results

Suppose a counting experiment sees 5 events. The result is quoted (using the ∆ ln L =
1
2 errors, even though this is a case where the full Neyman errors could be given) as
−
5+2.581
−1.916. Suppose further that it is repeated and the same result is obtained. With the
knowledge of the details we can obtain the combined result just by halving the total
measurement of 10+3.504
−1.419. But in general we would not
know this and just be given the measurements, and combine them using the above method.
This (using the linear variance model) gives a combined result of 5+1.747
−1.415. So the combined
result is exact, with discrepancies only in the fourth decimal place of the errors.

−2.838 to give an exact answer of 5+1.752

Table 1 shows these, together with the values obtained from other pairs of results with

the same sum.

Linear σ
5.000+1.737
−1.408
5.000+1.778
−1.432
5.038+1.936
−1.529
5.402+2.368
−1.826
7.350+3.149
−2.548
Table 1: Combining results in a case of two samples from the same Poisson distribution

x1
x2
5+2.581
−1.916 5+2.581
−1.916
6+2.794
−2.128 4+2.346
−1.682
7+2.989
−2.323 3+2.080
−1.416
−2.505 2+1.765
8+3.171
−1.102
9+3.342
−2.676 1+1.358
−0.6983

Linear V
5.000+1.747
−1.415
5.000+1.758
−1.425
5.009+1.793
−1.456
5.055+1.855
−1.515
5.203+1.942
−1.605

This shows that the technique, especially with the linear variance model, works very
well. There are discrepancies, but these are reasonable given the assumptions that have
had to be made. It is worth pointing out that the larger discrepancies of the ﬁnal two rows
are produced by rather unlikely experimental circumstances - the probability of 10 events
being split 9:1 or even 8:2 between the two experimental runs is small. (This shows up in
their χ2 values which are large enough to ﬂag a warning.)

11

5. Procedure for Combination of Errors

To combine errors when the likelihoods are not given in full, and only the errors are

available, we again parameterise them by the variable Gaussian model

lnL(~x) =

1
2

−

xi
σi + σ′

ixi (cid:19)

i (cid:18)

2

or

x2
i
Vi + V ′

i xi

X
where the xi represent deviations from the quoted result. Their total is u =
ﬁnd ˆL(u) the sum of Equation 23 is maximised, subject to the constraint
method of undetermined multipliers gives the solution as

i xi and to
xi = u. The
P

P

xi = u

wi
j wj

where wi =

ixi)3
P

(σi + σ′
2σi

or

(Vi + V ′
2Vi + V ′

i xi)2
i xi

(23)

(24)

(25)

This is an non-linear set of equations. However a solution can be mapped out, starting
at u = 0 for which all the xi are zero. Increasing u in small amounts, Equation 24 is used to
give the small the changes in the xi, and the weights are then re-evaluated using Equation
25.

This has also been implemented by a Java program obtainable at the web address
mentioned above. It has a similar user interface panel, and displays the form of ˆL(u) used
to read oﬀ the total ∆ ln L =

1
2 errors.

−

5.1 An example of combination of errors

Suppose that N events have been observed in an experiment, and to extract the signal
the number of background events must be subtracted. We suppose that there are several
such sources, determined by separate experiments, and that, for simplicity, these do not
have to be scaled; the backgrounds were determined by running the apparatus, in the
absence of signal, for the same period of time as the actual experiment.

Suppose that two backgrounds are measured, one giving 4 events and the other 5.
−1.682 and 5+2.581
These are reported as 4+2.346
1
−1.916. (again using the ∆lnL =
2 errors.) This
method gives the combined error as +3.333
−2.668. However in this case where the backgrounds
are combined with equal weight, one could just quote the the total number of background
events as 9+3.342
−2.676. The method’s error values are in impressive agreement with this. Further
examples are given in table 2

−

Linear
σ−

σ
σ+

Inputs
2.653 3.310
4 + 5
2.653 3.310
3 + 6
2.653 3.310
2 + 7
2.654 3.313
1 + 8
3 + 3 + 3
2.630 3.278
1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 2.500 3.098

Linear V
σ−
σ+
2.668 3.333
2.668 3.333
2.668 3.333
2.668 3.333
2.659 3.323
2.610 3.270

Table 2: Various combinations of Poisson errors which should give σ− = 2.676, σ+ = 3.342

12

6. Conclusions

If the full likelihood functions are not given, then there is no exact method for com-
bination of errors and results with asymmetric statistical errors. However the procedures
decribed here, which work by making an approximation to the likelihood function on the
basis of the quoted value and errors, appear to be reasonably accurate and robust. They
are also easy to implement and user.

Acknowledgements

The author gratefully acknowledges the support of the Fulbright Foundation

−

References
[1] R.J. Barlow:Asymmetric Systematic Errors, arXiv physics/0306168, (2003)
1
2 errors, arXiv physics/0403046, (2004)
[2] R.J. Barlow:A Note on ∆lnL =
[3] M. Schmelling:Averaging Measurements with Hidden Correlations and Asymmetric Er-
rors, arXiv:hep-ex/0006004, (2000)
[4] N.Read and D.A.S. Fraser, Likelihood inference in the presence of Nuisance Parameters,
Proc. PHYSTAT2003, Ed. L.Lyons, R. Mount, R. Reitmeyer, SLAC-PUB R 603 eConf
030908.
[5] M.S. Bartlett: On the Statistical Estimation of Mean Lifetimes, Phil. Mag. 44 244
(1953),
—
Mag. 44 1407 (1953)

Estimation of Mean Lifetimes from Multiple Plate Cloud Chamber Tracks, Phil.

13


1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
2
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A FULL BAYESIAN APPROACH FOR INVERSE PROBLEMS

Ali Mohammad-Djafari
Laboratoire des Signaux et Syst`emes (CNRS-ESE-UPS)
´Ecole Sup´erieure d’ ´Electricit´e,
Plateau de Moulon, 91192 Gif-sur-Yvette, France.
E-mail: djafari@lss.supelec.fr

Abstract. The main object of this paper is to present some general concepts of Bayesian
inference and more speciﬁcally the estimation of the hyperparameters in inverse problems.
We consider a general linear situation where we are given some data y related to the
unknown parameters x by y = Ax + n and where we can assign the probability laws
p(x|θ), p(y|x, β), p(β) and p(θ). The main discussion is then how to infer x, θ and β
either individually or any combinations of them. Diﬀerent situations are considered and
discussed. As an important example, we consider the case where θ and β are the precision
parameters of the Gaussian laws to whom we assign Gamma priors and we propose some
new and practical algorithms to estimate them simultaneously. Comparisons and links
with other classical methods such as maximum likelihood are presented.

Key words: Bayesian inference, Hyperparameter estimation, Inverse problems, Maxi-
mum likelihood

Introduction

1.
In a general Bayesian inference, we have the data y, a known relation between
the unknown parameters x and y and ﬁnally the hyperparameters β and θ. The
Bayesian estimation technique is now well established [1, 2, 3, 4, 5, 6, 7] and has
been used since many years to resolve the inverse problems in signal and image
reconstruction and restoration [10, 11, 12, 13, 14, 17, 18, 20, 21].

The ﬁrst step before applying the Bayes’ rule is to assign the prior probability
laws p(x|θ), p(y|x, β), p(θ) and p(β). The next step is to determine the posterior
laws and then to infer the unknowns. In this paper we are focusing more on the
second step than on the ﬁrst step. So we assume that all the direct probability
laws are known.

The main object of this paper is to show how can we infer simultaneously the

unknown parameters x and the hyperparameters β and θ from the data y.

Before going more in details let us give one example. This will permit us to
ﬁx the situations. Consider the case where the unknown parameters x represent
the pixel values of an unobserved image and the data y are the pixel values of an
observed image which is assumed to be a degraded version of it. If we consider a
linear degradation we have

y = Ax + n,

(1)

where A is a (m×n) matrix representing the degradation process and n represents

2

A. Mohammad–Djafari

the measurement uncertainty (noise) which is assumed to be additive, centered,
white, Gaussian and independent of x. This hypothesis leads us to

p(y|x, β) =

exp

−

β(y − Ax)t(y − Ax)

.

(2)

1
Z1(β)

1
2

(cid:26)

(cid:27)

In this case β is a positive parameter which is related to the noise variance σ2
β = 1/σ2

b and Z1(β) = (2β/π)m/2 is the normalizing factor.
Consider also, for this example, a Gaussian prior law for x :

b by

p(x|θ) =

exp

−

θφ(x)

1
Z2(θ)

1
2

(cid:26)

(cid:27)

with φ(x) = xtP −1

0 x,

(3)

where θ = 1/σ2
and Z2(θ) = (2θ/π)n/2|P 0|1/2.

x is a positive parameter, P 0 is the a priori covariance matrix of x

A well known case is the situation where θ, β and P 0 are known and we only
want to estimate x. In fact, in this special case, the joint law p(y, x|θ, β) and the
posterior law p(x|y, θ, β) are both Gaussian and we have

p(x|y, θ, β) ∝ exp

−

β(y − Ax)t(y − Ax) −

(4)

1
2

(cid:26)

1
2

θxtP −1

0 x

,

(cid:27)

and, if we note by

x = arg min
x

(cid:8)

then, it is easy to show that

b

J(x) = (y − Ax)t(y − Ax) − λxtP −1

0 x

with λ = θ/β,

(5)

(cid:9)

AtA + λP −1
0

−1

.

(6)

x|y ∼ N

x,

P

with

(cid:16)

(cid:17)

P Aty

(

x = β
P = β−1
b
b
b

b

b
(cid:1)
One can make a comparison with the classical regularization techniques for inverse
problems with smoothness hypothesis, where P −1
0 = DtD with D a matrix ap-
proximating a diﬀerentiation operator and λ is called the regularization parameter
[14].

(cid:0)

What we address here is the generalization of the problem of the determination
of the regularization parameter λ which has been studied for a long time [22, 23,
24, 25, 26, 27, 28, 15, 18, 30, 31] and is still an open problem.

What is proposed here is to consider the general case where θ and β are
considered to be unknown and we are facing to make inference as well about x
as about them. What we propose is to consider the hyperparameters θ and β in
the same manner than x, i.e; translate our prior knowledge about them by the
probability laws p(θ) and p(β), then determine the posterior laws and ﬁnally infer
about them from these posterior laws.

A FULL BAYESIAN APPROACH FOR INVERSE PROBLEMS

3

2. General Bayesian inference approach

Assume now that we know the expressions of all the prior laws. We can then
calculate the joint probability law:

p(y, x, θ, β) = p(y|x, β) p(x|θ) p(θ) p(β).

(7)

In an ideal case where we are given A, y, β and θ, to infer x we can calculate
the posterior law p(x|y, θ, β) and if we choose as the solution to our problem the
Maximum a posteriori (MAP) estimate, we have:

x = arg max

{p(x|y, θ, β)} = arg max

{p(y|x, β) p(x|θ)} .

(8)

x

x

But, unfortunately, in practical situations we are not given β and θ and the main
problem is how to infer them. We consider the following situations:

b

1. The ﬁrst is to estimate the three quantities simultaneously. We call this
method Joint Maximum a posteriori (JMAP) and the estimates are deﬁned
as

x,
(

θ,

β) = arg max

{p(y|x, β) p(x|θ) p(β) p(θ)} .

(9)

(x,θ,β)

One practical way to do this joint optimization is to use the following algo-
rithm

b

b

b

xk+1 = arg max
x
k+1

= arg max

k+1

= arg max

k

k

p(y|x,

β

) p(x|

θ

)

o

xk|θ) p(θ)
b

b
o
xk, β) p(β)

θ

β

n

p(

p(y|
b

n

n

o






θ
b
β
b

b

(10)

2. In the second case θ and β are considered as the nuisance parameters and are

b

integrated out of the problem and x is estimated by

x = arg max

{p(x|y)} = arg max

p(y, x, θ, β) dθ dβ

x

(cid:26)ZZ

(cid:27)

b

= arg max

p(y|x, β) p(β) dβ

p(x|θ) p(θ) dθ

.

(11)

x

x

(cid:26)ZZ

ZZ

(cid:27)

We call this method Marginalized MAP type one (MMAP1).

3. In the third case only θ is considered as the nuisance parameter and is inte-

grated out of the problem and x and β are estimated by

x,
(

β) = arg max
(x,β)

{p(x, β|y, θ)} = arg max

p(y, x, θ, β) dθ

b

b

= arg max

p(y|x, β) p(β)

(x,β) (cid:26)

ZZ

(x,β) (cid:26)ZZ
p(x|θ) p(θ) dθ

.

(cid:27)

(cid:27)

(12)

We call this method Marginalized MAP type two (MMAP2).

4

A. Mohammad–Djafari

(13)

(14)

4. Finally, in the last case we may ﬁrst estimate

θ and

β by

θ,
(

β) = arg max
(θ,β)

{p(θ, β|y)} = arg max

p(y, x, θ, β) dx

(cid:27)

b

b
(θ,β) (cid:26)ZZ
p(y|x, β) p(x|θ) dx

(cid:27)

b

b

= arg max

p(β) p(θ)

(θ,β) (cid:26)

= arg max
(θ,β)

ZZ

{p(β) p(θ)l(θ, β|y)} .

and then used them for the estimation of x by

x = arg max

p(x|y,

θ,

β)

.

x

n

o

b

b

b

We call this method Marginalized MAP type three (MMAP3).
Note that if p(θ) and p(β) are uniform functions of θ and β, then
θ and
β correspond to the classical maximum likelihood (ML) estimates because
l(θ, β|y) is, for a given y, the likelihood function of θ and β.
The calculus of l(θ, β|y) is not easy and so is its optimization. Many works
b
have been done on the subject. We distinguish three kind of methods:
– The ﬁrst is to use the Expectation-Maximization (EM) algorithm which has
been developed exactly in the context of ML parameter estimation [32, 4, 33].
– The second is to estimate the integral using a Monte Carlo simulation
method (Stochastic EM: SEM).
– The third is to make some approximations. For example, at each iteration
during the optimization, one may obtain an analytical expression for that in-
tegral by approximating the expression inside it by a second order polynomial
(Gaussian quadrature approximation).

b

We will consider this last method.

3. A case study
Let us consider the following simple linear inverse problem y = Ax + n and make
the following hypothesis:

− The noise n is considered to be white, centered and Gaussian with precision

β, so that we have

y|x, β ∼ N (Ax, β−1I) −→ p(y|x, β) =

exp

−

βky − Axk2

.

1
Z1(β)

1
2

(cid:26)

(cid:27)

(15)

where Z1(β) ∝ βm/2.

− Our prior prior knowledge about x can be translated by

p(x|θ) =

exp

−

θφ(x)

.

(16)

1
Z2(θ)

1
2

(cid:26)

(cid:27)

where we will consider the following special cases for φ(x):

A FULL BAYESIAN APPROACH FOR INVERSE PROBLEMS

5

• Gaussian priors:

φG(x) = xtP −1

0 x = kDxk2 −→ x|θ ∼ N (0, θ−1P −1
0 ),

which can also be written φG(x) =
cases:

j

i pij xixj with some special

φG(x) =

x2
j , or φG(x) =

P

P

|xj − xj−1|2.

• Generalized Gaussian priors:

j
X

j
X

φGG(x) =

|xj − xj−1|p,

1 < p ≤ 2.

j
X

• Entropic priors:

φE(x) =

S(xj) where S(xj ) =

x2
j , xj ln xj − xj, ln xj − xj

.

(cid:8)

(cid:9)

• Markovian priors:

φM (x) =

V (xj , xi), where V (xj , xi) is a potential function

n

j=1
X

j
X

Xi∈Nj

and where Nj is a set of sites considered to be neighbors of site j, for
example Nj = {j − 1, j + 1},

or Nj = {j − 2, j − 1, j + 1, j + 2}.

Note that, in all cases θ is generally a positive parameter. Note also that in
the ﬁrst case we have Z2(θ) ∝ θn/2. Unfortunately we have not an analytic
expression for Z2(θ) in the other cases. However, in the situations we are
concerned with, Z2(θ) can either be calculated numerically or approximated
by

Z2(θ) ∝ θαn/2.

(17)

− θ and β are both positive parameters. We choose Gamma prior laws for them:

θ ∼ G(a, ζ) −→ p(θ) ∝ θ(a−1)exp {−ζθ} −→ E {θ} = a/ζ, Var {θ} = a/ζ2

β ∼ G(b, ζ) −→ p(β) ∝ β(b−1)exp {−ζβ} −→ E {β} = b/ζ, Var {β} = b/ζ2

Now, using the following notations

Q(x) = ky − Axk2,

J0(x) = βQ(x) + θφ(x),

∇Q(x) = −2At(y − Ax),

and ∇J0(x) = β∇Q(x) + θ∇φ(x),

we can calculate the expression of the joint pdf p(y, x, θ, β) = p(y|x, β) p(x|θ) p(θ) p(β),
which can be written

p(y, x, θ, β) ∝ θ−(αn/2−a+1)β−(m/2−b+1)exp

−

J1(x)

,

(18)

1
2

(cid:26)

(cid:27)

6

A. Mohammad–Djafari

with

J1(x) = β[Q(x) + 2ζ] + θ[φ(x) + 2ζ] = J0(x) + 2ζ(θ + β).

(19)

This will let us to go further in details of some of the above mentioned cases. For
example in the Gaussian case we have:

x|y, θ, β ∼ N (

x,

P ) with

x = β

βAtA + θP −1
0

Aty and

P =

βAtA + θP −1
0

−1

−1

b
θ|y, x, β ∼ G(a − αn/2,

b

b

(cid:0)
b
[φ(x) + 2ζ]) −→ E {θ|y, x, β} =

(cid:1)

β|y, x, θ ∼ G(b − m/2,

[Q(x) + 2ζ]) −→ E {β|y, x, θ} =

1
2
1
2

(cid:1)

(cid:0)
2a − αn
[φ(x) + 2ζ]
2b − m
[Q(x) + 2ζ]

,

.

Now, let us consider the four aforementioned methods a little more in details.
3.1. Joint Maximum A Posteriori (JMAP)

Using the expressions and the notations of the last paragraph in (11) we have

to deal with the following algorithm:

xk+1 = arg max
x

p(y|x,

βk) p(x|

θk)

θk+1 = arg max
b

p(

xk|θ) p(θ)
b

J0(x,

= arg min
x
o
xk) + 2ζ]θ − (2a − αn − 2) ln θ

[φ(

n

,

βk,

θk)

b

b

,

o
= arg min
b

θ

βk+1 = arg max
b

p(y|
b

o

xk, β) p(β)

n
= arg min

o
xk) + 2ζ]β − (2b − m − 2) ln β

.

[Q(
b

β

n

o

n

n

n

θ

β

o

b
The two last equations have explicit solutions. In the case of Gaussian priors, the
ﬁrst equation has also an explicit solution. However, in general, we propose the
following gradient based algorithm:

b

b

Algorithm 1:

xk+1 = (1 − µ)

θk∇φ(

xk)],

0 < µ < 1,

θk)

βk,
xk) +
b
b

= (1 − µ)

xk,
βk∇Q(
b

xk − µ∇J0(
xk − µ[
b
(2a − αn − 2)
xk) + 2ζ]
b
[φ(
(2b − m − 2)
xk) + 2ζ]
b

[Q(

,
b

,

a > (αn + 2)/2,

b

b

b

b > (m + 2)/2.

b
θk+1 =

b
βk+1 =

b

The conditions a > (αn + 2)/2 and b > (m + 2)/2 are added to satisfy, when
θ and
necessary, the positivity constraint of
3.2. Marginalized Maximum A Posteriori MMAP1

β.

b

Considering θ and β as the nuisance parameters and integrating out them from

b

b

p(y, x, θ, β) we obtain

p(y, x) =

p(y, x, θ, β) dβ dθ ∝ [Q(x)+2ζ]−(m−2b)/2 [φ(x)+2ζ]−(αn−2a)/2 (20)

ZZ

Now, deﬁning

xMMAP = arg max

{p(x|y)} = arg min
x

x

1
2

(cid:26)

J2(x)

,
(cid:27)

with

J2(x) = (2a − αn) ln[Q(x) + 2ζ] + (2b − m) ln[φ(x) + 2ζ],

b

(21)

A FULL BAYESIAN APPROACH FOR INVERSE PROBLEMS

7

and trying to calculate this solution by an iterative gradient based algorithm, we
have to calculate

∇J2(x) =

(2a − αn)
[Q(x) + 2ζ]

∇Q(x) +

(2b − m)
[φ(x) + 2ζ]

∇φ(x).

Algorithm 2:

We propose then the following iterative algorithm:
xk+1 = (1 − µ)
xk)
βk∇Q(
= (1 − µ)
b

b

xk − µ∇J2(
xk − µ[
b
(2a − αn)
xk) + 2ζ]
b
(2b − m)
xk) + 2ζ]
b
[Q(

[φ(

b

,

,

θk =

b
βk =

a > αn/2,

b

b

b

b > m/2.

xk) +

θk∇φ(

xk)],

0 < µ < 1,

3.3. Marginalized Maximum A Posteriori MMAP2

b

b

In this case, θ only is considered as a nuisance parameter and is integrated out:

p(y, x, β) =

p(y, x, θ, β) dθ

ZZ

∝ β−m/2+b−1[φ(x) + 2ζ]−(αn−2a)/2exp

−

β[Q(x) + 2ζ]

.(22)

1
2

(cid:26)

Then, x and β are estimated by

(cid:27)

(23)

x,
(

β) = arg max

{p(y, x, β)} .

x,β

Noting

b

b

−2 ln p(y, x, β) = −(2b − m − 2) ln β + (2a − αn) ln[φ(x) + 2ζ] + β[Q(x) + 2ζ]

and diﬀerentiating it with respect to β gives

β =

2b − m − 2
[Q(x) + 2ζ]

.

So, noting

b

J3(x, β) = (2a − αn) ln[φ(x) + 2ζ] + β[Q(x) + 2ζ]

(24)

and

∇J3(x, β) =

∇φ(x) + β∇Q(x),

2a − αn
[φ(x) + 2ζ]

and using a gradient based algorithm for minimizing J3 with respect to x we
propose the following:

Algorithm 3:

xk+1 = (1 − µ)

xk+1 −

βk∇Q(x) −

θk∇φ(x),

0 < µ < 1,

,

2a − αn
xk) + 2ζ]
b
[φ(
2b − m − 2
xk) + 2ζ]
b
[Q(

,

a > (αn + 2)/2
b

b

b > (m + 2)/2.

θk =

b

b
βk =

b

b

A. Mohammad–Djafari

8

b

3.4. Maximum Likelihood or MMAP3

In this case ﬁrst x integrated out x from p(y, x, θ, β) to obtain:

β(b−1)
Z2(β)

θ(a−1)
Z1(θ)

1
2

p(y, θ, β) =

p(y, x, θ, β) dx =

exp

−

J1(x, β, θ)

dx (25)

ZZ
with

ZZ
J1(x, β, θ) = β[Q(x) + 2ζ] + θ[φ(x) + 2ζ].

(26)
Excepted the Gaussian case where J1 is a quadratic function of x, in general, it is
not easy to obtain an analytical expression for this integral. One can then try to
make a Gaussian approximation which means to develop J1 around its minimum
xMAP = arg minx {J1(x, β, θ)} by

(cid:26)

(cid:27)

J1(x, β, θ) ≃

(x −

xMAP)tM (x −

xMAP) + gt(x −

xMAP) + c,

(27)

1
2

where g = β∇Q(x) + θ∇φ(x) is the gradient of J1 and M is its Hessian, both
calculated for

xMAP. With this approximation we obtain

b

b

b

p(y, θ, β) = β−m/2+b−1θ−αn/2+a−1|M (β, θ)|− 1

2 exp

b

−

J1(

xMAP, β, θ)

.

(28)

Diﬀerentiating l(θ, β|y) = ln p(y, θ, β) with respect to β and θ gives

1
2

(cid:26)

(cid:27)

b
2a − αn − 2
xk) + 2ζ] + trace[M −1P −1
0 ]

.

[φ(

2b − m − 2

xk) + 2ζ] + trace[M −1AtA]

,

θ =

β =

[Q(
where P −1
0
b
Hessian of J1(x):

b

b
M (β, θ) = βAtA + θP −1
0 .

is the Hessian of φ(x), AtA is the Hessian of Q(x) and M is the

b

Using these expressions we propose the following algorithm:

Algorithm 4:

xk = arg min

J1(x,

βk,

θk)

= M (

βk,

θk)−1Aty,

θk+1 =

b

b
βk+1 =

x

n

o

[φ(

2a − αn − 2
b
b
,
xk) + 2ζ] + trace[M −1P −1
0 ]
2b − m − 2

b

xk) + 2ζ] + trace[M −1AtA]
b
[Q(

.

b

This algorithm needs the inversion of the matrix M which is very costly in practice.

b

b

4. Comparison and the main structure of the proposed algorithmes
Comparing the Algorithms 1 to 4, one can see that they all have the same
structure:

− for ﬁxed θ and β optimize locally a criterion J(x, β, θ), and
− update θ and β using the solution

x just obtained and iterate until conver-

gence.

Note also that only in Algorithm 4, the updating step takes account of the
b
measurement system operator A and the covariance structure P 0 of the input x.

A FULL BAYESIAN APPROACH FOR INVERSE PROBLEMS

9

5. Conclusions and perspectives

We considered the inverse problem of infering the unknowns x from the data y in
a special case of linear inverse problems y = Ax+n using a full bayesian approach
and presented four algorithms to estimate simultanously the hyperparameters θ
and β and the unknowns x. The main structure of all of these algorithms are
the same even if the procedure to deduce them have been diﬀerent. However,
we have not yet really tested them to give any conclusion about their relative
performances. Note however that one of them distinguishes itself from the others
by taking account of the measurement system operator A and the covariance
structure P 0 of x in the hyperparameters updating step and, by the same way,
by its calculation cost. We hope to be able to give some measure of their relative
performances in simulation and in real applications in near future.

References

1. G. Box and G.C. Tiao, Bayesian inference in statistical analysis. Addison-

Wesley publishing, 1972.

2. H. Sorenson, Parameter estimation. Marcel Dekker, Inc., 1980.
3. J. Besag, “Digital image processing : Towards Bayesian image analysis,” Jour-

nal of Applied Statistics, vol. 16, no. 3, pp. 395–407, 1989.

4. P. J. Green, “Bayesian reconstructions from emission tomography data using
a modiﬁed EM algorithm,” IEEE Transactions on Medical Imaging, vol. 9,
pp. 84–93, Mar. 1990.

5. D. Malec and J. Sedransk, “Bayesian methodology for combining the results
from diﬀerent experiments when the speciﬁcations for pooling are uncertain,”
Biometrika, vol. 79, no. 3, pp. 593–601, 1992.

6. G. Gindi, M. Lee, A. Rangarajan, and Z. I., “Bayesian reconstruction of
functional images using anatomical information as priors,” IEEE Transactions
on Medical Imaging, vol. MI-12, no. 4, pp. 670–680, 1993.

7. J. Bernardo and A. Smith, Bayesian Theory. Chichester, England: John

8. Barndorﬀ-Nielsen, Information and Exponential Model in Statistics. New-

Wiley, 1994.

York: John Wiley, 1978.

9. H. Derin, H. Elliott, R. Cristi, and D. Geman, “Bayes smoothing algorithms
for segmentation of binary images modeled by markov random ﬁelds,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-6, p. 4,
1984.

10. S. Geman and D. Geman, “Stochastic relaxation, Gibbs distributions, and
the Bayesian restoration of images,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. PAMI-6, p. 2, 1984.

11. A. Tarantola, Inverse problem theory : Methods for data ﬁtting and model

parameter estimation. Amsterdam: Elsevier Science Publishers, 1987.

12. J. Skilling, Maximum-Entropy and Bayesian Methods. Dordrecht, The Nether-

lands: Kluwer Academic Publisher, 1988.

13. Titterington and Rossi, “Another look at a Bayesian direct deconvolution

10

A. Mohammad–Djafari

method,” Signal Processing, vol. 9, pp. 101–106, 1985.

14. G. Demoment, “Image reconstruction and restoration : Overview of common
estimation structure and problems,” IEEE Transactions on Acoustics Speech
and Signal Processing, vol. 37, pp. 2024–2036, Dec. 1989.

15. K.-Y. Liang and D. Tsou, “Empirical Bayes and conditional inference with
many nuisance parameters,” Biometrika, vol. 79, no. 2, pp. 261–270, 1992.
16. R. E. McCulloch and P. E. Rossi, “Bayes factors for nonlinear hypotheses and
likelihood distributions,” Biometrika, vol. 79, no. 4, pp. 663–676, 1992.
17. J. Idier and Y. Goussard, “Markov modeling for Bayesian restoration of two-
dimensional layered structures,” IEEE Transactions on Information Theory,
vol. 39, pp. 1356–1373, July 1993.

18. A. Mohammad-Djafari, “On the estimation of hyperparameters in Bayesian
approach of solving inverse problems,” in Proceedings of IEEE ICASSP,
pp. 567–571, 1993.

19. A. Nallanathan and W. J. Fitzgerald, “Bayesian model selection applied to
spatial signal processing,” Proceedings of the IEE, vol. 141, pp. 76–80, Feb.
1994.

20. J. Diebolt and C. P. Robert, “Estimation of ﬁnite mixture distributions
through Bayesian sampling,” Journal of Royal Statistical Society B, vol. 56,
no. 2, pp. 363–375, 1994.

21. H. Carfantan and A. Mohammad-Djafari, “A Bayesian approach for nonlinear
inverse scattering tomographic imaging,” in Proceedings of IEEE ICASSP,
vol. IV, pp. 2311-2314, May 1995.

22. J. Cullum, “The eﬀective choice of the smoothing norm in regularization,”

Math. Comp., vol. 33, pp. 149–170, 1979.

23. Titterington, “General structure of regularization procedures in image recon-

struction,” Astrononmy and Astrophysics, vol. 144, pp. 381–387, 1985.

24. L. Youn`es, “Estimation and annealing for Gibbsian ﬁelds,” Annales de

l’institut Henri Poincar´e, vol. 24, pp. 269–294, Feb. 1988.

25. S. Lakshmanan and H. Derin, “Simultaneous parameter estimation and seg-
mentation of Gibbs random ﬁelds using simulated annealing,” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, vol. PAMI-11, no. 8,
pp. 799–813, 1989.

26. A. Mohammad-Djafari and J. Idier, “Maximum likelihood estimation of the
lagrange parameters of the maximum entropy distributions,” in Maximum En-
tropy and Bayesian Methods in Science and Engineering (C. Smith, G. Erik-
son, and P. Neudorfer, eds.), pp. 131–140, Kluwer Academic Publishers, 1991.
27. Thompson, Brown, Kay, and Titterington, “A study of methods of choos-
ing the smoothing parameter in image restoration by regularization,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 13, Apr.
1991.

28. E. Gassiat, F. Monfront, and Y. Goussard, “On simultaneous signal estimation
and parameter identiﬁcation using a generalized likelihood approach,” IEEE
Transactions on Information Theory, vol. IT-38, pp. 157–162, 1992.

29. T. J. Hebert and R. Leahy, “Statistic-based map image reconstruction from

A FULL BAYESIAN APPROACH FOR INVERSE PROBLEMS

11

poisson data using Gibbs prior,” IEEE trans. on Signal Processing, vol. 40,
pp. 2290–2303, Sept. 1992.

30. C. Bouman and K. Sauer, “Maximum likelihood scale estimation for a class
of markov random ﬁelds penalty for image regularization,” in Proceedings of
IEEE ICASSP, vol. V, pp. 537–540, 1994.

31. A. N. Iusem and B. F. Svaiter, “A new smoothing-regularization approach
for a maximum-likelihood problem,” Applied Mathematics and Optimization,
vol. 29, pp. 225–241, 1994.

32. A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from incomplete
data via the EM algorithm,” Journal of Royal Statistical Society B, vol. 39,
pp. 1–38, 1977.

33. Vardi and Lee, “From image deblurring to optimal investments maximum
likelihood solutions for positive linear inverse problems,” Journal of Royal
Statistical Society B, vol. 55, no. 3, pp. 569–612, 1993.


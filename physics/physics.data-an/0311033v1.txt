3
0
0
2
 
v
o
N
 
7
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
3
0
1
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Bayesian Wavelet Based Signal and Image
Separation

Mahieddine M. Ichir∗ and Ali Mohammad-Djafari∗

∗Laboratoire des Signaux et Systèmes,
Supélec, Plateau de Moulon, 3 rue Joliôt Curie, 91192 Gif-sur-Yvette, France

Abstract. In this contribution, we consider the problem of blind source separation in a Bayesian
estimation framework. The wavelet representation allows us to assign an adequate prior distribution
to the wavelet coefﬁcients of the sources. MCMC algorithms are implemented to test the validity of
the proposed approach, and the non linear approximation of the wavelet transform is exploited to
aleviate the algorithm.

INTRODUCTION

We ﬁnd applications of blind source separation (BSS) in many ﬁelds of data analysis:
chemistry, medical imaging (EEG, MEG), seismic data analysis and astronomical imag-
ing. Many solutions have been developped to try to solve this problem: Independant
Component Analysis (ICA) [3, 6], maximum likelihood estimation [5], and methods
based on second or higher order statistics of the signals [1, 2]. These methods have
proved their efﬁciency in many applications, however they do not apply for noisy obser-
vations models.

A different approach has been considered to solve the BSS problem, we ﬁnd in
[13, 11, 8] an introductory analysis of the problem in a Bayesian estimation framework.
Some of the methods outlined earlier can be reformulated via the Bayes rule, and a
similar formalism can be obtained.

In this contribution, we treat the BSS problem in a Bayesian estimation framework.
As in previous works on this subject [10, 7], the problem is transported to a transform
domain: the wavelet domain. The advantage of such an approach is that some invertible
transforms restructure the data, leaving them structures simpler to model, and this, as
will be seen later, is useful in the formulation of the problem as an inference problem.

The paper is organized as follows: In section-II we present the BSS problem, write the
associated equations and introduce the Bayesian solution of the problem. In section-III,
we transport the problem to a transformed data space (wavelet) and give the justiﬁcation
for that approach. In section-IV, we present the associated MCMC-based optimization
algorithm. We consider then the non-linear approximation of the wavelet transform to
introduce a denoising procedure by some thresholding rule. At the end, we conclude and
give future perspectives of the present work.

BAYESIAN BLIND SOURCE SEPARATION (BBSS)

Blind source separation (BSS) consists of recovering unobservable sources from a set of
their instantaneous and linear mixtures. The direct observational model can be described
by:

x(t) = As(t) + ǫ(t),

(1)
2 : 2D images}, x(t)t=1,...,T is the observed m-
where C = {Z : time series signals, Z
column vector, s(t)t=1,...,T is the unknown sources n-column vector, A is the (m × n)
full rank matrix, and ǫ(t)t=1,...,T is the noise m-column vector.

t ∈ C

The Bayesian approach to BSS starts by writing the posterior distribution of the
sources, jointly with the mixing matrix and any other parameters needed to describe
the problem:

P

s, A, Rǫ

x

∝ P

x

π (s, A, Rǫ)

(2)

(cid:12)
(cid:12)

(cid:1)

(cid:0)

x

s, A, Rǫ

(cid:0)
is the joint posterior distribution of the unknown sources, the
where P
is the likelihood of the
mixing matrix and the noise covariance matrix. P
observed data x and π (s, A, Rǫ) is the prior distribution that should reﬂect the prior
information we may have about s, A and Rǫ. The noise ǫ(t) is assumed Gaussian,
spacially independant and temporarily white: N (0, Rǫ), with Rǫ = diag (σ2

s, A, Rǫ
(cid:12)
(cid:12)

x

(cid:12)
(cid:12)

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

1, ..., σ2

m).

An important step in Bayesian inference problems is to assign appropriate expressions
to π (s, A, Rǫ). The likelihood P
is determined by the hypotheses made on
the noise ǫ(t). It is reasonable to assume that the sources, the mixing matrix and the noise
variance are independant:

s, A, Rǫ
(cid:12)
(cid:12)

x

(cid:1)

(cid:0)

s, A, Rǫ
(cid:12)
(cid:12)

π (s, A, Rǫ) = π (s) π (A) π (Rǫ)

The prior distribution π (A) can be determined by some physical knowledge of the mix-
ing mechanism. In our work, the mixing matrix is assigned a Gaussian prior distribution:

The appropriate selection of prior distributions is still a subject of intensive research. We
ﬁnd in [16, 12] some interesting work on this topic. We thus deﬁne, as results of these
works, a Gamma prior distribution for the inverse of the variances:

π (A) =

N

aij

Yi,j

(cid:0)

µij, σ2
ij
(cid:12)
(cid:12)

(cid:1)

π (x) = G(x|α, θ) ∝ xα−1e− x

θ

or in a matrix form, a Wishart distribution for the inverse of covariance matrices:

π(R−1) = W

R−1|Σ, ν

∝ |R|− ν−(m+1)

2

exp

(cid:0)

(cid:1)

ν
2

−
(cid:2)

Tr

R−1Σ−1
(cid:0)

(cid:1) (cid:3)

where m = rank(R).

to the sources:

Some work has been done on BSS [17, 15], by assigning a mixture of Gaussians prior

π (s) =

pl Nl

s

L

Xl=1

,

µl, τl
(cid:12)
(cid:12)

(cid:1)

(cid:0)

L

Xl=1

pl = 1

(3)

(4)

(5)

(6)

(7)

This distribution is very interesting, any distribution π∗ (s) can be well aproximated by
a Gaussian mixture distribution, and the higher L (number of Gaussians), the better the
approximation, but the higher the complexity of the associated model. The difﬁculty lies
than in how L should be chosen to well approximate the distribution with reasonable
complexity ?. We note that for L Gaussians, we need (3L − 1) parameters (p, µ, τ ) to
totally deﬁne the mixture.

BBSS IN THE WAVELET DOMAIN

(8)

(9)

(10)

An idea, that has been exploited with success, is to treat the problem in a tranform
domain. We ﬁnd in [14] a proposed solution to a spectral BSS problem. In [10, 7], a
ﬁrst approach to the problem has been treated in the wavelet domain. The particular
properties of these transforms: linearity and inversibility makes that the BSS problem is
formulated in a similar manner and that we can go back and forth without any difﬁculty.
The BSS problem described by equation (1) is rewritten in the wavelet domain as:

wj

x(k) = Awj

s (k) + wj

ǫ (k) k ∈ C, j = 1, . . . , J

where C = {Z : time series signals, Z

2 : 2D images}, and :

wj

s(k) = < s(t), ψj(t − k) > =

s(t) ψj(t − k) dt

ZC

where ψj(t) = 2−j/2ψ (2−jt). We point out to the fact that the statistical properties of the
noise does not change in the wavelet doamin:

ǫ(t) ∼ N

0, σ2
ǫ

=⇒ wj

ǫ (k) ∼ N

0, σ2
ǫ
(cid:0)

(cid:0)

(cid:1)

s (k), wj

x(k) and wj

(cid:1)
We will refer by wj
ǫ (k) to the wavelet coefﬁcients vectors of s(t), x(t)
and ǫ(t) at resolution j, respectively. The k-index will be dropped to aleviate the
expressions since wj
s deﬁne
identically the same vector unless speciﬁed.
The posterior distribution of the new unknowns is now given by:

ǫ are temporarily white, and thus wj

s (k) and wj

s and wj

P

s , A, Rǫ

wj
(cid:12)
(cid:12)
The wavelet transform has some particular properties that make it interesting for
Bayesian formulation of the BSS problem:

π (A) π (Rǫ)

wj
x
(cid:12)
(cid:12)

wj
(cid:0)

s , A, Rǫ

(11)

∝ P

wj
x

wj
s

π

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:1)

locality each wavelet atom ψj(t − k) is localised in time and frequency.
edge detection a wavelet coefﬁcient is signiﬁcant if and only if an irregularity is present

within the support of the wavelet atom.

These two properties have a great impact on the wavelet (1D/2D)-statistical signal
processing. The wavelet coefﬁcients can be reasonably considered uncorrelated due to
locality (we say that the wavelet transform acts as decorrelator), and assigned a separable

probabilty distribution:

π

n[j,k

wj

s(k)

=

π

wj

s(k)

o

Yj,k

(cid:0)

(cid:1)

(12)

The second property (edge detection) has a consequence on the type of the distribution

we will assign to the wavelet coefﬁcients:

The wavelet transform of natural sources results in a large number of small
coefﬁcients, and a small number of large coefﬁcients.

This property (sparsity) is shown in Figure (1). The prior distribution of the wavelet
coefﬁcients is then very well approximated by centered, peaky and heavy tailed like
distributions. Mallat has porposed in [9] to model the wavelet coefﬁcients by generalized
exponential distributions:

P (.) = KExp

−

γ > 0, 1 ≤ α ≤ 2

(13)

1
γ

|.|α

,

(cid:19)

(cid:18)

Crouse in [4] has assigned to the wavelet coefﬁcients a Gaussian mixture distribution to
capture the sparsity characteristic:

P (.) = p N

+ (1 − p) N

0, τH

,

τH >> τL

(14)

0, τL
.
(cid:12)
(cid:12)

(cid:0)

(cid:1)

.
(cid:12)
(cid:0)
(cid:12)

(cid:1)

where p = Prob.(wavelet coefﬁcient ∈ low energy state). In the sequel, we will only
emphasize on the Gaussian mixture model. For the generalized exponential case, we
refer to [7]. Note that we choose a two Gaussian mixture model with a total number of
parameters equals to three.

MCMC IMPLEMENTATION

wj

wj
x

s , A, Rǫ

Once we have deﬁned the priors and properly written the posterior distribution
, we deﬁne a posterior estimates of the different parameters that
P
characterizes the BSS problem. To do this, we will generate samples from the joint
(cid:1)
distribution (11), by means of MCMC algorithms (Monte Carlo Markov Chain methods)
and than choose the posterior means as estimates.

(cid:12)
(cid:12)

(cid:0)

Hidden variables

The conditional posterior distribution of the sources coefﬁcients is a mixture of

Gaussians of the type:

where

x, A, Rǫ

∝ N

s , Rǫ

π

P

wj
s

(cid:0)

wj
(cid:12)
(cid:12)

(cid:1)

n

wj
x
(cid:0)

Awj
(cid:12)
(cid:12)
n

L

wj
s
(cid:0)

θ

(cid:1)

(cid:12)
(cid:12)

π

wj
s

(cid:0)

=

θ
(cid:12)
(cid:12)

(cid:1)

π

wj
s i

Yi

(cid:0)

=

θ
(cid:12)
(cid:12)

(cid:1)

Yi

Xl

pliN

wj
s i

0, τli
(cid:12)
(cid:12)

(cid:1)

(cid:1)

(cid:0)

(15)

where i stands for the i-th source. The complexity of such model increases with in-
creasing n (for a 2-Gaussians wavelet model, a total of (2L − 1)n = 3n parameters has
to be deﬁned in order to describe the model). Thus the introduction of a label variable
zj ∈ {1, . . . , L}n = {1, 2}n = {Low state, High state}n and a conditional parametrisation
of the form:

wj
wj
s
s
(cid:0)
with P (zj ∈ L) = pL, and P (zj ∈ H) = pH = 1 − pL.

θ, zj ∈ [L, H]
(cid:1)

= N

π

(cid:0)

(cid:12)
(cid:12)

0, τ[L,H]
(cid:12)
(cid:12)

(cid:1)

(16)

The MCMC Algorithm

The hidden variables

1. zj ∼ P

wj

x, θ

=

P

zj, wj
s

wj

x, θ

zj
(cid:0)

(cid:12)
(cid:12)

(cid:1)

Zws

(cid:0)

= π

(cid:12)
(cid:12)
wj
x

(cid:1)
Awj
(cid:12)
(cid:12)

N

Zws

zj
(cid:0)
(cid:1)
(cid:0)
, and Rτ = diag (τ1, ..., τn).

s , Rǫ

(cid:1)

π

wj
s

(cid:0)

zj, Rτ
(cid:12)
(cid:12)

(cid:1)

= N

wj
s
(cid:0)

zj, Rτ
wj
0, Rτ
where π
s
The sources wavelet coefﬁcients
(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
x, zj, θ
wj
2. wj
(cid:12)
(cid:12)

s ∼ P

wj
s

(cid:1)

(cid:0)

(cid:0)

(cid:1)

where µs/z = Rs/zR−1
The mixing matrix

ǫ AT wj

x, and Rs/z =

(cid:0)

= N

= N

(cid:0)

wj
x
wj
s

Awj
s , Rǫ
(cid:12)
(cid:1)
µs/z, Rs/z
(cid:12)
(cid:12)
(cid:0)
(cid:12)
ǫ A + R−1
AT R−1
τ

N

wj
s
(cid:0)

(cid:12)
(cid:12)

0, Rτ

(cid:1)

(cid:1)
−1.

(cid:1)

3. A ∼ P

A

x, θ

wj
(cid:12)
(cid:12)

(cid:1)

(cid:0)

= N

= N

s , Rǫ

N

(cid:1)

Awj
wj
x
(cid:12)
µA, RA
A
(cid:12)
(cid:12)
(cid:12)

(cid:0)

(cid:0)

(cid:1)

A
(cid:0)

µa, Ra
(cid:12)
(cid:12)

(cid:1)

, RA = (R−1

ǫ ⊗ Css + R−1

a ),

where vec1 (µA) = RA
j,k wj
Css =
s
The hyperparameters

T wj

P

(R−1

ǫ ⊗ In) vec (Cxs) + µa
j,k wj
x

T wj
s .

(cid:17)

(cid:16)
s and Cxs =

P

4. θ ∼ P

wj
(cid:12)
(cid:1)
(cid:12)
where θ stands for the the noise covariance matrix Rǫ and the mixture parameters
Rτ = diag (τ1, ..., τn) (variances of the Gaussians in the mixture).
The noise covariance matrix

wj
(cid:12)
(cid:12)

s , A, θ

x, wj

θ
(cid:0)

π (θ)

s , A

wj
x

= P

(cid:1)

(cid:0)

4.a. R−1

ǫ ∼ P

x, wj

s , A

R−1
ǫ
(cid:0)

wj
(cid:12)
(cid:12)

= N

= W

(cid:1)

s , R−1
ǫ
,

Awj
wj
x
(cid:12)
(cid:0)
R−1
(cid:12)
ǫ
(cid:0)

Σ′, ν′
(cid:12)
(cid:12)

(cid:1)

W

R−1
ǫ

(cid:1)
i = 1, ..., m

(cid:0)

Σ, ν
(cid:12)
(cid:12)

(cid:1)

1 vec(.) is the vector representation of a matrix.

where ν′ = ν + T and Σ′ =
The Gaussians variances

(cid:0)

(cid:1)(cid:14)

νΣ + T Cǫ

(ν + T ), where Cǫ =

4.b.

τ j
i [L, H] ∼ P

τ j
i

wj
si
(cid:12)
(cid:12)

(cid:1)

= N

wj
si
(cid:0)
τ j
= IG
i

(cid:0)

where αj = T /2j + 2 and 1/θj
The prior probabilities

i =

(cid:0)P

(cid:0)

si.I
wj

k

(zj

i =l)

wj

ǫ wj
ǫ

T .

P

(cid:14)

τ j
i
(cid:0)
i = 1, ..., n

2, 1
(cid:12)
(cid:12)

(cid:1)

l = {L, H}.

IG

,

(cid:1)

0, τ j
i
(cid:12)
(cid:1)
αj, θj
(cid:12)
i
(cid:12)
(cid:0)
(cid:12)
2/2 + 1
(cid:1)

,
(cid:1)

5.

[pj

iL, pj
I

iH] ∼ P

pj
iL, pj

iH

(cid:0)

θ
(cid:12)
(cid:12)

(cid:1)

= D2 (u1 + niL, u2 + niH) ,

i = 1, ..., n

where nil =
eters (γ1, γ2) for the probability variables (pL, pH = 1 − pL).

i =l), and D2 (γ1, γ2) stands for the Dirichlet distribution with param-
(zj

P

k

SIMULATION RESULTS

To verify the plausibilty of the proposed algorithm, we have made some tests on simu-
lated data (256 x 256 pixels). In ﬁgure 2.a, we present an aerial image and a cloud image
that were linearily mixed to obtain the observed data in ﬁgure 2.b. The mixing matrix is
of the form:

The signal to noise ratio is of 20dB. The Symmlet-4 wavelet basis has been chosen (with
4 vanishing moments). The obtained estimates of the sources are presented in ﬁgure 2.c.
The evolution of the estimates of the elements of the matrix is presented in ﬁgure 3,
where the empirical posterior mean is found to be:

A =

.90 .50
.44 .87 (cid:21)

(cid:20)

ˆA =

.91 .58
.41 .82 (cid:21)

(cid:20)

To quantify the estimates of the sources, we choose a distance that is invariant under

a scale transformation (since the sources are estimated up to a scale factor):

(cid:12)
(cid:12)
(cid:12)
(cid:12)
In order to quantify the estimates of the mixing matrix, we measure the observation

Xt

δ (s1(t), s2(t)) = v
u
u
t

−

(17)

s1(t)
ks1(t)k

2

s2(t)
ks2(t)k (cid:12)
(cid:12)
(cid:12)
(cid:12)

distance deﬁned by:

δA =

δ ( ˆxi(t), xi(t))

(18)

1
m

m

Xi

where ˆx(t) = ˆAs(t) and x(t) = As(t). In the simulated example, δA = .033.

NON LINEAR MCMC IMPLEMENTATION

The implementation of the proposed MCMC algorithm is modiﬁed by making use of the
non linear approximation of the wavelet transfrorm:

fM [n] =

< f, ψj,k > ψj,k[n]

(19)

X{j,k}∈IM

where IM corresponds to the largest coefﬁcients, and fM [n] is the non linear approxi-
mation of f [n] by the M largest coefﬁcients. It is implemented by applying some non
linear function to the wavelet coefﬁcients of the form:

T (< f, ψj,k >) =

< f, ψj,k > for
0

elsewhere

(cid:26)

| < f, ψj,k > | ≥ χ

known as hard thresholding. We deﬁne equivalently the soft thresholding by:

T (< f, ψj,k >) =

< f, ψj,k > −χ for
0

elsewhere

(cid:26)

| < f, ψj,k > | ≥ χ

In step 1 of the MCMC algorithm, the hidden variable zj is sampled from the
posterior probability P (zj|.). The non linear approximation procedure consists then
of sampling only the coefﬁcients that are large (in a high energy state), that corresponds
to zj ∈ H:

1. zj ∼ P

zj

x, θ

wj
(cid:12)
(cid:12)

(cid:1)

(cid:0)

=

Zws

P

zj, wj
s

x, θ

(cid:0)

zj

= π

(cid:1)
Awj
zj, Rτ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= [ postL, postH ]n ⇒ zj ∈ {L, H}n

s , Rǫ

wj
s

Zws

P

π

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

wj
(cid:12)
(cid:12)
wj
x

the sampling of the sources coefﬁcients with a thresholding procedure is then:

wj
s

(zj = L) = 0

(cid:12)
(cid:12)
(zj = H) = N

2. 


wj
s

(cid:12)
(cid:12)



wj
s

(cid:0)

0, τH
(cid:12)
(cid:12)

(cid:1)

We point out to the fact that we do not have to specify the threshold χ, which is a hard
task by itself, it is automatically set by the classiﬁcation of the coefﬁcients into Low
energy coefﬁcients and High energy coefﬁcients. This additional procedure allows to
have estimates free from any residual noise, as will be seen in the simulations, and the
whole algorithm could be described as separation/denoising algorithm.

The non linear step has been applied to the same data set, the estimation results are

presented in ﬁgure 4 and the estimated mixing matrix is:

ˆA =

.88 .55
.47 .83 (cid:21)

(cid:20)

and the observation distance δA = .054.
The algorithm has been tested on 1D signals and the results presented in ﬁgure 6 show
the effect of the non linear MCMC implementation on denoising the estimates.

SUMMARY AND PERSPECTIVES

In this work we presented a Bayesian aproach to blind source separation in the wavelet
domain. The main interest to try to solve the problem in the wavelet domain is to be
able to use a simpler probabilistic model for the sources i.e. a two component Gaussian
mixture model with a total of three parameters as opposed to a 3L − 1 parametric model
in the direct model with L undetermined. Indeed, the interpretation of the mixture model
as a heirarchical hidden variable model gives us the ability to apply some automatic
thresholding rule to the wavelet coefﬁcients. Finally, we showed some performances of
the proposed method on simulated data.

Concerning our perspectives, we follow essentially these directions:

i) a quad tree Markovian modeling of the wavelet coefﬁcients to account for inter-

scale correlation.

ii) an adaptative basis selection criteria to improve the thresholding procedure.

a.

c.

300

200

100

0

0

400

200

0

2000

1500

1000

500

0
−200

500

1000

−200

0

200

600

400

200

0

600

400

200

0

−200

0

200

−200

0

200

0
−200

−100

0

100

200

−100

0

100

200

−50

0

50

100

b.

2500

2000

1500

1000

500

2500

2000

1500

1000

500

0
−100

d.

FIGURE 1. Sparsity property of the wavelet coefﬁcients: a. aerial image, b. histogramme of image (a),
c. the wavelet transform of image (a), d. histograms of the wavelet coefﬁcients in the different bands (c)

a.

b.

c.

δ = .30

δ = .26

FIGURE 2.
estimated sources (MCMC algorithm)

a. original (256 × 256 pixels) sources, b. linearily mixed and noisy observations, c.

REFERENCES

1. Adel Belouchrani, Karim Abed-Meraim, Jean-François Cardoso, and Eric Moulines. A blind
source separation technique using second-order statistics. IEEE Transactions on Signal Processing,
45(2):434–444, February 1997.
Jean François Cardoso. Higher-order contrasts for independant component analysis.
Computation, MIT Letters, pages 157–192. 1999.
Pierre Comon. Independant component analysis, a new concept ? Signal Processing, 36(3):287–314,
April 1994.

In Neural

2.

3.

a
a
a
a

11

12

21

22

1

0.75

0.5

0.25

0

0

50

100

150

200

FIGURE 3.

evolution of the estimation of the elements of the matrix A during the iterations

δ = .31

δ = .19

FIGURE 4.

estimated sources (non linear MCMC algorithm)

a.

b.

c.

FIGURE 5.

scattering plots of : a. originals sources, b. mixed data, c. estimated sources.

4. Matthiew S. Crouse, Robert D. Nowak, and Richard G. Baraniuk. Wavelet-based statistical signal
processing using hidden Markov models. IEEE Trans. Signal Processing, 46(4), April 1998.
5. M. Gaeta and J.-L Lacoume. Source separation without prior knowledge: the maximum likelihood

solution. In Proc. EUSIPO, pages 621–624. Spinger Verlag, 1990.

6. A. Hyvärinen, J. Karhunen, and E. Oja. Independent Component Analysis. John Wiley, New York,

2001.

a.

b.

c.

d.

δ = .21

δ = .15

δ = .15

δ = .13

FIGURE 6.
(SNR≈12.75dB), c. estimated sources without thresholding, d. estimated sources with thresholding.

simulation results on time series signals: a. originals sources, b. mixed data

7. Mahieddine M. Ichir and Ali Mohammad-Djafari. Séparation de sources modélisées par des on-

delettes. In Actes 19e coll. GRETSI, Paris, France, September 2003.

8. K. Knuth. A Bayesian approach to source separation. In Proceedings of Independent Component

9.

Analysis Workshop, pages 283–288, 1999.
Stephane G. Mallat. A theory of multiresolution signal decomposition: The wavelet representation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(7):674–693, July 1989.
10. A. Mohammad-Djafari and Mahieddine M. Ichir. Wavelet domain image separation. In Proceedings
of American Institute of Physics, MaxEnt2002, pages 208–223, Moscow, Idaho, USA, Aug. 2002.
International Workshop on Bayesian and Maximum Entropy methods.
11. Ali Mohammad-Djafari. A Bayesian approach to source separation.

In J.T. Rychert G. Erikson
and C.R. Smith, editors, Bayesian Inference and Maximum Entropy Methods, Boise, IH, July 1999.
MaxEnt Workshops, Amer. Inst. Physics.

12. Carlos C. Rodriguez. A geometric theory of ignorance. Bayesian Inference and Maximum Entropy

Methods. MaxEnt, August 2003.

California, Riverside, 1998.

13. D. Rowe. Correlated Bayesian Factor analysis. PhD thesis, Departement of Statistics, Univ. of

14. H. Snoussi, G. Patanchon, J.F. Macías-Pérez, A. Mohammad-Djafari, and J. Delabrouille. Bayesian
blind component separation for cosmic microwave background observations. In Robert L. Fry, editor,
Bayesian Inference and Maximum Entropy Methods, pages 125–140. MaxEnt Workshops, Amer.
Inst. Physics, August 2001.

15. Hichem Snoussi and Ali Mohammad-Djafari. Bayesian unsupervised learning for source separation
with mixture of gaussians prior. To appear in Int. Journal of VLSI Signal Processing Systems, 2002.
16. Hichem Snoussi and Ali Mohammad-Djafari. Information Geometry and Prior Selection. In C.J.
Williams, editor, Bayesian Inference and Maximum Entropy Methods, pages 307–327. MaxEnt
Workshops, Amer. Inst. Physics, August 2002.

17. Hichem Snoussi and Ali Mohammad-Djafari. MCMC Joint Separation and Segmentation of Hidden
In Neural Networks for Signal Processing XII, pages 485–494. IEEE workshop,

Markov Fields.
September 2002.


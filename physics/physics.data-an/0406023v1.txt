4
0
0
2
 
n
u
J
 
6
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
3
2
0
6
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

MAXIMUM ENTROPY MULTIVARIATE DENSITY ESTIMATION:
AN EXACT GOODNESS-OF-FIT APPROACH

SABBIR RAHMAN∗ & MAHBUB MAJUMDAR†

Theoretical Physics, Blackett Laboratory
Imperial College of Science, Technology & Medicine
Prince Consort Road, London SW7 2BZ, U.K.

ABSTRACT

We consider the problem of estimating the population probability distribution
given a ﬁnite set of multivariate samples, using the maximum entropy approach.
In strict keeping with Jaynes’ original deﬁnition, our formulation of the prob-
lem considers contributions only from the smoothness of the estimated distri-
bution (as measured by the entropy) and the loss functional associated with its
goodness-of-ﬁt to the sample data, and in particular does not make use of any
additional constraints that cannot be justiﬁed from the sample data alone. By
mapping the general multivariate problem to a tractable univariate one, we are
able to write down an exact expression for the goodness-of-ﬁt of an arbitrary
multivariate distribution to any given set of samples, thus solving a long-standing
problem. A single tunable parameter remains which parametrises all solutions
ranging from the maximally smooth uniform distribution at one extreme to the
best-ﬁtting distribution given by a sum of delta functions localised at the sample
points at the other. As a corollary we also give an exact solution to the ‘for-
ward problem’ of determining the expected distributions of samples taken from
a population with known probability distribution.

1. Introduction

According to Jaynes1, the maximum entropy distribution is “uniquely determined
as the one which is maximally noncommittal with regard to missing information, in
that it agrees with what is known, but expresses maximum uncertainty with respect
to all other matters”2.

On the other hand, Kapur and Kesavan3 state that “the maximum entropy distri-
bution is the most unbiased distribution that agrees with given moment constraints
because any deviation from maximum entropy will imply a bias”.

While the latter neatly encapsulates the modern interpretation of the maximum
entropy principle in its application to density estimation, it is not equivalent to the

∗E-mail: sarahman@alum.mit.edu
†E-mail: m.majumdar@imperial.ac.uk

deﬁnition given by Jaynes as it restricts its use to the case where the moments of the
population distribution are already known.

While this restriction may be convenient, it is simply not valid in any case in which
one is not simply trying to re-derive a standard distribution based upon its known
moments using maximum entropy principles. Rather, in practical applications the
moments of the population distribution are not (and indeed cannot) be known a
priori, and certainly cannot be determined on the basis of a ﬁnite number of samples.
In this paper, we give an explicit expression of the maximum entropy density
estimation problem in a form which is strictly in keeping with Jaynes’ original (and
precise) deﬁnition.

2. Reformulating the MaxEnt Problem

So let us return to basics and consider the problem of estimating the multivariate
population density distribution given a ﬁnite set of samples taken at random from
the population, assuming that the raw sample data is the only prior information we
have.
In this case, which is clearly of the most general practical applicability, the
requirement that the maximum entropy distribution ‘agrees with what is known’ is
equivalent to the requirement that the population distribution provides a good ﬁt to
the sample data. In this sense the maximum entropy distribution can be deﬁned as
“the distribution of maximum entropy subject to the provision of a good ﬁt to the
sample data”, with the only apparent uncertainty lying in the relative importance
which should be attached to each of the two contributions. While this uncertainty re-
ﬂects the supposed ill-posedness of the density estimation problem, Jaynes’ deﬁnition
implies that there should in fact exist a unique solution, so that even this uncertainty
is in principle resolvable. Although we do not attempt to clarify this issue here, the
matter certainly deserves further attention.

The deﬁnition given in the last paragraph allows us to formulate the maximum
entropy multivariate density estimation problem in precise mathematical terms. If
we denote the estimated distribution by f (r) where r ∈ RD, and the sample data set
by {x1, . . . , xN }, we would like to maximise the functional deﬁned by,

F [f (r)] = S[f (r)] + αG[f (r), {xi}] ,

(1)

R

where S[f (r)] = −
f (r) log f (r)dτ is just the (sample-independent) entropy of the
estimated distribution and G[f (r), {xi}] is the measure of the goodness-of-ﬁt of the
distribution to the sample data. There is also a tunable variable α ∈ [0, ∞] which
parametrises the solutions. It is clear by inspection that α = 0 implies the maximum
entropy solution represented by the uniform distribution f (r) = constant, while the
limit α → ∞ corresponds (independently of the precise speciﬁcation of G) to the
distribution given by the normalised sum f (r) = 1
N
i=1 δ(r − xi) of delta-functions
N
localised at the sample points. The solution for any other value of α will represent

P

some trade-oﬀ between maximising entropy and maximising the goodness-of-ﬁt. The
fact that neither of the two extremal solutions would be of use in practical applications
does suggest that there should exist a (perhaps problem-dependent) optimal value for
α, and hence a unique optimal density estimate.

3. Establishing the Goodness-of-Fit

We have yet to give the expression for the goodness-of-ﬁt G[f (r), {xi}]. In the
absence of an analytically rigorous and generally applicable measure of goodness-of-
ﬁt, various ad hoc schemes have been used in the past4,5. As we will now show,
there exists a unique analytical expression for the goodness-of-ﬁt of an arbitrary
multivariate probability distribution f (r) to a given set of sample data {x1, . . . , xN }.

3.1. Mapping Multivariate Estimation to a Univariate Problem

In particular, there exists a well-deﬁned procedure to map this complex multivari-
ate problem into a tractable, and in fact quite simple, univariate one. To proceed,
one needs to note that the probability of a sample taking values in a particular region
of RD is given by the area (or more generally the hypervolume) under the curve f (r)
over that region. Moreover we know that for a probability distribution, the total area
under the curve is normalised to unity.

The key step is to deﬁne a mapping Cf : RD → I (representing a particular kind
of cumulative probability density function corresponding to f (r)) from RD onto the
real line segment I = [0, 1] as follows,

Cf (y) =

f (r)Θ[f (y) − f (r)]dτ ,

Z

(2)

where y ∈ RD and Θ(x) is the Heaviside step function with Θ(x) = 1 for x ≥ 0
and Θ(x) = 0 otherwise. The mapping Cf will in general be many-to-one. Its utility
lies in the fact that if we take the set of samples {x1, . . . , xN } in RD and map them
to the the set of points {Cf (x1), . . . , Cf (xN )} on the segment I then, in view of the
equivalence between the probability and the area under the curve, the goodness-of-ﬁt
of f (r) to the samples {xi} is precisely equal to the goodness-of-ﬁt of the uniform
probability distribution g(x) = 1 deﬁned on the segment I to the mapped samples
{Cf (xi)}. We now derive an exact measure of the goodness-of-ﬁt in the latter case.

3.2. Uniformly Distributed Samples on a Real Line Segment

Consider a perfect random number generator which generates values uniformly
distributed in the range [0, 1]. Suppose we plan to use it to generate N random
samples. We can calculate in advance the probability distribution pN,i(x) of the i-th
sample (where the samples are labelled in order of increasing magnitude), as follows.

Let Xi be the random variable corresponding to the value of the i-th sample for
each i = 1 . . . N. Note that the probability of a number (selected at random from
[0, 1] assuming a uniform distribution) being less than some value x ∈ [0, 1] is simply
x, while the probability of it being greater than x is 1 − x. Thus, if we consider
the i-th value in a set of N samples taken at random, the probability that Xi takes
the value x is given by the product of the probability xi−1 that i − 1 of the values
are less than x and the probability (1 − x)N −i that the remaining N − i values are
greater than x, divided by a combinatorial factor ZN,i counting the number of ways
N integers can be partitioned into three sets of size i − 1, 1 and N − i respectively,

pN,i(x) ≡ P (Xi = x) = Z −1

N,i xi−1(1 − x)N −i .

(3)

¿From simple combinatorics, the value of ZN,i is given by,

ZN,i =

N!
(i − 1)!(N − i)!

=

Γ(N + 1)
Γ(i)Γ(N − i + 1)

= B−1(N − i + 1, i) ,

(4)

where B(p, q) is the Euler beta function which appears in the Veneziano amplitude
for string scattering6. That this value is correct can be checked using the fact that
pN,i(x) must be normalised so that
pN,i(x)dx = 1, and noting that the resulting
integral is just the deﬁnition of the beta function given above.

R

An expression for the goodness-of-ﬁt of the uniform distribution to a set of N
samples in [0, 1] can therefore be obtained by ﬁrst labelling the samples in order of
increasing magnitude and then calculating the likelihood given by,

L[{xi}] =

pN,i(xi) .

(5)

3.3. Generalisation to the Multivariate Case

Bearing in mind the mapping Cf : RD → I deﬁned in (2), we can generalise the
above to derive the exact expression of the goodness of ﬁt of an arbitrary multivariate
probability distribution f (r) to a set of N samples {x1, . . . , xN },

G[f (r), {xi}] =

pN,i(Cf (xi)) .

(6)

Yi=1
where the samples are now labelled in order of increasing magnitude of f (xi) and
hence Cf (xi).

The maximum entropy density estimate associated with the sample data is then
given by the distribution which maximises the functional obtained on substituting (6)
into (1). The parameter α ∈ [0, ∞] can be tuned as appropriate to the speciﬁc problem
under investigation, bearing in mind that a small value will emphasise the smoothness

N

Yi=1

N

of the resulting distribution, while a larger value will emphasise the goodness of ﬁt.
Algorithms implementing the optimisation procedure are under development and we
hope to present the results in a future paper.

3.4. A Corollary: The Forward Problem

Before ending, it is worth mentioning here as a corollary that the distributions
pN,i(x) of (3) also help us to solve the ‘forward problem’, i.e. that of determining
the expected distributions pf
N,i(r) of any set of N samples taken at random from a
multivariate population where the population density distribution f (r) is given. In
particular, we can apply the mapping Cf (paying careful attention to the degeneracies
present) to obtain the following expected distributions for the samples (ordered as
described below Eqn.(6)),

(7)

N,i(r) = J −1(r) pN,i(Cf (r)) ,
pf
where J(r) measures the (typically (D − 1)-dimensional) volume of the degeneracy
of f (r) (i.e. the volume of the subspace of RD sharing the same value of f (r)) for
each value of r. At special values the region of degeneracy may have dimensionality
less than (D − 1) in which case the value of pf
N,i(r) becomes irrelevant and can
safely be ignored or set to zero if desired. On the other hand for distributions which
contain D-dimensional subspaces throughout which f (r) is constant (the uniform
distribution being an obvious example), then special considerations will be required in
order to generalise the analysis leading to Eqn.(3) for the real line segment to irregular,
multidimensional, and possibly non-compact spaces. Such an analysis promises to
be highly non-trivial and we will not attempt to delve into such intricacies here,
particularly as such complications are unlikely to arise in practical applications.

It is often assumed that the deviations of observations from their expected values
follow a normal distribution7 for univariate data, leading to a χ2 measure of goodness-
of-ﬁta. Our exact results suggest that this approximation only holds if N is suﬃciently
large and presumably only then if f (r) is suﬃciently well-behaved. We will conclude
our analysis at this point.

4. Summary and Discussion

The purpose of the present work has been to reformulate the maximum entropy
(MaxEnt) density estimation problem in a way which is strictly in keeping with its
original deﬁnition as introduced by Jaynes. The importance of having such a precise
formulation hardly needs mentioning given the ubiquity of the estimation problem
throughout the sciences.
aA discussion of the trade-oﬀ between smoothness and goodness-of-ﬁt in the context of this assump-
tion appears in Gull (1989)8.

In reaching our formulation we have found it necessary to solve the long-standing
problem of obtaining a precise and explicit measure of the goodness-of-ﬁt of a generic
multivariate distribution to a set of sample data, which itself has very broad applica-
bility, particularly in the experimental sciences. As a corollary, we were also able to
propose the solution to the ‘forward problem’ - that of determining the distribution
of the samples when the population distribution is known. A potential application of
the latter is in Monte Carlo simulations though there are doubtless many others.

A single tunable parameter remains in our expression of the MaxEnt problem
which parametrises solutions ranging from those with maximal smoothness to those
providing maximal ﬁt to the data. In principle this parameter should also be deter-
mined uniquely by maximum entropy considerations. Resolving this will presumably
require a clear understanding of the relationship between entropy and likelihood as
they arise in our formulation. We have not attempted to tackle this question here,
though we intend to come back to it in future once computational algorithms imple-
menting the optimisation have been developed.

5. Acknowledgements

We would like to thank Wajid Mannan and Mohamed Mekias for their valuable

suggestions during the preparation of this manuscript.

6. References

(1957) 620-630.

1. E. T. Jaynes, ‘Information theory and statistical mechanics’, Phys. Rev. 106

2. X. Wu, ‘Calculation of maximum entropy densities with application to income

distribution’, J. Econometrics, 115 (2003) 347-354.

3. J. N. Kapur and H. K. Kesavan, Entropy Optimization Principles with Applica-

4. R. B. D’Agostino and M. A. Stephens (eds.), Goodness-of-Fit Techniques, (Dekker,

tions, (Academic Press, 1992).

1986).

5. B. Aslan and G. Zech, ‘Comparison of diﬀerent goodness-of-ﬁt tests’, Proceedings
of Advanced Statistical Techniques in Particle Physics, University of Durham, 19-
22 March 2002.

6. M. B. Green, J. H. Schwarz and E. Witten, Superstring Theory, Vol. 1, (Cambridge

7. B. Buck and V. A. Macaulay, Maximum Entropy in Action, (Oxford University

University Press, 1987).

Press, 1991).

8. S. F. Gull, ‘Developments in maximum entropy data analysis’. In Maximum en-
tropy and Bayesian methods, Cambridge, England 1988 (ed. J. Skilling), pp. 53-71
(Kluwer, Dordrecht, 1989).


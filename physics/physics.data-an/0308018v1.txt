3
0
0
2
 
g
u
A
 
5
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
1
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A generic scheme for choosing models and characterizations of complex systems

A. G. Rossberg∗
Zentrum f¨ur Datenanalyse und Modellbindung, Universit¨at Freiburg, Eckerstr. 1, 79104 Freiburg, Germany
(Dated: August 5, 2003)

It is argued that the two problems of choosing characterizations and models of complex systems
should not be considered independently. A particular criterion for these choices, oriented on the
potential usefulness of the results, is considered, and a generic formalization applicable to realistic
experiments is developed. It is applied to Kuramoto-Sivashinsky chaos.

PACS numbers: 05.65.+b, 05.45.-a, 07.05.Tp, 01.70.+Bw

The

systematic

characterization of

self-organized
structures is a long-standing challange to the science
‘Labyrinths’, ‘breathers’, ‘den-
of structure formation.
drites’, ‘worms’, ‘spiral-defect chaos’, or ‘scale-free net-
works’ are only few of the words that were introduced to
describe real and numerical experimental observations.
Images and natural language can usually communicate
what is ment, but as the number of observed structures
increases and distinctions become ﬁner, a more system-
atic approach seems desirable. The problem is felt par-
ticularly strong for the large variety of spatially irregular
structures and spatio-temporally chaotic states that have
been found [1].

In search for appropriate characterizations researchers
do often concentrate on those properties of the experi-
mental data that are easily modeled – those properties
of the data or the underlying structures that are gov-
erned by their own rules (one might call them “coherent
In this case, the choice of the character-
structures”).
ization depends on the available models. On the other
hand, only when a particular set of properties of exper-
imental data has been found to be characteristic for an
observed structure, one can meaningfully ask for a model
that reproduces this structure, i.e., a model that repro-
duces data with these properties. Modeling requires prior
characterization.

Intuition is the fallback most researchers rely on when
facing this circular relationship of modeling and charac-
terizing. In fact, intuition is an excellent guide. But for
some problem areas, e.g., those involving spatio-temporal
chaos, progress appears to have slowed down also due to a
lack of intuition about what the characteristic properties
and what appropriate models are. Even when intuition
is successful in choosing models and characterizations, it
is legitimate to ask if these choices are subjective in the
sense that they depend essentially on the way humans
observe the world (other beings might decide very diﬀer-
ently), or if they are the solution of some objective prob-
lem, that our intuition is just highly eﬃcient in solving.
Most of the approaches to the related problem of emer-
gence (e.g., [2, 3]) are based on the a priori assumption
of some limitation to observation (coarse graining), thus
involving an “inherently subjective” [2] component. For
an argument in favor of the objectivity of the choices it

is therefore important to formulate a criterion that does
not depend on such artiﬁcial limitations.

Here, a proposal for such a criterion is introduced.
It is ﬁrst stated on a heuristic level and then modeled
in a mathematical language; thus modeling the prob-
lem of modeling. This involves the combination of con-
cepts from computer science that proved powerful in the
context of structure formation – algorithmic complex-
ity (program length) [4, 5] and computational complex-
ity (execution time) [6] – with ideas from statistical test
theory [7]. It is shown that the circular relation between
models and characterizations is, in this case, not vicious:
the criterion leads to nontrivial choices. As an example,
the formalism is applied to the spatio-temporally chaotic
solutions of the Kuramoto-Sivashinsky Equation.

Consider the following requirements for models and

characterizations:

Characterizations should be easily communi-
cated and veriﬁed, be speciﬁc, and should,
over a wide control-parameter range, apply
to experimental data and be reproducible in
models.
Models should be easily communicated and
easily evaluated, show little artifacts, and re-
produce given characterizations.

(1)

(2)

The practical relevance of most of these requirements is
obvious. To see why it is desirable that characteriza-
tions are reproducible in models, notice that, from such
models, larger, composed models could be constructed,
that can then be used to explore and characterize situa-
tions not accessible experimentally (e.g., climate models).
Even though the existence of models of sub-systems that
reproduce the properties relevant for the composed model
is not guaranteed, in case that they exist, it is good to
know them. Now, as the general criterion, choose those
pairs of models and characterizations that jointly statisfy
conditions (1) and (2) as well as possible.

In order to formalize this criterion and make it acces-
sible to a rigorous analysis, both characterizations and
models are represented by computer programs: programs
that test data for particular properties, and programs
that generate data. The practical use of these programs
is illustrated in Fig. 1.

(a)

Control

Parameter

x ∈ C

rerun

Test

t

(cid:1)
(cid:0)
(cid:0)
(cid:1)

x ∈ C

y ∈ D

D/A

Experiment

A/D

(b) Control

x ∈ C

Parameter

x ∈ C

Model

g

rerun

Test

t

y ∈ D

FIG. 1: (a) Generic setup of a computer-controlled experi-
ment. (b) Data ﬂow in a test of a computational model.

Figure 1a shows a generic setup for a computer-
controlled experiment. The experimenter enters some
control parameter values at a console. The set of control
parameter values is encoded in a binary string x ∈ C,
where the control parameter format C is a subset of the
set {0, 1}n of all binary strings of length n for some
n ∈ N0. Based on these control parameter values, the
control parameters of the experiment are adjusted, usu-
ally via some D/A conversion.

When the experiment is run, experimental data is
recorded in binary form. Data is encoded in a binary
string y ∈ D, where the data format D is a subset of
the set {0, 1}m of all binary strings of length m for some
m ∈ N0. This could, for example, be image data or a time
series. Generally, y is a realization of a random variable
Y with values in D. The experiment is assumed repro-
ducible in the sense that repeated runs of the experiment
yield a sequence Y1, Y2, . . . of statistically independent,
identically distributed (i.i.d.) results.

A characterization is represented by a program t that
computes a statistical test on experimental data: A
test takes a control parameter x ∈ C as input, runs,
and then either halts with output e or requests a ﬁ-
nite number of (re)runs of the experiment and then halts
with an output of 1 or 0. By the output e the tests
t indicates that x is not within its range of validity
C[t] := {x ∈ C|output of t with input x is not e} [10].
The outputs 1 or 0 indicate that the null hypothesis (see
below) is accepted or rejected by the test, respectively.
When the test requests an experimental rerun, its ex-
ecution is suspended until the experimental result y is
written into a dedicated storage accessible by the test.

A model is represented by a computer program g that
generates data to be used in place of experimental data
(Fig. 1b): A generator takes a control parameter x ∈ C
as input, runs, outputs data y ∈ D and halts. In order

2

to produce random results, the program has access to a
source of independent, evenly distributed random bits.
Subsequent runs of a generator are fully independent.

As in conventional statistical test theory [7], the power
function is introduced. Denote by tx({yi}) the output
of the test t at control parameter x ∈ C[t] when applied
to the sequence of experimental results {yi} ∈ D∞ (for
formal simplicity, the sequences {yi} are assumed inﬁnite,
even though the tests use only ﬁnite subsequences). Let
{Yi} be a sequence of i.i.d. random results with values
in D∞. Deﬁne the power of the test function tx when
applied to {Yi} as the probability to reject {Yi}, i.e.,

pow(tx, {Yi}) := Pr[tx({Yi}) = 0]

(x ∈ C[t]).

(3)

Unlike in conventional test theory, there is no indepen-
dent null hypothesis H0 here that states the distribution
or the class of distributions of {Yi} that is tested for. In-
stead, given a test function tx, the null hypothesis, i.e.,
the class of distributions, is deﬁned by the condition

pow(tx, {Yi}) ≤ α,

(4)

where 0 < α < 1 is a ﬁxed signiﬁcance level [7] [11].

The ease or diﬃculty of communicating a test t or
model g, mentioned in requirements (1,2), is measured
by the lengths L(t), L(g) of the programs t and g. The
value of L(·) depends on the machine model. In the ex-
ample below, MMIX, an idealized modern microproces-
sor is used [8].

The ease or diﬃculty of verifying characterizations and
evaluating models is measured by the execution times
T (g), T (t) of the programs. To be speciﬁc, deﬁne T (·)
as the maximum of the expectation value of the run-
time over all x ∈ C and all distributions of data. Below,
time is measured by the number of “oops” (symbol: 1 υ)
counted by the MMIX emulation mmix-sim [8].

The often-encountered tradeoﬀ between L and T is
taken into account by assuming that there is a cost func-
tion depending on both resources, which increases strictly
monotonically with L at ﬁxed T and with T at ﬁxed L
but is otherwise unspeciﬁed. With this in mind, deﬁne
the relations (cid:22) (always cheaper or equal ) and ≺ (always
cheaper ) for programs p1, p2 by

p1 (cid:22) p2

def⇔ L(p1) ≤ L(p2) and T (p1) ≤ T (p2)

(5)

and

p1 ≺ p2

def⇔ p1 (cid:22) p2 and not p2 (cid:22) p1.

(6)

It turns out that the machine dependence of relations
(cid:22) and ≺ for implementations of algorithms on diﬀerent
processor models is weak. In principle, other resources
could also be taken into account in deﬁnition (5) such as,
for tests, the number of experimental runs required.

Since for every program p there is only a ﬁnite number
of programs with smaller or equal length, there is also

only a ﬁnite number of programs p′ such that p′ ≺ p or
p′ (cid:22) p. Below we need Lemma 1: Every nonempty set
P of tests or generators contains an element p which is
minimal with respect to the relation ≺, i.e., such that no
p′ ∈ P satisﬁes p′ ≺ p. This is a direct consequence of
the previous note and the transitivity and antireﬂexivity
of ≺. In general, there are several minimal elements, each
using its own mix of resources. This reﬂects the intuition
that there are several “good” models and characteriza-
tions for one experiment.

These concepts from statistics and computer science
are now combined to formalize requirement (2), except
for the condition regarding artifacts. Denote by gx the
sequence {Yi} of random outputs of generator g at con-
trol parameter x. Deﬁne for given C, D the notion of
an optimal generator g relative to a test t and a power
threshold 1 > γ > α by

optγ

t g def⇔ ^

pow(tx, gx) ≤ α and

(7a)

(7b)

pow(tx, g′

x) > γ and

pow(tx, g′

x) ≥ pow(tx, gx),

(7c)

x∈C[t]

^
g′≺g

_
x∈C[t]

^
g′(cid:22)g

_
x∈C[t]

where the quantiﬁers V (for all ) and W (there is) have
been introduced for brevity. Line (7a) states that g sat-
isﬁes t, line (7b) says that all cheaper generators are re-
jected by t with power > γ and line (7c) handles the
generators that use the same resources as g. The test t
is speciﬁc to g in the sense that it does not apply to any
g′ ≺ g.

In order to disentangle the circularity between models
and characterizations, consider now the problem of speci-
fying a generator by characterizing its output. For a i.i.d.
random sequence {Yi} denote by p[{Yi}] the distribution
function of its elements, i.e., p[{Yi}](y) := Pr[Y1 = y]
for y ∈ D. Call a generator g an optimal implementation
with respect to a set ˜C ⊂ C iﬀ there is no generator g′ ≺ g
x] ≡ p[gx] for all x ∈ ˜C. Theorem 1 : Given
such that p[g′
C and D, there is for every ˜C ⊂ C, every optimal im-
plementation g with respect to ˜C, and every 1 > γ > α,
a test t such that optγ
t g and C[t] = ˜C. Outline of the
proof: Explicitly construct t. x ∈ ˜C can be tested for by
keeping a list of ˜C in t. Since there is only a ﬁnite num-
ber of g′ (cid:22) g, the test must distinguish p[gx] from a ﬁnite
x] for all x ∈ ˜C, with
number of diﬀerent distributions p[g′
certainty γ if g′ ≺ g. This can be achieved by comparing
a suﬃciently accurate representation of p[gx], stored in t
for all x ∈ ˜C, with a histogram sampled from g′
x. With a
high number of samples, any degree of certainty can be
reached.(cid:3) The cost of testing is not taken into account,
yet.

The following deﬁnition formalizes the criterion stated
above for choosing models and characterizations; to ﬁnd

3

pairs (t, g) jointly satisfying conditions (1,2) as well as
possible. Only the validity of characterizations for ex-
periments is not contained in the deﬁnition: Given C
and D, call a pair (t, g) a basic model specifying charac-
terization (b.m.s.c.) iﬀ there is a 1 > γ > α such that
optγ
t g and there is no t′ ≺ t with C[t] ⊂ C[t′] and optγ
t′g.
This optimization with respect to t implies the avoid-
ance of artifacts, when artifacts are considered as proper-
ties that are speciﬁc and are cheaper to communicate and
verify than the property t that g is supposed to model.
The deﬁnition of a b.m.s.c. involves the simultaneous
minimization of cost with respect to t and g. An an-
swer to the question if there are any nontrivial solutions
to this double optimization problem – i.e., if the circular
relation between models and characterizations as consid-
ered here is vicious – is given by Theorem 2 : Given C
and D, there is, for every ˜C ⊂ C and every optimal im-
plementation g with respect to ˜C, a test t such that (t, g)
is a b.m.s.c. and ˜C ⊂ C[t]. Proof: Fix some 1 > γ > α.
By Theorem 1, the set S := {t|optγ
t g and ˜C ⊂ C[t]} is
nonempty. Theorem 2 is satisﬁed by any t ∈ S which is
minimal with respect to the half ordering ≺. By Lemma 1
such an element exists.(cid:3)

Only for a few b.m.s.c. (t, g) the test t also applies
to a given experiment. Generally, there will be some
fundamental level of description (the Schr¨odinger equa-
tion, say) at which a 1-to-1 model g of the experiment
can be constructed, and then a corresponding t exists by
Theorem 2. But these b.m.s.c. are often too expensive.
Finding cheaper b.m.s.c. that apply to the data requires
intuition, insight, and experience, and goes beyond the
scope of this work. The goal here was only to investigate
if an objective, well-posed problem of modeling and char-
acterizing exists, and to model it so that among several
solutions conceived some are selected.

As an example for an application, assume some ideal-
ized experiment, the fundamental description of which is
given by the Kuramoto-Sivashinsky (KS) equation

∂τ u = −∂2

ξ u − ∂4

ξ u + u∂ξu,

(8)

with periodic boundary conditions u(τ, ξ) = u(τ, ξ + Ξ),
as they apply for experiments in a ring-channel geometry.
In each experimental run, 128 equally spaced points of
u at distance ∆ξ = 0.8 (Ξ = 128 × ∆ξ) are sampled
200 times in ∆τ = 0.2 sampling intervals, while u is
evolving along the chaotic KS attractor. The data format
D is given by all sequences of 128 × 200 = 25600 8-byte
ﬂoating point numbers. There is no control parameter:
x is always the empty string and the only element of C.
The systematic construction of a pair (t, g) likely to be
a b.m.s.c. of the experiment goes from the generator g
over a corresponding test t to a veriﬁcation that the ex-
periment passes the test t. Practically ﬁnding a suitable
g requires a preliminary approximation of t characteriz-
ing the experiment. This ﬁrst, exploratory step is not
described here.

a)

b)

c)

τ

ξ

FIG. 2: Modeling and characterization of KS chaos. (a) Gray
coding of the output y of model g that approximates Eq. (8).
(b) Regions where ∂u(τ, ξ)/∂ξ > 0 (black) as used in the
“tree-test” t. (c) A precise solution of Eq. (8) for comparison.

The code for the generator g is a minimal-length im-
plementation of Eq. (8) on an MMIX processor. A dis-
cretization up,q locally approximately proportional to a
solution u(10 p ∆τ, q ∆x) of Eq. (8) is obtained by an Eu-
ler integration with in-place update of the form

up+1,q = c1 (up,q+2 + up+1,q−2) + c2 (up,q+1 + up+1,q−1)
(9)

+ (c3 + up,q+1 − up+1,q−1) up,q,

where (c1, c2, c3) ≈ (−0.05, 0.18, 0.75).
Including code
to handel the periodic boundaries, to initialize u0,ξ with
random numbers O(10−2), to drop a transient of 16 time
units, and to output y (Fig. 2a), this requires L(g) =
260 bytes and T (g) = 34 Mυ for a single run.

For the test t, a code is used that implicitly computes
the stripes ∂ξu(τ, ξ) > 0 (Fig. 2b) using data of every
k-th sampling interval (k ≈ 5). Then it determines for
each of N = 20 runs the total numbers of beginnings nb,
ends ne, mergers nm, and splits ns of the stripes along
the time axis, as well as the average number l of stripes.
The value of N implicitly determines δ.

If a combination (nb, ne, nm, ns, l) is repeated for two
runs, the test rejects the data stream in order to en-
force randomness. The averages nb, ne, nm, ns, l of these
statistics over all N runs are determined. The data is
rejected if ne > me ≈ 15.2 or ns > ms ≈ 0.4, which
enforces the tree-like geometry of the stripes and con-
sequently a minimal accuracy of g. Data is rejected if
(nl − ml)2 > vl or (nb − mb)2 > vb, which sets the length
and time scales of the tree structure [(ml, vl, mb, vb) ≈
(14., 0.03, 22., 1.5)]. Finally, data is rejected if the dif-
ference between the initial and ﬁnal number of stripes is
large, i.e., (ne +nm −nb −ns)2 > va ≈ 1.7, which enforces
the suppression of a transient in the generator. Within
the statistical error, t accepts g at the α = 0.1 signiﬁcance
level: pow(tx, gx) = 0.105(3) . α. Using precise numer-
ical simulations of Eq. (8), it was veriﬁed that solutions
of the fundamental description (Fig. 2c) are rejected by
t with a probability of only 0.03(1) < α. That is, t char-
acterizes the “experimental” data and is even robust to
small deviations from the fundamental description (8).
A compiler-optimized implementation [9] of t requires
L(t) = 1192 bytes and T (t) = 3.8 Mυ = N × 0.19 Mυ.

In principle, the precise values of the tuning parame-
ters in t could be determined by locally solving the opti-

4

mization problem for the condition for the pair (t, g) to
be a b.m.s.c. to the precision of the coding of the param-
eters. Regarding the question if this pair is also a global
solution of the optimization problem for a b.m.s.c., it
can only be said that this is a plausible conjecture. It
has been checked that the direct veriﬁcation of Eq. (9)
would yield a test that is shorter than the tree-test t,
but requires much more time. Likewise, generators more
explicitly coded to generate tree structures accepted by
t could be faster than g, but the examples investigated
indicate that, due to several conditional branches, they
would always be longer. Thus, no counterexamples could
be found. Notice that the information reduction per-
formed by t in concentrating on the stripes ∂ξu > 0 is
not externally imposed. Rather, it is the a consequence
of the rather small number of competing generators to
be excluded.

To the degree that the pair (t, g) described here it is a
b.m.s.c., it is also of practical relevance. The tree-test t
provides a fast, rather simple, and robust way to identify
KS chaos. There seems to be no other simple “expla-
nation” for the structure identiﬁed by t. On the other
hand, g provides a simple and, as it turns out, compera-
tively fast method to obtain approximations of KS chaos
on digital computers, which is important whenever re-
sources are scarce.

A formal scheme combining computation and statistics
for choosing models and characterizations has been laid
out. It models the main aspects of the practical problem.
The question if the choices are “intuitive” is presumably
hard to answer systematically. At least, it has been ar-
gued, they are useful: not because nature is a computer,
but because people use computers.

Work supported by the German BMBF (13N7955).

Electronic address: axel@rossberg.net

∗
[1] M. C. Cross and P. C. Hohenberg, Science 263, 1569

(1994).

[2] J. P. Crutchﬁeld, Physica D 75, 11 (1994).
[3] N. A. Baas, in Artiﬁcial Life III, edited by C. G. Langton

(Addison-Wesley, 1994), pp. 515–537.
[4] J. Machta, J. Stat. Phys. 70, 949 (1993).
[5] C. Moore and J. Machta, J. Stat. Phys. 99, 661 (2000).
[6] T. G. Dewey, Fractals 5, 697 (1997).
[7] E. L. Lehmann, Testing Statistical Hypotheses (Springer,

Heidelberg, 1997), 2nd ed.

[8] D. E. Knuth, MMIXware: A RISC Computer for the
Third Millennium, no. 1750 in Lecture Notes in Com-
puter Science (Springer, Heidelberg, 1999).
GCC for MMIX,

[9] H.-P. Nilsson,

the ABI,

http://bitrange.com/mmix/mmixfest-2001/mmixabi.html
(2001).

[10] This suppresses simple characterizations that are valid

only in complex subsets of C.

[11] From tx, computable tests for the same H0 at other sig-

niﬁcance levels can be constructed.


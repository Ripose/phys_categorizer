7
9
9
1
 
c
e
D
 
8
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
1
4
0
2
1
7
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

CV-NP BAYESIANISM BY MCMC

Cross Validated Non Parametric Bayesianism by Markov Chain Monte Carlo

CARLOS C. RODRIGUEZ

Department of Mathematics and Statistics
University at Albany, SUNY
Albany NY 12222, USA†

Abstract. Completely automatic and adaptive non-parametric inference is a pie
in the sky. The frequentist approach, best exempliﬁed by the kernel estimators,
has excellent asymptotic characteristics but it is very sensitive to the choice of
smoothness parameters. On the other hand the Bayesian approach, best exem-
pliﬁed by the mixture of gaussians models, is optimal given the observed data
but it is very sensitive to the choice of prior. In 1984 the author proposed to use
the Cross-Validated gaussian kernel as the likelihood for the smoothness scale pa-
rameter h, and obtained a closed formula for the posterior mean of h based on
Jeﬀreys’s rule as the prior. The practical operational characteristics of this bayes’
rule for the smoothness parameter remained unknown for all these years due to the
combinatorial complexity of the formula. It is shown in this paper that a version
of the metropolis algorithm can be used to approximate the value of h producing
remarkably good completely automatic and adaptive kernel estimators. A close
study of the form of the cross validated likelihood suggests a modiﬁcation and a
new approach to Bayesian Non-parametrics in general.

Key words: Cross validation, Density estimation, Bayes method, Kernel estima-
tor

1. Introduction

A basic problem of statistical inference is to estimate the probability distribution
that generated the observed data. In order to allow the data to speak by themselves,
it is desirable to solve this problem with a minimum of a priori assumptions on
the class of possible solutions. For this reason, the last thirty years have been
burgeoning with interest in nonparametric estimation. The main positive results
are almost exclusively from non-Bayesian quarters, but due to recent advances in
Monte Carlo methods and computer hardware, Bayesian nonparametric estimators
are now becoming more competitive.

†Email: carlos@math.albany.edu

2

CARLOS C. RODRIGUEZ

This paper aims to extend the idea in [?] to a new general Bayesian approach
to nonparametric density estimation. As an illustration one of the techniques is
applied to compute the estimator in [?]. In particular it is shown how to approxi-
mate the bayes estimator (for Jeﬀreys’ prior and quadratic loss) for the bandwidth
parameter of a kernel using a version of the metropolis algorithm. The computer ex-
periments with simulated data clearly show that the bayes estimator with Jeﬀreys’
prior outperforms the standard estimator produced by plain Cross-validation.

2. Density Estimation by Summing Over Paths

Any non-Bayesian nonparametric density estimator can be turned into a Bayesian
one by the following three steps:

1. Transform the estimator into a likelihood for the smoothness parameters via

the sum over paths technique introduced below.
2. Put a reference prior on the smoothness parameters.
3. Use the predictive distribution as the Bayesian estimator obtainable by Markov

Chain Monte Carlo.

The summing over paths method, springs from the simple idea of interchanging
sums and products in the expression for the cross-validated kernel likelihood (see
[?]). Without further reference to its humble origins, let us postulate the following
sequence of functions,

Φn = Φ (x0, x1, x2, . . . , xn|

h)

∝

n

Xall paths

j=0
Y

Kh

xij

xj −
(cid:0)

(cid:1)

(1)

where h > 0, Kh(x) = 1
h K(x/h) with K a density symmetric about 0. We call a
vector of indices, (i0, . . . , in) with the property that ij ∈ {
, a path
(more speciﬁcally a general unrestricted path, see below). The sum above runs over
all the nn+1 possible paths. The functions, Φn are deﬁned up to a proportionality
constant independent of the xj’s.

0, . . . , n

} \ {

}

j

Notice that by ﬂipping the sum and the product we get

xij

= f

0,n(x0)f

1,n(x1)

f

n,n(xn)

(2)

−

−

· · ·

−

1
nn+1

n

Xall paths

j=0
Y

Kh

xj −
(cid:0)

where,

(cid:1)

1
n

n

f

j,n(xj) =

−

Kh

xj −
(cid:0)

Xij =0
=j
ij

xij

.

(cid:1)

(3)

−

Thus, f
j,n(xj ) is nothing but the kernel density estimator of f (xj) based on all
the data except the jth observation xj. Under mild regularity conditions the kernel
estimator is known to converge (in every conceivable way) provided that h = hn
is taken as a function of n such that, hn →

0 and nhn → ∞

.
→ ∞

as n

6
CV-NP BY MCMC

3

The Φn’s can be used as a universal method for attaching a class of exchange-
able one parameter models to any set of observations. The positive scalar param-
eter h is the only free parameter, and diﬀerent models are obtained by changing
the kernel function K.

These empirical parametric models are invariant under relabeling of the ob-
servations (i.e. they are exchangeable) but they do not model the observations as
independent variables. Rather, these models introduce a pattern of correlations for
which there is a priori no justiﬁcation. This suggest that there might be improve-
ments in performance if the sum is restricted to special subsets of the set of all
nn+1 paths. Three of these modiﬁcations are mentioned in the following section.
Notice also that the ability of the Φn to adapt comes at the expense of regu-
larity. These models are always non-regular. If the kernel has unbounded support
then Φn integrates to inﬁnity but the conditional distribution of x0 given x1, . . . , xn
and h is proper. When the kernel has compact support the Φn are proper but still
non-regular since their support now depends on h.

The above recipe would have been a capricious combination of symbols if it
not were for the fact that, under mild regularity conditions, these models adapt to
the form of the true likelihood as n increases.

As a function of x0 = x, the Φn have the following asymptotic property,
Theorem 1 If x1, x2, . . . , xn are iid observations from an unknown pdf f which
is continuous a.s. and h = hn is taken as a function of n such that, hn →
0 and
nhn → ∞

, then,

→ ∞

as n

Φn
Φndx0

−

= f

0,n(x) + o(1) = f (x) + o(1).

(4)

where the little o is taken in probability as n

R

→ ∞

Proof (sketch only)

Just ﬂip the sum and the product to get again,

1
nn+1

n

Xall paths

j=0
Y

Kh

xj −
(cid:0)

(cid:1)

xij

= f

0,n(x)f

1,n(x1)

f

n,n(xn)

(5)

−

−

· · ·

−

Under the simple regularity conditions of the theorem, the kernel estimator is
known to converge in probability as n
. However, even though x0 appears
in all of the n + 1 factors, and their number goes to inﬁnity, still all the factors
are converging to the value of the true density at the given point. Therefore the
theorem follows.✷

→ ∞

It is worth noticing that the above theorem is only one of a large number of
results that are readily available from the thirty years of literature on density
estimation. In fact under appropriate regularity conditions the convergence can be
strengthen to be pointwise a.s., uniformly a.s., or globally a.s. in L1, or L2.

3. Paths, Graphs and Loops

Each of the nn+1 paths (i0, . . . , in) can be represented by a graph with nodes
x0, . . . , xn and edges from xj to xk if and only if ij = k. Here are some graphs

4

CARLOS C. RODRIGUEZ

✎☞x2 ✲
✎☞x1
(cid:0)(cid:0)✒✍✌
✍✌
❅❅■
(cid:0)(cid:0)✠
✎☞x3
✍✌

✎☞x0
✍✌

Figure 1. The graph of (2, 3, 1, 2)

for paths with n = 3. For example, the path (2, 3, 1, 2) is given a probability
proportional to

Kh(x0 −

x2)Kh(x1 −

x3)Kh(x2 −

x1)Kh(x3 −

x2)

(6)

and represented by the graph in ﬁgure [1]. Let’s call it a 1-3-loop. The path
(1, 2, 3, 0) is the single ordered loop of size four (a 4-loop), (3, 0, 1, 2) is the same
loop backwards (also a 4-loop), (2, 3, 0, 1) are two disconnected loops (a 2-2-loop)
and (1, 0, 0, 0) is connected and contains a loop of size two with x0 and x1 in it (a
1-1-2-loop). Draw the pictures!

The classiﬁcation of paths in terms of number and size of loops appears natu-
rally when trying to understand how Φn distributes probability mass among the
diﬀerent paths. To be able to provide simple explicit formulas let us take K in the
deﬁnition of Φn to be the standard gaussian density, i.e. from now on we take,

K(x) =

exp

1
√2π

x2
−
2

.
(cid:19)

(cid:18)

The gaussian kernel has unbounded support and that makes the total integral of
each path to diverge. Thus, the partition function

Z =

Φndx0 . . . dxn

Z

=

n

Xall paths Z

j=0
Y

xij

dx0 . . . dxn

Kh

xj −
(cid:0)

(cid:1)

is the sum of inﬁnities and it also diverges. Recall that this anomaly is the price
we need to pay for using a model with a ﬁnite number of free parameters (only
one in this case) and hoping to still adapt to the form of the true likelihood as
n
we can still write (formally) a
useful decomposition that will help explain how the Φn’s adapt and how to modify
the set of paths to improve the convergence. We ﬁrst need the following simple

. Even though the value of Z is in fact

→ ∞

∞

(7)

(8)

(9)

5

(10)

(12)

CV-NP BY MCMC

property of gaussians,

Ka(x

y)Kb(y

z)dy = K√a2+b2 (x

z).

−

−

−

Z

This can be shown by straight forward integration after completing the square.
Now notice that whatever the value of the integrals appearing in equation (9) that
value only depends on the type of loop that is being integrated. For this reason we
omit the integrand and simply denote the value of the integral with the integral
sign and the type of loop. With this notation we have,

Theorem 2

Zm1−

m2−

...

mk

loop

 Zm1−

loop!  Zm2−

loop!

 Zmk

−

loop!

=

. . .

(11)

−

−
loop = 1 and for m > 1,

More over,

1

−

R

=

1
√2π

1

h−
√m

L

loop

Zm

−

where we write formally L =

dx.

Proof

R

Equation (11) follows from Fubini’s theorem. To get (12) use Fubini’s theorem and
apply (10) each time to obtain,

loop

Zm

−

Kh(x0 −

x1)Kh(x1 −

x2)Kh(x2 −

x3) . . . Kh(xm

x0)dx0dx1 . . . dxm

1

−

=

=

. . .

=

=

Z

Z

Z

Z

1 −

−

1

−

K√2h(x0 −

x2) . . . Kh(xm

x0)dx0dx2 . . . dxm

1 −

−

K√m

1h(x0 −

−

xm

1)Kh(xm

−

1 −

−

x0)dx0dxm

1

−

K√mh(0)dx0 =

1
√2π

1

h−
√m

L

✷
Hence, by splitting the sum over all paths into,

=

+

+ . . . +

Xall paths

X2-2...-2-loops

X1-3-2...-2-loops

X(n+1)-loops

and applying the previous theorem we obtain,

Z = N2

2...

2

−

−

(n+1)/2

1
√2π

1

h−
√2

L

(cid:19)

(cid:18)

+ . . . + Nn+1

1
√2π

1

h−
√n + 1

L

1

(cid:19)

(cid:18)

(13)

where for simplicity we have assumed that n is odd and we denote by Nm1−
the total number of m1 −

mk
loops. Using simple combinatorial arguments

mk −

. . .

−

...

−

6

CARLOS C. RODRIGUEZ

it is possible to write explicit formulas for the number of loops of each kind.
The important conclusion from the decomposition (13) is that even though the
Φn appear to be adding equally over all paths, in reality they end up allocating
almost all the probability mass on paths with maximally disconnected graphs.
This is not surprising. This is the reason why there is consistency when assuming
iid observations. There is a built in bias towards independence. The bias can be
imposed explicitly on the Φn by restricting the paths to be considered in the sum.
Here are three examples:

loops:

2

−

QM:

Only paths (i0, . . . , in) that form a permutation of the integers
are considered.
2
−
Only maximally disconnected paths are considered.

loops:

. . .

−

−

2

0, 1, . . . , n

}

{

Paths as above but use

2 instead of Φn as the joint likelihoods.

Φn|
|

Preliminary simulation experiments seem to indicate that only maximally dis-
connected paths are not enough and that all the loops are too many. The QM
method has all the maximally disconnected paths but not all the loops (e.g. with
n = 5 the 3-3-loops can not be reached by squaring the sum of 2-2-2-loops) so it
looks like the most promising among the three. What is more interesting about the
QM method is the possibility of using kernels that can go negative or even complex
valued. More research is needed since very little is known about the performance
of these estimators.

4. Estimation by MCMC

We show in this section how to approximate the predictive distribution and the
bayes rule for the smoothness parameter by using Markov Chain Monte Carlo
techniques.

4.1. POSTERIOR MEAN OF THE BANDWIDTH

Apply bayes’ theorem to (1) to obtain the posterior distribution of h,

π(h

x, x1, x2, . . . , xn) =
|

Φ(x, x1, x2, . . . , xn|
0 Φ(x, x1, x2, . . . , xn|

∞

h)π(h)
τ )π(τ )dτ

where π is a function of h. It is worth noticing that π is not the prior on h. It is
only the part of the prior on h that we can manipulate. Recall that Φn integrates
to the function of h given by (13) so eﬀectively the prior that is producing (14) is,

R

Π(h)

π(h)
h(n+1)/2

∝

The posterior mean is then given by,

E(h

x, x1, x2, . . . , xn) = ˆhx =
|

∞

0 hΦ(x, x1, x2, . . . , xn|
0 Φ(x, x1, x2, . . . , xn|

∞

h)π(h)dh
h)π(h)dh

R
R

(14)

(15)

(16)

Equation (16) provides a diﬀerent estimator for each value of x. To obtain a single
global estimate for h just erase the x’s from (16) and change n to n
1 in the
−
δ equation
formulas below. When K is the univariate gaussian kernel and π(h) = h−
(16) simpliﬁes to:

CV-NP BY MCMC

ˆhx = Cn,δ

all paths α(i)s(i)
all paths α(i)

P

P
Γ
1
√2

n

j=0
X

Cn,δ =

1

n+δ
−
2
n+δ
2

Γ
(cid:0)

(cid:1)

(cid:1)

s2(i) =

(xj −

xij )2.

where,

i = (i0, . . . , in) is a path, α = s−

(n+δ) and

(cid:0)

7

(17)

(18)

(19)

Equation (17) follows from two applications of the formula,

∞

h−

(β+1) exp

dh =

2β/2Γ(β/2)s−

β

(20)

0
Z

s2
2h2

(cid:27)

−

(cid:26)

1
2

4.1.1. Bandwidth by Metropolis
To approximate equation (17) we use the fact that the ratio of the two sums is
the expected value of a random variable that takes the value s(i) on the path i
which is generated with probability proportional to α(i). The following version of
the Metropolis algorithm produces a sequence of averages that converge to the
expected value,

Algorithm

0) Start from somewhere

n

(1, 2, . . . , n, 0)
j=0(xj −

(n+δ)/2

(s2)−
P
0, sum

i
s2
α
N

←

←
←
←

xij )2

0, ave

0

←

←

1) Sweep along the components of i

for k from 0 to n do

{

i′k ←
∆k ←
s2′
←
α′
←
R
←

{

xik )(xi′

k

Uniform on
(xi′
k −
s2 + ∆k
(s2′)−
α′/α

(n+δ)/2

0, . . . , n

} \ {
+ xik −

k, ik}
2xk)

8

CARLOS C. RODRIGUEZ

if R > 1 or Unif[0,1] < R then

ik ←

{

i′k, s2

←

s2′, α

α′

}

←

2) Update the estimate for the average,

}

sum +√s2

sum
N
ave
←
goto 1)

←

←
N + 1

sum/N

4.2. THE PREDICTIVE DISTRIBUTION BY GIBBS

To sample from the predictive distribution, f (x
x1, x2, . . . , xn) we use Gibbs to
|
x1, x2, . . . , xn). Hence, we only need to
sample from the joint distribution, f (x, h
|
h, x1, x2, . . . , xn) and b)
know how to sample from the two conditionals, a) f (x
|
π(h
x, x1, x2, . . . , xn). To sample from a) we use the fact that this is (almost)
|
the classical kernel so all we need is to generate from an equiprobable mixture
of gaussians. To get samples from b) just replace the gaussian kernel into the
numerator of equation (14) to obtain, for π(h)

δ,

h−

∝

h−

(n+δ+1) exp

(21)

s2
2h2

.

(cid:27)

−

(cid:26)

π(h

x, x1, x2, . . . , xn)
|

∝

Xall paths
The integral with respect to h of each of the terms being added in (21) is pro-
(n+δ) (see (20)). Thus, by multiplying and dividing by this integral
portional to s−
each term, we can write,

π(h

x, x1, x2, . . . , xn)
|

∝

α(i)πs(i)(h)

(22)

Xall paths

where α(i) = (s(i))−

(n+δ) as before and,

πs(h)

∝

h−

(n+δ+1) exp

s2
2h2

(cid:27)

−

(cid:26)

(23)

From the change of variables theorem it follows that if y is Gamma( n+δ

2 , 1) then
s2
h =
2y follows the distribution (23). This shows that the posterior distribution of
h is a mixture of transformed gamma distributions. This mixture can be generated
by a simple modiﬁcation to the algorithm used to get the posterior mean of h.

q

5. Experiments on simulated and real data

I have coded (essentially) the algorithm described in the previous section in MAPLE
and tested it dozens of times on simulated data for computing a global value for
the smoothness parameter h. All the experiments were carried out with δ = 1

CV-NP BY MCMC

9

Solid=0.5N(-1,.5)+0.5N(1,1),n=50,3bumps=CV,2bumps=MCMC

0.4

0.3

0.2

0.1

-6

-4

-2

0

2

4

6

y

Figure 2. Posterior mean of global h vs plain cross-validation

1 on mixtures of gaussians. The experiments clearly indi-
i.e. with π(h) = h−
cate that the global value of h provided by the MCMC algorithm produce a
kernel estimator that is either identical to plain likelihood cross-validation or
clearly superior to it depending on the experiment. A typical run is presented
in ﬁgure [2] where the true density and the two estimators from 50 iid observa-
tions are shown. The MAPLE package used in the experiments is available at
http://omega.albany.edu:8008/npde.mpl.

For comparison with other density estimators in the literature we show in ﬁgure
[3] the estimate for the complete set of 109 observations of the Old Faithful geyser
data. These data are the 107 observations in [?] plus the two outliers 610 and 620.
This is a standard gaussian kernel with the global value of h = 14.217 chosen by
the MCMC algorithm.

10

CARLOS C. RODRIGUEZ

0.006

0.005

0.004

0.003

0.002

0.001

0

100

200

300

400

500

600

700

y

Figure 3. Estimate for the Old Faithful geyser data, h = 14.217

6. Conclusions

There is nothing special about dimension one. Only minor cosmetic changes (of
the kind: replace h to hd in some formulas) are needed to include the multivariate
case, i.e. the case when the xj ’s are d-dimensional vectors instead of real variables.
Very little is known about these estimators beyond of what it is presented in
this paper, In particular nothing is known about rates of convergence. There are
many avenues to explore with theory and with simulations but clearly the most
interesting and promissing open questions are those related to the performance of
the QM method above.


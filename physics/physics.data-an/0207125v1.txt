Marginalization using the metric of the likelihood

R. PREUSS∗ and V. DOSE

Max-Planck-Institut f¨ur Plasmaphysik, EURATOM Association

Boltzmannstr. 2, D-85748 Garching b. M¨unchen, Germany

(Dated: February 2, 2008)

Abstract

Although the likelihood function is normalizeable with respect to the data there is no guarantee

that the same holds with respect to the model parameters. This may lead to singularities in the

expectation value integral of these parameters, especially if the prior information is not suﬃcient

to take care of ﬁnite integral values. However, the problem may be solved by obeying the correct

Riemannian metric imposed by the likelihood. This will be demonstrated for the example of the

electron temperature evaluation in hydrogen plasmas.

2
0
0
2
 
l
u
J
 
1
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
2
1
7
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗Electronic address: preuss@ipp.mpg.de

I.

INTRODUCTION

have

Given data ~d, a linear parameter c and some function ~f meant to explain the data, we

~d = c

~f (T ) + ~ε

.

·

The vectors shall have dimension N according to the number of quantities measured. Due
= σ2.

to the measurement process the data is corrupted by noise, where

= 0 and

ε2

ε

h

i

h

i

Then by the principle of Maximum Entropy the likelihood function reads

p(D

c, σ, ~f, I)

|

exp

∝

(−

1
2σ2

cfi]2

[di −

)

,

i
X

which is clearly normalizeable for the data ~d and bound for every parameter showing up

as a functional dependency in f . The situation may change when we are looking for the

expectation value of some parameter of f , let say f = f (T ). Then we need to evaluate the

posterior of T with

T
h

i ∝

Z

T p(dT

D, I)

.

|

In order to connect the unknown posterior to the known likelihood we marginalize over all

the parameters which enter the problem, that is in our problem c and σ:

p(dT

D, I) =

p(dT, dc, dσ

D, I)

,

|

Zc Zσ

|

and make use of Bayes theorem:

p(T, c, σ

D, I)

p(D

T, c, σ, I) p(T, c, σ

I)

.

|

∝

|

|

Commonly, the inﬁnitesimal elements in equation (4) are identiﬁed with

p(dT, dc, dσ

D, I) = p(T, c, σ

D, I) dT dc dσ

.

|

|

In mathematical terms this would mean that the probability functions live in euclidean

(1)

(2)

(3)

(4)

(5)

(6)

space. They do not.

II. RIEMANNIAN METRIC

Parameterizations correspond to choices of coordinate systems. The problem to be solved

has to be invariant against reparametrizations [1], i.e. in the space of the probability functions

one has to get the same answer no matter what parameters were chosen to describe a model.

Therefore one is in need of a length measure µ which takes care of deﬁning a distance between

diﬀerent elements of this probability function space. This task is done by applying diﬀerential

geometry to statistical models, an approach which was baptized ’information geometry’ by

S. Amari [2]. Eq. (6) then reads correctly

p(dT, dc, dσ

D, I) = p(T, c, σ

D, I) µ(dT, dc, dσ)

.

(7)

|

|

µ(d~θ) = µ(~θ)d~θ is the natural Riemannian metric on a regular model (in our case the model
is parameterized by ~θ = (T, c, σ)). It results from second variations of the entropy [2, 3] and

is given by

where g is the Fisher information matrix:

µ(d~θ) =

det g(~θ)d~θ

q

gij =

∂2 log p(D
|
∂θi∂θj

~θ, I)

+

− *

.

(8)

(9)

For the above likelihood the metric reads explicitly

µ(σ, c, T )

∝

f 2
i

c
σ3 v
u
u
t

"

i
X

# "

i (cid:18)

X

# − "

(cid:19)

i
X

2

∂fi
∂T

2

fi

∂fi
∂T #

.

(10)

Notice that this approach is based on the assumption that the hypothesis space of the

likelihood deﬁnes the metric to be calculated in. This may not be the case if some prior

information was already used during data acquisition, e.g. the experimentalist uses his expert

knowledge in separating ’correct’ data from the rest. The latter instantly rules out certain

parts of all possible realizations of the likelihood function and results in a diﬀerent hypothesis

space.

III. SIMPLE EXAMPLE

First we want to demonstrate the relevance of using the correct metric with a simple

example which already has all the features of the real world problem further down.

fi(T ) = T

(T + xi)−

1xi

,

·

(11)

where the notation in i corresponds to the data points di. For simpliﬁcation let us assume
that the variance σ2 is known and we only have to marginalize over c in order to get the

posterior. What happens if we do not use the Riemannian metric? Then the marginalization

integral over c reads

quadratic form over c

In order to facilitate analytic calculation the exponent of the likelihood is written in a

p(T

D, I)

dc p(D

T, c, I) p(c

I)

.

|

|

|

∝

Z

(12)

[di −

cfi]2 = ( ~f T ~f )[c

c0]2 +

−

~dT ~d
"

−

( ~dT ~f )2
~f T ~f #

,

(13)

i
X
where c0 = ~dT ~f / ~f T ~f . For the prior p(c

I) the only thing we know is that c will be something

in between an upper and a lower limit, where it is reasonable to assume that the upper (lower)

bound is given by an unknown factor n (1/n) of the value c0 where the maximum of the

likelihood occurs. The principle of maximum entropy gives a ﬂat prior with

|






p(c

I) =

|

0

1
nc0 ∀
0 else

c

nc0

≤

≤

.

The integral over the c-dependent parts then reads

1
nc0

nc0

dc exp

1
2σ2 ( ~f T ~f )[c

−

−

c0]2

.

0
Z
One may check that for ~f T ~f

(cid:26)

(cid:27)
σ2 it is allowed to shift the integral boundaries to +/

−
inﬁnity with aﬀecting the value of the integrand up to a small error only. As a matter of

≫

fact for the chosen model parameters of N=3, xi=i, T =1, c=1 and σ=0.1 the error is in
7 of the correct integral. Notice that this is almost the same for every T in

the order of 10−

between 0 and inﬁnity. We ﬁnally get

p(T

D, I)

|

~f T ~f
~dT ~f

∝ q

exp

1
2σ2

~dT ~d
"

−

( ~dT ~f )2
~f T ~f #)

(−

.

(16)

A look at the behavior for large and small T gives

lim
0
T
→

p(T

D, I)

|

√N
i di
const
P

∝

∝

exp

(cid:26)
,

1
2σ2

−

(

~dT ~d

(cid:20)

−

P

i di)2
N

(cid:21)(cid:27)

lim
T
→∞

p(T

D, I)

|

√~xT ~x
~dT ~x
const

exp

(−

.

∝

∝

1
2σ2

~dT ~d
"

−

( ~dT ~x)2
~xT ~x #)

(14)

(15)

(17)

(18)

)
I
,

|

D
T
(
p

100

10−1

10−2

10−3

10−4

without metric
with metric

10−3

10−2

10−1

101

102

103

100
T

FIG. 1: Posterior p(T

D, I) with (solid line) and without (dashed line) the Riemannian metric.
|

Neglection produces non-vanishing tails.

Though one has no problem with the lower limit since the integrand is regular, the non-

vanishing posterior distribution for T

leads to an expectation value which depends on

where the integration limits are set (see Fig. 1).

→ ∞

Now we implement in the calculation the Riemannian metric. From Eq. (10) we get an

additional factor c, so the integration over the c-dependent parts changes to

nc0

1
nc0

0
Z

dc c exp

−

(cid:26)

1
2σ2 ( ~f T ~f)[c

−

c0]2

.

(19)

(cid:27)
inﬁnity with only minor error.

−

Again it is allowed to extend the integration limits to +/

The full posterior then gives

p(T

D, I)

exp

|

∝

1
2σ2

~dT ~d
"

−

( ~dT ~f )2
~f T ~f #)

(−

T

∂ ~f
∂T

∂ ~f
∂T ! −  

∂ ~f
∂T

T

2

~f
!

/( ~f T ~f ) .

(20)

What is now the behavior of p(T

D, I) for T approaching 0 and inﬁnity? The exponent in

Eq. (20) was already examined in Eqn. (17) and (18) to become constant, so we only have

 

v
u
u
t

|

to look at the square root.

4

xi
T + xi (cid:19)

− "

xi
T + xi (cid:19)

2

3

/

#

2

xi
T + xi (cid:19)

N 2
N

−

=

N

r

i (cid:18)

X

4

xi
1 + xi/T

− "

(cid:19)

i (cid:18)

X

i (cid:18)

X

2

3

/

#

(cid:19)

xi
1 + xi/T

xi
1 + xi/T

2

(cid:19)

i (cid:18)

X

lim
T
→

lim
T
→∞

i (cid:18)

X

0 v
u
u
t
1
T 2 v
u
u
t

i (cid:18)

X

= 0 ,

(21)

= 0

.

(22)

So indeed the square root term which stems from the metric does take care of zero tails in

the posterior! The nice decrease towards 0 is shown in Fig. 1 by the solid line.

IV. REAL WORLD PROBLEM

In the problem of determining the electron temperature in an hydrogen plasma heated

by electron cyclotron resonance, the model function T depends in a quite complicated way

Both V and R are matrices, but only the diagonal matrix V depends on T with entries on

on the temperature T :

the diagonal:

~f (T ) =

V (R

V )−

1~x

.

−

−

Vii =

1
+ 1
biT

ai
√T

,

(23)

(24)

where ai and bi are constants with respect to ion species i. Since the sensitivity of the

measurement apparatus is unknown one has to introduce a linear parameter c in order to

relate the data to the model, i.e. Eq. (1). Contrary to our simple problem we are not so

fortunate to know the variance σ exactly. The experimentalist can only provide an estimate

~s of the true errors ~σ with respect to each other but not on the total scale, so that we have

to introduce an overall multiplication factor ω, with σi = ωsi. In order to assign a prior to

ω the outlier tolerant approach [4] was chosen:

p(ω

α, γI) = 2

|

αγ
Γ(γ)

1
ω

2γ

exp

α
ω2

−

1
ω

.

(25)

(cid:19)
The expectation value of ω should be one, since the experimentalist does his estimation

(cid:18)

o

n

according to his best knowledge. Furthermore, from the characteristics of the measurement

process one can tell that the best guess of ~s should not deviate by more than 50% from the

true ~σ. This results in α = 1.28 and γ = 2.0076.

Now we follow the route explained above to evaluate the expectation value of T . Again

we start by marginalizing c (with the ﬂat prior of Eq. (14)) and ω without making use of

the Riemannian metric. This gives the posterior in T

p(T

D, I)

|

T ~ˆf
~ˆf
∝ q
T ~ˆf
~ˆd





α +

T~ˆd
1
~ˆd
2 

−



T ~ˆf )2
~ˆd
(
T ~ˆf
~ˆf









N
2 −

γ

−

1

−

.

(26)

For simplicity of notation the hat shall denote that the values have been divided by the

)
I
,

|

D
T
(
p
 
g
o

l

105

100

10−5

10−10

10−15

10−20

without metric
with metric
Riemannian metric

10−1

100

101

102

103

104

T

FIG. 2: Posterior p(T

D, I) with (solid line) and without (dashed line) the Riemannian metric
|

(dotted line). The incision at T = 118.43 K is a single point which is due to the parameterization

of the physical model. It does not aﬀect the integrability.

estimated error ~s: ˆdi = di/si. The posterior is displayed in Fig. 2. Here we have to face the

problem we observed above in the simple example. Though a non-vanishing tail for T

is not so harmful, the increase with T

results in a divergence.

→ ∞

Help comes by obeying the correct Riemannian metric. Then the posterior reads

p(T

D, I)

µ(T )

|

∝

1
T ~ˆf
~ˆf
q





α +

T~ˆd
1
~ˆd
2 

−



T ~ˆf )2
~ˆd
(
T ~ˆf
~ˆf









N
2 −

γ

−

1

−

where µ(T ) is just the metric of Eq. (10) without the terms in c and ω (marginalized over).

The situation changes completely (see Fig. 2) and the integral becomes feasible now.

0

→

(27)

V. CONCLUSION

The correct mathematical way to deal with marginalization integrals is to use the Rie-

mannian metric. This invariant measure takes care of deﬁning correct inﬁnitesimal elements

to be integrated over. Since parameterizations of a model may be subjective and vary with

the investigator of a problem, this is the only consistent way to get comparable answers in

probability space.

VI. ACKNOWLEDGMENT

We like to acknowledge discussions with C. Rodriguez.

[1] C. Rodriguez, “The metrics induce by the kullback number,” in Maximum Entropy and

Bayesian Methods, J. Skilling, ed., Kluwer Academic, Dordrecht, 1989.

[2] S. Amari, Diﬀerential-Geometrical Methods in Statistics, Springer-Verlag, Berlin, Heidelberg,

1985.

[3] C. Rodriguez, “From euclid to entropy,” in Maximum Entropy and Bayesian Methods,

J. W. T. Grandy, ed., Kluwer Academic, Dordrecht, 1991.

[4] V. Dose and W. von der Linden, “Outlier tolerant parameter estimation,” in Maximum Entropy

and Bayesian Methods, V. Dose et al., ed., Kluwer Academic, Dordrecht, 1999.


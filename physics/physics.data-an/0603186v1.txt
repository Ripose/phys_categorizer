Spectral properties of empirical covariance matrices for data with power-law tails

Zdzis law Burda,∗ Andrzej T. G¨orlich,† and Bart lomiej Wac law‡
Mark Kac Center for Complex Systems Research and Marian Smoluchowski Institute of Physics,
Jagellonian University, Reymonta 4, 30-059 Krakow, Poland
(Dated: February 21, 2014)

We present an analytic method for calculating spectral densities of empirical covariance matrices
for correlated data.
In this approach the data is represented as a rectangular random matrix
whose columns correspond to sampled states of the system. The method is applicable to a class of
random matrices with radial measures including those with heavy (power-law) tails in the probability
distribution. As an example we apply it to a multivariate Student distribution.

6
0
0
2
 
r
a

M
 
2
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
6
8
1
3
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

I.

INTRODUCTION

Random Matrix Theory provides a useful tool for description of systems with many degrees of freedom. A large
spectrum of problems in physics [1], telecommunication, information theory [2, 3, 4, 5] and quantitative ﬁnance
[6, 7, 8, 9, 10, 11, 12, 13] can be naturally formulated in terms of random matrices.

In this paper we apply random matrix theory to calculate the eigenvalue density of the empirical covariance matrix.
Statistical properties of this matrix play an important role in many empirical applications. More precisely, the problem
which we shall discuss here can be generally formulated in the following way. Consider a statistical system with N
correlated random variables. Imagine that we do not know a priori correlations between the variables and that we try
to learn about them by sampling the system T times. Results of the sampling can be stored in a rectangular matrix X
containing empirical data Xit, where the indices i = 1, . . . , N and t = 1, . . . T run over the set of random variables and
measurements, respectively. If the measurements are uncorrelated in time the two-point correlation function reads:

where C is called correlation matrix or covariance matrix. For simplicity assume that
C one can try to reconstruct it from the data X using the empirical covariance matrix:

Xiti
h

= 0. If one does not know

Xi1t1 Xi2t2 i
h

= Ci1i2 δt1t2 .

cij =

XitXjt,

1
T

T

t=1
X

which is a standard estimator of the correlation matrix. One can think of X as of an N
from the matrix ensemble with some prescribed probability measure P (X)DX. The empirical covariance matrix:

T random matrix chosen

×

1
T
depends thus on X. Here Xτ stands for the transpose of X. For the given random matrix X the eigenvalue density
of the empirical matrix c is:

XXτ

c =

(3)

where λi(c)’s denote eigenvalues of c. Averaging over all random matrices X:

ρ(X, λ)

1
N

≡

N

i=1
X

δ(λ

λi(c)),

−

Z
we can ﬁnd the eigenvalue density of c which is representative for the whole ensemble of X. We are interested in
how the eigenvalue spectrum of c is related to that of C [14, 15, 16]. Clearly, as follows from (1), the quality of the

ρ(λ)

ρ(X, λ)
i

=

≡ h

ρ(X, λ) P (X) DX,

(1)

(2)

(4)

(5)

∗burda@th.if.uj.edu.pl
†atg@th.if.uj.edu.pl
‡Corresponding author: bwaclaw@th.if.uj.edu.pl

→ ∞

, that is for r

information encoded in the empirical covariance matrix c depends on the number of samples or more precisely on the
0, the empirical matrix c perfectly reproduces the real
ratio r = N/T . Only in the limit T
→
covariance matrix C. Recently a lot of eﬀort has been made to understand the statistical relation between c and C
for ﬁnite r. This relation plays an important role in the theory of portfolio selection where Xit are identiﬁed with
normalized stocks’ returns and C is the covariance matrix for inter-stock correlations. It is a common practice to
reconstruct the covariance matrix from historical data using the estimator (2). Since the estimator is calculated for
a ﬁnite historical sample it contains a statistical noise. The question is how to optimally clean the spectrum of the
empirical matrix c from the noise in order to obtain a best quality estimate of the spectrum of the underlying exact
covariance matrix C. One can consider a more general problem, where in addition to the correlations between the
degrees of freedom (stocks) there are also temporal correlations between measurements [17]:

Xi1t1 Xi2t2i
h

= Ci1i2 At1t2 ,

(6)

given by an autocorrelation matrix A. If X is a Gaussian random matrix, or more precisely if the probability measure
P (X)DX is Gaussian, then the problem is analytically solvable in the limit of large matrices [17, 18, 19, 20]. One can
derive then an exact relation between the eigenvalue spectrum of the empirical covariance matrix c and the spectra
of the correlation matrices A and C.

In this paper we present an analytic solution for a class of probability measures P (X)DX for which the marginal
which means that the cumulative
distributions of individual degrees of freedom have power law tails: p(Xit)
distribution function falls like X −ν
. Such kind of distributions has been discussed previously [21, 22] but, up to
it
our knowledge, the spectral density of c remained unattainable analytically. The motivation to study such systems
comes from the empirical observation that stocks’ returns on ﬁnancial markets undergo non-Gaussian ﬂuctuations
with power-law tails. The observed value of the power-law exponent ν
3 seems to be universal for a wide class of
ﬁnancial assets [23, 24, 25]. Random matrix ensembles with heavy tails have been recently considered for 0 < ν < 2
using the concept of L´evy stable distributions [26, 27, 28]. Here we will present a method which extrapolates also to
the case ν > 2, being of particular interest for ﬁnancial markets.

X −1−ν
it

∼

≈

We will study here a model which on the one hand preserves the structure of correlations (6) and on the other
hand has power-law tails in the marginal probability distributions for individual matrix elements. More generally, we
will calculate the eigenvalue density of the empirical covariance matrix c (3) for random matrices X which have a
probability distribution of the form:

Pf (X)DX =

−1f (Tr Xτ C−1XA−1)DX,

N

where DX =

N,T
i,t=1 dXit is a volume element. The normalization constant

:

N

Q

= πd/2(DetC)T /2(DetA)N/2

and the parameter d = N T have been introduced for convenience. The function f is an arbitrary non-negative
function such that P (X) is normalized:

P (X)DX = 1.

In particular we will consider an ensemble of random matrices with the probability measure given by a multivariate

Student distribution:

N

R

Pν (X)DX =

Γ( ν+d
2 )
Γ( ν
2 )σd

N

(cid:18)

1 +

1
σ2 Tr Xτ C−1XA−1

− ν+d
2

DX.

(cid:19)

The two-point correlation function can be easily calculated for this measure:

(7)

(8)

(9)

(10)

Xi1t1Xi2t2 i
h

=

ν

Ci1i2 At1t2.

σ2

2

−

−

2 and for ν > 2 the last equation takes the form (6). With this choice of σ2 the two-point
We see that for σ2 = ν
function becomes independent on ν, however the formula for the probability measure (9) breaks down at ν = 2 and
cannot be extrapolated to the range 0 < ν
ν
which extrapolates easily to this range. In this case one has to remember that for ν > 2 the exact covariance matrix
ν−2 C, where C is the matrix in Eq. (9) with σ2 = ν. We will stick to this choice in the remaining part
is given by ν
of the paper.

2. An alternative and actually a more conventional choice is σ2

≤

≡

The marginal probability distribution for a matrix element Xit can be obtained by integrating out all others degrees
of freedom from the probability measure P (X)DX. One can see that for the Student probability measure (9) the
marginal distributions of individual elements have by construction power-law tails. For example if C is diagonal

C = Diag(C2
of the matrix X:

1 , . . . , C2

N ) and A = 1T then the marginal probability distributions can be found exactly for each element

pi(Xit) =

Γ( ν+1
2 )
2 )√νπCi (cid:18)

Γ( ν

1 +

X 2
it
νC2

i (cid:19)

− ν+1
2

.

X −1−ν
it

∼

for large Xit with amplitudes which depend on the index i and are independent
The distributions pi fall like
of t. If one thinks of a stock market, this means that stocks’ returns have the same tail exponent but diﬀerent tail
amplitudes. The independence of t means that the distributions pi(Xit) are stationary. More generally, for any C and
for A which is translationally invariant At1t2 = A(
) the marginal distributions of entries Xit can be shown to
t1
|
|
have power-law tails with the same exponent ν for all Xit and tail coeﬃcients which depend on i and are independent
of t, exactly expected from stocks’ returns on a ﬁnancial market.

−

t2

The main purpose of this paper is to calculate the spectral density of the empirical covariance matrix c for the
Student distribution (9). The method is similar to the one presented in [29, 30, 31] for a square Hermitian matrix.
It consists in an observation that every quantity averaged over the probability distribution having the form (7) can
1) “angular” variables and then of a “radial” variable. This shall be shortly presented in
be ﬁrst averaged over (d
sections II and III. In the section IV the main equation for the eigenvalue density of c for the radial ensemble (7) with
an arbitrary radial proﬁle f shall be given. The section V contains results for the Student distribution (9) including
some special cases.

−

II. RADIAL MEASURES

The radial measure (7) depends on one scalar function f = f (x2) of a real positive argument. In this section we
shall develop a formalism to calculate the eigenvalue spectrum ρf (λ) of the empirical covariance matrix (3) for such
radial ensembles. The calculation can be simpliﬁed by noticing that the dependence of ρf (λ) on the matrices C and
A actually reduces to a dependence on their spectra. This follows from an observation that for a radial measure (7)
the integral (5) deﬁning the eigenvalue density is invariant under simultaneous transformations:

C
A

X

→
→

˜C = ØCØτ
˜A = Qτ AQ
˜X = ØXQτ

→
N and T

xit ≡

Xit
CiAt

.

where Ø, Q are orthogonal matrices of size N
T , respectively. Choosing the orthogonal transformations
×
Ø and Q in such a way that ˜C and ˜A become diagonal: ˜C = Diag(C2
T ) with all Ci’s
and At’s being positive, we see that ρf (λ) depends on the matrices C and A indeed only through their eigenvalues.
Therefore, for convenience we shall assume that C and A are diagonal from the very beginning.

N ), ˜A = Diag(A2

1 , . . . , C2

1, . . . , A2

×

The radial form of the measure allows one to determine the dependence of the eigenvalue density ρf (λ) on the
radial proﬁle f (x2).
Intuitively, the reason for that stems from the fact that one can do the integration for the
radial ensembles (7) in two steps: the ﬁrst step is a sort of angular integration which is done for ﬁxed x and thus
is independent of the radial proﬁle f (x2), and the second one is an integration over x. A short inspection of the
formula (7) tells us that ﬁxed x corresponds to ﬁxed trace: Tr Xτ C−1XA−1, and thus that we should ﬁrst perform
the integration over the ﬁxed trace ensemble. We shall follow this intuition below.

Let us deﬁne a matrix x = C− 1

2 XA− 1

2 . Since we assumed that A and C are diagonal, A1/2 and C1/2 are also

diagonal with elements being square roots of those for A and C. The elements of x are:

They can be viewed as components xj, j = 1, . . . , d of a d-dimensional Euclidean vector, where the index j is
constructed from i and t. The length of this vector is:

d

N

T

x2

≡

x2
j =

j=1
X

i=1
X

t=1
X

it = Tr xτ x = Tr Xτ C−1XA−1,
x2

and thus the ﬁxed trace matrices X are mapped onto a d-dimensional sphere of the given radius x. It is convenient
Tr ωτ ω = 1. We can
to parameterize the d-dimensional vector x using spherical coordinates x = xω, where ω2

≡

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(19)

(21)

also use these coordinates to represent the matrix X:

X = C

1

2 = xC

1

2 ωA

1

2 = xΩ(ω),

1
2 xA
1
2 ωA

1

2 ,

C

Ω(ω)

≡

Tr Ωτ C−1ΩA−1 = 1.

III. ANGULAR INTEGRATION

where the deﬁnition of the matrix Ω(ω) is equivalent to Ωit ≡
d-dimensional space, Ω(ω) gives a radial projection of this point on a d-dimensional ellipsoid of ﬁxed trace:

CiAtωit. While ω gives a point on a unit sphere in

We are now prepared to do the integration over the angular variables Dω. In the spherical coordinates (15) the

radial measure (7) assumes a very simple form:

Pf (X)DX = π−d/2f (x2)xd−1dx Dω.

The normalization factor
formula for ρf (λ) in the form:

N

−1 from Eq. (7) cancels out. The spherical coordinates X = xΩ(ω) allow us to write the

ρf (λ) = π−d/2

ρ(X, λ) Pf (X) DX = π−d/2

Dω

ρ (xΩ(ω), λ) f (x2)xd−1dx.

(18)

Z

∞

Z

0

Z

Although the integration over the angular and the radial part cannot be entirely separated, we can partially decouple
x from Ω in the ﬁrst argument of ρ(xΩ(ω), λ). It follows from (4) that the rescaling X
αX by a constant gives
the relation:

→

ρ(αX, λ) = α−2ρ(X, α−2λ).

This observation can be used to rewrite the equation (18) in a more convenient form:

ρf (λ) = π−d/2

Dω

ρ

Ω(ω),

f (x2)xd−3dx =

∞

Z

0
Z

(cid:18)

λ
x2

(cid:19)

2
Γ(d/2)

∞

ρ∗

λ
x2

(cid:18)

(cid:19)

0
Z

f (x2)xd−3dx,

(20)

where Γ(z) is the Euler gamma function and

ρ∗(λ)

≡

1
Sd Z

ρ (Ω(ω), λ) Dω.

Here Sd denotes the hyper-surface area of d-dimensional sphere of radius one: Sd = 2πd/2/Γ( d
2 ). As we shall see
below the last expression is an eigenvalue distribution of the empirical covariance matrix for the ﬁxed trace ensemble
deﬁned as an ensemble of matrices X such that Tr Xτ C−1XA−1 = 1. From the structure of the equation (20) it is
clear that if ρ∗(λ) is known then ρf (λ) can be easily calculated for any radial proﬁle just by doing one-dimensional
integral. So the question which we face now is how to determine ρ∗(λ) for arbitrary C and A. We will do this by
a trick. Instead of calculating ρ∗(λ) directly from Eq. (21), we will express ρ∗(λ) by the corresponding eigenvalue
density ρG(λ) for a Gaussian ensemble, whose form is known analytically [17, 28]. Let us follow this strategy in the
next section.

IV. FIXED TRACE ENSEMBLE AND GAUSSIAN ENSEMBLE

The probability measure for the ﬁxed trace ensemble is deﬁned as

P∗(X)DX =

δ

Tr (Xτ C−1XA−1)

1

DX.

Γ( d
2 )

(22)

N
In the spherical coordinates ω the formula reads:

(cid:0)

2
Sd

P∗(X)DX =

δ(x2

1) xd−1dx Dω.

−

−

(cid:1)

One can easily check that the integration ρ∗(λ) =
the normalization condition for P∗(X) is fulﬁlled. Consider now a Gaussian ensemble:

ρ(X, λ)P∗(X)DX indeed gives (21). It is also worth noticing that

where

PG(X)DX

−1fG(Tr Xτ C−1XA−1)DX,

R

≡ N

fG(x2) =

1

2 x2
2d/2 e− 1

,

for which the spectrum ρG(λ) is known or more precisely it can be easily computed numerically in the thermodynamical
limit N, T
[17, 32, 33]. On the other hand as we learned in the previous section, the density of eigenvalues of
the empirical covariance matrix c can be found applying Eq. (20) to the Gaussian radial proﬁle (24):

→ ∞

Changing the integration variable to y: x2 = dy2 and rescaling the spectrum ρG by d: λ = Λ

d we eventually obtain:

ρG(Λ) =

21−d/2
Γ( d
2 )

∞

ρ∗

Λ
x2

(cid:18)

(cid:19)

0

Z

xd−3 e− 1

2 x2

dx.

dρG(dλ) =

∞

ρ∗

λ
y2

(cid:18)

(cid:19)

0
Z

1
y2

21−d/2dd/2
Γ( d
2 )

"

yd−1 e− 1

2 d y2

dy.

#

One can easily check that the formula in the square brackets tends to the Dirac delta for large matrices because then
d goes to inﬁnity:

lim
d→∞

21−d/2 dd/2
Γ( d
2 )

yd−1 e− 1

2 d y2

= δ(y

1),

−

ρ∗(λ) = dρG(dλ).

and thus the integrand in Eq. (26) gets localized around the value y = 1. Therefore for large d we can make the
following substitution:

Inserting it into Eq. (20) and changing the integration variable to y = dλ
paper:

x2 we ﬁnally obtain a central equation of this

ρf (λ) =

dd/2
Γ(d/2)

λd/2−1

∞

0
Z

ρG(y)f

y−d/2dy.

dλ
y

(cid:18)

(cid:19)

The meaning of this formula is the following: for any random matrix ensemble with a radial measure (7) the eigenvalue
density function ρf (λ) is given by a one-dimensional integral of a combination of the corresponding Gaussian spectrum
ρG(λ) and the radial proﬁle f (x). The equation holds in the thermodynamic limit: d = N T
and r = N/T =
const. Since in this limit we are able to calculate the spectrum ρG(λ) for arbitrarily chosen A, C, the formula (28)
gives us a powerful tool for computing spectra of various distributions. In the next section we shall apply it to the
multivariate Student ensemble (9).

→ ∞

V. MULTIVARIATE STUDENT ENSEMBLE

The radial proﬁle for the Student ensemble (9) is:

f (x2)

fν(x2) =

≡

Γ( ν+d
2 )
Γ( ν
2 )νd/2

x2
ν

(cid:19)

1 +

(cid:18)

− ν+d
2

.

We have chosen here the standard convention σ2 = ν since we would like to calculate the spectrum ρν(λ) also for
ν

2 (see the discussion at the end of the ﬁrst section). Inserting (29) into the equation (28):

≤

ρν(λ) =

d
ν

d/2 Γ( ν+d
2 )
2 )Γ( ν
Γ( d
2 )

(cid:16)

(cid:17)

λd/2−1

ρG(y)

1 +

∞

0
Z

− ν+d
2

y− d

2 dy,

dλ
νy

(cid:19)

(cid:18)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

and taking the limit d

:
→ ∞

lim
d→∞

d
ν

d/2 Γ( ν+d
2 )
Γ( d
2 )

(cid:16)

(cid:17)

y− d

2 λ

d
2 −1

1 +

(cid:18)

− ν+d
2

dλ
νy

(cid:19)

ν/2

=

ν
2

(cid:16)

(cid:17)

e− νy
2λ y

ν

2 λ− ν+2
2 ,

we see that the expression for ρν(λ) simpliﬁes to an expression which is independent of d:

ρν(λ) =

1
Γ( ν
2 )

ν
2

(cid:16)

(cid:17)

0
Z

ν/2

λ− ν

2 −1

∞

ρG(y) e− νy
2λ y

ν

2 dy.

The formula (30) works for all ν > 0. From the last equation we can infer the behavior of ρν(λ) for large λ. The
function ρG(y) has a compact support [17, 20, 32], therefore for large λ the exponential can be approximated well by
1. The function ρν (λ) has thus a long tail:

ρν(λ)

≈

λ− ν

2 −1

1
Γ( ν
2 )

·

ν
2

ν/2

∞

ν

ρG(y)y

2 dy,

0
Z

(cid:16)

(cid:17)
ν/2

where the integral does not depend on λ. The exponent
1 in the above power-law depends on the index ν
−
of the original Student distribution. The change from the power ν to the power ν/2 comes about because c is a
quadratic combination of X.

−

The power-law tail in the eigenvalue distribution (31) does not disappear in the limit of large matrices contrary
to the power-law tails in the eigenvalue distribution for an ensemble of matrices whose elements are independently
distributed random numbers. For such matrices, for ν > 2, the density ρ(λ) falls into the Gaussian universality class
and yields the Wishart spectrum [34]. One should remember that the multivariate Student distribution (9) discussed
here does not describe independent degrees of freedom even for A = 1T and C = 1N , in which case the degrees of
freedom are “uncorrelated” but not independent.

We have learned that the spectrum is unbounded from above. Let us now examine the lower limit of the spectrum.

Rewriting Eq. (30) in the form:

ρν (λ) =

ρG (2xλ) e−νxxν/2 dx,

2νν/2
Γ( ν
2 )

∞

0

Z

we see that as long as λ > 0 the function ρν(λ) is positive since ρG(x) is positive on a ﬁnite support. Thus the function
ρν(λ) vanishes only at λ = 0 and it is positive for any λ > 0. Contrary to the classical Wishart distribution for the
Gaussian measure, the spectrum (30) spreads over the whole real positive semi-axis. On the other hand, taking the
limit ν

of Eq. (32) and using the formula:

→ ∞

(30)

(31)

(32)

(33)

we obtain ρν→∞(λ) = ρG(λ) as expected, because in this limit the radial proﬁle fν(x2) given by Eq. (29) for the
Student distribution reduces to the Gaussian one (24).

Let us ﬁrst consider the case without correlations: C = 1N and A = 1T . The spectrum of the empirical covariance

for the Gaussian ensemble is given by the Wishart distribution:

where λ± = (1

√r)2 [14, 15, 16]. The corresponding spectrum (30) for the Student ensemble is then:

±

ρν(λ) =

1
2πrΓ( ν
2 )

ν
2

(cid:16)

(cid:17)

ν/2

λ−ν/2−1

λ+

λ−

Z

p

(λ+

y)(y

−

−

λ−) e− νy

2λ yν/2−1 dy.

(34)

The integral over dy can be easily computed numerically. Results of this computation for diﬀerent values of ν are
shown in Fig. 1. For increasing ν the spectrum ρν (λ) tends to the Wishart distribution but even for very large ν it

lim
ν→∞

2νν/2
Γ( ν
2 )

xν/2 e−νx = δ(x

1/2),

−

VI. EXAMPLES

ρG(λ) =

(λ+

λ)(λ

λ−),

−

−

1
2πrλ

p

)
λ
(
ν
ρ

)
λ
(
ν
ρ

1

0.8

0.6

0.4

0.2

0.6

0.5

0.4

0.3

0.2

0.1

2
λ

2
λ

)
λ
(
ν
ρ

0.5

0.4

0.3

0.2

0.1

0

0

0

0

1

3

4

FIG. 1: Spectra of the covariance matrix c for the Student distribution (9) with C = 1N and A = 1T , r = N/T = 0.1, for
ν = 1/2, 2, 5, 20 and 100 (thin lines from solid to dotted), calculated using the formula (34) and compared to the uncorrelated
Wishart (thick line). One sees that for ν → ∞ the spectra tend to the Wishart distribution.

0.1
λ

0.2

0

0

1

3

4

FIG. 2: Spectra of the empirical covariance matrix c calculated from Eq. (34) with r = 1/3, compared to experimental data
(stair lines) obtained by the Monte Carlo generation of ﬁnite matrices N = 50, T = 150. Inset: the left part of the same
distributions, points represent experimental data.

has a tail which touches λ = 0 as follows from Eq. (32). In Fig. 2 we have plotted ρν (λ) for ν = 0.5, 1 and 2 and
compared them to experimental results obtained by the Monte-Carlo generation of random matrices drawn from the
corresponding ensemble with the probability measure (9) for which eigenvalue densities were computed by numerical
diagonalization. The agreement is perfect. Actually it is even better than for the Gaussian case for the same size N .
As a second example we consider the case when C has two distinct eigenvalues λ1 and λ2 with degeneracies:
(1
1. Such a covariance matrix can be used to model the simplest
eﬀect of sectorization on a stock exchange. For example if all diagonal elements of the matrix C are equal 1 and all
oﬀ-diagonal are equal ρ0 (0 < ρ0 < 1) the model can be used to mimic a collective behavior on the market [9, 10].
In this case λ1 = 1
1)ρ0 is non-degenerated, hence p = 1/N . The
1 and λ2 = 1 + (N
eigenvector corresponding to the larger eigenvalue λ2 can be thought of as describing the correlations of all stocks.
µ and p being an arbitrary number between
For our purposes it is however more convenient to set λ1 = 1 and λ2

p)N for λ1 and pN for λ2, where 0

ρ0 has a degeneracy N

−

≤

−

≤

−

−

p

≡

)
λ
(
ν
ρ

0.4

0.8

0.6

0.2

0

0

2

6

8

4
λ

FIG. 3: Spectra ρν(λ) for C having two distinct eigenvalues: 1 and µ in proportion (1 − p) : p, calculated from Eq. (30) with
ρG given by formula (35), with r = 1/10, p = 1/2 and µ = 5. Thick solid line corresponds to the Gaussian case ν → ∞ while
thin lines to ν = 5, 20, 100. These lines are compared to Monte-Carlo results obtained by the generation and diagonalization
of ﬁnite matrices with N = 40, T = 400 (gray lines), which lie almost exactly on top of them and can be hardly seen by an
unarmed eye.

zero and one. The corresponding Wishart spectrum ρG(λ) can be obtained by solving equations given by a conformal
map [20]. The resulting spectrum has the form:

(35)

(36)

(37)

(38)

where

ρG(λ) =

1
π

Im
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M (Z(λ))
λ

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

M (Z) =

Z(λ) =

1
Z

−

p
1

+

−
−
a
3

+

(1

pµ

,

Z

−

µ
−
i√3)(3b
22/3E
3

−

a2)

(1 + i√3)E

−

6

21/3

,

E =

3√3

27c2

(cid:16)
pr)

p

·
18abc + 4a3c + 4b3

·

−

a2b2

27c + 9ab

−

−

−

2a3

1/3

,

(cid:17)

1

−

−

−

−

−

pr

µ(1

λ, b = λ(µ + 1)

where a = r
λµ. Inserting the above formula into Eq. (30)
we obtain an integral, which can be computed numerically for arbitrary r, µ, p. In Fig. 3 we show examples of this
computation for diﬀerent values of the index ν. In the same ﬁgure we compare the analytic results with those obtained
by the Monte Carlo generation and numerical diagonalization of random matrices for N = 40, T = 400. As before,
the agreement between the analytic and Monte-Carlo results is perfect. We see that the eﬀect on the spectrum of
introducing heavy tails increases with decreasing ν. When ν is decreasing from inﬁnity to zero the two disjoint islands
of the distribution develop a bridge to eventually end up as a distribution having only one connected component.

r) and c =

µ(1

−

−

−

VII. SUMMARY

In the paper we have developed a method for computing spectral densities of empirical covariance matrices for a
wide class of “quasi-Wishart” ensembles with radial probability measures. In particular we have applied this method
to determine the spectral density of the empirical covariance matrix for heavy tailed data described by a Student
multivariate distribution. We have shown that the spectrum ρ(λ) decays like λ−ν/2−1 where ν is the index of Student
distribution. The case of ν = 3 is of particular importance since it can be used in modeling stock markets. The
eigenvalue density spreads over the whole positive semi-axis in contrast to the Wishart spectrum which has a ﬁnite
support.

We have also derived a general formula for the eigenvalue spectrum of the empirical covariance matrix for radial
ensembles. The spectrum is given by a one-dimensional integral, which can be easily computed numerically. The
method works also in the case of correlated assets.

Acknowledgements

This work was supported by Polish Ministry of Science and Information Society Technologies grants: 2P03B-08225
(2003-2006) and 1P03B-04029 (2005-2008) and EU grants: MTKD-CT-2004-517186 (COCOS) and MRTN-CT-2004-
005616 (ENRAGE).

[1] T. Guhr, A. M¨uller-Groeling, H. A. Weidenm¨uller, Phys. Rept. 299 (1998) 189.
[2] A. L. Moustakas et al., Science 287 (2000) 287.
[3] A. M. Sengupta and P. P. Mitra, physics/0010081.
[4] R. M¨uller, IEEE Transactions on Information Theory 48 (2002) 2495.
[5] S. E. Skipetrov, Phys. Rev. E 67 (2003) 036621.
[6] L. Laloux, P. Cizeau, J.-P. Bouchaud and M. Potters, Phys. Rev. Lett. 83 (1999) 1467.
[7] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. N. Amaral, T. Guhr and H. E. Stanley, Phys. Rev. E 65 (2002) 066126.
[8] A. Utsugi, K. Ino and M. Oshikawa, Phys. Rev. E 70 (2004) 026110.
[9] S. Pafka and I. Kondor, Physica A 319 (2003) 487; Physica A 343 (2004) 623.
[10] G. Papp, S. Pafka, M. A. Nowak and I. Kondor, Acta Phys. Polon. B 36 (2005) 2757.
[11] T. Guhr and B. K¨alber, J. Phys. A 36 (2003) 3009.
[12] Y. Malevergna and D. Sornette, Physica A 331 (2004) 660.
[13] Z. Burda and J. Jurkiewicz, Physica A 344 (2004) 67.
[14] V. A. Marˇcenko and L. A. Pastur, Math. USSR-Sb. 1 (1967) 457.
[15] Z. D. Bai and J. W. Silverstein, J. Multivariate Anal. 54 (1995) 175.
[16] S. I. Choi and J. W. Silverstein, J. Multivariate Anal. 54 (1995) 295.
[17] Z. Burda, J. Jurkiewicz, B. Waclaw, Phys. Rev. E 71 (2005) 026111.
[18] J. Feinberg, A. Zee, J. Stat. Phys. 87 (1997) 473.
[19] A. M. Sengupta and P. P. Mitra, Phys. Rev. E 60 (1999) 3389.
[20] Z. Burda, A. G¨orlich, A. Jarosz, J. Jurkiewicz, Physica A 343 (2004) 295.
[21] P. Repetowicz, P. Richmond, math-ph/0411020.
[22] P. Cizeau, J. P. Bouchaud, Phys. Rev. E 50 (1994) 1810.
[23] P. Gopikrishnan, M. Meyer, L. A. N. Amaral, H. E. Stanley, Eur. Phys. J. B 3 (1998) 139.
[24] P. Gopikrishnan, V. Plerou, L. A. N. Amaral, M. Meyer, H. E. Stanley, Phys. Rev. E 60 (1999) 5305.
[25] R. Rak, S. Drozdz, J. Kwapien, physics/0603071.
[26] Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, I. Zahed, Physica A 343 (2004) 694.
[27] Z. Burda, J. Jurkiewicz, M. A. Nowak, Acta Phys. Polon. B 34 (2003) 87.
[28] Z. Burda, A. Jarosz, J. Jurkiewicz, M. A. Nowak, G. Papp, I. Zahed physics/0603024.
[29] F. Toscano, R. O. Vallejos, C. Tsallis, Phys. Rev. E 69 (2004) 066131.
[30] A. C. Bertuola, O. Bohigas, M. P. Pato, Phys. Rev. E 70 (2004) 065102.
[31] A. Y. Abul-Magd, Phys. Rev. E 71 (2005) 066207.
[32] Z. Burda, A. G¨orlich, J. Jurkiewicz and B. Waclaw, Eur. Phys. J. B 49 (2006) 319.
[33] Z. Burda, J. Jurkiewicz and B. Waclaw, Acta Phys. Pol. B 36 (2005) 2641.
[34] E. Brezin, A. Zee, Nucl. Phys. B 402 (1993) 613.


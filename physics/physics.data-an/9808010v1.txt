8
9
9
1
 
g
u
A
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
0
8
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

UNREAL PROBABILITIES
Partial Truth with Cliﬀord Numbers

Carlos C. Rodriguez
Department of Mathematics and Statistics
University at Albany, SUNY
Albany NY 12222, USA1

February 2, 2008

1carlos@math.albany.edu

Contents

1 Introduction

2 The Axioms

. . . . . . . . . . . . . . . . . . . . .
2.1 The Boolean Algebra
2.2 The algebra of Cliﬀord numbers
. . . . . . . . . . . . . . .
2.3 Deﬁnition of ψ . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 The truth of 0 . . . . . . . . . . . . . . . . . . . . . .

A

G

3 The spaces Hc

3.1 The Hc are Hilbert spaces . . . . . . . . . . . . . . . . . . . .
3.1.1 The inner product in Hc . . . . . . . . . . . . . . . . .
) . . . . . . . . . . .
A c)

3.2 The isomorphic spaces: H(

Hc(

A

≃

4 The truth with ψ

4.1 Propositions as operators
4.2 Commutativity, orthogonality and a Cliﬀord number times a

. . . . . . . . . . . . . . . . . . . .

proposition . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Independence

5.1 A restricted product rule . . . . . . . . . . . . . . . . . . . . .
Independence and Orthogonality . . . . . . . . . . . . . . . .
5.2

6 Flipping n coins

. . . . . . . . . . . . . . .
6.1 The Binomial experiment with ψs
P k
6.2 Computation of
. . . . . . . . . . . . . . . . .
n ψnk
6.3 Case: AB = BA . . . . . . . . . . . . . . . . . . . . . . . . .
6.4 Case: AB = 0 . . . . . . . . . . . . . . . . . . . . . . . . . . .
BA . . . . . . . . . . . . . . . . . . . . . . . .
6.5 Case: AB =

ψn −
k

2

−

7 The weak law of large numbers

. . . . . . . . . . . . . . . . . . . . .
7.1 Taking limits as n
. . . . . . . . . . . . . .
7.2 Flipping an inﬁnite number of coins
Interpretation and examples . . . . . . . . . . . . . . . . . . .
7.3
7.4 Why isn’t every one a frequentist? . . . . . . . . . . . . . . .

→ ∞

8 The Boolean algebra of Caticha’s temporal ﬁlters

8.1 The Markov Property . . . . . . . . . . . . . . . . . . . . . .
8.2 Time evolution and the Schr¨odinger equation . . . . . . . . .

1

2
2
3
4
4

4
5
5
6

7
7

9

10
10
12

12
13
14
16
16
17

19
20
22
24
25

25
26
27

1

9 Next:

28

2

Abstract

This paper introduces and studies the basic properties of Cliﬀord algebra
valued conditional measures.

1

Introduction

Probability theory was given a ﬁrm mathematical foundation in 1933, when
Kolmogorov [10] introduced his axioms. By deﬁning probability as an un-
interpreted special case of a positive measure with total unit mass (plus
an additional deﬁnition for independence), the subject exploded with new
results and found innumerable applications. In 1946, Cox (see [4]) showed
that the Kolmogorov axioms for probability are really theorems that follow
from basic desiderata about the representation of partial truth with real
numbers. We owe to Ed Jaynes (see [9]) the discovery of the importance
of Cox’s 1946 work ([4]). After Jaynes, it became clear why the calculus
of probability is so successful in the real world. Probability works because
its axioms axiomatize the right thing: partial truth of a logical proposition
given another. Even more, the rules of probability are unique in the sense
that any other set of consistent rules can be brought into the standard sum
and product rules by a change of scale (or we may say logical gauge). This
is in fact Cox’s main result and it makes futile the enterprise of looking for
alternatives to the calculus of normalized real valued probabilities. It is only
by allowing the partial truth of a proposition to be encoded by an object
other than a real number in the interval [0, 1] that we could ﬁnd alternatives
to the standard theory of probability.

We seek to ﬁnd out what happens when standard probability theory is
modiﬁed by relaxing the axiom that the probability of an event must be a
real number in the interval [0, 1]. We show that, by allowing the measure
of a proposition to take a value in a Cliﬀord Algebra, we automatically ﬁnd
the methods of standard quantum theory without ever introducing anything
speciﬁcally related to nature itself.

The main motivation for this article has come from realizing that the
derivations in Cox [4] still apply if real numbers are replaced by complex
numbers as the encoders of partial truth. This was ﬁrst mentioned by
Youssef [12] and checked in more detail by Caticha [2] who also showed
that non-relativistic Quantum theory, as formulated by Feynman [5], is the
only consistent calculus of probability amplitudes. By measuring propo-
sitions with Cliﬀord numbers we automatically include the reals, complex,
quaternions, spinors and any combination of them (among others) as special
cases.

1

2 The Axioms

In this section we introduce the notation and collect the simple properties
about Boolean and Cliﬀord algebras that will be needed for the deﬁnition
of ψ below.

2.1 The Boolean Algebra

A

A

Let
be a boolean σ-algebra of propositions a, b, c, . . .. We denote by 0 the
false proposition, by 1 the true proposition, by a + b the logical sum, by ab
the logical product and by ¯a the negation. Each proposition b
deﬁnes
the set

∈ A

A b where,

ba : a
{

∈ A }

= b

A

A b =
A

Clearly,
A b is a subset of
products and thus, Ab is a sub algebra of
fact that a = ac + a¯c it follows that

A

that contains b and 0, it is closed for sums and
with b as the unit. From the

Given two propositions a, b

we have,

∈ A

=

A

A c ⊕ A ¯c

A ⊃ A a ⊃ A ab

The set X
say that

A

is called the set of elementary propositions of

(and we

⊂ A
is a σ-algebra of propositions in X) if,

A

1. for x, y

X, xy = 0 whenever x

= y

2. every a

is the sum of propositions in X. We write

∈

∈ A

If

and

B
A
spectively, then
one deﬁnes the truth value of (a, b)
true only when both a
and b
σ-algebra of n copies of

are two Boolean σ-algebras of propositions in X and Y re-
Y if
is a Boolean σ-algebra of propositions in X
×
as the truth value of ab i.e.
n the

are true. We denote by

of propositions in X n. We have,

∈ A × B
∈ B

A × B

A

∈ A
A

n

P

∈ A

⇐⇒

P =

x1x2 . . . xn

(1)

(2)

(3)

(4)

(5)

a =

x

a
x
X
∈

P
Xx
∈

2

6
A

.
0
}
A \ {

and by this we mean that P is always the sum of propositions in X n and
X n is the conjunction of n propositions, one for each copy of X.
each x
Finally we let

∈

∗ =

Notice that these Boolean σ-algebras are nothing but the standard sets
where general measures (in particular probability measures) are deﬁned. We
chose the notation of logical sums and products instead of the traditional
set notation of unions and intersections to emphasize the fact that we are
interested in the encoding of partial truth of logical propositions, but this is
only a choice of notation and there is a complete one to one correspondence
between the two languages. As general references see e.g. Halmos [6] or
Chow and Teicher [3].

2.2 The algebra of Cliﬀord numbers

G

G

be an arbitrary ﬁnite dimensional Cliﬀord Algebra with real scalars.
Let
We try to follow the notation in [8]. We denote the elements of
by capital
letters like, A, B, C, . . .. A general Cliﬀord number M always expands as
the sum of its scalar, vector, bivector, etc parts like:

G

M = < M >0 + < M >1 + < M >2 + . . .

(6)

= α + u + B + . . .

If u and v are vectors
then their geometric (Cliﬀord) product uv can be decomposed into a

Where < M >k denotes the k-vector part of M .
in
symmetric part u

v and an antisymmetric part u

v as

G

·

uv =

(uv + vu) +

(uv

vu)

1
2
= u

v + u

v

∧

·

∧

−

1
2

The inner product between two vectors is always a scalar and their wedge
product is always a bivector. The operation of reversion of a cliﬀord number
M is denoted by M † and deﬁned as a linear operation with the properties,

α† = α, u† = u,

(M N )† = M †N †

where α is a scalar, u is a vector, and M and N are arbitrary Cliﬀord
numbers. The euclidean inner product on

is given by,

< M, N >

=< M †N >0

(10)

(7)

(8)

(9)

G

G

3

2.3 Deﬁnition of ψ

By a cliﬀord algebra valued conditional measure (or simply a ψ) we mean a
function,

ψ :

∗

A × A
(a, c)

7−→ G
7−→

ψ(a, c)

satisfying the following two axioms:

(I) If c

b then

⇒

ψ(a, c) = ψ(ab, c)

(II) If

a1, a2, . . .
{

} ⊂ A

, with ajak = 0 for j

= k, then

ψ

aj, c

=

ψ(aj, c)





Xj





Xj

(11)

(12)

(13)

Since the only property a proposition in

always has is its truth value,
we can interpret ψ(a, c) as the cliﬀord number that represents the truth in
a when c is certain. Axiom (I) says that c is certain (e.g. take a = 1 and
b = c) and axiom (II) says that the whole truth of a for a given c is always
the sum of the truths of its separate parts.

A

2.3.1 The truth of 0

By taking each aj = 0 in (13) we get,

ψ(0, c) = ψ(0, c) + ψ(0, c) + . . .

(14)

and therefore, ψ(0, c) is either 0
or unbounded but if it is unbounded
then all the propositions will be assigned an unbounded value since ψ(a, c) =
ψ(a + 0, c) = ψ(a, c) + ψ(0, c). Hence,

∈ G

ψ(0, c) = 0 for all c

∗

∈ A

(15)

3 The spaces Hc

The functions ψ, as deﬁned by (12) and (13), are speciﬁed independently
∗. So far, there is no link between the ψ in the domain of
at each c
discourse of c, i.e. ψ(
, c) and ψ in the more specialized domain of discourse
·
of bc, i.e. ψ(
, bc). We shall talk about changing domains of discourse in the
·
next section but in this section we describe the important properties that

∈ A

4

6
∈ A

the functions ψ(
, c) have as functions of their ﬁrst argument only, for ﬁxed
·
c
∗. To simplify the notation simply write ψ(a) instead of ψ(a, c) in
the formulas below. Thus, whenever the background proposition c is not
A c with
subject to change we take ψ as any σ-additive function deﬁned on
values in
. The condition (12) is automatically satisﬁed since c is the true
proposition in

G

Let Hc be the set of all σ-additive functions deﬁned on

A c with values

A c.

in

.

G

3.1 The Hc are Hilbert spaces

Since the sum of two σ-additive functions and the product of a σ-additive
function by a scalar are still σ-additive functions we have that the Hc are
vector spaces. The scalars are the scalars in
. In principle the ﬁeld of
scalars could be taken as the reals or the complex numbers but it seems
that the reals is all that is needed in most applications.

G

3.1.1 The inner product in Hc

For, ϕ, ψ

Hc deﬁne the real inner product between them by:

∈

< ϕ, ψ > =

< ϕ(x), ψ(x) >

G

=

< ϕ(x)†ψ(x) >0

X
Xx
∈

X
Xx
∈

(16)

(17)

By considering only ψs with ﬁnite norm we make Hc a real Hilbert space.
From now on we assume the ﬁnite norm to be part of the deﬁnition of Hc
itself, i.e.

Hc =

ψ : ψ is σ
{

−

additive on

A c and

< ψ†(x)ψ(x) >0 <

(18)

∞}

X
Xx
∈

G

Notice that the spaces Hc are complete for the inner product (17) since
is complete. When X is a ﬁnite set (i.e.
with the scalar product < ., . >
G
when it contains only a ﬁnite number of propositions) the proof is trivial,
just use the fact that if
Hc is a Cauchy sequence then for each
x
X the sequence
G
Hc is the limit of
and thus it converges to some φ(x)
∈ G
the original sequence in Hc. When X is inﬁnite we need to reinterpret the
sums as integrals, (for which we need a measure in X), and we also need to
-measurable densities, but after that, the proof is
reinterpret the ψs as

IN ⊂
∈
IN is also a Cauchy sequence of elements of
∈

φn}n
{
φn(x)
}n
{

and therefore φ

∈

∈

A

5

essentially the standard proof that L2 is complete. An important example
of an inﬁnite X occurs when the propositions in X are labeled with the
vectors in
. In this case the sum in (17) is replaced by the integral with
respect to the standard Lebesgue measure in X.

G

3.2 The isomorphic spaces: H(

A c)

≃

Hc(

)

A

In order to be able to understand the diﬀerences between the current ap-
proach and ordinary probability theory it is convenient to introduce two
other spaces closely related to Hc. These are, the space of all σ-additive
functions on

,

A c with values on
H(

A c) =

ψc :
{

G

A c −→ G |

and the space,

ψc σ-additive on

A c}

(19)

A

) =

Hc(

ψ :
{

A −→ G |

b, ψ(ab) = ψ(a)
}
(20)
Both are Hilbert with the inner product (17) and considering only elements
of ﬁnite norm.

ψ σ-additive on

AND if c

⇒

A

Notice that if ψ

) then its restriction to

Hc(

A

∈

A c belongs to H(

A c),

i.e.

and conversely, if ψc ∈

H(

c

|A

= ψc ∈

A c)
H(
ψ
A c) then the function ψ deﬁned by:
a
ψ(a) = ψc(ac)
∈ A

∀

belongs to Hc(
⇒
and multiplying both sides by c we get, bc = c from where,

) since it is clearly σ-additive and if c

A

b then ¯c + b = 1

ψ(ab) = ψc(abc) = ψc(ac) = ψ(a)

The map ψ
two spaces isomorphic.

→

ψc is obviously linear one to one and onto so it makes the

Consider now two propositions b and c such that c, bc

∗. Then we

∈ A

can write:

≃

H(

Hb(

≃
H(

A c)

A bc)
∈

A c)
H(b
b, c)
A bc) uniquely deﬁnes a function ϕ(
·

In other words each ψ(
, bc)
·
∈
Hb(
A c) and that is all we can say. Since the ψs are unnormalized we
can not write a general product rule as in normalized standard probability
theory. Nevertheless, it is possible to justify a restricted product rule for
independence as we do in section 5 below. When c = 1
we simply
write H(

) instead of H1(

) or H(

∈ A

(21)

A

A

A 1).

6

4 The truth with ψ

The remarkable fact about the functions ψ is that without committing to
, they still allow to tell what propositions
a particular value for ψ(1) in
are true. We show in this section that, b
is considered to be true by
ψ when ψ(¯ba) = 0
. By liberating ordinary probabilities from the
a
constrain that the probability of the whole space must always be ﬁxed at
one, we make the space of all possible assignments of partial truth into a
Hilbert space without losing the ability to identify truth.

∈ A

∈ A

G

∀

4.1 Propositions as operators

Each proposition b
deﬁnes two complementary linear operators on
Hc by multiplication, ˆb, and by addition, ˇb to the ﬁrst argument of ψ. In
symbols

∈ A

ˆbψ(a, c) = ψ(ab, c)
a
∀
ˇbψ(a, c) = ψ(a + b, c)

∈ A
a

∀

∈ A

(22)

(23)

To simplify the notation we often omit the hats and simply write bψ instead
of ˆbψ. From bb = b and b + b = b it follows that ˆb and ˇb are projectors and
therefore they are self-adjoint with eigen values either 0 or 1. We can write,

Theorem 1 The following two complementary statements are true.

1. If ψ

∈

Hc is an eigen vector of the operator ˆb with eigen value 1
then ψ(b) = ψ(1) and we say that ψ makes b true conditional on c.
Conversely, if c
Hc is an eigen vector of the
operator ˆb with eigen value 1.

b then every ψ

⇒

∈

2. If ψ

∈

Hc is an eigen vector of the operator ˇb with eigen value 1 then
ψ(b) = 0 and we say that ψ makes b false conditional on c . Conversely,
Hc is an eigen vector of the operator ˇb with
if c
eigen value 1.

¯b then every ψ

⇒

∈

Proof

1.

ˆbψ = ψ

⇒

ψ(ab, c) = ψ(a, c)

ψ(b, c) = ψ(1, c)

a

∀

∈ A ⇒

7

where the last implication follows by taking a = 1. Conversely, if c
then from (12) we have,

b

⇒

ψ(ab, c) = ψ(a, c)

a

∀

∈ A

Thus, ˆbψ = ψ.

2.

ˇbψ = ψ

⇒

ψ(a + b, c) = ψ(a, c)

ψ(b, c) = ψ(0, c)

a

∀

∈ A ⇒

where the last implication follows by taking a = 0. Conversely,

(c

¯b)

¯c + ¯b = 1

⇒

⇒

⇒

a + bc = a

a

∀

∈ A

Now from the fact that ψ is a function and applying (12) twice we
have,

ψ(a + bc, c) = ψ(a, c)

ψ(ac + bc, c) = ψ(a, c)

ψ(a + b, c) = ψ(a, c)

a

a

a

∀

∀

∀

∈ A

∈ A

∈ A

Thus, ˇbψ = ψ

•

The following theorem elaborates on the same theme.

Theorem 2 Let b
∈ A
sitions in X and let ψ

be an arbitrary proposition in a σ-algebra of propo-
H(

). The following are all equivalent:

∈

A

1. bψ = ψ i.e., ψ makes b true.

2. ¯bψ = 0 i.e., ψ makes ¯b false.

= 0

3.

4.

¯bψ
k
ψ
k

k

k
=

bψ
k

k

Proof:
4. First equivalence follows from ψ = bψ + ¯bψ,
We show that 1
the second equivalence is a property of the norm and the third equivalence
(¯bψ)
is Pythagoras theorem since (bψ)

⇔

⇔

⇔

2

3

⊥

•

8

(24)

(25)

(26)

(27)

(28)

It is evident from this last theorem that the norm in the Hilbert spaces
Hc provides a mechanism for translating the cliﬀord numbers ψ(b) assigned
) into positive real numbers
to the propositions in

by a function ψ

H(

A
¯bψ
k

2 =
k
=

A

∈
2 =
k
2
bψ
k

(1
k
ψ
k

−
2
k

b)ψ

− k

ψ
k

−

bψ

2
k

measuring how close is ψ from making the proposition b true. It is also clear
from (24) and (25) that it is the square of the norm and not just the norm
what is needed. It is only with the square of the norms that we can say that
the amount of truth of ¯b (measured by
2) equals the amount of truth
k
2) minus the amount of
assigned to the true proposition (measured by
k
truth assigned to b (measured by

¯bψ
k

1ψ
k

bψ
k

2).
k

4.2 Commutativity, orthogonality and a Cliﬀord number times

a proposition

Propositional operators can be composed to form other operators. Thus, if
a, b

) we have,

and ψ

H(

∈ A

A

∈
(ˆaˆb)ψ(x) = ˆa(ˆbψ(x)) = ψ(abx) = ˆ(ab)ψ(x)
(ˆaˇb)ψ(x) = ˆaψ(x + b) = ψ(ax + ab)
(ˇbˆa)ψ(x) = ˇbψ(ax) = ψ(ax + b)

and we can see that checks commute with other checks and hats commute
with other hats, but in general, hats don’t commute with checks.

If A

and b

we can deﬁne the operator Ab by,

∈ G

∈ A

(Ab)ψ(x) = A(bψ(x)) = Aψ(bx)

(29)

and similarly for Aˇb. These deﬁnitions allow a very rich algebra of oper-
ators that mix boolean and cliﬀord algebra properties in new ways. One
particularly interesting example of this kind of mix is given by the following
statement: mutually exclusive propositions are orthogonal. More explicitly,
if a, b
< aψ1, bψ2 >= 0 and
and ψ1, ψ2 ∈
pythagoras theorem holds,

) then, ab = 0

∈ A

H(

⇒

A

aψ1 + bψ2k
k

2 =

aψ1k
k

2 +

2

bψ2k
k

(30)

9

5

Independence

If the cliﬀord number ψ(a, c) is interpreted as a representation of the partial
truth of a when we assume c to be certain then there is only one rational
way to deﬁne independence namely:

Preliminary Deﬁnition: We say that ψ makes propositions a and b in
∗ if the additional knowledge
logically independent conditionally on c

A
of one of them does not aﬀect the value of ψ for the other. i.e,

∈ A

ψ(a, bc)

ψ(a, c)

ψ(b, ac)

ψ(b, c)

AN D

=

=

(31)

whenever the conditional ψs exist.

5.1 A restricted product rule

If we try to ﬁnd the value of ψ(ab, c) in terms of the partial truths that ψ
assigns to a and b, then the most general relation is,

ψ(ab, c) = F (ψ(a, c), ψ(a, bc), ψ(b, c), ψ(b, ac))

(32)

where F is an arbitrary function of its arguments. If we assume further that
a and b are logically independent conditionally on c then using (31) the most
general relation becomes,

ψ(ab, c) = F (ψ(a, c), ψ(b, c))

(33)

Let u = ψ(a, c), v = ψ(b, c) and w = ψ(d, c) and use the commutativity
and associativity properties of the logical product to get the following two
properties for the function F :

F (u, v) = F (v, u)

F (F (u, v), w) = F (u, F (v, w))

(34)

(35)

In other words, F must be symmetric and it must satisfy the usual associativ-
ity equation. If the ψs take values only on a commutative subspace of
(e.g.
reals, complex or pseudo scalars) then the only solution is F (u, v) = uv (see
= vu. Given that F (u, v) must be
[2]) but this can not be the solution if uv
symmetric, and that it must reduce to uv when u and v commute, and obvi-
ous solution is given by the symmetrization of the product, i.e. (uv + vu)/2.
In principle, it seems feasible that a modiﬁcation of the standard argument

G

10

6
at least for u, v in some subset of

of Aczel (see [1] or [11]) may yield the symmetrized product as the unique
solution of (35) and (34) for u, v
G
for which (35) is still true. At the present time there is no such proof. In
any case the lack of a proof for the uniqueness is not a deterrent to turn the
formula into the deﬁnition for independence. If it turns out that there are
multiple solutions (which seems highly unlikely) the results obtained from
this particular solution will still be valid. Thus, from now on we say that ψ
makes a and b (logically) independent given c if,

∈ G

ψ(ab, c) =

[ψ(a, c)ψ(b, c) + ψ(b, c)ψ(a, c)]

(36)

1
2

More generally we have,

Deﬁnition: We say that ψ makes a1, a2, . . . , an logically independent given
c if, for k = 1, 2, . . . , n and 1

n

i1 < i2 < . . . < ik ≤

≤

ψ(

aij , c) =

ψ(aσ(i1), c)ψ(aσ(i2 ), c) . . . ψ(aσ(ik ), c)

(37)

k

Yj=1

1
k!

σ
X

where the sum runs over all the permutations, σ of (i1, i2, . . . , ik).

The associativity equation (35) imposes a heavy restriction on the pos-

sible ψ assignments for independent propositions. In fact we have,

Theorem 3 If ψ makes three or more propositions a, b, d, . . . independent
conditionally on c then the cliﬀord numbers u = ψ(a, c), v = ψ(b, c), w =
ψ(d, c), . . . are such that each of them commutes with the anticommutator of
any other two.

Proof:
From (37) it suﬃces to show that v must commute with F (u, w) = (uw +
wu)/2 when a, b, d are independent given c.
The right hand side of (35) simpliﬁes to,

F (u, F (v, w)) =

uvw

uwv + vwu

Similarly the left hand side of (35) is given by,

F (F (u, v), w) =

uvw

vuw + wuv

wvu
}

−

wvu
}

−

Equating (38) to (39) and simplifying we get,

1
4 {

1
4 {

−

−

[v, F (u, w)] = 0

11

(38)

(39)

(40)

−

where [u, v] = uv

vu denotes the usual commutator product

Notice that when the cliﬀord numbers u, v, w, . . . either commute or an-
ticommute with each other then (40) is true. But there are many other
solutions. For example (40) is also true when u, v, w, . . . are arbitrary vec-
tors.

•

5.2

Independence and Orthogonality

The above deﬁnition for independence makes the following statement true:

Theorem 4 If ψ(ab, c) = 0 then ψ(a, c) anticommutes with ψ(b, c) when
and only when ψ makes a and b independent given c.

There is nothing like this in standard probability theory where mutually
exclusive events that are possible (i.e. that have positive probability) are
never independent. We are used to think that this makes sense, for if we
know that one of the events happens then we also know that the other
couldn’t happen. The events are totally linked so they can’t be independent.
This is ﬁne for real numbers, that are commutative, but not with clif-
ford numbers. There is however, a extreme case where the above theorem
is true even in standard probability theory. Suppose that a, b and c are
three mutually exclusive propositions. Then, ψ(ab, c) = ψ(a, c) = ψ(b, c) =
ψ(a, c)ψ(b, c) = 0 and we would have to say that a and b are independent
given c even though neither a nor b are possible given c. Anticommutativity
allows this to happen even when ψ(a, c) and ψ(b, c) are not zero. Two events
can be completely linked (i.e. mutually exclusive) and at the same time be
logically independent from each other! This is as weird as entanglement in
quantum mechanics.

6 Flipping n coins

If
A
every ψ
x

∈
X, i.e.,

∈

A

is a σ-algebra of propositions in X then, by the σ-additivity property,
by just giving ψ(x) for all

) is completely speciﬁed on

H(

ψ(x) =

ψ(y)δy(x)

(41)

where for x, y

X, δy(x) is 1
We consider the following special case.

∈ G

∈

otherwise.

∈ G

A

Xy
X
∈
if x = y and 0

12

6.1 The Binomial experiment with ψs

Let a be an arbitrary proposition and let X =
Clearly

1, 0, a, ¯a
{
is a boolean algebra of propositions in X. From (41) we have

a, ¯a
{

and

A

=

}

.
}

A

ψ(x) = Aδa(x) + Bδ¯a(x)

(42)

∈

A

H(

∈ G

) and A, B

where ψ
. This is the canonical Bernoulli experiment.
There are only two possible outcomes a and ¯a with partial truths encoded
by the cliﬀord numbers A = ψ(a) and B = ψ(¯a). As in standard probability
theory, consider now n independent repetitions of the Bernoulli experiment.
n of elements in
i.e., consider X n with its corresponding boolean algebra
X n (see (5)). From (41) a general ψn ∈

A
n) is given by,

H(

A

ψn(x) =

ψn(y)δy(x)

(43)

Xn
Xy
∈

From the assumption that ψn make the diﬀerent repetitions independent,
we obtain, using (37) that

ψn(x) = ψn(x1, . . . , xn) = Mn(m(x))

(44)

where for each integer k with 0
of the product AkBn
examples for n = 2 and n = 3,

−

is the symmetrization
k and m(x) is the number of a’s in x. Here are some

n, Mn(k)

∈ G

≤

≤

k

ψ2(a, a) = A2 = M2(2), ψ2(a, ¯a) = ψ2(¯a, a) =

[AB + BA] = M2(1)

ψ3(a, a, ¯a) = ψ3(a, ¯a, a) = ψ3(¯a, a, a) =

[A2B + ABA + BA2] = M3(2).

1
2

1
3

Now deﬁne the proposition P k

n by,

n ∈ A

P k

n = “exactly k of the n repetitions is an a”

(45)

Recall that by (22) we have,

n ψn(x) = ψn(P k
P k

n x) =

ψn(x)
0

if P k
n x
otherwise

= 0

(

(46)

By the ﬁrst part of theorem (1) we have that ψn makes P k
n ψn = ψn. So the question is: How far is ψn from making P k
P k
Answer:

n true when
n true?.

ψn −
k

2.
P k
n ψnk

13

6
6.2 Computation of

ψn −

k

2

P k
n ψnk

n,
n , are mutually exclusive propositions hence orthogonal (see (30)) and

To compute this distance we use the fact that P k
1
by pythagoras,

n , and its negation in

P k

A

−

ψn −
− k
k
Let us compute each of these terms. From (17),

ψnk
k

P k
n ψnk

2 =

2

2

P k
n ψnk

and using (43) and (44) we can write,

from where we obtain,

ψ†n(x)ψn(x) =

M †n(m(y1)Mn(m(y2))δy1(x)δy2 (x)

2 =

ψnk
k

Xnh
Xx
∈

ψ†n(x)ψn(x)

i0.

ψn(x) =

Mn(m(y))δy(x)

Xn
Xy
∈

=

M †n(m(x))Mn(m(x))

Xy1,y2
∈

Xn

Xn
Xx
∈

2 =

ψnk
k

2 =

ψnk
k

Mn(m(x))
|

2

2
Mn(j)
|

Xn |
Xx
∈
n
n
j !|
Xj=0  

and replacing in (48) we get,

the last equation followed from the fact that there are
propositions in
X n with exactly j components equal to a. We use the same fact again to
compute the other norm in (47),

(cid:0)

(cid:1)

n
j

to obtain,

P k
n ψnk
k

2 =

ψ†n(P k

n x)ψn(P k

n x)

i0

Xnh
Xx
∈

P k
n ψnk
k

2 =

n
k!|

 

2
Mn(k)
|

Replacing (50) and (52) in (47) we get,

ψn −
k

P k
n ψnk

2 =

n
j !|

2
Mn(j)
|

n
k!|

2
Mn(k)
|

−  

n

Xj=0  

14

(47)

(48)

(49)

(50)

(51)

(52)

(53)

Let us consider the proposition, P f

n deﬁned by,

n,ǫ ∈ A

P f

n,ǫ = “The observed frequency of a’s in n independent repetitions is

k/n with f

ǫ

f + ǫ”

(54)

= 0 when and only when the proportion
in other words for x
of a’s in x = (x1, . . . , xn) is within ǫ from the speciﬁed frequency f . The
proposition P f
n,ǫ is equal to the following disjunction of 2nǫ + 1 mutually
exclusive propositions P k
n :

∈

−
≤
X n, P f

k
n ≤
n,ǫx

hence, from (30) we get,

n(f +ǫ)

P f

n,ǫ =

P k
n

Xk=n(f
−

ǫ)

P f
n,ǫψnk
k

2 =

n(f +ǫ)

Xk=n(f
−

ǫ)

2

P k
n ψnk
k

and from (47) and (53) we can write,

n

n(f +ǫ)

2 =

ψn −
k

P f
n,ǫψnk

n
j !|
Xj=0  
In general this distance increases without limit as n
relative to the size of ψn. Let us deﬁne the relative error by,

2
Mn(j)
|

n
k!|

Xk=n(f
−

→ ∞

ǫ)  

−

2
Mn(k)
|

(57)

but it can converge

∆f

n,ǫ = k

2

P f
n,ǫψnk
ψn −
ψnk
k

2

Using (57) and (50) we have,

n(f +ǫ)

n
k!|

2
Mn(k)
|

∆f

n,ǫ = 1

Xk=n(f
−
n

−

ǫ)  
n
k!|

2
Mn(k)
|

Xk=0  
n,ǫ into three diﬀerent cases.

We separate the computation of ∆f

(55)

(56)

(58)

(59)

15

6
6.3 Case: AB = BA

From (59) we can write the following,

Theorem 5 If AB

= 0, AB = BA and

AkBn
|

k

−

|

=

k
A
|
|

B
|

n
|

k then,

−

where,

∆f

n,ǫ = 1

n(f +ǫ)

−

Xk=n(f
−

ǫ)  

n
k!

pk(1

p)n

−

k

−

p =

2
A
|
|
2 +
A
|
|

B
|

2
|

(60)

(61)

Proof:
Under the conditions of the theorem we have,

2 =
Mn(k)
|
|

2k
A
|
|

B
|

2(n
|

k)

−

replacing this last equation in (59) and noticing that,

n

Xk=0  

n
k!|

2k
A
|

B
|

2(n
|

k) =

−

2 +
A
|
|

B
|

2
|

(cid:16)

n

(cid:17)

we immediately obtain (60) and (61)
It is not always true that for A, B

even when AB =
AB
|
|
BA (take for example A = 1 + αu, B = 1
βu for a unit vector u and
scalars α and β) so the extra condition besides commutativity is needed for
the theorem to be true.

A
|

−

=

B

||

,

|

•
∈ G

6.4 Case: AB = 0

Unlike the real (or complex) numbers, the product of non zero cliﬀord num-
bers can be zero (e.g. take α = β = 1 in the example above) so this case is
not trivial. When AB = 0 the following is true,

Theorem 6 If AB = 0 then,

∆f

n,ǫ =

if 0 < f < 1

(62)

if f = 0

if f = 1

2n
A
|
|
2n +
A
|
|
1

B
|

2n
|

2n
B
|
|
2n +
A
|
|

B
|

2n
|

16






6
Proof:
Notice that when AB = 0 then all the symmetrized products, except the
two extremes are zero, i.e., Mn(k) = 0 for all 0 < k < n and Mn(n) = An
and Mn(0) = Bn. Substituting these values into (59) we obtain (62)

•

6.5 Case: AB =

BA

−

When A and B anticommute we have,

Theorem 7 If AB

= 0, AB =

BA and

AkBn
|

k

−

|

=

k
A
|
|

B
|

n
|

k then,

−

−
n(f +ǫ)

∆f

n,ǫ = 1

−

Xk=n(f
−
n

ǫ)

bn(k)(1

2λn(k))2

−

bn(k)(1

2λn(k))2

−

Xk=0

where bn(k) are the binomial probabilities,

bn(k) =

pk(1

p)n

−

k, with p as before. i.e., p =

n
k!

 

−

2
A
|
|
2 +
A
|
|

B
|

2
|

(64)

and the numbers λn(k) satisfy λn(k) = λn(n
n/2, λn(k) is
the chance of drawing and odd number of RED balls out of k draws without
replacement from a box containing either: n/2 REDS and n/2 BLUES if n
is even or (n + 1)/2 REDS and (n

1)/2 BLUES if n is odd.

k) and for k

≤

−

−

Proof:
Recall that Mn(k) is the symmetrization of AkBn
all the permutations of AkBn
assumed anticommutativity of A with B, each permutation is either AkBn
or

k, i.e., the average over
permutations and, by the
k

k so we have,

k. There are

AkBn

n
k

−

−

−

(cid:1)

(cid:0)

−

−

where ρ(n, k) is an integer. From the fact that
the transformation: A
A, and k
→
.
k)
=
ρ(n, k)
|
|
|
show that,

is invariant under
k) it follows that
In order to prove the theorem it is suﬃcient to

Mn(k)
|
|
(n
−
→

ρ(n, n
|

B, B

→

−

(cid:0)

(cid:1)

(63)

(65)

(66)

Mn(k) =

ρ(n, k)AkBn
n
k

k

−

=

1
|

2λn(k)
|

−

ρ(n, k)
|
|
n
k

(cid:0)

(cid:1)

17

6
since if (66) is true, by using the conditions of the theorem we have,

2 = (1
Mn(k)
|
|

−

2λn(k))2

2k
A
|
|

B
|

2(n
|

k)

−

(67)

2 +
A
and dividing the numerator and the denominator of (58) by (
|
|
we obtain (63).

B
|

2)n
|

≤

Let us show that (66) is true by giving an explicit formula for

ρ(n, k)
|
|
k by the k
n/2. To do this, represent each permutation of AkBn
when k
integers (j1j2 . . . jk) that correspond to the positions of the A’s in increasing
order. For example, for n = 6 and k = 3, the permutation ABABBA is
represented by (136), since the As are found at positions 1, 3 and 6. The
permutation AABBBA is represented by (126) etc. Deﬁne the parity of
(j1 . . . jk) as

−

parity of (j1j2 . . . jk) = (

1)j1+j2...+jk = (

1)j1(

1)j2 . . . (

1)jk

(68)

−

−

−

−

Note that the transposition of an A with a B, located next to it, changes
by one the position of that A in the permutation and hence, the parity of
the permutation obtained after the transposition is always the reverse of
the parity of the original permutation. From this and the fact that we can
transform any permutation into any other by a sequence of transpositions
it follows that two permutations have the same parity if and only if the
number of ﬂips (transpositions) necessary for transforming one permutation
into the other is even.

−

The permutation AkBn

k always corresponds to (12 . . . k) and therefore
an arbitrary permutation (j1j2 . . . jk) will have the same parity as AkBn
k if
the parity of the number of odd integers in the set
j1, j2, . . . , jk}
is the same
{
as the parity of the number of odd integers in the set
1, 2, . . . , k
. In other
{
}
words, if there are an even number of odd integers in the set
1, 2, . . . , k
}
{
then every permutation (j1j2 . . . jk) which also contains an even number of
k but if the number of odd integers in
odd integers can be reorder into AkBn
−
k. Therefore,
AkBn
j1, . . . , jk}
{
we can write

is odd then the permutation reorders into

−

−

−

ρ(n, k)
|
|

=

|

1)j1+j2+...+jk

(
−

|

j1<j2...<jk≤
Thus, if we call Ne the number of permutations with an even number of odd
and we call No the number of permutations with
integers among
an odd number of odds, then,

j1 . . . , jk}
{

X1
≤

n

(69)

(70)

ρ(n, k)
|
|

=

Ne −
|

No|

18

using the fact that Ne + No =

we also have that,

n
k

(cid:0)

(cid:1)
ρ(n, k)
|
|

=

n
k! −

| 

2No|

We now turn to the computation of No. Let No(m) be the total number of
permutations (j1j2 . . . jk) with exactly m of the positions of the A’s being
odd. We have,

No =

k
2 −

1

Xt=0

k

1

−
2

Xt=0






No(2t + 1)

if k is even

No(2t + 1)

if k is odd

No(m) = 


n/2
m

n/2
m
k

−
(cid:0)
(cid:1)(cid:0)
(n+1)/2
m

(cid:1)
(n
−
k
−

1)/2
m

if n is even

if n is odd

where, for 0

m

k

n/2

≤

≤

≤

(cid:1)

(cid:0)

1, 2, . . . , n
{

(cid:1)(cid:0)
this is because the set
contains an equal number of odd and
}
even numbers when n is even but the number of odds is one more than
the number of even when n is odd. So dividing (71) by
and using (72)
and (73) we obtain (66) with λn(k) deﬁned as the theorem says. There are
four diﬀerent formulas for λn(k) depending on the parities of n and k.Let
n/2 we have,
us check one of them. When n and k are both even and k

n
k

(cid:0)

(cid:1)

≤

(71)

(72)

(73)

(74)

λn(k) =

k
2 −

1

 

Xt=0

n/2
2t + 1! 
k
n
k!

 

−

n/2
2t

1!

−

and we can see that (74) is the chance of drawing an odd number of red
balls when drawing at random k balls, without replacement, from a box
containing n/2 red balls and n/2 blue balls. This completes the proof of the
theorem

•

7 The weak law of large numbers

19

7.1 Taking limits as n

In this section we compute

→ ∞

∆f

n,ǫ

lim
n
→∞

for the three cases considered in the previous section.

Theorem 8 If AB
AB =

AkBn
−
|
BA then, for all suﬃciently small ǫ > 0,

k
A
|
|

= 0,

B
|

n
|

=

−

k

|

k and either AB = BA or

−

∆f

n,ǫ =

lim
n
→∞

(

0 if f = p
= p
1 if f

where as before, p = |
A
|
if (f = 0 and

0

B

|

|

2

A
|
2+
|

2 . Moreover, if AB = 0, then

ǫ > 0,

∀

A
|
|

<

B
|

) or (f = 1 and
|

A
|
|

>

B
|

)
|

and either f = 0 or f = 1

∆f

lim
n
→∞

n,ǫ = 



1/2 if

=

B
A
|
|
|
otherwise

|

1

Proof
For the ﬁrst part we use equations, (60) and (63). By the usual gaussian
approximation for the binomial probabilities (e.g. see [?] p.59) we have that
n and any function gn with ﬁnite expectation
for any integers 0
≤
with respect to the standard gaussian,

k2 ≤

k1 ≤

k2

Xk=k1

k2
np
−
√npq

np
k1
−
√npq

Z

bn(k)gn(k) =

gn(np + x√npq)

x2
2 dx

e −

1
√2π

1 + o(n0)
(cid:17)

(cid:16)

(77)

thus, taking k1 = n(f
gn(y) = 0 outside [0, n] we obtain from equation (60) that,

ǫ), k2 = n(f + ǫ), gn(y) = 1 for 0

−

y

≤

≤

n and

∆f

n,ǫ = 1

lim
n
→∞

−

lim
n
→∞ Z

n(f

p+ǫ)

−
√npq

n(f

p
−
−
√npq

ǫ)

1
√2π

x2
2 dx

e −

= p for any 0 < ǫ <

the limits of the integral in
hence, when f
as
equation (78) are both positive or both negative and both going to
0 = 0. On the other hand when f = p for
n
so the desired limit is 1
1 = 0 and this shows that (75) is true for
any ǫ > 0 the desired limit is 1
the commutative case. To show (75) for the anticommutative case we take

→ ∞

−
−

f
|

∞

−

p

|

(75)

(76)

(78)

(79)

gn(y) =

1
4

y2(y

1)2

−

20

6
6
6
which increases like y6 and therefore it has ﬁnite expectation with respect
to the standard gaussian. If we show that

gn(k) = n2(1

2λn(k))2 + o(n0)

then it will follow from (80), (77) and (63) that,

∆f

n,ǫ = 1

lim
n
→∞

lim
n
→∞

R

−

gn(np + x√npq) 1
√2π

x2
2 dx

e −

gn(np + x√npq) 1
√2π

x2
2 dx

e −

(80)

(81)

and by the same reasoning as in the commutative case we obtain (75) for
the anticommutative case. Let us then show (80). Notice that from (74) we
can write,

λn(k) =

W (n, 2t + 1, k)

(82)

−

n(f

p+ǫ)

n(f

ǫ)

−
√npq
p
−
−
√npq
nq
√npq
np
−
√npq

R

k
2 −

1

Xt=0

where the hypergeometric probabilities,

W (n, m, k) =

 

 

−

n/2

m!

n/2
m ! 
k
n
k!
n/2
m !# "
1
nk  

1
nm  

"

1
m!

1

2 ( 1

2 −

1
n )

· · ·

1
nk

−
n
k!#
( 1
2 −
1
k! 1(1

"

=

=

h

n/2

m  
k

m!#

−

(83)

−
expanding the products up to terms of order (1/n) and letting W = W (n, m, k)
we have,

· · ·

−

−

h

m
1
n )
−
i h
1
n )(1

(k

1
m)!
−
2
n )

1

2 ( 1
2 −
(1

1
n )
· · ·
1
k
n )
−
i

( 1
2 −

k

m
n

1

)

−

−

i

W =

k
m! h

 

m

2−

1
{

−

1)

m(m
n

−

+ o(n−

k

−

2m
1)

1)
1
}
−
{
i h
k(k
n + o(n−
−

1)

(k

−

m)(k
n

−

m

1)

−

+ o(n−

1)
}
i

=

=

1

−

(cid:19)

(cid:26)

k

k

1
2

k
m! (cid:18)
 
k
m! (cid:18)
 

1
2 (cid:19)

(cid:26)

1
n

2
n

1 +

m(k

m) + o(n−

1)

−

(cid:27)

[m2 + (k

k] + o(n−

1)

1 +

k(k

1)

−
n

+ o(n−

1)

(cid:27) (cid:26)

(cid:27)

(84)

1

−

−
m)2

−

21

We can readily check that,

k
2 −

1

k

Xt=0  

2t + 1! (cid:18)

(cid:19)

1
2

k

=

1
2

(85)

(86)

(87)

(88)

and that,

k
2 −

1
(2t + 1)(k

Xt=0

2t

1)

−

−

k
2t + 1!

 

1
4

=

k(k

1)2k

−

1

−

From (85), (86), (84) and (82) we have,

Squaring both sides of (87) and multiplying through by 4n2 we obtain,

λn(k)

1
2

−

=

k(k

1)

−
4n

+ o(n−

1).

n2(1

2λn(k))2 =

k2(k

1)2 + o(n0)

−

1
4

−

which is exactly (80). This ends the proof for the anticommutative case.
The second part of the theorem i.e. (76) follows directly from (62) by taking
limits as n

→ ∞•

7.2 Flipping an inﬁnite number of coins

As in standard probability theory there is a subtle nuisance with limits
such as (75) and (76) that needs to be faced in order to have a straight
probabilistic interpretation for laws of large numbers. The problem with
(75) and (76) is that it is not clear how to paste all the ψn together into
. It was due to these kind of problems that modern measure-
one global ψ
theoretic probability theory was born.

∞

A

To be able to make statements about inﬁnite sequences of bernoulli trials
we need to specify a boolean σ-algebra,
∞, that contains at least those
statements. This can be done as in standard probability theory (e.g. see
∞ is deﬁned as the smallest σ-algebra containing the cylinder
[3]), i.e.
sets, in particular it contains the propositions P k
n deﬁned in (45) but now
n refers to the ﬁrst n repetitions in an inﬁnite sequence of bernoulli tri-
∞ we also need to construct the Hilbert space,
als. Having constructed
H(
. Again, the construction is not
trivial but well known in functional analysis as the standard construction of

∞), containing the functions ψ = ψ

A

A

A

∞

22

an inﬁnite tensor product of Hilbert spaces (e.g. see [7]). These standard
constructions allow us to write,

n ψ = P k
P k

n ψn

(89)

where ψ = ψ
statements (75) and (76) as,

∞ ∈

H(

A

∞). Equation (89) can be used to re-write the

Theorem 9 Let X∞ be the space of inﬁnite sequences of independent tosses
of a coin and let
∞ be the smallest σ-algebra containing all the propositions
P k
n about elements in X∞. If for each toss the ψ values for falling heads
and tails are the cliﬀord numbers A and B satisfying,

A

1.

2 +
A
|
|
2. AB

B
|
= 0

2 = 1
|

3. either AB = BA or AB =

AB

4.

AkBn
|

k

−

|

=

k
A
|
|

B
|

n
|

k

−

IN,

0

k

∀

≤

≤

n.

Then, for all suﬃciently small ǫ > 0 the propositions,

−
n

∀

∈

are true.

2

P |
∞

A
|
,ǫ ∈ A

∞

Proof
Under the conditions of the theorem we have from (89) and (75) that when
ψnk
the ψn are normalized i.e. when
k
2

= 1 for all n then,

ψ
k

−

P |

A
n,ǫ ψ
|

k →

0 as n

→ ∞

lim
n
→∞

2

P |

A
n,ǫ ψ = P |
∞

|

2

A
,ǫ ψ = ψ
|

(90)

2
A
,ǫ with eigen value 1 and
|

or equivalently,

so that ψ is an eigen vector of the operator P |
∞
thus, it makes the proposition true

We also have,

Theorem 10 Let X∞ and
pose that the cliﬀord numbers A and B satisfy,

A

∞ be as in the previous theorem but now sup-

1. AB = 0

•

23

6
2.

A
|
|

>

B
|

|

Then for all ǫ > 0 the propositions,

are true.

P 1
∞

,ǫ ∈ A

∞

Proof
Under the conditions of the theorem we have from (89) and (76) that when
the ψn are all of unit norm then

or equivalently,

ψ
k

−

P 1

n,ǫψ

k →

0 as n

→ ∞

lim
n
→∞

n,ǫψ = P 1
P 1
∞

,ǫψ = ψ

(91)

so that ψ is an eigen vector of the operator P 1
∞
thus, it makes the proposition true

,ǫ with eigen value 1 and

•

7.3

Interpretation and examples

The previous two theorems can be interpreted as in standard probability
theory. They say that an inﬁnite sequence of independent tosses of a coin
with ψ( heads ) = A and ψ( tails ) = B will have for sure (relative to ψ)
2 in the ﬁrst case and within ǫ from
a frequency of heads within ǫ from
A
|
|
1 in the AB = 0 case. When AB = 0 the theorem assures us that (again
relative to ψ) the coin will show up heads with frequency 100% whenever
A
|
|

>
The four conditions on A and B that are needed for the AB

= 0 case,
impose heavy restrictions on the possible values that A and B can take but
there are lots of examples. Let p be a real number in the interval [0, 1] and
consider,

B
|

!

|

Example 1

Example 2

A = √p

B =

1

p

(92)

−

p

A = √p

1
where ˆB = σ1σ2 . . . σr is a unit blade, i.e.
p
product of orthogonal (anticommuting) unit vectors σj.

B =

−
it can be factorized into a

(93)

p ˆB

24

6
Example 3

Example 4

(94)
where ˆA and ˆB are both unit blades possibly of diﬀerent dimensions.

B =

p

−

1

A = √p ˆA

p ˆB

A = √p eiα ˆA

(95)
where α and β are scalars, ˆA and ˆB are both unit blades and i is any
multivector such that i2 =
1 and i commutes or anticommutes with
both ˆA and ˆB i.e. i ˆA =

−
ˆAi and i ˆB =

B =

ˆBi

p

−

1

p eiβ ˆB

±

±

It can be readily check that all these examples satisfy the four conditions of
the theorem and hence, coin tosses with these ψs will show up heads with
probability p.

7.4 Why isn’t every one a frequentist?

For the same reason as in probability theory these laws of large numbers
can not be used to deﬁne what we mean by the partial truth that the coin
will show up heads in the next toss since the theorem only says that the
propositions P p
,ǫ are made true by ψ. So any attempt to use the law of
∞
large numbers as the deﬁnition of what ψ is, or means, is therefore circular.

8 The Boolean algebra of Caticha’s temporal ﬁl-

ters

B

B

B

be a σ-algebra of subsets of X. Notice that we are
Let X be a set and let
using the standard set notation for the elements of
instead of the logical
notation used in the rest of the paper. The reason for changing the notation
itself but
is that the boolean σ-algebra that we are trying to deﬁne is not
only based on
. Think of X as the set of possible locations for a point
particle and deﬁne the elementary propositions e(x, t) by the statement: the
particle is at location x at time t. As in [2], e(x, t) is a pure hypothesis not
the result of a measurement. The truth value of e(x, t) can be obtained, at
least in principle, by imagining a ﬁlter that covers all of X except at location
x where it has an inﬁnitesimal hole. This magical ﬁlter materializes only for
an instant at time t and then disappears leaving no trace of its existence. If
after time t we still ﬁnd the particle somewhere then we conclude that e(x, t)
is true. These ﬁlters form a boolean algebra with the deﬁnitions below.

B

25

Let T be a subset of the real line and deﬁne for t

the
proposition e(B, t) as: an elementary ﬁlter at time t with B open. Thus,
e(B, t) is true if and only if the statement: the particle is somewhere in B
at time t is true. We deﬁne the logical product of two elementary ﬁlters as
the operation of putting one on top of the other and we deﬁne the negation
of an elementary ﬁlter as the ﬁlter that closes the holes and opens the rest.
In symbols:

T and B

∈ B

∈

e(B1, t)e(B2, t) = e(B1 ∩
e(B, t) = e(B, t)
e(B1, t) + e(B2, t) = e(B1 ∪

B2, t)

B2, t)

(96)

(97)

(98)

where B = X
(98) follows from (96) and (97) by using De’Morgan’s law i.e.,

B is the complement of B with respect to X. Notice that

\

e(B1, t) + e(B2, t) = e(B1, t)e(B2, t)

= e(B1 ∩
= e(B1 ∪

B2, t)
B2, t)

We also have that for all s, t

T ,

∈

e(B1, s)e(B2, t) = “Filter at time s followed (or on top of) ﬁlter at time t”

e(B1, s) + e(B2, t) = “Filter at time s OR ﬁlter at time t”

e(φ, t) = “Barrier (nothing open) at time t” = 0

e(X, t) = “Absence of ﬁlter (all open) at time t” = 1

(99)

(100)

We deﬁne
e(B, t) i.e.,

F

as the smallest σ-algebra containing the elementary ﬁlters

= σ

e(B, t) : B

F

{

, t

T

∈

}

∈ B

(101)

The boolean algebra of temporal ﬁlters
of events of a stochastic process with state space X.

F

is a spell out of the usual algebra

8.1 The Markov Property

Due to the fact that there is no product rule for the unnormalized ψs we
cannot make use of the standard Markov property of probability theory di-
rectly. The following deﬁnition is all that is needed to recover non relativistic
quantum mechanics,

Deﬁnition: ψ

H(

) is said to have independent segments given c

∈

F

∈ F

26

if for all n = 1, 2, . . ., all times t1 < t2 < . . . < tn in T and all locations
x1, x2, . . . , xn in X the propositions

e(x1, t1)e(x2, t2), e(x2, t2)e(x3, t3), . . . , e(xn

1, tn

1)e(xn

1, tn

1)

−

−

−

−

are independent given c.

8.2 Time evolution and the Schr¨odinger equation

H(

When ψ
) has independent segments, it evolves according to the
Schr¨odringer equation. The usual jargon of quantum mechanics is recovered
with the notation,

F

∈

Probability Amplitude: ψ(e(x, s)e(y, t), e(x0, t0)) is the amplitude for the
particle to go from location x at time s to location y at time t > s given
that it was initially prepared at location x0 at time t0. We denote this
amplitude by K(y, t; x, s).

Wave Function: ψ(e(x, t), e(x0, t0)) is the amplitude of going from the ini-
tial position to location x at time t. It is often denoted by just Ψ(x, t).

Thus, with this notation, a particle which is prepared by e(x0, t0) and for
which ψ
) has independent segments conditionally on this prepara-
F
tion, will satisfy,

H(

∈

Ψ(x, t) =

[K(x, t; y, s)Ψ(y, s) + Ψ(y, s)K(x, t; y, s)]

(102)

since

Ψ(x, t) =

ψ(e(x, t)e(y, s), e(x0, t0))

=

ψ ([e(x0, t0)e(y, s)] [e(y, s)e(x, t)], e(x0, t0))

taking derivatives in (102) with respect to t and evaluating at t = s we
obtain,

∂Ψ(x, t)
∂t

=

1
2

∂K(x, t; y, s)
∂t

t=s

X
Xy
∈
Deﬁning the Hamiltonian H by,

(cid:20)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

t=s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ψ(y, s) + Ψ(y, s)

∂K(x, t; y, s)
∂t

t=s(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(103)

∂K(x, t; y, s)
∂t

=

i
¯h

−

H(x, y, s)

t=s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

27

1
2

X
Xy
∈

X
Xy
∈

X
Xy
∈

where i is any multivector that squares to
1 and that it commutes with all
the ψs. Relabeling s with t we can write Schr¨odinger equation for possible
non-commuting ψs as,

−

i¯h

∂Ψ(x, t)
∂t

=

1
2

X
Xy
∈

[H(x, y, t)Ψ(y, t) + Ψ(y, t)H(y, x, t)]

(104)

when the wave functions Ψ commute with the Hamiltonian, (e.g. when all
the ψs take values in a commutative subspace of
) (104) reduces to the
usual Schr¨odinger equation.

G

9 Next:

Using the Spacetime algebra How to connect the above with the Dirac-

Hestenes equation.

ψ assignments in the real continuous case Minimum Fisher informa-

tion and the Huber-Frieden derivation of the time independent Schr¨odinger
equation.

ψ and Brownian motion Nagasawa’s diﬀusion model.

Comments and conclusion What the hell is this all about and what it

may be likely to become....

References

[1] J. Aczel. Lectures on Functional equations and their appliations. Aca-

demic Press, New York., 1966.

[2] A. Caticha. Consistency, amplitudes and probabilities in quantum the-

ory. Phys. Rev., 1998.

[3] Y. S. Chow and H. Teicher. Probability Theory: Independence, In-
terchangeability, Martingales. Springer Texts in Statistics. Springer-
Verlag, second edition, 1988.

[4] R. T. Cox. Probability, frequency and reasonable expectation. Ameri-

can Journal of Physics, 14:1–13, 1946.

[5] R. P. Feynman. Rev. Mod. Phys., 20:267, 1948.

28

[6] P. R. Halmos. Measure Theory, volume 18 of GTM. Springer-Verlag,

1974.

[7] J. B. Hartle. Am. J. Phys., 36:704, 1968.

[8] D. Hestenes and G. Sobczyk. Cliﬀord Algebra to Geometric Calculus.

D. Reidel, 1984.

[9] E. T. Jaynes.

Probability theory:

The

logic of

science.

http://omega.albany.edu:8008/JaynesBook.html.

[10] A. N. Kolmogorov. Foundations of the Theory of Probability, 1933.

Chelsea, New York, 1950.

[11] C. R. Smith and G. J. Erickson. Probability theory and the associativity
equation.
In P. F. Fougere, editor, Maximum Entropy and Bayesian
Methods, pages 17–30. Dartmouth, USA, Kluwer, Academic publishers,
1990.

[12] S. Youssef. Mod. Phys. Lett., 6:225, 1991.

29


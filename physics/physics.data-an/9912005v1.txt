9
9
9
1
 
c
e
D
 
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
0
0
2
1
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Bayesian Field Theory
Nonparametric Approaches to Density
Estimation, Regression, Classiﬁcation, and
Inverse Quantum Problems

J¨org C. Lemm∗

Institut f¨ur Theoretische Physik I
Universit¨at M¨unster
Wilhelm–Klemm–Str.9
D–48149 M¨unster, Germany

Abstract

Bayesian ﬁeld theory denotes a nonparametric Bayesian approach
for learning functions from observational data. Based on the principles
of Bayesian statistics, a particular Bayesian ﬁeld theory is deﬁned by
combining two models: a likelihood model, providing a probabilistic
description of the measurement process, and a prior model, providing
the information necessary to generalize from training to non–training
data. The particular likelihood models discussed in the paper are those
of general density estimation, Gaussian regression, clustering, classi-
ﬁcation, and models speciﬁc for inverse quantum problems. Besides
problem typical hard constraints, like normalization and positivity
for probabilities, prior models have to implement all the speciﬁc, and
often vague, a priori knowledge available for a speciﬁc task. Nonpara-
metric prior models discussed in the paper are Gaussian processes,
mixtures of Gaussian processes, and non–quadratic potentials. Prior
models are made ﬂexible by including hyperparameters. In particular,
∗Email: lemm@uni-muenster.de, WWW: http://pauli.uni-muenster.de/∼lemm/

1

the adaption of mean functions and covariance operators of Gaussian
process components is discussed in detail. Even if constructed using
Gaussian process building blocks, Bayesian ﬁeld theories are typically
non–Gaussian and have thus to be solved numerically. According to in-
creasing computational resources the class of non–Gaussian Bayesian
ﬁeld theories of practical interest which are numerically feasible is
steadily growing. Models which turn out to be computationally too
demanding can serve as starting point to construct easier to solve
parametric approaches, using for example variational techniques.

Contents

1 Introduction

5

2 Bayesian framework

2.1 Basic model and notations . . . . . . . . . . . . . . . . . . . .
Independent, dependent, and hidden variables . . . . .

9
9
2.1.1
9
2.1.2 Energies, free energies, and errors . . . . . . . . . . . . 11
2.1.3 Posterior and likelihood . . . . . . . . . . . . . . . . . 13
2.1.4 Predictive density . . . . . . . . . . . . . . . . . . . . . 15
2.1.5 Mutual information and learning . . . . . . . . . . . . 16
2.2 Bayesian decision theory . . . . . . . . . . . . . . . . . . . . . 21
2.2.1 Loss and risk . . . . . . . . . . . . . . . . . . . . . . . 21
2.2.2 Loss functions for approximation . . . . . . . . . . . . 21
. . . 24
2.2.3 General loss functions and unsupervised learning
2.3 Maximum A Posteriori Approximation . . . . . . . . . . . . . 25
2.4 Normalization, positivity, and speciﬁc priors . . . . . . . . . . 28
2.5 Empirical risk minimization . . . . . . . . . . . . . . . . . . . 31
2.6
Interpretations of Occam’s razor . . . . . . . . . . . . . . . . . 33
2.7 A priori information and a posteriori control . . . . . . . . . . 34

3 Gaussian prior factors

3.1 Gaussian prior factor for log–probabilities

39
. . . . . . . . . . . 39
3.1.1 Lagrange multipliers: Error functional EL . . . . . . . 39
3.1.2 Normalization by parameterization: Error functional Eg 44
3.1.3 The Hessians HL, Hg . . . . . . . . . . . . . . . . . . . 45
3.2 Gaussian prior factor for probabilities . . . . . . . . . . . . . . 47
. . . . . . . 47

3.2.1 Lagrange multipliers: Error functional EP

2

3.4 Covariances and invariances

3.2.2 Normalization by parameterization: Error functional Ez 49
3.2.3 The Hessians HP , Hz . . . . . . . . . . . . . . . . . . . 50
3.3 General Gaussian prior factors . . . . . . . . . . . . . . . . . . 51
. . . . . . . . . . . . . . . . . . . . . 51
3.3.1 The general case
3.3.2 Example: Square root of P . . . . . . . . . . . . . . . . 53
3.3.3 Example: Distribution functions . . . . . . . . . . . . . 54
. . . . . . . . . . . . . . . . . . . 55
. . . . . . . . . . . . . . . . . 55
3.4.1 Approximate invariance
3.4.2 Approximate symmetries . . . . . . . . . . . . . . . . . 56
3.4.3 Example: Inﬁnitesimal translations . . . . . . . . . . . 57
3.4.4 Example: Approximate periodicity . . . . . . . . . . . 58
3.5 Non–zero means . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.6 Quadratic density estimation and empirical risk minimization
61
3.7 Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.7.1 Gaussian regression . . . . . . . . . . . . . . . . . . . . 64
3.7.2 Exact predictive density . . . . . . . . . . . . . . . . . 71
3.7.3 Gaussian mixture regression (cluster regression) . . . . 74
Support vector machines and regression . . . . . . . . . 75
3.7.4
3.8 Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
. . . . . . . . . . . . . . . . . . . 77
3.9

Inverse quantum mechanics

4 Parameterizing likelihoods: Variational methods

81
4.1 General parameterizations . . . . . . . . . . . . . . . . . . . . 81
4.2 Gaussian priors for parameters . . . . . . . . . . . . . . . . . . 83
4.3 Linear trial spaces . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.4 Mixture models . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.5 Additive models . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.6 Product ansatz . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.7 Decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.8 Projection pursuit . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.9 Neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . 91

5 Parameterizing priors: Hyperparameters

93
5.1 Prior normalization . . . . . . . . . . . . . . . . . . . . . . . . 93
5.2 Adapting prior means . . . . . . . . . . . . . . . . . . . . . . . 98
5.2.1 General considerations . . . . . . . . . . . . . . . . . . 98
5.2.2 Density estimation . . . . . . . . . . . . . . . . . . . . 98
5.2.3 Unrestricted variation . . . . . . . . . . . . . . . . . . 99

3

5.2.4 Regression . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.3 Adapting prior covariances . . . . . . . . . . . . . . . . . . . . 102
5.3.1 General case . . . . . . . . . . . . . . . . . . . . . . . . 102
5.3.2 Automatic relevance detection . . . . . . . . . . . . . . 103
5.3.3 Local smoothness adaption . . . . . . . . . . . . . . . . 104
5.3.4 Local masses and gauge theories . . . . . . . . . . . . . 105
5.3.5
Invariant determinants . . . . . . . . . . . . . . . . . . 106
5.3.6 Regularization parameters . . . . . . . . . . . . . . . . 108
. . . . . . . . . . . . . . 109
Integer hyperparameters . . . . . . . . . . . . . . . . . . . . . 114
. . . . . . . . . . . . . . . . . . . . . . . . . 115

5.4 Exact posterior for hyperparameters
5.5
5.6 Local hyperﬁelds

6 Non–Gaussian prior factors

121
6.1 Mixtures of Gaussian prior factors . . . . . . . . . . . . . . . . 121
6.2 Prior mixtures for density estimation . . . . . . . . . . . . . . 123
6.3 Prior mixtures for regression . . . . . . . . . . . . . . . . . . . 123
6.3.1 High and low temperature limits
. . . . . . . . . . . . 125
6.3.2 Equal covariances . . . . . . . . . . . . . . . . . . . . . 127
6.3.3 Analytical solution of mixture models . . . . . . . . . . 128
6.4 Local mixtures
. . . . . . . . . . . . . . . . . . . . . . . . . . 132
6.5 Non–quadratic potentials . . . . . . . . . . . . . . . . . . . . . 134

7 Iteration procedures: Learning

137
7.1 Numerical solution of stationarity equations
. . . . . . . . . . 137
7.2 Learning matrices . . . . . . . . . . . . . . . . . . . . . . . . . 140
7.2.1 Learning algorithms for density estimation . . . . . . . 140
7.2.2 Linearization and Newton algorithm . . . . . . . . . . 141
7.2.3 Massive relaxation . . . . . . . . . . . . . . . . . . . . 142
7.2.4 Gaussian relaxation . . . . . . . . . . . . . . . . . . . . 146
7.2.5
. . . . . . . . . . . . . . . . . . 147
7.2.6 Boundary conditions . . . . . . . . . . . . . . . . . . . 148
Initial conﬁgurations and kernel methods . . . . . . . . . . . . 150
7.3.1 Truncated equations
. . . . . . . . . . . . . . . . . . . 150
7.3.2 Kernels for L . . . . . . . . . . . . . . . . . . . . . . . 151
7.3.3 Kernels for P . . . . . . . . . . . . . . . . . . . . . . . 153
7.4 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . 154
7.4.1 Density estimation with Gaussian speciﬁc prior
. . . . 154
7.4.2 Density estimation with Gaussian mixture prior . . . . 159

Inverting in subspaces

7.3

4

1 Introduction

The last decade has seen a rapidly growing interest in learning from observa-
tional data. Increasing computational resources enabled successful applica-
tions of empirical learning algorithms in various areas including, for example,
time series prediction, image reconstruction, speech recognition, computer to-
mography, and inverse scattering and inverse spectral problems for quantum
mechanical systems. Empirical learning, i.e., the problem of ﬁnding underly-
ing general laws from observations, represents a typical inverse problem and
is usually ill–posed in the sense of Hadamard [204, 205, 208, 137, 107, 210]. It
is well known that a successful solution of such problems requires additional
a priori information. It is a priori information which controls the general-
ization ability of a learning system by providing the link between available
empirical “training” data and unknown outcome in future “test” situations.
We will focus mainly on nonparametric approaches, formulated directly
in terms of the function values of interest. Parametric methods, on the other
hand, impose typically implicit restrictions which are often extremely diﬃ-
cult to relate to available a priori knowledge. Combined with a Bayesian
framework [10, 14, 30, 136, 187, 161, 16, 64, 196, 97], a nonparametric ap-
proach allows a very ﬂexible and interpretable implementation of a priori
information in form of stochastic processes. Nonparametric Bayesian meth-
ods can easily be adapted to diﬀerent learning situations and have there-
fore been applied to a variety of empirical learning problems, including re-
gression, classiﬁcation, density estimation and inverse quantum problems
[157, 220, 134, 133, 129, 206]. Technically, they are related to kernel and reg-
ularization methods which often appear in the form of a roughness penalty
approach [205, 208, 177, 195, 141, 212, 83, 78, 108, 210]. Computationally,
working with stochastic processes, or discretized versions thereof, is more
demanding than, for example, ﬁtting a small number of parameters. This
holds especially for such applications where one cannot take full advantage
of the convenient analytical features of Gaussian processes. Nevertheless, it
seems to be the right time to study nonparametric Bayesian approaches also
for non–Gaussian problems as they become computationally feasible now at
least for low dimensional systems and, even if not directly solvable, they
provide a well deﬁned basis for further approximations.

In this paper we will in particular study general density estimation prob-
lems. Those include, as special cases, regression, classiﬁcation, and certain
types of clustering. In density estimation the functions of interest are the

5

|

|

|

|

≤

x, h), of producing output (“data”) y under con-
probability densities p(y
dition x and unknown state of Nature h. Considered as function of h, for
ﬁxed y, x, the function p(y
x, h) is also known as likelihood function and a
Bayesian approach to density estimation is based on a probabilistic model
for likelihoods p(y
x, h). We will concentrate on situations where y and x are
real variables, possibly multi–dimensional. In a nonparametric approach, the
variable h represents the whole likelihood function p(y
x, h). That means, h
|
1 for all x and
x, h)
p(y
may be seen as the collection of the numbers 0
all y. The dimension of h is thus inﬁnite, if the number of values which the
variables x and/or y can take is inﬁnite. This is the case for real x and/or y.
A learning problem with discrete y variable is also called a classiﬁca-
tion problem. Restricting to Gaussian probabilities p(y
x, h) with ﬁxed vari-
ance leads to (Gaussian) regression problems. For regression problems the
aim is to ﬁnd an optimal regression function h(x). Similarly, adapting a
mixture of Gaussians allows soft clustering of data points. Furthermore,
extracting relevant features from the predictive density p(y
x, data) is the
Bayesian analogon of unsupervised learning. Other special density estimation
problems are, for example, inverse problems in quantum mechanics where h
represents a unknown potential to be determined from observational data
[134, 133, 129, 206]. Special emphasis will be put on the explicit and ﬂexible
implementation of a priori information using, for example, mixtures of Gaus-
sian prior processes with adaptive, non–zero mean functions for the mixture
components.

≤

|

|

|

Let us now shortly explain what is meant by the term “Bayesian Field
x, h),
Theory”: From a physicists point of view functions, like h(x, y) = p(y
depending on continuous variables x and/or y, are often called a ‘ﬁeld’.1
Most times in this paper we will, as common in ﬁeld theories in physics, not
parameterize these ﬁelds and formulate the relevant probability densities or
f ), directly in
stochastic processes, like the prior p(h) or the posterior p(h
|
terms of the ﬁeld values h(x, y), e.g., p(h
f ). (In
X, y
|
the parametric case, discussed in Chapter 4, we obtain a probability density
f ) = p(ξ
p(h
|
The possibility to solve Gaussian integrals analytically makes Gaussian
processes, or (generalized) free ﬁelds in the language of physicists, very at-
1We may also remark that for example statistical ﬁeld theories, which encompass quan-
tum mechanics and quantum ﬁeld theory in their Euclidean formulation, are technically
similar to a nonparametric Bayesian approach [232, 96, 118].

f ) for ﬁelds h(x, y, ξ) parameterized by ξ.)

f ) = p(h(x, y), x

∈

∈

Y

|

|

6

tractive for nonparametric learning. Unfortunately, only the case of Gaussian
regression is completely Gaussian. For general density estimation problems
the likelihood terms are non–Gaussian, and even for Gaussian priors addi-
tional non–Gaussian restrictions have to be included to ensure positivity and
normalization of densities. Hence, in the general case, density estimation
corresponds to a non–Gaussian, i.e., interacting ﬁeld theory.

As it is well known from physics, a continuum limit for non-Gaussian the-
ories, based on the deﬁnition of a renormalization procedure, can be highly
nontrivial to construct, if possible at all. We will in the following not dis-
cuss such renormalization procedures but focus more on practical, numerical
learning algorithms, obtained by discretizing the problem (typically, but not
necessarily in coordinate space). This is similar, for example, to what is done
in lattice ﬁeld theories.

Gaussian problems live eﬀectively in a space with dimension not larger
than the number of training data. This is not the case for non–Gaussian
problems. Hence, numerical implementations of learning algorithms for non–
Gaussian problems require to discretize the functions of interest. This can
be computationally challenging.

For low dimensional problems, however, many non–Gaussian models are
nowadays solvable on a standard PC. Examples include predictions of one–
dimensional time series or the reconstruction of two–dimensional images.
Higher dimensional problems require additional approximations, like projec-
tions into lower dimensional subspaces or other variational approaches. In-
deed, it seems that a most solvable high dimensional problems live eﬀectively
in some low dimensional subspace.

There are special situations in classiﬁcation where positivity and normal-
ization constraints are fulﬁlled automatically. In that case, the calculations
can still be performed in a space of dimension not larger than the number
of training data. Contrasting Gaussian models, however the equations to be
solved are then typically nonlinear.

Summarizing, we will call a nonparametric Bayesian model to learn a
function one or more continuous variables a Bayesian ﬁeld theory, having
especially in mind non–Gaussian models. A large variety of Bayesian ﬁeld
theories can be constructed by combining a speciﬁc likelihood models with
speciﬁc functional priors (see Tab. 1). The resulting ﬂexibility of nonpara-
metric Bayesian approaches is probably their main advantage.

7

likelihood model

prior model

measurement process (Chap. 2)

generalization behavior (Chap. 2)

parameters (Chap. 3, 4)

hyperparameters (Chap. 5)

describes

is determined by

Examples include

density estimation (Chap. 3)

hard constraints (Chap. 2)

regression (Chap. 3)

Gaussian prior factors (Chap. 3)

classiﬁcation (Sect. 3)

mixtures of Gaussians (Sect. 6)

inverse quantum theory (Sect. 3) non–quadratic potentials (Sect. 6)

Table 1: A Bayesian approach is based on the combination of two models,
a likelihood model, describing the measurement process used to obtain the
training data, and a prior model, enabling generalization to non–training
data. Parameters of the prior model are commonly called hyperparameters.
In “nonparametric” approaches the collection of all values of the likelihood
function itself are considered as the parameters. A nonparametric Bayesian
approach for likelihoods depending on one or more real variables is in this
paper called a Bayesian ﬁeld theory. (Learning is treated in Chapter 7.)

8

The paper is organized as follows: Chapter 2 summarizes the Bayesian
framework as needed for the subsequent chapters. Basic notations are de-
ﬁned, an introduction to Bayesian decision theory is given, and the role of
a priori information is discussed together with the basics of a Maximum
A Posteriori Approximation (MAP), and the speciﬁc constraints for density
estimation problems. Gaussian prior processes, being the most commonly
used prior processes in nonparametric statistics, are treated in Chapter 3.
In combination with Gaussian prior models, this section also introduces the
likelihood models of density estimation, (Sections 3.1, 3.2, 3.3) Gaussian re-
gression and clustering (Section 3.7), classiﬁcation (Section 3.8), and inverse
quantum problems (Section 3.9). Notice, however, that all these likelihood
models can also be combined with the more elaborated prior models dis-
cussed in the following sections of the paper. Parametric approaches, useful
if a numerical solution of a full nonparametric approach is not feasible, are
the topic of Chapter 4. Hyperparameters, parameterizing prior processes
and making them more ﬂexible, are considered in Section 5. Two possibil-
ities to go beyond Gaussian processes, mixture models and non–quadratic
potentials, are presented in Section 6. Chapter 7 focuses on learning algo-
rithms, i.e., on methods to solve the stationarity equations resulting from
a Maximum A Posteriori Approximation. In this section one can also ﬁnd
numerical solutions of Bayesian ﬁeld theoretical models for general density
estimation.

2 Bayesian framework

2.1 Basic model and notations

2.1.1 Independent, dependent, and hidden variables

Constructing theories means introducing concepts which are not directly ob-
servable. They should, however, explain empirical ﬁndings and thus have to
be related to observations. Hence, it is useful and common to distinguish
observable (visible) from non–observable (hidden) variables. Furthermore,
it is often convenient to separate visible variables into dependent variables,
representing results of such measurements the theory is aiming to explain,
and independent variables, specifying the kind of measurements performed
and not being subject of the theory.

Hence, we will consider the following three groups of variables

9

1. observable (visible) independent variables x,

2. observable (visible) dependent variables y,

3. not directly observable (hidden, latent) variables h.

This characterization of variables translates to the following factorization
property, deﬁning the model we will study,

p(x, y, h) = p(y

x, h) p(x) p(h).

(1)

|

In particular, we will be interested in scenarios where x = (x1, x2,
analogously y = (y1, y2,
x, h) =
meaning that p(y

) and
) are decomposed into independent components,
i p(xi) factorize. Then,
i p(yi|
Q
p(x, y, h) =

xi, h) and p(x) =

xi, h) p(xi) p(h).

(2)

· · ·

· · ·

Q

|

p(yi|

Yi

Fig.1 shows a graphical representation of the factorization model (2) as a
directed acyclic graph [172, 117, 99, 186]. The xi and/or yi itself can also be
vectors.

∈

The interpretation will be as follows: Variables h

H represent possible
states of (the model of ) Nature, being the invisible conditions for dependent
variables y. The set H deﬁnes the space of all possible states of Nature for
the model under study. We assume that states h are not directly observable
and all information about p(h) comes from observed variables (data) y, x.
A given set of observed data results in a state of knowledge f numerically
represented by the posterior density p(h
|

f ) over states of Nature.
X describe the visible conditions (measure-
ment situation, measurement device) under which dependent variables (mea-
surement results) y have been observed (measured). According to Eq. (1)
they are independent of h, i.e., p(x
h) = p(x). The conditional density
|
p(y
x, h) of the dependent variables y is also known as likelihood of h (under y
given x). Vector–valued y can be treated as a collection of one–dimensional y
with the vector index being part of the x variable, i.e., yα(x) = y(x, α) = y(˜x)
with ˜x = (x, α).

Independent variables x

∈

|

In the setting of empirical learning available knowledge is usually sep-
n
}
(xD, yD) and, to make the problem well deﬁned, additional a priori in-
D, D0). Hypotheses h

arated into a ﬁnite number of training data D =
=
formation D0. For data D

(xi, yi)

≤

≤

{

{

1

i

|

D0 we write p(h
|

f ) = p(h
|

∪

10

x1

❄
y1

■

x2

❄
y2
▼

xn

❄
yn

· · ·

· · ·
✍

✒

h

Figure 1: Directed acyclic graph for the factorization model (1).

represent in this setting functions h(x, y) = p(y
x, h) of two (possibly multi-
dimensional) variables y, x. In density estimation y is a continuous variable
(the variable x may be constant and thus be skipped), while in classiﬁcation
problems y takes only discrete values.
In regression problems on assumes
p(y
x, h) to be Gaussian with ﬁxed variance, so the function of interest be-
comes the regression function h(x) =

dy yp(y

x, h).

|

|

2.1.2 Energies, free energies, and errors

|

R

Often it will turn out to be convenient to work with log–probabilities, un-
normalized probabilities, or energies, instead of probabilities. For example,
the posterior p(h
|

f ) can be written as

f ) = eL(h

|

f ) =

p(h
|

= e−

β(E(h

f )

|

βE(h

f )

=

f )
Z(h
|
f )
Z(H
|
f )) = e−
F (H
|

e−
Z(H
|
βE(h
|

−

|
f )
f )+c(H

f ),

|

with (posterior) log–probability

unnormalized (posterior) probabilities or partition sums

L(h
|

f ) = ln p(h
|

f ),

f ),

Z(h
|

Z(H

f ) =

|

dh Z(h
|

f ),

Z

(posterior) energy

f ) =

E(h
|

ln Z(h
|

f )

1
β

−

11

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

and (posterior) free energy

yielding

F (H

f ) =

ln Z(H

f )

|

|

=

ln

dh e−

βE(h

f ),

|

1
β
1
β

−

−

Z

f ) = e−

βE(h

f ),

|

Z(h
|

|

Z(H

f ) =

dh e−

βE(h

f ),

|

Z

dh represent a (functional) integral, for example over variables (func-

where
tions) h(x, y) = p(y

R

x, h), and

|

c(H

f ) =

ln Z(H

f ) = βF (H

f ).

|

−

|

|

Note that for the sake of simplicity we did not include the β–dependency of
the functions Z, F , c in the notation.

A central topic will be the maximization of the posterior (see Section
2.3) which corresponds to minimizing the posterior energy E(h
f ). Because
|
in the context of regularization theory and empirical risk minimization, an
optimal h∗ is obtained by minimizing an error functional we will often also
refer to the posterior energy E(h
f ) as (regularized) error functional for h.
|
(see Section 2.5).

|

Let us take a closer look to the integral over model states h. The variables
h represent the parameters describing the data generating probabilities or
x, h). In this paper we will mainly be interested in “nonpara-
likelihoods p(y
metric” approaches where the (x, y, h)–dependent numbers p(y
x, h) itself
are considered to be the primary degrees of freedom which “parameterize”
the model states h. Then, the integral over h is an integral over a set of
real variables indexed by x, y, under additional positivity and normalization
condition.

|

dh

→ Z  

dp(y

x, h)

.

|

!

Z

x,y
Y
f )
Mathematical diﬃculties arise for the case of continuous x, y where p(h
|
represents a stochastic process. and the integral over h becomes a functional
integral over (non–negative and normalized) functions p(y
x, h). For Gaus-
sian processes such a continuum limit can be deﬁned [46, 72, 212, 135] while

|

(12)

12

the construction of continuum limits for non–Gaussian processes is highly
non–trivial (See for instance [43, 33, 96, 232, 174, 217, 218, 31, 182] for per-
turbative approaches or [72] for non–perturbative φ4–theory.) In this paper
we will take the numerical point of view where all functions are considered
to be ﬁnally discretized, so the h–integral is well–deﬁned (“lattice regular-
ization” [37, 189, 150]).

Varying the parameter β generates an exponential family of densities
which is frequently used in practice by (simulated or deterministic) annealing
techniques for minimizing free energies [106, 144, 185, 38, 1, 188, 226, 63, 227,
228]. In physics β is known as inverse temperature and plays the role of a
Lagrange multiplier in the maximum entropy approach to statistical physics.
Inverse temperature β can also be seen as an external ﬁeld coupling to the
energy. Thus, the free energy F (H
f )) is a generating function for
the cumulants of the energy, meaning that cumulants of E can be obtained
by taking derivatives of F with respect to β [60, 7, 11, 150].

f ) (or c(H

|

|

For the sake of clarity, we have chosen to use the common notation for
conditional probabilities also for energies and the other quantities derived
from them. The same conventions will also be used for other probabilities,
so we will write for example for likelihoods

p(y

x, h) = e−

β′(E(y

x,h)

F (Y

|

−

x,h)),

|

|

(13)

Y . Temperatures may be diﬀerent for prior and likelihood. Thus, we

for y
∈
may choose β′

= β in Eq. (13) and Eq. (3).

2.1.3 Posterior and likelihood

Bayesian approaches require the calculation of posterior densities. Model
states h are commonly speciﬁed by giving the data generating probabilities
x, h). Posteriors are linked to likelihoods by Bayes’ theorem
or likelihoods p(y

|

B) =

p(A
|

p(B

A)p(A)
|
p(B)

,

which follows at once from the deﬁnition of conditional probabilities, i.e.,
p(A, B) = p(A
|

A)p(A). Thus, one ﬁnds

B)p(B) = p(B

f ) =

p(h
|

p(D

|
h) p(h
|
D0)

|
p(D

|

D0)

p(yD|

=

xD, h) p(h
|
xD, D0)

D0)

p(yD|

(14)

(15)

13

6
=

i p(xi, yi|

i p(xi, yi|

D0)

h)p(h
|
h)p(h
|

D0)

dh
Q

=

i p(yi|

xi, h)p(h
|

i p(yi|

xi, h)p(h
|

D0)

D0)

,

dh
Q

xD, D0, h) = p(yD|

xD, h) for the training data likelihood of h and
Q
Q
D0). The terms of Eq. (15) are in a Bayesian context often

R

R

using p(yD|
p(h
|
referred to as

D0, xi) = p(h
|

(16)

(17)

posterior =

likelihood

prior

∗

evidence

.

Eqs.(16) show that the posterior can be expressed equivalently by the joint
xi, h). When working
h) or conditional likelihoods p(yi|
likelihoods p(yi, xi|
with joint likelihoods, a distinction between y and x variables is not neces-
sary. In that case x can be included in y and skipped from the notation.
If, however, p(x) is already known or is not of interest working with condi-
tional likelihoods is preferable. Eqs.(15,16) can be interpreted as updating
(or learning) formula used to obtain a new posterior from a given prior prob-
ability if new data D arrive.

In terms of energies Eq. (16) reads,

f ) =

p(h
|

β

e−
Z(YD|

P

i E(yi

|

xi,h)

βE(h

D0)

−

|

xD, h) Z(H

D0) Z

|

dh

Z(YD|
e−

β

xD, h) Z(H
xi,h)
i E(yi

|
βE(h

D0)
D0)

,

−

|

|

(18)

where the same temperature 1/β has been chosen for both energies and the
normalization constants are

P

Z(YD|

xD, h) =

dyi e−

βE(yi

xi,h),

|

Z(H

D0) =

|

dh e−

βE(h

D0).

|

Yi Z

Z

The predictive density we are interested in can be written as the ratio of

two correlation functions under p0(h),

p(y

|

x, f ) = < p(y
|
< p(y
|
<
dh p(y

=

x, h) >H
x, h)

f

|

i p(yi|
Q
x, h) e−
βEcomb

Q
|
dh e−

=

R

R

xi, h) >H

i p(yi|
xi, h) >H
βEcomb

D0

|

D0

|

,

where <
= p(h
|

>H

D0 denotes the expectation under the prior density p0(h)
D0) and the combined likelihood and prior energy Ecomb collects the

· · ·

|

14

(19)

(20)

(21)

(22)

(23)

h–dependent energy and free energy terms

Ecomb =

E(yi|

xi, h) + E(h
|

D0)

F (YD|

−

xD, h),

Xi

with

F (YD|

xD, h) =

ln Z(YD|

xD, h).

1
β

−

Going from Eq. (22) to Eq. (23) the normalization factor Z(H
in numerator and denominator has been canceled.

|

D0) appearing

y, h) = (vxy, p(

We remark that for continuous x and/or y the likelihood energy term
xi, h) describes an ideal, non–realistic measurement because realistic
E(yi|
measurements cannot be arbitrarily sharp. Considering the function p(
, h)
as element of a Hilbert space its values may be written as scalar product
, h) ) with a function vxy being also an element in that
p(x
|
Hilbert space. For continuous x and/or y this notation is only formal as vxy
becomes unnormalizable. In practice a measurement of p(
, h) corresponds
to a normalizable v˜x˜y =
dx ϑ(x, y)vxy where the kernel ϑ(x, y) has to
ensure normalizability. (Choosing normalizable v˜x˜y as coordinates the Hilbert
space of p(
, h) is also called a reproducing kernel Hilbert space [170, 104,
105, 212, 135].) The data terms then become

dy

·|·

·|·

·|·

·|·

R

R

˜xi, h) =

p(˜yi|

dy

dx ϑi(x, y)p(y, x
|
h)

dy ϑi(x, y)p(y, x
R
|

h)

.

R

R

xi, h) is understood as limit ϑ(x, y)

yi)
The notation p(yi|
and means in practice that ϑ(x, y) is very sharply centered. We will assume
that the discretization, ﬁnally necessary to do numerical calculations, will
implement such an averaging.

xi)δ(y

δ(x

→

−

−

2.1.4 Predictive density

Within a Bayesian approach predictions about (e.g., future) events are based
on the predictive probability density, being the expectation of probability for
y for given (test) situation x, training data D and prior data D0

(24)

(25)

(26)

(27)

(28)

p(y

x, f ) =

|

f ) p(y

dh p(h
|
|
x, h) >H

Z
= < p(y

x, h)

f .

|

|

15

ˆ=p(y

x, hi), hi ∈

|

H

✛

p(y

x, htrue)

|

✒

F

p(y

x, f )

|

x, f ) for a state of knowledge f =
Figure 2: The predictive density p(y
|
f (D, D0) is in the convex hull spanned by the possible states of Nature hi
characterized by the likelihoods p(y
x, hi). During learning the actual pre-
x, f ) tends to move stochastically towards the extremal
dictive density p(y
point p(y

x, htrue) representing the “true” state of Nature.

|

|

|

· · ·

>H

f denotes the expectation under the posterior p(h
|

f ) =
Here <
p(h
D, D0), the state of knowledge f depending on prior and training data.
|
Successful applications of Bayesian approaches rely strongly on an adequate
choice of the model space H and model likelihoods p(y

x, h).

|

|

P

i p(y

x, f ) = =

Note that p(y

x, hi) p(hi|
∈

f ) is in the convex cone spanned
|
|
H, and typically not equal to one of
by the possible states of Nature h
x, h). The situation is illustrated in Fig. 2. During learning the
these p(y
x, h). Because
predictive density p(y
the training data are random variables, this approach is stochastic. (There
exists an extensive literature analyzing the stochastic process of learning and
generalization from a statistical mechanics perspective [57, 58, 59, 215, 222,
165]).

x, f ) tends to approach the true p(y

|

|

|

2.1.5 Mutual information and learning

The aim of learning is to generalize the information obtained from training
data to non–training situations. For such a generalization to be possible,
there must exist a, at least partially known, relation between the likelihoods

16

xi, h) for training and for non–training data. This relation is typically

p(yi|
provided by a priori knowledge.

One possibility to quantify the relation between two random variables
y1 and y2, representing for example training and non–training data, is to
calculate their mutual information, deﬁned as

(29)

(30)

(31)

M(Y1, Y2) =

p(y1, y2) ln

Y1,y2
Xy1
∈

∈

Y2

p(y1, y2)
p(y1)p(y2)

.

It is also instructive to express the mutual information in terms of (average)
information content or entropy, which, for a probability function p(y), is
deﬁned as

H(Y ) =

ln

p(y) ln p(y).

−

Y
Xy
∈

We ﬁnd

M(Y1, Y2) = H(Y1) + H(Y2)

H(Y1, Y2),

−

meaning that the mutual information is the sum of the two individual en-
tropies diminished by the entropy common to both variables.

To have a compact notation for a family of predictive densities p(yi|

xi, f )
) consisting of all possible values xi and
), so we can write

we choose a vector x = (x1, x2,
corresponding vector y = (y1, y2,

· · ·
· · ·

p(y

x, f ) = p(y1, y2,

x1, x2,

, f ).

· · · |

· · ·

|

(32)

We now would like to characterize a state of knowledge f corresponding to
predictive density p(y
x, f ) by its mutual information. Thus, we generalize
the deﬁnition (29) from two random variables to a random vector y with
components yi, given vector x with components xi and obtain the conditional
mutual information

|

or

M(Y

x, f ) =

|

dyi

p(y

x, f ) ln

!

|

Z  

Yi

x, f )

,

xj, f )

p(y
|
j p(yj|

Q

M(Y

x, f ) =

|

dyi H(Yi|

x, f )

H(Y

x, f )

,

−

|

(cid:19)

(cid:18)Z

(33)

(34)

in terms of conditional entropies

H(Y

x, f ) =

dy p(y

x, f ) ln p(y

x, f ).

(35)

|

− Z

|

|

17

|

|

(36)

(37)

In case not a ﬁxed vector x is given, like for example x = (x1, x2,
), but a
density p(x), it is useful to average the conditional mutual information and
dx p(x) in the above formulae.
conditional entropy by including the integral

· · ·

It is clear from Eq. (33) that predictive densities which factorize

R

p(y

x, f ) =

xi, f ),

p(yi|

Yi
have a mutual information of zero. Hence, such factorial states do not allow
any generalization from training to non–training data. A special example are
the possible states of Nature or pure states h, which factorize according to
the deﬁnition of our model

p(y

x, h) =

xi, h).

p(yi|

Yi
Thus, pure states do not allow any further generalization. This is consistent
with the fact that pure states represent the natural endpoints of any learning
process.

It is interesting to see, however, that there are also other states for which
the predictive density factorizes. Indeed, from Eq. (37) it follows that any
(prior or posterior) probability p(h) which factorizes leads to a factorial state,

p(h) =

p(h(xi))

p(y

x, f ) =

⇒

|

Yi

xi, f ).

p(yi|

Yi

(38)

i.e., (non–local) learning,

is impossible when

This means generalization,
starting from a factorial prior.

A factorial prior provides a very clear reference for analyzing the role
of a–priori information in learning.
In particular, with respect to a prior
factorial in local variables xi, learning may be decomposed into two steps,
one increasing, the other lowering mutual information:

1. Starting from a factorial prior, new non–local data D0 (typically called
a priori information) produce a new non–factorial state with non–zero
mutual information.

2. Local data D (typically called training data) stochastically reduce the
mutual information. Hence, learning with local data corresponds to a
stochastic decay of mutual information.

18

Pure states, i.e., the extremal points in the space of possible predictive
densities, do not have to be deterministic. Improving measurement devices,
stochastic pure states may be further decomposed into ﬁner components g,
so that

xi, h) =

p(yi|

dg p(g) p(yi|

xi, g).

Z

(39)

xi, g).

Imposing a non–factorial prior p(g) on the new, ﬁner hypotheses g enables
again non–local learning with local data, leading asymptotically to one of
the new pure states p(yi|
Let us exemplify the stochastic decay of mutual information by a simple
numerical example. Because the mutual information requires the integration
over all yi variables we choose a problem with only two of them, ya and
yb corresponding to two x values xa and xb. We consider a model with
four states of Nature hl, 1
x, h) =
(√2πσ)−

hi(x))2/(2σ2)) and local means hl(xj) =

4, with Gaussian likelihood p(y

1 exp (

(y

≤

≤

1.

l

|

Selecting a “true” state of Nature h, we sample 50 data points Di =
(xi, yi) from the corresponding Gaussian likelihood using p(xa) = p(xb) =
D0) we
0.5. Then, starting from a given, factorial or non–factorial, prior p(h
|
sequentially update the predictive density,

−

−

±

p(y

x, f (Di+1,

, D0)) =

|

· · ·

p(y

x, hl) p(hl|

|

Di+1,

· · ·

, D0),

(40)

by calculating the posterior

p(hl|

Di+1,

· · ·

, D0) =

p(yi+1|

xi+1, hl) p(hj|
xi+1, Di · · ·

p(yi+1|

Di · · ·
, D0)

, D0)

.

(41)

It is easily seen from Eq. (41) that factorial states remain factorial under
local data.

Fig. 3 shows that indeed the mutual information decays rapidly. Depend-
ing on the training data, still the wrong hypothesis hl may survive the decay
of mutual information. Having arrived at a factorial state, further learning
has to be local. That means, data points for xi can then only inﬂuence the
predictive density for the corresponding yi and do not allow generalization
to the other yj with j

= i.

For a factorial prior p(hl) = p(hl(xa))p(hl(xb)) learning is thus local from
the very beginning. Only very small numerical random ﬂuctuations of the
mutual information occur, quickly eliminated by learning. Thus, the predic-
tive density moves through a sequence of factorial states.

4

Xl=1

19

6
(a)

(c)

(e)

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

posterior

mutual information

10

20

30

40

50

10

20

30

40

50

(b)

(d)

(f)

10

20

30

40

50

10

20

30

40

50

10

20

30

40

50

10

20

30

40

50

xi, hl) with means

Figure 3: The decay of mutual information during learning: Model with 4
possible states hl representing Gaussian likelihoods p(yi|
1
±
f ) (a, c, e,
for two diﬀerent xi values. Shown are posterior probabilities p(hl|
on the left hand side, the posterior of the true hl is shown by a thick line) and
mutual information M(y) (b, d, f , on the right hand side) during learning
(a, b): The mutual information decays during learning
50 training data.
and becomes quickly practically zero. (c, d): For “unlucky” training data
the wrong hypothesis hi can dominate at the beginning. Nevertheless, the
mutual information decays and the correct hypothesis has ﬁnally to be found
through “local” learning. (e, f ): Starting with a factorial prior the mutual
information is and remains zero, up to artiﬁcial numerical ﬂuctuations. For
(e, f ) the same random data have been used as for (c, d).

0.007

0.006

0.005

0.004

0.003

0.002

0.001

0.007

0.006

0.005

0.004

0.003

0.002

0.001

     -16
2. 10

      -16
1.5 10

     -16
1. 10
     -17
5. 10

      -17
-5. 10
      -16
-1. 10

       -16
-1.5 10

20

2.2 Bayesian decision theory

2.2.1 Loss and risk

In Bayesian decision theory a set A of possible actions a is considered, to-
gether with a function l(x, y, a) describing the loss l suﬀered in situation x if
y appears and action a is selected [14, 119, 172, 187]. The loss averaged over
test data x, y, and possible states of Nature h is known as expected risk,

r(a, f ) =

x, f ) l(x, y, a).

dx dy p(x) p(y

Z

|
= < l(x, y, a) >X,Y
= < r(a, h) >H

f

f

|

|

>X,Y

f denotes the expectation under the joint predictive density

where <
p(x, y

· · ·

|

f ) = p(x)p(y

x, f ) and

|

|

r(a, h) =

dx dy p(x) p(y

x, h) l(x, y, a).

(45)

The aim is to ﬁnd an optimal action a∗

Z

Ar(a, f ).
a∗ = argmina
∈

2.2.2 Loss functions for approximation

Log–loss: A typical loss function for density estimation problems is the log–
loss

l(x, y, a) =

b1(x) ln p(y

x, a) + b2(x, y)

−

|

with some a–independent b1(x) > 0, b2(x, y) and actions a describing proba-
bility densities

Choosing b2(x, y) = p(y

|

Z

dy p(y

x, a) = 1,

x

X,

a

A.

|

∀
x, f ) and b1(x) = 1 gives

∈

∀

∈

|

|

r(a, f ) =

dx dy p(x)p(y

x, f ) ln

p(y
p(y

x, f )
x, a)

|
|

x, a) ) >X,

|

Z

p(y
p(y

x, f )
x, a)

= < ln

|
|
= < KL( p(y

>X,Y

f

|
x, f ), p(y

|

21

(42)

(43)
(44)

(46)

(47)

(48)

(49)

(50)

(51)

which shows that minimizing log–loss is equivalent to minimizing the (x–
averaged) Kullback–Leibler entropy KL( p(y
x, a) )[114, 115, 11, 41,
48].

x, f ), p(y

|

|

While the paper will concentrate on log–loss we will also give a short
summary of loss functions for regression problems. (See for example [14, 187]
for details.) Regression problems are special density estimation problems
where the considered possible actions are restricted to y–independent func-
tions a(x).

Squared–error loss: The most common loss function for regression prob-
It reads for one–
lems (see Sections 3.7, 3.7.2) is the squared–error loss.
dimensional y

l(x, y, a) = b1(x) (y

a(x))2 + b2(x, y),

−
with arbitrary b1(x) > 0 and b2(x, y). In that case the optimal function a(x)
is the regression function of the posterior which is the mean of the predictive
density

a∗(x) =

dy y p(y

x, f ) = < y >Y

x,f .

|

Z

This can be easily seen by writing

a(x))2 =

(y

−

=

y
(cid:16)
y
(cid:16)
2
−

−

−
y
(cid:16)

< y >Y

x,f + < y >Y

< y >Y

x,f

|
(cid:17)
< y >Y

−

2

+

x,f

|

(cid:17) (cid:16)

a(x)
(cid:16)

a(x)

−

−

2

a(x)

x,f −

|

< y >Y

2

(cid:17)
x,f

< y >Y

x,f

(55)

where the ﬁrst term in (55) is independent of a and the last term vanishes
after integration over y according to the deﬁnition of < y >Y

x,f . Hence,

(cid:17)

2

,

(cid:17)

|

|

|

r(a, f ) =

dx b1(x)p(x)

a(x)

< y >Y

−

2

+ const.

x,f

|

(56)

Z

(cid:17)
x,f . Notice that for Gaussian p(y

(cid:16)
This is minimized by a(x) =< y >Y
x, a)
|
with ﬁxed variance log–loss and squared-error loss are equivalent. For multi–
dimensional y one–dimensional loss functions like Eq. (52) can be used when
the component index of y is considered part of the x–variables. Alternatively,
loss functions depending explicitly on multidimensional y can be deﬁned. For
instance, a general quadratic loss function would be

|

(52)

(53)

(54)

|

|

l(x, y, a) =

ak)K(k, k′)(yk′

ak′(x)).

(57)

−

(yk −

Xk,k′

22

with symmetric, positive deﬁnite kernel K(k, k′).

Absolute loss: For absolute loss

l(x, y, a) = b1(x)

y

a(x)

+ b2(x, y),

|

−

|

(58)

with arbitrary b1(x) > 0 and b2(x, y). The risk becomes

r(a, f ) =

dx b1(x)p(x)

dy (a(x)

y) p(y

x, f )

−

|

Z
+

Z
dx b1(x)p(x)

−∞

∞

dy (y

a(x)) p(y

x, f ) + const.

(59)

a(x)

Z

a(x)
a(x)

m(x)

Z

−

−

|

|

Z

Z

= 2

dx b1(x)p(x)

dy (a(x)

y) p(y

x, f ) + const.′, (60)

where the integrals have been rewritten as
m(x)
a(x) +
R

∞m(x) introducing a median function m(x) which satisﬁes
R

−∞
R

−∞
R

=

+

a(x)

m(x)

a(x)
m(x) and
R

∞a(x) =
R

m(x)

Z

−∞

dy p(y

x, f ) =

|

1
2

,

x

∀

∈

X,

(61)

so that

m(x)

a(x)

 Z

−∞

dy p(y

x, f )

|

∞

− Z

m(x)

dy p(y

x, f )

= 0,

|

!

x

∀

∈

X.

(62)

Thus the risk is minimized by any median function m(x).

δ–loss and 0–1 loss : Another possible loss function, typical for classiﬁca-
tion tasks (see Section 3.8), like for example image segmentation [141], is the
δ–loss for continuous y or 0–1–loss for discrete y

l(x, y, a) =

b1(x)δ (y

a(x)) + b2(x, y),

−

−

(63)

with arbitrary b1(x) > 0 and b2(x, y). Here δ denotes the Dirac δ–functional
for continuous y and the Kronecker δ for discrete y. Then

r(a, f ) =

dx b1(x)p(x) p( y = a(x)

x, f ) + const.,

(64)

Z

|

so the optimal a corresponds to any mode function of the predictive density.
For Gaussians mode and median are unique, and coincide with the mean.

23

2.2.3 General loss functions and unsupervised learning

Choosing actions a in speciﬁc situations often requires the use of speciﬁc
loss functions. Such loss functions may for example contain additional terms
measuring costs of choosing action a not related to approximation of the
predictive density. Such costs can quantify aspects like the simplicity, imple-
mentability, production costs, sparsity, or understandability of action a.

Furthermore, instead of approximating a whole density it often suﬃces
like identifying clusters of similar y–values,
to extract some of its features.
ﬁnding independent components for multidimensional y, or mapping to an
approximating density with lower dimensional x. This kind of exploratory
data analysis is the Bayesian analogon to unsupervised learning methods.
Such methods are on one hand often utilized as a preprocessing step but
are, on the other hand, also important to choose actions for situations where
speciﬁc loss functions can be deﬁned.

From a Bayesian point of view general loss functions require in general
an explicit two–step procedure [123]: 1. Calculate (an approximation of) the
predictive density, and 2. Minimize the expectation of the loss function under
that (approximated) predictive density. (Empirical risk minimization, on the
other hand, minimizes the empirical average of the (possibly regularized) loss
function, see Section 2.5.) (For a related example see for instance [130].)

For a Bayesian version of cluster analysis, for example, partitioning a pre-
dictive density obtained from empirical data into several clusters, a possible
loss function is

l(x, y, a) = (y

a(x, y))2,

−

(65)

with action a(x, y) being a mapping of y for given x to a ﬁnite number of
cluster centers (prototypes). Another example of a clustering method based
on the predictive density is Fukunaga’s valley seeking procedure [56].

For multidimensional x a space of actions a(Pxx, y) can be chosen de-

pending only on a (possibly adaptable) lower dimensional projection of x.

For multidimensional y with components yi it is often useful to identify
independent components. One may look, say, for a linear mapping ˜y =
My minimizing the correlations between diﬀerent components of the ‘source’
variables ˜y by minimizing the loss function

l(y, y′, M) =

˜yi ˜y′j,

=j
Xi

(66)

with respect to M under the joint predictive density for y and y′ given

24

6
x, x′, D, D0. This includes a Bayesian version of blind source separation (e.g.
applied to the so called cocktail party problem [12, 6]), analogous to the
treatment of Molgedey and Schuster [149]. Interesting projections of mul-
tidimensional y can for example be found by projection pursuit techniques
[54, 95, 100, 195].

2.3 Maximum A Posteriori Approximation

In most applications the (usually very high or even formally inﬁnite dimen-
sional) h–integral over model states in Eq. (23) cannot be performed exactly.
The two most common methods used to calculate the h integral approxi-
mately are Monte Carlo integration [142, 84, 88, 184, 14, 65, 185, 18, 203,
221, 64, 157, 167, 158] and saddle point approximation [14, 40, 27, 159, 15,
232, 187, 64, 71, 123]. The latter approach will be studied in the following.
For that purpose, we expand Ecomb with respect to h around some h∗

Ecomb(h) = e(∆h,

)E(h)

∇

= Ecomb(h∗) + (∆h,

(h∗)) +

(∆h, H(h∗)∆h) +

h=h∗

(cid:12)
(cid:12)
(cid:12)

∇

1
2

(67)

· · ·

h∗), gradient
) denoting scalar products. In case p(y

(not acting on ∆h), Hessian H, and round
with ∆h = (h
x, h) is parameterized
,
brackets (
independently for every x, y the states h represent a parameter set indexed
by x and y, hence

−
· · ·

· · ·

∇

|

∇

(h∗) =

δEcomb(h)
δh(x, y) (cid:12)
h=h∗
(cid:12)
(cid:12)
(cid:12)
δ2Ecomb(h)
(cid:12)
δh(x, y)δh(x′, y′) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

h=h∗

=

δEcomb(p(y′
δp(y

|
x, h)

x′, h))

,

h=h∗

|

=

δ2Ecomb(p(y′′
δp(y

|
x, h)δp(y′|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
x′′, h))
(cid:12)
x′, h) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

h=h∗

H(h∗) =

are functional derivatives [90, 98, 26, 32] (or partial derivatives for discrete
x, y) and for example

(68)

(69)

(∆h,

(h∗)) =

dx dy (h(x, y)

h∗(x, y))

(h∗)(x, y).

(70)

∇

Z

−

∇

Choosing h∗ to be the location of a local minimum of Ecomp(h) the linear
term in (67) vanishes. The second order term includes the Hessian and
corresponds to a Gaussian integral over h which could be solved analytically

dh e−

β(∆h,H∆h) = π

d

2 β−

2 (det H)−

d

1

2 ,

(71)

Z

25

|

for a d–dimensional h–integral. However, using the same approximation for
the h–integrals in numerator and denominator of Eq. (23), expanding then
also p(y
x, h) around h∗, and restricting to the ﬁrst (h–independent) term
x, h∗) of that expansion, the factor (71) cancels, even for inﬁnite d. (The
p(y
result is the zero order term of an expansion of the predictive density in
powers of 1/β. Higher order contributions can be calculated by using Wick’s
theorem [40, 27, 159, 232, 101, 150, 123].) The ﬁnal approximative result for
the predictive density is very simple and intuitive

|

with

p(y

x, f )

p(y

x, h∗),

|

≈

|

h∗ = argminh

HEcomb = argmaxh
∈

D0).
(73)
The saddle point (or Laplace) approximation is therefore also called Maxi-
mum A Posteriori Approximation (MAP). Notice that this can equivalently
be seen as a saddle point approximation for the evidence of the data yD

xD, h)p(h
|

f ) = argmaxh

H p(yD|

H p(h
|
∈

∈

p(yD|

xD, D0) =

dh p(yD|

xD, h)p(h
|

D0).

Z

This equivalence is due to the assumption that p(y
x, h) is slowly varying at
the stationary point and has not to be included in the saddle point approx-
imation for the predictive density. For (functional) diﬀerentiable Ecomb Eq.
(73) yields the stationarity equation,

|

δEcomb(h)
δh(x, y)

= 0.

The functional Ecomb including training and prior data (regularization, sta-
bilizer) terms is also known as (regularized) error functional for h.

In practice a saddle point approximation may be expected useful if the
posterior is peaked enough around a single maximum, or more general, if the
posterior is well approximated by a Gaussian centered at the maximum. For
asymptotical results one would have to require

E(yi|

xi, h) =

Xi

1
β

−

Xi

L(yi|

xi, h),

to become β–independent for β
and data term. (See for example [36, 225]). If for example 1
n

with β being the same for the prior
xi, h)

→ ∞

i L(yi|

P

26

(72)

(74)

(75)

(76)

converges for large number n of training data the low temperature limit
1/β

0 can be interpreted as large data limit n

,

nEcomb = n

L(yi|
Notice, however, the factor 1/n in front of the prior energy. For Gaussian
p(y

x, h) temperature 1/β corresponds to variance σ2

E(h
|

xi, h) +

(77)

D0)

 −

Xi

!

.

1
n

→ ∞
1
n

→

|

1
σ2 Ecomb =

1
σ2  

1
2

(yi −
For Gaussian prior this would require simultaneous scaling of data and prior
variance.

h(xi))2 + σ2E(h
|

(78)

D0)

Xi

!

.

We should also remark that for continuous x,y the stationary solution h∗
f ). A common
needs not to be a typical representative of the process p(h
|
D0) re-
example is a Gaussian stochastic process p(h
f ) with prior energy E(h
|
|
lated to some smoothness measure of h expressed by derivatives of p(y
x, h).
Then, even if the stationary h∗ is smooth, this needs not to be the case
f ). For Brownian motion, for in-
for a typical h sampled according to p(h
|
stance, a typical sample path is not even diﬀerentiable (but continuous) while
the (stationary) mean path is smooth. Thus, for continuous variables only
βE(h) can be given an exact meaning as a Gaussian
expressions like
measure, deﬁned by a given covariance with existing normalization factor,
but not the expressions dh and E(h) alone [46, 60, 212, 102, 78, 135].
Interestingly, the stationary h∗ yielding maximal posterior p(h
f ) is not
|
x, f )
only useful to obtain an approximation for the predictive density p(y
but is also the optimal solution a∗ for a Bayesian decision problem with
log–loss and a

A = H:

dh e−

R

|

|

∈

Theorem: For a Bayesian decision problem with log–loss (47)

and analogously,

H r(a, h) = h,
argmina
∈

F r(a, f ) = f.
argmina
∈

(79)

(80)

Proof: Jensen’s inequality states that

dy p(y)g(q(y))

g(

dy p(y)q(y)),

(81)

Z

Z

≥

27

for any convex function g and probability p(y)
because the logarithm is concave

≥

0 with

dy p(y) = 1. Thus,

dy p(y

x, h) ln

|

− Z

p(y
p(y

x, a)
|
x, h) ≥ −
|

Z

ln

dy p(y

x, h)

= 0

(82)

dy p(y

x, h) ln p(y

x, a)

dy p(y

x, h) ln p(y

x, h),

(83)

R

p(y
p(y

x, a)
|
x, h)
|

|

|

|

⇒ − Z

|

|

≥ − Z

with equality for a = h. Hence

r(a, h) =

dx

dy p(x)p(y

x, h) (b1(x) ln p(y

x, a) + b2(x, y)) (84)

=

dx p(x)b1(x)

x, h) ln p(y

Z

− Z

− Z

≥ − Z
= r(h, h),

|
dy p(y

|

|

Z

Z

|
x, a) + const.

|

|

(85)

(86)

(87)

dx p(x)b1(x)

dy p(y

x, h) ln p(y

x, h) + const.

with equality for a = h. For a

F replace h

H by f

F .

q.e.d.

∈

∈

∈

2.4 Normalization, positivity, and speciﬁc priors

|

x, h). Thus, the prior density p(h
|

Density estimation problems are characterized by their normalization and
positivity condition for p(y
D0) can only
be non–zero for such h for which p(y
x, h) is positive and normalized over
y for all x.
(Similarly, when solving for a distribution function, i.e., the
integral of a density, the positivity constraint is replaced by monotonicity
and the normalization constraint by requiring the distribution function to
be 1 on the right boundary.) While the positivity constraint is local with
respect to x and y, the normalization constraint is nonlocal with respect to
y. Thus, implementing a normalization constraint leads to nonlocal and in
general non–Gaussian priors.

|

For classiﬁcation problems, having discrete y values (classes), the nor-
malization constraint requires simply to sum over the diﬀerent classes and
a Gaussian prior structure with respect to the x–dependency is not altered
[219]. For general density estimation problems, however, i.e., for continu-
ous y, the loss of the Gaussian structure with respect to y is more severe,
because non–Gaussian functional integrals can in general not be performed
analytically. On the other hand, solving the learning problem numerically

28

by discretizing the y and x variables, the normalization term is typically not
a severe complication.

To be speciﬁc, consider a Maximum A Posteriori Approximation, mini-

mizing

−

βEcomb =

L(yi|
where the likelihood free energy F (YD|
free energy F (H
tion with respect to h. The prior energy βE(h
|
positivity and normalization conditions

xD, h) is included, but not the prior
D0) which, being h–independent, is irrelevant for minimiza-
D0) has to implement the

xi, h) + βE(h
|

D0),

(88)

Xi

|

ZX(x, h) =

Z

dyi p(yi|
p(yi|

xi, h) = 1,

xi, h)

0,

≥

H

xi ∈
∀
yi ∈
∀

Xi,

Yi,

h
∀
∈
xi ∈
∀

Xi,

h

∀

∈

(89)

H.

(90)

It is useful to isolate the normalization condition and positivity constraint
deﬁning the class of density estimation problems from the rest of the problem
speciﬁc priors. Introducing the speciﬁc prior information ˜D0 so that D0 =
˜D0, normalized, positive

, we have

{

}

˜D0, norm., pos.) =

p(h
|

p(norm., pos.
|

p(norm., pos.

h)p(h
|
˜D0)

˜D0)

,

|

(91)

with deterministic, ˜D0–independent

h, ˜D0)

h) = p(norm., pos.
|

h)p(pos.
|

= p(norm.
|

p(norm., pos.
|
h) = δ(ZX −
and step function Θ. ( The density p(norm.
|
sible normalizations of p(y
p(pos.
|
nator p(norm., pos.
|
to h. We deﬁne the speciﬁc prior as

h) is normalized over all pos-
x, h), i.e., over all possible values of ZX, and
h) over all possible sign combinations.) The h–independent denomi-
˜D0) can be skipped for error minimization with respect

x, h)

(93)

(92)

,
(cid:17)

p(y

xy
Y

1)

Θ

(cid:16)

|

|

˜D0)

p(h
|

∝

E(h

e−

˜D0).

|

(94)

In Eq. (94) the speciﬁc prior appears as posterior of a h–generating pro-
cess determined by the parameters ˜D0. We will call therefore Eq. (94) the

29

posterior form of the speciﬁc prior. Alternatively, a speciﬁc prior can also be
in likelihood form

|

E( ˜D0

(95)

d ˜D0 e−

norm., pos.).

norm., pos.) = p( ˜D0|
p( ˜D0, h
h) p(h
|
|
As the likelihood p( ˜D0|
h) is conditioned on h this means that the normal-
h) remains in general h–dependent and must be in-
ization Z =
cluded when minimizing with respect to h. However, Gaussian speciﬁc priors
with h–independent covariances have the special property that according to
Eq. (71) likelihood and posterior interpretation coincide. Indeed, represent-
ing Gaussian speciﬁc prior data ˜D0 by a mean function t ˜D0 and covariance
1 (analogous to standard training data in the case of Gaussian regression,
K−
see also Section 3.5) one ﬁnds due to the fact that the normalization of a
Gaussian is independent of the mean (for uniform (meta) prior p(h))

R

(96)

(97)

(98)

(99)

˜D0) =

p(h
|

= p(t ˜D0|

h, K) =

−
,K(h

t ˜D0

))

−

1
2 (h

t ˜D0

−

e−

,K(h

t ˜D0

))

dh e−
1
2 (h

−

e−
R

1
2 (h

t ˜D0

t ˜D0
−
,K(h

dt e−

1
2 (h

−

−
t,K(h

t ˜D0

))

t))

−

.

Thus, for Gaussian p(t ˜D0|
prior energy in likelihood form becomes analogous to Eq. (94)

h, K) with h–independent normalization the speciﬁc

R

p(t ˜D0|
and speciﬁc prior energies can be interpreted both ways.

h, K)

e−

∝

E(t ˜D0 |

h,K),

Similarly, the complete likelihood factorizes

p( ˜D0, norm., pos.
|

h) = p(norm., pos.
|

h) p( ˜D0|

h).

According to Eq. (93) positivity and normalization conditions are im-
plemented by step and δ–functions. The positivity constraint is only active
when there are locations with p(y
x, h) = 0. In all other cases the gradient
has no component pointing into forbidden regions. Due to the combined
x, h) has to be larger than zero by deﬁnition, and
eﬀect of data, where p(y
|
smoothness terms the positivity condition for p(y
x, h) is usually (but not
always) fulﬁlled. Hence, if strict positivity is checked for the ﬁnal solution
then it is not necessary to include extra positivity terms in the error (see

|

|

30

Section 3.2.1). For the sake of simplicity we will therefore not include posi-
tivity terms explicitly in the following. In case a positivity constraint has to
be included this can be done using Lagrange multipliers, or alternatively, by
h)
writing the step functions in p(pos.
|
∞

x,y Θ(p(y

x, h))

Q
∞

∝

Θ(x

a) =

dξ

dηeiη(ξ

−

|
x),

(100)

−

a
Z

Z

−∞

and solving the ξ–integral in saddle point approximation (See for example
[57, 58, 59]).

Including the normalization condition in the prior p0(h
|

δ–functional results in a posterior probability

D0) in form of a

p(h
|

P

f ) = e

i Li(yi

|

xi,h)

E(h

˜D0)+˜c(H

˜D0)

−

|

|

δ

dy eL(y

|

x,h)

X
Yx
∈

(cid:18)Z

1

(cid:19)

−

(101)

with constant ˜c(H
E(h
speciﬁc prior e−

˜D0) related to the normalization of the
˜D0) =
|
˜D0). Writing the δ–functional in its Fourier representation
|

ln ˜Z(h
|

−

δ(x) =

∞

dk eikx =

dk e−

kx,

(102)

1
2π Z

−∞

1
2πi Z

i
∞

i
∞

−

i.e.,

δ(

dy eL(y

|

x,h)

1) =

−

Z

1
2πi Z

i
∞

i
∞

−

dΛX(x) eΛX (x)(1

−

dy eL(y|x,h)),

(103)

and performing a saddle point approximation with respect to ΛX(x) (which
is exact in this case) yields

P (h
|

P

f ) = e

i Li(yi

xi,h)

|

−

|

E(h

˜D0)+˜c(H

˜D0)+

dx ΛX (x)(1

|

dyeL(y|x,h)).

−

(104)

This is equivalent to the Lagrange multiplier approach. Here the stationary
ΛX(x) is the Lagrange multiplier vector (or function) to be determined by
x,h). Besides the Lagrange
the normalization conditions for p(y
|
multiplier terms it is numerically sometimes useful to add additional terms
to the log–posterior which vanish for normalized p(y

x, h) = eL(y

x, h).

|

R

R

R

|

2.5 Empirical risk minimization

In the previous sections the error functionals we will try to minimize in the
following have been given a Bayesian interpretation in terms of the log–
posterior density. There is, however, an alternative justiﬁcation of error

31

functionals using the Frequentist approach of empirical risk minimization
[208, 209, 210].

Common to both approaches is the aim to minimize the expected risk for

action a

r(a, f ) =

dx dy p(x, y

f (D, D0)) l(x, y, a).

(105)

Z

|

The expected risk, however, cannot be calculated without knowledge of the
f )
true p(x, y
the Frequentist approach approximates the expected risk by the empirical
risk

f ). In contrast to the Bayesian approach of modeling p(x, y

|

|

E(a) = ˆr(a, f ) =

l(xi, yi, a),

(106)

Xi

i.e., by replacing the unknown true probability by an observable empirical
probability. Here it is essential for obtaining asymptotic convergence results
f ) [208,
to assume that training data are sampled according to the true p(x, y
47, 179, 119, 210]. Notice that in contrast in a Bayesian approach the density
p(xi) for training data D does according to Eq. (16) not enter the formalism
because D enters as conditional variable. For more detailed discussion of
the relation of quadratic error functionals with Gaussian processes see for
example [168, 170, 171, 104, 105, 141, 212, 135].

|

From that Frequentist point of view one is not restricted to logarithmic
data terms as they arise from the posterior–related Bayesian interpretation.
However, like in the Bayesian approach, training data terms are not enough to
make the minimization problem well deﬁned. Indeed this is a typical inverse
problem [208, 107, 210] which can, according to the classical regularization
approach [204, 205, 152], be treated by including additional regularization
(stabilizer) terms in the loss function l. Those regularization terms, which
correspond to the prior terms in a Bayesian approach, are thus from the
point of view of empirical risk minimization a technical tool to make the
minimization problem well deﬁned.

The empirical generalization error for a test or validation data set inde-
pendent from the training data D, on the other hand, is measured using only
the data terms of the error functional without regularization terms. In empir-
ical risk minimization this empirical generalization error is used, for example,
to determine adaptive (hyper–)parameters of regularization terms. A typi-
cal example is a factor multiplying the regularization terms controlling the
trade–oﬀ between data and regularization terms. Common techniques using
the empirical generalization error to determine such parameters are cross–

32

validation or bootstrap like techniques [153, 5, 214, 200, 201, 76, 35, 212, 49].
From a strict Bayesian point of view those parameters would have to be
integrated out after deﬁning an appropriate prior [14, 138, 140, 21].

2.6

Interpretations of Occam’s razor

The principle to prefer simple models over complex models and to ﬁnd an op-
timal trade–oﬀ between data and complexity is often referred to as Occam’s
razor (William of Occam, 1285–1349). Regularization terms, penalizing for
example non–smooth (“complex”) functions, can be seen as an implementa-
tion of Occam’s razor.

The related phenomena appearing in practical learning is called over–
ﬁtting [208, 89, 21]. Indeed, when studying the generalization behavior of
trained models on a test set diﬀerent from the training set, it is often found
that there is a optimal model complexity. Complex models can due to their
higher ﬂexibility achieve better performance on the training data than sim-
pler models. On a test set independent from the training set, however, they
can perform poorer than simpler models.

Notice, however, that the Bayesian interpretation of regularization terms
as (a priori) information about Nature and the Frequentist interpretation
as additional cost terms in the loss function are not equivalent. Complexity
priors reﬂects the case where Nature is known to be simple while complex-
ity costs express the wish for simple models without the assumption of a
simple Nature. Thus, while the practical procedure of minimizing an error
functional with regularization terms appears to be identical for empirical risk
minimization and a Bayesian Maximum A Posteriori Approximation, the un-
derlying interpretation for this procedure is diﬀerent. In particular, because
the Theorem in Section 2.3 holds only for log–loss, the case of loss functions
diﬀering from log–loss requires from a Bayesian point of view to distinguish
explicitly between model states h and actions a. Even in saddle point ap-
proximation, this would result in a two step procedure, where in a ﬁrst step
the hypothesis h∗, with maximal posterior probability is determined, while
the second step minimizes the risk for action a
A under that hypothesis
h∗ [123].

∈

33

2.7 A priori information and a posteriori control

Learning is based on data, which includes training data as well as a pri-
ori data. It is prior knowledge which, besides specifying the space of local
hypothesis, enables generalization by providing the necessary link between
measured training data and not yet measured or non–training data. The
strength of this connection may be quantiﬁed by the mutual information of
training and non–training data, as we did in Section 2.1.5.

Often, the role of a priori information seems to be underestimated. There
are theorems, for example, proving that asymptotically learning results be-
come independent of a priori information if the number of training data goes
to inﬁnity. This, however,is correct only if the space of hypotheses h is al-
ready suﬃciently restricted and if a priori information means knowledge in
addition to that restriction.

In particular, let us assume that the number of potential test situations
x, is larger than the number of training data one is able to collect. As the
number of actual training data has to be ﬁnite, this is always the case if
x can take an inﬁnite number of values, for example if x is a continuous
variable. The following arguments, however, are not restricted to situations
were one considers an inﬁnite number of test situation, we just assume that
their number is too large to be completely included in the training data.

If there are x values for which no training data are available, then learn-
ing for such x must refer to the mutual information of such test data and
the available training data. Otherwise, training would be useless for these
test situations. This also means, that the generalization to non–training
situations can be arbitrarily modiﬁed by varying a priori information.

To make this point very clear, consider the rather trivial situation of
learning a deterministic function h(x) for a x variable which can take only
two values x1 and x2, from which only one can be measured. Thus, hav-
ing measured for example h(x1) = 5 then “learning” h(x2) is not possible
without linking it to h(x1). Such prior knowledge may have the form of a
2 which would allow a learning
“smoothness” constraint, say
algorithm to “generalize” from the training data and obtain 3
7.
Obviously, arbitrary results can be obtained for h(x2) by changing the prior
knowledge. This exempliﬁes that generalization can be considered as a mere
reformulation of available information, i.e., of training data and prior knowl-
edge. Except for such a rearrangement of knowledge, a learning algorithm
does not add any new information to the problem. (For a discussion of the

h(x2)

h(x1)

h(x2)

| ≤

−

≤

≤

|

34

related “no–free-lunch” theorems see [223, 224].)

Being extremely simple, this example nevertheless shows a severe prob-
lem. If the result of learning can be arbitrary modiﬁed by a priori informa-
tion, then it is critical which prior knowledge is implemented in the learning
algorithm. This means, that prior knowledge needs an empirical foundation,
just like standard training data have to be measured empirically. Otherwise,
the result of learning cannot expected to be of any use.

Indeed, the problem of appropriate a priori information is just the old
induction problem, i.e., the problem of learning general laws from a ﬁnite
number of observations, as already been discussed by the ancient Greek
philosophers. Clearly, this is not a purely academic problem, but is ex-
tremely important for every system which depends on a successful control
of its environment. Modern applications of learning algorithms, like speech
recognition or image understanding, rely essentially on correct a priori in-
formation. This holds especially for situations where only few training data
are available, for example, because sampling is very costly.

Empirical measurement of a priori information, however, seems to be
impossible. The reason is that we must link every possible test situation to
the training data. We are not able to do this in practice if, as we assumed, the
number of potential test situations is larger than the number of measurements
one is able to perform.

−

Take as example again a deterministic learning problem like the one dis-
cussed above. Then measuring a priori information might for example be
done by measuring (e.g., bounds on) all diﬀerences h(x1)
h(xi). Thus,
even if we take the deterministic structure of the problem for granted, the
number of such diﬀerences is equal to the number of potential non–training
situations xi we included in our model. Thus, measuring a priori information
does not require fewer measurements than measuring directly all potential
non–training data. We are interested in situations where this is impossible.
Going to a probabilistic setting the problem remains the same. For exam-
ple, even if we assume Gaussian hypotheses with ﬁxed variance, measuring
a complete mean function h(x), say for continuous x, is clearly impossible
in practice. The same holds thus for a Gaussian process prior on h. Even
this very speciﬁc prior requires the determination of a covariance and a mean
function (see Chapter 3).

As in general empirical measurement of a priori information seems to be
impossible, one might thus just try to guess some prior. One may think, for
example, of some “natural” priors. Indeed, the term “a priori” goes back

35

to Kant [103] who assumed certain knowledge to be necessarily be given “a
priori” without reference to empirical veriﬁcation. This means that we are
either only able to produce correct prior assumptions, for example because
incorrect prior assumptions are “unthinkable”, or that one must typically
be lucky to implement the right a priori information. But looking at the
huge number of diﬀerent prior assumptions which are usually possible (or
“thinkable”), there seems no reason why one should be lucky. The question
thus remains, how can prior assumptions get empirically veriﬁed.

Also, one can ask whether there are “natural” priors in practical learning
tasks. In Gaussian regression one might maybe consider a “natural” prior
to be a Gaussian process with constant mean function and smoothness–
related covariance. This may leave a single regularization parameter to be
determined for example by cross–validation. Formally, one can always even
use a zero mean function for the prior process by subtracting a base line
or reference function. Thus does, however, not solve the problem of ﬁnding
a correct prior, as now that reference function has to be known to relate
the results of learning to empirical measurements. In principle any function
could be chosen as reference function. Such a reference function would for
example enter a smoothness prior. Hence, there is no “natural” constant
function and from an abstract point of view no prior is more “natural” than
any other.

Formulating a general law refers implicitly (and sometimes explcitly) to
a “ceteris paribus” condition, i.e., the constraint that all relevent variables,
not explicitly mentioned in the law, are held constant. But again, verifying
a “ceteris paribus” condition is part of an empirical measurement of a priori
information and by no means trivial.

Trying to be cautious and use only weak or “uninformative” priors does
also not solve the principal problem. One may hope that such priors (which
may be for example an improper constant prior for a one–dimensional real
variable) do not introduce a completely wrong bias, so that the result of
learning is essentially determined by the training data. But, besides the
problem to deﬁne what exactly an uninformative prior has to be, such priors
are in practice only useful if the set of possible hypothesis is already suﬃ-
ciently restricted, so “the data can speak for themselves” [64]. Hence, the
problem remains to ﬁnd that priors which impose the necessary restrictions,
so that uninformative priors can be used.

Hence, as measuring a priori information seems impossible and ﬁnding
correct a priori information by pure luck seems very unlikely, it looks like also

36

successful learning is impossible. It is a simple fact, however, that learning
can be successful. That means there must be a way to control a priori
information empirically.

Indeed, the problem of measuring a priori information may be artiﬁcial,
arising from the introduction of a large number of potential test situations
and correspondingly a large number of hidden variables h (representing what
we call “Nature”) which are not all observable.

In practice, the number of actual test situations is also always ﬁnite, just
like the number of training data has to be. This means, that not all poten-
tial test data but only the actual test data must be linked to the training
data. Thus, in practice it is only a ﬁnite number of relations which must
be under control to allow successful generalization. (See also Vapnik’s dis-
tinction between induction and transduction problems.
[210]: In induction
problems one tries to infer a whole function, in transduction problems one is
only interested in predictions for a few speciﬁc test situations.)

This, however, opens a possibility to control a priori information em-
pirically. Because we do not know which test situation will occur, such an
empirical control cannot take place at the time of training. This means a
priori information has to be implemented at the time of measuring the test
data.
In other words, a priori information has to be implemented by the
measurement process [123, 126].

∀

b,

≤

≤

h(x)

Again, a simple example may clarify this point. Consider the prior in-
formation, that a function h is bounded, i.e., a
x. A direct
measurement of this prior assumption is practically not possible, as it would
require to check every value h(x). An implementation within the measure-
ment process is however trivial. One just has to use a measurement device
which is only able to to produce output in the range between a and b. This
is a very realistic assumption and valid for all real measurement devices.
Values smaller than a and larger than b have to be ﬁltered out or actively
projected into that range. In case we nevertheless ﬁnd a value out of that
range we either have to adjust the bounds or we exchange the “malfunction-
ing” measurement device with a proper one. Note, that this range ﬁlter is
only needed at the ﬁnite number of actual measurements. That means, a
priori information can be implemented by a posteriori control at the time of
testing.

A realistic measurement device does not only produce bounded output
but shows also always input noise or input averaging. A device with input
noise has noise in the x variable. That means if one intends to measure at

37

1

0.8

0.6

0.4

0.2

20

40

60

80

100

20

40

60

80

100

Figure 4: The l.h.s. shows a bounded random function which does not allow
generalization from training to non–training data. Using a measurement
device with input averaging (r.h.s.) or input noise the function becomes
learnable.

xi the device measures instead at xi + ∆ with ∆ being a random variable. A
typical example is translational noise, with ∆ being a, possibly multidimen-
sional, Gaussian random variable with mean zero. Similarly, a device with
input averaging returns a weighted average of results for diﬀerent x values
instead of a sharp result. Bounded devices with translational input noise, for
example, will always measure smooth functions [120, 20, 123]. (See Fig. 4.)
This may be an explanation for the success of smoothness priors.

The last example shows, that to obtain adequate a priori information
it can be helpful in practice to analyze the measurement process for which
learning is intended. The term “measurement process” does here not only
refer to a speciﬁc device, e.g., a box on the table, but to the collection of all
processes which lead to a measurement result.

We may remark that measuring a measurement process is as diﬃcult or
impossible as a direct measurement of a priori information. What has to
be ensured is the validity of the necessary restrictions during a ﬁnite num-
ber of actual measurements. This is nothing else than the implementation
of a probabilistic rule producing y given the test situation and the training
data. In other words, what has to be implemented is the predictive density
p(y
x, D). This predictive density indeed only depends on the actual test
situation and the ﬁnite number of training data. (Still, the probability den-
sity for a real y cannot strictly be empirically veriﬁed or controlled. We may
take it here, for example, as an approximate statement about frequencies.)
This shows the tautological character of learning, where measuring a priori
information means controling directly the corresponding predictive density.

|

1

0.8

0.6

0.4

0.2

38

The a posteriori interpretation of a priori information can be related to
a constructivistic point of view. The main idea of constructivism can be
characterized by a sentence of Vico (1710): Verum ipsum factum — the
truth is the same as the made [211]. (For an introduction to constructivism
see [216] and references therein, for constructive mathematics see [22].)

3 Gaussian prior factors

3.1 Gaussian prior factor for log–probabilities

3.1.1 Lagrange multipliers: Error functional EL

In this chapter we look at density estimation problems with Gaussian prior
factors. We begin with a discussion of functional priors which are Gaussian in
probabilities or in log–probabilities, and continue with general Gaussian prior
factors. Two section are devoted to the discussion of covariances and means
of Gaussian prior factors, as their adequate choice is essential for practical
applications. After exploring some relations of Bayesian ﬁeld theory and
empirical risk minimization, the last three sections introduce the speciﬁc
likelihood models of regression, classiﬁcation, inverse quantum theory.

We begin a discussion of Gaussian prior factors in L. As Gaussian prior
factors correspond to quadratic error (or energy) terms, consider an error
functional with a quadratic regularizer in L

(L, KL) =

dx dy dx′dy′L(x, y)K(x, y; x′, y′)L(x′, y′),

(107)

2
K =

L

||

||

1
2 Z

|

x, h) = ln p(y

writing for the sake of simplicity from now on L(x, y) for the log–probability
x, h). The operator K is assumed symmetric and positive
L(y
semi–deﬁnite and positive deﬁnite on some subspace. (We will understand
positive semi–deﬁnite to include symmetry in the following.) For positive
(semi) deﬁnite K the scalar product deﬁnes a (semi) norm by

|

L

K =

(L, KL),

||

||

q

(108)

K. The quadratic error term (107)
and a corresponding distance by
−
||
corresponds to a Gaussian factor of the prior density which have been called
˜D0) for L. In particular, we will consider
the speciﬁc prior p(h
|

˜D0) = p(L
|

L′

L

||

39

here the posterior density

R

P

1
2
−

f ) = e

i Li(xi,yi)

p(h
|

dx ΛX (x)(1

dxdydx′dy′L(x,y)K(x,y;x′,y′)L(x′,y′)+

dy eL(x,y))+˜c,,
(109)
where prefactors like β are understood to be included in K. The constant
˜c referring to the speciﬁc prior is determined by the determinant of K ac-
i Li but
cording to Eq. (71). Notice however that not only the likelihood
also the complete prior is usually not Gaussian due to the presence of the
normalization conditions. (An exception is Gaussian regression, see Section
3.7.) The posterior (109) corresponds to an error functional

P

−

R

R

EL = βEcomb =

(L, N) +

(L, KL) + (eL

δ(y), ΛX),

(110)

−

1
2

−

with likelihood vector (or function)

data vector (function)

L(x, y) = L(y

x, h),

|

N(x, y) =

δ(x

xi)δ(y

yi),

−

−

n

Xi

Lagrange multiplier vector (function)

ΛX(x, y) = ΛX(x),

probability vector (function)

eL(x, y) = eL(x,y) = P (x, y) = p(y

x, h),

|

and

δ(y)(x, y) = δ(y).

(111)

(112)

(113)

(114)

(115)

According to Eq. (112) N/n = Pemp is an empirical density function for the
joint probability p(x, y

h).

We end this subsection by deﬁning some notations. While functions
of vectors (functions) and matrices (operators), like eL, will be understood
element-wise, only multiplication is interpreted as matrix product. Element-
wise multiplication is written with the help of diagonal matrices. For that

|

40

purpose we denote diagonal matrices corresponding to vectors by bold letters.
For instance, the matrices (operators)

−
−

I(x, y; x′, y′) = δ(x
x′)δ(y
−
L(x, y; x′, y′) = δ(x
x′)δ(y
−
P(x, y; x′, y′) = eL(x, y; x′, y′)
x′)δ(y
x′)δ(y
x′)δ(y

= δ(x
N(x, y; x′, y′) = δ(x
ΛX(x, y; x′, y′) = δ(x

−
−
−

y′),
y′)L(x, y),

y′)P (x, y),
y′) N(x, y),
y′)ΛX(x),

−
−
−

(116)
(117)
(118)
(119)
(120)
(121)

(122)

correspond to the vectors or functions,

I(x, y) = 1,

and

L = LI, P = PI,

eL = eLI, N = NI, ΛX = ΛXI.

(123)

Being diagonal all these matrices commute with each other. Element-wise
multiplication can now be expressed as

(KL)(x′, y′, x, y) =

dx′′dy′′K(x′, y′, x′′, y′′)L(x′′, y′′, x, y)

Z

Z

=

dx′′dy′′K(x′, y′, x′′, y′′)L(x, y)δ(x

x′′)δ(y

y′′)

= K(x′, y′, x, y)L(x, y).

−

−

(124)

In general this is not equal to L(x′, y′)K(x′, y′, x, y). In contrast, the matrix
product KL with vector L

(KL)(x′, y′) =

dx dy K(x′, y′, x, y)L(x, y),

(125)

Z

does not depend on x, y anymore, while the tensor product or outer product,

(K

L)(x′′, y′′, x, y, x′, y′) = K(x′′, y′′, x′, y′)L(x, y),

(126)

⊗

depends on additional x′′, y′′.

Taking the variational derivative of (109) with respect to L(x, y) using

δL(x′, y′)
δL(x, y)

= δ(x

x′)δ(y

y′)

−

−

(127)

41

and setting the gradient equal to zero yields the stationarity equation

0 = N

KL

−

−

eLΛX.

Alternatively, we can write eLΛX = ΛXeL = PΛX.

The Lagrange multiplier function ΛX is determined by the normalization

condition

ZX(x) =

dy eL(x,y) = 1,

x

∀

∈

X,

Z

which can also be written

ZX = IXP = IXeL = I

or ZX = I,

in terms of normalization vector,

ZX(x, y) = ZX(x),

normalization matrix,

and identity on X,

ZX(x, y; x′, y′) = δ(x

x′)δ(y

y′) ZX(x),

−

−

IX(x, y; x′, y′) = δ(x

x′).

−

Multiplication of a vector with IX corresponds to y–integration. Being a
non–diagonal matrix IX does in general not commute with diagonal matrices
like L or P. Note also that despite IXeL = IXeLI = II = I in general IXP
= IXeL

= I = ZX. According to the fact that IX and ΛX commute, i.e.,

IXΛX = ΛXIX ⇔

[ΛX, IX] = 0,

and the same holds for the diagonal matrices

[ΛX, eL] = [ΛX, P] = 0,

it follows from the normalization condition IXP = I that

i.e.,

0 = (I

IXeL)ΛX = (I

IXP)ΛX.

−

−

42

IXPΛX = IXΛXP = ΛXIXP = ΛXI = ΛX,

(136)

(128)

(129)

(130)

(131)

(132)

(133)

(134)

(135)

(137)

6
For ΛX(x)
= 0 Eqs.(136,137) are equivalent to the normalization (129). If
there exist directions at the stationary point L∗ in which the normalization of
P changes, i.e., the normalization constraint is active, a ΛX(x)
= 0 restricts
the gradient to the normalized subspace (Kuhn–Tucker conditions [52, 17, 92,
178]). This will clearly be the case for the unrestricted variations of p(y, x)
which we are considering here. Combining ΛX = IXPΛX for ΛX(x)
= 0 with
the stationarity equation (128) the Lagrange multiplier function is obtained

ΛX = IX (N

KL) = NX −

−

(IXKL).

Here we introduced the vector

with components

NX = IXN,

(138)

(139)

NX(x, y) = NX(x) =

δ(x

xi) = nx,

(140)

−

Xi

giving the number of data available for x. Thus, Eq. (138) reads in compo-
nents

ΛX(x) =

δ(x

xi)

−

− Z

Xi

dy′′ dx′dy′ K(x, y′′; x′, y′)L(x′, y′).

(141)

Inserting now this equation for ΛX into the stationarity equation (128) yields

0 = N

KL

−

eL(NX −

−

IXKL) =

I

eLIX

(N

KL) .

(142)

−

(cid:16)

−

(cid:17)

Eq. (142) possesses, besides normalized solutions we are looking for, also
possibly unnormalized solutions fulﬁlling N = KL for which Eq. (138) yields
ΛX = 0. That happens because we used Eq. (136) which is also fulﬁlled
for ΛX(x) = 0. Such a ΛX(x) = 0 does not play the role of a Lagrange
multiplier. For parameterizations of L where the normalization constraint is
not necessarily active at a stationary point ΛX(x) = 0 can be possible for a
normalized solution L∗. In that case normalization has to be checked.

It is instructive to deﬁne

so the stationarity equation (128) acquires the form

TL = N

ΛXeL,

−

KL = TL,

43

(143)

(144)

6
6
6
which reads in components

dx′dy′ K(x, y; x′, y′)L(x′, y′) =

Z

δ(x

xi)δ(y

yi)

ΛX(x) eL(x,y), (145)

−

−

−

Xi

which is in general a non–linear equation because TL depends on L. For
1 the form (144) suggest however
existing (and not too ill–conditioned) K−
an iterative solution of the stationarity equation according to

Li+1 = K−

1TL(Li),

(146)

for discretized L, starting from an initial guess L0. Here the Lagrange multi-
plier ΛX has to be adapted so it fulﬁlls condition (138) at the end of iteration.
Iteration procedures will be discussed in detail in Section 7.

−

Z

1
2

g

−

(cid:17)

(cid:16)

3.1.2 Normalization by parameterization: Error functional Eg

Referring to the discussion in Section 2.3 we show that Eq. (142) can alter-
natively be obtained by ensuring normalization, instead of using Lagrange
multipliers, explicitly by the parameterization

L(x, y) = g(x, y)

ln

dy′ eg(x,y′), L = g

ln ZX,

−

(147)

and considering the functional

Eg =

N, g

ln ZX

+

ln ZX , K (g

ln ZX)

(148)

−

.
(cid:17)

−
(cid:16)

−

The stationary equation for g(x, y) obtained by setting the functional deriva-
tive δEg/δg to zero yields again Eq. (142). We check this, using

δ ln ZX(x′)
δg(x, y)

= δ(x

x′)eL(x,y),

−

δ ln ZX
δg

= IXeL =

T

,

eLIX
(cid:16)

(cid:17)

(149)

and

δL(x′, y′)
δg(x, y)

= δ(x

x′)δ(y

y′)

δ(x

−

−

−

−

x′)eL(x,y),

δL
δg

= I

−

IXeL,

(150)

δg denotes a matrix, and the superscript T the transpose of a matrix.

where δL
We also note that despite IX = IT
X

IXeL

= eLIX = (IXeL)T ,

(151)

44

6
is not symmetric because eL depends on y and does not commute with the
non–diagonal IX. Hence, we obtain the stationarity equation of functional
Eg written in terms of L(g) again Eq. (142)

0 =

−  

δL
δg !

T δEg
δL

= GL −

eLΛX =

eLIX

I

−

−

(N

KL) .

(152)

(cid:17)
Eg. Referring to the
δEg/δL is the L–gradient of
Here GL = N
discussion following Eq. (142) we note, however, that solving for g instead
for L no unnormalized solutions fulﬁlling N = KL are possible.

KL =

−

−

−

(cid:16)

In case ln ZX is in the zero space of K the functional Eg corresponds to
a Gaussian prior in g alone. Alternatively, we may also directly consider a
Gaussian prior in g

˜Eg =

N, g

ln ZX

+

g , K g

,

−
(cid:16)

−

(cid:17)

(cid:16)

(cid:17)

1
2

with stationarity equation

0 = N

Kg

−

−

eLNX.

(153)

(154)

Notice, that expressing the density estimation problem in terms of g, nonlo-
cal normalization terms have not disappeared but are part of the likelihood
term. As it is typical for density estimation problems, the solution g can be
calculated in X–data space, i.e., in the space deﬁned by the xi of the training
data. This still allows to use a Gaussian prior structure with respect to the
x–dependency which is especially useful for classiﬁcation problems [219].

3.1.3 The Hessians HL, Hg

The Hessian HL of
tives

−

EL is deﬁned as the matrix or operator of second deriva-

HL(L)(x, y; x′y′) =

(155)

δ2(

EL)

−
δL(x, y)δL(x′, y′) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

L

For functional (110) and ﬁxed ΛX we ﬁnd the Hessian by taking the derivative
of the gradient in (128) with respect to L again. This gives

HL(L)(x, y; x′y′) =

K(x, y; x′y′)

δ(x

x′)δ(y

y′)ΛX(x)eL(x,y)

(156)

−

−

−

−

or

(157)

HL =

K

−

−

ΛXeL.

45

The addition of the diagonal matrix ΛXeL = eLΛX can result in a negative
deﬁnite H even if K has zero modes. like in the case where K is a diﬀerential
operator with periodic boundary conditions. Note, however, that ΛXeL is
diagonal and therefore symmetric, but not necessarily positive deﬁnite, be-
cause ΛX(x) can be negative for some x. Depending on the sign of ΛX(x)
the normalization condition ZX(x) = 1 for that x can be replaced by the
inequality ZX(x)
1. Including the L–dependence of ΛX and
with

1 or ZX(x)

≤

≥

δeL(x′,y′)
δg(x, y)

= δ(x

x′)δ(y

−

−

y′)eL(x,y)

δ(x

x′)eL(x,y)eL(x′,y′),

(158)

−

−

i.e.,

−
we ﬁnd, written in terms of L,

δeL
δg

=

I
(cid:16)

eL IX

eL = eL

eL IXeL,

−

(cid:17)

(159)

Hg(L)(x, y; x′, y′) =

δ2(

Eg)

=
Z

dx′′dy′′

−
δL(x, y)δL(x′′, y′′)

 

δL(x′′, y′′)
δg(x′, y′)

δ2(

Eg)

−
δg(x, y)δg(x′, y′) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Eg)

δ(

+

−
δL(x′′, y′′)

L

δ2L(x′′, y′′)
δg(x, y)δg(x′, y′) !(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

L

=

K(x, y; x′, y′)

eL(x′,y′)eL(x,y)

dy′′dy′′′K(x′, y′′; x, y′′′)

−
+eL(x′,y′)

−

Z

dy′′K(x′, y′′; x, y) + eL(x,y)

dy′′K(x′, y′; x, y′′)

δ(x

−

+δ(x

−

−

Z
x′)δ(y

y′)eL(x,y)

NX(x)

−
x′)eL(x,y)eL(x′,y′)

(cid:18)
NX(x)

Z

− Z

(cid:18)

− Z

dy′′(KL)(x, y′′)

(cid:19)
.

(cid:19)

dy′′(KL)(x, y′′)

(160)

The last term, diagonal in X, has dyadic structure in Y , and therefore for
ﬁxed x at most one non–zero eigenvalue.
In matrix notation the Hessian
becomes

Hg =
=

I

(cid:16)
(I

−

−

−

−

K

eLIX
I
(cid:16)
(cid:17)
PIX) [K (I

−

IXeL
−
−
(cid:17)
IXP) + ΛXP] ,

I
(cid:16)

eLIX

ΛXeL

(cid:17)

(161)

−

46

↔

↔

x′,y

the second line written in terms of the probability matrix. The expression
is symmetric under x
y′, as it must be for a Hessian and as can
be veriﬁed using the symmetry of K = KT and the fact that ΛX and IX
commute, i.e., [ΛX, IX] = 0. Because functional Eg is invariant under a
shift transformation, g(x, y)
g′(x, y) + c(x), the Hessian has a space of
zero modes with the dimension of X. Indeed, any y–independent function
(which can have ﬁnite L1–norm only in ﬁnite Y –spaces) is a left eigenvector
of
with eigenvalue zero. The zero mode can be removed by pro-
jecting out the zero modes and using where necessary instead of the inverse
a pseudo inverse of H, for example obtained by singular value decomposi-
tion, or by including additional conditions on g like for example boundary
conditions.

eLIX

I
(cid:16)

→

−

(cid:17)

3.2 Gaussian prior factor for probabilities

3.2.1 Lagrange multipliers: Error functional EP

x, h) for the probability of y conditioned on x and
We write P (x, y) = p(y
h. We consider now a regularizing term which is quadratic in P instead of
L. This corresponds to a factor within the posterior probability (the speciﬁc
prior) which is Gaussian with respect to P .

|

f ) =e

p(h
|

iln Pi(xi,yi)

1
2
−

R
or written in terms of L = ln P for comparison,

P

dxdydx′dy′P (x,y)K(x,y;x′,y′)P (x′,y′)+

dx ΛX (x)(1

dy P (x,y))+˜c,

dxdydx′dy′eL(x,y)K(x,y;x′,y′)eL(x′,y′)+

dx ΛX (x)(1

dy eL(x,y))+˜c.

−

R

−

R

iLi(xi,yi)

1
2
−

f ) =e

p(h
|

P

Hence, the error functional is

R

−

In particular, the choice K = λ

2 I, i.e.,

EP = βEcomb =

(ln P, N) +

(P, K P ) + ( P

δ(y) , ΛX).

(164)

R

R

−

(162)

(163)

(165)

can be interpreted as a smoothness prior with respect to the distribution
function of P (see Section 3.3).

λ
2

(P, P ) =

λ
2 ||

P

2,

||

1
2

47

In functional (164) we have only implemented the normalization condition
for P by a Lagrange multiplier and not the positivity constraint. This is
suﬃcient if P (x, y) > 0 (i.e., P (x, y) not equal zero) at the stationary point
because then P (x, y) > 0 holds also in some neighborhood and there are no
components of the gradient pointing into regions with negative probabilities.
In that case the positivity constraint is not active at the stationarity point. A
typical smoothness constraint, for example, together with positive probability
at data points result in positive probabilities everywhere where not set to
zero explicitly by boundary conditions.
If, however, the stationary point
has locations with P (x, y) = 0 at non–boundary points, then the component
of the gradient pointing in the region with negative probabilities has to be
projected out by introducing Lagrange parameters for each P (x, y). This
may happen, for example, if the regularizer rewards oscillatory behavior.

The stationarity equation for EP is

0 = P−

1N

KP

ΛX,

−
with the diagonal matrix P(x′, y′; x, y) = δ(x
plied by P

−

−

0 = N

PKP

PΛX.

−

−

x′)δ(y

y′)P (x, y), or multi-

−

Probabilities P (x, y) are unequal zero at observed data points (xi, yi) so
P−

1N is well deﬁned.
Combining the normalization condition Eq. (136) for ΛX(x)
(166) or (167) the Lagrange multiplier function ΛX is found as

= 0 with Eq.

ΛX = IX (N

PKP ) = NX −

−

IXPKP,

(168)

where

IXPKP (x, y) =

dy′dx′′dy′′ P (x, y′)K(x, y′; x′′, y′′)P (x′′, y′′).

Z

Eliminating ΛX in Eq. (166) by using Eq. (168) gives ﬁnally

or for Eq. (167)

0 = (I

IXP)(P−

1N

KP ),

−

0 = (I

PIX)(N

PKP ).

−

−

−

For similar reasons as has been discussed for Eq. (142) unnormalized solutions
fulﬁlling N

PKP are possible. Deﬁning

−

TP = P−

1N

ΛX = P−

1N

−

NX −

−

IXPKP,

48

(166)

(167)

(169)

(170)

(171)

6
the stationarity equation can be written analogously to Eq. (144) as

with TP = TP (P ), suggesting for existing K−

1 an iteration

KP = TP ,

P i+1 = K−

1TP (P i),

starting from some initial guess P 0.

3.2.2 Normalization by parameterization: Error functional Ez

Again, normalization can also be ensured by parameterization of P and solv-
ing for unnormalized probabilities z, i.e.,

P (x, y) =

z(x, y)
dy z(x, y)

, P =

z
ZX

.

The corresponding functional reads

R

Ez =

N, ln

− (cid:18)

z
ZX (cid:19)

+

1
2 (cid:18)

z
ZX

, K

z
ZX (cid:19)

.

We have

δz
δz

= I,

δZX
δz

= IX,

δ ln z
δz

= z−

1 = (ZXP)−

1,

δ ln ZX
δz

= Z−

1
X IX,

with diagonal matrix z built analogous to P and ZX, and

δP
δz

=

δ(z/ZX)
δz

= Z−

1
X (I

PIX) ,

−

δ ln P
δz

1
= Z−
X

1

P−
(cid:16)

−

IX

,

(cid:17)

1

δZ −
X
δz

Z−

2
X IX,

=

−

1

δP −
δz

=

−

P−

2 Z−

1
X (I

PIX) .

−

The diagonal matrices [ZX, P] = 0 commute, as well as [ZX, IX] = 0, but
[P, IX]

= 0. Setting the gradient to zero and using

(172)

(173)

(174)

(175)

(176)

(177)

(178)

(179)

we ﬁnd

(I

−

PIX)T = (I

IXP) ,

−

0 =

−  

δP
δz !

T δEz
δP

49

6
1
= Z−
X

1

P−

h(cid:16)
1
X (I
= Z−

−

IX

N

(cid:17)
IXP)

(I

−
1N

−
P−
(cid:16)

IXP) KP

i

KP

−
1
X (GP −
IXP) GP = Z−
KP =
−

1N

(cid:17)

−
ΛX) = (GP −
δEz/δP of
−
−

= Z−

1
X (I

−

with P –gradient GP = P−
Ez and GP the cor-
responding diagonal matrix. Multiplied by ZX this gives the stationarity
equation (172).

ΛX) Z −

1
X ,

(180)

3.2.3 The Hessians HP , Hz

We now calculate the Hessian of the functional
the Hessian by diﬀerentiating again the gradient (166) of

−

EP . For ﬁxed ΛX one ﬁnds

EP

−
δ(x

Xi

xi)δ(y
−
P 2(x, y)

−

yi)

,

HP (P )(x, y; x′y′) =

K(x′y′; x, y)

δ(x

x′)δ(y

y′)

−

−

−

−

i.e.,

HP =

K

−

−

P−

2N.

Here the diagonal matrix P−

2N is non–zero only at data points.

Including the dependence of ΛX on P one obtains for the Hessian of

Ez

in (175) by calculating the derivative of the gradient in (180)

(181)

(182)

−

Hz(x, y; x′, y

) =
′

−

1
ZX(x)

K(x, y; x′, y
h

)
′

dy′′

p(x, y′′)K(x, y′′; x′, y′) + K(x, y; x′, y′′)p(x′, y′′)

dy′′dy′′′p(x, y′′)K(x, y′′; x′, y′′′)p(x′, y′′′)

(cid:17)

− Z

(cid:16)
+

Z

−

+δ(x

x′)δ(y

y′)

−

Xi

δ(x

−

xi)δ(y
p2(x, y)

−

yi)

δ(x

x′)

−

−

δ(x

xi)

−

Xi

δ(x

x′)

dx′′dy′′

K(x, y; x′′, y′′)p(x′′, y′′) + p(x′′, y′′)K(x′′, y′′; x′, y′)

−

−

Z

(cid:16)

−

Z

+ 2 δ(x

x′)

dy′′dx′′′dy′′′p(x, y′′)K(x, y′′; x′′′, y′′′)p(x′′′, y′′′)

(cid:17)
, (183)

1
ZX(x′)

i

50

i.e.,

(cid:17)

=

−

−

−

−

−

2N

(184)

IXP) K (I

PIX) + P−

Hz = Z−

1
PIX) Z−
X
−
1
X ,
ΛX) IX) Z−
2N

1
X (I
IXP)
K
P−
(I
−
(cid:16)
1
ΛX) + (GP −
X (IX (GP −
Z−
1
(I
Z−
X
−
−
h
1N
IXP−
NP−
−
+IXGP + GP IX −
Here we used [ΛX, IX] = 0. It follows from the normalization
dy p(x, y) =
1 that any y–independent function is right eigenvector of (I
IXP) with
zero eigenvalue. Because ΛX = IXPGP this factor or its transpose is also
contained in the second line of Eq. (184), which means that Hz has a zero
mode. Indeed, functional Ez is invariant under multiplication of z with a
y–independent factor. The zero modes can be projected out or removed by
including additional conditions, e.g. by ﬁxing one value of z for every x.

1IX + IXNIX
1
X .
Z−
2 IXΛX

(185)

R
−

i

3.3 General Gaussian prior factors

3.3.1 The general case

In the previous sections we studied priors consisting of a factor (the speciﬁc
prior) which was Gaussian with respect to P or L = ln P and additional
normalization (and positivity) conditions.
In this section we consider the
x, h) is expressed in terms of a function
situation where the probability p(y
φ(x, y). That means, we assume a, possibly non–linear, operator P = P (φ)
which maps the function φ to a probability. We can then formulate a learning
problem in terms of the function φ, meaning that φ now represents the hidden
variables or unknown state of Nature h.2 Consider the case of a speciﬁc prior
which is Gaussian in φ, i.e., which has a log–probability quadratic in φ

|

1
2

−

( φ , K φ ).

(186)

This means we are lead to error functionals of the form

Eφ =

( ln P (φ) , N ) +

( φ , K φ ) + ( P (φ) , ΛX ),

(187)

−

2 Besides φ also the hyperparameters discussed in Chapter 5 belong to the hidden

variables h.

1
2

51

where we have skipped the φ–independent part of the ΛX–terms. In general
cases also the positivity constraint has to be implemented.

To express the functional derivative of functional (187) with respect to
φ we deﬁne besides the diagonal matrix P = P(φ) the Jacobian, i.e., the
matrix of derivatives P′ = P′(φ) with matrix elements

P′(x, y; x′, y′; φ) =

δP (x′, y′; φ)
δφ(x, y)

.

The matrix P′ is diagonal for point–wise transformations, i.e., for P (x, y; φ) =
P ( φ(x, y) ). In such cases we use P ′ to denote the vector of diagonal elements
of P′. An example is the previously discussed transformation L = ln P for
which P′ = P. The stationarity equation for functional (187) becomes

0 = P′(φ)P−

1(φ)N

Kφ

P′(φ)ΛX,

−

−

and for existing PP′−

1 =(P′P−

1)−

1 (for nonexisting inverse see Section 4.1),

0 = N

PP′−

1K φ

−

PΛX.

−

From Eq. (190) the Lagrange multiplier function can be found by integration,
using the normalization condition IXP = I, in the form IXPΛX = ΛX for
ΛX(x)

= 0. Thus, multiplying Eq. (190) by IX yields

ΛX = IX

N
(cid:16)

−

= NX −

(cid:17)

PP′−

1K φ

IXPP′−

1K φ.

(191)

ΛX is now eliminated by inserting Eq. (191) into Eq. (190)

0 = (I

PIX)

−

N
(cid:16)

−

PP′−

1K φ

.

(cid:17)

A simple iteration procedure, provided K−
Eq. (189) in the form

1 exists, is suggested by writing

with

φ.

Kφ = Tφ, φi+1 = K−

1Tφ(φi),

Tφ(φ) = P′P−

1N

P′ΛX.

−

52

Table 2 lists constraints to be implemented explicitly for some choices of

(188)

(189)

(190)

(192)

(193)

(194)

6
φ

P (x, y)

P (φ)

P = P

constraints

norm

positivity

z(x, y)

P = z/

z dy

—

positivity

L(x, y) = ln P

R

P = eL

norm

g(x, y)

P = eg/

eg dy

—

—

—

Φ =

y dy′ P

P = dΦ/dy

boundary monotony

R

R

Table 2: Constraints for speciﬁc choices of φ

3.3.2 Example: Square root of P

We already discussed the cases φ = ln P with P ′ = P = eL, P/P ′ = 1 and
φ = P with P ′ = 1, P/P ′ = P . The choice φ = √P yields the common
L2–normalization condition over y

1 =

dy φ2(x, y),

Z

x

∀

∈

X,

(195)

≥

which is quadratic in φ, and P = φ2, P ′ = 2φ, P/P ′ = φ/2. For real φ the
positivity condition P

0 is automatically satisﬁed [77, 195].

For φ = √P and a negative Laplacian inverse covariance K =

∆, one
can relate the corresponding Gaussian prior to the Fisher information [34,
195, 191]. Consider, for example, a problem with ﬁxed x (so x can be skipped
from the natotion and one can write P (y)) and a dy–dimensional y. Then
one has, assuming the necessary diﬀerentiability conditions and vanishing
boundary terms,

−

( φ , K φ ) =

( φ , ∆ φ ) =

dy

−

dy

=

Xk Z

dy
4P (y)  

∂P (y)

∂yk !

2

dy

∂φ
∂yk (cid:12)
Xk (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
I F
k (0),

dy

1
4

Xk

Z

2

=

(196)

(197)

53

where I F

k (0) is the Fisher information, deﬁned as

I F
k (y0) =

∂P (y

y0)

2

−
∂y0

dy (cid:12)
P (y
(cid:12)
(cid:12)

Z

(cid:12)
y0)
(cid:12)
(cid:12)

−

=

dy

Z

∂ ln P (y
−
∂y0
k

2

y0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for the family P (

y0) with location parameter vector y0.
A connection to quantum mechanics can be found considering the training

· −

P (y

y0
k),

−

(198)

data free case

Eφ =

( φ, K φ ) + (ΛX, φ),

1
2

has the homogeneous stationarity equation

K φ =

2ΦΛX.

−
For x–independent ΛX this is an eigenvalue equation. Examples include
the quantum mechanical Schr¨odinger equation where K corresponds to the
system Hamiltonian and

(199)

(200)

(201)

2ΛX =

−

(φ, K φ)
(φ, φ)

,

In quantum mechanics Eq. (201) is the basis
to its ground state energy.
for variational methods (see Section 4) to obtain approximate solutions for
ground state energies [50, 183, 24].
Similarly, one can take φ =

Lmax) for L bounded from above by

(L

Lmax with the normalization

−

q

−

1 =

dy e−

φ2(x,y)+Lmax,

x

∀

∈

X,

(202)

and P = e−

φ2+Lmax, P ′ =

2φP , P/P ′ =

1/(2φ).

−

3.3.3 Example: Distribution functions

Z

−

Instead in terms of the probability density function, one can formulate the
prior in terms of its integral, the distribution function. The density P is then
recovered from the distribution function φ by diﬀerentiation,

P (φ) =

dy

Yk

∂φ
∂yk

dy

dy

=

Yk ∇ykφ =

Ok

54

R−

1
k φ. = R−

1φ,

(203)

resulting in a non–diagonal P′. The inverse of the derivative operator R−
is the integration operator R =

dy
k RkP with matrix elements

1

(204)

(205)

N

R(x, y; x′, y′) = δ(x

x′)θ(y

y′),

−

−

i.e.,

Rk(x, y; x′, y′) = δ(x

x′)

δ(yl −

y′l)θ(yk −

y′k).

−

=k
Yl

y

Thus, (203) corresponds to the transformation of (x–conditioned) density
functions P in (x–conditioned) distribution functions φ = RP , i.e., φ(x, y) =
P (x, y′)dy′. Because RTKR is (semi)–positive deﬁnite if K is, a speciﬁc
−∞
prior which is Gaussian in the distribution function φ is also Gaussian in the
R
density P . P′ becomes

P′(x, y; x′, y′) =

δ

dy

k ∇yk′ φ(x′, y′
δφ(x, y)

(cid:17)

(cid:16)Q

= δ(x

x′)

−

δ′(yk −

y′k).

(206)

dy

Yk

Here the derivative of the δ–function is deﬁned by formal partial integration

∞

Z

−∞

dy′ f (y′)δ′(y

y′) = f (y′)δ(y′

y)

f ′(y).

(207)

−

−

∞
−∞ −

|

−∞

) = 0 the variational derivative δ/(δφ(x,

Fixing φ(x,
)) is not needed.
The normalization condition for P becomes for the distribution function φ =
X. The positivity condition
RP the boundary condition φ(x,
) = 1,
y′,
for P corresponds to the monotonicity condition φ(x, y)
0,

X and to φ(x,

φ(x, y′),

−∞

y
∀

X.

∞

≥

≥

∈

x

x

x

∀

)

∀

∈

−∞

≥

∀

∈

3.4 Covariances and invariances

3.4.1 Approximate invariance

Prior terms can often be related to the assumption of approximate invariances
or approximate symmetries. A Laplacian smoothness functional, for exam-
ple, measures the deviation from translational symmetry under inﬁnitesimal
translations.

Consider for example a linear mapping

˜φ = Sφ,

φ

→

55

(208)

6
given by the operator S. To compare φ with ˜φ we deﬁne a (semi–)distance
deﬁned by choosing a positive (semi–)deﬁnite KS, and use as error measure

Here

1
2

(φ
(cid:16)

−

Sφ), KS(φ

Sφ)

=

−

(cid:17)

1
2

φ, Kφ
(cid:16)

.
(cid:17)

K = (I

S)T KS(I

−

S)

−

(209)

(210)

is positive semi–deﬁnite if KS is. Conversely, every positive semi–deﬁnite K
can be written K = WT W and is thus of form (210) with S = I
W and
KS = I. Including terms of the form of (210) in the error functional forces φ
to be similar to ˜φ.

−

A special case are mappings leaving the norm invariant

(φ, φ) = (Sφ, Sφ) = (φ, ST Sφ).

(211)

For real φ and ˜φ i.e., (Sφ) = (Sφ)∗, this requires ST = S−
Thus, in that case S has to be an orthogonal matrix
written

∈

1 and S∗ = S.
O(N) and can be

S(θ) = eA = e

i θiAi =

∞

1
k!  

k

,

θiAi

!

P

Xk=0

Xi

(212)

AT and real parameters θi. Selecting a set of
with antisymmetric A =
(generators) Ai the matrices obtained be varying the parameters θi form a
Lie group. Up to ﬁrst order the expansion of the exponential function reads
i θiAi. Thus, we can deﬁne an error measure with respect to an
S
inﬁnitesimal symmetry by

1 +

≈

−

P

1
2

Xi  

φ

(1 + θiAi)φ

φ

(1 + θiAi)φ

−

θi

, KS

−

θi

=

(φ,

1
2

!

AT

i KSAiφ).

Xi

(213)

3.4.2 Approximate symmetries

Next we come to the special case of symmetries, i.e., invariance under un-
der coordinate transformations. Symmetry transformations S change the
arguments of a function φ. For example for the translation of a function
φ(x)
c). Therefore it is useful to see how S acts
on the arguments of a function. Denoting the (possibly improper) eigen-
x), i.e.,
vectors of the coordinate operator x with eigenvalue x by (

˜φ(x) = Sφ(x) = φ(x

, x) =

→

−

·

|

56

(214)

(215)

(216)

x) = x
|

x), function values can be expressed as scalar products, e.g. φ(x)
x
|
= (x, φ) for a function in x, or, in two variables, φ(x, y) = (x
y, φ). (Note
that in this ‘eigenvalue’ notation, frequently used by physicists, for example
2x).) Thus, we see that the action of S on some function h(x) is
2
|
equivalent to the action of ST ( = S−

1 if orthogonal) on

x)

x)

⊗

=

|

Sφ(x) = (x, Sφ) = (ST x, φ),

|

or for φ(x, y)

Assuming S = SxSy we may also split the action of S,

(cid:16)

(cid:17)

Sφ(x, y) =

ST (x

y), φ

.

⊗

Sφ(x, y) =

(ST

x x)

⊗

(cid:16)

y, Syφ

.

(cid:17)

An example from physics are vector ﬁelds where x and φ(
, y) form three
dimensional vectors with y representing a linear combination of component
labels of φ.

·

Notice that S
|

x) does, for a general operator S, not have to be an eigen-
vector of the coordinate operator x again. Coordinate transformations, how-
x)
ever, are represented by operators S, which map coordinate eigenvectors
σ(x)) (and not to arbitrary vectors being
to other coordinate eigenvectors
x)). Hence, such coordinate transformations S just
linear combinations of
changes the argument x of a function φ into σ(x), i.e.,

|

|

|

Sφ(x) = φ(σ(x)),

(217)

with σ(x) a permutation or a one–to–one coordinate transformation. Thus,
even for an arbitrary nonlinear coordinate transformation σ the correspond-
ing operator S in the space of φ is linear. (This is one of the reasons why
linear functional analysis is so useful.)

A special case are linear coordinate transformations for which we can
˜φ(x) = Sφ(x) = φ(Sx), with S (in contrast to S) acting in the
write φ(x)
space of x. An example of such S are coordinate rotations which preserve
the norm in x–space, analogously to Eq. (211) for φ, and form a Lie group
i θiAi acting on coordinates, analogously to Eq. (212).
S(θ) = e

→

P

3.4.3 Example: Inﬁnitesimal translations

A Laplacian smoothness prior, for example, can be related to an approxi-
mate symmetry under inﬁnitesimal translations. Consider the group of d–
. This
dimensional translations which is generated by the gradient operator

∇

57

6
can be veriﬁed by recalling the multidimensional Taylor formula for expan-
sion of φ at x

S(θ)φ(x) = e

i θi

iφ(x) =

∇

φ(x) = φ(x + θ).

(218)

∞

(

P

Xk=0

i θi∇i)k
k!

P

Up to ﬁrst order S
error measure of Eq. (213) becomes

1 +

≈

i θi∆i. Hence, for inﬁnitesimal translations, the

(1 + θi∆i)φ

φ

(1 + θi∆i)φ

P

,

−

θi

φ

−

1
2

Xi  

θi

=

(φ,

1
2

!

Xi ∇

T

i ∇iφ) =

−

1
2

(φ, ∆φ).

(219)
assuming vanishing boundary terms and choosing KS = I. This is the clas-
sical Laplacian smoothness term.

3.4.4 Example: Approximate periodicity

As another example, lets us discuss the implementation of approximate pe-
riodicity. To measure the deviation from exact periodicity let us deﬁne the
diﬀerence operators

For periodic boundary conditions (
transpose of

∇
L
θ . Hence, the operator,

∇

R
θ , where (

θ )T denotes the
L

−∇

∇

R
θ φ(x) = φ(x)
L
θ φ(x) = φ(x

∇
∇

φ(x + θ),
φ(x).
θ)

−

−
−
θ )T =
L

∆θ =

L
θ ∇

∇

R
θ =

(
−

∇

θ )T
R

∇

R
θ ,

(220)
(221)

(222)

deﬁned similarly to the Laplacian, is positive deﬁnite, and a possible error
term, enforcing approximate periodicity with period θ, is

1
2

(

∇R(θ)φ,

∇R(θ)φ) =

−

1
2

(φ, ∆θφ) =

dx

φ(x)

φ(x + θ)

(223)

|

−

2.

|

1
2 Z

As every periodic function with φ(x) = φ(x + θ) is in the null space of ∆θ
typically another error term has to be added to get a unique solution of the
stationarity equation. Choosing, for example, a Laplacian smoothness term,
yields

(φ, (∆ + λ∆θ) φ).

(224)

1
2

−

58

In case θ is not known, it can be treated as hyperparameter as discussed in
Section 5.

Alternatively to an implementation by choosing a semi–positive deﬁnite
operator K with symmetric functions in its null space, approximate symme-
tries can be implemented by giving explicitly a symmetric reference function
t(x). For example, 1
t) ) with t(x) = t(x + θ). This possibility
will be discussed in the next section.

t, K(φ

2(φ

−

−

3.5 Non–zero means

≡

A prior energy term (1/2)(φ, K φ) measures the squared K–distance of φ to
the zero function t
0. Choosing a zero mean function for the prior process
is calculationally convenient for Gaussian priors, but by no means mandatory.
In particular, a function φ is in practice often measured relative to some non–
trivial base line. Without further a priori information that base line can in
principle be an arbitrary function. Choosing a zero mean function that base
line does not enter the formulae and remains hidden in the realization of the
measurement process. On the the other hand, including explicitly a non–
zero mean function t, playing the role of a function template (or reference,
target, prototype, base line) and being technically relatively straightforward,
can be a very powerful tool. It allows, for example, to parameterize t(θ) by
introducing hyperparameters (see Section 5) and to specify explicitly diﬀerent
maxima of multimodal functional priors (see Section 6. [123, 124, 125, 126,
127]) All this cannot be done by referring to a single baseline.
Hence, in this section we consider error terms of the form

1
2

φ
(cid:16)

−

t, K (φ

t)

−

.
(cid:17)

(225)

Mean or template functions t allow an easy and straightforward implementa-
tion of prior information in form of examples for φ. They are the continuous
analogue of standard training data. The fact that template functions t are
most times chosen equal to zero, and thus do not appear explicitly in the
error functional, should not obscure the fact that they are of key importance
for any generalization. There are many situations where it can be very valu-
able to include non–zero prior means explicitly. Template functions for φ can
for example result from learning done in the past for the same or for similar
tasks. In particular, consider for example ˜φ(x) to be the output of an empiri-
cal learning system (neural net, decision tree, nearest neighbor methods, . . .)

59

being the result of learning the same or a similar task. Such a ˜φ(x) would be
a natural candidate for a template function t(x). Thus, we see that template
functions could be used for example to allow transfer of knowledge between
similar tasks or to include the results of earlier learning on the same task in
case the original data are lost but the output of another learning system is
still available.

Including non–zero template functions generalizes functional Eφ of Eq.

(187) to

Eφ =

(ln P (φ), N) +

t, K (φ

t)

+ (P (φ), ΛX)

(226)

−

−

1
2
1
2

−

φ
(cid:16)
(φ, K φ)

−

−

(cid:17)

=

(ln P (φ), N) +

(J, φ)+(P (φ), ΛX)+const.(227)

In the language of physics J = Kt represents an external ﬁeld coupling to
φ(x, y), similar, for example, to a magnetic ﬁeld. A non–zero ﬁeld leads to a
non–zero expectation of φ in the no–data case. The φ–independent constant
stands for the term 1
1 J) for invertible K, and can be
2 (J, K−
skipped from the error/energy functional Eφ.

2(t, K t), or 1

The stationarity equation for an Eφ with non–zero template t contains

an inhomogeneous term Kt = J

with, for invertible PP′−

P′(φ)ΛX −

K (φ

t) ,

−

0 = P′(φ)P−

−
= 0,

1(φ)N
1 and ΛX 6
N
−

(cid:16)

ΛX = IX

PP′−

1K (φ

−

t)

.

(cid:17)

(228)

(229)

≡

φ = φ

0 in terms of

Notice that functional (226) can be rewritten as a functional with zero tem-
plate t
t. That is the reason why we have not included
non–zero templates in the previous sections. For general non–additive com-
binations of squared distances of the form (225) non–zero templates cannot
be removed from the functional as we will see in Section 6. Additive combi-
nations of squared error terms, on the other hand, can again be written as
one squared error term, using a generalized ‘bias–variance’–decomposition

−

e

N

1
2

φ

−

Xj=1 (cid:16)
with template average

tj, Kj (φ

tj)

=

t, K (φ

t)

+ Emin

(230)

1
2

φ
(cid:16)

−

−

(cid:17)

−

(cid:17)

t = K−

1

Kjtj,

(231)

N

Xj=1

60

assuming the existence of the inverse of the operator

K =

Kj.

N

Xj=1

(232)

and minimal energy/error

Emin =

V (t1,

tN ) =

(tj, Kj tj)

(t, K t),

(233)

N
2

· · ·

1
2

N

Xj=1

−

which up to a factor N/2 represents a generalized template variance V . We
end with the remark that adding error terms corresponds in its probabilistic
Bayesian interpretation to ANDing independent events. For example, if we
wish to implement that φ is likely to be smooth AND mirror symmetric, we
may add two squared error terms, one related to smoothness and another to
mirror symmetry. According to (230) the result will be a single squared error
term of form (225).

−

Summarizing, we have seen that there are many potentially useful ap-
plications of non–zero template functions. Technically, however, non–zero
template functions can be removed from the formalism by a simple substitu-
tion φ′ = φ
t if the error functional consists of an additive combination of
quadratic prior terms. As most regularized error functionals used in practice
have additive prior terms this is probably the reason that they are formulated
for t
0, meaning that non–zero templates functions (base lines) have to be
treated by including a preprocessing step switching from φ to φ′. We will see
in Section 6 that for general error functionals templates cannot be removed
by a simple substitution and do enter the error functionals explicitly.

≡

3.6 Quadratic density estimation and empirical risk

minimization

Interpreting an energy or error functional E probabilistically, i.e., assuming
βE + c to be the logarithm of a posterior probability under study, the
−
i ln Pi. Technically, however, it
form of the training data term has to be
would be easier to replace that data term by one which is quadratic in the
probability P of interest.

−

P

Indeed, we have mentioned in Section 2.5 that such functionals can be
justiﬁed within the framework of empirical risk minimization. From that

61

Frequentist point of view an error functional E(P ), is not derived from a
i l(xi, yi, P ), ap-
log–posterior, but represents an empirical risk ˆr(P, f ) =
proximating an expected risk r(P, f ) for action a = P . This is possible
under the assumption that training data are sampled according to the true
p(x, y
f ). In that interpretation one is therefore not restricted to a log–loss
for training data but may as well choose for training data a quadratic loss
like

P

|

1
2

(cid:16)

−

P

Pemp, KD (P

Pemp)

,

−

(cid:17)

(234)

choosing a reference density P emp and a real symmetric positive (semi–)/-
deﬁnite KD.

Approximating a joint probability p(x, y
would have to be the joint empirical density

|

h) the reference density Pemp

P joint

emp (x, y) =

δ(x

xi)δ(y

yi),

−

−

(235)

1
n

n

Xi

emp = N/n, as obtained from the training data. Approximating con-
x, h) the reference Pemp has to be chosen as condi-

i.e., P joint
ditional probabilities p(y
tional empirical density,

|

(236)

(237)

Pemp(x, y) =

i δ(x

xi)δ(y

yi)

−
i δ(x

−
xi)

−

P

=

N(x, y)
nx

,

or, deﬁning the diagonal matrix NX(x, x′, y, y′) = δ(x
δ(x

x′)δ(y

y′)

xi)

i δ(x

−

−

−

−

P

x′)δ(y

y′)NX(x) =

−

P

Pemp = N−

1
X N.

This, however, is only a valid expression if NX(x)
= 0, meaning that for all
x at least one measured value has to be available. For x variables with a
large number of possible values, this cannot be assumed. For continuous x
variables it is even impossible.

Hence, approximating conditional empirical densities either non–data x–
values must be excluded from the integration in (234) by using an operator
x′), or Pemp must be deﬁned also for
KD containing the projector
such non–data x–values. For existing VX = IX1 =
dy 1, a possible extension
˜Pemp of Pemp would be to assume a uniform density for non–data x values,

xD δ(x
∈

−

P

x′

R

62

6
yielding

˜Pemp(x, y) = 


P

yi)

xi)δ(y
−
xi)

i δ(x

−
i δ(x
1
dy 1

−

P

for

for

i δ(x

xi)

= 0,

P

i δ(x

xi) = 0.

−

−

P

(238)

This introduces a bias towards uniform probabilities, but has the advantage
to give a empirical density for all x and to fulﬁll the conditional normalization
requirements.



R

Instead of a quadratic term in P , one might consider a quadratic term in
the log–probability L. The log–probability, however, is minus inﬁnity at all
D. To work with a ﬁnite expression, one can choose
non–data points (x, y)
small ǫ(y) and approximate Pemp by

6∈

P ǫ

emp(x, y) =

ǫ(y) +

i δ(x
dy ǫ(y) +
P

−

xi)δ(y
i δ(x

−
xi)

yi)

,

(239)

R
dy ǫ(y) exists. For ǫ(y)

provided
ln P ǫ
emp >
A quadratic data term in P results in an error functional

= 0 also P ǫ

exists.

R
−∞

= 0,

P

x and Lǫ
∀

emp =

−
emp(x, y)

˜EP =

1
2

(cid:16)

−

1
2

−

(cid:17)

P

Pemp, KD (P

Pemp)

+

(P, K P ) + (P, ΛX),

(240)

skipping the constant part of the ΛX–terms. In (240) the empirical density
Pemp may be replaced by ˜Pemp of (238).

k Rk and R(y, y′) = θ(y

Positive (semi–)deﬁnite operators KD have a square root and can be
written in the form RT R. One possibility, skipping for the sake of simplicity
x in the following, is to choose as square root R the integration operator, i.e.,
y′). Thus, φ = RP transforms the density
R =
1φ.
function P in the distribution function φ, and we have P = P (φ) = R−
Here the inverse R−
k ∇yk (with appropriate
1 =
k ∆k is the product of one–
boundary condition) and
R−
dimensional Laplacians ∆k = ∂2/∂y2
k. Adding for example a regularizing
term (165) λ
2 (P, P ) gives

1 is the diﬀerentiation operator
1

RT

−

−

N

Q

Q

(cid:17)

(cid:16)

−

˜EP =

1
2

(cid:16)

P

−

=

1
2  

φ
(cid:16)

−

Pemp , RT R (P

Pemp)

+

(P, P )

λ
2

−

(cid:17)

φemp, φ

φemp

−

λ

φ,
(cid:16)

−

(cid:17)

Yk

∆k φ

!

(cid:17)

(241)

(242)

63

6
6
6
=

1
2m2

φ, (

∆k + m2I)φ

(φ , φemp) +

(φemp, φemp).

(243)

−

Yk

−

(cid:17)

(cid:16)
with m2 = λ−
given by φemp(y) = 1
n
x′)θ(y
P

1. Here the empirical distribution function φemp = RPemp is
yi) (or, including the x variable, φemp(x, y) =
= 0 which could be extended to a

−
1
yi) for NX(x)
NX (x)
linear ˜φ = R ˜Pemp for NX(x) = 0). The stationarity equation yields

xD δ(x
∈

i θ(y

−

−

P

x′

1
2

φ = m2

∆k + m2I

φemp.

(244)

1

−

!

 −

Yk

Q

−

∆ + m2I)−

k φ) the operator becomes (

1 which has the
For dy = 1 (or φ =
structure of a free massive propagator for a scalar ﬁeld with mass m2 and
is calculated below. As already mentioned the normalization and positivity
condition for P appear for φ as boundary and monotonicity conditions. For
non–constant P the monotonicity condition has not to be implemented by
Lagrange multipliers as the gradient at the stationary point has no compo-
nents pointing into the forbidden area. (But the conditions nevertheless have
to be checked.) Kernel methods of density estimation, like the use of Parzen
windows, can be founded on such quadratic regularization functionals [208].
Indeed, the one–dimensional Eq. (244) is equivalent to the use of Parzens
kernel in density estimation [169, 156].

3.7 Regression

3.7.1 Gaussian regression

An important special case of density estimation leading to quadratic data
terms is regression for independent training data with Gaussian likelihoods

xi, h) =

p(yi|

1
√2πσ

(yi−h(xi))2
2σ2

,

e−

(245)

xi, h) is speciﬁed by φ = h and the logarithmic term

with ﬁxed, but possibly xi–dependent, variance σ2. In that case P (x, y) =
i ln Pi becomes
p(yi|
quadratic in the regression function h(xi), i.e., of the form (225). In an inter-
pretation as empirical risk minimization quadratic error terms corresponds
a(x))2 for ac-
to the choice of a squared error loss function l(x, y, a) = (y
tion a(x). Similarly, the technical analogon of Bayesian priors are additional
(regularizing) cost terms.

−

P

64

6
We have remarked in Section 2.3 that for continuous x measurement of
h(x) has to be understood as measurement of a h(˜x) =
dx ϑ(x)h(x) for
sharply peaked ϑ(x). We assume here that the discretization of h used in
numerical calculations takes care of that averaging. Divergent quantities like
δ–functionals, used here for convenience, will then not be present.

R

We now combine Gaussian data terms and a Gaussian (speciﬁc) prior
with prior operator K0(x, x′) and deﬁne for training data xi, yi the operator

Ki(x, x′) = δ(x

xi)δ(x

x′),

(246)

−
and training data templates t = yi. We also allow a general prior template
t0 but remark that it is often chosen identically zero. According to (230) the
resulting functional can be written in the following forms, useful for diﬀerent
purposes,

−

Eh =

(h(xi)

yi)2 +

( h

t0, K0 (h

t0) )X

−

−

−

1
2

( h

ti, Ki(h

ti) )X +

( h

t0, K0 (h

t0) )X

(248)

−

−

−

−

tD, KD(h

tD))X +

(h

t0, K0(h

−

−

−

t0))X +ED

min (249)

1
2

1
2

( h

t, K (h

t) )X + Emin,

n

Xi=1
n

Xi=1
(h

1
2
1
2
1
2
1
2

−

−

=

=

=

with

KD =

Ki,

1
tD = K−
D

Kiti,

K =

Ki,

t = K−

1

Kiti,

n

Xi=1
n

Xi=0

and h–independent minimal errors,

ED

min =

(ti, Kiti)X + (tD, KDtD)X

,

!

Emin =

(ti, Kiti)X + (t, Kt)X

,

!

Xi=0
being proportional to the “generalized variances” VD = 2ED
2Emin/(n + 1). The scalar product (
·

min/n and V =
)X stands for x–integration only, for

·

,

65

(247)

(250)

(251)

(252)

(253)

(254)

−

n

Xi=1
n

Xi=0

n

Xi=1
n

1
2  
1
2  

the sake of simplicity however, we will skip the subscript X in the following.
The data operator KD

KD(x, x′) =

δ(x

xi)δ(x

x′) = nx δ(x

x′),

(255)

−

−

−

n

Xi=1

contains for discrete x on its diagonal the number of measurements at x,

nx = NX(x) =

δ(x

xi),

−

which is zero for x not in the training data. As already mentioned for con-
1
tinuous x a integration around a neighborhood of xi is required. K−
D is a
short hand notation for the inverse within the space of training data

K−

1
D = (IDKDID)−

1 = δ(x

x′)/nx,

−

ID denoting the projector into the space of training data

ID = δ(x

x′)

δ(x

xi).

−

−

Notice that the sum is not over all n training points xi but only over the
(Again for continuous x an integration around xi is
˜n
n diﬀerent xi.
required to ensure I2
D = ID). Hence, the data template tD becomes the mean
of y–values measured at x

≤

tD(x) =

y(xj),

(259)

and tD(x) = 0 for nx = 0. Normalization of P (x, y) is not inﬂuenced by a
change in h(x) so the Lagrange multiplier terms have been skipped.
The stationarity equation is most easily obtained from (250),

−
1 exists the unique solution
It is linear and has on a space where K−

0 = K(h

t).

(256)

(257)

(258)

(260)

(261)

n

Xi=1

˜n

Xi=1

1
nx

nx

j=1
X
xj =x

h = t.

66

We remark that K can be invertible (and usually is so the learning problem
1, necessary to
is well deﬁned) even if K0 is not invertible. The inverse K−
calculate t, is training data dependent and represents the covariance opera-
tor/matrix of a Gaussian posterior process. In many practical cases, however,
the prior covariance K−
(or in case of a null space a pseudo inverse of K0)
0
is directly given or can be calculated. Then an inversion of a ﬁnite dimen-
sional matrix in data space is suﬃcient to ﬁnd the minimum of the energy
Eh [212, 71].

1

Invertible K0: Let us assume ﬁrst deal with the case of an invertible
K0. It is the best to begin the stationarity equation as obtained from (248)
or (249)

n

0 =

Ki(h

Xi=1
= KD(h

ti) + K0(h

−
tD) + K0(h

t0)

−
t0).

−

−

h = t0 + K−

1

0 KD(tD −

h),

a = KD(tD −

h),

h = t0 + K−

1
0 a.

For existing K−
0

1

one can introduce

to obtain

Inserting Eq. (266) into Eq. (265) one ﬁnds an equation for a

I + KDK−
0

1

(cid:16)

(cid:17)

a = KD(tD −

t0).

Multiplying Eq. (267) from the left by the projector ID and using

KDID = IDKD,

a = IDa,

tD = IDtD,

(268)

one obtains an equation in data space

ID + KDK−

1
0,DD

(cid:16)

(cid:17)

a = KD(tD −

t0,D),

where

K−

1
0,DD = (K−

1
0 )DD = IDK−

= (K0,DD)−

1,

t0,D = IDt0.

(270)

1

0 ID 6

67

(262)

(263)

(264)

(265)

(266)

(267)

(269)

Thus,

where

and

(271)

(272)

(273)

a = CDD b,

CDD =

ID + KDK−

1
0,DD

1

−

,

(cid:17)

b = KD(tD −

t0).

(cid:16)

(cid:17)

In components Eq. (271) reads,

−

1

δkl + nxkK−

1
0 (xk, xl)

a(xl) = nxk (tD(xk)

t0(xk)) .

(274)

Xl (cid:16)

−

Having calculated a the solution h is given by Eq. (266)

−

Xm=0 (cid:16)

(cid:16)

m

(cid:17)

1

−

(cid:17)

1

1
0 CDDb = t0 + K−
0

h = t0 + K−

K−
(cid:16)
Eq. (275) can also be obtained directly from Eq. (261) and the deﬁnitions
(252), without introducing the auxiliary variable a, using the decomposition
K0t0 =

KDt0 + (K0 + KD)t0 and

1
D + K−

(tD −

1
0,DD

(275)

t0).

(cid:17)

−

K−

1KD = K−
0

1

I + KDK−
0

1

1

−

KD = K−
0

∞

1

KDK−
0

1

KD

(276)

(cid:16)

1

= K−
0

∞

−

Xm=0 (cid:16)

(cid:17)

m

(cid:17)

KDIDK−

1
0 ID

KD = K−
0

1

ID + KDK−

1
0,DD

KD.

(277)

K−

1
0 CDD is also known as equivalent kernel due to its relation to kernel

smoothing techniques [194, 87, 83, 71].

Interestingly, Eq. (266) still holds for non–quadratic data terms of the
form gD(h) with any diﬀerentiable function fulﬁlling g(h) = g(hD), where hD
= IDh is the restriction of h to data space. Hence, also the function of func-
tional derivatives with respect to h(x) is restricted to data space, i.e., g′(hD)
= g′D(hD) with g′D = IDg′ and g′(h, x) = δg(h)/δh(x). For example, g(h) =
yi) with V a diﬀerentiable function. The ﬁnite dimensional
vector a is then found by solving a nonlinear equation instead of a linear one
P
[68, 70].

n
i=1 V (h(xi)

−

Furthermore, one can study vector ﬁelds, i.e., the case where, besides
possibly x, also y, and thus h(x), is a vector for given x. (Considering the
variable indicating the vector components of y as part of the x–variable, this

68

is a situation where a ﬁxed number of one–dimensional y, corresponding to a
subspace of X with ﬁxed dimension, is always measured simultaneously.) In
that case the diagonal Ki of Eq. (246) can be replaced by a version with non–
zero oﬀ–diagonal elements Kα,α′ between the vector components α of y. This
corresponds to a multi–dimensional Gaussian data generating probability

xi, h) =

p(yi|

1
2

1
2

e−

det Ki
(2π)

k
2

P

α,α′ (yi,α

−

hα(xi)) K

i,α,α′ (xi)(yi,α′

hα′ (xi)),

−

(278)

for k–dimensional vector yi with components yi,α.

Non-invertible K0: For non–invertible K0 one can solve for h using
the Moore–Penrose inverse K#
0 . Let us ﬁrst recall some basic facts [53, 151,
13, 112]. A pseudo inverse of (a possibly non–square) A is deﬁned by the
conditions

A#AA# = A, AA#A = A#,

and becomes for real A the unique Moore–Penrose inverse A# if

(AA#)T = AA#,

(A#A)T = A#A.

A linear equation

is solvable if

In that case the solution is

Ax = b

AA#b = b.

x = A#b + x0 = A#b + y

A#Ay,

−
A#Ay is solution of the homogeneous equation Ax0 = 0 and
where x0 = y
vector y is arbitrary. Hence, x0 can be expanded in an orthonormalized basis
ψl of the null space of A

−

For an A which can be diagonalized, i.e., A = M−
the Moore–Penrose inverse is A# = M−

1D#M. Therefore

1DM with diagonal D,

x0 =

clψl.

Xl

AA# = A#A = I1 = I

I0.

−

69

(279)

(280)

(281)

(282)

(283)

(284)

(285)

−

(286)

(287)

(290)

(291)
(292)

where I0 =
= M−

P

l ψlψT
l

is the projector into the zero space of A and I1 = I

I0

1DD#M. Thus, the solvability condition Eq. (282) becomes

or in terms of ψl

I0b = 0,

( ψl, b) = 0,

l,

∀

meaning that the inhomogeneity b must have no components within the zero
space of A.

Now we apply this to Eq. (263) where K0 is diagonalizable because pos-
itive semi deﬁnite. (In this case M is an orthogonal matrix and the entries
of D are real and larger or equal to zero.) Hence, one obtains under the
condition

I0 (K0t0 + KD(tD −

h)) = 0,

(288)

for Eq. (283)

h = K#
where K0h0 = 0 so that h0 =
l clψl can be expanded in an orthonormalized
basis ψl of the null space of K0, assumed here to be of ﬁnite dimension. To
ﬁnd an equation in data space deﬁne the vector

0 (K0t0 + KD(tD −

h)) + h0,

(289)

P

to get from Eqs.(288) and (289)

a = KD(tD −

h),

0 = ( ψl, K0t0) + ( ψl, a),
h = K#

0 (K0t0 + a) +

l
∀
clψl.

Xl

These equations have to be solved for a and the coeﬃcients cl. Inserting Eq.
(292) into the deﬁnition (290) gives

(I + KDK#

0 )a = KDtD −

KDI1t0 −

KD

clψl,

(293)

Xl

using K#
condition (288) becomes

0 K0 = I1 according to Eq. (285). Using a = IDa the solvability

ψl(xi)a =

( ψl, K0t0 ),

l,

∀

(294)

˜n

Xi=1

−

70

the sum going over diﬀerent xi only. Eq. (293) for a and cl reads in data
space, similar to Eq. (269),

(295)
0 has been assumed invertible and ˜b is given by the
where ˜C−
right hand side of Eq. (293). Inserting into Eq. (292) the solution ﬁnally can
be written

1 = I + KDK#

a = ˜C˜b,

h = I1t0 + K#
0

˜C˜b +

clψl.

(296)

Xl

Again, general non–quadratic data terms g(hD) can be allowed. In that
case δg(hD)/δh(x) = g′(hD, x) = (IDg′)(hD, x) and Eq. (290) becomes the
nonlinear equation

a = g′(hD) = g′

ID

K#

0 (K0t0 + KD(tD −

h)) + h0

.

(297)

(cid:16)

(cid:16)

(cid:17)(cid:17)

The solution(s) a of that equation have then to be inserted in Eq. (292).

3.7.2 Exact predictive density

For Gaussian regression the predictive density under training data D and
prior D0 can be found analytically without resorting to a saddle point ap-
proximation. The predictive density is deﬁned as the h-integral

p(y

x, D, D0) =

dh p(y

|

dh p(y

=

Z

R

D, D0)

x, h)p(h
|
|
x, h)p(yD|

|
dh p(yD|

xD, h)p(h
|
D0)

xD, h)p(h
|

D0)

R

=

p(y, yD|
p(yD|
Denoting training data values yi by ti sampled with covariance Ki concen-
trated on xi and analogously test data values y = yn+1 by tn+1 sampled with
i
(co–)variance Kn+1, we have for 1

x, xD, D0)
xD, D0)

n + 1

(298)

.

≤

≤

and

p(yi|

p(h
|

xi, h) = det(Ki/2π)

2 e−

1

1
2

h

−

ti, Ki(h

ti)

−

(cid:16)

D0) = det(K0/2π)

2 e−

1

t0, K0(h

t0)

−

1
2

h

−

(cid:16)

,

(cid:17)

,

(cid:17)

(299)

(300)

71

hence

p(y

x, D, D0) =

|

dh e−

dh e−

1
2

n+1
i=0

P

1
2

n
i=0

P

h

−

h

−

(cid:16)

(cid:16)

ti, Ki(h

ti)

−

ti, Ki(h

ti)

−

+ 1
2

+ 1
2

(cid:17)

(cid:17)

P

n

P

n+1

i=0 ln deti(Ki/2π)

i=0 ln deti(Ki/2π)

R

R

.

(301)

Here we have this time written explicitly deti(Ki/2π) for a determinant calcu-
lated in that space where Ki is invertible. This is useful because for example
= deti KiK0. Using the generalized ‘bias–variance’–
in general deti Ki det K0 6
decomposition (230) yields

p(y

x, D, D0) =

|

t+, K+(h

t+)

+ n

2 V++ 1

2

−

n+1

i=0 ln deti(Ki/2π)

dh e−

1
2

h

−

(cid:16)

R

dh e−

1
2

h

−

t, K(h

t)

−

(cid:17)
+ n

2 V + 1

2

(cid:17)

P
n
i=0 ln deti(Ki/2π)

,

(302)

with

R

t = K−

1

Kiti,

K =

Ki,

t+ = K−
+

Kiti,

K+ =

Ki,

V =

ti, Kiti

t,

K
n

−

(cid:17)

(cid:16)

(cid:16)

n

Xi=0
n+1

Xi=0

1

n

Xi=0 (cid:16)
n+1

1
n

1
n

P

n

Xi=0

n+1

Xi=0
,
t
(cid:17)
K+
n

V+ =

ti, Kiti

t+,

−

(cid:16)

t+

.
(cid:17)

(cid:17)
Now the h–integration can be performed

Xi=0 (cid:16)

p(y

x, D, D0) =

|

e−

n

2

2 V++ 1
2 V + 1

n

2

e−

n+1

i=0 ln deti(Ki/2π)
i=0 ln deti(Ki/2π)

n

P

−

−

1

2 ln det(K+/2π)
2 ln det(K/2π)

1

P
Canceling common factors, writing again y for tn+1, Kx for Kn+1, detx for
detn+1, and using K+t+ = Kt + Kxy, this becomes

p(y

x, D, D0) = e−

|

1

2 (y,Ky y)+(y,Ky t)+ 1

2(t,(KK−1

+

K

K) t)+ 1

2 ln detx(KxK−1

+

−

K/2π).

(308)

Here we introduced Ky = KT

KxK−

1
+ Kx and used that

y = Kx −

det K−

1K+ = det(I

K−

1Kx) = detxK−

1K+

(309)

−

72

(303)

(304)

(305)

(306)

(307)

can be calculated in the space of test data x. This follows from K = K+ −
and the equality
A 0

1

Kx

det

 
1Kx, B = (I

−
B

= det(1

A)

1 !
1Kx, and Ix denoting the projector into

−

(310)

with A = IxK−
the space of test data x. Finally

−

Ix)K−

Ky = Kx −

KxK−

1
+ Kx = KxK−

1
+ K = (K

KK−

1
+ K),

(311)

−

yields the correct normalization of the predictive density

(312)

(313)

(314)

(315)

(316)

p(y

x, D, D0) = e−

|

1
2

y

−

¯y, Ky(y

¯y)

−

+ 1

2 ln detx(Ky/2π)

,

(cid:17)

with mean and covariance

¯y = t = K−

1

Kiti,

(cid:16)

n

Xi=0
KxK−

1
y =

K−

Kx −
It is useful to express the posterior covariance K−
K−

1
0 . According to

= K−

(cid:17)

(cid:16)

−

1
+ Kx

1

1
x + IxK−

1Ix.

1 by the prior covariance

1 + A B

 

0

1

−
1 !

=

 

1

(1 + A)−
0

−

(1 + A)−
1

1B

,

!

1
0,D ¯D, and K−

1
0,DD = IDK−

1
0 ID, K−

1
0,D ¯D =

with A = KDK−
1
IDK−
0 I ¯D, I ¯D = I

1
0,DD, B = KDK−
ID we ﬁnd

−

K−

1 = K−
0

1

I + KDK−
0

1

1

−

(cid:16)

(cid:17)
1
ID + KDK−
0,DD

1

1

−

(cid:18)(cid:16)

= K−
0

ID + KDK−
(cid:16)
1
D = (IDKDID)−
1. This means for example that K−
0,DD and it is not enough to invert IDK0ID = K0,DD 6

Notice that while K−
(IDK0ID)−
K−

(cid:17)
1
1 in general K−
0,DD = IDK−
=
0 ID 6
1
0 has to be known to ﬁnd
1.
In
= (K−

1
0,DD)−

KDK−

−

(cid:19)

(cid:17)

.

1

1

1
0,D ¯D + I ¯D

1
0,DD

−

1

1
0,DD

1

−

=

K−

1
D + K−

1
0,DD

K−

1
D , so Eq. (316) can

1

−

data space
be manipulated to give
(cid:16)

ID + KDK−

K−

1 = K−
0

1

ID

K−

1
D + K−

1
0,DD

(317)

(cid:17)

I

−

(cid:18)

(cid:16)

(cid:16)

73

(cid:17)

(cid:17)

1

−

IDK−
0

1

.

(cid:19)

This allows now to express the predictive mean (313) and covariance (314)
by the prior covariance

1

¯y = t0 + K−
0

K−
(cid:16)
1
1
y = Kx + K−
0,xx −

K−

1
D + K−

1
0,DD

K−

1
0,xD

(cid:16)

1

t0),

−

(tD −
(cid:17)
1
D + K−
K−

1
0,DD

1

−

K−

1
0,Dx.

(cid:17)

(318)

(319)

Thus, for given prior covariance K−

inverting the ˜n

˜n matrix

K =

×

1
0 both, ¯y and K−
K−

1
1
0,DD + K−
D

−

.

1

1
y , can be calculated by

Comparison of Eqs.(318,319) with the maximum posterior solution h∗ of
Eq. (275) now shows that for Gaussian regression the exact predictive density
p(y
x, h∗) have the
same mean

x, D, D0) and its maximum posterior approximation p(y

f

(cid:17)

(cid:16)

|

|

t =

dy y p(y

x, D, D0) =

dy y p(y

x, h∗).

(320)

Z

|

Z

|

The variances, however, diﬀer by the term IxK−

1Ix.

According to the results of Section 2.2.2 the mean of the predictive density
is the optimal choice under squared–error loss (52). For Gaussian regression,
therefore the optimal regression function a∗(x) is the same for squared–error
loss in exact and in maximum posterior treatment and thus also for log–loss
(for Gaussian p(y

x, a) with ﬁxed variance)

|

a∗MPA,log = a∗exact,log = a∗MPA,sq. = a∗exact,sq. = h∗ = t.

(321)

In case the space of possible p(y
x, a) is not restricted to Gaussian densi-
ties with ﬁxed variance, the variance of the optimal density under log–loss
1Ix from its maximum poste-
p(y
x, a∗exact,log) = p(y
|
rior approximation p(y

x, D, D0) diﬀers by IxK−
x, h∗).

x, a∗MPA,log) = p(y

|

|

|

|

3.7.3 Gaussian mixture regression (cluster regression)

Generalizing Gaussian regression the likelihoods may be modeled by a mix-
ture of m Gaussians

p(y

x, h) =

|

m
k p(k) e−

dy
P

m
k p(k) e−

β
2 (y

hk(x))2

−
β
2 (y

−

hk(x))2 ,

R

P

m

(322)

2 . Hence, h is here
where the normalization factor is found as
speciﬁed by mixing coeﬃcients p(k) and a vector of regression functions hk(x)

k p(k)

(cid:16)

(cid:17)

β
2π

P

74

specifying the x–dependent location of the kth cluster centroid of the mixture
model. A simple prior for hk(x) is a smoothness prior diagonal in the cluster
components. As any density p(y
x, h) can be approximated arbitrarily well
by a mixture with large enough m such cluster regression models allows to
interpolate between Gaussian regression and more ﬂexible density estimation.

|

The posterior density becomes for independent data

D, D0) =

p(h
|

p(h
D0)
|
xD, D0)
p(yD|

n

Yi P

m
k p(k) e−
m
k p(k)

β
2 (yi

−

hk(xi))2

m
2

β
2π

(cid:16)

(cid:17)

P

.

(323)

D0) —
Maximizing that posterior is — for ﬁxed x, uniform p(k) and p(h
|
equivalent to the clustering approach of Rose, Gurewitz, and Fox for squared
distance costs [188].

3.7.4 Support vector machines and regression

Expanding the regression function h(x) in a basis of eigenfunctions Ψk of K0

K0 =

λkΨkΨT
k ,

h(x) =

nkΨk(x)

(324)

Xk
yields for functional (247)

Xk

2

Eh =

Xi  

Xk

nkΨk(xi)

yi

!

−

+

2.

λk|

nk|

Xk

(325)

Under the assumption of output noise for training data the data terms may
for example be replaced by the logarithm of a mixture of Gaussians. Such
mixture functions with varying mean can develop ﬂat regions where the error
is insensitive (robust) to changes of h. Analogously, Gaussians with varying
mean can be added to obtain errors which are ﬂat compared to Gaussians
for large absolute errors. Similarly to such Gaussian mixtures the mean–
h(xi))2 may be replaced by an ǫ–insensitive
square error data term (yi −
|ǫ, which is zero for absolute errors smaller ǫ and linear for
h(xi)
error
larger absolute errors (see Fig.5). This results in a quadratic programming
problem and is equivalent to Vapnik’s support vector machine [209, 69, 210,
198, 199, 44]. For a more detailed discussion of the relation between support
vector machines and Gaussian processes see [213, 192].

yi −

|

75

Figure 5: Three robust error functions which are insensitive to small errors.
Left: Logarithm of mixture with two Gaussians with equal variance and
diﬀerent means. Middle: Logarithm of mixture with 11 Gaussians with equal
variance and diﬀerent means. Right: ǫ–insensitive error.

3.8 Classiﬁcation

In classiﬁcation (or pattern recognition) tasks the independent visible vari-
able y takes discrete values (group, cluster or pattern labels) [14, 56, 21, 42].
k Pk(x, h) = 1. Having re-
We write y = k and p(y
ceived classiﬁcation data D =
n
the density estimation
}
error functional for a prior on function φ (with components φk and P =
P (φ)) reads

x, h) = Pk(x, h), i.e.,
i
(xi, ki)

P
≤

≤

{

1

|

|

Ecl. =

ln Pki(xi; φ) +

φ

t, K (φ

t)

+ (P (φ), ΛX).

(326)

n

Xi

1
2

(cid:16)

−

−

(cid:17)

In classiﬁcation the scalar product corresponds to an integral over x and a
summation over k, e.g.,

t, K (φ

t)

=

dx dx′(φk(x)

tk(x))Kk,k′(x, x′)(φk′(x′)

tk′(x′)),

φ
(cid:16)

−

−

(cid:17)

Xk,k′ Z

−

−

(327)

and (P, ΛX) =

dx ΛX(x)

k Pk(x).

For zero–one loss l(x, k, a) = δk,a(x) — a typical loss function for classiﬁ-
cation problems — the optimal decision (or Bayes classiﬁer) is given by the
mode of the predictive density (see Section 2.2.2), i.e.,

P

R

a(x) = argmaxk p(k

x, D, D0).

(328)

In saddle point approximation p(k
|
ing Ecl.(φ) can be found by solving the stationarity equation (228).

x, φ∗) where φ∗ minimiz-

p(k

≈

|

For the choice φk = Pk positivity and normalization must be ensured.
For φ = L with P = eL positivity is automatically fulﬁlled but the Lagrange
multiplier must be included to ensure normalization.

|
x, D, D0)

76

likelihood p(y

x, h)

|

problem type

of general form

density estimation

discrete y

classiﬁcation

Gaussian with ﬁxed variance

mixture of Gaussians

regression

clustering

quantum mechanical likelihood inverse quantum mechanics

Table 3: Special cases of density estimation

P

Normalization is guaranteed by using unnormalized probabilities φk =
l zl (for which positivity has to be checked) or shifted log–
zk, P = zk/
l egl. In that case
likelihoods φk = gk with gk = Lk +ln
the nonlocal normalization terms are part of the likelihood and no Lagrange
P
multiplier has to be used [219]. The resulting equation can be solved in the
space deﬁned by the X–data (see Eq. (154)). The restriction of φk = gk to
linear functions φk(x) = wkx + bk yields log–linear models [143]. Recently
a mean ﬁeld theory for Gaussian Process classiﬁcation has been developed
[164, 166].

l eLl, i.e., Pk = egk/

P

Table 3 lists some special cases of density estimation. The last line of the
table, referring to inverse quantum mechanics, will be discussed in the next
section.

3.9

Inverse quantum mechanics

Up to now we have formulated the learning problem in terms of a function φ
having a simple, e.g., pointwise, relation to P . Nonlocalities in the relation
between φ and P was only due to the normalization condition, or, working
with the distribution function, due to an integration. Inverse problems for
quantum mechanical systems provide examples of more complicated, nonlocal
x, φ) and the hidden variables φ
x, h) = p(y
relations between likelihoods p(y
the theory is formulated in. To show the ﬂexibility of Bayesian Field Theory
we will give in the following a short introduction to its application to inverse

|

|

77

quantum mechanics. A more detailed discussion of inverse quantum problems
including numerical applications can be found in [124, 134, 133, 129, 206].

The state of a quantum mechanical systems can be completely described
by giving its density operator ρ. The density operator of a speciﬁc system
depends on its preparation and its Hamiltonian, governing the time evolution
of the system. The inverse problem of quantum mechanics consists in the
reconstruction of ρ from observational data. Typically, one studies systems
with identical preparation but diﬀering Hamiltonians. Consider for example
Hamiltonians have the form H = T + V, consisting of a kinetic energy part
T and a potential V. Assuming the kinetic energy to be ﬁxed, the inverse
problem is that of reconstructing the potential V from measurements. A
local potential V(y, y′) = V (y)δ(y
y′) is speciﬁed by a function V (y). Thus,
for reconstructing a local potential it is the function V (y) which determines
X, V ) = P (φ) and it is natural
X, ρ) = p(y
the likelihood p(y
to formulate the prior in terms of the function φ = V . The possibilities of
implementing prior information for V are similar to those we discuss in this
paper for general density estimation problems.
It is the likelihood model
where inverse quantum mechanics diﬀers from general density estimation.

x, h) = p(y

−

|

|

|

Measuring quantum systems the variable x corresponds to an hermitian
operator X. The possible outcome y of measurements are given by the eigen-
values of X, i.e.,

X

y >= y

y >,

|

|

|

y >, with dual < y

, denotes the eigenfunction with eigenvalue y. (For
where
the sake of simplicity we assume nondegenerate eigenvalues, the generaliza-
tion to the degenerate case being straightforward.) Deﬁning the projector

|

the likelihood model of quantum mechanics is given by

In the simplest case, where the system is in a pure state, say the ground

state ϕ0 of H fulﬁlling

the density operator is

ΠX,y =

y >< y

|

|

p(y

x, ρ) = Tr(ΠX,yρ).

|

H

ϕ0 >= E0|

|

ϕ0 >,

ρ = ρ2 =

ϕ0 >< ϕ0|

,

|

78

(329)

(330)

(331)

(332)

(333)

general pure state

stationary pure state

ground state

scattering

general mixture state

stationary mixture state

canonical ensemble

ρ

ψ >< ψ

|

|

ϕi(H) >< ϕi(H)

|
ϕ0(H)

|
>< ϕ0(H)

|

|

|

|

|

k p(k)

|

ψk >< ψk|
ϕi(H) >< ϕi(H)

H)

P
i p(i
|
|
(Tr e−

P

βH)−

1e−

βH

time–dependent pure state

U(t, t0)ψ(t0) >< U(t, t0)ψ(t0)

|

lim t→∞

t0→−∞ |

U(t, t0)ψ(t0) >< U(t, t0)ψ(t0)

|

Table 4: The most common examples of density operators for quantum
systems. In this Table ψ denotes an arbitrary pure state, ϕi represents an
eigenstate of Hamiltonian H. The unitary time evolution operator for a
t0)H. In scattering
time–independent Hamiltonian H is given by U = e−
one imposes typically additional speciﬁc boundary conditions on the initial
and ﬁnal states.

i(t

−

and the likelihood (331) becomes

p(y

x, h) = p(y

|

X, ρ) = Tr(
|

ϕ0 >< ϕ0|

|

y >< y

) =

ϕ0(y)

|

|

2.

|

(334)

Other common choices for ρ are shown in Table 4.

In contrast to ideal measurements on classical systems, quantum mea-
surements change the state of the system. Thus, in case one is interested
in repeated measurements for the same ρ, that density operator has to be
prepared before each measurement. For a stationary state at ﬁnite tempera-
ture, for example, this can be achieved by waiting until the system is again
in thermal equilibrium.

For a Maximum A Posteriori Approximation the functional derivative of
the likelihood is needed. Thus, for reconstructing a local potential we have

79

to calculate

δV (y)p(y

X, V ).

(335)

|
To be speciﬁc, let us assume we measure particle coordinates, meaning we
have chosen X to be the coordinate operator. For a system prepared to be
in the ground state of the unknown H, we thus have to ﬁnd,

δV (y)|

ϕ0(y)

2.

|

(336)

For that purpose, we take the functional derivative of Eq. (332), which yields

Projecting from the left by < ϕ0|
a local potential δV (y)H(y′, y′′) = δ(y

−

|

(H

E0)

δV (y)ϕ0 >= (δV (y)H

δV (y)E0)

ϕ0 > .

(337)

|
, using again Eq. (332) and the fact that for

−

y′)δ(y′

y′′), shows that

−
δV (y)H

−
ϕ0 >=

|

ϕ0(y)

|

2.

|

δV (y)E0 =< ϕ0|

(338)

δV (y)ϕ0 > = 0 and inserting a complete basis of eigenfunctions

Choosing < ϕ0|
ϕj > of H, we end up with
|

δV (y)ϕ0(y′) =

1
E0 −
From this the functional derivative of the quantum mechanical log–likelihood
(336) corresponding to data point yi can be obtained easily,

ϕj(y′)ϕ∗j (y)ϕ0(y).

(339)

=0
Xj

Ei

δV (y) ln p(yi|

X, V ) = 2Re

ϕ0(yi)−
(cid:16)

(cid:17)

1δV (y)ϕ0(yi)

.

(340)

The MAP equations for inverse quantum mechanics are obtained by including
the functional derivatives of a prior terms for V . In particular, for a Gaussian
prior with mean V0 and inverse covariance KV , acting in the space of potential
functions V (y), its negative logarithm, i.e., its prior error functional, reads

1
2

(V

−

V0, KV (V

V0)) + ln ZV ,

−

with ZV being the V –independent constant normalizing the prior over V .
Collecting likelihood and prior terms, the stationarity equation ﬁnally be-
comes

(341)

(342)

0 =

δV (y) ln p(yi|

Xi

X, V ) + KV (V

V0).

−

80

6
The Bayesian approach to inverse quantum problems is quite ﬂexible and
can be used for many diﬀerent learning scenarios and quantum systems. By
adapting Eq. (340), it can deal with measurements of diﬀerent observables,
for example, coordinates, momenta, energies, and with other density oper-
ators, describing, for example, time–dependent states or systems at ﬁnite
temperature [134].

The treatment of bound state or scattering problems for quantum many–
body systems requires additional approximations. Common are, for example,
mean ﬁeld methods, for bound state problems [50, 183, 24] as well as for
scattering theory [73, 24, 131, 132, 121, 122, 207] Referring to such mean
ﬁeld methods inverse quantum problems can also be treated for many–body
systems [133].

4 Parameterizing likelihoods: Variational

methods

4.1 General parameterizations

Approximate solutions of the error minimization problem are obtained by
restricting the search (trial) space for h(x, y) = φ(x, y) (or h(x) in regression).
Functions φ which are in the considered search space are called trial functions.
Solving a minimization problem in some restricted trial space is also called a
variational approach [90, 98, 26, 32, 24]. Clearly, minimal values obtained by
minimization within a trial space can only be larger or equal than the true
minimal value, and from two variational approximations that with smaller
error is the better one.

Alternatively, using parameterized functions φ can also implement the
prior where φ is known to have that speciﬁc parameterized form. (In cases
where φ is only known to be approximately of a speciﬁc parameterized form,
this should ideally be implemented using a prior with a parametrized tem-
plate and the parameters be treated as hyperparameters as in Section 5.)
The following discussion holds for both interpretations.

Any parameterization φ = φ(

) together with a range of allowed values
for the parameter vector ξ deﬁnes a possible trial space. Hence we consider
the error functional

ξl}

{

Eφ(ξ) =

( ln P (ξ), N ) +

( φ(ξ), K φ(ξ) ) + ( P (ξ), ΛX ),

(343)

−

1
2

81

for φ depending on parameters ξ and p(ξ) = p( φ(ξ) ). In the special case of
Gaussian regression this reads

Eh(ξ) =

( h(ξ)

tD, KD h(ξ)

tD ) +

( h(ξ), K h(ξ) ).

(344)

−

−

1
2

1
2

Deﬁning the matrix

Φ′(l; x, y) =

∂φ(x, y)
∂ξl

the stationarity equation for the functional (343) becomes

0 = Φ′P′P−

1N

Φ′Kφ

Φ′P′ΛX.

−
Similarly, a parameterized functional Eφ with non–zero template t as in (226)
would give

−

0 = Φ′P′P−

1N

Φ′K (φ

−

t)

−

−

Φ′P′ΛX.

To have a convenient notation when solving for ΛX we introduce

P′ξ = Φ′(ξ)P′(φ),

P′ξ(l; x, y) =

∂P (x, y)
∂ξl

=

dx′dy′

Z

∂φ(x′, y′)
∂ξl

δP (x, y)
δφ(x′, y′)

,

i.e.,

and

Gφ(ξ) = P′ξP−

1N

Φ′Kφ,

−

to obtain for Eq. (346)

P′ξΛX = Gφ(ξ).
For a parameterization ξ restricting the space of possible P the matrix P′ξ is
not square and cannot be inverted. Thus, let (P′ξ)# be the Moore–Penrose
inverse of P′ξ, i.e.,

(351)

(P′ξ)#P′ξ(P′ξ)# = P′ξ, P′ξ(P′ξ)#P′ξ = (P′ξ)#,

(352)

and symmetric (P′ξ)#P′ξ and P′ξ(P′ξ)#. A solution for ΛX exists if

P′ξ(P′ξ)#Gφ(ξ) = Gφ(ξ).

In that case the solution can be written

ΛX = (P′ξ)#Gφ(ξ) + VΛ −

(P′ξ)#P′ξVΛ,

82

(345)

(346)

(347)

(348)

(349)

(350)

(353)

(354)

(355)

(356)

(359)

(360)

(361)

with arbitrary vector VΛ and

from the right null space of P′ξ, representing a solution of

Λ0
X = VΛ −

(P′ξ)#P′ξVΛ

P′ξΛ0

X = 0.

Inserting for ΛX(x)
IXPΛX gives

= 0 Eq. (354) into the normalization condition ΛX =

ΛX = IXP

(P′ξ)#Gφ(ξ) + VΛ −
Substituting back in Eq. (346) ΛX is eliminated yielding as stationarity equa-
tion

(P′ξ)#P′ξVΛ

(357)

(cid:17)

(cid:16)

.

0 =

I

P′ξIXP(P′ξ)#

−

(cid:16)

Gφ(ξ) −

P′ξIXP

VΛ −

(cid:16)

(cid:17)

(P′ξ)#P′ξVΛ

,

(358)

(cid:17)

where Gφ(ξ) has to fulﬁll Eq. (353). Eq. (358) may be written in a form
similar to Eq. (193)

with

but with

Kφ(ξ)(ξ) = Tφ(ξ)

Tφ(ξ)(ξ) = P′ξP−

1N

P′ξΛX,

−

Kφ(ξ)(ξ) = Φ′KΦ(ξ),

being in general a nonlinear operator.

4.2 Gaussian priors for parameters

Up to now we assumed the prior to be given for a function φ(ξ)(x, y) de-
pending on x and y. Instead of a prior in a function φ(ξ)(x, y) also a prior in
another not (x, y)–dependent function of the parameters ψ(ξ) can be given.
A Gaussian prior in ψ(ξ) = Wψξ being a linear function of ξ, results in a
prior which is also Gaussian in the parameters ξ, giving a regularization term

1
2

( ξ, W T

ψ KψWψ ξ ) =

( ξ, Kξ ξ ),

(362)

1
2

83

6
where Kξ = W T
ψ KψWψ is not an operator in a space of functions φ(x, y) but
a matrix in the space of parameters ξ. The results of Section 4.1 apply to
this case provided the following replacement is made

Similarly, a nonlinear ψ requires the replacement

where

Φ′Kφ

Kξξ.

→

Φ′Kφ

Ψ′Kψψ,

→

Ψ′(k, l) =

∂ψl(ξ)
∂ξk

.

Thus, in the general case where a Gaussian (speciﬁc) prior in φ(ξ) and ψ(ξ)
is given,

Eφ(ξ),ψ(ξ) =

−
+

( ln P (ξ), N ) + ( P (ξ), ΛX )
1
2

( φ(ξ), K φ(ξ) ) +

1
2

( ψ(ξ), Kψ ψ(ξ) ),

(366)

or, including also non–zero template functions (means) t, tψ for φ and ψ as
discussed in Section 3.5,

Eφ(ξ),ψ(ξ) =

−
+

+

( φ(ξ)

( ln P (ξ), N ) + ( P (ξ), ΛX )
1
2
1
2

tψ, Kψ (ψ(ξ)

t, K (φ(ξ)

( ψ(ξ)

t) )

−

−

−

−

tψ) ).

(367)

The φ and ψ–terms of the energy can be interpreted as corresponding to
t, K, tψ, Kψ), (
a probability p(ξ
tψ, Kψ)), or, for example,
|
t, K) with one of the two terms term corresponding to a
ξ, Kψ) p(ξ
to p(tψ|
|
Gaussian likelihood with ξ–independent normalization.

t, K) p(ξ

= p(ξ

|

|

Φ′K(φ

t)

−

−

Ψ′Kψ(ψ

tψ)

P′ξΛX

−

−

The stationarity equation becomes

1N
0 = P′ξP−
= Gφ,ψ −

−
P′ξΛX,
which deﬁnes Gφ,ψ, and for ΛX 6
ΛX = IXP

= 0

for P′ξΛ0

X = 0.

(P′ξ)#Gφ,ψ + Λ0
X
(cid:16)

(cid:17)

,

84

(363)

(364)

(365)

(368)
(369)

(370)

6
Variable

Error

Stationarity equation

ΛX

EL

EP

E√P

Eφ

L(x, y)

P (x, y)

φ = √P

φ(x, y)

ξ

ξ

KL = N

eLΛX

−
1N

KP = P−

Kφ = 2Φ−

1N

Kφ = P′P−

1N

P′ΛX

ΛX

−
2ΦΛX

−

−

−

Eφ(ξ)

Φ′Kφ = P′ξP−

1N

P′ξΛX IXP

Eφ(ξ)ψ(ξ) Φ′K(φ

t) + Ψ′Kψ(ψ

tψ)

IXP

−
= P′ξP−

1N

−
P′ξΛX

−

KL)

−
PKP )

IX (N

IX(N

−

IX(N

1
2ΦKφ)

−
PP′−

1K φ

IX

N
(cid:16)

−

(cid:17)
(P′ξ)#Gφ(ξ) + Λ0
X
(cid:16)

(P′ξ)#Gφ,ψ + Λ0
X
(cid:16)

(cid:17)

(cid:17)

Table 5: Summary of stationarity equations. For notations, conditions and
comments see Sections 3.1.1, 3.2.1, 3.3.2, 3.3.1, 4.1 and 4.2.

4.3 Linear trial spaces

Choosing a ﬁnite linear trial space is also called the Ritz method and is
equivalent to solving a projected stationarity equation. Here

φ =

clBl

(371)

Xl
is expanded in a basis Bl, not necessarily orthonormal, and truncated to
terms with l < lmax. This gives for (187)

1
2

Xkl

ERitz =

( ln P (φ), N ) +

ckcl( Bk, K Bl ) + ( P (φ), ΛX ).

(372)

−

Solving for the coeﬃcients cl, l < lmax to minimize the error results according
to Eq.[346) and

Φ′(l; x, y) = Bl(x, y),

(373)

in

0 = ( Bl, P′P−

1 N )

ck( Bl, K Bk )

( Bl, P′ ΛX ),

lmax,

(374)

−

l
∀

≤

−

Xk

corresponding to the lmax–dimensional equation

KBc = NB(c)

ΛB(c),

(375)

−

85

with

c(l) = cl,

KB(l, k) = ( Bl, K Bk ),
NB(c)(l) = ( Bl, P′(φ(c)) P−
ΛB(c)(l) = ( Bl, P′(φ(c)) ΛX ).

1(φ(c)) N ),

(376)
(377)
(378)
(379)

Thus, for an orthonormal basis Bl Eq. (375) corresponds to Eq. (189) pro-
l Bl.
jected into the trial space by

l BT

The so called linear models are obtained by the (very restrictive) choice

φ(z) =

clBl = c0 +

clzl

(380)

Xl

with z = (x, y) and B0 = 1 and Bl = zl. Interactions, i.e., terms proportional
to products of z–components like cmnzmzn can be included. Including all pos-
sible interaction would correspond to a multidimensional Taylor expansion
of the function φ(z).

If the functions Bl(z) are also parametrized this leads to mixture models

P

1

Xl=0

for φ. (See Section 4.4.)

4.4 Mixture models

The function φ(z) can be approximated by a mixture model, i.e., by a linear
combination of components functions

φ(z) =

clBl(ξl, z),

(381)

X

with parameter vectors ξl and constants cl (which could also be included
into the vector ξl) to be adapted. The functions Bl(ξl, z) are often chosen to
depend on one–dimensional combinations of the vectors ξl and z. For example
ξl −
they may depend on some distance
(‘local or distance approaches’)
||
or the projection of z in ξl–direction, i.e.,
k ξl,kzk (‘projection approaches’).
(For projection approaches see also Sections 4.5, 4.8 and 4.9).

||

z

A typical example are Radial Basis Functions (RBF) using Gaussian
Bl(ξl, z) for which centers (and possibly covariances and also number of com-
ponents) can be adjusted. Other local methods include k–nearest neighbors
methods (kNN) and learning vector quantizations (LVQ) and its variants.
(For a comparison see [146].)

P

86

4.5 Additive models

Trial functions φ may be chosen as sum of simpler functions φl each depending
only on part of the x and y variables. More precisely, we consider functions
φl depending on projections zl = I(z)
l z of the vector z = (x, y) of all x and
y components. I(z)
l denotes an projector in the vector space of z (and not in
the space of functions Φ(x, y)). Hence, φ becomes of the form

φ(z) =

φl(zl),

(382)

Xl
so only one–dimensional functions φl have to be determined. Restricting
the functions φl to a parameterized function space yields a “parameterized
additive model”

φ(z) =

φl(ξ, zl),

(383)

Xl
which has to be solved for the parameters ξ. The model can also be gener-
alized to a model “additive in parameters ξl”

φ(z) =

φl(ξl, x, y),

(384)

Xl

where the functions φl(ξl, x, y) are not restricted to one–dimensional functions
depending only on projections zl on the coordinate axes. If the parameters ξl
determine the component functions φl completely, this yields just the mixture
models of Section 4.4. Another example is projection pursuit, discussed in
Section 4.8), where a parameter vector ξl corresponds to a projections ξl ·
z.
z) has
In that case even for given ξl still a one–dimensional function φl(ξl ·
to be determined.

An ansatz like (382) is made more ﬂexible by including also interactions

.

· · ·

φ(x, y) =

φl(zl) +

φkl(zk, zl) +

φklm(zk, zl, zm) +

(385)

···

Xkl

Xklm

(zk, zl,

Xl
The functions φkl
) can be chosen to depend on product terms like
zl,izk,j, or zl,izk,jzm,n, where zl,i denotes one–dimensional sub-variables of zl.
In additive models in the narrower sense [202, 85, 86, 87] zl is a subset of
, dx denoting
x, y components, i.e., zl ⊆ {
≤
the dimension of x, dy the dimension of y. In regression, for example, one
takes usually the one–element subsets zl =

yj|
for 1

dy}
dx.

xi|

· · ·

≤

≤

≤

1

1

j

i

l

dx} ∪ {
xl}

{

≤

≤

87

X

In more general schemes the projections of z do not have to be restricted
to projections on the coordinates axes. In particular, the projections can be
optimized too. For example, one–dimensional projections I(z)
z with
denotes a scalar product in the space of z variables) are
z, w
used by ridge approximation schemes. They include for regression problems
one–layer (and similarly multilayer) feedforward neural networks (see Section
4.9) projection pursuit regression (see Section 4.8) and hinge functions [28].
For a detailed discussion of the regression case see [71].

l z = w

Y (where

×

∈

·

·

The stationarity equation for Eφ becomes for the ansatz (382)
1N

Kφ

0 = P′lP−

P′lΛX,

−

−

with

Considering a density P being also decomposed into components Pl deter-
mined by the components φl

the derivative (387) becomes

P′l(zl, z′) =

δP (z′)
δφl(zl)

.

P (z) =

Pl(φl(zl)),

Xl

P′l(zl, z′k) =

δPl(z′l)
δφl(zl)

,

so that specifying an additive prior

1
2

Xkl

( φk −

tk, Kkl (φl −

tl) ),

the stationary conditions are coupled equations for the component functions
φl which, because P is diagonal, only contain integrations over zl–variables

0 =

P−

1N

δPl
δφl

Klk(φk −

tk)

−

δPl
δφl

ΛX.

−

Xk

For the parameterized approach (383) one ﬁnds

0 = Φ′lP′lP−

1N

Φ′lKφ

Φ′lP′lΛX,

−

−

with

For the ansatz (384) Φ′l(k, z) would be restricted to a subset of ξk.

Φ′l(k, zl) =

∂φl(zl)
∂ξk

.

88

(386)

(387)

(388)

(389)

(390)

(391)

(392)

(393)

4.6 Product ansatz

A product ansatz has the form

φ(z) =

φl(zl),

Yl

(394)

where zl = I(z)
l z represents projections of the vector z consisting of all x
and y components. The ansatz can be made more ﬂexible by using sum of
products

φ(z) =

φk,l(zl).

(395)

Xk Yl
The restriction of the trial space to product functions corresponds to the
Hartree approximation in physics.
(In a Hartree–Fock approximation the
product functions are antisymmetrized under coordinate exchange.)

For additive K =
=l Il′

l Kl with Kl acting only on φl, i.e., Kl = Kl ⊗
, with Il the projector into the space of functions φl = Ilφl, the

l′

P

quadratic regularization term becomes, assuming Il Il′ = δl,l′,
(cid:16)N

(cid:17)

( φ, K φ ) =

( φl, Kl φl )

( φl′, φl′ ).

(396)

Xl

Yl′
=l

For K =

l Kl with a product structure with respect to φl

N

( φ, K φ ) =

( φl, Kl φl ).

(397)

Yl
In both cases the prior term factorizes into lower dimensional contributions.

4.7 Decision trees

Decision trees [29] implement functions which are piecewise constant on rect-
angular areas parallel to the coordinate axes zl. Such an approach can be
written in tree structure with nodes only performing comparisons of the form
x < a or x > a which allows a very eﬀective hardware implementation. Such
a piecewise constant approach can be written in the form

φ(z) =

cl

Θ(zν(l,k) −

alk)

(398)

Yk
with step function Θ and zν(l,k) indicating the component of z which is com-
pared with the reference value alk. While there are eﬀective constructive

Xl

89

6
6
methods to build trees the use of gradient–based minimization or maximiza-
tion methods would require, for example, to replace the step function by a
sigmoid. In particular, decision trees correspond to neural networks at zero
temperature, where sigmoids become step functions, and which are restricted
to weights vectors in coordinate directions (see Section 4.9).

An overview over diﬀerent variants of decision trees together with a com-
parison with rule–based systems, neural networks (see Section 4.9) techniques
from applied statistics like linear discriminants, projection pursuit (see Sec-
tion 4.8) and local methods like for example k-nearest neighbors methods
(kNN), Radial Basis Functions (RBF), or learning vector quantization (LVQ)
is given in [146].

4.8 Projection pursuit

Projection pursuit models [55, 95, 45] are a generalization of additive models
(382) (and a special case of models (384) additive in parameters) where the
projections of z = (x, y) are also adapted

φ(z) = ξ0 +

φl(ξ0,l + ξl ·

z).

Xl

For such a model one has to determine one–dimensional ‘ridge’ functions φl
together with projections deﬁned by vectors ξl and constants ξ0, ξ0,l. Adap-
tive projections may also be used for product approaches

Similarly, φ may be decomposed into functions depending on distances to
adapted reference points (centers). That gives models of the form

φ(z) =

φl(ξ0,l + ξl ·

z).

Yl

φ(z) =

φl(

ξl −

||

z

),

||

Yl

which require to adapt parameter vectors (centers) ξl and distance functions
φl. For high dimensional spaces the number of centers necessary to cover a
high dimensional space with ﬁxed density grows exponentially. Furthermore,
as the volume of a high dimensional sphere tends to be concentrated near
its surface, the tails become more important in higher dimensions. Thus,
typically, projection methods are better suited for high dimensional spaces
than distance methods [195].

90

(399)

(400)

(401)

4.9 Neural networks

While in projection pursuit–like techniques the one–dimensional ‘ridge’ func-
tions φl are adapted optimally, neural networks use ridge functions of a ﬁxed
sigmoidal form. The resulting lower ﬂexibility following from ﬁxing the ridge
function is then compensated by iterating this parameterization. This leads
to multilayer neural networks.

Multilayer neural networks have been become a popular tool for regres-
sion and classiﬁcation problems [190, 116, 147, 89, 154, 215, 21, 186, 8].
One-layer neural networks, also known as perceptrons, correspond to the
parameterization

φ(z) = σ

wlzl −

b
!

= σ(v),

 

Xl

(402)

with a sigmoidal function σ, parameters ξ = w, projection v =
and zl single components of the variables x, y, i.e., zl = xl for 1
P
and zl = yl for dx + 1
≤
instead of sigmoids see [67].)

b
l wlzl −
dx
l
≤
≤
dx + dy. (For neural networks with Lorentzians

≤

l

Typical choices for the sigmoid are σ(v) = tanh(βv) or σ(v) = 1/(1 +
2βv). The parameter β, often called inverse temperature, controls the
e−
In particular, the sigmoid functions
sharpness of the step of the sigmoid.
become a sharp step in the limit β
, i.e., at zero temperature. In princi-
→ ∞
ple the sigmoidal function σ may depend on further parameters which then
— similar to projection pursuit discussed in Section 4.8 — would also have
to be included in the optimization process. The threshold or bias b can be
treated as weight if an additional input component is included clamped to
the value 1.

A linear combination of perceptrons

φ(x, y) = b +

Wlσ

Xl

 

Xk

wlkzk −

bk

!

,

(403)

has the form of a projection pursuit approach (399) but with ﬁxed φl(v) =
Wlσ(v).

In multi–layer networks the parameterization (402) is cascaded,

zk,i = σ

wkl,izl,i

bk,i)

= σ(vk,i),

(404)

1 −

−

!

mi−1

 

Xl=1

91

with zk,i representing the output of the kth node (neuron) in layer i and

vk,i =

wkl,izl,i

bk,i,

1 −

−

mi−1

Xl=1

(405)

being the input for that node. This yields, skipping the bias terms for sim-
plicity

mn−1

mn−2

m0

φ(z, w) = σ

wln−1,nσ

wln−1ln−2,n







Xln−2

Xln−1

· · ·
(406)


beginning with an input layer with m0 = dx + dy nodes (plus possibly nodes
to implement the bias) zl,0 = zl and going over intermediate layers with mi
mi to a single node output layer zn = φ(x, y).
nodes zl,i, 0 < i < n, 1
Commonly neural nets are used in regression and classiﬁcation to param-

Xl0

≤

≤









−

l

wl1l0,1zl0,0


σ

1 · · ·

,

eterize a function φ(x, y) = h(x) in functionals

E =

(yi −

Xi

h(xi, w))2,

(407)

quadratic in h and without further regularization terms. In that case, regu-
larization has to be assured by using either 1. a neural network architecture
which is restrictive enough, 2. by using early stopping like training procedures
so the full ﬂexibility of the network structure cannot completely develop and
destroy generalization, where in both cases the optimal architecture or al-
gorithm can be determined for example by cross–validation or bootstrap
techniques [153, 5, 214, 200, 201, 76, 35, 212, 49], or 3. by averaging over
ensembles of networks [157]. In all these cases regularization is implicit in
the parameterization of the network. Alternatively, explicit regularization or
prior terms can be added to the functional. For regression or classiﬁcation
this is for example done in learning by hints [2, 3, 4] or curvature–driven
smoothing with feedforward networks [19].

One may also remark that from a Frequentist point of view the quadratic
i(yi −
functional is not interpreted as posterior but as squared–error loss
a(xi, w))2 for actions a(x) = a(x, w). According to Section 2.2.2 minimization
(xi, yi)
i
sampled under the true
of error functional (407) for data
≤
density p(x, y
f ) yields therefore an empirical estimate for the regression
function

|
dy y p(y

x, f ).

n
}

≤

P

{

1

|

|

R

92

We consider here neural nets as parameterizations for density estimation
with prior (and normalization) terms explicitly included in the functional Eφ.
In particular, the stationarity equation for functional (343) becomes

0 = Φ′wP′P−

1N

Φ′wKφ

Φ′wP′ΛX,

−

−

with matrix of derivatives

Φ′w(k, l, i; x, y) =

∂φ(x, y, w)
∂wkl,i

(408)

(409)

= σ′(vn)

wln−1,nσ′(vln−1,n

1)

wln−1ln−2,n

−

Xln−1
wli+2li+1,i+2σ′(vli+1,i+1)wli+1k,i+1σ′(vli,i)zl,i
−

Xln−2

1,

1

−

· · ·

Xli+1

and σ′(v) = dσ(v)/dv. While φ(x, y, w) is calculated by forward propagating
z = (x, y) through the net deﬁned by weight vector w according to Eq. (406)
the derivatives Φ′ can eﬃciently be calculated by back–propagation according
to Eq. (409). Notice that even for diagonal P′ the derivatives are not needed
only at data points but the prior and normalization term require derivatives
at all x, y. Thus, in practice terms like Φ′Kφ have to be calculated in a
relatively poor discretization. Notice, however, that regularization is here
not only due to the prior term but follows also from the restrictions implicit
in a chosen neural network architecture. In many practical cases a relatively
poor discretization of the prior term may thus be suﬃcient.

Table 6 summarizes the discussed approaches.

5 Parameterizing priors: Hyperparameters

5.1 Prior normalization

In Chapter 4. parameterization of φ have been studied. This section now
D0). For Gaussian prior
discusses parameterizations of the prior density p(φ
densities that means parameterization of mean and/or covariance. The pa-
rameters of the prior functional, which we will denote by θ, are in a Bayesian
context also known as hyperparameters. Hyperparameters θ can be consid-
ered as part of the hidden variables.

|

In a full Bayesian approach the h–integral therefore has to be completed
by an integral over the additional hidden variables θ. Analogously, the prior

93

Ansatz

Functional form

to be optimized

P

+

P

+
P

linear ansatz

φ(z) =

l ξlBl(z)

linear model
with interaction

φ(z) = ξ0 +

l ξlzl

mixture model

φ(z) =

mn ξmnzmzn +
P

· · ·

P
ξ0,lBl(ξl, z)

ξl

ξ0, ξl
ξmn,

· · ·

ξ0,l, ξl

additive model
with interaction

φ(z) =

l φl(zl)

product ansatz

φ(z) =

P
l φl(zl)

mn φmn(zmzn) +

· · ·

φl(zl)
φmn(zmzn),

· · ·

φl(zl)

decision trees

φ(z) =

Q

l ξl

projection pursuit φ(z) = ξ0 +

P

Q

k Θ(zξlk −
l φl(ξ0,l +

ξ0,lk)

ξl, ξ0,lk, ξlk

l ξlzl) φl, ξ0, ξ0,l, ξl

neural net (2 lay.) φ(z) = σ(

P
l ξl σ(

P
k ξlkzk))

ξl, ξlk

P

P

Table 6: Some possible parameterizations.

94

densities can be supplemented by priors for θ, also be called hyperpriors, with
corresponding energies Eθ.

In saddle point approximation thus an additional stationarity equation
will appear, resulting from the derivative with respect to θ. The saddle point
approximation of the θ–integration (in the case of uniform hyperprior p(θ)
and with the h–integral being calculated exactly or by approximation) is also
known as ML–II prior [14] or evidence framework [79, 80, 197, 138, 139, 140,
21].

|

There are some cases where it is convenient to let the likelihood p(y
x, h)
depend, besides on a function φ, on a few additional parameters. In regres-
sion such a parameter can be the variance of the likelihood. Another example
is the inverse temperature β introduced in Section 6.3, which, like φ also ap-
pears in the prior. Such parameters may formally be added to the “direct”
hidden variables φ yielding an enlarged ˜φ. As those “additional likelihood pa-
rameters” are like other hyperparameters typically just real numbers, and not
functions like φ, they can often be treated analogously to hyperparameters.
For example, they may also be determined by cross–validation (see below) or
by a low dimensional integration. In contrast to pure prior parameters, how-
ever, the functional derivatives with respect to such “additional likelihood
parameters” contain terms arising from the derivative of the likelihood.

Within the Frequentist interpretation of error minimization as empirical
risk minimization hyperparameters θ can be determined by minimizing the
empirical generalization error on a new set of test or validation data DT being
independent from the training data D. Here the empirical generalization
error is meant to be the pure data term ED(θ) = ED(φ∗(θ)) of the error
functional for φ∗ being the optimal φ for the full regularized Eφ(θ) at θ and
for given training data D. Elaborated techniques include cross–validation
and bootstrap methods which have been mentioned in Sections 2.5 and 4.9.
Within the Bayesian interpretation of error minimization as posterior
maximization the introduction of hyperparameters leads to a new diﬃculty.
The problem arises from the fact that it is usually desirable to interpret the
error term Eθ as prior energy for θ, meaning that

with normalization

p(θ) =

Eθ

e−
Zθ

,

Zθ =

dθ e−

Eθ,

Z

95

(410)

(411)

(412)

(413)

(414)

(415)

(416)

(417)

represents the prior density for θ. Because the joint prior factor for φ and θ
is given by the product

one ﬁnds

Hence, the φ–dependent part of the energy represents a conditional prior
energy denoted here E(φ

θ). As this conditional normalization

p(φ, θ) = p(φ

θ)p(θ),

|

p(φ

θ) =

|

θ)

E(φ

e−
|
Zφ(θ)

.

|

Zφ(θ) =

dφ e−

E(φ

θ),

|

Z

EN (θ) = ln Zφ(θ)

is in general θ–dependent a normalization term

must therefore be included in the error functional when minimizing with
respect to θ.

It is interesting to look what happens if p(φ, θ) of Eq. (410) is expressed

in terms of joint energy E(φ, θ) as follows

Then the joint normalization

p(φ, θ) =

E(φ,θ)

e−

.

Zφ,θ

Zφ,θ =

dφ dθ e−

E(φ,θ),

Z

is independent of φ and θ and could be skipped from the functional. However,
in that case the term Eθ cannot easily be related to the prior p(θ).

Notice especially, that this discussion also applies to the case where Eθ
is assumed to be uniform so it does not have to appear explicitly in the
error functional. The two ways of expressing p(φ, θ) by a joint or conditional
In that
energy, respectively, are equivalent if the joint density factorizes.
case, however, θ and φ are independent, so θ cannot be used to parameterize
the density of φ.

Numerically the need to calculate Zφ(θ) can be disastrous because nor-
malization factors Zφ(θ) represent often an extremely high dimensional (func-
tional) integral and are, in contrast to the normalization of P over y, very
diﬃcult to calculate.

96

There are, however, situations for which Zφ(θ) remains θ–independent.
˜D0) (with
Let p(φ, θ) stand for example for a Gaussian speciﬁc prior p(φ, θ
the normalization condition factored out as in Eq. (91)). Then, because the
normalization of a Gaussian is independent of its mean, parameterizing the
mean t = t(θ) results in a θ–independent Zφ(θ).

|

Besides their mean, Gaussian processes are characterized by their covari-
1. Because the normalization only depends on det K a
ance operators K−
second possibility yielding θ–dependent Zφ(θ) are parameterized transfor-
mations of the form K
Indeed,
such transformations do not change the determinant det K. They are only
non–trivial for multi–dimensional Gaussians.

1 with orthogonal O = O(θ).

OKO−

→

For general parameterizations of density estimation problems, however,
the normalization term ln Zφ(θ) must be included. The only way to get rid
of that normalization term would be to assume a compensating hyperprior

resulting in an error term E(θ) =

Thus, in the general case we have to consider the functional

p(θ)

Zφ(θ),

∝
ln Zφ(θ) compensating EN (θ).

−

(418)

Eθ,φ =

(ln P (φ), N) + (P (φ), ΛX) + Eφ(θ) + Eθ + ln Zφ(θ).

−
θ) = Eφ and E(θ) = Eθ. The stationarity conditions have the

(419)

writing E(φ
form

|

δEφ
δφ
∂Eφ
∂θ

= P′(φ)P−

1(φ)N

P′(φ)ΛX,

−

=

Z′Z −

1
φ (θ)

−

E′θ,

−

(420)

(421)

(422)

with

Z′(l, k) = δ(l

k)

, E′θ(l) =

∂Zφ(θ)
dθl

−

∂Eθ
∂θl

.

For compensating hyperprior Eθ =
(421) vanishes.

−

ln Zφ(θ) the right hand side of Eq.

Finally, we want to remark that in case function evaluation of p(φ, θ)
is much cheaper than calculating the gradient (421), minimization methods
not using the gradient should be considered, like for example the downhill
simplex method [181].

97

5.2 Adapting prior means

5.2.1 General considerations

A prior mean or template function t represents a prototype, reference func-
tion or base line for φ. It may be a typical expected pattern in time series
prediction or a reference image in image reconstruction. Consider, for ex-
ample, the task of completing an image φ given some pixel values (training
data). Expecting the image to be that of a face the template function t may
be chosen to be some prototypical image of a face. We have seen in Section
3.5 that a single template t could be eliminated for Gaussian (speciﬁc) priors
t instead for φ. Restricting, however, to only a single
by solving for φ
−
template may be a very bad choice.
Indeed, faces for example appear on
images in many variations, like in diﬀerent scales, translated, rotated, var-
ious illuminations, and other kinds of deformations. We may now describe
such variations by a family of templates t(θ), the parameter θ describing
scaling, translations, rotations, and more general deformations. Thus, we
expect a function to be similar to only one of the templates t(θ) and want to
implement a (soft, probabilistic) OR, approximating t(θ1) OR t(θ2) OR
.
· · ·
A (soft, probabilistic) AND of approximation conditions, on the other
hand, is implemented by adding error terms. For example, classical error
functionals where data and prior terms are added correspond to an approxi-
mation of training data AND a priori data.

Similar considerations apply for model selection. We could for example
expect φ to be well approximated by a neural network or a decision tree. In
that case t(θ) spans for example a space of neural networks or decision trees.
Finally, let us emphasize again that the great advantage and practical feasi-
bility of adaptive templates for regression problems comes from the fact that
no additional normalization terms have to be added to the error functional.

5.2.2 Density estimation

The general case with adaptive means for Gaussian prior factors and hyper-
parameter energy Eθ yields an error functional

Eθ,φ =

(ln P (φ), N) +

t(θ), K (φ

t(θ))

+ (P (φ), ΛX) + Eθ. (423)

−

Deﬁning

1
2

φ
(cid:16)

−

t′(l; x, y) =

(424)

−

(cid:17)

∂t(x, y; θ)
∂θl

,

98

the stationarity equations of (423) obtained from the functional derivatives
with respect to φ and hyperparameters θ become

K(φ
t′K(φ

t) = P′(φ)P−
t) =

E′θ.

−

−
−

1(φ)N

P′(φ)ΛX,

−

Inserting Eq. (425) in Eq. (426) gives

t′P′(φ)P−

1(φ)N = t′P′(φ)ΛX −

E′θ.

(425)
(426)

(427)

If working with parameterized φ(ξ) extra prior terms Gaussian in some func-
tion ψ(ξ) can be included as discussed in Section 4.2. Then, analogously to
templates t for φ, also parameter templates tψ can be made adaptive with
hyperparameters θψ. Furthermore, prior terms Eθ and Eθψ for the hyperpa-
rameters θ, θψ can be added. Including such additional error terms yields

Eθ,θψ,φ(ξ),ψ(ξ) =

−
+

+

φ(ξ)

(ln P ( φ(ξ) ), N) + (P ( φ(ξ) ), ΛX)
1
2
1
2

tψ(θψ), Kψ (ψ(ξ)

t(θ), K (φ(ξ)

t(θ))

ψ(ξ)

−

−

−

(cid:16)

(cid:17)
tψ(θψ))

−
+Eθ + Eθψ ,

(cid:16)

(cid:17)

(428)

and Eqs.(425) and (425) change to

Φ′K(φ

t) + Ψ′Kψ(ψ

−

−
t′K(φ
t′ψKψ(ψ

−

tψ) = P′ξP−
E′θ,
t) =
−
E′θψ ,
tψ) =

−
−

1N

−

P′ξΛX,

(429)
(430)
(431)

where t′ψ, E′θψ , E′θ , denote derivatives with respect to the parameters θψ
or θ, respectively. Parameterizing Eθ and Eθψ the process of introducing
hyperparameters can be iterated.

5.2.3 Unrestricted variation

To get a ﬁrst understanding of the approach (423) let us consider the extreme
example of completely unrestricted t–variations. In that case the template
function t(x, y) itself represents the hyperparameter. (Such function hyper-
parameters or hyperﬁelds are also discussed in Sect. 5.6.) Then, t′ = I and

99

Eq. (426) gives K(φ
−
t = φ), resulting according to Eq. (229) in

t) = 0 (which for invertible K is solved uniquely by

ΛX = NX.

PD = Λ−

1
X,DN,

P (xi, yi) =

N(xi, yi)
NX(xi)

The case of a completely free prior mean t is therefore equivalent to a situation
without prior. Indeed, for invertible P′, projection of Eq. (427) into the x–
data space by ID of Eq. (258) yields

where ΛX,D = IDΛXID is invertible and PD = IDP . Thus for xi for which
yi are available

is concentrated on the data points. Comparing this with solutions of Eq.
(192) for ﬁxed t we see that adaptive means tend to lower the inﬂuence of
prior terms.

5.2.4 Regression

Consider now the case of regression according to functional (247) with an
adaptive template t0(θ). The system of stationarity equations for the regres-
sion function h(x) (corresponding to φ(x, y)) and θ becomes

−
−
It will also be useful to insert Eq. (435) in Eq. (436), yielding

t0) = KD(tD −
t0) = 0.

K0(h
t′0K0(h

h),

0 = t′0KD(h

tD).

−

For ﬁxed t Eq. (435) is solved by the template average t

h = t = (K0 + KD)−

1 (K0t0 + KDtD) ,

so that Eq. (436) or Eq. (437), respectively, become

(432)

(433)

(434)

(435)
(436)

(437)

(438)

(439)

(440)

0 = t′0K0(t

t0),

0 = t′0KD(t

tD).

−

−

100

(441)

(442)

(443)

→ ∞
(444)

It is now interesting to note that if we replace in Eq. (440) the full template
average t by t0 we get

which is equivalent to the stationarity equation

0 = t′0KD(t0 −

tD),

0 = H′KD(h

tD),

−

(the derivative matrix H′ being the analogue to Φ′ for h) of an error functional

ED,h(ξ) =

( h(ξ)

tD, KD(h(ξ)

tD) )

−

−

1
2

without prior terms but with parameterized h(ξ), e.g., a neural network. The
approximation h = t = t0 can, for example, be interpreted as limit λ
,

lim
λ
→∞

h = lim
λ
→∞

t = t0,

after replacing K0 by λK0 in Eq. (438). The setting h = t0 can then be
1
used as initial guess h0 for an iterative solution for h. For existing K−
0 h
= t0 is also obtained after one iteration step of the iteration scheme hi =
t0 + K−

1) starting with initial guess h0 = tD.
For comparison with Eqs.(440,441,442) we give the stationarity equations
for parameters ξ for a parameterized regression functional including an ad-
ditional prior term with hyperparameters

0 KD(tD −

hi
−

1

−

t0(θ)) ),
(445)

(446)

Eθ,h(ξ) =

( h(ξ)

tD, KD(h(ξ)

tD) )+

( h(ξ)

t0(θ), K0(θ)(h(ξ)

−

−

−

1
2

1
2

which are

0 = H′KD(h

tD) + h′K0(h

t0).

−

−

Let us now compare the various regression functionals we have met up to
now. The non–parameterized and regularized regression functional Eh (247)
implements prior information explicitly by a regularization term.

A parameterized and regularized functional Eh(ξ) of the form (344) cor-
responds to a functional of the form (445) for θ ﬁxed. It imposes restrictions
on the regression function h in two ways, by chosing a speciﬁc parameteriza-
tion and by including an explicit prior term. If the number of data is large
enough, compared to the ﬂexibility of the parameterization, the data term
of Eh(ξ) alone can have a unique minimum. Then, at least technically, no

101

additional prior term would be required. This corresponds to the classical
error minimization methods used typically for parametric approaches. Nev-
ertheless, also in such situations the explicit prior term can be useful if it
implements useful prior knowledge over h.

The regularized functional with prior– or hyperparameters Eθ,h (423) im-
plements, compared to Eh, eﬀectively weaker prior restrictions. The prior
term corresponds to a soft restriction of h to the space spanned by the pa-
In the limit where the parameterization of t(θ) is rich
rameterized t(θ).
enough to allow t(θ∗) = h∗ at the stationary point the prior term vanishes
completely.

The parameterized and regularized functional Eθ,h(ξ) (445),

including
prior parameters θ, implements prior information explicitly by a regular-
ization term and implicitly by the parameterization of h(ξ). The explicit
prior term vanishes if t(θ∗) = h(ξ∗) at the stationary point. The func-
tional combines a hard restriction of h with respect to the space spanned
by the parameterization h(ξ) and a soft restriction of h with respect to the
space spanned by the parameterized t(θ). Finally, the parameterized and
non–regularized functional ED,h(ξ) (443) implements prior information only
implicitly by parameterizing h(ξ). In contrast to the functionals Eθ,h and
Eθ,h(ξ) it implements only a hard restriction for h. The following table sum-
marizes the discussion:

Functional
Eh
Eh(ξ)
Eθ,h

Eq.
(247)
(344)
(423)

Eθ,h(ξ)

(445)

ED,h(ξ)

(443)

prior implemented
explicitly
explicitly and implicitly
explicitly
no prior for t(θ∗) = h∗
explicitly and implicitly
no expl. prior for t(θ∗) = h(ξ∗)
implicitly

5.3 Adapting prior covariances

5.3.1 General case

It includes
Parameterizing covariances K−
for example adapting the trade–oﬀ between data and prior terms (i.e., the
determination of the regularization factor), the selection between diﬀerent
symmetries, smoothness measures, or in the multidimensional situation the

1 is often desirable in practice.

102

determination of directions with low variance. As far as the normalization
depends on K(θ) one has to consider the error functional

Eθ,φ =

(ln P (φ), N) +

t, K(θ) (φ

t)

−

+ (P (φ), ΛX) + ln Zφ(θ) + Eθ,
(447)

(cid:17)

1
2

φ
(cid:16)

−

−

with

Zφ(θ) = (2π)

2 (det K(θ))−

d

1

2 ,

(448)

for a d–dimensional Gaussian speciﬁc prior, and stationarity equations

1
2

φ
(cid:16)

t,

−

∂K(θ)
∂θ

(φ

−

−
t)

(cid:17)

K(φ

t) = P′(φ)P−

1(φ)N

P′(φ)ΛX,

(449)

−
∂K(θ)

= Tr

K−

1(θ)

 

∂θ ! −

E′θ.

(450)

Here we used

∂
∂θ

∂
∂θ

1 ∂K

.

∂θ !

 

ln det K =

Tr ln K = Tr

K−

(451)

In case of an unrestricted variation of the matrix elements of K the hyper-
parameters become θl = θ(x, y; x′, y′) = K(x, y; x′, y′). Then, using

∂K(x, y; x′, y′)
∂θ(x′′, y′′; x′′′, y′′′)

= δ(x

x′′)δ(y

y′′)δ(x′

x′′′)δ(y′

y′′′),

(452)

−

−

−

−

Eqs.(450) becomes the inhomogeneous equation

1
2

(φ

t) (φ

t)T = Tr

K−

1(θ)

−

−

 

∂K(θ)

∂θ ! −

E′θ.

(453)

We will in the sequel consider the two special cases where the determinant
of the covariance is θ–independent so that the trace term vanishes, and where
θ is just a multiplicative factor for the speciﬁc prior energy, i.e., a so called
regularization parameter.

5.3.2 Automatic relevance detection

A useful application of hyperparameters is the identiﬁcation of sensible di-
rections within the space of x and y variables. Consider the general case
i θiKi. Treating the
of a covariance, decomposed into components K0 =

103

P

coeﬃcient vector θ (with components θi) as hyperparameter with hyperprior
p(θ) results in a prior energy (error) functional

1
2

(φ

t, (

−

−

Xi

θiKi)(φ

t) )

ln p(θ) + ln Zφ(θ).

(454)

−

−

The θ–dependent normalization ln Zφ(θ) has to be included to obtain the
correct stationarity condition for θ. The components Ki can be the compo-
∂2
nents of a negative Laplacian, for example, Ki =
yi. In that
case adapting the hyperparameters means searching for sensible directions in
the space of x or y variables. This technique has been called Automatic Rel-
evance Determination by MacKay and Neal [157]. The positivity constraint
for a can be implemented explicitly, for example by using K0 =
i Ki or
K0 =

∂2
xi or Ki =

i exp(θi)Ki.

i θ2

−

−

P

P

5.3.3 Local smoothness adaption

Similarly, the regularization factor of a smoothness related covariance op-
erator may be adapted locally. Consider, for example, a prior energy for
φ(x, y)

E(φ

θ) =

(φ

t, K(a, b)(φ

t) ),

|

−

1
2

(455)

with a Laplacian prior

mx

−

Xi

K(x, x′, y, y′; θ) =

eθx,i(x) δ(xi −

x′i) ∂2

xi −

eθy,i(y) δ(y

y′i) ∂2

yi, (456)

−

for mx–dimensional vector x and my–dimensional vector y depending on func-
tions θx,i(x) and θy,i(y) (or more general θx,i(x, y) and θy,i(x, y)) collectively
denoted by θ. Expressing the coeﬃcient functions as exponentials exp(θx,i),
exp(θy,i) is one possibility to enforce their positivity. Typically, one might
impose a smoothness hyperprior on the functions θx,i(x) and θy,i(y), for ex-
ample by using an energy functional

−

my

Xi

1
2

mx

Xi

E(φ, θ) +

(θx,i, Kθ,xθx,i) +

(θy,i, Kθ,yθy,i) + ln Zφ(θ),

(457)

with smoothness related Kθ,x, Kθ,y. The stationarity equation for a functions
θx,i(x) reads

0 = (Kθ,xθx,i)(x)

(φ(x, y)

t(x, y))

−

−

+∂θx,i(x) ln Zφ(θ).

∂2
xi (φ(x, y)
(cid:16)

−

t(x, y))

eθx,i(x)

(cid:17)

(458)

1
2

my

Xi

104

The functions θx,i(x) and θy,i(y) are examples of function hyperparameters
(see Sect. 5.6).

5.3.4 Local masses and gauge theories

The Bayesian analog of a mass term in quantum ﬁeld theory is a term propor-
tional to the identity matrix I in the inverse prior covariance K0. Consider,
for example,

K0 = θ2 I

∆,

−

(459)

≥

with θ real (so that θ2
0) representing a mass parameter. For large masses
φ tends to copy the template t locally, and longer range eﬀects of data points
following from smoothness requirements become less important. Similarly
to Sect. 5.3.3 a constant mass can be replaced by a mass function θ(x).
This allows to adapt locally that interplay between “template copying” and
smoothness related inﬂuence of training data. As hyperprior, one may use a
smoothness constraint on the mass function θ(x), e.g.,

1
2

(φ

−

t, M2(φ

t))

−

−

1
2

(φ

t, ∆(φ

t)) + λ (θ, Kθθ) + ln Zφ(θ),

(460)

−

−

where M denotes the diagonal mass operator with diagonal elements θ(x).

Functional hyperparameters like θ(x) represent, in the language of physi-
cists, additional ﬁelds entering the problem (see also Sect. 5.6). There are
In particular, a gauge
similarities for example to gauge ﬁelds in physics.
i θi(x),
theory–like formalism can be constructed by decomposing θ(x) =
so that the inverse covariance

P

K0 =

M2

i −

∂2
i

=

(cid:17)

Xi

Xi (cid:16)

(Mi + ∂i) (Mi −

∂i) =

D†i Di,

(461)

Xi

can be expressed in terms of a “covariant derivative” Di = ∂i + θi. Next, one
may choose as hyperprior for θi(x)

mx

Xi

1
2 


(θi,

∆ θi)

−

(

−

mx

Xi

mx

Xj

1
4

mx

Xij

F 2
ij





∂xiθi,

∂xj θj)

=

(462)

which can be expressed in terms of a “ﬁeld strength tensor” (for Abelian
ﬁelds),

(463)

Fij = ∂iθj −

∂jθi,

105

like, for example, the Maxwell tensor in quantum electrodynamics. (To relate
eiαφ one can
this, as in electrodynamics, to a local U(1) gauge symmetry φ
consider complex functions φ, with the restriction that their phase cannot
be measured.) Notice, that, due to the interpretation of the prior as product
p(φ
θ)p(θ), an additional θ–dependent normalization term ln Zφ(θ) enters the
energy functional. Such a term is not present in quantum ﬁeld theory, where
one relates the prior functional directly to p(φ, θ), so the norm is independent
of φ and θ.

→

|

5.3.5 Invariant determinants

In this section we discuss parameterizations of the covariance of a Gaussian
In that case no θ–
speciﬁc prior which leave the determinant invariant.
dependent normalization factors have to be included which are usually very
diﬃcult to calculate. We have to keep in mind, however, that in general a
large freedom for K(θ) eﬀectively diminishes the inﬂuence of the parameter-
ized prior term.

A determinant is, for example, invariant under general similarity trans-
formations, i.e., det ˜K = det K for K
1 where O could be
any element of the general linear group. Similarity transformations do not
1Oψ = λOψ.
change the eigenvalues, because from Kψ = λψ follows OKO−
Thus, if K is positive deﬁnite also ˜K is. The additional constraint that ˜K
has to be real symmetric,

˜K = OKO−

→

requires O to be real and orthogonal

˜K = ˜KT = ˜K†,

O−

1 = OT = O†.

(464)

(465)

Furthermore, as an overall factor of O does not change ˜K one can restrict O
to a special orthogonal group SO(N) with det O = 1. If K has degenerate
eigenvalues there exist orthogonal transformations with K = ˜K.

While in one dimension only the identity remains as transformation, the
condition of an invariant determinant becomes less restrictive in higher di-
mensions. Thus, especially for large dimension d of K (inﬁnite for continuous
x) there is a great freedom to adapt covariances without the need to calcu-
late normalization factors, for example to adapt the sensible directions of a
multivariate Gaussian.

106

A positive deﬁnite K can be diagonalized by an orthogonal matrix O
with det O = 1, i.e., K = ODOT . Parameterizing O the speciﬁc prior term
becomes
1
2

1
φ
2
(cid:16)
so the stationarity Eq. (450) reads

t, O(θ)DOT (θ) (φ

t, K(θ) (φ

(466)

,
(cid:17)

−

−

−

−

=

t)

t)

φ

(cid:16)

(cid:17)

∂O
∂θ

φ

t,

−

(cid:16)

DOT (φ

t)

=

−

(cid:17)

E′θ.

−

(467)

Matrices O from SO(N) include rotations and inversion. For a Gaussian
speciﬁc prior with nondegenerate eigenvalues Eq. (467) allows therefore to
adapt the ‘sensible’ directions of the Gaussian.

There are also transformations which can change eigenvalues, but leave
eigenvectors invariant. As example, consider a diagonal matrix D with di-
i λi. Clearly, any
agonal elements (and eigenvalues) λi 6
permutation of the eigenvalues λi leaves the determinant invariant and trans-
forms a positive deﬁnite matrix into a positive deﬁnite matrix. Furthermore,
one may introduce continuous parameters θij > 0 with i < j and transform
D

˜D according to

= 0, i.e., det D =

Q

→

˜λi = λiθij,

λi →

˜λj =

λj →

λj
θij

,

which leaves the product λiλj = ˜λi˜λj and therefore also the determinant
invariant and transforms a positive deﬁnite matrix into a positive deﬁnite
matrix. This can be done with every pair of eigenvalues deﬁning a set of
continuous parameters θij with i < j (θij can be completed to a symmetric
matrix) leading to

(468)

(469)

˜λi = λi

λi →

j>i θij
j<i θji

,

Q

which also leaves the determinant invariant

Q

det ˜D =

˜λi =

Yi

Yi  

λi

j>i θij
j<i θji !

Q

=

i

λi

! Q
i

Q

j>i θij
j<i θji

 

Yi

Yi

(470)
A more general transformation with unique parameterization by θi > 0,
i

= i∗, still leaving the eigenvectors unchanged, would be

Q

Q

Q

=

λi = det D.

˜λi = λiθi, i

= i∗;

˜λi∗ = λi∗

θ−
i

1

.

(471)

=i∗
Yi

107

6
6
6
This techniques can be applied to a general positive deﬁnite K after diago-
nalizing

K = ODOT

˜K = O ˜DOT

det K = det ˜K.

(472)

→

⇒

As example consider the transformations (469, 471) for which the speciﬁc
prior term becomes

1
2

φ
(cid:16)
and stationarity Eq. (450)

t, K(θ) (φ

−

t)

=

−

(cid:17)

1
2

φ
(cid:16)

−

t, OD(θ)OT (φ

(473)

−

t)

,
(cid:17)

1
2

φ
(cid:16)

−

∂D
∂θ

t, O

OT (φ

t)

=

E′θ,

−

−

(cid:17)

(474)

and for (469), with k < l,

∂D(i, j)
∂θkl

= δ(i

j)

δ(k

−

 

−

i) λk

l

=n>k θkn
n<k θnk

+ δ(l

i) λl

−

n>l θln
=n<l θnl !

Q
k

, (475)

or, for (471), with k

= i∗,

Q

Q

∂D(i, j)
∂θk

= δ(i

j)

δ(k

−

 

−

i) λk + δ(i

i∗) λi∗

−

θk

(476)

If, for example, K is a translationally invariant operator it is diagonalized
in a basis of plane waves. Then also ˜K is translationally invariant, but
its sensitivity to certain frequencies has changed. The optimal sensitivity
pattern is then determined by the given stationarity equations.

Q

1
.
=i∗ θl !

l

Q

5.3.6 Regularization parameters

Next we consider the example K(γ) = γK0 where θ
0 has been denoted γ,
representing a regularization parameter or an inverse temperature variable for
the speciﬁc prior. For a d–dimensional Gaussian integral the normalization
factor becomes Zφ(γ) = ( 2π
1/2. For positive (semi)deﬁnite K the
γ )
dimension d is given by the rank of K under a chosen discretization. Skipping
constants results in a normalization energy EN (γ) =

2 (det K0)−

d
2 ln γ. With

≥

d

−

(477)

∂K
∂γ

= K0

108

6
6
6
6
we obtain the stationarity equations

γK0(φ

1
2

(φ

t, K0 (φ

−

−

t) = P′(φ)P−
d
2 γ −

t)) =

E′γ.

−

1(φ)N

P′(φ)ΛX,

−

(478)

(479)

For compensating hyperprior the right hand side of Eq. (479) vanishes, giving
thus no stationary point for γ. Using however the condition γ
0 one sees
that for positive deﬁnite K0 Eq. (478) is minimized for γ = 0 corresponding
to the ‘prior–free’ case. For example, in the case of Gaussian regression the
solution would be the data template φ = h = tD. This is also known as
“δ–catastrophe”. To get a nontrivial solution for γ a noncompensating hy-
perparameter energy Eγ = Eθ must be used so that ln Zφ + EN is nonuniform
[14, 21].

≥

The other limiting case is a vanishing E′γ for which Eq. (479) becomes

γ =

(φ

d
t, K0 (φ

.

t))

(480)

t one sees that γ

−
For φ
. Moreover, in case P [t] represents a nor-
malized probability, φ = t is also a solution of the ﬁrst stationarity equation
. Thus, for vanishing E′γ the ‘data–free’ solution
(478) in the limit γ
φ = t is a selfconsistent solution of the stationarity equations (478,479).

→ ∞

→ ∞

→

−

Fig.6 shows a posterior surface for uniform and for compensating hyper-
prior for a one–dimensional regression example. The Maximum A Posteriori
Approximation corresponds to the highest point of the joint posterior over
γ, h in that ﬁgures. Alternatively one can treat the γ–integral by Monte–
Carlo–methods [219].

Finally we remark that in the setting of empirical risk minimization,
due to the diﬀerent interpretation of the error functional, regularization pa-
rameters are usually determined by cross–validation or similar techniques
[153, 5, 214, 200, 201, 76, 35, 195, 212, 49, 78].

5.4 Exact posterior for hyperparameters

In the previous sections we have studied saddle point approximations which
D, D0) simultaneously with
lead us to maximize the joint posterior p(h, θ

|

109

p

2

p

1

h

0

2

1

h

0

2
2

4
4

gamma
gamma

6
6

2
2

4
4

gamma
gamma

6
6

8
8

-1

10

8
8

-1

10

h)p(h
|

Figure 6: Shown is the joint posterior density of h and γ, i.e., p(h, γ
∝
γ, D0)p(γ) for a zero–dimensional example of Gaussian regression
p(yD|
with training data yD = 0 and prior data yD0 = 1. L.h.s: For uniform prior
2 ln γ, having
p(γ)
e−
, h = 1. R.h.s.: For compensating hyperprior
its maximum is at γ =
1)2 having its maximum is at γ = 0,
1/√γ so that p
p(γ)
e−
∝
h = 0.

1 so that the joint posterior becomes p

D, D0)

∞
∝

1)2+ 1

γ
2 (h

γ
2 (h

1
2 h2

2 h2

∝

∝

−

−

−

−

|

1

respect to the hidden variables h and θ

p(y

x, D, D0) = p(yD|

|

xD, D0)−

1

dh

dθ p(y

Z

Z

x, h) p(yD|

|

xD, h)p(h
|
D,D0), max w.r.t. θ and h

D0, θ)p(θ)

,

p(h,θ

|

∝
|

{z

(481)
}
x, h)

assuming for the maximization with respect to h a slowly varying p(y
at the stationary point.

|

This simultaneous maximization with respect to both variables is consis-
tent with the usual asymptotic justiﬁcation of a saddle point approximation.
For example, for a function f (h, θ) of two (for example, one–dimensional)
variables h, θ

dh dθ e−

βf (h,θ)

βf (h∗,θ∗)

1

2 ln det(βH/2π)

−

(482)

e−

≈

Z

for large enough β (and a unique maximum). Here f (h∗, θ∗) denotes the joint
minimum and H the Hessian of f with respect to h and θ. For θ–dependent
determinant of the covariance and the usual deﬁnition of β, results in a
function f of the form f (h, θ) = E(h, θ) + (1/2β) ln det(βK(θ)/2π), where

110

both terms are relevant for the minimization of f with respect to θ. For
large β, however, the second term becomes small compared to the ﬁrst one.
(Of course, there is the possibility that a saddle point approximation is not
adequate for the θ integration. Also, we have seen that the condition of a
positive deﬁnite covariance may lead to a solution for θ on the boundary
where the (unrestricted) stationarity equation is not fulﬁlled.)

Alternatively, one might think of performing the two integrals stepwise.
This seems especially useful if one integral can be calculated analytically.
Consider, for example

dh dθ e−

βf (h,θ)

βf (θ,h∗(θ))

dθ e−

1

2 ln det( β

2π

∂2f (h∗(θ))
∂h2

)

−

(483)

Z

≈ Z

which would be exact for a Gaussian h–integral. One sees now that mini-
mizing the complete negative exponent βf (θ, h∗) + 1
2 ln det(β(∂2f /∂h2)/2π)
with respect to θ is diﬀerent from minimizing only f in (482), if the second
derivative of f with respect to h depends on θ (which is not the case for
a Gaussian θ integral). Again this additional term becomes negligible for
large enough β. Thus, at least asymptotically, this term may be altered or
even be skipped, and diﬀerences in the results of the variants of saddle point
approximation will be expected to be small.

Stepwise approaches like (483) can be used, for example to perform Gaus-
sian integrations analytically, and lead to somewhat simpler stationarity
equations for θ–dependent covariances [219].

In particular, let us look at the case of Gaussian regression in a bit more
detail. The following discussion, however, also applies to density estimation
if, as in (483), the Gaussian ﬁrst step integration is replaced by a saddle point
approximation including the normalization factor. (This requires the calcu-
lation of the determinant of the Hessian.) Consider the two step procedure
for Gaussian regression

p(y

x, D, D0) = p(yD|

|

xD, D0)−

1

dθ p(θ)
Z

Z

dh p(y

x, h)p(yD|

xD, h)p(h
|

|

D0, θ)

,

exact

=

dθ p(θ

D, D0)

x, D, D0, θ)

p(θ)p(y,yD
|

x,xD,D0,θ)

p(y,θ
{z

|

∝
{z

|

x,D,D0) max w.r.t. θ
}
}
(484)

Z

exact

|
∝
p(y,θ
{z

|

|
|

|
p(y
|

exact

x,D,D0), max w.r.t. θ

}

|

{z

}
}

{z

111

x, xD, D0, θ) can be calculated analytically and in
where in a ﬁrst step p(y, yD|
a second step the θ integral is performed by Gaussian approximation around
a stationary point. Instead of maximizing the joint posterior p(h, θ
D, D0)
with respect to h and θ this approach performs the h–integration analytically
and maximizes p(y, θ
x, D, D0) with respect to θ. The disadvantage of this
approach is the y–, and x–dependency of the resulting solution.

|

|

Thus, assuming a slowly varying p(y

it appears simpler to maximize the h–marginalized posterior p(θ
D, D0), performing this h–integration exactly,

dh p(h, θ

|

x, D, D0, θ) at the stationary point
D, D0) =

|

|

R

p(y

x, D, D0) =

dθ p(θ

D, D0)

p(y

x, D, D0, θ)

.

(485)

|

Z

|
exact

|

exact

{z

}

|
|

max w.r.t. θ
{z
{z

}
}

|

Having found a maximum posterior solution θ∗ the corresponding anaytical
solution for p(y
x, D, D0, θ∗) is then given by Eq. (312). The posterior density
D, D0) can be obtained from the likelihood of θ and a speciﬁed prior p(θ)
p(θ

|

|

p(θ

D, D0) =

p(yD|

xD, D0, θ)p(θ)
xD, D0)

.

p(yD|
Hence, for Gaussian regression, the likelihood can be integrated analyti-

|

(486)

cally, analogously to Section 3.7.2, yielding [212, 220, 219],

p(yD|

xD, D0, θ) =

dh e−

1
2

n

i=0 (h

−

ti, Ki(h

ti))+ 1

2

−

n

i=0 ln deti(Ki/2π)

P

n

i=0 (ti, Kiti)+ 1

2 (t, Kt)+ 1

P
2 ln detD(

K/2π)

P
tD

t0,

K(tD

t0)

−

+ 1

2 ln detD

K

˜n
2 ln(2π)
e

−

(cid:16)
E+ 1

2 ln detD
e

−
(cid:17)
2 ln(2π),

˜n

−

K

e

Z

= e−

= e−
= e−

1
2

1
2

e

(487)

e
K = (K−

(cid:16)

t0,

t0)

tD −

1
D + K−

K(tD −

E = 1
1 = KD +
where
,
2
1KD, detD the determinant in data space, and we used that from
KDK−
(cid:17)
e
1
n
i Kj = δij for i, j > 0 follows
i=0 (ti, Kiti) = (tD, KDtD) + (t0, K0t0)
K−
n
= (tD, Kt), with K =
i=0 Ki. We already mentioned in Section 2.3 that
the Maximum A Posteriori Approximation (MAP) might also seen as sad-
i.e., the (θ–
dle point approximation for the θ–likelihood p(yD|
conditional) evidence of the data yD (see Eq.(74). Thus, in cases where

1
0,DD(θ))−

xD, D0θ),

P

P

f

f

112

the marginalization over h, necessary to obtain that evidence, cannot be per-
formed analytically, but has to be done in saddle point approximation, we
get the same results as for a MAP of the predictive density.

Now we are able to compare the three resulting stationary equations
for θ–dependent mean t0(θ), covariance K0(θ) and prior p(θ). Setting the
D, D0) with respect to θ to zero yields
derivative of the joint posterior p(h, θ

0 =

∂t0
∂θ

 

Tr

−

!

h)

, K0(t0 −
∂K0
∂θ ! −

K−
0

1

 

|

+

1
2

1
p(θ)

t0,

h

−
(cid:16)
∂p(θ)
∂θ

.

∂K0(θ)
∂θ

(h

t0)

−

(cid:17)

(488)

This equation which we have already discussed has to be solved simulta-
neously with the stationarity equation for h. While this approach is easily
adapted to general density estimation problems, its diﬃculty for θ–dependent
covariance determinants lies in calculation of the derivative of the determi-
nant of K0. Maximizing the h–marginalized posterior p(θ
D, D0), on the
other hand, only requires the calculation of the derivative of the determinant
of the ˜n

˜n matrix

K

|

+

1
2  

(tD −

t0),

∂
K
∂θ
f

(tD −

t0)

!

f
,

∂t0
∂θ

 

Tr

−

 

!

tD)

K(t0 −
1 ∂
K
∂θ ! −
f

f
K−

f

1
p(θ)

∂p(θ)
∂θ

.

(489)

1

K(tD −

Evaluated at the stationary h∗ = t0 + K−
t0), the ﬁrst term of Eq.
0
(488), which does not contain derivatives of the covariances, becomes equal to
the ﬁrst term of Eq. (489). The last terms of Eqs. (488) and (489) are always
identical. Typically, the data–independent K0 has a more regular structure
K. Thus, at least for one or two dimensional x, a
than the data–dependent
straightforward numerical solution of Eq. (488) by discretizing x can also be
a good choice for Gaussian regression problems.

f

f

Analogously, from Eq. (312) follows for maximizing p(y, θ

x, D, D0) with

|

respect to θ

×

0 =

0 =

, Ky(t

y)

+

 

∂t
∂θ

Tr

−

 

!

−
∂Ky
∂θ ! −

1

K−
y

1
2  

(y

−

1
D, D0)

t),

∂Ky
∂θ
∂p(θ

t)

!

(y

−
D, D0)
|
∂θ

,

(490)

p(θ

|

113

which is y–, and x–dependent. Such an approach may be considered if inter-
ested only in speciﬁc test data x, y.

We may remark that also in Gaussian regression the θ–integral may be
quite diﬀerent from a Gaussian integral, so a saddle point approximation
does not necessarily have to give satisfactory results. In cases one encoun-
f (θ)dθ =
ters problems one can, for example, try variable transformations
det(∂θ/∂θ′)f (θ(θ′))dθ′ to obtain a more Gaussian shape of the integrand.
Due to the presence of the Jacobian determinant, however, the asymptotic
R
interpretation of the corresponding saddle point approximation is diﬀerent
for the two integrals. The variablility of saddle point approximations results
from the freedom to add terms which vanish asymtotically but remains ﬁnite
in the nonasymptotic region. Similar eﬀects are known in quantum many
body theory (see for example [159], chapter 7.) Alternatively, the θ–integral
can be solved numerically by Monte Carlo methods[220, 219].

R

5.5

Integer hyperparameters

The hyperparameters θ considered up to now have been real numbers, or
vector of real numbers. Such hyperparameters can describe continuous trans-
formations, like the translation, rotation or scaling of template functions and
the scaling of covariance operators. For real θ and diﬀerentiable posterior,
stationarity conditions can be found by diﬀerentiating the posterior with
respect to θ.

Instead of a class of continuous transformations a ﬁnite number of al-
ternative template functions or covariances may be given. For example, an
image to be reconstructed might be expected to show a digit between zero
and nine, a letter from some alphabet, or the face of someone who is a mem-
ber of known group of people. Similarly, a particular times series may be
expected to be either in a high or in a low variance regime. In all these cases,
there exist a ﬁnite number of classes i which could be represented by speciﬁc
templates ti or covariances Ki. Such “class” variables i are nothing else than
hyperparameters θ with integer values.

Binary parameters, for example, allow to select from two reference func-
tions or two covariances that one which ﬁts the data best. E.g., for i =
θ

one can write

0, 1

∈ {

}

t(θ) = (1
K(θ) = (1

θ)t1 + θt2,
θ)K1 + θK2.

(491)
(492)

−
−

114

For integer θ the integral

θ (we will also write some-
i if integer and continuous hyperparameters occur), so that prior,
times
posterior, and predictive density have the form of a ﬁnite mixture with com-
ponents θ.

dθ becomes a sum

P

P

R

For a moderate number of components one may be able to include all
of the mixture components. Such prior mixture models will be studied in
Section 6.

If the number of mixture components is too large to include them all
explicitly, one again must restrict to some of them. One possibility is to
select a random sample using Monte–Carlo methods. Alternatively, one may
search for the θ∗ with maximal posterior. In contrast to typical optimization
problems for real variables, the corresponding integer optimization problems
are usually not very smooth with respect to θ (with smoothness deﬁned in
terms of diﬀerences instead of derivatives), and are therefore often much
harder to solve.

There exists, however, a variety of deterministic and stochastic integer op-
timization algorithms, which may be combined with ensemble methods like
genetic algorithms [91, 74, 39, 145, 113, 193, 148], and with homotopy meth-
ods, like simulated annealing [106, 144, 185, 38, 1, 188, 226, 63, 227, 228]. An-
nealing methods are similar to (Markov chain) Monte–Carlo methods, which
aim in sampling many points from a speciﬁc distribution (i.e., for example
at ﬁxed temperature). For them it is important to have (nearly) indepen-
dent samples and the correct limiting distribution of the Markov chain. For
annealing methods the aim is to ﬁnd the correct minimum (i.e., the ground
state having zero temperature) by smoothly changing the temperature from
a ﬁnite value to zero. For them it is less important to model the distribution
for nonzero temperatures exactly, but it is important to use an adequate
cooling scheme for lowering the temperature.

Instead of an integer optimization problem one may also try to solve a
0, 1
similar problem for real θ. For example, the binary θ
in Eqs. (491)
and (492) may be extended to real θ
[0, 1]. By smoothly increasing an
appropriate additional hyperprior p(θ) one can ﬁnally enforce again binary
hyperparameters θ

∈ {

0, 1

∈

}

.

∈ {

}

5.6 Local hyperﬁelds

Most, but not all hyperparameters θ considered so far have been real or
integer numbers or vectors with real or integer components θi. With the

115

unrestricted template functions of Sect. 5.2.3 or the functions parameterizing
the covariance in Sections 5.3.3 and 5.3.4, we have, however, also encountered
function hyperparameters or hyperﬁelds. In this section we will now discuss
function hyperparameters in more detail.

Functions can be seen as continuous vectors, the function values θ(u)
being the (continuous) analogue of vector components θi. In numerical cal-
culations, in particular, functions usually have to be discretized, so functions
stand for high dimensional vectors.

Typical arguments of function hyperparameters are x and, for general
density estimation, also y variables. Such functions θ(x) or θ(x, y) will be
called local hyperparameters or local hyperﬁelds. Local hyperﬁelds θ(x) can
be used, for example, to adapt templates or covariances locally. (For general
density estimation problems replace here and in the following x by (x, y).)

The price to be paid for the additional ﬂexibility of function hyperparam-
eters is a large number of additional degrees of freedom. This can consid-
erably complicate calculations and, requires a suﬃcient number of training
data and/or a suﬃciently restrictive hyperprior to be able to determine the
hyperﬁeld and not to make the prior useless.

To introduce local hyperparameters θ(x) we express real symmetric, pos-
itive (semi–) deﬁnite inverse covariances by square roots W, K = WT W =
dx WxW T

x where Wx represents the vector W(x,

). In components

R

K(x, x′) =

dx′′ WT (x, x′′)W(x′′, x′).

(493)

·

In terms of ‘ﬁltered diﬀerences’ ω(x) =
can be written

dx′W(x, x′) (φ(x′)

t(x′)) the prior

−

Z

p(φ)

1
2

e−

dx

ω(x)
|

|

2

.

∝

A local hyperparameter θ(x) may be introduced as follows

p(φ

θ) = e−

1
2

dx

ω(x;θ)
|

|

ln Zφ(θ) = e−

−

2

1
2

dx ((1

θ(x))
|

ω1(x)
|

−

2+θ(x)
|

ω2(x)
|

2)−

ln Zφ(θ),

|

with

R

or covariances

and, for instance, binary θ(x)

ω(x; θ) = (1

θ(x)) ω1(x) + θ(x) ω2(x),
0, 1

.

−

∈ {

}

Local modiﬁcations of ω(x) can be constructed from variants of templates

(494)

(495)

(496)

(497)
(498)

tx(θ) = (1
Wx(θ) = (1

θ(x)) t1,x + θ(x) t2,x,
θ(x)) W1,x + θ(x) W2,x,

−
−

R

R

R

116

the latter corresponding to

(1
h

−

K(θ) =

dx Kx(θ) =

dx

θ(x))W1,xW T

1,x + θ(x)W2,xW T
2,x

,

(499)

Z

Z
where Kx(θ) = Wx(θ)W T
x (θ). For real θ(x) in Eq. (496) additional terms
θ2 and (1
θ(x))θ(x) would appear in Eq. (495) ). Notice, that also the
unrestricted adaption of templates discussed in Sect. 5.2.3 corresponds to
the adaption of a real function θ(x).

−

i

A real θ variable can be converted into a binary variable by replacing θ

in Eq. (496). for example by

Bθ(x) = Θ(θ(x)

ϑ).

−

(500)

In case, however, the hyperprior is also formulated in terms of Bθ(x) this is
completely equivalent to a binary formulation.

Notice, that local templates tx(θ) for ﬁxed x are still functions tx(x′; θ)
of another x′ variable. Indeed, to obtain ω(x), the function tx is needed for
all x′ for which W has nonzero entries. For a given θ(x) the corresponding
eﬀective template t(θ) and eﬀective covariance K(θ) are, according to Eqs.
(250,247), given by

t(θ) = K(θ)−

1

Kx(θ) tx(θ),

K(θ) =

x
X
Kx(θ)

,

!

 

x
X

(501)

(502)

i.e., one may rewrite

ω(x, θ)

2 = (φ

|

−

x |
X

t, K (φ

t)) +

(tx, Kx tx)

(t, K t) .

(503)

−

x
X

−

The MAP of Gaussian regression for a prior corresponding to (503) at optimal
1 (KDtD + K(θ∗) t(θ∗)), according
θ∗ is therefore given by φ∗ = (KD +K(θ∗))−
to Section 3.7.

As example, consider the following prior energy,

E(φ

θ) =

φ

t0(θ), (φ

t0(θ))

+

(504)

|

1
2

(cid:16)

−

−

(cid:17)

1
2

φ, K0 φ
(cid:16)

(cid:17)

.

Because the covariance of the θ–dependent term is the identity, (t0)x(x′; θ)
is only needed for x = x′ and we may thus directly write t0(θ) denoting a

117

function t0(x; θ). To get the eﬀective prior template t for φ, however, both
terms have to be combined, yielding

E(φ

θ) =

|

φ

1
2 (cid:18)(cid:16)

−

t(θ), K(φ

t(θ))

+

t0(θ),

1

K−

t0(θ)

(505)

−

(cid:17)

(cid:16)

I
(cid:16)

−

(cid:17)

,
(cid:17)(cid:19)

with eﬀective template and eﬀective inverse covariance

t(θ) = K−

1t0(θ), K = I + K0.

(506)

For diﬀerential operators W the eﬀective t(θ) is thus a smoothed version of
t0(θ).

The extreme case would be to treat t and W itself as unrestricted hyper-
parameters. Notice, however, that increasing ﬂexibility tends to lower the
inﬂuence of the corresponding prior term. That means, using completely free
templates and covariances without introducing additional restricting hyper-
priors, just eliminates the corresponding prior term (see Section 5.2.3).

Hence, to restrict the ﬂexibility, typically a smoothness hyperprior is im-
posed to prevent highly oscillating functions θ(x). For example, to restrict
the number of discontinuities for a one–dimensional x, one may include a
factor like

κ
2

e−

dx δ(1

Cθ(x)),

−

p(θ)

∝

R

(507)

with constant κ, and

2

∂θ
∂x !

Cθ(x) = Θ

 



ϑθ

and step function Θ with Θ(x) = 0 for x

−



,

(508)

≤

∞

≤ ∞

with threshold 0

. In the binary case, where

ϑθ <
and Θ(x) = 1 for 1 < x
,
∞}
the term (508) counts the number of jumps. For real θ(x) an additional
∆θ) should be added in regions where it is deﬁned
smoothness prior like (θ,
t)) with
(The space of φ–functions for which a smoothness prior (φ
discontinuous t(θ) is deﬁned depends on the locations of the discontinuities.)
To enable diﬀerentiation the step function Θ could be replaced by a sigmoidal
function.

t, K(φ

∈ {

∂θ
∂x

−

−

−

≤

0,

0

(cid:16)

(cid:17)

2

The expression Cθ of Eq. (508) can be generalized to

Cθ(x) = Θ

ωθ(x)

2

|

−

ϑθ

,

(cid:17)

|
(cid:16)

118

(509)

where ωθ(x) = (WθBθ)(x) and Wθ is some hyperprior operator, analogous
to the operator W in the prior, acting on the function Bθ deﬁned in Eq.
(500).

Discontinuous functions φ can be approximated by using discontinuous
templates t(x; θ) or by eliminating matrix elements of the covariance which
connect the two sides of the discontinuity. For example, consider the discrete
version of a negative Laplacian with periodic boundary conditions,

and square root,

K = WT W =

2
1
−
0
0
0
1
−













1
−
2
1
−
0
0
0

0
1
−
2
1
−
0
0

0
0
1
−
2
1
−
0

W =

1
0
0
0
0
1
−













1
−
1
0
0
0
0

0
1
−
1
0
0
0

0
0
1
−
1
0
0

0
0
0
1
−
1
0

1
−
0
0
0
1
−
2

,













0
0
0
1
−
2
1
−

0
0
0
0
1
−
1

.













(510)

(511)

The ﬁrst three points can be disconnected from the last three points by
setting W(3) and W(6) to zero, namely,

W =

(512)

so that the smoothness prior is ineﬀective between points from diﬀerent re-
gions,

K = WT W =

(513)













1
0
0
0
0
0













1
−
1
0
0
0
0

0
1
−
0
0
0
0

0
0
0
1
0
0

0
0
0
1
−
1
0

0
0
0
0
1
−
0













1
1
−
0
0
0
0

1
−
2
1
−
0
0
0

119

0
1
−
1
0
0
0

0
0
0
1
1
−
0

0
0
0
1
−
2
1
−

0
0
0
0
1
−
1

.













In contrast to using discontinuous templates, training data are in this case
required for both regions to determine the free constants which correspond
to the zero mode of the Laplacian.

Non–Gaussian priors often provide an alternative to the use of function
hyperparameters. Similarly to Eq. (508) one may deﬁne a B(x) directly in
terms of φ,

like, for a negative Laplacian prior,

B(x) = Θ

ω1(x)

2

ω2(x)

2

|

− |

|

−

|
(cid:16)

,

ϑ
(cid:17)

(514)

∂(φ

2

t1)

∂(φ

2

t2)

B(x) = Θ

−
∂x


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Notice, that the functions ωi(x) and B(x) may be nonlocal with respect to
φ(x), meaning they may depend on more than one φ(x) value. The thresh-
old ϑ can be related to the prior expectations about ωi. A possible prior
formulated in terms of B can be,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





− (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−
∂x

.

ϑ



−

(515)

1
2

e−

dx (|

ω1(x)
|

2(1

−

B(x))+

ω2(x)
|

|

2B(x)

κ
2 δ(1

C(x))),

−

−

(516)

p(φ)

∝

R

with

C(x) = Θ

(WBB)(x)

ϑB

,

(517)

|
(cid:16)

2

|

−

(cid:17)

and some threshold ϑB and operator WB. Similarly to the introduction
of hyperparameters, one can again treat B(x) formally as an independent
ϑ)) in the
function by including a term λ (B(x)
prior energy and taking the limit λ

Θ (
|
.

ω2(x)

ω1(x)

− |

−

|

|

2

2

Eq. (516) looks similar to the combination of the prior (495) with the

−
→ ∞

hyperprior (507),

∝

R

p(φ, θ)

1
2

e−

dx (|

ω1(x)
|

2(1

−

Bθ(x))+

ω2(x)
|

|

2Bθ(x)

κ
2 δ(1

−

−

Cθ(x))

ln Zφ(θ)).

−

(518)

Notice, however, that the deﬁnition of Bθ (or Cθ, respectively), is diﬀerent
from that of B (or C). If the ωi diﬀer only in their templates, the normaliza-
tion term can be skipped. Then, identifying Bθ in (518) with a binary θ and
assuming ϑ = 0, ϑθ = ϑB, Wθ = WB, the two equations are equivalent for
2). In the absence of hyperpriors, it is indeed easily
θ = Θ (
|
seen that this is a selfconsistent solution for θ, given φ. In general, however,
there may be a trade oﬀ with the hyperprior, and another solution for θ, not

ω1(x)

ω2(x)

− |

|

|

2

120

selecting locally the smallest of the two prior contributions, might be better.
Non–Gaussian priors will be discussed in Section 6.5.

Hyperpriors, or analogous non–Gaussian prior terms, are for example
useful to enforce speciﬁc global constraints for θ(x) or B(x). In images, for
example, discontinuities are expected to form closed curves. Hyperpriors,
organizing discontinuities along lines or closed curves, are thus important for
image segmentation [65, 141, 61, 62, 221, 229].

6 Non–Gaussian prior factors

6.1 Mixtures of Gaussian prior factors

Complex, non–Gaussian prior factors, for example being multimodal, may be
constructed or approximated by using mixtures of simpler prior components.
In particular, it is convenient to use as components or “building blocks”
Gaussian densities, as then many useful results obtained for Gaussian pro-
cesses survive the generalization to mixture models [123, 124, 125, 126, 127].
We will therefore in the following discuss applications of mixtures of Gaus-
sian priors. Other implementations of non–Gaussian priors will be discussed
in Section 6.5.

In Section 5.1 we have seen that hyperparameters label components of
mixture densities. Thus, if j labels the components of a mixture model, then
j can be seen as hyperparameter. In Section 5 we have treated the corre-
sponding hyperparameter integration completely in saddle point approxima-
tion. In this section we will assume the hyperparameters j to be discrete and
try to calculate the corresponding summation exactly.

Hence, consider a discrete hyperparameter j, possibly in addition to con-
In contrast to the θ–integral we aim now in
tinuous hyperparameters θ.
treating the analogous sum over j exactly, i.e., we want to study mixture
models

p(φ, θ

˜D0) =

p(φ, θ, j

˜D0) =

p(φ

˜D0, θ, j)p(θ, j).

(519)

|

|

|

m

Xj

m

Xj

In the following we concentrate on mixtures of Gaussian speciﬁc priors. No-
tice that such models do not correspond to Gaussian mixture models for φ
as they are often used in density estimation. Indeed, the form of φ may be
completely unrestricted, it is only its prior or posterior density which is mod-
eled by a mixture. We also remark that a strict asymptotical justiﬁcation of

121

a saddle point approximation would require the introduction of a parameter
˜β so that p(φ, θ
˜D0)
j pj . If the sum is reduced to a single term then
˜β corresponds to β.

˜β ln
e

∝

P

|

We already discussed shortly in Section 5.2 that, in contrast to a product
of probabilities or a sum of error terms implementing a probabilistic AND
of approximation conditions, a sum over j implements a probabilistic OR.
Those alternative approximation conditions will in the sequel be represented
by alternative templates tj and covariances Kj. A prior (or posterior) den-
sity in form of a probabilistic OR means that the optimal solution does not
necessarily have to approximate all but possibly only one of the tj (in a met-
ric deﬁned by Kj). For example, we may expect in an image reconstruction
task blue or brown eyes whereas a mixture between blue and brown might
not be as likely. Prior mixture models are potentially useful for

1. Ambiguous (prior) data. Alternative templates can for example repre-

sent diﬀerent expected trends for a time series.

2. Model selection. Here templates represent alternative reference models
(e.g., diﬀerent neural network architectures, decision trees) and deter-
mining the optimal θ corresponds to training of such models.

3. Expert knowledge. Assume a priori knowledge to be formulated in
terms of conjunctions and disjunctions of simple components or build-
ing blocks (for example verbally). E.g., an image of a face is expected
to contain certain constituents (eyes, mouth, nose; AND) appearing
in various possible variants (OR). Representing the simple compo-
nents/building blocks by Gaussian priors centered around a typical
example (e.g.,of an eye) results in Gaussian mixture models. This con-
stitutes a possible interface between symbolic and statistical methods.
Such an application of prior mixture models has some similarities with
the quantiﬁcation of “linguistic variables” by fuzzy methods [110, 111].

For a discussion of possible applications of prior mixture models see also
[123, 124, 125, 126, 127]. An application of prior mixture models to image
completion can be found in [128].

122

6.2 Prior mixtures for density estimation

The mixture approach (519) leads in general to non–convex error functionals.
For Gaussian components Eq. (519) results in an error functional

Eθ,φ =

(ln P (φ), N) + (P (φ), ΛX)

e−( 1

2 (φ

−

tj(θ), Kj (θ) (φ

tj (θ)))+ln Zφ(θ,j)+Eθ,j),

−

−

−

−

ln

ln

Xj

Xj

=

e−

Eφ,j−

Eθ,j+cj ,

where

−

and

Eφ,j =

(ln P (φ), N) + (P (φ), ΛX) +

tj(θ), Kj(θ) (φ

tj(θ))

, (522)

1
2

φ
(cid:16)

−

−

(cid:17)

The stationarity equations for φ and θ

cj =

ln Zφ(θ, j).

−

e−

Eφ,j−

Eθ,j+cj ,

∂Eφ,j
∂θ

+

∂Eθ,j
∂θ

+ Z′jZ −

φ (θ, j)

1

e−

Eφ,j −

Eθ,j+cj ,

(525)

!

can also be written
m

δEφ,j
δφ

p(φ, θ, j

˜D0),

|

∂Eφ,j
∂θ

+

∂Eθ,j
∂θ

+ Z′jZ −

φ (θ, j)

1

p(φ, θ, j

!

˜D0).

|

Analogous equations are obtained for parameterized φ(ξ).

0 =

0 =

δEφ,j
δφ

m

Xj
m

Xj  

0 =

0 =

Xj
m

Xj  

(520)

(521)

(523)

(524)

(526)

(527)

6.3 Prior mixtures for regression

For regression it is especially useful to introduce an inverse temperature mul-
tiplying the terms depending on φ, i.e., likelihood and prior.3 As in regression
3As also the likelihood term depends on β it may be considered part of a ˜φ together
regression function h(x). Due to its similarity to a regularization factor we have included
β in this chapter about hyperparameters.

123

m

Xj

−

−
1
2

−

φ is represented by the regression function h(x) the temperature–dependent
error functional becomes

Eθ,h =

ln

−

e−

βEh,j −

Eθ,β,j+cj =

e−

Ej+cj ,

(528)

with

1
2

−

Ej = ED + E0,j + Eθ,β,j,

(529)

ED =

(h

tD, KD (h

tD)) , E0,j =

(h

tj(θ), Kj(θ) (h

tj(θ))) ,

−

(530)

ln

−

m

Xj

1
2

−

some hyperprior energy Eθ,β,j, and

cj(θ, β) =

ln Zh(θ, j, β) +

=

ln det (Kj(θ)) +

(531)

ln β

n
2
d + n
2

β
2

−

ln β

VD −
β
2

−

c

VD

ni

If we also maximize with respect to β we have to
with some constant c.
n
i Vi where Vi =
include the (h–independent) training data variance VD =
t2
k y(xk)2/ni −
D(xi) is the variance of the ni training data at xi. In case
every xi appears only once VD vanishes. Notice that cj includes a contribution
P
from the n data points arising from the β–dependent normalization of the
likelihood term. Writing the stationarity equation for the hyperparameter β
separately, the corresponding three stationarity conditions are found as

P

0 =

KD (h

tD) + Kj (h

tj)

e−

βEh,j−

Eθ,β,j+cj ,

(532)

−

(cid:17)
∂Kj
∂θ !!

1

K−
j

 

0 =

E′h,j + E′θ,β,j + Tr

e−

βEh,j −

Eθ,β,j+cj ,

(533)

0 =

E0,j +

∂Eθ,β,j
∂β

+

d + n

2β !

e−

βEh,j−

Eθ,β,j+cj .

(534)

m

Xj (cid:16)
m

Xj  

m

Xj  

As β is only a one–dimensional parameter and its density can be quite non–
Gaussian it is probably most times more informative to solve for varying
values of β instead to restrict to a single ‘optimal’ β∗. Eq. (532) can also be
written

h =

KD +

KDtD +

ajKjtj

,

(535)





m

Xj

ajKj 


1

−

 

124

m

Xl

!

with

aj = p(j

h, θ, β, D0) =

|
p(h
|

=

=

Ek+ck

Ej +cj

e−
m
k e−
θ, β, D0)
P
|
θ, β, D0)

p(h
P
|

=

2 ln det Kj

Eθ,β,k+ 1

2 ln det Kk

βE0,j

Eθ,β,j+ 1

e−
−
m
βE0,k−
k e−
j, θ, β, D0)p(j, θ
β, D0)

p(h, θ

|

|

β, D0)

, (536)

p(h
|

j, θ, β, D0)p(j

being thus still a nonlinear equation for h.

6.3.1 High and low temperature limits

It are the limits of large and small β which make the introduction of this
additional parameter useful. The reason being that the high temperature
limit β
0 gives the convex case, and statistical mechanics provides us with
high and low temperature expansions. Hence, we study the high temperature
and low temperature limits of Eq. (535).
In the high temperature limit β

0 the exponential factors aj become

→

h–independent

→

e−
m
k e−
In case one chooses Eθ,β,j = Eβ,j + βEθ one has to replace Eθ,β,j by Eβ,j. The
P
high temperature solution becomes

2 ln det Kk

β
0
→
−→

a0
j =

Eθ,β,k+ 1

(537)

aj

.

Eθ,β,j+ 1

2 ln det Kj

h = ¯t

(538)

with (generalized) ‘complete template average’

m

1

−

m

¯t =

KD +

a0
j Kj

Notice that ¯t corresponds to the minimum of the quadratic functional

KDtD +

a0
j Kjtj

Xj

Xl





 

!

.

(539)

E(β=

) =

∞

h
(cid:16)

−

tD, KD(h

tD)

+

−

a0
j

h

−

tj, Kj(h

tj)

−

(540)

.
(cid:17)

m

(cid:17)

Xj

(cid:16)

Thus, in the inﬁnite temperature limit a combination of quadratic priors by
OR is eﬀectively replaced by a combination by AND.

125

(543)

(544)

(545)

(546)

In the low temperature limit β

we have, assuming Eθ,β,j = Eβ +

→ ∞

e−

β(E0,j +Eθ)

Eβ −

Ej = e−

−

β(E0,j∗ +Eθ)

Eβ

−

e−

β(E0,j

E0,j∗ )

Ej

−

−

(541)

e−

β(E0,j∗ +Eθ)

Eβ −

−

Ej

for E0,j∗ < E0,j,

= j∗, p(j∗)

= 0, , (542)

Xj

j
∀

Ej + βEθ,

Xj

β
→∞
−→

meaning that

aj

β
→∞
−→

a∞j =

1 :
0 :

(

j = argminjE0,j = argminjEh,j
= argminjE0,j = argminjEh,j
j

.

Henceforth, all (generalized) ‘component averages’ ¯tj become solutions

h = ¯tj,

with

i.e.,

where

¯tj = (KD + Kj)−

1 (KDtD + Kjtj) ,

provided the ¯tj fulﬁll the stability condition

Eh,j(h = ¯tj) < Eh,j′(h = ¯tj),

j′

= j,

∀

Vj <

¯tj −

¯tj′, (KD + Kj′) (¯tj −

¯tj′)

+ Vj′,

j′

= j,

(547)

(cid:17)

∀

1
2

(cid:16)

Vj =

1
2  

tD, KD tD
(cid:16)

(cid:17)

(cid:16)

+

tj, Kj tj

−

(cid:17)

(cid:16)

¯tj, (KD + Kj) ¯tj

(548)

.
!

(cid:17)

That means single components become solutions at zero temperature 1/β
in case their (generalized) ‘template variance’ Vj, measuring the discrepancy
between data and prior term, is not too large. Eq. (535) for h can also be
expressed by the (potential) low temperature solutions ¯tj

h =

aj(KD + Kj)

aj (KD + Kj) ¯tj.

(549)

m





Xj

1

−

m





Xj

Summarizing, in the high temperature limit the stationarity equation
(532) becomes linear with a single solution being essentially a (generalized)
average of all template functions.
In the low temperature limit the sin-
gle component solutions become stable provided their (generalized) variance
corresponding to their minimal error is small enough.

126

6
6
6
6
6
6.3.2 Equal covariances

Especially interesting is the case of j–independent Kj(θ) = K0(θ) and θ–
independent det K0(θ). In that case the often diﬃcult to obtain determinants
of Kj do not have to be calculated.

For j–independent covariances the high temperature solution is according
to Eqs.(539,545) a linear combination of the (potential) low temperature
solutions

m

¯t =

a0
j

¯tj.

(550)

Xj
It is worth to emphasize that, as the solution ¯t is not a mixture of the
component templates tj but of component solutions ¯tj, even poor choices
for the template functions tj can lead to good solutions, if enough data are
available. That is indeed the reason why the most common choice t0 ≡
0 for
a Gaussian prior can be successful.

Eqs.(549) simpliﬁes to

m
j

¯tje−
m
j e−

βEh,j(h)

βEh,j (h)

−

Eθ,β,j+cj

−
Eθ,β,j+cj

=

m

Xj

m

Xj

aj¯tj = ¯t +

(aj −

j ) ¯tj,
a0

(551)

h =

P

P

where

¯tj = (KD + K0)−

1 (KDtD + K0tj) ,

and (for j–independent d)

aj =

Ej

e−
k e−

Ek

=

e−
k e−

βEh,j−

Eθ,β,j

βEh,k−

Eθ,β,k

=

β
2 aBj a+dj

,

β
2 aBka+dk

e−
k e−

introducing vector a with components aj, m

P

P

P

m matrices

Bj(k, l) =

¯tk −

(cid:16)

×
¯tj, (KD + K0) (¯tl −

¯tj)

(cid:17)

and constants

(552)

(553)

(554)

dj =

βVj −
−
with Vj given in (548). Eq. (551) is still a nonlinear equation for h, it
shows however that the solutions must be convex combinations of the h–
independent ¯tj. Thus, it is suﬃcient to solve Eq. (553) for m mixture coeﬃ-
cients aj instead of Eq. (532) for the function h.

Eθ,β,j,

(555)

127

The high temperature relation Eq. (537) becomes

aj

β
0
→
−→

a0
j =

Eθ,β,j

e−
m
k e−

,

Eθ,β,k

(556)

P
j = 1/m for a hyperprior p(θ, β, j) uniform with respect to j. The low

or a0
temperature relation Eq. (543) remains unchanged.

For m = 2 Eq. (551) becomes

h =

aj ¯tj =

2

Xj

¯t1 + ¯t2
2

+ (a1 −

a2)

¯t2

¯t1 −
2

=

¯t1 + ¯t2
2

+ (tanh ∆)

, (557)

¯t2

¯t1 −
2

with (¯t1 + ¯t2)/2 = ¯t in case Eθ,β,j is uniform in j so that a0

j = 0.5, and

∆ =

=

= β

E1

E2 −
2
β
a(B1 −
4

−

Eh,1

Eh,2 −
2
d1 −
2

d2

+

Eθ,β,1

Eθ,β,2 −
2
β
b(2a1 −
4

=

B2)a +

1) +

d2

d1 −
2

,

(558)

because the matrices Bj are in this case zero except B1(2, 2) = B2(1, 1) = b.
The stationarity Eq. (553) can be solved graphically (see Figs.7, 8), the
a1)2+d1,
solution being given by the point where a1e−
or, alternatively,

1+d2 = (1

a1)e−

β
2 b(1

2 ba2

−

−

β

a1 =

(tanh ∆ + 1) .

1
2

(559)

That equation is analogous to the celebrated mean ﬁeld equation of the
ferromagnet.

We conclude that in the case of equal component covariances, in addition
1–dimensional nonlinear
1.

to the linear low–temperature equations, only a m
equation has to be solved to determine the ‘mixing coeﬃcients’ a1,

, am

−

· · ·

−

6.3.3 Analytical solution of mixture models

For regression under a Gaussian mixture model the predictive density can be
calculated analytically for ﬁxed θ. This is done by expressing the predictive
density in terms of the likelihood of θ and j, marginalized over h

p(y

x, D, D0) =

dh dθ

|

Xj Z

xD, D0, θ, j)

p(θ, j) p(yD|
dθ p(θ, j)p(yD|

j

xD, D0, θ, j)

|

p(y

x, D, D0, θ, j).

(560)

P

R

128

0.2 0.4 0.6 0.8 1 a1

0.2 0.4 0.6 0.8 1 a1

0.2 0.4 0.6 0.8 1 a1

0.2 0.4 0.6 0.8 1 a1

0.2 0.4 0.6 0.8 1 a1

0.2 0.4 0.6 0.8 1 a1

β

−

−

2 ba2

β
2 b(1

a1)e−

1+d2 = (1

Figure 7: The solution of stationary equation Eq. (553) is given by the point
a1)2+d1 (upper row) or, equivalently, a1 =
where a1e−
1
2 (tanh ∆ + 1) (lower row). Shown are, from left to right, a situation at high
temperature and one stable solution (β = 2), at a temperature (β = 2.75)
near the bifurcation, and at low temperature with two stable and one unstable
solutions β = 4. The values of b = 2, d1 =
0.3025β used
for the plots correspond for example to the one–dimensional model of Fig.9
1, tD = 0.1. Notice, however, that the shown relation is
with t1 = 1, t2 =
−
valid for m = 2 at arbitrary dimension.

0.2025β and d2 =

−

−

(Here we concentrate on θ. The parameter β can be treated analogously.)
According to Eq. (487) the likelihood can be written

with

xD, D0, θ, j) = e−

β

E0,j(θ)+ 1

2 ln det( β

2π

Kj(θ)),

p(yD|

E0,j(θ) =

1
(tD −
2
1
1
j,DD(θ))−
D + K−

e

tj(θ),

Kj(θ)(tD −

e
tj(θ))) = Vj,

(561)

(562)

e

f

1 being a ˜n
f

Kj(θ) = (K−
˜n–matrix in data space. The
and
1Kj = KD −
equality of Vj and
Kj(KD + Kj)−
1
KD(KD + Kj,DD)−
K. For
j,DD)Kj,DD =
e
the predictive mean, being the optimal solution under squared–error loss
and log–loss (restricted to Gaussian densities with ﬁxed variance) we ﬁnd
therefore

×
E0,j can be seen using Kj −
1KD = Kj,DD −

Kj,DD(KD + K−

f

¯y(x) =

dy y p(y

x, D, D0) =

dθ bj(θ) ¯tj(θ),

(563)

with, according to Eq. (318),

Z

|

Xj Z

¯tj(θ) = tj + K−
j

1

Kj(tD −

tj),

(564)

f

129

1

0.75

0.5

0.25

0

0
0

4

3

2

beta

0.25
0.25

0.5
0.5

a1
a1

0.75
0.75

1

0

1
1

Figure 8: As in Fig.7 the plots of f1(a1) = a1 and f2(a1) = 1
are shown within the inverse temperature range 0

4.

β

2 (tanh ∆ + 1)

≤

≤

and mixture coeﬃcients

bj(θ) = p(θ, j

D) =

β

Ej (θ)

e−

−

Eθ,j+ 1
P

2 ln det(
R

∝

|

e

j

p(θ, j)p(yD|
dθp(θ, j)p(yD|
Kj(θ)),

xD, D0, θ, j)

xD, D0, θ, j)

(565)

E0,j + Eθ,j. For solvable θ–integral the coeﬃcients can

e

which deﬁnes
therefore be obtained exactly.
e

Ej = β

e

If bj is calculated in saddle point approximation at θ = θ∗ it has the
structure of aj in (536) with E0,j replaced by
Kj. (The inverse
temperature β could be treated analogously to θ. In that case Eθ,j would
have to be replaced by Eθ,β,j.)

Ej and Kj by

f

e

Calculating also the likelihood for j, θ in Eq. (565) in saddle point ap-
D0, θ∗, j), the terms
p(yD|
xD, h∗) in numerator and denominator cancel, so that, skipping D0 and

xD, D0, θ∗, j)

xD, h∗)p(h∗

≈

|

proximation, i.e., p(yD|
p(yD|
β,

bj(θ∗) =

p(h∗

j, θ∗)p(j, θ∗)
|
p(h∗, θ∗)

= aj(h∗, θ∗),

(566)

becomes equal to the aj(θ∗) in Eq. (536) at h = h∗.

Eq. (565) yields as stationarity equation for θ, similarly to Eq. (489)

0 =

bj

 

∂
Ej
∂θ −
e

Xj

Tr

1

K−
j

 

f

∂
Kj
∂θ !!
f

130

(567)

p

2

p

1

h

0

-1

2
2

4
4

beta
beta

6
6

8
8

-2

10

2
2

4
4

beta
beta

6
6

-1

8
8

-2

10

2

1

h

0

h, β)p(h
|

Figure 9: Shown is the joint posterior density of h and β, i.e., p(h, β

D, D0)
β, D0)p(β) for a zero–dimensional example of a Gaussian
p(yD|
∝
prior mixture model with training data yD = 0.1 and prior data yD0 =
1 and inverse temperature β. L.h.s.: For uniform prior (middle) p(β)

±
∝
1)2 + e−
1 with joint posterior p
e−
the maximum
(Here no factor 1/2 appears in front of ln β because
appears at ﬁnite β.
(cid:16)
normalization constants for prior and likelihood term have to be included.)
1)2 +
R.h.s.: For compensating hyperprior p(β)
2 (h+1)2 the maximum is at β = 0.
e−

1/√β with p

β
2 h2+ln β

β
2 (h+1)2

β
2 h2

β
2 (h

β
2 (h

2 h2

e−

e−

∝

∝

∝

(cid:17)

−

−

−

−

|

β

β

=

bj

   

∂tj(θ)
∂θ

Xj
1
2  

+

(tD −

∂

K−

1
j (θ)

Tr

−

 

f

,

Kj(θ)(tj(θ)

tD)

−

!

f
∂
tj(θ)),

Kj(θ)
∂θ
f
Kj(θ)
∂θ ! −
f

(tD −
1
p(θ, j)

tj(θ))

!

∂p(θ, j)

.

∂θ !

For ﬁxed θ and j–independent covariances the high temperature solution

is a mixture of component solutions weighted by their prior probability

¯y

β
0
→
−→

Xj

p(j) ¯tj =

a0
j

¯tj = ¯t.

Xj

The low temperature solution becomes the component solution ¯tj with min-

(568)

(569)

131

p

2

p

1

h

0

-1

2
2

4
4

beta
beta

6
6

8
8

-2

10

2
2

4
4

beta
beta

6
6

-1

8
8

-2

10

2

1

h

0

Figure 10: Same zero–dimensional prior mixture model for uniform hyper-
prior on β as in Fig.9, but for varying data xd = 0.3 (left), xd = 0.5 (right).

imal distance between data and prior template
¯tj∗ with j∗ = argminj(tD −

β
→∞
−→

¯y

tj,

Kj(tD −

tj)).

(570)

Fig.11 compares the exact mixture coeﬃcient b1 with the dominant solution
of the maximum posterior coeﬃcient a1 (see also [123]) which are related
according to (553)

f

aj =

e−

β
2 aBj a

Ej

−

k e−

β
2 aBka
e
−

Ek

=

bj e−
k bk e−

β
2 aBj a

.

β
2 aBka

(571)

P
6.4 Local mixtures

e

P

Global mixture components can be obtained by combining local mixture
components. Predicting a time series, for example, one may allow to switch
locally (in time) between two or more possible regimes, each corresponding
to a diﬀerent local covariance or template.

The problem which arises when combining local alternatives is the fact
that the total number of mixture components grows exponentially in the
number local components which have to be combined for a global mixture
component.

Consider a local prior mixture model, similar to Eq. (516),

p(φ

θ) = e−

dx;

ω(x;θ(x))
|

|

2

−

ln Zφ(θ)

|

(572)

R

132

2

4

6

8

10

12

14

0.9

0.8

0.7

0.6

0.5

Figure 11: Exact b1 and a1 (dashed) vs. β for two mixture components with
equal covariances and B1(2, 2) = b = 2,

E1 = 0.405,

E2 = 0.605.

e

e

where θ(x) may be a binary or an integer variable. The local mixture variable
θ(x) labels local alternatives for ﬁltered diﬀerences ω(x; θ(x)) which may
diﬀer in their templates t(x; θ(x)) and/or their local ﬁlters W(x; θ(x)). To
avoid inﬁnite products, we choose a discretized x variable (which may include
the y variable for general density estimation problems), so that

p(φ) =

p(θ)e−

ω(x;θ(x))
|

x |

ln Zφ(θ),

−

2

(573)

Xθ

P

where the sum

θ is over all local integer variables θ(x), i.e.,

P

=

Xθ

Xθ(x1) · · ·

Xθ(xl)

=

x
Y

Xθ(x1)

.





(574)

Only for factorizing hyperprior p(θ) =

x p(θ(x)) the complete posterior





Q

p(φ) =

p(θ(x))e−|

ω(x;θ(x))
|

2

−

ln Zφ(x,θ(x))





Yx′

Xθ(x′)



x (cid:16)
Y

p(θ(x))e−|

=

x
Y

Xθ(x) (cid:16)

ω(x;θ(x))
|

2

−

ln Zφ(x,θ(x))

(575)

(cid:17)

,

(cid:17)

Zφ =

e−|

ω(x;θ(x))
|

2

=

Zφ(x, θ(x)).

(576)

x
Y

Xθ(x) (cid:16)

(cid:17)

x
Y

factorizes

because

133

Under that condition the mixture coeﬃcients aθ of Eq. (536) can be ob-

tained from the equations, local in θ(x),

with

aθ = aθ(x1)

θ(xl) = p(θ

φ) =

aθ(x)

···

|

x
Y

aθ(x) =

p(θ(x))e−|
θ′(x) p(θ′(x))e−|

ω(x;θ(x))
|
−
ω(x;θ′(x))
|

2

2

−

ln Zφ(x;θ(x))

ln Zφ(x;θ′(x)) .

(577)

(578)

For equal covariances this is a nonlinear equation within a space of dimension
equal to the number of local components. For non–factorizing hyperprior the
equations for diﬀerent θ(x) cannot be decoupled.

P

6.5 Non–quadratic potentials

Solving learning problems numerically by discretizing the x and y variables
allows in principle to deal with arbitrary non–Gaussian priors. Compared to
Gaussian priors, however, the resulting stationarity equations are intrinsically
nonlinear.

As a typical example let us formulate a prior in terms of nonlinear and
non–quadratic “potential” functions ψ acting on “ﬁltered diﬀerences” ω =
t), deﬁned with respect to some positive (semi–)deﬁnite inverse co-
W(φ
variance K = WT W. In particular, consider a prior factor of the following
form

−

p(φ) = e−

dx ψ(ω(x))

ln Zφ =

−

(579)

E(φ)

e−

,

Zφ

R

where E(φ) =
dx ψ(ω(x)). For general density estimation problems we
understand x to stand for a pair (x, y). Such priors are for example used for
image restoration [65, 25, 155, 66, 231, 230].

R

For diﬀerentiable ψ function the functional derivative with respect to φ(x)

becomes

δφ(x)p(φ) =

e−

dx′ ψ(ω(x′))

ln Zφ

−

dx′′ ψ′(ω(x′′))W(x′′, x),

(580)

−

R

Z

with ψ′(s) = dψ(z)/dz, from which follows

δφE(φ) =

δφ ln p(φ) = WT ψ′.

−

(581)

134

t, W in Eq. (579) must be replaced by
For nonlinear ﬁlters acting on φ
ω′(x) = δφ(x)ω(x). Instead of one W a “ﬁlter bank” Wα with corresponding
Kα, ωα, and ψα may be used, so that

−

and

e−

α

dx ψα(ωα(x))

ln Zφ,

−

P

R

δφE(φ) =

WT

α ψ′α.

α
X
The potential functions ψ may be ﬁxed in advance for a given problem.
Typical choices to allow discontinuities are symmetric “cup” functions with
minimum at zero and ﬂat tails for which one large step is cheaper than many
small ones [221]). Examples are shown in Fig. 12 (a,b). The cusp in (b),
where the derivative does not exist, requires special treatment [230]. Such
functions can also be interpreted in the sense of robust statistics as ﬂat tails
reduce the sensitivity with respect to outliers [93, 94, 62, 23].

Inverted “cup” functions, like those shown in Fig. 12 (c), have been ob-
tained by optimizing a set of ψα with respect to a sample of natural images
[230]. (For statistics of natural images their relation to wavelet–like ﬁlters
and sparse coding see also [162, 163].)

While, for W which are diﬀerential operators, cup functions promote
smoothness, inverse cup functions can be used to implement structure. For
such W the gradient algorithm for minimizing E(φ),

becomes in the continuum limit a nonlinear parabolic partial diﬀerential
equation,

φnew = φold −

ηδφE(φold),

φτ =

−

α
X

WT

α ψ′α(Wα(φ

t)).

−

Here a formal time variable τ have been introduced so that (φnew −
→
φτ = dφ/dτ . For cup functions this equation is of diﬀusion type [160, 173], if
also inverted cup functions are included the equation is of reaction–diﬀusion
type [230]. Such equations are known to generate a great variety of patterns.
Alternatively to ﬁxing ψ in advance or, which is sometimes possible for
low–dimensional discrete function spaces like images, to approximate ψ by
sampling from the prior distribution, one may also introduce hyperparame-
ters and adapt potentials ψ(θ) to the data.

φold)/η

(582)

(583)

(584)

(585)

135

1

0.8

0.6

0.4

0.2

2.5

0

3

2

1

0.5

0

0

-0.5

-1

-1.5

-2

-2.5

(a)

(c)

(b)

1.5

-15

-10

-5

0

5

10

15

-15

-10

-5

0

5

10

15

-15

-10

-5

0

5

10

15

Figure 12: Non–quadratic potentials of the form ψ(x) = a(1.0
−
/b)γ)), [230]: “Diﬀusion terms”: (a) Winkler’s cup function [221] (a= 5,
x0
b = 10, γ = 0.7, x0 = 0), (b) with cusp (a= 1, b = 3, γ = 2, x0 = 0), (c)
4.8, b = 15, γ = 2.0 x0 = 0).
“Reaction term” (a =

1/(1 + (
|

−

x

|

−

136

|

−

−

For example, attempting to adapt a unrestricted function ψ(x) with hy-
perprior p(ψ) by Maximum A Posteriori Approximation one has to solve the
stationarity condition

0 = δψ(s) ln p(φ, ψ) = δψ(s) ln p(φ

ψ) + δψ(s) ln p(ψ).

(586)

From

it follows

with integer

δψ(s)p(φ

ψ) =

p(φ

ψ)

dx δ (s

ω(x))

δψ(s)Zφ,

(587)

|

−

|

Z

1
Z 2
φ

−

δψ(s) ln p(φ

ψ) = n(s)

< n(s) >,

−

|

Z

n(s) =

dx δ (s

ω(x)) ,

−

(588)

(589)

being the histogram of the ﬁltered diﬀerences, and average histogram

< n(s) > =

dφ p(φ

ψ) n(s).

(590)

Z

|

The right hand side of Eq. (588) is zero at φ∗ if, e.g., p(φ
which is the case for ψ(ω(x; φ)) = β (ω(x; φ)

φ∗),
−
limit.
Introducing hyperparameters one has to keep in mind that the resulting
additional ﬂexibility must be balanced by the number of training data and
the hyperprior to be useful in practice.

ω(x; φ∗))2 in the β

ψ) = δ(φ

→ ∞

−

|

7 Iteration procedures: Learning

7.1 Numerical solution of stationarity equations

−

(ln P, N) and the normal-
Due to the presence of the logarithmic data term
ization constraint in density estimation problems the stationary equations
are in general nonlinear, even for Gaussian speciﬁc priors. An exception are
(ln P, N)
Gaussian regression problems discussed in Section 3.7 for which
becomes quadratic and the normalization constraint can be skipped. How-
(ln P, N) are restricted
ever, the nonlinearities arising from the data term
to a ﬁnite number of training data points and for Gaussian speciﬁc priors one
may expect them, like those arising from the normalization constraint, to be
numerically not very harmful. Clearly, severe nonlinearities can appear for

−

−

137

general non–Gaussian speciﬁc priors or general nonlinear parameterizations
P (ξ).

As nonlinear equations the stationarity conditions have in general to be
solved by iteration. In the context of empirical learning iteration procedures
to minimize an error functional represent possible learning algorithms.
In the previous sections we have encountered stationarity equations

for error functionals Eφ, e.g., φ = L or φ = P , written in a form

0 =

δ(

Eφ)

−
δφ

= G(φ),

Kφ = T.

with φ–dependent T (and possibly K). For the stationarity Eqs. (144), (172),
and (193) the operator K is a φ–independent inverse covariance of a Gaussian
speciﬁc prior. It has already been mentioned that for existing (and not too
1 (representing the covariance of the prior process) Eq.
ill–conditioned) K−
(592) suggests an iteration scheme

φ(i+1) = K−

1T (φ(i)),

(593)

for discretized φ starting from some initial guess φ(0). In general, like for the
non–Gaussian speciﬁc priors discussed in Section 6, K can be φ–dependent.
Eq. (359) shows that general nonlinear parameterizations P (ξ) lead to non-
linear operators K.

Clearly, if allowing φ–dependent T , the form (592) is no restriction of
generality. One always can choose an arbitrary invertible (and not too ill–
conditioned) A, deﬁne

write a stationarity equation as

TA = G(φ) + Aφ,

Aφ = TA,

1. To obtain a numerical iteration scheme we
discretize and iterate with A−
will choose a linear, positive deﬁnite learning matrix A. The learning matrix
may depend on φ and may also change during iteration.

To connect a stationarity equation given in form (592) to an arbitrary

iteration scheme with a learning matrix A we deﬁne

(591)

(592)

(594)

(595)

(596)

B = K

A, Bη = K

−

1
η

A,

−

138

i.e., we split K into two parts

K = A + B =

A + Bη,

(597)

where we introduced η for later convenience. Then we obtain from the sta-
tionarity equation (592)

φ = ηA−

1(T

Bηφ).

(598)

To iterate we start by inserting an approximate solution φ(i) to the right–
hand side and obtain a new φ(i+1) by calculating the left hand side. This can
be written in one of the following equivalent forms

1
η

−

(599)

(600)

(601)

1

(602)

φ(i+1) = ηA−

1

T (i)

Bηφ(i)
1

−
(cid:16)
η)φ(i) + ηA−

= (1

−
= φ(i) + ηA−

1

Bφ(i)

(cid:17)

(cid:17)
T (i)

−
(cid:16)
Kφ(i)
−

,

(cid:17)

T (i)

(cid:16)

where η plays the role of a learning rate or step width, and A−
may be iteration dependent. The update equations (599–601) can be written

(cid:16)

(cid:17)

−

1 =

A(i)

∆φ(i) = ηA−

1G(φ(i)),

with ∆φ(i) = φ(i+1)
−
or Bη so that instead of A directly A−
calculate its inverse. For example operators approximating K−
easy to calculate may be good choices for A−

φ(i). Eq. (601) does not require the calculation of B
1 can be given without the need to
1 and being

1.

For positive deﬁnite A (and thus also positive deﬁnite inverse) conver-
Indeed, multiplying with

gence can be guaranteed, at least theoretically.
(1/η)A and projecting onto an inﬁnitesimal dφ gives

1
η

( dφ, A ∆φ ) =

dφ,

δ(

Eφ)

−
δφ

(cid:16)

= d(

Eφ).

−

(603)

φ=φ(i) (cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

In an inﬁnitesimal neighborhood of φ(i) where ∆φ(i) becomes equal to dφ
in ﬁrst order the left–hand side is for positive (semi) deﬁnite A larger (or
equal) to zero. This shows that at least for η small enough the posterior
Eφ increases i.e., the diﬀerential dEφ is smaller or equal to
log–probability
zero and the value of the error functional Eφ decreases.

−

139

Stationarity equation (128) for minimizing EL yields for (599,600,601),

L(i+1) = ηA−

1

N

Λ(i)

X eL(i)

−
 
η)L(i) + ηA−

1

−

!

KL(i) +

AL(i)

1
η
X eL(i)
−
KL(i)

Λ(i)

−

.

(cid:17)

N
−
(cid:16)
X eL(i)
Λ(i)

N
(cid:16)

−

= (1

−
= L(i) + ηA−

1

KL(i) + AL(i)

(605)

(604)

(cid:17)

(606)

The function Λ(i)
X is also unknown and is part of the variables we want to solve
for. The normalization conditions provide the necessary additional equations,
1 can be extended to include the iteration procedure for
and the matrix A−
ΛX. For example, we can insert the stationarity equation for ΛX in (606) to
get

L(i+1) = L(i) + ηA−

1

N

eL(i)

(NX −

−

IXKL)

KL(i)

.

−

i

h

If normalizing L(i) at each iteration this corresponds to an iteration procedure
for g = L + ln ZX.

Similarly, for the functional EP we have to solve (166) and obtain for

(607)

(601),

P (i+1) = P (i) + ηA−

1

KP (i)

= P (i) + ηA−

1

= P (i) + ηA−

1

T (i)
P −
1
P(i)−
P(i)−

1

N

N

(cid:16)

(cid:16)

(cid:16)

(608)

(609)

KP (i)

(cid:17)
Λ(i)
X −
NX −

−

−

(cid:17)
IXP(i)KP (i)

KP (i)

.(610)

−

(cid:17)

Again, normalizing P at each iteration this is equivalent to solving for z =
ZXP , and the update procedure for ΛX can be varied.

7.2 Learning matrices

7.2.1 Learning algorithms for density estimation

There exists a variety of well developed numerical methods for unconstraint
as well as for constraint optimization [175, 52, 81, 181, 82, 9, 17, 75, 178].
Popular examples include conjugate gradient, Newton, and quasi–Newton
methods, like the variable metric methods DFP (Davidon–Fletcher–Powell)
or BFGS (Broyden–Fletcher–Goldfarb–Shanno).

All of them correspond to the choice of speciﬁc, often iteration dependent,
learning matrices A deﬁning the learning algorithm. Possible simple choices

140

are:

A = I : Gradient
A = D : Jacobi
A = L + D : Gauss–Seidel

A = K : prior relaxation

(611)
(612)
(613)
(614)

where I stands for the identity operator, D for a diagonal matrix, e.g. the
diagonal part of K, and L for a lower triangular matrix, e.g. the lower tri-
angular part of K. In case K represents the operator of the prior term in
1 (corresponding to the co-
an error functional we will call iteration with K−
variance of the prior process) prior relaxation. For φ–independent K and T ,
η = 1 with invertible K the corresponding linear equation is solved by prior
relaxation in one step. However, also linear equations are solved by iteration
1 = I the gradient
if the size of K is too large to be inverted. Because of I−
algorithm does not require inversion.

On one hand, density estimation is a rather general problem requiring
the solution of constraint, inhomogeneous, nonlinear (integro–)diﬀerential
equations. On the other hand, density estimation problems are, at least
for Gaussian speciﬁc priors and non restricting parameterization, typically
“nearly” linear and have only a relatively simple positivity and normalization
constraint. Furthermore, the inhomogeneities are commonly restricted to a
ﬁnite number of discrete training data points. Thus, we expect the inversion
of K to be the essential part of the solution for density estimation problems.
However, K is not necessarily invertible or may be diﬃcult to calculate.
Also, inversion of K is not exactly what is optimal and there are improved
methods. Thus, we will discuss in the following basic optimization methods
adapted especially to the situation of density estimation.

7.2.2 Linearization and Newton algorithm

For linear equations Kφ = T where T and K are no functions of φ a spectral
radius ρ(M) < 1 (the largest modulus of the eigenvalues) of the iteration
matrix

M =

ηA−

1Bη = (1

η)I

ηA−

1B = I

ηA−

1K

(615)

−

−

−

−

would guarantee convergence of the iteration scheme. This is easily seen by
solving the linear equation by iteration according to (599)

φ(i+1) = ηA−

1T + Mφ(i)

(616)

141

= ηA−

1T + ηMA−

1T + M2φ(i
−

1)

= η

MnA−

1T.

∞

Xn=0

(617)

(618)

A zero mode of K, for example a constant function for diﬀerential operators
without boundary conditions, corresponds to an eigenvalue 1 of M and would
lead to divergence of the sequence φ(i). However, a nonlinear T (φ) or K(φ),
like the nonlinear normalization constraint contained in T (φ), can then still
lead to a unique solution.

A convergence analysis for nonlinear equations can be done in a linear

approximation around a ﬁxed point. Expanding the gradient at φ∗

δ(

Eφ)

−
δφ

(cid:12)
φ∗
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G(φ) =

+ (φ

φ∗) H(φ∗) +

(619)

−

· · ·

shows that the factor of the linear term is the Hessian. Thus in the vicinity
of φ∗ the spectral radius of the iteration matrix

M = I + ηA−

1H,

(620)

determines the rate of convergence. The Newton algorithm uses the negative
Hessian
H as learning matrix provided it exists and is positive deﬁnite.
Otherwise it must resort to other methods. In the linear approximation (i.e.,
for quadratic energy) the Newton algorithm

−

A =

H : Newton

−

(621)

is optimal. We have already seen in Sections 3.1.3 and 3.2.3 that the inho-
mogeneities generate in the Hessian in addition to K a diagonal part which
can remove zero modes of K.

7.2.3 Massive relaxation

We now consider methods to construct a positive deﬁnite or at least invertible
learning matrix. For example, far from a minimum the Hessian H may not
be positive deﬁnite and like a diﬀerential operator K with zero modes, not
even invertible. Massive relaxation can transform a non–invertible or not
H, into an invertible
positive deﬁnite operator A0, e.g. A0 = K or A0 =
or positive deﬁnite operators:

−

A = A0 + m2I : Massive relaxation

(622)

142

−

−

zI)−

A generalization would be to allow m = m(x, y). This is, for example, used
in some realizations of Newton‘s method for minimization in regions where
H is not positive deﬁnite and a diagonal operator is added to
H, using for
example a modiﬁed Cholesky factorization [17]. The mass term removes the
m2 is not in the spectrum of A0 and makes it positive
zero modes of K if
deﬁnite if m2 is larger than the smallest eigenvalue of A0. Matrix elements
m2 representing in this
( φ, (A0 −
case a complex variable, have poles at discrete eigenvalues of A0 and a cut
at the continuous spectrum as long as φ has a non–zero overlap with the
corresponding eigenfunctions. Instead of multiples of the identity, also other
operators may be added to remove zero modes. The Hessian HL in (157), for
example, adds a x–dependent mass–like, but not necessarily positive deﬁnite
term to K. Similarly, for example HP in (182) has (x, y)–dependent mass
P−

1 φ ) of the resolvent A−

2N restricted to data points.
While full relaxation is the massless limit m2

0 of massive relaxation,

1(z), z =

−

a gradient algorithm with η′ can be obtained as inﬁnite mass limit m2
with η

and m2/η = 1/η′.

→ ∞

→

Constant functions are typical zero modes, i.e., eigenfunctions with zero
eigenvalue, for diﬀerential operators with periodic boundary conditions. For
instance for a common smoothness term
∆ (kinetic energy operator) as
regularizing operator K the inverse of A = K + m2I has the form

−

→ ∞

A−

1(x′, y′; x, y) =

ddXkx ddYky
(2π)d

∞

=

Z

−∞

1
∆ + m2 .
−
eikx(x
−
x + k2
k2

−
y + m2

x′)+iky(y

y′)

,

(623)

(624)

with d = dX + dY , dX = dim(X), dY = dim(Y ). This Green‘s function or
matrix element of the resolvent kernel for A0 is analogous to the (Euclidean)
propagator of a free scalar ﬁeld with mass m, which is its two–point corre-
lation function or matrix element of the covariance operator. According to
xt the denominator can be brought into the exponent by intro-
1/x =
ducing an additional integral. Performing the resulting Gaussian integration
over k = (kx, ky) the inverse can be expressed as

∞0 dt e−
R

A−

1(x′, y′; x, y; m) = md
−

2A−

1(m(x

x′), m(y

y′); 1)

−

−

= (2π)−

d/2

m
+

 

x

|

x′|

−

y

|

!

y′|

−

(d

2)/2

−

143

K(d

2)/2(m
|

x

−

x′

y

+ m
|

|

−

−

|

y′

), (625)

in terms of the modiﬁed Bessel functions Kν(x) which have the following
integral representation

Kν(2

βγ) =

q

ν
2

1
2  

γ
β !

0
Z

∞

dt tν

1e

−

β
t −

γt.

(626)

Alternatively, the same result can be obtained by switching to d–dimensional
spherical coordinates, expanding the exponential in ultra-spheric harmonic
functions and performing the integration over the angle-variables [109]. For
the example d = 1 this corresponds to Parzens kernel used in density esti-
mation or for d = 3

A−

1(x′, y′; x, y) =

e−

m
|

x

−

x′

y

m
|

−

|−

y′

|.

(627)

|
The Green’s function for periodic, Neumann, or Dirichlet boundary con-

−

−

|

4π

x

x′|

1
+ 4π

y

y′|

ditions can be expressed by sums over A−

1(x′, y′; x, y) [72].

The lattice version of the Laplacian with lattice spacing a reads

ˆ∆f (n) =

1
a2

d

Xj

[f (n

aj)

2f (n) + f (n + aj)],

(628)

−

−

writing aj for a vector in direction j and length a. Including a mass term we
get as lattice approximation for A

ˆA(nx, ny; mx, my) =

δny,my (δnx+ax

i ,mx −

2δnx,mx + δnx

i ,mx)
ax

−

1
a2

−

dX

Xi=1

1
a2

−

dY

Xj=1

δnx,mx(δny+ay

j ,my −

2δny,my + δny

j ,my ) + m2δnx,mxδny,my
ay

(629)

−

Inserting the Fourier representation (102) of δ(x) gives

ˆA(nx, ny; mx, my) =

2d
a2

π

ddXkx ddYky
(2π)d
π

eikx(nx

−

mx)+iky(ny

my)

−

1 +

m2a2

2d −

× 


cos kx,i −

1
d

dY

Xj=1

,

cos ky,j


(630)

Z

−

1
d

dX

Xi=1

144

with kx,i = kxax

i , cos ky,j = cos kyay

j and inverse

ˆA−

1(nx, ny; mx, my) =

ˆA−

1(kx, ky)eikx(nx

−

mx)+iky(ny

my)

−

π

ddXkx ddYky
(2π)d
π

Z

−

π

π

−

−

=

mx)+iky(ny

1+ m2a2

eikx(nx
dX
1
d

ddXkx ddYky
(2π)d

a2
2d Z
(For m = 0 and d
P
≤
Subtracting formally the also inﬁnite ˆA−
For example in d = 1 one ﬁnds ˆA−
[96]. Using 1/x =

i=1 cos kx,i−
2 the integrand diverges for k
→

−
xt one obtains [180]

1(ny; my)

2d −

ˆA−

−
1
d

∞0 dt e−

0 (infrared divergence).
P
1(0, 0; 0, 0) results in ﬁnite diﬀerence.
my|

(1/2)
|

1(0; 0) =

ny −

−

my)

.

(631)

dY
j=1 cos ky,j

R
1(kx, ky) =

ˆA−

∞

dt e−

1
2 Z

0

µt+a−2t

dX
i

cos kx,i+

cos ky,j

dY
j

(cid:16)P

P

,

(cid:17)

(632)

with µ = d/a2 + m2/2. This allows to express the inverse ˆA−
1 in terms of
the modiﬁed Bessel functions Iν(n) which have for integer argument n the
integral representation

Iν(n) =

dΘ en cos Θ cos(νΘ).

(633)

π

1
π Z

0

One ﬁnds

1
2 Z

0

dX

Yi=1

ˆA−

1(nx, ny; mx, my) =

∞

µt

e−

K

nx,i

|

n′

−

x,i|

(t/a2)

K

|

my,j

m′

−

y,j|

(t/a2).

(634)
It might be interesting to remark that the matrix elements of the inverse
learning matrix or free massive propagator on the lattice ˆA−
1(x′, y′; x, y) can
be given an interpretation in terms of (random) walks connecting the two
points (x′, y′) and (x, y) [51, 180]. For that purpose the lattice Laplacian is
splitted into a diagonal and a nearest neighbor part

dY

Yj=1

1
a2 (2dI
where the nearest neighbor matrix W has matrix elements equal one for
nearest neighbors and equal to zero otherwise. Thus,

ˆ∆ =

(635)

W) ,

−

−

ˆ∆ + m2

1

−

=

−
(cid:16)

(cid:17)

1
2µ  

I

−

1
2µa2 W

!

1

−

=

1
2µ

n

1
2µa2 !

∞

Xn=0  

Wn,

(636)

145

can be written as geometric series. The matrix elements Wn(x′, y′; x, y) give
the number of walks w[(x′, y′)
= n connecting the two
points (x′, y′) and (x, y). Thus, one can write

(x, y)] of length

→

w

|

|

ˆ∆ + m2

1

−

(x′, y′; x, y) =

−

(cid:16)

(cid:17)

1
2µ

Xw[(x′,y′)
→

(x,y)]  

w

|

|

.

1
2µa2 !

(637)

7.2.4 Gaussian relaxation

As Gaussian kernels are often used in density estimation and also in function
approximation (e.g. for radial basis functions [176]) we consider the example

k

1
k!  

M2
2˜σ2 !

M2
2˜σ2

= e

A =

∞

Xk=0

: Gaussian

(638)

with positive semi–deﬁnite M2. The contribution for k = 0 corresponds to
a mass term so for positive semi–deﬁnite M this A is positive deﬁnite and
therefore invertible with inverse

A−

1 = e−

M2
2˜σ2 ,

(639)

(640)

which is diagonal and Gaussian in M–representation. In the limit ˜σ
for zero modes of M the Gaussian A−
to the gradient algorithm. Consider

or
1 becomes the identity I, corresponding

→ ∞

M2(x′, y′; x, y) =

δ(x

x′)δ(y

y′)∆

−

−

−

where the δ–functions are usually skipped from the notation, and

∂2
∂x2 +
denotes the Laplacian. The kernel of the inverse is diagonal in Fourier rep-
resentation

∂2
∂y2 ,

∆ =

A(k′x, k′y; , kx, ky) = δ(kx −

k′x)δ(ky −

k′y)e−

x+k2
k2
y
2˜σ2

(641)

and non–diagonal, but also Gaussian in (x, y)–representation

A−

1(x′, y′; x, y) = e−

2˜σ2 =

∆

k2
x+k2
y
2˜σ2 +ikx(x

−

x′)+iky(y

y′)

−

(642)

dkxdky
(2π)d e−

Z

146

d

=

˜σ
√2π !

 

e−

˜σ2((x

x′)2+(y

−

y′)2) =

−

1

(x−x′)2+(y−y′)2
2σ2

,

d e−

(643)

σ√2π
(cid:16)

(cid:17)

with σ = 1/˜σ and d = dX + dY , dX = dim(X), dY = dim(Y ).

7.2.5 Inverting in subspaces

Matrices considered as learning matrix have to be invertible. Non-invertible
matrices can only be inverted in the subspace which is the complement of
its zero space. With respect to a symmetric A we deﬁne the projector Q0 =
i ψT
i ψi into its zero space (for the more general case of a normal A
I
−
replace ψT
i by the hermitian conjugate ψ†i ) and its complement Q1 = I
Q0 =
P
−
i ψT
i ψi with ψi denoting orthogonal eigenvectors with eigenvalues ai 6
= 0 of
= 0. Then, denoting projected sub-matrices by QiAQj

A, i.e., Aψi = aiψi 6
P
= Aij we have A00 = A10 = A01 = 0, i.e.,

and in the update equation

only A11 can be inverted. Writing Qjφ = φj for a projected vector, the
iteration scheme acquires the form

A = Q1AQ1 = A11.

A∆φ(i) = η G

∆φ(i)

1 = ηA−
0 = η G0.

1
11 G1,

(644)

(645)

(646)
(647)

For positive semi–deﬁnite A the sub-matrix A11 is positive deﬁnite. If the
second equation is already fulﬁlled or its solution is postponed to a later
iteration step we have

φ(i+1)
1
φ(i+1)
0

= φ(i)
= φ(i)
0 .

1
1 + ηA−
11

T (i)
1 −

K(i)

11 φ(i)

1 −

K(i)

10 φ(i)

0

,

(cid:17)

(cid:16)

(648)

(649)

In case the projector Q0 = I0 is diagonal in the chosen representation the
projected equation can directly be solved by skipping the corresponding com-
ponents. Otherwise one can use the Moore–Penrose inverse A# of A to solve
the projected equation

∆φ(i) = ηA#G.

(650)

147

Alternatively, an invertible operator ˜A00 can be added to A11 to obtain a
1 = A−
complete iteration scheme with A−

11 + ˜A−

1
00

1

1
φ(i+1) = φ(i) + ηA−
11
(cid:16)
T (i)
0 −
1 = A−
1
11 + Q0, for instance,
results in a gradient algorithm on the zero space with additional coupling
between the two subspaces.

1 = (A11 + I00)−

1
11 + I00, = A−

The choice A−

K(i)
00 φ(i)

K(i)
01 φ(i)

T (i)
1 −
K(i)

1 −
K(i)

+η ˜A−
1
00

11 φ(i)

10 φ(i)

1 −

(651)

(cid:17)

(cid:16)

(cid:17)

.

0

0

7.2.6 Boundary conditions

For a diﬀerential operator invertability can be achieved by adding an operator
Y (boundary). More general, we consider an
restricted to a subset B
projector QB on a space which we will call boundary and the projector on
the interior QI = I
, and
require KBI = 0. That means K is not symmetric, but KII can be, and we
have

QB. We write QkKQl = Kkl for k, l

I, B

∈ {

X

−

⊂

×

}

K = (I

QB)K + QBKQB = KII + KIB + KBB.

(652)

−

For such an K an equation of the form Kφ = T can be decomposed into

(653)
(654)

(655)

KBBφB = TB,
KIBφB + KIIφI = TI,

with projected φk = Qkφ, Tk = QkT so that

1
BBTB,
φB = K−
1
φI = K−
II

TI −
The boundary part is independent of the interior, however, the interior can
depend on the boundary. A basis can be chosen so that the projector onto
the boundary is diagonal, i.e.,

KIBK−

(656)

(cid:17)

(cid:16)

.

1
BBTB

QB = IB =

(δ(xj)

δ(yj))

(δ(xj)

⊗

⊗

⊗

δ(yj))T .

Xj:(xj,yj)
∈

B

Eliminating the boundary results in an equation for the interior with adapted
inhomogeneity. The special case KBB = IBB, i.e., φB = TB on the boundary,
is known as Dirichlet boundary conditions.

148

Analogously, we may use a learning matrix A with boundary, correspond-

ing for example to a K with boundary conditions:

A = AII + AIB + ABB : Boundary
A = AII + AIB + IBB : Dirichlet boundary

(657)
(658)

For linear ABB the form (657) corresponds to general linear boundary condi-
tions. One can, however, also allow nonlinear boundary conditions. AII can
be chosen symmetric, and therefore positive deﬁnite, and the boundary of A
can be changed during iteration. Solving A(φ(i+1)
K(i)φ(i))
gives on the boundary and for the interior

φ(i)) = η(T (i)

−

−

(659)

(cid:17)
φ(i+1)
B −
(cid:16)

φ(i)
,
B
(660)
(cid:17)
B and K(i)
T (i)
BI = 0, or
φ(i)
B vanishes.

φ(i+1)
B = φi

1
B + ηA−
BB

T (i)
B −
(cid:16)
II φ(i)
I −

K(i)

K(i)

BBφ(i)

B −

K(i)

BIφ(i)

I

,

K(i)

IBφ(i)

B

A−

1
II AIB

−

φ(i+1)
I

= φi

1
I + ηA−
II

T (i)
I −

(cid:16)
For fulﬁlled boundary conditions with φ(i)
for ηA−
Otherwise, inserting the ﬁrst in the second equation gives
IBφ(i)

0 so the boundary is not updated, the term φ(i+1)

(cid:17)
K(i)
BB
(cid:16)

1
BB →

II φ(i)

= φi

B =

(cid:17)

−

1

ηA−

φ(i+1)
I

T (i)
1
I + ηA−
I −
II
1
1
II AIBA−
BB

K(i)
T (i)
B −
(cid:16)
Even if K is not deﬁned with boundary conditions, an invertible A can be
obtained from K by introducing a boundary for A. The updating process
can then for example be restricted to the interior and the boundary changed
during iteration.

B
(cid:17)
BIφ(i)
K(i)

K(i)
BBφ(i)

I −
K(i)

B −

(661)

−

(cid:16)

(cid:17)

.

I

B −

The following table summarizes the learning matrices we have discussed:

Learning algorithm Learning matrix
Gradient A = I
Jacobi A = D
Gauss–Seidel A = L + D
Newton A =

H

−
prior relaxation A = K

massive relaxation A = A0 + m2I

linear boundary A = AII + AIB + ABB
Dirichlet boundary A = AII + AIB + IBB

Gaussian A =

∞k=0

1
k!

M2
2˜σ2

k

M2
2˜σ2

= e

P

(cid:16)

(cid:17)

149

7.3

Initial conﬁgurations and kernel methods

7.3.1 Truncated equations

To solve the nonlinear Eq. (593) by iteration one has to begin with an ini-
tial conﬁguration φ(0).
In principle any easy to use technique for density
estimation could be chosen to construct starting guesses φ(0).

One possibility to obtain initial guesses is to neglect some terms of the full
stationarity equation and solve the resulting simpler (ideally linear) equation
ﬁrst. The corresponding solution may be taken as initial guess φ(0) for solving
the full equation.

Typical error functionals for statistical learning problems include a term
(L, N) consisting of a discrete sum over a ﬁnite number n of training data.
For diagonal P′ those contributions result (346) in n δ–peak contributions to
yi)
the inhomogeneities T of the stationarity equations, like
yi)/P (x, y) in Eq. (172). To ﬁnd an initial
xi)δ(y
in Eq. (144) or
guess, one can now keep only that δ–peak contributions Tδ arising from the
training data and ignore the other, typically continuous parts of T . For (144)
and (172) this means setting ΛX = 0 and yields a truncated equation

xi)δ(y

i δ(x

i δ(x

−

−

−

−

P

P

Kφ = P′P−

1N = Tδ.

Hence, φ can for diagonal P′ be written as a sum of n terms

φ(x, y) =

C(x, y; xi, yi)

P ′(xi, yi)
P (xi, yi)

,

n

Xi=1

1, provided the inverse K−

1 exists. For EL the resulting trun-
with C = K−
cated equation is linear in L. For EP , however, the truncated equations
remains nonlinear. Having solved the truncated equation we restore the
necessary constraints for φ, like normalization and positivity for P or nor-
malization of the exponential for L.

= K−

In general, a C

1 can be chosen. This is necessary if K is not
invertible and can also be useful if its inverse is diﬃcult to calculate. One
1
possible choice for the kernel is the inverse negative Hessian C =
H−
evaluated at some initial conﬁguration φ(0) or an approximation of it. A
simple possibility to construct an invertible operator from a noninvertible K
would be to add a mass term

−

(662)

(663)

(664)

C =

K + m2

CI

1

−

,

(cid:17)

(cid:16)

150

6
or to impose additional boundary conditions.

Solving a truncated equation of the form (663) with C means skipping

the term

C(P′ΛX + (K

C−

1)φ) from the exact relation

−

−
φ = CP′P−

1N

C(P′ΛX + (K

C−

1)φ).

−

−

(665)

A kernel used to create an initial guess φ(0) will be called an initializing
kernel.

A similar possibility is to start with an “empirical solution”

where φemp is deﬁned as a φ which reproduces the conditional empirical
density Pemp of Eq. (236) obtained from the training data, i.e.,

φ(0) = φemp,

Pemp = P (φemp).

In case, there are not data points for every x–value, a correctly normalized
initial solution would for example be given by ˜Pemp deﬁned in Eq. (238). If
zero values of the empirical density correspond to inﬁnite values for φ, like
in the case φ = L, one can use P ǫ
emp as deﬁned in Eq. (239), with small ǫ, to
obtain an initial guess.

Similarly to Eq. (663), it is often also useful to choose a (for example

smoothing) kernel C and use as initial guess

φ(0) = Cφemp,

or a properly normalized version thereof. Alternatively, one may also let the
(smoothing) operator C directly act on Pemp and use a corresponding φ as
initial guess,

φ(0) = (φ)(
−

1)CPemp),

assuming an inverse (φ)(
−

1) of the mapping P (φ) exists.

We will now discuss the cases φ = L and φ = P in some more detail.

(666)

(667)

(668)

(669)

(670)

7.3.2 Kernels for L

For EL we have the truncated equation

L = CN.

151

Normalizing the exponential of the solution gives
n

L(x, y) =

C(x, y; xi, yi)

ln

dy′ e

n
i

C(x,y′;xi,yi),

Xi

−

Z

P

or

L = CN

ln IXeCN .

−
Notice that normalizing L according to Eq. (671) after each iteration the
truncated equation (670) is equivalent to a one–step iteration with uniform
P (0) = eL(0) according to

(671)

(672)

(673)

L1 = CN + CP(0)ΛX,

CK)L is missing from the nontruncated equation (665),
where only (I
because the additional y–independent term CP(0)ΛX becomes inessential if
L is normalized afterwards.

−

Lets us consider as example the choice C =

1(φ(0)) for uniform initial
L(0) = c corresponding to a normalized P and KL(0) = 0 (e.g., a diﬀerential
operator). Uniform L(0) means uniform P (0) = 1/vy, assuming that vy =
dy
exists and, according to Eq. (138), ΛX = NX for KL(0) = 0. Thus, the
Hessian (161) at L(0)) is found as

H−

−

R

H(L(0)) =

I
−  

−

IX
vy !

K

I
 

−

IX
vy ! −  

I

−

IX
vy !

NX
vy

=

C−

1,

−

(674)

which can be invertible due to the presence of the second term.

Another possibility is to start with an approximate empirical log–density,

deﬁned as

(675)
with P ǫ
emp given in Eq. (239). Analogously to Eq. (668), the empirical log–
density may for example also be smoothed and correctly normalized again,
resulting in an initial guess,

emp,

emp = ln P ǫ

Lǫ

L(0) = CLǫ

ln IXeCLǫ

emp.

emp −
Similarly, one may let a kernel C, or its normalized version ˜C deﬁned below
in Eq. (680), act on Pemp ﬁrst and then take the logarithm

(676)

L(0) = ln( ˜CP ǫ

(677)
Because already ˜CPemp is typically nonzero it is most times not necessary to
work here with P ǫ
emp. Like in the next section Pemp may be also be replaced
by ˜Pemp as deﬁned in Eq. (238).

emp).

152

7.3.3 Kernels for P

For EP the truncated equation

P = CP−

1N,

(678)

i.e.,

is still nonlinear in P . If we solve this equation approximately by a one–
step iteration P 1 = C(P(0))−
1N starting from a uniform initial P (0) and
normalizing afterwards this corresponds for a single x–value to the classical
kernel methods commonly used in density estimation. As normalized density
results

P (x, y) =

i C(x, y; xi, yi)

dy′
P

i C(x, y′; xi, yi)

Xi

=

¯C(x, y; xi, yi),

(679)

R

P
P = N−

1

K,XCN = ¯CN,
with (data dependent) normalized kernel ¯C = N−
1
C,XC and NC,X the diagonal
1 or similar invertible
matrix with diagonal elements IXCN. Again C = K−
choices can be used to obtain a starting guess for P . The form of the Hessian
(182) suggests in particular to include a mass term on the data.

(680)

It would be interesting to interpret Eq. (680) as stationarity equation of
i ln P (xi, yi). Therefore, to
1N of this data term we multiply for existing ¯C−
1
P

a functional ˆEP containing the usual data term
obtain the derivative P−
Eq. (680) by P−

= 0 at data points, to obtain

1, where P

1 ¯C−

with data dependent

e

C−

1P = P−

1N,

C−

1(x, y; x′, y′) =

¯C−

1(x, y; x′, y′)
¯C(x, y; xi, yi)

.

i

Thus, Eq. (680) is the stationarity equation of the functional

P

e

ˆEP =

−

1
2

( N, ln P ) +

( P,

C−

1 P ).

(683)

To study the dependence on the number n of training data for a given C
x, x′, y′. For such

dy C(x, y; x′, y′) = λ,
consider a normalized kernel with
a kernel the denominator of ¯C is equal to nλ so we have

∀

e

(681)

(682)

(684)

R
C
nλ

¯C =

, P =

CN
nλ

153

6
i C(x, y; xi, yi) in the
1 becomes n independent, e.g., converging to the true av-
dx′dy′ p(x′, y′)C(x, y; x′, y′), the regularizing term in functional (683)

Assuming that for large n the empirical average (1/n)
denominator of
erage n
becomes proportional to n

C−

P

e

R

1

C−

nλ2,

(685)

∝
According to Eq. (77) this would allow to relate a saddle point approximation
to a large n–limit.

Again, a similar possibility is to start with the empirical density ˜Pemp
deﬁned in Eq. (238). Analogously to Eq. (668), the empirical density can for
example also be smoothed and correctly normalized again, so that

e

P (0) = ˜C ˜Pemp.

(686)

with ˜C deﬁned in Eq. (680).

Fig. 13 compares the initialization according to Eq.

(679), where the
smoothing operator ˜C acts on N, with an initialization according to Eq.
(686), where the smoothing operator ˜C acts on the correctly normalized
˜Pemp.

7.4 Numerical examples

7.4.1 Density estimation with Gaussian speciﬁc prior

In this section we look at some numerical examples and discuss implemen-
tations of the nonparametric learning algorithms for density estimation we
have discussed in this paper.

As example, consider a problem with a one–dimensional X–space and a

one–dimensional Y –space, and a smoothness prior with inverse covariance

where

and Laplacian

K = λx (KX ⊗

1Y ) + λy (1X ⊗

KY ) ,

KX = λ0IX −
KY = λ0IY −

λ2∆x + λ4∆2
λ2∆y + λ4∆2

λ6∆3
x
λ6∆3
y,

x −
y −

∆x(x, x′) = δ′′(x

x′) = δ(x

x′)

d2
dx2 ,

−

−

154

(687)

(688)
(689)

(690)

= 0 = λ0 = λ4 = λ6 this corresponds to the
and analogously for ∆y. For λ2 6
two Laplacian prior factors ∆x for x and ∆y for y. (Notice that also for λx =
λy the λ4– and λ6–terms do not include all terms of an iterated 2–dimensional
Laplacian, like ∆2 = (∆x + ∆y)2 or ∆4, as the mixed derivatives ∆x∆y are
missing.)

We will now study nonparametric density estimation with prior factors
being Gaussian with respect to L as well as being Gaussian with respect to
P .

The error or energy functional for a Gaussian prior factor in L is given

by Eq. (109). The corresponding iteration procedure is

L(i+1) = L(i) + ηA−

1

KL(i)

eL(i)

−
i(cid:17)
Written explicitly for λ2 = 1, λ0 = λ4 = λ6 = 0 Eq. (691) reads,

−

h

NX −

N
(cid:16)

IXKL(i)

.

(691)

Z

−

Xj

−

Here

Z

yB

yA

Z

L(i+1)(x, y) = L(i)(x, y) + η

A−

1(x, y; xj, yj)

(692)

Xj

+η

dx′dy′A−

1(x, y; x′, y′)

d2
d(x′)2 L(i)(x′, y′) +

d2
d(y′)2 L(i)(x′, y′)

"

δ(x′

xj)+

dy′′

d2
d(x′)2 L(i)(x′, y′′)+

dy′′

Z

d2
d(y′′)2 L(i)(x′, y′′)

eL(i)(x′,y′)

.









dy′′

d2
d(y′′)2 L(i)(x′, y′′) =

d
d(y′′)

L(i)(x′, y′′)

yB

(cid:12)
yA
(cid:12)
(cid:12)
(cid:12)
(cid:12)

vanishes if the ﬁrst derivative d
periodic.

dy L(i)(x, y) vanishes at the boundary or if

Analogously, for error functional EP (164) the iteration procedure

P (i+1) = P (i) + ηA−

1

−
becomes for λ2 = 1, λ0 = λ4 = λ6 = 0

1N

(P(i))−
h

NX −

IXP(i)KP (i)

KP (i)

.

(693)

−

i

P (i+1)(x, y) = P (i)(x, y) + η

(694)

A−

1(x, y; xj, yj)
P (i)(xj, yj)

Xj

+η

dx′dy′A−

1(x, y; x′, y′)

Z

d2
d(x′)2 P (i)(x′, y′) +

d2
d(y′)2 P (i)(x′, y′)

"

155

δ(x′

xj) +
Z

−

− 


Xj

dy′′P (i)(x′, y′′)

d2P (i)(x′, y′′)
d(x′)2

+

dy′′P (i)(x′, y′′)

d2P (i)(x′, y′′)
d(y′′)2

.

!#

Z

yB

yA

Z

dy′′P (i)(x′, y′′)

d2P (i)(x′, y′′)
d(y′′)2

=

Here

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P (i)(x′, y′′)

yB

dP (i)(x′, y′′)
d(y′′)

− Z

yA

yA

yB

dy′′

dP (i)(x′, y′′)
dy′′

!

 

2

,

(695)

where the ﬁrst term vanishes for P (i) periodic or vanishing at the boundaries.
(This has to be the case for i d/dy to be hermitian.)

We now study density estimation problems numerically.

In particular,
we want to check the inﬂuence of the nonlinear normalization constraint.
Furthermore, we want to compare models with Gaussian prior factors for L
with models with Gaussian prior factors for P .

The following numerical calculations have been performed on a mesh of
[1, 15], with periodic boundary
dimension 10
conditions on y and sometimes also in x. A variety of diﬀerent iteration and
initialization methods have been used.

[1, 10] and y

15, i.e., x

×

∈

∈

Figs. 14 – 17 summarize results for density estimation problems with only
two data points, where diﬀerences in the eﬀects of varying smoothness priors
are particularly easy to see. A density estimation with more data points can
be found in Fig. 21.

For Fig. 14 a Laplacian smoothness prior on L has been implemented. The
solution has been obtained by iterating with the negative Hessian, as long
as positive deﬁnite. Otherwise the gradient algorithm has been used. One
iteration step means one iteration according to Eq. (601) with the optimal
η. Thus, each iteration step includes the optimization of η by a line search
(For the ﬁgures the Mathematica function FindMinimum has
algorithm.
been used to optimize η.)

As initial guess in Fig. 14 the kernel estimate L(0) = ln( ˜C ˜Pemp) has been
employed, with ˜C deﬁned in Eq. (680) and C = (K+m2
CI) with squared mass
m2
C = 0.1. The fast drop–oﬀ of the energy EL within the ﬁrst two iterations
shows the quality of this initial guess. Indeed, this fast convergence seems
to indicate that the problem is nearly linear, meaning that the inﬂuence of

156

the only nonlinear term in the stationarity equation, the normalization con-
straint, is not too strong. Notice also, that the reconstructed regression shows
the typical piecewise linear approximations well known from one–dimensional
(normalization constraint free) regression problems with Laplacian prior.

CI) but with squared mass m2

Fig. 15 shows a density estimation similar to Fig. 14, but for a Gaussian
prior factor in P and thus also with diﬀerent λ2, diﬀerent initialization, and
slightly diﬀerent iteration procedure. For Fig. 15 also a kernel estimate P (0)
= ( ˜C ˜Pemp) has been used as initial guess, again with ˜C as deﬁned in Eq. (680)
and C = (K + m2
C = 1.0. The solution has been
obtained by prior relaxation A = K + m2I including a mass term with m2
∆ and periodic boundary conditions an
= 1.0 to get for a Laplacian K =
invertible A. This iteration scheme does not require to calculate the Hessian
HP at each iteration step. Again the quality of the initial guess (and the
iteration scheme) is indicated by the fast drop–oﬀ of the energy EP during
the ﬁrst iteration.

−

Because the range of P –values, being between zero and one, is smaller
than that of L–values, being between minus inﬁnity and zero, a larger Lapla-
cian smoothness factor λ2 is needed for Fig. 15 to get similar results than for
Fig. 14. In particular, such λ2 values have been chosen for the two ﬁgures
that the maximal values of the the two reconstructed probability densities P
turns out to be nearly equal.

Because the logarithm particularly expands the distances between small
probabilities one would expect a Gaussian prior for L to be especially eﬀective
for small probabilities. Comparing Fig. 14 and Fig. 15 this eﬀect can indeed
be seen. The deep valleys appearing in the L–landscape of Fig. 15 show that
small values of L are not smoothed out as eﬀectively as in Fig. 14. Notice,
that therefore also the variance of the solution p(y
x, h) is much smaller for
a Gaussian prior in P at those x which are in the training set.

|

−

Fig. 16 resumes results for a model similar to that presented in Fig.
∆3)–prior replacing the Laplacian (
14, but with a (
∆)–prior. As all
quadratic functions have zero third derivative such a prior favors, applied to
L, quadratic log–likelihoods, corresponding to Gaussian probabilities P . In-
deed, this is indicated by the striking diﬀerence between the regression func-
∆3)–prior produces a much rounder
tions in Fig. 16 and in Fig. 14: The (
−
regression function, especially at the x values which appear in the data. Note
however, that in contrast to a pure Gaussian regression problem, in density
estimation an additional non–quadratic normalization constraint is present.
In Fig. 17 a similar prior has been applied, but this time being Gaussian

−

157

∆3)–prior for L, a (

∆3)–prior for P
in P instead of L. In contrast to a (
−
implements a tendency to quadratic P . Similarly to the diﬀerence between
Fig. 14 and Fig. 16, the regression function in Fig. 17 is also rounder than
that in Fig. 15. Furthermore, smoothing in Fig. 17 is also less eﬀective for
smaller probabilities than it is in Fig. 16. That is the same result we have
found comparing the two priors for L shown in Fig. 15 and Fig. 14. This leads
to deeper valleys in the L–landscape and to a smaller variance especially at
x which appear in the training data.

−

Fig. 21 depicts the results of a density estimation based on more than
In particular, ﬁfty training data have been obtained by

two data points.
sampling with uniform p(x) from the “true” density

Ptrue(x, y) = p(y

x, htrue) =

|

1
2√2πσ0





(y−ha(x))2
2σ2

e−

0 + e−

(y−hb(x))2
2σ2
0

,

(696)





with σ0 = 1.5, ha(x) = 125/18 + (5/9)x, hb(x) = 145/18
(5/9)x, shown in
the top row of Fig. 18. The sampling process has been implemented using the
transformation method (see for example [181]). The corresponding empirical
density N/n (235) and conditional empirical density Pemp of Eq. (236), in
this case equal to the extended ˜Pemp deﬁned in Eq. (238), can be found in
Fig. 20.

−

Fig. 21 shows the maximum posterior solution p(y

x, h∗) and its loga-

rithm, the energy EL during iteration, the regression function

|

h(x) =

dy y p(y

x, htrue) =

dy y Ptrue(x, y),

|
(as reference, the regression function for the true likelihood p(y
x, htrue) is
given in Fig. 19), the average training error (or empirical (conditional) log–
loss)

Z

Z

|

(697)

<

ln p(y

x, h) >D=

−

|

log p(yi|

xi, h),

(698)

1
n

−

n

Xi=1

and the average test error (or true expectation of (conditional) log–loss) for
uniform p(x)

<

ln p(y

x, h) >Ptrue=

−

|

− Z

dy dx p(x)p(y

x, htrue) ln p(y

x, h),

(699)

|

which is, up to a constant, equal to the expected Kullback–Leibler distance
between the actual solution and the true likelihood,

KL

p(x, y
(cid:16)

|

|

− Z

(cid:17)

htrue), p(y

x, h)

=

dy dx p(x, y

htrue) ln

(700)

p(y

x, h)
|
x, htrue)

.

p(y

|

|

|

158

It has, in
The test error measures the quality of the achieved solution.
contrast to the energy and training error, of course not been available to the
learning algorithm.

The maximum posterior solution of Fig. 21 has been calculated by mini-
mizing EL using massive prior iteration with A = K + m2I, a squared mass
m2 = 0.01, and a (conditionally) normalized, constant L(0) as initial guess.
Convergence has been fast, the regression function is similar to the true one
(see Fig. 19).

Fig. 22 compares some iteration procedures and initialization methods
Clearly, all methods do what they should do, they decrease the energy func-
Iterating with the negative Hessian yields the fastest convergence.
tional.
Massive prior iteration is nearly as fast, even for uniform initialization, and
does not require calculation of the Hessian at each iteration. Finally, the
slowest iteration method, but the easiest to implement, is the gradient algo-
rithm.

Looking at Fig. 22 one can distinguish data–oriented from prior–oriented
initializations. We understand data–oriented initial guesses to be those for
which the training error is smaller at the beginning of the iteration than for
the ﬁnal solution and prior–oriented initial guesses to be those for which the
opposite is true. For good initial guesses the diﬀerence is small. Clearly,
the uniform initializations is prior–oriented, while an empirical log–density
ln(N/n + ǫ) and the shown kernel initializations are data–oriented.

The case where the test error grows while the energy is decreasing indi-
cates a misspeciﬁed prior and is typical for overﬁtting. For example, in the
ﬁfth row of Fig. 22 the test error (and in this case also the average train-
ing error) grows again after having reached a minimum while the energy is
steadily decreasing.

7.4.2 Density estimation with Gaussian mixture prior

Having seen Bayesian ﬁeld theoretical models working for Gaussian prior
factors we will study in this section the slightly more complex prior mixture
models. Prior mixture models are an especially useful tool for implementing
complex and unsharp prior knowledge. They may be used, for example,
to translate verbal statements of experts into quantitative prior densities
[123, 124, 125, 126, 127], similar to the quantiﬁcation of “linguistic variables”
by fuzzy methods [110, 111].

We will now study a prior mixture with Gaussian prior components in L.

159

Hence, consider the following energy functional with mixture prior

EL =

ln

−

Xj

with mixture components

pje−

Ej =

(L, N) + (eL, ΛX)

λE0,j

pje−

(701)

ln

−

Xj

Ej =

(L, N) + λE0,j + (eL, ΛX).

−

−

We choose Gaussian component prior factors with equal covariances but dif-
fering means

1
2
Hence, the stationarity equation for Functional (701) becomes

tj, K(L

E0,j =

L
(cid:16)

.
(cid:17)

tj)

−

−

0 = N

λK

L



−

eLΛX,

−

Xj

ajtj 


−


with Lagrange multiplier function

(702)

(703)

(704)

(705)

ΛX = NX −

λIXK

L



−

,

ajtj 


Xj

and mixture coeﬃcients


λE0,j
pje−
k pke−
The parameter λ plays here a similar role as the inverse temperature β for
prior mixtures in regression (see Sect. 6.3). In contrast to the β–parameter
in regression, however, the “low temperature” solutions for λ
are the
pure prior templates tj, and for λ

0 the prior factor is switched oﬀ.

→ ∞

(706)

aj =

λE0,k

P

.

Typical numerical results of a prior mixture model with two mixture
components are presented in Figs. 23 – 28. Like for Fig. 21, the true likelihood
used for these calculations is given by Eq. (696) and shown in Fig. 18. The
corresponding true regression function is thus that of Fig. 19. Also, the same
training data have been used as for the model of Fig. 21 (Fig. 20). The
two templates t1 and t2 which have been selected for the two prior mixture
components are (Fig. 18)

→

t1(x, y) =

t2(x, y) =

1
2√2πσt

e−




e−

(y−µ2)2
2σ2
t

,

1
√2πσt

(y−µa)2
2σ2

t + e−

(y−µb)2
2σ2
t

,





(707)

(708)

160

25/9 = 4.72, and µ2 =
with σt = 2, µa = µ2 + 25/9 = 10.27, µb = µ2 −
15/2. Both templates capture a bit of the structure of the true likelihood,
but not too much, so learning remains interesting. The average test error of
t1 is equal to 2.56 and is thus lower than that of t2 being equal to 2.90. The
minimal possible average test error 2.23 is given by that of the true solution
Ptrue. A uniform P , being the eﬀective template in the zero mean case of Fig.
21, has with 2.68 an average test error between the two templates t1 and t2.
Fig. 23 proves that convergence is fast for massive prior relaxation when
starting from t1 as initial guess L(0). Compared to Fig. 21 the solution is a bit
smoother, and as template t1 is a better reference than the uniform likelihood
the ﬁnal test error is slightly lower than for the zero–mean Gaussian prior
on L. Starting from L(0) = t2 convergence is not much slower and the ﬁnal
solution is similar, the test error being in that particular case even lower (Fig.
24). Starting from a uniform L(0) the mixture model produces a solution very
similar to that of Fig. 21 (Fig. 24).

The eﬀect of changing the λ parameter of the prior mixture can be seen
in Fig. 26 and Fig. 27. Larger λ means a smoother solution and faster
convergence when starting from a template likelihood (Fig. 26). Smaller λ
results in a more rugged solution combined with a slower convergence. The
test error in Fig. 27 already indicates overﬁtting.

Prior mixture models tend to produce metastable and approximately sta-
ble solutions. Fig. 28 presents an example where starting with L(0) = t2 the
learning algorithm seems to have produced a stable solution after a few it-
erations. However, iterating long enough this decays into a solution with
smaller distance to t1 and with lower test error. Notice that this eﬀect can
be prevented by starting with another initialization, like for example with
L(0) = t1 or a similar initial guess.

We have seen now that, and also how, learning algorithms for Bayesian
ﬁeld theoretical models can be implemented. In this paper, the discussion
of numerical aspects was focussed on general density estimation problems.
Other Bayesian ﬁeld theoretical models, e.g., for regression and inverse quan-
tum problems, have also been proved to be numerically feasible. Speciﬁcally,
prior mixture models for Gaussian regression are compared with so–called
Landau–Ginzburg models in [123]. An application of prior mixture mod-
els to image completion, formulated as a Gaussian regression model, can be
found in [128]. Furthermore, hyperparameter have been included in numer-
ical calculations in [124] and also in [128]. Finally, learning algorithms for
inverse quantum problems are treated in [134] for inverse quantum statistics,

161

and, in combination with a mean ﬁeld approach, in [133] for inverse quantum
many–body theory. Time–dependent inverse quantum problems will be the
topic of [129].

In conclusion, we may say that many diﬀerent Bayesian ﬁeld theoretical
models have already been studied numerically and proved to be computation-
ally feasible. This also shows that such nonparametric Bayesian approaches
are relatively easy to adapt to a variety of quite diﬀerent learning scenarios.
Applications of Bayesian ﬁeld theory requiring further studies include, for
example, the prediction of time–series and the interactive implementation of
unsharp a priori information.

Acknowledgements The author wants to thank Federico Girosi, Tomaso
Poggio, J¨org Uhlig, and Achim Weiguny for discussions.

References

[1] Aarts, E. & Korts, J. (1989) Simulated Annealing and Boltzmann Ma-

chines. New York: Wiley.

[2] Abu–Mostafa, Y. (1990) Learning from Hints in Neural Networks. Jour-

nal of Complexity 6, 192–198.

[3] Abu–Mostafa, Y. (1993) Hints and the VC Dimension. Neural Compu-

tation 5, 278–288.

[4] Abu–Mostafa, Y. (1993b) A method for learning from hints. Advances in
Neural Information Processing Systems 5, S. Hanson et al (eds.), 73–80,
San Mateo, CA: Morgan Kauﬀmann.

[5] Allen, D.M. (1974) The relationship between variable selection and data
augmentation and a method of prediction. Technometrics 16, 125.

[6] Amari, S., Cichocki, A., & Yang, H.H.(1996) A New Learning Algo-
rithm for Blind Signal Separation. in Advances in Neural Information
Processing Systems 8, D.S. Touretzky et al (eds.), 757–763, Cambridge,
MA: MIT Press.

[7] Balian, R. (1991) From Microphysics to Macrophysics. Vol. I. Berlin:

Springer Verlag.

162

[8] Ballard, D.H. (1997) An Introduction to Natural Computation. Cam-

bridge, MA: MIT Press.

[9] Bazaraa, M.S., Sherali, H.D., & Shetty, C.M. (1993) Nonlinear Program-

ming. (2nd ed.) New York: Wiley.

[10] Bayes, T.R. (1763) An Essay Towards Solving a Problem in the Doc-
trine of Chances. Phil. Trans. Roy. Soc. London 53, 370. (Reprinted in
Biometrika (1958) 45, 293)

[11] Beck, C. & Schl¨ogl, F. (1993) Thermodynamics of chaotic systems. Cam-

bridge: Cambridge University Press.

[12] Bell, A.J. & Sejnowski, T.J. (1995) Neural Computation 7(6), 1129–

1159.

York: Wiley.

Wiley.

Scientiﬁc.

[13] Ben–Israel, A. & Greville, Th.N.E. (1974) Generalized Inverses. New

[14] Berger, J.O. (1980) Statistical Decision Theory and Bayesian Analysis.

New York: Springer Verlag.

[15] Berger, J.O. & Wolpert R. (1988) The Likelihood Principle. (2nd ed.).

Hayward, CA: IMS Lecture Notes — Monograph Series 9.

[16] Bernado, J.M. & Smith, A.F. (1994) Bayesian Theory. New York: John

[17] Bertsekas, D.P. (1995) Nonlinear Programming. Belmont, MA: Athena

[18] Binder, K. & Heermann, D.W. (1988) Monte Carlo simulation in sta-

tistical physics: an introduction. Berlin: Springer Verlag.

[19] Bishop, C.M. (1993) Curvature–driven smoothing: a learning algo-
rithm for feedforward netsworks. IEEE Transactions on Neural Net-
works 4(5),882–884.

[20] Bishop, C.M. (1995) Training with noise is equivalent to Tikhonov reg-

ularization. Neural Computation 7 (1), 108–116.

163

[21] Bishop, C.M. (1995) Neural Networks for Pattern Recognition. Oxford:

Oxford University Press.

[22] Bishop, E. & Bridges, D. (1985) Constructive Analysis. Grundlehren der
Mathematischen Wissenschaften, Vol. 279. Berlin: Springer–Verlag.

[23] Black, M.J. & Rangarajan, A. (1996) On the Uniﬁcation of Line Pro-
cesses, Outlier Rejection, and Robust Statistics With Applications in
Early Vision. Int’l J. Computer Vision 19 (1).

[24] Blaizot, J.–P. & Ripka, G. (1986) Quantum Theory of Finite Systems.

Cambridge, MA: MIT Press.

[25] Blake, A. & Zisserman, A. (1987) Visual reconstruction Cambridge, MA:

MIT Press.

[26] Blanchard, P. & Bruening, E. (1982) Variational Methods in Mathemat-

ical Physics. Berlin: Springer Verlag.

[27] Bleistein, N. & Handelsman, N. (1986) Asymptotic Expansions of In-
tegrals. (Originally published in 1975 by Holt, Rinehart and Winston,
New York) New York: Dover.

[28] Breiman, L. (1993) Hinging hyperplanes for regression, classiﬁcation,
and function approximation. IEEE Trans. Inform. Theory 39(3), 999–
1013.

[29] Breiman, L., Friedman, J.H., Olshen, R.A., & Stone, C.J. (1993) Clas-

siﬁcation and Regression Trees, New York: Chapman & Hall.

[30] Bretthorst, G.L. (1988) Bayesian spectrum analysis and parameter esti-
mation. Lecture Notes in Statistics, Vol. 48. Berlin: Springer Verlag.
(Available at http://bayes.wustl.edu/glb/book.pdf.)

[31] Cardy, J. (1996) Scaling and Renormalization in Statistical Physics.

Cambridge: Cambridge University Press.

[32] Choquet–Bruhat Y., DeWitt–Morette, C., & Dillard–Bleick, M. (1982)
Analysis, Manifolds, and Physics. Part I. Amsterdam: North–Holland.

[33] Collins, J. (1984) Renormalization. Cambridge: Cambridge University

Press.

164

[34] Cox, D.R. & Hinkley, D.V. (1974) Theoretical Statistics. London: Chap-

man & Hall.

[35] Craven, P. & Wahba, G. (1979) Smoothing noisy data with spline func-
tions: estimating the correct degree of smoothing by the method of
generalized cross–validation. Numer.Math. 31, 377–403.

[36] Cressie, N.A.C. (1993) Statistics for Spatial Data. New York, Wiley.

[37] Creutz, M. (1983) Quarks, gluons and lattices. Cambridge: Cambridge

University Press.

[38] Davis, L. (ed.) (1987) Genetic Algorithms and Simulated Annealing. San

Mateo, CA: Morgan Kaufmann.

[39] Davis, L. (ed.) (1991) Handbook of Genetic Algorithms New York: Van

Nostrand Reinhold.

[40] De Bruijn, N.G. (1981) Asymptotic Methods in Analysis. (Originally
published in 1958 by the North–Holland Publishing Co., Amsterdam)
New York: Dover.

[41] Deco, G. & Obradovic, D. (1996) An Information–Theoretic Approach

to Neural Computing. New York: Springer Verlag.

[42] Devroye, L., Gy¨orﬁ, L., & Lugosi, G. (1996) A Probabilistic Theory of

Pattern recognition. New York: Springer.

[43] Di Castro, C. & Jona-Lasinio, G. (1976) The Renormalization Group
Approach to Critical Phenomena. In: Domb, C. & Green M.S. (eds.)
Phase Transitions and Critical Phenomena. London: Academic Press.

[44] Dietrich, R., Opper, M., & Sompolinsky, H. (1999) Statistical Mechanics
of Support Vector Networks. Physical Review Letters 82(14), 2975–2978.

[45] Donoho, D.L. & Johnstone, I.M. (1989) Projection–based approximation

and a duality with kernel methods. Ann.Statist. 17(1),58–106.

[46] Doob, J.L. (1953) Stochastic Processes. (New edition 1990) New York:

Wiley.

165

[47] Dudley, R.M. (1984) A course on empirical processes. Lecture Notes in

Mathematics, 1097:2-142.

[48] Ebeling, W., Freund, J., & Schweitzer, F. (1998) Komplexe Strukturen:

Entropie und Information. Stuttgart: Teubner.

[49] Efron, B. & Tibshirani R.J. (1993) An Introduction to the Bootstrap.

New York: Chapman & Hall.

[50] Eisenberg, J. & Greiner, W. (1972) Microscopic Theory of the Nucleus.

North–Holland, Amsterdam.

[51] Fern´andez, R., Fr¨ohlich, J., & Sokal, A.D. (1992) Random Walks,
Critical Phenomena, and Triviality in Quantum Field Theory. Berlin:
Springer–Verlag.

[52] Fletcher, R. (1987) Practical Methods of Optimization. New York: Wi-

ley.

Press.

[53] Fredholm I. (1903) Acta Math. 27.

[54] Friedman, J.H. & Tukey, J.W. (1974) A projection pursuit algorithm
for exploratory data analysis. IEEE Trans. Comput. 24, 1000–1006.

[55] Friedman, J.H. & Stuetzle, W. (1981) Projection pursuit regression.

J.Am.Statist.Assoc. 76(376), 817–823.

[56] Fukunaga, K. (1990) Statistical Pattern Recognition Boston: Academic

[57] Gardner, E. (1987) Maximum Storage Capacity in Neural Networks.

Europhysics Letters 4 481–485.

[58] Gardner, E. (1988) The Space of Interactions in Neural Network Models.

Journal of Physics A 21 257–270.

[59] Gardner, E. & Derrida B. (1988) Optimal Storage Properties of neural

Network Models. Journal of Physics A 21 271–284.

[60] Gardiner, C.W. (1990) Handbook of Stochastic Methods. (2nd Ed.),

Berlin: Springer–Verlag.

166

[61] Geiger, D. & Girosi, F. (1991) Parallel and Deterministic Algortihms for
MRFs: Surface Reconstruction. IEEE Trans. on Pattern Analysis and
Machine Intelligence 13 (5), 401–412.

[62] Geiger, D. & Yuille, A.L. (1991) A Common Framework for Image Seg-

mentation. Int’l J. Computer Vision 6 (3), 227–243.

[63] Gelfand, S.B. & Mitter, S.K. (1993) On Sampling Methods and Anneal-
ing Algorithms. Markov Random Fields – Tehory and Applications. New
York: Academic Press.

[64] Gelman, A., Carlin, J.B., Stern, H.S., & Rubin, D.B. (1995) Bayesian

Data Analysis. New York: Chapman & Hall.

[65] Geman, S. & Geman, D. (1984) Stochastic relaxation, Gibbs distribu-
tions and the Bayesian restoration of images. IEEE Trans. on Pattern
Analysis and Machine Intelligence 6, 721–741. Reprinted in Shafer &
Pearl (eds.) (1990) Readings in Uncertainty Reasoning. San Mateo, CA:
Morgan Kaufmann.

[66] Geman, D. & Reynoids, G. (1992) Constraint restoration and the Re-
cover of Discontinuities. IEEE Trans. on Pattern Analysis and Machine
Intelligence. 14, 367–383.

[67] Giraud, B.G., Lapedes, A., Liu, L.C., & Lemm, J.C. (1995) Lorentzian

Neural Nets. Neural Networks 8 (5), 757-767.

[68] Girosi, F. (1991) Models of noise and robust estimates. A.I.Memo 1287,
Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technol-
ogy.

[69] Girosi, F., (1997) An equivalence between sparse approximation and
support vector machines. A.I. Memo No.1606, Artiﬁcial Intelligence Lab-
oratory, Massachusetts Institute of Technology.

[70] Girosi, F., Poggio, T., & Caprile, B. (1991) Extensions of a theory of
networks for approximations and learning: Outliers and negative ex-
amples. In Lippmann, R., Moody, J., & Touretzky, D. (eds.) Advances
in Neural Information Processing Systems 3, San Mateo, CA: Morgan
Kaufmann.

167

[71] Girosi, F., Jones, M., & Poggio, T. (1995) Regularization Theory and
Neural Networks Architectures. Neural Computation 7 (2), 219–269.

[72] Glimm, J. & Jaﬀe, A. (1987) Quantum Physics. A Functional Integral

Point of View. New York: Springer–Verlag.

[73] Goeke, K., Cusson, R.Y., Gruemmer, F., Reinhard, P.–G., Reinhardt,

H., (1983) Prog. Theor. Physics [Suppl.] 74 & 75, 33.

[74] Goldberg, D.E. (1989) Genetic Algorithms in Search, Optimization, and

Machine Learning. Redwood City, CA: Addison–Wesley.

[75] Golden, R.M. (1996) Mathematical Methods for Neural Network Analysis

and Design. Cambridge, MA: MIT Press.

[76] Golup, G., Heath, M., & Wahba, G.(1979) Generalized cross validation
as a method for choosing a good ridge parameter. Technometrics 21,
215–224.

[77] Good, I.J. & Gaskins, R.A. (1971) Nonparametric roughness penalties

for probability densities. Biometrika 58, 255–277.

[78] Green, P.J. & Silverman, B.W. (1994) Nonparametric Regression and

Generalized Linear Models. London: Chapman & Hall.

[79] Gull, S.F. (1988) Bayesian data analysis – straight line ﬁtting. In
Skilling, J, (ed.) Maximum Entropy and Bayesian Methods. Cambridge,
511 –518, Dordrecht: Kluwer.

[80] Gull, S.F. (1989) Developments in maximum entropy data analysis. In
Skilling, J, (ed.) Maximum Entropy and Bayesian Methods. Cambridge
1988, 53 – 71, Dordrecht: Kluwer.

[81] Hackbusch, W. (1989) Integralgleichungen. Teubner Studienb¨ucher.

Stuttgart: Teubner.

[82] Hackbusch, W. (1993) Iterative L¨osung großer schwachbesetzter Gle-

ichungssysteme. Teubner Studienb¨ucher. Stuttgart: Teubner.

[83] H¨ardle, W. (1990) Applied nonparametric regression. Cambridge: Cam-

bridge University Press.

168

[84] Hammersley, J.M. & Handscomb, D.C. (1964) Monte Carlo Methods.

London: Chapman & Hall.

[85] Hastie, T.J. & Tibshirani, R.J. (1986) Generalized Additive Models.

Statist.Sci. 1,297–318.

[86] Hastie, T.J. & Tibshirani, R.J. (1987) Generalized Additive Models:

Some applications. J.Am.Statist.Assoc. 82,371–386.

[87] Hastie, T.J. & Tibshirani, R.J. (1990) Generalized Additive Models. Lon-

don: Chapman & Hall.

[88] Hastings, W.K. (1970) Monte Carlo sampling methods using Markov

chains and their applications. Biometrika 57, 97–109.

[89] Hertz, J., Krogh, A. & Palmer, R.G. (1991) Introduction to the Theory
of Neural Computation. Santa Fe Institute, Lecture Notes Volume I,
Addison–Wesley.

[90] Hilbert, D. & Courant,R. (1989) Methods of Mathematical Physics

Vol.1&2,(1st German editions 1924,1937, Springer) New York: Wiley.

[91] Holland, J.H. (1975) Adaption in Natural and Artiﬁcial Systems. Uni-

versity of Michigan Press. (2nd ed. MIT Press, 1992.)

[92] Horst, R., Pardalos, M., & Thoai, N.V. (1995) Introduction to Global

Optimization. Dordrecht: Kluwer.

[93] Huber, P.J. (1979) Robust Smoothing. In Launer, E. & Wilkinson G.

(eds.) Robustness in Statistics New York: Academic Press.

[94] Huber, P.J. (1981) Robust Statistics. New York: Wiley.

[95] Huber, P.J. (1985) Projection Pursuit. Ann.Statist. 13(2),435–475.

[96] Itzkyson, C. & Drouﬀe, J.–M. (1989) Statistical Field Theory. (Vols. 1

and 2) Cambridge: Cambridge University Press.

[97] Jaynes, E.T. (in preparation) Probability Theory: The Logic Of Science.

(Available at http://bayes.wustl.edu/etj/prob.html.)

[98] Jeggle, H. (1979) Nichtlineare Funktionalanalysis. Stuttgart: Teubner.

169

[99] Jensen, F.V. (1996) An Introduction to Bayesian Networks. New York:

Springer.

Press.

[100] Jones, M.C. & Sibson, R. (1987) What is Projection Pursuit? J. Roy.

Statist. Soc. A 150, 1–36.

[101] Kaku, M. (1993) Quantum Field Theory. Oxford: Oxford University

[102] van Kampen, N.G. (1992) Stochastic Processes in Physics and Chem-

istry. Amsterdam: North–Holland.

[103] Kant, I. (1911) Kritik der reinen Vernunft.(2nd ed.) Werke, Vol.3

Berlin: K¨onigliche Akademie der Wissenschaften.

[104] Kimmeldorf, G.S. & Wahba, G. (1970) A correspondence between
Bayesian estimation on stochastic processes and smoothing splines. Ann.
Math. Stat. 41, 495–502.

[105] Kimmeldorf, G.S. & Wahba, G. (1970) Spline functions and stochastic

processes. Sankhya Ser. A 32, Part 2, 173–180.

[106] Kirkpatrick, S., Gelatt Jr., C.D., & Vecchi, M.P. (1983) Optimization

by Simulated Annealing. Science 220, 671–680.

[107] Kirsch, A. (1996) An Introduction to the Mathematical Theory of In-

verse Problems. New York: Springer.

[108] Kitagawa, G., Gersch, W. (1996) Smoothness Priors Analysis of Time

Series New York: Springer.

[109] Kleinert, H.(1993) Pfadintegrale. Mannheim: Wissenschaftsverlag.

[110] Klir, G.J. & Yuan, B. (1995) Fuzzy Sets and Fuzzy Logic. Prentice Hall.

[111] Klir, G.J. & Yuan, B. (eds.) (1996) Fuzzy Sets, Fuzzy Logic, and Fuzzy

Systems. World Scientiﬁc.

[112] Koecher, M. (1985) Lineare Algebra und analytische Geometrie. Berlin:

Springer.

[113] Koza, J.R. (1992) Genetic Programming Cambridge, MA: MIT Press.

170

[114] Kullback, S. & Leibler R.A. (1951) On Information and Suﬃciency.

Ann.Math.Statist. 22, 79–86.

[115] Kullback, S. (1951) Information Theory and Statistics. New York: Wi-

ley.

[116] Lapedes, A. & Farber, R. (1988) How neural nets work. in Neural In-
formation Processing Systems, D.Z.Anderson, (ed.),442–456. New York:
American Institute of Physics.

[117] Lauritzen, S.L. (1996) Graphical Models. Oxford: Clarendon Press.

[118] Le Bellac, M. (1991) Quantum and Statistical Field Theory. Oxford

Science Publications, Oxford: Clarendon Press.

[119] Le Cam, L. (1986) Asymptotic Methods in Statistical Decision Theory.

New York: Springer.

[120] Leen, T.K. (1995) From Data Distributions to Regularization in In-

variant Learning. Neural Computation 7, 974–981.

[121] Lemm, J.C. (1995) Inhomogeneous Random Phase Approximation for
Nuclear and Atomic Reactions. Annals of Physics 244 (1), 136–200,
1995.

[122] Lemm, J.C. (1995) Inhomogeneous Random Phase Approximation: A

Solvable Model. Annals of Physics 244 (1), 201–238, 1995.

[123] Lemm, J.C. (1996) Prior Information and Generalized Questions.
A.I.Memo No. 1598, C.B.C.L. Paper No. 141, Massachusetts Institute
of Technology. (Available at http://pauli.uni-muenster.de/∼lemm.)

[124] Lemm, J.C. (1998) How to Implement A Priori Information: A Sta-
tistical Mechanics Approach. Technical Report MS-TP1-98-12, M¨unster
University, cond-mat/9808039.

[125] Lemm, J.C. (1998) Fuzzy Interface with Prior Concepts and Non-
Convex Regularization. In Wilfried Brauer (Ed.), Proceedings of the 5.
International Workshop ”Fuzzy-Neuro Systems ’98” (FNS ’98), March
19-20, 1998, Munich, Germany, Sankt Augustin: Inﬁx.

171

[126] Lemm, J.C. (1998) Quadratic Concepts. In Niklasson L., Bod´en, M.,
& Ziemke, T. (eds.) Proceedings of the 8th International Conference on
Artiﬁcial Neural Networks. (ICANN98) New York: Springer Verlag.

[127] Lemm, J.C. (1998) Fuzzy Rules and Regularization Theory. In ELITE
European Laboratory for Intelligent Techniques Engineering (ed.): Pro-
ceedings of the 6th European Congress on Intelligent Techniques and
Soft Computing (EUFIT ’98), Aachen, Germany, September 7-10, 1998,
Mainz, Aachen.

[128] Lemm, J.C. (1999) Mixtures of Gaussian Process Priors. In Proceed-
ings of the Ninth International Conference on Artiﬁcial Neural Networks
(ICANN99), IEEE Conference Publication No. 470. London: Institution
of Electrical Engineers.

[129] Lemm, J.C. (In preparation) Inverse Time–dependent Quantum Me-

chanics.

[130] Lemm, J.C., Beiu, V., & Taylor, J.G. (1995) Density Estimation as a
Preprocessing Step for Constructive Algorithms. In Kappen B., Gielen,
S. (eds.): Proceedings of the 3rd SNN Neural Network Symposium. The
Netherlands, Nijmegen, 14–15 September 1995, Berlin, Springer Verlag.

[131] Lemm, J.C., Giraud, B.G., & Weiguny, A. (1990) Mean ﬁeld ap-
proximation versus exact treatment of collisions in few–body systems.
Z.Phys.A – Atomic Nuclei 336, 179–188.

[132] Lemm, J.C., Giraud, B.G., & Weiguny, A. (1994) Beyond the time in-
dependent mean ﬁeld theory for nuclear and atomic reactions: Inclusion
of particle-hole correlations in a generalized random phase approxima-
tion. Phys.Rev.Lett. 73, 420, nucl-th/9911056.

[133] Lemm, J.C., Uhlig, J. (1999) Hartree-Fock Approximation for Inverse
Many-Body Problems. Technical Report, MS-TP1-99-10, M¨unster Uni-
versity, nucl-th/9908056.

[134] Lemm, J.C., Uhlig J., Weiguny, A. (1999) A Bayesian Approach to
Inverse Quantum Statistics. Technical Report, MS-TP1-99-6, M¨unster
University, cond-mat/9907013.

[135] Lifshits, M.A. (1995) Gaussian Random Functions. Dordrecht: Kluwer.

172

[136] Loredo T. (1990) From Laplace to Supernova SN 1987A: Bayesian In-
ference in Astrophysics. In Foug`ere, P.F. (ed.) Maximum-Entropy and
Bayesian Methods, Dartmouth, 1989, 81–142. Dordrecht: Kluwer.
(Available at http://bayes.wustl.edu/gregory/gregory.html.)

[137] Louis, A.K. (1989) Inverse und schlecht gestellte Probleme. Stuttgart:

Teubner.

[138] MacKay, D.J.C. (1992) The evidence framework applied to classiﬁca-

tion networks. Neural Computation 4 (5), 720–736.

[139] MacKay, D.J.C. (1992) A practical Bayesian framework for backprop-

agation networks. Neural Computation 4 (3), 448–472.

[140] MacKay, D.J.C. (1994) Hyperparameters: optimise or integrate out?
In Heidbreder, G. (ed.) Maximum Entropy and Bayesian Methods, Santa
Barbara 1993. Dordrecht: Kluwer.

[141] Marroquin, J.L., Mitter, S., & Poggio, T. (1987) Probabilistic solution
of ill–posed problems in computational vision. J. Am. Stat. Assoc. 82,
76–89.

[142] Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H.,
& Teller, E. (1953) Equation of state calculations by fast computing
machines. Journal of Chemical Physics 21, 1087–1092.

[143] McCullagh, P. & Nelder, J.A. (1989) Generalized Linear Models Lon-

don: Chapman & Hall.

[144] Mezard, M., Parisi, G., & Virasoro, M.A. (1987) Spin Glass Theory

and Beyond. Singapore: World Scientiﬁc.

[145] Michalewicz, Z. (1992) Genetic Algorithms + Data Structures = Evo-

lution Programs. Berlin: Springer–Verlag.

[146] Michie, D., Spiegelhalter, D.J., & Taylor, C.C. (Eds.) (1994) Machine
Learning, Neural and Statistical Classiﬁcation. New York: Ellis Hor-
wood.

[147] Minski, M.L. & Papert, S.A. (1990) Perceptrons. (Expanded Edition,

Original edition, 1969) Cambridge, MA: MIT Press.

173

[148] Mitchell, M. (1996) An Introduction to Genetic Algorithms. Cambridge,

MA: MIT Press.

[149] Molgedey, L. & Schuster, H.G. (1994) Separation of a mixture of inde-
pendent signals using time delayed correlations. Phys.Rev.Lett. 72(23),
3634–3637.

[150] Montvay, I. & M¨unster, G. (1994) Quantum Fields on a Lattice. Cam-

bridge: Cambridge University Press.

[151] Moore, E.H. (1920) Bull.Amer.Math.Soc. 26.

[152] Morozov, V.A. (1984) Methods for Solving Incorrectly Posed problems.

New York: Springer Verlag.

[153] Mosteller, F. & Wallace, D. (1963) Inference in an authorship problem.
A comparative study of discrimination methods applied to authorships
of the disputed Federalist papers. J. Amer. Statist. Assoc. 58, 275–309.

[154] M¨uller, B. & Reinhardt, J. (1991) Neural Networks. (2nd printing)

Berlin, Springer.

[155] Mumford, D. & Shah, J. (1989) Optimal Approximations by Piecewise
Smooth Functions and Associated Variational Problems. Comm. Pure
Applied Math. 42, 577–684.

[156] Nadaraya, E.A. (1965) On nonparametric estimates of density func-

tions and regression curves. Theor.Prob.Appl. 10,186–190.

[157] Neal, R.M. (1996) Bayesian Learning for Neural Networks. New York:

Springer.

[158] Neal, R.M. (1997) Monte Carlo Implementation of Gaussian Process
Models for Bayesian Regression and Classiﬁcation. Technical Report No.
9702, Dept. of Statistics, Univ. of Toronto, Canada.

[159] Negele, J.W. & Orland, H. (1988) Quantum Many–Particle Systems.
Frontiers In Physics Series (Vol. 68), Redwood City, CA: Addison–
Wesley.

174

[160] Nitzberg, M. & Shiota T. (1992) Nonlinear Image Filtering With Edge
and Corner Enhancement. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence. 14, (8) 862-833.

[161] O’Hagen, A. (1994) Kendall’s advanced theory of statistics, Vol. 2B:

Bayesian inference. London: Edward Arnold.

[162] Olshausen, B.A. & Field, D.J. (1995) Natural Image Statistics and
Eﬃcient Coding. Workshop on Information Theory and the Brain, Sept.
4–5, 1995, University of Stirling. Proceedings published in Network 7,
333-339.

[163] Olshausen, B.A. & Field, D.J. (1996) Emergence of simple–cell recep-
tive ﬁeld properties by learning a spares code for natural images. Nature
381, 607–609.

[164] Opper, M. (1999) Gaussian Processes for Classiﬁcation: Mean Field
Algorithms. Tech Report NCRG/1999/030, Neural Computing Research
Group at Aston University, UK.

[165] Opper, M. & Kinzel, W. (1996) Statistical Mechanics of Generalization.
In Domany, E., van Hemmen, J.L., & Schulten, K. (eds.) Models of
Neural Networks III. New York: Springer Verlag.

[166] Opper, M., & Winther, O. (1999) Mean ﬁeld methods for classiﬁcation
with Gaussian processes. In Kearns, M.S., Solla, S.S., & Cohn D.A.
(eds.) Advances in Neural Information Processing Systems 11., 309–315,
Cambridge, MA: MIT Press.

[167] ´O Ruanaidh, J.J.K. & Fitzgerald W.J. (1996) Numerical Bayesian

Methods Applied to Signal Processing. New York: Springer.

[168] Parzen, E. (1962) An approach to time series analysis.

Ann.Math.Statist. 32, 951–989.

[169] Parzen, E. (1962) On the estimation of a probability function and

mode. Ann.Math.Statist. 33(3).

[170] Parzen, E. (1963) Probability density functionals and reproducing ker-
nel Hilbert spaces. In Rosenblatt, M.(ed.) Proc. Symposium on Time
Series Analysis, 155–169, New York: Wiley.

175

[171] Parzen, E. (1970) Statistical inference on time series by rkhs methods.
In Pyke, R.(ed.) Proc. 12th Biennal Seminar, 1–37, Montreal, Canada:
Canadian Mathematical Congress.

[172] Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems. San Ma-

teo, CA: Morgan Kauﬀmann.

[173] Perona, P. & Malik J. (1990) Scale–Space and Edge Detection Using
Anisotropic Diﬀusion. IEEE Trans. on Pattern Analysis and Machine
Intelligence. 12(7), 629–639.

[174] Perskin, M.E. & Schroeder, D.V. (1995) An Introduction to Quantum

Field Theory. Reading, MA, Addison–Wesley.

[175] Pierre, D.A. (1986) Optimization Theory with Applications. New York:

Dover. (Original edition Wiley, 1969).

[176] Poggio, T. & Girosi, F. (1990) Networks for Approximation and Learn-

ing. Proceedings of the IEEE, Vol 78, No. 9.

[177] Poggio, T., Torre, V., & Koch, C. (1985) Computational vision and

regularization theory. Nature 317, 314–319.

[178] Polak, E. (1997) Optimization. New York: Springer Verlag.

[179] Pollard, D. (1984) Convergence of Stochastic Processes. New York:

Springer Verlag.

[180] Pordt, A. (1998) Random Walks in Field Theory In Meyer–Ortmanns,
H, Kl¨umper A. (eds.) (1998) Field Theoretical Tools for Polymer and
Particle Physics. Berlin: Springer Verlag.

[181] Press, W.H., Teukolsky, S.A., Vetterling, W.T., & Flannery, B.P.
(1992) Numerical recipes in C. Cambridge: Cambridge University Press.

[182] Ryder, L.H. (1996) Quantum Field Theory. Cambridge: Cambridge

University Press.

York: Springer Verlag.

[183] Ring, P., & Schuck, P. (1980) The Nuclear Many–Body Problem. New

176

[184] Ripley, B.D. (1977) Modelling spatial patterns (with discussion). Jour-

nal of the Royal Statistical Society series B 39, 172–212.

[185] Ripley, B.D. (1987) Stochastic Simulation. New York: Wiley.

[186] Ripley, B.D. (1996) Pattern Recognition and Neural Networks. Cam-

bridge: Cambridge University Press.

[187] Robert, C.P. (1994) The Bayesian Choice. New York: Springer Verlag.

[188] Rose, K., Gurewitz, E., & Fox, G.C. (1990) Statistical mechanics and

phase transitions in clustering. Phys. Rev. Lett. 65, 945–948.

[189] Rothe, H.J. (1992) Lattice Gauge Theories. Singapore: World Scien-

tiﬁc.

Wiley.

[190] Rumelhart, D.E., McClelland, J.L., and the PDP Research Group
(1986) Parallel Distributed Processing, vol.1& 2, Cambridge, MA: MIT
Press.

[191] Schervish, M.J. (1995) Theory of Statistics. New York: Springer Verlag.

[192] Sch¨olkopf, B., Burges C., & Smola, A. (1998) Advances in Kernel Meth-

ods: Support Vector Machines. Cambridge, MA: MIT Press.

[193] Schwefel, H.–P. (1995) Evolution and Optimum Seeking. New York:

[194] Silverman, B.W. (1984) Spline smoothing: The equivalent variable ker-
nel method. Ann. Statist. 12, 898–916. London: Chapman & Hall.

[195] Silverman, B.W. (1986) Density Estimation for Statistics and Data

Analysis. London: Chapman & Hall.

[196] Sivia, D.S. (1996) Data Analysis: A Bayesian Tutorial. Oxford: Oxford

University Press.

[197] Skilling, J. (1991) On parameter estimation and quantiﬁed MaxEnt. In
Grandy, W.T. & Schick, L.H. (eds.) Maximum Entropy and Bayesian
Methods. Laramie, 1990, 267 –273, Dordrecht: Kluwer.

177

[198] Smola A.J. & Sch¨olkopf, B, (1998) From regularization operators to
support vector kernels. In: Jordan, M.I., Kearns, M.J., & Solla S.A.
(Eds.): Advances in Neural Information Processing Systems, vol.10.
Cambridge, MA: MIT Press.

[199] Smola A.J., Sch¨olkopf, B, & M¨uller, K–R. (1998) The connection be-
tween regularization operators and support vector kernels. Neural Net-
works 11, 637–649.

[200] Stone, M. (1974) Cross–validation choice and assessment of statistical
predictions. Journal of the Royal Statistical Society B 36, 111-147.

[201] Stone, M. (1977) An asymptotic equivalence of choice of model by
cross–validation and Akaike’s criterion. Journal of the Royal Statistical
Society B 39, 44.

[202] Stone, C.J. (1985) Additive regression and other nonparametric mod-

els. Ann.Statist. 13,689–705.

[203] Tierney, L. (1994) Markov chains for exploring posterior distributions

(with discussion). Annals of Statistics 22, 1701–1762.

[204] Tikhonov, A.N. (1963) Solution of incorrectly formulated problems and

the regularization method. Soviet Math. Dokl. 4, 1035–1038.

[205] Tikhonov, A.N. & Arsenin, V.Y. (1977) Solution of Ill–posed Problems.

Washington, DC: W.H.Winston.

[206] Uhlig, J. (In preparation) PhD Thesis, M¨unster University.

[207] Uhlig, J., Lemm, J., & Weiguny, A. (1998) Mean ﬁeld methods for
atomic and nuclear reactions: The link between time–dependent and
time–independent approaches. Eur. Phys. A 2, 343–354.

[208] Vapnik, V.N. (1982) Estimation of dependencies based on empirical

data. New York: Springer Verlag.

[209] Vapnik, V.N. (1995) The Nature of Statistical Learning Theory. New

York: Springer Verlag.

[210] Vapnik, V.N. (1998) Statistical Learning Theory. New York: Wiley.

178

[211] Vico, G. (1858, original 1710) De antiquissima Italorum sapientia

Naples: Stamperia de’ Classici Latini.

[212] Wahba, G. (1990) Spline Models for Observational Data. Philadelphia:

SIAM.

[213] Wahba, G. (1997) Support vector machines, reproducing kernel Hilbert
spaces and the randomized GACV. Technical Report 984, University of
Wisconsin.

[214] Wahba, G. & Wold, S. 1975) A completely automatic French curve.

Commun. Statist. 4, 1–17.

[215] Watkin, T.L.H., Rau, A., & Biehl, M. (1993) The statistical mechanics

of learning a rule. Rev. Mod. Phys. 65, 499–556.

[216] Watzlawick, P. (ed.) (1984) The Invented Reality. New York: Norton.

[217] Weinstein, S. (1995) The Quantum Theory of Fields. Vol.1 Cambridge:

Cambridge University Press.

[218] Weinstein, S. (1996) The Quantum Theory of Fields. Vol.2 Cambridge:

Cambridge University Press.

[219] Williams, C.K.I. & Barber, D. (1998) Bayesian Classiﬁcation With
Gaussian Processes IEEE Trans. on Pattern Analysis and Machine In-
telligence. 20(12), 1342–1351.

[220] Williams, C.K.I. & Rasmussen, C.E. (1996) Gaussian Processes for
Regression. in Advances in Neural Information Processing Systems 8,
D.S. Touretzky et al (eds.), 515–520, Cambridge, MA: MIT Press.

[221] Winkler, G. (1995) Image Analysis, Random Fields and Dynamic

Monte Carlo Methods. Berlin: Springer Verlag.

[222] Wolpert, D.H. (ed.) (1995) The Mathematics of Generalization. The
Proceedings of the SFI/CNLS Workshop on Formal Approaches to Su-
pervised Learning. Santa Fe Institute, Studies in the Sciences of Com-
plexity. Reading, MA: Addison–Wesley.

[223] Wolpert, D.H. (1996) The Lack of A Priori Distinctions between Learn-

ing Algorithms. Neural Computation 8 (7), 1341-1390.

179

[224] Wolpert, D.H. (1996) The Existence of A Priori Distinctions between

Learning Algorithms. Neural Computation 8 (7), 1391-1420.

[225] Yakowitz, S.J. & Szidarovsky, F. (1985) A Comparison of Kriging With
Nonparametric Regression Methods. J.Multivariate Analysis. 16, 21-53.

[226] Yuille, A.L., (1990) Generalized deformable models, statistical physics

and matching problems. Neural Computation, 2, (1) 1–24.

[227] Yuille, A.L. & Kosowski, J.J. (1994) Statistical Physics Algorithm That

Converge. Neural Computation 6 (3), 341–356.

[228] Yuille, A.L., Stolorz, P., & Utans, J. (1994) Statistical Physics, Mix-
tures of Distributions, and EM Algorithm. Neural Computation, 6 (2),
334–340.

[229] Zhu, S.C. & Yuille, A.L. (1996) Region Competition: Unifying Snakes,
Region Growing, and Bayes/MDL for Multiband Image Segmentation.
IEEE Trans. on Pattern Analysis and Machine Intelligence 18 (9), 884–
900.

[230] Zhu, S.C. & Mumford, D. (1997) Prior Learning and Gibbs Reaction–
Diﬀusion. IEEE Trans. on Pattern Analysis and Machine Intelligence
19 (11), 1236–1250.

[231] Zhu, S.C., Wu, Y.N., & Mumford, D. (1997) Minimax Entropy prin-
ciple and Its Application to Texture Modeling. Neural Computation, 9
(8).

[232] Zinn–Justin, J. (1989) Quantum Field Theory and Critical Phenomena.

Oxford: Oxford Science Publications.

180

0.8
0.6
0.4
0.2
0

0.6

0.4

0.2

0

5
5

5
5

10
10

2

15

10
10

2

15

10

0.4

0.2

0

8

6

10

0.2

0.1

8

6

4

4

5
5

5
5

10

8

6

10

8

6

4

4

10
10

2

15

10
10

2

15

Figure 13: Comparison of initial guesses P (0)(x, y) for a case with two data
points located at (3, 3) and (7, 12) within the intervals y
∈
[1, 10] with periodic boundary conditions. First row: P (0) = ˜CN.
(The
smoothing operator acts on the unnormalized N. The following conditional
normalization changes the shape more drastically than in the example shown
in the second row.) Second row: P (0) = ˜C ˜Pemp. (The smoothing operator
acts on the already conditionally normalized ˜Pemp.) The kernel ˜C is given
by Eq. (680) with C = (K + m2
C = 1.0, and a K of the form of Eq.
(687) with λ0 = λ4 = λ6 = 0, and λ2 = 0.1 (ﬁgures on the l.h.s.) or λ2 = 1.0
(ﬁgures on the r.h.s.), respectively.

[1, 15] and x

CI), m2

∈

181

P

L

-1
-2
-3
-4

10

8

x

6

4

10

8

x

6

4

5
5

y
y

10
10

2

15
15

5
5

y
y

10
10

2

15
15

Energy during iteration

Regression function

0.6
0.4
0.2

2.2

2.15

2.1

2.05

2

1.95

1.9

2

4

6

8

i

10

2

4

6

8

x

10

|

R

R

dy yp(y

x, htrue) =

Figure 14: Density estimation with 2 data points and a Gaussian prior
factor for the log–probability L. First row: Final P and L. Second row:
The l.h.s. shows the energy EL (109) during iteration, the r.h.s. the regres-
sion function h(x) =
dy yPtrue(x, y). The dotted lines
indicate the range of one standard deviation above and below the regression
function (ignoring periodicity in x). The fast convergence shows that the
problem is nearly linear. The asymmetry of the solution between the x–
and y–direction is due to the normalization constraint, only required for y.
(Laplacian smoothness prior K as given in Eq. (687) with λx = λy = 1, λ0
= 0, λ2 = 0.025, λ4 = λ6 = 0. Iteration with negative Hessian A =
H if
positive deﬁnite, otherwise with the gradient algorithm, i.e., A = I. Initial-
ization with L(0) = ln( ˜C ˜Pemp), i.e., L(0) normalized to
dy eL = 1, with ˜C of
Eq. (680) and C = (K + m2
C = 0.1. Within each iteration step the
R
optimal step width η has been found by a line search. Mesh with 10 points
in x-direction and 15 points in y–direction, periodic boundary conditions in
x and y. The 2 data points are (3, 3) and (7, 12).)

CI), m2

−

14

12

10

8

6

4

2

182

P

L

-2.5
-5
-7.5
-10

10

8

x

6

4

10

8

x

6

4

5
5

y
y

10
10

2

15
15

5
5

y
y

10
10

2

15
15

Energy during iteration

Regression function

0.6
0.4
0.2
0

2.6

2.4

2.2

2

2

4

6

8

i

10

2

4

6

8

x

10

Figure 15: Density estimation with 2 data points, this time with a Gaussian
prior factor for the probability P , minimizing the energy functional EP (164).
To make the ﬁgure comparable with Fig. 14 the parameters have been chosen
so that the maximum of the solution P is the same in both ﬁgures (max P =
0.6). Notice, that compared to Fig. 14 the smoothness prior is less eﬀective
for small probabilities. (Same data, mesh and periodic boundary conditions
as for Fig. 14. Laplacian smoothness prior K as in Eq. (687) with λx = λy
= 1, λ0 = 0, λ2 = 1, λ4 = λ6 = 0. Iterated using massive prior relaxation,
i.e., A = K + m2I with m2 = 1.0. Initialization with P (0) = ˜C ˜Pemp, with
˜C of Eq. (680) so P (0) is correctly normalized, and C = (K + m2
C =
1.0. Within each iteration step the optimal factor η has been found by a line
search algorithm.)

CI), m2

14

12

10

8

6

4

2

183

P

L

-1
-2
-3
-4

10

8

x

6

4

10

8

x

6

4

5
5

y
y

10
10

2

15
15

5
5

y
y

10
10

2

15
15

Energy during iteration

Regression function

0.4

0.2

3.2

3

2.8

2.6

2.4

2.2

2

2

4

6

8

i

10

2

4

6

8

x

10

Density estimation with a (

∆3) Gaussian prior factor for
Figure 16:
the log–probability L. Such a prior favors probabilities of Gaussian shape.
(Smoothness prior K of the form of Eq. (687) with λx = λy = 1, λ0 = 0, λ2
= 0, λ4 = 0, λ6 = 0.01. Same iteration procedure, initialization, data, mesh
and periodic boundary conditions as for Fig. 14.)

−

14

12

10

8

6

4

2

184

P

L

-2
-4
-6
-8

10

8

x

6

4

10

8

x

6

4

5
5

y
y

10
10

2

15
15

5
5

y
y

10
10

2

15
15

Energy during iteration

Regression function

0.3
0.2
0.1
0

3.4

3.2

3

2.8

2.6

2

4

6

8

i

10

2

4

6

8

x

10

∆3) Gaussian prior factor for the
Figure 17: Density estimation with a (
probability P . As the variation of P is smaller than that of L, a smaller λ6
has been chosen than in Fig. 17. The Gaussian prior in P is also relatively
less eﬀective for small probabilities than a comparable Gaussian prior in L.
(Smoothness prior K of the form of Eq. (687) with λx = λy = 1, λ0 = 0, λ2
= 0, λ4 = 0, λ6 = 0.1. Same iteration procedure, initialization, data, mesh
and periodic boundary conditions as for Fig. 15.)

−

14

12

10

8

6

4

2

185

True P

True L

10

-5

8

-10

6

x

4

5
5

y
y

10
10

2

15
15

Template 1 (P)

5
5

y
y

10
10

4

2

15
Template 1 (L)

5
5

10
10

2

15
15

Template 2 (P)

4

2

10
10

15
Template 2 (L)

10

-3

-4

-5

8

6

4

10

-2
-4
-6
-8

8

6

4

5
5

5
5

10

8

6

x

10

8

6

10

8

6

0.2

0.1

0

0.1
0.075
0.05
0.025
0

0.15
0.1
0.05
0

5
5

10
10

2

15
15

10
10

4

2

15

true log–density Ltrue =
Figure 18: First row: True density Ptrue (l.h.s.)
log Ptrue (r.h.s.) used for Figs. 21–28. Second and third row: The two
templates t1 and t2 of Figs. 23–28 for P (tP
i , r.h.s.),
respectively, with tL
i . As reference for the following ﬁgures we
x, h) under the true
dy dx p(x)p(y
give the expected test error
p(y
x, htrue) for uniform p(x). It is for htrue equal to 2.23 for template t1 equal
to 2.56, for template t2 equal 2.90 and for a uniform P equal to 2.68.

i , l.h.s.) or for L (tL

x, htrue) ln p(y

i = log tP

R

|

|

|

186

Regression function

12

10

8

6

4

2

2

4

6

8

x

10

Figure 19: Regression function htrue(x) for the true density Ptrue of Fig.
18, deﬁned as h(x) =
dy yPtrue(x, y). The dotted lines
indicate the range of one standard deviation above and below the regression
function.

x, htrue) =

dy yp(y

R

R

|

Empirical density

Conditional empirical density

0.04
0.03
0.02
0.01
0
0
0

10

7.5

1
0.75
0.5
0.25
0
0
0

5

x

2.5

10

7.5

5

x

2.5

5
5

y
y

10
10

5
5

y
y

10
10

15
15

0

15
15

0

Figure 20: L.h.s.: Empirical density N(x, y)/n =
sampled from p(x, y
Corresponding conditional empirical density Pemp(x, y) = (N−

i 1.
−
x, htrue)p(x) with uniform p(x). R.h.s.:
1
X N)(x, y) =
xi). Both densities are obtained from

htrue) = p(y

xi)δ(y

i δ(x

yi)/

xi)

yi)

−

P

P

|

|

i δ(x

i δ(x

i δ(y

i /

−
the 50 data points used for Figs. 21–28.
P

−

−

P

P

P

187

P

L

0.15
0.1
0.05

10

-2
-2.5
-3
-3.5
-4

8

6

x

4

10

8

6

x

4

5
5

y
y

10
10

2

15
15

Energy

5
5

y
y

10
10

2

15
15

Regression function

10

20
Av. training err.

30

i

40

2

4

6

8

x

10

Test error

135
132.5
130
127.5
125
122.5
120

2.7

2.6

2.5

2.4

2.3

2.2

10

20

30

40

i

2.4

10

20

30

i

40

|

n

(1/n)
−
dy dx p(x)p(y

Figure 21: Density estimation with Gaussian prior factor for log–probability
L with 50 data points shown in Fig. 20. Top row: Final solution
x, h) and L = log P . Second row: Energy EL (109) dur-
P (x, y) = p(y
ing iteration and ﬁnal regression function. Bottom row: Average train-
xi, h) during iteration and average test error
ing error
x, h) for uniform p(x).
(Parameters: Zero
−
mean Gaussian smoothness prior with inverse covariance λK, λ = 0.5 and K
of the form (687) with λx = 2, λy = 1, λ0 = 0, λ2 = 1, λ4 = λ6 = 0, massive
prior iteration with A = K + m2I and squared mass m2 = 0.01. Initialized
with normalized constant L. At each iteration step the factor η has been
adapted by a line search algorithm. Mesh with 10 points in x-direction and
15 points in y–direction, periodic boundary conditions in y.)

i=1 log p(yi|
x, htrue) ln p(y
P

R

|

|

12

10

8

6

4

2.65

2.6

2.55

2.5

2.45

188

Energy

Av. training err.

Test error

10

20

30

i

40

Energy

10
20
Av. training err.

30

i

2.4

40

10

20

30

i

40

Test error

1

2

3
4
Energy

5

6

i

7

1

2

3

4

5

6

7

1

2

3

4

5

6

i

2.45

i

7

Av. training err.

Test error

1

2

3

4

i

5

Energy

2

1
4
Av. training err.

3

i

5

1

2

3

4

i

5

Test error

10

30

20
Energy

40

i

50

10

20

30

40

Av. training err.

i

50

2.45

10

20

30

40

i

50

Test error

10

30

20
Energy

40

i

50

10

20

30

40

Av. training err.

i

50

10

20

30

40

i

50

Test error

2.65

2.6

2.55

2.5

2.45

2.65

2.6

2.55

2.5

2.45
2.445
2.44
2.435
2.43
2.425
2.42
2.415

2.65

2.6

2.55

2.5

2.44

2.42

2.4

2.38

2.65

2.6

2.55

2.5

2.7

2.6

2.5

2.4

2.3

2.2

2.7

2.6

2.5

2.4

2.3

2.2

2.17
2.16
2.15
2.14
2.13
2.12
2.11

2.7

2.6

2.5

2.4

2.3

2.2

2.17
2.16
2.15
2.14
2.13
2.12
2.11

2.2

2.15

2.1

2.05

2

1.95

135
132.5
130
127.5
125
122.5
120

135
132.5
130
127.5
125
122.5
120

140

135

130

125

120

135
132.5
130
127.5
125
122.5
120

140

135

130

125

120

160

150

140

130

120

10

20

30

40

i

50

10

20

30

40

i

50

2.45

10

20

30

40

i

50

Figure 22: Comparison of iteration schemes and initialization. First row:
Massive prior iteration (with A = K + m2I, m2 = 0.01) and uniform initial-
H) and uniform initialization.
ization. Second row: Hessian iteration (A =
Third row: Hessian iteration and kernel initialization (with C = K + m2
CI,
m2
C = 0.01 and normalized afterwards). Forth row: Gradient (A = I) with
uniform initialization. Fifth row: Gradient with kernel initialization. Sixth
row: Gradient with delta–peak initialization. (Initial L equal to ln(N/n + ǫ),
10, conditionally normalized. For N/n see Fig. 20). Minimal num-
ǫ = 10−
ber of iterations 4, maximal number of iterations 50, iteration stopped if
L(i)

8. Energy functional and parameters as for Fig. 21.

< 10−

L(i
−

−

1)

|

−

|

189

P

L

0.15

0.1
0.05

-2

10

-3

-4

8

6

x

4

5
5

y
y

10
10

2

15
15

Energy

5
5

y
y

10
10

2

15

Regression function

10

8

6

x

4

2
1
Av. training err.

3

i

4

2

4

6

8

x

10

Test error

134

132

130

128

126

124

122

2.6

2.5

2.4

2.3

1

2

3

i

4

1

2

3

i

4

Density estimation with a Gaussian mixture prior for log–
Figure 23:
probability L with 50 data points, Laplacian prior and the two template func-
tions shown in Fig. 18. Top row: Final solution P (x, y) = p(y
x, h) and L =
log P . Second row: Energy Energy EL (701) during iteration and ﬁnal regres-
sion function. Bottom row: Average training error -(1/n)
xi, h)
x, h)
dy dx p(x)p(y
during iteration and average test error
for uniform p(x). (Two mixture components with λ = 0.5 and smoothness
prior with K1 = K2 of the form (687) with λx = 2, λy = 1, λ0 = 0, λ2
= 1, λ4 = λ6 = 0, massive prior iteration with A = K + m2I and squared
mass m2 = 0.01, initialized with L = t1. At each iteration step the factor
η has been adapted by a line search algorithm. Mesh with lx = 10 points
in x-direction and ly = 15 points in y–direction, n = 2 data points at (3, 3),
(7, 12), periodic boundary conditions in y. Except for the inclusion of two
mixture components parameters are equal to those for Fig. 21. )

|
i=1 log p(yi|
x, htrue) ln p(y
P

−

R

n

|

|

12

10

8

6

4

2.54

2.52

2.5

2.48

2.46

2.44

190

P

L

0.15
0.1
0.05
0

10

-2
-3
-4
-5

8

6

x

4

5
5

y
y

10
10

2

15
15

Energy

5
5

y
y

10
10

2

15
15

Regression function

10

8

6

x

4

2
1
Av. training err.

3

i

4

2

6

4
Test error

8

x

10

170

160

150

140

130

120

3.4

3.2

3

2.8

2.6

2.4

2.2

1

2

3

i

2.4

4

1

2

3

i

4

Figure 24: Using a diﬀerent starting point. (Same parameters as for Fig.
23, but initialized with L = t2.) While the initial guess is worse then that of
Fig. 23, the ﬁnal solution is even slightly better.

12

10

8

6

4

2.9

2.8

2.7

2.6

2.5

191

P

L

0.15
0.1
0.05

10

-2
-2.5
-3
-3.5

8

6

x

4

10

8

6

x

4

5
5

y
y

10
10

2

15
15

Energy

5
5

y
y

10
10

2

15
15

Regression function

2

1
4
Av. training err.

3

i

5

2

4

6

8

x

10

Test error

145

140

135

130

125

2.7

2.6

2.5

2.4

2.3

2.2

1

2

3

4

i

5

1

2

3

4

i

5

Figure 25:
Starting from a uniform initial guess. (Same as Fig. 23, but
initialized with uniform L.) The resulting solution is, compared to Figs. 23
and 24, a bit more wiggly, i.e., more data oriented. One recognizes a slight
“overﬁtting”, meaning that the test error increases while the training error is
decreasing. (Despite the increasing of the test error during iteration at this
value of λ, a better solution cannot necessarily be found by just changing
λ–value. This situation can for example occur, if the initial guess is better
then the implemented prior.)

12

10

8

6

4

2.65

2.6

2.55

2.5

2.45

2.4

192

P

L

0.1
0.075
0.05
0.025

10

-2.5
-3
-3.5
-4
-4.5

8

6

x

4

10

8

6

x

4

5
5

y
y

10
10

2

15
15

5
5

y
y

10
10

2

15
15

Energy

Regression function

2
1
Av. training err.

3

i

4

2

4

6
Test error

8

x

10

134

133

132

131

130

129

128

2.65

2.6

2.55

2.5

1

2

3

i

4

1

2

3

i

4

Figure 26: Large λ. (Same parameters as for Fig. 23, except for λ = 1.0.)
Due to the larger smoothness constraint the averaged training error is larger
than in Fig. 23. The fact that also the test error is larger than in Fig. 23
indicates that the value of λ is too large. Convergence, however, is very fast.

10

8

6

4

2.54
2.53
2.52
2.51
2.5
2.49
2.48

193

P

L

0.3
0.2
0.1
0

10

-1
-2
-3
-4
-5

8

6

x

4

5
5

y
y

10
10

2

15
15

Energy

5
5

y
y

10
10

2

15

Regression function

10

8

6

x

4

2

4

6
Av. training err.

8

10

i

12

2

4

6

8

x

10

Test error

135

130

125

120

115

110

105

2.6

2.4

2.2

2

1.8

2

4

6

8

10

i

12

2

4

6

8

10

i

12

Figure 27: Overﬁtting due to too small λ. (Same parameters as for Fig.
23, except for λ = 0.1.) A small λ allows the average training error to
become quite small. However, the average test error grows already after two
iterations. (Having found at some λ–value during iteration an increasing test
error, it is often but not necessarily the case that a better solution can be
found by changing λ.)

12

10

8

6

4

2

2.54

2.52

2.5

2.48

2.46

2.44

194

P

L

0.15
0.1
0.05
0

10

-2
-3
-4
-5

8

6

x

4

5
5

y
y

10
10

2

15
15

5
5

y
y

10
10

2

15

10

8

6

x

4

Energy

Regression function

4

2
Mixing coefficients

6

8

10

i

2

6

4
Test error

8

x

10

170

160

150

140

130

1

0.8

0.6

0.4

0.2

0

2

4

6

8

10

2

4

6

8

10

i

2.4

i

Figure 28: Example of an approximately stable solution. (Same parameters
as for Fig. 23, except for λ = 1.2, m2 = 0.5, and initialized with L = t2.) A
nearly stable solution is obtained after two iterations, followed by a plateau
between iteration 2 and 6. A better solution is ﬁnally found with smaller
distance to template t1. (The plateau gets elongated with growing mass m.)
The ﬁgure on the l.h.s. in the bottom row shows the mixing coeﬃcients aj of
the components of the prior mixture model for the solution during iteration
(a1, line and a2, dashed).

11
10
9
8
7
6
5
4

2.9

2.8

2.7

2.6

2.5

195


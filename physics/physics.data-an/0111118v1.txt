1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
1
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Probabilistic methods for data fusion

Ali Mohammad–Djafari

Laboratoire des Signaux et Syst`emes (CNRS–SUPELEC–UPS)
´Ecole Sup´erieure d’´Electricit´e
Plateau de Moulon, 91192 Gif–sur–Yvette Cedex, France.
E mail: djafari@lss.supelec.fr

Abstract. The main object of this paper is to show how we can use classical prob-
abilistic methods such as Maximum Entropy (ME), maximum likelihood (ML) and/or
Bayesian (BAYES) approaches to do microscopic and macroscopic data fusion. Actu-
ally ME can be used to assign a probability law to an unknown quantity when we have
macroscopic data (expectations) on it. ML can be used to estimate the parameters of a
probability law when we have microscopic data (direct observation). BAYES can be used
to update a prior probability law when we have microscopic data through the likelihood.
When we have both microscopic and macroscopic data we can use ﬁrst ME to assign a
prior and then use BAYES to update it to the posterior law thus doing the desired data
fusion. However, in practical data fusion applications, we may still need some engineering
feeling to propose realistic data fusion solutions. Some simple examples in sensor data
fusion and image reconstruction using diﬀerent kind of data are presented to illustrate
these ideas.

key words: Data fusion, Maximum entropy, Maximum likelihood, Bayesian data fusion,
EM algorithm.

1.

Introduction

Data fusion is one of the active area of research in many applications such as non
destructive testing (NDT), geophysical imaging, medical imaging, radio-astronomy,
etc. Our main object in this paper is not to focus on any of these applications.
Indeed, we want to show how we can use classical probabilistic methods such as
Maximum Entropy (ME), maximum likelihood (ML) and/or Bayesian (BAYES)
approaches to do data fusion.

First, we consider these three methods separately, and we describe brieﬂy each

method. Then we will see some interrelations between them.

We will see that ME can be used to assign a probability law to an unknown
quantity X when we have macroscopic data (expectations) on it. ML can be used
when we have assigned a parametric probability law, before getting the data, on
X and we want to estimate this parameter from some microscopic data (samples
of X). BAYES can be used to update probability laws, going from priors to
posteriors.

When we have both microscopic and macroscopic data we can use ﬁrst ME to

1

assign a prior and then use BAYES to update it to the posterior law, doing thus
the desired data fusion. In practical data fusion applications, however, we may
still need some engineering feeling to propose realistic data fusion solutions.

2. Short description of the methods

2.1. Maximum Entropy (ME)

ME can be used to assign a probability law to an unknown quantity when we
have macroscopic data (expectations) on it. To see this let note by X a quantity
of interest and try to see when and how we can use ME. We do this through a
given problem.

Problem P1: We have L sensors giving us L values {µl, l = 1, . . . , L}, representing
the mean values of L known functions {φl(X), l = 1, . . . , L} related to the unknown
X:

E {φl(X)} =

φl(x)p(x) dx = µl,

l = 1, . . . , L.

(1)

Z

The question is then how to represent our partial knowledge of X by a probability
law.

Obviously, this problem has not a unique solution. Actually these data deﬁne
a class of possible solutions and we need a criterion to select one of them. The
ME principle can give us this criterion and the problem then becomes:

maximize

S(p) = −

p(x) ln p(x) dx

Z

subject to

φl(x) p(x) dx = µl,

l = 1, . . . , L.

The solution is given by

p(x) =

exp

−

θlφl(x)

=

exp

−θtφ(x)

,

1
Z(θ)

(cid:2)

(cid:3)

1
Z(θ)

#

L

where

Z

L

"

l=1
X

is the partition function and {θ1, . . . , θl} are determined by the following system
of equations:

Z(θ) =

exp

−

θlφl(x)

dx

Z

"

l=1
X

#

−

∂ ln Z(θ)
∂θl

= µl,

l = 1, . . . , L,

See [1, 2] for more discussions.

(2)

(3)

(4)

2

2.2. Maximum Likelihood (ML)

Problem P2: Assume now that we have a parametric form of the probability law
p(x; θ) and a sensor gives us N values x = [x1, . . . , xN ] of X. How to determine
the parameters θ?

Two classical methods for solving this problem are:

− Moments Method (MM): The main idea is to write a set of equations (at least
L) relating the theoretical and empirical moments, and solve them to obtain
the solution:

Gl(θ) = E

X l

=

xl p(x; θ) dx =

xl
j, l = 1, . . . , L

(5)

(cid:8)

(cid:9)

Z

1
N

N

j=1
X

− Maximum Likelihood (ML): Here, the main idea is to consider the data as N
samples of X. Then, writing the expression of p(x; θ) and considering it as a
function of θ, the ML solution is deﬁned as

N

j=1
Y

θ = arg max

{l(θ|x)} with l(θ|x) = p(x; θ) =

p(xj; θ)

(6)

θ

1
Z(θ)

N

j=1
Y

It is interesting to note that, in the case of the generalized exponential families:

b

p(x; θ) =

exp

−

θl φl(x)

=

exp

−θt φ(x)

L

"

l=1
X

1
Z(θ)

#

(cid:2)

(cid:3)

we have

l(θ) =

p(xj ; θ) =

exp

−

θl φl(xj )

1
Z N (θ)

N

L

j=1
X

l=1
X









Then, it is easy to see that the ML solution is the solution of the following system
of equations:

(7)

(8)

(9)

∂ln Z(θ)
∂θl

=

1
N

N

j=1
X

φl(xj ),

l = 1, . . . , L

Comparing equations (4) & (9), we can remark an interesting relation between
these two methods. See also [3] for more discussions.

2.3. ML and incomplete data: EM Algorithm

Problem P3: Consider the problem P2, but now assume that the sensor gives us
M values y = [y1, . . . , yM ] related to the N samples x = [x1, . . . , xN ] of X by a
non invertible relation, y = Ax with M < N . How to determine θ ?

The solution here is still based on the ML. The only diﬀerence is the way to

calculate the solution. In fact we can write

p(x; θ) = p(x|y; θ) p(y; θ),

∀Ax = y.

(10)

3

Taking the expectation of both sides for a given value of θ = thetab′, we have

ln p(y; θ) = Ex|y;θ′ {ln p(x; θ)} − Ex|y;θ′ {ln p(x|y; θ)}

or written diﬀerently:

L(θ) = Q(θ; θ′) − V (θ, θ′).

Note that for a given θ′ and for all θ we have

L(θ) − L(θ′) = [Q(θ; θ′) − Q(θ′; θ′)] + [V (θ; θ′) − V (θ′, θ′)].

(13)

Now, using the Jensen’s inequality [4]

V (θ; θ′) ≤ V (θ′, θ′)

an iterative algorithm, known as Expectation-Maximization (EM), is derived:

E:

M:

(k)

Q

θ;

θ

(cid:18)
(k+1)

(cid:19)

b





θ

b

= Ex|y;θ(k) {ln p(x; θ)}

= arg max

Q

θ;

θ

θ (cid:26)

(cid:18)

(k)

(cid:19)(cid:27)

b

This algorithm insures to converge to a local maximum of the likelihood.

It is interesting to see that in the case of the generalized exponential families

(7), the algorithm becomes:

N

j=1
X

Step E:

Q(θ; θ′) = Ex|y;θ′ {ln p(x; θ)} = −N ln Z(θ) −

θtEx|y;θ′ {φ(xj )}

Step M:

−

∂ln Z(θ)
∂θl

=

1
N

N

j=1
X

E

xj |y;θ(k) {φl(xj )} ,

l = 1, . . . , L

(16)

Compare this last equation with those of (4) and (9) to see still some relations
between ME, ML and the EM algorithms.

Problem P4: Consider now the same problem P3 where we want to estimate
not only θ but also x. We can still use the EM algorithm with the following
modiﬁcation:

(11)

(12)

(14)

(15)

(17)

E:

Q

θ;

θ

= E

ln p(x; θ)|y;

θ

(k)






M:

(cid:18)
x(k)

b

(k+1)

θ
b

b

(cid:19)

(cid:26)

(k)

= E

x|y;

θ

(cid:26)
= arg max

b
θ (cid:26)

(k)

Q

θ;

θ

(cid:27)

(cid:18)

(cid:19)(cid:27)

b

b

(k)

(cid:27)

4

2.4. Bayesian Approach

Problem P5: Consider again problems P3 or P4 but now assume that the obser-
vations y are corrupted by noise: y = Ax + b.

The main tool here is the Bayesian approach where, we use the data-unknown
relation and the noise probability distribution to deﬁne the likelihood p(y|x; θ1) =
pb(y − Ax; θ1) and combine it with the prior p(x; θ2) through the Bayes’ rule to
obtain the posterior law

p(x|y; θ1, θ2) =

p(y|x; θ1) p(x; θ2)
m(y; θ1, θ2)

,

where

m(y; θ1, θ2) =

p(y|x; θ1) p(x; θ2) dx

ZZ

The posterior law p(x|y; θ1, θ2) contains all the information available on x. We
can then use it to make any inference on x. We can for example deﬁne the following
point estimators:

− Maximum a posteriori (MAP):

− Posterior Mean (PM):

b

x = arg max

px|y(x|y; θ1, θ2)

x

(cid:8)

(cid:9)

x = Ex|y {x} =

x px|y(x|y; θ1, θ2) dx

ZZ
− Marginal Posterior Modes (MPM):

b

x = arg max

{p(xi|y; θ1, θ2)} ,

xi

where

b

ZZ

p(xi|y; θ1, θ2) =

px|y(x|y; θ) dx1 . . . dxi−1 . . . dxi+1 . . . dxn

(23)

However, in practice, we face two great diﬃculties:

− How to assign the probability laws p(y|x; θ1) and p(x; θ2)?
− How to determine the parameters θ = (θ1, θ2)?

For the ﬁrst we can use either the ME principle when possible, or any other
invariance properties combined with some practical, scientiﬁc or engineering sense
reasoning. For the second, there are more speciﬁc tools, all based on the joint
posterior probability law

p(x, θ|y) ∝ p(y|x, θ) p(x|θ)p(θ) ∝ p(x, y|θ) p(θ) ∝ p(x|y, θ) p(θ),

(18)

(19)

(20)

(21)

(22)

5

The following are some known schemes:
• Joint Maximum a posteriori (JMAP):

• Generalized Maximum Likelihood (GML):

= arg max
(θ,x)

{p(x, θ|y)}

θ,

x

(cid:17)

(cid:16)

b

b
y 7→ JMAP 7→
7→

x
θ
b
b

xk
k
θ
b
b

o

(cid:27)

x(k) = arg max
x
(k)

= arg max

p(x|y; θ(k−1))

n

o
x(k)|y, θ)p(θ)

p(






θ
b

b

θ

n

y 7→
0
θ

7→

b
GML 7→
7→

θ = arg max

p(y|x) p(x; θ) dx






x = arg max
b

p(x|y;

θ)

b
y 7→ ML 7→

b
θ 7→ MAP 7→

θ (cid:26)Z
x

n

b

o

↑
y

x

b

• Marginalized Maximum Likelihood (MML):

b

• MML-EM:
An analytic expression for p(y; θ) is rarely possible. Consequently, considering
[y, x] as the complete data and y as the incomplete data, we can use the EM
algorithm to obtain the following scheme:





E: Q

θ;

θ

(k)

= Ex|y;θ(k) {ln p(x, y; θ)}

(cid:18)
(k+1)
= arg max
b

(cid:19)

M:

θ

(k)

Q

θ;

θ

θ (cid:26)

(cid:18)

(cid:19)(cid:27)

b

(0)

θ

↓

b
y 7→ ML-EM 7→

θ

(k)

b

y
↓
θ 7→ MAP 7→

x

b

b

b

6

3. Data fusion

In this section we consider some simple data fusion problems and analysis the way
we can use the previous schemes to solve them.
3.1. Sensors without noise

Problem P6: The sensor C1 gives N samples xa = {x1, . . . , xN } of X and stops.
The sensor C2 gives M samples yb = {y1, . . . , yM } related to x by y = Ax + b.
We are asked to predict the unobserved samples xb = {xN +1, . . . , xN +M } of X.

C1 : x1, . . . , xN
C2 :

xa

. . .
ya

xb
. . . ? . . .
y1, . . . , yM
yb

xa 7→
yb 7→

Fusion ?

7→
7→

xb
θ
b
b

We can propose the following solutions:

− Use xa to estimate θ, the parameters of p(x; θ) and use it then to estimate

xb from yb:

θ = arg max

{La(θ) = ln p(xa; θ)}

θ

xb = arg max
b
xb

b

p

xb|yb;
(cid:16)

n

θ

b

(cid:17)o

− Use both xa and yb to estimate xb:

xa 7→ ML 7→

θ 7→ MAP 7→
↑
yb

b

xb

b

θ = arg max

{La(p(xa; θ)}

xb,
(

θ) = arg max
b
θ)

(xb,

{p (xb, θ|xa, yb)}

θ

b

JMAP,GML or ML-EM

b

b

xa 7→ ML 7→

yb 7→
θ 7→
xa 7→
b

7→
7→

xb
θ
b
b

3.2. Fusion of homogeneous data

Problem P7: We have two types of data on the same unknown x, both related to
it through linear models:

x ✲ H 1

x ✲ H 2

b1
❄
✲ y = H 1x + b1
✲ ✐+
b2
❄
✲ z = H 2x + b2
✲ ✐+

7

For example, consider an X ray tomography problem where x represents the mass
density of the object and where y and z represent respectively a high resolution
projection and a low resolution projection.

We can use directly the Bayesian approach to solve this problem:

p(x|y, z) =

p(y, z|x) p(x)
p(y, z)

Actually the main diﬃculty here is to assign p(y, z|x).
If we assume that the
errors associated to the two sets of data are independent then the calculation can
be done more easily. For the purpose of illustration assume the following:

p(y|x; σ2

1) ∝ exp

−

|y − H 1x|2

p(z|x; σ2

2) ∝ exp

−

|z − H 2x|2

(cid:21)

(cid:21)

p(x; m, Σ) ∝ exp

−

1
2σ2
1
1
2σ2
2
1
2

(cid:20)

(cid:20)

(cid:20)

[x − m]tΣ−1[x − m]
(cid:21)
2, m, Σ) are given. Then we can

1, σ2

Indeed, assume that the hyper-parameters (σ2
use, for example, the MAP estimate, given by:

x = arg max

{p(x|y, z)} = arg min
x

x

{J(x) = J1(x) + J2(x) + J3(x)}

with

b

J1(x) =

|y − H 1x|2,

J2(x) =

|z − H 2x|2,

1
2σ2
1

1
2σ2
2

J3(x) =

[x − m]tΣ−1[x − m]

1
2

However, in practical applications, the data come from diﬀerent processes.
3.3. Real data fusion problems

Consider a more realistic data fusion problem, where we have two diﬀerent
kinds of data. As an example assume a tomographic image reconstruction problem
where we have a set of data y obtained by an X ray and a set of data z obtained
by an ultrasound probing system. The X ray data are related to the mass density
x of the matter while the ultrasound data are related to the acoustic reﬂectivity r
of the matter. Indeed, assume that, we have linear relations, both between y and
x and between z and r. Then we have:

x ✲ H 1

r ✲ H 2

b1
❄
✲ y = H 1x + b1
✲ ✐+
b2
❄
✲ z = H 2r + b2
✲ ✐+

8

Assuming that the two sets of data are independant, we can again use the Bayes
rule which now becomes

p(x, r|y, z) =

p(y, z|x, r) p(x, r)
p(y, z)

=

p(y|x) p(z|r) p(x, r)
p(y, z)

with

p(y, z) =

p(y|x) p(z|r) p(x, r) dr dx.

ZZ ZZ

Here also the main diﬃculty is the assignment of the probability laws p(y|x),
p(z|r), and more speciﬁcally p(x, r).

Actually if we could ﬁnd a mathematical relation between r and x, then the
problem would become the same as in the preceding case. To see this, assume that
we can ﬁnd a relation such as rj = g(xi+1 − xi) with g a monotonic increasing
function, from some physical reasons. For example, the fact that in the area where
there are some important changes in the mass density of the matter both x and
r change. Indeed, if g could be a linear function (an unrealistic hypothesis) then
we would have

y = H 1x + b1
z = H 2r + b2
r = Gx




−→

(cid:26)

y = H 1x + b1
z = GH 2r + b2



For more realistic cases we need a method which does not use a physically
based explicit expression of g. One approach proposed and used by Gautier et al.
[5, 6, 7, 8] is based on a compound Markovian model where the body object o is
assumed to be composed of three related quantities:

o = {r, x} = {q, a, x}

where q is a binary vector representing the positions of the discontinuities (edges)
in the body, a a vector containing the reﬂectivity values such that

qj = 0 −→ rj = 0,
qj = 1 −→ rj = aj

(cid:26)

and rj =

(cid:26)

g(xj+1 − xj )
0

|xj+1 − xj| > α

if
otherwise

and g is any monotonic increasing function.

With this model we can write

p(o, r) = p(x, a, q) = p(x|a, q) p(a|q) p(q)

and using the Bayes rule, we have

p(x, a, q|y, z) ∝ p(y, z|x, a, q) p(x, a, q) = p(y, z|x, a, q) p(x|a, q) p(a|q) p(q)

We illustrate this approach by making the following assumptions:

− Conditional independence of y and z: p(y, z|x, a, q) = p(y|x)p(z|a)

9

− Gaussian laws for b1 and b2

p(y|x; σ2

1) ∝ exp

−

|y − H 1x|2

;

p(z|a; σ2

2) ∝ exp

−

|z − H 2a|2

1
2σ2
2

(cid:20)

(cid:21)

1
2σ2
1

(cid:20)

− Bernoulli law for q:

p(q) ∝

i (1 − qi)1−λ
qλ

− Gaussian law for a|q:

(cid:21)

n

i=1
X

p(a|q) ∝ exp

−

, Q = diag[q1, . . . , qn]

1
2σ2
a

atQa
(cid:21)

(cid:20)
p(x|a, q) ∝ exp [−U (x|a, q)]

− Markovian model for x:

Then, based on

p(x, a, q|y, z) ∝ p(y|x) p(z|a) p(x|a, q) p(a|q) p(q)

we can propose the following schemes:

− Simultaneous estimation of all the unknowns with the joint MAP estimation

(JMAP):

(

x,

a,

q) = arg max

{p(x, a, q|y, z)}

(x,a,q)

y 7→
z 7→

JMAP

− First estimate the positions of the discontinuities q and then use them to

b

b

b

estimate x and a :

q = arg max

{p(q|y, z)}

q
a) = arg max
(x,a)




x,
(
b

{p(x, a|y, z,

q)}

y 7→
z 7→

Det.

7→

Est.

y 7→
q 7→
z 7→
b

7→
7→

x
a

b
b

− First estimate the positions of the discontinuities q using only z and then use

b

b


them to estimate x and a :

b

7→
7→
7→

x
a
q
b
b
b

q = arg max

{p(q|z)}

q
a) = arg max
(x,a)




x,
(
b

{p(x, a|y, z,

q)}

z 7→ Det.

7→

Est.

b
− First estimate q and a using only z and then use them to estimate x:



b

b

y 7→
q 7→
z 7→
b

7→
7→

q 7→
a 7→
y 7→
b
b

7→
7→

x
a

b
b

b

Est.

7→

x

q,
(

a) = arg max
q,a
{p(x|y,

x = arg max
b

x

b

(

{p(q, a|z)}

z 7→

Det.
&
Est.

b

a,

q)}

b

b

10

− First estimate only q using z, then estimate a using

q and z, and ﬁnally,

estimate x using

q,

a and y:

b

b

q = arg max

{p(q|z)}

q

a

a = arg max
b
x = arg max
b

x

{p(a|z,
{p(x|y,





q

b

b

q 7→
z 7→
b

b

q)}
a,
b

b

q)}

q 7→
b
a 7→
y 7→
b
b

b

z 7→ Det.

7→

Est.

7→

a

Est.

7→

x

− First estimate only q using z and then estimate x using

q and the data y:

b

q = arg max

{p(q|z)}

q

(

x = arg max
b

x

{p(x|y,

q)}

z 7→ Det.

7→

q

b
q 7→
y 7→
b

b

Est.

7→

x

b

Two more realistic solutions are:

b

b

Proposed method 1:
Estimate r using only z and estimate x and q using

r and y:

z 7→ Est.

7→

r

b

b

Reconstruction

r 7→
y 7→
b

p(r|z) ∝ p(z|r) p(r)

p(x, q|

r, y) ∝ p(y|x) p(x, q|

7→
7→

x
q

b
b
r)

b

For the ﬁrst part, with the assumptions made, we have

with

r = arg max

{p(r|z)} = arg min

{J1(r|z)}

r

b
J1(r|z) = |z − H 2r|2 + λ

(rj+1 − rj)2

b

r

j
X

and for the second part we have

x,
(

q) = arg max
(x,q)

{p(x, q|y,

r)} = arg min
(x,q)

{J2(x, q|y,

r)}

with

b

b

b

J2(x, q|y,

r) = |y−H 1x|2+λ

(1−qj)(xj+1 −xj)2+α1

qj(1−

rj)+α2

qj

rj

b

b
This last optimization is still too diﬃcult to do due to the presence of q and x
together. An easier solution is given below.

b

j
X

j
X

j
X

b

11

Proposed method 2:
Use the ultrasound data z to detect the locations of some of the boundaries and
use X ray data to make an intensity image preserving the positions of these dis-
continuities:

z 7→ Est.

7→

r 7→ qj = |rj |

7→

q

Est.

7→

x

|rj |

j

q 7→
y 7→
b

b

P

b

Here, we made slightly diﬀerent assumptions about the distributions of r and x.
Actually a generalized Gaussian distribution in place of Gaussian gives a good
compromise of discontinuity preservation and easy implementation. A typical
choice, for the ﬁrst case is

b

r = arg max

{p(r|z)} = arg min

r
J1(r|z) = ||z − H 2r||2 + λ1||r||p,

r

{J1(r|z)}

1 < p < 2

with

b

and for the second case is

with

J2(x|y,

{p(x|y,

x = arg max

x
q) = ||y − H 1x||2 + λ2
b
b

q)} = arg min
x

{J2(x|y;

q)}

(1 − qj)|xj+1 − xj |p,

1 < p < 2

b

j
X

b
The aim of this paper is not to go through more details on these methods. The
interested reader should refer to [8, 9].

4. Conclusions

To conclude brieﬂy:

expected values.

− ME can be used when we want to assign a probability law p(x) from some

− ML can be used when we have a parametric form of the probability law p(x, θ)
and we have access to direct observations x of X, and we want to estimate
the parameters θ.

− ML-EM extends the ML to the case of incomplete observations.
− When the observed data are noisy the Bayesian approach is the most appro-

− For practical data fusion problems the Bayesian approach seems to give all

priate.

the necessary tools we need.

− Compound Markov models are convenient models to represent signals and

images in a Bayesian approach of data fusion.

− The Bayesian approach is coherent and easy to understand. However, in real

applications, we have still much to do to implement it:
– Assignment or choice of the prior laws
– Eﬃcient optimization of the obtained criteria
– Estimation of the hyper-parameters
– Interpretation of the obtained results.

12

References

1. A. Mohammad-Djafari, Maximum Entropy and Linear Inverse Problems; A
Short Review, pp. 253–264. Paris, France: Kluwer Academic Publishers,
A. Mohammad-Djafari and G. Demoment ed., 1992.

2. A. Mohammad-Djafari, “Maximum d’entropie et probl`emes inverses en im-

agerie,” Traitement du Signal, pp. 87–116, 1994.

3. A. Mohammad-Djafari and J. Idier, Maximum Likelihood Estimation of the
Lagrange Parameters of the Maximum Entropy Distributions, pp. 131–140.
Seattle, USA: Kluwer Academic Publishers, C.R. Smith, G.J. Erikson and
P.O. Neudorfer ed., 1991.

4. M. Miller and D. Snyder, “The role of likelihood and entropy in incomplete-
data problems: Applications to estimating point-process intensities and toeplitz
constrained covariances,” Proceedings of the IEEE, vol. 75, pp. 892–906, July
1987.

5. S. Gautier, G. Le Besnerais, A. Mohammad-Djafari, and B. Lavayssi`ere, Data
fusion in the ﬁeld of non destructive testing. Santa Fe, U.S.A.: Kluwer Aca-
demic Publishers, K. Hanson ed., 1995.

6. S. Gautier, G. Le Besnerais, A. Mohammad-Djafari, and B. Lavayssi`ere, “Fu-
sion de donn´ees radiographiques et ultrasonores, en vue d’une applicaion en
contrˆole non destructif,” (Clermont-Ferrand), Second international workshop
on inverse problems in electromagnetism and acoustic, May 1995.

7. S. Gautier, Fusion de donn´es gammagraphiques et ultrasonores. Application au
contrˆole non destructif. PhD thesis, Universit´e de Paris-Sud, Orsay, septembre
1996.

8. S. Gautier, J. Idier, A. Mohammad-Djafari, and B. Lavayssi`ere, “Fusion
de donn´ees gammagraphiques et ultrasonores,” in GRETSI 97, (Grenoble,
France), pp. 781–784, 1997.

9. S. Gautier, B. Lavayssi`ere, G. Le Besnerais, and A. Mohammad-Djafari,
“L2+lp deconvolution and ultrasound imaging for non destructive evaluation,”
in QLCAV 97, vol. 1, (Le Creusot), pp. 212–213, 1997.

13


1
0
0
2
 
c
e
D
 
7
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
6
4
0
2
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Submitted to J. Statist. Comput. Simul.

1

Statistical inference and modeling with the
S distribution

Sergej V. Aksenov

Michael A. Savageau∗

Department of Microbiology and Immunology, The University of
Michigan, Ann Arbor, MI 48109†

Abstract

We consider the problem of statistical inference for the S distribution
and introduce new minimum distance estimators for the four parame-
ters of the S distribution using Kolmogorov-Smirnov, Cram´er-von Mises
and related distance metrics. Approximate goodness-of-ﬁt and conﬁdence
intervals for parameters are calculated using bootstrap methods. We dis-
cuss further how the S distribution can be used to solve various prob-
lems of statistical modeling associated with parameter inference, includ-
ing goodness-of-ﬁt tests, Monte Carlo simulations and modeling trends in
the distributions.

Keywords: S distribution, minimum distance estimators, goodness-

of-ﬁt, bootstrap

1

Introduction

Parameter estimation is important for solving various problems associated
with statistical inference, e.g. for hypothesis testing and for Monte Carlo
and stochastic modeling. In practical terms, the very ﬁrst step in such
modeling involves choosing a distribution function (d.f.) for the proba-
bility law that best describes the random process and/or available data.
Very often one is presented with a poorly understood process to model
and with samples of data of realistic (moderate) size, and then this choice
can be far from unique. In other words, several distributions in common
use (e.g., Normal, Logistic, Weibull, Laplace etc.) often can be used with
almost equal success. This diﬃculty can be alleviated by using distribu-
tional families. Such families however tend to be unwieldy mathematically
and even present serious compuational diﬃculties, for example, poles in
the Pearson system of distributions.

∗Corresponding author
†Email: aksenov@umich.edu (Sergej V. Aksenov), savageau@umich.edu (Michael A. Sav-

ageau)

A useful approach for these problems has been developed over the
past ten years. It involves a univariate continuous four-parameter distri-
butional family called the S distribution (Savageau, 1982) that is capable
not only of approximating many central and noncentral unimodal univari-
ate distributions rather well (Voit, 1992), but also of representing an un-
countable multitude of others as its parameters change smoothly (Sorribas
et al., 2000). It includes Exponential, Logistic, Uniform and Linear distri-
butions as special parametric cases. The S distribution derives its name
from the fact that it is based on the theory of S-systems (Savageau, 1976;
Voit, 1991). The versatility and relative mathematical simplicity of the
S distribution prompts for its use in statistical inference problems. We
note here that the errors resulting from approximation seem to be a small
price relative to the advantage of having a “best ﬁt” distribution readily
available for a particular problem. A number of parameter estimation
techniques have been proposed over the years (reviewed brieﬂy below).

In this article we propose minimum distance (MD) estimators for the
S distribution parameters that make use of goodness-of-ﬁt statistics of
supremum (Kolmogorov-Smirnov and Kuiper) and quadratic (Cram´er-
von Mises and Watson) types as distance metrics between empirical d.f.
and S distribution d.f. deﬁned by Equation (1) below. Note that there
is no categorization of data involved. The MD estimators then can be
used in testing the goodness-of-ﬁt hypothesis. We propose bootstrapping
to approximate critical values for a goodness-of-ﬁt test and then calculate
approximate conﬁdence intervals for the parameter estimates. Neither
goodness-of-ﬁt nor accuracy of estimates have previously been evaluated
for S distribution estimations. We illustrate the new method with exam-
ples of parameter inference using data generated from the S distribution.
We conclude with a discussion of the uses of the S distribution in sta-
tistical modeling problems. A computational realization of the proposed
estimation procedure is described elsewhere (Aksenov et al., 2001).

2 The S distribution

The S distribution is deﬁned in terms of its d.f. F (x) (Savageau, 1982),
which is the solution of the following initial value problem (i.v.p.) for an
ordinary diﬀerential equation (o.d.e.)

f (x) =

= α

F g − F h

,

F (x0) = F0

(1)

dF
dx

(cid:16)

(cid:17)

Note that the right-hand side of Equation (1) is also the probability func-
tion (p.f.) f (x), which is thus an algebraic function of the d.f. The S
distribution has four parameters

θ = (g, h, α, x0)

(2)

with x0 for location, α for scale and g and h for shape. Often it is conve-
nient to choose x0 as a median, F0 = 0.5.

The S distribution in Equation (1) is completely speciﬁed by its four
parameters θ. Conditions on the parameters, α > 0 and g < h, ensure

2

(3)

(4)

that F (x) is a proper d.f., i.e. a monotone function of the random variable
x with F (−∞) = 0 and F (∞) = 1.

We can obtain a simple condition on the parameters that provides for
unimodality of the S distribution. Diﬀerentiating the p.f. in (1) for x and
equating to zero we obtain

g
h

= F h−g

One can immediately see that, given g and h, this equation has a real
solution F (and hence mode xm) if g and h are both positive, g > 0 and
h > 0. Otherwise, the S distribution has a half-mode (i.e., is J-shaped).
This follows from the left-hand side of Equation (3) being negative if g and
h have diﬀerent signs, and greater than one if g and h are both negative.
The right-hand side of Equation (3) has a positive real value between 0
and 1 for any g < h.

We can deﬁne skewness of the S distribution by evaluating the d.f. at

the mode (Savageau, 1982):

S = F (xm) =

1/(h−g)

g
h

(cid:16)

(cid:17)

Now the S distribution is skewed to the right if the mode is less than the
median, S < 0.5, is symmetrical if the mode coincides with the median,
S = 0.5, and is skewed to the left if the mode is greater than the median,
S > 0.5. For negative g, the S distribution is skewed to the right.

Moments of the S distribution can be obtained numerically by inte-

grating the p.f.

µ′
r = E(xr) = α

xr

F g(x) − F h(x)

dx

(5)

∞

−∞

Z

(cid:16)

(cid:17)

simultaneously with the solution of the o.d.e. (1).

Quantiles of the S distribution can be obtained by using the fact that
there is a monotone one-to-one relation between the random variable and
the d.f. Thus, we can rewrite the o.d.e. (1) as

dx
dF

=

1
α

1
F g − F h ,

x(F0) = x0

(6)

The solution of this equation can be obtained numerically. A closed form
solution can be found in terms of elementary transcendental functions for
a certain subclass of parameters g and h (Voit and Savageau, 1984) or in
terms of Lerch’s transcendent for all g, h ∈ R (Hern´andez-Bermejo and
Sorribas, 2001). This is done by separating variables in either o.d.e. (1)
or (6) and integrating to obtain the following

x(F ) = x0 +

1
α

F

dt
tg − th

F0

Z

(7)

Voit and Savageau (1984) solved the integral in Equation (7) in terms of
elementary transcendental functions for g, h ∈ R, when g = (hσ − 1)/(σ −
1) and a signed rational number σ 6= 1, or for g, h ∈ R, when g < h = 1
and σ = 1.

3

Hern´andez-Bermejo and Sorribas (2001) represented the integrand in

Equation (7) as an inﬁnite sum to obtain

1
α

∞

F

F0

x(F ) = x0 +

tk(h−g)−gdt

(8)

Xk=0 Z
The sum converges to a ﬁnite value if k(h − g) − g 6= −1, which covers
g, h ∈ R when g < h ≤ 1 or 1 < g < h such that g 6= (hk+1)/(k+1) for k ∈
N. This condition deﬁnes a “generic” quantile solution. In the “generic”
case, the quantile function is expressed in terms of Lerch’s transcendent
as follows:

x = x0 +

1
α(1 − g)

F 1−gΦ

F h−g, 1,

1 − g
h − g

−

(cid:19)

F 1−g

0 Φ

F h−g
0

, 1,

(cid:18)

(cid:18)

(cid:18)

1 − g
h − g

(cid:19)(cid:19)

(9)

where Lerch’s transcendent Φ(z, s, v) is deﬁned by the following series (Magnus
et al., 1966):

Φ(z, s, v) =

|z| < 1,

v 6= 0, −1, . . .

(10)

zk
(v + k)s ,

∞

Xk=0

In the “nongeneric” case, which covers g, h ∈ R when 1 ≤ g < h and
g = (hk + 1)/(k + 1) for k ∈ N, the integral in (8) produces a logarithmic
term that has to be integrated separately. For g = 1, the logarithmic term
is at k∗ = 0 and so the quantile function is

x = x0 +

log

+

1
α  

F
F0

(cid:0)

1 − F h−1
0

1 − F h−1

/
h − 1
(cid:1)
(cid:0)

!

(cid:1)

(11)

and for g > 1, the logarithmic term is at k∗ = (g − 1)/(h − g) and the
quantile function is

x = x0 +

1
α 

log

F
F0

+

∞

F k(h−g)−g+1 − F k(h−g)−g+1
k(h − g) − g + 1

0

(12)

Xk=0,k6=k∗







Note that the “nongeneric” solution is identical to the one found by Voit
and Savageau (1984) for σ = −k.

The explicit quantile function given by Equations (9), (11) and (12) has
several important consequences for both parameter inference and model-
ing. First, it can be shown that limF →0+ x(F ) = constant for g < 1 (the
“generic” case). In other words, there is no inﬁnite tail for the correspond-
ing S distribution, which then becomes left-truncated. The truncation
point can be calculated by letting F = 0 in Equation (9)

x∗ = x0 −

F 1−g
0
α(1 − g)

Φ

(cid:18)

F h−g
0

, 1,

1 − g
h − g

(cid:19)

(13)

Second, the existence of a ﬁnite x∗ calls for care when solving o.d.e. (1)
numerically, because the solution of the o.d.e. is not unique at x∗ where

4

the solution F (x∗) = 0 joins the trivial solution F = 0. Nonuniqueness
can be formally checked by showing that the Lipschitz condition is not
satisﬁed at x∗. Most o.d.e. solvers, whose algorithms assume uniqueness
of the solution, will have trouble converging near x∗. Third, an explicit
expression for the mode of the S distribution, if it exists, is now possible
by substituting (4) into Equations (9), (11) and (12):

xm = x0 +

1
α(1 − g)

(1−g)/(h−g)

Φ

, 1,

g
h

(cid:18)

1 − g
h − g

−

(cid:19)

xm = x0 +

log

g
h

g
h

(cid:18)(cid:16)
, 1,

(cid:17)
1 − g
h − g

F 1−g

0 Φ

F h−g
0

1
α

(cid:18)

(cid:18)

1
F0

1 − F h−1
0

/

(cid:16)
1 −

(cid:17)
g
h

(cid:0)

(cid:1)

(cid:16)

h − 1
(cid:0)

(cid:1)

(cid:19)(cid:19)
1/(h−g)

+

(h−1)/(h−g)

(cid:17)





xm = x0 +

log

1
α

(cid:18)

∞

1
F0

g
h

1/(h−g)

+

(cid:16)
(k(h−g)−g+1)/(h−g)

(cid:17)

g
h

Xk=0,k6=k∗ (cid:18)(cid:16)

(cid:17)

1
k(h − g) − g + 1

(cid:19)

− F k(h−g)−g+1

0

×

(cid:19)

(14)

Finally, the quantile function makes it easy to use a direct inversion
method for the generation of random variates from the S distribution.

3 Methods for parameter inference

Existing techniques for estimating parameters of the S distribution, given
a random sample {xi} i = 1, . . . , n, have been based on graphical, nonlin-
ear regression (Voit, 1992), and maximum likelihood (ML) (Voit, 2000a)
methods. The graphical method is relatively straightforward. As F → 0+
the term F g dominates over the term F h and the plot of ln f vs. ln F is a
straight line ln f = ln α + g ln F with slope g and intercept α. Then, given
F at the inﬂection point, which corresponds to the mode (if it exists), and
the previously estimated g, one can estimate h from Equation (3).

Nonlinear regression can be accomplished with x vs. F of the o.d.e.
or with an equivalent representation in terms of the algebraic equation
f vs. F . Regression of the algebraic equation seems to be faster and
less numerically involved: however, estimating x0 is not possible. During
regression, the residual squared error (r.s.e.) is typically minimized and
data is represented in a categorical form. In a recent example of regression-
based estimation, Sorribas et al. (2000) propose to estimate x0 by the
sample median, to ﬁx one of the parameters (e.g., α as the inverse of the
sample standard deviation), and to ﬁt the remaining pair (e.g., g and h)
using the o.d.e. (1) and suitably categorized data. This procedure helps to
avoid the algorithmic diﬃculties associated with ﬁtting four parameters

5

simultaneously. Fixing some parameters and ﬁtting the others exposed
correlations between the “best-ﬁt” parameters. For example, for a given
random sample, ﬁxing α at increasing values produced pairs of g and h
where g was increasing and h was decreasing. In addition, one observed
uncertainties in the “best-ﬁt” parameters that resulted from optimization
runs being initialized with diﬀerent values, and from sampling variability.
Drawing on the closed-form quantile function given by Equations (9),
(11) and (12), Hern´andez-Bermejo and Sorribas (2001) proposed to use
least-squares ﬁtting of the S distribution quantiles to the sample quan-
tiles. Theoretical S distribution quantiles are evaluated at values of the
empirical d.f., which is a step function with jumps at the data points.

Recently, Voit (2000a) introduced a ML procedure to calculate esti-
mates for g and h. After using the algebraic relationship between the p.f.
and the d.f., the log-likelihood function is

log L(θ) = n log α +

log

F g(xi; θ) − F h(xi; θ)

(15)

n

i=1
X

(cid:16)

(cid:17)

Direct minimization of the function (15) involves numerical solution of the
o.d.e. (1) (unpublished computational realization by Voit and Schwacke;
also by Sorribas, personal communication). However, Voit suggested an
approximate ML method that requires only solution of nonlinear algebraic
equations. First, he replaces the theoretical S distribution d.f. evaluated
at data points F (xi; θ) by the empirical d.f. ˆF that is its consistent es-
timate. Second, he introduces a constraint on parameters in the form of
a ﬁxed integral in the phase space dF/dx vs. F , which makes log L(θ) a
function of only g and h. Diﬀerentiating log L with respect to g and h
and equating the derivatives to zero results in nonlinear equations for g
and h that are then solved iteratively:

0 =

0 =

1
g + 1

1
h + 1

+

−

1
h + 1

1
h − g

+

+

P

P

n

i=1 log ˆF (xi)
n

n

i=1 log ˆF (xi)/(1 − ˆF g−h(xi))
n

(16)

Note that the last point of the ordered sample causes a discontinuity
because ˆF (xn) = 1 by deﬁnition. Equations (16) can still be solved nu-
merically, provided one uses L’Hospital’s rule to evaluate the last term of
the sum:

log y

lim
y→1−

1 − yg−h = lim

y→1−

1/y
(h − g)yg−h−1 =

1
h − g

(17)

In summary, existing techniques for S distribution parameter estima-
tion tend to use categorized data and generally lack goodness-of-ﬁt infor-
mation. These deﬁciencies motivated us to develop MD estimators.

4 Minimum distance estimators

The MD method was introduced by Wolfowitz (1957) and since then has
proved to be a convenient method for strongly consistent parameter es-
to a
timation. The idea of the method is to match the empirical d.f.

6

theoretical one as closely as possible, using a distance function δ(·, ·). In
the context of the S distribution we have the d.f. (1) deﬁned on θ ∈ Θ
where Θ is the following subset of R4:

Θ := {θ : α > 0, g < h, h ∈ R, x0 ∈ R, C(θ, xi) ≤ 0}

(18)

where the nonlinear constraint function C is deﬁned as

C = x0 − mini=1,...,n(xi) +

F 1−g
0
α(1 − g)

Φ

(cid:18)

F h−g
0

, 1,

1 − g
h − g

(cid:19)

(19)

The nonlinear constraint (19) is essential for estimation because it en-
sures that the S distribution is consistent with the data at all times, and
at the optimum vector ˆθ in particular. This means that the truncation
point, which is ﬁnite, is less than the minimum observed data point and
is deﬁned at all data points. Of course this does not
thus that the d.f.
ensure against the possibility that an even lower data point just has not
been observed and thus that the true population should be truncated at
an even lower value of x, or not truncated at all. Note that evaluation
of the constraint function C depends on an accurate and fast method
for calculation of Lerch’s transcendent Φ(z, s, v). This is now possible
with recent advances in convergence acceleration techniques (Aksenov et
al., 2001; Jentschura et al., 1999; Jentschura et al., 2001). The d.f.
is
calculated at xi using Equation (1) as long as the desired solution is not
too close to the truncation point x∗. In the immediate proximity of x∗
one can solve the nonlinear (transcendental) equation (9) for F .

The empirical d.f. is deﬁned as a step function

ˆF (x) =

#(xi ≤ x)
n

(20)

where #(·) signiﬁes the number of xi less than or equal to x and weight
1/n is put on each point; if there are tied observations, proportionately
more weight is put on the unique points.

Given the distance metric δ( ˆF , F ), a MD estimator ˆθ is given by the

solution of the following equation

δ( ˆF (xi), F (xi; ˆθ)) = inf θ∈Θδ( ˆF (xi), F (xi; θ))

(21)

As the distance metric δ, we consider here four goodness-of-ﬁt statis-
tics of the supremum (Kolmogorov-Smirnov and Kuiper) and quadratic
(Cram´er-von Mises and Watson) types (D’Agostino and Stephens, 1986).
This allows us to combine estimation with testing the validity of ﬁt. The
Kolmogorov-Smirnov statistic D is the largest unsigned vertical distance
between ˆF and F , the Kuiper statistic V is the sum of the largest signed
vertical distances, the Cram´er-von Mises statistic W 2 is the integral of
the squared diﬀerences between ˆF and F , and the Watson statistic U 2 is
a modiﬁed version of W 2:

D = supx| ˆF (x) − F (x)|
F (x) − ˆF (x)
V = supx

+ supx

ˆF (x) − F (x)

W 2 = n

(cid:16)
∞

(cid:17)
ˆF (x) − F (x)

2

(cid:16)
dF (x)

(cid:17)

−∞

Z

(cid:16)

(cid:17)

7

U 2 = n

ˆF (x) − F (x)−

∞

−∞

Z
∞

−∞

Z

(cid:16)

(cid:16)
ˆF (x) − F (x)

(cid:17)

2

(cid:19)

dF (x)

dF (x)

(22)

For the one-sample problem that we are dealing with, computational for-
mulas for the statistics can be derived from (22) using the probability
integral transformation z = F (x), where z is uniformly distributed, and
letting zi = F (xi):

D = max

maxi=1,...,n

− zi

, maxi=1,...,n

zi −

(cid:18)

(cid:19)

(cid:18)

(cid:19)(cid:19)

i − 1
n

i
n

(cid:18)

V = maxi=1,...,n

− zi

+ maxi=1,...,n

zi −

i − 1
n

(cid:19)

(cid:18)

W 2 =

1
12n

+

U 2 = W 2 − n

i
n

(cid:18)

n

zi −

i=1 (cid:18)
X
n

 

i=1
X

(cid:19)
2i − 1
2n

2

(cid:19)
2

zi
n

− 0.5

!

The MD estimators ˆθ obtained as a solution of Equations (18), (19)
and (21) and with metrics ˆδ (23) can be used in testing the goodness-of-
ﬁt. Formally, we wish to test the composite hypothesis that the random
sample {xi} comes from the S distribution:

against general alternatives, where F is the class of the S distribution d.f.s

H0 : F ∈ F

F := {F (·, θ) : θ ∈ Θ}

Now given parameter estimates ˆθ, one calculates the empirical d.f.-
based goodness-of-ﬁt statistic ˆδ (23) and compares it with the critical point
corresponding to a speciﬁed signiﬁcance level, typically 0.01 or 0.05. For
the so-called case 0, when the distribution F (x) is completely speciﬁed,
asymptotic distributions of goodness-of-ﬁt statistics are known and critical
points have been tabulated (Stephens, 1970; Stephens, 1974). However,
in the general case when parameters are estimated from data, distribu-
tions of statistics have to be approximated. A relatively straightforward
though computationally-intensive way of obtaining the critical points is
to approximate sampling distributions of goodness-of-ﬁt statistics by the
bootstrap method. The asymptotic validity of the bootstrap method for
MD goodness-of-ﬁt tests was established in (Beran, 1986) and for more
general problems in (Romano, 1988). With the bootstrap method, one can
also calculate approximate conﬁdence intervals for parameter estimates.
The MD estimators have been shown to have an asymptotic distribu-
tion (Sahler, 1970; B¨olthausen, 1977).

The bootstrapping algorithm is as outlined in (Efron and Tibshi-
rani, 1993). One samples B times with replacement, either from an em-
pirical d.f. (20) of the sample (in a nonparametric mode) or from the

(23)

(24)

(25)

8

parametric model with parameters ˆθ (in a parametric mode), and calcu-
lates parameter estimates and goodness-of-ﬁt statistics exactly the same
way as with the original sample. These are now called bootstrap replica-
tions ˆθ∗ and ˆδ∗. The lower and upper critical values for a goodness-of-
ﬁt statistic δ, corresponding to a signiﬁcance level α < 0.5, are then the
k = [(B+1)α/2]th and (B+1−k)th largest values of the ordered bootstrap
replications ˆδ∗, respectively, where [·] signiﬁes taking the integer part.
The observed statistic value ˆδ is then compared with the critical values.
Comparison with the lower critical value ensures against the so called su-
peruniformity when the statistic takes too small a value (Stephens, 1970).
Equivalently, one can calculate the achieved signiﬁcance level (a.s.l.) of
a statistic ˆθ that is simply the empirical quantile based on an ordered
sample of replications ˆδ∗. The a.s.l. is then compared with the speciﬁed
signiﬁcance level of the test.

To obtain a (1 − α)100% equitailed bootstrap-percentile conﬁdence
interval for the parameter estimates, bootstrap replications ˆθ∗ are ordered,
and lower θ∗
up endpoints of the interval are again estimated
by the k = [(B + 1)α/2]th and (B + 1 − k)th largest values. However,
bootstrap-percentile intervals can have substantial coverage error as shown
in the following equation

lo and upper θ∗

P (θlo < θ < θup) = 1 − α + O(f (n))

(26)

The bootstrap-percentile method is ﬁrst-order accurate in that the rate
with which the coverage error goes to zero is f (n) = n−1/2, as the sample
size goes to inﬁnity. To improve the accuracy, the BCa (bias-corrected
and accelerated) method was proposed (Efron, 1987). In this method, the
endpoints θ∗
up are calculated as empirical α1th and α2th quantiles,
respectively,

lo and θ∗

α1 = Φ

ˆz0 +

α2 = Φ

ˆz0 +

(cid:18)

(cid:18)

ˆz0 + zα/2
1 − ˆa(ˆz0 + zα/2)
ˆz0 + z1−α/2
1 − ˆa(ˆz0 + z1−α/2)

(cid:19)

(cid:19)

where Φ is the standard Normal d.f. and zα is the αth quantile of the
standard Normal distribution, i.e. Φ(zα) = α. The bias-correction con-
stant ˆz0 is obtained as the proportion of bootstrapped replications that
are less than the observed value,

ˆz0 = Φ−1

#{ˆθ∗ < ˆθ}
B

!

 

where Φ−1 is the Normal quantile function (i.e., the inverse of the d.f.).
The acceleration connstant ˆa can be estimated in terms of the jackknife
values ˆθ(i) (θs estimated from the sample omitting the ith point):

(27)

(28)

(29)

n
i=1

n
i=1

ˆθ(i)/n − ˆθ(i)

3

ˆa =

6

P
n
i=1

(cid:16)P

(cid:18)

P

(cid:16)P

n
i=1

ˆθ(i)/n − ˆθ(i)

(cid:17)
2

3/2

(cid:19)

(cid:17)

9

The BCa intervals are second-order accurate in that the coverage error
goes to zero with rate f (n) = n−1.

Like all bootstrap estimates, the conﬁdence interval endpoints have
variance that is due to sampling error and bootstrap resampling error. We
can estimate the variance of endpoints (which are sample quantiles) using
the jackknife-after-bootstrap method in which for each ith data point
from the ordered sample, one groups bootstrap resamples that do not
contain that particular point and calculates conﬁdence interval endpoints
exactly as above over that collection of resamples, ˆθB(i) (Efron, 1992).
The estimate of variance is then

var(ˆθ) =

n

n − 1
n

i=1  
X

ˆθB(i) −

ˆθB(i)
n
i=1
n

2

!

P

(30)

One can use Equation (30) to decide if a given number of resamples B is
satisfactory by calculating the coeﬃcient of variation, cv = var1/2(ˆθ)/ˆθ,
as a function of B and choosing a threshold for cv, say 0.1 (which means
we are unwilling to accept more than 10% of contribution of Monte Carlo
error to the estimate).

The BCa intervals can be quite expensive to calculate, especially taking
into account the optimization step in Equations (18), (19) and (21). In
general, on the order of 1000 resamples might be commonly needed to
achieve a small Monte Carlo error (Efron, 1987).

However, one can focus on coverage accuracy of the bootstrap ap-
proximation and, instead of accumulating more bootstrap resamples to
reduce the error, use the number of resamples as a calibration parameter
to achieve a speciﬁed coverage. Such an approach leads to the extreme
bootstrap percetiles method (Lee, 2000). As a ﬁrst step in construct-
ing the equitailed percentile interval of nominal coverage (1 − α)100%,
one solves the following equations for the minimum required number of
resamples B

α/2 =

α/2 =

1
B + 1
1
B + 1

+

−

ˆab3
B
ˆab3
B

Bφ(b − b−1) = b

where b is the positive solution of equation

(31)

(32)

and φ is the standard Normal p.f. The maximum of the two solutions for
Equations (31) and (32) is then the derived minimum number of resam-
ples. Equations (31) and (32) are obtained from asymptotic expansions
of the extreme coverage associated with the bootstrap-percentile method,
and assume the validity of Edgeworth expansions for the bootstrap distri-
butions of the standardized bootstrapped statistic and the smooth model
for the statistic as a function of the mean. The validity of these equa-
tions is thought however to extend to more general statistical function-
als (Lee, 2000) and thus they are likely to be applicable here. The cov-
erage error of the extreme percentiles intervals goes to zero with rate

10

f (n) = n−1/2 log1/2 n, which is slower than with the BCa intervals. the
extreme percentile intervals can however provide a large reduction of com-
putational eﬀort because the minimum required B will typically be much
less than 1000. As with all bootstrap estimates, it is worth looking at
histograms of replications; highly skewed histograms are indicative of in-
accurate estimates of the tails of the bootstrap distributions and of the
requirement for more simulation eﬀort. Also note that the conﬁdence in-
tervals here are the marginal intervals for the parameters θ, constructed
from a multivariate empirical bootstrap distribution. Construction of the
simultaneous conﬁdence regions would be rather awkward given the di-
mension of θ. Conﬁdence regions for two-parameter location and scale
families based on the Kolmogorov-Smirnov statistic have been considered
in (Easterling, 1976; Littel and Rao, 1978).

5 Example: inference from the S distri-
bution data

Here we apply the MD estimators, derived using the above procedure,
to parameter inference for a random sample generated from a speciﬁed S
distribution. We use a computational realization of this procedure that
is a collection of Mathematica and C programs (Aksenov et al., 2001). A
Mathematica notebook documenting all the calculation in this section is
available from the corresponding author.

We generate a random sample of size n = 100 from an S distribu-
tion with the parameters in (2) given by θ = (0.5, 1.6, 1.0, 0.0). For the
particular random sample used here, the seed for the Mathematica ran-
dom number generator was 11235. This distribution is left-truncated with
truncation point x∗ = −1.70745 (see Equation (13)) and unimodal with
the mode at xm = −0.38601 (see Equation (14)).

As a ﬁrst attempt we estimate parameters using a combination of ex-
isting methods:
let ˆx0 be the sample median, ˆα be the inverse of the
standard deviation of the data and ˆg and ˆh be the approximate maximum
likelihood estimates calculated using Equations (16). These estimates and
the four goodness-of-ﬁt statistics (23) are shown in the third column of
Table 1. We do not adjust ˆα and ˆx0 to have better agreement with the
data as advised in (Voit, 2000a) since these estimates serve only as initial
guesses for the optimization. These initial estimates are reasonably close
to the population parameters for this particular sample, but of course we
are more interested in how the estimators behave in the long run when
applied to other samples from the same population. The goodness-of-
ﬁt statistics calculated with these estimates are much larger than those
calculated with the true parameters, but again we do not know how re-
producible this diﬀrence is in the long run.

The MD estimates obtained by using Equations (18), (19) and (21)
with each of the four distance metrics (23) are shown in the last four
columns of Table 1. The values of the goodness-of-ﬁt statistics are sub-
stantially lower than those calculated with the ﬁrst estimates or with the
true parameters. We are now ready for evaluating the goodness-of-ﬁt with

11

the bootstrap method.

The equitailed extreme-percentile conﬁdence intervals with intended
coverage of 95% for MD estimators with the four goodness-of-ﬁt statistics
are shown in Table 2. According to Equations (31) and (32) with α =
0.05, only 39 bootstrap resamples were needed for the approximation. We
performed calculations with nonparametric resampling from the empirical
d.f. and parametric resampling from the S distribution d.f. with MD
estimates ˆθ obtained using the corresponding goodness-of-ﬁt statistics.
Both nonparametric and parametric intervals have similar lengths and
shapes (data not shown). Note that the MD estimates based on the
diﬀerent goodness-of-ﬁt statistics have intervals of diﬀerent lengths. For
example, the quadratic statistics W 2 and U 2 give intervals for h and α
that are wider than those provided by the supremum statistics D and V .
Also, all estimators have the true values inside the intervals, except for the
parametric interval with Kuiper for α, which indicates a possible bias for
this estimator. Observed values of the goodness-of-ﬁt statistics are within
their respective 95% intervals, indicating that the null hypothesis (24)
cannot be rejected at the 0.05 signiﬁcance level. We note that the a.s.l.s
for nonparametric bootstraping with supremum statistics are somewhat
lower than those for parametric bootstrapping, making the nonparametric
tests conservative. This observation is reversed for the quadratic statistics.
An alternative way to calculate approximate conﬁdence intervals is
with the BCa method. Table 3 shows 95% equitailed intervals obtained
with 4000 parametric and nonparametric bootstrap resamples by using
Equations (27), (28) and (29). Note again that the supremum statistics
are more conservative for nonparametric than for parametric tests. The
availablility of 100 times more resamples than with the extreme-percentile
method permits more thorough investigation. For example, bootstrap
distributions of estimates diﬀer radically for the two types of functionals
(data not shown). For supremum functions D and V , distributions of all
estimates are more or less symmetric with moderate tails.
In contrast,
for quadratic functions W 2 and U 2, distributions are highly skewed. This
leads to wide intervals for ˆh and ˆα. Curiously, distributions for ˆg are
bimodal, indicating the presence of at least two local minima in the opti-
mization problem involving quadratic functions. Distributions for ˆx0 are
nearly symmetrical in all cases. High skeweness of distributions is accom-
panied by high variability of bootstrap estimates for the endpoints of the
conﬁdence intervals. The overall variability is expected to settle down at
the level of sampling variability with increasing number of resamples B.
This indeed happens for the upper endpoints of the estimation based on
the Kolmogorov-Smirnov distance function, but not for any of the lower
endpoints, and the pattern is more erratic as we move to the estimates
based on the quadratic functions (data not shown).

Plots of the empirical d.f. and the “best-ﬁt” S distribution d.f. are
shown in Figure 1. While all four MD estimators give visually good ap-
proximations, which is also evident from the close agreement among the
estimates and the population values, the properties of the estimators are
strikingly diﬀerent as discussed above.

12

6 Existing applications of the S distribu-
tion in statistical modeling and suggested
extensions

The ability of the S distribution to approximate diverse distributional
forms suggests its use in various stochastic models that give rise to uni-
variate unimodal distributions. Methods of parameter inference for the S
distribution, including the MD estimators proposed in this article, are of
course critical to any data-based modeling of this sort.

One of the main application areas is Monte Carlo modeling. Speciﬁ-
cally, one might be interested in repeated sampling from an S distribuion
that is the best numerical model for random data. Apart from parameter
estimation, eﬃcient random number generation from an S distribution
is required. An example of S distribution modeling in risk assessment
studies is provided by Voit and Schwacke (2000). They also described an
approximate method to sample from an S distribution, by interpolating
among tabulated S quantiles with a rational function. An exact method
is to use inversion with the quantile functions given by Equations (9),
(11) and (12) (Hern´andez-Bermejo and Sorribas, 2001).
In risk analy-
sis applications, the parameters of risk models are often uncertain and
it is desirable to investigate the sensitivity of risks to these parameters.
A fundamental approach is to assign an S distribution to parameters of
the risk model and simulate it many times in an attempt to evaluate
the statistcal uncertainties in the risk as a function of the input distri-
bution of the parameters. A similar application of S distributions can be
found for hierarchical Monte Carlo simulations in environmental assess-
ment, where distributions of several parameters are conditioned on each
other in a hierarchical fashion. In the analysis of mercury contamination in
king mackerel, this made it possible to obtain contaminant concentrations
more precisely than with marginal distributions that ignore statistical in-
terdependency between model parameters (Voit et al., 1995).

Monte Carlo simulations of a quite diﬀerent nature can be found in the
numerical application of mathematically controlled comparisons (Alves
and Savageau, 2000a; Alves and Savageau, 2000b; Alves and Savageau,
2000c). Mathematically controlled comparison (Savageau, 1972; Irvine,
1991; Hlavacek and Savageau, 1998) is a technique to study in quanti-
tative terms models of complex biological networks with alternative de-
signs. Well-worked applications of this technique led to the discovery of
design principles for biosynthetic pathways (Savageau, 1972), gene net-
works (Savageau, 1974; Savageau, 2001), and immune networks (Irvine
and Savageau, 1985a; Irvine and Savageau, 1985b). Mathematically, the
method is based on S-systems within the power-law formalism (Savageau,
1976), which leads to models of alternative designs that are often amenable
to analytical solution and general conclusions. The numerical exten-
sion is motivated by the need to eliminate some uncertainties associ-
ated with the classical analytical approach (e.g., numerical comparison
of alternatives that depends on speciﬁc parameter values), and also by
the need to address more complicated situations when power-law models
are intractable analytically such as models of elementary signal transduc-

13

tion modules based on covalent modiﬁcation of proteins. In these cases,
the idea is to sample parameters of such models from their (generally
unknown) distributions and to evaluate statistical properties of model
output properties, like steady-state levels of variables and logarithmic
gains. Although parameters were sampled from uniform distributions
in recent applications of numerical mathematically controlled compar-
isons (Alves and Savageau, 2000b; Alves and Savageau, 2000c; Alves and
Savageau, 2000d; Alves and Savageau, 2001), sampling parameters from
suitably chosen S distributions will be more appropriate in situations when
the distributions are clearly not uniform.

Another application of the extroadinary ﬂexibility of the S distribution
in modeling various data structures is the study of distributional trends,
which allows one to make inferences about the dynamics of probabilis-
tic models of some stochastic processes. Such models often include both
stochastic and deterministic components (Voit, 1996). An example of such
an approach is the analysis of trends in the distributions of tree sizes with
their age (Voit and Sorribas, 2000). Observations show that the distribu-
tion of tree trunk diameters changes with their age, even as radically as
reversing the skewness. By combining a deterministic component of the
process (growth function of a tree) with a stochastic one (distribution of
tree trunk diameter in the population) these authors were able to predict
the change in distributional shape as a function of age, which is in good
agreement with the observed data. Along the same lines, Sorribas et al.
(2000) considered growth trends in children (e.g. weight) with the mod-
iﬁcation that the trends of distributional parameters were established by
regression, rather than from a deterministic model. Similar technique also
was used in (Voit et al., 1995; Balthis et al., 1996).

7 Discussion

In this article we addressed an important issue in statistical modeling, that
of statistical inference about a distribution. The S distribution, which has
been demonstrated to be a highly useful tool for various kinds of statist-
cal modeling, so far has lacked an estimator that is relatively straightfor-
ward, that makes a minimum reduction of information in the data, and
that is amenable for goodness-of-ﬁt analyses. The MD estimators that we
propose ﬁll this gap. Several features of the MD estimators make them
viable alternatives to other methods. Under some regularity conditions,
MD estimators are strongly consistent (Sahler, 1970). This result applies
in particular to the supremum and quadratic distance functions consid-
ered in this article, and to the S distribution which has a continuous d.f.
Also, MD estimators are invariant with respect to transformation of the
estimand, a property that they share with ML estimators. Finally, the fol-
lowing feature makes MD estimators especially relevant in the context of
the S distribution. When the hypothesized model does not belong to the
class of parameterized d.f. models that generate the sample (i.e., when the
model is wrong), MD estimators provide the “best approximation” from
the class of models (25). This is fully in the spirit of the S distribution be-
ing the best approximating distribution of the unknown population. This

14

feature is not shared by other estimation methods, including ML and the
method of moments (Parr, 1981). Also, MD estimators are natural can-
didates for use in goodness-of-ﬁt tests if the distance function is used as
a goodness-of-ﬁt statistic, as noted by B¨olthausen (1977).

Given the numerical nature of the S distribution and the estimation
method, large-scale Monte Carlo siumulations will be needed to establish
properties of the estimator and the power of diﬀerent distance metrics
relative to other estimation methods, maximum likelihood in particular.
However, the example we have given of inference from a random sample
generated from an S distribution shows the utility of the new estima-
tor. First, BCa bootstrap conﬁdence intervals seem to require many more
resamples than is thought appropriate for a general case: even the 4000
resamples reported here are clearly not enough to reduce the variance of es-
timates of the conﬁdence intervals endpoints for all parameters. Variance
estimated with jackknife-after-bootstrap is even more erratic for quadratic
than for supremum statistics. Second, bootstrap distributions of MD es-
timates are highly skewed for all functions except Kolmogorov-Smirnov,
which makes it the only one to have conﬁdence intervals of reasonable
length and shape and to be overall more trustworthy. This observation
goes along with the ﬁnding that consonance sets for location and scale pa-
rameters based on the Kolmogorov-Smirnov statistic have some desirable
properties, i.e. they are ﬁnite and convex (Salvia, 1980). Reasons for the
rather erratic behavior of the quadratic distance functions seem related
to the fact that they are generally more sensitive to deviations from the
model than supremum functionals. During bootstrapping, variability of
resamples is more pronounced in the tails of the bootstrap distributions
and that is where the estimation of conﬁdence interval endpoints (as sam-
ple quantiles) takes place. Thus the variability of these estimates should
be greater with the quadratic functionals. For these reasons, the quadratic
functionals in the context of S distributions seem to be less robust for es-
timation purposes, in contrast to what was found for more traditional
location-scale families in Monte Carlo studies (Parr and Schucany, 1980).
Finally, expensive BCa intervals can be replaced by at least ten-fold less
expensive extreme percentiles intervals, with little loss of accuracy.

We have shown that MD estimation coupled with bootstrap analysis of
goodness-of-ﬁt makes the S distribution a valuable tool for various kinds
of Monte Carlo statistical modeling.

Acknowledgements

This work was supported in part by U.S. Public Health Service Grant
RO1-GM30054 from the National Institutes of Health.

References

D’Agostino, R. B., Stephens, M. A., 1986, Goodness-of-ﬁt techniques,

Marcel Dekker, Inc., New York and Basel.

15

Alves, R., Savageau, M. A., 2000, Comparing systemic properties of en-
sembles of biological networks by graphical and statistical methods,
Bioinformatics, 16, 527–533.

Alves, R., Savageau, M. A., 2000, Systemic properties of ensembles of
metabolic networks: application of graphical and statistical methods
to simple unbrunched pathways, Bioinformatics, 16, 534–547.

Alves, R., Savageau, M. A., 2000, Extending the method of mathemati-
cally controlled comparison to include numerical comparisons, Bioin-
formatics, 16, 786–798.

Alves, R., Savageau, M. A., 2000, Eﬀect of overall feedback inhibition in
unbranched biosynthetic pathways, Biophys. J., 79, 2290–2304.

Alves, R., Savageau, M. A., 2001, Irreversibility in unbranched pathways:
preferred positions based on regulatory considerations, Biophys. J.,
80, 1174–1185.

Aksenov, S. V., Savageau, M. A., 2001, Mathematica and C programs for
minimum distance estimation of the S distribution and for calculation
of goodness-of-ﬁt by bootstrap, in preparation.

Balthis, W. L., Voit, E. O., Meaburn, G. M., 1996, Setting prediction
limits for mercury concentrations in ﬁsh having high bioaccumulation
potential, Environmetrics, 7, 429–439.

Beran, R., 1986, Simulated power functions, Ann. Statist., 14, 151–173.

B¨olthausen, E., 1977, Convergence in distribution of minimum-distance

estimators, Metrika, 24 , 215–227.

Easterling, R. G., 1976, Goodness of ﬁt and parameter estimation, Tech-

Efron, B., 1987, Better bootstrap conﬁdence intervals, J. Amer. Statist.

nometrics, 18, 1–9.

Assoc., 82, 171–185.

Efron, B., 1992, Jackknife-after-bootstrap standard errors and inﬂuence

functions, J. Roy. Statist. Soc. Ser. B, 54, 83–127.

Efron, B., Tibshirani, R. J., 1993, An introduction to the bootstrap, Chap-

man & Hall, Inc., New York-London.

Hern´andez-Bermejo, B., Sorribas, A., 2001, Analytical quantile solution
for the S-distribution, random number generation and statistical data
modeling, Biom. J., 43,1007–1025.

Hlavacek, W. S., Savageau, M. A., 1998, Method for determining natural
design principles of biological control circuits, J. Intell. Fuzzy Syst.,
6, 147–160.

Irvine, D. H., 1991, The method of controlled mathematical comparisons,
in Voit, E. O., (Ed.), Canonical nonlinear modeling: S-system ap-
proach to understanding complexity, Van Nostrand Reinhold, New
York, 90–109.

Irvine, D. H., Savageau, M. A., 1985, Network regulation of the immune
response: alternative control points for suppressor modulation of ef-
fector lymphocytes, J. Immun., 134, 2100–2116.

16

Irvine, D. H., Savageau, M. A., 1985, Network regulation of the immune
response: modulation of suppressor lymphocytes by alternative sig-
nals including contrasuppression, J. Immun., 134, 2117–2130.

Jentschura, U. D., Mohr, P. J., Soﬀ, G., Weniger, E. J., 1999, Convergence
acceleration via combined nonlinear-condensation transformations,
Comput. Phys. Comm., 116, 28–54.

Jentschura, U. D., Becher, J., Soﬀ, G., Aksenov, S. V., Savageau,
M. A., Mohr, P. J., 2001, Implementation of the combined nonlinear-
condensation transformation in statistical analysis and experimental
mathematics, in preparation.

Lee, S. M. S., 2000, Nonparametric conﬁdence intervals based on extreme

bootstrap percentiles, Statist. Sinica, 10, 475–496.

Littel, R. C., Rao, P. V., 1978, Conﬁdence regions for location and scale
parameters based on the Kolmogorov-Smirnov goodness of ﬁt statis-
tic, Technometrics, 20, 23–27.

Magnus, W., Oberhettinger, F., Soni, R. P., 1966, Formulas and theorems
for the special functions of mathematical physics, Springer-Verlag
New York, Inc., New York.

Parr, W. C., 1981, Minimum distance estimation: a bibliography, Com-

mun. Statist. Theory Methods, 10, 1205–1224.

Parr, W. C., Schucany, W. R., 1980, Minimum distance and robust esti-

mation, J. Amer. Statist. Assoc., 75, 616–624.

Romano, J. P., 1988, A bootstrap revival of some nonparametric distance

tests, J. Amer. Statist. Assoc., 83, 698–708.

Sahler, W., 1970, Estimation by minimum-discrepancy methods, Metrika,

16, 85–106.

Salvia, A. A., 1980, Some fundamental properties of Kolmogorov-Smirnov

consonance sets, Technometrics, 22, 109–111.

Savageau, M. A., 1972, The behavior of intact biochemical control sys-

tems, Current Top. Cell. Regul., 6, 63–130.

Savageau, M. A., 1974, Comparison of classical and autogenous systems

of regulation in inducible operons, Nature, 252, 546–549.

Savageau, M. A., 1976, Biochemical systems analysis: a study of function

and design in molecular biology, Addison-Wesley, Cambridge, MA.

Savageau, M. A., 1982, A suprasystem of probability distributions, Biom.

J., 24, 323–330.

Savageau, M. A., 2001, Design principles for elementary gene circuits:

elements, methods, and examples, Chaos, 11, 142–159.

Sorribas, A., March, J., Voit, E. O., 2000, Estimating age-related trends in
cross-sectional studies using S-distributions, Statist. Med., 19, 697–
713.

Stephens, M. A., 1970, Use of Kolmogorov-Smirnov, Cram´er-von Mises
and related statistics without extensive tables, J. Roy. Statist. Soc.
Ser. B, 32, 115–122.

17

Stephens, M. A., 1974, EDF statistics for goodness of ﬁt and some com-

parisons, J. Amer. Statist. Assoc., 69, 730–737.

Voit, E. O., (Ed.), 1991, Canonical nonlinear modeling: S-system ap-
proach to understanding complexity, Van Nostrand Reinhold, New
York.

Voit, E. O., 1992, The S-distribution. A tool for approximation and clas-
siﬁcation of univariate, unimodal probability distributions, Biom. J.,
34, 855–878.

Voit, E. O., 1996, Dynamic trends in distributions, Biom. J., 38, 587–603.

Voit, E. O., 2000, A maximum likelihood estimator for shape parameters

of S-distributions, Biom. J., 42, 471–479.

Voit, E. O., Savageau, M. A., 1984, Analytical solutions to a generalized

growth equation, J. Math. Anal. Appl., 103, 380–386.

Voit, E. O., Schwacke, L. H., 2000, Random number generation from
right-skewed, symmetric, and left-skewed distributions, Risk Anal.,
20, 59–71.

Voit, E. O., Sorribas, A., 2000, Computer modeling of dynamically chang-
ing distributions of random variables, Math. Comput. Model., 31,
217–225.

Voit, E. O., Balthis, W. L., Holser, R. A., 1995, Hierarchical Monte Carlo
modeling with S-distributions: concepts and illustrative analysis of
mercury contamination in king mackerel, Environ. Internat., 21, 627–
635.

Wolfowitz, J., 1957, The minimum distance method, Ann. of Math.

Statist., 28, 75–88.

18

Parameter Population Combined estimate Minimum distance estimate

ˆg
ˆh
ˆα
ˆx0
D
V
W 2
U 2

1
9

0.5
1.6
1.0
0.0
0.0565035
0.0939742
0.0338124
0.0324051

0.810135
1.40772
0.923027
0.0212869
0.233542
0.410878
1.76827
1.74806

KS
0.516187
1.49055
1.18409
-0.00788733
0.0398301

KP
0.504587
1.36926
1.25795
0.0313802

0.0790387

CVM
0.628322
1.57269
1.29237
0.0226832

Wat
0.633029
1.62646
1.25799
0.0117285

0.0197829

0.0197229

Table 1: Parameter estimates from an S distribution sample data (n = 100). Estimates obtained by a combination of existing
methods (Combined estimate) and by minimizing KS (Kolmogorov-Smirnov), Kuiper (KP), Cram´er-von Mises (CVM) or
Watson (Wat) distance metrics (Minimum distance estimate). See text for details of calculation.

KP
(0.40623, 0.952086)
(0.289727, 0.935185)
(1.2042, 2.4769)
(1.13551, 2.1405)
(0.650582, 1.65693)
(1.02877, 1.56108)
(-0.33061, 0.258266)
(-0.27011, 0.250137)

CVM
(0.179366, 0.96643)
(0.208938, 0.988865)
(0.943656, 7.29396)
(0.814594, 3.64494)
(0.404169, 6.02715)
(0.503189, 13.1778)
(-0.178406, 0.159036)
(-0.157073, 0.262181)

Wat
(0.0777369, 1.01187)
(0.203831, 1.15758)
(0.869806, 7.98307)
(0.869556, 22.2877)
(0.398919, 20.7098)
(0.449198, 9.42905)
(-0.258809, 0.164895)
(-0.235997, 0.271462)

Parameter Conﬁdence interval

KS
(0.467339, 0.818564)∗
(0.271795, 0.846612)†
(1.329, 2.17525)
(0.975641, 2.01904)
(0.882613, 1.56611)
(0.946866, 1.551)
(-0.146193, 0.156423)
(-0.199951, 0.261195)
(0.0361164, 0.0718024)
p = 0.846154
(0.0241676, 0.0632652)
p = 0.641026

g

h

α

x0

D

V

W 2

U 2

2
0

(0.0668382, 0.130067)
p = 0.871795
(0.0533178, 0.108985)
p = 0.512821

(0.0108397, 0.0625526)
p = 0.615385
(0.012497, 0.0689029)
p = 0.769231

(0.0120847, 0.0504255)
p = 0.666667
(0.0112161, 0.0971751)
p = 0.717949

∗Top entries calculated with nonparametric bootstrap
b†Bottom entries calculated with parametric bootstrap

Table 2: Equitailed extreme-percentile bootstrap conﬁdence intervals. Intended coverage of 95% for minimum distance pa-
rameter estimates from sampled data (n = 100) obtained from the S distribution with parameters g = 0.5, h = 1.6, α = 1.0,
x0 = 0.0. Methods include KS (Kolmogorov-Smirnov), Kuiper (KP), Cram´er-von Mises (CVM) and Watson (Wat) goodness-
of-ﬁt statistics. See text for details of calculation.

KP
(0.204395, 0.693703)
(0.274027, 0.846456)
(0.981214, 1.65894)
(0.960117, 1.90602)
(0.914392, 1.63598)
(0.908877, 1.63787)
(-0.188277, 0.280725)
(-0.182983, 0.386315)

CVM
(0.0257174, 0.888425)
(0.104614, 0.959833)
(1.01331, 6.36995)
(0.915669, 5.68933)
(0.331079, 5.24991)
(0.354572, 7.13847)
(-0.133676, 0.196232)
(-0.190276, 0.251492)

Wat
(0.00721408, 0.955644)
(0.0916488, 1.04173)
(0.96578, 10.1783)
(0.885967, 6.62075)
(0.314168, 4.48109)
(0.353748, 7.24277)
(-0.173835, 0.181506)
(-0.202019, 0.278002)

Parameter Conﬁdence interval

KS
(0.296367, 0.681915)∗
(0.259294, 0.70718)†
(1.13667, 1.79035)
(1.14481, 1.94514)
(0.835117, 1.35433)
(0.846121, 1.31254)
(-0.196492, 0.151388)
(-0.216363, 0.225872)
(0.0282195, 0.0448809)
p = 0.8845
(0.02683, 0.0531105)
p = 0.634

g

h

α

x0

D

V

W 2

U 2

2
1

(0.0511786, 0.0907438)
p = 0.86125
(0.0572763, 0.111431)
p = 0.55325

(0.00993233, 0.0356645)
p = 0.64
(0.00744214, 0.0311884)
p = 0.81

(0.00989892, 0.037251)
p = 0.59575
(0.00707178, 0.0322339)
p = 0.7805

∗Top entries calculated with nonparametric bootstrap
†Bottom entries calculated with parametric bootstrap

Table 3: Equitailed BCa percentile bootstrap conﬁdence intervals. Intended coverage of 95% for minimum distance parameter
estimates from sampled data (n = 100) obtained from the S distribution with parameters g = 0.5, h = 1.6, α = 1.0,
x0 = 0.0. Methods include KS (Kolmogorov-Smirnov), Kuiper (KP), Cram´er-von Mises (CVM) and Watson (Wat) goodness-
of-ﬁt statistics. See text for details of calculation.

-1

1

2

3

x

Figure 1: Empirical (dots) and theoretical S distribution (lines) d.f.s calculated
with the MD estimates. See Table 1 for population and estimated parameters.

F
1

0.8

0.6

0.4

0.2

22


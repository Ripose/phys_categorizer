4
0
0
2
 
y
a
M
 
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
0
0
5
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

MZ-TH/04-05
February 2, 2008

Goodness-of-ﬁt tests in many dimensions

Andr´e van Hameren

Institut f¨ur Physik, Johannes-Gutenberg-Universit¨at,
Staudinger Weg 7, D-55099 Mainz, Germany
andrevh@thep.physik.uni-mainz.de

Abstract

A method is presented to construct goodness-of-ﬁt statistics in many dimensions
for which the distribution of all possible test results in the limit of an inﬁnite number
of data becomes Gaussian if also the number of dimensions becomes inﬁnite. Fur-
thermore, an explicit example is presented, for which this distribution as good as only
depends on the expectation value and the variance of the statistic for any dimension
larger than one.

1 Introduction

Goodness-of-ﬁt (GOF) tests are designed to test the hypothesis that a sample of data is distributed
following a given probability density function (PDF). The sample could, for example, consist of
results of a repeated experiment, and the PDF could represent the theoretical prediction for the
distribution of these results. The test consists of the evaluation of a function of the data, the GOF
statistic, and the qualiﬁcation of this result using the probability distribution of all possible results
when the hypothesis is true, the test-distribution (TD). Despite the consensus that GOF tests are
crucial for the validation of models in the scientiﬁc process, their success is mainly restricted
to one-dimensional cases, that is, to situations in which the data-points have only one degree of
freedom. The quest for GOF tests useful in situations where the number dim of dimensions is
larger than one still continues [1, 2, 3].

In the following, we will see that the difﬁculty with GOF tests in many dimensions is to keep
them distribution-free, that is, to construct them such that the TD is independent of the PDF.1 We
will, however, also see how GOF tests can be constructed such that the asymptotic TD, in the
for any PDF, so that it only
limit of an inﬁnite sample size, has a Gaussian limit for dim

1That is, for binning free tests, which we are considering.

→ ∞

1

depends on the expectation value and the variance of the GOF statistic this limit. Finally, we will
encounter an explicit example for which the asymptotic TD depends, for any PDF, as good as
only on the expectation value and the variance of the statistic for any dim > 1.

2 The structure of goodness-of-ﬁt tests

A GOF statistic is a function TN of the data sample {ωi}N
i=1 constructed such that, under the
hypothesis that the data are distributed in a space Ω following the theoretical PDF P, there is a
number t

such that

∞

lim
N

TN({ωi}N

i=1) = t

.

The initial, na¨ıve, trust in its usefulness stems from the idea that, for a sample of ﬁnite size, the
if the data are distributed following P, and that the value of TN
value of TN should be close to t
is probably not so close to t
if the data are not distributed following P. This idea immediately
∞
leads to the question what is “close”, which can be answered by the test-distribution (TD)

→∞

∞

∞

N(t) =

δ( t − TN({ωi}N

i=1) )

P

Z

P(ωi)dωi ,

(1)

N

Yi=1

P

where each integration variable ωi runs over the whole space Ω, and δ denotes the Dirac distri-
N gives the probability distribution of the value of TN under the hypothesis that the data
bution.
are indeed distributed following P. If it is very low at the value of TN for the empirical data, then
the hypothesis that these data are distributed following P has to be rejected; it would under that
hypothesis be very improbable to get such a value. In fact, knowledge of the value of the number
t

is not necessary. One only needs to know where the bulk of the TD is.
The evaluation of TN({ωi}N
i=1) and the qualiﬁcation of this result with

N constitute a GOF
∞
test. Notice that the TD is also necessary to qualify TN itself: it should consist of a peak around
the expectation value2

P

E(TN) =

TN({ωi}N

i=1)

P(ωi)dωi .

Z

N

Yi=1

If, for example,
value of TN that is equally probable and the test is not capable of distincting them.

N is almost ﬂat, then the test is useless since any data sample will lead to a

P

2.1 Difﬁculty in many dimensions

The difﬁculty with the construction of GOF tests for dim > 1 is that it is in general very hard
to calculate
N. There is a way to avoid this, by using the the distribution from the case that
P is constant. One then needs a mapping ϕ of the data-points such that the determinant of the
Jacobian matrix of this mapping is equal to P:

P

2If E(TN)

= t

then the statistic is biased.

∞

∂Xk(ϕ(ω))
∂Xl(ω)

= P(ω) ,

det
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

6
where Xk(ω) is the k-th coordinate of data-point ω. Under the hypothesis that the original data
are distributed following P, the mapped data are distributed following the uniform distribution.
For dim = 1, this mapping is simply given by the integrated PDF, or probability distribution
function

ϕ(ω) =

P(ω ′) dω ′

,

ω

Z
−

∞

since

Z

δ( t − TN({ϕ(ωi)}N

i=1) )

P(ωi)dωi =

δ( t − TN({ϕi}N

i=1) )

dϕi

N

Yi=1

Z

=

uniform
N

(t) ,

N

Yi=1

where each integration variable ϕi runs from 0 to 1.
limit N
uniform
N

. This asymptotic distribution

P

∞

uniform

P

P

→ ∞

. Tests for which this method can be applied are called distribution-free.

P
uniform
is, for popular tests, known in the
N
is assumed not to be too different from

2.2 Crude solution

P

N. At least an estimate of

For dim > 1, ﬁnding the mapping mentioned before is in general even more difﬁcult than ﬁnding
N can be found using a straightforward Monte Carlo technique: one
P
just has to generate ‘theoretical data samples’ the data-points of which are distributed following
P and make a histogram of the values of TN with these samples. Depending on how accessible
the analytic structure of P is, several techniques exist for generating the theoretical samples. In
the worst case that P is just given as a ‘black box’, the Metropolis-Hastings method can be used,
possibly with its efﬁciency improved by techniques as suggested in [4]. Notice that one does
not need extremely many samples, since one is, for this purpose, interested in the bulk of the
distribution, not in the tails.

Even with modern computer power, however, this Monte Carlo method can become very time
consuming, especially for large N and large dim. In the next section, we will see how practical
GOF statistics for dim > 1 can be constructed for which the asymptotic TD can be obtained in a
more efﬁcient way.

3 Construction of goodness-of-ﬁt statistics in many dimen-

sions

Several GOF statistics for the uniform distribution in many dimensions exist. They are called
discrepancies [5] and intensively studied in the ﬁeld of Quasi Monte Carlo integration [6, 7],
for which one uses low-discrepancy sequences of multi-dimensional integration-points. These
sequences give a faster convergence than expected from the common theory of Monte Carlo
integration, because they are distributed ‘more uniformly’ than uniformly distributed random
sequences; they give a GOF that is ‘unacceptably good’. When dim = 1, discrepancies can be

3

used directly as GOF tests for general PDFs using the ‘mapping method’ mentioned before, and
indeed, the Kolmogorov-Smirnov statistic is equivalent to the ∗-discrepancy, and the Cram´er-von
Mises statistic is equivalent to the L∗2-discrepancy.

In the following, we will have a look at the structure of discrepancies, and we will see how

they can be deformed into GOF statistics for general PDFs.

3.1 The structure of discrepancies

Discrepancies anticipate the fact that, if a sequence {ωi}N
i=1 is uniformly distributed in a space
Ω and N becomes large, then the average of a integrable function over the sequence should
converge to the integral over Ω of the function:

f

h

i

N

f

h

i

→

for N

,

→ ∞

f

N =

h

i

1
N

N

Xi=1

f(ωi)

and

=

f

h

i

ZΩ

f(ω)dω .

H

H

DN =

(cid:18) Z

H

N −

f

|
h

i

f

h

i

|r µ(df)

1/r

.

(cid:19)

Thus a class of functions

and a measure µ on

are chosen, and the discrepancy is deﬁned as

(2)

So it is the integration error measured in a class of functions. For example,
indicator functions of a family

of subsets of Ω with µ such that for r

H

could consist of

S

DN = sup
∈S

S

1
N Xωi

S

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

dω

.

ZS

(cid:12)
(cid:12)
(cid:12)
(cid:12)

→ ∞

∈
In this case, the discrepancy is the maximum error made, if the volume of each subset is estimated
using {ωi}N
i=1. Especially interesting are the quadratic discrepancies [11], for which r = 2, so
that they are completely determined by the two-point function of µ:

DN =

b(ωi, ωj)

1
N2

(cid:18)

N

Xi,j=1

1/2

,

(cid:19)

b(ω1, ω2) = c(ω1, ω2) −

[c(ω1, ω) + c(ω2, ω)] dω +

c(ω, η) dωdη,

ZΩ

ZΩZΩ

c(ω1, ω2) =

f(ω1)f(ω2) µ(df) .

So the discrepancy is the sum of the correlations of all pairs of data-points, measured with cor-
relation function b. If the measure µ itself is completely determined by its two-point function, it
is called Gaussian.

Z
H

4

where

with

where

3.2 From discrepancies to GOF statistics

Discrepancies are usually constructed in order to test the uniformity of sequences in a dim-
dimensional hyper-cube [0, 1)dim. We are interested in more general cases, in which we want
to test whether a sample {ωi}N
i=1 of data-points is distributed in a space Ω following a given
PDF P. We will assume that there exists an invertible mapping ϕ which maps these data-points
[0, 1)dim and for which the determinant J of the Jacobian matrix is known.
onto points ϕ(ωi)
The hypothesis dictates that the mapped points are distributed in the hyper-cube following (P
ϕ−1)/(J
points by {ωi}N

◦
ϕ−1). We will denote this PDF by P itself from now on, the sample of mapped data-

i=1 and the hyper-cube by Ω.

∈

◦

We want to use the idea, introduced before, to analise a data sample {ωi}N
N for different functions f. We will just have to keep in mind that, if {ωi}N

i=1 by looking at
i=1 is distributed

f
h
following P, then

i

where ‘fP’ denotes point-wise multiplication. This and the deﬁnition of the discrepancies lead
us to deﬁne the statistic

→ ∞

→

f

h

i

N

fP

h

i

for N

,

TN =

(cid:18) Z

H

|
h

fQ

N −

fQP

|r µ(df)

i

h

i

(cid:19)

1/r

,

where we inserted the function Q for ﬂexibility. It could be absorbed in the deﬁnition of µ, but
we prefer this formulation, in which we can stick to known examples for µ. We will see later on
that the ideal choice for Q is

Q = 1/√P .

We want to focus on the quadratic discrepancies for which µ is Gaussian from now on. Like in
[11], we shall deﬁne the statistic itself as an average case complexity, and not as a square-root of
an average:

(3)

(4)

TN = N

fQ

N −

fQP

|2 µ(df)

i

h

i

=

Qc(ωi, ωj) − 2

Qc(ωi, ω)

P(ω)dω

(cid:19)

N

ZΩ (cid:18)

Xi=1

|
h

Z
H
N

Xi,j=1

1
N

−N

Qc(ω, η) P(ω)P(η)dωdη ,

(5)

ZΩZΩ

where

Qc(ω1, ω2) = Q(ω1)Q(ω2) c(ω1, ω2) .

(6)

The reason for the extra factor N becomes clear when we calculate the expectation value of TN.
Assuming that the data-points are distributed independently following P, it is given by

E(TN) =

Qc(ω, ω) P(ω)dω −

Qc(ω1, ω2) P(ω1)P(ω2)dω1dω2 .

ZΩ

ZΩZΩ

5

So it is independent of N and the statistic is not biased. In order to write down the variance, we
shorten the notation such that the expectation value can be written as

E(TN) =

Qc1,1P1
i

h

−

Qc1,2P1P2
i

h

,

and the variance is given by

V(TN) =

1 −

(cid:18)

2
N(cid:19)(cid:18)

(Qc1,2Qc1,2 + Qc1,1Qc1,2)P1P2

i

(Qc1,1Qc2,3 + 4Qc1,2Qc2,3)P1P2P3

Qc1,2Qc3,4P1P2P3P4

i

h

−

h
+ 3

h

i

.

(cid:19)

(7)

(8)

Notice that the formulation with Gaussian measures on the function class corresponds to
n=1 of functions, a

a natural interpretation of the average of a square: given a sequence (un)M
sequence (σ2

n=1 of positive weights and a linear operation L, we have

n)M

M

Xn=1

nL(un)2 =
σ2

L

(cid:18)

Z

xnun (cid:19)

exp

−

(cid:18)

2

M

Xn=1

M

Xn=1

x2
n
2σ2
n(cid:19)

M

Yn=1

dxi
2πσ2
n

,

p

to

where the xn-integrals run from −
and weighted with (σ2
written as linear combination of (un)M
In the formulation of the statistic in terms of the two-point function this means that (un, σ2
gives its spectral decomposition:

. So the square averaged over the sequence (un)M
n=1
n=1 is equal to the square averaged over the class of functions that can be
n=1 measured with Gaussian weights with widths (σn)M
n=1.
n)M
n=1

n)M

∞

∞

c(ω1, ω2) =

σ2

nun(ω1)un(ω2) .

(9)

M

Xn=1

The sequence (un)M
positions (un, σ2
n)M
the famous χ2-statistic interpreted in this way there, with (un)M
non-overlapping subsets of Ω, and σ2

n=1 usually consists of an orthonormal basis, and several examples of decom-
n=1 can be found in [11], including cases with M =
. One can also ﬁnd
n=1 a set of indicator functions of

.
A closer look at formula (5) for the GOF statistic reveals that it is highly impractical for the
estimation of the TD with the Monte Carlo method, ﬁrstly because it is quadratic in the number
of data-points and secondly because a dim-dimensional integral has to be calculated for each
data sample.3 One such integral evaluation can be performed within acceptable time-scale using
Monte Carlo integration techniques, by generating integration-points ω distributed following P
N
i=1 Qc(ωi, ω). In order to make an estimate of the TD with a
and calculating the average of
histogram, however, one would have to calculate in the order of a thousand of such integrals.

n = 1/
h

un
i

P

∞

Fortunately, the precise deﬁnition of the statistic, or more explicitly the spectral decomposi-
becomes Gaussian

tion of the two-point function, can be chosen such that the asymptotic TD

3The 2dim-dimensional integral does not depend on the data sample, and has to be calculated only once.

∞

P

6

∞

, as we will see in the next section. This indicates that, for large dim,

for dim >
only
depends on the expectation value and the variance of the statistic. In section 5, we will see an
explicit example for which P inﬂuences
as good as only through the expectation value and
P
the variance for any dim > 1, even before
looks like a Gaussian. So instead of thousands of
dim-dimensional integrals for a histogram, one only has to calculate a dim, two 2dim, a 3dim and
a 4dim-dimensional integral for the expectation value (7) and the variance (8).

∞
P

P

∞

∞

4 Calculation of the asymptotic test-distribution

We approach the calculation of

N through its moment generating function

P

GN(z) = E( ezTN ) .

N can be recovered form GN by the inverse Laplace transformation

exp( S(t; z) )

,

S(t; z) = log GN(z) − tz ,

(10)

where Γ runs from −i
be simpliﬁed by the observation that the statistic (4) does not change if we replace

on the left side of any singularity of GN. The analysis of GN can

to i

N(t) =

P

dz
2πi

ZΓ

∞

∞

un

un −

unQP

1
Q h

←

i

h

in the spectral decomposition, since L(un) =
P

h

= 1). In other words, (4) with µ Gaussian and two-point function (9) is equivalent to

unQ

N −

unQP

is invariant (remember that

i

i

TN = N

fQ

2
N µ(df) =

h

Z
H

i

1
N

N

Xi,j=1

Q(ωi)Q(ωj)c(ωi, ωj) ,

(11)

with µ Gaussian and two-point function

c(ω1, ω2) =

un(ω1) − h

un(ω2) − h

(12)

unQP
Q(ω1) (cid:19)(cid:18)

i

unQP
Q(ω2) (cid:19)

i

.

M

Xn=1

σ2
n(cid:18)

With this decomposition, we can put
spirit of [9, 11], and write

h

i

fQP

equal to zero under the measure. We continue in the

P

h

i

where

TN =

c(ω, η)δN(ω)δN(η) dωdη ,

ZΩZΩ

δN(ω) =

δ(ωi − ω) ,

Q(ω)
√N

N

Xi=1

7

so that, using Gaussian integration rules, we ﬁnd that

ezTN =

e√2z

fδN
h

i µ(df) =

e√2z/Nf(ωi)Q(ωi )

µ(df) ,

(cid:19)

Z
H

N

(cid:18)

Z
H

Yi=1

and

GN(z) = E( ezTN ) =

P e√2z/NfQ

N µ(df) .

i
We shall restrict ourselves to the asymptotic distribution for N

h

Z
H

from now on. We ﬁnd

where we used the fact that that

fQP

can be taken equal to zero under the measure. Substituting

G

(z) = lim

GN(z) =

f2Q2 P
ez
h

→ ∞
i µ(df) ,

∞

N

→∞

i

h

Z

H

f(ω) =

un(ω) − h

and µ(df) =

M

Xn=1

xn(cid:18)

unQP
Q(ω) (cid:19)

i

− x2
2σ2
n

e

M

Yn=1

dxn
2πσ2
n

p

and applying well known Gaussian integration rules, we ﬁnd

G

(z) = det(1 − 2zA)−1/2 ,

(13)

with

∞

An,m = σnσm
h

unumQ2P

− σn
h

unQP

σm
h

umQP

.

i
The asymptotic generating function is now determined up to the positions of its singularities,
which can directly be written in terms of the eigenvalues (λn)M

n=1 of A, since

i

i

G

(z) =

(1 − 2zλn)

M

(cid:18)

Yn=1

∞

−1/2

.

(cid:19)

(14)

Another way to see how the eigenvalues affect the shape of the TD is by considering the cumu-
lants, which are generated by the logarithm of the generating function:

dk log G
dzk

∞

(z = 0) = 2k−1(k − 1)!

M

Xn=1

λk
n .

If A would consist only of a diagonal term plus a diadic term, then the access to its eigenvalues
would be relatively easy. Having in mind that the functions un are orthonormal, this can be
achieved by the choice

so that

An,m = σ2

nδn,m − σn
h

un√P

σm
h

i

um√P

.

i

(15)

Q = 1/√P ,

8

4.1 Gaussian limits

Without loss of generality, we may assume that the weights σn are ordered from large to small.
Then, it is not difﬁcult to see [11] that the eigenvalues (λn)M

n=1 of the matrix (15) satisfy

σ1

λ1

σ2

λ2

σ3

λ3

≥

≥

≥

≥

≥

≥ · · · ≥

σM−1

λM−1

σM

λM .

≥

≥

≥

(16)

It is important to realize that (16) holds whatever P is. The inﬂuence P may have on the shape of
is restricted to the freedom each of the eigenvalues λn has to change value between σn and
P
σn+1. The smallest eigenvalue is non-negative since the matrix A is positive: for any vector x
we have

∞

M

Xn,m=1

An,mxnxm =

σ2
nx2

n −

M

Xn=1
M

2

un√P

σn
h

xn (cid:19)

i

M

M

(cid:18)

Xn=1
M

(cid:18)

Xn=1

nx2
σ2

n −

≥

Xn=1

nx2
σ2

n (cid:19)(cid:18)

h
Xn=1

un√P

2

0 ,

i

(cid:19) ≥

where the ﬁrst inequality is by Schwarz, and the second one is based on the assumption that
(un)M

n=1 is an orthonormal (but not necessarily complete) set and
For the case that P = 1, it has been shown in [8] that

becomes Gaussian if and only if

= 1.

P

h

i

there is a limit for the statistic such that

P

∞

λ2
1
M
n=1 λ2
n

0 .

(17)

P
Typically, this limit may be dim
, as is shown in various examples. For simplicity, we
assume that σ1 = σ2, which is actually the case in most examples in [8]. Using this and (16),
it is easy to see that, if (17) holds, then also λ2
1/(
0, and
that the limit holds for any P. So we may conclude that whenever the spectral decomposition is
chosen such that σ1 = σ2 and there is a limit such that

M
n=2 σ2
n)

M
n=1 σ2
n)

0 and λ2

→ ∞

1/(

P

P

→

→

→

σ2
1
M
n=1 σ2
n

P

→

0 ,

then

becomes Gaussian in this limit.

P

∞

5 Example

The following example of a GOF statistic in many dimensions is based on the diaphony [10, 11],
and has the following spectral decomposition. The basis is the Fourier basis in dim dimensions:

u~n(ω) =

unk ( Xk(ω) )

, nk = 0, 1, 2, . . .

,

k = 1, 2, . . . , dim ,

dim

Yk=1

9

with

u0(x) = 1

, u2n−1(x) = √2 sin(2πnx)

, u2n(x) = √2 cos(2πnx) ,

for n from 1 to

. The corresponding weights are given by

∞
σ~n =

dim

Yk=1

σnk

with

σ0 = 1

, σ2n−1 = σ2n =

.

1
n

The two-point function is equal to

c(ω1, ω2) =

σ2

~nu~n(ω1)u~n(ω2) =

c1( Xk(ω1) − Xk(ω2) ) ,

X~n

dim

Yk=1

where

~n =

P

n1=0

P∞

n2 =0 · · ·

P∞
c1(x) = 1 +

ndim=0 and
P∞
π2
3

− 2π2(x mod 1)(1 − x mod 1) .

The only important difference with the two-point function of the diaphony is that there the con-
stant mode, the dim-dimensional basis function which is equal to 1, is missing. This makes sense
since the diaphony is constructed in order to test the uniform distribution and the contribution
of the constant mode cancels in (2). The advantage is that the diaphony is directly given by the
sum of all two-point correlations between the data-points and no integrals of two-point functions
have to be calculated. Notice that this cancellation also appears in (15): the ﬁrst row and column
of the matrix A consist of only zeros if P = 1, since all modes except the constant mode have
zero integral. For a general PDF these cancellations also exist, but not for a single mode, and
hence are not of practical use. For example, f = 1/Q cancels in (4).

It is useful to introduce the function ρ which counts the number of weights with the same

value:

X~n
The numbers ρ(s) increase as function of dim. Using ρ, (14) and (16), the generating function
can be written as

ρ(s) =

δs,1/σ~n .

G

(z) =

(1 − 2z/s2)ρ(s)−1(1 − 2zλs)

(cid:18)

∞
Ys=1

∞

−1/2

,

(cid:19)

where the numbers λs depend on the PDF under consideration, but are, following (16), restricted
by the relation

1/s2 > λs

1/(s + 1)2 .

≥

In order to ﬁnd the probability density
performed on G
extracting the ﬁrst and the second order terms in z:

, the inverse Laplace transformation (10) has to be
. The logarithm of the product can best be evaluated as described in [12], by

P

∞

∞

log G

(z) = Ez + 1

2Vz2 + ∞
Xs=1

∞

( gs(z) − g ′s(0)z − 1

2g ′′s (0)z2 ) ,

(18)

10

exact

one term

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

P

∞

2

4

6

8

10

12

14

Figure 1:

(t) for dim = 1 and P = 1 using (19) and (18) with one term.

where gs(z) = − ρ(s)−1
2 log(1 − 2zλs), and E and V are the expectation value
and the variance of the statistic. For the case that P = 1, so that λs = 1/(s + 1)2, they can be
calculated directly and are given by

log(1 − 2z/s2) − 1

2

Euniform =

1 +

(cid:18)

dim

π2
3 (cid:19)

− 1 , Vuniform = 2

1 +

− 2 .

dim

π4
45 (cid:19)

(cid:18)

P

by generating the eigenvalues λs at random, uniformly
We want to study the inﬂuence of P on
distributed within their borders, and plotting the result. First, however, we need to ﬁnd out
how many terms in the inﬁnite sum of (18) have to be taken into account in order to obtain
a trustworthy result. This can be done at dim = 1, since we know already that
will tend
to look like a Gaussian for larger values of dim so that the sum must become less important.
Furthermore, there is the advantage that at dim = 1 and P = 1 there exists a simple formula for
the generating function:

P

∞

∞

Guniform

(z) =

∞

√2π2z
sin √2π2z

.

(19)

In Figure 1, we present the result with this formula and with (18) using only one term. With
10 terms, the difference between the curves is invisible and this is the number we further use.
Results for dim = 2 are depicted in Figure 2. As expected, the curves look more ‘Gaussian’ than
the 1-dimensional curve. The crosses represent the case P = 1, and the two continuous curves
represent two cases with ‘typpical’ sets of random eigenvalues. The curves are clearly different,
but if we go over to standardized variables, that is, if we plot

√V

(√V t + E) ,

P

∞

11

P=1

random eigenvalues

random eigenvalues

0.12

0.1

0.08

0.06

0.04

0.02

0

5

P

∞

10

15

20

25

30

35

40

45

Figure 2:

(t) for dim = 2, for P = 1 and two sets of random eigenvalues λs.

so that the expectation value is equal to 0 and the variance is equal to 1, we ﬁnd Figure 3, and
we may conclude that the curves almost only depend on the expectation value and the variance.
Again, we know that this behavior becomes only stronger for higher values of dim because of the
for general P can, to satisfying accuracy, be approximated
Gaussian limit. We conclude that P
by

Vuniform/V

Vuniform/V (t − E) + Euniform

,

(cid:17)

∞
uniform

P

∞

(cid:16)p

, Euniform and Vuniform are the asymptotic test-distribution, the expectation value and

p

uniform

where
the variance for the case that P = 1.

P

∞

6 Conclusion

We have seen how to construct practical GOF statistics to test the hypothesis that a sample of
data is distributed following a given PDF in many dimensions, for which the asymptotic test-
distribution in the limit of an inﬁnite sample size becomes Gaussian in the limit of an inﬁnite
number of dimensions. Furthermore, we have seen an explicit example of such a statistic, for
which the asymptotic test-distribution depends on the PDF as good as only through the expecta-
tion value and the variance of the statistic for any number of dimensions larger than one.

Acknowledgment

This research has been supported by a Marie Curie Fellowship of the European Community
program “Improving Human Research Potential and the Socio-economic Knowledge base” un-

12

P=1

random eigenvalues

random eigenvalues

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

-3

-2

-1

0

1

2

3

4

5

Figure 3: √V

(√V t + E) for the same situations as in Figure 2.

P

∞

der contract number HPMD-CT-2001-00105, and Deutche Forschungsgemeinschaft through the
Graduiertenkolleg ‘Eichtheorien’ at the Johannes-Gutenberg-Universit¨at, Mainz.

References

[1] B. Aslan and G. Zech, Comparison of Different Goodness-of-Fit Tests, Durham 2002, Ad-
vanced statistical techniques in particle physics 166-175 (http://www.ippp.dur.ac.
uk/Workshops/02/statistics/proceedings.shtml).

[2] B. Aslan and G. Zech, A new class of binning free, multivariate goodness of ﬁt tests: the

energy tests (http://arxiv.org/abs/hep-ex/0203010).

[3] R. Raja, A Measure of the Goodness of Fit in Unbinned Likelihood Fits; End of Bayesianism,
eConf C030908:MOcT003, 2003 (http://arxiv.org/abs/physics/0401133).

[4] K.J. Abraham, A New Technique for Sampling Multi-Modal Distributions (http://

arxiv.org/abs/physics/9903044).

[5] R.F. Tichy and M. Drmota, Sequences, Discrepancies and Applications (Springer, 1997).

[6] H. Niederreiter, Random number generations and Quasi-Monte Carlo methods (SIAM

1992).

[7] Monte Carlo & Quasi-Monte Carlo Methods (http://www.mcqmc.org).

13

[8] A. van Hameren, R. Kleiss and J. Hoogland, Gaussian limits for discrepancies: I. Asymptotic
results, Comp. Phys. Comm. 107 (1997) 1-20 (http://arxiv.org/abs/physics/
9708014).

[9] A. van Hameren and R. Kleiss, Quantum ﬁeld theory for discrepancies, Nucl. Phys. B529

[PM] (1998) 737-762 (http://arxiv.org/abs/math-ph/9805008).

[10] P. Hellakalek and H. Niederreiter, The weighted spectral test: Diaphony, ACM Trans.

Model. Comput. Simul. 8, No. 1 (1998), 43-60.

[11] A. van Hameren, Loaded Dice in Monte Carlo: Importance sampling in phase space
integration and probability distributions for discrepancies, PhD-thesis (Nijmegen, 2001)
(http://arxiv.org/abs/hep-ph/0101094).

[12] F. James, J. Hoogland and R. Kleiss, Multidimensional sampling for integration and simu-
lation: measures, discrepancies and quasi-random numbers, Comp. Phys. Comm. 99 (1997)
180-220 (http://arxiv.org/abs/physics/9606309).

14


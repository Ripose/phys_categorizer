Estimating probability densities from short samples:
a parametric maximum likelihood approach

T. Dudok de Wit∗ and E. Floriani∗∗
Centre de Physique Th´eorique, Luminy, case 907,
13288 Marseille cedex 9, France
(June 16, 1998)

8
9
9
1
 
t
c
O
 
2
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
1
0
0
1
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A parametric method similar to autoregressive spectral
estimators is proposed to determine the probability density
function (pdf) of a random set. The method proceeds by
maximizing the likelihood of the pdf, yielding estimates that
perform equally well in the tails as in the bulk of the distri-
bution. It is therefore well suited for the analysis short sets
drawn from smooth pdfs and stands out by the simplicity of
its computational scheme. Its advantages and limitations are
discussed.

PACS numbers :
02.50.Ng Distribution theory and Monte Carlo studies
02.70.Hm Spectral methods

I. INTRODUCTION

There are many applications in which it is necessary
to estimate the probability density function (pdf) from
a ﬁnite sample of n observations
whose
true pdf is f (x). Here we consider the generic case in
which the identically distributed (but not necessarily in-
dependent) random variables have a compact support
xk

∈
The usual starting point for a pdf estimation is the

x1, x2, . . . , xn

[a, b].

{

}

naive estimate

ˆfδ(x) =

δ(x

xi) ,

−

1
n

n

i=1
X

(1)

where δ(.) stands for the Dirac delta function. Although
this deﬁnition has a number of advantages, it is useless for
practical purposes since a smooth functional is needed.
Our problem consists in ﬁnding an estimate ˆf (x) whose
integral over an interval of given length converges toward
. Many solutions have been
that of the true pdf as n
→ ∞
developed for that purpose:
foremost among these are
kernel techniques in which the estimate ˆfδ(x) is smoothed
locally using a kernel function K(x) [1–3]

ˆf (x) =

b

1
w

K

x

y

−
w

(cid:18)

(cid:19)

a
Z

ˆfδ(y) dy ,

(2)

whose width is controlled by the parameter w. The well-
known histogram is a variant of this technique. Although
kernel approaches are by far the most popular ones, the

1

choice of a suitable width remains a basic problem for
which visual guidance is often needed. More generally,
one faces the problem of choosing a good partition. Some
solutions include Bayesian approaches [4], polynomial ﬁts
[5] and methods based on wavelet ﬁltering [6].

An alternative approach, considered by many authors
[7–11], is a projection of the pdf on orthogonal functions

ˆf (x) =

αk gk(x) ,

(3)

k
X

where the partition problem is now treated in dual space.
This parametric approach has a number of interesting
properties: a ﬁnite expansion often suﬃces to obtain a
good approximation of the pdf and the convergence of the
series versus the sample size n is generally faster than for
kernel estimates. A strong point is its global character,
since the pdf is ﬁtted globally, yielding estimates that
are better behaved in regions where the lack of statistics
causes kernel estimates to perform poorly. Such a prop-
erty is particularly relevant for the analysis of turbulent
waveﬁelds, in which the tails of the distribution are of
great interest (e.g. [12]).

These advantages, however, should be weighed against
a number of downsides. Orthogonal series do not pro-
vide consistent estimates of the pdf since for increasing
number of terms they converge toward ˆfδ(x) instead of
the true density f (x) [13]. Furthermore, most series can
only handle continuous or piecewise continuous densities.
Finally, the pdf estimates obtained that way are not guar-
anteed to be nonnegative (see for example the problems
encountered in [14]).

The ﬁrst problem is not a major obstacle, since most
experimental distributions are smooth anyway. The sec-
ond one is more problematic. In this paper we show how
it can be partly overcome by using a Fourier series ex-
pansion of the pdf and seeking a maximization of the
likelihood

ˆL =

log ˆf (x) dx .

(4)

b

a

Z

The problem of choosing an appropriate partition then
reduces to that of ﬁtting the pdf with a positive deﬁnite
Pad´e approximant [15].

Our motivation for presenting this particular paramet-
ric approach stems from its robustness, its simplicity and
the originality of the computational scheme it leads to.
The latter, as will be shown later, is closely related to the

problem of estimating power spectral densities with au-
toregressive (AR) or maximum entropy methods [16–18].
To the best of our knowledge, the only earlier reference to
similar work is that by Carmichael [19]; here we empha-
size the relevance of the method for estimating pdfs and
propose a criterion for choosing the optimum number of
basis functions.

II. THE MAXIMUM LIKELIHOOD APPROACH

The method we now describe basically involves a pro-
jection of the pdf on a Fourier series. The correspondence
between the continuous pdf f (x) and its discrete charac-
teristic function φk is established by [20]

(5)

(6)

(7)

φk =

f (x) ejkx dx

f (x) = 2π

φk e

−jkx ,

+π

−π

Z

+∞

k=−∞
X

where φk = φ∗
C is hermitian [21]. Note that we have
applied a linear transformation to convert the support
from [a, b] to [

−k ∈

π, π].

For a ﬁnite sample, an unbiased estimate of the charac-
teristic function is obtained by inserting eq. 1 into eq. 5,
giving

−

ˆφk =

1
n

n

ejkxi .

i=1
X
The main problem now consists in recovering the pdf
from eq. 6 while avoiding the inﬁnite summation. By
working in dual space we have substituted the partition
choice problem by that of selecting the number of relevant
terms in the Fourier series expansion.

The simplest choice would be to truncate the series at

a given “wave number” p and discard the other ones

ˆf (x) = 2π

ˆφk e

−jkx .

(8)

+p

k=−p
X

Such a truncation is equivalent to keeping the lowest wave
numbers and thus ﬁltering out small details of the pdf.
Incidentally, this solution is equivalent to a kernel ﬁlter-
ing with K(x) = sin(πx)/πx as kernel. This kernel is
usually avoided because it suﬀers from many drawbacks
such as the generation of spurious oscillations.

An interesting improvement was suggested by Burg in
the context of spectral density estimation (see for exam-
ple [16,17]). The heuristic idea is to keep some of the low
wave number terms while the remaining ones, instead of
being set to zero, are left as free parameters:

+∞

ˆf (x) = 2π

−jxk

ˆαk e

k=−∞
X
with ˆαk = ˆφk,

k
|

| ≤

p .

(9)

2

The parameters ˆαk, for
consistently according to some criterion.

k
|

|

> p, are then ﬁxed self-

We make use of this freedom to constrain the solution
to a particular class of estimates. Without any prior in-
formation at hand, a reasonable choice is to select the
estimate that contains the least possible information or
is the most likely. It is therefore natural to seek a maxi-
mization of an entropic quantity such as the sample en-
tropy

ˆH =

ˆf (x) log ˆf (x) dx ,

(10)

or the sample likelihood

ˆL =

log ˆf (x) dx .

(11)

We are a priori inclined to choose the entropy because
our objective is the estimation of the pdf and not that
of the characteristic function. However, numerical inves-
tigations done in the context of spectral density estima-
tion rather lend support to the likelihood criterion [22].
A diﬀerent and stronger motivation for preferring a max-
imization of the likelihood comes from the simplicity of
the computational scheme it gives rise to.

This maximization means that the tail of the charac-

teristic function is chosen subject to the constraint

+π

−

−π

Z

+π

−π

Z

∂ ˆL
∂ ˆαk

= 0,

> p .

k
|

|

(12)

(14)

(15)

From eqs. 9 and 11 the likelihood can be rewritten as

ˆL =

+π

log

2π

−π

 

+∞

−jxk

ˆαk e

dx .

(13)

Z

k=−∞
X
As shown in the appendix, the likelihood is maximized
when the pdf can be expressed by the functional

!

ˆfp(x) =

1
k=−p cke−jkx ,

p

which is a particular case of a Pad´e approximant with
poles only and no zeros [15]. Requiring that ˆfp(x) is real
and bounded, it can be rewritten as

P

ˆfp(x) =

ε0
2π

1 + a1e−jx +
|

1

2 .
|

+ ape−jpx

· · ·
a1, . . . , ap

and of the nor-
The values of the coeﬃcients
{
malization constant ε0 are set by the condition that the
Fourier transform of ˆfp(x) must match the sample char-
acteristic function ˆφk for
k
|

| ≤
This solution has a number of remarkable properties,
some of which are deferred to the appendix. Foremost
among these are its positive deﬁnite character and the

p.

}

simple relationship which links the polynomial coeﬃ-
cients
to the characteristic function on which
they perform a regression. Indeed, we have

a1, . . . , ap

}

{

ˆφk + a1 ˆφk−1 + a2 ˆφk−2 +

· · ·

+ ap ˆφk−p = 0,
p .

k

1

≤

≤

(16)

This can be cast in a set of Yule-Walker equations whose
unique solution contains the polynomial coeﬃcients







=



−



. (17)



ˆφ−1
ˆφ0
ˆφ0
ˆφ1
...
...
ˆφp−1 ˆφp−2

· · ·
· · ·

ˆφ−p+1
ˆφ−p+2
...
ˆφ0

a1
a2
...
ap

· · ·















Advantage can be taken here of the Toeplitz structure of
ˆf (x) dx = 1)
the matrix. The proper normalization (
of the pdf is ensured by the value of ε0, which is given
by a variant of eq. 16

+π
−π
















R

ˆφ1
ˆφ2
...
ˆφp

ˆφ0 + a1 ˆφ−1 + a2 ˆφ−2 +

+ ap ˆφ−p = ε0 .

(18)

· · ·

Equations 15 and 17 illustrate the simplicity of the
method.

III. SOME PROPERTIES

A clear advantage of the method over conventional se-
ries expansions is the automatic positive deﬁnite charac-
ter of the pdf. Another asset is the close resemblance
with autoregressive or maximum entropy methods that
are nowadays widely used in the estimation of spectral
densities. Both methods have in common the estima-
tion of a positive function by means of a Pad´e approx-
imant whose coeﬃcients directly issue from a regression
(eq. 16). This analogy allows us to exploit here some
results previously obtained in the framework of spectral
analysis.

One of these concerns the statistical properties of
the maximum likelihood estimate. These properties are
badly known because the nonlinearity of the problem im-
pedes any analytical treatment. The analogy with spec-
tral densities, however, reveals that the estimates are
asymptotically normally distributed with a standard de-
viation [23,24]

ˆf .

σ ˆf ∝

σ ˆf ∝

ˆf .

This scaling should be compared against that of conven-
tional kernel estimates, for which

q
The key point is that kernel estimates are relatively less
reliable in low density regions than in the bulk of the dis-
tribution, whereas the relative uncertainty of maximum

(19)

(20)

3

likelihood estimates is essentially constant. The latter
property is obviously preferable when the tails of the dis-
tribution must be investigated, e.g. in the study of rare
events.

Some comments are now in order. By choosing a
Fourier series expansion, we have implicitly assumed that
the pdf was 2π-periodic, which is not necessarily the case.
Thus special care is needed to enforce periodicity, since
otherwise wraparound may result [25]. The solution to
this problem depends on how easily the pdf can be ex-
tended periodically. In most applications, the tails of the
distribution progressively decrease to zero, so periodicity
may be enforced simply by artiﬁcially padding the tails
with a small interval in which the density vanishes. We
do this by rescaling the support from [a, b] to an interval
3, 3] [26]. Once
which is slightly smaller than 2π, say [
the Pad´e approximant is known, the [
3, 3] interval is
scaled back to [a, b].

−
−

If there is no natural periodic extension to the pdf,
(for example if f (a) strongly diﬀers from f (b)) then the
choice of Fourier basis functions in eq. 3 becomes ques-
tionable and, not surprisingly, the quality of the ﬁt de-
grades. Even in this case, however, the results can still
be improved by using ad hoc solutions [27].

We mentioned before that the maximum likelihood
method stands out by computational simplicity. Indeed,
a minimization of the entropy would lead to the solution

log ˆfp(x)

∝

p

cke

−jkx ,

(21)

k=−p
X
whose numerical implementation requires an iterative
minimization and is therefore considerably more demand-
ing.

Finally, the computational cost is found to be compa-
rable or even better (for large sets) than for conventional
histogram estimates. Most of the computation time goes
into the calculation of the characteristic function, for
which the number of operations scales as the sample size
n.

IV. CHOOSING THE ORDER OF THE MODEL

The larger the order p of the model is, the ﬁner the
details in the pdf estimate are. Finite sample eﬀects,
however, also increase with p.
It is therefore of prime
importance to ﬁnd a compromise. Conventional criteria
for selecting the best compromise between model com-
plexity and quality of the ﬁt, such as the Final Predic-
tion Error and the Minimum Description Length [16–18]
are not applicable here because they require the series of
characteristic functions
to be normally distributed,
which they are not.

φk

}

{

Guided by the way these empirical criteria have been
chosen, we have deﬁned a new one, which is based on
the following observation: as p increases starting from 0,

the pdfs ˆfp(x) progressively converge toward a station-
ary shape; after some optimal order, however, ripples ap-
pear and the shapes start diverging again. It is therefore
reasonable to compare the pdfs pairwise and determine
how close they are. A natural measure of closeness be-
tween two positive distributions ˆfp(x) and ˆfp+1(x) is the
Kullback-Leibler entropy or information gain [28,29]

ˆI( ˆfp+1, ˆfp) =

ˆfp+1(x) log

dx ,

(22)

+π

−π

Z

ˆfp+1(x)
ˆfp(x)

which quantiﬁes the amount of information gained by
changing the probability density describing our sample
from ˆfp to ˆfp+1. In other words, if Hp (or Hp+1) is the
hypothesis that x was selected from the population whose
probability density is ˆfp ( ˆfp+1), then ˆI( ˆfp+1, ˆfp) is given
as the mean information for discriminating between Hp+1
and Hp per observation from ˆfp+1 [28].

≡

Notice that the information gain is not a distance be-
tween distributions; it nevertheless has the property of
being non negative and to vanish if and only if ˆfp
ˆfp+1.
We now proceed as follows : starting from p = 0 the order
is incremented until the information gain reaches a clear
minimum; this corresponds, as it has been checked nu-
merically, to the convergence toward a stationary shape;
the corresponding order is then taken as the requested
compromise. Clearly, there is some arbitrariness in the
deﬁnition of a such a minimum since visual inspection
and common sense are needed. In most cases, however,
the solution is evident and the search can be automated.
Optimal orders usually range between 2 and 10; larger
values may be needed to model discontinuous or com-
plex shaped densities.

V. SOME EXAMPLES

Three examples are now given in order to illustrate the

limits and the advantages of the method.

A. General properties

First, we consider a normal distribution with exponen-
tial tails as often encountered in turbulent waveﬁelds. We
simulated a random sample with n = 2000 elements and
the main results appear in Fig. 1.

The information gain (Fig. 1b) decreases as expected
until it reaches a well deﬁned minimum at p = 7, which
therefore sets the optimal order of our model. Since the
true pdf is known, we can test this result against a com-
mon measure of the quality of the ﬁt, which is the Mean
Integrated Squared Error (MISE)

MISE(p) =

[f (x)

ˆfp(x)]2dx .

(23)

b

a
Z

−

4

The MISE, which is displayed in Fig. 1b, also reaches
a minimum at p = 8 and thus supports the choice of
the information gain as a reliable indicator for the best
model. Tests carried out on other types of distributions
conﬁrm this good agreement.

Now that the optimum pdf has been found, its charac-
teristic function can be computed and compared with the
measured one, see Fig. 1a. As expected, the two charac-
teristic functions coincide for the p lowest wave numbers
(eq. 16); they diverge at higher wave numbers, for which
the model tries to extrapolate the characteristic function
self-consistently. The fast falloﬀ of the maximum like-
lihood estimate explains the relatively smooth shape of
the resulting pdf.

Finally, the quality of the pdf can be visualized in
Fig. 1d, which compares the measured pdf with the true
one, and an estimate based on a histogram with 101 bins.
An excellent agreement is obtained, both in the bulk of
the distribution and in the tails, where the exponential
falloﬀ is correctly reproduced. This example illustrates
the ability of the method to get reliable estimates in re-
gions where standard histogram approaches have a lower
performance.

B. Interpreting the characteristic function

The shape of the characteristic function in Fig. 1a is
reminiscent of spectral densities consisting of a low wave
number (band-limited) component embedded in broad-
band noise. A straightforward calculation of the expec-
tation of
indeed reveals the presence of a bias which
is due to the ﬁnite sample size

φk
|

|

ˆφk
E[
|

] =
|

φk
|

|

+

γ
√n

,

(24)

{

x
}

where γ depends on the degree of independence between
the samples in
. This bias is illustrated in Fig. 2
for independent variables drawn from a normal distribu-
tion, showing how the wave number resolution gradually
degrades as the sample size decreases.
Incidentally, a
knowledge of the bias level could be used to obtain con-
ﬁdence intervals for the pdf estimate. This would be
interesting insofar no assumptions have to be made on
possible correlations in the data set. We found this ap-
proach, however, to be too inaccurate on average to be
useful.

The presence of a bias also gives an indication of the
smallest scales (in terms of amplitude of x) one can re-
liably distinguish in the pdf. For a set of 2000 samples
drawn from a normal distribution, for example, compo-
nents with wave numbers in excess of k = 3 are hidden
by noise and hence the smallest meaningful scales in the
pdf are of the order of δx = 0.33. These results could
possibly be further improved by Wiener ﬁltering.

C. Inﬂuence of the sample size

To investigate the eﬀect of the sample length n, we
now consider a bimodal distribution consisting of two
normal distributions with diﬀerent means and standard
deviations. Such distributions are known to be diﬃcult
to handle with kernel estimators.

Samples with respectively n = 200, n = 2000 and
n = 20000 elements were generated; their characteristic
functions and the resulting pdfs are displayed in Fig. 3.
Clearly, ﬁnite sample eﬀects cannot be avoided for small
samples but the method nevertheless succeeds relatively
well in capturing the true pdf and in particular the small
peak associated with the narrow distribution. An anal-
ysis of the MISE shows that it is systematically lower
for maximum likelihood estimates than for standard his-
togram estimates, supporting the former.

D. A counterexample

The previous examples gave relatively good results be-
cause the true distributions were rather smooth. Al-
though such smooth distributions are generic in most
applications it may be instructive to look at a counterex-
ample, in which the method fails.

Consider the distribution which corresponds to a cut

through an annulus

1
2 1
| ≤
≤ |
0 elsewhere

x

2

.

f (x) =

(cid:26)

A sample was generated with n = 2000 elements and the
resulting information gains are shown in Fig. 4. There
is an ambiguity in the choice of the model order and
indeed the convergence of the pdf estimates toward the
true pdf is neither uniform nor in the mean. Increasing
the order improves the ﬁt of the discontinuity a little
but also increases the oscillatory behavior known as the
Gibbs phenomenon. This problem is related to the fact
that the pdf is discontinuous and hence the characteristic
function is not absolutely summable.

Similar problems are routinely encountered in the de-
sign of digital ﬁlters, where steep responses cannot be
approximated with inﬁnite impulse response ﬁlters that
have a limited number of poles [20]. The bad perfor-
mance of the maximum likelihood approach in this case
also comes from its inability to handle densities that van-
ish over ﬁnite intervals. A minimization of the entropy
would be more appropriate here.

VI. CONCLUSION

We have presented a parametric procedure for esti-
mating univariate densities using a positive deﬁnite func-
tional. The method proceeds by maximizing the likeli-

hood of the pdf subject to the constraint that the char-
acteristic functions of the sample and estimated pdfs co-
incide for a given number of terms. Such a global ap-
proach to the estimation of pdfs is in contrast to the bet-
ter known local methods (such as non-parametric kernel
methods) whose performance is poorer in regions where
there is a lack of statistics, such as the tails of the distri-
bution. This diﬀerence makes the maximum likelihood
method relevant for the analysis of short records (with
typically hundreds or thousands of samples). Other ad-
vantages include a simple computational procedure that
can be tuned with a single parameter. An entropy-based
criterion has been developed for selecting the latter.

The method works best with densities that are at least
once continuously diﬀerentiable and that can be extended
periodically. Indeed, the shortcomings of the method are
essentially the same as for autoregressive spectral esti-
mates, which give rise to the Gibbs phenomenon if the
density is discontinuous.

The method can be extended to multivariate densities,
but the computational procedures are not yet within the
realm of practical usage.
Its numerous analogies with
the design of digital ﬁlters suggest that it is still open to
improvements.

ACKNOWLEDGMENTS

(25)

We gratefully acknowledge the dynamical systems
team at the Centre de Physique Th´eorique for many
discussions as well as D. Lagoutte and B. Torr´esani
for making comments on the manuscript.
E. Flo-
riani acknowledges support by the EC under grant
nr. ERBFMBICT960891.

APPENDIX:

We detail here the main stages that lead to the pdf
estimate described in Sec. II because extensive proofs are
rather diﬃcult to ﬁnd in the literature.

The maximum likelihood condition (eq. 12) can be ex-

pressed as

+π

−π

Z

e−jkx

∞

l=−∞ ˆαle−jlx dx =

+π

e−jkx
ˆf (x)

−π

Z

dx = 0 ,

(A1)

P
> p [30]. This simply means that the Fourier

for

k
|

|
expansion of

k
|

|

−1

ˆf (x)
(cid:17)

> p and hence the solution must be

(cid:16)

should not contain terms of order

ˆfp(x) =

1
k=−p cke−jkx .

p

(A2)

P
The pdf we are looking for must of course be real, and so
the coeﬃcients should be hermitian ck = c∗
−k. We also
want the pdf to be bounded, which implies that

5

p

k=−p
X

−jkx

cke

= 0 ,

x

∀

[
−

∈

π, π] .

(A3)

ˆfp(x) =

ε0
2π

1

1 + a1e−jx +
|

· · ·

+ ape−jpx

2 ,
|

(A12)

Let us now deﬁne, for z complex

(A4)

(A5)

(A6)

(A8)

(A9)

and

C(z) =

c−kzk ,

p

k=−p
X

P (z) = zpC(z) .

P (z) = z2p

P

∗

1
z∗

(cid:20)

(cid:18)

(cid:19)(cid:21)

P (z) is a polynomial of degree 2p. It can be easily veriﬁed
that [31]

as a consequence of the hermiticity of the coeﬃcients ck.
In particular, this tells us that if z1 is a root of P (z), then
1/z∗
1 (the complex-conjugate of its mirror image with re-
spect to the unit circle) is also a root of P (z). From
eq. A3 we know that none of these roots are located on
the unit circle.

Let us now rearrange the roots of P (z), denoting by
the p roots lying outside the unit disk and by
z1, . . . , zp
{
1/z∗
the p other ones that are located inside
{
the unit circle. We can then write:

}
1, . . . , 1/z∗
p}

P (z) = c−p(z

z1)

(z

zp)

z

−

· · ·

−

1
z∗
1 (cid:19)

· · ·

−

z
(cid:18)

−

(cid:18)

,

1
z∗
p (cid:19)
(A7)

with

where

∗
1 · · ·
From this C(z) can be written as:

zp = cpz

c−pz1

· · ·

∗
p .

z

C(z) =

B(z)

B

±

∗

,

1
z∗

(cid:20)

(cid:18)

(cid:19)(cid:21)

B(z) =

cp

z1

(z

z1)

−

(z

zp) .

· · ·

−

(A10)

· · ·

(cid:12)
(cid:12)
(cid:12)
By construction, all the roots of B(z) are located outside
(cid:12)
the unit disk.

1
2

zp (cid:12)
(cid:12)
(cid:12)
(cid:12)

Finally, we get for ˆfp(x):

ˆfp(x) =

1
C(z = ejx)

=

±

1
2 .
B(ejx)
|
|

(A11)

All the solutions of the maximum likelihood principle, if
real and bounded, are thus of constant sign and have the
structure given by eq. A11. Excluding negative deﬁnite
solutions we obtain

+π

−π

Z

+π

−π

Z

where

ε0 =

2π
b0
|
b1, . . . , bp

2 ,
|

b∗
i
b∗
0

ai =

,

i = 1,

, p ,

(A13)

· · ·

{

are the coeﬃcients of the polynomial
where
+ apzp has all its roots
B(z) and A(z) = 1 + a1z +
outside the unit disk. The normalization constant ε0 is
set by the condition

· · ·

}

ˆfp(x) dx = 1 .

(A14)

a1, . . . , ap

are now identiﬁed on the
The coeﬃcients
}
basis that the characteristic function of the pdf estimate
ˆfp(x) should match the ﬁrst p terms of the sample char-
acteristic function exactly, namely

{

ˆαk = ˆφk =

ˆfp(x) ejkxdx ,

1

k

≤

≤

p .

(A15)

let us

this purpose,

To
the quantity
p
k=0 ak ˆαl−k. Recalling that A(z) is analytic in the unit
circle and making use of Cauchy’s residue theorem, we
P
obtain

compute

p

k=0 ak ˆφl−k = 0 , 1
k=0 ak ˆφ−k = ε0 .

p

P

l

p

≤

≤

(A16)

(A17)

P

Equation A16 ﬁxes the values of
and gives
the Yule-Walker equations (eq. 17). The solution is
unique provided that

a1, . . . , ap

{

}

ˆφ0
...
ˆφp−1

· · ·

· · ·

ˆφ−p+1
...
ˆφ0






det 




= 0 .

(A18)

The latter condition is veriﬁed except when a repetitive
pattern occurs in the characteristic function. In this hap-
pens then the order p should simply be chosen to be less
than the periodicity of this pattern.

Besides its positivity, the solution we obtain has a num-
ber of useful properties. First, note that all the terms of
its characteristic function can be computed recursively
by

ˆαk+1
ˆαk
...
ˆαk−p+2








=
















a1
−
1
0
...
0

a2
−
0
1
...
0

ap
· · · −
0
· · ·
0
· · ·
...
0

· · ·
















ˆαk
ˆαk−1
...
ˆαk−p+1



,





(A19)

in which the starting condition is set by the p ﬁrst values
of ˆφk. From this recurrence relation the asymptotic be-
havior of ˆφk as k
can be probed by diagonalizing

→ ∞

6

6
6
the state space matrix in eq. A19. The eigenvalues of
1/z∗
this matrix are the roots
(called poles),
1,
which by construction are all inside the unit disk. There-
fore

, 1/z∗
p}

· · ·

{

[9] R. A. Kronmal and M. Tarter, J. Amer. Statist. Assoc.

63, 925–952 (1968).

[10] B. R. Crain, Ann. Statist. 2, 454–463 (1974).
[11] A. Pinheiro and B. Vidakovic, Comp. Stat. Data Analy-

lim
k→∞ |

φk

| ∼

eλk ,

(A20)

[12] U. Frisch, Turbulence (Cambridge University Press,

sis, in the press.

Cambridge, 1995).

where λ is related to the largest root and is always neg-
ative since

log

λ = max

1
z∗
k (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
This exponential falloﬀ of the characteristic function ex-
(cid:12)
(cid:12)
plains why the resulting pdf is relatively smooth.

< 0 .

(A21)

k

Now that we have found a solution in terms of a [0, p]
Pad´e approximant, it is legitimate to ask whether a [q, p]
approximant of the type

ˆfq,p(x) =

d0 + d1e−jx +
1 + a1e−jx +
(cid:12)
|
(cid:12)

· · ·

· · ·

+ dqe−jqx
+ ape−jpx

2

2
(cid:12)
|
(cid:12)

(A22)

could not bring additional ﬂexibility and hence provide a
better estimate of the pdf. Again, we exploit the analogy
with spectral density estimation, in which the equivalent
of [q, p] Pad´e approximants are obtained with autoregres-
sive moving average (ARMA) models. The superiority of
ARMA over AR models is generally agreed upon [32], al-
though the MISE does not ﬁrmly establish it [17]. Mean-
while we note that there does not seem to exist a sim-
ple variational principle, similar to that of the likelihood
maximization, which naturally leads to a [q, p] Pad´e ap-
proximant of the pdf.

∗

∗∗

de Provence

Universit´e
ddwit@cpt.univ-mrs.fr
e-mail : ﬂoriani@cpt.univ-mrs.fr

and CNRS,

e-mail

:

[1] R. A. Tapia and J. R. Thompson, Nonparametric proba-
bility density estimation (The Johns Hopkins University
Press, Baltimore, 1978).

[2] B. W. Silverman, Density estimation for statistics and

data analysis (Chapman and Hall, London, 1986).

[3] A. Izenman, JASA 86, 205–224 (1991).
[4] D. H. Wolpert and D. R. Wolf, Phys. Rev. E 52, 6841–

[5] P. L. G. Ventzek and K. Kitamori, J. Appl. Phys. 75,

6854 (1995).

3785–3788 (1994).

[6] D. L. Donoho, in Wavelet analysis and applications, Y.
Meyer and S. Roques, eds. (´Editions Fronti`eres, Gif-sur-
Yvette, 1993) 109–128.

[7] N. N. ˇCenkov, Soviet Math. Doklady 3, 1559–1562

[8] S. C. Schwartz, Ann. Math. Statist. 38, 1262–1265

(1962).

(1967).

7

[13] L. Devroye and L. Gy¨orﬁ, Nonparametric density estima-

tion, the L1 view (Wiley, New York, 1985).
[14] K. Nanbu, Phys. Rev. E 52, 5832–5838 (1995).
[15] P. R. Graves-Morris, ed., Pad´e approximants (The Insti-

tute of Physics, London, 1972).

[16] M. B. Priestley, Spectral analysis and time series (Aca-

demic Press, London, 1981).

[17] S. Haykin, ed., Nonlinear methods for spectral analysis

(Springer-Verlag, Berlin, 1979).

[18] D. B. Percival and A. T. Walden, Spectral analysis for
physical applications (Cambridge University Press, Cam-
bridge, 1993).

[19] J.-P. Carmichael, The autoregressive method: a method
for approximating and estimating positive functions,
Ph.D. dissert. (State Univ. of New York, Buﬀalo, 1976).
[20] A. V. Oppenheim and R. W. Schafer, Discrete-time
signal processing (Prentice-Hall, Englewood Cliﬀs NJ,
1989).

[21] This is strictly speaking not the characteristic function,

but its complex conjugate.

[22] R. W. Johnson and J. E. Shore, Which is the better
entropy expression for speech processing: −S log S or
log S ? IEEE Trans. Acoust. Speech Signal Proc. 32,
129–137 (1984).

[23] K. N. Berk, Ann. Statist. 2, 489-502 (1974).
[24] E. Parzen, IEEE Trans. Autom. Control AC-19, 723-729

(1974).

[25] J. D. Scargle, Astroph. J. 343, 874–887 (1989).
[26] The width of the interval actually has little impact on the
outcome as long as it is much less than 2π. The width
and its asymmetry could possibly be tailored to the pdf
in order to improve the ﬁt a little.

[27] The simplest one consists in symmetrizing the pdf: the
initial sample {x1, x2, . . . , xn} is transformed into a new
one which is twice a large {x1, . . . , xn, 2a − x1, . . . , 2a −
xn}, and covers the interval [2a−b, b]. The pdf of this new
sample is computed and one half of it is kept to obtain
the desired result. This procedure doubles the volume of
data, but the computational cost remains approximately
the same since one half only of the data is actually needed
to estimate the characteristic function.

[28] S. Kullback and R. A. Leibler, Ann. Math. Stat. 22, 79-

86 (1951).

[29] C. Beck and F. Schl¨ogl, Thermodynamics of chaotic sys-
tems (Cambridge University Press, Cambridge, 1993).
[30] It can be readily veriﬁed that this solution indeed corre-

[31] D. E. Smylie, G. K. C. Clarke, T. J. Ulrich, Meth. Comp.

sponds to a maximum.

Phys. 13, 391–431 (1973).

[32] The main advantage of ARMA models is their ability to

model spectral densities that vanish locally.

0

5

10
wave number k

15

(a)

20

(b)

0
10

]
|
 

k

φ
 
|
[

E

−2

10

n=200
n=2000

n=20000

5

0

15

10
wave number k
FIG. 2. The expectation E[| ˆφk|] computed for sets of var-
ious sizes taken from the same normal distribution. The
noise-induced bias level goes down as the size increases, pro-
gressively revealing ﬁner details of the pdf.

20

0
10

|
 

k

φ
 
|

−2

10

0
10

i

I
 
n
a
g
 
n
o
i
t
a
m
r
o
f
n

i

L

 

d
o
o
h

i
l

e
k

i
l

−70

−75

−80

−85

−90

0
10

−2

)
x
(
f

10

−4

10

0

5

15

20

10
order p

(c)

−2

)
x
(
f

10

0

5

15

20

10
order p

(d)

−2

)
x
(
f

10

0
10

n=200

−4
10
0
10

n=2000

−4
10
0
10

n=20000

5

−5

0
amplitude x
FIG. 1. Example of a normal distribution with exponential
tails. The sample size is n = 2000. From top to bottom are
shown: (a) the magnitude | ˆφk| of the characteristic function
(thick line) and the ﬁt resulting from an 7’th order model; (b)
the information gain (thick line) and the MISE, both showing
a minimum around p = 7 which is marked by a circle; (c) the
likelihood ˆL associated with the diﬀerent pdfs estimated for
p =1–20; and ﬁnally (d) the maximum likelihood estimate of
the pdf (thick line), an estimate based on a histogram with
101 equispaced bins (dots) and the true pdf (thin line).

−2

)
x
(
f

10

−4

10

4

2

−2

−4

0
amplitude x
FIG. 3. The pdfs as calculated for sets of various sizes taken
from the same bi-normal distribution. The thick line desig-
nates the maximum likelihood estimate, the thin line the true
pdf and the dots a histogram estimate obtained from 61 eq-
uispaced bins. The optimum orders are respectively from top
to bottom p = 5, p = 6 and p = 11.

8

0
10

(a)

i

I
 
n
a
g
 
n
o
i
t
a
m
r
o
f
n

i

−1

10

0

1.5

1

0.5

)
x
(
f

0

−2

1.5

(c)

)
x
(
f

0.5

1

0

5

10
order p

15

20

(b)

p=2

p=1

−1

0
amplitude x

1

2

p=16

1

2

−1

−2

0
amplitude x
FIG. 4. Results obtained for an annular distribution; the
sample size is n = 2000. In (a) The information gain has no
clear minimum and hence there is no well deﬁned order for the
model. In (b) the estimated pdfs for p = 1 and p = 2 fail to ﬁt
the true pdf (thick line). Increasing the order (c) improves the
ﬁt but also enhances the Gibbs phenomenon. Dots correspond
to a histogram estimate with equispaced bins.

9


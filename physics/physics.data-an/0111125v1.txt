1
0
0
2
 
v
o
N
 
4
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
2
1
1
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A SCALE INVARIANT BAYESIAN METHOD TO SOLVE LINEAR
INVERSE PROBLEMS

Ali Mohammad-Djafari and J´erˆome Idier
Laboratoire des Signaux et Syst`emes (CNRS-ESE-UPS)
´Ecole Sup´erieure d’´Electricit´e,
Plateau de Moulon, 91192 Gif-sur-Yvette C´edex, France

Abstract.
In this paper we propose a new Bayesian estimation method to solve linear
inverse problems in signal and image restoration and reconstruction problems which has
the property to be scale invariant. In general, Bayesian estimators are nonlinear functions
of the observed data. The only exception is the Gaussian case. When dealing with linear
inverse problems the linearity is sometimes a too strong property, while scale invariance
often remains a desirable property. As everybody knows one of the main diﬃculties
with using the Bayesian approach in real applications is the assignment of the direct
(prior) probability laws before applying the Bayes’ rule. We discuss here how to choose
prior laws to obtain scale invariant Bayesian estimators. In this paper we discuss and
propose a familly of generalized exponential probability distributions functions for the
direct probabilities (the prior p(x) and the likelihood p(y|x)), for which the posterior
p(x|y), and, consequently, the main posterior estimators are scale invariant. Among
many properties, generalized exponential can be considered as the maximum entropy
probability distributions subject to the knowledge of a ﬁnite set of expectation values of
some knwon functions.

1.

Introduction

We address a class of linear inverse problems arising in signal and image recon-
struction and restoration problems which is to solve integral equations of the form:

gij =

f (r′) hij(r′) dr′ + bij,

i, j = 1, · · · , M,

(1)

ZZD

where r′ ∈ IR2, f (r′) is the object (image reconstruction problems) or the original
image (image restoration problems), gij are the measured data (the projections in
image reconstruction or the degraded image in image restoration problems), bij
are the measurement noise samples and hij(r′) are known functions which depend
only on the measurement system. To show the generality of this relation, we give
in the following some applications we are interested in:

− Image restoration:

g(xi, yj) =

f (x′, y′)h(xi − x′, yj − y′) dx′ dy′ + b(xi, yj)

,

i = 1, · · · , N
j = 1, · · · , M

,

ZZD

where g(xi, yj) are the observed degraded image pixels and h(x, y) is the point
spread function (PSF) of the measurement system.

ZZD

ZZD

− X-ray computed tomography (CT):

g(ri, φj ) =

f (x, y)δ(ri−x cos φi−y sin φi) dx dy+b(ri, φj)

,

i = 1, · · · , N
j = 1, · · · , M

,

where g(ri, φj) are the projections along the axis ri = x cos φi − y sin φi,
having the angle φj , and which can be considered as the samples of the Radon
transform (RT) of the object function f (x, y).

− Fourier Synthesis in radio astronomy, in SAR imaging and in diﬀracted wave

tomographic imaging systems:

g(uj, vj) =

f (x, y) exp [−j(ujx + vjy)] dx dy + b(uj, vj),

j = 1, · · · , M,

where uj = (uj, vj) is a radial direction and g(uj, vj ) are the samples of the
complex valued visibility function of the sky in radio astronomy or the Fourier
transform of the measured signal in SAR imaging.

Other examples can be found in [6, 7, 5, 8, 9].

In all these applications we have to solve the following ill-posed problem: how
to estimate the function f (x, y) from some ﬁnite set of measured data which may
also be noisy, because there is no experimental measurement device, even the most
elaborate, which could be entirely free from uncertainty, the simplest example
being the ﬁnite precision of the measurements.

The numerical solution of these equations needs a discretization procedure
which can be done by a quadrature method. The linear system of equations
resulting from the discretization of an ill-posed problem is, in general, very ill-
conditioned if not singular. So the problem is to ﬁnd a unique and stable solution
for this linear system. The general methods which permit us to ﬁnd a unique and
stable solution to an ill-posed problem by introducing an a priori information on
the solution are called regularization . The a priori information can be either in
a deterministic form (positivity) or in a stochastic form (some constraints on the
probability density functions).

When discretized, these problems can be described by the following:
“Estimate a vector of the parameters x ∈ IRn (pixel intensities in an im-
age for example) given a vector of measurements y ∈ IRm (representing,
for example, either a degraded image pixel values in restoration prob-
lems or the projections values in reconstruction problems) and a linear
transformation A relating them by:

y = Ax + b,

(2)

where b represents the discretization errors and the measurement noise
which is supposed to be zero-mean and additive.”
In this paper we propose to use the Bayesian approach to ﬁnd a regularized so-
lution to this problem. Noting that the Bayesian theory only gives us a framework
for the formulation of the inverse problem, not a solution of it. The main diﬃculty

is, in general, before the application of the Bayes’ formula, i.e.; how to formulate
appropriately the problem and how to assign the direct probabilities. Keeping
this fact in mind, we propose the following organization to this paper: In section
2. we give a brief description of the Bayesian approach with detail calculations of
the solution in the special case of Gaussian laws. In section 3. we discuss about
the scale invariance property and propose a familly of prior probability density
functions (pdf ) which insure this property for the solution. Finally, in section 4.,
we present some special cases and give detailed calculations for the solution.

2. General Bayesian approach

A general Bayesian approach involves the following steps:

− Assign a prior probability law p(x) to the unknown parameter to translate
our incomplete a priori information (prior beliefs) about these parameters;
− Assign a direct probability law to the measured data p(y|x) to translate the
lack of total precision and the inevitable existence of the measurement noise;
− Use the Bayes’ rule to calculate the posterior law p(x|y) of the unknown

parameters;

− Deﬁne a decision rule to give values

x to these parameters.

To illustrate the whole procedure, let us to consider an example; the Gaussian case.
If we suppose that what we know about the unknown input x is its mean E {x} =
x0 and its covariance matrix E {(x − x0)(x − x0)t} = Rx = σ2
xP , and what we
know about the measurement noise b is also its covariance matrix E
= Rb =
σ2
b I, then we can use the maximum entropy principle to assign:

bbt

b

(cid:8)

(cid:9)

p(x) ∝ exp

−

(x − x0)tRx

−1(x − x0)

,

and

p(y|x) ∝ exp

−

(y − Ax)tRb

Now we can use the Bayes’ rule to ﬁnd:

(cid:21)

−1(y − Ax)
(cid:21)

.

1
2

1
2

(cid:20)

(cid:20)

and use, for example, the maximum a posteriori (MAP) estimation rule to give a
solution to the problem, i.e.;

p(x|y) ∝ p(y|x) p(x),

x = arg max

{p(x|y)} ,

x

In fact, all we want to know is resumed in the
Other estimators are possible.
b
posterior law. In general, one can construct a bayesian estimator by deﬁning a
cost (or utility) function C(

x, x) and by minimizing its mean value

x = arg min

EX|Y {C(z, x)}
b

C(z, x)p(x|y) dx

.

= arg min

z

(cid:26)ZZ

(cid:9)

(cid:27)

z

(cid:8)

The two classical estimators:

b

(3)

(4)

(5)

(6)

− Posterior mean (PM):

x = EX|Y {x} =

x p(x|y) dx,

is obtained when deﬁning C(
− Maximum a posteriori (MAP):
b
x, x) = 1 − δ(

is obtained when deﬁning C(

x, x) = (

ZZ
x − x)t(
x = arg max

x − x), and
{p(x|y)},
x
b
x − x).

b

b

Now, let us go a little further inside the calculations. Replacing (3), and (4)

b

in (5), we calculate the posterior law:

b

b

1
2σ2
b

p(x|y) ∝ exp

−

J(x)

, with J(x) = (y−Ax)t(y−Ax)+λ(x−x0)tP −1(y−x0),

(cid:21)

(cid:20)
b /σ2
where λ = σ2
x. The posterior is then also a Gaussian. We can know use any
decision rule to obtain a solution. For example the maximum a posteriori (MAP)
solution is obtained by:

x = arg max

{p(x|y)} = arg min
x

x

{J(x)} .

Note that in this special Gaussian case both estimators, i.e.; the posterior mean
(PM) and the MAP estimators are the same:

b

and the minimization of the criterion J(x) which can also be written in the form:

b

x = EX|Y {x} = arg max

{p(x|y)}

x

J(x) = ||y − Ax||2 + λ||x − x0||2
P

can be considered as a regularization procedure to the inverse problem (2). Indeed,
the Bayesian approach will give us here a new interpretation of the regularization
parameter in terms of the signal to noise ratio, i.e.; λ = σ2

b /σ2
x.

J(x) is a quadratic function of x. The solution

x is then a linear function of the
data y. This is due to the fact that the problem is linear and all the probability
laws are Gaussian. Excepted this case, in general, the Bayesian estimators are
not linear functions of the observations y. However, we may not need that the
solution be a linear function of the data y, but the scale invariance is the minimum
property which is often needed.

b

3. Scale invariant Bayesian estimators

What we are proposing in this paper is to study in what conditions we can obtain
estimators who are scale invariant. Note that linearity is the combination of

additivity:

=⇒ y1 + y2 7→

x1 +

x2,

and

scale invariance:

y1 7→

b
x1.
x1 =⇒ ∀k > 0, ky1 7→ k

b

y1 7→
y2 7→

(cid:26)

x1,
x2
b
b

b

b

(7)

(8)

(9)

In a linear inverse problem what is often necessary is that the solution be scale
invariant. As we have seen in the last section when all the probability laws are
Gaussian then the Bayesian estimators are linear functions of the data, so that
the methods based on this assumption have not to take care about the scale of the
measured data. The Gaussian assumption is very restrictive. On the other hand,
more general priors yield the Bayesian estimators which are nonlinear functions of
data, so the result of the inversion method depend on the absolute values of the
measured data. In other words, two users of the method using two diﬀerent scale
factors would not get the same results, even rescaled:

b
x2

b

b
x2

b

y −→ k1 −→ Estimation −→

x1

y −→ k2 −→ Estimation −→

x2
k2 6=

x1
k1

b

b

A general nonlinear (scale variant) estimation method

What we want to specify in this paper is a family of probability laws for which these
estimators are scale invariant. So the user of the inversion method can process the
data without worrying about rescaling them to an arbitrary level and two users of
the method at two diﬀerent scales will obtain the proportional results:

y −→ k1 −→ Estimation −→

x1

y −→ k2 −→ Estimation −→

b
A scale invariant estimation method

b

x2
k2 =

x1
k1

To do this let us note

− θ all the unknown parameters deﬁning our measuring system (noise variance

σ2 and the prior law parameters for example),

− p1(x1|y1; θ1) and pk(xk|yk; θk) the two expressions of the posterior law for

scale 1 and for scale k with

Then, what we need is the following:

xk = kx1, yk = ky1.

∃θk = f (θ1, k) | ∀k > 0, ∀x1, y1,

pk (xk|yk; θk) =

1
kn p1(x1|y1; θ1),

(10)

which means that the functional form of the posterior law remains unchanged
when the measurement’s scale is changed. Only we have to modify the parameters
θk = f (θ1, k) which is only a function of θ1 and the scale factor k.

However, not all estimators based on this posterior will be scale invariant. The
cost function must also have some property to obtain a scale invariant estimator.
So, the main result of this paper can be resumed in the following theorem:

Theorem:

If ∃θk = f (θ1, k) | ∀k > 0, ∀x1, y1,

pk (xk|yk; θk) =

1
kn p1(x1|y1; θ1),

then any bayesian estimator with a cost function C(

x, x) satisfying:

C(

xk, xk) = ak + bkC(

x, x),
b

is a scale invariant estimator, i.e.;

b

b

xk(yk; θk) = k

x1(y1; θ1).

Proof:

b
In fact, it is easy to see the following:

b

xk(yk; θk) = arg min
zk

C(zk, xk)pk(xk|yk; θk) dxk

b

[bkC(z1, x1) + ak]

(cid:27)

1
kn p1(x1|y1; θ1)kn dx1

(cid:27)

(cid:26)ZZ

= k arg min
z1

= k arg min
z1

(cid:26)ZZ
bk

(cid:26)

= k arg min
z1
(cid:26)ZZ
x1(y1; θ1)

= k

C(z1, x1)p1(x1|y1; θ1) dx1 + ak

ZZ
C(z1, x1)p1(x1|y1; θ1) dx1

(cid:27)

(cid:27)

Note the great signiﬁcance of this result, even if the estimateur

x(y; θ) is a

b

nonlinear function of the observations y it stays scale invariant.

Now, the task is to search for a large familly of probability laws p(x) and p(y|x)
in a manner that the posterior law p(x|y) remains scale invariant. We propose to
do this search in the generalized exponential familly for two reasons:

b

− First the generalized exponential probability density functions form a very

rich one, and

− Second, they can be considered as the maximum entropy prior laws subject

to a ﬁnite number of constraints (linear or nonlinear).

Noting also that if p(x) and p(y|x) are scale invariant then the posterior p(x|y)
is also scale invariant and that there is a symmetry for p(x) and p(y|x), so that
it is only necessary to ﬁnd the scale invariance conditions for one of them. In the
following, without loss of generality, we consider the case where p(y|x) is Gaussian:

p(y|x; σ2) ∝ exp

−χ2(x, y; σ2)

, with χ2(x, y; σ2) =

(cid:2)

(cid:3)

1
2σ2 [y − Hx]t[y − Hx],
(11)

and ﬁnd the conditions for p(x) to be scale invariant. We choose the generalized
exponential pdf ’s for p(x), i.e.;

p(x; λ) ∝ exp

−

λiφi(x)

,

(12)

r

"

i=1
X

#

and ﬁnd the conditions on the functions φi(x) for which p(x) is scale invariant.

Note that these laws can be considered as the maximum entropy prior laws if

our prior knowledge is:

− What we know about x is:

E {φi(x)} = di,

i = 1, · · · , r,

− and what we know about the noise b is:

E {b} = 0,
bbt
E

= Rb = σ2I,

(cid:26)

where Rb is the covariance matrix of b.

(cid:9)
Now, using the equations (11) and (12) and noting by θ = (σ2, λ1, · · · , λr), by
λ = (λ1, · · · , λr), and by φ(x) = (φ1(x), · · · , φr(x)), we have

(cid:8)

p(x|y; θ) ∝ exp

−χ2(x, y; σ2) − λtφ(x)

,

(13)

and the scale invariance condition becomes:

(cid:2)

(cid:3)

∀k > 0, ∀x1, y1, χ2

k(xk, yk; σ2

k) + λt

kφ(xk) = χ2

1(x1, y1; σ2

1) + λt

1φ(x1) + cte.

But with the Gaussian choice for the noise pdf we have

∀k > 0, ∀x1, y1, χ2

k(xk, yk; σ2

k) =

||yk−Hxk||2 =

k2 ||y1−Hx1||2 = χ2

1(x1, y1; σ2

1),

1
2σ2
k

1
2k2σ2
1

and so the condition becomes

or equivalently,

∀k > 0, ∀x, λt

kφ(xk) = λt

1φ(x1) + cte,

(14)

pk(xk; λk) =

1
kn p1(x1; λ1) with λk = f (λ1, k).

Thus, in the case of centered Gaussian pdf for the noise, to have a scale invariant
posterior law it is suﬃcient to have a scale invariant prior law.
Now, assuming interchangeable (independent) pixels, i.e.;

p(x; λ) = exp

λ0 +

λiφi(x)

=

p(xj ; λ),

(15)

r

i=1
X

"

N

j=1
Y

#

or equivalently,

N

φi(x) =

φi(xj )

(16)

j=1
X
we have to ﬁnd the conditions on the scalar functions φi(x) of scalar variables x
who satisfy the equation (14) or equivalently

r

i=1
X

!

∀k > 0, ∀x,

λi(k) φi(kx) =

λi(1) φi(x) + cte

(17)

r

i=1
X

We have shown (see appendix) that, the functions φi(x) which satisfy these con-
ditions are all either the powers of x or the powers of ln x or a multiplication of
them. The general expressions for these functions are:

φ(x) =

cmn(ln x)n

xαm +

c0n(ln x)n, with M ≤ r and

Nm = r

M

Nm−1

m=1  
X

n=0
X

N0

n=0
X

(18)
where M and Nm are integer numbers, and cmn, c0n and αm are real numbers.
For a geometrical interpretation and more details see appendix. The following
examples show some special and interesting cases.

One parameter laws: Consider the case of r = 1. In this case we have

M

m=0
X

p(x; λ) ∝ exp [−λφ(x)] .

(19)

Applying the general rule with

r = 1 −→

(cid:26)

M = 0, N0 = 1,
−→ c00 + c01 ln x
M = 1, N0 = 0, N1 = 1, −→ c00 + c10xα1

we ﬁnd that the only functions who satisfy these conditions are:

φ(x)

=

xα, ln x

(cid:26)

(cid:27)

(cid:26)

(cid:27)

(20)

where α is a real number. There isc two interesting special cases:

− φ(x) = xα, resulting to: p(x) ∝ exp [−λxα] , α > 0, λ > 0, which is a gener-

− φ(x) = ln x, resulting to: p(x) ∝ exp [−λ ln x] , which is a special case of the

alized Gaussian pdf , and

Beta pdf .

Note that the famous entropic prior law:
p(x) ∝ exp [−λx ln x] of Gull and
Skilling [11, 4] does not verify the scale invariance property. But, if we add one
more parameter

p(x) ∝ exp [−λx ln x + µx] ,

then, it will satisfy this condition as we can see in the next section.

Two parameters laws: This is the case where r = 2 and we have:

p(x; λ) ∝ exp [−λφ1(x) − µφ2(x)] ,

(21)

and applying the general rule:

M = 2, N0 = 0, N1 = 1, N2 = 1, −→ c00 + c10xα1 + c20xα2
M = 1, N0 = 0, N1 = 2,
M = 1, N0 = 1, N1 = 1,
M = 0, N0 = 2,

−→ c00 + c10xα1 + c11xα1 ln x
−→ c00 + c10xα1 + c01 ln x
−→ c00 + c01 ln x + c02 ln2 x

we see that in this case the only functions (φ1, φ2) which satisfy these conditions
are:

r = 2 −→ 



(φ1(x), φ2(x))

=

(xα1 , xα2 ), (xα1 , xα1 ln x), (xα1 , ln x), (ln x, ln2 x)

(cid:26)

(cid:27)

(cid:26)

(22)
where α1 and α2 are two real numbers. Special cases are obtained when we choose
φ2(x) = x, the only possible functions for φ1(x) are then:

(cid:27)

{xα, ln x, x ln x} .

(23)

and we have the following interesting cases:

− φ1(x) = x2, resulting to:

−λx2 − µx

∝ exp

−λ

x + µ
2λ

2

,

which is a Gaussian pdf N

i
− φ1(x) = ln x, resulting to: p(x) ∝ exp [−λ ln x − µx] = x−λ exp [−µx] , which

λ , σ2 = 1
(cid:2)
2λ

h

(cid:0)

(cid:1)

(cid:3)

.

is the Gamma pdf , and ﬁnally,

(cid:0)

(cid:1)

− φ1(x) = x ln x, resulting to: p(x) ∝ exp [−λx ln x − µx] . which is known as

p(x) ∝ exp
m = −µ

the entropic pdf .

Three parameters laws: This is the case where r = 3. Once more applying the
general rule we ﬁnd:

M = 3, N0 = 0, N1 = 1, N2 = 1, N3 = 1, → c00 + c10xα1 + c20xα2 + c30xα3
M = 2, N0 = 0, N1 = 1, N2 = 2,
M = 2, N0 = 1, N1 = 1, N2 = 1,
M = 1, N0 = 0, N1 = 3,
M = 1, N0 = 1, N1 = 2,
M = 1, N0 = 2, N1 = 1,
M = 0, N0 = 3,

→ c00 + c10xα1 + c20xα2 + c21xα2 ln x
→ c00 + c01 ln x + c10xα1 + c20xα2
→ c00 + c10xα1 + c11xα1 ln x + c12xα1 ln2 x
→ c00 + c01 ln x + c10xα1 + c11xα1 ln x
→ c00 + c01 ln x + c02 ln2 x + c10xα1
→ c00 + c01 ln x + c02 ln2 x + c03 ln3 x






r = 3 →

which means:

(φ1(x), φ2(x), φ3(x))

=

(xα1 , xα2 , xα3 ), (xα1 , xα2 , ln x), (xα1 , xα1 ln x, xα1 ln2 x),

(cid:26)

(cid:27)

(cid:26)

(xα1 , xα1 ln x, ln x), (xα1 , xα2 , xα2 ln x), (xα1 , ln x, ln2 x),

(ln x, ln2 x, ln3 x)

(24)

(cid:27)

where α1, α2 and α3 are three real numbers.

4. Proposed method

The general procedure of the inversion method we propose can be resumed as
follows:

− Choose a set of functions φi(x) between the possibles ones described in the last
section and assign the prior p(x). In many imaging applications we proposed
and used successfully the following two parameters one:

p(x; λ) ∝ exp [−λ1H(x) − λ2S(x)] , with H(x) =

φ1(xj ), and S(x) =

φ2(xj )

N

j=1
X

N

j=1
X

where φ1(x) and φ2(x) choosed between the possible ones in (22) or (23).
bbt

− When what we know about the noise b is only its covariance matrix E

=

Rb = σ2

b I, then using the maximum entropy principle we have:

(cid:8)

(cid:9)

p(y|x) ∝ exp

−

Q(x)

, with Q(x) = (y − Ax)tRb

−1(y − Ax).

1
2

(cid:20)

(cid:21)

We may note that p(y|x) is also a scale invariant probability law.

− Using the Bayes’ rule and MAP estimator the solution is determined by

x = arg max

{p(x|y)} = arg min
x

x

{J(x)} , with J(x) = Q(x)+λ1H(x)+λ2S(x).

Note here also that, for the cases where one of the functions φ1(x) or φ2(x)
b
is a logarithmic function of x, we have to constraint its range to the positive
real axis, and we have to solve the following optimization problem

x = arg max
x>0

{p(x|y)} = arg min
x>0

{J(x)} .

This optimization is achieved by a modiﬁed conjugate gradients method.
− The choice of the functions φi(x) and the determination of the parameters

b

(λ1, λ2) in the ﬁrst step is still an open problem.
In imaging applications we propose to do this choice from our prior knowledge
on the nature of interested quantity (physics of the application). For example,
if the object x is a real quantity equally distributed on the positive and the
negative reals then a Gaussian prior, i.e.; (φ1(x) = x, φ2(x) = x2) is conve-
nient. But, if the object x is a positive quantity or if we know that it represents
small extent, bright and sharp objects on a nearly black background (images in
radio astronomy, for example), then we may choose (φ1(x) = x, φ2(x) = ln x),
or (φ1(x) = x, φ2(x) = x ln x) which are the priors with longer tails than the
Gaussian or truncated Gaussian one.
When the choice of the functions (φ1(x), φ2(x)) is done, we still have to deter-
mine the hyperparameters (λ1, λ2). For this two main approaches have been
proposed. The ﬁrst is based on the generalized maximum likelihood (GML)
which tries to estimate simultaneously the parameters x and the hyperparam-
eters θ = (λ1, λ2) by

x,
(

θ) = arg max
(x,θ)

{p(x, y; θ)} = arg max
(x,θ)

{p(y|x) p(x; θ)} ,

(25)

b

b

and the second is based on the marginalization (MML), in which the hyper-
parameters θ are estimated ﬁrst by

θ = arg max

p(y; θ) =

p(x, y; θ) dx

= arg max

p(y|x) p(x; θ) dx

,

θ (cid:26)

ZZ

(cid:27)

θ (cid:26)ZZ

(cid:27)
(26)

b
and then used for the estimation of x:

x = arg max

p(x|y;

θ)

= arg max

p(y|x) p(x|

θ)

.

(27)

x

n

o

x

n

o

b

What is important here is that both methods preserve the scale invariant
property. For practical applications we have recently proposed and used a
method based on the generalized maximum likelihood [8, 9] which has been
successfully used in many signal and image reconstruction and restoration
problems as we mentionned in the introduction [10].

b

b

5. Conclusions

Excepted the Gaussian case where all the Bayesian estimators are linear functions
of the observed data, in general, the Bayesian estimators are nonlinear functions
of the data. When dealing with linear inverse problems linearity is sometimes a
too strong property, while scale invariance often remains a desirable property. In
this paper we discussed and proposed a familly of generalized exponential proba-
bility distributions for the direct probabilities (the prior p(x) and the likelihood
p(y|x)), for which the posterior p(x|y), and, consequently, the main posterior es-
timators are scale invariant. Among many properties, generalized exponential can
be considered as the maximum entropy probability distributions subject to the
knowledge of a ﬁnite set of expectation values of some knwon functions.

1. Appendix: General case

We want to ﬁnd the solutions of the following equation:

∀k > 0, ∀x,

λi(k)φi(kx) =

λi(1)φi(x) + β(k)

(A.1)

r

i=1
X

r

i=1
X

Making the following changes of variables and notations

1/k = ˜k, kx = ˜x, λi(k) = ˜λi(˜k), and βi(k) = ˜βi(˜k),

equation (A.1) becomes

r

r

˜λi(˜k)φi(˜x) =

˜λi(1)φi(˜k˜x) + ˜β(˜k)

i=1
X
For convenience sake, we will drop out the tilde ˜, and note λi(1) = λi, so that we
can write

i=1
X

λi(k)φi(x) =

λiφi(kx) + β(k)

r

i=1
X

r

i=1
X

r

i=1
X

r

i=1
X

Noting

we have

S(x) =

λiφi(x),

and so S(kx) =

λiφi(kx)

r

i=1
X

λi(k)φi(x) = S(kx) + β(k)

(A.2)

Deriving r − 1 times this equation with respect to k we obtain

...

φ1(x)
φ2(x)
φ3(x)
...
φr(x)

λ′
i(k)φi(x)

= x S′(kx) + β′(k)

λ′′
i (k)φi(x)

= x2 S′′(kx) + β′′(k)

(A.3)

λ(r−1)
i

(k)φi(x) = xr−1S(r−1)(kx) + β(r−1)(k)

r

i=1
X
r

i=1
X
...

r

i=1
X

Combinig equations (A.2) and (A.3) in matrix form we have



λ1(k)
λ′
1(k)
λ′′
1 (k)
...
λ(r−1)
1

· · ·
· · ·
· · ·

λr(k)
λ′
r(k)
λ′′
r (k)
...
· · ·
· · · λ(r−1)
r







=



S(kx) + β(k)
xS′(kx) + β′(k)
x2S′′(kx) + β′′(k)
...
xr−1S(r−1)(kx) + β(r−1)(k)



(k)













(A.4)


If this matrix equation can be inverted, this means that any function φi(x) is
a linear combination of S(kx) + β(k) and its (r − 1) derivatives with respect to k:



















(k)

φi(x) =

ηi(k)

x(i−1)S(i−1)(kx) + β(i−1)(k)
i

h

,

r

i=0
X

(A.5)

and if this is not the case, this means that there exists an interval for k, for which
some of the functions λi(k) are linear combinations of the others [2]. In this case
let us show that we will go back to the situation of the problem of lower order r.
Let us to assume that the last column of the matrix is a linear combination of the
others, i.e.;

r−1

λr(k) =

γiλi(k).

i=1
X
Putting this in the equation (A.1) will give

r−1

i=1
X

r−1

"

i=1
X

#

r−1

i=1
X

λi(k)φi(kx)+

γiλi(k)

φr(kx) =

λi(1)φi(x)+β(k)+

γiλi(1)

φr(x)

r−1

"

i=1
X

#

and noting ψi(x) = φi(x) + γiφr(x) and ψi(kx) = φi(kx) + γiφr(kx) we obtain

λi(k)ψi(kx) =

λi(1)ψi(x) + β(k)

r−1

i=1
X

r

i=1
X

which is an equation in the same form of (A.1), but of lower order.

Deriving now both parts of the equation (A.5) with respect to k and noting

kx = u we obtain

ai uiSi(u) = a

(A.6)

r

i=0
X

This is the general expression of a rth order Euler–Cauchy diﬀerential equation
[1, 2] which is classically solved through the change of variable u = ex, and one
can ﬁnd the general expression of its solution in the following form:

S(x) =

cmn(ln x)n

xαm+

c0n(ln x)n with M = 0, · · · r, and

Nm = r

M

Nm−1

m=1  
X

n=0
X

N0

n=0
X

!

(A.7)
where M and Nm are integer numbers, and cmn, c0n and αm are real numbers. In
fact the most general solution also incorporate terms of the form

M

m=0
X

(ln x)n (αn cos(ln x) + βn sin(ln x))

xd

#

"

n
X

derived from complex αm and cmn. But we will not consider these terms because
the resulting pdf ’s have oscillatory behavior around zero.

One can give a geometric interpretation of the solutions given in (A.7). For

any given order r make a (r + 1) × (r + 1) table in the form

lnr x
...
ln2 x
ln x
1

×
1

xα1 xα2

· · · xαr

and let r mass points fall down into the columns. To each ﬁlled box is assigned a
function φi(x) by multiplying the corresponding powers of x and ln x on the same
line and the same column. To illustrate this, we give in the following the three

ﬁrst cases:

Case r = 1:

Case r = 2:

Case r = 3:

ln x

b
1 × a
1 xα1

φ(x)
xα1
ln x

a
b

ln2 x
ln x
1

d
bd
c
× abc
a
1 xα1 xα2

φ1(x)
xα1
xα1
xα1
ln x

φ2(x)
xα2
ln x
xα1 ln x
ln2 x

a
b
c
d

ln3 x
ln2 x
ln x
1

g
f g
bdf g

c
dc
× abcdef
1

e
a
abe
xα1 xα2 xα3

φ1(x)
xα1
xα1
xα1
xα1
xα1
xα1
ln x

φ2(x)
xα2
xα2

φ3(x)
xα3
ln x

xα1 ln x xα1 ln2 x
xα1 ln x
xα2
ln x
ln2 x

ln x
xα2 ln x
ln2 x
ln3 x

a
b
c
d
e
f
g

References

Paris, 1982.

Paris, 1968.

1. Angot A., “Compl´ements de math´ematiques,” Masson ed., Sixi`eme ´Edition,

2. Bass J., “Cours de math´ematiques,” Masson ed., Tome II, Quatri`eme ´Edition,

3. Demoment G., “Image Reconstruction and Restoration: Overview of Common
Estimation Structure and Problems,” IEEE Trans. on Acoustics, Speech, and
Signal Processing, Vol. 37, pp:2024-2036, (1989).

4. Gull S. F. and Skilling J., “Maximum entropy method in image processing,”

IEE Proc., 131-F, pp. 646-659, 1984.

5. Mohammad-Djafari A. and Idier J., “Maximum entropy prior laws of images
and estimation of their parameters,” in T.W. Grandy (ed.), Maximum-entropy
and Bayesian methods, Kluwer Academic Publishers, Netherlands, 1990.
6. Mohammad-Djafari A. and Demoment G., “Maximum entropy Fourier syn-
thesis with application to diﬀraction tomography,“ Applied Optics, Vol.26,
No. 10, pp:1745-1754, (1987).

7. Mohammad-Djafari A. and Demoment G., “Maximum entropy reconstruction
in X ray and diﬀraction tomography,” IEEE Trans. on Medical Imaging, Vol.
7, No. 4 pp:345-354, (1988).

8. Mohammad-Djafari A., “Bayesian Approach with Maximum Entropy Priors
to Imaging Inverse Problems, Part I: Fundations,” submitted to IEEE Trans.
on Image Processing, (August, 1993).

9. Mohammad-Djafari A., “Bayesian Approach with Maximum Entropy Priors
to Imaging Inverse Problems, Part II: Applications,” submitted to IEEE Trans.
on Image Processing, (August, 1993).

10. Nguyen M.K. and Mohammad-Djafari A., “Bayesian Maximum Entropy
Image Reconstruction from the Microwave Scattered Field Data,” in A.
Mohammad-Djafari and G. Demoment(ed.), Maximum Entropy and Bayesian
Methods, Kluwer Academic Publishers, the Netherlands, 1993.

11. Skilling J., “Maximum-Entropy and Bayesian Methods,” J. Skilling ed., Dor-

drecht: Kluwer Academic Publisher, 1988.


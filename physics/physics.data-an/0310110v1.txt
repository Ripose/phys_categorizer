PhyStat2003, SLAC, September 8-11

1

Multivariate Analysis from a Statistical Point of View

K.S. Cranmer
University of Wisconsin-Madison, Madison, WI 53706, USA

3
0
0
2
 
t
c
O
 
2
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
1
0
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Multivariate Analysis is an increasingly common tool in experimental high energy physics; however, many of the
common approaches were borrowed from other ﬁelds. We clarify what the goal of a multivariate algorithm should
be for the search for a new particle and compare diﬀerent approaches. We also translate the Neyman-Pearson
theory into the language of statistical learning theory.

1. Introduction

Multivariate Analysis is an increasingly common
tool in experimental high energy physics; however,
most of the common approaches were borrowed from
other ﬁelds. Each of these algorithms were developed
for their own particular task, thus they look quite dif-
ferent at their core. It is not obvious that what these
diﬀerent algorithms do internally is optimal for the the
tasks which they perform within high energy physics.
It is also quite diﬃcult to compare these diﬀerent al-
gorithms due to the diﬀerences in the formalisms that
were used to derive and/or document them. In Sec-
tion 2 we introduce a formalism for a Learning Ma-
chine, which is general enough to encompass all of
the techniques used with in high energy physics. In
Sections 3 & 4 we review the statistical statements
relevant to new particle searches and translate them
into the formalism of statistical learning theory.
In
the remainder of the note, we look at the main re-
sults of statistical learning theory and their relevance
to some of the common algorithms used within high
energy physics.

2. Formalism

Formally a Learning Machine is a family of func-
tions F with domain I and range O parametrized by
α ∈ Λ. The domain can usually be thought of as, or
at least embedded in, Rd and we generically denote
points in the domain as x. The points x can be re-
ferred to in many ways (e.g. patterns, events, inputs,
examples, . . . ). The range is most commonly R, [0, 1],
or just {0, 1}. Elements of the range are denoted by
y and can be referred to in many ways (e.g. classes,
target values, outputs, . . . ). The parameters α spec-
ify a particular function fα ∈ F and the structure of
α ∈ Λ depends upon the learning machine [1, 2].

In the modern theory of machine learning, the per-
formance of a learning machine is usually cast in the
more pessimistic setting of risk. In general, the risk,
R, of a learning machine is written as

R(α) =

Q(x, y; α) p(x, y)dxdy

(1)

Z

WEJT002

where Q measures some notion of loss between fα(x)
and the target value y. For example, when classifying
events, the risk of mis-classiﬁcation is given by Eq. 1
with Q(x, y; α) = |y−fα(x)|. Similarly, for regression1
tasks one takes Q(x, y; α) = (y − fα(x))2. Most of
the classic applications of learning machines can be
cast into this formalism; however, searches for new
particles place some strain on the notion of risk.

3. Searches for New Particles

The conclusion of an experimental search for a new
particle is a statistical statement – usually a decla-
ration of discovery or a limit on the mass of the hy-
pothetical particle. Thus, the appropriate notion of
performance for a multivariate algorithm used in a
search for a new particle is that performance mea-
sure which will maximize the chance of declaring a
discovery or provide the tightest limits on the hypo-
In principle, it should be a fairly
thetical particle.
straight-forward procedure to use the formal statis-
tical statements to derive the most appropriate per-
formance measure. This procedure is complicated by
the fact that experimentalists (and statisticians) can-
not settle on a formalism to use (i.e. Bayesians vs.
Frequentists). As an example, let us consider the Fre-
quentist theory developed by Neyman and Pearson [3].
This was the basis for the results of the search for the
Standard Model Higgs boson at LEP [4].

The Neyman-Pearson theory (which we review
brieﬂy for completeness) begins with two Hypothe-
ses: the null hypothesis H0 and the alternate hypoth-
esis H1 [3]. In the case of a new particle search H0
is identiﬁed with the currently accepted theory (i.e.
the Standard Model) and is usually referred to as the
“background-only” hypothesis. Similarly, H1 is iden-
tiﬁed with the theory being tested usually referred to
as the “signal-plus-background” hypothesis

1During the presentation, J. Friedman did not distinguish
between these two tasks; however, in a region with p(x, 1) = b
and p(x, 0) = 1 − b, the optimal f (x) for classiﬁcation and
regression diﬀer. For classiﬁcation, f (x) = {1 if b > 1/2, else 0},
and for regression the optimal f (x) = b.

2

PhyStat2003, SLAC, September 8-11

Next, one deﬁnes a region W ∈ I such that if the
data fall in W we accept the null hypothesis (and re-
ject the alternate hypothesis)2. Similarly, if the data
fall in I − W we reject the null hypothesis and accept
the alternate hypothesis. The probability to commit
a Type I error is called the size of the test and is given
by (note alternate use of α)

The probability to commit a Type II error is given by

α =

p(x|H0)dx.

I−W

Z

β =

p(x|H1)dx.

W

Z

(2)

(3)

Finally, the Neyman-Pearson lemma tells us that the
region W of size α which minimizes the rate of Type II
error (maximizes the power) is given by

and the risk is given by

β =

[1 − Θ(fα(x) − kα)] p(x, 1)dx
p(x, 1)dx

(6)

∝

Θ(fα(x) + kα) y p(x, y)dxdy.

R

R

Z

Extracting the integrand we can write the loss func-
tional as

Q(x, y; α) = Θ(fα(x) + kα) y.

(7)

Unfortunately, Eq. 1 does not allow for the global con-
straint imposed by kα (which is implicitly a functional
of fα), but this could be accommodated by the meth-
ods of Euler and Lagrange. Furthermore, the con-
straint cannot be evaluated without explicit knowl-
edge of p(x, y).

W =

x

(

p(x|H1)
p(x|H0)

> kα

.

)

(4)

4.1. Asymptotic Equivalence

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

R

4. The Neyman-Pearson Theory in the
Context of Risk

In Section 1 we provided the loss functional Q ap-
propriate for the classiﬁcation and regression tasks;
however, we did not provide a loss functional for
searches for new particles.
Having chosen the
Neyman-Pearson theory as an explicit example, it is
possible to develop a formal notion of risk.

Once the size of the test, α, has been agreed upon,
the notion of risk is the probability of Type II er-
ror β.
In order to return to the formalism outlined
in Section 2, identify H1 with y = 1 and H0 with
y = 0. Let us consider learning machines that have
a range R which we will compose with a step func-
tion ˜f (x) = Θ(fα(x) − kα) so that by adjusting kα
we insure that the acceptance region W has the ap-
propriate size. The region W is the acceptance region
for H0, thus it corresponds to W = {x| ˜f (x) = 0}
and I − W = {x| ˜f (x) = 1}. We can also trans-
late the quantities p(x|H0) and p(x|H1) into their
learning-theory equivalents p(x|0) = p(x, 0)/p(0) =
(1−y)p(x, y)/
p(x, 1)dx, re-
p(x, 0)dx and yp(x, y)/
spectively. With these substitutions we can rewrite
the Neyman-Pearson theory as follows. A ﬁxed size
gives us the global constraint

R

R

Θ(fα(x) − kα) (1 − y) p(x, y))dxdy
p(x, 0)dx

(5)

α =

R

2With m measurements, we should actually consider the
data as (x1, . . . , xm) ∈ Im, but, for ease of notation, let us
only consider m = 1.

Certain approaches to multivariate analysis lever-
age the many powerful theorems of statistics assum-
ing one can explicitly refer to p(x, y). This depen-
dence places a great deal of stress on the asymptotic
ability to estimate p(x, y) from a ﬁnite set of samples
{(x, y)i}. There are many such techniques for esti-
mating a multivariate density function p(x, y) given
the samples [5, 6]. Unfortunately, for high dimen-
sional domains, the number of samples needed to en-
joy the asymptotic properties grows very rapidly; this
is known as the curse of dimensionality.

In the case that there is no (or negligible) interfer-
ence between the signal process and the background
processes one can avoid the complications imposed
by quantum mechanics and simply add probabili-
ties. This is often the case with searches for new
particles, thus the signal-plus-background hypothe-
sis can be rewritten p(x, |H1) = nsps(x) + nbpb(x),
where ns and nb are normalization constants that
sum to unity. This allows us to rewrite the contours
of the likelihood ratio as contours of the signal-to-
background ratio.
In particular the contours of the
likelihood ratio p(x|H1)/p(x|H0) = kα can be rewrit-
ten as ps(x)/pb(x) = (kα − nb)/ns = k′
α.

The kernel estimation techniques described in this
conference represent a particular statistical approach
in which classiﬁcation is achieved by cutting on a dis-
criminant function D(x) [7]. The discriminant func-
tion D(x) = ps(x)/(ps(x) + pb(x)) is one-to-one with
ps(x)/pb(x) (which is in turn one-to-one with the like-
lihood ratio). These correspondences are only valid
asymptotically, and the ability to accurately approxi-
mate p(x) from an empirical sample is often far from
ideal. However, for particle physics applications, up to
5-dimensional multivariate analyses have shown good
performance [8]. Furthermore, they have the added
beneﬁt that they can be easily understood

WEJT002

PhyStat2003, SLAC, September 8-11

3

4.2. Direct vs. Indirect Methods

The loss functional deﬁned in Eq. 7 is derived from
a minimization on the rate of Type II error. This
is logically distinct from, but asymptotically equiv-
alent to, approximating the likelihood ratio.
In the
case of no interference, this is logically distinct from,
but asymptotically equivalent to, approximating the
signal-to-background ratio.
In fact, most multivari-
ate algorithms are concerned with approximating an
auxiliary function that is one-to-one with the likeli-
hood ratio. Because the methods are not directly con-
cerned with minimizing the rate of Type II error, they
should be considered indirect methods. Furthermore,
the asymptotic equivalence breaks down in most ap-
plications, and the indirect methods are no longer op-
timal. Neural networks, kernel estimation techniques,
and support vector machines all represent indirect so-
lutions to the search for new particles. The Genetic
Programming (GP) approach presented in Section 6
is a direct method concerned with optimizing a user-
deﬁned performance measure.

5. Statistical Learning Theory

The starting point for statistical learning theory is
to accept that we might not know p(x, y) in any an-
alytic or numerical form. This is, indeed, the case
for particle physics, because only {(x, y)i} can be ob-
tained from the Monte Carlo convolution of a well-
known theoretical prediction and complex numerical
In this case, the learn-
description of the detector.
ing problem is based entirely on the training samples
{(x, y)i} with l elements. The risk functional is thus
replaced by the empirical risk functional

l

1
l

Remp(α) =

Q(xi, yi; α).

(8)

i=1
X
There is a surprising result that the true risk
R(α) can be bounded independent of the distribution
p(x, y). In particular, for 0 ≤ Q(x, y; α) ≤ 1

R(α)≤Remp(α) +

h(log(2l/h) + 1) − log(η/4)
l

,
(cid:19)
(9)
where h is the Vapnik-Chervonenkis (VC) dimension
and η is the probability that the bound is violated. As
η → 0, h → ∞, or l → 0 the bound becomes trivial.

s(cid:18)

The VC dimension is a fundamental property of a
learning machine F , and is deﬁned as the maximal
cardinality of a set which can be shattered by F . “A
set {xi} can be shattered by F ” means that for each of
the 2h binary classiﬁcations of the points {xi}, there
exists a fα ∈ F which satisﬁes yi = fα(xi). A set
of three points can be shattered by an oriented line

WEJT002

Figure 1: Example of an oriented line shattering 3
points. Solid and empty dots represent the two classes
for y and each of the 23 permutations are shown.

as illustrated in Figure 1. Note that for a learning
machine with VC dimension h, not every set of h ele-
ments must be shattered by F , but at least one.

Eq. 9 is a remarkable result which relates the num-
ber of training examples l, a fundamental property
of the learning machine h, and the risk R indepen-
dent of the unknown distribution p(x, y). The bounds
provided by Eq. 9 are relatively weak due to their
stunning generality.

It is important to realize that with an independent
testing sample one can evaluate the true risk arbi-
trarily well. This testing sample, by deﬁnition, is not
known to the algorithm, so the bound is useful for
the design of algorithms through structural risk min-
imization. However, neural networks and most other
methods rely on an independent testing sample to aid
in their design and validation. An independent testing
sample is clearly a better way to assess the true risk
of a multivariate algorithm; however, Eq. 9 does shed
light on the issues of overtraining, suggests the num-
ber of training samples that are needed, and oﬀers a
tool to compare diﬀerent algorithms.

5.1. VC Dimension of Neural Networks

In order to apply Eq. 9, one must determine the VC
dimension of neural networks. This is a diﬃcult prob-
lem in combinatorics and geometry aided by algebraic
techniques. Eduardo Sontag has an excellent review of
these techniques and shows that the VC dimension of
neural networks can, thus far, only be bounded fairly
weakly [9]. In particular, if we deﬁne ρ as the number
of weights and biases in the network, then the best
bounds are ρ2 < h < ρ4. In a typical particle physics
neural network one can expect 100 < ρ < 1000, which
translates into a VC dimension as high as 1012, which
implies l > 1013 for reasonable bounds on the risk.
These bounds imply enormous numbers of training
samples when compared to a typical training sample
of 105. Sontag goes on to show that these shattered
sets are incredibly special and that the set of all shat-
tered sets of cardinality µ > 2ρ + 1 is measure zero in
general. Thus, perhaps a more relevant notion of the
VC dimension of a neural network is given by µ.

4

PhyStat2003, SLAC, September 8-11

6. Genetic Programming and Algorithms

Genetic Programming (GP) and Genetic Algo-
rithms (GA) are based on a similar evolutionary
metaphor in which “individuals” (potential solutions
to the problem at hand) compete with respect to a
user-deﬁned performance measure. For new particle
searches, the rate of Type II error, the signiﬁcance,
the exclusion potential, or G. Punzi’s suggestion [10]
are all reasonable performance measures. Ideally, one
would use as a performance measure the same pro-
cedure that will be used to quote the results of the
experiment. For instance, there is no reason (other
than speed) that one could not include discriminat-
ing variables and systematic error in the optimization
procedure (in fact, the author has done both).

The use of GP for the classiﬁcation is fairly lim-
ited; however, it can be traced to the early works on
the subject by Koza [11]. To the best of the author’s
knowledge, the ﬁrst application of GP within particle
physics will appear in [12]. The diﬀerence between the
algorithms is that GAs evolve a bit string which typ-
ically encodes parameters to a pre-existing program,
function, or class of cuts, while GP directly evolves
the programs or functions. For example, Field and
Kanev [13] used Genetic Algorithms to optimize the
lower- and upper-bounds for six 1-dimensional cuts
on Modiﬁed Fox-Wolfram “shape” variables. In that
case, the phase-space region was a pre-deﬁned 6-cube
and the GA was simply evolving the parameters for
the upper- and lower-bounds. On the other hand, GP
algorithm is not constrained to a pre-deﬁned shape or
Instead, the GP approach is con-
parametric form.
cerned directly with the construction of an optimal,
non-trivial phase space region (i.e. an acceptance re-
gion W ) with respect to a user-deﬁned performance
measure. GPs which only produce polynomial expres-
sions form a vector space, which allows for a quick
approximation of their VC dimension [9].

7. Conclusions

Clearly multivariate algorithms will have an in-
creasingly important role in high energy physics,
which necessitates that the ﬁeld develop a coherent
formalism and carefully consider what it means for
a method to be optimal. Statistical learning theory
oﬀers a formalism that is general enough to describe
all of the common multivariate analysis techniques,
and provides interesting results relating risk, the num-
ber of training samples, and the learning capacity of
the algorithm. However, independent testing sam-
ples and the global constraint on the rate of Type I
error places some strain on the risk formalism. Fi-
nally, when one takes into account limited training

data and systematic errors it is not clear that indirect
methods are truly optimizing an experiments sensitiv-
ity. Direct methods, such as Genetic Programming,
force analysts to be more clear about what statistical
statements they plan to make and remove an artiﬁcial
boundary between the goals of the experiment and the
optimization procedures of the algorithm.

Acknowledgments

This work was supported by a graduate research fel-
lowship from the National Science Foundation and US
Department of Energy Grant DE-FG0295-ER40896.

References

[1] V. Vapnik and A.J. Cervonenkis. The uniform
convergence of frequencies of the appearance of
events to their probabilities. Dokl. Akad. Nauk
SSSR, 1968. in Russian.

[2] V. Vapnik. The Nature of Statistical Learning
Theory. Springer, New York, 2nd edition, 2000.
[3] J.K Stuart, A. Ord and S. Arnold. Kendall’s
Advanced Theory of Statistics, Vol 2A (6th Ed.).
Oxford University Press, New York, 1994.

[4] Search for the standard model Higgs boson at

LEP. Phys. Lett., B565:61–75, 2003.

[5] D. Scott. Multivariate Density Estimation: The-
ory, Practice, and Visualization. John Wiley and
Sons Inc., 1992.

[6] K. Cranmer. Kernel estimation in high-energy
physics. Comput. Phys. Commun., 136:198–207,
2001.

[7] A. Askew. Event selection with adaptive gaussian

kernels. In PhyStat2003, 2003.

[8] L. H¨olmstrom et. al. A new multivariate tech-
nique for top quark search. Comput. Phys. Com-
mun., 88:195–210, 1995.

[9] E. Sontag. VC dimension of neural networks.
In C.M. Bishop, editor, Neural Networks and
Machine Learning, pages 69–95, Berlin, 1998.
Springer-Verlag.

[10] G. Punzi. Sensitivity of searches for new signals
and its optimization. In PhyStat2003, 2003.
[11] J.R. Koza. Genetic Programming: On the Pro-
gramming of Computers by Means of Natural Se-
lection. MIT Press, Cambridge, MA, 1992.
[12] K. Cranmer and R.S. Bowman. PhysicsGP: A
genetic programming approach to event selection.
submitted to Comput. Phys. Commun.

[13] R. D. Field and Y. A. Kanev. Using collider event
topology in the search for the six-jet decay of top
quark antiquark pairs. hep-ph/9801318, 1997.

WEJT002


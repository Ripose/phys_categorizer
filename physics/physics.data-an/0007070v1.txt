0
0
0
2

 
l
u
J
 

0
2

 
 
]
n
a
-
a
t
a
d

.
s
c
i
s
y
h
p
[
 
 

1
v
0
7
0
7
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Predictability, complexity and learning

William Bialek,1 Ilya Nemenman,1,2 and Naftali Tishby1,3

1NEC Research Institute, 4 Independence Way, Princeton, New Jersey 08540
2Department of Physics, Princeton University, Princeton, New Jersey 08544

3Institute for Computer Science, and Center for Neural Computation

Hebrew University, Jerusalem 91904, Israel

September 26, 2013

We deﬁne predictive information Ipred(T ) as the mutual information between
the past and the future of a time series. Three qualitatively diﬀerent behaviors
are found in the limit of large observation times T : Ipred(T ) can remain ﬁnite,
grow logarithmically, or grow as a fractional power law. If the time series allows
us to learn a model with a ﬁnite number of parameters, then Ipred(T ) grows
logarithmically with a coeﬃcient that counts the dimensionality of the model
space. While logarithmic growth of related information theoretic quantities has
been found in previous work on learning, power law growth has not been seen
in this context. We ﬁnd that it is associated, for example, with the learning
of inﬁnite parameter (or nonparametric) models such as continuous functions
with smoothness constraints. There are connections between the predictive in-
formation and measures of complexity that have been deﬁned both in learning
theory and in the analysis of physical systems through statistical mechanics and
dynamical systems theory. Further, in the same way that entropy provides the
unique measure of available information consistent with some simple and plausi-
ble conditions, we argue that the divergent part of Ipred(T ) provides the unique
measure for the complexity of dynamics underlying a time series. Finally, we
discuss how these ideas may be useful in diﬀerent problems in physics, statistics,
and biology.

1

Contents

1 Introduction

2 Fundamentals

3 Learning and predictability

3.1 A test case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Learning a parameterized distribution . . . . . . . . . . . . . . .
3.3 Learning a parameterized process . . . . . . . . . . . . . . . . . .
3.4 Taming the ﬂuctuations . . . . . . . . . . . . . . . . . . . . . . .
3.5 Beyond ﬁnite parameterization:

general considerations

. . . . . . . . . . . . . . . . . . . . . . . .
3.6 Beyond ﬁnite parameterization: example . . . . . . . . . . . . . .

4 Ipred as a measure of complexity
4.1 Complexity of statistical models
. . . . . . . . . . . . . . . . . .
4.2 Complexity of dynamical systems . . . . . . . . . . . . . . . . . .
4.3 A unique measure of complexity? . . . . . . . . . . . . . . . . . .

5 Discussion

6 References

3

5

9
10
14
18
20

24
26

29
30
32
33

36

41

2

1 Introduction

There is an obvious interest in having practical algorithms for predicting the
future, and there is a correspondingly large literature on the problem of time
series extrapolation.1 But prediction is both more and less than extrapolation:
we might be able to predict, for example, the chance of rain in the coming
week even if we cannot extrapolate the trajectory of temperature ﬂuctuations.
In the spirit of its thermodynamic origins, information theory (Shannon 1948)
characterizes the potentialities and limitations of all possible prediction algo-
rithms, as well as unifying the analysis of extrapolation with the more general
notion of predictability. Speciﬁcally, we can deﬁne a quantity—the predictive
information—that measures how much our observations of the past can tell us
about the future. The predictive information characterizes the world we are
observing, and we shall see that this characterization is close to our intuition
about the complexity of the underlying dynamics.

Prediction is one of the fundamental problems in neural computation. Much
of what we admire in expert human performance is predictive in character—the
point guard who passes the basketball to a place where his teammate will arrive
in a split second, the chess master who knows how moves made now will inﬂuence
the end game two hours hence, the investor who buys a stock in anticipation that
it will grow in the year to come. More generally, we gather sensory information
not for its own sake but in the hope that this information will guide our actions
(including our verbal actions). But acting takes time, and sense data can guide
us only to the extent that those data inform us about the state of the world at
the time of our actions, so the only components of the incoming data that have
a chance of being useful are those that are predictive. Put bluntly, nonpredictive
information is useless to the organism, and it therefore makes sense to isolate the
predictive information. It will turn out that most of the information we collect
over a long period of time is nonpredictive, so that isolating the predictive
information must go a long way toward separating out those features of the
sensory world that are relevant for behavior.

One of the most important examples of prediction is the phenomenon of
generalization in learning. Learning is formalized as ﬁnding a model that ex-
plains or describes a set of observations, but again this is useful precisely (and
only) because we expect this model will continue to be valid: in the language
of learning theory [see, for example, Vapnik (1998)] an animal can gain selec-
tive advantage not from its performance on the training data but only from its
performance at generalization. Generalizing—and not “overﬁtting” the training
data—is precisely the problem of isolating those features of the data that have
predictive value (see also Bialek and Tishby, in preparation). Further, we know

1The classic papers are by Kolmogoroﬀ (1939, 1941) and Wiener (1949), who essentially
solved all the extrapolation problems that could be solved by linear methods. Our understand-
ing of predictability was changed by developments in dynamical systems, which showed that
apparently random (chaotic) time series could arise from simple deterministic rules, and this
led to vigorous exploration of nonlinear extrapolation algorithms (Abarbanel et al. 1993). For
a review comparing diﬀerent approaches, see the conference proceedings edited by Weigend
and Gershenfeld (1994).

3

that the success of generalization hinges on controlling the complexity of the
models that we are willing to consider as possibilities. Finally, learning a model
to describe a data set can be seen as an encoding of those data, as emphasized
by Rissanen (1989), and the quality of this encoding can be measured using the
ideas of information theory. Thus the exploration of learning problems should
provide us with explicit links among the concepts of entropy, predictability, and
complexity.

The notion of complexity arises not only in learning theory, but also in several
other contexts. Some physical systems exhibit more complex dynamics than
others (turbulent vs. laminar ﬂows in ﬂuids), and some systems evolve toward
more complex states than others (spin glasses vs. ferromagnets). The problem
of characterizing complexity in physical systems has a substantial literature of
its own [for an overview see Bennett (1990)]. In this context several authors
have considered complexity measures based on entropy or mutual information,
although as far as we know no clear connections have been drawn among the
measures of complexity that arise in learning theory and those that arise in
dynamical systems and statistical mechanics.

An essential diﬃculty in quantifying complexity is to distinguish complexity
from randomness. A true random string cannot be compressed and hence re-
quires a long description; it thus is complex in the sense deﬁned by Kolmogorov
(1965, Li and Vit´anyi 1993, Vit´anyi and Li 2000), yet the physical process that
generates this string may have a very simple description. Both in statistical me-
chanics and in learning theory our intuitive notions of complexity correspond
to the statements about complexity of the underlying process, and not directly
to the description length or Kolmogorov complexity.

Our central result is that the predictive information provides a general mea-
sure of complexity which includes as special cases some relevant concepts from
learning theory and from dynamical systems. While the work on the complexity
of models in learning theory rests speciﬁcally on the idea that one is trying to
infer a model from data, the predictive information is a property of the data (or,
more precisely, of an ensemble of data) itself without reference to a speciﬁc class
of underlying models. If the data are generated by a process in a known class
but with unknown parameters, then we can calculate the predictive information
explicitly and show that this information diverges logarithmically with the size
of the data set we have observed; the coeﬃcient of this divergence counts the
number of parameters in the model, or more precisely the eﬀective dimension
of the model class, and this provides a link to known results of Rissanen and
others. But our approach also allows us to quantify the complexity of processes
that fall outside the ﬁnite dimensional models of conventional learning theory,
and we show that these more complex processes are characterized by a power–law
rather than a logarithmic divergence of the predictive information.

By analogy with the analysis of critical phenomena in statistical physics,
the separation of logarithmic from power–law divergences, together with the
measurement of coeﬃcients and exponents for these divergences, allows us to
deﬁne “universality classes” for the complexity of data streams. The power–law
or nonparametric class of processes may be crucial in real world learning tasks,

4

where the eﬀective number of parameters becomes so large that asymptotic
results for ﬁnitely parameterizable models are inaccessible in practice. There is
empirical evidence that simple physical systems can generate dynamics in this
complexity class, and there are hints that language also may fall in this class.

Finally, we argue that the divergent components of the predictive information
provide a unique measure of complexity that is consistent with certain simple
requirements. This argument is in the spirit of Shannon’s original derivation of
entropy as the unique measure of available information. We believe that this
uniqueness argument provides a conclusive answer to the question of how one
should quantify the complexity of a process generating a time series.

With the evident cost of lengthening our discussion, we have tried to give
a self–contained presentation that develops our point of view, uses simple ex-
amples to connect with known results, and then generalizes and goes beyond
these results.2 Even in cases where at least the qualitative form of our results
is known from previous work, we believe that our point of view elucidates some
issues that may have been less the focus of earlier studies. Last but not least,
we explore the possibilities for connecting our theoretical discussion with the
experimental characterization of learning and complexity in neural systems.

2 Fundamentals

The problem of prediction comes in various forms, as noted above. Information
theory allows us to treat the diﬀerent notions of prediction on the same footing.
The ﬁrst step is to recognize that all predictions are probabilistic—even if we
can predict the temperature at noon tomorrow, we should provide error bars or
conﬁdence limits on our prediction. The next step is to remember that, even
before we look at the data, we know that certain futures are more likely than
others, and we can summarize this knowledge by a prior probability distribution
for the future. Our observations on the past lead us to a new, more tightly con-
centrated distribution, the distribution of futures conditional on the past data.
Diﬀerent kinds of predictions are diﬀerent slices through or averages over this
conditional distribution, but information theory quantiﬁes the “concentration”
of the distribution without making any commitment as to which averages will
be most interesting.

Imagine that we observe a stream of data x(t) over a time interval −T <
t < 0; let all of these past data be denoted by the shorthand xpast. We are
interested in saying something about the future, so we want to know about the
data x(t) that will be observed in the time interval 0 < t < T ′; let these future
data be called xfuture. In the absence of any other knowledge, futures are drawn
from the probability distribution P (xfuture), while observations of particular past
data xpast tell us that futures will be drawn from the conditional distribution
P (xfuture|xpast). The greater concentration of the conditional distribution can
2Some of the basic ideas presented here, together with some connections to earlier work,
can be found in brief preliminary reports (Bialek 1995; Bialek and Tishby 1999). The central
results of the present work, however, were at best conjectures in these preliminary accounts.

5

be quantiﬁed by the fact that it has smaller entropy than the prior distribution,
and this reduction in entropy is Shannon’s deﬁnition of the information that
the past provides about the future. We can write the average of this predictive
information as

Ipred(T, T ′) = * log2(cid:20) P (xfuture|xpast)

P (xfuture)

(cid:21)+

(1)

= −hlog2 P (xfuture)i − hlog2 P (xpast)i

Each of the terms in Eq.

− [−hlog2 P (xfuture, xpast)i] ,

(2)
where h···i denotes an average over the joint distribution of the past and the
future, P (xfuture, xpast).
(2) is an entropy. Since we are interested in
predictability or generalization, which are associated with some features of the
signal persisting forever, we may assume stationarity or invariance under time
translations. Then the entropy of the past data depends only on the duration
of our observations, so we can write −hlog2 P (xpast)i = S(T ), and by the same
argument −hlog2 P (xfuture)i = S(T ′). Finally, the entropy of the past and the
future taken together is the entropy of observations on a window of duration
T + T ′, so that −hlog2 P (xfuture, xpast)i = S(T + T ′). Putting these equations
together, we obtain

Ipred(T, T ′) = S(T ) + S(T ′) − S(T + T ′).

(3)

In the same way that the entropy of a gas at ﬁxed density is proportional to
the volume, the entropy of a time series (asymptotically) is proportional to its
duration, so that limT →∞ S(T )/T = S0; entropy is an extensive quantity. But
from Eq. (3) any extensive component of the entropy cancels in the computation
of the predictive information: predictability is a deviation from extensivity. If we
write S(T ) = S0T + S1(T ), then Eq. (3) tells us that the predictive information
is related only to the nonextensive term S1(T ).
We know two general facts about the behavior of S1(T ). First, the correc-
tions to extensive behavior are positive, S1(T ) ≥ 0. Second, the statement that
entropy is extensive is the statement that the limit

lim
T →∞

S(T )

T

= S0

exists, and for this to be true we must also have

lim
T →∞

S1(T )

T

= 0.

(4)

(5)

Thus the nonextensive terms in the entropy must be subextensive, that is they
must grow with T less rapidly than a linear function. Taken together, these facts
guarantee that the predictive information is positive and subextensive. Further,
if we let the future extend forward for a very long time, T ′ → ∞, then we can
measure the information that our sample provides about the entire future,

Ipred(T ) = lim

T ′→∞Ipred(T, T ′) = S1(T ).

(6)

6

If we have been observing a time series for a (long) time T , then the total
amount of data we have taken in is measured by the entropy S(T ), and at
large T this is given approximately by S0T . But the predictive information
that we have gathered cannot grow linearly with time, even if we are making
predictions about a future which stretches out to inﬁnity. As a result, of the
total information we have taken in by observing xpast, only a vanishing fraction
is of relevance to the prediction:

lim
T →∞

Predictive Information

Total Information

=

Ipred(T )
S(T ) → 0.

(7)

In this precise sense, most of what we observe is irrelevant to the problem of
predicting the future.3

Consider the case where time is measured in discrete steps, so that we have
seen N time points x1, x2,··· , xN . How much have we learned about the un-
derlying pattern in these data? The more we know, the more eﬀectively we
can predict the next data point xN +1 and hence the fewer bits we will need to
describe the deviation of this data point from our prediction: our accumulated
knowledge about the time series is measured by the degree to which we can
compress the description of new observations. On average, the length of the
code word required to describe the point xN +1, given that we have seen the
previous N points, is given by

ℓ(N ) = −hlog2 P (xN +1|x1, x2,··· , xN )i bits,

(8)

where the expectation value is taken over the joint distribution of all the N + 1
points, P (x1, x2,··· , xN , xN +1). It is easy to see that

ℓ(N ) = S(N + 1) − S(N ) ≈

∂S(N )

∂N

.

(9)

As we observe for longer times, we learn more and this word length decreases.
It is natural to deﬁne a learning curve that measures this improvement. Usually
we deﬁne learning curves by measuring the frequency or costs of errors; here
the cost is that our encoding of the point xN +1 is longer than it could be if
we had perfect knowledge. This ideal encoding has a length which we can
ﬁnd by imagining that we observe the time series for an inﬁnitely long time,
ℓideal = limN→∞ ℓ(N ), but this is just another way of deﬁning the extensive
component of the entropy S0. Thus we can deﬁne a learning curve

,

7

Λ(N ) ≡ ℓ(N ) − ℓideal

= S(N + 1) − S(N ) − S0
= S1(N + 1) − S1(N )
≈

∂Ipred(N )

∂S1(N )

∂N

=

∂N

(10)

(11)

3We can think of Eq. (7) as a law of diminishing returns: although we collect data in
proportion to our observation time T , a smaller and smaller fraction of this information is
useful in the problem of prediction. These diminishing returns are not due to a limited lifetime,
since we calculate the predictive information assuming that we have a future extending forward
to inﬁnity. A senior colleague points out that this is an argument for changing ﬁelds before
becoming too expert.

and we see once again that the extensive component of the entropy cancels.

It is well known that the problems of prediction and compression are related,
and what we have done here is to illustrate one aspect of this connection. Specif-
ically, if we ask how much one segment of a time series can tell us about the
future, the answer is contained in the subextensive behavior of the entropy. If
we ask how much we are learning about the structure of the time series, then the
natural and universally deﬁned learning curve is related again to the subexten-
sive entropy: the learning curve is the derivative of the predictive information.
This universal learning curve is connected to the more conventional learning
curves in speciﬁc contexts. As an example (cf. Section 3.1), consider ﬁtting
a set of data points {xn, yn} with some class of functions y = f (x; α), where
the α are unknown parameters that need to be learned; we also allow for some
Gaussian noise in our observation of the yn. Here the natural learning curve is
the evolution of χ2 for generalization as a function of the number of examples.
Within the approximations discussed below, it is straightforward to show that
as N becomes large,

hχ2(N )i =

1
σ2 h[y − f (x; α)]2i → 2Λ(N ) + 1,

(12)

where σ2 is the variance of the noise. Thus a more conventional measure of
performance at learning a function is equal to the universal learning curve de-
ﬁned purely by information theoretic criteria, and this in turn is controlled by
subextensive terms in the entropy.

Diﬀerent quantities related to the subextensive entropy have been discussed
in several contexts. For example, the code length ℓ(N ) has been deﬁned as
a learning curve in the speciﬁc case of neural networks (Opper and Haussler
1995) and has been termed the “thermodynamic dive” (Crutchﬁeld and Shalizi
1998) and “N th order block entropy” (Grassberger 1986). Mutual information
between all of the past and all of the future (both semi–inﬁnite) is known also
as the “excess entropy,” “eﬀective measure complexity,” “stored information,”
and so on [see Shalizi and Crutchﬁeld (1999) and references therein, as well
as the discussion below].
If the data allow a description by a model with a
ﬁnite number of parameters, then mutual information between the data and the
parameters is of interest, and this is also the predictive information about all
of the future; some special cases of this problem have been discussed by Opper
and Haussler (1995) and by Herschkowitz and Nadal (1999). What is important
is that the predictive information or subextensive entropy is related to all these
quantities, and that it can be deﬁned for any process without a reference to a class
of models. It is this universality that we ﬁnd appealing, and this universality
is strongest if we focus on the limit of long observation times. Qualitatively,
in this regime (T → ∞) we expect the predictive information to behave in one
of three diﬀerent ways:
it may either stay ﬁnite, or grow to inﬁnity together
with T ; in the latter case the rate of growth may be slow (logarithmic) or fast
(sublinear power).

The ﬁrst possibility, limT →∞ Ipred(T ) = constant, means that no matter how
long we observe we gain only a ﬁnite amount of information about the future.

8

This situation prevails, for example, when the dynamics are too regular: for a
purely periodic system, complete prediction is possible once we know the phase,
and if we sample the data at discrete times this is a ﬁnite amount of information;
longer period orbits intuitively are more complex and also have larger Ipred, but
this doesn’t change the limiting behavior limT →∞ Ipred(T ) = constant.

Alternatively, the predictive information can be small when the dynamics
are irregular but the best predictions are controlled only by the immediate past,
so that the correlation times of the observable data are ﬁnite [see, for example,
Crutchﬁeld and Feldman (1997)]. Imagine, for example, that we observe x(t)
at a series of discrete times {tn}, and that at each time point we ﬁnd the value
xn. Then we can always write the joint distribution of the N data points as a
product,

P (x1, x2,··· , xN ) = P (x1)P (x2|x1)P (x3|x2, x1)··· .

(13)

For Markov processes, what we observe at tn depends only on events at the
previous time step tn−1, so that

P (xn|{x1≤i≤n−1}) = P (xn|xn−1),

and hence the predictive information reduces to

Ipred =* log2(cid:20) P (xn|xn−1)

P (xn)

(cid:21)+.

(14)

(15)

The maximum possible predictive information in this case is the entropy of the
distribution of states at one time step, which in turn is bounded by the logarithm
of the number of accessible states. To approach this bound the system must
maintain memory for a long time, since the predictive information is reduced
by the entropy of the transition probabilities. Thus systems with more states
and longer memories have larger values of Ipred.

More interesting are those cases in which Ipred(T ) diverges at large T . In
physical systems we know that there are critical points where correlation times
become inﬁnite, so that optimal predictions will be inﬂuenced by events in the
arbitrarily distant past. Under these conditions the predictive information can
grow without bound as T becomes large; for many systems the divergence is
logarithmic, Ipred(T → ∞) ∝ ln T . Long range correlation also are important
in a time series where we can learn some underlying rules. It will turn out that
when the set of possible rules can be described by a ﬁnite number of parameters,
the predictive information again diverges logarithmically, and the coeﬃcient of
this divergence counts the number of parameters. Finally, a faster growth is
also possible, so that Ipred(T → ∞) ∝ T α, and we shall see that this behavior
emerges from, for example, nonparametric learning problems.

3 Learning and predictability

Learning is of interest precisely in those situations where correlations or associ-
ations persist over long periods of time. In the usual theoretical models, there

9

is some rule underlying the observable data, and this rule is valid forever; exam-
ples seen at one time inform us about the rule, and this information can be used
to make predictions or generalizations. The predictive information quantiﬁes
the average generalization power of examples, and we shall see that there is a
direct connection between the predictive information and the complexity of the
possible underlying rules.

3.1 A test case

Let us begin with a simple example already mentioned above. We observe two
streams of data x and y, or equivalently a stream of pairs (x1, y1), (x2, y2), ··· ,
(xN, yN). Assume that we know in advance that the x’s are drawn independently
and at random from some distribution P (x), while the y’s are noisy versions of
some function acting on x,

yn = f (xn; α) + ηn,

(16)

where f (x; α) is a class of functions parameterized by α, and ηn is some noise
which for simplicity we will assume is Gaussian with some known standard
deviation σ. We can even start with a very simple case, where the function
class is just a linear combination of some basis functions, so that

f (x; α) =

αµφµ(x).

KXµ=1

(17)

The usual problem is to estimate, from N pairs {xi, yi}, the values of the pa-
rameters α; in favorable cases such as this we might even be able to ﬁnd an
eﬀective regression formula. We are interested in evaluating the predictive infor-
mation, which means that we need to know the entropy S(N ). We go through
the calculation in some detail because it provides a model for the more general
case.

To evaluate the entropy S(N ) we ﬁrst construct the probability distribution
P (x1, y1, x2, y2,··· , xN, yN). The same set of rules apply to the whole data
stream, which here means that the same parameters α apply for all pairs {xi, yi},
but these parameters are chosen at random from a distribution P (α) at the start
of the stream. Thus we write

P (x1, y1, x2, y2,··· , xN, yN)

=Z dKα P (x1, y1, x2, y2,··· , xN, yN|α)P (α) ,

(18)

and now we need to construct the conditional distributions for ﬁxed α. By
hypothesis each x is chosen independently, and once we ﬁx α each yi is correlated
only with the corresponding xi, so that we have

P (x1, y1, x2, y2,··· , xN, yN|α) =

NYi=1

10

[P (xi) P (yi|xi; α)] .

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

Our placement of the factors of N means that both Aµν and Bµ are of order
unity as N → ∞. These quantities are empirical averages over the samples
{xi, yi}, and if the φµ are well behaved we expect that these empirical means
converge to expectation values for most realizations of the series {xi}:

lim
N→∞

Aµν ({xi}) = A∞

µν =

lim
N→∞

Bµ({xi, yi}) = B∞

µ =

1

σ2Z dxP (x)φµ(x)φν (x) ,
KXν=1

µν ¯αν ,

A∞

Further, with the simple assumptions above about the class of functions and
Gaussian noise, the conditional distribution of yi has the form

Putting all these factors together,

P (yi|xi; α) =

P (x1, y1, x2, y2,··· , xN, yN)

1

1

√2πσ2

KXµ=1

2σ2 yi −

exp−
αµφµ(xi)!2 .
√2πσ2(cid:19)NZ dKα P (α) exp"−
P (xi)#(cid:18) 1
KXµ=1
KXµ,ν=1

Aµν ({xi})αµαν + N

N
2

y2

1
2σ2

NXi=1

i#
Bµ({xi, yi})αµ# ,

=" NYi=1
× exp"−

where

Aµν ({xi}) =

Bµ({xi, yi}) =

1

σ2N

1

σ2N

NXi=1
NXi=1

φµ(xi)φν (xi), and

yiφµ(xi).

where ¯α are the parameters that actually gave rise to the data stream {xi, yi}.

In fact we can make the same argument about the terms inP y2

i ,

lim
N→∞

y2

i = N σ2" KXµ,ν=1

NXi=1

¯αµA∞

µν ¯αν + 1# .

Conditions for this convergence of empirical means to expectation values are
at the heart of learning theory. Our approach here is ﬁrst to assume that
this convergence works, then to examine the consequences for the predictive
information, and ﬁnally to address the conditions for and implications of this
convergence breaking down.

11

Putting the diﬀerent factors together, we obtain

P (x1, y1, x2, y2,··· , xN, yN)

P (xi)#(cid:18) 1

√2πσ2(cid:19)NZ dKαP (α) exp [−N EN (α;{xi, yi})] ,

f→" NYi=1

where the eﬀective “energy” per sample is given by

EN (α;{xi, yi}) =

1
2

+

1
2

KXµ,ν=1

(αµ − ¯αµ)A∞

µν (αν − ¯αν).

Here we use the symbolf→ to indicate that we not only take the limit of large

N , but also neglect the ﬂuctuations. Note that in this approximation the de-
pendence on the sample points themselves is hidden in the deﬁnition of ¯α as
being the parameters that generated the samples.

The integral that we need to do in Eq. (27) involves an exponential with a
large factor N in the exponent; the free energy FN is of order unity as N → ∞.
This suggests that we evaluate the integral by a saddle point or steepest descent
approximation [similar analyses were performed by Clarke and Barron (1990),
by MacKay (1992), and by Balasubramanian (1997)]:

Z dKαP (α) exp [−N EN (α;{xi, yi})] ≈ P (αcl)
× exp(cid:20)−N EN (αcl;{xi, yi}) −

N
2π −

K
2

1
2

ln

ln det FN + ···(cid:21) ,

(29)

where αcl is the “classical” value of α determined by the extremal conditions

(27)

(28)

(30)

(31)

the matrix FN consists of the second derivatives of EN ,

∂EN (α;{xi, yi})

∂αµ

= 0,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)α=αcl

FN =

∂2EN (α;{xi, yi})

∂αµ∂αν

,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)α=αcl

and ··· denotes terms that vanish as N → ∞. If we formulate the problem of
estimating the parameters α from the samples {xi, yi}, then as N → ∞ the
matrix NFN is the Fisher information matrix (Cover and Thomas 1991); the
eigenvectors of this matrix gives the principal axes for the error ellipsoid in
parameter space, and the (inverse) eigenvalues give the variances of parameter
estimates along each of these directions. The classical αcl diﬀers from ¯α only
in terms of order 1/N ; we neglect this diﬀerence and further simplify the calcu-
lation of leading terms as N becomes large. After a little more algebra, then,

12

we ﬁnd the probability distribution we have been looking for:

P ( ¯α) exp(cid:20)−

P (x1, y1, x2, y2,··· , xN, yN)

f→" NYi=1

P (xi)# 1
ZA =q(2π)K det A∞.

ZA

where the normalization constant

N
2

ln(2πeσ2) −

K
2

ln N + ···(cid:21) , (32)

(33)

Again we note that the sample points {xi, yi} are hidden in the value of ¯α that
gave rise to these points.4
To evaluate the entropy S(N ) we need to compute the expectation value of
the (negative) logarithm of the probability distribution in Eq. (32); there are
three terms. One is constant, so averaging is trivial. The second term depends
only on the xi, and because these are chosen independently from the distribution
P (x) the average again is easy to evaluate. The third term involves ¯α, and we
need to average this over the joint distribution P (x1, y1, x2, y2,··· , xN, yN). As
above, we can evaluate this average in steps: ﬁrst we choose a value of the
parameters ¯α, then we average over the samples given these parameters, and
ﬁnally we average over parameters. But because ¯α is deﬁned as the parameters
that generate the samples, this stepwise procedure simpliﬁes enormously. The
end result is that

S(N ) = N(cid:20)Sx +

1
2

log2(2πeσ2)(cid:21) +

log2 N + Sα +hlog2 ZAiα +··· , (34)
where h···iα means averaging over parameters, Sx is the entropy of the distri-
bution of x,

K
2

(35)

(36)

and similarly for the entropy of the distribution of parameters,

Sx = −Z dx P (x) log2 P (x),
Sα = −Z dKα P (α) log2 P (α).

4We emphasize again that there are two approximations leading to Eq. (32). First, we
have replaced empirical means by expectation values, neglecting ﬂuctuations associated with
the particular set of sample points {xi, yi}. Second, we have evaluated the average over
parameters in a saddle point approximation. At least under some condition, both of these
approximations would become increasingly accurate as N → ∞, so that this approach should
yield the asymptotic behavior of the distribution and hence the subextensive entropy at large
N . Although we give a more detailed analysis below, it is worth noting here how things can go
wrong. The two approximations are independent, and we could imagine that ﬂuctuations are
important but saddle point integration still works, for example. Controlling the ﬂuctuations
turns out to be exactly the question of whether our ﬁnite parameterization captures the true
dimensionality of the class of models, as discussed in the classic work of Vapnik, Chervonenkis,
and others [see Vapnik (1998) for a review]. The saddle point approximation can break
down because the saddle point becomes unstable or because multiple saddle points become
important.
It will turn out that instability is exponentially improbable as N → ∞, while
multiple saddle points are a real problem in certain classes of models, again when counting
parameters doesn’t really measure the complexity of the model class.

13

The diﬀerent terms in the entropy Eq. (34) have a straightforward interpre-

tation. First we see that the extensive term in the entropy,

S0 = Sx +

1
2

log2(2πeσ2),

(37)

reﬂects contributions from the random choice of x and from the Gaussian noise
in y; these extensive terms are independent of the variations in parameters α,
and these would be the only terms if the parameters were not varying (that
is, if there were nothing to learn). There also is a term which reﬂects the
entropy of variations in the parameters themselves, Sα. This entropy is not
invariant with respect to coordinate transformations in the parameter space,
but the term hlog2 ZAiα compensates for this noninvariance. Finally, and most
interestingly for our purposes, the subextensive piece of the entropy is dominated
by a logarithmic divergence,

S1(N ) →

K
2

log2 N (bits).

(38)

The coeﬃcient of this divergence counts the number of parameters independent
of the coordinate system that we choose in the parameter space. Furthermore,
this result does not depend on the set of basis functions {φµ(x)}. This is a hint
that the result in Eq. (38) is more universal than our simple example.

3.2 Learning a parameterized distribution

The problem discussed above is an example of supervised learning: we are given
examples of how the points xn map into yn, and from these examples we are to
induce the association or functional relation between x and y. An alternative
view is that pair of points (x, y) should be viewed as a vector ~x, and what
we are learning is the distribution of this vector. The problem of learning a
distribution usually is called unsupervised learning, but in this case supervised
learning formally is a special case of unsupervised learning; if we admit that
all the functional relations or associations that we are trying to learn have an
element of noise or stochasticity, then this connection between supervised and
unsupervised problems is quite general.

Suppose a series of random vector variables {~xi} are drawn independently
from the same probability distribution Q(~x|α), and this distribution depends
on a (potentially inﬁnite dimensional) vector of parameters α. As above, the
parameters are unknown, and before the series starts they are chosen randomly
from a distribution P (α). With no constraints on the densities P (α) or Q(~x|α)
it is impossible to derive any regression formulas for parameter estimation, but
one can still calculate the leading terms in the entropy of the data series and
thus the predictive information. We begin by writing, by analogy with Eq. (18),

P (~x1, ~x2,··· , ~xN) =Z dKαP (α)

NYi=1

14

Q(~xi|α).

(39)

Now suppose as before that we know the parameters ¯α that gave rise to the
particular data {~xi}. We can write

P (~x1,··· , ~xN) =

=

NYj=1
NYj=1

NYi=1(cid:20) Q(~xi|α)
Q(~xi| ¯α)(cid:21)

Q(~xj| ¯α)Z dK αP (α)
Q(~xj| ¯α)P (α)Z dKα exp [−NEN (α;{~xi})] ,
NXi=1

1
N

EN (α;{~xi}) = −

Q(~xi| ¯α)(cid:21) .
ln(cid:20) Q(~xi|α)
Q(~x| ¯α)(cid:21) − ψ(α, ¯α;{xi}),
EN (α;{~xi}) = −Z dDxQ(x| ¯α) ln(cid:20) Q(~x|α)

(40)

(41)

(42)

As before we expect empirical means to converge to expectation values, so that

where ψ → 0 as N → ∞; here we neglect ψ, and return to this term below.
The second term in the free energy Eq. (42) is the Kullback–Leibler diver-
gence, DKL( ¯α||α), between the true distribution characterized by parameters
¯α and the possible distribution characterized by α. Thus at large N we have

P (~x1, ~x2,··· , ~xN)f→

Q(~xj| ¯α)Z dKαP (α) exp [−N DKL( ¯α||α)] ,

NYj=1

(43)

where again the notationf→ reminds us that we are not only taking the limit of

large N but also making another approximation in neglecting ﬂuctuations. By
the same arguments as above we can proceed (formally) to compute the entropy
of this distribution, and we ﬁnd

1 (N ),

S(N ) ≈ S0 · N + S(a)
S0 = Z dKαP (α)(cid:20)−Z dDxQ(~x|α) log2 Q(~x|α)(cid:21) , and
1 (N ) = −Z dK ¯αP ( ¯α) log2(cid:20)Z dK αP (α)e−N DKL( ¯α||α)(cid:21) .

S(a)

(44)

(45)

(46)

1

Here S(a)
is an approximation to S1 that neglects ﬂuctuations ψ. This is the
same as the annealed approximation in the statistical mechanics of disordered
systems, as has been used widely in the study of supervised learning problems
(Seung et al. 1992).

The extensive term S0, Eq. (45), is the average entropy of a distribution in
our family of possible distributions, generalizing the result of Eq. (37). The
subextensive terms in the entropy are controlled by the N dependence of the
partition function

Z( ¯α; N ) =Z dKαP (α) exp [−N DKL( ¯α||α)] ,

15

(47)

and S1(N ) = −hlog2 Z( ¯α; N )i ¯α is analogous to the free energy. Since what
is important in this integral is the Kullback–Leibler (KL) divergence between
diﬀerent distributions, it is natural to ask about the density of models that are
KL divergence D away from the target ¯α,

ρ(D; ¯α) =Z dK αP (α)δ[D − DKL( ¯α||α)];

(48)

note that this density could be very diﬀerent for diﬀerent targets. The density
of divergences is normalized because the original distribution over parameter
space, P (α), is normalized,

Finally, the partition function takes the simple form

Z dDρ(D; ¯α) =Z dK αP (α) = 1.
Z( ¯α; N ) =Z dDρ(D; ¯α) exp[−N D].
Z(β) =Z dEρ(E) exp[−βE],

(49)

(50)

(51)

We recall that in statistical mechanics the partition function is given by

where ρ(E) is the density of states that have energy E, and β is the inverse tem-
perature. Thus the subextensive entropy in our learning problem is analogous
to a system in which energy corresponds to the Kullback–Leibler divergence
relative to the target model, and temperature is inverse to the number of exam-
ples. As we increase the length N of the time series we have observed, we “cool”
the system and hence probe models which approach the target; the dynamics
of this approach is determined by the density of low energy states, that is the
behavior of ρ(D; ¯α) as D → 0.
The structure of the partition function is determined by a competition be-
tween the (Boltzmann) exponential term, which favors models with small D,
and the density term, which favors values of D that can be achieved by the
largest possible number of models. Because there (typically) are many param-
eters, there are very few models with D → 0. This picture of competition
between the Boltzmann factor and a density of states has been emphasized in
previous work on supervised learning (Haussler et al. 1996).

The behavior of the density of states, ρ(D; ¯α), at small D is related to the
more intuitive notion of dimensionality. In a parameterized family of distribu-
tions, the Kullback–Leibler divergence between two distributions with nearby
parameters is approximately a quadratic form,

DKL( ¯α||α) ≈

1

2Xµν

(¯αµ − αµ)Fµν (¯αν − αν) + ··· ,

(52)

where F is the Fisher information matrix. Intuitively, if we have a reasonable
parameterization of the distributions, then similar distributions will be nearby

16

in parameter space, and more importantly points that are far apart in parameter
space will never correspond to similar distributions; Clarke and Barron (1990)
refer to this condition as the parameterization forming a “sound” family of
distributions. If this condition is obeyed, then we can approximate the low D
limit of the density ρ(D; ¯α):

ρ(D; ¯α) = Z dK αP (α)δ[D − DKL( ¯α||α)]

1

≈ Z dK αP (α)δ"D −
2Xµν
= Z dK αP ( ¯α + U · ξ)δ"D −

(¯αµ − αµ)Fµν(¯αν − αν )#
2Xµ

µ# ,

Λµξ2

1

(53)

where U is a matrix that diagonalizes F ,

(U T · F · U)µν = Λµδµν .

(54)
The delta function restricts the components of ξ in Eq. (53) to be of order √D
or less, and so if P (α) is smooth we can make a perturbation expansion. After
some algebra the leading term becomes

ρ(D → 0; ¯α) ≈ P ( ¯α)

2πK/2
Γ(K/2)

(detF )−1/2 D(K−2)/2.

(55)

Here, as before, K is the dimensionality of the parameter vector. Computing
the partition function from Eq. (50), we ﬁnd

Z( ¯α; N → ∞) ≈ f ( ¯α) ·

Γ(K/2)
N K/2 ,

(56)

where f ( ¯α) is some function of the target parameter values. Finally, this allows
us to evaluate the subextensive entropy, from Eqs. (46, 47):

S(a)

1 (N ) = −Z dK ¯αP ( ¯α) log2 Z( ¯α; N )

→

K
2

log2 N + ···

(bits),

(57)

(58)

where ··· are ﬁnite as N → ∞. Thus, general K–parameter model classes
have the same subextensive entropy as for the simplest example considered
in the previous section. To the leading order, this result is independent even
of the prior distribution P (α) on the parameter space, so that the predictive
information seems to count the number of parameters under some very general
conditions.

Although Eq. (58) is true under a wide range of conditions, this cannot be
the whole story. Much of modern learning theory is concerned with the fact
that counting parameters is not quite enough to characterize the complexity of
a model class; the naive dimension of the parameter space K should be viewed in

17

conjunction with the pseudodimension (also known as the shattering dimension
or Vapnik–Chervonenkis dimension dVC), which measures capacity of the model
class, and with the phase space dimension d, which accounts for volumes in the
space of models (Vapnik 1998, Opper 1994). Both of these dimensions can
diﬀer from the number of parameters in several ways. One possibility is that
dVC is inﬁnite when the number of parameters is ﬁnite, a problem discussed
below. Another possibility is that the determinant of F is zero, and hence
dVC and d are both smaller than the number of parameters because we have
adopted a redundant description.
It is possible that this sort of degeneracy
occurs over a ﬁnite fraction but not all of the parameter space, and this is
one way to generate an eﬀective fractional dimensionality. One can imagine
multifractal models such that the eﬀective dimensionality varies continuously
over the parameter space, but it is not obvious where this would be relevant.
Finally, models with d < dVC < ∞ are also possible [see, for example, Opper
(1994)], and this list probably is not exhaustive.
The calculation above, Eq. (55), lets us actually deﬁne the phase space
dimension through the exponent in the small DKL behavior of the model density,

ρ(D → 0; ¯α) ∝ D(d−2)/2,

(59)

and then d appears in place of K as the coeﬃcient of the log divergence in
S1(N ) (Clarke and Barron 1990, Opper 1994). However, this simple conclusion
can fail in two ways. First, it can happen that a macroscopic weight gets
accumulated at some nonzero value of DKL, so that the small DKL behavior
is irrelevant for the large N asymptotics. Second, the ﬂuctuations neglected
here may be uncontrollably large, so that the asymptotics are never reached.
Since controllability of ﬂuctuations is a function of dVC (see Vapnik 1998 and
later in this paper), we may summarize this in the following way. Provided
that the small DKL behavior of the density function is the relevant one, the
coeﬃcient of the logarithmic divergence of Ipred measures the phase space or
the scaling dimension d and nothing else. This asymptote is valid, however,
only for N ≫ dVC. It is still an open question whether the two pathologies that
can violate this asymptotic behavior are related.

3.3 Learning a parameterized process

Consider a process where samples are not independent, and our task is to learn
their joint distribution Q(~x1,··· , ~xN|α). Again, α is an unknown parameter
vector which is chosen randomly at the beginning of the series. If α is a K
dimensional vector, then one still tries to learn just K numbers and there are still
N examples, even if there are correlations. Therefore, although such problems
are much more general than those considered above, it is reasonable to expect
that the predictive information is still measured by (K/2) log2 N provided that
some conditions are met.

One might suppose that conditions for simple results on the predictive in-
formation are very strong, for example that the distribution Q is a ﬁnite order

18

Markov model. In fact all we really need are the following two conditions:

S [{~xi}|α] ≡ −Z dN ~x Q({~xi}|α) log2 Q({~xi}|α)

→ NS0 + S∗
0 ;

S∗
0 = O(1) ,

(60)
(61)

DKL [Q({~xi}| ¯α)||Q({~xi}|α)] → NDKL ( ¯α||α) + o(N ) .

Here the quantities S0, S∗
0 , and DKL ( ¯α||α) are deﬁned by taking limits N → ∞
in both equations. The ﬁrst of the constraints limits deviations from extensivity
to be of order unity, so that if α is known there are no long range correlations
in the data—all of the long range predictability is associated with learning the
parameters.5 The second constraint, Eq. (61), is a less restrictive one, and it
ensures that the “energy” of our statistical system is an extensive quantity.

With these conditions it is straightforward to show that the results of the
previous subsection carry over virtually unchanged. With the same cautious
statements about ﬂuctuations and the distinction between K, d, and dVC, one
arrives at the result:

S(N ) = S0 · N + S(a)
S(a)
log2 N + ···
1 (N ) =

K
2

1 (N ) ,

(bits),

(62)

(63)

where ··· stands for terms of order one. Note again that for the results Eq. (63)
to be valid, the process considered is not required to be a ﬁnite order Markov
process. Memory of all previous outcomes may be kept, provided that the
accumulated memory does not contribute a divergent term to the subextensive
entropy.

It is interesting to ask what happens if the condition in Eq. (60) is violated,
so that there are long range correlations even in the conditional distribution
0 = (K ∗/2) log2 N . Then the
Q(~x1,··· , ~xN|α). Suppose, for example, that S∗
subextensive entropy becomes

S(a)
1 (N ) =

K + K ∗

2

log2 N + ···

(bits).

(64)

We see the that the subextensive entropy makes no distinction between pre-
dictability that comes from unknown parameters and predictability that comes
from intrinsic correlations in the data; in this sense, two models with the same
K + K ∗ are equivalent. This, actually, must be so. As an example, consider a
chain of Ising spins with long range interactions in one dimension. This system
can order (magnetize) and exhibit long range correlations, and so the predic-
tive information will diverge at the transition to ordering. In one view, there
is no global parameter analogous to α, just the long range interactions. On

5Suppose that we observe a Gaussian stochastic process and we try to learn the power
spectrum.
If the class of possible spectra includes ratios of polynomials in the frequency
(rational spectra) then this condition is met. On the other hand, if the class of possible
spectra includes 1/f noise, then the condition may not be met. For more on long range
correlations, see below.

19

the other hand, there are regimes in which we can approximate the eﬀect of
these interactions by saying that all the spins experience a mean ﬁeld which is
constant across the whole length of the system, and then formally we can think
of the predictive information as being carried by the mean ﬁeld itself. In fact
there are situations in which this is not just an approximation, but an exact
statement. Thus we can trade a description in terms of long range interactions
(K ∗ 6= 0, but K = 0) for one in which there are unknown parameters describ-
ing the system but given these parameters there are no long range correlations
(K 6= 0, K ∗ = 0). The two descriptions are equivalent, and this is captured by
the subextensive entropy.6

3.4 Taming the ﬂuctuations

The preceding calculations of the subextensive entropy S1 are worthless unless
we prove that the ﬂuctuations ψ are controllable.
In this subsection we are
going to discuss when and if this, indeed, happens. We limit the discussion to
analysis of ﬂuctuations in the case of ﬁnding a probability density (Section 3.2);
the case of learning a process (Section 3.3) is very similar.

Clarke and Barron (1990) solved essentially the same problem. They did not
make a separation into the annealed and the ﬂuctuation term, and the quantity
they were interested in was a bit diﬀerent from ours, but, interpreting loosely,
they proved that, modulo some technical assumptions on diﬀerentiability of
functions in question, the ﬂuctuation term always approaches zero. However,
they did not investigate the speed of this approach, and we believe that, by
doing so, they missed some important qualitative distinctions between diﬀer-
ent problems that can arise due to a diﬀerence between d and dVC. In order
to illuminate these distinctions, we here go through the trouble of analyzing
ﬂuctuations all over again.

Returning to Eqs. (40, 42) and the deﬁnition of entropy, we can write the

entropy S(N ) exactly as

S(N ) = −Z dK ¯αP ( ¯α)Z NYj=1

[d~xj Q(~xj| ¯α)]

× log2" NYi=1

Q(~xi| ¯α)Z dKαP (α) e−N DKL( ¯α||α)+N ψ(α, ¯α;{~xi})# .

This expression can be decomposed into the terms identiﬁed above, plus a new
contribution to the subextensive entropy that comes from the ﬂuctuations alone,
S(f)
1 (N ):

S(N ) = S0 · N + S(a)

1 (N ) + S(f)

1 (N ),

6There are a number of interesting questions about how the coeﬃcients in the diverging
predictive information relate to the usual critical exponents, and we hope to return to this
problem in a later paper.

20

S(f)
1

= −Z dK ¯αP ( ¯α)
NYj=1
× log2(cid:20)Z dK αP (α)

Z( ¯α; N )

[d~xj Q(~xj| ¯α)]

e−N DKL( ¯α||α)+N ψ(α, ¯α;{~xi})(cid:21) ,

(65)

where ψ is deﬁned as in Eq. (42), and Z as in Eq. (47).

1

Some loose but useful bounds can be established. First, the predictive in-
formation is a positive (semideﬁnite) quantity, and so the ﬂuctuation term may
not be smaller then the value of −S(a)
as calculated in Eqs. (58, 63). Sec-
ond, since ﬂuctuations make it more diﬃcult to generalize from samples, the
predictive information should always be reduced by ﬂuctuations, so that S(f)
is negative. This last statement corresponds to the fact that for the statistical
mechanics of disordered systems, the annealed free energy always is less than
the average quenched free energy, and may be proven rigorously by applying
Jensen’s inequality to the (concave) logarithm function in Eq. (65); essentially
the same argument was given by Opper and Haussler (1995). A related Jensen’s
inequality argument allows us to show that the total S1(N ) is bounded,

S1(N ) ≤ NZ dKαZ dK ¯αP (α)P ( ¯α)DKL( ¯α||α)

≡ NhDKL( ¯α||α)i ¯α,α,

(66)

so that if we have a class of models (and a prior P (α)) such that the average
Kullback–Leibler divergence among pairs of models is ﬁnite, then the subexten-
sive entropy is necessarily properly deﬁned. Note that hDKL( ¯α||α)i ¯α,α includes
S0 as one of its terms, so that usually S0 and S1 are well– or ill–deﬁned together.
Tighter bounds require nontrivial assumptions about the classes of distri-
butions considered. The ﬂuctuation term would be zero if ψ were zero, and ψ
is the diﬀerence between an expectation value (KL divergence) and the corre-
sponding empirical mean. There is a broad literature that deals with this type
of diﬀerence (see, for example, Vapnik 1998).

We start with the case when the pseudo-dimension (dVC) of the set of prob-
ability densities {Q(~x|α)} is ﬁnite. Then for any reasonable function F (~x; β),
deviations of the empirical mean from the expectation value can be bounded by
probabilistic bounds of the form

β (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P(sup

1

NPj F (~xj; β) −R d~x Q(~x| ¯α) F (~x; β)

L[F ]

< M (ǫ, N )e−cN ǫ2

,

> ǫ)

(67)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where c and L[F ] depend on the details of the particular bound used, while
M (ǫ, N ) depends on dVC. Typically, c is a constant of order one, and L[F ] is
either some moment of F or the range of its variation. In our case, F is the
log–ratio of two densities, so that L[F ] may be assumed bounded for almost
all β without loss of generality in view of Eq. (66).
In addition, M (ǫ, N ) is
ﬁnite at zero, and grows at most subexponentially in both arguments. Bounds

21

of this form may have diﬀerent names in diﬀerent contexts: Glivenko–Cantelli,
Vapnik–Chervonenkis, Hoeﬀding, Chernoﬀ, ...; for review see Vapnik (1998) and
the references therein.

To start the proof of ﬁniteness of S(f)
1

in this case, we ﬁrst show that only the
region α ≈ ¯α is important when calculating the inner integral in Eq. (65). This
statement is equivalent to saying that at large values of α − ¯α the KL diver-
gence almost always dominates the ﬂuctuation term, that is, the contribution
of sequences of {~xi} with atypically large ﬂuctuations is negligible (atypicality
is deﬁned as ψ ≥ δ, where δ is some small constant independent of N ). Since
the ﬂuctuations decrease as 1/√N [see Eq. (67)], and DKL is of order one, this
is plausible. To show this, we bound the logarithm in Eq. (65) by N times the
supremum value of ψ. Then we realize that the averaging over ¯α and {~xi} is
equivalent to integration over all possible values of the ﬂuctuations. The worst
case density of the ﬂuctuations may be estimated by diﬀerentiating Eq. (67)
with respect to ǫ (this brings down an extra factor of N ǫ). Thus the worst case
contribution of these atypical sequences is

∼Z ∞

δ

S(f),atypical
1

dǫ N 2ǫ2M (ǫ)e−cN ǫ2

∼ e−cN δ2

≪ 1 for large N.

(68)

This bound lets us focus our attention on the region α ≈ ¯α. We expand the
exponent of the integrand of Eq. (65) around this point and perform a simple
Gaussian integration. In principle, large ﬂuctuations might lead to an instability
(positive or zero curvature) at the saddle point, but this is atypical and therefore
is accounted for already. Curvatures at the saddle points of both numerator and
denominator are of the same order, and throwing away unimportant additive
and multiplicative constants of order unity, we obtain the following result for
the contribution of typical sequences:

S(f),typical
1

Bµ =

(A)µν =

∼ Z dK ¯αP ( ¯α) dN ~xYj
N Xi
N Xi

∂ log Q(~xi| ¯α)

∂2 log Q(~xi| ¯α)

∂ ¯αµ∂ ¯αν

1

1

∂ ¯αµ

Q(~xj| ¯α) N (BA−1 B) ;

(69)

,

hBi~x = 0 ;

,

hAi~x = F .

Here h···i~x means an averaging with respect to all ~xi’s keeping ¯α constant.
One immediately recognizes that B and A are, respectively, ﬁrst and second
derivatives of the empirical KL divergence that was in the exponent of the inner
integral in Eq. (65).

We are dealing now with typical cases. Therefore, large deviations of A
from F are not allowed, and we may bound Eq. (69) by replacing A−1 with
F −1(1+δ), where δ again is independent of N . Now we have to average a bunch
of products like

∂ log Q(~xi|¯α)

∂ ¯αµ

(F −1)µν

∂ log Q(~xj|¯α)

∂ ¯αν

(70)

22

over all ~xi’s. Only the terms with i = j survive the averaging. There are
K 2N such terms, each contributing of order N −1. This means that the total
contribution of the typical ﬂuctuations is bounded by a number of order one
and does not grow with N .

This concludes the proof of controllability of ﬂuctuations for dVC < ∞.
One may notice that we never used the speciﬁc form of M (ǫ, N ), which is the
only thing dependent on the precise value of the dimension. Actually, a more
thorough look at the proof shows that we do not even need the strict uniform
convergence enforced by the Glivenko–Cantelli bound. With some modiﬁcations
the proof should still hold if there exist some a priori improbable values of α and
¯α that lead to violation of the bound. That is, if the prior P (α) has suﬃciently
narrow support, then we may still expect ﬂuctuations to be unimportant even
for VC–inﬁnite problems.

admissible solutions of a learning problem, such that C = S Cn. The idea is

A proof of this can be found in the realm of the Structural Risk Minimization
(SRM) theory (Vapnik 1998). SRM theory assumes that an inﬁnite structure
C of nested subsets C1 ⊂ C2 ⊂ C3 ⊂ ··· can be imposed onto the set C of all
that, having a ﬁnite number of observations N , one is conﬁned to the choices
within some particular structure element Cn, n = n(N ), when looking for an
approximation to the true solution; this prevents overﬁtting and poor general-
ization. Then, as the number of samples increases and one is able to distinguish
within more and more complicated subsets, n grows. If dVC for learning in any
Cn, n < ∞, is ﬁnite, then one can show convergence of the estimate to the
true value as well as the absolute smallness of ﬂuctuations (Vapnik 1998). It is
remarkable that this result holds even if the capacity of the whole set C is not
described by a ﬁnite dVC.

In the context of SRM, the role of the prior P (α) is to induce a structure
on the set of all admissible densities, and the ﬁght between the number of
samples N and the narrowness of the prior is precisely what determines how
the capacity of the current element of the structure Cn, n = n(N ), grows with
N . A rigorous proof of smallness of the ﬂuctuations can be constructed based
on well known results, as will be detailed elsewhere (Nemenman 2000). Here we
focus on the question of how narrow the prior should be so that every structure
element is of ﬁnite VC–dimension, and one can guarantee eventual convergence
of ﬂuctuations to zero.

Consider two examples. A variable x is distributed according to the following

probability density functions:

(71)

(72)

Q(x|α) =

Q(x|α) =

1
√2π
R 2π

exp(cid:20)−

1
2

(x − α)2(cid:21) , x ∈ (−∞; +∞) ;

exp (− sin αx)

0 dx exp (− sin αx)

, x ∈ [0; 2π) .

Learning the parameter in the ﬁrst case is a dVC = 1 problem, while in the
second case dVC = ∞. In the ﬁrst example, as we have shown above, one may
construct a uniform bound on ﬂuctuations irrespective of the prior P (α). The

23

second one does not allow this. Suppose that the prior is uniform in a box
0 < α < αmax, and zero elsewhere, with αmax rather large. Then for not too
many sample points N , the data would be better ﬁtted not by some value in
the vicinity of the actual parameter, but by some much larger value, for which
almost all data points are at the crests of − sin αx. Adding a new data point
would not help, until that best, but wrong, parameter estimate is less than
αmax.7 So the ﬂuctuations are large, and the predictive information is small in
this case. Eventually, however, data points would overwhelm the box size, and
the best estimate of α would swiftly approach the actual value. At this point
the argument of Clarke and Barron would become applicable, and the leading
behavior of the subextensive entropy would converge to its asymptotic value of
(1/2) log N . On the other hand, there is no uniform bound on the value of N for
which this convergence will occur—it is guaranteed only for N ≫ dVC, which
is never true if dVC = ∞. For some suﬃciently wide priors this asymptotically
correct behavior would be never reached in practice. Further, if we imagine
a thermodynamic limit where the box size and the number of samples both
become large, then by analogy with problems in supervised learning (Seung et
al. 1992, Haussler et al. 1996) we expect that there can be sudden changes in
performance as a function of the number of examples. The arguments of Clarke
and Barron cannot encompass these phase transitions or “aha!” phenomena.

While much of learning theory has properly focused on problems with ﬁnite
VC dimension, it might be that the conventional scenario in which the num-
ber of examples eventually overwhelms the number of parameters or dimensions
is too weak to deal with many real world problems. Certainly in the present
context there is not only a quantitative, but also a qualitative diﬀerence be-
tween reaching the asymptotic regime in just a few measurements, or in many
millions of them. Finitely parameterizable models with ﬁnite or inﬁnite dVC
fall in essentially diﬀerent universality classes with respect to the predictive
information.

3.5 Beyond ﬁnite parameterization:

general considerations

The previous sections have considered learning from time series where the un-
derlying class of possible models is described with a ﬁnite number of parameters.
If the number of parameters is not ﬁnite then in principle it is impossible to
learn anything unless there is some appropriate regularization of the problem.
If we let the number of parameters stay ﬁnite but become large, then there
is more to be learned and correspondingly the predictive information grows in
proportion to this number, as in Eq. (58). On the other hand, if the number of
parameters becomes inﬁnite without regularization, then the predictive infor-
mation should go to zero since nothing can be learned. We should be able to see

7Interestingly, since for the model Eq. (72) KL divergence is bounded from below and above,
for αmax → ∞ the weight in ρ(D; ¯α) at small DKL vanishes, and a ﬁnite weight accumulates at
some nonzero value of D. Thus, even putting the ﬂuctuations aside, the asymptotic behavior
based on the phase space dimension is invalidated, as mentioned above.

24

this happen in a regularized problem if the regularization becomes weaker and
weaker: eventually the regularization would be insuﬃcient and the predictive
information would vanish. The only way this can happen is if the subextensive
term in the entropy grows more and more rapidly with N as we weaken the
regularization, until ﬁnally it becomes extensive at the point where learning be-
comes impossible. More precisely, if this scenario for the breakdown of learning
is to work, there must be situations in which the predictive information grows
with N more rapidly than the logarithmic behavior found in the case of ﬁnite
parameterization.

Subextensive terms in the entropy are controlled by the density of models
as function of their Kullback–Leibler divergence to the target model.
If the
models have ﬁnite VC and phase space dimensions then this density vanishes
for small divergences as ρ ∼ D(d−2)/2. Phenomenologically, if we let the number
of parameters increase, the density vanishes more and more rapidly. We can
imagine that beyond the class of ﬁnitely parameterizable problems there is a
class of regularized inﬁnite dimensional problems in which the density ρ(D → 0)
vanishes more rapidly than any power of D. As an example, we could have

ρ(D → 0) ≈ A exp(cid:20)−

B

Dµ(cid:21) ,

µ > 0,

(73)

that is an essential singularity at D = 0. For simplicity we assume that the
constants A and B can depend on the target model, but that the nature of the
essential singularity (µ) is the same everywhere. Before providing an explicit
example, let us explore the consequences of this behavior.

From Eq. (50) above, we can write the partition function as

Z( ¯α; N ) = Z dDρ(D; ¯α) exp[−N D]

≈ A( ¯α)Z dD exp(cid:20)−
≈ ˜A( ¯α) exp(cid:20)−

µ + 2
µ + 1

1
2

B( ¯α)

Dµ − N D(cid:21)
ln N − C( ¯α)N µ/(µ+1)(cid:21) ,

(74)

where in the last step we use a saddle point or steepest descent approximation
which is accurate at large N , and the coeﬃcients are

˜A( ¯α) = A( ¯α)(cid:18) 2πµ1/(µ+1)
C( ¯α) = [B( ¯α)]1/(µ+1)(cid:18)

µ + 1 (cid:19)1/2

· [B( ¯α)]1/(2µ+2)
µµ/(µ+1) + µ1/(µ+1)(cid:19)

1

Finally we can use Eqs. (57, 74) to compute the subextensive term in the entropy,
keeping only the dominant term at large N ,

S(a)
1 (N ) →

1
ln 2hC( ¯α)i ¯αN µ/(µ+1)

(bits),

where h···i ¯α denotes an average over all the target models.

(75)

25

This behavior of the ﬁrst subextensive term is qualitatively diﬀerent from
everything we have observed so far. A power law divergence is much stronger
than a logarithmic one. Therefore, a lot more predictive information is accumu-
lated in an “inﬁnite parameter” (or nonparametric) system; the system is much
richer and more complex, both intuitively and quantitatively.

Subextensive entropy also grows as a power law in a ﬁnitely parameterizable
system with a growing number of parameters. For example, suppose that we
approximate the distribution of a random variable by a histogram with K bins,
and we let K grow with the quantity of available samples as K ∼ N ν. Equation
(58) suggests that in a K–parameter system, the N th sample point contributes
∼ K/2N bits to the subextensive entropy. If K changes as mentioned, the N th
example then carries ∼ N ν−1 bits. Summing this up for all the samples from
the ﬁrst to the last one, we ﬁnd S(a)
1 ∼ N ν, and if we let ν = µ/(µ + 1) we
obtain Eq. (75). Note that the growth of the number of parameters is slower
than N (ν = µ/(µ + 1) < 1), which makes sense.

Power law growth of the predictive information illustrates the point made
earlier about the transition from learning more to ﬁnally learning nothing as
the class of investigated models becomes more complex. As µ increases the
problem becomes richer and more complex, and this is expressed in the stronger
divergence of the ﬁrst subextensive term of the entropy; for ﬁxed large N , the
predictive information increases with µ. However, if µ → ∞ the problem is too
complex for learning—in our model example the number of bins is growing in
proportion to the number of samples, which means that we are trying to ﬁnd
too much detail in the underlying distribution. As a result, the subextensive
term becomes extensive and stops contributing to predictive information. Thus,
at least to the leading order, predictability is lost, as promised.

3.6 Beyond ﬁnite parameterization: example

The discussion in the previous section suggests that we should look for power–
law behavior of the predictive information in learning problems where rather
than learning ever more precise values for a ﬁxed set of parameters, we learn
a progressively more detailed description—eﬀectively increasing the number of
parameters—as we collect more data. One example of such a problem is learning
the distribution Q(x) for a continuous variable x, but rather than writing a
parametric form of Q(x) we assume only that this function itself is chosen from
some distribution that enforces a degree of smoothness. There are some natural
connections of this problem to the methods of quantum ﬁeld theory (Bialek,
Callan, and Strong 1996) which we can exploit to give a complete calculation
of the predictive information, at least for a class of smoothness constraints.

We write Q(x) = (1/l0) exp[−φ(x)] so that positivity of the distribution is
automatic, and then smoothness may be expressed by saying that the “energy”
(or action) associated with a function φ(x) is related to an integral over its
derivatives, like the strain energy in a stretched string. The simplest possibility

26

along this line of ideas is that the distribution of functions is given by

P [φ(x)] =

exp"−

1
Z

l

2Z dx(cid:18) ∂φ

∂x(cid:19)2# δ(cid:20) 1

l0Z dx e−φ(x) − 1(cid:21) ,

(76)

where Z is the normalization constant for P [φ], the delta function insures that
each distribution Q(x) is normalized, and l sets a scale for smoothness. If distri-
butions are chosen from this distribution, then the optimal Bayesian estimate of
Q(x) from a set of samples x1, x2,··· , xN converges to the correct answer, and
the distribution at ﬁnite N is nonsingular, so that the regularization provided
by this prior is strong enough to prevent the development of singular peaks at
the location of observed data points (Bialek, Callan, and Strong 1996). Fur-
ther developments of the theory, including alternative choices of P [φ(x)], have
been given by Periwal (1997, 1998), Holy (1997) and Aida (1998); for a detailed
numerical investigation of this problem see Nemenman and Bialek (2000). Our
goal here is to be illustrative rather than exhaustive.8

From the discussion above we know that the predictive information is related
to the density of Kullback–Leibler divergences, and that the power–law behavior
we are looking for comes from an essential singularity in this density function.
With Q(x) = (1/l0) exp[−φ(x)], we can write the KL divergence as

We want to compute the density,

where we introduce a factor M which we will allow to become large so that
we can focus our attention on the interesting limit D → 0. To compute this
integral over all functions φ(x), we introduce a Fourier representation for the
delta function, and then rearrange the terms:

2π

ρ(D; ¯φ) = MZ dz
exp(izM D)Z [dφ(x)]P [φ(x)] exp(−izM DKL) (80)
exp(cid:18)izM D +
l0 Z dx ¯φ(x) exp[− ¯φ(x)](cid:19)
= MZ dz
l0 Z dxφ(x) exp[− ¯φ(x)](cid:19) .(81)
×Z [dφ(x)]P [φ(x)] exp(cid:18)−

izM

izM

2π

The inner integral over the functions φ(x) is exactly the integral which was
evaluated in the original discussion of this problem (Bialek, Callan and Strong

8We caution the reader that our discussion in this section is less self–contained than in
other sections. Since the crucial steps exactly parallel those in the earlier work, here we just
give references.

27

1

DKL[ ¯φ(x)||φ(x)] =

l0Z dx exp[− ¯φ(x)][φ(x) − ¯φ(x)] .
ρ(D; ¯φ) = Z [dφ(x)]P [φ(x)]δ(cid:0)D − DKL[ ¯φ(x)||φ(x)](cid:1)

= MZ [dφ(x)]P [φ(x)]δ(cid:0)M D − M DKL[ ¯φ(x)||φ(x)](cid:1) ,

(77)

(78)

(79)

1996); in the limit that zM is large we can use a saddle point approximation, and
standard ﬁeld theoretic methods allow us to compute the ﬂuctuations around
the saddle point. The result is that

Z [dφ(x)]P [φ(x)] exp(cid:18)−

izM

l0 Z dxφ(x) exp[− ¯φ(x)](cid:19)

izM

= exp(cid:18)−

l0 Z dx ¯φ(x) exp[− ¯φ(x)] − Seﬀ[ ¯φ(x); zM ](cid:19) ,
∂x(cid:19)2
ll0 (cid:19)1/2Z dx exp[− ¯φ(x)/2].
2Z dx(cid:18) ∂ ¯φ

2(cid:18) izM

+

1

l

Seﬀ [ ¯φ; zM ] =

(82)

(83)

(84)

Now we can do the integral over z, again by a saddle point method. The
two saddle point approximations are both valid in the limit that D → 0 and
M D3/2 → ∞; we are interested precisely in the ﬁrst limit, and we are free to
set M as we wish, so this gives us a good approximation for ρ(D → 0; ¯φ). The
result is that

ρ(D → 0; ¯φ) = A[ ¯φ(x)]D−3/2 exp(cid:18)−

B[ ¯φ(x)]

D (cid:19) ,

A[ ¯φ(x)] =

B[ ¯φ(x)] =

1

√16πll0
1

exp(cid:20)−

l

2Z dx(cid:0)∂x ¯φ(cid:1)2(cid:21)Z dx exp[− ¯φ(x)/2](85)

.

(86)

16ll0(cid:18)Z dx exp[− ¯φ(x)/2](cid:19)2

Except for the factor of D−3/2, this is exactly the sort of essential singularity
that we considered in the previous section, with µ = 1. The D−3/2 prefactor
does not change the leading large N behavior of the predictive information, and
we ﬁnd that

S(a)
1 (N ) ∼

1

2 ln 2√ll0*Z dx exp[− ¯φ(x)/2]+ ¯φ

N 1/2,

(87)

where h···i ¯φ denotes an average over the target distributions ¯φ(x) weighted once
again by P [ ¯φ(x)]. Notice that if x is unbounded then the average in Eq. (87)
is infrared divergent; if instead we let the variable x range from 0 to L then
this average should be dominated by the uniform distribution. Replacing the
average by its value at this point, we obtain the approximate result

S(a)
1 (N ) ∼

1

2 ln 2

√N (cid:18) L

l(cid:19)1/2

bits.

(88)

To understand the result in Eq.

(88), we recall that this ﬁeld theoretic
approach is more or less equivalent to an adaptive binning procedure in which

28

we divide the range of x into bins of local sizepl/N Q(x) (Bialek, Callan, and

Strong 1996). From this point of view, Eq.
(88) makes perfect sense: the
predictive information is directly proportional to the number of bins that can
be put in the range of x. This also is in direct accord with a comment from the
previous subsection that power law behavior of predictive information arises
from the number of parameters in the problem depending on the number of
samples.

This counting of parameters allows a schematic argument about the small-
ness of ﬂuctuations in this particular nonparametric problem. If we take the

hint that at every step ∼ √N bins are being investigated, then we can imagine
that the ﬁeld theoretic prior in Eq. (76) has imposed a structure C on the set
of all possible densities, so that the set Cn is formed of all continuous piecewise
linear functions that have not more than n kinks. Learning such functions for
ﬁnite n is a dVC = n problem. Now, as N grows, the elements with higher

capacities n ∼ √N are admitted. The ﬂuctuations in such a problem are known

to be controllable (Vapnik 1998), as will be discussed in more detail elsewhere
(Nemenman 2000).

One thing which remains troubling is that the predictive information de-
pends on the arbitrary parameter l describing the scale of smoothness in the
distribution.
In the original work it was proposed that one should integrate
over possible values of l (Bialek, Callan and Strong 1996). Numerical simu-
lations demonstrate that this parameter can be learned from the data itself
(Nemenman and Bialek 2000), but perhaps even more interesting is a formula-
tion of the problem by Periwal (1997, 1998) which recovers complete coordinate
invariance by allowing (eﬀectively) l to vary with x.
In this case the whole
theory has no length scale, and there also is no need to conﬁne the variable x to
a box (here of size L). We expect that this coordinate invariant approach will
lead to a universal coeﬃcient multiplying √N in the analog of Eq. (88), but
this remains to be shown.

In summary, the ﬁeld theoretic approach to learning a smooth distribution
in one dimension provides us with a concrete, calculable example of a learn-
ing problem with power–law growth of the predictive information. The sce-
nario is exactly as suggested in the previous section, where the density of KL
divergences develops an essential singularity. Heuristic considerations (Bialek,
Callan, and Strong 1996; Aida 1999) suggest that diﬀerent smoothness penalties
and generalizations to higher dimensional problems will have sensible eﬀects on
the predictive information—all have power–law growth, but smoother functions
have smaller powers (less to learn) and higher dimensional problems have larger
powers (more to learn)—but real calculations for these cases remain challenging.

4 Ipred as a measure of complexity

The problem of quantifying complexity is very old. Solomonoﬀ (1964), Kol-
mogorov (1965), and Chaitin (1975) investigated a mathematically rigorous no-
tion of complexity that measures (roughly) the minimum length of a computer

29

program that simulates the observed time series [see also Li and Vit´anyi (1993)].
Unfortunately there is no algorithm that can calculate the Kolmogorov complex-
ity of a particular data set. In addition, algorithmic or Kolmogorov complexity
is closely related to the Shannon entropy, which means that it measures some-
thing closer to our intuitive concept of randomness than to the intuitive concept
of complexity [as discussed, for example, by Bennett (1990)]. These problems
have fueled continued research along two diﬀerent paths, representing two major
motivations for deﬁning complexity. First, one would like to make precise an
impression that some systems—such as life on earth or a turbulent ﬂuid ﬂow—
evolve toward a state of higher complexity, and one would like to be able to
classify those states. Second, in choosing among diﬀerent models that describe
an experiment, one wants to quantify a preference for simpler explanations or,
equivalently, provide a penalty for complex models that can be weighed against
the more conventional goodness of ﬁt criteria. We bring our readers up to date
with some developments in both of these directions, and then discuss the role
of predictive information as a measure of complexity. This also gives us an
opportunity to discuss more carefully the relation of our results to previous
work.

4.1 Complexity of statistical models

The construction of complexity penalties for model selection is a statistics prob-
lem. As far as we know, however, the ﬁrst discussions of complexity in this
context belong to philosophical literature. Even leaving aside the early contri-
butions of William of Ockham on the need for simplicity, Hume on the problem
of induction, and Popper on falsiﬁability, Kemeney (1953) suggested explicitly
that it would be possible to create a model selection criterion that balances
goodness of ﬁt versus complexity. Wallace and Burton (1968) hinted that this
balance may result in the model with “the briefest recording of all attribute
information.” Even though he probably had a somewhat diﬀerent motivation,
Akaike (1974a, 1974b) made the ﬁrst quantitative step along these lines. His
ad hoc complexity term was independent of the number of data points and was
proportional to the number of free independent parameters in the model.

These ideas were rediscovered and developed systematically by Rissanen in a
series of papers starting from 1978. He has emphasized strongly (Rissanen 1984,
1986, 1987) that ﬁtting a model to data represents an encoding of those data,
or predicting future data, and that in searching for an eﬃcient code we need to
measure not only the number of bits required to describe the deviations of the
data from the model’s predictions (goodness of ﬁt), but also the number of bits
required to specify the parameters of the model (complexity). This speciﬁca-
tion has to be done to a precision supported by the data.9 Rissanen (1984) and
Clarke and Barron (1990) in full generality were able to prove that the optimal
encoding of a model requires a code with length asymptotically proportional to

9Within this framework Akaike’s suggestion can be seen as coding the model to (subopti-

mal) ﬁxed precision.

30

the number of independent parameters and logarithmically dependent on the
number of data points we have observed. The minimal amount of space one
needs to encode a data string (minimum description length or MDL) within
a certain assumed model class was termed by Rissanen stochastic complexity,
and in recent work he refers to the piece of the stochastic complexity required
for coding the parameters as the model complexity (Rissanen 1996). This ap-
proach was further strengthened by a recent result (Vit´anyi and Li 2000) that
an estimation of parameters using the MDL principle is equivalent to Bayesian
parameter estimations with a “universal” prior (Li and Vit´anyi 1993).

There should be a close connection between Rissanen’s ideas of encoding
the data stream and the subextensive entropy. We are accustomed to the idea
that the average length of a code word for symbols drawn from a distribution
P is given by the entropy of that distribution; thus it is tempting to say that
an encoding of a stream x1, x2,··· , xN will require an amount of space equal
to the entropy of the joint distribution P (x1, x2,··· , xN ). The situation here is
a bit more subtle, because the usual proofs of equivalence between code length
and entropy rely on notions of typicality and asymptotics as we try to encode
sequences of many symbols; here we already have N symbols and so it doesn’t
really make sense to talk about a stream of streams. One can argue, however,
that atypical sequences are not truly random within a considered distribution
since their coding by the methods optimized for the distribution is not opti-
mal. So atypical sequences are better considered as typical ones coming from
a diﬀerent distribution [a point also made by Grassberger (1986)]. This allows
us to identify properties of an observed (long) string with the properties of the
distribution it comes from, as was done by Vit´anyi and Li (2000). If we ac-
cept this identiﬁcation of entropy with code length, then Rissanen’s stochastic
complexity should be the entropy of the distribution P (x1, x2,··· , xN ).
As emphasized by Balasubramanian (1996), the entropy of the joint distri-
bution of N points can be decomposed into pieces that represent noise or errors
in the model’s local predictions—an extensive entropy—and the space required
to encode the model itself, which must be the subextensive entropy. Since in the
usual formulation all long–term predictions are associated with the continued
validity of the model parameters, the dominant component of the subextensive
entropy must be this parameter coding, or model complexity in Rissanen’s ter-
minology. Thus the subextensive entropy should be the model complexity, and
in simple cases where we can describe the data by a K–parameter model both
quantities are equal to (K/2) log2 N bits to the leading order.

The fact that the subextensive entropy or predictive information agrees with
Rissanen’s model complexity suggests that Ipred provides a reasonable measure
of complexity in learning problems. On the other hand, this agreement might
lead the reader to wonder if all we have done is to rewrite the results of Rissa-
nen et al. in a diﬀerent notation. To calm these fears we recall again that our
approach distinguishes inﬁnite VC problems from ﬁnite ones and treats non-
parametric cases as well. Indeed, the predictive information is deﬁned without
reference to the idea that we are learning a model, and thus we can make a link
to physical aspects of the problem.

31

4.2 Complexity of dynamical systems

There is a strong prejudice that the complexity of physical systems should be
measured by quantities that are at least related to more conventional thermody-
namic quantities (temperature, entropy, . . .), since this is the only way one will
be able to calculate complexity within the framework of statistical mechanics.
Most proposals deﬁne complexity as an entropy–like quantity, but an entropy
of some unusual ensemble. For example, Lloyd and Pagels (1988) identiﬁed
complexity as thermodynamic depth, the entropy of the state sequences that
lead to the current state. The idea is clearly in the same spirit as the mea-
surement of predictive information, but this depth measure does not completely
discard the extensive component of the entropy (Crutchﬁeld and Shalizi 1999)
and thus fails to resolve the essential diﬃculty in constructing complexity mea-
sures for physical systems: distinguishing genuine complexity from randomness
(entropy), the complexity should be zero both for purely regular and for purely
random systems.

New deﬁnitions of complexity that try to satisfy these criteria (Lopez–Ruiz
et al. 1995, Gell–Mann and Lloyd 1996, Shiner et al. 1999, Sole and Luque 1999,
Adami and Cerf 2000) and criticisms of these proposals (Crutchﬁeld et al. 1999,
Feldman and Crutchﬁeld 1998, Sole and Luque 1999) continue to emerge even
now. Aside from the obvious problems of not actually eliminating the extensive
component for all or a part of the parameter space or not expressing complexity
as an average over a physical ensemble, the critiques often are based on a clever
argument ﬁrst mentioned explicitly by Feldman and Crutchﬁeld (1998). In an
attempt to create a universal measure, the constructions can be made over-
universal: many proposed complexity measures depend only on the entropy
density S0 and thus are functions only of disorder—not a desired feature. In
addition, many of these and other deﬁnitions are ﬂawed because they fail to
distinguish among the richness of classes beyond some very simple ones.

In a series of papers, Crutchﬁeld and coworkers identiﬁed statistical com-
plexity with the entropy of causal states, which in turn are deﬁned as all those
microstates (or histories) that have the same conditional distribution of futures
(Crutchﬁeld and Young 1989, Shalizi and Crutchﬁeld 1999). The causal states
provide an optimal description of a system’s dynamics in the sense that these
states make as good a prediction as the histories themselves. Statistical com-
plexity is very similar to predictive information, but Shalizi and Crutchﬁeld
(1999) deﬁne a quantity which is even closer to the spirit of our discussion:
their excess entropy is exactly the mutual information between the semi–inﬁnite
past and future. Unfortunately, by focusing on cases in which the past and
future are inﬁnite but the excess entropy is ﬁnite, their discussion is limited to
systems for which (in our language) Ipred(T → ∞) = constant.
In our view, Grassberger (1986) has made the clearest and the most appeal-
ing deﬁnitions. He emphasized that the slow approach of the entropy to its
extensive limit is a sign of complexity, and has proposed several functions to
analyze this slow approach. His eﬀective measure complexity is the subexten-
sive entropy term of an inﬁnite data sample. Unlike Crutchﬁeld et al., he allows

32

this measure to grow to inﬁnity. As an example, for low dimensional dynamical
systems, the eﬀective measure complexity is ﬁnite whether the system exhibits
periodic or chaotic behavior, but at the bifurcation point that marks the onset
of chaos, it diverges logarithmically. More interestingly, Grassberger also notes
that simulations of speciﬁc cellular automaton models that are capable of uni-
versal computation indicate that these systems can exhibit an even stronger,
power–law, divergence.

Grassberger (1986) also introduces the true measure complexity, which is
the minimal information one needs to extract from the past in order to pro-
vide optimal prediction. This quantity is exactly the statistical complexity of
Crutchﬁeld et al., and the two approaches are actually much closer than they
seem. The relation between the true and the eﬀective measure complexities,
or between the statistical complexity and the excess entropy, closely parallels
the idea of extracting or compressing relevant information (Tishby et al. 1999,
Bialek and Tishby, in preparation), as discussed below.

4.3 A unique measure of complexity?

We recall that entropy provides a measure of information that is unique in satis-
fying certain plausible constraints (Shannon 1948). It would be attractive if we
could prove a similar uniqueness theorem for the predictive information, or any
part of it, as a measure of the complexity or richness of a time dependent signal
x(0 < t < T ) drawn from a distribution P [x(t)]. Before proceeding with such
an argument we have to ask, however, whether we want to attach measures of
complexity to a particular signal x(t), or whether we are interested in measures
(like the entropy itself) which constitute an average over the ensemble P [x(t)].
In most cases, including the learning problems discussed above, it is clear
that we want to measure complexity of the dynamics underlying the signal, or
equivalently the complexity of a model that might be used to describe the signal.
This is very diﬀerent from trying to deﬁne the complexity of a single realization,
because there can be atypical data streams. Either we must treat atypicality
explicitly, arguing that atypical data streams from one source should be viewed
as typical streams from another source, as discussed by Vit´anyi and Li (2000),
or we have to look at average quantities. Grassberger (1986) in particular has
argued that our visual intuition about the complexity of spatial patterns is
an ensemble concept, even if the ensemble is only implicit [see also Tong in
the discussion session of Rissanen (1987)]. So we shall search for measures of
complexity that are averages over the distribution P [x(t)].

Once we focus on average quantities, we can start by adopting Shannon’s
if there are N equally
postulates as constraints on a measure of complexity:
likely signals, then the measure should be monotonic in N ;
if the signal is
decomposable into statistically independent parts then the measure should be
additive with respect to this decomposition; and if the signal can be described
as a leaf on a tree of statistically independent decisions then the measure should
be a weighted sum of the measures at each branching point. We believe that
these constraints are as plausible for complexity measures as for information

33

measures, and it is well known from Shannon’s original work that this set of
constraints leaves the entropy as the only possibility. Since we are discussing
a time dependent signal, this entropy depends on the duration of our sample,
S(T ). We know of course that this cannot be the end of the discussion, because
we need to distinguish between randomness (entropy) and complexity. The path
to this distinction is to introduce other constraints on our measure.

First we notice that if the signal x is continuous, then the entropy is not in-
variant under transformations of x. It seems reasonable to ask that complexity
be a function of the process we are observing and not of the coordinate system
in which we choose to record our observations. The examples above show us,
however, that it is not the whole function S(T ) which depends on the coordinate
system for x;10 it is only the extensive component of the entropy that has this
noninvariance. This can be seen more generally by noting that subextensive
terms in the entropy contribute to the mutual information among diﬀerent seg-
ments of the data stream (including the predictive information deﬁned here),
while the extensive entropy cannot; mutual information is coordinate invariant,
so all of the noninvariance must reside in the extensive term. Thus, any measure
complexity that is coordinate invariant must discard the extensive component
of the entropy.

The fact that extensive entropy cannot contribute to complexity is discussed
widely in the physics literature (Bennett 1990), as our short review above shows.
To statisticians and computer scientists, who are used to Kolmogorov’s ideas,
this is less obvious. However, Rissanen (1986, 1987) also talks about “noise” and
“useful information” in a data sequence, which is similar to splitting entropy into
its extensive and the subextensive parts. His “model complexity,” aside from
not being an average as required above, is essentially equal to the subextensive
entropy. Similarly, Whittle [in the discussion of Rissanen (1987)] talks about
separating the predictive part of the data from the rest.

If we continue along these lines, we can think about the asymptotic expansion
of the entropy at large T . The extensive term is the ﬁrst term in this series,
and we have seen that it must be discarded. What about the other terms? In
the context of learning a parameterized model, most of the terms in this series
depend in detail on our prior distribution in parameter space, which might seem
odd for a measure of complexity. More generally, if we consider transformations
of the data stream x(t) that mix points within a temporal window of size τ , then
for T >> τ the entropy S(T ) may have subextensive terms which are constant,
and these are not invariant under this class of transformations. On the other
hand, if there are divergent subextensive terms, these are invariant under such
temporally local transformations.11 So if we insist that measures of complexity
be invariant not only under instantaneous coordinate transformations, but also
under temporally local transformations, then we can discard both the extensive

10Here we consider instantaneous transformations of x, not ﬁltering or other transformations

that mix points at diﬀerent times.

11Throughout this discussion we assume that the signal x at one point in time is ﬁnite
dimensional. There are subtleties if we allow x to represent the conﬁguration of a spatially
inﬁnite system.

34

and the ﬁnite subextensive terms in the entropy, leaving only the divergent
subextensive terms as a possible measure of complexity.

An interesting example of these arguments is provided by the statistical
mechanics of polymers. It is conventional to make models of polymers as random
walks on a lattice, with various interactions or self avoidance constraints among
diﬀerent elements of the polymer chain. If we count the number N of walks with
N steps, we ﬁnd that N (N ) ∼ AN γzN (de Gennes 1979). Now the entropy is
the logarithm of the number of states, and so there is an extensive entropy S0 =
log2 z, a constant subextensive entropy log2 A, and a divergent subextensive
term S1(N ) → γ log2 N . Of these three terms, only the divergent subextensive
term (related to the critical exponent γ) is universal, that is independent of the
detailed structure of the lattice. Thus, as in our general argument, it is only
the divergent subextensive terms in the entropy that are invariant to changes
in our description of the local, small scale dynamics.

We can recast the invariance arguments in a slightly diﬀerent form using
the relative entropy. We recall that entropy is deﬁned cleanly only for discrete
processes, and that in the continuum there are ambiguities. We would like to
write the continuum generalization of the entropy of a process x(t) distributed
according to P [x(t)] as

Scont = −Z Dx(t) P [x(t)] log2 P [x(t)],

(89)

but this is not well deﬁned because we are taking the logarithm of a dimensionful
quantity. Shannon gave the solution to this problem: we use as a measure
of information the relative entropy or KL divergence between the distribution
P [x(t)] and some reference distribution Q[x(t)],

Q[x(t)](cid:19) ,
Srel = −Z Dx(t) P [x(t)] log2(cid:18) P [x(t)]

(90)

which is invariant under changes of our coordinate system on the space of signals.
The cost of this invariance is that we have introduced an arbitrary distribution
Q[x(t)], and so really we have a family of measures. We can ﬁnd a unique com-
plexity measure within this family by imposing invariance principles as above,
but in this language we must make our measure invariant to diﬀerent choices of
the reference distribution Q[x(t)].

The reference distribution Q[x(t)] embodies our expectations for the signal
x(t); in particular, Srel measures the extra space needed to encode signals drawn
from the distribution P [x(t)] if we use coding strategies that are optimized for
Q[x(t)]. If x(t) is a written text, two readers who expect diﬀerent numbers of
spelling errors will have diﬀerent Qs, but to the extent that spelling errors can
be corrected by reference to the immediate neighboring letters we insist that
any measure of complexity be invariant to these diﬀerences in Q. On the other
hand, readers who diﬀer in their expectations about the global subject of the
text may well disagree about the richness of a newspaper article. This suggests
that complexity is a component of the relative entropy that is invariant under
some class of local translations and misspellings.

35

Suppose that we leave aside global expectations, and construct our reference
distribution Q[x(t)] by allowing only for short ranged interactions—certain let-
ters tend to follow one another, letters form words, and so on, but we bound the
range over which these rules are applied. Models of this class cannot embody
the full structure of most interesting time series (including language), but in
the present context we are not asking for this. On the contrary, we are looking
for a measure that is invariant to diﬀerences in this short ranged structure. In
the terminology of ﬁeld theory or statistical mechanics, we are constructing our
reference distribution Q[x(t)] from local operators. Because we are considering
a one dimensional signal (the one dimension being time), distributions con-
structed from local operators cannot have any phase transitions as a function of
parameters; again it is important that the signal x at one point in time is ﬁnite
dimensional. The absence of critical points means that the entropy of these dis-
tributions (or their contribution to the relative entropy) consists of an extensive
term (proportional to the time window T ) plus a constant subextensive term,
plus terms that vanish as T becomes large. Thus, if we choose diﬀerent reference
distributions within the class constructible from local operators, we can change
the extensive component of the relative entropy, and we can change constant
subextensive terms, but the divergent subextensive terms are invariant.

To summarize, the usual constraints on information measures in the contin-
uum produce a family of allowable complexity measures, the relative entropy to
an arbitrary reference distribution. If we insist that all observers who choose
reference distributions constructed from local operators arrive at the same mea-
sure of complexity, or if we follow the ﬁrst line of arguments presented above,
then this measure must be the divergent subextensive component of the entropy
or, equivalently, the predictive information. We have seen that this component
is connected to learning, quantifying the amount that can be learned about dy-
namics that generate the signal, and to measures of complexity that have arisen
in statistics and in dynamical systems theory.

5 Discussion

We have presented predictive information as a characterization of data streams.
In the context of learning, predictive information is related directly to general-
ization. More generally, the structure or order in a time series or a sequence
is related almost by deﬁnition to the fact that there is predictability along the
sequence. The predictive information measures the amount of such structure,
but doesn’t exhibit the structure in a concrete form. Having collected a data
stream of duration T , what are the features of these data that carry the predic-
tive information Ipred(T )? From Eq. (7) we know that most of what we have
seen over the time T must be irrelevant to the problem of prediction, so that
the predictive information is a small fraction of the total information; can we
separate these predictive bits from the vast amount of nonpredictive data?

The problem of separating predictive from nonpredictive information is a
special case of the problem discussed recently (Tishby et al. 1999, Bialek and

36

Tishby, in preparation): given some data x, how do we compress our description
of x while preserving as much information as possible about some other variable
y? Here we identify x = xpast as the past data and y = xfuture as the future.
When we compress xpast into some reduced description ˆxpast we keep a certain
amount of information about the past, I(ˆxpast; xpast), and we also preserve a
certain amount of information about the future, I(ˆxpast; xfuture). There is no
single correct compression xpast → ˆxpast; instead there is a one parameter family
of strategies which trace out an optimal curve in the plane deﬁned by these two
mutual informations, I(ˆxpast; xfuture) vs. I(ˆxpast; xpast).

The predictive information preserved by compression must be less than the
total, so that I(ˆxpast; xfuture) ≤ Ipred(T ). Generically no compression can pre-
serve all of the predictive information so that the inequality will be strict, but
there are interesting special cases where equality can be achieved. If prediction
proceeds by learning a model with a ﬁnite number of parameters, we might have
a regression formula that speciﬁes the best estimate of the parameters given the
past data; using the regression formula compresses the data but preserves all of
the predictive power. In cases like this (more generally, if there exist suﬃcient
statistics for the prediction problem) we can ask for the minimal set of ˆxpast such
that I(ˆxpast; xfuture) = Ipred(T ). The entropy of this minimal ˆxpast is the true
measure complexity deﬁned by Grassberger (1986) or the statistical complexity
deﬁned by Crutchﬁeld and Young (1989).

In the context of statistical mechanics, long range correlations are char-
acterized by computing the correlation functions of order parameters, which
are coarse–grained functions of the system’s microscopic variables. If we know
something about the nature of the order parameter (e.g., whether it is a vec-
tor or a scalar), then general principles allow a fairly complete classiﬁcation
and description of long range ordering and the nature of the critical points at
which this order can appear or change. On the other hand, deﬁning the order
parameter itself remains something of an art. For a ferromagnet, the order pa-
rameter is obtained by local averaging of the microscopic spins, while for an
antiferromagnet one must average the staggered magnetization to capture the
fact that the ordering involves an alternation from site to site, and so on. Since
the order parameter carries all the information that contributes to long range
correlations in space and time, it might be possible to deﬁne order parameters
more generally as those variables that provide the most eﬃcient compression of
the predictive information, and this should be especially interesting for complex
or disordered systems where the nature of the order is not obvious intuitively; a
ﬁrst try in this direction was made by Bruder (1998). At critical points the pre-
dictive information will diverge, and the coeﬃcients of these divergences should
be related to the standard scaling dimensions of the order parameters, but the
details of this connection need to be worked out.

If we compress or extract the predictive information from a time series we
are in eﬀect discovering “features” that capture the nature of the ordering in
time. Learning itself can be seen as an example of this, where we discover the
parameters of an underlying model by trying to compress the information that
one sample of N points provides about the next, and in this way we address

37

directly the problem of generalization (Bialek and Tishby, in preparation). The
fact that (as mentioned above) nonpredictive information is useless to the or-
ganism suggests that one crucial goal of neural information processing is to
separate predictive information from the background. Perhaps rather than pro-
viding an eﬃcient representation of the current state of the world—as suggested
by Attneave (1954), Barlow (1959, 1961), and others (Atick 1992)—the nervous
system provides an eﬃcient representation of the predictive information.12 It
should be possible to test this directly by studying the encoding of reasonably
natural signals and asking if the information which neural responses provide
about the future of the input is close to the limit set by the statistics of the
input itself, given that the neuron only captures a certain number of bits about
the past. Thus we might ask if, under natural stimulus conditions, a motion
sensitive visual neuron captures features of the motion trajectory that allow
for optimal prediction or extrapolation of that trajectory; by using informa-
tion theoretic measures we both test the “eﬃcient representation” hypothesis
directly and avoid arbitrary assumptions about the metric for errors in predic-
tion. For more complex signals such as communication sounds, even identifying
the features that capture the predictive information is an interesting problem.
It is natural to ask if these ideas about predictive information could be used
to analyze experiments on learning in animals or humans. We have emphasized
the problem of learning probability distributions or probabilistic models rather
than learning deterministic functions, associations or rules. It is known that
the nervous system adapts to the statistics of its inputs, and this adaptation is
evident in the responses of single neurons (Smirnakis et al. 1996, Brenner et
al. 2000); these experiments provide a simple example of the system learning
a parameterized distribution. When making saccadic eye movements, human
subjects alter their distribution of reaction times in relation to the relative prob-
abilities of diﬀerent targets, as if they had learned an estimate of the relevant
likelihood ratios (Carpenter and Williams 1995). Humans also can learn to
discriminate almost optimally between random sequences (fair coin tosses) and
sequences that are correlated or anticorrelated according to a Markov process;
this learning can be accomplished from examples alone, with no other feedback

12If, as seems likely, the stream of data reaching our senses has diverging predictive infor-
mation then the space required to write down our description grows and grows as we observe
the world for longer periods of time. In particular, if we can observe for a very long time
then the amount that we know about the future will exceed, by an arbitrarily large factor,
the amount that we know about the present. Thus representing the predictive information
may require many more neurons than would be required to represent the current data. If we
imagine that the goal of primary sensory cortex is to represent the current state of the sensory
world, then it is diﬃcult to understand why these cortices have so many more neurons than
they have sensory inputs. In the extreme case, the region of primary visual cortex devoted
to inputs from the fovea has nearly 30,000 neurons for each photoreceptor cell in the retina
(Hawken and Parker 1991); although much remains to be learned about these cells, it is dif-
ﬁcult to imagine that the activity of so many neurons constitutes an eﬃcient representation
of the current sensory inputs. But if we live in a world where the predictive information in
the movies reaching our retina diverges, it is perfectly possible that an eﬃcient representation
of the predictive information available to us at one instant requires thousands of times more
space than an eﬃcient representation of the image currently falling on our retina.

38

(Lopes and Oden 1987). Acquisition of language may require learning the joint
distribution of successive phonemes, syllables, or words, and there is direct ev-
idence for learning of conditional probabilities from artiﬁcial sound sequences,
both by infants and by adults (Saﬀran et al. 1996; 1999). These examples, which
are not exhaustive, indicate that the nervous system can learn an appropriate
probabilistic model,13 and this oﬀers the opportunity to analyze the dynamics
of this learning using information theoretic methods: What is the entropy of N
successive reaction times following a switch to a new set of relative probabilities
in the saccade experiment? How much information does a single reaction time
provide about the relevant probabilities? Following the arguments above, such
analysis could lead to a measurement of the universal learning curve Λ(N ).

The learning curve Λ(N ) exhibited by a human observer is limited by the
predictive information in the time series of stimulus trials itself. Comparing
Λ(N ) to this limit deﬁnes an eﬃciency of learning in the spirit of the discussion
by Barlow (1983); while it is known that the nervous system can make eﬃcient
use of available information in signal processing tasks [cf. Chapter 4 of Rieke et
al. (1997)], it is not known whether the brain is an eﬃcient learning machine in
the analogous sense. Given our classiﬁcation of learning tasks by their complex-
ity, it would be natural to ask if the eﬃciency of learning were a critical function
of task complexity: perhaps we can even identify a limit beyond which eﬃcient
learning fails, indicating a limit to the complexity of the internal model used
by the brain during a class of learning tasks. We believe that our theoretical
discussion here at least frames a clear question about the complexity of internal
models, even if for the present we can only speculate about the outcome of such
experiments.

An important result of our analysis is the characterization of time series or
learning problems beyond the class of ﬁnitely parameterizable models, that is the
class with power–law divergent predictive information. Qualitatively this class
is more complex than any parametric model, no matter how many parameters
there may be, because of the more rapid asymptotic growth of Ipred(N ). On
the other hand, with a ﬁnite number of observations N , the actual amount of
predictive information in such a nonparametric problem may be smaller than
in a model with a large but ﬁnite number of parameters. Speciﬁcally, if we
have two models, one with Ipred(N ) ∼ AN ν and one with K parameters so
that Ipred(N ) ∼ (K/2) log2 N , the inﬁnite parameter model has less predictive
information for all N smaller than some critical value

Nc ∼(cid:20) K

2Aν

log2(cid:18) K

2A(cid:19)(cid:21)1/ν

.

(91)

In the regime N << Nc, it is possible to achieve more eﬃcient prediction by try-
ing to learn the (asymptotically) more complex model, as illustrated concretely
in simulations of the density estimation problem (Nemenman and Bialek 2000).
Even if there are a ﬁnite number of parameters—such as the ﬁnite number of

13As emphasized above, many other learning problems, including learning a function from
noisy examples, can be seen as the learning of a probabilistic model. Thus we expect that
this description applies to a much wider range of biological learning tasks.

39

synapses in a small volume of the brain—this number may be so large that we
always have N << Nc, so that it may be more eﬀective to think of the many
parameter model as approximating a continuous or nonparametric one.

It is tempting to suggest that the regime N << Nc is the relevant one for
much of biology. If we consider, for example, 10 mm2 of inferotemporal cortex
devoted to object recognition (Logothetis and Sheinberg 1996), the number of
synapses is K ∼ 5 × 109. On the other hand, object recognition depends on
foveation, and we move our eyes roughly three times per second throughout
perhaps 15 years of waking life during which we master the art of object recog-
nition. This limits us to at most N ∼ 109 examples. Remembering that we must
have ν < 1, even with large values of A Eq. (91) suggests that we operate with
N < Nc. One can make similar arguments about very diﬀerent brains, such as
the mushroom bodies in insects (Capaldi, Robinson and Fahrbach 1999). If this
identiﬁcation of biological learning with the regime N << Nc is correct, then
the success of learning in animals must depend on strategies that implement
sensible priors over the space of possible models.

There is one clear empirical hint that humans can make eﬀective use of
models that are beyond ﬁnite parameterization (in the sense that predictive
information diverges as a power–law), and this comes from studies of language.
Long ago, Shannon (1951) used the knowledge of native speakers to place bounds
on the entropy of written English, and his strategy made explicit use of pre-
dictability. Shannon showed N –letter sequences to native speakers (readers),
asked them to guess the next letter, and recorded how many guesses were re-
quired before they got the right answer. Thus each letter in the text is turned
into a number, and the entropy of the distribution of these numbers is an upper
bound on the conditional entropy ℓ(N ) [cf. Eq. (8)]. Shannon himself thought
that the convergence as N becomes large was rather quick, and quoted an es-
timate of the extensive entropy per letter S0. Many years later, Hilberg (1990)
reanalyzed Shannon’s data and found that the approach to extensivity in fact
was very slow: certainly there is evidence for a large component S1(N ) ∝ N 1/2,
and this may even dominate the extensive component for accessible N . Ebel-
ing and P¨oschel (1994; see also P¨oschel, Ebeling, and Ros´e 1995) studied the
statistics of letter sequences in long texts (like Moby Dick) and found the same
strong subextensive component.
It would be attractive to repeat Shannon’s
experiments with a slightly diﬀerent design that emphasizes the detection of
subextensive terms at large N .14

In summary, we believe that our analysis of predictive information solves
the problem of measuring the complexity of time series. This analysis uniﬁes
ideas from learning theory, coding theory, dynamical systems, and statistical

14Associated with the slow approach to extensivity is a large mutual information between
words or characters separated by long distances, and several groups have found that this
mutual information declines as a power law. Cover and King (1978) criticize such observations
by noting that such behavior is impossible in Markov chains of arbitrary order. While it is
possible that existing mutual information data have not reached asymptotia, the criticism of
Cover and King misses the possibility that language is not a Markov process. Of course it
cannot be Markovian if it has a power–law divergence in the predictive information.

40

mechanics.
In particular we have focused attention on a class of processes
that are qualitatively more complex than those treated in conventional learning
theory, and there are several reasons to think that this class includes many
examples of relevance to biology and cognition.

Acknowledgements

We thank V. Balasubramanian, A. Bell, S. Bruder, C. Callan, A. Fairhall,
G. Garcia de Polavieja Embid, R. Koberle, A. Libchaber, A. Melikidze, A.
Mikhailov, O. Motrunich, R. Rumiati, R. de Ruyter van Steveninck, N. Slonim,
T. Spencer, S. Still, S. Strong, and A. Treves for many helpful discussions. Our
collaboration was aided in part by a grant from the US–Israel Binational Science
Foundation to the Hebrew University of Jerusalem, and work at Princeton was
supported in part by funds from NEC.

6 References

Abarbanel, H. D. I., Brown, R., Sidorowich, J. J., & Tsimring L. S. (1993). The
analysis of observed chaotic data in physical systems, Revs. Mod. Phys.
65, 1331–1392.

Adami, C., & Cerf N. J. (2000). Physical complexity of symbolic sequences.

Physica D 137, 62–69. See also adap-org/9605002.15

Akaike, H. (1974a).

Aida, T. (1999). Field theoretical analysis of on–line learning of probability dis-
tributions, Phys. Rev. Lett. 83, 3554–3557. See also cond-mat/9911474.
Information theory and an extension of the maximum
likelihood principle, in Second international symposium of information
theory, B. N. Petrov and F. Csaki, eds. Budapest: (Akademia Kiedo,
Budapest).

Akaike, H. (1974b). A new look at the statistical model identiﬁcation, IEEE

Trans. Automatic Control. 19, 716–723.

Atick, J. J. (1992). Could information theory provide an ecological theory of
sensory processing? In Princeton Lectures on Biophysics, W. Bialek, ed.,
pp. 223–289 (World Scientiﬁc, Singapore).

Attneave, F. (1954). Some informational aspects of visual perception, Psych.

Rev. 61, 183–193.

Balasubramanian, V. (1997). Statistical inference, Occam’s razor, and statisti-
cal mechanics on the space of probability distributions, Neural Comp. 9,
349–368. See also cond-mat/9601030.

15Where available, we give references to the Los Alamos e–print archive. Papers may be
retrieved from the web site http://xxx.lanl.gov/abs/*/*, where */* is the reference; thus
Adami and Cerf (2000) is found at http://xxx.lanl.gov/abs/adap-org/9605002. For preprints
this is a primary reference; for published papers there may be diﬀerences between the published
version and the e–print.

41

Barlow, H. B. (1959). Sensory mechanisms, the reduction of redundancy and
intelligence, in Proceedings of the Symposium on the Mechanization of
Thought Processes, vol. 2, D. V. Blake and A. M. Uttley, eds., pp. 537–
574 (H. M. Stationery Oﬃce, London).

Barlow, H. B. (1961). Possible principles underlying the transformation of sen-
sory messages, in Sensory Communication, W. Rosenblith, ed., pp. 217–
234 (MIT Press, Cambridge).

Barlow, H. B. (1983). Intelligence, guesswork, language, Nature 304, 207–209.
Bennett, C. H. (1990). How to deﬁne complexity in physics, and why, in Com-
plexity, Entropy and the Physics of Information, W. H. Zurek, ed., pp.
137–148 (Addison–Wesley, Redwood City).

Bialek, W. (1995). Predictive information and the complexity of time series,

NEC Research Institute technical note.

Bialek, W., Callan, C. G., & Strong, S. P. (1996). Field theories for learning
probability distributions, Phys. Rev. Lett. 77, 4693–4697. See also
cond-mat/9607180.

Bialek, W., & Tishby, N. (1999). Predictive information, preprint. Available at

cond-mat/9902341.

Bialek, W., & Tishby, N. (in preparation). Extracting relevant information.
Brenner, N., Bialek, W., & de Ruyter van Steveninck, R. (2000). Adaptive

rescaling optimizes information transmission, Neuron 26, 695–702.

Bruder, S. D. (1998). Ph.D. Dissertation, Princeton University.
Capaldi, E. A., Robinson, G. E., & Fahrbach S. E. (1999). Neuroethology
of spatial learning: The birds and the bees, Annu. Rev. Psychol. 50,
651–682.

Carpenter, R. H. S., & Williams, M. L. L. (1995). Neural computation of log

likelihood in control of saccadic eye movements. Nature 377, 59–62.

Chaitin, G. J. (1975). A theory of program size formally identical to information

theory. J. Assoc. Comp. Mach. 22, 329–340.

Clarke, B. S., & Barron A. R. (1990).

Information–theoretic asymptotics of

Bayes methods. IEEE Trans. Inf. Thy. 36, 453–471.

Cover, T. M., & King, R. C. (1978). A convergent gambling estimate of the

entropy of English, IEEE Trans. Inf. Thy. 24, 413–421.

Cover, T. M., & Thomas, J. A. (1991). Elements of Information Theory (Wiley,

New York).

Crutchﬁeld, J. P., & Feldman, D. P. (1997). Statistical complexity of simple 1–d
spin systems. Phys. Rev. E 55, 1239–1243R. See also cond-mat/9702191.
Crutchﬁeld, J. P., Feldman, D. P., & Shalizi C. R. (1999). Comment on “Simple

measure for complexity,” preprint. Available at chao-dyn/9907001.

Crutchﬁeld, J. P., & Shalizi, C. R. (1998). Thermodynamic depth of casual
states: objective complexity via minimal representation. Phys. Rev. E

42

59, 275–283. See also cond-mat/9808147.

Crutchﬁeld, J. P., & Young, K. (1989). Inferring statistical complexity, Phys.

Rev. Lett. 63, 105–108.

Ebeling, W., & P¨oschel, T. (1994). Entropy and long-range correlations in

literary English, Europhys. Lett. 26, 241–246.

Feldman, D. P., & Crutchﬁeld, J. P. (1998). Measures of statistical complexity:

why?, Phys. Lett. A 238, 244–252. See also cond-mat/9708186.

Gell–Mann, M., & Lloyd, S. (1996). Information measures, eﬀective complexity,

and total information. Complexity 2, 44–52.

de Gennes, P.–G. (1979). Scaling Concepts in Polymer Physics (Cornell Uni-

versity Press, Ithaca and London).

Grassberger, P. (1986). Toward a quantitative theory of self–generated com-

plexity, Int. J. Theor. Phys. 25, 907–938.

Haussler, D., Kearns, M., Seung, S., & Tishby, N. (1996). Rigorous learning
curve bounds from statistical mechanics, Machine Learning 25, 195–236.
Hawken, M. J., & Parker, A. J. (1991). Spatial receptive ﬁeld organization in
monkey V1 and its relationship to the cone mosaic, in Computational
models of visual processing, M. S. Landy and J. A. Movshon, eds., pp.
83–93 (MIT Press, Cambridge)

Herschkowitz, D., & Nadal, J.–P. (1999). Unsupervised and supervised learning:
mutual information between parameters and observations, Phys. Rev. E,
59, 3344–3360.

Hilberg, W. (1990). The well–known lower bound of information in written
language: Is it a misinterpretation of Shannon experiments? (in German)
Frequenz 44, 243–248.

Holy, T. E. (1997). Analysis of data from continuous probability distributions,

Phys. Rev. Lett. 79, 3545–3548. See also physics/9706015.

Kemeney, J. G. (1953). The use of simplicity in induction. Philos. Rev. 62,

391–315.

Kolmogoroﬀ, A. (1939).

Sur l’interpolation et extrapolations des suites sta-

tionnaires, C. R. Acad. Sci. Paris 208, 2043–2045.

Kolmogorov, A. N. (1941). Interpolation and extrapolation of stationary ran-
dom sequences (in Russian), in Izv. Akad. Nauk. SSSR Ser. Mat.
5, 3–14; translation in Selected Works of A. N. Kolmogorov, vol. II.
A. N. Shiryagev, ed., pp. 272–280 (Kluwer Academic Publishers, Dor-
drecht, The Netherlands).

Kolmogorov, A. N. (1965). Three approaches to the quantitative deﬁnition of

information. Prob. Inf. Trans. 1, 4–7.

Li, M., & Vit´anyi, P. (1993). An Introduction to Kolmogorov Complexity and

its Applications (Springer-Verlag, New York).

Lloyd, S., & Pagels, H. (1988). Complexity as thermodynamic depth, Ann.

43

Phys. 188, 186–213.

Logothetis, N. K., & Sheinberg, D. L. (1996). Visual object recognition, Annu.

Rev. Neurosci. 19, 577–621.

Lopes, L. L., & Oden, G. C. (1987). Distinguishing between random and non-
random events, J. Exp. Psych.: Learning, Memory, and Cognition 13,
392–400.

Lopez–Ruiz, R., Mancini, H. L., & Calbet, X. (1995). A statistical measure of

complexity. Phys. Lett. A 209, 321–326.

MacKay, D. J. C. (1992). Bayesian interpolation, Neural Comp. 4, 415–447.
Nemenman, I. (2000). Ph.D. Dissertation. Princeton University.
Nemenman, I., & Bialek, W. (2000). Learning a continuous distribution: Sim-

ulations with ﬁeld theoretic priors, preprint.

Opper, M. (1994). Learning and generalization in a two-layer neural network:
the role of the Vapnik–Chervonenkis dimension. Phys. Rev. Lett. 72,
2113–2116.

Opper, M., & Haussler, D. (1995). Bounds for predictive errors in the statistical

mechanics of supervised learning. Phys. Rev. Lett. 75, 3772–3775.

Periwal, V. (1997). Reparametrization invariant statistical inference and grav-

ity, Phys. Rev. Lett. 78, 4671–4674. See also hep-th/9703135.

Periwal, V. (1998). Geometrical statistical inference, Nucl. Phys. B 554[FS],

719–730. See also adap-org/9801001.

P¨oschel, T., Ebeling, W., & Ros´e, H. (1995). Guessing probability distributions

from small samples, J. Stat. Phys. 80, 1443–1452.

Rieke, F., Warland, D., de Ruyter van Steveninck, R., & Bialek, W. (1997).

Spikes: Exploring the Neural Code, (MIT Press, Cambridge).

Rissanen, J. (1978). Modeling by shortest data description, Automatica, 14,

465–471.

Rissanen, J. (1984). Universal coding, information, prediction, and estimation.

IEEE Trans. Inf. Thy. 30, 629–636.

Rissanen, J. (1986). Stochastic complexity and modeling. Ann. Statist. 14,

1080–1100.

Rissanen, J. (1987). Stochastic complexity, J. Roy. Stat. Soc. B, 49, 223–239,

253–265.

Rissanen, J. (1989). Stochastic Complexity and Statistical Inquiry (World Sci-

entiﬁc, Singapore).

Rissanen, J. (1996). Fisher information and stochastic complexity. IEEE Trans.

Inf. Thy. 42, 40–47.

Saﬀran, J. R., Aslin, R. N., & Newport, E. L. (1996). Statistical learning by

8–month–old infants, Science 274, 1926–1928.

Saﬀran, J. R., Johnson, E. K., Aslin, R. H., & Newport, E. L. (1999). Statistical
learning of tone sequences by human infants and adults, Cognition 70,

44

27–52.

Seung, H. S., Sompolinsky, H., & Tishby, N. (1992). Statistical mechanics of

learning from examples. Phys. Rev. A 45, 6056–6091.

Shalizi, C. R., & Crutchﬁeld, J. P. (1999). Computational mechanics: pattern
and prediction, structure and simplicity, preprint. Available at cond-
mat/9907176.

Shiner, J., Davison, M., & Landsberger, P. (1999). Simple measure for com-

plexity. Phys. Rev. E 59, 1459–1464.

Shannon, C. E. (1948). A mathematical theory of communication, Bell Sys.
Tech. J. 27, 379–423, 623–656. Reprinted in C. E. Shannon and W.
Weaver, The Mathematical Theory of Communication (University of Illi-
nois Press, Urbana, 1949).

Shannon, C. E. (1951). Prediction and entropy of printed English, Bell Sys.
Tech. J. 30, 50–64. Reprinted in N. J. A. Sloane and A. D. Wyner,
eds., Claude Elwood Shannon: Collected papers (IEEE Press, New York,
1993).

Smirnakis, S., Berry III, M. J., Warland, D. K., Bialek, W., & Meister, M.
(1997). Adaptation of retinal processing to image contrast and spatial
scale, Nature 386, 69–73.

Sole, R. V., & Luque, B. (1999). Statistical measures of complexity for strongly

interacting systems, preprint. Available at adap-org/9909002.

Solomonoﬀ, R. J. (1964). A formal theory of inductive inference, Inform. and

Control 7, 1–22, 224–254.

Tishby, N., Pereira, F., & Bialek, W. (1999). The information bottleneck
method, in Proceedings of the 37th Annual Allerton Conference on Com-
munication, Control and Computing, B. Hajek and R. S. Sreenivas, eds.,
pp. 368–377 (University of Illinois). See also physics/0004057

Vapnik, V. (1998). Statistical Learning Theory (John Wiley & Sons, New York).
Vit´anyi, P., & Li, M. (2000). Minimum description length induction, Bayesian-
ism, and Kolmogorov Complexity, IEEE Trans. Inf. Thy. 46, 446–464.
See also cs.LG/9901014.

Weigend, A. S., & Gershenfeld, N. A., eds. (1994). Time series prediction: Fore-
casting the future and understanding the past (Addison–Wesley, Reading
MA).

Wiener, N. (1949). Extrapolation, Interpolation and Smoothing of Time Series

(Wiley, New York).

45


7
9
9
1
 
n
u
J
 
0
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
1
0
6
0
7
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The Analysis of Data from Continuous Probability Distributions

Timothy E. Holy
Department of Physics, Princeton University, Princeton, New Jersey, 08544
(June 10, 1997)

Abstract

Conventional statistics begins with a model, and assigns a likelihood of ob-
taining any particular set of data. The opposite approach, beginning with the
data and assigning a likelihood to any particular model, is explored here for
the case of points drawn randomly from a continuous probability distribution.
A scalar ﬁeld theory is used to assign a likelihood over the space of probability
distributions. The most likely distribution may be calculated, providing an
estimate of the underlying distribution and a convenient graphical representa-
tion of the raw data. Fluctuations around this maximum likelihood estimate
are characterized by a robust measure of goodness-of-ﬁt. Its distribution may
be calculated by integrating over ﬂuctuations. The resulting method of data
analysis has some advantages over conventional approaches.

When the outcome of an experiment falls into one of a few categories, the frequency of
a particular outcome is an estimate of its probability. For example, by repeatedly ﬂipping
a coin we learn about the probability of obtaining heads. But when the outcome of an
experiment is one of a continuum, no ﬁnite set of data can determine the frequency of
each outcome. One common method of estimating the underlying probability distribution
is to group observations into categories, a procedure known as “binning.” The histogram
(the frequency of observations in each bin) is then used as an estimate of the underlying
probability distribution. While binning is widely used, it has a number of undesirable
consequences.
It requires a choice of bins (both their number and sizes), and diﬀerent
choices lead to diﬀerent histograms. Thus even the appearance of raw data, when presented
in graphical format, depends on arbitrary choices. Binning also throws information away,
since diﬀerent outcomes are grouped together.

An alternative approach has been presented [1,2] to estimate the probability distribution.
These authors assign a likelihood P [Q
x1, . . . , xN ] that the distribution Q(x) describes the
|
data x1, . . . , xN . The underlying distribution might then be estimated as the one which
maximizes P [Q
|

x1, . . . , xN ]. By Bayes’ rule,

P [Q
|

x1, . . . , xN ] =

P [x1, . . . , xN |

Q]P [Q]

P [x1, . . . , xN ]
Q(x1)
· · ·
Q Q(x1)

· · ·

Q(xN )P [Q]

Q(xN )P [Q]

,

=

D

R

1

(1)

(2)

where P [Q] is some a priori likelihood of the distribution Q. As no ﬁnite set of data can
specify an arbitrary function of a continuous variable, a choice for P [Q] is necessary to
regularize the inverse problem. This choice encapsulates our baises in an explicit fashion.
(These biases are implicit in other approaches, e.g., in our interpretation of a histogram.)

,

−∞

What form should P [Q] have? By setting Q(x) = ψ2(x) [1], where ψ may take any value
), we may insure that Q is non-negative. ψ will be referred to as the amplitude
in (
by analogy with quantum mechanics. P [Q] should incorporate our bias that Q be “smooth”
[3]. “Smoothness” is enforced by penalizing large gradients in Q—or rather, in ψ. Finally,
Q should be normalized. In one dimension, the a priori distribution is

∞

P [ψ] =

exp

1
Z

dx

(∂xψ)2

ℓ2
2

"− Z

δ

1
(cid:18)

#

− Z

dx ψ2

,

(cid:19)

where Z is the normalization factor and ℓ is a constant which controls the penalty applied
to gradients. The delta function enforces normalization of the distribution Q.

x1, . . . , xN ] of a distribution Q, given the data, is therefore

The probability P [Q
|
x1, . . . , xN ]

P [ψ

ψ2(xN )

ψ2(x1)
ℓ2
2

dx

· · ·
(∂xψ)2

δ

1
(cid:18)

#

− Z

dx ψ2

(cid:19)

|

exp

×

= e−

S[ψ]δ

∝

"− Z
1
(cid:18)

dx ψ2

,

(cid:19)

− Z

S[ψ] =

dx

Z

ℓ2
2

 

(∂xψ)2

2 ln ψ

−

δ(x

xi)

.

!

−

Xi

where the eﬀective action S is

What is the most likely distribution (amplitude), given the data? From Eq. (5), this is
the ψ which minimizes the action, subject to the normalization constraint. This ψ will be
called the classical amplitude, ψcl. To handle the normalization constraint, we subtract a
Lagrange multiplier term λ(1

dx ψ2) from the action; ψcl satisﬁes the equations

−

R

ℓ2∂2

xψcl + 2λψcl −

−

δ(x

xi) = 0,

−

2
ψcl

Xi

The solution to these equations may be written

dx ψ2

cl = 1.

Z

ψcl(x) = √κ

aie−

κ

x

|

−

xi

|,

Xi
where κ2 = 2λ/ℓ2. Each data point therefore contributes one peak of width 1/κ to the
amplitude ψcl. This is reminiscent of kernel estimation [4], using the amplitude rather than
the probability distribution. Eqs. (7) imply

(3)

(4)

(5)

(6)

(7a)

(7b)

(8)

2

FIG. 1. The classical action, Eq. (10), as a function of ln κ for data drawn randomly from
a gaussian distribution with zero mean and unit variance. Long dash, N = 2000; short dash,
N = 200; dots, N = 20.

(9a)

(9b)

(10)

(11)

2λai

aje−

κ

xi

|

−

xj

| = 1,

i = 1, . . . , N

Xj

N
2λ

+

Xi,j

aiaj κ
|

xi −

xj|

e−

κ

xi

xj

|

−

| = 1.

S[ψcl] = N

λ(κ)

ln Qcl(xi).

−

−

Xi

δ(x

xi)

−

≈

N ¯Q(x).

Xi

These N + 1 equations determine λ and the ai as a function of κ [5].

Using the equation of motion, Eqs. (7), the classical action S[ψcl] may be written

For the proper choice of κ one might hope that Qcl ≈
data points xi arise from the true distribution ¯Q(x), we expect

¯Q, the true distribution. Since the

dx ¯Q(x) ln ¯Q(x), which can be
Therefore, the last term of Eq. (10) is approximately N
interpreted as the entropy (or the information [6]). Using perturbation theory one may
N, so the ﬁrst two terms of Eq. (10) (the penalty for
show that when Qcl ≈
gradients) approximately cancel (more precisely, increase much less rapidly than N).

¯Q, then λ

≈

R

How does one choose κ? In Figure 1, the classical action is plotted against ln κ for data
sets generated from a gaussian distribution. One sees that, over a region of width ln N,
S[ψcl] is insensitive to the precise choice of κ. Therefore, κ may be chosen by ﬁnding the
point of minimum sensitivity

[7,8].

Once κ has been chosen, the maximum likelihood distribution Qcl(x) = ψ2

cl(x) is uniquely
determined. An example of results from this procedure are shown in Figure 2. One sees

dS[ψcl]/d ln κ
|

|

3

FIG. 2. The classical distribution Qcl, for data drawn randomly from a gaussian distribution

(solid line). Dashed curve, N = 2000; dotted curve, N = 20.

convergence towards the underlying distribution as N increases. Note that even for N = 20
the estimate Qcl is illuminating; the advantages of this method over binning are especially
great for small data sets.

While Qcl represents the most likely distribution, other “nearby” distributions should
also be considered. The action may be expanded around the classical amplitude, which to
second order in the ﬂuctuations δψ yields [9]

where

S[ψcl + δψ]

S[ψcl] +

χ2[δψ]

1
4

≈

ℓ2
2

 

+

dx

Z

(∂xδψ)2 + λδψ2

,

!

χ2[δψ] = 4

δψ2(xi)
ψ2
cl(xi)

.

Xi
χ2 is a measure of the goodness of ﬁt between a trial distribution Q = ψ2 and the data.
It is the direct analogue of the conventional χ2 (which here will be called χ2
1); to see this,
re-write χ2 as

using Eq. (11). Now suppose that Q and ¯Q are close, Q(x) = ¯Q(x) + ǫ(x). Then we may
expand the diﬀerence of square roots as

χ2 = 4

dx

Z

(ψ(x)

ψcl(x))2

−
ψ2
cl(x)

≈

4N

dx

Q

Z

(cid:18)q

¯Q
(cid:19)

− q

δ(x

xi)

−

Xi

2

4

(12)

(13)

(14)

Q

(cid:18)q

− q

2

¯Q
(cid:19)

1
4

ǫ2
¯Q

,

≈

which establishes the connection to the traditional deﬁnition χ2
1.
This deﬁnition of χ2 has a number of advantages over χ2
1. Because of the quadratic
dependence on ǫ and the ¯Q term in the denominator, χ2
1 is quite sensitive to the tails of
distributions. In contrast, χ2 as deﬁned in Eq. (13) is robust. It is linear in
is
large, and has no potentially small term in the denominator. Therefore, this deﬁnition χ2
is more robust than χ2
1. Another advantage is that binning is unnecessary. This eliminates
the problems of lost information and arbitrary bin-sizes and -boundaries (and simpliﬁes
the process of ﬁtting, as one need not worry about shifting bin-boundaries). Finally, this
deﬁnition of χ2 is essentially symmetric (exactly so in Eq. (14)), and consequently is a true
metric on the space of probability distributions. (The form in Eq. (14) is known as the
squared Hellinger distance [4].)

when

ǫ
|

ǫ
|

|

|

How is χ2 distributed? To lowest order, the likelihood of any particular ﬂuctuation η is

P [η

x1, . . . , xN ]

dx ψclη

|

exp

×

 −

1
4

δ

∝

(cid:18)Z

χ2[η]

− Z

(cid:19)
ℓ2
2

 

dx

(∂xη)2 + λη2

.

!!

The distribution P (χ2) may in principle be calculated by integrating Eq. (16) over all η with
ﬁxed χ2; a realizable alternative is to calculate its Laplace transform, ˜P (α) =
,
where the expectation is relative to the distribution of η in Eq. (16).

αχ2[η]

e−

h

i

R

→

0+

ψcl/ǫ)−

1
√πǫe−

y2/ǫ. This adds a term (

One challenge in evaluating any integral over η is the “orthogonality condition”
dx ψclη) in Eq. (16). One way to handle this condition is to use the delta-function
δ (
dx ψclη)2/ǫ to the argu-
representation δ(y) = limǫ
ment of the exponential; the path integral may then be expressed formally in terms of
1/2, where L is the appropriate operator (arising from the action,
det(L + ψcl ⊗
Eq. (12)) and ψcl ⊗
ψcl is the matrix with the (x, x′) element equal to ψcl(x)ψcl(x′). The
non-local terms proportional to 1
ǫ are large and must be handled ﬁrst. We know that
limǫ
ǫ in the
determinant must vanish. (This happens because of the all-order singularity of the matrix
ψcl ⊗
ǫ is large, we may evaluate this determinant exactly by working
to ﬁrst order in 1

0+ ǫ det(L + ψcl ⊗
ψcl.) So even though 1
ǫ . Therefore

ψcl/ǫ) must be ﬁnite, so all the terms diverging worse than 1

→

R

det

L +

 

ψcl

ψcl ⊗
ǫ

!

= det L det

1 +

 

L−

ψcl

!
ψcl)

1ψcl ⊗
ǫ
1ψcl ⊗
ǫ

.

!

Tr(L−

= det L

1 +

 

Now we can take the limit ǫ
of χ2 (properly normalized) is therefore

→

0+; the integral over all η is now complete. The distribution

(15)

(16)

(17)

(18)

˜P (α) =

D(γ)T (γ)
D(1)T (1) #

"

1/2

−

,

5

(19)

(20)

(21)

(22)

(23)

(24)

where γ = 4α + 1,

D(γ) =

det(

ℓ2∂2

−

x + 2λ + 2γ
ℓ2∂2

det(

i δ(x
x + 2λ)
P

−

xi)/Qcl)

,

−

T (γ) =

dx dx′ Kγ(x, x′)ψcl(x)ψcl(x′),

and the propagator Kγ = L−

Z
1 satisﬁes

ℓ2∂2

xKγ + 2λKγ +

−

δ(x

xi)Kγ = δ(x

x′).

−

−

2γ
Qcl

Xi

The terms of Eq. (18) can be evaluated exactly. First, consider the ratio of the determi-

nants, Eq. (19). Standard techniques [10] allow one to express D(γ) as the limit as x
of the function E(x; γ), where E satisﬁes

→ ∞

∂2
xE

−

−

2κ∂xE +

δ(x

xi)E = 0

−

γκ2
λQcl

Xi

and E(x) = 1 for x smaller than the smallest data point. Between data points, E(x) =
xi), and a short calculation shows that Ei and Fi satisfy a simple recursion
Ei + Fie−
relation.

2κ(x

−

The traces T (γ) are computed as follows:
dx′ K0(x, x′)ψcl(x′). gγ may be parametrized as

R

let gγ(x) =

dx′ Kγ(x, x′)ψcl(x′) and g0 =

R

gγ(x) = g0(x) +

cie−

κ

x

|

−

xi

|,

√κ
4λ

Xi

and from Eq. (21) the ci satisfy the linear equations

ci + γµi

[cj + (1 + κ
|

xi −

xj|

)aj] e−

κ

xi

|

−

xj

| = 0.

Xj

κ

where µi =
remaining integral over x (which may be done analytically).

2λQcl(xi). Then T (γ) may be expressed in terms of the ci by computing the

In the limit of large N, we may put Qcl ≈

This completes the evaluation of the distribution of χ2. One sees that diﬀerent data
sets yield diﬀerent P (χ2). Therefore, it may be illustrative to consider the limit of large N,
where the distribution of χ2 assumes a more universal form.
¯Q and λ

N. We write χ2 in a form
X dx δψ2 where,
similar to Eq. (14), but introduce a small but necessary change: χ2
heuristically, X is the region over which we may expect to ﬁnd data points. We need only
the size X of X, which may be deﬁned as X = 1
Qcl(xi). The determinant operator is
N
x + κ2(1 + γ)) inside X. Then the ratio of determinants
∂2
ℓ2(
1)X. The traces do not
eκ(√1+γ
(ignoring all but the exponential-order terms) is D(γ)
≈
contribute to the exponential-order terms. Consequently,

x + κ2) outside X, and ℓ2(
∂2

4N

−

−

≈

≈

P

−

R

1

i

6

˜P (α)

e−h

χ2

i(√1+2α

−

1),

≈

(25)

(26)

χ2

h

where
approximately 1/√2 per bin, i.e.,
transform in Eq. (25) to obtain

i ≈

≈

κX/√2. Note that if we identify 1/κ as the eﬀective bin width, then

is
0.7 per degree of freedom. We may invert the Laplace

χ2

h

i

P (z)

χ2
h
i
√2πz3

≈

exp

χ2

1
i  

"h

−

2

h

χ2
i
2z !#

.

−

z
χ2

h

i

The conventional approach to statistics emphasizes the model: given a model, one cal-
culates the likelihood of obtaining a particular data set. This likelihood is measured by the
conventional χ2. Its distribution is over (hypothetical) repeated trials of the experiment,
assuming gaussian errors. In contrast, the approach presented here emphasizes the data:
given a data set, one calculates the likelihood that it is described by a particular model.
This likelihood is measured by χ2; its distribution is over all possible models.

The approach presented here has two major advantages over conventional methods. First,
it provides a technique for visualizing data sets, retaining all the information in the data and
requiring no arbitrary choices. Second, it provides a robust measure of goodness-of-ﬁt. Its
distribution can be calculated, and so may be used for statistical analysis. The availability
of a fast algorithm [5] makes computation time negligible even for large data sets. This
technique should be generalizable to higher dimensions [2].

ACKNOWLEDGMENTS

TEH is supported by a Lucent Technologies Ph.D. Fellowship. I thank S. Strong and W.
Bialek for useful conversations. This work is dedicated to W. H. Press, B. P. Flannery, S.
A. Teukolsky, and W. T. Vetterling.

7

REFERENCES

[1] I. J. Good and R. A. Gaskins, Biometrika 58, 255 (1971).
[2] W. Bialek, C. G. Callan, and S. P. Strong, Phys. Rev. Lett. 77, 4693 (1996).
[3] Without such a bias, e.g., if we choose P [Q] = 1, the most likely Q is the solipsistic

1
N

i δ(x

xi) [2].

−

P

×

[4] L. Devroye, A Course in Density Estimation (Birkh¨auser, Boston, 1987).
[5] Eqs. (9) are solved by Newton’s method, i.e., by linearizing around the solution. An
N block of the resulting matrix equation may be put in the form Au = b, where
N
A = 1 + ∆W, ∆ is a diagonal matrix, and Wij = e−
|. Note that Eq. (24) has
the same form. Solving this linear equation is nominally an O(N 3) process. However,
it is possible to do much better, because (when x1, . . . , xN are sorted in increasing
1 in place of W allows all operations to be
order) Ω = W−
performed in O(N) time, a very signiﬁcant savings for large data sets. Source code
may be requested from holy@puhep1.princeton.edu. Computational issues were also
considered in J. Ghorai and H. Rubin, J. Stat. Comput. Simul. 10, 65 (1979). Existence
and uniqueness of a non-negative ψcl was shown in G. F. de Montricher, R. A. Tapia,
and J. R. Thompson, Ann. Stat. 3, 1329 (1975).

1 is tridiagonal. Using Ω−

xj

xi

−

κ

|

[6] C. E. Shannon and W. Weaver, The Mathematical Theory of Communication (University

of Illinois Press, Urbana, 1949).

[2],
Q(xN )

[7] P. M. Stevenson, Phys. Rev. D 23, 2916 (1981).
the smoothing parameter cannot be set until the expected value
[8] In Ref.
Q(x1)
has been calculated, which requires integrating over the ﬂuctuations
h
1/2) do not qualitatively
and a WKB analysis. Here the ﬂuctuations ([2λD(1)T (1)]−
change Figure 1; even the optimum choice for κ is changed little. Note that the choice
ℓ

in Ref. [2] is (regrettably) zero for many common distributions ¯Q.

· · ·

i

[9] One must decide whether the λ terms are included in computing the ﬂuctuations. The
two choices yield very similar results; the version used here turns out to be somewhat
simpler to implement.

∗

[10] S. Coleman, in Aspects of Symmetry (Cambridge University Press, Cambridge, 1975),

Chap. 7 (Appendix 1).

8


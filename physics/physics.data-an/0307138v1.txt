3
0
0
2
 
l
u
J
 
9
2
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
3
1
7
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Entropy Estimates from Insuﬃcient Samplings

Peter Grassberger
Complex Systems Research Group, John-von-Neumann Institute for Computing,
J¨ulich Research Center, D-52425 J¨ulich, Germany
(Dated: February 3, 2014)

We present a detailed derivation of some estimators of Shannon entropy for discrete distributions.
They hold for ﬁnite samples of N points distributed into M “boxes”, with N and M → ∞, but
N/M < ∞. In the high sampling regime (≫ 1 points in each box) they have exponentially small
biases. In the low sampling regime the errors increase but are still much smaller than for most other
estimators. One advantage is that our main estimators are given analytically, with explicitly known
analytical formulas for the biases.

≈

It is well known that estimating (Shannon) entropies
If one naively re-
from ﬁnite samples is not trivial.
places the probability pi to be in “box” i by the ob-
served frequency, pi
ni/N , statistical ﬂuctuations
tend to make the distribution look less uniform, which
leads to an underestimation of the entropy. There have
been numerous proposals on how to estimate the bias
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. Some make quite
strong assumptions [5, 7], others use Bayesian methods
[6, 11, 12]. As pointed out in [4, 13], one can devise es-
timators with arbitrarily small bias, but these will then
have very large statistical errors. In the present paper
we want to revisit a method used in [4]. There a very
simple correction term was derived which seems to be
a very good compromise between bias, statistical errors,
and ease of use. Unfortunately, the treatment in [4] was
not quite systematic, and in particular the corrections
going beyond the proposed term were wrong. It is the
purpose of the present letter to provide a more system-
atic presentation of the method used in [4], to correct
some of the errors made there, and to propose an esti-
mator which is again very easy to use and which should
be better than that proposed in [4].

≫

We consider M

1 “boxes” (states, possible exper-
imental outcomes, ...) and N
1 points or particles
distributed randomly and independently into the boxes.
We assume that each box has weight pi (i = 1, . . . M )
i pi = 1. Thus each box i will contain a random
with
number ni of points, with E[ni] = piN . Their distribu-
tion is binomial,

P

≫

P (ni; pi, N ) =

N
ni(cid:19)

(cid:18)

pni
i (1

−

pi)N −ni .

(1)

Since entropy H is a sum over terms each of which de-
pends only on one index i, we only need these marginal
distributions instead of the more complicated and non-
factorizing joint distribution. Some of the pi can be zero,
but in the following we shall assume that none of them
is large, i.e. pi
1 for all i. In that limit the numbers
ni are Poisson distributed,

≪

strictly only in the limit N
0
footnote [14].

∀

→
i, but the general case is not much more diﬃcult, see

→ ∞

→ ∞

, M

, ni/N

Our aim is to estimate the entropy,

H =

pi ln pi = ln N

zi ln zi,

(3)

M

−

Xi=1

1
N

−

M

Xi=1

ni

from an observation of the numbers
(in the follow-
ing, all entropies are measured in “natural units”, not in
bits). The estimator ˆH(n1, . . . nM ) will of course have
both statistical errors and a bias, i.e.
if we repeat this
experiment, the average of ˆH will in general not be equal
to H,

{

}

∆H

E[ ˆH]

H

= 0.

≡

−

(4)

, M

→ ∞

→ ∞

In the limit N
, the statistical error
will go to zero (because essentially one averages over
many boxes), but the bias will remain ﬁnite unless also
ni
i in this limit, which we will not assume in the
following. Indeed it is well known that the naive estima-
tor, obtained by assuming zi = ni without ﬂuctuations,

→ ∞ ∀

ˆHnaive = ln N

ni ln ni,

(5)

1
N

−

M

Xi=1

is negatively biased, ∆Hnaive < 0.

In the limit of large N and M each contribution zi ln zi
to the entropy will be statistically independent, and can
thus also be estimated independently by some estimator
which is only a function of ni [4],

zi ln zi

zi ln zi = niφ(ni)

(6)

such that its expectation value is

d

E[

zi ln zi] =

niφ(ni)P (ni; zi).

(7)

≈

∞

Xni=1

d

P (ni; zi) =

zni
i
ni!

−zi

e

E[ni] = piN . The error in going from Eq.(1)
with zi
to (2) is O(1/N ). Thus all derivations given below hold

≡

(2)

Notice that the sum here runs only over strictly positive
values of ni. Eﬀectively this means that we have assumed
that observing an outcome ni = 0 does not give any
information: If ni = 0, we do not know whether this is

6
because of statistical ﬂuctuations or because pi = 0 for
that particular i.

The resulting entropy estimator is then [4] [14]

We write Γ(n+1)/Γ(n+1+q) = B(n+1, q)/Γ(q) and use
the integral representation for the beta function (Ref.[15],
paragraph 6.2.1)

ˆHφ = ln N

M
N

−

nφ(n)

(8)

B(n + 1, q) =

dt (1

(16)

1

Z
0

t)ntq−1.

−

2

with the overbar indicating an average over all boxes,

nφ(n) =

niφ(ni).

(9)

1
M

M

Xi=1

Its bias is

∆Hφ =

(z ln z

E[nφ(n)]).

(10)

M
N

−

It will turn out that some of the derivations given be-
low simplify if we consider instead of the Shannon case
the more general Renyi entropies,

H(q) =

ln

pq
i

1

−
1

−

q

q

1

1

M

Xi=1
M

Xi=1

zq
i −

=

[ln

q ln N ].

(11)

The Shannon case is recovered by taking the limit q
1,
→
H = limq→1 H(q). Eqs.(6) to (10) are then replaced by
zq
zq
i = niφ(ni, q) with φ(n) = dφ(n, q)/dq
i ] =
ni niφ(ni, q)P (ni; zi), and
b
b
P

q=1, E[
|

∆ exp((1

q)H(q))φ =

(zq

E[nφ(n, q)]).

(12)

M
N

−

−

≥

n!

−

,

q)!

(n

zq =

b

since the factorial moments satisfy [14]

∞

n!

−

(n

q)!

Xn=q

P (n; z) = zq.

(14)

This suggests that it might be a good strategy to look
ﬁrst at the generalization of the l.h.s. for arbitrary q, and
then analyze more closely the diﬀerence with zq. In addi-
tion, we will see that we should start with negative real q,
and go to positive q only later by analytic continuation.

We thus deﬁne

A(

q, z) =

−

∞

Xn=1

Γ(n + 1)
Γ(n + 1 + q)

zn
n!

−z

e

E[

Γ(n + 1)
Γ(n + 1 + q)

].

≡

Since both this integral and the sum over n in the deﬁni-
tion of A(
q, z) are absolutely convergent, we can inter-
change them. The sum can then be done exactly, giving

−

A(

q, z) =

−

dt tq−1(e

−tz

−z).

e

1

z

1
Γ(q) Z
0
z−q
Γ(q) Z
0

=

dx xq−1e

−x

−

−

e−z
Γ(1 + q)

. (17)

The last term arises since the sum over n extends only
∞
z
z we can express
0 =
from 1 to
the ﬁrst term as a Gamma function and the second as an
R
R
incomplete Gamma function ([15], paragraph 6.5.3),

∞
0 −
R

. Writing now

∞

A(

q, z) = z

−q

−

z−q
Γ(q)

−

Γ(q, z)

e−z
Γ(1 + q)

.

−

(18)

Here we can ﬁnally continue analytically to positive q.
Furthermore we use the recursion relation (Ref.[15], para-
graph 6.5.22)

Γ(a, z) =

Γ(1 + a, z)

(19)

1
a

e−zxa
a

−

to arrive ﬁnally at

E[

Γ(n + 1)

Γ(n + 1

q)

−

] = zq

zq

−

−

Γ(1

q)

−

Γ(1

q, z).

(20)

(13)

E[nψ(n)] = z ln z + zE1(z) .

(21)

Here, ψ(x) = d ln Γ(x)/dx is the digamma function, and

E1(x) = Γ(0, x) =

(22)

∞

e−xt
t

dt

Z
1

is an exponential integral (Ref.[15], paragraph 5.1.4).

Eq.(21) is our ﬁrst important result. For large values
e−z. Thus, if z = E[n] is large, it is
of z, zE1(z)
an exponentially good approximation to simply neglect
the last term in Eq.(21). We call the resulting entropy
estimator ˆHψ [4] [14],

≈

ˆHψ = ln N

niψ(ni).

(23)

1
N

−

M

Xi=1

0, and in
0 we have also zE1(z)
Moreover, for z
between 0 and
the function is positive with a single
maximum at z = 0.434... where zE1(z) = 0.2815.... If

→
∞

→

(15)

For integer q

2, the bias-free estimator is given by

(in the following we shall suppress the index i)

For the Shannon case we take the derivative with respect
to q at q = 1 and obtain [14]

we simply neglect the last term, we make thus a negative
bias, but at most by

On the one hand, using formula 0.244 of [16], one can
write this integral as an inﬁnite sum,

0 <

∆Hψ = zE1(z)M/N < 0.2815 . . .

M/N. (24)

−

×

Gn = ψ(n) + (

1)n

−

∞

Xl=0

1
(n + 2l)(n + 2l + 1)

,

(31)

3

∞

−

Xn=1

1)n
(
−
n + 1

zn
n!

−z = e

e

−z

e−z
z

+

e−2z
z

−

(26)

−
Therefore, combining this with Eq.(21), we have

−

E1(2z)).

(32)

If we approximate further ψ(x)
naive estimator. The better approximation ψ(x)
−
1/2x gives Miller’s correction [1, 3]. It can be shown that

ln x, we obtain the

ln x

≈

≈

E[n ln n] > E[n ln n

1/2] > E[nψ(n)] > z ln z

(25)

for all positive z. Thus both the naive estimate and
Miller’s correction are worse than ˆHψ. The diﬀerence
is especially big for large z, where the error of the
naive estimate goes to M/2N , the error after applying
M/zN , while the error of ˆHψ is
Miller’s correction is

∼

z)M/N .

exp(
But we can do even better. First we notice that

−

−

∼

which has the same leading behaviour for large z as
zE1(z).
0, is positive for
all z
), and is smaller than zE1(z) for all z. Thus,
[0,
replacing ψ(n) by

It also goes to zero for z

∞

→

∈

ψ(n) +

(
−

1)n
n(n + 1)

(27)

gives an improved estimator. Apart from a misprint, this
is the estimator recommended in [4], Eq.(13).

This equation had been derived in [4] somewhat un-
systematically, using asymptotic series expansions in an
uncontrolled way. Because of that, the discussion of the
more general approximation, Eq.(11) in that paper, is
wrong. In particular, Eq.(11) holds (for q
1) not for
all integer R, but only for odd values of R. Furthermore,
the fact that the terms neglected in Eq.(11) decrease as
z−Re−z for large z does not mean that Eq.(11) is exact
in the limit R
. Finally, in contrast to what is said
there, this limit can be taken without a risk of statistical
errors blowing up, at least for q

→ ∞

→

1.

Instead of following the derivation of [4], we consider
the semi-inﬁnite sequence of real numbers G1, G2, . . . de-
ﬁned by

→

γ

G1 =
−
−
γ
G2 = 2
−
G2n+1 = G2n,

ln 2,

ln 2,

−

and

G2n+2 = G2n +

2
2n + 1

(n

1).

≥

Thus G2n =
γ
−
Using the representation ψ(n) =
1), one checks that
. . . + 1/(n

−

ln 2 + 2/1 + 2/3 + 2/5 + . . .+ 2/(2n

1).
γ + 1/1 + 1/2 + 1/3 +

−

−

−

(28)

(29)

which can be compared to Eq.(11) of [4] with q
odd R

. On the other hand, we obtain

→

1 and

→ ∞

E[n(Gn

−

ψ(n))] =

n(Gn

ψ(n))

−

−z

e

zn
n!
∞

(
−
(n

xz)n−1
1)!

−

∞

Xn=1

1

− Z
0

dx
x + 1

−zz

e

Xn=1

−xz

1

dx
x + 1

e

−

−zz

e

Z

0
z(E1(z)

=

=

=

E[nGn] = z ln z + zE1(2z).

(33)

This is our main result. Since the last term decreases
as e−2z, the error made when neglecting it decreases ex-
ponentially faster with z = E[n] than when neglecting
the last term in Eq.(21), for large z. Thus, if all boxes
have E[ni] > 5, say, the error committed is < e−10 which
should be negligible in all practical cases. More gener-
ally, the error made by neglecting the last term is again
always negative, and it is bounded by

0 <

∆HG < 0.1407 . . .

M/N,

(34)

−

×

where [14]

ˆHG = ln N

niGni

(35)

1
N

−

M

Xi=1

is our proposed best estimator.

Let us denote by z∗ = 0.217 . . . the position of the
maximum of zE1(2z). For z < z∗ this function is convex.
Thus, if N/M < z∗, the distribution of z-values over the
boxes which gives the maximal bias is a delta function,
N/M ), and Eq.(34) can be improved to
P (z) = δ(z

E1(2N/M ). For N/M

0 this diverges

→

∼

−

∆HG
−
≤
ln(M/N ).

We might add that truncating the sum in Eq.(31) at
any ﬁnite l also gives valid estimators whose errors are
between those of ˆHG and ˆHψ, but there seems no reason
to prefer any of them over ˆHG or ˆHψ. Taking only the
term with l = 0 gives Eq.(27).
The error terms E[nφ(n)]

z ln z for φ(n) = ln n, ln n

−
1)n/n/(n + 1), and Gn are shown
1/2, ψ(n), ψ(n) + (
in Fig.1, together with one more curve discussed below.
The functions φ(n) themselves are shown in Fig.2.

−

−

We can give estimators with even smaller absolute bias,
< 0.1407 . . ., but they have several draw-

Gn = ψ(n) + (

1)n

−

Z
0

1

xn−1
x + 1

dx.

(30)

i.e. with
backs:

∆H
|

|

)
n
(
φ

3

2

1

0

-1

-2

-3

z
 
n
l
 
z
 
 
-
 
 

z
>
 
)
n
(
φ
 
n
 
<

0.6

0.5

0.4

0.3

0.2

0.1

0

-0.1

0

φ(n) = ln n
φ(n) = ln n - 1/2n
φ(n) = ψ(n)
φ(n) = ψ(n) + (-1)n /n/(n+1)
φ(n) = Gn
φ(n)  annealed

0

2

4

6

10

12

14

8

n

FIG. 1: Error terms for ﬁxed z = E[n] and for diﬀerent func-
tions φ(n). While the ﬁrst ﬁve are analytic, the last one is
just one typical simulated annealing result. Diﬀerent cost
functions, annealing schemes, and random number sequences
will give slightly diﬀerent results.

φ(n) = ln n
φ(n) = ln n - 1/2n
φ(n) = ψ(n)
φ(n) = ψ(n) + (-1)n /n/(n+1)
φ(n) = Gn
φ(n)  annealed

1

2

3

4

5

6

z = <n>

FIG. 2: Functions φ(n) corresponding to the error terms
shown in Fig.1. Notice that they are deﬁned only for inte-
ger n. Values at non-integer n are just plotted to guide the
eye.

•

•

Their biases can have either sign.

We were only able to ﬁnd them numerically, by
minimizing (by simulated annealing) a cost func-
tion like e.g. the L2 norm

δ =

Z

0

∞

∞

dz
√z |

Xn=1

nφ(n)

zn
n!

−z

e

−

z ln z

2.
|

(36)

4

•

→

P

∞
n=1 n [φ(n)

The resulting function φ(n) replacing ψ(n) resp.
Gn is not monotonic, and its total variation as mea-
Gn]2 would diverge
sured e.g. by
−
as δ
0 (indeed, the results shown in Figs.1 and 2
were obtained by adding 0.0002 times this term as
a regularizer to the L2 norm). This is the most se-
rious drawback. It means that large cancellations
must occur and thus statistical errors blow up in
the limit δ
0 (if N is kept ﬁnite), as is to be
expected on general grounds [4]. There cannot be
any estimator of H completely free of bias for ﬁnite
N . Notice that Gn is the “best” sequence which is
still monotonic. Estimates based on non-monotonic
φ(n) might be useful if one has important contribu-
tions from extremely small z, i.e. if either N
M
or if the distribution of pi is so uneven that many
boxes have small (but not too small) zi.

→

≪

≥

I have applied the above estimators to the six exam-
ples shown in Fig.4 of [12]. In each of these examples the
number of boxes was M
1000, although the number of
non-empty boxes was smaller in some of them. Never-
theless, the distributions were severely undersampled in
300. In all cases the annealed φ(n)
most cases when N
shown in Fig.2 gave statistical errors smaller or compa-
rable to the Bayesian estimators of [12], and the bias was
smaller than the statistical errors for all N
300. In all
but two cases (Zipf’s law and β = 1, with β deﬁned in
[12]) the bias was negligible even down to N = 10. With
Eq.(35), the bias was signiﬁcant (> 2σ) in the same two
cases for all N
300, and in the case β = 0.02 for
N = 10.

≥

≤

≤

In summary, I hope to have clariﬁed the arguments
and corrected the mistakes made in [4], and I have sub-
stantially improved on the results. I have proposed a new
analytic estimator for Shannon entropy which has very
small systematic errors, except when the average num-
ber of points per box is much smaller than 1. Its sta-
tistical errors should be larger than those of the naive
estimator (since there contributions from ni = 1 and
from ni > 1 partially cancel), but this diﬀerence should
be small. In addition, it is shown that numerically ob-
tained estimators can be useful for extremely undersam-
pled cases. The estimator ˆHψ and the ﬁrst correction
based on Eq.(27) can be generalized straightforwardly to
Renyi entropies, but I was not able to generalize the new
estimator, Eq.(35), to q
= 1. The present estimators can
not match the best Bayesian estimators [12] when the
sampling is extremely low, but they are much simpler to
use and more robust, as no guess of any prior distribution
is needed.

Typical results obtained in this way are shown in
Figs.1 and 2 [17].

I want to thank Walter Nadler for carefully reading the

manuscript, and to Liam Paninski for correspondence.

[1] G. Miller, Note on the bias of information estimates. In
H. Quastler, ed., Information theory in psychology II-B,

pp 95-100 (Free Press, Glencoe, IL 1955).

6
5

The last term can be estimated very similarly to the last
term in Eq.(21), in particular it is positive and is bounded
N−1 (1 − p)N . In the estimator Eq.(23)
for all N and p by N
which results from neglecting this term, replacing the
Poisson distribution by the correct binomial one amounts
to replacing ln N by ψ(N ) (bringing e.g. Miller’s correc-
tion from M/2N down to (M − 1)/2N [13]). Similarly, in
Eq.(35) one should replace ln N by GN , in order to cor-
rect for the most important O(1/N ) term not included in
the Poisson approximation. For all estimators (including
those where φ(n) is obtained numerically), one should
replace in Eq.(8) ln N by ψ(N ).

[15] M. Abramowitz and I. Stegun, eds., Handbook of Mathe-

matical Functions (Dover, New York 1965).

[16] I.S. Gradshteyn and I.M. Ryshik, Tables of Integrals, Se-
ries, and Products (Academic Press, New Yrok 1965).
[17] The coeﬃcients φ(n) for this solution can be obtained by
sending an e-mail to p.grassberger@fz-juelich.de.

[2] B. Harris, Colloquia Math. Soc. Janos Bolya, p. 323

(175).

[3] H. Herzel, Sys. Anal. Mod. Sim. 5, 435 (1988).
[4] P. Grassberger, Phys. Lett. A 128, 369 (1988).
[5] A.O. Schmitt, H. Herzel, and W. Ebeling, Europhys.

Lett. 23, 303 (1993).

[6] D.H. Wolpert and D.R. Wolf, Phys. Rev. E 52, 6841

[7] T. Poschel, W. Ebeling, and H. Rose, J. Stat. Phys. 80,

(1995).

1443 (1995).

[8] S. Panzeri and A. Treves, Network: Computation in Neu-

ral Systems 7, 87 (1996).

[9] T. Sch¨urmann and P. Grassberger, Chaos 6, 414 (1996).
[10] S. Strong, R. Koberle, Rob R. de Ruyter van Steveninck,
and W. Bialek, Phys. Rev. Lett. 80, 197-200 (1998).
[11] D. Holste, I. Grosse, and H. Herzel, J. Phys. A 31, 2551

(1998).

[12] I. Nemenman, F. Shafee, and W. Bialek, Entropy and
inference, revisited. In T.G. Dietterich et al., eds., Ad-
vances in neural information processing 14 (MIT Press,
Cambridge 2002).

[13] L. Paninski, Neural Computation 15, 1191 (2003)
[14] For the correct binomial distribution, Eq.(14) is replaced
by E[n!/(n − q)!] = pqN !/(N − q)!, and Eq.(21) by

E[nψ(n)] = z ln z + z[ψ(N ) − ln N ] + z

1−p

xN−1dx
1 − x

.

Z

0


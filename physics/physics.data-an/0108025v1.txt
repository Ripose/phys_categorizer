1
0
0
2
 
g
u
A
 
5
1
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
5
2
0
8
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Entropy and Inference, Revisited

Ilya Nemenman,1 Fariel Shafee,2 and William Bialek1
1NEC Research Institute, 4 Independence Way, Princeton, New Jersey 08540
2Department of Physics, Princeton University, Princeton, New Jersey 08544

nemenman/bialek
}

{

@research.nj.nec.com, fshafee@princeton.edu

February 1, 2013

Abstract

We study properties of popular, near–uniform, priors for learning undersampled probability dis-
tributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an
Occam–style phase space argument allows us to salvage the priors and turn the problems into a sur-
prisingly good estimator of entropies of discrete distributions.

Learning a probability distribution from examples is one of the basic problems in data analysis.
Common practical approaches introduce a family of parametric models, leading to questions about model
selection. In Bayesian inference, computing the total probability of the data arising from a model involves
an integration over parameter space, and the resulting “phase space volume” automatically discriminates
against models with larger numbers of parameters—hence the description of these volume terms as
Occam factors [1, 2]. As we move from ﬁnite parameterizations to models that are described by smooth
functions, the integrals over parameter space become functional integrals and methods from quantum
ﬁeld theory allow us to do these integrals asymptotically; again the volume in model space consistent
with the data is larger for models that are smoother and hence less complex [3]. Further, at least under
some conditions the relevant degree of smoothness can be determined self–consistently from the data, so
that we approach something like a model independent method for learning a distribution [4].

}

{

qi

· · ·

, i = 1, 2,

The results emphasizing the importance of phase space factors in learning prompt us to look back to
a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space. Here
the probability distribution is just a list of numbers
, K, where K is the number of
bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to
believe that any qi and qj should be similar. The task is to learn this distribution from a set of examples,
K
which we can describe as the number of times ni each possibility is observed in a set of N =
i=1 ni
samples. This problem arises in the context of language, where the index i might label words or phrases,
so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about
similarity are consistent with the constraints of a metric space. Similarly, in bioinformatics the index i
might label n–mers of the the DNA or amino acid sequence, and although most work in the ﬁeld is based
on metrics for sequence comparison one might like an alternative approach that does not rest on such
assumptions. In the analysis of neural response, once we ﬁx our time resolution the response becomes a
set of discrete “words,” and estimates of the information content in the response are determined by the
probability distribution on this discrete space. What all of these examples have in common is that we
K. Thus,
often need to draw some conclusions with data sets that are not in the asymptotic limit N

P

≫

1

while we might use a large corpus to sample the distribution of words in English by brute force (reaching
N

K with K the size of the vocabulary), we can hardly do the same for two or three word phrases.
≫
In models described by continuous functions, the inﬁnite number of “possibilities” can never be
overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of
this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities
may play this role. If we have a joint distribution of two variables the analog of a smooth distribution
would be one which does not have too much mutual information between them. Even more simply, we
might say that smooth distributions have large entropy. While the idea of “maximum entropy inference”
is common [5], the interplay between constraints on the entropy and the volume in the space of models
seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly
sensible, more or less uniform priors on the space of discrete probability distributions correspond to
disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that
K requires a more uniform prior on the entropy,
reliable inference outside the asymptotic regime N
and we offer one way of doing this. While many distributions are consistent with the data when N
K,
we provide empirical evidence that this ﬂattening of the entropic prior allows us to make surprisingly
reliable statements about the entropy itself in this regime.

≫

≤

At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform

priors on the space of distributions. The natural “uniform” prior is given by

Pu(
{

qi

}

) =

K

1
Zu

δ

1

 

−

!

i=1
X

qi

, Zu =

dq1dq2 · · ·

dqK δ

1

−

 

ZA

K

i=1
X

qi

!

where the delta function imposes the normalization, Zu is the total volume in the space of models, and
the integration domain
is such that each qi varies in the range [0, 1]. Note that, because of the normal-
ization constraint, an individual qi chosen from this distribution in fact is not uniformly distributed—this
is also an example of phase space effects, since in choosing one qi we constrain all the other
.
}
What we mean by uniformity is that all distributions that obey the normalization constraint are equally
likely a priori.

qj6=i

A

{

Inference with this uniform prior is straightforward. If our examples come independently from

then we calculate the probability of the model

with the usual Bayes rule:

qi

P (
{

ni

) =

}|{

}

ni

P (
{

qi
Pu(
)
}
}|{
{
)
ni
Pu(
}
{

qi

}

, P (
{

ni

qi

) =

}|{

}

(qi)ni .

K

i=1
Y

qi

}

{
)

If we want the best estimate of the probability qi in the least squares sense, then we should compute the
conditional mean, and this can be done exactly, so that [6, 7]

=

qi
h

i

ni + 1
N + K

.

Thus we can think of inference with this uniform prior as setting probabilities equal to the observed fre-
quencies, but with an “extra count” in every bin. This sensible procedure was ﬁrst introduced by Laplace
(cf. [8]). It has the desirable property that events which have not been observed are not automatically
assigned probability zero.

A natural generalization of these ideas is to consider priors that have a power–law dependence on the

(1)

qi

,

}

{

(2)

(3)

(4)

probabilities:

β(
{

P

qi

}

) =

1
Z(β)

δ

1

−

 

K

K

qi

qβ−1
i

,

i=1
X

!

i=1
Y

2

It is interesting to see what typical distributions from these priors look like. Even though different qi’s are
not independent random variables due to the normalizing δ–function, generation of random distributions
is still easy: one can show that if qi’s are generated successively (starting from i = 1 and proceeding up
to i = K) from the Beta–distribution

qi

P (qi) = B

; β, (K

i)β

, B (x; a, b) =

1

 

−

j<i qj

−

!

xa−1(1

x)b−1

−
B(a, b)

,

}

{

}

qi

qi

β(
{

P
then the probability of the whole sequence

is
). Fig. 1 shows some typical distributions gen-
P
erated this way. They represent different regions of the
range of possible entropies: low entropy (
1 bit, where
only a few bins have observable probabilities), entropy
in the middle of the possible range, and entropy in the
vicinity of the maximum, log2 K. When learning an un-
known distribution, we usually have no a priori reason to
expect it to look like only one of these possibilities, but
choosing β pretty much ﬁxes allowed “shapes.” This will
be a focal point of our discussion.

∼

0.8

  0
0.2

q

q

  0
0.01

q

Even though distributions look different, inference

   0
0

with all priors Eq. (4) is similar [6, 7]

β = 0.0007,  S = 1.05 bits

β = 0.02, S = 5.16 bits

β = 1, S = 9.35 bits

qi
h

β =
i

ni + β
N + κ

,

κ = Kβ.

200

400

600

800

1000

bin number
Figure 1: Typical distributions, K = 1000.

Together with the Laplace’s formula, β = 1, this family includes the usual maximum likelihood estimator
0, that identiﬁes probabilities with frequencies, as well as the Krichevsky–Troﬁmov (KT)
(MLE), β
estimator, β = 1/2 (cf. [9]), the Schurmann–Grassberger (SG) estimator, β = 1/K [8], and other
popular choices.

→

To understand why inference in the family of priors deﬁned by Eq. (4) is unreliable, consider the
entropy of a distribution drawn at random from this ensemble. Ideally we would like to compute this
whole a priori distribution of entropies,

β(S) =

P

dq1dq2 · · ·

dqK Pβ(
{

qi

}

) δ

S +

"

qi log2 qi

,

#

Z

K

i=1
X

but this is quite difﬁcult. However, as noted by Wolpert and Wolf [6], one can compute the moments of
β(S) rather easily. Transcribing their results to the present notation (and correcting some small errors),

P
we ﬁnd:

ξ(β)

σ2(β)

≡ h

S[ni = 0]

β = ψ0(κ + 1)
i
≡ h
(δS)2[ni = 0]
β =
i

β + 1
κ + 1

−
ψ1(β + 1)

−

ψ0(β + 1) ,

ψ1(κ + 1) ,

where ψm(x) = (d/dx)m+1 log2 Γ(x) are the polygamma functions.

(5)

(6)

(7)

(8)

(9)

3

2

1

K

0.8

0.2

0.6

0.4

g
o

0.5

0.1

0.4

0.8

0.3

0.2

0.7

0.6

0.9

)
β
(
σ

≈

l
 
/
 
)
β
(
ξ

K=10  
K=100 
K=1000

This behavior of the moments is shown on Fig. 2.
We are faced with a striking observation: a priori dis-
tributions of entropies in the power–law priors are ex-
tremely peaked for even moderately large K. Indeed, as
a simple analysis shows, their maximum standard devia-
tion of approximately 0.61 bits is attained at β
1/K,
where ξ(β)
1/ ln 2 bits. This has to be compared
with the possible range of entropies, [0, log2 K], which is
asymptotically large with K. Even worse, for any ﬁxed
O(K 0),
β and sufﬁciently large K, ξ(β) = log2 K
1/√κ. Similarly, if K is large, but κ is
and σ(β)
√κ. This paints a
small, then ξ(β)
results in a
lively picture: varying β between 0 and
smooth variation of ξ, the a priori expectation of the en-
tropy, from 0 to Smax = log2 K. Moreover, for large
K, the standard deviation of
β(S) is always negligible
relative to the possible range of entropies, and it is neg-
1/K). Thus a
ligible even absolutely for ξ
seemingly innocent choice of the prior, Eq. (4), leads to a disaster: ﬁxing β speciﬁes the entropy almost
uniquely. Furthermore, the situation persists even after we observe some data: until the distribution is
well sampled, estimate of the entropy is dominated by the prior!

Figure 2: ξ(β)/ log2 K and σ(β) as func-
tions of β and K; gray bands are the region
of
σ(β) around the mean. Note the transi-
tion from the logarithmic to the linear scale
at β = 0.25 in the insert.

κ, and σ(β)

0
1e−7 1e−5 1e−3 .25 

1 (β

≫

∞

≫

≈

∝

∝

∝

±

−

P

0
0

1.5

0.5

2.0 

1.0 

1.5 

1
β

2

β

Thus it is clear that all commonly used estimators mentioned above have a problem. While they may
1, they are deﬁnitely a poor tool to learn
or may not provide a reliable estimate of the distribution
entropies. Unfortunately, often we are interested precisely in these entropies or similar information–
theoretic quantities, as in the examples (neural code, language, and bioinformatics) we brieﬂy mentioned
earlier.

qi

}

{

Are the usual estimators really this bad? Consider this: for the MLE (β = 0), Eqs. (8, 9) are formally
wrong since it is impossible to normalize
P0(S) = δ(S) still
}
holds. Indeed, SML, the entropy of the ML distribution, is zero even for N = 1, let alone for N = 0.
In general, it is well known that SML always underestimates the actual value of the entropy, and the
correction

). However, the prediction that

P0(
{

qi

S = SML +

+ O

K ∗
2N

1
N 2

(10)

(cid:19)

(cid:18)
is usually used (cf. [8]). Here we must set K ∗ = K
1 to have an asymptotically correct result.
Unfortunately in an undersampled regime, N
K, this is a disaster. To alleviate the problem, different
authors suggested to determine the dependence K ∗ = K ∗(K) by various (rather ad hoc) empirical [10]
or pseudo–Bayesian techniques [11]. However, then there is no principled way to estimate both the
residual bias and the error of the estimator.

≪

−

The situation is even worse for the Laplace’s rule, β = 1. We were unable to ﬁnd any results in the
literature that would show a clear understanding of the effects of the prior on the entropy estimate, SL.
1/√K and is almost
And these effects are enormous: the a priori distribution of the entropy has σ(1)

1In any case, the answer to this question depends mostly on the “metric” chosen to measure reliability. Minimization of bias,
variance, or information cost (Kullback–Leibler divergence between the target distribution and the estimate) leads to very different
“best” estimators.

∼

4

δ-like. This translates into a very certain, but nonetheless possibly wrong, estimate of the entropy. We
believe that this type of error (cf. Fig. 3) has been overlooked in some previous literature.

The Schurmann–Grassberger estimator, β = 1/K, deserves a special attention. The variance of
β(S) is maximized near this value of β (cf. Fig. 2). Thus the SG estimator results in the most uniform a
P
priori expectation of S possible for the power–law priors, and consequently in the least bias. We suspect
that this feature is responsible for a remark in Ref. [8] that this β was empirically the best for studying
printed texts. But even the SG estimator is ﬂawed: it is biased towards (roughly) 1/ ln 2, and it is still a
priori rather narrow.

β = 0.001
β = 0.02 
β = 1    

5

4

3

2

0

S
−

 

 

>
S
<

β

1

−1

−2

−3
  10 

qi
h

Summarizing, we conclude that simple power–law
priors, Eq. (4), must not be used to learn entropies
when there is no strong a priori knowledge to back
them up. On the other hand, they are the only pri-
,
ors we know of that allow to calculate
i
Is there a way to resolve the prob-
. . . exactly [6].
lem of peakedness of
β(S) without throwing away
their analytical ease? One approach would be to use
actual(S[qi]) as a prior on
ﬂat
β (
{
P
. This has a feature that the a priori distribution of S
qi
{
deviates from uniformity only due to our actual knowl-
β(S) does.
edge
However, as we already mentioned,
β(S[qi]) is yet to
be calculated.

P
) = Pβ ({qi})

actual(S[qi]), but not in the way

Pβ (S[qi]) P

χ2
h

S
h

,
i

,
i

qi

P

P

P

}

}

−

δ(S

ξ)dξ.

1 =
qi
(
{

Another way to a ﬂat prior is to write

(S) =
If we ﬁnd a family of priors
, parameters) that result in a δ-function over S,
R
P
}
and if changing the parameters moves the peak across
the whole range of entropies uniformly, we may be able
β(S) is almost a δ-function! 2
to use this. Luckily,
ξ(β) =

S[ni = 0]

P

P

  30 

 100 

 1000

 3000

10000

 300 
N
Figure 3: Learning the β = 0.02 distribution
from Fig. 1 with β = 0.001, 0.02, 1. The
actual error of the estimators is plotted; the
error bars are the standard deviations of the
posteriors. The “wrong” estimators are very
certain but nonetheless incorrect.

In addition, changing β results in changing

h

β across the whole range [0, log2 K]. So we may hope that
i

qi

(
{

}

P

; β) =

1
Z

δ

1
 

−

K

K

qi

i=1
X

!

i=1
Y

qβ−1
i

dξ(β)

dβ P

(β)

may do the trick and estimate entropy reliably even for small N , and even for distributions that are
atypical for any one β. We have less reason, however, to expect that this will give an equally reliable
estimator of the atypical distributions themselves.1 Note the term dξ/dβ in Eq. (11). It is there because
ξ, not β, measures the position of the entropy density peak.

Inference with the prior, Eq. (11), involves additional averaging over β (or, equivalently, ξ), but is

nevertheless straightforward. The a posteriori moments of the entropy are

Sm =

dξ ρ(ξ,

Sm[ni]

)
ni
{
h
dξ ρ(ξ, [ni])

}

R

iβ(ξ)

, where

2The approximation becomes not so good as β → 0 since σ(β) becomes O(1) before dropping to zero. Even worse, Pβ(S)
is skewed at small β. This accumulates an extra weight at S = 0. Our approach to dealing with these problems is to ignore them
while the posterior integrals are dominated by β’s that are far away from zero. This was always the case in our simulations.

c

R

(11)

(12)

5

ρ(ξ, [ni]) =

(β (ξ))

P

Γ(κ(ξ))
Γ(N + κ(ξ))

Γ(ni + β(ξ))
Γ(β(ξ))

.

K

i=1
Y

(13)

Sm[ni]

h

Here the moments
iβ(ξ) are calculated at ﬁxed β according to the (corrected) formulas of
Wolpert and Wolf [6]. We can view this inference scheme as follows: ﬁrst, one sets the value of β
and calculates the expectation value (or other moments) of the entropy at this β. For small N , the ex-
pectations will be very close to their a priori values due to the peakedness of
β(S). Afterwards, one
integrates over β(ξ) with the density ρ(ξ), which includes our a priori expectations about the entropy of
(β (ξ))], as well as the evidence for a particular value of β [Γ-terms
the distribution we are studying [
in Eq. (13)].

P

P

The crucial point is the behavior of the evidence. If it has a pronounced peak at some βcl, then the
integrals over β are dominated by the vicinity of the peak,
S is close to ξ(βcl), and the variance of the
estimator is small. In other words, data “selects” some value of β, much in the spirit of Refs. [1] – [4].
However, this scenario may fail in two ways. First, there may be no peak in the evidence; this will result
in a very wide posterior and poor inference. Second, the posterior density may be dominated by β
0,
which corresponds to MLE, the best possible ﬁt to the data, and is a discrete analog of overﬁtting. While
all these situations are possible, we claim that generically the evidence is well–behaved. Indeed, while
small β increases the ﬁt to the data, it also increases the phase space volume of all allowed distributions
and thus decreases probability of each particular one [remember that
β has extra β counts in each
i
bin, thus distributions with qi < β/(N + κ) are strongly suppressed]. The ﬁght between the “goodness
of ﬁt” and the phase space volume should then result in some non–trivial βcl, set by factors
N in the
exponent of the integrand.

qi
h

→

∝

b

}

P

qi

(
{

Figure 4 shows how the prior, Eq. (11), performs on some of the many distributions we tested. The
) and, therefore, are
left panel describes learning of distributions that are typical in the prior
also likely in
; β). Thus we may expect a reasonable performance, but the real results exceed all
expectations: for all three cases, the actual relative error drops to the 10% level at N as low as 30 (recall
that K = 1000, so we only have
0.03 data points per bin on average)! To put this in perspective,
simple estimates like ﬁxed β ones, MLE, and MLE corrected as in Eq. (10) with K ∗ equal to the number
of nonzero ni’s produce an error so big that it puts them off the axes until N > 100. 3 Our results have
two more nice features: the estimator seems to know its error pretty well, and it is almost completely
unbiased.

β(
{

∼

qi

P

}

One might be puzzled at how it is possible to estimate anything in a 1000–bin distribution with just a
few samples: the distribution is completely unspeciﬁed for low N ! The point is that we are not trying to
learn the distribution — in the absence of additional prior information this would, indeed, take N
K
— but to estimate just one of its characteristics. It is less surprising that one number can be learned well
with only a handful of measurements. In practice the algorithm builds its estimate based on the number
of coinciding samples (multiple coincidences are likely only for small β), as in the famous Ma bound
[12].

≫

}

P

qi

; β)? Since there is no

What will happen if the algorithm is fed with data from a distribution
(
{

that is strongly atypical
in
,
}
there is some β which produces distributions with the same mean entropy as S[˜qi]. Such β should be
determined in the usual ﬁght between the “goodness of ﬁt” and the Occam factors, and the correct value
of entropy will follow. However, there will be an important distinction from the “correct prior” cases.
The value of β indexes available phase space volumes, and thus smoothness (complexity) of the model

in our prior, its estimate may suffer. Nonetheless, for any

˜qi

˜qi

˜qi

{

}

}

{

{

3More work is needed to compare our estimator to more complex techniques, like in Ref. [10, 11].

6

^

S

 
/
 
)
 

 

S
−
S

 

 
(

S

 
/
 
)
 

 

S
−
S

 

 
(

S

 
/
 
)
 

 

S
−
S

 

 
(

 0.6

    

    

   0

−0.2
 0.6

    

    

   0

−0.2
 0.1

   0

    

    

−0.3

^

^

(a)

(b)

β = 0.0007
S = 1.05 bits

β = 0.02
S = 5.16 bits

β = 1.0
S = 9.35 bits

^

S

 
/
 
)
 

 

S
−
S

 

 
(

S

 
/
 
)
 

 

S
−
S

 

 
(

S

 
/
 
)
 

 

S
−
S

 

 
(

 0.3
    
    
   0
    
    
    
−0.4
 0.4

    

   0

−0.2
 0.4

    

   0

−0.2

^

^

β = 0.02
K = 2000 (half empty)
S = 5.16 bits

Zipf’s law: q
 ~ 1/i
i
K = 1000
S = 7.49 bits

 ~ 50 − 4 (ln i)2
q
i
K = 1000
S = 4.68 bits

  10 

  30 

 100 

 1000

 3000

10000

 300 
N
Figure 4: Learning entropies with the prior Eq. (11) and
estimator are plotted; the error bars are the relative widths of the posteriors.
Fig. 1. (b) Distributions atypical in the prior. Note that while
one has to do an honest integration over β to get
δ-function, the uncertainty at any ﬁxed β is very small (see Fig. 3).

(β) = 1. The actual relative errors of the
(a) Distributions from
βcl,
i
β(S) is almost a

S may be safely calculated as just

S2 and the error bars. Indeed, since

S
h

 300 
N

10000

 1000

 3000

 100 

  10 

  30 

P

P

b

c

class [13]. In the case of discrete distributions, smoothness is the absence of high peaks. Thus data with
faster decaying Zipf plots (plots of bins’ occupancy vs. occupancy rank r) are rougher. Since
)
}
produces Zipf plots like nr = a(β, N )
b(β) ln r, any distribution with nr decaying faster (rougher) or
slower (smoother) than this cannot be explained well with the same βcl for different N . So we should
expect to see βcl growing (falling) for qualitatively smoother (rougher) cases as N grows.

β(
{

−

qi

P

Figure 4(b) and Tbl. 1 illustrate these points. First, we study the
β = 0.02 distribution from Fig. 1. However, we added a 1000 extra bins,
each with qi = 0. Estimator performs remarkably well, and βcl does not
drift because the ranking law remains the same. Then we turn to the fa-
1/r, which
mous Zipf’s distribution, so common in Nature. It has nr
is qualitatively smoother than our prior allows. Correspondingly, we get
an upwards drift in βcl. Finally, we analyze a “rough” distribution, which
4(ln r)2, and βcl drifts downwards. Clearly, one would
has qr
want to predict the dependence βcl(N ) analytically, but this requires cal-
culation of predictive information (complexity) for the involved distribu-
tions [13] and is a work for the future. Notice that, the entropy estimator
for atypical cases is almost as good as for typical ones. A possible excep-
tion is the 100–1000 points for the Zipf distribution—they are about two
standard deviations off. We saw similar effects in some other “smooth” cases also. This may be another
manifestation of an observation made in Ref. [4]: smooth priors can easily adapt to rough distribution,
but there is a limit to the smoothness beyond which rough priors become inaccurate.

N 1/2 full Zipf rough
10−3
10−1
units
·
·
1907 16.8
10
0.99 11.5
30
0.86 12.9
100
8.3
1.36
300
6.4
2.24
1000
5.4
3.36
3000
4.5
4.89
10000
Table 1: βcl for solutions
shown on Fig. 4(b).

10−2
·
1.7
2.2
2.4
2.2
2.1
1.9
2.0

50

∝

∝

−

To summarize, an analysis of a priori statistics of common power–law Bayesian estimators unsur-
faced some very undesirable features in them. We are fortunate, however, that these minuses can be
easily turned into pluses, and the resulting estimator of entropy is precise, knows its own error, and gives
amazing results for a very large class of distributions.

7

References

[1] D. MacKay, Neural Comp. 4, 415–448 (1992).

[2] V. Balasubramanian, Neural Comp. 9, 349–368 (1997), adap-org/9601001.

[3] W. Bialek, C. Callan, and S. Strong, Phys. Rev. Lett. 77, 4693–4697 (1996), cond-mat/9607180.

[4]

I. Nemenman and W. Bialek, Advances in Neural Inf. Processing Systems 13, 287–293 (2001),

[5] J. Skilling, in Maximum entropy and Bayesian methods, J. Skilling ed. (Kluwer Academic Publ., Amsterdam,

cond-mat/0009165.

1989), pp. 45–52.

[6] D. Wolpert and D. Wolf, Phys. Rev. E, 52, 6841–6854 (1995), comp-gas/9403001.

[7]

I. Nemenman, Ph.D. Thesis, Princeton, (2000), ch. 3, physics/0009032.

[8] T. Schurmann and P. Grassberger, Chaos 6, 414–427 (1996).

[9] F. Willems, Y. Shtarkov, and T. Tjalkens, IEEE Trans. Inf. Thy., 41, 653–664 (1995).

[10] S. Strong et al., Phys. Rev. Lett. 80, 197–200 (1998), cond-mat/9603127.

[11] S. Panzeri and A. Treves, Network: Comput. in Neural Syst. 7, 87–107 (1996).

[12] S. Ma, J. Stat. Phys. 26, 221 (1981).

[13] W. Bialek, I. Nemenman, N. Tishby, physics/0007070.

8


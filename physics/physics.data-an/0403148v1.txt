4
0
0
2
 
r
a

M
 
1
3
 
 
]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[
 
 
1
v
8
4
1
3
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A Bayesian approach to change point analysis
of discrete time series

Ali Mohammad-Djafari and Olivier F´eron

Laboratoire des Signaux et Syst`emes,
Unit´e mixte de recherche 8506 (CNRS-Sup´elec-UPS)
Sup´elec, Plateau de Moulon, 91192 Gif-sur-Yvette, France
emails = djafari,feron@lss.supelec.fr

ABSTRACT

In this work we consider time series with a ﬁnite number of discrete point changes. We assume
that the data in each segment follows a diﬀerent probability density functions (pdf). We focus
on the case where the data in all segments are modeled by Gaussian probability density functions
with diﬀerent means, variances and correlation lengths. We put a prior law on the change point
instances (Poisson process) as well as on these diﬀerent parameters(conjugate priors) and give
the expression of the posterior probality distributions of these change points. The computations
are done by using an appropriate Markov Chain Monte Carlo (MCMC) technique.

The problem as we stated can also be considered as an unsupervised classiﬁcation and/or
segmentation of the time serie. This analogy gives us the possibility to propose alternative mod-
eling and computation of change points, which are more appropriate for multivariate signals,
for example in image processing.

key words: Bayesian change-points estimation, classiﬁcation and segmentation.

1. INTRODUCTION

Figure 1 shows typical change point problems we consider in this work. Note that, very often
people consider problems in which there is only one change point.1 Here we propose to consider
more general problems with any number of change points. However, very often the change point
analysis problems need online or real time detection algorithms,2–5 while here, we focus only on
oﬀ line methods where we assume that we have gathered all the data and we want to analyse
it to detect change points who have been occured during the observation time. Also, even if we
consider here change point estimation of 1-D time series, we can extend the proposed method to
multivariate data, for example the images where the change point problems become equivalent
to segmentation. One more point to position this work is that, very often the models used in
change point problems assume to know perfectly the model of the signal in each segment, i.e., a
linear or nonlinear regression model,5–9 while here, we use a probabilistic model for the signals
in each segment which gives probably more generality and applicability when we do not know
perfectly those models.
∗Correspondence: E-mail: djafari@lss.supelec.fr

Same variances, different means

Same means, different variances

Same means and variances, different correlation lengths

different pdfs (uniform, Gaussian, Gamma

t0

t1

t2

tn

t0+T

change points

Figure 1. Change point problems description: In the ﬁrst row, only mean values of the diﬀerent
segments are diﬀerent. In the second row, only variances are changed. In the third row only the
correlation strengths are changed. In the ﬁfth row, the whole nature shape of their probability
distribution have been changed. The last row show the change points tn.

More speciﬁcally, we model the time series by a hierarchical Gauss-Markov modeling with
hidden varaibles which are themselves modeled by a Markov model. Though, in each segment
which corresponds to a particular value of the hidden variable, the time series is assumed to
be modeled by a stationnary Gauss-Markov model. However, we choosed a simple parametric
model deﬁned only with three parameters of mean µ, variance σ2 = 1/τ and a parameter ρ
measuring the local correlation strength of the neighboring samples.

The choice of the hidden variable is also important. We have studied three diﬀerent modeling:
i) change point time instants tn, ii) classiﬁcation labels zn or iii) a Bernouilli variable qn which
is always equal to zero except when a change point occurs.

The rest of the paper is organized as follows: In the next section we introduce the notations
and ﬁxe the objectives of the paper. In section 3 we consider the model with explicite change
point times as the hidden variables and propose particular modeling for them and an MCMC
algorithm to compute their a posteriori probabilities.
In sections 4 and 5 we consider the
two other aformentionned models. Finally, we show some simulation results and present our
conclusions and perspectives.

2. NOTATIONS AND MODELING
We note by x = [x(t0), · · · , x(t0 + T )]′ the vector containing the data observed from time t0 to
t0 + T . We note by t = [t1, · · · , tN ]′ the unknown change points and note x = [x0, x1, · · · , xN ]′
where xn = [x(tn), x(tn + 1), · · · , x(tn+1)]′, n = 0, · · · , N represent the data samples in each
segment. In the following we will have tN +1 = T .

We model the data xn = [x(tn), x(tn + 1), · · · , x(tn+1)]′, n = 0, · · · , N in each segment by

a Gauss-Markov chain:

p(x(tn)) = N (µn, σ2
n)

p(x(tn + l)|x(tn + l − 1)) = N (ρn x(tn + l − 1) + (1 − ρn)µn, σ2

n(1 − ρ2

n)),

with

ln = tn+1 − tn + 1 = dim [xn]

l = 1, · · · , ln − 1
(1)

Then we have

p(xn) = p(x(tn))

p(x(tn + l)|x(tn + l − 1))

ln

Yl=1
1
2σ2
n

N

n=0
X

p(xn) ∝ exp

−

(x(tn) − µn)2

(cid:26)

(

exp

−

1
n(1 − ρ2

n))

2(σ2

(cid:27)

ln

Xl=1

p(xn) = N (µn1, Σn) with Σn = σ2

n Toeplitz([1, ρn, ρ2

n, · · · , ρln

n ])

[x(tn + l) − ρnx(tn + l − 1) − (1 − ρn)µn]2

)

(2)

Noting by t = [t1, · · · , tN ] the vector of the change points and assuming that the samples

from any two segments are independent, we can write:

p(x|t, θ, N) =

N (µn1, Σn) =

(xn − µn1)′Σ−1

n (xn − µn1)

N

n=0
Y

N

|Σn|−1/2
(2π)(ln/2)

 

n=0
Y

exp

−

!

(

N

n=0
X

where we noted θ = {µn, σn, ρn, n = 0, · · · , N}.

Note that

− ln p(x|t, θ, N) =

(ln/2) ln(2π) +

ln |Σn| −

(xn − µn1)′Σ−1

n (xn − µn1)

(4)

1
2

N

1
2

n=0
X

and when the data are i.i.d., (Σn = σnI) this becomes

− ln p(x|t, θ, N) = (T /2) ln(2π) +

(ln/2) ln σ2

n −

k(xn − µn1)k2
2σ2
n

N

n=0
X

Then, the inference problems we will be faced are the following:

1
2

N

n=0
X

N

n=0
X

)
(3)

(5)

1. Infer on θ given x and t;

2. Infer on t given x and θ;

3. Infer on t and θ given x;

4. Infer on θ given x.

5. Infer on t given x;

It is clear that the ﬁrst problem is the easiest.

The classical maximum likelihood estimation (MLE) approach can handle only the ﬁrst three

problems by maximizing p(x|t, θ), respectively, with respect to θ, to t and jointly (

t,

θ):

• Estimating θ given x and t:

θ = arg maxθ {p(x|t, θ)}

b

b

• Estimating t given x and θ:

t = arg maxt {p(x|t, θ)}
b

• Estimating t and θ given x:

θ) = arg max(t,θ) {p(x|t, θ)}

t,
(
b

However, we must be careful to check the boundedness of the likelihood function before using
any optimization algorithm. The optimization with respect to θ when t is known can be done
easily, but the optimization with respect to t is very hard and computationally costly.

b

b

The two last problems cannot be handled easily because they need to deﬁne the likelihood
fuctions p(x|θ) and p(x|t) which need integrations with respect to t or θ of p(x|t, θ). There
may not be possible to ﬁnd analytical expressions for these integrals which may even not exist.

3. BAYESIAN ESTIMATION OF THE CHANGE POINT TIME
INSTANTS

In Bayesian approach, one assigns prior probability laws on both t and θ and use the posterior
probability law p(t, θ|x) as a tool for doing any inference. Choosing a prior pdf for t is also
usual in classical approach. A simple model is the following:

tn = tn−1 + ǫn with ǫn ∼ P(λ),

where εn are assumed iid end λ is the a priori mean value of time intervals (tn − tn−1). if N is
the number of changepoint we can take λ = T

p(t|λ) =
ln p(t|λ) = −(N + 1)λ + ln(λ)

N +1
n=1 P(tn − tn−1|λ) =

Q

N +1

N +1. With this modeling we have :
n=1 e−λ λ(tn−tn−1)
(tn−tn−1)!
N +1
n=1 (tn − tn−1) −
Q

N +1
n=1 ln((tn − tn−1)!)

With this prior selection, we have

P

P

p(x, t|θ, N) = p(x|t, θ, N) p(t|λ, N)

(6)

(7)

(8)

and

p(t|x, θ, N) ∝ p(x|t, θ, N) p(t|λ, N)

(9)

In Bayesian approach, one goes one step further with assigning prior probability laws to the

hyperparameters θ, i.e., p(θ) and then one writes the joint a posteriori :

p(t, θ|x, λ, N) ∝ p(x|t, θ, N) p(t|λ, N) p(θ|N)

(10)

where here we noted θ = {µn, σ2

n, ρn, n = 1, · · · , N}.
To go further in details, we need to assign p(θ).The following is our selection:

p(µn) = N (µ0, σ2
0)
p(σ2
n) = IG(α0, β0)
p(ρn) = U([0, 1])

which correspond mainely to the conjugate or reference priors.

Given all these, we propose the following Gibbs MCMC algorithm:

Iterate until convergency
. sample t
. sample θn :
µn
σ2
n
ρn

using p(t|x, θ, N)

using p(µn|x, t, N)
using p(σ2
n|x, t, N)
using p(ρn|x, t, N)

3.1. Sampling t using p(t|x, θ, N )
P. Fearnhead showed10 that it is possible to perform perfect simulation of p(t|x, θ, N) when
we have assumed that segments of data separated by a changepoint tn are independant. This
simulation can be obtained by a method based on recursion on the changepoints. An approxi-
mation of this method is possible to obtain an algorithm whose computational cost is linear in
the number of observations. The main principle of this algorithm is to compute the following
probabilities :
Let note xt:s = [x(t), x(t + 1), . . . , x(s)], and

R(t, s|λ) = p(xt:s|t, s in the same segment, λ)

Q(t|λ) = p(xt:s| changepoint at t − 1, λ), Q(1) = p(x|λ)

Let also note F (t|λ) the associated cumulative distribution function of the prior density P(tn −
tn−1|λ) which is deﬁned by (7).
We compute R(t, s|λ) with the following relation :

R(t, s)|λ) =

p(xt:s|θ, λ)p(θ)dθ

Z

The computation of Q(t|λ) can be done recursively by the following result : for t = 1, . . . , T ,

Q(t|λ) =

R(t, s|λ)Q(s + 1|λ)P(s + 1 − t|λ) + R(t, T |λ)(1 − F (T − t|λ)),

T −1

s=t
X

This result is shown by P. Fearnhead10 . And he also demonstrates that the posterior distribution
of tn given tn−1 is

p(tn|tn−1, x, λ) =

R(tn−1, tn|λ)Q(tn + 1|λ)P(tn − tn−1|λ)
Q(tn−1|λ)

and the posterior distribution of no further changepoint is given by

p(tn = T |tn−1, x, λ) =

R(tn−1, T |λ)(1 − F (T − tn−1 − 1|λ))
Q(tn−1|λ)

3.2. Sampling θn using p(θn|x, t, N )

We may note that, thanks to the conjugacy, we have:

p(µn|x, t) = N (

µn,

σ2
n) with

p(σ2

b
b
n|x, t) = IG(
βn) with
αn,

+ 1′Σ−1

n xn
−1

i





(cid:26)

µn =

µ0
σ2
n
σ2
0
h
1′Σ−1
n 1 + 1
σ2
n =
σ2
b
b
0
(cid:16)
αn = α0 + ln
2
b
βn = β0 + 1
b
b

(cid:17)
2(xn − µn1)′R−1

n (xn − µn1),

b
b
n, · · · , ρln
where Rn = Toeplitz([1, ρn, ρ2

n ]). Then the simulation of these densities is quite simple.

p(ρn|x, t) is not a classical law. Its expression is given by :

p(ρn|x, t, N) =

p(ρn|xn, t, N)

N

n=0
Y

∝

∝

1
n(1 − ρ2
σ2
n)
1
n(1 − ρ2
σ2
n)

(cid:18)

(cid:18)

ln
2

(cid:19)

ln
2

(cid:19)

exp

−

(cid:26)

(

exp

−

1
n(1 − ρ2
n)
1
n(1 − ρ2
n)

2σ2

2σ2

ln

Xl=1

(xn − µn1)′R−1

n (xn − µn1)

(cid:27)

(x(tn + l) − ρnx(tn + l − 1) − (1 − ρn)µn)2

)

Then we can not sample easily this density.
The solution we propose is to use, in this step, a Hastings-Metropolis algorithm for sampling
this density. As an instrumental density we propose to use a Gaussian approximation of the
posterior density, i.e., we estimate the mean mρn and the variance σ2
ρn of p(ρn|x, t, N) and
we use a Gaussian law N (mρn, σ2
ρn) to obtain a sample. This sample is accepted or rejected

following p(ρn|x, t, N). In practice we compute mρn and σ2
their deﬁnition :

ρn calculating by approximation of

mρn −→

ρn

p(ρn|x, t, N)

σ2
ρn −→

ρ2
n

p(ρn|x, t, N) − m2
ρn

1

0
Z

1

0
Z

4. OTHER FORMULATIONS

Other formulation can also exist. We introduce two sets of hidden variables
z = [z(t0), · · · , z(t0 + T )]′ and q = [q(t0), · · · , q(t0 + T )]′

where

q(t) =

(cid:26)

1 if z(t) 6= z(t − 1)
0 elsewhere

1 if t = tn, n = 0, · · · , N
0 elsewhere

.

(11)

=

(cid:26)

and where z(t) takes an integer value k in each segment : k = 1, . . . , N + 1. With these two
related hidden variables, we can propose two other modeling to be used in change point analysis.
For example, q can be modeled by a Bernouilli process

P (Q = q) = λ

j qj (1 − λ)

j(1−qj ) = λ

j qj (1 − λ)N −

j qj

P

P

P

P

and z can be modeled by a Mrkov chain, i.e., {z(t), t = 1, · · · , T } forms a Markov chain:

P (z(t) = k) = pk,
P (z(t) = k|z(t − 1) = l) = pkl, with

k = 1, · · · , K,

k pkl = 1.

These two models are related. In the ﬁrst one, λ plays the role of the mean value of the segment
lengths and in the second pk and pkl give more precise control of the segment lengths. In the
multivariate case, or more precisely in bivariate case (image processing), q may represent the
contours and z the labels for the regions in the image. Then, we may also give a Markov model
for them. For example, if we note by r ∈ S the position of a pixel, S the set of pixels positions
and by V(r) the set of pixels in the neighorhood of the pixel position r, we may use an Ising
model for q

P

or a Potts model for z:

P (Q = q) ∝ exp

−ρ

δ(z(r) − z(s))

r∈S
X

Xs∈V(r)






P (z) ∝ exp

−ρ

δ(z(r) − z(s))

.




Xr∈S

Xs∈V(r)









where rho in the ﬁrst controls the mean lengths of the contours in the image and in the second
the mean surface of the regions in the image. Other more complexe modelings are also possible.





With these auxiliary variables, we can write

p(x|z, θ) =

P (zj = n)N (µn1, Σn) =

pkN (µn1, Σn)

(15)

N

n=1
X

N

n=1
X

if we choose K = N. Here, θ = {N, {µn, σn, pn, n = 1, · · · , N} , (pkl, k, l = 1, · · · , N)} and the
model is a mixture of Gaussians.

We can again assign appropriate prior law on θ and give the expression of p(z, θ|x) and do

any inference on z, θ.

(12)

(13)

(14)

Finally, we can also use q as the auxiliary variable and write

p(x|q, θ) = (2π)−N/2

N

 

n=1
Y
+ (2π)−(T −N )/2

N

1/σn

exp

−

!

(

(x(tn) − µn)2

)

1
2σ2
n

N

n=1
X

1/σ(ln−1)
n

exp

−

!

(

 

n=1
Y

N

= (2π)−T /2

1/σ(ln)
n

exp

−

!

(

1
2σ2
n

 

n=1
Y

j=1
X

(cid:2)

T

1
2σ2
n

T

(1 − qj) (xj − xj−1)2

j=1
X
(1 − qj) (xj − xj−1)2 + qj (xj − µn)

)

)
(cid:3)
(16)

and again assign appropriate prior law on θ and give the expression of p(q, θ|x) and do any
inference on q, θ. We are still working on using these auxiliary hidden variables particularly for
applications in data fusion in image processing and we will report on these works very soon.

5. SIMULATION RESULTS

To test the feasability and to mesaure the performances of the proposed algorithms, we generated
a few simple cases corresponding to only changes of one of the three parameters µn, σ2
n and ρn.
In each case we present the data, the histogram of the a posteriori samples of t during the ﬁrst
and the last iterations of the MCMC algorithm. For each case we also give the value of the
parameters used to simulate the data, the estimated values when the changepoints are known
and the estimated values by the proposed method.

5.1. Change of the means

We can see in ﬁgure 2 that we obtain precise results on the position of the changepoints. In
the case of change of means, the algorithm is very fast to converge to the good solution. In fact
it needs only few iterations (about 5). The main cause of this results is the importance of the
means in the likelihood p(x|t, θ, N).
We can also see in table 1 that the estimations of the means are very precise, particularly when
the size of the segment is long.

Different means

50th iteration

First iteration

Change points

t0

t1

t2

t3

t4

t0+T

Figure 2. Change in the means. up to down : simulated data, histogram in the 50th iteration,
histogram in the ﬁrst iteration, real position of the changepoints.

m ˆm|x, t
1.4966
1.5
1.7084
1.7
1.4912
1.5
1.6940
1.7
1.9012
1.9

ˆm|x
1.4969
1.7013
1.5015
1.6929
1.8915

Table 1. Estimated value of the means

5.2. Change in the variances

We can see in ﬁgure 3 that we have again good results on the position of the changepoints.
However, for little diﬀerence of variances, the algorithm give an uncertainty on the exact position
of the changepoint. This can be justiﬁed by the fact that the simulated data give itself this
uncertainty.
In table 2 we can see again good estimations on the variances on each segments.

Different variances

50th iteration

First iteration

Change points

t0

t1

t2

t3

t4

t0+T

Figure 3. Change in the variances. up to down : simulated data, histogram in the 50th
iteration, histogram in the ﬁrst iteration, real position of the changepoints.

σ2
0.01
1
0.001
0.1
0.01

ˆσ2|x, t
0.0083
0.9918
0.0007
0.0945
0.0079

ˆσ2|x
0.0081
0.9598
0.0026
0.0940
0.0107

Table 2. Estimated value of the variances

5.3. Change in the correlation coeﬃcient

The results showed in ﬁgure 4 are worse than in the two ﬁrst cases. The position of the
changepoints are less precise, and we can see that another changepoint appears. This aﬀects the
estimation of the correlation coeﬃcient in the third segment because the algorithm alternates
between two positions of changepoint. This problem can be justiﬁed by the fact that a value of
the correlation coeﬃcient near 1 implies locally a change of the mean, which can be considered
by the algorithm as a changepoint. Also this problem appears when the size of the segments
are far from the a priori size λ.

Different correlation coefficient

t0

t1

t2

t3

t4

t0+T

Figure 4. Change in the correlation coeﬃcient. up to down : simulated data, histogram in the
50th iteration, histogram in the ﬁrst iteration, real position of the changepoints.

50th iteration

First iteration

Change points

a
0
0.9
0.1
0.8
0.2

ˆa|x
0.0988
0.7875
0.3737
0.8071
0.1710

Table 3. Estimated vaue of the correlation coeﬃcients

5.4. Inﬂuence of the prior law

In this section we study the inﬂuence of the a priori on λ, i.e., the size of the segments. In the
following we ﬁx the number of changepoints as before and we change the a priori size of the
segments by λ0 = λ
2 and λ1 = 2λ. We apply then our algorithm on the change of the correlation
coeﬃcient.

Different correlation coefficient

50th iteration

First iteration

Change points

t0

t1

t2

t3

t4

t0+T

Figure 5. Diﬀerent correlation coeﬃcient with λ0 = 1
T
N +1. up to down : simulated data,
2
histogram in the 50th iteration, histogram in the ﬁrst iteration, real position of the changepoints.

In ﬁgure 5, we can see that the algorithm has detected other changepoints, forming segments
whose size is near λ0. This result shows the importance of the a priori when the data are not
enough signiﬁcant. We can also see this conclusion in ﬁgure 6 where only three changepoints
are detected, forming segments whose size is again near λ1. We can also remark that ﬁxing a
priori a size λ comes down to ﬁx the number of changepoints. Our algorithm give then good
results for instance if we have a good a priori on the number of changepoints.

6. CONCLUSIONS

REFERENCES

1. M. Basseville, “Detecting changes in signals and systems – a survey,” Automatica, vol. 24, no. 3,

pp. 309–326, 1988.

2. M. Wax, “Detection and localization of multiple sources via the stochastic signals model,” IEEE

Transactions on Signal Processing, vol. 39, pp. 2450–2456, November 1991.

3. J. J. Kormylo and J. M. Mendel, “Maximum-likelihood detection and estimation of Bernoulli-
Gaussian processes,” IEEE Transactions on Information Theory, vol. 28, pp. 482–488, 1982.

Different correlation coefficient

50th iteration

First iteration

Change points

t0

t1

t2

t3

t4

t0+T

Figure 6. Diﬀerent correlation coeﬃcient with λ1 = 2 T
N +1. up to down : simulated data,
histogram in the 50th iteration, histogram in the ﬁrst iteration, real position of the changepoints.

4. C. Y. Chi, J. Goustias, and J. M. Mendel, “A fast maximum-likelihood estimation and detection
algorithm for Bernoulli-Gaussian processes,” in Proceedings of the International Conference on
Acoustic, Speech and Signal Processing, (Tampa, fl), pp. 1297–1300, April 1985.

5. J. K. Goutsias and J. M. Mendel, “Optimal simultaneous detection and estimation of ﬁltered
discrete semi-Markov chains,” IEEE Transactions on Information Theory, vol. 34, pp. 551–568,
1988.

6. J. J. Oliver, R. A. Baxter, and C. S. Wallace, “Unsupervised Learning using MML,” in Ma-
chine Learning: Proceedings of the Thirteenth International Conference (ICML 96), pp. 364–372,
Morgan Kaufmann Publishers, 1996.

7. J. P. Hughes, P. Guttorp, and S. P. Charles, “A non-homogeneous hidden Markov model for

precipitation occurrence,” Applied Statistics, vol. 48, no. 1, pp. 15–30, 1999.

8. L. J. Fitzgibbon, L. , and D. L. Dowe, “Minimum message length grouping of ordered data,”
in Algorithmic Learning Theory, 11th International Conference, ALT 2000, Sydney, Australia,
December 2000, Proceedings, vol. 1968, pp. 56–70, Springer, Berlin, 2000.

9. L. Fitzgibbon, D. L. Dowe, and L. Allison, “Change-point estimation using new minimum mes-
sage length approximations,” in Proceedings of the Seventh Paciﬁc Rim International Conference
on Artiﬁcial Intelligence (PRICAI-2002) (M. Ishizuka and A. Sattar, eds.), vol. 2417 of LNAI,
(Berlin), pp. 244–254, Japanese Society for Artiﬁcial Intelligence (JSAI), Springer-Verlag, August
2002.

10. P. Fearnhead, “Exact and eﬃcient bayesian inference for multiple changepoint problems,” tech.

rep., Department of math. and stat., Lancaster university.


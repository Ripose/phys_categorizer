1
0
0
2
 
p
e
S
 
8
1
 
 
]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[
 
 
1
v
3
4
0
9
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Optimal use of time dependent probability density data to

extract potential energy surfaces

MPI f¨ur Quantenoptik, Hans-Kopfermann Str. 1, 85748 Garching, Germany

Department of Chemistry, Princeton University, Princeton, New Jersey 08544-1009, USA

MPI f¨ur Quantenoptik, Hans-Kopfermann Str. 1, 85748 Garching, Germany

Lukas Kurtz

Herschel Rabitz∗

Regina de Vivie-Riedle†

(September 18, 2001)

Abstract

A novel algorithm was recently presented to utilize emerging time dependent

probability density data to extract molecular potential energy surfaces. This

paper builds on the previous work and seeks to enhance the capabilities of the

extraction algorithm: An improved method of removing the generally ill-posed

nature of the inverse problem is introduced via an extended Tikhonov reg-

ularization and methods for choosing the optimal regularization parameters

are discussed. Several ways to incorporate multiple data sets are investigated,

including the means to optimally combine data from many experiments ex-

ploring diﬀerent portions of the potential. In addition, results are presented

on the stability of the inversion procedure, including the optimal combina-

∗hrabitz@princeton.edu

†rdv@mpq.mpg.de

1

tion scheme, under the inﬂuence of data noise. The method is applied to the

simulated inversion of a double well system to illustrate the various points.

Typeset using REVTEX

2

I. INTRODUCTION

To fully understand chemical dynamics phenomena it is necessary to know the underly-

ing potential energy surfaces (PES) [1]. Surfaces can be obtained by two means: ab initio

calculations [2–6] and the inversion of suitable laboratory data [7–14]. This paper is con-

cerned with an emerging class of laboratory data [15–17] with special features for inversion

purposes. Traditional sources of laboratory data for inversion produce an indirect route

to the potential requiring the solution of Schr¨odinger’s equation [18] in the process. An

alternative suggestion [19,20] has been put forth to utilize ultrafast probability density data

from diﬀraction observations or other means [21–26] to extract adiabatic potential surfaces.

Such data consists of the absolute square of the wavefunction. Although the phase of the

overall wavefunction is not available, there is suﬃcient information in this data to extract

the potential fully quantum mechanically without the solution of Schr¨odinger’s equation.

Instead, the proposed procedure rigorously reformulates the inversion algorithm as a linear

integral equation utilizing Ehrenfest’s theorem [27] for the position operator. Additional

attractive features of this algorithm are (a) the procedure may be operated non-iteratively,

(b) no knowledge is required of the molecular excitation process leading to the data and (c)

the regions where the potential may be reliably extracted are automatically revealed by the

data.

Extensive eﬀorts are under way to achieve the necessary temporal and spatial resolution

of the probability density data necessary for inversion processes as well as for other applica-

tions [20]. In anticipation of these developments a number of algorithmic challenges require

attention to provide the means to invert such data. This paper aims to build on the previous

work [19] and address some of these needs. In particular this paper will consider (i) optimal

choices for regularizing the inversion procedure, (ii) incorporation of multiple data sets and

(iii) inclusion of data sampled at discrete time intervals. These concepts are developed and

illustrated for the simulated inversion of a double well potential.

The paper is organized as follows. The basic inversion procedure and the model system

3

are given in Section II. Based on the inversion algorithm derived in Ref. [19] an extended

regularization procedure is presented in Section III followed by a discussion of a modiﬁed

time integration scheme applicable to diﬀerent types of experimental data sampling. This

development naturally leads to consideration of an optimal combination of data from diﬀer-

ent measurements. A proof on how to optimally combine the data is given in Appendix A.

The stability of this data combination procedure under the inﬂuence of noise is discussed as

well. Section V summarizes the ﬁndings of this paper.

II. THE BASIC INVERSION PROCEDURE AND THE MODEL SYSTEM

The algorithms developed in this paper will be illustrated for a one-dimensional system

but the generalization to higher dimensions is straightforward [28]: the major diﬀerence with

higher dimensions is the additional computational eﬀort involved. Atomic units are used

throughout this work.

For a system whose dynamics is governed by the Schr¨odinger equation

the time evolution of the average position obeys Ehrenfest’s theorem

i

∂
∂t

ψ(x, t) =

−

(cid:20)

1
2m

∂2
∂x2 + V (x)

ψ(x, t)

(cid:21)

0 = m

x ρ(x, t) dx +

u(x) ρ(x, t) dx ,

d2
dt2

Z

Z

(1)

(2)

where u(x) = dV (x)/dx and ρ(x, t) = |ψ(x, t)|2. In this work the probability density ρ(x, t)

is assumed to be observed in the laboratory and the goal is to determine the potential energy

surface (PES) V (x) from the gradient u(x).

Following [19], Eq.(2) can be used to construct a Gaussian least squares minimization

problem to determine the PES gradient u(x)

J0{u(x)} =

u(x) ρ(x, t) dx + m

x ρ(x, t) dx

dt .

(3)

T

1
T

Z0 (cid:20)Z

d2
dt2

Z

2

(cid:21)

The time averaging acts as a ﬁltering process to increase inversion reliability by gathering

together more data and reducing the ill-posedness of the problem. This increase in reliability

4

is in principle only limited by the exploratory ability of the wavepacket. Beyond some point

in time little information on the potential may be gained by taking further temporal data

starting from any potential initial condition.

Variation with respect to u(x) results in a Fredholm integral equation of the ﬁrst kind

(4)

(5)

(6)

(7)

δJ0{u(x)}
δu(x)

= 0 ⇒

A(x′, x) u(x′) dx′ = b(x)

Z

with righthand side (RHS)

and symmetric, positive semideﬁnite kernel

b(x) = −

ρ(x, t)

x′ ρ(x′, t) dx′ dt

m
T

T

Z0

d2
dt2

Z

A(x′, x) =

ρ(x′, t)ρ(x, t) dt .

1
T

T

Z0

Treated as an inverse problem, Eq.(4) produces the desired PES gradient u(x) as its solution.

For numerical implementation we resort to the matrix version and its formal solution

A · u ∆x = b ⇒ u = A−1 · b ∆x−1; .

Here the integral in Eq.(4) is evaluated at points of equal spacing ∆x.

This approach to seeking the PES has a number of attractive features [19]. The formu-

lation requires no knowledge of any preparatory steps to produce a speciﬁc ψ(x, 0) which

evolves freely to produce ρ(x, t). The generation of A(x, x′) and b(x) depends only on ρ(x, t)

and begins when the observation process is started. Moreover, although this is a fully quan-

tum mechanical treatment there is no need to solve Schr¨odinger’s equation to extract the

PES. The dominant entries of A(x, x′) and b(x) automatically reveal the portions of the

PES that may be reliably extracted. The linear nature of Eq.(4) is very attractive from a

practical perspective.

Notwithstanding these attractions, a principal problem to manage is the generally sin-

gular nature of the kernel of the integral equation in Eq.(4). The wavepacket can only

5

explore a portion of the PES, and it is not possible to continuously monitor the wavepacket

with arbitrary accuracy. Hence the A-matrix will always have a nontrivial nullspace. The

resulting solution u(x) will only be reliable in regions where ρ(x, t) has signiﬁcant mag-

nitude during its evolution. The inversion procedure can manage the null space with the

help of a suitable regularization procedure. Singular value decomposition and iterative so-

lution schemes are available (cf. [29,30] for an overview), but here we will employ extended

Tikhonov regularization (see Section III).

The procedures developed in this paper are applied to a simulated inversion with a system

taken to have a slightly asymmetric double well potential [31]

V (x) =

(x − q0) +

(x − q0)2(x + q0)2 + ∆

∆
2q0

ˆV − ∆/2
q4
0

with parameters

q0 = 1.0

∆ = 0.000 257 (asymmetry)

ˆV = 0.006 25

(barrier height) .

In the work of N. Doˇsli´c et al. [31] this PES represents a one dimensional model for the

intramolecular proton transfer in substituted malonaldehyde (see Fig. 1). The particle mass

is accordingly that of hydrogen.

The wavepacket propagations to obtain the simulated ρ(x, t) data employed the split

operator method (cf. [32,33]). For propagation as well as inversion we used a grid with

8192 points over the range −4.0 6 x 6 4.0. A time step ∆tprop = 3 was chosen and

total propagation time was T = 1200. The small values of ∆tprop and ∆xprop ensured good

convergence of the numerical propagation procedure.

The initial wavefunctions were normalized Gaussian wavepackets of width σ = 0.05.

As stated earlier, the inversion algorithm requires no knowledge of how these packets were

formed, but generally one may assume that a suitable external laser ﬁeld was applied for

6

(8)

(9)

(10)

(11)

times t < 0. The initial packets were placed at the left (L) and right minimum (R) of the

PES, on top of the barrier (T), and at a location high on the potential (H). The wavepacket

positions are illustrated in Fig. 1 and their exact values, the associated average energies

and the classical turning points at these energies are given table I. The inversion process

employed a time step and grid spacing that diﬀered from those used in the propagation,

as high spatial and temporal resolution is diﬃcult to attain in the laboratory. Hence, we

employed only a portion of all the available propagation data ρ(x, t) in time and space. We

will present inversion results using every 16th propagation grid point (i.e., ∆x = 16 · ∆xprop)

and every ﬁfth available snapshot (i.e., ∆t = 5 · ∆tprop); even fewer snapshots could be used

over a longer period of time with the criterion that roughly the same total amount of data

is retained. The inversion results from these lower resolution data are very encouraging.

The kernel matrices A for condition H and T are shown in Fig. 2; similar plots apply to the

cases L and R. The kernels are symmetric with values covering a large dynamic range from

∼ 103 down to 10−8 on the plotted domain. Signiﬁcant entries are found predominantly on

the matrix diagonal, close to the origin of the wavepacket, and also in the vicinity of the

classical turning points. Beyond the classical turning points at a distance of approximately

±2.0 the kernel values fall oﬀ very rapidly.

The features of the kernel in Fig. 2 coincide with the nature of the inverse problem

mentioned earlier: symmetry, ill-posedness, and automatic identiﬁcation of the range where

the PES may be be reliably extractable (i.e., where the kernel entries are large). For con-

ﬁguration H the relevant range is −2 . x . 2 and for conﬁguration T only the vicinity

of the barrier top should yield reliable PES information. In both cases we cannot expect

reasonable solutions beyond ±2.0, which coincides with the classical turning points given in

table I.

7

III. AN IMPROVED REGULARIZATION PROCEDURE

Tikhonov regularization [34] is straightforward to implement with simple control provided

by suitable weight parameters. It provides a well deﬁned means to stabilize the inversion

and extract reliable PES information in those regions allowed by the data.

This investigation goes beyond the initial work [19] to carefully explore various regu-

larization options. Regularization has the goal of improving the accuracy of the solution,

assuring stability and ease of use including computational simplicity. The functional J0 was

augmented by a regularization term involving a set of increasingly higher order diﬀerential

operators acting on u(x)

J1{u(x)} = J0{u(x)} +

ανξ−1

N

ν=0
X

ξ
Z (cid:20)(cid:18)

d
dx

(cid:19)

ν

2

u(x)

dx ,

(cid:21)

(12)

with real coeﬃcients αν > 0 and a reference length ξ. In practice ξ may be thought of as

the spatial resolution of the data and in the present numerical simulation it was taken as

∆x. For a multidimensional system, ξ and αν will become direction dependent tensors. The

parameter ξ acts to ensure that all the new terms added to J0 have the same units as [u]2

as well as permits comparison of the roles of the dimensionless regularization parameters αν

for diﬀerent ν and diﬀerent grid spacings ∆x.

The previous work [19] did not employ a reference length as only the ν = 0 regularization

term was considered. The parameter α0 penalizes the value of u(x). The new terms go

beyond and impose extra pressure on the gradient (ν = 1), the curvature (ν = 2) of u(x),

etc. .

Variation of J1 with respect to u(x) yields the modiﬁed inversion prescription

A(x′, x) + δ(x − x′) ·

ανξ−1

u(x′) dx′ = b(x) .

(13)

Z "

N

ν=0
X

−ξ2 d2
dx′2
(cid:18)

ν

#

(cid:19)

The sum added to J0 in Eq.(12) for regularization consisted of purely positive terms with

derivatives of up to Nth order, resulting in an alternating series of only even derivatives up

to order 2N in Eq.(13). Moreover, the Fredholm integral equation of the ﬁrst kind has been

8

transformed into an integro-diﬀerential equation for u(x) with the added terms dominating

in the regions where kernel is singular.

Due to the rapid growth in the order of the derivatives it is often suﬃcient to set N = 2,

i.e., retaining standard, gradient, and curvature Tikhonov regularization. For numerical

application Eq.(13) may be transformed into the matrix problem

A + α0∆x−2 11 − α1 D + α2(∆x)2 Q

· u ∆x = b ,

(14)

employing the unit matrix 11, as well as the second

(cid:2)

(cid:3)

0

−2

1

1 −2
. . .

1
. . .

. . .

1 −2

1

0

1 −2















,















D =

1
(∆x)2

(15)

(16)

and the forth order diﬀerentiation band matrices

Q =

1
(∆x)4

6 −4

1

−4

6 −4

0

1

· · · 0
...

1 −4

6 −4

0

0
...

1 −4
. . .
. . .

6 −4
. . .
. . .

1
. . .

0
. . .

. . .

0

1















.















These are simple diﬀerencing expressions for the derivatives involved. Higher order expres-

sions for the derivatives could be considered, but ﬁnite data resolution and laboratory noise

will generally not warrant or support the added complexity.

To investigate the inverse solution’s dependence on the various regularization parameters

in Eq.(14) several parameter scans for all four conﬁgurations L, T, R, H were performed for

diﬀerent resolutions ∆x and combinations of αν-parameters. For the discussion in this paper,

we selected typical results for the situation of H with ∆x = 16∆xprop. The curves in Fig. 3

show the solution defect |∆u| and the system defect |∆s| as deﬁned below in Eqs.(17) and

9

(20). While only |∆s| is an experimentally accessible ﬁgure of merit, an investigation of

|∆u| here allows for quantifying the quality of the inverse solution. For both error measures

reported the plots are generated for each αν independently while the others are kept zero.

( uexact(x) − u(x) )2 dx

.

(17)

Figures 3a and 3b display the solution defect

|∆u| =

1
xb − xa



xb

Zxa

1/2






Figure 3a is computed with xa = −2.0, xb = 2.0 (i.e.

the central domain indicated in

Fig. 2 and table I within which the inversion is expected to be valid) and Fig. 3b with

xa = −4.0, xb = 4.0 (i.e., the full simulation range). The diﬀerences between the two cases

are striking. The corresponding solution defects show a completely diﬀerent shape with

minima that diﬀer by several orders of magnitude in αν. In Fig. 3b the magnitude of the

error in the active domain −2 . x . 2 is overestimated. This behavior in Fig. 3b is due

to large deviations between the exact gradient and the inversion solution for the gradient,

which cannot be recovered reliably in the domain’s outer limits. Thus we conclude that |∆u|

scans should only be computed over the regions actually reached to a signiﬁcant degree by

the wavepacket (cf., Fig. 3a) to achieve reliable estimates of the inversion quality.

The latter point is illustrated in Figs. 4b and 4c with the inverted results for u(x) ane

V (x) with pure α1 regularization of conﬁgurations H/H1 where α1 is given in table II. The

two cases H/H1 diﬀer in the domain employed in the inversion (i.e., the active domain for

H and the full domain for H1) and in the choice of optimal α1 determined according to the

|∆u| scans. Thus we further conclude that the inversion process should be conﬁned to the

active domain to maintain stability.

To ﬁnd suitable integration regions from the laboratory data the normalized lefthand

and righthand variance

σ2
ℓ (t) =

(x − hxi)2ρ(x, t) dx

2ρ(x, t) dx

hxi

Z−∞

∞

Zhxi

,

Z−∞

hxi

∞

,

Zhxi

10

σ2
r (t) =

(x − hxi)2ρ(x, t) dx

2ρ(x, t) dx

(18)

(19)

of the position operator can be helpful. Together with the position average hxi they can

provide an estimate for the PES domain predominantly covered by the wavepacket motion.

We present all three quantities (hx(t)i and σℓ(t), σr(t) as grey shaded regions) in Fig. 4a.

The results clearly show that for conﬁguration H the range −2 . x . 2 is suitable. For

conﬁgurations L, T, R an even smaller range is best (cf., table II).

All the computations revealed that a gradient Tikhonov regularization based on α1 per-

forms better than the standard regularization based on α0 utilized earlier [19]. There is

some additional improvement in choosing the curvature regularization α2, but we found it

to be less stable for coarse grids, which will be the standard situation in actual application.

We also found little improvement in mixing the diﬀerent regularization schemes.

In

general the αν regularization with the largest errors masks the positive eﬀects of the others.

Hence for all cases of the PES reconstruction we utilized only α1 regularization (cf., the

inversion in Figs. 4a and 4b with the optimal parameters given in table II).

As a measure of inversion quality and the role of regularization, we desire a quantity

that is strictly available from the laboratory data ρ(x, t). A good choice is the system defect

|∆s| deﬁned by the norm of satisfying the system equation (4) with the inverse solution u(x)

found via Eq. (13)

|∆s| =

b(x) −

A(x, x′)u(x′) dx′

1
L

"

Z (cid:18)

Z

2

1/2

dx

#

(cid:19)

.

(20)

The values of |∆s| will depend on the regularization parameters αν. Weak regularization will

produce a small value of |∆s|, but likely artiﬁcial structures in the PES. Over regularization

will result in a smooth PES, that is systematically in error with diminished inﬂuence from

the kernel A(x, x′) on the inverse solution. The best choice for the αν is generally where

|∆s| has risen and leveled oﬀ in a stable region as shown in Fig. 3c. The ﬁgure shows that

|∆s| naturally tends to zero as αν → 0+ and monotonically rises until it reaches a plateau.

There is very good agreement between the values of αν which show good results for |∆u|

in Fig. 3a and the stable regularization region identiﬁed in Fig. 3c. Thus |∆s| should be of

practical utility in assigning regularization parameter values.

11

The generally self-similar structures in Figs. 3a and 3c suggest that every regularization

operator has a roughly similar eﬀect. This added robustness is also attractive for practical

application if it holds up regardless of the system.

IV. COMBINING DISTINCT SETS OF LABORATORY DATA

Sections IV A and IV B will cover diﬀerent approaches to combining distinct sets of

laboratory density data. Finally Section IV C will explore the impact of data noise on the

inversion.

A. Optimal combination of experimental data

The functional J0{u(x)} in its original form in Eq.(3) is expressed in terms of a uniform,

continuous time integration of observed ρ(x, t) data. However, experimental circumstances

including measurements at discrete snapshots in time or changes in the quality of data

sampling may necessitate employing a weight function ω(t) for a generalized approach to

the time integration in the functional J0. Thus we deﬁne ˆJ0 as

ˆJ0{u(x)} =

u(x) ρ(x, t) dx + m

x ρ(x, t) dx

ω(t) dt .

(21)

∞

Z0 (cid:20)Z

d2
dt2

Z

2

(cid:21)

The choice ω(t) = [ Θ(t) − Θ(t − T ) ]/T , with Θ being the Heaviside step function, will

reduce ˆJ0 to J0.

Variation of Eq.(21) leads to a modiﬁed inverse problem

δ ˆJ0{u(x)}
δu(x)

= 0 ⇒

ˆA(x′, x) u(x′) dx′ = ˆb(x) ,

with the new kernel

and RHS

ˆA(x′, x) =

ρ(x′, t)ρ(x, t) ω(t) dt

Z

∞

Z0

12

(22)

(23)

ˆb(x) = −m

ω(t) ρ(x, t)

x′ ρ(x′, t) dx′ dt .

(24)

T

Z0

d2
dt2

Z

The weight ω(t) does not alter the regularization terms in Eq.(13). If ˆb(x) is rewritten using

partial integration over time, then the weight function must be considered in this process.

The above equations were applied to two generic cases. First, we considered data gath-

ered as snapshots in time i.e., ω(t) =

T
j=1 δ(tj − t), and evaluated Eqs. (23) and (24) with

this weight. This procedure simply reduced all time integrations to sums over the sampled

P

ρ data. Next, we considered the case in which the measurement process has been divided

into two continuous time intervals of length T1 and T2 separated by a period of time τ . A

reasonable choice of weights would either be

ω(t) =

Θ(t) − Θ(t − T1)
T1 + T2

+

Θ(t − τ − T1) − Θ(t − τ − T1 − T2)
T1 + T2

.

or

ω(t) =

Θ(t) − Θ(t − T1)
T1

+

Θ(t − τ − T1) − Θ(t − τ − T1 − T2)
T2

.

The choice depends on the desired emphasis to be given to the two data intervals. Here

we chose to give the longer interval a larger contribution in ˆA(x, x′) than the shorter one,

and this can be better achieved with using Eq.(25); this choice is reasonable, provided the

measured data ρ(x, t) in both intervals are of comparable quality. Clearly many other issues

can be incorporated into the choice of ω(t) dictated by what is known about the nature of

(25)

(26)

the data and the information sought about the PES.

The kernel is now

ˆA(x′, x) =

1

T1 + T2 

T1

T1+τ +T2

+

ZT1+τ





Z0



and the RHS reads

ρ(x′, t)ρ(x, t) dt

(27)

ˆb(x) = −

m

T1 + T2 

T1

T1+τ +T2

+

ZT1+τ

Z0






13

d2
dt2

Z

ρ(x, t)

x′ ρ(x′, t) dx′ dt .

(28)

The interpretation of the weight in Eq.(25) is associated with performance of the inversion

with an interrupted gathering of data from a single experiment. To explore this point further

it is useful to rewrite Eqs.(27) and (28) as

[ A1(x, x′) + A2(x, x′) ] u(x′) dx′ = b1(x) + b2(x) ,

(29)

Z

where the indices “1” and “2” denote the evident two data time domains.

In this form

the gathering of data from one interrupted experiment can also be interpreted as ﬁnding

the simultaneous solution to the inverse problem of two diﬀerent experiments. These two

experiments could possibly be prepared with distinct controls could, for example, explore

diﬀerent regions of the PES.

We found that it is optimal to simply combine these sets of data by addition as indicated

in Eq.(29). This procedure will yield an inverse solution u0(x) with accuracy greater than a

linear combination u(x) = µu1(x) + νu2(x) of separate solutions to the individual problems

“1” and “2” as explained below.

spective system equation

Consider two experiments that yield two diﬀerent inverse solutions satisfying their re-

A1,2(x, x′)u1,2(x′) dx′ = b1,2(x) .

(30)

Z

Naturally there should be only a unique exact uex.(x) for the physical system. Hence both

system solutions u1,2 in Eq.(30) can be decomposed into the exact solution and contamina-

tion pieces from the kernel’s nullspace

u1,2(x) = uex.(x) + a1,2(x) + r1,2(x) .

(31)

The functions a1,2 and r1,2 are associated with the nullspace of the two kernels with a1,2(x) ∈

ker(A1) ∩ ker(A2) being the contamination from the common nullspace of A1 and A2 and

r1,2(x) the residual contribution unique to the respective kernel. The goal is to use the data

to ﬁnd an optimal solution u0(x) with the smallest possible nullspace contribution.

14

Exploiting the linearity of the inverse problem, we may add the two pieces of Eq.(30) to

get

A1(x, x′)u1(x′) dx′ +

A2(x, x′)u2(x′) dx′ = b1(x) + b2(x) .

(32)

Z

Z

This doesn’t fully satisfy Eq.(29) and it is in general not possible to construct the optimal

solution u0(x) as a linear combination u0(x) = µ u1(x) + ν u2(x) with constant coeﬃcients

µ, ν. To elucidate this point, we insert u0(x) into Eq.(29) and with the help of Eqs.(30)

and (31) we get the cross terms

A1(x, x′)u2(x′) dx′ = b1(x) +

A1(x, x′)r2(x′) dx′ = b1(x) + 1ε2(x)

Z

Z

Z

Z

A2(x, x′)u1(x′) dx′ = b2(x) +

A2(x, x′)r1(x′) dx′ = b2(x) + 2ε1(x) ,

(33)

where the prefactors µ, ν have been omitted. Hence u0(x) is not an optimal solution of

Eq.(29) since it leaves errors iεj(x) that cannot be eliminated. However, by employing

Eq. (29) and adding the kernels and RHSs we can improve the quality of the inversion.

No error terms like iεj(x) will appear since by construction the resulting u0(x) can be

decomposed as u0(x) = u(x) + a0(x). A contribution from r0(x) as in Eq.(31) will not arise,

as proved in Appendix A. Thus, the solution of the combined problem will gain in quality

by virtue of the reduced nullspace of the new kernel A1 + A2.

These optimality results are rigorous but it must be added that in general any combina-

tion of a ﬁnite amount of data will not fully eliminate the nullspace. However in the cases

under comparison here the assumption that a similar degree of robustness can be attained

certainly holds true.

As argued above, we chose the weighting function in Eq.(25) to result in observation-

duration proportional entries in A1(x, x′) and A2(x, x′). Hence it is quite natural to add A =

A1 + A2. However, choosing the approach Eq.(26) normalizes each data set independently.

This logic naturally leads to considering the optimal combination of data to form A =

σA1 + δA2 where σ and δ are positive constants. This specially weighted form, or a positive

deﬁnite combination A = (1 − β)A1 + βA2 with β ∈ (0, 1), might be useful especially in the

15

presence of diﬀerent degrees of noise in the two data sets. An iterative numerical scheme to

optimize β could then help to improve the solution by minimizing the eﬀects of nullspace

contamination.

The optimal combination of data by addition of kernels Ai(x, x′) and RHSs bi(x) pre-

sented above was applied to the double well system with results for the gradient u(x) and

PES V (x) shown in Fig. 5. Information was successively added to the kernel A(x, x′) by

combining the data sets to form LT, LTR, and LTRH with the notation based on the initial

conditions shown in Fig. 1. In each case all conﬁgurations are weighted equally. The optimal

α1 values employed and defect measures are given in table II.

While the individual inverse problem solutions based on L, T, R, and H reproduce the

potential in their respective neighborhoods quite well, they fail to give adequate results

for the other portions of the potential. On the other hand, the reconstruction of large

parts of the PES is successful if we optimally combine the data of the three experiments

LTR. However, contrary to intuition, we observe that the solution is less satisfactory from

combining all the data LTRH; some additional oscillations appear along with a dip in the

vicinity of the initial wavepacket for H. Apparently the nullspace of the expanded domain

cannot be fully managed by α1 regularization alone; no attempt was made to simultaneously

introduce α0 and α2 regularization.

B. Other combinations of data

Several other schemes for combining the raw density data can be envisioned, apart from

the approach in Section IV A. One candidate would be the direct combination of ρ(x, t)

data from diﬀerent experiments. As an illustration we will treat the case of two diﬀerent ρ’s

with

ρ(x, t) = ρ1(x, t) + ερ2(x, t)

(34)

16

and ε being a positive constant. This combination is physically acceptable, as Ehrenfest’s

theorem in Eq.(2) is linear in the probability density. Insertion of this sum into the functional

J0{u(x)} and variation with respect to u(x) will yield a formulation analogous to the one

describing inversion under the inﬂuence of noise in the data (see Section IV C) in Eq.(38)

upon comparison of Eqs.(36) and (34).

The terms proportional to ε0 and ε2 will exactly correspond to what was found earlier

in Eq.(29). However, the terms proportional to ε represent a cross correlation between

ρ1 and ρ2. These cross terms can be signiﬁcant, and they act to introduce an element

of undesirable structure, often oscillatory, in the equations determining u(x). On physical

grounds it is also artiﬁcial to directly correlate the independent experimental data ρ1 and

ρ2 when seeking u(x).

Hence, the scheme of adding together the bare ρ-data is expected to produce unreliable

results. To support this argument we present a test on such a ρ-combination consisting of

the sum of all four densities of the initial conﬁgurations L, T, R, and H

ρΣ(x, t) = ρL(x, t) + ρT (x, t) + ρR(x, t) + ρH(x, t) .

(35)

The corresponding inverted gradient and PES respectively are shown in Figs. 5a and 5b.

The solution is rather poor and far worse than the LTRH combination using the same data.

This result should not be taken to construe that other combinations of data might not give

satisfactory results. However, the combination of Ai and bi in Section IV A is quite natural

and produces excellent inversion results.

C. The inﬂuence of noise on the inversion

Any real ρ-data will always be contaminated by some degree of noise. In an additive

model this noise contaminated data ρn(x, t) can be represented as

ρn(x, t) = ρ(x, t) + εγ(x, t) ,

(36)

17

where ε > 0 is a ordering parameter and the noise is described by the spatio-temporal

function γ(x, t). We assume that γ(x, t) is a randomly varying function with vanishing

average contribution and free from systematic error such that

1
T

T

Z0

γ(x, t)σ(x, t) dt

T →∞−→ 0

(37)

for any function σ(x, t) of bounded norm over time that is not correlated with γ(x, t).

Inserting the ansatz in Eq.(36) into the functional J0{u(x)} in Eq.(3) and taking the

ﬁrst variation, the equation determining u(x) is obtained

ρ(x′, t)ρ(x, t) dt u(x′) dx′ +

ρ(x′, t)γ(x, t) dt u(x′) dx′

γ(x′, t)ρ(x, t) dt u(x′) dx′ +

γ(x′, t)γ(x, t) dt u(x′) dx′

T

ε
T

T

Z

Z0

ε2
T

Z

Z0

= −

ρ(x, t)

x′ρ(x′, t) dx′ dt − ε

ρ(x, t)

x′γ(x′, t) dx′ dt

d2
dt2

Z

d2
dt2

Z

m
T

T

Z0

T

Z0

d2
dt2

Z

d2
dt2

Z

γ(x, t)

x′ρ(x′, t) dx′ dt − ε2 m
T

γ(x, t)

x′γ(x′, t) dx′ dt .

(38)

The terms proportional to ε0 recover the original unperturbed system in Eqs.(4-6). Assuming

the data noise level to be small, the terms in ε2 on both sides of Eq.(38) can be neglected.

We ﬁrst turn to the kernel side of Eq.(38) and denote all terms in ε1 as the error kernel

T

Z

Z0

T

1
T

+

ε
T

Z

Z0

T

m
T

Z0

T

−ε

m
T

Z0

δA(x, x′)

δA(x, x′) =

γ(x′, t)ρ(x, t) dt +

ρ(x′, t)γ(x, t) dt .

(39)

ε
T

T

Z0

ε
T

T

Z0

Each term involves the computation of two-point spatial correlations between functions.

However, the functions γ and ρ are uncorrelated, and the temporal integral of their product

is expected to result in only small random contributions to the kernel over x and x′, especially

for longer time integration as follows from Eq. (37). Following similar logic, the terms

proportional to ε1 on the RHS of Eq.(38) should be negligible, especially for long time

18

integration. Neglecting the ε2 terms ﬁnally leaves only the ﬁrst term proportional to ε0 on

the RHS.

Hence, the functional J0 exhibits some inherent capability to deal with slightly noisy

data. The time integration process averages out these noise eﬀects so that they should have

a decreasing impact on the inverse solution u(x). Longer periods of temporal data should

make their behavior better.

These results are also in accordance with the stability analysis presented in [19]. Resort-

ing to the matrix version of the inverse problem (cf., Eq.(14)) the authors proved (Eq.(25)

in Ref. [19]) that the relative error in the solution u after regularization is bounded by the

relative errors in the data δb and δA.

Moreover it was found (Eqs.(41) and (49) [19]) that small perturbations in the noise εγ

will result in small proportional perturbations in b and A, which is excellent behavior for

any application with ﬁnite time integration. These results can now be extended to the long

time integration limit where the ε1 terms in Eq.(39) should further diminish in signiﬁcance

for T → ∞. Similar arguments apply to the RHS b [35].

Equation (39) also demonstrates why the direct combination of bare ρ(x, t) data dis-

cussed in Section IV B performs less satisfactory than the optimal combination scheme in

Section IV A. In contrast to the slightly perturbed system cross term δA(x, x′) above, the

analogous term arising from directly combining the ρ data will not vanish. This will in-

troduce an undesirable error contribution to the inverse problem. In contrast, the optimal

combination scheme for diﬀerent sets of data in Section IV A should proﬁt from the inherent

stability of the inversion procedure to deal with slightly noisy systems since this technique

involves a sequence of separate time integrations.

V. SUMMARY AND OUTLOOK

This paper presented new results that improve and extend a recently suggested pro-

cedure [19] to extract potential energy surfaces (PES) from the emerging experimentally

19

observable probability density |ψ(x, t)|2 data. The results of this paper should also be

applicable to the more general case of extracting the dipole function from the additional

observation of the applied laser electric ﬁeld [20].

An easy to implement regularization scheme was introduced, which increases the ac-

curacy of the computed PES without loss of numerical stability. Furthermore an optimal

reconstruction method was presented which combines data from diﬀerent measurements.

This scheme was argued to be optimal in the sense of reducing the nullspace of the inverse

problem and hence increasing the domain of the extracted PES. Evidence was presented that

this scheme is stable under the inﬂuence of noise, but further investigations will be necessary

to fully conﬁrm these results. We hope that the developments in this paper stimulate the

generation of appropriate probability density data for inversion implementation.

ACKNOWLEDGMENTS

The authors would like to acknowledge Karsten Sundermann who shared interest in

this subject from its inception. RdVR thanks “Fonds der Chemischen Industrie” and HR

would like to acknowledge the Department of Energy. LK acknowledges DFG’s ﬁnancial

support through the project “SPP Femtosekundenspektroskopie”. He also would like to

thank Angelika Hofmann for the propagation code and Jens Schneider as well as Berthold-

Georg Englert for discussions.

APPENDIX A: OPTIMALITY PROOF

This section presents the lemma and its proof underlying the optimal combination of

data from diﬀerent measurements.

Lemma 1 Given two Hermitian, positive semideﬁnite operators A1, A2 : H → H acting on

the Hilbert space H and their sum A = µA1 + νA2 with coeﬃcients µ, ν ∈ R > 0, it then

holds that

20

ker(A) = ker(A1) ∩ ker(A2) .

For ﬁnite dimensional ranges this implies that

rank(A) = rank(A1) + rank(A2) − dim ( Range(A1) ∩ Range(A2) ) .

In other words: Adding two positive semideﬁnite, Hermitian operators will reduce the

nullspace of the combined operator to that of the intersection of both nullspaces. The

generalization to a ﬁnite sum of operators A =

N
k=1 αkAk with constant αk > 0 is evident.

Neither positivity nor Hermiticity can be omitted. Without the former criterion, a

counter example is A2 = −A1, with µ = ν = 1. As an example, without the latter criterion,

P

the two R3×3 operators

1 1 1

0 1 1



0 0 0

1 0 0



A1 = 











with ranks 3, 2, and 1 lead to the contradiction 1







0 0 1

1 1 0

, A2 = 






⇒ A1 + A2 = 






!
= 3 + 2 − 2.

1 1 1

1 1 1

1 1 1









Proof: As both operators A1 and A2 are Hermitian, they have diagonal representations

with respect to their eigenvectors A1|λ1,ii = λ1,i |λ1,ii and A2|λ2,ji = λ2,j |λ2,ji. Without

loss of generality we choose the normalized eigenvectors {|λ1,ii} as the basis of H.

Clearly, H can be decomposed in the following two ways into orthogonal subspaces

and also

H = ker(A1) ⊕ Range(A1)

H = ker(A2) ⊕ Range(A2) .

In a similar fashion we can partition the spectrum of A1, and hence H’s basis, into all

eigenvectors that form a basis of Range(A1) and those that generate ker(A1). Since H is

21

(A1)

(A2)

(A3)

a complete linear space and A1, A2, A are linear operators, it is suﬃcient to consider the

basis states only. For any such state |λ1,ii we ﬁnd

hλ1,i|A|λ1,ii = µhλ1,i|A1|λ1,ii + νhλ1,i|A2|λ1,ii

= µλ1,i + νΛi ,

(A4)

where we deﬁne the mean Λi = hλ1,i|A2|λ1,ii =

j |hλ2,j|λ1,ii|2 λ2,j ≥ 0. This quantity is

always positive (or zero) by virtue of A2 being positive semideﬁnite.

P

In accordance with the decomposition in Eqs.(A2) and (A3) four diﬀerent cases are to

be distinguished:

|λ1,ii ∈ Range(A1) : 



|λ1,ii ∈ ker(A1) : 


|λ1,ii /∈ ker(A2) ⇒ hλ1,i|A|λ1,ii = µλ1,i + νΛi > 0

|λ1,ii ∈ ker(A2) ⇒ hλ1,i|A|λ1,ii = µλ1,i + 0 > 0

|λ1,ii /∈ ker(A2) ⇒ hλ1,i|A|λ1,ii = 0 + νΛi > 0

|λ1,ii ∈ ker(A2) ⇒ hλ1,i|A|λ1,ii = 0 + 0

(A5)

Therefore only (basis) vectors that lie in both nullspaces will belong to the nullspace of A,



which proves the ﬁrst part of the Lemma. The second part follows from the linear algebraic

dimension relation

dim ( Range(A1) + Range(A2) )

= rank(A1) + rank(A2) − dim ( Range(A1) ∩ Range(A2) ) ,

(A6)

where “+” on the lefthand side denotes all linear combinations of the vectors in both ranges.

Now, any vector that lies either in Range(A1) or in Range(A2) will, with an argument

similar to Eq.(A5), always be in Range(A). We are thus allowed to replace

rank(µA1 + νA2) = dim ( Range(A1) + Range(A2) ) ,

(A7)

which completes our proof.

The values of µ, ν > 0 are arbitrary, although often physical constraints may suggest

that some speciﬁc values may be better than others (see the discussion in Section IV A).

22

We note that the lemma’s ﬁrst part could have been proved without using a basis.

The decomposition Eq.(A2) and the diﬀerentiation of Eq.(A5) into |φi ∈ ker(A1) or |φi /∈

ker(A1) for any |φi ∈ H suﬃces. However, the second part of the lemma requires the basis

vectors.

23

REFERENCES

[1] R. D. Levine and R. B. Bernstein, Molecular Reaction Dynamics (Oxford Univ. Press,

New York, N.Y., 1974).

[2] M. Born and J. R. Oppenheimer, Annalen der Physik 84, 457 (1927).

[3] D. R. Hartree, Proceedings of the Cambridge Philosophical Society 24, 89 (1928).

[4] V. A. Fock, Zeitschrift f¨ur Physik 61, 126 (1930).

[5] V. A. Fock, Zeitschrift f¨ur Physik 62, 795 (1930).

[6] A. Szabo and N. S. Ostlund, Modern Quantum Chemistry: Introduction to Advanced

Electronic Structure Theory (Macmillan Publishing, New York, 1982).

[7] P. M. Morse, Physical Review 34, 57 (1929).

[8] R. B. Gerber, M. Shapiro, U. Buck, and J. Schleusener, Physical Review Letters 41,

[9] B. Lowe, M. Pilant, and W. Rundell, SIAM Journal on Mathematical Analysis 23, 482

236 (1978).

(1992).

[10] T.-S. Ho and H. Rabitz, Journal of Physical Chemistry 97, 13447 (1993).

[11] Quantum Inversion Theory and Applications, edited by H. V. von Geramb (Springer,

New York, N.Y., 1994).

[12] R. Fabiano, R. Knobel, and B. Lowe, IMA Journal or Numerical Analysis 15, 75 (1995).

[13] D. H. Zhang and J. C. Light, Journal of Chemical Physics 103, 9713 (1995).

[14] T. Ho, H. Rabitz, S. Choi, and M. Lester, Journal of Chemical Physics 104, 1187 (1996).

[15] A. Zewail, Journal of Physical Chemistry 97, 12427 (1993).

[16] M. Shapiro, Journal of Physical Chemistry 100, 7859 (1996).

24

[17] J. D. Geiser and P. M. Weber, Journal of Chemical Physics 108, 8004 (1998).

[18] E. Schr¨odinger, Annalen der Physik 79, 361 (1926).

[19] W. Zhu and H. Rabitz, Journal of Chemical Physics 111, 472 (1999).

[20] W. Zhu and H. Rabitz, Journal of Physical Chemistry A 103, 10187 (1999).

[21] Z. Vager, R. Naaman, and E. P. Kanter, Science 244, 426 (1989).

[22] K. Kwon and A. Moscowitz, Physical Review Letters 77, 1238 (1996).

[23] A. Assion et al., Physical Review A 54, R4605 (1996).

[24] J. C. Williamson et al., Nature 386, 159 (1997).

[25] J. L. Krause, K. J. Schafer, M. Ben-Nun, and K. R. Wilson, Physical Review Letters

79, 4978 (1997).

[26] R. R. Jones, Physical Review A 57, 446 (1998).

[27] P. Ehrenfest, Zeitschrift f¨ur Physik 45, 455 (1927).

[28] W. Zhu and H. Rabitz, Journal of Physical Chemistry B 104, 10863 (2000).

[29] A. K. Louis, GAMM-Mitteilungen 1, 5 (1990).

[30] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical Recipes in

C: The Art of Scientiﬁc Computing, 2nd ed. (Cambridge University Press, ADDRESS,

1994).

102, 9645 (1998).

(1982).

[31] N. Doˇsli´c, O. K¨uhn, J. Manz, and K. Sundermann, Journal of Physical Chemistry A

[32] M. D. Feit and J. A. Fleck, Applied Optics 17, 3990 (1978).

[33] M. D. Feit, J. A. Fleck, and A. Steiger, Journal of Computational Physics 47, 412

25

[34] A. N. Tikhonov and F. John, in Solutions of Ill-Posed Problems, Scripta Series in

Mathematics, edited by V. Arsenin (Winston, Washington, D.C., 1977).

[35] For b(x), a related issue pointed out in [19] is the stability of b(x) in view of the need to

take the second time derivative of the probability density. An approach based on partial

integration over time has been proposed calling for a ﬁrst order time derivative only.

However, a check of the inversion performance based on partial integration produced

unsatisfactory results. It will always be extremely diﬃcult to reliably compute the terms

−

ρ(x, t)

hx(t)i

m
T

d
dt

T

0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

at only a few ρ(x, t) snapshots in time. One inevitably needs to work with one-sided

derivatives at t = 0 and T , which signiﬁcantly diminishes the accuracy.

26

TABLE I. Characteristics of the initial wavepackets

Conﬁguration index

x0

hψ0|H|ψ0i

classical turning points

H

R

T

L

1.75

0.9977

0.0052

-1.002

0.081

0.055

0.061

0.054

left

-2.1563

-2.0013

-2.0403

-1.9996

right

2.1534

1.9978

2.0370

1.9961

The conﬁguration indices H, R, T, and K corresponding to the locations of wavepacket

initial positions are shown in Fig. 1. All wavepackets start with equal width σ = 0.05 and

are initially at rest centered at the respective starting position x0. The average energy of

each packet as well as the corresponding turning points of an equivalent classical particle of

the same energy are given.

27

Conﬁguration

α1

|∆u| × 10−3

|∆s| × 10−3

TABLE II. Inversion regularization information

3.3×10−5

1.0

0.033

0.007

0.033

0.333

0.01

0.01

100.0

xa

-4.0

-2.0

-1.5

-1.5

-1.5

-1.5

-1.5

-1.5

-1.5

xb

4.0

2.0

1.5

1.5

1.5

1.5

1.5

1.5

1.5

384.58

11.52

7.16

9.02

6.53

9.53

3.83

2.78

3.10

0.03

23.46

1.06

0.05

1.07

111.63

12.42

0.70

0.49

H1

H

R

T

L

Σ

LTRH

LTR

LT

In this numerical case study the optimal regularization parameter value α1 was identiﬁed

by scanning its eﬀect on the solution defect |∆u|. The inversion domains are xa 6 x 6 xb.

The system defect is |∆s|. The ﬁrst ﬁve rows apply to the individual PES reconstructions

shown in Fig. 4, and the last four rows refer to measurement combinations shown in Fig. 5.

See the text for details.

28

0.1

0.08

0.06

0.04

0.02

0

]
.
u
.
a
[
 
)
x
(
V

O

C

R

O

C

H

H

H

C

T

L

R

−4

−3

−2

−1

2

3

4

1

0
x [a.u.]

FIG. 1. The substituted malonaldehyde model system with its corresponding one dimensional

potential energy function as given in Eq.(8). L, T, R, H indicate the diﬀerent wavepacket initial

positions utilized for the simulated inversions.

29

(a)

(b)

-2

0
x [a.u.]

2

4

4

2

]
.
u
0
.
a
[
 
’
x

-2

2

]
.

.

u
0
a

[
 
’
x

-2

-4

-4

4

-4

-4

-2

0
x [a.u.]

2

4

FIG. 2. Contour plots of the kernel matrices A. (a) conﬁguration H and (b) conﬁguration T.

The numerical values for the matrix entries range from ∼ 103 on the diagonal to ∼ 10−8 on the

boundaries. The contour levels correspond to: 1 (outer line), 31, 61, . . . , 211.

30

α
α
α

0

1

2

]
.

u

.

a

[
 
|

2
.
.
2
−

u
∆

|

]
.
u
.
a
[
 
|
u
∆

|

]
.
u
.
a
[
 
|
s
∆

|

0.15

0.10

0.05

0.00
0.55

0.50

0.45

0.40

2

10

0

10
−2

10

10

10

10

−4

−6

−8

(a)

(b)

(c)

31

−8

10

−6

10

−4

10

10

0

10

2

10

4

10

6
10

−2
α

i

FIG. 3. αi parameter scans performed with conﬁguration H. Panels (a) and (b) display the

solution defect |∆u| with respect to two diﬀerent inversion ranges: −2 6 x 6 2 and −4 6 x 6 4,

respectively. Panel (c) shows the system defect |∆s| for the entire domain −4 6 x 6 4.

1200
600
0

]
.
u
.
a
[
 
t

1200
600
0

]
.
u
.
a
[
 
t

H/H1

R

T

L

−3

−2

−1

0
< X (t) > [a.u.]

1

2

3

4

(a)

]
.
u
.
a
[
 
t

1200
600
0

]
.
u
.
a
[
 
t

1200
600
0
−4

2

(b)

]
.
u
.
a
[
 
)
x
(
u

]
.

u
a

.

[
 
)
x
(
V

1.6

1.2

0.8

0.4

0

−0.4

0.3

0.25

0.2

0.15

0.1

0.05

H1

H

R

T

L

H1

H

R

T

L

0
x [a.u.]

FIG. 4.

32

(c)

0

−4

−3

−2

−1

1

2

3

4

FIG. 4. Extractions of the potential under the conditions given in table II. (a) the time

evolution of the position average hx(t)i accompanied by the left- and righthand variance

(i.e., shaded regions bounded by Eqs.(18) and (19)) to indicate the regions predominantly

covered by the probability densities. The grey domains on the extreme left and right mark

classically forbidden areas (cf. table I).(b) the reconstructed u(x) and the corresponding

potential V (x) in (c) with a suitably chosen additive constant. For comparison the exact

solutions are included as dashed lines. The individual curves have been oﬀset for graphical

reasons and the detailed presentation of V (x) is restricted to |x| . 2.5 since the boundary

regions will not be extracted correctly due to lack of data sampling there.

33

1.6

1.2

0.8

0.4

0

−0.4

0.25

0.2

0.15

0.1

0.05

0

]
.
u
.
a
[
 
)
x
(
u

]
.
u
.
a
[
 
)
x
(
V

Σ

LTRH

LTR

LT

Σ

LTRH

LTR

LT

(a)

(b)

34

−4

−3

−2

−1

1

2

3

4

0
x [a.u.]

FIG. 5. Extraction of the PES for optimally combined (LT, LTR, and LTRH) as well as

ρ-combined data (Σ). See the text and table II for details. The curves for the derivative u(x)

in (a) and the PES in (b) have been oﬀset for graphical clarity and exact solutions (dashed lines)

added for comparison. For optimal combinations of the data the original and reconstructed PES

are almost indistinguishable.


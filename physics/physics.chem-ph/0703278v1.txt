7
0
0
2
 
r
a

M
 
0
3
 
 
]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[
 
 
1
v
8
7
2
3
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Dynamic and static limitation in reaction
networks, revisited

A. N. Gorban ∗

University of Leicester, UK

O. Radulescu

Universit´e de Rennes 1, France

Abstract

The concept of limiting step gives the limit simpliﬁcation: the whole network be-
haves as a single step. This is the most popular approach for model simpliﬁcation
in chemical kinetics. However, in its simplest form this idea is applicable only to the
simplest linear cycles in steady states. For such the simplest cycles the nonstation-
ary behaviour is also limited by a single step, but not the same step that limits the
stationary rate. In this paper, we develop a general theory of static and dynamic
limitation for all linear multiscale networks, not only for simple cycles. Our main
mathematical tools are auxiliary discrete dynamical systems on ﬁnite sets and spe-
cially developed algorithms of “cycles surgery” for reaction graphs. New estimates
of eigenvectors for diagonally dominant matrices are used.

Multiscale ensembles of reaction networks with well separated constants are in-
troduced and typical properties of such systems are studied. For any given ordering
of reaction rate constants the explicit approximation of steady state, relaxation
spectrum and related eigenvectors (“modes”) is presented. In particular, we proved
that for systems with well separated constants eigenvalues are real (damped oscil-
lations are improbable). For systems with modular structure, we propose to select
such modules that it is possible to solve the kinetic equation for every module in
the explicit form. All such “solvable” networks are described. The obtained multi-
scale approximations that we call “dominant systems” are computationally cheap
and robust. These dominant systems can be used for direct computation of steady
states and relaxation dynamics, especially when kinetic information is incomplete,
for design of experiments and mining of experimental data, and could serve as a
robust ﬁrst approximation in perturbation theory or for preconditioning.

∗ Corresponding author: Department of Mathematics, University of Leicester, LE1 7RH, UK
Email addresses: ag153@le.ac.uk (A. N. Gorban ), ovidiu.radulescu@univ-rennes1.fr

(O. Radulescu).

Preprint submitted to Elsevier

20 February 2014

1 Introduction

Which approach to model reduction is the most important? Population is not the ultimate
judge, and popularity is not a scientiﬁc criterion, but “Vox populi, vox Dei,” especially
in the epoch of citation indexes, impact factors and bibliometrics. Let us ask Google. It
gave on 31st December 2006:

for “quasi-equilibrium” – 301000 links;
for “quasi steady state” 347000 and for “pseudo steady state” 76200, 423000 together;
for our favorite “slow manifold” [1,2] 29800 links only, and for “invariant manifold”
slightly more, 98100;
for such a framework topic as “singular perturbation” Google gave 361000 links;
for “model reduction” even more, as we did expect, 373000;
but for “limiting step” almost two times more – 714000!

•
•
•

•
•
•

Our goal is the general theory of static and dynamic limitation for multiscale networks.
The concept of limiting step gives, in some sense, the limit simpliﬁcation: the whole
network behaves as a single step. As the ﬁrst result of our paper we introduce further
speciﬁcation in this idea: the whole network behaves as a single step in statics, and
as another single step in dynamics, the stationary rate and the relaxation time to this
stationary rate are limited by diﬀerent reaction steps, and we describe how to ﬁnd these
steps.

The concept of limitation is very attractive both for theorists and experimentalists. It is
very useful to ﬁnd conditions when a selected reaction step becomes the limiting step.
We can change conditions and study the network experimentally, step by step. It is very
convenient to model a system with limiting steps: the model is extremely simple and can
serve as a very elementary building block for further study of more complex systems, a
typical situation both in industry and in systems biology.

In IUPAC Compendium of Chemical Terminology one can ﬁnd two articles with deﬁnition
of limitation [3,4]. Rate-determining step (rate-limiting step): “These terms are best re-
garded as synonymous with rate-controlling step.” “A rate-controlling (rate-determining
or rate-limiting) step in a reaction occurring by a composite reaction sequence is an ele-
mentary reaction the rate constant for which exerts a strong eﬀect – stronger than that
of any other rate constant – on the overall rate.”

It is not wise to object to a deﬁnition and here we do not object, but, rather, complement
the deﬁnition by additional comments. The main comment is that usually when people
are talking about limitation they expect signiﬁcantly more: there exists an elementary
reaction a rate constant for which exerts such a strong eﬀect on the overall rate that the
eﬀect of all other rate constants together is signiﬁcantly smaller. Of course, this is not yet
a formal deﬁnition, and should be complemented by a deﬁnition of “eﬀect”, for example,
by “control function” identiﬁed by derivatives [3] of the overall rate of reaction, or by
other overall rate “sensitivity parameters”.

2

For the IUPAC Compendium deﬁnition a rate-controlling step always exists, because
among the control functions generically exists the biggest one. For the notion of limitation
that is used in practice there exists a diﬀerence between systems with limitation and
systems without limitation.

An additional problem arises: are systems without limitation rare or should they be treated
equitably with limitation cases? The arguments in favor of limitation typicalness are as
follows: the real chemical networks are very multiscale with very diﬀerent constants and
concentrations. For such systems it is improbable to meet a situation with compatible
eﬀects of diﬀerent stages. Of course, these arguments are statistical and apply to generic
systems from special ensembles.

During last century, the concept of limiting step was revised several times. First simple
idea of a “narrow place” (a least conductive step) could be applied without adaptation
only to a simple cycle of irreversible steps that are of the ﬁrst order (see Chap. 16 of [5]
or the paper of R.K. Boyd [6]). When researchers try to apply this idea in more general
situations they meet various diﬃculties such as:

•

•

ES

Some reactions have to be “pseudomonomolecular.” Their constants depend on concen-
trations of outer components, and are constant only under condition that these outer
components are present in constant concentrations, or change suﬃciently slow. For ex-
E + P (E
ample, the simplest Michaelis–Menten enzymatic reaction is E + S
here stands for enzyme, S for substrate, and P for product), and the linear catalytic
cycle here is S
Even under ﬁxed outer components concentration, the simple “narrow place” behaviour
could be spoiled by branching or by reverse reactions. For such reaction systems deﬁni-
tion of a limiting step simply as a step with the smallest constant does not work. The
A1. Even if the constant of
simplest example is given by the cycle: A1 ↔
the last step A3 →
A1 is the smallest one, the stationary rate may be much smaller than
k3b (where b is the overall balance of concentrations, b = c1 + c2 + c3), if the constant
of the reverse reaction A2 →

S. Hence, in general we must consider nonlinear systems.

A1 is suﬃciently big.

A2 →

A3 →

ES

→

→

→

→

In a series of papers [7,8], D.B. Northrop clearly explained these diﬃculties with many
examples based on the isotope eﬀect analysis and suggested that the concept of rate–
limiting step is “outmoded”. Nevertheless, the main idea of limiting is so attractive that
Northrop’s arguments stimulated the search for modiﬁcation and improvement of the
main concept.

W.J. Ray (Jr.) [9] proposed to use the sensitivity analysis. He considered cycles of re-
versible reactions and suggested a deﬁnition: The rate–limiting step in a reaction sequence
is that forward step for which a change of its rate constant produces the largest eﬀect on
the overall rate. In his formal deﬁnition of sensitivity functions the reciprocal reaction rate
(1/W ) and rate constants (1/ki) were used (see [9] ) and the connection between forward
and reverse step constants (the equilibrium constant) was kept ﬁxed.

Ray’s approach was revised by G.C. Brown and C.E. Cooper [10] from the system control
analysis point of view (see [11]). They stress again that there is no unique rate–limiting
step speciﬁc for an enzyme, and this step, even if it exists, depends on substrate, product

3

and eﬀector concentrations. They demonstrated also that the control coeﬃcients

C W

ki =

ki
W

∂W
∂ki ![S],[P ],...

,

 

where W is the stationary reaction rate and ki are constants, are additive and obey the
summation theorems (as concentrations do). Simple relation between control coeﬃcients
of rate constants and intermediate concentrations was reported in [12]. This relation
connects two type of experiments: measurement of intermediate levels and steady–state
rate measurements.

For the analysis of nonlinear cycles the new concept of kinetic polynomial was developed
[13,14]. It was proven that the stationary state of the single-route reaction mechanism of
catalytic reaction can be described by a single polynomial equation for the reaction rate.
The roots of kinetic polynomial are the values of the reaction rate in the steady state. For
a system with limiting step the kinetic polynomial can be approximately solved and the
reaction rate found in the form of a series in powers of the limiting step constant [15].

In our approach, we analyze not only the steady state reaction rates, but also the re-
laxation dynamics of multiscale systems. We focused mostly on the case when all the
elementary processes have signiﬁcantly diﬀerent time scales. In this case, we obtain “limit
simpliﬁcation” of the model: all stationary states and relaxation processes could be ana-
lyzed “to the very end”, by straightforward computations, mostly analytically. Chemical
kinetics is an inexhaustible source of examples of multiscale systems for analysis. It is not
surprising that many ideas and methods for such analysis were ﬁrst invented for chemical
systems.

In Sec. 2 we analyze a simple example and the source of most generalizations, the cat-
alytic cycle, and demonstrate the main notions on this example. This analysis is quite
elementary, but includes many ideas elaborated in full in subsequent sections.

There exist several estimates for relaxation time in chemical reactions (for example, [16]),
but even for the simplest cycle with limitation the main property of relaxation time is not
widely known. For a simple irreversible catalytic cycle with limiting step the stationary
rate is controlled by the smallest constant, but the relaxation time is determined by the
second in order constant. Hence, if in the stationary rate experiments for that cycle we
mostly extract the smallest constant, in relaxation experiments another, the second in
order constant will be observed.

It is also proven that for cycles with well separated constants damped oscillations are
impossible, and spectrum of the matrix of kinetic coeﬃcients is real. For general reaction
networks with well separated constants this property is proven in Sec. 4.

Another general eﬀect observed for a cycle is robustness of stationary rate and relaxation
time. For multiscale systems with random constants, the standard deviation of constants
that determine stationary rate (the smallest constant for a cycle) or relaxation time (the
second in order constant) is approximately n times smaller than the standard deviation of
the individual constant (where n is the cycle length). Here we deal with the so-called “order

4

statistics”. This decrease of the deviation as n−
summation, where it decreases with increasing n as n−

1/2.

1 is much faster than for the standard error

In more general settings, robustness of the relaxation time was studied in [17] for chem-
ical kinetics models of genetic and signalling networks. We proved in [17] that for large
multiscale systems with hierarchical distribution of time scales the variance of the inverse
relaxation time (as well as the variance of the stationary rate) is much lower than the
variance of the separate constants. Moreover, it can tend to 0 faster than 1/n, where n
is the number of reactions. It was demonstrated that similar phenomena are valid in the
nonlinear case as well. As a numerical illustration we used a model of signalling network
that can be applied to important transcription factors such as NFkB.

Each multiscale system is characterized by its structure (the system of elementary pro-
cesses) and by the rate constants of these processes. To make any general statement about
such systems when the structure is given but the constants are unknown it is useful to
take the constant set as random and independent. But it is not obvious how to chose the
random distribution. The usual idea to take normal or uniform distribution meets obvious
diﬃculties, the time scales are not suﬃciently well separated.

Statistical approach to chemical kinetics was developed in [18,19] and high-dimensional
model representations (HDMR) were proposed as eﬃcient tools to provide a fully global
statistical analysis of a model. The work [20] was focused on how the network properties
are aﬀected by random rate constant changes. The rate constants were transformed to a
logarithmic scale to ensure an even distribution over the large space.

The log-uniform distribution on suﬃciently wide interval helps us to improve the situation,
min log k and β = max log k.
indeed, but a couple of extra parameters appears: α =
, β
We have to study the asymptotics α
. This approach could be formalized
by means of the uniform invariant distributions of log k on Rn. These distributions are
ﬁnite–additive, but not countable–additive (not σ-additive).

→ −∞

→ ∞

−

The probability and measure theory without countable additivity has a long history. In
Euclid’s time only arguments based on ﬁnite–additive properties of volume were legal.
Euclid meant by equal area the scissors congruent area. Two polyhedra are scissors–
congruent if one of them can be cut into ﬁnitely many polyhedral pieces which can be
re-assembled to yield the second. But all proofs of the formula for the volume of a pyramid
involve some form of limiting process. Hilbert asks in his third problem: are two Euclidean
polyhedra of the same volume scissors congruent? The answer is “no” (a review of old
and recent results is presented in [46]). There is another invariant of cutting and gluing
polyhedra.

Finite–additive invariant measures on non-compact groups were studied by G. Birkhoﬀ
[39] (see also [40], Chap. 4). The frequency–based Mises approach to probability foun-
dations [41], as well as logical foundations of probability by R. Carnap [42] do not need
the σ-additivity. Non-Kolmogorov probability theories are discussed now in the context
of quantum physics [44], nonstandard analysis [45] and many other problems (and we do
not pretend to provide here a full review of related works).

5

We answer the question: What does it mean “to pick a multiscale system at random”?
We introduce and analyze a notion of multiscale ensemble of reaction systems. These
ensembles with well separated variables are presented in Sec. 3.

The best geometric example that helps us to understand this problem is one of Lewis
Carroll’s Pillow Problems (published in 1883) [21]: “Three points are taken at random
on an inﬁnite plane. Find the chance of their being the vertices of an obtuse-angled
triangle.” (In an acute-angled triangle all angles are comparable, in an obtuse-angled
triangle the obtuse angle is bigger than others and could be much bigger.) The solution of
this problem depends signiﬁcantly on the ensemble deﬁnition. What does it mean “points
are taken at random on an inﬁnite plane”? Our intuition requires translation invariance,
but the normalized translation invariant measure on the plain could not be σ-additive.
Nevertheless, there exist ﬁnite–additive invariant measures.

Lewis Carroll proposed a solution that did not satisfy modern scientists. There exists a lot
of attempts to improve the problem statement [22,23,24,25]: reduction from inﬁnite plane
to a bounded set, to a compact symmetric space, etc. But the elimination of paradox
destroys the essence of Carroll’s problem. If we follow the paradox and try to give a
meaning to “points are taken at random on an inﬁnite plane” then we replace σ-additivity
of the probability measure by ﬁnite–additivity and come to the applied probability theory
for ﬁnite–additive probabilities. Of course, this theory for abstract probability spaces
would be too poor, and some additional geometric and algebraic structures are necessary
to build rich enough theory.

This is not just a beautiful geometrical problem, but rather an applied question about
proper deﬁnition of multiscale ensembles. We need such a deﬁnition to make any general
statement about multiscale systems, and brieﬂy analyze lessons of Carroll’s problem in
Sec. 3.

In this section we use some mathematics to deﬁne the multiscale ensembles with well
separated constants. This is necessary for background of the analysis of systems with
limitation, but technical consequences are rather simple. We need only two properties of
a typical system from multiscale ensemble with well separated constants:

(1) Every two reaction rate constants k, k′ are connected by relation k

k′ or k

k′

≫

≪

(with probability close to 1);

(2) The ﬁrst property persists, if we delete two constants k, k′ from the list of constants,
and add a number kk′ or a number k/k′ to that list (with probability close to 1).l

If the reader can use these properties (when it is necessary) without additiona clariﬁcation,
it is possible to skip reading Sec. 3 and go directly to more applied sections. In. Sec. 4 we
study static and dynamic properties of linear multiscale reaction networks. An important
instrument for that study is a hierarchy of auxiliary discrete dynamical system. Let Ai
Aj be edges (reactions), and kji be the
be nodes of the network (“components”), Ai →
constants of these reactions (please pay attention to the inverse order of subscripts). A
discrete dynamical system φ is a map that maps any node Ai in a node Aφ(i). To construct
a ﬁrst auxiliary dynamical system for a given network we ﬁnd for each Ai the maximal
kji for all j, and φ(i) = i if there are no reactions
constant of reactions Ai →

Aj: kφ(i)i ≥

6

Aj. Attractors in this discrete dynamical system are cycles and ﬁxed points.

Ai →
The fast stage of relaxation of a complex reaction network could be described as mass
transfer from nodes to correspondent attractors of auxiliary dynamical system and mass
distribution in the attractors. After that, a slower process of mass redistribution between
attractors should play a more important role. To study the next stage of relaxation,
we should glue cycles of the ﬁrst auxiliary system (each cycle transforms into a point),
deﬁne constants of the ﬁrst derivative network on this new set of nodes, construct for this
new network a (ﬁrst) auxiliary discrete dynamical system, etc. The process terminates
when we get a discrete dynamical system with one attractor. Then the inverse process of
cycle restoration and cutting starts. As a result, we create an explicit description of the
relaxation process in the reaction network, ﬁnd estimates of eigenvalues and eigenvectors
for the kinetic equation, and provide full analysis of steady states for systems with well
separated constants.

The problem of multiscale asymptotics of eigenvalues of non-selfadjoint matrices was
studied by Vishik, Ljusternik [27] and Lidskii [28]. Recently, some generalizations were
obtained by idempotent (min-plus) algebra methods [29]. These methods provide natural
language for discussion of some multiscale problems [30]. In the Vishik–Ljusternik–Lidskii
theorem and its generalizations the asymptotics of eigenvalues and eigenvectors for the
family of matrices

Aij(ǫ) = aijǫAij + o(ǫAij ) is studied for ǫ > 0, ǫ

→

0.

In the chemical reaction networks that we study, there is no small parameter ǫ with a
given distribution of the orders ǫAij of the matrix nodes. Instead of these powers of ǫ we
have orderings of rate constants. On the other hand, the matrices of kinetic equations
have some speciﬁc properties. The possibility to operate with the graph of reactions
(cycles surgery) signiﬁcantly helps in our constructions. Nevertheless, there exists some
similarity between these problems and, even for general matrices, graphical representation
is useful. The language of idempotent algebra [30], as well as nonstandard analysis with
inﬁnitisemals [31], can be used for description of the multiscale reaction networks, but
now we postpone this for later use.

A multiscale system where every two constants have very diﬀerent orders of magnitude is,
of course, an idealization. In parametric families of multiscale systems there could appear
systems with several constants of the same order. Hence, it is necessary to study eﬀects
that appear due to a group of constants of the same order in a multiscale network. The
system can have modular structure, with diﬀerent time scales in diﬀerent modules, but
without separation of times inside modules. We discuss systems with modular structure
in Sec. 5. The full theory of such systems is a challenge for future work, and here we study
structure of one module. The elementary modules have to be solvable. That means that
the kinetic equations could be solved in explicit analytical form. We give the necessary and
suﬃcient conditions for solvability of reaction networks. These conditions are presented
constructively, by algorithm of analysis of the reaction graph.

It is necessary to repeat our study for nonlinear networks. We discuss this problem and
perspective of its solution in conclusion. Here we again use the experience summarized
in the IUPAC Compendium [3] where the notion of controlling step is generalized onto
nonlinear elementary reaction by inclusion of some concentration into “pseudo-ﬁrst order

7

rate constant”.

2 Static and dynamic limitation in a simple catalytic cycle

2.1 General properties of a cycle

The catalytic cycle is one of the most important substructures that we study in reaction
networks. In the reduced form the catalytic cycle is a set of linear reactions:

A1 →

A2 →

. . . An →

A1.

Reduced form means that in reality some of these reaction are not monomolecular and
include some other components (not from the list A1, . . . An). But in the study of the
isolated cycle dynamics, concentrations of these components are taken as constant and
are included into kinetic constants of the cycle linear reactions.

we use the simpliﬁed notation ki because
For the constant of elementary reaction Ai →
the product of this elementary reaction is known, it is Ai+1 for i < n and A1 for i = n.
The elementary reaction rate is wi = kici, where ci is the concentration of Ai. The kinetic
equation is:

−
where by deﬁnition w0 = wn. In the stationary state ( ˙ci = 0), all the wi are equal: wi = w.
This common rate w we call the cycle stationary rate, and

˙ci = wi

wi,

1 −

(1)

(2)

w =

b
+ . . . 1
kn

1
k1

; ci =

w
ki

,

i ci is the conserved quantity for reactions in constant volume (for general
where b =
case of chemical kinetic equations see elsewhere, for example, [26]). The stationary rate
w (2) is a product of the arithmetic mean of concentrations, b/n, and the harmonic mean
of constants (inverse mean of inverse ki).

P

2.2 Static limitation in a cycle

If one of the constants, kmin, is much smaller than others (let it be kmin = kn), then

kn
ki

+ o

kn
ki !!

, ci = b

+ o

kn
ki

 

kn
ki !!

,

 

Xi<n

(3)

cn = b

1

 

w = knb

 

−

Xi<n
1 + O

Xi<n

 
kn
ki !!

,

 

Xi<n

8

or simply in linear approximation

cn = b

1

 

−

kn
ki !

Xi<n

, ci = b

, w = knb,

kn
ki

where we should keep the ﬁrst–order terms in cn in order not to violate the conservation
law.

The simplest zero order approximation for the steady state gives

cn = b, ci = 0 (i

= n).

This is trivial: all the concentration is collected at the starting point of the “narrow place”,
but may be useful as an origin point for various approximation procedures.

So, the stationary rate of a cycle is determined by the smallest constant, kmin, if kmin is
suﬃciently small:

In that case we say that the cycle has a limiting step with constant kmin.

w = kminb

if

kmin
ki ≪

1.

=kmin

Xki

2.3 Dynamical limitation in a cycle

If kn/ki is small for all i < n, then the kinetic behaviour of the cycle is extremely simple:
the coeﬃcients matrix on the right hand side of kinetic equation (1) has one simple zero
1 nonzero eigenvalues
eigenvalue that corresponds to the conservation law

ci = b and n

−

λi =

ki + δi (i < n).

P

−

0 when

where δi →
It is easy to demonstrate (7): let us exclude the conservation law (the zero eigenvalue)
i<n ci. In these

ci = b and use independent coordinates ci (i = 1, . . . n

1); cn = b

i<n

0.

P

kn
ki →

coordinates the kinetic equation (1) has the form
P

−

−

P

˙c = Kc

knAc + knbe1

−

where c is the vector–column with components ci (i < n), K is the lower triangle matrix
with nonzero elements only in two diagonals: (K)ii =
1), (K)i+1, i = ki
(i = 1, . . . n
2) (other elements are equal to zero); A is the matrix with nonzero elements
−
i = 0 for 1 < i < n).
only in the ﬁrst row: (A)1i ≡
After that, eq. (7) follows simply from continuous dependence of spectra on matrix.

1, e1 is the ﬁrst basis vector (e1

ki (i = 1, . . . n

1 = 1, e1

−

−

The relaxation time of a stable linear system (8) is, by deﬁnition, τ = [min
{
1, . . . n

1. For small kn,

]−

1

Re(

λi)

i =

−

|

−

}

−
In other words, kτ is the second slowest rate constant: kmin ≤

≈

τ

1/kτ , kτ = min
{

ki |

i = 1, . . . n

1

.

}
kτ ≤

... .

9

(4)

(5)

(6)

(7)

(8)

(9)

6
6
2.4 Relaxation equation for a cycle rate

A deﬁnition of the cycle rate is clear for steady states because stationary rates of all
elementary reactions in cycle coincide. There is no common deﬁnition of the cycle rate for
nonstationary regimes. In practice, one of steps is the step of product release (the “ﬁnal”
step of the catalytic transformation), and we can consider its rate as the rate of the cycle.
Formally, we can take any step and study relaxation of its rate to the common stationary
rate. The single relaxation time approximation gives for rate wi of any step:

˙wi = kτ (kminb

wi); wi(t) = kminb + e−

kτ t(wi(0)

kminb),

(10)

−

−

where kmin is the limiting (the minimal) rate constant of the cycle, kτ is the second in
order rate constant of the cycle.

So, for catalytic cycles with the limiting constant kmin, the relaxation time is also deter-
mined by one constant, but another one. This is kτ , the second in order rate constant.
It should be stressed that the only smallness condition is required, kmin should be much
smaller than other constants. The second constant, kτ should be just smaller than others
(and bigger than kmin), but there is no

condition for kτ required.

≪

One of the methods for measurement of chemical reaction constants is the relaxation
spectroscopy [32]. Relaxation of a system after an impact gives us a relaxation time or
even a spectrum of relaxation times. For catalytic cycle with limitation, the relaxation
experiment gives us the second constant kτ , while the measurement of stationary rate
gives the smallest constant, kmin. This simple remark may be important for relaxation
spectroscopy of open system.

2.5 Ensembles of cycles and robustness of stationary rate and relaxation time

Let us consider a catalytic cycle with random rate constants. For a given sample constants
k1, . . . kn the ith order statistics is equal its ith-smallest value. We are interested in the
ﬁrst order (the minimal) and the second order statistics.

k1, . . . kn}
For independent identically distributed constants the variance of kmin = min
{
is signiﬁcantly smaller then the variance of each ki, Var(k). The same is true for statistic
of every order. For many important distributions (for example, for uniform distribution),
Var(k)/n2. For big n it goes to zero
the variance of ith order statistic is of order
∼
Var(k)/n. To illustrate this, let us
faster than variance of the mean that is of order
consider n constants distributed in interval [a, b]. For each set of constants, k1, . . . kn we
ki2 ≤
introduce “symmetric coordinates” si: ﬁrst, we order the constants, a
kin.
b, then calculate s0 = ki1 −
. . . kin ≤
−
(s0, . . . sn) maps a cube [a, b]n onto n-dimensional simplex
Transformation (k1, . . . kn)
(s0, . . . sn)
a
∆n =
and uniform distribution on a cube transforms into
}
uniform distribution on a simplex.

a, sj = kij+1 −

ki1 ≤
1), sn = b

kij (j = 1, . . . n

7→
i si = b

−

≤

∼

−

{

|

P

For large n, almost all volume of the simplex is concentrated in a small neighborhood of its

10

center and this eﬀect is an example of measure concentration eﬀects that play important
role in modern geometry and analysis [34]. All si are identically distributed, and for
a) the ﬁrst moments are: E(s) = 1/(n+1) = 1/n+o(1/n),
normalized variable s = si/(b
E(s2) = 2/[(n + 1)(n + 2)] = 2/n2 + o(1/n2),

−

Var(s) = E(s2)

(E(s))2 =

−

n
(n + 1)2(n + 2)

=

1
n2 + o

1
n2

.

(cid:18)

(cid:19)

a)2/n2 +o(1/n2). The standard deviation of kmin goes
Hence, for example, Var(kmin) = (b
to zero as 1/n when n increases. This is much faster than 1/√n prescribed to the deviation
of the mean value of independent observation (the “law of errors”). The same asymptotic
1/n is true for the standard deviation of the second constant also. These parameters
∼
ﬂuctuate much less than individual constants, and even less than mean constant (for more
examples with applications to statistical physics we address to [35]).

−

It is impossible to use this observation for cycles with limitation directly, because the
inequality of limitation (6) is not true for uniform distribution. According to this inequal-
= kmin). To provide this inequality
ity, ratios ki/kmin should be suﬃciently small (if ki 6
we need to use at least the log-uniform distribution: ki = exp ∆i and ∆i are independent
variables uniformly distributed in interval [α, β] with suﬃciently big (β

α)/n.

−

One can interpret the log-uniform distribution through the Arrhenius law: k = A exp(
where ∆G is the change of the Gibbs free energy inreaction (it includes both energetic
and entropic terms: ∆G = ∆H
T S, where ∆H is enthalpy change and ∆S is entropy
change in reaction, T is temperature). The log-uniform distribution of k corresponds to
the uniform distribution of ∆G.

−

−

∆G/kT ),

−

≫

α)/n

For log-uniform distribution of constants k1, . . . kn, if the interval of distribution is suf-
ﬁciently big (i.e. (β
1), then the cycle with these constants has the limiting
step with probability close to one. More precisely we can show that for any two constants
ki, kj the probability P[ki/kj > r or kj/ki > r] = (1
α))2 approaches one
for any ﬁxed r > 1 when β
. Relaxation time of this cycle is determined by the
second constant kτ (also with probability close to one). Standard deviations of kmin and
kτ are much smaller than standard deviation of single constant ki and even smaller than
i ki/n. This eﬀect of stationary rate and relax-
standard deviation of mean constant
ation time robustness seems to be important for understanding robustness in biochemical
networks: behaviour of the entire system is much more stable than the parameters of its
parts; even for large ﬂuctuations of parameters, the system does not change signiﬁcantly
the stationary rate (statics) and the relaxation time (dynamics).

log(r)/(β

→ ∞

−

−

−

P

α

2.6 Systems with well separated constants and monotone relaxation

The log-uniform identical distribution of independent constants k1, . . . kn with suﬃciently
big interval of distribution ((β
α)/n
1) gives us the ﬁrst example of ensembles
with well separated constants: any two constants are connected by relation
with
probability close to one. Such systems (not only cycles, but much more complex networks
too) could be studied analytically “up to the end”.

≪

≫

≫

or

−

11

Some of their properties are simpler than for general networks. For example, the damping
oscillations are impossible, i.e. the eigenvalues of kinetic matrix are real (with probability
close to one). If constants are not separated, damped oscillations could exist, for example,
if all constants of the cycle are equal, k1 = k2 = . . . = kn = k, then (1 + λ/k)n = 1
and λm = k(exp(2πim/n)
1), the case m = 0 corresponds to the
linear conservation law. Relaxation time of this cycle may be relatively big: τ = 1
cos(2π/n))−

n2/(2πk) (for big n).

1) (m = 1, . . . n

k (1

−

−

−

1

∼

The catalytic cycle without limitation can have relaxation time much bigger then 1/kmin,
where kmin is the minimal reaction rate constant. For example, if all k are equal, then
20/k. In more detail the possible relations between τ and the
for n = 11 we get τ
slowest constant were discussed in [33]. In that paper, a variety of cases with diﬀerent
relationships between the steady-state reaction rate and relaxation was presented.

≈

−
∈

knA (8) has a pair of complex eigenvalues with nonzero
For catalytic cycle, if a matrix K
gknA has a degenerate eigenvalue
[0, 1] the matrix K
imaginary part, then for some g
(we use a simple continuity argument). With probability close to one, kmin ≪ |
for
any two ki, kj that are not minimal. Hence, the kmin-small perturbation cannot transform
matrix K with eigenvalues ki (7) and given structure into a matrix with a degenerate
eigenvalue. For proof of this statement it is suﬃcient to refer to diagonal dominance of
K (the absolute value of each diagonal element is greater than the sum of the absolute
values of the other elements in its column) and classical inequalities.

ki −

kj|

−

Let us give a detailed proof based on the explicit form of K left and right eigenvectors.
Let vector–column xi and vector–row li be right and left eigenvectors of K for eigenvalue
j. Let us choose a
j = 0 (j < i) and
i), and
kj) (j

ki. For coordinates of these eigenvectors we use notation xi

i = 1. It is straightforward to check that xi
i) and li
1 −
j

−
normalization condition xi
j = 0 (j > i), xi
li

j+1 = kjxj/(kj+1 −

j and li

1 = kj

1lj/(kj

i = li

ki) (j

≥

≤

−

−

−

xi
i+m =

m

Yj=1

ki+j
ki+j −

−

1
ki

; li
i
−

m =

m

Yj=1

ki

j

−
j −

−

ki

ki

(when these coordinates have sense). Under selected normalization condition, the inner
product of eigenvectors is: lixj = δij, where δij is the Kronecker delta.

For ensembles with well separated constants, with probability close to one,

(11)

(12)

Hence,
m| ≈
shift nominators in the product (11) on such a way:

0. To demonstrate that also

m| ≈

1 or

|

|

li
i
−

li
i
−

xi
i+m| ≈

|

1 or

xi
i+m| ≈

|

0, we

ki

j

−
j −

−

ki

ki ≈ (

1,
0,

if ki ≪
if ki ≫

ki
j;
−
ki+j,

xi
i+m =

ki
ki+m −

ki

m

1

−

Yj=1

ki+j
ki+j −

.

ki

12

Exactly as in (12), each multiplier ki+j/(ki+j −

ki) here is either almost 1 or almost 0,

and ki/(ki+m −

ki) is either almost 0 or almost

1. In this 0-1 asymptotics

−
j > ki for all j = 1, . . . m, else li
i
−
1 and ki+m < ki,

1 if ki+j > ki for all j = 1, . . . m

m = 0;

−

li
i = 1, li
m = 1 if ki
i
−
−
i = 1, ri
ri
i+m =
else ri
i+m = 0.

−

(13)

−

i+m =

In this asymptotic, only two coordinates of right eigenvector ri can have nonzero values,
i = 1 and ri
ri
1 where m is the ﬁrst such positive integer that i + m < n and
ki+m < ki. It is possible that such m does not exist. In that case only ri
i = 1. (Let us
1.
remind that we consider vectors in the subspace
−
In the full system of coordinates c1, . . . cn, the last case corresponds to ri
1.)
−
For left eigenvector li, li
i = . . . li
j = 0 where j > 0 and q is the ﬁrst such
i
−
positive integer that i
1 < ki. It is possible that such q does not exist.
1 > 0 and ki
q
j = 1. It is straightforward to check that in this asymptotic lirj = δij.
In that case all li
i
−

i ci = 1 with coordinates c1, . . . cn

q = 1 and li
i
−

i = 1, ri

n =

−

−

P

−

−

−

q

q

The simplest example gives the order k1 > k2 > ... > kn: li
i
−
ri
n =

1 and all other coordinates of eigenvectors are zero.

j = 1 for j

0, ri

i = 1,

≥

−

For less trivial example, let us ﬁnd the asymptotic of left and right eigenvectors for a
chain of reactions:

A1→

5 A2→

3 A3→

4 A4→

1 A5→

2 A6,

where the upper index marks the order of rate constants: k4 > k5 > k2 > k3 > k1 (ki is
the rate constant of reaction Ai →
For left eigenvectors, rows li, we have the following asymptotics:

...).

(1, 0, 0, 0, 0, 0), l2
(0, 0, 0, 1, 0, 0), l5

(0, 1, 0, 0, 0, 0), l3
(0, 0, 0, 1, 1, 0).

≈

(0, 1, 1, 0, 0, 0),

(14)

l1
l4

≈
≈

For right eigenvectors, columns ri, we have the following asymptotics (we write vector-
columns in rows):

r1
r4

≈
≈

−

(1, 0, 0, 0, 0,
(0, 0, 0, 1,

1), r2
−
1, 0), r5

(0, 1,
−
(0, 0, 0, 0, 1,

1, 0, 0, 0), r3

(0, 0, 1, 0, 0,

1),

≈

−

(15)

1).

−

For convenience, we use all six coordinates, c1

6.

−

|

(A)ij|

The matrix elements of A in the eigenbasis of K are (A)ij = liAxj. From obtained
. 1 (with probability close to one). This estimate
estimates for eigenvectors we get
does not depend on values of kinetic constants. Now, we can apply the Gershgorin theorem
knA in the eigenbasis
(see, for example, [36] and for more details [37]) to the matrix K
of K: the characteristic roots of K
knRi(A), where
Ri(A) =
(A)ij|
. If the discs do not intersect, then each of them contains one and
only one characteristic number. For ensembles with well separated constants these discs
do not intersect (with probability close to one). Complex conjugate eigenvalues could not
belong to diﬀerent discs. In this case, the eigenvalues are real – there exist no damped
oscillations.

knA belong to discs

−
z + ki| ≤

j |

−

P

|

≈
≈

≈
≈

13

2.7 Limitation by two steps with comparable constants

If we consider one-parametric families of systems, then appearance of systems with two
comparable constants may be unavoidable. On a continuous way ki(s) from one system
with well separated constants to another such system constants may coincide: such a point
s that ki(s) = kj(s) may exist, and this existence may be stable, that is, such a point
persists under continuous perturbations.

For catalytic cycle, we are interested in the following intersection only: kmin and the
second constant are of the same order, and are much smaller than other constants. Let
these constants be kj and kl, j

= l. The limitation condition is

The steady state reaction rate and relaxation time are determined by these two con-
stants. In that case their eﬀects are coupled. For the steady state we get in ﬁrst order
approximation instead of (4):

w =

kjkl
kj + kl

b; ci =

=

w
ki

b
ki

kjkl
kj + kl

(i

= j, l);

cj =

bkl
kj + kl 


1

−

=j,l
Xi

1
ki

kjkl
kj + kl 


; cl =

bkj
kj + kl 


1

−

=j,l
Xi

1
ki

.

kjkl
kj + kl 


Elementary analysis shows that under the limitation condition (16) the relaxation time is

1
kj

+

1
kl ≫

1
ki

.

=j,l
Xi

τ =

1
kj + kl

.

(16)

(17)

(18)

The single relaxation time approximation for all elementary reaction rates in a cycle with
two limiting reactions is

˙wi = kjklb

(kj + kl)wi; wi(t) =

−

kjkl
kj + kl

b + e−

(kj +kl)t

wi(0)

 

kjkl
kj + kl

.

b
!

−

(19)

The catalytic cycle with two limiting reactions has the same stationary rate w (17) and
relaxation time (18) as a reversible reaction A

B with k+ = kj, k− = kl.

↔

In two-parametric families three constants can meet. If three smallest constants kj, kl, km
have comparable values and are much smaller than others, then static and dynamic prop-
erties would be determined by these three constants. Stationary rate w and dynamic of
relaxation for the whole cycle would be the same as for 3-reaction cycle A
A
with constants kj, kl, km. The damped oscillation here are possible, for example, if kj =
kl = km = k, then there are complex eigenvalues λ = k(
2 ). Therefore, if a cycle
manifests damped oscillation, then at least three slowest constants are of the same order.
The same is true, of course, for more general reaction networks.

3
2 ±

i √3

→

→

→

−

B

C

14

6
6
6
6
6
In N-parametric families of systems N + 1 smallest constants can meet, and near such
a “meeting point” a slow auxiliary cycle of N + 1 reactions determines behaviour of the
entire cycle.

3 Multiscale ensembles and ﬁnite–additive distributions

3.1 Ensembles with well separated constants, formal approach

In previous section, ensembles with well separated constants appear. We represented them
by a log-uniform distribution in a suﬃciently big interval log k
[α, β], but we were not
interested in most of probability distribution properties, and did not use them. The only
property we really used is: if ki > kj, then ki/kj ≫
1 (with probability close to one). It
a for any preassigned value of a that does not
means that we can assume that ki/kj ≫
depend on k values. One can interpret this property as an asymptotic one for α
,
β

→ −∞

∈

.

→ ∞

That property allows us to simplify algebraic formulas. For example, ki + kj could be
substituted by max

(with small relative error), or

ki, kj}

{

aki + bkj
cki + dkj ≈ (

a/c,
b/d,

if ki ≫
if ki ≪

kj;
kj,

for nonzero a, b, c, d (see, for example, (12)).

k1, if
Of course, some ambiguity can be introduced, for example, what is it, (k1 + k2)
k2? If we ﬁrst simplify the expression in brackets, it is zero, but if we open brackets
k1 ≫
without simpliﬁcation, it is k2. This is a standard diﬃculty in use of relative errors for
round-oﬀ. If we estimate the error in the ﬁnal answer, and then simplify, we shall avoid
this diﬃculty. Use of o and
symbols also helps to control the error qualitatively: if
k1 = k1o(1).
k1 ≫
The last expression is neither zero, nor absolutely small – it is just relatively small with
respect to k1.

k2, then we can write (k1 + k2) = k1(1 + o(1)), and k1(1 + o(1))

O

−

−

,
The formal approach is: for any ordering of rate constants, we use relations
≪
a for any preassigned value of a that does not depend on k
and assume that ki/kj ≫
values. This approach allows us to perform asymptotic analysis of reaction networks. A
special version of this approach consists of group ordering: constants are separated on
several groups, inside groups they are comparable, and between groups there are relations
. An example of such group ordering was discussed at the end of previous section

and

≫

or

≫
(several limiting constants in a cycle).

≪

15

3.2 Probability approach: ﬁnite additive measures

The asymptotic analysis of multiscale systems for log-uniform distribution of independent
) is possible, but parameters α, β do not
constants on an interval log k
present in any answer, they just should be suﬃciently big. A natural question arises, what
is the limit? It is a log-uniform distribution on a line, or, for n independent identically
distributed constants, a log-uniform distribution on Rn).

[α, β] (α, β

→ ∞

∈

It is well known that the uniform distribution on Rn is impossible: if a cube has positive
probability ǫ > 0 (i.e. the distribution has positive density) then the union of N > 1/ǫ
such disjoint cubes has probability bigger than 1 (here we use the ﬁnite–additivity of
probability). This is impossible. But if that cube has probability zero, then the whole
space has also zero probability, because it can be covered by countable family of the cube
translation. Hence, translation invariance and σ-additivity (countable additivity) are in
contradiction (if we have no doubt about probability normalization).

Nevertheless, there exists ﬁnite–additive probability which is invariant with respect to
Euclidean group E(n) (generated by rotations and translations). Its values are densities
of sets.

Let D

Rn be a Lebesgue measurable subset. Density of D is the limit (if it exists):

⊂

ρ(D) = lim
→∞

r

λ(D

Bn
r )

,

∩
λ(Bn
r )

(20)

r is a ball with radius r and centre at origin. Density of Rn is 1, density of
where Bn
every half–space is 1/2, density of bounded set is zero, density of a cone is its solid angle
(measured as a sphere surface fractional area). Density (20) and translation and rotational
H = ∅ then
invariant. It is ﬁnite-additive: if densities ρ(D) and ρ(H) (20) exist and D
H) = ρ(D) + ρ(H).
ρ(D

H) exists and ρ(D

∩

∪

∪

Every polyhedron has a density. A polyhedron could be deﬁned as the union of a ﬁnite
number of convex polyhedra. A convex polyhedron is the intersection of a ﬁnite number
of half-spaces. It may be bounded or unbounded. The family of polyhedra is closed with
respect to union, intersection and subtraction of sets. For our goals, polyhedra form suﬃ-
ciently rich class. It is important that in deﬁnition of polyhedron ﬁnite intersections and
unions are used. If one uses countable unions, he gets too many sets including all open
sets, because open convex polyhedra (or just cubes with rational vertices) form a basis of
standard topology.

Of course, not every measurable set has density. If it is necessary, we can use the Hahn–
Banach theorem and study extensions ρEx of ρ with the following property:

ρ(D)

ρEx(D)

ρ(D),

≤

≤

where

λ(D

Bn
r )

ρ(D) = lim
→∞

r

inf

∩
λ(Bn
r )

, ρ(D) = lim
→∞

r

sup

λ(D

Bn
r )

.

∩
λ(Bn
r )

16

Functionals ρ(D) and ρ(D) are deﬁned for all measurable D. We should stress that such
extensions are not unique. Extension of density (20) using the Hahn–Banach theorem for
picking up a random integer was used in a very recent work [43].

One of the most important concepts of any probability theory is the conditional probabil-
ity. In the density–based approach we can introduce the conditional density. If densities
ρ(D) and ρ(H) (20) exist, ρ(H)
H) exists, then we call
it conditional density:

= 0 and the following limit ρ(D

|

ρ(D

H) = lim
→∞

r

|

λ(D

∩
λ(H

H

∩

Bn
r )

.

∩
Bn
r )

(21)

For polyhedra the situation is similar to usual probability theory: densities ρ(D) and ρ(H)
always exist and if ρ(H)
= 0 then conditional density exists too. For general measurable
sets the situation is not so simple, and existence of ρ(D) and ρ(H)
= 0 does not guarantee
existence of ρ(D

H).

|

On a line, convex polyhedra are just intervals, ﬁnite or inﬁnite. The probability deﬁned
on polyhedra is: for ﬁnite intervals and their ﬁnite unions it is zero, for half–lines x > α or
x < α it is 1/2, and for the whole line R the probability is 1. If one takes a set of positive
probability and adds or subtracts a zero–probability set, the probability does not change.

If independent random variables x and y are uniformly distributed on a line, then their
linear combination z = αx + βy is also uniformly distributed on a line. Indeed, vector
(x, y) is uniformly distributed on a plane (by deﬁnition), a set z > γ is a half-plane, the
correspondent probability is 1/2. This is a simple, but useful stability property. We shall
use this result in the following form. If independent random variables k1, . . . kn are log-
for real αi is also log-uniformly
uniformly distributed on a line, then the monomial
distributed on a line.

i=1 kαi

n

i

Q

3.3 Carroll obtuse problem and paradoxes of conditioning

Lewis Carroll’s Pillow Problem #58 [21]: “Three points are taken at random on an inﬁnite
plane. Find the chance of their being the vertices of an obtuse–angled triangle.”

⊂

A random triangle on an inﬁnite plane is presented by a point equidistributed in R6. Due
to the density – based deﬁnition, we should take and calculate the density of the set of
obtuse–angled triangles in R6. This is equivalent to the problem: ﬁnd a fraction of the
sphere S5
R6 that corresponds to obtuse–angled triangles. Just integrate...
. But there
remains a problem. Vertices of triangle are independent. Let us use the standard logic
for discussion of independent trials: we take the ﬁrst point A at random, then the second
point B, and then the third point C. Let us draw the ﬁrst side AB. Immediately we ﬁnd
that for almost all positions of the the third point C the triangle is obtuse–angled (see
[22]). L. Carroll proposed to take another condition: let AB be the longest side and let C
be uniformly distributed in the allowed area. The answer then is easy – just a ratio of areas
of two simple ﬁgures. But there are absolutely no reasons for uniformity of C distribution.
And it is more important that the absolutely standard reasoning for independently chosen
points gives another answer than could be found on the base of joint distribution. Why

17

6
6
6
these approaches are in disagreement now? Because there is no classical Fubini theorem
for our ﬁnite–additive probabilities, and we cannot easily transfer from a multiple integral
to a repeated one.

There exists a much simpler example. Let x and y be independent positive real number.
This means that vector (x, y) is uniformly and independently distributed in the ﬁrst
quadrant. What is probability that x
y? Following the deﬁnition of probability based
on the density of sets, we take the correspondent angle and ﬁnd immediately that this
probability is 1/2. This meets our intuition well. But let us take the ﬁrst number x and
look for possible values of y. The result: for given x the second number y is uniformly
distributed on [0,
y. For the inﬁnite
), and only a ﬁnite interval [0, x] corresponds to x
rest we have x < y. Hence, x < y with probability 1. This is nonsense because of symmetry.
So, for our ﬁnite–additive measure we cannot use repeated integrals (or, may be, should
use them in a very peculiar manner).

∞

≥

≥

3.4 Law of total probability and orderings

For polyhedra, there appear no conditioning problems. The law of total probabilities holds:
if Rn =
Rn is a
i=1Hi, Hi are polyhedra, ρ(Hi) > 0, ρ(Hi ∩
∪
polyhedron, then

Hj) = 0 for i

= j, and D

⊂

m

ρ(D) =

ρ(D

Hi) =

ρ(D

Hi)ρ(Hi).

∩

|

(22)

m

Xi=1

m

Xi=1

Our basic example of multiscale ensemble is log-uniform distribution of reaction constants
in Rn
+ (log ki are independent and uniformly distributed on the line). For every ordering
kj1 > kj2 > . . . > kjn a polyhedral cone Hj1j2...jn in Rn is deﬁned. These cones have
equal probabilities ρ(Hj1j2...jn) = 1/n! and probability of intersection of cones for diﬀerent
orderings is zero. Hence, we can apply the law of total probability (22). This means that
we can study every event D conditionally, for diﬀerent orderings, and than combine the
results of these studies in the ﬁnal answer (22).

For example, if we study a simple cycle then formula (4) for steady state is valid with any
given accuracy with unite probability for any ordering with the given minimal element
kn.

For cycle with given ordering of constants we can ﬁnd 0-1 approximation of left and right
eigenvectors (13). This approximation is valid with any given accuracy for this ordering
with unite probability.

If we consider suﬃciently wide log-uniform distribution of constants on a bounded interval
instead of the inﬁnite axis then these statements are true with probability close to 1.

For general system that we study below the situation is slightly more complicated: new
terms, auxiliary reactions with monomial rate constants kς =
i could appear with
integer (but not necessary positive) ςi, and we should include these kς in ordering. It
follows from stability property that these monomials are log-uniform distributed on inﬁnite
interval, if ki are. Therefore the situation seems to be similar to ordering of constants,

i kςi

Q

18

6
= 0.

but there is a signiﬁcant diﬀerence: monomials are not independent, they depend on ki
with ςi 6
Happily, in the forthcoming analysis when we include auxiliary reactions with constant
kς, we always exclude at least one of reactions with rate constant ki and ςi 6
= 0. Hence,
for we always can use the following statement (for the new list of constants, or for the old
b for positive a, b
kj2 ≫
one): if kj1 > kj2 > . . . > kjn then kj1 ≫
means: for any given ε > 0 the inequality εa > b holds with unite probability.

kjn, where a

. . .

≫

≫

If we use suﬃciently wide but ﬁnite log-uniform distribution then ε could not be arbi-
trarily small (this depends on the interval with), and probability is not unite but close
to one. For given ε > 0 probability tends to one when the interval width goes to inﬁnity.
It is important that we use only ﬁnite number of auxiliary reactions with monomial con-
stants, and this number is bounded from above for given number of elementary reactions.
For completeness, we should mention here general algebraic theory of orderings that is
necessary in more sophisticated cases [47,48].

4 Relaxation of multiscale networks and hierarchy of auxiliary discrete dy-

namical systems

4.1 Deﬁnitions, notations and auxiliary results

4.1.1 Notations

In this Sec., we consider a general network of linear (monomolecular) reactions. This
network is represented as a directed graph (digraph): vertices correspond to components
Ai, edges correspond to reactions Ai →
Aj with kinetic constants kji > 0. For each vertex,
Ai, a positive real variable ci (concentration) is deﬁned. A basis vector ei corresponds to
Ai with components ei
j = δij, where δij is the Kronecker delta. The kinetic equation for
the system is

or in vector form: ˙c = Kc.

dci
dt

=

Xj

(kijcj −

kjici),

Aj
To write another form of (23) we use stoichiometric vectors: for a reaction Ai →
the stoichiometric vector γji is a vector in concentration space with ith coordinate
1,
jth coordinate 1, and zero other coordinates. The reaction rate wji = kjici. The kinetic
equation has the form

−

Xi,j
where c is the concentration vector. One more form of (23) describes directly dynamics
of reaction rates:

dc
dt

=

wjiγji,

(23)

(24)

(25)

dwji
dt  

= kji

= kji

dci
dt !

(wil −

wli).

Xl

19

It is necessary to mention that, in general, system (25) is not equivalent to (24), because
there are additional connections between variables wji. If there exists at least one Ai with
kji/kli. If
Aj and Ai →
two diﬀerent outgoing reactions, Ai →
Aj on the set of Ai (see
the reaction network generates a discrete dynamical system Ai →
below), then the variables wji are independent, and (25) gives equivalent representation
of kinetics.

= l), then wji/wli ≡

Al (j

A linear conservation law is a linear function deﬁned on the concentrations q(c) =
n
i=1 qici, whose value is preserved by the dynamics (23). The set of all the conservation
laws forms the left kernel of the matrix K. Equation (23) always has a linear conservation
P
i ci = const. If there is no other independent linear conservation law, then
law: b(c) =
the system is weakly ergodic.

P

Two vertices are called adjacent if they share a common edge. A path is a sequence of
adjacent vertices. A graph is connected if any two of its vertices are linked by a path. A
maximal connected subgraph of graph G is called a connected component of G. Every
graph can be decomposed into connected components.

A directed path is a sequence of adjacent edges where each step goes in direction of an
edge. A vertex A is reachable by a vertex B, if there exists an oriented path from B to A.

A nonempty set V of graph vertexes forms a sink, if there are no oriented edges from
A3 the one-vertex
Ai ∈
V . For example, in the reaction graph A1 ←
V to any Aj /
∈
A3}
A1}
are sinks. A sink is minimal if it does not contain a strictly smaller
sets
and
{
{
sink. In the previous example,
are minimal sinks. Minimal sinks are also called
ergodic components.

A2 →

A1}

A3}

{

{

,

A digraph is strongly connected, if every vertex A is reachable by any other vertex B.
Ergodic components are maximal strongly connected subgraphs of the graph, but inverse
is not true: there may exist maximal strongly connected subgraphs that have outgoing
edges and, therefore, are not sinks.

We study ensembles of systems with a given graph and independent and well separated
kinetic constants kij. This means that we study asymptotic behaviour of ensembles with
independent identically distributed constants, log-uniform distributed in suﬃciently big
interval log k
, or just a log-uniform distribution on inﬁnite
axis, log k

[α, β], for α

→ −∞

→ ∞

∈
R.

, β

∈

4.1.2 Sinks and ergodicity

If there is no other independent linear conservation law, then the system is weakly ergodic.
The weak ergodicity of the network follows from its topological properties.

The following properties are equivalent and each one of them can be used as an alternative
deﬁnition of weak ergodicity:

(1) There exist the only independent linear conservation law for kinetic equations (23)

(this is b(c) =

i ci = const).

P

20

6
(2) For any normalized initial state c(0) (b(c) = 1) there exists a limit state c∗ =
exp(Kt) c(0) that is the same for all normalized initial conditions. (For all c,
exp(Kt) c = b(c)c∗.)

limt
limt

→∞

(3) For each two vertices Ai, Aj (i

= j) we can ﬁnd such a vertex Ak that oriented paths
exist from Ai to Ak and from Aj to Ak. This means that the following structure
exists:

→∞

One of these paths can be degenerated: it might be i = k or j = k.
(4) The network has only one minimal sink (one ergodic component).

Ai →

. . .

Ak ←

→

. . .

←

Aj.

For every monomolecular kinetic system, the Jordan cell for zero eigenvalue of matrix
K is diagonal and the maximal number of independent linear conservation laws (i.e. the
geometric multiplicity of the zero eigenvalue of the matrix K) is equal to the maximal
number of disjoint ergodic components (minimal sinks).

Ai1, . . . Ail}

Let G =
malized invariant distribution) cG with the following properties: cG
cG
i > 0 for all i

be an ergodic component. Then there exists a unique vector (nor-
i1, . . . il}
,

b(cG) = 1, KcG = 0.

i = 0 for i /

∈ {

{

i1, . . . il}

∈ {

If G1, . . . Gm are all ergodic components of the system, then there exist m independent
positive linear functionals b1(c), ... bm(c) such that

i bi = b and for each c

exp(Kt)c =

lim
t
→∞

P
bi(c)cGi.

m

Xi=1

(26)

So, for any solution of kinetic equations (23), c(t), the limit at t
is a linear combi-
nation of normalized invariant distributions cGi with coeﬃcients bi(c(0)). In the simplest
, components of vectors cG1, cG2 are
example, A1 ←
(1, 0, 0) and (0, 0, 1), correspondingly. For functionals b1,2 we get:

A3, G1 =

A2 →

, G2 =

A1}

A3}

→ ∞

{

{

b1(c) = c1 +

c2; b2(c) =

c2 + c3,

(27)

k1
k1 + k2

k2
k1 + k2

A3, correspondingly. We
where k1, k2 are rate constants for reaction A2 →
k2. Hence, one
can mention that for well separated constants either k1 ≫
of the coeﬃcients k1/(k1 + k2), k2/(k1 + k2) is close to 0, another is close to 1. This is
an example of the general zero–one law for multiscale systems: for any l, i, the value of
functional bl (26) on basis vector ei, bl(ei), is either close to one or close to zero (with
probability close to 1).

A1, and A2 →

k2 or k1 ≪

We can understand better this asymptotics by using the Markov chain language. For
non-separated constants a particle in A2 has nonzero probability to reach A1 and nonzero
probability to reach A3. The zero–one law in this simplest case means that the dynamics
of the particle becomes deterministic: with probability one it chooses to go to one of
A3, we
vertices A2, A3 and to avoid another. Instead of branching, A2 →
A3. Graphs without branching represent
select only one way: either A2 →
discrete dynamical systems.

A1 and A2 →

A1 or A2 →

21

6
4.1.3 Decomposition of discrete dynamical systems

is a semigroup 1, φ, φ2, ...,
Discrete dynamical system on a ﬁnite set V =
V is a periodic point, if φl(Ai) = Ai for some l > 0;
where φ is a map φ : V
else Ai is a transient point. A cycle of period l is a sequence of l distinct periodic points
A, φ(A), φ2(A), . . . φl
1(A) with φl(A) = A. A cycle of period one consists of one ﬁxed
point, φ(A) = A. Two cycles, C, C ′ either coincide or have empty intersection.

A1, A2, . . . An}

V . Ai ∈

→

{

−

The set of periodic points, V p, is always nonempty. It is a union of cycles: V p =
For each point A
that φq(A)
of cycle Cj and use notation Att(Cj) =
diﬀerent cycles, Att(Cj)
points τ (A) > 0.

∪jCj.
V there exist such a positive integer τ (A) and a cycle C(A) = Cj
τ (A). In that case we say that A belongs to basin of attraction
Att(Cj). For
Att(Cl) = ∅. If A is periodic point then τ (A) = 0. For transient

. Of course, Cj ⊂

C(A) = Cj}

Cj for q

≥

A

∈

∩

∈

{

|

So, the phase space V is divided onto subsets Att(Cj). Each of these subsets includes
one cycle (or a ﬁxed point, that is a cycle of length 1). Sets Att(Cj) are φ-invariant:
φ(Att(Cj))
Cj consist of transient points and there exists
such positive integer τ that φq(Att(Cj)) = Cj if q

Att(Cj). The set Att(Cj)

τ .

⊂

\

≥

4.2 Auxiliary discrete dynamical systems and relaxation analysis

4.2.1 Auxiliary discrete dynamical system

For each Ai, we deﬁne κi as the maximal kinetic constant for reactions Ai →
. For correspondent j we use notation φ(i): φ(i) = arg maxj{
maxj{
φ(i) is deﬁned under condition that for Ai outgoing reactions Ai →
the deﬁnition: φ(i) = i if there exist no such outgoing reactions.

Aj: κi =
. The function
Aj exist. Let us extend

kji}

kji}

The map φ determines discrete dynamical system on a set of components V =
. We
call it the auxiliary discrete dynamical system for a given network of monomolecular reac-
tions. Let us decompose this system and ﬁnd the cycles Cj and their basins of attraction,
Att(Cj).

Ai}

{

Notice that for the graph that represents a discrete dynamic system, attractors are ergodic
components, while basins are connected components.

An auxiliary reaction network is associated with the auxiliary discrete dynamical system.
Aφ(i) with kinetic constants κi. The correspondent kinetic
This is the set of reactions Ai →
equation is

or in vector notations (24)

˙ci =

κici +

κjcj,

−

Xφ(j)=i

(28)

(29)

dc
dt

= ˜Kc =

κiciγφ(i) i;

˜Kij =

κjδij + κjδi φ(j).

−

Xi

22

For deriving of the auxiliary discrete dynamical system we do not need the values of
rate constants. Only the ordering is important. Below we consider multiscale ensembles
of kinetic systems with given ordering and with well separated kinetic constants (kσ(1) ≫
kσ(2) ≫
In the following, we analyze ﬁrst the situation when the system is connected and has only
one attractor. This can be a point or a cycle. Then, we discuss the general situation with
any number of attractors.

... for some permutation σ).

4.2.2 Acyclic auxiliary system with one attractor: structure and eigenvectors

In the simplest case, the auxiliary discrete dynamical system is acyclic and has only one
attractor, a ﬁxed point. Let this point be An (n is the number of vertices). For such a
system, it is easy to ﬁnd explicit analytic solution of kinetic equation (28). First of all,
these systems have a characteristic property among all auxiliary dynamical systems: the
Aφ(i) form a basis in the subspace of concentration
stoichiometric vectors of reactions Ai →
1 reaction, and their stoichiometric vectors are
space with
−
independent. On the other hand, existence of cycles implies linear connections between
stoichiometric vectors, and existence of two attractors in acyclic system implies that the
number of reactions is less n
1, and their stoichiometric vectors could not form a basis
in n
1-dimensional space.

i ci = 0: there exist n

−

P

−

For ensembles with well separate constants, relaxation of the whole network is approxi-
mated by solution of auxiliary kinetic equation (28) with high accuracy, with probability
close to 1. To prove this statement, let us ﬁrst ﬁnd left and right eigenvectors of matrix
˜K of auxiliary kinetic system (28) for acyclic auxiliary dynamics. In this case, for any
vertex diﬀerent from the attractor there is an eigenvector. Right eigenvectors will be con-
structed by recurrence starting from the vertex and moving in the direction of the ﬂow.
The construction is in opposite direction for left eigenvectors.

P

For zero eigenvalue, the right eigenvector r0 has only one nonzero coordinate, cn = 1, the
left eigenvector is a raw l0 = (1, 1, . . . 1) (this corresponds to the linear conservation law
i ci = const), the normalization condition holds: l0r0 = 1. Of course, l0 is left
l0c =
eigenvector with zero eigenvalue for any network of monomolecular reactions. If for such
a network the auxiliary discrete dynamical system is acyclic and has only one attractor,
a ﬁxed point, then vector r0 is right eigenvector with zero eigenvalue, because in this case
there is no outgoing reaction of the form An →
Nonzero eigenvalues of ˜K (28) are
1). For left and right eigenvectors
of ˜K with eigenvalue
κi we use notations li (vector-raw) and ri (vector-column), corre-
spondingly, and apply normalization condition ri
i = 1. For given i, τi is the minimal
integer such that φτi(i) = n (this is a “relaxation time” i.e. the number of steps to reach
are diﬀerent. For right eigenvector ri only
attractor). All indices
coordinates ri

k = 0, 1, . . . τi}
φk(i) (k = 0, 1, . . . τi) could have nonzero values, and

−
i = li

Ai (i = 1, . . . n

κi (i = 1, . . . n

φk(i)

1).

−

−

−

{

|

( ˜Kri)φk+1(i) =

κφk+1(i)ri

φk+1(i) + κφk(i)ri

φk(i) =

κiri

φk+1(i).

−

−

23

Hence,

Hence,

ri
φk+1(i) =

ri
φk(i) =

κi

κφk(i)
κφk+1(i) −
=

κi

k

Yj=0
1
k
−

κφj(i)
κφj+1(i) −
κφj+1(i)
κφj+1(i) −

κi
κφk+1(i) −
The last transformation is convenient for estimation of the product for well separated
constants (compare to (12)):

Yj=0

κi

κi

.

(30)

κφj+1(i)
κφj+1(i) −

κi ≈ (

1, if κφj+1(i) ≫
0, if κφj+1(i) ≪

κi,
κi;

κi
κφk+1(i) −

κi ≈ (

1, if κi ≫
−
0, if κi ≪

κφk+1(i),
κφk+1(i).

(31)

j could have nonzero value only if there exists such
0 that φq(j) = i (this q is unique because the auxiliary dynamical system has no

For left eigenvector li coordinate li
q
cycles). In that case (for q > 0),

≥

For every fraction in (32) the following estimate holds:

(li ˜K)j =

κjli

j + κjli

φ(j) =

κili
j.

−

−

li
j =

κj
κj −

κi

li
φ(j) =

q

1

−

Yk=0

κφk(j)
κφk(j) −

.

κi

κφk(j)
κφk(j) −

κi ≈ (

1, if κφk(j) ≫
0, if κφk(j) ≪

κi,
κi.

(32)

(33)

As we can see, every coordinate of left and right eigenvectors of ˜K (30), (32) is either 0
1, or close to 0 or to 1 (with probability close to 1). We can write this asymptotic
or
±
i = 1 and li
representation explicitly (analogously to (13)). For left eigenvectors, li
j = 1 (for
= j) if there exists such q that φq(j) = i, and κφd(j) > κi for all d = 0, . . . q
i
1, else
j = 0. For right eigenvectors, ri
li
1 if κφk(j) < κi and for all positive
m < k inequality κφm(j) > κi holds, i.e. k is ﬁrst such positive integer that κφk(j) < κi.
Vector ri has not more than two nonzero coordinates. It is straightforward to check that
in this asymptotic lirj = δij.

i = 1 and ri

φk(j) =

−

−

In general, coordinates of eigenvectors li
j, ri
j = i because the auxiliary system is acyclic. On the other hand, lirj = 0 if i
because that are eigenvectors for diﬀerent eigenvalues, κi and κj. Hence, lirj = δij.

j are simultaneously nonzero only for one value
= j, just

For example, let us ﬁnd the asymptotic of left and right eigenvectors for a branched acyclic
system of reactions:

A1→

7 A2→

5 A3→

6 A4→

2 A5→

4 A8, A6→

1 A7→

3 A4

24

6
6
where the upper index marks the order of rate constants: κ6 > κ4 > κ7 > κ5 > κ2 > κ3 >
κ1 (κi is the rate constant of reaction Ai →
For left eigenvectors, rows li, we have the following asymptotics:

...).

(1, 0, 0, 0, 0, 0, 0, 0), l2
(0, 0, 0, 1, 0, 0, 0, 0), l5
(0, 0, 0, 0, 0, 1, 1, 0)

≈
≈

l1
l4
l7

≈
≈
≈

(0, 1, 0, 0, 0, 0, 0, 0), l3
(0, 0, 0, 1, 1, 1, 1, 0), l6

(0, 1, 1, 0, 0, 0, 0, 0),
(0, 0, 0, 0, 0, 1, 0, 0).

(34)

For right eigenvectors, columns ri, we have the following asymptotics (we write vector-
columns in rows):

r1
r4
r7

≈
≈
≈

(1, 0, 0, 0, 0, 0, 0,
(0, 0, 0, 1,
(0, 0, 0, 0,

1), r2
−
1, 0, 0, 0), r5
1, 0, 1, 0).

≈
≈

−
−

(0, 1,
(0, 0, 0, 0, 1, 0, 0,

1, 0, 0, 0, 0, 0), r3
1), r6

−

(0, 0, 1, 0, 0, 0, 0,
(0, 0, 0, 0, 0, 1,

1),
−
1, 0),

(35)

−

For convenience, we use all eight coordinates, c1

8.

−

−

≈
≈

≈
≈

4.2.3 Relaxation of a system with acyclic auxiliary dynamical system

Let us assume that the auxiliary dynamical system is acyclic and has only one attractor,
a ﬁxed point. This means that stoichiometric vectors γφ(i) i form a basis in a subspace
Al the following linear
of concentration space with
operators Qil can be deﬁned:

i ci = 0. For every reaction Ai →

P

Qil(γφ(i) i) = γli, Qil(γφ(p) p) = 0 for p

= i.

(36)

The kinetic equation for the whole reaction network (24) could be transformed in the
form

γφ(i) iκici

dc
dt

1 +



=

=

Xi


1 +









=

1 +

kli
κi

klj
κj

klj
κj

Qil

Qjl

Qjl


=φ(i)

Xl l

Xj,l (l

=φ(j))

Xj,l (l

=φ(j))

Xi

˜Kc,

γφ(i) iκici

(37)

where ˜K is kinetic matrix of auxiliary kinetic equation (29). By construction of auxiliary
= φ(i). Notice also that
dynamical system, kli ≪
does not depend on rate
constants.

Qjl|

κi if l

|

Let us represent system (37) in eigenbasis of ˜K obtained in previous subsection. Any
matrix B in this eigenbasis has the form B = (˜bij), ˜bij = liBrj =
s, where (bqs)
is matrix B in the initial basis, li and rj are left and right eigenvectors of ˜K (30), (32).
In eigenbasis of ˜K the Gershgorin estimates of eigenvalues and estimates of eigenvectors
are much more eﬃcient than in original coordinates: the system is stronger diagonally

qbqsrj

qs li

P

25

6
6
6
6
6
dominant. Transformation to this basis is an eﬀective preconditioning for perturbation
theory that uses auxiliary kinetics as a ﬁrst approximation to the kinetics of the whole
system.

First of all, we can exclude the conservation law. Any solution of (37) has the form
c(t) = br0 + ˜c(t), where b = l0˜c(t) =
i ˜ci(t) = 0. On the subspace of concentration space
with

i ci = 0 we get

P

(38)

P

= (1 +

dc
dt

)diag

E
κ1, . . .

{−
κn

κ1, . . .

κn

c,

1}

−

−

E

εij| ≪
= (εij),
where
on the main diagonal. If
|
eigenvalues of matrix (1+
κi.
with

1, and diag
εij| ≪
)diag
E

|

{−

is diagonal matrix with

{−

1
−
1 then we can use the Gershgorin theorem and state that
κi +θi

are real and have the form λi =

κ1, . . .

1}

κn

−

−

−

−

κ1, . . .

κn

−

1}

−

−

|

|

εij| ≪

θi| ≪
1 (for ensembles with well separated constants, with proba-
To prove inequality
bility close to 1) we use that the left and right eigenvectors of ˜K (30), (32) are uniformly
bounded under some non-degeneration conditions and those conditions are true for well
separated constants. For ensembles with well separated constants, for any given positive
g < 1 and all i, j (i
= j) the following inequality is true with probability close to 1:
> gκi. Let us select a value of g and assume that this diagonal gap condition is
κi −
|
always true. In this case, for every fraction in (30), (32) we have estimate

κj|

κi
κj −
|
Therefore, for coordinates of right and left eigenvectors of ˜K (30), (32) we get

κi|

1
g

<

.

ri
φk+1(i)|

|

<

1
gk <

1
gn ,

<

li
j|

|

1
gq <

1
gn .

(39)

|

|

and

θi|

εij|

/κi from above as const

We can estimate
. So, the eigen-
values for kinetic matrix of the whole system (37) are real and close to eigenvalues of
auxiliary kinetic matrix ˜K (29). For eigenvectors, the Gershgorin theorem gives no re-
sult, and additionally to diagonal dominance we must assume the diagonal gap condition.
Based on this assumption, we proved the Gershgorin type estimate of eigenvectors in
Appendix 1. In particular, according to this estimate, eigenvectors for the whole reaction
network are arbitrarily close to eigenvectors of ˜K (with probability close to 1).

kls/κs}

=φ(s){

maxl

×

So, if the auxiliary discrete dynamical system is acyclic and has only one attractor (a
ﬁxed point), then the relaxation of the whole reaction network could be approximated by
the auxiliary kinetics (28):

c(t) = (l0c(0))r0 +

n

−

1
(lic(0))ri exp(

κit),

−

(40)

Xi=1
where l0 and r0 are left (vector-raw) and right (vector-column) eigenvectors of ˜K cor-
i = δin, and δin is Kronecker delta), li
respondent to zero eigenvalue (l0 = (1, 1, . . . 1), r0
and ri are left and right eigenvectors of ˜K correspondent to eigenvalue
κi. For li and ri

−

26

6
6
one can use exact formulas (30) and (32) or 0-1 asymptotic representations based on (33)
and (31) for multiscale systems. This approximation (40) could be improved by iterative
methods, if necessary.

4.2.4 Auxiliary system with one cyclic attractor

The second simple particular case on the way to general case is a reaction network with
components A1, . . . An whose auxiliary discrete dynamical system has one attractor, a
cycle with period τ > 1: An
. . . An →
τ +1 (after some change
of enumeration). We assume that the limiting step in this cycle (reaction with minimal
constant) is An →
τ +1. If auxiliary discrete dynamical system has only one attractor
then the whole network is weakly ergodic. But the attractor of the auxiliary system may
not coincide with a sink of the reaction network.

τ +1 →

τ +2 →

An

An

An

−

−

−

−

There are two possibilities:

(1) In the whole network, all the outgoing reactions from the cycle have the form
τ +2, . . . An
τ +j (i, j > 0). This means that the cycle vertices An

τ +1, An

An
form a sink for the whole network.

τ +i →

An

−

−

−

−

(2) There exists a reaction from a cycle vertex An
τ +1, An

τ +i to Am, m
is not a sink for the whole network.

that the set

An

≤

−

n

−

τ . This means

{

−

τ +2, . . . An}

−

{

τ +1, An

τ +1, An

In the ﬁrst case, the limit (for t
) distribution for the auxiliary kinetics is the well-
→ ∞
studied stationary distribution of the cycle An
τ +2, . . . An described in Sec. 2 (2),
−
τ +2, . . . An}
An
is the only ergodic component for the whole
(3) (4), (6). The set
network too, and the limit distribution for that system is nonzero on vertices only. The
stationary distribution for the cycle An
τ +1 approximates
τ +1 →
the stationary distribution for the whole system. To approximate the relaxation process,
let us delete the limiting step An →
τ +1 from this cycle. By this deletion we produce
an acyclic system with one ﬁxed point, An, and auxiliary kinetic equation (29) transforms
into

. . . An →

τ +2 →

An

An

An

−

−

−

−

−

−

−

dc
dt

= ˜K0c =

κiciγφ(i) i.

n

1

−

Xi=1

(41)

As it is demonstrated, dynamics of this system approximates relaxation of the whole
κi (i < n), the corresponded
network in subspace
eigenvectors are represented by (30), (32) and 0-1 multiscale asymptotic representation is
P
based on (33) and (31).

i ci = 0. Eigenvalues for (41) are

−

−

−

−

−

An

An

∈ {

τ +1, An

τ +1, An

τ +2, . . . An}

{
τ +2, . . . An}

is not a sink for the whole net-
Aj with

In the second case, the set
work. This means that there exist outgoing reactions from the cycle, An
Aj /
τ +i
that corresponds to the cycle reaction An
τ +i+1 is much bigger than any other
τ +i that corresponds to a “side” reaction An
τ + i + 1):
constant kj n
−
κn
kj n
τ +i. This is because deﬁnition of auxiliary discrete dynamical system and
τ +i ≫
assumption of ensemble with well separated constants (multiscale asymptotics). This in-
equality allows us to separate motion and to use for computation of the rates of outgo-
Aj the quasi steady state distribution in the cycle. This means
ing reaction An

. For every cycle vertex An
τ +i →

τ +i the rate constant κn

τ +i →

τ +i →

Aj (j

= n

An

−

−

−

−

−

−

−

−

−

τ +i →

−

27

6
1

−

≤

P

i
≤

τ cn

τ +1 =

τ +i and substitute the reaction An

that we can glue the cycle into one vertex A1
τ +1 with the correspondent concentration
n
Aj by A1
c1
Aj with
n
τ +1 →
n
−
the rate constant renormalization: k1
τ +1 = kj n
τ +1. By the superscript
j n
QS we mark here the quasistationary concentrations for given total cycle concentration
c1
Aj to another vertex of
n
the cycle (usually to An): we can substitute the reaction An
Aj by the reaction
τ +i →
An

τ +1. Another possibility is to recharge the link An

Aj with the rate constant renormalization:

τ +i →
−
τ +icQS
τ +i/c1
n
n
−

τ +i →
−

−

−

−

−

−

−

−

τ +q →

−

kj n

τ +q = kj n

−

τ +icQS
n
−

−

τ +i/cQS

n

τ +q.

−

(42)

We apply this approach now and demonstrate its applicability in more details later in this
section.

−

−

{

n

into a point A1
n

τ +i = cnκn/κn
An

τ +1. We say that components An

For the quasi-stationary distribution on the cycle we get cn
The original reaction network is transformed by gluing the cycle
An}
τ +1, An
system belong to the component A1
τ +1 of the new system. All the reactions Ai →
n
i, j
τ remain the same with rate constant kji. Reactions of the form Ai →
≤
−
i
n
τ , j > n
−
≤
−
A1
into Ai →
τ +1 with the same rate constant kji. Reactions of the form Ai →
n
−
i > n
n
τ , j
−
≤
−
Aj with the “quasistationary” rate constant kQS
into reactions A1
n
−
After that, we select the maximal kQS
ji
is the rate constant for reaction A1
n
τ (internal reactions of the site) vanish.
with i, j > n

τ +2, . . . An}
τ +2, . . . An}
ji = kjiκn/κn
ji . This k(1)
τ kQS
Aj in the new system. Reactions Ai →

i < τ ).
τ +i (1
≤
−
τ +2, . . .
τ +1, An
−
τ +2, . . . An of the original
Aj with
Aj with
) transform
Aj with
) transform
τ +i.

τ (incoming reactions of the cycle

τ (outgoing reactions of the cycle

for given j: k(1)
j n
−

τ +1 = maxi>n

τ +1 →

τ +1 →

τ +1, An

τ +1, An

τ +1
Aj

An

An

j n

{

{

−

−

−

−

−

−

−

−

−

−

−

−

−

Among reactions of the form An

Am (m

n

τ ) we ﬁnd

−

τ +i →
τ +i = max
i,m {

κ(1)
n
−

≥

−

km n

τ +iκn/κn

−

.

τ +i}

−

Let the correspondent i, m be i1, m1.

(43)

−

−

{

n

−

≤

−

≤
n

τ , A1
n

A1, . . . An

−
τ , j > n

τ transform into Ai →

After that, we create a new auxiliary discrete dynamical system for the new reaction
network on the set
. We can describe this new auxiliary system as a
τ +1}
result of transformation of the ﬁrst auxiliary discrete dynamical system of initial reaction
Aj with
network. All the reaction from this ﬁrst auxiliary system of the form Ai →
i, j
Aj with
τ remain the same with rate constant κi. Reactions of the form Ai →
A1
i
τ +1 with the same rate constant κi. One more
n
reaction is to be added: A1
τ +i. We “glued” the cycle
n
into one vertex, A1
τ +1, and added new reaction from this vertex to Am1 with maximal
n
possible constant (43). Without this reaction the new auxiliary dynamical system has
only one attractor, the ﬁxed point A1
τ +1. With this additional reaction that point is not
n
. . . A1
ﬁxed, and a new cycle appears: Am1 →
n
Again we should analyze, whether this new cycle is a sink in the new reaction network,
etc. Finally, after a chain of transformations, we should come to an auxiliary discrete
dynamical system with one attractor, a cycle, that is the sink of the transformed whole
reaction network. After that, we can ﬁnd stationary distribution by restoring of glued

Am1 with rate constant κ(1)
n
−

τ +1 →

τ +1 →

Am1.

−

−

−

−

−

28

cycles in auxiliary kinetic system and applying formulas (2), (3) (4), (6) from Sec. 2.
First, we ﬁnd the stationary state of the cycle constructed on the last iteration, after
that for each vertex Ak
j that is a glued cycle we know its concentration (the sum of all
concentrations) and can ﬁnd the stationary distribution, then if there remain some vertices
that are glued cycles we ﬁnd distribution of concentrations in these cycles, etc. At the end
of this process we ﬁnd all stationary concentrations with high accuracy, with probability
close to one.

As a simple example we use the following system, a chain supplemented by three reactions:

2 A3→
where the upper index marks the order of rate constants.

5 A6, A6→

1 A2→

4 A5→

3 A4→

6 A4, A5→

A1→

7 A2, A3→

8 A1,

(44)

Auxiliary discrete dynamical system for the network (44) includes the chain and one
reaction:

A1→

1 A2→
It has one attractor, the cycle A4→
system, because there exists an outgoing reaction A5→
6 A4 into a vertex A1
4 A5→
By gluing the cycle A4→
supplemented by two reactions:

3 A4→
5 A6→

2 A3→
4 A5→

5 A6→

5 A6→
7 A2.

4 A5→
6 A4. This cycle is not a sink for the whole

6 A4.

4 we get new network with a chain

3 A1

? A1.

4, A1

A1→

4 A5→

5 A6→

6 A4, κ5 = k65).

? A2, A3→
4→

2 A3→
24 = k25κ6/κ5 (κ6 = k46 is the limiting step of the cycle

1 A2→
Here the new rate constant is k(1)
A4→
Here we can make a simple but important observation: the new constant k1
24 = k25κ6/κ5
has the same log-uniform distribution on the whole axis as constants k25, κ6 and κ5 have.
The new constant k1
24 depends on k25 and the internal cycle constants κ6 and κ5, and is
independent from other constants.

(45)

Of course, k(1)
orderings, k(1)
necessary. But for both orderings the auxiliary dynamical system for network (45) is

24 and k13 are a priori unknown. Both
24 < k13, are possible, and should be considered separately, if

24 < κ5, but relations between k(1)
24 > k13 and k(1)

4→

3 A1

? A2

A1→

2 A3→

1 A2→
(of course, κ(1)
4 < κ3 < κ2 < κ1). It has one attractor, the cycle A2→
cycle is not a sink for the whole system, because there exists an outgoing reaction A3→
The limiting constant for this cycle is κ(1)
one point, A2
A1→
auxiliary discrete dynamical system is the same graph A1→
we do not need further transformations.

? A2. This
? A1.
24 = k25k46/k65. We glue this cycle into
2. The new transformed system is very simple, it is just a two step cycle:
12 = k13κ(1)
4 /κ3 = k13k25k46/(k65k43). The
? A1, this is a cycle, and

? A1. The new reaction constant is k(2)

4 = k(1)

2 A3→

1 A2

3 A1

1 A2

2→

2→

4→

Let us ﬁnd the steady state on the way back, from this ﬁnal auxiliary system to the
original one. For steady state of each cycle we use formula (4).

29

The steady state for the ﬁnal system is c1 = bk(2)
A2
2 includes the cycle A2→
c3 = c(2)
2 k(1)
24 /k43, c(1)
4 = c(2)
5 A6→
4 A5→
A4→
c6 = c(1)
4 (1
−

3 A1
4→
k(1)
24 /k32 −
6 A4. The steady state of this cycle is c4 = c(1)
k46/k54 −

2 = b(1
? A2. The steady state of this cycle is c2 = c(2)
k(1)
24 /k43). The component A1

k(2)
12 /k21). The component
24 /k32,
4 includes the cycle
4 k46/k65,

4 k46/k54, c5 = c(1)

2 A3→
2 (1
−

12 /k21, c2

k46/k65).

2 k(1)

−

i ci = 0 is approximated by relaxation
For one catalytic cycle, relaxation in the subspace
of a chain that is produced from the cycle by cutting the limiting step (Sec. 2). For reaction
networks under consideration (with one cyclic attractor in auxiliary discrete dynamical
system) the direct generalization works: for approximation of relaxation in the subspace

P

i ci = 0 it is suﬃcient to perform the following procedures:

P
•

•

To glue iteratively attractors (cycles) of the auxiliary system that are not sinks of the
whole system;
To restore these cycles from the end of the ﬁrst procedure to its beginning. For each
of cycles (including the last one that is a sink) the limited step should be deleted, and
the outgoing reaction should be recharged to the head of the limiting steps (with the
proper normalization), if it was not deleted before as a limiting step of one of the cycles.

The heads of outgoing reactions of that cycles should be recharged to the heads of the
Aq. If for a glued cycle Ak there
limiting steps. Let for a cycle this limiting step be Am →
exists an outgoing reaction Ak
Aj with the constant κ (43), then after restoration we
→
Aj with the rate constant κ. Kinetic of the resulting
add the outgoing reaction Am →
acyclic system approximates relaxation of the initial networks (under assumption of well
separated constants, for given ordering, with probability close to 1).

1 A2

3 A1

2 A3→

? A2. The reaction A1→
4→

? A1. The limiting step in this cycle is A2
2 is glued cycle A2→

Let us construct this acyclic network for the same example (44). The ﬁnal cycle is
? A1. After cutting we get A1→
1 A2
A1→
2.
2→
2→
The component A2
2 corresponds to
1 A2 (in this case, this is the only reaction from A1 to cycle; in other case
the reaction A1→
one should take the reaction from A1 to cycle with maximal constant). The limiting step
in the cycle is A1
1 A2→
4. The component
A1
6 A4 from the previous step. The limiting step in this
5 A6→
4 is the glued cycle A4→
6 A4. After restoring this cycle and cutting the limiting step, we get an acyclic
cycle is A6→
5 A6 (as one can guess from the beginning: this coincidence
3 A4→
1 A2→
system A1→
is provided by the simple constant ordering selected in (44)). Relaxation of this system
approximates relaxation of the whole initial network.

? A2. After cutting, we get a system A1→
4→

4 A5→
4 A5→

2 A3→

2 A3→

1 A2

3 A1

To demonstrate possible branching of described algorithm for cycles surgery (gluing,
restoring and cutting) with necessity of additional orderings, let us consider the following
system:

6 A3→
The auxiliary discrete dynamical system for reaction network (46) is

4 A3, A4→

1 A2→

3 A5→

2 A4→

A1→

5 A2,

(46)

A1→

1 A2→

6 A3→

2 A4→

3 A5→

4 A3.

30

A2 is k1

2 A4→
1 A2→

3 we get the new network A1→

3 A5→
6 A1
3→
23 = k24k35/k54, where kij is the rate constant for the reaction Aj →

4 A3. This cycle is not a sink for the whole
It has only one attractor, a cycle A3→
5 A2 leads from that cycle. After gluing the cycle into a
network (46) because reaction A4→
? A2. The rate constant for the reaction
vertex A1
A1
Ai in
3→
the initial network (k35 is the cycle limiting reaction). The new network coincides with its
? A2. This cycle is a sink, hence, we can start
auxiliary system and has one cycle, A2→
3→
the back process of cycles restoring and cutting. One question arises immediately: which
constant is smaller, k32 or k1
23. The smallest of them is the limiting constant, and the
answer depends on this choice. Let us consider two possibilities separately: (1) k32 > k1
23
and (2) k32 < k1
23. Of course, for any choice the stationary concentration of the source
component A1 vanishes: c1 = 0.

6 A1

(1) Let as assume that k32 > k1
is (according to (4)) c2 = bk1
A1
2 A4→
3 is a glued cycle A3→
c5 = c1
k35/k54).

23/k32, c1
3 A5→

3(1

k35/k43 −

−

23. In this case, the steady state of the cycle A2→
k1
23/k32), where b =
3k35/k43, c4 = c1
4 A3. Its steady state is c3 = c1

? A2
ci. The component
3k35/k54,

3 = b(1

6 A1

3→

−

P

23. The ﬁnal auxiliary system after gluing cycles is A1→

Let us construct an acyclic system that approximates relaxation of (46) under the same as-
sumption (1) k32 > k1
? A2.
1 A2→
? A2 from the cycle. We get an acyclic system
Let us delete the limiting reaction A1
3→
4 A3. Let us restore this
2 A4→
3 A5→
3. The component A1
A1→
3 is the glued cycle A3→
2 A4→
1 A2→
4 A3. We get an acyclic system A1→
cycle and delete the limiting reaction A5→
Relaxation of this system approximates relaxation of the initial network (46) under addi-
tional condition k32 > k1

1 A2→

6 A3→

6 A1

6 A1

3→

23.

3 A5.

(2) Let as assume now that k32 < k1
is (according to (4)) c2 = b(1
it was above: c3 = c1
c1
3).

k32/k1
3k35/k43, c4 = c1

−

? A2
23. In this case, the steady state of the cycle A2→
3 = bk32/k1
23), c1
23. The further analysis is the same as
3k35/k54, c5 = c1
k35/k54) (with another
3(1

6 A1

3→

k35/k43 −

−

Let us construct an acyclic system that approximates relaxation of (46) under assumption
(2) k32 < k1
? A2,
23. The ﬁnal auxiliary system after gluing cycles is the same, A1→
but the limiting step in the cycle is diﬀerent, A2→
3. After cutting this step, we get acyclic
3, where the last reaction has rate constant k1
1 A2←
system A1→
23.

1 A2→

6 A1

6 A1

? A1

3→

The component A1
the limiting reaction A5→
transforms into connection A5→

3 is the glued cycle A3→

4 A3. The connection from glued cycle A1

3 A5→
? A2 with the same constant k1

2 A4→

23.

4 A3. Let us restore this cycle and delete
? A2 with constant k1
23

3→

We get the acyclic system: A1→
known: k21 > k43 > k54 > k1

1 A2, A3→

3 A5→
23, and we can substitute the sign “?” by “4”: A3→

? A2. The order of constants is now
4 A2.

3 A5→

2 A4→

2 A4→

For both cases, k32 > k1
explicitly and to write the solution to the kinetic equations in explicit form.

23 = k24k35/k54) and k32 < k1

23 it is easy to ﬁnd the eigenvectors

23 (k1

31

4.3 Cycles surgery for auxiliary discrete dynamical system with arbitrary family of at-

tractors

In this subsection, we summarize results of relaxation analysis and describe the algorithm
of approximation of steady state and relaxation process for arbitrary reaction network
with well separated constants.

4.3.1 Hierarchy of cycles gluing

Let us consider a reaction network
W
is
stants. The set of vertices of
A
has the form Ai →
Aj, Ai, Aj ∈ A
reaction from
R
kji}
we deﬁne κi = maxj{
For each Ai ∈ A
if kji = 0 for all j.

with a given structure and ﬁxed ordering of con-
. Each
and the set of elementary reactions is
. The correspondent constant is kji.
. In addition, φ(i) = i

and φ(i) = arg maxj{

kji}

W

R

The auxiliary discrete dynamical system for the reaction network
deﬁned by the map φ on the set
system Φ = Φ
and the set of reactions Ai →
has the same set of vertices
A
κi. Auxiliary kinetics is described by ˙c = ˜Kc, where ˜Kij =
−

A

W

. Auxiliary reaction network

=

W

is the dynamical

VW
Aφ(i) with reaction constants
κjδij + κjδi φ(j).

V

W

are ﬁxed points Af 1, Af 2, ...

is also a sink for the reaction network

. If all attractors of the
Every ﬁxed point of Φ
system Φ
then the set of stationary distributions for the
initial kinetics as well as for the auxiliary kinetics is the set of distributions concentrated
. In this case, the auxiliary reaction network is acyclic,
the set of ﬁxed points
and the auxiliary kinetics approximates relaxation of the whole network

Af 1, Af 2, ...
}

∈ A

W

{

W

.

W

In general case, let the system Φ
have several attractors that are not ﬁxed points, but
cycles C1, C2, ... with periods τ1, τ2, ... > 1. By gluing these cycles in points, we transform
is transformed into Φ1. For
1. The dynamical system Φ
the reaction network
W
1 persists: Φ1 is the auxiliary
these new system and network, the connection Φ1 = Φ
discrete dynamical system for

into

W

W

1.

W

W

W

For each cycle, Ci, we introduce a new vertex Ai. The new set of vertices,
A1, A2, ...
∪iCi) (we delete cycles Ci and add vertices Ai).

(

1 =

A

A ∪

B (A, B

) can be separated into 5 groups:

→

∈ A

} \

{
All the reaction between A

∈ ∪iCi;
(1) both A, B /
∈ ∪iCi, but B
Ci;
(2) A /
∈
Ci, but B /
∈ ∪iCi;
(3) A
∈
Ci, B
(4) A
= j;
Cj, i
∈
Ci.
(5) A, B

∈

∈

Reactions from the ﬁrst group do not change. Reaction from the second group transforms
Ai (to the whole glued cycle) with the same constant. Reaction of the third
into A
B with the rate constant renormalization (42): let the cycle C i
type changes into Ai
→
A1, and the reaction rate
...Aτi →
be the following sequence of reactions A1 →

A2 →

→

32

6
A1). For the limiting reaction of the cycle
Ai+1 is ki (kτi for Aτi →
constant for Ai →
B, then the new
Ci we use notation klim i. If A = Aj and k is the rate reaction for A
reaction Ai
B has the rate constant kklim i/kj. This corresponds to a quasistationary
distribution on the cycle (4). The same constant renormalization is necessary for reactions
of the fourth type. These reactions transform into Ai
Aj. Finally, reactions of the ﬁfth
type vanish.

→

→

→

1. Strictly speaking, the whole network

,
After we glue all the cycles of auxiliary dynamical system in the reaction network
W
1 is not necessary, and in eﬃcient
we get
realization of the algorithm for large networks the computation could be signiﬁcantly
reduced. What we need, is the correspondent auxiliary dynamical system Φ1 = Φ
1 with
auxiliary kinetics.

W

W

W

To ﬁnd the auxiliary kinetic system, we should glue all cycles in the ﬁrst auxiliary system,
1 the reaction of
and then add several reactions: for each Ai it is necessary to ﬁnd in
the form Ai
B with maximal constant and add this reaction to the auxiliary network.
If there is no reaction of the form Ai
B for given i then the point Ai is the ﬁxed point
for

→
1 and vertices of the cycle Ci form a sink for the initial network.

→

W

W

After that, we decompose the new auxiliary dynamical system, ﬁnd cycles and repeat
gluing. Terminate when all attractors of the auxiliary dynamical system Φm become ﬁxed
points.

4.3.2 Reconstruction of steady states

After this termination, we can ﬁnd all steady state distributions by restoring cycles in the
f 2, ... be ﬁxed points of Φm. The set of steady
auxiliary reaction network
states for
. Let us
take one of these distributions, c = (cm
f 2, ...) (we mark the concentrations by the same
indexes as the vertex has; other ci = 0).

m is the set of all distributions on the set of ﬁxed points
f 1, cm

m. Let Am

f 2, ...
}

f 1, Am

f 1, Am

Am

V

V

{

To make a step of cycle restoration we select those vertexes Am
f i that are glued cycles and
substitute them in the list Am
f 1, Am
f 2, ... by all the vertices of these cycles. For each of those
cycles we ﬁnd the limiting rate constant and redistribute the concentration cm
f i between
the vertices of the correspondent cycle by the rule (4) (with b = cm
f i). As a result, we get
a set of vertices and a distribution on this set of vertices. If among these vertices there
are glued cycles, then we repeat the procedure of cycle restoration. Terminate when there
is no glued cycles in the support of the distribution. The resulting distribution is the
approximation to a steady state of
can be approximated
by this method.

, and all steady states for

W

W

, it is
In order to construct the approximation to the basis of stationary distributions of
suﬃcient to apply the described algorithm to distributions concentrated on a single ﬁxed
point Am

f j = δij, for every i.

f i, cm

W

The steady state approximation on the base of the rule (4) has ﬁrst order in the limiting
constants. The zero order approximation also makes sense. For one cycle is gives (5):

33

all the concentration is collected at the start of the limiting step. The algorithm for the
zero order approximation is even simpler than for the ﬁrst order. Let us start from the
distributions concentrated on a single ﬁxed point Am
f j = δij for some i. If this point
is a glued cycle then restore that cycle, and ﬁnd the limiting step. The new distribution
is concentrated at the starting vertex of that step. If this vertex is a glued cycle, then
repeat. If it is not then terminate. As a result we get a distribution concentrated in one
vertex of

f i, cm

.

A

4.3.3 Dominant kinetic system for approximation of relaxation

, we
To construct an approximation to the relaxation process in the reaction network
also need to restore cycles, but for this purpose we should start from the whole glued
m (not only from ﬁxed points as we did for the steady state
m on
network network
1 and so on some of glued cycles
approximation). On a step back, from the set
should be restored and cut. On each step we build an acyclic reaction network, the ﬁnal
network is deﬁned on the initial vertex set and approximates relaxation of

m to

W

A

A

A

V

m

−

.

W

−

V

To make one step back from
from
m

m let us select the vertices of
2 , .... Each Am
1 , Am
m
1. Let these vertices be Am
−
V
1, Am
1
...Am
1
1
Am
−
−
−
i1 →
iτi →
i1
V
1
steps in these cycles are Am
1
Am
. Let us substitute each vertex Am
−
−
iτi →
i1
i
vertices Am
...Am
1
m reactions Am
1
, ...Am
1
and add to
−
−
−
−
iτi
i1 →
iτi
i1
V
the cycle reactions without the limiting step) with correspondent constants from

m that are glued cycles
i corresponds to a glued cycle from
, of the length τi. We assume that the limiting
m by di
V
(that are
1.
m

Am
1
−
i2 →
1

Am
1
−
i2 →

, Am
−
i2

in
1

A

−

V

B in

i →

B with the same constant, i.e. outgoing reactions Am

If there exists an outgoing reaction Am
Am
1
−
iτi →
the heads of the limiting steps. Let us rearrange reactions from
m
These reactions have prototypes in
V
reactions. If there exists a reaction Am
i →
1
and substitute the reaction by Am
−
iτi →

m then we substitute it by the reaction
... are reattached to
i →
Am
i .
V
1 (before the last gluing). We simply restore these
B,

Am
j then we ﬁnd the prototype in
V
B with the same constant, as for Am
i →

m of the form B

→
Am
j .

1, A

→

V

m

−

−

m

1, but the reaction set diﬀers from the
After that step is performed, the vertices set is
1: the limiting steps of cycles are excluded and the outgoing
reactions of the network
reactions of glued cycles are included (reattached to the heads of the limiting steps). To
2, substitute
make the next step, we select vertices of
−
these vertices by vertices of cycles, delete the limiting steps, attach outgoing reactions to
the heads of the limiting steps, and for incoming reactions restore their prototypes from

1 that are glued cycles from

A

A

V

V

m

m

m

−

−

−

m

2, and so on.

−

V

After all, we restore all the glued cycles, and construct an acyclic reaction network on
. We call this
the set
system the dominant system of

. This acyclic network approximates relaxation of the network

and use notation dom mod(

W

A

).

W

W

34

4.3.4 Zero-one law for nonergodic multiscale networks

W

of the initial network

∪iAtt(Am
) this partition transforms into partition of
W

f i of the discrete dynamical system Φm are the glued ergodic com-
The ﬁxed points Am
ponents Gi ⊂ A
. At the same time, these points are attrac-
tors of Φm. Let us consider the correspondent decomposition of this system with parti-
m =
tion
f i). In the cycle restoration during construction of dominant system
A
dom mod(
f i) transforms
into Ui and Gi ⊂
It is straightforward to see that during construction of dominant systems for
the network
dom mod(

from
m no connection between Ui are created. Therefore, the reaction network

V
) is a union of networks on sets Ui without any link between sets.

∪iUi, Att(Am
f i) in hierarchical gluing of cycles).

Ui (and Ui transforms into Att(Am

W

A

A

=

:

W

If G1, . . . Gm are all ergodic components of the system, then there exist m independent
positive linear functionals b1(c), ... bm(c) that describe asymptotical behaviour of kinetic
Ul cA where
system when t
∈
cA is concentration of A. Hence, for the initial reaction network
with well separated
constants

) these functionals are: bl(c) =

(26). For dom mod(

→ ∞

W

W

P

A

bl(c)

cA.

≈

Ul
XA
∈

(47)

This is the zero–one law for multiscale networks: for any l, i, the value of functional bl (26)
on basis vector ei, bl(ei), is either close to one or close to zero (with probability close to 1).
We already mentioned this law in discussion of a simple example (27). The approximate
equality (47) means that for each reagent A
there exists such an ergodic component
G of
preferably into elements of G even if there exist
that A transforms when t
paths from A to other ergodic components of

→ ∞

∈ A

W

.

W

4.4 Example: a prism of reactions

Let us demonstrate work of the algorithm on a typical example, a prism of reaction
that consists of two connected cycles (Fig. 1,2). Such systems appear in many areas of
biophysics and biochemistry (see, for example, [49]).

For the ﬁrst example we use the reaction rate constants ordering presented in Fig. 1a.
For this ordering, the auxiliary dynamical system consists of two cycles (Fig. 1b) with
the limiting constants k54 and k32, correspondingly. These cycles are connected by four
reaction (Fig. 1c). We glue the cycles into new components A1
1 and A1
2 (Fig. 1d), and the
2. Following the general rule (k1 = kklim/kj),
reaction network is transformed into A1
we determine the rate constants: for reaction A1

1 ↔

A1

A1
2

k1
21 = max
{

1 →
k41k32/k21, k52, k63k32/k13}

,

and for reaction A1

A1
1

2 →

k1
12 = k36k54/k46.

There are six possible orderings of the constant combinations: three possibilities for the
21 < k1
choice of k1
12.

21 and for each such a choice there exist two possibilities: k1

12 or k1

21 > k1

35

1

1A

6

2A

7

9

3

4A

5

5A

a)

4
3A

10 8
2

6A

1A

1

1A

6

2A

4
3A

9

4A

2

5

1

1A

6

2A

7

9

3

4A

5

4
3A

10 8
2

6A

5A

b)

6A

5A

1

6

2A

4
3A

4A

2

9

5

6A

5A

d)

1
1A

1
2A

c)

1A

4A

2A

3A

5A

6A

e)

1
1A

?

?

1
2A

Fig. 1. Gluing of cycles for the prism of reactions with a given ordering of rate constants in
the case of two attractors in the auxiliary dynamical system: (a) initial reaction network, (b)
auxiliary dynamical system that consists of two cycles, (c) connection between cycles, (d) gluing
cycles into new components, (e) network

1 with glued vertices.

W

12. If k1

21 and k1

The zero order approximation of the steady state depends only on the sign of inequality
between k1
k1
12 then almost all concentration in the steady state is
accumulated inside A1
A4 we ﬁnd that in
A5 →
the steady state almost all concentration is accumulated in A4 (the component at the
A5). Finally, the eigenvector for zero
beginning of the limiting step of this cycle, A4 →
eigenvalue is estimated as the vector column with coordinates (0, 0, 0, 1, 0, 0).

2. After restoring the cycle A4 →

A6 →

21 ≫

21 ≪

1. After restoring the cycle A1 →

k1
If, inverse, k1
12 then almost all concentration in the steady state is accumulated
inside A1
A1 we ﬁnd that in the steady
A3 →
state almost all concentration is accumulated in A2 (the component at the beginning of
A3). Finally, the eigenvector for zero eigenvalue is
the limiting step of this cycle, A2 →
estimated as the vector column with coordinates (0, 1, 0, 0, 0, 0).

A2 →

For analysis of relaxation, let us analyze one of the six particular cases separately.

1. k1

21 = k41k32/k21 and k1

21 > k1

12.

1 is A1

In this case, the ﬁnite acyclic auxiliary dynamical system, Φm = Φ1, is A1

A1
2, and
1 →
A1
A3 and
2. We restore both cycles and delete the limiting reactions A2 →
1 ↔
A5. This is the common step for all cases. Following the general procedure, we
21 = k41k32/k21
A1, and the

W
A4 →
A1
substitute the reaction A1
2 by A2 →
(because A2 is the head of the limiting step for the cycle A1 →
A1
prototype of the reaction A1
2 is in that case A1 →

A4 with the rate constant k1

A2 →

A3 →

1 →

1 →

A4.

We ﬁnd the approximate system for relaxation description: reactions A3 →
A5 →
A4 with original constants, and reaction A2 →
A6 →
k1
21 = k41k32/k21.

A2 and
A4 with the rate constant

A1 →

36

This system graph is acyclic and, moreover, represents a discrete dynamical system, as it
should be (not more than one outgoing reaction for any component). Therefore, we can
estimate the eigenvalues and eigenvectors on the base of formulas (31), (33). It is easy to
determine the order of constants because k1
21 = k41k32/k21: this constant is the smallest
nonzero constant in the obtained acyclic system. Finally, we have the following ordering
2 A4.
of constants: A3→
So, the eigenvalues of the prism of reaction for the given ordering are (with high accuracy,
k41k32/k21. The relaxation
with probability close to one)
time is τ = k21/(k41k32).

5 A4, A5→

1 A2→

3 A1→

4 A6→

k13 <

k65 <

k46 <

k21 <

−

−

−

−

−

We use the same notations as in previous sections: eigenvectors li and ri correspond to
... . The
the eigenvalue
left eigenvectors li are:

κi, where κi is the reaction rate constant for the reaction Ai →
−

l1
l4

≈
≈

(1, 0, 0, 0, 0, 0), l2
(1, 1, 1, 1, 1, 1), l5

(1, 1, 1, 0, 0, 0), l3
(0, 0, 0, 0, 1, 0), l6

(0, 0, 1, 0, 0, 0),
(0, 0, 0, 0, 0, 1).

≈
≈

≈
≈

The right eigenvectors ri are (we represent vector columns as rows):

r1
r4

≈
≈

1, 0, 0, 0, 0), r2

(1,
(0, 0, 0, 1, 0, 0), r5

−

(0, 1, 0,

≈
(0, 0, 0,

1, 0, 0), r3

−
1, 1, 0), r6

(0,
≈
−
(0, 0, 0,

1, 1, 0, 0, 0),
1, 0, 1)

≈

−
The vertex A4 is the ﬁxed point for the discrete dynamical system. There is no reaction
... . For convenience, we include the eigenvectors l4 and r4 for zero eigenvalue,
A4 →
κ4 = 0. These vectors correspond to the steady state: r4 is the steady state vector, and
the functional l4 is the conservation law.

−

≈

The correspondent approximation to the general solution of the kinetic equation for the
prism of reaction (Fig. 1a) is:

(48)

(49)

(50)

c(t) =

ri(li, c(0)) exp(

κit).

−

6

Xi=1

Analysis of other ﬁve particular cases is similar. Of course, some of the eigenvectors and
eigenvalues can diﬀer.

The ﬁrst order in constants ratios approximation for the steady state is described above
as application of the rule (4) for restoring cycles. The ﬁrst order approximation for eigen-
vectors is presented in Appendix 1.

Of course, diﬀerent ordering can lead to very diﬀerent approximations. For example, let
us consider the same prism of reactions, but with the ordering of constants presented in
Fig. 2a. The auxiliary dynamical system has one cycle (Fig. 2b) with the limiting constant
k36. This cycle is not a sink to the initial network, there are outgoing reactions from its
vertices (Fig. 2c). After gluing, this cycles transforms into a vertex A1
1 (Fig. 2d). The glued
1 the rate constant for the reaction A4 →
network,
A1
k41k36/k21, k46}
.
Hence, there are not more than four possible versions: two possibilities for the choice of

1 is k54, and the rate constant for the reaction A1

1 (Fig. 2e), has two vertices, A4 and A1

A4 is k1 = max

1 →

W

{

37

1

1A

7

2A

4

10

5

4A

2

5A

a)

3
3A

6 8
9

6A

1

1A

b)

3
3A

8

6A

2A

4

5A

d)

lim

2

1

1A

2A

4

5A

10

4A

2

1

1A

2A

5

4A

2

10

9

3
3A

8

6A

3
3A

8

4

6A

5A

c)

1
1A

2A

5A

4A

1A

3A

6A

10

?

1
1A

e)

Fig. 2. Gluing of a cycle for the prism of reactions with a given ordering of rate constants in
the case of one attractors in the auxiliary dynamical system: (a) initial reaction network, (b)
auxiliary dynamical system that has one attractor, (c) outgoing reactions from a cycle, (d) gluing
of a cycle into new component, (e) network

1 with glued vertices.

W

k1 and for each such a choice there exist two possibilities: k1 > k54 or k1 < k54 (one of
them is impossible, because k46 > k54).

Exactly as it was in the previous example, the zero order approximation of the steady
state depends only on the sign of inequality between k1 and k54. If k1
k54 then almost
all concentration in the steady state is accumulated inside A1. After restoring the cycle
A3 →
A3 we ﬁnd that in the steady state almost all concentration
A1 →
is accumulated in A6 (the component at the beginning of the limiting step of this cycle,
A3). The eigenvector for zero eigenvalue is estimated as the vector column with
A6 →
coordinates (0, 0, 0, 0, 0, 1).

A5 →

A6 →

A2 →

≫

≫

If k1
k54 then almost all concentration in the steady state is accumulated inside A4.
This vertex is not a glued cycle, and immediately we ﬁnd the approximate eigenvector for
zero eigenvalue, the vector column with coordinates (0, 0, 0, 1, 0, 0).

Let us analyze the relaxation process for one of the possibilities: k1 = k46, and, therefore
k1 > k54. We restore the cycle, delete the limiting step, transform the reaction A1
A4
A4 with the same constant k1 = k46 and get the chain with ordered
into reaction A6 →
5 A4. Here the nonzero rate constants kij have the same
1 A2→
3 A1→
constants: A3→
value as for the initial system (Fig. 2a). Left eigenvectors are (including l4 for the zero
eigenvalue):

2 A6→

4 A5→

1 →

l1
l4

≈
≈

(1, 0, 0, 0, 0, 0), l2
(1, 1, 1, 1, 1, 1), l5

(1, 1, 1, 0, 0, 0), l3
(0, 0, 0, 0, 1, 0), l6

(0, 0, 1, 0, 0, 0);
(1, 1, 1, 0, 1, 1).

(51)

≈
≈

≈
≈

38

Right eigenvectors are (including r4 for the zero eigenvalue):

r1
r4

≈
≈

1, 0, 0, 0, 0), r2

(1,
(0, 0, 0, 1, 0, 0), r5

−

(0, 1, 0, 0, 0,

≈
(0, 0, 0, 0, 1,

1), r3

−
1), r6

(0,
≈
−
(0, 0, 0,

1, 1, 0, 0, 0);
1, 0, 1),

≈

−

≈

−

(52)

Here we represent vector columns as rows.

For the approximation of relaxation in that order we can use (50).

5 Limitation in modular structure and solvable modules

5.1 Modular limitation

∞

kls or kij ≫

The simplest one-constant limitation concept cannot be applied to all systems. There is
another very simple case based on exclusion of “fast equilibria” Ai ⇋ Aj. In this limit,
the ratio of reaction constants Kij = kij/kji is bounded, 0 < a < Kij < b <
, but for
kls holds. (One usually
diﬀerent pairs (i, j), (l, s) one of the inequalities kij ≪
calls these K “equilibrium constant”, even if there is no relevant thermodynamics.) W.J.
Ray (Jr.) [9] discussed that case systematically for some real examples. Of course, it is
possible to create the theory for that case very similarly to the theory presented above.
This should be done, but it is worth to mention now that the limitation concept can be
applied to any modular structure of reaction network. Let for the reaction network
W
the set of elementary reactions
∪iRi. We can
consider the related multiscale ensemble of reaction constants: let the ratio of any two
rate constants inside each module be bounded (and separated from zero, of course), but
the ratios between modules form a well separated ensemble. This can be formalized by
Ri on a time scale coeﬃcient ki. If we
multiplication of rate constants of each module
assume that ln ki are uniformly and independently distributed on a real line (or ki are
independently and log-uniformly distributed on a suﬃciently large interval) then we come
to the problem of modular limitation. The problem is quite general: describe the typical
behavior of multiscale ensembles for systems with given modular structure: each module
has its own time scale and these time scales are well separated.

is partitioned on some modules:

R

R

=

Development of such a general theory is outside the scope of our paper, and here we
just ﬁnd building blocks for the future theory, solvable reaction modules. There may be
many various criteria of selection the reaction modules. Here are several possible choices:
individual reactions (we developed the theory of multiscale ensembles of individual reac-
tions in this paper), couples of mutually inverse reactions, as we mentioned above, acyclic
reaction networks, ...

.

Among the possible reasons for selection the class of reaction mechanisms for this purpose,
there is one formal, but important: the possibility to solve the kinetic equation for every
module in explicit analytical (algebraic) form with quadratures. We call these systems
“solvable”.

39

5.2 Solvable reaction mechanisms

Let us describe all solvable reaction systems (with mass action law), linear and nonlinear.

Formally, we call the set of reaction solvable, if there exists a linear transformation of
coordinates c
a such that kinetic equation in new coordinates for all values of reaction
7→
constants has the triangle form:

This system has the lower triangle Jacobian matrix ∂ ˙ai/∂aj.

To construct the general mass action law system we need: the list of components,
A1, ... An}

and the list of reactions (the reaction mechanism):

{

=

A

where r is the reaction number, αri, βrk are nonnegative integers (stoichiometric coeﬃ-
cients). Formally, it is possible that all βk = 0 or all αi = 0. We allow such reactions.
They can appear in reduced models or in auxiliary systems.

A real variable ci is assigned to every component Ai, ci is the concentration of Ai, c is the
concentration vector with coordinates ci. The reaction kinetic equations are

where γr is the reaction stoichiometric vector with coordinates γri = βri −
the reaction rate. For mass action law,

αri, wr(c) is

where kr is the reaction constant.

Physically, equations (55) correspond to reactions in ﬁxed volume, and in more general
case a multiplier V (volume) is necessary:

dai
dt

= fi(a1, a2, ... ai).

αriAi →

Xi

Xk

βrkAk,

dc
dt

=

γrwr(c),

r
X

wr(c) = kr

cαri
i

,

Yi

d(V c)
dt

= V

γrwr(c).

r
X

αriAi →

Xk, k>i

βrkAk

40

Here we study the systems (55) and postpone any further generalization.

The ﬁrst example of solvable systems give the sets of reactions of the form

(53)

(54)

(55)

(56)

(57)

(components Ak on the right hand side have higher numbers k than the component Ai
on the left hand side, i < k). For these systems, kinetic equations (55) have the triangle
form from the very beginning.

The second standard example gives the couple of mutually inverse reactions:

αiAi ⇋

βkAk,

Xi

Xk
γ, γi = βi −
±

(58)

−

αi. The kinetic equation ˙c =
these reactions have stoichiometric vectors
(w+
w−)γ has the triangle form (53) in any orthogonal coordinate system with the last
i γici. Of course, if there are several reactions with proportional
coordinate an = (γ, c) =
stoichiometric vectors, the kinetic equations have the triangle form in the same coordinate
systems.

P

The general case of solvable systems is essentially a combination of that two (57), (58),
with some generalization. Here we follow [50] and present an algorithm for analysis of
reaction network solvability. First, we introduce a relation between reactions “rth reaction
directly aﬀects the rate of sth reaction” with notation r
s if there exists such Ai
→
that γriαsi 6
= 0)
and the rate of the sth reaction depends on Ai concentration (αsi 6
= 0). For that relation
we use r
s (“rth reaction
aﬀects the rate of sth reaction”): r
s if there exists such a sequence s1, s2, ... sq that
... sq →
r

= 0. This means that concentration of Ai changes in the rth reaction (γri 6
s. For transitive closure of this relation we use notation r

→
s2 →
s1 →

s: r

→

→

(cid:23)

(cid:23)

s.

The hanging component of the reaction network
that for all reactions
αri = 0. This means that all reaction rates do not depend on concentration of Ai. The
hanging reaction is such element of
s only if γs = λγr for
some number λ. An example of hanging components gives the last component An for the
triangle network (57). An example of hanging reactions gives a couple of reactions (58) if
they do not aﬀect any other reaction.

is such Ai ∈ A

with number r that r

W

R

(cid:23)

we should ﬁnd all hanging com-
In order to check solvability of the reaction network
, correspondingly. After that, we
and
ponents and reactions and delete them from
R1. Next, we
get a new system,
A1 and the reaction set
A1 and
should ﬁnd all hanging components and reactions for
R1. Iterate until no hanging components or hanging reactions could be found. If the ﬁnal
set of components is empty, then the reaction network
is solvable. If it is not empty,
then

W
R
W1 and delete them from

A
W1 with the component set

is not solvable.

W

W

A1, A2, A3, A4}
and
For example, let us consider the reaction mechanism with
A3. There are no
reactions A1 + A2 →
2A3, A1 + A2 →
A3. After deletion
hanging components, but two hanging reactions, A3 →
of these two reactions, two hanging components appear, A3 and A4. After deletion these
two components, we get two hanging reactions, A1 +A2 →
0 (they coincide).
We delete these reactions and get two components A1, A2 without reactions. After deletion
these hanging components we obtain the empty system. The reaction network is solvable.

=
A
{
A4, A4 →
A4 and A4 →
0, A1 +A2 →

A3 + A4, A3 →

An oriented cycle of the length more than two is not solvable. For each number of ver-

41

tices one can calculate the set of all maximal solvable mechanisms. For example, for ﬁve
components there are two maximal solvable mechanisms of monomolecular reactions:

A5;

A3 →

A4, A1 →

A5, A2 ↔

A5, A1 →

A4, A1 →

A4, A1 →

A3, A4 ↔

A3, A1 →

A5, A4 ↔
A5.

A2 →
A2, A1 →

(1) A1 →
(2) A1 →
It is straightforward to check solvability of these mechanism. The ﬁrst mechanism has
A5. After deletion of these reaction, the system
a couple of hanging reactions, A4 ↔
becomes acyclic, of the form (57). The second mechanism has two couples of hanging
A5. After deletion of these reactions, the system also trans-
reactions, A2 ↔
forms into form (57). It is impossible to add any new monomolecular reactions between
A1, A2, A3, A4, A5}
Finally, we should mention connections between solvable reaction networks and solvable
Lie algebras [51,52]. Let us remind that matrices M1, ... Mq generate a solvable Lie algebras
if and only if they could be transformed simultaneously into a triangle form by a change
of basis.

to these mechanisms with preservation of solvability.

A3 and A4 ↔

{

The Jacobian matrix for the mass action law kinetic equation (55) is:

where

J =

∂ci
∂cj !

 

=

wrJr =

r
X

Xrj

wr
cj

Mrj,

Jr = γrα⊤r diag

Mrj = αrjγrej

⊤,

1
c1

,

1
c2

(cid:26)

, ...

1
cn (cid:27)

=

1
cj

Mrj,

Xj

(59)

(60)

α⊤r is the vector row (αr1, ... αrn), ej

⊤ is the jth basis vector row with coordinates ej

⊤k = δjk.

The Jacobian matrix (59) should have the lower triangle form in coordinates ai (53) for all
nonnegative values of rate constants and concentrations. This is equivalent to the lower
triangle form of all matrices Mrj in these coordinates. Because usually there are many
zero matrices among Mrj, it is convenient to describe the set of nonzero matrices.

For the rth reaction Ir =
i

Ir. For each i = 1, ... n we deﬁne a matrix

i

αri 6

|

{

}

∈

= 0

. The reaction rate wr depends on ci if and only if

mri = 

0, 0, ... γr

... 0

.




i

|{z}




The ith column of this matrix coincides with the vector column γr. Other columns are
equal to zero. For each r we deﬁne a set of matrices
=
∪rMr.
The reaction network
generates
a solvable Lie algebra.

, and
is solvable if and only if the ﬁnite set of matrices

Mr =

mri |

M
M

Ir}

W

∈

{

i

Classiﬁcation of ﬁnite dimensional solvable Lie algebras remains a diﬃcult problem [52].

42

It seems plausible that the classiﬁcation of solvable algebras associated with reaction
networks can bring new ideas into this ﬁeld of algebra.

6 Conclusion: Concept of limit simpliﬁcation in multiscale systems

In this paper, we study networks of linear reactions. For any ordering of reaction rate
constants we look for the dominant kinetic system. The dominant system is, by deﬁnition,
the system that gives us the main asymptotic terms of the stationary state and relaxation
in the limit for well separated rate constants. In this limit any two constants are connected
by relation

or

.

≫

≪

The topology of dominant systems is rather simple; they are those networks which are
graphs of discrete dynamical systems on the set of vertices. In such graphs each vertex has
no more than one outgoing reaction. This allows us to construct the explicit asymptotics
of eigenvectors and eigenvalues. In the limit of well separated constants, the coordinates
1 or 0. All algorithms
of eigenvectors for dominant systems can take only three values:
are represented topologically by transformation of the graph of reaction (labeled by re-
action rate constants). We call these transformations “cycles surgery”, because the main
operations are gluing cycles and cutting cycles in graphs of auxiliary discrete dynamical
systems.

±

In the simplest case, the dominant system is determined by the ordering of constants.
But for suﬃciently complex systems we need to introduce auxiliary elementary reactions.
i kςi
They appear after cycle gluing and have monomial rate constants of the form kς =
i .
The dominant system depends on the place of these monomial values among the ordered
constants.

Q

Construction of the dominant system clariﬁes the notion of limiting steps for relaxation.
There is an exponential relaxation process that lasts much more than the others in (40),
(50). This is the slowest relaxation and it is controlled by one reaction in the dominant
system, the limiting step. The limiting step for relaxation is not the slowest reaction, or the
second slowest reaction of the whole network, but the slowest reaction of the dominant
system. That limiting step constant is not necessarily a reaction rate constant for the
initial system, but can be represented by a monomial of such constants as well.

The idea of dominant subsystems in asymptotic analysis was proposed by Newton and
developed by Kruskal [54]. A modern introduction with some historical review is presented
in [55]. In our analysis we do not use the degrees of small parameters (as it was done in
[27,28,29,54,55]), but operate directly with the rate constants orderings.

To develop the idea of systems with well separated constants to the state of a mathemat-
ical notion, we introduce multiscale ensembles of constant tuples. This notion allows us
to discuss rigorously uniform distributions on inﬁnite space and gives the answers to a
question: what does it mean “to pick a multiscale system at random”.

Now we have the complete theory and the exhaustive construction of algorithms for linear

43

reaction networks with well separated rate constants. There are several ways of using the
developed theory and algorithms:

(1) For direct computation of steady states and relaxation dynamics; this may be useful
for complex systems because of the simplicity of the algorithm and resulting formulas
and because often we do not know the rate constants for complex networks, and
kinetics that is ruled by orderings rather than by exact values of rate constants may
be very useful;

(2) For planning of experiments and mining the experimental data – the observable
kinetics is more sensitive to reactions from the dominant network, and much less
sensitive to other reactions, the relaxation spectrum of the dominant network is
explicitly connected with the correspondent reaction rate constants, and the eigen-
vectors (“modes”) are sensitive to the constant ordering, but not to exact values;
(3) The steady states and dynamics of the dominant system could serve as a robust ﬁrst
approximation in perturbation theory or as a preconditioning in numerical methods.

From a theoretical point of view the outlook is more important. Let us answer the question:
what has to be done, but is not done yet? Three directions for further development are
clear now:

(1) Construction of dominant systems for the reaction network that has a group of con-
between them). We considered
stants with comparable values (without relations
cycles with several comparable constants in Sec. 2, but the general theory still has
to be developed.

≫

(2) Construction of dominant systems for reaction networks with modular structure. We
can assume that the ratio of any two rate constants inside each module be bounded
and separated from zero, but the ratios between modules form a well separated
ensemble. A reaction network that has a group of constants with comparable values
gives us an example of the simplest modular structure: one module includes several
reactions and other modules arise from one reaction. In Sec. 5 we describe all solvable
modules such that it is possible to solve the kinetic equation for every module in
explicit analytical (algebraic) form with quadratures (even for nonconstant in time
reaction rate constants).

→
cB, then we can consider this reaction as B

(3) Construction of dominant systems for nonlinear reaction networks. The ﬁrst idea
here is the representation of a nonlinear reaction as a pseudomonomolecular reac-
tion: if for reaction A + B
... concentrations cA and cB are well separated, say,
cA ≫
... with rate constant depen-
dent on cA. The relative change of cA is slow, and we can consider this reaction
cB. We can
as pseudomonomolecular until the relation cA ≫
assume that in the general case only for small fraction of nonlinear reactions the
pseudomonomolecular approach is not applicable, and this set of genuinely nonlin-
ear reactions changes in time, but remains small. For nonlinear systems, even the
realization of the limiting step idea for steady states of a one-route mechanism of
a catalytic reaction is nontrivial and was developed through the concept of kinetic
polynomial [15].

→
cB changes to cA ∼

Finally, the concept of “limit simpliﬁcation” will be developed. For multiscale nonlinear
reaction networks the expected dynamical behaviour is to be approximated by the system

44

of dominant networks. These networks may change in time but remain small enough. The
corresponding structure of fast–slow time separation in phase space is not necessarily a
smooth slow invariant manifold, but may be similar to a “crazy quilt” and could consist
of fragments of various dimensions that do not join continuously.

Appendix 1: Estimates of eigenvectors for diagonally dominant matrices with
diagonal gap condition

(61)

(62)

The famous Gershgorin theorem gives estimates of eigenvalues. The estimates of cor-
respondent eigenvectors are not so well known. In the paper we use some estimates of
eigenvectors of kinetic matrices. Here we formulate and prove these estimates for general
matrices. Below A = (aij) is a complex n
(sums of non-
j,j
×
aji|
diagonal elements in rows), Qi =
(sums of non-diagonal elements in columns).

n matrix, Pi =

aij|

=i |

=i |

P

j,j

Gershgorin theorem ([36], p. 146): The characteristic roots of A lie in the closed region
GP of the z-plane

Analogously, the characteristic roots of A lie in the closed region GQ of the z-plane

P

[i

[i

GP =

GP
i

(GP

i =

z

{

z

|

aii| ≤

.

Pi}

−

GQ =

GQ
i

(GQ

i =

z

{

z

|

aii| ≤

.

Qi}

−

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Areas GP

i and GQ

i are the Gershgorin discs.

Gershgorin discs GP
= j. If discs GP
i
(i = 1, . . . n) are isolated, then the spectrum of A is simple, and each Gershgorin disc GP
i
contains one and only one eigenvalue of A ([36], p. 147). The same is true for discs GQ
i .

i (i = 1, . . . n) are isolated, if GP

j = ∅ for i

i ∩

GP

Below we assume that Gershgorin discs GQ
all i, j

aii −
Let us introduce the following notations:

|

ajj|

i (i = 1, . . . n) are isolated, this means that for

> Qi + Qj.

(63)

Qi
aii|

|

= εi,

aij|
|
ajj|
|

= χij

εi =

 

δli

!

, min
j

|

Xl

ajj|

aii −
aii|
|

= gi.

(64)

Usually, we consider εi and χij as suﬃciently small numbers. In contrary, gi should not
be small, (this is the gap condition). For example, if for any two diagonal elements aii, ajj
either aii ≫
GQ
Let λ1 ∈
< Q1). Let us estimate the correspondent
a11|
right eigenvector x(1) = (xi): Ax(1) = λ1x(1). We take x1 = 1 and write equations for xi
(i
= 1):

ajj or aii ≪
1 be the eigenvalue of A (

ajj, then gi & 1 for all i.

λ1 −

|

where θ1 = λ1 −

a11,

θ1|

|

a11 −

θ1)xi +

aijxj =

ai1,

−

=1,i
Xj, j

(aii −
< Q1.

(65)

45

6
6
6
6
6
Let us introduce new variables

In these variables,

˜x = (˜xi), ˜xi = xi(aii −

a11) (i = 2, . . . n).

1

 

−

˜xi +

a11 !

aij
ajj −

=1,i
Xj, j

˜xj =

ai1,

−

a11

θ1
aii −
B)˜x =

˜a1, where ˜a1 is a vector column with coordinates ai1.
or in matrix notations: (1
because of gap condition and smallness of εi and χij we χij we can consider matrix B as
B) is reversible (for detailed estimate
a small matrix, for assume that
of

see below).

< 1 and (1

−

−

−

B

B

k

k

k

k

For ˜x we obtain:

and for residual estimate

For eigenvector coordinates we get from (67):

and for residual estimate

˜x =

˜a1 −
−

B(1

−

B)−

1˜a1,

B(1

B)−

k

−

1˜a1k ≤

1

B

k
k
B
− k

k

.

˜a1k

k

xi =

ai1
aii −

−

a11 −

1˜a1)i

(B(1

B)−
a11

−
aii −

(B(1

|

B)−
a11|

−
aii −

|

1˜a1)i|

B

k
k
B
− k

˜a1k
k
aii −

a11|

.

k

|

≤

1

(66)

(67)

(68)

(69)

(70)

Let us give more detailed estimate of residual. For vectors we use l1 norm:
The correspondent operator norm of matrix B is

=

x
k

k

.

xi|

|

P

B

k

k

= max
=1 k
x
k
k

Bx

k ≤

max
j

.

bij|

|

Xi

With the last estimate for matrix B (66) we ﬁnd:

bii| ≤

|

Q1
aii −

|

≤

a11|

ε1
g1 ≤

ε
g

,

=

bij|

|

aij|
|
ajj −

|

≤

a11|

χij
gj ≤

χ
g

(i

= j),

(71)

where ε = maxi εi, χ = maxi,j χij, g = mini gi. By deﬁnition, ε
the simple estimate holds:
nε/(g
get:

|
nε) (under condition g > nε). Finally,

ε/g. Therefore,

bij| ≤

k
˜a1k

k ≤

Bx

−

≥
nε/g and,

χ, and for all i, j
B

/(1

B

)

≤
k
= Q1 and for residual estimate we

− k

k

k

≤

g(g

nε)

(i

= 1).

(72)

xi +
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai1
aii −

a11 (cid:12)
(cid:12)
(cid:12)
(cid:12)

k
nε2

−

46

6
6
6
More accurate estimate can be produced from inequalities (71), if it is necessary. For our
goals it is suﬃcient to use the following consequence of (72):

xi| ≤

|

χ
g

+

nε2

g(g

nε)

−

(i

= 1).

(73)

With this accuracy, eigenvectors of A coincide with standard basis vectors, i.e. with eigen-
vectors of diagonal part of A, diag

.

a11, . . . ann}

{

Appendix 2: Time separation and averaging in cycles

In Sec. 2 we analyzed relaxation of a simple cycle with limitation as a perturbation of the
linear chain relaxation by one more step that closes the chain into the cycle. The reaction
rate constant for this perturbation is the smallest one. For this analysis we used explicit
estimates (13) of the chain eginvectors for reactions with well separated constants.

Of course, one can use estimates (30), (31) (32) and (33) to obtain a similar perturbation
analysis for more general acyclic systems (instead of a linear chain). If we add a reaction
to an acyclic system (after that a cycle may appear) and assume that the reaction rate
constant for additional reaction is smaller than all other reaction constants, then the
generalization is easy.

This smallness with respect to all constants is required only in a very special case when
Aj (with the rate constant kji) and there is no
the additional reaction has a form Ai →
... in the non-perturbed system. In Sec. 5 and Appendix 1
reaction of the form Ai →
we demonstrated that if in a non-perturbed acyclic system there exists another reaction
κi only. This
of the form Ai →
inequality allows us to get the uniform estimates of eigenvectors for all possible values of
other rate constants (under the diagonally gap condition in the non-perturbed system).

... with rate constant κi, then we need inequality kji ≪

A2 →
...
... rate constants κi. We add a perturbation A1 →

For substantiation of cycles surgery we need additional perturbation analysis for zero
A1 with reaction
eigenvalues. Let us consider a simple cycle A1 →
0 (from A1 to nothing) with
Ai →
rate constant ǫκ1. Our goal is to demonstrate that the zero eigenvalue moves under this
ǫw∗(1 + χw), the correspondent left and right eigenvectors r0 and
perturbation to λ0 =
l0 are r0
i = 1 + χli, and χw, χri and χli are uniformly small for a given
suﬃciently small ǫ under all variations of rate constants. Here, w∗ is the stationary cycle
reaction rate and c∗i are stationary concentrations for a cycle (2) normalized by condition
λ0 is ǫ-small with respect to any reaction of the cycle:

i = c∗i (1 + χri) and l0

i c∗i = 1. The estimate ǫw∗ for

An →

→

−

w∗ = κic∗i < κi for all i (because c∗i < 1), and ǫw∗
P

κi for all i.

≪

−

The kinetic equation for the perturbed system is:

˙c1 =

(1 + ǫ)κ1c1 + κncn,

˙ci =

κici + κi

1ci

1 (for i

= 1).

(74)

−

−

−

−

47

6
6
In the matrix form we can write

˙c = Kc = (K0 −

ǫk1e1e1

⊤)c,

(75)

where K0 is the kinetic matrix for non-perturbed cycle. To estimate the right perturbed
eigenvector r0 and eigenvalue λ0 we are looking for transformation of matrix K into the
θre1
⊤, where K is a kinetic matrix for extended reaction system with
form K = Kr −
i ri = 1. In that case, r is the eigenvector, and
components A1, ...An, Krr = 0 and
λ =

θr1 is the correspondent eigenvalue.

P

−

To ﬁnd vector r, we add to the cycle new reactions A1 →
and subtract the correspondent kinetic terms from the perturbation term ǫe1e1
that, we get K = Kr −

Ai with rate constants ǫκ1ri
⊤c. After

⊤ with θ = ǫk1 and

θre1

(Krc)1 =
(Krc)i =

ǫk1(1
k1c1 −
−
kici + ǫk1ric1 + ki
−

−

r1)c1 + kncn,
1ci

−

−

1 for i > 1

We have to ﬁnd a positive normalized solution ri > 0,
i ri = 1 to equation Krr = 0. This
is the ﬁxed point equation: for every positive normalized r there exists unique positive
i c∗i (r) = 1. We have to solve the
normalized steady state c∗(r): Krc∗(r) = 0, c∗i > 0,
equation r = c∗(r). The solution exists because the Brauer ﬁxed point theorem.

P

P

ǫk1rir1 = ki

If r = c∗(r) then kiri −
1. We use notation w∗i (r) for the correspondent
stationary reaction rate along the “non-perturbed route”: w∗i (r) = kiri. In this notation,
w∗i (r)
< ǫk1r1).
ǫriw∗1(r) = w∗i
−
Assume ǫ < 1/4 (to provide 1

w∗1(r)
< ǫw∗1(r) (or
ǫ) < 1 + 2ǫ). Finally,

w∗i (r)
|
2ǫ < 1/(1

1(r). Hence,

kiri −

k1r1|

1ri

−

−

−

|

|

−

ri =

1
ki

−
±
1 + χi
1
kj

j

P

= (1 + χi)c∗i

χi|
where the relative errors
|
non-perturbed system. For cycles with limitation, ri ≈
the eigenvalue we obtain

< 3ǫ and c∗i = c∗i (0) is the normalized steady state for the
< 3ǫ. For

(1 + χi)klim/ki with

χi|

|

λ0 =
=

ǫw∗1(r) =
ǫw∗(1 + χ) =

−

−
−

ǫw∗i (r)(1 + ςi)

ǫkic∗i (0)(1 + χ)

−

|

|

χ
|

χ
|

ςi|

< 3ǫ.

< ǫ and

for all i, with
< 3ǫ. Therefore, λ0 is ǫ-small rate constant ki of
the non-perturbed cycle. This implies that λ0 is ǫ-small with respect to the real part of
every non-zero eigenvalue of the non-perturbed kinetic matrix K0 (for given number of
components n). For the cycles from multiscale ensembles these eigenvalues are typically
ki for non-limiting rate constants, hence we proved for λ0 even more
real and close to
than we need.

−

|

Let us estimate the correspondent left eigenvector l0 (a vector row). The eigenvalue is
known, hence it is easy to do just by solution of linear equations. This system of n
1
equations is:

−

l1(1 + ǫ)k1 + l2k1 = λ0l1,

liki + li+1ki = λ0li, i = 2, ...n

1.

(79)

−

−

−

48

(76)

(77)

(78)

For normalization, we take l1 = 1 and ﬁnd:

l2 =

+ 1 + ǫ

l1,

li+1 =

+ 1

li i > 2.

(80)

λ0
k1

 

!

λ0
ki

 

!

Formulas (77), (78) and (80) give the backgrounds for surgery of cycles with outgoing
reactions. The left eigenvector gives the slow variable: if there are some incomes to the
cycle, then

˙c1 =

(1 + ǫ)κ1c1 + κncn + φ1(t),

˙ci =

κici + κi

1ci

1 + φi(t) (for i

= 1)

(81)

−

−

−

−
and for slow variable

c =

lici we get

P

e

c
d
dt
e

= λ0

c +

liφi(t).

Xi

e

(82)

0 with rate constants k = ǫki give the same eigenvalue

This is the kinetic equation for a glued cycle. In the leading term, all the outgoing reactions
Ai →
Of course, similar results for perturbations of zero eigenvalue are valid for more general
ergodic chemical reaction network with positive steady state, and not only for simple
cycles, but for cycles we get simple explicit estimates, and this is enough for our goals.

ǫw∗ (78).

−

Acknowledgements. This work was supported by British Council Alliance Franco-
British Research Partnership Programme.

References

[1] A. N. Gorban and I. V. Karlin,

Invariant manifolds for physical and chemical kinetics,

volume 660 of Lect. Notes Phys. Springer, Berlin-Heidelberg-New York, 2005.

[2] A.

N.

Method
of invariant manifold for chemical kinetics, Chem. Eng. Sci., 58, 21 (2003), 4751–4768.
Preprint online: http://arxiv.org/abs/cond-mat/0207231.

Gorban

Karlin,

and

V.

I.

[3] Rate-controlling step, in: IUPAC Compendium of Chemical Terminology, Electronic version,

http://goldbook.iupac.org/R05139.html .

[4] Rate-determining step (rate-limiting step),

in:

IUPAC Compendium of Chemical

Terminology, Electronic version, http://goldbook.iupac.org/R05140.html .

[5] H. S. Johnston, Gas phase reaction theory, Roland Press, New York, 1966.

[6] R. K. Boyd, Some common oversimpliﬁcations in teaching chemical kinetics, J. Chem. Educ.

55 (1978), 84–89.

[7] D. B. Northrop, Minimal kinetic mechanism and general equation for deiterium isotope
eﬀects on enzymic reactions: uncertainty in detecting a rate-limiting step, Biochemistry 20
(1981), 4056–4061.

[8] D. B. Northrop, Uses of isotope eﬀects in the study of enzymes, Methods 24 (2001), 117–124.

49

6
[9] W. J. Ray (Jr.), A rate–limiting step: a quantitative deﬁnition. Application to steady–state

enzymic reactions, Biochemistry 22 (1983), 4625–4637.

[10] G. C. Brown and C. E. Cooper, Control analysis applied to a single enzymes: can an isolated

enzyme have a unique rate–limiting step? Biochem. J. 294 (1993), 87–94.

[11] A. Cornish-Bowden, M. L. Cardenas, Control on Metabolic Processes, Plenum Press, New

York, 1990.

[12] B. N. Kholodenko, H. V. Westerhoﬀ, G. C. Brown, Rate limitation within a single enzyme

is directly related to enzyme intermediate levels, FEBS Letters 349 (1994) 131–134.

[13] G. S. Yablonskii, M.Z. Lazman and V.I. Bykov, Stoichiometric number, molecularity and

multiplicity, React. Kinet. Catal. Lett. 20 (1-2) (1982), 73–77.

[14] M. Z. Lazman and G. S. Yablonskii, Kinetic polynomial: a new concept of chemical kinetics,
In: Patterns and Dynamics in Reactive Media, The IMA Volumes in Mathematics and its
Applications, Springer Verlag, Berlin - Heidelberg - New York, (1991), 117–150.

[15] M. Z. Lazman, G. S. Yablonskii, Steady-state rate of complex reactions under limitation and
near equilibrium (one-route mechanism of catalytic reaction), React. Kinet. Catal. Lett. 37
(2) (1988), 379–384.

[16] V. M. Cheresiz, G. S. Yablonskii, Estimation of relaxation times for chemical kinetic

equations (linear case), React. Kinet. Catal. Lett, 22, 1-2 (1983), 69–73.

[17] A. N. Gorban, O. Radulescu, Dynamical robustness of biological networks with hierarchical

distribution of time scales, arXiv e-print q-bio.MN/0701020, 2007.

[18] Li, G., C. Rosenthal, and H. Rabitz, High dimensional model representations. J. Phys.

Chem. A. 105 (2001), 7765–7777.

[19] Li, G., S.-W. Wang, H. Rabitz, S. Wang, and P. Jaﬀe, Global uncertainty assessments by

high dimensional model representations (HDMR), Chem. Eng. Sci., 57 (2002), 4445-4460.

[20] X-j. Feng, S. Hooshangi, D. Chen, G. Li, R. Weiss, and H. Rabitz, Optimizing Genetic

Circuits by Global Sensitivity Analysis, Biophys J. 87(4) (2004), 2195-2202.

[21] L. Carroll, (C. L. Dodgson), Mathematical Recreations of Lewis Carroll: Pillow Problems

and a Tangled Tale, Dover, 1958.

[22] R. K. Guy, There are three times as many obtuse-angled triangles as there are acute-angled

ones, Mathematics Magazine 66 (3) (1993), 175–179.

[23] S. Portnoy, A Lewis Carroll Pillow Problem: Probability of an Obtuse Triangle, Statistical

[24] B. Eisenberg, R. Sullivan, Random Triangles in n Dimensions, The American Mathematical

Science 9 (2) (1994), 279–284.

Monthly 103, 4 (1996), 308–318.

72–75.

[25] R. Falk, E. Samuel-Cahn, Lewis Carroll’s Obtuse Problem, Teaching Statistics 23 (3) (2001),

[26] G. S. Yablonskii, V. I. Bykov, A. N. Gorban, V. I. Elokhin, Kinetic models of catalytic
reactions. Comprehensive Chemical Kinetics, Vol. 32, Compton R. G. ed., Elsevier,
Amsterdam (1991).

50

[27] M. I. Vishik and L. A. Ljusternik, Solution of some perturbation problems in the case of
matrices and self-adjoint or non-selfadjoint diﬀerential equations. I, Russian Math. Surveys,
15(3) (1960), 1-73.

[28] V. Lidskii, Perturbation theory of non-conjugate operators. U.S.S.R. Comput. Math. and

Math. Phys., 6 (1) (1965), 73–85.

[29] M. Akian, R. Bapat, S. Gaubert, Min-plus methods in eigenvalue perturbation theory and

generalised Lidskii–Vishik–Ljusternik theorem, arXiv e-print math.SP/0402090.

[30] G. L. Litvinov, V. P. Maslov (Eds.), Idempotent mathematics and mathematical physics,

Contemporary Mathematics, AMS, Providence, RI, 2005.

[31] S. Albeverio, J. Fenstad, R. Hoegh-Krohn and T. Lindstrom, Nonstandard methods in

stochastic analysis and mathematical physics. Academic Press, Orlando etc., 1986.

[32] M. Eigen, Immeasurably fast reactions, Nobel Lecture, December 11, 1967, In: Nobel
Lectures, Chemistry 1963-1970, Elsevier Publishing Company, Amsterdam, 1972, 170–203.

[33] G. S. Yablonskii , V. M. Cheresiz, Four types of relaxation in chemical kinetics (linear case),

React. Kinet. Catal. Lett 24, (1-2) (1984), 49–53.

[34] M. Gromov, Metric structures for Riemannian and non-Riemannian spaces. Progress in

Mathematics, 152. Birkhauser Boston, Inc., Boston, MA, 1999.

[35] A. Gorban, Order-disorder separation: Geometric revision, Physica A 374, 85–102.

[36] M. Marcus and H. Minc, A survey of matrix theory and matrix inequalities, Dover, New-

York, 1992.

[37] R.S. Varga, Gerschgorin and His Circles, Springer series in computational Mathematics, 36,

Springer, Berlin – Heidelberg – New York, 2004

[38] W. Rudin, Functional Analysis, McGraw-Hill, New York, 1991.

[39] G. Birkhoﬀ, A note on topological groups, Composito Math. 3 (1936), 427–430.

[40] E. Hewitt, A. Ross, Abstract Harmonic Analysis, Vol. 1, Springer, Berlin – G¨ottingen –

[41] R. von Mises, The Mathematical Theory of Probability and Statistics, Academic Press,

Heidelberg, 1963.

London, 1964.

[42] R. Carnap, Logical Foundations of Probability, University of Chicago Press, Chicago, 1950.

[43] M. Adamaszek, Picking a random integer, Newsletter of the European Mathematical Society,

Issue 62, December 2006, 21–23.

Probab. Appl. 46 (2002), 256–273.

[44] A. Yu. Khrennikov, Interpretations of probability and their p-adic extensions. Theory

[45] P. A. Loeb, Conversion from nonstandard to standard measure spaces and applications in

probability theory, Trans. Amer. Math. Soc. 211 (1975), 113-122.

[46] W.D. Neumann, Hilbert’s 3rd Problem and Invariants of 3-manifolds, In: Geometry &
Topology Monographs, Vol. 1: The Epstein Birthday Schrift, University of Warwick,
Coventry, UK, 383–411.

51

[47] L. Robbiano, Term orderings on the polynomial ring, In: Proc. EUROCAL 85, vol. 2, ed. by
B. F. Caviness, Lec. Notes in Computer Sciences 204, Springer, Berlin – Heidelberg – New
York – Tokyo, 1985, 513–518.

[48] G.-M. Greuel, G. Pﬁster, A Singular Introduction to Commutative Algebra, Springer, Berlin

– Heidelberg – New York, 2002.

[49] M. Kurzynski, A synthetic picture of intramolecular dynamics of proteins. Towards a
contemporary statistical theory of bio-chemical processes, Prog. Biophys. Mol. Biol. 69(1)
(1998), 23–82.

[50] A. N. Gorban, V. I. Bykov, G. S. Yablonskii, Essays on chemical relaxation, Novosibirsk:

[51] N. Jacobson, Lie algebras, Republication of the 1962 original, Dover Publications, Inc., New

[52] W. A. de Graaf, Lie Algebras: Theory and Algorithms, North-Holland Mathematical Library,

36. Amsterdam, Elsevier, 2000.

[53] W. A. de Graaf, Classiﬁcation of Solvable Lie Algebras, Experimental Mathematics, 14 (1)

[54] M. D. Kruskal, Asymptotology, In: Mathematical Models in Physical Sciences, ed. by S.

Dobrot, Prentice-Hall, Englewood Cliﬀs, New Jersey, 1963, 17-48.

[55] R. B. White, Asymptotic Analysis of Diﬀerential Equations, Imperial College Press & World

Scientiﬁc, London, 2006.

Nauka, 1986.

York, 1979.

(2005), 15–25.

52


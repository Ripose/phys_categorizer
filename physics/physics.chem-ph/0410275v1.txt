4
0
0
2
 
t
c
O
 
8
2
 
 
]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[
 
 
1
v
5
7
2
0
1
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

On the eﬃciency of exchange in parallel tempering Monte Carlo simulations

Cristian Predescu
Department of Chemistry and Kenneth S. Pitzer Center for
Theoretical Chemistry, University of California, Berkeley, CA 94720

Mihaela Predescu
Department of Mathematics, Bentley College, Waltham, MA 02452

Cristian V. Ciobanu
Division of Engineering, Colorado School of Mines, Golden, CO 80401
(Dated: February 20, 2014)

We introduce the concept of eﬀective fraction, deﬁned as the expected probability that a conﬁg-
uration from the lowest-index replica successfully reaches the highest-index replica during a replica
exchange Monte Carlo simulation. We then argue that the eﬀective fraction represents an adequate
measure of the quality of the sampling technique, as far as swapping is concerned. Under the hy-
pothesis that the correlation between successive exchanges is negligible, we propose a technique for
the computation of the eﬀective fraction, technique that relies solely on the values of the acceptance
probabilities obtained at the end of the simulation. The eﬀective fraction is then utilized for the
study of the eﬃciency of a popular swapping scheme in the context of parallel tempering in the
canonical ensemble. For large-dimensional oscillators, we show that the swapping probability that
minimizes the computational eﬀort is 38.74%. By studying the parallel tempering swapping eﬃ-
ciency for a 13-atom Lennard-Jones cluster, we argue that the value of 38.74% remains roughly the
optimal probability for most systems with continuous distributions that are likely to be encountered
in practice.

PACS numbers: 05.10.Ln, 02.70.Tt
Keywords: Monte Carlo, parallel tempering, replica exchange, exchange eﬃciency

I.

INTRODUCTION

Since its introduction in the context of Monte Carlo
simulations,1,2,3 the technique of exchanging conﬁgura-
tions between replicas characterized by slightly diﬀer-
ent parameters has been widely employed as a means to
accelerate the convergence of Monte Carlo simulations.
Perhaps the most widely utilized technique is parallel
tempering in the canonical ensemble,3,4,5 where the in-
dex parameter is the temperature. However, the index
parameter may also be the energy, as in the case of paral-
lel tempering in the microcanonical ensemble,6 the chem-
ical potential, as in the hyperparallel tempering method,7
a delocalization parameter, as in the q-jumping Monte
Carlo method,8 or suitable modiﬁcations of the poten-
tial, as in the Hamiltonian replica exchange method.9,10

Irrespective of the index parameter that is utilized,
the main features of the replica exchange technique are
as follows.2,3,4,5 Instead of one, a number of N parallel
replicas of the system are simulated using Monte Carlo
sampling or molecular dynamics, for example. This is
called the parallel step. The N diﬀerent replicas have
distributions that are characterized by diﬀerent values of
an index parameter, for example, diﬀerent temperatures.
The replicas are usually ordered with the index parame-
ter in a monotonic fashion. The Markov chains utilized
for sampling in the parallel step have diﬀerent equilibra-
tion times. For deﬁniteness and perhaps after reordering,
we may assume that the lowest-index Markov chain equi-

librates the slowest, whereas the highest-index Markov
chain is the fastest.
In the swapping step, exchanges
between replicas of diﬀerent indexes are attempted peri-
odically or randomly in order to induce mixing between
the Markov chains utilized for the parallel step. The
replica exchanges are accepted with the Metropolis et al
acceptance probability11,12 and rejected with the remain-
ing probability. This way, detailed balance is enforced
and the sampled distributions remain stationary with re-
spect to the swapping step. The induced mixing helps
the lowest-index Markov chain to equilibrate faster, by
“borrowing” the property of fast equilibration from the
highest-index Markov chain.

The rate at which conﬁgurations leave the lowest-index
Markov chain and successfully climb the ladder of index
parameter values up to the highest-index chain plays a
crucial role in ensuring an eﬃcient mixing. The fraction
of conﬁgurations that do so during the Monte Carlo sim-
ulation will be called the “eﬀective fraction.” We point
out that the exchange step is not capable of destroying
or creating new conﬁgurations. In the worst case, an un-
suitable conﬁguration originating from the lowest-index
replica has to reach the highest-index replica in order to
be destroyed and replaced by a more favorable conﬁg-
uration.
It is then apparent that the higher the eﬀec-
tive fraction is, the better the mixing. The purpose of
the present paper is to provide a means to compute the
eﬀective fraction. The work is facilitated by the obser-
vation that, in the limit that the states of the parallel
Markov chains become uncorrelated between two succes-

sive swapping events, the study of the mixing eﬃciency
turns out to be independent of the particular sampling
technique utilized for the parallel step.

In Section II, we perform a complete mathematical
treatment of the problem of computing the eﬀective frac-
tion for the most general cyclic swapping strategy. At
the end of the section, we make the simplifying assump-
tion that the Monte Carlo states between two successive
swapping events are independent. Using this, we demon-
strate that the computation of the eﬀective fraction is re-
duced to the case of a cyclic composition of discrete-state
temporally homogeneous Markov chains with transition
probabilities given by the average swapping probabilities.
Section II is mathematically quite involved. However,
short of an exact mathematical treatment, there appears
to be no other way to demonstrate how the problem of
computing the eﬀective fraction simpliﬁes in the limit
of uncorrelated exchanges. Familiarity with the general
theory of discrete-state Markov chains, for instance as
provided in Ref. 13, is assumed. For the practitioner
that is more interested in applying the developed theory,
we have detailed the main steps involved in the compu-
tation of the eﬀective fraction in Section III, even if at
the risk of reiterating some of the information already
provided in Section II.

The eﬀective fraction concept can be utilized to study
the eﬃciency of various swapping strategies. It can also
be utilized for the determination of optimal acceptance
probabilities. We illustrate this use by studying the op-
timal eﬃciency for parallel tempering simulations in the
canonical ensemble. We work with the popular two-step
swapping strategy, in which exchanges to the nearest
neighbors of lower and higher index are attempted pe-
riodically, in a successive fashion. We demonstrate that
the optimal acceptance probability for high-dimensional
oscillators is about 38.74%, an interesting result which,
by means of a numerical example, is shown to hold for
more complex distributions. The result is interesting not
only because of its apparent applicability to many of the
systems with continuous distributions, but also because
the optimal acceptance probability is actually not very
high. This observation appears to somewhat contradict
the general belief that low acceptance probabilities au-
In fact, we
tomatically imply a low mixing eﬃciency.
demonstrate that the acceptance probabilities may lie
anywhere in the interval [7%, 82%], with a penalty of at
most 100% in the computational cost.

We end our journey with a set of recommendations
for the practitioner. Perhaps, the most important one
is that the values of the acceptance probabilities alone
should not be used as a means of judging the quality
of the mixing eﬃciency. Rather, the eﬀective number of
swapping events, which is the product between the eﬀec-
tive fraction and the total number of swapping attempts,
must be calculated and employed for such purposes.

2

(1)

(2)

II. THE EFFECTIVE FRACTION OF SWAPS
AS A MEASURE OF EXCHANGE EFFICIENCY

In standard implementations of parallel Monte Carlo
techniques, (statistically) independent Markov chains
{X i
n; n ≥ 0} deﬁned on the same state space Ω and hav-
ing same or diﬀerent stationary normalized distributions
{ρi(x); x ∈ Ω, 1 ≤ i ≤ N } are run in parallel on diﬀerent
machines. In the replica exchange Monte Carlo, swaps
(also called exchanges) are attempted between states X i
n
and X j
n of two “neighboring” Markov chains of indexes i
and j. The swaps are then accepted with the conditional
probability

a(X i

n, X j

n) = min

ρi(X j
ρi(X i

n)ρj(X i
n)
n)ρj(X j
n) )

,

1,

(

or rejected with the remaining probability

r(X i

n, X j

n) = 1 − a(X i

n, X j

n).

This acceptance/rejection rule ensures that the detailed
balance condition is satisﬁed and that the product proba-
bility distribution ρ(x1)ρ(x2) · · · ρ(xN ) is stationary with
respect to the swapping step.2,3 The expected (average)
values of the acceptance and rejection probabilities are
deﬁned by

aij = E

a(X i

n, X j
n)

=

ρi(x)ρj (y)a(x, y)dxdy (3)

(cid:2)

and

ZΩ ZΩ

(cid:3)

rij = E

r(X i

n, X j
n)

= 1 − aij ,

(4)

(cid:3)
respectively. These averages are to be evaluated during
the Monte Carlo simulation itself.

(cid:2)

The term “neighboring” refers to the fact that the
Markov chain distributions are normally chosen to de-
pend continuously upon an index parameter T , so that
ρi(x) = ρ(x; Ti). The chains are indexed in such a way
that the parameter increases (or decreases) with i, i.e.,

Tmin = T1 < T2 < · · · < TN = Tmax.

For instance, in the case of parallel tempering Monte
Carlo simulations in the canonical ensemble, the index
parameter is the temperature. By decreasing the diﬀer-
ence between the index parameter values of two succes-
sive Markov chains, one increases the exchange probabil-
ity. Indeed, by continuity, it is clear from Eq. (1) that

a(X i

n, X j

n) → 1

as Ti → Tj.

In replica exchange, the normal course of the simula-
tion is interrupted by swaps between neighboring conﬁg-
urations. The simulation occurs as follows

. . . X i

n−1

Met−→ X i
n

swap
−→ X ′i
n

Met−→ X i

n+1 . . . ,

3

(5)

where M et refers to a normal Metropolis update, al-
though other sampling techniques may be utilized for the
parallel step. If X i
n is involved in swapping at time n,
then X i
n always denotes the state of the chain i prior to
the replica exchange. If needed, the state after exchange
will be denoted by a prime sign.

A cyclic swapping strategy consists of a sequence
of direct transition matrices T1, T2, . . . , TP that de-
scribe parallel exchanges between some pairs of replicas.
The parallel exchanges are attempted at time intervals
k1, k2, . . . , kP . We shall call a succession of such P swap-
ping events a cycle. The entire sequence is then repeated
periodically every K = k1 + k2 + · · · + kP Monte Carlo
steps. There are two main requirements we need to en-
force in choosing the transition matrices, with the sec-
ond one, which ensures ergodicity of the swapping event,
being stated later in the present section. The ﬁrst re-

quirement is that any given matrix Tk must contain only
direct transitions between disjoint pairs of particles, so
that any given particle is involved in exchanges with at
most one other particle. An example of such a matrix
is given by Eq. (5). On the main diagonal, a matrix Tk
may contain either 1 or some rejection probabilities for
direct transitions. The oﬀ-diagonal terms are either 0
or the corresponding acceptance probabilities for direct
transitions. If two particles i and j are involved in direct
transitions during an exchange described by Tk at some
ii and t(k)
Monte Carlo step n, then the entries t(k)
jj of the
n), whereas the entries t(k)
matrix Tk must equal r(X i
ij
and t(k)
n). The particles i and j are
not allowed to participate at any other exchanges during
this particular swapping step.

ji must equal a(X i

n, X j

n, X j

0
...

(cid:17)

Tk(X 1

n, X 2

n, . . . , X N

n ) =

r(X 1

a(X 1

n, X 3
n)
0
n, X 3
n)
0
















r(X 2

a(X 2

0
n, X 4
n)
0
n, X 4
n)
0
...

a(X 1

r(X 1

n, X 3
n)
0
n, X 3
n)
0

0
...

a(X 2

n) 0 · · ·

0 · · ·

0
n, X 4
0
n, X 4
0
...

0 · · ·

n) 0 · · ·
1 · · ·
...
. . .

r(X 2
















The entries of the direct transition matrices are ran-
dom variables depending upon the state of the Monte
Carlo replicas prior to an exchange. It is convenient to
restrict our attention only to the states that are involved
in swaps and arrange them as the sequence Y1, Y2, . . ..
Accordingly, if [n] is the largest integer strictly smaller
than n/P and {n} = n − [n]P is the remainder, then

Yn =

X 1

[n]K+k{n}

[n]K+k{n}

[n]K+k{n}

, X 2

, . . . , X N

,

(6)

(cid:16)

for n = 1, 2, 3, . . .. The collection of future states, from
the perspective of somebody that starts observing the
simulation at time n, is denoted by

Fn = (Yn, Yn+1, Yn+2, . . .).

(7)

Of course, as far as swapping is concerned, the entire
simulation is represented by F1.

The Monte Carlo simulation is assumed to have equi-
librated. Moreover, the transition or the group of
Metropolis transitions responsible for the parallel step
are assumed time independent. In these conditions, the
sequences Yn, Yn+1, Yn+2, . . . are stationary with period
P for all n ≥ 1, i.e., they have the same distribution
as the shifted sequences Yn+P , Yn+P +1, Yn+P +2, . . .. The
reason the sequence Y1, Y2, Y3, . . . is not stationary with

respect to shifting by 1 is that the transition matri-
ces T1, T2, . . . , TP are generally diﬀerent. However, the
marginal distributions of Y1, Y1, Y3, . . . are the same for
an equilibrated Monte Carlo simulation and are given
by the stationary distribution ρ(x1)ρ(x2) . . . ρ(xN ). As
a consequence, the expected values of the entries of the
direct transition matrices Tk(Yn) are independent of n,
a property already utilized in Eqs. (3) and (4). We shall
denote these common expected values by τ (k)
ij , whereas
the entire matrix of expected values is denoted by Tk.

Because they are stochastic, in fact doubly-stochastic
by symmetry, the matrices Tk(Yn) may be interpreted
as representing transition probabilities of a discrete-state
temporally inhomogeneous Markov chain {Θn(F1); n ≥
1} with random transition probabilities, acting on the
space of indexes I = {1, 2, . . . , N }. More precisely, con-
sidering again the unique decomposition n = [n]P + {n},
with [n] the largest integer strictly smaller than n/P , the
transition probabilities of the Markov chain Θn(F1) are
deﬁned by the equation

P [Θn+1(F1) = j|Θn(F1) = i] = t({n})

(Yn).

ij

(8)

Markov chains with random transition probabilities are
collections of temporally inhomogeneous Markov chains,
each characterized by transition probabilities depending

on the particular course of the simulation, as speciﬁed by
the values of the random vectors Y1, Y2, Y3, . . . that make
up F1. Conditioned on the starting position Θ1(F1) = i,
the random variable Θn+1(F1) represents the state (the
replica index) reached by a conﬁguration that originally
started from replica i, given a particular course of the
simulation F1 and given that n swapping events have oc-
curred. The signiﬁcance of the transition probabilities
appearing in Eq. (8) is as follows. Given a particular
course of the simulation F1, the transition probability
t({n})
(Yn) represents the probability that a conﬁguration
ij
that has reached replica i, after n swapping attempts,
ends up in replica j, after yet another attempt. As we
shall see, our development builds heavily on this inter-
pretation.

The second requirement that must be enforced in
choosing the transition matrices Tk is that the product

TP TP −1 · · · T1

(9)

of the matrices made up of the expected values of the en-
tries of Tk must induce an ergodic discrete-state Markov
chain on the index space I = {1, 2, 3, . . .}. Ergodicity (or
irreducibility) means that starting from any arbitrary in-
dex i, we may reach any other state j in a ﬁnite number
of steps. Clearly, ergodicity of the product matrix given
by Eq. (9) is a natural requirement for any replica ex-
change strategy, because accessibility from a state to any
other state is required. We shall accept without proof
that the ergodicity of this product matrix is a suﬃcient
condition to ensure ergodicity of the temporally inhomo-
geneous Markov chain Θn(F1). For temporally inhomo-
geneous Markov chains with random transition proba-
bilities, the condition of ergodicity can be stated as the
requirement that, starting from any position i and at any
time n, the probability to reach any other state j in a ﬁ-
nite number of steps is non-zero, for almost all histories
of the simulation.

As previously mentioned, the purpose of replica ex-
change is to reduce the equilibration time of the Monte
Carlo chain characterized, say, by the index parame-
ter Tmin, by using the property of fast equilibration of
the chain characterized by the index parameter Tmax.
Of course, many of the intermediate chains do partic-
ipate and help the lowest-temperature chain to equili-
brate faster. However, we shall address the worst sce-
nario setting, in which the conﬁgurations in the lowest-
temperature replica have to climb the entire ladder of
temperatures in order to get destroyed and be replaced
with more favorable conﬁgurations. In other words, the
implicit assumption is that a conﬁguration receives a sta-
tus of “energetically unfavorable” just because it origi-
nates from the lowest-temperature replica. The conﬁgu-
ration cannot change its status as the result of the modi-
ﬁcations undergone while climbing through the ladder of
temperatures. It may only change its status upon arrival
at the highest-temperature replica.

Consistent with our worst scenario model, a direct
measure of the eﬃciency of the replica exchange method

4

is represented by the fraction fN of conﬁgurations that
start in the replica of lowest index 1 and successfully
reach the replica of highest index N , during the simula-
tion. For the remainder of this section, our goal is the
computation of this quantity, which shall be called the
eﬀective fraction. Although the “unfavorable” tag that a
conﬁguration from the lowest-index replica receives may
be justiﬁable only at the beginning of the simulation,
we shall assume that the Markov chain is equilibrated.
To parallel similar situations in the chemical literature,
the reader is reminded of the classical transition state
theory for reaction rates, where the standard model is
a steady supply of reactant molecules that are, never-
theless, in thermodynamic equilibrium during the entire
reaction. A more stringent assumption, which requires
special care, is that the time intervals k1, k2, . . . , kP be-
tween consecutive replica exchanges are suﬃciently large
that the variables Y1, Y2, Y3, . . . may be assumed indepen-
dent. We shall make this assumption at the very end of
the section, in order to obtain formulas that are feasible
to evaluate.

1j

For deﬁniteness, let us assume that we are at the swap-
ping time n. Again, [n] represents the largest integer
strictly smaller than n/P , whereas {n} is the remainder
{n} = n − [n]P . The remainder {n} always takes values
in the state space I = {1, 2, . . . , N }. For any n ≥ 1, the
matrix T
{n} contains the transition probabilities of the
Markov chain Θn(F1) at time n. The probability that the
conﬁguration in the lowest-temperature replica jumps di-
rectly into a state j ∈ {2, 3, . . . N } is t({n})
(Yn). This is
so because the acceptance/rejection step for exchanges
between the replicas 1 and j is the result of a random
variable that only depends on the current state of the two
replicas [according to Eq. (1)] and is independent of the
other random variables controlling the course of the sim-
ulation. As already stated, given a particular course of
the simulation F1, the time evolution of the index of any
tagged conﬁguration is described by the Markov chain
Θn(F1). The conﬁguration may also remain in the state
j = 1 with probability t({n})
If so, then it will get a
second chance to thermalization the next swapping time.
A conﬁguration starting in some position j ≥ 2 at the
next swapping time n+1 may return to the conﬁguration
1 without ﬁrst passing through the conﬁguration N . Let
Qj(Fn+1) be the probability of this event. Notice that,
consistent with the Markov property of the Monte Carlo
simulation, Qj(Fn+1) depends only on the future swap-
ping states. The conﬁguration may also go to the replica
j = N without ﬁrst passing through the j = 1. We let
Pj(Fn+1) be the probability of the second event. Clearly,
PN (Fn+1) = 1 and QN (Fn+1) = 0. More generally, er-
godicity requires that

11

.

Qj(Fn+1) + Pj (Fn+1) = 1,

(10)

that is, starting from j at any time n + 1, the conﬁgura-
tion eventually hits either the lowest-index or the highest-
Indeed, for
index conﬁgurations, with probability one.

instance, a non-vanishing probability for the event that
starting at j we may never reach 1 contradicts the ergod-
icity of the Markov chain Θn(F1).

The event that a conﬁguration starting at a position
j ≥ 2 returns to replica 1 without ﬁrst passing through
replica N is not an eﬀective event. In this case, the con-
ﬁguration will get a second chance to jump out of replica
1 and reach the highest-index replica in future exchanges.
The reader should notice that the conﬁguration will even-
tually hit replica 1 at some time m + n in the future, by
ergodicity. Even if it reaches replica N after that, it
should not be counted as eﬀective at the current time
n, because we risk to double count the single event of
reaching N : once for the present time n and again for
the future time m + n. The complementary event that
the conﬁguration reaches replica N without ﬁrst passing
through replica 1 is eﬀective. Summing over all inter-
mediate states j = 2, it follows that the the probability
that the conﬁguration exits replica 1 at time n and then
reaches replica N , without passing through replica 1 ﬁrst,
is given by the formula

t({n})
1j

(Yn)Pj (Fn+1).

(11)

N

j=2
X

If the entire Monte Carlo simulation consists of NS
swapping events, then the fraction of conﬁgurations that
successfully change their status from unfavorable to fa-
vorable is

t({n})
1j

(Yn)Pj(Fn+1).

1
NS

NS

N

j=2
X

n=1
X
that

variables
Remembering
Y1, Y2, Y3, . . .
stationary with period P and in-
voking the ergodic theorem, we learn that the above
expression converges to

collection

the

of

is

fN =

1
P

P

N

n=1
X

j=2
X

E

t(n)
1j (Yn)Pj(F{n+1})
i
h

,

(12)

almost surely.

Although the eﬀective fraction fN can be computed
during the Monte Carlo simulation itself, the procedure
is rather tedious. To obtain an expression for fN that
is easier to evaluate, we shall make the simplifying as-
sumption that the time intervals k1, k2, . . . , kP between
consecutive replica exchanges are so large that the vari-
ables Y1, Y2, Y3, . . . are independent. Replacing the ex-
pected values in Eq. (12) by products of expected values,
we obtain

fN =

1
P

P

N

n=1
X

j=2
X

E

E

t(n)
1j (Yn)
i
h

P

N

=

1
P

n=1
X

j=2
X

Pj (F{n+1})

(cid:2)
(cid:3)
1j p({n+1})
τ (n)

j

,

(13)

5

where we have introduced the notation p(n)
j = E [Pj(Fn)]
for the average probabilities that a conﬁguration starting
in the position j ≥ 2 at time n ∈ {1, 2, . . . , P } ﬁrst hits
replica N .

j

j1 (Yn).

We are now ready to establish a recurrence relation
between the average hitting probabilities p(n)
. If we start
from state j = 2, 3, . . . , N − 1 at time n ≤ P , we may
jump to 1 with probability t(n)
In this case, the
conditional probability to reach N before reaching 1 is
zero. We may also jump to N with probability t(n)
jN (Yn).
In this case, the conditional probability to ﬁrst reach N
is 1. Finally, we may jump to i = 2, 3, . . . , N − 1 with
probability t(n)
ji (Yn). The conditional probability to ﬁrst
reach N from the new position, while starting at time n+
1, is Pi(Fn+1). Summing over all intermediate states i,
we must obtain the conditional probability to ﬁrst reach
N while starting from j at time n. Therefore,

Pj(Fn) = t(n)

jN +

t(n)
ji (Yn)Pi(Fn+1),

(14)

for all 2 ≤ j ≤ N − 1 and 1 ≤ n ≤ P . Taking the
expected values of the left and right-hand sides and again
using independence and stationarity with period P , we
obtain the system of equations

j = τ (n)
p(n)

jN +

ji p({n+1})
τ (n)

i

,

(15)

N −1

i=2
X

N −1

i=2
X

for all 2 ≤ j ≤ N − 1 and 1 ≤ n ≤ P . Of course p(n)
for all 1 ≤ n ≤ P .

N = 1

j

The system of equations obtained in the preceding
paragraph may or may not uniquely determine the av-
erage hitting probabilities. For example, let us imagine
the swapping strategy in which direct exchanges are only
attempted between replica 1 and the rest of the replicas.
In this case, τ (n)
ji = δji for all i, j ≥ 2. Any arbitrary hit-
ting probabilities p(n)
satisfy Eq. (15). However, in this
situation, it is clear that p(n)
j = 0 for all 2 ≤ j ≤ N − 1.
More generally, the system of equations given by Eq. (15)
is to be solved iteratively starting with the initial guess
p(1)
j = 0, for all 2 ≤ j ≤ N − 1. At the next step of iter-
ation, with the help of Eq. (15), we compute the values
p(n)
for n = P and 2 ≤ j ≤ N − 1. The procedure is re-
j
peated down to n = 1 and new cycles of computation are
performed until self-consistency is attained. The result-
ing solution is used together with Eq. (13) to compute
the eﬀective fraction fN . To conclude, if we assume that
the Monte Carlo states involved in exchanges are inde-
pendent, then all that is needed for the computation of
the eﬀective fraction fN are the expected values τ (n)
of
ij
the direct transition matrices.

III. MORE ON THE COMPUTATION OF THE
EFFECTIVE FRACTION

In the preceding section, we have argued that the ef-
fective fraction fN constitutes a measure of the swapping
eﬃciency that is more adequate than, for example, the
values of the acceptance and rejection probabilities alone.
The eﬀective fraction represents the fraction of conﬁgura-
tions originating from the lowest-index replica that suc-
cessfully climb the ladder of index parameter values and
reach the highest-index replica, during the Monte Carlo
simulation. If the replica exchanges are performed suf-
ﬁciently infrequently that the exchanged conﬁgurations
may be considered statistically independent, the eﬀective
fraction is solely determined by the values of the average
acceptance/rejection exchange probabilities, as explained
below.

Using the values of the average acceptance/rejection
probabilities determined during the Monte Carlo simu-
lation, one constructs the transition matrices {Tk; 1 ≤
k ≤ P } that make up a swapping cycle. For example,
a popular swapping strategy involves pairs of neighbor-
ing replicas only.
In the ﬁrst step, parallel swaps are
attempted between the pairs (1, 2), (3, 4), etc. At the
second step, parallel swaps are attempted between the
pairs (2, 3), (4, 5), etc. The whole exchange cycle is then
and τ (2)
repeated periodically. The entries τ (1)
of the
ij
ij
two transition matrices are those from the arrays

and

· · ·
0
0
r12 a12
0
· · ·
0
a12 r12
0
0
r34 a34 · · ·
0 a34 r34 · · ·
0
...
...
...
. . .

...

0
0
1
0 r23 a23
0 a23 r23
0
0
0
...
...
...

0 · · ·
0 · · ·
0 · · ·
r45 · · ·
...
. . .









,









T1 = 






T2 =









(16)

(17)

respectively.

The selection of the swapping strategy must be done in
such a way that a) any given replica must not be involved
in more than one exchange at any particular swapping
time and b) the Markov chain induced by the product
matrix

TP TP −1 · · · T1

must be ergodic. These two properties are obviously re-
spected by the two transition matrices given by Eqs. (16)
and (17), provided that none of the acceptance probabil-
ities a12, a23, . . . , aN −1,N is zero.

Next, we compute the average probabilities p(n)

that a
conﬁguration starting at the position j = 2, 3, . . . N − 1

j

6

and at the simulation time n = 1, 2, . . . , P reaches the
replica N without hitting replica 1 ﬁrst. In order to do
so, we solve iteratively the system of equations

N −1

j = τ (n)
p(n)

jN +

ji p({n+1})
τ (n)

i

,

(18)

i=2
X
for j = 2, 3, . . . , N − 1 and n = 1, 2, . . . , P . The reader is
reminded that

{n + 1} =

(cid:26)

if n = P,

1,
n + 1, otherwise.

(19)

The iteration is started with n = P , using the initial
guess p(1)
j = 0 for all j = 2, 3, . . . , N −1. After the hitting
probabilities p(n)
for n = P are computed, the procedure
is repeated for n = P − 1, down to n = 1. New cycles
of computation are then initiated, until self-consistency
is attained.

j

Once the solution of Eq. (18) is established, the eﬀec-
tive fraction fN is computed with the help of the formula

fN =

1
P

P

N

n=1
X

j=2
X

1j p({n+1})
τ (n)

j

.

(20)

The values p(n)
deﬁnition.

N needed in Eq. (20) are equal to 1, by

If the acceptance probabilities for exchanges between
neighboring replicas are tuned to be equal to the common
value ai,i+1 = a, then an explicit formula for fN can be
derived in the case of the two-step swapping strategy
presented earlier in the section. The formula reads

fN (a) =

a
2

1
(1 − a)N + 2a − 1

(21)

and can be proven by induction as follows. Jumps out
of the replica 1 cannot be realized during the action of
the transition matrix T2. When the transition matrix T1
acts, there is a direct jump only in replica j = 2, jump
that happens with probability a. Since P = 2, Eq. (20)
reduces to

fN =

p(2)
2 (N ),

a
2

where we have explicitly stated that the hitting proba-
bility p(2)

2 depends on N .

It remains to establish that a particle starting at the
position 2 during the second swapping event reaches
replica N before hitting replica 1 with probability

p(2)
2 (N ) =

1
(1 − a)N + 2a − 1

.

(22)

Trivially, p(2)
2 (2) = 1, so Eq. (22) is veriﬁed for N = 2.
Let N ≥ 3. All events that eventually reach N without
ﬁrst hitting 1 also hit N − 1 without ﬁrst hitting 1. The

probability that the second, larger class of events happens
is p(2)
2 (N − 1). The events always arrive at replica N − 1
through an exchange between replicas N − 2 and N − 1.
At the next exchange, the conﬁgurations in N − 1 may
jump into N directly, with probability a. These events
are eﬀective. They may also get rejected with probability
1 − a. In this second case, they may hit 1 before hitting
N , event that happens with probability p(2)
2 (N ) by the
symmetry of the problem, or may hit N before hitting 1
with the remaining probability, 1−p(2)
2 (N ), case in which
they are again eﬀective. Combining everything, we end
up with the recursive formula

2 (N ) = p(2)
p(2)

2 (N − 1)

a + (1 − a)

2 (N )

.

1 − p(2)
h

io

n

The reader may easily check that Eq. (22) does, indeed,
verify the recursion. The proof is concluded by the prin-
ciple of induction. Perhaps, the most important step in
the proof is the observation that the probability to hit 1
without ﬁrst hitting N , while starting from the replica
N − 1, is the same as the probability to hit N without
ﬁrst hitting 1, while starting from replica 2, by symmetry.
Eq. (21) shows that the eﬀective fraction decreases
with the increase of the number of replicas, an expected
result. However, for a given system, the number of repli-
cas necessary to achieve a given acceptance probability
is also a function of a. Therefore, the eﬀective fraction
becomes a function depending solely on the acceptance
probability a. We shall use this observation in the con-
text of parallel tempering in the canonical ensemble, to
study the optimal value for the acceptance probability.

IV. OPTIMAL ACCEPTANCE PROBABILITIES
FOR PARALLEL TEMPERING IN THE
CANONICAL ENSEMBLE

If the length of a replica exchange Monte Carlo simula-
tion consists of NMC parallel steps and if complete cycles
of swaps are performed every K steps, with P swapping
events per cycle, then the total number of eﬀective swap-
ping events is

Nef = fN

NMC P
K

,

(23)

where fN is the eﬀective fraction and N is the number of
replicas. The total number of eﬀective swapping events
represents a direct measure of the quality of the exchange.
We believe, it is the correct number to be reported for
any replica exchange Monte Carlo simulation.

The computational eﬀort to obtain Nef eﬀective ex-
changes is proportional to the number of replicas, more
precisely to NMC N . Thus, the eﬃciency of the swapping
strategy is measured by the ratio

7

which gives the computational time per eﬀective swap-
ping event. Among all swapping strategies that ensure
the same number of eﬀective swaps Nef , the most eﬃ-
cient ones are those that maximize the fraction N/fN .

In this section, we study the optimal acceptance prob-
ability for parallel tempering simulations in the canonical
ensemble. We work with the two-step swapping strategy
considered in the preceding section. We also assume that
the acceptance probabilities for swaps between neighbor-
ing replicas have been tuned to be equal to the common
value ai,i+1 = a. Our goal is to establish the acceptance
probability that maximizes the eﬃciency of the replica
exchange method, as measured by the ratio

N/fN = 2N [(1 − a)N + 2a − 1]/a.

(25)

We ﬁrst study the case of high-dimensional harmonic os-
cillators, for which an analytical formula for the depen-
dence of the number of replicas N with the acceptance
probability is available. Then, we perform a numerical
study for a 13-particle Lennard-Jones cluster.

In the case of high-dimensional oscillators, the num-
ber of replicas necessary to achieve a given acceptance
probability a is given by the formula14

N (a) ≈ t

erf−1(1 − a) ,

(26)

where

(cid:14)

t = (2d)1/2 ln(βmax/βmin)/4

is a parameter independent of the acceptance probability
a. The function erf−1(x) represents the inverse of the
error function.

Substituting the expression of N (a) in Eq. (25), and
using that (1 − a)N (a) ≫ 2a − 1, we obtain the following
asymptotic formula

N/fN ≈ 2t2(1/a − 1)

erf−1(1 − a)

−2

,

(27)

(cid:2)

(cid:3)

which is valid for large values of t. We readily see that
the minimum of N/fN is independent of the value of
the parameter t. This minimum, which is unique, as
apparent from Fig. 1, can be determined numerically and
has the approximate value of ao = 0.3874. Thus, the
optimal acceptance probability for harmonic oscillators
is about 38.74%.

Let us analyze a little more closely the issue of eﬃ-
ciency, by studying an example. Assume that for a cer-
tain value of t, the number of replicas necessary to achieve
the optimal acceptance probability ao = 0.3874 is N . If,
to the contrary, we choose to run our simulation with an
acceptance of a = 0.666, then Eq. (26) demands that the
number of replicas be 2N . Thus, the computational ef-
fort is twice as large. The number of eﬀective swapping
events is

NMC N
Nef

=

N
fN

K
P

,

(24)

Nef = fN

NMC P
K

=

0.305
t

NMC P
K

.

3.0

2.0

1.0

t
r
o
f
f
e
 
l
a
n
o
i
t
a
t
u
p
m
o
C

8

 80

 60

 40

 20

s
a
c
i
l
p
e
r
 
f
o

 
r
e
b
m
u
N

LJ13
ho34

= 0.3874

0.5

0.0

1.0

 0

0.0

0.5

1.0

FIG. 1: Computational costs per eﬀective swapping event,
as functions of the acceptance probability. The costs are rel-
ative to their values at ao = 0.3874, The theoretical curve
for high-dimensional oscillators (solid line) and the eﬃciency
curve obtained experimentally for the LJ13 cluster (dashed
line) are purposefully superimposed to demonstrate their like-
ness. Also shown is the interval of acceptance probabilities
for which the increase in the computational eﬀort is at most
100%, relative to the optimal value.

FIG. 2: Number of replicas necessary to achieve a prescribed
acceptance probability a. The number of replicas necessary
for the LJ13 cluster (dashed lines) is roughly 70% larger than
for a corresponding 34-dimensional harmonic oscillator (solid
line). The 5 missing degrees of freedom, up to 3 · 13 = 39,
correspond to translations and rotations of the cluster. Since
they do not aﬀect the exchange acceptance probabilities, these
degrees of freedom are not considered.

If we apply the optimal strategy, the apparent number of
eﬀective replicas is

Nef =

0.193
t

NMC P
K

,

a smaller number. However, the optimal strategy utilizes
only half of the number of processors. We can utilize
the other half to run a second, statistically independent,
simulation. The number of eﬀective swaps for the same
computational eﬀort becomes twice as large, that is,

Nef =

0.386
t

NMC P
K

.

The ratio 0.386/0.305 = 1.267 measures the loss of eﬃ-
ciency when the acceptance a = 0.666 is utilized. It says
that, if the non-optimal strategy is utilized, one must in-
vest 26.7% more computational eﬀort to obtain the same
number of eﬀective swapping events.

The reader may argue that 26.7% is not always a crit-
ical saving. It is common practice to attempt a modi-
ﬁcation of the algorithm only if the computational time
can be reduced to half or less. From Fig. 1, where we
present the computational costs N/fN relative to their
optimal values, we see that the eﬃciency remains good
for a large range of acceptance probabilities. In fact, the
computational costs increase by more than 100% of the
optimal value only if the acceptance ratio falls outside
the interval [7%, 82%]. Thus, except for certain special
simulations for which a correct allocation of computa-
tional resources is critical, there seems to be little to be

gained by tuning the acceptance probabilities to the op-
timal value, as long as the acceptance probabilities are
maintained in the [7%, 82%] interval.

We have also computed an eﬃciency curve for the Ne
realization of the Lennard-Jones cluster made up of 13
particles. The physical parameters of the system and
the Monte Carlo simulation technique are the same as
in Ref. 14. In the range of temperatures employed, 3 K
to 30 K, the system undergoes two phase transitions:
a solid-liquid transition at about 10 K and a liquid-gas
transition at about 18 K, respectively. Thus, the simula-
tion can be considered representative of what a chemical
physicist is likely to encounter in practice. In the interval
of temperature speciﬁed, we have run diﬀerent parallel
tempering simulations for diﬀerent numbers of replicas,
using the two-step swapping strategy. For each num-
ber of replicas, we have determined the schedule of tem-
peratures that makes all acceptance probabilities con-
stant, using the adaptive procedure of Hukushima and
Nemoto.3

The resulting acceptance probabilities have been uti-
lized to construct a function that expresses the depen-
dence of the number of replicas with the acceptance prob-
ability. This function is plotted in Fig. 2, where we also
plot the number of replicas for a 34-dimensional harmonic
oscillator, for comparison purposes. Owing to the in-
crease in the heat capacity at the two phase-transition
points, the number of replicas necessary to attain a pre-
scribed acceptance probability is roughly 70% larger, in
the case of the LJ13 cluster. Nevertheless, the two curves

are quite similar in shape, an early hint that the shapes
of the eﬃciency curves are also similar, upon normal-
ization. Because N (a) is a discontinuous function (the
number of replicas can only be an integer), we have de-
cided to linearly interpolate the values of N (a), so that
to obtain a continuous dependence and avoid certain os-
cillations in the plots. The interpolated values have been
utilized together with Eq. (25) to produce an eﬃciency
curve. As readily seen from Fig. 1, the computational
costs, relative to the optimal value, are almost identi-
cal to those for high-dimensional oscillators. Therefore,
we expect that the conclusions drawn for harmonic os-
cillators remain generally true of many of the systems
with continuous distributions that may be encountered
in practical applications.

V. CONCLUSIONS

Contrary to the general belief, the values of the accep-
tance probabilities alone do not represent a measure of
the quality of the simulation. Rather, they measure the
eﬃciency of the simulation. The quality of the simulation
is more adequately measured by the number of eﬀective
swapping events, which is the product between the to-
tal number of swapping events and the eﬀective fraction.
The eﬀective fraction, the computation of which has been
detailed in Section III, represents the expected probabil-
ity that a conﬁguration originating from the lowest-index
replica eventually reaches the highest-index replica. We
believe the practitioners in the ﬁeld are better served if
they utilize the total number of eﬀective swapping events
as a measure of the quality of the simulation. The in-
vested computational eﬀort is minimal and the result is
also useful for other purposes as, for instance, in studying
the relative eﬃciency of alternative swapping strategies.
We have provided an example of such a study, by de-
termining the optimal acceptance probability for parallel
tempering simulations in the canonical ensemble.

For parallel tempering simulations of systems with con-
tinuous distributions, the acceptance probability that en-
sures the highest eﬃciency (the highest number of ef-
fective swaps per unit of computational cost) is around
38.74%, the optimal acceptance probability for high-
dimensional oscillators. We hope, this has been proven

9

convincingly enough by the study we have performed on
the swapping eﬃciency for the 13-atom Lennard-Jones
cluster. Another interesting ﬁnding is that the accep-
tance probability may take any value between 7% and
82%, with an increase of at most a factor of 2 in the
computational eﬀort.

For those simulations where an eﬃcient allocation of
computational resources is required, as is the case when
parallel tempering is utilized as a stochastic minimiza-
tion technique, an acceptance probability close to 38.74%
should be employed. The computational resources that
are saved can be better utilized to improve the quality
of the sampling for the parallel step or to run additional
statistically independent simulations. In cases where the
number of intermediate temperatures is larger than the
optimal number because it is dictated by other consider-
ations (for instance, to ensure that a temperature plot is
suﬃciently detailed), the eﬃciency can be increased by
employing swapping strategies that propose more aggres-
sive moves, to the next-to-nearest neighbor, for example.
An important assumption in the present development
has been that the sampling responsible of the paral-
lel step (Metropolis or otherwise) eﬀectively decorrelates
the states involved in successive swapping attempts. Of
course, this is unlikely to be true of most simulations that
are encountered in practice. Further research is necessary
in order to understand how this correlation aﬀects the ef-
fective fraction or the optimal acceptance probabilities.
Yet again, we conclude that the computational resources
saved by employing an optimal swapping strategy may
ﬁnd a better utilization in reducing the correlation of the
Markov chain responsible for the parallel step, for exam-
ple, by increasing the number of intermediate Metropolis
steps.

Acknowledgments

This work was supported in part by the National Sci-
ence Foundation Grant Number CHE-0345280, the Di-
rector, Oﬃce of Science, Oﬃce of Basic Energy Sciences,
Chemical Sciences, Geosciences, and Biosciences Divi-
sion, U.S. Department of Energy under Contract Num-
ber DE AC03-65SF00098, and the U.S.-Israel Binational
Science Foundation Award Number 2002170.

1 R. H. Swendsen and J.-S. Wang, Phys. Rev. Lett. 57, 2607

2 C. J. Geyer, in Computing Science and Statistics: Pro-
ceedings of the 23rd Symposium on the Interface, ed. E.
M. Keramigas, (Interface Foundation: Fairfax, 1991), pp.
156 - 163.

3 K. Hukushima and K. Nemoto, J. Phys. Soc. Jpn. 65, 1604

(1986).

(1996).

5 U. H. E. Hansmann, Chem. Phys. Lett. 281, 140 (1997).
6 F. Calvo, J. P. Neirotti, D. L. Freeman, and J. D. Doll, J.

Chem. Phys. 112, 10350 (2000).

7 Q. Yan and J. J. de Pablo, J. Chem. Phys. 111, 9509

8 I. Andricioaei and J. E. Straub, J. Chem. Phys. 107, 9117

(1999).

(1997).

9 Y. Sugita, A. Kitao, and Y. Okamoto, J. Chem. Phys. 113,

4 J. P. Neirotti, F. Calvo, D. L. Freeman, and J. D. Doll, J.

6042 (2000).

Chem. Phys. 112, 10340 (2000).

10 H. Fukunishi, O. Watanabe, and S. Takada, J. Chem. Phys.

116, 9058 (2002).

11 N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A.
M. Teller, and E. Teller, J. Chem. Phys. 21, 1087 (1953).
12 M. Kalos and P. Whitlock, Monte Carlo Methods (Wiley-

Interscience, New York, 1986).

13 C. M. Grinstead and J. L. Snell, Introduction to Probabil-
ity: Second Revised Edition (AMS, Providence, 1997).
14 C. Predescu, M. Predescu, and C. V. Ciobanu, J. Chem.

Phys. 120, 4119 (2004).

10


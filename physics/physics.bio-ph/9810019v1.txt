Markov analysis of stochastic resonance in a periodically driven
integrate-ﬁre neuron

Hans E. Plesser∗ and Theo Geisel
Max-Planck-Institut f¨ur Str¨omungsforschung and Fakult¨at f¨ur Physik, Universit¨at G¨ottingen,
Bunsenstraße 10, 37073 G¨ottingen, Germany
(February 2, 2008)

Abstract

We model the dynamics of the leaky integrate-ﬁre neuron under periodic
stimulation as a Markov process with respect to the stimulus phase. This
avoids the unrealistic assumption of a stimulus reset after each spike made
in earlier work and thus solves the long-standing reset problem. The neuron
exhibits stochastic resonance, both with respect to input noise intensity and
stimulus frequency. The latter resonance arises by matching the stimulus
frequency to the refractory time of the neuron. The Markov approach can
be generalized to other periodically driven stochastic processes containing a
reset mechanism.

87.10.+e, 05.40.+j, 02.50.Ey, 02.50.Ga

8
9
9
1
 
t
c
O
 
2
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
9
1
0
0
1
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Typeset using REVTEX

∗Electronic address: plesser@chaos.gwdg.de

1

I. INTRODUCTION

Periodically modulated stochastic processes have been studied intensely over the last two
decades under the paradigm of stochastic resonance: the transduction of signals is optimal
in the presence of a particular amount of noise. First suggested to explain the periodicity of
ice-ages [1], stochastic resonance has since been demonstrated in a wide range of experiments
and the underlying mechanisms are well understood. A recent review of the ﬁeld is given
in [2].

The concept of stochastic resonance has met with particular attention in the neuro-
sciences [3–7]. The brain achieves an enormous signal processing performance in the presence
of noise from a wide range of sources, ranging from stochastic membrane channel openings
on a molecular level, via highly irregular ﬁring patterns of individual neurons to distracting
stimuli in perception. The improvement of signal transduction on all of these levels has now
been demonstrated experimentally [8–10]. Recently, the ﬁrst direct evidence for the behav-
ioral relevance of stochastic resonance has been reported [11], underlining the importance
of stochastic resonance in neurobiology.

In short, neurons are threshold devices that receive an input I(t) which charges the
membrane of the neuron like a leaky capacitor. When the potential v(t) across the membrane
reaches a threshold Θ, a spike is ﬁred: the membrane potential makes a brief but strong
excursion (duration
100mV). This spike is transmitted as output to
other neurons. After the spike, the membrane potential is reset to a resting value v0, some
30mV below the threshold [12]. As the shape of the spikes is stereotypical, information is
only conveyed by the spike times.

2ms, amplitude

≈

≈

This has led to the leaky integrate-ﬁre model of neuronal dynamics [13]. In between two

spikes, the membrane potential is governed by

τm ˙v(t) =

v(t) + I(t) + ζ(t) .

−

(1)

Here, τm is the time-constant of the membrane, which represents the internal time-scale of the
neuron and ζ(t) is an as yet undeﬁned noise process, comprising, e.g., stochastic membrane
potential ﬂuctuations and irregular input to the neuron from sources uncorrelated to I(t).
As the potential reaches the threshold, a spike is recorded and the potential is reset to
v(t) = v0 instantaneously.

−

P

k δ(t

tk) at the times of threshold crossings

For Gaussian white noise ζ(t) the evolution of the membrane potential v(t) from reset
potential to threshold is equivalent to an Ornstein–Uhlenbeck process with drift I(t) and an
absorbing boundary at v = Θ. The output of the neuron is modeled as a sequence of delta
(spike
pulses f (t) =
train). The spike train is a stochastic point process, speciﬁed entirely by the spike times
tk

.
}
This biologically most interesting stochastic process has so far escaped a rigorous anal-
ysis, in spite of several partially successful attempts [14,15]. For a list of open issues see
Sec. V.C.4 of the review by Gammaitoni et al. [2]. This is in marked contrast to the treatment
of mathematically more accessible, but biologically less plausible models, such as bistable
dynamic systems [16–19] and threshold devices without reset [20–22], in which stochastic
resonance has been well established.

v(t) = Θ

t
|

=

tk

{

}

{

}

{

2

The essential diﬃculty arises from the reset after each spike: there is no well-deﬁned
membrane potential distribution for asymptotic times, as used in the case of reset-free
threshold detectors. Instead we have to analyze each inter-spike-interval separately and then
put these pieces together to obtain the spike train as a whole. To facilitate this, past work has
assumed that the durations of all inter-spike-intervals (τk = tk
tk−1) were identically and
independently distributed (i.i.d.), i.e. that the spike train is a stationary renewal process [23].
But in the presence of time-dependent input I(t), this would require identical input within
each inter-spike-interval (ISI). This is the much criticized reset assumption: If the model
neuron were to describe a neuron in the auditory nerve while you are listening to a music
tape, the reset assumption requires that upon the ﬁring of each spike the tape should be
rewound to exactly the position it had at the time of the last spike!

−

In this work we show how to analyze the response of the leaky integrate-ﬁre neuron to
periodic stimuli without undue assumptions. The distribution of the length of individual
inter-spike intervals is computed numerically [15], and spike trains are then assembled as
Markov chains from these intervals. We obtain probability distributions for the length
of inter-spike intervals and the stimulus phases at which spikes occur. These distributions
should be directly comparable to experiments employing sustained stimulation with periodic
signals. The signal processing performance of the neuron is judged by the signal-to-noise
ratio (SNR) of the output spike train. The SNR is maximal at an optimal noise amplitude
for ﬁxed stimulus frequency and at a resonance frequency for ﬁxed noise amplitude. The
latter resonance is a consequence of a time-scale matching between stimulus and membrane
time-constant. All computations are veriﬁed by simulations.

In Sec. II, we show how to exploit the Markov property of the integrate-ﬁre neuron to
determine its response to sinusoidal input I(t). The performance of the model neuron as a
signal processing device is investigated in Sec. III. The results are discussed in Sec. IV.

II. MARKOV ANALYSIS

For an input current consisting of a constant oﬀset and a sinusoidal component, and

Gaussian white noise the Langevin equation (1) reads

˙v(t) =

v(t) + µ + q cos(Ωt + φ0) + √Dξ(t) ,

−

(2)

where time and potential have been scaled to their respective natural units τm and Θ; the
reset potential is set to v0 = 0. The input is characterized by the DC oﬀset µ, stimulus
amplitude q, frequency Ω and initial phase φ0. The noise has amplitude √D and auto-
t′). In the remainder of this article, we will investigate this
correlation
model. For a derivation of the type of input current used here from more elementary models,
see [24].

ξ(t)ξ(t′)

= δ(t

−

h

i

In the absence of noise (D = 0), spikes will only be generated if

v∞ = lim
t→∞

v(t) = µ +

q
√1 + Ω2

> 1 .

Therefore, we classify stimuli as sub-threshold if v∞
1 and as supra-threshold otherwise.
In this work, we will focus on the biologically more interesting sub-threshold regime [25].

≤

3

(3)

(4)

(5)

(6)

The methods presented here are applicable independent of the choice of stimulus parameters.
We only require the presence of noise, i.e. D > 0.

Suppose that an initial spike has occured at time t0 = 0, corresponding to stimulus
phase φ0. The next spike follows at time t1 = inf
and stimulus phase
≥
φ1 = (Ωt1 + φ0) mod 2π, whence cos(Ω(t
t1) + φ1) = cos(Ωt + φ0) for t > t1. This suggests
to re-write Eq. (2) in terms of the time t′ that has passed since the most recent spike at
phase φ. Thus, given this phase, the potential evolves from v(t′ = 0
φ) = v0 = 0 until the
next spike according to

t > t0|

v(t)

−

}

{

1

|

˙v(t′

φ) =

v(t′

φ) + µ + q cos(Ωt′ + φ) + √Dξ(t′) .

|

−

|

The next spike is ﬁred after an interval τ , as soon as the threshold condition is met

τ = inf

t′ > 0

v(t′

φ)

{

|

|

.

1

}

≥

The inter-spike intervals are connected by the iteration equations

φk = (Ωτk + φk−1) mod 2π ,

tk = tk−1 + τk ,

leading to the output spike train

∞

∞

j

f (t) =

δ(t

tj) =

−

δ

t

−

τk

.

j=0
X
The reset of the membrane potential to v0 = 0 after each spike completely erases the
memory of the neuron. The subsequent behavior of the neuron therefore depends on its past
only through the absolute time of the spike tk, i.e. the spike train is a Markov process.

j=0
X

k=1
X

(cid:17)

(cid:16)

We have thus split the task of solving the dynamics of the integrate-ﬁre neuron into
two parts. We will ﬁrst solve the ﬁrst-passage-time problem posed by Eqs. (3) and (4) for
a given phase φ of the last spike, before assembling the spike train from the inter-spike
intervals according to Eqs. (5) and (6).

A. Conditional ISI distribution

|

The ﬁrst-passage-time problem for the membrane potential posed by Eqs. (3, 4) yields
φ) of the inter-spike-interval lengths τ for a given stimulus phase φ at
the distribution ρ(τ
the beginning of the interval (conditional ISI distribution). To the best of our knowledge,
no analytic solution is known for this seemingly simple ﬁrst-passage time problem of the
Ornstein–Uhlenbeck process. The approximations suggested in [14] are valid in a restricted
parameter range only—low stimulus frequencies in particular—and appear to yield qualita-
tive rather than quantitative agreement with simulations.

We employ here a numerical method to compute the inter-spike-interval distributions.
The method is discussed in detail in [15], and we only sketch it here. In the absence of an
u, s′; φ) that the membrane potential is w at time
absorbing threshold the probability
t′ if it was u at time s′ < t′ is a Gaussian distribution. The mean is given by the solution

(w, t′

P

|

4

at time t′ of Eq. (3) for the noise-free case (D = 0) with initial condition v(s′
the variance is σ2(t′) = D
2 (1
by the integral equation [26]

φ) = u, while
e−2(t′−s′)). Then, the inter-spike-interval distribution is given

−

|

(1, t′

0, 0) =

P

|

t′

0 P
Z

(1, t′

|

1, τ )ρ(τ

φ)dτ .

|

(7)

This equation is solved for ρ using standard techniques [27]. Source code is available on
request.

As shown in Fig. 1, the conditional inter-spike-interval distributions ρ(τ

φ) may depend
strongly on φ. First, they contain a series of exponentially decaying peaks that are separated
by the stimulus period T = 2π/Ω. These peaks represent spikes that are well phase-locked to
the stimulus and we will refer to them as periodic peaks. An additional peak appears at short
intervals τ for certain phases φ. This peak reﬂects the rise time of the membrane potential
Its location is not related to the stimulus period T , but reﬂects the
towards threshold.
intrinsic time-scale of the neuron and deﬁnes its refractory time, i.e. the minimum interval
between two spikes. Thus, we will refer to this peak as the refractory peak. It corresponds
to two or more spikes ﬁred in rapid succession within a single stimulus period (a burst).
There is thus a qualitative dependence of the distributions ρ on the phase φ that can lead
to interesting consequences for the ﬁring behavior of the neuron.

|

The relation between periodic and refractory peaks depends on the stimulus parameters,
particularly on the frequency and the noise amplitude. We will discuss this relationship in
Sec. II C.

B. Markov process in phase

Let us now turn to the problem of assembling spike trains from inter-spike-intervals
according to Eqs. (5, 6). The length of an interval following a spike at time t and stimulus
phase φ = [Ωt + φ0] mod 2π is distributed according to ρ(τ
φ). Therefore, the probability
that the next spike will occur at phase ψ is given by

|

(ψ

φ) =

T

|

ρ(τ

φ)δ(ψ

[Ωτ + φ] mod 2π)

|

−

dτ
Ω

.

∞

0
Z

(ψ

φ) the transition probability of the spike phase. We will now consider the
We will call
Markov process of the spike phases φk instead of the Markov process made up of the spike
times tk.

T

|

If we deﬁne the spike phase distribution χ(k) (φ) as the probability (across an ensemble
of neurons or repetitions of an experiment) that the kth spike in a train will be ﬁred at
stimulus phase φ, then this probability will evolve according to

(8)

(9)

As the neuron ﬁres repetitively while driven by a stationary periodic stimulus, the spike train
emitted by the neuron will approach a stationary Markov process with phase distribution

χ(k+1) (ψ) =

(ψ

φ) χ(k) (φ) dφ .

2π

0 T

Z

|

5

2π

(10)

χ(s) (ψ) = lim
k→∞

χ(k) (ψ) =

(ψ

φ) χ(s) (ψ) dφ .

T

|

|

(ψ

0
Z
The stationary phase distribution χ(s) (ψ) is the eigenfunction to eigenvalue 1 of the kernel
φ), and is guaranteed to exist because this kernel is a conditional probability distri-
T
bution [28]. Any initial phase distribution will converge to the unique stationary solution
φ) > 0 everywhere [29]. That the latter condition holds in the presence
provided that
of noise can be seen as follows. For sub-threshold stimuli, noise may drive the potential
across the ﬁring threshold at any time τ > 0 in principle, yielding a possibly tiny, but non-
zero probability of spikes at any phase. The same argument holds true for supra-threshold
stimuli, where noise may keep potential below threshold up to any time. In the absence of
noise, neither convergence nor uniqueness are assured.

(ψ

T

|

To facilitate numerical treatment, we discretize the phase. Since the conditional inter-
spike-interval distributions ρ(τ
φ) are smooth in both time and phase due to the presence of
noise in the input, this discretization will introduce only minor numerical errors. It is largely
equivalent to applying numerical methods to solve the kernel eigenvalue problem [28]. Using
L bins of width ∆ψ (∆ψ = 2π/L) we obtain the spike phase distribution vector

|

(11)

(12)

(13)

χ = (χ0, χ1, . . . , χL−1)tr , χj =

χ(ψ)dψ ,

(j+1)∆ψ

j∆ψ

Z

and the phase transition matrix T with elements

Tjk =

(ψ

k∆ψ) dψ ,

j, k = 0, . . . , L

1.

−

(j+1)∆ψ

j∆ψ

Z

T

|

The evolution equation (9) simpliﬁes from convolution to matrix-vector multiplication

χ(k+1) = T

χ(k) ,

·
and the stationary distribution χ(s) is the eigenvector to eigenvalue 1 of the matrix T. We
have thus reduced the Markov process to a Markov chain.

In practice, we obtain the transition matrix T by numerically evaluating equations (8)
and (12), with ρ(τ
φ) from Eq. (7). The stationary distribution is then found using standard
eigenvector routines. For all data shown here, we used the discretization L = 72, ∆ψ =
π/36 = 5◦. In ﬁgures of transition matrices and phase distributions the axis will run from

|

π to π as this renders structures more clearly.

−

An example for the phase evolution of an initially uniform distribution towards the
stationary state under the inﬂuence of a transition matrix T is given in Fig. 2. To “read”
the transition matrix, note that the matrix columns correspond to the phase φk of the spike
preceding the interval, the rows to the phase φk+1 of the spike terminating it. The phase
π to π from bottom to top in phase distribution vectors χ and the rows of
axes run from
the transition matrix T, and from right to left across the columns of T. Thus, the horizontal
bar in the transition matrix shown in Fig. 2 indicates that for most values of φk the next
π/6. This bar corresponds to the periodic peaks of the ISI
spike will occur around φk+1 ≈ −
π/4 . φk . π/6, the matrix is dominated by a “ﬁnger”, running parallel
distributions. For

−

−

6

to the matrix diagonal. Within this range of phases, a spike will be followed by another
spike at a slightly later phase, as shown in Fig. 2b. Figuratively speaking, the neuron ﬁres
a burst of spikes, but there is always a chance that two subsequent spikes will be one or
in the Markov chain
more stimulus periods apart, even though they are close in phase:
description, all information about actual interval lengths is lost. The ﬁnger results from the
refractory peak of the ISI distributions.

Figure 3 shows the dependence of transition matrix and stationary phase distribution
on the noise amplitude for slow stimuli (T & 10). For low noise, the transition matrix
is dominated by the horizontal bar, which intersects with the matrix diagonal, indicating
a stochastic ﬁxed point. This results in a sharply peaked spike phase distribution. At
intermediate noise, the ﬁnger is more pronounced, while the bar barely touches the matrix
diagonal, leading to a stochastic limit cycle with two preferred phases: the neuron often
ﬁres bursts of two successive spikes. At high noise, the ﬁnger stretches all along the matrix
diagonal, while the horizontal bar has disappeared altogether. The neuron ﬁres rapidly, but
largely uncorrelated with the stimulus and the phase distribution is virtually ﬂat.

This means that for very low noise the spike train of the neuron is nearly a stationary re-
ψ∗). Here ψ∗ is the location of
newal process with inter-spike-intervals i.i.d. according to ρ(τ
the maximum of the stationary phase distribution, which depends not only on the stimulus
parameters, but also on the noise amplitude. For high noise amplitudes, the response of the
neuron is largely independent of the stimulus, and may thus be described by a stationary
renewal process as well—the ISIs reduce to the refractory peak. But at intermediate noise
levels—i.e. those essential to the observation of stochastic resonance—the stationary phase
distribution may be multimodal. Thus the correlations between the phases of subsequent
spikes have to be taken into account using the Markov ansatz. Multimodal phase distribu-
tions as discussed here are not just hypothetical: they have been observed in sensory neurons
of goldﬁsh upon stimulation with sinusoidal water waves [30].

|

For fast stimuli (T . 10), the stationary phase distribution smears out much more along
the phase axis, and does not show multimodality, because the refractory time of the neuron
becomes comparable to the stimulus period and bursting is no longer possible, see Fig. 4. At
low to intermediate noise, the distribution is too wide to be replaced by its mode as in the
renewal ansatz, but still suﬃciently narrow to provide for a response that is well phase-locked
to the stimulus. Therefore, the Markov approach is essential for high frequency stimuli as
well.

C. Stationary ISI distribution

Once the stationary phase distribution is known, the inter-spike-interval distribution of
the stationary ﬁring process is obtained by averaging the conditional ISI distributions over
phase

ρ(τ ) =

ρ(τ

ψ)χ(s) (ψ) dψ .

2π

0

Z

|

(14)

The average interval length thus is

7

∞

τ ρ(τ )dτ .

=

τ
h

i

0
Z

(15)

ρ(τ ) is the inter-spike-interval distribution that we expect to ﬁnd in experiments with tonic
stimulation. In contrast to a stationary renewal process, this averaged ISI distribution does
not contain a full description of the spike train.

Typical ISI distributions ρ(τ ) are given in Figs. 5 and 6 for the same parameters as used
in Figs. 3, 4, respectively. For low noise, they contain only periodic peaks, located precisely
at integer multiples of the stimulus period T : the neuron can only ﬁre in a small time window
within each period, and several periods may be skipped in between spikes. This indicates a
ﬁring pattern that is well phase-locked to the stimulus. ISI distributions with comparable
structure have been found in neurons of the auditory system in diﬀerent species [31,32]. For
high noise, the ISI distributions reduce to the refractory peak, i.e. a largely random ﬁring
pattern.

For intermediate noise, the ISI distributions depend strongly on the stimulus frequency.
For high frequency (Fig. 6), we ﬁnd merely a superposition of periodic and refractory peaks:
spikes preferentially occur at intervals that are multiples of the stimulus period, but this
phase-locking is weak. This is very diﬀerent for slow stimuli (Fig. 5), where the refractory
peak is clearly separated from a wide peak at τ = T = 40, the latter exposing some sub-
structure. This can be understood as follows. The maximum of ρ(τ ) at τ = T corresponds
to two spikes ﬁred each at the optimal phase in two subsequent periods. In contrast, if a
period that contained a burst of two spikes is followed by another period containing a burst,
then typically the ﬁrst spike will be slightly earlier than the optimal phase, the second one
a bit later. Thus, the interval between the second spike of the ﬁrst burst and the ﬁrst spike
of the second burst is shorter than the stimulus period, leading to the side-peak at τ
35.
The bursts themselves give rise to the refractory peak. This again indicates that the spike
train is not a stationary renewal process.

≈

Along with results obtained using the Markov chain approach, Figs. 3–6 display phase and
ISI distributions obtained from simulated trains of 20,000 spikes. The agreement between
Markov model and simulation is excellent. Source code for the simulation based on [33] is
available on request.

III. STOCHASTIC RESONANCE

To assess the performance of the integrate-ﬁre neuron as a signal processing device, we
evaluate the signal-to-noise ratio (SNR) of the spike train generated in response to periodic
input. In doing so, one should keep in mind the purpose of the output spike train. It has to
convey information to other neurons in the brain within a certain time window, as the brain
has to respond quickly to stimuli. Therefore, the relevant quantity is the signal-to-noise ratio
that can be achieved by measuring the spike train over a ﬁnite observation time To [34].

8

The one-sided power spectral density of a stationary spike train f (t) [as deﬁned in Eq. (6)]

over a time interval To is [35]

A. Signal-to-noise ratio

S′
To(ω)=

2

To

1
πTo *(cid:12)
0
Z
(cid:12)
tj ,tk<To
(cid:12)
1
(cid:12)
πTo *

f (t)eiωtdt
(cid:12)
(cid:12)
(cid:12)
(cid:12)
eiω(tj −tk)
+

+

.

=

(16)

(17)

(18)

j,k
X
The average is to be taken over the ensemble of all spike trains, that is, over the set of
all conditional ISI distributions and their (j
k)-fold convolutions. This problem appears
intractable.

−

The situation is greatly simpliﬁed if ω is the stimulus frequency Ω or one of its harmonics.

Expressing the spike times as tj = (mj + ψj

2π )T , Eq. (16) for ω = nΩ simpliﬁes to

S′
To(nΩ) =

1
πTo *

tj ,tk<To

j,k
X

ein(ψj −ψk)

,

+

where n, mj are integers, ψj
vation period To, on average To/
the spike train. We therefore ﬁx the upper limit of the summation at No =
τ
h
x. This yields as an approximation

[ 0, 2π), and T = 2π/Ω is the stimulus period. In the obser-
spikes will occur, regardless of the detailed structure of
, where
is the largest integer not exceeding

is the average interval length from Eq. (15) and

To/
⌊

x
⌋
⌊

τ
h

τ
h

i⌋

∈

i

i

S′
To(nΩ)

≈

STo(nΩ) =

1
πNo

No

τ
h

i *

j,k=1
X

ein(ψj −ψk)

.

+

The task of computing an expectation with respect to all possible spike trains is now
reduced to that of averaging over all possible sequences of spike phases. Their distribution
and correlations are completely characterized by the transition matrix T, permitting for
evaluation of Eq. (18) in closed form. The actual calculation is straightforward albeit lengthy
algebra and is provided in the appendix. The ﬁnal result may be written as

STo(nΩ) =

1 + A(n, No) + (No

1)B(n)

(19)

1
τ

h

π

i h

−

i

where the functions A(n, N0) and B(n) are given in the appendix. Note that A(n, No) is
bounded as No
. For a Poissonian spike train, both A and B are identically zero,
→ ∞
yielding a white power spectrum [23].

At ﬁrst, it might seem surprising that the spectrum contains a term, (No

1)B(n),
that scales linearly with the number of spikes in the train. This is a consequence of the
periodic component of the spike train introduced by the driving stimulus, leading to a mixed
spectrum consisting of a continuous background and a discrete spectrum of harmonics [35].

−

9

For inﬁnite observation time, i.e. No
power spectrum.

→ ∞

, this gives rise to the terms

δ(ω

nΩ) in the

∼

−

A typical power spectrum is shown in Fig. 7, indicating close agreement of Eq. (19)
with results obtained by numerical Fourier transformation of simulated spike trains. The
approximation made in ﬁxing the summation limit in Eq. (18) is therefore well justiﬁed.
The dip in the noise background of the spectrum at low frequencies is a consequence of the
refractory period of the neuron, while the weak hump at ω
1 indicates the presence of
bursts [36]. Spectra consisting only of this background have been found in neurons of higher
cortical areas of monkeys in the absence of periodic input [37].

≈

Since the power spectral density can only be evaluated in closed form at multiples of
the stimulus frequency, we approximate the noise background as Poissonian white noise
)−1 of a spike train of equal intensity [34]. The signal-to-noise ratio obtainable
SP = (π
from the spike train within the observation time To is therefore given by

τ
h

i

SNRTo =

= 1 + A(1,

) + (

1)B(1) .

(20)

STo(Ω)
SP

To
τ

(cid:22)

h

i (cid:23)

To
τ
h

(cid:22)

i (cid:23)

−

The signal-to-noise ratio for three diﬀerent stimulus frequencies is shown in Fig. 8 vs. the
noise amplitude, again in excellent agreement with simulation results. Stochastic resonance
(SR) is clearly present at all frequencies, as the SNR attains its maximum for an intermediate
noise level. The striking new feature is that the overall maximum in the SNR is reached
π/3, which we thus call the resonance frequency. The
at an intermediate frequency Ωr
same qualitative dependence of the SNR on noise amplitude and stimulus frequencies is
observed over a wide range of stimulus parameters, including weakly supra-threshold cases
(0.4 . µ < 1, 0.4 . q/(1

µ) . 1.2; data not shown).

≈

Note that the stochastic resonance reported in an earlier paper [15] is an artifact of the
renewal ansatz employed in that work. There, the stimulus phase is reset to an arbitrarily
chosen value φ0 after each spike, and the signal-to-noise ratio is computed for an inﬁnite
observation time. The SNR is maximized for that noise level at which the periodic peaks
of the ISI distribution ρ(τ
φ0) are centered about the multiples of the stimulus period T .
But if, for low noise, one uses for each noise level D a diﬀerent φ0(D), namely the mode
of the stationary phase distribution as discussed in Sec. II B, the periodic peaks are at
multiples of T for all noise intensities, whence the SNR does not drop oﬀ for D
0 and
no resonance occurs (data not shown). This observation underlines the importance of the
Markov approach.

→

|

−

B. Time-scale matching

In contrast to stochastic resonance in dynamical systems, SR with respect to the noise
amplitude is not induced by the matching of time-scales in threshold systems, but results
from stochastic linearization of the response function of the neuron [34,38].
In contrast,
the additional resonance along the frequency axis arises in the integrate-ﬁre neuron as a
consequence of matching the stimulus period to the intrinsic time scale of the neuron in
an appropriate manner. This is demonstrated in Fig. 9. For a stimulus at the resonance
frequency Ωr, the peak at τ = T in the stationary ISI distribution can “grow” in place as

10

noise is increased, without being disturbed by the refractory peak. Indeed, the latter arises
at the location of the ﬁrst periodic peak and shifts away from τ = T only for very large noise.
In this way, the ﬁring rate of the neuron can be increased without loosing the phase-locking
to the stimulus. Compare this to the cases of lower (Fig. 5) and higher (Fig. 6) frequencies:
in both cases, high ﬁring rates can only be achieved by raising the noise amplitude to a
point where the refractory peak has either replaced (Ω < Ωr) or smeared out (Ω > Ωr) the
periodic peaks, resulting in a ﬁring pattern poorly phase-locked to the stimulus.

This competition of precision and intensity is demonstrated by a phenomenological ansatz
for the SNR. A measure of phase-locking between stimulus and response is the vector
, where ψ are the spike phases [39]. Cs = 1 indicates perfect and
strength Cs =
Cs = 0 no locking. If the neuron attempts to measure the degree of phase-locking from a
(cid:12)
(cid:10)
√N . Thus, we expect
train of N = To/
(cid:12)
that the signal-to-noise ratio will roughly given by

(cid:11)(cid:12)
spikes, the quality of measurement will be
(cid:12)
i

τ
h

eiψ

∼

SNRphen ≈

Cs√N = Cs

.

To
τ

h

i

s

(21)

Figure 10 demonstrates that this simple model describes the behavior of the SNR well. In
particular, the two-fold stochastic resonance is reproduced.

In short, to elicit a strong output signal from the model neuron, a suﬃcient input noise
level is required. But this comes at a cost, as the quality of the output, i.e. the precision of
the phase locking, deteriorates as noise is added. The maximum SNR represents the optimal
compromise between signal strength and quality.

IV. DISCUSSION

In this paper, we have shown that the periodically driven integrate-ﬁre neuron can be
analyzed in the framework of a Markov process. This avoids the unrealistic assumption of a
stimulus reset after each spike, the most serious shortcoming of previous work [14,15], and
this answers question (1) raised by Gammaitoni et al. in Sec. V.4.C of their review [2]. Their
second questions concerns the fact that the neural membrane is a rectiﬁer: even a strong
negative input current will not lower the membrane potential more than a few millivolts
below the reset potential v0. This would indeed be a problem if the DC oﬀset µ of the
input were much smaller than the amplitude q of the AC stimulus. Preliminary evidence
suggests that the best ﬁt of inter-spike-interval distributions generated by the model with
experimental data from the cat’s auditory system [32] is obtained for sub-threshold stimuli
with µ
q. In this regime, the membrane potential is quickly raised to v0 + µ and then
oscillates around this level, unaﬀected by rectiﬁcation. Finally, Gammaitoni and co-authors
question the validity of the approximations used to compute the ISI distributions in [14].
This matter is avoided here by numerically computing these distributions. A study of the
validity of approximate closed-form ISI distributions will be given elsewhere [40].

≫

The Markov formalism presented in this paper is applicable to any periodically driven
stochastic process with a reset. The only required ingredients are the conditional ﬁrst-
passage-time distributions ρ(τ
φ) and the iteration equations (5). The generalization to
more complex stimuli, e.g. including amplitude modulation, is straightforward.

|

11

With the Markov machinery at hand, we have demonstrated that the signal-to-noise ratio
of the output of the neuron is maximized at an optimal noise amplitude for ﬁxed frequency
and at a resonance frequency for ﬁxed noise intensity. Stochastic resonance with respect to
the stimulus frequency, termed bona ﬁde stochastic resonance, has been described in bistable
systems before [41,42]. Therefore, our ﬁndings for a non-dynamical threshold neuron extend
the universality of stochastic resonance to the case of bona ﬁde SR. Recent criticism [43] of
the original deﬁnition of bona ﬁde SR, based on residence time distributions, does not apply
to our study.

Neurons in the auditory system can phase lock to acoustic stimuli with high acuity and
utilize this for the precise localization of sound sources [44]. Our results show that strong
signals that are well phase locked to a stimulus may be achieved in spite of the noise ubiq-
uitous in the neural system. Stochastic resonance might therefore be one of the underlying
mechanisms of stereo hearing. First qualitative comparisons indicate good agreement be-
tween response properties of the integrate-ﬁre neuron and of auditory neurons. An intriguing
question in this respect is the relevance of the bona ﬁde SR to the neural system. It may
serve to tune neurons as bandpass ﬁlters of a special kind: only stimuli in a certain frequency
window will be transmitted with high intensity and precise phase locking. A detailed study
will be the topic of a future publication.

ACKNOWLEDGMENTS

This work was supported by Deutsche Forschungsgemeinschaft through SFB 185 “Nicht-
lineare Dynamik”. HEP gratefully acknowledges the hospitality of the Laboratory for Neural
Modeling, Frontier Research Program, RIKEN, Wako-shi, Saitama, Japan, where this work
started.

APPENDIX A: COMPUTING THE POWER SPECTRAL DENSITY

To prove Eq. (19), i.e.

STo(nΩ)=

M

ein(ψj −ψk)

j,k=1
X

i *
1 + A(n, M) + (M

+

1
πM

τ

h

i h

1
τ

h

=

π

1)B(n)

,

−

i

we split the double sum into the diagonal and oﬀ-diagonal terms

STo(nΩ) =

[1 + hM (nΩ) + h∗

M (nΩ)] ,

1
τ

h

i

π

1
M

M

M −k

hM (nΩ) =

ein(ψk+j −ψk)

,

k=1
X

j=1
X

(cid:10)

(cid:11)

12

(A1)

(A2)

the asterisk denoting complex conjugation and M =

To/τ
⌊

.
⌋

Since we are considering a stationary Markov process, all ψk are identically distributed
according to χ(s), while correlations between ψk and ψk+j are given by the jth power of the
transition matrix T yielding

ein(ψk+j−ψk)

= ˆa(n)tr

Tj

ˆb(n)

·

·

(A3)

with vectors

(cid:10)

(cid:11)

ˆatr(n) = (1, ein∆ψ, e2in∆ψ, . . . , e(L−1)in∆ψ)

ˆbtr(n) = (χ(s)(0), . . . , e−(L−1)in∆ψχ(s)((L

1)∆ψ))

−

Upon inserting Eq. (A3) into Eq. (A2), we observe that the expression for hM depends

only on j but not on k so that we may perform the outer summation to obtain

hM (nΩ) = ˆatr(n)

M −1

1
M

h

j=1
X

(M

j)Tj

−

ˆb(n) .
i

Diagonalizing T leads to

Here, the diagonal matrix S(M ) is given by

hM (nΩ) = a(n)tr

S(M )

b(n) .

·

·

with

S(M )
mm = 


M

1

−
2
λm

1

λm

−

+

1
M

λm(λM
(λM

m −

1)
1)2

m −

for m = 1 ,

for m > 1 ,



·

·

T = C

L

C−1 , L = diag(1 >

λ2| ≥

|

. . .

) ,

λL|

≥ |

a(n) = Ctrˆa(n) , b(n) = C−1ˆb(n) .

(A4)

(A5)

Inserting Eq. (A4) into Eq. (A1), we have

STo(nΩ) =

1 + 2 Re

atr(n)S(M )b(n)

.

(A6)

(cid:0)
Finally, we split the matrix S(M ) into the parts pertaining to the discrete and the con-

(cid:1)(cid:3)

(cid:2)

tinuous parts of the spectrum and deﬁne the functions A and B

1
τ
h

i

π

S(M ) = diag( M −1

, 0, . . . , 0) + diag(0, S(M )

22 , . . . , S(M )

LL ) ,

2

A(n, M) = 2 Re

atr(n) diag(0, S(M )
h

13

22 , . . . , S(M )

LL ) b(n)

,

i

B(n) = Re [a1(n)b1(n)] .

Rewriting Eq. (A6) accordingly, we arrive at the desired expression for the power spectral
density

STo(nΩ) =

1 + A(n, M) + (M

1)B(n)

.

−

i

1
τ

π

h
To see that A is bounded as M
diagonal entries of S(M ) with m > 1 and

i h

, note that A depends on M only through the

< 1. For these we have

lim
M →∞ |

S(M )
mm |

=

<

, m > 1 .

∞

→ ∞
|

λm

|
λm

1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λm (cid:12)
(cid:12)
(cid:12)
(cid:12)

14

REFERENCES

[1] R. Benzi, A. Sutera, and A. Vulpiani, J Phys A 14, L453 (1981).
[2] L. Gammaitoni, P. H¨anggi, P. Jung, and F. Marchesoni, Rev Mod Phys 70, 223 (1998).
[3] A. Longtin, J Stat Phys 70, 309 (1993).
[4] K. Wiesenfeld and F. Moss, Nature 373, 33 (1995).
[5] J. E. Levin and J. P. Miller, Nature 380, 165 (1996).
[6] J. J. Collins, T. T. Imhoﬀ, and P. Grigg, J Neurophysiology 76, 642 (1996).
[7] P. Cordo et al., Nature 383, 769 (1996).
[8] S. M. Bezrukov and I. Vodyanoy, Biophys J 73, 2456 (1997).
[9] J. K. Douglass, L. Wilkens, E.Pantazelou, and F. Moss, Nature 365, 337 (1993).
[10] M. Stemmler, M. Usher, and E. Niebur, Science 269, 1877 (1995).
[11] F. Moss and D. F. Russell, Animal behavior enhanced by noise, Computational Neuro-

science Meeting ’98, Santa Barbara, CA, July 1998.

[12] J. G. Nicholls, A. R. Martin, and B. G. Wallace, From Neuron to Brain, 3 ed. (Sinauer,

Sunderland, Mass., 1992).

[13] H. C. Tuckwell, Stochastic Processes in the Neurosciences (SIAM, Philadelphia, 1989).
[14] A. R. Bulsara et al., Phys Rev E 53, 3958 (1996).
[15] H. E. Plesser and S. Tanaka, Phys Lett A 225, 228 (1997).
[16] B. McNamara and K. Wiesenfeld, Phys Rev A 39, 4854 (1989).
[17] P. Jung, Physics Reports 234, 175 (1993).
[18] T. Zhou, F. Moss, and P. Jung, Phys Rev A 42, 3161 (1990).
[19] A. Bulsara et al., J theor Biol 152, 531 (1991).
[20] Z. Gingl, L. B. Kiss, and F. Moss, Europhys. Lett. 29, 191 (1995).
[21] P. Jung, Phys Rev E 50, 2513 (1994).
[22] K. Wiesenfeld et al., Phys Rev Lett 72, 2125 (1994).
[23] D. R. Cox and P. A. W. Lewis, The Statistical Analysis of Series of Events (Methuen,

[24] P. L´ansk´y, Phys Rev E 55, 2040 (1997).
[25] R. Kempter, W. Gerstner, J. L. van Hemmen, and H. Wagner, Neural Computation 10,

[26] N. G. van Kampen, Stochastic Processes in Physics and Chemistry, 2nd ed. (North-

Holland, Amsterdam, 1992).

[27] P. Linz, Analytical and Numerical Methods for Volterra Equations (SIAM, Philadelphia,

[28] C. T. H. Baker, The Numerical Treatment of Integral Equations (Clarendon Press, Ox-

[29] R. von Mises, Mathematical Theory of Probability and Statistics (Academic Press, New

York, 1964), ed. by H. Geiringer.
[30] J. Mogdans, private communication.
[31] J. E. Rose, J. F. Brugge, D. J. Anderson, and J. E. Hind, J Neurophysiology 30, 769

London, 1966).

1987 (1998).

1985).

ford, 1977).

(1967).

[32] R. A. Lavine, J Neurophysiology 34, 467 (1971).
[33] D. T. Gillespie, Phys Rev E 54, 2084 (1996).
[34] M. Stemmler, Network 7, 687 (1996).

15

[35] M. B. Priestley, Spectral Analysis and Time Series (Academic Press, London, 1996).
[36] J. Franklin and W. Bair, SIAM J Appl Math 55, 1074 (1995).
[37] W. Bair, C. Koch, W. Newsome, and K. Britten, J Neuroscience 14, 2870 (1994).
[38] L. Gammaitoni, Phys Rev E 52, 4691 (1995).
[39] J. M. Goldberg and P. B. Brown, J Neurophysiology 32, 613 (1969).
[40] H. E. Plesser and W. Gerstner, submitted.
[41] L. Gammaitoni, F. Marchesoni, and S. Santucci, Phys Rev Lett 74, 1052 (1995).
[42] V. Berdichevsky and M. Gitterman, J Phys A 29, L447 (1996).
[43] M. H. Choi, R. F. Fox, and P. Jung, Phys Rev E 57, 6335 (1998).
[44] W. Gerstner, R. Kemptner, J. L. van Hemmen, and H. Wagner, Nature 383, 76 (1996).

16

FIGURES

T

τ

2T

(a)

|

)
φ
τ
(
ρ

0.2

0.1

(b)

|

)
φ
π
2
 
d
o
m

 

 
]
φ
+
 
τ
 
Ω

[
(
ρ

0.2

0.1

17

−π  

−π/2

π/2 

π   

   0  
[Ω τ + φ] mod 2π

π/6 (solid) and φ = π/6 (dashed); other
FIG. 1. (a) Conditional ISI distributions for φ =
−
10−5. T = 40 is the stimulus period. The
parameters µ = 0.95, q = 0.048, Ω = 0.05π, D = 6
·
refractory mode at small τ is present only for φ =
π/6. The small modes around 2T correspond
to the probability of skipping a period. (b) The same distributions as in (a), but now plotted vs.
π/25 coincide for the ﬁrst and
phase, ψ = [Ωτ + φ] mod 2π, shifted to [
second stimulus period, while the refractory mode is clearly set apart.

π, π]. The modes at

≈ −

−

−

χ(1)

(a)

 π

 T

χ(0)

=

⋅

π 

   0

−π

0.15

0.1

0.05

0

χ(8)

χ(7)

χ(6)

χ(5)

χ(4)

χ(3)

χ(2)

χ(1)

χ(0)

(b)

 π

   0

−π

   0

−π

FIG. 2. (a) Graphic representation of the Markov chain iteration given by Eq. (13). The
dashed line is the matrix diagonal. Probability is given by grayscale as indicated by the colorbar.
(b) Evolution of an initially uniform phase distribution under subsequent multiplications with T,
from right to left. See text for details. Stimulus parameters: µ = 0.95, q = 0.05, Ω = 0.02π,
D = 1.3

10−4.

·

18

(a)

1
+
k

  0  

ψ

−π 
π 

(c)

1
+
k

  0  

ψ

−π 
π 

(e)

1
+
k

  0  

ψ

−π 
π 

 0  
ψ
k

−π

−π/2

   0  
ψ

π/2 

 0  
ψ
k

−π

−π/2

   0  
ψ

π/2 

 0  
ψ
k

−π

−π/2

   0  
ψ

π/2 

FIG. 3. Phase transition matrices T (a, c, e) and corresponding stationary phase distributions
10−6
χ(s) (b, d, f) for stimulus frequency Ω = 0.05π at three diﬀerent noise intensities D = 6.2
10−3 (e, f); other parameters: µ = 0.95, q = 0.05.
(a, b), D = 7.0
The grayscale is the same for all matrices, white indicating vanishing probability. Error bars in
the phase distributions indicate standard error of mean from simulated trains of 20,000 spikes.
Observe the diﬀerent scalings of the ordinate.

10−5 (c, d), and D = 4.8

·

·

·

(b)

]

ψ

[

)
s
(
χ

0.2

0.1

0

(d)

]

ψ

[

)
s
(
χ

0.1

0.05

(f)

]

ψ

[

)
s
(
χ

0.02

0.01

0

0

19

]

ψ

[

)
s
(
χ

0.03

0.06

0.05

0.04

0.02

0.01

0

0.3

0.25

0.2

0.1

0.05

0

)
τ
(
ρ

0.15

   0  
ψ
FIG. 4. Stationary phase distributions χ(s) for stimulus frequency Ω = 0.5π and noise intensi-
10−2 (dash-dotted). Everything
10−3 (dashed), and D = 3.0

10−4 (solid), D = 4.8

−π/2

π/2 

·

·

ties D = 7.8
else is as in Fig. 3.

·

FIG. 5. Stationary ISI distributions for slow stimuli. Noise intensities are D = 6.2

10−6 (solid),
10−3 (dash-dotted). All other parameters are as in Fig. 3,

·

10−5 (dashed), and D = 4.8

D = 7.0
error bars again indicate simulation results.

·

·

20

60

80

40

τ

20

)
τ
(
ρ

0.2

0.4

0.35

0.3

0.25

0.15

0.1

0.05

0

−10

−15

−20

−25

−30

)

ω
(
S

4

8

12

20

24

28

16

τ

FIG. 6. Stationary ISI distributions for fast stimuli. Noise intensities are D = 7.8

10−4 (solid),
10−2 (dash-dotted). All parameters are as in Fig. 4, and

·

10−3 (dashed), and D = 3.0

D = 4.8
error bars are from simulations.

·

·

−35

    0   

2Ω 

4Ω 

6Ω 

8Ω 

10Ω 12Ω 14Ω

ω
FIG. 7. Power spectral density from an observation time of To = 200 for the same stimulus
as in Fig. 3(b, e). Circles indicate results at stimulus harmonics from the Markov chain analysis,
while the drawn out line is obtained by FFT from a simulated train of 20,000 spikes. Ticks on the
abscissa mark multiples of the stimulus period Ω = 0.05π.

21

]

B
d
[
 

R
N
S

12

10

8

6

4

2

0

−2

)
τ
(
ρ

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

√D
FIG. 8. Signal-to-noise ratio vs. noise amplitude for three diﬀerent stimulus frequencies:
Ω = 0.1π (dashed), Ω = 0.33π (solid) and Ω = 0.5π (dash-dotted). Other parameters are µ = 0.95
and q = 0.05. Error bars show s.e.m. from simulated trains of 20,000 spikes.

τ
FIG. 9. Inter-spike-interval distributions for the resonance frequency Ωr and three noise inten-
10−3 (dash-dotted); other

10−4 (dashed) and D = 4.8

10−4 (solid), D = 7.8

sities D = 1.3
·
parameters µ = 0.95, q = 0.05.

·

·

T

2T

3T

4T

5T

22

0.05

0.1

0.15

0.2

0.25

0.3

0.35

√D
FIG. 10. Signal-to-noise ratio from the phenomenological model of Eq. (21). All parameters
are as in Fig. 8, with stimulus frequencies Ω = 0.1π (dashed), Ω = 0.33π (solid) and Ω = 0.5π
(dash-dotted).

]

B
d
[
 

R
N
S

n
e
h
p

6

5

4

3

2

1

0

−1

−2

−3

0

23


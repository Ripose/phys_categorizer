Minimum Entropy Approach to Word Segmentation Problems

Bin Wang

Institute of Theoretical Physics, Chinese Academy of Sciences,

P.O. Box 2735, Beijing 100080, P. R. China.

State Key Laboratory of Scientiﬁc and Engineering Computing,

Institute of Computational Mathematics and Scientiﬁc/Engineering Computing,

P.O. Box 2719, Beijing 100080, P. R. China.

Abstract

Given a sequence composed of a limit number of characters, we try to “read”

it as a “text”. This involves to segment the sequence into “words”. The diﬃ-

culty is to distinguish good segmentation from enormous number of random

ones. Aiming at revealing the nonrandomness of the sequence as strongly as

possible, by applying maximum likelihood method, we ﬁnd a quantity called

segmentation entropy that can be used to fulﬁll the duty. Contrary to

commonplace where maximum entropy principle was applied to obtain good

solution, we choose to minimize the segmentation entropy to obtain good

segmentation. The concept developed in this letter can be used to study

the noncoding DNA sequences, e.g., for regulatory elements prediction, in

eukaryote genomes.

0
0
0
2

 

g
u
A
9
2

 

 
 
]
h
p
-
o
i
b

.
s
c
i
s
y
h
p
[
 
 

1
v
2
3
2
8
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

I. INTRODUCTION.

The problem addressed in this paper is rather elementary in statistics. It is best described

as the following: suppose one who knows nothing about English language was given a

sequence of English letters, which was actually obtained by taking oﬀ all the interwords

delimiters among a sample of English text, how could he recover the words of the text by

choosing to insert spaces between adjacent letters? Note that the only thing he can consult

is the statistical properties of the sequence?

Any two adjacent letters can be chosen to belong to the same word (keep adjacent) as well

as belong to separate words (be separated by space). Suppose the sequence length is N. Any

choice on the connectivity between N − 1 pairs of adjacent letters is called a segmentation.

There are a total of 2N −1 possible segmentations. The word segmentation problem is to

ﬁnd ways to distinguish the correct segmentation – in the sense that adjacent letters in the

original text keep adjacent while letters separated by spaces and/or punctuation marks in

the original text are separated by spaces in the segmentation – from others.

Although the problem seems toy-like, its fundamental importance for statistical linguis-

tics is evident. We study on it, however, also for practical purposes. Noncoding sequences

in the genomes of species play essential rule on the regulation of gene expression and func-

tion [1]. However the development of computational methods for extracting regulatory

elements is far behand DNA sequencing and gene ﬁnding [2]. One reason is the lack of

eﬃcient way to discriminate large amount of sequence signals in noncoding DNA sequences.

Through linguistic study it has been shown that noncoding sequences in eukaryotic genomes

are structurally much similar to natural and artiﬁcial language [3]. Thus many may expect

to “read” the noncoding sequences as a “text”. Actually, eﬀorts have been given to build a

dictionary for genomes [4,5]. Li et al. [5] showed the connection between regulatory elements

prediction and word segmentation in noncoding DNA sequences of eukaryote genomes. We

expect that progress on word segmentation problem may help to deepen our knowledge on

noncoding regions of eukaryote genomes. Besides, word segmentation is an important issue

for Asian languages (e.g., Chinese and Japanese) processing [6], because they lack interword

delimiters.

2

II. SEGMENTATION ENTROPY AND ITS CONNECTION TO WORD

SEGMENTATION PROBLEM.

To tackle word segmentation problem, we ﬁrst consider a problem under constraints, so

that one important concept – segmentation entropy – can be introduced. The constraints

will be released at the end of this paper. Suppose we have known that there are nl words of

length l (l = 1, 2, · · ·) in the original text. Obviously,

Under these constraints – Words Length Constraints WLC – there are totally

nll = N.

X

l

(Pl nl)!
Ql nl!

(1)

(2)

segmentations. For example, for the following story, there are totally 3.12e144 segmenta-

tions, while the number under WLC is about 1.33e97.

The Fox and the Grapes

Once upon a time there was a fox strolling through the woods. He came upon

a grape orchard. There he found a bunch of beautiful grapes hanging from a high

branch.

“Boy those sure would be tasty,” he thought to himself. He backed up and took a

running start, and jumped. He did not get high enough.

He went back to his starting spot and tried again. He almost got high enough this

time, but not quite.

He tried and tried, again and again, but just couldn’t get high enough to grab the

grapes.

Finally, he gave up.

As he walked away, he put his nose in the air and said: “I am sure those grapes are

sour.”

Following least eﬀort principle [7], it is appreciable in natural languages to combine

existing words to express diﬀerent meaning. Shannon [8] pointed out the importance of

3

redundancy in natural languages long ago: generally speaking, nearly half of the letters in

a sample of English text can be deleted while someone else can still restore them. These

properties of natural language ensure the sequence obtained by taking oﬀ interword de-

limiters from a certain text being highly nonrandom and showing determinant and regular

characteristics. It is expected that the correct segmentation reveals these characteristics as

strongly as possible. From information point of view, this means that, if a form of infor-

mation entropy can be properly deﬁned on each segmentation, the entropy of the correct

segmentation will be the smallest.

Interestingly, a maximum likelihood approach leads to the same proposal and automati-

cally gives the deﬁnition of the entropy. Given one sequence of length N, we expect to ﬁnd a

likelihood function which reaches its maximum on the correct segmentation. For a concrete

segmentation, we assign a probability to each word in it

wi → pi,

i = 1...M

with

The likelihood function is written as

M

X
i=1

pi = 1.

Zs =

M

Y
i=1

mili

pi

(3)

(4)

(5)

where mi is the number of word wi in the segmentation, and li is the length of the word.

By maximizing the likelihood function subjected to eq.(4) we obtain

pi =

mili

N

.

Thus the maximum likelihood for the segmentation is

Zs =

M

Y
i=1

(

mili

N

)mili.

The segmentation with maximum likelihood is just the one minimizing

S = −

lnZs

N

= −

M

X
i=1

mili

N

ln(

mili

N

).

4

(6)

(7)

(8)

This function has the form of entropy [8] and will be called Segmentation Entropy (SE).

Starting from a maximum likelihood approach, we now come to the suggestion to mini-

mize the segmentation entropy. This is in contrast to commonplace. Maximizing likelihood

leads to maximizing certain entropy in some cases [9,10]. As a general principle for inves-

tigating statistical problems, maximum entropy method has been successfully applied in a

variety of ﬁelds [9,10]. We propose that, instead of applying maximum entropy principle,

one may choose to minimize certain entropy (minimum entropy principle) in some problems.

This seems attractive especially in the era of bioinformatics when most of the problems are

to reveal regularity in large amount of seemingly random sequences.

Because the present is a statistical method, the text under study needs to be not too

short. For example, when we tried to ﬁnd the segmentation with the smallest segmentation

entropy for the saying

God is nowhere as much as he is in the soul... and the soul means the world

(By Meister Eckhart, 14-century Dominican priest, Preacher, and Theologian), it was found

that, among a total of 343062720 segmentations under WLC, there are 15 segmentations

whose SE is 2.3684, smaller than 2.3802 of the correct one. One example is

god isnow he rea smuchas he is int he soul andt he soul meanst he world,

in which the ﬁve “he” and two “soul” are revealed.

Unfortunately, present computational power does not permit to exhaustively study even a

text as short as “the Fox and the Grapes”, the number of permitted segmentations for which

is 1.33E + 97 under WLC. We choose to see the relevance of the concept of segmentation

entropy in some special ways. The study focuses on “The Fox and the Grapes”.

To change a segmentation slightly, one way is to choose two adjacent words along the

sequences randomly and then exchange their length. This way the original two words may

change to diﬀerent words. This procedure can be repeated on the resulting segmentations.

The change does not violate the WLC. Because of the large number of possible choices in

each step, the segmentation is expected to become increasingly dissimilar to the original

one. Starting from the correct segmentation of “The Fox and the Grapes”, we expect to

see the evolution of SE by changing the segmentation this way. Figure 1 shows that SE

5

increase drastically in the ﬁrst 500 steps, and then reaches and ﬂuctuates around certain

equilibrium value. Compared with the gap between the equilibrium value and the original

SE, the ﬂuctuation is minor. This shows that, at least locally, the correct segmentation is

at the minimum of SE. Actually, we have traced a trajectory of evolution up to 1010 steps.

No segmentation with SE smaller than the correct one was observed. This implies that SE

of the correct segmentation is also globally minimal.

The distribution of segmentation entropy may give further insight to the atypicality

of the correct SE. We randomly sampled 1010 segmentations in the following way: while

keeping the WLC, the length of each words in the segmentation is assigned randomly. The

distribution of SE is shown in Fig. 2. The minimal SE we sampled is 4.5298, still much

higher than 4.097 of the correct segmentation (see Fig. 1). It is interesting to observe that

the distribution shows fractal characteristics. The fractal-like distribution presents also for

other text, even for random sequence (Fig. 3). The fractal-like feature is determined by the

WLC and the statistical structure of the sequence under study. In Fig. 3 we compared the

distribution of SE of two sequences (under the same WLC), the original sequence of “The

Fox and the Grapes” and a random sequence obtained by randomizing the order of letters

in the text. The result is in accordance with the fact that the original sequence is in a much

more ordered state, manifesting that segmentation entropy captures the statistical structure

of the sequences successfully.

There is one way to estimate the number of segmentations the SE of which is 4.097, the

value for the correct segmentation. See Fig. 4 in which the distribution of SE in Fig. 2 are

shown in logrithmic scale here. The left edge of the distribution fall on a line. The edge can

be ﬁtted by e(165x−750.42). The number of segmentations with SE x among the totally 1.33e97

possible segmentations under WLC is:

c(x) =

1.33e97
9 × 109

e(165x−750.42).

(9)

We obtained c(4.097) = 0.96. From the distribution of SE shown in Fig. 3(a) we obtained

the same value of c(4.097). The estimation support the idea that segmentation entropy of

correct segmentation is unique.

We now consider how to release the WLC. Unfortunately, searching the segmentation

with the smallest SE among all the possible is sure to fail to ﬁnd the correct one. For

6

example, SE of the segmentation in which the whole sequence is considered as one word

(single-word segmentation) is 0, the smallest possible SE. Also, the segmentation in which

each letter is viewed as a separate word (N-word segmentation) has a considerably small

SE (2.8655 for “The fox and the grapes”). These are called side attraction eﬀects. These

examples show that smaller SE does not necessarily means better segmentation when we

compare the SEs of segmentations under diﬀerent WLC (here WLC refers to any partition of

numbers of words of various length satisfying eq.(1), not necessarily the same as the original

text.) The bias induced by diﬀerent WLC must be taken oﬀ. In order to do so, we suggest

to use

RS =

S
S0

(10)

instead of S. Here S0 is the average SE under the same WLC of a sequence obtained by

randomizing the order of letters in the original text. S0 plays the role of chemical potential

for a thermodynamic system [11]. RS for the single word and N-word segmentations are 1,

the largest possible value. By searching segmentation with the smallest RS, it is expected

to ﬁnd meaningful segmentation. For examples, for the segmentation

god isnow he rea smuchas he is int he soul andt he soul meanst he world,

which has already been shown above, RS is 0.8601; while

god is now he re as much as he is int he soul an dt he soul me an st he world

is a better – actually one of the best – segmentation according to RS (RS = 0.8259).

Intuitively this is reasonable, because in this second segmentation, more repeated “words”

– two copies of “is”, “as” and “an” – are revealed. Another segmentation

god is now he re as much as he is in thesoul an d thesoul me an st he world,

which diﬀers from the second segmentation by revealing the two “thesoul”, has a moderately

small RS: 0.8481. Comparison shows that the ﬁve repeats of “he” is the most preferred part

in good segmentations.

7

III. CONCLUDING REMARKS.

In statistical linguistics many eﬀorts are given on signal extracting and statistical infer-

ence. Our method, however, is new on at least two points. First, there is neither assumption

on distribution [12] nor demand for training sets, lexical or grammatical knowledge [6]. This

feature is important for studying biological sequences, because present knowledge on the

“language” (DNA) of life is still lack. Second, instead of extracting a limit number of sig-

nals, we try to “read” the sequence exactly as a “text”. A text includes more than words: it

also includes the organization of words. The results of segmentation form a basis for many

further elaborations.

Principally, the concept of segmentation entropy can be applied to study the noncoding

DNA sequences of eukaryote genomes. It is expected that the study may gives more than

some meaningful “words” or regulatory elements. Possible applications are not conﬁned to

studying noncoding DNA sequences of course. Segmentation entropy can be used to ﬁnd

patterns in any symbolic sequences. However, the application of segmentation entropy is re-

stricted by the diﬃculty to ﬁnd the segmentation with the smallest Rs from the vast amount

possible ones. We are now developing algorithm that can be used for regulatory binding

sites prediction. in the algorithm the principle of minimun entropy will be incorporated in.

ACKNOWLEDGMENTS

I thanks Professor Bai-lin Hao who helps to make the computing possible. I also thanks

Professor Wei-mou Zheng and Professor Bai-lin Hao for stimulating discussions. Mr. Xiong

Zhang carefully read the manuscript. The work was supported partly by National Science

Fundation.

8

REFERENCES

[1] See, e.g., W. Li, Molecular Evolution (Sinauer Associates, 1997).

[2] A.G. Pedersen, P. Baldi, Y. Chauvin, and S. Brunak, Comput. Chem. 23, 191 (1997).

[3] R.N. Mantegna, S.V. Buldyrev, A.L. Goldberger, S. Havlin, C.-k. Peng, M. Simons, and H.E.

Stanley, Phys. Rev. Lett. 73, 3169 (1994).

[4] V. Brendel, J.S. Beckmann, and E.N. Trifonov, J. Biomol. Struct. Dyn. 7, 11 (1986); P.A.

Pevzner, M.Y. Borodovsky, and A.A. Mironov, J. Biomol. Struct. Dyn. 6, 1013 (1989).

[5] H.J. Bussemaker, H. Li, and E.D. Siggia, Preprint.

[6] J.M. Ponte, and W.B. Croft, UMass Computer Science Tech Rep. 1996-2002 (1996), available
at ftp://ftp.cs.umass.edu/pub/techrept/techreport/1996; R. Ando and L. Lee, Cornell CS
Report TR99-1756 (1999), available at http://www.cs.cornell.edu/home/llee/papers.html.

[7] G.K. Zipf, human Behavior and the Principle of Least Eﬀort (Addison-Wesley Press, Reading,

1949).

[8] C.E. Shannon, Bell System Tech. J. 27, 379 (1948).

[9] B.R. Frieden, J. Opt. Soc. Am. 62, 511 (1972); E.T. Jaynes, Phys. Rev. 106, 620 (1975); 108,

171 (1975).

[10] N. Wu, The Maximum Entropy Method and its Applications in Radio Astronomy, Ph.D. thesis

(Sydney University, 1985).

[11] See, e.g., L.E. Reichl, A Modern Course in Statistical Physics (Anorld, 1980).

[12] S.D. Peitra, V.D. Peitra, and J. Laﬀerty, IEEE Transactions Pattern Analysis and Machine

Intelligence 19, 1 (1997).

9

FIGURES

y
p
o
r
t
n
e
 
n
o
i
t
a
t
n
e
m
g
e
s

4.7

4.6

4.5

4.4

4.3

4.2

4.1

0

50 100 150 200 250 300 350 400

steps

0

500

1000

1500

2000
steps

2500

3000

3500

4000

4.7

4.6

4.5

4.4

4.3

4.2

4.1

y
p
o
r
t
n
e
 
n
o
i
t
a
t
n
e
m
g
e
s

FIG. 1. The evolution of segmentation entropy. Starting from the correct one, the segmentation

was change stepwisely by exchanging the lengths of a pair of adjacent words randomly chosen along

the sequence. The doted line corresponds to the smallest segmentation entropy 4.5298 for the 1010

randomly sampled segmentations, see Fig. 2.

10

6e+08

5e+08

4e+08

3e+08

2e+08

1e+08

n
o

i
t

u
b
i
r
t
s
d

i

0
4.64

4.65

4.66

4.67

4.68

4.69

segmentation entropy

4.7

4.71

FIG. 2. The distribution of the segmentation entropy of 9 ×109 segmentations randomly chosen

for the text “The Fox and the Grapes”. The numbers of words of various length in the original

text were ﬁrst counted. In the sampled segmentations these numbers were kept, but the length of

each word along the sequence were randomly assigned.

11

1.4e+08

1.2e+08

1e+08

original sequence

n
o
i
t
u
b
i
r
t
s
d

i

8e+07

6e+07

4e+07

2e+07

0
3e+08

2.5e+08

2e+08

n
o

i
t

random sequence

1.5e+08

u
b
i
r
t
s
d

i

1e+08

5e+07

0
4.64

4.65

4.66

4.67

segmentation entropy

4.68

4.69

4.7

4.71

FIG. 3. Comparison of the distribution of segmentation entropy for two sequences: the original

sequence of “The Fox and the Grapes”, and a random sequence obtained by randomizing the order

of letters in the original text. For each sequence, 109 segmentations are randomly sampled in the

way described in the caption of Fig. 2.

12

1e+09

1e+08

1e+07

1e+06

100000

n
o
i
t
u
b
i
r
t
s
d

i

10000

1000

100

10

1
4.54

4.56

4.58

4.6

4.62

Text segmentation

4.64

4.66

4.68

4.7

4.72

FIG. 4. The distribution of segmentation shown in Fig. 2 is shown in log scale here. The line

along the left edge of the distribution is e(165x−750.42).

13


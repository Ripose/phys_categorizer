9
9
9
1
 
l
u
J
 
4
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
2
4
0
7
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Some Exact Results of Hopﬁeld Neural
Networks and Applications

Hong-Liang Lu#, and Xi-Jun Qiu

Physics Department, Science College, Shanghai University

Abstract

A set of ﬁxed points of the Hopﬁeld type neural network was under investigation.

Its connection matrix is constructed with regard to the Hebb rule from a highly

symmetric set of the memorized patterns. Depending on the external parameter the

analytic description of the ﬁxed points set had been obtained. And as a conclusion,

some exact results of Hopﬁeld neural networks were gained.

PACS Number(s): 87.10.+e, 05.45.+b

Key Words: Neural Networks, Hebb rule, Fixed points

Over the past decade, there has been an explosion of interest in so-called artiﬁcial neural

network(ANN) technology. ANNs are a model of computing inspired by the brain[1]-[5],

consisting of a collection of model ”neurons” connected by model ”synapses.” Compu-

tation in the network is performed in a distributed fashion, by propagating excitatory

and inhibitory activations across the synapses, and computing the neuronal outputs as a

nonlinear (typically sigmoidal) function of total synaptic input. These networks also have

a capacity to ”learn” to perform a given computation by adjusting real-valued synaptic

connection strengths (weight values) between units. ANNs are of considerable interest

both for biological modeling of information processing in the nervous system, and for

solving many classes of complex real-world applications.

# Hong-Liang Lu, Corresponding author, Ph.D student major to Radio Physics in Physics Depart-
ment, Science College. Address: Physics Department, Science College, 20 Chengzhong Road, Jiading,
Shanghai 201800, China. Email: xjqiu@srcap.stc.sh.cn

Xi-Jun Qiu, Professor and Ph.D supervisor of Theoretical Physics, Physics Department, Shang-

hai University.

1

J.J.Hopﬁeld boosted neural network research at the beginning of the 1980s with the

publication of a famous paper on artiﬁcial neural networks, which he used for pattern

completion and to solve optimization problems[4]. These networks consist of one layer of

neurons that are completely connected with each other. Hopﬁeld analysed the behavior

of networks belonging to that type, and could prove mathematically that stable behavior

may be achieved under certain conditions.

It can be shown that the dynamic behavior of Hopﬁeld type neural networks is de-

scribed by an energe surface. Each network state corresponds to a certain position on

that surface. Through external clamping, neurons may be forced to certain states of

activity, and thus the whole network may be forced to move to a well deﬁned point on

the energy surface. If the network is released, i.e. external clamping is removed, it will

change its state in such a way that it moves on the energy surface towards new states of

lower energy. Finally, neuron states will stop changing if a local minimum in the energy

surface is reached. Through careful selection of weights, ocillations will be avoided. A

set of ﬁxed points of the Hopﬁeld type neural network[4][6]is under investigation.

Its

connection matrix is constructed with regard to the Hebb rule from a (p × n)-matrix S

of memorized patterns:

1 − x
1
...
1

1

. . .
1 . . . 1
1 − x . . .
1 . . . 1
...
...
. . .
. . . 1 − x 1 . . . 1

1
1
...

...
1

. . .

.









S =









Here n is the number of neurons, p is the number of memorized patterns ~s(l), which are

the rows of the matrix S, and x is an arbitrary real number.

Depending on x the memorized patterns ~s(l) are interpreted as p distorted vectors of

the standard

~ε(n) = (1, 1, . . . , 1

).

n
{z

}

|

(1)

We denote by ~ε(k) the conﬁguration vector which is collinear to the bisectrix of the

principle orthant standard-vector. Next, n is the number of the spin variables, p is the

number of the memorized patterns and q = n − p is the number of the nondistorted

coordinates of the standard-vector. Conﬁguration vectors are denoted by small Greek

letters. We use small Latin letters to denote vectors whose coordinates are real.

2

The problem is as follows: the network has to be learned by p-times showing of the

standard (1), but a distortion has slipped in the learning process. How does the ﬁxed points

set depends on the value of this distortion x?

Depending on the distortion parameter x the analytic description of the ﬁxed points

set has been obtained. It turns out to be very important that the memorized patterns

~s(l) form a highly symmetric group of vectors: all of them correlate one with another in

the same way:

(~s(l), ~s(l′)) = r(x),

(2)

where r(x) is independent of l, l′ = 1, 2, . . . , p. Namely this was the reason to use the

words ”highly symmetric” in the title.

It is known [7], that the ﬁxed points of a network of our kind have to be of the form:

~σ∗ = (σ1, σ2, . . . , σp, 1, . . . , 1),

σi = {±1}, i = 1, 2, . . . , p.

(3)

Let’s join into one class Σ(k) all the conﬁguration vectors ~σ∗ given by Eq.(3), which have

k coordinates equal to ”–1” among the ﬁrst p coordinates. The class Σ(k) consists of C k
p

conﬁguration vectors of the form (3), and there are p + 1 diﬀerent classes (k = 0, 1, . . . , p).

Our main result can be formulated as a Theorem.

Theorem. As x varies from −∞ to ∞ the ﬁxed points set is exhausted in consecutive

order by the classes of the vectors[8]

Σ(0), Σ(1), . . . , Σ(K),

and the transformation of the ﬁxed points set from the class Σ(k−1) into the class Σ(k)

occurs when x = xk:

xk = p

n − (2k − 1)
n + p − 2(2k − 1)

,

k = 1, 2, . . . , K.

n−1 < 1

If p−1
realized one after another and K = p. If p−1

3, according this scheme all the p transformations of the ﬁxed points set are

n−1 > 1

3, the transformation related to

is the last. The network has no other ﬁxed points.

K =

n + p + 2
4

(cid:21)

(cid:20)

3

The Theorem makes it possible to solve a number of practical problems. We would

like to add that the Theorem can be generalized onto the case of arbitrary vector

~u = (u1, u2, . . . , up, 1, . . . , 1),

p

Xi=1

u2
i = p

being a standard instead the standard (1). Here memorized patterns ~s(l) are obtained by

the distortion of the ﬁrst p coordinates of the vector ~u with regard to the fulﬁllment of

The obtained results can be interpreted in terms of neural networks, Ising model and

Eqs.(2).

factor analysis.

Acknowledgement The authors acknowledge the beneﬁt of extended interaction

with Miss Jie-yan Bai of Shanghai Research and Development Center for Fiber Optic

Technology, Shanghai 803 Research Institute, who has helped us develop or clarify several

ideas and issues that appear in this paper. We also extend our thanks to Prof. Yu-Long

Mo of the School of Information and Communication Engineering, Shanghai University.

References

[1] McCulloch M.S. W.Pitts, A logical caculus of the ideas immanent in nervous activity,

Bull. of Math. Biophys.5, 1943,pp.115-133

[2] Hebb D.O., The organization of Behavior, Wiley, New York,1949

[3] Rosenblatt F., Principles of Neurodynamics,Spartan Books, 1962

[4] Hopﬁeld J.J., Neural networks and physical systems with emergent collective com-

putational abilities, Proc. Natl. Acad. Sci. U.S.A., Vol.79, 1982, pp.2554-2558

[5] Rumelhart D.E., McClelland J.L., Parallel Distributed Processing, Vol.1, 2, MIT

Press, Cambridge, MA, 1986

[6] Hopﬁeld J.J., Neurons with Graded Respone have collective computational prop-

erties like those of two-state neurons, Proc. Natl. Acad. Sci. U.S.A., Vol.81, 1984,

pp.3088-3092

4

[7] L.B.Litinsky. Direct calculation of the stable points of a neural network. Theor. and

Math. Phys.101, 1492 (1994)

(1999)

[8] L.B.Litinsky. Fixed points of Hopﬁled type neural networks, cond-mat/9901251

5


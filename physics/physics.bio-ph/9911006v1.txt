9
9
9
1
 
v
o
N
 
4
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
6
0
0
1
1
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Genetic Algorithms in Time-Dependent Environments

Christopher Ronnewinkel,

Claus O. Wilke and Thomas Martinetz

temporary address:

Institut f¨ur Neuroinformatik,
Ruhr-Universit¨at Bochum,
D-44780 Bochum, Germany

Institut f¨ur Neuro- und Bioinformatik,
Universit¨at L¨ubeck,
Ratzeburger Allee 160,
D-23538 L¨ubeck, Germany

Contact: ronne@neuroinformatik.ruhr-uni-bochum.de

(to be published in the Proceedings of the 2nd EvoNet Summerschool,
Natural Computing Series, Springer)

November 2, 1999

Abstract

The inﬂuence of time-dependent ﬁtnesses on the inﬁnite population dynamics
of simple genetic algorithms (without crossover) is analyzed. Based on general
arguments, a schematic phase diagram is constructed that allows one to charac-
terize the asymptotic states in dependence on the mutation rate and the time
scale of changes. Furthermore, the notion of regular changes is raised for which
the population can be shown to converge towards a generalized quasispecies.
Based on this, error thresholds and an optimal mutation rate are approximately
calculated for a generational genetic algorithm with a moving needle-in-the-
haystack landscape. The so found phase diagram is fully consistent with our
general considerations.

Genetic algorithms (GAs) as special instances of evolutionary algorithms have been
established during the last three decades as optimization procedures, but mostly for
static problems (see [1] for an overview and [2] for an in-depth presentation of the ﬁeld).
In view of real-world applications, such as routing in data-nets, scheduling, robotics
etc., which include essentially dynamic optimization problems, there are two alterna-
tive optimization strategies. On the one hand, one can take snapshots of the system
and search “oﬄine” for the optimal solutions of the static situation represented by each
of these snapshots. In this approach, the algorithm is restarted for every snapshot and
solves the new problem from scratch. On the other hand, the optimization algorithm
might reevaluate the real, current situation in order to reuse information gained in the
past. In this case, the algorithm works “online”. As can be argued from the analo-
gies to natural evolution, evolutionary algorithms seem to be promising candidates for
“online” optimization [1, 3]. The reevaluation of the situation or environment then
introduces a time-dependency of the ﬁtness landscape. This time-dependency occurs
as external to the algorithm’s population and does not emerge from coevolutive inter-
actions. Coevolutive interactions as an alternative source of time dependency in the
ﬁtness landscape are not within the scope of this work.

1

In the last years, many diﬀerent methods and extensions of standard evolutionary
algorithms for the case of time-dependent ﬁtnesses have been analyzed on the basis of
experiments (see [3] for a review) but only seldom on the basis of theoretical arguments
(see [4, 5]). To take a step into the direction of a better theoretical understanding of
“online” evolutionary algorithms, we will study the eﬀects of simple time dependencies
of the ﬁtness landscape on the dynamics of GAs (without crossover), or more generally
saying, of populations under mutation and probabilistic selection. As we will see, it is
possible to characterize the asymptotic states of such a system for a particular class
of dynamic ﬁtness landscapes that is introduced below. The asymptotic state forms
the basis on which it can be decided whether the population is able to adapt to, or
track, the changes in the ﬁtness landscape. Our mathematical formalism applies to
GAs as well as to biological self-replicating systems, since the analyzed GA model and
Eigen’s quasispecies model [6, 7, 8] in the molecular evolution theory (see [9] for a
recent review) are very similar. Hence, all introduced concepts for GAs are valid and
relevant in analogous form for molecular evolutionary systems.

In the following section, we will introduce the model to be analyzed and show the
correspondence to the quasispecies model. Then, we will introduce the mathematical
framework, based on which we will formally characterize the asymptotic state as ﬁxed
point. After presenting the main concepts, we will proceed with the construction of a
phase diagram that allows to characterize the order found in the asymptotic state for
diﬀerent parameter settings. Finally, a moving needle-in-the-haystack (NiH) landscape
is analyzed and its phase diagram, including the optimal mutation rate, is calculated.

1 Mathematical Framework

In order to study the inﬂuence of a time-dependent ﬁtness landscape on the dynamics of
a genetic algorithm (GA), we consider GAs to be discrete dynamical systems. A detailed
introduction to the resulting dynamical systems model is given by Rowe [10] (in this
book). Here, we will only shortly introduce the basic concepts and the notations we
use within the present work.

The GA is represented as a generation operator G(m)

acting on the space Λm of
all populations of size m for some given encoding of the population members. If we
choose the members i to be encoded as bit-strings of length l, this state space is given
by

t

Λm = {(n0, . . . , n2l−1)/m |

i ni = m, ni ∈ N0},

where ni denotes the number of bit-strings in the population, which are equal to the
binary representation of i ∈ {0, . . . , 2l − 1}.

P

The generation operator maps the present population onto the next generation,

x(t + 1) = G(m)

[x(t)].

t

This is achieved by applying a sampling procedure that draws the members of the next
generation’s population x(t+1) according to their expected concentrations hx(t+1)i ∈
Λ∞ which are deﬁned by the mixing [10, 11] and the selection scheme. For an inﬁnite
population size, the sampling acts like the identity resulting in

G(∞)

t x(t) = x(t + 1) = hx(t + 1)i.

2

t

Hence, Gt := G(∞)
represents in fact the mixing and selection scheme. For ﬁnite
population size, hx(t + 1)i ∈ Λ∞ is approximated by using the sampling process to
obtain x(t + 1) ∈ Λm. The deviations thereby possible become larger with decreasing
m and distort the ﬁnite population dynamics as compared to the inﬁnite population
case. This results in ﬂuctuations and epoch formation as shown in [10, 11, 12]. In
the following, we will consider the inﬁnite population limit, because it reﬂects the
exact ﬂow of probabilities for a particular ﬁtness landscape.
In a second step, the
ﬂuctuations and epoch formation introduced by the ﬁniteness of a real population can
be studied on the basis of that underlying probability ﬂow.

The generation operator is assumed to decompose into a separate mutation and a

separate selection operator, like

Gt = M · S(t),

(1)

where the selection operator S(t) contains the time dependency of the ﬁtness land-
scape. Crossover is not considered in this work.

Inspired by molecular evolution, and also by common usage, we assume that the
mutation acts like ﬂipping each bit with probability µ. If we set the duration of one
generation to 1, µ equals to the mutation rate. The mutation operator then takes on
the form

1 − µ
µ

µ
1 − µ

M =

(cid:18)

⊗l

(cid:19)

,

i. e. Mij = µdH(i,j)(1 − µ)l−dH(i,j),

where ⊗ denotes the Kronecker (or canonical tensor) product and dH(i, j) denotes the
Hamming distance of i and j.

To keep the description analytically tractable, we will focus on ﬁtness-proportionate

selection,

S(t) · x = F (t) · x

hf (t)ix, where F (t) = diag

(cid:14)

and hf (t)ix =

f0(t), . . . , f2l−1(t)
i fi(t)xi = kF (t) · xk1.
(cid:1)

(cid:0)

This will already provide us with some insight into the general behavior of a GA in
time-dependent ﬁtness landscapes.

P

Since the GA corresponding to Eq. 1 applies mutation to the current population and
selects the new population with complete replacement of the current one, it is called
a generational GA (genGA). In addition to genGAs, steady-state GAs (ssGAs) with a
two step reproduction process are also in common use: First, a small fraction γ of
the current population is chosen to produce mγ mutants according to some heuristics.
Second, another fraction γ of the current population is chosen to get replaced by those
mutants according to some other heuristics (see [14, 15, 16] and references therein). We
can include ssGAs into our description in an approximate fashion by simply bypassing a
fraction (1 − γ) of the population into the selection process without mutation, whereas
the remaining fraction γ gets mutated before it enters the selection process. The
generation operator then reads

Gt = [(1 − γ)1 + γM] S(t).

(2)

By varying γ within the interval ]0, 1], we can interpolate between steady-state be-
havior (ssGA) for γ ≪ 1 and generational behavior (genGA) for γ = 1. Equation 2 is

3

only an approximation of the true generation operator for ssGAs because the heuristics
involved in the choice of the mutated and replaced members are neglected. But in the
next section, the heuristics are expected to play a minor role for our general conclusion
on an inertia of ssGAs against time-variations.

At this point, we want to review shortly the correspondence of our GA model with
the quasispecies model, extensively studied by Eigen and coworkers [6, 7, 8] in the
context of molecular evolution theory (see also [13] in this book). The quasispecies
model describes a system of self-replicating entities i (e. g. RNA-, DNA-strands) with
replication rates fi and an imperfect copying procedure such that mutations occur.
For simplicity reasons, the overall concentration of molecules in the system is held
constant by an excess ﬂow Φ(t). In the above notation, the continuous model reads

˙x(t) = [M · F (t) − Φ(t)] x(t),

(3)

where the ﬂux needs to equal the average replication, Φ(t) = hf (t)ix(t), in order to
keep the concentration vector x(t) normalized. This model might then be discretized
via t → t/δt, which unveils the similarity to a ssGA:

x(t + 1) =

(1 − δt hf (t)ix(t))1 + δt M · F (t)

x(t)

for δt ≪ 1.

(4)

(cid:3)

(cid:2)

By comparison with Eq. 2, we can easily read oﬀ that γ = δt hf (t)ix(t) =: γx(t). This
means a low (resp. high) average ﬁtness leads to a small (resp. large) replacement – a
property that is not wanted in the context of optimization problems, which GAs are
usually used for, because one does not want to remain in a region of low ﬁtness for a
long time. Another diﬀerence to ssGAs is the fact that in the continuous Eigen model,
selection acts only on the mutated fraction of the population – although this leads
only to subtle diﬀerences in the dynamics of ssGAs and the Eigen model.

Equation 3 is commonly referred to as ‘continuous Eigen model’ in the literature,
because of the continuous time, and Eq. 4 is simply its discretized form which can
be used for numerical calculations. Nonetheless, the notion ‘discrete Eigen model’ is
seldom used for Eq. 4 but it is often used for the genGA,

x(t + 1) = [M · S(t)] x(t),

(5)

in the literature. This stems from the identical asymptotic behavior of Eqs. 4 and 5
for static ﬁtness landscapes. However, there are diﬀerences for time-dependent ﬁtness
landscapes, as we will see in the following two sections.

2 Regular Changes and Generalized Quasispecies

In the case of a static landscape, the ﬁxed points of the generation operator, which
are in fact stationary states of the evolving system (if contained within Λm, see [10]),
can be found by solving an eigenvalue problem, because of

x = Gx ⇐⇒ MF x = hf ixx .

(6)

Let λi and vi denote the eigenvalues and eigenvectors of MF with descending order
λ0 ≥ · · · ≥ λ2l−1 and kvik1 = 1. For µ 6= 0, 1 the Perron-Frobenius theorem assures

4

the non-degeneracy of the eigenvector v0 to the largest eigenvalue and moreover it
assures v0 ∈ Λ∞. Often, v0 is called Perron vector. After a transformation to the
basis of the eigenvectors {vi} it can be straightforwardly shown that x(t) converges
to v0 for t → ∞. The population represented by v0 was called the ‘quasispecies’ by
Eigen, because this population does not consist of only a single dominant genotype,
or string, but it consists of a particular stable mixture of diﬀerent genotypes.

Let us now consider time-dependent landscapes. If the time dependency is intro-

duced simply by a single scalar factor, like

F (t) = F ρ(t) with ρ(t) ≥ 0 for all t,

it immediately drops out of the selection operator for GAs. For the continuous Eigen
model, we note that the eigenvectors of F (t) and F are the same and that λi(t) =
λi ρ(t). Since ρ(t) ≥ 0, which is necessary to keep the ﬁtness values positive, the
order of the eigenvalues remains, such that MF (t) will show the same quasispecies
v0 as MF . Contrasting to that special case, a general, individual time dependency
of the string’s ﬁtnesses does indeed change the eigenvalues and eigenvectors of MF (t)
compared to MF . For an arbitrary time dependency the Perron vector is constantly
changing, and therefore, we cannot even deﬁne a unique asymptotic state. However,
this problem disappears for what we call regular changes. After having established a
theory for such changes, we can then take into account more and more non-regular
ingredients. What do we mean by “regular change”? We deﬁne it heuristically in the
following way: a regular change is a change that happens with ﬁxed duration τ and
obeys some deterministic rule that is the same for all change cycles. Let us express the
latter more formally and make it more clear what we mean by “same rule of change”.
Within a change cycle, we allow for an arbitrary time dependency of the ﬁtness, up to
the restriction that two diﬀerent change cycles must be connected by a permutation
of the sequence space. Thus, if the time dependency is chosen for one change cycle, e.
g. the ﬁrst change cycle starting at t = 0, it is already ﬁxed for all other cycles, apart
from the permutations. We will represent permutations π from the permutation group
S

2l of the sequence space as matrices

(Pπ)ij = δπ(i),j

for i, j ∈ {0, . . . , 2l − 1}.

The permutations of vectors x and matrices A are obtained by

(Pπx)i = xπ(i)

and (PπAP T

π )i,j = Aπ(i),π(j),

where P T

π denotes the transpose of Pπ with the property P T

π = Pπ−1 = P −1
π .

In reference to the ﬁrst change cycle, we deﬁne the ﬁtness landscape F (t) as being
single-time-dependent, if and only if for each change cycle n ∈ N0 there exists a
permutation πn ∈ S2l, such that for all cycle phases ϕ ∈ {0, . . . , τ − 1}

Pn F (ϕ + nτ ) P T

n = F (ϕ)

(abbreviatory Pn := Pπn).

We will call each permutation Pn a jump-rule, or simply rule, which connects F (ϕ+nτ )
and F (ϕ). To make predictions about the asymptotic state of the system, we need
to relate the generation operators of diﬀerent change cycles to each other. This is

5

readily achieved if the permutations Pn commute with the mutation operator M. The
condition for this being the case is that for all i, j,

Mij = Mπn(i),πn(j)

or equivalently dH(i, j) = dH

πn(i), πn(j)

.

Thus, the Hamming distances dH(i, j) need to be invariant under the permutations
Pn. Geometrically this means that the ﬁtness landscape gets “translated” or “rotated”
by those permutations without changing the neighborhood relations. Then, we ﬁnd
for arbitrary n ∈ N and ϕ ∈ {0, . . . , τ − 1},

(cid:0)

(cid:1)

Gϕ+nτ = P T

n GϕPn.

(7)

To study the asymptotic behavior of the system, it is useful to accumulate the time
dependency of a change cycle by introducing the τ -generation operators,

Γn := Gτ −1+nτ · · · Gnτ

for all n ∈ N0.

Because of Eq. 7, all these operators are related to Γ0 by

Γn = P T

n Γ0Pn,

This property allows us to write the time evolution of the system in the form

x(ϕ + nτ ) = P T

n−1Γ0Pn−1 · · · P T

1 Γ0P1 Γ0 x(ϕ),

(8)

where ϕ ∈ {0, . . . , τ − 1} denotes in the following always the phase within a cycle.

Let us consider the special case of a single rule P being applied at the end of each
change cycle, which results in Pn = (P )n, e. g. imagine a ﬁtness peak that moves at a
constant “velocity” through the string space. We will see below that for those cases
it is possible to identify the asymptotic state with a quasispecies in analogy to static
ﬁtness landscapes. Because of that, we can now deﬁne the notion of regularity of a
ﬁtness landscape formally in the following manner:

A time-dependent ﬁtness landscape F (t) is regular , if and only if: (i) the ﬁtness
landscape is single-time-dependent, (ii) there exists some rule P ∈ S
2l which is applied
at the end of each cycle such that Pn = (P )n, and (iii) the rule P commutes with the
mutation operator M.

In this case, we get with P P T = 1 the time evolution

x(ϕ + nτ ) =

n

P T

P Γ0

n x(ϕ).

To proceed, it is useful to permute the concentrations compatible to the rule of the
ﬁtness landscape. By this, concentrations are measured in reference to the ﬁtness
landscape structure of the start cycle n = 0. We will denote those concentrations by
x′(t) and they are related to the concentrations x(t) by

(cid:0)

(cid:1)

(cid:0)

(cid:1)

x′(ϕ + nτ ) = (P )n x(ϕ + nτ )

= (P Γ0)n x(ϕ)

and x′(ϕ) = x(ϕ).

For example, if there is no time-dependency within the cycles, some x′
i will for all
cycles measure the concentration of the highest ﬁtness string, independent of its current

(9)

(10)

6

position in string space. Thus, x′(t) evolves in a ﬁtness landscape with periodic change,
which can also be seen from the second line of Eq. 10. In analogy to the static case
Eq. 6, the calculation of ﬁxed points of x′(t) is equivalent to an eigenvalue problem,

x′(t + τ ) = x′(t) ⇐⇒ P

Γ0 x′(t) = kP

Γ0 x′(t)k1 x′(t),

Γ0 is the unnormalized τ -generation operator obtained from the accumulation

where
of the unnormalized generation operators

e
e
Gϕ = MF (ϕ).

e

The corresponding periodic quasispecies v0 can be calculated for all phases ϕ of
e

the change cycle from the Perron vector v0 of P Γ0 in the following way,

x′(ϕ + nτ ) n→∞

−−−→ v0(ϕ) = Gϕ−1 · · · G0 v0

for ϕ ∈ {0, . . . , τ − 1}.

(11)

To ﬁnd the asymptotic states of the concentrations x(t), we simply need to invert Eq.
10,

x(ϕ + ντ ) =

P T

νx′(ϕ + ντ )

for ν ∈ {0, . . . , η − 1},

(12)

where η := ord P is the order of the group element P ∈ S2l.
(cid:0)

(cid:1)

The essential reason for the existence of asymptotic states for x(t) lies in the
2l. Because of P η = 1, we ﬁnd directly from Eq.

ﬁniteness of the permutation group S
9 the asymptotic state

x(ϕ + ˜nη τ ) = (P Γ0)η ˜n x(t) ˜n→∞

−−−→ v0(ϕ),

where v0(ϕ) is the same as in Eq. 11, because (P Γ0)η and P Γ0 have the same eigen-
vectors, in particular the same Perron vector. Moreover, we get

x

ϕ + (ν + ˜nη)τ

˜n→∞
−−−→

P T

νv0(ϕ)

for ν ∈ {0, . . . , η − 1},

(13)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

which is the same result as Eqs. 11 and 12 yield. In the limit of long strings l → ∞,
ord P is not necessarily ﬁnite anymore. If ord P l→∞
−−−→∞, then the asymptotic states
Eq. 13 for x(t) do not exist, but Eq. 11 still holds. Hence, a quasispecies exists even
in the limit l → ∞ if measured in reference to the structure of the ﬁtness landscape.
In conclusion, Eqs. 11 and 13 represent the generalized quasispecies for the class
of regular ﬁtness landscapes which includes as special cases static and periodic ﬁtness
landscapes. In fact, the simplest case of a regular change is a periodic variation of
the ﬁtness values fi(t) = fi(t + τ ) because no permutations are involved (P = 1) and
hence x′(t) = x(t) for all t. The quasispecies was generalized for this case already in
[17] and – using a slightly diﬀerent formalism – in [4]. In Section 4, we will study a
more complicated example.

3 Schematic Phase Diagram

To get an intuitive feeling for the typical behavior of ssGAs and genGAs, let us consider
some special lines in the plane spanned by the mutation rate µ and the time scale for
changes τ , as shown in Fig. 1. The mutation operator represents only for µ < 1/2 a
copying procedure with occurring errors, whereas for µ > 1/2 it systematically tends
to invert strings, i. e. it resembles an inverter with occurring errors. Since mutation
should introduce weak modiﬁcations to the strings, we will consider only µ ≤ 1/2.

7

0.5

µ

e
t
a
r
-
n
o
i
t
a
t
u
m

0

∼ 1/γ

disorder line

disordered phase

ssGA
time-average

error-threshold

c
i
t
a
t
s
-
i
s
a
u
q

time-average

time-scale for changes τ

Figure 1: Schematic phase diagram: time-average regions due to low mutation (dark gray)
and large inertia (light gray, left), quasi-static region for slow changes (light gray,
right).

Disorder line: For µ = 1/2, the Perron vector of MF (t) is always vT

0 = (1, . . . , 1)/2l.
The population will therefore converge towards the disordered state. Because of
the continuity of M in µ, we already enter a disordered phase for µ ≈ 1/2.

Time-average region: For µ = 0, the mutation operator is the identity. We ﬁnd as
time evolution simply the product average over the ﬁtness of the evolved time
steps:

x(t + τ ) =

S(ϕ)

x(t)

t+τ −1

"

ϕ=t
Y

#

(cid:14)

= ˜F (t + τ, t) x(t)

k . . . k1 with ˜F (t + τ, t) =

t+τ −1
ϕ=t F (ϕ).

Q

Since diagonal operators commute, the order in which the F (ϕ) get multiplicated
does not make any diﬀerence. For the case of a τ -periodic landscape, ˜F =
˜F (t + τ, t) = ˜F (τ, 0) is independent of t. The quasispecies is then a linear
superposition of the eigenvectors of the largest eigenvalue of the product averaged
ﬁtness landscape ˜F – there might be more then one such eigenvector, since
˜F is diagonal and the Perron-Frobenius theorem does not apply. Because of
the continuity of M in µ the dynamics are governed already for 0 < µ ≪ 1
by the product average ˜F . Analogous conclusions apply to those non-periodic
landscapes for which by choosing a suitable time scale τ a meaningful average
˜F (t + τ, t) can be deﬁned.

8

f

f

f

τ

τ

f

genotype

genotype

genotype

genotype

0.5

µ∗
av

µ

e
t
a
r

r
o
r
r
e

0.5

µ

e
t
a
r

r
o
r
r
e

disordered

disordered

temporarily
ordered

ordered

ordered

disordered

oscillation period τ

cycle length τ

Figure 2: Phase diagrams for (left): needle-in-the-haystack with oscillating height at fre-
quency ω = 2π/τ , (right): needle-in-the-haystack that jumps after τ time steps
to a randomly chosen nearest neighbor.

For ssGAs, γ is small and we ﬁnd to ﬁrst order in τ γ:

x(t + τ ) = (1 − τ γ) ˜F (t + τ, t)

τ −1

1
τ

(cid:18)

ϕ=0
X

+ τ γ

S(t + τ ) · · · M

· · · S(t)

+ O

(τ γ)2

.

ϕth factor from left

(cid:19)

(cid:0)

(cid:1)

If τ γ ≪ 1 holds, the time evolution is governed by ˜F (t, t + τ ). For changes on
a time scale τ , we ﬁnd time-averaged behavior if τ ≪ 1/γ. Thus, the width of
the time-average region is proportional to 1/γ. A detailed analysis of the eﬀect
of the diﬀerent positions of the mutation operator M within the τ γ-term, which
is otherwise an arithmetic time-average, has not yet been carried out.

|{z}

Quasi-static region: If the changes happen on a time scale τ very large compared
to the average relaxation time (∼ 1/hλ0 − λ1i) the quasispecies grows nearly
without noticing the changes. Thus, in the quasi-static region all quasispecies
that might be expected from the static landscapes ˜F = F (t) will occur at some
time during one cycle τ .

Wilke et al. raise in [18] the schematic phase diagram of the continuous Eigen
model, which exhibits the same time-average phases as that for ssGAs. Their result is
in perfect agreement with two recently, explicitly studied time-dependent landscapes.
First, Wilke et al. studied in [17] a needle-in-the-haystack (NiH) landscape with oscil-
lating, τ -periodic ﬁtness of the needle, i. e.

f0(t) > f1 = · · · = f2l−1 = 1

and f0(t) = σ exp {ε sin(2π t/τ )} .

The continuous model was represented for δt → 0 as Eq. 4 and the periodic quasispecies
Eq. 11 was calculated. Figure 2 (left) shows the resulting phase diagram. For small τ ,

9

f

00

00

genotype

00

10

b its
11

h e r

h ig
01

01

11
lower bits

10

00

10

00

11

01

Figure 3: A regularly moving needle-in-the-haystack for string length l = 4.

In (left),
the solid arrow represents the next jump to happen, whereas the gray and solid
arrows all together represent the jumps that happen one after the other under
the rule P of rotating the two lower bits as shown in (right) with rotation angle
π/2 at every jump.

the error threshold is given by the one of the time-averaged landscape, whereas for large
τ , the error threshold oscillates between minimum and maximum values corresponding
to mint f0(t) and maxt f0(t), as expected in the quasi-static regime. Second, Nilsson
and Snoad studied in [19] a moving NiH that jumps randomly to one of its nearest
neighbor strings every τ time steps. The time-average of this landscape over many
jump cycles is a totally ﬂat or neutral landscape, which explains the extension of the
disordered phase to small µ and small τ as it is shown in Fig. 2 (right). In the quasi-
static region, order is expected because the needle stays long enough at each position
for a quasispecies to grow. Hence, we can understand the existence of the observed
and calculated phase diagrams in Fig. 2 from simple arguments.
In fact, they are
special instances of the general schematic phase diagram depicted in Fig. 1.

In the following, we will consider regularly moving NiHs and derive the inﬁnite
population behavior of a genGA in such landscapes. This is interesting, since genGAs
should be considered to adapt faster to changes compared to ssGAs, as the missing
time-average region of genGAs for small τ suggests. To clarify whether a diﬀerent
phase diagram compared to Fig. 2 (right) emerges for genGAs with moving NiH, we
will calculate the phase diagram including the optimal mutation rate that maximizes
a lower bound for the concentration of the needle string in the population.

4 Generational GA and a moving NiH

In this section, we want to analyze quantitatively the asymptotic behavior of a genGA
with NiH that moves regularly in the sense of Section 2 to one of its l nearest neighbors
every τ time steps. At the end, we will also be able to comment on the case of a NiH
that jumps randomly to one of its nearest neighbors.

A simple example of a NiH that moves regularly to nearest neighbors is shown
in Fig. 3 (left). Each jump corresponds to a π/2-rotation of the four-dimensional
hypercube {0, 1}4 along the 1100 axis, i. e. the lower two bits are rotated as shown
in Fig. 3 (right). We will call the set of strings {P n i | n ∈ N} which is obtained by

10

110

010

100

000

111

011

101

001

P≪
000→000 111→111
001→010 011→110
010→100 110→101
100→001 101→011

Figure 4: The equivalence of a 2π/3-rotation along the 1 · · · 1 axis and a cyclic 1-bit left-

shift, denoted by P≪, for string length l = 3.

applying the same rule P ∈ S2l over and over to some initial string i ∈ {0, 1}l, the
orbit of i under P . The period length 4 of the orbit shown in Fig. 3 (left) originates
from the rotation angle π/2 and hence is independent of the string length l. The orbits
of such rotations will always be restricted to only four diﬀerent strings. For reasons
that will become clear below, we are looking for regular movements of the needle that
are not restricted to such a small subspace of the string space. Instead, the needle
is supposed to move ‘straight away’ from previous positions in string space. Since a
complete classiﬁcation and analysis of all possible regular movements for given string
length l and jump distance d is out of the scope of this work, we will simply give an
example of a rule P ∈ S
2l that generates such movements: the composition of a cyclic
1-bit left-shift, which we denote by P≪, and an exclusive-or with 0 · · · 01, which we
denote by P⊕. For string length l ≤ 3, P≪ corresponds to a 2π/l rotation along the
1 · · · 1 axis as can be seen in Fig. 4. Moreover, the orbit of 0 · · · 0 under P⊕≪ = P⊕ ◦P≪
is shown in Fig. 5 also for l = 3. For arbitrary string length l, it is more diﬃcult to
visualize the action of P≪ and hence of P⊕≪. But, it is easily veriﬁed that starting
from all zeros 0 · · · 0, the string with n ≤ l ones 0 · · · 01 · · · 1 will be reached after
exactly n jumps. Moreover, the orbit of 0 · · · 0 under P⊕≪ has the period length 2l.
In the limit of long strings l → ∞, this periodicity is broken because the needle never
(i. e. after ∞ many jumps) returns to all zeros 0 · · · 0, but – as we have shown in Eq.
11 using Eq. 10 – there still exists an asymptotic quasispecies.

How does our simple GA behave with a NiH that moves according to P⊕≪? In Fig. 6,
two typical runs of a genGA with a NiH like that are depicted. The setting (m, l, f0, τ )
was kept ﬁxed but two diﬀerent mutation rates µ were chosen.
In the case of Fig.
6 (right), the mutation rate is ‘too high’ to allow the population to track the movement.
The concentration of the future needle string (solid line) cannot grow much within one
jump cycle resulting in a decreasing initial condition (bullet) for the growth of the
needle concentration (dotted line) in the next cycle. The population looses the peak
– in this case after ≈ 90 generations. It might happen that the population ﬁnds the
needle again by chance (or better saying the moving needle jumps into the population),
but the population will not be able to stably track the movement. Contrasting to that,
the mutation rate was chosen to maximize the concentration of the future needle string
at the end of each jump cycle (bullets) in Fig. 6 (left). Since in that case, the best
achievable initial condition is given to each jump cycle, the movement of the needle

11

(5)
110

010

(6)
100

000
(1)

111
(4)

011
(3)

101

001
(2)

P⊕≪
(1) 000→001 (4) 111→110
(2) 001→011 (5) 110→100
(3) 011→111 (6) 100→000
101→010

010→101

Figure 5: The orbit of 0 · · · 0 under P⊕≪ (black dots) for string length l = 3. The numbers
(1), . . . , (6) show the order in which the strings are visited by the needle, starting
from 000.

0.025

0.02

0.015

0.01

0.005

n
o
i
t
a
r
t
n
e
c
n
o
c

xﬁx(∞)

0

0

20

40

80
number of generations

60

100

0

0

40

20
number of generations

60

80

100

Figure 6: Run of a genGA with regularly moving needle-in-the-haystack. The parameter
setting was m = 1000000, l = 20, f0 = 5, τ = 4, (left): µ = 0.022, (right):
µ = 0.055.
In both cases the system evolved for 100 generations (not shown)
without any occurring jumps in order to let a typical quasispecies grow around
the initial needle string. In generation 20 the ﬁrst jump happened and afterwards
every τ = 4 generations. solid line: x1(n, t), dotted line: x0(n, t), bullet: jump –
x0(n + 1, 0) = x1(n, τ ).

0.025

0.02

xﬁx(∞)

0.015

xﬁx(4)

0.01

0.005

12

ﬁxed point xﬁx = xﬁx(∞, l, f0, τ, µ)

) 0.02
0
,
1
+
n
(
0
x
=
)
τ
,

0.015

0.01

n
(
1
x

e
m
o
c
t
u
o

0.005

0

0

f0 = 10, l = 20, τ = 4, µ = µopt

0.005

0.01

0.015

0.02

0.025

0.03

initial condition x0(n, 0) = x1(n − 1, τ )

Figure 7: The ﬁxed point which is reached by an inﬁnite population for n → ∞.

is tracked with the highest possible stability for the given setting (m, l, f0, τ ). As can
be expected from Fig. 6 and is aﬃrmed by further experiments, the bullets keep on
ﬂuctuating around an average value for n → ∞ which is for the inﬁnite population
given by the quasispecies Eq. 11. In the following, we are going to model that system
with some idealizations and we will calculate a lower boundary for this average value.
We adopt the viewpoint of permuting the concentration vector compatible to the
movement of the needle as we have done implicitly in Fig. 6 and formally in the
deﬁnition of x′(t) in Eq. 10, but we drop the primes henceforth. The concentration
of the needle string within jump cycle n is denoted by x0(n, ϕ) and the concentration
of the string the needle will move to with the (n + 1)th jump (i. e. the future needle
string in jump cycle n) is denoted by x1(n, ϕ). The initial cycle prior to which no
jump has occurred is n = 0. Within a cycle, the time or generation is counted as
phase ϕ ∈ {0, . . . , τ }. Two succeeding cycles are connected by the (approximated)
rule of change

x0(n + 1, 0) = x1(n, τ )

and x1(n + 1, 0) ≈ 0.

(14)

The second relation is an approximation which is made to simplify the coming calcu-
lations, but it holds only if the needle jumps onto a string which has not been close to
one of the previous needle positions. Otherwise, the future needle string could already
be present with a concentration signiﬁcantly larger than 1/2l ≈ 0. In Fig. 6, we have
chosen the rule P⊕≪ to get experimental data for a case in which this assumption is
fulﬁlled. Later on we will see that we can still make useful comments about cases in
which that approximation is partly broken.

If we plot x0(n + 1, 0) = x1(n, τ ) against x0(n, 0), we get an intuitive picture for
the system’s evolution towards the quasispecies. The concentration x0(n, 0) converges

13

for n → ∞ towards a ﬁxed point,

xﬁx := lim
n→∞

x0(n, 0),

as shown in Fig. 7 for a ﬁnite value of xﬁx. Obviously, this ﬁxed point depends on the
full setting xﬁx = xﬁx(m, l, f0, τ, µ). Since we are especially interested in the eﬀects
of various cycle lengths τ and mutation rates µ, we keep (m, l, f0) ﬁxed, such that
xﬁx = xﬁx(τ, µ).

In the remaining of this section, we will calculate x0(n + 1, 0) = x1(n, τ ) in depen-
dence on x0(n, 0), which is the solid curve in Fig. 7, for arbitrary parameter settings.
From this knowledge, we will construct the phase diagram. Since we stay within one
jump cycle, we drop n to take oﬀ some notational load.

4.1 Derivation of the Fixed Point Concentrations

To calculate x1(τ ), it is suﬃcient to take only x0 and x1 into account, because the
assumed initial condition is x1(0) ≈ 0, such that the main growth of x1 is produced
by the mutational ﬂow from the needle. Moreover, we assume µ to be small enough
such that terms proportional to µ2 can be neglected. This means we restrict ourselves
to the case in which the system is mainly driven by one-bit mutations. Without
normalization, the evolution equations then read

y0(t + 1) = (1 − µ)l f0 y0(t) +
y1(t + 1) = µ(1 − µ)l−1f0 y0(t) + (1 − µ)l

µ(1 − µ)l−1 y1(t)
y1(t),
(cid:9)

(cid:8)

,

(15)

where yi denote unnormalized concentrations in contrast to the normalized concentra-
tions xi.

For f0(1 − µ) ≫ µ, which is always the case for large enough f0, we can further ne-
glect the back-ﬂow {· · · } from the future needle string compared to the self-replication
of the current needle string. The solution of Eq. 15 is then given by

(1 − µ)lf0

y0(t) =
y1(t) = κt(µ) y0(0) + (1 − µ)lty1(0),

y0(0),

(cid:2)

t

(cid:3)

with

κt(µ) = µ(1 − µ)lt−1αt
f t
0−1
t
ν=1 f ν
f0−1.

0 = f0

αt =

(

The coeﬃcient κt(µ) measures the growth of y1(t) starting from the initial condition
y1(0) ≈ 0, y0(0) 6= 0. As long as y0(t)+y1(t) ≪ 1, this gives already a good approxima-
tion for the concentrations x0(t) and x1(t). But in general, this approximation breaks
down for large t, because of the exponential growth of y0(t). We need to normalize
our solution, which can be done by

P

x(t) = y(t)

hf i0 · · · hf it−1, where hf it = (f0 − 1)x0(t) + 1.

(16)

By expressing the ﬁtness averages in terms of y0(t), we ﬁnd, after solving a simple

(cid:14)

14

0.02

0.015

0.01

0.005

x
ﬁ
x

n
o
i
t
a
r
t
n
e
c
n
o
c

exact numerical
O(µ2) numerical
O(µ2) analytic

(l = 20 for all curves)

= 20

f0 = 2

= 5

= 10

0

0

0.02

0.04

0.06
mutation rate µ

0.08

0.1

0.12

Figure 8: Comparison of the exact numerical and the O(µ2) calculation for diﬀerent values

of the needle ﬁtness f0.

recursion,

hf i0 · · · hf it−1 = 1 + (f0 − 1)

t−1
ν=0(1 − µ)lνf ν
0

x0(0)

= 1 + (f0 − 1)βt(µ)x0(0),

(cid:2)P

(cid:3)

where βt(µ) =

˜f t−1
˜f −1

and ˜f = (1 − µ)lf0.

Finally, we arrive at the normalized concentrations

x0(t) =

(1 − µ)lf0

t

x0(0)

[1 + (f0 − 1)βt(µ)x0(0)] ,

x1(t) =

(cid:2)
κt(µ) x0(0) + (1 − µ)ltx1(0)

(cid:3)

.

[1 + (f0 − 1)βt(µ)x0(0)] .

(cid:2)

(cid:3) .

The asymptotic state can now be calculated by using the initial condition x1(0) ≈
0, x0(0) 6= 0 and demanding x1(τ ) = x0(0). It is easily veriﬁed that for the ﬁxed point
follows

xﬁx(τ, µ) =

κτ (µ) − 1
(f0 − 1)βτ (µ)

.

(17)

4.2 Consistency in the Quasi-Static Limit

How can we test the quality of the approximate result Eq. 17? For large cycle lengths
τ , we enter the quasi-static regime, where we can approximate the population at the
end of each cycle by the quasispecies of the corresponding static landscape. Figure 8
shows a comparison of the exact numerical calculations of the quasispecies (τ → ∞)
and the O(µ2) calculations (τ = 100). In the numerical O(µ2) calculation, the back-
ﬂow from the ﬁrst error class to the needle string is included. Overall, we ﬁnd the error

15

l = 20
f0 = 10

x
ﬁ
x

n
o
i
t
a
r
t
n
e
c
n
o
c

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

0

0

τ = 3

= 4

= 5

→ ∞

τ = 2

0.02

0.04

0.06

0.08

0.1

0.12

mutation rate µ

Figure 9: Fixed point concentration xﬁx(τ, µ) for diﬀerent values of τ . For faster changes,

the ﬁxed point concentration rapidly drops down.

threshold and the maximum of the ﬁxed point concentration well represented. This
also suggests that the deviation of the O(µ2) approximation from the exact values
should be small for smaller τ , because those deviations add up for τ → ∞ by the
iterative procedure.

How do the calculated ﬁxed point concentrations compare to simulations with
(large) ﬁnite population? In Fig. 6, the values of xﬁx(∞, µ) and xﬁx(4, µ) are shown.
For τ → ∞, the deviation from the average hx1(n, ϕ)i (in generations 0 − 20) is in
fact the same as what can be read oﬀ in Fig. 8. The deviation of xﬁx(4, µ) from the
average value hx0(n, 0)i in generations 24, 28, . . . , 100 is signiﬁcantly larger. This is
caused by the neglect of all other strings’ contributions apart from the current needle
string’s contribution to the ﬂow onto the future needle string. These neglected contri-
butions increase the average ﬁxed point concentration measured in the experiment in
comparison to the calculated value xﬁx(τ, µ). But even though there are deviations,
we conclude that the approximately calculated value is always a lower bound for the
exact value. In the next section, we will use this observation to derive an expression
for the mutation rate that maximizes the average ﬁxed point concentration.

4.3 Phase Diagram

In Fig. 9, the ﬁxed point values xﬁx(τ, µ) are shown for small cycle lengths τ . For the
shown parameter setting, the region with xﬁx(2, µ) > 0 is extremely small. We notice
that there are two error thresholds, one for ‘too low’ mutation rates, µth<, and one
for ‘too high’ mutation rates, µth>. The intuition behind that was already given in
Section 3. For too low mutation rates the population becomes slow and evolves in the
averaged, ﬂat landscape, whereas for too high mutation rates the usual transition to
the disordered phase takes place. In the following we will calculate the phase diagram
starting from Eq. 17.

16

Error Thresholds: The error thresholds are given by

xﬁx(τ, µ) = 0 ⇐⇒ κτ (µ) = 1.

(18)

This is the same condition as one would get using only unnormalized concentrations
yi(t). Since yi(t) ≈ 0 near the error thresholds, the neglect of the normalization is not
critical for the calculation of the error thresholds themselves, whereas it is important
for the optimal mutation rate and of course for the ﬁxed point concentration. Since Eq.
18 cannot be solved for µ in closed form, we write down the corresponding recursion
relation that converges, for a suitable starting value of µ, to the solution of Eq. 18 in
the limit k → ∞,

µ(k)
th< = 1

ατ

.
µ(k)
th> = 1 −

th<

1 − µ(k−1)
(cid:16)
1

ατ µ(k−1)

th>

,

(cid:17)
1/(lτ −1)

(cid:16)

.

(cid:17)

µ(0)
th< = 0,

, µ(0)

−1/l
th> = 1 − f
0

=: µ∞
th.

For µth<, a good starting value is 0, since µth< ≈ 0 anyway. For µth>, the approximate
value for the error threshold of the static (i. e. τ → ∞) landscape µ∞
th can be chosen,
which is obtained by calculating the ﬁxed point [using Eq. 15 and 16],

x0(t + 1) = x0(t) ⇐⇒ x∞

ﬁx =

(1 − µ)lf0 − 1
f0 − 1

,

setting it to zero and solving for µ.

Optimal Mutation Rate:
In order to track changes with the best achievable sta-
bility for a given setting (m, l, f0, τ ), the lowest possible concentration (inﬁmum of)
x0(n, ϕ) needs to be maximized, because a low concentration might result in the loss
of the needle string in a ﬁnite population. Since for inﬁnite populations x0(n, ϕ) is
monotonously increasing with ϕ it is suﬃcient to maximize x0(n, 0). Moreover, we
derived above that x0(n, 0) approaches the ﬁxed point value xﬁx(τ, µ) for n → ∞. For
ﬁnite populations, we expect similar behavior but the strict monotony of x0(x, ϕ) in
ϕ will be destroyed by ﬂuctuations and also the ﬁxed point value itself will ﬂuctuate
around some average value hxﬁxi as can be seen in Fig. 6. However, the safest way to
avoid any loss of the needle string is still to maximize the average ﬁxed point value
hxﬁxi. In this sense, we deﬁne the optimal mutation rate µopt as the one that maximizes
hxﬁxi. In the previous Section 4.2, we noted that our approximated inﬁnite population
value xﬁx(τ, µ) represents a lower bound for hxﬁxi, where the maxima of the two curves
are expected to coincide for ﬁxed τ . Thus, µopt can be obtained by maximization of
xﬁx(τ, µ).

We can derive an expression for the optimal mutation rate µopt from

If we neglect the µ dependence of βτ (µ) in Eq. 17, which corresponds to the approach
τ →∞
in [19], we simply ﬁnd µNS
−−−→ 0, this result is inconsis-
tent with the quasi-static limit, because µopt should approach the value for which the

opt(τ, l) = 1/lτ . Because of µNS
opt

∂xﬁx
∂µ

(τ, µopt) = 0

17

l = 30

= 20

= 10

τ = 100

f0 = 2, 5, 10, 15, 20

x
ﬁ
˜x

n
o
i
t
a
r
t
n
e
c
n
o
c

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

0.02

0.04

0.06

0.08

0.1

0.12

mutation rate µ

Figure 10: The optimal mutation rate µ∞

opt(f0, l) from Eq. 19 in dependence on needle

height f0 and string length l.

concentration of 1-mutants in the quasispecies of the corresponding static NiH land-
scape is maximized. We conclude that the µ dependence of βτ (µ) cannot be neglected
for the correct optimal mutation rate, which we are now going to calculate.

For ατ ≫ 1, which is the case for τ ≫ 1 and f0 > 1, or τ ≈ 1 and f0 ≫ 1, we
can neglect the −1 in the numerator of xﬁx(τ, µ) and take only ατ into account for the
calculation of ∂xﬁx/∂µ. After some algebra, we ﬁnd

µopt =

( ˜f τ − 1)( ˜f − 1)
l( ˜f τ +1 − (τ + 1) ˜f + τ )

, where ˜f = f0(1 − µopt)l.

Since ˜f = ˜f(µopt), this equation cannot be solved in a closed form for µopt. However,
for τ → ∞ the equation simpliﬁes to

In the case ˜f > 1, we ﬁnd

µ∞
opt =

( ˜f − 1)/l ˜f
0

: ˜f > 1
: ˜f ≤ 1.

(

(1 − lµ∞

opt)(1 − µ∞

opt)l = 1

f0.

By approximating (1 − µ)l ≈ (1 − lµ)2, we get a cubic equation. The real root of that
equation is approximately [20] given by (see also Fig. 10)

(cid:14)

µ∞
opt(f0, l) ≈ µ+

1 +

(l − 1)µ+(1 − lµ+)

(cid:20)

3l(l − 1)µ2

+ − 2µ+(3l − 1) + 4

(cid:21)

with µ+ =

−1/2
1 + f
0

.

(19)

1
l

h

i

18

µ

e
t
a
r

n
o
i
t
a
t
u
m

0.12

0.1

0.08

0.06

0.04

0.02

0

1

2 ,

10

f0 =
µth
µopt
l = 20

µ∞
th

µ∞
opt

µ∞
th

µ∞
opt

10

cycle length τ

100

Figure 11: The calculated phase diagram for a genGA with stochastically moving needle-
in-the-haystack; two settings are shown: f0 = 2, 10, for both l = 20.

th< and µ(5)

th>. To show the convergence property of µ(k)

Resulting Phase Diagram: From the above, we are able to plot the phase diagram
for our model as shown in Fig. 11. Two settings are plotted. For f0 = 2 (resp. 10) the
diamonds (resp. circles) are the numerically obtained error thresholds. The solid and
dash-dotted lines are µ(5)
th<,>
are plotted for f0 = 10 as dashed lines. Obviously, the needed corrections to the chosen
starting values increase for smaller τ , such that more iterations are needed to describe
the error thresholds correctly for small τ . The expressions µ(5)
th<,> are already a good
approximation for the given settings. Representing the quasi-static limit, µ∞
th is plotted
as dotted line and gets consistently approached by µth>(τ ) for τ → ∞. Furthermore,
µ∞
opt is plotted as dash-dot-dotted line. The numerically measured values for µopt(τ )
are shown for f0 = 2 (resp. 10) as triangle (resp. squares). They approach µ∞
opt very
quickly already for τ ≈ 20 (resp. 10).

th<,>, µ(0)

We conclude that the above quantitative description is in good agreement with
the numerical observations and approaches the quasi-static region in a consistent way.
Moreover, the phase diagram ﬁts well into the general one raised in Section 3. Even
in the considered case of a genGA, we ﬁnd – depending on the parameter setting – a
time-averaged phase for very small τ . The time-averaged phase broadens for small f0.

4.4 Stochastically moving NiH

Up to now, we analyzed a regularly moving NiH, for example with the rule P⊕≪. What
happens if the NiH is allowed to move to a randomly picked nearest neighbor, as it is
shown in Fig. 12 for l = 4? Two typical runs of a genGA with this ﬁtness landscape are
depicted in Fig. 13. The setting (m, l, f0, τ ) was chosen the same as in Fig. 6 which
allows for a direct comparison of the GA’s behavior for regularly and stochastically
moving NiHs. The overall behavior is similar. For large mutation rates, the population
looses the needle string, whereas the moving needle is tracked stably for mutation rates
close to the above deﬁned optimal mutation rate. In addition, strong ﬂuctuations in

19

f

00

00

genotype

00

10

b its
11

h e r

h ig
01

01

11
lower bits

10

00

Figure 12: A stochastically moving needle-in-the-haystack for string length l = 4. The
needle is allowed to jump to one of its nearest neighbors which is chosen at
random.

n
o
i
t
a
r
t
n
e
c
n
o
c

0.025

0.02

0.015

0.01

0.005

0

0

xﬁx(∞)

20

40

60

80

100

20

40

60

80

100

number of generations

number of generations

Figure 13: Run of a genGA with stochastically moving needle-in-the-haystack. The param-

eter setting in (left) and (right) were the same as in Fig. 6 (left) and (right).

0.025

0.02

xﬁx(∞)

0.015

xﬁx(4)

0.01

0.005

0

0

20

the values of x1(n, 0) (lower ends of solid lines) as well as x0(n + 1, 0) = x1(n, τ )
(bullets) occur in the stochastic case. These result from back-jumps. If, at the end of
the current cycle, the needle jumps back to the string it has been to in the previous
cycle, then x1(n, 0) = x0(n − 1, τ ) is signiﬁcantly larger than zero. This can be seen in
Fig. 13 (right) at generations 36, 40 and 64 and also in Fig. 13 (left) at generations 72
and 88 (the gaps in Fig. 13 (left) correspond to x1, x0 being much larger than 0.025).
If no back-jumps occur, as in generations 24 − 72 in Fig. 13 (left), the system with
stochastic NiH behaves nearly indistinguishable from the one with regularly moving
NiH. Since back-jumps always increase the concentrations of the needle string in the
very next occurring jumps, the above calculated ﬁxed point xﬁx(τ, µ) is still a lower
bound. Thus, our previous notion of optimal mutation rate remains applicable to the
stochastically moving NiH although the assumption x1(n, 0) ≈ 0 from Eq. 14 is not
always fulﬁlled.

Nilsson and Snoad [19] did their analysis of the continuous Eigen model Eq. 3 with
stochastic NiH in a similar way as we did above. In analogy to their calculation for the
continuous Eigen model, we ﬁnd for a genGA the optimal mutation rate µNS
opt(τ, l) =
1/lτ which is inconsistent with the quasi-static limit (see Section 4.3). The reason is
the missing normalization in the work of Nilsson and Snoad. Furthermore, they could
not derive an expression for the ﬁxed point concentration xﬁx(τ, µ) because of that
same reason.

4.5 Jumps of larger Distance

l
k

th<,> and µ∞

To conclude this section about the behavior of genGAs with diﬀerent kinds of NiHs that
move to nearest neighbors, let us shortly discuss jumps of Hamming distance d larger
than one. Obviously, the analytical calculations get more complicated, because the
O(µ2)-approximation is not suﬃcient anymore as it connects only nearest neighbors.
To describe jumps of a larger distance, the concentrations of some intermediate se-
quences need to be taken into account, so that we have to solve a time evolution much
more complicated than Eq. 15. Hence, we cannot make simple statements for ﬁnite τ .
On the other hand, the system approaches the quasi-static region for large τ and it is
characterized by µ∞
opt as we have seen in Fig. 11. The exact quasispecies for
τ → ∞ is shown in Fig. 14. The plotted values are error class concentrations, in order
to make the higher error classes visible at all. Each k-mutant has a concentration of
in the quasispecies state, because for a NiH the mutant’s ﬁtness depends only
˜xk/
k-mutants have the same
on its Hamming distance to the needle and therefore all
concentration in the quasispecies. For ﬁnite populations, this is only true on average,
because the asymptotic state is distorted by ﬂuctuations. But in the following, we
assume that the quasispecies is still representative for the average distribution of the
population in the asymptotic state. Then, the optimal mutation rate in the sense of
Section 4.3 for jumps of distance d is by deﬁnition the position of the maximum of ˜xd.
For d ≥ l/2, optimal mutation rate and error threshold become identical. Although ˜xd
is maximized for mutation rates close to the error threshold it amounts, as do all other
concentrations to only ≈ 1/2l, which leads to an approximately random drift for ﬁnite
populations. On the other hand, the chance of tracking the needle decreases even fur-
ther for small mutation rates because then the concentration ˜xd becomes even smaller.
In this sense, the quasispecies distribution, which is centered on the needle string, is

l
k

(cid:0)

(cid:1)

(cid:1)

(cid:0)

21

µ∞
opt,1

µ∞
opt,2

µ∞
opt,3

µ∞
opt,4

f0 = 10
l = 20

i

˜x

s
n
o
i
t
a
r
t
n
e
c
n
o
c

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.02

0.04

0.06

0.08

0.1

0.12

mutation rate µ

Figure 14: The quasispecies for the static NiH in dependence on the mutation rate µ. The
concentrations ˜xi of the ith error class for i ∈ {0, . . . , ⌊l/2⌋} are depicted. The
optimal mutation rates for jumps of Hamming distance d = 1, 2, 3, 4 are shown
as dotted lines.

useless for tracking the next jump if d ≥ l/2. This also suggests – in agreement with
the experimental ﬁndings of Rowe [13] (in this book) – that ﬁnite populations are for
low mutation rates unable to track large jumps – in particular in the extreme case
d = l. Only for jumps of d < l/2 the corresponding error class concentration ˜xd shows
a concentration maximum signiﬁcantly above 1/2l. From the heights of the concen-
tration maxima, we see that the diﬃculty of tracking the changes increases with the
Hamming distance d of the jumps. Vice versa, the advantage a population gets after
a jump from its structure prior to the jump decreases with increasing jump distance
d. In addition, a mutation rate which is simultaneously optimal for more than one
distance cannot be found.

5 Conclusions and Future Work

On the basis of general arguments, the phase diagrams of population-based mutation
and probabilistic selection systems like the above genGA, ssGA and Eigen model in
time-dependent ﬁtness landscape can be easily understood. The notion of regular
changes allows for an exact calculation of the asymptotic state in the sense of a gen-
eralized, time-dependent quasispecies. For a genGA with NiH that moves regularly to
nearest neighbors, the quasispecies can be straightforwardly calculated under simpli-
fying assumptions. The result is a lower bound for the exact quasispecies. With that
lower bound, we have constructed the phase diagram in the inﬁnite population limit.
This phase diagram is in agreement with the one raised from general arguments.

In order to improve our analysis, we need to weaken our assumptions. In particu-
lar, we have to overcome the restriction of taking into account only the ﬂow from the
current towards the future needle string. The presence of other contributions to the

22

ﬂow has to be modeled in some way. Another future step could be an investigation
of the ﬂuctuations that are introduced by the ﬁniteness of realistic populations (dis-
creteness of Λm) around the quasispecies. This would lead to a lower boundary for the
population size above which the needle string is not lost due to those ﬂuctuations.

An extension of our analysis to non-regularities like the occurrence of more than
a single jump rule, can be achieved by averaging the time evolution Eq. 8 for n → ∞
according to each rule’s probability of being applied. A similar averaging procedure
will be necessary if ﬂuctuations of the cycle length τ are present. Finally, an extension
of the description to broader, more realistic peaks, as well as GA models including
crossover and other selection schemes, are important topics for future work.

References

[1] T. B¨ack, U. Hammel and H.-P. Schwefel. Evolutionary Computation: Comments
on the History and Current State. IEEE Transactions on Evol. Comp. 1(1), p. 3,
1997.

[2] T. B¨ack, D. B. Fogel and Z. Michalewicz, editors. Handbook of Evolutionary

Computation. IOP Publishing, Bristol, 1997.

[3] J. Branke. Evolutionary Algorithms for Dynamic Optimization Problems, A

Survey. Technical Report 387, AIFB University Karlsruhe, 1999.

[4] J. E. Rowe. Finding attractors for periodic ﬁtness functions. In W. Banzhaf
et al., editors, Proceedings to GECCO 1999, Morgan Kaufmann, San Mateo,
p. 557, 1999.

[5] L. M. Schmitt, C. L. Nehaniv and R. H. Fujii. Linear analysis of genetic algo-

rithms. Theoretical Computer Science 200, p. 101, 1998.

[6] M. Eigen. Selforganization of matter and the evolution of biological macro-

molecules. Naturwissenschaften 58, p. 465, 1971.

[7] M. Eigen and P. Schuster. The Hypercycle – A Principle of Natural Self-

Organization. Springer-Verlag, Berlin, 1979.

[8] M. Eigen, J. McCaskill and P. Schuster. The molecular quasispecies. Adv. Chem.

Phys. 75, p. 149, 1989.

[9] E. Baake and W. Gabriel. Biological evolution through mutation, selection, and

drift: An introductory review. Ann. Rev. Comp. Phys. 7, in press, 1999.

[10] J. E. Rowe. The dynamical systems model of the simple Genetic Algorithm. this

issue, p. XXX, 1999.

Press, Cambridge, 1999.

[11] M. D. Vose. The simple Genetic Algorithm – Foundations and Theory. MIT

[12] E. van Nimwegen, J. P. Crutchﬁeld and M. Mitchell. Statistical Dynamics of the
Royal-Road genetic algorithms. Theoretical Computer Science, special issue on
Evolutionary Computation, A. Eiben, G. Rudolph, editors, in press, 1998.

23

[13] J. E. Rowe. Cyclic Attractors and Quasispecies Adaptability. this issue, p. XXX,

1999.

[14] K. DeJong and J. Sarma. Generation Gaps Revisited. In L. D. Whitley, editor,
Foundations of Genetic Algorithms 2, Morgan Kaufmann, San Mateo, p. 19,
1993.

[15] A. Rogers and A. Pr¨ugel-Bennett. Modeling the Dynamics of a Steady State Ge-
netic Algorithm. In W. Banzhaf and C. Reeves, editors, Foundations of Genetic
Algorithms 5, Morgan Kaufmann, San Mateo, p. 57, 1998.

[16] J. Branke, M. Cutaia and H. Dold. Reducing Genetic Drift in Steady State
Evolutionary Algorithms. In W. Banzhaf et al., editors, Proceedings to GECCO
1999, Morgan Kaufmann, San Mateo, p. 68, 1999.

[17] C. O. Wilke, C. Ronnewinkel and T. Martinetz. Molecular Evolution in time-
dependent Environments. In D. Floreano, J.-D. Nicoud and F. Mondada, editors,
Proceedings to European Conference on Artiﬁcial Life 1999 , Springer, Berlin,
p. 417, 1999.

[18] C. O. Wilke and C. Ronnewinkel. Dynamic Fitness landscapes in the Quasispe-

cies model. in preparation.

[19] M. Nilsson and N. Snoad. Error Thresholds on dynamic Fitness-Landscapes.

Working Paper 99-04-030, Santa Fe Institute, 1999.

[20] A more detailed explanation and analysis of the used approximation will be

presented elsewhere.

24


9
9
9
1
 
b
e
F
 
3
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
7
6
0
2
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Symbols and synergy in a neural code

Naama Brenner,1 Steven P. Strong,1,2 Roland Koberle,1,3 William Bialek,1
and Rob R. de Ruyter van Steveninck1

1NEC Research Institute, 4 Independence Way, Princeton, New Jersey 08540
2Institute for Advanced Study, Olden Lane, Princeton, New Jersey 08544
3Instituto di F´isica de S˜ao Carlos, Caixa Postal 369
Universidade de S˜ao Paulo, 13560–970 S˜ao Carlos, SP Brasil

Understanding a neural code requires knowledge both of the elementary
symbols that transmit information and of the algorithm for translating these
symbols into sensory signals or motor actions. We show that these questions
can be separated: the information carried by any candidate symbol in the
code—a pattern of spikes across time or across a population of cells—can
be measured, independent of assumptions about what these patterns might
represent. By comparing the information carried by a compound pattern
with the information carried independently by its parts, we measure directly
the synergy among these parts. We illustrate the use of these methods by
applying them to experiments on the motion sensitive neuron H1 of the ﬂy’s
visual system, where we conﬁrm that two spikes close together in time carry
far more than twice the information carried by a single spike. We analyze
the sources of this synergy, and provide evidence that pairs of spikes close
together in time may be special symbols in the code of H1.

1

1 Introduction

Throughout the nervous system, information is encoded in sequences of iden-
tical action potentials or spikes. The representation of sense data by these
spike trains has been studied for seventy years [1], but there remain many
open questions about the structure of this code. A full understanding of the
code requires that we identify its elementary symbols and that we charac-
terize the messages which these symbols represent. Many diﬀerent possible
elementary symbols have been considered, implicitly or explicitly, in previ-
ous work on neural coding. These might be the numbers of spikes in time
windows of ﬁxed size, or alternatively the individual spikes themselves might
be the building blocks of the code.
In cells that produce bursts of action
potentials, these bursts might be special symbols that convey information
in addition to that carried by single spikes. Yet another possibility is that
patterns of spikes—across time in one cell or across a population of cells—
can have a special signiﬁcance; this last possibility has received renewed
attention as techniques emerge for recording the activity of many neurons
simultaneously.

In many methods of analysis, questions about the symbolic structure of
the code are mixed with questions about what the symbols represent. Thus,
in trying to characterize the feature selectivity of neurons, one often makes
the a priori choice to measure the neural response as the spike count or
rate in a ﬁxed window. Conversely, in trying to assess the signiﬁcance of
synchronous spikes from pairs of neurons, or bursts of spikes from a single
neuron, one might search for a correlation between these events and some
particular stimulus features. In each case conclusions about one aspect of the
code are limited by assumptions about another aspect. Here we show that
questions about the symbolic structure of the neural code can be separated
out and answered in an information theoretic framework, using data from
suitably designed experiments. This framework allows us to address directly
the signiﬁcance of spike patterns or other compound spiking events: How
much information is carried by a compound event? Is there redundancy
or synergy among the individual spikes? Are particular patterns of spikes
especially informative?

2

Methods to assess the signiﬁcance of spike patterns in the neural code

share a common intuitive basis:

•

•

Patterns of spikes can play a role in representing stimuli if and only if
the occurrence of patterns is linked to stimulus variations.

The patterns have a special role only if this correlation between sensory
signals and patterns is not decomposable into separate correlations be-
tween the signals and the pieces of the pattern, e.g.
the individual
spikes.

We believe that these statements are not controversial. Diﬃculties arise
when we try to quantify this intuitive picture: What is the correct measure
of correlation? How much correlation is signiﬁcant? Can we make statements
independent of models and elaborate null hypotheses?

The central claim of this paper is that many of these diﬃculties can
be resolved by careful use of ideas from information theory. Shannon [2]
proved that entropy and information provide the only measures of variability
and correlation that are consistent with simple and plausible requirements.
Further, while it may be unclear how to interpret, for example, a 20% increase
in correlation between spike trains, an extra bit of information carried by
patterns of spikes means precisely that these patterns provide a factor of two
increase in the ability of the system to distinguish among diﬀerent sensory
inputs. In this work we show see that there is a direct method of measuring
the information (in bits) carried by particular patterns of spikes, independent
of models for the stimulus features that these patterns might represent. In
particular, we can compare the information conveyed by spikes patterns with
the information conveyed by the individual spikes that make up the pattern,
and determine quantitatively whether the whole is more or less than the sum
of its parts.

While this method allows us to compute unambiguously how much in-
formation is conveyed by particular patterns, it does not tell us what this
information is. Making the distinction between two questions, the structure
of the code and the algorithm for translation, we only answer the ﬁrst of
these two. We believe that ﬁnding the structural properties of a neural code

3

independent of the translation algorithm is an essential ﬁrst step towards un-
derstanding anything beyond the single spike approximation. The need for
identifying the elementary symbols is especially clear when complex multi
neuron codes are considered. Moreover, a quantitative measure of the in-
formation carried by compound symbols will be useful in the next stage of
modeling the encoding algorithm, as a control for the validity of models.

2 Formalism

In the framework of information theory [2] signals are generated by a source
with a ﬁxed probability distribution, and encoded into messages by a channel.
The coding is probabilistic, and the joint distribution of signals and coded
messages determines all quantities of interest; in particular the information
transmitted by the channel about the source is an average over this joint
distribution.
In studying a sensory system, the signals generated by the
source are the stimuli presented to the animal, and the messages in the
communication channel are sequences of spikes in a neuron or in a population
of neurons. Both the stimuli and the spike trains are random variables, and
they convey information mutually because they are correlated. The problem
of quantifying this information has been discussed from several points of view
[3, 4, 5, 6]. Here we address the question of how much information is carried
by particular “events” or combinations of action potentials.

2.1 Deﬁning information from one event

A discrete event E in the spike train is deﬁned as a speciﬁc combination of
spikes. Examples are a single spike, a pair of spikes separated by a given time,
spikes from two neurons that occur in synchrony, and so on.
Information
is carried by the occurrence of events at particular times and not others,
implying that they are correlated with some stimulus features and not with
others. Our task is to express this information in terms of quantities that
are easily measured experimentally.

In experiments, as in nature, the animal is exposed to stimuli at each
instant of time. We can describe this sensory input by a function s(t′),

4

(1)

(2)

which may have many components to parameterize the time dependence of
multiple stimulus features. In general, the information gained about s(t′) by
observing a set of neural responses is

I =

Ds(t′)P [s(t′) & response] log2

responses Z
X

P [s(t′) & response]
P [s(t′)]P [response] !

,

 

where information is measured in bits. It is useful to note that this mutual
information can be rewritten in two complementary forms:

P [response]

Ds(t′)P [s(t′)

response] log2 P [s(t′)

response]

|

|

I =

Ds(t′)P [s(t′)] log2 P [s(t′)]

− Z
+

= S[P (s)]

S[P (s

responses
X

− h

Z
response)]
i

|

response,

or

I =

−

responses
X

P (response) log2 P (response)

+

Ds(t′)P [s(t′)]

P [response

s(t′)] log2 P [response

s(t′)]

Z
= S[P (response)]

responses
X

S[P (response
|

s)]

is,

− h

|

|

(3)

where S denotes the entropy of a distribution; by
h· · ·is we mean an average
over all possible values of the sensory stimulus, weighted by their probabil-
ity of occurrence, and similarly for
responses. In the ﬁrst form, Eq. (2),
h· · ·i
we focus on what the responses are telling us about the sensory stimulus
[7]: diﬀerent responses point more or less reliably to diﬀerent signals, and
our average uncertainty about the sensory signal is reduced by observing the
neural response.
In the second form, Eq. (3), we focus on the variability
and reproducibility of the neural response [8, 6]. The range of possible re-
sponses provides the system with a capacity to transmit information, and the
variability which remains when the sensory stimulus is speciﬁed constitutes
noise; the diﬀerence between the capacity and the noise is the information.
We would like to apply this second form to the case where the neural
response is a particular type of event. When we observe an event E, in-
formation is carried by the fact that it occurs at some particular time tE.

5

The range of possible responses is then the range of times 0 < tE < T in
our observation window. Alternatively, when we observe the response in a
particular small time bin of size ∆t, information is carried by the fact that
the event E either occurs or does not. The range of possible responses then
includes just two possibilities. Both of these points of view have an arbitrary
element: the choice of bin size ∆t and the window size T . Characterizing
the properties of the system, as opposed to our observation power, requires
taking the limit of high time resolution (∆t
0) and long observation times
(T
). As will be shown in the next section, in this limit the two points
of view give the same answer for the information carried by an event.

→ ∞

→

2.2

Information and event rates

A crucial role is played by the event rate rE(t), the probability per unit
time that an event of type E occurs at time t, given the stimulus history
s(t′). Empirical construction of the event rate rE(t) requires repetition of
same stimulus history many times, so that a histogram can be formed (see
Figure 1). For the case where events are single spikes, this is the familiar
time dependent ﬁring rate or post-stimulus time histogram (Figure 1c); the
generalization to other types of events is illustrated by Figs. 1c and 1d.
Intuitively, a uniform event rate implies that no information is transmitted,
whereas the presence of sharply deﬁned features in the event rate implies
that much information is transmitted by these events; see, for example, the
discussion by Vaadia et al.
[9]. We now formalize this intuition, and show
how the average information carried by a single event is related quantitatively
to the time dependent event rate.

Let us take the ﬁrst point of view about the neural response variable,
in which the range of responses is described by the possible arrival times t
of the event. What is the probability of ﬁnding the event at a particular
time t? Before we know the stimulus history s(t′), all we can say is that the
event can occur anywhere in our experimental window of size T , so that the
probability is uniform P (response) = P (t) = 1/T , with an entropy of S[P (t)] =
log2 T . Once we know the stimulus, we also know the event rate rE(t),
and so our uncertainty about the occurrence time of the event is reduced.

6

Events will occur preferentially at times where the event rate is large, so
the probability distribution should be proportional to rE(t); with proper
normalization P (response
s) = rE(t)/(T ¯rE). Then the conditional
entropy is

s) = P (t
|

|

S[P (t
|

s)] =

T

− Z
0
1
T Z

−

=

s)

dt P (t
|
rE(t)
¯rE

s) log2 P (t
|
rE(t)
¯rET !

log2

dt

 

T

0

.

(4)

(5)

(6)

In principle one should average this quantity over diﬀerent stimuli s(t′), how-
ever if the time T is long enough and the stimulus history suﬃciently rich, it
is self-averaging. The reduction in entropy is then the gain in information,
so

I(E; s) = S[P (t)]

−

s)]

S[P (t
|
rE(t)

=

T

1
T Z

0

dt

log2

rE(t)

.

 

¯rE !

 

¯rE !

This formula expresses the information conveyed by an event of type E as an
integral over time, of a quantity which depends only on the responses. There
is no explicit dependence on the joint distribution of stimuli and responses;
it is implicit that by integrating over time we are in fact sampling the distri-
bution of stimuli, whereas by estimating the function rE(t) as a histogram
we are sampling the distribution of the responses given a stimulus. We may
write the information equivalently as an average over the stimulus instead of
over time,

I(E; s) =

rE(t)

*  

¯rE !

log2

rE(t)
¯rE ! +s

,

 

where here the average is over all possible value of s weighted by their prob-
abilities P (s).

0, 1

In the second view, the neural response is a binary random variable,
σE ∈ {
, marking the occurrence or non occurrence of an event of type
E in a small time bin of size ∆t. Suppose, for simplicity, that the stimulus
takes on a ﬁnite set of values s with probabilities P (s). These in turn induce

}

7

s) = rE(s)∆t, with an average
the event E with probability pE(s) = P (σE=1
probability for the occurrence of the event ¯pE =
s P (s) rE(s)∆t = ¯rE∆t.
The information is the diﬀerence between the prior entropy and the condi-
σE)
tional entropy: I(E; s) = S(s)
, where the conditional entropy is
|
an average over the two possible values of σE. The conditional probabilities
are found from Bayes’ rule,

S(s

−h

P

i

|

P(s

σE = 1) =

P(s

σE = 0) =

|

|

P (s)pE(s)
¯pE
P (s)(1
(1

−
−

pE(s))
¯pE)

,

(7)

and with these one ﬁnds the information,

XσE =0,1
pE(s)

−

s
X

s
X

I(E; s) =

P (s) log2 P (s) +

P (s

σE) logs P (s

σE)

|

|

=

P (s)

pE(s) log2
"

 

¯pE !

−

+ (1

pE(s)) log2

1

pE(s)

 

−
1

−

.(8)

¯pE !#

This expression is again an average over all stimulus values, of a property
which only depends on the responses. Taking the limit ∆t
0, consistent
with the requirement that the event can occur at most once, one ﬁnds the
average information conveyed in a small time bin; dividing by the average
probability of an event one obtains Eq. (6) as the information per event.

→

Equation (6), and its time averaged form (5), is an exact formula which
can be used in any situation where a rich stimulus history can be presented
repeatedly. It enables the evaluation of the information for arbitrarily com-
plex events, independent of assumptions about the encoding algorithm.

Let us consider in more detail the simple case where events are single
spikes. Then the average information conveyed by a single spike becomes an
integral over the time dependent spike rate r(t),

I(1 spike; s) =

T

1
T Z

0

dt

r(t)

log2

r(t)

.

 

¯r !

 

¯r !

(9)

It makes sense that the information carried by single spikes should be related
to the spike rate, since this rate as a function of time gives a complete de-
scription of the ‘one body’ statistics of the spike train, in the same way that

8

the single particle density describes the one body statistics of a gas or liquid.
Several previous works have noted this relation, and the formula (9) has an
interesting history. If the spike train is a modulated Poisson process, then
Eq. (9) provides an upper bound on information transmission (per spike) by
the spike train as a whole [10]. In studying the coding of location by cells
in the rat hippocampus, Skaggs et al.
[11] assumed that successive spikes
carried independent information, and that the spike rate was determined by
the instantaneous location, and obtained Eq. (9) with the time average re-
placed by an average over locations. DeWeese [12] showed that the rate of
information transmission by a spike train could be expanded in a series of
integrals over correlation functions, where successive terms would be small
if the number of spikes per correlation time were small; the leading term,
which would be exact if spikes were uncorrelated, is Eq. (9). Panzeri et al.
[13] show that Eq. (9), multiplied by the mean spike rate to give an informa-
tion rate (bits/s), is the correct information rate if we count spikes in very
brief segments of the neural response, which is equivalent to asking for the
information carried by single spikes. For further discussion of the relation to
previous work, see Appendix A.

A crucial point here is the generalization to Eq. (5), and this result applies
to the information content of any point events in the neural response—pairs
of spikes with a certain separation, coincident spikes from two cells, ... —not
just single spikes. Moreover, in the analysis of experiments we will emphasize
the use of this formula as an exact result for the information content of single
events, rather than an approximate result for the spike train as a whole, and
this approach will enable us to address questions concerning the structure of
the code and the role played by various point events.

3 Experiments in the ﬂy visual system

In this section, we use our formalism to analyze experiments on the movement
sensitive cell H1 in the visual system of the blowﬂy Calliphora vicina. We
address the issue of the information conveyed by pairs of spikes in this neuron,
as compared to the information conveyed independently by single spikes. The
quantitative results of this section —numbers for the information, eﬀects of

9

synergy and redundancy among spikes— are speciﬁc to this system and to
the stimulus conditions used. The theoretical approach, however, is valid
generally and can be applied similarly to other experimental systems, to ﬁnd
out the signiﬁcance of various patterns in single cells or in a population.

3.1 Synergy between spikes

The experimental setup described in Appendix B gives us control over the
input and output of the H1 neuron in the ﬂy. The horizontal motion across
the visual ﬁeld is the input sensory stimulus s(t), which we draw from a
probability distribution P (s), and the spike train recorded from H1 is the
neural response. Figure 1a shows a segment of the stimulus presented to the
ﬂy, and 1b illustrates the response to many repeated presentations of this
segment. The histogram of spike times across the ensemble of repetitions
provides an estimate of the spike rate r(t) (Fig. 1c), and Eq. (5) gives the
information carried by a single spike, I(1 spike; s) = 1.53
0.05 bits. Figure
2 illustrates the details of how the formula was used, with an emphasis on
the eﬀects of ﬁniteness of the data. In this experiment, a stimulus of length
T = 10 sec was repeated 350 times. As seen from Figure 2, a stable result
could be obtained from a smaller number of repetitions.

±

If each spike were to convey information independently, then with the
mean spike rate ¯r = 37 spikes/s, the total information rate would be Rinfo =
56 bits/s. We used the variability and reproducibility of continuous segments
in the neural response [6], in order to estimate the total information rate in
the spike train in this experiment, and found that Rinfo = 75 bits/s. Thus,
the information conveyed by the spike train as a whole is larger than the
sum of contributions from individual spikes, indicating cooperative informa-
tion transmission by patterns of spikes in time. This synergy among spikes
motivates the search for especially informative patterns in the spike train.

We consider compound events that consist of two spikes separated by
a time τ , with no constraints on what happens between them. Figure 1
shows segments of the event rate rτ (t) for τ = 3(
1) ms (Fig. 1d), and for
τ = 17(
1) ms (Fig. 1e). The information carried by spike pairs as a function
of the interspike time τ , computed from Eq. (5), is shown in Fig. 3. For large

±

±

10

∼

τ spikes contribute independent information, as expected. This independence
40 ms, comparable to the behavioral response
is established within
30
times of the ﬂy [14]. There is a mild redundancy (
20%) at intermediate
10
separations, and a very large synergy (up to

130%) at small τ .

∼

−

−

∼

Related results were obtained using the correlation of spike patterns with
stimulus features [7]. There the information carried by spike patterns was es-
timated from the distribution of stimuli given each pattern, thus constructing
a statistical model of what the patterns “stand for” (see details in Appendix
A). Since the time dependent stimulus is in general of high dimensionality, its
distribution cannot be sampled directly and some approximations must be
made. de Ruyter van Steveninck and Bialek [7] made the approximation that
patterns of a few spikes encode projections of the stimulus onto low dimen-
sional subspaces, and the information carried by such patterns was evaluated
only in this subspace. The informations obtained in this approximation are
bounded from above by the true information carried by the patterns, as
estimated directly with the methods presented here.

3.2 Origins of synergy

Synergy means, quite literally, that two spikes together tell us more than
two spikes separately. Synergistic coding is often discussed for populations
of cells, where extra information is conveyed by patterns of coincident spikes
from several neurons [15, 16, 17, 18], while here we see direct evidence for
extra information in pairs of spikes across time. The mathematical framework
for describing these eﬀects is the same, and a natural question is: what are
the conditions for synergistic coding?

The average synergy Syn[E1, E2; s] between two events E1 and E2 is the
diﬀerence between the information about the stimulus s conveyed by the
pair, and the information conveyed by the two events independently,

Syn[E1, E2; s] = I[E1, E2; s]

(I[E1; s] + I[E2; s]).

(10)

We can rewrite the synergy as:

Syn[E1, E2; s] = I[E1; E2

s]

I[E1; E2].

(11)

−

|

−

11

The ﬁrst term is the mutual information between the events computed across
an ensemble of repeated presentations of the same stimulus history. It de-
scribes the gain in information due to the locking of compound event (E1, E2)
to particular stimulus features. If events E1 and E2 are correlated individ-
ually with the stimulus but not with one another, this term will be zero,
and these events cannot be synergistic on average. The second term is the
mutual information between events when the stimulus is not constrained, or
equivalently the predictability of event E2 from E1. This predictability lim-
its the capacity of E2 to carry information beyond that already conveyed by
E1. Synergistic coding (Syn > 0) thus requires that the mutual information
among the spikes is increased by specifying the stimulus, which makes precise
the intuitive idea of ‘stimulus dependent correlations’.

Returning to our experimental example, we identify the events E1 and
E2 as the arrivals of two spikes, and consider the synergy between them as
a function of the time τ between them. In terms of event rates, we compute
the information carried by a pair of spikes separated by a time τ , Eq. (5),
as well as the information carried by two individual spikes. The diﬀerence
between these two quantities is the synergy between two spikes, which can
be written as

Syn(τ ) =

log2

−

(cid:18)

(cid:19)

¯rτ
¯r2
T

log2

T

0

+

1
T Z
rτ (t)
¯rτ

+

dt

rτ (t)
¯rτ
rτ (t + τ )
¯rτ

dt

"

+

1
T Z

0

rτ (t)

"

r(t)r(t
r(t)

τ ) #

−
log2[r(t)].

2

−

¯r #

(12)

The ﬁrst term in this equation is the logarithm of the normalized correlation
function, and hence measures the rarity of spike pairs with separation τ ;
the average of this term over τ is the mutual information between events
in Eq. (11). The second term is related to the local correlation function
and measures the extent to which the stimulus modulates the likelihood of
spike pairs. The average of this term over τ gives the mutual information
conditional on knowledge of the stimulus [the ﬁrst term in Eq. (11)]. The
average of the third term over τ is zero, and numerical evaluation of this
term from the data shows that it is negligible at most values of τ .

We thus ﬁnd that the synergy between spikes is approximately a sum of
two terms, whose averages over τ are the terms in Eq. (11). A spike pair with

12

a separation τ then has two types of contributions to the extra information
it carries: the two spikes can be correlated conditional on the stimulus, or
the pair could be a rare and thus surprising event. The rarity of brief pairs is
related to neural refractoriness, but this eﬀect alone is insuﬃcient to enhance
information transmission; the rare events must also be related reliably to the
stimulus. In fact, conditional on the stimulus, the spikes in rare pairs are
from
strongly correlated with each other, and this is visible in Fig. 1a:
trial to trial, adjacent spikes jitter together as if connected by a stiﬀ spring.
To quantify this eﬀect, we ﬁnd for each spike in one trial the closest spike
in successive trials, and measure the variance of the arrival times of these
spikes. Similarly, we measure the variance of the interspike times. Figure 4a
shows the ratio of the interspike time variance to the sum of the arrival time
variances of the spikes that make up the pair. For large separations this ratio
is unity, as expected if spikes are locked independently to the stimulus, but
as the two spikes come closer it falls below one quarter.

Both the conditional correlation among the members of the pair (Fig.
4a) and the relative synergy (Fig. 4b) depend strongly on the interspike
separation. This dependence is nearly invariant to changes in image contrast,
although the spike rate and other statistical properties are strongly aﬀected
by such changes. Brief spike pairs seem to retain their identity as specially
informative symbols over a range of input ensembles. If particular temporal
patterns are especially informative, then we would lose information if we
failed to distinguish among diﬀerent patterns. Thus there are two notions of
time resolution for spike pairs: the time resolution with which the interspike
time is deﬁned, and the absolute time resolution with which the event is
marked. Figure 5 shows that, for small interspike times, the information
is much more sensitive to changes in the interspike time resolution (open
symbols) than to the absolute time resolution (ﬁlled symbols). This is related
to the slope in Figure 2: in regions where the slope is large, events should be
ﬁnely distinguished in order to retain the information.

13

3.3

Implications of synergy

The importance of spike timing in the neural code has been under debate for
some time now. We believe that some issues in this debate can be clariﬁed
using a direct information theoretic approach. Following MacKay and Mc-
Culloch [3], we know that marking spike arrival times with higher resolution
provides an increased capacity for information transmission. The work of
Strong et al.
[6] shows that for the ﬂy’s H1 neuron, the increased capacity
associated with spike timing is indeed used with nearly constant eﬃciency
down to millisecond resolution. This eﬃciency can be the result of a tight
locking of individual spikes to a rapidly varying stimulus, and it could also
be the result of temporal patterns providing information beyond rapid rate
modulations. The analysis given here shows that for H1, pairs of spikes
can provide much more information than two individual spikes, information
transmission is much more sensitive to the relative timing of spikes than to
their absolute timing, and these synergistic eﬀects survive averaging over all
similar patterns in an experiment. On the time scales of relevance to ﬂy
behavior, the amount of synergy among spikes in H1 allows this single cell to
provide an extra factor of two in resolving power for distinguishing diﬀerent
trajectories of motion across the visual ﬁeld.

4 Summary

In summary, information theory allows us to quantify the symbolic structure
of a neural code independent of the rules for translating between spikes and
stimuli. In particular, this approach tests directly the idea that patterns of
spikes are special events in the code, carrying more information than expected
by adding the contributions from individual spikes. These quantities can be
measured directly from data. It is of practical importance that the formulas
rely on low order statistical measures of the neural response, and hence do not
require enormous data sets to reach meaningful conclusions. The method is
of general validity and is applicable to patterns of spikes across a population
of neurons, as well as across time.

In our experiments on the ﬂy visual system, we found that an event com-

14

posed of a pair of spikes can carry far more than the information carried
independently by its parts. Two spikes that occur in rapid succession appear
to be special symbols that have an integrity beyond the locking of individual
spikes to the stimulus. This is analogous to the encoding of sounds in writ-
ten English: the symbols ‘th,’ ‘sh,’ and ‘ch’ are each elementary and stand
for sounds that are not decomposable into sounds represented by each of
the constituent letters. For such pairs to act eﬀectively as special symbols,
mechanisms for ‘reading’ them must exist at subsequent levels of processing.
Synaptic transmission is sensitive to interspike times in the 2 – 20 ms range
[19], and it is natural to suggest that synaptic mechanisms on this time scale
play a role in such reading. Recent work on the mammalian visual system
[20] provides direct evidence that pairs of spikes close together in time can
be especially eﬃcient in driving postsynaptic neurons.

Acknowledgements

We thank G. Lewen and A. Schweitzer for their help with the experiments
and N. Tishby for many helpful discussions. Work at the IAS was supported
in part by DOE grant DE–FG02–90ER40542, and work at the IFSC was
supported by the Brazilian agencies FAPESP and CNPq.

Appendix A: Relation to previous work

Patterns of spikes and their relation to sensory stimuli have been quantiﬁed
in the past through the use of correlation functions. The event rates that we
have deﬁned here, which are directly connected to the information carried by
patterns of spikes by Eq. (5), are in fact just properly normalized correla-
tion functions. The event rate for pairs of spikes from two separate neurons
is related to the joint post-stimulus time histogram deﬁned by Aertsen and
coworkers [21, 9] Making this connection explicit is also an opportunity to
see how the present formalism applies to events deﬁned across two cells.

Consider two cells, A and B, generating spikes at times

tB
,
i }
It will be useful to think of the spike trains as sums of unit

tA
i }

respectively.

and

{

{

15

impulses at the spike times,

Then the time dependent spike rates for the two cells are

ρA(t) =

ρB(t) =

δ(t

δ(t

−

−

tA
i )

tB
i ).

i
X

i
X

rA(t) =
rB(t) =

h

ρA(t)
i
ρB(t)

trials,

trials,

(A.1)

(A.2)

(A.3)

(A.4)

h

h· · ·i

i
where
trials denotes an average over multiple trials in which the same time
dependent stimulus s(t′) is presented. These spike rates are the probabilities
per unit time for the occurrence of a single spike in either cell A or cell
B, also called the post-stimulus time histogram (PSTH). We can deﬁne the
probability per unit time for a spike in cell A to occur at time t and a spike
in cell B to occur at time t′, and this will be the joint post-stimulus time
histogram,

JPSTHAB(t, t′) =

ρA(t)ρB(t′)

trials.

(A.5)

h
Alternatively, we can consider an event E deﬁned by a spike in cell A at time
t and a spike in cell B at time t
τ , with the relative time τ measured to a
precision of ∆τ . Then the rate of these events is

−

i

rE(t) =

∆τ /2

dt′ JPSTHAB(t, t

τ + t′)

−∆τ /2
Z
∆τ JPSTHAB(t, t

−

τ ),

−

≈

(A.6)

(A.7)

where the last approximation is valid if our time resolution is suﬃciently
high. Applying our general formula for the information carried by single
events, Eq. (5), the information carried by pairs of spikes from two cells can
be written as an integral over diagonal “strips” of the JPSTH matrix,

I(E; s) =

T

1
T Z

0

dt

JPSTHAB(t, t
JPSTHAB(t, t

τ )
τ )

it

−
−

log2

"

JPSTHAB(t, t
JPSTHAB(t, t

τ )
τ )
it #

−
−

,

h
(A.8)
where
it is an average of the JPSTH over time, which is
equivalent to the standard correlation function between the two spike trains.

JPSTHAB(t, t

τ )

−

h

h

16

The discussion by Vaadia et al.

[9] emphasizes that modulations of the
JPSTH along the diagonal strips allows correlated ﬁring events to convey
information about sensory signals or behavioral states, and this information
is quantiﬁed by Eq. (A.8). The information carried by the individual cells is
related to the corresponding integrals over spike rates, Eq. (9). The diﬀerence
between the the information conveyed by the compound spiking events E,
and the informations conveyed by spikes in the two cells independently, is
precisely the synergy between the two cells at the given time lag τ . For τ = 0,
it is the synergy –or extra information– conveyed by synchronous ﬁring of
the two cells.

We would like to connect the present approach also with previous work
which focused on how events reduce our uncertainty about the stimulus [7].
Before we observe the neural response, all we know is that stimuli are chosen
from a distribution P [s(t′)]. When we observe an event E at time tE, this
should tell us something about the stimulus in the neighborhood of this time,
and this knowledge is described by the conditional distribution P [s(t′)
tE].
If we go back to the deﬁnition of the mutual information between responses
and stimuli, we can write the information conveyed by one event in terms
this conditional distribution,

|

I(E; s) =

Ds(t′)

dtEP [s(t′), tE] log2

Z

Z

 

P [s(t′), tE]
P [s(t′)]P [tE] !
P [s(t′)
tE]
|
P [s(t′)] !

 

|

=

dtEP [tE]

Ds(t′)P [s(t′)

tE] log2

. (A.9)

If the system is stationary then the coding should be invariant under time
translations:

P [s(t′)

tE] = P [s(t′ + ∆t′)
This invariance means that the integral over stimuli in Eq. (A.9) is indepen-
dent of the event arrival time tE, so we can simplify our expression for the
information carried by a single event,

tE + ∆t′].

(A.10)

|

|

I(E; s) =

dtEP [tE]

Ds(t′)P [s(t′)

Z

=

Ds(t′)P [s(t′)

tE] log2

|

 

tE] log2

 
tE]

|
P [s(t′)
|
P [s(t′)] !

.

tE]

P [s(t′)
|
P [s(t′)] !

(A.11)

Z

Z

Z

Z

17

This formula was used by de Ruyter van Steveninck and Bialek [7] To connect
with the present work, we express the information in Eq. (A.11) as an average
over the stimulus,

I(E; s) =

tE]

P [s(t′)
|
P [s(t′)] !

log2

P [s(t′)
tE]
|
P [s(t′)] ! +s

.

 

*  

Using Bayes’ rule,

P [s(t′)
P [s(t′)]

|

tE]

P [tE|

=

s(t′)]

P [tE]

=

rE(tE)
¯rE

(A.12)

(A.13)

where the last term is a result of the distributions of event arrival times being
proportional to the event rates, as deﬁned above. Substituting the back to
Eq. (A.11), one ﬁnds the equivalent of Eq. (6).

Appendix B: Experimental setup

In the experiment we used a female blowﬂy, which was a ﬁrst generation
oﬀspring of a wild ﬂy caught outside. The ﬂy was put inside a plastic tube
and immobilized with wax, with the head protruding out. The proboscis
was left free so that the ﬂy could be fed regularly with some sugar water.
A small hole was cut in the back of the head, close to the midline on the
right side. Through this hole, a tungsten electrode was advanced into the
lobula plate. This area, which is several layers back from the compound eye,
includes a group of large motion detector neurons with wide receptive ﬁelds
and strong direction selectivity. We recorded spikes extracellularly from one
of these, the contralateral H1 neuron [22]. The electrode was positioned such
that spikes from H1 could be discriminated reliably, and converted into TTL
pulses by a simple threshold discriminator. The TTL pulses fed into a CED
1401 interface, which time stamped the digitized spikes at 10 µs resolution.
To keep exact synchrony over the duration of the experiment, the spike tim-
ing clock was derived from the same internal CED 1401 clock that deﬁned
the frame times of the visual stimulus.

The stimulus was an rigidly moving bar pattern, displayed on a Tektronix
608 high brightness display. The radiance at average intensity was about 20

18

·

·

±

sr), which amounts to about 5

104 eﬀectively transduced photons
mW/(m2
per photoreceptor per second [23]. The bars were oriented vertically, with
intensities chosen at random to be ¯I(1
C), where C is the contrast. The
distance between the ﬂy and the screen was adjusted so that angular sub-
tense of a bar equaled the horizontal interommatidial angle in the stimulated
part of the compound eye. This setting was found by determining the eye’s
spatial Nyquist frequency through the reverse reaction [24]. For this ﬂy, the
horizontal interommatidial angle was 1.45◦, and the distance to the screen
105 mm. The ﬂy viewed the display through a round 80 mm diameter di-
aphragm, showing approximately 30 bars. From this we estimate the number
of stimulated ommatidia in the eye’s hexagonal raster to be about 612.

Frames of the stimulus pattern were refreshed every 2 ms, and with each
new frame the pattern was displayed at a new position. This resulted in
an apparent horizontal motion of the bar pattern, which is suitable to ex-
cite the H1 neuron. The pattern position was deﬁned by a pseudorandom
sequence, simulating independent random numbers uniformly distributed be-
tween -0.47◦ to + 0.47◦ (equivalent to -0.32 to +0.32 omm, horizontal om-
matidial spacings). This corresponds to a diﬀusion constant of 18.1(◦)2/s or
8.6 omm2/s. The sequence of pseudorandom numbers contained a repeating
part and a nonrepeating part, each 10 seconds long, with the same statistical
parameters. Thus in each 20 second cycle the ﬂy saw a 10 second movie
that it had seen 20 seconds before, followed by a 10 second movie that was
generated independently.

19

References

[1] E. D. Adrian, The Basis of Sensation: The Action of the Sense Organs

(W. W. Norton, New York, 1928).

[2] C. E. Shannon, Bell Sys. Tech. J. 27, 379–423, 623–656 (1948).

[3] D. MacKay and W. S. McCulloch, Bull. Math. Biophys. 14, 127–135

(1952).

[4] L. M. Optican and B. J. Richmond, J. Neurophys. 57, 162–178 (1987).

[5] F. Rieke, D. Warland, R. de Ruyter van Steveninck, and W. Bialek,
Spikes: Exploring the Neural Code (MIT Press, Cambridge, 1997).

[6] S. P. Strong, R. Koberle, R. R. de Ruyter van Steveninck, and W. Bialek,

Phys. Rev. Lett. 80, 197–200 (1998).

[7] R. de Ruyter van Steveninck and W. Bialek, Proc. R. Soc. Lond. Ser.

B 234, 379–414 (1988).

[8] R. R. de Ruyter van Steveninck, G. D. Lewen, S. P. Strong, R. Koberle,

and W. Bialek, Science 275, 1805–1808, (1997).

[9] E. Vaadia, I. Haalman, M. Abeles, H. Bergman, Y. Prut, H. Slovin, and

A. Aertsen, Nature 373, 515–518 (1995).

[10] W. Bialek, in 1989 Lectures in Complex Systems, SFI Studies in the
Sciences ofComplexity, Lect. Vol. II, E. Jen, ed., pp. 513–595 (Addison-
Wesley, Menlo Park CA, 1990)].

[11] W. E. Skaggs,B. L. McNaughton, and K. M. Gochard, in Advances in
Neural Information Processing 5, S. J. Hanson, J. D. Cowan, and C. L.
Giles, eds., pp. 1030–1037 (Morgan Kaufmann, San Mateo CA, 1993).

[12] M. DeWeese (Network 7, 325, 1996).

[13] S. Panzeri, G.. Biella, E. T. Rolls, W. E. Skaggs, and A. Treves, Network

7, 365–370 (1996).

20

[14] M. F. Land and T. S. Collett, J. Comp. Physiol. 89, 331–357 (1974).

[15] M. Abeles, H. Bergmann, E. Margalit, and E. Vaadia, J. Neurophysiol.

70, 1629–1638 (1993).

[16] J. J. Hopﬁeld, Nature 376, 33–36 (1995).

[17] M. Meister, Proc. Nat. Acad. Sci. (USA) 93, 609–614 (1995).

[18] W. Singer and C. M. Gray, Ann. Rev. Neurosci. 18, 555–586 (1995).

[19] K. Magelby, in Synaptic Function, G. M. Edelman, V. E. Gall, and K.
M. Cowan, eds., pp. 21–56 (John Wiley and Sons, New York, 1987).

[20] W. M. Usrey, J. B. Reppas and R. C. Reid, Nature 395, 384 (1998).

[21] A.M. Aertsen, G.L. Gerstein, M.K. Habib and G. Palm, J. Neurophysiol.

61, 900–917 (1989).

[22] N. Franceschini, A. Riehle and A. le Nestour, in Facets of Vision, D. G.
Stavenga and R. C. Hardie, eds., pp. 360–390 (Springer-Verlag, Berlin,
1989). K. Hausen and M. Egelhaaf, ibid., pp. 390–424.

[23] A. Dubs, S.B. Laughlin and M.V. Srinivasan, J. Physiol. 317, 317–334

(1984).

[24] K.G. Gotz, Kybernetic 2, 77–92 (1964).

21

Figures

Fig. 1. Generalized event rates in the stimulus–conditional response en-
semble. A time dependent visual stimulus is shown to the ﬂy (a), with the
time axis deﬁned to be zero at the beginning of the stimulus. This stimulus
runs for 10 s, and is repeatedly presented 360 times. The responses of the
H1 neuron to 60 repetitions are shown as a raster (b), in which each dot
represents a single spike. From these responses, time dependent event rates
rE(t) are estimated: the ﬁring rate (post-stimulus time histogram) (c); the
rate for spike pairs with interspike time τ = 3
1 ms (d) and for pairs with
τ = 17
1 ms (e). These rates allow us to compute directly the information
transmitted by the events, using Eq. (5).

±

±

→

Fig. 2. Finite size eﬀects in the estimation of the information conveyed
by single spikes. (a) Information as a function of the bin size ∆t used for
computing the time dependent rate r(t) from all 350 repetitions (circles),
and from 100 of the repetitions (ﬁlled triangles). A linear extrapolation to
the limit ∆t
0 is shown for the case where all repetitions were used (solid
line). (b) Information as a function of the inverse number of repetitions N,
for a ﬁxed bin size ∆t = 2ms. (c) Statistical error due to the ﬁniteness of the
time segment of length T . Information shown as a function of the inverse
time segment 1/T , was obtained by dividing the full 10-sec segments into
smaller segments of length T (circles). For each such division, the errorbars
represent the standard deviation of the values obtained from the diﬀerent
1/√T )
time intervals. These error bars should follow a square-root law (σ
if the small segments are independent. The dashed line shows the best power
law ﬁt to the sequence of standard deviations, which extrapolates to an er-
rorbar of σ

0.05 for the full 10-sec segment.

∝

≈

Fig. 3. Information about the signal transmitted by pairs of spikes, com-
puted from Eq. (5), as a function of the time separation between the two
spikes. The dotted line shows the information that would be transmitted by
the two spikes independently (twice the single spike information).

22

Fig. 4. (a) Ratio between the variance of interspike time and the sum of
variances of the two spike times. Variances are measured across repeated
presentations of same stimulus, as explained in the text. This ratio is plot-
ted as a function of the interspike time τ , for two experiments with diﬀerent
image contrast. (b) Extra information conveyed cooperatively by pairs of
spikes, expressed as a fraction of the information conveyed by the two spikes
independently. While the single spike information varies with contrast, (1.5
bits/spike for c=0.1 compared to 1.3 bits/spike for c=1), the fractional syn-
ergy is almost contrast independent.

Fig. 5. Information conveyed by spike pairs as a function of time resolu-
tion. An event –pair of spikes– can be described by two times: the separation
between spikes (relative time), and the occurrence time of the event with re-
spect to the stimulus (absolute time). The information carried by the pair
depends on the time resolution in these two dimensions, both speciﬁed by the
bin size ∆t. Open symbols are measurements of the information for a ﬁxed
absolute-time resolution of 2 ms, and a variable relative-time resolution ∆t.
Closed symbols correspond to a ﬁxed relative-time resolution of 2 ms, and
a variable absolute-time resolution ∆t. For short intervals, the sensitivity
to coarsening of the relative time resolution is much greater than to coars-
ening of the absolute time resolution In contrast, sensitivity to relative and
absolute time resolution is the same for the longer, nonsynergistic, interspike
separations.

23

)
s
/
g
e
d
(
 
 
 
 
)
t
(
s

l

a
i
r
t

)
s
/
1
(

 
 
)
t
(
E
 
r

200

(a)

0

−200

(b)

40

20

0

200

0

0

(d)

200

(e)

200

0

0

Fig. 1

(c)

spikes

3ms
pairs

17ms
pairs

0.2

0.1
t (s)

)
s
t
i

b
(
 
n
o

i
t

a
m
r
o
f
n
I

2

1

0

2

1

2

1

0

0

5
∆
   t (ms)

10

(a)

15

Fig. 2

0
0.00

0.02

0.04

0.06

  1/ N 

(b)

(c)

0.1

1.0

   1/T (1/s)

Fig. 3

8.0

6.0

4.0

2.0

)
s
t
i

b
(
 
n
o

i
t

a
m
r
o
f
n
I

0.0

0

20

t (ms)

40

(a)

(b)

c=1
c=0.1

Fig. 4

)
2 2
 
σ
+
2
 
1
σ
(
/
)
 
(
 
2
σ

t

1.0

0.5

0.0

0.5

0.0

fractional          

1.0

synergy 

0

10

20

30

40

t (ms)

)
s
t
i
b
(
 
n
o
i
t
a
m
r
o
f
n
I

5

4

3

2

1

Fig. 5

3 ms

9 ms

17 ms

10

2

6

4
      ∆        

 t (ms)

8


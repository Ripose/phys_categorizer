2
0
0
2
 
b
e
F
 
5
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
4
1
0
2
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Timing and Counting Precision in the Blowﬂy Visual System

Rob de Ruyter van Steveninck and William Bialek
NEC Research Institute,
4 Independence Way,
Princeton NJ 08540,
USA

Abstract

·

We measure the reliability of signals at three levels within the blowﬂy visual system, and
present a theoretical framework for analyzing the experimental results, starting from the Pois-
son process. We ﬁnd that blowﬂy photoreceptors, up to frequencies of 50-100 Hz and photon
105/s, operate well within an order of magnitude from ideal
capture rates of up to about 3
photon counters. Photoreceptors signals are transmitted to LMCs through an array of chem-
ical synapses. We quantify a lower bound on LMC reliability, which in turn provides a lower
bound on synaptic vesicle release rate, assuming Poisson statistics. This bound is much higher
than what is found in published direct measurements of vesicle release rates in goldﬁsh bipolar
cells, suggesting that release statistics may be signiﬁcantly sub-Poisson. Finally we study H1,
a motion sensitive tangential cell in the ﬂy’s lobula plate, which transmits information about a
continuous signal by sequences of action potentials. In an experiment with naturalistic motion
stimuli performed on a sunny day outside in the ﬁeld, H1 transmits information at about 50%
coding eﬃciency down to millisecond spike timing precision. Comparing the measured reliability
of H1’s response to motion steps with the bounds on the accuracy of motion computation set by
photoreceptor noise, we ﬁnd that the ﬂy’s brain makes eﬃcient use of the information available
in the photoreceptor array.

1

Introduction

Sensory information processing plays a crucial role in the life of animals, including man, and
perhaps because it is so important it seems to happen without much eﬀort. In contrast to this, our
subjective experience suggests that activities of much lower urgency, such as proving mathematical
theorems or playing chess, require substantial conscious mental energy, and this seems to make them
inherently diﬃcult. This may deceive us into thinking that processing sensory information must
be trivially easy. However, abstract tasks such as those mentioned are now routinely performed
by computers, whereas the problems involved in making real-life perceptual judgments are still far
from understood. Playing chess may seem much more diﬃcult than discerning a tree in a landscape,
but that may just mean that we are very eﬃcient at identifying trees. It does not tell us anything
about the “intrinsic” diﬃculty of either of the two tasks.

Thus, to ﬁnd interesting examples of information processing by the brain we do not need to
study animals capable of abstract thinking. It is suﬃcient that they are just good at processing
sensory data. Partly for this reason, sensory information processing by insects has been an active
ﬁeld of study for many years. Undeniably, insects in general have simpler brains than vertebrates,
but equally undeniably, they do a very good job with what they do have. Noting that insect brains
are very small, Roeder (1998) remarks:

Yet insects must compete diversely for their survival against larger animals more copi-
ously equipped. They must see, smell, taste, hear, and feel. They must ﬂy, jump, swim,

1

Figure 1: Two ﬂies in a chase. The black ﬂy is chased by the gray one for a duration of about a second. Flies
(Fannia canicularis) were ﬁlmed from below, and their positions estimated every 20 ms. The chasing ﬂy estimates
the position of the leading one purely by means of visual input. Modiﬁed from Fig. 4 in Land and Collett (1974).

crawl, run, and walk. They must sense as acutely and act as rapidly as their predators
and competitors, yet this must be done with only a fraction of their nerve cells.

Spectacular examples of insect behavior can be found in Tinbergen (1984) and Berenbaum (1995).
Brodsky (1994) describes the acrobatics of ﬂy ﬂight: “The house ﬂy can stop instantly in mid
ﬂight, hover, turn itself around its longitudinal body axis, ﬂy with its legs up, loop the loop, turn
a somersault, and sit down on the ceiling, all in a fraction of a second.” An example of some of
this performance is shown in Fig. 1, and clearly the acrobatic behavior displayed there must be
mediated by impressive feats of sensory information processing.

In this chapter we will look at the tip of this iceberg, and study some aspects of visual informa-
tion processing in the blowﬂy. Insects lend themselves well for electrophysiological and behavioral
studies, especially for quantitative analysis, and the emphasis will be on quantifying the accuracy
of neural signals and neural computations, as advocated explicitly by Bullock (1970). This is a
fundamentally probabilistic outlook, requiring a suitable statistical description of signal and noise.
It also is the principal description from the point of view of representation and processing of infor-
mation. For example, if you (or the brain) are asked to estimate the light intensity in the outside
world based on the reading of a photoreceptor voltage, then of course you need to know the gain,
i.e., the conversion factor from light intensity to photoreceptor voltage. To ﬁnd the light intensity
you just divide the voltage by this gain. The accuracy of your estimate depends on the magnitude
of the cell’s voltage noise, and this translates into an uncertainty in light intensity through the
same gain factor. But then the uncertainty in the estimate depends on the ratio of the signal
voltage and the noise voltage, and not on the speciﬁc value of the gain. Note that the gain is a “bi-
ological” parameter—why does the cell produce millivolts and not hundreds of millivolts?—while

2

the accuracy of estimates is measured in the same units as the physical stimulus in the outside
world. Further, we will see that there are often absolute limits to accuracy that are set by physical
principles. Thus by studying the accuracy of estimates we move from a characterization of neurons
that is speciﬁc to the their biological context toward a characterization that is more absolute and
universal.

The example of estimating light intensity by looking at the voltage in a photoreceptor admittedly
is very simple. Nonetheless, it illustrates an important point, namely that we can quantify on an
absolute scale the performance of neurons as encoders and processors of sensory information even
without a complete understanding of the underlying physiological mechanisms. The challenge that
we address in later sections of this chapter is to give a similarly quantitative description for neurons
at higher stages of processing, where the interpretation of the neural signals is much more diﬃcult.
We begin at the beginning of vision, namely light. Because light is absorbed as an irregular
stream of photons, it is an inherently noisy physical signal. This is modeled mathematically by the
Poisson process, of which we give some mathematical details which will be used later on. Then
we will focus on the performance of the photoreceptor cells of the ﬂy’s retina which capture light
and convert it into an electrical signal. These cells encode signals as graded, or analog, variations
of their membrane potential. This representation is noisy in part due to the ﬂuctuations in the
photon ﬂux, and in part due to limitations imposed by the cell itself, and we can tease apart these
contributions. Then we will look at the accuracy of chemical synaptic transmission between the
photoreceptor cell and the LMC (Large Monopolar Cell) by comparing signal and noise in the
presynaptic and postsynaptic cells. A chemical synapse releases discrete vesicles, and therefore the
Poisson process is a natural starting point for a mathematical description of signal transfer across
the synapse.

Of course, having a representation of light intensities in the outside world does not in itself
constitute vision. It is the task of the brain to make sense of the ever ﬂuctuating signals in the
photoreceptor array. As an example we consider the estimation of wide-ﬁeld motion from signals
in the photoreceptor array, which is a relatively simple neural computation. We will analyze the
limits to the reliability of this computation, given the reliability of the photoreceptor signals, and
we will compare this to the reliability of performance of H1, a spiking, wide ﬁeld motion sensitive
cell in the ﬂy’s brain.

By comparing the measured reliability of cells with the physical limits to reliability set by the
noise in the input signal we get an idea of the statistical eﬃciency of nerve cells and of neural
computation. This also makes it possible to quantify rates of information transfer, which, although
by no means the whole story, nevertheless captures the performance of nerve cells in a useful single
number.

2 Signal, Noise and Information Transmission in a Modulated

Poisson Process

A Poisson process is a sequence of point events with the property that each event occurs indepen-
dently of all the others. We will treat some of the mathematics associated with Poisson processes
and shot noise, but our emphasis is on an intuitive, rather than a rigorous presentation. Many of
the issues discussed here can be found in a treatment by Rice (1944, 1945), which also appears in
an excellent collection of classic papers on noise and stochastic processes by Wax (1954). A more
comprehensive mathematical treatment is given by Snyder and Miller (1991).

The Poisson process is a useful model ﬁrst of all because it describes the statistics of photon
capture very well. There are exotic light sources which deviate from this (Saleh and Teich 1991),
but biological organisms deal with normal thermal light sources. The photoreceptor response can
then be modeled to ﬁrst approximation as shot noise, which is a linearly ﬁltered Poisson process.
The ﬁltering process itself may be noisy, as is the case in phototransduction. Consequently, we also

3

treat the more general case in which the ﬁlter itself is a random waveform. The Poisson process is
often used to model other point processes occurring in the nervous system, such as vesicle release at
a chemical synapse, or trains of action potentials. We will present examples of both, and show that
for vesicle release the Poisson process is probably not a good approximation, and for describing
spikes in H1 it is certainly not good. But even then it is useful to understand the Poisson process,
because it provides a simple example with which to ground our intuition.

2.1 Description of the Poisson process

Many of the notions treated in this section are illustrated in Fig. 2. Fig. 2B for example, shows
a single realization of a point process on the time axis. This can be written as a series of events
. A useful mathematical abstraction is to represent each event as a delta
occurring at times t1, t2,
function, δ(t

tk), so that the full series is a sum of these functions:

· · ·

−

ρ(t) =

δ(t

tk).

−

Xk

(1)

Here the delta function has the dimension of inverse time, and an area equal to 1, so that the
integral of ρ(t) over a time window will be equal to the number of events in that window. What
distinguishes a Poisson process from other sequences of events is that the occurrence of each event
is random and completely independent of the occurrence of all the others. Eqn. 1 describes one
single outcome, which is analogous to observing the number on a die in one single throw. Although
the particular result may be important in a game of dice, it is not the particular realization that we
are interested in here. Instead we wish to derive what we can expect under “typical” conditions.
A construction that helps in such derivations is to think not of one single outcome of the process,
but of a great number of independent realizations. This is similar to the concept in statistical
mechanics of an ensemble of independent systems, each in its own state, and all obeying the same
macroscopic conditions (see for example Schr¨odinger 1952). The advantage of this mental picture
is that it provides a convenient starting point for reasoning about typical or average behavior.

For the die we usually take it for granted that the chance of getting any particular number from
1 to 6 is just 1/6. But we could also imagine an immense number of dice thrown at random, and
if we were to count the ones that landed on 6, we would ﬁnd their proportion to the total to be
very close to 1/6. Obviously, using the ensemble concept for reasoning about dice is a bit overdone.
Later on we will look at experiments where the same time dependent stimulus sequence is repeated
a large number of times, while neural responses are recorded. In that case it will be useful to think
about a large set of outcomes—our independent trials—and to distinguish ensemble averages over
that set of trials from time averages . Ensemble averages will generally be indicated by
, and
. Here we use the term “ensemble” in a loose sense, and primarily to make the
time averages by
distinction with time averages. A synthesized example is shown in Fig. 2E, while Fig. 17B shows
a set of spike timing data from a real experiment.

h· · ·i

· · ·

Suppose that Eqn. 1 represents a realization of a Poisson process that has a constant, time
independent rate of λ, that is on average we expect to see λ pulses in a one second time window.
How does this relate to the sequence described by Eqn. 1? In the ensemble picture we construct
a large number, M , of independent realizations of ρ(t), denoted ρ1(t), ρ2(t),
, ρM (t), with total
counts N1, N2,

, NM . The ensemble average, r(t) of ρ(t) is then:

· · ·

· · ·

r(t) =

ρ(t)
i
h

= lim

M →∞

1
M

M

Nm

Xm=1

Xkm=1

δ(t

tkm) = λ.

−

(2)

Obviously λ has the same dimension, inverse time, as the delta function. But how do we derive this
result in the ﬁrst place? Let us introduce a rectangular pulse, [Π(t/∆t)]/∆t. Here Π(t/∆t) is by
< ∆t/2 and zero otherwise (see Bracewell, 1978, and Rieke et al. 1997,
deﬁnition equal to 1 for

t
|

|

4

)
s
/

1
(
 
)
t
(

λ

200

100

0

)
s
/

1
(
 
)
t
(

ρ

)

V
m

(
 
)
t
(

h
v

l

a
i
r
t

)

V
m

(
 

)
t
(

h
v

)
t
(

h
V

〉

〈
=

 
)
t
(
h

)
t

n
e
v
e
/
V
m

(

C

0

t (ms)

100

A

B

D

E

F

0

100

200

300

400

500

Figure 2: Illustration of some of the basic concepts used in the analysis. A: Time dependent rate, λ(t), of a Poisson
process. B: A single realization of statistically independent events on the time axis, generated from the rate function
shown in A. The arrows represent delta functions. C: A temporal ﬁlter, h(t), used to smooth the train of events in B
to produce the example of a single shot noise trace, vh(t), shown in panel D. E: A raster representation of the outcome
of 25 trials in which events are generated by the rate function λ(t) shown in A. F: The average trace, Vh(t) = hvh(t)i,
of a large number of trials ﬁltered by h(t).

t (ms)

5

A.1), so that [Π(t/∆t)]/∆t is ∆t wide, and 1/∆t high. This obviously has unit area, just like the
“real” delta pulse, and it becomes more like a delta pulse if we let ∆t shrink to zero. If ∆t
1/λ,
then such pulses will (almost) never overlap. If we now also imagine time to be discretized in bins
∆t, then it becomes a matter of bookkeeping to count what proportion of the realizations in the
λ. To normalize to units of time
ensemble has a pulse in a given time bin. The answer is P1 = ∆t
we must divide this by the binwidth, ∆t, so the rate will be λ.

≪

·

Thus we expect to see about µ = λ

T events occurring in a window of T seconds in a single
realization, independent of where we put the window. Of course, µ is an average, and we do not
expect to observe exactly µ events each time, if only because µ can be any nonnegative number,
while each event count value must be an integer. It is interesting to know how the values in each
trial are distributed. This is described by the Poisson distribution:

·

P (N ) =

(λT )N
N !

·

−λT .
e

(3)

2
i

. Further,
i

= λ
It is easy to show that this distribution has mean value µ =
−
, and this means that σ2 = µ, which is an important property
=
N
h
i
of the Poisson distribution. A distribution which is more narrow than a Poisson distribution has
σ2 < µ, and is often called sub-Poisson. The broader distribution, with σ2 > µ, is called super-
Poisson.

N 2
h

T , and variance σ2 =

N 2
h

N
h

N
h

2
i

− h

N

i

i

·

2.2 The modulated Poisson process

If we want to study visual information transfer we should not conﬁne ourselves to looking at steady
light levels, as constant visual stimuli have a zero rate of information transmission. One reading of
a signal value can of course carry information, but if it does not change over time the information
rate, in bits/s, goes to zero. Perhaps for that reason the visual system does not seem to care very
much for constant input signal levels. Later we will see the example of Large Monopolar Cells which
ﬁlter out practically all of the standing signal in the ﬂy’s photoreceptors. In real life, signals are
changing all the time, and in real experiments we usually modulate signals and probe the response
of nerve cells to these changes. To study the response of photoreceptors one may deliver ﬂashes of
light, sine waves of various frequencies, pseudorandom signals, and so on. All of these are really
modulations of an underlying Poisson process, namely the rate at which photons are captured. We
therefore must extend our description of the homogeneous Poisson process to an inhomogeneous,
or modulated, version in which the rate is a function of time. So suppose that the rate is time
dependent λ = λ(t), as depicted in Fig. 2A. If we have a large number of outcomes of a Poisson
process modulated with λ(t), then in analogy to Eqn. 2 the ensemble average, r(t), must equal
λ(t):

r(t) =

ρ(t)
i
h

= lim

M →∞

1
M

M

Nm

δ(t

tkm) = λ(t).

Xm=1

Xkm=1

−

(4)

2.3 Correlation functions and spectra

Because the pulses in a Poisson process occur independently of one another it is easy to compute
correlation functions and power spectral densities. These correlation functions are natural objects
from a physical point of view, and are used widely in the analysis of experimental data. There is
a diﬃculty, however, in that correlation functions are averages, and we have two diﬀerent notions
of averaging: averaging over time, and averaging over an ensemble of responses to identical inputs.
In many ways the situation is similar to that in the statistical mechanics of disordered systems,
where we can average over thermal ﬂuctuations, over diﬀerent realizations of the quenched disorder,
or over both. Our discussion here and in the next section is brief, summarizing results that are

6

largely well known. What we aim at ultimately is the derivation of Eqn. 23, connecting observable
quantities to the underlying rates of Poisson events.

We recall that if we average the signal ρ(t) over many trials in which the inputs are the same,
then we obtain the Poisson rate λ(t),
= λ(t). Since diﬀerent pulses occur independently of
one another, once we know the rate λ(t) there is no additional mechanism to carry correlations
across time. On the other hand, since ρ(t) is built out of delta functions at the moments of the point
events, there will be singularities if we compute correlations at short times. It is straightforward
to show that

ρ(t)
i
h

′

ρ(t)ρ(t
h

)
i

= λ(t)λ(t

) + λ(t)δ(t

′

′

t

).

−

Thus if we compute the “connected” correlation function—subtracting oﬀ the means of the terms
taken separately—all that remains is the delta function term:

′

ρ(t)ρ(t
h

)
c
i

′

ρ(t)ρ(t

≡ h
= λ(t)δ(t

)
i − h
′
).
t

−

ρ(t)

′

ρ(t

)
i

ih

It is interesting that if we now average over time, the answer is insensitive to the time dependence
of the rate. More precisely, the time average of the connected correlation is

∞

ρ(t)ρ(t + τ )
h

1
T Z
0
and this is true whether or not the Poisson process is modulated. In this sense the ﬂuctuations in
the Poisson stream are independent of the modulations, and this is an important way of testing for
the validity of a Possion description.

ρ(t)ρ(t + τ )
i
h

= λδ(τ ),

lim
T →∞

ic ≡

(7)

dt

Instead of averaging over an ensemble of trials we can also average directly over time. Now we
have no way of distinguishing between true correlations among successive events (which would indi-
cate a departure from Poisson statistics) and correlations that are “carried” by the time dependence
of the rate itself. The result is that

[ρ(t)ρ(t + τ )] = [λ(t)λ(t + τ )] + ¯λδ(τ ),
[ρ(t)ρ(t + τ )]c = [∆λ(t)∆λ(t + τ )] + ¯λδ(τ ),

λ. Thus the (connected) autocorre-
where the ﬂuctuations in the rate are deﬁned as ∆λ(t) = λ(t)
lation of the pulse train, computed by averaging over time, has a contribution from the (connected)
autocorrelation function of the rate and a delta function contribution.

−

We recall that the integral of ρ(t) over a time window counts the number of events in that
window. The variance of the count found in a window thus is equal to a double integral of the ρ—ρ
correlation function (see, for example, Appendix 2 in Rieke et al. 1997). The delta function in the
autocorrelation leads to a term in the count variance that grows linearly with the size of the window
over which we count, much as a delta function in the velocity autocorrelation for a Brownian particle
leads to diﬀusion; here we have a diﬀusion of the count. Since the mean count grows linearly with
the size of the counting window, the variance and mean are growing in proportion to one another,
and in fact the proportionality constant is one. This equality of variance and mean is one of the
deﬁning features of the Poisson process.

If we try to analyze the signal ρ(t) in the frequency domain, then the ﬁrst step is to subtract
ρ(t). Note that ρ(t) = λ if we average for a suﬃciently
the time average and deﬁne δρ(t) = ρ(t)
long time. Then if we Fourier transform and compute the obvious correlation functions, we obtain:

−

(5)

(6)

(8)

(9)

7

(10)

(11)

(12)

(13)

(14)

(15)

δ ˆρ(f ) =

dt exp(+2πif t)δρ(t),

Z
= δ(f + f

′

)Sδρ(f ),

′

δ ˆρ(f )δ ˆρ(f
h

)
i

Sδρ(f ) = S∆λ(f ) + λ,

where we use ˆ
to denote Fourier transforms. Thus the spectral density of ﬂuctuations in ρ, Sδρ,
·
is related to the spectral density of rate ﬂuctuations, S∆λ, plus a “white noise” term with an
amplitude equal to the mean rate. This structure parallels that for the correlation functions in
Eqn. (9) because the spectra and correlation functions form a Fourier transform pair,

[ρ(t)ρ(t′)]c =

df exp[

2πif (t

−

−

Z

′

t

)]Sδρ(ω);

this is the Wiener–Khinchine theorem.

Notice that if we have an ensemble of many trials with the same input, then we can deﬁne a
= λ(t).
λ(t), then the equations analogous to (11) and (12) above are as

diﬀerent “ﬂuctuation” in the signal ρ by subtracting oﬀ the time dependent mean
Thus if we write ∆ρ(t) = ρ(t)
follows:

ρ(t)
i
h

−

′

= δ(f + f

)S∆ρ(f )

′

∆ˆρ(f )∆ˆρ(f
h

)
i
S∆ρ(f ) = ¯λ.

Again we see that the connected ﬂuctuations are independent of the modulation.

2.4 Shot noise

Idealized Poisson events have zero duration, or equivalently, inﬁnite bandwidth. This is clearly
unrealistic; real signals have ﬁnite rise and decay times. We can give the Poisson process some more
meat by replacing every zero-duration event with a ﬁxed waveform and having all the waveforms
in the train be additive. Another way of saying this is that the train of events in Eqn. 1 is ﬁltered
by a linear ﬁlter h(t) (see Fig. 2C), and this is often a fair ﬁrst order model for the physics
underlying shot noise, such as pulses of electrons from a photomultiplier tube. The output is found
by convolving this ﬁlter with the sequence of delta functions:

vh(t) =

δ(t

tk)

h(t) =

−

⊗

h(t

tk),

−

Xk

Xk

(16)

where we use vh to denote the result, because in our applications the ﬁltered process will always
be a voltage. An example of a ﬁltered process is given in Fig. 2D. If the width of h(t) is very small
compared to 1/λ, then the shot noise still looks a lot like the original Poisson process in the sense
that there are clearly separated events. If, however, h(t) is much larger than the average separation
between events then vh(t) will be a rather smooth signal, and its value will be distributed according
to a Gaussian. This is easy to see: When at any one point in time, vh is the sum of a large number
of ﬁltered events, spread randomly in time, the central limit theorem (Feller 1966) tells us that the
probability distribution of that sum approaches a Gaussian, at least for reasonable forms of h(t).
Convolution in the time domain is equivalent to multiplication of the Fourier transforms in the
frequency domain, and so, using Eqn. 12, we can write down the power spectral density of the shot
noise process as:

Sv(f ) =

S∆λ(f ) + λ
h

i · |

ˆh(f )
2,
|

(17)

8

where we use the subscript v in Sv to remind us that the noise power density will represent the
power density spectrum of voltage ﬂuctuations measured from a cell. Notice that the spectral
density includes contributions from the “signal”—the time variation of λ—as well as from noise
due to randomness in the event times. Below we will isolate the “true” noise component of the
spectrum, which we will call Nv(f ).

Again these results are what we obtain by making a Fourier analysis, which is related to the
correlation functions that are deﬁned by averaging over time. On the other hand, if we can observe
many responses to the same inputs, then we can compute an average over this ensemble of trials,
to deﬁne the ensemble averaged output, Vh(t):

Vh(t)

vh(t)
i

≡ h

= lim

M →∞

[δ(t

tkm)

h(t)]

−

⊗

M

Nm

Xm=1

Xkm=1

M

Nm

1
M

1
M

= 

lim
M →∞


= λ(t)

h(t),

⊗

δ(t

tkm)

h(t)

Xm=1

Xkm=1

−

⊗



(18)

where the ﬁrst step follows because convolution is a linear operation, so that the order of summation
and convolution can be changed, and the second step is from Eqn. 4. The end result is illustrated in
Fig. 2F. Equation 18 thus simply states that the ensemble averaged output of a ﬁltered modulated
Poisson process is equal to the ﬁltered rate. This will be used in analyzing the transduction of
contrast by photoreceptors and second order cells, which for moderate contrast ﬂuctuations are
reasonably close to linear. Thus from an experiment where we generate λ(t) and measure Vh(t) we
can in principle derive h(t).

It will be convenient to write λ(t) as the product of a constant, λ0, and a contrast modulation
[1 + c(t)], where c(t) represents contrast as a function of time. This
function c(t): λ(t) = λ0
also conforms closely to the experiment, where we use c(t) as a signal to modulate a light source
with an average photon ﬂux equal to λ0.
In the experiment we will present the same stimulus
waveform a large number of times to generate an ensemble of responses. Now, using the frequency
representation of Eqn. 18 we get:

·

ˆVh(f ) = ˆλ(f )

ˆh(f )

= λ0

·
ˆc(f )

·

ˆh(f )

·

(f

= 0).

(19)

Thus the Fourier transform of the ensemble averaged response equals the product of the Fourier
In an experiment we set the stimulus, λ(t) in the above
transforms of the rate and the ﬁlter.
equation, and we measure Vh(t). Using these data and Eqn. 19 we can directly compute the
transfer function ˆh(f ) that translates the stimulus into the average response. But we can also look
at the ﬂuctuations around this average,

and now we want to compute the power spectral density of these ﬂuctuations. The key is to realize
that

(20)

(21)

which means that

∆vh(t) = vh(t)

Vh(t),

−

∆vh(t) = h(t)

∆ρ(t),

⊗

9

6
where we use the notation Nv because this truly is a spectral density of noise in the response vh.
Again the crucial result is that the noise spectral density is determined by the mean counting rate,
independent of the modulation. There is even a generalization to the case where the ﬁlter h(t) is
itself random, as discussed in the next section.

These results give us a way of calculating the underlying average Poisson rate from the observed

quantities in the experiment:

Nv(f ) =

ˆh(f )
2S∆r(f )
|
|
ˆh(f )
2,
= λ0
|
|

λ0 =

ˆVh(f )
2
2 .
|
|
ˆc(f )
Nv(f )
|

· |

(22)

(23)

This of course is valid only for an ideal, noiseless, system, the only noise being that introduced by
Poisson ﬂuctuations. We could choose to apply Eqn. 23 to the quantities we measure, and then
consider the result as a measurement of an eﬀective Poisson rate (see also Dodge et al. 1968).
This can then be compared to the rate we expect from an independent measurement of photon ﬂux
(which we can make—see the experimental sections) to assess how close the real system comes to
being ideal. But we can make the decription a bit more realistic by introducing a random ﬁlter in
the chain of events leading to the cell’s response.

2.5 A Poisson process ﬁltered by a random ﬁlter

The analysis presented above relies on the ﬁlter h(t) having a prescribed, ﬁxed shape. That assump-
tion is not entirely realistic. More to the point, we are interested in characterizing the limitations
of the system as a transmitter of information, and so it is precisely this deviation from strict de-
terminacy that we wish to analyze. Phototransduction is a biochemical process, rooted in random
collisions of molecules. Not surprisingly therefore, quantum bumps in photoreceptors are known to
ﬂuctuate (Fuortes and Yeandle 1964, Baylor et al. 1979, Wong et al. 1980, Laughlin and Lillywhite
1982), varying both in shape and in latency time. We would like to incorporate the eﬀect of such
ﬂuctuations in our formulation, and we do that here for the simplest case. Suppose a Poisson
event at time tk is ﬁltered by a ﬁlter hk(t), that the shape of this ﬁlter is drawn from a probability
distribution of ﬁlter shapes,
[h(t)], and that these draws are independent for diﬀerent times tk.
As before, the contributions of the ﬁltered events are assumed to add linearly. We can picture this
distribution of ﬁlter shapes again as an ensemble, and of course this ensemble has some average
ﬁlter shape,
. Because everything is still linear, we can exchange orders of averaging, and so
h(t)
i
h
it is easy to see that we can replace the ﬁxed shape h(t) in Eqn. 19 by its ensemble average. In
other words in the case of independently ﬂuctuating ﬁlters we obtain:

P

ˆvh(f )
i
h
and instead of being able to measure the ﬁxed shape of a ﬁlter we have to settle for characterizing
its average.

(24)

· h

= ˆλ(f )

ˆh(f )
,
i

ˆVh(f ) =

Finally we should compute the eﬀect of variable ﬁlter shapes on the power density spectrum,
that is we want the analogue of Eqn. 22. Here it is useful to remember that the power density
If bumps of variable shapes
spectrum is the Fourier transform of the autocorrelation function.
are generated at random times, then each bump correlates with itself. The correlations with the
others, because their shapes and arrival times are assumed independent, will lead to a constant.
The autocorrelation of the bump train is then the ensemble average of the autocorrelation of all
individual bumps. Likewise the power density spectrum is the ensemble average of the power
spectra of the individual bumps. The end result is then that:

10

We can now deﬁne an eﬀective Poisson rate that we compute from the experimental data:

Nv(f ) = λ0
2 = λ2
ˆVh(f )
|
|

· h|
0 · |h

ˆh(f )
2
|
ˆh(f )

i
2
i|

2.
ˆc(f )
|

· |

ˆλeﬀ (f ) =

ˆVh(f )
2
|
|
2
ˆc(f )
Nv(f )
· |
|
ˆh(f )
2
i|
ˆh(f )
2
|

|h

h|

i

·

,

= λ0

(25)

(26)

(27)

i

i|

|h

2.

ˆh(f )

is diﬀerent from that of

and this is in general a function of frequency, because when h(t) ﬂuctuates in shape, the behavior
of

ˆh(f )
2
h|
|
It is worthwhile ﬂeshing out what eﬀect ﬂuctuations in diﬀerent bump characteristics have on
ˆλeﬀ (f ). Here we consider variations in amplitude and in latency. There is good evidence that such
variations in bump parameters occur almost independent of one another (Howard 1983, Keiper and
Schnakenberg 1984). Variations in the amplitude of the bump can be modeled by assuming that
= 1,
individual bump shapes are described by β
and variance σ2
β, and where h0(t) has a ﬁxed shape. It is easy to derive that in that case we have
ˆλeﬀ (f ) = λ0/(1 + σ2
β). In other words, random variations in bump amplitude lead to a frequency
independent decrease in the eﬀective Poisson rate. Any form of noise that leads to a spectrally ﬂat
eﬀective decrease in photon ﬂux is therefore sometimes referred to as multiplicative (Lillywhite and
Laughlin 1979). An important special case is that of a decrease in the photon ﬂux. This can be
described by taking β to be a random variable with value either 0 or 1/p1, with p1 the probability
of a photon being transduced (so that
1). This leads to
ˆλeﬀ (f ) = p1

h0(t), where β is a random variable with mean

= 1, as required, and σ2

λ0, as expected.

β = 1/p1

β
h

β
h

−

i

i

·

Another important source of randomness is a ﬂuctuating latency time from photon absorption
to bump production. Suppose that we have a ﬁxed shape, h0(t) as before, but that the time delay
is distributed, independent for diﬀerent bumps, according to p(tlat). Displacing random events in a
random way preserves the independence and the mean rate, so that there is no eﬀect on the noise
power density. The timing with respect to external modulations is compromised, however, and this
mostly aﬀects the reliability at high frequencies. Again, starting from Eqn. 27 it is easy to derive:
ˆλeﬀ (f ) = λ0
2—that is, the frequency dependence of the eﬀective Poisson rate is given by
ˆp(f )
|
the Fourier power transform of the latency distribution (de Ruyter van Steveninck and Laughlin
1996b). Because p(tlat) is a probability distribution, its Fourier transform must go to 1 for f
0,
if the distribution is smooth. Thus, if bumps have random latencies
and will go to 0 for f
the eﬀective Poisson rate will be frequency dependent. Low frequencies are not aﬀected whereas
the eﬀective rate goes to zero in the limit of very high frequencies.

→ ∞

→

· |

·

Fluctuations in the duration of bumps as well as external additive noise in general have frequency
dependent eﬀects on ˆλeﬀ (f ). If the aim is to describe the phototransduction cascade and the other
processes occurring in the cell, then it is interesting to try and tease all the contributions apart,
and this may not be easy. Here, however, our goal is more modest, in that we want to quantify
the reliability of photoreceptors and the LMCs onto which they project; compare the results to
the limits imposed by the stochastic nature of light; and explore some of the consequences of
photoreceptor signal quality for visual information processing.

2.6 Contrast transfer function and equivalent contrast noise

It is useful to deﬁne two other quantities. The ﬁrst is the contrast transfer function, deﬁned by:

11

ˆH(f ) =

ˆVh(f )
ˆc(f )

= λ0

ˆh(f )
,
i

· h

(28)

(29)

which expresses the cell’s gain not as the translation from single photon to voltage, but from contrast
to voltage. This is practical because photoreceptors and LMCs work mostly in a regime of light
intensities where bumps are fused, and then it is natural to think of these cells as converting contrast
into voltage. In addition to this transduced contrast there is voltage noise. It is conceptually useful
to express these noise ﬂuctuations as an eﬀective noise ηc added to the contrast itself,

ˆv(f ) = ˆH(f )[ˆc(f ) + ˆηc(f )].

The spectral density of this eﬀective noise source is then the equivalent contrast noise power density
Nc(f ), and has units of (contrast)2/Hz. Since contrast itself is dimensionless this means that Nc
has units of 1/Hz, and hence 1/Nc has the units of a rate; we will see below that for an ideal photon
counter this rate is exactly the mean rate at which photons are being counted. To ﬁnd Nc(f ) we
inverse ﬁlter the measured voltage noise power density by the contrast power transfer function:

,

(30)

Nc(f ) =

Nv(f )
ˆH(f )
2
|
|
and we can easily derive that Nc(f ) = 1/λeﬀ (f ). If we now have a cascade of elements, such as
the photoreceptor and the LMC, and we measure the equivalent contrast noise power density at
each stage, we would like to deﬁne the accuracy of the interposing element, in this case the array
of synapses between photoreceptors and LMC. Using the equivalent contrast noise power density
it is easy to do this: If we measure Nc1(f ) and Nc2(f ) for two elements in a cascade, then for all f
we must have Nc2(f )
Nc1(f ), and the diﬀerence is the contribution of the element in between. In
the particular case of photoreceptors and LMCs we have to be careful to include the eﬀect of having
six photoreceptors in parallel. This, assuming we may treat them as statistically independent but
otherwise equal, is easy to do: When elements are combined in parallel we divide Nc(f ) for the
individual one by the number of elements to get the equivalent contrast noise power density of the
combination. The charm of working with the equivalent contrast noise power density is that it
allows us to compute the result of combining elements in series and in parallel, in the same way
that we calculate the net resistance of series and parallel combinations of resistors.

≥

3 The Early Stages of Fly Vision

3.1 Anatomy of the blowﬂy visual system

A good proportion of the surface of a ﬂy’s head is taken up by its compound eyes, which is a direct
indication that the eyes are very important to the ﬂy. This is also clear from other considerations:
The energy consumed by all the photoreceptors in the blowﬂy’s retina in bright light is about 8%
of the total energy consumption of a ﬂy at rest (Laughlin et al. 1998). In the blowﬂy, each eye
has about 5000 lenses, corresponding to 5000 pixels of visual input. These pixels are distributed
over almost a hemisphere, so the two eyes provide the ﬂy with almost complete surround vision.
The male’s eyes are somewhat larger than the female’s, because the visual ﬁelds of the male’s two
eyes overlap in the dorsofrontal region of visual space. This region is used in detecting and chasing
females.

3.2 Optics

The compound eye of insects forms an image onto an array of photoreceptors through a large
number of tiny lenses (with diameters ranging typically from 10 to 30 µm; see Fig. 3). Each

12

Figure 3: Top: Part of the retina, modiﬁed from Stavenga (1974), showing two ommatidia, each containing one lens,
several pigment cells, and 8 photoreceptor cells. Light enters the lens and is focused on the tips of the rhabdomeres.
It then travels in a bound mode along the rhabdomere, in which it can be absorbed by a membrane bound rhodopsin
molecule (Stavenga 1995). Through a series of biochemical steps this then leads to a measureable electrical response
across the cell membrane. Center: Schematic horizontal cross section through the head of a typical ﬂy, modiﬁed from
Kirschfeld (1979). The areas mainly relevant for visual information processing are the retina with its photoreceptors,
the lamina, where photoreceptor signals are combined and ﬁltered, and the medulla and lobula complex, where more
complex information processing takes place. Bottom: Exploded view of giant tangential cells of the lobula plate,
modiﬁed from Hausen (1981). The outlines of the two lobula plates and the esophagus are drawn in thin lines. The
lobula plate is an output station of the visual system, where tangential cells collect motion information over large
areas of the visual ﬁeld. From here signals are sent to the thoracic ganglion where they are used in the control of ﬂight.
Cells drawn in this ﬁgure collect their information in the right lobula plate; H1 and V2 project to the contralateral
plate. The tangential cells of the lobula plate encode information about wide ﬁeld motion. All are direction selective,
that is their ﬁring rate increases for motion in their preferred direction, while spike activity is suppressed for motion
in the opposite direction. The H cells code horizontal, and the V cells vertical motion. CH has a more complicated
directional selectivity. The labels HS and VS refer to groups of cells. The tangential cells are unique and identiﬁable,
so that they can be numbered. H1 in particular is a good cell to record from both because it is very easy to ﬁnd on
the contralateral side, and because it is inward sensitive, responding preferentially to motion from the side toward
the midline of the animal. The combination of this directional selectivity with the contralateral projection is unique,
so that H1 can be identiﬁed unambiguously from the electrophysiological recording.

13

−

≈

100

200µm), thin (

lens belongs to an ommatidium, which typically contains eight photoreceptors, and a number of
optical screening cells. Part of the photoreceptor cell membrane is densely folded, and forms a long
(
2µm) cylinder, called the rhabdomere. The membrane consists mainly
≈
of phospholipids and proteins so that its refractive index is higher than that of the surrounding
watery medium. Therefore, the rhabdomere acts as a waveguide, and light can travel along its long
axis in a bound mode. The combination of a lens and the tip of an optical waveguide in its focal
plane forms a spatial ﬁlter (Snyder 1979): Only light coming from a small angular region can enter
λ/D,
the waveguide. The physical limit to the resolution of this system is set by diﬀraction: ∆φ
1◦. See Exner (1891), Barlow
and with λ = 500 nm and D = 25 µm, we have ∆φ
(1952), and Feynman et al. (1963) for an analysis of the optics of compound eyes. The physics
of the lens-photoreceptor system is well understood, and theory is in very good agreement with
physiological ﬁndings (van Hateren 1984).

1/50 rad

≈

≈

≈

Flies do not have an iris pupil or eyelids, yet they need to protect their photoreceptors from
excessively intense light. This is accomplished by an elegant mechanism depicted in the top box in
Fig. 3. The top ommatidium shows a dark adapted photoreceptor, which has tiny pigment granules
dispersed through its cell body. In the light adapted state shown in the bottom ommatidium these
granules have migrated close to the photoreceptor rhabdomere. The granules absorb light, and
because they are close to the light guiding rhabdomere, they act as a light absorbing cladding
that captures up to 99% of the incoming photon ﬂux. This then prevents the photoreceptor from
saturating in bright daylight (Howard et al. 1987). The eﬀectiveness of the pupil as a light absorber
is regulated by feedback (Kirschfeld and Franceschini 1969, Franceschini and Kirschfeld 1976).

The photoreceptors in each ommatidium are numbered R1-R8, and arranged such that R7
lies on the optical axis of the lens. R8 lies behind R7, while the rhabdomeres of R1-R6 lie in a
trapezoidal pattern around the center, as shown by the cross section in the top frame in Fig. 3.
The optical axes of neighboring ommatidia point in slightly diﬀerent directions and they diﬀer by
an amount that matches the angular diﬀerence among the photoreceptors in a single ommatidium.
Therefore, eight photoreceptor cells in seven neighboring ommatidia share a common direction of
view. Receptors R1-R6 in all ommatidia have the same spectral sensitivity (Hardie 1985), and
those R1-R6 receptors in neighboring ommatidia that share the same direction of view combine
their signals in the next layer, the lamina. This is known as neural superposition (Braitenberg
1967, Kirschfeld 1967, 1979). Receptors R7/R8 are special, having a spectral sensitivity diﬀerent
from R1-R6, and bypassing the lamina, projecting directly to the medulla.

3.3 Reliability and adaptation

Because single photon responses are more or less standardized it seems a good idea to model the
photoreceptor voltage as a shot noise process, consisting of a stream of photon induced events often
referred to as “quantum bumps” (Dodge et al. 1968, Wong et al. 1980, de Ruyter van Steveninck
and Laughlin 1996a,b). However, we are dealing with a highly adaptive system. As discussed in
the previous section, the variance of shot noise should be proportional to the rate of the underlying
Poisson process. Panels B and C of Fig. 4 show that the variance at a mean bump rate of 300/s is
105/s. So shot noise does not appear to be a good model. The
much higher than at a rate of 3
solution to this dilemma is that the bumps change shape so that both their amplitude and their
width decrease when the photoreceptor is exposed to higher light intensity (see also Fig. 9). This
is the gist of the “adapting bump model” formulated by Wong et al. (1980). Clearly, when bumps
adapt, the shot noise model loses some of its generality in terms of predicting the expected response
amplitude to a certain stimulus or of predicting the noise power spectral density at diﬀerent light
intensities. However, one much more crucial aspect remains—the signal to noise ratio. Even if
bump amplitudes adapt to the ambient light intensity, the frequency dependent signal to noise
ratio depends only on the rate of the underlying Poisson process. One last caveat is in place
here: In the standard shot noise model, the “bump” shape is taken to be ﬁxed. Surely, in any

·

14

2

0

2

0

2

0

)

V
m

(
 
t
s
e
r
 

m
o
r
f
 
l
a
i
t
n
e
t
o
p
 
r
o
t
p
e
c
e
r
o
t
o
h
p

-2

pulsed source: single bumps

-2

bump rate=300/s

D

E

F

G

H

I

-2

bump rate=3⋅105/s

1

time (s)

0

2

0.00

0.05

0.10

10-2

10-1

100

time (s)

probability density
(1/mV)

Figure 4: Intracellular photoreceptor recordings. A, B, C: samples of photoreceptor voltage, each 2 seconds long.
A: Recording in the dark adapted state, with a light source that emitted brief ﬂashes leading to about 0.5 photons
captured per ﬂash on average. B: The same cell, with a continuous light source, and a photon capture rate of 300 per
second. C: As B, but now at a 100 times higher light intensity. Panels D, E, F show part of the traces in repectively
A, B, C but at higher time resolution. G, H, I: Amplitude distributions of the signals in A, B, C. Note that these
are drawn sideways, so that the vertical scales in all panels are the same. The probability densities are drawn on a
logarithmic scale, which means that the curves should approximate parabolas for Gaussian amplitude distributions.

A

B

C

15

realistic system there is noise, and here one can distinguish two of its net eﬀects: A variation in
the amplitude of the bumps, which limits the reliability of the system at all frequencies, and a loss
in timing precision, which aﬀects the higher fequencies more than the lower frequencies, as treated
in section 2.5. See Stieve and Bruns (1983) for more details on bump shape and de Ruyter van
Steveninck and Laughlin (1996b) for its eﬀects on the overall reliability of the photoreceptor.

The results we present below were obtained in experiments using a green light emitting diode
(LED) as a light source, which was always modulated around the same mean light level. The mean
photon ﬂux into the ﬂy’s photoreceptors was set by using ﬁlters of known density in the optical path
between LED and ﬂy. Light intensities were calibrated by counting the rate of quantum bumps at
a low light intensity, as depicted in Fig. 4a . This was generally done with the LED delivering short
(1 ms) ﬂashes, using ﬁlters of optical density between 3 and 4, that transmit a fraction of 10−3
to 10−4 of the incident photons. From this low level, light intensities were increased in calibrated
steps, and we can so deﬁne an extrapolated bump rate for each setting of light intensity in the
experiments. Note that this procedure does not specify the absolute quantum eﬃciency of photon
detection. At low light levels this eﬃciency, deﬁned as the probability for an on axis photon of
optimal wavelength (490 nm) to lead to a quantum bump, is about 50% (Dubs et al. 1981, de
Ruyter van Steveninck 1986).

In most experiments the LED was modulated with a pseudorandom waveform of duration 2s,
sampled at 1024 Hz with independent values at all sample times. This waveform was constructed
in the frequency domain, by computing a series of 1024 random numbers on [0, 2π), representing
the phases of Fourier components. That list, concatenated with an appropriate list of negative fre-
quency phase components, was inverse Fourier transformed, resulting in a sequence of real numbers
representing a series of 2048 time samples that was used as the contrast signal c(t). This procedure
ensured that the amplitudes of all frequency components were equal. This is not necessary in prin-
ciple, but it is very convenient in practice to have a signal that is free from spectral dropouts. The
use of pseudorandom waveforms to study neural signal processing has a long history; for a review
see Rieke et al. (1997). In the photoreceptors and LMCs of the ﬂy, the ﬁrst such experiments were
done by French and J¨ahrvilehto (1978).

Fig. 5 summarizes the measurements made in a typical experiment on a photoreceptor. The
same contrast waveform (top) was presented repeatedly and the response to each presentation
recorded. From a large number of repetitions we obtain an ensemble of responses, from which we
compute the ensemble average shown in Fig. 6A. Clearly the traces in Fig. 5B share the same overall
shape, but are not identical. Each trace diﬀers from the ensemble average, and these ﬂuctuations
represent the noise in the system. An ensemble of noise traces is obtained simply by subtracting
the ensemble averaged response from each individual trace.

We ﬁrst characterize the cell’s linear response by computing the ratio of the Fourier transform of
the ensemble averaged voltage waveform Vh(t), and the Fourier transform of the contrast modulation
waveform c(t), as in Eqn. 28. The square of the absolute value of this ratio gives us the contrast
power transfer function. The ensemble of noise traces is described by a power spectral density,
which we ﬁnd by averaging the Fourier power spectra of all the individual noise traces in the
experimental ensemble, or equivalently by computing the variance of each Fourier coeﬃcient across
the ensemble of noise traces. Finally, the ratio of these two functions, as deﬁned in Eqn. 27, is
the eﬀective Poisson rate. If the photoreceptor were an ideal photon counter, the ratio should not
depend on frequency, and be numerically equal to the extrapolated bump rate. Fig. 7 shows that
the ratio does depend on frequency, and that it goes down at the higher frequencies, notably so
above 100 Hz. This is a consequence of the limited temporal resolution of the transduction cascade
which after all consists of a sequence of chemical reactions, each of which depends on random
103 incoming bumps per second the photoreceptor
collisions between molecules. However, at 3.8
acts essentially as an ideal photon counter up to about 50 Hz. The deviation from this at low
frequencies is due to excess low frequency noise in the noise power density (see Fig. 7B,E) which

·

16

t
s
a
r
t

n
o
c

1

0

-1

A

B

0

20

40

60

80

100

time (ms)

Figure 5: A: Modulation of the LED light source by a computer generated pseudorandom signal sampled at ≈1
ms intervals. This panel shows a small, 100 ms section of the total trace which was 2 s long. In the experiment
the contrast waveform depicted here is presented repeatedly, and the response of the cell to all these repetitions is
recorded. B: Twenty examples of individual traces recorded from a blowﬂy photoreceptor in response to the contrast
waveform shown in A.

17

Photoreceptor

A

B

C

Large Monopolar Cell

E

F

G

D

H

)

V
m

(

)

V
m

(

e
s
n
o
p
s
e
r

 

e
c
a
r
t
 

i

e
s
o
n

e
s
n
o
p
s
e
r

)

V
m

(

e
c
a
r
t
 
e
s
o
n

i

)

V
m

(

2

0

2

0

-2

-2

8

6

4

2

0

-2

-4

-6

4

2

0

-2

-4

0

20

40

60

80

100

0.01

0.1

1

time (ms)

Probability density
(1/mV)

Figure 6: Sample traces of an experiment on a photoreceptor and an LMC, and amplitude distributions. A: A 100
ms segment of the ensemble averaged response of a photoreceptor to the modulation waveform shown in Fig. 5A.
B: Example of a 100 ms noise trace, that is the diﬀerence between one trace in Fig. 5B and trace A in this ﬁgure.
C, D As A, B above, but for an LMC. Note that the vertical scales in panels A-D are all the same: The LMC’s
signal is much larger than that of the photoreceptor. E: Amplitude distribution of the average voltage trace of the
photoreceptor. Dots: measured values; line: Gaussian ﬁt. F: As E, but now for the photoreceptor noise waveform.
G, H: as E, F but for the LMC.

18

is almost certainly due to 1/f noise in the equipment. At the higher extrapolated bump rate the
photoreceptor’s eﬃciency is constant up to almost 100 Hz, and is about a factor of 2 from ideal.
The behavior of the LMC is a bit diﬀerent in that it acts as a high pass ﬁlter, and transmits low
frequencies less reliably than would an ideal detector. However, for intermediate frequencies the
104 per second comes close to ideal. At the highest bump
LMC stimulated at a bump rate of 7.5
rate, 7.5
50
Hz). At this frequency the absolute performance of the LMC is quite impressive, being equivalent
to an eﬀective photon ﬂux of about 106 events per second.

106 per second, the LMC deviates from ideal by a factor of 8 at its best frequency (

≈

·

·

Fig. 8 presents an overview of the cell’s best performance, and compares it to the theoretical
limit. Here we show the maximum of the eﬀective Poisson rate for six photoreceptors and three
LMCs, each at multiple light levels. These data are plotted as a function of the photoreceptor’s
extrapolated bump rate, which for LMCs is the extrapolated LMC bump rate divided by six,
because it receives input from six photoreceptors. For the photoreceptors, at low light levels, the
measured maximum and the extrapolated bump rate are very close, and they are at most a factor
of two apart at the highest light intensities measured here. This indicates that the photoreceptor
105 extrapolated
is designed to take advantage of each transduced photon, up to ﬂuxes of about 3
bumps/s, and up to about 100 Hz. At bump rates around 105/s and higher the eﬃciency of the
photoreceptor begins to decline. This is caused primarily by the action of the pupil (Howard et al.
1987; see also section 3.2), which attenuates the photon ﬂux propagated through the rhabdomere
and thus increases the eﬀective contrast noise as described in section 2.5.

·

One reasonably expects that if the ﬂy’s brain is well designed, it would put the relatively high
quality of the photoreceptor signals to good use. This would then imply that the accuracy of visual
information processing is not too far from the photon shot noise limit at the light levels studied here.
A ﬁrst check is to see if neural superposition is eﬃcient in this statistical sense. Comparing the
measurements of maximal LMC eﬀective photon ﬂux with the ideal (open symbols and top dashed
line in Fig. 8), we see that this is indeed the case up to photoreceptor bump rates around 104–105
per second. For higher light intensities the LMC, as the photoreceptor, becomes less eﬃcient. There
is a hint in the data that the LMC declines somewhat faster than the photoreceptor, perhaps due
to limitations in the reliablity of synaptic transmission, as was noticed by Laughlin et al. (1987).
Given that neurons have a dynamic range much smaller than typical sensory signals, the ques-
tion arises of how the nervous system copes with the input it receives in order to transmit and
process information eﬃciently. Ultimately this is a matter of optimal statistical estimation, and
the result will therefore depend on the statistical and dynamical characteristics of the signals that
the animal encounters in its environment. It is well known, and we have seen examples already,
that photoreceptors adapt their gain to the ambient light intensity, which may vary over many
orders of magnitude. The usefulness of this adaptation seems obvious: At higher light intensity
the absolute gain should be brought down to keep the transduced contrast ﬂuctuations within the
cell’s voltage operating range. But implicit in this explanation is that the average light levels must
change relatively slowly compared to the contrast ﬂuctuations. If the sun ﬂickered unpredictably
on time scales of a second or so, and with an amplitude equal to that of the day night cycle, then
it would be of no use at all to adapt the photoreceptor gain. Only because large changes tend to
be slow does it make sense to design a system that tracks their mean, and changes its gain so as to
encode the faster ﬂuctuations more eﬃciently.

The eﬀect of adaptation is easier to appreciate when the responses are plotted in the time
domain. Fig. 9 shows impulse responses of both cell types, at four light levels spanning three
orders of magnitude. All the curves are derived from inverse Fourier transforms of the contrast
transfer function ˆH(f ) deﬁned earlier. The top two panels show the absolute gain—that is the
response normalized to a single photon capture. In the bottom panels the response is normalized
to a 1 ms lightﬂash with an amplitude equal to the mean light level, so these are normalized to
contrast. It is clear from the ﬁgures that there is a large range of gain control, as the shapes of the

19

photoreceptor

LMC

A

3.8105
3.8103

7.5106
7.5104

D

E

F

r
e

f
s
n
a
r
t
 
r
e
w
o
p

 
t
s
a
r
t

n
o
c

2

)
t
i

n
u

 
t
s
a
r
t

n
o
(cid:10)c
 
/
 

V
m

(

y
t
i
s
n
e
d
 
r
e
w
o
(cid:10)p
 
e
s
o
n

i

)
z
H

 
/
 

2
V
m

(

e
t
a
r
 
n
o
s
s
o
P
 
e
v
i
t
c
e

i

f
f

e

)
s
/
1
(

103

102

101

100

10-1

10-1

10-2

10-3

10-4

10-5

10-6

107

106

105

104

103

102

1

10

100

1000

1

10

100

1000

frequency (Hz)

frequency (Hz)

Figure 7: Characterization of a photoreceptor and an LMC in the frequency domain, each for two light intensities.
The legends of A and B give these intensities, expressed as extrapolated bump count rates. A: Photoreceptor contrast
power gain as a function of frequency. B: Power spectral densities of the ensemble of noise traces represented by
Fig. 6B. C: The eﬀective Poisson rate, calculated as the ratio of contrast power gain (panel A) and the power spectral
density (panel B) at each frequency. If the photoreceptor were an ideal photon counter, not adding any noise itself,
then the eﬀective Poisson rate should be spectrally ﬂat, at a level of the extrapolated bump rate given by the legend
in panel A. This is depicted by the dashed lines in C. D, E, F: as A, B, C, but now for an LMC.

B

C

20

)
s
/
1
(

i

 

 
f
f
e
R
m
u
m
x
a
m
 
d
e
r
u
s
a
e
m

106

105

104

103

102

102

103

104

105

106

photoreceptor bump rate (1/s)

Figure 8: Comparison of the best measured statistical performance of photoreceptors and LMCs to the theoretical
limit imposed by photon shot noise. The peak values of the eﬀective Poisson rate curves, such as those in Fig. 7C,F
are plotted as a function of the rate at which photons are absorbed by a photoreceptor. The rate of photon absorption
is extrapolated from a dark adapted experiment, in which individual bumps are counted (see Fig. 4A). The dashed
lines represent the behavior of an ideal photon counter for photorecptors (lower line) and LMCs (upper line). Along
the abscissa the bump count calibration values of the LMC are divided by 6 to get the bump rate of each of the
presynaptic photoreceptors. Statistically eﬃcient neural superposition requires that the LMC uses all photons from
all six photoreceptors from which it receives input, which means that the LMC data points should then follow the
upper dashed line.

responses normalized to single photons vary dramatically. Expressed as contrast gain, the responses
become higher in amplitude and sharper in time as the light intensity increases.

The photoreceptor primarily seems to scale down its photon conversion gain, both in amplitude
and in time course. The LMC ﬁlters out the photoreceptor’s DC level, except perhaps at the
lowest light intensities, and it also scales the gain (Laughlin and Hardie 1978). The combined
eﬀect leads to a scaling of the LMC amplitude ﬂuctuations. Remarkably, this scaling is such that
when stimulated with naturalistic, and very non-Gaussian, sequences, the LMC produces a voltage
output that follows a Gaussian quite closely (van Hateren 1997). There is evidence that adaptation
of this system is set so as to optimize information transmission rates under diﬀerent conditions
(van Hateren 1992).

The data of Fig. 9 show the behavior of the cells while they are in their adapted states, as care
was taken to to let the system adapt before the measurement was done. It is also interesting to
study the time course of adaptation, and here we will look in particular at adaptation of synaptic
transmission.

3.4 Eﬃciency and adaptation of the photoreceptor-LMC synapse

The link between the photoreceptors and the LMC is a parallel array of chemical synapses. The
detailed anatomy of this projection is well known (reviewed by Shaw 1981), and counts have been
made of the number of active zones between photoreceptors and LMCs. The total number of
synapses between one photoreceptor and an LMC is on average about 220 (Nicol and Meinertzhagen
1982, Meinertzhagen and Fr¨ohlich 1983), so that the total number of active zones feeding into an
LMC is close to 1320. Although this number was obtained from the houseﬂy Musca, which is smaller
than the blowﬂy, the total number is unlikely to be far oﬀ. The photoreceptor-LMC synapses are

21

photoreceptor

LMC

3.6×105
3.6×104
3.6×103
3.6×102

7.5×106
7.5×105
7.5×104
7.5×103

C

D

µ

)
n
o

t

o
h
p
/
V

(
 
e
s
n
o
p
s
e
r
 
e
s
u
p
m

l

i

0.6

0.4

0.2

0.0

e
s
n
o
p
s
e
r
 
e
s
u
p
m

l

i

l

)
e
s
u
p
 
t
s
a
r
t
n
o
c
 
t
i
n
u
/
V
m

(

2

1

0

0

10

20

30

40

50

0

10

20

30

40

50

time (ms)

time (ms)

Figure 9: Impulse responses of blowﬂy photoreceptors and LMCs, measured in the adapted state at diﬀerent light
intensities. The average light intensity, expressed as the extrapolated rate of eﬀective photon capture in bumps/s,
is given in the legends above A and C. The responses are scaled in two diﬀerent ways. A: Photoreceptor impulse
response, scaled to represent the electrical response of a photoreceptor to the capture of a single photon. B: The
same data as in A, but here expressed as the electrical response to a contrast pulse of 1 ms wide, with an amplitude
equal to the mean light intensity. C, D: as A, B, but for an LMC.

A

B

0.0

-0.1

-0.2

-0.3

5

0

-5

-10

22

tonically active, just like the synapses of vertebrate retinal bipolar cells. This means that even in
the absence of contrast ﬂuctuations, they release a stream of vesicles (Shaw 1981, Uusitalo et al.
1995, Lagnado et al. 1999). Also they pass on graded potentials, and it thus seems a reasonable
ﬁrst approximation to model them as units that release vesicles with a rate depending on the
presynaptic potential. One interesting question is then whether vesicle release can be modeled as a
modulated Poisson process. At ﬁrst sight this would seem wasteful, in the sense that if the synapse
would have better control over its vesicle release, it could emit vesicles in a much more deterministic
way. One might imagine that the synapse functions somewhat as a voltage controlled oscillator,
releasing vesicles in a regular stream at a rate determined by the presynaptic voltage. One way to
get at this issue is to measure the reliability of the synapse, and estimate from this a lower bound
on the release rate, assuming that release is Poisson. If, through other independent methods, one
can make a good estimate of the average total release rate, then one can compare the two rates.
If the rate estimated from the Poisson assumption were found to be much higher than the total
average rate, one would have a strong indication for tight control of vesicle release.

As argued in section 2.6, one can ﬁnd the equivalent contrast noise power of a cascaded system
by adding the equivalent contrast noise power of its separate elements Nc(f ) = Nc1(f ) + Nc2(f ).
What we would like to do here is to infer the equivalent contrast noise power for the synapse from
measurements of the equivalent contrast noise of the photoreceptor and the LMC. We have the
data, because the equivalent contrast noise power is just the inverse of the eﬀective Poisson rate
plotted in Fig. 7C,F: Nc(f ) = 1/λeﬀ (f ) (see section 2.6). The measurements of eﬀective Poisson
rates in photoreceptors and LMCs, together with the given number of active zones, allow us to make
an estimate of the eﬀective Poisson rate of a single synaptic contact. This cannot be done directly
because it has not been possible in practice to make a simultaneous recording from a photoreceptor
and its postsynaptic LMC in vivo, so we have to interpolate. From a large number of experiments on
diﬀerent cells at diﬀerent light levels we compute Nc(f ), and we do this separately for photoreceptors
and LMCs. We interpolate each set of curves to ﬁnd a smooth surface describing the overall behavior
as a function of both frequency and bump rate (see de Ruyter van Steveninck and Laughlin 1996a).
We estimate the synaptic contribution, NcSyn(f ) by subtracting the interpolated values obtained for
photoreceptors, divided by 6 to account for the parallel projection of 6 photoreceptors, from those
NcPR(f )/6. The diﬀerences are small, and not so easy to
describing LMCs: NcSyn(f ) = NcLMC(f )
estimate, which already indicates that the synaptic array itself cannot be much less reliable than
the photoreceptor. Here we only present data from the highest light levels used in the experiment,
mainly because there all signals are most reliable, and the eﬀect of internal noise sources is most
conspicuous.

−

Note that NcSyn(f ) describes the equivalent contrast noise of the full array of 1320 synapses in
parallel. Each synapse is driven by the same photoreceptor voltage ﬂuctuations, and each modulates
its vesicle release rate accordingly. We assume that apart from this common driving force, all
synapses release vesicles in a statistically independent way, and that all are equally eﬀective and
reliable. Then we can simply divide 1/NcSyn, as deﬁned above, by 1320 to get the eﬀective Poisson
rate for the single synapse, as argued in Sect. 2.6. This then provides a lower bound on the reliability
of a single synapse. If the assumptions mentioned here are invalid, then there must be at least one
synapse in the array that does better than this average.

Fig. 10 shows the result of the calculation, expressed as 1/Nc(f ), the equivalent Poisson rate
for one single active zone. The curve has a maximum of about 540 events per second per synaptic
zone. Unfortunately we cannot directly identify this number with the supposed vesicle release rate.
This can be done for photon ﬂux modulations because in that case we know the proportion, c(t),
by which we modulate the ﬂux: λ(t) = [1 + c(t)]
λ0. For the photoreceptor-LMC synapse, one
may describe the modulation of the release rate by a gain factor g (which may also be frequency
If g is
dependent) that converts photoreceptor voltage into vesicle ﬂux (Laughlin et al. 1987).
high, then the synapse encodes relatively reliably at a low mean vesicle rate. The price is that the

·

23

photoreceptor-LMC synapse

i

e
t
a
r
 
n
o
s
s
o
P
 
e
v
i
t
c
e
f
f
e

)
s
/
1
(

102

104

103

101

100

1

10

100

1000

frequency (Hz)

Figure 10: Eﬀective Poisson rate for a single synaptic active zone as a function of frequency, computed as described
in the text.

operating range will be small, as the rate cannot go below zero. If g is low, on the other hand, then
the operating range is large, but the reliability of transmission is relatively poor. In other words,
we measure an eﬀective rate λeﬀ , from which we wish to estimate a physical Poisson event rate.
This means we must get an estimate of g, and we must understand how g aﬀects our measurement.
To begin with the latter, if we just apply Eqn. 23 to the case of a real rate λ0 modulated by g
c,
then we would measure an eﬀective rate λg depending on g:

·

λg = λ0

g
· |

2,
|

(31)

while release would shut down at contrast values below cmin =
1/g. To keep the argument simple
we neglect the possible frequency dependence of g. That is justiﬁed here because we will not reach
precise conclusions anyway, and the frequency dependence, being rather smooth, is not likely to
aﬀect the ﬁnal result of the analysis too much. We need to ﬁnd a way to estimate the operating
range of the synapse, and we can try to get at that by estimating cmin. One hint is that LMCs are
reasonably linear when the contrast ﬂuctuations are not too large, perhaps of order 20% to 30%,
but we would like to make this a bit more precise.

−

Fig. 11 presents data suggesting that we can see the synapse shutting down. The stimulus
(Fig. 11A) is a 200 ms square wave of 95% contrast, repeated 360 times. Panel B shows the average
photoreceptor response which follows that stimulus with a bit of sag. The LMC (panel C) responds
phasically and with inverted sign. When the photoreceptor voltage makes its downward transition,
the vesicle release rate decreases, and because the neurotransmitter (histamine, Hardie 1988) opens
In panel D we plot the ﬂuctuations (8 samples) of the
chloride channels the LMC depolarizes.
LMC potential around its average waveform. These show a rather dramatic eﬀect just after the
light to dark transition. It seems that synaptic transmission is completely shut down when the
photoreceptor hyperpolarizes, and bounces back about 15 ms later, very similar to results reported
by Uusitalo et al. (1995). This is conﬁrmed by the standard deviation of the ﬂuctuation waveforms
shown in panel E. The ﬂuctuations during constant light, say from 20-100 ms and from 130-200 ms,
are due to a combination of photoreceptor noise ampliﬁed by the synapse and intrinsic noise of the
vesicle release itself. We can also add a little probe signal to the large square wave stimulus, and we
see a similar eﬀect in the gain of the synapse: The photoreceptor ﬂuctuations are not transmitted
during the same 15 ms window, as can be seen in Fig. 12. The apparent shutdown coincides with

24

e
v
i
t

l

a
e
r

y
t
i
s
n
e

t

n

i
 
t

h
g

i
l

 
r
o

t

p
e
c
e
r
o

t

o
h
P

)

V
m

(
 

e
g
a
r
e
v
a

e
g
a
r
e
v
a
 
C
M
L

)

V
m

(

i

e
s
o
n
 
C
M
L

)

V
m

(

d
r
a
d
n
a
t
s
 
C
M
L

)

V
m

(
 
n
o
i
t
a
v
e
d

i

2

1

0

5

0

10

-5

-10

20

10

0

-10

-20

-30

-5

5

0

2

1

0

A

B

C

D

E

0

50

100

150

200

time (ms)

Figure 11: Averaged responses of a photoreceptor and an LMC, and LMC ﬂuctuations in response to a large
amplitude modulation. A: The stimulus contrast sequence is a square wave of amplitude 0.95, and duration 200 ms,
sampled in 256 bins at 1280 Hz. This waveform is presented 380 times, while the response of a photoreceptor or an
LMC is recorded. B: Ensemble averaged photoreceptor response. C: Ensemble averaged LMC response. D: Example
of 8 traces showing ﬂuctuations of the LMC response around its average waveform. E: Time dependent standard
deviation of the LMC ﬂuctuation traces. The standard deviation plotted here at each instant of time is the standard
deviation across the ensemble of LMC voltage ﬂuctuations, all taken at the same phase of the square wave stimulus.

25

2

1

0

10

1

e
v
i
t

l

a
e
r

y
t
i
s
n
e

t

n

 
i
t

h
g

i
l

i

n
a
g

 
c
i
t

p
a
n
y
s

A

B

0

50

100

150

200

time (ms)

Figure 12: Characterization of the time dependent synaptic gain when the ﬂy is stimulated with a 200 ms square
wave pattern. A: Example of a combined waveform. To measure the gain of the photoreceptor and LMC along the
200 ms stimulus of Fig. 11, a small binary probe signal was added to the square wave. An example of the combined
contrast waveform is shown here. B: Gain of synaptic transmission between photoreceptor and LMC, as the system
cycles through the 200 ms period. See text for further explanation.

≈

the photoreceptor voltage being halfway between the light and dark adapted value, which in turn
is induced by an intensity drop of almost 100%. Shutdown thus seems to correspond to about 50%
2. Combined with our earlier estimate of an eﬀective rate of 540/s,
modulation, in other words, g
from Eqn. 31 we get a rate of 540/22 = 135 vesicles per second, based on the measured reliability.
This is lower than the rate of 240 per second reported by Laughlin et al. (1987), but in view of
the errors the discrepancy is not too surprising. We would like to compare the more conservative
estimate of this number with a more direct measurement of average release rate. Unfortunately
there are no conclusive data on synaptic release rates of the photoreceptor-LMC synapse in the
ﬂy. There are experimental estimates of tonic release rates for goldﬁsh retinal bipolar cells which,
like the ﬂy photoreceptor, transmit graded potentials across a chemical synapse. In a recent paper,
Lagnado et al. (1999) report 23 vesicles/s in this system, a factor of 6 lower than what we estimate
here.

The numbers we derive here are certainly not precise, and a comparison between very diﬀerent
species is always tenuous. Although no hard conclusions can be drawn, the comparison points to an
interesting possibility. The discrepancy between the high Poisson release rate required to explain
the reliability on the one hand, and the lower measured release rates in goldﬁsh on the other, is
large. The most interesting explanation for this, in our view, is that the Poisson assumption is not
valid, and that the synapse would be capable of releasing vesicles with much higher precision than
expected from that.

Of course Fig. 11 points out another interesting aspect of transmission by the synapse, namely
that it is highly adaptive. The synapse seems to reset itself to follow the large swings in the DC
component of the photoreceptor voltage, presumably to be able to encode ﬂuctuations around this
average more eﬃciently. It has been known for a long time that photoreceptor-LMC transfer is
adaptive (Laughlin and Hardie 1978, Laughlin et al. 1987), and this has been interpreted as a
resetting of parameters to optimize information transmission (van Hateren 1992, Laughlin 1989).
Here we take a look at how fast this type of adaptation takes place.

26

(cid:10)
The experiment consists of the same square wave contrast modulation as shown in Fig. 11, but
now a small amplitude random binary probe is added to the large waveform (see Fig. 12). At each
presentation of the 200 ms square wave the probe is a series of 256 binary random values, played
out at a sampling rate of 1280 Hz, and its random sequence is diﬀerent at each presentation. The
responses of the cell are recorded, also at 1280 Hz sampling rate. After the experiments we correlate
the probe stimulus with the voltage variations ∆vm(t) = vm(t)
V (t) it induces in the response,
at consecutive points along the 200 ms square wave. That is, we compute:

−

′

Φp∆v(t, t

) = lim

M →∞

1
M

M

Xm=1

pm(t)

×

′

∆vm(t

)

represents an outer product. Thus
where the m stands for the diﬀerent presentations, and
Φp∆v(t, t′) is the crosscorrelation between probe stimulus p(t) taken at time t, and voltage response
to the probe, ∆v(t′), taken at t′, which we ﬁnd by computing the ensemble averaged outer product
of the probe with the response. In practice we treat pm(t) and ∆vm(t) as vectors of 256 elements
each, spanning the 200 ms repeat period, and Φp∆v(t, t′) is then a 256
256 crosscorrelation matrix.
We make the same measurement in a photoreceptor and an LMC, and so get two crosscorrelation
matrices. As before, we regard the photoreceptor and the LMC as a cascaded system, and, as long as
things are linear, one should be able to describe the cascade as the following matrix multiplication:

×

×

Φp∆vLMC = Φp∆vPR ·

Φp∆vSyn.

Note that we do not suggest that the system is linear in the response to the large amplitude square
wave. It deﬁnitely is not. But what we try to characterize here are the small ﬂuctuations due to
the probe around the large amplitude average waveform induced by the square wave. That can be
reasonably assumed linear, but nonstationary as a result of the square wave. This nonstationarity
then naturally leads to two time indices, and they are both represented in the matrix formulation.
Ideally, from Eqn. 33 we should be able to derive the synaptic cross correlation matrix by inverting
the photoreceptor matrix, and multiplying it with the LMC matrix. Unfortunately, this procedure
is rather unstable, both because of experimental noise, and because of the strong high frequency
components in the LMC signal. Therefore we will settle for something more modest here, and
calculate the probe induced variance. The stimulus induced time dependent variance is computed
as the diagonal of the probe–response crosscorrelation matrix multiplied by its transpose:

·
and to compute the synaptic contribution we now take the ratio of the LMC and the photoreceptor
diagonals, or if we want to express the linear gain we take the square root of this quantity:

i ·

−

σ2
p∆v(t) =

Φp∆v(t, t
h

′

)

Φp∆v(t, t

′

)T

δ(t

′

t

),

(32)

(33)

(34)

(35)

σp∆vSyn(t) =

σp∆vLMC(t)
σp∆vPR(t)

,

as shown in Fig. 12B. The ﬁgure again suggests a shutdown of synaptic transmission during the
falling phase of the photoreceptor voltage. Further, the overall gain as deﬁned here switches from
about 2.5 during the bright phase to about 7 during the dim phase. The data indicate that this
switch in gain is also accompanied by a change in shape of the synaptic impulse response, which
seems to become sharper and more biphasic during the bright phase (see Juusola et al. 1995). To
some extent these eﬀects are also seen in the photoreceptor response, whose gain decreases and
speeds up. In the photoreceptor that is presumably due in large part to the change in membrane
conductance accompanying the change in membrane potential. In the case of the synapse it seems
likely that the dynamics of vesicle release changes, and this interpretation is supported by the
apparent shutdown in transmission.

27

0

50

100

150

200

time (ms)

Figure 13: Example of a 200 ms segment of spike activity recorded extracellularly from H1. The spikes shown here
are timed at 10µs precision and stored for oﬀ line analysis.

4 Coding in a Blowﬂy Motion Sensitive Neuron

Thus far we have considered the reliability and precision of phototransduction and synaptic trans-
mission, two of the ﬁrst steps in vision. Now we want to look “deeper” into the brain, to a point
where some nontrivial computations have been done. There are two very diﬀerent questions. First,
we are interested in the precision of the computation itself: Is the brain making use of all the
information available in the array of photoreceptor signals? Second, we want to understand the
way in which the brain represents the results of its computations: What is the structure of the
“neural code”? For an accessible example of these issues we turn to the visual motion sensitive
neurons in the ﬂy’s lobula plate.

The ﬂy’s lobula plate contains a number of motion sensitive cells, shown in Fig. 3, that are
direction selective and have wide visual ﬁelds. They are thought to achieve wide ﬁeld sensitivity
by adding the contributions of a large number of small ﬁeld motion sensitive cells from the ﬂy’s
medulla (Single and Borst 1998, reviewed by Laughlin 1999). One important function of these lobula
plate tangential cells is to provide input information for course control (Hausen and Wehrhahn
1983, Hausen and Egelhaaf 1989, Krapp et al. 1998). A distinct advantage of these cells in this
preparation is that they allow long and stable recording. When care is taken to do minimal damage
to the ﬂy, and to feed it regularly, the same cell can be recorded from for several days on end. This is
important in many of our studies of neural coding because there the general aim of the experiment
is to characterize probability distributions of stimulus response relations, rather than only averages.
The data we present here are all obtained by extracellular recording with conventional techniques
(see de Ruyter van Steveninck and Bialek 1995 for more experimental details). The nature of the
signal is drastically diﬀerent from what we saw before. We are now dealing with spikes, as depicted
in Fig. 13, instead of analog voltages, and one of the important issues is how we must interpret their
temporal sequences. Spikes are transmitted by the nervous system at rates of usually not more than
a few hundred per second, and certainly not at rates typical for photons entering photoreceptors.
In contrast to photons, spikes are placed on the time axis, one by one, by physiological processes.
It is therefore not unnatural to think that their position likewise could be read out and interpreted
by other physiological processes. Indeed, one long standing issue in understanding neural coding
is whether this is the case: Does the timing of individual spikes matter, or can we aﬀord to coarse
grain time and average spike counts over large windows? Here we will address that question for the
case of H1, a motion sensitive neuron in the ﬂy’s brain. It is sensitive to horizontal inward motion
(see also the legend to Fig. 3), and its visual ﬁeld covers just about a full hemisphere.

28

In most of the experiments the ﬂy watches an extended pattern generated on a display oscil-
loscope (Tektronix 608), written at a 500 Hz frame rate. One drawback of this setup is that the
stimulated area of the visual ﬁeld is only a fraction of the total ﬁeld of the cell, and that the light
levels are rather low, corresponding roughly to those at dusk. At the end of this chapter we will
present data on a ﬂy that was rotated around the vertical axis by a computer controlled stepper
motor. Doing this experiment outside in a wooded environment the ﬂy is stimulated with natural
scenes of high light intensity, and by playing a rotational motion sequence derived from measured
ﬂight data, shown in Fig. 1, one expects to get closer to the ideal of presenting naturalistic stimuli.

4.1 Retinal limitations to motion estimation

As we have seen earlier, ﬂies are quick and acrobatic, so it seems entirely reasonable to assume that
the components of the ﬂight control system are optimized to perform as accurately and quickly as
possible. This should then obviously be true for the lobula plate tangential cells as well. Of course
some general principles apply, and, like all sensory neurons they must work within the limits set
by the reliability of their input signals.

It is instructive to make a rough estimate of what precision we can expect from a wide ﬁeld cell
that takes as input photoreceptor signals with realistic amounts of noise. Let us try to compute
an estimate of the limits to timing precision of the response with respect to stimulus, as set by
the photoreceptor signal quality. This is relatively easy to do, and it is relevant in a discussion
of coding by spike timing. In many experiments we stimulate the ﬂy with a large, high contrast
bar pattern that moves randomly, jittering back and forth in the horizontal direction. The power
density spectrum of the signal we, and presumably H1, are interested in is the velocity power
density, given by Svel(f ). This has dimensions (◦/s)2/Hz, because we are dealing with angular
velocity, and this is customarily given in ◦/s.

The photoreceptors in the ﬂy’s retina have a proﬁle of angular sensitivity that is determined
by the optics of the lens and the waveguide behavior of the receptor cell itself; this proﬁle is often
1.4◦ (Smakman
approximated by a Gaussian. For the blowﬂy frontal visual ﬁeld, its halfwidth is
0.5◦. Now suppose we have a contrast
et al. 1984), corresponding to a “standard deviation” σPSF
edge with intensity stepping from I = I0
c0) aligned on the optical axis of
(1
this Gaussian point spread function. Then, if the edge moves by a small amount δx, the contrast
step in the photoreceptor is:

≈
(1 + c0) to I = I0
·

≈

−

·

δc =

δI
I0 ≈

1
I0 ·

2

c0

·

I0
·
√2π

·
σ

δx

=

·

δx

c0
·
π/2

σ

,

·

p

which converts a position change δx into a contrast change δc. This allows us to derive a contrast
power spectrum, if we can convert the velocity power spectrum into the appropriate position power
spectrum. Position is the integral of velocity, which means that we must divide the Fourier trans-
form of velocity by frequency to get the Fourier transform of the position signal (see Bracewell
1978). Here the relevant quantities are power spectra, so we must use f 2 to make the correct
conversion: Spos(f ) = Svel(f )/f 2. The contrast power density spectrum is now:

Sc(f ) =

Spos(f ) =

2

δc
δx (cid:21)

(cid:20)

·

2

c2
0 ·
·
σ2
π
·

·

Svel(f )
f 2

.

In the experiment we stimulate a large number, M , of photoreceptors and when a wide ﬁeld pattern
moves rigidly then all photoreceptors are stimulated in a coherent way. That means that the total
power of the signal available in the photoreceptor array scales as M 2. Finally, in the experiment we
control the velocity stimulus, and thus Svel(f ). All this combined leads to a contrast signal power
spectrum:

(36)

(37)

29

equivalent contrast power 
of motion signal

100

10-1

10-2

10-3

10-4

10-5

/

)
z
H
1
(
 
y
t
i
s
n
e
d

 
r
e
w
o
p

 

i

e
s
o
n

 
t
s
a
r
t

n
o
c
 
t

l

n
e
a
v
u
q
e

i

10-6

1

10

100

1000

frequency (Hz)

Figure 14: Circles: Equivalent contrast noise power spectral density of blowﬂy photoreceptors at diﬀerent light
intensities. The input light intensities, expressed in extrapolated bumps per second, are indicated by the diamonds
connected by dotted lines. Solid thick line: Equivalent contrast noise power density of a motion signal calculated for
a wide ﬁeld pattern, under conditions typical for our experiments. See text for further explanation.

ScM (f ) =

2M 2
π

c2
0 ·
σ2

Svel(f )
f 2

,

·
·

·

for the set of M photoreceptors stimulated by coherent, that is, rigid, motion.

To derive a limit of timing precision, or equivalently a limiting frequency, we must compare
this available signal spectrum to the relevant noise power spectrum. In section 2.6 we deﬁned the
equivalent contrast noise power of a single photoreceptor cell. In a pool of M photoreceptor cells,
their independent noise powers add, and we have:

To deﬁne a limit to time resolution we determine the frequency at which ScM (f )/NcM (f ), the signal
to noise ratio, crosses one:

NcM (f ) = M

Nc(f ).

·

ScM (f )
NcM (f )

=

2M 2
π

c2
0 ·
σ2

Svel(f )
f 2

·
·

·

·

M

·

1
Nc(f )

=

M

Sc(f )

·
Nc(f )

.

Sc(f ),
In Fig. 14 we therefore plot Nc(f ) at four light levels for a single photoreceptor, along with M
10(◦/s)2/Hz, and a small
for conditions typical of our experiment: c0 = 0.3, M
(
0.3) correction for the fact that the edge can not be expected to be exactly at the center (the
≈
correction factor averages over edge position). The crossover frequency, fcross of signal to noise ratio
can be read from the ﬁgure directly, and lies between approximately 150 to 400 Hz, depending on
light intensity. That means that when we stimulate H1 with the typical stimuli described here, we
fcross) in the order of one to several milliseconds
may expect to observe timing precision δt

3800, Svel(f )

1/(2π

≈

≈

·

(38)

(39)

(40)

≈

·

30

if the ﬂy’s brain eﬀectively uses all the motion information present in the photoreceptor array. We
should note that the approximation we make here is for small δx in Eqn. 36.

·

In Fig. 15 we show raster plots of a 500 ms segment of responses of H1 to a repeated dynamical
motion trace. The same experiment was done at diﬀerent light intensities, ranging over 4 orders
104 transduced photons per photoreceptor per second. It
of magnitude, from an estimated 5 to 5
is clear by visual inspection that spikes tend to line up better when the light intensity is higher,
up to the highest light intensity used in the experiment, suggesting that external noise may be the
limiting factor (see also Fermi and Reichardt 1963). That impression is conﬁrmed by the histograms
of spike arrival times to the right of the rasters, which have standard deviations ranging from 1.4
to 7.1 ms. These histograms describe the probability distributions of the ﬁrst spike that follows
t =410 ms. The highest light intensity in this experiment corresponds roughly to the second-highest
(light gray dots) intensity in the photoreceptor data of Fig. 14. At ﬁve per second, the lowest bump
rate shown here, bumps in a single photoreceptor typically are nonoverlapping, and we see that
there is still a modulation of the response of H1. Dubs et al. (1981) showed that ﬂies respond
behaviorally to moving patterns at light levels where single photon absorptions are nonoverlapping,
and from the classical work of Hecht et al. (1942) we know that humans can perceive light ﬂashes
under these conditions.

One can think of other parameters likely to aﬀect the quality of the input signal for a wide
ﬁeld motion sensitive cell, for example contrast, ﬁeld of view, and stimulus velocity amplitude. In
experiments where these parameters are varied we see eﬀects on spike jitter qualitatively similar
to what is shown here. It thus seems that the precision of spike timing in H1 is close to being
determined by the information available to it in the photoreceptor array, i.e., bump latency jitter
sets the threshold under the photon capture rate conditions of the experiments in Fig. 15. From
Fig. 14 we can also read that the photoreceptor equivalent contrast noise has a very steep frequency
dependence at high frequencies. This means that once the conditions of the experiment are such
that the output accuracy is in the millisecond regime, only relatively large changes in stimulus
parameters will lead to appreciable improvements in timing precision.

We can ask a related question, namely, how reliably can the arrival time of a spike tell us
something about the strength of the stimulus that preceded it, and how close does that get to the
photoreceptor limits. If we know what message the neuron encodes, we can use a computational
model that retrieves that message from the sensory periphery. The model relevant for H1 is the
Reichardt correlator model (Reichardt 1961, Reichardt and Poggio 1976), which describes a speciﬁc
functional computation for extracting motion information from an array of photoreceptors. Its basic
interaction is a multiplication of ﬁltered signals originating from neighboring directions of view. The
Reichardt model was formulated heuristically, but it has been shown to be the optimal solution to
a general problem of motion estimation in the presence of noise, in the limit where the signal to
noise ratio is low (Potters and Bialek 1994).

The experiment to measure reliability is very simple (de Ruyter van Steveninck and Bialek
1995). We present the ﬂy with a pattern that makes sudden motion steps of several sizes, and
record the responses of H1. From a large number of presentations we obtain a histogram of arrival
times of the ﬁrst and the second spike following the stimulus. If we compare two such histograms
in response to two diﬀerent step sizes we can compute the discriminability of those two stimuli as
a function of time. This is done by framing the question as a decision problem (Green and Swets
1966). A stimulus is presented once, and the cell generates a response which is observed by some
hypothetical observer. The question then is: Given the cell’s response, what can the observer say
about the identity of the stimulus, and how often is that assessment right? This is meaningful
only if the distributions of responses to diﬀerent stimuli are known to the observer, and are also
diﬀerent. In the analysis presented here, the hypothetical observer can judge the spike train in
real time, starting at the moment of stimulation (or rather 15 ms after that, to minimize eﬀects of
spontaneous rate). The assumption that the time of stimulus presentation is known does of course

31

Figure 15: Responses of a blowﬂy H1 neuron to movement of a wide ﬁeld pattern at diﬀerent mean light levels. In
this experiment the ﬂy looked at a moving pattern through a round diaphragm with a diameter corresponding to 30
horizontal interommatidial angles. The velocity was random with a ﬂat power spectral density. A: A 500 ms sample
segment of the stimulus velocity. B: Raster plots of H1 spikes obtained at ﬁve diﬀerent light intensities given by the
estimated average photon ﬂux for each single photoreceptor. C: Histograms of the timing of the ﬁrst spike ﬁred after
t = 410 ms. With decreasing photon ﬂux the response latency increases. Moreover, the peaks become wider (with σ
the standard deviation of a ﬁtted Gaussian), suggesting that timing precision may be limited by photoreceptor noise.
At the lowest light level shown here there is still a visible modulation of H1’s rate, but the timing of a single event
is too spread out to produce a clear peak.

32

not correspond to any natural situation, as there the timing of stimuli must be inferred from the
sensory input as well. It is, however, still a valid characterization of the precision with which the
ﬂy’s visual brain performs a computation.

0.78 + 0.5

S = 0.24◦) and p(τ0
|

Fig. 16a shows two conditional distributions, p(τ0

S = 0.36◦) for the ﬁrst
|
spike arrival after stimulus presentation, for two step stimuli of diﬀerent size. For the large step,
the ﬁrst spike tends to come earlier than for the small step. This means that the observer should
choose the large step when he or she sees a short interval, and the small step for a relatively large
interval. The crossover for this case is at about 32 ms. How accurate will this judgement be on
average? As can be read from the cumulative probability distributions in Fig. 16B, the probability
for having a spike before 32 ms with the large step is 0.78. Thus if the large step was presented
then the observer will make the right choice with a probability of 0.78. The cumulative distribution
for the small step at 32 ms is 0.48, which means that if the small step was presented, the observer
will choose correctly in a proportion of 1
0.48 = 0.52. If each step has a prior chance of 0.5,
then in an experiment where the two steps would be mixed at random, the proportion of correct
decisions would be PC = 0.5
0.52 = 0.65. It is convenient to translate PC into a distance
measure, and Green and Swets (1966) propose to use the distance between Gaussian distributions
of unit standard deviation that would give rise to the same value of PC . This measure is known as
d′ and is very widely used in the psychophysical literature. There is a simple 1:1 correspondence
between d′ and PC, and for our case we ﬁnd d′ = 0.68. From the given spike timing distributions
we can also construct a continuous function d′(tobs) that describes the equivalent distance as it
evolves over the observation time interval since stimulus presentation. To see that, we should
simply divide the distribution in a part that is described by the measured distribution up to tobs,
assign the probability that is as yet “unused” to one total remaining probability, and then treat
this constructed distribution in the same way as the previously deﬁned spike timing distributions.
The time dependent d′(t) based on the ﬁrst spike only is shown as the solid line in Fig. 16C. This
line plateaus at t = 32 ms, as from that moment on the choice will always be ﬁxed. Instead of
considering only the ﬁrst spike we can also look at the combination of the ﬁrst and the second spike
arrival time, that is
. The distributions for these are not shown, but the reasoning is entirely
}
similar. The end result, d′ as a function of time after presentation, is shown in the same panel as
the dashed line. It is clear in this case that the second spike carries substantial extra information
about stimulus identity.

τ0, τ1
{

−

·

·

The comparison of the measured data to the ideal motion detector model can now be made. We
measure representative photoreceptor power spectral densities, and the number of photoreceptors
stimulated in the H1 experiment, and then apply the Reichardt model to compute its average
step response as well as its output power spectral density (de Ruyter van Steveninck and Bialek
1995). From these we compute a time dependent d′
model(t), which is plotted as the dash-dot line in
Fig. 16C. The crucial comparison is between the slope of the measured and the computed d′(t). In
model(t) rises about twice as fast as the measured d′(t). This shows that H1
the range of 23-28 ms, d′
approaches, within a factor of two, the performance of an optimal motion detector limited only by
noise in the photoreceptor array. Given that the signal passes through at least four synapses to be
computed, this precision is quite remarkable. In this case we measure how accurately H1 represents
the amplitude of motion steps. The estimation takes place over a somewhat extended time interval,
and is therefore not limited by the bump jitter that sets the timing resolution of the photoreceptor
array, but rather by low frequency (roughly below 100 Hz) accuracy of the photoreceptor. As this
latter is close to the photon shot noise limit (Figs. 7,14) we are reminded that the precision of
neurons in a functioning brain is not just given by the physiology, but is determined in part, or in
this case maybe even dominated, by the statistical properties of the stimulus (see also Bialek et al.
1991, de Ruyter van Steveninck and Bialek 1995, Rieke et al. 1997).

Motion discrimination using the spike train output of H1 thus provides us with an example in
which the performance of the nervous system approaches basic physical limits set by the structure

33

measured distributions

distributions from Poisson (cid:10)mo del

100

O

0.36

100

A

)
s
/
1
(
 
e
t
a
r
 
g
n
i
r
i
f

100

50

0

O

0.24

O

0.36

0

100

time (ms)

O

0.24

ideal observer

ideal observer

D

E

F

{τ0,τ1}

τ0

0.78

1
-

s
 
/
 
)

0

τ

(
p

)

0

τ

(

P

)
’
d
(
 
y
t
i
l
i

b
a
n
m

i

i
r
c
s
d

i

50

0

1.0

0.5

0.0

1.5

1.0

0.5

0.0

20

30

40

50

60

20

30

40

50

60

time  (ms)

time  (ms)

◦
(thick line) and a 0.36

Figure 16: Statistics of H1’s responses to small motion steps of a wide ﬁeld pattern, and equivalent Poisson statistics.
A: Histograms, normalized as probability densities of the ﬁrst spike ﬁred starting 15 ms after the stimulus step, for a
◦
(thin line) step. B: Cumulative distributions for the same data as in panel A. C: Time
0.24
dependent discriminability parameter, d
(t), computed for the step size pair depicted in A, both for single spikes
(τ0), and for spike pairs ({τ0, τ1}). Also shown is the discriminability computed for an ideal observer using realistic
photoreceptor signals as inputs. D: First spike histograms, as in A, but now for a modulated Poisson process. These
data were computed using the measured PSTHs (inset in D) for the two step sizes as the rate of the underlying
modulated Poisson process. E and F: As B and C, but now for the modulated Poisson case.

′

50

0

1.0

0.5

0.0

1.5

1.0

0.5

0.0

B

C

{τ0,τ1}

τ0

34

of the inputs in the retina. There are other examples of this near optimal performance, in systems
ranging from human vision to bat echolocation to spider thermoreception (for discussion see Rieke
et al. 1997). This level of performance requires the nervous system to meet two very diﬀerent re-
quirements. First, the system must be suﬃciently reliable or “quiet” that it does not add signiﬁcant
excess noise. This is especially challenging as the signals propagate through more and more layers
of neurons, since the synapses between cells are sometimes observed to be the noisiest components.
Second, optimal performance requires that the system make use of very particular algorithms that
provide maximal separation of the interesting feature (motion, in the case of H1) from the back-
ground of noise. In general these computations must be nonlinear and adaptive, and the theory of
optimal signal processing (Potters and Bialek 1994) makes predictions about the nature of these
nonlinearities and adaptation that can be tested in independent experiments.

In our broader discussion on the relevance of Poisson ﬁring it is now interesting to quantify to
what extent deviations from Poisson behavior help in encoding the stimulus in a spike train. To
get an idea of this we do exactly the same analysis as described above, but then on synthetic spike
trains that are generated by a modulated Poisson process with the same time dependent rate as
the measured spike train. The inset in Fig. 16D shows the post stimulus time histogram for the
two step sizes used before in the analysis. From these two, the spike arrival distributions in panel
d are computed, and from these again we construct d′(t). Figure 16F shows the end result: The
discriminability based on timing of the ﬁrst spike alone goes from d′ = 0.68 to d′ = 0.46, while in
the Poisson case the second spike adds only 0.1 to the d′ based on the ﬁrst, so that at 50 ms after the
step, the Poisson value for d′ is less than half that measured from real spikes. Neural refractoriness
was not incorporated in the synthetic train, and it seems likely that that the increased reliability
of the real neuron can at least be partly attributed to that.

4.2 Taking the ﬂy outside: Counting and timing precision in response to natural

stimuli

There is a long tradition of using discrimination tasks, as in the step discrimination experiment
of the previous section, to probe the reliability of perception in humans and also the reliability of
neurons. But such simple tasks are far from the natural ones for which evolution selected these
neurons. As a ﬂy ﬂies through the world, angular velocity varies continuously, and this variation has
a complicated dynamics. In the past decade, a number of experiments has been done which attempt
to approach these more natural conditions. Speciﬁcally, experiments with pseudorandom velocity
waveforms presented on display oscilloscopes have been used to measure information transmission
in H1 by reconstructing the stimulus from the spike train (de Ruyter van Steveninck and Bialek
1988, Bialek et al. 1991, Haag and Borst 1997), and by more direct methods (de Ruyter van
Steveninck et al. 1997, Strong et al. 1998). The general conclusion of these studies is that the
timing of spikes in the millisecond range does indeed carry signiﬁcant information, in line with the
photoreceptor limits discussed earlier. One would like to know to what extent such conclusions are
also relevant to still more natural conditions encountered in ﬂy ﬂight (see for example Warzecha et
al. 1998).

Ideally one would perform experiments in the natural habitat of the animal, while it is behaving
as freely as possible. Good examples are the study of responses of auditory neurons in Noctuid
moths to the cries of bats ﬂying overhead (Roeder 1998), and the study of optic nerve responses
of Limulus lateral eye while the animal is moving under water (Passaglia et al. 1997). Of course,
in each speciﬁc case concessions are made to be able to record neural signals, and one must decide
what is the best compromise between realistic conditions and getting interpretable data.

Here we present data from a setup that allows us to record from a ﬂy while it is rotating on a
stepper motor. The rotational motion is mechanically precise, and arbitrary rotation sequences can
be programmed. In the case described here the ﬂy was rotated with a time sequence corresponding
to the rotations executed by the ﬂies shown in Fig. 1 (based on Land and Collet 1974), except that

35

for stability reasons the amplitude of the entire trace was set to half the value measured from the
real ﬂies. The ﬂight trajectories were repeated with their sign inverted, so that for each trajectory
we stimulate H1 in complementary ways. It is thus as if we record from the two H1 cells at opposite
sides of the head. The setup is portable and the experiment was done outside in the shade of some
bushes on a sunny afternoon. Therefore, the light intensities, the stimulated area of the visual ﬁeld,
and the spatial characteristics of the scene are realistic samples of what the animal encounters in
nature. The motion trace is somewhat natural, although the rotational velocities are smaller than
those measured in free ﬂight, and the measurement was done on a diﬀerent species of ﬂy.

Important for our analysis is that we can repeat the same motion trace a large number of times,
which is of course not really a part of natural behavior. However, it allows us to make quantitative
statements about information transmission in the measured spike trains that rely only on the
degree of reproducibility of the response to repeated stimuli. Because of this, those statements are
independent of any assumptions on how the stimulus is encoded in the spike train, so in that sense
they are rather universal.

Fig. 17 presents data from such an experiment. The rotational velocity waveform is shown in
panel A. Note that the velocity amplitudes are very large compared to those of the white noise
stimuli shown in Fig. 15. The traces in panel B are labeled H1+ and H1
. In reality they were
obtained from the same H1 cell, but with a switch of sign in the velocity trace, as described earlier.
alternate their activity quite precisely, and that repeateable patterns
It is clear that H1+ and H1
of ﬁring occur, such as the pair of spikes in H1+ at about 1900 ms. The edges in spike activity are
sharp. Panel C shows the position of the ﬁrst spike occurring after 2085 ms in H1
. A histogram
of arrival times of this spike is shown in Fig. 17E, together with a Gaussian ﬁt with standard
deviation 0.73 ms.

−

−

−

In addition to this precision of spike timing with reference to the stimulus, there can also be
an internal reference, so that the relative timing of two or more spikes, either from one neuron or
among diﬀerent neurons, may carry information (MacKay and McCulloch, 1952). Panel D and F
give an example, where H1 generates a 2 ms interspike interval upon a particularly strong stimulus
with a standard deviation of 0.18 ms.

Consequently, interspike intervals may act as special symbols in the code, carrying much more
information about the stimulus than what is conveyed by two single spikes in isolation. This was
shown indeed to hold for H1 (de Ruyter van Steveninck and Bialek 1988, Brenner et al. 2000).
Findings like these should make us cautious. For a complete description of the spike train, timing
precision at diﬀerent levels of resolution may be required, depending on what aspect of the spike
train we are talking about. In particular, relative spike timing may have to be much better resolved
than absolute timing to recover neural information (see speciﬁcally Brenner et al. 2000).

The data presented above show episodes in the stimulus that induce accurately timed events in
the spike train, on the millisecond scale. One may worry that such events are very special, and that
most spikes are not well deﬁned on the time axis. In other words, we need a “bulk” measure of spike
timing precision. The most general way of specifying that is to study the information transmitted
by the spike train as a function of time resolution ∆t. We do that here by estimating two measures
of entropy, the total entropy and the noise entropy, directly from the spike train (cf. de Ruyter
van Steveninck et al. 1997, Strong et al. 1998). Loosely speaking, the total entropy measures the
size of the neuron’s “vocabulary.” We calculate it from the distribution, P (W ), of neural ﬁring
patterns, or words W : Stotal =
ΣW P (W ) log2[P (W )]. Here W is a vector of nW entries, and each
entry gives a spike count in a bin of size ∆t. All nW bins taken together form a string of length T .
Typical values for ∆t are one to a few ms, while T is of order 5-30 ms. P (W ) is approximated by
the histogram of all ﬁring patterns that the neuron generates in the experiment. The noise entropy
characterizes how much the neuron deviates from repeating the same ﬁring pattern at the same
phase of a repeated stimulus. If the stimulus is periodic in time with period Tstim, then from an
experiment with a large number of repetitions we can form histograms of ﬁring patterns W (t) at

−

36

Figure 17: Direct observations of H1 spike timing statistics in response to rotational motion derived from Land and
Collett (1974) free ﬂight data (see Fig. 1). The ﬂy was immobilized in a specially designed miniature recording setup,
which was ﬁxed to a computer controlled stepper motor. This setup was used in a wooded outdoor environment, in
which the ﬂy was rotated repeatedly (200 times in total) along the same motion trajectory. For technical reasons the
rotational velocity used in this experiment was scaled down to half of the free ﬂight value. A: A 500 ms segment of
the motion trace used in the experiment. B: Top: raster of 25 trials showing occurrences measured from H1. Bottom:
25 trials with spike occurrences from the same cell, but in response to a velocity trace that was the negative of the one
shown in A. For ease of reference we call these conditions H1+ and H1- respectively. C: 25 samples of the occurrence
time of the ﬁrst spike ﬁred by H1- following t=2085 ms (indicated by the arrow connecting the axis of panel B to
panel C). D: Time interval from the spike shown in C to the spike immediately following it. E: Probability density
for the timing of the spike shown in C. The spread is characterized by σ = 0.73 ms, which is deﬁned here as half
the width of the peak containing the central 68.3% of the total probability. If the distribution were Gaussian, then
this would be equivalent to the standard deviation. Here we prefer this deﬁnition instead of one based on computing
second moments. The motivation is that there can be an occasional extra spike, or a skipped spike, giving a large
37
outlier which has a disproportionate eﬀect on the width if it is calculated from the second moment. Filled squares

∆t=2ms

B

total entropy
noise entropy
information 

400

300

200

100

)
s
/
s
t
i

b
(
 

e

t

a
r
 
y
p
o
r
t

n
e

0

0

A

)
s
/
s
t
i

b
(
 

e

t

a
r
 
y
p
o
r
t

n
e

400

300

200

100

0

50

100

150

1

2

3

4

1/T (s-1)

time resolution (ms)

Figure 18: Information in ﬁring patterns obtained from an experiment with naturalistic motion stimuli (see legends
for Figs. 1, 17). A: Rate of total entropy and noise entropy as a function of 1/T , for time resolution ∆t = 2 ms. The
ﬁgure shows the corresponding entropy rates, that is the values of total and noise entropy as deﬁned above, divided by
T . The ﬁts (solid lines) to the two data sets are extrapolated to zero value of the abscissa, corresponding to T → ∞.
The diﬀerence of the extrapolated rate for the total entropy and the noise entropy respectively, is the estimate of the
information rate at the given time resolution. See Strong et al. (1998) for a more detailed explanation. B: Rates of
total entropy, noise entropy and information transmission were computed as explained in the text, and plotted here
for diﬀerent values of time resolution. It is clear that even in the millisecond range the information transmission
increases when time resolution becomes ﬁner.

t

≤

≤

each time t, 0
Tstim. If the neuron were an ideal noiseless encoder then the response would
be the same every time, and for each presentation we would ﬁnd the same ﬁring pattern W (t) at
time t. Then P (W
t) would equal one for W = W (t), and zero otherwise, so that the noise entropy
|
would be zero. In practice, of course, the noise entropy diﬀers from zero, and it will also vary with
t)].
time t. The time average of the noise entropy is Snoise = Snoise(t) =
|
The information transmitted by the neuron is the diﬀerence of the total and noise entropies, and
Snoise (Strong et al. 1998). With enough
therefore depends on both T and ∆t: I(T, ∆t) = Stotal
data we can extrapolate to the limit T
If we
quantify that limiting information rate as a function of the time resolution ∆t, we ﬁnally arrive at
a reasonable bulk measure of the time resolution at which information can be read out from the
neuron.

to get an estimate of the information rate.

t) log2[P (W
|

ΣW P (W

→ ∞

−

−

≥

The results are shown in Fig. 18B, which plots the total entropy rate, the noise entropy rate,
and the information rate, all as a function of time resolution. The information rate still increases
going from ∆t = 2 ms to ∆t = 1 ms, to reach 120 bits per second. The eﬃciency of encoding,
that is the proportion of the total entropy used for transmitting information, is about 50% for
2 ms, and slightly lower than that for ∆t = 1 ms. Because the information transmission rate
∆t
must be ﬁnite, and the total entropy grows without bound as ∆t becomes smaller, the eﬃciency
must go to zero asymptotically as ∆t goes to zero. Due to the limited size of the dataset it is
not possible to make hard statements about the information transmitted, and thus the eﬃciency,
at time resolutions better than 1 ms. It is clear, however, that spike timing information in the
millisecond range, also under natural stimulus conditions, is present in the spike train, and that it
could be highly relevant to the ﬂy for getting around.

In the spirit of the discussion in section 4.1 we can also ask whether under these more natu-
ral conditions, spikes are generated according to a modulated Poisson process. As mentioned in
section 2.1, the variance of a Poisson distribution is equal to its mean. Furthermore, if spikes are

38

4

3

2

1

0

0.5

0.4

0.3

0.2

0.1

0.0

1.0

0.8

0.6

0.4

0.2

0.0

t
n
u
o
c
 
n
a
e
m

e
c
n
a
i
r
a
v
 
t
n
u
o
c

t
n
u
o
c
 
n
a
e
m

 
/
 
e
c
n
a
i
r
a
v
 
t
n
u
o
c

measured
Poisson

d

0

1

2

3

mean count

e
c
n
a
i
r
a
v
 
t
n
u
o
c

3

2

1

0

a

b

c

39

4200

4300

4400

4500

4600

4700

time (ms)

Figure 19: Example of counting statistics in response to a natural motion stimulus. The data are for the segment
represented as H1+ in Fig. 17B. A: Ensemble average count in a 10 ms wide sliding windows. B: Ensemble variance
of the count in the same sliding window as in A. C: Ratio of ensemble variance to ensemble average. Where both
the variance and the mean are zero the value of the ratio is set to 1. For a Poisson process, all datapoints in this
plot should have a value of 1. D: Dots: Scatter plot of simultaneous pairs of the ensemble variance and the ensemble
average count. Straight line: Statistics of a Poisson process.

generated according to a modulated Poisson process, the spike count in a certain window should
spread according to a Poisson distribution. Thus, if we compare segments of the responses to
repeated identical stimuli, and we compare the variance of the count in a large number of such
segments, we can see whether the spike statistics deviate from Poisson (for more details see de
Ruyter van Steveninck et al. 1997). In Fig. 19 we show the response segment labeled H1+ in Fig.
17B, and we compute both the mean count and the variance across trials, for a 10 ms wide sliding
window. When we plot the ratio of variance to mean we see that, as soon as there is spike activity,
the ratio drops to values between 0.1 and 0.2. The comparison of the scatter plot to the Poisson
behavior in Fig. 19D makes it clear that there is no strong overall trend for the variance to scale
with the mean for these conditions (this, however, may be diﬀerent for much longer time windows,
see Teich and Khanna 1985). Similar results were reported for other systems (Berry et al. 1997,
Meister and Berry 1999) and for ﬂy H1 in laboratory conditions (de Ruyter van Steveninck et al.
1997).

These results suggest that when stimuli are dynamic enough, spiking sensory neurons may op-
erate in a regime far removed from the Poissonlike behavior they are often assumed to have. This
sub-Poisson behavior can be attributed, at least partly, to the relative refractoriness of spiking
cells. Refractoriness tends to regularize spike trains (Hagiwara 1954), and to increase the informa-
tion carried by short intervals (de Ruyter van Steveninck and Bialek 1988, Brenner et al. 2000).
Therefore, when operating at the same time dependent rate, the real H1 neuron carries much more
information about the stimulus than a modulated Poisson train (de Ruyter van Steveninck et al.
1997).

5 Discussion and Conclusions

As the ﬂy moves through its environment, its array of photoreceptors contains an implicit represen-
tation of motion, and this representation is corrupted by noise. ¿From this the visual brain must
extract a time dependent estimate of motion in real time. In some limiting cases, such as stepwise
motion and the high frequency limit of white noise motion (cf. section 4.1), we can estimate the
limits to the precision with which such a running estimate can be made. In those cases we ﬁnd
that the computation performed by the ﬂy’s brain up to H1 is eﬃcient in the sense that that H1
retreives a substantial fraction of the motion information implicitly present in the photoreceptor
array. To support those conclusions we have presented data on the statistical eﬃciency of signals
at several levels within the blowﬂy’s visual system.

Photoreceptors are stimulated by a Poisson stream of photons and, as long as the modulation is
not too strong, they respond linearly to contrast. At low frequencies (up to 50-100 Hz, depending
on illumination) the signal to noise ratio with which they encode contrast is close to the limits
imposed by the Poisson nature of photon capture. Thus, blowﬂy photoreceptors are eﬃcient at
frequencies up to 50-100 Hz in the sense that they use almost all the information present in the
photon stream. At higher frequencies the photoreceptor loses eﬃciency as latency jitter becomes
the dominating noise source.

The LMCs, directly postsynaptic to the photoreceptors, are also eﬃcient (Laughlin et al. 1987;
this chapter, section 3.4). This means that neural superposition indeed works as was suggested long
ago by Braitenberg (1967) and Kirschfeld (1967). As the LMCs receive their signals through an
array of chemical synapses, the measured signal to noise ratio of the LMC sets a lower bound to the
precision with which these synapses operate. If we hypothesize that vesicle release can be thought
of as a modulated Poisson process, then from the measurements we estimate that each synaptic
active zone should emit vesicles at a tonic rate of well over a hundred vesicles per second. Based on
similar considerations Laughlin et al. (1987) arrive at an even higher number. Direct measurements
of vesicle release in this system are unavailable, but recent measurements in goldﬁsh bipolar cells
(Lagnado et al. 1999) give a value six times lower than our estimates of tonic rates. This suggests

40

that the hypothesis of Poisson release could be wrong, and that vesicle release is much more tightly
regulated. Perhaps the large number of diﬀerent proteins involved in vesicle docking and release
(Kuno 1995), and the delicate anatomical ultrastructure of the synapse (Nicol and Meinertzhagen
1982, Meinertzhagen and Fr¨olich 1983) have a role to play in this type of regulation.

We ﬁnd that H1 can generate spikes with strongly subPoisson statistics, if it is driven by stimuli
representative for at least some of the natural behavior of the ﬂy. Chasing behavior in ﬂies plays an
important role in reproduction and territorial defense (Land and Collett 1974) and it presumably
taxes the ﬂy’s sensory and motor systems to the fullest. It is therefore an interesting limit in which
to study the performance of the ﬂy’s visual information processing capabilities. However, ﬂies do
not often engage in this behavior, and the chases typically last not much longer than a second. One
may therefore ask how long the visual system can keep up with these strong dynamic stimuli. As
a casual observation, it seems that H1 keeps on reporting about the ﬂy’s motion, and we see no
signs of habituation when the ﬂy spins around in simulated ﬂight patterns for up to 20 minutes.

A ﬂy, its name notwithstanding, spends most of its time sitting still (Dethier 1976, on page 13
gives a revealing list of how ﬂies spend their time). If the ﬂy’s environment does not move, or if
movement is steady and slow enough, H1 generates spikes approximately as a Poisson process. But
at the same mean ﬁring rate, when stimulated with strong dynamic signals, H1 can be ﬁring far
from Poisson, and this makes the encoding more eﬃcient in the sense that the information per spike
increases substantially (de Ruyter van Steveninck et al. 1997). Over the years the Poisson process
has been the model of choice for describing neural ﬁring statistics, at least to ﬁrst approximation
(see for example Tolhurst et al. 1983, Britten et al. 1993). As H1 can be either close to that limit
or far from it, depending on conditions, it is a matter of debate which condition is more relevant.
Reference to natural behavior is not conclusive here, because both sitting still and chasing other
ﬂies are natural behaviors. We feel that studying neural responses to dynamic stimuli is more
interesting and rewarding, both because there is already a long history of characterizing responses
to static stimuli, and because one can reasonably assume that well designed dynamic stimuli test
the information processing capabilities of the nervous system to the fullest.

From the experiment it has also become clear that H1 can generate spikes that are locked to
certain stimulus events with millisecond timing precision. Moreover, interspike intervals can be
0.18 ms
deﬁned even better, and we saw an example in which an interval was generated with
accuracy. Overall, the spike train carries information at
50% eﬃciency at least down to the
millisecond time scale.

≈

≈

At the very least, the combination of these observations should make one cautious in interpreting
or modeling neural signals as modulated Poisson processes. As we saw, under some conditions this
may be a fair approximation but in others, speciﬁcally those that approach conditions of natural
ﬂight, it deﬁnitely is not. One may wonder whether this latter observation is more generally valid.
Here the report by Gur et al. (1997) oﬀers an important clue: The authors studied cells in monkey
primary visual cortex, and noticed that these showed marked sub-Poisson statistics when care was
taken to exclude episodes with eye movements from the analysis. Thus, from the point of view of
specifying neural reproducibility, eye movements add variability. But that of course is by no means
necessarily true for the monkey if it knows when it holds its eyes still. It is thus quite possible that
sub-Poisson ﬁring is a more general phenomenon, and relevant to natural stimulus conditions; this
could be revealed in experiments designed to control carefully for variability in the neural response
that is knowable by the animal.

We have encountered an example of this in the H1 data presented in section 4.2. Fig. 17C shows
the arrival times of spikes with reference to the stimulus, and these jitter by 0.73 ms. Fig. 17D
shows the jitter in the interspike interval beginning with the spike depicted in panel C, which at
0.18 ms is much tighter. This implies that the ﬂuctuations in the absolute timing of the two spikes
are strongly correlated. If the interval length plays a role in the interpretation of H1 somewhere
downstream, then in a sense the ﬂy corrects for the ﬂuctuations by knowing that they are correlated.

41

It would even be more interesting if ﬂuctuations correlated among diﬀerent neurons, as that would
mean that the relative timing between spikes from diﬀerent cells may carry extra information.
Preliminary data from double recording experiments indicate that this is indeed the case for the
two H1 neurons in the left and right lobula plates, again under approximately natural stimulus
conditions.

Acknowledgments

The work presented here grew out of collaborations and discussions with many people, and it
is our great pleasure to thank Naama Brenner, Simon Laughlin, Geoﬀ Lewen, Roland K¨oberle,
Al Schweitzer, and Steve Strong for sharing their insights and helping us out. We thank Simon
Laughlin in particular for his thoughtful comments.

42

References

[1] Barlow HB (1952): The size of ommatidia in apposition eyes. J Exp Biol 29, 667-674.

[2] Baylor DA, Lamb TD, Yau KW (1979): Responses of retinal rods to single photons, J Physiol

(Lond) 288, 613-634.

[3] Berenbaum MR (1995): Bugs in the System: Insects and Their Impact on Human Aﬀairs.

Addison-Wesley Publishing Co., Reading MA.

[4] Berry M, Warland DK, Meister M (1997): The structure and precision of retinal spike trains.

Proc Natl Acad Sci USA 94-10, 5411-5416.

[5] Bialek W, Rieke F, de Ruyter van Steveninck RR, Warland D (1991): Reading a neural code.

Science 252, 1854-1857.

York NY.

[6] Bracewell RN (1978): The Fourier Transform and its Applications, 2nd ed. McGraw-Hill, New

[7] Braitenberg V (1967) Patterns of projection in the visual system of the ﬂy. I. Retina-lamina

projections. Exp Brain Res 3, 271-298.

[8] Brenner N, Strong SP, Koberle R, Bialek W, de Ruyter van Steveninck RR (2000) Synergy in

a neural code. Neural Comp 12, 1531-1552.

[9] Britten KH, Shadlen MH, Newsome WT, Movshon JA (1993) The responses of MT neurons

to variable strength stochastic motion stimuli. Visual Neurosci 10, 1157-1169.

[10] Bullock TH (1970): The reliability of neurons. J Gen Physiol 55, 565-584.

[11] Dethier VG (1976): The Hungry Fly: A Physiological Study of the Behavior Associated with

Feeding. Harvard University Press, Cambridge MA.

[12] Dodge FA, Knight BW, Toyoda J (1968): Voltage noise in Limulus visual cells. Science 160,

88-90.

[13] Dubs A, Laughlin SB, Srinivasan MV (1981): Single photon signals in ﬂy photoreceptors and

ﬁrst order interneurones at behavioural threshold. J Physiol 317, 317-334.

[14] Exner S (1891): Die Physiologie der facettirten Augen von Krebsen und Insekten. Verlag
Franz Deuticke, Vienna. Reprinted as: The Physiology of the Compound Eyes of Insects and
Crustaceans. Springer Verlag, Heidelberg (1989).

[15] Feller W (1966): An Introduction to Probability Theory and its Applications, vol I. John Wiley,

New York NY.

Reading MA.

[16] Fermi G, Reichardt W (1963): Optomotorische Reaktionen de Fliege Musca Domestica.
Abh¨angigkeit der Reaktion von der Wellenl¨ange, der Geschwindigkeit, dem Kontrast, und
der mittleren Leuchtdichte bewegter periodischer Muster. Kybernetik 2, 15-28.

[17] Feynman RP, Leighton R, Sands M (1963): The Feynman Lectures in Physics. Addison-Wesley,

[18] Franceschini N, Kirschfeld K (1976): Le contrˆole automatique du ﬂux lumineux dans l’oeil
compos´e des Dipt`eres. Propri´et´es spectrales, statiques et dynamiques du m´ecanisme. Biol
Cybern 31, 181-203.

43

[19] French AS, J¨arvilehto M (1978): The transmission of information by ﬁrst and second order

neurons in the ﬂy visual system. J Comp Physiol 126, 87-96.

[20] Fuortes MGF, Yeandle S (1964): Probability of occurrence of discrete potential waves in the

eye of Limulus. J Gen Physiol 47, 443-463.

[21] Green DM, Swets JA (1966): Signal Detection Theory and Psychophysics. John Wiley, New

York NY.

4, 234-240.

[22] Gur M, Beylin A, Snodderly DM (1997): Response variability of neurons in primary visual

cortex (V1) of alert monkeys. J Neurosci 17, 2914-2920.

[23] Haag J, Borst A (1997): Encoding of visual motion information and reliability in spiking and

graded potential neurons. J Neurosci 17, 4809-4819.

[24] Hagiwara S (1954): Analysis of interval ﬂuctuations of the sensory nerve impulse. Jpn J Physiol

[25] Hardie RC (1985): Functional organization of the ﬂy retina. In: Ottoson D (ed) Progress in

Sensory Physiology 5. Springer, Berlin, Heidelberg, New York, pp 2-79.

[26] Hardie RC (1988): Eﬀects of antagonists on putative histamine receptors in the ﬁrst visual

neuropile of the houseﬂy Musca domestica. J Exp Biol 138, 221-241.

[27] van Hateren JH (1984): Waveguide theory applied to optically measured angular sensitivities

of ﬂy photoreceptors. J Comp Physiol A 154, 761-771.

[28] van Hateren JH (1992): Real and optimal neural images in early vision. Nature 360, 68-70.

[29] van Hateren JH (1997): Processing of natural time-series of intensities by the visual system of

the blowﬂy. Vision Res 37, 3407-3416.

[30] Hausen K (1981): Monocular and binocular computation of motion in the lobula plate of the

ﬂy, Verh Dtsch Zool Ges 49-70.

[31] Hausen K, Wehrhahn C (1983): Microsurgical lesion of horizontal cells changes optomotor yaw

responses in the blowﬂy Calliphora erythrocephala. Proc R Soc Lond B 219, 211-216.

[32] Hausen K, Egelhaaf M (1989): Neural mechanisms of visual course control in insects. In:

Stavenga DG, Hardie RC (eds) Facets of Vision. Springer, New York, pp 391-424.

[33] Hecht S, Shlaer S, Pirenne MH (1942): Energy, quanta, and vision. J Gen Physiol 25, 819-840.

[34] Howard J (1983) Variations in the voltage response to single quanta of light in the photore-

ceptors of Locusta migratoria. Biophys Struct Mech 9, 341-348.

[35] Howard J, Blakeslee B, Laughlin SB (1987): The intracellular pupil mechanism and photore-
ceptor signal: noise ratios in the ﬂy Lucilia cuprina. Proc R Soc Lond B 231, 415-435.

[36] Juusola M, Uusitalo RO, Weckstr¨om M (1995): Transfer of graded potentials at the

photoreceptor-interneuron synapse. J Gen Physiol 105, 117-148.

[37] Keiper W, Schnakenberg J (1984) Statistical analysis of quantum bump parameters in Limulus

ventral photoreceptors. Z Naturforsch 39c, 781-790.

[38] Kirschfeld K (1967): Die Prokjektion der optischen Umwelt auf das Raster der Rhabdomere

im Komplexauge von Musca. Exp Brain Res 3, 248-270.

44

[39] Kirschfeld K (1979): The visual system of the ﬂy: physiological optics and functional anatomy
as related to behavior. In: Schmitt FO, Worden FG (eds) Neurosciences: Fourth Study Pro-
gram. MIT Press, Cambridge, MA, pp 297-310.

[40] Kirschfeld K, Franceschini N (1969): Ein Mechanismus zur Steuerung des Lichtﬂusses in den

Rhabdomeren des Komplexauges von Musca. Kybernetik 6, 13-22.

[41] Krapp HG, Hengstenberg B, Hengstenberg R (1998): Dendritic structure and receptive-ﬁeld
organization of optic ﬂow processing interneurons in the ﬂy. J Neurophysiol 79, 1902-1917.

[42] Kuno M (1995) The Synapse: Function, Plasticity and Neurotrophism. Oxford University

Press, Oxford.

[43] Lagnado L, Gomis A, Job C (1999): Continuous vesicle cycling in the synaptic terminal of

retinal bipolar cells. Neuron 17, 957-967.

[44] Land MF, Collett TS (1974): Chasing behavior of houseﬂies (Fannia canicularis): A descrip-

tion and analysis. J Comp Physiol 89, 331-357.

[45] Laughlin SB (1989): Coding eﬃciency and design in visual processing. In: Stavenga DG,

Hardie RC (eds) Facets of Vision. Springer, New York, pp 213-234.

[46] Laughlin SB (1999): Visual motion: Dendritic integration makes sense of the world. Current

Biol 9, R15-R17.

[47] Laughlin SB, Hardie R (1978): Common strategies for light adaptation in the perpheral visual

systems of ﬂy and dragonﬂy. J Comp Physiol 128, 319-340.

[48] Laughlin SB and Lillywhite PG (1982): Intrinsic noise in locust photoreceptors. J Physiol 332,

25-45.

[49] Laughlin SB, Howard J, Blakeslee B (1987): Synaptic limitations to contrast coding in the

retina of the blowﬂy Calliphora. Proc R Soc Lond B 231, 437-467.

[50] Laughlin SB, de Ruyter van Steveninck RR, Anderson J (1998): The metabolic cost of neural

information. Nature Neuroscience 1, 36-41.

[51] Lillywhite PG, Laughlin SB (1979) Transducer noise in a photoreceptor. Nature 277, 569-572.

[52] MacKay D, McCulloch WS (1952): The limiting information capacity of a neuronal link. Bull

Math Biophys 14, 127-135.

[53] Meinertzhagen IA, Fr¨ohlich A (1983): The regulation of synapse formation in the ﬂy’s visual

system. Trends in Neurosci 7, 223-228.

[54] Meister M, Berry MJ (1999) The neural code of the retina. Neuron 22, 435-450.

[55] Nicol D, Meinertzhagen IA (1982): An analysis of the number and composition of the synaptic

populations formed by photoreceptors of the ﬂy. J Comp Neurol 207, 29-44.

[56] Passaglia C, Dodge F, Herzog E, Jackson S, Barlow R (1997): Deciphering a neural code for

vision. Proc Natl Acad Sci USA 94, 12649-12654.

[57] Potters M, Bialek W (1994). Statistical mechanics and visual signal processing. J Phys I France

4, 1755-1775.

45

[58] Reichardt W (1961): Autocorrelation, a principle for the evaluation of sensory information by
the central nervous system. In: Rosenblith WA (ed) Principles of Sensory Communication.
John Wiley, New York, NY, pp. 303-317.

[59] Reichardt W, Poggio T (1976): Visual control of orientation behavior in the ﬂy. Part I: A

quantitative analysis. Q Rev Biophys 9, 311-375.

[60] Rice SO (1944-45): Mathematical analysis of random noise. Bell System Technical Journal,

23, 24. Reprinted in Wax (1954), pp. 133-294.

[61] Rieke F, Warland D, de Ruyter van Steveninck R, Bialek W (1997): Spikes: Exploring the

Neural Code. MIT Press, Cambridge MA.

[62] Roeder KD (1998): Nerve Cells and Insect Behavior. Harvard University Press, Cambridge

MA.

UK.

[63] de Ruyter van Steveninck RR (1986): Real-time performance of a movement-sensitive neuron

in the blowﬂy visual system. Thesis, University of Groningen, The Netherlands.

[64] de Ruyter van Steveninck R, Bialek W (1988) Real-time performance of a movement-sensitive
neuron in the blowﬂy visual system: coding and information transfer in short spike sequences.
Proc R Soc Lond B 234, 379-414.

[65] de Ruyter van Steveninck R, Bialek W (1995): Reliability and statistical eﬃciency of a blowﬂy

movement-sensitive neuron. Phil Trans R Soc Lond B 348, 321-340.

[66] de Ruyter van Steveninck RR, Laughlin SB (1996a): The rate of information transfer at

graded-potential synapses. Nature 379, 642-645.

[67] de Ruyter van Steveninck RR, Laughlin SB (1996b): Light adaptation and reliability in blowﬂy

photoreceptors. Int J Neural Syst 7, 437-444.

[68] de Ruyter van Steveninck RR, Lewen GD, Strong SP, Koberle R, Bialek W (1997): Repro-

ducibility and variability in neural spike trains. Science 275, 1805-1808.

[69] Saleh BEA, Teich MC (1991): Fundamentals of Photonics Wiley, New York.

[70] Schr¨odinger E (1952): Statistical Thermodymamics. Cambridge University Press, Cambridge,

[71] Shaw SR (1981): Anatomy and physiology of identiﬁed non-spiking cells in the photoreceptor-
lamina complex of the compound eye of insects, especially Diptera. In: Roberts A, Bush BMH
(eds) Neurons Without Impulses. Cambridge Univ Press, Cambridge UK.

[72] Single S, Borst A (1998): Dendritic integration and its role in computing image velocity.

Science 281, 1848-1850.

[73] Smakman JGJ, van Hateren JH, Stavenga DG (1984): Angular sensitivity of blowﬂy pho-
toreceptors: Intracellular measurements and wave-optical predictions. J Comp Physiol A155,
239-247.

[74] Snyder AW (1979): The physics of vision in compound eyes. In: Autrum H (ed) Handbook of
Sensory Physiology VII/6A: Comparative physiology and evolution of vision in invertebrates.
A: Invertebrate photoreceptors. Springer, Heidelberg, pp 225-314.

[75] Snyder DL, Miller MI (1991): Random Point Processes in Time and Space (second edition).

Springer, Heidelberg.

46

[76] Stavenga DG (1974): Visual receptor optics, rhodopsin and pupil in ﬂy retinula cells. Thesis,

University of Groningen, Groningen, The Netherlands.

[77] Stavenga DG (1995): Insect retinal pigments. Spectral characteristics and physiological func-

tions. Progr Ret Eye Res 15, 231-259.

[78] Stieve H, Bruns M (1983): Bump latency distribution and bump adaptation of Limulus ventral
nerve photoreceptor in varied extracellular calcium concentrations. Biophys Struct Mech 9,
329-339.

[79] Strong SP, Koberle R, de Ruyter van Steveninck RR, Bialek W (1998): Entropy and informa-

tion in neural spike trains. Phys Rev Lett 80, 197-200.

[80] Teich MC, Khanna SM (1985) Pulse number distribution for the spike train in the cat’s audi-

tory nerve. J Acoust Soc Am 77, 1110-1128.

[81] Tinbergen N (1984): Curious Naturalist (revised edition). Univ. of Massachusetts Press,

Amherst MA.

[82] Tolhurst DJ, Movshon JA, Dean AF (1983) The statistical reliability of signals in single neurons

in cat and monkey visual cortex. Vision Res 23, 775-785.

[83] Uusitalo RO, Juusola M, Kouvalainen E, Weckstr¨om M (1995): Tonic transmitter release in a

graded potential synapse. J Neurophysiol 74, 470-473.

[84] Warzecha A-K, Kretzberg J, Egelhaaf M (1998): Temporal precision of the encoding of motion

information by visual interneurons. Curr Biol 8, 359-368.

[85] Wax N (1954): Noise and Stochastic Processes. Dover, New York NY.

[86] Wong F, Knight BW, Dodge FA (1980): Dispersion of latencies in photoreceptors of Limulus

and the adapting bump model. J Gen Physiol 71, 249-268.

47


2
0
0
2
 
n
a
J
 
2
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
7
2
0
1
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Spike timing and the coding of naturalistic sounds in a
central auditory area of songbirds

Brian D. Wright,1−3 Kamal Sen,1−3 William Bialek1,4,5 and Allison J. Doupe1−3
1Sloan–Swartz Center for Theoretical Neurobiology
2Departments of Physiology and 3Psychiatry
University of California at San Francisco, San Francisco, California 94143–0444
4NEC Research Institute, 4 Independence Way, Princeton, New Jersey 08540
5Department of Physics, Princeton University, Princeton, New Jersey 08544
{bdwright/kamal/ajd}@phy.ucsf.edu, wbialek@princeton.edu

July 24, 2013

Abstract

In nature, animals encounter high dimensional sensory stimuli that have complex statistical and
dynamical structure. Attempts to study the neural coding of these natural signals face challenges
both in the selection of the signal ensemble and in the analysis of the resulting neural responses.
For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of
conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra
ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld
L) to continuous playback of long segments from this ensemble. Following methods developed in the
ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus
without any assumptions about which features of the stimulus are relevant. Preliminary results indicate
that large amounts of information are carried by spike timing, with roughly half of the information
accessible only at time resolutions better than 10 ms; additional information is still being revealed as
time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking
of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike
patterns. Initial results show that in ﬁeld L, temporal patterns give at least ∼ 20% extra information.
Thus, single central auditory neurons can provide an informative representation of naturalistic sounds,
in which spike timing may play a signiﬁcant role.

1 Introduction

Nearly ﬁfty years ago, Barlow [1] and Attneave [2] suggested that the brain may construct a neural code
that provides an efﬁcient representation for the sensory stimuli that occur in the natural world. Slightly
earlier, MacKay and McCulloch [3] emphasized that neurons that could make use of spike timing—
rather than a coarser “rate code”—would have available a vastly larger capacity to convey information,
although they left open the question of whether this capacity is used efﬁciently. Theories for timing codes
and efﬁcient representation have been discussed extensively, but the evidence for these attractive ideas

1

remains tenuous. A real attack on these issues requires (at least) that we actually measure the information
content and efﬁciency of the neural code under stimulus conditions that approximate the natural ones.
In practice, constructing an ensemble of “natural” stimuli inevitably involves compromises, and the
responses to such complex dynamic signals can be very difﬁcult to analyze.

At present the clearest evidence on efﬁciency and timing in the coding of naturalistic stimuli comes
from central invertebrate neurons [4, 5] and from the sensory periphery [6, 7] and thalamus [8, 9] of
vertebrates. The situation for central vertebrate brain areas is much less clear. Here we use the songbird
auditory system as an accessible test case for these ideas. The set of songbird telencephalic auditory areas
known as the ﬁeld L complex is analogous to mammalian auditory cortex and contains neurons that are
strongly driven by natural sounds, including the songs of birds of the same species (conspeciﬁcs) [10,
11, 12, 13]. We record from the zebra ﬁnch ﬁeld L, using naturalistic stimuli that consist of recordings
from groups of 10-40 conspeciﬁc birds. We ﬁnd that single neurons in ﬁeld L show robust and well
modulated responses to playback of long segments from this song ensemble, and that we are able to
maintain recordings of sufﬁcient stability to collect the large data sets that are required for a model
independent information theoretic analysis. Here we give a preliminary account of our experiments.

2 A naturalistic ensemble

Auditory processing of complex sounds is critical for perception and communication in many species,
including humans, but surprisingly little is known about how high level brain areas accomplish this task.
Songbirds provide a useful model for tackling this issue, because each bird within a species produces a
complex individualized acoustic signal known as a song, which reﬂects some innate information about
the species’ song as well as information learned from a “tutor” in early life. In addition to learning their
own song, birds use the acoustic information in songs of others to identify mates and group members,
to discriminate neighbors from intruders, and to control their living space [14]. Consistent with how
ethologically critical these functions are, songbirds have a large number of forebrain auditory areas with
strong and increasingly specialized responses to songs [11, 15, 16]. The combination of a rich set of
behaviorally relevant stimuli and a series of high-level auditory areas responsive to those sounds provides
an opportunity to reveal general principles of central neural encoding of complex sensory stimuli. Many
prior studies have chosen to study neural responses to individual songs or altered versions thereof. In
order to make the sounds studied increasingly complex and natural, we have made recordings of the
sounds encountered by birds in our colony of zebra ﬁnches. To generate the sound ensemble that was
used in this study we ﬁrst created long records of the vocalizations of groups of 10-40 zebra ﬁnches
in a soundproof acoustic chamber with a directional microphone above the bird cages. The group of
birds generated a wide variety of vocalizations including songs and a variety of different types of calls.
Segments of these sounds were then joined to create the sounds presented in the experiment. One of the
segments that was presented (∼ 30 sec) was repeated in alternation with different segments.

We recorded the neural responses in ﬁeld L of one of the birds from the group to the ensemble of
natural sounds played back through a speaker, at an intensity approximately equal to that in the colony
recording. This bird was lightly anesthetized with urethane. We used a single electrode to record the
neural response waveforms and sorted single units ofﬂine. Further details concerning experimental tech-
niques can be found in Ref. [13].

2

Figure 1: A. Spike raster of 4 seconds of the responses of a single neuron in ﬁeld L to a 30 second
segment of a natural sound ensemble of zebra ﬁnch sounds. The stimulus was repeated 80 times. B.
Peri-stimulus time histogram (PSTH) with 1 ms bins. C. Sound pressure waveform for the natural sound
ensemble. D. Blowup of segment shown in the box in A. The scale bar is 50 ms.

3 Information in spike sequences

The auditory telencephalon of birds consists of a set of areas known as the ﬁeld L complex, which receive
input from the auditory thalamus and project to increasingly selective auditory areas such as NCM, cHV
and NIf [12, 17] and ultimately to the brain areas specialized for the bird’s own song. Field L neurons
respond to simple stimuli such as tone bursts, and are organized in a roughly tonotopic fashion [18],
but also respond robustly to many complex sounds, including songs. Figure 1 shows 4 seconds of the
responses of a cell in ﬁeld L to repeated presentations of a 30 sec segment from the natural ensemble
described above. Averaging over presentations, we see that spike rates are well modulated. Looking at
the responses on a ﬁner time resolution we see that aspects of the spike train are reproducible on at least
a ∼ 10 ms time scale. This encourages us to measure the information content of these responses over a
range of time scales, down to millisecond resolution.

Our approach to estimating the information content of spike trains follows Ref. [4]. At some time t
(deﬁned relative to the repeating stimulus) we open a window of size T to look at the response. Within
this window we discretize the spike arrival times with resolution ∆τ so that the response becomes a

3

Total Entropy
Noise Entropy
Mutual Info

)
c
e
s
/
s
t
i
b
(
 
e
t
a
R
 
n
o
i
t

a
m
r
o

f

n

I

40

35

30

25

20

15

10

5

0

0

0.01

0.02

0.03

0.04

0.05

0.06

1/N

repeats

Figure 2: Mutual information rate for the spike train is shown as a function of data size for ∆τ = 2 ms
and T = 32 ms.

“word” with T /∆τ letters. If the time resolution ∆τ is very small, the allowed letters are only 1 and 0,
but as ∆τ becomes larger one must keep track of multiple spikes within each bin. Examining the whole
experiment, we sample the probability distribution of words, PT (W ), and the entropy of this distribution
sets the capacity of the code to convey information about the stimulus:

Stotal(T ; ∆τ ) = −

PT (W ) log2 PT (W ) bits,

(1)

XW

where the notation reminds us that the entropy depends both on the size of the words that we consider and
on the time resolution with which we classify the responses. We can think of this entropy as measuring
the size of the neuron’s vocabulary.

Because the whole experiment contributes to deﬁning the vocabulary size, estimating the distribution
PT (W ) and hence the total entropy is not signiﬁcantly limited by the problems of ﬁnite sample size.
This can be seen in Fig. 2 in the stability of the total entropy with changing the number of repeats used
in the analysis. Here we show the total entropy as a rate in bits per second by dividing the entropy by the
time window T .

While the capacity of the code is limited by the total entropy, to convey information particular words
in the vocabulary must be associated, more or less reliably, with particular stimulus features. If we look

4

at one time t relative to the (long) stimulus, and examine the words generated on repeated presentations,
we sample the conditional distribution PT (W |t). This distribution has an entropy that quantiﬁes the
noise in the response at time t, and averaging over all times we obtain the average noise entropy,

Snoise(T ; ∆τ ) =

−

PT (W |t) log2 PT (W |t)

bits,

(2)

(cid:28)

XW

(cid:29)t

where h· · ·it indicates a time average (in general, h· · ·ix denotes an average over the variable x). Tech-
nically, the above average should be an average over stimuli s, however, for a sufﬁciently long and rich
stimulus, the ensemble average over s can be replaced by a time average. For the noise entropy, the
problem of sampling is much more severe, since each distribution PT (W |t) is estimated from a number
of examples given by the number of repeats. Still, as shown in Fig. 2, we ﬁnd that the dependence of our
estimate on sample size is simple and regular; speciﬁcally, we ﬁnd

S(T ; ∆τ ; Nrepeats) = S(T ; ∆τ ; ∞) +

+ · · · .

(3)

A
Nrepeats

This is what we expect for any entropy estimate if the distribution is well sampled, and if we make
stronger assumptions about the sampling process (independence of trials etc.) we can even estimate the
correction coefﬁcient A [19]. In systems where much larger data sets are available this extrapolation
procedure has been checked, and the observation of a good ﬁt to Eq. (3) is a strong indication that
larger sample sizes will be consistent with S(T ; ∆τ ) = S(T ; ∆τ ; ∞); further, this extrapolation can be
tested against bounds on the entropy that are derived from more robust quantities [4]. Most importantly,
failure to observe Eq. (3) means that we are in a regime where sampling is not sufﬁcient to draw reliable
conclusions without more sophisticated arguments, and we exclude these regions of T and ∆τ from our
discussion.

Ideally, to measure the spike train total and noise entropy rates, we want to go to the limit of inﬁnite
word duration. A true entropy is extensive, which here means that it grows linearly with spike train
word duration T , so that the entropy rate S = S/T is constant. For ﬁnite word duration however, words
sampled at neighboring times will have correlations between them due, in part, to correlations in the
stimulus (for birdsong these stimulus autocorrelation time scales can extend up to ∼ 100 ms). Since the
word samples are not completely independent, the raw entropy rate is an overestimate of the true entropy
rate. The effect is larger for smaller word duration and the leading dependence of the raw estimate is

S(T ; ∆τ ; ∞) = S(∞; ∆τ ; ∞) +

+ · · · ,

(4)

B
T

where B > 0 and we have already taken the inﬁnite data size limit. We cannot directly take the large
T limit, since for large word lengths we eventually reach a data sampling limit beyond which we are
unable to reliably compute the word distributions. On the other hand, if there is a range of T for which
the distributions are sufﬁciently well sampled, the behavior in Eq. (4) should be observed and can be
used to extrapolate to inﬁnite word size [4]. We have checked that our data shows this behavior and that
it sets in for word sizes below the limit where the data sampling problem occurs. For example, in the
case of the noise entropy, for ∆τ = 2 ms, it applies for T below the limit of 50 ms (above this we run
into sampling problems). The total entropy estimate is nearly perfectly extensive.

Finally, we combine estimates of total and noise entropies to obtain the information that words carry

about the sensory stimulus,

I(T ; ∆τ ) = Stotal(T ; ∆τ ) − Snoise(T ; ∆τ ) bits.

(5)

5

Figure 2 shows the total and noise entropy rates as well as the mutual information rate for a time window
T = 32 ms and time resolution ∆τ = 2 ms. The error bars on the raw entropy and information rates
were estimated to be approximately ±0.2 bits/sec using a simple bootstrap procedure over the repeated
trials. The extrapolation to inﬁnite data size is shown for the mutual information rate estimate (error
bars in the extrapolated values will be < ±0.2 bits/sec) and is consistent with the prediction of Eq. (3).
Since the total entropy is nearly extensive and the noise entropy rate decreases with word duration due
to subextensive corrections as described above, the mutual information rate shown in Fig. 2 grows with
word duration. We ﬁnd that there is an upward change in the mutual information rate (computed at
∆τ = 2 ms and T = 32 ms) of ∼ 7%, in the large T limit. For simplicity in the following, we shall
look at a ﬁxed word duration T = 32 ms that is in the well-sampled region for all time resolutions ∆τ
considered.

The mutual information rate measures the rate at which the spike train removes uncertainty about the
stimulus. However, the mutual information estimate does not depend on identifying either the relevant
features of the stimulus or the relevant features of the response, which is crucial in analyzing the response
to such complex stimuli.
In this sense, our estimates of information transmission and efﬁciency are
independent of any model for the code, and provide a benchmark against which such models could be
tested.

One way to look at the information results is to ﬁx our time window T and ask what happens as we
change our time resolution ∆τ . When ∆τ = T , the “word” describing the response is nothing but the
number of spikes in the window, so we have a rate or counting code. As we decrease ∆τ , we gradually
distinguish more and more detail in the arrangement of spikes in the window. We chose a range of T
values from 30 − 100 ms in our analyses to cover previously observed response windows for ﬁeld L
neurons and to probe the behaviorally relevant time scale (∼ 100 ms) of individual song syllables or
notes. For T = 32 ms, we show the results (extrapolated to inﬁnite data size) in the upper curve of
Fig. 3. The spike train mutual information shows a clear increase as the timing resolution is improved.
In addition, Fig. 3 shows that roughly half of the information is accessible at time resolutions better than
10 ms and additional information is still being revealed as time resolution is improved to 2 ms.

4 Information in rate modulation

Knowing the mutual information between the stimulus and the spike train (deﬁned in the window T ),
we would like to ask whether this can be accounted for by the information in single spike events or
whether there is some additional information conveyed by the patterns of spikes. In the latter case, we
have precisely what we mean by a temporal or timing code: there is information beyond that attributable
to the probability of single spike events occurring at time t relative to the onset of the stimulus. By event
at time t, we mean that the event occurs between time t and time t + ∆τ , where ∆τ is the resolution
at which we are looking at the spike train. This probability is simply proportional to the ﬁring rate (or
peri-stimulus time histogram (PSTH)) r(t) at time t normalized by the mean ﬁring rate ¯r. Speciﬁcally if
the duration of each repeated trial is Trepeat we have

P (1 spk @ t|s(t

)) =

′

r(t) ∆τ
¯r Trepeat

,

(6)

where s(t′) denotes the stimulus history (t′ < t). The probability of a spike event at t, a priori of knowing
the stimulus history, is ﬂat: P (1 spk @ t) = ∆τ /Trepeat. Thus, the mutual information between the

6

4.5

)
c
e
s
/
s
t
i

b
(
 
e

t

 

a
R
n
o
i
t
a
m
r
o
f
n
I

3.5

2.5

5

4

3

2

Spike Train 

Independent Events 

1.5

0

5

10

15

20

∆τ (ms)

25

30

35

Figure 3: Information rates for the spike train (T = 32 ms) and single spike events as a function of time
resolution ∆τ of the spike rasters, corrected for ﬁnite data size effects.

stimulus and the single spike events is [20]:

I(1 spike; ∆τ ) = S[P (1 spk @ t)] − hS[P (1 spk @ t|s)]is

=

r(t)
¯r

(cid:28)

log2 (cid:18)

r(t)

¯r (cid:19) (cid:29)t

bits,

(7)

where r(t) is the PSTH binned to resolution ∆τ and the stimulus average in the ﬁrst expression is
replaced by a time average in the second (as discussed in the calculation of the noise entropy in spike
train words in the previous section). We ﬁnd that this information is approximately 1 bit for ∆τ = 2
ms. Supposing that the individual spike events are independent (i.e. no intrinsic spike train correlations),
the information rate in single spike events is obtained by multiplying the mutual information per spike
(Eq. 7) by the mean ﬁring rate of the neuron (∼ 3.5 Hz). This gives an upper bound to the single
spike event contribution to the information rate and is shown in the lower curve of Fig. 3 (error bars
are again < ±0.2 bits/sec). Comparing with the spike train information (upper curve), we see that at
a resolution of ∆τ = 2 ms, there is at least ∼ 20% of the total information in the spike train that
cannot be attributable to single spike events. Thus there is some pattern of spikes that is contributing
synergistically to the mutual information. The fact discussed, in the previous section, that the spike train

7

information rate grows subextensively with the the word duration out to the point where data sampling
becomes problematic is further conﬁrmation of the synergy from spike patterns. Thus we have shown
model-independent evidence for a temporal code in the neural responses.

5 Conclusion

Until now, few experiments on neural responses in high level, central vertebrate brain areas have mea-
sured the information that these responses provide about dynamic, naturalistic sensory signals. As em-
phasized in earlier work on invertebrate systems, information theoretic approaches have the advantage
that they require no assumptions about the features of the stimulus to which neurons respond. Using this
method in the songbird auditory forebrain, we found that patterns of spikes seem to be special events
in the neural code of these neurons, since they carry more information than expected by adding up the
contributions of individual spikes. It remains to be determined what these spike patterns are, what stim-
ulus features they may encode, and what mechanisms may be responsible for reading such codes at even
higher levels of processing.

Work at UCSF was supported by grants from the NIH (NS34835) and the Sloan-Swartz Center for
Theoretical Neurobiology. BDW and KS supported by NRSA grants from the NIDCD. We thank Katrin
Schenk and Robert Liu for useful discussions.

Acknowledgments

References

[1] Barlow, H.B. (1961). Possible principles underlying the transformation of sensory messages. In Sen-

sory Communication, W.A. Rosenblith, ed., pp. 217–234 (MIT Press, Cambridge, MA).

[2] Attneave, F. (1954). Some informational aspects of visual perception. Psychol. Rev. 61, 183–193.

[3] MacKay, D. and McCulloch, W.S. (1952). The limiting information capacity of a neuronal link. Bull.

Math. Biophys. 14, 127–135.

[4] Strong, S.P., Koberle, R., de Ruyter van Steveninck, R. and Bialek, W. (1998). Entropy and informa-

tion in neural spike trains, Phys. Rev. Lett. 80, 197–200.

[5] Lewen, G.D., Bialek, W. and de Ruyter van Steveninck, R.R. (2001). Neural coding of naturalistic

motion stimuli. Network 12, 317–329, physics/0103088.

[6] Rieke, F., Bodnar, D.A. and Bialek, W. (1995). Naturalistic stimuli increase the rate and efﬁciency

of information transmission by primary auditory afferents. Proc. R. Soc. Lond. B 262, 259–265.

[7] Berry II, M.J., Warland, D.K. and Meister, M. (1997). The structure and precision of retinal spike

trains. Proc. Nat. Acad. Sci. (USA) 94, 5411–5416.

[8] Reinagel, P. and Reid, R.C. (2000). Temporal coding of visual information in the thalamus. J. Neu-

rosci. 20, 5392–5400.

8

[9] Liu, R.C., Tzonev, S., Rebrik, S. and Miller, K.D. (2001). Variability and information in a neural

code of the cat lateral geniculate nucleus. J. Neurophysiol. 86, 2789–2806.

[10] Scheich, H., Langner, G. and Bonke, D. (1979). Responsiveness of units in the auditory neostriatum
of the guinea fowl (Numida meleagris) to species-speciﬁc calls and synthetic stimuli II. Discrimination
of Iambus-Like Calls. J. Comp. Physiol. A 132, 257–276.

[11] Lewicki, M.S. and Arthur, B.J. (1996). Hierarchical organization of auditory temporal context sen-

sitivity. J. Neurosci. 16(21), 6987–6998.

[12] Janata, P. and Margoliash, D. (1999). Gradual emergence of song selectivity in sensorimotor struc-

tures of the male zebra ﬁnch song system. J. Neurosci. 19(12), 5108–5118.

[13] Theunissen, F.E., Sen, K. and Doupe, A.J. (2000). Spectral temporal receptive ﬁelds of nonlinear

auditory neurons obtained using natural sounds. J. Neurosci. 20(6), 2315–2331.

[14] Searcy, W.A. and Nowicki, S. (1999). In The Design of Animal Communication, M.D. Hauser and

M. Konishi, eds., pp. 577–595 (MIT Press, Cambridge, MA).

[15] Margoliash, D. (1983). Acoustic parameters underlying the responses of song-speciﬁc neurons in

the white-crowned sparrow. J. Neurosci. 3(5), 1039–1057.

[16] Sen, K., Theunissen, F.E. and Doupe, A.J. (2001). Feature analysis of natural sounds in the songbird

auditory forebrain. J. Neurophysiol. 86, 1445–1458.

[17] Stripling, R., Kruse, A.A. and Clayton, D.F. (2001). Development of song responses in the zebra
ﬁnch caudomedial neostriatum: role of genomic and electrophysiological activities. J. Neurobiol. 48,
163–180.

[18] Zaretsky, M.D. and Konishi, M. (1976). Tonotopic organization in the avian telencephalon. Brain

Res. 111, 167–171.

[19] Treves, A. and Panzeri, S. (1995). The upward bias in measures of information derived from limited

data samples. Neural Comput., 7, 399–407.

[20] Brenner, N., Strong, S., Koberle, R. and Bialek, W. (2000). Synergy in a neural code, Neural

Comput. 12, 1531–1552, physics/9902067.

9


2
0
0
2
 
n
u
J
 
0
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
5
7
0
6
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Sensitivity Analysis of Stoichiometric Networks: An Extension of
Metabolic Control Analysis to Non-equilibrium Trajectories

Brian P. Ingalls∗ and Herbert M. Sauro†
Control and Dynamical Systems, California Institute of Technology, CA
{ingalls,hsauro}@cds.caltech.edu

Abstract

A sensitivity analysis of general stoichiometric networks is considered. The results are
presented as a generalization of Metabolic Control Analysis, which has been concerned pri-
marily with system sensitivities at steady state. An expression for time-varying sensitivity
coeﬃcients is given, and the Summation and Connectivity Theorems are generalized. The
results are compared to previous treatments. The analysis is accompanied by a discussion
of the computation of the sensitivity coeﬃcients and an application to a model of photo-
transduction.

1

Introduction

Sensitivity analysis is an important tool in the study of systems dependent on external param-
eters. By their nature, stoichiometric networks allow an elegant description of the relationships
between component and systemic sensitivities. These relationships, particularly those existing
at equilibria of the system, have been addressed by the ﬁelds of Biochemical Systems Theory
(BST) [17] and Metabolic Control Analysis (MCA) [5, 8].

This paper describes the computation and interpretation of system sensitivities over arbi-
trary trajectories. The results are presented in the framework provided by MCA, which is a
theory devoted to the analysis of the distribution of control within a network and how the
behaviour of the system relates to the properties of the components. With few exceptions the
MCA literature treats networks in steady state.

MCA has had great success in describing the control and regulation of systems at steady
state. However, in the analysis of an increasing domain of examples it becomes necessary
In many systems it is the transient
to consider sensitivities along non-steady trajectories.
or oscillatory behaviour which is of primary interest (e.g. in signal transduction or cell cycle
regulation). Some extensions of MCA to non-steady behaviour have appeared in the literature
([1, 4, 7, 11, 12, 15]). These papers generalize the steady state results of MCA to special cases
of dynamic behaviour (e.g. periodic behaviour or trajectories near a stable equilibrium). The
analysis presented in this paper extends these results by measuring time varying sensitivities
along arbitrary trajectories.

∗Supported by Air Force Research Laboratory Cooperative Agreement No. F30602-01-2-0558
†Supported by Air Force Research Laboratory Cooperative Agreement No. F30602-01-2-0558 and Japan

Sciences and Technology Corporation (JST) Kitano ERATO Project

1

A standard MCA analysis might treat, for example, the eﬀect of increasing an enzyme
concentration on the steady state value of some related metabolite. Using the analysis described
below, one can determine the sensitivity to the perturbation throughout the time evolution of
the system, regardless of the nature of the trajectory. Based on these time-varying sensitivities,
the Summation and Connectivity Theorems of MCA are extended to give conditions which
hold for all time. When applied at steady state, these results reduce to the standard analysis
of MCA.

The statements in this paper apply to arbitrary system dynamics, including transients,
convergence to steady state, and oscillations. However, as pointed out in previous papers
([4, 11]), sensitivity coeﬃcients derived for oscillating systems must be interpreted with care.
A discussion on applications to oscillating systems appears below.

The outline of the paper is as follows. Time-varying sensitivity coeﬃcients are deﬁned and
derived, and their computation is discussed brieﬂy. The main results of MCA – the Summation
and Connectivity Theorems – are then extended to the time-varying case. Finally, the sensitivity
analysis is illustrated by treating some examples, including a model of phototransduction.

2 Preliminaries

A network consisting of n chemical species involved in m reactions is modelled. The n-vector
s is composed of the concentrations of each species. The constant r-vector p is composed of
the (external) parameters of interest in the model. The m-vector valued function v = v(s, p, t)
describes the rate of each reaction as a (possibly time-varying) function of species concentrations
and parameter values. Finally the n by m stoichiometry matrix N describes the network:
component Ni,j is equal to the net number of individuals of species i produced or consumed in
reaction j. The network can then be modelled by the ordinary diﬀerential equation

d
dt

s(t) = Nv(s(t), p, t)

for all t ≥ 0.

(1)

We allow the vector p to contain the initial states of any of the species concentrations as well
as any other parameters which will directly aﬀect the rates of the reactions (e.g. concentrations
of enzymes and external eﬀectors).

We assume that the function v(s, p, t) is continuous and is continuously diﬀerentiable in s
and p for each ﬁxed t. We further assume that for each initial condition s(0) = s0 and each
choice of parameters p the unique solution of (1) is deﬁned for all t ≥ 0. We denote this solution
by s(t, s0, p), and will use simply s(t, p) or s(t) when no confusion will arise.

Before embarking on an analysis of system (1) it is prudent to ﬁrst consider any linear de-
pendencies inherent in the state variables of the system (which will simplify both the analysis
and the computation). Each conserved moiety in the network corresponds to a linearly depen-
dent row in the stoichiometry matrix N. We follow the procedure and terminology described
by Reder [14] (see also [9]) in the reduction of the system.

Let n0 denote the row rank of N. If n0 = n, then no reduction is necessary. Otherwise, we
begin by re-ordering the rows of N so that the ﬁrst n0 rows are linearly independent. Let NR
be the reduced stoichiometry matrix which results from truncating the last n − n0 rows of N.
Since the truncated rows can be formed by linear combination of the rows of NR, the matrix
N can be written as the product

N = LNR

2

where the n × n0 matrix L, called the link matrix, has the form

L =

In0
L0 (cid:21)

.

(cid:20)

(Here and below the notation Iq will be used for the q × q identity matrix).

The advantage of this decomposition can now be realized. Since each conserved moiety
allows one species concentration to be determined as a function of the others, we may decompose
the species vector s into independent and dependent species vectors si and sd respectively.
Ordering the components of s to match the rows of N, we write s = (si, sd) where si is an
n0-tuple and sd is an (n − n0)-tuple. (This involves a minor abuse of notation since s, si and
sd are all column vectors.) Then equation (1) can be written as

d
dt

(cid:20)

si(t)
sd(t)

(cid:21)

Hence

= LNRv(s(t), p, t) =

NRv(s(t), p, t)

for all t ≥ 0.

In0
L0 (cid:21)

(cid:20)

d
dt

d
dt

sd(t) = L0

si(t)

for all t ≥ 0,

and so sd(t) − L0si(t) is an integral of motion of (1), i.e. this diﬀerence is constant throughout
the evolution of the system. We introduce the constant (n − n0)-vector T to quantify this
relationship. Any trajectory of the system satisﬁes

sd(t) = L0si(t) + T

for all t ≥ 0,

(2)

where T is deﬁned in terms of the initial conditions as

T = sd(0) − L0si(0).

In analysis and computation, attention can be restricted to the independent species si, since
the corresponding results incorporating the dependent vector sd are arrived at immediately
through the relationship (2). That is, one need only consider the reduced system

d
dt

si(t) = NRv(s(t), p, t) = NRv((si(t), L0si(t) + T), p, t)

for all t ≥ 0.

(3)

3 Sensitivity Analysis

We now present a general sensitivity analysis of the system (1).

3.1 Deﬁnitions

Following [18] and [1], we make the basic deﬁnition.

Deﬁnition 3.1 Given an initial condition s(0) = s0 and a set of parameter values p0, we deﬁne
the time-varying concentration sensitivity coeﬃcients (or concentration response coeﬃcients) as
the elements of the n × r matrix function Rs(·) given by

Rs(t) :=

∂s(t, p)
∂p

|p=p0 = lim
∆p→0

s(t, p0 + ∆p) − s(t, p0)
∆p

for all t ≥ 0.

2

3

Remark 3.2 These time varying response coeﬃcients can be interpreted in exact analogy to
the standard (steady state) response coeﬃcients of MCA. The diﬀerence is that the response
to a perturbation along an entire trajectory in now considered, rather than at a particular
equilibrium state.

Take for example a system which has an asymptotically stable equilibrium (say sss), and
consider a set of initial conditions (say s0) which do not match the steady state solution. Letting
the system evolve from s0, we may observe some initial transient, followed by convergence to sss
as time tends to inﬁnity. Alternatively, if we make a small perturbation to a system parameter,
we may observe a diﬀerent transient, followed by convergence to a diﬀerent steady state. The
response coeﬃcient deﬁned above provides a measure of the diﬀerence between this “perturbed
trajectory” and the “nominal” (unperturbed) trajectory at each time t. As time tends to
inﬁnity, each trajectory will converge to its steady state, and so the response coeﬃcient will
converge to the steady state response of MCA.

This time-history analysis is particularly useful when studying systems in which transient
behaviour plays a key role in the mechanism. For example, in systems performing phototrans-
duction or action potential transfer, the role of the system is to produce a (transient) spike in
the concentration of a certain species. The analysis presented here will allow elucidation of the
eﬀect of parameter perturbations on the behaviour of such a system.

It should be noted that the response coeﬃcient Rs(·) is deﬁned with respect to a particular
parameter choice p0 and a particular initial condition s0. Should one be interested in comparing
the system response to perturbations at diﬀerent times, a separate response coeﬃcient must be
deﬁned for each such time, since each choice will yield a distinct “initial” state (by re-setting
2
time to zero at the point chosen).

Remark 3.3 Since the vector p is allowed to contain the initial states as components, sensi-
tivity coeﬃcients with respect to initial conditions are included in the above deﬁnition. When
pursuing a steady state analysis, it is often more convenient to consider the total amount of
each conserved moiety as a parameter rather than the initial conditions of particular species
(see e.g. [9]). However, in this dynamic analysis, perturbation of the initial concentrations of
diﬀerent species will have distinct eﬀects on the time-history; it is only at steady state that these
eﬀects can be described equivalently in terms of perturbations in pools of conserved moieties.
2

In addition to the sensitivities of the species concentrations, it is also of interest to consider
the sensitivities of the various rates v(s, p, t).
In expressing these responses, we conform to
the MCA usage of the term ﬂux for the rate of a system reaction.
(In general, these rates
do not represent ﬂuxes in the usual sense of the word as a rate of transfer of some quantity;
the rate of passage of mass through a reaction pathway must also take the stoichiometry into
consideration.)

Deﬁnition 3.4 Given an initial condition s(0) = s0 and set of parameter values p0, we deﬁne
the time-varying ﬂux sensitivity coeﬃcients (or ﬂux response coeﬃcients) as the elements of the
m × r matrix function RJ(·) given by

RJ(t) :=

∂v(s(t, p), p, t)
∂p

|p=p0 =

∂v(s, p, t)
∂s
∂v(s, p, t)
∂s

∂s(t, p)
∂p

Rs(t) +

+

∂v(s, p, t)
∂p
∂v(s, p, t)
∂p

=

for all t ≥ 0,

4

where the derivatives are evaluated at p = p0 and s = s(t, p0).

2

Remark 3.5 The ﬂux response coeﬃcients can be interpreted analogously to the concentration
response coeﬃcients: RJ(t) gives the response in the rates at time t to a perturbation at time
2
zero. At steady state, these coeﬃcients reduce to their standard MCA counterparts.

3.2 Computation

The sensitivity coeﬃcients are deﬁned by a ﬁrst-order linear ordinary diﬀerential equation
which follows from taking the derivative of (1) with respect to p. Dropping some functional
dependencies for ease of legibility, we have:

∂
∂p

ds(t)
dt

= N

∂v(t)
∂s

∂s(t)
∂p

+

∂v(t)
∂p

for all t ≥ 0

which becomes, after switching the order of diﬀerentiation,

(cid:18)

(cid:18)

(cid:19)

(cid:19)

d
dt

∂s(t)
∂p

= N

∂v(t)
∂s

∂s(t)
∂p

+

∂v(t)
∂p

for all t ≥ 0.

(4)

Recall that the sensitivity coeﬃcients are deﬁned with respect to a particular choice of s0 = s(0)
and p0. This choice ﬁxes a trajectory s(t) = s(t, s0, p0), and hence determines the function
v(t) = v(s(t), p, t) as well. The initial conditions for (4) (i.e. the components of the matrix
∂p (0)) are immediate from the deﬁnition of the sensitivity coeﬃcients: the j, k-th entry of ∂s
∂s
∂p (0)
will be zero unless the k-th parameter is the initial condition of the j-th species, in which case
the initial value of the sensitivity coeﬃcient will be one. Equation (4) can be solved for the
concentration sensitivities ∂s
∂p (·) = Rs(·), and the ﬂux sensitivities can be computed according
to deﬁnition 3.4. However, we can reduce the burden of computation by taking advantage of
the dependencies described in equation (2).

Diﬀerentiating (3) with respect to p, we ﬁnd

∂
∂t

∂si(t)
∂p

(L0si(t) + T) +

= NR

= NR

= NR

(cid:18)

(cid:20)(cid:18)

∂v(t)
∂si
∂v(t)
∂si
∂v(t)
∂s

L

(cid:20)

∂si(t)
∂p

+

∂v(t)
∂sd

+

∂v(t)
∂sd

L0

∂si(t)
∂p

+

(cid:19)
∂v(t)
∂sd

∂
∂p
∂si(t)
∂p
∂T
∂p

∂T
∂p

+

∂v(t)
∂sd
∂v(t)
∂p

(cid:21)

+

∂v(t)
∂p
(cid:19)
∂v(t)
∂p

+

(cid:21)
for all t ≥ 0.

(5)

The initial conditions are determined as before.

Remark 3.6 While there are special cases in which the response coeﬃcients can be derived ex-
plicitly (as discussed below), in most cases the solution to equation (4) (or more eﬃciently (5))
must be computed numerically. In performing such a calculation, one must keep in mind that
the right-hand-side depends on the time-varying value of the species concentration vector s(t).
A simple computational strategy is to solve (3) and (5) as a single set of diﬀerential equa-
tions, determining si(·) and ∂si(·)
∂p simultaneously. This would allow, for example, an adaptive
integrator to reduce the step-size during computation should either equation demand it.

5

The reader should note that the pair of ODE’s are coupled, but that the coupling is in
∂p . Thus the increase in complexity comes simply

cascade – equation (3) is independent of ∂si(·)
from having two equations to solve, not from any intertwining of the solutions.

An algorithm for computation of the time-varying response coeﬃcients has been imple-
mented in MATLAB and in the biochemical simulator Jarnac [16]. Scripts of the implementa-
2
tion are available online (http://www.sys-bio.org).

3.3 Analytical Solution

The solution to equation (5) can be expressed in terms of the variation of parameters formula
(derived in standard texts on ODE’s, e.g. [3]):

∂si
∂p

∂si
∂p

(t) = Φ(t)

(0) + Φ(t)

Φ−1(τ )NR

t

0

Z

∂v(τ )
∂sd

∂T
∂p

+

∂v(τ )
∂p

(cid:18)

(cid:19)

dτ

for all t ≥ 0,

(6)

where the matrix-valued function Φ(·) (called the fundamental matrix for equation (5)) satisﬁes
the homogeneous part of (5), i.e. Φ(·) is deﬁned as the solution of the following initial value
problem

Φ(t) =

NR

L

Φ(t),

Φ(0) = In0.

d
dt

∂v(t)
∂s

(cid:20)

(cid:21)

(7)

Note that (6) does not in general provide an explicit formula for the solution to (5) (since (7)
does not admit an explicit solution, in general). Nevertheless, knowing the solution takes this
In addition, as will be demonstrated in
form can simplify both analysis and computation.
Section 4, there are special cases in which (6) does indeed provide an explicit solution.

In the subsequent analysis, it will be convenient to partition the parameter vector p into the
set of parameters which aﬀect the rates (denoted pv) and the set of initial conditions (denoted
ps). From the fact that ∂v
= 0, we see that formula (6) can be partitioned as
∂ps

(0) = ∂T
∂pv

= ∂si
∂pv

(t) = Φ(t)

(0) + Φ(t)

t

Φ−1(τ )NR

∂v(τ )
∂sd

∂T
∂ps

dτ

(t) = Φ(t)

Φ−1(τ )NR

dτ

for all t ≥ 0.

(8)

To provide a more elegant exposition in what follows, we consider the extension of (8) to
∂p (·). Since sd(t) = L0si(t) + T for all t ≥ 0 and

the entire response coeﬃcient matrix Rs(·) = ∂s
∂T
∂pv

= 0, (8) gives

(t) = L0Φ(t)

Φ−1(τ )NR

for all t ≥ 0.

(9)

Together, (8) and (9) give

(t) = LΦ(t)

Φ−1(τ )NR

for all t ≥ 0.

(10)

∂si
∂ps
∂si
∂pv

∂sd
∂pv

∂s
∂pv

As for the ﬂux-response coeﬃcients,

∂v
∂pv

(t) =

∂v(t)
∂s

LΦ(t)

Φ−1(τ )NR

∂v(τ )
∂pv

dτ +

∂v(t)
∂pv

for all t ≥ 0.

∂si
∂ps
t

0
Z

0

Z

t

t

0

Z

t

0

Z

0
Z
∂v(τ )
∂pv

∂v(τ )
∂pv

dτ

∂v(τ )
∂pv

dτ

6

3.4 Scaled Coeﬃcients

Computations in MCA are typically done using “scaled” coeﬃcients: derivatives are taken with
respect to the logarithms of the variables, giving relative (rather than absolute) responses.
These scaled sensitivities have the advantage that they are dimensionless. Further, they allow
direct comparison of responses at diﬀerent states or across diﬀerent parameters.

The scaled responses are related to their unscaled counterparts through multiplication by a
ratio of the values involved. These relationships can be elegantly described at the matrix level.
Following [9], we deﬁne the diagonal matrices

Ds(·) := diag s(·)

Dv(·) := diag v(s(·), p, ·)

Dp := diag p

whose entries are given by the coeﬃcients of the corresponding vectors. The scaled response
matrices, denoted with a “tilde” (

) are then given by

Rs(·) = (Ds(·))−1DpRs(·) =

e

Rv(·) = (Dv(·))−1DpRJ(·) =
e

∂ ln s(·)
∂ ln p
∂ ln v(·)
∂ ln p

.

e
4 MCA Theorems

We next consider generalizations of the main results of Metabolic Control Analysis to the case
of time-varying sensitivities. These results (the Summation and Connectivity Theorems, and
the resulting Control-Matrix Equation) are usually stated in terms of control coeﬃcients and
elasticities, into which the response coeﬃcient matrices are factored ([14, 9]). The time varying
responses deﬁned above do not, in general, allow such a factorization. However, the responses
can be interpreted as the result of applying an appropriately deﬁned control operator to an
elasticity. In this sense, the standard MCA results can be extended to the time-varying case.
Moreover, we will show that under certain assumptions on the derivatives of the rates, one can
recover direct generalizations in terms of matrix factorizations (as was done in [7]).

4.1 Control Operators

Given a trajectory of system (1), we deﬁne the substrate- and parameter-elasticities of the
system as

εs(t) :=

εp(t) :=

for all t ≥ 0.

∂v
∂s

(t)

∂v
∂p

(t)

These vectors describe component (or “local”) sensitivities of the isolated reactions associated
with the system.

The steady state Summation and Connectivity Theorems admit elegant mathematical de-
scriptions due to the fact that the steady state concentration and ﬂux responses can each be
decomposed into a product of two terms: a matrix of control coeﬃcients and a vector of param-
eter elasticities. It is clear from equation (10) that the general time-varying response cannot
be expressed as a product involving the parameter elasticities (since this time-varying term
appears inside the integral). However, the relation between response and elasticity can be ex-
pressed as the action of a control operator on the elasticity vector, as follows. (An operator, in
this case, is an object that maps functions to functions. An introduction to operators appears
in the appendix.)

7

Deﬁnition 4.1 The concentration control operator Cs maps m-vector valued functions to n-
vector valued functions as follows. Given a continuous m-vector valued function y(·) deﬁned
on the non-negative reals, Cs(·)(y(·)) is an n-vector valued function deﬁned by

Cs(t)(y(·)) := LΦ(t)

Φ−1(τ )NR y(τ ) dτ

for all t ≥ 0.

t

0
Z

In a like manner we deﬁne the ﬂux control operator.

Deﬁnition 4.2 The ﬂux control operator CJ maps m-vector valued functions to m-vector
valued functions as follows. Given a continuous m-vector valued function y(·) deﬁned on the
non-negative reals, CJ(·)(y(·)) is an m-vector valued function deﬁned by
CJ(t)(y(·)) := εs(t)Cs(t)(y(·)) + y(t)

for all t ≥ 0.

2

2

The elasticities and control operators can be scaled in the same manner as the response co-
eﬃcients. Each of the results described below has an analogous statement in terms of scaled
quantities, determined simply by including the scaling factors.

With these deﬁnitions in hand, the generalizations of the partitioned response properties
([10, 9]) follow immediately. If the parameter vector is such that the parameters only aﬀect the
rates (i.e. p = pv), then

Rs(t) = Cs(t)(εp(·))

RJ(t) = CJ(t)(εp(·))

for all t ≥ 0.

We next indicate how the Summation and Connectivity Theorems can be interpreted in the
light of these deﬁnitions. We will also show how the action of these operators reduces to matrix
multiplication when the system is at steady state.

4.2 The Summation Theorem

The Summation Theorem can be stated in terms of the control operators as follows.

Theorem 1 (Summation Theorem) If the m-vector k lies in the nullspace of N (i.e. Nk =
0, and so NRk = 0 as well), then (using the simpliﬁed notation described in the appendix)

Cs(t)(k) = 0

and

CJ(t)(k) = k

for all t ≥ 0.

Proof. The result follows from the deﬁnitions of the control operators. If NRk = 0, then

and

Cs(t)(k) = LΦ(t)

Φ−1(τ )NR k dτ = 0

for all t ≥ 0,

t

0

Z

CJ(t)(k) = εsCs(t)(k) + k = k

for all t ≥ 0.

To state the result in a more standard form, we allow the control operators to act on matrices

as follows. If r1(·), . . . , rn(·) are m-vector valued functions, then we interpret

Cs(t)([r1(·) . . . rn(·)]) := [Cs(t)(r1(·)) . . . Cs(t)(rn(·))] .

With this notation, a direct corollary of the Summation Theorem is the following.

8

Corollary 4.3 If the matrix K has columns which form a basis for the nullspace of N, then

Cs(t)(K) = 0

and

CJ(t)(K) = K

for all t ≥ 0.

The Summation Theorem is normally stated as a property of the control coeﬃcients of a
system at steady state. However, the steady state version can be given an equivalent formulation
which does not make reference to control coeﬃcients.

Proposition 4.4 (Summation Theorem – steady state) Suppose the system is at steady
state, so we may drop time dependencies on all terms. If the parameter vector is such that the
parameters only aﬀect the rates (i.e. p = pv) and each column of the matrix εp = ∂v
∂p lies in
the nullspace of N, then

Rs = 0

and

RJ = εp.

Stated in this way, the Theorem can be extended directly to time-varying responses. From

Theorem 1, we have the following.

Corollary 4.5 (Summation Theorem – alternative statement) If the parameter vector
is such that the parameters only aﬀect the rates (i.e. p = pv) and each column of the matrix
εp(t) = ∂v

∂p (t) lies in the nullspace of NR for each t ≥ 0, then

Rs(t) = 0

and

RJ(t) = εp(t)

for all t ≥ 0.

4.2.1 Special Case: εp constant

In special cases, Theorem 1 reduces to more familiar variants. A simpliﬁcation which can be
made to the general case described above is to assume that some derivatives of the rate law
v(·, ·, ·) are constant throughout the evolution of the system. This was the case considered by
Heinrich and Reder in [7], where it was argued that for systems near a stable equilibrium, it
may be reasonable to approximate the derivatives of v by their values at the equilibrium.

If one restricts to the special case where the parameter elasticity εp is constant throughout
the evolution of the system, the control operators need not be deﬁned to act on arbitrary
functions, but only on constant m-vectors. The action of the operators can in this case be
described simply as matrix multiplication, e.g. for any constant m-vector y,

Cs(t)(y) =

LΦ(t)

Φ−1(τ )NR dτ

y

for all t ≥ 0.

t

0
Z

(cid:20)

(cid:21)

In this case, through a slight abuse of notation, time varying control coeﬃcients can be

deﬁned by

Cs(t)

:= LΦ(t)

Φ−1(τ )NR dτ

CJ(t)

:= εs(t)LΦ(t)

Φ−1(τ )NR dτ + Im

t

0

Z

t

0

Z

9

Here, we can state a more standard Summation Theorem:

if the matrix K has columns

which form a basis for the null space of N (equivalently of NR), then for all t ≥ 0,

Cs(t)K = 0
CJ(t)K = K,

where the left-hand-side is interpreted as a matrix product.

4.3 Connectivity Theorem

The Connectivity Theorem for control operators is as follows.

Theorem 2 (Connectivity Theorem) Applying the control operators to the function εs(·)L =
∂v
∂s (·)L yields, for all t ≥ 0,

Cs(t)(εs(·)L) = L(Φ(t) − In0)
CJ(t)(εs(·)L) = εs(t)LΦ(t).

While the right-hand-sides of the equations above may not look familiar, the reader should note
that if Φ(t) = 0 then these reduce to more standard connectivity relations. In what follows we
will consider a case where Φ(t) approaches zero as steady state is reached, so that the classical
MCA result is recovered.
Proof. The deﬁnition of the concentration control operator gives

Cs(t)(

(·)L) = LΦ(t)

Φ−1(τ )NR

L dτ

for all t ≥ 0.

(11)

∂v
∂s

t

0

Z

∂v(τ )
∂s

From equation (7), we have that

Φ(t)

Φ−1(t) = NR

d
dt

(cid:20)

(cid:21)

∂v(t)
∂s

L

for all t ≥ 0.

(12)

Moreover, from the fact that

0 =

In0 =

[Φ(t)Φ−1(t)] =

Φ(t)

Φ−1(t) + Φ(t)

d
dt

d
dt

d
dt

(cid:20)

(cid:21)

d
dt

(cid:20)

Φ−1(t)
(cid:21)

,

we see that

Thus, from (11) and (12),

Φ−1(t) = −Φ−1(t)

Φ(t)

Φ−1(t)

for all t ≥ 0.

d
dt

d
dt

(cid:20)

(cid:21)

Cs(t)(

(·)L) = LΦ(t)

Φ−1(τ )NR

∂v
∂s

∂v(τ )
∂s

L dτ

= LΦ(t)

0

Z
= −LΦ(t)

Φ−1(τ )

Φ(τ )

Φ−1(τ ) dτ

d
dτ

(cid:20)

(cid:21)

Φ−1(τ ) dτ

for all t ≥ 0.

t

t

0

Z

t

d
dτ

0

Z

10

Application of the Fundamental Theorem of Calculus gives

Cs(t)(

(·)L) = −LΦ(t)[Φ−1(t) − Φ−1(0)]

∂v
∂s

= L(Φ(t) − In0)

for all t ≥ 0,

as Φ−1(0) = In0 = Φ(0). With the deﬁnition of the ﬂux control operator, we conclude

CJ(t)(εs(·)L) = εs(t)L(Φ(t) − In0) + εs(t)L
= εs(t)LΦ(t)

for all t ≥ 0.

The Connectivity Theorem is typically stated as a property of the steady state control
coeﬃcients. As is the case for the Summation Theorem, an equivalent formulation can be given
which does not make reference to control coeﬃcients, as follows.

Proposition 4.6 (Connectivity Theorem – steady state) Suppose the system is at steady
state, so we may drop time dependencies on all terms. If the parameter vector is such that the
parameters only aﬀect the rates (i.e. p = pv) and the elasticities satisfy εp = εsL, then

Rs = −L

and

RJ = 0.

There is a direct extension to the general case. From Theorem 2, we have the following.

Corollary 4.7 (Connectivity Theorem – alternative statement) If the parameter vector
is such that the parameters only aﬀect the rates (i.e. p = pv) and the elasticities satisfy εp(t) =
εs(t)L for each t ≥ 0, then

Rs(t) = L(Φ(t) − In0)

and

RJ(t) = εs(t)LΦ(t)

for all t ≥ 0.

4.3.1 Special Case: εs and εp constant

Again, in the special case where the derivatives of v(·, ·, ·) are constant throughout the trajec-
tory, the analysis simpliﬁes.

If εs = ∂v

∂s is constant throughout the evolution of the system, the fundamental matrix

deﬁned by equation (7) can be described explicitly as the matrix exponential

Φ(t) = eNRεsLt

for all t ≥ 0.

In this case (6) provides an explicit formula for the sensitivity coeﬃcients.

Heinrich and Reder considered the case in which both εp and εs are constant along the
trajectory of the system in [7]. In that case, the control operators again need only be deﬁned
on constant vectors, their action reducing to matrix multiplication. That is, given any constant
m-vector y,

Cs(t)(y) =

LeNRεsLt

e−NRεsLτ NR dτ

y

(cid:20)

(cid:20)

t

0

Z

t

0

Z

11

(cid:21)

(cid:21)

=

L

eNRεsL(t−τ )NR dτ

y

for all t ≥ 0.

(13)

The matrix NRεsL is the Jacobian of equation (3). In the case where it is nonsingular, (13)
further reduces to

Cs(t)(y) =

L(eNRεsLt − In0)(NRεsL)−1NR

y

for all t ≥ 0.

This assumption of nonsingularity is rather standard (see, e.g. [14]); in particular, it holds when
the analysis is being carried out near an asymptotically stable equilibrium point.

(cid:2)

(cid:3)

Under these assumptions the control coeﬃcients take the form of the time-varying matrices

Cs(t)
CJ(t)

:= L(eNRεsLt − In0)(NRεsL)−1NR
:= εsL(eNRεsLt − In0)(NRεsL)−1NR + Im,

and the Connectivity Theorem reduces to the form presented in [7]: for all t ≥ 0,

If such a a trajectory further satisﬁes the condition that it is approaching an asymptotically

stable steady state, then it will be the case that

Then, taking limits, we ﬁnd that the control coeﬃcients reduce to their standard forms [14] as
the steady state is approached,

Cs(t)εsL = L(eNRεsLt − In0)
CJ(t)εsL = εsLeNRεsLt.

eNRεsLt = 0.

lim
t→∞

Cs(t) = −L(NRεsL)−1NR

CJ(t) = −εsL(NRεsL)−1NR + Im.

lim
t→∞
lim
t→∞

The time-varying Summation and Connectivity Theorems described above reduce to the stan-
dard statements in this case, as described in [7].

4.4 Control Matrix Equation

Together, the steady state Summation and Connectivity Theorems provide a relationship be-
tween the elasticities and control coeﬃcients. An elegant description of this relation is the
Control Matrix Equation [9]. This equation has a direct generalization in terms of the control
operators described above.

Corollary 4.8 If K is a matrix whose columns form a basis for the nullspace of N, then,
making use of the notation for vectors of operators introduced in the appendix,

CJ(t)
Cs(t)

(cid:20)

(cid:21)

(cid:0)

K −εsL

=

(cid:20)

(cid:1)

K −εs(t)LΦ(t)
0 L(In0 − Φ(t))

(cid:21)

for all t ≥ 0.

2

12

Figure 1: Pathway

Sensitivity − S1
Sensitivity − S2

y
t
i
v
i
t
i
s
n
e
S

0.3

0.2

0.1

0

−0.1

−0.2

−0.3

−0.4

−0.5

−0.6

0

0.5

1

1.5

2.5

3

3.5

4

2
Time

Figure 2: Perturbation in k1

5 Applications and a Caveat for Oscillating Systems

We present applications of the above analysis to three models. The ﬁrst is a “toy” model
– a stable two-species pathway. This analysis provides a straightforward illustration of the
interpretation of time-varying response coeﬃcients. The second example is a simple oscillating
system. In light of this example we provide a brief discussion of the interpretation of sensitivity
coeﬃcients of oscillatory systems. Finally, we provide a “realistic” example – an analysis of a
phototransduction model based on work of Reike and Baylor [2]. Here the power of a time-
varying sensitivity analysis is demonstrated, since we are able to make statements about the
eﬀect of parameter variations on the critical transient behaviour displayed by the model.

5.1 Basic Pathway

Consider the simple pathway shown in Figure 1. Assume mass-action kinetics for each of the
reactions. We will model the system with the following parameter values: v0 = 4, k1 = 3,
k−1 = 0.5, and k2 = 2. Since the ﬂow into S1 is ﬁxed, the system is asymptotically stable to
the equilibrium (S1, S2) = ( 5
3 , 2). We will consider the unscaled response coeﬃcients deﬁned for
initial conditions at the steady state (recall Rs(·) depends on the nominal parameters values and
the initial state). Thus in this case the nominal trajectory will be the steady state trajectory
(S1(t), S2(t)) = ( 5

3 , 2) for all t ≥ 0.

First consider the sensitivity to a perturbation in k1 (shown in Figure 2). The steady state
response of S2 to such a perturbation is nil (since at steady state, S2 = v0
k2 ). However, the
analysis shows (and intuition concurs), that an increase in k1 will produce a transient increase

13

Sensitivity − S1
Sensitivity − S2

Sensitivity − S1
Sensitivity − S2

−0.5

0

0.5

1

1.5

2.5

3

3.5

4

2
Time

Figure 3: Perturbation in k−1

1

0

1

0

0.5

y
t
i
v
i
t
i
s
n
e
S

0.5

y
t
i
v
i
t
i
s
n
e
S

−0.5

0

0.5

1

1.5

2.5

3

3.5

4

2
Time

Figure 4: Perturbation in S1(0)

in S2, which fades gradually as S1 decreases to accommodates the perturbed parameter. The
response in S1 grows to reach its steady state value – a small overshoot can be seen at about
time t = 1. Not surprisingly, the situation for perturbations in k−1 is similar (but reversed),
with a transient decrease in S2 and a positive steady state response in S1 (Figure 3). One can
read from the graphs how the eﬀect on each concentration changes over time. For instance, the
eﬀect of these changes on S2 is felt most strongly at about t = 0.4.

Finally, we consider the eﬀect of a perturbation in the initial value of S1 (Figure 4). Again,
the results are as expected, with transient eﬀects of the perturbations “washing through” the
pathway – the result being no eﬀect on the steady state solution.

Having considered the eﬀect of perturbations around a steady state, we now turn our at-

tention to a system whose time-behaviour is more complex.

5.2 Oscillatory Systems

We consider another simple two species pathway, based on a model which has been employed
in investigations of glycolytic oscillations [6]. The network is shown in Figure 5. The rates are

14

Figure 5: Oscillatory System

S1
S2
Sensitivity − S1
Sensitivity − S2

y
t
i
v
i
t
i
s
n
e
S
/
n
o
i
t
a
r
t
n
e
c
n
o
C

3.5

2.5

3

2

1

0

1.5

0.5

−0.5

0

2

4

8

10

12

6
Time

Figure 6: Perturbation in S1(0)

calculated by mass-action, with the activation by S2 appearing as a multiplicative factor (i.e.
the rate of production of S2 is given by k1S1(1 + Sq
2)). For parameters in a certain range, the
concentrations of S1 and S2 follow oscillatory trajectories; the positive feedback from S2 drives
the oscillation.

We choose nominal parameter values of v0 = 8, k1 = 1, k2 = 5, k3 = 1, and q = 3,
and nominal initial values of (S1(0), S2(0)) = (1, 3). The oscillatory behaviour is a limit cycle
(cf. [6]), and so any initial conditions in a neighbourhood of these points will yield similar
trajectories.

We ﬁrst consider perturbations in initial values. The limit cycle behaviour is stable; after
a perturbation of the initial conditions, the system will tend to the limit cycle as time tends
to inﬁnity. However, unlike the previous case where convergence to the same behaviour meant
that the response shrank to zero as time moved on, for this model (and for systems with
limit cycles in general), the phase of the oscillations depends on the initial values. Thus the
perturbed trajectory is out of phase with the nominal trajectory. The result is a periodic
response – oscillating with the same period as the limit cycle and indicating the diﬀerence
in the trajectories at each point in time. The response for perturbations in S1(0) is shown
in Figure 6, along with the nominal trajectory. The response for perturbations in S2(0) (not
shown) is similar.

Care must be taken in extending this set of ideas to perturbations of kinetic parameters of

15

y
t
i
v
i
t
i
s
n
e
S
/
n
o
i
t
a
r
t
n
e
c
n
o
C

8

6

4

2

0

−2

−4

−6

−8

−10

−12

0

S1
S2
Sensitivity − S1
Sensitivity − S2

2

4

8

10

12

6
Time

Figure 7: Perturbation in v0

an oscillatory system. In general, the results of such analysis may not be useful. The response
coeﬃcients for perturbations in v0 are shown (again, along with the nominal trajectories) in
Figure 7. While the sensitivities match the period of the system in a quasi-periodic behaviour,
they oscillate more and more widely, and continue to do so as time tends to inﬁnity. As a result,
these responses are not especially useful when trying to predict the eﬀect of ﬁnite perturbations
on a real system. The trouble, as pointed out in [4, 11], is that after a perturbation in a kinetic
parameter, the system again exhibits a limit cycle behaviour, but this time with a diﬀerent
period. When two trajectories follow a similar cycle with diﬀerent periods, they will always
reach a point when they are completely out of phase, no matter how small the diﬀerence in
their periods. As the perturbations get smaller, the diﬀerence in period shrinks as well, so that
it takes longer and longer for this “point of maximum diﬀerence” to be reached. However, as
long as the perturbation is nonzero, that point will always be achieved. Since the response
coeﬃcient is deﬁned as the limit of the ratio of the diﬀerence in trajectories to the size of
the perturbation as the perturbation goes to zero, we ﬁnd that as time tends to inﬁnity, the
response approaches this “maximum diﬀerence” divided by zero, that is it diverges.

In [4] and [11], some clever strategies have been devised for interpreting the sensitivities of
oscillatory systems. Kholodenko et al. treat the case of asymptotically stable systems under
the inﬂuence of periodic external forces in [11]. Such systems exhibit limit cycles whose period
is set by the external force. In this case, perturbations in kinetic parameters do not result in
changes in the period, and so the response coeﬃcients are more meaningful. In [4], Demin et
al. treat perturbations of autonomously oscillating systems by measuring the response in the
Fourier coeﬃcients of the trajectory. Fourier control coeﬃcients are introduced, which give a
useful interpretation of sensitivities for any periodic systems.

Having considered the interpretation of response coeﬃcients for steady state and oscillating
systems, we now treat the most interesting case – a system in which the behaviour of interest
occurs during transients.

5.3 Phototransduction

Phototransduction is the process by which organisms convert light signals into nerve signals. As
in all signal transduction pathways, the steady state behaviour of the system is less interesting

16

Figure 8: Phototransduction Pathway

than its transient response. We will illustrate a sensitivity analysis of the single photon response
in a simple model of phototransduction loosely based on previous modeling work in [2] and [13].
The response mechanism is modelled as follows. The system is at steady state in the
absence of light input. A photon of light activates a rhodopsin molecule to start the system
response. Activated rhodopsin (Ra) in turn activates a heterotrimeric G protein (G). The
activated G protein α-subunit (Ga) dissociates from its beta-gamma subunits (βγ) and then
binds with a cGMP phosphodiesterase (PDE) to form the active PDEa-Ga complex. The PDE
enzyme catalyzes the degradation of cGMP in the cell. Activation by Ga greatly enhances the
catalytic activity of PDE, resulting in a reduction in cellular levels of cGMP. The cGMP-gated
ion channels in the cell membrane then close, causing a net eﬄux of calcium ion (Ca), a net
outward current (J), and a corresponding hyperpolarization of the cell’s membrane.

The deactivation phase of the response is modelled as follows. Activated G protein α
subunit, either free (Ga) or bound (PDEa-Ga), is converted to the deactivated form (Gd).
Finally, the deactivated G protein (Gd) recombines with the beta-gamma subunit (βγ) to form
the heterotrimeric G protein (G).

The model incorporates four independent variables (G, Ga, PDEa-Ga, and cGMP) along
with ﬁve dependent variables (Gd, PDE, βγ, Ca, and J) and several parameters as indicated
below. Note, J denotes not the actual current but rather the magnitude of the deviation from
the resting current (DarkJ). The input to the system is the time-varying level of Ra. The
network is shown in Figure 8. The dynamics are given by

v1 = kGa Ra
v2 = kGd Ga
v3 = kG1 βγ Gd
v4 = kP DE1 Ga PDE − kP DE1m PDEa-Ga
v5 = kP DE2 PDEa-Ga

v6 =

kcG

Ca
Kg

q

1 +

v7 = kcGd0 cGMP + kcGd cGMP PDEa-Ga

(cid:17)

(cid:16)

17

Gd = Gtot − G − Ga

PDE = PDEtot − PDEa-Ga

Ca =

βγ = βγtot − G
kCi
kCo (cid:18)
J = DarkJ

cGMP
K0.5 (cid:19)
1 −

3

3

cGMP
K0.5 (cid:19)

!

 

(cid:18)

For simulations, we choose nominal parameter values and initial conditions as follows

kGa = 1.0 ms −1
kG1 = 0.002 ms −1 molecules −1 µm2
kP DE1m = 0.000001 ms −1
kcG = 0.058 mM ms −1
q = 2.0
kcGd = 0.000004 ms −1 molecules −1 µm2 kCo = 1.44
kCi = 0.72

kGd = 0.005 ms −1
kP DE1 = 0.0003 ms −1 molecules −1 µm2
kP DE2 = 0.005 ms −1
Kg = 0.2 mM
kcGd0 = 0.002 ms −1

K0.5 = 4 mM

Gtot = 3000 molecules µm−2
βγtot = 3000 molecules µm−2

PDEtot = 500 molecules µm−2
DarkJ = 12.0 pA

G (0) = 3000 molecules µm−2
PDEa-Ga (0) = 0 molecules µm−2

Ga (0) = 0 molecules µm−2
cGMP (0) = 4.0 mM

The single activated rhodopsin molecule excited by a single photon is represented by a

square 100ms pulse,

0 0 ≤ t ≤ 100
1 100 < t ≤ 200
0 200 < t

,

Ra (t) =






measured in molecules µm−2.

We ﬁrst consider the eﬀect of a perturbation in the parameter kGa, which describes the
eﬀect of activated rhodopsin on the activation of G. The nominal trajectories for G, Ga and
(There is no activity until time t = 100 since
PDEa-Ga are shown in Figure 9 and Figure 10.
the system is at steady state while the input is zero). Figure 11 shows the absolute (unscaled)
sensitivity of G, Ga and PDEa-Ga to perturbations in kGa. As expected, an increase in kGa
causes a decrease in the level of G and an increase in the levels of Ga and PDEa-Ga. These
responses track the trajectories themselves, with the largest response being observed when each
species is farthest from its “resting” (steady) state.

The primary signal of interest in this system is the level of current (J) produced by the
input. The time history of the current produced by the nominal input and parameter values
is shown in Figure 12. The eﬀect of perturbations in three diﬀerent parameters on J are
shown in Figure 13. Since this analysis is meant to compare the eﬀects of the changes, the
relative (i.e. scaled) responses are shown. The relative strengths of perturbations in the three
parameters kGa, kGd and kP DE1 are immediate. For instance, to increase the response in J, it
is clear that an increase in kGa will have far more impact than an equal (relative) increase in
kP DE1 or decrease in kGd.

18

)

2

i

n
o
r
c
m
/
s
e
u
c
e
o
m

l

l

(
 
n
o

i
t

a
r
t

n
e
c
n
o
C

3000

2990

2980

2970

2960

2950

2940

2930

2920

2910

0

80

70

60

50

40

30

20

10

)

2

i

n
o
r
c
m
/
s
e
u
c
e
o
m

l

l

(
 
n
o
i
t
a
r
t
n
e
c
n
o
C

0

0

80

60

40

20

0

−20

−40

−60

−80

0

)
s
m
2
−

 

i

n
o
r
c
m
 
s
e
u
c
e
o
m

l

l

(
 
s
e
t
i
v
i
t
i
s
n
e
S
 
d
e
a
c
s
n
U

l

100

200

300

400

600

700

800

900

1000

500
Time (ms)

Figure 9: Nominal trajectory for G

G

Ga
PDEa−Ga

100

200

300

400

600

700

800

900

1000

500
Time (ms)

Figure 10: Nominal trajectory for Ga, PDEa-Ga

sensitivity −− G
sensitivity −− Ga
sensitivity −− PDEa−Ga

100

200

300

400

600

700

800

900

1000

500
Time (ms)

Figure 11: Absolute sensitivities to perturbations in kGa

19

0

0

100

200

300

400

600

700

800

900

1000

500
Time (ms)

Figure 12: Nominal trajectory of J

J

Ga

Gd

k
k
k

PDE1

0.6

0.4

0.2

)

A
p
(
 
t
n
e
r
r
u
C

0.045

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

0

i

i

l

)
s
s
e
n
o
s
n
e
m
d
(
 
s
e
t
i
v
i
t
i
s
n
e
S
 
d
e
a
c
S

l

−0.005

0

100

200

300

400

600

700

800

900

1000

500
Time (ms)

Figure 13: Relative sensitivities of J

20

6 Conclusion

Since its introduction, Metabolic Control Analysis has proven to be a useful tool in discovering
the distribution of control in biochemical systems at steady state. In this paper, the compu-
tation and interpretation of system sensitivities has been considered for arbitrary trajectories.
The main results of MCA (Summation and Connectivity Theorems) have been shown to have
a valid interpretation not just at steady state, but throughout the system dynamics. In analyz-
ing the distribution of control for systems in which transients are of interest, the time varying
sensitivity may prove to be a valuable tool in determining system behaviour.
Acknowledgment The authors would like to thank Tau-Mu Yi for providing the phototrans-
duction model and for many useful discussions.

A Appendix: A Primer on Operators

A function from R into R is deﬁned as a rule which assigns to each real number∗ a unique real
number in its range, for example, the function h(·) deﬁned by h(t) = t2 for each t ∈ R. This
can be generalized by allowing functions to act on pairs of numbers, or n-tuples in general (e.g.
g(x, y, z) = x2 − yz).

The concept of a function can be extended to yield an operator, which takes a function as

its argument. For example, we could consider the operator T (·) deﬁned by integration:

1

T (f (·)) =

f (s) ds

0
Z
for each function f (·) deﬁned on R (provided the integral of f exists). Deﬁne the functions h
and r by h(t) = t2 and r(t) = cos(t). Then

T (h(·)) =

and

T (r(·)) =

cos(s) ds = sin(1).

1

s2 ds =

1
3

,

0

Z

1

0
Z

Another example of an operator is function evaluation. Deﬁne G by G(f (·)) = f (5) for each
function f (·). Then G(h(·)) = 25 and G(r(·)) = cos(5).

The operators T and G deﬁned above are commonly referred to as functionals, since they
map functions to numbers. More generally, by allowing an operator to have a second, scalar ar-
gument, we can construct operators which map functions to functions. For example, extending
T to depend on a scalar argument t, we could deﬁne

T by

T (t, f (·)) =

f (s) ds.

b

t

0
Z

An alternative notation is
a function deﬁned for t ≥ 0:
b
s2 ds =

T (t)(h(·)) =

t

0

Z

b

t3
3

b
and

As another example, we could modify G by deﬁning
and

G(t)(r(·)) = cos(2t), for all t ∈ R.

∗The symbol R denotes the set of real numbers.

b

b

21

T (t)(f (·)) =

T (t, f (·)). Thus, given a real-valued function, T returns

T (t)(r(·)) =

cos(s) ds = sin(t).

t

0
Z

G(t)(f (·)) = f (2t). Then

G(t)(h(·)) = (2t)2

b

As described in the main text, we can extend the action of operators to row vectors in a

component-wise fashion, for example

T (t)([ h(·)

r(·) ]) = [

sin(t) ].

t3
3

Moreover, we can combine operators column-wise, yielding the notation

b

([ h(·)

r(·) ]) =

t3
3
(2t)2

(cid:20)

sin(t)
cos(2t)

.

(cid:21)

"

T (t)
G(t) #
b
b

Particular cases of operator actions which appear in the main text are:

• constant functions as arguments to operators. For example, deﬁning the function k(·) by

k(t) = 3 for all t ∈ R, we have

T (k(·)) = 3,

T (t)(k(·)) = 3t.

Using a simpliﬁed notation, we can write

b

T (3) = 3,

T (t)(3) = 3t;

b
• operators which act by multiplication. For example, deﬁne the operator H by H(t)(f (·)) =

H(t)(h(·)) = 3t3

H(t)(r(·)) = 3t cos(t)

H(t)(3) = 9t.

3tf (t). Then

References

[1] Acerenza, L., Sauro, H. M. and Kacser, H., “Control analysis of time-dependent metabolic

systems,” Journal of Theoretical Biology, 151 (1989), pp. 423–444.

[2] Rieke, F. and Baylor, D. A., “Origin of reproducibility in the responses of retinal rods to

single photons,” Biophysical Journal, 75 (1998), pp. 1836–1857.

[3] Boyce, W. E. and DiPrima, R. C., Elementary Diﬀerential Equations and Boundary Value

Problems, Fifth Edition, John Wiley & Sons, New York, 1992.

[4] Demin, O. V., Westerhoﬀ, H. V. and Kholodenko, B. N., “Control analysis of stationary

ﬁxed oscillations,” Journal of Physical Chemistry B, 103 (1999), pp. 10695–10710.

[5] Fell, D.A., “Metabolic control analysis – a survey of its theoretical and experimental de-

velopment,” Biochemical Journal, 286 (1992), pp. 313–330.

[6] Heinrich, R., Rapoport, S. M. and Rapoport, T. A., “Metabolic regulation and mathemat-

ical models,” Progress in Biophysics & Molecular Biology, 32 (1977), pp. 1–82.

[7] Heinrich, R. and Reder, C., “Metabolic control analysis of relaxation processes,” Journal

of Theoretical Biology, 151 (1991), pp. 343–350.

[8] Heinrich, R. and Schuster, S., The Regulation of Cellular Systems, Chapman & Hall, New

York, 1996.

22

[9] Hofmeyr, J-H. S., “Metabolic control analysis in a nutshell,” Proceedings of the Interna-
tional Conference on Systems Biology, Pasadena, California, November 2000, pp. 291–300.

[10] Kascer, H. and Burns, J. A., “The control of ﬂux”, Symp. Soc. Exp. Biol., 27 (1973),

pp. 65-104.

[11] Kholodenko, B. N., Demin, O. V. and Westerhoﬀ, H. V., “Control analysis of periodic
phenomena in biological systems,” Journal of Physical Chemistry B, 101 (1997), pp. 2070–
2081.

[12] Kohn, M. C., Whitley, L. M. and Garﬁnkel, D., “Instantaneous ﬂux control analysis for

biochemical systems,” Journal of Theoretical Biology, 76 (1979), pp. 437–452.

[13] Pugh, Jr., E. N. and Lamb, T. D., “Ampliﬁcation and kinetics of the activation steps in

phototransduction,” Biochemica et Biophysica Acta, 1141 (1993), pp. 111–149.

[14] Reder, C., “Metabolic control theory: a structural approach,” Journal of Theoretical Bi-

ology, 135 (1988), pp. 175–201.

[15] Reijenga, K. A., Westerhoﬀ, H. V., Kholodenko, B. N. and Snoep, J. L., “Control anal-
ysis for autonomously oscillating biochemical networks,” Biophysical Journal, 82 (2002),
pp. 99–108.

[16] Sauro, H. M., “Jarnac: A system for interactive metabolic analysis,” in Animating the
Cellular Map: Proceedings on BioThermoKinetics, Hofmeyr, J.-H. S., Rohwer, J. M. and
Snoep, J. L., eds., Stellenbosch University Press, 2000.

[17] Savageau, M.A., Biochemical Systems Analysis. A Study of Function and Design in Molec-

ular Biology, Addison-Wesley, Reading, MA.

[18] Tomovi´c, R., Sensitivity Analysis of Dynamic Systems, McGraw-Hill, New York, 1963.

23


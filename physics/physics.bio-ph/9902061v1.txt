9
9
9
1
 
b
e
F
 
2
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
1
6
0
2
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Statistical properties of spike trains: universal and stimulus-dependent aspects

Naama Brenner1, Oded Agam2, William Bialek1 and Rob de Ruyter van Steveninck1
1 NEC Research Institute, 4 Independence Way, Princeton, NJ 08540
2 Racah Institute of Phsyics, Hebrew University, Jerusalem

Statistical properties of spike trains measured from a sensory neuron in-vivo are studied experi-
mentally and theoretically. Experiments are performed on an identiﬁed neuron in the visual system
of the blowﬂy. It is shown that the spike trains exhibit universal behavior over short time, modulated
by a stimulus-dependent envelope over long time. A model of the neuron as a nonlinear oscillator
driven by noise and an external stimulus, is suggested to account for these results. The model
enables a theoretic distinction of the eﬀects of internal neuronal properties from eﬀects of external
stimulus properties, and their identiﬁcation in the measured spike trains. The universal regime is
characterized by one dimensionless parameter, representing the internal degree of irregularity, which
is determined both by the sensitivity of the neuron and by the properties of the noise. The envelope
is related in a simple way to properties of the input stimulus as seen through nonlinearity of the
neural response. Explicit formulas are derived for diﬀerent statistical properties in both the univer-
sal and the stimulus-dependent regimes. These formulas are in very good agreement with the data
in both regimes.

I. INTRODUCTION

Many cells in the nervous system respond to stimula-
tion by generating action potentials (spikes). Time se-
quences of these spikes are the basis for encoding infor-
mation and for communication between neurons [1]. A
pattern of spikes across time contains, in addition to the
message being encoded, the signature of the biophysical
spike generation mechanism, and of the noise in the neu-
ron and its environment [2]. These factors are generally
inter-related in a complicated way, and it is not clear how
to disentangle their eﬀect on the measured spike train.

The biophysical mechanism for generating action po-
tentials was ﬁrst described successfully by Hodgkin and
Huxley [3]. Their description accounted for the stereo-
typed shape of an action potential, which is a robust
property, largely independent of external conditions. The
Hodgkin Huxley model describes the neuron as a complex
dynamical system; sustained ﬁring (a continuous train of
spikes) comes about when the dynamical system is driven
into an oscillatory mode. This picture is consistent with
experiments on isolated neurons: many of these tend to
ﬁre periodic spike trains in response to direct current
injection, implying an oscillator like behavior. The fre-
quency of these trains is deterministically related to the
strength of the applied current; Adrian (1928) suggested
long ago that this property could be used to code the
strength of the input. Diﬀerent neurons vary in the shape
of the response function relating frequency to input. Fol-
lowing Hodgkin and Huxley, many microscopic models of
the neuron were constructed in the same spirit [2]. One
aim of this type of modeling is to produce the diﬀerent
frequency-current (f /I) response curves by ﬁtting model
parameters.

Spike trains measured in vivo, however, show a very
diﬀerent behavior: many neurons seem to ﬁre stochas-
tically, even when external conditions are held ﬁxed.

This fact initiated what seems to be an unrelated line
of research, that of describing spike trains by models of
stochastic processes [5,6]. These models can sometimes
describe correctly statistical properties of the spikes
trains, such as the distribution of intervals, but in gen-
eral the parameters of the models remain unrelated to
physiological characteristics of real systems [7].

Several fundamental questions concerning the statis-
tical properties of spike trains thus remain unresolved,
despite the large literature on this subject: How is the
periodic behavior of the isolated neuron to be reconciled
with the more irregular behavior in a complex network?
What is a useful characterization of the degree of this
irregularity, and how does it depend on external condi-
tions? How sensitive are the statistical properties to the
microscopic biophysical details of the neuron, and to the
statistics of the noise? Can eﬀects of the sensory stimulus
be separated and recognized at the output?

Here we present a theory which provides some answers
to the above questions. We use the notion of a frequency
function to describe the neuron’s response [8], and con-
nect it to the stochastic ﬁring in a network through the
introduction of noise. Under some conditions we ﬁnd that
the statistical properties of spike trains are universal on
the time scale of a few spikes [9]. This means that they
are independent of the details of the internal oscillator, of
the noise and of external stimulation. All these are cap-
tured by a single dimensionless parameter, related to the
phase diﬀusion of the oscillator; this parameter charac-
terizes the internal irregularity of the point process. On
the time scale of many spikes, the universal behavior is
modulated by an envelope reﬂecting the input stimulus.
We present experimental data for the statistical proper-
ties of spikes trains measured from an identiﬁed motion
sensor in the visual system of the blowﬂy, under various
external stimulation. These data are shown to be very
well described by the theory.

1

The paper is organized as follows: In section 2 we de-
ﬁne our model, and show how the frequency of the oscil-
lator is related to the rate of the measured point process.
In Section 3 we consider the statistical properties of the
model, when the ﬂuctuations in the inputs are rapid rel-
ative to the typical interspike time 1/r. We derive ex-
plicit formulas for the statistical properties of the spike
train, and show that all details aﬀect these properties
only through the irregularity parameter.
In Section 4,
we consider the case of an additional slow time scale in
the inputs, which is much longer than 1/r. We show that
the conditional rate can be approximated by a product
of two distinct parts: a universal part, depending only
on the irregularity parameter, and a stimulus-dependent
part, which modulates it. In each section a comparison
of the theoretic results with measurements from the ﬂy
is presented.

II. FREQUENCY INTEGRATOR

A sequence of spikes will be described as a train of

Dirac δ-functions:

ρ(t) =

δ(t

tk).

−

Xk

(1)

In this approximation the height and shape of the action
potentials are neglected, and all the information is con-
tained in their arrival times: the spike train is a point
process. If the system is driven by a signal s(t) and a
noise n(t), both continuous functions of time, then we
can imagine that the neuron evaluates some functional
[s(t), n(t)] and produces a spike when this crosses a

F
threshold:

ρ(t) =

[s(t′), n(t′)]

(2)

d
F
dt

.

k

−

!

δ

 F

Xk

Formula (2) describes a very general class of models: a
crosses a ﬁxed threshold, and
spike is generated when
the process resets after spiking. The operator
can be
linear or nonlinear, deterministic or stochastic, and can
depend on the history of the signal and the noise in a
complicated way [10]. We focus on the following more
speciﬁc form of

F

F

:

F

t

0
Z

constant stimulus s, in the absence of noise, our model
neuron generates a periodic spike train with a frequency
f (s), consistent with the behavior in isolation. Starting
from a microscopic level of modeling, many parameters
need to be tuned to produce a required form of the f /I
relation [12]. Here we use the f /I relation as a phe-
nomenological description of the neuron, and base the
statistical theory on this description.

Now we would like to “embed” our model neuron in a
noisy environment, such as a complex sensory network,
while it is still subject to a constant stimulus s. In gen-
eral there can be many noise sources in such a network:
the signals coming in from the external world are not
perfect, the sensory apparatus (such as photoreceptors)
is noisy, connections between cells in the network intro-
duce noise, and ﬁnally the cell itself can generate noise
(for example, channel noise [19]). We introduce noise as
an additional random function n(t) added to the input.
This simpliﬁed scheme is justiﬁed by the fact that ﬁnal
results do not depend on the details of the noise distri-
bution, therefore n(t) is understood as an eﬀective noise.
Retaining the notion of a local frequency, the noise
causes frequency modulations in the spike train. Under
the conditions s = const, n = 0 the oscillatory behavior is
related to some periodic trajectory in parameter space.
Assuming that this trajectory is stable, the addition of
n(t) will cause the system to occupy a volume in param-
eter space surrounding this trajectory. The phase of the
oscillator will not advance at a constant rate f (s), but
instead will be given by

˙Φ(t) = f [s+n(t)].

(4)

The strength of the noise and the sensitivity of f (
) to
·
changes in the inputs, both determine the frequency mod-
ulation depth, or the amount of randomness in the phase
advancement.

The frequency function is an internal property of the
neuron. One would like to relate it to the ﬁring rate
function in the presence of noise, which can be measured
experimentally. Considering still the case of a constant s
and introducing the average over noise
, the average
spike train is

h· · ·i

ρs(t)
i
h

=

Xk *

δ (Φ(t)

k) ˙Φ(t)

.

(5)

−

+

[s(t), n(t)] =

f [s(u) + n(u)] du

Φ(t),

(3)

F

≡

Using the Poisson summation formula,

≥

where f
0 is the frequency response function charac-
terizing the neuron. This model is related to Integral
Frequency Pulse Modulation models and to the stan-
dard integrate-and-ﬁre model [11,10,7]. The motivation
for deﬁning a deterministic frequency response function
comes from the measured behavior of isolated neurons in
response to direct current injection. When driven by a

δ(x

k) =

−

ei2πmx

Xk

m
X

we can write the average spike train as

ρs(t)
i
h

=

ei2πmΦ(t) ˙Φ(t)
.
i
h

m
X

(6)

(7)

2

For a stationary noise n(t), and time long enough for
the system to be in a steady state, this average is inde-
pendent of the time t. We assume that the noise has a
short correlation time τn; this implies that the noise can
have an arbitrary distribution at each time, but that it is
uncorrelated with the noise value at a time much later.
For t larger than the noise correlation time, τn, Φ(t) is
an integral of many independent random variables, and
is approximately Gaussian by the central limit theorem.
Then, one can substitute the average of the exponent by
the exponent of the two ﬁrst cumulants:

ei2πmΦ(t)
h
δΦ(t)2
h

where
=
lants of Φ(t) are:

i

i ≈
Φ(t)2
h

ei2πmhΦ(t)i−2π

2

2

m

hδΦ(t)

i

2

(8)

2. The ﬁrst two cumu-
Φ(t)
i

i − h

Φ(t)
i
h

=

δΦ(t)2
h

i

=

t

0 h
Z
t

t

0
0 Z
Z

f [s+n(t′)]dt′

= rt

i

(9)

dt′dt′′

δf [s+n(t′)]δf [s+n(t′′)]
.
i
h

But f is correlated only over a short time, on the order of
τn and therefore the double integral can be approximated
by

δΦ(t)2
h

i ≈

0
Z
Dt,

≈

∞

t

d¯t

−∞

Z

dζ

δf (s + n(¯t+
h

ζ
2

))δf (s + n(¯t

−

ζ
))
2
i
(10)

δf 2
with D = τnh
time t arbitrarily large, we ﬁnd

. Using Eq. (8) in (7), and taking the
i

∗

∗

=

(11)

= [f

G](s),

f (s + n)
i
h

ρ(s)
h
i
where
denotes convolution and G is the distribution
function of the noise at a given point of time. For a sta-
tionary noise distribution, this implies that the average
ﬁring rate as a function of signal s has the form of f (s)
smeared by the noise. This result is independent of the
exact form of the function f or of the noise distribution.
It relates the f /I curve to a measurable quantity, an av-
erage ﬁring rate in the presence of noise; in the measured
quantity, much of the ﬁne details of f will be smeared by
the noise.

In order to compare to data, we must now identify the
stimulus s(t) in the experiment. If the neuron is isolated
in a dish and current is injected directly into the cell,
then s(t) should be naturally identiﬁed with this cur-
rent. In experiments with an intact sensory system one
would like to connect s(t) with the external stimulus. In
the following discussion we will use as our test case the
visual system of the blowﬂy. In our experiment, a live
immobilized ﬂy views various visual stimuli, chosen to
excite the response of the cell H1. This large neuron is lo-
cated several layers back from the eyes, and receives input
through connections to many other cells. It is identiﬁed

as a motion detector, responding optimally to wide-ﬁeld
rigid horizontal motion, with strong direction selectivity
[21,22]. The ﬂy watches a screen with a random pattern
of vertical dark and light bars, moving horizontally with
a velocity s(t). We record the electric signal of H1 extra-
cellularly, and register a sequence of spike timings
tk}
[23].

{

An advantage of this system is that we have empiri-
cal knowledge of what stimulus feature is relevant to this
it responds to wide ﬁeld motion in the horizontal
cell:
direction. Thus we may identify the input to the cell
directly with a one dimensional external signal, the mo-
tion of the pattern on the screen. Figure 1 shows the
ﬁring rate averaged over many presentations of the same
stimulus, both as a function of time (Fig. 1a) and as a
function of the instantaneous value of s(t) (Fig. 1b). If
the velocity on the screen is slowly varying in time, the
ﬁring rate of H1 follows this velocity closely: Fig. 1a
of
shows the time-dependent ﬁring rate r(t) =
H1 in response to a random signal. Since the signal varies
very slowly, one may use Eq. (11) locally, and if we plot
the ﬁring rate as a function of the instantaneous value of
s, we see the smoothed response function f (s) (Fig. 1b).
If the signal varies rapidly, ﬁltering mechanisms at early
stages of the visual pathway become important, and s(t)
which directly drives the cell is a modiﬁed version of the
velocity on the screen. In this case, it is more diﬃcult
to map out directly the response curve [24]. It should be
noted, however, that we present the response function in
Fig. 1 only for purpose of illustration; in what follows
we shall assume that this function is well deﬁned, but
we will not need to know its exact shape. Moreover, our
results will show that to account for the statistical prop-
erties of the spike trains very little information about the
response function is required.

ρ[s(t)]
i
h

(a)

200

(b)

)
c
e
s
/
s
e
k
p
s
(
 
 
 
r

i

 200

 0

r(t)
s(t)

100

0

0

2

4
t  (sec)

6

−3

0

3

6

s  (deg/sec)

FIG. 1. Firing rate of H1 as a function of time, averaged
over trials: r(t) = hρ(t)i [dots], compared to the input signal
s(t) [solid line], for a slow signal (a) and a fast signal (c). We
repeat the signal many times to obtain a sampling of the noise
ensemble. The units of velocity are spacings of the compound
eye’s lattice (ommatidia) per second.

3

It should be noted that the response function of the
neuron f (
) is not a ﬁxed property, but may change with
·
external conditions. The theory presented here is valid
within a steady state, in which f (
) takes a particular
·
form and does not change with time. Rather than being
a limitation, the context-dependence of f (
) opens the
·
possibility to investigate adaptive changes in the neural
response, when adjusting to diﬀerent steady states [24].

III. THE UNIVERSAL REGIME

In this section we present the statistical theory for the
case of a constant input signal, s(t) = s, and a random
short correlated noise of arbitrary distribution. The re-
sult is a renewal process with special symmetry prop-
erties, reﬂecting the underlying neuronal oscillator. We
provide explicit formulas for the correlation function and
the number variance. As will be shown in later sections,
the results obtained here are valid in a limited time range
if the stimulus s(t) is time dependent.

A. Distribution of Intervals

The distribution of inter–spike intervals is the distri-
bution of times for which Φ(t) = 1. Due to the unidirec-
tionality of the phase diﬀusion, these times are unique,
and so the probability density is simply

P (t) =

δ(t

Φ−1(1))

=

δ(Φ(t)

1) ˙Φ(t))

. (12)

*

−

+

*

−

+

It is more convenient to calculate the cumulative distri-
bution,

F (t) =

P (t′)dt′

(13)

t

0

Z

which can be expressed in terms of the step function, Θ,
and its Fourier transform:

F (t) =

Θ(Φ(t)
h

1)
i

−

=

∞

dp
2πip

−∞

Z

e−i2πp

ei2πpΦ(t)
h

.
i

F (t) =

∞

−∞

dp
2πip

ei2πprt−2π

2

2
p

Dt

=

1 + erf

Z
1
2

(cid:18)

rt
1
−
√2Dt (cid:19)(cid:19)

(cid:18)

and the interval density is

P (t) =

rt + 1
√8πDt3

2

e−(rt−1)

/2Dt.

(14)

(15)

(16)

4

In Appendix 1 this result is derived for a discrete sum of
non-negative random variables, directly from the central
limit theorem. The density (16) depends on two parame-
ters, the average ﬁring rate r and the diﬀusion coeﬃcient
D, both of dimensionality [time]−1. In the derivation, we
used only the non–negativity of the frequency integrator
to write down Eq. (12), and the Gaussian approxima-
tion (8) for Φ(t); therefore changes in the model which
retain these properties will not aﬀect the interval den-
sity. Eq. (16) is similar to the ﬁrst passage time of the
Wiener process [7,31]: it has the same exponent, but this
exponent is multiplied by a diﬀerent function of t. As
will be shown below, this results in signiﬁcantly diﬀerent
symmetry properties of the function.

To understand the qualitative properties of the point
process, it is convenient to examine it in dimensionless
time units, namely to deﬁne the time such that the av-
erage rate is 1. We denote this time as x = rt. In this
representation, the statistics depend on one dimension-
less parameter, γ,

γ =

=

D
r

δf 2
τnh
f
h

i

i

and the interval density is

P (x) =

x + 1
8πγx3

2

e−(x−1)

/2γx.

(17)

(18)

p

The parameter γ governs the decay of the interval den-
sity both near the origin and at large t. For small
γ these decays are strong, indicating a narrow density,
and for large γ they are weaker and the distribution is
broader. More formally, the moment generating func-
tion, G(λ) =
, of (18) can be calculated using the
i
integral representation,

e−λx
h

P (x) =

dp(1 + iπγp)ei2π(x−1)p−2π

2

2
γp

x.

(19)

∞

−∞

Z

The result is

G(λ) =

1 +

1
2

(cid:18)

1
√1 + 2λγ

(cid:19)

1

γ (1−√1+2λγ),

e

(20)

x
i
h
δx2
h

i

= 1 + γ/2

= γ +

γ2.

5
4

(21)

(22)

Note that the average interval length is not equal to the
inverse of the average rate, which is 1 in our units. In
general, inversion does not commute with averaging; for
small γ, however, the inverse average and the average of
the inverse are similar.

Using the Gaussian approximation (8) for Φ(t), we ﬁnd
that

and the ﬁrst two moments are

p

/
i

δx2
h

Deﬁning the coeﬃcient of variation by

x
i
h
[34], it is seen that the coeﬃcient of variation is approxi-
mately proportional to √γ for small γ, and to γ for large
γ. It can take on arbitrarily small and large values de-
pendent upon γ. This is a more quantitative way of see-
ing that our family of distributions interpolates between
low–variability (or regular) and high–variability (or ir-
regular) limits. It should be noted that this distribution
arises from a simple integration model of many indepen-
dent inputs [35]. As noted already by several authors
[12,8], it is not only the properties of the inputs but also
of the internal neural response that determine the degree
of irregularity of a spike train. In our model, the param-
eter controlling this irregularity (17), accounts for both
these eﬀects.

→

The fact that γ controls the behavior of the density
(18) at both tails, takes the quantitative form of an in-
variance under the transformation x
1/x, with the
Jacobian properly accounted for. This implies that the
cumulative distribution of the intervals between succes-
sive spikes is identical to the distribution of inverse in-
tervals, when both are measured in dimensionless units.
Since our spike train is a renewal process, this invariance
of the interval distribution implies an invariance of the
process as a whole. In general, for a point process with
, one may construct the dual process
time intervals
with intervals
; this has the natural interpreta-
tion of local frequencies. The process deﬁned by (18) is
self-dual: all statistical properties of the dual process are
identical to the original one. For the process deﬁned by
(16), this invariance holds up to a global rescaling of the
axis. Fig. 2 shows that this is indeed a property of the
measured data: the cumulative distributions of intervals
and inverse intervals overlap when plotted in dimension-
less units.

xi}
{
1/xi}
{

−

exactly k
1 other spikes in between them. In their land-
mark paper, Gerstein and Mandelbrot (1964) observed
that the scaled interval distributions of low orders in spike
trains from the cat cochlear nucleus, have a similar shape.
This observation motivated them to suggest the random
walk model for the membrane voltage. In our model, we
can calculate the scaled interval distribution directly: it
is the distribution of times for which Φ(t) = k,

Fk(t) =

1 + erf

1
2

(cid:18)

rt
k
−
√2Dt (cid:19)(cid:19)

.

(cid:18)

(23)

(16)
In this notation, the interval distribution of Eq.
is the scaled distribution of order 1. Figure 3a shows
the ﬁrst three scaled interval distributions, as measured
experimentally, together with Eq. (23). The two ﬁtting
parameters of the theory, the average rate r and the diﬀu-
sion constant D, are ﬁtted once for all three graphs. Ac-
cording to the observation of Gerstein and Mandelbrot,
these curves should have the same shape after rescal-
ing the time axis to dimensionless units rt/k. Figure 3b
shows the scaled interval distributions in dimensionless
time units. These curves have a similar shape, but do not
quite overlap. As is easily seen from Eq. (23), the trans-
t/k gives a function of the same general
formation t
form, but with a diﬀerent value of D. Thus the family
of curves Fk(t) with parameters r, D obey the following
equation:

→

F (r,D)
k

(r, D
(t/k) = F
1

k )

(t),

(24)

n
o

i
t

i

u
b
i
r
t
s
D
e
v
i
t

 

l

a
u
m
u
C

(a)

1.0

0.5

0.0

0

(b)

intervals
frequencies

where the superscripts denote the dependence on the pa-
rameters. In Figure 3b, it is clearly seen that the steep-
ness of the curve increases with increasing k, consistent
with a decrease in the diﬀusion constant D. The point
rt/k = 1 is where the distributions cross, in agreement
with the theoretical equation (24).

1

2

0

1

2

3

                    normalized interval/frequency

FIG. 2.

A symmetry of the spike train point process:
the distribution of intervals between neighboring spikes is the
same as the distribution of the inverse intervals (local frequen-
cies), up to a constant scaling factor. Data are shown from
two experiments with constant velocity stimuli, of magnitude
10.5 (a) and 0.16 (b) deg/s.

A generalization of the interval distribution is the
scaled interval distribution of order k, deﬁned as the dis-
tribution of intervals between pairs of spikes that have

5

k=1

2

3

(a)

data
theory

1.0

k

P

0.5

0.0

0

(b)

k=1
k=2
k=3

20

40

60

0

1

2

3

t (ms)

rt/k

FIG. 3.

(a) Scaled interval distributions of orders
k = 1, 2, 3, representing the probability of ﬁnding a pair of
spikes at a given time interval, with exactly k − 1 spikes in
between. Data are shown in grey dotted lines, while theory
(Eq. 23) is shown in a solid black line. Fitting parameters of
the theory, r and D, are ﬁtted once for all three curves. (b)
Scaled interval distributions in dimensionless time units. In
agreement with Eq. (24), these curves are similar but have
slightly diﬀerent slopes, corresponding to an eﬀective value
of D which depends on k. The curves cross at rt/k = 1, as
predicted by Eq. (24).

The derivation of the interval density is independent
of many microscopic details of the neuron and of the
noise, and it is therefore expected to describe correctly
the behavior of many diﬀerent systems. Figure 4 shows
a comparison of the interval distribution (16) with ex-
perimental data from diﬀerent parts of the visual system
in several animals. Figure 4a shows data measured in
our experiment on the motion sensitive neuron H1 in the
visual system of the blowﬂy. In this experiment, the ﬂy
watched a random pattern of dark and light bars moving
at a constant velocity of
0.16 deg/sec. The best ﬁt
∼
to the data was found with an irregularity parameter of
γ = 0.1. Figure 4b is adapted from data published by
Robson and Troy (1987). In this experiment, a station-
ary sinusoidal grating was presented to an anesthetized
cat, and spikes were recorded from neurons in the retina.
The particular neuron these data were recorded from was
identiﬁed as a “Q-type” neuron, characterized by regu-
lar spiking. The best ﬁt of Eq. (16) was obtained with
γ = 0.015. Figure 4c shows data measured from isolated
goldﬁsh retina by Levine and Shefner (1977) in dark-
ness. These data are well described by Eq. (16) with
γ = 0.1. Figure 4d contains data measured by Cattaneo
et al. (1981) from visual cortex of anesthetized cat, in
response to a drifting sinusoidal grating. This interval
density is best ﬁt with γ = 0.3.

(a)

Fly H1

(b)

Cat retina

)
s
/

1
(
 

P

 

100

30

20

10

)
s
/
1
(
 

P

0

0

0

10

20

0

20

40

(c)

Goldfish
retina

0.1

0.1

100

%

4

8

6

2

0

0.015

(d)
Cat visual
cortex

0.3

50 100 150 200

0

                                 t  (ms)

10

20
                    

30

40

FIG. 4.

Distribution of intervals between neighboring
spikes, experiments and theory. The data are from diﬀer-
ent sensory systems in diﬀerent animals: (a) ﬂy visual system
(lobula plate motion detector), (b) Cat retina, (c) goldﬁsh
retina (d) cat visual cortex. For more details about these pub-
lished data see text. The y-axis is the probability of seeing
an interval, in units of 1/s (a-c), and the relative probability
in percent (d). The solid black lines are a ﬁt of Eq. (16), and
number in each panel indicates the value of the irregularity
parameter γ in the ﬁt.

These data indicate that the classiﬁcation of spike
trains to universality classes according to the irregular-
ity parameter γ is a useful one, and can be applied to
many systems. Systems with very diﬀerent microscopic
properties can belong to the same universality class, and
their interval density is well described by a theory with
one dimensionless parameter.
It will be shown in sec-
tion 4, that this classiﬁcation can be applied also under
conditions of rich dynamic stimuli; in this case, the in-
ternal irregularity parameter γ describes the statistical
properties on short time scales.

B. Correlation function

An important statistical property of the spike train
is its (auto)-correlation function,
. Whereas
ρ(t)ρ(0)
i
h
many models have been used to calculate the interval
density, less attention has been paid to the correlation
function [16]. In Appendix B we derive the correlation
function under the assumption that the noise correlation
time τn is much shorter than the typical interval between
spikes 1/r. The result is:

ρ(t)ρ(0)
i
h

= rδ(t) + r

P (r,D)
k

t
(
|

)
|

(25)

Xk6=0

where P (r,D)
interval distributions of Eq. (23),

k

(t) are the densities derived from the scaled

P (r,D)
k

(t) =

F (r,D)
k

(t) =

d
dt

rt + k
√8πDt3

2

e(rt−k)

/2Dt.

(26)

It is convenient to think about the probability per unit
time of ﬁnding a spike at time t
= 0, conditional on the
event that a spike is found at time t = 0. This quan-
tity R(t) is proportional to the correlation function in
the region t

= 0, and is called the conditional rate:

R(t) =

P (r,D)
k

t
(
|

)
|

≡

R(r,D)
U

(t).

(27)

Xk6=0

The density labeled k is the probability per unit time
for ﬁnding a pair of spikes separated by a time t with
exactly (k
1) spikes in between. These independent
events, when added together, give the total probability

−

6

6
6
U

→ ∞
∼

per unit time to ﬁnd a pair of spikes separated by a time
t, which is just the conditional rate. For small k these in-
dividual densities P (r,D)
are narrow and their peaks can
k
they smear and overlap to give
be resolved, and as k
asymptotically R(t)
r. The degree of regularity, 1/γ,
is associated with the number of densities which can be
resolved. The notation R(r,D)
(t) is introduced to empha-
size that this is a universal function, independent of the
detailed neuronal response f (s), and of the detailed prop-
erties of the noise. Figure 5 shows the conditional rates
for experiments with constant velocity, together with the
best ﬁt to Eq. (25). The constant value of the velocity
stimulus s increased among parts (a-d) of the ﬁgure; the
average ﬁring rate r increases and the irregularity γ de-
creases. This is clearly due to refractory eﬀects: as the
ﬁring rate increases, the repulsive interaction among the
spikes becomes more important, and the spike train be-
comes more stiﬀ, causing a more regular behavior. The
form of the repulsive interaction and the function describ-
ing the correlation function, however, remain the same.

(a)

 
 
)
s
/
s
e
k
p
s
(
 
 
)
t
(

i

R

120

(c)

60

40

20

0

80

40

0

 
 
 

(b)

(d)

120

80

40

0

160

120

80

40

0

−40

−20

20

40

−40

−20

20

40

0
t  (ms)

0
t  (ms)

FIG. 5. Conditional rate for the spike train in experiments
with a constant velocity stimulus. Data are shown by a gray
histogram, calculated directly from the spike times by bin-
ning them into 4 ms bins. Theoretical expression (Eq. 25) is
shown by a solid black line. The four parts of the ﬁgure cor-
respond to diﬀerent values of the constant velocity stimulus:
0.7 (a), 2.6 (b), 10.5 (c) and 42.2 (d) deg/sec. As the motion
signal becomes stronger, the average ﬁring rate increases and
the irregularity parameter decreases (see also Fig. 6).

C. Number variance

The number variable N (t) is a random function which
counts the number of spikes in the time window [0 : t].
Deﬁned by

N (t) =

ρ(t′)dt′,

(28)

t

0
Z

it provides a useful, less detailed, characterization of the
spike train point process. To study the statistics of this
variable, we write it as:

7

N (t) =

Θ(Φ(t)

k) = Int(Φ(t))

(29)

−

∞

Xk=1

where Int(y) is the integer part of y. Its Fourier repre-
sentation is

N (t) = Φ(t) +

ϕ +

ei2πm(Φ(t)+1−ϕ) (30)

1
2 −

1
2πim

Xm6=0

where ϕ is uniformly distributed on [0, 1], accounting
for a random position of the ﬁrst spike in a window
= rt, and
(see App. C). The number mean is
around this mean N (t) ﬂuctuates. Since the correla-
tion function of spikes is of ﬁnite range, the long time
asymptotic behavior of the number variance is diﬀusive:
σ2
Dt [1]. It is of interest, however, to de-
N =
rive a complete expression for this quantity also for short
times, where correlations between spikes are important.
Using (30), it is shown in Appendix C that

δN 2(t)
h

N (t)
i
h

i ∼

σ2
N = Dt +

1
(πm)2

∞

m=1
X

1

−

h

cos(2πmrt) e−2π

2

2

m

Dt

. (31)

i

Written as function of the number mean, i.e. as a func-
tion of the dimensionless time variable x = rt, the number
variance is:

σ2
N = γx +

1
(πm)2

∞

m=1
X

1

−

h

cos(2πmx)e−2π

2

2

m

γx

.

i
(32)

Figure 6 shows the number variance as a function of the
number mean, as calculated from an experiments with
constant velocity stimuli. Part (a) shows a scatter-plot
of the values obtained in various windows in the experi-
ments, with a line showing the average. Part (b) of the
ﬁgure shows a detailed view of one such average plot,
with the theoretical prediction Eq. (32) in solid black
line. The number variance of a Poisson process is shown
for comparison. The “diﬀusion constant” in the number
variable is γ, implying again its role as an irregularity pa-
rameter: the more stochastic the point process, the faster
is the diﬀusion in the number variance. Similar to Fig-
ure 4, a higher constant signal induces a higher ﬁring rate
and a lower irregularity of the spike train. Although the
data presented here have γ < 1, this is not a fundamen-
tal property of the theory, and in general, γ can take on
also values larger than 1, resulting in a “super-Poisson”
behavior.

(a)

(b)

Poisson

2 N

σ

5

4

3

2

1

0

1.0

0.5

g / s

e

2   d

6

0 . 1
0 . 0 5 4   d e g / s

0

2

4
x

6

0.0

0

8

1

3

4

2
x

γ
 

0.08

0.12

0.10

0.06

0.04

FIG. 6. Number variance as a function of number mean.
(a) Scatter plots of variances, as calculated in diﬀerent time
windows in the experiment. Data are shown from two ex-
periments, with constant velocities of 0.054 and 0.162 deg/s.
Solid lines indicate the average value of the variance for a
given mean. (b) Comparison to theory: the average value at
each number mean, is compared to the theoretic formula (Eq.
32), which is shown by a solid black line. In this experiment,
the velocity was 0.162 deg/s.

D. Irregularity and stiﬀness of spike trains

We have presented a statistical theory for a frequency
integrator model under constant stimulation and short-
range correlated noise. This results in a renewal point
process, with the density of intervals given by Eq. (16).
This process is characterized by an irregularity parame-
ter γ, which depends both on the variance of the noise
and on the sensitivity of the frequency response to noise;
it is the depth of frequency modulation, resulting from
these two eﬀects, that determines the irregularity of the
process. This one-parameter family of processes can de-
scribe many diﬀerent spike trains, ranging from almost
periodic to almost Poisson.

In the data presented above, a correlation was found
between the global average ﬁring rate and the irregular-
ity γ. Figure 7 shows a plot of the diﬀerent values of
γ obtained by ﬁtting to the equations in this section,
for two sets of experiments. The irregularity decreases
roughly linearly with the ﬁring rate, with a saturation at
very high ﬁring rate. This is the limit where the abso-
lute refractory period is approached. The simple relation
between γ and r holds only for the case of constant stim-
uli, where frequency modulations are essentially induced
by noise. When these modulations are aﬀected also by
a time varying sensory stimulus, very diﬀerent behaviors
is found (see next section).

20

40

60

80

100 120

r  (spk/s)

FIG. 7.

Dimensionless irregularity parameter, γ, as a
function of the average ﬁring rate r. The diﬀerent symbols
correspond to two diﬀerent experiments, performed each with
a diﬀerent set of constant velocity signals.

IV. TIME DEPENDENT STIMULUS

In the previous section, we considered the case of a
constant input signal, s(t) = s. More generally signals
coming into the system have a temporal structure, and
additional time scales enter the problem. If s(t) is slowly
varying relative to 1/r, universal behavior is expected
on short times, and slower modulations will appear on
longer times. In this section we consider the statistical
properties in the case of a slowly varying input signal. We
show that the conditional rate can be approximated by a
product of the universal function Eq. (27), and a slowly
varying envelope reﬂecting the temporal correlations of
the input signal. This envelope is calculated for some
simple cases. It is shown that the theory ﬁts the data
very well, even when the time scale separation required
in theory is only marginally satisﬁed by the experimental
conditions.

A. The Telegraph Approximation

Consider a random input signal s(t), that can take on
positive as well as negative values. Figure 8 shows a seg-
ment of some spike trains recorded from H1 in response
to such a stimulus, which was repeated many times. The
most striking eﬀect in the ﬁgure is the existence of wide
regions with spikes, and wide regions which are empty;
the typical time for these regions is much larger than the
interspike time. This partition into regions is a conse-
quence of the direction selectivity of the H1 neuron:
it
ﬁres when the eﬀective stimulus is in its preferred di-
rection, and is inhibited by stimulus in the opposite di-
rection. Although the velocity stimulus one the screen
varies rapidly in this experiment (every 2ms an indepen-
dent value is chosen), the spiking and quiet regions in the

8

spike trains have a much slower typical time scale. This
results from intermediate processing: the velocity on the
screen is not identical to the eﬀective stimulus driving the
cell, since it is ﬁltered by the photoreceptors and other
elements in the visual pathway.

To describe the phenomenon of spiking regions and
quiet regions, we imagine the spike train to be multiplied
by a telegraph signal, which keeps track of the algebraic
sign of the eﬀective stimulus:

ρ(t)

ρS(t)ρE(t),

≈

(33)

where ρE(t) = Θ[s(t)] is the telegraphic envelope of the
spike train. Figure 8 shows an illustration of the tele-
graph signal, which demonstrates that the the time scale
of the eﬀective stimulus sign change is longer than the
typical spike time, 1/r.

.
o
N

 
l
a
i
r
T

40

20

0

0

0.5
t (s)

1

FIG. 8. Response of the H1 neuron to repeated presen-
tations of a time-dependent stimulus. Each line shows the
response to one of the presentations, with the origin t = 0
corresponding to the onset of the stimulus; each dot repre-
sents a spike . The black solid line is an illustration of the
telegraphic envelope that multiplies the spike train, due to
the strong direction selectivity of the neuron.

Assuming that the telegraph envelope is statistically
independent of the short time structure in the spike train,
one may write the correlation function as a product,

ρ(t)ρ(0)
i
h

=

ρS(t)ρS(0)ρE(t)ρE(0)
h
i
ρE(t)ρE(0)
ρS(t)ρS(0)
.
i

ih

≈ h

(34)

h· · ·i

Now brackets
denote averaging over both the noise
and the random stimulus. To perform the averages, we
use the separation of time scales:
imagine dividing the
time axis into blocks of size τs, the correlation time of
the eﬀective stimulus. Performing ﬁrst the average over
the noise in each block separately, the ﬁrst term in the
product gives the correlation function of Eq. (25), with
the parameters r, D determined by the local value of s:

ρS(t)ρS(0)
h

i ≈ (

r(s) R(r(s),D(s))
U
0,

(t),

s > 0
0
s

≤

(35)

9

≈

= 0. If the response is saturated, r(s)

for t
rΘ(s), and
the ﬁring rate does not change much inside each spiking
region; moreover the diﬀusion constant D is determined
mainly by properties the noise, and is the same in all the
spiking regions. Therefore, on averaging the ﬁrst term
over s, one has αR(r,D)
, where α is the coverage fraction,
deﬁned as the fraction of time in which the stimulus is
ρE(t)
positive (
i
h

The second term in the product (34) is a correlation
function of the input signal as seen through the tele-
graphic envelope:

= α).

U

ρE(t)ρE(0)
i
h

=

.
Θ[s(t)]Θ[s(0)]
i
h

(36)

Special care should be taken around the point t = 0,
since it is aﬀected by the delta function singularities. The
whole correlation function then takes the form

ρ(t)ρ(0)
h

i ≈

rαδ(t) + rR(r,D)

(t)

U

,
Θ[s(t)]Θ[s(0)]
i
h

(37)

and the conditional rate is

R(t) = R(r,D)

U

.
Θ[s(t)]Θ[s(0)]
i
h

(38)

≤

This formula expresses the conditional rate as a product
of two terms. The ﬁrst term reﬂects internal properties
of the noise and of the neuron, similar to the result of the
previous section; RU is parameterized by an eﬀective rate
r and diﬀusion constant D. This function has an oscil-
latory structure on a time scale 1/D, which thus deﬁnes
the universal regime of the correlation function, in which
properties of the inputs do not have an important eﬀect.
The second term contains information about statistics
of the incoming stimulus, as seen through the nonlinear
response of the neuron. It modulates the universal func-
tion with a slower structure. Intuitively, the condition of
time scale separation can be understood as follows: on
short times, t
1/D, the envelope is almost constant
and the oscillations of the universal part are visible. As
these oscillations decay, on times t > 1/D, the stimulus
induced structure sets in. The independence between the
two factors aﬀecting the probability of ﬁnding a spike at
time t given a spike at time 0, results in a product form.
Eq. (36) indicates that in the telegraph approxima-
tion, the envelope of the correlation function depends on
the statistics of the zero-crossings of the stimulus. These
statistics for a random continuous function are in general
very diﬃcult to calculate [17]. Here we need only the cor-
relation function of the algebraic sign of a random signal,
and we can proceed in two ways. In the next sub-section
we consider the case in which zero crossings are indepen-
dent, and the resulting correlation function is a simple
exponential. Another simpliﬁcation occurs if the incom-
ing stimulus has Gaussian statistics, and in this case one
may derive an exact formula for the correlation function
of the nonlinear rate.

6
B. Random independent spiking regions

Let us ﬁrst assume that the spiking regions occupy
random independent positions along the time axis, with
lengths ∆ drawn independently from some distribution.
This is a crude approximation that may not be justiﬁed
for many experimental conditions. However, it is the sim-
plest type of telegraphic signal, and is characterized by
a small number of parameters; therefore we use this ap-
proximation as a starting point. The correlation function
of such a telegraph signal is derived in Appendix D, and
the result is,

ρE(t)ρE(t′)
i
h

= α2 α
µ

∞

|t−t′|

Z

p(∆)(∆

t′

t
− |

−

)d∆ (39)
|

where p(∆) is the distribution of lengths of positive re-
gions in the telegraph signal, and µ is the average length
of such a positive region. The second term clearly decays
for a well behaved p(∆), and asymptoti-
for (t
−
cally there remains only the square of the average signal,
2 = α2. For the case of an exponential distribution
ρE
h
i
of positive regions,

→ ∞

t′)

p(∆) =

e−∆/µ,

1
µ

(40)

(41)

the correlation function is also exponential,

ρE(t)ρE(t′)
i
h

= α2 + αµe−(t−t

)/µ.

′

The exponential decay of the envelope, Eq. (41), gives a
good ﬁt to many of the measured data sets. It has addi-
tional ﬁtting parameters, α and µ, which are the coverage
fraction and average length of the positive regions in the
telegraph envelope of the spike train.
It uses no prior
knowledge of the input stimulus, and relies primarily on
the direction selectivity of the response. Figure 9 shows
the correlation function calculated from spike trains, in
two experiments with random signals, of 20Hz (a,b) and
500Hz (c,d) bandwidth. Although in these experiment
we know the properties of the visual stimulus presented
to the ﬂy, this stimulus is rapid, and pre-processing takes
place in earlier stages of the visual pathway; the eﬀective
signal entering the H1 neuron is therefore a ﬁltered and
probably distorted version of the motion on the screen.
Therefore, we try the simple picture of the telegraph ap-
proximation, rather than rely on the detailed properties
of the visual stimulus. The black solid line in Figure 9 is
a ﬁt to a product of the universal function and the en-
velope correlation in the telegraph approximation, (41).
The time scale of the exponential decay is µ
50ms for
≈
the slower varying stimulus, and µ
20ms for the faster
≈
In the case of the fast stimulus, the cor-
varying one.
relation is probably limited by the ﬁltering processes in
the visual system, thus indicating that the time scale for
20ms. This is on the order of the
these processes is

≈

behavioral time scale for changes in ﬂight course in these
ﬂies, found by Land and Collett (1974) to be

30ms.

≈

(a)

(c)

 
 
 
 
 
 
)
s
/
s
e
k
p
s
(
 
 
)
t
(

i

R

200

0

100

50

0

(b)

(d)

−20

0
t  (ms)

20

−100

100

0
t  (ms)

FIG. 9. Conditional rates for spike trains, in the short
time and long time regimes. Data (gray histogram) are mea-
sured in experiments where the visual stimulus was a pattern
of light and dark bars moving with a random time-dependent
velocity. The velocity signal has diﬀerent bandwidths, of 20
Hz (a,b) and 500 Hz (c,d). The theoretic formula of Eq. (41)
is presented as a solid black line.

C. A Gaussian input signal

If the incoming random signal is drawn from a Gaus-
sian distribution, one may calculate exactly the correla-
tion function of a nonlinear response r(s). We focus on
the simple case where the response is a step function at
zero, and the stimulus has zero mean, corresponding to
a coverage fraction of α = 1/2. In this case,

Θ[s(t)]Θ[s(0)]
i
h

=

1
4

+

1
2π

),
c(t)
arcsin(
|
|

(42)

s(t)s(0)
i
h

is the correlation function of the
where c(t) =
Gaussian signal. In principle, one may calculate the cor-
relation function of a general nonlinear response r(s) in
the Gaussian case, but for our purposes it is suﬃcient to
consider the step response. We expect that in an experi-
ment where the driving stimulus varies slowly, the eﬀect
of intermediate ﬁlters will be negligible and one can take
the motion on the screen to be essentially equivalent to
the stimulus s(t) driving the neuron. Figure 10 shows the
correlation function measured from an experiment where
the input signal was a slowly varying random function
of time, with a Gaussian distribution. The correlation
function as calculated from the data is shown as a gray
histogram, whereas the solid black line is given by

R(r) = rR(r,D)

U

+

1
2π

1
4

(cid:20)

)
c(t)
arcsin(
|
|
(cid:21)

,

(43)

10

where c(t) was taken from the known distribution of the
input signal. The two ﬁtting parameters are r, related
to the global ﬁring rate, and D, the diﬀusion constant in
the universal regime.

(a)

(b)

i

)
s
/
s
e
k
p
s
(
 
 
)
t
(

R

300

200

100

0

−20

0
t  (ms)

20

−200

200

0
t  (ms)

FIG. 10. Conditional rate for spike trains, in the short
time and long time regimes. Data (gray histogram) are mea-
sured in experiments where the visual stimulus was a pattern
of light and dark bars moving with a random time-dependent
velocity, which is very slow and has a Gaussian distribution.
The theoretic formula of Eq. (43) is presented as a solid black
line.

D. Linear rectiﬁer approximation

In analogy to Eq. (37), one expects that if the non-
linear response of the neuron is f (
), the conditional rate
·
will take the form

R(r,D)
U

≈

(44)

R(t)

r[s(t)]r[s(0)]
i
h
where r(s) is the ﬁring rate as a function of stimulus,
obtained by averaging over noise only, and r, D are some
eﬀective global parameters representing an average over
diﬀerent regimes where s is almost constant. In this sec-
tion we show results from experiments where the response
r(s) cannot be approximated by a step function, and
apart from the direction selectivity the ﬁring seems to
follow the input stimulus linearly. A better approxima-
tion for the response would therefore be a linear rectiﬁer.
We used sine wave stimuli of diﬀerent amplitudes and pe-
riods. Figure 11 shows the ﬁring rate averaged over noise,
together with the stimulus, for one such experiment.

 200

 100

 0

)
c
e
s
/
s
e
k
p
s
(
 
 
 
r

i

0.0

1.0

0.5
t  (sec)

FIG. 11.

Stimulus and average response for a sine wave
experiment. A random pattern of dark and light horizontal
lines was moved by a sine wave velocity (solid black line), and
this stimulus was repeatedly presented to the ﬂy. The average
ﬁring rate was calculated over the repeated presentations as
a function of time (circles). The response of the neuron is not
saturated, and it follows closely the positive part of th sine
wave stimulus. (Compare the saturated response in Fig. 1a).

We used the approximation

r(s)

sΘ(s)

≈
envelope of

the
to evaluate
rate
the
. For a sine wave stimulus of frequency
r[s(t)]r[s(0)]
h
i
Ω, a straightforward calculation yields

conditional

r[s(t)]r[s(0)]
i
h

=

cos(Ω

1
2

π
t
Ω − |

+

|#

1
2Ω

t
|

)
|
"

sin(Ω

t
|

).
|

(45)

(46)

The conditional rate is, then, given by Eq. (44), with
Eq. (46) as the envelope. If we use our knowledge about
the frequency of the input signal, we expect to get a de-
scription of the conditional rate with only the universal
ﬁtting parameters r and D, and no additional parameters
for the envelope. Figure 12 shows two correlation func-
tions for the sine wave experiments, in the universal (a,c)
and the stimulus-dependent (b,d) regimes. No additional
parameters of the stimulus other than its frequency were
used; the solid black curve was obtained from Eq. (44)
with the envelope given by (46). The periodic nature
of the envelope is evident in the data, and the theoretic
prediction describes both the short range and the long
range features well.

200

100

0
200

100

)
s
/
s
e
k
p
s
(
 
)
t
(

i

R

(a)

(c)

(b)

(d)

0

−20

0
t  (ms)

20

−500

500

0
t  (ms)

FIG. 12. Conditional rates for spike trains in the short
time and long time regimes. Data (gray histograms) are mea-
sured in an experiment where a pattern of dark and light bars
were moved horizontally with a sine wave velocity, with pe-
riods of 0.25 sec (a,b) and 0.5 sec (c,d). Theory (black solid
lines) is obtained from Eq. (44), with the ﬁtting parameters
r and D, and using the known frequency of the stimulus.

11

E. Irregularity and stimulus properties

In this section, we showed that the conditional rate of
the spike trains has approximately a product form. One
term in the product is very similar to the conditional
rate for a constant stimulus, and describes the behavior
on short times. This term is universal in the sense that it
depends on only two simple parameters, the global aver-
age rate and the internal irregularity of ﬁring. The other
term carries information about temporal correlations in
the stimulus, and is a long-time envelope over the uni-
versal term. We have shown how this envelope can be
calculated in several cases, giving a very good ﬁt of the
data.

A naive quasi static application of the universal theory
would tell us that the short-time behavior is character-
ized by local values of the parameters r, D, and that these
change slowly as the external stimulus varies slowly. This
seems inconsistent with the short time behavior exhib-
ited by the data (Figures 8a, 10a, 10c). If the parame-
ters would change with the local changes of the external
stimulus, the oscillatory structure deﬁned by a period of
r would be considerably washed out when averaged over
the diﬀerent values of s. The data, however, show pro-
nounced oscillations with a well deﬁned period. This is
consistent with the notion that in a changing environ-
ment, the system achieves a steady state with the distri-
bution as a whole, thus obtaining global values for the
statistical parameters of its ﬁring.

Further evidence for this picture is given by Figure 13,
where the correlation function is plotted in normalized
time units, and is normalized by the height of the ﬁrst
peak. After rescaling of the two axes, there is only one di-
mensional parameter describing the short time behavior:
the irregularity γ. Although the correlation functions
were measured under diﬀerent experimental conditions of
time varying stimuli, the curves are approximately over-
lapping in the short time regime (x < 2). Thus, under
high signal-to-noise ratio, where the frequency modula-
tions are mainly induced by the stimulus and not by the
noise, the system tends to ﬁx the internal irregularity
parameter γ at some preferred value. This cannot be
explained by a simple quasi static behavior; it proba-
bly involves subtle adaptive mechanisms of the neuron
which try to maximize the dynamic range in each stim-
ulus ensemble. This subject is currently under further
investigation [24].

constant
random
sine

)
1
(
R

/
)
x
(
R

 
 
 
 
 

1.0

0.5

0.0

−3

3

0
x

FIG. 13.

correlation functions from diﬀerent experiments
with diﬀerent time-varying stimuli,
in dimensionless units.
The overlap at short times indicates that the parameter left
after rescaling, the internal irregularity, is similar in all these
cases.

V. DISCUSSION

The statistical properties of spike trains generated by
a sensory neuron under various stimulation conditions
were considered. Experiments were performed in vivo on
a blowﬂy, where the visual stimulus was well controlled,
and spike trains were measured extracellularly from an
identiﬁed motion sensor. This neuron is known to re-
spond to wide ﬁeld horizontal motion.

The main theoretical questions addressed were: (i) how
does the statistical behavior of spike trains in vivo relate
to the biophysics of the spike generation mechanism in
the cell, and (ii) how can the eﬀects of internal properties
be separated from those induced by the external sensory
stimulus (or the input to the neuron). These questions
were addressed within the framework of a model, which
describes the neuron as a nonlinear oscillator driven both
by noise and by an external stimulus. In general the eﬀect
of these two is the same: to cause frequency modulations
in the oscillator. We considered here the case where the
noise and the stimulus are very diﬀerent in their temporal
characteristics, namely the noise is rapid and the stim-
ulus is slow, compared to the time scales typical of the
spiking. This separation of time scales enables the the-
oretic analysis of the model, which in turn provides an
understanding of how the diﬀerent factors are reﬂected
in the statistical properties of the spike trains.

It was found that on the time scale of a few spikes, sta-
tistical behavior is rather universal and can be described
by a renewal process. This process is derived from phase
diﬀusion of a nonlinear oscillator, and has special sym-
metry properties reﬂecting the underlying oscillator. It is
characterized by one dimensionless parameter which rep-
resents the internal degree of irregularity of the process.
This parameter interpolates between highly regular and

12

highly stochastic limits. The parametrizaion allows spike
trains from diﬀerent parts of the visual system, and even
from diﬀerent organisms, to be classiﬁed in a simple way.
On the time scale of many spikes, eﬀects of the sensory
stimulus become important and are reﬂected in the form
of a slowly varying envelope which modulates the univer-
sal functions. Using only simple features of the nonlinear
neuronal response, the theory provides quantitative pre-
dictions for the statistical properties, which are found to
be in very good agreement with our measured data in
both the short time and the long time regimes.

Justiﬁcation for assuming a separation of time scales
between noise and stimulus comes from the fact that sig-
nals in the visual system are ﬁltered, and therefore the ef-
fective signals reaching an interneuron are relatively slow.
As often is the case in comparing theory and experiment,
we found that the agreement of the theoretical predic-
tions with the data extend to a regime where the required
separation of time scales is only marginally satisﬁed by
the experimental conditions.

The understanding of how the stimulus is reﬂected
in the statistical properties of the spike train could be
used “backwards”: in cases where little is known about
the stimulus, analysis of the long time behavior of the
correlation function can give us information about the
time scales involved in this stimulus, with only a gross
description of the neural response (direction selectivity,
degree of saturation). Possibly this understanding can be
extended to cross-correlation between several neurons; in
that case the separation between stimulus-induced and
internal properties will be important in assessing the
connections between the neurons. This is subject for
future research.

Appendix A

In this Appendix we show that for a unidirectional ran-
dom walk, (a “thermal ratchet” walk), the density of
(18),
times between barrier crossing is given by Eq.
in the limit where many steps are needed to cross the
barrier.

We consider the discrete case. Let xi be non–negative
0), identically distributed and
= σ2. Deﬁne the

random variables (xi ≥
= µ and
independent, with
random variable of their sum as

δx2
h

x
i
h

i

N

XN =

xi.

(A.1)

i=1
X
Let C be a constant positive number, then for large
enough N one has from the central limit theorem,

Prob
{

XN ≤

C

=

}

C

1
√2πN σ2
1
2

1 + erf

(cid:20)

Z

exp

{−
−∞
C
N µ
−
√2N σ2

(cid:18)

(cid:19)(cid:21)

=

N µ)2

(XN −

2N σ2

}

(A.2)

13

x

2

where erf(x) = 2
0 e−t
dt. Now deﬁne N1 to be the
π
number of steps required to ﬁrst reach the barrier. Then
R
from the non–negativity it follows that

Prob
{

XN ≤

C

= Prob
{

N1 ≥

}

N

= 1

}

Prob
{

N1 ≤

−

N

.

}
(A.4)

There are some normalization subtleties here, but let us
imagine that the sample space is composed of trajecto-
ries with a ﬁnite number of steps M which is much larger
than the typical number needed to cross threshold. Then,

Prob
{

N1 ≤

N

}

=

1 + erf

1
2

(cid:20)

N µ
C
−
√2N σ2

(cid:18)

(cid:19)(cid:21)

.

(A.5)

Appendix B

In this Appendix, we derive Eq.
the deﬁnition,

25. Starting from

ρ(t)ρ(0)
i
h

=

δ(Φ(t)

k)δ(Φ(0)

l) ˙Φ(t) ˙Φ(0)

.

−

−

*

Xk,l

+

(B.1)

The term at t = 0 must be taken care of separately: it
corresponds to k = k′,

Φ(t)

k

δ

Φ(t)

Φ(0)

−

!

 

−

!+

˙Φ(t) ˙Φ(0)

Φ(t)

k

−

!

δ(t)
˙Φ(t)

˙Φ(t) ˙Φ(0)

+

 

 

δ
Xk *
δ
Xk *
= δ(t′)

=

δ

Φ(t)

k

˙Φ(t)

−

!

+

Xk *

 

= r δ(t),

(B.2)

As expected for a point process of average rate r. For
= 0, we calculate the conditional rate R(t), the
times t
probability of ﬁring at some time t given a spike at time
0:

R(t) =

δ(Φ(t)
h

−

k) ˙Φ(t)
i

=

Xk6=0

Xk6=0

d
dt h

Θ(Φ(t)

.
k)
i

−

(B.3)

(A.3)

Using the integral representation of the Theta function,
we write

6
R(t) =

d
dt

∞

dp
2πip

−∞

Z

Xk6=0

e−i2πk

ei2πΦ(t)
h

.
i

δN (t) = δΦ(t) + (ϕ

) +

1
2

−

1
2πim

ei2πm[Φ(t)+1]

Xm6=0

(C.3)

(B.4)

As before, we use the Gaussian approximation for Φ(t),
Eq. (8), which is justiﬁed for times t satisfying τn ≪
dp
2πip

ei2πp(rt−k)−2π

R(t) =

d
dt

2
p

Dt

t:

∞

2

ν

dp

dyei2πy−2pi

2

2
p

Dt

(B.5)

where ν = rt

k. Doing ﬁrst the integral dp, we ﬁnd

−∞

Z

∞

=

Xk6=0

Xk6=0

d
dt

−∞

Z

Z

−

R(t) =

Xk6=0

Z

d
dt

d
dt

=

Xk6=0

= r

Xk6=0

ν

dy
√2πDt

2

ey

/2Dt

1
2

erf

(cid:20)
(cid:18)
t + k/r
√8πDt3

ν
√2Dt (cid:19)
e(rt−k)

2

+ const

(cid:21)

/2Dt.

(B.6)

Appendix C

Here we discuss the properties of the number variable,
which counts the number of spikes in the window [0 : t].
One must specify how the point t = 0 is chosen, and
there are two natural choices: (i) start counting at a
spike, and (ii) start counting at a random point in the
spike train. The second choice of a random origin is
the more commonly used. In this case, it is convenient
to deﬁne an auxiliary variable ϕ = Φ(t1), the phase of
the integrator Φ(t) at the time of the ﬁrst spike in the
window. This variable is uniformly distributed in [0 : 1],
and with this we have

N (t) = Int[Φ(t)+1
∞

−

ϕ]

1
πm

−

m=1
X

= Φ(t) + 1

ϕ +

sin[2πm(Φ(t)+1

ϕ]

. (C.1)

1
2

−

−

and average its square:

σ2
N =

δN (t)2
h

=

δΦ(t)2
h

i

+

1
2 −

(
h

ϕ)2

i

i
1
2 −

+ 2

δΦ(t)(
h

ϕ)
i

+

2(
h

1
(2πi)2mm′ h

+

Xm,m′6=0

1
2 −

ϕ)

Xm6=0
ei2π(m+m

′

1
2πim

e[i2πm[Φ(t)+1−ϕ]

i

)[Φ(t)+1−ϕ]

.
i

(C.4)

δΦ(t)2
= Dt; averaging the second term
By Eq. (9),
h
over ϕ gives 1
12 . Due to the independence of ϕ and Φ(t),
the ﬁrst cross-term vanishes while the second cross-term
decouples into

i

1
2πim h

2

−

Xm6=0

e[2πim[Φ(t)+1]

ϕe−2πimϕ

(C.5)

ih

.
i

Performing the average over ϕ,

ϕe−i2πmϕ
h

i

=

d
dµ

1
2πim h

e−2πimϕµ

= −

1
2πim

, (C.6)

and we have for the second cross-term

∞

1

m=1
X

(πm)2 cos(2πmrt)e−2π

2

2

m

Dt,

(C.7)

where the Gaussian approximation was used when aver-
aging over Φ(t). In the last double sum of Eq. (C.4), all
terms vanish by averaging over ϕ except for the terms
m + m′ = 0, which gives

µ=1

i(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
(2πm)2 =

1
12

.

Xm6=0

(C.8)

Adding the terms together we ﬁnd

1
2 ensures that N (0) = 0. Since
Note that the constant
ϕ only depends on the choice of origin, it is independent
of Φ(t), and therefore

−

σ2
N = Dt +

1
6

+

∞

m=1
X

1

(πm)2 cos(2πmrt)e−2π

2

2

m

Dt (C.9)

N (t)
i
h

=

Φ(t)
i
h

+

1
2 − h

ϕ
i

+

1
2πim h

ei2πm[Φ(t)+1−ϕ]

i

which is the same as Eq. (31).

Xm6=0
e[i2πm[Φ(t)+1]

1
2πim h

e−i2πmϕ

i

ih

Appendix D

= rt +

= rt.

Xm6=0

Now to calculate the number variance we deﬁne the ﬂuc-
tuation

(C.2)

In this Appendix we derive Eq. (41) for the correlation
function of the envelope of the spike train, in the tele-
graph approximation. The envelope ρE(t) is composed
of a train of characteristic window functions,

14

ρE(t)
i
h

=

Xk

χ

Tk −

 

∆k
2

, Tk +

∆k
2 !

(D.1)

as illustrated in Fig. 14. This function has a Fourier
transform

[1] Spikes: Exploring the Neural Code, F. Rieke, D. War-
land, R. de Ruyter van Steveninck and W. Bialek, MIT
Press 1997.

[2] Electric Current Flow in Excitable Cells, J.J.B. Jack, D.
Noble and R.W. Tsien, Clarendon Press, Oxford 1975.
[3] A.L. Hodgkin and A.F. Huxley, J. Physiol. 117, 500

ρE(ω)
i
h

=

ρE(t)e−iωt

∞

−∞

Z

=

Xk

e−iωTk sin(ω∆k/2)

(ω/2)

+ αδ(ω),

(D.2)

where Tk denote the middle points of the positive signal
regions.

∆

k

T
k

∆

k+1

T

k+1

FIG. 14.

These positive regions are assumed to be distributed
over the time axis independently with a density β per
unit time, and with an average length of
= µ. Aver-
aging in the frequency domain gives

∆
i
h

2
ρE(ω)
|

i

h|

= β h

sin2(ω∆/2)
i
(ω/2)2

+ (βµ)2δ(ω).

(D.3)

In the time domain this expression transform to

ρE(t)ρE(0)
i
h

= (βµ)2
∞

+ β

p(∆)d∆

0
Z

−∞

Z

∞

dω
2π

sin2(ω∆/2)
(ω/2)2

The last tern in this expression is the Fourier transform
of a product of two sinc function, which is the convolu-
tion of two square windows. This convolution has the
form

(D.4)

eiωt.

(D.5)

χ

χ =

∗

(

0
∆

,
t
− |

,
|

t
|
t
|

> ∆
∆

|
| ≤

which is equivalent to Eq. (39).

Acknowledgments
Many thanks to G. Lewen, for preparing the experiments,
and to N. Tishby and A. Schweitzer for comments.

15

(1952).

don (1928).
[5] Models of

[4] E.D. Adrian, The basis of sensation, Christophers, Lon-

the Stochastic Activity of Neurons, A.V.
Holden, Lecture Notes in Biomathematics, Springer-
Verlag 1976.

[6] Stochastic Models for Spike Trains of Single Neurons, G.
Sampath and S.K. Srinivasan, Lecture Notes in Biomath-
ematics, Springer-Verlag 1997.

[7] H.C. Tuckwell, Introduction to theoretical neurobiology

(Cambridge University Press , 1988).

[8] A formal reduction of conductance-based models to a
phase model is presented in B.S. Gutkin and G.B. Er-
mentrout, Neur. Comp. 10, 1047 (1998).

[9] N. Brenner, O. Agam, W. Bialek and R. de Ruyter van

Steveninck, Phys. Rev. Lett. 81, 4000 (1998).

[10] For a review of integrate-to-threshold models, see A.M.
Bruckstein and Y.Y. Zeevi, Biol. Cyb. 34, 63 (1979).

[11] G. Gestri, Biophys. J. 11, 181 (1971).
[12] T.W. Troyer and K.D. Miller, Neur. Comp. 9, 971 (1997).
[13] J.G.Robson and J.B.Troy, J. Opt. Soc. America A 4, 2301

[14] A. Cattaneo, L. Maﬀei and C. Morrone, cells, Proc. R.

Soc. Lond. B 212, 279 (1981).

[15] M.W. Levine and J.M. Shefner, retinal Biophys. J. 19,

(1987).

241 (1977).

[16] G. Gestri and P. Piram, Biol. Cybern. 17, 199 (1975).
[17] S.O. Rice, in Selected papers on noise and stochastic pro-
cesses, Ed. N. Wax, Dover publications, NY (1954).
[18] M. F. Land and T. S. Collett, J. Comp. Physiol. 89,

[19] E. Schneidman, B. Freedman and I. Segev, Neur. Comp.

331–357 (1974).

10, 7, p. 1679 (1998).

[20] W. Bialek, F. Rieke, R.R de Ruyter van Steveninck,
“Reading the Neural Code”, Science 252, 854 (1991).
[21] N. Franceschini, A. Riehle and A. le Nestour, in Facets
of Vision, Edited by D.G. Stavenga and R.C. Hardie
(Springer-Verlag, 1989)

[22] K. Hausen and M. Egelhaaf in Facets of Vision, Edited by
D.G. Stavenga and R.C. Hardie (Springer-Verlag, 1989)
[23] For details see R. de Ruyter van Steveninck and W.
Bialek, Phil. Trans. R. Soc. Lond. B (1995) 348, 321.
[24] N. Brenner, W. Bialek and R. de Ruyter van Steveninck,
“Adaptive rescaling of the neural response”, preprint
(1999).

[25] R. de Ruyter van Steveninck and W. Bialek, Proc. R.

Soc. Lond. Ser. B 234, 379–414 (1988).

[26] Z.F. Mainen and T.J. Sejnowski, ”Reliability of Spike
Timing in Neocortical Neurons”, Science 268, 1503
(1995).

[27] W. Bair and C. Koch, ”Temporal Precision of Spike
Trains in Extrastriate Cortex of the Behaving Macaque
Monkey”, Neur. Comp. 8, 1185 (1996).

[28] Rob R. de Ruyter van Steveninck, G.D. Lewen, S.P.
Strong, R. Koberle and W. Bialek, Science 275, p. 1805
(1997).

[29] P. Bedenbaugh and G. L. Gerstein, ”Rectiﬁcation of Cor-
relation by a Sigmoid Nonlinearity”, Biol. Cybern. 70, p.
219 (1994).

[30] P.I.M. Johannesma, in Neural Networks - Proc. of the
School on Neural Networks, Ravello 1967, Ed. E. R. Ca-
ianiello, Springer Verlag N.Y. (1968).

[31] G.L. Gerstein and B. Mandelbrot, Biophys. J. 4, 41

(1964).

[32] I.F Blake and W.C. Lindsey, Level crossing problems for
random processes, IEEE Trans. Info. Th., vol.IT-19, no.3,
295, (1973).

[33] Statistical Mechanics of Phase Transitions, J. M. Yeo-

mans, Clarendon Press, Oxford 1992.
Rev.

[34] S. Hagiwara, “Analysis of interval ﬂuctuation of the sen-
sory nerve impulse”, Jpn. J. Physiol 4, 234 (1954).
[35] W.R. Softky and C. Koch, of J. Neurosc. 13, 334 (1993).

16


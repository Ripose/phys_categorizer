Stochastic dynamics simulations in a new generalized ensemble

Ulrich H.E. Hansmann,a, 1 Frank Eisenmenger,b, 2 and Yuko Okamotoa, 3

a Department of Theoretical Studies, Institute for Molecular Science
Okazaki, Aichi 444-8585, Japan

bInstitute for Biochemistry, Medical Faculty of the Humboldt University Berlin
10115 Berlin, Germany

ABSTRACT

We develop a formulation for molecular dynamics, Langevin, and hybrid Monte Carlo
algorithms in the recently proposed generalized ensemble that is based on a physically
motivated realisation of Tsallis weights. The eﬀectiveness of the methods are tested with
an energy function for a protein system. Simulations in this generalized ensemble by the
three methods are performed for a penta peptide, Met-enkephalin. For each algorithm, it
is shown that from only one simulation run one can not only ﬁnd the global-minimum-
energy conformation but also obtain probability distributions in canonical ensemble at any
temperature, which allows the calculation of any thermodynamic quantity as a function
of temperature.

8
9
9
1
 
t
c
O
 
7
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
2
5
0
0
1
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1 e-mail: hansmann@mtu.edu Present address: Department of Physics, Michigan Technological Uni-

versity, Houghton, MI 49931, U.S.A.

2 e-mail: eisenmenger@rz.hu-berlin.de
3 e-mail: okamotoy@ims.ac.jp

1.

INTRODUCTION

For many important physical systems like biological macromolecules it is very diﬃcult

to obtain the accurate canonical distribution at low temperatures by conventional sim-

ulation methods. This is because the energy function has many local minima, sepa-

rated by high energy barriers, and at low temperatures simulations will necessarily get

trapped in the conﬁgurations corresponding to one of these local minima. In order to over-

come this multiple-minima problem, many methods have been proposed. For instance,

the generalized-ensemble algorithms, most well-known of which is the multicanonical ap-

proach, [1, 2] are powerful ones and were ﬁrst introduced to the protein-folding problem in

Ref. [3]. Simulations in the multicanonical ensemble perform 1D random walk in energy

space. They can thus avoid getting trapped in states of energy local minima. Besides mul-

ticanonical algorithms, simulated tempering [4, 5] and 1/k-sampling [6] have been shown

to be equally eﬀective generalized-ensemble methods in the protein folding problem.[7]

The simulations are usually performed with Monte Carlo scheme, but recently molecular

dynamics version of multicanonical algorithm was also developed.[8, 9]

The generalized-ensemble approach is based on non-Boltzmann probability weight fac-

tors, and in the above three methods the determination of the weight factors is non-trivial.

We have recently shown that a particular choice of the weight factor of Tsallis statisti-

cal mechanics,[10] which is a nonextensive generalization of Boltzmann-Gibbs statistical

mechanics, can be used for a generalized-ensemble Monte Carlo simulation.[11] The ad-

vantage of this ensemble is that it greatly simpliﬁes the determination of the weight factor.

The purpose of the present work is to generalize this Monte Carlo approach to other

simulation techniques. Here, we consider three commonly used algorithms: molecular

dynamics, [12] Langevin,[13] and hybrid Monte Carlo.[14] The performances of the algo-

rithms are tested with the system of an oligopeptide, Met-enkephalin.

2. METHODS

2.1. Monte Carlo in the new ensemble

In the canonical ensemble at temperature T each state with potential energy E is weighted

2

by the Boltzmann factor:

WB(E, T ) = e

−βE ,

where the inverse temperature β is deﬁned by β = 1/kBT with Boltzmann constant kB.

This weight factor gives the usual bell-shaped canonical probability distribution of energy:

PB(E, T ) ∝ n(E) WB(E, T ) ,

where n(E) is the density of states. For systems with many degrees of freedom, it is

usually very diﬃcult to generate a canonical distribution at low temperatures. This is

because there are many local minima in the energy function, and simulations will get

trapped in states of energy local minima.

Generalized-ensemble algorithms are the methods that overcome this diﬃculty by

performing random walks in energy space, allowing simulations to escape from any state

of energy local minimum. Here, we discuss one of the latest examples of simulation

techniques in generalized ensemble.[11] The probability weight factor of this method is

given by

W (E) =

1 + β0

(cid:18)

E − EGS
nF

(cid:19)

−nF

,

where T0 = 1/kBβ0 is a low temperature, nF is the number of degrees of freedom, and EGS

is the global-minimum potential energy (when EGS is not known, we use its best estimate).

Note that this weight is a special case of the weights used in Tsallis generalized statistical

mechanics, [10] where the Tsallis parameter q is chosen as

(1)

(2)

(3)

(4)

Note also that through the substraction of EGS it is ensured that the weights will always

be positive deﬁnite.

The above choice of q was motivated by the following reasoning. [11] We are interested

in an ensemble where not only the low-energy region can be sampled eﬃciently but also

the high-energy states can be visited with ﬁnite probability. In this way the simulation

can overcome energy barriers and escape from local minima. The probability distribution

of energy should resemble that of an ideal low-temperature canonical distribution, but

q = 1 +

1
nF

.

3

with a tail to higher energies. The Tsallis weight at low temperature

WT S(E) = [1 + (q − 1)β0(E − EGS)]

− 1

q−1

(5)

has the required properties when the parameter q is carefully chosen. Namely, for suitable

q > 1 it is a good approximation of the Boltzmann weight WB(E, T0) = exp(−β0(E −

EGS)) for (q − 1)β0(E − EGS) ≪ 1 , while at high energies it is no longer exponentially

suppressed but only according to a power law with the exponent 1/(q − 1). To ensure

that simulations are able to escape from energy local minima, the weight should start

deviating from the exponentially damped Boltzmann weight at energies near its mean

value. This is because at low temperatures there are only small ﬂuctuations of energy

around its mean (< E >T0). In Eq. (5) we may thus set

The mean value at low temperatures is given by the harmonic approximation:

(q − 1) β0 (< E >T0 −EGS) =

1
2

.

< E >T0 = EGS +

kBT0 = EGS +

nF
2

nF
2β0

.

(6)

(7)

Substituting this value into Eq. (6), we obtain the optimal Tsallis parameter in Eq. (4).

We remark that the calculation of the weight factor is much easier than in other

generalized-ensemble techniques, since it requires one to ﬁnd only an estimator for the

ground-state energy EGS, which can be done, for instance, by a procedure described in

Ref. [11].

As in the case of other generalized ensembles, we can use the reweighting techniques

[15] to construct canonical distributions at various temperatures T . This is because the

simulation by the present algorithm samples a large range of energies. The thermodynamic

average of any physical quantity A can be calculated over a wide temperature range by

< A >T = Z

,

(8)

dx A(x) W −1(E(x)) e−βE(x)

dx W −1(E(x)) e−βE(x)

Z

where x stands for conﬁgurations.

4

In the following subsections, we describe how to implement Langevin, molecular dy-

namics, and hybrid Monte Carlo algorithms in the new ensemble deﬁned by the weight

of Eq. (3). We remark that Langevin and molecular dynamics algorithms for Tsallis

statistical mechanics were also developed in Refs. [16] and [17], respectively.

2.2. Langevin algorithm

The Langevin algorithm[13] is used to integrate the following diﬀerential equation:

˙qi = −

+ ηi = fi + ηi ,

∂E
∂qi

(9)

where qi (i = 1, · · · , N) is the (generalized) coordinates of the system, E is the potential

energy, fi is the “force” acting on the particle at qi, and ηi is a set of independent Gaussian

distributed random variables with a variance:

< ηi(tl)ηj(tm) >= 2kBT0δijδ(tl − tm).

(10)

Here (and hereafter), we set all the masses mi equal to unity for simplicity. It can be

shown that the dynamics based on the Langevin algorithm yields a canonical distribution
PB(E, T0) ∝ n(E) WB(E, T0) = n(E)e−β0E.

In order to generalize this technique to

simulations in the new ensemble, we rewrite the weight factor in Eq. (3) as

W (E) = exp

−β0

ln

1 + β0

nF
β0

"

(cid:18)

(

E − EGS
nF

,

(cid:19)#)

Deﬁning now an eﬀective potential energy by [16, 17]

Eef f (E) =

ln

1 + β0

nF
β0

(cid:18)

E − EGS
nF

,

(cid:19)

we see that Langevin simulations in the new ensemble can be performed by replacing E

in Eq. (9) by Eef f (E):

(11)

(12)

(13)

(14)

˙qi = −

+ ηi ,

∂Eef f (E)
∂E
1

∂E
∂qi

=

1 +

(E − EGS)

β0
nF

fi + ηi .

Note that the procedure that led to the above equations is exactly the same as the one

we followed when we developed molecular dynamics and related algorithms in another

generalized ensemble, i.e., multicanonical ensemble.[8]

5

For numerical work one has to integrate the above equation by discretizing the time

with step ∆t and therefore for actual simulations we use the following diﬀerence equation:

qi(t + ∆t) = qi(t) + ∆t 





1

β0
nF

1 +

(E(t) − EGS)

.

fi(t) + ηi(t)





(15)

Using the above equation we will sample in the Langevin simulation the same ensemble

as in a Monte Carlo simulation with the weight of Eq. (3). Hence, we can again use the

re-weighting techniques and calculate thermodynamic averages according to Eq. (8).

2.3. Molecular dynamics and hybrid Monte Carlo algorithms

Once the formulation of Langevin algorithm for the new ensemble is given, the implemen-

tation of molecular dynamics algorithm is straightforward.

The classical molecular dynamics algorithm is based on a Hamiltonian

where πi are the conjugate momenta corresponding to the coordinates qi. Hamilton’s

equations of motion are then given by

H(q, π) =

π2
i + E(q1, · · · , qN ) ,

1
2

N

Xi=1

∂H
∂πi

∂H
∂qi

˙qi =

= πi ,

˙πi = −

= −

= fi ,

∂E
∂qi





and they are used to generate representative ensembles of conﬁgurations. For numerical

work the time is discretized with step ∆t and the equations are integrated according to

the leapfrog (or other time reversible integration) scheme:

qi(t + ∆t) = qi(t) + ∆t πi

πi

t +

∆t

= πi

t +

(cid:18)

(cid:19)

(cid:18)

3
2

∆t
2 (cid:19)

t +

∆t
2 (cid:19)
(cid:18)
+ ∆t fi(t + ∆t) .

,





The initial momenta {πi( ∆t

2 )} for the iteration are prepared by

πi

∆t
2 (cid:19)

(cid:18)

= πi(0) +

fi(0) ,

∆t
2

with appropriately chosen qi(0) and πi(0) (πi(0) is from a Gaussian distribution).

In order to generalize this widely used technique to simulations in our case, we again

propose to replace E by Eef f (of Eq. (12)) in Eq. (17). A new set of Hamilton’s equations

6

(16)

(17)

(18)

(19)

of motion are now given by

˙qi = πi ,

˙πi = −

∂Eef f
∂qi

=

1

β0
nF

1 +

(E − EGS)





fi .

(20)

This is the set of equations we adopt for MD simulation in our new ensemble. For nu-

merical work the time is again discretized with step ∆t and the equations are integrated

according to the leapfrog scheme.

The hybrid Monte Carlo algorithm[14] is based on the combination of molecular dy-

namics and Metropolis Monte Carlo algorithms [18]. Namely, each proposal for the Monte

Carlo update is prepared by a short MD run starting from the actual conﬁguration. In

this sense, the algorithm is based on a global update, while in the conventional Metropo-

lis method one is usually restricted to a local update. Furthermore, the Metropolis step

ensures that the sampled conﬁgurations are distributed according to the chosen ensem-

ble, while convential molecular dynamics simulations are hampered by diﬃcult-to-control

systematic errors due to ﬁnite step size in the integration of the equations of motion.

Given the set of coordinates {qi} of the previous conﬁguration and choosing the cor-

responding momenta {πi} from a Gaussian distribution, a certain number of MD steps
are performed to obtain a candidate conﬁguration {q′

i}. This candidate is accepted

i, π′

according to the Metropolis Monte Carlo criterion with probability

p = min{1, e−β0(H(q′,π′)−H(q,π))} ,

(21)

where H is the Hamiltonian in Eq. (16). The time reversibility of the leapfrog integration

scheme ensures detailed balance and therefore convergence to the correct distribution. The

whole process is repeated for a desired number of times (Monte Carlo steps). The number

of integration (leapfrog) steps NLF and the size of the time step ∆t are free parameters

in the hybrid Monte Carlo algorithm, which have to be tuned carefully. A choice of too

small NLF and ∆t means that the sampled conﬁgurations are too much correlated, while

too large NLF (or ∆t) yields high rejection rates. In both cases the algorithm becomes

ineﬃcient. The generalization of this technique to simulations for our ensemble can again

be made by replacing the potential energy E by Eef f (of Eq. (12)) in the Hamiltonian of

7

Eq. (16).

3. RESULTS AND DISCUSSION

The eﬀectiveness of the algorithms presented in the previous section is tested for the

system of an oligopeptide, Met-enkephalin. This peptide has the amino-acid sequence

Tyr-Gly-Gly-Phe-Met. The potential energy function that we used is given by the sum

of electrostatic term, Lennard-Jones term, and hydrogen-bond term for all pairs of atoms

in the peptide together with the torsion term for all torsion angles. The parameters for

the energy function were adopted from ECEPP/2.[19]-[21] The computer code SMC [22]

was modiﬁed to accomodate the algorithms.

For the generalized coordinates {qi} we used the dihedral angles. The peptide-bond
dihedral angles ω were ﬁxed to be 180◦ for simplicity. This leaves 19 dihedral angles

as generalized coordinates (nF = 19). The global-minimum potential energy EGS in

this case was obtained previously and we have EGS = −10.7 kcal/mol.[26] As for the

temperature, we set T0 = 50 K (or, β0 = 10.1 [

kcal/mol ]), following the case for the Monte
Carlo simulation in the present generalized ensemble.[11] We used these numerical values

1

for nF , EGS, and β0 in Eq. (3). We remark that the convention for energy values in the

present code, SMC, [22] is slightly diﬀerent from the one in the previous Monte Carlo

work.[11] Thus, the above value of EGS is accordingly diﬀerent from that in Ref. [11]. By

the deﬁnition of generalized ensembles, which are based on non-Boltzmann weight factors,

one cannot obtain information on the real dynamics of the system by the MD algorithm,

and only static thermodynamic quantities can be calculated. For this reason we do not

need to consider the equations of motion for dihedral space as presented in Ref. [23], but

can use the much simpler form for the kinetic energy term as given in the previous section

(see Eq. (16)).

For the MD simulations in our ensemble, we made a single production run with the

total number of time steps NLF = 800, 000 × 19 and the time-step size ∆t = 0.0075 (in

arbitrary units). For Langevin algorithm, a production run with the same number of time

steps (NLF = 800, 000×19) as in the MD simulation was performed, but our optimal time-

step size was only ∆t = 0.00028. This indicates that the simulation moves more slowly

8

through phase space, and we expect slower convergence to the Tsallis distribution than

in MD case. For the hybrid Monte Carlo algorithm, an MD simulation with 19 leapfrog

steps was made for each Monte Carlo step and a production run with 400,000 MC steps

was made. Since the Metropolis step in hybrid Monte Carlo corrects for errors due to

the numerical integration of the equation of motion, the time-step size ∆t can be large

for this algorithm. We chose ∆t = 0.01375 in our units. The initial conformation for all

three simulations was the ﬁnal (and therefore equilibrized) conformation obtained from a

Monte Carlo simulation of 200,000 sweeps, following 1,000 sweeps for thermalization with

the same weight (in each sweep all of the 19 angles were updated once).

In Fig. 1 the time series of the total potential energy are shown for the three simulations

with the new weight. They all display a random walk in energy space as they should for a

simulation with the weight of Eq. (3). All the lowest-energy conformations obtained were

essentially the same (with only a small amount of deviations for each dihedral angle) as

that of the global-minimum energy conformation previously obtained for the same energy
function (with ω = 180◦) by other methods.[24, 3, 25] The global-minimum potential

energy value obtained by minimization is −10.7 kcal/mol.[25] The random walks of the

MD and hybrid MC simulations visited the global-minimum region (E < −10 kcal/mol)

six times and eight times, respectively, while that of the Langevin simulation reached the

region only twice. These visits are separated by the walks towards the high-energy region

much above E = 16 kcal/mol, which corresponds to the average energy at T = 1000

K.[3] Hence, the rate of convergence to the generalized ensemble is the same order for all

three methods (with MD and Langevin algorithms slightly slower). As discussed below,

however, the results of thermodynamic quantity calculations all agree with each other,

implying that the methods are equally reliable.

In Fig. 2 the time series of the overlap O of the conformation with the ground state is

plotted. Our deﬁnition of the overlap, which measures how similar a given conformation

is to the lowest-energy conformation, is given by

O(t) = 1 −

θ(t)
i − θ(GS)

i

,

(22)

where θ(t)
i

and θ(GS)
i

(in degrees) stand for the nF torsion angles of the conformation at

1
90 nF

nF

Xi=1 (cid:12)
(cid:12)
(cid:12)

9

(cid:12)
(cid:12)
(cid:12)

t-th simulation step and the lowest-energy conformation, respectively. Symmetries for

the side-chain angles were taken into account and the diﬀerence θ(t)
projected into the interval [−180◦, 180◦]. Our deﬁnition guarantees that we have

i − θ(GS)

i

was always

with the limiting values

0 ≤ < O >T ≤ 1 ,

< O(t) >T → 1 ,
< O(t) >T → 0 ,

(

T → 0 ,
T → ∞ .

(23)

(24)

Only the result from the hybrid Monte Carlo simulation is given in Fig. 2, since the other

two simulations give similar results. Note that there is a clear anti-correlation between

the potential energy E and overlap O (compare Figs. 1c and 2), indicating that the lower

the potential energy is, the larger the overlap is (closer to the ground state).

Simulations in generalized ensemble can not only ﬁnd the energy global minimum but

also any thermodynamic quantity as a function of temperature from a single simulation

run. As an example, we show in Fig. 3 the average potential energy as a function of tem-

perature calculated from independent runs of the three algorithms together with that from

Monte Carlo results of Ref. [26] which rely on a multicanonical Monte Carlo simulation.

The results all agree within error bars.

Another example of such a calculation is the average overlap as a function of tem-

perature. The results are essentially the same for the three algorithms. That from the

MD algorithm is shown in Fig. 4. We see that the average overlap approaches 1 in the

limit the temperature going to zero, as it should (see Eq. (24)). We remark that the

average overlap approaches the other limiting value, zero (see Eq. (24)), only very slowly

as the temperature increases. This is because < O >T = 0 corresponds to a completely

random distribution of dihedral angles which is energetically highly unfavorable because

of the steric hindrance of both main and side chains.

CONCLUSIONS

In this article we have shown that the generalized-ensemble algorithm based on a special

realisation of Tsallis weights is not restricted to Monte Carlo simulations, but can also be

used in combination with other simulation methods such as molecular dynamics, Langevin,

10

and hybrid Monte Carlo algorithms. We have tested the performances of the above three

methods in the generalized ensemble for a simple peptide, Met-enkephalin. The results

were comparable to those of the original Monte Carlo version [11] in that the rate of

convergence to the generalized ensemble is of the same order and that the thermodynamic

quantities calculated as functions of temperature all agree with each other. We believe

that there is a wide range of applications for the generalized-ensemble versions of molecular

dynamics and related algorithms. For instance, the generalized-ensemble MD simulations

may prove to be a valuable tool for reﬁnement of protein structures inferred from X-ray

Our simulations were performed on the computers of the Computer Center at the Institute

for Molecular Science, Okazaki, Japan. This work is supported, in part, by a Grant-in-

Aid for Scientiﬁc Research from the Japanese Ministry of Education, Science, Sports and

and/or NMR experiments.

Acknowledgements:

Culture.

References

(1992) 9.

[1] B.A. Berg and T. Neuhaus, Phys. Lett. B267 (1991) 249; Phys. Rev. Lett. 68

[2] B.A. Berg, Int. J. Mod. Phys. C3 (1992) 1083.

[3] U.H.E. Hansmann and Y. Okamoto, J. Comp. Chem. 14 (1993) 1333.

[4] A.P. Lyubartsev, A.A.Martinovski, S.V. Shevkunov, and

P.N. Vorontsov-

Velyaminov, J. Chem. Phys. 96 (1992) 1776.

[5] E. Marinari and G. Parisi, Europhys. Lett. 19 (1992) 451.

[6] B. Hesselbo and R.B. Stinchcombe, Phys. Rev. Lett., 74 (1995) 2151.

[7] U.H.E. Hansmann and Y. Okamoto, J. Comp.Chem. 18 (1997) 920.

[8] U.H.E. Hansmann, Y. Okamoto, and F. Eisenmenger, Chem. Phys. Lett. 259 (1996)

321.

11

[9] N. Nakajima, H. Nakamura, and A. Kidera, J. Phys. Chem. 101 (1997) 817.

[10] C. Tsallis, J. Stat. Phys. 52 (1988) 479.

[11] U.H.E. Hansmann and Y. Okamoto, Phys. Rev. E 56 (1997) 2228.

[12] For instance, L. Verlet, Phys. Rev. 159 (1967) 98.

[13] G. Parisi and Y.-S. Wu, Sci. Sin. 24 (1981) 483.

[14] S. Duane, A.D. Kennedy, B.J. Pendleton, and D. Roweth, Phys. Lett. B195 (1987)

216.

[15] A.M. Ferrenberg and R.H. Swendsen, Phys. Rev. Lett. 61 (1988) 2635; ibid. 63

(1989) 1658(E), and references given in the erratum.

[16] D.A. Stariolo, Phys. Lett. A185 (1994) 262.

[17] I. Andricioaei and J.E. Straub, J. Chem. Phys. 107 (1997) 9117.

[18] N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, and E. Teller, J.

Chem. Phys. 21 (1953) 1087.

79 (1975) 2361.

[19] F.A. Momany, R.F. McGuire, A.W. Burgess, and H.A. Scheraga, J. Phys. Chem.

[20] G. N´emethy, M.S. Pottle, and H.A. Scheraga, J. Phys. Chem. 87 (1983) 1883.

[21] M.J. Sippl, G. N´emethy, and H.A. Scheraga, J. Phys. Chem. 88 (1984) 6231.

[22] The program SMC was written by F. Eisenmenger.

[23] A.K. Mazur, V.E. Dorofeev, and R.A. Abagyan, J. Comp. Phys. 92 (1991) 261.

[24] Y. Okamoto, T. Kikuchi, and H. Kawai, Chem. Lett. 1992 1275.

[25] H. Meirovitch, E. Meirovitch, A.G. Michel, and M. V´asquez, J. Phys. Chem. 98

(1994) 6241.

[26] F. Eisenmenger and U.H.E. Hansmann, J. Phys. Chem. B 101 (1997) 3304.

12

Figure Captions

• FIG. 1. (a) Time series of the total potential energy E (kcal/mol) from a Langevin

simulation in the new generalized ensemble. The simulation consists of 800, 000 ×19

time steps with step size ∆t = 0.000028. (b) Time series of E from a molecular

dynamics simulation in the new generalized ensemble. The simulation consists of

800, 000 × 19 time steps with step size ∆t = 0.0075. (c) Time series of E from a

hybrid Monte Carlo simulation in the new generalized ensemble. The simulation

consists of 400,000 MC steps. For each MC step an MD run of 19 time steps was

made with step size ∆t = 0.01375.

• FIG. 2. Time series of overlap function O (as deﬁned in the text) from the hybrid

Monte Carlo simulation in the new generalized ensemble. The simulation consists

of 400,000 MC steps. For each MC step an MD run of 19 time steps was made with

step size ∆t = 0.01375.

• Fig. 3. The average potential energy < E >T (kcal/mol) as a function of temper-

ature T (K) obtained from independent runs of the three algorithms and a multi-

canonical Monte Carlo simulation of Ref. [26]

• FIG. 4: The average overlap function < O >T (deﬁned in the text) as a function

of temperature T (K) obtained from the molecular dynamics simulation in the new

generalized ensemble.

13

 

E

35

30

25

20

15

10

5

0

-5

-10

-15

0

100000 200000 300000 400000 500000 600000 700000 800000

Langevin Step

 

E

35

30

25

20

15

10

5

0

-5

-10

-15

0

100000 200000 300000 400000 500000 600000 700000 800000

MD Step

 

E

35

30

25

20

15

10

5

0

-5

-10

-15

0

50000 100000 150000 200000 250000 300000 350000 400000

MC Step

 

O

1

0.8

0.6

0.4

0.2

0

0

50000 100000 150000 200000 250000 300000 350000 400000

MC Step

>
E
<

35

30

25

20

15

10

5

0

-5

-10

-15

0

Langevin
MD
Hybrid MC
Multicanonical MC

 

T

200

400

600

800

1000

>
O
<

1

0.8

0.6

0.4

0.2

0

0

 

T

200

400

600

800

1000


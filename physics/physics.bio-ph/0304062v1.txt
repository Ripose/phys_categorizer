3
0
0
2
 
r
p
A
 
7
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
2
6
0
4
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Classiﬁcation Scheme.

Stability of Negative Image Equilibria in Spike-Timing Dependent Plasticity

Alan Williams∗ and Patrick D. Roberts†
Neurological Sciences Institute, Oregon Health & Science University, 505 NW 185th Avenue, Beaverton, OR 97006

Todd K. Leen‡
Department of Computer Science and Engineering,
OGI School of Science & Engineering, Oregon Health & Science University
(Dated: December 23, 2013)

We investigate the stability of negative image equilibria in mean synaptic weight dynamics gov-
erned by spike-timing dependent plasticity (STDP). The neural architecture of the model is based
on the electrosensory lateral line lobe (ELL) of mormyrid electric ﬁsh, which forms a negative image
of the reaﬀerent signal from the ﬁsh’s own electric discharge to optimize detection of external electric
ﬁelds. We derive a necessary and suﬃcient condition for stability, for arbitrary postsynaptic poten-
tial functions and arbitrary learning rules. We then apply the general result to several examples of
biological interest.

PACS numbers: 87.18.Sn,87.19.La,75.10.Nr

I.

INTRODUCTION

Synaptic plasticity is thought to be a fundamental
mechanism for learning and adaptation in biological neu-
ral networks [1]. The activity dependence of synaptic
plasticity has been observed experimentally [2, 3], but
the precise nature of that dependence, and its func-
tional or computational consequences, are still largely
unknown. The purpose of the present article is to de-
rive clear functional consequences from speciﬁc forms of
activity-dependent synaptic plasticity.

Current models of synaptic plasticity are of two main
types: rate-based, and timing-based. In rate-based mod-
els, changes in synaptic weight depend on the mean spike
rate of presynaptic and postsynaptic cells, usually via
correlations [4, 5]. Since mean spike rates are averages
over time windows containing many spikes, the timing
of individual spikes is unimportant in rate-based models.
Recent experimental studies [6, 7, 8] have shown that in
some systems the precise timing of individual spikes can
have a pronounced eﬀect on synaptic plasticity. Mod-
els of such spike-timing dependent plasticity (STDP) [9]
calculate changes in synaptic weights by combining the
eﬀect of all pairs of presynaptic and postsynaptic spikes
[10, 11, 12, 13, 14], where the eﬀect of each pair is a func-
tion of the time between them (called the spike-timing
dependent learning rule).

One system in which STDP has been observed exper-
imentally, and where its functional role is understood,
is the electrosensory lateral line lobe (ELL) of mormyrid
electric ﬁsh [7]. The mormyrid identiﬁes objects in its en-

∗Electronic address: williaal@ohsu.edu
†Electronic address: robertpa@ohsu.edu
‡Electronic address: tleen@cse.ogi.edu

vironment by emitting a stereotyped electrical discharge
and detecting the perturbations to the resulting electrical
ﬁeld at the skin surface due to external objects. To cancel
the predictable sensory input due to its own discharge,
the mormyrid sends a series of time-delayed, time-locked
inputs to the ELL, synchronized to the ﬁsh’s electrical
discharge [15]. The neurons receiving these inputs have
plastic synapses onto neurons receiving primary aﬀer-
ent input. The repeated time-locked inputs, paired with
the reaﬀerent input, act via a spike-timing dependent
learning rule to change synaptic weights, in such a way
that the summed postsynaptic potential due to the time-
locked inputs forms a negative image of the potential
due to the ﬁsh’s own discharge [16]. This eﬀectively nulls
out the sensory eﬀect of the ﬁsh’s own discharge, thus
improving detectability of perturbations due to external
objects.

To be behaviorally useful to the ﬁsh, the set of synaptic
weights which create the negative image must be a sta-
ble equilibrium for the weight dynamics induced by the
spike-timing dependent learning rule. Conditions for ex-
istence and stability of such equilibria were ﬁrst explored
in [17]; the present paper is an extension and reﬁnement
of that work. The principal extension is the derivation of
an analytic criterion for stability of negative image equi-
libria for arbitrary postsynaptic potential functions and
arbitrary spike-timing dependent learning rules.

II. FRAMEWORK

The model consists of a single postsynaptic cell driven
by an array of time-locked presynaptic cells, a repeated
external input, and other unspeciﬁed inputs collectively
modeled as a single noisy external input [18, 19, 20] (Fig.
1). This architecture is based on the mormyrid ELL,
but is general enough to capture the dynamics of other

2

after a presynaptic spike. Let φ(x) be the periodic ex-
ternal input, and U (x, t) the total postsynaptic potential
due to the non-noisy inputs. We assume that for each t,
the mean instantaneous postsynaptic spike rate density
(in x) is given by f (U (x, t)) for some positive and strictly
increasing function f . The function f can be thought of
as the eﬀective gain of the postsynaptic cell in the pres-
ence of the noisy inputs. High or low noise correspond
to an f with small or large maximum slope respectively.
No attempt is made to include a refractory period for
postsynaptic spikes; and we will assume the period of φ
is greater than the refractory period of the presynaptic
neurons, so that refractoriness on the presynaptic side is
irrelevant.

Changes in weights will be implemented as discrete
steps with no internal time course. In the present model
there are two natural choices for the time at which weight
changes occur: asynchronously (instantaneously, when-
ever a presynaptic or postsynaptic spike occurs), or syn-
chronously (once per sweep of the repeated external in-
put, updating all weights simultaneously). We adopt
the latter strategy, updating weights at x = 0 for each
Z. The value of wi in the period beginning
t = nT, n
at (0, t) is then independent of x, and will be denoted
wi(t). For synchronous updating to be a reasonable ap-
proximation, we must assume that weight changes per
cycle are small relative to the weights themselves (slow
learning rate). Changes in weights due to diﬀerent spikes
or spike pairs are assumed to add linearly.

∈

In biological systems, synaptic weights have bounded
magnitude and do not change sign. Since the present
paper is focused solely on the dynamics near equilibria,
we impose no boundary conditions in the model. The re-
sults still apply to the biological case provided the weight
equilibria are in the region enclosed by biological bounds.

We assume homogeneous parameters: the scalar α and
are the same for all presynaptic neu-
the functions
rons, and the times xi are regularly spaced, xi = iδ,
i = 0, 1, . . . , N

1 for some δ > 0, N = T /δ

1.

L

E

,

−

≫

|

L

s
|

(s),

E
> τE, τL respectively, with τE, τL ≪
≪

For simplicity in the derivation of the weight dynamics,
(s) are zero or
it will be convenient to assume that
T .
negligible for
We will also require the learning rate to be slow: T
τw,
where τw is the time-scale on which weights undergo sig-
niﬁcant relative change. For the existence of approximate
negative image states we will need the spacing of presy-
naptic spike times much smaller than the widths of
and
: δ
summarized as

E
τE, τL. These time-scale assumptions can be

≪

L

δ

≪

(τE, τL)

T

τw.

≪

≪

Typical values for the mormyrid ELL are: δ < 1ms [15],
[C.C. Bell, private communication], τE ∼
20ms [7], τL ∼
80ms [C.C. Bell, private communication],
40ms [7], T
τw ∼

∼
102T [7].

FIG. 1: Schematic of the architecture. The postsynaptic cell
receives inputs from N presynaptic neurons, a repeated ex-
ternal input φ(x), and unspeciﬁed noisy inputs. Presynaptic
neuron i spikes at time xi in each period of φ, and has synap-
tic weight wi onto the postsynaptic cell.

neural systems hypothesized to have an array of time-
delayed, time-locked inputs [21, 22]. The framework for
the neural dynamics is the spike response model [23],
without refractoriness.

Each presynaptic cell i spikes exactly once at a ﬁxed
time within each sweep of the repeated external input,
causing a corresponding postsynaptic potential response
(PSP) in the postsynaptic cell.

The total membrane potential in the postsynaptic cell
is the sum of these PSPs, weighted by synaptic eﬃcacies
(weights) wi, and the two external inputs. This mem-
brane potential induces the postsynaptic cell to spike at a
certain (noisy) rate. Each presynaptic spike causes a con-
stant (nonassociative) change in the weight wi, and each
postsynaptic and presynaptic spike pair causes a change
in wi according to a spike-timing dependent learning rule,
namely a function of the time diﬀerence between the
postsynaptic and presynaptic spikes (associative learn-
ing).

∈

The repeated external

input is modeled as a peri-
odic input with period T. We use two time variables:
[0, T ) for the time within each repetition of the ex-
x
Z for the time of initiation
ternal input, and t = nT , n
∈
of each such period [19, 20, 24]. General dynamical quan-
tities will be functions of the pair (x, t). Let xi be the
time within each period when presynaptic cell i spikes,
and wi(x, t) its corresponding weight. Since presynap-
tic spikes are time-locked to the external input, xi
is
(s) be the PSP evoked by neu-
independent of t. Let
is causal:
ron i at time s after a spike. We assume
(s) = 0 for s < 0. Let α be the nonassociative weight
E
change due to a presynaptic spike, and
(s) the asso-
ciative weight change due to a postsynaptic spike time s

L

E

E

(a)

(b)

3

(a)

(b)

FIG. 2: Changes in weight due to pairing of presynaptic and
postsynaptic spikes.
(a) Pairing of a postsynaptic spike at
time (x, t) and presynaptic spike by neuron i at time (xi, t)
causes a change
xi) in weight wi. (b) For x within τL
of a period edge, we must include pairing with presynaptic
spikes in the neighboring period. Pairing of a postsynaptic
spike at time (x, t) and presynaptic spike by neuron i at time
(xi, t + T ) cause a change

T ) in weight wi.

(x

(x

xi

−

L

L

−

−

FIG. 3: Postsynaptic potential due to presynaptic spikes. (a)
Potential at time (x, t) due to presynaptic spike by neuron i
at time (xi, t) is wi(t)
xi). (b) For x within τE of 0,
we must include the potential due to presynaptic spikes in
the preceding period. The potential at time (x, t) due to the
presynaptic spike by neuron i at time (xi, t
T )

T ) is wi(t

xi + T ).

(x

(x

−

−

−

E

E

−

III. WEIGHT DYNAMICS

To obtain the mean weight dynamics, we compute the
mean value of wi(t + T )
wi(t). The nonassociative
−
change in wi(t) due to the single presynaptic spike at
(xi, t) is α. For the associative change due to presynap-
tic and postsynaptic spike pairs, consider the eﬀect of
a single postsynaptic spike at (x, t). The pairing of this
spike with the presynaptic spike at (xi, t) causes a change
xi) in wi. To properly handle edge eﬀects, we also
T )

L
include the pairing with presynaptic spikes at (xi, t
and (xi, t + T ), for a total change of

(x

−

−

L

−

−

(x

(x

T ) +

xi) +

xi −

(x
xi + T ).
L
T , at
For typical biological applications, where τL ≪
most one of the above terms is non-negligible, but all
xi is within
must be included to handle cases where x
τL of T or
T allows us
−
to approximate Eq. (1) by

−
T (Fig. 2). In addition, τL ≪

(1)

−

L

(x

xi −

−

nT ) =

(x

xi),

◦

L

−

◦

where
(s) =
L
with period T .

∞
−∞ L

(s

−

nT ) is the periodization of

Quantity (2) is the change in wi(t) due to a single
postsynaptic spike at (x, t). Postsynaptic spikes between
t and t+T occur at a mean rate density f (U (x, t)); hence
the mean total change due to all postsynaptic spikes be-
tween t and t + T is

∞

−∞ L
X

P

T

0

Z

dx f (U (x, t))

(x

xi).

◦

L

−

The mean total change in wi(t) due to both nonassocia-
tive and associative learning is therefore

wi(t)
i

h△

= α +

dx f (U (x, t))

(x

xi).

(3)

◦

L

−

T

0
Z

We now compute the postsynaptic potential U (x, t). The
contribution to U (x, t) due to the presynaptic spike by
neuron i at (xi, t
xi + nT ).
T this quantity is non-negligible for at most
For τE ≪
one value of n, either n = 0 (current period) or n =
1
−
(previous period). But to properly handle edge eﬀects
(Fig. 3) we include both, for a total contribution of

nT ) is wi(t + nT )
E

(x

−

−

wi(t

T )

(x

−

E

xi −

T ) + wi(t)
E

(x

−

−

xi).

(4)

We assume that the learning rate is suﬃciently slow that
we may approximate quantity (4) by

wi(t)(
E

(x

xi −

T ) +

(x

xi)).

(5)

−

E
T allows us to approximate quantity (5)

−

∞

−∞ E
X

P

wi(t)

(x

xi −

nT ) = wi(t)
E

(x

−

−

xi),

(6)

◦

◦

where
(s) =
E
with period T .

∞
−∞ E

(s

−

nT ) is the periodization of

E

Quantity (6) is the contribution to U (x, t) from neuron
i. The total postsynaptic potential is the summed con-
tribution from all presynaptic neurons, plus the repeated

Finally, τE ≪
by

(2)

L

4

◦

{

E

For generic

, then the analog of Eq.

and φ, Eq. (8) cannot be made an exact
equality for all x, because that would require solving in-
ﬁnitely many independent linear equations (one for each
x) in only ﬁnitely many unknowns (the N weights
).
wj}
But if we replace the discrete set of weights wj by a con-
tinuum weight density
(8)
can, under certain conditions, be made exact for all x.
Given such a density, we then recover the biological case
of discrete weights
(8) is approx-
imately true by deﬁning the set
to be a discrete
approximation to

for which Eq.
wj}
(y) be a weight density, with

(y)dy being the
total weight for presynaptic spikes occurring between y
and y + dy, for y
[0, T ). The continuum analog of Eq.
∈
(8), with exact equality for all x, is

wj}

Let

W

W

W

W

{

{

.

T

0
Z

dy

(y)

(x

W

y) = U0 −

−

φ(x).

(9)

◦

E

To solve this equation for
W
composition. Let Wn = (1/T )
Z be the Fourier coeﬃcients for
2πn/T , n

we take the Fourier de-
T
0 dy eiknyW (y) for kn =
, and let
R
◦

W

∈

and φ. Then Eq. (9)

En, φn be the coeﬃcients for
becomes

E

Wne−ikny)(

Eme−ikm(x−y))

WnEme−ikmx

dyei(km−kn)y

m=−∞
X
T

∞

0

Z

n=−∞
X
∞

m=−∞
X
WnEne−iknx.

= T

n=−∞
X

W

Hence

satisﬁes Eq. (9) if and only if

W0 =

φ0

,

U0 −
T E0
φn
T En

Wn = −

, n

= 0.

(10)

Given such a
, we construct approximate negative im-
age states with discrete weights as follows. Deﬁne g(x)
to be the deviation from a negative image:

W

g(x) = φ(x)

U0 −

−

◦

E

wj

(x

xj ).

−

N

j=1
X

(11)

{

wj}
Then
is small relative to U0 −
of weights deﬁned by

is an approximate negative image state if g(x)
φ(x), for all x. Consider the set

wj = δ

(xj),

W

where δ is the spacing of the xj. These weights can be
thought of as a discrete approximation to the weight den-
(y). Substituting into Eq. (11) and using Eq. (9)
sity

W

FIG. 4: An approximate negative image. If the postsynaptic
N
j=1 wj (t)

potential U (x, t) = φ(x) +

xj) is approxi-

(x

◦

mately some constant U0, then the potential
xj) due to presynaptic spikes alone is approximately U0

P

N
j=1 wj (t)

E

−

P

◦

(x
E
−
φ(x).
−

external input:

U (x, t) = φ(x) +

◦

wj (t)
E

(x

−

xj)

(7)

N

j=1
X

Equations Eq. (3) and Eq. (7) deﬁne the mean weight
◦

dynamics. The common periodicity of the functions
◦

E
and φ is an important feature, allowing the systematic

,

L
use of Fourier techniques.

∞

φne−iknx

n=−∞
X
T

dy(

∞

0
Z

∞

n=−∞
X
∞

U0 −

=

=

IV. THE NEGATIVE IMAGE

{

wi}

A set of weights

for which the total postsynaptic
potential U (x, t) is approximately constant in x will be
referred to as an approximate negative image state. For
such a state the contribution to the postsynaptic poten-
tial due to the presynaptic cells alone is, up to an additive
constant U0, an approximate negative image (Fig. 4) of
the external input φ:

N

j=1
X

◦

E

wj

(x

xj)

−

U0 −

≃

φ(x).

(8)

◦

In the following, we ﬁrst show that approximate nega-
tive image states exist provided a certain condition holds
on the Fourier coeﬃcients of the postsynaptic potential

E

function
and the repeated external input φ, and pro-
vided the presynaptic spike time-spacing δ is suﬃciently
small. We then show that for a particular value of U0
(depending on α,
, and f ) there exists an approximate
negative image state which is also an equilibrium (ﬁxed
point) for the weight dynamics.

L

◦

6
gives

This requires

g(x) =

δ

(xj)
E

W

(x

−

xj)

−

dy

(y)

(x

y).

W

−

◦

T

0
Z

N

j=1
X

This is the diﬀerence between a Riemann sum and the
integral it approximates. The error theorem for Riemann
sums then gives an upper bound for g:

g(x)
|

| ≤

δ

T
2

max
y |

d
dy

[
W

◦

E

(y)

(x

.
y)]
|

−

(12)

◦

E

◦

to be small, we need

Hence, for
g(x)
|
|
be diﬀerentiable in y, hence
y. A theorem of Fourier series [25] says that
diﬀerentiable if

y) to
(y)
(y) to be diﬀerentiable in
(y) is
. By Eq. (10) this

W

W

(x

<

−

E

W
nWn|

∞
n=−∞|

∞

places a constraint on the Fourier coeﬃcients of

and φ:

P

◦

E

∞

n=−∞|
X

nφn
En |

<

.
∞

(13)

This inequality requires φn to go to zero as n
more rapidly than En/n2.
quency (large

→ ±∞
In particular, the high fre-
) spectral content of φ must be less than
|

n
|
the high frequency content of

. Intuitively, in order for

◦

◦

E

E

−

with a smooth weight density

the convolution of
W
to be able to “match” the high frequency components of
φ, the high frequency content of φ cannot be too large.
If Eq. (13) is satisﬁed, and δ is suﬃciently small, then
from Eq. (12) the deviation g(x) from an exact negative
image is small, hence approximate negative image states
exist.

We now show that for a particular U0 there exists an
approximate negative image state that is an equilibrium
for the weight dynamics. From Eq. (3), a weight state
wj}
{
satisﬁes

is an equilibrium if U (x) = φ(x)+

N
j=1 wj

xj )

(x

−

E

◦

P

α +

dx f (U (x))

(x

xi) = 0 for all i.

(14)

T

0

Z

◦

L

−

,
wj}
This is a system of N equations in the N unknowns
but they are nonlinear equations for nonlinear f . In gen-
eral such equations need not have solutions, but for ap-
proximate negative image states the nonlinearity is in
some sense “small”, and this will allow us to show that
solutions exist provided δ is suﬃciently small.

{

For an approximate negative image state we have
U0, and we wish this
(14). First deﬁne U0 so that Eq.

U (x) = U0 + g(x) with g(x)
U (x) to satisfy Eq.
(14) would be satisﬁed if g(x) were identically zero:

≪

α +

dx f (U0)

(x

xi) = 0 for all i.

(15)

T

0

Z

◦

L

−

5

(16)

(17)

f (U0) =

for all i

α

−
◦

(x

(x)

T
0 dx
L
α
R
−
◦
T
0 dx
R

xi)

−
,

=

L
where the independence of i follows from the periodicity

of

. Hence our desired U0 exists and is given by

◦

L

U0 = f −1

−
T
0 dx

α
◦

L

,
(x) (cid:17)

(cid:16)

provided α,

R
and f satisfy

◦

L

min
u

f (u) <

< max

f (u).

u

(18)

L
From Eq. (15), U (x) = U0 + g(x) satisﬁes Eq. (14) if
and only if

α
◦

−
T
0 dx
R

(x)

T

0
Z

dx [f (U0 + g(x))

f (U0)]

(x

xi) = 0 for all i.

◦

L

−

−

(19)
f (U0) and Li(x) =

For brevity let h(x) = f (U0 + g(x))
◦

−

xi). Then Eq. (19) can be written as

(x

L

−

h, Lii
h

= 0 for all i,

(20)

where

,
h·

·i

is the inner product deﬁned by

f1, f2i
h

=

T

0
Z

dx f1(x)f2(x),

(21)

for f1, f2 in the space X of smooth functions on the
interval [0, T ].

Let H be the set of functions h corresponding to all

possible values of the weights

H =

h : h(x) = f (φ(x)

wj

(x

xj))

f (U0),

{

N

−

j=1
X

:

wj}

{

◦

E

wj ∈

−

−
R, j = 1, . . . , N

,

}

◦

N
j=1 wj

where we have used U0 + g(x) = φ(x)
xj)
E
from Eq. (11). Let S be the subspace of X consisting
P
, and S⊥ be the
of all linear combinations of the
(inﬁnite dimensional) subspace of X orthogonal to S (in
the inner product deﬁned by Eq. (21)). Then there exists
an h satisfying Eq. (20) if and only if H and S⊥ have
nonempty intersection:

−
Li}

(x

−

{

H

∩

S⊥

=

.

∅

(22)

We claim that condition (22) holds if δ is suﬃciently
small. If δ is small, then the bound (12) implies that g(x)
f (U0)
is small for all x. In that case h(x) = f (U0+g(x))

−

6
is approximately its linearization in g, which we denote
by h0(x):

h(x)

h0(x) = f ′(U0)g(x)

≃

= f ′(U0)(φ(x)

U0 +

wj

(x

xj )).

−

N

j=1
X

◦

E

−

Let H0 be the set of such h0 corresponding to all possible
values of the weights

:

wj}

{

H0 =

h0 : h0(x) = f ′(U0)(φ(x)

U0 +

{

N

◦

wj

(x

xj )),

−

E
j=1
X
R, j = 1, . . . , N

,

}

−

wj ∈

Then the condition that H0 have nonempty intersection
with S⊥,

H0 ∩

S⊥

=

,

∅

(23)

=
is equivalent to existence of h0 ∈
0 for all i. This is equivalent to the linearization of the
system (19):

H0 such that

h0, Lii
h

T

0
Z

dx f ′(U0)(φ(x)

−

N

j=1
X

◦

E

◦

L

−

−

which can be rewritten as

Qijwj = γ,

(24)

N

j=1
X

where γ =

T

0 dx f ′(U0)(φ(x)

U0) and

−

R

Qij = f ′(U0)

T

0
Z

◦

E

dx

(x

xj )

(x

xi).

(25)

−

−

◦

L

{

wj}

This is a system of N linear inhomogeneous equations
in the N unknowns
, which has a solution provided
the coeﬃcient matrix Q is invertible. The eigenvalues
of Q will be calculated in the following section, and for
generic
all eigenvalues are nonzero. Hence Q is
generically invertible, so that condition (23) holds. Fur-
thermore, the intersection of H0 with S⊥ is generically
transversal (not tangent).

and

L

E

→

→

0, H

Now as δ

H0 (in the metric induced by
the inner product (21)). By the openness of transver-
sal intersection (inﬁnite dimensional version, [26]), any
suﬃciently small perturbation of H0 also intersects S⊥.
Hence for δ suﬃciently small, H intersects S⊥ (Fig. 5),
hence h satisfying Eq. (20) exists. The corresponding
weight state
is an approximate negative image equi-
librium.

wj}

{

6

FIG. 5: Transversal intersection theorem. If H0 has transver-
sal intersection with S⊥ and H is suﬃciently close to H0, then
H intersects S⊥.

V. STABILITY CRITERION

We now derive a necessary and suﬃcient condition for
the mean stability of approximate negative image equilib-
ria, by examining the linearized weight dynamics around
be an approximate negative image
ˆwj}
such states. Let
{
equilibrium satisfying Eq. (11) with

U (x, t) = U0 + ˆg(x) +

◦

vj(t)
E

(x

−

xj ),

N

j=1
X

where vj(t) = wj (t)
ˆwj is the deviation of weight j from
its equilibrium value, and ˆg(x) is the deviation from a
negative image in the equilibrium state
. To ﬁrst
order in vj we then have

ˆwj}

−

{

f (U (x, t))

f (U0+ˆg(x))+f ′(U0+ˆg(x))

≃

N

j=1
X

◦

vj(t)
E

(x

−

xj ).

(27)
wi(t) =

△

Substituting Eq. (27) into Eq. (3) and using

vi(t) yields

△

vi(t)
i

h△

= α +

dx f (U0 + ˆg(x))

(x

xi)

+f ′(U0 + ˆg(x))

◦

vj(t)
E

(x

−

xj ))

(x

xi).

◦

L

◦

L

−

−

T

0
Z
N

j=1
X

From the equilibrium condition Eq. (14) and Eq. (26),
the term in Eq. (28) of zeroth order in vj vanishes. Hence

vi(t)

h△

i ≃

Pij vj(t),

N

j=1
X

U0 +

wj

(x

xj ))

(x

xi) = 0

U (x) = φ(x) +

ˆwj

(x

xj ) = U0 + ˆg(x)

(26)

N

j=1
X

◦

E

−

for all i,

Solving for φ(x) in Eq. (26) and substituting into (7)
yields

6
7

where

Pij =

T

0
Z

dx f ′(U0 + ˆg(x))
E

(x

−

xj ))

(x

xi).

−

◦

L

◦

eikT = 1 and the functions eikxi are independent func-
tions of i. Here we choose

kn =

, n = 0, 1, ..., N

1.

−

2πn
T

Now assume δ is suﬃciently small that

The corresponding eigevalues of Q are

ˆg(x)

≪

f ′(U0)
f ′′(U0)

for all x.

Then f ′(U0 + ˆg(x))
Qij,
where Q is the matrix deﬁned in Eq. (25), and we obtain

f ′(U0) for all x, so that Pij ≃

≃

N

j=1
X

N

j=1
X

vi(t)

h△

i ≃

Qijvj (t).

Taking the mean on both sides and using

vi(t)
i

h△

=

vi(t)
i

△h

yields

vi(t)

△h

i ≃

Qijh

.
vi(t)
i

(28)

= 0, hence for

near
Eq. (28) gives the linearized dynamics for
near the approximate neg-
wi(t)
vi(t)
h
i
h
i
.
ˆwj}
ative image equilibrium
The system Eq. (28) is stable if and only if all eigen-
values of Q + I have norm less than 1. Due to periodicity

vi(t)
i
h

{

◦

◦

,

E

L

xi}

and regular spacing of the

of
, the matrix Q has
the property that each of its rows equals the row above it
shifted one entry to the right (and wrapped around at the
edges). Such matrices are called circulant [27] and their
eigenvectors and eigenvalues are easily found, as follows.
Let u be the vector with components ui = eikxi ; then

{

(Qu)i =

Qijuj

=

eikxj

dx

(x

xj)

(x

xi),

−

−

N

j=1
X
N

j=1
X

◦

◦

E

◦

E

0
Z

T

T

0
Z

◦

◦

L

◦

L

so that

(Qu)i
ui

=

N

j=1
X

L

and

, the integral in the above ex-
By periodicity of
E
xi modulo T . The factor
pression is a function of xj −
eik(xj −xi) is also a function of xj −
xi modulo T provided
eikT = 1. Now if the
are regularly spaced, the sum
xi}
xi modulo T is independent of
over j of a function of xj −
i. In that case Eq. (29) would imply (Qu)i/ui indepen-
dent of i, hence u is an eigenvector of Q, with eigenvalue
the right hand side of Eq. (29). We get a complete set
of such eigenvectors by taking N values of k such that

{

λn =

eikn(xj−xi)

N

j=1
X

T

0
Z

dx

(x

xj )

(x

xi).

−

−

◦

L

◦

E

xi, making the change of variables

Letting zj = xj −
y = xj −

◦

E

,

◦

L

◦

x and using periodicity of

gives

λn =

eiknzj

(
−

y)

(zj −
L

y)

N

j=1
X
N

j=1
X

T

dy

◦

E

0
Z

◦
L ∗T

◦

E
e

=

eiknxj

(xj ),

◦

◦

where

zontal reﬂection (
E
we have replaced zj by xj in the sum.
e

∗T is convolution on the interval [0, T ],
zj}
(
−
e
< 1 for all n:
1 + λn|
|
Stability:

The stability condition is

y)), and since

(y) =

E

{

is hori-

=

xj}

{

1 +
|

N

j=1
X

kn =

2πn
T

eiknxj

◦

◦
L ∗T

(xj )
|

E
e
, n = 0, 1, ..., N

1.

−

< 1,

(30)

In the biological setting two limiting regimes are of spe-

small) and dense spacing

cial interest: slow learning (
(δ small).
◦

◦

L

If

is small, then so are the eigenvalues of Q. If λn =

L

an + ibn with an, bn real, we have
2 = (1 + an)2

1 + λn|
|

−

n = 1 + 2an + (a2
b2

b2
n).

n −

If an and bn are suﬃciently small, this quantity is less

◦

suﬃciently
than 1 if and only if an < 0. Hence for
small, all eigenvalues of Q + I have norm less than 1 if
and only if all eigenvalues of Q have negative real part1.
The stability condition then becomes Re λn < 0 for all
n. Hence in the slow learning limit Eq. (30) becomes

L

Re

eiknxj

(xj ) < 0, n = 0, 1, ..., N

1. (31)

−

N

j=1
X

◦
L ∗T

◦

E
e

1 The slow learning limit can thus be thought of as the continuous
time (continuous t) limit. All eigenvalues of Q having negative
real part is equivalent to stability of the system dhvi/dt = Qhvi
and hence of T dhvi/dt = Qhvi, which is the continuous time
version of Eq. (28).

eik(xj −xi)

dx

(x

xj )

(x

xi). (29)

−

−

Slow learning:

The dense spacing limit (δ

0) is the continuum limit
in xi. The discrete weight density wi/T is replaced by
a continuum weight density W (x), sums over xj are re-
placed by integrals over x, and N

. This yields

→

→ ∞

λn =

dx eiknx

(x) n = 0, 1, ...

T

0
Z

◦
L ∗T

◦

E
e

Hence λn is just the nth Fourier coeﬃcient of
Fourier convolution theorem then gives

. The

◦
L∗T

◦

E
e

λn =

◦
Ln

◦
En =

◦
Ln

◦
En,

e

◦
En,

◦
En,

◦
Ln are the nth Fourier coeﬃcients of

where
◦
◦
E,
E,
z. Substituting into the stability condition
1 + λn|
|
for all n gives the dense spacing limit of Eq. (30):

◦
L respectively, and z is the complex conjugate of
< 1

e

e

Dense spacing:

◦
Ln

◦

En|

1 +
|

< 1, n = 0, 1, ...

(32)

Finally, with both slow learning and dense spacing the
stability condition becomes

Slow learning, dense spacing:

◦
Ln

◦
En] < 0, n = 0, 1, ...

Re[

(33)

A further simpliﬁcation follows in the long period limit,
,
T . Holding τE, τL constant and taking T
→ ∞
in Eq. (33) approach Fourier
. The stability condition then becomes

τE, τL ≪
the Fourier series of
transforms of

L

E

◦

◦

,

,

E

L

Slow learning, dense spacing, long period:

Re[

[
L

F

] (k)

[
E

F

](k)] < 0,

k

,
(
−∞

∈

∞

).

(34)

For the calculation of examples we will work in the slow
learning, dense spacing, long period limit, which is the
limit of primary biological interest in the mormyrid ELL.

VI. GENERAL REMARKS

L

The Roles of Nonassociative and Associative
Learning. Both nonassociative and associative learn-
ing (α and
, respectively) play a role in whether approx-
imate negative image equilibria exist, via Eq. (18). They
are also both involved in determining the location of such
equilibria, via Eq. (14). The interpretation of Eq. (14) is
that at equilibrium, the mean change due to nonassocia-
tive learning (α) must be precisely opposite to the mean
term). If the
change due to associative learning (the
postsynaptic spike rate density f is bounded, this places
a relative magnitude constraint on α and
, namely Eq.
(18). If this constraint is violated then the mean changes

L

L

8

due to associative and nonassociative learning are unable
to balance one another, and no negative image equilib-
rium is possible.

By contrast, only associative learning plays a role in
the stability of approximate negative image equilibria,
via Eq.
(30). The irrelevance of nonassociative learn-
ing for stability has an intuitive interpretation: near
an approximate negative image equilibrium, the mean
nonassociative change is cancelled by the mean associa-
tive change due the constant postsynaptic potential U0
around which U (x, t) ﬂuctuates. Only the deviations of
U (x, t) from U0 cause a net change in the weights, and
these changes are purely associative (due to postsynaptic
spikes generated by U (x, t)). Alternatively, nonassocia-
tive learning can be analogized to a constant externally
applied force in a physical system. Such a force changes
the location of equilibria, but has no eﬀect on the dy-
namics around equilibria.

E

The Role of the Repeated Input.

For a given
, the repeated input φ
postsynaptic potential response
plays a role in the existence of approximate negative im-
age states via Eq. (13): φ cannot have too much high
frequency content relative to

for such states to exist.

E
Assuming φ is such that approximate negative image
states exist, it then plays the important role of deter-
mining the weight conﬁgurations in such states, an in
particular in approximate negative image equilibria, via
Eq. (8).

But φ plays no role in the stability of the resulting neg-
ative image equilibrium. This is intuitively reasonable,
since in approximate negative image states φ is “nulled
out” by the summed postsynaptic potentials due to time-
locked presynaptic spikes.
The Role of Noise.

The functional form of the
mean postsynaptic spike rate f aﬀects the existence and
location of the negative image equilbrium via Eq. (18)
and Eq. (14).

But provided f is strictly increasing (so that f ′ is pos-
itive), f has no eﬀect on the stability of the equilibrium.
Hence the classiﬁcation of learning rules as (mean) stable
or unstable is, except for this mild monotonicity require-
ment, insensitive to the ﬁne structure of the noise. This
is a post hoc justiﬁcation for not modeling the noise in
more detail.

Canonically Stable Learning Rule:

.
−E
the dense spacing and slow learning limit, suppose

=

L

In
=

. The stability condition Eq. (33) is then

−E

2 > 0

for all n,

◦

En|
|

◦

or in other words,
generic

, the learning rule
Area Sign Condition.

= 0 for all n. Since this is true for
=
is generically stable.
In the dense spacing and

−E

En 6

L

E

slow learning limit, consider n = 0 in Eq. (33). Since

◦
E0 =
◦

◦
E0 are just the areas under the functions

L
, Eq. (33) says that for stability, these areas must

and

and

E

L

(35)

◦
L0
◦

9

E

L

total mean weight change per cycle is therefore approx-
imately α. Hence negative image equilibria for pure an-
tisymmetric learning rules are only possible if α = 0 (no
nonassociative learning).

Cooperative Stability.

It follows from Eq. (30)
that the sum of stable learning rules is stable; but it
, there exist pairs of
is also clear that given a generic
L2, each individually unstable, for
L1 and
learning rules
which the sum
L2 is stable. This is most easily
L1 +
seen by direct computation in the slow learning, dense
spacing, long period limit, via Eq. (34) (see the examples
calculated below).

E

L

and

Interchanging

Duality Principle.

in Eq.
(25) transforms Qij to Qji, hence Q to QT , hence Q + I
to (Q + I)T . The eigenvalues of a matrix are unchanged
by transposition. The stability condition, that all eigen-
values of Q + I have norm less than 1, is thus invariant
under interchange of
and learning rule
PSP

L
E
are a stable pair if and only if the

. In other words, a PSP

and learning rule

are a stable pair.

and

L

E

E

This has potential biological relevance if the functional
forms of PSPs and associative learning rules overlap. The
single-lobe exponential and alpha function learning rules
treated in the examples, below, are also plausible PSPs,
hence duality applies.

Inversion Principle.

Replacing

by

and

E

E

−E

−L

E
PSP

in Eq.

L
by
(25) leaves Qij invariant, hence Q + I
invariant. The stability condition is therefore invariant
. In other words, a PSP
and
under inversion of both
are a stable pair if and only if the
and learning rule
are a stable pair.

and learning rule

−L
In particular, the stable learning rules for an inhibitory
PSP are just minus the stable learning rules for the
corresponding excitatory PSP. Plasticity at inhibitory
synapses was explored in [28], and preliminary experi-
mental evidence given in [29].

−E

L

L

(33) or Eq.
or

Independence of Normalization.

In the slow
learning and dense spacing limit, the stability conditions
Eq.
(34) are invariant under multiplica-
by positive constants. Hence, provided the
tion of
E
L
magnitudes of
are not so large that the slow learn-
ing assumption is violated, stability does not depend on
those magnitudes. In particular, in working with speciﬁc
examples it is not necessary to give
any overall
normalization.

or

or

L

L

E

E

VII. EXAMPLES

E

L

and

Working in the slow learning, dense spacing, long pe-
riod limit, we now compute explicit criteria for stabil-
ity when
have functional forms commonly used
in the spike-timing dependent plasticity literature. The
PSP
will be assumed excitatory and causal, and of ex-
will
ponential or alpha function form. The learning rule
consist of one or two “lobes”: a “pre-before-post” lobe
(presynaptic spike before postsynaptic spike) and/or a
“post-before-pre” lobe (postsynaptic spike before presy-

L

E

] (k)] changes sign at k0, then for the prod-
F
] (k)] to be negative around k0 we must
[
] (k)Re[
E
] (k) also change sign at k0, in the opposite sense to

[
E
F

FIG. 6: If Re[
[
uct
L
[
have
L
]](k).
Re[

F
F
[
E

F

be opposite in sign. If they are the same sign, then the
negative image is unstable. In particular, if
are
both nonnegative, the negative image is unstable. Hence,
is any pure potentiating
if
learning rule, the negative image is unstable. Similarly,
inhibitory PSPs with purely depressing learning rules are
unstable.

is an excitatory PSP and

and

L

L

E

E

Symmetric and Antisymmetric Learning Rules.
In the dense spacing, slow learning, long period limit,
there is a nonempty, positive measure set of postsynap-
tic response functions for which purely symmetric or
purely antisymmetric learning rules are generically un-
stable. This follows from the fact that the Fourier trans-
forms of symmetric and antisymmetric functions are pure
real and pure imaginary, respectively.

Suppose the real part of the Fourier transform of

has

E

a zero:

Re[

[
E

F

] (k0)] = 0,

for some k0.

(36)

Then, generically, Re[
pose

is symmetric, so that

] (k)] changes sign at k0. Sup-
] (k) is pure real. Then

[
E

F

[
L

F

L

[
E

[
L

[
L
[
E

] (k)].

[
E

Re[

] (k)

](k)] =

] (k) Re[

F

F

F

F

F
Since Re[
] (k)] changes sign at k0, for the stability
condition Eq. (34) to be satisﬁed for k near k0, we must
] (k) change sign at k0, in the opposite sense to
have
Re[
] (k0) = 0,
. Hence generic
which is untrue for generic symmetric
symmetric learning rules are unstable for postsynaptic
response functions satisfying Eq. (36).

[
L
] (k)]; see Fig. 6. But this forces

F
[
E

[
L

F

F

L

Similarly, if the imaginary part of the Fourier trans-

form of

has a zero:

E

Im[

[
E

F

] (k0)] = 0,

for some k0.

(37)

then generic antisymmetric learning rules are unstable.

Pure antisymmetric learning rules have another diﬃ-
T
culty: since they satisfy
(x) = 0, near a negative
0 dx
image equilibrium the mean weight change per cycle due
R
is zero, to ﬁrst order in g. The
to an antisymmetric

L

L

10

follows:

E

(pE )(x) = xpE e−x/τE H(x)
(pL)
I
L
(pL)
II

(x) =

L

xpL e−x/τL1 H(x)

(x) = σ1 (σ2x)pL e−σ2x/τLH(σ2x)

(one-lobe)

x)pL ex/τL2 H(

x)

(two-lobe)

−

σ1
τ pL+1
L1
A
τ pL+1
L2

+

(
−

where H is the Heaviside function

H(x) =

1 x
0,
0 x < 0.

≥

(

The parameters (see Fig. 7) are as follows: pE, pL = 0
for an exponential or 1 for an alpha function; τE, τL,
τL1 and τL2 are positive time constants; σ1 = +1 for
1 for a depressing lobe; σ2 =
a potentiating lobe or
+1 for a pre-before-post lobe or
1 for a post-before-pre
, A is the area of the post-
lobe; and for the two-lobe
L
before-pre lobe, with the area of the pre-before-post lobe
normalized to
1 . We impose no overall normalization
or
on

, since this has no eﬀect on stability.

−

±

−

E
We assume an excitatory PSP

L

; to obtain the stable

cases for the inhibitory PSP
in the stable cases for
by

A).

E

E

, simply replace

−E

(i.e. replace σ1 by

by
L
−L
σ1 and A

−

L

−
For both the one-lobe and two-lobe

, there are four
possible combinations of pE and pL: exponential or alpha
function PSP with exponential or alpha function learning
rule. We will refer to these four cases as ee, ea, ae, and
aa, with the ﬁrst letter in the pair indicating that the
PSP is exponential or alpha function, and the second
letter referring to the learning rule.
The Fourier transforms of these

are rational
functions in k, and the stability condition will reduce to
the requirement that a certain polynomial in k2, whose
coeﬃcients are themselves polynomials in the parameters
of
, be negative for all k. Since the algebra in
all cases is essentially the same, diﬀering only in the size
and coeﬃcients of the resulting polynomial, we present
only one case (aa, one-lobe) in full detail and for all other
cases simply list the end results.

and

and

L

L

E

E

A. Alpha Function PSP, One-Lobe Alpha Function
Learning Rule

For

we have

(x) = xe−x/τE H(x)
(x) = σ1σ2xe−σ2x/τL H(σ2x)

E

L

] (k) =

[
E

F

] (k) =

[
L

F

τ 2
E
ikE)2
σ1τ 2
L
σ2ikτL)2

(1

(1

−

−

(a)

(b)

(c)

E

L

FIG. 7: PSPs and learning rules used in the examples. (a) The
(pE )(x) is exponential for pE = 0 and an alpha func-
PSP
tion for pE = 1. (b) One-lobe learning rules of alpha func-
(1)
I (x), for the four possible combinations of σ1
tion form,
(potentiating or depressing) and σ2 (pre-before-post or post-
before-pre).
(c) Two-lobe learning rules of alpha function
(1)
form,
II (x), for the four possible combinations of σ1 (pre-
before-post lobe potentiating or depressing) and the sign of
A (post-before-pre lobe potentiating or depressing). The area
of the pre-before-post lobe is normalized to
1, and the area
of the post-before-pre lobe is A.

±

L

naptic spike). Each lobe will be of exponential or alpha
function form, and either potentiating (positive) or de-
can be written as
pressing (negative). Such

and

E

L

11

E

E

by

and

and

Duality is also applicable here. Interchanging

L
in this example is equivalent to interchanging τE and τL
1. The multiplications
and multiplying both
L
oﬀset and we are left with r replaced by 1/r. It follows
) is s1 <
that if the interval of stability for the pair (
r < s2, the corresponding interval for the pair (
)
,
E
L
is s1 < 1/r < s2. But by duality these intervals must
coincide; hence we must have s1 = 1/s2. This is indeed
the case for s1 = 3

2√2 and s2 = 3 + 2√2.

−

L

E

,

The instability of the σ1 = +1 case for any τE and τL,
by the failure of the stability condition at k = 0, is just
the Area Sign Condition.

−

B. Summary of Results

For the one-lobe learning rules the stable parameter

ranges are all easily calculated:

σ1 =

1, σ2 = +1:

all τL/τE

σ1 =

1, σ2 = +1:

τL/τE < 2

σ1 =

1, σ2 = +1:

τL/τE > 1/2

ee:

ea:

ae:

aa:

−

−

−

−

σ1 =

1, σ2 = +1: 2

√3 < τL/τE < 2 + √3.

−

L

is not depressive and pre-before-post. For

Note that in all four cases we get instability, for all τL
and τE, if
L
depressive and pre-before-post, all four cases have some
range of τL/τE in which
is stable. The extent of that
range depends critically on the precise functional form
of
; but for 1/2 < τL/τE < 2 we have stability
independent of the functional form of

and

and

L

L

E

For the two-lobe learning rules the polynomial arising
out of the stability condition has coeﬃcients depending
on σ1 and on three continuous parameters: r1, r2 and A,
where r1 = τL1/τE, r2 = τL2 /τE. The polynomials are
given in the Appendix.

E

.
L

−

In all four cases, σ1 = +1 is always unstable. For
σ1 =
1, the boundaries of the stable region in (r1, r2)
for various values of A are plotted numerically in Fig. 9.
For one-lobe learning rules we found that only depres-
sive and pre-before-post permits stability. For two-lobe
learning rules, the pre-before-post lobe must be depres-
sive for stability, and the post-before-pre lobe cannot
have area A greater than 1. This is just the Area Sign
Condition: the area of the pre-before-post lobe is -1, and
for stability when paired with an excitatory PSP
the
total area under the learning rule

must be negative.

E

A
|
|

For A < 1, the eﬀect of the post-before-pre lobe shows
the following general trends: in cases ee and ae, as the
absolute area
of the post-before-pre lobe increases,
the stable region in the relative time constants r1 and
r2 tends to shrink. Hence the post-before-pre lobe can
be thought of as destabilizing in such cases. In cases ee
and ae the situation is less clear. Increasingly negative
A (larger depressive post-before-pre) is uniformly desta-
bilizing, but increasingly positive A (larger potentiating

L

, in case aa.
FIG. 8: Range of stable one-lobe
L
The learning rule must be depressive and pre-before-post,
2√2 < τL/τE < 3 + 2√2. Stable examples are
with 3
drawn with solid lines; endpoints of the stable interval are
drawn with dashed lines.

for given

−

E

leading to

Re

[
L

F

] (k)

[
E

F

](k)

n

o
= C Re[σ1(1 + iσ2kτL)2(1
= Cσ1[σ2
Lτ 2
Ek4 + (4σ2τLτE −
2τ 2
Lk2)2(1+τ 2

where C = τ 2
Lτ 2
the stability condition is then

E/[(1+σ2

2τ 2

−

ikτE)2]
σ2
2τ 2

τ 2
E)k2 + 1]

L −

Ek2)2]. Since C > 0,

σ1[σ2

2r2k4 + (4σ2r

2r2
σ2

−

−

1)k2 + 1] < 0
for all k

(38)

where r = τL/τE. The expression on the left is a
quadratic in k2. The condition is impossible for σ1 = +1
1 more work is required.
(it fails at k = 0) but for σ1 =
The quadratic ax2 + bx + c is negative for all x
0 if and
only if a < 0 and (b < 0 or b2
4ac < 0). Applying this
−
condition to Eq. (38) with σ1 =

1 yields

≥

−

−

r2
σ2)2(r2

−
−

4σ2r + 1 < 0
6σ2r + 1) < 0

or

(r

−

(39)
(40)

3

2

−

For σ2 =

1 these give

−
2√2 < r <

2 + √3 or
√3 < r <
3 + 2√2, both of which are impossible
√3 < r < 2 + √3
2√2 < r < 3 + 2√2; the former is contained in the

−
−
because r > 0. For σ2 = +1 we get 2
or 3
latter, giving stability if and only if

−

−

−

−

−

σ1 =

1, σ2 = +1,
< 3 + 2√2.

−
τL
τE

2√2 <

3

−

(41)

The only stable case is depressive and pre-before-post,
with τL/τE constrained to lie in a ﬁnite interval (Fig.
8). Note that this interval contains τL/τE = 1, where
(the canonically stable learning rule).

=

L

−E

12

FIG. 9: Boundary curves of the stable region in (r1, r2) for various values of A, for two-lobe
1. Curves are
labelled by A. Curves with A > 0 are drawn with solid lines, curves with A < 0 with dashed lines. In all cases the region of
stability is on the side of the curve containing the diamond (
) in the upper left corner of the plot. The interval of stability for
the corresponding one-lobe learning rules is the portion of the r1-axis in bold.

with σ1 =

−

L

⋄

post-before-pre) appears to be destabilizing for small r2
but stabilizing for large r2.

Cooperative stability, in which a two-lobe rule is stable
while each of its lobes individually would be unstable,
occurs in cases ea, ae, and aa: any point (r1, r2) in a
stable region, with r1 outside the interval in which the
corresponding one-lobe rule is stable is an example of
cooperative stability.

Finally, the shape and extent of stable regions for two-
lobe learning rules, or the extent of stable intervals for
one-lobe learning rules, depend critically on whether

E

L

and
are exponential or alpha function in form. This
suggests that in order to infer even such qualitative prop-
erties as stability or instability in a biological context, the
learning rule must be known with considerable precision.

However, for particular values of some parameters the
dependence on functional form may be such that useful
conclusions can still be drawn in the absence of such pre-
cision; for example, the stability in one-lobe, depressive
1, indepen-
pre-before-post learning rules with τL/τE ≃
dent of whether
are exponential or alpha function
L
in form. This particular ﬁnding has direct relevance to

or

E

the learning rule observed experimentally in mormyrid
ELL [7]. The experimental data is not precise enough to
suggest a particular functional form, but does indicate a
one-lobe, depressive, pre-before-post rule, with a width
of the same order of magnitude as the width of a PSP.
Stability of such a rule is consistent with the analytic
results derived above.

ae:

aa:

VIII. APPENDIX

For completeness we provide below the polynomial
conditions for stability of the two-lobe learning rules
treated in the examples.

ak4 + bk2 + c < 0 for all k
a = σ1r1r2
Ar2
2 −
b = σ1(r2
2 + r1) + A(r2
c = σ1 + A

1 −

1r2

r2)

ee:

ea:

13

ak4 + bk2 + c < 0 for all k
a = σ1r2
b = σ1(r2
c = σ1 + A

2(2r1 −
1)
2 + 2r1 −

Ar2
−
1) + A(r2

1(2r2 + 1)
1 −

2r2 −

1)

1r2
2 + Ar4
1r4
2
r1r3
2 + 3r2
2 + 4r1r4
1r3
r4
2r2
2 −
2 −
4r4
1r2 = 2r2
r3
1r2
1 + 2r3
r4
1r2 −
2 −
2)(r2 + 1)2 + r2
1 + r2
3r1r2 + (r2

ak8 + bk6 + ck4 + dk2 + e < 0 for all k
a = σ1r2
b = σ1(
−
+ Ar1(
−
c = σ1(
−
4r1r2))
1 +r2
3r1r2 +(r2
+A(
−
d = σ1(2r2
r2
1 + 4r1 −
2 −
e = σ1 + A

1)2 +r2
1(1
−
1) + A(2r2
1 −

2)(r1 −

2 + 2r1r2
1r2
2)
1r2 + 3r2
1r2
2)
2(1 + 4r1 −
4r1r2))
4r2−
r2
1)
4r2 −
2 −

Acknowledgments

Ar3

2(2r2 −
3r2
2 −

ak6 + bk4 + ck2 + d < 0 for all k
a = σ1r1r3
1)
b = σ1r2(r3
6r1r2 + r2)
c = σ1(2r2
d = σ1 + A

1 + 2r1) + A(2r2
r2

2 −

1 −

−

1r2(2r1 + 1)

1r2 + 6r1r2 + r1) + Ar1(r3

r2
2 −

1 −
2r2)

3r1r2

2 −

We would like to thank Dr. Gerhard Magnus, Dr.
Nathaniel Sawtell, and the members of Dr. Curtis Bell’s
lab for insightful discussions. This material is based upon
work supported by the National Science Foundation un-
der Grant No. IBN-0114558, and by the National Insti-
tute of Mental Health under Grant No. R01-MH60364.

[1] D. O. Hebb, The Organization of Behavior (John Wiley

and Sons, New York, 1949).

[2] T. Lomo, Exp Brain Res 12, 46 (1971).
[3] T. V. Bliss and T. Lomo, J Physiol 232, 331 (1973).
[4] T. J. Sejnowski, J. Theor. Biol. 69, 385 (1977).
[5] E. L. Bienenstock, L. N. Cooper, and P. W. Munro, J

Neurosci 2, 32 (1982).

Science 275, 213 (1997).

387, 278 (1997).

[6] H. Markram, J. L¨ubke, M. Frotscher, and B. Sakmann,

(2000).

[16] C. C. Bell, D. Bodznick, J. Montgomery, and J. Bastian,

Brain. Beh. Evol. 50, 17 (1997), (suppl.1).
[17] P. D. Roberts, Phys. Rev. E 62, 4077 (2000).
[18] R. Kempter, W. Gerstner, and J. L. van Hemmen, Phys-

ical Review E 59, 4498 (1999).

[19] P. D. Roberts, J. Compu. Neurosci. 7, 235 (1999).
[20] P. D. Roberts and C. C. Bell, J. Compu. Neurosci. 9, 67

[21] R. H. Hahnloser, A. A. Kozhevnikov, and M. S. Fee, Na-

[7] C. C. Bell, V. Han, Y. Sugawara, and K. Grant, Nature

ture 419, 65 (2002).

[8] Q. Bi and M. ming Poo, J. Neurosci. 18, 10464 (1998).
[9] L. F. Abbott and S. B. Nelson, Nature Neurosci. (suppl.)

3, 1178 (2000).

77, 2360 (1997).

bern. 69, 503 (1993).

[23] W. Gerstner, R. Ritz, and J. L. van Hemmen, Biol. Cy-

[10] M. C. W. van Rossum, G. Q. Bi, and G. G. Turrigiano,

J. Neurosci. 20, 88128821 (2000).

[24] P. D. Roberts, J. Neurophysiol. 84, 2035 (2000).
[25] D. C. Champeney, A Handbook of Fourier Theorems

[11] J. Rubin, D. D. Lee, and H. Sompolinsky, Phys Rev Lett

(Cambridge University Press, 1987).

[22] D. Ehrlich, J. H. Casseday, and E. Covey, J Neurophysiol

[12] M. Yoshioka, Phys Rev E Stat Nonlin Soft Matter Phys

Flows (W. A. Benjamin, Inc., 1967).

86, 364 (2001).

65, 011903 (2002).

[13] V. P. Zhigulin, M. I. Rabinovich, R. Huerta, and H. D.
Abarbanel, Phys Rev E Stat Nonlin Soft Matter Phys
67, 021901 (2003).

[14] H. Cateau and T. Fukai, Neural Comput 15, 597 (2003).
[15] C. C. Bell, K. Grant, and J. Serrier, J. Neurophysiol. 68,

843 (1992).

[26] R. Abraham and J. Robbin, Transversal Mappings and

[27] P. J. Davis, Circulant Matrices (John Wiley & Sons,

1979).

[28] P. D. Roberts, Neurocomputing 32-33, 243 (2000).
[29] V. Han, C. C. Bell, K. Grant, and Y. Sugawara, J. Comp.

Neurol. 404, 359 (1999).


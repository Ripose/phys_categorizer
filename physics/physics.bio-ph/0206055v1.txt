2
0
0
2
 
n
u
J
 
7
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
5
5
0
6
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Graph-driven features extraction from microarray
data

Jean-Philippe Vert and Minoru Kanehisa
Bioinformatics Center
Institute for Chemical Research
Kyoto University
Uji, Kyoto 611-0011, Japan
Jean-Philippe.Vert@mines.org
kanehisa@kuicr.kyoto-u.ac.jp

June 15, 2002

Abstract

Gene function prediction from microarray data is a ﬁrst step toward
better understanding the machinery of the cell from relatively cheap and
easy-to-produce data. In this paper we investigate whether the knowledge
of many metabolic pathways and their catalyzing enzymes accumulated
over the years can help improve the performance of classiﬁers for this
problem.

The complex network of known biochemical reactions in the cell re-
sults in a representation where genes are nodes of a graph. Formulating
the problem as a graph-driven features extraction problem, based on the
simple idea that relevant features are likely to exhibit correlation with
respect to the topology of the graph, we end up with an algorithm which
involves encoding the network and the set of expression proﬁles into ker-
nel functions, and performing a regularized form of canonical correlation
analysis in the corresponding reproducible kernel Hilbert spaces.

Function prediction experiments for the genes of the yeast S. Cere-
visiae validate this approach by showing a consistent increase in perfor-
mance when a state-of-the-art classiﬁer uses the vector of features instead
of the original expression proﬁle to predict the functional class of a gene.

Keywords: microarray, gene expression, network, pathway, diﬀusion ker-

nel, kernel CCA, feature extraction, function prediction.

1 Introduction

Following the near completion of many genome sequencing projects and the iden-
tiﬁcation of genes coding for proteins in these genomes, the research paradigm

1

is shifting toward a better understanding of the functions of the genes and their
interactions. This discipline, broadly called functional genomics is expected to
provide new insights into the machinery of the cell and suggest new therapeutic
targets by better focusing on the precise molecules or processes responsible for
a given disease.

Functional genomics has been boosted since the mid 1990’s by the intro-
duction of the DNA microarray technology [SSDB95, BB00], which enables the
monitoring of the quantity of messenger RNA (mRNA) present in a cell for
several thousands genes simultaneously, at a given instant. As mRNA is the in-
termediate molecule between the blueprint of a protein on the DNA strand and
the protein itself, it is expected that the quantity of mRNA reﬂects the quan-
tity of the protein itself, and that variations in the quantity of mRNA when a
cell is confronted to various experimental conditions reﬂects the genetic regu-
lation process. Consequently functional characterization of a protein from its
expression proﬁle as measured by several microarray hybridation experiments is
supposed to be possible to some extent, and initial experiments conﬁrmed that
many genes with similar function yield similar expression patterns [ESBB98].
As data accumulate the incentive to develop precise methods to assign functions
to genes from expression proﬁles increases.

Proteins can have many structural or functional roles. In particular proteins
known as enzymes catalyze chemical reactions which enable cells to acquire en-
ergy and materials from its environment, and to utilize them to maintain their
own biochemical network. Decades of careful experiments have helped charac-
terize many reactions taking place in the cell together with some of the genes
playing a role in their control, and this information has now been integrated
into several databases including WIT [OLP+00] or KEGG [KGKN02]. Such
databases provide a view of the set of proteins as the nodes of a large and
complex network, where two genes are linked when they catalyze two successive
reactions.

The question motivating this paper is whether this network can help im-
prove the performance of function prediction algorithms based on microarray
data only. To this end we propose a graph-driven feature extraction process from
the expression proﬁles, based on the idea that patterns of expression which cor-
respond to actual biological events, such as the activation of a series of chemical
reactions forming a chemical pathway, are likely to be shared by genes close to
each other with respect to the network topology. Translating this idea mathe-
matically we end up with a features extraction process equivalent to performing
a generalization of canonical correlation analysis (CCA) between the represen-
tations of the genes in two diﬀerent reproducing kernel Hilbert spaces, deﬁned
respectively by a diﬀusion kernel [KL02] on the gene graph and by a linear ker-
nel on the expression proﬁles. The CCA can be performed in these RKHS using
the kernel-CCA algorithm presented in [BJ01].

Relationships between expression proﬁles and biochemical pathways have
been subject to much investigation in the recent years. As microarray data are
much cheaper to produce than precise pathway data, pathway reconstruction
or validation from expression data has been attracting much attention since

2

the availability of public microarray data [FLNP00, AMK00]. Extraction of co-
clusters, i.e., clusters of genes in the network which have similar expression has
also been investigated recently [NGK01, HZZL02]. On the technical point of
view the integration of several sources of data has been investigated with diﬀer-
ent approaches, e.g., combining expression data and genomic location informa-
tion in a Bayesian framework [HGJY02], combining expression data with phylo-
genetic proﬁles by kernel operations [PWCG01], or deﬁning distances between
genes by combining distances measured from diﬀerent data types [MPT+99].

This paper is organized as follows. Section 2 translates mathematically the
feature extraction problem and contains basic notations and deﬁnitions, fol-
lowed by a short review of some properties of RKHS relevant for our purpose in
Section 3. Sections 4 and 5 describe respectively how two important properties
of features can be expressed in terms of norms in RKHS, and Section 6 describes
the feature extraction process. Experimental results are presented in Section 7,
followed by a discussion in Section 8.

2 Problem deﬁnition

2.1 Setting and notations

Before focusing on expression proﬁles and biochemical pathways, we ﬁrst formu-
late in a more abstract way the problem we are dealing with. The set of genes is
represented by a ﬁnite set
∈ X
represents a gene. The information provided by the microarray experiments and
the pathway database are represented respectively as:

= n, where each element x

of cardinality

|X |

X

•

a mapping e :
x, for any x in
sequel we assume that the proﬁles have been centered, i.e.:

Rp, where e(x) is the expression proﬁle for the gene
, and p is the number of measurements available. In the

X →
X

e(x) = 0.

x∈X
X

(1)

•

A simple graph Γ = (
vertices are the genes
genes, as extracted from the biochemical pathway database.

) (without loops and multiple edges) whose
represent the links between

,
E
and whose edges

X
X

E

{

∈ X

∼
} ∈ E

y for any (x, y)

The notation x
x, y

2 means that there is an edge between
. Our goal in the sequel is to use the graph Γ in order
x and y, i.e.,
to extract features from the expression proﬁles e relevant for the functional
classiﬁcation of the genes. In this context we formally deﬁne a feature to be a
= RX the set of possible features. The
R, and we denote by
mapping f :
F
set of centered features is denoted by
F0 =
. For any
feature f
the same notation is used to represent the n-dimensional vector
(cid:8)
, and f ′ denotes its transpose. The
f = (f (x))x∈X indexed by the elements of
X
constant unit vector is denoted 1 = (1, . . . , 1).

x∈X f (x) = 0

X →

∈ F

∈ F

P

(cid:9)

f

:

3

2.2 Feature relevance

Features can be derived from the mapping e. As an example, projecting e to a
given direction v

Rp gives the feature fe,v deﬁned for any x in

by:

∈

fe,v(x) = v′e(x).

X

(2)

{

G

∈

=

Rp

} ⊂ F

If v represents a particular expression pattern, then fe,v quantiﬁes how each
gene correlates with this pattern. In this paper we restrict ourselves to such
the set of linear features.
fe,v, v
linear features, and denote by
Observe that by hypothesis (1), each linear feature is also centered by (2), i.e.,
G ⊂ F0.
Biological events such as synthesis of new molecules or transport of sub-
strates usually require the coordinated actions of many proteins. Genes encod-
ing such proteins are therefore likely to share particular patterns of expression
over diﬀerent experimental conditions, e.g. simultaneous overexpression or in-
Rp representing this pattern should therefore be partic-
hibition. A vector v
ularly correlated (positively or negatively) with the genes participating in the
biological process. As a result, linear features fe,v corresponding to biologically
Rd are more likely to have a larger variance than those
relevant patterns v
corresponding to patterns unrelated to any biological event, where the variance
is deﬁned by:

∈

∈

fe,v

∀

∈ G

,

V (fe,v) =

x∈X fe,v(x)2
2
||

v
||

.

P

(3)

On the other extreme a pattern v

Rp orthogonal to all proﬁles leads to
∈
a feature fe,v with null variance, and is clearly unlikely to be related to any
biological process requiring gene expression.
It follows that the variance (3)
captured by a feature is a ﬁrst indicator of its biological pertinence. In order
to prevent confusion with other criteria in the sequel, we will call a feature
relevant if it captures much variations between expression proﬁles in the sense
of (3), and irrelevant otherwise. The reader can observe that searching for the
most relevant features can be done by performing a principal component analysis
(PCA) [Jol96] of the proﬁles, the ﬁrst principal components corresponding to
the most relevant features; however we now show that relevance is not the only
criterion which can be used to select features.

2.3 Feature smoothness

Relevance as deﬁned in Section 2.2 is an intrinsic properties of the set of proﬁles,
as it is deﬁned in terms of variation captured, and no other information about
the relationships between genes is used.

Independently of any microarray experiment, many metabolic pathways have
been experimentally characterized over the years. These collections of chemical
reactions involve proteins as enzymes, whose presence or absence plays a major
role in monitoring the reaction. Actual biological event usually involve series of

4

such reactions, also called pathways. Genes involved in consecutive reactions of
pathways are likely to share particular patterns of expression, corresponding to
the activation or not of the corresponding pathway.

As a result a pattern v

Rp which corresponds to a true biological event,
such as the activation or inhibition of a pathway, is likely to be shared by clusters
of genes in the graph of genes where two genes are linked if they participate in
consecutive reactions. On a more global scale, such a feature is more likely to
vary smoothly on the graph of genes, in the sense that variations between linked
genes be as small as possible, than a noisy pattern unrelated to any biochemical
event which would not exhibit any particular correlation between genes linked
to each other in the graph.

∈

Such features are called smooth in the sequel, by opposition to rugged fea-
tures which vary a lot with respect to the graph topology. These notions are
formalized and quantiﬁed in terms of a norm in a Hilbert space in Section 4, but
before developing these technicalities we can already sketch a feature extraction
process based on this intuitive deﬁnition.

2.4 Problem formulation

From the discussions in Sections 2.2 and 2.3 two criteria appear to characterize
“good” candidate features : their relevance on the one hand (Section 2.2) based
on a statistical analysis of the set of proﬁles, and their smoothness on the other
hand (Section 2.3) which results from the analysis of the variations of the feature
with respect to the topology of the graph of genes.

Good candidate features are smooth and relevant in the same time. These
two properties are however not always correlated: it might be possible to ﬁnd
many relevant but rugged features, as well as smooth but irrelevant features. A
reasonable approach to extract meaningful features is therefore to try to ﬁnd
a compromise between these two criteria, and to extract features which are as
smooth and relevant in the same time as possible.

Although this statement can be translated mathematically in many diﬀerent

ways, we investigate in the sequel the following formulation:

Problem 1 Extract pairs of features (f1, f2)

∈ F0 × G

such that:

f1 be smooth,

f2 be relevant,

•

•

f1 and f2 be correlated.

•
These three goals are usually contradictory and a trade-oﬀ must be found
between them. Observe that if either the smoothness or the relevance conditions
are removed, the problem is likely to be ill-posed. For instance, if the smooth-
ness requirement is removed then any relevant feature f2 is perfectly correlated
with itself; on the other hand if the relevance conditions disappears then many
smooth features f1 can probably be correlated with linear features which are

5

(4)

(5)

not necessarily relevant (this possibility increases when the dimension p of the
proﬁles increases, as the set of linear features increases too).

Let us now formulate Problem 1 mathematically. The correlation between

any two centered features (f1, f2)

2
0 is equal to:

∈ F

f ′
1f2
f ′
1f1

.

f ′
2f2

c(f1, f2) =

As already mentioned the maximization of c(f1, f2) over
problem.

p

p

F0 × G

is an ill-posed

Suppose we can deﬁne a smoothness functional h1 :

R+ for any feature,
F →
R+ for linear features, in such a way that
and a relevance functional h2 :
lower values of the functional h1 (resp. h2) corresponds to smoother (resp. more
relevant) features. Then one way to formalize the trade-oﬀ between correlation
and relevance / smoothness is to solve the following maximization problem:

G →

max
(f1,f2)∈F0×G

f ′
1f2
f ′
1f1 + δh1(f1)

f ′
2f2 + δh2(f2)

,

p

p

where δ is a regularization parameter. When δ = 0 we recover the ill-posed
problem of maximizing the correlation (4), and the larger δ the smoother (resp.
the more relevant) the feature f1 (resp. f2) which solves (5). As a result, a
solution (f1, f2) of (5) is a reasonable solution to Problem 1, with δ controlling
the trade-oﬀ between correlation on the one hand, smoothness and relevance on
the other hand.

Equation (5) is therefore the problem we consider is the sequel. In order to
solve it we need to 1) express the relevance and smoothness functional h1 and
h2 mathematically and 2) solve the maximization problem (5) with these func-
tionals. These two steps are not independent. In particular there is an incentive
to express mathematically h1 and h2 in such a way that (5) be computationally
solvable.

If f1 and f2 were restricted to be linear functionals obtained by projecting
two diﬀerent vector representations of the genes on particular directions, then
the maximization of (4) would be the exactly the ﬁrst canonical correlation
between f1 and f2 [Hot36], as obtained by classical canonical correlation analysis
(CCA). Linear algebra algorithms involving eigenvector decomposition exist to
perform CCA. However f1 is not restricted to be a linear feature, and (4) is
consequently ill-posed.

Formulated as (5), however, we recover a slight generalization of CCA intro-
duced in [BJ01] and called kernel-CCA. More precisely, kernel-CCA is formu-
lated as:

max
(f1,f2)∈H1×H2

f ′
1f1 + δ

f ′
1f2
f1||H1
||

f ′
2f2 + δ

f2||H2
||

,

(6)

p
H2 are two reproducible kernel Hilbert spaces (see Section 3)
. Problem (6) is equivalent to a generalized eigenvalue problem

p

where
on the space

H1 and
X

6

[BJ01] and can be solved iteratively to extract several pairs of features (see
Section 6.2).

In order to use the algorithm of [BJ01] we therefore need to restate (5) in
terms of optimization in RKHS like (6). This involves 1) expressing
F0 as a
as
RKHS whose norm is a smoothness functional (Section 2.3), 2) expressing
G
a RKHS whose norm is a relevance functional (Section 5), and 3) solving the
resulting problem (6).

3 Reproducing kernel Hilbert space

Before carrying out the program sketched in Section 2.4 we ﬁrst recall some
deﬁnitions and basic properties of RKHS in order to make this paper as self-
contained as possible. Good introductions on RKHS can be found in [Aro50,
Sai88, Wah90, SS02] from which we borrow most of the materials presented in
this section.

3.1 Basic deﬁnitions

be a set (which we don’t necessarily assume to be ﬁnite in this section),
R a symmetric positive deﬁnite function, in the sense that for
l Gram matrix Ki,j = K(xi, xj ) be

X →
N and (x1, . . . , xl)

l the l

Let
X
and K :
every l
positive semideﬁnite.

∈

∈ X

×

Then it is known that the linear span of set of functions

K(., x), x

∈ X } ⊂
RX which satisﬁes the following

{

RX can be completed into a Hilbert space
“reproducing property”:

H ⊂

(f, x)

∀

∈ H × X

,

f (x) =

K(., x), f
h

iH ,

(7)

where < ., . >H represents the inner product of
f = K(., x′) in (7) we obtain:

H

. In particular, by plugging

(x, x′)
∀

2,

K(., x), K(., x′)
h

iH = K(x, x′).

∈ X
The Hilbert space
is called a reproducing kernel Hilbert space [Aro50] to
emphasize the property (7). In order to make this rather abstract result clearer,
let us show how the space
is ﬁnite, which is the case of
interest in this paper.

can be built when

H

H

X

(8)

Let us therefore take

to be the ﬁnite set of genes, and suppose ﬁrst that
2 is positive deﬁnite,
the n
i.e., that its eigenvalues are all positive. It can then be diagonalized as follows:

n Gram matrix Kx,y = K(x, y) for any (x, y)

∈ X

×

X

n

K =

λiφiφ′
i,

i=1
X
where the eigenvalues satisfy 0 < λ1 ≤
is an associated orthonormal basis of eigenvectors.

. . .

≤

λn and the set (φ1, . . . , φn)

(9)

n

∈ F

7

(10)

(11)

(12)

(13)

∈ F

(14)

We can now take the Hilbert space to be

H
in terms of the decomposition of any f

, and deﬁne the inner product
in the basis of eigenvectors:

=

F
∈ H

in

H

as follows:

f =

aiφi,

n

i=1
X

n

n

aiφi,

biφi

=

*

i=1
X

i=1
X

+H

aibi
λi

.

n

i=1
X

It is easy to check that the Hilbert space deﬁned by (11) satisﬁes the reproducing
property (7), and is therefore a RKHS associated with the kernel K(., .).
The columns of the Gram matrix being independent, any feature f

can

∈ H

be uniquely represented as follows:

or in an equivalent matrix form:

f (.) =

α(x)K(x, .),

x∈X
X

f = Kα.

is called the dual coordinate of f .

This representation is called the dual representation of f , and the vector α =
(α(x))x∈X ∈ F
space
between two features (f, g)
is given by:

The dual representation is useful to express the inner product in the Hilbert
. Indeed, using (12) and (8) it is easy to check that the inner product
2 respectively

2 with dual coordinates (α, β)

∈ F

∈ F

H

f, g

iH =

h

α(x)β(y)K(x, y) = α′Kβ.

X(x,y)∈X 2
-norm of a feature f

∈ F

with dual coordinates α

is

In particular the

H

given by:

f
||

2
H = α′Kα.
||

The inner product in the original space L2(

with the dual representation: for any (f, g)
respectively we have by (13) and using the fact that K is symmetric:

) can also simply be expressed
2 with dual coordinates (α, β)

X
∈ F

f ′g =

f (x)g(x) = α′K 2β.

x∈X
X

In case the kernel K is just positive semideﬁnite, with r being the multiplicity
of 0 as eigenvalue, then we can follow the same construction with the index i
ranging from r + 1 to n in (9), (10) and (11). In that case the RKHS
is the
r. The dual representation still
linear span of
−
α
makes sense but is deﬁned up to an element of
{

, of dimension n

RX , Kα = 0

φr+1, . . . , φn

H

∈

{

}

}

.

8

3.2 RKHS and smoothness functional

One classical application of the theory of RKHS is regularization theory to solve
ill-posed problems [TA77, Iva76, Wah90, GJP95]. Indeed it is well known that
RN the norm in the
for many choices of kernels K(., .) on continuous spaces
||H is intimately related to the smoothness properties
corresponding RKHS
of the functions f
RN and
2.

The following classical example is relevant for us. Consider a set

a translation-invariant kernel of the form K(x, y) = k(x
Then the RKHS

is composed of the functions f

y) for any (x, y)
) such that:

f
||
.
∈ H

−
L2(

X ⊂

X ⊂

∈ X

H

∈

X

f
||

||H =

RN

Z

ˆf (ω)
2
|
|
ν(ω)

dω <

,
∞

(15)

(16)

where ˆf (ω) is the Fourier transform of f and ν(ω) is the Fourier transform of
k(.) [GJP95, SSM98]. Functionals of the form (15) are known to be smooth-
ness functionals (in which case smoothness is deﬁned in terms of Fourier trans-
form, i.e., smooth functions are functions with few energy at high frequency),
where the rate of decrease to zero of ν controls the smoothness properties of
the function in the RKHS. For example, for the Gaussian radial basis function
k(x

2/2σ2) the norm in the RKHS takes the form:
||

y) = exp(

−||

−

−

x

y

f
||

||H =

− p
2

2πσ2

2
σ
2 ||ω||

2

e

(cid:0)

(cid:1)

RN

Z

ˆf (ω)
2dω.
|
|

Equation (16) shows that the energy of f at a frequency ω should decrease
2/2) for its
-norm to be ﬁnite. Functions with much
at least as exp(
||
energy at high-frequency have a large norm in
, which therefore acts as a
smoothness functional.

σ2

H

H

−

ω

||

We refer the reader to [TA77, Iva76, Wah90, GJP95] for more details on the
connections between RKHS and smoothness functionals, as well as for applica-
tions to solve ill-posed problems. In the sequel we will adapt these approaches
to discrete spaces

in order to fulﬁll the program sketched in Section 2.4

X

4 Smoothness functional on a graph

As pointed out in Sections 2.4 our interest is now to derive a “smoothness
functional” for features f
with respect to the graph Γ expressed as a norm
in a RKHS.

∈ F

4.1 Fourier transform on graphs

Equation (15) shows that the norm in a RKHS on a continuous space associated
with a translation-invariant kernel is deﬁned in terms of Fourier transform. A
natural approach to adapt the construction of smoothing functional to functions
deﬁned on a graph is therefore to adapt the Fourier transform to that context.

9

As a matter of fact Fourier transforms on graphs have been extensively studied
in spectral graph theory [Chu97, Moh91, Moh97, Sta96], as we now recall.

Let D be the n

n diagonal matrix of vertex degrees of the graph Γ, i.e.,

×

(x, y)

∀

∈ X

2, Dx,y =

0
deg(x)

(

= y,
if x
if x = y,

where deg(x) is the number of edges involving x in Γ, and let A be the adjacency
matrix deﬁned by:

(x, y)

∀

∈ X

2, Ax,y =

1
0

(

if there is an edge between x and y in Γ,
otherwise .

Then the n

n matrix:

×

L = D

A

−

is called the (discrete) Laplacian of Γ. The discrete Laplacian L is a central
concept in spectral graph analysis [Moh97]. It shares many important properties
with the familiar diﬀerential operator

∆(.) = div(grad(.))

−

on Riemannian manifolds. It is symmetric, semideﬁnite positive, and singular.
The eigenvector (1, . . . , 1) belongs to the eigenvalue λ1 = 0, whose multiplicity
is equal to the number of connected components of Γ.

Let us denote by

0 = λ1 ≤
φi, i = 1, . . . , n

. . .

≤

λn

the eigenvalues of L and
an orthonormal set of associated
eigenvectors. Just like the Fourier basis functions are eigenfunctions of the
continuous Laplacian on RN , the eigenvectors of L can be regarded as a dis-
crete Fourier basis on the graph Γ [Sta96], with frequency increasing with their
eigenvalues.

{

}

Although the term “frequency” is not well deﬁned for functionals on a graph,
the reader can get an intuition of the fact that the functions (φi, i = 1, . . . , n)
“oscillates” more and more on the graph as i increases through the following
two well-known results:

Applying the classical equality [Moh97]:

•

f
∀

∈ F

,

f ′Lf =

(f (x)

f (y))2 ,

−

x∼y
X

to an eigenfunction φ of L with eigenvalue λ gives the following equality:

(φ(x)

φ(y))2 = λ.

−

x∼y
X

(17)

Equation (17) conﬁrms that the larger λ, the more the associated eigen-
function varies between adjacent vertices of the graph.

10

6
•

An other classical result concerns the number of maximal connected com-
ponents of the graph where a feature has a constant sign. The ﬁrst
eigenfunction being constant, it has only one such component, namely
the whole graph. For the other eigenfunctions, the discrete nodal do-
main theorem which translate Courant’s famous nodal theorem for ellip-
tic operators on Riemannian manifolds [Cha84] to the discrete settings
[dV93, Fri93, vdH96, DGL+01] states that the number of maximal con-
nected subsets of
where φi does not change sign is equal to i in the case
where all eigenvalues have multiplicity 1 (see a more general statement in
[DGL+01]). Together with the fact that each eigenfunction φi for i > 1
has zero mean (because it is orthogonal to the constant function φ1) this
shows that φi “oscillates” more and more on the graph, in the sense that
it changes sign more and more often as i increases.

X

By similarity with the continuous case the basis (φi)i=1,... ,n is called a
Fourier basis, higher eigenvalues corresponding to higher frequencies. Any fea-
ture f

can be expanded in terms of this basis:

∈ F

f =

ˆfiφi,

n

i=1
X

(18)

(19)

if and ˆf =

where ˆfi = φ′
is called the discrete Fourier transform
of f . This provides a way to analyze features in the frequency domain, and in
particular to measure their smoothness as we now show.

ˆf1, . . . , ˆfn
(cid:16)

(cid:17)

4.2 Graph smoothness functional

The Laplacian matrix L is semideﬁnite positive and can therefore be used as
a Kernel Gram matrix. The multiplicity of 0 as eigenvalue is the number of
connected components of the graph, and the associated eigenvectors are the
functions constant on each connected components. Following Section 3 the
associated RKHS
r and is made of the set of features with
zero mean on each connected component. By (11) the norm of any function
f

has dimension n

is given by:

H

−

∈ H

f
||

2
H =
||

m

ˆf 2
i
λi

,

i=r+1
X

where ˆf is the Fourier transform of f (18) and λ is the ordered set of eigenvalues
of L.

However, as shown in Section 4.1, the smoothness of φi decreases with i;
because λi increases with i, the norm (19) in the RKHS associated with the
kernel L increases with smoothness, and is therefore a “ruggedness functional”
instead of a smoothness functional in the sense deﬁned in Section 3. To illustrate
this we can observe that:

i
∀

∈ {

r + 1, . . . , n

,

φi

||H =

||

1
√λi

,

}

11

hence

φi

||H decreases with i.

||

Transforming this ruggedness functional into a smoothness functional can

be performed by a simple operation on the kernel as follows:

Deﬁnition 1 For any decreasing mapping ζ : R+
ζ-kernel Kζ :

R by:

2

R+

→

\{

}

0

, we deﬁne the

X

→

(x, y)

∀

∈ X

2,

Kζ(x, y) =

ζ(λi)φi(x)φi(y),

n

i=1
X

where 0 = λ1 ≤
(φ1, . . . , φn) an associated orthonormal Fourier basis.

. . .

≤

λn are the eigenvalues of the graph Laplacian and

The mapping ζ being assumed to take only positive values, the matrix Kζ is
.

deﬁnite positive and is therefore a valid kernel, with associated RKHS
From the discussion above it is now clear that:

H

=

F

Proposition 1 The norm
a smoothing functional, given for any feature f
ˆf

Rn by:

.
||
||

ζ in the RKHS associated with the kernel Kζ is
with Fourier transform

∈ F

∈

f
||

2
ζ =
||

n

ˆf 2
i
ζ(λi)

.

(20)

i=1
X
Proof Equation (20) is a direct consequence of Deﬁnition 1 and (11). The fact
that
ζ is a smoothing functional is simply a translation of the fact that ζ(λi)
decreases with i, hence the relative contribution of the Fourier components in
(20) increases with their frequency.

.
||
||

Proposition 1 shows that the smoothness functional associated with a func-
tion ζ is controlled by its rate of decrease to 0. An example of valid ζ function
with rapid decay is the following:

R+,

ζ(x) = e−τ x,

x

∀

∈

(21)

where τ is a parameter. In that case we recover the diﬀusion kernel introduced
and discussed in [KL02]. The authors of this paper show that the diﬀusion
kernel shares many properties with the continuous Gaussian kernel K(x, y) =
2/2σ2) on Rp, and can therefore be considered as its discrete
exp(
x
−||
||
version.

−

y

Combining (20) and (21) we obtain that the norm in the RKHS associated

with the diﬀusion kernel is given by:

f
∀

,

∈ F

ζ =

f
||

||

eτ λi ˆf 2
i ,

(22)

hence the high frequency energy of f is strongly penalized by this kernel, and
the penalization increases with the parameter τ .

n

i=1
X

12

L = Φ′ΛΦ,

Kζ = Φ′ζ(Λ)Φ,

Kζ = e−τ L.

Before continuing we should observe that in concrete applications the com-
putation of the kernel Kζ for a given ζ can be performed by diagonalizing the
Laplacian matrix as:

where Λ is a diagonal matrix with diagonal element Λi,i = λi, and computing:

where ζ(Λ) is a diagonal matrix with diagonal element ζ(Λ)i,i = ζ(λi). We can
also observe that the diﬀusion kernel can be written using the matrix exponential
as:

Although other choices of ζ lead to other kernels, discussing them would be
beyond the scope of this paper so we will restrict ourselves to using the diﬀusion
kernel as a smoothing functional in the sequel. The conclusion of this section
is that by using the diﬀusion kernel we can build a RKHS
whose norm
||H is a smoothness functional.
.
||

H

=

F

5 Relevance functional

Let us now consider the problem of deﬁning a relevance functional. First observe
Rp with orthogonal projection v0 on the linear span of
that any direction v
satisﬁes fe,v = fe,v0 . As a result the search of linear features
e(x), x
{
fe,v can be restricted to directions belonging to this linear span, which can be
parametrized as:

∈ X }

∈

v =

β(x)e(x),

x∈X
X

(23)

where β

is called the dual coordinate of v (deﬁned up to an element of

∈ F
, Kβ = 0

).

β

{

∈ F
The positive semideﬁnite Gram matrix Kx,y = e(x)′e(y), singular due to the
which consists of features of

}

centering of proﬁles (1), deﬁnes a RKHS
the form:

H ⊂ F

f (.) =

γ(x)K(x, .)

=

=

x∈X
X

x∈X
X

 

x∈X
X

γ(x)e(x)′e(.)

γ(x)e(x)

e(.),

′

!

where γ

. Equation (23) shows that

is exactly the set of linear features

, and by (14) the semi-norm of

∈ F

H
is given by:

G

H
,

fe,v

∀

∈ G

fe,v

||

||H = β′Kβ,

(24)

13

where β is the dual coordinate of v deﬁned by (23).

On the other hand, combining (2), (3) and (23) shows that the variance of
can be expressed in terms of the dual coordinate β of v as

a feature fe,v
follows:

∈ G

V (fe,v) =

P

x∈X fe,v(x)2
2
v
||
||
(v′e(x))2
v′v

x∈X
X
β′K 2β
β′Kβ

.

=

=

From this we see that the larger the ratio between β′K 2β and β′Kβ the more
relevant the feature fe,v, where v has dual coordinates β. By observing that
e,vfe,v = β′K 2β, and by (24) we see that a natural
fv,e = Kβ and therefore f ′
relevance functional to plug into (5) in order to counterbalance the eﬀect of f ′
1f1
is the following:

h2(fe,v) = β′Kβ =

fe,v

||H.

||

(25)

Indeed the larger h2(fe,v) compared to f ′
the less variation is captured by fe,v. The functional (25) is deﬁned on
norm of a RKHS, which was the goal assigned in Section 2.4.

e,vfe,v the smaller V (fe,v), and therefore
as the

G

6 Extracting smooth correlations

6.1 Dual formulation

Let us now put together the elements we have developed up to now. In Section
4 we have shown that any feature f

can be represented as:

∈ F
f = K1α,

where K1 is the diﬀusion kernel Gram matrix derived from the Laplacian ma-
τ L), and α is the dual coordinate vector of f in the
trix L by K1 = exp(
−
corresponding RKHS
. Moreover, we deﬁned a smoothness functional
H1 =
as:
f
∀

||H1 = α′K1α.
In Section 5 we showed that every linear feature fe,v

can also be repre-

h1(f ) =

f
||

∈ F

F

,

∈ G

sented in a dual form:

fe,v = K2β,

where K2 is the kernel Gram matrix K2(x, y) = e(x)′e(y) for any (x, y)
and β is the dual coordinate vector in the corresponding degenerate RKHS
H2 =

. Moreover a relevance functional was deﬁned as:
||H2 = β′K1β.

he(fe,v) =

Rp,

f
||

v
∀

∈ X

∈

G

2

14

Plugging these results into (5) leads to the following formulation of the initial

problem in terms of dual coordinates:

max
(α,β)∈F 2

γ(α, β),

(26)

with

γ(α, β) =

α′K1K2β

1

(α′ (K 2

1 + δK1) α)

2 (β′ (K 2

2 + δK2) β)

.

1
2

(27)

F × G

F0 × G

instead of

Observe that this is the dual formulation of (5) except that the optimization
is done in
. Moreover, in order keep the interpretation
||H1 as a smoothing functional the kernel K1 should not be centered in
.
of
||
the feature space, as in usual kernel CCA [BJ01] and kernel PCA [SSM99].
As the following Proposition shows, this is however not a problem because the
features whose dual coordinates maximize (26) are centered anyway, and the
optimization in for f1 ∈ F
F0:
Proposition 2 For any (α, β)
tered version of f = K1α, i.e.:

2, let α0 be the dual coordinate of the cen-

is therefore equivalent to the maximization for f

∈ F

∈

Then the following holds:

R, K1α0 = K1α + ǫ1,

∈
x∈X K1α0(x) = 0.

(

ǫ
∃

P

γ(α0, β)

γ(α, β),

≥

with equality if and only if α = α0.
coordinates α and β solve (26) are centered.

In particular, the features whose dual

Proof Because the proﬁles
K21 = 0, and therefore:

{

∈ X }

e(x), x

are supposed to be centered we have

α′K1K2β = (α′

0K1 + ǫ1′)K2β = α′

0K1K2β.

Let (φ1, . . . , φn) denote an orthonormal Fourier basis, where φ1 is constant.
Then any feature f = K1α is centered by removing the contribution of φ1 in its
Fourier expansion, i.e.,

f0 = K1α0 =

ˆfiφi.

n

i=2
X

15

As a result we obtain from (11):

α′K1α =

=

f
||
n

||H1
ˆf 2
i
λi

i=1
X
n

≥

ˆf 2
i
λi
i=2
X
Kα0||H1
=
||
= α0K1α0,

where the inequality on the third line is an equality if and only if ˆf1 = 0, i.e., f
is centered. Moreover, using Pythagorean equality in L2(
) for the orthogonal
vectors 1 and Kα0 we easily get:

X

α′K 2

1 α =
=

f
||L2(X )
||
K1α0 + ǫ1
||
K1α0||
||
K1α0||
0K 2
1 α0

2
L2(X )
||
2
L2(X ) +
2
L2(X )

||

=

≥ ||
= α′

ǫ1

2
L2(X )
||

Combining this inequalities with the deﬁnition of γ (26) proves the Lemma.

6.2 Features extraction

Stated as (26) the problem is similar to the kernel canonical correlation problem
studied in [BJ01]. In particular, by diﬀerentiating with respect to α and β we see
that (α, β) is a solution of (26) if and only if it satisﬁes the following generalized
eigenvalue problem:

0
K2K1

K1K2
0

α
β

= ρ

K 2

1 + δK1
0

(cid:19) (cid:18)

(cid:19)

(cid:18)

0

K 2
2 + δK2 (cid:19) (cid:18)

α
β

(cid:19)

(cid:18)

(28)

with ρ the largest possible. The reader is referred to [BJ01] for details about
the derivation of (28). Let ¯n = min(n, p). As pointed out in this paper solving
(28) provides a series of pairs of features:

(αi, βi) , i = 1, . . . , ¯n

{
with decreasing values of γ(αi, βi) for which the gradient
α,βγ is null, equiv-
alent to the extraction of successive canonical directions with decreasing corre-
lation in classical CCA. The resulting features f1,i = K1αi and f2,i = K2βi are
therefore a set of features likely to have decreasing biological relevance when i
increases, and are the features we propose to extract in this paper.

∇

}

The classical way to solve a generalized eigenvalue problem Bρ = λCρ is
to perform a Cholesky decomposition of C as C = E′E, to deﬁne µ = Eρ and

16

to solve the standard eigenvector problem (E′)−1BE−1µ = λµ. However the
matrix K 2
2 + δK2 is singular so it must be regularized for this approach to be
numerically stable. Following [BJ01] this can be done by adding δ2/4 on the
diagonal, and observing that:

K 2 + δK +

I =

K +

δ2
4

(cid:18)

2

,

δ
2

I

(cid:19)

leads to the following regularized problem:

0
K2K1

K1K2
0

α
β

= ρ

(cid:19) (cid:18)

(cid:19)

(cid:18)

(cid:18)

(K1 + δ′I)2
0

0
(K2 + δ′I)2

α
β

(cid:19) (cid:18)

(cid:19)

,

(29)

where δ′ = δ/2. If (α, β) is an generalized eigenvector solution of (29) belonging
ρ. As a result the
to the generalized eigenvalue ρ, then (
spectrum of (29) is symmetric : (ρ1,
ρn,
ρi = 0 for i > p.

α, β) belong to
−
ρ1, . . . , ρn,

−
ρn) with ρ1 ≥

. . .

≥

−

−

6.3 Feature extraction process

K2βi, i =
Solving (29) results in two sets of features
{
. Features of the form Kα1 are computed from the position of the
1, . . . , ¯n
}
genes in the gene graph, while features of the form K2β are computed from the
expression proﬁles.

K1αi, i = 1, . . . , ¯n

and

}

{

In concrete applications, the position of a still uncharacterized gene in the
gene graph is not known, while its expression proﬁle can be measured. As a
result the only way to extract features for such a gene is to use the features
. These features are obtained by projecting the expression
K2βi, i = 1, . . . , ¯n
{
}
proﬁles to the respective directions:

vi =

βi(x)e(x),

i = 1, . . . , ¯n.

(30)

x∈X
X

Therefore features can be extracted from any expression proﬁle e by projections
on these directions. We can now summarize a typical use of the the feature
extraction process presented in this paper as follows:

is supposed to be the disjoint union of two subsets

The set of genes
X
and
X1 are present in the gene network
genes which have been assigned a precise role in a pathway, while
the set of uncharacterized genes.

X1
X2. Expression proﬁles are measured for all genes, but only genes in
X1 is the set of
G
X2 is

). Hence

X1,

= (

E

Use the set

e(x), x

X1 to extract features from the set of expression proﬁles
, by solving (29).
using the graph

∈ X1}

{
Derive a set of expression patterns by (30).

G

•

•

•

17

•

Extract features from the expression proﬁles
them on the derived expression patterns.

{

e(x), x

by projecting

∈ X2}

This process provides a way to replace the expression patterns of an unchar-
acterized gene by a vector of features which hopefully are more biologically
relevant than the raw proﬁles themselves. Any data mining algorithms, e.g.
clustering of functional classiﬁcation methods, can then be applied on this new
representation.

7 Experiments

In order to evaluate the relevance of the pathway-driven features extraction pro-
cess presented in this paper we performed functional classiﬁcation experiments
with the genes of the yeast Saccharomyces Cerevisiae. The main goal of these
experiments is to test whether a state-of-the-art classiﬁer, namely a support
vector machine, performs best by working directly with the expression proﬁles
of the genes, or by using the vectors of features.

7.1 Pathway data

The LIGAND database of chemical compounds and reactions in biological path-
ways [GOH+02, GNK98] is part of the Kyoto Encyclopedia of Genes and Genomes
(KEGG) [KGKN02, Kan97]. As of February 2002 it consists of a curated set
of 3579 metabolic reactions known to take place in some organisms, together
with the substrates involved and the classiﬁcation of the catalyzing enzyme as
an EC number.To each reaction are associated one or several EC numbers, and
to each EC number are associated one or several genes of the yeast genome.
Using this information we created a graph of genes by linking two genes when-
ever they were assigned two EC number known to catalyze two reactions which
share a common main compound (secondary compounds such as water or ATP
are discarded).

In other words two genes are linked in the resulting graph if they have the
possibility to catalyze two successive reactions, the main product of the ﬁrst
one being the main substrate of the second one. Although it is far from being
certain that all the genes candidates to catalyze a given reaction (because they
are assigned an EC number supposed to represent a family of potential enzymes
catalyzing the reaction) actually catalyze it in the cell, these data nevertheless
provide a global picture of the possible relationships between genes in terms
of catalyzing properties. In particular a path in this graph corresponds to a
possible series of reactions catalyzed by the successive genes met along the path.
The resulting graph involves 774 genes of S. Cerevisiae, linked with 16,650

edges.

18

7.2 Microarray data

Publicly available microarray expression data were collected from the Stanford
Microarray Database [SHBK+01]. The data include yeast response to various
experimental conditions, including metabolic shift from fermentation to respi-
ration [DIB97], alpha-factor block release, cdc15 block release, elutriation time
course, cyclin over-expression [SSZ+98], sporulation [CDE+98], adaptive evo-
lution [FBBR99], stress response [GSK+00], manipulation in phosphate level
[ODB00], cell cycle [ZSV+00], growth conditions of excess copper or copper
deﬁciency [GKI+00], DNA damage response [GHM+01], and transfer from a
fermentable to a nonfermentable carbon source [KDBS01].

Combining these data results in 330 data points available for 6075 genes,
i.e., almost all known or predicted genes of S. cerevisiae. Each data point
produced by a DNA microarray hybridation experiment represents the ratio
of expression levels of a particular gene under two experimental conditions.
Following [ESBB98, BGL+00] we don’t work directly with this ratio but rather
with its normalized logarithm deﬁned as:

(x, i)

∀

∈ X × {

1, . . . , 330

,

e(x)i =

}

log Ex,i/Rx,i
330
j=1 log2 Ex,i/Rx,i

,

qP

where Ex,i is the expression level of gene x in experiment i and Ri is the expres-
sion level in the corresponding reference state. Missing values were estimated
with the software KNNimpute [TCS+01].

7.3 Functional classes

The January 10, 2002, version of the functional classiﬁcation catalogue of the
Comprehensive Yeast Genome Database (CYGD) [MFG+02] is a comprehen-
sive classiﬁcation of 3936 yeast genes into 259 functional classes organized in
a hierarchy. The classes vary in size between 1 and 2258 genes (for the class
“subcellular localization”), and not all of them are supposed to be correlated
with gene expression [BGL+00]. Only classes with at least 20 genes (after re-
moving the genes present in the gene graph, see next Section) are considered
as benchmark datasets for function prediction algorithm in the sequel, which
amounts to 115 categories.

7.4 Gene function prediction

Following the general approach presented in Section 6.3 the gene prediction
experiment involves two steps:

•

The 669 genes in the gene graph derived from the pathway database with
known expression proﬁles are used to perform the feature extraction pro-
cess by solving (30).

19

•

The resulting linear features are extracted from the expression proﬁles of
the disjoint set of 2688 genes which are in the CYGD functional cata-
logue but not in the pathway database. Systematic evaluation of the per-
formance of support vector machines to predict each CYGD class either
from the expression proﬁles themselves [BGL+00] or from the features ex-
tracted is then performed on this set of genes using 3-fold cross-validation
averaged over 10 iterations.

Support vector machine (SVM) [Vap98, CST00, SS02] is a class of machine
learning algorithms for supervised classiﬁcation which has been shown to per-
form better that other machine learning techniques, including Fisher’s linear
discriminant, Parzen windows and decision trees on the problem of gene func-
tional classiﬁcation from expression proﬁles [BGL+00]. We therefore use SVM
as a state-of-the-art learning algorithm to assess the gain resulting from replac-
ing the original expression proﬁles by vectors of features.

y

x

−

−||

Experiments were carried out with SVM Light [Joa99], a public and free
implementation of SVMs. To ensure a comparison as fair as possible between
diﬀerent data representations, all vectors were scaled to unit length before being
sent to the SVM, and all SVM used a radial basis kernel with unit width, i.e.,
2). The trade-oﬀ parameter between training error and
k(x, y) = exp(
||
margin was set to its default value (namely 1 in the case where all vectors have
unit length), and the cost factor by which training errors on positive examples
outweigh errors on negative examples was set equal to the ratio of the number
of positive examples and the number of negative examples in the training set.
We compared the performance of SVM working directly on the expression
proﬁles. as in [BGL+00], with SVM working on the vectors of features extracted
by the procedure described in this paper, for various choices of regularization
parameters δ, width of the diﬀusion kernel τ and numbers of features selected.
For each experiment the performance is measured by the ROC index, deﬁned
as the area under the ROC curve, i.e., the plot of true positives versus false
positives, and normalized to 100 for a perfect classiﬁer. The ROC curve itself
is obtained by varying a threshold and classify genes by comparing the score
output by the SVM with this threshold. A random classiﬁer has an average
ROC index of 50.

7.5 Setting the parameters

Our feature extraction process contains two free parameters, namely the width
Intuitively, the
τ of the diﬀusion kernel and the regularization parameter δ.
larger τ and δ, the smoother and more relevant the features extracted, at the
expense of a decrease between their correlations. As pointed out in [BJ01] the
parameter δ is expected to decrease linearly with n, and a reasonable value is
δ = 0.001 for n of the order of 1000. An initial value of τ = 1 was chosen.

We varied independently δ and τ in order to check their inﬂuence. For
a ﬁxed δ = 0.001 we tested the performance of SVM based on the features
, where all 330 features are used.
0.5, 1, 2, 5
extracted with the parameter τ

∈ {

}

20

Average ROC Percentage of classes best predicted

Table 1: Performance comparison for various τ
τ
0.5
1
2
5

61.4
61.4
60.0
55.2

37
35
20
8

δ
0.001
0.001
0.001
0.001

δ
0.0005
0.001
0.002
0.005

Table 2: Performance comparison for various δ
τ Average ROC Percentage of classes best predicted
1
1
1
1

61.4
61.4
61.4
61.6

17
18
25
39

Table 1 shows the ROC index averaged over all 115 classes with more than
20 genes for each of the four SVM, as well as the percentage of classes best
predicted by each method. The best performance is reached for τ = 1, with
an important deterioration when τ increases to 5. A larger τ means by (22)
that rugged features are more strongly penalized, so larger τ tend to generate
smoother features. The deterioration when τ increases shows the importance of
not excessively penalizing ruggedness.

We also checked the inﬂuence of the regularization parameter δ, which con-
trols the trade-oﬀ between correlation on the one hand, smoothness and rele-
vance on the other hand. Table 2 compares the performances of SVM based on
the features extracted with the parameters τ = 1 and δ
This shows a small (in terms of ROC index increase) but consistent (in terms
of number of classes best predicted) increase in performance when δ increases
from 0.0005 to 0.005. This illustrates the importance of regularization, and
therefore the improvement gained by imposing some smoothness and relevance
constraints to the features.

∈ {

0.0005, 0.001, 0.002, 0.005
}

.

7.6 Number of features

From now on we ﬁx the parameters to τ = 1 and δ = 0.001. As the feature
extraction process is supposed to extract up to p = 330 features by decreasing
biological relevance, one might ask if classiﬁcation performance could increase
by only keeping the most relevant features, and hopefully removing noise by
discarding the remaining ones. To check this we measured the performance of
SVM using an increasing number of features. Results are shown on Table 3, and
show that it is on average more interesting to use all features as the performance
increases with the number of features used. Exceptions to this average principle
include classes such as fermentation, ionic homeostasis, assembly of protein
complexes, vacuolar transport, phosphate metabolism or nucleus organization,
which are better predicted with less than 100 features as shown on Figure 7.6

21

Table 3: Performance comparison for various numbers of features, with δ =
0.001 and τ = 1

Number of features Average ROC Percentage of classes best predicted

50
100
150
200
250
300
330

55.3
57.9
58.9
59.9
60.6
61.2
61.4

3
10
9
7
17
17
37

"fermentation"
"ionic_homeostasis"
"protein_complexes"
"vacuolar_transport"
"nucleus_organization"

75

70

65

60

55

50

x
e
d
n
i
 

C
O
R

45

50

100

150

250

300

350

200
Number of features

Figure 1: Classiﬁcation performance for various classes

22

100

90

80

70

60

50

40

s
e

l
i
f

i

 

o
r
p
n
o
s
s
e
r
p
x
e

 

 

n
o
d
e
s
a
b

 
x
e
d
n

i
 

C
O
R

30

40

50

60

70
ROC index based on extracted features

80

90

100

Figure 2: Comparison of the classiﬁcation performance of SVM based on ex-
pression proﬁles (y axis) or extracted features (x axis). Each point represents
one functional class.

7.7 Functional classiﬁcation performance

In order to check whether the features extraction provides any advantage over
the direct use of expression proﬁles for gene function prediction we ﬁnally com-
pared the performance of a SVM using all features extracted with the parameters
δ = 0.001 and τ = 1, with the performance of a SVM using directly the gene
expression proﬁles. Figure 7.7 shows the ROC index obtained by each of the
two methods for all 115 functional classes. Except for a few classes, there is a
clear improvement in classiﬁcation performance when the genes are represented
as vectors of features, and not directly as expression proﬁles.

Table 4 shows that the ROC index averaged over all classes increases signif-
icantly between the two representations (from 54.9 to 61.2). Moreover Figure
7.7 shows that most of the classes seem almost impossible to learn from their
expression proﬁles only (when the ROC index is around 45 - 55, i.e. not bet-
ter than a random classiﬁer), but can somehow be learned by their vectors of
features, as the ROC index jumps in the range 55-65 for many of those classes.
Some classes exhibit a dramatic increase in ROC index, as shown in Table 5
which lists the classes largest absolute increase in ROC index between the two
experiments.

23

Table 4: ROC index averaged over 115 functional classes by SVM using diﬀerent
representations of the data

Data representation Average ROC
Expression proﬁles
Vector of features

54.6
61.4

Table 5: ROC index for the prediction of categories based on expression proﬁles
or features vectors. The categories listed are the one which exhibit the largest
increase in ROC index between these two representations.

Expression Features

Class
Heavy metal ion transporters (Cu, Fe, etc.)
Ribosome biogenesis
Protein synthesis
Directional cell growth (morphogenesis)
Regulation of nitrogen and sulphur utilization
Nitrogen and sulfur metabolism
Translation
Cytoplasm
Endoplasmic reticulum
Amino acid transport

55.2
70.9
61.6
44.3
49.0
44.3
50.7
55.0
59.5
75.1

83.5
94.6
84.3
64.7
68.6
63.8
69.8
73.4
77.0
58.3

Increase
+28.3
+23.7
+22.7
+20.4
+19.6
+19.5
+19.1
+18.4
+17.5
+16.8

8 Discussion and conclusion

This paper proposes an algorithm to extract features from gene expression pro-
ﬁles based on the knowledge of a biochemical network linking a subset of genes.
Based on the simple idea that relevant features are likely to exhibit correla-
tion with respect to the topology of the network, we end up with a formulation
which involves encoding the network and the set of expression proﬁles into to
kernel functions, and performing a regularized canonical correlation analysis in
the corresponding reproducible kernel Hilbert spaces.

Results presented in Section 7 are encouraging and conﬁrm the intuition that
incorporating valuable information, such as the knowledge of the precise position
of many genes in a biochemical network, helps extracting relevant informations
from expression proﬁles. While this problem has still attracted relatively few at-
tention because the number of expression data has always been small compared
to the number of genes until recently, it is expected to be more and more impor-
tant as the production of expression data becomes cheaper and the underlying
technology more widespread.

A detailed analysis of the experimental results reveals that functional cate-
gories related to metabolism, protein synthesis and subcellular localization ben-
eﬁt the most from the representation of genes as vectors of features. In the case
of metabolism and protein synthesis related categories, this can be explained by
the fact that many pathways related to this process are present in the pathway

24

database, so relevant features have probably been extracted. The case of sub-
cellular localization proteins is more surprising, as they seem to be more related
to structural properties than functional properties of the genes, but certainly
reﬂects the functional role of the organelles themselves. As an example a sud-
den need of energy might promote the activity in mitochondria and require the
synthesis of proteins to be directed to this location, even though they might not
be directly involved as enzymes.

On the technical point of view the approach developed in this paper can
be seen as an attempt to encode various types of information about genes into
kernels. The diﬀusion kernel K1 encodes the gene network, and the linear
kernel K2 summarizes the expression proﬁles. Recent research shows that this
approach can in fact be generalized to many other sources of information about
genes, as many kernels have been engineered and continue to be developed for
particular types of data. Apart from classical kernels for ﬁnite-dimensional
real-valued vectors [Vap98] which can be used to encode any vectorial gene
representation, e.g. expression proﬁles, and from diﬀusion kernels which can
encode any gene network, e.g. network derived from biochemical pathway or
protein interaction networks, relevant examples of recently developed kernels
include the Fisher kernel to encode how the amino-acid sequence of a protein is
related to a given hidden Markov model [JDH00] or to encode the arrangement of
transcription factor binding site motifs in its promoter region [PWCG01], several
string kernels to encode the information present in the amino-acid sequence
itself [Hau99, Wat00, LEN02, Ver02a, LSST+02], or a tree kernel to encode
the phylogenetic proﬁle of a protein [Ver02b]. This increasing list suggests a
uniﬁed framework to represent various types of informations, which is obtained
by “kernelizing the proteome”, i.e., tranforming any type of information into an
adequate kernel.

Parallel to the apparition of new kernels recent years have witnessed the
development of new methods, globally referred to as kernel methods, to perform
various data mining algorithm from the knowledge of the kernel matrix only.
Apart from the most famous support vector machine algorithm for classiﬁcation
and regression [BGV92, Vap98], other kernel methods include principal compo-
nent analysis [SSM99], clustering [BHHSV01], Fisher discriminants [MRW+99]
or independent component analysis [BJ01].

These recent developments open the door to new analysis opportunities
which we believe can be particularly suited to the new discipline of proteomics
whose central concepts, genes or proteins, are deﬁned through a variety of dif-
ferent points of view (as sequences, structures, expression patterns, position in
networks, ...), the integration of which promises to unravel some of the secrets
of life.

9 Acknowledgements

We would like to thank Yasushi Okuno for help and advices with the pathway
data, and Olivier Bousquet for simulating discussions. This work was supported

25

by the Research for the Future Program of the Ministry of Education, Culture,
Sports, Science and Technology, Japan.

References

[AMK00]

T. Akutsu, S. Miyano, and S. Kuhara. Inferring qualitative rela-
tions in genetic networks and metabolic pathways. Bioinformatics,
16(8):727–734, 2000.

[Aro50]

[BB00]

N. Aronszajn. Theory of reproducing kernels. Transactions of the
American Mathematical Society, 68:337 – 404, 1950.

P.O. Brown and D. Botstein. Exploring the new world of the
genome with dna microarrays. Nature Genetics, 21:33–37, 2000.

[BGL+00] Michael P. S. Brown, William Noble Grundy, David Lin, Nello Cris-
tianini, Charles Walsh Sugnet, Terence S. Furey, Jr. Manuel Ares,
and David Haussler. Knowledge-based analysis of microarray gene
expression data by using support vector machines. Proc. Natl.
Acad. Sci. USA, 97:262–267, 2000.

[BGV92]

B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm
In Proceedings of the 5th annual
for optimal margin classiﬁers.
ACM workshop on Computational Learning Theory, pages 144–152.
ACM Press, 1992.

[BHHSV01] Asa Ben-Hur, David Horn, Hava T. Siegelmann, and Vladimir Vap-
nik. Support vector clustering. Journal of Machine Learning Re-
search, 2:125–137, 2001.

[BJ01]

F. R. Bach and M. I. Jordan. Kernel independent component anal-
ysis. Technical Report UCB//CSD-01-1166, UC Berkeley, 2001.

[CDE+98] S. Chu, J. DeRisi, M. Eisen, J. Mulholland, D. Botstein, P.O.
Brown, and I. Herskowitz. The transcriptional program of sporu-
lation in budding yeast. Science, 282:699–705, 1998.

[Cha84]

[Chu97]

[CST00]

I. Chavel. Eigenvalues in Riemannian geometry. Academic Press,
Orlando, Fl., 1984.

Fan R.K. Chung. Spectral graph theory, volume 92 of CBMS Re-
gional Conference Series. American Mathematical Society, Provi-
dence, 1997.

Nello Cristianini and John Shawe-Taylor. An introduction to Sup-
port Vector Machines and other kernel-based learning methods.
Cambridge University Press, 2000.

26

[DGL+01] E. B. Davies, G. M. L. Gladwell, J. Leydold, , and P. F. Stadler.
Discrete nodal domain theorems. Lin. Alg. Appl., 336:51–60, 2001.

[DIB97]

[dV93]

Joseph L. DeRisi, Vishwanath R. Iyer, and Patrick O. Brown. Ex-
ploring the metabolic and genetic control of gene expression on a
genomic scale. Science, 278(5338):680–686, 1997.

Y.C. de Verdi`ere. Multiplicit´es des valeurs propres Laplaciens dis-
crets et Laplaciens continus. Rendiconti di Matematica, 13:433–460,
1993.

[ESBB98] Michael B. Eisen, Paul T. Spellman, Patrick O. Brown, and David
Botstein. Cluster analysis and display of genome-wide expression
patterns. Proc. Natl. Acad. Sci. USA, 95:14863–14868, Dec 1998.

[FBBR99] Tracy L. Ferea, David Botstein, Patrick O. Brown, and R. Frank
Rosenzweig. Systematic changes in gene expression patterns fol-
lowing adaptive evolution in yeast. Proc. Natl. Acad. Sci. USA,
96(17):9721–9726, 1999.

[FLNP00] Nir Friedman, Michal Linial, Iftach Nachman, and Dana Pe’er.
Using bayesian networks to analyze expression data. Journal of
Computational Biology, 7(3-4):601–620, 2000.

[Fri93]

J. Friedman. Some geometric aspects of graphs and their eigen-
functions. Duke MAthematical journal, 69:487–525, March 1993.

[GHM+01] A.P. Gasch, M. Huang, S. Metzner, D. Botstein, S.J. Elledge,
and P.O. Brown. Genomic expression responses to DNA-damaging
agents and the regulatory role of the yeast ATR homolog Mec1p.
Mol. Biol. Cell, 12(10):2987–3003, 2001.

[GJP95]

Frederico Girosi, Michael Jones, and Tomaso Poggio. Regulariza-
tion theory and neural networks architectures. Neural Computa-
tion, 7(2):219–269, 1995.

[GKI+00] C. Gross, M. Kelleher, V.R. Iyer, P.O. Brown, and D.R. Winge.
Identiﬁcation of the copper regulon in saccharomyces cerevisiae by
DNA microarrays. J. Biol. Chem., 275(41):32310–32316, 2000.

[GNK98]

S. Goto, T. Nishioka, and M. Kanehisa. LIGAND: chemical
database for enzyme reactions. Bioinformatics, 14:591–599, 1998.

[GOH+02] S. Goto, Y. Okuno, M. Hattori, T. Nishioka, and M. Kanehisa.
LIGAND: database of chemical compounds and reactions in bio-
logical pathways. Nucleic Acid Research, 30:402–404, 2002.

[GSK+00] Audrey P. Gasch, Paul T. Spellman, Camilla M. Kao, Orna
Carmel-Harel, Michael B. Eisen, Gisela Storz, David Botstein, and
Patrick O. Brown. Genomic expression programs in the response of

27

yeast cells to environmental changes. Mol. Biol. Cell, 11:4241–4257,
Dec 2000.

[Hau99]

David Haussler. Convolution kernels on discrete structures. Tech-
nical report, UC Santa Cruz, 1999.

[HGJY02] A.J. Hartemink, D.K. Giﬀord, T.S. Jaakkola, and R.A. Young. Us-
ing graphical models and genomic expression data to statistically
validate models of genetic regulatory networks. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Kevin Lauerdale, and Teri E.
Klein, editors, Proceedings of the Paciﬁc Symposium on Biocom-
puting 2002, pages 422–433. World Scientiﬁc, 2002.

[Hot36]

H. Hotelling. Relation between two sets of variates. Biometrika,
28:322–377, 1936.

[HZZL02] D. Hanisch, A. Zien, R. Zimmer, and T. Lengauer. Co-clustering
of biological networks and gene expression data. Bioinformatics,
2002.

[Iva76]

[JDH00]

[Joa99]

[Jol96]

[Kan97]

V.V. Ivanov. The theory of approximate methods and their ap-
plication to the numerical solution of singular integral equations.
Nordhoﬀ International, Leiden, 1976.

Tommi Jaakkola, Mark Diekhans, and David Haussler. A discrimi-
native framework for detecting remote protein homologies. Journal
of Computational Biology, 7(1,2):95–114, 2000.

Thorsten Joachims. Making large-scale svm learning practical. In
B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 169–184. MIT Press,
1999.

I.T. Jolliﬀe. Principal component analysis. Springer-Verlag, New-
York, 1996.

M. Kanehisa. A database for post-genome analysis. Trends Genet.,
13:375–376, 1997.

[KDBS01] K.M. Kuhn, J.L. DeRisi, P.O. Brown, and P. Sarnow. Global and
speciﬁc translational regulation in the genomic response of Sac-
charomyces cerevisiae to a rapid transfer from a fermentable to
a nonfermentable carbon source. Mol. Cell. Biol., 21(3):916–927,
2001.

[KGKN02] M. Kanehisa, S. Goto, S. Kawashima, and A. Nakaya. The KEGG
databases at GenomeNet. Nucleic Acid Research, 30:42–46, 2002.

[KL02]

R. I. Kondor and J. Laﬀerty. Diﬀusion kernels on graphs and other
discrete input. In ICML 2002, 2002.

28

[LEN02]

Christina Leslie, Eleazar Eskin, and William Staﬀord Noble. The
spectrum kernel: a string kernel for svm protein classiﬁcation.
In Russ B. Altman, A. Keith Dunker, Lawrence Hunter, Kevin
Lauerdale, and Teri E. Klein, editors, Proceedings of the Paciﬁc
Symposium on Biocomputing 2002, pages 564–575. World Scien-
tiﬁc, 2002.

[LSST+02] Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristian-
ini, and Chris Watkins. Text classiﬁcation using string kernels.
Journal of Machine Learning Research, 2:419–444, 2002.

[MFG+02] H.W. Mewes, D. Frishman, U. G¨uldener, G. Mannhaupt, K. Mayer,
M. Mokrejs, B. Morgenstern, M. M¨unsterkoetter, S. Rudd, and
B. Weil. MIPS: a database for genomes and protein sequences.
Nucleic Acid Research, 30(1):31–34, 2002.

[Moh91]

[Moh97]

B. Mohar. The laplacian spectrum of graphs. In Y. Alavi, G. Char-
trand, O. Ollermann, and A. Schwenk, editors, Graph theory, com-
binatorics, and applications, pages 871–898, New-York, 1991. John
Wiley and Sons, Inc.

B. Mohar. Some applications of laplace eigenvalues of graphs. In
G. Hahn and G. Sabidussi, editors, Graph Symmetry: Algebraic
Methods and Applications, volume 497 of NATO ASI Series C,
pages 227–275. Kluwer, Dordrecht, 1997.

[MPT+99] Edward M. Marcotte, Matteo Pellegrini, Michael J. Thompson,
Todd O. Yeates, and David Eisenberg. A combined algorithm for
genome-wide prediction of protein function. Nature, 402:83–86,
November 1999.

[MRW+99] S. Mika, G. R¨atsch, J. Weston, B. Sch¨olkopf, and K.R. M¨uller.
Fisher discriminant analysis with kernels. In Y.-H. Hu, J. Larsen,
E. Wilson, and S. Douglas, editors, Neural Networks for Signal
Processing IX, pages 41–48. IEEE, 1999.

[NGK01]

[ODB00]

A. Nakaya, S. Goto, and M. Kanehisa. Extraction of correlated
gene clusters by multiple graph comparison. In Genome Informatics
2001, pages 44–53. Universal Academy Press, Tokyo, Japan, 2001.

Nobuo Ogawa, Joseph DeRisi, and Patrick O. Brown. New compo-
nents of a system for phosphate accumulation and polyphosphate
metabolism in saccharomyces cerevisiae revealed by genomic ex-
pression analysis. Mol. Biol. Cell, 11:4309–4321, Dec 2000.

[OLP+00] Ross Overbeek, Niels Larsen, Gordon D. Pusch, Mark D’Souza, Ev-
geni Selkov Jr, Nikos Kyrpides, Michael Fonstein, Natalia Maltsev,
and Evgeni Selkov. WIT: integrated system for high-throughput
genome sequence analysis and metabolic reconstruction. Nucleic
Acid Research, 28:123–125, 2000.

29

[PWCG01] Paul Pavlidis, Jason Weston, Jinsong Cai, and William Noble
Grundy. Gene functional classiﬁcation from heterogeneous data. In
Proceedings of the Fifth Annual International Conference on Com-
putational Biology, pages 249–255, 2001.

[Sai88]

S. Saitoh. Theory of reproducing Kernels and its applications. Long-
man Scientiﬁc & Technical, Harlow, UK, 1988.

[SHBK+01] G. Sherlock, T. Hernandez-Boussard, A. Kasarskis, G. Binkley,
J.C. Matese, S.S. Dwight, M. Kaloper, S. Weng, H. Jin, C.A. Ball,
M.B. Eisen, and P.T. Spellman. The stanford microarray database.
Nucleic Acid Research, 29(1):152–155, Jan 2001.

[SS02]

Bernhard Sch¨olkopf and Alexander J. Smola. Learning with Ker-
nels: Support Vector Machines, Regularization, Optimization, and
Beyond. MIT Press, Cambridge, MA, 2002.

[SSDB95] M. Schena, D. Shalon, R.W. Davis, and P.O. Brown. Quantitative
monitoring of gene expression patterns with a complimentary DNA
microarray. Science, 270:467–470, 1995.

[SSM98]

[SSM99]

[SSZ+98]

[Sta96]

[TA77]

A.J. Smola, B. Sch¨olkopf, and K.-R. M¨uller. The connection be-
tween regularization operators and support vector kernels. Neural
Networks, 11(4):637–649, 1998.

Bernhard Sch¨olkopf, Alexander J. Smola, and Klaus-Robert M¨uller.
Kernel principal component analysis. In B. Sch¨olkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Methods - Support Vec-
tor Learning, pages 327–352. MIT Press, 1999.

Paul T. Spellman, Gavin Sherlock, Michael Q. Zhang, Vish-
wanath R. Iyer, Kirk Anders, Michael B. Eisen, Patrick O. Brown,
David Botstein, and Bruce Futcher. Comprehensive identiﬁcation
of cell cycle-regulated genes of the yeast saccharomyces cerevisiae
by microarray hybridization. Mol. Biol. Cell, 9:3273–3297, 1998.

Peter F. Stadler. Landscapes and their correlation functions. J.
Math. Chem., 20:1–45, 1996.

A.N. Tikhonov and V.Y. Arsenin. Solutions of ill-posed problems.
W.H. Winston, Washington, D.C., 1977.

[TCS+01] Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,
Trevor Hastie, Robert Tibshirani, David Botstein, and Russ B.
Altman. Missing value estimation methods for NA microarrays.
Bioinformatics, 17:520–525, 2001.

[Vap98]

Vladimir N. Vapnik. Statistical Learning Theory. Wiley, New-York,
1998.

30

[vdH96]

[Ver02a]

[Ver02b]

[Wah90]

[Wat00]

H. van der Holst. Topological and spectral graph characterizations.
PhD thesis, Universiteit van Amsterdam, 1996.

Jean-Philippe Vert. Support vector machine prediction of sig-
nal peptide cleavage site using a new class of kernels for strings.
In Russ B. Altman, A. Keith Dunker, Lawrence Hunter, Kevin
Lauerdale, and Teri E. Klein, editors, Proceedings of the Paciﬁc
Symposium on Biocomputing 2002, pages 649–660. World Scien-
tiﬁc, 2002.

Jean-Phlippe Vert. A tree kernel to analyze phylogenetic proﬁles.
Bioinformatics, 2002. To appear.

G. Wahba. Spline Models for Observational Data, volume 59 of
CBMS-NSF Regional Conference Series in Applied Mathematics.
SIAM, Philadelphia, 1990.

C. Watkins. Dynamic alignment kernels.
In A.J. Smola, P.L.
Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors, Advances in
Large Margin Classiﬁers, pages 39–50. MIT Press, Cambridge, MA,
2000.

[ZSV+00] Gefeng Zhu, Paul T. Spellman, Tom Volpe, Patrick O. Brown,
David Botstein, Trisha N. Davis, and Bruce Futcher. Two yeast
forkhead genes regulate the cell cycle and pseudohyphal growth.
Nature, 406:90–94, 2000.

31


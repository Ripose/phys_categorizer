A uniﬁed approach to the study of temporal,
correlational and rate coding

Stefano Panzeri¶ and Simon R. Schultz§
¶ Neural Systems Group, University of Newcastle upon Tyne
Department of Psychology, Ridley Building
Newcastle upon Tyne, NE1 7RU, U.K.
§ Howard Hughes Medical Institute and
Center for Neural Science, New York University
4 Washington Place, New York, NY 10003, U.S.A.

September 25, 2013

9
9
9
1
 
g
u
A
 
3
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
7
2
0
8
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

Abstract

We demonstrate that the complete (MacKay and McCulloch, 1952) information
contained in the spike times of a population of neurons can be broken up into a series
of terms which have physiological signiﬁcance. This occurs in the coding r´egime in
which few spikes are emitted in the relevant time window. This approach allows us to
study the additional information contributed by spike timing beyond that present in
the spike counts, to examine the contributions to the whole information of diﬀerent
statistical properties of spike trains, such as ﬁring rates and correlation functions,
and forms the basis for a new quantitative procedure for the analysis of simultane-
ous multiple neuron recordings. It also provides theoretical constraints upon neural
coding. We ﬁnd that there is a transition between two coding r´egimes depending
on the size of the relevant observation timescale with respect to the characteristic
timescale of stimulus-induced response ﬂuctuations. For time windows shorter than
the timescale of the stimulus-induced ﬂuctuations, the additional purely temporal
information is of third order, and hence spike counts necessarily dominate the popu-
lation information. For time-windows much longer than the characteristic timescale,
the additional timing information can be of ﬁrst order, and thus aﬀect the instan-
taneous information rate. It is also found that, over short timescales, correlations
between spikes cannot dominate the information representation, and the temporal
structure of correlations contributes less to temporal coding than the temporal vari-
ations of the ﬁring rates. Stimulus-independent cross-correlations in the response
variability can contribute positively to population coding only when they are op-
posite in sign to cross-correlations in the mean response proﬁles of diﬀerent cells,
and this eﬀect is maximal when rates and cross-correlations oscillate with similar
frequencies. We also ﬁnd that refractoriness and other timing correlation eﬀects can
have a beneﬁcial, but second-order, eﬀect on coding.

2

1 Introduction

Neurons respond to sensory stimulation by producing sequences of essentially identical
action potentials, or spikes; information about such stimulation is thus, at the most fun-
damental level, represented in the central nervous system by the spike emission times of
populations of these neurons. In principle, the temporal pattern of spikes across the neu-
ronal population provides a large capacity for fast information transmission (MacKay and
McCulloch, 1952). However, it is still unclear how much of this theoretical capacity is
actually exploited by the brain. It has been known since the early days of neurophysiology
(Adrian, 1926) that much sensory information is conveyed by stimulus modulated varia-
tions of the mean discharge rate of individual neurons (a “ﬁring rate” code). In addition,
the typically large variability of interspike intervals (ISI) of cortical neurons in some cir-
cumstances might suggest to some that variations in spike timing represent mainly noise
to be averaged away (Shadlen and Newsome, 1998). On the other hand, in some cir-
cumstances if the stimulus is modulated on a very short time scale, precisely replicable
sequences of spikes can be obtained (Bair and Koch, 1996; Buracas et al., 1998). Does
this represent temporal coding, or is the relevant time window for counting spikes merely
very short? Recent experiments have also suggested that temporally coded information is
present in the spike trains of primate V1 complex cells (Victor and Purpura, 1998; Mech-
ler et al., 1998). Evidence has accrued that some information appears to be encoded by
stimulus (or behaviour) related changes in the coordination of timing of ﬁring between
small populations of cortical cells (Vaadia et al., 1995; deCharms and Merzenich, 1996;
Riehle et al., 1997; see Bair, 1999 for a more comprehensive review of recent spike timing
literature). The issue is clearly deserving of in-depth quantitative study.

A rigorous information theoretic measure (Shannon, 1948; Cover and Thomas, 1991)
can be used to clarify fully the modalities of neuronal information encoding. Such an
approach would gain much analytical advantage by allowing the total information about
a set of stimulus parameters, contained in the population spike trains, to be explicitly
separated into distinct components. Each such component should be indicative of the en-
coding mechanisms involved. In other words, this approach should provide a measure of
how many bits of information were present when ignoring temporal details of the responses
and considering spike counts only, and how many bits were instead solely due to temporal
structure of the spike train. It should also quantify which particular features of the tem-
poral pattern of activity carry more information, e.g., by separating out the information
carried by temporal modulation of the instantaneous spiking probability from the informa-
tion contained in the temporal correlations between spikes – etc. The use of the limit of
rapid information representation, in which only a few spikes are emitted in the considered
time period, makes it possible to perform this analysis.

This short time scale limit is not only a convenient approach to a complex problem,
but it is relevant to transmission of sensory information processing by the activity of nerve
cells. Single unit recording studies in primates have demonstrated that the majority of
information about static visual stimuli is often transmitted in windows as short as 20-
50ms (Tov´ee et al., 1993; Heller et al., 1995; Rolls et al., 1999). Information about time-

3

dependent signals is often conveyed by single sensory cells by producing about one spike
per characteristic time of stimulus variation (Rieke et al., 1996). At the population level,
processing can be even faster if diﬀerent cells carry independent information (Rolls et al.,
1997). Event related potential studies of the human visual system (Thorpe et al., 1996)
provide further evidence that the processing of information in a multiple stage neural
system can be extremely rapid. Finally, the assessment of the information content of
interacting assemblies which may last for only a few tens of milliseconds (Singer et al., 1997)
requires the use of such short windows.

In this article, by extending previous work valid only for the spike count case (Panzeri
et al., 1999), we use the short time window limit to produce an analytical expansion (to
second order in time) of the mutual information carried by the spike times of a neuronal
population about a set of (possibly time-varying) stimuli. This procedure naturally sepa-
rates out the contributions of both instantaneous ﬁring rates and temporal correlations be-
tween spikes to the total information conveyed by the simultaneously recorded spike trains.
It also extracts from the spike train the statistical quantities that bear more information.
Therefore it compresses the spike train in a way eﬃcient for information estimation. This
makes it practically applicable to data-sets and time windows relevant to many neurophys-
iological experiments. Also, it allows an homogeneous comparison with the information
carried just by the number of spikes ﬁred by each cell (Panzeri et al., 1999). It is thus
relevant in addressing the problem of the extra amount of information purely encoded in
the spike times of a whole population, a problem which has been addressed systematically
so far only for single units (Optican and Richmond, 1987; Tov´ee et al., 1993; Victor and
Purpura, 1998).

This expansion of the mutual information is not only a method of practical utility for
analysis of simultaneously recorded spike trains, but it also provides theoretical insights
into the information coding capabilities of spike trains. It shows that, in short but physi-
ological time scales, temporal encoding contributes signiﬁcantly to the rate of information
transmission when the instantaneous ﬁring rates of the cells ﬂuctuate with a time scale
If the
shorter than the time window in which most of the information is transmitted.
instantaneous probabilities of spike emission do not ﬂuctuate quickly compared to the
time window considered, the additional information present in the temporal structure of
the signal is minimal. It is also shown that in general over short time scales correlations
between spikes cannot dominate information representation, and the temporal structure
of correlations between spikes contributes less to temporal encoding than variations in
the instantaneous ﬁring rates of individual cells. We also derive theoretical constraints
on synergistic coding, by studying which combinations of relative phase and timescales
of variations for rates and correlations are capable to add substantial information. The
role of response parameters of biophysical relevance, such as the refractory period, is also
considered.

2 Basic concepts and deﬁnitions

4

2.1 The information carried by neuronal spike trains and its

short time scale expansion.

i }, ta

Consider a period of time centered at t0 and of (short) duration T , associated with a
particular presentation of a sensory stimulus. The stimulus may be either dynamic or
static in nature.
In the former case, the stimulus characteristics vary within the time
window T ; the latter situation is merely a special case of the former. Let us denote each
diﬀerent stimulus history (time course of characteristics within T ) as s(τ ), a function
of time from stimulus onset chosen from the set S of experimentally presented stimulus
histories1. The neuronal population response to the stimulus is described by the collection
of spike arrival times {ta
i being the i-th spike emitted by the a-th neuron. Although
the spike arrival time is a continuous variable, it can be experimentally measured only with
a ﬁnite precision ∆t. Therefore an experimental measure of a spike sequence is obtained
by dividing the time window T into small time bins of size ∆t (e.g. 1 ms long), in which
only up to one spike per cell can be observed. A spike sequence {ta
i } is thus represented
as a sequence of binary numbers, one for each time bin and cell, with the response “1” in
the bins corresponding to the spike times. The probability of observing a spike sequence
{ta
i }|s(τ )), and
P ({ta
i }|s(τ )) must be determined by
repeating each of the stimulus histories in the set exactly the same way on multiple trials2.
Following Shannon (1948), we can write down the mutual information provided by the
simultaneous spike trains about the whole set of stimulus histories as

i } when a particular stimulus history s(τ ) was present is denoted as P ({ta

i }) is its average across all stimulus histories. P ({ta

I({ta

i }; S) =

Ds(τ )

P [{ta

i }|s(τ )] log2

Z

Xta

i

P [{ta

i }|s(τ )]
i })

P ({ta

.

(1)

ta
i

P

is shorthand for summation over all possible spike arrival times {ta

i } in the
where
interval t0−T /2 < t < t0+T /2, and over all possible total spike counts from the population.
In this equation, the functional integration notation Ds(τ ) reﬂects the fact that in principle,
a continuous set of stimulus histories can be created. In practice, only a discrete, often very
limited, number of stimulus histories can be used to test the cell. Therefore, for brevity,
s∈S P (s). However, it should
the τ is dropped henceforth, and we replace
be remembered that in general, s refers to a stimulus history rather than a static stimulus.
R
i }) are available. In practical
application to experimental data, however, we have only a small to moderate number of
events from which to compute the frequency table, and as a result a bias is introduced
which must be corrected for (Panzeri and Treves, 1996). As explained above, the procedure
introduced here suﬀers much less from ﬁnite sampling problems than a direct estimation
of the information from the full response probabilities. A following paper will report the
1An example of this set might be a sinusoidal grating moving with constant speed along one of a number

P
So far it has been assumed that the true probabilities P ({ta

Ds(τ ) with

of possible directions.

2Following a standard notation, we indicate probabilities with P (·) and probability densities with p(·). A
probability density p(·) has the physical meaning of probability per unit time, and it becomes a probability
when multiplied by the length dt of the small time bin.

5

full details on this issue; however, the approach described in the Appendix B of Panzeri
et al. (1999) can be taken and applied straightforwardly.

We assume that, when using a discrete time resolution, the size ∆t of the small bins
is comparable to (though smaller than) the size of the total time window T ; of course if
∆t is much smaller than T , then the continuous approximation is valid. In this case the
summation over time bins in (1) can in fact be replaced with integral over spike arrival
times:

I({ta

i }; S) =

P (s)

Dta

i p({ta

i }|s) log2

(2)

p({ta
p({ta

i }|s)
i })

.

Xs∈S

Z

As all the information carried by neurons is contained in the spike arrival times, eq. (2)
quantiﬁes the maximal amount of information that can be extracted (or decoded) from
a set of neurons in response to a particular set of stimuli. The precision of the time
measurement of course limits the experimental estimate of entropy and information that
are carried by the spike trains of the population; taking the high time resolution limit
(∆t ≪ T ) is equivalent to characterising the properties of the system, as opposed to our
observation power (Strong et al., 1998).

Instead of considering the information contained in the spike sequence, one can focus
only on the number of spikes emitted by each cell. A quantiﬁcation of the neuronal pop-
ulation response as a vector n (each component being the number of spikes ﬁred by each
neuron in the window considered) implicitly assumes that temporal relationships between
diﬀerent spikes are irrelevant in coding the stimulus, and that only the mean response
strength of each cell matters. The information contained in the spike count has the follow-
ing expression:

I(n; S) =

P (s)P (n|s) log2

Xs∈S

n
X

P (n|s)
P (n)

.

(3)

(P (n|s) being the spike count probability for given stimulus s, and P (n) being its aver-
age across stimuli). A comparison of the information in the full spike trains (2) to the
information in the spike count (3) leads to a direct understanding of the importance to
information coding of the temporal relationships between individual spikes in a sequence.
One of the main advances of this paper is to provide an analytical framework to test the
role of temporal correlations between spikes in encoding stimulus information.

Now, the information in the spike trains can be approximated by a power series

I({ta

i }; S) = It({ta

i }; S) T + Itt({ta

i }; S)

+ . . .

(4)

T 2
2

where It(·) refers to the instantaneous information rate and Itt(·) to the second time deriva-
tive of the information, the instantaneous information “acceleration”. (A similar expansion
is also performed for the information in the spike counts, as in Panzeri et al. (1999)). For
short time scales, only the ﬁrst order and second order terms survive - higher order terms
in the series become negligible. The time derivatives of the information can be calculated
by taking advantage of the narrowness of the time windows, as will be explained.

6

2.2 Deﬁnitions of correlation functions for short time scales

There are two kinds of correlations that inﬂuence the information. These have been pre-
viously termed “signal” and “noise” correlations (Gawne and Richmond, 1993). They can
be distinguished by separating the responses into “signal” (the average response to each
stimulus as a function of time) and “noise” (the variability of responses from the average
to each stimulus). The correlations in the response variability represent the tendency of
the cells to ﬁre more (or less) than average when a particular event (e.g. a spike from
another neuron) is observed with a given time delay. Note that we do not assume that the
trial by trial variability is just due to noise, and our analysis is independent of the cause of
this variability, which could arise from a number of factors. We name the correlations in
the variability as “noise correlations” only for consistency with previous literature (Gawne
and Richmond, 1993; Gawne et al., 1996). In fact, our results reported below precisely
quantify the contribution of correlations in the trial by trial variability to the information
carried by the neuronal population, and show in which cases correlations in this variability
contribute positively to information transmission.

Let us focus ﬁrst on the discrete time resolution case, which is the one relevant experi-
mentally. In this case, for each single trial to stimulus s, the spike train (density) of each
cell a (among C cells in total) can be represented as a sum of pulses at the spike time bins:

where δt1,t2 is the Kroenecker delta function (it is 1 if t1 and t2 label the same time bin,
and zero otherwise). The time dependent ﬁring rate of each cell a is

ρa(t; s) =

i

δt,ta
∆t

Xi

ra(t; s) = ρa(t; s)

(5)

(6)

(the bar denotes the average over multiple trials in which the same stimulus history s was
presented). These spike rates quantify the time-dependent probability per unit time of a
single spike from a certain cell, and are also called post-stimulus time histograms (PSTH).
j; s) quantifying noise correlation in the
short time scale limit is in terms of the conditional ﬁring probabilities of observing one
spike from cell a in the time bin centered at ta
i , given that cell b emitted a spike in the time
bin centered at tb

One way to introduce the parameters γab(ta

j when stimulus s was presented:

i , tb

P (ta

i |tb

j ; s) ≡ ra(ta

i ; s) ∆t(1 + γab(ta

i , tb

j; s)) + O(∆t2).

(7)

It has been assumed in the above that the conditional probabilities (7) scale propor-
tionally to ∆t; this is the only assumption underlying our time expansion. It is a natural
assumption, as it merely implies that the probability of observing a spike in a time bin
is proportional to the resolution ∆t of the measurement. It can be violated only in the
implausible case of spikes locked to one another with inﬁnite time precision, but in any
case it can be veriﬁed for any given dataset.

7

The function γab(ta

i , tb
of an autocorrelation function for a = b (γaa(ta
be observed in the short time bin). γ can be measured from the data as follows:

j; s) has the meaning of a cross-correlation function for a 6= b, and
i ; s) ≡ −1, as no more than one spike can

i , ta

γab(ta

i , tb

j; s) =

γaa(ta

i , ta

i ; s) = −1

ρa(ta
(ρa(ta

i ; s)ρb(tb
i ; s)ρb(tb

j; s)
j; s))

− 1, if a 6= b or ta

i 6= tb
j

(8)

and is named the “scaled cross correlation density” (Aertsen et al., 1989). It is a joint
post-stimulus time histogram (JPSTH) from which the amount of coincidences purely
due to rate modulation with time has been subtracted (implemented by the “- 1” in eq.
It is
(8), which is equivalent to the shift predictor subtraction (Aertsen et al., 1989)).
normalised to the number of coincidences expected in the uncorrelated case, and it can
vary from −1 to ∞; negative γab(s)’s indicate anticorrelation, whereas positive γab(s)’s
indicate correlation. This normalisation of the JPSTH is used because it has a number of
advantages for short time windows over the more well-known Pearson JPSTH normalization
for this application, as described in Panzeri et al. (1999); it should be noted that of course
the particular normalised correlation measure that the equations are expressed in terms of
is just a notational issue. For any given time window, the scaled correlation coeﬃcients
can be converted to Pearson coeﬃcients (see Panzeri et al., 1999). Since we are considering
a single short time window, note that these correlations are observed over repeated trials
with the same stimulus.

In order to maintain homogeneity with respect to the noise correlation case, we choose to
j) in the mean responses

quantify the correlations in the signal, i.e. the correlations νab(ta
of the neurons across the set of stimuli, as a signal scaled crosscorrelation coeﬃcient:

i , tb

νab(ta

i , tb

j) =

< ra(ta

i ; s)rb(tb
i ; s) >s< rb(tb

< ra(ta

j; s) >s

j; s) >s

− 1

The deﬁnition is similar to that of scaled noise correlation; the main diﬀerence is that
s P (s)(· · ·), not across trials. As before,
now the average is across stimuli: < (· · ·) >s≡
νab(ta

i , tb
When passing to the high temporal resolution limit, essentially the same procedures
and deﬁntions outlined above apply, provided that one uses Dirac delta function δ(t1 − t2)
instead of Kroenecker functions in the deﬁnition of spike density:

j) varies between −1 and ∞.

P

ρa(t; s) =

δ(t − ta
i )

Xi
and that one uses probability densities instead of the corresponding conditional probabili-
ties in eq. (7):

p(ta

i |tb

j ; s)dt ≡ ra(ta

i ; s) dt(1 + γab(ta

i , tb

j; s)) + O(dt2).

The correlational parameters that inﬂuence the spike count information are the scaled
correlation coeﬃcients of the spike counts in each trial (Panzeri et al., 1999), and are

(9)

(10)

(11)

8

obtained by an appropriate sum over time bins (or an integration in the high resolution
limit) of the above expressions. The trial-by-trial “noise” spike-count scaled correlation
coeﬃcient is:

γab(s) =

dta
i

R

R

dtb
(

jra(ta
dta

i ra(ta

i ; s)rb(tb
i ; s))(
R

R

j; s)(1 + γab(ta
jrb(tb
j; s))

dtb

i , tb

j, s))

− 1,

and the “signal” correlation in the mean spike count response proﬁles is quantiﬁed as:

νab =

<

i ra(ta

dta
i ra(ta

i ; s)
i ; s) >s<
R

dta
R

dtb

jrb(tb
dtb

<

j; s) >s

jrb(tb

j; s) >s

− 1

R

R

The above expressions (12,13) represent the generalization to the time-varying case of
the correlation coeﬃcient introduced in Panzeri et al. (1999). Of course their value does
not depend on the time resolution used, but only upon the total number of spikes emitted
in each trial. The expression for (12,13) in the ﬁnite resolution case is of course obtained
replacing integrals with sums as follows:

(12)

(13)

(14)

lim
∆t→0

n
X

f (tn)∆t =

dtf (t)

Z

2.3 Response probability quantiﬁcation.

If the instantaneous ﬁring rates conditional upon the ﬁring of other neurons are non-
divergent, as assumed in eq. (7), the short time scale expansion of response probabilities
becomes essentially an expansion in the total number of spikes emitted by the population in
response to a stimulus. The only responses which contribute to the transmitted information
up to order k are the responses with up to k spikes from the population; the only relevant
events for the second order analysis are therefore those with no more than two spikes
emitted in total. This is valid for any time-resolution. For brevity, we report the results
only for the inﬁnite time resolution case:

+

dta
1

dtb

2ra(ta

1; s)rb(tb

2; s)(1 + γab(ta

1, tb

2; s))

P (0|s) = 1 −

dta

1ra(ta

1; s) +

C

Xa=1 Z
C

C

p(ta

1|s)dta

Xa=1
1 = ra(ta

Xb=1 Z
1; s)dta
1

Z

C

1 −

 

1
2

1
2

p(ta

1tb

2|s)dta

1dtb

2 =

ra(ta

1; s)rb(tb

1, tb

2; s))dta

1dtb
2

a, b = 1, · · · , C;

(15)

Xb=1 Z
2; s)(1 + γab(ta

dtb

2rb(tb

2; s)(1 + γab(ta

1, tb

2; s))

a = 1, · · · , C

!

where P (0|s) is the probability of zero response (no cells ﬁre), p(ta
density of observing just one spike from cell a at the speciﬁed time location, and p(ta

1|s) is the probability
2|s)

1tb

9

is the probability density of observing a pair of spikes at the given times3. The expression
(15) for the probabilities can be derived by requiring that the probabilities are normalized
to one, that the conditional ﬁring probabilities scale proportionally to the time resolution,
as in eq. (11), and by using the deﬁnitions of time dependent rate and correlations given
above. The probabilities for the ﬁnite time resolution case follow straightforwardly from
(15) by replacing probability densities of spike emissions with probabilities on l.h.s, and
replacing dt with ∆t and integrals with sums, as in (14), on the r.h.s.

3 Analytical results for the information in spike times

and the role of temporal correlations

In this section we report the result of the insertion of the second order response probabil-
ities into the information contained both in the spike times (2) and the spike counts (3).
This yields exact expressions quantifying the impact of pairwise temporal correlations on
the information transmitted by groups of spiking neurons. A comparison between spike
timing and spike count information precisely quantiﬁes the amount of information that
is conveyed purely by the temporal relationships between the spikes. These information
quantities depend upon the time-dependent ﬁring rates r(·), noise correlations γ(·) and
signal correlations ν(·). Interactions among more than two cells (or spikes) do not aﬀect
the information up to second order.

For brevity we report only the ﬁnal results for the calculation of the information deriva-
tives. The sketch of their calculation in terms of the probabilities (15) is as follows. First,
one inserts the probabilities (15) into the sum over responses in eq.(2). Then, for each term
of the sum over responses, one uses the power expansion of the logarithm as a function of
t: log2(1 − t x) = − 1
. Finally, using eq. (4), integrating over spike times, and
ln2
grouping together all the terms in the sum which have the same power, one obtains the
expressions for the information derivatives reported below.

(tx)j
j

∞
j=1

P

3.1

Information in the spike times of the neuronal ensemble in
short time scales

In the short time scale limit, the ﬁrst (It) and second (Itt) information derivatives suﬃce
to fully describe the information kinetics. The ﬁrst-order information contained in the
spike times contribution (i.e. that obtained by multiplying the instantaneous information
transmission rate It(·) by the time window duration T ) is (Bialek et al., 1991):

It({ta

i }; S) T =

dta

ra(ta; s) log2

C

Xa=1 Z

*

ra(ta; s)
hra(ta; s′)is′ +s

(16)

3The probability of two spikes is divided by 2 to prevent over-counting due to equivalent permutations,
rather than restrict the sum over events to non-equivalent permutations, as we did in Panzeri et al. (1999)

10

This is the ﬁrst component of the mutual information, i.e. the rate of the population infor-
mation transmission. It is simply a sum of single cells contributions, and it is insensitive
to correlation both in the signal and in the noise. The expression for the second order
contribution (i.e. the second temporal derivative multiplied by T 2/2) breaks up into three
terms:

Itt({ta

i }; S)

T 2
2

=

1
2 ln 2

C

C

dta
1

dtb

2 hra(ta

1; s)is

rb(tb

2; s)

Xa=1
1, tb
νab(ta

Xb=1 Z
Z
2) + (1 + νab(ta

1, tb

2)) ln(

D

1

s
E

1 + νab(ta

1, tb
2)

)

(cid:21)

+

+

×
(cid:20)

1
2

1
2

C

C

Xa=1
C

Xb=1 Z
C

Xa=1

Xb=1 Z

× log2

D

dta
1

dtb
2

ra(ta

1; s)rb(tb

2; s)γab(ta

1, tb

2; s)

log2(

1

1 + νab(ta

1, tb
2)

)

s
E

(cid:21)

dta
1

dtb
2

P (s)ra(ta

1; s)rb(tb

2; s)(1 + γab(ta

1, tb

2; s))

(cid:20)D

Z

Z

(1 + γab(ta

1, tb
1; s′)rb(tb

ra(ta

(cid:20)

s
X
ra(ta
2; s))
D
2; s′)(1 + γab(ta

1; s′)rb(tb
1, tb

2; s′)
E
2; s′))

s′

.

s′ (cid:21)
E

(17)

1, tb

We will refer to these terms as components 2a, 2b and 2c of the information respectively.
When each spike is completely independent, as in a Poisson process, it is apparent
that only the ﬁrst term of Eq. 17 survives. This ﬁrst term is always negative, meaning
that the information carried by a Poisson process always slows down after the ﬁrst spike.
If there is any deviation from independence in the timing of successive spikes, then the
other terms can contribute to the information through non-zero autocorrelation density
γaa(ta
2). If there is any relationship between the times of spike emission of diﬀerent cells,
then they can contribute through non-zero cross-correlation γab(ta
2). As with the spike
count information terms detailed in (Panzeri et al., 1999), the second of these terms (the
“stimulus-independent correlational component”) reﬂects contributions from a level of cor-
relation which is not stimulus dependent. The third term is non-zero only in the presence of
stimulus dependence of the correlation between spikes, and is called “stimulus-dependent
correlational component”. The natural separation of the second order information into
three components is important because each component reﬂects the contribution of a dif-
ferent relevant encoding mechanism. When applying this analysis to real neuronal data, a
signiﬁcant amount of information found in the third component of Itt, relative to the total
information, would clearly signal that cells are transmitting information mainly by partic-
ipating in a stimulus (or context) dependent correlational assembly (Singer et al., 1997).
A speciﬁc example will be given in Fig. 4.

1, tb

3.2

Information in the spike count of the neuronal ensemble in
short time scales

In the same way, the short time scale expansion can be carried out for the spike count
information. This was performed in detail in Panzeri et al. (1999), and here we report

11

brieﬂy the main results in order to cast them into identical notation to that above for
comparison. The spike count information derivatives are similar to (17), but they depend
only on the mean rate across time and on the spike count correlation coeﬃcients (12,13).
The ﬁrst order contribution is:

It(n; S) T =

dtara(ta; s) log2

C

Xa=1 *Z

dtara(ta; s)
dtara(ta; s′)is′ +s
R

h

(18)

As before, the second order contribution is broken down into three components:

R

Itt(n; S)

T 2
2

=

×

+

+

C

C

1
2 ln 2

Xa=1

Xb=1 (cid:28)Z

νab + (1 + νab) ln(
(cid:20)
1
2

dta

C

C

Xa=1
C

Xb=1(cid:20)(cid:28)(cid:18)Z
C

1
2

dta

1ra(ta

1; s)

dtb

2rb(tb

2; s)

(cid:29)s (cid:28)Z

(cid:29)s

1
1 + νab

)

(cid:21)

1ra(ta

1; s)

dtb

2rb(tb

2; s)

γab(s)

log2(

(cid:19) (cid:18)Z

(cid:19)

(cid:29)s(cid:21)

1
1 + νab

)

P (s)

dta

1ra(ta

1; s)

dtb

2rb(tb

2; s)

(1 + γab(s))

Xa=1

s
Xb=1 X

(cid:18)Z
(1 + γab(s))
1ra(ta

dta

DR
1; s′))

(cid:19) (cid:18)Z

(cid:19)

dta

1; s′)

1ra(ta
dtb

2rb(tb

2; s′)
R

2; s′)
dtb
2rb(tb
s′
E
(1 + γab(s′))

× log2

(cid:20)

(
D

R

(cid:16)R

(cid:17)

.
s′ (cid:21)
E

(19)

The information in the spike count depend only on the total number of spike per cell
on each trials, and not on the spike times. Therefore the information in the spike count,
and all the quantities upon which it depends, are independent of the time resolution ∆t.
For simplicity the results are given just for the high time resolution limit.

3.3 Range of validity of the analytical results

The short time scale limit utilized here formally requires that the mean number of spikes
in the considered window is small. However, the actual range of validity of the order T 2
approximation will depend on how well the information time dependence of the responses
of a neuronal population ﬁt a quadratic approximation. Panzeri et al. (1999) showed,
by means of Integrate and Fire computer simulations of neurons with ﬁring rates and
correlations in the typical cortical range, that the second order approximation is a good
description of the full information for up to several hundred millisecond time windows
for single cells and pairs. Scaling considerations (conﬁrmed by simulation) suggested that
the range of applicability for the spike count information should shrink as the inverse
of the number of cells for larger populations. The same range of validity must hold for
the temporal information as well when the ﬁring rates and correlations of the neurons
vary slowly with respect to the time window considered, since in this limit the additional
temporal information is small (see next section). The additional numerical test required

12

here is therefore on the range of applicability of the second order approximation for the full
spike timing information when the parameters describing the neuronal ﬁring probabilities
ﬂuctuate quite rapidly within the time window T .

For this purpose we simulated neurons driven by two stimuli. The response of each
neuron to each stimulus is modeled as a 1 ms resolution Poisson process with a time
dependent ﬁring rate4. The analytical approximations to the information are tested against
the true information computed directly from the model response probabilities. The ﬁrst
stimulus gives a constant (ﬂat) mean response r0 = 30 spikes/sec. across time, whereas
the second stimulus produces a response centered around r0 and sinusoidally modulated
at a frequency5 1/τc in some cases diﬀerent for each cell c and amplitude r0. After a
full period, the two stimuli are indistinguishable on the basis of spike count alone, and
therefore this is a good model to study temporal coding. Fig. 1 shows the accuracy of the
ﬁrst and second order approximation to the information in the spike times for a single cell
with responses to the second stimulus oscillating very fast (at a frequency of 500 Hz). The
second order approximation is nearly exact up to 200 ms, even in the extreme case of such
fast ﬂuctuations. The range of validity seems not to be signiﬁcantly aﬀected by changing
the frequency of oscillations. We have also carried out simulations with a set of two cells
with either Poisson or cross-correlated ﬁring. We found that the scaling expectation that
the range of validity shrinks as 1/C with population size is conﬁrmed up to the time
window lengths that we could test (not shown)6. It can be concluded that the analysis
is pertinent for time scales relevant to neuronal coding, and the small ensembles of cells
typically recorded during in vivo experimental sessions.

4 The role of time scales of ﬁring rate and correlation
variations in pure temporal encoding of information

Our formalism quantiﬁes the information I({ta
i }; S) in the spike times and I(n; S) in spike
rates as a function of the ﬁring rates and of the correlation functions. Since both infor-
mation measures are available, we can address the further problem of when information
transmission is not dominated by spike counts. In other words, we are interested in how
much extra information, not contained in the simple spike count, is conveyed by the tem-
poral relations between the spikes. This extra information is precisely quantiﬁed by the
diﬀerence between I({ta

i }; S) and I(n; S).

We ﬁnd that the crucial parameter, determining when there is substantial additional
information in the spike times which is not contained in the spike counts only, is the typical
4The response in each 1 ms time bin is generated with a time dependent ﬁring rate independently of

the response in other time bins.

across the neuron’s receptive ﬁeld.

5This might e.g. be produced by a grating with a spatial wavelength to velocity ratio of τc moving

6When simulating two cells, we could not test systematically time windows as long as 200 ms because the
number of spike trains over which we have to sum to compute the full information increases exponentially
both with the length of the window and with the population size

13

time scale of variation of the ﬁring rate and correlation functions. There are two distinct
coding r´egimes, depending on whether the characteristic time scale of stimulus-induced
variations is small or large with respect to time window T of interest.

4.1 The quasi-static case (temporal encoding is weakly relevant)

If the scale of time variation of any time dependent function f (t) is larger than the time
window considered, then f (t) can be approximated by its power expansion and the following
quasi-static approximation holds:

t0+T /2

t0−T /2

Z

f (t)dt = f (t0) T +

∂2
∂t2 f (t)|t=t0

T 3
24

+ O(T 5)

(20)

As a consequence, if both rate and correlation functions vary slowly in the time domain
of interest, only the rate and correlation values near the centre of the interval (t0) are
important, and the extra amount of information in the spike times is sub-leading (only
of order T 3). The expression for the T 3 leading term to the extra information in the
spike timing can be computed by applying eq. (20) to the integrals over time involved in
computing information rates (16,18). The result depends only on the ﬁring rate variations
near t0, and not on correlations between the spikes7:

i }; S) − I(n; S) =

I({ta
T 3
24 ln 2

s
X

P (s)

r(t; s)|t=t0

∂
∂t

(∂/∂t)r(t; s)|t=t0
r(t0; s)

 

−

< (∂/∂t)r(t; s′)|t=t0 >s′
< r(t0; s′) >s′

!

. (21)

Being of order T 3, pure temporal coding does not aﬀect, in this r´egime, both the
rate and the acceleration of information transmission. Pure temporal information would
accumulate very slowly with time. Therefore information is dominated by the spike counts
in this r´egime.

4.2 Fast parameter variation (temporal encoding is very rele-

vant)

If either ﬁring rates of correlation functions ﬂuctuate with time with a characteristic time
scale which is of the order of the time window considered or smaller, then the quasi-static
approximation (20) breaks down, and there is a transition to a diﬀerent coding phase where
temporal coding can be relevant. For example, if a function of time f (t) (e.g. ﬁring rate)
varies sinusoidally with time, then eq. (20) would be broken when the period of oscillation
τc is equal to or shorter than the time window T . Furthermore, in the particular case
in which τc ≪ T , the integral of the function over the time window does not depend on
the value of the function at the center of the interval as in eq(20), or on the frequency of
response variations, but only on the average value of the function in a period τc:

7The term in Eq. 21 can be shown to be non-negative for any rate function, as required by information

theoretical consistency.

14

t0+T /2

t0−T /2

Z

f (t)dt =

1
τc Zτc

(cid:20)

f (t)dt
(cid:21)

T + O(T 2)

(22)

As a consequence, if the time scale of rate ﬂuctuations is of the order of the window T
or smaller, then we are in a phase in which the extra amount of pure temporal information
is proportional to T (and not to T 3 as in the slow variations phase):

I({ta

i }; S) − I(n; S) =

dta

ra(ta; s) log2

*

C

Xa=1 Z
C

Xa=1 *Z

ra(ta; s)
hra(ta; s′)is′ +s
dtara(ta; s)
dtara(ta; s′)is′ +s
R

h

R

−

dtara(ta; s) log2

+ O(T 2)

(23)

This means that when rates ﬂuctuate rapidly the actual rate of information transmission
(measured in bits/sec) can be considerably faster than that with spike counts only. There-
fore in this phase there is a substantial amount of pure temporal information, on the top
of the spike count information. In some cases temporal encoding can even be dominant
with respect to spike count only.

We studied particular cases by simulating mean ﬁring rate and correlation parameters
for each timestep as described in the Section 3.3. The following results were obtained with
1 ms time resolution, but have also been conﬁrmed by numerical integration in the inﬁnite
time-resolution limit. The eﬀect of timing precision is considered separately.

Fig. 2 illustrates a situation in which the spike count of a single cell responding according
to a Poisson process is unable to discriminate between the stimuli after a full period, but
such information is available from spike timing. The time window is grown from the onset
of responses to the stimulus. In this case, after 60 ms (a full cycle of oscillation), the spike
count information drops to zero, whereas the full temporal information accumulates cycle
after cycle. After one cycle the information is purely temporal.

The eﬀect of stimulus modulation frequency on the full temporal information is shown
in Fig. 3a. Fig. 3b shows the purely temporal information for the same situation (i.e. the
full information minus the spike count information). The diﬀerence between the fast and
slow r´egimes can be clearly seen by comparing these two ﬁgures. Slow ﬂuctuations lead
to negligible (of order T 3) pure temporal information. In the fast regime, pure temporal
information increases roughly proportionally to the window length. Note also that, if
ﬂuctuations are very fast (τc ≪ T ) and the spike incidence is measured with high temporal
precision, the amount of information is roughly independent of the frequency of oscillation,
as predicted by (22).

If the rates vary slowly with respect to the window T of interest, but the correlations
vary on a faster time scale, the extra amount of information can be only of second order
in T . This means that the instantaneous rate of information transmission is unaﬀected
by the temporal structure of the spike train, but the temporal information can still be
appreciable. An example of this type of situation is shown in Fig. 4. This ﬁgure plots the
information in the responses of a pair of cells, each of which has the same characteristics
as those in Figures 2 and 3. Both of the cells’ ﬁring rates are of slow (1 Hz) modulation

15

by one stimulus only. However, the correlation between them is modulated more rapidly,
with a frequency of 100 Hz, and it is stimulus dependent (the correlation for one stimulus
is equal in magnitude but opposite in sign to that of the other stimulus). There is, as can
be seen, a substantial contribution to the information in the third, or stimulus dependent,
component of the second order information. This contribution has no counterpart in the
spike count information, which remains much smaller.

4.3 The impact of precision of spike timing

The total information contained in the spike times (1) is also a function of the precision ∆t
with which the spikes are measured (or equivalently, of the precision in the spike timing it-
self). Experimental measures of the information with diﬀerent values of ∆t can address the
question of to what temporal precision information is transmitted in the cerebral cortex.
The whole question of whether spike timing is important is really the question of whether
the use of time resolution as short as a few milliseconds signiﬁcantly increases the infor-
mation extracted (Strong et al., 1998). Measuring information at high resolutions with a
direct method is made diﬃcult by the exponential increase of the number of trials needed
as ∆t is decreased (Strong et al., 1998). Our formalism gives a simple expression for the
information in spike timing as a function of the timing precision ∆t (the discrete version of
eqs. 16,17), and requires only quadratically increasing number of trials as ∆t → 0. Thus
it can be used to measure the information with resolutions that would be impractical with
direct methods using data sets of the size of typical cortical recording sessions (Panzeri
and Treves, 1996).

As an example, Fig. 5 illustrates the impact of precision on the full temporal information
contained in 32 ms from a single cell with responses oscillating at diﬀerent frequencies. The
timing precision has no eﬀect on the information only if it is much smaller than the typical
time scale of response parameter variations τc. If ∆t ≃ τc, then information is strongly
underestimated with respect to the inﬁnite resolution limit.

5 Synergy and redundancy in temporal information

A crucial point in understanding the representation of information by a group of coding
elements is knowing how the information from the individual elements is combined. Is the
amount of information obtained from the whole pool greater than the sum of that from
each individual element (synergistic) or less than the sum (redundant)? In between these
two cases, there exists a situation where the information from each element is independent,
and the total information thus increases linearly as the number of elements is increased.
This yields two notions of “synergy” that are relevant to the current analysis. The ﬁrst
one is the synergy that exists between assemblies of cells; the second one is the synergy
that exists between spikes. The two deﬁnitions are discussed as follows.

16

5.1 Synergy between cells

This measure quantiﬁes if there is additional information in the population responses which
is not present in single cells taken separately. A measure of synergy between cells is crucial
in understanding how the messages carried by single cells combine together to form a
representation of the external stimuli. The amount of synergy between cells is deﬁned as
the total information from the ensemble minus the sum of that from the individual cells
(and the redundancy is simply the negative of this). This means that, to second order, the
synergy is simply the sum of the oﬀ-diagonal (a 6= b) elements of Eq. 17. This is because
the ﬁrst order terms (Eq. 16 and the a = b terms in the summation exist for both the
ensemble information and the sum of the individual cell informations, thus eliminating in
the subtraction.

The amount of synergy between cells is critically dependent on the cross-correlations
between cells. Eq. 17 shows that, to have synergistic coding, non-zero cross-correlations
between cells are needed (in the absence of correlations only the ﬁrst, negative-deﬁnite term
of Itt would survive). Even when noise correlations are not stimulus dependent, however,
it is possible to achieve synergistic coding. In this case, the second, stimulus-independent
correlational component of Itt can increase the total information. This happens, for each
pair of cells, when the noise cross-correlation γab(ta
j; s) is opposite in sign to the signal
j) for times ta
correlation νab(ta
j. (this basic mechanism of synergy for the temporal
information is identical in principle to that considered for spikes counts in (Oram et al.,
1998; Abbott and Dayan, 1999; Panzeri et al., 1999), but it should be valid for separate
time pairs). Cross-correlations between cortical neurons often oscillate and vary rapidly in
sign with time (K¨onig et al., 1995). Our analysis shows that in this case, to obtain maximal
synergistic eﬀect the correlation in the signal oscillation should oscillate, with respect to the
cross-correlogram, in counter-phase (i.e. with opposite sign) and with similar frequency.
(If e.g. the signal oscillates with lower frequency the eﬀect of cross-correlation would be
washed away by their rapid change of sign with respect to the signal).

i , tb

i , tb

i , tb

The eﬀect of the relative frequencies and phase of signal and noise variations with
time is illustrated in Fig. 6. Two Poisson cells are simulated, responding to two stimuli
as in Fig. 2a, but with oscillation frequency in response to the second stimulus of 10
and 20 Hz respectively. The trial-by-trial variability of the responses of the two cells is
cross-correlated. The cross-correlation function decays exponentially, with time constant
of 25 ms, as a function of time between spikes (K¨onig et al., 1995). When the noise cross-
correlation between the two cells is chosen to oscillate with the same frequency as the signal
oscillation, synergy is obtained when signal and noise correlation are in counter-phase, and
a high redundancy is achieved when they are in phase. When instead signal and noise
oscillate on very diﬀerent time scales, correlations play a much less signiﬁcant role. The
neuronal model used in this simulation is certainly far from realistic. However, the results of
the mathematical analysis ensure that the result discussed here is general and valid beyond
the simple models used for the ﬁgures. Finally, we note that when cross-correlations are
stimulus dependent, then also the third term of Eq. (17) becomes non-zero, and additional
interactions between response parameters can arise. They can be studied by specifying the

17

stimulus dependence of correlations and taking into account also the third component of
(17).

5.2 Synergy between spikes

The other notion of synergy, that is relevant only to the temporal information, is the syn-
ergy between spikes. This is obviously deﬁned for single cells as well as for populations. It
quantiﬁes whether patterns of spikes carry extra information in addition to what is repre-
sented in single spikes. Motivated by the notion that particular sequences of spikes may
have a special role in encoding stimuli (Vaadia et al., 1995), Brenner et al. (1999) recently
introduced a synergy measure of this type. In that work, single “events” (e.g. single spikes,
or pairs of spikes occurring with a given time delay irrespective of whether there are other
spikes in-between) are singled out of the whole spike train, and the information carried by
occurrence of single events is computed8. Brenner et al. considered the synergy to be the
information from a complex event (e.g. a pair of spikes occurring at speciﬁed times) minus
the information from each of those individual spikes constituting that pair.

Another way to measure the synergy between spikes suggested by the present analysis
would be, instead of separating contributions of single events, to take into account their
contribution to the information conveyed by the whole spike train. Our short time scale
expansion shows that, if cells never emit more than one spike in response to stimuli, then
the ﬁrst order approximation (16) to the information is exact. In general, if up to k spikes
can be emitted in the time window, then the information contained in the full spike train is
exactly described by the ﬁrst k orders of the expansion. If we focus for simplicity on pairs
of spikes, a meaningful deﬁnition of pairwise synergy between spikes is then simply equal
to the second order temporal information terms given by Eq. 17. These terms measure the
additional information about the stimulus (beyond that due simply to single spikes) that
is present in whole the spike train due to the fact that pairs of spikes constitute the spike
train.

It is interesting to note that the introduction of a refractory period has an accompa-
nying eﬀect on the synergy between spikes, as measured by the fractional second order
contribution to the overall temporal information. Fig. 7a shows the eﬀect of the refractory
period on the synergy between spikes. It is apparent that for the Poisson process (in which
spikes are not entirely independent because of the rate modulation) the second order con-
tribution is small or negative. For longer refractory periods there is an increasing peak
in the early period of the response. This eﬀect appears to involve an interaction between
the absolute refractory period and the time-constant of stimulus modulation of the rates;
the stimulus modulation timescale determines the width of the spike-synergy peak (see
Fig. 7b), whereas the refractory period length determines its height. A relative refractory
8The deﬁnition of information carried by the occurrence of a single event, is a generalization of Eq. 16.
However, the Brenner et al. (1999) deﬁnition of information carried e.g. by single spikes only, separated
form the rest of the spike train, is diﬀerent from the contribution of the occurrence of just a single spike
in a given trial to the information from the full spike train. In fact, the latter quantity also has terms of
higher order.

18

period (achieved by exponential decay of the autocorrelation to zero with time-constant
20 ms after the 2ms absolute refractory period) is also shown in the ﬁgure. The eﬀect
of this is to further boost the second order information (and of course the total temporal
information) for a short period with time scale determined by the dominant frequency of
stimulus modulation of the rate parameter.

6 Discussion

We have demonstrated that the (MacKay and McCulloch, 1952) information contained in
the speciﬁcation of the times of spike emission of a population of cells can be broken up
into a series of terms that have physiological signiﬁcance, just as the information contained
in the spike counts from a population can (Panzeri et al., 1999). Use of this series approach
has a number of advantages over brute force computation for both the understanding of
the theory of coding with spike trains, and for the analysis of neurophysiological data. This
approach necessarily compares the contributions of diﬀerent information-bearing parame-
ters on correct and equal terms, and shows how they combine to yield the full information
available from the spike train. It allows analytical study of the statistical properties of
population spike trains, such as refractoriness, autocorrelation, and timing relationships
between cells, to be conducted with regard to understanding which parts of the full tem-
poral information they contribute to and which they do not. Furthermore, computation
of the information components from experimental data could in principle even be done
on-line.

Quantiﬁcation of the information carried by the spike times is made diﬃcult, even for
single cells, by the large amount of data needed to adequately sample the probabilities of
occurrence of the observed spike sequences in response to a stimulus. When considering a
population of neurons, the required amount of data increases exponentially with the pop-
ulation size. Existing studies of the encoding of information in spike times have therefore
relied upon compression methods, like Principal Component Analysis (Optican and Rich-
mond, 1987), or stimulus-decoding algorithms (Victor and Purpura, 1998), or both (Heller
et al., 1995), to compress the temporal pattern of activity onto a smaller response space
which can be sampled experimentally. Their eﬀectiveness is thus dependent on the decod-
ing algorithm used and cannot be checked for the data-set under study. Our procedure
instead provides a way to overcome this problem. It is in fact easier to compute these in-
formation components from limited samples of neurophysiological data than with a direct
approach. This is because the information is computed by using only the subset of all the
possible variables characterising the spike train that carry more information. Therefore the
complexity of the response space is reduced in the way that preserves the most information,
and accurate estimates can be obtained by limited datasets9. Its accuracy can be veriﬁed
on the data-set under study by checking the validity of the assumptions made, or even the
magnitude of the neglected terms if one extends the calculation to higher orders in T .

9This will be reported in detail in a following paper, containing also systematic analysis of physiological

data.

19

This study reinforces the point (Rieke et al., 1996) that the crucial question that we
must ask in order to understand how important temporal encoding is is: how short is the
timescale (or equivalently: the precision) of response variations with respect to the length
of the time window relevant for information transmission? The answer to this question is
likely to depend on the set of external correlates that the neurons have to code for, and
on their typical time scale of variation. For example, recent papers have shown that MT
neurons produce, in response to rapid transition from anti-preferred to preferred motion
direction, a spiking activity for 25-40 ms only, which is locked to the transition time with
high timing precision (5-7 ms) (Bair and Koch, 1996; Buracas et al., 1998). This seems to
indicate that much information about which directional stimulus was presented is encoded
in spike counts only in windows of the order of 30 ms, whereas stimulus transition times are
encoded in spike times with a few ms precision. Classic results about temporal encoding
of visual static stimuli revealed that additional information, not present in spike counts,
was encoded in the ﬁrst few Principal Components extracted from 300-500 ms epochs of
neuronal responses (Optican and Richmond, 1987; Tov´ee et al., 1993). However, most of
the information extracted from long windows was just encoded in the ﬁring rates computed
in windows taken near the onset of response and as short as 20-30 ms (Tov´ee et al., 1993).
Recent studies have shown that precise spike sequences in this shorter window do not convey
any additional information above that present in the spike counts (Oram et al., 1999).
This indicates that in the case of static stimuli the extra amount of information present
in Principal Components extracted from long windows does not reﬂect genuine temporal
encoding, but rather that the relevant window is much shorter, and that the spike count
is the basic variable for encoding in this window.

Shadlen and Newsome (1998) argue that because the interspike interval in the responses
of cortical neurons is highly variable, the rapid information transmission achieved by the
cerebral cortex (i.e. a substantial information being transmitted in one ISI or less) must
imply redundancy of signal. Their argument is based on the idea that to obtain reliability in
short timescales it is necessary to average away the large observed variability of individual
ISIs by replicating the signal through many similar neurons. In other words, the need for
rapid information transmission should strongly constrain the cortical architecture. The
fact that the ﬁrst order temporal information transmitted by a population is simply the
sum of all single-cell contributions (Eq. 16) demonstrates that it is instead not necessary
to transmit many copies of the same signal to ensure rapid and reliable transmission. If in
fact each cell conveys some information about the stimuli, this sums up in less than one
ISI, and high population information rates can be achieved.

When trial-by-trial correlations are not strongly modulated by the stimulus, their im-
pact on information representation was shown to crucially depend on their interplay with
the signal correlation. To carry positive information, the correlation in the mean re-
sponse proﬁles of pairs of cell should be opposite, across diﬀerent time lags, to the noise
cross-correlation. Normalised cross-correlation functions between cortical neurons are often
found to vary rapidly in sign with time (K¨onig et al., 1995; Singer et al., 1997). When this
is the case, the signal correlation should oscillate with opposite phase and similar frequency
to achieve signiﬁcant correlational information transmission. As signal correlation origi-

20

nates from ﬁring rates, trial-by-trial correlations should ﬂuctuate with frequencies similar
In any case, when re-
to that of ﬁring rates to achieve optimal correlational encoding.
sponse proﬁles of diﬀerent neurons are independent, and little signal correlation is present,
weakly stimulus modulated correlations do not aﬀect information transmission at all (Oram
et al., 1998; Panzeri et al., 1999). This is important as signal correlation between cortical
neurons was reported to be very small for stimulus sets of increasing complexity (Young and
Yamane, 1992; Gawne et al., 1996; Rolls et al., 1997; DeAngelis et al., 1999), and therefore
correlations might be less important under natural conditions than when using artiﬁcial
and limited laboratory stimuli. This shows that the contribution of cross-correlations to
the representation of the external world can be more complex than what might be expected
from naive visual inspection of cross-correlograms. This reinforces the need for a rigorous
information theoretic analysis of the role of trial-by-trial correlations in solving complex
encoding problems, like feature binding (Singer et al., 1997).

A further point worth discussion is the refractory period. The eﬀect of the introduction
of a refractory period is to increase the information via the second order component 2b
of the temporal information. It has been noted previously that the temporal correlations
introduced into the spike train by refractoriness can have a beneﬁcial eﬀect on coding.
This occurs by regularization of the higher ﬁring rate parts of the response, producing
a more deterministic relation between stimulus and response (de Ruyter van Steveninck
et al., 1997; Berry II and Meister, 1998). The present analysis conﬁrms this eﬀect, and adds
several new facts: that the eﬀect is second order in time, and that it involves interaction
between the refractory and dominant stimulus-induced modulation timescales, such that
the width of the period of increased information is determined by the stimulus frequency
characteristics, whereas the amount of extra information is determined by the eﬀective
duration of refraction. This study provides a means to understand how other parameters
of biophysical relevance can be adjusted to maximise encoding capacity.

In conclusion, a full understanding of the coding properties of a neural system cannot
be achieved simply by computing the total information about the stimuli contained in its
spike trains; rather, it is necessary to at the very least discover how this total is comprised
from the individual information-bearing parameters. The results that we have presented
here encourage us to think that this is possible.

Acknowledgements

We thank P. Dayan, J. Gigg, E.T. Rolls, J.W. Scannell, A. Treves, and M.P. Young for
useful discussions. This research was supported by the Wellcome Trust Programme Grant
055075/Z/98 (S.P.) and by the Howard Hughes Medical Institute (S.R.S.)

References

21

Abbott, L. F. and Dayan, P. (1999). The eﬀect of correlated variability on the accuracy of

a population code, Neural Comp. 11: 91–101.

Adrian, E. D. (1926). The impulses produced by sensory nerve endings: Part I, J. Physiol.

(Lond.) 61: 49–72.

Aertsen, A. M. H. J., Gerstein, G. L., Habib, M. K. and Palm, G. (1989). Dynamics of
neuronal ﬁring correlation: modulation of “eﬀective connectivity”, J. Neurophysiol.
61: 900–917.

Bair, W. (1999). Spike timing in the mammalian visual system, Current Opinion in Neu-

robiology 9: 447–453.

Bair, W. and Koch, C. (1996). Temporal precision of spike trains in extrastriate cortex of

the behaving macaque monkey, Neural Computation 8(6): 779–786.

Berry II, M. J. and Meister, M. (1998). Refractoriness and neural precision, J. Neurosci.

18(6): 2200–221.

Bialek, W., Rieke, F., de Ruyter van Steveninck, R. R. and Warland, D. (1991). Reading

a neural code, Science 252: 1854–1857.

Brenner, N., Strong, S. P., Koberle, R., Bialek, W. and de Ruyter van Steveninck, R.

(1999). Synergy in a neural code, Neural Computation p. In press.

Buracas, G. T., Zador, A. M., de Weese, M. R. and Albright, T. D. (1998). Eﬃcient
discrimination of temporal patterns by motion-sensitive neurons in primate visual
cortex, Neuron 20: 959–969.

Cover, T. M. and Thomas, J. A. (1991). Elements of information theory, John Wiley, New

York.

de Ruyter van Steveninck, R. R., Lewen, G. D., Strong, S. P., Koberle, R. and Bialek, W.
(1997). Reproducibility and variability in neural spike trains, Science 275: 1805–1808.

DeAngelis, G. C., Ghose, G. M., Ohzawa, I. and Freeman, R. D. (1999). Functional micro-
organization of primary visual cortex; receptive ﬁeld analysis of nearby neurons, J.
Neurosci. 19: 4046–4064.

deCharms, R. C. and Merzenich, M. M. (1996). Primary cortical representation of sounds

by the coordination of action potentials, Nature 381: 610–613.

Gawne, T. J., Kjaer, T. W., Hertz, J. A. and Richmond, B. J. (1996). Adjacent visual
cortical complex cells share about 20% of their stimulus-related information, Cerebral
Cortex 6: 482–489.

Gawne, T. J. and Richmond, B. J. (1993). How independent are the messages carried by

adjacent inferior temporal cortical neurons?, J. Neurosci. 13: 2758–2771.

22

Heller, J., Hertz, J. A., Kjaer, T. W. and Richmond, B. J. (1995). Information ﬂow and

temporal coding in primate pattern vision, J. Comp. Neurosci. 2: 175–193.

K¨onig, P., Engel, A. K. and Singer, W. (1995). Relation between oscillatory activity and
long-range synchronization in cat visual cortex, Proc. Natl. Acad. Sci. USA 92: 290–
294.

MacKay, D. and McCulloch, W. S. (1952). The limiting information capacity of a neuronal

link, Bull. Math. Biophys. 14: 127–135.

Mechler, F., Victor, J. D., Purpura, K. P. and Shapley, R. (1998). Robust temporal coding
of contrast by V1 neurons for transient but not steady-state stimuli, J. Neurosci.
18(16): 6583–6598.

Optican, L. M. and Richmond, B. J. (1987). Temporal encoding of two-dimensional pat-
terns by single units in primate inferior temporal cortex: III. Information theoretic
analysis, J. Neurophysiol. 57: 162–178.

Oram, M. W., F¨oldi´ak, P., Perrett, D. I. and Sengpiel, F. (1998). The ’Ideal Homunculus’:

decoding neural population signals, Trends in Neurosciences 21(6): 259–265.

Oram, M. W., Wiener, M. C., Lestienne, R. and Richmond, B. (1999). Stochastic nature
of precisely timed spike patterns in visual system neuronal response, J. Neurophysiol.
81: 3021–3033.

Panzeri, S., Schultz, S. R., Treves, A. and Rolls, E. T. (1999). Correlations and the encoding

of information in the nervous system, Proc. R. Soc. Lond. B 266: 1001–1012.

Panzeri, S. and Treves, A. (1996). Analytical estimates of limited sampling biases in

diﬀerent information measures, Network 7: 87–107.

Riehle, A., Grun, S., Diesmann, M. and Aertsen, A. M. H. J. (1997). Spike synchroniza-
tion and rate modulation diﬀerentially involved in motor cortical function, Science
278: 1950–1953.

Rieke, F., Warland, D., de Ruyter van Steveninck, R. R. and Bialek, W. (1996). Spikes:

exploring the neural code, MIT Press, Cambridge, MA.

Rolls, E. T., Tovee, M. and Panzeri, S. (1999). The neurophysiology of backward visual

masking: Information analysis, J. Cognitive Neurosci. 11: 335–346.

Rolls, E. T., Treves, A. and Tov´ee, M. J. (1997). The representational capacity of the
distributed encoding of information provided by populations of neurons in primate
temporal visual cortex, Exp. Brain Res. 114: 149–162.

Shadlen, M. N. and Newsome, W. T. (1998). The variable discharge of cortical neurons:
implications for connectivity, computation and coding, J. Neurosci. 18(10): 3870–
3896.

23

Shannon, C. E. (1948). A mathematical theory of communication, AT&T Bell Labs. Tech.

J. 27: 379–423.

Singer, W., Engel, A. K., Kreiter, A. K., Munk, M. H. J., Neuenschwander, S. and Roelf-
sema, P. (1997). Neuronal assemblies: necessity, signature and detectability, Trends
in Cognitive Sciences 1: 252–261.

Strong, S., Koberle, R., de Ruyter van Steveninck, R. and Bialek, W. (1998). Entropy and

information in neural spike trains, Physical Review Letters 80: 197–200.

Thorpe, S., Fize, D. and Marlot, C. (1996). Speed of processing in the human visual

system, Nature 381: 520–522.

Tov´ee, M. J., Rolls, E. T., Treves, A. and Bellis, R. P. (1993). Information encoding and
the response of single neurons in the primate temporal visual cortex, J. Neurophysiol.
70: 640–654.

Vaadia, E., Haalman, I., Abeles, M., Bergman, H., Prut, Y., Slovin, H. and Aertsen, A.
(1995). Dynamics of neuronal interactions in monkey cortex in relation to behavioural
events, Nature 373: 515–518.

Victor, J. D. and Purpura, K. P. (1998). Spatial phase and the temporal structure of the

response to gratings in V1, J. Neurophys. 80: 554–571.

Young, M. P. and Yamane, S. (1992). Sparse population coding of faces in the inferotem-

poral cortex, Science 256: 1327–1331.

24

Figure Captions

Fig 1: The accuracy of the ﬁrst (eq. 16) and second order (eq. 17) approximations
to the full information (eq. 1, full line) in the spike times of a single cell with Poisson
responses to two stimuli. The cell responded to stimulus 1 with a constant (in time) rate
of 30 spikes/sec., and to stimulus 2 with a spike rate oscillating sinusoidally around 30
spikes/sec. with period 2 ms and amplitude 30 spikes/sec.

Fig 2: Comparison of complete temporal and spike count information. a The mod-
ulation of the ﬁring rates of a single non-homogeneous Poisson neuron by two stimuli. b
The evolution of the full temporal and the spike count information as the time window is
increased in width from stimulus onset.

Fig 3: The eﬀect of the frequency of stimulus modulation of the rates. a Full temporal
information. b Purely temporal information only. The main requirement for temporal
contribution to the information can be seen easily in the comparison of these two ﬁgures –
fast ﬂuctuation of the ﬁring rates in comparison to the timescale of observation.

Fig 4: A situation in which stimulus dependent correlation in spike timing between
cells can make a sizeable, but second order, contribution to the total information, via
component 2c of the temporal information. This inofrmation is purely temporal and is
In this simulation the rates of each of the individual cells
not present in spike counts.
were modulated like in Fig 2a, but at 1 Hz frequency. The correlation between them was
modulated at frequency f = 100Hz according to the following function for ﬁrst stimulus:
γab(ta, tb; s) = exp(−|ta − tb|/λ) sin(2πf ta) sin(2πf tb) , where the decay constant λ is 25
ms. (The cross-correlation in response the second stimulus is equal but opposite in sign).
This particular cross-correlation function was chosen to match the one used in Fig. 6, but
the result are indicative of the behavior of any fast-oscillating cross-correlation.

Fig 5: The eﬀect of precision of spike timing on the information available from 32 ms
of the response of a single neuron responding with sinusoidal modulation of ﬁring by the
stimulus (as in Fig. 3). We used an absolute refractory period of 4 ms in this simulation
(in order to have at most one spike per time bin for all the precisions used). The data
obtained with the continuous-time limit (obtained by numerical integration) are included
as the y-intercept information values.

Fig 6. (a): The eﬀect of the relative frequencies and phase of signal and noise temporal
variations in achieving synergistic coding. Two Poisson cells are simulated, responding
to two stimuli as in Fig. 2a, but with oscillation frequency in response to the second
stimulus of f1 = 10Hz and f2 = 20Hz respectively. The signal correlation (9) is in
this simple case proportional to sin(2πf1ta) sin(2πf2tb). The information extracted form
the pair of cell is compared to the sum of the information carried by individual cells.
To generate a cross-correlation model that could be tuned to match the oscillations in
the signal, the cross-correlation was chosen of this form for both stimuli: γab(ta, tb; s) =
γexp(−|ta − tb|/λ) sin(2πkf1ta) sin(2πkf2tb). (an exponential decay between spikes with
λ = 25ms was inserted to match real cortical correlograms, see (K¨onig et al., 1995)) The
constant γ in front of the correlation function was set either to 0 (no cross-correlation),
to -1 (signal and noise in counter-phase), or to 1 (signal in phase with the noise). k was

25

set to 1 to allow signal and noise to oscillate with similar timescales. When signal and
noise have opposite signs and similar frequencies, there is a synergistic code, i.e. there is
more information in the pair than in the sum of the two single cells. (b) As in (a), but k
is set to 10, in order to have noise oscillating 10 times faster than the signal. The eﬀect
of trial-by-trial cross-correlations is washed away by their rapid variations with respect to
the signal.

Fig 7: (a). Synergy between spikes, as measured by the fraction of the temporal
information contained in the second order terms. This is shown for the example of a
cell whose ﬁring rate is modulated by a 40 Hz sinusoid for one stimulus as in the earlier
ﬁgures. A non-homogeneous Poisson process is contrasted with processes augmented with
an absolute refractory period of between 1 and 16 ms. Another process, which has a
relative refractory period with autocorrelation decaying exponentially (time-constant 20
ms) in addition to a 2 ms absolute refractory period, is also shown (circles). (b). The
interaction of a 4 ms refractory period with stimulus-induced modulation frequency. The
fraction of second-order information (synergy between spikes) is shown for three diﬀerent
frequencies.

26

True information
1st order       
2nd order       

0.5

0.4

0.3

0.2

0.1

i

i

)
s
t
i
b
(
 
s
e
m
T
 
e
k
p
S
 
n
i
 
n
o
i
t
a
m
r
o
f
n
I

0
0

50

100
Time after onset (ms)

150

200

Figure 1: Panzeri et al

27

a

0
0

50

)
z
H

(
 
e
t
a
R
 
g
n
i
r
i
F

0.5

0.4

0.3

0.2

0.1

)
s
t
i
b
(
 
n
o
i
t
a
m
r
o
f
n
I

b

0
0

Stimulus 1
Stimulus 2

50

100

150

spike times 
spike counts

50

100

150

Time after stimulus onset (ms)

Figure 2: Panzeri et al

28

1 Hz 
4 Hz 
20 Hz
50 Hz

1 Hz 
4 Hz 
20 Hz
50 Hz

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

)
s
t
i
b
(
 
s
e
m

i

i
t
 
e
k
p
s
 
n
i
 
n
o
i
t
a
m
r
o
f
n
I

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

)
s
t
i
b
(
 
n
o
i
t
a
m
r
o
f
n
I
 
g
n
m
T
e
r
u
P

 

i

i

0
0

a

50

100

150

Time after stimulus onset (ms)

0
0

b

50

100

150

Time after stimulus onset (ms)

Figure 3: Panzeri et al

29

full temporal     
spike count       
1st order temporal
2nd order a       
2nd order b       
2nd order c       

)
s
t
i

b
(
 

n
o

i
t

a
m
r
o

f

n

I

0.6

0.5

0.4

0.3

0.2

0.1

0

−0.1
0

20

40

60

80

100

Time after stimulus onset (ms)

Figure 4: Panzeri et al

30

)
s
t
i
b
(
 
n
o
i
t
a
m
r
o
f
n
I

0.12

0.1

0.08

0.06

0.04

0.02

0

5 Hz  
50 Hz 
100 Hz
250 Hz

0.25

0.5

1

2

4

Time resolution (ms)

Figure 5: Panzeri et al

31

signal as fast as noise

sum of single cells           
2 cells ; γ = 0;  no corr
2 cells ; γ = +1         
2 cells ; γ = −1         

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
s
t
i
b
(
 
n
o
i
t
a
m
r
o

f
n
I

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
s
t
i
b
(
 
n
o
i
t
a
m
r
o

f

n

I

0
0

a

20

40

60

Time after onset (ms)

80

100

signal much slower than noise

sum of single cells           
2 cells ; γ = 0;  no corr
2 cells ; γ = +1         
2 cells ; γ = −1         

0
0

b

20

40

60

Time after onset (ms)

80

100

Figure 6: Panzeri et al

32

τ
refr
τ
refr
τ
refr
τ
refr
τ
refr
τ
refr

 = 0           
 = 1 ms        
 = 2 ms        
 = 4 ms        
 = 16 ms       
 = 2 ms + decay

0.3

0.25

0.2

0.15

0.1

0.05

0

n
o
i
t
c
a
r
f
 
r
e
d
r
o
 
d
n
2

−0.05

−0.1

−0.15
0

0.3

0.25

0.2

0.15

0.1

0.05

0

n
o
i
t
c
a
r
f
 
r
e
d
r
o
 
d
n
2

a

10

20
30
Time after onset

40

50

20 Hz
40 Hz
80 Hz

−0.05
0

b

10

20

30

40

50

Time after onset (ms)

Figure 7: Panzeri et al

33


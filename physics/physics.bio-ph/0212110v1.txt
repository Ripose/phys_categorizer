2
0
0
2
 
c
e
D
 
1
3
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
1
2
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Analyzing neural responses to natural signals:
Maximally informative dimensions

Tatyana Sharpee,1 Nicole C. Rust,2 and William Bialek1,3

1 Sloan–Swartz Center for Theoretical Neurobiology and Department of Physiology
University of California at San Francisco, San Francisco, California 94143–0444
2 Center for Neural Science, New York University, New York, NY 10003
3 Department of Physics, Princeton University, Princeton, New Jersey 08544
sharpee@phy.ucsf.edu, rust@cns.nyu.edu, wbialek@princeton.edu

February 21, 2014

We propose a method that allows for a rigorous statistical analysis of neu-
ral responses to natural stimuli which are non–Gaussian and exhibit strong
correlations. We have in mind a model in which neurons are selective for
a small number of stimulus dimensions out of a high dimensional stimulus
space, but within this subspace the responses can be arbitrarily nonlinear. Ex-
isting analysis methods are based on correlation functions between stimuli
and responses, but these methods are guaranteed to work only in the case of
Gaussian stimulus ensembles. As an alternative to correlation functions, we
maximize the mutual information between the neural responses and projec-
tions of the stimulus onto low dimensional subspaces. The procedure can be
done iteratively by increasing the dimensionality of this subspace. Those di-
mensions that allow the recovery of all of the information between spikes and
the full unprojected stimuli describe the relevant subspace. If the dimension-
ality of the relevant subspace indeed is small, it becomes feasible to map the
neuron’s input–output function even under fully natural stimulus conditions.
These ideas are illustrated in simulations on model visual and auditory neu-
rons responding to natural scenes and sounds, respectively.

1 Introduction

From olfaction to vision and audition, a growing number of experiments are examin-
ing the responses of sensory neurons to natural stimuli (Rieke et al., 1995; Theunissen
et al., 2000; Vinje & Gallant, 2000; Lewen et al., 2001; Sen et al., 2001; Vickers et al., 2001;
Vinje & Gallant, 2002; Ringach et al., 2002). Observing the full dynamic range of neural
responses may require using stimulus ensembles which approximate those occurring in
nature (Rieke et al., 1997; Simoncelli & Olshausen, 2001), and it is an attractive hypothe-
sis that the neural representation of these natural signals may be optimized in some way

1

(Barlow, 1961, 2001; von der Twer & Macleod, 2001; Bialek, 2002). Many neurons exhibit
strongly nonlinear and adaptive responses that are unlikely to be predicted from a combi-
nation of responses to simple stimuli; for example neurons have been shown to adapt to
the distribution of sensory inputs, so that any characterization of these responses will de-
pend on context (Smirnakis et al., 1996; Brenner et al., 2000a; Fairhall et al., 2001). Finally,
the variability of a neuron’s responses decreases substantially when complex dynamical,
rather than static, stimuli are used (Mainen & Sejnowski, 1995; de Ruyter van Steveninck
et al., 1997; Kara et al., 2000; de Ruyter van Steveninck et al., 2001). All of these arguments
point to the need for general tools to analyze the neural responses to complex, naturalistic
inputs.

The stimuli analyzed by sensory neurons are intrinsically high dimensional, with di-
mensions D ∼ 102−103. For example, in the case of visual neurons, the input is commonly
speciﬁed as light intensity on a grid of at least 10×10 pixels. Each of the presented stimuli
can be described as a vector s in this high dimensional stimulus space. It is important that
stimuli need not be pictured as isolated points drawn from this space. Thus, if stimuli are
varying continuously in time we can think of the stimulus s as describing a recent window
of the stimulus history (e. g., the past N frames of the movie with total dimensionality N
times larger than that of a single frame). The distribution of stimuli P (s) is sampled along
some meandering trajectory in this space; we will assume this process is ergodic, so that
we can exchange averages over time with averages over the true distribution as needed.
Even though direct exploration of a D ∼ 102 − 103 dimensional stimulus space is
beyond the constraints of experimental data collection, progress can be made provided
we make certain assumptions about how the response has been generated. In the sim-
plest model, the probability of response can be described by one receptive ﬁeld (RF) or
linear ﬁlter (Rieke et al., 1997). The receptive ﬁeld can be thought of as a template or spe-
cial direction v in the stimulus space such that the neuron’s response depends only on a
projection of a given stimulus s onto v, although the dependence of the response on this
projection can be strongly nonlinear. In this simple model, the reverse correlation method
(de Boer & Kuyper, 1968; Rieke et al., 1997; Chichilnisky, 2001) can be used to recover the
vector v by analyzing the neuron’s responses to Gaussian white noise. In a more general
case, the probability of the response depends on projections s(µ) = ˆe(µ) · s, µ = 1, ..., K, of
the stimulus s on a set of K vectors {ˆe(1), ˆe(2), ... , ˆe(K)}:

P (spike|s) = P (spike)f (s(1), s(2), ..., s(K)),

(1)

where P (spike|s) is the probability of a spike given a stimulus s and P (spike) is the average
ﬁring rate. Even though the ideas developed below can be used to analyze input–output
functions f with respect to different neural responses, such as patterns of spikes in time
(de Ruyter van Steveninck & Bialek, 1988; Brenner et al., 2000b), we choose a single spike
as the response of interest. The vectors {ˆe(i)} may also describe how the time dependence
of the stimulus s affects the probability of a spike. We will call the subspace spanned by
the set of vectors {ˆe(i)} the relevant subspace (RS).

Equation (1) in itself is not yet a simpliﬁcation if the dimensionality K of the RS is
equal to the dimensionality D of the stimulus space. In this paper we will use the con-
cept of dimensionality reduction (de Ruyter van Steveninck & Bialek, 1988; Brenner et al.,

2

2000a; Bialek & de Ruyter van Steveninck, 2002) and assume that K ≪ D. The input–
output function f in Eq. (1) can be strongly nonlinear, but it is presumed to depend only
on a small number of projections. This assumption appears to be less stringent than that of
approximate linearity which one makes when characterizing neuron’s response in terms
of Wiener kernels [see for example the discussion in Section 2.1.3 of Rieke et al. (1997)].
The most difﬁcult part in reconstructing the input–output function is to ﬁnd the RS. Note
that for K > 1, a description in terms of any linear combination of vectors {ˆe(i)} is just
as valid, since we did not make any assumptions as to a particular form of the nonlinear
function f .

Once the relevant subspace is known, the probability P (spike|s) becomes a function
of only a few parameters, and it becomes feasible to map this function experimentally,
inverting the probability distributions according to Bayes’ rule:

f ({s(i)}) =

P ({s(i)}|spike)
P ({s(i)})

.

(2)

If stimuli are chosen from a correlated Gaussian noise ensemble, then the neural response
can be characterized by the spike–triggered covariance method (Brenner et al., 2000a;
de Ruyter van Steveninck & Bialek, 1988; Bialek & de Ruyter van Steveninck, 2002). It
can be shown that the dimensionality of the RS is equal to the number of nonzero eigen-
values of a matrix given by a difference between covariance matrices of all presented
stimuli and stimuli conditional on a spike. Moreover, the RS is spanned by the eigen-
vectors associated with the nonzero eigenvalues multiplied by the inverse of the a priori
covariance matrix. Compared to the reverse correlation method, we are no longer limited
to ﬁnding only one of the relevant directions ˆe(i). Both the reverse correlation and the
spike–triggered covariance method, however, give rigorously interpretable results only
for Gaussian distributions of inputs.

In this paper we investigate whether it is possible to lift the requirement for stimuli to
be Gaussian. When using natural stimuli, which certainly are non–Gaussian, the RS can-
not be found by the spike–triggered covariance method. Similarly, the reverse correlation
method does not give the correct RF, even in the simplest case where the input–output
function in Eq. (1) depends only on one projection; see Appendix A for a discussion of
this part. However, vectors that span the RS clearly are special directions in the stimulus
space independent of assumptions about P (s). This notion can be quantiﬁed by Shannon
information. Therefore the current implementation of dimensionality reduction idea is
complementary to the clustering of stimuli conditional on a spike done in the informa-
tion bottleneck method (Tishby et al., 1999); see also (Dimitrov & Miller, 2001). In Sec. 2
we discuss how an optimization problem can be formulated to ﬁnd the RS. A particular
algorithm used to implement the optimization scheme is described in Sec. 3. In Sec. 4 we
illustrate how the optimization scheme works with natural stimuli for model orientation
sensitive cells with one and two relevant directions, much like simple and complex cells
found in primary visual cortex, as well as a model auditory neuron responding to natural
sounds. It also is possible to calculate expected values of projections between the un-
derlying relevant directions and the reconstructed vectors for a given data set size. The
advantage of this optimization scheme is that it does not rely on any speciﬁc statistical
properties of the stimulus ensemble, and thus can be used with natural stimuli.

3

2 Information as an objective function

When analyzing neural responses, we compare the a priori probability distribution of
all presented stimuli with the probability distribution of stimuli which lead to a spike
(de Ruyter van Steveninck & Bialek, 1988). For Gaussian signals, the probability distri-
bution can be characterized by its second moment, the covariance matrix. However, an
ensemble of natural stimuli is not Gaussian, so that neither second nor any other ﬁnite
number of moments is sufﬁcient to describe the probability distribution. In this situa-
tion, Shannon information provides a rigorous way of comparing two probability dis-
tributions. The average information carried by the arrival time of one spike is given by
(Brenner et al., 2000b)

Ispike =

dDsP (s|spike) log2

Z

P (s|spike)
P (s)

"

.

#

The information per spike as written in (3) is difﬁcult to estimate experimentally, since it
requires either sampling of the high–dimensional probability distribution P (s|spike) or a
model of how spikes were generated, i.e. the knowledge of low–dimensional RS. How-
ever it is possible to calculate Ispike in a model–independent way, if stimuli are presented
multiple times to estimate the probability distribution P (spike|s). Then,

Ispike =

P (spike|s)
P (spike)

*

log2

P (spike|s)
P (spike) #+s

,

"

where the average is taken over all presented stimuli. This can be useful in practice (Bren-
ner et al., 2000b), because we can replace the ensemble average his with a time average,
and P (spike|s) with the time dependent spike rate r(t). Note that for a ﬁnite dataset of
N repetitions, the obtained value Ispike(N) will be on average larger than Ispike(∞), with
difference ∼ 1/(P (spike)N 2 ln 2) (Treves & Panzeri, 1995), the true value Ispike can also be
found by extrapolating to N → ∞ (Brenner et al., 2000b; Strong et al., 1998). Measure-
ment of Ispike in this way provides a model independent benchmark against which we can
compare any description of the neuron’s input–output relation.

Having in mind a model in which spikes are generated according to a projection onto
a low dimensional subspace, we start by projecting all of the presented stimuli on a par-
ticular direction v in the stimulus space, and form probability distributions

ds · · · P (s), and h· · · |spikeis =
where h· · ·is =
value conditional on the occurrence of a spike. The information

ds · · · P (s|spike) denotes an expectation

R

Pv(x) = hδ(x − s · v)is,
Pv(x|spike) = hδ(x − s · v)|spikeis,

R

Z

I(v) =

dxPv(x|spike) log2

Pv(x|spike)
Pv(x)

"

#

provides an invariant measure of how much the occurrence of a spike is determined by
projection on the direction v. It is a function only of direction in the stimulus space and

(3)

(4)

(5)
(6)

(7)

4

does not change when vector v is multiplied by a constant. This can be seen by noting that
for any probability distribution and any constant c, Pcv(x) = c−1Pv(x/c). When evaluated
along any vector, I(v) ≤ Ispike. The total information Ispike can be recovered along one
particular direction only if v = ˆe(1), and the RS is one dimensional.

By analogy with (7), one could also calculate information I(v1, ..., vn) along a set of

several directions {v1, ..., vn} based on the multi-point probability distributions:

Pv1,...,vn({xi}|spike) =

Pv1,...,vn({xi}) =

n

*

Yi=1
n

*

Yi=1

δ(xi − s · vi)|spike

,

+s

δ(xi − s · vi)

.

+s

If we are successful in ﬁnding all of the K directions ˆe(i) in the input–output relation
(1), then the information evaluated in this subspace will be equal to the total information
Ispike. When we calculate information along a set of K vectors that are slightly off from the
RS, the answer, of course, is smaller than Ispike and is initially quadratic in small deviations
δvi. One can therefore hope to ﬁnd the RS by maximizing information with respect to K
vectors simultaneously. The information does not increase if more vectors outside the
RS are included. For uncorrelated stimuli, any vector or a set of vectors that maximizes
I(v) belongs to the RS. On the other hand, as discussed in Appendix B, the result of
optimization with respect to a number of vectors k < K may deviate from the RS if stimuli
are correlated. To ﬁnd the RS, we ﬁrst maximize I(v), and compare this maximum with
Ispike, which is estimated according to (4). If the difference exceeds that expected from
ﬁnite sampling corrections, we increment the number of directions with respect to which
information is simultaneously maximized.

3 Optimization algorithm

In this section we describe a particular algorithm we used to look for the most informative
directions in order to ﬁnd the relevant subspace. We make no claim that our choice of
the algorithm is most efﬁcient. However, it does give reproducible results for different
starting points and spike trains with differences taken to simulate neural noise. Overall,
choices for an algorithm are broader because the information I(v) as deﬁned by (7) is a
continuous function, whose gradient can be computed. We ﬁnd (see Appendix C for a
derivation)

∇vI =

dxPv(x) [hs|x, spikei − hs|xi] ·

d
dx

Pv(x|spike)
Pv(x)

,

#

"

Z

where

hs|x, spikei =

dDs sδ(x − s · v)P (s|spike),

1
P (x|spike) Z

and similarly for hs|xi. Since information does not change with the length of the vector,
we have v · ∇vI = 0, as also can be seen directly from Eq. (10).

5

(8)

(9)

(10)

(11)

white noise stimuli 

naturalistic stimuli 

500

 
)
 

e
k
p
s

i

I
/
I
(

P

250

(a)

(b)

(c)

0
10

−4

−3

10

−2

10

−1

10

0
10
 
spike

I/I

0.5

1

I/I

 
spike

3.5

2.5

1.5

3

2

1

0.5

0

0

Figure 1: The probability distribution of information values in units of the total informa-
tion per spike in the case of (a) uncorrelated binary noise stimuli, (b) correlated Gaussian
noise with power spectrum of natural scenes, and (c) stimuli derived from natural scenes
(patches of photos). The distribution was obtained by calculating information along 105
random vectors for a model cell with one relevant direction. Note the different scales in
the different panels.

As an optimization algorithm, we have used a combination of gradient ascent and
simulated annealing algorithms: successive line maximizations were done along the di-
rection of the gradient. During line maximizations, a point with a smaller value of infor-
mation was accepted according to Boltzmann statistics, with probability ∝ exp[(I(vi+1) −
I(vi))/T ]. The effective temperature T is reduced by factor of 1 − ǫT upon completion of
each line maximization. Parameters of the simulated annealing algorithm that were used
in the examples below are the starting temperature T0 = 1 and ǫT [we used ǫT = 0.05 for
a simple cell, and ǫT = 0.005 for a complex cell].

The problem of maximizing a function often is related to the problem of making a
good initial guess. It turns out, however, that the choice of a starting point is much less
crucial in cases where the stimuli are correlated. To illustrate this point we plot in Fig. 1
the probability distribution of information along random directions v both for white noise
and for naturalistic stimuli in a model with one relevant direction. For uncorrelated stim-
uli, not only is information equal to zero for a vector that is perpendicular to the relevant
subspace, but in addition the derivative is equal to zero. Since a randomly chosen vec-
tor has on average a small projection on the relevant subspace (compared to its length)
vr/|v| ∼

n/d, the corresponding information can be found by expanding in vr/|v|:

q

I ≈

v2
r
2|v|2

Z

dxPˆeir (x)

2

P ′
ˆeir (x)
Pˆeir (x) !

 

[hsˆer|spikei − hsˆeri]2

(12)

where vector v = vr ˆer + virˆeir is decomposed in its components inside and outside the RS,
r i/|v|2) =
respectively. The average information for a random vector is, therefore, ∼ (hv2
n/d.

6

In cases where stimuli are drawn from a Gaussian ensemble with correlations, an ex-
pression for the information values has a similar structure to (12). The statistical indepen-
dence in the stimulus space is obtained by transforming to the Fourier space and normal-
izing axes by a square root of the power spectrum S(k). In the new basis, both the vectors
{ˆe(i)} forming the RS and the randomly chosen vector v along which information is being
S(k). Thus, if we now substitute the dot product v2
evaluated are to be multiplied by
r
i (v ∗ ˆe(i))2, where
by a convolution weighted by the power spectrum

q

n

v ∗ ˆe(i) =

k v(k)ˆe(i)(k)S(k)

k v2(k)S(k)

P

k ˆe(i)2(k)S(k)

P

,

(13)

qP

qP

then Eq. (12) will describe information values along randomly chosen vectors v for corre-
lated Gaussian stimuli with the power spectrum S(k). Even though both vr and v(k) are
Gaussian variables with variance ∼ 1/D, the weighted convolution has not only a much
larger variance but is also strongly non–Gaussian [the non–Gaussian character is due to
k v2(k)S(k) in Eq. (13)]. As for the variance, it can be estimated
the normalizing factor
as < (v ∗ ˆe)2 >= 4π/ ln2 D, in cases where stimuli are taken as patches of correlated Gaus-
sian noise with the two-dimensional power spectrum S(k) = A/k2. The large values of
the weighted dot product v ∗ ˆe(i) result not only in signiﬁcant information values along a
randomly chosen vector, but also in large lengths of the derivative ∇I, which is no longer
dominated by noise, contrary to the case of uncorrelated stimuli. We ﬁnd that randomly
choosing one of the presented frames as a starting guess is sufﬁcient.

P

4 Results

We tested the scheme of looking for the most informative directions on model neurons
that respond to stimuli derived from natural scenes and sounds. As visual stimuli we
used scans across natural scenes, which were taken as black and white photos digitized
to 8 bits with no corrections for the camera’s light intensity transformation function. Some
statistical properties of the stimulus set are shown in Fig. 2. Qualitatively, they reproduce
the known results on the statistics of natural scenes (Ruderman & Bialek, 1994; Ruderman,
1994; Dong & Attick, 1995; Simoncelli & Olshausen, 2001). Most important properties
for this study are strong spatial correlations, as evident from the power spectrum S(k)
plotted in panel (b), and deviations of the probability distribution from a Gaussian one.
The non–Gaussian character can be seen in panel (c), where the probability distribution
of intensities is shown, and in panel (d) which shows the distribution of projections on a
Gabor ﬁlter [in what follows the units of projections, such as s(1), will be given in units of
the corresponding standard deviations]. Our goal is to demonstrate that even though the
correlations present in the ensemble are non–Gaussian, they can be successfully removed
from the estimate of vectors deﬁning the RS.

4.1 A model simple cell

Our ﬁrst example is based on the properties of simple cells found in the primary visual
cortex. A model phase and orientation sensitive cell has a single relevant direction ˆe(1)

7

(a) 

100

200

300

400

−2

10

0
10

(c) 

P(I) 

10

−4
−2

−1

(b) 

  
−π/ 

 S( k) 

 

0

  

π 
 
−π 
  

0
10
(d) 

−2

10

−4

10

P(s(1) ) 

200

400

600

 

0

  

π 
 

0

2
1
(I−<I> ) /σ(I)

−8−6−4−2 0 2 4 6 8

(s(1)−<s(1)> ) /σ(s(1) )

Figure 2: Statistical properties of the visual stimulus ensemble. Panel (a) shows one of the
photos. Stimuli would be 30x30 patches taken from the overall photograph. In panel (b)
we show the logarithm of the two-dimensional power spectrum of patches 256x256 pixels
in size; units of the Fourier variable are radians per pixel. (c) The probability distribution
of light intensity in units of the standard deviation σ(I). (d) The probability distribution of
projections between stimuli and a Gabor ﬁlter, also in units of the corresponding standard
deviation σ(s(1)).

shown in Fig. 3(a). A given stimulus s leads to a spike if the projection s(1) = s · ˆe(1)
reaches a threshold value θ in the presence of noise:

P (spike|s)
P (spike)

≡ f (s(1)) = hH(s(1) − θ + ξ)i,

(14)

where a Gaussian random variable ξ of variance σ2 models additive noise, and the func-
tion H(x) = 1 for x > 0, and zero otherwise. Together with the RF ˆe(1), the parameters θ
for threshold and the noise variance σ2 determine the input–output function.

When the spike–triggered average (STA), or reverse correlation function, is computed
from the responses to correlated stimuli, the resulting vector will be broadened due to
spatial correlations present in the stimuli (see Fig. 3b). For stimuli that are drawn from a
Gaussian probability distribution, the effects of correlations could be removed by multi-
plying vsta by the inverse of the a priori covariance matrix, according to the reverse cor-
vsta, Eq. (21). However this procedure tends to
relation method, ˆvGaussian est ∝ C
amplify noise. To separate errors due to neural noise from those due to the non–Gaussian
character of correlations, note that in a model, the effect of neural noise on our estimate
of the STA can be eliminated by averaging the presented stimuli weighted with the exact
ﬁring rate, as opposed to using a histogram of responses to estimate P (spike|s) from a

−1
a priori

8

(a) 

model filter

(b) 

"exact" STA

10

20

30

10

20

30

1.0

0.5

10

20

30

decorrelated STA

(c) 

10
reconstruction

20

30

(d) 

10

20

30

(e) 

20
10
(f)  P(spike|s  v

) 

max

30

 * I/I

                     

spike

0.5
                                   
 ^ 
o v  e
                 
1

0  

0
10

2
10

4
10
T−1 

0  

−3 −2 −1

0

1

2
s⋅ v

3

 
max

Figure 3: Analysis of a model simple cell with RF shown in (a). The “exact” spike-
triggered average vsta is shown in (b). Panel (c) shows an attempt to remove correlations
−1
vsta; (d) vector ˆvmax found by maximizing
according to reverse correlation method, C
a priori
information; (e) convergence of the algorithm according to information I(v) and projec-
tion ˆv · ˆe(1) as a function of inverse effective temperature T −1. (f) The probability of a spike
P (spike|s · ˆvmax) (crosses) is compared to P (spike|s(1)) used in generating spikes (solid
line). Parameters σ ≈ 0.31 and θ ≈ 1.84, both given in units of standard deviation of s(1),
which is also the units for x-axis in panel (f).

ﬁnite set of trials. We have used this “exact” STA,

vsta =

dDs sP (s|spike) =

dDsP (s) sP (spike|s),

(15)

Z

1
P (spike) Z

in calculations presented in Figs. 3 (b) and (c). Even with this noiseless STA (the equiv-
alent of collecting an inﬁnite data set), the standard decorrelation procedure is not valid
for non–Gaussian stimuli and nonlinear input–output functions (1), as discussed in detail
in Appendix A. The result of such a decorrelation in our example is shown in Fig. 3(c). It
clearly is missing some of the structure in the model ﬁlter, with projection ˆe(1)·ˆvGaussian est ≈
0.14. The discrepancy is not due to neural noise or ﬁnite sampling, since the “exact” STA
was decorrelated; the absence of noise in the exact STA also means that there would be no
justiﬁcation for smoothing the results of the decorrelation. The discrepancy between the
true receptive ﬁeld and the decorrelated STA increases with the strength of nonlinearity
in the input–output function.

10

20

30

10

20

30

  
1.0

9

1

0.95

e(1) ⋅ v

 
max

0.9

0.85

0.8

0.75
0

0.2

0.4

0.6

N−1

0.8
 
spike

1
−4

x 10

Figure 4: Projection of vector ˆvmax that maximizes information on RF ˆe(1) is plotted as
a function of the number of spikes. The solid line is a quadratic ﬁt in 1/Nspike, and the
dashed line is the leading linear term in 1/Nspike. This set of simulations was carried out
for a model visual neuron with one relevant direction from Fig. 3(a) and the input/output
function (14) with parameter values σ ≈ 0.61σ(s(1)), θ ≈ 0.61σ(s(1)). For this model neu-
ron, the linear approximation for the expected error is applicable for Nspike ∼

> 30, 000.

In contrast, it is possible to obtain a good estimate of the relevant direction ˆe(1) by max-
imizing information directly, see panel (d). A typical progress of the simulated annealing
algorithm with decreasing temperature T is shown in Fig. 3(e). There we plot both the
information along the vector, and its projection on ˆe(1). We note that while information I
remains almost constant, the value of projection continues to improve. Qualitatively this
is because the probability distributions depend exponentially on information. The ﬁnal
value of projection depends on the size of the data set, see below. In the example shown
in Fig. 3 there were ≈ 50, 000 spikes with average probability of spike ≈ 0.05 per frame,
and the reconstructed vector has a projection ˆvmax · ˆe(1) ≈ 0.9. Having estimated the RF,
one can proceed to sample the nonlinear input-output function. This is done by construct-
ing histograms for P (s · ˆvmax) and P (s · ˆvmax|spike) of projections onto vector ˆvmax found
by maximizing information, and taking their ratio, as in Eq. (2). In Fig. 3(f) we compare
P (spike|s · ˆvmax) (crosses) with the probability P (spike|s(1)) used in the model (solid line).

4.2 Estimated deviation from the optimal direction
When information is calculated from a ﬁnite data set, the vector v which maximizes I will
deviate from the true RF ˆe(1). The deviation δv = v−ˆe(1) arises because the probability dis-
tributions are estimated from experimental histograms and differ from the distributions
found in the limit on inﬁnite data size. For a simple cell, the quality of reconstruction can
be characterized by the projection v · ˆe(1) = 1 − 1
2δv2, where both v and ˆe(1) are normal-
ized, and δv is by deﬁnition orthogonal to ˆe(1). The deviation δv ∼ A−1∇I, where A is the
Hessian of information. Its structure is similar to that of a covariance matrix:

Aij =

dxP (x|spike)

(hsisj|xi − hsi|xihsj|xi).

(16)

1
ln 2 Z

d
dx

 

ln

P (x|spike)

2

P (x) !

10

When averaged over possible outcomes of N trials, the gradient of information is zero
for the optimal direction. Here in order to evaluate hδv2i = Tr[A−1h∇I∇I T iA−1], we need
to know the variance of the gradient of I. Assuming that the probability of generating
a spike is independent for different bins, we can estimate h∇Ii∇Iji ∼ Aij/(Nspike ln 2).
Therefore an expected error in the reconstruction of the optimal ﬁlter is inversely propor-
tional to the number of spikes. The corresponding expected value of projection between
the reconstructed vector and the relevant direction ˆe(1) is given by:

v · ˆe(1) ≈ 1 −

hδv2i = 1 −

1
2

Tr′[A−1]
2Nspike ln 2

,

(17)

′

means that the trace is taken in the subspace orthogonal to the model ﬁlter1.
where Tr
The estimate (17) can be calculated without knowledge of the underlying model, it is
∼ D/(2Nspike). We emphasize that the error estimate according to Eq. (17) is of the same
order as an expected error of the reverse correlation method when it is applied for Gaus-
′2(s(1))i]. Of course, if the
sian ensembles. The latter is given by (Tr[C −1] − C −1
reverse correlation method were to be applied to the non–Gaussian ensemble, the errors
would be larger. In Fig. 4 we show the result of simulations for various numbers of tri-
als, and therefore Nspike. The average projection of the normalized reconstructed vector
v on the RF ˆe(1) behaves initially as 1/Nspike (dashed line). For smaller data sets, in this
case Nspikes ∼
spikes become important for estimating the expected
errors of the algorithm. Happily these corrections have a sign such that smaller data sets
are more effective than one might have expected from the asymptotic calculation.

< 30, 000, corrections ∼ N −2

11 )/[2Nspikehf

4.3 A model complex cell

A sequence of spikes from a model cell with two relevant directions was simulated by
projecting each of the stimuli on vectors that differ by π/2 in their spatial phase, taken
to mimic properties of complex cells, as in Fig. 5. A particular frame leads to a spike
according to a logical OR, that is if either s(1) = s · ˆe(1), −s(1), s(2) = s · ˆe(2), or −s(2) exceeds
a threshold value θ in the presence of noise. Similarly to (14),

P (spike|s)
P (spike)

= f (s(1), s(2)) = hH(|s(1)| − θ − ξ1) ∨ H(|s(2)| − θ − ξ2)i ,

(18)

where ξ1 and ξ2 are independent Gaussian variables. The sampling of this input–output
function by our particular set of natural stimuli is shown in Fig. 5(c). As is well known,
reverse correlation fails in this case because the spike–triggered average stimulus is zero,
although with Gaussian stimuli the spike–triggered covariance method would recover
the relevant dimensions. Here we show that searching for maximally informative di-
mensions allows us to recover the relevant subspace even under more natural stimulus
conditions.

1 By deﬁnition δv1 = δv · ˆe(1) = 0, and therefore hδv2
Because ˆe(1) is an eigenvector of A with zero eigenvalue, A−1
take the trace in the subspace orthogonal to ˆe(1).

1i ∝ A−1

11 is to be subtracted from hδv2i ∝ Tr[A−1].
11 is inﬁnite. Therefore a proper treatment is to

11

Figure 5: Analysis of a model complex cell with relevant directions ˆe(1) and ˆe(2) shown in
(a) and (b). Spikes are generated according to an “OR” input-output function f (s(1), s(2))
with the threshold θ ≈ 0.61σ(s(1)) and noise standard deviation σ = 0.31σ(s(1)). Panel
(c) shows how the input-output function is sampled by our ensemble of stimuli. On the
right, we show vectors v1 and v2 found by maximizing information I(v1, v2).

We start by maximizing information with respect to one direction. Contrary to the
result Fig. 3(e) for a simple cell, one optimal direction recovers only about 60% of the total
information per spike [Eq. (4)]. Perhaps surprisingly, because of the strong correlations
in natural scenes, even a projection onto a random vector in the D ∼ 103 dimensional
stimulus space has a high probability of explaining 60% of total information per spike, as
can be seen in Fig. 1. We therefore go on to maximize information with respect to two
directions. Vectors v1 and v2 that maximize I(v1, v2) are not orthogonal, and are also
rotated with respect to ˆe(1) and ˆe(2). However, the quality of reconstruction, as well as the
value of information I(v1, v2), is independent of a particular choice of basis with the RS.
The appropriate measure of similarity between the two planes is the dot product of their
normals. In the example of Fig. 5, ˆn(ˆe(1),ˆe(2)) · ˆn(v1,v2) ≈ 0.8 2. We make vectors v1 and v2
orthogonal to each others upon completion of the algorithm.

Maximizing information with respect to two directions requires a signiﬁcantly slower
cooling rate, and consequently longer computational times. However, the expected error
in the reconstruction, 1 − ˆn(ˆe(1),ˆe(2)) · ˆn(v1,v2), follows a N −1
spike behavior, similarly to (17), and
is roughly twice that for a simple cell given the same number of spikes.

2When information is maximized with respect to one vector, the natural length scale over which I(v)
has to be explored is provided by the length of the vector |v| itself, since information is only a function of
direction. When maximizing information with respect to several directions, an analogous length scale is
given by the square root of the determinant

(v1v1)
(vnv1)

...
...

(v1vn)
(vnvn)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

of all pairwise products between the trial vectors vi. For example in the case of n = 2, information I(v1, v2)
depends only on the normal to the plane formed by trial vectors v1 and v2. The length of the normal is
equal to a square root of the determinant in Eq. (19).

(19)

12

4.4 A model auditory neuron with one relevant direction
Because stimuli s are treated as vectors in an abstract space, the method of looking for the
most informative directions can be applied equally well to auditory as to visual neurons.
Here we illustrate the method by considering a model auditory neuron with one relevant
direction, which is shown in Fig. 6(c) and is taken to mimic the properties of cochlear
neurons. The model neuron is probed by two ensembles of naturalistic stimuli: one is
a recording of a native Russian speaker reading a piece of Russian prose, and the other
one is a recording of a piece of English prose read by a native English speaker. Both of
the ensembles are non–Gaussian and exhibit amplitude distributions with long, nearly
exponential tails, cf. Fig. 6(a), which are qualitatively similar to those of light intensities
in natural scenes (Voss & Clarke, 1975; Ruderman, 1994). However, the power spectrum
is different in the two cases, as can be seen in Fig. 6(b). The differences in the correlation
structure in particular lead to different STAs across the two ensembles, cf. panel (d). Both
of the STAs also deviate from the model ﬁlter shown in panel (c).

Despite differences in the probability distributions P (s), it is possible to recover the
relevant direction of the model neuron by maximizing information. In panel (c) we show
the two most informative vectors found by running the algorithm for the two ensembles,
and replot the model ﬁlter from (a) to show that the three vectors overlap almost perfectly.
The progress of the algorithm in the case of a stimulus being a piece of English prose is
shown in panel (d), both in terms of the percentage of the total information explained by
the optimal direction, and its projection on the relevant direction ˆe(1). Thus different non–
Gaussian correlations can be successfully removed to obtain an estimate of the relevant
direction. Therefore if the most informative vector changes with the stimulus ensemble,
this can be interpreted as caused by adaptation to the probability distribution.

5 Summary

Features of the stimulus that are most relevant for generating the response of a neuron
can be found by maximizing information between the sequence of responses and the
projection of stimuli on trial vectors within the stimulus space. Calculated in this man-
ner, information becomes a function of direction in stimulus space. Those directions that
maximize the information and account for the total information per response of interest
span the relevant subspace. The method allows multiple directions to be found. The re-
construction of the relevant subspace is done without assuming a particular form of the
input–output function. It can be strongly nonlinear within the relevant subspace, and
is estimated from experimental histograms for each trial direction independently. Most
importantly, this method can be used with any stimulus ensemble, even those that are
strongly non–Gaussian as in the case of natural images. We have illustrated the method
on model neurons responding to natural scenes and sounds.

13

0

10

Amplitude (units of RMS) 

10

5
20
frequency (kHz) 

15

(a) 

100

S(ω)

(b) 

1

2 

1 

2 

(c) 

10

1

0.1

0.01

0.001

0.1  

 

0    

−0.1 

5

10

t(ms) 

(e) 

0

1

5

10

t(ms) 

(d) 

2 

1

(f) 

0
10

 
)
e
d
u
t
i
l

p
m
A
P

(

 

−2

10

−4

10

10

−6
−10

0.1  

0    

−0.1 

0.1  

0    

−0.1 

 

 

 

 

0

0

                             

 * I/I
spike
o    v  e
1

5

10

t(ms) 

0
10

2
10

4
10
T−1 

6
10

Figure 6: A model auditory neuron is probed by two natural ensembles of stimuli: a piece
of English prose (1) and a piece of of Russian prose (2) . The size of the stimulus ensemble
was the same in both cases, and the sampling rate was 44.1 kHz. (a) The probability distri-
bution of the sound pressure amplitude in units of standard deviation for both ensembles
is strongly non–Gaussian. (b) The power spectra for the two ensembles. (c) The relevant
direction of the model neuron, of dimensionality D = 500. (d) The STA is broadened
in both cases, but differs among the two cases due to differences in the power spectra
of the two ensembles. (c) Vectors that maximize information for either of the ensembles
overlap almost perfectly with each other, and with the model ﬁlter from (a), which is also
replotted here from (c). (d) The progress of the algorithm of looking for the most infor-
mative directions according the I/Ispike (stars) and projection onto ˆe(1) (open circles). The
input-output function had parameter values σ ≈ 0.9σ(s(1)) and θ ≈ 1.8σ(s(1)).
Acknowledgments

We thank K. D. Miller for many helpful discussions. Work at UCSF was supported in
part by the Sloan and Swartz Foundations and by a training grant from the NIH. Our
collaboration began at the Marine Biological Laboratory in a course supported by grants
from NIMH and the Howard Hughes Medical Institute.

A Limitations of the reverse correlation method

Here we examine what sort of deviations one can expect when applying the reverse corre-
lation method to natural stimuli even in the model with just one relevant direction. There

14

are two factors that, when combined, invalidate the reverse correlation method: The non–
Gaussian character of correlations and the nonlinearity of the input/output function. In
its original formulation (de Boer & Kuyper, 1968), the neuron is probed by white noise
and the relevant direction ˆe(1) is given by the STA ˆe(1) ∝ hsr(s)i. If the signals are not
white, i.e. the covariance matrix Cij = hsisji is not a unit matrix, then the STA is a broad-
ened version of the original ﬁlter ˆe(1). This can be seen by noting that for any function
F (s) of Gaussian variables {si} the identity holds:

hsiF (s)i = hsisjih∂sj F (s)i.

When property (20) is applied to the vector components of the STA, hsir(s)i = Cijh∂sj r(s)i.
Since we work within the assumption that the ﬁring rate is a (nonlinear) function of pro-
jection onto one ﬁlter ˆe(1), r(s) = r(s(1)), the later average is proportional to the model
j hr′(s(1))i. Therefore, we arrive at the prescription of the reverse
ﬁlter itself, h∂sj ri = ˆe(1)
correlation method
i ∝ C −1
ˆe(1)

ij hsjr(s)i.

(21)

The Gaussian property is necessary in order to represent the STA as a convolution of
the covariance matrix Cij of the stimulus ensemble and the model ﬁlter. To understand
how the reconstructed vector obtained according to Eq. (21) deviates from the relevant
direction, we consider weakly non–Gaussian stimuli, with the probability distribution

PnG(s) =

P0(s)eǫH1(s),

1
Z

where P0(s) is the Gaussian probability distribution with covariance matrix C, and the
normalization factor Z = heǫH1(s)i. The function H1 describes deviations of the probability
distribution from Gaussian, and therefore we will set hsiH1i = 0 and hsisjH1i = 0, since
these averages can be accounted for in the Gaussian ensemble. In what follows we will
keep only the ﬁrst order terms in perturbation parameter ǫ. Using the property (20), we
ﬁnd the STA to be given by

hsirinG = hsisji [h∂jri + ǫ(h∂j(rH1)i0 − hrihH1i)] ,

where averages are taken with respect to the Gaussian distribution. Similarly, the covari-
ance matrix Cij evaluated with respect to the non–Gaussian ensemble is given by:

Cij =

hsisjeǫH1i = hsisji + ǫhsiskihsj∂k(H1)i

1
Z

so that to the ﬁrst order in ǫ, hsisji = Cij − ǫCikhsj∂k(H1)i. Combining this with Eq. (23),
we get

hsirinG = const × Cij ˆe(1)

j + ǫCijh

r − s(1)hr′i
(cid:16)

(cid:17)

∂j(H1)i.

The second term in (25) prevents the application of the reverse correlation method for
non–Gaussian signals. Indeed, if we multiply the STA (25) with the inverse of the a pri-
ori covariance matrix Cij according to the reverse correlation method (21), we no longer

(20)

(22)

(23)

(24)

(25)

15

model filter e(1)

P(spike|s(1)) 

1

0.5

(a) 

10

20

30

example frame

10 20 30

0
−0.5
"exact" STA

0

0.5
decorrelated 
 "exact"  STA

(b) 

(c) 

10

20

30

10

20

30

10

20

30

10

20

30

10

20

30

10

20

30

10 20 30

10 20 30

10 20 30

10 20 30

10 20 30

10 20 30

Figure 7: The non–Gaussian character of correlations present in natural scenes invalidates
the reverse correlation method for neurons with a nonlinear input-output function. Here,
a model visual neuron has one relevant direction ˆe(1) and the nonlinear input/output
function, shown in (a). The “exact” STA is used (15) to separate effects of neural noise
from alterations introduced by the method. The decorrelated “exact” STA is obtained by
multiplying the “exact” STA by the inverse of the covariance matrix, according to (21). (b)
Stimuli are taken from a correlated Gaussian noise ensemble. The effect of correlations in
STA can be removed according to (21). When patches of photos are taken as stimuli (c)
for the same model neuron as in (b), the decorrelation procedure gives an altered version
of the model ﬁlter. The two stimulus ensembles have the same covariance matrix.
obtain the RF ˆe(1). The deviation of the obtained answer from the true RF increases with
H1, which describes the deviation of the probability distribution from Gaussian. Since
natural stimuli are known to be strongly non–Gaussian, this makes the use of the reverse
correlation problematic when analyzing neural responses to natural stimuli.

The difference in applying the reverse correlation to stimuli drawn from a correlated
Gaussian ensemble vs. a non–Gaussian one is illustrated in Figs. 7 (b) and (c). In the
ﬁrst case, shown in (b), stimuli are drawn from a correlated Gaussian ensemble with the
covariance matrix equal to that of natural images. In the second case, shown in (c), the
patches of photos are taken as stimuli. The STA is broadened in both cases. Even though
the two-point correlations are just as strong in the case of Gaussian stimuli as they are in
the natural stimuli ensemble, Gaussian correlations can be successfully removed from the
STA according to Eq. (21) to obtain the model ﬁlter. On the contrary, an attempt to use
reverse correlation with natural stimuli results in an altered version of the model ﬁlter.
We reiterate that the apparent noise in the decorrelated vector is not due to neural noise
or ﬁnite datasets, since the “exact” STA has been used (15) in all calculations presented in

16

Figs. 7 and 8.

The reverse correlation gives the correct answer for any distribution of signals if the
probability of generating a spike is a linear function of si, since then the second term in
Eq. (25) is zero. In particular, a linear input-output relation could arise due to a neural
noise whose variance is much larger than the variance of the signal itself. This point is
illustrated in Figs. 8 (a), (b), and (c), where the reverse correlation method is applied to
a threshold input-output function in the limit low, moderate, and high signal-to-noise
ratios. For small signal-to-noise ratios where the noise standard deviation is similar to
that of projections s(1), the threshold nonlinearity in the input-output function is masked
by noise, and is effectively linear. In this limit, the reverse correlation can be applied with
the exact STA. However, for experimentally calculated STA at low signal-to-noise ratios
the decorrelation procedure results in strong noise ampliﬁcation. On the other hand, at
higher signal-to-noise ratios decorrelation fails due to the nonlinearity of the input-output
function in accordance with (25).
P(spike|s(1))

 "exact" STA

decorrelated 
"exact" STA  

1  

(a) 

0.5

0  

0

1  

0  

0

1  

0  

0

(b) 

0.5

(c) 

0.5

0.5

1

10

20

30

10

20

30

0.5

1

10

20

30

10

20

30

10

20

30

 

10

20

30

10

20

30

 

10

20

30

 

10

20

30

 

10

20

30

17

0.5

1

10

20

30

10

20

30

Figure 8: Application of the reverse correlation method to a model visual neuron with
one relevant direction ˆe(1) and a threshold input/output function of decreasing values
of noise variance σ/σ(s(1))s ≈ 6.1, 0.61, 0.06 in (a), (b), and (c) respectively. The model
P (spike|s(1)) becomes effectively linear when signal-to noise ratio is small. The reverse
correlation can be used together with natural stimuli, if the input-output function is linear.
Otherwise, the deviations between the decorrelated STA and the model ﬁlter increase
with nonlinearity of P (spike|s(1)).

B Maxima of I(v): what do they mean?

The relevant subspace of dimensionality K can be found by maximizing information si-
multaneously with respect to K vectors. The result of maximization with respect to a
number of vectors that is less than the true dimensionality of the relevant subspace may
produce vectors which have components in the irrelevant subspace. This happens only
in the presence of correlations in stimuli. As an illustration, we consider the situation
where the dimensionality of the relevant subspace K = 2, and vector ˆe(1) describes the
most informative direction within the relative subspace. We show here that even though
the gradient of information is perpendicular to both ˆe(1) and ˆe(2), it may have components
outside the relevant subspace. Therefore vector vmax that corresponds to the maximum of
I(v) will then lie outside the relevant subspace.

∇I(ˆe(1)) =

dx1P (x1)

Z

d
dx1

P (x1|spike)
P (x1)

(hs|x1, spikei − hs|x1i),

x1 ≡ s(1)

(26)

We could rewrite the conditional averages hs|x1i =
hs|x1, spikei =
s(2):

R

dx2f (x1, x2)P (x1, x2)hs|x1, x2i/P (x1|spike) with convention x1 ≡ s(1), x2 ≡
R

dx2P (x1, x2)hs|x1, x2i/P (x1) and

∇I(ˆe(1)) =

dx1dx2P (x1, x2)hs|x1, x2i

Z

P (spike|x1, x2) − P (spike|x1)
P (spike)

d
dx1

ln

P (x1|spike)
P (x1)

. (27)

Because we assume that the direction ˆe(1) is the most informative within the relevant
subspace, ˆe(1)∇I = ˆe(2)∇I = 0, so that the integral in (27) is zero when hs|s(1), s(2)i =
s1,2, which is certainly true for uncorrelated stimuli. However, the gradient may have
components outside the relevant subspace in directions in which hs|s(1), s(2)i is not a linear
function of s(1) and s(2). By looking for a maximum of information we will therefore be
driven outside the relevant subspace. The deviation of vmax from the relevant subspace is
also proportional to the strength of the dependence on the second parameter s(2), because
of the factor [P (s(1), s(2)|spike)/P (s(1), s(2)) − P (s(1)|spike)/P (s(1))] in the integrand.

C The gradient of information

According to the expression (7), the information I(v) depends on the vector v only through
the probability distributions Pv(x) and Pv(x|spike). Therefore we can express the gradient
of information in terms of gradients of those probability distributions:

dx

∇vI =

1
ln 2 Z
where we took into account that
gradients of the probability distributions, we argue that,

Pv(x|spike)
Pv(x)

∇v(Pv(x|spike)) −

ln
"

R

Pv(x|spike)
Pv(x)

∇v(Pv(x))

,

(28)

#

dxPv(x|spike) = 1 and does not change with v. To ﬁnd

∇vPv(x) = ∇v

dsP (s)δ(x − s · v)

= −

dsP (s)sδ′(x − s · v) =

[p(x)hs|xi] ,

(29)

(cid:20)Z

(cid:21)

Z

d
dx

18

and analogously for Pv(x|spike):

∇vPv(x|spike) =

[p(x|spike)hs|x, spikei] .

d
dx

(30)

Substituting expressions (29) and (30) into Eq. (28) and integrating once by parts we ob-
tain:

∇vI =

dxPv(x) [hs|x, spikei − hs|xi] ·

Z

d
dx

Pv(x|spike)
Pv(x)

,

#

"

which is the expression (10) of the main text.

References

Barlow, H. (1961). Possible principles underlying the transformations of sensory images.
In Sensory Communication, edited by W. Rosenblith. MIT Press, Cambridge, 217–234.

Barlow, H. (2001). Redundancy reduction revisited. Network: Comput. Neural Syst., 12,

241–253.

179.

Bialek, W. (2002). Thinking about the brain. In Physics of Biomolecules and Cells, edited by
H. Flyvbjerg, F. J ¨ulicher, P. Ormos, & F. David. EDP Sciences, Les Ulis; Springer-Verlag,
Berlin, 485–577. See also physics/0205030.3

Bialek, W., & de Ruyter van Steveninck, R. R. (2002). Features and dimensions: Motion

estimation in ﬂy vision. In preparation.

de Boer, E., & Kuyper, P. (1968). Triggered correlation. IEEE Trans. Biomed. Eng., 15, 169–

Brenner, N., Bialek, W., & de Ruyter van Steveninck, R. R. (2000a). Adaptive rescaling

maximizes information transmission. Neuron, 26, 695–702.

Brenner, N., Strong, S. P., Koberle, R., Bialek, W., & de Ruyter van Steveninck, R. R.
(2000b). Synergy in a neural code. Neural Computation, 12, 1531–1552. See also
physics/9902067.

Chichilnisky, E. J. (2001). A simple white noise analysis of neuronal light responses. Net-

work: Comput. Neural Syst., 12, 199–213.

Dimitrov, A. G., & Miller, J. P. (2001). Neural coding and decoding: communication chan-

nels and quantization. Network: Comput. Neural Syst., 12, 441–472.

Dong, D. W., & Atick, J. J. (1995). Statistics of natural time-varying images. Network:

Comput. Neural Syst., 6, 345–358.

3Where available we give references to the physics e–print archive, which may be found at
http://arxiv.org/abs/*/*; thus Bialek (2002) is available at http://arxiv.org/abs/physics/0205030. Pub-
lished papers may differ from the versions posted to the archive.

19

Fairhall, A. L., Lewen, G. D., Bialek, W., & de Ruyter van Steveninck, R. R. (2001). Efﬁ-

ciency and ambiguity in an adaptive neural code. Nature, 787–792.

Kara, P., Reinagel, P., & Reid, R. C. (2000). Low response variability in simultaneously

recorded retinal, thalamic, and cortical neurons. Neuron, 27, 635–646.

Lewen, G. D., Bialek, W., & de Ruyter van Steveninck, R. R. (2001). Neural coding
of naturalistic motion stimuli. Network: Comput. Neural Syst., 12, 317–329. See also
physics/0103088.

Mainen, Z. F., & Sejnowski, T. J. (1995). Reliability of spike timing in neocortical neurons.

Science, 268, 1503–1506.

Rieke, F., Bodnar, D. A., & Bialek, W. (1995). Naturalistic stimuli increase the rate and
efﬁciency of information transmission by primary auditory afferents. Proc. R. Soc. Lond.
B, 262, 259–265.

Rieke, F., Warland, D., de Ruyter van Steveninck, R. R., & Bialek, W. (1997). Spikes: Ex-

ploring the neural code. MIT Press, Cambridge.

Ringach, D. L., Hawken, M. J., & Shapley, R. (2002). Receptive ﬁeld structure of neurons
in monkey visual cortex revealed by stimulation with natural image sequences. Journal
of Vision, 2, 12–24.

Ruderman, D. L. (1994). The statistics of natural images. Network: Compt. Neural Syst., 5,

517–548.

Ruderman, D. L., & Bialek, W. (1994). Statistics of natural images: scaling in the woods.

Phys. Rev. Lett., 73, 814–817.

de Ruyter van Steveninck, R. R., & Bialek, W. (1988). Real-time performance of a
movement-sensitive neuron in the blowﬂy visual system: coding and information
transfer in short spike sequences. Proc. R. Soc. Lond. B, 265, 259–265.

de Ruyter van Steveninck, R. R., Borst, A., & Bialek, W. (2001). Real time encoding of mo-
tion: Answerable questions and questionable answers from the ﬂy’s visual system. In
Processing Visual Motion in the Real World: A Survey of Computational, Neural and Ecolog-
ical Constraints, edited by J. M. Zanker & J. Zeil. Springer–Verlag, Berlin, 279–306. See
also physics/0004060.

de Ruyter van Steveninck, R. R., Lewen, G. D., Strong, S. P., Koberle, R., & Bialek, W.
(1997). Reproducibility and variability in neural spike trains. Science, 275, 1805–1808.

Sen, K., Theunissen, F. E., & Doupe, A. J. (2001). Feature analysis of natural sounds in the

songbird auditory forebrain. J. Neurophysiol., 86, 1445–1458.

Simoncelli, E., & Olshausen, B. A. (2001). Natural image statistics and neural representa-

tion. Annu. Rev. Neurosci., 24, 1193–1216.

20

Smirnakis, S. M., Berry, M. J., Warland, D. K., Bialek, W., & Meister, M. (1996). Adaptation

of retinal processing to image contrast and spatial scale. Nature, 386, 69–73.

Strong, S. P., Koberle, R., de Ruyter van Steveninck, R. R., & Bialek, W. (1998). Entropy
and information in neural spike trains. Phys. Rev. Lett., 80, 197–200. See also cond-
mat/9603127.

Theunissen, F. E., Sen, K., & Doupe, A. J. (2000). Spectral-temporal receptive ﬁelds of
nonlinear auditory neurons obtained using natural sounds. J. Neurosci., 20, 2315–2331.

Tishby, N., Pereira, F. C., & Bialek, W. (1999). The information bottleneck method. In Pro-
ceedings of the 37th Allerton Conference on Communication, Control and Computing, edited
by B. Hajek & R. S. Sreenivas. University of Illinois, 368–377. See also physics/0004057.

Treves, A., & Panzeri, S. (1995). The upward bias in measures of information derived

from limited data samples. Neural Comp., 7, 399–407.

von der Twer, T., & Macleod, D. I. A. (2001). Optimal nonlinear codes for the perception

of natural colours. Network: Comput. Neural Syst., 12, 395–407.

Vickers, N. J., Christensen, T. A., Baker, T., & Hildebrand, J. G. (2001). Odour-plume

dynamics inﬂuence the brain’s olfactory code. Nature, 410, 466–470.

Vinje, W. E., & Gallant, J. L. (2000). Sparse coding and decorrelation in primary visual

cortex during natural vision. Science, 287, 1273–1276.

Vinje, W. E., & Gallant, J. L. (2002). Natural stimulation of the nonclassical receptive ﬁeld

increases information transmission efﬁciency in V1. J. Neurosci., 22, 2904–2915.

Voss, R. F., & Clarke, J. (1975). ’1/f noise’ in music and speech. Nature, 317–318.

21


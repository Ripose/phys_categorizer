1
0
0
2
 
t
c
O
 
6
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
4
7
0
0
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Information loss in an optimal maximum likelihood decoding

In´es Samengo∗
Centro At´omico Bariloche and Instituto Balseiro
(8400) San Carlos de Bariloche, R´ıo Negro, Argentina

The mutual information between a set of stimuli and the elicited neural responses is compared
to the corresponding decoded information. The decoding procedure is presented as an artiﬁcial
distortion of the joint probabilities between stimuli and responses. The information loss is quantiﬁed.
Whenever the probabilities are only slightly distorted, the information loss is shown to be quadratic
in the distortion

PACS numbers: 07.05.Mh,87.10.+e,87.19.Dd,89.70.+c

Understanding the way external stimuli are repre-
sented at the neuronal level is one central challenge in
neuroscience. An experimental approach to this end (Op-
tican and Richmond 1987, Eskandar et al. 1992, Tov´ee
et al. 1993, Kjaer et al. 1994, Heller et al. 1995, Rolls
et al. 1996, Treves et al. 1996, Rolls et al. 1997, Treves
1997, Rolls and Treves 1998, Rolls et al. 1998) consists
in choosing a particular set of stimuli s ∈ S which can
be controlled by the experimentalist, and exposing these
stimuli to a subject whose neural activity is being regis-
tered. The set of neural responses r ∈ R is then deﬁned
as the whole collection of recorded events. It is up to the
researcher to decide which entities in the recorded signal
are considered as events r. For example, r can be de-
ﬁned as the ﬁring rate in a ﬁxed time window, or as the
time diﬀerence between two consecutive spikes, or the k
ﬁrst principal components of the time variation of the
recorded potentials in a given interval, and so forth.

Once the stimulus set S and the response set R have
been settled, the joint probabilities P (r, s) may be esti-
mated from the experimental data. This is usually done
by measuring the frequency of the joint occurrence of
stimulus s and response r, for all s ∈ S and r ∈ R. The
mutual information between stimuli and responses reads
(Shannon 1948)

I =

P (r, s) log2

s
X

r
X

P (r, s)
P (r)P (s)

,

(cid:21)

(cid:20)

where

P (r) =

P (r, s)

s
X

P (s) =

P (r, s).

r
X
The mutual information quantiﬁes how much can be
learned about the identity of the stimulus shown just
by looking at the responses. Accordingly, and since I
is symmetrical in r and s, its value is also a measure of
the amount of information that the stimuli give about
the responses. From a theoretical point of view, I is
the most appealing quantity characterizing the degree of
correlation between stimuli and responses that can be de-
ﬁned. This stems from the fact that I is the only additive

(1)

(2)

(3)

functional of P (r, s) ranging from zero (for uncorrelated
variables) up to the entropy of stimuli or responses (for
a deterministic one to one mapping) (Fano 1961, Cover
and Thomas 1991).

However, even if formally sound, the mutual informa-
tion has a severe drawback when dealing with experimen-
tal data. Many times, and speciﬁcally when analyzing
data of multi-unit recordings, the response set R is quite
large, its size increasing exponentially with the number
of neurons sampled. Therefore, the estimation of P (r, s)
from the experimental frequencies may be far from accu-
rate, specially when recording from the vertebrate cor-
tex, where there are long time scales in the variability
and statistical structure of the responses. The mutual
information I, being a non linear function of the joint
probabilities, is extremely sensitive to the errors that
may be involved in their measured values. As derived
in Treves and Panzeri (1995), Panzeri and Treves (1996)
and Golomb et al. (1997), the mean error in calculat-
ing I from the frequency table of events r and s is linear
in the size of the response set. This analytical result
has been obtained under the assumption that diﬀerent
responses behave independently. Although there are sit-
uations where such a condition does not hold (Victor and
Purpura, 1997) it is widely accepted that the bias grows
rapidly with the size of the response set.

Therefore, a common practice when dealing with large
response sets is to calculate the mutual information not
between S and R, but between the stimuli and another
set T each of whose elements t is a function of the true re-
sponse r, that is, t = t(r) (Treves 1997, Rolls and Treves
1998). It is easy to show that if the mapping between r
and t is one to one, then the mutual information between
S and R is the same as the one between S and T . How-
ever, for one to one mappings, the number of elements in
T is the same as in R. A wiser procedure is to choose
a set T that is large enough not to lose the relevant in-
formation, but suﬃciently small as to avoid signiﬁcant
limited sampling errors. One possibility is to perform
a decoding procedure (Gochim et al. 1994, Rolls et al.
1996, Victor and Purpura 1996, Rolls and Treves 1998).
In this case, T is taken to coincide with S. To make
this correspondence explicit, the set T will be denoted

by S′ and its elements t by s′. Each s′ in S′ is taken
to be a function of r, and is called the predicted stim-
ulus of response r. As stated in Panzeri et al. (1999),
this choice for T is the smallest that could potentially
preserve the information of the identity of the stimulus.
The data processing theorem (Cover and Thomas, 1991)
states that since s′ is a function of r alone, and not of
the true stimulus s eliciting response r, the information
about the real stimulus can only be lost and not created
by the transformation from r → s′. Therefore, the true
information I is always at least as large as the decoded
information ID, the latter being the mutual information
between S and S′[1]. In order to have I and ID as close as
possible, it is of course necessary to choose the best s′ for
every r. The procedure consists in identifying which of
the stimuli was most probably shown, for every elicited
response. The conditional probability of having shown
stimulus s given that the response was r reads

P (s|r) =

P (r, s)
P (r)

.

(4)

(5)

Therefore, the stimulus that has most likely elicited

response r is

s′(r) = max

s

P (s|r) = max

P (r, s).

s

(5), a mapping r → s′

By means of Eq.
is estab-
lished: each response has its associated maximum likeli-
hood stimulus. Equation (4) provides the only deﬁnition
of P (s|r) that strictly follows Bayes’ rule, so in this case,
the decoding is called optimal. There are other alterna-
tive ways of deﬁning P (s|r) (Georgopoulos et al. 1986,
Wilson and McNaughton 1993, Seung and Sompolinsky
1993, Rolls et al. 1996) some of which have the appealing
property of being simple enough to be plausibly carried
out by downstream neurons themselves. The purpose of
this letter, however, is to quantify how much informa-
tion is lost when passing from r to s′ using an optimal
maximum likelihood decoding procedure.

In general, there are several r associated with a given
s′. One may therefore partition the response space R
in separate classes C(s) = {r/s′(r) = s}, one class for
every stimulus. The number of responses in class s′ is
Ns′ . Of course, some classes may be empty. Here, the
assumption is made that each r belongs to one and only
class (that is, Eq. (5) has a unique solution).

2

(8)

Clearly, with these deﬁnitions the decoded information

ID =

P (s′, s) log2

s
X

s′
X

P (s′, s)
P (s′)P (s)

(cid:20)

(cid:21)

may be calculated, and has, in fact, been used in several
experimental analyses (Rolls et al. 1996, Treves 1997,
Rolls and Treves 1998, Panzeri et al. 1999). However, up
to date, no rigorous relationship between I and ID has
been established. The derivation of such a relationship
is the main purpose here.

When performing a decoding procedure, r is replaced
by s′. Such a mapping allows the calculation of P (s′, s),
after which any additional structure, which may even-
tually have been present in P (r, s), is neglected. For
example, if two responses r1 and r2 encode the same
stimulus s′ it becomes irrelevant whether, for a given s,
P (r1, s) is much bigger that P (r2, s) or, on the contrary,
P (r1, s) ≈ P (r2, s). The only thing that matters is the
value of the sum of the two: their global contribution to
P (s′, s). As a consequence, it seems natural to consider
the detailed variation of P (r, s) within each class, when
estimating the information lost in the decoding.

In this spirit, and aiming at quantizing such a loss of

information, P (r, s) is written as

P (r, s) =

+ ∆(r, s),

(9)

P [s′(r), s]
Ns′(r)

where ∆(r, s) = P (r, s) − P [s′(r), s]/Ns′(r). Thus, the
joint probability P (r, s), which in principle may have
quite a complicate shape in R space, is separated into two
terms. The ﬁrst one is ﬂat inside every single class C(s′),
and the second is whatever needed to re-sum P (r, s). It
should be noticed that

∆(r, s) = 0,

(10)

r∈C(s′)
X

for all s. Summing Eq. (9) in s,

P (r) =

+ ∆(r),

(11)

where

∆(r) =

∆(r, s),

(12)

P [s′(r)]
Ns′(r)

s
X

The joint probability of showing stimulus s and decod-

and

ing stimulus s′(r) reads

P (s′, s) =

P (r, s),

(6)

r∈C(s′)
X

and the overall probability of decoding s′,

∆(r) = 0.

(13)

Replacing Eqs. (9) and (11) in the mutual information
(1), one arrives at

P (s′) =

P (s′, s) =

P (r).

(7)

s
X

r∈C(s′)
X

I = ID +

P (r, s) log2

(14)

P (r, s)
Q(r, s)

,

(cid:21)

(cid:20)

r∈C(s′)
X

r
X

s
X

3

where

Q(r, s) =

+ ∆(r)

P [s′(r), s]
Ns′

P [s′(r), s]
P (s′)

Eq.
show that

(15)

Next, by making a second order Taylor expansion of
(14) in the distorsions ∆(r, s) and ∆(r) one may

P (s′, s)

E(s′, s)
2 ln 2

+ O(∆2),

(19)

is a properly deﬁned distribution, since it can be shown
to be normalized and non-negative. The term in the right
of Eq. (14) is the Kullback-Leibler divergence (Kullback
1968) between the distributions P and Q, which is guar-
anteed to be non negative. This conﬁrms the intuitive
result ID ≤ I, the equality being only valid when

I = ID +

s
X

s′
X

where

E(s′, s) =

1
Ns′

∆(r)P [s′(r), s] = ∆(r, s)P [s′(r)],

(16)

for all r and s.

Equation (14) states the quantitative diﬀerence be-
tween the full and the decoded information, and is the
main result of this letter. The amount of lost information
is therefore equal to the informational distance between
the original probability distribution P (r, s) and a new
function Q(r, s). It can be easily veriﬁed that

ID =

Q(r, s) log2

(17)

Q(r, s)
Q(r)Q(s)

,

(cid:21)

(cid:20)

s
X

r
X

where

where

r∈C(s′) "(cid:18)
X

∆(r, s)
P (s′, s)/Ns′

2

−

(cid:19)

(cid:18)

∆(r)
P (s′)/Ns′

2

#

(cid:19)
(20)

Therefore, in the small ∆ limit, the diﬀerence between I
and ID is quadratic in the distortions ∆(r, s) and ∆(r).
This means that if in a given situation these quantities
are guaranteed to be small, then the decoded information
will be a good estimate of the full information. Equation
(20) is equivalent to

E(s′, s) =

P (r, s)
P (s′, s)/Ns′

2

−

(cid:19)

(cid:18)

P (r)
P (s′)/Ns′

2

(cid:19)

+C(s′)

*(cid:18)

,

(21)

Q(r) =

Q(r, s) = P (s),

Q(s) =

Q(r, s) = P (r).

(18)

s
X

r
X

Therefore, the decoded information can be interpreted
as a full mutual information between the stimuli and the
responses, but with a distorted probability distribution
Q(r, s). In this context, the diﬀerence I − ID is no more
than the distance between the true distribution P (r, s)
and the distorted one Q(r, s).

When is Eq. (16) fulﬁlled? Surely, if there is at most
one response in each class, ∆ is always zero, and I = ID.
Also, if P (r, s) is already ﬂat in each class, there is no in-
formation loss. However, if P (r, s) is not ﬂat inside every
class, but obeys the condition P (r, s) = Ps′ (r)P (s′, s) for
a suitable P (s′, s) and some function Ps′ (r) that sums up
to unity within C(s′), one can easily show that Eq. (16)
holds. Just notice that this case implies that if r1 and
r2 belong to C(s′), then P (r1, s)/P (r2, s) is independent
of r, for all s. In other words, within each class C(s′),
the diﬀerent functions P (r|s) obtained by varying s dif-
fer from one another by a multiplicative constant. These
conditions coincide with the ones given by Panzeri et al.
(1999) for having an exact decoding, within the short
time limit. However, in the present derivation there are
no assumptions about the interval in which responses are
measured. Therefore, the decoding being exact whenever
Eq. (16) is fulﬁlled is not a consequence of the short time
limit carried out by Panzeri et al. (1999), but rather, a
general property of the maximum likelihood decoding.

hf (r)iC(s′) =

f (r).

1
N (s′)

r∈C(s′)
X

As a consequence, the relevant parameter in determin-
ing the size of E(s′, s) is given by the mean value—
within C(s′)—of a function that essentially measures how
diﬀerent are the true probability distributions P (r, s)
and P (r), from their ﬂattened versions P (s′, s)/Ns′ and
P (s′)/Ns′ .

To summarize, this letter presents the maximum like-
lihood decoding as an artiﬁcial—but useful—distortion
of the distribution P (r, s) within each class C(s′). The
decoded information is shown to be also a mutual infor-
mation, the latter calculated with the distorted probabil-
ity distribution. The diﬀerence between I and ID is the
Kullbach-Leibler distance between the true and distorted
distributions. As such, it is always non negative, and it
is easy to identify the conditions for the equality between
the two information measures. Finally, for small distor-
tions ∆, the amount of lost information is expressed as
a quadratic function in ∆. In short, the aim of the work
is to present a formal way of quantizing the eﬀect of an
optimal maximum likelihood decoding.

It should be kept in mind that in real situations, where
only a limited amount of data is available, the estimation
of P (r|s) may well involve a careful analysis in itself.
Some kind of assumption (as for example, a Gaussian
shaped response variability) is usually required. The va-
lidity of the assumptions made depend on the particular
data at hand. An inadequate choice for P (r|s) may of
course lead to a distorted value of I, and in fact, the bias

may be in either direction. If the choice of P (r|s) does
not even allow the correct identiﬁcation of the maximum
(5)), then the calculated
likelihood stimulus (see Eq.
value of ID will also be distorted. The purpose of this
letter, however, is to quantify how much information is
lost when passing from r to s′(r). No attempt has been
made to quantify I or ID, for diﬀerent estimations of
P (r|s).

Sometimes, P (s′, s) is deﬁned in terms of P (r, s) with-
out actually decoding the stimulus to be associated to
each response. For example, P (s′, s) can be introduced as
r P (r, s′)P (r, s)/P 2(r) (Treves, 1997). This approach,
although formally sound, is not based in a r → s′ map-
P
ping, and does not allow a partition of R into classes.
It is therefore is not directly related to the analysis pre-
sented here. However, there might be analogous deriva-
tions where one may get to quantify the information loss
also in this case.

Acknowledgements

I thank Bill Bialek, Anna Montagnini and Alessan-
dro Treves for very useful discussions. This work has
been partially supported with a grant of Proﬀ. Treves,
of the Human Frontier Science Program, number RG
01101998B.

References

- Bialek, W., Rieke, F., de Ruyter van Steveninck, R.
R., & Warland, D. (1991). Reading a neural code.
Science, 252, 1854 - 1857.

- Cover, M. T., & Thomas, J. A., (1991). Elements

of Information Theory. New York: Wiley.

- Eskandar, E. N., Richmond, B. J., & Optican, L.,
M. (1992). Role of inferior temporal neurons in
visual memory. I. Temporal encoding of informa-
tion about visual images, recalled images, and be-
havioural context. J. Neurophysiol., 68, 1277 -
1295.

- Fano, R. M. (1961) Transmission of Information:
A Statistical Theory of Communications. New
York: MIT.

- Georgopoulos, A. P., Schwartz, A., & Kettner, R.
E. (1986). Neural population coding of movement
direction. Science, 233, 1416 - 1419.

- Gochin, P. M., Colombo, M., Dorfman, G. A., Ger-
stein, G. L., & Gross, C. G. (1994). Neural ensem-
ble encoding in inferior temporal cortex. J. Neuro-
physiol, 71, 2325 - 2337.

4

- Golomb, D., Hertz, J., Panzeri, S., Treves, A., &
Richmond, B. (1997). How well can we estimate
the information carried in neuronal responses from
limited samples? Neural Comp., 9, 649 - 655.

- Heller, J., Hertz, J. A., Kjaer, T. W., & Richmond,
B. J. (1995). Information ﬂow and temporal coding
in primate pattern vision. J. Comput. Neurosci.,
2, 175 - 193.

- Kjaer, T. W., Hertz, J. A., & Richmond, B. J.
(1994). Decoding cortical neuronal signals: net-
works models, information estimation and spatial
tuning. J. Comput. Neurosci., 1, 109 - 139.

- Kullback, S., (1968).

Information theory and

statistics. New York: Dover.

- Optican, L. M., & Richmond, B. J. (1987). Tempo-
ral encoding of two dimensional patterns by single
units in primate inferior temporal cortex: III In-
formation theoretic analysis. J. Neurophysiol., 57,
162 - 178.

- Panzeri, S., & Treves, A. (1996). Analytical esti-
mates of limited sampling biases in diﬀerent infor-
mation measures. Network, 7, 87 - 107.

- Panzeri, S., Treves, A., Schultz, S., & Rolls, E. T.
(1999). On decoding the responses of a population
of neurons from short time windows. Neural Com-
put., 11, 1553 - 1577.

- Rieke, R., Warland, D., de Ruyter van Steveninck,
R. R., & Bialek W., (1996). Spikes: Exporing the
Neural Code. Cambridge: MIT Press.

- Rolls, E. T., Critchley, H. D., & Treves, A. (1996).
Representation of Olfactory Information in the Pri-
mate Orbitofronal Cortex. J. Neurophysiol., 75,
(5), 1982 - 1996.

- Rolls, E. T., Treves, A., & Tov´ee, M. J. (1997). The
representational capacity of the distributed encod-
ing of information provided by populations of neu-
rons in primate temporal visual area. Exp. Brain.
Res., 114, 149 - 162.

- Rolls, E. T., & Treves, A. (1998). Neural Networks
and Brain Function. Oxford: Oxford University
Press.

- Rolls, E. T., Treves, A., Robertson, R. G., Georges-
Francois, P., & Panzeri, S. (1998).
Information
About Spatial View in an Ensemble of Primate Hip-
pocampal Cells. J. Neurophysiol., 79, 1797 - 1813.

- de Ruyter van Steveninck, R. R., & Laughlin, S. B.
(1996). The rates of information transfer at graded-
potential synapses. Nature, 379, 642 - 645.

- Seung, H. S., & Sompolinsky, H. (1993). Simple
models for reading neural population codes. Proc.
Nac. Ac. Sci. USA, 90, 10749 - 10753.

- Shannon, C. E. (1948). AT&T Bell Laboratories

Technical Jounal ,27, 379 - 423.

- Tov´ee, M. J., Rolls, E. T., Treves, A. & Bellis, R.
J. (1993). Information encoding and the responses
of single neurons in the primate temporal visual
cortex. J. Neurophysiol., 70, 640 - 654.

- Treves, A., & Panzeri, S. (1995). The upward bias
in measures of information derived from limited
data samples. Neural Comp., 7, 399 - 407.

- Treves, A., Skaggs, W. E., & Barnes, C. A. (1996).
How much of the hippocampus can be explained by
functional constraints? Hippocampus, 6, 666 - 674.

- Treves, A. (1997). On the perceptual structure of

face space. BioSyst., 40, 189 - 196.

- Victor, J. D., & Purpura, K. P. (1996). Nature
and precision of temporal coding in visual cortex:

5

a metric space analysis. J. Neurophysiol., 76, 1310
- 1326.

- Victor, J. D., & Purpura, K. P. (1997). Metric-
space analysis of spike trains: theory, algorithms
and application. Network 8 127 - 164

- Wilson, M. A., & McNaughton, B. L. (1993). Dy-
namics of the Hippocampal Ensemble Code for
Space. Science, 261, 1055 - 1058.

Electronic address: samengo@cab.cnea.gov.ar

∗
[1] It should be kept in mind, however, that when ID is cal-
culated from actual recordings, its value is typically over-
estimated, because of limited sampling. Therefore, when
dealing with real data sets, one may eventually obtain a
value for ID that surpasses the true mutual information
I. Nevertheless, whenever the number of elements in S ′
is
signiﬁcantly smaller than the number of responses r, the
sampling bias in ID will be bound by the one obtained in
the estimation of I.


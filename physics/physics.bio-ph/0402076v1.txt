Fastest Learning in Small World Neural Networks

D. Simard, L. Nadeau, and H. Kr¨oger∗

D´epartement de Physique, Universit´e Laval, Qu´ebec, Qu´ebec G1K 7P4, Canada

Abstract

We investigate learning in neural networks. We consider a layered feed-forward network with

back propagation. We ﬁnd that the network of small world architecture minimizes the learning

time.

PACS numbers:

4
0
0
2
 
b
e
F
 
6
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
6
7
0
2
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗ Corresponding author, Email: hkroger@phy.ulaval.ca

1

The concept of small world networks and the variant of scale free networks has become

very popular recently [1, 2, 3, 4, 5, 6, 7, 8, 9], after the discovery that such networks are

realized in diverse areas as the organisation of human society (Milgram’s experiment) [1],

in the WWW [2, 6], in the internet [10], in the distribution of electrical power (western

US) [1], and in the metabolic network of the bacterium Escherichia coli [8, 11]. This leads

to the question: Is the architecture of small world networks realized in the human brain?

Watts and Strogatz [1] already hypothesized that the brain has such architecture, based

on fast synchronisation in a coupled oscillator model. From neurobiology one knows that
strict short range networks are unrealistic. In land living animals (108 − 1011 neurons in
the cortex), connections are sparse (102 − 104 per neuron) and the wiring volume scales like
N 4/3 [12]. In particular, there is evidence for sparse functional connectivity in the neocortex
and in the hippocampus [13]. The range of connections is 0.1 − 1mm, except for a small

fraction of long-ranged ones. This possibly hints to small world architecture. Actually, the

small world architecture has been observed in the nervous system of 282 neurons in the

nematode worm Caenorhabditis elegans [1, 14].

Our objective was to ﬁnd out: Why has nature chosen to use small world architecture

in living organisms? For which task is it beneﬁcial, if the brain (or part of) is organized

in this way? To answer such questions computer simulations of neural network models are

most suitable. In a Hodgkin-Huxley network the small world architecture has been found

to give a fast and synchronized response to stimuli in the brain [15]. In associative memory

models it was observed that the small world architecture yields the same memory retrieval

performance as randomly connected networks, using only a fraction of total connection

length [16]. Likewise, in the Hopﬁeld associative memory model the small world architecture

turned out to be optimal in terms of memory storage abilities [17].

In the present work we study supervised learning in a multi-layered feed-forward net-

work. Such neural architecture is relevant in biology, as it occurs in the visual cortex and

the auditive cortex. We change the architecture of neural connections from regular to ran-

dom while keeping the number of connections ﬁxed. Somewhere in between the network

2

becomes ”small world”. We found that the learning time becomes minimal when the neural

connectivity is maximally of small world architecture.

These results have wide range implications in biology. First, during the formation of

neural connections in early life and for learning during the whole life, the ”small world” net-

work stucture gives a tremendous functional advantage. This advances the understanding

of the organisation of the cortex. It gives new insight into the relation between complexity

in the organisation of the brain and its functional task. It advances our understanding of

the basis of neural learning. Moreover, our model makes predictions on local clustering

and connectivity in areas of cortex with layered structure, and compares with those exper-

imentally found in the nematode worm C .elegans. Finally, in Darwin’s theory of evolution

there is mutation and survival of the ﬁttest. The ability of fastest learning is certainly

a very important criterion of ﬁtness, may be the most important one. Thus our ﬁnding

of a strong correlation between fastest learning and small world architecture quite likely

imposes a constraint on the evolution of the nervous system in biological species.

Around 1960, the Perceptron model [18] has been widely investigated. In this model,

neurons are arranged in two layers. The information (representing action potentials prop-

agating in axons and dendrites of biological neurons) feeds forward, i.e. goes from some

input area to neurons in the ﬁrst layer, and from there to neurons in the second layer.

The biological justiﬁcation of such architecture is the fact that real neural structures in the

brain are often multi-layered, and largely but not totally feed-forward [19]. An example

is the visual cortex, which is organized in several layers. The Perceptron model has been

extended to include several layers. For the task of learning, it is important to ﬁnd optimal

weights wij between neurons (representing synaptic connections) such that for a given set

of input and output patterns (training patterns) the network generates output patterns as

close as possible to the target patterns. We used the algorithm of back propagation [20],

which is a method to determine those weights starting from the last layer and then going

backwards to the second-last layer etc. There are alternative, potentially faster methods to

determine those weights, like e.g. simulated annealing. Here we aim to compare diﬀerent

3

network architectures with respect to learning, using as reference a standard algorithm to

determine the weights.

According to Watts and Strogatz, a small world network is characterized by a clustering

coeﬃcient C and an average path length L [1]. The clustering coeﬃcient mesures the

probability that neurons b and c are connected, given that neuron a is connected to neurons

b and c. The path length from neuron a to neuron b is the minimal number of connections

to get from a to b. The deﬁnition of a small world network is that C is large and L is

small, although values may depend on the particular system.

Instead of measuring the

network architecture by the functions C and L, we have used the functions of local and

global connectivity length, Dlocal and Dglobal [21]. They are more suitable because they

allow to take into account the connectivity matrix and the weight matrix in networks and

also to treat networks with some unconnected neurons. It has been shown that Dglobal is

equivalent to L, and Dlocal is equivalent to 1/C [21]. Thus, if a network is small world,

both, Dlocal and Dglobal should become small.

The process of learning consists of a gradual change of connections and weights, such

that the network, when given the input training patterns, yields output patterns as close

as possible to the output training patterns. We have studied the change of the network

architecture and the corresponding learning time under rewiring of neurons, while keeping

the number k of connections ﬁxed. Initially, neurons are connected feed-forward, i.e. any

neuron of any layer connects to all neurons of the subsequent layer. Then by randomly

cutting local connections and replacing them randomly by arbitrary connections (including

long ranged ones) the topology changes gradually.

In particular, the initial connections

are regular and after many rewiring steps the topology becomes random. Somewhere in

between lies the small world architecture [1]. After each rewiring step, the training of the

network is repeated and the weights are adjusted by supervised learning. Then local and

global connectivity length are computed and the learning time Tlearn is determined, which

is deﬁned as the number of times necessary to present input patterns in order to make the

error smaller than a given error limit.

4

FIG. 1: Small world architecture minimizes learning time. Network of 10 layers and 10 neurons

per layer. Top: Connectivity Dlocal and Dglobal versus number of rewirings. Architecture is small
world at minima of Dlocal and Dglobal (Nrewire = 450). Bottom: Learning of 3 patterns. Learning
time becomes minimal also at Nrewire = 450.

We considered networks of 5 by 5 (5 layers with 5 neurons per layer), 10 by 10 and

15 by 15 neurons. Local and global connectivity for the 10 by 10 network are shown in

Fig.[1]. One observes that both, Dlocal and Dglobal, become small, indicating small world
architecture, and display common minima at Nwiring ≈ 450. Similar results have been
obtained with 5 by 5 neurons (minima at Nwiring ≈ 20) and 15 by 15 neurons (minima
at Nwiring ≈ 2000). The computation of learning times is very time consuming. We have

computed this quantity for the 5 by 5 and 10 by 10 network. In the 5 by 5 network we

investigated the functional dependence of learning time on the number of learned patterns.

5

While for a single pattern the learning time descends beyond Nrewire = 20, for 2 patterns

a minimum becomes apparent at Nrewire = 50 and ﬂuctuates strongly for Nrewire > 50.

However, for 3 patterns, a clear minumum is visible at Nrewire = 30, consistent with the

location of minima in Dlocal and Dglobal. For the 10 by 10 network, and learning of 3
patterns, the learning time is presented in Fig.[1], displaying a minimum at Nrewire ≈ 450.

This shows that the learning becomes fastest just when the network has most pronounced

small world structure. We note that the number of connections involved in generating a

small world architecture is smaller than the number of connections in a fully connected

network. For the 5 by 5 network, we have kept the number of connections at k = 125,

while the fully connected network would have kf ull = 250 connections, i.e. k/kf ull = 0.5.

For the 10 by 10 network, k/kf ull = 0.18, it is even sparser. Moreover, we found that

the minima in Dlocal, Dglobal and Tlearn are more pronounced in the 10 by 10 network

than in the 5 by 5 network. The gain (reduction) in learning time is about a factor 2

for the 10 by 10 network. Thus we conclude from this study that during the formation

of neural connections (plasticity) in early life and for learning during the whole life, the

small world network stucture gives a clear functional advantage. Finally, we can turn

the argument around. Under the assumption that nature organized the brain such that

learning becomes fastest, then our model makes quantitative predictions about the neural

connectivity in parts of the brain with layered feed-forward architecture, like the visual

cortex. We observed that the values where Dlocal and Dglobal become minimal do not diﬀer

very much when changing the size of the network. We found Dlocal is 7; 7.3; 8 and Dglobal
is 2.6; 3.4; 3.7 for the 5 × 5; 10 × 10; 15 × 15 network, respectively. Thus we predict
Dlocal = 8 ± 0.5 and Dglobal = 3.7 ± 0.5, which corresponds to C = 0.125 ± 0.008 and
L = 3.7 ± 0.5.
In contrast, the neural net of C. elegans (not layered feed-forward) has

C = 0.28 and L = 2.65 [1].

We believe that the results obtained are more generally valid beyond that model. This

is based on the study of a second model with non-supervised (Hebbian) learning rule where

we investigated fastest learning time vs. architecture. Again we found that the small-world

6

network performs better than a random one.

Finally, our results have important implications on artiﬁcial intelligence and artiﬁcial

neural networks. Neural networks are being widely used as computational devices for

optimisation problems based on learning in applications like pattern recognition, error

detection, and quality control in industrial production, like of car parts or sales of air-

line tickets. Our study reveals a clear advantage and suggests to use ”small world” or

”scale-free” networks in such applications.

Acknowledgements

DeKoninck, A. Destexhe and M. Steriade.

This work has been supported by NSERC Canada. H.K. is grateful for discussions with Y.

[1] Watts, D.J. & Strogatz, S.H. Nature 393, 440-442 (1998).

[2] Albert, R., Jeong, H. & Barab´asi, A.L. Nature 401, 130-131 (1999).

[3] Barab´asi, A.L. & Albert, R. Science 286, 509 (1999).

[4] Albert, R., Jeong, H. & Barab´asi, A.L. Nature 406, 378-382 (2000).

[5] Jeong, H., Tombor, B., Albert, R., Oltvai, Z.N. & Barab´asi, A.L. Nature 407, 651-654 (2000).

[6] Huberman, B.A. & Adamic, L.A. Nature 401, 131 (1999).

[7] Kleinberg, J.M. Nature 406, 845 (2000).

[8] Strogatz, S.H. Nature 410, 268-276 (2001).

[9] Watts, D.J., Dodds P.S. & Newman, M.E.J. Science 296 1302 (2002).

[10] Faloutsos, M., Faloutsos, P. & Faloutsos, C. Comp. Comm. Rev. 29, 251-262 (1999).

[11] Wagner, A. Proc. R. Soc. London B268, 1803-1810 (2001).

[12] Hofman, M.A. Brain Behav. Evol. 32, 17-26 (1988).

[13] Stephan, K.E. et al. Philos. Trans. R. Soc. London B355, 111-126 (2000).

[14] Achacoso, T.B. & Yamamoto, W.S. AY’s Neuroanatomy of C. elegans for Computation, CRC

Press, Boca Raton, FL (1992).

7

[15] Corbacho, H.R., Lago-Fern´andez, F., Sig¨uenza, L.F. & Sig¨uenza, J.A. Phys. Rev. Lett. 84,

2758-2761 (2000).

[16] Bohland, J.W.& Minai, A.A. Neurocomputing 38-40, 489-496 (2001).

[17] C.L. Labiouse, A.A. Salah, I. Starikova, ”The impact of connectivity on the memory capac-

ity and the retrieval dynamics of Hopﬁeld-type networks,” in Proceedings of the Santa Fe

Complex Systems Summer School, pp. 77-84, Budapest, NM: SFI, 2002.

[18] Rosenblatt, F. Principles of Neurodynamics, Spartan, New York (1962).

[19] Hertz, J., Krogh, A. & Palmer, R.G. Introduction to the Theory of Neural Computation, Santa

Fe Institute studies in the sciences of complexity, Lecture Notes Vol. I, Addison-Wesley Pub.,

Redwood City, California (1991).

[20] Rummelhart, D.E., Hinton, G.E. & Williams, R.J. Nature 323, 533-536 (1986).

[21] Marchiori, M. & Latora, V. Physica A285, 539-546 (2000).

8


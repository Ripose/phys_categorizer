2
0
0
2
 
y
a
M
 
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
3
0
0
5
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Determination of Functional Network Structure
from Local Parameter Dependence Data

Boris N. Kholodenko
Department of Pathology, Anatomy and Cell Biology
Thomas Jeﬀerson University, PA
Boris.Kholodenko@mail.tju.edu

Eduardo D. Sontag
Dept. of Mathematics
Rutgers University, NJ, USA
sontag@hilbert.rutgers.edu

Abstract

In many applications, such as those arising from the ﬁeld of cellular networks, it is often desired to
determine the interaction (graph) structure of a set of diﬀerential equations, using as data measured
sensitivities. This note proposes an approach to this problem.

1

Introduction

Suppose given a system of diﬀerential equations

˙x1 = f1(x1, . . . , xn, p1, . . . , pm)
˙x2 = f2(x1, . . . , xn, p1, . . . , pm)

...

˙xn = fn(x1, . . . , xn, p1, . . . , pm) ,

(1)

where the vector of state variables x(t) = (x1(t), . . . , xn(t)) evolves in some open subset X ⊆ Rn and
the vector of parameters p = (p1, . . . , pm) can be chosen from an open subset P ⊆ Rm, for some n and
m. In cellular networks, for instance, the state variables xi might represent the concentrations of certain
proteins, mRNA, etc., at diﬀerent time instants, and the parameters pi might represent the concentration
levels of certain enzymes which are maintained at a constant value during a particular experiment (see,
e.g., [1, 2]).

In many ﬁelds of application, it is often the case that the equations deﬁning the system (that is, the
form of the functions fi describing the vector ﬁeld) are unknown, even in general form, but one wishes
nonetheless to determine the interaction graph of a system (1), that is to say, to know which variables
directly inﬂuence which variables, as well as the relative strengths of these interactions ([3]). A more
limited goal might be to determine if a certain feedback loop is present in the system (cf. [4]).

To help in this task, experimental data is usually available, measuring solutions of system (1) for
various initial states and parameter settings. A special case, the one treated in this note, is that in which
experimental data provides us with the location of steady states associated to parameter values near a
given set of parameters ¯p. We show in this note how, starting from such data, and under an assumption
that seems natural, it is indeed possible to determine this interaction graph and relative strengths of
interactions. We then extend the method to non-steady state measurements. A more global analysis of
the problem is also possible, and will be presented in a follow-up report.

1

2 Problem Formulation

The steady states of system (1) associated to a given parameter vector p = (p1, . . . , pm) are the solutions
x = (x1, . . . , xn) of the set of n simultaneous algebraic equations

f1(x1, . . . , xn, p1, . . . , pm) = 0
f2(x1, . . . , xn, p1, . . . , pm) = 0

...

fn(x1, . . . , xn, p1, . . . , pm) = 0 .

(2)

(3)

(4)

We will assume that there is a function ξ : P → X which assigns, to each parameter vector p ∈ P, a
steady state ξ(p) of (1), that is to say, one has that fi(ξ(p), p) = 0 for all i = 1, . . . , n and all p ∈ P.
We suppose that the functions fi as well as ξ are continuously diﬀerentiable. A particular parameter
vector ¯p is also given, and the problem will be formulated in terms of the behavior of steady states near
¯x = ξ(¯p).

Experimental Data

It will be assumed that an n × m matrix Σ = (σkj ) is given, representing the “sensitivities”

σkj =

(¯p)

∂ξk
∂pj

aij =

(¯x, ¯p)

∂fi
∂xj

for each k = 1, . . . , n and each j = 1, . . . , m. This matrix of partial derivatives may be estimated
numerically from the values of ξ(p) on a neighborhood of the chosen parameter ¯p.

Desired Information

Consider the n × n matrix A = (aij ) deﬁned by:

for every i, j = 1, . . . , , n.

Ideally, one would want to ﬁnd the matrix A, since this matrix completely describes the inﬂuence of
each variable xj upon the rate of change of each other variable xi. Unfortunately, such an objective is
impossible to achieve from the local steady-state data Σ, or even from the knowledge of the complete
(global) mapping ξ. This is because the same mapping ξ also solves the set of equations

λ1f1(x1, . . . , xn, p1, . . . , pm) = 0
λ2f2(x1, . . . , xn, p1, . . . , pm) = 0

...

λnfn(x1, . . . , xn, p1, . . . , pm) = 0 ,

Ai = (ai1, . . . , ain) ,

i = 1, . . . , n

for any constants λi, but, on the other hand, multiplication of fi by λi results in aij = λiaij . In other
words, the best that one could hope is for the data Σ to determine the rows

of A only up to scalar multiples.

Thus, a more realistic objective is to attempt to identify the rows Ai up to a scalar multiple only.
For example, if we assume that aii 6= 0 for each i (a realistic assumption when stable systems are being
interconnected), this amounts to ﬁnding the ratios aij/aii for each i 6= j.

2

Assumptions

We will make two assumptions which will suﬃce for us to solve the problem of determining the rows Ai
of A up to scalar multiples. The ﬁrst assumption is a strong but reasonable structural one, while the
second represents a weak algebraic nondegeneracy condition.

We will suppose known, for each i ∈ {1, . . . , n}, a subset Si of the index set {1, . . . , m} so that the

following property holds:

(∀ j ∈ Si)

(¯x, ¯p) = 0

∂fi
∂pj

which is in turn implied by the structural condition:

(∀ j ∈ Si) fi does not depend upon pj .

This prior information about the system structure is far less restrictive than it might appear at ﬁrst
sight. Indeed, it is usually the case that “compartmental” information is available, for instance telling us
that the concentration of a certain enzyme has no direct inﬂuence on an unrelated biochemical reaction,
that an extracellular signaling molecule does not aﬀect directly a cytoplasmatic reaction, and so forth.

The second assumption is as follows. For each j ∈ {1, . . . , m}, we introduce the vector

(5)

(6)

Σj = col (σ1j, . . . , σnj) =

∂ξ1
∂pj

∂ξn
∂pj

(¯p)
...

(¯p)











representing the jth column of the matrix Σ, and we consider, for each i ∈ {1, . . . , n} the linear subspace
Hi of Rn spanned by the vectors

The assumption is:

{Σj | j ∈ Si}.

dim Hi ≥ n − 1 ∀ i = 1, . . . , n .

Note that this amounts to saying that the dimension of Hi is either n or n − 1. (Generically, we may
expect this dimension to be n − 1, because the orthogonality relation to be shown below is the only
algebraic constraint.)

3 Solution

With the above assumptions, the problem that we posed can be solved as follows. We ﬁx any index
i ∈ {1, . . . , n}, and take partial derivatives in the equation fi(x(p), p) = 0 with respect to the variable
pj, for each index j in the set Si, and evaluate at x = ¯x and p = ¯p:

0 =

fi(¯x, ¯p) =

(¯x, ¯p)

(¯x) +

(¯x, ¯p) = Ai · Σj

∂
∂pj

n

X
k=1

∂fi
∂xk

∂xk
∂pj

∂fi
∂pj

where the second term vanishes by assumption (5).

Since this happens for every j ∈ Si, we conclude that the vector Ai is orthogonal to Hi. As Hi has

dimension n or n − 1, this determines Ai up to a scalar multiple, which is what we wanted to prove.

Of course, it is trivial to actually compute Ai from the data. If Hi has dimension n, then Ai = 0;
this is a degenerate case. When the dimension is n − 1, one simply picks a basis {Σj1 , . . . , Σjn−1 } of Hi,
and any vector Σ0 linearly independent from the elements of this basis (a randomly chosen vector has
this property), and then solves (for example) the nonsingular set of equations Ai · Σ0 = 1, Ai · Σjℓ = 0,
ℓ = 1, . . . , n − 1, to ﬁnd a nonzero Ai (all possible Ai are scalar multiples of this one). Alternatively,
provided that one knows that aii 6= 0, one may simply normalize to aii = −1 and then determine the

3

remaining entries of Ai by solving a linear set of n − 1 equations. Observe also, that if it is known a
priori that certain entries aij vanish, then one may redeﬁne the space Hi to be spanned only by the
vectors listing the appropriate components of the sensitivities ∂ξi/∂pj’s, and a potentially much smaller
number of parameter perturbations may be required.

4 Modular Approach

It is also possible to apply our techniques in a “modular” context, in which only the derivatives ∂fi/∂xj
with respect to communicating intermediaries are calculated ([4]). Let us brieﬂy explain this.

We assume that the entire network consists of an interconnection of n subsystems or “modules”, each

of which is described by a set of diﬀerential equations such as:

˙xj = g0,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm)
˙y1,j = g1,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm)
˙y2,j = g2,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm)

...

˙yℓj ,j = gℓj,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm) ,

where the variables xj represent “communicating” or “connecting” intermediaries of module j that
transmit information to other modules, whereas the variables y1,j, . . . , yℓ,j represent chemical species
that interact within module j. The integer ℓj, j = 1, . . . , n is in general diﬀerent for each of the n
modules and represents the number of chemical species in the jth module.

We will assume that, for each ﬁxed module, the Jacobian of (g1, . . . , gℓj,j) with respect to y1, . . . , yℓj ,
evaluated at the steady state corresponding to ¯p (assumed to exist, as before) is nonsingular. The
Implicit Mapping Theorem then implies that one may, in a neighborhood of this steady state, solve

(7)

(8)

g0,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm) = 0
g1,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm) = 0
g2,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm) = 0

...

gℓj,j(y1,j, . . . , yℓ,j, x1, . . . , xn, p1, . . . , pm) = 0

for the variables xj, y1, . . . , yℓj as a function of x1, . . . , xn, p1, . . . , pm. One concludes that, around this
steady state corresponding to ¯p, the functions xj satisfy implicit equations of the form

xj = hj(x1, . . . , xn, p1, . . . , pm)

which we can rewrite in the form (2), using fj(x, p) = xj − hj(x, p). The analysis then proceeds as
before. The generalization to the case of more than one communicating intermediate in a module,
namely a vector (xj,1, . . . , xj,kj ), is obvious.

5 Avoiding Derivatives

The technique that was described assumes that we know the sensitivity matrix Σ, which is obtained
by evaluating the partial derivatives ∂ξi/∂pj at the particular parameter value ¯p. Ordinarily, these
derivatives would be estimated by ﬁnite diﬀerences. For instance, suppose that one measures ¯x = ξ(¯p)
as well as ξ(¯p + djp), where

dj p = col (0, . . . , 0, dpj, 0, . . . , 0)

(entry in jth position) and we view dpj as a “small” perturbation of the jth parameter. Denoting

dj xi := ξi(¯p + djp) − ¯xi

(9)

4

obviously one may estimate Σ using the following approximation:

∂ξi
∂pj

(¯p) ≈

djxi
dpj

.

In order to calculate this ratio, both djxi and dpj must be known.

However, in certain experimental situations it may well be impossible to estimate the values of dpj.
This might appear to be contradictory, since we are assuming that we perform experiments which change
the values of p. But one can easily envision an experimental setup in which a certain external variable
(in a cell biology situation, for instance, a growth factor) is known to inﬂuence a certain parameter pj.
Varying this external variable therefore produces a perturbation in pj, and hence an appropriate dj x
which is measured, but dpj itself may be hard to measure.

It is rather surprising that we can still achieve our goal of estimating the rows of A (up to scalar
multiples) even in the absence of information about the dpj’s! To see intuitively why this is plausible,
consider the following argument. Let us say that we have just a scalar parameter p and a scalar function
f (x1, x2) so that f (ξ1(p), ξ2(p)) ≡ 0, and that ¯x = ξ(¯p) = (0, 0). In a neighborhood of p = ¯p, we may
assume that f is linear, so we have a linear relation (with unknown coeﬃcients)

The method discussed so far would take derivatives at p = ¯p:

a1ξ1(p) + a2ξ2(p) = 0 .

a1

dξ1
dp

(¯p) + a2

(¯p) = 0

dξ2
dp

and thus (assuming that the derivative is not zero), we know that the row (a1, a2) must be a multiple
of (−(dξ2/dp)(p), (dξ1/dp)(p)). A completely diﬀerent argument (analogous to using a two-point as
opposed to a slope-point formula in order to ﬁnd the equation of a line) would simply take the original
equation a1ξ1(p) + a2ξ2(p) = 0 (valid only p ≈ ¯p, since this was an approximation of f ) and say that
the row (a1, a2) must be a multiple of (−ξ2(p), ξ1(p)), for any ﬁxed p ≈ ¯p. There is no inconsistency
between the two estimates, since they only diﬀer (approximately) by multiplication by the scalar dp:

(−ξ2(p), ξ1(p)) ≈ (−(dξ2/dp)(p), (dξ1/dp)(p)) dp

and we only care about scalar multiples. Let us now say this in general.

Since fi(ξ(¯p + djp), ¯p + djp) − f (¯x, ¯p) = 0 − 0 = 0, we have, taking a Taylor expansion, that

dpj + o(dpj) =

(¯x, ¯p)

(¯p) dpj + o(dpj) = 0 .

(10)

d
dpj

fi(ξ(p), p)(cid:12)
(cid:12)
(cid:12)

p= ¯p

whenever j ∈ Si. Substituting

into (10), we conclude that

n

X
k=0

∂fi
∂xk

∂ξk
∂pj

∂ξk
∂pj

djxk = ξk(¯p + djp) − ¯xk =

(¯p) dpj + o(dpj)

(11)

(¯x, ¯p) djxk = o(dpj ) provided that j ∈ Si .

Since dpj ≈ 0 and dj xk = O(dpj ), this is an approximate orthogonality relation, and we now make the
approximation:

(¯x, ¯p) dj xk = 0 provided that j ∈ Si .

(12)

In conclusion, and introducing the matrix Γ = (γkj ) = (djxk) instead of Σ, we have that Ai · Γj = 0 for
all j ∈ Si, where Γj = col (γ1j , . . . , γnj). Now we consider, for each i ∈ {1, . . . , n}, the linear subspace
Ki of Rn spanned by the vectors {Γj | j ∈ Si} and assume that dim Ki ≥ n − 1 for all i. We conclude
that the vector Ai is orthogonal to Ki, and this once again determines Ai up to a scalar multiple.

n

X
k=0

∂fi
∂xk

n

X
k=0

∂fi
∂xk

5

6 Non-Steady State Analysis

Let us sketch here how one might extend our methodology to use non-steady state data. In general, we
denote by ξ(t, x0, p) the solution of (1) with initial condition x0, at time t and using parameters p. Let
us suppose that we can measure the sensitivities (3) at some speciﬁc point in time, and for some speciﬁc
solution ξ(¯t, ¯x0, ¯p):

(13)

for each k = 1, . . . , n and each j = 1, . . . , m, and we let Σ = (σkj ). We also need now the mixed second
derivatives:

σkj =

(¯t, ¯x0, ¯p)

∂ξk
∂pj

ηij =

(¯t, ¯x0, ¯p)

∂2ξi
∂pj∂t

and instead of Σj = col (σ1j , . . . , σnj) as in (6), we consider for each i ∈ {1, . . . , n} and j ∈ {1, . . . , m}
the vector

Σij = col (ηij , σ1j , . . . , σnj ).
(14)
We deﬁne, for each i ∈ {1, . . . , n}, Hi as the linear subspace of Rn+1 spanned by the vectors {Σij | j ∈ Si}.
We let now aij = ∂fi
(¯t, ¯x0, ¯p). Fixing any index i ∈ {1, . . . , n}, we take partial derivatives on both sides
∂xj
of the diﬀerential equation

with respect to the variable pj, for each index j in the set Si, and evaluate at x = ¯x, t = ¯t, and p = ¯p,
to obtain:

∂
∂t

ξi(t, x0, p) = fi(ξ(t, x0, p), p)

ηij =

fi(¯x, ¯p) =

aikσkj

∂
∂pj

n

X
k=1

from which we conclude that [−1, Ai] · Σj = 0 whenever j ∈ Si, and hence that [−1, Ai] is orthogonal to
Hi. With appropriate genericity conditions, this orthogonality, perhaps in conjunction with conditions
at other times t or points p, will restrict the possible vectors Ai and more generally the interaction graph.
(For example, if dim Hi = n, then we have a unique solution.) Derivatives with respect to parameter
values can be replaces by diﬀerences, just as in the steady state case. We will discuss this further in a
future contribution.

References

[1] Kholodenko, B.N., Demin, O.V., Moehren, G., and Hoek, J B. (1999) “Quantiﬁcation of short term

signaling by the epidermal growth factor receptor,” J. Biol. Chem. 274, 30169-30181.

[2] Moehren, G., Markevich, N., Demin, O., Kiyatkin, A., Goryanin, I., Hoek, J.B., and Kholodenko,
B.N. (2002) “Temperature dependence of the epidermal growth factor receptor signaling network
can be accounted for by a kinetic model,” Biochemistry 41, 306-320.

[3] Chevalier, T., Schreiber, T., and Ross, J. (1993) “Toward a systematic determination of complex

reaction mechanisms,” J. Phys. Chem. 97, 6776–6787.

[4] Kholodenko, B.N., Kiyatkin A., Bruggeman F., Sontag E.D., Westerhoﬀ H., Hoek J. (2002) “Un-
tangling the wires: a novel strategy to trace functional interactions in signaling and gene networks,”
submitted for publication.

6


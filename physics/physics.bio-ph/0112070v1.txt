1
0
0
2
 
c
e
D
 
0
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
0
7
0
2
1
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Adaptive sampling by information maximization

Christian K. Machens∗
Innovationskolleg Theoretische Biologie, Invalidenstr. 43,
Humboldt-University Berlin, 10115 Berlin, Germany
(Dated: February 2, 2008)

The investigation of input-output systems often requires a sophisticated choice of test inputs to
make best use of limited experimental time. Here we present an iterative algorithm that continuously
adjusts an ensemble of test inputs online, subject to the data already acquired about the system
under study. The algorithm focuses the input ensemble by maximizing the mutual information
between input and output. We apply the algorithm to simulated neurophysiological experiments
and show that it serves to extract the ensemble of stimuli that a given neural system “expects” as
a result of its natural history.

PACS numbers: 87.10.+e, 89.70.+c, 07.05.-t, 87.19.-j

Biophysical systems often have many degrees of free-
dom and thus one needs large numbers of variables and
parameters to describe them. Without strong prior
knowledge about the intrinsic dynamics of such a sys-
tem, one is left with inferring its function from data
obtained by experiments or observations. Given a sys-
tem where we control a set of “input” variables x =
(x(1), x(2), . . . , x(n)) and measure another set of “output”
variables y = (y(1), y(2), . . . , y(m)), we can actively ma-
nipulate the data acquisition by selecting the most infor-
mative test inputs. Yet how should one choose the test
inputs to learn most about the input-output relation?

Within the classical Volterra-Wiener system identiﬁ-
cation methods [1], the input space is sampled by draw-
ing inputs from a probability distribution p(x); a com-
mon choice is Gaussian “white noise”. However, not
all aspects of the system’s input-output relation may be
equally important. In neurobiology, for instance, one is
especially interested in inputs x about which a given sen-
sory system conveys most information. In the spirit of
importance sampling [2], one might therefore focus the
data acquisition on those x that contribute most to the
information transfer. For a given input distribution, the
information provided by a single input can then be quan-
tiﬁed as I(x) = Hy − Hy(x) where Hy is the entropy of
the output distribution p(y) and Hy(x) is the entropy
of the conditional probabilities p(y|x) which characterize
the input-output relation [3, 4]. Hence, the appropriate
focusing is achieved by an input distribution popt(x) that
maximizes the mutual information I = hI(x)i where the
angular brackets denote averaging over popt(x).

Without any information about the system and its
input-output relation, the optimal
input distribution
popt(x) is unknown. Any experimental test of the system
must therefore start with drawing the test inputs from
some predeﬁned distribution pφ(x) that depends on a set
of parameters φ = (φ(1), . . . , φ(L)). Once data about the
system has been acquired, however, one need not adhere
to this initial choice of an input distribution.
Instead,
one should adapt the parameters or even the structure

of pφ(x) to better focus on the important inputs.
In
this letter, we show how to systematically perform this
adaptation. By iterating the adaptation procedure, the
acquired data becomes ever more useful and the input
distribution approaches the optimum.

Adapting the input distribution—For mathematical
simplicity, we assume that both input and output take
discrete values. Say that we have already tested the sys-
tem with N diﬀerent inputs xi each of which was pre-
sented Mi times while measuring the outputs yij with i =
1 . . . N and j = 1 . . . Mi. We deﬁne the set of all diﬀer-
ent output values measured so far by {yk : k = 1 . . . K}.
Our present knowledge about the system is summarized
by the conditional probability that an output yk was ob-
tained from the input xi,

q(yk|xi) =

δyij ,yk .

(1)

1
Mi

Mi

X
j=1

The estimated probabilities q(yk|xi) allow us to re-
evaluate the relative importance of the inputs xi in terms
of their potential contribution to the mutual information.
To measure this contribution, we assign a probability or
“weight” q(xi) to every input. Initially we assume that
all inputs xi contribute equally and set q1(xi) = 1/N .
To ﬁnd a combination of weights that maximizes the in-
formation transfer, we use the Blahut-Arimoto algorithm
[5] and readjust the weights,

qn+1(xi) =

qn(xi) exp (cid:16)

q(yk|xi) log

1
Z

K

X
k=1

q(yk|xi)
qn(yk) (cid:17).

(2)

P

N
i=1 q(yk|xi)qn(xi) and Z is a normal-
Here qn(yk) =
N
i=1 qn+1(xi) = 1. Accord-
ization constant so that
P
ing to Eq. (2), the weight of an input xi is decreased
if its conditional output distribution q(yk|xi) is similar
to the total output distribution qn(yk). In contrast, the
weight of an input xi is increased if the respective dis-
tributions diﬀer. When Eq. (2) is iterated, the weights

converge and reach a global maximum of the mutual in-
formation [5]. In practice, we terminate the process once
|1 − qn+1(xi)/qn(xi)| < ǫ for all i and some chosen preci-
sion ǫ and set qopt(xi) = qn+1(xi).

The weights or probabilities qopt(xi) describe the rel-
ative frequencies with which the respective inputs xi
should be drawn. Consequently, we need to adapt the
parameters φ so as to ﬁnd a matching distribution pφ(x).
Here we determine the new parameters φ by maximizing
the log-likelihood function [6],

log L(x1, . . . , xN |φ) =

qopt(xi) log pφ(xi)

(3)

N

X
i=1

where the probabilities qopt(xi) provide the appropriate
weights. For some model distributions, e.g. Gaussians,
the maximum can be found analytically. In general, how-
ever, one has to evaluate the maximum numerically.

The input distribution given by the new parameter val-
ues can be used to draw new test inputs, present them
to the system and measure the respective outputs. After
a certain amount of data has been acquired, the param-
eters φ of the input distribution can be adapted again.
The resulting iterative algorithm moves the input distri-
bution towards an optimal ensemble.

Model quality and convergence—Every maximum of
the mutual information with respect to p(x) is a global
maximum [4]. Hence, if the model distribution does not
rule out any inputs, i.e., pφ(x) > 0 for all x and φ, the
estimates of the input-output relation, Eq. (1), converge,
and therefore qopt(xi) → popt(xi). Accordingly, the mu-
tual information ID = hH q
y − Hy(x)iq achieves the infor-
mation capacity of the system; here, the index q denotes
that the respective quantities and averages are calculated
with respect to qopt(xi).

The model distribution pφ(x) converges towards an op-
timal ﬁt of popt(x). To control how well the model dis-
tribution captures the structure of the optimal distribu-
tion, one can check the mutual information achieved by
the model, IM = hH φ
y −Hy(x)iφ, which is calculated with
N
respect to rφ(xi) = pφ(xi)/[
j=1 pφ(xj)]. The fraction γ
P
of the mutual information captured by the model is then
deﬁned as

γ =

IM
ID

(4)

and provides a measure for the quality of the model.
Hence, if γ falls signiﬁcantly below one, the model does
no longer capture the structure of the optimal ensemble;
in such a case, one might increase the complexity of the
model.

In general, the algorithm will not be able to adapt the
input ensemble if the presented inputs always result in
the same output value. Similarly, there is no possibility
to weight the inputs xi diﬀerently if every input elicits a
new, diﬀerent output. However, the latter problem can

2

0.0 0.2 0.4 0.6 0.8 1.0
Probability

(a)

t
n
u
o
c
 
e
k
p
S

i

40

30

20

10

0

(b)

0.08

y
t
i
l
i

b
a
b
o
r
P

0.06

0.04

0.02

-10

1

0

10

µ
2
Current (   A/cm  )

20

2

4

1

0

-10

0

10

µ
2
Current (   A/cm  )

20

FIG. 1: Approaching the optimal input ensemble of a neuron
with static, one-dimensional input and output. (a) Plot of the
conditional probability distribution p(C|I) with spike count C
and input current I. The uncertainties at I ≈ 22 µA/cm
are
due to a decline in spike size that makes it impossible to detect
2
,
the spikes in the noisy voltage output. For I ≈ 28 µA/cm
the model neuron ceases to generate spikes. (b) Approaching
the optimal input distribution (bars). Shown are the initial
distribution (1), the distributions of the iterations (2) and (4),
as well as the ﬁnal distribution (∞). (Simulation parameters:
2
,
n = 1, m = 1, L = 2, A = 10, B = 5, ǫ = 0.1, ση = 4 µA/cm
fη = 1000 Hz)

2

be solved by discretizing the output side into a smaller
number of possible outputs. The input space, on the
other hand, can be discretized as ﬁne as needed without
impeding the convergence of pφ(x).

Example—To illustrate the method, we study a numer-
ical simulation of a Hodgkin-Huxley-type model neuron
[7]. The model neuron transforms an input current I
into a voltage output V . For constant current values
I < 0 µA/cm2, the voltage approaches a stable equi-
librium. For current values I > 0 µA/cm2, the model
undergoes a saddle-node bifurcation and generates peri-
odically occurring action potentials, also called spikes [8].
Stochastic aspects of neural activity are incorporated by
adding Gaussian white noise with a ﬁxed standard devi-
ation ση and a cut-oﬀ frequency fη to the input.

We start with a simple one-dimensional parametriza-
tion of
input and output. The inputs are 100-ms-
long, discretized current steps (∆I = 1 µA/cm2),
restricted to a physiologically realistic range of I =
−12 . . . 28 µA/cm2. The outputs are given by the num-
ber of spikes, C, during the corresponding time window.
The resulting probabilistic relation of spike count versus
current is displayed in Fig. 1(a).

For this one-dimensional input-output system, we can
compute an exact solution of the information maximiza-

tion problem. The optimal input distribution popt(I) is
depicted by the vertical bars in Fig. 1(b); the shape of
popt(I) corresponds to the slope of the input-output rela-
tion [9]. Note that there is a slight increase in the proba-
bilities of inputs far below threshold (I ≤ −10 µA/cm2).
These inputs result almost certainly in a zero spike count
output. At the same time, inputs closer to threshold
(I ≈ −9 . . . − 1 µA/cm2) are more likely to produce
spikes. As the optimal input distribution favors inputs
that are more reliable, the inputs closer to threshold are
neglected.

(cid:2) P

P
.

To study the performance of the iterative algorithm,
we model the optimal input distribution by a truncated
Gaussian. As initial parameter values, we choose a mean
φ(1) = −10 µA/cm2 and a standard deviation φ(2) =
10 µA/cm2. In each iteration, we draw A current values
from the Gaussian, test them B times on the system,
and adapt the parameters. For our Gaussian model, the
maximum likelihood estimate of the new parameters is
N
given by φ(1) =
i=1(Ii −
1/2
φ(1))2qopt(Ii)
(cid:3)

N
i=1 Iiqopt(Ii) and φ(2) =

The Gaussian model distributions are displayed in
Fig. 1(b) for the ﬁrst few iterations. Most of the cur-
rent values drawn from the initial distribution fall be-
low the spiking threshold of the neuron. Consequently,
the algorithm shifts the Gaussian distribution into the
spiking regime of the neuron. After about 10 iterations,
the mutual information rate saturates at ≈ 40 bits/sec.
Since both the ﬁnal Gaussian model pφ(I) and the op-
timal input distribution popt(I) lead to approximately
the same information transfer, the landscape of the mu-
tual information with respect to the input distribution
is relatively ﬂat around the maximum;
it suﬃces if
the input distribution covers the relevant input range
(I ≈ 0 . . . 25 µA/cm2). Note, that due to the maximum-
likelihood estimation, Eq. (3), the ﬁnal Gaussian distri-
bution has the same mean and variance as the optimal
distribution.

example—The

Multi-dimensional

computational
power of the algorithm becomes clearly visible for
high-dimensional input spaces. As an example, consider
the above model neuron when the input consists of
time-varying,
statistically stationary currents, dis-
cretized in time steps of ∆t1. Following [10], we slide
overlapping windows of length T = n∆t1 across the
input current trace and use the values within each
window as input vector Ii = (I (1)
). For each
of these inputs Ii, the output Cij is given by the spike
times, discretized in time steps of ∆t2 = T /m, during
the corresponding window. Hence, each input consists
of n real-valued numbers bounded within the interval
I = −12 . . . 28 µA/cm2, and each output consists of
m numbers whose values are either zero (no spike) or
one (spike). Note, that we do not explicitly discretize
the current values; we instead assume that every input

, . . . , I (n)

i

i

(a)

0
.
2

5
.
1

0
.
1

5
.
0

)
 
1
 
z
H
 
4
 
-
 
m
c
 
2
 
A
u
(
 
r
e
w
o
P

)
c
e
s
/
s
t
i
b
(
 
n
o
i
t
a
m
r
o
f
n
I

700
600
500
400
300
200
100
0

3

0

50

100
Iteration no.

150

200

0
0
2

0
5
1

n

0
0
1

atio

r

Ite

0
5

)

2
m
c
/
A
µ
(
 
e
g
a
r
e
v
A

10

5

0

1

0.8

0.6

0.4

0.2

γ

 
n
o
i
t
c
a
r
f
 
n
o
i
t
a
m
r
o
f
n
I

0

0

0

500

1000

Frequency (Hz)

1500

0
2000

(b)

()

Information rate ID

Model quality

0

50

100
Iteration no.

150

200

50

100
Iteration no.

150

200

FIG. 2: Approaching the optimal input ensemble of a neu-
ron with time-varying input and output.
(a) Evolution of
average and power spectrum. (b) Evolution of information
rate and (c) model quality for three diﬀerent initial condi-
(Simulation parameters: n = 64, m = 16, L = 33,
tions.
2
A = 1000, B = 20, ǫ = 0.1, ση = 4 µA/cm
, fη = 1000 Hz,
∆t1 = 0.25 ms, ∆t2 = 1 ms, T = 16 ms; windows slided by
∆t2; accordingly, A∆t2B × 100 iterations ≈ 34 minutes)

Ii is unique. For simplicity, we use a Gaussian input
it
distribution. As the input is real and stationary,
suﬃces to use L = n/2 + 1 parameters for describing
average and power spectrum of the current trace.

(cid:3)

1/2

(cid:2) P

n/2
i=1 φ(i)

To test the system, we choose an initial distribution
with an average φ(1) = 0 µA/cm2 and a ﬂat power
=
spectrum with standard deviation σ =
10 µA/cm2. For this prior, only 50% of the input val-
ues lie above threshold and the inputs will rarely lead
to high ﬁring rates, cf. Fig. 1. Consequently, we do not
properly explore the full range of the input-output rela-
tion; if, for example, we test the system for 30 minutes
with input currents drawn from this initial distribution,
the information rate ID does not exceed ≈ 300 bits/sec.
When using the iterative algorithm to adapt the pa-
rameters of the input ensemble, on the other hand, the
information rate ID saturates around ≈ 670 bits/sec af-
ter about 20 minutes. Figure 2(a) shows how the power
spectrum is shaped during the iterations. Only input
frequencies below 500 Hz are well suited for the infor-
mation transfer, the cut-oﬀ is roughly determined by the

maximum ﬁring rate of the model neuron. The overall
increase in power leads to input currents that override
the additive noise η of the model neuron.

Initial conditions, convergence, and degeneracies—
When the initial distribution is very narrow (ﬂat power
spectrum up to fc = 1000 Hz, with σ = 1 µA/cm2,
φ(1) = 20 µA/cm2, Fig. 2(b), dotted line), most of the
input currents drive the neuron maximally and thereby
very reliably. The strong initial bias leaves the algo-
rithm with little maneuvering space for the parameter
re-estimation so that it takes longer to approximate an
optimal input distribution.

In the worst case, every input leads to the same output
value. With the initial choice φ(1) = −6 µA/cm2 and σ =
10 µA/cm2, the input does not elicit any spikes during
the ﬁrst iterations, cf. Fig. 2(b), dashed line. However,
once a spike has appeared, the statistics of the model
distribution immediately moves into the direction of the
statistics of the inputs Ii that caused a spike. When the
algorithm has tracked the relevant input range, a rapid
increase of the information rate follows.

the mutual

In the examples studied,

information
reaches approximately the same value independent of the
initial conditions, cf. Fig. 2(b). Although there is always
a clear preference for frequencies below 500 Hz, however,
the parameters of the optimal input ensemble do not con-
verge to the same set of values. Consequently, there is
no unique combination of parameters that maximizes the
mutual information; an observation that generalizes be-
yond the speciﬁc examples chosen. Nonetheless, the ﬁnal
input distributions always capture about γ = 80% of the
mutual information ID, cf. Fig. 2(c).

In general, there might be “degenerate” subsets in
stimulus space, i.e., sets of stimuli that lead to the same
output value.
In these cases, the total probability as-
signed to such a subset can be distributed in an arbi-
trary way on the subset and any statistical parameters φ
that depend on these subsets can assume diﬀerent val-
ues without signiﬁcant consequences for the information
transfer.

Neurophysiological interpretation—Recent studies in-
dicate that sensory neurons convey large amounts of in-
formation if the properties of the stimulus ensembles used
match those of natural stimuli [11]. Here we have shown
how to extract a stimulus ensemble that conveys the max-
imum possible information without any prior knowledge.
The proposed method could therefore serve to ﬁnd the
ensemble of stimuli that a given neuron naturally “ex-
pects”. Note that in contrast to previous online algo-
rithms such as Alopex or Simplex [12], we are not looking
for a single optimal stimulus but rather for a complete
ensemble of stimuli.

Our results demonstrate also that the optimal stimu-
lus ensemble depends on the chosen criteria about what
aspect of the output carries the relevant information.
Hence, if the investigated model neuron conveys informa-

4

tion in its average ﬁring rate, then it best encodes slow-
varying current values in the range I = 0 . . . 23 µA/cm2.
Synaptic inputs should therefore drive the neuron in the
corresponding range. If, on the other hand, a neuron en-
codes its information in the precise timing of spikes, then
the synaptic input should be of a more binary nature and
either fully excite or fully inhibit the neuron. Measuring
the time courses of a neuron’s membrane potential thus
allows conclusions about the used neural code under op-
timal conditions.

I thank A.V.M. Herz and M.B. Stemmler for stimu-
lating discussions and H. Herzel for helpful comments on
the manuscript. This work was supported by the DFG
through the Innovationskolleg Theoretische Biologie and
the Graduiertenkolleg 120.

∗ Electronic address: c.machens@itb.biologie.hu-berlin.de;
URL: http://itb.biologie.hu-berlin.de/~machens
[1] N. Wiener, Nonlinear Problems in Random Theory (MIT
Press, 1958); Y. L. Lee and M. Schetzen, Internat. J.
Control 2, 237 (1965). G. Palm and T. Poggio, SIAM J.
Appl. Math 34, 524 (1978).

[2] C. Itzykson and J. M. Drouﬀe, Statistical Field Theory,

Vol. 2 (Cambridge University Press, 1989).

[3] C. E. Shannon and W. Weaver, The mathematical the-
ory of communication (University of Illinois Press, 1949);
M. R. DeWeese and M. Meister, Network: Comput. in
Neural Systems 10, 325 (1999).

[4] T. M. Cover and J. A. Thomas, Elements of information

theory (Wiley, 1991);

[5] S. Arimoto, IEEE Trans. Inform. Theory IT-18, 14
(1972); R. E. Blahut, IEEE Trans. Inform. Theory IT-
18, 460 (1972).

[6] R. Barlow, Statistics. (Wiley, 1989).
[7] X.-J. Wang and G. Buzs´aki, J. Neurosci. 16, 6402 (1996).
[8] E. M. Izhikevich, Int. J. Bifurc. and Chaos 10, 1171

(2000).

[9] For a deterministic input-output system, the mutual in-
formation is given by I = Hy since Hy(x) = 0 for all
x. Maximizing the mutual information results in a uni-
form distribution of the outputs y. For a one-dimensional
system with a monotonic relation y = f (x), we have
p(x)dx = p(y)dy and the optimal input distribution is
simply given by p(x) ∝ dy/dx. This relation is approxi-
mately preserved in the stochastic case.

[10] S. P. Strong, R. Koberle, R. R. de Ruyter van Steveninck,

and W. Bialek, Phys. Rev. Lett. 80, 197 (1998).

[11] F. Rieke, D. A. Bodnar, and W. Bialek, Proc. R. Soc.
Lond. B 262, 259 (1995); H. Attias and C. E. Schreiner,
in Advances in Neural Information Processing Systems
10, edited by M. I. Jordan et al.
(MIT Press, 1998),
pp. 103–109; C. K. Machens, M. B. Stemmler, P. Prinz,
R. Krahe, B. Ronacher, and A. V. M. Herz, J. Neurosci.
21, 3215 (2001).

[12] E. Harth and E. Tzanakou, Vision Res. 14, 1475 (1974);
I. Nelken, Y. Prut, E. Vaadia, and M. Abeles, Hearing
Res. 72, 237 (1994).


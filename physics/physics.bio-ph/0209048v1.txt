Dynamical transitions in the evolution of learning algorithms by selection

Juan Pablo Neirotti and Nestor Caticha
Departamento de F´ısica Geral, Instituto de F´ısica, Universidade de S˜ao Paulo, Rua do Mat˜ao Travessa R 187,
CEP 05508-900 S˜ao Paulo, Brazil
(January 14, 2014)

2
0
0
2
 
p
e
S
 
1
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
8
4
0
9
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Abstract

We study the evolution of artiﬁcial learning systems by means of selection. Genetic program-
ming is used to generate a sequence of populations of algorithms which can be used by neural
networks for supervised learning of a rule that generates examples. In opposition to concentrat-
ing on ﬁnal results, which would be the natural aim while designing good learning algorithms, we
study the evolution process and pay particular attention to the temporal order of appearance of
functional structures responsible for the improvements in the learning process, as measured by
the generalization capabilities of the resulting algorithms. The eﬀect of such appearances can be
described as dynamical phase transitions. The concepts of phenotypic and genotypic entropies,
which serve to describe the distribution of ﬁtness in the population and the distribution of symbols
respectively, are used to monitor the dynamics. In diﬀerent runs the phase transitions might be
present or not, with the system ﬁnding out good solutions, or staying in poor regions of algorithm
space. Whenever phase transitions occur, the sequence of appearances are the same. We identify
combinations of variables and operators which are useful in measuring experience or performance
in rule extraction and can thus implement useful annealing of the learning schedule. We also ﬁnd
combinations that can signal surprise, measured, on a single example, by the diﬀerence between
prediction and the correct output. Structures that measure performance always appear after those
for measuring surprise. Invasions of the population by such structures in the reverse order were
never observed.
PACS numbers: 05., 84.35.+i, 87.23.Kg

I. INTRODUCTION

In this paper we consider the dynamics of automatic
design of learning algorithms for neural networks. We
use Genetic Programming (GP) as a tool to generate
a sequence of generations of populations of programs
which implement a learning algorithm. Programs at one
generation give rise through cross over and mutations
to oﬀspring programs in the next generation according
to their ﬁtness. The ﬁtness -which deﬁnes the problem-
is related in this study to the eﬃciency of the learning
algorithm implemented by the program. We choose a
measure of eﬃciency based on the ability of general-
ization, related to the expected error of the output on
examples which are statistically independent from the
training set.

Although GP is similar in spirit and actually inspired
by the Genetic Algorithm (GA) of Holland [1], since
both mimic natural evolution, the idea of GP put for-

ward by Koza [2], diﬀers from GA in very important
ways.1 GP deals with programs, represented by strings
of symbols -variables or functional operators- which can
have as inputs diﬀerent types of variables and operators
across the population, as well as along the generations.
In some loose sense GP allows for great variability and
thus for the emergence of more of something that could
be dubbed complexity.

Usually the automatic design of programs has as an
aim the solution of a problem and a measure of how
far a given candidate goes in that direction is given by
the ﬁtness. We are not only interested in ﬁnal results,
but rather the road towards that goal and its charac-
terization are the main issues. We have chosen to study
perceptron learning, a suﬃciently simple learning prob-
lem that can be studied, as far as ﬁnal results are con-
cerned, by analytical means but which presents a wealth
of interesting results. By analyzing the development of
learning algorithms we expect to learn something about
the dynamics along which diﬀerent variable combina-

1

In brief, GA optimizes a function which depends on a pa-
rameter vector by studying the evolution of a population
of such vectors or strings of parameter values, generating
oﬀspring vectors by several operations, such as cross-over,
mutations, etc.

1

tions become useful and invade the population of pro-
grams. We ﬁnd dynamic phase transitions as diﬀerent
functional structures change from being irrelevant to
useful and ﬁnd evidence that point to a strict tempo-
ral order in the sequence of such appearances. Some
structures, though useful at later stages are irrelevant at
ﬁrst and remain so until some other structure is mature
enough and thus potentialize the utility of the former.
The characterization of a population can be made
through the use of several diﬀerent complementary
tools. What we call the phenotypic or functional level
description deals with quantities that measure the ex-
pression of important traits. At this level the program
diﬀerences are irrelevant as long as they give rise to the
implementations of the same function. The main tool,
the phenotypic entropy S describes the distribution of
ﬁtness in the population. At the genotypic or program
level, diﬀerent programs are diﬀerent even if they give
rise to the same numbers, for their potential of gener-
ating new successful programs in the following genera-
tion depends on the particular symbols which exist at
present. We can introduce several genotypic entropies
which describe the distribution of probabilities of sym-
bols, of two contiguous symbols and so on. We will
restrict to dealing with single symbol distributions and
we characterize them by H the genotypic entropy [3].

The crossover of programs, obtained by a cutting and
pasting process described bellow, can be diﬃcult to im-
plement in common programming languages such as C
and Fortran. The major part of the work in GP has
been developed in LISP which is also the case in this
study. We have developed also a protocol for simula-
tion of LISP on a parallel architecture on a cluster of
machines running Linux, which is described in [4].

The paper is organized as follows. In section 2 a brief
description of GP from the very special point of view
which interests us here is followed by a description of the
problem which GP aims at solving. Section 3 presents
the results and concluding remarks can be found in the
last section.

II. THE PROBLEM AND THE METHOD

A. Problem: Learning by a perceptron

The learning problem to be analyzed by the GP must
strike a balance between being complex enough so that
interesting dynamics arises and simple to the point that
details can be understood and simulations performed.
The perceptron meets these demands and has a long
and distinguished history. For an extensive view from
a Statistical Mechanics perspective see [5]. We consider
the realizable teacher-student learning scenario. The
IRN (here obtained
perceptron classiﬁes vectors S
i.i.d from a uniform distribution) in two categories with
S).
labels σJµ =

∈
1 according to the rule σJµ = sgn(J

±

·

2

∈

∈

The objective of the learning dynamics is to determine
IRN from pairs of ex-
the weight or synaptic vector J
amples (Sµ, σBµ) which carry information about a rule.
We restrict ourselves to the simplest case of noiseless
realizable rules, which mean that the labels were un-
corrupted and generated by another perceptron with a
IRN unknown to us. We consider
weight vector B
on-line learning, which means that J will be built se-
quentially by modiﬁcations induced by the arrival of
new pairs of examples. We even concentrate on the
particular form of modulated Hebbian learning, where
the increments of J are described by a modulation func-
Jµ−1 = f σBµSµ/√N . This
tion f , thus ∆Jµ = Jµ −
is not very restrictive as a large fraction of the previ-
ously studied algorithms, both on-line and oﬀ-line, may
be put in a similar way and in the (thermodynamic)
limit of large networks it can represent asymptotically
eﬃcient learning, which even saturate Bayesian bounds.
We deal with questions about the modulation func-
tion, such as : (i) What are the variables upon which
the modulation function depends? (ii) What is the best
function? (iii) In the event that the machine has no ac-
cess to all of the useful variables, which ones can be
left out and which are relevant? That is, in the path
towards the development of a more sophisticated algo-
rithm, the machines at earlier stages may not dispose
all relevant variables, then which are the ones that are
relevant in the earlier stages and which become so later?
Is there any discernible pattern in the order these vari-
ables are incorporated? That we can indeed identify
such time ordering in our simulations is the main result
of this paper.

Related questions have been addressed before [6], see:
about best results and Bayesian bounds [7], for a vari-
ational point of view about the perceptron learning in
[8], about feedforward architectures with hidden units
in [9–11], for drifting rules in [12,13], in an unsupervised
scenario [14], from a more general Bayesian perspective
in [15–17]; in the case of oﬀ-line learning in [7,18]. From
the perspective of time ordering it has been discussed
in [19].

B. Method: Algorithm construction by Genetic
Programming

In this section we describe brieﬂy our implementa-
tion of GP for the problem at hand. We do not con-
sider the evolution of machine architecture, which is left
for future work and just deal with the evolution of the
modulation function.

Conventional GA work manipulating ﬁxed-length
character strings that represent candidate solutions of
a given problem. For many problems, hierarchical com-
puter programs are the most natural representation for
the solution. Since the size and the shape of the pro-
gram that represents the solution are unknown in ad-
vance, the program should have the potential of chang-

ing its size and shape. The aim of GP is getting com-
puters to program themselves by providing a domain
independent way to search the space of possible com-
puter programs, for one that solves a given problem.
The principle that rules GP is, as in GA, the survival
of the ﬁttest.

Starting from a population of randomly created com-
puter programs, the GP operations are used to generate
the population of the next generation. The programs
are ranked by their ﬁtness and then the GP operations
are applied again. These two steps are then iterated.

The most common computer language used in GP is
LISP, therefore we will refer to the population individu-
als as programs or LISP S-expressions indistinctly. We
call Faithful S-expressions (FSEs) a lists of symbols that
do not return an error message when evaluated. Com-
ponents, also called atoms, of the S-expressions can be
either functional operators or variables. The set of all
operators used in the S-expressions is
and the set of
. The choice of these sets depends on
all variables is
the nature of the problem being faced. For instance,
if the solution of a problem can be represented by a
quotient of polynomials,
x
=
{
1
. For example, a FSE is (+ (+ x x) (* x (- x (- x
}
x)))), which is a (non unique) LISP representation of
the function 2x + x2. The simplest FSE is an operator
followed by the appropriate number of variables (two in
the example above). All FSEs have an operator as ﬁrst
element, and following elements should be variables or
FSEs. Unfaithful S-expressions are for instance: (x x),
(+ x *) and (x - x).

+ - * /

and

=

F

F

V

V

{

}

LISP’s most prominent characteristic with regard to
GP is that programs and data have a common form and
are treated in the same manner. This common form is
equivalent to the parse tree for the computer program
and allows to genetically manipulate the parts of the
program (i.e., subtrees of the parse tree).

+

+

+

*

+

+

x

x

x

x

x

x

iﬁcation into the new generation, ensuring the preser-
vation of structures that made them successful. Mu-
tation is implemented by randomly changing an atom
of an individual chosen at random. The new and old
atoms must be of the same kind to ensure faithfulness.
Finally the modiﬁed tree is copied into the new gen-
eration. In order to accelerate the dynamics diﬀerent
mutation rates can be used for diﬀerent atom types. Al-
though there are no sexes associated to the programs,
cross-over can be better described as the sexual GP op-
eration. In our experiments, the ﬁrst parent is chosen
among the reproduced fraction of the population (those
programs that have been copied from the past genera-
tion) by tournament [2]. The second parent is chosen
by tournament among the entire population. An atom
is selected randomly in each parent. The subtrees (or
leaves) with roots in the selected atoms are interchanged
to generate two oﬀsprings.
In order to avoid uncon-
trolled growth if the depth of any of the oﬀsprings is
above a given threshold, the program is deleted.

After a new population is created, the ﬁtness of each
individual is measured and so a new ranking is built.
There is a great freedom in choosing the ﬁtness func-
tion. It is always a macroscopic or phenotypic quantity,
i.e. a function of the expressed characters, and although
it reﬂects the microstructure, it is not a function of the
genetic details of the individual. Errors in the mea-
surement of the ﬁtness have a bearing on the dynamics,
not entirely diﬀerent from the temperature in simulated
annealing.

Our numerical experiments have been performed in
a Pentium III, 800 MHz PC, Linux cluster, using the
strategy described in [4]. The GP parameters used in
the simulation are presented in Table 1. At generation
zero a population of 500 faithful S-expressions is created
at random. The programs have (in agreement with Ta-
ble 1) a maximum depth of 7 nested parenthesis. The
sets used to build the programs are

=

σJµ σBµ h Sµ Jµ}
{

,

V

x

−

x

−

where h = Sµ ·

Jµ/

Jµk
k

, and

−

x

x

−

x

x

=

Psqr Pexp Plog abs +

% p. pN. ev

vv+ vv

,

− ∗

∗

−}

F

{

2x + x 2

4x

FIG. 1. LISP programs as parsing trees before and after
a GP mutation. A randomly selected atom in the parse tree
is changed to another randomly selected atom of the same
type. In this example a multiplication operation is replaced
by an addition.

The GP operations considered in the present work are
asexual reproduction, mutation and cross-over. In the
operation of asexual reproduction a certain fraction of
the top ranked individuals are copied without any mod-

, vv+, and vv

where Psqr, Pexp, Plog, and % are the protected square
root, exponential, logarithm and division; abs, +, -, and
* are the usual absolute value, addition, subtraction and
are the
multiplication; and p., pN., ev
∗
inner product, normalized inner product, the product
of a scalar times a vector, the addition of two vectors
and the subtraction of two vectors respectively. Pro-
tected functions are functions whose deﬁnition domains
have been extended in order to accept a larger set of
arguments. The deﬁnitions of these functions appear in
Table 2.

−

3

+

*

x

x

x

−

x

x

−

x

+

+

+

*

x

x

x

+

x

x

x

x

+

x

−

x

−

x

*

x

*

−

−

x

x

x

2x + x 2

x − 2x 2

2x + 2x 2

x − x 2

FIG. 2. GP cross-over. Two parents are selected from the
population. A random point in each tree is selected. The
branches that grow from the point are interchanged in order
to generate two oﬀsprings.

·

∈

The inner product is the usual inner product among
IRN then the normalized inner prod-
vectors. If x, y
y/N . Other operations involving vectors have
uct is x
to be deﬁned. The ev
takes two arguments, a scalar
x and a vector v, and returns a vector w with compo-
nents wi = x vi. The sum (diﬀerence) of two vectors
) takes two vectors v and w, and returns a
vv+ (vv
vector z, with components zi = vi+wi

wi).

−

∗

When the process of creation of programs is done, be-
fore performing the GP operations to generate the next
generation, the ﬁtness has to be calculated. Because the
programs represent the learning algorithm of a neural
network, a good measure of the learning algorithm per-
formance, should be based on the generalization error,
which measures the probability that the classiﬁcation of
the network σJµ is diﬀerent from the correct label σBµ

(zi = vi−

eg (µ) =

Θ (
h

−

σBµσJµ)

,

iLµ

which in the thermodynamic limit

eg (µ) =

arccos ρ ,

1
π

Jµ·B

where ρ = limN→∞ (cid:16)

kJµkkBk (cid:17) and µ indicates how

many examples have been presented to the network,
which we call the age of the individual. The average is
over training sets of µ pairs of examples. Since the aim
is to obtain algorithms with the smallest possible gener-
alization error, which depends on the age -taken as the
number of examples already to which the network has
been exposed- , we chose a ﬁtness that incorporates the
variation of eg with age and average over age so that
the asymptotic stage is at least as important as earlier
stages. For the kth member of the population

F (k) =

µeg (µ)

P

X
µ=1

a

≤

≤

is the ﬁtness and P is the total number of examples
(maximum age) presented to the network. The popula-
tion is ranked according to ﬁtness and the best 10% are
asexually reproduced into the next generation (accord-
ing to the Reproduction Rate on Table 1). The other
90% is generated by cross-over. The ﬁrst parent is cho-
sen from the best 10% of the population. To do so we
select ﬁrst a number a such that 1
P with a prob-
ability proportional to a (the higher the a the higher the
probability to choose it). a is the age of the individ-
uals that are going to participate in the tournament.
From the best 10% of the population, ten individuals
are selected at random. From comparison of their gen-
eralization error at age a, the individual with smaller
eg(a) is selected for cross-over. To select the second
parent a similar mechanism is applied. Ten individuals
are selected at random from the entire population, and
their generalization errors at age a are compared. The
winner is chosen to mate. To perform the cross-over,
sub-trees of both parents are selected at random. Inter-
nal points (i.e. operators) are selected more frequently
than external points (i.e. variables) in order to make
the individuals grow (see Table 1).2

If either one of the oﬀsprings has a depth bigger than
17, it is deleted. With a mutation rate of 0.01% (one
every 20 generations) a mutation is performed to the
oﬀsprings. Because the pairs Jµ Jµ become rare after
few generations (at the beginning of the simulation, the
learning algorithms that use J are not eﬃcient) we keep
injecting this pair with a rate of 0.2% (at least one in-
dividual per generation receives this pair). Diﬀerent
mutation rates just serve the purpose of accelerating
the dynamics and decrease the time scale of the typical
time that it takes for interesting things to happen. The
process is repeated until the new population reaches the
full size ﬁxed here at 500. To calculate the generaliza-
tion error an average is taken over at least 50 sets of
P
µ=1.
examples

LP =

(Sµ, σµ)
}
{

2

Observe that an operator in a S-expression is always a root
of another S-expression, while a variable is a S-expression by
itself.

4

Parameters
Population Size
Reproduction Rate
Mutation Rate
JJ Mutation Rate
Max. Depth Gen. 0
Max. Depth Gen. G
Prob. Internal Point Select. (cross-over)
Tournament Participants
Vector Sizes
Maximum Number of Training Examples
Maximum Number of Sets of Examples
Slave Processors
β

Values
500
10%
0.01%
0.2%
7
17
90%
10
11
100
50
10
1

TABLE 1. Control parameters for the GP simulation in

our experiments.

Function
(Psqr x)
(Pexp x)
(Plog x)
(% x y) (if (> 1.d-17 (abs y)) 1.d17 (/ x y))

Deﬁnition
(sqrt (abs x))
(exp (min 13.0 x))
(log (max 1.d-17 x))

TABLE 2. Deﬁnition of the protected function as FSEs.
The protected square root is just the square root of the ab-
solute value of its argument. In this manner we extended its

domain into the negatives. The exponential is well deﬁned
in the reals. Although, in order to avoid overﬂows we have
to impose a cut-oﬀ. The protected logarithm has a cut-oﬀ
at a small positive number to extend its domain to the non-
positive numbers. And the protected quotient allows the
division by zero (if the absolute value of the denominator is
smaller than a tiny number the protected quotient returns
a big number, if not it just returns the usual quotient).

III. RESULTS

To characterize the distribution of the ﬁtness across
the population we introduced the normalized ﬁtness, a
measure of the fraction of the total (exponential) ﬁt-
ness that an individual has, in a way analogous to the
canonical state at temperature β (although the system
is not in equilibrium with any temperature reservoir):

n(i)
β =

exp
(cid:0)−
M
k=1 exp

βF (i)

(cid:1)
βF (k)

,

(cid:1)

(cid:0)−

P

where F (k) is the ﬁtness measure of th kth individual of
the population. Note that smaller values of the ﬁtness
are associated to better performances. The use of the
exponential ampliﬁes the importance of the individuals
with better performance and β was kept equal to 1. We
introduce the entropy of the normalized ﬁtness

)
0
(

F

0.5

0.6

0.55

0.45

0.4

0

t
n
e
n
o
p
x
E

1.3

1.05

0.8

0.55

0.3

200

600

800

0

200

600

800

400
G

BOG’s exponent
Mean exponent

400
G

FIG. 3. (Left) Fitness of the best-of-generation individual of the population vs. the number of generations. A sudden
change takes place around 380 generations. (Right) The exponent of algebraic decay of eg. Upper curves: BOG, lower
curves: population average.

5

S =

M

−

X
k=1

n(k)
β

ln (cid:16)n(k)

β (cid:17) ,

Although the history of the population varies from run
to run, we have identiﬁed some systematic occurrences.

a function of the expressed characters of the population
(ﬁtness), thus dubbed the phenotypic entropy or Ph-
entropy. Note that this entropy is largest when all the
members of a population have the same ﬁtness and that
the appearance of a distinguished individual, for better
or worst, is signaled by a decrease in Ph-entropy.

Each FSE in the population has a well deﬁned length
λ(k), i.e the number of atoms (operators and variables)
that make it up. We deﬁne the mean length L as

L =

λ(k) .

1
M

M

X
k=1

6.2

6.1

S

6

5.9

5.8

0

To characterize the internal structure of the pro-
grams, we estimate for each position i the probability
that symbol sq (a variable or an operator) appears at
i) by measuring the frequency over all
position i, ω (sq|
the population. The genotypic entropy (or G-entropy)
which is a function of the micro structure of the indi-
viduals in the population is then deﬁned as [3]

H =

− X
sq∈Q

X
i

ω (sq|

i) log|Q| ω (sq|

i) ,

where

=

Q

F ∪ V

Several numerical experiments, starting from diﬀer-
ent random seeds, have been performed using the GP
described above.

ω

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

(0)
λ

250

200

150

100

 50

  0

0            100          200          300          400          500          600

G

FIG. 4. Color coded bar graph of the best-in-generation
(BOG) individual. The time G is in the horizontal axis
is measured in generations and the length of the program
is in the vertical axis. Color of pixel at coordinates (G, i)
codes for the frequency ω (sq|i), according to the color scale,
at which the symbol sq (which is the i-th atom of the
best-of-generation FSE) appears at position i, at generation
G.

6

200

600

800

400

G

FIG. 5. Phenotypic entropy as a function of the number

of generations G.

In most of the runs we have found a drastic change in
behavior which can be well described as a phase tran-
sition, although we have neither taken thermodynamic
limits associated to inﬁnite network dimension nor in-
ﬁnite population. The time of the occurrence varied
widely from one simulation to other.
In some simu-
lations, the population did not undergo the transition
but it could well happen that we just did not wait long
enough. In what follows we consider an illustrative run
which presents clearly some features that are typical of
other runs. We found a dramatic change of behavior
around generation 380 that can be seen by using sev-
eral diﬀerent signatures. Figure 3 (left) shows the ﬁt-
ness of the most adapted program or best-of-generation
(BOG) as a function of time. The exponent that gov-
erns the decay of eg shows a sharp change, specially if
the population average is compared to that of the BOG.
Finite size errors are responsible for the fact that expo-
nents larger than one can be found. To understand how
representative of the whole population is the BOG we
composed a color coded bar graph (see ﬁg. 4) where
each vertical bar represents the BOG program written
as a string of symbols, time is measured in generations
in the horizontal axis. At the position of each symbol
in the program a colored square represents the empiric
probability of the symbol in the population. Note that
quite rapidly an initial symbol is predominant in the
population. This is invariantly found in all runs and
it is always a symbol that ensures that the modulation
function is positive, for other wise the learning would
be anti-Hebbian and ineﬃcient. The initial part of the
code is very robust and thus is shared by almost all the
population. There is an obvious change in the length
of the BOG which will be considered bellow, but notice
before the transition the upper part is moderately com-
mon (green) and after the transition the upper part is
more variable or less frequent. These changes can also
be monitored by the entropies.

400

300

200

100

H
 
d
n
a
L

 

||J||

5

4

3

2

1

Mean Length
G−Entropy

80

60

40

20

0
−20 −10 0

10 20

Residuals

200

150

H

100

50

0

    0 < G < 365
365 < G < 890

200

L

0

0

400
G

200

600

800

0

100

300

FIG. 6. (Left) Genotypic entropy and mean length as functions of the number of generations G. The transition can be
seen by the sharp change around G = 380. (Right) H vs. L. Two linear ﬁts are shown for data before the transition
(crosses) and data after the transition (circles). To see that two linear ﬁts are necessary we did a single linear ﬁt of the
whole data set and plotted (inset) the histograms of the residuals to the single linear model. The two histograms are for
data before and after the transition respectively and the separation of the two peaks lends support to the modeling by two
linear regimes.

G-entropy increases and thereby makes the BOG less
frequent than immediately after the transition. Now
the ﬁtness distribution is sharper around the BOG and
therefore the Ph-entropy decreases and oscillates over a
wider range.

The G-entropy and the mean length are linearly cor-
related. This is natural since G-entropy, as deﬁned
should be extensive. What is not as expected is the
fact that there are two distinct linear regimes before
and after the transition. To see this we did a linear
ﬁt to the whole data set and plotted an histogram of
the residuals, that is the diﬀerence between the actual
value of a data point and the corresponding value of
the linear model. The two histograms in the inset of
ﬁg. 6 show clearly a systematic error for the single lin-
ear model. These results prove the existence of a quite
sharp transition, but do not hint at the nature of the
changes in the individual programs nor the reasons for
the improvements in ﬁtness. The question is trying to
understand what happened from a functional point of
view that led to such an improvement in generalization
ability.

There are two quantities or functional structures that
are of interest both in a quantitative and qualitative
analysis of the of learning algorithms. The ﬁrst, which
can be associated to the product hσBµ can be func-
tionally described as quantifying a measure of surprise.
This is because if hσBµ > 0 the network will classify cor-
rectly the example with classiﬁcation label σBµ, while

0

0.1

0.3

0.4

0.2
eg

FIG. 7. Typical behavior for late stage generation: the
length of the weight vector ||J||
increases monotonically
when the error of generalization decreases, thus it can be
used as a measure of the experience of the individual or
of its performance in solving the classiﬁcation problem. It
leads to eﬃcient annealing of the learning rates.

Both entropies (phenotypic and genotypic) present
changes about the same time (ﬁgs. 5 and 6). The Ph-
entropy shows a much larger variability after the tran-
sition, the G-entropy and the mean length both have
an almost discontinuous break at the transition. The
fact that the Ph-entropy has a decreasing trend after
the transition can be attributed to the fact that the

7

In ﬁg. 7 we show a graph of

if hσBµ < 0, the classiﬁcation is wrong. Thus it gives
a signal of how wrong or correct was the classiﬁcation
and also how stable that classiﬁcation is under changes
of the weight vector. This is obviously an important
factor to take into account while incorporating the in-
formation in a given example. The second functional
structure we will concentrate on is something that can
estimate the performance or acquired experience of the
network in the implementation of the rule. This, if
properly used is akin to annealing of the learning rate
or of the functional annealing in learning algorithms.
This can be implemented by using the length of the
weight vector Jµ.
J
k
k
as a function of the generalization error for a program
with a good ﬁtness in the later stages of the simulation.
The monotonic behavior is a typical result. It can be
shown, at least in the thermodynamic limit, that for
algorithms which do not measure surprise their gener-
alization error decays as µ−1/2 and for them annealing
is useless. Learning algorithms that use surprise have a
µ−1
better performance
and algorithms that use
both surprise and annealing by experience have an even
better performance since can have smaller coeﬃcients of
µ−1. A crude measure of the capacity of a population
of using a functional structure may be given by the fre-
quency that the combination of variables is found. This
is admittedly crude since the position in the program
determines whether it is useful or not. On the other
hand the absence of such combination does not rule out
the possibility that some other combination is doing the
job in a more cumbersome manner. In ﬁg. 8 we plot the
density of pairs hσBµ (surprise) and the density of pairs
JJ (performance) in the entire population, as functions
of the number of generations.

eg ∝
(cid:0)

(cid:1)

0.1

0.075

0.05

0.025

e
c
n
a
m
r
o
f
r
e
P
d
n
a
 
e
s
i
r
p
r
u
S

 

Surprise
Performance

0
−20

180

380

580

780

G

FIG. 8. Typical behavior of the density of pairs hσBµ and
Jµ Jµ as functions of time G measured in generations. No-
tice the sharp rise at the beginning of the density of hσBµ
surprise measuring pairs and the later rise of Jµ Jµ , per-
formance measuring pairs. The time ordering is robust and
was never seen in the reverse order.

It is possible to observe a fast change in the frequency of

8

pairs of symbols related surprise before 20 generations.
JJ pairs are almost immediately all but extinguished
from the population. hσBµ pairs are distributed very
frequently and its presence oscillates across the popu-
lation and through the generations, while JJ pairs in-
troduced by mutations are not able to invade the pop-
ulation. At the time of the transition, surprise is be-
ing correctly measured and now the appearance of JJ
leads to an improvement in ﬁtness since it leads to an
estimate of the generalization error and permits the im-
plementation of correct annealing schedules. This suc-
It is reason-
cessful strategy invades the population.
able to associate the improvement in the ﬁtness with
the emergent use of experience by the elements of the
population. Notice that injections through mutations
of performance structures were non invading before the
transition. Of course this can be explained by claim-
ing that not every kind of annealing is beneﬁcial but
most important, before surprise is measured correctly,
no annealing scheme is useful, and therefore individu-
als which could measure JJ did not beneﬁt from such
knowledge.

The sequence of symbols of the BOG individual be-
fore and after the change in the density of pairs JJ mir-
ror that increase. In ﬁg. 9 we present the most adapted
individual at generations 300, 350, 400 and 450. Just
before the transition there is no pair JJ present in the
program (the two ﬁrst programs). After the transition
the best individual suﬀers a decrease in size and several
pairs JJ appear. According to the color scale, red sym-
bols are extremely frequent in the population at that
position, green symbols are just frequent at that posi-
tion, and violet are quite unlikely to be found. We can
see that after the transition, the third program, presents
symbols mostly in the violet. 50 generations later there
are islands of green in the BOG. That means that the
genetic character of the best individual has invaded the
population.

A more general analysis of the density of pairs can
be done with the help of ﬁg. 10. In these pictures we
present the relative frequencies at which each possible
pair appear in the population. The vertical axis repre-
sents the ﬁrst element of the pair, the horizontal axis
the second element. The size of the white squares rep-
resent the frequency of the pair, relative to the most
frequent pair (represented by the largest square in each
picture). In panel (a) we present the density of pairs at
generation 300, (b) corresponds to generation 350, (c)
to generation 400 and (d) to generation 450. In (a) and
(b) there are no pairs JJ. The most frequent pair is the
combination σBµσBµ, which is just a 1, but not quite
since it can evolve into diﬀerent directions. After the
transition, in panels (c) and (d), this pair remains the
most frequent, but important changes have happened.
There are small white squares for the pair JJ repre-
senting the emergence of the use of experience by the
learning algorithms.

+

-

+

+

-

-

*

+

*

%

+

%

+

-

+

*

-

+

J

J

-

-

*

-

+

+

-

J

%

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

-

+

+

-

*

J

%

J

J

%

PN.

%

%

%

+

*

+

+

-

*

J

J

+

+

-

+

J

H

+

+

+

+

-

-

-

%

%

+

*

ABS %

% ABS

% ABS

-

%

%

SIGB +

SIGB SIGB +

+

SIGB +

SIGB SIGB SIGB SIGB

-

SIGB +

%

SIGB

-

H SIGB +

+

-

H SIGB +

+

SIGB SIGB SIGB SIGB +

SIGB SIGB +

SIGB SIGB +

SIGB SIGB +

SIGB SIGB

H SIGB +

SIGB SIGB

% ABS % ABS

-

H SIGB SIGJ

SIGB SIGB +

SIGB SIGB

H SIGB SIGB SIGB

-

H SIGB +

+

SIGB %

SIGB %

H SIGB SIGJ

-

H SIGB +

SIGB SIGB SIGB %

%

+

SIGB +

+

SIGB SIGB SIGB

H SIGB SIGJ

SIGB SIGB SIGB SIGB %

+

SIGB SIGB SIGJ

SIGB +

SIGB SIGB SIGB

SIGB +

SIGB %

SIGB +

SIGB SIGB +

SIGB SIGB SIGB +

+

SIGB +

SIGB SIGB +

+

SIGB SIGB +

SIGB SIGB +

SIGB SIGB SIGB +

+

+

SIGB

H SIGB +

+

SIGB SIGB SIGB SIGB +

+

SIGB SIGB +

SIGB SIGB +

+

+

SIGB SIGB +

SIGB +

SIGB +

SIGB SIGB

H SIGB +

SIGB SIGB +

+

SIGB SIGB SIGB +

+

SIGB SIGB

-

H SIGB +

-

H SIGB

+

%

+

*

+

+

+

+

+

-

+

%

%

%

%

H

+

*

%

+

-

+

-

-

+

+

SIGJ

+

SIGB SIGB SIGB SIGB +

+

SIGB SIGB SIGB SIGB

ABS %

% ABS

-

% ABS

-

%

%

SIGB SIGB +

SIGB SIGB SIGB SIGB SIGB

H SIGB +

-

%

SIGB +

+

SIGB SIGB SIGB +

+

+

SIGB SIGB SIGB SIGB +

SIGB

-

H

SIGB SIGB +

+

SIGB SIGB +

SIGB +

SIGB SIGB +

SIGB SIGB +

SIGB +

+

SIGB SIGB SIGB

H SIGB +

SIGB SIGB

% ABS % ABS

H SIGB SIGJ

+

SIGB SIGB SIGB

-

H SIGB SIGB SIGB

-

H SIGB +

+

SIGB %

%

SIGB %

*

H SIGB SIGJ

H SIGB +

SIGB +

SIGB +

SIGB SIGB SIGB %

-

H SIGB %

H SIGB SIGJ

*

H SIGB SIGJ SIGB SIGB %

%

-

%

%

%

*

+

*

SIGB SIGB +

SIGB SIGB SIGB SIGB SIGB SIGB +

SIGB %

% SIGB +

+

%

+

+

SIGB %

H SIGB

H SIGB +

+

+

SIGB SIGB SIGB SIGB SIGB +

%

+

SIGB SIGB +

SIGB SIGB SIGB +

SIGB +

SIGB +

SIGB SIGB SIGB +

+

SIGB SIGB +

SIGB SIGB +

SIGB SIGB +

+

SIGB SIGB SIGB

+

SIGB +

SIGB SIGB +

SIGB

-

H SIGB +

+

SIGB SIGB +

SIGB

H SIGB

%

*

H SIGB SIGJ

+

SIGB SIGB SIGB

-

H SIGB +

+

SIGB SIGB +

SIGB %

%

%

SIGB %

H SIGB SIGJ

H SIGB +

SIGB SIGB SIGB +

H SIGB SIGB SIGB

ABS %

% ABS

-

% ABS

%

%

% PN.

J

J

SIGB +

+

SIGB SIGB SIGB SIGB

H SIGB +

H SIGB SIGJ

SIGB SIGB SIGB +

SIGB SIGB

% ABS % ABS

%

SIGB SIGJ

+

SIGB % PN.

J

J

SIGB SIGB

H SIGB SIGB SIGB

H SIGB +

H

+

%

SIGB

%

SIGB %

-

H SIGB SIGJ

-

H SIGB +

SIGB +

SIGB +

-

H SIGB SIGB SIGB

% SIGB %

-

H SIGB SIGJ

+

%

%

% PN.

J

J

SIGB +

+

SIGB SIGB SIGB SIGB SIGB %

+

SIGB SIGB +

SIGB SIGB SIGB SIGB SIGB +

SIGB %

%

% PN.

J

J

SIGB +

+

SIGB SIGB SIGB SIGB

+

+

-

+

*

ABS %

% ABS

% ABS

-

%

%

% PN.

J

SIGB +

SIGB SIGB SIGB

-

H

%

SIGB +

SIGB SIGB SIGB SIGB +

H SIGB SIGJ

+

%

% SIGB %

J

-

%

%

%

*

+

SIGB +

SIGB SIGB SIGB SIGB SIGB %

SIGB SIGB SIGB % PN.

SIGB SIGB +

SIGB SIGB SIGB SIGB +

SIGB SIGB

-

% ABS % ABS

-

% PN.

SIGB +

SIGB SIGB SIGB

-

H SIGB SIGB SIGB

-

H SIGB +

SIGB %

%

%

H SIGB SIGJ

-

H SIGB +

-

H SIGB SIGJ

SIGB %

% SIGB +

SIGB SIGB SIGB SIGB

SIGB SIGB SIGB %

% SIGB %

H SIGB SIGJ

+

%

%

% PN.

J

J

SIGB +

+

SIGB

SIGB SIGB PN.

SIGB SIGB %

SIGB SIGB +

SIGB SIGB SIGB SIGB SIGB +

SIGB %

%

% PN.

SIGB +

SIGB SIGB % ABS

% PN.

SIGB SIGJ SIGB %

%

%

% PN.

%

J

%

J

+

PN.

J

J

SIGB PN.

SIGB +

SIGB SIGB SIGB

SIGB

*

SIGB +

J

+

J

SIGB SIGB SIGB +

% PN.

J

J

SIGB +

+

%

% SIGB %

SIGB +

SIGB SIGB SIGB

SIGB SIGB ABS

-

%

%

% PN.

J

J

SIGB +

SIGB SIGB SIGB

% PN.

J

SIGB +

%

-

% PN.

H

%

J

%

%

%

*

-

*

-

+

%

%

%

%

% PN.

% PN.

SIGB SIGJ

SIGB %

+

*

J

-

+

J

J

%

J

H

+

SIGB SIGB SIGB SIGB SIGB

*

+

+

+

+

+

%

-

-

+

%

+

%

+

+

+

+

*

+

+

-

-

+

-

-

+

-

*

*

-

+

-

-

*

J

J

+

J

+

J

+

-

-

-

+

H

-

%

-

-

-

J

FIG. 9. The strings of symbols are the programs best-of-generation at generations 300, 350, 400 and 450. The colors

represent the frequencies ω (sq|i), according to the color scale.

The modulation functions of BOG’s at diﬀerent
stages of the evolution can also be understood along
the line of surprise-performance analysis. At earlier
stages the BOG is unable to use surprise. Although
surprise functional structures are found throughout the
population, their incorrect use makes the BOG an an-
nealed Hebbian algorithm.
It is known that anneal-
ing will not improve the Hebbian learning and the fre-

quency of performance functional structures decreases
It will only appear in very mod-
until it vanishes.
est ways through mutation and, repeatedly individu-
als which use it becomes extinct. Later on surprise is
ﬁnally well accounted for and correctly classiﬁed exam-
ples cause typically smaller Hebbian corrections than
those incorrectly classiﬁed. At that point the correct
use of surprise potentializes the beneﬁcial use of func-

9

tional structures that measure performance. Then a
correctly annealed algorithm emerges that resembles
quite closely the modulation functions found through
Bayesian or variational approaches (ﬁg. 11).

(a)

(b)

r
q
s
P

p
x
e
P

g
o
P

l

s
b
a

 

 

+

 
 

 
-
 
 

 
*
 
 

 

%

 
 

 
.

p

 

.

N
p

 

*
v
e

 

+
v
v

 

-
v
v

 

B
g
S

i

J
g
S

i

 

h

 
 

 

S

 
 

 

J

 
 

r
q
s
P

p
x
e
P

g
o
P

l

s
b
a

 

 

+

 
 

 
-
 
 

 
*
 
 

 

%

 
 

 
.

p

 

.

N
p

 

*
v
e

 

+
v
v

 

-
v
v

 

B
g
S

i

J
g
S

i

 

h

 
 

 

S

 
 

 

J

 
 

(c)

(d)

Psqr
Pexp
Plog

 abs
  + 
  - 

  * 
  % 
 p. 
 pN.

 ev*
 vv+
 vv-

SigB
SigJ
  h 
  S 

  J 

Psqr

Pexp
Plog
 abs

  + 
  - 
  * 
  % 

 p. 
 pN.
 ev*

 vv+
 vv-
SigB

SigJ
  h 
  S 
  J 

r
q
s
P

p
x
e
P

g
o
P

l

s
b
a
 

 

+

 
 

 
-
 
 

 
*
 
 

 

%

 
 

 
.

p

 

.

N
p

 

*
v
e
 

+
v
v
 

-
v
v
 

B
g
S

i

J
g
S

i

 

h

 
 

 

S

 
 

 
J
 
 

r
q
s
P

p
x
e
P

g
o
P

l

s
b
a
 

 

+

 
 

 
-
 
 

 
*
 
 

 

%

 
 

 
.

p

 

.

N
p

 

*
v
e
 

+
v
v
 

-
v
v
 

B
g
S

i

J
g
S

i

 

h

 
 

 

S

 
 

 
J
 
 

FIG. 10. Density of pairs for generations 300 (a), 350 (b),
400 (c), and 450 (d). In all the cases the most frequent pair
is σBµσBµ (SigB SigB). Only in the last two panels the pair
JJ appears.

IV. CONCLUSIONS

Evolutionary programming techniques provide the
means to automatically design programs which solve
certain class of problems.
In this paper, however we
were not interested in the ﬁnal result, the problem that
GP was set out to solve has been previously analyzed
from many angles and a detailed understanding of on-
line learning in perceptrons has been achieved. Rather
we concentrated on the dynamics of evolution and have
detected dynamical changes in the behavior of the GP
solutions that we have not hesitated in dubbing dynam-
ical transitions. This is not a conventional phase tran-
sition associated to singularities arising in the thermo-
dynamic limit. A few runs failed to present the transi-
tion, maybe because of time limitations, but it was seen
in many diﬀerent runs. Some features were never re-
producible but others were present in every transition.
As examples of those features that depend upon con-
tingencies we include the number of generations before
the transition takes place, the width of the transitions
(some were just about ten generations wide, others took
several tens of generations) and the result of the GP, i.e.
the program that implements the best learning algo-
rithm. These are mainly important from a constructive
point of view when the solution to the problem is the
main concern. We tried, instead to identify robust fea-
tures which can be conﬁdently expected to occur every
time the transition takes place. In serving such purpose
we have characterized the dynamics by looking at Ph-
and G-entropies which give a picture of the distribution
of phenotypic ﬁtness and functional or symbolic struc-
ture respectively. Large entropic ﬂuctuations are well
described by power laws.

||J|| = 40

||J|| = 30

||J|| = 20

||J|| = 10

||J|| =  0

5

4

3

2

1

0

   

30

20

10

0

||J|| = 40

||J|| = 30

||J|| = 20

||J|| = 10

||J|| = 0

−5 −4 −3 −2 −1 0 1 2 3 4 5
hσ

Bµ

−5 −4 −3 −2 −1 0 1 2 3 4 5
hσ

Bµ

−5 −4 −3 −2 −1 0 1 2 3 4 5
hσ

Bµ

FIG. 11. Modulation functions. (Left) Early stage where surprise is not measured and annealing by experience is ineﬀec-
tive. (Center) Intermediate stage, now surprise is used but annealing by experience has been lost. (Right) Late stage, after
the transition, where surprise through the measurement of hσBµ and annealing kJk are correctly implemented.

Psqr
Pexp
Plog

 abs
  + 
  - 

  * 
  % 
 p. 
 pN.

 ev*
 vv+
 vv-

SigB
SigJ
  h 
  S 

  J 

Psqr

Pexp
Plog
 abs

  + 
  - 
  * 
  % 

 p. 
 pN.
 ev*

 vv+
 vv-
SigB

SigJ
  h 
  S 
  J 

f

25

20

15

10

5

0

10

The conformation diagram gives a bird’s eye view of
the relation of the BOG and the frequency of symbols
in the population as well as its length The main robust
feature can be identiﬁed once the transition has been
understood from a functional point of view, in terms of
two concepts: the surprise that newly arrived informa-
tion elicits and how such information should be taken
into account based on how much experience the network
has in solving the task at hand. A temporal order can
be identiﬁed in every transition. It was never found oth-
erwise. Performance can be useful only after surprise is
measured correctly.

There are several possible extensions of this prob-
lem. From a biological point of view there is a sug-
gestive similarity with the time order in which certain
structures responsible for measuring surprise and per-
formance have appeared. Will this order be found in
more complex artiﬁcial settings?
Is this biologically
signiﬁcant? Can it be extended to other functional
structures? It should also be quite interesting to fur-
ther analyze phase transitions in the automatic design
of programs.

The simulations described here were done on a clus-
ter made possible through the eﬀorts of J. L. deLyra, C.
E. I. Carneiro and coworkers. The cluster’s construc-
tion was partially supported by FAPESP and CNPq.
JPN received ﬁnancial support from FAPESP and NC
received partial support from CNPq. Discussions with
Osame Kinouchi and Mauro Copelli where important
during the earlier stages of this work.

[1] J. H. Holland “Adaptations in Natural and Artiﬁcial
Systems: An Introductory Analysis with Applications
to Biology, Control and Artiﬁcial Intelligence” U. of
Michigan Press, Ann Arbor (1975).

[2] J. R. Koza “Genetic Programming: on the Program-
ming of Computers by Means of Natural Selection”,
MIT Press (1992).

[3] This is similar to that introduced in C. Adami, C. Ofria,
and T. C. Collier, Proc. Nat. sci. USA 97, 4463 (2000),
but not restricted to ﬁxed size program length.

[4] J. P. Neirotti

and N. Caticha,

available

at

http://www.fge.if.usp/˜nestor/.

[5] A. Engel and C. Van den Broeck , “Statistical Mechan-
ics of Learning”, Cambridge University Press (2000).

[6] S. Amari, IEEE Transactions EC-16, 299 (1967).
[7] M. Opper and D. Haussler, Phys. Rev. Lett. 66, 2677

[8] O. Kinouchi and N. Caticha, J. Phys. A: Math. and

[9] M. Copelli and N. Caticha, J. Phys. A: Math. and Gen.

[10] R. Vicente and N. Caticha, J. Phys. A: Math. and Gen.

[11] R. Simonetti and N. Caticha, J. Phys. A: Math. and

[12] M. Biehl and Schwarze, J. Phys. A: Math. and Gen. 20,

(1991).

Gen. 25, 6243 (1992).

28, 1615 (1994).

30, L599 (1997).

Gen. 29, 6243 (1996).

733 (1993).

Gen. 26, 6161 (1993).

76, 8874 (1996).

[14] C. Van den Broeck and P. Reiman, Phys. Rev. Lett.

[15] M. Opper, Phys. Rev. Lett. 77, 4671 (1996).
[16] M. Opper in “On-line learning in Neural Networks”, pg.
363, D. Saad Ed., Cambridge University Press (1998).
[17] S. Sara and O. Winther in “On-line learning in Neural
Networks”, pg. 379, D. Saad Ed., Cambridge University
Press (1998).

[18] O. Kinouchi and N. Caticha, Phys. Rev. E 54, R54

[19] N. Caticha and O. Kinouchi, Philosophical Magazine B

(1996).

77, 1565 (1998).

ACKNOWLEDGEMENTS

[13] O. Kinouchi and N. Caticha, J. Phys. A: Math. and

11


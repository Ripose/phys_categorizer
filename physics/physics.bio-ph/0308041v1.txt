3
0
0
2
 
g
u
A
 
1
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
1
4
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Ensembles of Protein Molecules as Statistical Analog Computers ∗

Victor Eliashberg †

Avel Electronics, Palo Alto, California, www.brain0.com

Abstract

1 Introduction

A class of analog computers built from large numbers
of microscopic probabilistic machines is discussed. It
is postulated that such computers are implemented in
biological systems as ensembles of protein molecules.
The formalism is based on an abstract computa-
tional model referred to as Protein Molecule Machine
(PMM). A PMM is a continuous-time ﬁrst-order
Markov system with real input and output vectors, a
ﬁnite set of discrete states, and the input-dependent
conditional probability densities of state transitions.
The output of a PMM is a function of its input and
state. The components of input vector, called gen-
eralized potentials, can be interpreted as membrane
potential, and concentrations of neurotransmitters.
The components of output vector, called generalized
currents, can represent ion currents, and the ﬂows of
second messengers. An Ensemble of PMMs (EPMM)
is a set of independent identical PMMs with the same
input vector, and the output vector equal to the sum
of output vectors of individual PMMs. The paper
suggests that much more complex computational re-
sources are available at the level of individual neurons
than those used in the traditional computational the-
ories of neural networks. There is simply not enough
neurons in the brain to replace the discussed statisti-
cal molecular computations by the statistical neural
network computations.

∗in press
†visit the web site www.brain0.com for information

1

After the classical work of Hodgkin and Huxley [6], it
is widely recognized that the conformational changes
in the sodium and potassium channels account for the
generation of nerve spike. In this speciﬁc case, the
time constants of the corresponding temporal pro-
cesses are rather small (on the order of a few mil-
liseconds).
It is known that in some other cases
(such as the ligand-gated channels) the time con-
stants associated with conformational changes in pro-
tein molecules can have much larger values [1, 5, 9].
The growing body of evidence suggests that such
slower conformational changes have direct behavioral
implications [7, 8, 2]. That is, the dynamical compu-
tations performed by ensembles of protein molecules
at the level of individual cells play important role in
complex neuro-computing processes.

An attempt to formally connect some eﬀects of
cellular dynamics with statistical dynamics of con-
formations of membrane proteins was made in [4].
The present paper discusses a generalization of this
formalism. The approach is based on an abstract
computational model referred to as Protein Molecule
Machine (PMM). The name expresses the hypothesis
that such microscopic machines are implemented in
biological neural networks as protein molecules. A
PMM is a continuous-time ﬁrst-order Markov system
with real input and output vectors, a ﬁnite set of
discrete states, and the input-dependent conditional
probability densities of state transitions. The output
of a PMM is a function of its input and state.

The components of input vector, called generalized
potentials, can be interpreted as membrane potential,
and concentrations of neurotransmitters. The com-

ponents of output vector, called generalized currents,
can be viewed as ion currents, and the ﬂows of second
messengers.

An Ensemble of PMMs (EPMM) is a set of inde-
pendent identical PMMs with the same input vec-
tor, and the output vector equal to the sum of out-
put vectors of individual PMMs. The paper explains
how interacting EPMMs can work as robust statisti-
cal analog computers performing a variety of complex
computations at the level of a single cell.

The EPMM formalism suggests that much more
computational resources are available at the level
of a single neuron than is postulated in traditional
computational theories of neural networks.
It was
previously shown [2, 3] that such cellular computa-
tional resources are needed for the implementation of
context-sensitive associative memories (CSAM) ca-
pable of producing various eﬀects of working memory
and temporal context.

A computer program employing the discussed for-
malism was developed. The program, called CHAN-
NELS, allows the user to simulate the dynamics of
a cell with up to ten diﬀerent voltage-gated chan-
nels, each channel having up to eighteen states. Two
simulation modes are supported:
the Monte-Carlo
mode (for the number of molecules from 1 to 10000),
and the continuous mode (for the inﬁnite number of
molecules). New program capable of handling more
complex types of PMMs is under development. (Visit
the web site www.brain0.com for more information.)

The rest of the paper consists of the following sec-
tions:

2. Abstract Model of Protein Molecule Machine

(PMM)

3. Example: Voltage-Gated Ion Channel as a PMM

4. Abstract Model of Ensemble of Protein Molecule

Machines (EPMM)

5. EPMM as a Robust Analog Computer

6. Replacing Connections with Probabilities

7. Examples of Computer Simulation

8. EPMM as a Distributed State Machine (DSM)

2

9. Why Does the Brain Need Statistical Molecular

Computations?

10. Summary

2 Abstract Model of Protein
Molecule Machine (PMM)

A Protein Molecule Machine (PMM) is an ab-
stract probabilistic computing system (X, Y, S, α, ω),
where

• X and Y are the sets of real input and output

vectors, respectively

• S= {s0, ..sn−1} is a ﬁnite set of states

• α : X × S × S → R′ is a function describing
the input-dependent conditional probability den-
sities of state transitions, where α(x, si, sj)dt is
the conditional probability of transfer from state
sj to state si during time interval dt, where
x ∈ X is the value of input, and R′ is the set
of non-negative real numbers. The components
of x are called generalized potentials. They can
be interpreted as membrane potential, and con-
centrations of diﬀerent neurotransmitters.

• ω : X × S → Y is a function describing output.
The components of y are called generalized cur-
rents. They can be interpreted as ion currents,
and the ﬂows of second messengers.

Let x ∈ X, y ∈ Y, s ∈ S be, respectively, the values
of input, output, and state at time t, and let Pi be
the probability that s = si. The work of a PMM is
described as follows:

=

α(x, si, sj)Pj − Pi

α(x, sj , si)

(1)

Xj6=i

dPi
dt

Xj6=i

at t = 0

Pi = 1

n−1

Xi=0

y = ω(x, s)

(2)

(3)

proteins) can be treated as PMMs. That is, at this
level, the exact biophysical and biochemical mecha-
nisms are not important. What is important are the
properties of ion channels as abstract machines.

This situation can be meaningfully compared with
the general relationship between statistical physics
and thermodynamics. Only some properties of
molecules of a gas (e.g., the number of degrees of free-
dom) are important at the level of thermodynamics.
Similarly, only some properties of protein molecules
are important at the level of statistical computations
implemented by the ensembles of such molecules.

The general structure of a voltage-gated ion chan-
nel is shown schematically in Figure 2a. Figures 2b
and 2c show how this channel can be represented as
a PMM. In this example the PMM has ﬁve states
s ∈ {0, 1, ..4}, a single input x = V (the membrane
potential) and a single output y = I (the ion cur-
rent). Using the Goldman-Hodgkin-Katz (GHK) cur-
rent equation we have the following expression for the
output function ω(x, s).

Ij = ω(V, j) =

pjz2F V ′(Cin − Coute−zV ′
1 − e−zV ′

)

(7)

where

x = V

• Ij is the ion current in state s = j with input

Figure 2: Ion channel as a PMM

3

Figure 1: Internal structure of PMM

Summing the right and the left parts of (1) over
i = 0, ..n − 1 yields

d(

n−1
i=0 Pi)
dt

P

= 0

so the condition (2) holds for any t.
The internal structure of a PMM is shown in Fig-
ure 1, where dpij is the probability of transition from
state sj to state si during time interval dt. The gray
circle indicates the current state s = si. The output
y = ω(x, s) is a function of input and the current
state.
For the probability of transition from state sj to state
si we have

dpij = α(x, si, sj)Pjdt

It follows from (1) that

dPi =

(dpij − dpji)

Xj6=i

(4)

(5)

(6)

3 Example: Voltage-Gated Ion

Channel as a PMM

Ion channels are studied by many diﬀerent disci-
plines: biophysics, protein chemistry, molecular ge-
netics, cell biology and others (see extensive bibli-
ography in [5]). This paper is concerned with the
information processing (computational) possibilities
of ion channels.

I postulate that, at the information processing
level, ion channels (as well as some other membrane

• pj [cm/sec] is the permeability of the channel in

state s = j

• z is the valence of the ion (z = 1 for K + and

N a+, z = 2 for Ca++)

• F = 9.6484 ·104 [C/mol] is the Faraday constant

• V ′ = V F

RT is the ratio of membrane potential to
the thermodynamic potential, where T [K] is the
absolute temperature, R = 8.3144 [J/K · mol]
is the gas constant

• Cin and Cout [mol] are the cytoplasmic and ex-
tracellular concentrations of the ion, respectively

One can make diﬀerent assumptions about the func-
tion α(x, sj , si), describing the conditional probabil-
ity densities of state transitions. It is convenient to
represent this function as a matrix of voltage depen-
dent coeﬃcients aij(V ).

a00(V ) .. a0j(V ) .. a0m(V )

α =

ai0(V ) .. aij (V ) .. aim(V )

(8)

















where m = n − 1. Note that the diagonal elements of
this matrix are not used in equation (1).

In the model of spike generation discussed in [4]
both sodium, N a+, and potassium, K + channels
were treated as PMMs with ﬁve states shown in Fig-
ure 2. Coeﬃcients a10, a21, a32 where assumed to
be sigmoid functions of membrane potential, and co-
eﬃcients a43 and a04 - constant. In the case of the
sodium channel, s = 3 was used as a high permeabil-
ity state, and s = 4 was used as inactive state. In
the case of potassium channel, s = 3 and s = 4 were
assumed to be high permeability states.

Note. As the experiments with the program CHAN-
NELS (mentioned in Section 1) show, in a model with
two voltage-gated channels (K + and N a+), the spike
can be generated with many diﬀerent assumptions
about functions α and ω.

4 Abstract Model of Ensem-
ble of Protein Molecule Ma-
chines (EPMM)

An Ensemble of Protein Molecule Machines (EPMM)
is a set of identical independent PMMs with the same
input vector, and the output vector equal to the sum
of output vectors of individual PMMs.

The structure of an EPMM is shown in Figure 3,
where N is the total number of PMMs, yk is the
output vector of the k-th PMM, and y is the output
vector of the EPMM. We have

y =

yk

N

Xk=1

(9)

Let Ni denote the number of PMMs in state s = i
(the occupation number of state i). Instead of (9) we
can write

y =

Niω(x, si)

(10)

Ni (i = 0, ...n − 1) are random variables with the
binomial probability distributions

n−1

Xi=0

m
N (cid:19)

(cid:18)

Ni has the mean µi = N Pi and the variance
σ2
i = N Pi(1 − Pi).

Figure 3: The structure of EPMM

4

am0(V ) .. amj(V ) .. amm(V )

P {Ni = m} =

i (1 − Pi)N −m
P m

(11)

Let us deﬁne the relative number of PMMs in state
s = i (the relative occupation number of state i) as

The implementation using n integrating opera-
tional ampliﬁers shown in Figure 5 is not very reli-

ei =

Ni
N

(12)

The behavior of the average ei is described by the

equations similar to (1) and (2).

dei
dt

Xj6=i

=

α(x, si, sj)ej − ei

α(x, sj , si)

(13)

Xj6=i

at t = 0

ei = 1

(14)

n−1

Xi=0

The average output y is equal to the sum of average
outputs for all states.

n−1

Xi=0

p

y = N

ω(x, si)ei

(15)

The standard deviation for ek is equal to

σk =

Pk(1 − Pk)/N

(16)

It is convenient to think of the relative occupation
numbers ek as the states of analog memory of an
EPMM. In [2, 3, 4] the states of such dynamical cel-
lular short-term memory (STM) were called E-states.

Figure 4 illustrates the implementation of E-states
as relative occupation numbers of the microscopic
states of a PMM. The number of independent E-state
variables is equal to n − 1. The number is reduced
by one because of the additional equation (14).

5 EPMM as a Robust Analog

Computer

An EPMM can serve as a robust analog computer
with the input–controlled coeﬃcient matrix shown in
Figure 5. Since all the characteristics of the statistical
implementation of this computer are determined by
the properties of the underlying PMM, this statistical
molecular implementation is very robust.

Figure 4: Relative occupation numbers of the micro-
scopic states of a PMM as the macroscopic states of
analog memory of the corresponding EPMM

Figure 5: Simulation of an EPMM using integrat-
ing operational ampliﬁers with input–controlled co-
eﬃcient matrix

5

able. The integrators based on operational ampliﬁers
with negative capacitive feedback are not precise, so
condition (14) will be gradually violated. (A better
implementation should use any n − 1 equations from
(13) combined with equation (14).) In the case of the
discussed statistical implementation condition (14) is
guaranteed because the number of PMMs, N , is con-
stant.

6 Replacing Connections with

Probabilities

The most remarkable property of the statistical im-
plementation of the analog computer shown in Fig-
ure 5 is that the matrix of input-dependent macro-
scopic connections is implemented as the matrix of
input-dependent microscopic probabilities. For a suf-
ﬁciently large number of states (say, n > 10),
it
would be practically impossible to implement the cor-
responding analog computers (with required biologi-
cal dimensions) relying on traditional electronic oper-
ational ampliﬁers with negative capacitive feedbacks
that would have to be connected via diﬃcult to make
matrices of input-dependent coeﬃcients.

A single neuron can have many diﬀerent EPMMs
interacting via electrical messages (membrane poten-
tial) and chemical messages (diﬀerent kinds of neu-
rotransmitters). As mentioned in Section 3, the
Hodgkin-Huxley [6] model can be naturally expressed
in terms of two EPMMs (corresponding to the sodium
and potassium channels) interacting via common
membrane potential (see Figure 6a). Figure 6b shows
two EPMMs interacting via a second messenger. In
this example, EPMM1 is the primary transmitter re-
ceptor and EPMM2 is the second messenger receptor.

7 Examples of Computer Sim-

ulation

Figure 7 presents examples of computer simulation
done by program CHANNELS mentioned in Sec-
tion 1. Lines 2-4 in Figure 7a display random pulses
of sodium current produced by 1, 2, and 3 PMMs, re-
spectively, representing sodium channel, in response
to the pulse of membrane potential shown in line 1.
Line 4 shows a response of 100 PMMs. (A description
of the corresponding patch-clamp experiments can be
found in [11, 5]).

Figure 7b depicts the spike of membrane potential

Figure 6: Two EPMMs interacting via a) electrical
and b) chemical messages

Figure 7: Examples of computer simulation by pro-
gram CHANNELS

6

produced by two interacting EPMMs representing en-
sembles of sodium and potassium channels (N → ∞).
In this simulation, the sodium and potassium chan-
nels were represented as ﬁve-state PMMs mentioned
in Section 3. The speciﬁc values of parameters are
not important for the purpose of this illustration.

8 EPMM as

a Distributed

State Machine (DSM)

Let the number of PMMs go to inﬁnity (N → ∞). In
this case EPMM is a deterministic system described
by the set of diﬀerential equations 13 and 14. In some
cases of highly nonlinear input-dependent coeﬃcients
α(x, i, j), it is convenient to think about this dynam-
ical system as a distributed state machine (DSM).
Such machine simultaneously occupies all its discrete
states, with the levels of occupation described by the
occupation vector e = (e0, ...en−1). We replaced ei
by ei, since N → ∞.

This interpretation oﬀers a convenient language
for representing dynamical processes whose outcome
In the
depends on the sequence of input events.

Figure 8: EPMM as a distributed state machine
(DSM)

7

same way as a traditional state machine is used as
a logic sequencer, a DSM can be used as an analog
sequencer. The example shown in Figure 8 illustrates
this interesting possibility.

If the sequence of input events is AB the DSM
ends up ”almost completely” in state 2 (lines 1-3).
The BA sequence leads to state 4 (lines 4-6). Many
diﬀerent implementations of a DSM producing this
sequencing eﬀect can be found. Here is an example
of an EPMM implementation:
Let x = (x1, x2), s ∈ {0, 1, 2, 3, 4}, and let α(x, i, j)
be described as follows:
if input satisﬁes condition
x1 > xthr1 & x2 ≤ xthr2 (event A) then α(x, i, j) > 0
for transitions (i, j) ∈ {(1, 0), (4, 3)}; if input satisﬁes
condition x1 ≤ xthr1 & x2 > xthr2 (event B) then
α(x, i, j) > 0 for transitions (i, j) ∈ {(2, 1), (3, 0)}.
In all other cases α(x, i, j) = 0.

This example can be interpreted as follows. If in-
put x1 exceeds its threshold level xthr1 before input
x2 exceeds its threshold level xthr2, the EPMM ends
up ”mostly” in state 2. If these events occur in the
reverse direction, the EPMM ends up ”mostly” in
state 4.

9 Why Does the Brain Need
Statistical Molecular Compu-
tations?

Starting with the classical work of McCulloch and
Pitts [10] it is well known that any computable func-
tion can be implemented as a network of rather sim-
ple artiﬁcial neurons. Though the original concept of
the McCullough-Pitts logic neuron is now replaced by
a more sophisticated notion of a leaky integrate-and-
ﬁre (LIF) neuron [12], this model is still very simple
as compared to the EPMM formalism discussed in
the present paper.

Why does the brain need statistical molecular com-
putations? The answer to this question is straight-
forward. There is simply not enough neurons in the
brain to implement the required computations in the
way proposed by the traditional collective theories of
neural networks [2, 3].

10 Summary

Acknowledgements

1. A class of statistical analog computers built from
large numbers of microscopic probabilistic ma-
chines is introduced. The class is based on
the abstract computational model called Protein
Molecule Machine (PMM). The discussed statis-
tical computers are represented as Ensembles of
PMMs (EPMMs). (Sections 2 and 4.)

2. It is postulated that at the level of neural com-
putations some protein molecules (e.g., ion chan-
nels) can be treated as PMMs. That is, at this
level, speciﬁc biophysical and biochemical mech-
anisms are important only as tools for the phys-
ical implementation of PMMs with required ab-
stract computational properties. (Section 3.)

3. The macroscopic states of analog memory of the
discussed statistical computers are represented
by the average relative occupation numbers of
It was pro-
the microscopic states of PMMs.
posed [2, 3, 4] that such states of cellular ana-
log memory are responsible for the psychologi-
cal phenomena of working memory and temporal
context (mental set). (Section 4.)

4. In some cases, it is useful to think of an EPMM
as a distributed state machine (DSM) that simul-
taneously occupies all its discrete states with dif-
ferent levels of occupation. This approach oﬀers
a convenient language for representing dynami-
cal processes whose outcome depends on the se-
quence of input events. (Section 8.)

5. A computer program employing the discussed
formalism was developed. The program, called
CHANNELS, allows the user to simulate the dy-
namics of a cell with up to ten diﬀerent voltage-
gated channels, each channel having up to eigh-
teen states. Two simulation modes are sup-
ported: the Monte-Carlo mode (for the number
of molecules from 1 to 10000), and the continu-
ous mode (for the inﬁnite number of molecules).
New software capable of handling more complex
types of PMMs is under development. (Visit the
web site www.brain0.com for more information.)

8

I express my gratitude to Prof. B. Widrow, Prof. L.
Stark, Prof. Y. Eliashberg, Prof. M. Gromov, Dr. I.
Sobel, and Dr. P. Rovner for stimulating discussions.
I am especially thankful to my wife A. Eliashberg for
constant support and technical help.

References

[1] Changeux, F. (1993). Chemical Signaling in the
Brain. Scientiﬁc American, November, 58-62.

[2] Eliashberg, V. (1989). Context-sensitive associa-
tive memory: ”Residual excitation” in neural
networks as the mechanism of STM and mental
set. Proceedings of IJCNN-89, June 18-22, 1989,
Washington, D.C. vol. I, 67-75. Eliashberg, V.
(1989).

[3] Eliashberg, V. (1990). Universal learning neu-
rocomputers. Proceeding of the Fourth Annual
parallel processing symposium. California state
university, Fullerton. April 4-6, 1990. 181-191.

[4] Eliashberg, V. (1990). Molecular dynamics of
short-term memory. Mathematical and Com-
puter modeling in Science and Technology. vol.
14, 295-299.

[5] Hille, B. (2001). Ion Channels of Excitable Mem-
branes. Sinauer Associates. Sunderland, MA

[6] Hodgkin, A.L., Huxley, A.F. 1952. A Quantita-
tive Description of Membrane Current and its
Application to Conduction and Excitation in
Nerve. Journal of Physiology, 117, 500-544.

[7] Kandel, E.R., and Spencer, W.A. (1968). Cellu-
lar Neurophysiological Approaches in the Study
of Learning. Physiological Rev. 48, 65-134.

[8] Kandel, E., Jessel,T., Schwartz, J. (2000). Prin-

ciples of Neural Science. McGraw-Hill.

[9] Marder, E., Thirumalai, V. (2002). Cellular,
synaptic and network eﬀects of neuromodula-
tion. Neural Networks 15, 479-493 .

[10] McCulloch, W. S. and Pitts, W. H. (1943). A
logical calculus of the ideas immanent in nervous
activity. Bulletin of Mathematical Biophysics,
5:115-133.

[11] Nichols, J.G., Martin, A.R., Wallace B.G.,
(1992) From Neuron to Brain, Third Edition,
Sinauer Associates.

[12] Spiking Neurons in Neuroscience and Technol-
ogy. 2001 Special Issue, Neural Networks Vol.
14.

9


2
0
0
2
 
n
u
J
 
2
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
1
3
0
6
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Process Pathway Inference
via Time Series Analysis

Chris Wiggins1,2 and Ilya Nemenman2
1Department of Applied Physics and Applied Mathematics
and Center for Computational Biology and Bioinformatics (C2B2)
Columbia University, New York NY 10027
2Kavli Institute for Theoretical Physics
University of California, Santa Barbara, CA 93106

November 25, 2013

Abstract

Motivated by recent experimental developments in functional genomics, we construct and
test a numerical technique for inferring process pathways, in which one process calls another
process, from time series data. We validate using a case in which data are readily available and
formulate an extension, appropriate for genetic regulatory networks, which exploits Bayesian
inference and in which the present–day undersampling is compensated for by prior understanding
of genetic regulation.

Preprint number: NSF-ITP-02-47

1 Motivation

The last decade has witnessed stunning advances in experimental biology, particularly in the ﬁelds
of neuroscience and genomics, which have made possible ‘data–driven’ biological investigations. As
examples, the quantitative revolution of genomics has provided terabytes of transcriptome data; and
neuroscientists routinely record for hours or even days from multiple neurons simultaneously. This
transformation stands as a challenge to theorists who hope to advance understanding by making
connection between experiment and ﬁrst principles models.1

In genomics, for example, we are presented with the expression levels of thousands of genes,
but our ability to model is limited not only quantitatively, in that there are myriad unknown rate
constants and binding parameters, but qualitatively, in that a sizable fraction of proteins and genes
remain of uncharacterized function [1]. Similarly, in neuroscience, we can model patches of cellular
membranes, synapses, and (at least electro–physiologically) entire cells [2]. However, this modeling
hinges on numerous unknown parameters, and even if we can perform massive computations involved
in the study of even rather small biological neural networks, the sensitivity to these parameters still
makes the whole approach intractable. The astronomical amounts of experimental data are troubling
computationally, but even more immediate problems are the lack of reductive descriptions of the
underlying phenomena and undersampling — an inability of the data to determine the (slightly
smaller) astronomical number of important microscopic parameters appearing in theoretical models.

1The mathematization of such models are referred to below as the ‘microscopic equations’; consider for example
those of ﬂuid dynamics which govern, yet certainly fail to encapsulate, such phenomena as turbulence and the tumbling
of a falling leaf.

1

Presented with such an imbalance, it is important to distinguish among the possible questions we
can ask as well as the possible tools at our disposal for answering them. That is, one can ask what the
system is doing (a nontrivial question when the language for discovery is an astronomical number of
unorganized data) before one asks how it is doing what it does. The latter involves building models
of some microscopic ﬁdelity. The former may be answered without reference to microscopics by a
model–independent, data–driven phenomenological approach.

A useful historical analogy is that of particle physics of the late 1950’s, in which an explosion of
data from accelerators was equally daunting and similarly irreducible. At that time physicists were
not yet asking the how questions (cross sections, isospin multiplets, etc.) but were instead carefully,
statistically, inferring the presence or absence of features in the data; for example, exploiting prior
knowledge of quantum mechanics to constrain reasonable shapes for peaks in the data (hallmarks
of newly discovered particles — the ‘resonances’).

In this analogy, neuroscience is still dealing with the existence of peaks. Indeed, only recently
(see Ref. [3] and further works by the same authors) it has become clear that precise timings of
single spikes are very important for understanding the neural code. This is a basic, objective,
model–independent observation, and it is not surprising that Shannon’s information theory, which
was speciﬁcally designed with these types of questions in mind, turned out to be extremely useful.
In genomics, however, the quantities of interest are easier to identify: gene expressions are largely
governed by the underlying regulation networks. Now is the time to attempt to infer these networks
— still the what question — which corresponds to inferring the peaks in the data. This is a requisite
step before classifying the possible networks and explaining the classiﬁcation rules — an answer
to how the system does what it does. Trying to answer how before carefully exploring what might
ultimately produce many epi–manipulations of the data, but little signiﬁcant understanding.

Said otherwise, presented with data describing natural phenomena, one should form a phe-
nomenology of experimental results, then inferences from the data in light of this phenomenology,
and ﬁnally microscopic models. Genomics is currently at the penultimate step, and, armed with
careful informatics, here meaning the incorporation of data with prior knowledge in the absence of
detailed models, we hope to reduce the data into a representation which allows description, predic-
tion, and ultimately control.

An example of data reduction convenient for representation is cataloging of the regulatory net-
works. However, such cataloging is not a model independent task: at the very least, our microscopic
model includes the existence of the networks. Further, even if we are only interested in a network’s
connection diagram and do not care much about the exact details of the connections, identiﬁca-
tion of the network still involves determination of many parameters. Thus it is not clear that
information–theoretic approaches will be of great use. However, it is plausible that our intuition of
how the underlying microscopic dynamics translates into macroscopic probabilistic models may play
a big role. The main purpose of this paper is to show that this intuition, appropriately mathema-
tized in a principled way as the a priori knowledge — the priors in Bayesian statistics — may be
self–consistently incorporated into a macroscopic probabilistic model of process pathways without
detailed, sophisticated modeling of microscopic dynamics. We will ﬁrst show this on a simple syn-
thetic example, and then suggest some extensions of the ideas with an eye towards genetic regulatory
networks.

2 Functional genomics

As mentioned, the motivating problem here is time series informatics applied to functional genomics.2
We therefore brieﬂy review genetics and characterize the relevant experiments, the data from which

2It is important here to diﬀerentiate functional genomics, or ‘post–genomics’, from sequencing genomics. The
latter is the set of techniques associated with obtaining the genetic sequence of an organism. The former is the set of
techniques which try to put this information to use.

2

will be used in inferring the underlying connectivities and possibly control.

2.1 A brief review of genetics

The central goal of functional genomics is the understanding of the interactions among distinct
parts of the genome — the genes. Each gene consists of long words composed of thousands of coding
base pairs of DNA which are then transcribed into mRNA, which is then translated into protein.
Many of these proteins, called transcription factors, then regulate the rate of production of mRNA
transcribed either by their own genes or other genes. The working of all the genes thus forms a
genetic regulatory network, and may be thought of as a dynamical system. Inputs include elements
of the physical world which aﬀect the activity of the transcription factors, and outputs may be
considered as the concentrations of the translated proteins or, at a deeper level, the transcribed
mRNA. While the proteins are ultimately responsible for cellular function, the mRNA are more
easily experimentally measured via DNA microarrays.

2.2 A brief review of DNA microarrays

Only recently has it become possible to probe the expression of a number of genes comparable to the
total number of genes in the entire genome of an organism via microarrays of nucleic acids, commonly
known as ‘DNA chips.’ The most common application of such a chip is to monitor simultaneously
the expression of thousands of genes by detecting hybridization of nucleic acid originating from a
biological sample to target nucleic acids lying on the chip. One can then probe, for example, the
diﬀerences in gene expression between cancerous and non–cancerous cells of the same specialization
[4, 5, 6, 7, 8, 9, 10], or the expression of diﬀerent genes as a function of the phase of the cell cycle
[11, 12], or of the response of cells to chemical or physical perturbation. The two latter types of
experiments produce time series of gene expressions, and they will be the focal point of our discussion
from now on.

The ﬁrst DNA microarrays were made by Aﬀymetrix in the early 1990s [13, 14, 15].

In this
technique, DNA oligonucleotides are attached to a surface in a speciﬁc spatial pattern, directed by
optically activated chemical synthesis. One can build an arbitrary oligonucleotide sequence in a
small area (approximately 20 microns per target) on the surface. However, the initial setup cost
of creating the chip makes the technique infeasible for any application for which less than several
hundred masks will be created (and sold). Individual researchers are completely without ﬂexibility
to change the chip to ﬁt a particular area of investigation.

Functional genomics further beneﬁted from a second technology in 1996, when Pat Brown’s lab
at Stanford introduced the spot chip [16, 17]. This highly customizable technique exploits robotic
deposition of drops on a microscope slide. The automation makes creating new and diﬀerent slides a
simple operation. Moreover, one can create typically 120 slides at a time. Individual researchers can
thus design custom experiments, placing genes at locations or redundancies of their choosing. The
gene fragments used in the spotting technique are hundreds of base pairs in length, and therefore
less sensitive to single base pair mismatches.

3 Methods

3.1 Chemical network reconstruction

Genomic data is certainly not the ﬁrst dynamic data for which reverse engineering of the interaction
network has been attempted. A similar problem has been faced historically in chemistry, in which
one would like to infer the underlying reactions responsible for observed data. Such reaction networks
typically are sparse, that is, the typical connectivity is far less than the total number of chemical

3

species. This is also true in genomics, where one gene typically interacts with no more than a few
dozen others [18].

To highlight the parallels, one may state the question as follows: armed with suﬃcient temporal
data taken from a number of interacting reagents (here, chemicals), is it possible to infer the circuit
diagram? One possible strategy was proposed in 1995 [19], tested ﬁrst on simulated data, and later
on the glycolytic pathway [20], and recently reﬁned in light of ideas from information theory [21].
However, this strategy has yet to be successfully applied to any reactions which were not known
by the authors beforehand, nor subjected to a ‘blind’ test, as in the annual CASP test among the
protein folding community.3 In addition, unlike in chemical kinetics, where the data is produced
by moderately nonlinear and rapidly interrogated dynamical systems, genomic datasets are highly
undersampled and are more like a set of fuzzy logic gates, or leaky boolean circuits. Thus, successful
application of techniques inspired by chemical networks to genomic data is, at best, doubtful.

We may nonetheless attempt reverse engineering of regulatory networks with a similar philosophy,
in that, rather than trying to ﬁt to a precise microscopic model, we attempt to parameterize a
minimal phenomenological model and infer macroscopic parameters from it.

3.2 Synthetic network reconstruction

In order to test any new attempt to infer connectivity from dynamics, it is useful to study a system
which is qualitatively similar, e.g., which demonstrates degrees of freedom which turn on and oﬀ
other degrees of freedom in a near–complete or ‘fuzzy logic gate’ way and for which connectivity is
sparse; yet for which data are readily available and obvious to interpret. To that end, we collected
data from a multiuser UNIX machine, recording the relative CPU usage of all processes (the analogue
of mRNA concentration), user ID, and process name, as a function of time. This can be done in an
automated way via bash script,4 and the results analyzed via MATLAB.

A typical time course, automatically labeled via MATLAB, is shown in Fig. 1 for a particular
(anonymous) user at a large department of applied mathematics. One axis is the job ‘number,’
ordered by frequency of occurrence over all users; the other axis is time (roughly in seconds). The
height (and color) indicates CPU usage.

3.3 Modeling synthetic data

We begin with the minimal probabilistic model of process pathways, in which the strength of each
process at subsequent time steps is linearly determined by the strength of all current processes.
Similar linear models have been used with some success in understanding genomic data, including
clustering via dynamics [22]. The most general model in discrete time is the AR(p) model, in which
we include the possibility that the state now is a function of the p previous observations of the
system. We do not yet include the possibility of hidden degrees of freedom. Mathematically, we
may pose the model as

gt = w0 + M1gt−1 + M2gt−2 + . . . + Mpgt−p + ξt

hξi,tξj,t′ i = Cijδt,t′ ,

(1)

(2)

where the degree of freedom at observation t is gt, the transition matrices are Mj, and the noise
correlation C is as–yet undetermined.

One may ﬁt for the most probable transition matrices Mj as well as the oﬀset w0 and the noise
correlation matrix C, using, for example, the standard Schwartz’s Bayesian Information Criterion
[23] to determine the most likely value of p.5 Excellent numerical techniques and general purpose
libraries have been designed for solving this problem [24].

3Critical Assessment of techniques for protein Structure Prediction; http://predictioncenter.llnl.gov/.
4(GNU) Bourne–Again SHell
5See Sec. 5.1 for a brief discussion of this model selection technique.

4

Figure 1: CPU percentage for all the processes called by a particular user during the observation
(approximately 103 seconds). Processes are numbered according to their relative frequency over all
users during the observation time.

Note that we do not claim the actual interactions among the processes are linear, as Eq. (1) seems
to imply. Indeed, as stated above, the exact values of the transition matrices Mj are of very little
interest to us, and we are only interested in the topological features of the network. It is reasonable
to expect that the absence of a connection between two processes will be ﬁt well by a zero in the
corresponding transition matrix element, while the presence of a connection of any type will result
in its nonzero value. The mismatch between the linear form of Eq. (1) and the actual dynamics will
manifest itself in a large variance Cij . However, if we are not interested in the exact values of Mj,
this should not adversely eﬀect our determination of connections.

4 Results

Fitting the observed CPU usages to the transition state model, one ﬁnds the most probable p value
is 1, indicating a lack of inertia in the system; the resulting transition matrix M1 is plotted in Fig. 2.
The noise covariance matrix was quite small, despite the naivet´e of the model.

Causal connections between jobs are labeled with ‘→’, e.g., ‘emacs→latex’ or ‘emacs drives

latex.’ We highlight several remarkable features:

1. Processes familiar to anyone who has used the typesetting software LATEXwill be readily ap-
parent: one edits a ﬁle (e.g., in emacs, xemacs, or vi), then compiles with latex, and views
the result in xdvi and ﬁnally ghostscript (‘gs’). Similarly one observes emacs drives latex,
latex drives gs, etc.

5

0.6

0.4

0.2

0

-0.2

-0.4

emacs→xemacs

vi→xdvi.bin

xdvi.bin→vi

xemacs→emacs

latex→gs

gs→gv
emacs→gv

xemacs→gv

emacs→latex

xemacs→latex

latex→mc

latex→gv

10

20

30

xemacs→mc
10

40

process

activator

emacs→mc

30

20

40

Figure 2: The transition state matrix M1 resulting from the data in Fig. 1.

2. Note that the matrix is not symmetric: one axis describes processes which ‘activate’ other pro-
cesses; the second describes which process is acted on. For example, latex drives ghostscript
but ghostscript does not drive latex.

3. The transition matrix shows ‘upregulation’ as well as ‘downregulation’: some processes dis-

courage other processes at later times.

4. Diagonal elements have not been labeled as they simply describe the likelihood the process

will continue on to the next time step.

5. Note also how the transition matrix correctly infers the highly sparse connectivity of these
disparate jobs. The vast majority of elements are 0, as they should be, since processes are not
inﬂuenced by those called by other users.

For comparison, in Fig. 3 we also show an example of a transition matrix when the CPU usages
are replaced with randomly generated data. Any reasonable structure is absent here. These results
are in accord with our intuition that the proposed probabilistic model for data reduction, Eq. (1),
although an incomplete description, still leads to a reasonable reconstruction of network connectivity.

5 Modeling gene regulation

In the example above, as mentioned, data are plentiful. In genomic time series, data are scarce.
However, the above exercise is designed to test a phenomenological model into which one can incor-
porate additional knowledge about genetic regulation. This goal, constraining possible models by
compensating for sparse data with prior knowledge, is mathematized via Bayesian analysis.

6

dns→.usr.lib.nfs.mountd

.usr.lib.nfs.mountd→gv

fsflush→.usr.lib.nfs.mountd

0.1

0.05

0

-0.05

-0.1

Mail→xemacs

40

30

popper→xemacs

xterm→.bin.sh

40

20

10

activator

.usr.bin.ps→.usr.sbin.nscd

30

20

10

process

Figure 3: The transition state matrix M1 resulting from random data.

We emphasize that we are not using Bayesian analysis to attempt to ﬁt for the innumerable
unknowns in a microscopic model, e.g., a chemical kinetics model of transcriptional regulation.
We are not interested in these parameters but in couplings and network topology. We instead
augment the successful phenomenological or ‘macroscopic’ model above with prior knowledge about
transcriptional regulation.

5.1 Bayesian statistics

A brief summary of Bayesian statistics is in order. We refer the interested reader to standard
textbooks (cf. Ref. [25]) for discussions of philosophical implications of Bayesian statistics, as well as
standard statistical properties of Bayesian estimators. We focus below only on the relevant features.

Bayes rule itself,

P (b|a) =

P (a|b)P (b)
P (a)

,

is merely a rewriting of the rules of joint probabilities. The connection with interpretation of
experiments is made by identifying b as the model, or speciﬁcally the vector of parameters θ of the
model; and a as the data D. Then

In Eq. (4), the left hand side is called the posterior, and the ﬁrst term in the numerator in the
right hand side is called the likelihood. The strength of Bayesian methods comes from P (θ), the

P (θ|D) =

P (D|θ)P (θ)
P (D)

,

P (D) =

dθ P (θ, D) =

dθ P (D|θ)P (θ).

Z

Z

7

(3)

(4)

(5)

prior. The prior summarizes knowledge about the probability of a model before the the results,
D, of an experiment are observed. When experimental data are abundant the exact speciﬁcation
of the prior is usually unimportant (cf. Refs. [26, 27]); when data are scarce the prior constrains
the space of available models. While careless a priori assumptions may constrain the observer to
the wrong part of the model space, in the analysis of genetic regulatory networks one may exploit
well established knowledge about transcriptional regulation to construct appropriate priors, as we
illustrate in Sec. 5.2.2.

Armed with the prior, one next ﬁnds the a posteriori expected value of the parameters:

hθiD =

dθ θP (θ|D) =

Z

R

dθ θ P (θ)P (D|θ)
P (D)

≡

hθ P (D|θ)i
hP (D|θ)i

,

(6)

where h. . .i and h. . .iD denote expectations over the prior and the posterior respectively. When the
θ which
number of data N is large we expect the posterior to be tightly peaked around some value
θ is the ﬁrst
maximizes the posterior (maximum a posteriori probability, or MAP, values), and then
order term in the saddle point asymptotic expansion of hθiD in powers of 1/N .
b
Even for severely undersampled problems, N is usually large enough so that

θ ≈ hθiD, and it is
tempting to replace integrals in Eq. (6) by their saddle point values. One of the greatest realizations
in Bayesian theory in the last decades was that such replacement is wrong [23, 28, 26, 29, 30]. Indeed,
when averaging over all possible models, each of the integrals in Eq. (6) will have a contribution
θ. For example, under some very general conditions, and with
from ﬂuctuations around the value
an assumption that log P (D|θ) scales linearly in the number of data, the total probability of the
data, P (D), has the following expansion in powers of 1/N

b

b

b

∂2
∂θi∂θj (cid:12)
(cid:12)
(cid:12)
(cid:12)b

θ

log P (D) = log P (D|

θ) −

log N −

log det

K
2

1
2

log P (D|θ)
N

+ log P (

θ) + o(N 0) ,

(7)

b

b

(cid:20)
where K is the number of parameters in the model (dimensionality of θ). The integral in the
numerator of Eq. (6) can be written in a similar fashion. We see that the terms beyond the maximum
θ) are generally negative and their magnitude grows with K. Thus
likelihood contribution log P (D|
these terms provide a built–in punishment for model complexity. For this reason, they are known
in the literature as as the Occam razor (cf. Ref. [30]).

b

(cid:21)

To illustrate, imagine that the prior admits two model families Θ1 and Θ2 with diﬀerent param-
eters θ1, θ2, such that K1 ≡ dim θ1 < K2 ≡ dim θ2. As an example, consider ﬁtting a function
with a polynomial of low degree (K1) or high degree (K2). Usually we would expect the model
θ1). Thus if we
family with more parameters to be better at explaining the data: P (D|
were to choose a model family that explains the data best based on the maximum likelihood alone,
a more complex model would win. However, the estimates within the larger model family, Θ2, are
less robust to small ﬂuctuations, and this is picked up by the integration over all parameters: even
θ2), the relation between the probabilities of the model
though P (D|
families P (Θ1|D) and P (Θ2|D), as determined by Eqs. (6, 7), may be diﬀerent. In particular, for
N ≫ 1 the likelihood term, which scales linearly with N , always wins, and Bayesian model selec-
tion approaches that of the maximum likelihood. However, for small N the diﬀerence between the
likelihood and the other terms in Eq. (7) is less profound, and a simpler model family, which is not
the best in explaining the data, may turn out victorious.

θ1) may be smaller than P (D|

θ2) > P (D|

c

c

c

c

In short, Bayes rule shows how a simpler model may be less likely, yet more probable.
Before ending our quick review of Bayesian statistics, two more comments are in order. First,
as the model selection arguments are mostly important for small N , where log N ∼ 1, it would be a
mistake to ignore O(1) terms in Eq. (7) and use just K/2 log N as a model complexity punishment
(Schwartz’s Bayesian Information Criterion [23], also mentioned in Sec. 3.3).
In particular, we
believe that such replacement may be a cause of a common observation that the Bayesian Criterion

8

overpunishes complex models (cf. Ref. [31]). Second, even though in this work we will be mostly
dealing with ﬁnite parameter models, application of nonparameteric methods to biological data
certainly holds promise. Occam–type arguments for such cases have been discussed in, for example,
Refs. [32, 27].

5.2 Bayesian inference of regulatory dynamics

5.2.1 A simple model

Let gt stand for the vector of expressions (mRNA concentrations) of genes in a microarray experiment
at time t. The number of genes, Kg ≡ dim gt, can be on the order of thousands. In principle, gt+1
can depend on concentrations at all times that preceded t + 1. However, if we view the gene
expression mechanism in cells as a realization of some chemical kinetics, governed by ﬁrst order
diﬀerential equations, then it is reasonable to expect that the concentrations depend only on their
immediate past, i.e., gt+1 = f (gt).6 We begin with the simplest possible dynamic, a simpliﬁed
version of Eqs. (1, 2):

(8)

(9)

(10)

gt+1 − gt = Mgt + ξt ,
hξi,tξj,t′ i = σ2 δij δt,t′ ,

where the noise is Gaussian, and M and σ are unknown. This is equivalent to

P (gt+1|gt, M, σ) =

1
(2πσ2)Kg/2 exp

−

1
2σ2

(cid:20)

(cid:10)(cid:12)
(cid:12)

gt+1 − gt − Mgt

2

.

t

(cid:21)

(cid:11)

(cid:12)
(cid:12)

where h· · ·it indicates empirical averaging over time.

Notice that unlike in Eqs. (1, 2) the noise in this model is not correlated among the genes;
moreover, variances are equal for all genes. Below we formulate an extension that incorporates hidden
degrees of freedom (e.g., biochemistry) whose omission may otherwise lead to large or correlated
noise (cf. Sec. 5.2.3). However, with only a handful of experiments available we cannot hope that
data will be able to determine millions of elements of a full covariance matrix. As data become more
plentiful, it may even make sense to bypass Eqs. (8, 9) completely and pursue model independent
feature extraction (as formulated in Secs. 6.1 and 6.2). Below we pursue the possibility that the
current simple model exhibits some of the success evidenced in Sec. 4.

5.2.2 Biological priors

Even in the simplistic form of Eqs. (8, 9), the dynamic still contains too many parameters (∼ K 2
g) to
be tractable. We therefore search for biologically motivated priors to constrain the possible values of
M. An example of such a prior would be, for example, that genes with similar regulatory sequences
should be regulated similarly. More directly, genes whose promoters have similar numbers of certain
important motifs should be co–expressed, an ansatz used with notable success by Bussemaker et
al. [34] in discovering regulatory regions.

This may be expressed in the following prior

P (M|µ, ℓ) ∝ exp

−

Kg

ℓ2
2

Xi6=j,k (cid:18)

Mik − Mjk
dij

2

−

µ2
2

(cid:19)

Kg

ij
X

M 2

ij

,







(11)

6Support for this choice is strong, as such models have been used with great success in the design of small genetic
networks (see, e.g., Ref. [33] for a review). Further, in a study clustering genes by their dynamics, Ramoni et al. tested
higher order models, but found that the ﬁrst order dynamics gave the most probable result [22]. Finally, recall that
in Sec. 4 above the ﬁrst order model turned out the most probable, as well.

9

where the ﬁrst term punishes for diﬀerent regulation of genes with similar regulating sequences, and
the second assures proper normalization of priors by eﬀectively constraining the range of possible
Mij.7 Here dij is a distance function measuring deviation between regulatory regions of genes i and
j in terms of the number of each regulatory motif appearing, weighted by the relative importance
of that motif, found by considering the entire time series as in Ref. [34] or ‘quality factors’ as in
Ref. [35].

The parameter ℓ plays the role of a smoothing length, as in Ref. [32], and, lacking a ﬁrst–
principles estimate of its value, we must integrate over ℓ, weighted by an appropriate prior. As
explained in Ref. [27], it is not only likely that such integration will choose the proper value of
ℓ almost independently of such prior, but it may even balance a slightly improper choice of the
distance measure dij and the diﬀerence form (Mik − Mjk)2. The same comments relate to the mass
µ and the noise variance σ as well.

An enjoyable feature of this prior is that, like the likelihood, Eq. (10), it is exponentially quadratic
in the unknowns M. Thus the posterior expectations are Gaussian integrals, which may be performed
analytically using the standard Wigner current technique [36]. Following Eq. (6) we ﬁnd for the a
posteriori values of the connection matrix:

hMijiD =

log Z(J) ,

J=0

Z(J) =

dσ dµ dℓP (σ)P (µ)P (ℓ)

1/2

det A
det(A + G)

(cid:20)

(cid:21)

.

(B + J) (A + G)−1 (B + J)
(cid:21)
Here the curvature tensor A at the saddle point of Eq. (11) is given by

(cid:20)

Aij,kl =

µ2 + ℓ2

(cmj + cmi)

δikδjl − 2ℓ2cikδjl

∂
∂Jij (cid:12)
(cid:12)
(cid:12)
(cid:12)

Z
× exp





1
2

Kg

m
X

and the time–lagged correlation, equal–time correlation, and ‘closeness’ (the inverse of the distance
matrix) matrices are





t

(cid:11)

B =

G =

cij =

(gt+1 − gt)gT
t

1
σ2
1
σ2

(cid:10)
d−2
ij
0

(

(cid:10)
gtgT
t

t ,
(cid:11)

if i 6= j,
if i = j.

G = I ⊗ G ,

(12)

(13)

(14)

(15)

(16)

(17)

The mass µ regulates the integrals and is expected to be small. Then det A/ det(A + G) scales as
a large positive power of ℓ2/(ℓ2 + const), and therefore decreases as ℓ decreases. On the other hand,
the exponent in Eq. (13) involves (A + G)−1, which scales as 1/(ℓ2 + const) for large ℓ. The exponent
thus decreases as ℓ increases. We may then reasonably expect that the integrand in Eq. (13) will be
peaked at some non–trivial value of ℓ; this peak should be sharp since both B and G involve large
number of samples. This may be viewed as the smoothing length selection by the data [27].

Note that the priors over the hyperparameters σ, µ, ℓ in Eq. (13) are as yet undeﬁned. Since,
as mentioned above (see also Refs. [26]), their actual forms are of little importance, we may hope
that by choosing them appropriately we may be able to render the integrals in Eq. (13) analytically
tractable.

7Physicists will recognize these as the ‘kinetic’ and ‘mass’ terms in a Lagrangian, respectively.

10

5.2.3 Hidden control

The formalism deserves experimental testing. However, one further extension oﬀers a substantial
improvement. DNA microarrays oﬀer only a partial view into the workings of a cell, since numer-
ous important degrees of freedom remain unobserved. In mathematical modeling of the yeast cell
cycle, for example, considerable eﬀort8 has been exerted to ﬁne–tune models in which only chem-
istry controls the processes, and the genetic expression is a mere passive function of this chemical
control. While this may or may not prove to be an accurate characterization, some chemical control
unobserved by gene chip experiments certainly exists. Inclusion will clearly necessitate a model of
some structure other than that of Eq. (8). Unlike the assumption of linearity, which merely leads
to misestimation of the values of interactions, this eﬀect may make the dynamical system appear to
be not of ﬁrst order, or introduce a gene–dependent noise correlation matrix and artiﬁcial couplings
between genes that dominate the real ones. To avoid this, we need to supplement the vector of genes
g with a vector of unknown hidden degrees of freedom h. Then the evolution will take form

gt+1 − gt
ht+1 − ht(cid:19)
(cid:18)

=

(cid:18)

Mgg Mgh
Mhg Mhh

gt
ht(cid:19)

+

ξt
ηt(cid:19)

(cid:18)

,

(cid:19) (cid:18)

hξi,tξj,t′ i = σ2 δijδt,t′ ,

hηi

tηj

t′ i = vij δt,t′ .

(18)

(19)

(20)

Within Bayesian analysis, one may integrate over the unknown degrees of freedom and their
possible couplings while remaining agnostic about the identities of h, and similarly sum over their
possible dimensionality Kh. As data are scarce, it is reasonable to expect that models with small
number of hidden units will dominate the posterior. Thus we allow a full correlation matrix for
the Gaussian noise in the hidden units, ηt, since this adds only a few additional parameters to our
model (Mgg, of course, has a few million), but allows necessary ﬂexibility.

One will need a prior over newly introduced M’s. Since chemistry couples to expression via the
transcription factors, and therefore via the regulatory sequences, we can write a similar prior as
above, namely

P (Mgh|µgh, ℓ) ∝ exp

−

ℓ2
2

Kg

Kh

Xi6=j

Xk  

M gh

ik − M gh
dij

jk

2

−

µ2
gh
2

!

Kg

Kh

i
X

j
X









(M gh

ij )2

,

(21)

with a diﬀerent mass, but the same kinetic term as in Eq. (11). In the absence of precise identiﬁcation
of the hidden degrees of freedom, we lack a biological prior on Mhg, Mhh and must therefore choose
a prior that does not spoil the analytic tractability of the resulting integrals.

6 Outlook

The main diﬃculty in obtaining time series data is not biological or technological, but rather ﬁ-
nancial. Aﬀymetrix chips, the more reliable of the two dominant technologies, are quite expensive.
As estimated in [38], a 24-point time series with replication factor of 3 currently costs ∼ $57, 600.
As the cost of the technology decreases, and as data become more plentiful, the role of priors in
inferring connectivity and possible causation diminishes.

Moreover, as noted before, with increasing data comes the possibility of model independent,
nonparametric feature extraction by learning the joint probability distributions of expression levels
at diﬀerent times (cf. Ref. [32]). We highlight two promising such directions below.

8See, e.g., Ref. [37] for a review.

11

6.1 Mutual information and entropy distance

A completely model independent visualization tool of informatics is a low dimensional embedding
of the connectivity diagram of the multiple genes via some meaningful metric. To this end, armed
with a successfully learned joint probability, one may use information theory to deﬁne distance in a
principled way.

An example of such a diagram based on information theoretic ideas is presented for simulated

chemical kinetics in [21], in which the mutual information

(22)

(23)

I(i → j) =

dgidg+

j P (gi, g+

j ) log2

Z

P (gi, g+
j )
P (gi)P (gj) !

 

was used for embedding, with the time–lagged joint probability of degrees of freedom P (gi, g+
j ) ≡
hP (gi(t), gj(t + τ ))it learned by histogramming. While mutual information is a useful similarity
measure, it is not a distance in that it does not obey the triangle inequality. However, the ‘entropy
distance’

DH (i → j) = −

dgidg+

j P (gi, g+

j ) log2

Z

P (gi, g+

j )2σiσj
P (gi)P (gj) !

 

does obey the triangle inequality [39] and thus can be used as a metric, based on which one can
form an embedding in lower dimensions and construct a process diagram as in Refs. [19, 20, 21].
Here, by σ{i,j} we mean the uncertainties in measurements of g{i,j}, respectively. Note that this
distance is reparameterization invariant under monotonic reparameterizations x → f (x); y → g(y),
σx → σf /f ′, etc.

6.2 Clustering by meaningful information: the information bottleneck

Clustering without identifying a speciﬁc property of interest is meaningless. In most cases, if we try
to learn from data (ﬁt a curve, select a model, extrapolate, cluster, etc.) we are doing so not to ﬁnd
the parameters per se, but to use them to make predictions of future data [40]. Thus in dynamics
the variables of relevance are the future gene expressions, and one should cluster data by maximally
compressing them while retaining the most information about the subsequent time steps.9

This idea was put on ﬁrm information–theoretic ground recently with the development of the
information bottleneck [41], which, given a joint probability distribution between degrees of freedom
(e.g., gene expressions at some time) and a quantity of interest (e.g., gene expressions at subse-
quent times), allows an iterative calculation of the meaningful clusters in a probabilistic clustering
algorithm.

An important aspect omitted from current formulations of the information bottleneck is Bayesian
integration over possible joint probability distributions; this procedure smoothes the data and avoids
clustering noise. We expect that this will be one of the most promising as well as principled lines of
research in bioinformatics.

6.3 Prognosis

The future is promising for such data–driven techniques: data are becoming more plentiful, com-
putational power continues to exponentiate, and the data themselves are becoming more reliable,
as those who hope to interpret them study more carefully their statistics and analyses.10 However,

9In contrast, genes are usually clustered by similarity of their expression levels measured in some ad hoc metric

[12, 31].

10See, e.g., Ref. [42, 43] for one such careful analysis of Aﬀymetrix data and Aﬀymetrix’s standard data analysis.

12

before any new techniques in computational biology will be widely exploited, they must be ‘veri-
ﬁed’ by comparing with results agreed upon in the biological community. In this case, veriﬁcation
will entail corroboration of inferred causal relations among genes (or within an inferred module of
genes) with the biological literature. We anticipate that such time series based techniques will ﬁnd
common usage as tests for consensus with known connectivities become more standardized, and we
look forward to their continued development and implementation.

Acknowledgments

The authors are grateful to Dimitris Anastassiou, Guillaume Bal, William Bialek, Harmen Bussema-
ker, Michael Elowitz, Stanlslas Leibler, Christina Leslie, Bud Mishra, Alex Rikun, Burkhard Rost,
Ana Radovanovic, Andrey Rzhetsky, Misha Samoilov, Tapio Schneider, Boris Schraiman, Susanne
Still, Naftali Tishby, and John Tyson for many stimulating discussions. The authors were partially
supported by NSF Grant No. PHY–9907949 to the Kavli Institute for Theoretical Physics.

References

[1] G. D. Stormo and K. Tan. Mining genome databases to identify and understand new gene

regulatory systems. Current Opinion in Microbiology, 5:149–153, 2002.

[2] P. Dayan and L. F. Abbott. Theoretical Neuroscience: Computational and Mathematical Mod-

eling of Neural Systems. MIT Press, Cambridge, MA, 2001.

[3] F. Rieke, D. Warland, R. de Ruyter van Steveninck, and W. Bialek. Spikes: Exploring the

Neural Code. MIT Press, Cambridge, MA, 1996.

[4] M. G. Walker, W. Volkmuth, E. Sprinzak, D. Hodsdon, and T. Kliner. Prediction of gene func-
tion by genome–scale expression analysis: Prostate cancer–associated genes. Genome Research,
9:1198–1203, 1999.

[5] T. R. Golub et al. Molecular classiﬁcation of cancer: class discover and class prediction by gene

expression monitoring. Science, 286:628–629, 1999.

[6] U. Alon. Broad pattern of gene expression revealed by clustering analysis of tumor and normal

colon tissues probed by oligonucleotide arrays. PNAS USA, 96:6745–6750, 1999.

[7] C. M. Perou et al. Distinctive gene expression patterns in human mammary epithelial cells and

breast cancers. PNAS USA, 96:9212–9217, 1999.

[8] D. T. Ross et al. Systematic variation in gene expression patterns in human cancer cell lines.

Nature Genetics, 24:227–235, 2000.

[9] U. Scherf et al. A gene expression database for the molecular pharmacology of cancer. Nature

Genetics, 24:236–244, 2000.

[10] D. Pinkel. Cancer cells, chemotherapy, and gene clusters. Nature Genetics, 24:208–209, 2000.

[11] R. Cho et al. A genome–wide transcriptional analysis of the mitotic cell cycle. Mol. Cell,

2:65–71, 1998.

[12] P. T. Spellman et al. Comprehensive identiﬁcation of cell cycle–regulated genes of the yeast
saccharomyces cerecisiae by microarray hybridization. Mol. Biol. Cell, 9:3273–3297, 1998.

[13] S. P. A. Fodor, J. L. Read, M. C. Pirrund, L. Styer, A. T. Lu, and D. Solas. Light-directed,

spatially addressable parallel chemical synthesis. Science, 251:767–773, 1991.

13

[14] S. P. A. Fodor, R. Rava, X. H. C. Huang, A. C. Pease, C. P. Holmes, and C. L. Adams.

Multiplexed biochemical assays with biological chips. Nature, 364:555–556, 1993.

[15] R. J. Lipshutz, S. P. A. Fodor, T. R. Gingeras, and D. J. Lockhard. High density synthetic

oligonucleotide arrays. Nature Genetics Supplement, 21:20–24, 1999.

[16] M. Schena, D. Shalon, R. Heller, A. Chai, P. O. Brown, and R. W. Davis. Parallel human genome
analysis: Microarray–based expression monitoring of 1000 genes. PNAS USA, 92:10614–10619,
1996.

[17] D. Shalon, S. J. Smith, and P. O. Brown. A DNA microarray system for analyzing complex
DNA samples using two–color ﬂuorescent probe hybridization. Genome Research, 6:639–645,
1996.

[18] N. Friedman, M. Linial, I. Nachman, and D. Pe’er. Using Bayesian networks to analyze ex-
In Proc. Fourth Annual Intern. Conf. on Computational Molecular Biology

pression data.
(RECOMB), pages 127–135, 2000.

[19] A. Arkin and J. Ross. Statistical construction of chemical reaction mechanisms from measured

time–series. J. of Phys. Chem., 99:970–979, 1995.

[20] A. Arkin, P. Shen, and J. Ross. A test case of correlation metric construction of a reaction

pathway from measurements. Science, 277:1275–1279, 1997.

[21] M. Samoilov, A. Arkin, and J. Ross. On the deduction of chemical reaction pathways from

measurements of time series of concentrations. Chaos, 11:108–114, 2001.

[22] M. Ramoni, P. Sebastiani, and P. Cohen. Bayesian clustering by dynamics. Machine Learning,

47:91–121, 2002.

[23] G. Schwartz. Estimating the dimension of a model. Ann. Stat., 6:461–464, 1978.

[24] A. Neumaier and T. Schneider. Estimation of parameters and eigenmodes of multivariate

autoregressive models. ACM Transactions on Mathematical Software, 27:27–57, 2001.

[25] S. J. Press. Bayesian statistics: principles, models, and applications. John Wiley & Sons, New

York, 1989.

[26] B. S. Clarke and A. R. Barron. Information–theoretic asymptotics of Bayes methods. IEEE

Trans. Inf. Thy., 36:453–471, 1990.

[27] I. Nemenman and W. Bialek. Occam factors and model independent Bayesian learning of

continuous distributions. Phys. Rev. E, 65, 2002.

[28] E. T. Janes. Inference, method, and decision: Towards a Bayesian philosophy of science. J.

Am. Stat. Assoc., 74, 1979.

[29] D. J. C. MacKay. Bayesian interpolation. Neural Comp., 4:415–447, 1992.

[30] V. Balasubramanian. Statistical inference, Occam’s razor, and statistical mechanics on the

space of probability distributions. Neural Comp., 9:349–368, 1997.

[31] Y. Barash and N. Friedman. Context–speciﬁc Bayesian clustering for gene expression data.
In Proc. Fifth Annual Intern. Conf. on Computational Molecular Biology (RECOMB). ACM
Press, 2001.

[32] W. Bialek, C. Callan, and S. Strong. Field theories for learning probability distributions.

Phys. Rev. Lett., 77:4693–4697, 1996.

14

[33] J. Hasty, D. McMillen, F. Isaacs, and J. J. Collins. Computational studies of gene regulatory

networks: in numero molecular biology. Nature Reviews Genetics, 2:268–279, 2001.

[34] H. Bussemaker, E. Siggia, and H. Li. Regulatory element detection using correlation with

expression. Nature Genetics, 27:167–171, 2001.

[35] H. J. Bussemaker, H. Li, and E. D. Siggia. Building a dictionary for genomes: Identiﬁcation of
presumptive regulatory sites by statistical analysis. Proc. Natl. Acad. Sci., 97:10096, 2000.

[36] J. Zinn-Justin. Quantum ﬁeld theory and critical phenomena. Clarendon Press, Oxford, 1996.

[37] J. Tyson, C. Chen, and B. Novak. Network dynamics and cell physiology. Nature Reviews

Molecular Cell Biology, 2:908–916, 2001.

[38] C. Langmead, T. Yan, C. R. McClung, and B. R. Donald. Phase–independent rhythmic anal-
In Proc. Sixth Annual Intern. Conf. on Research
ysis of genome–wide expression patterns.
in Computational Molecular Biology (RECOMB), Washington DC (April 18–21, 2002), pages
205–215, 2002.

[39] D. J. C. MacKay. Information theory, inference and learning algorithms, draft 2.4.1. Textbook
in preparation, currently 600 pages long, http://www.inference.phy.cam.ac.uk/mackay/itprnn/,
to be published by C.U.P., 1999.

[40] W. Bialek, I. Nemenman, and N. Tishby. Predictability, complexity, and learning. Neur. Comp.,

13:2409–2463, 2001.

[41] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the
37th Annual Allerton Conference on Communication, Control and Computing, pages 368–377.
University of Illinois Press, 1999.

[42] F. Naef, D. A. Lim, N. Patil, and M. O. Magnasco. DNA hybridization to mismatched templates:

A chip study. Physical Review E, 65:040902R, 2002.

[43] F. Naef, D. A. Lim, N. Patil, and M. O. Magnasco. From features to expression: High–density
oligonucleotide array analysis revisited. In Proceedings of the DIMACS Workshop on Analysis
of Gene Expression Data 2001, 2002. also e–print physics/0102010.

15


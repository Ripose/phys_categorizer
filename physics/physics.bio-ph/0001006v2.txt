0
0
0
2
 
n
a
J
 
5
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
2
v
6
0
0
1
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Temporal correlations and neural spike train entropy

Simon R. Schultz1 and Stefano Panzeri2
1 Howard Hughes Medical Institute and Center for Neural Science, New York University,
4 Washington Place, New York, NY 10003, U.S.A.
2 Department of Psychology, Ridley Building, University of Newcastle upon Tyne,
Newcastle upon Tyne, NE1 7RU, U.K.

Sampling considerations limit the experimental conditions under which information theoretic
analyses of neurophysiological data yield reliable results. We develop a procedure for computing the
full temporal entropy and information of ensembles of neural spike trains, which performs reliably for
extremely limited samples of data. This approach also yields insight upon the role of correlations
between spikes in temporal coding mechanisms. The method is applied to recordings from the
monkey visual cortex, yielding 1.5 and 0.5 bits per spike for simple and complex cells respectively.
PACS numbers: 87.19.Nn,87.19.La,89.70.+c,07.05.Kf

Cells in the central nervous system communicate by
means of stereotypical electrical pulses called action po-
tentials, or spikes [1]. The information content of neural
spike trains is fully described by the sequence of times of
spike emission. In principle, the pattern of spike times
provides a large capacity for conveying information be-
yond that due to the code commonly assumed by phys-
iologists, the number of spikes ﬁred [2]. Reliable quan-
tiﬁcation of this spike timing information is made diﬃ-
cult by undersampling problems that can increase expo-
nentially with the precision of spike observation. While
advances have been made in experimental preparations
where extensive sampling may be undertaken [3–6], our
understanding of the temporal information properties of
nerve cells from less accessible preparations such as the
mammalian cerebral cortex is limited.

This Letter presents an analytical expression which al-
lows the ensemble spike train entropy to be computed
from limited data samples, and relates the entropy and
information to the instantaneous probability of spike oc-
currence and the temporal correlations between spikes.
This is achieved by power series expansion of the entropy
in the time window of observation [7], keeping terms of
up to second order, and subtraction of an analytical es-
timate of the bias due to ﬁnite sampling. Comparison
is made with other procedures such as the ‘brute force’
approach [4,9]; the analytical expression is found to give
substantially better performance for data sizes of the or-
der typically obtained from mammalian neurophysiology
experiments, as well as providing insight into potential
coding mechanisms.

Consider a time period of duration T , associated with
a dynamic or static sensory stimulus, during which the
activity of C cells is observed. The neuronal population
response to the stimulus is described by the collection
of spike arrival times {ta
i }, ta
i being the time of the i-th
spike emitted by the a-th neuron. The spike time is ob-
served with ﬁnite precision ∆t, and this bin width is used
to digitise the spike train. The total entropy of the spike
train ensemble is

H({ta

i }) = −

P ({ta

i }) log2 P ({ta

i }),

(1)

ta
i
X

where the sum over ta
is over all possible spike times
i
within T and over all possible total spike counts from
the population of cells. This entropy quantiﬁes the to-
tal variability of the spike train. Each diﬀerent stimu-
lus history (time course of characteristics within T ) is
denoted as s. The noise entropy, which quantiﬁes the
variability to repeated presentations of the same stimu-
lus, is H noise = hH({ta
i }|s)is, where the angular brackets
s∈S P (s).
indicate the average over diﬀerent stimuli,
The mutual information that the responses convey about
which stimulus history invoked the spike train is the dif-
ference between these two quantities.

P

These entropies may be expanded as a Taylor series in

the time window of measurement,

H = T Ht +

Htt + O(T 3).

(2)

T 2
2

This becomes essentially an expansion in the total num-
ber of spikes emitted; the only responses which contribute
to order k are those with up to k spikes emitted in total.
The conditional ﬁring probabilities can be written

P (ta

i |tb

j; s) ≡ ra(ta

i ; s) ∆t

1 + γab(ta

i , tb

j; s)

+ O(∆t2), (3)

(cid:3)

(cid:2)

and are assumed to scale proportionally to ∆t in the
short timescale limit. Only the ﬁrst order conditional
ﬁring probability aﬀects the entropy to second order in
In the above, ra(t; s) is the time-
the time window.
dependent instantaneous ﬁring rate and is measurable
from the data. The bar indicates the average over mul-
tiple trials in which the same stimulus history was pre-
sented. The scaled correlation function γab is measured
as [8,10]:

ra(ta
ra(ta

i ; s)rb(tb
i ; s)rb(tb

j; s)
j; s)

γab(ta

i , tb

j; s) =

γaa(ta

i , ta

i ; s) = −1.

− 1, a 6= b or ta

i 6= tb
j

(4)

Denoting the no spikes event as 0 and the joint occur-
rence of a spike from cell a at time ta
1 and a spike from cell
b at time tb
1tb
2, the conditional response probabilities
are:

2 as ta

P (0|s) = 1 −

ra(ta

1; s)∆t +

ra(ta

1; s)rb(tb

2; s)

1 + γab(ta

1, tb

2; s)

∆t2

C

a=1
X

ta
1
X

1
2

C

ab
X

ta
1 Xtb
X

2

(cid:2)

(cid:3)

P (ta

1|s) = ra(ta

1; s)∆t − ra(ta

1; s)

rb(tb

2; s)

1 + γab(ta

1, tb

2; s)

∆t2

a = 1, · · · , C

P (ta

1tb

2|s) =

ra(ta

1; s)rb(tb

2; s)

1
2

2

Xtb
b=1
X
1 + γab(ta

1, tb

2; s)

(cid:2)
∆t2

(cid:3)
a = 1, · · · , C,

b = 1, · · · , C.

(5)

ab indicates the sum over both a and b from 1 to C. The unconditional response probabilities are simply
i }) into Eq. 1 and keeping only terms up to and including O(T ) yields for the

i }|s)is. Inserting p({ta

(cid:2)

(cid:3)

where
p({ta
ﬁrst order total entropy

i }) = hp({ta

P

T Ht =

1
ln 2

hra(ta

1; s)∆tis −

hra(ta

1; s)∆tis log2 hra(ta

1; s)∆tis .

(6)

a
X
i }|s) yields an expression for the ﬁrst order noise entropy T H noise
Similarly, inserting p({ta
which is identical, except
that there is a single stimulus average h·is around the entire second term. Continuing the expansion, the additional
terms up to second order are:

a
X

t

ta
1
X

ta
1
X

T 2
2

Htt =

1
2 ln 2

T 2
2

H noise

tt =

ab
X

ta
1 Xtb
X

2 (cid:8)(cid:10)
ra(ta

ta
t Xtb
X
2 (cid:10)

ab
X

ta
1 Xtb
X
2 (cid:10)

+

ab
X
1
2 ln 2

+

ab
X

ta
t Xtb
X

2

*

ra(ta

1; s)rb(tb

2; s)

1 + γab(ta

1, tb

2; s)

s − hra(ta

1; s)is

rb(tb

2; s)

s

∆t2

1; s)rb(tb

2; s)

1, tb

2; s)

∆t2

(cid:2)
1 + γab(ta

(cid:2)
2; s)γab(ta
1; s)rb(tb

ra(ta

1, tb

2; s)

(cid:3)
s ∆t2
(cid:11)

(cid:3)(cid:11)
s log2
(cid:11)

(cid:10)

(cid:11)
(cid:9)
hra(ta
1; s)is

ra(ta

1; s)rb(tb

2; s)

1 + γab(ta

1, tb

2; s)

q(cid:10)

(cid:2)

(7)

s

(cid:3)(cid:11)

ra(ta

1; s)rb(tb

2; s)

1 + γab(ta

1, tb

2; s)

∆t2 log2

(cid:2)

(cid:3)

ra(ta

1; s)

ra(ta

1; s)rb(tb

2; s)

1 + γab(ta

1, tb

2; s)

.

(8)

+

s

(cid:3)

It is easily veriﬁed that the diﬀerence between the total
and noise entropies gives the expression for the mutual
information detailed in [10].

It has recently been found that correlations, even if
independent of the stimulus identity, can increase the
information present in a neural population [11,8]. This
eﬀect is governed by the similarity of tuning of the cells
across the stimuli, and applies both to correlations be-
tween neurons and between diﬀerent spikes emitted by
the same neuron [10]. The equations derived above ex-
plain how this is realised in terms of entropy. The sec-
ond order total entropy can be rewritten in a form which
shows that it depends only upon the grand mean ﬁring
rates across stimuli, and upon the correlation coeﬃcient
of the whole spike train, Γ(·) (deﬁned across all trials
rather than for ﬁxed stimulus as with Eqn. 4):

T 2
2

Htt =

T 2
2 ln 2

hra(ta

1; s)is

rb(tb

2; s)

s

(9)

ab
X
i , tb

ta
1 Xtb
X
j) − [1 + Γab(ta

2

×

Γab(ta

(cid:10)
j)] ln[1 + Γab(ta

(cid:11)

i , tb

i , tb

j)]

.

(cid:8)

It follows that the second order entropy is maximal when
(cid:9)
Γ(·) = 0, and non-zero correlations in the spike trains
(indicating statistical dependence) always decrease the

q

(cid:2)

total response entropy. However, statistical dependence
in the full spike train recorded across all stimuli does not
necessarily imply neuronal interaction [12]. If the signal
correlation

νab(ta

i , tb

j) =

< ra(ta

i ; s)rb(tb
i ; s) >s< rb(tb

< ra(ta

j; s) >s

j; s) >s

− 1

(10)

is negative, then positive γ(s)’s reduce the overall sta-
tistical dependency and thus increase the entropy of the
whole spike train. The entropy increase is maximum for
the γ value which leads to exactly Γ = 0. The eﬀect
is opposite when signal correlation is positive. In com-
parison, the noise or conditional entropy is always de-
creased by γ 6= 0 - at ﬁxed stimulus the only statistical
dependencies in the spike train are those measured by
γ. The increase/decrease of the population information
depending upon the signs of the signal and noise corre-
lation is thus not due to a change in the behaviour of
the noise entropy, but to the increase or decrease of the
total entropy. Statistical dependence always decreases
entropy, but neuronal (or spike time) interaction may in-
crease entropy itself, by eliminating or reducing the sta-
tistical dependencies introduced by other covariations. It

is intriguing to speculate a speciﬁc role for synaptic in-
teractions in compensating for the statistical dependency
introduced by necessary covariations such as ﬁring rate
and slow systemic covariations.

The rate and correlation functions must be estimated
from a limited number of experimental trials, which leads
to a bias in each of the entropy components. This bias
can be estimated by the method derived in [13]:

Hbias =

−R
2N ln 2

H noise

bias =

−1
2N ln 2

Rs

(11)

s∈S
X

where R is the number of relevant (non-zero) response
bins. For the ﬁrst order entropy, it is the number of non-
zero bins of the dynamic rate function; for the second
order entropy, it is the number of relevant bins in the
space of pairs of spike ﬁring times. For the noise en-
tropy, the conditional response space is used, and for the
frequency entropy, the response space is that of the full
temporal words.

There is some subtlety as to how the number of ‘rele-
vant’ response bins should be counted. If zero occupancy
count is observed, it is ambiguous whether that indicates
true zero occupancy probability or local undersampling.
Naive counting of the bins based on raw occupancy prob-
abilities results in underestimation of the bias (in fact
providing a lower bound upon it) and thus underesti-
mation of the entropy. An alternative strategy is to use
Bayes’ theorem to reestimate the expectation value of the
number of occupied bins, as described in [13].

These procedures were compared by estimating the
entropy of a time-dependent simulated Poisson process
for diﬀerent data sizes. The series estimator using the
Bayesian bias estimate is the only one which gives accept-
able performance in the range of 10-20 trials per stimulus,
although there is little to choose between it and the naive
series estimator above about 50 trials. The uncorrected
frequency (‘brute force’) estimator is inadequate.

Also shown in Fig. 1 is the Ma lower bound upon the
entropy [14], which performs comparatively poorly. The
Ma bound has been proposed as a useful bound which
is relatively insensitive to sampling problems [6]. The
Ma bound is tight only when the probability distribution
of words at ﬁxed spike count is close to uniform; this is
not the case in general. To understand the behaviour of
the Ma bound for short time windows, we calculated se-
ries terms. The Ma entropy already diﬀers from the true
entropy at ﬁrst order:

T H Ma

t =

1
ln 2

a
X

ta
1
X

hra(ta

1; s)∆tis

−

hra(ta

1; s)∆tis log2

a;ta
1
X

P

a;ta
1

P

a;ta
1

hra(ta

s ∆t

1; s)i2
1; s)is

hra(ta

(12)

variations, or more cells with diﬀerent response proﬁles,
the Ma bound would be worse than depicted.

<

<

2.5

1.5

2

1

)
s
t
i
b
(
 
y
p
o
r
t
n
E

0.5

0
0
10

0.6

0.5

0.4

0.3

0.2

y
c
n
e
c
i
f
f
e
 

i

i

g
n
d
o
c

0.1
0
10

1
10

2
10

3
10

4
10

3
10

4
10

1
10

2
10
Trials per stimulus

total entropy (Bayes counting)    
noise entropy (Bayes counting)    
total entropy (naive counting)    
frequency entropy (naive counting)
frequency entropy (uncorrected)   
Ma bound                          

FIG. 1. Estimates of the entropy of an inhomogeneous
Poisson process with mean rate r(t) = 50 sin(2π50t)
spikes/sec. in response to one stimulus, and zero spikes/sec.
for a second equiprobable stimulus. 30 ms time windows of
data are used for all curves, with a bin width of 3 ms. The ar-
rowhead at the upper right corner indicates the true entropy,
calculated analytically. Inset: the eﬀect of entropy and in-
formation bias on the estimated coding eﬃciency. Compared
are the bias-corrected series entropy (our best estimate; solid
line) and the uncorrected frequency entropy (dot-dashed line).

The mutual

information suﬀers less from sampling
problems than do the individual entropies, since to some
extent the biases of the total and noise entropies elim-
inate. This is particularly true for a small number of
stimuli; the eﬀect will diminish as the number of stim-
uli increases, and the bias behaviour of the information
can be expected to become worse (see Eqn. 11). A re-
lated quantity often used to characterise neural coding,
the coding eﬃciency [15,5] (deﬁned as the mutual infor-
mation divided by the total entropy), does not have this
elimination property, and in fact compounds the eﬀects
of both total and noise entropy biases. This is shown
in the inset of Fig. 1, which shows the coding eﬃciency
versus the number of trials of data per stimulus for both
the bias-corrected series estimator (solid) and the raw
frequency (‘brute-force’ ) estimator (dot-dashed). One
might be cautioned against use of the brute-force ap-
proach for calculating the information eﬃciency.

The ﬁrst order approximation coincides with the true en-
tropy rate if and only if there are no variations of rate
across time and cells. If there were higher frequency rate

To demonstrate their applicability, we applied these
techniques to data recorded from the primary visual cor-
tex (V1) of anaesthetised macaque monkeys [17]. The

s
m
 
0
3
 
n
i
 
s
t
i
b

4.5

3.5

4

3

2

1

2.5

1.5

0.5

0
2 

one empirical assumption made in this analysis (Eq. 3 -
that the probability of observing a spike at time ti given
that one has been observed at time tj scales with ∆t) was
examined by computing the average conditional spiking
probability as the bin width is decreased (i.e. spikes are
observed with higher precision). This assumption might
be expected to break down if there were spikes synchro-
nised with near-inﬁnite precision. For all cells examined,
the assumption was valid, as is shown in the inset of
Fig. 2.

complex cells
simple cells 

0
10

y
t
i
l
i

b
a
b
o
r
P

−1

10

−2

10

Entropies

−3

10

1 

  

  
Precision ∆ t (ms)

  

  

           10

0.3

0.2

0.1

0

i

y
c
n
e
c
i
f
f
e
 
g
n
d
o
C

i

Efficiencies

Information

3 

4 

5 

6 

7 

8 

9 

10

Precision ∆t (ms)

FIG. 2. Temporal entropy and information in spike trains
recorded from 7 simple and 14 complex cells in the monkey
primary visual cortex. For clarity population mean and stan-
dard error are shown. Black symbols at the far right indicate
complex cell spike count entropy (diamond) and information
(circle). White symbols similarly for simple cells. Coding ef-
ﬁciencies are also shown (lines without error bars; right axes).
Inset: The conditional spiking probability scales with ∆t as
the binwidth becomes small. Each symbol type represents a
diﬀerent cell (black, a complex cell and white, a simple cell).

Fig. 2 shows entropy estimates for two classes of V1
cells. The entropy of the spiking process continued to
rise as the observation precision was increased, up to a
resolution of 2 ms. For 30 ms time windows and 2 ms
bin width, the information rate for the complex cells was
9 ± 1 (s.e.m.) bits/sec., or 0.5 bits per spike. For the
simple cells it was 11 ± 2 bits/sec. or 1.5 bits per spike.
The coding eﬃciencies of up to 19 and 31% (maximal in
the spike count limit) for this type of stimulation were
substantially below the > 50% eﬃciencies that have been
reported for insect sensory neurons [5,6,16].

As neuroscience enters a quantitative phase of develop-
ment, information theoretic techniques are being found
useful for the analysis of data from physiological experi-
ments. Sampling considerations have however prevented
their application to many interesting experimental prepa-

rations. The methods developed here broaden the scope
of the study of neuronal information properties consider-
ably. In particular, they make possible the reliable anal-
ysis of recordings from both the anaesthetised and awake
mammalian cerebral cortex.

SRS is supported by the HHMI, and SP by the Well-

come Trust.

[1] N. Wedenskii, Bull. de l’Acad. de St. Petersbourg
XXVIII, 290 (1883); E. D. Adrian, J. Physiol. (Lond.)
61, 49 (1926).

[2] D. MacKay and W. S. McCulloch, Bull. Math. Biophys.

[3] F. Theunissen et al., J. Neurophys. 75, 1345, 1996.
[4] R. R. de Ruyter van Steveninck et al., Science 275, 1805

14, 127 (1952).

(1997);

[5] F. Rieke, D. Warland, R. de Ruyter van Steveninck, and
W. Bialek, Spikes: exploring the neural code (MIT Press,
Cambridge, MA, USA, 1997).

[6] S. Strong et al., Physical Review Letters 80, 197 (1998).
[7] A number of previous studies have reported ﬁrst order
expansions of the information: W. E. Skaggs, B. L. Mc-
Naughton, K. Gothard, and E. Markus, in Advances in
Neural Information Processing Systems, eds. S. Hanson,
J. Cowan, and C. Giles (Morgan Kaufmann, San Mateo,
1993), Vol. 5, pp. 1030–1037; S. Panzeri et al., Network
7, 365 (1996).; N. Brenner et al. physics/9902067. Sec-
ond order series expansion of the spike count information
from an ensemble of cells was performed in [8]. An alter-
native cluster expansion method has also been used by
M. de Weese, Network 7, 325 (1996).

[8] S. Panzeri et al. Proc. R. Soc. Lond. B 266, 1001 (1999).
[9] G. T. Buracas et al., Neuron 20, 959 (1998).
[10] S. Panzeri and S. R. Schultz, physics/9908027.
[11] L.F. Abbott and P. Dayan, Neur. Comp. 11, 91-101
(1999); M. W. Oram et al., Trends in Neurosci. 21, 259-
265 (1998).

[12] C. F. Brody, Neur. Comp. 11, 1536 (1999).
[13] S. Panzeri and A. Treves, Network 7, 87 (1996).
[14] S.-K. Ma, J. Stat. Phys. 26, 221 (1981).
[15] F. Rieke, D. Warland and W. Bialek, Europhys. Lett. 22,

151 (1993).

[16] A. Dimitrov and J. P. Miller, Neurocomputing, in press.
[17] The data used was from procedures to extract the ori-
entation tuning of V1 cells. The stimuli were sinusoidal
gratings of 16 diﬀerent orientations placed over the re-
ceptive ﬁeld of the cell. Each cycle of the moving grating
was considered to be an experimental trial; only cells with
≥ 96 cycles available were selected from the database, in
order to study timing precisions as high as 2 ms. J. R.
Cavanaugh, W. Bair and J. A. Movshon, Society for Neu-
roscience Abstracts, 24, 1875 (1998); J. R. Cavanaugh,
W. Bair and J. A. Movshon, Society for Neuroscience Ab-
stracts, 25, 1048 (1999). See J. A. Movshon and W. T.
Newsome, J. Neurosci. 16, 7733, 1996 for experimental
methods from this laboratory. We thank J. Cavanaugh,
W. Bair and J.A. Movshon for making their data avail-
able to us.


3
0
0
2
 
n
a
J
 
9
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
4
1
0
1
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

What causes a neuron to spike?

Blaise Ag¨uera y Arcas1 and Adrienne L. Fairhall2
1Rare Books Library and 2Department of Molecular Biology,
Princeton University, Princeton, New Jersey 08544
blaisea@princeton.edu fairhall@princeton.edu

February 2, 2008

Abstract

The computation performed by a neuron can be formulated as a combination of
dimensional reduction in stimulus space and the nonlinearity inherent in a spiking
output. White noise stimulus and reverse correlation (the spike-triggered average and
spike-triggered covariance) are often used in experimental neuroscience to ‘ask’ neu-
rons which dimensions in stimulus space they are sensitive to, and to characterize the
nonlinearity of the response. In this paper, we apply reverse correlation to the simplest
model neuron with temporal dynamics—the leaky integrate-and-ﬁre model—and ﬁnd
that even for this simple case standard techniques do not recover the known neural
computation. To overcome this, we develop novel reverse correlation techniques by
selectively analyzing only ‘isolated’ spikes, and taking explicit account of the extended
silences that precede these isolated spikes. We discuss the implications of our methods
to the characterization of neural adaptation. Although these methods are developed
in the context of the leaky integrate-and-ﬁre model, our ﬁndings are relevant for the
analysis of spike trains from real neurons.

1 Introduction

There are two distinct approaches to characterizing a neuron based on spike trains recorded
during stimulus presentation. One can take the view of a downstream neuron, and ask how
the spike train should be decoded to reconstruct some aspect of the stimulus. Alternatively,
one can take the neuron’s own view, and try to determine how the stimulus is encoded,
i.e. which aspects of the stimulus trigger spikes. This paper is concerned with the latter
problem: the identiﬁcation of relevant stimulus features from neural data.

The earliest attempts at neural characterization, including classic electrophysiological
experiments like those of Adrian [1], presented the neural system with simple, highly stereo-
typed stimuli with one or at most a few free parameters; this makes it relatively straightfor-
ward to map the input/output relation, or tuning curve, in terms of these parameters. While

1

this approach has been invaluable, and is still often used to advantage, it has the shortcoming
of requiring a strong assumption about what the neuron is ‘looking for’. As this assumption
is relaxed by probing the system using stimuli with a larger number of parameters, the length
of the experiment needed to probe neural response grows combinatorially.

In some cases, white noise can be used as an alternative stimulus which naturally contains
‘every’ signal [2]. The stimulus feature best correlated with spiking is then the spike triggered
average (STA), i.e.
the average stimulus history preceding a spike [3, 4]. Such reverse
correlation methods have been further developed and applied to characterize the motion-
sensitive identiﬁed neuron H1 in the ﬂy visual system [5]. By extending reverse correlation
to second order, one can identify multiple directions in stimulus space relevant to the neuron’s
spiking ‘decision’ [6, 7]. These techniques allow the experimenter to probe neural response
stochastically, using minimal assumptions about the nature of the relevant stimulus.

In this paper, we will examine in detail the relation between the stimulus features ex-
tracted through white noise analysis and the computation actually performed by the neuron.
In the usual interpretation of white noise analysis, the features extracted using reverse cor-
relation are considered to be linear ﬁlters which, when convolved with the stimulus, provide
the relevant inputs to a nonlinear spiking decision function. While the features recovered
by reverse correlation are optimal for stimulus reconstruction [8], we will show that they
are distinct from the ﬁlter or ﬁlters relevant to the neuron’s spiking decision. In particular,
reconstruction ﬁlters necessarily depend on the stimulus ensemble, while the ﬁlters relevant
for spiking—at least, for simple neurons like leaky integrate-and-ﬁre—are properties of the
model alone.

The main diﬃculty in interpreting the spike triggered average is that spikes interact,
in the sense that the occurrence of a spike aﬀects the probability of the future occurrence
of a spike. Both stimulus reconstruction and the identiﬁcation of triggering features are
aﬀected by this issue. Some of the eﬀects of correlations between spikes have been treated in
[9, 10, 11, 12]. To separate the inﬂuence of previous spikes from the inﬂuence of the stimulus
itself, we propose to analyze only ‘isolated’ spikes—spikes suﬃciently distant in time from
previous spikes that they can be considered to be statistically independent. This procedure
allows us to recover exactly the relevant stimulus features. However, it introduces additional
complications. By requiring an extended silence before a spike, we bias the prior statistical
ensemble. This bias will emerge in the white noise analysis, and we will have to identify it
explicitly.

We carry out this program for the simplest model neuron with temporal dynamics: the
leaky integrate-and-ﬁre neuron. We choose this model because, unlike more complex models,
we know exactly which stimulus feature causes spiking, and can make a precise comparison
with the output of the analysis.

The methods presented and validated in the present work using the integrate-and-ﬁre
model have been applied to the more biologically interesting Hodgkin–Huxley neuron [13],
resulting in new insights into its computational properties. Application of the same analytic
techniques to real neural data is currently underway.

2

2 Characterizing neural response

In this section we brieﬂy review material that has appeared elsewhere [14, 13, 7].

2.1 The linear/nonlinear model

Consider a neuron responding to a dynamic stimulus I(t). While here we will consider I to
be a scalar function, in general, it may depend on any number of additional variables, such
as frequency, spatial position, or orientation. We can suppress these dependencies without
loss of generality. Even without additional dependent variables, I(t) is a high-dimensional
stimulus, as the occurrence of a spike at time t0 generally depends on the stimulus history,
I(t < t0). The eﬀective dimensionality of this input is determined by the temporal extent
of the relevant stimulus, and the eﬀective sampling rate, which is imposed by biophysical
limits. For the computation of the neuron to be characterizable at all, the dimensionality of
the stimulus relevant to spiking must generally be much lower than this total dimensionality.
The simplest approach to dimensionality reduction is to approximate the relevant dimensions
as a low-dimensional linear subspace of I(t).

If the neuron is sensitive only to a low dimensional linear subspace, we can deﬁne a small

set of signals s1, s2,

, sK by ﬁltering the stimulus,

· · ·

sµ =

dτ fµ(τ )I(t0

τ ) ,

−

∞

0
Z

so that the probability of spiking depends only on these few signals,

P [spike at t0

I(t < t0)] = P [spike at t0] g(s1, s2,

, sK) .

|

· · ·

Classic characterizations of neurons in the retina, for example [15], typically assume that
these neurons are sensitive to a single linear dimension of the stimulus; the ﬁlter f1 then cor-
responds to the (spatiotemporal) receptive ﬁeld, and g is proportional to the one-dimensional
tuning curve. A number of other neurons, particularly early in sensory pathways, have been
similarly characterized, e.g. [16, 17, 18]. Here we will concentrate primarily on the problem
; once they are found, g can be obtained easily by direct sampling
fµ}
of ﬁnding the ﬁlters
{
, sK], assuming that the relevant dimensionality K is small.
s1, s2,
of P [spike at t0
· · ·

|

2.2 Reverse correlation methods

While one would like to determine the neural response in terms of the probability of spiking
I(t < t0)], we follow [5] in considering instead the distri-
given the stimulus, P [spike at t0
bution of signals conditional on the response, P [I(t < t0)
spike at t0]. This quantity can be
extracted directly from the known stimulus and the resulting spike train. The two probabil-
ities are related by Bayes’ rule,

|

|

P [spike at t0

I(t < t0)]

P [I(t < t0)

spike at t0]

|

P [spike at t0]

|
P [I(t < t0)]

.

(3)

(1)

(2)

=

3

P [spike at t0] is the mean spike rate, and P [I(t < t0)] is the prior distribution of stimuli. If
I(t) is Gaussian white noise, then this distribution is a multidimensional Gaussian; further-
more, the distribution of any ﬁltered version of I(t) will also be Gaussian.

The ﬁrst moment of P [I(t < t0)

spike at t0] is the spike-triggered average,

STA(τ ) =

[dI] P [I(t < t0)

spike at t0]I(t0

τ ) .

|

−

|

Z

We can also compute the covariance matrix of ﬂuctuations around this average,

Cspike(τ, τ ′) =

Z

[dI] P [I(t < t0)

spike at t0]I(t0

τ )I(t0

|

−

−

−

τ ′)

STA(τ )STA(τ ′).

(5)

In the same way that we compare the spike-triggered average to some constant average
level of the signal in the whole experiment, an important advance of [6] was to compare the
covariance matrix Cspike with the covariance of the signal averaged over the whole experiment,

Cprior(τ, τ ′) =

[ds] P [s(t < t0)]s(t0

τ )s(t0

−

τ ′) .

−

Z

If the probability of spiking depends on only a few relevant features, the change in the
covariance matrix, ∆C = Cspike
Cprior, has a correspondingly small number of outstanding
−
stimulus] depends on K linear projections of the stimulus as
eigenvalues. Precisely, if P [spike
|
in Eq. (2), and if the inputs I(t) are chosen from a Gaussian distribution, then the rank of the
matrix ∆C is exactly K. Further, the eigenmodes associated with nonzero eigenvalues span
the relevant subspace. A positive eigenvalue indicates a direction in stimulus space along
which the variance is increased relative to the prior, while a negative eigenvalue corresponds
to reduced variance.

3 Leaky integrate-and-ﬁre

The leaky integrate-and-ﬁre neuron is perhaps the most commonly used approximation to
real neurons. Its dynamics can be written:

(4)

(6)

(7)

C

dV
dt

V
R −

−

= I(t)

CVc

δ(t

ti) ,

−

Xi

where C is a capacitance, R is a resistance and Vc is the voltage threshold for spiking.
The ﬁrst two terms on the right hand side model an RC circuit; the capacitor integrates
the input current as a potential V while the resistor dissipates the stored charge (hence
ti}
‘leaky’). The third term implements spiking: spikes are deﬁned as the set of times
such that V (ti) = Vc. When the potential reaches Vc, a restoring current is instantaneously
injected to bring the stored charge back to zero, resetting the system. This formulation
emphasizes the similarity between leaky integrate-and-ﬁre and the more biophysically real-
istic Hodgkin–Huxley equation for current ﬂow through a patch of neural membrane; in the

{

4

Hodgkin–Huxley system, however, the nonlinear spike-generating term is replaced with ionic
currents of a more complex (and less singular) form. Hence the simple properties of the
leaky integrate-and-ﬁre model—a single degree of freedom V , instantaneous spiking, total
resetting of the system following a spike, and linearity of the equation of motion away from
spikes—are only rough approximations to the properties of realistic neurons.

Integrating Eq. (7) away from spikes, we obtain

assuming initialization of the system in the distant past at zero potential. The right hand
side can be rewritten as the convolution of I(t) with the causal exponential kernel

CV (t) =

dτ exp

I(τ ) ,

t

−∞

Z

τ
t
−
RC (cid:19)

(cid:18)

f∞(τ ) =

0
exp(

(

τ /RC)

−

if τ
0
≤
if 0 < τ ,

CVc =

dτ f∞(τ )I(t1

τ ) .

−

∞

−∞

Z

so the condition for a spike at time t1 is that this convolution reach threshold:

The interaction between spikes complicates this picture somewhat. If the system spiked last
at time t0 < t1, we must replace the lower limit of integration in Eq. (8) with t0, as no
current injected before t0 can contribute to the accumulated potential. However, if we wish
always to evaluate stimulus projections by integrating over the entire current history I(t) as
in Eq. (10), then we must replace f∞ with an entire family of kernels dependent on the time
to the previous spike, ∆t = t1

t0:

−

f∆t(τ ) = 


0
exp(
0

−

τ /RC)

≤

if τ
0
if 0 < τ < ∆t
if ∆t

τ .

≤

This equation completes an exact model of the leaky integrate-and-ﬁre neuron.



4 Response to white noise

We now consider the statistical behavior of the integrate-and-ﬁre neuron when driven with
Gaussian white noise current of standard deviation σ,

For the following analysis, we take R = 10 kΩ, C = 1 µF, Vc = 10 mV and σ = √200 µA,
and use Euler integration with a time step of 0.05 msec. These values were chosen to mimic
biophysically plausible parameters for a 1 cm2 patch of membrane, but none of our results
depend qualitatively on this choice.

I(t)
h
I(t)I(t′)

i
i

h

= 0,
= σ2δ(t

t′).

−

5

(8)

(9)

(10)

(11)

(12)

Figure 1: Spike-triggered average of a leaky integrate-and-ﬁre neuron; shown inset with a
logarithmic I-axis and the pure exponential σf∞ (dotted line).

4.1 Unqualiﬁed reverse correlation

(9).

At ﬁrst glance, it would appear that the neuron is perfectly described by a reduced model
in one dimension: the current ﬁltered by the exponential kernel of Eq.
If so, the
spike-triggered average under white noise stimulation should be proportional to this ﬁlter.1
However, as is evident in Fig. 1, the STA is not exponential, nor does it asymptote to
the appropriate decay rate 1/RC. One reason is that, in the presence of previous spikes,
the integrate-and-ﬁre neuron is not low-dimensional. Recall that the relative timing ∆t of
the previous spike selects a ﬁlter f∆t (Eq. (11)); these ﬁlters are truncated versions of the
original exponential f∞. The STA will therefore include contributions from each of these
ﬁlters, weighted by their probability of occurrence (as determined by the interspike interval
distribution).2
It is readily shown that the orthogonal component of each ﬁlter f∆t is a
∆t; so the set of relevant ﬁlters spans the entire stimulus space.
δ-function at
Covariance analysis reveals this high dimensionality very clearly. In Fig. 2 we show the
spectrum of eigenvalue magnitudes of ∆C as a function of the number of spikes summed to
accumulate the matrix; with decreasing noise, an arbitrary number of signiﬁcant eigenmodes
emerge. As shown in Fig. 3, none of these eigenmodes correspond to an exponential ﬁlter.
1In possible linear combination with a derivative-like ﬁlter, arising from the additional constraint that

−

the threshold must be crossed from below. This point will be addressed in more detail later.

2This is not a prescription for recovering f∞ from the STA: the STA is further inﬂuenced by other eﬀects,

including the average stimulus histories leading up to previous spikes.

6

The ﬁrst mode most closely resembles an exponential, but as shown in the right half of Fig.
3, it is more peaked near t = 0, and its time constant is very diﬀerent from RC.

Figure 2: Eigenvalue magnitudes of the spike-triggered covariance diﬀerence matrix ∆C as
a function of the number of spikes used to accumulate the matrix. An arbitrary number of
stable modes emerge, given enough data. The stable modes all have negative eigenvalues,
i.e. spikes are associated with reduced stimulus variance along these dimensions. The dotted
line, shown for reference, is proportional to 1/

Nspikes.

4.2

Isolated spikes

It is clear that making progress requires ﬁrst considering the causes of a single spike in-
dependently of the spiking history. Therefore we will include in our analysis only spikes
which follow previous spikes after a suﬃciently long interval. The appropriate interval can
be estimated from the interspike interval distribution, Fig. 4.

As in many real neurons, the interval distribution has three main features: a ‘refractory
period’ during which spikes are strongly suppressed, a peak at a preferred interval, and an
exponential tail. Intervals in the exponential tail follow Poisson statistics; for intervals in
this range, we can take spikes to be statistically independent. With reference to Fig. 4, we
set our isolated spike criterion—very conservatively—as 75 msec of previous ‘silence’.

As shown in Fig. 5, even with this constraint, we do not recover a spike-triggered average
matching f∞. We should note that this is no longer, strictly speaking, a spike-triggered
average, but an event-triggered average, where the event includes the preceding silence.
Thus we can expect the silence itself to contribute to any average we construct. To ﬁrst
order, during silence there is a constant negative bias, since certain positive currents which
would have caused a spike are excluded from the average. Similarly, we expect that the

q

7

Figure 3: Left: the leading six modes of the unqualiﬁed spike-triggered change in covariance
∆C. Right: only the ﬁrst mode is monophasic; here it is plotted on a logarithmic y-axis
(circles) with f∞ for reference (solid line).

Figure 4: Interspike interval histogram for a leaky integrate-and-ﬁre neuron. 5
were accumulated in 0.27 msec bins.

×

106 spikes

8

Figure 5: Triggered average stimulus for isolated spikes. Isolated spikes are deﬁned as spikes
which are preceded by at least 75 msec of silence. The inset shows the triggered average with
the constant silence bias subtracted on a logarithmic I-axis, again with the pure exponential
σf∞ drawn as a dotted line for reference.

covariance of the current will be altered from that of the Gaussian prior during extended
silences. Because extended silence is not locked to any particular moment in time, the
covariance matrix is stationary during silence.

We construct the covariance matrix as before: ∆Ciso = Cisolated spike

Cprior. An entire
series of modes once again appears, Fig. 6. This time, they arise because silence is, by
deﬁnition, translationally invariant, resulting in a Fourier-like spectrum. Modes associated
with the spike, on the other hand, are locked to a particular time, and should therefore not
have Fourier-like modes. In particular, these spiking modes should have temporal support
only in the time immediately before the spike. This implies that two types of modes—silence-
and spike-associated—should emerge independently in the covariance analysis, and should
exhibit a clear diﬀerence in their domain of support.

−

−

−

45] msec. At

In Fig. 7, we consider the fraction of the energy of each mode over the interval t
65,

∈
45 msec, the exponential ﬁlter f∞ has decayed to 1% of its peak
[
−
value, so we are well away from the spike at t = 0. The 75 msec silence criterion also puts
the lower boundary of this interval well inside the stationary silence (at least 10 msec after
the previous spike). As we see, exactly two modes emerge as local, with their energy fraction
decaying with the noise in the matrix until reaching a stable value well below the rest of the
modes.

The ﬁrst localized mode, shown in Fig. 8, is the long-awaited exponential ﬁlter f∞. Using
our modiﬁed second-order reverse correlation analysis on ‘experimental data’ we have thus
ﬁnally been able to recover the stimulus feature to which the leaky integrate-and-ﬁre neuron
is sensitive.

9

Figure 6: Eigenvalue magnitudes of the isolated spike-triggered change in covariance ∆Ciso
as a function of the number of isolated spikes used to accumulate the matrix. The stable
modes again have negative eigenvalues.

65, 0]
Figure 7: Fraction of the energy in each mode of ∆Ciso (modes are deﬁned over t
msec) in the silent interval t
45] msec, as a function of the number of isolated
spikes used to accumulate ∆Ciso. The eigenvalue for a mode with zero energy in this interval
would decrease like 1/Nspikes, as the only contribution would come from noise. Here, two
modes emerge with very small energy in this interval, though they stabilize at small nonzero
values. These modes are localized to the time immediately before the spike, but as they have
exponential support, their energy in the silence does not fall all the way to zero.

[
−

[
−

65,

−

∈

∈

10

Figure 8: The two spike-associated modes of ∆Ciso. The ﬁrst (left) is the recovered expo-
nential ﬁlter of the leaky integrate-and-ﬁre spiking condition; the measured mode is marked
with dots, and the line is f∞ (normalized). The second mode (right) is the ‘ﬁrst crossing’
constraint.

4.3 Constraints
The second mode is a consequence of an implicit constraint in the spiking model. Na¨ively,
˙V > 0; that is, the threshold must be crossed
we know that at the moment when V = Vc,
from below. Hence

∞

−∞

Z

dτ I(t

τ )

f∞(τ ) < 0,

d
dτ

−

making the neuron also sensitive to the time derivative of f∞. The integrate-and-ﬁre neuron
is peculiar in that f∞ and df∞/dτ are almost linearly dependent:

d
dτ

f∞(τ ) =

δ(τ )

(

−

(RC)−1 exp(

τ /RC)

−

if τ
0.
≤
if 0 < τ .

The orthogonal part of this ﬁlter is therefore only δ(τ ), and we can rewrite this extra con-
dition more simply as I(0) > Vc/R.

However, we see that although the second spiking mode is indeed very sharply peaked
at τ = 0, it is not a δ-function. While the above argument is correct in constraining the
possible values of I(0), we have also required no threshold crossings for an extended time
prior to the spike. Hence certain trajectories I(t) which satisfy the two basic conditions
(V (0) = Vc and ˙V (0) > 0) are still disallowed, because they would have led to a spike earlier.
The second mode is therefore an extended version of the derivative condition. Although it
appears to have very extended support, this is due to orthogonalization with respect to the
ﬁrst mode, f∞. The true constraint is positive deﬁnite and highly peaked, with virtually all
of its energy concentrated in the

5 msec interval prior to the spike.

We can more easily understand this constraint by considering the time-dependent dis-
tribution of V (t) leading up to an isolated spike, shown in the left panel of Fig. 9. The
process V (t) leading up to a spike is a centrally biased random walk with an absorbing

∼

(13)

(14)

11

barrier at Vc and a capacitatively determined correlation time. If we trace the distribution
of V backward in time from the spike at t = 0, we see a constrained diﬀusive process from
a δ-function at the spike time to the steady-state silence distribution. The constraint on V
enforces a corresponding constraint on I, whose time-dependent distribution is shown in the
right panel of Fig. 9. Notice that the I distribution is only noticeably distorted from its
silence steady-state at times very near the spike.

Figure 9: The time-dependent distributions of V (t) (left) and I(t) (right) leading up to
an isolated spike at t = 0.
In both cases, the distributions with the highest means
correspond to the ﬁnal time slice (they are thus also the most constrained). For V ,
msec. For I, the times are
,
the times shown are
0.05
0.3,
0.4
msec. The
15 msec;

14,
0.6,
{−
}
distribution of V is virtually indistinguishable from its silence steady-state by
convergence to the silence steady-state is much faster for I, occurring by

1.5,
−
, and

−
5 msec.

14.5,
0.8,

0.5
}
0.35,

−
{−

−
{−

15,
,

{−
1.05

· · ·
−

5.05,

4.05,

−
−

−
}

· · ·

· · ·

−

−

−

−

1,

}

,

,

−

4.4 Silence modes

Finally, let us return to the spectrum of silence-associated modes in Fig. 7. Several of these
are shown overlaid in Fig. 10. It is immediately clear that they have the expected Fourier-like
structure over the period of silence. Their behavior near the spike is less obvious, however:
each mode appears to execute an FM chirp, culminating in a sharp feature at the spike time
itself. Clearly, while it is true by construction that spike-associated covariance modes have
no support in periods of extended silence, the converse is not true. This structure is due to
the essential non-stationarity of silence near spikes: at some point, the silence must come to
an abrupt end.

It is clear that in order to implement our procedure for separating silence from spikes,
we could not have taken the approach of looking for modes with signiﬁcant energy near the
spike. All of the silence modes have interesting structure near the spike, and could easily
be mistaken for additional spike-generating features. Like the spike-associated modes, these
silence modes have negative eigenvalues, indicating reduced stimulus variance in certain

12

Figure 10: The leading six modes of ∆Ciso associated with silence (i.e. with nonvanishing
65,
support over t
45] msec). Although they are Fourier-like in the silence, they chirp
−
leading up to the spike.

[
−

∈

dimensions. Yet they do not cause spiking, nor do they inhibit it; they are in no meaningful
sense ‘suppressive’ modes. They simply show us the second-order statistics of silences, which
emerge from the enforced absence of spikes. This underscores the necessity of considering
a suﬃciently extended silence before isolated spikes for the translationally invariant part of
the silence modes to emerge, distinguishing them from spiking modes.

This structure also highlights the reason why we could not, as one might expect, simply
subtract from Cisolated spike a prior obtained from extended silences instead of Cprior to obtain
a simple, low-dimensional ∆Ciso. While such a subtraction results in a local matrix around
the spike, it cannot take into account the non-stationary part of the silence, and leaves
the spike-related and non-stationary silence-related aspects of the covariance near the spike
mixed. This again results in a high-dimensional matrix with incorrect (mixed) modes.

5 Adaptation

It has previously been observed that the STAs of some neurons depend on the stimuli used
to probe their response [16, 19]. White noise stimulus was originally introduced to circum-
vent this diﬃculty by sampling stimulus space in an unbiased way; however, one must still
choose a noise variance. While one would hope that this does not aﬀect the STA, it has been
shown that, at least in some sense, even the integrate-and-ﬁre neuron ‘adapts’ to the stimulus
variance [20, 18], showing both a change in the overall STA and in its measured nonlinear
decision function. The reason for this form of adaptation is simple: the statistics of spike
intervals depend on the stimulus variance, as the time to reach threshold depends on the

13

variance. By construction, however, neither the spike-triggering feature f∞ nor the decision
function g (here simply the threshold) of the integrate-and-ﬁre neuron actually depends on
the stimulus statistics. As we have discussed, the unqualiﬁed STA is a linear combination of
the ﬁlters f∆t conditioned on the time to the last spike; the contribution of each ﬁlter to the
overall STA thus depends on the probability P (∆t) of the interspike interval ∆t. As diﬀerent
stimulus ensembles produce diﬀerent interspike interval distributions, these coeﬃcients are
functions of the stimulus ensemble. Therefore even in the absence of adaptation in the linear
ﬁlters f , the overall STA will be stimulus-dependent. Because sampling the input/output
relation normally involves convolving the stimulus with the STA, the measured nonlinearity
will also appear to be stimulus-dependent, despite the fact that the neuron’s nonlinear de-
cision function g (based on convolution of the stimulus with the ﬁxed ﬁlters f ) is also ﬁxed.
Hence this form of ‘adaptation’ does not reﬂect any changing property or internal state of
the neuron in response to the stimulus statistics. Rather, it arises from the nonlinearity
inherent in spiking itself.

This emphasizes the signiﬁcance of our methods: we have introduced a way to extract
from data the intrinsic, unique spike-triggering feature, removing the eﬀects of the stimulus-
dependent interspike interval statistics. These statistics are produced by the model when
driven by a particular stimulation regime; they are not part of the model itself. If a neuron
does in fact exhibit ‘true’ adaptation in the sense that the functions f and g are variance-
dependent, then the isolated spike analysis presented here will reveal this. Such variance
dependence can come about as a result of explicit gain control mechanisms, e.g.
[21], or
because nonlinearities in the subthreshold neural response select non-constant stimulus fea-
tures, an eﬀect which can play an important role in more complex models such as the
Hodgkin–Huxley neuron [13].

6 Discussion

While the interspike interval statistics are not an intrinsic model property, they do depend
on an aspect of the complete characterization of the neuron that we have explicitly neglected
here: the interspike interaction. There are two senses in which addressing only isolated spikes
appears to be a limitation of our method. One is practical:
in most situations, this will
substantially reduce the number of spikes considered in the analysis. Further, much of the
work on white noise analysis to date has focussed on the problem of stimulus reconstruction,
and our method does not provide a recipe for decoding non-isolated spikes. With respect to
the second point, there are good reasons for considering isolated spikes ﬁrst. Evidence from
several systems suggests that isolated spikes contain more information about the stimulus
than the spikes that follow them [15, 22, 23]. This is clear from ﬁrst principles, as non-isolated
spikes are triggered only partly by the stimulus, and partly by the timing of the previous
spike. Understanding the complex interactions between spikes and the stimulus to create
spike patterns is obviously an important next step in understanding the neural code, but
the logical starting point is to establish the causes of isolated spikes. If we attempt stimulus
reconstruction using reverse correlations averaged over all spikes, then the reconstruction

14

will be incorrect; similarly, predicting spike timing without knowledge of the recent spiking
history is generally impossible. A step along the path of treating spike interactions explicitly
has recently been made in the modelling of retinal ganglion cell spike trains [24].

The separation of spikes into isolated and non-isolated also allows a more eﬃcient use
of the data—ensemble averages will not be complicated by the inclusion of eﬀects from
interspike interaction. Thus although we are using less data, it is probably the case that we
are using the data more eﬀectively, by summing stronger covariance signals from a simpler,
lower-dimensional space. While in a simulation the amount of data can be unbounded, and
we have used up to millions of spikes to demonstrate our points here, we note that the
104 spikes. This can be seen from the
signiﬁcant eﬀects all emerge very clearly after
evolution of the eigenvalues and silence energy fractions with increasing spike count (Figs.
2, 6 and 7). This number is in a range often accessible to the experimenter.

∼

It should be noted that the need to consider the eﬀect of silence arises even in the
standard analysis of non-isolated spikes. Because all spiking neurons have a refractory period,
there is always an implicit silence or isolation constraint, albeit sometimes of only a few
milliseconds. The second spike-associated mode identiﬁed here (Fig. 8) is localized precisely
in the disallowed interval, and in fact emerges almost identically in the unqualiﬁed covariance
analysis. Its contribution to the highly peaked STA is very signiﬁcant, since both the spike-
conditional variance and mean along this stimulus dimension are substantially altered from
the prior. Recall that this mode is related to a derivative condition expressing upward
threshold crossing, but it is extended, as there is always a ﬁnite period of silence prior to a
spike. An analogous mode should arise for all spiking neurons, though its shape will depend
strongly on the shape of the spike-generating ﬁlter or ﬁlters. Reduced stimulus variance in
the direction of this derivative-like feature should not be thought of as part of the cause
of a spike; it is a necessary concomitant to spike generation conditioned on the ﬁrst ﬁlter
alone. This inevitable derivative-like condition is relevant to the inverse problem of stimulus
reconstruction from a spike train, but not to the forward problem of causally predicting spike
times from the stimulus. Notice that this implies that, in general, convolving the stimulus
with the STA—either unqualiﬁed or for isolated spikes only—is not an ideal way to predict
spike times.

In summary, we have shown that standard white noise analysis is unable to recover the
known ﬁlter properties of the leaky integrate-and-ﬁre neuron. We show that this is due to the
inﬂuence of interactions between spikes, and we demonstrate that by removing this inﬂuence,
requiring spikes to be ‘isolated’, we are able to recover the computation performed by the
neuron. However, in so doing, we introduce a complication: we must address not only the
isolated spikes, but the silences that precede them. Silences produce complex structure in the
covariance analysis, which is nonetheless statistically independent and clearly distinguishable
from structure tied to the spikes themselves. In a sense, the distinction between spike-related
and silence-related features is artiﬁcial, for silence is simply the complement to spiking:
silence is structured by the absence of features which would produce a spike. The distinction
is useful, however, in allowing us to use the isolated spikes to identify the spike-generating
feature or features unambiguously.

15

We have shown that failing to consider spikes in isolation leads to distortion in the ﬁlters
obtained through standard white noise reverse correlation. Our conclusions are relevant
not just for the simple case of the leaky integrate-and-ﬁre neuron, but for any system in
which the presence of a spike has an inﬂuence on the generation of subsequent spikes. The
new isolated spike method presented should improve the accuracy of white noise analysis of
neural spike trains with interspike interactions. In some cases, it may signiﬁcantly change
our picture of what stimulus features a neuron is sensitive to.

We would like to thank William Bialek for his advice, discussion, and encouragement
to study this problem.

16

References

1928.

1968.

[1] E. Adrian. The Basis of Sensation: The Action of the Sense Organs. Christophers, London,

[2] P.Z. Marmarelis and K. Naka. White-noise analysis of a neuron chain: an application of the

Wiener theory. Science, 175:1276–8, 1972.

[3] E. de Boer and P. Kuyper. Triggered correlation. IEEE Trans. Biomed. Eng., 15:169–179,

[4] Hugh Bryant and Jos´e P. Segundo. Spike initiation by transmembrane current: a white-noise

analysis. J. Physiol., 260:279–314, 1978.

[5] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement sensitive
information transfer in short spike sequences. Proc. Roy. Soc.

in the blowﬂy visual system:
Lond. B, 234:379–414, 1988.

[6] N. Brenner,
Steveninck.
http://xxx.lanl.gov/abs/physics/9902067.

S. Strong, R. Koberle, W. Bialek,

Synergy in a neural code. Neural Comp., 12:1531–1552, 2000.

and R. R. de Ruyter van
See also

[7] W. Bialek and R. R. de Ruyter van Steveninck. Features and dimensions: motion estimation

in ﬂy vision. in preparation, 2002.

[8] F. Rieke, D. Warland, W. Bialek, and R. R. de Ruyter van Steveninck. Spikes: exploring the

neural code. The MIT Press, New York, 1997.

[9] W. Kistler, W. Gerstner, and J. Leo van Hemmen. Reduction of the Hodgkin-Huxley equations

to a single-variable threshold model. Neural Computation, 9:1015–1045, 1997.

[10] M.C. Eguia, M.I. Rabinovich, and H.D. Abarbanel. Information transmission and recovery in

neural communications channels. Phys. Rev. E, 62:7111–7122, 2000.

[11] P. H. Tiesinga.

Information transmission and recovery in neural communications channels

revisited. Phys. Rev. E, 64, 2001.

[12] M.J. Chacron, A. Longtin, and L. Maler. Negative interspike interval correlations increase the
neuronal capacity for encoding time-dependent stimuli. J. Neurosci., 21:5328–5343, 2001.

[13] B. Ag¨uera y Arcas, A. L. Fairhall, and W. Bialek. Computation in a single neuron: Hodgkin
and Huxley revisited. 2002. preprint available at http://arxiv.org/abs/physics/0212113.

[14] B. Ag¨uera y Arcas, W. Bialek, and A. L. Fairhall. What can a single neuron compute? In
T.K. Leen, T.G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing
Systems 13, pages 75–81. MIT Press, 2001.

[15] M. J. Berry II and M. Meister. The neural code of the retina. Neuron, 22:435–450, 1999.

[16] F. Theunissen, K. Sen, and A. Doupe. Spectral-temporal receptive ﬁelds of nonlinear auditory

neurons obtained using natural sounds. J. Neurosci., 20:2315–2331, 2000.

17

[17] K.J. Kim and F. Rieke. Temporal contrast adaptation in the input and output signals of

salamander retinal ganglion cells. J. Neurosci., 21:287–299, 2001.

[18] L. Paninski, B. Lau, and A. Reyes. Noise–driven adaptation:

in vitro and mathematical

analysis. Neurocomputing, to appear, 2003.

[19] G.D. Lewen, W. Bialek, and R.R. de Ruyter van Steveninck. Neural coding of naturalistic

motion stimuli. Network, 12:317–329, 2001.

[20] M.E. Rudd and L.G. Brown. Noise adaptation in integrate-and ﬁre neurons. Neural Compu-

tation, 9:1047–1069, 1997.

[21] R. Shapley, C. Enroth-Cugell, A.B. Bonds, and A. Kirby. Gain control in the retina and retinal

dynamics. Nature, 236:352–353, 1972.

[22] R.R. de Ruyter van Steveninck and W. Bialek. Reliability and statistical eﬃciency of a blowﬂy

movement-sensitive neuron. Phil. Trans. R. Soc. Lond. B, 348:321–40, 1995.

[23] S. Panzeri, R.S. Petersen, S.R. Schultz, M. Lebedev, and M.E. Diamond. The role of spike
timing in the coding of stimulus location in rat somatosensory cortex. Neuron, 29:769–77,
2001.

[24] J. Keat, P. Reinagel, R. C. Reid, and M. Meister. Predicting every spike: a model for the

responses of visual neurons. Neuron, 30(3):803–817, 2001.

18


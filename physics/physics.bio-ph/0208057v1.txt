2
0
0
2
 
g
u
A
 
4
1
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
7
5
0
8
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Maximally informative dimensions:
Analyzing neural responses to natural signals

Tatyana Sharpee,1 Nicole C. Rust,2 and William Bialek1,3

1 Sloan–Swartz Center for Theoretical Neurobiology and Department of Physiology
University of California at San Francisco, San Francisco, California 94143–0444
2 Center for Neural Science, New York University, New York, NY 10003
3 Department of Physics, Princeton University, Princeton, New Jersey 08544
sharpee@phy.ucsf.edu, rust@cns.nyu.edu, wbialek@princeton.edu

February 2, 2008

We propose a method that would allow for a rigorous statistical analysis of
neural responses to natural stimuli, which are non–Gaussian and exhibit strong
correlations. We have in mind a model in which neurons are selective for a
small number of stimulus dimensions out of the high dimensional stimulus
space, but within this subspace the responses can be arbitrarily nonlinear. Ex-
isting analysis methods are based on correlation functions between stimuli
and responses, but these methods are guaranteed to work only in the case of
Gaussian stimulus ensembles. As an alternative to correlation functions, we
maximize the mutual information between the neural responses and projec-
tions of the stimulus onto low dimensional subspaces. The procedure can be
done iteratively by increasing the dimensionality of this subspace. Those di-
mensions that allow the recovery of all of the information between spikes and
the full unprojected stimuli describe the relevant subspace. If the dimensional-
ity of the relevant subspace indeed is small, it becomes feasible to map out the
neuron’s input–output function even under fully natural stimulus conditions.
These ideas are illustrated in simulations on model visual neurons responding
to natural scenes.

1 Introduction

From olfaction to vision and audition, a growing number of experiments [1]–[8] are ex-
amining the responses of sensory neurons to natural stimuli. Observing the full dynamic
range of neural responses may require using stimulus ensembles which approximate
those [9, 10] occurring in nature, and it is an attractive hypothesis that the neural repre-
sentation of these natural signals may be optimized in some way [11]–[14]. Many neurons
exhibit strongly nonlinear and adaptive responses that are unlikely to be predicted from
a combination of responses to simple stimuli; in particular neurons have been shown to

1

adapt to the distribution of sensory inputs, so that any characterization of these responses
will depend on context [15, 16]. Finally, the variability of neural response decreases sub-
stantially when complex dynamical, rather than static, stimuli are used [17]–[20]. All of
these arguments point to the need for general tools to analyze the neural responses to
complex, naturalistic inputs.

The stimuli analyzed by sensory neurons are intrinsically high dimensional, with di-
mensions D ∼ 102 − 103. For example, in the case of visual neurons, the input is speciﬁed
as light intensity on a grid of at least 10 × 10 pixels. Each of the presented stimuli can be
described as a vector s in this high dimensional stimulus space. It is important that stimuli
need not be pictured as being drawn as isolated points from this space. Thus, if stimuli
are varying continuously in time we can think of the stimulus s as describing a recent
window of the stimulus history (e. g., the past K frames of the movie, with dimensional-
ity K times larger than for the description of a single frame) and then the distribution of
stimuli P (s) is sampled along some meandering trajectory in this space; we will assume
this process is ergodic, so that we can exchange averages over time with averages over
the true distribution as needed.

Even though direct exploration of a D ∼ 102 − 103 dimensional stimulus space is
beyond the constraints of experimental data collection, progress can be made provided
we make certain assumptions about how the response has been generated. In the simplest
model, the probability of response can be described by one receptive ﬁeld (RF) or linear
ﬁlter [9]. The receptive ﬁeld can be thought of as a template or special direction v in
the stimulus space such that the neuron’s response depends only on a projection of a
given stimulus s onto v, although the dependence of the response on this projection can
be strongly nonlinear. In this simple model, the reverse correlation method [9, 21] can be
used to recover the vector v by analyzing the neuron’s responses to Gaussian white noise.
In a more general case, the probability of the response depends on projections si = ˆei · s
of the stimulus s on a set of vectors {ˆe1, ˆe2, ... , ˆen}:

P (spike|s) = P (spike)f (s1, s2, ..., sn),

(1)
where P (spike|s) is the probability of a spike given a stimulus s and P (spike) is the average
ﬁring rate. Even though the ideas developed below can be used to analyze input–output
functions f with respect to different neural responses, such as patterns of spikes in time
[22, 23], we choose a single spike as the response of interest. The vectors {ˆei} may also
describe how the time dependence of stimulus s affects the probability of a spike. We will
call the subspace spanned by the set of vectors {ˆei} the relevant subspace (RS).

Equation (1) in itself is not yet a simpliﬁcation if the dimensionality n of the RS is
equal to the dimensionality D of the stimulus space. In this paper we will use the idea of
dimensionality reduction [15, 22, 24] and assume that n ≪ D. The input–output function
f in Eq. (1) can be strongly nonlinear, but it is presumed to depend only on a small number
of projections. This assumption appears to be less stringent than that of approximate
linearity which one makes when characterizing neuron’s response in terms of Wiener
kernels (see, for example, the discussion in Section 2.1.3 of Ref. [9]). The most difﬁcult
part in reconstructing the input–output function is to ﬁnd the RS. Note that for n > 1, a
description in terms of any linear combination of vectors {ˆei} is just as valid, since we did
not make any assumptions as to a particular form of nonlinear function f .

2

Once the relevant subspace is known, the probability P (spike|s) becomes a function of
only few parameters, and it becomes feasible to map this function experimentally, invert-
ing the probability distributions according to Bayes’ rule:

f ({si}) =

P ({si}|spike)
P ({si})

.

If stimuli are chosen from a correlated Gaussian noise ensemble, then the neural response
can be characterized by the spike–triggered covariance method [15, 22, 24].
It can be
shown that the dimensionality of the RS is equal to the number of nonzero eigenvalues of
a matrix given by a difference between covariance matrices of all presented stimuli and
stimuli conditional on a spike. Moreover, the RS is spanned by the eigenvectors asso-
ciated with the nonzero eigenvalues multiplied by the inverse of the a priori covariance
matrix. Compared to the reverse correlation method, we are no longer limited to ﬁnding
only one of the relevant directions ˆei. Both the reverse correlation and the spike–triggered
covariance method, however, give rigorously interpretable results only for Gaussian dis-
tributions of inputs.

In this paper we investigate whether it is possible to lift the requirement for stimuli to
be Gaussian. When using natural stimuli, which certainly are non–Gaussian, the RS can-
not be found by the spike–triggered covariance method. Similarly, the reverse correlation
method does not give the correct RF, even in the simplest case where the input–output
function in Eq. (1) depends only on one projection. However, vectors that span the RS
clearly are special directions in the stimulus space independent of assumptions about
P (s). This notion can be quantiﬁed by Shannon information, and an optimization prob-
lem can be formulated to ﬁnd the RS. We illustrate how the optimization scheme works
with natural stimuli for model orientation sensitive cells with one and two relevant di-
rections, much like simple and complex cells found in primary visual cortex. It also is
possible to estimate average errors in the reconstruction. The advantage of this optimiza-
tion scheme is that it does not rely on any speciﬁc statistical properties of the stimulus
ensemble, and can thus be used with natural stimuli.

2 Information as an objective function

When analyzing neural responses, we compare the a priori probability distribution of all
presented stimuli with the probability distribution of stimuli which lead to a spike [22].
For Gaussian signals, the probability distribution can be characterized by its second mo-
ment, the covariance matrix. However, an ensemble of natural stimuli is not Gaussian,
so that neither second nor any other ﬁnite number of moments is sufﬁcient to describe
the probability distribution. In this situation, Shannon information provides the rigorous
way of comparing two probability distributions. The average information carried by the
arrival time of one spike is given by [23]

(2)

(3)

Ispike =

dDsP (s|spike) log2

Z

P (s|spike)
P (s)

"

.

#

3

The information per spike as written in (3) is difﬁcult to estimate experimentally, since it
requires either sampling of the high–dimensional probability distribution P (s|spike) or a
model of how spikes were generated, i.e. the knowledge of low–dimensional RS. How-
ever it is possible to calculate Ispike in a model–independent way, if stimuli are presented
multiple times to estimate the probability distribution P (spike|s). Then,

Ispike =

P (spike|s)
P (spike)

*

log2

P (spike|s)
P (spike) #+s

,

"

where the average is taken over all presented stimuli. As discussed in [23], this is useful
in practice because we can replace the ensemble average his with a time average, and
P (spike|s) with the time dependent spike rate r(t). Note that for a ﬁnite dataset of N
trials, the obtained value Ispike(N) will be on average larger than Ispike(∞), with difference
∼ Nstimuli/(Nspike 2 ln 2), where Nstimuli is the number of different stimuli, and Nspike is the
number of elicited spikes [25]. The true value Ispike can also be found by extrapolating
to N → ∞ [23, 26]. Measurement of Ispike in this way provides a model independent
benchmark against which we can compare any description of the neuron’s input–output
relation.

Having in mind a model in which spikes are generated according to projection onto a
low dimensional subspace, we start by projecting all of the presented stimuli on a partic-
ular direction v in the stimulus space, and form probability distributions

where h· · · |spikei denotes an expectation value conditional on the occurrence of a spike.
The information

Pv(x|spike) = hδ(x − s · v)|spikeis,
Pv(x) = hδ(x − s · v)is,

I(v) =

dxPv(x|spike) log2

Z

Pv(x|spike)
Pv(x)

"

#

provides an invariant measure of how much the occurrence of a spike is determined by
projection on the direction v. It is a function only of direction in the stimulus space and
does not change when vector v is multiplied by a constant. This can be seen by noting that
for any probability distribution and any constant c, Pcv(x) = c−1Pv(x/c). When evaluated
along any vector, I(v) ≤ Ispike. The total information Ispike can be recovered along one
particular direction only if v = ˆe1, and the RS is one dimensional.

By analogy with (7), one could also calculate information I(v1, ..., vn) along a set of

several directions {v1, ..., vn} based on the multi-point probability distributions:

Pv1,...,vn({xi}|spike) =

Pv1,...,vn({xi}) =

n

*

i=1
Y
n

*

i=1
Y

δ(xi − s · vi)|spike

,

+s

δ(xi − s · vi)

.

+s

If we are successful in ﬁnding all of the n directions ˆei in the input–output relation
(1), then the information evaluated in this subspace will be equal to the total information

4

(4)

(5)
(6)

(7)

(8)

(9)

Ispike. When we calculate information along a set of n vectors that are slightly off from
the RS, the answer is, of course, smaller than Ispike and is initially quadratic in deviations
δvi. One can therefore hope to ﬁnd the RS by maximizing information with respect to n
vectors simultaneously. The information does not increase if more vectors outside the RS
are included. For uncorrelated stimuli, any vector or a set of vectors that maximizes I(v)
belongs to the RS. On the other hand, the result of optimization with respect to a number
of vectors k < n may deviate from the RS if stimuli are correlated. To ﬁnd the RS, we ﬁrst
maximize I(v), and compare this maximum with Ispike, which is estimated according to
(4). If the difference exceeds that expected from ﬁnite sampling corrections, we increment
the number of directions with respect to which information is simultaneously maximized.
The information I(v) as deﬁned by (7) is a continuous function, whose gradient can

be computed. We ﬁnd

Z

where

∇vI =

dxPv(x) [hs|x, spikei − hs|xi] ·

d
dx

Pv(x|spike)
Pv(x)

,

#

"

hs|x, spikei =

dDs sδ(x − s · v)P (s|spike),

1
P (x|spike) Z

and similarly for hs|xi. Since information does not change with the length of the vector,
v · ∇vI = 0 (which can also be seen from (10) directly).

As an optimization algorithm, we have used a combination of gradient ascent and
simulated annealing algorithms: successive line maximizations were done along the di-
rection of the gradient. During line maximizations, a point with a smaller value of infor-
mation was accepted according to Boltzmann statistics, with probability ∝ exp[(I(vi+1) −
I(vi))/T ]. The effective temperature T is reduced upon completion of each line maxi-
mization.

3 Results

We tested the scheme of looking for the most informative directions on model neurons
that respond to stimuli derived from natural scenes. As stimuli we used patches of black
and white photos digitized to 8 bits, in which no corrections were made for camera’s
light intensity transformation function. Our goal is to demonstrate that even though the
correlations present in natural scenes are non–Gaussian, they can be successfully removed
from the estimate of vectors deﬁning the RS.

3.1 A model simple cell

Our ﬁrst example is based on the properties of simple cells found in the primary visual
cortex. A model phase and orientation sensitive cell has a single relevant direction ˆe1
shown in Fig. 1(a). A given frame s leads to a spike if the projection s1 = s · ˆe1 reaches a
threshold value st in the presence of noise:

(10)

(11)

(12)

P (spike|s)
P (spike)

≡ f (s1) = hθ(s1 − st + ξ)i,

5

10

20

30

10

20

30

1.0

0.5

(a) 

model filter

(b) 

"exact" STA

10

20

30

decorrelated STA

(c) 

10
reconstruction

20

30

(d) 

10

20

30

(e) 

10
20
(f)  P(spike|s  v

) 

max

30

 * I/I

                     

spike

0.5
                                   
 ^ 
o v  e
                 
1

0  

0
10

2
10

4
10
T−1 

0  

s

 
min

 
s
max

10

20

30

10

20

30

  
1.0

6

Figure 1: Analysis of a model simple cell with RF shown in (a). The “exact” spike-
triggered average vsta is shown in (b). Panel (c) shows an attempt to remove correlations
−1
vsta; (d) vector ˆvmax found by maximizing
according to reverse correlation method, C
a priori
information; (e) convergence of the algorithm according to information I(v) and projec-
tion ˆv · ˆe1 as a function of inverse effective temperature T −1. (f) The probability of a spike
P (spike|s · ˆvmax) (crosses) is compared to P (spike|s1) used in generating spikes (solid line).
Parameters σ = 0.05(smax − smin) and st = 0.8(smax − smin) [smax and smin are the maximum
and minimum values of s1 over the ensemble of presented stimuli].

where a Gaussian random variable ξ of variance σ2 models additive noise, and the func-
tion θ(x) = 1 for x > 0, and zero otherwise. Together with the RF ˆe1, the parameters st for
threshold and the noise variance σ2 determine the input–output function.

The spike–triggered average (STA), or reverse correlation function [9, 21], shown in
Fig. 1(b), is broadened because of spatial correlations present in the stimuli. In a model,
the effect of noise on our estimate of the STA can be eliminated by averaging the presented
stimuli weighted with the exact ﬁring rate, as opposed to using a histogram of responses
to estimate P (spike|s) from a ﬁnite set of trials. We have used this “exact” STA,

vsta =

dDs sP (s|spike) =

dDsP (s) sP (spike|s),

(13)

Z

1
P (spike) Z

in calculations presented in Fig. 1(bc). If stimuli were drawn from a Gaussian probability
distribution, they could be decorrelated by multiplying vsta by the inverse of the a priori
covariance matrix, according to the reverse correlation method, ˆvGaussian est ∝ C −1
vsta.
The procedure is not valid for non–Gaussian stimuli and nonlinear input–output func-
tions (1). The result of such a decorrelation is shown in Fig. 1(c). It clearly is missing some
of the structure in the model ﬁlter, with projection ˆe1 · ˆvGaussian est ≈ 0.14. The discrepancy
is not due to neural noise or ﬁnite sampling, since the “exact” STA was decorrelated;
the absence of noise in the exact STA also means that there would be no justiﬁcation for
smoothing the results of the decorrelation. The discrepancy between the true receptive
ﬁeld and the decorrelated STA increases with the strength of nonlinearity in the input–
output function.

a priori

In contrast, it is possible to obtain a good estimate of the relevant direction ˆe1 by max-
imizing information directly, see panel (d). A typical progress of the simulated annealing
algorithm with decreasing temperature T is shown in Fig. 1(e). There we plot both the in-
formation along the vector, and its projection on ˆe1. The ﬁnal value of projection depends
on the size of the data set, see below. In the example shown in Fig. 1 there were ≈ 50, 000
spikes with average probability of spike ≈ 0.05 per frame, and the reconstructed vector
has projection ˆvmax · ˆe1 ≈ 0.9. Having estimated the RF, one can proceed to sample the
nonlinear input-output function. This is done by constructing histograms for P (s · ˆvmax)
and P (s · ˆvmax|spike) of projections onto vector ˆvmax found by maximizing information,
and taking their ratio, as in Eq. (2). In Fig. 1(f) we compare P (spike|s · ˆvmax) (crosses) with
the probability P (spike|s1) used in the model (solid line).

3.2 Estimated deviation from the optimal direction
When information is calculated from a ﬁnite data set, the vector v which maximizes I will
deviate from the true RF ˆe1. The deviation δv = v − ˆe1 arises because the probability dis-
tributions are estimated from experimental histograms and differ from the distributions
found in the limit on inﬁnite data size. For a simple cell, the quality of reconstruction
can be characterized by the projection v · ˆe1 = 1 − 1
2δv2, where both v and ˆe1 are normal-
ized, and δv is by deﬁnition orthogonal to ˆe1. The deviation δv ∼ A−1∇I, where A is the
Hessian of information. Its structure is similar to that of a covariance matrix:

Aij =

dxP (x|spike)

(hsisj|xi − hsi|xihsj|xi).

(14)

1
ln 2 Z

d
dx

 

ln

P (x|spike)

2

P (x) !

When averaged over possible outcomes of N trials, the gradient of information is zero
for the optimal direction. Here in order to evaluate hδv2i = Tr[A−1h∇I∇I T iA−1], we
need to know the variance of the gradient of I. By discretizing both the space of stimuli
and possible projections x, and assuming that the probability of generating a spike is
independent for different bins, we estimate h∇Ii∇Iji ∼ Aij/(Nspike ln 2). Therefore an
expected error in the reconstruction of the optimal ﬁlter is inversely proportional to the
number of spikes and is given by:

1 − v · ˆe1 ≈

hδv2i =

1
2

Tr[A−1]
2Nspike ln 2

7

(15)

1

0.95

 ⋅ v
e
1

 
max

0.9

0.85

0.8

0  

1  

2  

N−1

 
spike

3

  
10−5 

Figure 2: Projection of vector ˆvmax that maximizes information on RF ˆe1 is plotted as a
function of the number of spikes to show the linear scaling in 1/Nspike. In this series of
simulations, the average probability of a spike (12) had parameter values σ = 0.1(smax −
smin) and st = 0.6(smax − smin).

In Fig. 2 we plot the average projection of the normalized reconstructed vector v on the
RF ˆe1, and show that it scales correctly with the number of spikes.

3.3 A model complex cell

A sequence of spikes from a model cell with two relevant directions was simulated by
projecting each of the stimuli on vectors that differ by π/2 in their spatial phase, taken
to mimic properties of complex cells, as in Fig. 3. A particular frame leads to a spike
according to a logical OR, that is if either s1 = s · ˆe1, −s1, s2 = s · ˆe2, or −s2 exceeds a
threshold value st in the presence of noise. Similarly to (12),

P (spike|s)
P (spike)

= f (s1, s2) = hθ(|s1| − st − ξ1) ∨ θ(|s2| − st − ξ2)i ,

(16)

where ξ1 and ξ2 are independent Gaussian variables. The sampling of this input–output
function by our particular set of natural stimuli is shown in Fig. 3(c). Some, especially
large, combinations of values of s1 and s2 are not present in the ensemble. As is well
known, reverse correlation fails in this case because the spike–triggered average stimulus
is zero, although with Gaussian stimuli the spike–triggered covariance method would
recover the relevant dimensions. Here we show that searching for maximally informative
dimensions allows us to recover the relevant subspace even under more natural stimulus
conditions.

We start by maximizing information with respect to one direction. Contrary to the
result Fig. 1(e) for a simple cell, one optimal direction recovers only about 60% of the total
information per spike [Eq. (4)]. Perhaps surprisingly, because of the strong correlations

8

in natural scenes, even projection onto a random vector in the D ∼ 103 dimensional stim-
ulus space has a high probability of explaining 60% of total information per spike. We
therefore go on to maximize information with respect to two directions. An example of
the reconstruction of input–output function of a complex cell is given in Fig. 3. Vectors
v1 and v2 that maximize I(v1, v2) are not orthogonal, and are also rotated with respect to
ˆe1 and ˆe2. However, the quality of reconstruction is independent of a particular choice of
basis with the RS. The appropriate measure of similarity between the two planes is the
dot product of their normals. In the example of Fig. 3, ˆn(ˆe1,ˆe2) · ˆn(v1,v2) ≈ 0.8.

model 
 
e
1

reconstruction 
 
v
1

(a) 

 
10

20

30
(b) 

10

20

30

(c) 

10

20

 
e
2

30

10

20

v

 
2

30

10

20
) 
,s
f(s
2
1

30

10

20
) 
,sv
f(sv
2
1

30

Figure 3: Analysis of a model complex cell with relevant direction ˆe1 and ˆe2 shown in
(a) and (b). Spikes are generated according to an “OR” input-output function f (s1, s2)
with the threshold st = 0.6(smax − smin) and noise variance σ = 0.05(smax − smin). Panel
(c) shows how the input-output function is sampled by our ensemble of stimuli. Dark
pixels for large values of s1 and s2 correspond to cases where P (s1, s2) = 0. On the right,
we show vectors v1 and v2 found by maximizing information I(v1, v2) together with the
corresponding input-output function with respect to projections s · v1 and s · v2, panel (f).

Maximizing information with respect to two directions requires a signiﬁcantly slower
cooling rate, and consequently longer computational times. However, the expected error
in the reconstruction, 1 − ˆn(ˆe1,ˆe2) · ˆn(v1,v2), follows a N −1
spike behavior, similarly to (15), and is
roughly twice that for a simple cell given the same number of spikes.

(d) 

10

20

30

(e) 

10

20

30

(f) 

9

4 Remarks

In conclusion, features of the stimulus that are most relevant for generating the response
of a neuron can be found by maximizing information between the sequence of responses
and the projection of stimuli on trial vectors within the stimulus space. Calculated in this
manner, information becomes a function of direction in a stimulus space. Those direc-
tions that maximize the information and account for the total information per response
of interest span the relevant subspace. This analysis allows the reconstruction of the rele-
vant subspace without assuming a particular form of the input–output function. It can be
strongly nonlinear within the relevant subspace, and is to be estimated from experimen-
tal histograms. Most importantly, this method can be used with any stimulus ensemble,
even those that are strongly non–Gaussian as in the case of natural images.

Acknowledgments

We thank K. D. Miller for many helpful discussions. Work at UCSF was supported in
part by the Sloan and Swartz Foundations and by a training grant from the NIH. Our
collaboration began at the Marine Biological Laboratory in a course supported by grants
from NIMH and the Howard Hughes Medical Institute.

References

[1] F. Rieke, D. A. Bodnar, and W. Bialek. Naturalistic stimuli increase the rate and
efﬁciency of information transmission by primary auditory afferents. Proc. R. Soc.
Lond. B 262:259–265, 1995.

[2] F. E. Theunissen, K. Sen, and A. J. Doupe. Spectral-temporal receptive ﬁelds of non-
J. Neurosci. 20:2315–2331,

linear auditory neurons obtained using natural sounds.
2000.

[3] W. E. Vinje and J. L. Gallant. Sparse coding and decorrelation in primary visual

cortex during natural vision. Science 287:1273–1276, 2000.

[4] G. D. Lewen, W. Bialek, and R. R. de Ruyter van Steveninck. Neural coding of nat-
uralistic motion stimuli. Network: Comput. Neural Syst. 12:317–329, 2001. See also
physics/0103088.

[5] K. Sen, F. E. Theunissen, and A. J. Doupe. Feature analysis of natural sounds in the

songbird auditory forebrain. J. Neurophysiol. 86:1445–1458, 2001.

[6] N. J. Vickers, T. A. Christensen, T. Baker, and J. G. Hildebrand. Odour-plume dy-

namics inﬂuence the brain’s olfactory code. Nature 410:466–470, 2001.

[7] W. E. Vinje and J. L. Gallant. Natural stimulation of the nonclassical receptive ﬁeld
increases information transmission efﬁciency in V1. J. Neurosci. 22:2904–2915, 2002.

10

[8] D. L. Ringach, M. J. Hawken, and R. Shapley. Receptive ﬁeld structure of neurons in
monkey visual cortex revealed by stimulation with natural image sequences. Journal
of Vision 2:12–24, 2002.

[9] F. Rieke, D. Warland, R. R. de Ruyter van Steveninck, and W. Bialek. Spikes: Exploring

the neural code. MIT Press, Cambridge, 1997.

[10] E. Simoncelli and B. A. Olshausen. Natural image statistics and neural representa-

tion. Annu. Rev. Neurosci. 24:1193-1216, 2001.

[11] H. B. Barlow. Possible principles underlying the transformation of sensory messages.

In Sensory Communication, W. Rosenblith, ed., pp. 217–234 (MIT Press, Cambridge).

[12] H. B. Barlow. Redundancy reduction revisited. Network: Comput. Neural Syst. 12:241-

253, 2001.

[13] W. Bialek. Thinking about the brain. To be published in Physics of Biomolecules and
Cells, H. Flyvbjerg, F. J ¨ulicher, P. Ormos, and F. David, eds. (EDP Sciences, Les Ulis;
Springer-Verlag, Berlin 2002). See also physics/0205030.

[14] T. von der Twer and D. I. A. Macleod. Optimal nonlinear codes for the perception of

natural colours. Network: Comput. Neural Syst. 12:395-407, 2001.

[15] N. Brenner, W. Bialek, and R. de Ruyter van Steveninck. Adaptive rescaling opti-

mizes information transmission, Neuron 26:695–702, 2000.

[16] A.L. Fairhall, G. D. Lewen, W. Bialek, and R. R. de Ruyter van Steveninck, Efﬁciency

and ambiguity in an adaptive neural code. Nature 412:787–792, 2001.

[17] Z. F. Mainen and T. J. Sejnowski. Reliability of spike timing in neocortical neurons.

Science 268:1503–1506, 1995.

[18] R. R. de Ruyter van Steveninck, G. D. Lewen, S. P. Strong, R. Koberle, and W. Bialek.

Reproducibility and variability in neural spike trains. Science 275:1805–1808, 1997.

[19] P. Kara, P. Reinagel, and R. C. Reid. Low response variability in simultaneously

recorded retinal, thalamic, and cortical neurons. Neuron 27:635–646, 2000.

[20] R. de Ruyter van Steveninck, A. Borst, and W. Bialek. Real time encoding of motion:
Answerable questions and questionable answers from the ﬂy’s visual system. In Pro-
cessing Visual Motion in the Real World: A Survey of Computational, Neural and Ecological
Constraints, J. M. Zanker and J. Zeil, eds., pp. 279–306 (Springer–Verlag, Berlin, 2001).
See also physics/0004060.

[21] E. de Boer and P. Kuyper. Triggered correlation. IEEE Trans. Biomed. Eng. 15:169–179,

1968.

[22] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a
movement-sensitive neuron in the blowﬂy visual system: coding and information
transfer in short spike sequences. Proc. R. Soc. Lond. B 234:379–414, 1988.

11

[23] N. Brenner, S. P. Strong, R. Koberle, W Bialek, and R. R. de Ruyter van Steveninck.
Synergy in a neural code. Neural Comp. 12:1531-1552, 2000. See also physics/9902067.

[24] W. Bialek and R. R. de Ruyter van Steveninck. Features and dimensions: Motion

estimation in ﬂy vision. In preparation.

[25] A. Treves and S. Panzeri. The upward bias in measures of information derived from

limited data samples. Neural Comp. 7:399-407, 1995.

[26] S. P. Strong, R. Koberle, R. R. de Ruyter van Steveninck, and W. Bialek. Entropy
and information in neural spike trains. Phys. Rev. Lett. 80:197–200, 1998. See also
cond-mat/9603127.

12


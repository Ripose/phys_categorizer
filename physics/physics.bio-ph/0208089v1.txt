Dynamical Monte Carlo method for stochastic epidemic models

O.E. Ai´ello and M.A.A. da Silva

Departamento de F´ısica e Qu´ımica,

FCFRP,Universidade de S˜ao Paulo,

14040-901 Ribeir˜ao Preto,SP, - Brasil.

Published textdate)

Abstract

(Dated: August 19, 2002; Received textdate; Revised textdate; Accepted textdate;

A new approach to Dynamical Monte Carlo Methods is introduced to simulate markovian pro-

cesses. We apply this approach to formulate and study an epidemic Generalized SIRS model.

The results are in excellent agreement with the forth order Runge-Kutta method in a region of

deterministic solution. Introducing local stochastic interactions, the Runge-Kutta method is not

applicable, and we solve and check it self-consistently with a stochastic version of the Euler Method.

The results are also analyzed under the herd-immunity concept.

2
0
0
2
 
g
u
A
 
6
2
 
 
]
h
p
-
o
i
b
.
s
c
i
s
y
h
p
[
 
 
1
v
9
8
0
8
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

I.

INTRODUCTION

Epidemic systems have been systematically and mathematically formulated in a

continuous-deterministic approach, taking immediate advantages of many numerical meth-

ods and techniques developed for system of diﬀerential equations. The stochastic framework,

although more realistic in principle, is more complex for analyses because of the level of detail

required and therefore, perhaps, has been traditionally less preferable than the deterministic

ones [1, 2, 3]. However, improved machine technology has spread the use of computationally

intensive methods to solve a great diversity of epidemic models, and so simulation techniques,

as Monte Carlo (MC) [4, 5], are becoming more popular in this matter.

In some MC studies of time-dependent phenomena, results are reported in terms of inte-

gral Monte Carlo steps (MCS) [6], which obfuscate a deﬁnitive role of the time. Ambiguities

surrounding the relationship between MC time and real time preclude rigorous comparison

of simulated results to theory and experiment. Within the past few years, the idea that

MC methods can be utilized to simulate dynamical process has been advanced in many

publications [7, 8, 9, 10].

The aim of the present work is to present a dynamical Monte Carlo method for simulating

markovian processes. Another purpose is to incorporate explicit spatial components into

epidemic model and to analyze the dynamics of the spread of infections with this method.

The method is applied to the compartmental Susceptible-Infected-Recovered (SIR)

model and a variant SIRS obtained by the inclusion of a reﬂux of susceptible to the system,

that is, once recovered the individual turn again to the class of susceptibles. Mean ﬁeld and

local interactions will be considered. For the cases described by mean ﬁeld, the solutions ob-

tained by dynamical Monte Carlo are compared with the results obtained by Runge-Kutta

method.

In cases that Runge-Kutta method is not applicable, the MC space-dependent

results are checked self-consistently using a stochastic version of the Euler method [11] and

analyzed under the herd-immunity concept [12].

The work is subdivided in the following way: in the next section, we describe the method.

In the following, we delineate the corresponding Monte Carlo procedure. Then, we describe

the model and ﬁnally we apply the methodology for the solution of the SIRS model.

2

II. THE METHOD

During the temporal evolution of a system deﬁned by stochastic variables, transitions

will happen. Through these transitions, the probability to ﬁnd the system in a given state

is modiﬁed until that the stationary equilibrium state is reached, in which the transitions

cannot cause changes in the distribution of probabilities.

The description of the evolution of the distribution of probabilities for markovian pro-

cesses is made by means of the Master Equation:

dPi(t)
dt

=

wj→iPj −

wi→jPi,

j
X

j
X

(1)

in that Pi(t) is the probability to ﬁnd the system in the state i in the time t, and wi→j is

the transition probability per unit of time. The ﬁrst term on the right side describes the

rate of all transitions arriving in the considered state (increasing its probability), and the

second term describes the rate of all transitions leaving the considered state (decreasing its

probability).

Considering Tij as the probability of transition of the state i to j, we can write wij = Tij
τ i
[13], where τ i is the characteristic time constant (lifetime) of the state i. The probability of

transition Tij determines the sequence of events, and reciprocally it depends on the event that

cause the transition; so wi→j is also event dependent. Thus, the pathway of the evolution of

the system depends on the sequence of events: when a given state i is met, the next state

j will be reached through an event whose probability is implicitly in Tij. The probabilities,

i Pi(t) = 1 and

Pi(t) and Tij, obey the normalization conditions

j Tij = 1.
We note that all states equally likely occur only at equilibrium, because

j wj→i =
j wi→j for every i and time. In fact, if we suppose that P = Pi(t) independent of i, the
j wj→i −
j wi→j. If we sum both sides of this
j wi→j,
dt = 0, what implies that
therefore P = constant. Suppose now that a state i is reached through an event e with

equation (1) may be rewritten: dlnP
P
expression over all possible states we get dlnP

j wj→i =

dt =

P

P

P

P

P

P

P

a probability a priori governed by (1). To guarantee the stated equilibrium property, we

assume that these probabilities are all equal, independent of the event type. In this way,

when we have the possibility of more than one event, we choose one of them with equal

probability, establishing part of the hierarchy of the process.

We now start by choosing a convenient physical extensive microscopic quantity Ai what

3

is time independent for each state i. The mean value for this quantity at the time t is given

by:

This equation represents the macroscopic physical quantity A(t). Diﬀerentiating both sides

of the equation above, with respect to t, we get

A(t) =

=

A
i

h

Pi(t)Ai.

i
X

dA(t)
dt

=

dPi(t)
dt

Ai,

i
X

dA(t)
dt

=

i
X

j
X

wj→iPj∆Aij.

dA(t)
dt

=

X(ij)

wj→iPjaδij,

and by substituting (1) in (3) follows:

dA(t)
dt

=

wj→iPjAi −

i
X

j
X

i
X

j
X

wi→jPiAi.

Deﬁning ∆Aij = Ai −

Aj , and as i and j sweep all possible states, we may rewrite (4) as

Let us consider now only the next-neighbor states j of the state i. If we measure “dis-

tances” among the states, let us say for the amount

∆Aij|
|
= a, we can write the equation (4) as

value is

∆Aij|

|

, such that the non-null minimum

∆Aij|
in which one the symbol (ij) denotes a pair of ﬁrst neighbor states, and δij = ∆Aij/
.
|
Now consider other physical quantity A† that is the source for the quantity A. Thus, we

can rewrite (5) as:

dA(t)
dt

=

j PjA†
r+

j −

r−
j PjAj,

j
X

j
X

where the rate rj =

wj→iii is resulting from the average, of the transition probabilities
per unit of time, over the ensemble of ﬁrst neighbor states i of j in the time t, i.e., the

h

mesoscopic rates. Here, the ensemble means a group of accessible conﬁgurations in a small

time around the time t. In this sense we are using the time dependent ergodicity idea [14],

and generally the systems are non ergodic in non equilibrium states. The superscripts “+”

4

(2)

(3)

(4)

(5)

(6)

(7)

and “

” mean the contributions respectively to increase and to decrease the quantity A(t).
j = r− are constant (or only function of the time)

j = r+ and r−

In the particular cases that r+

−

we have:

dA
dt

= r+A†

r−A,

−

which is similar to the kinetic equation for chemical reactions of ﬁrst order

and A the respective concentrations of the chemical elements

† ⇄

, being A†

A

A
[15]. The equilibrium

† and

A

A

can be obtained imposing the balance at macroscopic level (or mesoscopic) r+A† = r−A.

This follows immediately from the detailed balance, but it is not necessary at all.

We can write the equation (6) in the integral form

A(t)

A(t0) =

wj→iPj(t)aδijdt.

t

(ij)
P
Discretizing the equation (9), we may write:

Z

t0

−

A(t)

A(t0)

−

≃

wj→iPj(tk)aδij∆tk.

n

Xk=0 X(ij)

sented by

Let now be the group of possible probabilities of transition per unit of time wj→i repre-
Pt.
The phase space can be divided into N parts, in such way that each part can contain only

, being the states i and j occurring around an instant t, wmax

t = sup

wj→i}

Pt =

{

one element of the system. Thus, each element of the time in the equation (10) can be

represented for

Starting from some initial condition, we can do the following iterative process:

∆tk =

1
wmax
tk N

.

A(tk+1) = A(tk) +

wj→iPj(tk)aδij∆tk.

X(ij)
At each Monte Carlo step k a time interval ∆tk is calculated using (11). We denote 1 MCS

here as a single trial to change the state of the system. The probabilities per unit of time

wj→i are drawn randomly using the hierarchy described in the next section of this work.

The procedure is repeated, in a suﬃcient number, to get a sample of the distribution Pj(tk),

and so the expression (10) is estimated.

5

(8)

(9)

(10)

(11)

(12)

III. MONTE CARLO SIMULATION

In a dynamical interpretation, the MC method provides a numerical solution to the

Master Equation. In order to the MC procedure do this, during the simulation, a sequence of

events is generated separate by time intervals dictated by the transition probabilities. These

times are accomplished on a scale at which no two events occur simultaneously. The task

of the MC algorithm is to create a chronological sequence of the distinct events separated

by certain interevent times. When the equilibrium is reached, the physical quantities of the

system in the macroscopic scale no more evolve, and we may interpret this as the “time

stopped” at this scale.

We can consider n = lN, with l sweepings on the discretized space in the equation (10);

in the limit of N

we have the exact solution of the equation (6) for a given initial

→ ∞

condition. With this consideration, and the expression (11), the equation (10) can be written

in the form:

A(t)

A(t0) =

−

ℓN

wj→i
wmax

1
N

Xk=0 X(ij) (cid:18)

tk (cid:19) (cid:18)

(cid:19)

Pj(tk)aδij.

(13)

We can create a hierarchical process choosing the transition probabilities as:

T ∗
j→i =

wj→i
wmax
tk

,

(14)

which reproduces the correct frequencies of events in every time tk to solve (13).

To execute the MC procedure, an element is selected randomly with probability 1/N,

and thus a transition is tried with probability given by (14). The space is swept l times, with

the increment of time in each MCS given by (11). Beginning with the same initial condition

for the physical quantities, the process can be repeated and we take average quantity A(t) in

each instant t. We should emphasize that the probabilities Pj are generated by this process.

As a given state is chosen with its correct probability in a given time, an ideal MC procedure

leads to

A(t)

A(t0) =

r+A†

r−A

jk −
(cid:11)
where the average is taken on the ensemble of the states jk in each instant tk. This is an

Xk=0 (cid:16)(cid:10)

(cid:17) (cid:18)

−

(cid:19)

(cid:11)

(cid:10)

jk

(15)

1
wmax
tk N

,

ℓN

approach for the integration of the equation (5).

6

Observing some points is necessary: ﬁrst, generally diﬀerent runs give diﬀerent time

results tk at the same MCS k, and the sample average can be done with linear interpolation

or extrapolation of the group of data, in each MC realization. These values interpolated or

extrapolated may be interpreted as resulting from some virtual state. Second, in a complete

sweep around a time tk, the value wmax

tk

should be approximately constant in order not

changing the hierarchy and with that the result. Third, as the conﬁgurations do not vary

drastically in few steps, the microscopic transitions reproduce the mesoscopic results.

Another approach consists in estimating the interevent times with the following rule

∆te

k =

,

f k
e a
j Ak
rk
j
jk and Ae

(16)

(17)

where re

jk = r+

jk = r−
result of the experiment increases or it decreases the quantity A. The quantity f k

jk = Ajk depending, respectively, if the
e is a factor

jk and Ae

jk, or re

jk = A†

dependent of the e-event and it obeys the relationship

e = 1, for each time tk. We
emphasized that the time given by (16) represents the mean waiting time for transitions

e f k

P

from a given state jk to any neighbor state i; if the microscopic state stays unchanged, the

time does not develop. It can be shown that this procedure leads to the same result that

(11) in each MCS observing that

∆tk =

i  

e
X

X

wk
j→i
wmax
tk ! (cid:18)

1
N

(cid:19)

∆te
k.

P
e = s, we have f k

As re

jkAe

jk = a

i wjk→i, using the equation (16) and the normalization conditions for f k

(17), we obtain the expression (11). In particular, if we choose f k

e in
e = 0, for every event, except

s = 1. With this condition, the time between events has the meaning of the

the relative frequencies of occurrence of events are all equal, we may deﬁne f k

waiting time between type-s events. Based on this and in the fact that at the equilibrium
Nk,
e is the total number of events, in

Nk =
a time interval (arbitrary) near to some time tk. The frequency ratios imply that we are

e is the number of e

events, and

where nk

e ≡

e nk

nk
e /

−

P

considering a kind of average to estimate the interevent times. Since we have the rates, or

the probabilities of transition per unit of time, that deﬁne the time intervals between events

in some scale, we may construct a MC algorithm to solve the Master Equation. Thus, we

obtain the time evolution of physical quantities of the system.

Formally, to generate the distribution Pi(t), we must consider independent pathways by

generating several independent markov chains. Thus, to calculating A(t) at the time t0 we

7

can, also, use directly the equation (2). To do this, suppose that we have local equilibrium,

so we may measure the properties of the system. If we chose a given state i of the system

with probability P ∗

i (t0), we may rewrite the equation (2) by [5]

If we make P ∗

i (t0) = Pi(t0) at equilibrium, we obtain

P

< A(t0) >=

i Pi(t0)Ai/P ∗
i Pi(t0)/P ∗

i (t0)
i (t0)

.

P

< A(t0) >=

L∗
i=1 Ai
L∗

,

P

(18)

(19)

where L∗ is the number of all possible states of the system at the time t0. The state prob-

abilities at the time t0 are obtained by constructing an ensemble of conﬁgurations using

appropriated transition rates. This may be extended to any time t. The states labeled by i

may be considered as virtual states corresponding to possible data interpolation or extrapo-

lation. The practical procedure is the following: at a given experiment (in the construction

of a trajectory), labeled by m, we may get the measurements of any appropriated physical

quantity Am(t) obtained by linear extrapolation or interpolation using two consecutive data

points. After perform L experiments (realizations), at some time t, the mean value of A is

A(t)

≈

L
m=1

procedure making L
P

Am
L , L being the total number of experiments. Note that if we idealize this
, we obtain the complete ensemble that give-us the correct mean

→ ∞

values of the physical quantities (transition probabilities, time transition, etc.) for each time

t. Ensuring that diﬀerent experiments are independent, the error for the quantities involved

in the process for each time t may be calculated by the expression [5]:

σA
√L

=

r

< A2 >

< A >2

,

−
L

(20)

where A is some quantity, like the number of infected individuals.

IV. EPIDEMIC MODELS

The conventional treatment of epidemic systems is formulated starting from a group

of compartments that represent each of the possible states of its elements, with rates of

transfers among the several pairs of compartments. Mathematically this subject turns into

systems of diﬀerential equations.

8

Considering a generic system (population, epidemic agents, etc.) and its space distribu-

tions, any epidemic is characterized by a temporal and space evolution, and in each region,

the density of the elements can vary with the time. With this optics we considered such

phenomenon as a stochastic process in which one random variable is the time. This focus

seeks to propitiate the incorporation of more details in the study of the epidemic process

and with that to allow the analysis of more complex models and therefore more realists.

The SIRS model considers a population with N individuals divided in three classes about

certain transmissible disease: S (susceptible individuals), I (infected) and R (recovered).

The evolution of the disease gives him according to the outline S(t)

I(t)

R(t)

S(t).

→

→

→

Based on the equations (7), the SIRS model is formalized in a quite generic way through

the following group of equations:

dS
dt

dI
dt

dR
dt

=

=

=

j
X

j
X

j
X

rRS
j PjRj −

rSI
j PjSj,

rSI
j PjSj −

rIR
j PjIj,

rIR
j PjIj −

rRS
j PjRj,

j
X

j
X

j
X

where S, I and R are respectively the number of susceptible, infected and recovered individ-

uals. The mesoscopic rates are rSI
j

, rIR
j

and rRS

j

, for each state j, respectively, from S

, I

R and R

S . If the rates are constant (or only function of the time) the solution

→

→

of the system depends only on the initial values of the parameters.

We did the following restrictions:

1) the increase rate of those infected is directly proportional to the number of infected and

some power µ of susceptible

j rSI
ones decrease in the same proportion;

(cid:16)P

(cid:17)

j PjSj = bSµI/N µ

; and by consequence the susceptible

j PjIj = aI

2) the removal rate of those infected is directly proportional to the number of infected
j rIR
3) the rate that the recovered go to the susceptible is directly proportional to the recovered
j rRS

; and consequently the recovered increase in the same proportion;

. These restrictions took to the system ((21)) supply:

j PjRj = mR

(cid:17)

(cid:16)P

(cid:16)P

(cid:17)

dS
dt

= mR

bSµI
N µ

−

9

(21)

(22)

(23)

I

→

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

in which µ is denominated safety number in power [16]. The system equations above repre-

sent the formulation of the continuous-deterministic SIRS model.

The steady-state solutions are determined by the conditions dS/dt = 0, dI/dt = 0,

dR/dt = 0; the nontrivial solution occurs for ﬁnite values of Sσ, Iσ, and Rσ, viz.,

Depending on the removal rate a of the infectives, infection parameter b, and renewal m,

there exist stable solutions around the steady state that correspond to recurrent epidemics,

or damped (fading) recurrent waves. These variant supplies oscillatory solutions that vanish

with the time, reaching a stationary state in that the numbers of elements of each class

stay constant. This model is a generalization of the classical SIR system [17, 18], which

is readily recovered from (24) - (26) by setting µ = 1 , m = 0 and are represented by the

diﬀerential equations [19]:

The SIR class of compartmental models has several deterministic and stochastic versions,

as the SIS and the SEIR model [18, 20, 21]. When spatial variables are not included, they

are often considered as deterministic mean ﬁeld models, based on the chemical “mass action”

principle.

In this work, we also considered epidemic processes as resultant of the action of a mean

ﬁeld and of the interaction among the closest individuals (local interaction). With this

dI
dt
dR
dt

=

bSµI
N µ −

aI

= aI

mR,

−

Sσ = (a/b)1/µN,
1

(a/b)1/µ

Iσ =

−
1 + a/m

Rσ =

1

(a/b)1/µ

−
1 + m/a

N,

N.

dS
dt
dI
dt
dR
dt

=

bSI

−

= bSI

aI

−

= aI

10

end, it is increased to the model SIRS a term that promotes the infection through the
contact among infected individuals and susceptibles. So, wR→S = m, wS→I = Γ b
N µ Sµ−1I +
p0)n], and wI→R = a are the eﬀective transition rates, respectively, of a site

Λ [1

(1

−

−

(individual) recovered became susceptible, a susceptible became infected and an infected

became recovered. The parameters Γ and Λ balance, respectively, the global (mean ﬁeld)

and the local (nearest neighbors) variables; the relation Γ+Λ = 1 is satisﬁed. The parameter

Therefore, (1

p0 is the probability for a susceptible to become infected due to a just one-infected neighbor.
p0)n is the probability for a susceptible not to be infected if it has n infected
p0)n is the probability for a susceptible to be infected if it has

neighbors, thus 1

(1

−

n infected neighbors. The standard infection rate b, recovery rate a, exponent µ and the

renewal rate m are positive

(1) parameters, and S(t) + I(t) + R(t) = N, with dN/dt = 0.

−

−

O

When the renewal parameter is non zero (m

= 0), appears a continuous inﬂux of susceptible

into the system, producing oscillations in the number of elements of the populational class

=

S, I, R

. Therefore, fading recurrent epidemics may occur before the steady (endemic)

C
state is reached. The power µ introduces a modiﬁcation in the original SIR model that take

{

}

in account nonhomogeneous mixing of susceptible and infective.

When only the mean ﬁeld interaction is considered, the Runge-Kutta method is enough

for resolution of the SIRS model. However, besides these cases, the dynamical Monte Carlo

method also supplies the stochastic dynamic for the systems with local interaction.

V. APPLICATION OF THE MCD TO THE SIRS MODEL

In this work we consider a square lattice of N = M

M sites with M = 200. The

×

initial condition for the system is set up by I0 infective being randomly distributed on the

lattice (N >> I0) and the remaining sites being occupied by S0 = N

I0 susceptible;

−

therefore, R0 = 0. The simulation develops systematically by choosing one site of the lattice

at a random at a time. Depending on its status (susceptible, infected or recovered), the

transition to another status is tried through a set of transition probabilities, obeying the

cyclic route S

I

R

S, with the populational class

=

S, I, R

being updated

→

→

→

C

{

}

properly. If the transition is successful, the system is now in a new state, and so a time

delay is assigned to this transition. This process is repeated until the steady state is reached.

In order to construct a hierarchical process, we set for a particular event α (S

I,

→

11

6
I

→

→

R, or R

S, in our case) the probability of transition T ∗

k,α as follows [8]:

T ∗
k,α = wα/wmax,

(33)

where wα ∈ P
wmax = sup
P

=

wS→I, wI→R, wR→S}
, that is, the largest probability in

{

P

is the transition probability for the event α and

. Thus, each particular trial will be gauged

according to a balance of rates, producing a hierarchical sequence of events. Operationally,

R1 > T ∗

T ∗
k,α is compared against a random number 0
When
incremental random time ∆tα is calculated from (16) as follows: ∆tα = ∆I+
= ∆I−
and S† = R. The frequencies fI+, fI, and fS+ are numbers of events that respectively increase
P

k,α, the new state is rejected; otherwise the new state is accepted and an
wSn→I I † , or, ∆tα
, I † = S,

1, taken from a uniform distribution.

wI→RI , or, ∆tα = ∆S+

wR→SS† , where ∆I+ =

≤ R1 ≤

, ∆S+ =

, ∆I− =

fS+

fI+

fI+

e fe

e fe

e fe

S+

I−

P

P

P

P

P

I+

I, decrease I and increase S at the time t. The sum

e fe is the total number of events.

The ﬁgures, 1 and 2, show the temporal evolution of S(t), I(t) and R(t) for SIR

P

and SIRS models respectively. Continuous lines represent numerical (fourth-order Runge-

Kutta) checking solutions, and open circles correspond to the Dynamical MC simulation.

Accuracies of the numerical solutions were checked using the steady state exact solution and

the errors were estimated as less than 0.1%. The results with respect to the MC simulation

shown in these ﬁgures correspond to an average of 20 independent trajectories, a number

suﬃcient to produce soft curves and illustrate the agreement with the checking solutions.

We introduce now the local term with the weight Λ, and the variable n as an integer

in the interval from n = 0 up to 8, since ﬁrst and second nearest infected neighbors are

indistinguishably considered for each susceptible. From a computational point of view, the

main consequence of introducing space-dependent variables is that the Runge-Kutta method

cannot be any more applied in the resulting model.

To see the self-consistency of the approach we integrate numerically (21

23) given con-

−

stant (or piecewise constant) time step as in (10) by choosing the maximum local transition

probability per unit of time. This maximum is in fact actualized at each MC step. The

quantities S, I and R are calculated with iterations; the rates are chosen randomly by the

MC procedure. This procedure can be considered as a stochastic version of the Euler method

and it has been used to check the MC solutions, showing excellent agreement[11]; here we

do not show these results because they are very close, less then 1% of diﬀerences.

12

The time evolution (number of infective) shown in Figure 3 correspond to Λ = 0.1, 0.5

and 0.9, and p0 = 0.1. Note that as Λ increases the epidemic severity is reduced. This

indicates that epidemic outbreak mechanisms involving only local contacts are less eﬃcient

than those whose propagation are due to a wider-range mechanisms. Note also, that for

larger Λ the second peak of the curve is signiﬁcantly displaced to the right. This eﬀect may

be explained by the establishment of a protecting shield (herd immunity eﬀect).

Depending on local contact (1

(1

p0)n the size of the removal class interferes essen-

−

−

tially in the infection mechanism because the infective character of the neighborhood of one

susceptible is determined by the number of infectives of the neighborhood (shield eﬀect).

Figure 4 illustrates graphically the shield eﬀect.

VI. CONCLUSIONS

In this work we examined and applied the dynamical Monte Carlo method to the epidemic

SIRS model . We showed that, once established the hierarchy and the relationship between

Monte Carlo step and real time, the dynamic aspects of the system, including properties out

of the equilibrium, can be simulated. With that, the power and the generality of the Monte

Carlo simulation can be used to obtain the temporal evolution of deterministic or stochastic

systems.

We emphasize that low correlations between events are not required as did in the reference

[22]. It is necessary that the results for independent runs be uncorrelated, so we can use

the averages obtained for each time t to represent properly the physical quantities of the

process. To do this we need a local equilibrium hypothesis, what may be at ﬁrst glance

restrictive, however we may even reduce the time observation suﬃciently, say order of the

lifetime τ i, such that the system does not have time to leave some metastable states. So, we

can average it there. In the practice of the simulation this is done by increasing the number

of observations, i.e., the number of time experiments.

The system studied here is suﬃciently general to illustrate several aspects of the real-time

evolution, determined by dynamical Monte Carlo simulation.

The authors gratefully acknowledges funding support from FAPESP Grant n. 00/11635-7

and 97/03575-0. The authors would also like to thank Dr. A. Caliri for many stimulating

13

discussions and suggestions.

14

Figure Captions

FIG. 1. SIR model. The ﬁgure show, how the number of susceptible S, infective I and

recovered R evolve with time t. The numerical value for the model parameters are a = 0.2,

b = 0.8. There is a good agreement between the MC results (open circles) and solutions

provided by Runge-Kutta method (line)

FIG. 2. SIRS model. The ﬁgure show the time evolution of the S, I, R. The parameter

values are a = 0.2, b = 0.8, m = 0.01 and µ = 2. The error between the MC results (open

circles) and Runge-Kutta calculations are less than 0.1%.

FIG. 3. In this ﬁgure, it is shown the eﬀect of spatial variables for SIRS model: the

parameter Λ balances the local and global variables (Λ + Γ = 1); the herd immunity eﬀect

increases with Λ and it is responsible for the displacement of the curve second peak to the

right.

FIG. 4. The shield eﬀect: a snapshot of the system evolution at t

40 , when S = 19400

≃

(yellow surface), R = 19200 (black) and I = 1400 (red spots) for (Λ = 0.9).

15

[1] D. Mollison, editor, Epidemic Models: Their Structure and Relation to Data, Cambridge Univ

Press (1995).

[2] J. D. Murray, Mathematical Biology, Spriger-Verlag, New York, Berlin (1989).

[3] S. Blount and S. Yakowitz, Math. and Comp. Mod. 32 139 (2000).

[4] N. Metropolis, A. W. Rosenbluth, A. H. Teller, and E. Teller, J. Chem. Phys. 21, 1087 (1953).

[5] K. Binder, Monte Carlo Method in Statistical Physics (Spriger-Verlag, Berlin, 1986).

[6] V.J. Haas, A. Caliri, and M.A.A. da Silva, J. of Biol. Phys., 25, 309 (1999).

[7] D. T. Gillespie, J. Comp. Phys. 22, 403 (1976).

[8] K. A. Fichtorn and W. H. Weinberg, J. Chem. Phys. 95, 1090 (1991).

[9] Pei-Lin Cao, Phys. Rev. Lett. 73, 2595 (1994).

[10] A. Prados, J.J. Brey, and B. S´anchez-Rey, Journal of Statistical Physics 89, 709 (1997).

[11] O.E. Aiello and M. A. A. Silva, http://xxx.lanl.gov/abs/physics/0205039.

[12] R. W. Thomas (Ed.), Spatial Epidemiology, Pion, London, 1990.

[13] P.G. Hoel, S.C. Port, and C.J. Stone, Introduction to Stochastic Processes (Waveland Press,

Inc., Prospect Heights, Illinois, 1987).

[14] K. Binder, Rep. Prog. Phys. 60, 487 (1997).

[15] At larger scale, averages of the wi→j give rates to the events; for example we can obtain the
Tij
τ , where τ is the characteristic time of the
τ i

overall transition rate by: w =

= 1

Pi

j

i

[16] N.T.J. Bailey, The Mathematical Theory of Infectious Diseases and its Applications, Charles

system.

P

P

Griﬃn & Company LTD, 1975.

[17] W.O. Kermack, A. G. McKendrick, Proc. Roy. Soc. A 115, 700 (1927); 138, 55 (1932); 141,

94 (1933).

[18] G.H. Weiss and M. Dishon, Math. Biosci. 11, 261 (1971).

[19] R. M. Anderson & R. M. May, Nature 280, 361 (1979).

[20] C.J Rhodes and R.M. Anderson, Philos. Trans. Roy. Soc. B 351,1679 (1996).

[21] A. Johansen, J. Theo. Biol. 178, 45 (1996).

[22] O.E. Aiello, V.J. Haas, A. Caliri, and M. A. A. Silva, Physica A. 282, 546 (2000).

16

FIGURE 1

R(t)

 Runge-Kutta
 MC

3
-
0
1
x
)
t
(
I

40

30

20

10

0

S(t)

I(t)

0

10

20

30

40

50

Time (days)

3
-
0
1
x
)
t
(
S

3
-
0
1
x
)
t
(

R

3
-
0
1
x
)
t
(
I

40

35

30

25

20

15

25

20

15

10

10

5

0

8

6

4

2

0

FIGURE 2

 Runge-Kutta
 MC

 Runge-Kutta
 MC

 Runge-Kutta
 MC

0

100

200

300

400

500

600

Time (days)

FIGURE 3

            Λ
  0.1
  0.5
  0.9

3
-
0
1
x
)
t
(
I

12

8

4

0

0

100

200

300

400

500

600

Time (days)

FIGURE 4

 S
 I 
 R 


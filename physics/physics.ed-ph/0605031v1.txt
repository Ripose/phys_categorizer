Andrea Baronchelli†, Emanuele Caglioti‡ and Vittorio Loreto†
† Physics Dept. and INFM-SMC La Sapienza University, P.le A. Moro 2, 00185
Rome, ITALY
‡ Mathematics Dept. La Sapienza University, P.le A. Moro 2, 00185 Rome, ITALY

Abstract. Physics concepts have often been borrowed and independently developed
by other ﬁelds of science. In this perspective a signiﬁcant example is that of entropy
in Information Theory. The aim of this paper is to provide a short and pedagogical
introduction to the use of data compression techniques for the estimate of entropy and
other relevant quantities in Information Theory and Algorithmic Information Theory.
We consider in particular the LZ77 algorithm as case study and discuss how a zipper
can be used for information extraction.

6 Measuring complexity with zippers
0
0
2
 
y
a
M
 
3
 
 
]
h
p
-
d
e
.
s
c
i
s
y
h
p
[
 
 
1
v
1
3
0
5
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1. Introduction

Strings of symbols are nowadays widespread in all the ﬁelds of science. On the one hand
many systems are intrinsically described by sequences of characters: DNA, written texts,
bits in the transmission of digital data, magnetic domains in storage data devices, etc.
On the other hand a string of characters is often the only possible description of a
natural phenomenon. In many experiments, for example, one is interested in recording
the variation in time of a given physical observable (for instance the temperature of a
system), thus obtaining a time series, which, suitably codiﬁed, results in a sequence of
symbols.

Given a string of symbols the main problem is quantifying and then extracting
the information it contains. This acquires diﬀerent meanings in diﬀerent contexts. For
a DNA string, for instance, one could be interested in separating portions coding for
proteins from not coding parts. Diﬀerently in a written text important information are
the language in which it is written, its author, the subject treated etc.

Information Theory (IT) is the branch of science which deals, among other things,
with the problems we have mentioned. In a seminal paper dated 1948 Claude Shannon
pointed out the possibility of quantifying the information contained in a (inﬁnite) string
of characters [1]. Adopting a probabilistic approach, i.e. focusing the attention on the
source generating a string, the famous Shannon-McMillan theorem shows that there is a
limit to the possibility of compressing a string without loosing the information it brings.
This limit is proportional to the entropy (or informatic content) of that string [1, 2].

A remark is interesting now. The name entropy is not accidental, and information
theory represents one of the best examples of a concept developed in physics whose

Measuring complexity with zippers

2

role became of primary importance also in another ﬁeld. Historically, the concept of
entropy was initially introduced in thermodynamics in a phenomenological context.
Later, mainly by the contribution of Boltzmann, a probabilistic interpretation of the
entropy was developed in order to clarify its deep relation with the microscopic structure
underlying the macroscopic bodies. On his hand, Shannon, generalizing the concept of
entropy in the apparently unrelated ﬁeld of communication systems, was able to establish
a self consistent information theory. For a recent excursus about the notion of entropy
see [3]. We shall describe more precisely Shannon’s approach in the following section,
but we refer the interested reader to [4] for a discussion of the connections between
Shannon and microscopic entropies.

A radically diﬀerent approach to the information problem, namely the Algorithmic
[5, 6, 7, 8], was developed towards the half of the 1960s.
Information Theory (AIT)
It showed again, from a diﬀerent point of view, that a good way of quantifying the
information embedded in a string is that of trying to describe it in the shortest possible
way.

In this framework it seems natural to look at those algorithms expressly conceived
to compress a ﬁle (i.e. a string of bytes), known as zippers. A zipper takes a ﬁle and
tries to minimize its length. However, as we have mentioned, there is a theoretical limit,
represented by the entropy of the considered sequence, to the performance of a zipper.
A compression algorithm able to reach this theoretical limit is said to be “optimal”.
Thus an optimal zipper can be seen as an ideal tool to estimate the informatic content
of a string, i.e. to quantify the information it brings. In this paper we shall discuss this
possible application of data compression algorithms together with its shortcomings.

the important

Finally, besides

scientiﬁc problem of measuring how much
information is contained in a string, one could ask if it is possible to extract that
information. With a slight abuse of the word, we can address the level of the kind
of information contained in a sequence as the semantic level. We are then interested
in asking whether it is possible to access to the semantic level from a information
theoretical, “syntactic”, analysis of a string. We shall show that, under certain
assumptions, this is indeed the case in many diﬀerent circumstances.

The outline of this paper is as follows. In Section II we make a short introduction to
some Information Theory concepts; in Section III we describe the optimal compression
algorithm LZ77; in Section IV, ﬁnally, we illustrate with some examples the possible
applications of the illustrated information extraction techniques.

2. Entropy and complexity

In Shannon’s probabilistic approach to information, born in an engineering context,
the communication scheme is fundamental. A message is ﬁrst produced by a source
of information, then is codiﬁed in a way proper for the transmission in a channel and
ﬁnally, before arriving to the receiver, it must be brought back to the original form.
All these steps are of great theoretical interest, but for our purposes we will concentrate

Measuring complexity with zippers

3

on the source uniquely. This is a device able to form a message adding one symbol per
unit time, chosen in agreement with some probabilistic rules, to the previously emitted
ones. Here we consider only cases in which the possible characters are ﬁnite in number,
the alphabet X is ﬁnite. The source can then be identiﬁed with the stochastic
i.e.
process it obeys. Shannon’s IT always deals with ergodic sources. A rigorous deﬁnition
of ergodic processes is out of the scope of this paper. We shall limit ourselves to an
intuitive deﬁnition. A source is ergodic if it is stationary (the probability rules of the
source do not vary in time) and it holds the following property. If Nl is the number of
occurrences of a generic sequence Y = y1, ..., ys in a string X of length l > s, then:

P {|

lim
l→∞

Nl
l
i.e. the averages made over an emitted string, Nl
P (xi1, ..., xxs = y1, ..., ys), in the limit of inﬁnite string length.

− P (xi1, ..., xis = y1, ..., ys)| < ǫ} = 1

∀ ǫ, ys

(1)

l , coincide with those made over time

Now, if x is a n-symbols sequence chosen from the X n possible sequences of that

length, we introduce the N−block entropy as:

Hn = H(X1, X2, .., Xn) = −

p(x) log p(x)

(2)

x∈X n
X
where p(x) is the probability for the string x to be emitted. The diﬀerential entropy
hn = Hn+1 − Hn represents the average information carried by the n + 1 symbol when
the n previously emitted characters are known. Noting that the knowledge of a longer
past history cannot increase the uncertainty on the next symbol, we have that hn cannot
increase with n, i.e. it holds hn+1 ≤ hn and for an ergodic source we deﬁne the Shannon
entropy h as:

h = limn→∞ hn = limn→∞

Hn
n

.

(3)

The entropy of a source is a measure of the information it produces. In other words
h can be viewed as a measure of the surprise we have analyzing a string generated by a
stochastic process. Consider for example the case of a source emitting a unique symbol
A with probability 1. For that source it would hold h = 0, and in fact we would have
no surprise observing a new A. On the other hand if the probability of occurrence of
the symbol A is quite small our surprise will be proportionally large. In particular it
turns out to be proportional to the absolute value of the logarithm of its probability.
Then h is precisely the average surprise obtained by the stochastic process. Remarkably
it can be shown that h, apart from multiplicative coeﬃcients, is the only quantity that
measures the surprise generated by a stochastic process [2].
More precisely, the role of h as an information measure can be fully recognized in the
Shannon-McMillan theorem [1, 2]. Given a N characters-long message emitted by an
ergodic source, it states that:

(i) It exists a coding for which the probability for the message to require more than

Nh2 = (Nh/ log 2) bits tends to zero when N tends to inﬁnity.

Measuring complexity with zippers

4

(ii) It does not exist a coding for which the probability for the message to require less

than Nh2 bits tends to one when N tends to inﬁnity.

A completely diﬀerent approach to information related problems is that of the
Algorithmic Information Theory [5, 6, 7, 8]. In this context the focus is on the single
sequence, rather than on its source, and the basic concept is the Algorithmic Complexity:
the entropy of a string of characters is the length (in bits) of the smallest program
which produces as output the string and stops afterwards. This deﬁnition is abstract. In
particular it is impossible, even in principle, to ﬁnd such a program and as a consequence
the algorithmic complexity is a non computable quantity. This impossibility is related to
the halting problem and to Godel’s theorem [9]. Nevertheless also this second approach
indicates that searching for the most concise description of a sequence is the way for
estimating the amount of information it contains. As one could expect, in fact, there
is a connection between the Algorithmic Complexity of a string and the Entropy of its
source, but we refer the interested reader to [9] for a detailed discussion.

Up to this point our attention has been devoted to the characterization of a single
string. Both IT and AIT, however, provide several measures of relations of remoteness,
or proximity, between diﬀerent sequences. Among these, it is interesting to recall the
notion of relative entropy (or Kullback-Leibler divergence) [16, 17, 18] which is a measure
of the statistical remoteness between two distributions. Its essence can be easily grasped
with the following example.
Let us consider two stationary memoryless sources A and B emitting sequences of 0 and
1: A emits a 0 with probability p and 1 with probability 1 − p while B emits 0 with
probability q and 1 with probability 1−q. The optimal coding for a sequence emitted by
A codiﬁes on average every character with h(A) = −p log2 p − (1 − p) log2(1 − p) bits
(the Shannon entropy of the source). This optimal coding will not be the optimal one
for the sequence emitted by B. In particular the entropy per character of the sequence
emitted by B in the coding optimal for A will be:

C(A|B) = −q log2 p − (1 − q) log2(1 − p)

(4)

while the entropy per character of the sequence emitted by B in its optimal coding is
−q log2 q − (1 − q) log2(1 − q). Eq.(4) deﬁnes the so-called cross-entropy per character
of A and B. The number of bits per character waisted to encode the sequence emitted
by B with the coding optimal for A is the relative entropy of A and B:

d(A||B) = C(A|B) − h(A) = −q log2

− (1 − q) log2

(5)

p
q

1 − p
1 − q

.

A linguistic example will help to clarify the situation: transmitting an Italian text with
a Morse code optimized for English will result in the need of transmitting an extra
number of bits with respect to another coding optimized for Italian: the diﬀerence is
a measure of the relative entropy between, in this case, Italian and English (supposing
the two texts are each one archetypal representations of their Language, which is not).

Measuring complexity with zippers

5

Input Sequence Output Sequence

Pointer
(3,5)

P

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:0)
(cid:1)

(cid:0)
(cid:1)
(cid:0)
(cid:1)
(cid:0)
(cid:1)

(cid:0)
(cid:1)
(cid:0)
(cid:1)
(cid:0)
(cid:1)

Figure 1. Scheme of the LZ77 algorithm: The LZ77 algorithm searches in the
look-ahead buﬀer for the longest substring (in this case substring of colors) already
occurred and replaces it with a pointer represented by two numbers: the length of the
matching and its distance

It is important to remark that the relative and cross entropies are not distances
(metric) in the mathematical sense, since they are not symmetric and do not satisfy
in general the triangular inequality. Deﬁning a true distance between strings is
an important issue both for theoretical and practical reasons (see for some recent
approaches [11, 12, 13] and for a short review [21]).

3. Zippers

In the previous section we have seen two diﬀerent approaches to the characterization
of the information, the Classical and the Algorithmic ITs. We have also seen that,
despite their profound diﬀerences, both of them indicate that the way to quantify the
information of a string is to ﬁnd its shortest description, i.e. to compress it. Driven by
this fact, in this paragraph we shall illustrate the LZ77 compression algorithm, that,
asymptotically, is able to get to the Shannon limit.

The Lempel and Ziv algorithm LZ77 [14] (see Figure 1) (used for instance by gzip
and zip commercial zippers) achieves compression exploiting the presence of duplicated
strings in the input data. The second occurrence of a string is replaced by a pointer to
the previous string given by two numbers: a distance, representing how far back into
the window the sequence starts, and a length, representing the number of characters for
which the sequence is identical. More speciﬁcally the zipper reads sequentially the input
N-symbols sequence, x = x1, ...., xN . When n symbols have already been analyzed, LZ77
ﬁnds the longest string starting at symbol n + 1 which has already been encountered in
the previous n characters. In other words LZ77 looks for the largest integer m such that

Measuring complexity with zippers

6

the string xn+1, ..., xn+m already appeared in x1, ..., xn. The string found is then codiﬁed
its length m and the distance from its previous occurrence. If no
with two numbers:
already encountered string starts at position n the zipper simply writes the symbol
appearing in that position in the compressed sequence and starts a new search from
position n + 1.
From the above description it is intuitive that LZ77 performs better and better as the
number of processed symbols grows. In particular, for inﬁnitely long strings (emitted by
ergodic sources), its performance is “optimal”, i.e. the length of the zipped ﬁle divided
by the length of the original ﬁle tends to h/ ln 2 [15]. The convergence to this limit,
however, is extremely slow. Said code rate the average bits per symbol needed to encode
the sequence, it holds:

code rate ≃ h2 + O

ln ln N
ln N !

 

(6)

(7)

Notwithstanding its limitations, LZ77 can then be seen as a tool for estimating
the entropy of a sequence. However, the knowledge of h2, though interesting from a
theoretical point of view is often scarcely useful in applications. For practical purposes,
on the other hand, methods able to make comparisons between strings are often required.
A very common case, for instance, is that in which one has to classify an unknown
sequence with respect to a dataset of known strings: i.e. one has to decide which known
strings is closer (in some sense) to the unknown string.

In Section II we have introduced the relative entropy and the cross entropy between
two sources. Recently, a method has been proposed for the estimate of the cross entropy
between two strings based on LZ77 [11]. Recalling that the cross entropy C(A|B)
between two strings A and B, is given by the entropy per character of B in the optimal
coding for A, the idea is that of appending the two sequences and zipping the resulting
In this way the zipper “learns” the A ﬁle and, when encounters the B
ﬁle A + B.
If B is not too
subsequence, tries to compress it with a coding optimized for A.
long [20, 21], thus preventing LZ77 from learning it as well, the cross entropy per
character can be estimated as:

C(A|B) ≃

LA+B − LA
LB

where LX is the length of the compressed X sequence. This method is strictly related to
the Ziv-Merhav algorithm [19] to estimate the relative entropy between two individual
sequences.

4. Examples and numerical results

In this section we illustrate the behavior of LZ77 in experiments of entropy estimation
and of recognition with two examples. Figure 2 reports the LZ77 code rates when zipping
Bernoulli sequences of various lengths. A Bernoulli string is generated by extracting
randomly one of K allowed symbols with probability 1/K (K = 10 in our case). The

Measuring complexity with zippers

7

d
e
t
a
m

i
t
s
e

h

4,2

4,1

4

3,9

3,8

4
10

5

10

N

6
10

7
10

Figure 2. Entropy estimation: The number of bits per characters of the zipped
sequence hestimated is plotted versus the length N of the original one. Bernoulli
sequences with K = 10 symbols are analyzed. The zipper performs better with longer
strings, but the convergence towards the optimal compression, thought theoretically
is extremely slow. The Shannon entropy of the considered sequences is
proved,
h2 ≃ 3.32 and, for strings of approximately 8 × 106 characters, hestimated is 18%
larger than this value.

entropy of such strings is simply log K. From the Figure it is evident that the zipper
performs better and better with longer strings, though, as seen in (6), the convergence
is extremely slow. It is important to remark how there exist more eﬃcient algorithms
to estimate the entropy of a string. We refer the interested reader to [22] for a recent
review. It is nevertheless useful to quote the so-called Shannon Game to estimate the
entropy of English (see [23] for an applet) where the identiﬁcation of the entropy with
a measure of surprise is particularly clear.

In Figure 3 an experiment of recognition is reported [20]. Here an unknown sequence
is compared, in the sense discussed in the previous section, with a number of known
strings. The idea to test is that the unknown sequence was emitted by the same source
of the closer known one. The source is here a Lozi map, i.e. a dynamical system of the
form:

xn+1 = 1 − a|xn| + yn
yn+1 = bxn

(

where a and b are parameters. The sequence of symbols used in the following test is
obtained taking 0 when x ≤ 0 and 1 when x > 0. For b = 0.5, numerical studies show
that the Lozi map is chaotic for a in the interval (1.51, 1.7). For a discussion of the Lozi
map, computation of Lyapunov exponents and representation of its symbolic dynamics
in terms of Markov chains, see [24].

Figure 3 reports the result of this test. A Lozi map with a = 1.6, b = 0.5 and initial
condition x = 0.1, y = 0.1 has been used to generate the sequence A, of length 10000,
that will be used as unknown sequence. As probing sequences we have generated a sets

Measuring complexity with zippers

8

Figure 3. Recognition experiment: The relative entropy, estimated by means
of LZ77 as discussed in text, between an unknown sequence A and a set of known
strings B allows to identify the source of A. All the sequences are generated by a Lozi
map, and the problem is to identify the parameter aA = 1.6 of the source of A. The
minimum of relative entropy allows clearly to identify this parameters indicating that
A is closer to the string B generated with aB = aA than to any strings generated with
diﬀerent values of a.

of sequences, B of length 1000, obtained with Lozi maps with the parameters b = 0.5
and aB varying between 1.52 and 1.7. The quantity computed and reported in the graph
is an estimate of the Kullback-Leibler entropy d(B||A) = C(A|B) − C(B∗|B), where
C(B∗|B) is the estimate, in the framework of our scheme, of the entropy rate of B and
B∗ is another set of sequences of length 10000. As it is evident, in our experiment, the
closer sequence to the unknown one is the one with a = 1.6 and this means that the
recognition experiments was successful.

5. Conclusions

The possibility of quantifying the information contained in a string of symbols has been
one of the great advancements in science of the last 60 years. Both Shannon’s and
Algorithmic approaches indicate that ﬁnding a synthetic description of a string is a
way to determine how much information it stores. It is then natural focusing on those
algorithms conceived expressly to compress a string, also known as zippers. In this paper
we have introduced some fundamental concepts of Information Theory and we have
described the LZ77 compressor. This zipper has the property of being asymptotically
optimal, thus being also a potential tool for estimating the entropy of a string. More
interestingly, we have discussed the possibility for LZ77 to be used for the estimation of
such quantities such as cross or relative entropy which measure the remoteness between
diﬀerent strings. Finally we have shown a simple example of entropy estimation for a
Bernoulli sequence and a successful experiment of recognition between strings emitted
by a Lozi map with diﬀerent parameters.

Measuring complexity with zippers

Acknowledgments

9

we thank Valentina Alﬁ, Dario Benedetto, Andrea Puglisi and Angelo Vulpiani for many
interesting discussions and contributions to this work. V.L. acknowledges the partial
support of the ECAgents project funded by the Future and Emerging Technologies
program (IST-FET) of the European Commission under the EU RD contract IST-1940.
E.C. acknowledges the partial support of the European Commission through its 6th
Framework Programme ”Structuring the European Research Area” and the contract
Nr. RITA-CT-2004-505493 for the provision of Transnational Access implemented as
Speciﬁc Support Action.

Appendix

We report here an example of implementation of LZ77. It must be intended as a didactic
illustration, since actual implementations of the algorithm contain several optimizations.
• Build a vector V whose jth component V [j] is the jth symbol of string S that must be compressed;
• Build a vector I whose jth component, I[j], is the position of the closest previous occurrence of

symbol v appearing in V [j], or 0 if symbol v has never appeared before;

• Build an empty vector C which will contain the processed V (i.e. the processed S);

• deﬁne i = 0;

while (i < |V |) do:

deﬁne p = I[i], lmax = 1, pmax = 0;

while (p 6= 0) do:

deﬁne l = 1;

while (V [i + l] = V [p + l] and (p + l) < i) do:

l = l + 1;

p = I[p];

if l > lmax do lmax = l, pmax = p;

if (l > 1) append to vector C the token (lmax, i − pmax);

else, if (l = 1), append to vector C the token (0, 0, V [i]);

i = i + l;

Before concluding we mention two of the most common adjoint features to the LZ77
algorithm. The ﬁrst aims at codifying better the length-distance token. This is often
achieved by zipping further the compressed string exploiting its statistical properties
with the Huﬀman algorithm [23]. The second feature is due to the necessity of speeding
up the zipping process in commercial zippers.
It consists in preventing LZ77 from
looking back for more than a certain number w of symbols. Such a modiﬁed zipper is
said to have a “w-long sliding window”.

Measuring complexity with zippers

10

References

[1] C.E. Shannon, The Bell System Technical J. 27, 623 (1948).
[2] A.I. Khinchin, Mathematical Foundations of Information Theory (Dover, New York, 1957).
[3] M. Falcioni, V. Loreto and A. Vulpiani, in The Kolmogorov Legacy in Physics, R: Livi and A.
Vulpiani eds. Lecture note in Physics, vol. 636 (Springer, 2003). See also G. Parisi’s contribution
in the same volume.

[4] G. Parisi, Statistical Field Theory (Addison Wesley, New York, 1988).
[5] G.J. Chaitin, Journal of the Association for Computer Machinery 13, 547 (1966).
[6] G.J. Chaitin, Information, randomness and incompleteness (2nd ed.) (World Scientiﬁc, Singapore,

2002).

(2004).

[7] A.N. Kolmogorov, Problems of Information Transmission 1, 1 (1965).
[8] R.J. Solomonov, Information and Control 7, 1 and 224 (1964).
[9] M. Li and P.M.B. Vit´anyi, An introduction to Kolmogorov complexity and its applications (2nd.

ed.) (Springer, 1997).

[10] T. Cover and J. Thomas, Elements of information theory (Wiley, New York, 1991).
[11] D. Benedetto, E. Caglioti and V. Loreto, Phys. Rev. Lett. 88, 048702 (2002).
[12] H.H. Otu and K. Sayood, Bioinformatics 19, 2122 (2003).
[13] M. Li, X. Chen, X. Li, B. Ma and P.M.B. Vitanyi, IEEE Trans Information Theory 50, 3250

[14] A. Lempel and J. Ziv, em IEEE Transactions on Information Theory 23, 337 (1977).
[15] A.D. Wyner and J. Ziv, Proceeding of the IEEE 82, 872 (1994).
[16] S. Kullback and R.A. Leibler, Annals of Mathematical Statistics 22, 79 (1951).
[17] S. Kullback, Information Theory and Statistics (Wiley, 1959).
[18] T. Cover and J. Thomas, Elements of information theory (Wiley, New York, 1991).
[19] J. Ziv and N. Merhav, IEEE Trans. Inf. Th. 39, 1270 (1993).
[20] A. Puglisi, D. Benedetto, E. Caglioti, V. Loreto and A. Vulpiani, Physica D 180, 92 (2003).
[21] A. Baronchelli, E. Caglioti and V. Loreto, J. Stat. Mech P04002 (2005).
[22] T. Schuermann and P. Grassberger, Chaos 6, 414 (1996).
[23] http://www.math.ucsd.edu/ ˜ crypto/java/ENTROPY/
[24] A. Crisanti, G. Paladin and A. Vulpiani, Products of Random Matrices in Statistical Physics

(Springer-Verlag, Berlin, 1993).

[25] D.A. Huﬀman, Proceedings of the Institute of Radio Engineers 40, 1098 (1952).


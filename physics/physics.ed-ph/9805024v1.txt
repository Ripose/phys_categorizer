8
9
9
1
 
y
a
M
 
8
1
 
 
]
h
p
-
d
e
.
s
c
i
s
y
h
p
[
 
 
1
v
4
2
0
5
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Statistical Mechanics in a Nutshell∗

Jochen Rau
Max-Planck-Institut f¨ur Physik komplexer Systeme
N¨othnitzer Straße 38, 01187 Dresden, Germany

February 2, 2008

Contents

1 Some Probability Theory

1.1 Constrained distributions . . . . . . . . . . . . . . . . . . . . . . .
1.2 Concentration theorem . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Frequency estimation . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . .
Jaynes’ analysis of Wolf’s die data . . . . . . . . . . . . . . . . . .
1.5
1.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Macroscopic Systems in Equilibrium

2.1 Macrostate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 First law of thermodynamics . . . . . . . . . . . . . . . . . . . . .
2.3 Example: Ideal quantum gas . . . . . . . . . . . . . . . . . . . . .
2.4 Thermodynamic potentials . . . . . . . . . . . . . . . . . . . . . .
2.5 Correlations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Linear Response

3.1 Liouvillian and Evolution . . . . . . . . . . . . . . . . . . . . . . .
3.2 Kubo formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Example: Electrical conductivity . . . . . . . . . . . . . . . . . .

2
2
2
5
7
8
10

10
10
13
15
16
19

19
19
21
22

∗Part I of a course on “Transport Theory” taught at Dresden University of Technology,

Spring 1997. For more information see the course web site
http://www.mpipks-dresden.mpg.de/

jochen/transport/intro.html

∼

1

1 Some Probability Theory

1.1 Constrained distributions

A random experiment has n possible results at each trial; so in N trials there
are nN conceivable outcomes. (We use the word “result” for a single trial, while
“outcome” refers to the experiment as a whole; thus one outcome consists of an
enumeration of N results, including their order. For instance, ten tosses of a die
(n = 6, N = 10) might have the outcome “1326642335.”) Each outcome yields a
set of sample numbers
. In
and relative frequencies
many situations the outcome of a random experiment is not known completely:
One does not know the order in which the individual results occurred, and often
one does not even know all n relative frequencies
but only a smaller number
m (m < n) of linearly independent constraints

fi = Ni/N, i = 1 . . . n
}

Ni}

fi}

{

{

{

Gi

afi = ga

,

a = 1 . . . m .

(1)

n

Xi=1

As a simple example consider a loaded die. Observations on this badly bal-
anced die have shown that 6 occurs twice as often as 1; nothing peculiar was
observed for the other faces. Given this information only and nothing else, i.e.,
not making use of any additional information that we might get from inspection
of the die or from past experience with dice in general, all we know is a single
constraint of the form (1) with

2 :
0 :
1 :

i = 1
i = 2 . . . 5
i = 6

Gi

1 = 


−



(2)

and g1 = 0.

The available data –in the form of linear constraints– are generally not suﬃ-
cient to reconstruct unambiguously the relative frequencies
. These frequen-
cies may be regarded as Cartesian coordinates of a point in an n-dimensional
[0, 1] and the normal-
vector space. The m linear constraints, together with fi ∈
fi = 1, then just restrict the allowed points to some portion
ization condition
of an (n
1)-dimensional hyperplane.

fi}

m

{

−

−

P

1.2 Concentration theorem

for the results i = 1 . . . n, the
Given an a priori probability distribution
probability that N trials will yield the –generally diﬀerent– relative frequencies
fi}

pi}

is

{

{

prob(
{

fi}|{

pi}

, N) =

N!
N1! . . . Nn!

pN1
1

. . . pNn
n

.

(3)

2

(4)

(5)

(6)

(7)

(8)

(9)

Here the second factor is the probability for one speciﬁc outcome with sample
, and the ﬁrst factor counts the number of all outcomes that give
numbers
rise to the same set of sample numbers. With the deﬁnition

Ni}

{

Ip(f ) :=

fi ln

fi
pi

−

Xi
, p =

pi}

{

fi}

{
p, N) = prob(f

prob(f

|

and the shorthand notations f =

we can also write

In particular, for two diﬀerent data sets
probabilities is given by

f, N) exp[NIp(f )]

.

|
fi}

{

and

f ′
i}

{

the ratio of their respective

prob(f
|
prob(f ′
|

p, N)
p, N)

=

prob(f
prob(f ′

f, N)
|
f ′, N)
|

exp[N(Ip(f )

Ip(f ′))]

−

where, by virtue of Stirling’s formula

it is asymptotically

√2πx xxe−x

,

x!

≈

prob(f
prob(f ′

f, N)
|
f ′, N) ≈ v
u
|
u
t

Yi

f ′
i
fi

.

As the latter ratio is independent of N, for large N and nearby distributions
f ′
p, N) is completely dominated by
the exponential:

f the variation of prob(f

p, N)/prob(f ′

≈

|

|

prob(f
|
prob(f ′
|

p, N)
p, N) ≈

exp[N(Ip(f )

Ip(f ′))]

.

−

Hence the probability with which any given frequency distribution f is realized is
essentially determined by the quantity Ip(f ): The larger this quantity, the more
likely the frequency distribution is realized.

Consider now all frequency distributions allowed by m linearly independent
constraints. As we discussed earlier, the allowed distributions can be visualized
as points in some portion of an (n
m
In this
hyperplane portion there is a unique point at which the quantity Ip(f ) attains a
maximum I max
; we call this point the “maximal point” f max. (That the maximal
point is indeed unique can be seen as follows: Suppose there were not one but
two maximal points corresponding to frequency distributions f (1) and f (2). Then
the mixture ¯f = (f (1) + f (2))/2 would have Ip( ¯f) > I max
, which would be a
x1 . . . xn−m−1}
contradiction.)
in the
hyperplane such that

p
It is possible to deﬁne new coordinates

1)-dimensional hyperplane.

−

−

{

p

3

(10)

(11)

(13)

(14)

they are linear functions of the

;

fi}

{

the origin (~x = 0) is at the maximal point; and

in the vicinity of the maximal point

Ip(~x) = I max

ar2 + O(r3)

,

a > 0 ,

p −

•

•

•

where

n−m−1

r :=

x2
j

.

v
u
u
t
Frequency distributions that satisfy the given constraints (1) and whose Ip(~x)
diﬀers from I max
by more than ∆I thus lie outside a hypersphere around the
maximal point, the sphere’s radius R being given by aR2 = ∆I. The probability
that N trials will yield such a frequency distribution outside the hypersphere is

Xj=1

p

prob(Ip < (I max

∆I)

m constraints) =

p −

|

∞
R dr rn−m−2 exp(
−
∞
0 dr′ r′n−m−2 exp(
R
−
R

Nar2)
Nar′2)

.

(12)

Here the factors rn−m−2 in the integrand are due to the volume element, while
the exponentials exp(
)) stem from the ratio (9).
Substituting t = Nar2, deﬁning

Nar2) = exp(N(Ip(~x)

I max
p

−

−

and using

one may also write

s := (n

m

3)/2

−

−

Γ(s + 1) =

∞

0
Z

dt ts exp(

t)

−

prob(Ip < (I max

∆I)

m constraints) =

p −

|

1
Γ(s + 1) Z

∞

N ∆I

dt ts exp(

t)

;

(15)

−

which for large N (N

s/∆I) can be approximated by

≫

prob(Ip < (I max

∆I)

m constraints)

p −

|

1
Γ(s + 1)

≈

(N∆I)s exp(

N∆I) .

(16)

−

→ ∞

As the number N of trials increases, this probability rapidly tends to zero for
any ﬁnite ∆I. As N
, therefore, it becomes virtually certain that the (aside
from m constraints) unknown frequency distribution has an Ip very close to I max
.
Hence not only does the maximal point represent the frequency distribution that
is the most likely to be realized (cf. Eq. (9)); but in addition, as N increases,
all other –theoretically allowed– frequency distributions become more and more
concentrated near this maximal point. Any frequency distribution other than the
maximal point becomes highly atypical of those allowed by the constraints.

p

4

1.3 Frequency estimation

We have seen that the knowledge of m (m < n) “averages” (1) constrains, but
fails to specify uniquely, the relative frequencies
. In view of this incomplete
information the relative frequencies must be estimated. Our previous consid-
erations suggest that the most reasonable estimate is the maximal point: that
distribution which, while satisfying all the constraints, maximizes Ip(f ). This
leads to a variational equation

fi}

{

where the constraints, as well as the normalization condition
implemented by means of Lagrange multipliers. Its solution is of the form

i fi = 1, have been

δ

fi ln

+ η

fi +

λa

Gi

afi

= 0

#

Xi

a
X

Xi

"
Xi

fi
pi

f max
i =

1
Z

exp

ln pi − h

 

ln p

ip −

P

λaGi
a

!

a
X

Z =

exp

ln pi − h

 

ln p

ip −

Xi

λaGi
a

!

.

a
X

ln p

ip :=

h

pj ln pj

Xj

with

The term

has been introduced by convention; it cancels from the ratio in (18) and so does
not aﬀect the frequency estimate. The expression in the exponent simpliﬁes if
pi}
and only if the a priori distribution
ln p

is uniform: In this case,

(21)

ip = 0 .

The m Lagrange parameters
prescribed averages

ga}

{

must be adjusted such as to yield the correct

. They can be determined from

{
ln pi − h
λa

{

}

∂
∂λa ln Z =

−

ga

,

a set of m simultaneous equations for m unknowns. Finally, inserting (18) into
the deﬁnition of Ip(f ) gives

I max
p =

ln p

ip + ln Z +

h

λaga

.

a
X

There remains the task of specifying the –possibly nonuniform– a priori prob-
are those probabilities one would assign before
ability distribution
having asserted the existence of the constraints (1); i.e., being still in a state of
ignorance. This “ignorance distribution” can usually be determined on the basis

. The

pi}

pi}

{

{

5

(17)

(18)

(19)

(20)

(22)

(23)

of symmetry considerations: If the problem at hand is a priori invariant under
pi}
, too, must exhibit this same group in-
some characteristic group then the
variance.1 For example, if a priori we do not know anything about the properties
of a given die then our prior ignorance extends to all faces equally. The prob-
lem is therefore invariant under a relabelling of the faces, which trivially implies
pi = 1/6
. In more complicated random experiments, especially those involving
{
continuous and hence coordinate-dependent distributions, the task of specifying
the a priori distribution may be less straightforward.2

}

{

For illustration let us return to the example of the loaded die, characterized
solely by the single constraint (2). What estimates should we make of the relative
frequencies
with which the diﬀerent faces appeared? Taking the a priori
probability distribution –assigned to the various faces before one has asserted the
die’s imperfection– to be uniform,
, the best estimate (18) for the
frequency distribution reads

pi = 1/6

fi}

}

{

{

with only a single Lagrange parameter λ1 and

Z −1 exp(

−

2λ1) :
Z −1 :
Z −1 exp(λ1) :

i = 1
i = 2 . . . 5
i = 6

f max
i = 




Z = exp(

2λ1) + 4 + exp(λ1)

.

−

The Lagrange parameter is readily determined from

∂
∂λ1 ln Z =

−

g1 = 0 ,

λ1 = (ln 2)/3 .

with solution

This in turn gives the numerical estimates

f max
i = 


0.107 :
0.170 :
0.214 :

i = 1
i = 2 . . . 5
i = 6

(24)

(25)

(26)

(27)

(28)

with an associated



(29)
1The rationale underlying this consistency requirement has historically been called the “Prin-

I max
p = ln(1/6) + ln Z =

0.019 .

−

ciple of Insuﬃcient Reason” (J. Bernoulli, Ars Conjectandi, 1713).

2see for example E. T. Jaynes, Prior probabilities, IEEE Trans. Systems Sci. Cyb. 4, 227

(1968)

6

n

Xi=1

The above algorithm for estimating frequencies can be iterated. Suppose
that beyond the m constraints (1) we learn of l additional, linearly independent
constraints

Gi

afi = ga

,

a = (m + 1) . . . (m + l) .

(30)

In order to make an improved estimate that takes these additional data into
account we can either, (i) starting from the same a priori distribution p as before,
apply the algorithm to the total set of (m + l) constraints; or (ii) iterate: use
the previous estimate (18), which was based on the ﬁrst m constraints only, as a
new a priori distribution f max
p′, and then repeat the algorithm just for the l
additional constraints. Both procedures give the same improved estimate f max′.
Associated with this improved estimate is

7→

I max
p

′ = I max

p + If max(f max′)

.

(31)

1.4 Hypothesis testing

Now we consider random experiments for which complete frequency data are
available. Suppose that, based on some insight we have into the systematic
inﬂuences aﬀecting the experiment, we conjecture that the observed relative fre-
quencies can be fully characterized by a set of constraints of the –by now familiar–
form (1), and that hence the observed relative frequencies can be ﬁtted with a
maximal distribution (18). This maximal distribution contains m ﬁt parameters
λa
(the Lagrange parameters) whose speciﬁc values depend on the averages
}
{
It represents our theoretical
ga}
, which in turn are extracted from the data.
{
model or hypothesis.

In general, the experimental frequencies f and the theoretical ﬁt f max do
not agree exactly. Must the hypothesis therefore be rejected, or is the deviation
merely a statistical ﬂuctuation? The answer is furnished by the concentration
theorem: Let N be the number of trials performed to establish the experimental
distribution, let

(32)

∆I = I max

Ip(f )

p −
≫

m

s/∆I) the probability that statistical
and s = (n
ﬂuctuations alone yield an Ip-diﬀerence as large as ∆I is given by (16); typically
the hypothesis is rejected whenever this probability is below 5%,3

3)/2. For large N (N

−

−

prob(Ip < (I max

p −

∆I)

m constraints) < 5% .

(33)

|
Rejecting a hypothesis means that the chosen set of constraints was not complete,
and hence that important systematic eﬀects have been overlooked. These must be
incorporated in the form of additional constraints. In this fashion one can proceed
iteratively from simple to ever more sophisticated models until the deviation of
the ﬁt from the experimental data ceases to be statistically signiﬁcant.

3The hypothesis test presented here is closely related to the better-known χ2 test.

7

i
1
2
3
4
5
6

fi
∆i
0.16230
-0.00437
0.17245 +0.00578
-0.02182
0.14485
0.14205
-0.02464
0.18175 +0.01508
0.19960 +0.02993

Table 1: Wolf’s die data: frequency distribution f and its deviation ∆ from the
uniform distribution.

1.5 Jaynes’ analysis of Wolf’s die data

The above prescription for testing hypotheses and –if rejected– for iteratively
improving them by enlarging the set of constraints has been lucidly illustrated
by E. T. Jaynes in his analysis of Wolf’s die data.4 Rudolph Wolf (1816–1893), a
Swiss astronomer, had performed a number of random experiments, presumably
In one of these experiments a die
to check the validity of statistical theory.
(actually two dice, but only one of them is of interest here) was tossed 20, 000
times in a way that precluded any systematic favoring of any face over any other.
from
The observed relative frequencies
the a priori probabilities
are given in Table 1. Associated with the
observed distribution is

and their deviations

∆i = fi −

{
pi = 1/6

fi}
}

pi}

{

{

Ip(f ) =

0.006769 .

−

Our “null hypothesis” H0 is that the die is ideal and hence that there are no
constraints needed to characterize any imperfection (m = 0); the deviation of the
experimental from the uniform distribution, with associated

I max(H0)
p

= Ip(p) = 0 ,

is merely a statistical ﬂuctuation. However, the probability that statistical ﬂuc-
tuations alone yield an Ip-diﬀerence as large as

∆I H0 = I max(H0)

p

Ip(f ) = 0.006769

is practically zero: Using Eq. (16) with N = 20, 000 and s = 3/2 we ﬁnd

prob(Ip < (I max

(37)
4E. T. Jaynes, Concentration of distributions at entropy maxima, in: E. T. Jaynes, Papers on
Probability, Statistics and Statistical Mechanics, ed. by R. D. Rosenkrantz, Kluwer Academic,
Dordrecht (1989).

0 constraints)

p −

∆I H0)

10−56

∼

|

.

(34)

(35)

(36)

−

8

Therefore, the null hypothesis is rejected: The die cannot be perfect.

Our analysis need not stop here. Not knowing the mechanical details of the
die we can still formulate and test hypotheses as to the nature of its imperfections.
Jaynes argued that the two most likely imperfections are:

•

a shift of the center of gravity due to the mass of ivory excavated from the
spots, which being proportional to the number of spots on any side, should
make the “observable”

Gi

1 = i

3.5

−

•

= 0; and

have a nonzero average g1 6
errors in trying to machine a perfect cube, which will tend to make one
dimension (the last side cut) slightly diﬀerent from the other two.
It is
clear from the data that Wolf’s die gave a lower frequency for the faces
(3,4); and therefore that the (3-4) dimension was greater than the (1-6) or
(2-5) ones. The eﬀect of this is that the “observable”

Gi

2 =

(

−

1 :
2 :

i = 1, 2, 5, 6
i = 3, 4

has a nonzero average g2 6

= 0.

Our hypothesis H2 is that these are the only two imperfections present. More
speciﬁcally, we conjecture that the observed relative frequencies are characterized
by just two constraints (m = 2) imposed by the measured averages

and that hence the observed relative frequencies can be ﬁtted with a maximal
distribution

In order to test our hypothesis we determine

ﬁx the Lagrange parameters by requiring

and then calculate

g1 = 0.0983 and g2 = 0.1393 ;

f max(H2)
i

=

exp

1
Z

2

 −

a=1
X

λaGi
a

!

.

Z =

exp

6

Xi=1

2

 −

a=1
X

λaGi
a

!

,

∂
∂λa ln Z =

ga

−

I max(H2)
p

= ln(1/6) + ln Z +

λaga

.

2

a=1
X

9

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

With this algorithm Jaynes found

I max(H2)
p

=

0.006534

−

and thus

∆I H2 = I max(H2)

p

Ip(f ) = 0.000235 .

−

The probability for such an Ip-diﬀerence to occur as a result of statistical ﬂuctu-
ations is (with now s = 1/2)

(47)

p −

prob(Ip < (I max

∆I H2)

2 constraints)

2.5% ,

≈

|
much larger than the previous 10−56 but still below the usual acceptance bound
of 5%. The more sophisticated model H2 is therefore a major improvement over
the null hypothesis H0 and captures the principal features of Wolf’s die; yet there
are indications that an additional very tiny imperfection may have been present.
Jaynes’ analysis of Wolf’s die data furnishes a useful paradigm for the exper-
imental method in general. All modern experiments at particle colliders (CERN,
Desy, Fermilab. . . ), for example, yield data in the form of frequency distributions
over discrete “bins” in momentum space, for each of the various end products
of the collision. The search for interesting signals in the data (new particles,
new interactions, etc.) essentially proceeds in the same manner in which Jaynes
revealed the imperfections of Wolf’s die: by formulating physically motivated
hypotheses and testing them against the data. Such a test is always statistical in
nature. Conclusions (say, about the presence of a top quark, or about the pres-
ence of a certain imperfection of Wolf’s die) can never be drawn with absolute
certainty but only at some –quantiﬁable– conﬁdence level.

1.6 Conclusion

In all our considerations a crucial role has been played by the quantity Ip: The
algorithm that yields the best estimate for an unknown frequency distribution is
based on the maximization of Ip; and hypotheses can be tested with the help of
Eq. (16), i.e., by simply comparing the experimental and theoretical values of Ip.
We shall soon encounter the quantity Ip again and see how it is related to one of
the most fundamental concepts in statistical mechanics: the “entropy.”

2 Macroscopic Systems in Equilibrium

2.1 Macrostate

For complex systems with many degrees of freedom (like a gas, ﬂuid or plasma)
the exact microstate is usually not known. It is therefore impossible to assign to
the system a unique point in phase space (classical) or a unique wave function

10

(49)

(50)

(51)

(52)

(quantal), respectively. Instead one must resort to a statistical description: The
system is described by a classical phase space distribution ρ(π) or an incoherent
mixture

ˆρ =

fi|
of mutually orthogonal quantum microstates
, respectively. (Where the dis-
tinction between classical and quantal does not matter we shall use the generic
symbol ρ.) Probabilities must be real, non-negative, and normalized to one;
which implies the respective properties

i
i}

i
ih

(48)

Xi

i
|

{|

ρ(π)∗ = ρ(π)

,

ρ(π)

0 ,

dπ ρ(π) = 1

≥

Z

ˆρ† = ˆρ ,

ˆρ

0 ,

tr ˆρ = 1 .

≥

In this statistical description every observable A (real phase space function or
Hermitian operator, respectively) is assigned an expectation value

A
iρ =

h

Z

dπ ρ(π)A(π)

iρ = tr(ˆρ ˆA)
A

h

,

respectively.

Typically, not even the distribution ρ is a priori known. Rather, the state of
a complex physical system is characterized by very few macroscopic data. These
data may come in diﬀerent forms:

as data given with certainty, such as the type of particles that make up the
system, or the shape and volume of the box in which they are enclosed.
These exact data we take into account through the deﬁnition of the phase
space or Hilbert space in which we are working;

as prescribed expectation values

Gaiρ = ga

h

,

a = 1 . . . m

(53)

{

Ga}

of some set
of selected macroscopic observables. Examples might be
the average total energy, average angular momentum, or average magneti-
zation. Such data, which are of a statistical nature, impose constraints of
the type (1) on the distribution ρ; or

•

as additional control parameters on which the selected observables
may explicitly depend, such as an external electric or magnetic ﬁeld.

Ga}

{

or

or

•

•

11

with

and

(54)

(55)

(56)

(57)

(58)

According to our general considerations in Section 1.3 the best estimate for the
thus characterized macrostate is a distribution of the form (18). In the classical
case this implies

ρ(π) =

exp

ln σ(π)

ln σ

1
Z

 

− h

iσ −

a
X

λaGa(π)

!

Z =

dπ exp

ln σ(π)

ln σ

Z

 

− h

iσ −

a
X

λaGa(π)

;

!

while for a quantum system

1
Z

ˆρ =

exp

ln ˆσ

ln σ

 

− h

λa ˆGa

!

iσ −

a
X

Z = tr exp

ln ˆσ

ln σ

 

− h

λa ˆGa

!

.

iσ −

a
X

In both cases σ denotes the a priori distribution. The auxiliary quantity Z is
referred to as the partition function.5

The phase space integral or trace in the respective expressions for Z depend
on the speciﬁc choice of the phase space or Hilbert space; hence they may depend
on parameters like the volume or particle number. Furthermore, there may be an
or of the a priori distribution σ on ad-
Ga}
explicit dependence of the observables
ditional control parameters. Therefore, the partition function generally depends
λa
not just on the Lagrange multipliers
but also on some other parameters
hb

. In analogy with the relation (22) one then deﬁnes new variables

}

{

{

{

}

γb :=

∂
∂hb ln Z .

(In contrast to (22) there is no minus sign.) The
are
called the thermodynamic variables of the system; together they specify the sys-
tem’s macrostate. The thermodynamic variables are not all independent: Rather,
they are related by (22) and (58), that is, via partial derivatives of ln Z. One
says that hb and γb, or ga and λa, are conjugate to each other.

ga}

γb}

and

{

{

}

}

{

{

,

,

λa

hb

Some combinations of thermodynamic variables are of particular importance,
which is why the associated distributions go by special names. If the observables
that characterize the macrostate –in the form of sharp values given with certainty,
5Readers already familiar with statistical mechanics might be disturbed by the appearance
of σ in the deﬁnitions of ρ and Z. Yet this is essential for a consistent formulation of the
theory: see, for instance, our remarks at the end of Section 1.3 on the possibility of iterating
In most practical applications σ is uniform and hence
the frequency estimation algorithm.
ln σ

σ = 0. Our deﬁnitions of ρ and Z then reduce to the conventional expressions.
i

ln σ

− h

12

{

or in the form of expectation values– are all constants of the motion then the
system is said to be in equilibrium. Associated is an equilibrium distribution
Ga}
of the form (54) or (56), with all
being constants of the motion. Such
an equilibrium distribution is itself constant in time, and so are all expectation
values calculated from it.6 The set of constants of the motion always includes the
Hamiltonian (Hamilton function or Hamilton operator, respectively) provided it
is not explicitly time-dependent. If its value for a speciﬁc system, the internal
energy, and the other macroscopic data are all given with certainty then the
resulting equilibrium distribution is called microcanonical; if just the energy is
given on average, while all other data are given with certainty, canonical; and if
both energy and total particle number are given on average, while all other data
are given with certainty, grand canonical.

{

}

hb

and

Strictly speaking, every description of the macrostate in terms of thermody-
namic variables represents a hypothesis: namely, the hypothesis that the sets
Ga}
are actually complete. This is analogous to Jaynes’ model for
{
Wolf’s die, which assumes that just two imperfections (associated with two ob-
servables G1, G2) suﬃce to characterize the experimental data. Such a hypothesis
may well be rejected by experiment. If so, this does not mean that our rationale
for constructing ρ –maximizing Iσ under given constraints– was wrong. Rather,
it means that important macroscopic observables or control parameters (such as
“hidden” constants of the motion, or further imperfections of Wolf’s die) have
been overlooked, and that the correct description of the macrostate requires ad-
ditional thermodynamic variables.

2.2 First law of thermodynamics

Changing the values of the thermodynamic variables alters the distribution ρ and
with it the associated

I max
σ ≡

Iσ(ρ) =

ln σ

iσ + ln Z +

h

λaga

a
X

.

.

(59)

(60)

By virtue of Eqs. (22) and (58) its inﬁnitesimal variation is given by

dI max

σ = d

ln σ

iσ +

h

λadga +

γbdhb

Xb

a
X
As the set of constants of the motion always contains the Hamiltonian its value for
the given system, the internal energy U, and the associated conjugate parameter,
which we denote by β, play a particularly important role. Depending on whether
the energy is given with certainty or on average, the pair (U, β) corresponds to a
pair (h, γ) or (g, λ). For all remaining variables one then deﬁnes new conjugate
parameters

la := λa/β , ma := γa/β
6Here we have assumed that there is no time-dependence of the a priori distribution σ.

(61)

13

such that in terms of these new parameters the energy diﬀerential reads

dU = β−1d(I max

σ − h

ln σ

iσ)

−

ladga −

mbdhb

.

a
X

Xb

A change in internal energy that is eﬀected solely by a variation of the pa-

rameters

ga}

{

or

hb

{

}

is deﬁned as work

δW :=

ladga −

Xb

−

a
X

mbdhb

;

some commonly used pairs (g, l) and (h, m) of thermodynamic variables are listed
in Table 2. If, on the other hand, these parameters are held ﬁxed (dga = dhb = 0)
then the internal energy can still change through the addition or subtraction of
heat

δQ :=

1
kβ

k d(I max

σ − h

ln σ

iσ)

.

Here we have introduced an arbitrary constant k. Provided we choose this con-
stant to be the Boltzmann constant

we can identify the temperature

and the entropy

to write δQ in the more familiar form

k = 1.381

10−23J/K ,

×

T :=

1
kβ

S := k (I max

σ − h

ln σ

iσ)

δQ = T dS .

S = k ln Z + k

λaga

.

a
X

dU = δQ + δW ,

The entropy is related to the other thermodynamic variables via Eq. (59), i.e.,7

The relation

which reﬂects nothing but energy conservation, is known as the ﬁrst law of ther-
modynamics.

7Even though the entropy, like the partition function, is related to measurable quantities
it is essentially an auxiliary concept and does not itself constitute a physical observable: In
quantum mechanics, for example, there is nothing like a Hermitian “entropy operator.”

14

(62)

(63)

(64)

(65)

(66)

(67)

(68)

(69)

(70)

(g, l)

(N,
(M,
(P,
(~p,
(~L,

µ)
B)
E)
~v)
~ω)

−
−
−
−
−

(h, m)
(V, p)

names
volume, pressure

−

(N,
µ) particle number, chemical potential
(B, M) magnetic induction, magnetization
(E, P )

electric ﬁeld, electric polarization
momentum, velocity
angular momentum, angular velocity

Table 2: Some commonly used pairs of thermodynamic variables. In cases where
B) and (B, M), the proper choice depends on the
two pairs are given, e. g., (M,
B) is adequate if the magnetization
speciﬁc situation: For example, the pair (M,
M is a constant of the motion whose value is given on average; while the pair
(B, M) should be used if there is an externally applied magnetic ﬁeld B which
plays the role of a control parameter.

−

−

2.3 Example: Ideal quantum gas

We consider a gas of non-interacting bosons or fermions. We suppose that the
total particle number is not given with certainty (but possibly on average, as in
the grand canonical ensemble) so the system must be described in Fock space. We
ˆGa}
whose expectation values are furnished
further suppose that the observables
as macroscopic data are all of the single-particle form

{

Gi

{

a}

are arbitrary (c-number) coeﬃcients and the

denote number
where the
of single-particle states.
operators pertaining to some orthonormal basis
Provided the a priori distribution σ is uniform, the best estimate for the macro-
state has the form

i
i}

{|

{

ˆNi}

with

{

}

For example, in the grand canonical ensemble (energy and total particle number
given on average) the parameters
are functions of the single-particle energies
ǫi

{
, the inverse temperature β and the chemical potential µ:

αi

}

ˆGa =

Gi
a

ˆNi

,

Xi

ˆρ =

exp

1
Z

αi ˆNi

!

 −

Xi

αi =

λaGi
a

.

a
X

αi = β(ǫi

µ)

.

−

15

(71)

(72)

(73)

(74)

The partition function

Z = tr exp

αi ˆNi

=

!

 −

Xi

conﬁgurations {N1,N2,...} Yi (cid:16)

X

(cid:17)

Ni

e−αi

(75)

factorizes, for we work in Fock space where we sum freely over each Ni:

Z =

Ni

e−αi

=:

Zi

.

Yi XNi (cid:16)

(cid:17)

Yi

The sum over Ni extends from 0 to the maximum value allowed by particle
statistics:

for bosons, 1 for fermions. Consequently, each factor Zi reads

∞

∓
the upper sign pertaining to bosons and the lower sign to fermions. This gives

(cid:16)

(cid:17)

Zi =

1

∓1

e−αi

,

ln Z =

ln

1

∓

(cid:16)

∓

Xi

e−αi

(cid:17)

and hence the average occupation

ni ≡ h

Niiρ =

−

∂
∂αi ln Z =

−1

eαi
(cid:16)

1

∓

(cid:17)

of any single-particle state i. Using the inverse relation

together with the speciﬁc realization of Eq. (69),

αi = ln(1

ni)

ln ni

±

−

S = k ln Z + k

αini

,

Xi

we ﬁnd for the entropy

S =

k

−

[ni ln ni ∓

(1

±

ni) ln(1

ni)]

.

±

Xi

2.4 Thermodynamic potentials

Like the partition function, thermodynamic potentials are auxiliary quantities
used to facilitate calculations. One example is the (generalized) grand potential

related to the internal energy U via

Ω(T, la, hb) :=

ln Z ,

1
β

−

Ω = U

T S +

laga

.

−

a
X

16

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

Its diﬀerential

dΩ =

SdT +

gadla

mbdhb

−

a
X

−

Xb

shows that S, ga and mb can be obtained from the grand potential by partial
diﬀerentiation; e.g.,

S =

∂Ω
∂T !la,hb

−  

,

where the subscript means that the partial derivative is to be taken at ﬁxed la, hb.
In addition to the grand potential there are many other thermodynamic poten-
tials: Their deﬁnition and properties are best summarized in a Born diagram (Fig.
1). In a given physical situation it is most convenient to work with that potential
which depends on the variables being controlled or measured in the experiment.
For example, if a chemical reaction takes place at constant temperature and pres-
sure (controlled variables T ,
mb}
), and the observables of interest are the
) then
particle numbers of the various reactants (measured variables
the reaction is most conveniently described by the free enthalpy G(T, Ni, p).

Ni}

ga}

=

=

{

}

{

{

{

p

When a large system is physically divided into several subsystems then in
these subsystems the thermodynamic variables generally take values that diﬀer
from those of the total system. In the special case of a homogeneous system all
variables of interest can be classiﬁed either as extensive –varying proportionally
to the volume of the respective subsystem– or intensive –remaining invariant
under the subdivision of the system. Examples for the former are the volume
itself, the internal energy or the number of particles; whereas amongst the latter
are the pressure, the temperature or the chemical potential.
In general, if a
thermodynamic variable is extensive then its conjugate is intensive, and vice
versa. If we assume that the temperature and the
are intensive, while the
hb

and the grand potential are extensive, then

la

}

{

{

}

and hence

Ωhom(T, la, τ hb) = τ

Ωhom(T, la, hb)

τ > 0

∀

·

−

Xb

Ωhom =

mbhb

.

This implies the Gibbs-Duhem relation

SdT

gadla

hbdmb = 0 .

−

a
X

−

Xb

For an ideal gas in the grand canonical ensemble, for instance, we have the
temperature T and the chemical potential
intensive, whereas the
volume

{
and the grand potential Ω are extensive; hence

{−

hb

la

=

=

µ

V

}

}

{

}

{

}

Ωi.gas(T, µ, V ) =

p(T, µ) V

.

(90)

(85)

(86)

(87)

(88)

(89)

−

17

Ξ

(=0)

g

Ω

G

F

χ2

χ1

H

l

U

T

S

{

{

T, l, h
}

S, g, m
}

. Their conjugates

Figure 1: Born diagram. Corners correspond to thermodynamic potentials: the
grand potential Ω, the free energy F , the internal energy U, the enthalpy H, the
free enthalpy G, the potential Ξ (which vanishes for a homogeneous system), as
well as two rarely used potentials χ1 and χ2. Sides of the cube correspond to
thermodynamic variables: T , S, g, l, h and m. Opposite sides are conjugate to
each other, and associated with each conjugate pair is a dotted “basis vector.”
Each corner is a function of the adjacent sides; e.g., the enthalpy H is a function
can be obtained from H by partial diﬀer-
of
entiation, the sign depending on whether the requested conjugate variable is at
) or tail (+) of a basis vector; e.g., T = +∂H/∂S. One can go from
the head (
one corner to the next by moving parallel or antiparallel to a basis vector, thereby
(i) changing variables such as to get the correct dependence of the new potential,
and (ii) adding (if moving parallel) or subtracting (if moving antiparallel) the
product of the conjugate variables that are associated with the basis vector. For
instance, in order to obtain the free enthalpy G from the enthalpy H one (i) uses
T = +∂H/∂S to solve for S(T, g, m), since the free enthalpy will be a function
; and then (ii) subtracts the product T S to get
S, g, m
of
}
G(T, g, m) = H(S(T, g, m), g, m)
T S(T, g, m). This procedure is known as a
Legendre transformation. Successive application allows one to calculate all ther-
modynamic potentials from the grand potential Ω and hence, ultimately, from
the partition function Z.

T, g, m
}

rather than

−

−

{

{

m

h

18

2.5 Correlations

Arbitrary expectation values
depend on the Lagrange multipliers
ters
ﬁxed, the expectation value

λa
{
. If the Lagrange multipliers vary inﬁnitesimally while the

A
iρ in the macrostate (54) or (56), respectively,
as well as –possibly– on other parame-
are held

hb

hb

{

{

}

}

}

h

A
iρ changes according to
h
A
.
δGa; A
iρ =

iρdλa

−

d

h

a h
X

A; B

iρ :=

h

Z

dπ ρ(π)A(π)∗B(π)

Here

;

h

iρ is the canonical correlation function with respect to the state ρ:

in the classical case or

iρ :=
in the quantum case, respectively. The observable δGa is deﬁned as

ˆρν ˆA† ˆρ1−ν ˆB
h

dν tr

A; B

0
Z

h

i

1

The correlation matrix

δGa := Ga − h

Gaiρ

.

Cab :=

δGa; δGbiρ =

h

−  

∂gb
∂λa !λ,h

∂2

=

∂λa∂λb ln Z

 

!λ,h

thus relates inﬁnitesimal variations of λ and g:

dgb =

dλaCab

,

dλa =

dgb(C −1)ba

.

(96)

−

a
X

−

Xb

The subscripts λ, h of the partial derivatives indicate that they must be taken
with all other
held ﬁxed. Returning to our example of the ideal
quantum gas, we immediately obtain from (79) the correlation of occupation
numbers

and all

λa

hb

{

}

}

{

δNi; δNjiρ =

h

−

∂nj
∂αi = δij ni(1

±

ni)

.

3 Linear Response

3.1 Liouvillian and Evolution

The dynamics of an expectation value

A
iρ is governed by the equation of motion

h

(91)

(92)

(93)

(94)

(95)

(97)

(98)

d

A
h
dt

iρ

=

i
L

A
iρ +

h

*

∂A
∂t +ρ

.

19

Here we have allowed for an explicit time-dependence of the observable A. Clas-
sically, the Liouvillian
takes the Poisson bracket with the Hamilton function
H(π),

L

in canonical coordinates π =
commutator with the Hamilton operator ˆH,

{

; whereas in the quantum case it takes the

∂

∂Qj −

∂H
∂Qj

∂
∂Pj !

=

i
L

∂H
∂Pj
Xj  
Qj, Pj}

= (i/¯h) [ ˆH,

i
L

.

]

∗

An observable A for which i
a state ρ for which
Liouvillian is Hermitian with respect to the canonical correlation function,

A + ∂A/∂t = 0 is called a constant of the motion;
ρ = 0 is called stationary. Only for a stationary ρ the

L

L

A;

B

iρ =

h

iρ ∀
is deﬁned as the solution of the diﬀerential equation

hL

L

A, B .

A; B

The evolver

U

∂
∂t U

(t0, t) = i

(t0, t)

U

L

(t0, t0) = 1. As long as the Liouvillian
with initial condition
time-dependent, the solution has the simple exponential form

U

L

is not explicitly

(t0, t) = exp[i(t

t0)

]

L

−

;

U

however, we shall not assume this in the following. The evolver determines –at
least formally– the evolution of expectation values via

iρ(t) =
A
Multiplication with a step function

h

(t0, t)A

iρ(t0)

.

hU

θ(t

t0) =

−

(

0 : t
t0
1 : t > t0

≤

yields the so-called causal evolver

U
(where ‘<’ symbolizes ‘t0 < t’) which satisﬁes another diﬀerential equation

−

·

U<(t0, t) :=

(t0, t)

θ(t

t0)

(106)

∂
∂t U<(t0, t) = i

U<(t0, t)

L

+ δ(t

t0)

.

−

If a (possibly time-dependent) perturbation is added to the Liouvillian,

(99)

(100)

(101)

(102)

(103)

(104)

(105)

(107)

(108)

(V ) :=

L

+

V

,

L

20

then the perturbed causal evolver
integral equation

U

(V )
< is related to the unperturbed

U< by an

(V )
< (t0, t) =

U

U<(t0, t) +

dt′

(V )
< (t0, t′) i
V

U

(t′)

U<(t′, t)

.

(109)

∞

−∞

Z

(V )
< (t0, t′) in the integrand
Iteration of this integral equation –re-expressing the
in terms of another sum of the form (109), and so on– yields an inﬁnite series, the
. Truncating this series after the term of order
terms being of increasing order in
n gives an approximation to the exact causal evolver in n-th order perturbation

U

V

V
theory.

3.2 Kubo formula

The Kubo formula describes the response of a system to weak time-dependent
external ﬁelds φα(t). Before t = 0 the external ﬁelds are zero and the system is
assumed to be in an initial equilibrium state

characterized by some set
of constants of the motion at zero ﬁeld (and
with the a priori distribution σ taken to be uniform). Then the external ﬁelds
are switched on:

Ga[0]
}

{

How does an arbitrary expectation value
perturbation? The general solution is

A
i

h

(t) evolve in response to this external

ρ(0) =

exp

1
Z

λaGa[0]

!

 −

a
X

φα(t) =

0
φα(t)

: t
0
≤
: t > 0

(

.

(t) =

A
i

h

hU

[φ]
< (0, t)A

,

i0

(110)

(111)

(112)

hi0 stands for the expectation value in the initial equilibrium state ρ(0). We
where
assume that the observable A does not depend explicitly on time or on the ﬁelds
φα(t). The Hamiltonian H[φ] and with it the Liouvillian
[φ], on the other hand,
generally do depend on the external ﬁelds. Provided the ﬁelds are suﬃciently
weak, the Liouvillian may be expanded linearly:

L

[φ(t)]

[0] +

L

≈ L

α
X

φ=0

[φ]
∂
L
∂φα (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

φα(t)

.

(113)

[0] is assumed to be not explicitly time-dependent;
The zero-ﬁeld Liouvillian
the linear correction to it generally is, and may be regarded as a time-dependent
(t). Application of ﬁrst order time-dependent perturbation the-
perturbation
V
U<.
ory then yields the evolver

(t) and the zero-ﬁeld evolver

[φ]
< in terms of

L

U

V

21

φα(t′)

.

(114)

Assuming for simplicity that

(t) =

A
i

h

∞

−∞

α Z
X

h

dt′

A
i0 = 0 we thus ﬁnd
∂
[φ]
L
∂φα (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

*

i

φ=0 U<(t′, t)A

+

0

With the help of the mathematical identity (prove it!)
i0λa

[φ]Ga[0]; B

i0 =

[φ]B

i
L

i
L

h

B

∀

a h
X

we can also write

(115)

(117)

(118)

(120)

(121)

h

∞

dt′

A
i

(t) =

∂
[φ]
L
∂φα (cid:12)
(cid:12)
(cid:12)
(cid:12)
In general, the constants of the motion depend explicitly on the external ﬁelds.
(cid:12)
They satisfy

U<(t′, t)A

φα(t′)

Ga[0];

α Z
X

(116)

a
X

λa

φ=0

−∞

*

+

i

.

0

[φ′]Ga[φ]

L
= 0 for φ′

[φ]Ga[φ] = 0

φ ,

∀

= φ. Together with the Leibniz rule this

yet generally
implies

L

which we use to obtain

Ga[0] =

[0]

−L

φ=0

,

∂Ga[φ]
∂φα (cid:12)
φ=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[φ]
∂
L
∂φα (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞

dt′

(t) =

A
i

h

−

−∞

α Z
X

a
X

λa

[0]

i
L

*

;

U<(t′, t)A

+

0

φ=0

φα(t′)

.

(119)

The right-hand side of this equation has the structure of a convolution, so in the
frequency representation we obtain an ordinary product

∂Ga[φ]
∂φα (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ω) =

A
i

h

α
X

α (ω)φα(ω)
χA

.

The coeﬃcient

χA

α (ω) =

∞

λa

0
Z

−

a
X

dt exp(iωt)

[0]

i
L
*

∂Ga[φ]
∂φα (cid:12)
φ=0
(cid:12)
(cid:12)
(cid:12)
(cid:12)

; A(t)

+

0

with A(t) :=
sion for the dynamical susceptibility is known as the Kubo formula.

U<(0, t)A is called the dynamical susceptibility. The above expres-

3.3 Example: Electrical conductivity

The conductivity σik(ω) determines the linear response of the current density ~j to
a (possibly time-dependent) homogeneous external electric ﬁeld ~E. We identify

φα

→

Ei

, A

jk

, χA

α (ω)

σik(ω)

.

→

→

(122)

22

6
6
Since a conductor is an open system with the number of electrons ﬁxed only
on average, its initial state must be described by a grand canonical ensemble:
Ga[φ]
.
{
}
In principle, the formula for the conductivity then contains both ∂H/∂Ei and
∂N/∂Ei; but the latter vanishes, and there remains only

, with associated Lagrange parameters

H[ ~E], N

} → {

} → {

βµ

λa

β,

−

{

}

∂H
∂Ei

=

eQi

,

−

(123)

with Qi denoting the i-th component of the position observable and e the electron
charge. We use the general formula (121) for the susceptibility to obtain

σik(ω) = eβ

dt exp(iωt)

[0]Qi; jk(t)

.

(124)

i
L

h

i0

∞

0

Z

The current density is related to the velocity V k by

jk = enV k

,

(125)

where n is the number density of electrons. Furthermore, i
the conductivity is proportional to the velocity-velocity correlation:

L

[0]Qi = V i. Hence

σik(ω) = e2nβ

dt exp(iωt)

V i; V k(t)

.

(126)

h

i0

∞

0
Z

This result is rather intuitive. In a dirty metal or semiconductor, for instance, the
electrons will often scatter oﬀ impurities, thereby changing their velocities. As
a result, the velocity-velocity correlation function will decay rapidly, leading to
a small conductivity. In a clean metal with fewer impurities, on the other hand,
the velocity-velocity correlation function will decay more slowly, giving rise to a
correspondingly larger conductivity.

23


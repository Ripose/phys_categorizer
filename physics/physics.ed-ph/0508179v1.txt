5
0
0
2
 
g
u
A
 
4
2
 
 
]
h
p
-
d
e
.
s
c
i
s
y
h
p
[
 
 
1
v
9
7
1
8
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Concepts of Renormalization in Physics

Jean Alexandre
Physics Department, King’s College
WC2R 2LS, London, UK
jean.alexandre@kcl.ac.uk

Abstract

A non technical introduction to the concept of renormalization is given, with an em-
phasis on the energy scale dependence in the description of a physical system. We ﬁrst
describe the idea of scale dependence in the study of a ferromagnetic phase transition, and
then show how similar ideas appear in Particle Physics. This short review is written for
non-particle physicists and/or students aiming at studying Particle Physics.

1 Introduction

Nature provides us with a huge amount of phenomena, which can be explained by diﬀerent
theories, depending on the scale of the physical processes. At the atomic level: Quantum
Mechanics is the most relevant theory; in every-day life: Newtonian Mechanics explains
the trajectories of rigid bodies; at the cosmological scale, General Relativity is necessary
to describe the evolution of the Universe.

Obviously, to each scale corresponds another set of parameters that help us describe
Physics. This is actually valid within a given theory and one does not need to change the
scale of observation so dramatically to see diﬀerent descriptions of the system: consider the
example of ﬂuid mechanics. The Reynolds number of a ﬂow is a characteristic dimensionless
quantity which helps deﬁne diﬀerent regimes. It is given by Re = UL/ν, where ν is the
viscosity of the ﬂuid, U and L are typical speed and length of the ﬂow respectively. Suppose
that U and ν are ﬁxed. For ﬂuids ﬂowing over short distances (Re << 1), viscosity eﬀects
dominate and inertia is negligible. Surface tension can also play a role. For ﬂuids ﬂowing
over large distances (Re >> 1), viscosity can be neglected and non linearities dominate the
system, leading to turbulence and instabilities. Therefore we see that, depending on the
typical scale over which the ﬂuid is observed, diﬀerent parameters have to be considered
to describe its ﬂow.

In a very general manner, ”renormalization” deals with the evolution of the description
of a system with the scale of observation. Renormalization was introduced as a tool to
predict physical properties in phase transitions, as will be described in this article, and
Kenneth Wilson was given for this the Nobel Prize in 1982 (seminal papers of his are [1]).

1

Renormalization also happens to be necessary to avoid mathematical inconsistencies when
computing physical quantities in Particle Physics. Historically though, renormalization ap-
peared in Particle Physics independently of its use for the description of phase transitions,
but it was then understood that both procedures actually have the same physical content,
and a unifying description of renormalization was set up. This introductory review aims
at describing these ideas.

In section 2, we start by an intuitive description of the technique which leads to the
idea of scale dependence, in a ferromagnetic system. We explain here how to obtain the
temperature dependence of physical quantities, near a phase transition. The appearance
of scale dependence in Particle Physics is explained in section 3. We describe here how
the necessity to introduce a regularization for the computation of quantum corrections to
physical processes leads to a scale-dependent theory. Section 4 comes back to the ideas
developed for a ferromagnetic system, applied to Particle Physics, and the connection
between quantum ﬂuctuations and thermal ﬂuctuations is emphasized.

For non-physicist readers, a few concepts used in the text are deﬁned in the appendix.

2 Renormalization and phase transitions

The use of scale dependence in the description of a system proved to be very fruitful in
predicting the behaviour of physical quantities, as magnetic susceptibility or heat capacity,
in the vicinity of a 2nd order phase transitions (see def.1 in the appendix). These ideas are
explained in many places and good introductory books are for example [2, 3].

Consider a ferromagnetic sample at temperature T . This system is made out of spins
located at the sites of a lattice, and to each spin corresponds a magnetic moment such
that spins are related by a magnetic interaction. When T is larger than some critical
temperature TC, the magnetization of the sample is zero: thermal ﬂuctuations are too
important and destroy the magnetic order. When T is less than TC, the spin interactions
dominate the thermal ﬂuctuations and spins are ordered along a given direction (which
depends on the past of the sample).

If we look at the system for T well above TC, each spin interacts mainly with its near-
est neighbours: the correlation length (the typical distance over which spins interact) is
of the order of few lattice spacings. But as T approaches TC, the magnetic interactions
are not dominated by thermal ﬂuctuations anymore and play a more important role: the
correlation length grows. In this situation, a fair description of the system must involve
an increasing number of degrees of freedom and mean ﬁeld approximations or naive per-
turbation expansions become useless.

What saves us here is the following assumption (the ”scaling hypothesis”): the macro-
scopic behaviour of the system depends on physical properties that occur at the scale of the
correlation length ξ, and smaller scales should not be responsible for it. The idea of what
is called ”renormalization group transformation” (RGT) is then to get rid of these smaller
details by deﬁning a new theory with lattice spacing ξ instead of the distance between
the original lattice sites, so as to be left with a description in terms of relevant degrees of

2

freedom only. An RGT thus has the eﬀect to decrease the resolution in the observation of
the system.

The procedure to achieve this is the following: starting from a system with lattice
spacing a, one deﬁnes blocks of spins of size sa, where s > 1 is the dimensionless scale
factor (see ﬁg.1). To each block of spin corresponds a block variable, i.e. a new spin
variable depending on the conﬁguration of the original spins inside the block. There are
several ways to deﬁne this block variable and this step corresponds to the so called blocking
procedure or ”coarse graining”: the block variables describe the system as if the latter was
zoomed out. The new theory, deﬁned on the lattice with spacing sa, is obtained by noting
that the partition function (see def.3) of the system should be invariant under this coarse
graining. To see this more precisely, let H be the Hamiltonian (see def.2) of the original
system, deﬁned on the original lattice with spacing a, and Hs the Hamiltonian of the
blocked system with lattice spacing sa. The invariance of the partition function Z gives

Z =

exp

−

(cid:18)

H
kBT (cid:19)

=

exp

−

(cid:18)

Hs
kBT (cid:19)

,

Xblocks
spins is the sum over the original spins and

Xspins

where
on the new lattice. Eq.(1) helps us deﬁne the block Hamiltonian Hs as;

P

P

blocks is the sum over the block spins

exp

−

(cid:18)

Hs
kBT (cid:19)

=

˜

X

exp

−

(cid:18)

H
kBT (cid:19)

,

where ˜
the block spins unchanged.
P

is the constrained sum over the original spins that leave a given conﬁguration of

Therefore one has a way to deﬁne the Hamiltonian Hs of the coarse grained system,
with lattice spacing sa. For each value of the scale factor s, Hs is deﬁned by a set of
parameters {µs} and the evolution of {µs} with s constitutes the so called renormalization
ﬂows. These ﬂows depend on the theory that is deﬁned on the original lattice, as well as
on the blocking procedure. But the essential physical content lies in the behaviour of these
ﬂows at large distances: the diﬀerent theories described by Hs, with running s, have the
same large-scale behaviours, deﬁned by the relevant parameters. Increasing s removes the
irrelevant microscopic details which do not inﬂuence the macroscopic physics.

”Relevant” and ”irrelevant” have actually a precise meaning. One ﬁrst has to deﬁne
In
a ﬁxed point of the RGT: this is a theory invariant under the blocking procedure.
principle, a ﬁxed point can describe an interacting theory, but usually it describes a free
theory. Once this ﬁxed point {µ⋆} is deﬁned, one can linearize the renormalization group
transformations around {µ⋆} and deﬁne in the space of parameter (see ﬁg.2):

• Relevant directions: along which the theory goes away from the ﬁxed point. The
corresponding dimensionless parameters become very large when s → ∞ and thus
dominate the theory;

• Irrelevant directions: along which the theory approaches the ﬁxed point. The corre-
sponding dimensionless parameters become very small when s → ∞ and thus become
negligible:

(1)

(2)

3

Figure 1: Example of a blocking procedure in a planar ferromagnetic system. The block
spin variable σ can for example be obtained by adding the values ±1 of the nine original
spins inside the block: σ = 1 if the sum is positive and σ = −1 if the sum is negative.

One then deﬁnes the notion of ”universality class” as a group of microscopic theories
having the same macroscopic behaviour. Theories belonging to the same universality class
diﬀer by irrelevant parameters, and ﬂow towards the same large-distance physics as s → ∞.
How can one ﬁnally get physical quantities out of this renormalization procedure?
Such quantities, as magnetic susceptibility or heat capacity, are obtained via correlation
functions (see def.4). Let G(a) be a correlation function deﬁned on the original lattice,
homogeneous to a length to the power [G]. After RGTs it can be shown that this correlation
function reads, when s → ∞

G(sa) ≃ s[G]+ηG(a),

(3)

where η is called the anomalous dimension, arising from thermal ﬂuctuations. By choosing
s = TC/(T − TC), which goes to inﬁnity when the temperature reaches the critical tem-
perature, one can see that this very anomalous dimension can actually be identiﬁed with
a critical exponent (see def.5) of the theory, whose prediction is therefore possible with
renormalization group methods (the power [G] is a natural consequence of the rescaling).

This section showed how scale dependence comes into account in the description of a
system on a lattice. The ideas introduced here go beyond this critical phenomenon and
the next section shows how they arise in Particle Physics.

4

fixed 
point

relevant

irrelevant

irrelevant

microscopic
theories

Figure 2: Relevant and irrelevant directions in the parameter space. The renormalization
ﬂows are indicated by the blue arrows. Two microscopic theories with diﬀerent relevant
parameters lead to diﬀerent large-scale physics, whereas the latter are the same if the mi-
croscopic description diﬀer by irrelevant parameters.

3 Renormalization in Particle Physics - First part

When one considers elementary processes in Particle Physics, the usual way to proceed is
to start form a classical description, based on equations of motion, and then look for the
quantum corrections. These quantum corrections involve the exchange of elementary par-
ticles, or the creation/annihilation of pairs of particles. We consider here an example taken
from Quantum Electrodynamics (QED) which captures the essential features. Among the
huge bibliography explaining theses eﬀects, the reader can look at a good description for
example in [4].

In ﬁg.3, a photon propagating creates a pair electron/positron, which ﬁnally annihilates
to generate another photon. This process is possible as a consequence of two fundamental
properties of relativistic quantum theory:

• Relativistic aspect:

the equivalence mass/energy enables the massless photon to
”split” into massive electron and positron. For this event to start playing a role,
the energy of the photon should be at least twice the mass energy of the electron,
i.e. roughly 106 eV;

• Quantum aspect: the uncertainty energy/time allows the pair electron/positron to

5

exist, during a time proportional to the inverse of their mass. This time is of the
order of 10−21 s.

γ

γ

Figure 3: Creation/annihilation of a virtual pair electron/positron. Being antiparticles,
the electron and positron have same mass, but opposite charge (and thus electric charge is
conserved at a vertex photon-electron-positron).

This electron/positron pair is said virtual since it is probabilistic and exists for a very
short time only. Nevertheless, this quantum eﬀect inﬂuences the strength of the interaction
between electric charges, and therefore contributes to the value of the electron’s charge that
is measured.

The computation of the quantum correction to the photon propagation shown in ﬁg.3
involves an integration over all the Fourier modes of the electron/positron. This integration
happens to be divergent if it is done in a straightforward manner. The origin of this
divergence is the point-like structure of the electron/positron. To avoid this divergence,
one has to regularize the integration, and several techniques exist. For example:

• A naive regularization would be to put a cut oﬀ in Fourier space, which is equivalent
to give a radius to the electron/positron. But this procedure does not respect gauge
invariance, which is unacceptable since this would lead to the violation of electric
charge conservation. Therefore this regularization method cannot be used in QED;

• The Pauli-Villars method, which consists in adding massive particles to the system,
with couplings that are not constrained by physical requirements. A good choice of
these masses and couplings can then cancel the divergences and the additional, non
physical, particles decouple from the dynamics of the system in the limit where their
masses go to inﬁnity. This method has the drawback to be cumbersome from the
computational point of view.

• The dimensional regularization consists in computing the would-be divergent integral
in 4 − ε dimensions, where ε << 1 and ”4” stands for 3 space and 1 time dimen-
sions (such integrals can be deﬁned mathematically). The would-be divergence then
appears as a pole in ε and we are left with a regularization dependent electron’s
charge. The reader interested in detailed technical aspects can go to [5]. Note that
this space time dimension 4 − ε has nothing to do with additional dimensions nec-
essary for the study of String Theories (the latter extra dimensions are suppose to
be real), but is just a mathematical trick to give a meaning to the quantum theory.

+

e

−

e

6

This regularization is technically more advanced, but is also the simplest one from
the computational point of view.

Whatever regularization method is used, the essential point is that it necessarily involves
the introduction of an energy or mass scale. We recall here that length and energy scales
are related by Heisenberg inequalities which imply that small distances correspond to high
energies (called ultraviolet - UV - in analogy with the optical spectrum), and large distances
correspond to low energies (called infrared - IR). In the standards of particle accelerators,
a typical ”low energy” is of the order of the electron’s mass energy (≃ 5 × 105 eV) and a
typical ”high energy” is of the order of the Z boson’s mass energy (≃ 9 × 1010 eV).

Let us continue the discussion with the example of the dimensional regularization.
The next step is to give a meaning to the electron’s charge q depending on ε. This is
actually possible when looking at the ﬂow of q with the energy scale E introduced by the
regularization: the derivative dq/dE happens to be ﬁnite in the limit where ε → 0.

What we ﬁnally obtain is the ﬂow of the electron’s charge with an energy scale that was
put by hand. We need then an initial condition chosen at some value of the energy scale,
what is done in the following way: the value of the electron’s charge in the limit of low
energies is the one measured in the laboratory, i.e. in the deep IR region where E = mc2
(m=electron’s mass, c=speed of light) which is the minimum value for the electron’s energy.
This energy dependence of the electron’s charge is not a formal result, but a real
phenomenon that is observed in particle accelerators: as the energy increases in a collision
of two electrically charged particles, the strength of the interaction (which deﬁnes the
electron’s charge) increases. The physical interpretation is the following. The quantum
vacuum being full of these virtual electron/positron pairs, an electron put in this vacuum
can polarize it, as shown on ﬁg.4. As a consequence, as one goes away from the electron, one
observes a decreasing charge since the electron is screened by the virtual electric dipoles.
The ”bare” charge is the one that would be measured at a distance of the order of the
classical electron radius (≃ 10−15m). The physical charge, measured in the laboratory, is
”dressed” by quantum corrections.

The Landau pole: In the speciﬁc example of the electron’s charge running with an
energy scale, one actually meets a new problem: this charge, increasing with the energy
of the physical process, seems to diverge at a speciﬁc energy, called the ”Landau pole”. It
is true that QED is not valid anymore at the scale of the Landau pole where Quantum
Gravity has to be taken into account, but this argument does not explain why there seems
to be a problem in the mathematical structure of the theory. It has been argued thought
[7, 8, 9] that this Landau pole is an artifact coming from the oversimpliﬁcation of the
renormalization equations, and that it actually disappears when one takes into account
the evolution of the fermion mass with the occurrence of quantum ﬂuctuations.

Renormalizability: As was described, the original parameters that deﬁne the bare theory
become scale dependent after regularization and have diﬀerent values in the IR and in the
UV. A theory is said to be renormalizable if the number of parameters that have to be
In a non-renormalizable theory, one
redeﬁned in this way is ﬁnite (QED for example).

7

−
+

−

+

+

−

+

−

−
+

−
+

−

+

+

−

+

−

+

−

−
+

−

e

+
−

Figure 4: An electron polarizes the quantum vacuum made out of virtual electric dipoles
electron/positron. As a consequence, the eﬀective charge that is seen depends on the dis-
tance of observation.

would in principle need to redeﬁne an inﬁnite set of parameters to give a meaning to the
theory. But such a theory can be seen as an eﬀective theory, valid up to a certain energy
scale only, and thus is not useless. Many of these non renormalizable theories are actually
of great interest.

To conclude this section, one can stress that the essence of the renormalization proce-
dure lies in the fact that it is possible to turn a would-be divergence into an energy scale
dependence.

4 Renormalization in Particle Physics - Second part

We are coming back in this section to the ideas developed in section 2. As we have already
discussed, the number of interacting degrees of freedom located at the sites of a lattice with
spacing a becomes huge near a phase transition, since the correlation length ξ increases.
In the limit of a diverging correlation length, the ratio a/ξ goes to zero and the degrees
of freedom tend to form a continuous set, what leads to a ”ﬁeld theory”. In this situation
the discreet index labeling a lattice site becomes the continuous coordinate of a point in
space time. This is what happens in Quantum Field Theory, where the degrees of freedom
are the values of a ﬁeld at each point x of space time. This ﬁeld is a physical entity that
can create or annihilate a particle at the point x.

But independently of the discrete/continuous feature, one has a similar situation as the

8

one in critical phenomena, and thus one can develop the same ideas related to renormal-
ization. Apparently, the procedure provided by the renormalization group transformations
is very diﬀerent from the procedure consisting in regulating divergent integrals, but these
actually lead to the same renormalization ﬂows, as is now discussed. For a review on these
methods applied to Quantum Field theory, see [10] and references therein.

The starting point is the path integral (see def.7) deﬁning the quantum theory (Richard

Feynman, Nobel Prize 1965), the analogue of the partition function:

Z =

D[φ] exp

Z

i
~ S[φ]

,

(cid:19)

(cid:18)

where S is the classical action (see def.6) describing the theory with degrees of freedom φ(x).
D[φ] stands for the integration over all the possible conﬁgurations of φ, just
The symbol
as the partition function involves a summation over all the microscopic states of the system.
~, the elementary quantum action, plays the role of kBT :
it is responsible for quantum
ﬂuctuations represented here by the oscillating integrand (the complex exponential).

R

To proceed with the RGTs as these were deﬁned in section 2, we write at each point of
space time φ(x) = Φ(x) + ˜φ(x) where Φ contains the IR degrees of freedom and ˜φ contains
the UV degrees of freedom. An easy way to implement this is to go to Fourier space and
deﬁne a momentum scale k such that Φ(p) 6= 0 if |p| ≤ k and ˜φ(p) 6= 0 if |p| > k, where p
is the momentum, coordinate in Fourier space. With this decomposition, the original ﬁeld
φ has the following Fourier components:

and the path integral can be written

φ(p) = Φ(p)
φ(p) = ˜φ(p)

if |p| ≤ k
if |p| > k,

Z =

D[Φ]

Z

Z

D[ ˜φ] exp

i
~S[Φ + ˜φ]
(cid:19)

,

(cid:18)

D[ ˜φ] stands for the summation over the UV degrees of freedom and

D[Φ] the
where
summation over the IR degrees of freedom. This last equation deﬁnes the action Sk of the
system observed at length scales ≥ 1/k:

R

R

i
~ Sk[Φ]

exp

(cid:18)

=

(cid:19)

Z

D[ ˜φ] exp

i
~ S[Φ + ˜φ]
(cid:19)

.

(cid:18)

Note the similarity with the equation (2) which deﬁnes the Hamiltonian Hs. Sk is called the
”running action” and contains parameters which take into account all the quantum eﬀects
that occur at length scales smaller than 1/k. For very large values of k (large compared
to another momentum scale of the problem) Sk coincides with the classical action, and as
k → 0, Sk tends to the full ”eﬀective action”, containing all the quantum ﬂuctuations in
the system. The eﬀective action is the quantity that is looked for, and RGTs provide us
with an algorithmic way to compute it.

9

(4)

(5)

(6)

(7)

The problem is that the integration (7) is not always easy and can only be done per-
turbatively in most of the interesting cases. There is though an interesting point: starting
from the cut oﬀ k, an inﬁnitesimal RGT from Sk to Sk−∆k leads to an exact equation in
the limit where ∆k → 0. This is the important Wegner-Houghton equation [11], which
opened the way to the so-called Exact Renormalization Group Equations studies, a whole
area of research in Particle Physics.

The above procedure has a problem though: it is based on the use of a ”sharp cut oﬀ”

to separate the IR and UV degrees of freedom, which leads to two problems:

• If the IR ﬁeld Φ is not a constant conﬁguration, the Wegner-Houghton equation leads
to singularities in the limit ∆k → 0, as a consequence of the non-diﬀerentiability of
the sharp cut oﬀ;

• A sharp cut oﬀ is not consistent with gauge invariance (see def.8): if a gauge symme-
try is present in the degrees of freedom φ, the classiﬁcation IR/UV ﬁeld is not valid
anymore after a gauge transformation.

Another approach, which avoids non-diﬀerentiability of the sharp cut oﬀ, is to intro-
duce a ”smooth cut oﬀ”, providing instead a progressive elimination of the UV degrees of
freedom, as is done with the Polchinski equation [12]. The choice of this smooth cut oﬀ
is not unique, but it has been argued that the eﬀective action obtained when k → 0 is
independent of this choice (for a review, see [13] and references therein). The details of this
technique will not be discussed here, but the essential point is that the renormalization
ﬂows that are obtained are consistent with those discussed in section 3. One can under-
stand this in an intuitive way: the would-be divergences in Particle Physics occur at high
energies, or momenta, whereas the physical quantities are obtained in the IR, when the
energy scale decreases and the parameters get dressed by the quantum corrections. This is
the same procedure leading to the eﬀective description that is obtained by coarse graining
a system. Therefore it is expected that both procedures have actually the same physical
content.

An alternative point of view: There is another way to describe the generation of quan-
tum eﬀects in connection with the concept of scale dependence. A detailed description
of this procedure is given in [14]. The idea is to start from a classical theory contain-
ing a very large mass scale, such that quantum ﬂuctuations are frozen and thus can be
neglected compared to the classical description of the system. As this mass decreases,
quantum ﬂuctuations progressively appear in the system and the parameters tend to their
physical values. The interesting point is that it is possible to describe the appearance of
quantum ﬂuctuations by an exact equation, as the Wegner-Houghton or the Polchinski
equation. The advantage though is that this scheme is independent of any cut oﬀ proce-
dure, since it does not deal with a classiﬁcation of the degrees of freedom in terms of their
Fourier modes, but in terms of their quantum ﬂuctuations’ amplitude. It is consistent with
gauge invariance and reproduces the well known renormalization ﬂows that are obtained
by regularizing would-be divergences.

10

An important achievement provided by these diﬀerent methods is the ressumation of
quantum corrections of diﬀerent orders in ~. The method described in section 3, dealing
with ﬂows obtained after regularizing would-be divergences, is valid order by order in ~.
The renormalized quantities indeed have to be deﬁned at every order in the perturbative
expansion in ~. Instead, the exact renormalization methods are not perturbative and take
into account all the orders in ~. It should not be forgotten though, that in order to solve
these exact renormalization equations, one needs to make assumptions on the functional
dependence of the running action (degrees of freedom dependence). These assumptions
are usually based on what is called the gradient expansion, an expansion of the action in
powers of the derivatives of the ﬁeld. This approximation is valid in the IR since it assumes
low momenta of the physical processes. There are other assumptions that can be made,
and the relevance of each of these approximations depends on the system that is studied.

5 Conclusion

Renormalization provides a common framework to critical phenomena and Particle Physics,
where scale dependence plays an important role in describing Physics in a consistent way.
Renormalization ﬂows of parameters deﬁning a system relate microscopic details to macro-
scopic behaviours. In this context the IR theory gives an eﬀective description taking into
account the UV dynamics dressed by quantum corrections.

Note that the concept of renormalization is in principle independent of would-be diver-
gences. It is the presence of these divergences which enforces the use of renormalization.
The toy model QED in 2 space dimensions is an example where no divergences occur but
the ﬂow of the parameters with the amplitude of the quantum corrections can be studied
[15]. In such a theory, physical quantities can be expressed in terms of the bare as well as
the dressed parameters.

Let us make here a comment on supersymmetric theories, which have been introduced
in order to cancel dominant divergences. In these models, each bosonic degree of freedom
has a fermionic partner and vice versa. This feature has the eﬀect to cancel some of the
quantum corrections and, as a result, some of the bare parameters do not get dressed after
quantization. Supersymmetric particles, though, have not been observed experimentally
yet, but might be conﬁrmed by the next generation of particle accelerators. An old but
classic and pedagogical review of supersymmetry is given in [16].

Finally, one can consider that every theory is an eﬀective theory, resulting from the
elimination of processes occurring at higher energies, or at smaller lengths. The ultimate
theory, from which everything is then generated is called the ”Theory of Everything” and
is certainly not ready to be found... But the essential point is that, looking for this Theory
of Everything, one arrives at many exciting and challenging achievements.

11

Appendix

~ ≃ 10−34 m2 kg s−1

1 eV≃ 1.6 × 10−19 J
def.1 2nd order phase transition: For such a transition, ﬁrst derivatives of the Gibbs free
energy are contiunous, as the entropy or volume per unit mole, such that there is no latent
heat and the two phases do not coexist. The discontinuous physical quantities are given by
second derivatives of the Gibbs free energy, as the heat capacity or the thermal expansivity.

kB ≃ 1.4 × 10−23 m2 kg s−2 K−1

def.2 Hamiltonian: Function of the degrees of freedom, whose values are the energies of
the system. In a quantum description, degrees of freedom are replaced by operators, such
that the Hamiltonian is an operator, whose eigen values are the energies of the system.

def.3 Partition function: Sum over the microscopic states of a system of the Boltzmann
factors exp(−H/kBT ), where H=Hamiltonian of a given conﬁguration, kB=Boltzmann
constant and T =temperature. The computation of the partition function, as a function
of T , leads to the complete knowledge of the physical properties of the system in contact
with a heat source at temperature T .

def.4 Correlation function: Thermal (or quantum) ﬂuctuations at diﬀerent points of a
system are correlated, and correlation functions measure how these correlations depend on
the distance between the points where ﬂuctuations are looked at. The knowledge of these
correlation functions leads to the prediction of physical properties of the system.
def.5 Critical exponent: Near a 2nd order phase transition, several physical quantities
diverge as the temperature T reaches the critical temperature TC. This divergence can be
expressed as a power law of the form 1/tα, where t = (T /TC − 1) → 0 and α is the critical
exponent.

def.6 Action: A classical system follows a trajectory which is found by minimizing a
function of its degrees of freedom. This function is the action and is homogeneous to
Energy×Time. In the case of ﬁeld theory, S is a ”functional” = a function of a continuous
set of variables.

def.7 Path integral: A quantum system does not follow a trajectory: its degrees of freedom
can take any values, due to quantum ﬂuctuations. The quantum states of the system are
randomly distributed around the would-be classical trajectory, and the path integral is
the sum over every ”quantum trajectory” (not necessarily diﬀerentiable), each of which
is attributed the complex weight exp(iS/~), where S=action corresponding to a given
quantum trajectory and ~=Plank constant/2π. The computation of the path integral, as
a function of the source which generates a (classical) state of the system, leads to the
complete knowledge of the latter.

def.8 Gauge invariance: The electromagnetic ﬁeld can be expressed in terms of potentials,
which are not unique: speciﬁc redeﬁnitions of the potentials should let the electromagnetic
ﬁeld unchanged, which is called gauge invariance. The quantization of the electromagnetic
ﬁeld is based on these potentials, such that gauge invariance must be preserved throughout
the quantization procedure, for the physical quantities to be gauge invariant.

12

References

[1] K. Wilson, ”Renormalization group and critical phenomena 1: Renormalization group

and the Kadanoﬀ scaling picture”, Phys.Rev.B4 (1971): 3174.
K. Wilson, ”Renormalization group and critical phenomena 2: Phase space cell anal-
ysis of critical behaviour”, Phys.Rev.B4 (1971): 3184.

[2] D. Amit, ”Field Theory, the Renormalization Group and Critical Phenomena”, Mc

[3] M. Le Bellac, ”Quantum and Statistical Field Theory”, Oxford University Press

Graw Hill (1982).

(1992).

[4] K. Gotfried, V.F. Weisskopf, ”Concepts of Particle Physics”, Vol.1 (basic and quali-

tative concepts) and Vol.2 (more technical), Oxford University Press (1986).

[5] J. Collins, ”Renormalization”, Cambridge University Press (1984).

[6] L. Brown, ”Renormalization, from Landau to Lorentz”, Springer-Verlag (1993).

[7] M. Gockeler, R.Horsley, V.Linke, P.Rakow, G.Schierholz, H.Stuben, ”Is there a Lan-

dau pole problem in QED?”, Phys.Rev.Lett.80 (1998): 4119.

[8] J. Alexandre, J. Polonyi, K. Sailer, ”Functional Callan-Symanzik equations for QED”,

Phys.Lett.B531 (2002): 316;

[9] H. Gies, J. Jaeckel, ”Renormalization ﬂow of QED”, Phys.Rev.Lett.93 (2004): 110405.

[10] J. Polonyi, ”Lectures on the functional renormalization group method”, Central

Eur.J.Phys.1 (2004):1.

Phys.Rev.A8 (1973): 401.

269.

[11] F.J. Wegner, A. Houghton, ”Renormalization group equation for critical phenomena”,

[12] J. Polchinski, ”Renormalization and eﬀective Lagrangians”, Nucl.Phys.B231 (1984):

[13] J. Berges, N. Tetradis, C. Wetterich, ”Nonperturbative renormalization ﬂow in quan-

tum ﬁeld theory and statistical physics”, Phys.Rep.363 (2002): 223.

[14] J. Polonyi, K. Sailer, ”Renormalization group in the internal space”, Phys.Rev.D71

(2005): 025010.

[15] T. Appelquist, M. Bowick, D. Karabali, L.C.R. Wijewardhana, ”Spontaneous chiral

symmetry breaking in three dimensional QED”, Phys.Rev.D33 (1986): 3704;
J. Alexandre, ”An alternative approach to dynamical mass generation in QED3”,
Ann.Phys.312 (2004): 273.

[16] M. Sohnius, ”Introducing supersymmetry”, Phys.Rep.128 (1985): 39.

13


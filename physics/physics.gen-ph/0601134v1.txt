Computational Improvements to Matrix Operations

Gordon Chalmers

e-mail: gordon@quartz.shango.com

Abstract

An alternative to the matrix inverse procedure is presented. Given a bit register
which is arbitrarily large, the matrix inverse to an arbitrarily large matrix can be
peformed in O(N 2) operations, and to matrix multiplication on a vector in O(N).
This is in contrast to the usual O(N 3) and O(N 2). A ﬁnite size bit register can lead
to speeds up of an order of magnitude in large matrices such as 500 × 500. The FFT
can be improved from O(N ln N) to O(N) steps, or even fewer steps in a modiﬁed
butterﬂy conﬁguration.

6
0
0
2
 
n
a
J
 
8
1
 
 
]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[
 
 
1
v
4
3
1
1
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The matrix inverse is the backbone to the modern STAP process; this has to be
performed every time a set of data is trained in the ﬁeld of view. The complexity
is one of the primary origins of the cost to the large computing required to process
the data. Any improvement in the complexity to performing the matrix inverse is a
desirable.

Typically the matrix inverse to MN ×N is performed in O(N 3) operations. There
are several variants, including the LU and QR variants. The LU diagonalization is
twice as fast as the (Guass) QR form [1]. The LU factorization requires splitting the
matrix into the product of upper and lower diagonal matrices M = LU. The inverse
is performed by inverting the respective components. The QR form requires splitting
the matrix M into an orthogonal component Q times its projection R.

There is a simpliﬁcation of the matrix inverse by grouping the entries of the matrix
Mij into larger numbers. For example, the ﬁrst row of the matrix has elements N1, N2,
. . .. A larger number can be built of these entries by placing the digits into one number
N1N2 . . .. For the computing purposes a zero number N0 with as many digits as the
entries Mij is required, and the rows are grouped into the number N1N0N2N0 . . .. The
rows of the matrix are now used as a single number in the diagonalization procedure
in the LU factorization.

For example, the zeroing out of the matrice’s ﬁrst column requires using the ﬁrst
row; a number bi1 is used to multiply M1j so that Mi1 = −bi1Mi1/M11. This number
multiplies the entire row of the matrix M1j and is added to the ith row. In doing so,
a set of zeroes is produced in the ﬁrst column of the matrix M; the numbers bi1 are
placed in the lower diagonal factor matrix of L. The procedure is iterated using the
diagonal elements Mjj to construct upper diagonal and lower matrices L and U. The
computational cost of one of the multiplications is N 2 due to the N elements in the
row and the multiplications and additions to the N elements in the column.

In using the larger number the N 2 operations to create a zero column can be
reduced to N operations. This requires the zero number N0 to have a suﬃcient
number of digits so that

b(N1N0N2N0 . . .) = (bN1)N0(bN2)N0 . . .

(1)

(2)

(N1N0N2N0 . . .) + (M1M0M2M0 . . .) = (N1 + M1)N0(N2 + M2)N0 . . . ,

as one number. The bit register in the processor has to be able to handle these two
operations, multiplication by a scalar and addition. The N operations in the bit

2

register to treat the multiplication and addition of the original row has been reduced
to one multiplication and one addition. The numbers b which multiply the larger
numbers N1N0N2N0 . . . are collected into the lower diagonal matrix L.

A separate matrix is required to discern if the subtraction of a positive number

to this number is negative or positive. For example,

(N1N0N2N0 . . .) − (M1M0M2M0 . . .)

(3)

could have negative entries Ni − Mi but the absolute value is used in the composition
of the number N − M. The subtraction process doesnt work well in the procedure,
and the numbers are separated into Ni − Mi independently.

The processing of using the larger numbers instead of the smaller numbers is
that typical processes such as the matrix inverse and the FFT can be reduced in
complexity from O(N 3) and N ln N to O(N 2) and N.

STAP Example

The use of spacetime adaptive processing requires the training of data using a
covariance matrix. This matrix is canonically symmetric in the acquisition of data,
satisfying the multiplicative product X = xixj. The inverse of the covariance matrix
is unwieldy, being performed in O(N 3) steps, but must be performed in conventional
STAP processes and signal location.

The product xixj represents a probability distribution, with positive entries. An
alternative matrix satisiﬁes Xij = −Xij with positive entries along the diagonal, is
far more convenient in the matrix inverse procedure. The inverse of the latter matrix
can be performed theoretically in O(N 2) steps. The limitation is set by the number
of bits in the bit register; the O(N 2) arises from an arbitrarily large bit register.

Consider the LU reduction of the alternative covariance matrix ˜X. The process
of adding the rows to null the lower left triangular portion of U requires only adding
the numbers M1/N1 ∗ (N1N0N2N0 . . .) to (M1M0M2M0 . . .). For example the second
row modeled by the single number (M1M0M2M0 . . .) has a negative entry for M1 and
positive entries Mj. The addition of M1/N1 ∗ Nj to the Mj occurs in one operation
theoretically due to the size of the individual numbers. The M1/N1 is stored in the
lower left triangular matrix L in the process of the LU factorization. The operation
is repeated to ﬁrst nullify the left column of U (except the diagonal component), and
then the process is repeated for the other columns. As there are N 2 components in

3

the matrix the LU factorization of ˜X requires N 2 steps; this is considerably faster
than N 3 when N is of the order of a thousand or more.

The question is whether data can be trained with the alternate to the covariance
matrix. The ˜X contains the same information but with minus signs placed to partially
antisymmetrize. It appears clear by the conventional use of stap in locating signals,
and in eliminating noise, that this should be possible.

Matrix Multiplication

The same use of the bit register and organizing the rows of the matrix in terms
of whole numbers can be used to simplify matrix multiplication. Usual multiplication
of a matrix by a vector requires O(N 2) steps. This can be reduced to O(N) with a
reordering of the matrix and vector information.

Consider all positive entries in the matrix M and all positive entries in the vector
v. The vector consists of one number (v1v0v2v0 . . .), and the columns of the matrix
consist of individual numbers ˜Mj = (M1M0M2M0 . . .). The multiplication of one
column by the vector element is accomplished in one step: vi ∗ ˜Mj, with the element
of the vector used. This results in vi ˜Mj = (viM1M0viM2M0 . . .. The total matrix
multiplication is then accomplished by adding the previous multiplications: P vi ˜Mj.
The vector resultant from the matrix multiplications are stored in the decomposition
of P vi ˜Mj. The total operations to matrix multiply is 2N steps, and not O(N 2). This
reduction can be substantial for numbers N of the order of a thousand.

j and ˜M −

The previous example pertains to the matrix M and vector consisting of positive
values. Minus signs in the matrix can be incorporated very simply by separating
the elements ˜Mj = (M1M0M2M0 . . .) into the respective positive entries and negative
entries: ˜M +
j . A vector with positive entries can be used to multiply both
−
the vi ˜Mj = vi ˜M +
j entries. The addition of the column vectors is achieved by
j and P vi ˜M −
adding separately the positive entries and the negative entries: P vi ˜M +
j .
Then the individual entries of the two terms are required to be subtracted. The net
total number of steps is O(N).

j + vi ˜M

A simple application of the of the matrix multiplication of a vector is the fast
fourier transform. The butterﬂy reduction of the usual multiplication of the vector
lowers the O(N 2) to O(N ln N) steps. Theoretically, by separating the matrix into
real and complex parts, with the minus signs handled separately, can achieve a theo-
retical FFT in O(N) steps. This exponentially faster than the butterﬂy conﬁguration.

The butterﬂy conﬁguration can also be analyzed with subtle memory allocation

4

of the data transfer to an approximate ln N operations. The data has to be reorderd
in traversing the butterﬂy.

Conclusions

The theoretical improvements in the matrix inverse from O(N 3) steps to O(N 2)
steps, and matrix times vector from O(N 2) to O(N) steps has profound impact in
computational science. Unfortunately, a bit register of a large size is required.
In
conventional computing registers, there is a waste depending on the data size. For
example, a 256 bit register handling 32 bit data can be optimized by a factor of 8,
which is still substantial.

Ideally, the theoretical drop of the matrix inverse and the matrix multiplication
by a factor of N is suitable for more advanced computing apparatus. The theoretical
bounds in the optimization are achieved with large bit registers, which can be designed
in several contexts.

5

References

[1] Gene H. Golub and Charles F. Van Loan, Matrix Computations, Johns Hopkins

Studies in Mathematical Sciences, 1996.

6


Russell K. Standish
High Performance Computing Support Unit
University of New South Wales
Sydney, 2052, Australia
R.Standish@unsw.edu.au, http://parallel.hpc.unsw.edu.au/rks

Abstract. Ensemble theories have received a lot of interest recently as a means of
explaining a lot of the detailed complexity observed in reality by a vastly simpler
description “every possibility exists” and a selection principle (Anthropic Principle)
“we only observe that which is consistent with our existence”. In this paper, I show why
in an ensemble theory of the universe, we should be inhabiting one of the elements of
that ensemble with least information content that satisﬁes the anthropic principle. This
explains the eﬀectiveness of aesthetic principles such as Occam’s razor in predicting
usefulness of scientiﬁc theories. I also show, with a couple of reasonable assumptions
about the phenomenon of consciousness, the linear structure of quantum mechanics
can be derived.

PACS numbers: 03.65.Ta, 89.70.+c

2 Why Occam’s Razor
0
0
2
 
n
u
J
 
1
1
 
 
]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[
 
 
3
v
0
2
0
1
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Submitted to: J. Phys. A: Math. Gen.

Why Occam’s Razor

1. Introduction

2

Wigner[1] once remarked on “the unreasonable eﬀectiveness of mathematics”,
encapsulating in one phrase the mystery of why the scientiﬁc enterprise is so successful.
There is an aesthetic principle at large, whereby scientiﬁc theories are chosen according
to their beauty, or simplicity. These then must be tested by experiment — the surprising
thing is that the aesthetic quality of a theory is often a good predictor of that theory’s
explanatory and predictive power. This situation is summed up by William de Ockham
“Entities should not be multiplied unnecessarily” known as Occam’s Razor.

We start our search into an explanation of this mystery with the anthropic
principle[2]. This is normally cast into either a weak form (that physical reality must
be consistent with our existence as conscious, self-aware entities) or a strong form (that
physical reality is the way it is because of our existence as conscious, self-aware entities).
The anthropic principle is remarkable in that it generates signiﬁcant constraints on
the form of the universe[2, 3]. The two main explanations for this are the Divine
Creator explanation (the universe was created deliberately by God to have properties
suﬃcient to support intelligent life), or the Ensemble explanation[3] (that there is a
set, or ensemble, of diﬀerent universes, diﬀering in details such as physical parameters,
constants and even laws, however, we are only aware of such universes that are consistent
with our existence). In the Ensemble explanation, the strong and weak formulations of
the anthropic principle are equivalent.

Tegmark introduces an ensemble theory based on the idea that every self-consistent
mathematical structure be accorded the ontological status of physical existence. He
then goes on to categorize mathematical structures that have been discovered thus
far (by humans), and argues that this set should be largely universal, in that all self-
aware entities should be able to uncover at least the most basic of these mathematical
structures, and that it is unlikely we have overlooked any equally basic mathematical
structures.

An alternative ensemble approach is that of Schmidhuber’s[4] — the “Great
Programmer”. This states that all possible halting programs of a universal Turing
machine have physical existence. Some of these programs’ outputs will contain self-
aware substructures — these are the programs deemed interesting by the anthropic
principle. Note that there is no need for the UTM to actually exist, nor is there any
need to specify which UTM is to be used — a program that is meaningful on UTM1
can be executed on UTM2 by prepending it with another program that describes UTM1
in terms of UTM2’s instructions, then executing the individual program. Since the set
of programs (ﬁnite length bitstrings) is isomorphic to the set of whole numbers N, an
enumeration of N is suﬃcient to generate the ensemble that contains our universe. The
information content of this complete set is precisely zero, as no bits are speciﬁed. This
has been called the “zero information principle”.

In this paper, we adopt the Schmidhuber ensemble as containing all possible
descriptions of all possible universes, whilst remaining agnostic on the issue of whether

Why Occam’s Razor

3

this is all there is.‡ Each self-consistent mathematical structure (member of the Tegmark
ensemble) is completely described by a ﬁnite set of symbols, and a countable set
of axioms encoded in those symbols, and a set of rules (logic) describing how one
mathematical statement may be converted into another.§ These axioms may be encoded
as a bitstring, and the rules encoded as a program of a UTM that enumerates all possible
theorems derived from the axioms, so each member of the Tegmark ensemble may be
mapped onto a Schmidhuber one.k. The Tegmark ensemble must be contained within
the Schmidhuber one.

An alternative connection between the two ensembles is that the Schmidhuber
ensemble is a self-consistent mathematical structure, and is therefore an element of the
Tegmark one. However, all this implies is that one element of the ensemble may in
fact generate the complete ensemble again, a point made by Schmidhuber in that the
“Great Programmer” exists many times, over and over in a recursive manner within his
ensemble. This is now clearly true also of the Tegmark ensemble.

2. Universal Prior

The natural measure induced on the ensemble of bitstrings is the uniform one, i.e. no
bitstring is favoured over any other. This leads to a problem in that longer strings are
far more numerous than shorter strings, so we would conclude that we should expect to
see an inﬁnitely complex universe.

However, we should recognise that under a UTM, some strings encode for identical
programs as other strings, so one should equivalence class the strings. In particular,
strings where the bits after some bit number n are “don’t care” bits, are in fact
equivalence classes of all strings that share the ﬁrst n bits in common. One can see that
the size of the equivalence class drops oﬀ exponentially with the amount of information
encoded by the string. Under a UTM, the amount of information is not necessarily
equal to the length of the string, as some of the bits may be redundant. The sum

PU (s) =

X
p:U computes s from p and halts

2−|p|,

(1)

where |p| means the length of p, gives the size of the equivalence class of all halting
programs generating the same output s under the UTM U. This measure distribution
is known as a universal prior, or alternatively a Solomonoﬀ-Levin distribution[5]. We
assume the self-sampling assumption[6, 7], essentially that we expect to ﬁnd ourselves in

‡ For example, this ensemble does not include uncomputable numbers — but should these peculiar
mathematical beasts be accorded physical existence?
§ Strictly speaking, these systems are called recursively enumerable formal systems, and are only
a subset of the totality of mathematics, however this seem in keeping with the spirit of Tegmark’s
suggestion
k In the case of an inﬁnite number of axioms, the theorems must be enumerated using a dovetailer
algorithm. The dovetailer algorithm is a means of walking an inﬁnite level tree, such that each level is
visited in ﬁnite time. An example is that for a n-ary tree, the nodes on the ith level are visited between
steps ni and ni+1 − 1.

Why Occam’s Razor

4

one of the universes with greatest measure, subject to the constraints of the anthropic
principle. This implies we should ﬁnd ourselves in one of the simplest possible universes
capable of supporting self-aware substructures (SASes). This is the origin of physical
law — why we live in a mathematical, as opposed to a magical universe. This is why
aesthetic principles, and Ockam’s razor in particular are so successful at predicting good
scientiﬁc theories. This might also be called the “minimum information principle”.

There is the issue of what UTM U should be chosen. Schmidhuber sweeps this
issue under the carpet stating that the universal priors diﬀer only by a constant factor
due to the compiler theorem, along the lines of

PV (s) ≥ PU V PU (s)

where PU V is the universal prior of the compiler that interprets U’s instruction set in
terms of V ’s. The inequality is there because there are possibly native V -code programs
that compute s as well. Inverting the symmetric relationship yields:

PU V PU (s) ≤ PV (s) ≤ (PV U )−1PU (s)

The trouble with this argument, is that it allows for the possibility that:

PV (s1) ≪ PV (s2), but PU (s1) ≫ PU (s2)

So our expectation of whether we’re in universe s1 or s2 depends on whether we chose
V or U for the interpreting UTM.

There may well be some way of resolving this problem that leads to an absolute
measure over all bitstrings. However, it turns out that an absolute measure is not
required to explain features we observe. A SAS is an information processing entity
that maps descriptions onto meanings. This mapping implies an equivalence relation
on descriptions, namely two descriptions are equivalent if they share the same meaning.
Information content, or complexity of the description is given by the Shannon formula[8]:

C(x) = − log P (x)

(2)

where P (x) is the proportion of messages equivalent to x out of all messages.

Meanings are discrete, and can be enumerated, so the SAS deﬁnes a function from
the set of descriptions onto the set of whole numbers. If this function happens to be
recursive (ie computable), then the function may be replaced by the equivalent Turing
machine, and the P (x) above is just the Universal Prior measure (1).

Thus we do not need to specify an arbitrary reference machine in order to generate
a measure — an observer within the ensemble induces a measure on that ensemble that
favours simpler descriptions. We should expect to ﬁnd ourselves in a universe with one of
the simplest underlying structures (compatible with our existence), as measured by our
own information processing abilities. This does not preclude the possibility that other
more complex universes (by our own perspective) may be the simplest such universe
according to the self-aware inhabitants of that universe. This is the bootstrap principle
writ large.

Why Occam’s Razor

3. The White Rabbit Paradox

5

An important criticism leveled at ensemble theories is what John Leslie calls the failure
of induction[9, §4.69]. If all possible universes exist, then what is to say that our orderly,
well-behaved universe won’t suddenly start to behave in a disordered fashion, such that
most inductive predictions would fail in them. This problem has also been called the
White Rabbit paradox[10], presumably in a literary reference to Lewis Carrol.

This sort of issue is addressed by consideration of measure. We should not worry
about the universe running oﬀ the rails, provided it is extremely unlikely to do so.
Note that Leslie uses the term range to mean what we mean by measure. At ﬁrst
consideration, it would appear that there are vastly more ways for a universe to act
strangely, than for it to stay on the straight and narrow, hence the paradox.

Evolution has taught us to be eﬃcient classiﬁers of patterns, and to be robust in
the presence of errors.
It is important to know the diﬀerence between a lion and a
lion-shaped rock, and to establish that diﬀerence in real time. Only a ﬁnite number of
the description’s bits are processed by the classiﬁer, the remaining being “don’t care”
bits. Around each compact description is a cloud of completely random descriptions
considered equivalent by the observer. The size of this cloud decreases exponentially
with the complexity of the description.

So what are the chances of the laws of physics breaking down, and of us ﬁnding
ourselves in one of Lewis Carrol’s creations? Such a universe will have a very complex
description — for instance the coalescing of air molecules to form a ﬁre breathing
dragon would involve the complete speciﬁcation of the states of some 1030 molecules, an
absolutely stupendous amount of information, compared with the simple speciﬁcation
of the big bang and the laws of physics that gave rise to life as we know it. The chance
of this happening is equally remote, via (2).

4. Quantum Mechanics

In the previous sections, I demonstrate that formal mathematical systems are the most
compressible, and have highest measure amongst all members of the Schmidhuber
In this work, I explicitly assume the validity of the Anthropic Principle,
ensemble.
namely that we live in a description that is compatible with our own existence. This
is by no means a trivial assumption — it is entirely possible that we are inhabiting a
virtual reality where the laws of the world needn’t be compatible with our existence.
However, to date, the Anthropic Principle has been found to be valid[2].

In order to derive consequences of the Anthropic Principle, one needs to have
a model of consciousness, or at very least some necessary properties that conscious
observer must exhibit. I will explore the consequences of just two such properties of
consciousness.

The ﬁrst assumption to be made is that observers will ﬁnd themselves embedded
in a temporal dimension. A Turing machine requires time to separate the sequence of

Why Occam’s Razor

6

states it occupies as it performs a computation. Universal Turing machines are models
of how humans compute things, so it is possible that all conscious observers are capable
of universal computation. Yet for our present purposes, it is not necessary to assume
observers are capable of universal computation, merely that observers are embedded in
time.

The second assumption, which is related to Marchal’s computational indeterminism[11],

is that the simple mathematical description selected from the Schmidhuber ensemble
describes the evolution of an ensemble of possible experiences. The actual world expe-
rienced by the observer is selected randomly from this ensemble. More accurately, for
each possible experience, an observer exists to observe that possibility. Since it is im-
possible to distinguish between these observers, the internal experience of that observer
is as though it is chosen randomly from the ensemble of possibilities. This I call the
Projection Postulate.

The reason for this assumption is that it allows for very complex experiences to
It is a very generalised form of Darwinian
be generated from a very simple process.
evolution, which exhibits extreme simplicity over ex nihilo creation explanations of life on
Earth. Whilst by no means certain, it does seem that a minimum level of complexity of
the experienced world is needed to support conscious experience of that world according
the the anthropic principle.

This ensemble of possibilities at time t we can denote ψ(t). Ludwig[12, D1.1]
introduces a rather similar concept of ensemble, which he equivalently calls state to
make contact with conventional terminology. At this point, nothing has been said of
the mathematical properties of ψ. I shall now endeavour to show that ψ is indeed an
element from complex Hilbert space, a fact normally assumed in conventional treatments
of Quantum Mechanics.

The projection postulate can be modeled by a partitioning map A : ψ −→ {ψa, µa},
where a indexes the allowable range of potential observable values corresponding to A,
ψa is the subensemble satisfying outcome a and µa is the measure associated with ψa
(Pa µa = 1).

Finally, we assume that the generally accepted axioms of set theory and probability
theory hold. Whilst the properties of sets are well known, and needn’t be repeated here,
the Kolmogorov probability axioms are[5]:

(A1) If A and B are events, then so is the intersection A ∩ B, the union A ∪ B and the

diﬀerence A − B.

(A2) The sample space S is an event, called the certain event, and the empty set ∅ is

an event, called the impossible event.

(A3) To each event E, P (E) ∈ [0, 1] denotes the probability of that event.

(A4) P (S) = 1.

(A5) If A ∩ B = ∅, then P (A ∪ B) = P (A) + P (B).

(A6) For a decreasing sequence

A1 ⊃ A2 ⊃ · · · ⊃ An · · ·

7

(3)

(4)

(5)

(6)

(7)

(8)

Why Occam’s Razor

of events with Tn An = ∅, we have limn→∞ P (An) = 0.
Consider now the projection operator P{a} : V −→ V , acting on a ensemble ψ ∈ V ,
V being the set of all such ensembles, to produce ψa = P{a}ψ, where a ∈ S is an outcome
of an observation. We have not at this stage assumed that P{a} is linear. Deﬁne addition
for two distinct outcomes a and b as follows:

P{a} + P{b} = P{a,b}

from which it follows that

PA⊂S = X
a∈A

P{a}

PA∪B = PA + PB − PA∩B
PA∩B = PAPB = PBPA

These results extend to continuous sets by replacing the discrete sums by integration
over the sets with uniform measure. Here, as elsewhere, we use Σ to denote sum or
integral respectively as the index variable a is discrete of continuous.

Let the ensemble ψ ∈ V ≡ {PAψ|A ⊂ S} be a “reference state”, corresponding
It encodes information about the whole ensemble. Denote the

to the certain event.
probability of a set of outcomes A ⊂ S by Pψ(PAψ). Clearly

Pψ(PSψ) = Pψ(ψ) = 1

by virtue of (A4). Also, by virtue of equation (5) and (A4),

Pψ((PA + PB)ψ) = Pψ(PAψ) + Pψ(PBψ)

if A ∩ B = ∅

Assume that equation (7) also holds for A ∩ B 6= ∅ and consider the possibility that

A and B can be identical. Equation (7) may be written:

Pψ((aPA + bPB)ψ) = aPψ(PAψ) + bPψ(PBψ), ∀a, b ∈ N

Thus, the set V naturally extends by means of the addition operator deﬁned by equation
(3) to include all linear combinations of observed states, at minimum over the natural
numbers. If A ∩ B 6= ∅, then Pψ((PA + PB)ψ) may exceed unity, so clearly (PA + PB)ψ
is not necessarily a possible observed outcome. How should we interpret these new
“nonphysical” states?

At each moment that an observation is possible, an observer faces a choice about
what observation to make. In the Multiverse, the observer diﬀerentiates into multiple
distinct observers, each with its own measurement basis.
In this view, there is no
preferred basis[13].

The expression Pψ((aPA + bPB)ψ) must be the measure associated with a observers
choosing to partition the ensemble into {A, ¯A} and observing an outcome in A and b
observers choosing to partition the ensemble into {B, ¯B} and seeing outcome B. the
coeﬃcients a and b must be be drawn from a measure distribution over the possible
choices of measurement. The most general measure distributions are complex, therefore
the coeﬃcients, in general are complex[14]. We can comprehend easily what a positive

Why Occam’s Razor

8

measure means, but what about complex measures? What does it mean to have an
observer with measure −1? It turns out that these non-positive measures correspond
to observers who chose to examine observables that do not commute with our current
observable A. For example if A were the observation of an electron’s spin along the
z axis, then the states |+i + |−i and |+i − |−i give identical outcomes as far as A is
concerned. However, for another observer choosing to observe the spin along the x axis,
the two states have opposite outcomes. This is the most general way of partitioning the
Multiverse amongst observers, and we expect to observe the most general mathematical
structures compatible with our existence.

The probability function P can be used to deﬁne an inner product as follows. Our
reference state ψ can be expressed as a sum over the projected states ψ = Pa∈S P{a}ψ ≡
Pa∈S ψa. Let V ∗ = L(ψa) be the linear span of this basis set. Then, ∀φ, ξ ∈ V , such
that φ = Pa∈S φaψa and ξ = Pa∈S ξaψa, the inner product hφ, ξi is deﬁned by

hφ, ξi = X
a∈S

φ∗

aψaPψ(ψa)

It is straightforward to show that this deﬁnition has the usual properties of an inner
product, and that ψ is normalized (hψ, ψi = 1). The measures µa are given by

µa = Pψ(ψa) = hψa, ψai

= hψ, Paψi
= |hψ, ˆψai|2
where ˆψa = ψa/pPψ(ψa) is normalised.

Until now, we haven’t used axiom (A6). Consider a sequence of sets of outcomes
A0 ⊃ A1 . . ., and denote by A ⊂ An∀n the unique maximal subset (possibly empty),
such that ¯A Tn An = ∅. Then the diﬀerence PAi − PA is well deﬁned, and so
h(PAi − PA)ψ, (PAi − PA)ψi = Pψ((PAi − PA)ψ)

= Pψ((PAi + P ¯A − PS)ψ)
= Pψ(PAi∩ ¯A).

By axiom (A6),

lim
n→∞

h(PAi − PA)ψ, (PAi − PA)ψi = 0,

so PAiψ is a Cauchy sequence that converges to PAψ ∈ V . Hence V is complete under
the inner product (9). It follows that V ∗ is complete also, and is therefore a Hilbert
space.

The most general form of evolution of ψ is given by:

dψ
dt

= H(ψ)

Some people may think that discreteness of the world’s description (ie of the
Schmidhuber bitstring) must imply a corresponding discreteness in the dimensions of
the world. This is not true. Between any two points on a continuum, there are an
inﬁnite number of points that can be described by a ﬁnite string — the set of rational

(9)

(10)

(11)

(12)

(13)

Why Occam’s Razor

9

numbers being an obvious, but by no means exhaustive example. Continuous systems
may be made to operate in a discrete way, electronic logic circuits being an obvious
example. Therefore, the assumption of discreteness of time is actually a specialisation
(thus of lower measure according to the Universal Prior) relative to it being continuous.
Axiom (A3) constrains the form of the evolution operator H. Since we suppose
that ψa is also a solution of equation 13 (ie that the act of observation does not change
the physics of the system), H must be linear. The certain event must have probability
of 1 at all times, so

0 =

dPψ(t)(ψ(t))
dt

= d/dthψ, ψi

= hψ, Hψi + hHψ, ψi

H† = − H,

i.e. H is i times a Hermitian operator.

5. Discussion

(14)

A conventional treatment of quantum mechanics (see eg Shankar[15]) introduces a
set of 4-5 postulates that appear mysterious.
In this paper, I introduce a model of
observation based on the idea of selecting actual observations from an ensemble of
possible observations, and can derive the usual postulates of quantum mechanics aside
from the Correspondence Principle.¶ Even the property of linearity is needed to allow
disjoint observations to take place simultaneously in the universe. Weinberg[16, 17]
experimented with a possible non-linear generalisation of quantum mechanics, however
found great diﬃculty in producing a theory that satisﬁed causality. This is probably
due to the nonlinear terms mixing up the partitioning {ψa, µa} over time. It is usually
supposed that causality[3], at least to a certain level of approximation, is a requirement
for a self-aware substructure to exist.
It is therefore interesting, that relatively mild
assumptions about the nature of SASes, as well as the usual interpretations of probability
and measure theory lead to a linear theory with the properties we know of as quantum
mechanics. Thus we have a reversal of the usual ontological status between Quantum
Mechanics and the Many Worlds Interpretation.

6. Acknowledgments

I would like to thank the following people from the “Everything” email discussion list
for many varied and illuminating discussions on this and related topics: Wei Dai,
Hal Finney, Gilles Henri, James Higgo, George Levy, Alastair Malcolm, Christopher
Maloney, Jaques Mallah, Bruno Marchal and J¨urgen Schmidhuber.

¶ The Correspondence Principle states that classical state variables are represented in the quantum
formulation by replacing appropriately x → X and p → −i~d/dx.

Why Occam’s Razor

10

In particular, the solution presented here to the White Rabbit paradox was devel-
oped during an email exchange between myself and Alistair Malcolm during July 1999,
archived on the everything list (http://www.escribe.com/science/theory). Alistair’s ver-
sion
at
http://www.physica.freeserve.co.uk/p101.htm.

solution

found

may

web

this

site

his

on

be

of

I would also like to thank the anonymous reviewer for suggesting Ludwig’s book[12].
Whilst the intuitive justiﬁcation in that book is very diﬀerent, there is a remarkable
congruence in the set of axioms chosen to the ones presented in this paper. Unfortunately
the standard of rigour in Ludwig’s book precludes easy comprehension of the results.
[1] E. P. Wigner. Symmetries and Reﬂections. MIT Press, Cambridge, 1967.
[2] J. D. Barrow and F. J. Tipler. The Anthropic Cosmological Principle. Clarendon, Oxford, 1986.
Is ”the theory of everything” merely the ultimate ensemble theory. Annals of
[3] Max Tegmark.

Physics, 270:1–51, 1998.

[4] J¨urgen Schmidhuber. A computer scientist’s view of life, the universe and everything.

In
C. Freska, M. Jantzen, and R. Valk, editors, Foundations of Computer Science: Potential-
Theory-Cognition, volume 1337 of Lecture Notes in Computer Science, pages 201–208. Springer,
Berlin, 1997.

[5] Ming Li and Paul Vit´anyi. An Introduction to Kolmogorov Complexity and its Applications.

Springer, New York, 2nd edition, 1997.

[6] J. Leslie. The End of the World. Routledge, London, 1996.
[7] B. Carter. The anthropic principle and its implications for biological evolution. Phil. Trans. Roy.

Soc. Lond., A310:347–363, 1983.

[8] Russell K. Standish. On complexity and emergence. Complexity International, 9, 2001.
[9] John Leslie. Universes. Routledge, New York, 1989.
[10] Bruno Marchal. Conscience et m´ecanisme. Technical Report TR/IRIDIA/95, Brussels University,

1995.

2001.

[11] Bruno Marchal. Computation, consciousness and the quantum. Teorie e modelli, 6(1):29–44,

[12] Gunther Ludwig. Foundations of Quantum Mechanics I. Springer, Berlin, 1983.
[13] Henry P. Stapp. The basis problem in many-world theories. Canadian Journal of Physics, 2002.

submitted. arXiv:quant-ph/0110148.

[14] Donald L. Cohn. Measure Theory. Birkh¨auser, Boston, 1980.
[15] Ramamurti Shankar. Principle of Quantum Mechanics. Plenum, New York, 1980.
[16] Steven Weinberg. Testing quantum mechanics. Annals of Physics, 194:336–386, 1989.
[17] Steven Weinberg. Dreams of a Final Theory. Pantheon, New York, 1992.


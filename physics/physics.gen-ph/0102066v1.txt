An iterative algorithm for generating selected eigenspaces of large matrices

F. Andreozzi, A. Porrino, and N. Lo Iudice
Dipartimento di Scienze Fisiche, Universit`a di Napoli Federico II,
and Istituto Nazionale di Fisica Nucleare.
Complesso Universitario di Monte S. Angelo, Via Cintia, 80126 Napoli, Italy

We propose a new iterative algorithm for generating a subset of eigenvalues and eigenvectors of
large matrices and give convergence criteria for the iterative process. We show that the method
can be turned naturally into an importance sampling algorithm which greatly reduces the number
of basis states needed for an accurate determination of the eigenvectors. Finally, we argue that,
because of its extreme simplicity and eﬃciency, the method may represent a valid alternative to the
much more sophisticated importance sampling approaches currently adopted.

02.70.-c 21.60.-n 71.10.-w

It has become customary in many branches of physics to resort to the diagonalization of an Hamiltonian matrix
in large-dimensional spaces as a tool for an accurate determination of the properties of complex quantum systems.
Lattice models in more than one dimension, in all their variants, quantum dots with many electrons and nuclear
shell-model problems involving many valence nucleons in a major shell are widely known examples. For most of the
systems to be studied, however, the space dimensions are so large as to render a complete diagonalization out of reach
of the computational resources existing now as well as in a foreseeable future. It is therefore compulsory to make
the hypothesis that sampling a small fraction of the basis states suﬃces to generate the eigensolutions to the desired
accuracy.

This is the underlying assumption of the quantum Monte Carlo methods [1] where one uses a properly deﬁned
function of the Hamiltonian as a stochastic matrix which guides a Markov process to sample the basis. These
techniques are quite eﬀective for computing ground states properties [2] only if the stochastic matrix is positive. This
condition, however, is not fulﬁlled in many cases. The same Monte Carlo methods, when used to generate a truncated
basis for diagonalizing the many-body Hamiltonian [3], become quite involved. One has in fact to deal with the
redundancy of the basis states, inherent to the stochastic process, which may slow considerably the convergence of
the procedure, and with the problem of the restoration of the symmetries generally broken in stochastic approaches.
The importance sampling inspires also approaches dealing with the direct diagonalization of the Hamiltonian matrix.
Stochastic diagonalization [4,5] is an example. This method samples the basis states relevant to the ground state
through a combination of plane (Jacobi) rotations and matrix inﬂation. It is therefore free of minus-sign problems.

In the same spirit, we developed an iterative method, extremely easy to implement, for generating a subset of
eigenvectors of a large matrix. Under given conditions, the iterative process converges to any selected set of eigen-
vectors, whatever is the selection criterion adopted. The convergence conditions become also suﬃcient if we generate
the lowest or the highest eigenvalues. The method can be naturally turned into an importance sampling algorithm
which greatly enhances the eﬃciency of the iterative process.

We assume ﬁrst that the matrix A is symmetric and is obtained from a self-adjoint operator ˆA in an orthonormal
basis {| 1i, | 2i, . . . , | N i}. Its matrix elements are therefore aij = hi | ˆA | ji. For the sake of simplicity, we illustrate
the procedure for a one-dimensional eigenspace, selected according to a given prescription rule. The algorithm consists
of a ﬁrst approximation loop and a subsequent iteration of reﬁnement loops. The ﬁrst loop goes through the following
steps:

1a) Start with the ﬁrst two vectors of the basis and diagonalize the matrix

, where we have put

λ(1)
a12
1
a12 a22 (cid:19)

(cid:18)

and the corresponding eigenvector | φ(1)

2 i = K (1)

2,1 | φ(1)

1 i + K (1)

2,2 | 2i satisfying the

1
0
0
2
 
b
e
F
 
1
2
 
 
]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[
 
 
1
v
6
6
0
2
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

λ(1)
1 = a11.
1b) Select the eigenvalue λ(1)
2
1 i ≡| 1i.

assigned rule, where | φ(1)

for j = 3, . . . , N

1c) compute b(1)

j = hφ(1)

j−1 | ˆA | ji.

1d) Diagonalize the matrix

j−1 b(1)
λ(1)
b(1)
j

j
ajj !

.

 

1

1e) Select the eigenvalue λ(1)

and the corresponding eigenvector | φ(1)

j

j i satisfying the given prescription.

end j

N

The ﬁrst loop yields an approximate eigenvalue λ(1)
i=1 K (1)
φ(2)
0 i =
reﬁnement loops:
P
for n = 2, 3, . . .

till convergence (if any)

N i ≡|
N,i | ii. With these new entries we start an iterative procedure which goes through the following

0 and an approximate eigenvector | ψ(1)i ≡| φ(1)

N ≡ E(1) ≡ λ(2)

for j = 1, 2, . . . , N

2a) Compute b(n)
2b) Solve the generalized eigenvalue problem

j−1 | ˆA | ji.

j = hφ(n)

j−1 b(n)
λ(n)
b(n)
j

j
ajj !

− λ

1
K (n)

j−1,j

 

K (n)

j−1,j
1 ! #

= 0.

det

"  

j

2c) Select the eigenvalue λ(n)

and the corresponding eigenvector | φ(n)

i satisfying the assigned criterion.

j

end j

end n.

It is worth to point out that, since the current eigenvector is not orthogonal to any of the basis vectors, the generalized
eigenvalue problem 2b) replaces the standard one 1d). The n-th loop yields an approximate eigenvalue λ(n)
N ≡ E(n) ≡
λ(n+1)
0

. As for the eigenvector, we observe that, at any step of the j-loop, we have

| φ(n)
j

i = p(n)

j

| φ(n)

j−1i + q(n)

j

| ji,

with the appropriate normalization condition [p(n)
yields the n-th eigenvector

j

]2 + [q(n)

j

]2 + 2 p(n)

j q(n)

j K (n)

j−1,j = 1. The iteration of Eq. (1)

where the numbers P (n)

i

are deﬁned as

| ψ(n)i ≡| φ(n)

N i = P (n)

0

| ψ(n−1)i +

P (n)
i

q(n)
i

| ii,

N

i=1
X

P (n)
i

=

N

p(n)
k

k=i+1
Y

(i = 0, 1, . . . , N − 1)

; P (n)

N = 1.

The algorithm deﬁnes therefore the sequence of vectors (2), whose convergence properties we can now examine. The
numbers q(n)

can be expressed as

and p(n)

j

j

| B(n)
j

|

j−1,j − b(n)

j

)2 + 2K (n)

j−1,j (ajj K (n)

j−1,j − b(n)

j

)B(n)

j + (B(n)

j

)2

,

1
2

i

q(n)
j =

(ajj K (n)
h
j = (ajj K (n)
p(n)

j−1,j − b(n)

j

)

q(n)
j
B(n)
j

,

where

It is apparent from these relations that, if

h

i

B(n)

j =

j−1 − λ(n)
λ(n)

j

− K (n)

j−1,j

)(λ(n)

j−1 − λ(n)

j

)

(ajj − λ(n)
h

j

1
2

.

i

the sequence | ψ(n)i has a limit | ψi, which is an eigenvector of the matrix A. In fact, deﬁning the residual vectors

a direct computation gives for their components

| λ(n)

j−1 − λ(n)

j

| → 0,

∀j,

| r(n)i = ( ˆA − E(n)) | ψ(n)i,

2

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

N

l = p(n)
r(n)
− p(n)
N

(all − λ(n)
h

l

l−1 − λ(n)
λ(n)

l

l

l−1 − λ(n)
K (n)

l,l−1 +

)(λ(n)

)

1
2

+ q(n)
N

alN − λ(n)

N δlN

i
n
N −1 − λ(n)
λ(n)
N

K (n)

l,N −1

.

o

(9)

n(cid:0)

(cid:1)

(cid:0)

(cid:1)

o

In virtue of Eq. (7), the norm of the n-th residual vector converges to zero, namely || r(n) ||→ 0. Eq. (7) gives therefore
a necessary condition for the convergence of the | ψ(n)i to an eigenvector | ψi of A, with a corresponding eigenvalue
E = lim E(n). This condition holds independently of the prescription adopted for selecting the eigensolution. Indeed,
we never had to specify the selection rule in steps 1b), 1e) and 2c). Eq. (7) is not only a necessary but also a suﬃcient
condition for the convergence to the lowest or the highest eigenvalue of A. In fact, the sequence λ(n)
is monotonic
(decreasing or increasing, respectively), bounded from below or from above by the trace and therefore convergent.

j

Having proved the convergence of the iterative procedure to an exact eigenvector, let us now show that the algorithm
lends itself to straightforward extensions and improvements which may widen its range of applicability and may
render the diagonalization process extremely eﬀective. Obvious extensions are obtained by removing some of the
initial assumptions. The same iterative procedure suggests that there is no need to assume the orthogonality of
the basis states we started with. A non-orthogonal basis can be treated by simply substituting steps 1a) and 1d)
of the ﬁrst loop with the appropriate generalized eigenvalue problem. Also, we can relax the assumption that A is
symmetric (Hermitian). We have only to update both right and left eigenvectors and perform steps 1c) and 2a) for
both non-diagonal matrix elements.

The method can be turned into an importance sampling algorithm. We need just to impose in the ﬁrst loop that

a state | ji with eigenvalue λj is to be retained only if

∆j =| λj − λj−1 |≃

(b(1)
j )2
| ajj − λj−1 |

> ǫ,

where ǫ is an arbitrarily ﬁxed parameter. As we shall see, the importance sampling leads to an eﬀective drastic
reduction of the matrix dimensions.

Last, but not least, the algorithm can be easily reformulated in a context which allows to compute at once any
number m of eigenvectors. We have simply to replace the two-dimensional matrices with multidimensional ones
having the following block structure: A m × m submatrix diagonal in the selected m eigenvalues, which replaces
λ(n)
j−1,j . We
j
developed an importance sampling algorithm also for the multidimensional case. Such an extension will be outlined
in an extended version of this work.

, a m′ × m′ submatrix corresponding to ajj and two m × m′ oﬀ-diagonal blocks replacing b(n)

or K (n)

j

In order to test the eﬃciency and the convergence rate of the iterative procedure, we applied the algorithm to a
system of 20 particles distributed over 20 doubly-degenerate equispaced single-particle levels with a level spacing of
1 MeV and interacting through a two-body pairing interaction of constant strength G = 0.32 MeV. The resulting
Hamiltonian matrix is of the order 184756. This many-body problem, which is relevant to many mesoscopic systems
like nuclei and superconducting grains [6], represents a severe test. Indeed, because of the oﬀ-diagonal long range
order of the system [7] and the uniform spacing of the single particle levels, we are far from a perturbative regime
and have no a priori prescription for cutting the dimensions of the basis space.

We adopted ﬁrst the single vector iterative procedure, which converges to a single eigenvector, and have computed
the lowest, the ﬁrst excited and the highest eigenstates (Table I). The ﬁrst excited state was obtained by selecting, at
each step, the approximate eigenvector having the second basis state as dominant component. The rapid convergence
of the algorithm is quite apparent. It is however worth noting that the convergence rate depends crucially on the
initial ordering of the basis. We found that the convergence is much faster if we order the diagonal matrix elements
in a monotonic sequence.

The results obtained by adopting the multidimensional version of the algorithm are shown in Table II, where the
ﬁrst ﬁve eigenvalues are reported. Its faster convergence with respect to the one-dimensional case is to be noticed.
Such a greater eﬃciency is most likely due to the following feature. The orthogonality constraint implicit in the
diagonalization procedure, when enforced within a multidimensional space, allows to identify and characterize in more
detail and with much less ambiguity the selected eigensolutions to be determined. Such a more precise characterization
allows to discriminate even between eigenvectors having the same basis states as dominant components.

The eﬀectiveness of the importance sampling algorithm is illustrated in Table III, where the results obtained for the
lowest and the highest eigenvalue by using decreasing values of the sampling parameter ǫ are shown. The corresponding
number of selected basis states is also given. Clearly the use of such an algorithm has made possible a substantial
reduction of the dimensions of the problem. We found a similar reduction also in the multidimensional case.

3

The just outlined test suggests that this algorithm may be usefully adopted for generating the ground state or a
selected number of eigenstates. In the ﬁrst case, it may represent an alternative to Monte Carlo techniques. Indeed,
the method is immune to the fermion sign problem, is much more straightforward and more direct, yielding explicitly
the ground state wave function. It may be also a simpler and more eﬃcient alternative to stochastic diagonalization
since, in all the steps of our procedure, we deal only with 2 × 2 matrices. In the second case, it may represent an
eﬃcient tool for an accurate study of mesoscopic systems, like quantum dots with many electrons and heavy nuclei.
Studies of quantum dots based on exact diagonalization of the Hamiltonian were feasible only for a small number
of electrons or when the calculation was conﬁned within relatively small subspaces [8]. These restrictions should be
largely removed by the present algorithm implemented with the importance sampling. As for the nuclear systems,
we expect that the method will enable us to carry out a practically exact and exhaustive study of the spectroscopic
properties of heavy nuclei, for which full shell model studies are still unfeasible.

ACKNOWLEDGMENT. The work was partly supported by the Prin 99 of the Italian MURST

[1] A comprehensive account of the existing techniques can be found in Quantum Monte Carlo Methods in Physics and Chem-

istry, M. P. Nightingale and C. J. Umrigar eds.,(Kluwer Academic Publishers, the Netherlands, 1999).

[2] See for instance J.A. White, S.E. Koonin, and D.J. Dean, Phys. Rev. C 61, 034303 (2000).
[3] T. Otsuka, M. Honma, and T. Mizusaki, Phys. Rev. Lett. 81, 1588 (1998).
[4] H. de Raedt and W. von der Linden, M. Frick, Phys. Rev. B 45, 8787 (1992).
[5] H. de Raedt and M. Frick, Phys. Rep. 231, 107 (1993).
[6] J. Dukelsky, and G. Sierra, Phys. Rev. Lett. 83, 172 (1999).
[7] C. N. Yang, Rev. Mod. Phys. 34, 694 (1962).
[8] J. J. Palacios, L. Martin-Moreno, G. Chiappe, E. Louis, and C. Tejedor, Phys. Rev. B 50, 5760 (1994)-II.

TABLE I. Results obtained by the single vector iteration algorithm for the two lowest and the highest eigenvalues of the

pairing Hamiltonian matrix. The ﬁrst row gives the order of the iteration.

4
105.4730
107.8385
307.2827

6
105.4727
107.8395
307.2827

TABLE II. Results obtained by the multidimensional algorithm for the lowest ﬁve eigenvalues of the pairing Hamiltonian

matrix. The ﬁrst row gives the order of the iteration.

2
105.4879
107.8235
307.2632

2
105.4729
107.8398
109.8008
109.8011
111.7699

TABLE III. Results obtained by the single vector algorithm for the lowest and the highest eigenvalue using importance

sampling. The rows marked n give the number of sampled basis states.

n

n

121
105.7336
102
307.2611

480
105.5937
177
307.2675

2554
105.4943
1952
307.2819

5772
105.4802
2186
307.2822

8
105.4727
107.8396
307.2827

6
105.4727
107.8396
109.8006
109.8006
111.7697

19115
105.4735
3241
307.2825

4
105.4727
107.8396
109.8006
109.8006
111.7697

4


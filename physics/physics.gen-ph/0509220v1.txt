5
0
0
2
 
p
e
S
 
6
2
 
 
]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[
 
 
1
v
0
2
2
9
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

ON THE STATISTICAL VIEWPOINT
CONCERNING THE 2nd LAW OF
THERMODYNAMICS
0R

A REMINDER ON THE EHRENFESTS’ URN MODEL

Domenico Giulini
Universit¨at Freiburg i.Br.
Physikalisches Institut
Hermann-Herder-Strasse 3
D-79104 Freiburg, Germany

Abstract

In statistical thermodynamics the 2nd law is properly spelled out in terms
of conditioned probabilities. As such it makes the statement, that ‘entropy
increases with time’ without preferring a time direction. In this paper we
try to explain this statement—which is well known since the time of the
Ehrenfests—in some detail within a systematic Bayesian approach.

1

Introduction

First, we wish to make the statement in the abstract more precise. To this end, we
think of an idealized system, whose state may only change at sharp, discrete times.
This allows us to speak unambiguously about next and previous points in time.
Now we make the following

Assumption. A time ti the system is in a state z(ti) of non-maximal entropy. The
statistical 2nd law now makes the following statement about conditioned probabil-
ities (the condition will not be repeated):

Statement 1. The probability, that the state z(ti) will develop in the future to a
state z(ti+1) of larger entropy, is larger than the probability for a development into
a state of smaller entropy.

1

Statement 2. The probability, that the state z(ti) has developed from the past from
a state z(ti−1) of larger entropy, is larger than the probability of a development
from a state of smaller entropy.

Consequence 1. The likely increase of entropy in the future state development
z(ti)
z(ti+1) does not imply a likely decrease for the (ﬁctitious) past develop-
7→
ment z(ti)

z(ti−1), but also a likely increase.

Consequence 2. The most likely development z(ti−1)
z(ti) is that of decreas-
ing entropy. Somewhat ironically one may say, that it is more likely for the state
z(ti) to come about through the improbable development from a more probable
state z(ti−1), than through the probable development from an improbable state.

7→

7→

To properly understand the last consequence, recall that our condition is placed
z(ti+1) this means a retarded or initial
z(ti), however, an advanced or ﬁnal condition. It is this

on z(ti), that is at time ti. For z(ti)
condition, for z(ti−1)
change of condition which makes this behaviour of entropy possible.

7→

7→

Consequence 3. The mere (likely) increase of entropy does not provide an orien-
tation of time. It does not serve to deﬁne a ‘thermodynamic arrow of time’. Rather,
an orientation is usually given by considering a ﬁnite time-interval and imposing
a low-entropy condition at one of the two ends of the interval. Without further
structural elements which would allow to distinguish the two ends, the apparently
existing two possibilities to do so are, in fact, identical. An apparent distinction is
sometimes introduced by stating, that the condition at one end is to be understood
as initial. But at this level this merely deﬁnes initial to be used for that end, where
the condition is placed.

Many notions any types of reasoning in statistical thermodynamics can be well
illustrated in terms of the Ehrenfest’s urn-model, which is to be regarded as a toy
model of a thermodynamic system, and whose detailed description we present be-
In particular, this holds true for the consequences listed above, for whose
low.
partial illustration this model was designed by Paul and Tatiana Ehrenfest [1]; see
also [2]-[5]. Our presentation will be more detailed than theirs. Nothing of what we
say will be essentially new. Besides being more detailed, we try to take a Bayesian
approach. In what follows it will be important to alway relate to the general for-
malism of statistical thermodynamics in order to not provoke ‘easy’ or ‘intuitive’
but uncontrolled reasonings. There is always a certain danger for this to happen
in the context of simple models. The Appendix collects some elementary notions
which are not explained in the main text. These will be relevant in the following
section.

2

2 The Urn-Model

∈

∈

· · ·

7→

, N}

{0, 1}, i

Physical observables correspond to functions Γ

Think of two urns, U0 and U1, among which one distributes N numbered balls.
For exact equipartition to be possible, we assume N to be even. A microstate is
given by the numbers of balls contained in U1 (the complementary set of numbers
then label the balls in U0). To formalize this we associate a two-valued quantity
{1, . . . , N}, to each ball, where xi = 0 (xi = 1) stands for the
xi
i’th ball being in U0 (U1). This identiﬁes the set of microstates, which we will
call Γ (it corresponds to phase space), with Γ = {0, 1}N, a discrete space of of 2N
elements. It can be further identiﬁed with the set of all functions {1,
→
{0, 1}, i
xi. Mathematically speaking, the space Γ carries a natural measure, µΓ ,
Γ its cardinality: µΓ (Λ) = |Λ|. We now
given by associating to each subset Λ
⊂
make the physical assumption, that the probability measure (normalized measure)
νΓ := 2−NµΓ gives the correct physical probabilities. Note that this is a statement
about the dynamics, which here my be expressed by saying, that in the course of the
dynamics of the system, all microstates are reached equally often on time average.
R. We call the set of such
functions Ø. Conversely, it is generally impossible to associate a physically realiz-
able observable to any element in Ø. Let {O1, . . . , On} =: Øre
Ø the physically
realizable ones 1, which we can combine into a single n component observable
Rn is injective, the state is determined by the value of Ore.
Ore
In case of thermodynamical systems it is essential to be far away from injectivity,
Rn should have a sufﬁciently large pre-image
in the sense that a given value α
O−1
re (α)
Γ . The coarse-grained of macroscopic state space in then given by the
Rn of the realized observables Ore. To every macrostate α
image Ω
Ω cor-
responds a set of microstates: Γα := O−1
re (α)
Γ . The latter form a partition of Γ :
α∈Ω Γα = Γ .
= β and
Γα

Γβ =
∩
∅
The realized observable for the urn-model is given by the number of balls in
N
i=1 xi. Its range is the set Ω = {0, 1, . . . , N} of macrostates,
U1, that is, Ore = P
which contains N + 1 elements. The macrostates are denoted by z. To z there
corresponds the set Γz of
microstates. The probability measure νΓ induces
so-called a priori probabilities for macrostates z:

Øn. If Ore : Γ

⊂
⊂

if α

N
z

→

→

S

⊂

⊂

∈

∈

∈

(cid:16)

(cid:17)

Wap(z) = νΓ (Γz) = 2−N

N
z

.

(cid:18)

(cid:19)

(1)

LetX : Ω

R be the random variable z

X(z) = z. Its expectation value E and

1The subscript ‘re’ can be read as abbreviation for ‘realized’ or ‘relevant’.

→

7→

3

6
standard deviation S with respect to the a priori distribution (1) are given by2

E(X, ap) =

S(X, ap) =

,

N
2
√N
2

.

(2)

(3)

(4)

(5)

(6)

(7)

The system has a Markofﬁan random evolution, which is deﬁned as follows:
} with tj > ti for j > i, a
at every discrete lying time ti, where i = {0, 1, 2,
· · ·
random generator picks a number n in the interval 1
N. Subsequently
the ball with number n changes the urn. There are two possibilities: The ball
with number n has been in urn U0 so that the change of macrostate is given by
z + 1. Alternatively, the ball has been in U1 and the change of macrostate is
z
1; ti+1|z; ti), that given
z − 1. The conditional probabilities, W(z
given by z
the state z at time ti the evolution will yield the state z
1 at time ti+1 are given
by

±
±

→

→

≤

≤

n

W(z + 1; ti+1|z; ti) =

= : Wret(z + 1|z),

W(z − 1; ti+1|z; ti) =

= : Wret(z − 1|z).

N − z
N
z
N

Since these are independent of time, we can suppress the arguments ti. We just
1 is one time step after than z, that is,
have to keep in mind, that the left entry, z
the probabilities are past-conditioned or retarded. We indicate this by writing Wret.
Let W(z; ti) denote some chosen absolute probability for the state to be z at
W(z; ti) the probability distribution at time ti. The dy-
Wi+1, on such

time ti and Wi : z
→
namics described above will now induce a dynamical law, Wi
distributions, given by

→

±

W(z; ti+1) = W(z; ti+1|z + 1; ti) W(z + 1; ti)
+ W(z; ti+1|z − 1; ti) W(z − 1; ti)
N − z + 1
N

W(z + 1; ti) +

z + 1
N

=

W(z − 1; ti),

2Note

E(X; ap) = 2−N

= 2−NN

N−1

X
m=0 (cid:18)

N − 1
m

=

N
2

(cid:19)

E(X2 − X; ap) = 2−N

z(z − 1)

= 2−NN(N − 1)

N

z

X
z=1

N
z

(cid:19)

(cid:18)
N

X
z=2

N−2

X
m=0 (cid:18)

N − 2
m

=

N(N − 1)
4

.

(cid:19)

N
z

(cid:18)

(cid:19)

4

whose Markofﬁan character is obvious. To be sure, Wi, i > 0, will depend on
the initial distribution W0. This dependence will be essential if W0 is far from
equilibrium and the number of time steps i not much larger than the number N of
balls. Conversely, one expects that for Wi will approach an equilibrium distribution
Wstat for i

N, where Wstat is independent of W0. Its uniqueness is shown by

≫

Theorem 1. A distribution Wstat which is stationary under (7) is uniquely given by
Wap in (1).

Proof. We show, that Wstat can be uniquely determined from (7). To this end, we
assume a time independent distribution Wstat and write (7) in the form

Wstat(z + 1) =

Wstat(z) −

Wstat(z − 1).

(8)

N
z + 1

N − z + 1
z + 1

Since Wstat(−1) = 0 we have for z = 0 that Wstat(1) = NWstat(0), hence recursively
2N(N − 1)Wstat(0) and Wstat(3) = 1
Wstat(2) = 1
6N(N − 1)(N − 2)Wstat(0). By
induction we get the general formula Wstat(z) =
Wstat(0). Indeed, inserting
this expression for z and z − 1 into the right hand side of (7), we obtain

N
z

(cid:16)

(cid:17)

Wstat(z + 1) =

N
z + 1

N
z

−

N − z + 1
z + 1

N
z − 1
(cid:18)
(N − z + 1)

(cid:18)
(cid:19)
N(N − 1)

(cid:20)

Wstat(0)

(cid:19)(cid:21)
Wstat(0)

· · ·
(z + 1)!

Wstat(0).

(9)

= (N − z)

=

N
z + 1

(cid:18)

(cid:19)

The value of Wstat(0) is ﬁnally determined by the normalization condition:

1 =

Wstat(z) = Wstat(0)

= Wstat(0) 2N

Wstat(0) = 2−N.

(10)

n

X
z=0

N

X
z=0 (cid:18)

N
z

(cid:19)

⇒

2.1 Future-conditioned probabilities and Bayes’ rule

∪· · ·∪

Given a probability space and a set of events, {A1, . . . , An}, which is 1.) complete,
An = 1 (here 1 denotes the certain event), and 2.) mutually exclusive,
i.e. A1
Aj = 0 (here 0 denotes the impossible event). The probability of
i.e. i
Ai
n
an event B then obeys Bayes’ rule3: W(B) = P
k=1 W(B|Ak)W(Ak). This is just
3We deliberately avoid to call it Bayes’ theorem.

= j

⇒

∩

5

6
what we used in (6). This rule now allows us to deduce the inversely conditioned
probabilities:

(11)

(12)

W(Ak|B) =

W(B|Ak)W(Ak)
n
i=1 W(B|Ai)W(Ai)

.

P

We now identify the Ai with the N + 1 events (z ′; ti) at the ﬁxed time ti, where
z ′ = 0, . . . , N + 1, and Ak with the special event (z
1; ti). Further we identify
the event B with (z; ti+1), i.e. with the occurrence of z at the later time ti+1. Then
we obtain:

±

W(z

1; ti | z; ti+1) =

±

±

1; ti)W(z

W(z; ti+1|z

1; ti)
N
z′=0 W(z; ti+1|z ′; ti)W(z ′; ti)
1; ti)

P
W(z; ti+1|z

1; ti)W(z

±

±

W(z; ti+1)

±

=

.

(13)

Hence, given Wi, a formal application of Bayes’ rule allows us to express the future
conditioned (‘advanced’) probabilities in terms of the past conditioned (‘retarded’)
ones. In our case we think of the latter ones as given by (4-5). Hence we obtain
the conditioned probability for (z
1; ti), given that at the later time ti+1 the state
will z occur:

±

W(z + 1; ti|z; ti+1) =

W(z − 1; ti|z; ti+1) =

W(z + 1; ti)
W(z + 1; ti) + N−z+1
W(z − 1; ti)

z+1 W(z − 1; ti)

W(z − 1; ti) + z+1

N−z+1W(z + 1; ti)

,

.

(14)

(15)

2.2 Flow equilibrium

The condition for having ﬂow equilibrium for the pair of times ti, ti+1 reads

±

W(z

1; ti+1|z; ti)W(z; ti) = W(z; ti+1|z
±
It already implies Wi = Wap, since (4-5) give4W(z + 1; ti) = N−z
leads to W(z; ti) =

z+1 W(z; ti) which
W(0; ti). Since 1 = Pz W(z; ti) we have W(0; ti) =

1; ti)W(z

1; ti).

(16)

N
z

±

(cid:17)
4Without using (4-5) one gets

(cid:16)

W(z ± 1; ti+1|z; ti)W(z; ti) = W(z; ti+1|z ± 1; ti)W(z ± 1; ti)

= W(z ± 1; ti|z; ti+1)W(z; ti+1)

(17)

where the last equality is the identity W(a|b)W(b) = W(b|a)W(a). The local (in time) condition
of ﬂow equilibrium is therefore equivalent to (cf. 19)

W(z ± 1; ti+1|z; ti)
W(z ± 1; ti|z; ti+1)

=

W(z; ti+1)
W(z; ti)

.

(18)

6

2−N. Using Theorem 1, we conclude that ﬂow equilibrium at ti implies Wj = Wap
for j

i.

≥

2.3 Time-reversal invariance

To be distinguished from ﬂow equilibrium is time-reversal invariance. The latter is
given by the following equality of past- and future-conditioned probabilities:

W(z

1; ti+1|z; ti)

±

1; ti|z; ti+1)

= W(z
(13)= W(z; ti+1|z

±

1; ti)

±

W(z
1; ti)
±
W(z; ti+1)

,

(4,5)

⇐⇒

W(z; ti+1)

W(z + 1; ti)

=

=

z + 1
N − z
N − z + 1
z

W(z − 1; ti).

(19)

(20)

(21)

(22)

It is interesting to note that the condition of time-reversal invariance is weaker
that that of ﬂow equilibrium. The former is implied, but does not itself imply
the equilibrium distribution. Let us explain this in more detail: Equations (21-
22) imply (7), since N−z
(22) = (7). Hence (21-22) are stable
N ×
under time evolution (7). Conversely, (21-22) is implied by (7) and the following
equation, expressing the equality of the right hand sides of (21) and (22):

(21) + z

N ×

W(z + 1; ti) =

N − z
z + 1

N − z + 1
z

W(z − 1; ti).

(23)

Indeed, eliminating W(z + 1; ti) in (7) using (23), one gets

W(z; ti+1) =

W(z − 1; ti)

W(z + 1; ti),

(24)

N − z + 1
z

(18)
=

z + 1
N − z

hence (21-22). Time-reversal invariance for future times is therefore equivalent to
the ‘constraint’ (23) for the initial condition. It allows for a one-parameter family
of solutions, since it determines Wi for given p := W(0; ti) and q := W(1; ti).
Indeed, in analogy to the proof of Theorem 1 one gets Wi(z) = p
for z even
(cid:17)
and Wi(z) = q
N
N
= 2N−1,
z
N
z
the normalization condition leads to 1 = 2N−1(p + q
q = N(2−(N−1) − p).
N)
(cid:16)
(cid:17)
(cid:16)
(cid:17)
(cid:17)
[0, 2−(N−1)] faithfully parameterizes all distributions obeying
This shows that p
(23). One should note that solutions to (23) are closed under convex sums. In this
way one sees, that the obtained distributions are the convex sum Wi = pWe +
(1 − p)Wo of the ‘even’ distribution, We(z) = (1 − (−1)z−1)2−N
and ‘odd’

for z odd. Since Pz=even

(cid:16)
= Pz=odd

N
z

N
z

⇒

∈

(cid:16)

N
z

(cid:16)

(cid:17)

7

distribution, Wo(z) = (1 − (−1)z)2−N
interval within the simplex ∆N, which connects the point We in the N
∆13...N−1 with the point Wo on the ( N
interval ∆∗, we have

. Solutions to (23) form a closed
2 –sub-simplex
2 + 1)–sub-simplex ∆24···N. If we call this

(cid:16)

(cid:17)

N
z

Theorem 2. The set ∆∗
is invariant under time evolution. The future de-
velopment using W(z; ti+1|z ′; ti) and the past development using W(z; ti|z ′; ti+1)
coincide.5

⊂ W

It is of central importance to note that the past development is, mathematically
speaking, not the inverse operation to the future development. The reason being
precisely that such a change in the direction of development is linked with a change
from retarded to advanced conditionings in the probabilities.

3 General Consequences

In the following we want to restrict to the equilibrium condition. In this case the
future-conditioned probabilities are independent of the ti and we can write W(z
1; ti|z; ti+1) =: Wav(z

1|z). Hence we have:

±

±

Wret(z + 1|z) = Wav(z + 1|z) =

Wret(z − 1|z) = Wav(z − 1|z) =

,

N − z
N
z
N

,

from which statements 1 and 2 made in the Introduction follow. Indeed, let z =
z(ti) > N/2, then the probabilities that at time ti−1 or ti+1 the state was or will
be z − 1 is, in both cases, given by z
N. The probability for the state z + 1 at time
ti−1 or ti+1 is N−z
N . Now, every change of state in the direction of the equilibrium
distribution leads to an increase in entropy (see below). Hence the probability of
z
having a higher entropy at ti−1 or ti+1 is
N−z times that of having a lower entropy.
If z = z(ti) < N/2 we have to use the inverse of that.

5Explicitly one can see the preservation of (23) under time evolution (7) as follows: Given that

the initial distribution Wi satisﬁes (23), the development (7) is equivalent to (21-22). Hence

W(z − 1; ti) =

W(z; ti+1)

W(z + 1; ti) =

W(z + 2; ti+1)

z
N − z + 1
z + 2
N − z − 1

which allows to rewrite (23) for Wi into (23) for Wi+1.

(27)

(28)

(25)

(26)

8

3.1 Boltzmann Entropy

→

R. We stress that since Ω is
Boltzmann Entropy SB is a function SB : Ω
deﬁned only after a choice of coarse graining (i.e. a choice of Øre) has been made,
Boltzmann Entropy, too, must be understood as relative to that choice.6 The value
SB(z) in the macro state z is deﬁned by SB(z) := ln µΓ (Γz). For the urn model
this corresponds to the logarithm of microstates that correspond to the macrostate
z. In what follows it will sometimes be more convenient to label the macrostate
[−1, 1] of range independent of
not by z
N. Let the latter be deﬁned by z = N
1
and approximate ln N! = N ln N − N + O(ln N) (Stirling formula), we obtain the
following expression for the Boltzmann entropy:

2 (1 + σ). If we assume that N, z, (N − z)

[0, N], but rather by a parameter σ

≫

∈

∈

SB(z) = N ln N − z ln z − (N − z) ln(N − z),

SB(σ) = −

N
2

ln

(cid:20)

1 − σ2
4

+ σ ln

1 + σ
1 − σ

.

(cid:21)

(29)

(30)

It obeys SB(σ) = SB(−σ) = SB(|σ|), which just corresponds to the invariance of
the ﬁrst expression under z
[ln 2N, 0] is strictly monotonically decreasing. That SB(σ = 1) = 0 is best seen
→
in the limit z
N of (29). Despite Stirling’s approximation this value is, in fact,
exact, as one easily infers from the fact that z = N just corresponds to a single
microstate. In contrast, the given value at σ = 0 is only approximately valid.

N − z. Considered as function of |σ|, SB : [0, 1]

7→

→

3.2 Consequences 1 and 2

The quantitative form of Consequences 1 and 2 are given by the solution to the
following exercises: Let the state at time ti be z = z(ti). Calculate the conditioned
probabilities for z(ti) i) a local maximum, ii) a local minimum, iii)

(i) z(ti) being a local maximum,

6This apparently non objective character of entropy is often complained about. But this criticism
is based on a misconception, since the term thermodynamical system is not deﬁned without a choice
for Øre. This is no different in phenomenological thermodynamics, where the choice of ‘work degrees
of freedom’, {yi}, (the relevant or controlled degrees of freedom) is part of the deﬁnition of ‘system’.
Only after they have been speciﬁed can one deﬁne the differential one-form of heat, δQ, as the
difference between the differential of total energy, dE, and the differential one-form of reversible
work, δA := fidyi. (Here δ is just meant to indicate that the quantity in question is a one-form, not
that it is the differential, d, of a function; i.e. dδA 6= 0 and dδQ 6= 0 in general.) Hence we deﬁne
δQ := dE − δA. Roughly speaking, one may say that ‘heat’ is the energy that is localized in the
non-relevant (not controlled) degrees of freedom.

9

(ii) z(ti) being a local minimum,

(iii) z(ti) lying on a segment of positive slope,

(iii) z(ti) lying on a segment of negative slope.

(z) re-
Let the corresponding probabilities be Wmax(z), Wmin(z), W
spectively. These are each given by the product of one past and one future condi-
tioned probability. This being a result of the Markofﬁan character of the dynamics,
i.e. that for given (z, ti) the dynamical evolution (z; ti)
1; ti+1) is inde-
pendent of z(ti−1). Using (27-28) we obtain:

(z), and W

→

(z

±

↓

↑

Wmax(z) = Wav(z − 1|z)Wret(z − 1|z) =

Wmin(z) = Wav(z + 1|z)Wret(z + 1|z) =

W

(z) = Wav(z − 1|z)Wret(z + 1|z) =

W

(z) = Wav(z + 1|z)Wret(z − 1|z) =

↑

↓

(cid:16)

(cid:17)
1 −

2

,

2

,

z
N
1 −

1 −

z
N

z
(cid:16)
N
z
N

(cid:16)

(cid:16)

z
(cid:17)
N
z
N

,

.

(cid:17)

(cid:17)

For z/N > 1
2) the probability Wmax (Wmin) dominates the other ones. Ex-
pressed in terms of σ the ratios of probabilities are given by the simple expressions:

2 (z/N < 1

1 + σ
1 − σ

:

1 − σ
1 + σ

Wmax(σ) : Wmin(σ) : W

(σ) : W

(σ) =

: 1 : 1.

(35)

↓
In the limiting case of inﬁnitely many ti we get that the state z is z2/(N2 − z2) =
(1 + σ)2/2(1 − σ) times more often a maximum than any other of the remaining
three possibilities.

↑

We also note an expression for the expected recurrence time, T (z), for the state
z.7 It is derived in [5] (there formula (66)). If the draws from the urns have constant
time separation ∆t one has

and hence a connection between mean recurrence time and entropy:

(31)

(32)

(33)

(34)

(36)

(37)

(cid:20)
7Note that we talk about recurrence in the space Ω of macrostates (‘coarse grained’ states), not

(cid:21)

in the space Γ of microstates.

T (z) =

∆t
Wap(z)

,

S(z) = ln

2N∆t
T (z)

.

10

Reference [5] also shows the recurrence theorem, which for discrete state
spaces asserts the recurrence of each state with certainty. More precisely:
let
W ′(z ′; ti+n|z; ti) be the probability that for given state z at time ti the state z ′
occurs at time ti+n for the ﬁrst time after ti (this distinguishes W ′ from W), then
P∞n=1 W ′(z; ti+n|z; ti) = 1.

3.3 Coarse grained Gibbs entropy and the H-theorem

We recall that the Gibbs entropy SG lives on the space of probability distributions
(i.e. normed measures) on Γ and is hence independent of the choice of Øre. In con-
trast, the coarse grained Gibbs entropy, Scg
G , lives on the probability distributions
on Ω, Scg
R, and therefore depends on Øre. Since the former does serve,
G :
after all, as a Øre independent deﬁnition of entropy (even though, thermodynam-
ically speaking, not a very useful one), we distinguish the latter explicitly by the
superscript ‘cg’. If at all, it is Scg
G and not SG that thermodynamically can we be
compared to SB. The function Scg
G is given by

W →

Scg
G (W) = −

N

X
z=0

W(z)

ln

W(z)
Wstat(z)

.

(cid:21)

·

(cid:20)

(38)

The structure of this expression is highlighted by means of the generalized H-
theorem, which we explain below.8 Since the two entropies SB and Scg
G are deﬁned
on different spaces, Ω and
, it is not immediately clear how to compare them.
W
To do this, we would have to agree on what value of Scg
G we should compare with
SB(z), i.e. what argument W
Ω. A natural
candidate is the distribution centered at z, that is, W(z ′) = δz(z ′), which is 1 for
z ′ = z and zero otherwise. From (38) we then obtain

should correspond to z

∈ W

∈

Scg
G (δz) = SB(z) − N ln 2 .

(39)

Let us now turn to the generalized H-theorem. Let Φ : R

R be a con-
vex function. Then for any ﬁnite family m := {x1, . . . , xn} of not necessarily
pairwise distinct points in R we have the following inequality Φ(Pi αixi)
Pi αiΦ(xi)
∀
index pair i, j, such that xi

≤
R≥0 with Pi αi = 1, where equality holds iff there is no
= 0. In the latter case the convex sum

αj
8Usually this expression is called the relative entropy [of W relative to Wstat]. As [absolute]
entropy of W one then understands the expression − Pz W(z) ln W(z). The H-theorem would be
valid for the latter only if the constant distribution (in our case W(z) = 1/(N + 1)) is an equilibrium
distribution, which is not true for the urn model.

= xj and αi

αi

→

∈

·

11

6
6
is called trivial. We now deﬁne a function H :

R through

W × W →
W(z)
W ′(z)

.

N

X
z=0

H(W, W ′) :=

W ′(z)Φ

(40)

(cid:20)

≥

7→

(cid:21)
Wi+1, Wi+1(z) := Pi W(z|z ′)Wi(z ′), where
Consider a time evolution Wi
0 and Pz W(z|z ′) = 1. We also assume that no row of the ma-
clearly W(z|z ′)
trix W(z|z ′) just contains zeros (which would mean that the state labelled by the
corresponding row number is impossible to reach). We call such time evolutions
and the corresponding matrices non-degenerate.
In what follows those distribu-
for which W(z) > 0

tions W
, will play a
special role. We call them generic. The condition on W(z|z ′) to be non-degenerate
then ensures that the evolution leaves the set of generic distributions invariant. Af-
ter these preparations we formulate

z, i.e. from the interior

W ⊂ W

∈ W

∀

◦

Theorem 3 (generalized H-theorem). Let W ′
non-degenerate; then H(Wi+1, W ′

H(Wi, W ′

i+1)

i).

i be generic and the time evolution

≤
Proof. (Adaptation of the proof of theorem 3 in [6] for the discrete case.) We
deﬁne a new matrix V(z, |z ′) := [W ′
i(z ′), which generates the
time evolution for Wi(z)/W ′

i(z) and obeys Pz′ V(z|z ′) = 1. It follows:

i+1(z)]−1W(z|z ′)W ′

H(Wi+1, W ′

i+1) =

W ′

i+1(z) Φ

N

X
z=0
N

X
z=1
N

X
z′=0
N

N

X
z=0

=

≤

Wi+1(z)
′
i+1(z)
W
N

(cid:21)

(cid:20)

W ′

i+1(z) Φ

V(z|z ′)

X
z′=0

"

W ′

i+1(z)V(z|z ′) Φ

Wi(z ′)
W ′
i(z ′) #

Wi(z ′)
W ′
i(z ′)

(cid:21)

(cid:20)

W ′

i(z ′) Φ

=

X
z′=0
= H(Wi, W ′
i)

(cid:20)

Wi(z ′)
W ′
i(z ′)

(cid:21)

(41)

(42)

(43)

(44)

(45)

Equality in (43) holds, iff the convex sum in the square brackets of (42) is trivial.

Picking a stationary distribution for W ′, which in our case is the unique dis-
tribution Wstat, then H is a function of just one argument which does not increase

12

in time. Taking in addition the special convex function Φ(x) = x ln(x), then we
obtain with Scg

G := −H the above mentioned entropy formula.
Let from now on Φ be as just mentioned. Then we have, due to ln(x)

1−x−1,

≥

with equality iff x = 1:

H(W, W ′) =

W(z) ln

(W(z) − W ′(z)) = 0,

(46)

N

X
z=0

= 0

⇐⇒

W(z)
W ′(z)

(cid:20)
(cid:21)
W(z) = W ′(z)

≥

N

X
z=0
z.

∀

(47)

Let us denote by a distance function on a set M any function d : M

×
R≥0, such that d(x, y) = d(y, x) and d(x, y) = 0
→
(This is more
general than a metric, which in addition must satisfy the triangle inequality.) A
map τ : M
d(x, y)

M is called non-expanding with respect to d, iff d(τ(x), τ(y))
M. We have

x = y.

x, y

M

⇔

≤

→
∈

∀

◦

◦

R, D(W, W ′) := H(W, W ′) + H(W ′, W) is a
Theorem 4. D :
distance function with respect to which every proper non-degenerate time evolution
is non-expanding.

W →

W ×

Proof. Symmetry is clear and (47) immediately implies D(W, W ′)
0 with
equality iff W = W ′, as follows from the separate positivity of each summand.
Likewise (45) holds for each summand, so that no distance increases.

≥

4 Thermodynamic limit and deterministic dynamics

In this section we wish to show how to get a deterministic evolution for random
variables in the limit N
. To this end we ﬁrst consider the discrete, future
directed time evolution of the expectation value of the random variable X(z) = z.
We have

→ ∞

E(X, ti+1) =

z ′Wi+1(z ′) =

z ′Wret(z ′|z)Wi(z)

(48)

N

N

X
z′=0

X
z=0

=

(z + 1)

+ (z − 1)

Wi(z)

N − z
N

z
N

(cid:21)

= 1 +

1 −

E(X, ti).

2
N

(cid:19)

(cid:18)

(49)

N

X
z′=0
N

X
z=0 (cid:20)

13

In the same way we get

E(X2, ti+1) =

N

X
z=0 (cid:20)

(z + 1)2N − z
N

+ (z − 1)

Wi(z)

z
N

(cid:21)

= 1 + 2E(X, ti) + (1 − 4/N) E(X2, ti)

(50)

V(X, ti+1) = E(X2, ti+1) − E2(X, ti+1)

= (1 − 4/N) V(X, ti) +

E(X, ti) −

4
N

4
N2E2(X, ti)

(51)

By the evolution being ‘future directed’ one means that Wret and not W av are used
in the evolution equations, as explicitly shown in (48). In this case one also speaks
of ‘forward-directed evolution’.

In order to carry out the limit N

σ, where σ = 2z

N − 1 as above; hence X = N

→ ∞

we use the new random variable Σ :
2 (1 + Σ). Simple replacement

Ω
→
yields

E(Σ, ti+1) = (1 − 2/N) E(Σ, ti)

V(Σ, ti+1) = (1 − 4/N) V(Σ, ti) +

4
N2

1 − E2(Σ, ti)
(cid:17)
(cid:16)

.

In order to have a seizable fraction of balls moved within a macroscopic time span
τ, we have to appropriately decrease the time steps ∆t := ti+1 − ti with growing
N, e.g. like ∆t = 2
Nτ, where τ is some positive real constant. Its meaning is to be
the time span, in which N/2 balls change urns. Now we can take the limit N
of (52) and (53),

→ ∞

d
dt
d
dt

E(Σ, t) = −

E(Σ, t) =

E(Σ, t) = E0 exp

V(Σ, t) = −

V(Σ, t) =

V(Σ, t) = V0 exp

1
τ
2
τ

⇒

⇒

,

−(t − t1)
τ
(cid:19)
−2(t − t2)
τ

,

(cid:19)

(cid:18)

(cid:18)

where E0, V0, t1, t2 are independent constants. These equations tell us, that 1) the
expectation value approaches the equilibrium value Σ = 0 exponentially fast in the
future, and 2) it does so with exponentially decaying standard deviation. The half
mean time of both quantities is the time for N/2 draws.

According to the discussions in previous sections it is now clear, that in case
of equilibrium identical formulae would have emerged if Wav instead of Wret had
been used, for then Wav = Wret. Most importantly to note is, that the backward
evolution is not obtained by taking the forward evolution and replacing in it t

(52)

(53)

(54)

(55)

7→

14

−t. The origin of this difference is the fact already emphasized before (following
Theorem 2), that Wav(z; z ′) is not the inverse matrix to Wret(z; z ′), but rather the
matrix computed according to Bayes’ rule.

5 Appendix

In this Appendix we collect some elementary notions of probability theory, adapted
to our speciﬁc example.

The space of elementary events9 is Ω = {0, 1, . . . , N}. By

: = (cid:8)X : Ω
: = (cid:8)W : Ω

X

W

R

(cid:9)
R≥0 | X
z∈Ω

→

→

W(z) = 1(cid:9)

we denote the sets of random variables and probability distributions respectively,
, X(N)) deﬁnes a
where
bijection which allows us to identify
with the
N-simplex

(X(0), X(1),
with RN+1. This identiﬁes

RN+1, X

. The map

W ⊂ X

X →

7→

· · ·

W

X

∆N := (cid:8)(W(0),

· · ·

, W(N))

RN+1 | W(z)

∈

0, X
z

≥

W(z) = 1(cid:9) ⊂

RN+1 .

Its boundary, ∂∆N, is the union of all (N − K)-simplices:
∆N | 0 = W(i1) =

∆i1···iK := (cid:8)(W(0), . . . , W(N))

for all K. Its interior is

:=

− ∂

, so that W

Expectation value E, variance V, and standard deviation S are functions

= W(iK)(cid:9)

(59)

· · ·
W(z)

◦

∈

W ⇔

= 0

z.

∀

◦

W

W

∈

W

R, deﬁned as follows:
R,

W →
E :

E(X, W) := X
z∈Ω

X(z)W(z)

X × W →

X × W →

V :

S :

R≥0, V(X, W) := E((X −

X
h

)2, W) = E(X2, W) − E2(X, W)
i
(61)

R≥0,

S(X, W) :=

V(X, W)

simply denotes the constant function
p

X × W →
E(X, W), and
X
where in (61)
h
E2(X, W) := [E(X, W)]2. In the main text we also write E(X, s) if the symbol s
, like s = ap for the a priori distribution (1), or E(X, ti)
uniquely labels a point in
for the distribution Wi at time ti.

X
h

7→

: z

W

i

i

9‘Elementary’ is merely to be understood as mathematical standard terminology, not in any phys-
ical sense. For example, in the urn model, Ω is obtained after coarse graining form the space of
physically ‘elementary’ events.

(56)

(57)

(58)

X ×

(60)

(62)

15

6
References

[1] Paul and Tatiana Ehrenfest: “ ¨Uber eine Aufgabe aus der Wahrscheinlichkeit-
srechnung, die mit der kinetischen Deutung der Entropievermehrung zusam-
menh¨angt”. Mathematisch-Naturwissenschaftliche Bl¨atter, 3 (1906).

[2] Paul and Tatiana Ehrenfest: “ ¨Uber zwei bekannte Einw¨ande gegen das Boltz-

mannsche H-Theorem”. Physikalische Zeitschrift, 9 (1907), 311-314.

[3] Paul and Tatiana Ehrenfest: “Begrifﬂiche Grundlagen der statistischen Auffas-
sung in der Mechanik”. Encyklop¨adie der Mathematischen Wissenschaften IV
2, II, Heft 6.

[4] K.W.F. Kohlrausch and Erwin Schr¨odinger: “Das Ehrenfestsche Modell der

H-Kurve”. Physikalische Zeitschrift XXVII (1926), 306–313.

[5] Mark Kac: “Random Walk and the Theory of Brownian Motion”. Amer. Math.
Monthly, 54 (1947), 369-391. Reprinted in: Selected Papers on Noise and
Stochastic Processes, edited by Nelson Wax (Dover Publications, New York,
1954).

[6] R. Kubo: “H-Theorems for Markofﬁan Processes”. In: Perspectives in Statisti-
cal Physics, edited by H.J. Ravech´e (North-Holland Publ. Comp., Amsterdam,
1981).

16


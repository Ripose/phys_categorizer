Computational Enhancement to Programmers

Gordon Chalmers

e-mail: gordon as number@yahoo.com

Abstract

Computing according to laymens procedures is changed to contain a paradigm of
inoptimality in the high level and assembled code. The code is changed to maximize
the ﬂow of information contained in the electrons so that they function more as a
group and without unwanted coherence eﬀects. Exponential eﬀects are suspected
in the improved operation of the programming. From a laymens point of view the
maladjustment of substandard code could result in a factor of a thousand in such
programs as Microsoft Works which can be speed intensive.

6
0
0
2
 
g
u
A
 
4
 
 
]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[
 
 
1
v
5
4
0
8
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Modern computer programmers usually take the rule of thumb short computer
code means fast computer program. This is an archaic philosophy that doesnt take
into account the hardware of the computer. Typically hardware is a word that means
equipment or large piece of machinery and in the average laymen or company doesnt
realize that a small chip is such. The interwoven ﬂow of data in a single chip has
not been utilized for various reasons including both the presence of danger and the
presence of legality.

For example cohering a billion electrons in such an enviroment can cause large
wanted and unwanted electromagnetic ﬂuctuations that could cause the chip to en-
hance its program or cause the chip harm or even destruction. Truly an unwanted
virus in the form of a program could store information in the knucks and krannys of
a modern chip so that em eﬀects are localized in certain regions. This is to warn the
programmer of an altered program that generates legality to its use.

Alternatively the storage of information as it traverses the nanowiring of the
computer chip can be used to facilitate the speed and memory allocation due to the
eﬀects of one electron on another or on a local group. Indeed these eﬀects can improve
the programs eﬃciency by large factors up to billions or more. The user code which
guides the ﬂow of electrons is required to specify the ﬂow of the individual electrons.
The actual code that results in the eﬃciency is not necessarily the shortest piece of
software that a professional programmer is required to present.

A challenge to a programmer is to write a seemingly unoptimal piece of software
that is not necessarily the shortest one. This is complex and requires safeguards
potentially. It is also machine dependent in that it will function with diﬀerent quality
on one chip than on the other. The computational cost is clearly a function of the
degree of level of the computer code as it is assembled into machine language. The
contribution here is to alter the usual computer code into one in which the program
acts more eﬃciently without the coherence eﬀect which could be shunned and caused
by clocking two or more electrons in a neighborhood so that they are transmitted
more quickly. Rather the code can be designed so that without coherence eﬀects the
informaton and implementation of the program transmits electrons in a more local
nature as it is processed through the chip. The gain in this procedure is likely to
be a factor of a thousand. Considering the commonality of certain programs such
as oﬃce programs or machines and programs built for fast processing like numerical
simulators the improvements could result in factors of a thousand or such.

The implementation of such a program is diﬃcult for a single programmer to
implement without advanced software tools. One could try compiling the program

2

in billions of possible ways to ﬁnd an optimal program. Of course this program is
not on the market to the general public and could result in compiling a program in
a day instead of seconds. And this has obvious advantages to the user. Furhermore
in a conventional program the memory allocation of the various call functions can be
rearranged to enhance the speed and even accuracy of the program.

A point in note is the physical ﬂaw in the Intel chip ﬁrst noticed ﬁve years ago
in that there is a computational bit ﬂaw once in 109 of a divide operation. This
can be circumvented perhaps with a square root in the frequency of occurence by
reassemblying the code in a certain form.

The procedure of guiding the bits through the chip is nonuniversal and usually
linear as understood by a chip designer who prefer a data here to there instead of
a how from here to there. Rather a transcendental ﬂow of information within the
chip is required perhaps even requiring extra bits tagged onto an chirp of data to be
transmitted so that it aﬀects the ﬂow of information in a seemingly inoptimal man-
ner. This is very nonstandard in typical computer programs and appears inoptimal.
However the extra bit could cause the memory allocation to occur diﬀerently in the
hardware with a little redundancy. Even the slightest redundancy could exponentiate
in the form of xq with q on the order of millions. Clearly such a procedure is not
obvious given the linearity of chip structure and the programmers inability to analyze
a billion versions of seemingly inoptimal code. There are various ways to change the
ﬂow of information with nonstandard assembly [1]

However an assembler could be made to fabricate an assembled code from typical
code that would store and transmit information in the best possible manner and
also the safest. Safe here means that the program wont overuse certain areas of the
chip meaning groups of electrons too much. This is to point out that improving the
gigahertz processing can be terahertz on your laptop with something as simple as using
a seemingly counterintuitive programming style with the use of various techniques
including transcendental operations.

The use of polytopes can be eﬃcient techique to simulate a chip. Large compa-
nies could easily optimize their computer programs for users with speciﬁc computer
chips such as AltaVec or Intel or others without increasing the coherence eﬀects at all
which would result in speed ups that could be exponential without any side eﬀects
to your computer. Factors of a thousand or more are expected for generic programs
and should certainly be worth the investment for time consuming operating systems
such as Windows which is not time sensitive but rather for packages such as Maple
of Mathematica of which there are hundreds of thousands of interfaces. As a an-

3

othe point bread and butter packages such as radar operations usually optimize the
hardware blocks including chips and FPGA and ASIC designs with routing of infor-
mation centralized on point to point rather than data speciﬁc type that could enhance
the ﬂow of information through their individual chips. A modern radar could have
multiple chips including those of FPGAs or ASICs in series and the improvement
in eﬃciency which is algorithmic speciﬁc will further exponentiate which is perhaps
more diﬃcult with the ﬁeld reprogrammable.

A recommendation is the construction of a random sampling of assemblers with
things like put an extra bit here or there and further operations so that a conventional
chip will compile diﬀerently and improve both the accuracy and eﬃciency of a generic
public program. It will be speciﬁc to individual chips designed for the masses and
could incorporate some higher level operations if the design of the chip is included.
Perhaps a factor of a million could be achieved in performance speed of your desktop
application which leads to many man hours wasted removed for the user. The poten-
tial payoﬀ truly warrants an investigation and construction of a new compiler from
big industries.

It should also be noted that is probabilistically possible to trap an electron at a
speciﬁc site in the chip itself. This trapping is useful for a coherence eﬀect dependent
on the program that could speed up the electrons by factors of millions. This is
potentially dangerous and can be used to deform speciﬁc information within the
memory allocation so that certain types of information are physically inaccessible
without speciﬁc data unsecuring it. Physically could be to garble the information or
harm the chip. Likewise programs could be developed against this harm from threat
aware viruses. Programs could also be made to operate in extreme environments
including satellites that encounter speciﬁc types of cyclic radiation including solar
ﬂares or in quantum computers which radiate.

4

.

References

[1] Gordon Chalmers, unshared.

5


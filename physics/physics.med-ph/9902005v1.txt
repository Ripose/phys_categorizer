9
9
9
1
 
b
e
F
 
1
 
 
]
h
p
-
d
e
m

.
s
c
i
s
y
h
p
[
 
 
1
v
5
0
0
2
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A Bayesian test for the appropriateness of a model in the
biomagnetic inverse problem

R. Hasson∗ and S.J. Swithenby†,
The Open University,
Milton Keynes,
MK7 6AA
UK

February 2, 2008

Abstract

This paper extends the work of Clarke [1] on the Bayesian foundations of the biomagnetic inverse
problem. It derives expressions for the expectation and variance of the a posteriori source current prob-
ability distribution given a prior source current probability distribution, a source space weight function
and a data set. The calculation of the variance enables the construction of a Bayesian test for the ap-
propriateness of any source model that is chosen as the a priori infomation. The test is illustrated using
both simulated (multi-dipole) data and the results of a study of early latency processing of images of
human faces.

Keywords

Biomagnetic inverse problem, Bayesian.

∗Applied Mathematics department
†Physics department

1

1

Introduction

The magnetoencephalographic (MEG) inverse problem (sometimes known as the biomagnetic inverse prob-
lem) is the process of identifying the source current distribution inside the brain that gives rise to a given
set of magnetic ﬁeld measurements outside the head. The problem is diﬃcult because the detectors are
limited in number and are sensitive to widespread source currents, and because of the existence of silent
and near-silent sources. ’Silent sources’ are conﬁgurations of current density inside the brain which give zero
magnetic ﬁeld outside the head (e.g. all radial sources are silent when the head is modelled as a conducting
sphere). It follows that the general biomagnetic inverse problem is ill-posed and under-determined.

The most common way of reducing the problem and rendering it tractable is by characterising the
source in terms of a limited number of eﬀective current dipoles. Such source descriptions provide links
with the dominant functional architecture model of the brain in which processing is described in terms of
localised activity with interactions between essentially separate regions. Multiple dipole models have enjoyed
considerable success in the analysis of sensory and motor cortex (e.g. [2, 3, 4, 5]).

Growing evidence for the existence of more diﬀuse brain networks have led to an interest in distributed
source algorithms. Several have been proposed [6, 7, 8, 9, 10, 11]. These algorithms have been designed to
cope with the non-uniqueness of the problem, primarily by restriction of the source space and by regularisa-
tion. Each algorithm leads to a unique solution (from the inﬁnite number available) through its particular
choice of source basis, weight functions, noise assumptions, and, in many cases, cost function. There has
been an extended debate about the accuracy and value of these methods. This proceeds at two levels; the
technical ability of the various algorithms to recover a simulated source distribution (often quoted in terms
of one or more source current dipoles), and the electrophysiological appropriateness of the type of source
structure favoured by particular algorithm parameters. So, for example the simple minimum norm solution
[7], which tends to produce a grossly smeared and superﬁcial source distribution may be compared with the
LORETA solution [10] which favours smooth but regionally conﬁned current distributions. The issues have
been fully debated in recent conferences [12, 13]).

The many to one nature of the mapping of sources to magnetic ﬁelds suggests that a probabilistic approach
to reconstructing the sources from the magnetic ﬁeld could be used. A Bayesian probabilistic approach was
ﬁrst proposed by Clarke [1]. More recently, Baillet et al have described an iterative approach which combines
both spatial and temporal constraints within a uniﬁed Bayesian framework designed to allow the estimation
of source activity that varies rapidly with position, e.g. across a sulcus [14]. Schmidt et al have developed
a probabilistic algorithm in which a bridge is made between distributed and local source solutions through
the use of a regional descriptor within the source representation [15]. In this case, a Monte Carlo method is
used in the absence of an analytic solution for the expectation value of the source current.

Here, we are proposing an alternative Bayesian approach. It includes the explicit assumption of both a
prior source current probability distribution and a source space weight function, and allows the calculation
of the expectation and variance of the a-posteriori source current probability distribution. The derivation
of these quantities is detailed in Section 3. The inclusion of the prior probability and the calculation of the
variance provide a means by which the consistency of a model (assumed as the prior) can be tested against
the data to reveal those parts of the source distribution that are statistically robust and, conversely, where
the model is inadequate. Numerical calculation of signiﬁcance is possible. A straightforward extension of
this idea is the direct comparison of two data sets to reveal where, within a given model, there are signiﬁcant
diﬀerences in their associated source distributions. In the ﬁnal part of this paper, both simulated and real
data will be used to illustrate these various uses of the technique.

2 Speciﬁcation of the problem

The arrangement of sources and detectors for the biomagnetic inverse problem is shown in Figure 1. The
sources giving rise to the measurements are assumed to be restricted within a source space Q, which may
be smaller than the whole head volume (e.g. if the sources are assumed to be cortical). The current density
within the volume Q is assumed to belong to the space of square integrable vector ﬁelds on Q, which we call
J.

The measurement process typically gives successive sets of data (∼ 100 channels) every millisecond. In
this paper the data for each time instant is processed independently, and the data from a single time instant

2

Figure 1: A schematic experimental geometry.

is collected into a vector ˜m ∈ RN .
If ~j(~r) ∈ J then the measurement process can be represented by a
functional z : J → RN . A subscript notation will be used to identify the sensor, i.e. zi is the ideal reading
from the ith sensor. So the basic equation is:

˜mi = zi(~j) + ei

where the ei are the measurement errors, which are assumed to be normally distributed with zero mean
and covariance matrix α2D (where α is the standard deviation of the errors and D is a symmetric, positive
deﬁnite matrix).

To compute the functional z( ) on a computer (i.e. to solve the forward problem) requires a volume
conductor model of the head. In this paper the precise model used is irrelevant, so the ﬁnal results will be
written in terms of z. This is done is via the Green’s functions ~Li for the problem, which are deﬁned by

zi(~j) =

~Li(~r) · ~j(~r) d~r

ZQ

Stated simply, the inverse problem is to estimate ~j(~r) given the data vector ˜m. Obviously the given data
are not enough to determine ~j(~r) uniquely (for several reasons). The approach adopted here starts from the
same point as used in Clarke [1], a statement of Bayes’s theorem:

P(~j ∈ A|m ∈ B) =

P(m ∈ B|~j ∈ A)P(~j ∈ A)
P(m ∈ B)

where A is a set of currents and B is a set of measurements. This equation reads, the a posteriori probability
of a current set A after the measurement B is proportional to the probability of producing the measurement
B given that the current is in the set A times the a priori probability of the current set A. (P(m ∈ B) is a
constant for any measurement set B).

In this paper the probability distributions (both the prior probability and the errors) are assumed to
be gaussian and so it is permissible to work with probability density functions. A further simpliﬁcation
is achieved by shrinking the measurement set B to a single point { ˜m} (this ignores the ﬁniteness of the
precision of the measurements). Equation 3 then becomes:

ρ ˜m(~j) ∝ ρ(~j) × ǫ( ˜m − z(~j))

where ρ is the a priori distribution, ρ ˜m is the a posteriori distribution and ǫ is the error distribution.
Throughout the paper, probability density functions will only be determined up to a constant. The constant
of proportionality is found by requiring that the probability is normalised to one. In this paper both ρ and ǫ
are assumed to be Gaussian and then ρ ˜m is calculated to be Gaussian. An error probability density function
ǫ consistent with the Gaussian assumption may be written:

(1)

(2)

(3)

(4)

(5)

ǫ(e) ∝ exp

−

1
2α2 eTD

−1

e

(cid:27)

(cid:26)

This generally valid expression will be retained throughout the derivation in this paper. In practice, the
noise covariance matrix D may be diﬃcult to estimate and, for simplicity, the simple form D = I will be
used in the later illustrations.

3

In this section formulae for the maximum likelihood current distribution and also the expected error distri-
bution are derived under speciﬁc assumptions. First, an inner product on J is deﬁned by:

3 Derivation

~j1 , ~j2
D

E

=

ZQ

~j1(~r) · ~j2(~r)
ω(~r)

d~r

where ω(~r) is a weighting distribution deﬁned on the source space Q. This provides a method of inputting
prior information of the location of sources (e.g. gained from MRI images) into the algorithm.

Clarke [1] assumed that the maximum likelihood prior current density was identically zero. Here that
restriction is avoided and an arbitrary prior current ~jp will be introduced as a parameter of the method.
The a priori probability distribution on J is deﬁned using ~jp and the inner product:

ρ(~j) ∝ exp

−

1
2β2

(cid:26)

~j − ~jp , ~j − ~jp
D

E(cid:27)

where β is the assumed a priori standard deviation.

To proceed further a basis is needed for J. A ‘natural’ choice is a basis that is related to the measurement
functional z. So an obvious candidate is a basis derived from the adjoint map to the measurement map from
J to RN . This gives a map ~z : RN → J (since J is self-dual) deﬁned by:

~z(a) , ~j
D

E

= aTz(~j)

Explicitly this gives the set of linearly independent distributions {ω(~r)~Li(~r)}. This set is extended into
a basis of J that includes the silent sources by adding vectors { ~Li} which are chosen to be orthogonal to the
{ω~Li} i.e.

D
Since {ω(~r)~Li}

E
{ ~Li} is a basis of J a general current density ~j(~r) can be written in terms of this basis as

ω~Li , ~Lj

= 0

∀i, j

~j(~r) =

aiω(~r)~Li(~r) +

bi ~Li(~r)

i
X

S
N

i=1
X

~j =

a
b
(cid:18)
A simple computation gives

~jp =

(cid:19)

,

ap
bp

(cid:18)

(cid:19)

To simplify the notation the components of currents are written in column vector notation:

~j − ~jp , ~j − ~jp
D

E
ω~Li , ω~Lj
D

where Pij =

a − ap
b − bp

=

(cid:18)

and Qij =

T

P 0
0 Q

(cid:19)

(cid:18)
~Li , ~Lj

a − ap
b − bp

(cid:19) (cid:18)

(cid:19)

since by construction

ω~Li , ~Lj

= 0.

Now the two a priori probability density functions (Equation 7 and Equation 5) may be combined with

D
Bayes’s theorem (Equation 4) to obtain the a posteriori probability density:

E

D

E

E

ρ ˜m(~j) ∝ exp

−

1
2β2

(cid:26)

(cid:18)

a − ap
b − bp

T

P 0
0 Q

a − ap
b − bp

exp

−

1
2α2 ( ˜m − z(~j))

T

(cid:19)

(cid:18)

(cid:19) (cid:18)

(cid:19)(cid:27)

(cid:26)

−1

D

( ˜m − z(~j))

(13)

(cid:27)

The task now is to manipulate this equation so that it is explicitly in the form of a gaussian distribution.

As a ﬁrst step the exponentials are combined:

ρ ˜m(~j) ∝ exp

−

1
2α2

T

a − ap
b − bp

ζP 0
0
(cid:18)

ζQ

a − ap
b − bp

(cid:19) (cid:18)

(cid:19)

(cid:26)

(cid:20)(cid:18)

(cid:19)

+ ( ˜m − P a)TD

−1

( ˜m − P a)

(14)

(cid:21)(cid:27)

4

(6)

(7)

(8)

(9)

(10)

(11)

(12)

where ζ = α2/β2 and z(~j) has been replaced by P a. Next, the terms involving operators on a are simpliﬁed by
completing the square All constant terms can be absorbed into the normalization constant of the probability
density function and are ignored in this derivation.

Operators on a = (a − ap)

ζP (a − ap) + ( ˜m − P a)

T

T

= aTζP a − 2aTζP ap + aTP D
= aT

ζP + P D

−1

P

a − 2aT[P D
−1

= (a − ˜a)
(cid:0)

ζP + P D
(cid:1)

T

P

(a − ˜a) + const.

−1

D
P a − 2aTP D

( ˜m − P a)
−1

−1

˜m + const.

−1

˜m + ζP ap] + const.

where ˜a is deﬁned by:

(cid:0)

(cid:1)

ζP + P D

P

˜a = P D

−1

−1

˜m + ζP ap
−1

(cid:0)

So,

(cid:1)
A modiﬁed expression for ρ ˜m(~j) is now available,

˜a = (ζD + P )

( ˜m + ζDap)

ρ ˜m(~j) ∝ exp

−

1
2α2

1
2α2

a − ˜a
b − bp

a − ˜a
b − bp

(cid:18)

(cid:18)

(cid:19)

T

(cid:19)

T

P D−1P + ζP 0
0
(cid:18)

ζQ
−1

P D−1P + ζP
0
(cid:18)(cid:0)

(cid:1)

a − ˜a
b − bp

(cid:19) (cid:18)
0
Q−1/ζ

(cid:26)

(

(cid:19)(cid:27)
−1

a − ˜a
b − bp

(cid:19)

(cid:18)

(cid:19))

∝ exp

−

This is explicitly in the form of a gaussian from which the mean current and covariance matrix can be
identiﬁed by inspection. At this stage it is clear that the mean value of b is bp i.e. that there is no change
from our prior knowledge (this is because by construction all the information that the experiment provides
is orthogonal to the ~Li).

In order to produce images of the a posteriori current density, it is necessary to ﬁnd the distribution of
a single statistic that can be computed. The statistic is deﬁned through a ‘test current’ ~t = χVk (~r)ˆeα where
χVk (~r) is the characteristic function of a voxel in the brain and ˆeα is a unit vector. This is another departure
from Clarke [1] in which a delta function test current is assumed. This choice causes problems because the
inner product

which is needed below (see Equation 46) is undeﬁned for a delta function.

~t , ~t

The distribution of the statistic λ =

~t , ~j
elements required to construct ~t are identiﬁed:
D
E

(cid:11)

(cid:10)

will now be determined. First, the coeﬃcients of the basis

Equation 22 is projected onto this particular linear combination of co-ordinates to ﬁnd the probability

density of λ

λ =

~t , ~j
D

E

=

~t ,

*

i
X

aiω(~r)~Li(~r) +

bi ~Li(~r)

=

ai

~t , ω(~r)~Li(~r)
D
= uTa + vTb, say.

i
X

E

i
X
+

bi

i
X

+
~t , ~Li(~r)
D

E

ρ ˜m(λ) ∝ exp

−

1
2α2

(

2
(uTa + vTb − uT˜a − vTbp)

uT(P D−1P + ζP )

−1

u + vTQ−1v/ζ )

The mean of λ can be identiﬁed from Equation 27 by inspection.

mean of λ = uT˜a + vTbp

5

(15)

(16)
(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

The term vTbp cannot be computed directly since the basis ﬁelds ~Li have not been deﬁned explicitly. The
problem may be overcome by expanding

~t , ~jp
D
ap
i ω(~r)~Li(~r) +

E
bp
~Li(~r)
i

~t , ~jp
D

E

=

~t ,

*

i
X
ap
i

=

~t , ω(~r)~Li(~r)
D
E

i
X

= uTap + vTbp

i
X
+

bp
i

i
X

+
~t , ~Li(~r)
D
E

mean of λ = uT˜a +

− uTap

~t , ~jp
D

E
−1

= uT(ζD + P )

( ˜m + ζDap) +

− uTap

= uT(ζD + P )

( ˜m − P ap) +

−1

~t , ~jp
D
~t , ~jp
D

E

E

Equation 28 may now be rewritten using only references to known vector ﬁelds as:

This equation explicitly writes the expectation value of the statistic λ =

as a sum of two terms.
The second term is the statistic for the prior current, and so the ﬁrst term can be identiﬁed as the correction
to the prior suggested by the measurements i.e. the ﬁrst term shows the diﬀerence between the expectation
of
before and after the experiment was made. This is the ﬁrst central result of this paper and it is
worth stating explicitly:

E

~t , ~j
D

E

~t , ~j
D

Change in expectation of

~t ,

= uT(ζD + P )

−1

˜m − z(~jp)
(cid:17)
(cid:16)

Using Equation 27 it is possible go further than this and determine the statistical signiﬁcance of the

statistic. This is because the variance of the variable λ can also be read oﬀ from Equation 27 as:

D

E

2
variance of λ = α

1
ζ
In order to derive an expression in the form of computable matrices, the term vTQ−1v must be rewritten.

uT
(cid:20)

P + ζP

vTQ

(36)

P D

u +

−1

−1

−1

(cid:21)

v

(cid:1)

(cid:0)

To do so, ~t is written as a linear combination of basis elements:

~t(~r) =

xiω(~r)~Li(~r) +

yi ~Li(~r)

i
X

i
X

Of course x and y are related to u and v. In fact:

ui =

~t , ω(~r)~Li(~r)
D
E

=

=

=

*

j
X
xj

D
xjPij

j
X

j
X

xjω(~r)~Lj(~r) +

yj ~Lj(~r) , ω(~r)~Li(~r)

+

j
X

ω(~r)~Lj(~r) , ω(~r)~Li(~r)
E

Similarly, v = Qy. Using these relationships, the inner product of ~t with itself can be computed:

~t , ~t

=

xiω(~r)~Li(~r) +

yi ~Li(~r) ,

xjω(~r)~Lj(~r) +

yj ~Lj(~r)

i
X

j
X

j
X

+

(cid:10)

(cid:11)

*

i
X

= xTP x + yTQy

6

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(37)

(38)

(39)

(40)

(41)

(42)

(43)
(44)

from which,

~t , ~t

= uTP

−1

u + vTQ

−1

v

Equations 36 and 45 can now be combined to generate the following formula for the variance:

(cid:10)

(cid:11)

2
variance of λ = α

−1

P D

P + ζP

−1

u +

~t , ~t

− uTP

−1

u

1
ζ

uT
(cid:20)

(cid:21)
(cid:1)

(cid:0)

(cid:1)

(cid:11)

(cid:0)(cid:10)
This equation is a generalisation of the results of Clarke [1], some consequences of it were explored in [16]
and [17]. It is interesting to note that the existence of a prior current density does not aﬀect this variance.
It may be helpful to relate features of this equation to the measurement system. The second term in
Equation 46 is multiplied by α2/ζ = β2 and so is independent of the assumed noise levels in the detectors. It
represents a variance derived from the ﬁnite number of measurements and the geometry of the experiment.
Since it is proportional to β2, it can be interpreted in terms of the truncation error becoming less and less
important as the certainty of the prior distribution ~jp increases.

The ﬁrst term in Equation 46 is proportional to α2. It shows how the noise in the data is reﬂected into
source space. The unregularised form of this term was derived previously by Ioannides et al in [18] using
an ad hoc argument. Ioannides et al obtained the regularised form by replacing occurrences of P −1 in the
unregularised form by (P + ζI)

. In the notation used here this gives a ﬁrst term as follows

−1

This does not agree with Equation 46 which has the form (in the case of the measurement error being
uncorrelated, i.e. D = I)

ﬁrst term = uT(P + ζI)

−1

(P + ζI)

u

−1

ﬁrst term = uTP

−1

(P + ζI)

u

−1

(45)

(46)

(47)

(48)

The behaviour as ζ tends to inﬁnity is that the variance tends to zero. This is reasonable since, for ﬁxed
experimental noise levels (i.e. ﬁxed α), ζ tending to inﬁnity corresponds to β tending to zero, which in turn
corresponds to greater and greater certainty that the prior is correct. When β is zero the a priori current
distribution is known with absolute certainty. This is consistent with the above analysis which indicates
that, in this case, the a posteriori current density is certain to be equal to the a priori current density.

Note that, when computing the term uTP −1u in Equation 46, any reasonable algorithm (e.g. Choleski’s
algorithm) for computing P −1u can be used even though the matrix P is ill-conditioned. This is because
the large residual vector which results in this calculation is annihilated by the inner product with u. So the
computation of the whole term is well conditioned.

4 Applications

The main analytical results of this paper (Equations 34, 35 and 46) provide the means of solving the MEG
inverse problem with speciﬁc assumptions and of assessing the robustness of the solution. In this section,
this approach will be illustrated through three studies: a simulation of a few-dipole source set; an analysis
of the appropriate dipole model for data from a real experiment on face processing; and a comparison of
responses to diﬀerent visual stimuli from the same real experiment.

All the illustrations are based on the same experimental arrangement and the same instrument, the
Neuromag-122T M [19]. This is a helmet MEG system that contains 61 pairs of ﬁrst order gradiometers
(∂Br/∂θ, ∂Br/∂φ in spherical polar co-ordinates), covering the head (Figure 2). The outputs of each pair
of detectors are closely related to the dominant tangential ionic current ﬂow in the region underlying the
relevant sensors. Also shown in Figure 2 is the assumed source space, a 2-d spherical shell of radius 0.08 m
covering a 2 radian by 2 radian solid angle over posterior regions of the cortex.

The source conﬁguration for the ﬁrst simulation study was three dipoles distributed in an isosceles
triangle conﬁguration (co-ordinates (0,-0.08 m,-0.02 m), (-0.05 m,-0.06 m,0.02 m) and (0.05 m,-0.06 m,0.02 m)
with orientations (1,0,1), (1,0,-1), (1,0,1) respectively). The positions of the dipoles were chosen so as to
represent approximately a central primary and two bilateral secondary visual processing areas. The precise

7

(a)

(b)

(c)

Figure 2: The experimental geometry. a) and c) show the 61 measurement sites in the helmet arrangement.
Each square represents a pair of orthogonal detectors. Also shown is the source space, consisting of a
discretised mesh covering a part of a spherical shell. b) Shows the coordinate system with the x-axis along
the line joining the pre-auricular points and the y-axis joining the inion and nasion.

locations are not exactly on the source space shell but are displaced by 3 mm, 1 mm, and 1 mm respectively
from the shell. The dipole locations and activation curves are shown in Figure 3. The central source is
activated ﬁrst, followed by synchronous activation of the lateral sources. The forward problem is solved
using a homogeneous sphere conductor model centred at the origin. Gaussian noise has been added to the
computed dipole signal so that the integrated noise power is equal to 50% of the integrated signal power.
Examples of simulated data are shown in Figure 3c.

The simplest approach to the inverse problem is to use Equation 34 with a zero prior current distribution.
This simpliﬁcation results in the same formulae as the probabilistic algorithm that has been used for several
years [20, 21]. The resulting expectation of the a-posteriori current distribution is shown in Figure 4a.

However, using our analysis, it is possible to employ an approach which goes further in comparing
diﬀerent source descriptions. Single or few dipole solutions can be generated from the magnetic ﬁeld data
and used as prior estimates of the current distribution. It is then possible to identify the appropriateness of
this particular dipole-model prior estimate by computing from Equation 35 the change in the expectation
associated with including the measurement information without constraining the ﬁnal solution to a dipolar
form. Because the statistic provides spatial information it indicates directly those areas where the dipole
model solution has been modiﬁed. Using the variance associated with the a-posteriori current (computed
from Equation 46) allows us to plot the number of standard deviations of the change in expectation at each
point in source space.

To illustrate the usefulness of this technique, two prior descriptions of the source current for this data
are postulated - a single moving dipole model and a two moving dipole model. The optimal solutions have
been found through exhaustive search of the discretised source space by least squares minimisation of the
ﬁt to the data. The positions of the ﬁtted dipoles for nine time slices are shown in Figure 4b. They may be
compared with the nearest points on the source space to the actual dipole positions. Using each model in
turn as a prior current and Equations 35 and 46, the statistical signiﬁcance of the diﬀerences between the
a-posteriori and the a-priori current distributions are calculated (Figure 4b).

It can be seen that a single dipole is a good model for the ﬁrst two time instants and also for the
ninth. This is not surprising as only one dipole is active at these times. The signiﬁcance plots for the other
times suggest systematic discrepancies between the model and the data. The restricted localisation of these
signiﬁcant diﬀerences points to additional localised sources that have been omitted from the model. The
next step is the two dipole model (Figure 4c). The two-dipole signiﬁcant diﬀerence diagram suggests that
this model is adequate for all but four time instants. Comparison with the activation curves identiﬁes these
as being the times when all three dipoles are active.

The second illustration uses data from an evoked response study of face-processing using the same exper-
imental system [23]. Human subjects were presented brieﬂy with photographs of human faces and control

8

(a)

(b)

15

(c)

10

5

0

-5

-10

-15

-20

-25

-30

-35

0

100

200

300

400

500

600

700

800

0

100

200

300

400

500

600

700

800

10

(d)

5

0

-5

-10

-15

-20

-25

0

100

200

300

400

500

600

700

800

0

100

200

300

400

500

600

700

800

Figure 3:
a) An isometric projection of the experimental geometry. The dots on the source space are
the feet of the perpendiculars from the three dipole sources used in this simulation. The activation curves
(dipole moment vs. time in milliseconds) for these three dipoles are shown in b). The longer period curve
corresponds to the dominant (central) source. The other curve corresponds to the synchronous and equally
activated lateral sources. The highlighted detector sites are those for which the data is shown in c) channel
1 d) channel 2 and e) channel 3. Note that it is only in the case of Channel 3 that the signal follows clearly
the activation curve for the closest dipole.

25

(e)

20

15

10

5

0

-5

-10

-15

-20

-25

9

(a)

(b)

(c)

(a) The maximum likelihood current density with a zero a priori current distribution. Each frame
Figure 4:
is a two dimensional representation of the shell source space pictured in Figure 2. The 9 frames are snapshots
at time 0 ms, 100 ms, 200 ms, etc. (see Figure 3). They should be read left to right and top to bottom. The
optimal regularization parameter was determined by the L-curve method [22] to be 0.283 × trace(P )/N . The
crosses on the ﬁrst frame of this sequence show the positions of the three source dipoles in this representation
of the source space. (b) The signiﬁcant diﬀerences between a single moving dipole model and the computed
data. The black dots denote the projected position of the ﬁtted dipole. (c) As (b) but with a two moving
dipole model.

objects (e.g. animals) and their neural responses were recorded as a function of time after the stimulus. It is
known that the early response to face images involves widespread activity in the posterior brain but there is
limited evidence for the precise distribution and timecourse of the neuronal sources. One suggestion is that
there are three major areas of activity; in occipital cortex and both right and left ventral occipito-temporal
cortex [24, 25]. Strong occipital activity (starting about 100 ms after the stimulus) is expected to lead to
concurrent activity in the two other regions with a stronger response in the right hemisphere [23]. The
hypothesised arrangement is therefore similar in geometry to the simulated measurement already discussed.
Figure 5 shows the same set of outputs that were presented for the simulated system. In this case, the
ﬁtted dipoles may be thought of as representing a discrete limited region of source current. Figure 5a suggests
early central activity (frame 3) followed by less prominent localised activity on the right (frame 5). These
source regions are reﬂected in the 1-dipole solutions (Figure 5b). However, comparison with Figure 3b shows
that the accuracy of the single dipole ﬁt is less than for the simulated data even though the noise levels are
comparable. This may suggest that there are other active sources present. There is no evidence that these
are recovered by the 2-dipole model (Figure 5c) as there is little improvement in the signiﬁcant diﬀerence
maps generated using a two dipole model as the prior (see for example the strong similarity between frame
4 in Figures 5b and 5c). It would be reasonable to infer that the additional sources are diﬀuse.

The third illustration relates to the main thrust of the face processing study by Swithenby et al [23],
which was to identify statistically signiﬁcant evidence for diﬀerences between the responses to faces and the
other complex visual stimuli. In the initial analysis the strength of evoked activity within a certain region
and latency span was parameterised in terms of the signal power integrated over a group of channels and
a speciﬁed latency span. These calculations revealed that the brain activity in the right occipito-temporal
region following face presentation is signiﬁcantly diﬀerent (p=0.05) from activity following non face images
during the latency span 110 to 170 ms. No other consistent and statistically signiﬁcant diﬀerences were
found. This data-space analysis, though useful, was complicated by the need to survey the large number of
possible choices of channel group and latency range.

The Bayesian framework developed here provides an alternative direct means of directly comparing
responses to two stimuli. By using one data set as the prior and comparing it with the other data set it is
possible to identify those regions in source space where there is a statistically signiﬁcant diﬀerence between
the two source structures within the same source model. We have carried out this calculation for the face
and control stimuli as a function of position and latency for a simple two-dimensional source space consisting
of a part spherical shell whose radius is similar to that of the cortical surface (Figure 6).

10

(a)

(b)

(c)

(a) The maximum likelihood current density for the face response with a zero a priori current
Figure 5:
distribution. The 9 frames represent equal steps in time from 70 ms to 241 ms after the stimulus. (b) The
signiﬁcant diﬀerences between a single moving dipole model and the computed data. The black dots denote
the projected positions of a single dipole. (c) As (b) but with a two moving dipole model.

Figure 6: The maximum likelihood current density for the face response with a a priori current distribution
derived from the control experiment with pictures of animals. The 9 frames represent equal steps in time
from 70 ms to 241 ms after the stimulus. The regularisation parameter ζ was chosen by using the L-curve
method [22]. The optimal value was 1.4 × trace(P )/N .

11

The evidence for statistically signiﬁcant diﬀerences in the right occipito-temporal region at about 155 ms
is clear. However there is no evidence for diﬀerences in earlier latencies, in particular with respect to the
early source shown in Figure 5a.

5 Discussion

The illustrations provided above oﬀer ways of exploiting the new Bayesian results that have been derived.
Dipole predictions have been systematically examined using a measure that goes beyond a simple scalar
goodness of ﬁt measure (i.e. percentage of variance accounted for) to a statistically valid map of ﬁtting
signiﬁcance. This addresses a long standing issue, the unreliability of the goodness of ﬁt measure as a
reliable test of the appropriateness of a given model in explaining a given set of data [26]. The spatially
discriminated Bayesian approach gives a test which is more reliable when there is fundamental concern about
the appropriateness of a model. The second example shows how these ideas may be applied to real data to
assess the complexity of the dipole model that a given data set can sustain. In similar fashion, the third
example illustrates how the Bayesian framework allows the direct comparison of two data sets in order to
identify quantitatively the regions of statistically signiﬁcant source activities.

These ideas may be extended further within MEG (and EEG) data analysis. An obvious extension is
to perform dipole analysis as a precursor to a distributed Bayesian analysis. This may provide a way of
not only reﬁning information about the depth of source activity but also of assessing the reliability of depth
estimates. Other possibilities include exploration of the dynamics. For example, times at which there are
statistically signiﬁcant change in the data could be identiﬁed by using a source distribution estimated from
each time as the prior for an analysis of the data from the next sampling instant.

In summary, the analysis presented here comprises a Bayesian estimator of a source current distribution
in the biomagnetic inverse problem. This is generalisable to other systems and may be used as the engine
for tests of signiﬁcant diﬀerence in source models and data.

References

[1] C.J.S. Clarke. Error estimates in the biomagnetic inverse problem. Inverse Problems, 10:77–86, 1994.

[2] S. Vanni, B. Rockstroh, and R. Hari. Cortical sources of human short-latency somatosensory evoked

ﬁelds to median and ulnar nerve stimuli. Brain Research, 737:25–33, 1996.

[3] H. Buchner, L. Adams, A. Muller, I. Ludwig, A. Knepper, A. Thron, K. Niemann, and M. Scherg.
Somatotopy of human hand somatosensory-evoked potentials and 3D-NMR tomography. Electroenceph.
clin. Neurophys., 96(2):121–134, 1995.

[4] F. Mauguiere, I. Merlet, N. Forss, S. Vanni, V. Jousmaki, P. Adeleine, and R. Hari. Activation of a
distributed somatosensory cortical network in the human brain. a dipole modelling study of magnetic
ﬁelds evoked by median nerve stimulation. 1. location and activation timing of sef sources. Electroenceph.
clin. Neurophys., 104(4):281–289, 1997.

[5] M. Hoshiyama, R. Kakigi, S. Koyama, S. Watanabe, and M. Shimojo. Activity in posterior parietal cor-
tex following somatosensory stimulation in man: Magnetoencephalographic study using spatio-temporal
source analysis. Brain Topography, 10(1):23–30, 1997.

[6] M.S. H¨am¨al¨ainen and R.J. Ilmoniemi. Interpreting measured magnetic ﬁelds of the brain: Estimates of

current distributions. Technical Report TKK-F-A559, Helsinki University of Technology, 1984.

[7] M.S. H¨am¨al¨ainen and R.J. Ilmoniemi. Interpreting magnetic ﬁelds of the brain: minimum norm esti-

mates. Med. & Biol. Eng. & Comput., 32:35–42, 1994.

[8] A.A. Ioannides, J.P.R. Bolton, R. Hasson, and C.J.S. Clarke. Localised and distributed source solutions
for the biomagnetic inverse problem II. In S.J.Williamson, M.Hoke, G.Stroink, and M.Kotani, editors,
Advances in Biomagnetism, pages 591–595, New York, August 1989. Plenum Press.

12

[9] J.Z. Wang, S.J. Williamson, and L. Kaufman. Magnetic source images determined by a lead-ﬁeld analysis
- the unique minimum-norm least-squares estimation. IEEE Transactions on Biomedical Engineering,
39:665–675, 1992.

[10] R.D. Pascual-Marqui. Low resolution brain electromagnetic tomography. Brain Topography, 7:180,

1994.

[11] I.F. Gorodnitsky, J.S. George, and B.D. Rao. Neuromagnetic source imaging with focuss: a recursive

weighted minimum norm algorithm. Electroenceph. clin. Neurophys., 95:231–251, 1995.

[12] W. Skrandies (Ed.). Extended discussion of LORETA. International Society for Brain Electromagnetic

Topography Newsletter, 6, ISSN 0947-5133, December 1995.

[13] C. Wood (ed.). Workshop on the meg inverse problem.

In Advances in Biomagnetism Research:

Biomag96, (Eds: C. Aine et al) Springer-Verlag, New York, In press, 1998.

[14] S. Baillet and L. Garnero. A Bayesian approach to introducing anatomo-functional priors in the

EEG/MEG inverse problem. IEEE Trans. Biomed. Eng., 44(5):374–385, 1997.

[15] D. M. Schmidt, J.S. George, and C.C. Wood. Bayesian inference applied to the electromagnetic inverse

problem. Preprint la-ur-97-4813, Los Alamos National Laboratory, 1997.

[16] R. Hasson and S.J. Swithenby. A quantitative analysis of the EEG and MEG inverse problems. In
C. Baumgartner, L. Deecke, G. Stroink, and S.J. Williamson, editors, BIOMAGNETISM: Fundamental
research and clinical applications, pages 455–457, Amsterdam, August 1993. Elsevier Science.

[17] R. Hasson and S. J. Swithenby. Aspects of non-uniqueness: how one source aﬀects the reconstruction
of another. In Advances in Biomagnetism Research: Biomag96, (Eds: C. Aine et al) Springer-Verlag,
New York, In press, 1998.

[18] A.A. Ioannides, J.P.R. Bolton, and C.J.S. Clarke. Continuous probabilistic solutions to the biomagnetic

inverse problem. Inverse Problems, 6:523–542, 1990.

[19] J.E.T. Knuutila, A.I. Ahonen, M.S. H¨am¨al¨ainen, M.J. Kajola, P.P. Laine, O.V. Lounasmaa, L.T.
Parkkonene, J.T.A Simola, and C.D. Tesche. A 122-channel whole cortex SQUID system for mea-
suring the brain’s magnetic ﬁeld. IEEE Trans. Mag., 29 (6):3315–3320, 1993.

[20] C.J.S. Clarke. Probabilistic modelling of continuous current sources. In J.Nenonen, H.-M.Rajala, and
T.Katila, editors, Biomagnetic Localisation and 3D modelling, pages 117–125, Helsinki, May 1991.
Helsinki University of Technology, Dept. of Technical Physics.

[21] C.J.S. Clarke and B.S. Janday. The solution of the biomagnetic inverse problem by maximum statistical

entropy. Inverse Problems, 5:483–500, 1989.

[22] P.C. Hansen. Regularization tools. Numerical Algorithms, 6:1–35, 1994.

[23] S.J. Swithenby, A.J. Bailey, S. Br¨autigam, O.E. Josephs, V. Jousm¨aki, and C.D.Tesche. Neural pro-
cessing of human faces: a magnetoencephalographic MEG study. Expt. Brain Res., 118:501–510, 1998.

[24] S.T. Lu, M.S. H¨am¨al¨ainen, R. Hari, R.J. Ilmoniemi, O.V. Lounasmaa, M. Sams, and V. Vilkman.
Seeing faces activates three separate areas outside the occipital visual cortex in man. Neuroscience, 43
(2/3):287–290, 1991.

[25] E. Halgren, T. Raij, K. Marinkovic, V. Jousmaki, and R. Hari. Magnetic ﬁelds evoked by faces in the

human brain: 1. topography and equivalent dipole locations. Society for Neuroscience, 21:562, 1995.

[26] B.S. Janday and S.J. Swithenby. The use of the symmetric sphere model in magnetoencephalographic
analysis. In S.N. Erne and G.L. Romani, editors, Advances in Biomagnetism Functional Localization:
A challenge for Biomagnetism, pages 153–160, London, 1989. World Scientiﬁc.

13


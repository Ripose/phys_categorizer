1
0
0
2
 
r
p
A
 
4
 
 
]
h
p
-
d
e
m

.
s
c
i
s
y
h
p
[
 
 
1
v
0
2
0
4
0
1
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Information preserved guided scan pixel diﬀerence coding for medical images

Kunio Takaya
Electrical Engineering, University of Saskatchewan, Saskatoon, CANADA

C. Tannous∗ and Li Yuan
Electrical Engineering, University of Saskatchewan, and TRLabs, Saskatoon, CANADA
(Dated: March 28, 2001)

This paper analyzes the information content of medical images, with 3-D MRI images as an
example, in terms of information entropy. The results of the analysis justify the use of Pixel
Diﬀerence Coding for preserving all information contained in the original pictures, lossless coding
in other words. The experimental results also indicate that the compression ratio CR=2:1 can
be achieved under the lossless constraints. A pratical implementation of Pixel Diﬀerence Coding
which allows interactive retrieval of local ROI (Region of Interest), while maintaining the near low
bound information entropy, is discussed.

Keywords: pixel diﬀerence coding, information preserved coding, entropy, auto-correlation, his-

togram, MRI (Magnetic Resonance Imaging).

I.

INTRODUCTION

This research attempts at developing an information
preserving image coding suitable for handling medical
images which usually do not allow any degree of image
degradation in comparison with the source images [1, 3].
JPEG, one of the well accepted compression standards,
exhibits an excellent compression gain, but does not
insure that the decoded result is the exact replica of
the original. A variation of DPCM or pixel diﬀerence
coding, namely ”Guided Scan Pixel Diﬀerence Coding
was studied. A major modiﬁcation to the classical
in terms
DPCM is the inclusion of scan information,
of the direction used to calculate the pixel diﬀerence
information does not
and step size. This additional
signiﬁcantly increase the volume of coded data.
It
actually opens the extensibility for the DPCM to be able
in a similar
to handle multi-dimensional image data,
manner as JPEG is extended to MPEG to handle movie
presentation. Another important asp ect of medical
image coding, which is the ability to access the full
details of a local image (often referred to as ROI, Region
of Interest), is also satisﬁed. Neither the location of a
ROI nor the scanning method is restricted so that a local
area of ROI can be selected anywhere in the original
image. The ﬂexibility in scanning makes it possible to
use the same code for real time movie presentation in the
same way MPEG uses JPEG. This research considers
two stage redundancy removal, one by taking pixel
diﬀerences and the other by an entropic source coding
similar to the Huﬀman coding to achieve a redundancy
removal of approximately 4-5 bpp (bits per pixel).

∗Present address: Laboratoire de Magntisme de Bretagne, UPRES
A CNRS 6135, Universit de Bretagne Occidentale, BP: 809 Brest
CEDEX, 29285 FRANCE

The sizes of medical images have increased with the
advancement in various medical
imaging modalities,
typically in MRI and X-ray CAT scan. MRI can scan
a whole body within a reasonable time of 30 minutes
and X-ray CAT has adopted helical scanning for a
higher spatial resolution.
In the case of 3D scanning,
the image size could be as large as 100 million pixels.
Images of this size are usually stored on writable laser
disks (CD-WORM), but this imposes serious problems
when the retrieval of such data is attempted through a
communication channel in the wide area network (WAN)
environment using 64 kbits/sec bit-rate telephone chan-
nels.

The problems associated with the compression of med-
ical images are two fold. The ﬁrst requirement is that
compression must be absolutely non-degradable. Regard-
less of whether a picture has been aﬀected by measure-
ment noise that occurs in the process of physical mea-
surement and in image reconstruction, the source data
acquired at an imager must be retained without any kind
of losses. The second condition is the integrity of the ob-
jects contained in a picture data. Physicians examine
very closely the image in a ROI (Region of Interest) or
a number of ROI’s. A selected ROI needs to be imme-
diately accessible and it must be translated into a serial
stream of compressed data for transmission via a com-
munication channel.

II. TECHNICAL BACKGROUND

The method is best represented by the name Guided
Scan Pixel Diﬀerence Coding [3], a variation of DPCM
widely used in compressing voice signals. It is well known
that a PCM voice signal of 6-8 bits can be compressed
down to a 3 bit DPCM. Even discounting the fact that
DPCM uses a nonlinear quantization scale having a
greater step size for a larger signal level, it is easy to

remove 3 bits per code word, reducing the total volume
down to half of the original.
In comparison, JPEG
performs a compression rate of 10 and is better than
DPCM, because it permits the encoded image to degrade
somewhat from the original. DPCM is a non-degradable
source coding if the quantization step size is ﬁxed to the
same linear scale as already done in the source images.
Furthermore, the encoding/decoding method is basically
the simple operation of subtraction/summation. Further
compression to remove the redundancy leftover from
DPCM is accomplished by an entropy coding such as
Huﬀman code.

In order to satisfy the other aspect of medical image
compression, accessibility to a ROI, additional informa-
tion describing how an individual code representing a
pixel diﬀerence is derived, must be attached at the ex-
pense of compression gain.
In the case of 2D images,
there are four (up, down, left and right) or eight (if di-
agonal diﬀerences are considered) possible directions by
which a pixel diﬀerence can be calculated.
If an ROI
is speciﬁed by a person viewing an image specifying sev-
eral guide marks surrounding the ROI, the length of each
raster along a speciﬁed scan direction must be registered.
Also, the step size used to calculate the pixel diﬀerence
must be included, if coarsely sampled images need to be
transmitted for viewing an overall image at a reduced im-
age quality. These additional pieces of information could
be 4 bits for directions and about 3 bits to indicate the
step size per picture. Information regarding a length of
scanning depends on how irregular a shape is set to be
an ROI. Nevertheless, no signiﬁcant increase in the total
volume of coded words is expected. The term ”Guided
Scan is meant to include the information as to how an
image or a ROI is scanned. The way of scanning an image
should be left for the image coding system to determine,
so that the total volume of information can be minimized
for individual selected ROI’s. Another interesting aspect
of the Guided Scan DPCM is the structural similarity
observed in the forward or backward diﬀerence interpo-
lation algorithm. Since the tree of high order diﬀerences,
necessary for the interpolation, can be calculated easily
from the ﬁrst order diﬀerences coded by the Guided Scan
DPCM, there is potential to artiﬁcially increase the spa-
tial resolution of the source image without distorting the
image.

I. If eight bits are used to code an image and the proba-
bility for any number between 0 and 255 to occur is the
same, the information entropy is 8. If only one value, say
128, occurs all times, the entropy is zero. Adjacent pixels
in a digitized image are highly correlated. If two adjacent
pixels are considered, the probability for the ﬁrst pixel x
to take a value i, that for the second pixel y to take j,
and the joint probability for the pixel x to take a value i
and the pixel y to take j are given respectively as follows:

pi =

, pj =

and pij =

(2)

Ni
N

Nj
N

Nij
N

The entropies based on the joint probability and the

conditional probabilities are then obtained as follows:

H(x, y) = − X
i,j

pij log2pij

H(x|y) = − X
i,j

pij log2

H(y|x) = − X
i,j

pij log2

pij
pj
pij
pi

We also know that H(x, y) = H(x) + H(y|x) = H(y) +

H(x|y). Since theoretically:

H(y|x) − H(y) ≤ 0

(4)

The entropy calculated for the second pixel y know-
ing the occurrence of the ﬁrst pixel x is smaller than the
entropy calculated from y alone. The uncertainty coeﬃ-
cient of y

U (y|x) =

H(y) − H(y|x)
H(y)

is indicative of the dependency of y on x. The sym-

metrical uncertainty:

U (x, y) =

H(x)U (x|y) + H(y)U (y|x)
H(x) + H(y)

yields 0 if x and y are completely independent and 1 if

they are completely dependent.

2

(3)

(5)

(6)

III.

INFORMATION ENTROPY

IV. ENTROPY CALCULATED FROM
STRETCHED EXPONENTIAL PDF

The information entropy is deﬁned by:

H = −

pi log2pi

(1)

I

X
i=1

When the probability density function (PDF), ob-
tained from a histogram of pixel intensities, is well ap-
proximated by the stretched exponential probability den-
sity function [5, 6, 7]:

where pi is the probability associated with the occur-
rence of a value i when the total number of values used is

p(x) = Ke−( |x|
α )

β

with K =

(7)

β
2αΓ( 1
β )

the theoretical entropy can be calculated from:

H = 2K

α
β

(−lnK)Γ( 1

β ) + Γ(1 + 1
β )
ln2

After evaluating the absolute mean µ and standard
deviation σ, the parameters α and β are extracted from
the equation [2]:

where, the function F is deﬁned by:

and α is deﬁned by:

F (β) =

µ2
σ2 + µ2

F (x) =

2
Γ( 2
x )
x )Γ( 3
x )

Γ( 1

α =

(σ2 + µ2)Γ( 1
β )
Γ( 3
β )

Fig. 1 depicts the variation of the entropy H as a
function of α in the interval [0.1-5] for a given β picked
in the interval [0.8-1.5].

V. PRELIMINARY STUDIES ON
INFORMATION ENTROPY

A 3-D MRI brain section image which consists of 64
slices was analyzed with an attempt to ﬁnd the lower
bound of the information entropy for medical images [3].
The lower bound is a deﬁnite measure that tells exactly
how many bits can be removed per pixel as redundancy.
One slice of the 3-D image is shown in Fig. 2. Two
diﬀerent approaches are taken to calculate the entropies
for the image shown in Fig. 2.

1. Use Eq. 1 which does not consider the dependency
between adjacent pixels for n-th order diﬀerence
images (n = 0,...,10).

2. Use Eq. 3 which takes the dependency of a pixel on
its immediate neighbours into account (equivalent
to a ﬁrst order Markov model).

The ﬁrst approach ﬁnds a crude information entropy,
if it is applied to the original picture since the redun-
dancy due to pixel-to-pixel correlation is still intact. In
order to ﬁnd a more accurate information entropy, it is
necessary to remove the redundancy by some means that
allows the recovery of the original image. A diﬀerence
image is introduced for this purpose. When a 2-D image
is denoted by A and z−1 represents the one-step shift op-
erator towards the positive side of the horizontal axis (or
vertical or diagonal axis), the ﬁrst diﬀerence is:

3

D1 = A − z−1A = (1 − z−1)A

(12)

The second diﬀerence is given by

2
D2 = (1 − z−1)D1 = (1 − z−1)

A

(13)

Thus the n-th diﬀerence is given by

n
Dn = (1 − z−1)

A

(14)

In order to make reconstruction possible, the ﬁrst col-
umn of the ﬁrst diﬀerence must retains the ﬁrst column
of the original if horizontal shift is used. The second
diﬀerence must retain the ﬁrst column of the original
and the second column of the ﬁrst diﬀerence in its sec-
ond column. Additional DC restoration columns must
be progressively added when a new diﬀerence image is
created. Successive applications of this diﬀerence oper-
ator remove the pixel-to-pixel correlation and resulting
images become gradually more random. Table 1 shows
how the entropy associated with such a diﬀerence image
varies when n is increased.

(8)

(9)

(10)

(11)

Diﬀerence image Entropy
5.615405
4.861321
5.448454
6.240144
7.103492
7.997103
8.908238
9.813683
10.725728
11.610693
12.468448

n =0
n=1
n=2
n=3
n=4
n=5
n=6
n=7
n=8
n=9
n=10

TABLE I: Entropies (bits/pixel) of Diﬀerence Images

2-D auto-correlation functions for the diﬀerence im-
ages of n= 0, 1 and 2 are shown in Fig. 3. The top
ﬁgure is the 2-D auto-correlation of the original image.
Comparing the middle n =1 and the bottom n =2, it is
observed that the central peak of n=2 is sharper (less
correlated) than that of n=1. As for the entropies calcu-
lated, the entropy drastically drops to the minimum at
the very ﬁrst diﬀerence operation then it starts increasing
as n increases. Successive diﬀerence operations seem to
decorrelate the image and make it more random, but the
entropy monotonously increases after the ﬁrst diﬀerence.
This phenomenon can be explained from the frequency
response of the n-th diﬀerence operation described by the
transfer function of a high pass ﬁlter,

n
G(z) = (1 − z−1)

(15)

Since the magnitude response is given by:

|G(ejω)| = 2nsinn(

where 0 ≤ ω < π

(16)

ω
2

)

and the bandwidth becomes wider as n increases. The
power of the n-th diﬀerence image is greater than that
of (n-1)th diﬀerence image, and so is the variance.

The second approach using Eq. 3 considers the depen-
dency between adjacent pixels in calculating information
entropies so that it is no longer necessary to decorrelate
images. Since the ﬁrst diﬀerence achieves the minimum
entropy value, Eq. 3 is applied to the image of the ﬁrst
diﬀerence. Table 2 summarizes the results. The entropy
H(x, y) based on the joint probability pij is divided by
2 to translate it into the entropy per pixel. This value is
signiﬁcantly smaller than H(x) or H(y).

Another interesting result is found by ﬁtting the
stretched exponential PDF [4] to the histogram of the
original image shown in Fig. 4. The entropy for the large
peak located in the lower gray scale range of < 50 is cal-
culated to be 2.749977 without including the range > 50.
This is nearly one half of the total entropy found for the
original image. A whole picture of a medical image usu-
ally contains a signiﬁcant portion of dark background.

Entropy
H(x)
H(y)

value
4.861343
4.861343
H(x, y) 9.159570
H(x, y)/2 4.579785
4.298226
4.298226

H(y|x)
H(x|y)

TABLE II:
Pixel-to-pixel Correlation

Information Entropies Considering Adjacent

VI. DISCUSSION

Reviewing the experimental results presented in the
previous sections, some conclusive remarks can be made
to determine the strategy for information preserving
source coding. Information preserving coding or lossless
coding generally means that the picture received is
the picture sent in terms of the bit structure of the
image, not in terms of the visual
impression of the
image before and after image compression/transmission.
In this narrow sense of lossless coding, altering pix-
els is prohibited. No alterations in quantization are
permitted. With these strict constraints, the only
possible source of redundancy that could be removed is
limited to the pixel-to-pixel correlation. Pixel diﬀerence
coding (DPCM) alone brings the information entropy

4

down to its near minimum. As seen in the entropy
H(x, y), the result of the pixel diﬀerence coding can be
further trimmed but not to a large extent. The MR
images analyzed are all 8 bit images. According to
the sample calculations shown in this paper and other
tested results, a bare-bone information entropy per
pixel is slightly greater than 4 bits/pixel. An optimistic
compression ratio CR is therefore CR=2:1, as long
as a near optimum compression algorithm, typically
Huﬀman coding, is used. Blending the Huﬀman code
and the codes which consider state transitions that
frequently occur within a near zero range of the pixel
diﬀerence scale, a small
improvement in compression
ratio will be made. For example, a run of successive
zeros up to a length of 10 can be treated as a code word
if the frequencies of such occurrences are suﬃciently high.

Further improvement of CR requires removing a con-
straint on quantization. As used in DPCM in voice signal
coding, there are several methods to set up a quantiza-
tion scale which minimizes the information entropy. If
the methods of transform coding are allowed, CR can be
improved drastically. Approaches to control the quality
of medical images, for example texture, appearance of
speckles, maintaining repeated basic patterns as fractal
images do, etc...
If one coding method can assure the
ﬁdelity of a certain image attribute, it might be consid-
ered as a better coding scheme. The medical community
may be prepared to accept it as a better replacement for
the totally reconstructible information preserving coding.

If some loss is permitted in medical image coding, the
n-th order diﬀerence images discussed in this paper has
another potential application for image coding. Recall-
ing that the power spectrum S(ωx, ωy) of an n-th order
diﬀerence image Dn(x, y) relates to its auto-correlation
R(x, y) with the 2-D Fourier transform,

S(ωx, ωy) = F R(x, y)

= {F Dn(x, y)}{F Dn(x, y)}∗

(17)

it is apparent that the magnitude information of
Dn(x, y) is contained in the highly concentrated peak of
R(x, y). Since the phase information is lost in calculating
R(x, y), it is necessary to preserve the phase of Dn(x, y).
The auto-correlation of 6 F Dn(x, y) shown in Fig.
5
indicates that the magnitude of phase is also highly
concentrated at zero of the 2-D phase auto-correlation.
The n-th order diﬀerence image produces a highly
concentrated auto-correlation both in magnitude and
in phase. By preserving the proﬁle of the peak and
discarding the rest, it seems possible to achieve a high
compression gain.
It is however known that image
degradation is usually enhanced by inaccurate phase
estimation.

Acknowledgement Li Yuan acknowledges support
through a TRlabs fellowship. C. Tannous acknowledges

support from NSERC president’s fund grant.

5

[1] Saghri J. A., Techer, A. G. and Reagan, J. T., Practical
Transform Coding of Multispectral Imagery, IEEE Sig.
Proc. Magazine, vol. 12, no. 1, 1995.

[2] Press, W. H., Flannery, B. P., Teukolsky, S. A. and Vetter-
ling W. T., Numerical Recipes in C , Cambridge University
Press, 1989.

[3] Takaya, K., Sarty, G. and Li, X., Multiresolution 2-
Dimensional Edge Analysis Using Wavelets, IEEE Wes-
canex 93 Conf. Proc., Saskatoon, May, 1993.

[4] Daubechies,

I., Ten Lectures on Wavelets, SIAM,

Philadelphia, 1992.

[5] Mallat, S. and Zhong, S., Wavelet Maxima Representa-
tion, Wavelets and Applications, Y. Meyer Ed., Springer-
Verlag, 1991.

[6] Mallat, S. and Zhong, S., Characterization of Signals from
Multiscale Edges, IEEE Trans. Patt. Analysis and Ma-
chine Intell., vol. PAMI-14, no. 7, July 1992.

[7] Mallat S., A theory of Multiresolution signal decomposi-
tion: the Wavelet representation IEEE Trans. Patt. Anal-
ysis and Machine Intell., vol. PAMI-11, 674- 693, 1989.

Fig. 1: Theoretical entropy calculated for stretched expo-
nential probability density functions. The graphs
show the entropy versus α with β as a parameter.

Fig. 2: A Slice of a 3-D MRI Head Image (256x256)

Fig. 3: 2D Auto-correlation, original, 1st diﬀerence, 2nd
diﬀerence image and phase image of the original
(from top to bottom). Top vertical scale is in units
of 107 , middle is in units of 105 whereas bottom
part is in 106 units.

Fig. 4: Histograms for the original, 1st diﬀerence and 2nd

diﬀerence images.

Fig. 5: Auto-correlation of the phase image. Vertical scale

is in units of 104.

Figure Captions

This figure "fig1.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/physics/0104020v1

This figure "fig2.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/physics/0104020v1

This figure "fig4.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/physics/0104020v1

This figure "fig5.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/physics/0104020v1


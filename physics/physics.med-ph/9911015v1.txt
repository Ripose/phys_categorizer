Is the best estimate of power equal to
the power of the best estimate?

R Hasson

Department of Applied Mathematics, The Open University, Milton Keynes, United
Kingdom

Abstract.
In an inverse problem, such as the determination of brain activity given
magnetic ﬁeld measurements outside the head, the main quantity of interest is often
the power associated with a source. The ‘standard’ way to determine this has been
to ﬁnd the best linear estimate of the source and calculate the power associated with
this. This paper proposes an alternative method and then relationship to this previous
method of estimation is explored both algebraically and by numerical simulation.

In abstract terms the problem can be stated as follows. Let H be a Hilbert space
with inner product h , i. Let L be a linear map: H → Rn
. Suppose that we are given
data b ∈ Rn
such that b = Lx + e where e is a vector of random variables with zero
mean and given covariance matrix which represents measurement errors. The problem
that is addressed in this paper is to estimate hx,
X is an operator on H
(e.g. the characteristic function of a region of interest).

Xxi where

b

b

KEYWORDS: Linear inverse problem, biomagnetic inverse problem, magnetoen-

cephalography (MEG).

AMS classiﬁcation scheme numbers: 65J20, 92C55, 65R30.

Submitted to: Inverse Problems

9
9
9
1
 
v
o
N
 
0
1
 
 
]
h
p
-
d
e
m

.
s
c
i
s
y
h
p
[
 
 
1
v
5
1
0
1
1
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Best estimate of power

1. Introduction

2

This paper solves a problem that arose in the study of the inverse problem in
magnetoencephalography (MEG) [1, 2]. The dominant concern in MEG analysis has
been to produce source maps of current density in the brain and to co-register these to
anatomical data (e.g. [1, 3]). However, this may not be the most appropriate approach
when there is a focus on speciﬁc source regions in the brain, e.g. the thalamus, fusiform
gyrus etc. In these cases it may be more appropriate to generate an activation curve,
a graph of the power dissipated in a speciﬁed region as a function of time. Several
methods of generating activation curves have been proposed (e.g.
[4, 5, 6]). This aim
of this paper is to derive an algorithm for generating activation curves that is optimal
with respect to the L2-norm.

Another argument for the use of activation curves is the direct comparison with
other functional brain imaging modalities such as positron emission tomography (PET)
and functional magnetic resonance imaging (fMRI). These modalities produce images
of quantities, e.g. regional cerebral blood ﬂow (rCBF), that are correlated with power
dissipated rather than current density. This suggests that in order to compare results
across modalities we should use magnetic ﬁeld data to produce an estimate of the power
dissipated, i.e. an activation curve.

b

In Section 2 a more general problem is solved in the setting of a linear map from a
Hilbert space to a ﬁnite dimensional Hilbert space. The main result from Section 2 (i.e.
Equation 16) can be applied independently to each time instant of the data from a MEG
experiment. The method proposed is to ﬁnd a matrix Y such that bT Y b approximates
Xxi (T denotes matrix transposition). The derivation of the optimal matrix Y
hx,
(Equation 16) with respect to the L2-norm is contained in Section 2. Section 3 goes on
to compare the main results of Section 2 with the na¨ıve algorithm which ﬁrst computes
an estimate, xreg, using Tikhonov regularization and then computes hxreg,
Xxregi.
This algorithm was used in [4] to extract measures of brain activity.
b

In Section 4 we specialize to the study of the MEG inverse problem. Deﬁnitions
appropriate to this application are introduced and a simulation study is described. In
Section 5 an important special case is considered where the region of interest is the
whole brain. A simpliﬁed equation (Equation 32) for this case is derived and this is
compared with the total signal power which is commonly used as an estimate of brain
activity. Section 6 is a discussion of the merits of the algorithm together with the issues
to be addressed before applying the method in practice.

2. Methods

Let H be a Hilbert space with inner product h , i. Let L be a linear map: H → Rn.
Suppose that we are given data b ∈ Rn such that

b = Lx + e

(1)

Best estimate of power

3

where e is an unknown vector of random variables with zero mean and covariance matrix
C which represents measurement error. Suppose that the problem of ﬁnding an x ∈ H
corresponding to a b ∈ Rn is an ill-posed problem. The problem here is to estimate
hx,

X is an operator on H.

Xxi where

b

It should be noted that no assumptions are made about the noise in the
b
measurement channels other than it has zero mean and a well deﬁned covariance matrix
C, i.e. if the measurement noise is denoted by a vector e then the covariance matrix is
deﬁned by Cij = eiej where

denotes an expectation value.

Now deﬁne the adjoint map L† by

hx, L†bi = (Lx)T b,

for all x ∈ H, b ∈ Rn.

(2)

Here we are concerned with the image space I of L†. Let {
basis of Rn and choose a corresponding basis of I, {ψi : i = 1 . . . n}, where ψi = L†

ei : i = 1 . . . n} be the usual

ei.

The matrix Y will be chosen to minimize the error for points in I. The starting point
in choosing an optimal matrix Y is to derive a suitable cost function to be minimized.
We start by expanding bT Y b.

b

b

bT Y b = (Lx + e)T Y (Lx + e) = (Lx)T Y Lx + eT Y Lx + (Lx)T Y e + eT Y e(3)

n

As mentioned above we focus on points in I ⊆ H, so we express x ∈ I in terms of our
i=1 aiψi, where ai ∈ R are scalars which will be written collectively as a
basis: x =
vector a. Equation 3 can be simpliﬁed because the expression Lx appears repeatedly,
so start by simplifying this expression:

P

(Lx)T

ej = hx, L†

eji = h

aiψi

, ψji =

aihψi, ψji.

n

(cid:16)

Xi=1

(cid:17)

n

Xi=1

b

b

The right hand side of Equation 4 can be written as the jth component of a product
P a where Pij = hψi, ψji. Note that P is a symmetric positive deﬁnite n × n matrix.
Substituting for Lx in Equation 3 gives:

bT Y b = aT P Y P a + eT Y P a + aT P Y e + eT Y e.

X onto I has a matrix representation with respect to the
The projection of the operator
Xψji where i, j = 1, . . . , n. Hence the target expression
basis {ψi} deﬁned by Xij = hψi,
b
can be written in terms of the vector a:

b

hx,

Xxi = aT Xa,

where x =

aiψi.

n

Xi=1

b

For Y to be a good estimator, the right hand sides of Equations 5 and 6 should be ‘close’
for all a ∈ Rn. One way of achieving this is to minimize the cost function E deﬁned by:

E = kX − P Y P k2

2 + keT Y P k2

2 + kP Y ek2

2 + keT Y ek2
2.

where k k2 is the L2-norm. Equation 7 can be interpreted in physical terms. The ﬁrst
X by Y . The second and third terms
term is the error in approximating the operator
give a measure of the overlap,induced by Y , between the measurement error and the

b

(4)

(5)

(6)

(7)

Best estimate of power

imaging space, I. Note that these terms are equal for a symmetric Y . The fourth term
is a measure of how Y magniﬁes the measurement error.

To minimize E, ∂E/∂Yik is derived for each element of the matrix Y . This gives
N 2 equations to solve for the N 2 unknowns Yik. These may be written as a single
matrix equation. In order to illustrate the manipulations involved, the method will be
elaborated for the fourth term in Equation 7. The fourth term is expanded using the
deﬁnition of the L2-norm:

keT Y ek2

2 =

eαYαβeβ

(cid:16) Xα,β

2

.

(cid:17)

This is diﬀerentiated to obtain:

∂keT Y ek2
2
∂Yik

= 2

eαYαβeβ

eiek = 2

eieαYαβeβek.

(cid:16) Xα,β

(cid:17)

Xα,β

We proceed by replacing the products of random variables with their expectation values,
i.e. eieα = Ciα and eβek = Cβk:

∂keT Y ek2
2
∂Yik

= 2

CiαYαβCβk.

Xα,β

This is the ikth term of the matrix product CYC. Similarly, all of the other terms in
Equation 7, when diﬀerentiated, give terms that can be written as the ikth elements of
a product. So, the equations can be collected as:

− 2P XP + 2P 2Y P 2 + 2P 2Y C + 2CY P 2 + 2CY C = 0.

(11)

This may be written in the form:

(P 2 + C)Y (P 2 + C) = P XP.

(12)
This equation can be solved in many ways, for example by deﬁning Z = Y (P 2 + C) and
solving for Z ﬁrst and then for Y . This easily implemented procedure was rejected as
it computes an non-symmetric Y when starting with a symmetric matrix X, because
of the numerical problems associated with ill-conditioned matrices. So an alternative
scheme which preserves symmetry was devised. Let λi be the eigenvalue of the matrix
P with eigenvector φi. Then the matrices X and C can be represented with respect to
the basis {φi} as new matrices X ′ and C ′, i.e.

With these deﬁnitions, the matrix Y can be ﬁnally expressed as:

X =

C =

Xik

Xik

φiX ′

ikφT
k ,

where X ′

ik = φT

i Xφk,

φiC ′

ikφT
k ,

where C ′

ik = φT

i Cφk.

Y = (P 2 + C)−1P

φiX ′

ikφT
k

P (P 2 + C)−1

(cid:17)

(cid:16) Xik
λiλkφi(C ′+ λ2

=

Xik

i I)−1X ′

ik(C ′+ λ2

kI)−1φT

k

4

(8)

(9)

(10)

(13)

(14)

(15)

(16)

Best estimate of power

input symmetric matrix X.

The matrix Y computed using the above formula is always symmetric for a given

Frequently the covariance matrix C is not known and the assumption is made that
the random variables ei are independent Gaussian random variables with a variance ζ
that is considered to be a parameter of the method. With this assumption C = ζI and
Equation 16 becomes:

5

(17)

Y =

Xik

λi
λ2
i + ζ

λk
λ2
k + ζ

φiX ′

ikφT
k

3. Comparison with na¨ıve method

We now compare Equation 17 with the corresponding equation derived by the na¨ıve
method mentioned in the introduction. The na¨ıve method for computing hx,
Xxi is to
compute a minimum norm estimate using Tikhonov regularization to get xreg and then
compute the inner product.

b

To compute a xreg the ﬁrst step is to choose a ﬁnite dimensional subspace R ⊆ H
that has an orthonormal basis {rα : α = 1, . . . m}. The subspace R will be called the
representation space and the regularized solution xreg will lie in this space. The linear
map L : H → Rn deﬁnes a linear map from R to Rn by restriction that we will also call
L.

Now compute a singular value decomposition of L : R → Rn as L = UΣV T , where
Σ is a diagonal matrix with non-negative entries σ1, σ2, . . . σn and U and V are matrices
with orthonormal columns, i.e. U T U = V T V = I. Applying Tikhonov regularization
[7] to the inverse problem gives xreg = V DU T b, where D is a diagonal matrix given by
D = (Σ2 + ζI)−1Σ. So the power dissipated by this source can be computed by:

hxreg,

V DU T b
(cid:1)
where X is the matrix representation of the operator
The right hand side of Equation 18 is of the form bT

Xxregi =
b

bT UDV T

X

(cid:1)

(cid:0)

(cid:0)

,

X on R, i.e. Xαβ = hrα,
Y is deﬁned to be:

Y b where
b

Y = UDV T X V DU T

e

e

(18)

Xrβi.

b
(19)

e

The comparison with the method in the previous section relies on the relationship
between the linear operator L and the Gram-Schmidt matrix P that we will now derive.
Suppose for a moment that the representation space R was the whole of H and that the
basis {rα} is a complete orthonormal basis for R = H. In this case:

Pij = hψi, ψji =

hψi, rαihrα, ψji,

by completeness,

(20)

hL†

ei, rαihrα, L†

eji,

using the deﬁnition of ψi,(21)

=

=

=

Xα

Xα

Xα
eT
i L

b

b

b

i (Lrα)(Lrα)T
eT

b
ej,

rαrT
α

LT
(cid:17)

(cid:16) Xα

b
ej,

b

using the deﬁnition of L†,(22)

by linearity,

(23)

Best estimate of power

=

j LLT
eT

ei.

6

(24)

b

b

The right hand side of this equation is the ijth component of the matrix product LLT .
So under the assumption that {rα} is a complete orthonormal basis for H then P = LLT .
Now returning to the case when R ⊂ H we can see that for a good choice of R the
P deﬁned to be LLT will be approximately equal to P . This is not surprising
matrix
since to compute the Gram-Schmidt matrix P on a computer one usually takes a suitable
representation space R and computes LLT . The singular value decomposition of L
immediately gives an eigenvalue decomposition of
P = LLT = UΣV T V ΣU T = UΣ2U T ,

P since

(25)

e

e

where the last equality follows from the fact that the columns of V are orthonormal. So
the matrix

φi given by the columns of U.

P has eigenvalues σ2

i with eigenvectors,

e

By a similar argument to the above it can be seen that the matrix

X ′ deﬁned to

e

e

be V T X V approximates the matrix X ′ so we have:

Y = UD

X DU T =

e

e

σi
σ2
i + ζ

σk
σ2
k + ζ

Xik

φi

X ′
ik

φT
k ,

e

e

e

e

(26)

e

e

φi ≃ φi,

Now we can compare Equation 26 with Equation 17. For a good representation space
X ′ ≃ X ′ and so the major diﬀerence between the two approaches
R we have
is that λi ≃ σ2
i . The eﬀect of this change can be seen by plotting out the graphs of
the functions on the interval [0, 1] (this is the only range of interest since we could
dividing by the largest singular value restrict to this interval). These graphs are shown
in Figure 1 where it can be seen that Equation 17 attenuates the contribution from the
small singular values and has a sharper cut-oﬀ than is the case for Equation 26. The
eﬀect of this is that Equation 17 should attenuate the noise component, which is usually
associated with the small singular values.

1.0

y

0.5

0.0

0.0

0.5

x

1.0

Figure 1. Graphs of the functions x/(x2 + ζ) (solid curve) and x2/(x4 + ζ) (dashed
curve) for ζ = 0.5.

Best estimate of power

4. Application

7

(28)

Now we apply our results to the MEG inverse problem, i.e. the problem of recovering
information about source current density inside the brain given measurements of the
magnetic ﬁeld outside the brain. Let Ω denote the brain volume. The Hilbert space of
interest to us is, L2(Ω), the space of square integrable vector ﬁelds deﬁned on the brain
volume Ω together with the inner product:

h~j1,~j2i =

~j1(~r) · ~j2(~r)
ω(~r)

ZΩ

d~r,

for all ~j1,~j2 ∈ L2(Ω).

(27)

The factor ω(~r) is a weighting factor that allows some ﬂexibility in the procedure. The
only restriction imposed on ω(~r) is that the integral over each voxel is ﬁnite. In other
papers the factor ω(~r) has been interpreted as a probability weight [8].

It is interesting in this context to look at the the spatial selectivity implicit in the
use of the matrix Y as it varies in source space. Then the sensitivity proﬁle of Y at a
point in source space, ~r0, is deﬁned to be

I(~r0) =

(L~d i

~r0)T Y (L~d i

~r0),

3

Xi=1

~r0 is the current dipole distribution, i.e. ~d i

where ~d i
is an orthogonal set of unit vectors and δ( ) denotes the Dirac delta function.

~r0(~r) = δ(~r−~r0)

ei where {

ei : i = 1, 2, 3}

The spatial selectivity, I(~r0), may be thought of as an instrumental generalization
of the lead ﬁeld of a single measurement channel. The deﬁnition is designed so that in
the case when Yik = 1 when i = k = n0 and 0 otherwise then the sensitivity I(~r0) is the
square of the magnitude of the lead ﬁeld of channel n0. Note that the above deﬁnition
of I(~r0) is diﬀerent from the original deﬁnition proposed in [9].

b

b

To illustrate the method a simple simulated experimental system (Figure 2) has
been investigated. The head is modelled as a homogeneous conducting sphere of radius
8.9 cm with its centre at (0, 0, −0.07 cm). The source space is a 9 cm×9 cm square thin
lamina consisting of 33×33 voxels in the plane z = −0.01 cm with centre (0, 0, −0.01 cm).
The measurement instrument is a hexagonal array of 37 second order axial gradiometers
with baseline 5 cm with the lowest ’sensing’ coils in the plane z = 4 cm. Now consider,
in the context of the simulated system, the simplest possible region of interest operator
X = δ(~r − ~rc) where ~rc = (0, 0, −0.01 cm) is the centre of source space. This type
of operator might be adopted if one simply wished to focus on a small volume of
b
source space. The matrix Y used as an estimator from this operator is calculated
using Equation 17. The sensitivity proﬁle for this Y matrix is shown in Figure 2.

The reconstruction of an activation curve has been tested on simulated data using
this region of interest operator and simulated data from a time varying target dipole
at (0, 0, 0 cm), i.e. 1 cm from the region of interest. The moment of the dipole varies
sinusoidally at 10 Hz, with an envelope that rises linearly from zero at 200 ms to a
maximum at 300 ms after which it remains constant. To show the insensitivity to dipole
orientation the dipole moment was made to rotate smoothly in a tangential plane —

Best estimate of power

8

0.06

y/m

0.0

0.045

y/m

0.0

-0.06

-0.06

0.0

x/m

-0.045

0.06

-0.045

0.0

x/m

0.045

Figure 2.
(left) A plan view of the experiment geometry. Crosses denote source
space voxels and diamonds denote the projections of the centres of the detector coils.
(right) The sensitivity proﬁle in source space of the Y matrix that is derived from the
operator

X = δ(~r − ~rc).

b

this rotation is not discernible in the activation curve. In addition to the target dipole
there is distractor dipole at (0, 0.02 cm, 0), which is active from 0 to 100 ms (triangular
envelope) and again from 400 ms (square envelope).

In the period from 200 ms to 400 ms when only the target dipole is active, the
calculated (power) activation curve matches closely that of the target. However, the
existence of the distractor dipole within the sensitive region (see Figure 2) gives rise to
apparent activity between 0 ms and 100 ms and inaccuracy in the calculated activation
curve for the period after 400 ms. The distractor dipole adds to the estimated power
dissipated when it is parallel to the target and subtracts when the target dipole has
rotated to be anti-parallel.

Error bars for the activation curve can be estimated using the last term in
Equation 7 to give the amount of measurement noise reﬂected in the activation curve.
The estimate is given by

α,β CαβYαβ.

5. Total brain activity

P

As a special case of Equation 17 the task of ﬁnding an estimate of the total activity in
the source space is considered. In this case the operator

X is the identity and so

Xij = hψi,

Xψji = hψi, ψji = Pij

b

So the matrix X ′ can be calculated as follows
i P φj = λjφT

i Xφj = φT

ij = φT

X ′

b

i φj = λjδij

where δij is the Kronecker delta. So, in this case, Y is given by the simpliﬁed formula:

Y =

Xij

λi
λ2
i + ζ

λj
λ2
j + ζ

φiλjδijφT

j =

λ3
i
i + ζ)2 φiφT
(λ2

i

Xi

(29)

(30)

(31)

Best estimate of power

9

1.0

relative
power

0.5

0

0

250

time/ms

500

Figure 3. Activation curves for a simulated experiment. The solid line and the dotted
lines are the activation curves of the target and distractor dipoles. The diamonds are
the calculated activation curve from the Y matrix whose sensitivity proﬁle is shown in
Figure 2. The error bars, omitted for clarity, would be approximately twice the height
of the diamonds.

This gives the following formula for computing the total activity.

Total activity, A(t) =

λ3
i
i + ζ)2

(λ2

2

φT
i b(t)
(cid:0)

(cid:1)

Xi

where b(t) is the vector of measurements collected at time t.

Previously when an estimate of the total brain activity was needed the power in

the signals was used, i.e.

Total signal power, B(t) = b(t)T b(t)

(32)

(33)

These two methods have been compared for the simulated data described above as shown
in Figure 4. In Figure 4 it can be seen that the estimate A(t) (shown as the solid line
on the left) more closely approximates the true activation of the dipoles (dashed curve)
than the estimate B(t). In fact, if the error in the estimate is measured by the integral
of the squared discrepancies between the curves then the error for A(t) is 2.6 × 10−4
whilst the error for B(t) is 6.6 × 10−4.

6. Discussion

We have shown that it is possible to directly compute the ‘power’ associated with a
source without computing the source ﬁrst. The method seems robust to noise and is not
dependent on the noise having a Gaussian proﬁle. Correlations between measurement
channels are fully taken into account. In particular it was shown that activation curves
of brain regions can obtained from magnetic ﬁeld data. The method provides an easily
computable way of tracking the power dissipated in a speciﬁc region of the brain.

Best estimate of power

10

0.015
A(t)
0.01

0.005

0
0

0.015
B(t)
0.01

0.005

0
0

250

time/ms

500

250

time/ms

500

Figure 4. (left) A comparison of the total brain activity, A(t), (solid line) with a plot
of the power of the dipolar sources that generated the simulated data (dashed line). In
order to compare with the right-hand diagram both curves are normalized to enclose a
unit area. (right) A comparison of the total signal power, B(t), (solid line) with a plot
of the power of the dipolar sources that generated the simulated data (dashed line).
In order to compare with the left-hand diagram both curves are normalized to enclose
a unit area.

To use the method eﬀectively the practical problem is to eﬀectively estimate the
covariance matrix. For evoked response experiments the covariance matrix, C, can
be estimated from the prestimulus period. For other experiments it might be more
suitable to make the a priori assumption that the noise is uncorrelated Gaussian noise
with variance a α2 that could be considered as a parameter. As α increases, the more
closely the Y matrix sensitivity pattern matches the region of interest, but the larger
the error bars on the resulting activation curve.

Finally, to answer the question in the title, I would say that if best is interpreted
in a least L2-norm sense then the answer is no. The best way to estimate the power
associated with a source is to compute it directly.

References

[1] Jukka Sarvas. Basic mathematical and electromagnetic concepts of the biomagnetic inverse problem.

Phys. Med. Biol., 32(1):11–22, 1987.

[2] M. H¨am¨al¨ainen, R. Hari, R.J. Ilmoniemi, J. Knuutila, and O.V. Lounasmaa. Magnetoencephalog-
raphy - theory, instrumentation, and applications to noninvasive studies of the working human
brain. Reviews of modern physics, 65(2):413–497, 1993.

[3] D. Schwartz, D. Lemoine, E. Poisot, and C. Barillot. Registration of MEG/EEG data with 3D

MRI: methodology and precision issues. Brain Topography, 9(2):101–116, 1996.

[4] K.D. Singh, A.A. Ioannides, R. Hasson, U. Ribary, F. Lado, and R. Llinas. Extraction of dynamic
patterns from distributed current solutions of brain activity. In M. Hoke, S.N. Ern´e, Y.C. Okada,
and G.L. Romani, editors, BIOMAGNETISM: Clinical Aspects, pages 767–773, Amsterdam,
August 1992. Elsevier.

[5] C.D. Tesche, M.A. Uusitalo, R.J. Ilmoniemi, M. Huotilainen, M. Kajola, and O. Salonen. Signal
space projections of MEG data characterise both distributed and well-localised neuronal sources.
Electroenceph. Clin. Neurophysiol., 95:189–200, 1995.

[6] S. E. Robinson and D. F. Rose. Current source image estimation by spatially ﬁltered MEG.

In
M. Hoke, S. N. Ern´e, Y. C. Okada, and G. L. Romani, editors, Biomagnetism: Clinical Aspects,
pages 761–765, Amsterdam, 1992. Elsevier.

[7] P.C. Hansen. Regularization tools. Numerical Algorithms, 6:1–35, 1994.

Best estimate of power

11

[8] R. Hasson and S.J. Swithenby. The theoretical basis of iterative distributed solutions to the
In Advances in Biomagnetism Research: BIOMAG96, (Eds: C.

biomagnetic inverse problem.
Aine et al) Springer-Verlag, New York, In press, 1999.

[9] R. Hasson and S.J. Swithenby. Activation curves from optimally shaped regions. In T. Yoshimoto,
M. Kotani, S. Kuriki, H. Karibe, and N. Nakasato, editors, Recent Advances in Biomagnetism,
pages 205–208. Tohoku University Press, Sendai, 1999. ISBN 4-925085-19-0 C3047.


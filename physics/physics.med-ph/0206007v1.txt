2
0
0
2
 
n
u
J
 
3
 
 
]
h
p
-
d
e
m

.
s
c
i
s
y
h
p
[
 
 
1
v
7
0
0
6
0
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Spherical Harmonics Interpolation,

Computation of Laplacians

and Gauge Theory

Starlab Research Knowledge Nugget 2001-10-25
Created November 1, 2001. Revised May 2002.
Status: Public

Giulio Ruﬃni∗, Josep Marco, Carles Grau
Starlab Barcelona S.L., February 2, 2008
Ediﬁci de l’Observatori Fabra, C. de l’Observatori s.n.
Muntanya del Tibidabo, 08035 Barcelona, Spain
Tel://+34 93 254 03 66, http://starlab.es

∗email: giulio@starlab-bcn.com

Abstract

The aim in this note is to deﬁne an algorithm to carry out minimal curvature
spherical harmonics interpolation, which is then used to calculate the Laplacian
for multi-electrode EEG data analysis. The approach taken is to respect the data.
That is, we implement a minimal curvature condition for the interpolating sur-
face subject to the constraints determined from the multi-electrode data. We
implement this approach using spherical harmonics interpolation. In this elegant
example we show that minimization requirement and constraints complement each
other to ﬁx all degrees of freedom automatically, as occurs in gauge theories. That
is, the constraints are respected, while only the orthogonal subspace minimization
constraints are enforced. As an example, we discuss the application to interpolate
control data and calculate the temporal sequence of laplacians from an EEG Mis-
match Negativity (MMN) experiment (using an implementation of the algorithm
in IDL).

2

Contents

1. Minimization of Curvature with constraints

1.1. The functional and the equations . . . . . . . . . . . . . . . . . . . .
1.2. Solving the equations . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3. M and SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4. M M T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5. M T M . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6. Projection operators: P = M T
M . . . . . . . . . . . . .

−1

1.7. Projection operators: ¯P = 1 − M T

M . . . . . . . . . . .

M M T
(cid:16)

−1

(cid:17)
M M T

(cid:16)

(cid:17)

2. Complementarity: the physical and gauge sub-space

3. The algorithm

4. Analysis of sample experimental data

4.1. Control group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2. Curvature index . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5. Analogy to Constrained Mechanics and Gauge Theories

5.1. The Lagrangian in physics and χ2

. . . . . . . . . . . . . . . . . . .

2
3
4
5
6
6
6

7

7

8

9
9
10

11
11

1

1. Minimization of Curvature with constraints

This paper focuses on an algorithm for minimal curvature interpolation on the
surface of a sphere (that is, a 2D problem). The interpolation will then be used for
a Laplacian calculation. This is straightforward if spherical harmonics are used for
the interpolation.

There are many ways to interpolate data on a surface. Here we choose to
respect the original data points and minimize the curvature of the interpolating
surface. Note that this may not always be a good idea, as the data may itself be
“contradictory” due to noise impact. Nonetheless, in the present situation this is
not an issue. We will be dealing with scalp voltage measurements obtained from
an even grid of electrodes. No matter how noisy the data is, there always exists an
interpolation solution for a given set of measurements (e.g., there will not be two
contradictory measurements at the same electrode site).

We will thus impose full respect for the original data to ﬁx part of the interpo-
lation parameter space, and ﬁll the rest using the constraint of minimal curvature.
The goal is to carry out an interpolation with

v(ψ, θ) =

alm Yl,m(θ, ψ)

(1.1)

N

l

Xl=0

Xm=−l

minimizing the total curvature:

2
dS |∆Φ|

= Φ([alm]) =

l2

2
(l + 1)

|alm|

2.

(1.2)

Z

N

l

Xl=1

Xm=−l

The Laplacian, in spherical harmonics, is given by the simple relation

∆v(ψ, θ) = −

l(l + 1) alm Yl,m(θ, ψ),

(1.3)

Xm=−l
where r is the radius of the spherical head (we used 11.36 cm for an average head).

Xl=0

N

l

1
r2

Since the array v is real, and since (Yl,m)∗ = Yl,−m, it follows that (al,m)∗ =
al,−m. This point is important if we want to write the problem using complex
variables and treating al,m and (al,m)∗ as independent quantities. We have to write
the problem minimizing something that carries this symmetry forth. Thus,

χ2

= a† · Ba + (v − M a)† · λ + λ† · (v − M a),

(1.4)

where † denotes hermitean conjugation, is a valid minimizing functional. The fact
that the interchange of a and a∗ leaves χ invariant means that both will be treated
equally and extremization with respect to either will yield the same equation, albeit
in complex conjugate form.

2

The code implementation of this is a bit more complicated for and will forego
for now the complex approach. In [1] there is an interesting section explaining how
to translate a complex problem into a real one.

To translate the problem to the real domain, here we work with the expansion

v(ψ, θ) =

ar
l0 Yl,0(θ, ψ)) +

ar
lm Re[Yl,m(θ, ψ)] + ai

lm Im[Yl,m(θ, ψ)]
#

(1.5)

N

Xl=0 "

l

m=1
X

Here v is the array of voltage values at each electrode position. In the present work
we will have 31 such electrode values. The equation we want to solve (the hard
constraints) can be written in the form

v = M a.

(1.6)

To give a pictorial description of this matrix, M has rows of the form [Yl,m(θi, ψi)]:

Im[(Y1,1(θ1, ψ1)]
Im[(Y1,1(θ2, ψ2)]
...
...
Im[(Y1,1(θ31, ψ31)]

.....
.....
.....
.....
.....





M =

Y0,0(θ1, ψ1)
Y0,0(θ2, ψ2)
...
...

Y1,0(θ1, ψ1))
Y1,0(θ2, ψ2))
...
...

Re[Y1,1(θ1, ψ1)]
Re[Y1,1(θ2, ψ2)]
...
...








Y0,0(θ31, ψ31) Y1,0(θ31, ψ31) Re[Y1,1(θ31, ψ31)]







We will use lower case letters to denote vectors (such as a and λ, our unknowns,
and v, the potential data vector, a 31-vector). Then we have to be extra careful
with the dimensions of our matrices.

Let us refer to the original equation M a = v. These are in fact 31 equations
for an inﬁnite number of unknowns—not a promising perspective. So we change
strategy! The goal in our game is to minimize the curvature

C =

at · Ba,

1
2

(1.7)

where B is a symmetric matrix, while respecting the data (the constraints). Note
that without the constraints in the previous equation, the solution to this mini-
mization problem is simple: a0 is free, the rest must be all zero. We now that
a0 represents the mean voltage, so it is well determined by the constraints. The
problem we are posing is therefore well deﬁned: there exists a unique solution.

1.1. The functional and the equations

Our problem now is to minimize

χ =

aT · Ba + λT · (v − M a)

(1.8)

1
2

with respect to the unknowns a and λ.

These are the players, and we need to present them. First of all, a is the spherical
harmonics coeﬃcient tensor, the object we are really after. This, in principle, is a

3

∞-array. Then there is v, the aforementioned 31-vector of voltage data values
measured at a ﬁxed time t. Now, this means M is a 31x∞ matrix (the notation is
”output”x”input”) dimensions. Now, let us summarize:

To proceed, let us expand Equation 1.8:

and now diﬀerentiate to get the stationary point:

a ∈ IR∞
31

v, λ ∈ IR

M : IR∞ −→ IR

31

M T M : IR∞ −→ IR∞
31
31

M M T : IR

−→ IR

B : IR∞ −→ IR∞

χ =

aT · Ba + λT · (v − M A)

=

aiBijaj + λi(vi − Mijaj),

1
2
1
2

∂χ
∂ak

1
2

1
2

=

δikBijaj +

aiBik − λiMij

= aiBik − λiMik.

Ba = M T λ

M a = v.

Diﬀerentiating with respect to λ yields again Equation 1.6. Our new equations are
now

Now we have to solve them. The main point is to be careful with the dimensions of
all the objects—the situation is a bit trickier than usual.

1.2. Solving the equations

Let us now add M T times the second equation to the ﬁrst one (note that, perhaps
for the ﬁrst time, we are not adding pears to apples):

(B + M T M )a = M T (λ + v).

(1.15)

This step is important, now we have on the left a matrix of full rank.

Now, from the second Equation in 1.13 we see (assuming that M M T has full

rank, as it should and we shall see below)

λ = (M M T )−1M Ba.

(1.16)

4

(1.9)

(1.10)

(1.11)

(1.12)

(1.13)

(1.14)

This we can take and plug into the previous result,

(B + M T M )a = M T (M M T )−1M Ba + M T v.

Hence

B + M T M − M T (M M T )−1M B
h

i

a = M T b,

an important result, and

a =

B + M T M − M T (M M T )−1M B

M T b ≡ Q−1

n×nM T b, .

−1

i

h

with

Qn×n =

B + M T M − M T (M M T )−1M B
h

i

.

Now, can be this be simpliﬁed? Conceptually yes. The operator M T (M M T )−1M
is a projection operator. Let us describe it in more detail because this situation is
a nice example between constraints and minimization requirements.

1.3. M and SVD

To try to see how to simplify, we will use the Singular Value decomposition. Recall
that a very powerful conceptual and practical tool for studying this problem is
provided by the Singular Value Decomposition theorem of matrices (SVD)[1], which
states that given (here m < n, m = 31 and n = ∞) any m × n matrix Mm×n, there
exist essentially unique matrices Um×n (m × n), Wn×n (n × n), and Vn×n (n × n)
such that

Mm×n = Um×n Wn×n V T

n×n.

These matrices have further properties: Wn×n is diagonal, with entries bigger or
equal to zero, Vn×n is orthogonal,

Vn×nV T

n×n = V T V = 1n×n,

and the nonzero columns of Um×n form also an orthogonal matrix(Um×nU T
1m×m and

n×m =

n×mUm×n = P m
U T

n×n = diag(1, 1, .., 1, 0, .., 0)n×n .

(1.23)

There as many nonzero columns as the rank of M as there are nonzero diagonal
entries in W .

The power of this decomposition theorem is that it tells us what the kernel and
range of A are: the kernel of A is spanned by the columns or rows of V which
correspond to the zero diagonal elements of W , and the range is spanned by the
columns of U which correspond to the nonzero diagonal elements of W .

(1.17)

(1.18)

(1.19)

(1.20)

(1.21)

(1.22)

5

1.4. MM T

For instance, consider

M M T = U W V T V W U T = U31×∞W 2

∞×∞U T

∞×31

(1.24)

Hence, one is tempted to write

(M M T )−1

= Um×n W −2

n×nU T

n×n,

(1.25)

which is ill deﬁned, since W has nonzero diagonal entries. Let us deﬁne, in this
operation, that 1/0=0. That is, all the diagonal entries of W which are 0 are left
alone under inversion. Does this work?

M M T

M M T

−1

(cid:16)

(cid:17)

= Um×n W 2

n×m Um×n W −2

m×n U T
n×n diag (1, 1, .., 1, 0, .., 0)n×n W −2

n×n U T

n×m

= Um×nW 2
= Um×n diag(1, 1, .., 1, 0, .., 0)n×n U T

n×m

n×n U T

n×m

= 1m×m

(1.26)

so this is indeed the inverse.

1.5. M T M

What about M T M ? This is

= Vn×n W 2

n×n V T

n×n.

This is a large matrix of rank 31.

M T M = Vn×n Wn×n U T

n×m Um×n Wn×n V T

n×n

(1.27)

1.6. Projection operators: P = M T

MM T

−1

M

(cid:16)

(cid:17)

Now,

M T

M M T

−1

(cid:16)

(cid:17)

M = Vn×n Wn×n U T

n×m Um×n W −2

n×nU T

n×m Um×n Wn×n V T

n×n

= Vn×n Wn×n W −2
= Vn×n diag(1, 1, .., 1, 0, .., 0)n×n V T

n×n Wn×n V T

n×n

n×n

≡ Pn×n.

(1.28)

This projection operator maps the columns of V corresponding to nonzero entries
in the diagonal of W to themselves (acts as the identity), and to zero the other
ones. It projects, therefore, the linear n-space into and m-subspace.

6

1.7. Projection operators: ¯P = 1 − M T

MM T

−1

M

Now, recall,

But

and

(cid:16)

(cid:17)

Qn×n =

B + M T M − M T (M M T )−1M B
h

i

.

B − M T (M M T )−1M B = [1n×n − Pn×n]B

= ¯Pn×nB,

Qn×n =

M T M + ¯Pn×nB
h

i

.

note how this matrix has full rank, and how the equations have worked out so
that only the “free” subspace of M T M is aﬀected by the curvature equation. The
minimizing set of equations are used to complement the constraints to ﬁx a unique,
well deﬁned solution.

Let us emphasize that P and ¯P = 1 − P are both projection operators: P 2 = P .

2. Complementarity: the physical and gauge sub-space

We can now rewrite equation 1.18 as

where

and

Q · a = b′

Q = M T M + ¯P B,

b′ = M T b.
To better interpret these terms, let us work now in the U and V basis. In this basis,

(2.3)







w2
0
0
0
1
0 w2
0
0
2
...
0
0
0
0 w2
0
m 0
0
0
0
0
...
...
...
...
0
0
0
0

0
0
0
0
0
...
0

...
...
...
...
...
...
0

0
0
0
0
0
...
0

+

Q =
























as one would expect. In the second term we schematically show that the projection
operator “deletes” the impact of B in the physical subspace. What about b′? This
is similarly given by

























, (2.4)

...

...

...

...



0
0
0
0

0
0
0
0

...
...
...
...

0
0
...
0

0
0
0
0
0
0
0
0
0
0
0
0
B B B B B ... B
...
...
...
B B B B B B B

(1.29)

(1.30)

(1.31)

(2.1)

(2.2)

(2.5)

b′
1
b′
2
...
b′
m
0
...
0

7















.















By “physical” subspace we mean the sub-vector space aﬀected by the hard con-
straints associated, in this example, to the measurements we choose to respect. The
complement of this sub-space is called, in other contexts, gauge sub-space. It is the
free-ﬂoating part of the solution space, the one that we need to somehow ﬁx.

Thus, we see that the method of minimization subject to constraints leads to
a nice interpretation in which the physical degrees of freedom (associated to the
constraints) are not aﬀected by the minimization extra requirement, while the gauge
degrees of freedom are ﬁxed by the curvature minimization requirement.
In the
language of ﬁber bundles, which is appropriate for handling gauge theories and
which we could have used in the above discussion, by the choice of B we are choosing
a particular (gauge) ﬁber bundle section.

This method implies that we trust fully the constraints, that is, we trust fully
the measurements, as if there was no noisy. This is, in general, not a good strategy,
as the data may be very noisy and inconsistent. For this purpose, following our
intuition as based on the previous discussion, we can generalize a bit Q to write

Q = M T M + ¯P B + νP C,

(2.6)

where ν is a “tuning” parameter and C is a new (normalized) constraint aﬀecting
now only the physical subspace (note that tuning the middle term has zero impact).
An immediate choice is C = B, of course.

The ﬁrst term in Q is acting only in P -space. This encodes the limited in-
formation available from the data. The second one encodes the information from
curvature minimization requirement as projected to ¯P -space. The third, new, term,
encodes any additional information we may want to add to “smooth” or regularize
the information in the ﬁrst term, which may be noise or otherwise unreliable.

How much should be added to the physical part? As much as needed to ﬁx the
solution, but no more. That is, if we have some a priori knowledge on the noise
characteristics associated to the P -space constraints, we can evaluate the entropy
associated to this condition. We then need to add enough information to bring the
entropy down to the desired (or needed) value.

3. The algorithm

The goal is to implement code to solve

a =

B + M T M − M T (M M T )−1M B
h

i

−1

M T b ≡ Q−1

n×nM T b,

with

Qn×n =

B + M T M − M T (M M T )−1M B
h

i

.

Once the coeﬃcients are available, a new M is constructed from a interpolated set
of electrode positions, iM . This matrix is speciﬁed here by a set of 40×20 positions
(the larger number for longitude positions).

(3.1)

(3.2)

8

Using the interpolating matrix, it is then simple to compute the interpolated

potential ﬁeld and its Laplacian.

4. Analysis of sample experimental data

Here we give a simple example using an implementation of this algorithm devel-
oped at Starlab. A MMN sample data set from an Experiment carried out at the
University of Barcelona Neurodynamics Laboratory is analyzed for illustration. For
the purposes of the present discussion it suﬃces to mention that the 32 electrode
(plus a reference) data set corresponds to an average of MMN EEG from 17 control
group subjects, and that it corresponds to 100 ms prior to an auditive stimulus up
to 500 ms later. For more information see [2] and forthcoming publications. Here
we show results for lmax=20.

4.1. Control group

This is the output for the data interpolation (32 electrodes, time in ms):

And this is the resulting Laplacian:

9

4.2. Curvature index

This a graph of the curvature of the ﬁtted minimal surface, an interesting measure
(the blue curve, the other curve refers to another experimental group).

10

5. Analogy to Constrained Mechanics and Gauge Theories

In this set of notes I intend to introduce very quickly to the Lagrangian for-
malism in physics, the impact of symmetries of the action to the solution space,
their relation to gauge theory, and how all these strange things happen to relate
to the interpolation problem (for more on this and further references, see xxx-gr-
qc9806058). What all these have in common is their origin in a minimization (or
extremization, to be precise) problem.

5.1. The Lagrangian in physics and χ2

It is interesting to form an analogy to the theory constrained theories in classical
mechanics.

Given a system with ndegrees of freedom, each speciﬁed by a coordinate qi(t),

we can obtain its dynamics by minimizing the functional (called the action)

S[qi(t)] =

L(qi(t), ˙qi(t)) dt,

(5.1)

t2

t1

Z

that is, one looks for the set of qi(t)such that given that at t1 and t2the values of the
coordinates are to be held ﬁxed, one varies the functions so as to ﬁnd the minimum
of S. In general, and although this may seem less familiar to you, one can rewrite
this as

S[qi(t)] =

qi(t)M ij(t, t′)qj(t′) dtdt′ −

V (qi(t)) dt

(5.2)

t2

t2

t1 Z

Z

t1

t2

t1

Z

If the last term is a quadratic (as it happens in the case of doable physics, e.g., the
harmonic oscillator), we ﬁnd that we are trying to minimize a quadratic functional,
just as was the case in the previous sections, where the goal was to minimize the
curvature. There is no single surface of minimal curvature, as a constant term can
always be added, for example.

Allowing for linear terms, then, we are trying to minimize something that looks

like

S = qT M q + q · j + c

(5.3)

where q here stands for a long vector in which there is an entry for each time
parameter (think of time as a discrete index if this helps)and for each possible value
of the index i. By a symmetry we mean a transformation of the quantities q → q′so
that S → S:
i.e., so that the action is invariant. For instance, and to simplify
things, imagine that the action we want to minimize is

where x, which we will call here the state vector or state, for short, is an n-vector,
and y is an m-vector (this equation may look familiar to you!). Let us now use the
Singular Value Decomposition theorem to rewrite

(5.4)

(5.5)

χ2

2
(x) = (y − Ax)

A = U W V T

11

Recall that W is a n × n diagonal matrix with entries bigger or equal to zero, V is
an orthogonal n × n-matrix (V V =T V V T = In×n), and that U is and m × n-matrix
with orthogonal columns (U T U = In×n).

Imagine now that the last m diagonal entries of W are zero. What does this
mean? It means that the two states x and x + αjvjare equivalent as far as χ2 is
concerned, where by vj we mean the j-th column of V :

χ2

(x) = χ2

(x + αivi),

i = 1, ..., m

(5.6)

The fact that the m coeﬃcients αi are all independent of each other is what makes
this transformation symmetry a local symmetry in the physics lingo. It tells us that
of the original n degrees of freedom, only n − m really matter. These are called
the physical degrees of freedom. The rest are called gauge or unphysical, degrees of
freedom. These are the source of our inversion problems: the minimization problem
does not have a unique solution. In physics one ﬁxes this in some way or another,
usually by adding a so-called gauge-ﬁxing conditions. This means to pick a set
of coeﬃcients αi, and this is usually done by adding a set of m equations to the
problem that ﬁx these. Let B be a rank-m n × n-matrix. Then we’d like to add
such a condition to the problem. Notice the condition Bx = 0 is to ﬁx the gauge
degrees of freedom then B has to have maximal rank in the gauge subspace (i.e., it
must completely span the gauge subspace).

If we believe in the data, one must avoid increasing χ2 while ﬁxing the gauge
degrees of freedom. It is possible, as we now discuss, to ﬁx these in a reasonable
manner, without getting away from the data. We have in fact discovered how to do
that in the previous discussion, but it may be useful to repeat the exercise in this
context.

Suppose then that for some physical reason we want to ask that Cx = 0 in
order to ﬁx the gauge degrees of freedom, but that we have been a little naive and
have not worried about the rank of C. Proceeding as is usual, then, we choose to
minimize the functional

Upon minimization we obtain the equation

χ2

2
(x) = (y − Ax)

+ λ(Cx)

2.

(AT A + λC T C)x = AT y.

Let x = V x′. This is the state in “SVD coordinates”. The ﬁrst n − m entries in
the vector x′ are physical coordinates, the rest are gauge. Then, using the fact that
AT A = V W 2V T , we can rewrite this equation as

V T (AT A + λC T C)V V T x = V T AT y,

or

(5.7)

(5.8)

(5.9)

(5.10)

(W 2

+ λV T C T CV )x′ = W U T y.

12

This equation reﬂects the fact that without the additional constraint (i.e., set λ = 0),
the physical degrees of freedom are well-ﬁxed, while the gauge ones are not. If we
want to avoid disturbing the physical part of the state, we now project the constraint
into the gauge subspace: V T C T CV → PgaugeV T C T CV , where Pgauge is a diagonal
matrix with zeros everywhere except the last m last diagonal elements. Now the
solution to

(W 2

+ λPgaugeV T C T CV )x′ = W U T y

(5.11)

is as before as far as the physical degrees of freedom are concerned. If the rank of
PgaugeV T C T CV is m we will have ﬁxed a complete solution. To see more clearly
what has happened, let us factorize x′ = xphy + xgauge, where xgauge ∼ αivi is the
projection of x into the gauge subspace , we can rewrite the above equations as

W 2xphys = W U T y,

which ﬁxes the physical degrees of freedom , as it should, and

PgaugeV T C T CV (xphy + xgauge) = 0.

(5.12)

(5.13)

In this way we ensure that we are not imposing any constraint equations beyond
those that we are really allowed to. The matrix PgaugeV T C T CV is to have rank m.
If C is fully ranked, then it will. If not, we may still be ok. We can now go back to
the original coordinates and write (notice that λ is irrelevant now)

(AT A + V PgaugeV T C T C)x = AT y

(5.14)

Another way to reason this result is the following. As we mentioned, the physical
degrees of freedom, Pphysx, are ﬁxed. We want to really ﬁnd now the set of gauge
degrees of freedom such that (C ′ = V T CV is the expression of the constraint in the
SVD coordinates)

C ′(Pgaugex′ + Pphysx′)

2

(5.15)

is minimum. If we vary the gauge degrees of freedom in this functional and set it
equal to zero we ﬁnd the equation

(cid:0)

(cid:1)

PgaugeC ′T C ′Pgauge + PgaugeC ′T C ′Pphys

x′ = 0,

(5.16)

(cid:16)

(cid:17)

or PgaugeC ′T C ′x′ = 0, since Pgauge + Pphys = In×n. In the original coordinates, this
reads V PgaugeV T C T Cx = 0, as we wrote before.

By construction, then, there exists now a unique solution to the problem

AT A + V PgaugeV T C T C
(cid:16)

(cid:17)

x = AT y.

(5.17)

References

[1] S A Teukolsky, W H Press, W T Vettering, Flannery, Numerical Recipes in
Fortran, The Art of Scientiﬁc Computing, Cambridge University Press, 1994.

13

[2] Ruﬃni, G., Galan, F., Marco, J., Polo, MD., Escera, C., Rius, A., Grau, C.,
Estudio de patrones temporo-espaciales de activacion cerebral durante la produc-
cion de Mismatch Negativity en el alcoholismo cronico, Santiago de Compostela,
21st September, 2000. Abstract available at http://www.starlab.es.

[3] Ruﬃni, G., PhD thesis, The Quantization of Simple Parametrized Theories, UC

Davis, 1995 (available at http://www.starlab.es).

14


3
0
0
2
 
t
c
O
 
0
3
 
 
]
h
p
-
d
e
m

.
s
c
i
s
y
h
p
[
 
 
1
v
1
5
1
0
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Developments in EIT Reconstruction Algorithms

William R.B. Lionheart
Department of Mathematics, UMIST, UK

Abstract. We review developments,
in Electrical
Impedance Tomography (EIT), for the 4th Workshop on Biomedical Applications
of EIT, Manchester 2003. We focus on the necessity for three dimensional data
collection and reconstruction, eﬃcient solution of the forward problem and present
and future reconstruction algorithms. We also suggest common pitfalls or “inverse
crimes” to avoid.

issues and challenges

1. Introduction

As the papers in this special issue and the 4th Workshop on Biomedical Applications
of EIT show, reconstruction algorithms for electrical impedance tomography remain
an active and exciting area of research. In this article we aim to draw attention to the
best current practise as well so pitfalls to avoid, also to highlight some active areas of
development and some of the promising methods have not yet been implemented in
practical algorithms.

By Electrical Impedance Tomography (EIT) we mean the process of estimating
internal admittivity (complex conductivity) changes from low frequency current and
voltage measurements through a system of electrodes at the surface. Although the
details of the instrumentation design varies between biomedical, geophysical and
industrial applications the mathematics of the reconstruction problem is essentially
the same. The main diﬀerences being in the conﬁguration of the electrodes used, and
the a priori information available.

2. Three dimensionality

imaging is used the problem is genuinely three-
In most cases where electrical
dimensional. An exception being industrial or non-destructive testing problems where
the conductivity and the electrodes can be assumed independent of one coordinate.
Why then is there such a large body of published work in which data is collected on a
three dimensional body and the reconstruction performed assuming the body was two
dimensional? There are two main reasons: speed and the fact that to some limited
extent it worked. The issues of speed include the speed of data collection. The belief
that electrodes arranged in a single plane would yield an adequate reconstruction of
the conductivity in that plane, and consequent use of a small number of electrodes
resulted in a fast data collection system. The number of electrodes applied is also
a signiﬁcant factor when they have to be applied individually to the skin which
in itself is a time consuming process. However it is known that data measured
on a three dimensional body cannot be ﬁtted accurately to any two dimensional

Developments in EIT Reconstruction Algorithms

2

conductivity distribution [29]. Moreover attempts to ﬁt a two dimensional model
result in errors of position and shape of anomalies [54]. Another factor in the choice of
two dimensional reconstruction methods was the cost of fast processors and memory
required to perform three-dimensional forward modelling and reconstruction, which
in the 1980s and early 1990s were prohibitive.

Of these factors the one which remains is the inconvenience of applying a fully
three dimensional system (for example multiple planes) of electrodes to a human
subject, a technological problem which must be overcome if EIT is to be used
eﬀectively as a medical imaging technique.

2.1. Is it tomgraphy?

It is worth mentioning here that EIT, despite its now traditional name agreed at
the ﬁrst Sheﬃeld EIT meeting in 1986, is not tomographic in that it is not possible to
reconstruct an image slice-by-slice. (One can of course reconstruct a three dimensional
conductivity distribution and then display it on any desired slice.) The physical
explanation for this is that unlike X-rays, low frequency electric current cannot be
conﬁned to a plane even by a system of external electrodes, and that a change in
conductivity anywhere in the domain aﬀects can eﬀect all measurements not just
those on a ray path.

An obvious extension of the traditional array of equally spaced electrodes in a
plane to a three dimensional data collection system is to employ multiple planes of
electrodes. If electrodes are excited in pairs on a given plane it is necessary to make
measurements of voltage on the electrodes in other planes as well. If one is modifying
the traditional planar system for this purpose, and voltages are measured between
adjacent pairs of electrodes, one must also make measurements between planes as
well for a complete set of transfer impedance data. Clearly only one measurement
between one pair of electrodes on adjacent planes is suﬃcient by superposition. This
conﬁguration is however only an expedient way to employ inﬂexible equipment and is
not likely to be an ideal data collection scheme. The optimal arrangement of electrodes
for imaging brain or lung function for example has yet to be determined.

There is interesting a connection between EIT and tomographic imaging which
has yet to be exploited in a practical reconstruction algorithm. Suppose that one
is able to measure the complete transfer impedance on a plane intersecting a three
dimensional object, while there is no known way of reconstructing the conductivity on
this plane Greenleaf and Uhlmann [23] show that the integral of the conductivity over
that plane is known. Suppose now that we use a large number of surface electrodes
so that the transfer impedance, and hence the integral of the conductivity, is known
for a wide class of planes. Reconstructing the conductivity reduces to the problem of
inverting a Radon plane transform with limited data.

3. Forward Problem solution

For accurate EIT reconstruction it is necessary ﬁrst to have a model capable of
predicting the voltages on electrodes for a given conductivity distribution. The
conductivity is adjusted until the voltages ﬁt to measurement precision. This forward
model must also be capable of predicting the electric ﬁelds in the interior given the
conductivity, as this is used in the reconstruction algorithms (See section 5.2). It is
clear that the voltages on the electrodes must be predicted to a precision better than

Developments in EIT Reconstruction Algorithms

3

the accuracy of the measurements. But the accuracy required of the interior electric
ﬁelds and consequently the sensitivity of measurements to conductivity changes, is a
matter which has received little attention.

3.1. Choice of method

As we are interested in non-homogeneous conductivity distributions on irregular
domains Finite Element Method (FEM) is the natural choice [54] , although ﬁnite
diﬀerence and ﬁnite volume methods have also been employed. Where the conductivity
is known and homogeneous in some sub domain, especially a neighbourhood of the
boundary, and attractive proposition is to use a hybrid boundary element and ﬁnite
element method. As the highest ﬁeld strengths are typically near at the edges of
electrodes this is where the densest mesh is required. With the Boundary Element
Method (BEM) only the surface needs to be discretized with a consequent saving on
the size of matrix to be inverted. On the other hand the matrix for the hybrid system
has a dense blocks coupling for the BEM and the coupling between BEM and FEM,
and care has to be taken to use and eﬃcient solution scheme for such a matrix.

3.2. Custom versus commercial FEM code

For medical applications, as well as industrial applications where conductive electrodes
are in contact with an aqueous solution, the Complete Electrode Model must be
employed for accurate prediction of the electrode voltages [48]. This takes account
of both the shunting eﬀect of the conducting electrodes and the contact impedance
layer between the electrode and the solution (see (3) below). As this is not a standard
type of boundary condition it is not easily implemented is commercial ﬁnite element
packages where the source code is not accessible. This together with the necessity to
update the system matrix rapidly with a revised conductivity militates against the
use of a closed source commerical FEM solver. Another factor is the ease with which
the Jacobian can be assembled from low level access to products of gradients of shape
functions (See section 5.2. Even FEMLAB [18] which has considerable lowlevel access
to internal structures through Matlab does not allow this at present.

In practise the assembly of the FEM system matrix, at least for simple linear
tetrahedral elements is easily implemented and the step from two to three dimensions
requires only the calculations of integrals of products of shape functions over faces
under electrodes [54, 40]. The steps which involve considerably more time and care
are mesh generation and the eﬃcient solution of the linear system. The design of
three dimensional mesh generators is a major research topic in itself, and we have
yet to ﬁnd an existing program, commercial or free, which is ideal for EIT. The main
requirements are for an eﬀcient mesh of an object composed of smooth but irregular
surfaces, which respects interior boundaries, and electrodes on the surface. The mesh
density needs to be determined as a function of position so that high ﬁeld strengths
(for example near electrodes) can be accurately represented without excessive density
in areas where the ﬁeld varies slowly. Even if such a program were available one would
still need to measure the external shape of the body accurately, and in the case of the
human head intricate internal structures such as the skull needs to be segmented from
X-Ray CT or MRI scans [5].

Developments in EIT Reconstruction Algorithms

4

3.3. Eﬃcient forward solution

The computational complexity of the three-dimensional forward problem means that
attention has to be paid to the eﬃciency of the forward solution. For two dimensional
problems a standard approach to solving the linear system resulting from the ﬁnite
element discretization is to employ Choleski factoization (or LU decomposition for
the case of complex admittivity) together with forward and backward substitution
to give the potential for each applied current pattern [22]. As the system matrix
is sparse, renumbering the degrees of freedom to reduce the ﬁll in of non-zeors is
the Choleski factors is common practise. The Symmetric Multiple Minimum Degree
algorithm is a standard choice which tends to move degrees of freedom with higher
“valencies” (coupled to many others) to the bottom of the vector. With the complete
electrode model degrees of freedom include both ﬁnite element nodal values and
electrode voltages. The Matlab function symmmd implements the multiple minimum
degree algorithm, and for the complex case the Column Multiple minimum degree
colmmd function can be used which reduces the ﬁll in for the LU factorization. It
needs to be stressed that although node renumbering can be expensive it needs to
be done only once for each mesh and the result stored for future use.
In three
dimensions iterative solution methods become more attractive, although each iterative
step must be applied to the multiple right-hand sides for each current pattern. For the
real case Conjugate Gradient method is a favourite choice. Convergence is improved
using a preconditioner which is an approximate inverse to the system matrix. If the
conductivity does not vary over too wide a range during the reconstruction process
this approximate inverse can be precomputed based on a typical conductivity, and
as the inversion is typically performed by incomplete Choleski factorization the same
consideration of node renumbering applies as for direct methods. For the complex
case other Krylov subspace iterative methods [41] such as GMRES can be used [42].

3.4. Accuracy of forward solution

In iterative methods time can be saved if a solution is not required to full machine
precision, and as our boundary voltage data is not so accurate it is unnecessary to
solve to this accuracy. The accuracy required for the interior electric ﬁelds and hence
the Jacobian is not well understood. That said the accuracy of the ﬁnite element
method approximation is well studied and there are a priori estimates [9] for the
error in terms of the mesh size (r convergence) and the order of the elements used h
convergence. There are also a posteriori error estimates based on calculated solutions
[9] which have not yet been widely used in EIT. Although there is some work using
higher order elements [54] the best choice of element for EIT remains an open problem,
and the possibility of using vector elements [11] to calculate electric ﬁelds and current
densities accurately in the interior is largely unexplored for our problem. The use of
inﬁnite elements to model unbounded regions, or at least regions which while bounded
have a substantial part where we have no surface data, is an interesting possibility [51].
Possible applications include limbs when the torso is being imaged, as well as problems
where the body is treated as an inﬁnite half space such as in geophysical imaging or
the use of a small surface electrode array in medical EIT.

Developments in EIT Reconstruction Algorithms

5

3.5. Multigrid and adaptive meshing

So far we have considered the situation in which same mesh is used throughout the
forward solution process. One attractive alternative is to solve ﬁrst on a course grid to
give the crude features of a solution which is then extrapolated to a ﬁner grid where a
mode accurate solution is calculated. A systematic treatment of this cycling between
ﬁner and courser grids used in multigrid solution algorithms. At least for regular
grids the theoretical optimum of solving a system whith time complexity O(N ) can
be approached for N degrees of freedom. The geometric problems of calculating
a heirachy of grids for a complex object can be avoided using algebraic multigrid
methods which are described elsewhere in this issue [47].

Another strategy for reducing the solution time is to adaptively vary the ﬁnite
element mesh used. During the forward solution process for a ﬁxed conductivity
the mesh density is increased where the high ﬁeld strengths are found and decreased
where the potential is gently varying. This results in a more accurate solution than
a regular mesh with the same number of degrees of freedom. One complexity for our
problem is that we have multiple right-hand sides, so we can choose either to use one
mesh for all current patterns, in which case it may be unnecessarily ﬁne near passive
electrodes, or to use one mesh for each drive conﬁguration. Although typically the
highest ﬁeld strengths appear near the edges of electrodes (including passive electrodes
if the contact impedance is low), sharp contrasts in conductivity can also give rise to
high ﬁeld strengths. For example an apperture in the skull (low conductivity) for a
relatively high conductivity blood vessel or nerve. An adaptive meshing algorithm will
increase the mesh density here once the conductivity contrast has been predicted by
the reconstruction algorithm. The thesis of Mollinari [35] explores the use of adaptive
meshing in both forward and inverse problems in EIT and gives a clear indication that
this is a fruitful area for further study.

4. Regularised Newton Methods

While numerous ad hoc reconstruction methods have been tried for EIT the standard
approach is to use one of a family of regularized Newton-type methods. The approach
is to some extent the obvious one: the problem is non-linear so linearize, the linear
problem is ill-posed so regularize, the linear approximation cannot reconstruct large
contrasts or complex geometries so the process must be applied iteratively. There
are of course many variations on this basic approach and we will sketch some typical
ones. First let us assume that the conductivity σ has been represented by a ﬁnite
number of parameters s. The simpest for this takes is as a sum of basis functions such
as the characteristic functions of a set of regular or irregular voxels, or smooth basis
functions. Other choices would include a detailed model involving conductivity values
as well as parameters describing shape internal regions [26, 27].

4.1. Regularization

Our forward operator F gives us V = F (s) the measured voltages at the boundary.
We will leave aside the adaptive current approach [21, 14] where the measurements
taken depend on the conductivity. As the goal is to ﬁt the voltage measurements
Vmeas, the simplest approach is to minimize the sum of squares error

||Vmeas − F (s)||2

Developments in EIT Reconstruction Algorithms

6

the so called output least squares approach.
In practise it is not usual to use the
raw least squares approach, but at least a weighted sum of squares which reﬂects the
reliability of each voltage. Such approaches are common both in optimization and the
statistical approach to inverse problems.

Minimization of the voltage error (for simple parameterizations of σ) is doomed
to failure as the problem is illposed. In practise the minimum lies in a long narrow
valley of the objective function[13]. For a unique solution one must include additional
information about the conductivity. An example is to include a penalty G(s) for highly
oscillatory conductivites in our minimization, and seek to minimize
f (s) = ||Vmeas − F (s)||2 + G(s).

A typical simple choice [53] is

G(s) = α2||L(s − sref)||2

where L is a matrix approximation to some partial diﬀerential operator and sref
a reference conductivity (for example including known anatomical features). The
minimisation of f represents a trade-oﬀ between ﬁtting the data exactly and
not making the derivatives of σ too large, the trade oﬀ being controlled by the
regularisation parameter α. Other smooth choices of G include the inverse of a
In these cases where G is smooth and for α large
Gaussian smoothing ﬁlter [8].
enough the Hessian of f will be positive deﬁnite, we can then deduce that f is a convex
function [56, Ch 2], so that a critical point will be a strict local minimum, guaranteeing
the success of smooth optimization methods. Such regularization however will prevent
us from reconstructing conductivities with a sharp transition, such as an organ
Including the Total Variation, that is the integral of |∇σ|, still rules
boundary.
out wild ﬂuctuations in conductivity while allowing step changes. The cost is that
the inclusion of an absolute value destroys the diﬀerentiability of f and means that
we must employ non-smooth optimization methods. See [57] for analysis of Total
Variation regularization of EIT and [6] for an implementation.

In the statistical approach [56, Ch 4] to regularisation the minimiser of f is
maximises the a posteriori probability (the MAP estimate) assuming independent
Gaussian error with mean zero and unit variance on the measurements and the a
priori information on σ represented by the probability distribution exp(−G(s)/2).

4.2. Linearized Problem

F (s0) + J(s − s0)

Consider the simpliﬁed case is where F (s) is replaced by a linear approximation

where J is the Jacobian matrix of F calculated at some initial conductivity estimate
s0 (not necessarily the same as sref). Deﬁning δs = s − s0 and δV = Vmeas − F (s0) the
solution to the linearized regularization problem (a quadratic minimization problem
for f ) is given by

δs = (J

∗

J + α2L

∗

L)

−1(J

∗

δV + α2L

∗

L(sref − s0))

(1)

or any of the equivalent forms [50]. While there are many other forms of regularization
possible for a linear ill-conditioned problem this generalised Tikhonov (or Tikhonov-
Phillips) regularization has the beneﬁt that the a priori information it incorporates
is made explicit and that under Gaussian assumptions it is the statstically defensible
MAP estimate. If only a linearised solution to be used with a ﬁxed initial estimate

Developments in EIT Reconstruction Algorithms

7

s0 the Jacobian J and a factorization of (J ∗J + +α2L∗L) can be precalculated oﬀ-
line. The eﬃciency of this calculation is then immaterial and the regularized solution
can be calculated using the factorization with complexity O(N 2) for N degrees of
freedom in the conductivity (which should be smaller than the number of independent
measurements). Although LU factorization would be one alternative, perhaps a better
choice is to use the Generalized Singular Value Decomposition GSVD [24], which allows
the regularized solution to be calculated eﬃciently for any value of α. The GSVD is
now a standard tool for understanding the eﬀect of the choice of the regularization
matrix L in a linear ill-conditioned problem, and has been applied to linearised
EIT[8]. The use of a single linearized Tikhonov regualized solution is widespread
in medical industrial and geophysical EIT, the NOSER algorithm [17] being a well
known example.

4.3. Backprojection

It is an interesting historical observation that in the medical and industrial applications
of EIT numerous authors have calculated J and then proceeded to use ad hoc
regularized inversion methods to calculate an approximate solution. Often these
are variations on standard iterative methods which, if continued would for a well
posed problem converge to the Moore-Penrose generalised solution. It is a standard
method in inverse problems to use an iterative method but stop short of convergence
(Morozov’s stopping criteria tells us to stop when the output error ﬁrst falls below
the measurement noise). Many linear iterative schemes can be represented as a ﬁlter
on the singular values. However the have the weakness that the a priori information
included is not as explicit as in Tikhonov regularisation. One extreme example of
the use of an ad hoc method is the method described by Kotre [28] in which the
normalized transpose of the Jacobian is applied to the voltage diﬀerence data. In the
Radon transform used in X-Ray CT [38], the formal adjoint of the Radon transform
is called the back projection operator. It produces at a point in the domain the sum of
all the values measured along rays through that point. Although not an inverse to the
Radon transform itself, a smooth image can be obtained by backprojecting smoothed
data, or equivalently by back-projecting then smoothing the resulting image.

The Tikhonov regularization formula (1) can be interpreted in a loose way
as the back-projection operator J ∗ followed be application of the spatial ﬁlter
(J ∗J + +α2L∗L)−1. Although this approach is quite diﬀerent from the ﬁltered back
projection along equipotential lines of Barber and Brown [3, 43] it is sometimes
confused with this in the literature. Kotre’s back projection was until recently widely
used in the process tomography community for both resistivity (ERT) and permittivity
(ECT) imaging [58]. Often supported by the fallacious arguments, in particular that
it is fast (it is no faster than the application of any precomputed regularized inverse)
and that it is commonly used (only by those who know no better). In an interesting
development the application of a normalised adjoint to the residual voltage error for
the linearised problem was suggested for ECT, and later recognised as yet another
reinvention of the well known Landweber iterative method. Although there is no good
reason to use pure linear iteration schemes directly on problems with such small a
number of parameters as they can be applied much faster using the SVD, an interesting
variation is to use such a slowly converging linear solution together with projection
on to a constraint set. A method which has been shown to work well in ECT [15].

Developments in EIT Reconstruction Algorithms

8

4.4. Iterative solutions

The use of linear approximation is only valid for small deviations from the reference
conductivity. In medical problems conductivity contrasts can be large, but there is a
good case for using the linearized method to calculate a change in admittivity between
two states, measured either at diﬀerent times or with diﬀerent frequencies. Although
this has been called “dynamic imaging” in EIT the term diﬀerence imaging is now
preferred (dynamic imaging is a better used to describe statistical time series method
such as [52] ). In industrial ECT modest variations of permittivity are commonplace.
In industrial problems and in phantom tanks it is possible to measure a reference data
set using a homogenious tank. This can be used to calibrate the forward model, in
particular the contact impedance can be estimated [25]. In an in vivo measurement
there is no such possibility and it may be that the mismatch between the measured
data and the predictions from the forward model dominated by the errors in electrode
position, boundary shape and contact impedance rather than interior conductivity.
Until these problems are overcome it is unlikely, in the author’s opinion, to be worth
using iterative non-linear methods in vivo using individual surface electrodes. Note
however that such methods are in routine use in geophysical problems [31, 32].

The essence of non-linear solution methods is to repeat the process of calculating
the Jacobian and solving a regularised linear approximation. However a common way
to explain this is to start with the problem of minimizing f , which for a well chosen G
will have a critical point which is the minimum. At this minimum ∇f (s) = 0 which
is a system of N equations in N unknowns which can be solved by multi-variable
Newton-Raphson method. The Gauss-Newton approximation to this, which neglects
terms involving second derivatives of F , is a familiar Tikhonov formula updating the
n th approximation to the conductivity parameters sn

sn+1 = sn + (J

∗

nJn + α2L

∗

L)

−1(J

∗

n(Vmeas − F (sn)) + α2L

∗

L(sref − sn)

where Jn is the Jacobian evaluated at sn, and care has to be taken with signs . Notice
that in this formula the Tikhonov parameter is held constant throughout the iteration,
by contrast Levenberg-Marquardt[36] method applied to ∇f = 0 would add a diagonal
matrix λD in addition to the regularization term α2L∗L but would reduce λ to zero
as a solution was approached. For an interpretation of λ as a Lagrangian multiplier
for an optimization constrained by a trust region see [56, Ch 3]. Another variation
on this family of methods is, given an update direction from the Tikhonov formula,
to do an approximate line search to minimize f in that direction. Both methods are
described in [56, Ch 3].

The parameterization of the conductivity can be much more speciﬁc than voxel
values or coeﬃcients of smooth basis functions. One example is to assume that the
conductivity is piecewise constant on smooth domains and reconstruct the shapes
parameterized by Fourier series [26, 27] or by level sets. For this and other model
based approaches the same family of smooth optimization techniques can be used as for
simpler parameterizations, although the Jacobian calculation may be more involved.

5. Jacobian calculations

In optimization based methods it is often necessary to calculate the derivative of
the voltage measurements with respect to a conductivity parameter. The complete
matrix of partial derivatives of voltages with respect to conductivity parameters is

Developments in EIT Reconstruction Algorithms

9

the Jacobian matrix, sometimes in the medical and industrial EIT literature called
the sensitivity matrix, or the rows are called sensitivity maps. We will describe
here the basic method for calculating this eﬃciently with a minimal number of
forward solutions. Let is be said ﬁrst that there are methods where the derivative
is calculated only once and although the forward solution is calculated repeatedly
as the conductivity is updated. This is the diﬀerence between Newton-Kantorovich
method and Newton’s method. There are also Quasi-Newton methods in which the
Jacobian is update approximately from the forward solutions that have been made.
Indeed this has been used in geophysics [32]. It also worth pointing out that were
the conductivity is parameterized in a non-linear way for example using shapes of an
anatomical model, that the Jacobian with respect to those new parameters can be
calculated using the chain rule.

5.1. Perturbation in power

We take for simplicity the real case ξ = σ. Using the weak form of ∇ · σ∇u = 0 (or
Green’s identity), for any w

σ∇u · ∇w dV =

wσ

dS

∂u
∂n

Z∂Ω

ZΩ

Here dV and dS are volume and surface measures. We use the complete electrode
model with contact impedance zl which says that on electrode El

for a constant Vl, the total current on electrode El is

u = Vl − zlσ

∂u
∂n

σ

∂u
∂n

ZEl

dS = Il

and ∂u/∂n = 0 away from electrodes. For the special case w = u we have the power
conservation formula,

σ|∇u|2 dV =

u σ

dS =

∂u
∂n

Z∂Ω

Vl − zlσ

dS (5)

∂u
∂n (cid:19)

σ

∂u
∂n

ZEl (cid:18)

Xl

hence

ZΩ

ZΩ

σ|∇u|2 dV +

2

∂u
∂n (cid:19)

zl

σ

(cid:18)

=

VlIl

Xl

ZEl

Xl

This simply states that the power input is dissipated either in the domain Ω or by the
contact impedance layer under the electrodes.

5.2. Standard formula for Jacobian

We now take perturbations σ → σ + δσ, u → u + δu and Vl → Vl + δVl, with the
current in each electrode Il held constant. We calculate the ﬁrst order perturbation,
and argue as in [16] that the terms we have neglected are higher order in the L∞ norm
on δσ. The details of the calculation are given for the complete electrode model case
in [39], the result is

IlδVl = −

δσ|∇u|2 dV

ZΩ

Xl

(2)

(3)

(4)

(6)

(7)

10

(8)

(9)

(10)

Developments in EIT Reconstruction Algorithms

This gives only the total change in power, to get the change in voltage on a
particular electrode Em when a current pattern is driven in some or all of the other
I m
l = δlm.
electrodes we simply solve for the special ‘measurement current pattern’
To emphasize the dependance of the potential on a vector of electrode currents
I = (I1, . . . , IL) we write u(I). The hypothetical measurement potential is u(Im),
by contrast the potential for the d-th drive pattern u(Id). Applying the power
perturbation formula (7) to u(Id) + u(Im) and u(Id) − u(Im) and then subtracting
gives the familiar formula

e

δVdm = −

δσ∇u(Id) · ∇u(Im) dV

ZΩ

While this formula gives the Frechet derivative for δσ ∈ L∞(Ω), considerable care is
needed to show that the voltage data is Frechet diﬀerentiable in other norms, such as
those needed to show that the total variation regularization scheme works [57].

In the special case of the Sheﬃeld adjacent pair drive, adjacent pair measurement
protocol, commonly used in two dimensional EIT we have potentials ui for the i-th
drive pair and voltage measurement Vij for a constant current I

δVij = −

δσ∇ui · ∇uj dV

1
I 2 ZΩ

To calculate the Jacobian matrix one must choose a discretizarion of the conductivity.
The simplest case is to take the conductivity to be piecewise constant on polyhedral
domains such as voxels or tetrahedral elements. Taking δσ to be the characteristic
function of the k-th voxel we have for a ﬁxed current pattern

∂Vdm
∂σk

= −

Zvoxel k

∇u(Id) · ∇u(Im) dV

For the case of a complex admittance one must repeat this calculation taking care
to use the real component of dissipated power Vl ¯Il. Some EIT and capacitance
tomography systems use a constant voltage source and in this case the change in
power of an increase in admittivity will have the opposite sign to the constant current
case.

Some iterative nonlinear reconstruction algorithms, such as nonlinear Landweber,
or non-linear conjugate gradient [56] require the evaluation of transpose (or adjoint)
of the Jacobian multiplied by a vector J ∗z. For problems where the Jacobian is very
large it may be undesirable to store the Jacobian and then apply its transpose to z.
Instead the block of zi corresponding to the ith current drive is written as distributed
source on the measurement electrodes. A forward solution is performed with this as
the boundary current pattern so that when this measurement ﬁeld is combined with
the ﬁeld for the drive pattern as 10, and this block accumulated to give J ∗z. For
details of this applied to diﬀuse optical tomography see [2], and for a general theory
of adjoint sources see [56]

For fast calculation of the Jacobian using (10) one can precompute the integrals
of products of ﬁnite element basis functions over elements.
If non constant basis
functions are used on elements, or higher order elements used one could calculate the
product of gradients of FE basis functions at quadrature points in each element. As
this depends only on the geometry of the mesh ans not the conductivity this can be
precomputed unless one is using an adaptive meshing strategy. The same data is used
in assembling the FE system matrix eﬃciently when the conductivity has changed
but not the geometry. It is these factors particularly which make current commercial
FEM software unsuitable for use in an eﬃcient EIT solver.

Developments in EIT Reconstruction Algorithms

11

6. Inverse crimes and common pitfalls

The ill-posed nature of inverse problems means that any reconstruction algorithm will
have limitations on what images it can accurately reconstruct and that the images
degrade with noise in the data. When developing a reconstruction algorithm it is usual
to test it initially on simulated data. Moreover the reconstruction algorithms typically
incorporates a forward solver. A natural ﬁrst test is to use the same forward solver to
generate simulated data with no simulated noise and to then ﬁnd to one’s delight that
the simulated conductivity can be recovered fairly well, the only diﬃculties being if it
violates the a priori assumptions built into the reconstruction and the limitations of
ﬂoating point arithmetic. Failure of this basic test is used as a diagnostic procedure
for the program.

6.1. Use a diﬀerent mesh

If one is fortunate enough to have a good data collection system and phantom, and
someone skilled enough to make some accurate measurements with the system one
could then progress to attempting to reconstruct images from experimental data.
However more often the case that the next stage is to test further with simulated
data and it at this stage that one must take care not to cheat and commit a so called
“inverse crime”. The best practise is to use an independent forward model, at the very
least in the case of a ﬁnite element forward model one would use a much ﬁner mesh,
and one which was not a strict reﬁnement of the mesh used in the forward solver in the
reconstruction. It is an obvious point perhaps but the simulated conductivity must be
represented on this mesh and unless we are explicitly using a priori information about
for example the location of the boundary of an anomaly the mesh used for simulate
data should not be ‘known’ to the reconstruction program.

6.2. Simulating noise

It is necessary to study the eﬀect of measurement error on the reconstruction. The
ﬁrst reﬂex of the mathematically trained is to use a pseudo random number generator
to add independent Gaussian noise to each measurement with an identical variance.
There are some tightly controlled laboratory situations where experimentalists can
carefully calibrate their apparatus to remove systematic error leaving only random
error from discretization of measurements and random eﬀects such as thermal noise.
By averaging large numbers of measurements the Central Limit Theorem means that
independent identical Gaussian noise is a good approximation to the true statistics of
the data error. However in electrical imaging we are rarely in this fortunate situation.
Even using phantom tanks there are many sources of error which are hard to calibrate
away. The statistical characterisation of instrumentation error in EIT is a topic which
In vivo studies suﬀer further sources of error
requires considerable further study.
including variable contact impedance, motion artifact, variable surface geometry all
of which produce correlated errors in the data.

Even simple simulation of discretization error requires some understanding of the
measurement system one has in mind. While the data is discretized into a binary
representation of the voltage with a ﬁxed precision by an analogue to digital converter
(ADC), the input to the ADC has already been scaled by an ampliﬁer. In a Sheﬃeld-
type adjacent pair drive system [4] this scale factor is determined by the position of
the measurement pair relative to the drive pair to make best use of the range of the

Developments in EIT Reconstruction Algorithms

12

ADC. Multiple drive systems employ diﬀerent strategies [19, 59]. One should at least
add identically distributed noise to suitably scaled measurements. Whatever scheme
is chosen for simulating noise it should be carefully described, often a phrase such
as “5% random noise was added” is used without saying if this is 5% of the largest
voltage (which could completely destroy smaller measurements) or that percentage of
each measurement.

6.3. Pseudo random numbers

Two small caveats about the use of pseudo random number generators, such as the
randn function in Matlab. First that unless the seed for the pseudo-random number
generator is changed the same sequence of random numbers is generated each time
Matlab is started. It possible therefore that many of us are using the same sequence of
“random” numbers! Secondly that for each error level one should generate a sequence
or ensemble of pseudo-random errors and perform the reconstruction for each. One
can then ﬁnd the mean error in reconstruction and the mean error in ﬁtting the
data, as well as their variance. As the EIT reconstruction problem is nonlinear adding
Gaussian error to the data will not produce even multivariate Gaussian reconstruction
error, so strictly one would have to consider the full distribution of the errors, but for
small noise levels one can expect a linear approximation to be valid. In practise the
danger is that the pseudo-random sequence will produce an out-lier, and it one is
plotting reconstruction error against data error one would notice the discrepancy,
which presumably explains why the culprits usually get away with the “inverse
misdemeanour” of not using an ensemble of pseudo random errors.

6.4. Thou shallst not tweak

Finally there is some sharp practise which applies equally to simulated and
experimental results. The ﬁrst of these we will call “tweaking”. Reconstruction
programs have a number of adjustable parameters such as Tikhonov factors and
stopping criteria for iteration, as well as levels of smoothing, basis constraints and small
variations of an algorithms. While there are rational ways of choosing reconstruction
parameters based on the data (such as generalized cross validation and L-curve), and
on an estimate of the data error (Morotzov’s stopping criterion), a practical procedure
often employed is to simulate, or measure experimentally, a variety of conductivity
distributions the ﬁnd by trial and error parameters in the reconstruction program
which do a reasonably well, knowing what the image was meant to be. One could
compare this with the training set used for a neural net. We would then not be
surprised to ﬁnd that conductivity distributions close to the training set can be
reconstructed satisfactorily. A more interesting test is to see how the algorithm
performs when we deviate from the training set. There are plenty of examples of
conference papers where reconstruction algorithms are shown to perform very well
on single circular anomalies, and reconstructions of complex objects with varying
contrasts are absent. As we know the problem is illposed, it is inevitable that there are
some conductivity distributions which can not be reconstructed well with a particular
algorithm, in particular ones which violate the a priori assumptions. It is therefore
no dishonour to present the failures as well as the successes. To avoid the temptation
to tweak the algorithm to produce the best results given that the correct result is
known, the best procedure would be to conduct blind trials for both simulated and

Developments in EIT Reconstruction Algorithms

13

experimental data.

The author is aware that what is suggested here is best practise is a high standard
to aim for, and that examples of this crimes and misdemeanours can be found in
his own work, but the intention is to elevate the standard in published work in
EIT reconstruction generally and to highlight these pitfalls. While in conference
presentations it is accepted to describe work in progress, and to confess to any inverse
crimes and misdemeanours, it is the author’s opinion that they should be avoided in
journal publications.

7. Further developments in reconstruction

In this breif review there is not space to describe in any detail many of the exciting
current development in reconstruction algorithms. Fortunately many of them are
treated in other articles in this special issue. Before highlighting some of these
developments it is worth emphasising that for illposed problem a priori information is
essential for a stable reconstruction algorithms, and it is better that this information
is incorporated in the algorithm in a systematic and transparent way. Another general
priciple of inverse problems is to think carefully what information is required by the end
user. Rather than attempting to produce an accurate image what is often required
in medical (and indeed most other) applications is an estimate of a much smaller
number of parameters which can be used for diagnosis. For example we may know
that a patient has two lungs as well as other anatomical features but we might want to
estimate their water content to diagnose pulminary oedema. A sensible strategy would
be devise an anatomical model of the thorax and ﬁt a few parameters of shape and
conductivity rather than pixel conductivity values. The disadvantage of this approach
is that each application of EIT gives rise to its own specialised reconstruction method,
which must be carefully designed for the purpose. In the author’s opinion the future
development of EIT systems, including electrode arrays and data acquisition systems
as well as reconstruction software, should focus increasingly on speciﬁc applications,
although of course such systems will share many common components.

7.1. Beyond Tikhonov regularization

We have also discussed the use of more general regularization functionals including
total variation. For smooth G traditional smooth optimization techniques can be used,
whereas for total variation the Primal Dual Interior Point Method is promising [6].
In general there is a trade-oﬀ between incorporating accurate a priori information
and speed of reconstruction. Where the regularization term is a partial diﬀerential
operator, the solution of the linearized problem is a compact perturbation of a partial
diﬀerential equation. This suggests that multigrid methods may be used in the
solution of the inverse problem as well. For a single linearized step this has been
done for the EIT problem by McCormick and Wade [34], and for the non-linear
problem by Borcea [10].
In the same vein adaptive meshing can be used for the
inverse problem [35]. In both cases there is the interesting possibility to explore the
interaction between the meshes used for forward and inverse solution.

At the extreme end of this spectrum we would like to describe the prior probability
distribution and for a known distribution of measurement noise and calculate the entire
posterior distribution. Rather than giving one image, such as the MAP estimate
If the probability is
gives a complete description of the probability of any image.

Developments in EIT Reconstruction Algorithms

14

bimodal for example, one could present the two local maximum probability images.
If one needed a diagnosis, say of a tumour, the posterior probability distribution
could be used to calculate the probability that a tumour like feature was there.
The computational complexity of calculating the posterior distribution for all but the
simplest distributions is enormous, however the posterior distribution can be explored
using the Markov Chain Monte Carlo Method (MCMC) which has been applied to
two dimensional EIT [37]. For this to be a viable technique for the 3D problem highly
eﬃcient forward solution will be required.

7.2. Direct non-linear methods

Iterative methods which use optimization methods to solve a regularized problem are
necessarily time consuming. The forward problem must be solved repeatedly and the
calculation of an updated conductivity is also expensive. The ﬁrst direct method to
be proposed was the Layer Stripping algorithm [46] however this is yet to be shown to
work well on noisy data. An exciting recent development is the implementation of a
Scattering Transform algorithm proposed by Nachman. Siltanen [45] showed that this
can be implemented stably and applied to in vitro data. The main limitation of this
technique is that is inherently two dimensional and no one has found a way to extend
it to three dimensions, also in contrast to the more explicit forms of regularization it
is not clear what a priori information is incorporated in this method as the smoothing
is applied by ﬁltering the data. A strength of the method is its ability to accurately
predict absolute conductivity levels. In some cases where long electrodes can be used
and the conductivity varies slowly in the direction in which the electrodes are oriented
a two dimensional reconstruction may be a useful approximation. This is perhaps
more so in industrial problems such as monitoring ﬂow in pipes with ECT or ERT. In
some situations a direct solution for a two dimensional approximation could be used
as a starting point for an iterative three dimensional algorithm.

Two further direct methods show considerable promise for speciﬁc applications.
The monotonicity method of Tamburrino et al [49] relies on the monotonicity of the
map ρ 7→ Rρ where ρ is the real resistivity and Rρ the transfer impedance matrix.
This method, which is extremely fast, relies on the resistivity of the body to be known
to be one of two values. It works equally well in two and three dimensions and is
robust in the presence of noise. The time complexity scales linearly with the number
of voxels (which can be any shape) and scales cubically in the number of electrodes.
It works for purely real or imaginary admittivity, (ERT or ECT), and for Magnetic
Induction Tomography for real conductivity. It is not known if it can be applied to
the complex case and it requires the voltage on current carrying electrodes.

Linear sampling methods [12, 44] have a similar time complexity and advantages
as the monotonicity method. While still applied to piecewise constant conductivities,
linear sampling methods can handle any number of discrete conductivity values
provided the anomalies separated from each other by the background. The method
does not give an indication of the conductivity level but rather locates the jump
discontinuities in conductivity. Both monotonicity and linear sampling methods are
likely to ﬁnd application in situations where a small anomaly is to be detected and
located, for example breast tumours.

Finally a challenge remains to recover anisotropic conductivity which arises in
applications from ﬁbrous or stratiﬁed media (such as muscle), ﬂow of non-spherical
particles (such as red blood cells), or from compression (for example in soil). The

Developments in EIT Reconstruction Algorithms

15

inverse conductivity problem at low frequency is known to suﬀer from insuﬃciency of
data, but with suﬃcient prior knowledge (for example [30]) the uniqueness of solution
can be restored. One has to take care that the imposition of a ﬁnite element mesh
does not predetermine which of the family of consistent solutions is found [1].

8. Conclusions

In conclusion, until medical EIT data is reconstructed using the best available
methods the results will be inconclusive. Many an experimental study has be spoilt
when carefully collected data has been attacked by a crude two-dimensional linear
reconstruction algorithm and the resulting ‘blurry blobs’ taken as evidence that EIT
is not suitable for the desired task. Careful consideration of a priori information,
measurement error and the model parameters required is needed, together with close
collaboration between mathematicians and experimentalists. Periodically a mood
arises at an EIT meeting that the technique will never ﬁnd real application in medicine,
however it is my contention that a particular application should not be dismissed as
impossible until both hardware and software specialists working together have given
it their ‘best shot’.

9. References

[1] Abscal JP,2003, MSc Thesis University of Manchester.
[2] Arridge S 1999 Optical tomography in medical imaging Inverse Problems 15 R41-93
[3] Barber D and Brown B, 1986, Recent developments in applied potential tomography-apt
Information Processing in Medical Imaging ed S L Bacharach (Amsterdam: Nijhoﬀ) pp 106
21

[4] Brown B H and Seagar A D 1987 The Sheﬃeld data collection system Clin. Phys. Physiol. Meas.

8 Suppl A 91-7

[5] RH Bayford, A Gibson, A Tizzard, AT Tidswell and DS Holder, 2001. Solving the forward
problem for the human head using IDEAS (Integrated Design Engineering Analysis Software)
a ﬁnite element modelling tool. Physiological Measurements, 22, 55-63.

[6] Borsic A, 2002, Regularization Methods for Imaging from Electrical Measurements, PhD Thesis,

Oxford Brookes University.

[7] Borsic A, McLeod CN and Lionheart WRB, 2001, Total variation regularisation in EIT

reconstruction 2nd World Congr. on Industrial Process Tomography (Hannover)

[8] Borsic A, Lionheart WRB, McLeod CN, 2002 Generation of anisotropic-smoothness

regularization ﬁlters for EIT, IEEE Transactions of Medical Imaging, 21, 596 -603.

[9] Babuska I, and Strouboulis T, 2001, The Finite Element Method and its Reliability, (Oxford:

[10] Borcea L 2001 A nonlinear multigrid for imaging electrical conductivity and permittivity at low

Oxford University Press)

frequency Inverse Problems 17 329 59

[11] Bossavit A, 1998, Computational Electromagnetism, Variational formulations, Edge elements,

Complementarity, (Boston: Academic Press).

[12] Br¨uhl M, 2001, Explicit Characterization of Inclusions in Electrical Impedance Tomography,

[13] Breckon WR, 1990, Image Reconstruction in Electrical Impedance Tomography, Ph.D. Thesis,

SIAM J. Math Anal, 32, 1327-1341

Oxford Brookes Polytechnic.

[14] Breckon WR and Pidcock MK, 1988, Data errors and reconstruction algorithms in electrical

impedance tomography Clin. Phys. Physiol. Meas. 9 No 4A, 105-109

[15] Byars M, 2001, Developments in Electrical Capacitance Tomography, Proc. World Congress on

Industrial Process Tomography, Hannover, 542-?.

[16] Calder´on AP,1980, On an inverse boundary value problem. In Seminar on Numerical Analysis
and Its Applications to Continuum Physics, pp. 67-73, Rio de Janeiro, Sociedade Brasileira
de Matematica.

[17] Cheney, M, Isaacson D, Newell JC, Simske S and Goble J, 1990, NOSER: An algorithm for

solving the inverse conductivity problem. Int. J. Imaging Systems & Technology 2, 66-75.

Developments in EIT Reconstruction Algorithms

16

[18] COMSOL, The FEMLAB reference manual, COMSOL AB, Stockholm, 2000.
[19] Cook RD., Saulnier GJ, Gisser DG, Goble JC, Newell JC and Isaacson D., 1994, ACT 3: A high
speed high precision electrical impedance tomograph. IEEE Trans. Biomed. Eng. 41, 713-722.
[20] Gilbert, JR, Moler C, and Schreiber R, Sparse, 1992 Matrices in MATLAB: Design and

Implementation, SIAM Journal on Matrix Analysis and Applications 13, 333-356.

[21] Gisser G, Isaacson D and Newell JC, 1987, Current topics in impedance imaging, Clin. Phys.

[22] Golub GH and Van Loan CF, 1996, Matrix Computations, 3rd ed. (Baltimore, MD: Johns

Physiol. Meas., 8 Suppl A, 39-46.

Hopkins University Press).

[23] Greenleaf, A and Uhlmann G, 2001, Local uniqueness for the Dirichlet-to-Neumann map via the

two-plane transform. Duke Math. J. 108, 599–617.

[24] Hansen PC, 1998, Rank-deﬁcient and discrete ill-posed problems: numerical aspects of linear

inversion, (Philadelphia: SIAM).

[25] Heikkinen LM, Vilhunen T, West RM and Vauhkonen M, 2002, Simultaneous reconstruction of
electrode contact impedances and internal electrical properties: II. Laboratory experiments
Meas. Sci. Technol. 13, 1855-1861

[26] Kolehmainen K, Arridge SR, Lionheart WRB, Vauhkonen M and Kaipio JP, 1999, Recovery
of region boundaries of piecewise constant coeﬃcients of elliptic PDE from boundary data,
Inverse Problems 15, 1375-1391

[27] Kolehmainen V, Vauhkonen M, Kaipio JP and Arridge SR, 2000, Recovery of piecewise constant

coeﬃcients in optical diﬀusion tomography, Optics Express 7, 468-480.

[28] Kotre C J 1989 A sensitivity coeﬃcient method for the reconstruction of electrical impedance

tomograms Clin. Phys. Physiol. Meas. 10 275-81

[29] Lionheart WRB, 1999, Uniqueness, shape, and dimension in EIT, Annals New York Acad. Sci.,

[30] Lionheart WRB, 1997, Conformal uniqueness results in anisotropic electrical impedance imaging.

873, 466-471

Inverse Problems,13, 125-134.

[31] Loke M.H. and Barker R., 1996. Rapid least-squares inversion of apparent resistivity

pseudosections by a quasi-Newton method: Geophysical Prospecting, 44, 131-152.

[32] Loke MH and Barker RD, 1996, Practical techniques for 3D resistivity surveys and data inversion,

Geophysical Prospecting, 44, pp. 499-523.

[33] Mayavi, 2003, The MayaVi Data Visualizer, http://mayavi.sourceforge.net
[34] McCormick S F and Wade J G, 1993 Multigrid solution of a linearized, regularized least-squares

problem in electrical impedance tomography Inverse Problems 9 697 713

[35] Molinari M, 2003, High Fidelity Imaging in Electrical Impedance Tomography. PhD Thesis,

University of Southampton.

J. Appl. Math. 11, 431-441.

[36] Marquardt D. 1963, An algorithm for least squares estimation of nonlinear parameters. SIAM

[37] Meng S, West R, Ackroyd R, 2003,Markov Chain Monte Carlo techniques and spatial-temporal,

modelling for medical EIT, this issue.

[38] Natterer F, 1982, The Mathematics of Comuterized Tomogrpahy, Wiley
[39] Polydorides N and Lionheart WRB, 2002,A Matlab toolkit for three-dimensional electrical
impedance tomography: a contribution to the Electrical Impedance and Diﬀuse Optical
Reconstruction Software project, Meas. Sci. Technol. 13 1871-1883

[40] Polydorides, N, 2002, Image Reconstruction Algorithms for Soft Field Tomography, PhD Thesis,

UMIST.

[41] Polydorides, N Lionheart, WRB McCann, H, 2002, Krylov subspace iterative techniques: on the
detection of brain activity with electrical impedance tomography. IEEE Trans Med Imaging,
21, 596 -603.

[42] Saad Y and Schultz MH, 1986, GMRES: A generalized minimal residual algorithm for solving

nonsymmetric linear systems, SIAM J. Sci. Statist. Comput., 7, 856-869.

[43] Santosa F, Vogelius M, 1991, A backprojection algorithm for electrical impedance imaging, SIAM

J. Appl. Math. 50, 216-243

[44] Schappel B, 2003, Electrical Impedance Tomography of the Half Space: Locating Obstacles
by Electrostatic Measurements on the boundary, Proceedings of the 3rd World Congress on
Industrial Process Tomography, Banﬀ, Canada, September 2-5, 788-793.

[45] Siltanen S, Mueller J, Isaacson D, 200, An implementation of the reconstruction algorithms of

Nachman for the 2D inverse conductivity problem, Inverse Problems, 16, 2000, 681-699.

[46] Somersalo E, Isaacson D and Cheney M 1992 A linearized inverse boundary value problem for

Maxwell s equations J. Comput. Appl. Math. 42 123 36

[47] Soleimani M, Powell C, 2003, Black-box Algebraic Multigrid for the 3D Forward Problem arising

Developments in EIT Reconstruction Algorithms

17

in Electrical Resistance Tomography, this issue.

[48] Somersalo E, Cheney M and Isaacson D, 1992, Existence and uniqueness for electrode models

for electric current computed tomography. SIAM J. Appl. Math., 52, 1023-1040.

[49] Tamburrino A, Rubinacci G, 2002, A new non-iterative inversion method in electrical resistance

tomography, Inverse Problems, 18, 2002

[50] A Tarantola, 1987, Inverse Problem Theory, Elsevier.
[51] Vauhkonen PJ, Vauhkonen M, Kaipio JP, Errors due to the truncation of the computational

domain in static three-dimensional electrical impedance tomography. ???

[52] Vauhkonen M, Karjalainen PA, and Kaipio JP, 1998, A Kalman Filter ap- proach to track
fast impedance changes in electrical impedance tomogra- phy, IEEE Trans Biomed Eng, 45,
486-493.

[53] Vauhkonen M, 1997, Electrical Impedance Tomography and Prior Information, PhD Thesis,

University of Kuopio.

[54] Vauhkonen PJ, 1999, Second order and Inﬁnite Elements in Three-Dimensional Electrical
Impedance Tomography, Phil.Lic. thesis, Department of Applied Physics, University of
Kuopio, Finland, report series ISSN 0788-4672, report No. 2/99.

[55] Vauhkonen M, Lionheart WRB, Heikkinen LM, Vauhkonen PJ and Kaipio JP, 2001 A Matlab
package for the EIDORS project to reconstruct two-dimensional EIT images Physiol. Meas.
22 107-11

[56] Vogel. C, 2001, Computational methods for inverse problems (Philadelphia: SIAM).
[57] Wade JG, Senior K and Seubert S, 2003 Convergence of Derivative Approximations in the Inverse

Conductivity Problem, SIAM J. Appl. Math, to appear.

[58] York T (ed), 1999, Proceedings of the 1st World Congress on Industrial Process Tomography,

Buxton, UK, (Leeds: VCIPT)

[59] Zhu QS, McLeod CN, Denyer CW, Lidgey FJ andLionheart WRB, 1994, Development of a

real-time adaptive current tomograph Physiol. Meas. 15, A37-A43


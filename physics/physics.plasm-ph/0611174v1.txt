6
0
0
2
 
v
o
N
 
7
1
 
 
]
h
p
-
m
s
a
l
p
.
s
c
i
s
y
h
p
[
 
 
1
v
4
7
1
1
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

dHybrid : a massively parallel code for hybrid
simulations of space plasmas

L. Gargat´e a,b,

∗ R. Bingham b,c R. A. Fonseca d,a L. O. Silva a
aGoLP/CFP, Instituto Superior T´ecnico, Av. Rovisco Pais, Lisbon, Portugal
bRutherford Appleton Laboratory, Chilton, Didcot, OXON, OX11 0QX, UK
cDepartment of Physics, University of Strathclyde, Glasgow G4 ONG, Scotland
dDCTI, Instituto Superior de Ciˆencias do Trabalho e da Empresa, Av. For¸cas
Armadas, Lisbon, Portugal

Abstract

A massively parallel simulation code, called dHybrid, has been developed to perform
global scale studies of space plasma interactions. This code is based on an explicit
hybrid model; the numerical stability and parallel scalability of the code are studied.
A stabilization method for the explicit algorithm, for regions of near zero density, is
proposed. Three-dimensional hybrid simulations of the interaction of the solar wind
with unmagnetized artiﬁcial objects are presented, with a focus on the expansion of
a plasma cloud into the solar wind, which creates a diamagnetic cavity and drives
the Interplanetary Magnetic Field out of the expansion region. The dynamics of
this system can provide insights into other similar scenarios, such as the interaction
of the solar wind with unmagnetized planets.

Key words: hybrid codes, particle MHD codes, space plasmas, AMPTE, artiﬁcial
atmospheres, solar wind
PACS: 52.65.Ww, 52.65.Kj, 96.50.Ek

1 Introduction

To understand many space plasma scenarios, such as the solar wind interaction
with cometary atmospheres or with unmagnetized planets (e.g. Mars) [1,2] it
is usually necessary to invoke the dynamics of ions. On one hand, MHD codes

∗ Corresponding author. Tel.: +351 21 8419336; fax: +351 21 8464455

Email address: luisgargate@cfp.ist.utl.pt (L. Gargat´e).

Preprint submitted to Elsevier Science

2 February 2008

cannot always capture all the physics, (e.g. ﬁnite Larmor radius eﬀects). On
the other hand, full particle in cell (PIC) codes are computationally demanding
and it is not always possible to simulate large scale space phenomena [3,4,5].
Hybrid codes are useful in these problems, where the ion time scale needs to be
properly resolved and the high frequency modes on the electron time/length
scales do not play a signiﬁcant role.

To address problems where the hybrid approximation is necessary we have
developed the code dHybrid, in which the ions are kinetic (PIC) and the elec-
trons are assumed massless and treated as a ﬂuid [6,7,8]. There is also the
possibility to use it as a particle MHD code [9,10], by neglecting the ion ki-
netics. The present version of dHybrid is fully parallelized, thus allowing for
the use of massively parallel computers. Advanced visualization is performed
by taking advantage of the close integration with the OSIRIS data analysis
and visualization package [11].

A stability analysis of the algorithm is performed, and stabilization mecha-
nisms (associated with numeric instabilities due to very low density regions)
with minimum impact on performance are discussed. The parallel scalability
and performance of the algorithm is also presented.

The dHybrid framework allows the study of a wide class of problems, including
global studies of space plasma shock structures. This is illustrated by a set of
simulations of an artiﬁcial gas release in the solar wind, depicting the AMPTE
release experiments. The relevance of the example chosen is due to its resem-
blance to the solar wind interaction with planetary/cometary exospheres (e.g.
Mars and Venus)[12,13], thus illustrating the possible scenarios to be tackled
with dHybrid.

In the following section we describe the hybrid model used in dHybrid. Its
numerical implementation, focusing on stability and parallel scalability, is pre-
sented in section 3. We then describe the key features of the shock structures
formed by the solar wind interaction with an unmagnetized artiﬁcial atmo-
sphere. Comparison between present three dimensional simulations and pre-
vious 2D simulations [13,14] are also presented in section 4. Finally, we state
the conclusions.

2 Hybrid model and dHybrid

Hybrid models are commonly used in many problems in plasma physics (for a
review see, for instance, ref. [6]). When deriving the hybrid set of equations the
displacement current is neglected in Amp`ere’s Law and the kinetics of electrons
is not considered. Various hybrid approximations can be considered if electron

2

mass, resistivity and electron pressure are included or not in the model. Quasi-
neutrality is also implicitly assumed. The appropriate approximation is chosen
in accordance to which time scales and spatial scales are relevant for the
dynamics of the system.

In dHybrid, the electron mass, the resistivity and the electron pressure are not
considered, but due to the code structure, such generalization is straightfor-
ward. Shock jump conditions (i.e. Rankine-Hugoniot relations) are altered by
implicitly neglecting the electron temperature. The diﬀerences are signiﬁcant
when the β of the plasma dominating the shock is high and in this case care
should be taken when analyzing results. The electric ﬁeld under these con-
~B, in which ~Ve is the electron ﬂuid velocity. The
ditions is thus ~E =
electric ﬁeld is perpendicular to the local magnetic ﬁeld, since the massless
electrons short-circuit any parallel component of the electric ﬁeld, and it can
be determined from

~Ve ×
−

~E =

~V
−

×

~B +

1
neµ0

(cid:16)∇ ×

~B(cid:17) ×

~B

n R

where ~V = 1
fi ~v d~v is the ion ﬂuid velocity. The magnetic ﬁeld is advanced
in time through Faraday’s law where the electric ﬁeld is calculated from eq.
(1). Ions in the hybrid model have their velocity determined by the usual
Lorentz force. The Boris particle pusher is used, using the electric ﬁeld and
the magnetic ﬁeld to advance velocities in time [15]. In the particle MHD
model one uses

1

d~v
dt

=

µ0nM (cid:16)∇ ×

~B(cid:17) ×

~B +

kBT
nM ∇

n

to determine individual particle velocities, where the second term on the right
hand side is the pressure term for the ions, assuming an adiabatic equation
of state, and where kB is the Boltzmann constant. Ion ﬂuid velocity is then
obtained in the usual manner, integrating over the velocities [9].

The ion species in dHybrid are then always represented by ﬁnite sized particles
to be pushed in a 3D simulation box. The ﬁelds and ﬂuid quantities, such as
the density n and ion ﬂuid velocity ~V , are interpolated from the particles using
quadratic splines [16] and deﬁned on a 3D regular grid. These ﬁelds and ﬂuid
quantities are then interpolated back to push the ions using quadratic splines,
in a self consistent manner. Equations are solved explicitly, based on a Boris
pusher scheme to advance the particles [15] in the hybrid approach, and a two
step Lax-Wendroﬀ scheme to advance the magnetic ﬁeld [4,5]. Both schemes
are second order accurate in space and time, and are time and space centered.

The present version of dHybrid uses the MPI framework as the foundation of

(1)

(2)

3

the communication methods between processes, and the HDF5 framework as
the basis of all diagnostics. The three-dimensional simulation space is divided
across processes, and 1D, 2D and 3D domain decompositions are possible.

The code can simulate an arbitrary number of particle species and, for each of
them, either the particle MHD or the hybrid model can be applied. Periodic
boundary conditions are used for both the ﬁelds and the particles, and ion
species are simulated with arbitrary charge to mass ratios, arbitrary initial
thermal velocity and spatial conﬁgurations. This ﬂexibility allows for simula-
tions where only the kinetic aspects of one of the ion species is followed.

Normalized simulation units are considered for all the relevant quantities.
Time is normalized to λpi/cs, space is normalized to λpi, mass is normalized
to the proton mass mp and charge is normalized to the proton charge e, where
λpi = c/ωpi is the ion collisionless skin depth with ωpi = qn0e2/ǫ0mp and
where cs = q(kBTi + kBTe)/mp is ion the sound velocity. In this system the
magnetic ﬁeld is normalized to mp cs/e λpi and the electric ﬁeld is normalized
to mp c2
s/e λpi. Henceforth all equations will be expressed in these normalized
units.

Using the described implementation of the hybrid model, dHybrid can model a
wide range of problems, from unmagnetized to magnetized plasmas in diﬀerent
conﬁgurations.

3 Stability and scalability of dHybrid

The stability criterion on the time-step for all the algorithm is determined by
the Lax-Wendroﬀ method, as this is usually more stringent than the stability
condition for the Boris algorithm due to the rapid increase in the Alfv`en
velocity as the density goes to zero. The discretized equation (1) is

~En

i,j,k =

~V n
i,j,k ×

−

~Bn

i,j,k +

~Bn

(cid:16)∇ ×

i+1/2,j+1/2,k+1/2(cid:17) ×

~Bn

i,j,k

(3)

1
nn

i,j,k

and the two step space-centered and time-centered Lax-Wendroﬀ scheme to
solve Faraday’s law is

~Bn+1/2

i+1/2,j+1/2,k+1/2 = ~Bn

i+1/2,j+1/2,k+1/2 −

∆ t
2 (cid:16)∇ ×

~En

i,j,k(cid:17)

~Bn+1

i,j,k = ~Bn

i,j,k −

∆ t (cid:16)∇ ×

~En+1/2

i+1/2,j+1/2,k+1/2(cid:17)

(4)

(5)

4

where ∆t represents the time step, the 1/2 index represent values displaced
by half cell size, where i, j and k represent grid points along x, y and z and n
represents the iteration step. These equations thus require the use of staggered
grids [17], where the displaced terms are calculated using an average of the
eight neighbor values around a given point.

−

i,j,k, nn

i,j,k and Bn

i,j,k from eq. (3), (iii) the magnetic ﬁeld Bn+1/2

The general layout of the dHybrid algorithm is as follows. One starts of with
1/2 and n (interpolated from
particle positions at time n, velocities at time n
1/2), and the magnetic ﬁeld at time n grid 1 (position indexes i, j, k).
time n
−
In step (i) V n
i+1/2,j+1/2,k+1/2 are calculated from particles veloc-
ities, positions and from B values in grid 1, (ii) electric ﬁeld is calculated at
En
i+1/2,j+1/2,k+1/2 is calculated from
eq. (4), (iv) particle velocites are calculated at vn+1/2 using the Boris algo-
rithm, positions at xn+1 and xn+1/2 are calculated from xn+1 = xn + ∆t vn+1/2
and xn+1/2 = (xn + xn+1)/2 and density and ﬂuid velocity are calculated in
grid 2: nn+1/2
i+1/2,j+1/2,k+1/2. In step (v) the magnetic ﬁeld
is calculated at Bn+1/2
i+1/2,j+1/2,k+1/2 is
calculated using eq. (3) displaced half grid cell, (vii) the magnetic ﬁeld is ad-
vanced in time to Bn+1
i,j,k using eq. (5) and ﬁnally, (viii) particle velocities are
advanced via Boris algorithm to vn+1.

from grid 2 values and then (vi) En+1/2

i+1/2,j+1/2,k+1/2 and V n+1/2

i,j,k

To obtain the Courant-Friedrichs-Levy stability condition, linearized versions
of eq. (3) through eq. (5) are used considering constant density, arbitrary
velocities and parallel propagating waves relative to the background magnetic
ﬁeld. The equations are then Fourier transformed to local grid modes, parallel
ei (kx ∆x i−n ω ∆t), where ∆x is the cell size in x.
plane waves

∼

An ampliﬁcation matrix relating ~Bn+1
that all the eigenvalues of the ampliﬁcation matrix be λi ≤
stability criterion

i,j,k with ~Bn

i,j,k is then obtained. Requiring
1, yields the

n ∆x2

∆t

≤

n Vx ∆x

|

B

|

±

(6)

where n is the background density, B is the constant magnetic ﬁeld, Vx is
the ion ﬂuid velocity along x and the two signs are due to the two diﬀerent
eigenvalues. The condition (6) sets a limit on the time step or, inversely, given
a time step a limit on the lowest allowable density which can be present in a
grid cell. We stress that all quantities in eq. (6) are expressed in normalized
units. Using the same calculation method, a stability criterion was found in
[18] for similar ﬁeld equations using a diﬀerent implementation of the Lax-
Wendroﬀ algorithm. The stability criterion however is not the same since the
speciﬁcs of the numerical approach diﬀer: our implementation, described in
[6], makes use of staggered grids to improve accuracy and guarantee that the

5

equations are always space centered.

As can be seen from eq. (3), under the MHD and hybrid models, the algo-
rithm breaks in regions where the density is close to zero [19]. The problem
is either physical, if particles are pushed out of a given region of space by a
shock or other means, or it can be a numerical artifact due to poor particle
statistics. One method to tackle this problem is by an implicit calculation of
the electric ﬁeld [6], which requires an iteration method to solve the electric
ﬁeld equation. One other method, discussed in [19,20] involves the use of an
artiﬁcial resistivity.

If the problem is physical, it can be avoided by considering that if no particles
exist in a given region of space, then both the charge density ρ and current
density ~J are zero, and the electric ﬁeld is deﬁned by
2 ~E = 0. This equation
has to be solved only on volumes where the density goes below a given stability
threshold. This region can span several processes and thus will involve several
communication steps. Fast elliptic solvers can be used to solve this equation,
although the complex vacuum/plasma boundaries that arise complicate the
problem.

∇

Usually it is found that the problem is numerical in nature and it is only due
to the limited number of computational particles per mesh cell used in the
simulation. Thus, if in any given mesh cell there are too few particles, the
density drops and the Alfv`en velocity vA = B/√µ0nm increases, breaking the
stability criterion for the ﬁeld solver.

Three methods are considered to maintain the stability of the algorithm: (i)
the number of particles per cell can be increased throughout the simulation,
(ii) the time step can be decreased, and (iii) a non-physical background density
can be added as needed in the unstable zones, rendering them stable. The two
former options are obvious, and yield meaningful physical results at expense
of computational time. The last solution is non-physical and, thus, can yield
non-physical results. In short, the ﬁrst two options are chosen and the last
one is implemented to be used only as last resort. Each time the electric ﬁeld
is to be calculated, the density in each mesh cell is automatically checked to
determine if it is below the stability threshold, using eq. (6), and is set to the
minimum value if it is. The minimum density value is thus calculated using
the time step, the grid size, and the local values for the magnetic ﬁeld and
ﬂuid velocity, minimizing the impact of using a non-physical solution. Testing
showed that as long as the number of cells that are treated with this method
is kept low enough (up to 0.1% of the total number of cells), the results do
not change signiﬁcantly.

The approach followed here guarantees good parallel scalability of the algo-
rithm since the algorithm is local. The overall algorithm was also designed

6

as to be as local as possible and to minimize the number of communication
steps between processes. This was accomplished by joining several vectors to
transmit at the same step and it resulted in the following parallel scheme:
(i) after the ﬁrst step in the main algorithm guard cells of nn
i,j,k and
Bn
i+1/2,j+1/2,k+1/2 are exchanged between neighboring processes, (ii) guard cells
of En
i+1/2,j+1/2,k+1/2 are exchanged, (iv)
particles that crossed spacial boundaries are exchanged between neighboring
processes, (v) guard cells of nn+1/2
are
exchanged, (vi) guard cells of En+1/2
i+1/2,j+1/2,k+1/2 are exchanged and ﬁnally (vii)
guard cells of Bn+1

i,j,k are exchanged, (iii) guard cells of Bn+1/2

i+1/2,j+1/2,k+1/2 and Bn+1/2

i+1/2,j+1/2,k+1/2, V n+1/2

i,j,k, V n

i,j,k are exchanged.

i,j,k

Scalability of dHybrid was studied on a Macintosh dual G5 cluster at 2 GHz,
interconnected with a Gigabit ethernet network. The particle push time is
1.7 µs per particle, and the ﬁeld solver time is 8.4% of the total iteration time
for 1000 iterations on a single process.

One plasma species is initialized evenly across the simulation box with a drift
velocity of 5 cs, a thermal temperature of 0.1 cs and a charge to mass ratio of
1 (protons). A perpendicular magnetic ﬁeld with constant intensity of B0 =
5 mp cs e−1 λ−1
is set across the box. The benchmark set up consists of two
diﬀerent ”parallel” scenarios.

pi

Fig. 1. Code speed up when problem size is kept ﬁxed for all simulations (indepen-
dent of np).

In the ﬁrst scenario a 96 cell cubic grid is used, with 8 particles per cell,
all diagnostics oﬀ, and 1000 iterations are performed. The simulation space
is then evenly split up among the number of processes in each run, in a 1D
partition. The average time per iteration is taken, relative to the time per
iteration in a single processor. Fig. 1 compares the ideal speed up against
the achieved results. The minimum speed up obtained is
60% when using
32 processors. We observe that in this case the maximum problem size that
could be set on one machine is limited by memory considerations and therefore,
when the problem is split up by 32 processors, the problem size per processor
is much smaller and the communication time relative to the overall iteration
time starts to penalize the code performance, reaching 40% of the loop time

∼

7

for 32 processors.

Fig. 2. Code speed up when problem size is doubled when doubling the number of
processes.

In the second scenario, the problem size increases proportionally to the number
of processors used. Fig. 2 shows the results for runs with 2, 4, 8, 16 and
32 processor runs. The eﬃciency in this case is 94% showing good parallel
scaling, as expected, with similar eﬃciency as other state-of-the-art massively
parallel codes [11,21]. The total communication time takes 8.5% of the total
iteration time in this case, thus showing that this problem is more indicative
of the parallel eﬃciency of the algorithm; the penalty for scaling up to many
processors is not signiﬁcant. Other test runs with the same setup but with
no magnetic ﬁeld were considered, and the eﬃciency in this case was higher
with a constant value of
99% for all the runs. This indicates that the drop
in eﬃciency patent in Fig. 2 is mainly due to particle load balancing across
processes, induced by the magnetic ﬁeld, which makes particles have a Larmor
radius of

10% the simulation box size in the x dimension.

∼

∼

4 3D simulations of unmagnetized objects

As a test problem for dHybrid, we have modeled the interaction of the solar
wind with an unmagnetized object, mimicking the AMPTE release experi-
ments, thus allowing the validation of the code against the AMPTE experi-
mental results and other codes [18,22,23,24,25,26].

The AMPTE experiments consisted of several gas (Lithium and Barium) re-
leases in the upstream solar wind by a spacecraft orbiting the earth [27,28,29].
After the release, the expanding cloud of atoms is photoionized by the solar
ultraviolet radiation, thus producing an expanding plasma and forming an ob-
stacle to the ﬂowing solar wind. The solar wind drags the Sun’s magnetic ﬁeld
corresponding, at 1 AU, to a fairly uniform background ﬁeld with a magnitude
of about 10 nT in the AMPTE experiments. The measured solar wind density
was nsw = 5 cm−3, ﬂowing with a velocity of vsw ∼
540 km/s and having an ion

8

∼

2.1 km/s.

53 km/s. The cloud expansion velocity was

acoustic speed of cs ∼
A number of codes were developed to study the AMPTE release experiments
in the solar wind, most of them 2D codes. These simulations show that the
MHD model lacked certain key physics [30,31]. The correct modeling of the
cloud dynamics can only be obtained in hybrid simulations, because in the
AMPTE releases, the ion Larmor radius of the solar wind particles is in the
same order of magnitude of the cloud size itself. The problem is intrinsically
kinetic and can only be fully assessed in a 3D simulation as this yields realistic
ﬁeld decays over space providing realistic dynamics for the ions. This is also of
paramount importance for the ion pick up processes in planetary exospheres
[32,33].

In our simulations, the background magnetic ﬁeld was set to B0 = 0.773 nT
with a solar wind velocity of vsw = 80.4 km/s and with a cloud expansion
velocity of vc = 16.1 km/s. The relative pressure of the solar wind plasma due
to the embedded magnetic ﬁeld (low β plasma), and the relative pressure of the
plasma cloud due to the expanding velocity of the cloud (high β plasma), were
kept ﬁxed. These two pressures control the shock structure, and determine the
physical behavior of the system.

×

The simulations were performed on a computational cubic grid of 3003 cells,
10−3 λpi/cs and a cubic
12 solar wind particles per cell, a time step of t = 6.25
box size of 150 λpi in each dimension. A 2D parallel domain decomposition in
x and y was used. The normalizing quantities are λpi = 102 km for spatial
dimensions, cs = 53.6 km/s for the velocities, λpi/cs = 1.9 s for the time,
106 m−3 for the density. In the
5.49 nT for the magnetic ﬁeld and n0 = 5
x side to the +x side and
simulations, the solar wind is ﬂowing from the
the magnetic ﬁeld is perpendicular to this ﬂow, along the +z direction. As
the cloud expands, a magnetic ﬁeld compression zone is formed in the front
of the cloud. The solar wind ions are deﬂected around the cloud due to the
y direction piling up in the lower region of the
magnetic barrier, drift in the
cloud, and are accelerated in this process.

−

×

−

The magnetic ﬁeld assumes a particular importance to test the model as it
has a very clear signature, characteristic of the AMPTE experiments. In Fig. 3
the magnetic ﬁeld evolution is shown in the center plane of the simulation. As
the plasma bubble expands a diamagnetic cavity is formed due to the outward
ﬂow of ions that creates a diamagnetic current. A magnetic ﬁeld compression
zone is also evident - the kinetic pressure of the cloud ions drives the Inter-
planetary Magnetic Field outwards, creating the compressed magnetic ﬁeld.
These results reproduce 2D run results obtained in previous works [13,14], and
are in excellent agreement with the AMPTE experiments.

In Fig. 4, taken at the same time steps, it is visible that the solar wind ions

9

Fig. 3. Slice of the magnetic ﬁeld magnitude at z = 75 λpi,
for times a)
t = 6.25 λpi/cs, b) t = 12.5 λpi/cs, c) t = 18.75 λpi/cs and d) t = 25 λpi/cs. Iso–
lines of the magnetic ﬁeld magnitude are also shown.

Fig. 4. Slice of the solar wind density at z = 75 λpi, for times a) t = 6.25 λpi/cs, b)
t = 12.5 λpi/cs, c) t = 18.75 λpi/cs and d) t = 25 λpi/cs. A pile up zone is visible
along with a density depleted zone.

−

x
are being pushed out of the cloud area. The solar wind coming from the
direction is deﬂected around the magnetic ﬁeld pile up zone and drifts in the
y direction. This is due to the the electric ﬁeld generated inside the cloud,
−
dominated by the outﬂowing ions, that creates a counter clock-wise electric
ﬁeld. This electric ﬁeld, depicted in Fig. 5, is responsible for the solar wind ion
y direction. The same electric ﬁeld also aﬀects the cloud ions,
drift in the
that are pushed out in the +y side of the cloud, and pushed back in on the
other side. The ejection of ions in the +y side, known as the rocket eﬀect [23],
y direction due to
is one of the reasons of the reported bulk cloud drift in the
momentum conservation and is thoroughly examined along with other eﬀects
in [14].

−

−

10

One other interesting aspect is that as the simulation evolves, there are regions
of space in which the density drops, making this test problem a good choice
to test the low density stability problem resolution. It was found that density
dropped below the stability limit not in the center of the cloud, but behind
it, in the downwind area in the +x side of the cloud (Fig. 4). In the center of
the cloud although the solar wind is pushed out, high density is observed, due
to the presence of the cloud ions. It was also found that this was an example
of a low density due only to poor particle statistics, as it happened only when
8 particles per cell were used and was eliminated by increasing the number
of particles per cell to 12. The results, however, were very similar in the two
runs with 8 particles per cell versus the 12 particle per cell run, due to the
non-physical stabilization algorithm used.

Fig. 5. Slice of the electric ﬁeld at z = 75 λpi, for time t = 25 λpi/cs. Field vectors
show counter clockwise rotating electric ﬁeld.

The total ion ﬂuid velocity is shown in Fig. 6. The dark isosurface in the outer
part of the cloud represents ﬂuid velocities in the order of 2 cs. This is a clear
indication of an acceleration mechanism acting on these particles, which is due
to the same electric ﬁeld generated by the outﬂowing cloud ions.

Fig. 6. Ion ﬂuid velocity ﬁeld lines, iso-surfaces and projections. The darker iso-sur-
face represents ﬂuid velocities of

2 cs.

∼

The observations made at the time of the AMPTE releases match the results
from the simulations. A shock like structure, as in the simulations, was ob-
served and reported along with a large diamagnetic cavity, coincident with the

11

peak cloud density area [18,29]. Both the instability in the cloud expansion
~B direction, ob-
and the rocket eﬀect responsible for the cloud recoil in the ~v
served in the actual experiments and reported in several papers [13,14,26,27],
were also observed in dHybrid simulations.

×

y downwind side of the cloud, and
Other features like ion acceleration on the
y side of the cloud, which were unobserved
the charge density pile up on the
in previous simulations, were captured in our simulations due to the use of
much higher resolutions. These eﬀects are due to the cloud expansion that
creates an electric ﬁeld capable of deﬂecting the solar wind around the cloud
and accelerate the solar wind particles.

−

−

5 Conclusions

In this paper we have presented the code dHybrid, a three dimensional mas-
sively parallel numerical implementation of the hybrid model. The stability of
the algorithm has been discussed and a stabilization criterion with no impact
on parallel scalability has been proposed and tested. The AMPTE release ex-
periments were modeled, and the main physical features were recovered with
dHybrid.

Zero densities on hybrid and MHD models are a source of instabilities. In the
hybrid case these are usually numerical artifacts due to poor particle statistics.
Numerical stability analysis of dHybrid has been carried out and a constraint
both on the time step and on the density minimum has been found. This
constraint helps to eﬀectively determine at run time where instabilities will
occur and suppress them.

The parallel scalability of the algorithm has been studied yielding a 94% par-
allel eﬃciency for scaled problem sizes for typical runs with 32 processes,
showing excellent scalability, an indication that the parallelization method is
eﬃcient in tackling larger problems. The zero density stabilized algorithm does
not suﬀer from parallel performance degradation, thus avoiding the pitfalls of
other solvers that require inter-process communication steps.

dHybrid has been tested through comparison both with previous two dimen-
sional codes and the experimental results from the AMPTE release experi-
ments. The key features of the AMPTE release experiments are recovered by
dHybrid.

Parallel dHybrid allows the full scale study of the solar wind interaction with
unmagnetized objects. Similar problems, such as planetary exosphere erosion
in Mars and in Venus can be tackled, and will be the topic of future papers.

12

Acknowledgements

The authors wish to acknowledge Dr. J. N. Leboeuf for providing the code
dComet which led to the development of dHybrid. This work is partially sup-
ported by FCT (Portugal), the European Research Training Network on Tur-
bulent Layers under European Commission contract HPRN-CT-2001-00314
and CCLRC Center for Fundamental Physics (UK).

References

[1] Z. Dobe, K. B. Quest, V. D. Shapiro, K. Szego, J. D. Huba, Interaction of the
Solar Wind with Unmagnetized Planets, Phys. Rev. Letters 83 (2) (1999) 260

[2] R. Z. Sagdeev, V. D. Shapiro, V. I. Shevchenko, A. Zacharov, P. Kir´aly, K.
Szeg´o, A. F. Nagy, R. J. L. Grard, Wave activity in the neighborhood of the
bowshock of mars, Geophys. Res. Letters 17 (6) (1990) 893

[3] J. M. Dawson, Particle simulation of plasmas, Rev. Mod. Phys. 55 (1983) 403

[4] C. K. Birdsall, A. B. Langdon, Plasma Physics Via Computer Simulation,

Institute of Physics Publishing Bristol and Philadelphia (1998)

[5] R. W. Hockney, J. W. Eastwood, Computer Simulation Using Particles,

Institute of Physics Publishing Bristol and Philadelphia (1994)

[6] A. S. Lipatov, The Hybrid Multiscale Simulation Technology, Springer, (2002)

[7] S. H. Brecht, V. A. Thomas, Multidimensional simulations using hybrid

particles codes, Comp. Phys. Commun. 48 (1) (1988) 135

[8] F. Kazeminezhad, J. M. Dawson, J. N. Leboeuf, R. Sydora and D. Holland,
A Vlasov particle Ion Zero Mass Electron Model for Plasma Simulations, J.
Comp. Phys. 102 (2) (1992) 277

[9] J. N. Leboeuf, T. Tajima, J. M. Dawson, A magnetohydrodynamic particle code

for ﬂuid simulation of plasmas, J. Comp. Phys. 31 (1979) 379

[10] F. Kazeminezhad, J. N. Leboeuf, F. Brunel, J. M. Dawson, A Discrete Model

for MHD Incorporating the Hall Term, J. Comp. Phys. 104 (1993) 398

[11] R. A. Fonseca, L. O. Silva, F. S. Tsung, V. K. Decyk, W. Lu, C. Ren, W. B.
Mori, S. deng, S. Lee, T. katsouleas, J. C. Adam, OSIRIS: A Three-Dimensional,
Fully Relativistic Particle in Cell code for Modeling Plasma Based Accelerators,
Lecture Notes on Computer Science 2331, 342, Springer-Verlag, Heidelberg
(2002)

[12] N. Omidi, D. Winske, Steepening of kinetic magnetosonic waves into shocklets
- Simulations and consequences for planetary shocks and comets, J. Geophys.
Res. 95 (1990) 2281

13

[13] R. Bingham, D. A. Bryant, D. S. Hall, J. M. Dawson, F. Kazeminezhad, J. J.
Su, C. M. C. Nairn, AMPTE observations and simulation results, Comp. Phys.
Commun. 49 (1988) 257

[14] F. Kazeminezhad, J. M. Dawson, R. Bingham, Simulations and qualitative
analysis of the AMPTE experiments, J. Geophys. Res. 98 (A6) (1993) 9493

[15] J. P. Boris, Relativistic plasma simulation-optimization of a hybrid code, Proc.

Fourth Conf. Num. Sim. Plasmas, Naval Res. Lab, 3-67, 1970

[16] V. K. Decyk, S. R. Karmesin, A. de Boer, P. C. Liewer, Optimization of particle-
in-cell codes on reduced instruction set computer processors, Comp. in Phys.,
10 (3) 1996

[17] K. S. Yee, Numerical Solution of Initial Boundary Value Problems Involving
Maxwells Equations in Isotropic Media, IEEE Trans. Ant. Prop. 14 (3) (1966)
302

[18] F. Kazeminezhad, Hybrid Modeling of Plasmas and Applications to Fusion and

Space Physics, UCLA PhD thesis (1989)

[19] D. W. Hewett, A global method of solving the electron-ﬁeld equations in a
zero-inertia-electron-hybrid plasma simulation code, J. Comp. Phys. 38 (1980)
378

[20] T. D. Arber, Hybrid Simulation of the Nonlinear Evolution of a Collisionless,

Large Larmor Radius Z Pinch, Phys. Rev. Lett. 77 (1996) 1766

[21] R. A. Fonseca, Experimental and Numerical Study of Laser-Plasma Electron

Accelerators, IST PhD thesis (2002)

[22] A. Valenzuela, G. Haerendel, H. Foeppl, F. Melzner, H. Neuss, The AMPTE

artiﬁcial comet experiments, Nature, 320, 700 (1986)

[23] G. Haerendel, G. Paschmann, W. Baumjohann, C. W. Calrson, Dynamics of

theAMPTE artiﬁcial comet, Nature, 320, (6064), 720 (1986)

[24] S. C. Chapman, S. J. Schwartz, One-dimensional hybrid simulations of
boundary layer processes in the AMPTE solar wind lithium releases, J.
Geophys. Res. 92 (A10) (1987) 11059

[25] J. B. Harold, A. B. Hassam, A simulation of the december 1984 solar wind

AMPTE release, Geophys. Res. Letters 18 (2) (1991) 135

[26] P. A. Delamere, D. W. Swift, H. C. Stenbaek-Nielsen, A three-dimensional
hybrid code simulation of the December 1984 solar wind AMPTE release,
Geophys. Res. Letters 26 (18) (1999) 2837

[27] P. A. Bernhardt, R. A. Roussel-Dupre, M. B. Pongratz, G. Haerendel, A.
Valenzuela, D. A. Gurnett and R. R. Anderson, Observations and Theory of
the AMPTE Magnetotail Barium Releases, J. Geophys. Res. 92 (A6) (1987)
5777

14

[28] A. B. Hassam and J. D. Huba, Structuring of the AMPTE magnetotail barium

releases, Geophys. Res. Letters 14 (1987) 60

[29] R. Bingham, V. D. Shapiro, V. N. Tsytovich, U. de Angelis, M. Gilman and
V. I. Shevchenko,Theory of wave activity occurring in the AMPTE artiﬁcial
comet, Phys. Fluids B 3 (1991) 1728

[30] R. Bingham, F. Kazeminezhad, R. Bollens, J. M. Dawson, Simulation of Ampte
Releases: A Controlled Global Active Experiment, ISPP-7 ”Piero Caldirola”,
Bologna, E. Sindoni and A. Y. Wong (Eds.) (1991)

[31] R. J. Bollens, Computer Modeling of Active Experiments in Space Plasmas,

UCLA PhD thesis (1993)

[32] J. G. Luhmann, C. T. Russell, J. L. Phillips, A. Barnes, On the role of the
quasi-parallel bow shock in ion pickup - A lesson from Venus?, J. Geophys.
Res. 92 (1987) 2544

[33] K. Sauer, E. Dubinin, M. Dunlop, K. Baumgartel, V. Tarasov, Low-frequency
electromagnetic waves near and below the proton cyclotron frequency at the
AMPTE Ba release: Relevance to comets and Mars, J. Geophys. Res. 104 (A4),
6763

15


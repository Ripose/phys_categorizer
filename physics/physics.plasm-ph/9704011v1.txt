7
9
9
1
 
r
p
A
 
0
1
 
 
]
h
p
-
m
s
a
l
p
.
s
c
i
s
y
h
p
[
 
 
1
v
1
1
0
4
0
7
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Simulating Plasma Turbulence in Tokamaks

Jeremy Kepner
Scott Parker
Viktor Decyk

The development of magnetic fusion energy is a continuing national pri-
ority. Present day toroidally conﬁned plasma experiments (tokamaks) are
producing fusion power comparable to the input power needed to heat the
plasma (Pf usion ≈ 0.3Pin). One of the basic physics problems is to under-
stand turbulent transport phenomena, which cause particles and energy to
escape the conﬁning magnetic ﬁelds. Plasma turbulence is challenging from a
theoretical point of view because of the nonlinearity and high dimensionality
of the governing equations. Experimental measurements in the core region of
a tokamak are limited by the extremely high temperatures, O(108) Kelvin,
of the conﬁned plasma. The high levels of both theoretical and experimental
diﬃculty highlight the potentially important role of numerical simulations in
developing a predictive model for turbulent transport. Such a model would
dramatically reduce uncertainties in tokamak design and could lead to en-
hanced operating regimes, both of which would reduce the size and expense
of a fusion reactor.

An understanding of turbulent transport and the exploration of modes of
operation that suppress turbulence are central goals of the numerical toka-
mak project, one of the Grand Challenges of the national HPCC program.
The process of modeling entails three main steps: (1) the development of
a simpliﬁed model of tokamaks that encompasses the essential physics of
the relevant instabilities, (2) the creation of numerical algorithms for solving
the governing equations, and (3) implementation of these methods on mas-
sively parallel architectures. This third step is necessary if we are to achieve
simulations of suﬃcient size and resolution to explain the trends seen in

1

experiments.

The state of the plasma is given by the distribution function f (x, v, t),
whose evolution is described by the gyrokinetic equations—a reduced set of
equations derived from the Vlasov equation by phase averaging over the ion
gyromotion while keeping only the relevant temporal and spatial scales [3, 6].
This averaging of the fast gyromotion reduces the dimensionality of the gov-
erning equations from six to ﬁve. In addition, recently developed numerical
methods make it possible to follow only the perturbations of the distribution
function δf from a stationary equilibrium (see [10] and references therein).
Even with the considerable complexity of the gyrokinetic equations and the
so-called “δf method,” the algorithms are analogous to those used in con-
ventional particle-in-cell (PIC) simulations. PIC codes are both memory-
and CPU-intensive, and the eﬀective use of high performance computing, in
particular massively parallel architectures, which require a domain decom-
position of the problem, is essential for the success of the numerical tokamak
project. Fortunately, this problem lends itself naturally to a one-dimensional
decomposition.

As an example of a few partially realized goals of the numerical toka-
mak project, we present here some recent results for the simulation of modes
that may act as barriers to turbulent transport. Our three-dimensional gy-
rokinetic simulation code is being used to study two eﬀects that are linearly
stabilizing and that may cause the formation of transport barriers: reversed
magnetic shear and peaked density proﬁles [11]. We have found that weak
or negative magnetic shear, in combination with a peaked density proﬁle rel-
ative to the temperature proﬁle, greatly suppresses turbulence in the central
region of the simulations. Similar features have been seen experimentally [7].

Tokamak Geometry

The essential geometry of a tokamak is that of a torus deﬁned by a major
radius R and a minor radius a (see Figure 1). The ions within the plasma
move rapidly around the torus, gyrating tightly along the magnetic ﬁeld lines,
like rings on a wire. The radius of the gyration is ρ = vt/Ω, where vt is the
transverse velocity and Ω = eB/mc is the gyro frequency, with e being the
charge, B the magnetic ﬁeld strength, m the particle mass, and c the speed
of light. The essential scale (resolution) of the system (simulation) is set by
a/ρ. Typically R ≈ 270 cm, a ≈ 85 cm, and ρ ≈ 0.15 cm.

2

R

a

Figure 1: Schematic drawing of a tokamak with major radius R and minor
radius a showing the path of a gyrating particle (black line) along a magnetic
ﬁeld line (gray line) with a gyro radius ρ.

The simplest arrangement of ﬁeld lines is obtained by wrapping current-
carrying wires tightly around the minor axis of the torus, creating straight
magnetic ﬁeld lines aligned with the torus. Unfortunately, the magnetic
ﬁeld exerts a greater force on the inside of the torus, which causes the ions
in the plasma to drift across the ﬁeld lines. This problem can be partially
alleviated by twisting the ﬁeld lines into a helical shape in such a way that the
drifts approximately cancel. A byproduct of the twisting is that the particle
trajectories become far more complex and are increasingly susceptible to a
wide range of instabilities, which tend to grow along the toroidal modes of the
tokamak. Figure 2 shows a simpliﬁed example of a typical toroidal mode—an
m = 3, n = 2 mode (m is the poloidal mode number, and n is the toroidal
mode number). This could be, for example, the linear growth phase of an ion
density perturbation. The linear phase of an instability in realistic plasmas
typically has much larger values for m and n (10–100), but a similar helical
twist (the ratio m/n). In addition, the realistic case has a sheared magnetic
geometry (i.e., a radially varying amount of helical twist in the magnetic ﬁeld
lines). Generally, the isosurfaces of a perturbation follow the helical shape
of the magnetic ﬁeld lines.

3

r
Figure 2: Example of an m = 3, n = 2 mode in a torus with no shear in
the helical magnetic ﬁeld lines; m is the poloidal mode number, and n is the
toroidal mode number. The isosurface shows the geometry of a typical linear
growth mode within the plasma; it also traces the magnetic ﬁeld lines.

Governing Equations

The governing gyrokinetic equations are phase space conserving and have the
same form as the Vlasov equation:

∂f
∂t

+ ˙z · ∂f
∂z

= 0

where z = (R, vk, µ), with R being the guiding center position of the gyrating
particle, vk the particle’s velocity along the magnetic ﬁeld line, and µ a
constant of motion that parameterizes the particle’s velocity perpendicular
to the magnetic ﬁeld.

The electrostatic toroidal gyrokinetic equations used for all the results
discussed here are those derived by Hahm [4]. Because the variations in f
are quite small, f = f0(z)+δf(z, t)(≈ 1%) is used to carry out a perturbative
expansion. δf in the expansion is solved by integrating the characteristics
of the resulting gyrokinetic equations. More details can be found in [9] and
in references therein. It is important to realize that although the solution of
these equations is algorithmically analogous to the solution of F = ma plus
Maxwell’s equations, much theoretical care and eﬀort has been devoted to
simplifying the problem while retaining the important physics. Simpliﬁca-

4

tions include reduction of the dimensionality, elimination of short space–time
scales, and reduction of particle simulation noise.

Numerical Method and Parallel Implementation

Particle-in-cell simulation has been used in the plasma physics community
for several decades. The general idea is that the particles interact with both
self-created and externally imposed electromagnetic ﬁelds. The code thus
has two distinct parts and data structures. For illustration purposes, we
consider the conventional electrostatic equations, which retain the essence of
the algorithm [2]. The trajectories of particles with mass m and charge q are
given by

dvi(t)/dt = (q/m)E(xi(t)),
dxi(t)/dt = vi(t),

where the subscript i refers to the i-th particle. The algorithm generally used
to solve this set of equations is a time-centered leapfrog scheme,

vi(t + dt/2) = vi(t − dt/2) + (qi/mi)E(xi(t)),

xi(t + dt) = xi(t) + vi(t + dt/2),

where the electric ﬁeld at the particle’s position E(xi) is found by interpola-
tion from electric ﬁelds previously calculated on a grid. The interpolation, a
gather operation that involves indirect addressing, accounts for a substantial
part of the computation time.

In the second part of the code, the ﬁelds created by the particles must be

found from Poisson’s equation,

∇2φ = −4π X qn(x)

with E = −∇φ and the sum being over each type of particle species. Fourier
transform methods are used to solve this equation on a grid. Typically, the
time for the ﬁeld solver is not large. The source term n(x) is calculated from
the particle position by an inverse interpolation,

5

n(x) = X S(x − xi),

where S is a particle shape function. This is a scatter operation that also in-
volves indirect addressing and consumes a substantial part of the calculation
time.

Since the dominant part of the calculation in a particle code involves
interpolation between particles and grids, it is important for a parallel im-
plementation that these two data structures reside on the same processor.
Diﬀerent processors are assigned diﬀerent regions of space, and particles are
assigned to processors according to their locations [8]. As particles move
from one region to another, they are moved to the processor associated with
the new region. Because particles must also access neighboring regions of
space during the interpolation, extra guard cells are kept in each processor,
to be combined or replicated as needed after the particles are processed.
The passing of particles from one processor to another is performed by a
particle-manager subroutine. The passing of ﬁeld data between guard cells
is performed by a ﬁeld-manager subroutine.

The ﬁeld solver uses Fourier transform methods. There are two FFTs
per timestep, one for the charge density and one for the electric potential. A
parallel complex-to-complex FFT was developed with a transform method in
which the coordinate local to the processor is transformed ﬁrst, the data are
then transposed so that the coordinate that was distributed becomes local,
and, ﬁnally, the remaining coordinate is transposed. The maximum number
of processors that can be used is limited by the maximum number of grid
points in any one coordinate, but this is not a severe constraint at present
since the numerical tokamak is designed for systems in which this number is
about 512.

During the transpose phase of the FFT, each processor sends one piece of
data to every other processor. This can be accomplished in a number of ways,
but the safest is always to have one message sent, one message received, and
so on. Flooding the computer with large numbers of simultaneous messages
tends to overﬂow system resources and is not always reliable.

The structure of the main loop of the simpliﬁed code is summarized as

follows:

1. Particle coordinates are updated by an acceleration subroutine.

6

2. Particles are moved to neighboring processors by a particle-manager

3. Particle charge is deposited on the grid by a deposit subroutine.
4. Guard cells are combined by a ﬁeld-manager subroutine.
5. Complex-to-complex FFT of charge density is performed by an FFT

6. Electric potential is found in Fourier space by a Poisson solver subrou-

7. Complex-to-complex FFT of electric potential is performed by an FFT

8. Electric potential is distributed to guard cells by a ﬁeld-manager sub-

subroutine.

subroutine.

tine.

subroutine.

routine.

This structure has the beneﬁcial feature that the physics modules, items
1, 3, and 6, contain no communication calls (except for a single call to sum
energies across processors). These modules can easily be modiﬁed by re-
searchers who do not have special knowledge of parallel computing or mes-
sage passing. The communications modules, items 2, 4, and 8, handle data
management but do not perform any calculation, and can be used by physi-
cists as black boxes, where only the input and output must be understood.
The FFT, items 5 and 7, are the usual sequential FFTs, with an additional
embedded transpose subroutine, which can also be used as a black box. Fur-
thermore, since most message-passing libraries are quite similar, moving from
one distributed-memory computer to another simply involves replacing the
message-passing calls in the communications modules with new ones.

General Behavior

When a simulation starts with a very small initial perturbation, the run
generally has two identiﬁable phases. The ﬁrst is the linear phase, where
modes (i.e., standing waves) grow exponentially. Lower-dimensional, time-
independent eigenvalue techniques that are fairly well understood theoreti-
cally can also be used to ﬁnd linear modes. The second phase is the turbulent
stationary state, during which the growth of the dominant linear modes satu-
rates and the system settles down to a statistical steady state. The transition
from linear to turbulent behavior is demonstrated in Figure 3.

7

Parallel Performance

In our implementation, we use a one-dimensional domain decomposition
along the toroidal axis, which is generally more eﬃcient and signiﬁcantly
easier to program than a full three-dimensional decomposition. In addition,
the number of particles per processor remains relatively constant along this
axis, which minimizes load imbalance. Furthermore, since the mean ﬂow
of the particles has been subtracted oﬀ in the perturbation expansion, the
particles themselves do not move rapidly across cells, which keeps the com-
munication low. For these reasons, we would expect to get excellent parallel
performance.

Figure 3: Time evolution of ion density in a simulation of tokamak plasma
showing transition to a turbulent state. Time sequences advance from left to
right, top row to bottom row.

In the most recent eﬀorts, Fortran 77 and the PVM message-passing
library have been used to port the code to the Cray T3D [1]. Production
runs on the T3D show a performance of approximately 14.4 MFlops per
processor (10% of the theoretical peak of 150 MFlops, which is typical for
applications of this type). To test the scalability of the code, 12 runs were
timed with diﬀerent numbers of processors and problem sizes. These times,
along with the parameters of the runs, are shown in Table 1.

To ﬁrst order, the problem size is given by the total number of parti-

8

particles
/grid cell processors

No. of

No. of
particles
213
214
215
216
217
218
219
220
221
222
223
224

grid
size
163
163
163
323
323
323
643
643
643
1283
1283
1283

2
4
8
2
4
8
2
4
8
2
4
8

wallclock speed (msec-processor
sec/step
0.12
0.19
0.34
0.44
0.78
1.41
1.91
3.37
6.35
9.16
17.0
32.9

/particles-step)
0.23
0.19
0.16
0.21
0.19
0.17
0.23
0.20
0.19
0.27
0.25
0.25

16
16
16
32
32
32
64
64
64
128
128
128

Table 1: T3D timings for diﬀerent numbers of processors and problem sizes.

cles. The total amount of computer resources consumed is the time per step
multiplied by the number of processors. These two quantities are plotted in
Figure 4. In a perfectly scalable code, which is represented by the straight
line in Figure 4, the resource consumption should be proportional to the
problem size. What is most impressive is that the full code, with all di-
agnostics and outputs, was used to obtain these results. Simply put, this
means that doubling the number of CPUs or halving the size of the problem
will halve the computation time. In addition, it is worth mentioning that
the code has a very low communication/computation ratio (< 0.01). This
ratio is a rough measure of the fraction of time spent waiting for processors
to transmit information, and the low value for our program is an indication
that it can be expected to perform well if even more processors or faster
processors are used. This is encouraging because the Cray T3E will have
processors that are signiﬁcantly faster.

9

10000

1000

100

10

p
e
t
s
 
/
 
s
d
n
o
c
e
s
 
e
d
o
n

1
103

104

105

106

107

108

# of particles

Figure 4: Scaling of the code with problem size and number of processors.
The straight line indicates the expected time from a simple scaling up of the
smallest simulation.

Simulating Transport Barriers

Transport barriers in tokamaks have been an important aspect in a host of
operational modes, such as the H-mode, where edge transport is thought
to be greatly reduced through poloidal shear ﬂow. In the recent enhanced
reverse shear experiments on TFTR [7], it has been reported that density
and ion heat transport are below conventional neoclassical levels in the core.
Comprehensive linear calculations [5, 12] show this region to be locally stable
to micro-instabilities. New gyrokinetic simulations presented here show that
the combined eﬀects of reversed magnetic shear and a peaked density proﬁle
allow for a good conﬁnement zone in the core region [11]. Figure 5 shows the
diﬀerence in energy ﬂux for simulations with and without reversed magnetic
shear and a peaked density proﬁle. These new results are diﬀer with past
simulations (that did not include these eﬀects), which have generally shown
a global (slow) relaxation of the temperature proﬁle.

10

y
t
i
s
n
e
D

r
a
e
h
S
 
c
i
t
e
n
g
a
M

 

x
u
l
F
y
g
r
e
n
E

0
3

 

3

2

1

2

1

0

2

1

0

0

-1
3

 

 

 

 

 

 

 

 

 

 

 

20

40

60

80

100

Radius

Figure 5: Density (top), magnetic shear (middle), and energy ﬂux (bottom)
proﬁles for the case with (solid line) and without (dotted line) peaked density
and reversed magnetic shear proﬁles. A large decrease in the energy ﬂux can
be seen in the reversed shear case.

Future Work

Future directions for this work include the incorporation of more detailed
physics, such as an electron model that includes a trapped component and
studies of the eﬀects of magnetic perturbations. Ultimately, the goal is to
understand plasma turbulence at a level that is detailed enough to allow
quantitative predictions of heat transport. This will reduce uncertainties
in design, and hence the cost, of future tokamak reactors. As shown here,
progress toward this long-term goal can be made through close interaction
between theory and direct numerical simulation.

11

Acknowledgments

The authors especially thank J. Cummings, W.W. Lee, H. Mynick, R. Sam-
taney, E. Valeo, and N. Zabusky. Much of this work was carried out at the
Princeton Plasma Physics Laboratory as an active part of the community-
wide Numerical Tokamak Project supported through the HPCC Initiative.
Computing resources were provided by JPL at Caltech, ACL at LANL,
NERSC at LLNL, and the Pittsburgh Supercomputing Center. This work
is supported by the U.S. Department of Energy, through Contract DE-
AC02-76CHO-3073, Grant DE-FG02-93ER25179.A000, and the Computa-
tional Science Graduate Fellowship Program.

References

[1] V.K. Decyk, How to write (nearly) portable Fortran programs for par-

allel computers, Computers in Physics, 7(1993), pp. 418–424.

[2] V.K. Decyk, Skeleton PIC codes for parallel computers, Computer

Physics Communications, 87(1995), pp. 87–94.

[3] E.A. Frieman and L. Chen, Nonlinear gyrokinetic equations for low-
frequency electromagnetic waves in general plasma equilibrium, Physics
of Fluids, 25(1982), pp. 502–507.

[4] T.S. Hahm, Nonlinear gyrokinetic equations for tokamak microturbu-

lence, Physics of Fluids, 31(1988), pp. 2670–2673.

[5] C. Kessel, J. Manickam, G. Rewoldt, and W.M. Tang, Im-
proved plasma performance in tokamaks with negative magnetic shear,
Physical Review Letters, 72(1994), pp. 1212–1215.

[6] W.W. Lee, Gyrokinetic particle simulation model, Journal of Compu-

tational Physics, 72(1987), pp. 243–269.

[7] F.M. Levinton, et.al., Improved conﬁnement with reversed magnetic
shear in TFTR, Physical Review Letters, 75(1995), pp. 4417–4420.

[8] P.C. Liewer, V.K. Decyk, and A. de Boer, A general concur-
rent algorithm for plasma particle-in-cell simulation codes, Journal of
Computational Physics, 85(1989), pp. 302–322.

12

[9] S.E. Parker, W.W. Lee and R.A. Santoro, Gyrokinetic simula-
tion of ion temperature gradient driven turbulence in 3D toroidal geom-
etry, Physical Review Letters, 71(1993), pp. 2042–2045.

[10] S.E. Parker and W.W. Lee, A fully nonlinear characteristic method
for gyrokinetic simulation, Physics of Fluids B, 5(1993), pp. 77–86.

[11] S.E. Parker, H.E. Mynick, M. Artun, J.C. Cummings, V. De-
cyk, J.V. Kepner, W.W. Lee, and W.M. Tang, Radially global
gyrokinetic simulation studies of transport barriers, Physics of Plasmas,
3(1996), pp. 1959–1966.

[12] G.W. Rewoldt and W.M. Tang, private communication, 1995.

Jeremy Kepner (jvkepner@astro.princeton.edu) is a graduate student
in the Department of Astrophysics at Princeton University. Scott Parker
(sparker@buteo.colorado.edu) is a professor in the Department of Physics
at the University of Colorado in Boulder. Viktor Decyk (vdecyk@pepper.physics.ucla.edu)
is a research scientist in the Department of Physics at the University of Cal-
ifornia in Los Angeles.

13


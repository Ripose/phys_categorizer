A complexity measure for symbolic sequences and applications to

DNA

Ana P. Majtey,1 Ram´on Rom´an-Rold´an,2 and Pedro W. Lamberti1

1Facultad de Matem´atica, Astronom´ıa y F´ısica

Universidad Nacional de C´ordoba

Ciudad Universitaria, 5000 C´ordoba, Argentina

CONICET
2Departamento de F´isica Aplicada, Universidad de Granada, Granada, Spain

(Dated: February 2, 2008)

Abstract

We introduce a complexity measure for symbolic sequences. Starting from a segmentation pro-

cedure of the sequence, we deﬁne its complexity as the entropy of the distribution of lengths of

the domains of relatively uniform composition in which the sequence is decomposed. We show

that this quantity veriﬁes the properties usually required for a “good” complexity measure.

In

particular it satisﬁes the one hump property, is super-additive and has the important property of

being dependent of the level of detail in which the sequence is analyzed. Finally we apply it to the

evaluation of the complexity proﬁle of some genetic sequences.

PACS numbers: 05.20.-y, 64.60.Cn, 05.45.+b

Key words: Complexity, segmentation, DNA sequences.

6
0
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
s
s
a
l
c
.
s
c
i
s
y
h
p
[
 
 
1
v
3
1
1
6
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

I.

INTRODUCTION

In the last few years the term complexity has become frequent in scientiﬁc literature [1, 2, 3].

This has conveyed the introduction of diverse complexity measures in diﬀerent areas of

science. Kolgomorov’s algorythmic complexity [4], Lempel & Ziv’s measure [5], Bennet’s

thermodynamic depth [1],[6], physical complexity [7] or Lopez-Ruiz, Mancini & Calvet’s

complexity measure [8], are some of the examples that have caught most attention. In fact,

this list does not reﬂect all the proposed complexity measures.

In spite of these eﬀorts, and reﬂecting such diversity, consensus is to be reached about a

precise deﬁnition of the complexity concept that would allow its quantiﬁcation. It is possible

that one of the main diﬃculties to reach that consensus is the lack of a language that is

common to all the diﬀerent areas of science in which the concept is meant to be introduced.

As an example, the notion of information and its quantiﬁer, the entropy, is usually present in

measures proposed to evaluate the complexity of a system or of a process. At the same time,

entropy, in physics is a measure of the disorder of the system, which grows as the disorder

grows. However, intuitively, a complex system may simultaneously involve order as well as

disorder. Two extreme cases are to be considered when, in physics, a complexity measure

is searched. Firstly, a perfect crystal (a completely ordered system) and on the other hand

the ideal gas (a completely disordered system). Clearly both systems have no complexity

(or an extremely low complexity). In general, a properly deﬁned complexity measure should

reach its maximum at some intermediate level between the order of the completely regular

and the disorder of the absolutely random. This desirable characteristic for all complexity

measures is known as the one hump property.

Very often, a complex system is described as one formed by many non-lineal elements

that interact with each other [9]. These interactions give the system the capacity to auto-

organize [10]. Given the fact that complexity comes from the interactions of the single units,

these interactions must be taken into account when deﬁning a measure that quantiﬁes the

complexity of a system. When the diﬀerent parts of a system, e.g., the molecules of an

ideal gas in equilibrium, do not interact, their behavior can be understood as the sum of its

separated components. But, when interdependencies occur, this is not valid anymore and

to quantify the complexity we need a measure that takes those bonds into consideration [3].

An adequate complexity measure should be super-additive, meaning that the two systems’

2

juxtaposition gives as a result a system in which complexity equals or exceeds the addition

of the considered systems. This means that the (extensive) complexity of the whole is equal

or larger than the sum of the (extensive) complexities of the parts. Here we are devoted to

investigate a complexity measure for symbolic sequences. In this case, the super-additive

property reads as follows: if CS1 and CS2 denote the complexities of two symbolic sequences

S1 and S2, with corresponding lengths L1 and L2, then

(L1 + L2) CS1S2 ≥ L1 CS1 + L2 CS2

(1)

where CS1S2 denotes the complexity of the juxtaposition of S1 and S2.

The complexity measure we introduce in the present work takes into account the lengths

of the segments of relatively uniform content in which a symbolic sequence is divided. To

establish the segmentation we must look for compositionally homogeneous segments. Then,

two extreme cases may occur after the segmentation process:

• all the resulting segments have the same length (periodic sequence),

• the sequence has not been segmented (random sequence).

These two cases correspond with the perfect crystal and the ideal gas mentioned earlier, and

as we will see, they have a null complexity, according to our deﬁnition. Now the next step

is to characterize what we will take as the most complex sequence, that is, we must ﬁx a

third point over the complexity plot. In order to do that, we go along the following line of

reasoning: when the probability, of measuring a particular value of a certain quantity, varies

inversely as a power of that value, it is said that the quantity follows a power law. The

importance of the distributions following a power law in physics and related areas has been

pointed out by the ubiquity of such laws in a wide range of phenomena. This type of laws

rules as much the frequency of the use of words in any human language as the number of

moon craters of a particular size [11]. In general it is accepted that a power law dependence

is an indication of hierarchical organization. More interestingly, this kind of behavior also

appears in brain dynamics studies.

In fact, it is known that the brain constantly makes

complex functional nets corresponding to the traﬃc between regions. In this case it is found

that the probability for k regions to be temporarily correlated with a given region satisﬁes a
rule k−µ where µ ≈ 2 [12]. To us, this example proves to be highly signiﬁcant because brain

3

dynamics is a milestone case of auto-organization and undoubtedly of what we can consider

as a complex system. At its time, auto-organization is seen as the modelling mechanism to

a great amount of systems in Nature.

According to these precedents, we consider reasonable to take as a high complexity se-

quence, one that has a lengths distribution of patches of relatively uniform composition

following a power law, i.e. the probability P (l) of ﬁnding a patch of relatively homogeneous

composition with length l, is given by:

P (l) ∼

1
lµ .

(2)

We suppose further that the most complex sequence is the one in which the interdepen-

dence between subsegments is maximum. To quantify that interdependence, we use the

autocorrelation function, C(l) [13]. Interdependence is maximum when the autocorrelation

function is ﬂat. There exists an interesting relationship between the exponent µ in (2), and

the behavior of the autocorrelation function [13]. In fact, for a length distribution law given

by (2) it has been shown that the standard deviation in the symbol content of the sequence,

F (l), has a behavior of the form

and the autocorrelation function follows a power law

F (l) ∼ lα

C(l) ∼

1
lγ

with γ = 2 − 2α. For an exponent µ ≤ 2 corresponds an exponent α = 1 and therefore

γ = 0, that is, a ﬂat autocorrelation function [13]. Thus, for extremely long sequences a

ﬂat autocorrelation is associated to a segments lengths distribution that complies with a

power law in which µ ≤ 2. It should be emphasized that every exponent µ ≤ 2 leads to a

ﬂat autocorrelation function. However the exponent µ = 1 corresponds to a statistically self

similar distribution of patches along the sequence [14]. These facts suggest us to take as the

most complex sequence the one with a lengths distribution of patches of relatively uniform

composition is given by the law (2) with µ = 1.

This work is organized as follows: In Section II we describe the sequence segmentation

method implemented; in Section III we introduce a complexity measure and study its basic

properties; in Section IV we apply the introduced measure to real genomic sequences; ﬁnally

we present some conclusions.

4

II. SEGMENTATION METHOD

In this section we describe the segmentation algorithm applied to the study of the sequence

structure. The method is based on the Jensen-Shannon entropic divergence (JSD) and it

was successfully applied to the study of DNA sequences [15]. DNA sequences are formed by

patches or domains of diﬀerent nucleotide composition; given the huge spatial heterogeneity

of most genomes, the identiﬁcation of compositional patches or domains in a sequence is a

critical step in understanding large-scale genome structure [16].

The JSD is a measure of distance between probability distributions. Although it was

initially deﬁned as a distance between two probability distributions, Lin has proposed a
generalization to several probability distributions [17]. Let P (k) = {p(k)
a set of M probability distributions (Pi p(k)
N possible values Xi; p(k)
to the distribution P (k). The JSD for these probabilities distributions is deﬁned by:

denotes the probability of occurrence of the value Xi according

i = 1, k = 1..M), for a discrete variable X with

, i = 1..N}, k = 1..M,

i

i

JS[P (1), .., P (M )] = H[X

π(k)P (k)] −

π(k)H[P (k)]

(3)

M

X
k

k

is the Shannon’s entropy and the numbers π(k), k =

where H[P ] = − Pj pj log2 pj
1..M, Pk πk = 1 are weights properly chosen.

The JSD is non negative, bounded and can be interpreted in the frame of information

theory [22].

Incidentally we mention that the JSD has been proposed as a complexity

measure for genomic sequences [16].

In the context of symbolic sequences analysis, the probabilities pi are approximated by

the frequency of occurrence of each symbol throughout the sequence. For a DNA sequence,

the symbols are the nucleotides {A; C; T ; G}.

If we want to compare the compositional

content of two symbolic sequences, let us say S1 and S2, of lengths L1 and L2, we can
use the expression (3), where the weights are taken equal to π(k) = Lk/L, k = 1, 2, with

L = L1 + L2. In this case the probability distributions P (1) and P (2) are approximated by

the frequency of occurrence of the diﬀerent symbols throughout each sequence.

The segmentation procedure allows to decompose the sequence into domains or subse-

quences with a diﬀerent base composition in comparison to the two adjacent subsequences,

at a given level of statistical signiﬁcance or threshold, Du. This threshold is associated with

the level of details in which the sequence is analyzed [22].

5

In order to make this paper self-contained we will describe the basic steps in the seg-

mentation procedure. For a more detailed description we refer the reader to reference [15].

Let us suppose that we deﬁne a moving cursor along the complete sequence. For each posi-

tion of the cursor, it results two subsequences, one to the left and other to the right of the

cursor. For each subsequence we can evaluate the occurrence frequency of each symbol and

then calculate the JSD for each position of the cursor. The position that corresponds to a

maximum of the JSD above the threshold elected, Du, is taken as a cut point. Clearly these

points corresponds to the maximum of the discrepancy between the compositional content

of each subsequences. The procedure is repeated for each resulting subsequence until the

JSD be greater than the threshold value.

When segmenting symbolic sequences with simple domain structures, homogeneous do-

mains can be consistently found (if purely random ﬂuctuations are excluded). However,

when the method is applied to long-range correlated sequences, such homogeneity vanishes:

by relaxing the threshold value, we ﬁnd new domains within other domains, previously taken

as homogeneous under a higher threshold value. This domains-within-domains phenomenon

points to complex compositional heterogeneity in DNA sequences, which is consistent with

the hierarchical nature of biological complexity [16]. We will back to this point at the end

of the present work.

III. DEFINITION OF THE COMPLEXITY

Let us consider a symbolic sequence S of length L (i.e., L is the number of symbols in the

sequence). Let us assume that by segmenting the sequence according to procedure described

in the preceding section, we can decompose the sequence in Ns patches or domains of diﬀerent

compositional content (up to a signiﬁcance level Du) [22]. Let us denote by li, i = 1...Ns,

the lengths of each one of these segments. Obviously

In general these lengths are not all diﬀerent. Let us denote by Ω the subset of lengths li

such that li 6= lj if i 6= j:

Ω = {(lα1, ..., lακ), lαi 6= lαj if i 6= j, κ ≤ Ns}

(4)

Ns

X
i=1

li = L

6

Let Nαi be the number of segments of length lαi. Then P
now an arbitrary partition A = {Aj}ν

κ
i=1 Nαi = Ns. Let us consider
j=1, of the interval [1, L] with ν − 1 (the number of

subintervals), in principle, arbitrary:

1 = A1 < A2 < ... < Aν−1 < Aν = L

(5)

We name the quantity ∆j = Aj − Aj−1 j = 2, ..., ν as the amplitude of the corresponding

Let us denote by ˜Nj the number of patches in the segmented sequence with length belong-
ν
˜Nj = Ns is satisﬁed. Finally let us denote
j=2

ing to the interval [Aj−1, Aj). The condition P
by fj the occurrence frequency of segments whose length belongs to the interval [Aj−1, Aj)

(with the convention that the interval corresponding to j = ν includes the extreme value

subinterval.

L):

From the knowledge of the frequencies F = {fj} we can evaluate the Shannon’s entropy

fj =

fj = 1

˜Nj
Ns

;

ν

X
j=2

HS(F ; A, Du) ≡ H[F ] = −

fj log2 fj

ν

X
j=2

(6)

(7)

Clearly this quantity depends on the partition A, and on the signiﬁcance level Du at what

the segmentation was done, that is, it depends on the level of detail at what the sequence

was analyzed. Therefore we have included explicitly the partition A and the signiﬁcance

There are two cases in which the entropy (7) does not depend on the particular partition

value Du as arguments in HS.

chosen:

1. a idealized periodic sequence and

2. a idealized random sequence.

Here what is meant by idealized is that the respective character is detected to every signiﬁ-

cant level of detail of the analysis. In the ﬁrst case, there exists only one value (the period)

for the length of the segments. Therefore fJ = 1 for some value 2 ≤ J ≤ ν and fj = 0 for

all other j. Thus, for a periodic sequence HS = 0 for any partition of the interval [1, L].

Analogously, due to the fact that a random sequence is not segmented at any signiﬁcant

7

level of detail (by the proper meaning of signiﬁcant), only one of the fj is diﬀerent of zero:

fν = 1. Thus we also have HS = 0 is this case. These two extreme cases are the correspond-

ing ones with the crystal and the isolated ideal gas, in the physical context. In that sense,

HS(F ; A, Du) is a good candidate as a complexity measure. It should be emphasized that

HS has information about the segmentation of the sequence. The fact that HS vanishes for

a periodic and a random sequence, suggests to investigate it as a measure of complexity.

However, it should be also indicated that, in order to be a true characteristic of the sequence

under study, a complexity measure must be independent of any arbitrary parameter. For it,

a particular partition is adopted by reﬁning the complexity measure

Now we proceed to characterize, in a formal way, what we will take as the most complex

sequence. Let us assume that after the segmentation procedure, at a given level of detail,

the sequence S is decomposed in Ns segments of uniform compositional content, and let us

suppose that we are able to identify a power law for the distribution of the segments length:

λ∗
l=1 l−µ, λ∗ is a cutoﬀ length and µ ≥ 1. As we indicated in the
where Z(µ, λ∗) = P
introduction and for the reasons there expressed we chose µ = 1. The cutoﬀ λ∗ have to do

with the ﬁnite size of the sequence S. Its value can be deduced from the condition

From the distribution law (8), and for a given partition A, we can evaluate the frequencies

Nl =

Ns
Z(µ, λ∗)

l−µ

Ns

Z(µ − 1, λ∗)
Z(µ, λ∗)

= L

fj =

1
Ns

X
lǫ[Aj,Aj+1−1]

Nl,

(8)

(9)

(10)

and from these one, the entropy (7).

At this point we look for the partition A that makes the entropy (7) to reach a maximum

value when the frequencies (10) are replaced. Due to a fundamental property of the entropy,

the maximum value of HS(F ; A, Du) is reached for a partition A such that all the frequencies

fj are equal for all j, that is, the number of segments belonging to the interval [Aj−1, Aj)
is the same for all j. Due to the cutoﬀ, there exists a value j∗ such that fj = 0 for
j > j∗. Hence, the maximum of the entropy corresponds to the biggest j∗ consistent with
the uniformity condition for the fj. The entropy HS(F ; A, Du) will be, in this case, log2 j∗.

8

To satisfy the above two conditions, that is, the uniformity of fj for j ≤ j∗ and the
biggest value for j∗, we must ﬁnd a partition A of the interval [1, L] such that the number of

segments in each interval is constant and equal to one. These requirements can be expressed

as a set of equations to be satisﬁed by the extremes of each one of the intervals of the

partition A:

1 +

1
2µ + . . . +
1
Aµ
3

+ . . . +

1
(A2 − 1)µ =
1
(A4 − 1)µ =

1
Aµ
2
1
Aµ
4

+ . . . +

+ . . . +

1
(A3 − 1)µ
1
(A5 − 1)µ

1
Aµ

j∗−2

+ . . . +

1
(Aj∗−1 − 1)µ =

1
Aµ

j∗−1

+ . . . +

1
(λ∗)µ

...

(11)

with µ = 1.

As we are looking for the maximum j∗ it is obvious from the previous set of equations

that we must take A2 = 2. The rest of the amplitudes ∆j = Aj − Aj−1 can be obtained from

the set of equations (11).

sequence S of length L. We deﬁne it as:

Now we are in position to introduce our complexity measure for an arbitrary symbolic

CS = H[FL],

(12)

where H[FL] is the entropy of the distribution of lengths of the domains in which the sequence

has been decomposed, evaluated according to the partition of the interval [1, L] given by the

relations (11) with µ = 1.

The evaluation of complexity (12) for an arbitrary sequence S of length L requires:

1. To calculate the partition A corresponding to the length L according to (11) for µ = 1;

2. by using the segmentation procedure described in section II, at certain signiﬁcance

value Du, evaluate the set of length Ω and from it the frequencies fj given by (6) for

the partition A;

3. ﬁnally, evaluate the entropy HS given by (7).

Incidentally it is worth to mention that for a greater value of µ compatible with the

ﬂat autocorrelation condition (µ ≤ 2), the entropy H[FL] evaluated following the previously

9

described steps, takes values extremely slow. Therefore, besides the conceptual motives that

led to the election of µ = 1, there are practical ones as well.

IV. APPLICATIONS AND RESULTS

In this section we apply the proposed measure to the evaluation of the complexity for

some DNA sequences. In all examples the quaternary alphabet {A, T, C, G} is used. These

evaluations allow us, on one side, to study the main properties of the measure, such as the

dependence with the level of detail in the analysis of the sequence and the super-additivity

property; on the other we can investigate our measure as an adequate tool for unravelling

certain structural features within the DNA, for instance, the content of introns and exons,

and its relation with evolutionary aspect of the genome.

As it was already claimed, an appropriate complexity measure should take into account

the level of detail at what the system under study is analyzed [19]. To check this dependence

we apply the measure (12) to real DNA sequences with diﬀerent correlation structure and

to a computer generated random sequence. Figure 1 shows the complexity CS as a function

of the threshold level, Du, for the genomic sequences HUMTCRADCV, the ECO110k and

the random one (this kind of plots are known as complexity proﬁle). The ﬁrst one is a

human DNA sequence with long range correlations [20]. The second one is an uncorrelated

bacterial sequence. A ﬁrst remarkable aspect of CS is that there exists a range for the

signiﬁcance value Du, 20 ≤ Du ≤ 50, for which it gets the null value when evaluated for

the random sequence. This random sequence has been built with identical composition that

those of the ECO110k. For Du belonging to this interval, the values of the complexity for

the human sequence are greater than those for the bacterial one. This fact is consistent

with taking as range of interest for the threshold the interval previously indicated. One

noticeable characteristic of the complexity proﬁles for the natural sequences, is that, unlike

those obtained for the complexity measure introduced in [16], do not go to zero as the

threshold Du increases.

Another investigated aspect of CS has to do with the super-additivity property, eq. (1).

In ﬁgure 2 we show the complexity proﬁles for the complete DNA sequences ECO110k and

the human beta-globulin HUMHBB, and the weighted sum of the complexity proﬁles for

two arbitrary subsequences of these two sequences. Clearly the equation (1) is veriﬁed. It

10

is obvious from the deﬁnition of CS that the complexity of any self concatenation of an

arbitrary sequence is equal to complexity of the original sequence whenever the fusion point

coincides with a cut point resulting from the segmentation procedure. If this is not the case,

the resulting value for the complexity of the concatenated sequence might be, for very long

sequences, slightly diﬀerent to the complexity of the original sequence.

It is known that only a small portion of the genome of higher organisms encodes infor-

mation for amino acid sequences of proteins [21]. The role of introns (continuous noncoding

regions in DNA) and intergenomic sequences (noncoding DNA fragments intertwined be-

tween coding regions) remain still unknown. The study of the statistical properties of the

noncoding regions has shown the existence of long range correlations which indicate the

presence of an underlying structural order in the intron and intergenomic segments. This

structural order is made apparent in the complexity proﬁles shown in ﬁgure 3, where we

have plotted the complexity values for the coding and noncoding regions of the human

chromosome 22.

Genomic sequences are a valuable source of information about the evolutionary history

of species [23]. In particular it has been possible to relate some statistical characteristics

observed along genomic sequences to the inﬂuences of a variety of ongoing processes including

evolution [24].

In this context we conclude this work evaluating the complexity CS for

homologous DNA sequences of diﬀerent species; in particular for the myosin heavy-chain. In

general it can be observed that there exists a concordance between the biological complexity

of the species and the values of CS. It should be emphasized that there exists a relationship

between the percentage of introns and the long-range correlations in the sequence. This fact

is clearly manifested by the complexity CS as can be observed in ﬁgure 4.

AKNOWLEDGMENT

APM and PWL are grateful to Secretaria de Ciencia y T´ecnica de la Universidad Nacional

de C´ordoba, Argentina, for ﬁnancial assistance. AM is a fellowship holder of CONICET and

PWL is a member of CONICET. This work was partially supported by Grant BIO2002-

04014-C03-03 from Spanish Government. The authors like to thank Professors Jos´e Oliver

11

and Domingo Prato for useful comments.

[1] C.H. Bennett, in Complexity, Entropy, and Physics of Information, SFI Studies in the Sciences

of Complexity, W. Zurek (Ed.), Addison-Wesley Press (1990).

[2] S. A. Kauﬀman. The Origins of Order: Self-Organization and Selection in Evolution. Oxford

University Press (1993).

[3] R. Sol´e & B. Goodwin. Signs of Life: How Complexity pervades Biology. Basic Books (2000).

[4] A. N. Kolmogorov 1965. Prob. Info. Trans., 1:1-7.

[5] A. Lempel & J. Ziv 1976. IEEE Transaction on Information Theory, 22(1):75-81.

[6] S. Lloyd & H. Pagels Ann. Phys. 188:186-213 (1988).

[7] C. Adami, BioEssays 24:1085-1094 (2002).

[8] R. Lopez-Ruiz, H. Mancini & X. Calbet, Phys. Lett. A 209:321-326 (1995).

[9] D.R. Chialvo, Physica A 340:756-765 (2004).

[10] P. Bak, C. Tang & K. Wiesenfeld, Phys. Rev. Lett. 59:381-384 (1987).

[11] M. Newman, arXiv:cond-mat/0412004v2 (2005).

[12] V.M. Eguluz, D.R. Chialvo, G.A. Cecchi, M. Baliki and A. Vania Apkarian, Phys. Rev. Lett,

94, 018102 (2005).

[13] H.E. Stanley, S.V. Buldyrev, A.L. Goldberger, Z.D. Goldberger, S. Havlin, R.N. Mantegna,

S.M. Ossadnik, C.-K.Peng & M. Simons, Physica A, 205: 214-253 (1994).

[14] P. Bernaola-Galv´an, P. Carpena, R. Rom´an-Rold´an & J., Gene 300:105-115 (2002).

[15] P. Bernaola-Galv´an, J. Oliver, R. Rom´an-Rold´an, Phys. Rev. Lett. 83:3336-3339 (1999).

[16] R. Rom´an-Rold´an, P. Bernaola-Galv´an & J. Oliver, Phys. Rev. Lett. 80:1344-1347 (1998).

[17] J. Lin, IEEE Trans. Inf. Theory 37:145-151 (1991).

[22] I. Grosse, P. Bernaola-Galvan, P. Carpena, R. Roman Roldan, J. Oliver & H.E. Stanley, Phys.

Rev. E, 65:041905-16 (2002).

[19] W. Li, Complexity 3(2):33-37 (1997).

[20] C-K. Peng, S.V. Buldyrev, A.L. Goldberger, S. Havlin, F. Sciortino, M. Simons and H.E.

Stanley, Nature 356, 168-170 (1992).

[21] Genes VI, Oxford University Press, Oxford (1997).

[22] I. Grosse, H. Herzel, S. Buldyrev and H.E. Stanley, Phys. Rev. E 61:5624-5628 (2000).

12

[23] M.A. Huynen and P. Bork, Proc. Natl. Acad. Sci. USA 95 5849-5856 (1998)

[24] S. V. Buldyrev, A. L. Goldberger, S. Havlin, C.-K. Peng, H. E. Stanley and M. Simons,

Biophys. J. 65: 2675-2681 (1993).

13

2.0

1.8

1.6

1.4

1.2

s

C

1.0

0.8

0.6

0.4

0.2

0.0

-0.2

 ECO110k

 HUMCTRADCV

 Random

 

0

10

20

30

40

50

D

u

FIG. 1: Complexity proﬁles of two natural sequences and a computer generated random sequence.

In this last case, the sequence has the same compositional content that the ECO110k.

14

 

2.0(cid:13)

1.9(cid:13)

1.8(cid:13)

1.7(cid:13)

1.5(cid:13)

1.4(cid:13)

1.3(cid:13)

S

1.6(cid:13)

C

 

 HUMHBB(cid:13)
 (L(cid:13)
+L(cid:13)
H(cid:13)
H1(cid:13)
 ECO(cid:13)
E(cid:13)
 (L(cid:13)

+L(cid:13)

1(cid:13)

E1(cid:13)

1(cid:13)

H(cid:13)

)/L(cid:13)

H2(cid:13)

2(cid:13)

HUM(cid:13)

E(cid:13)

)/L(cid:13)

E2(cid:13)

2(cid:13)

ECO(cid:13)

10(cid:13)

15(cid:13)

20(cid:13)

25(cid:13)

30(cid:13)

35(cid:13)

40(cid:13)

FIG. 2: Complexity proﬁles for the sequences ECO110k (LECO = 111408 bp) and HUMHBB

(LHU M = 73308 bp). The ﬁlled symbols correspond to the complexity for the whole sequences, and

the empty ones correspond to the (weighted) sum of the complexities for two arbitrary subsequences

of each sequence. The subsequences were taken in such a way that their juxtaposition were equal

to the complete sequence (LE1 = 57120 bp and LE2 = 54288 bp; LH1 = 42720 bp and LH2 =

30588bp).

 (cid:13)

D(cid:13)

u(cid:13)

15

(cid:13)
(cid:13)
(cid:13)
1.90

1.85

1.80

1.75

S

C

1.70

1.65

1.60

1.55

 Noncoding

 Coding

 

30

35

40

45

50

FIG. 3: Diﬀerences in CS between coding and noncoding regions of the sequence corresponding to

human chromosome 22.

 

D

u

16

2.0

1.8

1.6

1.4

1.2

S

C

0.8

0.4

0.2

0.0

-0.2

 Human

1.0

 Rat

0.6

 Brugia

 Chicken

 Drosophila

 Acanthamoeba

 Caernohabditis

 Yeast

 

D

u

10

20

30

40

50

60

 

FIG. 4: Complexity proﬁles of myosin heavy-chain genes in diﬀerent species (total length, percent-

age of introns): Human (28438bp, 74%), Rat (25759bp, 77%), Chicken (31111bp, 74%), Drosophila

(22663bp, 66%), Brugia (11766bp, 32%), Acathamoeba (5894bp, 10%), Caenorhabditis (10780bp,

14%), Yeast (6108bp, 0%)

17


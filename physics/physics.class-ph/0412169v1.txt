4
0
0
2
 
c
e
D
 
8
2
 
 
]
h
p
-
s
s
a
l
c
.
s
c
i
s
y
h
p
[
 
 
1
v
9
6
1
2
1
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Memory Functions of the Additive Markov chains:
Applications to Complex Dynamic Systems

S. S. Melnyk, O. V. Usatenko, and V. A. Yampol’skii ∗
A. Ya. Usikov Institute for Radiophysics and Electronics
Ukrainian Academy of Science, 12 Proskura Street, 61085 Kharkov, Ukraine

A new approach to describing correlation properties of complex dynamic systems with long-
range memory based on a concept of additive Markov chains (Phys. Rev. E 68, 061107 (2003)) is
developed. An equation connecting a memory function of the chain and its correlation function is
presented. This equation allows reconstructing the memory function using the correlation function
of the system. Thus, we have elaborated a novel method to generate a sequence with prescribed
correlation function. Eﬀectiveness and robustness of the proposed method is demonstrated by
simple model examples. Memory functions of concrete coarse-grained literary texts are found and
their universal power-law behavior at long distances is revealed.

PACS numbers: 05.40.-a, 02.50.Ga, 87.10.+e

The problem of long-range correlated dynamic systems
(LRCS) has been under study for a long time in many
areas of contemporary physics [1, 2], biology [3, 4], eco-
nomics [4, 5], etc. [4, 6]. An important example of com-
plex LRCS are naturally written texts [7, 8, 9]. The
eﬃcient method for investigating long-range correlations
into such systems consists in the decomposition of the
space of states into a ﬁnite number of parts labelled by
deﬁnite symbols, which are naturally ordered according
to the dynamics of the system. The most frequently used
method of the decomposition is based on the introduc-
tion of two parts of the phase space. In other words, the
approach assumes mapping two kinds of states onto two
symbols, say 0 and 1. Thus, the problem is reduced to in-
vestigating the statistical properties of binary sequences.
It might be thought that the coarse graining procedure
could result in losing, at least, the short-range memory
between symbols in a sequence. However, as indicated
below, this procedure retains, although not completely,
the correlations at all distances. This means that, for the
analysis of correlating properties of the dynamic systems,
there is no point in coding every symbol (associating ev-
ery part of the phase space of the system with its binary
code), as it is done, for example, in Ref. [8]. It is suﬃ-
cient to use the coarse graining procedure. It also means
that there is no need to use a more complicated method
of mapping using ternary or more valued functions for
the coarse graining renormalization.

One of the ways to get a correct insight into the
nature of correlations in a system consists in an abil-
ity of constructing a mathematical object (for example,
a correlated sequence of symbols) possessing the same
statistical properties as the initial system. There ex-
ist many algorithms for generating long-range correlated
the inverse Fourier transformation [6], the
sequences:

∗E-mail: yam@ire.kharkov.ua

expansion-modiﬁcation Li method [10], the Voss proce-
dure of consequent random additions [11], the correlated
Levy walks [12], etc. [6, 13]. We believe that, among the
above-mentioned methods, using the many-step Markov
chains is one of the most important, because it oﬀers a
possibility to construct a random sequence with deﬁnite
correlation properties in the most natural way. This was
demonstrated in Ref. [14], where the concept of additive
Markov chain with the step-like memory function (which
allows the analytical treatment) was introduced. There
exist some dynamic systems (coarse-grained sequences
of Eukarya’s DNA and dictionaries) with the correlation
properties that can be well described by this model.

In the present work, we continue investigating into ad-
ditive Markov chains with more complex memory func-
tions. An equation connecting mutually-complementary
characteristics of a random sequence, i.e. the memory
and correlation functions, is obtained. Upon ﬁnding the
memory function of the original random sequence on the
basis of the analysis of its statistical properties, namely,
its correlation function, we can build the corresponding
Markov chain, which possesses the same statistical prop-
erties as the initial sequence. Eﬀectiveness and robust-
ness of the proposed method is demonstrated by simple
model examples. This method is most essential for some
applications, e.g., for the construction of correlated se-
quence of elements which can be used to fabricate the ef-
fective ﬁlters of electrical or optical signals, Ref. [15]. The
suggested method allowed us to ﬁnd memory functions of
concrete coarse-grained literary texts and to reveal their
universal power-law behavior at long distances.

Let us consider a homogeneous binary sequence of sym-
bols, ai = {0, 1}. To determine the N -step Markov
chain we have to introduce the conditional probabil-
ity P (ai
| ai−N , ai−N +1, . . . , ai−1) of occurring the
deﬁnite symbol ai (for example, ai = 0) after sym-
bols ai−N , ai−N +1, . . . , ai−1. Thus,
it is necessary to
deﬁne 2N values of the P -function corresponding to
each possible conﬁguration of the symbols in N -word

ai−N , ai−N +1, . . . , ai−1. The value of N is referred to
as the memory length of Markov chain.

Considering that we are going to deal with the se-
quences possessing the memory length of order of 106,
we need to make some simpliﬁcation of the P -function.
We suppose that it has the additive form,

P (ai = 0 | ai−N , ai−N +1, . . . , ai−1)

=

f (ai−k, k),

(1)

N

k=1
X

and corresponds to the additive inﬂuence of the previous
symbols upon the generated one. The value of f (ai−k, k)
is the contribution of symbol ai−k to the conditional
probability of occurring the symbol zero at the ith site.
The homogeneity of the Markov chain is provided by the
i-independence of conditional probability Eq. (1).
Let us rewrite Eq. (1) in an equivalent form,

N

r=1
X
N

r=1
P

P (ai = 0 | .) = 1 − ¯a +

F (r)(¯a − ai−r).

(2)

N

r=1
P

Here ¯a =

f (0, r)/[1 −

(f (1, r) − f (0, r))] and

F (r) = f (1, r) − f (0, r).
It is possible to show that ¯a
is the value of ai averaged over the whole sequence. We
refer to F (r) as the memory function (MF). It describes
the strength of inﬂuence of previous symbol ai−r upon
a generated one, ai. To the best of our knowledge, the
concept of memory function for many-step Markov chains
was introduced in Ref. [9]. The function P (. | .) contains
the complete information about correlation properties of
the Markov chain. Typically, the correlation function
and other moments are employed as the input charac-
teristics for the description of the correlated random se-
quences. However, the correlation function describes not
only the direct interconnection of the elements ai and
ai+r, but also takes into account their indirect interac-
tion via all other intermediate elements. Our approach
operates with the ”origin” characteristics of the system,
speciﬁcally, with the memory function. The correlation
and memory functions are mutual-complementary char-
acteristics of a random sequence in the following sense.
The numerical analysis of a given random sequence en-
ables one to directly determine the correlation function
rather than the memory function. On the other hand,
it is possible to construct a random sequence using the
memory function, but not the correlation one. Therefore,
we believe that the investigation of memory function of
the correlated systems will permit one to disclose their in-
trinsic properties which provide the correlations between
the elements.

2

L

l=1
P

which participates in a correlated Brownian motion. Ev-
ery element of the sequence corresponds to the instant
change of particle’s coordinate. Every L-word (the sub-
sequence of symbols of the length L in the sequence) can
be regarded as one of the realizations of the ensemble of
correlated Brownian trajectories in the ”temporal” inter-
val L. This point of view on the symbolic sequence makes
it possible to use the statistical methods for investigating
the dynamic systems.

We consider the distribution WL(k) of the words of
deﬁnite length L by the number k of unities in them,

ki(L) =

ai+l, and the variance D(L) of ki(L),

D(L) = (k − ¯k)2,

(3)

where the deﬁnition of average value of g(k) is g(k) =
L

g(k)WL(k). It follows from Eq. (2) that the positive

k=0
MF values result in the persistent diﬀusion where previ-
P
ous displacements of the Brownian particle in some direc-
tion provoke its consequent displacement in the same di-
rection. The negative values of the MF correspond to the
anti-persistent diﬀusion where the changes in the direc-
tion of motion are more probable. In terms of the Ising
model with long-range particles interactions that could
be naturally associated with the Markov chains, the pos-
itive (negative) values of the MF correspond to the fer-
romagnetic (anti-ferromagnetic) interaction of particles.
The additive form (1) of the conditional probability func-
tion corresponds to the pair interaction and disregard of
many-particles interactions.

The memory function used in Refs. [9, 14] was char-
acterized by the step-like behavior and deﬁned by two
parameters only: the memory depth N and the strength
of symbol’s correlations. Such a memory function de-
scribes only one type of correlations in a given system,
the persistent or anti-persistent one, which results in the
super- or sub-linear dependence D(L). Obviously, both
types of correlations can be observed at diﬀerent scales in
the same system. Thus, one needs to use more complex
memory functions for detailed description of the systems
with both type of correlations. Besides, we have to ﬁnd
out a relation connecting the mutually-complementary
characteristics of random sequence, the memory and cor-
relation functions.

We suggest below two methods for ﬁnding the memory
function F (r) of a random binary sequence with a known
correlation function. The ﬁrst one is based on the mini-
mization of a ”distance” Dist between the Markov chain
generated by means of a sought-for MF and the initial
sequence of symbols. This distance is determined by a
formula,

Dist = (ai − P1(i))2.

(4)

A dichotomic symbols in a Markov chain can be
thought of as the sequence of states of some particle,

Here P1(i)
curring generated symbol ai = 1 after

conditional probability of oc-
symbols

the

is

ai−N , ai−N +1, . . . , ai−1,

P1(i) = 1 − P0(i),

(5)

with P0(i) = P (ai = 0 | .) determined by Eq. (2). From
minimization equation δDist/δF (r) = 0 one obtains the
relation connecting the memory function F (r) and the
correlation one K(r):

K(r) =

F (r

)K(r − r

).

′

′

(6)

N

r′=1
X

The correlation function K(r) is determined by the usual
equation,

K(r) = aiai+r − ¯a2, K(0) = ¯a(1 − ¯a).

(7)

By deﬁnition, the correlation function is even, K(r) =
K(|r|). Equation (6) can also be obtained by a straight-
forward calculation of expression aiai+r in Eq. (7) using
the deﬁnition (2) of memory function.

0.050

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

0.045

0.040

K

0.035

0.030

0

5

10

15

20

25

30

r

0.08

0.06

0.04

F

0.02

0.00

0

5

10

15

20

25

30

r

FIG. 1: The initial memory function Eq. (9) (solid line) and
the reconstructed one (dots) vs the distance r. In inset, the
correlation function K(r) obtained by a numerical analysis of
the sequence constructed by means of the memory function
Eq. (9).

The second method resulting from the ﬁrst one, estab-
lishes a relationship between the memory function F (r)
and the variance D(L),

M (r, 0) =

′

F (r

)M (r, r

),

(8)

N

r′=1
X

′

′

3

0.08

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

0.06

               

               

               

               

               

               

               

               

               

               

0.0

               

               

               

               

               

               

               

               

               

               

0

5

10

15

20

25

30

0.02

r

0.04

K

0.00

-0.02

0.3

0.2

0.1

F

-0.1

r

0

5

10

15

20

25

30

FIG. 2: The model correlation function K(r) described by
Eq. (10) (solid line). The dots correspond to the reconstructed
correlation function. In inset, the memory function F (r) ob-
tained by numerical solution of Eq. (6) with correlation func-
tion Eq. (10).

Let us verify the robustness of our method by numeri-
cal simulations. We consider a model ”triangle” memory
function,

F (r) = 0.008

r,
20 − r,
0,

(

1 ≤ r < 10,
10 ≤ r < 20,
r ≥ 20,

(9)

presented in Fig. 1 by solid line. Using Eq. (2), we con-
struct a random non-biased, ¯a = 1/2, sequence of sym-
bols {0, 1}. Then, with the aid of the constructed binary
sequence of the length 106, we calculate numerically the
correlation function K(r). The result of these calcula-
tions is presented in inset Fig. 1. One can see that the
correlation function K(r) mimics roughly the memory
function F (r) over the region 1 ≤ r ≤ 20. In the region
r > 20, the memory function is equal to zero but the cor-
relation function does not vanish [16]. Then, using the
obtained correlation function K(r), we solve numerically
Eq. (6). The result is shown in Fig. 1 by dots. One can
see a good agrement of initial, Eq. (9), and reconstructed
memory functions F (r).

The main and very nontrivial result of our paper con-
sists in the ability to construct a binary sequence with
an arbitrary prescribed correlation function by means of
Eq. (6). As an example, let us consider the model corre-
lation function,

K(r) = 0.1

sin(r)
r

,

(10)

′

′

′

M (r, r

) = D(r − r

)− (D(−r

)+ r[D(−r

+ 1)− D(−r

)]).

′

It is a set of linear equations for F (r) with coeﬃcients
determined by D(r). The relations, K(r) = [D(r − 1) −
2D(r) + D(r + 1)]/2 obtained in Ref. [9] and D(−r) =
D(r) are used here.

presented by the solid line in Fig. 2. This kind of the cor-
relation function is important in the problem of Anderson
localization, see Ref. [15]. We solve Eq. (6) numerically
to ﬁnd the memory function F (r) using this correlation
function. The result is presented in inset Fig. 2. Then
we construct the binary Markov chain using the obtained

4

memory function F (r). To check up a robustness of the
method, we calculate the correlation function K(r) of
the constructed chain (the dots in Fig. 2) and compare
it with Eq. (10). One can see an excellent agreement be-
tween the initial and reconstructed correlation functions.

0.04

0.00

F

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

               

10

               

               

               

               

               

               

               

               

               

               

1

2

3

10

10

10

L

7

6

5

4

3

2

1

0

10

10

10

n

D

10

10

10

10

-1

10

2

1

0

n

 

 

D

/

D

L

0

1

2

3

4

5

6

10

10

10

10

10

10

10

FIG. 3: The normalized variance Dn(L) for the coarse-grained
text of Bible (solid line) and for the sequence generated by
means of the reconstructed memory function F (r) (dots). The
dotted straight line describes the non-biased non-correlated
Brownian diﬀusion, D0(L) = L/4. The inset demonstrates
the anti-persistent dependence of ratio Dn(L)/D0(L) on L at
short distances.

Let us demonstrate the eﬀectiveness of our concept
of the additive Markov chains when investigating the
correlation properties of coarse grained literary texts.
First, we use the coarse-graining procedure and map
the letters of the text of Bible [17] onto the symbols
zero and unity (here, (a − m) 7→ 0, (n − z) 7→ 1).
Then we examine the correlation properties of the con-
structed sequence and calculate numerically the variance
D(L). The result of simulation of the normalized vari-
ance Dn(L) = D(L)/4¯a(1 − ¯a) is presented by the solid
line in Fig. 3. The dominator 4¯a(1 − ¯a) in the equation
for the normalized variance Dn(L) is inserted in order
to take into account the inequality of the numbers of ze-
ros and unities in the coarse-grained literary texts. The
straight dotted line in this ﬁgure describes the variance
D0(L) = L/4, which corresponds to the non-biased non-
correlated Brownian diﬀusion. The deviation of the solid
line from the dotted one demonstrates the existence of
correlations in the text. It is clearly seen that the diﬀu-
sion is anti-persistent at small distances, L <
∼ 300, (see
inset Fig. 3) whereas it is persistent at long distances.

The memory function F (r) for the coarse-grained text
of Bible at r < 300 obtained by numerical solution of
Eq. (8) is shown in Fig. 4. At long distances, r > 300,
the memory function can be nicely approximated by the
power function F (r) = 0.25r−1.1, which is presented by
the dash-dotted line in inset Fig. 4.

Note that the region r <

∼ 40 of negative anti-persistent

-0.04

               

               

               

               

               

2

3

4

 

L

10

10

10

               

               

               

               

               

-3

10

               

               

               

               

               

-0.08

               

               

               

               

               

               

               

               

               

               

1

10

100

-4

1x10

F

-5

1x10

L

FIG. 4: The memory function F (r) for the coarse-grained
text of Bible at short distances. In inset, the power-law de-
creasing portions of the F (r) plots for several texts. The dots
corresponds to ”Pygmalion” by B. Shaw. The solid line corre-
sponds to power-law ﬁtting of this function. The dash dotted
and dashed lines correspond to Bible in English and Russian,
respectively.

memory function provides much longer distances L ∼ 300
of anti-persistent behavior of the variance D(L).

Our study reveals the existence of two characteristic
regions with diﬀerent behavior of the memory function
and, correspondingly, of persistent and anti-persistent
portions in the D(L) dependence. This appears to be a
prominent feature of all texts written in any language.
The positive persistent portions of the memory func-
tions are given in inset Fig. 4 for the coarse-grained
English- and Russian-worded texts of Bible (dash-dotted
and dashed lines, Refs. [17] and [18], correspondingly).
Besides, for comparison, the memory function of the
coarse-grained text of ”Pygmalion” by B. Shaw [19] is
presented in the same inset (dots), the power-law ﬁtting
is shown by solid line.

Thus, we have demonstrated the eﬃciency of descrip-
tion of the symbolic sequences with long-range correla-
tions in terms of the memory function. An equation con-
necting the memory and correlation functions of the sys-
tem under study is obtained. This equation allows recon-
structing a memory function using a correlation function
of the system. Actually, the memory function appears to
be a suitable informative ”visiting card” of any symbolic
stochastic process. The eﬀectiveness and robustness of
the proposed method is demonstrated by simple model
examples. Memory functions for some concrete examples
of the coarse-grained literary texts are constructed and
their power-law behavior at long distances is revealed.
Thus, we have shown the complexity of organization of
the literary texts in contrast to a previously discussed
simple power-law decrease of correlations [20]. It should
be noted, however, that the linguistic aspects of our dis-
cussion require a regular and systematic study.

We have examined the simplest examples or random
sequences, the dichotomic one. Nevertheless, our prelim-
inary consideration suggests that the presented theory
can by generalized to the arbitrary additive Markov pro-
cess with a ﬁnite or inﬁnite number of states and with
discrete or continuous ”time”. A study in this direction
is in progress.

The proposed approach can be used for the analysis of

other correlated systems in diﬀerent ﬁelds of science.

We thank A. Krokhin and M. Johansson for helpful

discussions.

[1] U. Balucani, M. H. Lee, V. Tognetti, Phys. Rep. 373,

409 (2003).

[2] I. M. Sokolov, Phys. Rev. Lett. 90, 080601 (2003).
[3] R. F. Voss, Phys. Rev. Lett. 68, 3805 (1992).
[4] H. E. Stanley et. al., Physica A 224,302 (1996).
[5] R. N. Mantegna, H. E. Stanley, Nature (London) 376,

46 (1995).

[6] A. Czirok, R. N. Mantegna, S. Havlin, and H. E. Stanley,

Phys. Rev. E 52, 446 (1995).

[7] A. Schenkel, J. Zhang, and Y. C. Zhang, Fractals 1, 47

5

(1993).

(2000).

[8] P. Kokol, V. Podgorelec, Complexity International, 7, 1

[9] O. V. Usatenko, V. A. Yampol’skii, S. S. Mel’nyk, and
K. E. Kechedzhy, Phys. Rev. E 68, 061107 (2003).

[10] W. Li, Europhys. Let. 10, 395 (1989).
[11] R. F. Voss, in: Fundamental Algorithms in Computer
Graphics, ed. R. A. Earnshaw (Springer, Berlin, 1985) p.
805.

[12] M. F. Shlesinger, G. M. Zaslavsky, and J. Klafter, Nature

(London) 363, 31 (1993).

[13] I. F. Herbut, arXiv:cond-mat/0007266.
[14] O. V. Usatenko and V. A. Yampol’skii, Phys. Rev. Lett.

90, 110601 (2003).

[15] F. M. Izrailev, A. A. Krokhin, and S. E. Ulloa, Phys.

Rev. B 63, 041102(R) (2001).

[16] The existence of the ”additional tail” in the correlation
function is in agreement with Ref. [9] and corresponds to
the well known fact that the correlation length is always
larger then the region of memory function action.
[17] The Old Testament of the King James Version of the

Bible, http://www.writersbbs.com/bible/.

[18] Russian Synodal LiO 31/7/91, http:

//lib.ru/ hris-

tian/bibliya/nowyj zawet.txt.

[19] http://eserver.org/drama/pygmalion/default.html.
[20] I. Kanter, D. A. Kessler, Phys. Rev. Lett. 74, 22 (1995).


6
0
0
2
 
n
u
J
 
6
 
 
]
h
p
-
s
s
a
l
c
.
s
c
i
s
y
h
p
[
 
 
1
v
3
5
0
6
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Optimal estimation for Large-Eddy Simulation of
turbulence and application to the analysis of subgrid
models

A. Moreaua, O. Teytaudb and J.P. Bertoglioc

a LASMEA, Universit´e Blaise Pascal, 24, avenue des Landais, 63177 Aubi`ere
b LRI, Universit´e Paris Sud, 91405 Orsay
c LMFA, Ecole Centrale de Lyon, 36 avenue Guy de Collongue, 69134 Ecully

Abstract

The tools of optimal estimation are applied to the study of subgrid models for Large-
Eddy Simulation of turbulence. The concept of optimal estimator is introduced and
its properties are analyzed in the context of applications to a priori tests of subgrid
models. Attention is focused on the Cook and Riley model in the case of a scalar
ﬁeld in isotropic turbulence. Using DNS data, the relevance of the β assumption is
estimated by computing (i) generalized optimal estimators and (ii) the error brought
by this assumption alone. Optimal estimators are computed for the subgrid variance
using various sets of variables and various techniques (histograms and neural networks).
It is shown that optimal estimators allow a thorough exploration of models. Neural
networks are proved to be relevant and very eﬃcient in this framework, and further
usages are suggested.

1 Introduction

The principle of Large Eddy Simulations is to solve the evolution equations only for
the large scales of a turbulent ﬂow. Since the large eddies do not contain all the
information that would be necessary to compute the future of a given ﬂow[1], the
evolution equations for the large eddies are not closed. It is then a generic problem
in any type of LES, that the unknown terms have to be approximated using known
quantities, i.e. quantities that can be computed directly using information associated
with the resolved ﬁeld, or quantities estimated via additional subgrid variables solution
of auxiliary evolution equations (a subgrid turbulent tensor [2], a subgrid probability
[3], or a subgrid spectrum [4]...).

The aim of the present paper is to introduce the concept of optimal estimator as a
tool to estimate the minimal error that a perfect subgrid model based on a given set of
known large scale quantities (the variables of the model) will generate (the irreducible

1

error). This optimal estimator strategy is considered by the authors of the present
paper as a very helpful process in the ﬁeld of subgrid modeling, since it provides a way
of assessing the relevance of the set of variables on which a subgrid model will be built,
before having to specify the precise form of the model.

The optimal estimator can be computed numerically if the true subgrid term is
known, that is to say using the results of a Direct Numerical Simulation (DNS). It
is therefore a concept that is developed in the framework of what is usually referred
to as ”a priori” test of subgrid models. The ﬁrst sections of the paper are devoted
to presenting the method of building the optimal estimator using diﬀerent techniques.
The classical technique relies on histograms, and a new and promising technique based
on neural networks is introduced. It is pointed out in section 7, that neural networks
are indeed particularly relevant and eﬃcient to build the optimal estimator when the
number of parameters to be included in the subgrid model increases. Some classical
results in optimal estimation will also be exposed in the two following sections.

In the second part of the paper (sections 4 to 7), the interest of optimal estimators is
illustrated in the particular case of the subgrid modeling problem for a simple reaction
term depending on a single passive scalar in isotropic homogeneous turbulence. The
procedure is used to assess the performances of the popular Cook and Riley model[5],
which uses a β-distribution as a presumed form of the Filtered Density Function (FDF).
It is shown how it allows to distinguish between the various sources of errors in the
model. Once the error brought by each assumption contained in the model has been
estimated, it is easy to identify at which level improvements could be made.

We will particularly compare the error associated with the choice of the β distri-
butions, with the one resulting from the use of a sub-model to estimate the subgrid
variance. We will address the problem of the relevancy of the diﬀerent variables which
have been used in the literature[5, 6] to express the subgrid variance.

2 Properties of optimal estimators

The optimal estimator Ω for a quantity γ, from a set π of quantities (that we will
call the variables) and a norm ||.||, minimizes ||Ω(π) − γ||. Optimal estimators are
typically deﬁned for the L2-norm (quadratic error) and then Ω minimizes ||Ω(π)−γ||2 =
(Ω(π) − γ)2
where h.i is the statistical mean (also called the expectation). The
is null if and only if γ is a deterministic function of π , so that
quantity
(cid:10)
(Ω(π) − γ)2

is generally not null.

(Ω(π) − γ)2

(cid:11)

(cid:11)

A model is a function g(π) which aims at approaching γ (and thus Ω(π)) as closely
(cid:10)
as possible. In a recent work[1] Langford and Moser ﬁrmly asserted that the quadratic
error is the relevant error to consider in LES. This point of view is here adopted without
further discussion and therefore the quadratic error will be retained throughout the
paper as the relevant criterium for assessing the quality of a subgrid model. This
section surveys and shows some properties of optimal estimators in relation with the
quadratic error.

(cid:10)

(cid:11)

The quadratic error made by g(π) when estimating γ is

Eg =

(γ − g(π))2
D

E

.

2

(1)

The quadratic error satisﬁes the following orthogonality relation (see the proof in

appendix A)

(γ − g(π))2

=

(γ − hγ| πi)2

+

( hγ| πi − g(π))2

.

Since the last term in the RHS of equation 2 is the expectation of a positive quantity,

D

E

D

E

D

E

it is positive. Thus for any g

(γ − g(π))2

≥

(γ − hγ| πi)2

.

D

E

D
This relation means that any subgrid model g, built on the set of variables π, will
lead to quadratic errors larger than the one made by the conditional expectation hγ| πi.
We will call the error made by the conditional expectation the irreducible error
made by estimating γ using π, since no model using π as variables can make a smaller
error. The last term in (2) can be written

E

( hγ| πi − g(π))2

=

( hγ| πi − g(π))2 p(π) dπ.

(4)

D

E

Z

It is equal to zero only if g(π) = hγ| πi. This means that the conditional expectation
is the unique best model[7]. For the quadratic error, the optimal estimator Ω(π) using
π as variables is thus the conditional expectation hγ| πi.

Let π′ be a set of variables that can be computed using π. The optimal estimator

for π′ is thus implicitly a function of π, so that equation (3) becomes

γ − hγ| π′

2

≥

(γ − hγ| πi)2

.

The irreducible error associated with π′ is then always greater than the irreducible
error associated with π.

D(cid:0)

E

(cid:11)(cid:1)

D

E

The optimal estimators veriﬁes another property that is called the “successive con-

ditioning”

hγ| hγ| πii = hγ| πi

The proof of this property is given in appendix B. When considering a given model, it
is common to draw a cloud of points with γ on the y axis and g(π) on the x axis[5, 6].
For a given value a of g(π), it is possible to compute the mean of γ for all the points
that are close to a. This can be done for all the values of a and drawn on the ﬁgure(i.e.,
moving averages). This is a way of representing hγ| g(π) = ai, which is a function of a.
The successive conditioning of the optimal estimators means that if g(π) is an optimal
estimator, then all the points computed as described should be on the y = x line.

Provided they can be computed, optimal estimators (i) allow to know if a given
model is far from the optimal estimator by comparing the error it makes to the irre-
ducible error ; (ii) suggest ways of improving models just by representing the optimal
estimators when this is possible and (iii) allow to compare diﬀerent sets of variables
quantitatively, by computing the irreducible error for each set.

(2)

(3)

(5)

(6)

3

3 Practical computation of optimal estimators

Now that the properties of optimal estimators have been presented, we will turn to the
practical computation of optimal estimators using data : optimal estimation. Optimal
estimation from data in non-parametric frameworks (i.e. without prior knowledge of
the function to be approximated) is a wide area of research consisting in designing
algorithms, termed learning algorithms, that use data (π1, γ1), . . . , (πn, γn) to compute
approximations fn of the optimal Ω, with some nice convergence properties of fn to Ω
as n → ∞.

The main usual hypothesis is that the data are independent and identically dis-
tributed. Various results of Universal Consistency (UC), i.e. asymptotic convergence
towards the optimal function in Lp-norm, have been proved for various techniques ;
histogram-rules ([8]), k-nearest neighbours [9], neural networks ([13]), gaussian support
vector machines ([10]) ; various general results using VC-theory include wide families of
methods ([11]). There is no possible universal convergence rate ; a method is better or
worse than another depending on the distribution of the examples. However, various
heuristics for choosing between various lerning-algorithms are well-known : support
vector machines are often eﬃcient for generalizing from very small samples or when
relevant kernels can be deﬁned, k-nearest neighbours only need a metric, histograms
are simple and interpretable, neural networks do not work well in huge (non-sparse)
dimensionality but can deal with very large numbers of examples.

In consequence, two techniques have been chosen here, in the framework of L2-
norm (quadratic error) using large DNS data. The ﬁrst one is the most intuitive and
is based on the fact that optimal estimators are conditional expectations. This is the
“histogram technique”. The second one is based on the fact that optimal estimators
minimize the quadratic error. It uses neural networks.

First, a conditional expectation can be approximated by a piecewise-constant
function (a histogram). The π space (whose dimension is the number of variables
of the model) is discretized in small cells. Each data point (a value of γ and of π
that has produced by a DNS) belongs to a given cell of the π space. Let us consider
the piecewise-constant function which associates to each cell the mean of γ for all the
data points belonging to the cell. This function is an approximation of the optimal
estimator. When you have π, you can take as an approximation of hγ| πi the value of
the previous function for the cell corresponding to π.

The main diﬃculty is the choice of the size of the cell.

If the size is too big,
the piecewise-constant function will obviously not be a good approximation of the
conditional expectation.
If the size of the cell is too small, too few points will be
contained in each cell and the value associated with each cell will not be reliable.

In order to overcome this diﬃculty, one has to divide the data into two parts. The
ﬁrst one is used for the computation of the piecewise-constant function. Then the error
made by the piecewise-constant function when estimating γ for the second part of the
data will be computed. This error is the generalization error. The relevant size for
the cells is the one for which the generalization error is minimized. Figure 1 shows the
generalization error in function of the number of cells for a given range.

The optimal estimators can be approached using neural networks instead of

4

0.3

0.25

0.2

0.15

0.1

0.05

0

0

Figure 1: Typical generalization error versus the number of cells for each parameter.

100

200

300

400

500

piecewise-constant functions. A neural network can be seen as a parametric func-
tion. For a perceptron with a single hidden layer[12], this function can be written

N

Nπ

g(π) =

Aj tanh

Bjk πk + bj

+ a.

(7)

 

!

j=1
X

Xk=1
where N is the number of neurons in the hidden layer, Nπ is the number of variables
in π and πk is the kth parameter.
In the NN-terminology, the parameters Bjk are
called the weights of the ﬁrst layer, the Aj are the second layer of weights ; a and
the bj are the thresholds. By adjusting the weights of a neural network, it is possible
to approach almost any function. Formally, neural networks with one hidden layer
of neurons have the universal approximation property, i.e. they can approximate any
measurable function for the L2-norm, and they have the statistical consistency, i.e.
this convergence occurs with probability one when the network is trained from data if
the number of neurons increases properly[13]. A typical neural network is represented
ﬁgure 2.

As previoulsy, the data is split into two parts. Using the ﬁrst part, the neural net-
work is trained : the weights are adjusted so that the error made by the neural network
is minimized. This means that the neural network is a numerical approximation of the
optimal estimator. The learning is made using a back-propagation algorithm[14, 12].

Then, the generalization error is computed. The generalization error is the quadratic
error made by the neural network on the rest of the data (on the data that have not
been used for choosing the weights). The number of hidden neurons is chosen in order
to minimize this generalization error.

Neural networks allow to compute optimal estimators for a number of variables
which is greater than 3. Let us just stress that neural networks are usual tools for
pattern recognition, estimation of conditional expectations, density estimation [12].
They have been applied in various areas of physics ([15, 16]), even in ﬂuid mechanics[17].
Other forms of statistical learning tools derived from neural networks have also been
experimented, particularly Support Vector Machines ([18, 19, 20]). We have chosen
neural networks because Support Vector Machines, at least in their most standard

5

g(  )π

1

a1

A11

A12

A13

B11

B21 B31

π

1

π

2

B12

B22

B32

b2

b1

b3

1

Figure 2: Representation of the neural network used to approximate the optimal estimators
in the case where the number of neurons in the hidden layer is 3 and with two variables only.

form, do not use the mean square error, and therefore are not conditional-expectation
estimators, whereas standard neural networks are[13]. However, less standard forms of
Support Vector Machines could also be used[21]; but SVM are much slower for large
data sets such as the ones we will use further on.

4 Filtered Density Functions

The case of a scalar ﬁeld c(~x) advected by turbulence is now considered (c(~x) repre-
senting for instance a temperature or the concentration of a chemical species). The
large scales of the scalar ﬁeld are denoted by c(~x) deﬁned as

c (~x) =

1
h3

c(~x′, t) d~x′.

ZD(~x)
where D(~x) denotes a cube of edge-length h, centered in ~x. The . ﬁlter considered here
is then a box ﬁlter in the physical space. It is a positive ﬁlter : the ﬁltering can be
written as a convolution c = G ∗ c of the scalar ﬁeld by a function

which is positive (see appendix C). Here, H is the Heaviside function. In the Fourier
space, the ﬁlter corresponds to a product of the Fourier transform of the scalar ﬂuctu-
ation by

e
In this article, the scalar ﬁeld is bounded : 0 ≤ c(~x) ≤ 1. As already pointed out[5],
the large eddies are bounded in the same way only if G ≥ 0 and

G = 1.

We now consider quantities that can be written as :

R

G(~x) =

H

− |xi|

1
h3

3

i=1
Y

h
2

(cid:18)

(cid:19)

G(~k) =

sinc

ki h

.

1
2

(cid:18)

(cid:19)

3

i=1
Y

f (c)(~x)

6

(8)

(9)

(10)

(11)

It is not speciﬁed here, but f (c) represents a quantity that
where f is a function.
is important for the simulation and whose ﬁltered value requires closure : a (simple)
chemical reaction term for example. The quantity f (c)(~x) only depends on the Filtered
Density Function (FDF) which is deﬁned[22] by

C − c(~x′)
(cid:17)
(cid:16)
The link between the FDF and the quantity of interest is the following relation[22]

ds(C, ~x) = δ

(~x)

(12)

f (c)(~x) =

f (C) ds(C, ~x) dC

(13)

Z

which means that the knowledge of ds (which is sometimes called the Subgrid PDF)
allows to compute f (c) whatever f is. The FDF is not a statistical quantity :
it is
deﬁned for a given realization of the ﬂow and for a given ~x. Now that this has been
underlined, the space dependence of the FDF will be omitted in the following - as for
f (c) or c.

The mean (on the cube D(~x) and not in the statistical sense) of the FDF is its ﬁrst

moment. It is simply equal to c since

x ds(x) dx =

x δ(x − c) dx = c.

Z

Z

The variance of the FDF will be called the subgrid variance

For a given cube D = D(~x), let us consider the proportion of the cube which
contains a scalar ﬁeld bounded above by C. It will be noted VD(C) and can be written

where H is the Heaviside function. We then have the following relation

σ2
s =

(x − c)2 ds(x) dx = c2 − c2.

Z

VD(C) =

H(C − c(~x)) d~x,

1
h3

Z

∂VD
∂C

∂
∂C

=

=

1
h3
1
h3

Z

Z

H(C − c(~x)) d~x

δ(C − c(~x)) d~x = ds

The FDF thus gives information about the distribution of the values of the scalar ﬁeld
C
−∞ ds(x) dx. Points can be chosen randomly in
in the cube D(~x) since VD(C) =
the cube D(~x). A histogram made using the values of the scalar at these points will
approach the FDF. This gives a way of computing the FDF provided that the ﬁeld is
known everywhere in the cube D(~x).

R

We used data from a pseudo-spectral DNS with periodic boundaries.

In such a
simulation, the evolution equation of the ﬂow is solved in the Fourier space. The ﬁelds

(14)

(15)

(16)

7

of the velocity and of the scalar are then deﬁned by their ﬁrst Fourier modes. For the
scalar, this can be written

c(x, y, z) =

aj,k,l eiω0(j x+k y+l z),

(17)

+κ

+κ

+κ

j=−κ
X

k=−κ
X

l=−κ
X

where ω0 = 2π
L (L being the size of the simulation domain) where κ is the number
of modes retained and the ai,j,k coeﬃcients are the amplitudes of the Fourier modes.
The Fast Fourier Transform (FFT) computes the ﬁeld at special points (grid nodes)
because (i) there is a mapping between the values of the ﬁelds at the grid nodes and
the amplitudes of the Fourier modes and (ii) equation (17) can be factorized, so that
the computation of the ﬁeld at the grid nodes is easier. But let us stress the fact that
the ﬁeld is deﬁned everywhere in the physical space by (17). It can be computed for
an arbitrary ~x by summing the contributions of the diﬀerent modes at this particular
point. This operation is of course very costly compared to a FFT, but it is necessary.
We have tried to approximate the ﬁeld between the nodes using a simple interpolation,
which requires less computation time, but the results are not satisfactory. The FDF
computed using interpolation substantially diﬀers from the one computed using the
rigorous formula (17).

The box ﬁlter (8) cannot easily be computed in the physical space. On the contrary,
it is simple to perform in the Fourier space, since the amplitudes of the Fourier modes
just have to be multiplied by a function given by (10). This is a rigorous method in
the sense that the obtained ﬁeld exacly satisﬁes (8) in the physical space, reﬂecting the
fact that the ﬁeld is indeed implicitly deﬁned everywhere in the physical space when
the Fourier modes are known.

The DNS we have performed use a particular injection method for the scalar
ﬁeld. Periodically in time, large cubes are chosen randomly in the simulation do-
main. “Fresh” scalar is injected in these cubes : in half the cubes the ﬁeld is put to
zero and in the other half it is put to 1. A view of the scalar ﬁeld is shown ﬁgure
3. It has to be stressed that the scalar ﬂuctuation always satisﬁes 0 ≤ c ≤ 1. The
characteristics of the simulations are detailed in [23].

Figure 4 shows several FDFs with extremely close means and variances. The cubes

have been chosen so that c = 0.5 ± 0.01 and σ2

s = 0.0055 ± 0.0001.

5 FDF modelling

In the framework of LES, the FDF can be estimated either by solving an equation
governing its evolution[24, 3] or by using a model for the FDF. The model proposed by
Cook and Riley[5] uses a presumed form for the FDFs. It has drawn much attention
and has been the subject of several studies[6, 25]. In this model, the main assumption
is that the FDF can be approximated by a β distribution with same mean and same
the same ﬁrst moments). The deﬁnition of the β
variance as the real FDFs (i.e.
distribution is as follows:

xa−1(1 − x)b−1
B(a, b)

,

β

x; c, σ2
s

=

(cid:0)

(cid:1)

8

(18)

Figure 3: A view of the scalar ﬁeld for a 2563 DNS. A recent injection can be seen, appearing
as a white homogeneous zone.

10

5

0

0

9

0.2

0.4

0.6

0.8

1

Figure 4: Comparisons between a β law (solid line) and several arbitrary chosen FDFs, of
same mean (c = 0.5 ± 0.01) and variance (σ2

s = 0.0055 ± 0.0001)

with

B(a, b) =

Γ(a) Γ(b)
Γ(a + b)

=

1

0

Z

xa−1(1 − x)b−1 dx

(19)

Γ(a) being the gamma function of Euler.

In order to have the right mean and variance, a and b must be chosen so that

a = c

− 1

and b =

− a.

c (1 − c)
σ2
s

a
c

(cid:19)
The presumed form for the FDF can be used in equation (13). This provides a model
for f (c) whatever f is, using c and σ2
s as variables. The estimator of f (c) can be written

(cid:18)

f (x) β

x; c, σ2
s

dx.

Z

(cid:0)

(cid:1)

(20)

Since the deﬁnition of the variables is crucial for the optimal estimators, we will pay
much attention to the often implicit choice of the fundamental variables. Here c and
σ2
s are the variables. Hence we will denote π1 =
this ﬁrst set of fundamental
variables. The choice of β distributions will be called the “β assumption”.

(cid:9)
s cannot be computed using the large eddies c only. A sub-
. a test
model is thus necessary so that the β assumption can be used. Let us denote
ﬁlter of a characteristic size twice as big as for .. Cook and Riley assume that the
subgrid variance is proportional to the quantity

The subgrid variance σ2

c, σ2
s

(cid:8)

b

α =

c2 −

2
c

.

This estimation is used to approximate the FDF and the whole model thus provides
b
an estimation of f (c) based on the variables c and α only. We will denote π2 = {c, α}
this second set of variables.

b

Another sub-model has been proposed by Pierce and Moin[26]. They assume that
σ2
s is proportional to the modulus of the gradient of the ﬁltered scalar, which we
will denote ǫc = (∂ic)2.
In the following, we will denote π2 = {c, ǫc} the variables
corresponding to this modelization.

6 Validity of the β assumption

Our purpose is to know if there is an optimal choice for the presumed form of the FDF.
Of course this optimal choice depends on the variables π used for the model. The fact
that there is an optimal choice is not obvious. The optimal estimators used in the ﬁrst
part are deﬁned in the case where a scalar quantity γ has to be estimated using π.
Here the model provides a function for each diﬀerent value of π. No relevant measure
of the error can be deﬁned in this case. But we have the relation

f (c)

π

D

E

(cid:12)
(cid:12)
(cid:12)

=

=

f (C) ds(C) dC

π

(cid:28)Z

(cid:29)
f (C) hds(C)| πi dC.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(21)

(22)

Z

10

10

5

0

0

0.2

0.4

0.6

0.8

1

Figure 5: Comparisons between hds(C)| c, σ2
(ii) c = 0.25 and σ2

s = 0.01 (iii) c = 0.1 and σ2

s = 0.01.

s i and β(C; c, σ2

s ) for (i) c = 0.5 and σ2

s = 0.0055

This means that hds| πi is the optimal estimator for the approximation of the FDF
using π since when it is used in equation (13), the estimator of f (c) which is obtained
is the optimal estimator for f (c) using π. In this particular case the optimal estimator
concept can therefore easily be extended. This quantity has already been considered in
the case where the variables are c and σ2
s . It has been called the conditional FDF[27].
It is sometimes refered to as the FPDF[28].

This optimal estimator can be computed using the following natural method : ﬁrst,
grid nodes are chosen for which π has a value very close to an arbitrary given one, then
the FDF is computed for each of these nodes, and ﬁnally, all the FDFs are averaged
to obtain the optimal estimator.

Let us consider the set of variables π1 =

, which is the case when the subgrid
variance is known. Comparisons between beta distributions and the optimal estimators
(cid:8)
are shown for three sets of values of the variables in ﬁgure 5. The cubes which are
selected for the computation of the optimal estimators are chosen so that c = 0.5± 0.01
and σ2
s = 0.01± 0.001
for the second one and c = 0.1 ± 0.01 and σ2

s = 0.0055± 0.0001 for the ﬁrst comparison, c = 0.25± 0.01 and σ2

s = 0.01 ± 0.001 for the last one.

(cid:9)

c, σ2
s

The correspondence between the β distributions and the optimal estimators is ex-
cellent. The β distributions can thus be considered as a very appropriate presumed
form for the FDF, as long as σ2
s is known. We must point out that this does not mean
that the FDFs are actually β distributions[5], as shown ﬁgure 4. We agree [5] that
the β assumption seems to be appropriate for any subvolume. Here, the size ﬁlter is
about four grid nodes and has been chosen so that all values of c, σ2
s (or α) are well
represented in the statistical sampling process.

As far as we know, no other analytic statistical distribution could be more accurate.
We think that a mathematical property of β distributions could explain these results
but we were not able to ﬁnd it.

Let us consider the case when the variables of the model are c and α. Figure 6

11

8

6

4

2

0

0

0,2

0,4

0,6

0,8

1

C

Figure 6: Comparisons between a β law (solid line) and several arbitrary chosen FDFs, for
c = 0.5 ± 0.01 and α = 0.0125 ± 0.0005.

shows examples of FDFs for cubes which present the same values of c and α. When
compared to ﬁgure 4, it is observed that there are much larger diﬀerences between the
FDFs. This is due to the fact that for given c and α, diﬀerent values of σ2
s are observed.
This is what we will call the subgrid variance dispersion. For a given π the subgrid
variance dispersion can be quantiﬁed by the irreducible error made when estimating
σ2
s using π. When for instance this error is small, the subgrid variance of cubes with
σ2
very close π values will be very close to
s

π
Figure 7, the optimal estimators are compared to a β distribution whose variance
(cid:12)
σ2
(cid:12)
s for all the FDFs). The cubes which are selected for
is
s
the computation of the optimal estimators are chosen so that c = 0.5 ± 0.01 and
α = 0.0125 ± 0.0005 for the ﬁrst case, c = 0.355 ± 0.005 and α = 0.01 ± 0.001 for the
second case, and for the last one c = 0.1 ± 0.01 and α = 0.001 ± 0.001.

(the average of σ2

c, α

(cid:12)
(cid:12)

(cid:10)

(cid:11)

(cid:11)

(cid:10)

.

In this case, the β distributions are not very close to the optimal estimators. This
is due to the variance dispersion. This means that there is a better presumed form
when α is used - closer to the optimal estimator. However, this form would be relevant
for π = {c, α} only.

When choosing a set π with less subgrid variance dispersion, the form of the optimal
estimator must tend towards the form of the optimal estimator when σ2
s is known (when
the dispersion is null). Hence, we conclude that the β assumption is better when the
subgrid variance dispersion is small.

Now that we have established that, when σ2

s is unknown, β distributions are not as
clearly appropriate as when the subgrid variance is known, the question is : is the error
due to the diﬀerence between the optimal estimator of the FDF and the β distribution
a signiﬁcant one ?

Using optimal estimators, it is possible to compute the supplementary error brought
by the β assumption alone for the estimation of f (c). This must be done for a given

12

4,5

3,5

5

4

3

2

2,5

1,5

1

0,5

0

0

0,2

0,4

0,6

0,8

1

C

Figure 7: Comparisons between hds(C)| c, αi and β(C; c, hσ2

s | c, αi)

f .

Let us consider the following estimator for f (c) using π :

gf (π) =

f (C) β

C; c,

σ2
s

π

dC.

(23)

Z

(cid:0)
It is obtained by replacing ds in (13) by a β distribution. The variance of the β
distribution is the optimal estimator of σ2
s using π. The error made by this estimator
. The supplementary error
can be compared to the irreducible error made by
made by the estimator (23) reﬂects the fact that β distributions are not exactly the
optimal estimators as shown ﬁgure 7 (or ﬁgure 5 even if the diﬀerence is slight).

f (c)

(cid:11)(cid:1)

D

E

π

(cid:10)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

We will now present results that have been obtained using histograms. All the
quadratic errors have been normalized by the variance of the quantity which is esti-
mated. This allows to compare two errors even if the quantity to estimate is not the
same.

The results concerning the estimation of σ2

s are presented in table 1 concerning
the estimation of σ2
s . The irreducible error that is computed here reﬂects the subgrid
variance dispersion. This dispersion is smaller when the variables are c and ǫc than
when the variables are c and α.

Since the error brought by the β assumption must be computed for a given f we
have chosen to do so for several β distributions with diﬀerent means and variances.
This is a very plausible choice[26]. The comparison between the optimal estimators
and the estimator gf (π) is presented in table 2. Column one is the error made by the
optimal estimators, i.e. the irreducible error. Column two is the error made by gf (π),
i.e. when using the β assumption.

It can be observed that the supplementary error due to the β assumption is always
much smaller than the irreducible error. In most cases, it is one order of magnitude
smaller. This conﬁrms the previous results on the optimal estimators of the FDF.

13

Estimator Normalized error
hσ2
s | c, ǫci
hσ2
s | c, αi
σ2
s = κ α

0.215
0.259
0.359

Table 1: Normalized quadratic errors for several optimal estimators of σ2
s (the optimal
estimators are computed using the histogram technique). The constant κ is chosen so that
the error made by the Cook and Riley estimator of σ2

s is minimized.

The fact that the β distributions are not very close to the optimal estimators of
the FDFs has not a measurable incidence on the supplementary error. The latter can
often be neglected and there is no need to search for a more relevant presumed form
for the FDF.

Since the irreducible errors are normalized, they can be compared. The conclusion is
that σ2
s is a quantity which is very diﬃcult to estimate. The quantity f (c) is in general
easier to estimate. In addition, the better σ2
s is estimated using a set of variables, the
better the f (c) are estimated. For a few variables, the relative relevancy of a set does
not depend on f .

7 Neural networks

As already underlined[5], the estimation of the subgrid variance σ2
s is the main problem
in the presumed FDF approach. Rather complex models have been proposed[6] using
dynamic approach and many new variables. Table 3 shows the irreducible error for
diﬀerent sets of variables computed using neural networks. One of these variables (c)
has often been neglected in the diﬀerent models proposed. The results shown here
suggest this is a mistake. On the contrary, the results show that the dissipation ǫ does
not bring much information about σ2
s .

In addition, they show that the more variables are included in π, the better the
estimation of the subgrid quantities - with errors much smaller than with usual models.
Neural network can thus be considered as a new way towards optimal LES in the sense
of Langford and Moser[1]. We think that they could be used directly for the modelling
of unknown terms in LES and that they present some advantages : they do not need
much data to estimate the conditional expectations correctly and they can reproduce
highly non linear behaviours. It is possible to take physics directly into account when
choosing the variables (galilean invariance[1], scale invariance or dimensional analysis
arguments).

8 Conclusion

In agreement with Langford and Moser[1], we have used and explored the idea that
in the framework of LES, once a set of variables has been chosen to estimate a given

14

Variables (π) Error of

f (c)

π

Error of gf (π)

E

(cid:12)
(cid:12)
(cid:12)

0.031
0.055
0.072

D
Mean 0.35, variance 0.01
c, σ2
0.043
s
c,ǫc
0.095
c,α
0.136
Mean 0.15, variance 0.01
c, σ2
s
c,ǫc
c,α
Mean 0.5, variance 0.01
c, σ2
s
c,ǫc
c,α
Mean 0.5, variance 0.036
c, σ2
s
c,ǫc
c,α

0.041
0.092
0.130

0.011
0.064
0.079

0.051
0.099
0.140

0.042
0.070
0.091

0.063
0.107
0.146

0.013
0.066
0.081

Variables

Irreducible error

c, α
c, ǫc
c, α, ǫc
c, α, ǫ
ǫc
c,
α, ǫc, ǫ
c, α, ǫc, ǫ
c
b
ǫc
c, α, ǫc,
b
b
ǫc
c,
c, α, ǫc, ǫ
b
b

0,259
0,215
0,192
0,258
0,173
0,158
0,152
0,137

15

Table 2: Normalized quadratic errors of diﬀerent estimators for the estimation of f (c). The
mean and the variance of the diﬀerent f that have been chosen are speciﬁed.

Table 3: Irreducible error linked to diﬀerent parameter sets. 643 examples are used for eval-
uating the weights of the neural networks and (1283 − 643) examples are used for estimating
the irreducible error.

b

subgrid term, there is only one optimal estimator. Some of the properties of optimal
estimators that are of interest for LES have been presented throughout this article. We
have shown how optimal estimators could be computed using simple techniques if the
number of variables is small, or neural networks if the number of variables is greater
than 3.

As an illustration, the concept of optimal estimator was applied to the Cook and Ri-
ley subgrid model[5] for the scalar ﬂuctuation, a model which is widely used and whose
basis has already retained much attention in the literature. The concept of optimal
estimator has been extended to the approximation of FDF and the main conclusion is
that the β assumption is very appropriate. When the error directly associated with the
β assumption for the estimation of a given quantity is compared with the irreducible
error, it is found to be small. The largest error are associated with the estimation of
the subgrid variance of the scalar ﬂuctuations. That is the very point on which the
Cook and Riley model needs to be improved.

In the paper we did not attempt to propose any new practical model for this scalar
variance, but we used neural networks to see how closely the subgrid variance could
be estimated. The error made by the optimal estimator is smaller when the number of
relevant variables is increased.

The optimal estimation technique provides a way of assesssing which set of param-
eters will potentially lead to the most accurate subgrid model. It does not provide any
information on how to specify the formulation of the model, but simply indicates if
eﬀorts for building a model with a given set of parameters are likely to be fruitful.

As the number of retained parameters is increased, it can become more and more
diﬃcult to propose a model formulation on the ground of physical reasoning, and this
might suggest using the optimal estimator directly instead of a model. Then, neural
networks would be used directly - instead of a modelization. This “NN guided LES”
could constitute an optimal LES as deﬁned by[1, 29]. We did not perform such a
simulation in the present paper. This could be the subject of a future study (following
the path explored by [29]).

This paper is an illustration of the relevance of optimal estimation techniques for the
problem of subgrid modeling for LES. These techniques allow a thorough exploration
of the behaviours of models in LES indicating the points which have to be improved
in the models.

A Orthogonality relation

Let us give a short demonstration of (2). The quadratic error made by an estimator
g(π) can be developed :

(γ − g(π))2

=

D

E

(γ − hγ| πi)2
E
D
+2 h(γ − hγ| πi) ( hγ| πi − g(π))i

( hγ| πi − g(π))2
D

+

E

16

Now, we have to show that the last term is null. It can be written

2

(γ − hγ| πi) ( hγ| πi − g(π)) p(γ, π) dγ dπ

Z

Z

= 2

( hγ| πi − g(π)) p(π)

(γ − hγ| πi) p(γ|π) dγ

dπ

(cid:18)Z

(cid:19)

γp(γ|π) dγ, the previous quantity is null and the orthogonality relation

Since hγ| πi =
holds.

R

B Successive conditioning

Let g(π) be an estimator of γ using π. We will note p(g(π) = g) or p(g) the PDF that
g(π) = g. Then we have

hγ| g(π) = gi p(g(π) = g) =

γ p(γ, g) dγ

And if g(π) = hγ| πi then

hγ| hγ| πi = gi p(g) =

hγ| πi p(π) δ( hγ| πi − g) dπ

=

=

=

=

Z

Z

Z

Z

γ p(γ, π, g) dγ dπ

γ p(γ, π) δ(g(π) − g) dγ dπ

γ p(γ|π) dγ

p(π) δ(g(π) − g) dπ

Z (cid:18)Z

(cid:19)

hγ| πi p(π) δ(g(π) − g) dπ.

p(π) δ( hγ| πi − g) dπ

Z
= g

Z

Z

= g p(g)

= g

p(π, g) dπ

Hence, hγ| g(π) = gi = g, which can be written

hγ| hγ| πii = hγ| πi .

C Filtering

The ﬁltering can be written

c(~r) =

G(~r − ~x) c(~x) d~x

Z

17

or c = G ∗ c. Let us assume that we have 0 ≤ c ≤ 1 and that we want

If G(~x) ≥ 0 ∀~x we can write

0 ≤ c ≤ 1.

0 ≤ c ≤ 1.

If G is not positive, the inequality can be false. Thus it is necessary that G should be
positive. In order to have (24) veriﬁed, G must have the property 1 = 1, which can be
written

G = 1.

R

Let us now deﬁne VD(C) = H(C − c(~x)). The physical meaning of this quantity is
not as clear as in the case of the box ﬁlter. Anyway, the FDF is still the derivative of
VD. We can write

VD(a) − VD(b) = H(a − c) − H(b − c).

If a ≥ b then H(a − c(~x) − H(b − c(~x)) ≥ 0. If the ﬁlter is positive, then VD(a) ≥ VD(b)
and VD is a growing function. If G is not positive, this is not granted. Since the FDF
is the derivative of VD, it is positive only if the ﬁlter is positive. Finally, we have
VD(1) = 1 =

ds, so that the FDF is normalized only if

G = 1.

R

The ﬁlters are generally chosen so that

G = 1. The previous arguments show that
they should be chosen positive, too. The box ﬁlter has been chosen for historical[5, 6]
R
and clarity reasons. But it is an invertible ﬁlter, whereas the ﬁlters that should be
considered in the framework of LES should be non-invertible[1]. All the non-invertible
ﬁlters that have been proposed are not positive. We propose here a new ﬁlter that
could be more relevant. It is deﬁned by

R

(24)

(25)

G(~x) =

2
π ℓ

sinc2 x
ℓ

.

3

i=1
Y

This ﬁlter is non-invertible (since the Fourier Transform of sinc2 is a triangle function),
normalized and positive. We think that the choice of a particular ﬁlter has not much
importance while the error made by the estimators is large. For very small errors, a
diﬀerence could probably be found between the error made by the estimators when
using an invertible ﬁlter and the error when using a non-invertible ﬁlter. This could
probably be seen using neural networks when computing the irreducible error.

References

[1] J.A. Langford and R.D. Moser. Optimal LES formulations for isotropic turbulence.

J. Fluid Mech., 398:321–346, 1999.

[2] U. Schumann. Subgrid scale model for ﬁnite diﬀerence simulations of turbulent
ﬂows in plane channels and annuli, Journal of Computational Physics, 18:376-404,
1975.

[3] P.J. Colucci, F.A. Jaberi, P. Givi, and S.B. Pope. Filtered density function for
large eddy simulation of turbulent reacting ﬂows. Phys. Fluids, 10 (2):499–515,
1998.

18

[4] J.P. Bertoglio, A stochastic subgrid model for sheared turbulence. Macroscopic
modelling of turbulent ﬂows, Lecture Notes in Physics, Berlin and New York,
Springer-Verlag, p. 100-119, 1985.

[5] A.W. Cook, J.J. Riley, A subgrid model for equilibrium chemistry in turbulent

ﬂows, Phys. Fluids 6 (8) (1994) 2868-2870

[6] C. Wall, B.J. Boersma, and P. Moin. An evaluation of the assumed beta probabil-
ity density function subgrid-scale model for large eddy simulation of nonpremixed
turbulent combustion with heat release. Phys. Fluids, 12 (10):2522–2529, 2000.

[7] R. Deutsch. Estimation theory, Prentice Hall, 1965.

[8] L. Gorden and R. Olshen. Asymptotically eﬃcient solutions to the classiﬁcation

problem. Ann. Statist. vol. 6, pp. 515-533 (27), 1978.

[9] C. Stone. Consistent nonparametric regression. Annals of Statistics, 8:1348–1360,

1977.

1995.

[10] I. Steinwart. Consistency of support vector machines and other regularized kernel
classiﬁers Information Theory, IEEE Transactions on Volume 51, Issue 1, pp128
- 142 , 2005.

[11] V.N. Vapnik and A.Ya. Chervonenkis. Theory of Pattern Recognition (in Russian).

Nauka, Moscow, 1974.

[12] C.M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press,

[13] H. White. Connectionist Nonparametric Regression: Multilayer Feedforward Net-
works Can Learn Arbitrary Mappings, Neural Networks, 3, 535-550, 1990.

[14] D. Rumelhart, G. Hinton, and R. Williams. Learning internal representations by
error propagation. Neurocomputing, J. Anderson and E. Rosenfeld eds., 675-695.
Cambridge (MA), MIT Press, 1988.

[15] S. Haykin, J. Principle. Using Neural Networks to Dynamically model Chaotic
events such as sea clutter; making sense of a complex world. IEEE Signal Pro-
cessing Magazine 66:81, 1998.

[16] J.-M. Friedt, O. Teytaud and M. Planat. Learning from Noise in Chua’s Oscillator.
Proceedings of the 16th International Conference on Noise in Physical Systems and
1/fFluctuations, pp 491-496, 2001.

[17] C. Lee, J. Kim, and D. Babcok. Application of neural networks for turbulence

control for drag reduction. Phys. Fluids, 9 (6):1740–1747, 1997.

[18] S. Mukherjee, E. Osuna, F. Girosi, Nonlinear prediction of chaotic time series
in Proc.of IEEE NNSP 97, Amelia Island, FL,

using support vector machines,
1997.

[19] J.-M. Friedt, O. Teytaud, D. Gillet and M. Planat. Simultaneous amplitude and
frequency noise analysis in Chua’s circuit - neural network based prediction and
analysis. In Proc. of VIII Van Der Ziel Symposium on Quantum 1/f Noise and
OtherLow Frequency Fluctuations in Electronic Devices, St Louis , to appear,
2001.

19

[20] H. Ali and Y. Najjar, Neuronet - Based Approach for Assessing the Liquefaction
Potential of Soils. Transportation Research Records, No. 1633, pp. 3 -8, 1999.

[21] , J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor and J. Vandewalle,
Least Squares Support Vector Machines. World Scientiﬁc, Singapore, 2002.

[22] S.B. Pope, Pdf methods for turbulent reactive ﬂows, Prog. Energy Combust. Sci.

11 (1985) 119-192

[23] M. Elmo, A. Moreau, J.P. Bertoglio, V.A. Sabel’nikov, Mixing in isotropic tur-
bulence with scalar injection and applications to subgrid modeling, Flow, Turb.
Combust. 65 (2000) 113-131

[24] F. Gao and E.E. O’Brien. A large-eddy simulation scheme for turbulent reacting

ﬂows. Phys. Fluids A, 5 (6):1282–1284, 1993.

[25] J. Jimenez, A. Li˜nan, M.M. Rogers, and F.J. Higuera. A priori testing of subgrid
models for chemically reacting non-premixed turbulent shear ﬂows. J. Fluid Mech,
349, 149-171, 1997.

[26] C.D. Pierce and P. Moin. A dynamic model for subgrid-scale variance and dissi-

pation rate of a conserved scalar. Phys. Fluids, 10 (12):3041–3044, 1998.

[27] C. Tong, Measurements of conserved scalar ﬁltered density function in a turbulent

[28] H. Pitsch, Large-Eddy Simulation of Turbulent Combustion, Annu. Rev. Fluid

jet, Phys. Fluids, 13:2923, 2001.

Mech., 38:453-82, 2006.

[29] S. V¨olker, R.D. Moser and P. Venugopal, Optimal large eddy simulation of tur-
bulent channel ﬂow based on direct numerical simulation statistical data Phys.
Fluids 14:3675, 2002.

20


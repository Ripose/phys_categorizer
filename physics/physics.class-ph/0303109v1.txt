3
0
0
2
 
r
a

M
 
6
2
 
 
]
h
p
-
s
s
a
l
c
.
s
c
i
s
y
h
p
[
 
 
1
v
9
0
1
3
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Information capacity of optical ﬁber channels with zero average dispersion

K. S. Turitsyn1, S. A. Derevyanko2,3, I. V. Yurkevich4, S. K. Turitsyn2

1Landau Inst.
for Theor. Physics, Moscow, Kosygina 2, 117940, Russia
2Photonics Research Group, Aston University, Birmingham B4 7ET, UK
3Institute for Radiophysics and Electronics, Kharkov 61085, Ukraine
4The Birmingham University, Birmingham B15 2TT, UK

We study the statistics of optical data transmission in a noisy nonlinear ﬁber channel with a weak
dispersion management and zero average dispersion. Applying path integral methods we have found
exactly the probability density functions of channel output both for a non-linear noisy channel and
for a linear channel with additive and multiplicative noise. We have obtained analytically a lower
bound estimate for the Shannon capacity of considered nonlinear ﬁber channel.

PACS numbers: 42.81.-i, 42.65.-k, 42.79.Sz, 05.10.Gg

Introduction The classical theorem of information the-
ory [1] states that the capacity of a power-constrained
transmission in an additive Gaussian noise channel
growths logarithmically with increase of the signal to
noise ratio (SNR). Thus, an improvement of the capacity
(maximum average information per symbol that can be
transmitted through the channel) in such systems can
be achieved by increase of the signal power assuming
that the noise level is not aﬀected. The Gaussian statis-
tics of noise is a fundamental assumption in derivation
of this widely known Shannon’s result. Properties and
applications of bandlimited linear channels with addi-
tive white Gaussian noise (AWGN) form a foundation
of modern information theory. It should be emphasized
that the AWGN linear channel model is not just a sim-
ple mathematical construction, but is applied directly to
many practical problems such as, for instance, deep-space
communication. However, in some applications, nonlin-
ear response of a transmission medium must be taken
into account. Evidently, properties of nonlinear informa-
tion channel can be signiﬁcantly diﬀerent from that for
AWGN models. Interaction of noise and signal in non-
linear transmission channel can result in non-Gaussian
statistics of received signals. The theory of non-Gaussian
information channels though being an evident challenge
for many decades is not yet well established compared to
the success of AWGN models. Studies in this fundamen-
tal research area are additionally motivated by practical
technological achievements and growing demand for ef-
ﬁcient high-speed, high quality communications. Recent
progress in ﬁber optics attracts much fresh interest to the
information theory of non-Gaussian nonlinear communi-
cation channels [2]-[7]. Optical ﬁber waveguides made of
silica present low loss, ultra-high capacity, cost-eﬃcient
transmission media with many attractive features. Us-
ing optical ampliﬁers to recover signal power simultane-
ously at many carrier frequencies (channels) within the
ﬁber bandwidth it is possible to transmit optical infor-
mation data over thousands of kilometres. It is already
well recognized, however, that the nonlinear response of
the transmission medium (Kerr eﬀect nonlinearity) plays

a crucial role in limiting the aggregate capacity of opti-
cal ﬁber systems. Accumulation of nonlinear interactions
with propagation along the transmission line makes ﬁber
information channels essentially nonlinear. Evidently,
nonlinear impairments (or in other words, a level of sig-
nal corruption due to nonlinearity) depend on the signal
power. Therefore, in nonlinear channels an increase of
the signal power would not necessarily improve the sys-
tem capacity. Recently, in their pioneering work Mitra
and Stark suggested that from the information theory
perspective under certain conditions one can treat essen-
tially nonlinear noisy channels as linear ones with eﬀec-
tive multiplicative noise [2]. Applying this idea to multi-
channel optical ﬁber transmission systems they derived
a heuristic linear model with multiplicative noise that
presumably approximates some features of the original
nonlinear channel. Though a connection between statis-
tical properties of such an eﬀective ”nonlinear noise” and
system/signal characteristics is still a subject of further
research and justiﬁcation, this intuitive approach outlines
a possible way to treat nonlinear transmission channels.
In order to compute the Shannon capacity it is neces-
sary to make one more step beyond determination of a
conditional probability. Namely, one has to ﬁnd the opti-
mal input signal statistics (that is even more complicated
functional problem). The channel capacity
deﬁned by
Shannon is a maximum of the following functional (called
mutual information) with respect to the statistics of in-
put signal X, given by distribution function p(X):

C

= max

C

p(X) Z D

D

X

YP (X, Y ) log2

P (X, Y )
Pout(Y )p(X)

.

(1)

D

XP (Y

X) p(X) is the joint distribu-
Here P (X, Y ) = P (Y
|
tion function of input X and output Y ; Pout(Y ) =
X)p(X), and all speciﬁc properties of a com-
|
munication channel are given by the conditional prob-
R
ability P (Y
X). To the best of our knowledge the only
|
case for which there exists an explicit analytical solution
of the corresponding functional optimization problem is
when the joint distribution of input and output signals
are Gaussian. In this case the Shannon capacity can be

explicitly expressed through an input-output pair corre-
lation matrix introduced by Pinsker [8]. Diﬃculties in
the analysis of non-Gaussian nonlinear channels to some
extent are caused by a relatively limited number of ap-
propriate mathematical methods. Therefore, to practi-
cally estimate the capacity of the nonlinear ﬁber channel
most authors [5]-[7] apply the Pinsker formula that for
the Gaussian noise coincides with Shannon’s deﬁnition,
but as a matter of fact gives only the lower bound on
the capacity [2]. An interesting open problem is capabil-
ity of the Pinsker formula to mimic the true behavior of
the capacity of nonlinear information channels especially
in the case of large input signal power. Computation of
Shannon capacity for any realistic optical ampliﬁer trans-
mission system is a very complicated problem which is
unlikely to be solved analytically. Therefore, it is of cru-
cial importance for further progress in this area to ﬁnd
basic simpliﬁed models of ﬁber nonlinear channels that
can be treated analytically. Such solvable models can
provide guidance to analysis of much more complicated
general problems in the information theory of nonlinear
ﬁber channels.

In this Letter we present a theoretical analysis of a
physical model which describes the transmission of light
signals in a noisy nonlinear ﬁber channel with zero aver-
age dispersion. To examine the similarity and diﬀerence
between the eﬀects of nonlinearity and multiplicative
noise, in parallel, we study a linear model of the channel
with both additive and multiplicative noise. We calcu-
late analytically the probability density function (PDF)
of the channel output for both models. Using our derived
conditional probabilities we analyze the capacity of cor-
responding transmission systems. We compare here two
approaches to the estimation of system capacity: ﬁrst,
based on Pinsker’s formula for input-output correlation
matrix and, second, directly applying Shannon’s deﬁni-
tion of the capacity.

The average propagation of a complex light envelope
E(z, t) in a noisy optical ﬁbre line with the so-called weak
[9], [10]) in
dispersion management (see for details e.g.
the main order is described by the stochastic nonlinear
Schr¨odinger equation:

∂E
∂z

= i

< d >
2

∂2E
∂t2 + i

E
|

2E + n.
|

(2)

Here n(z, t) is an additive complex white noise with zero
mean and correlation function (see for notations [9])

< n(z, t)n∗(z′, t′) >=< n0 > δ(z

z′)δ(t

t′).

(3)

−

−

In the present Letter we restrict consideration to the case
of weakly dispersion-managed ﬁber systems with zero av-
erage dispersion < d >= 0. The propagation equation
then is eﬀectively reduced to the Langevin equation for
E(z, 0) with the regularized
the regularized ﬁeld u(z)

≡

2

noise η(z)

n(z, 0).

≡

du
dz −

i

u
|

2 u = η,
|

u(z = 0) = u0,

(4)

Here η(z) is a white noise with zero mean and correlation
function < η(z)η∗(z′) >= Dδ(z
z′), where D = 2W <
n0 > is the regularized noise intensity. To restore the
capacity for a bandwidth limited signal one has simply
to multiply all the corresponding results by the channel
bandwidth W .

−

Calculation of a conditional probability Some statisti-
cal properties of system (4) including higher-order mo-
menta have been studied by Mecozzi [11]. However, the
method used in [11] did not permit explicit computation
of the PDF which is required in the analysis of system ca-
pacity. Therefore, to calculate the conditional probabil-
ity P (u, z
u0) we apply here the so-called Martin-Siggia-
|
Rose formalism [12] that presents the conditional PDF
of the output as the following functional integral:

q(z)=u

−

dz

′

L[q(z

′

)]

z

R0

,

(5)

P (u, z

u0) =
|

q e

D

Z
q(0)=u0

i

−

q
|

q′
|

2 q
|

[q] =
where the eﬀective Lagrangian is deﬁned as
L
2. Integral (5) can be calculated analyt-
(2D)−1
|
2]
˜q(z′)
ically. The substitution q(z) = ˜q(z) exp[i
|
|
brings Lagrangian to its free form. The Jacobian of this
transform is unity and in the new variables the integral
becomes Gaussian. After simple straightforward algebra
it can be reduced to

0 dz′
R

z

P (u, z

u0) =
|

+∞

Xm=−∞

eimφ

Z

dφ′
2π

′

′

e−imφ

P

(r, φ′, z

r0, φ0)
|

(6)

where the auxiliary “partition function” is

′

P

(r, φ′, z

r0, φ0)
|

q(z)=r eiφ

′

≡

Z
q(0)=r0 eiφ0

z

R0

q e

D

−

′

dz

{i m |q|2+ 1

2D |q

′

|2}

(7)
(here u = reiφ, u0 = r0eiφ0). The eﬀective action de-
composes into sum of the classical part and a ﬂuctuating
part that does not depend on the limits. The ﬂuctuating
ﬁeld is calculated by expanding over the complete set of
eigenfunctions of the operator
m satisfying zero
boundary conditions at z′ = 0 and z′ = z. Omitting de-
tails of these operations we present a ﬁnal expression for
the conditional probability of our nonlinear channel:

z + k2
∂2

−

P (u, z

u0) =
|

1
2π

+∞

Xm=−∞

eim(φ−φ0) Pm(r, z

r0)
|

=

1
2πD

Xm=−∞

+∞

eim(φ−φ0) e−

2D km coth kmz

r2 +r2
0

sinh kmz

km I|m|(qm) ,

(8)

here qm = kmr r0/(D sinh(kmz)), km = √2imD and I|m|
is the modiﬁed Bessel function.

Next we establish an analogy between the considered
nonlinear channel (NLCH) and a linear channel with mul-
tiplicative noise (LMNCH):

u′

−

iv u = η,

u(z = 0) = u0,
< η∗(z′)η(z) >= Dδ(z
< v(z)v(z′) >= D′δ(z

z′),
z′)

−
−

(9)

(10)
(11)

Applying a similar procedure to above we derive the con-
ditional probability function of the form Eq.(8) with re-
placement Pm →

˜Pm where

˜Pm(r, z

r0) =
|

1
Dz

′

e−m2D

z/2 I|m|

rr0
Dz (cid:17)

(cid:16)

r2 +r2
0
2Dz .

e−

(12)

Note that if the information is transmited using only
signal power (the so-called intensity modulatuion - di-
the conditional probabil-
u
rect detection systems) r =
|
ity takes the form (after integration in polar coordinates
(r, φ) over phase φ:

dφP (u, z

|

u0) = P0(r, z
|

r0)):
|

P0(r, z

r0) =
|

r0) =
|

1
Dz

I0

r r0
zD (cid:17)

(cid:16)

r2 +r2
0
2 z D . (13)

e−

R
P0(r, z

e

Note that in both cases (nonlinear and eﬀective multi-
plicative noise channels) formulae (8) and (12) yield the
same result.

Channel capacity First we revise the procedure com-
monly used in the recent literature for the channel ca-
pacity estimation. We demonstrate here that the consid-
eration based on pair correlation functions [8] can lead
to results very diﬀerent from the Shannon capacity and,
therefore, should be used with caution. Some authors
[5, 6] instead of using the original Shannon deﬁnition cal-
culate capacity by exploiting a simpler Pinsker formula
based on a complex self-conjugate input-output correla-
tion matrix Cαβ :

log2

CG ≡

Det diag(Cαβ)
Det Cαβ

, Cαβ ≡

< uαu∗

β >

(14)

Here indices α, β = input, output; and brackets stand
for the average over noise (η for non-linear problem and
η and v for double noise model) and over statistics of
the input signal u0 (which is assumed to be Gaussian).
Deﬁned in this way the Gaussian capacity
CG coincides
with the Shannon capacity for the case of Gaussian joint
input-output distributions which corresponds to the lin-
ear channel with additive noise [8]. For nonlinear chan-
nels or channels with multiplicative noise the Gaussian
capacity (14) represents the lower estimate for the true
Shannon capacity

(see [2]).

We start from the calculation of the correlation matrix.
To perform noise averages we use either PDF (8) or (12).
S , Cout,out =
It is easy to ﬁnd that Cin,in =

2

C

u0|

D|

E ≡

3

2
u(z)
|

= (S + N ), N

2D z regardless the model.

E

D|
However, the cross-correlations Cin,out =
diﬀerent

≡

u0 u∗(z)
i
h

are

Cin,out = 




S sech2k1z

(1+(S/N )k1z tanh k1z)2 , NLCH

′

S e−D

z/2, LMNCH

(15)






where k1 = √2iD. Note that SNR = S/N = s changes
only due to variation of S, while N = 2Dz is ﬁxed as we
consider here a ﬁxed transmission distance. Substitution
of the correlation matrix into the deﬁnition Eq.(14) yields
the ﬁnal result

CG = log2 (cid:20)

1 +

where

s

4
1 + s b(z))
(1 + s) a(z)
|
|

−

s (cid:21)

,

(16)

a(z) =

cosh k1z
z,
eD

′

4, NLCH
|
LMNCH

|

(cid:26)

b(z) =

k1z tanh k1z, NLCH
LMNCH

0,

(cid:26)

(17)

(18)

CG de-
It is seen from Eq. (16) that with increase of SNR
cays to zero for the case of the nonlinear channel (simular
to conclusions made in [5, 6]) and tends to a constant for
the case of the multiplicative noise channel. However,
below we will show that in both cases the true Shannon
capacity
is unbounded and grows logarithmically with
increase of S/N similar to the linear channel.

C

Direct estimate of the Shannon capacity Following
de-
Shannon [1] we consider now the channel capacity
ﬁned as a maximum of the mutual information with re-
spect to the statistics of input, u0, given by distribution
function p(u0) under the ﬁxed average input power S:

C

= max

p(u0) Z

C

d2ud2u0P (u, u0) log2

P (u, u0)
Pout(u)p(u0)

.

(19)

R

d2u0P (u

u0) connecting output
The conditional probability P (u
|
and input probabilities: Pout(u) =
u0)p(u0)
|
is given either by (8) or (12). Note that the Shannon
deﬁnition allows one to obtain directly an estimate of
capacity. Any arbitrary trial distribution p(u0) provides
for a certain low boundary estimate of the capacity
.
C
The closer a trial function is to the optimal distribution of
p(u0) the better is our approximation of the true capacity.
Applying the so-called Klein inequality for two arbitrary
probability distribution functions P and

d2ud2u0P (u, u0) log2

Z

0

(20)

P
P (u, u0)

(u, u0) ≥

P

we obtain the following chain of inequalities:

d2ud2u0 P (u, u0) log2

C ≥ Z

≥ Z

P (u, u0)
Pout(u)p(u0)
(u, u0)
Pout(u)p(u0)

d2ud2u0 P (u, u0) log2 P

(21)

P

where
is an arbitrary PDF (by this we mean that it is
non-negative and normalized) and p(u0) is an arbitrary
(not optimal) initial signal distribution. Next we exploit
and p(u0) in (21) by chosing p(u0) =
an arbitrariness of
(2π)−1p(r0),
r0) p(r0). Here we
|
assumed that both an input distribution p(u0) and
are
r0) is the radial conditional
phase independent and P0(r
|
probability given by Eq.(13). Substitution of these trial
functions into inequality Eq.(21) brings it to the form

(u, u0) = (2π)−2P0(r

P

P

P

r0)
P0(r
|
r′)p(r′)
dr′r′P0(r
|

.

C ≥ Z

drdr0 r r0P0(r

r0) p(r0) log2
|

R

(22)
Evaluation of the r.h.s. of this inequality for any trial
function leads to an estimate of a lower bound for the
Shannon capacity. Substituting P (r, z
r0) from (13), and
|
considering a Gaussian trial function for input statistics
r2
p(r0) = (2/S) exp(
0/S), after simple algebra we ob-
tain:

−

C ≥ C0(s) = ln(1 + s)
−
dx xK0(x

F1(s) = s−1

∞

Z
0

p

2s + F1(s)

(23)

1 + s−1)I0(x) ln I0(x)

where I0 and K0 are modiﬁed Bessel functions and s =
S/N is the SNR. Then the main contribution from the in-
tegral F1(s) to the asymptotic behavior of
C0(s) for large
1. Using the asymptotic
s comes from the region x
expansion of modiﬁed Bessel functions we get

≫

This proves that
are both unbounded as S/N

1
2

ln s + O(1).

C ≥
C0 and hence the Shannon capacity
.
→ ∞

C

→ ∞

Our result in particular shows that a naive straight-
forward application of the Pinsker formula for evaluation
of the capacity of a nonlinear channel as, for instance, in
[5, 6] can lead to wrong conclusions regarding the asymp-
totic behavior of the capacity with S/N
. Note
that for the speciﬁc problem considered here it is pos-
sible to modify the deﬁnition of Cαβ to obtain correct
asymptotics for capacity using input-output correlation
CG constructed with cor-
matrix. Indeed, calculation of
r2
= S F2(s),
r r0i
relators
0
0 dx x2 I0(x) K0(x√1 + s−1), leads to
F2(s) = (2s2)−1
(cid:11)
(cid:10)
F2(s)]). Taking into account
CG = ln(1 + s)/(1 + s [1
R
that F2(s
1 one can see that it gives the correct
)
→ ∞
asymptotic behavior for capacity. Unfortunately, there is
no general recipe for choosing the correct correlators in
the Pinsker formula.

= S + N,

= S,
∞

r
h

→

−

i

h

Discussion and conclusions We have examined the
statistics of optical data transmission in a noisy nonlin-
ear ﬁber channel with a weak dispersion management
and zero average dispersion. We have also studied simi-
larity and diﬀerence between eﬀects of nonlinearity and

4

multiplicative noise, considering in parallel a linear chan-
nel with multiplicative (and additive) noise. Using an-
alytically calculated conditional PDF we analyzed the
Shannon transmission capacity for both models. We did
manage to ﬁnd analytically a lower bound estimate for
the Shannon capacity of the nonlinear ﬁber channel con-
sidered here. We revise the Pinsker formula which has
been used without justiﬁcation in some recent works and
show that the Gaussian capacity deﬁned through the pair
correlation functions should be used with caution in the
case of nonlinear transmission channels. To incorporate
the optimization procedure inherent for the Shannon def-
inition one needs to elaborate all possible correlators and
ﬁnd those which are essential, i. e. are much greater
than others. Those correlators may then be used in the
Pinsker formula to provide a simple and tractable expres-
sion for the channel capacity. That would not be neces-
sary if the Shannon deﬁnition could be worked out. Un-
fortunately, it is hardly the case for any more or less prac-
tical problem of interest. Another important result of our
analysis is that nonlinearity and multiplicative noise do
not necessarily degrade input-output correlations in the
same way. Therefore, relating the nonlinear problem to
a linear one with multiplicative noise has to be carefully
justiﬁed for each speciﬁc transmisison system model.

This work was supported by INTAS Young Scientist
Fellowship No YS 2002 - 165 (S.D.) and by the Liver-
hulme Trust project A/20010049 (S.D., S.K.T.). I.V.Y.
gratefully acknowledges support by the Leverhulme Trust
under the contract F/94/BY and by the EPSRC grant
GR/R95432.

[1] C.E. Shannon, Bell Syst. Tech. J., 27, 379 (1948)
[2] P. Mitra and J. Stark, Nature, 411, 1027 (2001)
[3] A. Mecozzi, and M. Shtaif, IEEE PTL, 13(9), 1029

[4] E. Desurvire, Optics Letters, 25(10), 701 (2000)
[5] J. Tang, Journal of Lightwave Technology, 19, 1104

[6] J. Tang, Journal of Lightwave Technology, 19, 1110

(2001)

(2001)

(2001)

[7] A. Green, P. Littlewood, P. Mitra, and L. Wegener, Phys.

Rev. E , 66, 46627 (2002)

[8] M.S. Pinsker, Information and Information Stability of
Random Variables and Processes, (Holden Day, 1964)
[9] E. Iannoe, F. Matera, A. Mecozzi, and M. Settembre,
Nonlinear Optical Communication Networks (John Wi-
ley & Sons, 1998)

[10] S. K. Turitsyn, S. B. Medvedev, M. P. Fedoruk, and E.

G. Turitsyna, Phys. Rev. E 61, 3127 (2000)

[11] A. Mecozzi, JOSA B, 11, 462, (1994)
[12] J. Zinn-Justin, Quantum Field Theory and Critical Phe-

nomena, (Oxford University Press, 2002)


3
0
0
2
 
t
c
O
 
3
2
 
 
]
h
p
-
s
s
a
l
c
.
s
c
i
s
y
h
p
[
 
 
1
v
7
1
1
0
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Additive Entropies of degree-q and the Tsallis
Entropy

B. H. Lavenda1 and J. Dunning-Davies2
1Universit`a degli Studi Camerino 62032 (MC) Italy;
email: bernard.lavenda@unicam.it
2 Department of Physics, University of Hull, Hull HU6 7RX
England; email: j.dunning-davies@hull.ac.uk

Abstract

The Tsallis entropy is shown to be an additive entropy of degree-q that
information scientists have been using for almost forty years. Neither is
it a unique solution to the nonadditive functional equation from which
random entropies are derived. Notions of additivity, extensivity and ho-
mogeneity are clariﬁed. The relation between mean code lengths in coding
theory and various expressions for average entropies is discussed.

1 The ‘Tsallis’ Entropy

In 1988 Tsallis [1]published a much quoted paper containing an expression
for the entropy which diﬀered from the usual one used in statistical mechan-
ics. Previous to this, the R´enyi entropy was used as an interpolation formula
that connected the Hartley–Boltzmann entropy to the Shannon–Gibbs entropy.
Notwithstanding the fact that the R´enyi entropy is additive, it lacks many
other properties that characterize the Shannon–Gibbs entropy. For example,
the R´enyi entropy is not subadditive, recursive, nor does it possess the branch-
ing and sum properties [2]. The so-called Tsallis entropy ﬁlls this gap, while
being nonadditive, it has many other properties that resemble the Shannon–
Gibbs entropy. It is no wonder then that this entropy ﬁlls an important gap.

Yet, it appears odd, to say the least, that information scientists have left
such a gaping void in their analysis of entropy functions. A closer analysis of
the literature reveals that this is not the case and, indeed, a normalized Tsallis
entropy seems to have ﬁrst appeared in a 1967 paper by Havrda and Charv´at
[3] who introduced the normalized ‘Tsallis’ entropy

Sn,q(p1, . . . , pn) =

(1)

n

 

i=1
X

pq
i − 1

(21−q − 1)

! (cid:30)

n
i=1 pi = 1, and parameter q > 0,
for a complete set of probabilities, pi, i.e.
but q 6= 1. The latter requirement is necessary in order that (1) possess the

P

1

fundamental property of the entropy; that is, it is a concave function. According
to Tsallis [4], only for q > 0 is the entropy, (1), said to be expansible [2] [cf. (6)
below].

2 Properties of Additive Entropy of Degree-q

The properties used to characterize the entropy are [2, 5]:

1. Concavity

Sn,q

pixi

≥

piSn,q(xi)

(2)

n

 

i=1
X

n

i=1
X

!

where the nonnegative n-tuple, (p) = (p1, . . . , pn), forms a complete prob-
ability distribution. For ordinary means, the n-tuple, (x) = (x1, . . . , xn),
represents a set of nonnegative numbers which constitute a set indepen-
dent variables. What constitutes the main diﬃculty in proving theorems
on characterizing entropy functions in information theory is that the ‘in-
dependent variables’, (x), are not independent of their ‘weights’, (p) [6].

Coding theory, to be discussed in the next section, derives the functional
dependencies in a very elegant way through optimization. The entropies
S(xi) represent the costs of encoding a sequence of lengths xi, whose
probabilities are pi. Minimizing the mean length associated with the cost
function, expressed as the weighted mean of the cost function, gives the
optimal codeword lengths xi as functions of their probabilities, pi. Con-
sequently, the entropies that result when the xi are evaluated at their
optimal values by expressing them in terms of their probabilities, pi, con-
stitute lower bounds to the mean lengths for the cost function.

2. Non-negativity

3. Symmetry

Sn,q(p1, . . . , pn) ≥ 0

Sn,q(p1, . . . , pn) = Sn,q(p[1], . . . , p[n])

where [] denotes any arbitrary permutation of the indices on the proba-
bilities. For the entropy, the symmetry property (4) means that it should
not depend upon the order in which the outcomes are labelled.

4. The sum property

Sn,q(p1, . . . , pn) =

Sn,q(pi)

n

i=1
X

where Sn,q is a measurable function on ]0, 1[.

(3)

(4)

(5)

2

5. Expansibility

Sn+1,q(0, p1, . . . , pn) = Sn,q(p1, . . . , pn),

(6)

meaning that the entropy should not change when an outcome of proba-
bility zero is added.

6. Recursivity of degree-q

Sn,q(p1, . . . , pn) = Sn−1,q(p1+p2, p3, . . . , pn)+(p1+p2)qS2,q

p2
p1 + p2 (cid:19)
(7)
asserting that if a choice is split into two successive choices, the original
entropy will be the weighted sum of the individual entropies. Recursivity
implies the branching property by requiring at the same time the additivity
of the entropy as well as the weighting of the diﬀerent entropies by their
corresponding probabilities [7].

p1
p1 + p2

(cid:18)

,

,

7. Normality

8. Decisivity

9. Additivity of degree-q

S2,q ( 1

2 , 1

2 ) = 1

S2,q(1, 0) = S2,q(0, 1) = 0

(8)

(9)

(10)

Snm,q(p1q1, . . . , p1qm, . . . , pnqm) = Sn,q(p1, . . . , pn)
+ (21−q − 1)Sn,q(p1, . . . , pn)Sm,q(q1, . . . , qm)

for any two complete sets of probabilities, (p) and (q). As late as 1999,
Tsallis [4] refers to (10) as exhibiting “a property which has apparently
never been focused before, and which we shall refer to as the composability
property.” Here, composability means something diﬀerent than in infor-
mation theory [2], in that it “concerns the nontrivial fact that the entropy
S(A + B) of a system composed of two independent subsystems A and B
can be calculated from the entropies S(A) and S(B) of the subsystems,
without any need of the microscopic knowledge about A and B, other than
the knowledge of some generic universality class, herein the nonextensive
universality class, represented by the entropic index q . . .”[4].

However, the additive entropy of degree-q, (1), is not the only solution to
the functional equation (10) for q 6= 1. The average entropy

SA

n,q(p1, . . . , pn) =

n

1/q

q

q − 1 


1 −

pq
i

!

 

i=1
X




(11)

also satisﬁes (10), with the only diﬀerence that (1 − q)/q replaces the
coeﬃcient in the multiplicative term [8]. Since the weighted mean of





3

n,q(λp1, . . . , λpn) = λSA

degree-q is homogeneous, the pseudo-additive entropy (11) is a ﬁrst-order
homogeneous function of (p), SA
n,q(p1, . . . , pn). It
can be derived by averaging the same solution to the functional equation
(10), in the case q 6= 1, as that used to derive the Tsallis entropy, except
with a diﬀerent exponent and normalizing factor, under the constraint
that the probability distribution is complete [9]. Although the pseudo-
additive entropy (11) lacks the property of recursivity, (7), it is monotonic,
continuous, and concave for all positive values of q. Weighted means have
been shown to be measures of the extent of a distribution [10], and (11)
relates the entropy to the weighted mean rather than to the more familiar
logarithm of the weighted mean, as in the case of the Shannon and R´enyi
entropies.

Tsallis, in fact, associates additivity with extensivity in the sense that for
independent subsystems

Snm,q(p1q1, . . . , pnqm) = Sn,q(p1, . . . , pn) + Sm,q(q1, . . . , qm)

(12)

According to Tsallis [4], superadditivity, q < 1, would correspond to su-
perextensivity, and subadditivity, q > 1, would correspond to subexten-
sivity. According to Callen [11], extensive parameters have values in a
composite system that are equal to the sum of the values in each of the
systems. Anything that is not extensive is labelled intensive, although
Tsallis would not agree [cf. (30) below]. For instance if we consider black-
body radiation in a cavity of volume V , having an internal energy, U , and
magnify it λ times, the resulting entropy

λS(U, V ) = 4

3 σ1/4(λU )3/4(λV )1/4,

(13)

will be λ times the original entropy, S(U, V ), where σ is the Stefan-
Boltzmann constant. Whereas extensitivity involves magnifying all the
extensive variables by the same proportion, additivity in the sense of being
superadditive or subadditive deals with a subclass of extensive variables,
because the condition of extensivity of the entropy imposes that the de-
terminant formed from the second derivatives of the entropy vanish [12].
The entropy of black-body radiation, (13), is extensive yet it is subaddi-
tive in either of the extensive variables. The property of subadditivity is
what Lorentz used to show how interactions lead to a continual increase
in entropy [12]. This is a simple consequence of Minkowski’s inequality,

u3/4
1 + u3/4

2 ≥ (u1 + u2)3/4,

where u = U/V is the energy density. Hence, (sub-or super-) extensivity
is something very diﬀerent from (sub-or super-) additivity.

10. Strong additivity of degree-q

Smn,q(p1q11, . . . , pnq1m, . . . , pnqnm) = Sn,q(p1, . . . , pn)

+

pq
j Sm,q(qj1, . . . , qjm)

(14)

n

j=1
X

4

where qij is the conditional probability. Strong additivity of degree-q
describes the situation in which the sets of outcomes of two experiments
are not independent. Additivity of degree-q, (10), follows from strong
additivity by setting q1k = q2k = · · · = qmk = qk, and taking (1) into
consideration [2].
A doubly stochastic matrix (qij), where m = n, is used in majorization
to distribute things, like income, more evenly [13], and this leads to an
increase in entropy. For if

qj =

qij pi,

n

i=1
X

(15)

and

that

n

n

n

n

qj =

pi

qij =

pi = 1,

j=1
X

i=1
X

j=1
X

i=1
X

it follows from the convexity of ψ = x ln x, or

ψ

qij pi

≤

qijψ(pi),

n

 

i=1
X

n

i=1
X

!

n

i=1
X

n

n

i=1
X

j=1
X

Sn,1(q1, . . . , qn) = −

qi ln qi ≥ −

qij pi ln pi

(16)

P

n
since
i=1 qij = 1. We may say that p majorizes q, p ≻ q if and only if
(15) for some doubly stochastic matrix (qij ) [14]. A more even spread of
incomes increases the entropy. Here we are at the limits of equilibrium
thermodynamics because we are invoking a mechanism for the increase
in entropy, which in the case of incomes means taking from the rich and
giving to the poor [15]. This restricts q in the ‘Tsallis’ entropy to ]0, 1[.
Values of q in ]1, 2[ show an opposing tendency of balayage or sweeping
out [16]. Whereas averaging tends to decrease inequality, balayage tends
to increase it [15].
Yet Tsallis [4] refers to processes with q < 1, i.e. pq
i > pi, as rare events,
and to q > 1, i.e. pq
i < pi as frequent events. However, only in the case
where q < 1 will the Shannon entropy, (16) be a lower bound to other
entropies like, the R´enyi entropy

SR

n,q =

1
1 − q  

ln

n

i=1
X

pq
i

!

(17)

which is the negative logarithm of the weighted mean of pq−1
. The R´enyi
entropy has the attributes of reducing to the Shannon–Gibbs entropy, (16),
in the limit as q → 1, and to the Hartley–Boltzmann, entropy

i

Sn,0(1/n, . . . , 1/n) = ln n

(18)

5

in the case of equal a priori probabilities pi = 1/n. This leads to the
property of

11. n-maximal

Sn,q(p1, . . . , pn) ≤ Sn,q

, . . .

(19)

1
n

(cid:18)

1
n

(cid:19)

for any given integer n ≥ 2. The right-hand side of (19) should be a
monotonic increasing function of n. As we have seen, the tendency of the
entropy to increase as the distribution becomes more uniform is due to
the property of concavity (2). Hence, it would appear that processes with
q < 1 would be compatible with the second law of thermodynamics, rather
than being rare exceptions to it!

12. Continuity: The entropy is a continuous function of its n variables. Small
changes in the probability cause correspondingly small changes in the en-
tropy. Additive entropies of degree-q are small for small probabilities,
i.e.,

lim
p→0

S2,q(p) = lim
p→0

pq + (1 − p)q − 1
21−q − 1

.

3 Coding Theory and Entropy Functions

The analogy between coding theory and entropy functions has long been known
[18]. If k1, . . . , kn are the lengths of codewords of a uniquely decipherable code
with D symbols then the average codeword length

piki

n

i=1
X

is bounded from below by the Shannon-Gibbs entropy (16) if the logarithm is
to the base D. The optimal codeword length is ki = − ln pi, which represents
the information content in event Ei. If D = 1
2 contains exactly one
bit of information.

2 then pi = 1

Ordinarily, one tries to keep the average codeword length (20) small, but it
cannot be made smaller than the Shannon-Gibbs entropy. An economical code
has frequently occurring messages with large pi and small ki. Rare messages are
those with small pi and large ki. The solution ni = − ln pi has the disadvantage
that the codeword length is very great if the probability of the symbol is very
small. A better measure of the codeword length would be

n

1
τ

log

 

piDτ ki

!

i=1
X
where τ = (1 − q)/q, thereby limiting q to the interval [0, 1]. As τ → ∞, the
limit of (21) is the largest of the ki, independent of pi. Therefore, if q is small
enough, or τ large enough, the very large ki’s will contribute very strongly to

(20)

(21)

6

the average codeword length (21), thus keeping it from being small even for very
small pi. The optimal codeword length is now

ki = −q ln pi +

n

pq
i ,

i=1
X

showing that the R´enyi entropy is the lower bound to the average codeword
length (21) [18]. Just as the pi = D−ki are the optimum probabilities for the
Shannon-Gibbs entropy, the optimum probabilities for the R´enyi entropy are
the so-called escort probabilities,

−ki =

D

pq
i
n
i=1 pq

i

.

(22)

P
As pi → 0, the optimum value of ki is asymptotic to −q ln pi so that the optimum
length is less than − ln pi for q < 1 and suﬃciently small pi. This provides
additional support for keeping q within the interval [0, 1] [16].

Although the R´enyi entropy is additive it does not have other properties
listed above; for instance, it is not recursive and does not have the branching
property nor the sum property. It is precisely the ‘Tsallis’ entropy which ﬁlls
the gap, while not being additive, it has many of the other properties that
an entropy should have [19]. Therefore, in many ways the additive entropy of
degree-q (1) is closer to the Shannon entropy, (16) than the R´enyi entropy is.
The so-called additive entropies of degree-q can be written as

n

i=2
X

y
1 − x

(cid:18)

(cid:19)

Sn,q(p1, . . . , pn) =

(p1 + . . . + pi)qf

pi
p1 + . . . + pi (cid:19)

,

(cid:18)

(23)

where the function f is a solution to the functional equation

f (x) + (1 − x)qf

= f (y) + (1 − y)qf

x
1 − y

,

(cid:19)

(cid:18)

subject to f (0) = f (1), which was rederived by Curado and Tsallis [20], and the
property of additivity of degree-q (10) was referred to them as pseudo-additivity,
omitting the original references. What these authors appeared to have missed
are the properties of strong additivity, (14) and recursivity of degree-q (7). These
properties can be proven by direct calculation using the normalized additive
entropy of degree-q, (1). Additive entropies of degree-q ≥ 1 are also subadditive.
Moreover, additive entropies of degree-q satisfy the sum property, (5) where

Sq(pi) = (pq

i − pi)/(21−q − 1) ≥ 0.

(24)

Only for q > 0 will (24), and consequently (1), be concave since
q (pi) = q(q − 1)pq−2

/(21−q − 1) ≤ 0,

S′′

i

7

where the prime stands for diﬀerentiation with respect to pi. This is contrary
to the claim that the additive entropy of degree-q is “extremized for all values
of q”[1]. It can easily be shown that the concavity property

Sq

n

 

i=1
X

pixi

≥

!

n

i=1
X

piSq(xi),

implies the monotonic increase in the entropy (19). Setting pi = 1/n and using
the sum property (5) lead to

Sn,q(p1, . . . , pn) =

Sq(pi) ≤ nSq

= nSq

= Sn,q

, . . . ,

n

pi
n

i=1 (cid:16)
X

(cid:17)

1
n

(cid:18)

(cid:19)

1
n

(cid:18)

1
n

,

(cid:19)

n

i=1
X

showing that Sn,q(1/n, . . . , 1/n) is maximal.

In order to obtain explicit expressions for the probabilities, Tsallis and col-

laborators maximized their non-normalized entropy

ST

n,q(p1 . . . , pn) =

pq
i − 1

/(1 − q)

(25)

n

 

i=1
X

!

with respect to certain constraints. Taking their cue from Jaynes’ [21] formalism
of maximum entropy, (25) was to be maximized with respect to the ﬁnite norm
[22]

∞

−∞

Z

p(x) dx = 1

and the so-called q average of the second moment [20]

∞

−∞

Z

x2

q =

(cid:10)

(cid:11)

x2[σ p(x)]q d(x/σ) = σ2.

(26)

The latter condition was introduced because the variance of the distribution did
not exist, and the weights, (pq), have been referred to as ‘escort’ probabilities
[cf.
(22) above]. The resulting distribution is almost identical to Student’s
distribution

p(x|µ) =

r

µ
π

(q − 1)
(3 − q)

Γ (1/(q − 1))
Γ ((3 − q)/2(q − 1))

1 +

(q − 1)
(3 − q)

µx2

(cid:18)

(cid:19)

−1/(q−1)

(27)

where (3 − q)/(q − 1) is the number of degrees of freedom, and µ is the Lagrange
multiplier for the constraint (26) [23].

The Gaussian distribution is the only stable law with a ﬁnite variance, all
the other stable laws have inﬁnite variance. These stable laws have much larger
tails than the normal law which is responsible for the inﬁnite nature of their
variances. Their initial distributions are given by the intensity of small jumps,
where the intensity of jumps having the same sign of x, and greater than x in
absolute value is [24]

(28)

F (x) =

c
xβ ,

8

for x > 1. For β < 1, the generalized random process, which is of a Poisson
nature, produces only positive jumps, whose intensity (28) is always increasing.
No moments exist, and the fact that

Z(λ) = e

−λβ

(29)

where λ is both positive and real, follows directly from P´olya’s theorem: If for
each λ, Z(0) = 1, Z(λ) ≥ 0, Z(λ) = Z(−λ), Z(λ) is decreasing and continu-
ous convex on the right half interval, then Z(λ) is a generating function [25].
Convexity is easily checked for 0 < β ≤ 1, and it is concluded that z(λ) is a
generating function. In other words,

1 − Z(λ) = −

−λx

1 − e

dF (x) = Γ(1 − β)λ

−β

∞

0
Z

(cid:0)

(cid:1)

exists for a positive argument of the Gamma function, and that implies β < 1.
This does not hold on the interval 1 < β < 2, where it makes sense to talk
about a compensated sum of jumps, since a ﬁnite mean exists.
In the limit
β = 2, positive and negative jumps about the mean value become equally as
probable and the Wiener-L´evy process results, which is the normal limit.
If
one introduces a centering term in the expression, λx, the same expression for
the generating function, (29), is obtained to lowest power in λ, as λ → 0 and
x → ∞, such that their product is ﬁnite.

These stable distributions, 0 < β < 1, (and quasi-stable ones, 1 < β < 2,
because the eﬀect of partial compensation of jumps introduces an arbitrary addi-
tive constant) are related to the process of super-diﬀusion, where the asymptotic
behavior of the generalized Poisson process has independent increments with in-
tensity (28). For strictly stable processes, the super-diﬀusion packet spreads out
faster than the packet of freely moving particles, while a quasi-stable distribu-
tion describes the random walk of a particle with a ﬁnite mean velocity. It was
hoped that these tail distributions could be described by an additive entropy
of degree-q, where the degree of additivity would be related to the exponent
of the stable, or quasi-stable, distribution. Following the lead of maximum en-
tropy, where the optimal distribution results from maximizing the entropy with
all that is known about the system, the same would hold true for maximizing
the additive entropy of degree-q. However, it was immediately realized that the
variance of the distribution does not exist.

Comparing the derivative of the tail density (28) with (27) identiﬁes β =
(3 − q)/(q − 1), requiring the stable laws to fall in the domain 5
3 < q < 3 [22].
However, it is precisely in the case in which we are ignorant of the variance
that the Student distribution is used to replace the normal since it has much
fatter tails and only approaches the latter as the number of degrees of freedom
increases without limit [24]. Just as the ratio of the diﬀerence of the mean
of a sample and the mean of the distribution to the standard deviation is dis-
tributed normally, the replacement of the standard deviation by its estimator is
distributed according to the Student’s distribution. This distribution (27) was
not to be unexpected, because it stands in the same relation to the normal law

9

as the ‘Tsallis’ entropy, (25), in the limit as the number of degrees of freedom
is allowed to increase without limit.

Whereas weighted means of order-q

Mq =

1/q

i

n

i=1 pixq
i=1 pi (cid:19)

n

hxiq :=

n

i=1 pq
i xi
i=1 pq

n

i

,

(cid:18) P
do have physical relevance for diﬀerent values of q, the so-called q-expectation
P

P
has no physical signiﬁcance for values of q 6= 1. Since the connection between
P
statistical mechanics and thermodynamics lies in the association of average val-
ues with thermodynamic variables, the q-expectations would lead to incorrect
averages. This explains why for Tsallis the internal energy of a composite sys-
tem is not the same as the internal energies of the subsystems, and makes the
question “if we are willing to consider the nonadditivity of the entropy, why it is
so strange to accept the same for the energy?”[26] completely meaningless. Yet,
the zeroth law of thermodynamics, and the derivation of the Tsallis nonintensive
inverse temperature,

β =

[1 − (1 − q)Sn,q],

(30)

∂Sn,q
∂Uq (cid:30)

where Uq is the q-expectation of the internal energy, rest on the fact that the
total energy of the composite system is conserved [27].

It is as incorrect to speak of ‘Tsallis’ statistics [28] as it would be to talk of
R´enyi statistics. These expressions are mere interpolation formulas leading to
statistically meaningful expressions for the entropy in certain well-deﬁned limits.
Whereas for the R´enyi entropy the limits q → 1 and q → 0 give the Shannon-
Gibbs and Hartley-Boltzmann entropies, respectively, without assuming equal
probabilities, the additive entropy of degree-q reduces to the Shannon entropy in
the limit as q → 1, but it must further be assumed that the a priori probabilities
are equal in order to reduce it to the Hartley-Boltzmann entropy. Hence, only
the R´enyi entropies are true interpolation formulas.

Either the average of − ln pi leading to the Shannon entropy, or the negative
of the weighted average of pq−1
, resulting in the R´enyi entropy will give the
property of additivity [2]. Whereas the Shannon entropy is the negative of the
logarithm of the geometric mean of the probabilities,

i

Sn,1(p1, . . . , pn) = − ln Gn(p1, . . . , pn),

where

Gn(p1 . . . , pn) = Πn

i=1ppi

i

is the geometric mean, the R´enyi entropy is the negative of the logarithm of the
weighted mean

SR

n,q = − ln Mq−1,

10

where

1/(q−1)

Mq−1 =

n

 

i=1
X

pipq−1

i

!

is the weighted mean of pq−1
. If the logarithm is to the base 2, the additive
entropies of degree-q are exponentially related to the R´enyi entropies of order-q
by

i

Sn,q =

2(1−q)SR

n,q − 1

(21−q − 1),

(cid:16)

(cid:17) (cid:14)

which make it apparent that they cannot be additive. But nonadditivity has
nothing to do with nonextensivity.

As a concluding remark it may be of interest to note that undoubtedly the
oldest expression for an additive entropy of degree-2 was introduced by Gini
[29] in 1912, who used it as an index of diversity or inequality. Moreover,
generalizations of additive entropies of degree-q are well-known.
It has been
claimed that “Tsallis changed the mathematical form of the deﬁnition of entropy
and introduced a new parameter q”[30]. Generalizations that introduce additive
entropies of degree-q + ri − 1 [31]

Sn,q,r1,...,rn(p1, . . . , pn) =

n

i=1 pq+ri−1
i=1 pri

n

i

i

  P

− 1

(21−q − 1),

! (cid:30)

P
with n + 1 parameters, should give even better results when it comes to curve
ﬁtting.

References

[1] C. Tsallis, J. Stat. Phys. 52, 470 (1988).

[2] J. Acz´el and Z. Dar´oczy, On the Measures of Information and their Char-

acterization (Academic Press, New York, 1975).

[3] J. Havrda and F. Charv´at, Kybernetika (Prague) 3, 30 (1967), and ﬁrst
acknowledged by Tsallis in C. Tsallis, Solitons and Fractals 6, 539 (1995).

[4] C. Tsallis, Braz. J. Phys. 29, 1 (1999).

[5] A. M. Mathai and P. N. Rathie, Basic Concepts in Information Theory and

Statistics (Wiley, New York, 1975).

[6] J. Ac´zel, Lectures on Functional Equations and their Applications (Aca-

demic Press, New York, 1966).

[7] A. R´enyi, Probability Theory (North-Holland, Amsterdam, 1970).

[8] D. E. Boekee and J. C. A. Van der Lubbe, Inform. Contr. 45, 136 (1980).

[9] S. Arimoto, Inform. Contr. 19, 181 (1971).

11

[10] L. L. Campbell, Z. Wahrscheinlichkeitstheorie verw. Geb. 5, 217 (1966).

[11] H. B. Callen, Thermodynamics and and Introduction to Thermostatics, 2nd

edn. (Wiley, New York, 1985).

[12] B. H. Lavenda, Statistical Physics: A Probabilistic Approach (Wiley-

Interscience, New York, 1991).

[13] A. W. Marshall and I. Olkin, Inequalities: Theory of Majorization and its

Applications (Academic Press, San Diego, 1979).

[14] G. H. Hardy, J. E. Littlewood, and G. P´olya, Inequalities 2nd. edn. (Cam-

bridge U. P., Cambridge, 1952).

[15] B. C. Arnold, Majorization and the Lorenz Order: A Brief Introduction

(Springer-Verlag, Berlin, 1987).

[16] B. H. Lavenda, Int. J. theoret. Phys. 37, 3119 (1998); B. H. Lavenda and

J. Dunning-Davies, in preparation.

[17] A. R´enyi, in Proc. Fourth Berkeley Symp. Math. Statist. Prob. (Wiley, New

York, 1961), vol. 1, pp. 547–561.

[18] L. L. Campbell, Inform. Contr. 8, 423 (1966); J. Acz´el and J. Dhombres,
Functional Equations in Several Variables (Cambridge U. P., Cambridge,
1972); B. H. Lavenda, J. Phys. A 31, 5651 (1998).

[19] Z. Dar´oczy, Inform. Contr. 16, 36 (1970), and ﬁrst acknowledged by Tsallis

in [20].

[20] E. M. F. Curado and C. Tsallis, J. Phys. A24 L69 (1991); Corrigenda 24,

3187 (1991); 25, 1019 (1992).

[21] E. T. Jaynes, Phys. Rev. 106, 620 (1957).

[22] C. Tsallis, S. V. F. Levy, A. M. C. Souza, and R. Maynard, Phys. Rev.

Lett. 75, 3589 (1995).

[23] A. M. C. Souza and C. Tsallis, Physica A 236, 52 (1997).

[24] B. de Finetti, Theory of Probability, vol. 2 (Interscience, New York, 1990).

[25] K. L. Chung, A Course in Probability Theory (Academic Press, San Diego,

1974).

[26] C. Tsallis, R. S. Mendes, and A. R. Plastino, Physica A 261, 534 (1998).

[27] S. Abe, Physica A 269, 403 (1999).

[28] Science 300, 249 (2003).

[29] C. Gini, Studi Economico-Giuridici della Facolt´a di Giurisprudenza

dell’Universit´a di Cagliari AIII, parte II.

12

[30] A. Chou, Science 297, 1268 (2002).

[31] P. N. Rathie, Kybernetika 7, 125 (1971).

13


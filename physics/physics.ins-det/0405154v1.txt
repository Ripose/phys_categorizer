4
0
0
2
 
y
a
M
 
9
2
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
4
5
1
5
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The ATLAS Tile Calorimeter Test Beam
Monitoring Program

Paolo Adragna, Andrea Dotti, Chiara Roda
Universit`a di Pisa e
Istituto Nazionale di Fisica Nucleare, Sezione di Pisa

Abstract

During 2003 test beam session for ATLAS Tile Calorimeter a moni-
toring program has been developed to ease the setup of correct running
condition and the assessment of data quality. The program has been
built using the Online Software services provided by the ATLAS Online
Software group. The ﬁrst part of this note contains a brief overview of
these services followed by the full description of Tile Calorimeter mon-
itoring program architecture and features. Performances and future
upgrades are discussed in the ﬁnal part of this note.

1 Introduction

The monitoring program described here has been developed in the frame-
work of the calibration Test Beam periods carried out at CERN on ATLAS
Tile Calorimeter. The ATLAS calorimetric system is a composite detector
which exploits diﬀerent techniques at diﬀerent rapidity regions to optimize
the calorimeter performance while maintaining a high enough radiation re-
sistance. The Tile sampling calorimeter (TileCal) is the central hadronic
section of this system. It consists of three longitudinal samplings of iron
plates and scintillating tiles. TileCal is composed by one central barrel and
two side (extended) barrels consisting of 64 modules each. During the three-
year calibration program about 12% of these modules have been exposed to
test beams.

The monitoring program described here (in the following referred to as
PMP) has been developed for the 2003 Test Beam period. This application,
based on ROOT [1] and on the software developed by the ATLAS Online
Software Group, allows to monitor both Tile Calorimeter modules and beam
detector data. This program has allowed to obtain a fast setup of beam
conditions as well as a fast check of the calorimeter data quality.

A short account of the software environment where the program has
been developed and a detailed description of the TileCal Monitoring Task
are given in the second and third section respectively.

1

2 General feature of monitoring applications

Atlas Online Software provides a number of services which can be used to
build a monitoring system [2]. Their main task is to carry requests about
monitoring data (e.g. request of event blocks, request of histograms...) from
monitoring destinations to monitoring sources and then the actual monitored
data (e.g. event blocks, histograms...) back from sources to destinations. Four
services are provided to access diﬀerent types of information [3, 4, 5, 6]:

• Event Monitoring Service;

• Information Service;

• Histogramming Service;

• Error Reporting System.

PMP uses the Event Monitoring Service while other services will be included
in future upgrades.

Figure 1: Structure of the Event Monitoring Service [8].

The Event Monitoring Service (EMS) provides events to the User Moni-
toring task sampling them from any point of the data ﬂow chain. The system,
shown on ﬁgure 1, consists of the following subsystems [7]:

• the Event Sampler, which is responsible for sampling event data ﬂow-
ing through the DAQ system and for storing them in the Event Dis-
tribution subsystem;

2

• the Event Distribution, which is responsible of the distribution, on

• the User Monitoring Task, which requests events through the Event

demand;

Iterator.

The implementation of the User Monitoring Task is described in detail

in the following sections.

3 The TileCal monitoring task

The TileCal Test Beam monitoring program is an object oriented applica-
tion developed in C++. It is completely based on the ROOT framework
and in particular data storage, graphical user interface and event handling
fully exploit the use of ROOT classes and methods. The program has been
developed under Linux operating system for the i686 architecture. In ﬁgure
2 the code working diagram is drawn [9].

Figure 2: Schematic structure of the Tile Calorimeter monitoring program.

The input to PMP may be either raw data ﬁles stored on disk or online
sampled events provided by the Event Distribution. In both cases data are
expected to be written in the standard ATLAS format [10].

Once the event is accessed, data are unpacked and interpreted. Event un-
packing proceeds through detector independent methods up to the localiza-

3

tion of the Read Out Driver (ROD) Fragments. Detector dependent methods
are implemented for the extraction of data inside this fragment. Each event
contains both Tile Calorimeter and beam detector data (Cerenkov counters,
scintillators and wire chambers). All relevant information is extracted, event
by event, and stored in a ROOT Tree [11] residing in memory, while raw
data are discarded and the buﬀer is freed. From this point on the analysis
is performed using only data stored in the ROOT Tree. Histograms pro-
duced during the analysis can be immediately displayed using the presenter
included inside the Graphical User Interface (GUI).

The possibility of reading raw data ﬁles from disk not only greatly sim-
pliﬁes the debugging process but also allows to run simple analysis tasks on
the just acquired data.

3.1 Monitoring program architecture

The best way of explaining PMP architecture is probably to follow the path
a typical user would walk on to work with the application [12]. As shown in
the dependency diagram of ﬁgure 3, PMP abstract structure is divided into
three main blocks.

Test Beam DAQ System

Local File

IPC Partition

Data Source

DataFile

Consumer

Presenter

DisplayWindow

ScrollingTabFrame

CanvasUpdateAsynch

DataBase

Main

ROOT Tree

Unpacking and Decoding

DataConnection

RawData

PMTChannel

Drawer

BeamADCData

BeamTDCData

AddrADCData

LaserADCData

DigiParData

RawAtlasEvent

SubDetectorFragment

ROSFragment

ROBFragment

RODFragment

Figure 3: Dependency Diagram for PMP classes.

The ﬁrst block, named in ﬁgure as Data Source, includes the classes
DataFile and Consumer which are responsible for fetching a buﬀer of
data from the selected source.

The buﬀer memory address is passed to the Unpacking and Decode block
by the class DataConnection. The task of this second block is to extract
the fragments needed and to interpret them.

The third block, the Presenter, fetches the histograms produced and
ﬁlled by DataBase and organizes their graphical display into tabs. This
block is also responsible for managing the user required actions.

4

The main panel

When the program is started the main panel (ﬁgure 4) is created.

Figure 4: Main control Panel of PMP.

It is built by an instance of the class MainFrame that, apart from
drawing the GUI, is also responsible for the event handling and for the
subsequent actions. From this stage on the user can completely drive the
application (e.g. choose the trigger type and the data input source, opening
the presenter...). The event access is implemented using the classes DataFile
and Consumer for event retrieving either from data ﬁle or from online
Event Monitoring Service respectively. In both cases a full event is buﬀered
in memory and passed to the unpacking and decoding phase.

Unpacking and decoding

The event is structured according to the ATLAS event format: the detector
data is encapsulated into 4 layers of headers, all having similar structures.
In order to easily reach the detector data ﬁve classes have been developed:

• RawAtlasEvent

• SubDetectorFragment

• ROSFragment

• ROBFragment

• RODFragment

Their algorithms allow to identify the corresponding block of data and
their ﬁelds. These classes are implemented in such a way that a call to the
method RawAtlasEvent::ReadFromMem triggers the complete event

5

Figure 5: Example of a set of PMP histograms as they appear in the Presenter.

decoding: in fact this method calls SubDetectorFragment::ReadFrom-
Mem for every SubDetector block of data, every SubDetectorFragment::-
ReadFromMem calls ROSFragment::ReadFromMem for every ROS
block and so on.

The modularity of this structure allows to easily add sub-detectors man-
aging the decoding of their new data format just by including the appropriate
Decoding routine.

Once the whole chain has been executed the pointers to all RODs in the
event are stored in a vector. The decoding of the detector dependent data is
managed by the method RawData::Decode which determines the correct
unpacking algorithm for each subsystem on the base of the ROD id value
(every ROD, and consequently every kind of data, is identiﬁed by a unique
number indicated as source identiﬁer ).

ROOT Tree

Once fully unpacked, data are stored in a ROOT Tree and the raw data
buﬀer is freed. The tree structure is created on the base of the event struc-
ture. Since this one is not known before starting a data taking (for example
the number of detectors may change in diﬀerent data takings) the tree ar-
rangement is built according to the structure of the ﬁrst retrieved event.
The data stored in the the ROOT Tree is then used for simple analysis and
to ﬁll histograms.

The Tree normally acts as an interface between the data decoding and

6

Figure 6: Example of event display. The ﬁgure shows a representation of the energy
released inside Tile Calorimeter Modules by a pion, impinging at η = 0.35 and
producing a signal in cells A4, BC4, D4.

the analysis phase where histograms are ﬁlled; however it allows also to save
a complete collection of sampled and preprocessed events on disk for a more
in-depth analysis with standard ROOT tools.

Presenter

The list of created histograms is stored inside an instance of the class
DataBase.Histograms can be viewed with a simple presenter in a separate
window. Diﬀerent graphical sections lay on the same display. Within each
section, plots are drawn in diﬀerent embedded canvas whose coexistence is
granted using ROOT’s capabilities of organizing graphic widgets into a table
layout [11] (ﬁgure 5). The presenter refers to the class database to get the
appropriate histograms to be displayed. The use of the TQObject ROOT
class and the Signal – Slot mechanism [13] allows to plot histograms in real
time and to refresh each canvas independently.

The Signal – Slot mechanism has also been applied to the implementation

7

of histogram rebinning and resetting. Each histogram is associated with a
set of buttons, clicking each of them causes the emission of a signal. The
latter is caught by DataBase which performs the appropriate action.

PMP displays plots relative to the beam detectors (Cerenkov counters,

beam proﬁle, coincidence scintillators) and to TileCal modules.

Among the diﬀerent plots there is a simple event display where the energy
deposited in each cell of the three calorimeter modules is shown (ﬁgure 6).
Here the three modules of Test Beam setup are represented as two di-
mensional histograms, while each calorimeter cell is drawn as a single par-
allelepiped with height proportional to energy deposit. On the axes one can
read the η value and the depth (the three longitudinal layers denoted A,
BC, D) of the signal.

On all the displayed histograms it is possible to perform standard ROOT

actions (rescaling, ﬁtting, ...).

4 Program performances

The monitoring program has been extensively tested to check the usage of
CPU and memory during the data taking periods at Test Beam. The test
is performed on a PentiumIII class machine at 1GHz with 256 MB of RAM
running Linux RedHat 7.3 (CERN version).

The mean CPU load is 41% while the average usage of physical memory

is 18%.

)

B
K

(
 
y
r
o
m
e
M

45000

40000

35000

30000

25000

0

100

200

300

400

500

600

700

800

900

Time (s)

Figure 7: PMP Memory usage (physical plus swap) as function of running time.

The total memory (physical plus swap) usage is shown in ﬁg. 7 as a

8

function of running time. The steep increase of used memory, shown on the
plot around 550 seconds, represents the opening of the presenter GUI.

The monitoring of the memory usage has proved to be an important tool

to ﬁnd and correct memory leaks during the development.

5 Future plans and conclusions

Although the program is quite ﬂexible from the maintenance point of view,
thanks to its high modularity and the use of ROOT Tree to decouple un-
packing from histogramming, it still suﬀers from lack of optimization which
limit some aspects of his performance.

The main problem is speed. PMP keeps on making remarkable eﬀorts to
transform raw data into values ready-to-use for analysis and plotting, but
his single-threaded conﬁguration is quite penalizing. In particular situation
of CPU overloading, the execution of the graphical presenter code lowers
the performances considerably.

To solve this problem we plan to split the graphical part from the compu-
tational one, switching to a client-server conﬁguration. The computational
part which will unpack, decode and ﬁll the required histograms, will transfer
the ﬁlled histograms to the Histogramming Service provided by the Online
Software Group. The graphical part will fetch histograms from this service
and will display them. The graphical part will be conﬁgurable in order to
be easily used both for diﬀerent detectors and for a combined run.

MDT Cha 2 vs Tile

Corr
Corr

Entries 
Entries 
Mean x 
Mean x 
Mean y 
Mean y 
RMS x  
RMS x  
RMS y  
RMS y  

 18676
 18676
  114.7
  114.7
  1.732
  1.732
  67.08
  67.08
  3.369
  3.369

14

12

10

s
t
i

H

 
f
o
 
r
e
b
m
u
N

8

6

4

2

0

10

2

10

E (pC)

Figure 8: Number of hits recorded by the ﬁrst Muon Chamber (MDT) versus total
deposited energy in the hadronic Tile Calorimeter obtained from data recorded at
Combined Test beam during September 2003. The two populated regions on the top
left and on the low right regions correspond to muons and pions respectively.

9

A ﬁrst look at the possibilities of upgrading the monitoring program
here discussed to monitor a larger set of detectors has been tried at the
combined Test Beam in September 2003. MDT and SCT were simply in-
cluded using their standard data decoding routines. This allowed to obtain
simple online histograms showing correlated information from diﬀerent de-
tectors (Figure 8).

This experience has allowed to better understand bad and good features
of the present program and has helped us to plan the future program struc-
tures. It is foreseen to test the future versions of the monitoring program
during the 2004 Combined Test Beam.

6 Acknowledgments

We would like to thank Lorenzo Santi for giving us the code he wrote for the
monitoring of the pixel detector which was the starting point of our project.
We would also like to thank J. E. Garcia Navarro, S. Gonzalez, R. Ferrari,
W. Vandelli for helping us with the integration of MDT and SCT detectors
in the monitoring program during September 2003 Combined Test Beam.

References

[1] R. Brun, Fons Rademakers, ROOT - An Object Oriented Data Anal-
ysis Framework, Proceedings AIHENP’96 Workshop, Lausanne, Sep.
1996, Nucl. Inst. & Meth. in Phys. Res. A 389 (1997) 81-86. See also
http://root.cern.ch

[2] ATLAS High-Level Trigger, Data Acquisition and Controls, TDR, AT-

LAS TDR 16

157

[3] S. Kolos, Online Monitoring User’s Guide, ATLAS DAQ Technical Note

[4] S. Kolos, Information Service User’s Guide, CERN

[5] D. van Heesch, Onine Histogramming API, CERN

[6] D. Burckhart, M. Caprini, A. Di Simone, R. Jones, S. Kolos, A. Radu,

Message Reporting System, ATLAS DAQ Technical Note 59

[7] Online Software Training, ATLAS DAQ Technical Note 149

[8] S. Kolos, Online Software Monitoring Service, Presentation given during

TDAQ Monitoring Workshop, CERN, February 2003

[9] P. Adragna, A. Dotti, C. Roda, The TileCal Monitor for Test Beams,

Presentation given during DIG Forum, CERN, June 2003

10

[10] C. Bee, D. Francis, L. Mapelli, R. McLaren, G. Mornacchi, J. Petersen,
f. Wickens, The Raw Event Format in the ATLAS Trigger & DAQ,
ATL-DAQ-98-129

[11] S. Panacek ed., ROOT An Object Oriented Data Anlysis Framework,

Users Guide

[12] http://atlas.web.cern.ch/Atlas/SUB DETECTORS/TILE/

test-

beam/PMP Monitoring Help/PMP TileCal TB Online-Monitor.html

[13] http://doc.trolltech.com/signalslots.html

11


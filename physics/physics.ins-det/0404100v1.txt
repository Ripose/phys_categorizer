4
0
0
2
 
r
p
A
 
1
2
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
0
0
1
4
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Data handling, reconstruction, and simulation
for the KLOE experiment

F. Ambrosino e, A. Antonelli b, M. Antonelli b, C. Bini i,
C. Bloise b, P. Branchini k, G. Capon b, T. Capussela e,
E. De Lucia i, P. De Simone b, S. Dell’Agnello b, A. Denig c,
A. Di Domenico i, C. Di Donato e, S. Di Falco g, B. Di Micco k,
A. Doria e, M. Dreucci b, A. Farilla k, A. Ferrari k, M.L. Ferrer b,
G. Finocchiaro b, C. Forti b, G.F. Fortugno b, C. Gatti i,
P. Gauzzi i, S. Giovannella b, E. Gorini d, M. Incagli g,
G. Lanfranchi b, J. Lee-Franzini b,ℓ, D. Leone c,
M. Martemianov b, M. Martini b, W. Mei b, S. Miscetti b,
∗, S. M¨uller c, F. Nguyen k, M. Palutan b,
M. Moulson b,

E. Pasqualucci i, L. Passalacqua b, A. Passeri k, V. Patera b,h,
F. Perfetto e, M. Primavera d, P. Santangelo b, E. Santovetti j,
G. Saracino e, B. Sciascia b, F. Scuri g, I. Sﬁligoi b,
A. Sibidanov b,f, T. Spadaro b, M. Testa i, P. Valente i,
B. Valeriani c, G. Venanzoni g, A. Ventura d, S. Ventura i,
R. Versaci k, I. Villella e, G. Xu b,a

aPermanent address: Institute of High Energy Physics, CAS, Beijing, China.
bLaboratori Nazionali di Frascati dell’INFN, Frascati, Italy.
cInstitut f¨ur Experimentelle Kernphysik, Universit¨at Karlsruhe, Germany.
dDipartimento di Fisica dell’Universit`a e Sezione INFN, Lecce, Italy.
eDipartimento di Scienze Fisiche dell’Universit`a “Federico II” e Sezione INFN,
Napoli, Italy
f Permanent address: Budker Institute of Nuclear Physics, Novosibirsk, Russia
gDipartimento di Fisica dell’Universit`a e Sezione INFN, Pisa, Italy.
hDipartimento di Energetica dell’Universit`a “La Sapienza”, Roma, Italy.
iDipartimento di Fisica dell’Universit`a “La Sapienza” e Sezione INFN, Roma,
Italy.
jDipartimento di Fisica dell’Universit`a “Tor Vergata” e Sezione INFN, Roma,
Italy.
kDipartimento di Fisica dell’Universit`a “Roma Tre” e Sezione INFN, Roma, Italy.
ℓPhysics Department, State University of New York at Stony Brook, USA.

Preprint submitted to Elsevier Science

4 December 2013

Abstract

The broad physics program of the KLOE experiment is based on the high event rate
at the Frascati φ factory, and calls for an up-to-date system for data acquisition and
processing. In this review of the KLOE oﬄine environment, the architecture of the
data-processing system and the programs developed for data reconstruction and
Monte Carlo simulation are described, as well as the various procedures used for
data handling and transfer between the diﬀerent components of the system.

Key words: Oﬄine computing, data handling, event reconstruction, Monte Carlo
PACS: 29.85.+c, 07.05.Bx, 07.05.Kf, 07.05.Tp

1 Introduction

KLOE is a general-purpose experiment permanently installed at the Frascati
φ factory, DAΦNE. The KLOE detector was designed for the study of CP
violation in the neutral-kaon system. The versatility of the experiment allows
for a rich physics program, including measurements of radiative φ decays, nu-
merous decays of charged and neutral kaons, and measurement of the hadronic
cross section, among other topics.

×

The most interesting channels have branching ratios on the order of 10−3 or
smaller. For precision measurement of these decays, the DAΦNE collider has
1032 cm−2 s−1. At this luminos-
been designed to achieve a luminosity of 5
ity, the φ production cross section of about 3 µb translates into an event
rate of 1.5 kHz. Bhabha events within the acceptance, together with machine-
background and cosmic-ray events, contribute a similar amount to the total
acquisition rate. The average KLOE event size is 2.7 kB. We therefore re-
quire a data-acquisition (DAQ) system capable of handling a throughput of
10 MB/s with high eﬃciency, a data-processing environment with ﬁle servers
that provide bandwidth on the order of 100 MB/s, and a data-storage system
capable of handling on the order of a petabyte of data. These numbers are
similar to those for other major experiments currently running, and place the
design and implementation of the DAQ and oﬄine systems among the more
challenging projects in the high-energy physics community.

The high sensitivity needed for the study of CP -violation eﬀects and quantum

∗ Corresponding author: M. Moulson, Laboratori Nazionali di Frascati dell’INFN,
Via E. Fermi, 40, I-00044 Frascati (RM), Italy.

Email address: moulson@lnf.infn.it (M. Moulson).

2

interference patterns in the neutral-kaon system requires that experimental
systematics be kept under strict control. To this end, billions of events must be
generated, with the most accurate simulation possible of the detector response
and machine-background eﬀects.

KLOE data taking for physics began in the year 2000. A total integrated
−1 was collected by the end of 2002. KLOE data
luminosity of about 500 pb
collection is expected to resume at a rate of 10 pb

−1/day in 2004.

In this paper, we discuss the KLOE oﬄine data-processing system. We brieﬂy
describe the KLOE detector in Sec. 2. The main features of the data-processing
environment and the operation of the computer farm are discussed in Sec. 3.
The algorithms used in the reconstruction code and their implementation are
described in Sec. 4. The KLOE Monte Carlo and its use in event-simulation
campaigns is discussed in Sec. 5. In Sec. 6, we summarize and draw some
conclusions from our experience.

2 The KLOE detector

π+π− and KL →
For the discrimination of the CP -violating decays KL →
3π0 decays, we
π0π0 from the much more abundant KL →
require of the detector good momentum resolution for charged tracks, as well as
full solid-angle coverage and excellent energy and time resolution for photons.
Moreover, given the rather long mean decay length of the KL at DAΦNE
(3.4 m), a large detector is required in order to have reasonable geometrical
acceptance.

πµν and KL →

The KLOE detector is composed of two subdetectors: a large drift chamber
(DC) to measure charged tracks, and an electromagnetic calorimeter (EmC)
to detect photons. Both are immersed in the 0.52 T ﬁeld of a superconducting
solenoid.

×

The drift chamber [1] is a cylinder of 25 (198) cm inner (outer) radius and 332
cm length; it contains 12 582 drift cells distributed in 58 cylindrical layers. For
2 cm2, while for the 46 outer
the 12 inner layers, the cell dimensions are 2
3 cm2. In order to provide uniform coverage throughout
layers, they are 3
the chamber volume, all wires are stereo wires. The signs of the stereo angles
(with respect to the beam axis) alternate from layer to layer, and the mag-
nitude of the stereo angle for each layer gradually increases, from 60 mrad
for the innermost layer to 150 mrad for the outermost. The total number of
wires (sense + ﬁeld + guard) is about 52 000. The spatial resolution in the
rφ plane is about 150 µm; in the z direction, the spatial resolution depends on
the stereo angle and is about 2 mm. The chamber is ﬁlled with a gas mixture

×

3

of 90% helium and 10% isobutane. This low-Z mixture has been chosen to
reduce the eﬀects of regeneration, photon conversion, and multiple scattering,
where the latter has a particularly signiﬁcant eﬀect on the momentum res-
olution for tracked particles given the momenta involved in the experiment
0.4% for
(100-500 MeV/c). The transverse-momentum resolution is σpt/pt
large-angle tracks. Vertices inside the chamber are reconstructed with a spa-
tial resolution of
3 mm. The chamber was recently instrumented with ADCs
to supplement the experiment’s particle-identiﬁcation capability with dE/ dx
information for reconstructed tracks.

<
∼

∼

The electromagnetic calorimeter [2] is of the sampling type, and is made of lead
layers and scintillating ﬁbers, with a volume proportion of lead:ﬁber:epoxy =
42:48:10. The total thickness of the EmC is 23 cm, corresponding to about
15 X0. The EmC is composed of a barrel and two endcaps. The barrel is
divided into 24 modules. Each endcap is divided into 32 (vertical) modules,
which have a C shape to close the solid angle as much as possible. The light
from the ﬁbers is viewed by a photomultiplier tube (PMT) at each end to
determine the time of ﬂight and impact point along the direction of the ﬁbers.
The readout is segmented in depth into 5 planes (each 4.4 cm thick, except
for the outermost, which is 5.2 cm thick), and in the coordinate transverse to
the ﬁbers into columns 4.4 cm wide. In all, there are 4880 PMTs. To complete
the coverage of the solid angle, two small calorimeters, QCAL [3], made of
lead and scintillating tiles, are wrapped around the low-β quadrupoles. The
PMT signals (after an electronic delay of about 200 ns) are sent to ADCs for
amplitude analysis, to TDCs for time-of-ﬂight measurement, and to the trigger
modules. The energy resolution for photons is σE/E = 5.7%/√E(GeV) and
the time resolution is σt = [54/√E(GeV)
50] ps. The photon impact point
is measured with a precision of
1 cm
in the transverse coordinate.

1 cm/√E(GeV) along the ﬁbers and

∼

∼

⊕

∼

The trigger [4] is based on energy deposits in 88 calorimeter sectors (formed by
grouping adjacent readout elements) and on drift-chamber signals. The level-1
trigger, which starts data readout with minimal delay, requires energy deposits
above threshold (E > 50 MeV in the barrel, E > 150 MeV in the endcaps) in
two EmC sectors, or
15 DC wire signals within 250 ns. Low-angle Bhabha
events can be downscaled at this level. The level-2 trigger, which validates the
level-1 trigger, requires further multiplicity or geometrical conditions for EmC
120 DC wire signals within a 1.2 µs time window (the
energy deposits, or
maximum drift time is 1–1.5 µs, depending on cell size). A cosmic-ray veto is
applied at level 2. The acquisition dead time is about 2.7 µs (corresponding to
a 0.8% loss at a typical rate of 3 kHz). A level-3 trigger ﬁlter is implemented
in software to review and enforce the cosmic-ray veto decision made at level
2.

∼

The trigger is synchronized with a demultiplied DAΦNE radio-frequency signal

4

that corresponds to every fourth bunch crossing. (tsync = 4tbunch = 10.85 ns).
The association of the event with the proper bunch crossing, or determination
of the event-start time, is made during oﬄine reconstruction.

The DAQ system [5] handles about 23 000 front-end channels (ADC, TDC
and trigger modules) hosted in VME crates organized in ten chains. Sub-
events from each chain are sent through an FDDI switch to the online farm for
event building, formatting, and monitoring. The online farm consists of seven
IBM 7026-H50 SMPs, each with four 332-MHz PowerPC 604e processors. The
online servers write the raw-data ﬁles to 1.4 TB of locally mounted SSA disks.
The readout system has been designed for a sustained rate of 10 MB/s. At
1031 cm−2 s−1 during 2002, the trigger rate was
a typical luminosity of 5
1.6 kHz and the average event size was 2.7 kB, leading to a sustained data
acquisition rate of 4.3 MB/s, which was managed using three out of seven
online nodes.

×

3 The oﬄine computing environment

Raw data from the online systems are reconstructed on the KLOE oﬄine
farm. In this section, we ﬁrst give an overview of the procedure by which
raw data are reconstructed, divided into analysis streams, and then further
reduced into data-summary tape (DST) streams. (Monte Carlo production
is also performed on the oﬄine farm; the processing of Monte Carlo events
is described in Sec. 5.) We then describe the oﬄine hardware environment,
the data-handling system (which is common to both the online and oﬄine
environments), and the oﬄine software environment.

3.1 Overview of data processing

The event-builder processes running on the online farm write raw events to the
online disk pool in 1-GB ﬁles. Data taking is divided into runs of approximately
−1 in year 2002). Typically, about 20 raw-
equal integrated luminosity (200 nb
data ﬁles are written per run. For each run, the run number is used to uniquely
associate to the events

•
•
•

a set of calibration constants;
values for machine parameters such as energy, beam position, etc.;
quantities related to the detector status such as high- and low-voltage set-
tings, trigger thresholds, drift-chamber gas parameters, dead-channel lists,
etc.

5

All data are permanently stored in a tape library as described in Sec. 3.3. Raw-
data ﬁles are kept on disk until calibration and reconstruction are completed.
The archival of raw-data ﬁles and the availability of free space on the online
disk pool are managed by the data-handling system as described in Sec. 3.4.

For the drift-chamber calibration [6], two procedures are in use. The ﬁrst and
most commonly used procedure performs a fast analysis to test the valid-
ity of the most recent values of the calibration constants. This program runs
concurrently with data taking, using cosmic-ray events selected and buﬀered
by the DAQ system. The second procedure performs a complete analysis of
cosmic-ray muon tracks in the DC to update the calibration constants; it is
launched only if the existing calibrations fail to describe the detector perfor-
mance. This typically happens only a few times during an entire data-taking
period, essentially when the atmospheric pressure changes by more than 1%.
The drift-chamber calibration procedures are further described in Sec. 4.2.

For the calorimeter, the calibration procedure [2] is started at the end of each
run and lasts about two hours. The procedure uses Bhabha and γγ events
selected by the DAQ system: the 500 MeV photons are used to set the absolute
energy and time scales, while the higher-statistics sample of 500 MeV electrons
and positrons allows the equalization of the energy scale between diﬀerent
calorimeter columns. With an integrated luminosity of 200 nb−1, the time
scale is determined to within 10 ps, and the energy scale is accurate at the
percent level.

Various other processes running on the online servers perform on-the-ﬂy re-
construction of selected events to monitor the status of the detector and data-
taking conditions (such as hardware eﬃciencies, noise rates, machine energy,
and beam-spot position). The slow-control system combines these data with
hardware-status information (such as high- and low-voltage settings and dead-
channel maps); it also receives information from the DAΦNE control systems
on machine parameters (such as beam currents and number of bunches) and
sends information on the status of the experiment to the DAΦNE operators.
Monitoring information from all of these sources is summarized and written to
the central KLOE database described in Sec. 3.4. Geometry ﬁles and calibra-
tion constants, as well as some information on long-term detector conditions,
are stored using the CERN HEPDB database [7].

Event reconstruction is performed on the oﬄine farm. The reconstruction pro-
gram datarec starts immediately after the completion of the calibration jobs
for the run. Each of the 20 or so raw-data ﬁles making up the run are pro-
cessed in parallel by a separate reconstruction job. Each job produces one
reconstructed ﬁle for each analysis stream.

6

In practice, a single job manager periodically interrogates the database, iden-
tiﬁes new runs ready for processing, and starts jobs on the free CPUs of the
oﬄine farm. The status of these jobs and the overall status of the oﬄine farm
itself are monitored via the web interface to the slow-control system. The re-
construction jobs provide additional data-quality and monitoring information,
a summary of which is available from the slow-control web interface.

The reconstruction program datarec consists of several modules that per-
form the following tasks:

•
•

loading of DC and EmC calibration constants;
EmC cluster reconstruction from single cells and determination of deposited
energy and time of ﬂight;
determination of the correct bunch crossing;
rejection of machine-background and cosmic-ray events;
pattern recognition and track ﬁtting for charged particles in the DC;
vertex reconstruction for charged particles;
association of DC tracks with EmC clusters;
event classiﬁcation.

•
•
•
•
•
•
The algorithms developed for these tasks are described in Sec. 4.

The processing path for event reconstruction has been designed to ﬁlter out
machine-background and cosmic-ray events at an early stage, before tracking
in the DC, which is the most CPU-intensive reconstruction task. The ﬁlter
algorithm, filfo, is based only on information from the EmC, and is able to
cut out a signiﬁcant portion of background events.

For easier and faster access to the data sample, the last step of the recon-
struction procedure is the classiﬁcation of events on the basis of topological
information into diﬀerent ﬁles (or streams), to be used for diﬀerent physics
analyses. Currently, ﬁve streams are deﬁned, containing Bhabha scattering
π+π−π0
events, φ decays into charged kaons, φ decays into neutral kaons, φ
decays, and radiative φ decays.

→

The latter four streams undergo a further level of data reduction, in which
only the information used in the ﬁnal stages of physics analysis is retained.
The resulting set of data-summary tapes (DSTs) is about six times smaller in
size than the corresponding set of reconstruction output ﬁles, and can be kept
largely on disk for easy access by any user program. DST production is auto-
matically launched once a run has been completely reconstructed. Besides data
reduction itself, other tasks needed for the optimization of the reconstruction
of each stream are performed during DST production. For example, a reﬁned
track ﬁt is performed for events containing charged kaons. This ﬁt properly
uses the kaon mass in the treatment of energy loss and multiple scattering for
identiﬁed kaon tracks.

7

DB2 server
IBM F50 4×PPC604e 166

100 Mbps

1 Gbps

online farm
7 IBM H50 4×PPC604e 332 
1.4 TB SSA disk 

nfs

offline production farm
19 IBM B80 4×POWER3 375
8 Sun E450 4×UltraSPARC-II 400

CISCO
Catalyst
6000

AFS cell
2 IBM H70 4×RS64-III 340
1.7 TB SSA + 0.5 TB FC disk

afs

offline analysis farm
4 IBM B80 4×POWER3 375
2 Sun E450 4×UltraSPARC-II 400

nfs

file servers

2 IBM H80 6×RS64-III 500

nfs

managed disk space
6.3 TB

0.8 TB SSA: offline staging
1.5 TB SSA + 4.0 TB FC: latent disk cache 

tape library
324 TB

IBM 3494, 5400 60GB slots, 2 robots 
12 Magstar E1A drives

Fig. 1. KLOE computing hardware conﬁguration during 2001–2002.

Because of the continuous improvement in our understanding of the perfor-
mance of the detector and the increasing statistical sensitivity aﬀorded by the
growth of the data set, the calibration procedures and reconstruction algo-
rithms are in constant evolution. To allow physics analyses to beneﬁt from
the corresponding improvements in reconstruction quality, we periodically re-
process raw data that was originally processed with an earlier version of the
reconstruction code. During the ﬁrst four months of 2002, the data sample of
−1 collected in 2001 was completely reprocessed to include improve-
∼
ments to the timing calibration of the calorimeter, the background ﬁlter, and
the selection criteria for charged and neutral kaons. The 2001 and 2002 data
were thus reconstructed using an identical path and homogeneous code.

180 pb

3.2 Oﬄine farm

The conﬁguration of the KLOE computing hardware is schematically repre-
sented in Fig. 1.

The oﬄine farm consists of a mix of IBM 7026-B80 SMPs running AIX, each
with four 375-MHz Power3 CPUs; and Sun E450 SMPs running Solaris, each
with four 400-MHz UltraSPARC II CPUs. In all, 23 B80s and 10 E450s are
available, and provide a total processing power equivalent to about 110 of the
processors installed in the B80s, or about 30 000 SPECint2000.

The CPU time needed for data reconstruction and simulation is summarized
in Table 1. Here and throughout this paper, all CPU times are referred to a
single processor on one of the B80 servers. The CPU time needed for data
reconstruction depends on the eﬀectiveness of the filfo ﬁlter in rejecting

8

Task

CPU time/event (ms) CPU time/fb

−1 (days)

Data reconstruction

Data simulation (φ decays)

Monte Carlo reconstruction

20

200

175

9600

6650

5100

Table 1
CPU-time consumption for reconstruction and Monte Carlo simulation on the
KLOE oﬄine farm. All CPU times refer to a single processor on one of the B80
servers.

background events in the presence of variable data-taking conditions. The
entries in the table reﬂect the data-taking conditions in 2002, when filfo was
able to reduce the input rate by 60%. Such events are rejected immediately
after reconstuction in the EmC, which takes only 5 ms. For events passing the
ﬁlter, DC reconstruction takes about 40 ms, where this number is a sample-
30 ms),
weighted average of the reconstruction times for Bhabha events (
φ-decay events (
120 ms), and a small fraction of unrejected background
events (15–40 ms). Averaged over all input events, then, the time needed to
reconstruct an event is 20 ms.

∼

∼

Currently, about 80% of the processing power is used for production-related
tasks; the remainder is allocated to physics analysis tasks. Additional machines
can be opened to user batch and interactive sessions as the need arises. In this
conﬁguration, the total processing power allocated to production is adequate
for the purposes of data reconstruction in parallel with acquisition. Fig. 2
illustrates the progress of the 2002 data-taking campaign. The growth of the
reconstructed data set closely follows that of the acquired data set. From
the point of view of both hardware and software, the operation of the oﬄine
systems is seen to be smooth and reliable.

The time needed for DST production varies from stream to stream. This is
in part because of the diﬀerent abundances of selected events, and in part
because the algorithms applied vary in CPU intensity (as noted in Sec. 3.1,
K +K − events are completely re-reconstructed at the DST production stage).
−1/CPU hour for the K +K − stream,
DST-production rates range from 50 nb
−1/CPU hour for the radiative φ-decay stream. Processing of all four
to 600 nb
streams proceeds at 40 nb−1/CPU hour.

During the past three years of operation, the power of the oﬄine farm has
grown in parallel with the demands of the experiment, from 16 B80 CPU
equivalents in the year 2000, to the 110 currently available. As part of an
oﬄine-system upgrade for the year 2004, ten new IBM p630 servers, each
with four 1.45-GHz Power4+ processors, are currently being installed. This

9

)
1
-
b
p
(
 
y
t
i
s
o
n
i
m
u
l
 
d
e
t
a
r
g
e
t
n
I

25

20

15

10

5

0

a)

250 b)

)
1
-
b
p
(
 
y
t
i
s
o
n
i
m
u
l
 
d
e
t
a
r
g
e
t
n
I

200

150

100

50

0

2.5

5

7.5

10 12.5 15 17.5 20 22.5
Data-taking week

2.5

5

7.5

10 12.5 15 17.5 20 22.5
Data-taking week

Fig. 2. a) Integrated luminosity per week in 2002. b) Total integrated luminosity
vs. data-taking week in 2002. Histograms refer to the data taking; triangles refer to
the reconstructed sample.

increases the total CPU power of the oﬄine farm to about 225 B80 equivalents,
or about 60 000 SPECint2000. The upgrade will provide CPU power suﬃcient
for reconstruction, DST processing, and Monte Carlo production, simultane-
ously and in parallel with the acquisition of data at an average luminosity of
1

1032 cm−2 s−1.

×

3.3 Data storage, data access, and networking

Data are permanently stored in an IBM 3494 tape library. The library has
12 Magstar 3590 tape drives which can read and write at 14 MB/s, dual ac-
tive accessors, and space for about 5400 60-GB cartridges, for a maximum
capacity of about 324 TB. The library is maintained using IBM’s Tivoli Stor-
age Manager [8]. The library usage is summarized in Table 2. Note that the
−1) decreases from year to year be-
speciﬁc volume of the raw data (TB/pb
cause of background reduction due to better software ﬁlters and improved
DAΦNE operations. During the running period scheduled for 2004, we expect
that DAΦNE upgrades recently completed will allow us to collect a data set
−1. To store the new data, we will need at least an additional
of about 2 fb
300 TB of long-term storage capacity. To satisfy this need, we are currently
in the process of ordering a second tape library.

A 6.3-TB oﬄine-disk pool is used for data transfers to and from the library.
The disk pool consists of 4.0 TB of Fibre Channel (FC) and 2.3 TB of SSA
disks, conﬁgured in striping mode. Two IBM 7026-H80 SMPs running AIX,
each with six 500-MHz RS64-III CPUs and 2 GB of RAM, locally mount the

10

Year

Int. Lum. (pb

−1) Raw (TB) Recon. (TB) MC (TB) DST (TB)

2000

2001

2002

Total

20

180

288

488

21

47

33

101

7

18

27

52

5

7

12

24

-

3

4

7

Table 2
KLOE tape library usage at the end of 2002. The entries for DSTs include MC
DSTs. DSTs were not produced for the 2000 data. A total of 184 TB are currently
occupied.

oﬄine-disk pool and tape library and are used as ﬁle servers. With the two ﬁle
servers working in concert, aggregate I/O rates of over 100 MB/s have been
obtained.

Analysis jobs usually use DSTs as input. For the 2001–2002 data, the set of
DSTs occupies 4 TB; MC DSTs occupy an additional 3 TB. About 5.5 TB
of the oﬄine disk pool is used to cache ﬁles recalled from the tape library
by the data-handling system; copies of the bulk of the DSTs reside in this
cache for prompt access. The output from analysis jobs is written to user and
working-group areas on the KLOE AFS cell. The AFS cell is served by two
IBM 7026-H70 SMPs, each with four 340-MHz RS64-III CPUs, 850 GB of
SSA disks, and 250 GB of FC disks, for a total cell capacity of 2.2 TB. Users
can access the AFS cell from PCs running Linux on their desktops to perform
the ﬁnal stages of their analyses.

Network connections are routed through a Cisco Catalyst 6000 switch. The
ﬁle and AFS servers are connected to the switch via Gigabit Ethernet. Con-
nections to all other nodes are via Fast Ethernet.

3.4 Data handling

A diagram of the data-handling scheme is presented in Fig. 3.

When new data are acquired, the online servers write the raw ﬁles to the
online-disk pool. These ﬁles are then asynchronously archived to the tape
library over an NFS mount by the archiver daemon. The archiving processes
are tailored to minimize the number of tape mounts while guaranteeing enough
space on the disk pool.

Normally, reconstruction is performed while the raw ﬁles are still resident on

11

KID/nfs

offline
production

nfs

0.8 TB

archiver
(nfs)

user
analysis

afs

2.2 TB

KID

KID

DAQ
online

1.4 TB

archiver
(nfs)

324 TB

tape 
library

disk 
cache

recall

5.5 TB

Fig. 3. Schematic layout of KLOE data handling.

disk. For input to the reconstruction processes from the online disk, events are
either read across an NFS mount or served by the data-handling system using a
custom TCP/IP protocol, which is provided by the KLOE Integrated Dataﬂow
package (kid) [9]. Reconstruction output is written via NFS to the oﬄine-disk
pool, from which it is asynchronously archived to tape. DSTs for each run are
produced from the reconstruction output ﬁles, usually immediately after the
run has been completely reconstructed. In this case, the reconstructed events
may be read back in across the NFS mount for DST production. When ﬁles
already archived and deleted from the online- or oﬄine-disk pools must be
processed on the oﬄine farm, the recalld daemon restores the ﬁles from tape
to the recall disk cache, from where they are served to the oﬄine processes
using the kid protocol. The spacekeeper daemon ensures the availability of
disk space in the staging areas by deleting ﬁles that have been archived. The
successful completion of calibration, reconstruction, and archival are signaled
by ﬂags in the database (see below).

The same model for data access used for reconstruction applies to user analysis
jobs running on the oﬄine farm. In principle, users may need to analyze raw,
reconstructed, or DST ﬁles. If the ﬁles requested are resident on the online-
or oﬄine-disk pools, they are copied to the recall disk cache by recalld to
be served to the user processes; otherwise, they are restored to the recall disk
cache from tape. A filekeeper daemon ensures the availability of free space
in the recall areas, deleting old ﬁles when necessary to make space for newly
recalled data.

A central database based on IBM’s DB2 [10] is used to keep track of the
locations of the several million ﬁles comprising the data set [11]. Each ﬁle
is logged in the database when it is created. The database entry contains
the reconstruction status of the ﬁle, allowing ﬁles that require processing to
be easily identiﬁed. This database also contains run-by-run information on

12

data-taking conditions and operational parameters of the detector, as noted
in Sec. 3.1.

The backbone of the data-handling system is the kid package, which con-
sists of two pieces: a centralized data-handling daemon, which coordinates
the distributed ﬁle-moving services; and a client library, with an easy-to-use
URL-based interface that allows access to ﬁles independent of their locations.
kid URLs may incorporate SQL queries used to interrogate the ﬁle database.
Examples of such URLs include:

•

•

All raw ﬁles in the stated run range that have not yet been reconstructed:
dbraw:run nr between 23000 and 24000 and analyzed is not null
All reconstructed ﬁles in the KSKL stream for a given run:
dbdatarec:run nr = 23015 and stream code = ksl

3.5 Software environment

The datarec program is built upon the framework provided by the analy-
sis control (a c) package developed at FNAL [12]. a c provides the tools
for building the executable from KLOE analysis modules, as well as a user
interface that allows the processing sequence and choice of enabled streams
to be speciﬁed at run time. In order to use a c in the KLOE environment,
numerous customizations of the library have been implemented; in particular,
the kid package (Sec. 3.4) has been seamlessly interfaced. The source code
versions for analysis modules used in the datarec program are tracked using
cvs [13].

The data format consists of independent collections of tabular data structures,
or banks, for each event. They are read and written using the ybos package
[14], which provides tools for platform-independent memory management and
for the deﬁnition of tabular data structures that can be manipulated in Fortran
code.

An interface to the zlib library [15] has also been added to a c to allow reading
and writing of compressed data. The compression/decompression routines are
transparently called from a c internals. A compression factor of about 0.6 is
obtained for reconstructed output.

3.6 Analysis considerations

In addition to production jobs, user analysis jobs also run on the oﬄine farm.
In 2003, about 20% of the oﬄine CPU power was avaliable to users for the

13

production of histograms and Ntuples. About two-thirds of the machines open
to user sessions were reserved for batch jobs, with queues managed by IBM’s
LoadLeveler [16].

As an example of the execution time for user jobs, consider the analysis of
108 events in 1.4 TB of
the 2001–2002 KSKL data set, which consists of 4.5
DSTs, the majority of which are resident on disk in the recall disk cache for
prompt access. With six batch jobs running in parallel (the default per-user
maximum), the entire data set can be analyzed in six days elapsed. The output
size ranges from 10 to 100 GB, which can be accessed in situ on the AFS cell
or copied oﬀ to a user’s desktop PC.

×

4 Reconstruction program and algorithms

4.1 Reconstruction algorithms for the drift chamber

The track-reconstruction algorithms [17] are based on the program developed
for the ARGUS drift chamber [18]. This program has been adapted to the
all-stereo geometry of the KLOE DC and tuned to the speciﬁc topology of
KLOE events to optimize the eﬃciency of vertex reconstruction throughout
the DC volume. The detailed DC geometry, the space-time (s–t) relations
for the diﬀerent types of drift cells, and the map of the magnetic ﬁeld are
described in detail in the database. Event reconstruction is performed in three
steps: 1) pattern recognition, 2) track ﬁtting, and 3) vertex ﬁtting. Each step
is handled separately and produces the input information for the subsequent
step; this information is stored in ybos banks.

The ﬁrst step of the track-reconstruction chain is pattern recognition (PR).
The PR algorithm searches for track candidates and provides rough estimates
of their parameters. Track segments are ﬁrst searched for in the xy plane;
then the z projections are obtained. In an axial drift chamber, the particle
trajectory in the xy plane is well approximated by a circle (except for cor-
rections due to energy loss and multiple scattering, which are negligible at
the PR stage). In the KLOE DC, since the wires are strung with a stereo
angle, a particle leaves a pattern that appears as two nearby circles, one for
each stereo view. The PR algorithm ﬁrst searches for track candidates in each
stereo view. Starting from the outermost layer, hit chains are built up by as-
sociating hits close in space on the basis of curvature compatibility. In order
to resolve left-right ambiguities, a minimum of four hits in at least two wire
layers are required to create a single-view track candidate.

At the end of the hit-association stage for each view, a ﬁlter exclusively assigns

14

hits shared between track candidates to the better candidate. Each track can-
didate is then ﬁtted and its parameters are computed. The track candidates
from the two views are then combined in pairs according to their curvature
values and geometrical compatibility. Finally, the z projection for each pair
is determined from a three-dimensional ﬁt to all associated hits. At the PR
stage, the magnetic ﬁeld is assumed to be homogeneous, multiple scattering
and energy loss are not treated, and rough s–t relations (see Sec. 4.2) are used.

The track-ﬁtting (TF) procedure minimizes a χ2 function based on the com-
parison between the measured and expected drift distances for each hit. Re-
current tracing relations are used at each step to determine the positions of
successive hits from the estimated track parameters and the rough s–t rela-
tions; the drift distance is then corrected using more reﬁned s–t relations that
depend on the track parameters. Drift distances are recalculated with each
iteration of the ﬁt to make use of the previous determination of the track
orientation with respect to the cell.

Tracks are described by connected helical segments. Local variations in the
magnetic ﬁeld are taken into account at each step, together with the eﬀects of
energy loss and multiple scattering. The momentum loss between consecutive
hits is computed assuming the pion mass. Multiple scattering is accounted for
by dividing the track into segments such that the estimated transverse dis-
placement due to multiple scattering over the length of the segment is smaller
than the spatial resolution. The values of the eﬀective scattering angles in the
transverse and longitudinal planes are then treated as additional parameters
in the track ﬁt.

After a ﬁrst iteration, a number of procedures improve the quality of the track
ﬁt. In particular, dedicated algorithms are used to

check the sign assignment of the drift distance hit by hit;
add hits that were missed by the PR algorithm;
reject hits wrongly associated to the track by the PR algorithm;
identify split tracks and join them;
identify kinked tracks and split them.

•
•
•
•
•
As an example of the performance of the TF procedure, in Fig. 4 we illustrate
the momentum resolution for Bhabha events as a function of the polar angle
θ. Over a large range in θ, σp/p is
0.3%. The deterioration of the resolution
at low angle is in accordance with the expected cot θ behavior.

∼

At the end of the DC-reconstruction chain, the tracks from the TF procedure
are used to search for primary and secondary vertices. For each track pair, a
χ2 function is evaluated from the distances of closest approach between tracks;
the covariance matrices from the TF stage are used to evaluate the errors. The
vertex position is determined by minimizing this χ2. To reduce the number of

15

)

%

(
 
 

p
/
p

1.2

1

0.8

0.6

0.4

0.2

0

40

60

80

100

120

140
  (deg)

Fig. 4. Momentum resolution σp/p as a function of polar angle θ, for Bhabha events.

combinations, the tracks are ﬁrst extrapolated to the beam-crossing point in
the transverse plane and primary vertices are searched for using tracks with
an impact parameter smaller than 10% of their radius of curvature. Secondary
vertices are then searched for among tracks not associated to any other vertex.
For tracks that intersect the beam-pipe or inner DC walls, in the extrapolation,
the track momentum is corrected for energy loss and the eﬀect of multiple
scattering is taken into account in the covariance matrix. The pion mass is
assumed for the evaluation of these corrections.

For vertices inside the beam pipe, the vertex-position resolution is about 2 mm
in x, y, and z. In Fig. 5, we show the distribution of the vertex-position
π+π− decays. The ππ invariant-mass distributions
residuals in x for MC KS →
π+π− decays in data and MC samples are compared in Fig. 6. The
for KS →
mass resolution for this decay is seen to be

0.8 MeV/c2.

∼

Work is in progress on an algorithm to calculate the speciﬁc ionization dE/ dx
for reconstructed tracks on the basis of the charge measurements from the
ADCs recently added to the DC readout electronics.

4.2 Calibration of the space-time relations

Several eﬀects inﬂuence the time response of the KLOE DC. The drift velocity
of the helium-based gas mixture does not saturate with the electric ﬁeld, so
the relation between the drift time and the impact parameter of the track is
not linear. Moreover, due to the geometry of the drift cells, the electric ﬁeld
conﬁguration changes along the wire. This eﬀect produces a dependence of the

16

q
s
s
t
n
e
v
E

80000

70000

60000

50000

40000

30000

20000

10000

0

s
t
n
e
v
E

2250

2000

1750

1500

1250

1000

750

500

250

-3

-2

-1

0

1

2

3

Reconstructed - true X-coordinate of vertex (cm)

Fig. 5. Distribution of vertex-position residuals in x for MC KS →

π+π− events.

Data

MC

0
492

494

496

498

500

502

504
 (MeV/c2)

mp

Fig. 6. Invariant mass distributions for KS →
show the distributions for data and MC events, respectively.

π+π− events. Points and histogram

space-time (s–t) relations upon the orientation of the track and its position
along the wire.

Simulations have shown that the s–t relations can be parameterized in terms
of the angles β and ˜φ deﬁned in Fig. 7 [19]. Six cells with diﬀerent values of β
have been chosen as reference cells. For each reference cell, the s–t relations
are parameterized for 36 bins in ˜φ, each 10◦ wide. Since only the upper half of
the cell is deformed, in 20 of the bins in ˜φ, the s–t relations are the same for
all six reference cells. There are therefore a total of 16
6 + 20 = 116 param-
eterizations for the small cells, and 116 for the large cells. Each s–t relation

×

17

p
β

φ~

track

Fig. 7. Deﬁnition of the angles β and ˜φ.

is represented as a 5th-order Chebyshev polynomial [20], tdrift = PCheb(C k
where tdrift is the measured time, d is the impact parameter, and the 6
coeﬃcients C k
relations as described above.

i , d),
232
i (k = 1, . . . , 232 and i = 0, . . . , 5) parameterize the “ﬁne” s–t

×

The s–t relations are determined using cosmic-ray events, which illuminate
the chamber volume nearly uniformly and cover the entire range in the angle
˜φ. At the PR level, the values of ˜φ and β for each cell are unknown, since the
trajectory of the particle has not yet been determined. At this level, the cell
response is therefore described by a single s–t relation, which is an average
over all track orientations and drift-cell shapes. This “raw” s–t relation is
parameterized by the sum of three polynomials.

There are four contributions to the signal arrival time for each wire:

t = tTOF + twire + tdrift + t0.

(1)

Here, tTOF is the particle time of ﬂight up to the wire hit, twire is the propaga-
tion time of the signal along the wire, tdrift is the drift time, and t0 is a time
oﬀset. The oﬀsets t0 are calculated using cosmic-ray events at the beginning
of each data-taking period (i.e., every few months), or whenever the readout
electronics are reconﬁgured. About 107 events are required in order to obtain
the t0 estimates. The tdrift + t0 terms are isolated by computing tTOF and twire
event-by-event, approximating cosmic-ray tracks by straight lines [6].

Calibration of the s–t relations is performed by an iterative procedure which
reconstructs tracks, checks the residuals (the diﬀerence between the impact
parameters estimated using the existing s–t relations and those given by the
track ﬁt), and, if required, produces a new set of calibration parameters. The

18

)

m

(
 
n
o
i
t
u
l
o
s
e
R

500

450

400

350

300

250

200

150

100

50

0

0

)

m

(
 
n
o
i
t
u
l
o
s
e
R

500

450

400

350

300

250

200

150

100

50

0.2

0.4

0.6

0.8

1

Drift distance (cm)

0

0

0.2

0.4

0.6

0.8

1

1.2
Drift distance (cm)

1.4

Fig. 8. Spatial resolution as a function of the impact parameter for small (left) and
large (right) cells.

procedure starts by reconstructing a calibration sample (typically, cosmic-ray
events) with the standard PR and TF algorithms. The mean residuals as a
function of reconstructed impact parameter are then obtained for each set
of hits corresponding to each of the 232 s–t relations. The impact parameters
estimated from the drift time of each hit are then corrected by the correspond-
ing value of the mean residual, and the tracks are reconstructed again. The
iteration is halted when for each of the 232 parameterizations, the corrections
are smaller than 40 µm for hits in the central part of the drift region of their
cells. Finally, the 232 ﬁne s–t relations are ﬁtted, and the new coeﬃcients C k
i
are calculated.

The calibration program is incorporated into the KLOE online system. A syn-
chronous procedure automatically starts at the beginning of each run, and se-
lects 80 000 cosmic-ray events from the event-building nodes using kid. These
events are then tracked using the existing s–t relations, and the absolute value
of the average of the residuals for hits in the central part of the drift region
is monitored. If this value exceeds 40 µm, 300 000 cosmic-ray events are col-
lected, and the asynchronous procedure described above produces a new set
of calibration constants. Depending on background conditions, the ﬁlters on
the farm select events at a rate between 25 and 30 Hz. The event collection
therefore takes therefore about 3 hours, and a comparable amount of time
is needed for the analysis [6]. A complete recalibration is only necessary a
few times per data-taking period, essentially when the atmospheric pressure
changes by more than 1%.

Fig. 8 shows the resolution averaged over all wires as a function of the recon-
structed impact parameter. The spatial resolution is better than 200 µm over
a large part of the drift cell.

19

m
m
4.3 Momentum calibration

→

→
π+π−, KL →

e+e−, e+e−
πℓν, KL →

π+π−π0, KS →
→
π±π0, and K ±
→

The calibration of the absolute momentum scale was performed with the
−1 data sample collected in 2000 [21], in parallel with a survey of the
2.4 pb
mechanical distortions of the chamber and calibration of the space-time rela-
tions. Two- and three-body processes such as e+e−
µ+µ−,
→
π+π−π0,
π+π−, KL →
e+e−
K ±
µ±ν were employed. Depending on the process, the
invariant mass, missing mass, or secondary momentum in the rest frame of
the decaying particle was reconstructed; deviations from the nominal values of
these quantities were used as benchmarks for the calibration procedure. This
approach allowed the investigation of distortion eﬀects over the entire volume
of the detector and the full range of momentum. Initially, the reconstructed
momenta of low-angle Bhabha electrons deviated from the expected values
by as much as 8 MeV/c. In general, the deviations showed a complex depen-
dence on momentum, polar angle, azimuthal angle, and production point of
the tracked particle.

Two main sources of distortions were identiﬁed:

(1) Measurement artifacts in the magnetic-ﬁeld map

The magnetic ﬁeld was mapped with a mechanical system for the posi-
tioning of an array of Hall probes at nominal ﬁeld values of 0.3, 0.45,
and 0.6 T before the DC was inserted into the solenoid [22]. In 2001,
the maps were reexamined, with extra terms introduced to account for
distortions due both to misalignment of the Hall probes with respect to
their nominal positions on the arm spanning the solenoid volume, and to
rotations and translations of the arm with respect to its nominal position
in the KLOE reference system. Most of these geometrical eﬀects could
be isolated because the measurements were performed twice: ﬁrst with
the measurement arm moving from one end of the solenoid to the other,
and then in the opposite direction, with the orientation of the measur-
ing device reversed. Artifact ﬁeld components thus appeared in the sum
or diﬀerence of measurements performed by the same probe or by two
neighboring probes. The typical size of artifact ﬁeld components in the
transverse plane was about 0.004 T.

(2) Saturation of the magnetic ﬁeld

For optimum DAΦNE performance, KLOE must work at a nominal ﬁeld
value of about 0.52 T. A comparison of the maps at the three diﬀerent
nominal ﬁeld values showed evidence for saturation. The eﬀect was also
found in a set of very precise measurements of the ﬁeld as a function of
current performed on the solenoid axis using an NMR probe. The NMR
data showed deviations from linearity as large as 1%, increasing with
distance from the solenoid axis and decreasing with distance from the

20

endplates. Global corrections for the saturation of the longitudinal ﬁeld
component were applied using the shape of the excitation curve obtained
by the NMR probe; local corrections were applied by interpolation of the
three maps. Unfortunately, global saturation corrections for the trans-
verse ﬁeld components could not be computed. These corrections are
thought to be on the order of 0.001 T in magnitude.

With these corrections, low-angle Bhabha electrons are reconstructed with
systematic momentum deviations of less then 500 keV/c, or approximately
0.1%. Similar accuracy is found for all benchmark modes. The residual sys-
tematic diﬀerences can be ascribed to interpolation error in the saturation
correction.

4.4 Reconstruction algorithms for the calorimeter

The calorimeter is segmented into 2440 cells, which are read out by PMTs at
each end (referred to as sides A and B in the following). Both charges QA,B
ADC
and times tA,B
TDC are recorded. For each cell, the particle arrival time t and the
impact point s along the ﬁber direction are reconstructed using the times at
the two ends as

t = 1

s = v

2(tA + tB
tB
2 (tA

−

−

−

tB
tA
0 )
0 −
tA
0 + tB

0 ),

−

L
2v ,

TDC, where cA,B are the TDC calibration constants, tA,B

with tA,B = cA,B tA,B
are
the overall time oﬀsets, and L and v are the cell length and the light velocity
in the ﬁbers. The impact position in the transverse direction is provided by
the locations of the readout elements.

0

The energy signal Ei on each side of cell i is determined as

Ei = κE gi(s)

Si
Smip, i

,

Q0, ADC is the charge collected after subtraction of the zero-
where S = QADC −
oﬀsets, and Smip is the response to a minimum-ionizing particle crossing the
calorimeter center. The correction factor g(s) accounts for light attenuation
as a function of the impact position s along the ﬁber, while κE is the overall
energy scale factor. The ﬁnal value of Ei for the cell is taken as the mean of
the determinations at each end.

21

(2)

(3)

0

The calibration constants related to minimum-ionizing particles, Smip and g,
are acquired with a dedicated trigger before the start of each long data-taking
period. The time oﬀsets tA,B
and the light velocity v in the ﬁbers are evaluated
every few days using high-momentum cosmic rays selected using drift-chamber
information. In this iterative procedure, the tracks reconstructed in the drift
chamber are extrapolated through the calorimeter, and the residuals between
the expected and measured times for each cell are minimized. Finally, a pro-
cedure to determine the value of κE and to reﬁne the values of tA,B
runs
online [5]; it uses Bhabha and e+e−
γγ events to establish a new set of
→
constants each 100–200 nb−1 (i.e., approximately every 2 hours during nor-
mal data taking). The procedures used to calibrate the calorimeter are further
discussed in Ref. 2.

0

Calorimeter reconstruction starts by applying the calibration constants to con-
vert the measured quantities QADC and tTDC to the physical quantities S and t.
Position reconstruction and energy/time corrections vs. s are then performed
for each ﬁred cell. Next, a clustering algorithm searches for groups of cells
belonging to a given particle. In the ﬁrst step, cells contiguous in rφ or xz
are grouped into pre-clusters. In the second step, the longitudinal coordinates
and arrival times of the pre-clusters are used for further merging and/or split-
ting. The cluster energy, Ecl, is the sum of the energies for all cells assigned
to a cluster. The cluster position, (x, y, z)cl, and time, tcl, are computed as
energy-weighted averages over the contributing cells. Cells are included in the
cluster search only if times and amplitudes are available on both sides; oth-
erwise, they are listed as “incomplete” cells. The available information from
most of the incomplete cells is added to the existing clusters at a later stage
by comparison of the (x, y) positions of such cells with the cluster centroids.

|

|

→

∆x
|

∆x
|

The production of fragments from electromagnetic showers has been studied
by comparing data and Monte Carlo samples of e+e−
γγ events, with tight
selection cuts applied to the two highest-energy clusters in the event (the
between
“golden photons”). The distribution of the minimum distance
the golden photons and any of the other clusters is characterized by reason-
able agreement between data and MC at large values of
; at low values
of
an appreciable discrepancy is observed. In this latter case, a simi-
lar discrepancy is observed for the distribution of the diﬀerence in time, ∆t,
between the selected clusters. The multiplicity of fragments in data exceeds
that in MC events by about a factor of two and is dominated by clusters with
energy below 50 MeV. We attribute these discrepancies to small inaccuracies
in the descriptions of the shower development and time response in the Monte
Carlo, so that the longitudinal cluster-breaking procedure performs diﬀerently
for data and MC events. For this reason, depending upon the multiplicity of
photons in the event, a split-cluster recovery procedure is applied at the anal-
, ∆t, and
ysis level to merge close clusters depending on their values of
energy.

∆x
|

∆x
|

|

|

22

s
t
n
e
v
E

a)

1000

s
t
n
e
v
E

b)

3000

2500

2000

1500

1000

500

0

800

600

400

200

0

100

150

200

400

500

mp  (MeV/c2)

600
mK (MeV/c2)

Fig. 9. Reconstructed invariant-mass distributions for a) π0’s and b) KS’s from
KS →

π0π0 events. Points are for data; solid line is for MC.

→

The energy, timing, and position resolutions for photons are measured using
π+π−π0 and radiative Bhabha samples. In both cases, the energy
e+e−
Eγ and direction ˆpγ of one of the photons are predicted with high precision
using only tracking information. The calorimeter response and resolution as
a function of the photon polar angle θγ and energy Eγ can therefore be pa-
rameterized, and the photon detection eﬃciency can be measured with high
accuracy.

≈

∼

∼

The energy response as a function of Eγ shows a linearity better than 1%
down to 60 MeV, while a drop in the response of
3% is observed at low en-
ergy. This is mostly due to imperfect recovery of the “incomplete” cells. The
energy resolution is dominated by sampling ﬂuctuations and is well parame-
terized as 5.7%/√E(GeV). The light yield has been estimated by looking at
the ﬂuctuations in the ratio of the energy response at each side, EA/EB, and
700 photoelectrons per side for 1 GeV photons impinging at
corresponds to
2.7%/√E(GeV)]. The timing
the center of the calorimeter [σE/E, p.e. stat.
resolution has also been determined; the stochastic term is dominated by the
light yield, and scales as 54 ps/√E(GeV), while a constant term of 140 ps
must be added in quadrature to account for the jitter introduced by rephasing
the KLOE trigger with the machine RF. The contribution due to the preci-
sion of the channel-by-channel calibration is estimated to be
50 ps. In the
transverse coordinates, the position resolution is dominated by the readout
4.4/√12 cm, while in the longitudinal coordinate, s, it
granularity, and is
shows the expected 1.2 cm/√E(GeV) energy dependence. The reconstruction
of the masses of neutral mesons (π0, η, KS, KL) decaying to n-photon ﬁnal
states shows that, at KLOE energies, the mass resolution is completely dom-
inated by the energy resolution, while the mass scale is set with an accuracy
better than 1%. In Fig. 9, we compare the distributions of reconstructed π0
and KS masses for KS →

π0π0 events from data and MC.

∼

∼

23

4.5 Determination of the absolute time scale and event-start time

To run at the design luminosity, DAΦNE can operate with 120 bunches per
ring, which corresponds to a bunch-crossing period equal to the machine RF
period, tRF = 2.715 ns. Due to the large spread of the particle arrival times
and short bunch-crossing period, the trigger time does not identify the bunch
crossing that produced an event; the time at which this bunch crossing oc-
curred must therefore be determined oﬄine. In order not to spoil the excellent
EmC time resolution, the start to the TDC system is obtained by synchro-
nizing the level-1 trigger with a clock that is phase-locked to the DAΦNE
radiofrequency signal. The clock period is 4tRF = 10.85 ns. The calorimeter
times are measured in common-start mode and are given by the TDC stops
from the discriminated PMT signals:

tcl = tTOF + δc −

NBC tRF,

(4)

where tTOF is the time of ﬂight of the particle from the event origin to the
calorimeter, δc is the sum of all oﬀsets due to electronics and cable delays, and
NBC tRF is the time needed to generate the TDC start (see Fig. 10).

→

The quantities δc and tRF are determined using e+e−
γγ events. For such
rcl/c shows well-separated peaks corre-
events, the distribution of ∆TOF = tcl −
sponding to the diﬀerent values of NBC for events in the sample (see Fig. 11a).
We deﬁne δc as the position of the largest peak in the distribution, and obtain
tRF from the distance between peaks. This is done by calculating the dis-
crete Fourier transform of the ∆TOF distribution and ﬁtting the peak around
ν = 1/tRF (see Fig. 11b). The absolute TDC time scale is obtained by impos-
ing tRF(ﬁt) = tRF. Both δc and tRF are determined with precision better than
4 ps for every 200 nb

−1 accumulated.

π0π0), we found
While measuring the ratio BR(KS →
0.8% to the time scale to
it necessary to apply an absolute correction of
eliminate an observed dependence of β∗
K on the trigger-formation time [23,24].
The error on the time scale was found to originate from two cooperating eﬀects:

π+π−)/BR(KS →
∼

•

•

As seen from the distribution of ∆TOF as a function of zcl in Fig. 11c, the
characteristic value of NBC in e+e−
γγ events varies as a function of
longitudinal position along the barrel. This is due to the light-propagation
time in the ﬁbers, which is the dominant delay in trigger-signal formation.
Because of a residual slewing eﬀect, for any given value of NBC, ∆TOF de-
pends on zcl, as seen from Fig. 11d.

→

When taken together, these two eﬀects lead to an error in determining the
distance between the peaks in the ∆TOF distribution. Since 2001, we have

24

2.715 ns

1

2

3

4

5

6

7

8

9

10

RF

t(phi)

RF/4

Ttrg(LVL1)

10.86 ns

Ttrg(LVL1+rephasing)

T(true) = T(flight)

T(stop)

T(flight) + D

T (reph)

TDC start

T(measured)

TDC(stop)

Fig. 10. Timing scheme for bunch-crossing signal, calorimeter signals, and level-1
trigger formation.

corrected for the dependence of ∆TOF on zcl using an ad hoc procedure before
0.7% correction to the
calibrating the calorimeter. This provides a stable
time scale.

−

δc −

Since we want the cluster times to correspond to particle times of ﬂight, a
NBC tRF must be subtracted from all cluster times
time oﬀset t0, evt ≡
(see Eq. [4]). The trigger-formation time NBC tRF varies on an event-by-event
basis; it is determined oﬄine at diﬀerent points of the reconstruction path. A
zeroth-order value for NBC (and hence t0, evt) is obtained by assuming that the
earliest cluster in the event is due to a prompt photon from the interaction
point. By imposing tTOF = rcl/c for this cluster, we obtain

t0, evt = δc −

Nint

"

−
tRF

tRF,

#

rcl/c

tcl + δc

(5)

where Nint[ ] stands for the nearest integer to the quantity in brackets. We
refer to t0, evt as the event-start time.

25

s
t
n
e
v
E

a)

b)

Const
Mean
Sigma

 4.7510
0.3704
0.0184

50

60

70
t- r/c (ns)

0.3

0.4

0.5

 (GHz)

)
s
n
(
 
c
/
r

t-

c)

)
s
n
(
 
c
/
r

t-

d)

2

)

(

G

/

2

)

(
F

5

4

3

2

1

0

58.8

58.6

58.4

58.2

58

57.8

57.6

350

300

250

200

150

100

50

0

80
75
70
65
60
55
50
45
40
35
30

-200

-100

0

100

200

-200

-100

0

100

200

z (cm)

z (cm)

Fig. 11. Calibration of EmC time scale using e+e−
γγ events: a) Distribution of
∆TOF, b) Detail of the peak at ν = 1/tRF in the discrete Fourier transform of the
∆TOF distribution, c) ∆TOF as a function of zcl, d) ∆TOF as a function of zcl for a
single peak in the ∆TOF distribution, corresponding to a single value of NBC.

→

Soft clusters coming from the accidental coincidence of machine-background
events with the e+e− collision can arrive earlier than the fastest cluster from
the collision event itself. To increase the reliability of the estimate of t0, evt, the
cluster used for its evaluation must also satisfy the conditions Ecl > 50 MeV
and (x2

cl)1/2 > 60 cm.

cl + y2

4.6 Track-to-cluster association

The track-to-cluster association module establishes correspondences between
tracks in the drift chamber and clusters in the calorimeter.

The procedure starts by assembling the reconstructed tracks and vertices into
decay chains and isolating the tracks at the ends of these chains. For each
of these tracks, the measured momentum and the position of the last hit in

26

n
w
w
the drift chamber are used to extrapolate the track to the calorimeter. The
extrapolation gives the track length Lex from the last hit in the chamber to the
calorimeter surface, and the momentum pex and position xex of the particle at
the surface. The resulting impact point is then compared with the positions xcl
of the reconstructed cluster centroids. A track is associated to a cluster if the
distance to the centroid in the plane orthogonal to the direction of incidence
of the particle on the calorimeter, Dtcl =
, is less than
30 cm. For each track, the associated clusters are ordered by ascending Dtcl
values.

xex) × pex/

(xcl −
|

pex||

|

Various event-classiﬁcation algorithms classify clusters as due to neutral or
charged particles. Most of these algorithms treat clusters as due to neutral
particles if no associated tracks are identiﬁed by the track-to-cluster associa-
tion module.

While the standard track-to-cluster association algorithm provides the infor-
mation necessary to estimate the arrival time for a charged particle at the
surface of the calorimeter, the interval between the time of particle incidence
and the measured cluster-centroid time, ∆tEmC, can be signiﬁcant, and must be
taken into consideration in time-of-ﬂight based particle-identiﬁcation schemes.
For example, for π+’s which interact deeply (25–30 cm) in the calorimeter,
∆tEmC can be as much as 1 ns, as compared to a time of ﬂight of
8 ns. This
time interval directly reﬂects the temporal proﬁle of the energy deposition for
the incident particle, and varies by particle species. For each species (e+, e−,
µ+, µ−, π+, and π−), a simple, linear parameterization can be used to relate
∆tEmCi
to the depth of the centroid along the direction of particle incidence.
h
Because of residual diﬀerences between the temporal shower proﬁles observed
in data and simulated in the Monte Carlo, these parameterizations have been
performed separately using data and MC events. They are available for use in
calculating expected particle times of ﬂight at the analysis level.

∼

4.7 Event classiﬁcation

The KLOE event-classiﬁcation library is composed of diﬀerent modules for
the identiﬁcation of the major physics channels at DAΦNE. The main classi-
ﬁcation algorithms include those for the identiﬁcation of

generic background: beam background, cosmic-ray muons, and fragments of
small-angle Bhabhas;
large-angle Bhabhas and e+e−
tagged KL or KS decays;
tagged K + or K − decays;
π+π−π0 decays;
φ

γγ events;

→

•

•
•
•
•

→

27

•

π+π− + nγ and fully neutral nγ ﬁnal states coming from various primary
processes such as e+e−
→
f0(980)γ or a0(980)γ, etc.

ηγ or η′γ, e+e−

π+π−γ, e+e−

→

→

→

→

φ

φ

Background events are discarded, while all of the other samples are separately
archived (see also Sec. 3.1). In the following, we discuss the criteria used to
identify events in each of these categories.

The background-rejection algorithm is based on calorimeter clustering and
DC hit counting, so that background events can be eliminated before DC re-
construction, which is the most CPU-intensive section of our reconstruction
program. For the identiﬁcation of background events, cuts are applied on the
number of clusters; the number of DC hits; the total energy in the calorimeter;
the average polar angle, position, and depth of the (two) most energetic clus-
ter(s); and the ratio between the number of hits in the innermost DC layers
and the total number of DC hits. These cuts have been studied to minimize
losses for physics channels. Additionally, a simple cut on anomalously high
total energy deposits in the calorimeter is included to reject rarer machine-
background topologies due to sporadic DAΦNE beam-loss events.

The KLOE trigger system includes a veto for cosmic-ray muons that uses
dedicated thresholds on the energy deposition in the outermost layer of the
0.6 kHz out
calorimeter. Cosmic-ray events that survive the trigger veto (
of
3 kHz) are rejected by the background ﬁlter by identiﬁcation of at least
one cluster pair with relative timing, total energy deposition, and energy re-
leased in the outermost calorimeter layer consistent with those expected for a
relativistic muon.

∼

∼

Small-angle Bhabha electrons can strike the focusing quadrupoles and shower
inside the magnets and/or the QCAL calorimeter. Fragments from these show-
ers are sometimes suﬃcient to trigger the experiment. Events of this type are
identiﬁed by the presence of spatially concentrated clusters on the endcap
calorimeters that arrive within a narrow time window.

→

Large-angle Bhabha and e+e−
γγ events are selected to calibrate the
calorimeter and to evaluate the luminosity. These events are identiﬁed us-
ing only calorimetric information. They must have at least two clusters with
energy 300 MeV < Ecl < 800 MeV and polar angle between 45◦ < θ <
135◦. These clusters must arrive within a narrow time window and have
< 10◦. A stringent cut on the angle between the two most
180◦
|
energetic clusters, x1·x2/
0.975, is used to separate γγ events from
Bhabhas.

θ1 −

x1||

x2|

θ2|

−

−

<

|

A more precise measurement of the integrated luminosity is obtained by reﬁn-
ing the large-angle Bhabha event selection with track reconstruction informa-
tion. In particular, the two tracks in the event with the greatest number of asso-

28

ciated DC hits must be of opposite charge and have momenta p > 400 MeV/c
and polar angles 55◦ < θ < 125◦. The agreement obtained for the distributions
of important quantities such as the energy and angle of the Bhabha clusters
for data and Monte Carlo events (generated with babayaga [25, 26]) demon-
strates that the event counting in the ﬁducial angular region is accurate to
the same level as the precision of the generator itself.

At KLOE, it is possible to tag KS, KL, K +, and K − beams: the presence
of a KS (KL) signals the presence of a KL (KS) on the opposite side of the
detector, and the same applies for K +’s and K −’s. Pure KL beams are tagged
π+π− decay. One charged vertex from two
by the identiﬁcation of the KS →
particles originating near the interaction point (IP) is required. Loose cuts
on vertex position, particle momenta, and invariant mass are applied. The
reconstruction of the KS decay allows the KL momentum to be predicted
with a precision of better than 2 MeV/c. The overall tagging eﬃciency is
70%. KS beams are tagged by KL interactions in the calorimeter barrel.
∼
These interactions are signaled by high-energy clusters with typical arrival
times of 30 ns due to the low momentum (110 MeV/c) of the kaons produced
at DAΦNE. KL clusters used to tag KS’s must have energy Ecl > 100 MeV and
velocity 0.17 < β < 0.28, and must not be associated to any tracks in the drift
chamber. The KS momentum is determined with a precision of better than
2 MeV/c, as is also the case for the KL beam. A KS beam can also be tagged
π+π−π0 decays, which are identiﬁed by the presence of a
by looking for KL →
vertex in the DC satisfying kinematic cuts, and two clusters from the π0
γγ
decay. These clusters must satisfy opening-angle and time-of-ﬂight cuts and
must not be associated to any tracks in the DC.

→

At KLOE, since the KSKL pairs from φ decay are initially in a pure, antisym-
metric state, the ﬁnal-state decay products show characteristic interference
patterns. By studying the relative-time distributions for decays to diﬀerent
ﬁnal states, it is possible to measure various CP - and CP T -violation param-
eters [27]. The most interesting events for this type of analysis are those in
which the KS and KL decays occur in close proximity to each other, i.e.,
both occur near the IP. In order to maximize the selection eﬃciency for such
topologies, a dedicated algorithm has been developed. This algorithm searches
for the presence of any combination of pairs of track and photon vertices that
represent a possible pair of KS and KL decay modes. Good track vertices must
have exactly two tracks of opposite charge.

Events are selected for the charged-kaon sample by the identiﬁcation of either
a pair of candidate kaon tracks originating near the IP, or a K
µν or
ππ0 decay in the DC. In the ﬁrst case, two tracks of opposite charge
K
with total momentum compatible with the φ decay kinematics are required.
In the second case, the kaon decay is recognized as a charged vertex with
two connected tracks of the same sign of charge. The vertex must lie within

→

→

29

a)

1

b)

)
c
/
V
e
G

(
 
 
 
|

2
p

|
 

+

 
|

1
p

|

0.9

0.8

0.7

0.6

0.5

-0.05 -0.025

0

0.025 0.05

0.97

0

0.02

0.04
0.06
 Emiss   (GeV)

cpmiss 

π+π−π0 decays are
vs. cpmiss −
Fig. 12. Distribution of
in the bottom-left region of the plot, while π+π−(γ), µ+µ−(γ), and e+e−(γ) events
are concentrated in the top-right region. b) Enlarged view of the top-right region,
showing the contributions from π+π−(γ), µ+µ−(γ), and e+e−(γ) events.

Emiss. a) φ

p1|
|

p2|

→

+

|

40 < Rxy < 150 cm, and the momentum of the secondary in the rest frame of
the kaon must be within the range 180 < p∗ < 270 MeV/c.

→

π+π−π0 sample is obtained by searching for a vertex near the IP
The φ
< 15 cm) with two connected tracks of opposite charge. Cuts
(Rxy < 8 cm,
on the sum of the track momenta, psum =
, the missing momentum,
+
pmiss, and the missing energy, Emiss, are used to isolate the sample (see Fig. 12).

p2|
|

p1|

z
|

|

|

The search for the ﬁnal states (π+π−, µ+µ−, e+e−) + nγ requires one charged
vertex near the IP with psum < 1020 MeV/c, mππ > 90 MeV, and cpmiss −
Emiss >
Emiss
are used to separate π/µ/e ﬁnal states, as seen in Fig. 12.

50 MeV. Diﬀerent windows in psum and the quantity cpmiss −
−

Fully neutral nγ ﬁnal states are identiﬁed by the presence of at least three
clusters in the calorimeter that are not associated to tracks in the DC, and
which have times of ﬂight consistent with photon travel from the IP.

1.03

1.02

1.01

1

0.99

0.98

30

-
4.8 Redetermination of the event-start time

As explained in Sec. 4.5, the event-start time t0, evt, or equivalently, the in-
teger number of bunch crossings NBC needed for trigger formation, must be
determined oﬄine by analysis of the cluster times. Before tracking and event
classiﬁcation, NBC is obtained by assuming that the earliest qualifying cluster
in the event is due to a photon coming from the IP. This ﬁrst determination
allows the event to be reconstructed and classiﬁed by physics channel. How-
ever, many physics channels contain no prompt photons in the ﬁnal state, so
this determination of NBC, and therefore, the corrected cluster times t(0)
cl , may
diﬀer from the actual times of ﬂight by an integer number of bunch crossings
∆NBC:

For such events, it is usually possible to obtain the remaining correction term
using a recognized topology associated to a cluster. The term needed is then

t(0)
cl = tTOF −

∆NBC tRF.

∆NBC tRF =

L
βc −

t(0)
cl ,

(6)

(7)

where L is the estimated path length from the origin to the selected cluster,
and β is evaluated using the relevant mass hypothesis. For example, if ∆NBC
is evaluated from a primary track, β is evaluated from the track momentum.
If the track associated to the cluster comes from a secondary vertex, the term
L/βc becomes
i Li/βic, where the sum is over the contributions from primary
and secondary particles (including possibly photons). The times of all clusters
in the event are then reevaluated as tcl = t(0)
cl + ∆NBC tRF. This procedure has
been implemented for events classiﬁed as

P

µν decay;

ππ0 or K
→
π+π− decay;

charged kaons, by the identiﬁcation of a K
→
neutral kaons, by the identiﬁcation of a KS →
neutral radiative decays.

•
•
•
ππ0 topology is recognized, the extrap-
For charged-kaon events, if the K
olations to the calorimeter of the clusters from the π0
γγ decay and the
charged-pion track can be used to determine ∆NBC. If instead the K
µν
→
topology is recognized, ∆NBC is estimated from the momenta and lengths of
π+π− de-
the kaon and muon tracks. For neutral-kaon events with KS →
cays, ∆NBC is determined using the ﬁrst pion to reach the calorimeter. Neu-
tral radiative decays do contain prompt photons; the goal in redetermining
the event-start time in this case is to correct situations in which NBC is at
ﬁrst incorrectly determined because of the accidental coincidence of (a) beam-
background cluster(s). For such events, if the second cluster with Ecl > 50 MeV

→

→

31

and Rxy > 60 cm arrives more than 4 ns after the ﬁrst, ∆NBC is calculated
using the second cluster.

4.9 Reconstruction of photon vertices in KL decays

The positions of photon vertices from KL decays are obtained from the cluster
times. Each photon deﬁnes a time-of-ﬂight triangle: the ﬁrst side is the segment
from the IP to the KL decay vertex, LK; the second is the segment from the
KL decay vertex to the centroid of the calorimeter cluster, Lγ; and the third
is the segment from the IP to the cluster centroid, L. The direction ˆLK is
initially known because the KL decay is tagged. The photon-vertex position
is speciﬁed by the distance LK, which is determined from

L2 + L2

2L · LK = L2
γ,
K −
LK/βK + Lγ = ctγ,

(8)

where tγ is the cluster time and βK is the KL velocity.

π+π−
For the evaluation of LK, the KL decay must be tagged by a KS →
decay. The direction of the KL is given by pKL = pφ −
pKS , where pφ is the
mean φ momentum as determined from Bhabha events in the same run. The
position of the IP is obtained by backward extrapolation along the KS ﬂight
path.

LK is evaluated for each neutral cluster with energy Ecl > 7 MeV. The energy-
weighted average of the values of LK for each cluster is used as the ﬁnal LK
measurement.

The accuracy in the location of the photon vertex has been studied using
π+π−π0 decays, in which the decay position can be independently
KL →
determined using clusters and tracks, with much greater precision in the latter
case. The dependence of the position resolution on decay distance is illustrated
in Fig. 13.

5 Monte Carlo: physics generators and detector simulation

The KLOE Monte Carlo program, geanfi, is based on the geant 3.21 li-
brary [28, 29] widely used in current high-energy and astroparticle physics
experiments. geanfi incorporates a detailed description of the KLOE appa-
ratus, including

32

)

m
c
(
 
 
)
K
L

(

3.5

2.5

3

2

1

0

1.5

0.5

40

60

80

100

120

140

160
LK  (cm)

Fig. 13. Resolution σ(LK ) on the determination of the KL decay length using photon
π+π−π0 events. The contributions from the
vertices, as a function of LK, in KL →
uncertainties on the point of photon incidence on the calorimeter, the cluster time,
and the KL ﬂight direction are shown as the dot-dashed, dashed, and dotted lines,
respectively.

the interaction region: the beam pipe, the low-β quadrupoles, and the QCAL
calorimeters;
the drift chamber;
the endcap and barrel calorimeters;
the superconducting magnet and the return yoke structure.

•

•
•
•

A set of specialized routines has been developed to simulate the response of
each detector, starting from the basic quantities obtained from the geant
particle-tracking and energy-deposition routines. In Secs. 5.3 and 5.4, we dis-
cuss various aspects of the simulation of the DC and EmC response and com-
pare performance results obtained using data and Monte Carlo events.

5.1 Generators for continuum processes and φ production

geanfi contains the code to generate the physics of interest at DAΦNE. The
cross sections for the relevant processes in e+e− collisions at √s = 1.02 GeV
are listed in Table 3.

A precise Bhabha-event generator is required for the measurement of the
DAΦNE luminosity. To reach an accuracy of a few per mil for the eﬀec-
tive cross section, radiative corrections must be properly treated. bhagen,

33

s
Process
e+e−

→

e+e−
e+e−
e+e−
e+e−
e+e−

→

→

→

→

→

e+e−(γ)

µ+µ−(γ)
π+π−(γ)

γγ(γ)

ωπ0

φ

Polar angle
20◦ < θ < 160◦
55◦ < θ < 125◦
20◦ < θ < 160◦
20◦ < θ < 160◦
20◦ < θ < 160◦

σ (µb)

6.2

0.46

0.085

0.080

0.30

0.008

3.1

Table 3
Cross sections for several e+e− interaction processes at √s = 1.02 GeV. For the
process e+e−

φ, the visible cross section is listed.

→

an exact O (α) generator based on the calculations of Ref. 30, has been im-
plemented in geanfi from the very beginning. More recently, the babayaga
generator [25,26] has been interfaced with geanfi. This generator is based on
the application to QED of the parton-shower method originally developed for
perturbative QCD calculations. The generator takes into account corrections
due to initial-state radiation (ISR), ﬁnal-state radiation (FSR), and ISR-FSR
interference, and has an estimated accuracy of 0.5%. babayaga can also be
used to generate e+e−

µ+µ− and e+e−

π+π− events.

→

→

→

→

π+π−) using e+e−

KLOE can measure σ(e+e−
π+π−γ events in which
the photon is radiated from the initial state. For this analysis, we use the
phokhara 3 generator [31], which includes leading-order (LO) and next-to-
leading-order (NLO) treatment of the ISR and FSR terms. NLO eﬀects have
been shown to have an impact on the precision achievable for the KLOE
measurement of σ(e+e−
π+π−). A previous generator developed by the
same authors, eva [32], was based on LO calculations of the ISR and FSR
diagrams, supplemented by an approximate inclusion of additional collinear
radiation based on structure functions. KLOE can also generate events with
eva. The possibility of changing the structure functions has been used in our
analyses of radiative φ decays.

→

→

The process e+e−
ωπ0 is simulated with all ω decay modes enabled, the ω
width taken into account, and a 1 + cos2 θ dependence assumed for the ωπ0
angular distribution. In particular, the process e+e−
π0γ is
→
f0(980)γ
one of the background channels for the analysis of the decays φ
and a0(980)γ; it is treated according to the VDM matrix element described in
Ref. 33.

ωπ0 with ω

→

→

34

b
n

K+ K-

KS KL

r p

h g

Fig. 14. Dependence on √s of the cross section for φ-meson production and de-
cay into each of the major modes, K +K −, KSKL, ρπ, and ηγ. Curves show the
parameterization used in the MC; points are KLOE measurements from 2002.

MeV

The simulation of φ-meson production and decay includes the production of
ISR photons by the interacting beams. The ISR generator is based on the
Kleiss formalism discussed in Ref. 34, in which it is shown that the O (α)
radiative corrections completely factorize from the lowest-order interaction
cross section. The eﬀects of hard, soft, and virtual bremsstrahlung photons are
taken into account (hard photons, with E > 1 MeV, are explicitly simulated)
by multiplying a photon-emission factor with the nonradiative cross section
evaluated at an eﬀective CM energy that depends on the hardness of the ISR
photon. The MC dependence on √s of the cross section for φ production
followed by decay into each of the dominant modes (K +K −, KSKL, ρπ, and
ηγ) is shown in Fig. 14 and compared with KLOE measurements conducted
in 2002.

1

5.2 Generators for meson decays

The routines in the geant library simulate two- and three-body decays ac-
cording to pure phase-space distributions. Only the main decay modes of
muons, pions, kaons, and η mesons are simulated. We have enriched the list
of simulated particle-decay modes to include rare decays and reﬁned the kine-
matic distributions of the secondaries to include the correlations expected from
the matrix elements for the diﬀerent decay processes.

The generator for φ events discussed in Sec. 5.1 selects the φ decay channel

35

and declares the decay products to geant. Initial-state reactions and the
beam-energy spread of the machine (∆Ebeam/Ebeam = 0.04% at DAΦNE) are
taken into account event by event in the simulation of the decay kinematics.

K +K − and KSKL decays, the kaons are distributed as dN/ d cos θ

For φ
sin2 θ in the polar angle.

→

∝

→

In the φ
ρπ channel, the ρ decays dominantly to ππ; other possible ρ
decays are to πγ, ηγ, and π±η. The three-body phase space of the secondaries
is modiﬁed assuming a Breit-Wigner shape for the ρ resonance, with mρ =
776.1 MeV/c2 and Γρ = 145.6 MeV/c2 for all three ρ charge states. In φ
ρπ
decays with ρ
3π decay and the
ππ, the contribution from direct φ
interference between direct and ρ-mediated process are simulated, using the
values measured by KLOE for the relative contributions from each term [35].

→

→

→

Scalar mesons from radiative φ decays are distributed as dN/ d cos θ
1 +
cos2 θ in the polar angle and are generated by a separate set of routines, which
in some cases (e.g., the eva generator, customized for KLOE) oﬀer a choice
of production models.

∝

Besides the major modes, the list of neutral-kaon decays simulated includes
rare decays such as KS →

π+π−π0, and KS →

πℓν, KS →

π0π0π0.

For the simulation of semileptonic kaon decays, kaon decays into two pions,
and leptonic decays of charged kaons, radiative corrections are taken into
account. In order to avoid problems with divergences at low radiated-photon
energy, we use the method of Ref. 36 to sum the amplitudes for virtual and
real radiative processes to all orders of α. We have veriﬁed that the soft-photon
approximation used in this treatment is valid for the entire range of photon
energies in the kaon decays of interest. Whenever a decay is generated in which
the radiated energy is more than 0.1 MeV, a ﬁnal-state photon is explicitly
simulated.

2 = 1+gu+hu2+jv+kv2, where u = (s3−

3π decays are generated according to the form
The Dalitz plots for the K
π+ and v = (s1−
M
π+,
|
si. The values of the parameters g, h,
while si = (PK −
j, and k used in the simulation are those published by the PDG [37].

Pi)2 and s0 = (1/3)

s2)/m2

s0)/m2

→

|

The π0 decays simulated include the Dalitz decay π0
modes of the η and η′ mesons are simulated.

→

e+e−γ. All decay

P

36

5.3 Drift chamber simulation

The chamber geometry as simulated consists of a cylindrical carbon-ﬁber and
aluminum inner wall, a cylindrical carbon-ﬁber outer wall, and two spherical
carbon-ﬁber endplates. The average material burden contributed by the read-
out electronics installed on the endplates is also taken into account. The two
stiﬀening rings at the edges of the endplates and the 12 carbon-ﬁber struts are
simulated as well. In order to reduce CPU time consumption, the 52 000 wires
are not described in the geant geometry as volumes, but their presence is
taken into account at the tracking level. All parameters used to describe the
chamber geometry are stored in the database.

Tracking in the drift chamber is performed by a dedicated package that uses
standard geant routines for particle propagation and for interactions in the
medium. The cell geometry is calculated for each tracking step using the wire
positions and stereo angles stored in the database; the wire sags are also taken
into account. When a particle hits a wire, a multiple-scattering simulation
using the appropriate wire material is performed. The energy loss in each cell
is also computed.

For each cell crossed, the program computes the distance of closest approach
between the track helix and the nearest sense wire. These distances are con-
verted to drift times using s–t relations that are parameterized as described
in Sec. 4.2. The constants describing the s–t relations used for this conversion
are obtained from a detailed simulation of the electron drift performed with
the garfield program [38].

At the digitization stage, the TDC-signal arrival time is calculated, with the
drift time, the particle time of ﬂight, and the propagation time of the signal
along the wire taken into account. For cells crossed by more than one particle
(or more than once by the same particle), only the signal coming from the
ﬁrst hit is registered. The raw signal arrival times are then written to output
banks that serve as the input to the reconstruction program. An algorithm for
digitization of the charge values for each wire to simulate the measurements
from the recently installed ADCs is currently under development.

The drift-chamber reconstruction of simulated data is essentially identical to
that of real data, with two notable exceptions. First, a dedicated reconstruc-
tion module allows hits on dead channels to be deleted (the conﬁguration of
dead channels during data taking is stored in the database run by run). Sec-
ond, the s–t relations used for the track reconstruction are obtained by the
calibration procedure described in Sec. 4.2, using simulated cosmic-ray events.

37

5.4 Calorimeter simulation

In order to reduce CPU consumption, the geant representation of the calorime-
ter geometry does not include a detailed description of the individual ﬁbers
embedded in the grooved lead plates. An approximate geometry consisting of
thin, alternating layers of lead and scintillator is used instead.

The starting point for the simulation of the EmC response is the energy de-
position of the incident particle in the active material, ∆E. The light yield
collected at each end of a calorimeter module is calculated by correcting ∆E
as a function of the point of impact along the ﬁbers to account for light at-
tenuation. The resulting energy is converted into a number of photoelectrons,
Npe, using an average value for the light-yield conversion constant, YMC, and
applying Poisson statistics to simulate the ﬂuctuations.

To each photoelectron, a time is assigned by adding scintillation and light-
propagation times to the arrival time of the particle. The number of photo-
electrons and the photoelectron times are accumulated for each detector cell,
i.e., for the entire volume viewed by each individual PMT. The energy mea-
sured for each PMT is obtained by dividing the total number of photoelectrons
by YMC. The ﬁnal PMT time measurement is obtained from the time distri-
bution of the photoelectrons collected. In order to simulate the behavior of
the constant-fraction discriminators used in the experiment, this time is set
to the value corresponding to the integration of 15% of the complete signal.

π+π−π0 events in tuning the simulation
We have made extensive use of φ
of the calorimeter. In such events, the energy and momentum of one of the
photons can be accurately predicted from the reconstruction of the π+π−
vertex and the position alone of the cluster from the other photon. No other
calorimetric information is needed.

→

→

To establish the thickness of the lead and scintillator planes in the simulated
geometry, we have minimized the diﬀerences between the shower shapes for
π+π−π0 events in the data set, the
photons in data and MC events. Using φ
distribution of the depth of the ﬁrst plane ﬁred by incident photons of given
energy Eγ and polar angle θγ has been ﬁt with a discretized exponential func-
tion with mean-depth parameter λ. In Fig. 15a, the dependence of λ on Eγ is
shown for diﬀerent values of θγ. The distributions ﬂatten above 200 MeV, as
expected when the cross section for e+e−-pair creation approaches the plateau
limit corresponding to an interaction length of 7/9 X0. The plateau values of
the interaction length for diﬀerent θγ intervals shown in Fig. 15b correspond
to values for X0 of
1.2 cm. This is in reasonable agreement with the radia-
tion length estimated a priori from the known composition of the calorimeter
modules [2]. Using the same technique, we have also measured the eﬀective

∼

38

20(cid:176) < q
40(cid:176) < q
60(cid:176) < q
80(cid:176) < q

g  < 30(cid:176)
g  < 50(cid:176)
g  < 70(cid:176)
g  < 90(cid:176)

0

50

100

150

200

250

300

350

400

Eg  (MeV)

a)

b)

2.5

1.5

0.5

3

2

1

0

3

2

1

2.5

1.5

0.5

)

m
c
(
 
h
t
g
n
e
l
 
n
o
i
t
c
a
r
e
t
n
I

)

m
c
(
 
h
t
g
n
e
l
 

n
o
i
t
c
a
r
e
t
n

i
 

u
a
e
t
a
l
P

0

0

10

20

30

40

50

60

70

80

90
g  (deg)

Fig. 15. a) Mean interaction-length parameter λ as a function of photon energy Eγ
for diﬀerent intervals in the photon polar angle θγ. b) Limiting (plateau) values of
λ for diﬀerent intervals in θγ.

radiation length in the Monte Carlo and varied the relative thickness of the
lead and scintillator planes in order to establish agreement with data. This
procedure leads to a representation of the calorimeter module as 220 layers of
480 µm of lead plus 620 µm of scintillator.

→

π+π−π0 events with
To calibrate the calorimeter response, we have used φ
particles crossing the center of the calorimeter modules (s = s0) to determine
the average light-yield conversion constant for data, Y , as a function of the
energy of the incident particle. The relation between Y and YMC is YMC =
Y gi(s0)/fe (recall that gi is the correction factor for light attenuation in the
ﬁbers of the ith cell; fe is the sampling fraction for electromagnetic showers). If
Poisson statistics dominate the ﬂuctuations in the energy response, we expect
the distributions of the ratios EA/EB and (EA
EB)/(EA + EB), where the
values EA,B refer to the energy measurement at each side of the module,
to have variances σ = √2/Npe. We obtain Y = 0.6–0.7 p.e./MeV per side.
This has led us to set YMC = 19 p.e./MeV in the most recent version of the
MC. After these adjustments, reasonable agreement between MC and data

−

39

q
)

%

(
 

Eg

/

E

)

%

(
 

Eg

/
)

E

(

0

-1

-2

-3

-4

-5

40
35
30
25
20
15
10
5
0

a)

b)

0

50

100

150

200

250

300

Eg  (MeV)

0

50

100

150

200

250

300

Eg  (MeV)

π+π−π0 events with
Fig. 16. Energy response and resolution determined using φ
photons incident on the calorimeter barrel: a) relative linearity of energy response,
∆Eγ/Eγ, as a function of Eγ; b) relative energy resolution, σ(Eγ)/Eγ, as a function
of Eγ. Solid (open) circles are for data (MC).

→

is observed for the energy response and resolution as a function of Eγ (see
Fig. 16).

With the geometry and response of the calorimeter thus simulated, assuming
that the visible energy follows the spectrum of energy loss inside the scintilla-
tor, we obtain sampling fractions fe = 11% for electromagnetic showers, and
fµ = 18% for minimum-ionizing particles. The ratio fe/fµ = 0.6 is 20% lower
than the value measured using a test beam. The same discrepancy between MC
and data has been found for the position of the minimum-ionizing peak from
µ+µ−(γ)
the most energetic pions in φ
π+π−π0 events in data are currently being used to adjust the average
and φ
energy loss of pions and muons in the scintillator in order to obtain good MC-
data agreement on the calorimeter energy response over the entire momentum
range of interest.

π+π−π0 events. Samples of e+e−

→

→

→

The eﬀect of the cracks between the barrel modules is illustrated in Fig. 17,

40

D
g
s
g
E

/

E

0.1

0.05

0

-0.05

-0.1

-0.15

-0.2

0

2

4

6

8

10

12
mod(f

14
, 15(cid:176) )

Fig. 17. Energy response as a function of the azimuthal distance from the boundaries
between modules of the barrel calorimeter. Solid (open) circles are for data (MC).

Eγ)/Eγ as a function of azimuthal distance from
which shows the ratio (Ecl −
π+π−π0 events. A clear deterio-
the module boundaries for photons from φ
→
1◦ of the module boundaries. This
ration in the response is observed within
eﬀect is due to ﬁbers broken during the ﬁnal milling of the modules, and it is
not easy to include in the MC given the representation of the geometry in use.
We simulate this eﬀect during event reconstruction by weighting the recon-
structed energies with a function of the azimuthal positions of the generated
hits. A similar eﬀect is observed in the endcaps; in this case, the magnitude
of the eﬀect is smaller, and it is not yet corrected for.

±

For the time simulation, the scintillation curve for single photoelectrons has
been tuned to reproduce the stochastic contribution to the timing resolution of
54 ps/√E(GeV). The MC-data agreement after the adjustment is reasonable.
The constant contribution to the timing resolution observed in data,
140 ps,
is mostly due to jitter introduced when rephasing the trigger with the machine
RF signal. To simulate this eﬀect, an oﬀset sampled from a Gaussian with a
width of 140 ps is added in common to all time signals in the event.

∼

5.5 Trigger simulation

The KLOE trigger is emulated in software during event reconstruction. Non-
triggering events are retained in the output, but the result of the trigger em-
ulation is encoded in the data stream, allowing MC estimates of the trigger

41

D
g
g
eﬃciency to be obtained.

For the emulation of the EmC trigger, the energy deposited in each calorimeter
element and the PMT-signal arrival times are ﬁrst read out. For each trigger
sector, the energies of all cells ﬁred within a coincidence window of 3.5 ns
are summed, where this interval approximately corresponds to the width of
the actual PMT signals. By comparison to a set of discriminators reproducing
the hardware circuitry, these sums are transformed into logic signals of 70-ns
duration. Three diﬀerent sets of thresholds are used to distinguish φ decays,
Bhabha events, and cosmic-ray events. The threshold values are determined
from the analysis of real data on a run-by-run basis. The resulting logic signals
are used to compute the multiplicity of hit sectors on the barrel and each of the
two endcaps, and ﬁnally combined to produce the φ, Bhabha, and cosmic-ray
trigger signals.

The signals from the DC wires are read out and shaped at 250 ns. As in the
hardware, the signals from wires in diﬀerent groups of adjacent DC planes
are summed. These “superlayer” signals are then summed in turn to get the
eﬀective DC multiplicity as a function of time. A level-1 DC trigger is set
whenever this sum exceeds a given threshold. The sum is then integrated over
a 1.2 µs interval and compared to another threshold to deﬁne the level-2 DC
signal. The values of these two thresholds are determined from the analysis of
real data on a run-by-run basis.

Finally, the DC- and EmC-trigger signals are combined to deliver the ﬁnal
level-1 and level-2 trigger decisions with the correct timing relative to the
start of the event as generated. Once the trigger time has been simulated,
it is rounded to the next highest multiple of 4tRF to simulate the rephasing
of the experiment’s level-1 trigger with the machine clock. A time interval
corresponding to an integer number of bunch crossings from one to four is
then subtracted from the rephased trigger time; this corresponds to randomly
specifying the particular bunch crossing that produced the event. The result
is the simulated value of t0, evt; this value is then applied to the times of all
calorimeter and drift-chamber hits.

5.6 Machine background simulation

A detailed simulation of detector activity from the accidental coincidence of
hits from machine background is required in order to obtain the high precision
and careful control of systematics needed for most KLOE physics analyses.
This activity consists mainly of noise hits in the DC and low-energy clusters
in the EmC, mostly at small angles. Background hits in the chamber and
calorimeter are added to the simulated events at the reconstruction stage.

42

40 nb. Since e+e−

For the 2001–2002 data, this background was obtained from e+e−
γγ events
satisfying speciﬁc topological cuts. These events are selected from KLOE data
γγ events are fully neutral, all
with a cross section of
DC hits in these events are considered background, in addition to all EmC
clusters not identiﬁed as belonging to the γγ topology (care is taken to cor-
rectly distinguish clusters from initial state radiation or from cluster splitting,
which actually belong to the γγ topology, from those due to machine back-
ground).

→

→

∼

A ﬁle containing background hits is created for each raw ﬁle in the data
set. As discussed in Sec. 5.7, an MC run corresponds to a set of raw ﬁles in
data. We insert the hits from each event in the set of background ﬁles into
multiple events in the corresponding MC run, with a reuse factor chosen to
ensure that all background events are used roughly the same number of times.
This ensures reproduction of the time-variable background spectrum in the
simulated output.

For both the EmC and DC, when hits are inserted, their timing relative to
the start time of the γγ event from which they were extracted is preserved.
The insertion takes place before the trigger simulation is performed, so that
simulated and inserted hits are temporally aligned. Hit-blocking eﬀects are
reproduced. In the drift chamber, a background hit that arrives earlier than a
simulated hit on the same wire causes the simulated hit to be removed from
the event, and vice versa. On the calorimeter, if both a background hit and a
simulated hit occupy the same cell, the earlier arrival time on each side of the
cell is retained, while the energy read out at each side is taken from the sum
of the two hits. The trigger simulation is then performed, and the simulated
and inserted hits are then t0-smeared simultaneously using the algorithm of
Sec. 5.5.

For the drift chamber, the s–t relations used for simulated events and for
real data are suﬃciently similar so that all hits—simulated and inserted—
can be reconstructed with the MC s–t relations. A correction is made to the
energy scale when calorimeter hits are inserted. This correction ensures that
the inserted calorimeter hits reconstruct with the same energy that they had
in the data event from which they were extracted.

5.7 Monte Carlo production campaigns

An extensive simulation campaign for the 2001–2002 KLOE data set is cur-
rently near completion. This campaign is focused on the production of general-
purpose samples, such as samples in which all decays of the φ are present in
proportion to their natural branching ratios, or in which the φ always decays

43

to KSKL but all possible ﬁnal states are present. Such samples are particularly
useful for understanding backgrounds in studies of rare decays. The produc-
tion procedure is geared towards providing high-statistics samples. The total
number of events in each sample is established using an eﬀective luminosity
scale factor, which ranges from 0.2 for general-purpose simulations such as
3.1 µb), to 5 for dedicated simulations such
φ
→
as e+e−
50 nb). In all, current plans call for the
production of about 109 events.

π+π−γ (cross section

all (peak cross section

→

∼

∼

In order to track run-by-run variations in the operating conditions of the
collider and detector, an MC sample is generated for each run in the data set,
with the number of events proportional to the integrated luminosity of the run
under simulation, and such parameters as machine energy, momentum of the
collision center-of-mass, beam-spot position, map of dead detector elements,
and trigger thresholds set to correspond to the run conditions. Background
hits in the EmC and DC are inserted with special care in order to ensure
reproduction of the background spectra resulting from variations with time
within each run (see Sec. 5.6). As a result of these procedures, time-variable
conditions are correctly averaged in the sample of MC events corresponding
to any given group of runs in the data set.

For the production of a given sample, one job is submitted for each run in
the data set. A production job handles generation, reconstruction, and DST
creation. In order to have intermediate ﬁles of reasonable size, it is usually
necessary to split the generation into several processes. A reconstruction pro-
cess immediately follows each generation process. DSTs are made after all
generation and reconstruction is complete. Production is started by submit-
ting a large number of jobs to a batch queue managed by IBM’s LoadLeveler
utility [16].

When MC events are reconstructed, several algorithms intended to complete
the simulation are run before any of the actual reconstruction algorithms.
Background hits in the EmC and DC are ﬁrst inserted. Hits on dead wires of
the drift chamber are next removed. The trigger emulator is then run, after
which hits on hot drift-chamber wires can be removed (in the reconstruction
of real data, they are removed at the input stage). Finally, the t0-smearing
algorithm is applied. After these steps, the same algorithms used for the re-
construction of real data are run, in the same order described in Sec. 3.1.

The only other special treatment given to MC events concerns the behavior
of the machine-background ﬁlter and the event-classiﬁcation module. Like the
trigger emulator, these modules only record their decisions in the output ﬁle;
they do not actually suppress events. In particular, MC events are not divided
up into streams at the reconstruction stage; only one reconstruction output
ﬁle is produced from each generator output ﬁle.

44

Program

Events (106) CPU time (days) Output size (TB)

all

φ

→

φ
→
e+e−

KSKL

ππγ

→

255

410

36

1100

1800

110

6.9

11.0

0.8

Table 4
Statistics for some Monte Carlo production campaigns completed to date

The reconstruction output ﬁle contains enough information to allow recovery
of the events as generated, before the introduction of background hits. There-
fore, only the reconstructed output ﬁles are archived; generator output ﬁles
are discarded.

In the last stage of the production job, DSTs are produced. The same ﬁve types
of DSTs as for real data can be produced for MC events, with the application
of the same stream-speciﬁc algorithms described in Sec. 3.2. However, for the
production of dedicated MC samples (e.g., for the process e+e−
π+π−γ),
only the DST types of interest are produced. Event streaming is performed
at the DST-production stage. In addition to all events classiﬁed on the ba-
sis of reconstructed quantities, each MC DST stream contains all events with
topologies as-generated relevant to the physics of the stream. MC DSTs also
contain a minimal set of information about the true event topology. All in-
formation in the geant KINE and VERT banks is present in the DSTs, but
there is no information about individual hits. In place of the hit banks them-
selves, the correspondences between reconstructed topologies (clusters, tracks)
and simulated particles (KINE tracks) are kept. Like data DSTs, MC DSTs
are archived and recalled to the NFS-mounted disk cache for prompt access.

→

→

KSKL, the entire 2001–2002 data set (

Table 4 gives a statistical summary of the Monte Carlo production campaigns
completed to date. In the two general-purpose production campaigns, φ
all
−1) was simulated
and φ
at luminosity scale factors of 0.2 and 1, respectively. Events such as these
require 200 ms to generate and 175 ms to reconstruct on the CPUs in the B80
servers; the running times in the table were obtained with 60 CPUs. For the
−1), at a
e+e−
ππγ campaign, only the 2001 data were simulated (
170 pb
108 events were
luminosity scale of 5. In these three campaigns, a total of 7
produced in about three months of real time.

450 pb

∼
×

→

→

∼

45

6 Conclusions

The high event rate at DAΦNE—1.5 kHz of φ decays accompanied by a similar
yield of Bhabha events within the acceptance, which must be downscaled, and
of machine-background and cosmic-ray events, which must be rejected—has
required us to design and operate a large, complex, and reliable system for
data acquisition and oﬄine data processing.

The DAQ system described in Ref. 5 has guaranteed a bandwidth of 3 kHz
during data taking, while simultaneously handling various tasks related to
data-quality control and subdetector calibration and monitoring. At present,
the mass-storage and data-handling systems manage over 100 TB of raw data
and a comparable amount of reconstructed data both from the detector and
from the experiment’s Monte Carlo.

We have carefully designed and optimized the oﬄine software environment to
ensure that data is reconstructed immediately following acquisition. As part
of this eﬀort, we have developed various tools for detector calibration, access
to reconstructed data, process scheduling, and the like. We have placed special
emphasis on maximizing the eﬃciency and precision of the reconstruction pro-
gram. As a result, the performance speciﬁcations of the detector—momentum
and vertex resolution for the drift chamber and energy and time resolution for
the calorimeter—have been fully satisﬁed.

At the same time, we have implemented a continuing series of improvements
to the simulation of the detector response, the representation of machine back-
ground, and the accuracy of the physics generators in the experiment’s Monte
Carlo. As a result of this development program, excellent agreement between
data and Monte Carlo has been obtained for the distributions of key variables,
and the Monte Carlo has become a reliable and important tool for physics
analysis.

We have dedicated a signiﬁcant amount of work to the construction of a stable,
scalable data-processing system with the ﬂexibility to exploit all of the avail-
able resources. During the past four years of operation, we have implemented
a series of important upgrades to keep pace with the growing demands of the
experiment. With the upgrades already scheduled for 2004, the environment
will be well suited to handle predicted increases in the DAΦNE luminosity.

46

Acknowledgements

It is almost impossible to thank all of the people who have contributed to
making the KLOE oﬄine system a successful reality. We are most grateful
for the strong support, help, and advice received from our KLOE colleagues.
We would also like to thank A. Andryakov, A. Calcaterra, F. Donno, and
W. Kim for their invaluable contributions to the preparation of the recon-
struction package in the early stages of software development. We acknowl-
edge fruitful collaboration with the staﬀ of the LNF Computing Service and
the LNF Technical Services for their support of the smooth operation of the
KLOE computing environment. One of us (A.D.) would like to acknowledge
support from the Emmy Noether Program of the Deutsche Forschungsgemein-
schaft. Travel and subsistence for A.D., D.L., S.M., and B.V. related to this
work was supported by TARI, contract HPRI-CT-1999-00088.

References

[1] M. Adinolﬁ, et al., The tracking detector of the KLOE experiment, Nucl.

Instrum. Meth. A 488 (2002) 51.

[2] M. Adinolﬁ, et al., The KLOE electromagnetic calorimeter, Nucl. Instrum.

Meth. A 482 (2002) 364.

A 483 (2002) 649.

Meth. A 492 (2002) 134.

[3] M. Adinolﬁ, et al., The QCAL tile calorimeter of KLOE, Nucl. Instrum. Meth.

[4] M. Adinolﬁ, et al., The trigger system of the KLOE experiment, Nucl. Instrum.

[5] A. Aloisio, et al., Data acquisition and monitoring for the KLOE detector, Nucl.

Instrum. Meth. A 516 (2004) 288.

[6] G. Cabibbo, et al., The calibration of the space-time relations in the KLOE

drift chamber, KLOE Note 175 (2002).
URL http://www.lnf.infn.it/kloe/pub/knote/kn175.ps

[7] L. Williams, et al., HEPDB, reference manual, version 1.19, CERN Program

Library Long Writeup Q180 (1995).
URL http://wwwasdoc.web.cern.ch/wwwasdoc/hepdb html3/hdbmai%n.html

[8]

IBM Tivoli Storage Manager.
URL http://www.ibm.com/software/tivoli/products/storage-mgr

[9] KLOE Collaboration, I. Sﬁligoi, et al., KID - KLOE Integrated Dataﬂow, in:
Proceedings of CHEP 2001 (Beijing, 3–7 Sep. 2001), Science Press, Beijing,
2001.
URL http://www.lnf.infn.it/kloe/pub/doc/ka082.pdf

47

[10] DB2 Product Family.

URL http://www.ibm.com/software/data/db2

[11] I. Sﬁligoi, KLOE database - the bookkeeping and computing system catalogue,

KLOE Note 184 (2003).
URL http://www.lnf.infn.it/kloe/pub/knote/kn184.pdf

[12] E. Sexton-Kennedy, M. Shapiro, D. R. Quarrie, A beginner’s guide to

Analysis Control and Build Job, CDF Note 384 (1996).
URL http://www-cdf.fnal.gov/oﬄine/a c/a c.html

[13] Concurrent Versions System.

URL http://www.cvshome.org

[14] D. R. Quarrie, B. Troemel, YBOS programmer’s reference manual (1992).

URL http://www-cdf.fnal.gov/oﬄine/ybos/murat/ybos.01.html

[15] J. Gailly, M. Adler, zlib 1.1.4 manual (2002).
URL http://www.gzip.org/zlib/manual.html

[16] LoadLeveler V2.2.

URL http://www.ibm.com/servers/eserver/ecatalog/us/software

[17] A. Antonelli, et al., Short description of the track reconstruction program with

the KLOE drift chamber, KLOE Memo 194 (1998).

[18] H. Albrecht, et al., Nucl. Instrum. Meth. A 275 (1989) 1.

[19] P. De Simone, et al., Cell response parameterization, KLOE Note 98 (1994).

[20] M. Primavera, S. Spagnolo, Parametrization of the time to distance relations

and resolution in the KLOE drift chamber, KLOE Note 165 (1997).
URL http://www.lnf.infn.it/kloe/pub/knote/kn165.ps

[21] A. Antonelli, et al., Momentum calibration of the KLOE drift chamber, KLOE

Memo 231 (2001).

[22] A. Antonelli, et al., The KLOE ﬁeld map revisited, KLOE Memo 233 (2001).

[23] KLOE Collaboration, A. Aloisio, et al., Phys. Lett. B 538 (2002) 21.

[24] M. Palutan, T. Spadaro, P. Valente, Measurement of Γ(KS →
π0π0) with KLOE 2000 data, KLOE Note 174 (2002).
URL http://www.lnf.infn.it/kloe/pub/knote/kn174.ps.gz

π+π−)/Γ(KS →

[25] C. M. Carloni Calame, et al., Nucl. Phys. B 584 (2000) 459.

[26] C. M. Carloni Calame, et al., The BABAYAGA event generator, hep-

ph/0312014 (2003).

[27] C. D. Buchanan, et al., Phys. Rev. D 45 (1992) 4088.

[28] R. Brun, et al., GEANT3, CERN-DD/EE/84-1 (1984).

48

[29] R. Brun, et al., GEANT: Simulation program for particle physics experiments,

user guide and reference manual, CERN-DD-78-2-REV (1978).

[30] F. A. Berends, R. Kleiss, Nucl. Phys. B 228 (1983) 537.

[31] H. Czy˙z, et al., The radiative return at φ- and B-factories: FSR at next-to-

leading order, hep-ph/0308312 (2003).

[32] F. Binner, J. H. K¨uhn, K. Melnikov, Phys. Lett. B 459 (1999) 279.

[33] N. N. Achasov, V. V. Gubin, Phys. Rev. D 63 (2001) 094007.

[34] F. A. Berends, R. Kleiss, Nucl. Phys. B 260 (1985) 32.

[35] KLOE Collaboration, A. Aloisio, et al., Phys. Lett. B 561 (2003) 55.

[36] S. Weinberg, Phys. Rev. 140 (1965) 516.

[37] Particle Data Group, K. Hagiwara, et al., Phys. Rev. D 66 (2002) 010001.

[38] R. Veenhof, et al., GARFIELD, a drift-chamber simulation program, User’s

guide, version 5.05, CERN Program Library (1994).

49


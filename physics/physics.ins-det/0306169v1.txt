3
0
0
2
 
n
u
J
 
4
2
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
9
6
1
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

The CDF Silicon Vertex Trigger∗
Bill Ashmanskasa†, A. Barchiesib, A. Bardic, M. Barid, M. Baumgarte, S. Belforted, J. Berryhille,
M. Bogdane, R. Carosic, A. Cerrif, G. Chlachidzeg, R. Culbertsone, M. Dell’Orsoc, S. Donatic, I. Fiorih,
H. Frische, S. Galeottic, P. Giannettic, V. Glagolevg, A. Legeri, Y. Liui, T. Maruyamae, E. Meschic,
L. Monetai, F. Morsanic, T. Nakayae, G. Punzic, M. Rescignob, L. Ristoric, H. Sanderse, S. Sarkarb,
A. Semenovg, M. Shochete, T. Speeri, F. Spinellac, H. Vatagac, X. Wui, U.K. Yange, L. Zanellob,
A.M. Zanettid

(for the CDF-II Collaboration)

aArgonne National Laboratory, Argonne, IL 60439, USA

bINFN, Sezione di Roma I and University La Sapienza, I-00173 Roma, Italy

cINFN, University and Scuola Normale Superiore of Pisa, I-56100 Pisa, Italy

dINFN, Sezione di Trieste, I-34012 Trieste, Italy

eEnrico Fermi Institute, University of Chicago, Chicago, IL 60637, USA

f Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA

gJoint Institute for Nuclear Research, Dubna, Russia

hUniversity of Padova and INFN, Sezione di Padova, I-35031 Padova, Italy

iUniversity of Geneva, CH-122, Geneva 4, Switzerland

The CDF experiment’s Silicon Vertex Trigger is a system of 150 custom 9U VME boards that reconstructs
axial tracks in the CDF silicon strip detector in a 15 µsec pipeline. SVT’s 35 µm impact parameter resolution
enables CDF’s Level 2 trigger to distinguish primary and secondary particles, and hence to collect large samples of
hadronic bottom and charm decays. We review some of SVT’s key design features. Speed is achieved with custom
VLSI pattern recognition, linearized track ﬁtting, pipelining, and parallel processing. Testing and reliability are
aided by built-in logic state analysis and test-data sourcing at each board’s input and output, a common inter-
board data link, and a universal “Merger” board for data fan-in/fan-out. Speed and adaptability are enhanced
by use of modern FPGAs.

1. CDF TRIGGER OVERVIEW

The recently upgraded Collider Detector at
Fermilab (CDF) experiment [1] pursues a broad
physics program at Fermilab’s Tevatron proton-
antiproton collider, comprising topics as diverse
as top quark production and charmed meson de-
cay. In the present Tevatron data-taking period

∗Talk presented at the 9th Pisa Meeting on Advanced De-
tectors, La Biodola, Isola d’Elba, May 25-31, 2003.
†Corresponding author. E-mail: wja@hep.anl.gov

(“Run 2”), the c.m. energy is √s = 1.96 TeV,
the bunch-crossing interval is 396 ns (with a pos-
sible upgrade to 132 ns for high luminosity), and
1032 cm−2s−1 to date
peak luminosities are 0.5
1032 cm−2s−1.
and climbing toward a goal of 2
CDF’s new drift chamber [2] and silicon detec-
tor [3] are discussed in detail elsewhere in these
proceedings.

×

×

One challenge for a hadron collider experi-
ment is to extract signals of interest eﬃciently
from much larger backgrounds. To illustrate the

2

inelastic cross-
orders of magnitude, the total
section at the Tevatron is about 50 mb, while
the b-quark cross-section within CDF’s accep-
tance (transverse momentum pT > 6 GeV, ra-
y
< 1) is about 10 µb, and the t-quark
pidity
|
cross-section is about 5 pb. At luminosities above
1032 cm−2s−1, the mean number of interac-
0.35
tions per beam crossing exceeds 1. Reducing the
1.7 MHz beam-crossing rate to CDF’s 70 Hz DAQ
output rate implies a trigger rejection of 25000.

×

|

Good background rejection in the trigger re-
quires fast identiﬁcation of distinctive signal sig-
natures. In the CDF trigger, many important sig-
natures exploit fast charged-particle track recon-
struction in the bending plane of the spectrom-
eter, transverse to the beam axis. The trigger
matches drift chamber tracks with EM calorime-
ter showers, muon chamber stubs, and silicon
detector data, respectively, to identify electrons,
muons, and b and c daughters.

CDF uses a three-level trigger. On each
beam crossing (396 or 132 ns), the entire front
end digitizes (silicon samples and holds). A
5.5 µs pipeline of programmable logic forms axial
drift chamber tracks and can match these with
calorimeter and muon-chamber data. On Level 1
accept, front-end boards store the event to one
of four buﬀers (silicon digitizes and transmits to
the silicon trigger and event builder). Level 2
processing, with about 30 µs latency, adds fast
silicon tracking, calorimeter clustering, and EM
calorimeter shower-max data. The ﬁnal Level 2
decision is made in software on a single-board
computer, so a wider range of thresholds and de-
rived quantities is possible (e.g. transverse mass
of muon track pairs), even for information that is
in principle available at Level 1. On Level 2 ac-
cept, front-end VME crates transmit to the event
builder. At Level 3, a farm of 250 commodity PCs
runs full event reconstruction. This is the ﬁrst
stage at which three-dimensional tracks (e.g. for
invariant mass calculation) are available. Events
passing Level 3 are written to disk.

While some optimization remains to be done,
the maximum output at L1/L2/L3 is approxi-
mately 35000/350/70 Hz. Each of these rates
is an order of magnitude higher than in CDF’s
1992-96 running period. In addition, drift cham-

ber tracking has moved from L2 to L1, and sili-
con tracking has moved from oﬄine to L2. These
three changes allow CDF to collect large sam-
ples of fully hadronic bottom and charm decays,
by requiring two drift chamber tracks at L1, re-
quiring each track to have a signiﬁcant (at least
120 µm) impact parameter at L2, and perform-
ing full software tracking at L3 to conﬁrm the
hardware tracking. The samples made possible
by CDF’s front-end, trigger, and DAQ upgrades
have yielded novel physics results [4] at an early
stage of Run 2.

CDF’s Level 1 drift chamber hardware track
processor, XFT [5], is a cornerstone of the CDF
trigger. For every bunch crossing, with 1.9 µs
latency, it ﬁnds tracks of pT > 1.5 GeV with
96% eﬃciency. XFT obtains coarse hit data (two
time bins) from each axial drift chamber wire,
ﬁnds line segments in the 12 measurement lay-
ers of each axial superlayer, then links segments
from these four superlayers to form track candi-
dates. XFT’s resolutions, σ( 1
pT ) = 1.7%/GeV
and σ(φ0) = 5 mrad, are only about a factor of
10 coarser than those of the oﬄine reconstruction.

2. SVT TRACK PROCESSING

For each event passing Level 1, the Silicon
Vertex Trigger (SVT) [6,7,8] swims each XFT
track into the silicon detector, associates silicon
hit data from four detector planes, and produces
a transverse impact parameter measurement of
35 µm resolution (50 µm when convoluted with
the beam spot) with a mean latency of 24 µs,
9 µs of which are spent waiting for the ﬁrst sili-
con data. SVT’s impact parameter resolution for
pT
2 GeV is comparable to that of oﬄine tracks
that do not use Layer 00 (mounted on the beam
pipe), which is not yet available in SVT.

≈

For ﬁducial oﬄine muon tracks from J/ψ de-
cay, having pT > 1.5 GeV and hits in the four
silicon planes used by SVT, measured SVT eﬃ-
ciency is 85%. The most suitable deﬁnition of
eﬃciency in a given context depends on what one
aims to optimize: restricting the denominator to
pT > 2 GeV increases the eﬃciency to 90%, while
relaxing the requirements on which layers contain
oﬄine silicon hits reduces the eﬃciency to 70%,

and looser ﬁducial requirements reduce the eﬃ-
ciency further; the ultimate denominator for SVT
would be all XFT-matched oﬄine silicon tracks
that are useful for physics analysis.

SVT is a system of 150 custom 9U VME boards
containing FPGAs, RAMs, FIFOs, and one ASIC
design. CPUs are used only for initialization and
monitoring. SVT’s input comprises 144 optical
ﬁbers, 1 Gbit/s each, and one 0.2 Mbit/s LVDS
cable; its output is one 0.7 Mbit/s LVDS cable.

O

Three key features allow SVT to carry out in
15 µs a silicon track reconstruction that typically
(0.1 s) in software: a highly paral-
requires
lel/pipelined architecture, custom VLSI pattern
recognition, and a linear track ﬁt in fast FPGAs.
The silicon detector’s modular, symmetric ge-
ometry lends itself to parallel processing. SVT’s
ﬁrst stage, converting a sparsiﬁed list of channel
numbers and pulse heights into charge-weighted
hit centroids, processes 12
×
×
longitudinal
radial) silicon planes in 360 iden-
tical FPGAs. The overall structure of SVT re-
ﬂects the detector’s 12-fold azimuthal symme-
try. Each 30◦ azimuthal slice is processed in its
own asynchronous, data-driven pipeline that ﬁrst
computes hit centroids, then ﬁnds coincidences
to form track candidates, then ﬁts the silicon hits
and drift chamber track for each candidate to ex-
tract circle parameters and a goodness of ﬁt.

5 (azimuthal

×

×

6

In SVT’s usual conﬁguration, a track candi-
date requires a coincidence of an XFT track and
hits in a speciﬁed four (out of ﬁve available) sili-
con layers. To deﬁne a coincidence, each detector
plane is divided into bins of programmable width,
typically 250-700 µm, and XFT tracks are swum
to the outer radius of the silicon detector and
binned with 3 mm typical width. For each 30◦
slice, the set of 32K most probable coincidences
(“patterns”) is computed oﬄine in a Monte Carlo
program and loaded into 256 custom VLSI asso-
ciative memory (AM) chips. For every event, each
binned hit is presented in parallel to the 256 AM
chips, and the hit mask for each of the 128 pat-
terns per chip is accumulated in parallel. When
the last hit has been read, a priority encoder enu-
merates the patterns for which all ﬁve layers have
a matching hit. The processing time is thus lin-
ear in the total number of hits in each slice and

3

linear in the number of matched patterns.

c

d

×

d
|
|

< 1 mm,

There is no exact linear relationship between
the transverse parameters c, φ, d of a track in a
solenoidal ﬁeld and the coordinates at which the
track meets a set of ﬂat detector planes: the coor-
cos3 φ , tan φ, and
dinates are more closely linear in
< 15◦,
cos φ . But for pT > 2 GeV,
φ
|
|
a linear ﬁt biases d by at most a few percent. By
linear regression to Monte Carlo data, we derive
6 coeﬃcients V and 3 intercepts ~p0 relating
the 3
~p = (c, φ, d) to the vector ~x of cXFT, φXFT, and
four silicon hits: ~p = ~p0 + V
~x. The same regres-
sion produces coeﬃcients C and intercepts ~χ0,
corresponding to the ﬁt’s 3 degrees of freedom,
with which we calculate constraints ~χ = ~χ0 +C
~x
and the usual χ2 =
2. In the start-of-run down-
|
load, we precompute ~p and ~χ for the coordinates
at the edge of each pattern and store them in ﬂash
memory. Using each candidate’s pattern ID as a
hint, the ﬁtter board computes corrections to ~p
and ~χ with respect to the pattern edge, using 8-
bit multiplication in 6 parallel FPGAs, in 250 ns
per ﬁtted track. Tracks passing programmable
goodness-of-ﬁt cuts propagate downstream.

~χ
|

·

·

3. SVT DIAGNOSTIC FEATURES

An SVT whose processing time, resolution, or
ineﬃciency were 20-30% larger would still have
enabled novel physics results at CDF. But an
SVT that could not be commissioned quickly or
operated reliably would have been a failure. Sev-
eral design features of SVT contributed to its
rapid commissioning and reliable operation.

The essence of SVT’s component-based archi-
tecture is captured by the SVT cable and the SVT
Merger board. Nearly all SVT internal data—
hit centroids, drift chamber tracks, pattern IDs,
track candidates, and ﬁtted SVT tracks—travel
as LVDS signals on common 26-conductor-pair
cables carrying data bits, a data strobe, a ﬂow-
control signal, and a ground pair. The data are
variable-length packets of 21-bit words, plus end-
packet and end-event bits. Data fan-in and fan-
out are performed inside FPGAs, not on back-
planes, by a universal Merger board that con-
catenates event data for up to four SVT cable
inputs and provides two SVT cable outputs. Ev-

4

ery fan-in stage compares event IDs for its sources
and can drive a backplane error line on mismatch.
A parity bit for each cable-event provides a ba-
sic check of data integrity.
It is illustrative of
SVT’s design strategy that the SVT cable and
Merger board were prototyped and tested before
the boards to cluster hits, ﬁnd and ﬁt tracks, etc.
The Merger board is reminiscent of the fan-
in/fan-out modules found in NIM trigger elec-
tronics, and lends itself to the same kind of inven-
tive ad-hoc cabling for producing quick results in
test stands and during system commissioning.

On each end of every SVT cable is a circu-
lar memory buﬀer that records—as a logic state
analyzer—the last 105 words sent or received on
that cable. Comparing a sender’s output buﬀer
with a receiver’s input buﬀer checks data trans-
mission. Comparing a board’s input and out-
put with emulation software checks data process-
ing. The memories also serve as sources and
sinks of test patterns for testing single boards,
a small chain of boards, a slice of SVT, SVT as
a standalone system, or the data paths to SVT’s
external sources and sink. The buﬀers can be
frozen and read by monitoring software parasiti-
cally during data-taking, and all of SVT’s buﬀers
can be frozen together, via backplane signals,
when any board detects an error condition, such
as invalid data.

By polling SVT’s circular memories during
beam running,
large samples of track and hit
data, pattern IDs, etc.—unbiased by L2 or L3
trigger decisions—are sampled and statistically
analyzed to monitor data quality. A beam-ﬁnding
program monitors 107 tracks per hour, ﬁtting and
reporting to the accelerator control network an
updated Tevatron beamline ﬁt every 30 seconds.
The beam ﬁt is also written to the DAQ event
record and used to correct in-situ every SVT
track’s impact parameter for the sinusoidal bias
vs φ resulting from the beamline’s oﬀset from the
detector origin, so that the trigger is immune to
modest beam oﬀsets.

The ﬂexibility of FPGAs has been exploited
throughout SVT, enabling SVT to adapt to un-
forseen circumstances when commissioning the
detector and trigger as a whole. Later boards had
the beneﬁt of more ﬂexible programmable chips.

In particular, the board that subtracts the beam
oﬀset track-by-track—the last SVT board to be
built—illustrates well both the utility of modern
FPGAs and the virtue of a component-based ar-
chitecture. It was designed as a clean-up board,
beyond the SVT baseline, to ensure that at most
one SVT track is output per XFT track. SVT’s
modularity allowed this ﬁnal processing stage to
be added seamlessly. Progress in FPGA technol-
ogy allowed the board to consist essentially of in-
put circuitry + large FPGA + output circuitry.
With this design, it was straightforward to adapt
the board to subtract a sinusoidal beam oﬀset—
which proved more convenient than the baseline
plan to steer the Tevatron beam. This clean-up
board has found even further uses, such as record-
ing SVT’s event-by-event processing time into the
DAQ event record and online monitor.

In conclusion, the Silicon Vertex Trigger has
been commissioned and operated successfully for
CDF’s ﬁrst year of Run 2 physics data. Among
the key reasons for this system’s success are its
modular architecture and its ability to sink and
source test data at a wide range of pipeline stages,
both in tests and during beam runs. SVT’s ﬂex-
ibility and diagnostic features were particularly
valuable during the CDF commissioning period.

REFERENCES

[CDF-II Collaboration],

1. R. Blair

et al.
FERMILAB-PUB-96-390-E.
2. D. Ambrose, these proceedings.
3. L. Miller, these proceedings.
4. D. Acosta et al.

[CDF Collaboration],
FERMILAB-PUB-03/048-E. Submitted to
Phys. Rev. D.

5. E.J. Thomson et al.

[CDF Collaboration],

IEEE Trans. Nucl. Sci. 49, 1063 (2002).
6. S. Belforte et al. [CDF Collaboration], Nucl.

Instrum. Meth. A501:201-206, 2003.

7. SVT Technical Design Report internal note

CDF/DOC/TRIGGER/PUBLIC/3108.
8. M. Dell’Orso, L. Ristori, Nucl.

Meth. A278:436-440, 1989.

Instrum.


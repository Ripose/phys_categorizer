6
0
0
2
 
n
u
J
 
5
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
2
4
0
6
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

Data processing model for the CDF experiment

J. Antos, M. Babik, D. Benjamin, S. Cabrera, A.W. Chan, Y.C. Chen, M. Coca,
B. Cooper, S. Farrington, K. Genser, K. Hatakeyama, S. Hou, T.L. Hsieh, B. Jayatilaka,
S.Y. Jun, A.V. Kotwal, A.C. Kraan, R. Lysak, I.V. Mandrichenko, P. Murat, A. Robson,
P. Savard, M. Siket, B. Stelzer, J. Syu, P.K. Teng, S.C. Timm, T. Tomura, E. Vataga, and
S.A. Wolbers

Abstract— The data processing model

for the CDF
experiment
is described. Data processing reconstructs
events from parallel data streams taken with different
combinations of physics event triggers and further splits
the events into datasets of specialized physics datasets.
The design of the processing control system faces strict
requirements on bookkeeping records, which trace the
status of data ﬁles and event contents during processing
and storage. The computing architecture was updated to
meet the mass data ﬂow of the Run II data collection,
recently upgraded to a maximum rate of 40 MByte/sec. The
data processing facility consists of a large cluster of Linux
computers with data movement managed by the CDF data
handling system to a multi-petaByte Enstore tape library.
The latest processing cycle has achieved a stable speed of

J. Antos, and M. Babik are with Institute of Experimental Physics,

Slovak Academy of Sciences, Slovak Republic.

D. Benjamin, S. Cabrera, M. Coca, and A.V. Kotwal are with Duke

University, Durham, NC 27708, USA.

A.W. Chan, Y.C. Chen, S. Hou, T.L. Hsieh, R. Lysak, M. Siket, and
P.K. Teng are with Institute of Physics, Academia Sinica, Nankang,
Taipei, Taiwan.

B. Cooper is with University College London, London WC1E 6BT,

United Kingdom.

United Kingdom.

S. Farrington is with Liverpool University, Liverpool L69 7ZE,

K. Genser, I.V. Mandrichenko, P. Murat, J. Syu, S.C. Timm,
and S.A. Wolbers are with Fermi National Accelerator Laboratory,
Batavia, IL 60510 USA.

K. Hatakeyama is with The Rockefeller University, New York, NY

10021, USA.

48109, USA.

USA.

19104, USA.

Kingdom.

Japan.

87131, USA.

B. Jayatilaka is with University of Michigan, Ann Arbor, MI

S.Y. Jun is with Carnegie Mellon University, Pittsburgh, PA 15213,

A.C. Kraan is with University of Pennsylvania, Philadelphia, PA

A. Robson is with Glasgow University, Glasgow G12 8QQ, United

P. Savard is with University of Toronto, Toronto M5S 1A7, Canada.
B. Stelzer is with University of California, Los Angeles, Los

Angeles, CA 90024, USA.

T. Tomura is with University of Tsukuba, Tsukuba, Ibaraki 305,

E. Vataga is with University of New Mexico, Albuquerque, NM

35 MByte/sec (3 TByte/day). It can be readily scaled by
increasing CPU and data-handling capacity as required.

Index Terms— PACS: 07.05-t. Keywords: Computer sys-

tem; data processing; GRID

I. INTRODUCTION

High-energy physics has advanced over the years
through the use of higher energy and higher inten-
sity particle beams and more capable detectors lead-
ing to the collection of larger volumes of data. The
Tevatron Run II project has increased the intensity
and energy of the proton and anti-proton beams [1].
The Collider Detector at Fermilab (CDF) experi-
ment has improved its data acquisition capacity in
the Run II program and is committed to studying
the frontier of particle physics at Tevatron [2]. The
computing facility was also upgraded for processing
larger volumes of data.

In this paper we ﬁrst describe the CDF data
acquisition system with parallel streams of raw data
being logged to a mass storage library managed
by the Enstore software system [3]. Section 2 de-
scribes the trigger system and the organization of
data streaming. Data processing further splits the
events into datasets of various physics interests. The
CDF computing requirement and the usage of PC
cluster are discussed in section 3. The computing
binary job is organized by collecting the required
executable and software library in an archived ﬁle. It
is suitable for submission in a distributed computing
environment. The data processing architecture is
described in section 4.

The CDF data ﬁles on tapes are registered to
a Data File Catalog (DFC) [4]. Data access has
recently been migrated from direct
tape read to
the SAM (Sequential Access to data via Metadata)
data handling system [5]. Prior to this change, data

processing was operated with the Farm Processing
System (FPS) [6]. The PC cluster acting as the
data production farm was upgraded with the CDF
Analysis Farm (CAF) software [7] which imple-
ments job submission and network access to the
CDF data-handling system. Details of the FPS and
SAM production farm systems are described in
Section 5. The upgrade of the CDF data processing
has incorporated the advances of SAM and CAF
and is compatible with recent GRID computing
development at Fermilab [8].

The design of the data processing includes task
preparation, bookkeeping of input, output splitting
and concatenation, and the ﬁnal storage to Enstore
tape library. The bookkeeping is required to ensure
that there is no missing or duplicated data. For the
FPS farm, it is achieved using an internal database
and for the SAM farm by ﬁle metadata registered
in the SAM database, and is discussed in Section
6. The computing resource management and recov-
ery of production errors are also discussed. The
processing capacity allows data to be reconstructed
in a timely manner. The use cases described in
Section 7 include detector monitoring, calibration,
and physics data processing. The experience of CDF
data processing is discussed in Section 8. The SAM
farm processing is scalable by increasing parallel
processes to the network and the Enstore access
limit. Data processing capacity and scalability are
discussed in Section 9.

II. RAW DATA RECORDING

The CDF detector

is a large general pur-
pose solenoidal detector which combines preci-
sion charged particle tracking with fast projective
calorimetry and ﬁne grained muon detection. Mea-
surements of track trajectories and energy depo-
sitions in sub-detectors are collected for charged
and neutral particles that result from the proton-
anti-proton collision. The data is organized into
logical sets, each set being part of a “run”. Each
“run” corresponds to a time period during which
the detector and beam conditions are stable. The
readout electronics has a tiered trigger architecture
to accommodate a 396 ns proton-antiproton bunch
crossing time. To select only the most interesting
events a three level triggering system is used. The
data ﬂow is illustrated in Fig. 1. A pipelined Level-1
and Level-2 trigger is used before full events are se-
lected for Level-3. The Level-2 trigger accept ﬂags

2

Fig. 1.
CDF data of sub-detectors are pipelined for selection of
the level-1,2 trigger system. The complete event is reconstructed and
selected by the Level-3 trigger. Selected events are written to tapes in
parallel streams. Data are retrieved and processed by the production
farm. The output is split and concatenated into 52 physics datasets.

an event for readout. Data collected in DAQ buffers
are then transferred to the Level-3 processor farm,
where the complete event is assembled, analyzed,
and, if accepted, written out to permanent storage.
Output of Level-3 is handled by a Consumer-Server
and Data-Logging subsystem which delivers events
to mass storage and also to online consumer pro-
cesses for monitoring purposes [9].

Stream trigger contents

event size (kByte)
monitoring
195

ratio to total size (%)

A

B

C

D

E

G

H

J

high ET leptons, missing ET

high ET photons, di-photons

study stream

tau, lepton pair, Z to bb, higgs

B to ππ, Bs to Dsπ

J/ψ to leptons, Υ to leptons

140

122

344

146

139

136

146

2.5

11.5

13.0

11.2

16.0

11.2

24.0

10.6

QCD jet, Di-jet, Miss Et plus jets, high PT b-jet

STATISTICS OF DATA STREAMS IN 2005. LISTED ARE THE

TRIGGER CONTENTS, AVERAGE EVENT SIZE AND THE RATIO TO

TABLE I

TOTAL SIZE.

The event triggers were chosen with the physics
goals of the experiment in mind. Each event satisﬁes
one or more triggers. Events accepted by similar
triggers are written to one of 8 data streams listed
in Table I. Each of the 8 data streams will be further
split into a total of 52 datasets as part of the event
processing stage. Each event is written to one or
more of the 8 data streams based on the triggers that
it satisﬁes. The streams and datasets are chosen to
maximize the physics utility of the datasets while at
the same time reducing the number of events that are
written to multiple data streams. The data is stored
directly to Enstore tapes.

The raw data streams are for physics as well as for
detector monitoring purposes. The size of an physics
event is on average 140 kByte. Overlap between
streams is about 5% and increases with trigger rate.
The fractions of data streams vary: the largest one is
for a dataset used to study physics related to charm
and bottom quarks and it represents about 24% of
all accepted events. Up to early 2006, a total of
2.4 billion events were taken with total size of 387
TByte.

III. COMPUTING REQUIREMENT

The processing farm is required to reconstruct
the data collected by the experiment in a timely
fashion. The type of computing required for CDF
data reconstruction can be characterized as loosely-
coupled parallel processing [10]. Each event
is
independent in the sense that it can be processed
by the ofﬂine reconstruction code without use of
information from any other event. The reconstruc-
tion program transforms digitized electronic signals

Fig. 2. The CDF production farm infrastructure (stale of 2006).

3

from the CDF sub-detectors into information that
can be used for physics analysis. The quantities cal-
culated include particle trajectories and momentum,
interaction vertex positions, energy deposition, and
particle identities.

The CDF data processing facility is constructed
using cost-effective dual CPU and multi-core CPU
PCs running Linux. The farm consists of a large
number of PCs that run the CPU-intensive binary
codes. Data are buffered in and out of the farm
using a disk cache system. Data ﬁles are provided
by the CDF data-handling and database server for
allocation and registration of ﬁles to the Enstore
mass storage. The hardware architecture of the CDF
production facility is shown in Fig. 2.

The experiment requires rapid availability of data
for data analysis with only a short delay to acquire
the necessary detector calibrations. To accomplish
rapid data processing through the farms, adequate
capacity in network and CPU is required. From
2001 through 2004 the CDF experiment collected
data at a peak throughput of 20 MByte/sec. This
was recently upgraded to 40 MByte/sec. The event
processing requires 2-5 CPU seconds on a Pentium
III 1 GHz PC. To accommodate the peak rate of
40 MByte/sec, corresponding to about 25 million
events (3.5 TByte) data logging a day, the total CPU
required is about 1 THz.

IV. DATA PROCESSING ARCHITECTURE

The event reconstruction jobs are prepared using
information from the CDF database. A binary job
submitted to a worker node ﬁrst receives a binary
“tarball”, which is an archive of ﬁles created with
the Unix tar utility. It is self-contained, having the
event reconstruction executable and all necessary
libraries. The input ﬁle is copied from the CDF data-
handling system to the working area. The required
detector calibration and event trigger records are
then accessed. An input ﬁle is approximately 1
GByte in size and takes about 5 hours on a Pentium
III 1 GHz machine to process. The output is split
into multiple ﬁles. Each ﬁle corresponds to a pri-
mary dataset deﬁned by the event type in the trigger
system. An event may satisfy several trigger patterns
and is consequently written to multiple datasets that
are consistent with that event’s triggers. The overlap
of events in multiple datasets is about 6%.

Once the processing job is successfully ﬁnished,
all output ﬁles are copied to a cache area on a ﬁle-

server where small ﬁles of the same dataset are
concatenated in run and event order to a 1 GByte
ﬁle suitable for tape logging. Depending on the
event type, the size can vary from 20 kByte to over
300 kByte. The output event has physics quantities
added to the input, and is larger. The total size of
production output increases by 20%. For effective
data handling, some of the datasets (of streams H
and J) are also split with compressed event contents,
and the ﬁle size is reduced by a factor of three.

The Run II CDF data-handling system has well-
deﬁned interfaces and operation [11] to provide
input data for the farm and to write output to the
Enstore mass storage system. Raw data from the
experiment is ﬁrst written to tape in Enstore. These
tapes are registered in the Data File Catalog as
a set of tables in an Oracle database. The data-
handling system has been migrated to a SAM data-
handling system with the ﬁle description recorded as
metadata in a database. The SAM data management
is organized around a set of servers communicating
via CORBA [12] to store and retrieve ﬁles and
associated metadata. Data ﬁles are registered and
stored to required SAM cache and Enstore storage.
At Fermilab, the CDF data-handling system has a
large dCache disk pool [13] interface to Enstore
storage. By launching a SAM project with a pre-
deﬁned dataset, SAM delivers ﬁles to the desired
cache area.

V. CDF PRODUCTION FARM

The CDF production farm performs computing
and network intensive tasks in a cost-effective man-
ner and is an early model for such computing. His-
torically, Fermilab has used clusters of processors to
provide large computing power with dedicated pro-
cessors (Motorola 68030) [14] or commercial UNIX
workstations [15]. Commodity personal computers
replaced UNIX workstations in the late 1990’s.

The production farm development started in the
late ’90s. The challenge in building and operating
such a system is in managing the large ﬂow of
data through the computing units. In addition, the
production farm operation is required to be easily
manageable, fault-tolerant and scalable, with good
monitoring and diagnostics.

CDF Run II data collected in 2001 to 2004 were
processed by the ﬁrst developed Farm Processing
System (FPS) using the Fermilab developed FBSNG

4

batch system [16]. The disk cache used for input
and output ﬁles in process was a collection of
“dfarm” ﬁle systems [17] consisting space on the
IDE disk drives on the workers. An upgrade to
the SAM data-handling system was conducted in
2004. The SAM data-handling system is supported
by Fermilab for ﬁle cataloging and delivery in a
distributed computing system. The SAM production
farm was streamlined with the FPS farm functions
packaged in a modular interface to the CAF com-
puting facility and SAM data-handling system. The
CAF is a Linux PC farm with interface modules
for batch job submission and access to the CDF
data management system and databases. Jobs can be
submitted to batch systems like FBSNG and Condor
[18] in a uniform manner. The new SAM data
production system is suitable for job submission
to any computing facility in the world that uses
CAF interface with direct access to the SAM data-
handling and database connections. The production
farm is thus a genuine Linux PC farm in a shared
computing environment with other CDF computing
facilities deployed at Fermilab and in many CDF
collaboration institutes all over the world. Many of
them are GRID computing facilities supplied with
a CAF headnode and SAM stations.

A. FPS farm

The Farm Processing System was the software
that managed, controls and monitored the CDF
production farm from 1999 to late 2005. It was
designed to be ﬂexible in conﬁguration for produc-
tion of datasets operated independently in parallel
farmlets. A farmlet is a subset of the farm resources
speciﬁed for the input dataset, the executable and the
output conﬁguration for concatenation. Its execution
is handled by its own daemons taking care of
consecutive processing in production and its records
are written in the internal database.

The FPS-farm had two server nodes hosting the
FBSNG batch submission system, dfarm server, and
the MySQL [19] server used as the farm processing
database. The dual CPU workers were purchased
over years with old nodes replaced after three years
service. At its peak in mid-2004, there were 192
nodes in service. The dfarm used as the working
cache was as large as 23 TByte including three
ﬁle-servers each having 2 TByte. The input and
output (I/O) interface to the Enstore storage was

5

a time. A ﬁle-set is a collection of ﬁles with
a typical size of 10 GByte. The stager fetches
DFC records for input and checks that proper
calibration constants are available. The staging
jobs are submitted to the input I/O nodes and
the ﬁle-sets are copied to their scratch area, and
afterwards to dfarm.

• Dispatcher does the job submission through
batch manager to worker nodes. It looks for the
staged input ﬁle, which is then copied into the
worker scratch area. The binary tarball and run-
splitter of trigger parameters are also copied.
The reconstruction program runs locally on the
worker nodes and the output ﬁles are written
locally. At the end of the job the output ﬁles
are then copied back to dfarm.

• Collector gathers histogram ﬁles, log ﬁles and
any additional relevant ﬁles to a place where
members of the collaboration can easily access
them for the need of validation or monitoring
purposes.

• Concatenator is responsible for collecting out-
put ﬁles in run and event order to be concate-
nated into larger ﬁles with a target ﬁle size
of 1 GByte. It performs a similar task to the
dispatcher according to the internal database
records for a list of ﬁles and the concatenation
jobs are submitted to output nodes. The output
nodes collect ﬁles corresponding to a ﬁle-set
size (≈ 10 GByte) from dfarm to the local
scratch area and executes a merging program
to read events in the input ﬁles in increasing
order of run numbers. It has a single output
truncated into 1 GByte ﬁles. These ﬁles are
directly copied to tapes, and DFC records are
written.

Since all of the farmlets shared the same sets of
processors and data storage, the resource manage-
ment was a vital function of FPS for distribution
and prioritization of CPU and dfarm space among
the farmlets. The additional daemons are:

• Resource manager controls and grants allo-
cations for network transfers, disk allocations,
CPU and tape access based on a sharing algo-
rithm that grants resources to each individual
farmlet and shares resources based on priori-
ties. This management of resources is needed
in order to prevent congestion either on the
network or on the computers themselves and

Fig. 3. FPS production farm architecture.

conducted by 16 Pentium nodes conﬁgured with
the pnfs ﬁle system [20] that accesses the Enstore
tape library. The number of workers and I/O nodes
could be scaled to as large a number as was required
and was chosen to match the total data through-put
capacity and the number of Enstore movers (tape-
drives) available.

The production algorithm is logically a long
pipeline with the constraint that ﬁles must be han-
dled in order. The data ﬂow in FPS is illustrated
in Fig. 3. The FPS tasks were conducted with
control daemons running on the servers for resource
management and job submission. Monitoring and
control interfaces for farm operation included a java
server to the control daemons and a web server for
monitoring. The operation daemons were conﬁgured
speciﬁcally for production of a input “dataset”,
usually of a speciﬁc data taking periods of CDF.

The FPS production began with a farmlet menu
selecting input dataset and run range, tarball ver-
sion, and output directories. Input ﬁles were fetched
directly from Enstore tapes and the outputs were
written to output tapes. The ﬁles moving through
dfarm were controlled by four production daemons.
The daemons communicated with the resource man-
ager daemon and the internal MySQL database to
schedule job submission. The internal database was
used for task control and ﬁle-tracking. The tasks in
sequence controlled by the daemons are :

• Stager is responsible for ﬁnding and delivering
ﬁles from tapes based on the farmlet menu
for data ﬁles in a run range in the dataset.
Jobs are typically submitted one “ﬁle-set” at

to use certain resources more effectively.

• Dfarm inventory manager controls usage of
the distributed disk cache on the worker nodes
that serves as a front-end cache between the
tape pool and the farm.

• Fstatus is a daemon that checks periodically
whether all of the services that are needed for
the proper functioning of the CDF production
farm are available and to check the status of
each computer in the farm. Errors are recog-
nized by this daemon and are reported either
to the internal database which can be viewed
on the web or through the user interfaces.
The FPS system status was shown in real time
on a web page giving the status of data processing,
ﬂow of data, and other useful information about the
farm and data processing. The web interface was
coded in the PHP language [21] and RRDtool [22]
for efﬁcient storage and display of time series plots.
The structural elements in the schema include output
from FPS modules, a parser layer to transform data
into a format suitable for RRDtool, a RRDtool cache
to store this data in a compact way, and ﬁnally the
web access to RRD ﬁles and queries from MySQL
for real time display of ﬁle-tracking information.

The FPS framework ran on one of the servers and
depended on the kernel services namely the FBSNG
batch system, and the FIPC (Farm Interprocess
communication) between the daemons and dfarm
server governing available disk space on the worker
nodes. Daemons had many interfacing components
that allowed them to communicate with the other
needed parts of the ofﬂine architecture of the CDF
experiment. Those included mainly the DFC (Data
File Catalog) and the Calibration Database.

B. SAM production farm

A production farm upgrade was necessary to
accommodate the increasing data acquisition rate
and the migration of the CDF data-handling sys-
tem to SAM. The production farm was converted
to a CAF facility with direct access to the CDF
dCache disk pool and Enstore tape storage. The
functions of the FPS farm were replaced by new
modular applications for job submission to CAF
and ﬁle delivery performed by SAM applications.
The worker nodes are only used for job execution.
The dfarm ﬁle system was not used; instead, staging
of input ﬁles is conducted by SAM on a dedicated

6

Fig. 4. SAM production farm architecture.

dCache disk pool. The job outputs are sent to a
“durable storage” on which the ﬁles reside brieﬂy
before being stored elsewhere. The durable storage
in use are disk RAIDs on ﬁle-servers equipped with
one or more giga-bit ethernet links. In consideration
of network bandwidth and disk cache usage, the
SAM production farm is conﬁgured for direct ﬁle
copy from the dCache system where input ﬁles are
staged from Enstore. Concatenated output ﬁles are
transferred directly to Enstore.

The production job submission is controlled by
scheduled cron jobs on a server node that is conﬁg-
ured as a SAM station to issue SAM client tasks.
The SAM farm design is modular and allows for
more ﬂexible use of worker nodes and storage. The
data ﬂow in the SAM production farm is illustrated
in Fig. 4. A production task is launched as a SAM
project associated with a predeﬁned user dataset
containing the ﬁles to be processed.

• Preparation of input datasets:

Input data to be processed are selected by
querying raw data DFC records, readiness in
data-handling system, and the presence of de-
tector calibration. The input datasets are orga-
nized in run sequence of one or multiple runs
from a single raw data stream.
The dataset bookkeeping list is updated contin-
uously. The expected output ﬁles are checked
for. A dataset not yet having complete output
will be submitted to CAF.

• Starting a SAM project for CAF submis-

sion:

A user deﬁned dataset not in use is checked
to see whether its output is complete. If not, a
SAM project is launched and a SAM consumer
process is established for delivery of ﬁles in the
dataset to the dCache pool. A CAF submission
follows for the associated project. The binary
tarball is distributed to worker nodes in CAF.
It is extracted by each process on the local
working area and the task to be executed is
given by a shell script. A SAM query is then
made asking for the location of an input ﬁle
on dCache. The input ﬁle is copied to the
local working area. The detector calibration
and online split table are fetched. The binary
job is executed for one input data ﬁle. After the
program is successfully terminated, all output
ﬁles are declared to SAM and are copied to the
dedicated durable storage nodes.
The worker script asks for another ﬁle not yet
consumed. The same execution is practised un-
til all ﬁles in dataset are consumed. Afterwards,
the project is closed, the CAF job terminates
and all log ﬁles are copied to servers.
The SAM metadata of output ﬁles are used
extensively for bookkeeping purpose. It has a
parentage record of the input ﬁle, and after
concatenation will be updated with the ﬁle it
is merged into. The completeness of a dataset
is checked by querying all its output metadata.
Monitoring of a SAM project is illustrated in Fig. 5,
which shows the ﬁle consumption status in time.
In this case input ﬁles are already buffered on
dCache and are quickly distributed to workers on
a CAF. The processing of an input ﬁle takes about
4 hours and the workers stay busy until all ﬁle are
consumed.

Fig. 5. The consumption of ﬁles by a SAM project is plotted for a
dataset of 71 input ﬁles. Files were quickly ”buffered” to CAF and
the batch job was conﬁgured to use 30 CPU’s. The executable lasted
for approximately 4 hours. Consumed ﬁles were being ”swapped”.
The project was terminated after all ﬁles were swapped.

7

The output of CAF jobs is sent to durable storage
on ﬁle servers. The durable storage consists of a
total of 52 directories; one for each primary physics
dataset. The concatenation job is controlled by a
cron job which does a periodic check on the total
number of ﬁles accumulated in a primary physics
dataset. If the number of ﬁles passes a threshold (for
example, 100 ﬁles) a concatenation job is launched
to merge ﬁles into output ﬁles of size close to 1
GByte. The operations in sequence are:

• Preparation of ﬁle list:

The SAM metadata of ﬁles to be merged are
checked to ensure none was processed previ-
ously. These ﬁles are sorted by run number into
lists of ﬁles with a constraint that the size of
each output ﬁle is close to and not exceeding
1 GByte.

• Concatenation job:

A concatenation binary is executed in order
for ﬁles on each lists. It does the ﬁle opening,
block-move to the merged output, until all ﬁles
in a list are ﬁnished. The merged output ﬁles
are sent to the “merged” directory ready to
be stored to SAM. SAM metadata is declared
for the merged ﬁle recording the parents of
their input ﬁles. The metadata of ﬁles being
merged are updated recording the ﬁle they have
entered.
• SAM store:

Merged ﬁles are scanned periodically by a cron
job to check whether they exceed a threshold
(for example 10 ﬁles). If a ﬁle has not been
stored, the SAM store command is issued to
copy it to Enstore mass storage and its meta-
data are declared. The threshold size is tuned
to optimize Enstore operations.
The metadata of a stored ﬁle will
later be
updated with its tape volume and location in the
pnfs ﬁle system. A merged ﬁle having complete
metadata records for its tape location is then
deleted from durable storage disks.

The concatenation is issued with no constraint
imposed on the SAM project status. This allows for
modular operation separating usage of durable stor-
age from CAF computing activities. In order to have
concatenation ﬁles organized in run order, the CAF
jobs are scheduled such that the output arrives in
order. The threshold on the number of ﬁles required
for launching concatenation is used as a throttle to

allow sufﬁcient time to wait for slow CPU nodes
whose output arrives late. Files missing as a result
of other failures will be recovered automatically by
the job submission, and with sufﬁcient buffer size
in the durable storage, the ﬁle order is organized
by the concatenation process. Previously in the FPS
operation, concatenation waited for speciﬁc output
ﬁles of submitted jobs. As the executable may crash
and hardware does fail, it resulted in unexpected
congestion in data ﬂow waiting for missing ﬁles.
This is relaxed in the new concatenation algorithm,
checking only ﬁles that have arrived at durable
storage.

The usage of large ﬁle servers for durable storage
has provided robust ﬁle management. This, however,
has led to concern over collecting hundreds of ﬁles
from CAF workers through giga-bit ethernet links.
To achieve maximum ﬁle movement speed, con-
catenation jobs run locally on the ﬁle servers. The
ﬁnal storage copying merged ﬁles to Enstore tape
typically runs at 20 MByte/sec. A ﬁle server with
dual Pentium 2.6 GHz CPUs can move, concatenate
and SAM store to tape about 1 TByte/day.

VI. BOOKKEEPING, RESOURCE MANAGEMENT
AND RECOVERY

The production farm processes hundreds of ﬁles
at a time and effective bookkeeping is essential.
The contents of bookkeeping are the history of the
ﬁles as they ﬂow through production, relation of
input and output, and status of binary reconstruction
and hardware. The MySQL database was a good
choice for the FPS system, and was used since the
production farm development. The ﬁle status in FPS
was traced by job daemons in three tables. Usage
of these tables are:

• Stage-in table:

A cron job does a periodic check on the farmlet
run range, makes a query to the DFC database
to update the stage-in table for input ﬁles to
be processed. The stager daemon is operated
according to this table to copy ﬁles from En-
store, and afterwards move their records off the
stage-in table to a history table.

• Reconstruction table:

The dispatcher daemon checks on staged ﬁles
and their history in stage-in table. Files not
yet processed are registered in this table and
are later submitted to FBSNG. The ﬁle con-
sumption status are then updated by the worker

8

script. Once a ﬁle is successfully processed, its
records are copied to the corresponding history
table. Its output ﬁles are registered into the
concatenation table to be picked up by the
concatenation daemon.
• Concatenation table:

A concatenation job is organized with a query
to MySQL for processed ﬁles of approximately
10 GByte. The concatenation process split out-
put into 1 GByte ﬁles. Therefore an input ﬁle
often is split into two concatenated ﬁles. This
leads to a difﬁculty in recovery, if required, for
tracing its input.

For debugging purpose,
the FPS bookkeeping
database has recorded details on ﬁle delivery time
and binary crash records. These records are valuable
to diagnose data handling and binary problems. Use
of these records provided valuable guidelines in the
upgrade to the SAM production farm.

The bookkeeping for the SAM farm was stream-
lined by appending ﬁle parentage records to meta-
data of ﬁles in the SAM data handling system. Job
submission is based on the cumulative records of
datasets in SAM and is therefore simpler than the
FPS farm in tracking individual ﬁles. The com-
munication of production tasks with DFC records
and SAM metadata is illustrated in Fig. 6. These
operations are:

• Input datasets:

Preparation of input datasets is conducted by
a periodic cron job fetching online records for
newly available data. New data are organized
into SAM datasets that are retrieved by the cron
job doing CAF submission.

• Metadata of reconstruction output:

The completeness of an input dataset
is
checked for the expected output. Each output
ﬁle has metadata created when it is successfully
written. Its records on parent and daughter ﬁles
are the two tags that help deﬁne its complete
history in production. The parent is the input
ﬁle from which it is created, and the daughter
is the merged ﬁle after concatenation.
• Metadata of concatenation output:
The reconstruction output ﬁles
stored in
durable storage are checked for the existence
of their daughters in the concatenated ﬁles.
Those not yet been merged are collected for
concatenation. This procedure prevents dupli-

9

action in the course of processing. This relies on
the cron job scripts that check bookkeeping records
for automated recovery and return to a state from
which the farm can safely continue to operate.
The human effort
is then spared for debugging
abnormal failures in hardware or other failures. The
parentage of metadata used for the SAM production
farm provides a convenient bookkeeping interface.
A concatenated ﬁle is a merge of full ﬁles. It is
easier to recover based upon its parentage list.

VII. PRODUCTION FARM USE CASES

The mass data reconstruction is used for various
purposes, including detector and data monitoring,
calibration creation, processing for physics analysis,
and reprocessing when that is required to take into
account improvements in calibrations and/or soft-
ware. The three main categories of farm operation
are:

• Calibration:

The most urgent calibration required by CDF
is the determination of the proton-antiproton
beam-line position. This process is run as soon
as new data is available in SAM. The exe-
cutable uses only a small portion of tracking
data and output histogram ﬁles are sent to a
dedicated storage area for rapid availability.
Meaningful physics data are processed with
a complete set of detector calibrations that
may require large statistics of certain types of
events. Calibration is a time consuming proce-
dure because it requires careful examination of
the detector performance and understanding of
variations from what is expected.

• Detector monitoring:

A monitoring data stream is processed instantly
once the beam-line calibration is available.
The purpose is for examination of detector
performance. Depending on the colliding beam
quality and the presence of sub-detectors, the
data quality is categorized. Usage of physics
datasets is deﬁned accordingly.

• Physics data processing:

the
Once the calibration data are available,
raw data streams are processed. For the con-
venience of bookkeeping and resource man-
agement,
the data streams are scheduled in
parallel. In the SAM farm operation, one raw
data stream is processed by about 200 workers

Fig. 6. Communication with online DFC records and SAM metadata
by the production tasks along the data ﬂow from left to right.

cation of production output. The metadata of a
merged ﬁle inherits all the parents of its input
from reconstruction. This creates a complete
bookkeeping chain for production datasets and
outputs. The archive of parentage records sum-
maries the production activity.

The bookkeeping of the SAM production farm has
advanced from counting individual ﬁles to datasets:
each is submitted as a SAM project to a correspond-
ing CAF job. Therefore the monitoring of produc-
tion is reduced to monitoring datasets in progress.
The resource management is thus focused on the
number of live datasets, ﬂow of data-handling, and
the durable storage reserved.

Binary crashes and hardware failure always occur
and are not easily predictable. To prevent conges-
tion, in the SAM farm design a latch mechanism
was implemented by tracking the availability of
resources. The resource management falls into the
categories of CPU, cache storage, database ser-
vices and data-handling system. The old FPS farm
managed resources using daemons that monitored
their status. To be robust, the SAM farm design
is simpliﬁed with a frequent cron job monitoring
the resources and updating a status page. New
production tasks are prevented if any of the required
services and resources are missing. The concatena-
tion process is ﬂexible and is independent of the
CAF usage. Jobs are not submitted if insufﬁcient
space is available on durable storage.

Having hundreds of ﬁles at a time moving
through the system, it is necessary to have auto-
matic error recovery with minimal human inter-

and the output from all of the workers is sent to
one durable storage ﬁle server. The production
farm throughput for each stream is about 1
TByte per day.
If required, the production farm also does re-
processing of primary datasets, to take advan-
tage of improved binary code or calibrations. In
such case concatenation may not be required.

VIII. EXPERIENCE WITH THE PRODUCTION
FARM

The production farm has been in operation since
the beginning of CDF Run II data collection. The
Tevatron Run II commissioning run started in Oc-
tober, 2000 and the beginning of proton-antiproton
collisions in April, 2001. The data taken under var-
ious beam and detector conditions, consist of about
7.6 million events. The early processing experience
using a prototype FPS production farm gave some
conﬁdence that the farm had the capacity to handle
the volume of data coming from the detector and
also uncovered many operational problems that had
to be solved.

Beginning in June, 2001, both the Tevatron and
the CDF detector operated well and began to pro-
vide signiﬁcant data samples for ofﬂine reconstruc-
tion. This early data was written in 4 streams and the
output of the farms was split into 7 output datasets.
The CDF experiment wrote data at a peak rate of 20
MByte/sec, which met the design goal. The farms
were able to reconstruct data at the same peak rate.
The output systems of the farm were adjusted to
increase their capacity to handle the large output of
the farms. More staging disk was added to provide a
larger buffer and additional tape-drives were added.
By early 2002 the accelerator was running
steadily and the CDF detector was performing well.
Data was recorded in 8 data streams and the out-
put was split into 52 physics datasets. Data was
processed as quickly as possible with preliminary
calibration and was normally run through the farms
within a few days. Approximately 1.2 billion events
were collected and processed between 2002 and
2004. Upgrades were made to the farm with the
addition of new nodes for processing as well as
improved I/O capability. A major reprocessing of
all data collected from the beginning of 2002 was
begun in the fall of 2003. The output of this process-
ing was later reprocessed with improved calibration

10

of calorimetry and tracking. The reprocessing oc-
curred in March, 2004 and the production farm was
operated at full capacity at a rate of processing of
10 million events per day.

To accommodate the migration of CDF comput-
ing to a distributed environment, the SAM farm up-
grade was started in winter of 2004. Building on the
knowledge gained from the FPS farm performance,
the upgrade advanced quickly. The SAM farm tests
were completed in early 2005. By the summer of
2005 the migration was completed. The SAM farm
processing, with its own group of 200 nodes is as
effective as the FPS farm and the operation is easier.
The SAM production farm was tested further on
its scalability and on job submission to a distributed
to
computing environment. Test
remote CAF overseas to prove the mechanism. For
effective network access to dCache and Enstore
storage. The production jobs for 2005 data were
conducted on two CAFs (SAM production farm and
CDF analysis farm) at Fermilab. The largest number
of CPUs used was 560, and was not yet limited,
given a stable processing rate of over 20 million
events a day. A single CAF can be expanded to up to
one thousand CPUs. The bottleneck observed comes
from the 1 Gbit/s network links writing to durable
storage. This bottleneck can be easily eliminated by
dividing the processing into more data processing
streams and more ﬁle servers.

jobs were sent

IX. DATA PROCESSING CAPACITY

The CPU speed and data through-put rate are
two factors that determine the data reconstruction
capacity of the production farm. The computing
time required for an event depends on the event
characteristics determined by the event trigger in
different data streams and the intensity of the proton
and antiproton beams. More intense beams lead to
multiple events per beam crossing which in turn lead
to more CPU time per event. The event size and
CPU time varies for different raw data streams. In
Fig. 7 the CPU time per event is illustrated for the
monitoring stream (Stream A) that has an average
event size that is twice larger than the physics data
streams. The CPU time on a dual Pentium III 1 GHz
machine varies from 1 to 10 sec depending on the
beam intensity and event size.

Inefﬁciency in utilizing CPU is primarily due to
the execution time lost during ﬁle transfers of the

11

Stream_A

Stream_A

)
c
e
s
(
 
t

n
e
v
e

 
/
 

e
m

i
t
 

U
P
C

9
8
7
6
5
4
3
2
1
0

9
8
7
6
5
4
3
2
1
0

Processed, integrated x10-1
Processed/day
Logged/day

50000

40000

30000

20000

10000

)
k
(
 
s
e
t
v
E

0

0

80

200

400
Event size (kB)

60
40
20
L (1030 s-1cm-2)
CPU time per event versus the proton-antiproton beam
Fig. 7.
luminosity and event size of the detector monitoring stream. The
CPU time is normalized to Pentium III 1.0 GHz. The separation in
bands is attributed to the presence of the silicon detector (larger event
size) and to event types.

executable and data ﬁles to and from the worker
scratch area. The data ﬂow is ultimately limited
by the capacity and speed to retrieve and store
ﬁles in the Enstore tape storage. Enstore ﬁles are
read/written directly to/from the durable storage
the tape read/write has a
disk without a buffer,
latency of a few minutes to mount
the tape in
the tape driver. For maximum efﬁciency many ﬁles
of the same dataset are written as a group. The
instantaneous Enstore read/write speed may exceed
30 MByte/sec. However, the average rate drops to
below 20 MByte/sec because of latency in estab-
lishing connection to the mass storage system (this
includes mounting and positioning the tape estab-
lishing the end-to-end communication). Depending
on the latency spent on tape mounting and network
trafﬁc, a steady tape writing rate by one mover for
is about 1 TByte/day.

The output of concatenated ﬁles is copied to
tapes. A tape is restricted to ﬁles of the same
dataset. The tape writing is limited to one mover per
dataset at a time, to ensure that ﬁles are written on
tape sequentially. The effectiveness in tape writing
is a concern because of the limited cache space
and output bandwidth. Concatenation by the FPS
farm was conducted on output nodes. On a Pentium
2.6 GHz node the CPU time is about 24 minutes
for processing 10 GByte, followed by 10 minutes
of copying concatenated ﬁles to tape. In order to
improve the effectiveness copying ﬁles of a dataset
to tapes, up to three concatenation jobs for the same
dataset were submitted in series to eliminate the
waiting time by the mover. Also, running a mix of
jobs from different datasets in parallel increases the
through-put of the farm by increasing the number
of movers writing tapes.

10

20

30

50

60

70

80

40
Days

Fig. 8. Daily processing and logging rates. The integrated processing
rate is shown in line.

The FPS farm capacity may be illustrated by the
latest data reprocessing of primary physics datasets
performed in March 2004. To maximize resource
usage, the reprocessing was performed on ﬁve farm-
lets with each farmlet processing one dataset. The
stager was submitted one dataset at a time, therefore
the farm CPU usage came in waves of two or more
datasets at a time. The stage-in was effective in
feeding data ﬁles to dfarm. The CPU usage was
efﬁcient. The data processing rate is shown in Fig. 8.
On average a through-put of 10 million events (1.5
TByte) per day to the Enstore storage was achieved.
The data logging lasted two extra weeks for a large
B physics dataset that accounted for about a quarter
of the total CDF data. It was the latest dataset
processed and was saturated at about 1 TByte per
day in writing tapes.

The SAM production farm exploits the advan-
tages of the new data handling system and the
capacity of submitting jobs to multiple CAF farms.
The performance may be illustrated in data process-
ing of 2005. The production jobs were submitted
to two CAFs with a total 560 CPU processing 6
raw data streams in parallel. The network through-
put with workers of Pentium-4 3 GHz CPU was
roughly saturating at 200 CPU for one data stream
with reconstruction output sent to durable storage on
one ﬁle server equipped with one 1 Gbit/s network
port. With more than 200 CPU engaged for one
data stream, the reconstruction output from workers
was often waiting in queue to copy ﬁles to durable
storage. The ﬁle server equipped with 1 Gbit/s
ethernet has a constant trafﬁc of over 50 MByte/sec
shared by input and output. The data throughput rate
on a ﬁle server was operated at less than 1 TByte
daily.

The data through-put was tuned to optimize the
use of network, durable storage, and the concatena-
tion of two jobs running on the durable ﬁle servers.

12

at Fermilab. We may triple the data processing
capacity by allocating more resources in use. Also
the continuous growth of computing will allow CDF
to proceed in processing and analyzing data through
to the end of the life of the experiment.

REFERENCES

[1] Tevatron Run II Handbook, http://www-bd.fnal.gov/runII.
[2] The CDF II Collaboration, “The CDF-II Detector: Technical

Design Report”, Fermilab-Pub-96/390-E, October, 1996.

[3] Computing Devision, Fermi National Accelerator Laboratory,
“Enstore mass storage system”, http://www-isd.fnal.gov/enstore/.
[4] P. Calaﬁura et al., “The CDF Run II Data Catalog and Access
Modules”, in ”CHEP 2000, Computing in high energy and nuclear
physics” 494-497, Padova, Italy , 2000.

[5] The SAMGrid project, http://projects.fnal.gov/samgrid/.
[6] J. Antos et al., “Data production of a large Linux PC Farm for

the CDF experiment”, arXiv:hep-ex/0603008, 4 Mar 2006.
[7] CDF Central Analysis Farm (CAF), http://cdfcaf.fnal.gov/.
[8] M. Norman et al., “OSG-CAF - A single point of submission
for CDF to the Open Science Grid”, in “CHEP 2006, Computing
in high energy and nuclear physics” Mumbai, India, 2006; OSG-
doc-371.

[9] Y.S. Chung et al., “The Level-3 Trigger at the CDF Experiment
at Tevatron Run II”, IEEE Trans. Nucl. Sci., 52 1212 (2005).
[10] T. Nash, “Event Parallelism: Distributed Memory Parallel Com-
puting for High Energy Physics Experiments”, Comp. Phys.
Comm. 57 (1989) 47.

[11] R. Colombo, et al., “The CDF Computing and Analysis System:
First Experience”, in “CHEP 2001, Computing in high energy and
nuclear physics” 15-19, Beijing, China, 2001; FERMILAB-Conf-
01/300-E, November, 2001.

[12] Object Management Group, Inc., Needham, MA 02494, USA,

[13] dCache, a distributed storage management data caching system,

http://www.corba.org/.

http://dcache.desi.de/.

[14]

I. Gaines and T. Nash, “Use of New Computer Technologies in
Elementary Particle Physics”, Ann. Rev. Nucl. Part. Sci. 37 177
(1987).

[15] F. Rinaldo, S. Wolbers, “Loosely coupled parallel processing at

Fermilab”, Comput. Phys. 7 184 (1993).

[16] Farms and Clustered Systems Group, Fermi National Ac-
celerator Laboratory, “FBSNG - Next Generation of FBS”,
http://www-isd.fnal.gov/fbsng/.

[17]

I. Mandrichenko et al., “Disk Farm Installation and Administra-
tion Guide”, v1.6, Nov 11, 2001, http://www-isd.fnal.gov/dfarm/.

[18] Condor project, University of Wisconsin-Madison,

http://www.cs.wisc.edu/condor/manual/.

[19] MySQL AB, Uppsala, Sweden, http://www.mysql.com/.
[20]

Information Technology Group, Deutsches Elektronen-
Synchrotron DESY, “The Perfectly Normal File System”,
http://www-pnfs.desy.de/.

[21] The PHP Group, http://www.php.net.
[22] T. Oetiker, “RRDtool”,

http://people.ee.ethz.ch/˜oetiker/webtool/rrdtool/.

Fig. 9. SAM farm CPU load (shaded) in production.

Fig. 10. Network of SAM farm input to durable storage (shaded)
and output to Enstore (line).

Each data stream in process was limited to about
500 GByte per day. The total processing speed was
kept at full CPU usage, corresponding to a total of
over 20 million events (3 TByte) processed. Shown
in Fig. 9 is the load of SAM farm CPU for a run
range of 320 million events. A typical daily trafﬁc
of input and output to Enstore is shown in Fig. 10.
The usage of CPU became less consistent towards
the end of the production. It was because some small
data streams were ﬁnished, leaving fewer number of
streams in production, and busy trafﬁc writing to a
smaller number of ﬁle servers.

X. CONCLUSION

The CDF production farms have been success-
fully providing the computing processing required
for the CDF experiment in Run II. The success of
this system has in turn enabled successful analysis
of the wealth of new data being collected by the
CDF experiment at Fermilab. The system has been
modiﬁed and enhanced during the years of its opera-
tion to adjust to new requirements and to enable new
capabilities. It was recently upgraded to adapt to the
SAM data handling system to be portable on dis-
tributed computing facilities. Its current processing
capacity of 20 million events per day is sufﬁcient
for the data taking rate at 40 MByte/sec maximum
speed. It can be scaled by increasing the number of
input data streams as well as CPU, durable storage,
and tape logging. The production farm system is
about a third of the total CDF computing capacity


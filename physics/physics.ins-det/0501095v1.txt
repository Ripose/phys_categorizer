5
0
0
2
 
n
a
J
 
8
1
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
5
9
0
1
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

INNOVATIVE EXPERIMENTAL PARTICLE PHYSICS
THROUGH TECHNOLOGICAL ADVANCES –
PAST, PRESENT AND FUTURE

H. W. K. CHEUNG

Fermi National Accelerator Laboratory,
P.O. Box 500,
Batavia, IL 60510-0500, USA
E-mail: cheung@fnal.gov

This mini-course gives an introduction to the techniques used in experimental
particle physics with an emphasis on the impact of technological advances. The
basic detector types and particle accelerator facilities will be brieﬂy covered with
examples of their use and with comparisons. The mini-course ends with what
can be expected in the near future from current technology advances. The mini-
course is intended for graduate students and post-docs and as an introduction to
experimental techniques for theorists.

1. Introduction

Despite the fancy title of this mini-course, the intention is to give a brief
introduction to experimental particle physics. Since there are already some
excellent introductions to this topic and some textbooks that cover vari-
ous detectors in detail, a more informal approach to the topic is given in
this mini-course. Some basic detector elements are covered while reviewing
examples of real experiments, and experimental techniques are introduced
by comparing competing experiments. Some aspects of experimental design
are also brieﬂy reviewed. Hopefully this will provide a more engaging intro-
duction to the subject than a traditional textbook. This short mini-course
cannot replace a real experimental physics course; the reader is just given a
taste. Unfortunately the lack of space for this writeup means that not even
the basic detection methods and detector types can be described. Instead
detector types in italics will be brieﬂy described in a glossary at the end of
this writeup. For further reading, the reader can ﬁnd the relevant physics
of particle interactions and detailed descriptions of many diﬀerent types of
particle detectors in a number of textbooks and articles.1

1

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

2

Although this mini-course is devoted to the physics impact of some
signiﬁcant technological advances, it should be noted that the improvements
in experimental techniques usually progress in steady steps. Quite often
advances are linked to steady progress in the following areas:

•
•
•
•
•
•

Higher energy available or/and higher production rate.
Improvements in momentum or/and position resolution.
Better particle identiﬁcation methods.
Increase in detector coverage or energy resolution.
More powerful signal extraction from background.
Higher accuracy (due to increase in data statistics, reduction of
experimental systematic uncertainties, or reduction in theoretical
uncertainties).

Discoveries are often made through a series of incremental steps, though
of course the discoveries themselves can be in a surprising direction! The
topics I have chosen for the two lectures of this mini-course is the discovery
and subsequent study of the charm quark, and the future of bottom quark
physics. The outlines for the two lectures are illustrated in Fig. 1.

2. Part I: Discovery of Charm

The discovery of the J/ψ meson is well documented by many books and
articles2 as well as in the Nobel lectures of Ting3 and Richter.4 Besides
being a great classic story of discovery, we can also use it to illustrate some
of the detection techniques and the physics and ideas behind the design of
the experiments involved.

2.1. A Missed Opportunity: Resolutions Matter!

Since the leptons, electrons and muons, are basically point-like, stable or
long-lived, and interact primarily via the well understood and calculable
electroweak force, they have served as the “eyes” in probing many exper-
imental processes. One of those processes under study in the 1970’s was
hadron interactions. The interests in this study included the investigation
of the electromagnetic structure of hadrons, the study of the then-called
“Heavy photons” ρ, ω and φ and the search for additional ones, as well as
the search for the neutral intermediate vector boson, the Z 0.

One experiment doing such a study oﬀers a lesson on the impor-
tance of experimental resolution. This was an experiment at Brookhaven
National Laboratory (BNL) using the Alternating-Gradient Synchrotron

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

3

Figure 1. Outline for the the mini-course: (top) lecture 1; (bottom) lecture 2.

→

(AGS) carried out by Leon Lederman’s group. They performed studies of
µ+µ−X and missed discovering the J/ψ in 1970, four years before
p + U
the actual discovery.

A diagram showing Lederman’s 1970 experiment is given in Fig. 2.5,6
The experiment was to study the interaction of the 22-30 GeV proton beam
on a Uranium target. The aim was to detect a pair of oppositely charged
muons coming from the interaction.

The emphasis of this experiment was to get a clean signature for muons

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

4

Figure 2. Diagram of the spectrometer for the Lederman 1970 experiment.5

directly produced in the target. The main background to eliminate was
muons from the decay of pions and kaons. A high atomic number target like
Uranium has a short interaction length which serves to both cause a lot of
the proton beam to interact and also to absorb pions and kaons produced in
the interactions before they can decay. This is followed by additional mate-
rial to absorb non-muonic backgrounds and low energy muons. Muons from
hadron decay typically have lower energy than those directly produced in
the primary proton-Uranium interaction. Another specially shaped heavy
absorber serves to absorb more background muons while scintillator ho-
doscopes measure the direction of the surviving muons. The ﬁnal material
at the end of the detector serves to measure the range and therefore the
energy of the muons.

Although all the absorber material helps to give a much cleaner sample
of dimuon events, it also causes a lot of multiple Coulomb scattering (MCS),
especially as the material is of high Z and therefore has short radiation
length. This large MCS limited the dimuon mass resolution at 3 GeV/c2
400 MeV/c2. Even with all the absorber the signal-
to about
to-background (S/B) is relatively small. The S/B at low dimuon mass
2 GeV/c2) was about 2%, increasing up to 50% at higher dimuon mass
(
≈
5 GeV/c2). This means relatively large background subtractions are
(
≈
needed. Another concern was the low acceptance and eﬃciency at low
dimuon mass which therefore needed larger corrections.

13%, or

≈

≈

The raw and corrected dimuon mass distributions are given in Fig. 3.
Although the large background subtraction and the uncertainty in the cor-
rection might have contributed to the missed discovery of a peak at the
J/ψ mass, if the dimuon mass resolution were suﬃciently better, the J/ψ
peak would still have been observed. The experimenters did many tests

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

5

Figure 3. Results for the dimuon mass spectrum from the Lederman 1970 experiment.5
(left) raw distribution; (right) corrected spectrum.

and gave limits for a narrow state, but they had to conclude in the end
that there was “no forcing evidence of resonant structure.”

2.2. Elements of Experimental Design

With hindsight how would we change Lederman’s 1970 experiment so we
could observe the J/ψ? Instead of leaving it as a task to the reader, it is
instructive to go through this in a little detail. The most obvious things to
include in a redesign are the following:

(i) Improve the momentum resolution, which means having less material
for MCS, using a magnet for momentum determination and using a
ﬁner spatial resolution detector than a scintillator hodoscope.
(ii) Increase the S/B, which means separating muons better from hadrons

and enriching the sample of dimuons vs. single muons.

(iii) Achieve better acceptance and eﬃciency, which for a study of the
dimuon mass spectrum means obtaining a ﬂatter eﬃciency as a func-
tion of dimuon mass. A smooth eﬃciency across the dimuon mass is
probably ﬁne as long as the eﬃciency (correction) is well understood.

The average angular deﬂection due to MCS of directly produced (signal)
(LTarget/λ0).
(ZTarget/pµ)pLTarget
muons is given by θMCS
Where λ0 is the radiation length. So to reduce the eﬀects of MCS one
should select a short target with long radiation length and use as high a
beam energy as possible to produce more higher momentum signal muons.

(1/pµ)

×

∼

∼

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

6

Targets with low Z/A will have longer radiation lengths but they also
have lower density and thus a longer target would be needed to get the
same number of inelastic proton interactions in the target. A large signal
sample needs a target with high atomic number, since the dimuon signal
ATarget. Another consideration is that the absorption probability
rate
for pions and kaons
Target. Thus the S/B would increase with heavier
targets and dense targets. One would need to do a Monte Carlo simulation
to study what target material is optimal.

A0.7

∼

∼

If the eﬀects of MCS can be suﬃciently reduced we would need to de-
termine the momentum of the muon more precisely. This can be achieved
with a magnetic spectrometer where the deﬂection in a known magnetic
ﬁeld can give the magnitude of the momentum. The angle of deﬂection can
be obtained with low mass Multiwire Proportional Chambers (MWPC’s)
placed before any of the hadron absorbers.

Figure 4. Schematic for one possible redesign of a detector studying dimuons where the
main consideration was with improving the dimuon mass resolution.

An initial redesign of the detector might look something like the
schematic in Fig. 4. However this does not have all the absorbing ma-
terial of the 1970 Lederman detector to reduce background muons. For
that experiment the S/B
0.04, while in our initial design it could be as
small as 10−6! To see how the S/B could be improved one has to consider
the sources of background. The main ones are given below:

∼

(i) Direct single muons – these should be relatively small at AGS ener-
gies since the production would be through electroweak processes.
Production via the decay of τ leptons or charm particles is of the
same level as the J/ψ, so getting an accidental dimuon pair through
these decays should cause a negligible background.

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

7

(ii) Muons from decays of hadrons – these happen early due to an
exponential decay and should therefore be absorbed early before
they can decay. Also lower momentum hadrons will decay rela-
tively sooner and thus make up a larger fraction of the decay muon
background. One could try to reject softer muons from the data
analysis. Also one could make multiple measurements of the mo-
mentum to reject muons from decay in ﬂight.

(iii) Hadrons from “punch through” – a signal in a detector element
placed after an absorber can arise due to the end of a hadronic
shower leaking through the absorber. One can try to detect this
by having multi-absorber/detection layers which can be used to rec-
ognize a hadronic shower signal from a typically minimum ionizing
muon signature. One can also try to momentum analyze through
a magnetic absorber.

Figure 5. Schematic for a second possible redesign of a detector studying dimuons where
S/B was included as a consideration as well as the dimuon mass resolution.

Another example of a revised design for the dimuon detector, this time
also taking into consideration the S/B is shown in Fig. 5. It is seen that
to do better than the 1970 Lederman experiment one needs a more com-
plicated detector with considerably more advanced detectors. Even so one
can see there is still a compromise made between getting the best S/B and
the best dimuon mass resolution. One would need to do a serious Monte
Carlo simulation to determine the optimal choices.

We have only really touched on the elements of experimental design.
For example timing considerations have been completely ignored and we
have assumed the wire chambers can handle the necessary rates. Instead
of pursuing this further, and also before showing you Lederman’s solution,
we ﬁrst turn to see how Ting solves this experimental design problem.

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

8

2.3. Ting’s Solution

It was recognized that the same physics could be studied by observing pairs
of electrons instead of dimuons. Electrons can be produced by the same
decays and have the same J P C as muons, thus dielectrons should also be
produced by the J P = 1− ρ, ω, φ and J/ψ. However electrons diﬀer in
that they are about 200 times lighter than muons. This greatly changes the
considerations for a detector designed to measure dielectron pairs compared
to dimuon pairs. Although kaon and pion decays are no longer a serious
source of background for a study of dielectrons, the electrons undergo much
more scattering and absorption than muons. Thus the choice of materials
and the detector types used to identify and track electrons is quite diﬀerent.

Figure 6. Schematic of the detector for Ting’s J/ψ observation experiment.3

Figure 6 shows a schematic of the spectrometer used by Ting in his
J/ψ observation experiment. It can be seen that the spectrometer is quite
complex. Low Z beryllium targets and low mass MWPC’s are used to avoid
too many photons converting to e+e− pairs. The use of a multi-magnet
spectrometer and MWPC’s helped to achieve a very ﬁne mass resolution of
5 MeV/c2. Without electron identiﬁcation the S/B would have been
≈
∼
10−6, thus a relative background rejection of 106–108 was needed. Typically
a single particle identiﬁcation detector can achieve a relative background
rejection of 102–103 so multiple systems were combined. Both ˇCerenkov
counters and electromagnetic calorimeters were used to identify electons.
A special ˇCerenkov counter was used to speciﬁcally reject background from

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

9

→

γe+e−. A relative background rejection of 108 was achieved and,
π0
together with a ﬁne dielectron mass resolution, a spectacularly narrow and
clean J/ψ signal was seen. The results and details of how this analysis
was done are well documented in Ting’s Nobel lecture3 and the published
papers.7

2.4. Richter’s Solution

There is another half of the J/ψ discovery story that cannot be covered
in this writeup because of insuﬃcient space. Revealed in that half would
be additional important experimental techniques. For example Richter’s
observation of the J/ψ was made in an e+e− collider, a relatively new
innovation at that time. A nearly 4π detector was used including wire spark
chambers and electromagnetic shower counters. The J/ψ mass resolution
was much better since it was governed by knowledge of the beam energy
and thus the widths of states can be much better measured. That half
of the story and the subsequent studies are well documented in Richter’s
Nobel lecture.4

2.5. Improving Charmonium Spectroscopy
An e+e− collider is an excellent study tool. This was recognized by Ting as
well as by Richter. It is specially well suited to perform detailed studies of
vector particles once their mass is known. This has been the case for char-
monium, for bottomonium and for the Z 0. Narrow states with unknown
masses are diﬃcult to ﬁnd. However special modiﬁcations were made to the
SPEAR e+e− storage ring to enable scans in energy in a relatively short
time. This enabled the discovery of the J/ψ by Richter’s team as well as
some of the charmonium excited states. A disadvantage is that the e+e−
collisions can only directly produce states with J P = 1−, thus only these
are measured with ﬁne resolution. While some of the non-(J P = 1−) char-
monium states could be observed through the decays of the ψ′, see Fig. 7,
the measurements of their masses and widths can no longer be obtained
with just the knowledge of the beam energies.

Further improvement in the knowledge of the charmonium spectrum has
been achieved by using low energy p¯p collisions in an antiproton accumu-
lator. The ﬁrst of these experiments was done at the CERN ISR in R704,
then in E760 and E835 at the Fermilab antiproton accumulator.8 In the
Fermilab experiments, a hydrogen gas-jet target is used and the antiproton
beam is tuned to produce and precisely measure charmonium states of any

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

10

Figure 7. Schematic of the charmonium energy levels.

J P . The charmonium states are tagged by their electromagnetic decays us-
ing lead glass shower counters and scintillating ﬁbres. All that is needed is
to recognize signal from background. The actual mass and width measure-
ments is determined with exquisite (
0.01%) resolution due to excellent
knowledge of the beam energy.

∼

2.6. Lederman’s Two Solutions

We conclude the ﬁrst part of the mini-course with two of Lederman’s so-
lutions to the dilepton experiment design problem. The ﬁrst is his 1976
experiment that looked at e+e− pairs using a higher energy beam running
at Fermilab.9 This detector was a relatively simple experiment using a mag-
netic spectrometer for momentum determination and a lead-glass calorime-
ter for electron identiﬁcation. Although the J/ψ was clearly visible in this
experiment, the background was still relatively high. A cluster of events
6 GeV/c2 which lead to a claim of a possible
was observed at Me+e−
observation of a narrow peak at this mass. What was observed was most
likely a background ﬂuctuation.6 Lederman’s second solution was a 1977
experiment to look at dimuons using a far more complicated detector and
again running at Fermilab.10 This was a far more successful experiment in
which the ﬁrst observation of the Υ was made, the ﬁrst indication of a new
ﬁfth quark. Unfortunately observations of new quarks were apparently no
longer deemed worthy of a Nobel prize by this time. However the reader

≈

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

11

need not feel too bad for Leon Lederman since he was awarded the Nobel
prize anyway in 1988, sharing it with Melvin Schwartz and Jack Steinberger
for their use of neutrino beams and discovering a second type of neutrino,
the muon neutrino.11

3. Part II: More on Charm and Bottom Quarks

In Part I the discovery of charm was used to introduce some basic detectors
components. Scintillators, Photomultiplier Tubes, wire chambers, magnetic
spectrometers, ˇCerenkov counters, and electromagnetic calorimeters were
mentioned. In Part II additional experimental topics are covered, namely
the following: particle identiﬁcation systems; the use of precision position
detectors to observe detached vertices; the use of diﬀerent beam types; and
the evolution of trigger systems. The story for this part of the mini-course
is the advancement of detection of particles containing charm and bottom
quarks. The outline of this part is illustrated in the bottom section of
Fig. 1.

3.1. Open Charm Discovery

In Sec. 2 we introduced the discovery of the charmonium (c¯c) states where
the charm quantum number is hidden. The charm quark explanation of the
observed narrow states became universally accepted once states with open
charm were discovered.

≈

→

→

dW ∗)

20. The c

sW ∗)/Γ(c

Vcd
|
→

The two most commonly produced charm mesons are the D0 (c¯u) and
the D+ (c ¯d). The charm quark decays quickly to either a strange quark or a
down quark. The ratio of the rates for these two decays is given by the ratio
of the square of two CKM matrix elements: Γ(c
∼
→
sW ∗ decay is called Cabibbo favoured while
2
2/
Vcs
|
|
|
dW ∗ is Cabibbo suppressed. The virtual W can decay to either
the c
quarks or leptons. Thus most of the D0 and D+ mesons decay to states
with a strange quark. The easiest decay modes to reconstruct are the all
K −π+π+π−.
charged modes: D+
Since pions are the more copiously produced hadrons in an interaction, one
needs to distinguish kaons from pions to observe these open charm signals.
Besides ˇCerenkov counters there are other particle identiﬁcation meth-
ods for charged hadrons. One example is a Time-of-Flight (TOF) detector,
this was used in the discovery of open charm two years after the discovery
of the J/ψ. The discovery was made using the Mark I experiment at the
SPEAR e+e− collider, the same spectrometer which was used in the dis-

K −π+; and D0

K −π+π+; D0

→

→

→

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

12

covery of the J/ψ. The e+e− collider gives an inherently lower background
than hadron-hadron collisions since the electron and positron annihilate
K −π+, K −π+π+π−
completely. However the D+
were only discovered after the collection of additional data and using the
TOF system to separate kaons from pions.

K −π+π+ and D0

→

→

A TOF system works by measuring the time it takes for a charged
particle to travel between two points. For particles of the same momentum,
this time diﬀerence depends on the particle’s mass.

Figure 8.
(b) Illustration of a Gaussian resolution function and example of non-Gaussian tails.

(a) Time-of-ﬂight diﬀerences for pairs of particles plotted against momentum;

Figure 8(a) shows the diﬀerence in time-of-ﬂight over one metre for pairs
of hadrons. The performance of a TOF system is given by the distance
(L) traveled between the two time measurements and the resolution (σ∆t)
with which the time-of-ﬂight measurement is made. Long distances and
ﬁne resolution are needed. For example for Mark I L
≈
400 ps. This means that one can get 2σ∆t separation between kaons and
pions for momenta < 1 GeV/c, i.e. at very low momentum. Even if the
time measurement resolution can be considerably reduced, e.g. to
100 ps
for the Fermilab CDF Run II experiment, it can be seen from Fig. 8(a)
that with L
2 m, a 2σ∆t separation between kaons and pions is only
achieved for momenta < 2 GeV/c. Even with only a 2σ∆t separation at
low momentum, the decays D0
→
K −π+π+ could be isolated suﬃciently from background at Mark I for them

K −π+π+π− and D+

K −π+, D0

2 m and σ∆t

→

→

≈

≈

≈

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

to make the discovery.12 The mass plots are shown in Fig. 9, the D0
K −π+ distribution is shown without and with a TOF kaon selection.

13

→

Figure 9. Mass plots from Mark I: (Left) D0; (right) D+.

The gas ˇCerenkov counters mentioned in Part I can be used to separate
kaons and pions at higher momenta, but typically collider experiments like
Mark I and CDF do not have the necessary space for them. There are a
number of alternate particle identiﬁcation systems.1

3.2. Measurement Uncertainties

At this point it is worth making an aside about experimental resolution
and the meaning of a 2σ∆t separation. The importance of mass resolution
was introduced in Sec. 2.1. Not only is the size of the resolution important,
but the resolution function or shape matters also. When a quantity is
measured experimentally one does not typically obtain an exact number,
but there is some uncertainty. This uncertainty is normally separated into
two components. One component is essentially statistical in nature and
arise due to a lack of precision. The other is typically non-statistical and is
due to our limited knowledge and aﬀects the accuracy of the measurement.
The former is called the statistical uncertainty or statistical error, while the
latter is referred to as the systematic uncertainty.

A classic example of a statistical uncertainty is that due to limited
statistics. E.g. when measuring the lifetime of a particle we have a limited

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

14

number of particles to use, thus the lifetime distribution is measured with
limited precision which leads to an uncertainty in the extracted lifetime.
Another example is measuring a distance with a measuring tape. There
is some uncertainty in positioning the tape at one end and in the reading
and the precision of the scale at the other end. To reduce this uncertainty
the measurement can be repeated many times and the average value used.
We can illustrate the resolution with this simple example. If a histogram
is made of these measurements (frequency vs. [value
nominal]) ideally the
distribution is Gaussian as shown by the solid line in Fig. 8(b). The reso-
lution is the sigma of the Gaussian distribution, and it gives the statistical
uncertainty of any single measurement.

−

−

Imagine Fig. 8(b) shows the distribution of time-of-ﬂight measurements
for pions, where the nominal value is subtracted oﬀ and normalized to
the resolution ((∆t
∆t0)/σ∆t). For any given pion the ∆t measured
In particular for a
can fall anywhere within the Gaussian distribution.
small fraction of the time it could be larger than 2σ∆t from nominal, for a
Gaussian distribution this probability is about 3%. Suppose that for a kaon
∆t0) is greater than 2σ∆t. Then by requiring
the measured value of (∆t
(∆t
∆t0) > 2σ∆t we can select kaons and reject 97% of pions. This is
for an ideal Gaussian distribution. The resolution function typically has
non-Gaussian tails that go out much further as illustrated crudely by the
dashed lines in Fig. 8(b). The rejection in this case would not be as good
as 97%. Thus one needs to know the resolution function and must take
care to try to avoid large non-Gaussian tails. For a real TOF system, non-
Gaussian tails could arise from a number of sources, and the tails could
also be asymmetric. Some of these sources include the following.

−

−

•

•

•

•

The counter giving the time signal is ﬁnite in size and the mea-
surement will depend on where the particle hits the counter.
The system is made up of many counters whose relative timing and
locations are not perfect.
The calibration is not perfect, e.g. calibration tracks do not al-
ways come from exactly the same point, and the start time is not
perfectly known.
Some eﬀects like MCS may aﬀect the resolution and cause it to
vary with the particle momentum.

Further coverage of statistical uncertainties and how to determine and
handle them are beyond the scope of this mini-course, but there are many
excellent books on this subject.13

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

15

The other component of a measurement uncertainty is called the sys-
tematic uncertainty and it is typically not statistical in nature. A classic
example can again be illustrated by the case of measuring a distance with
a measuring tape. If the scale of the measuring tape is wrong we would get
a systematic error. Of course, if it were known that the scale was incorrect,
we would correct the scale and the systematic error would be eliminated.
Now let us assume that we must calibrate the measuring tape ourselves.
We can only calibrate the scale within a certain accuracy, and this leads
to a systematic uncertainty in the distance measured. For a more realis-
tic example consider measuring the lifetime of a decaying particle. For a
short lived particle like the D0, the time is not directly measured. Instead,
the distance (L) traveled between production and decay is measured and
the momentum of the particle is also measured. The proper time for the
decay is then t = LmD0/pD0. Besides the length and momentum scales,
there are other potential sources of systematic uncertainties. The lifetime
is extracted from a lifetime distribution containing many particle decays.
This distribution may not be a pure exponential but could be modiﬁed due
to detector acceptance and eﬃciency. The correction for acceptance and
eﬃciency is typically determined using a Monte Carlo simulation. There
are inherent uncertainties in the simulation that lead to an uncertainty in
the correction function and thus to a systematic uncertainty in the lifetime.
If the particle passes through matter before decaying or the daughter par-
ticles pass through matter, the lifetime distribution can also be aﬀected
by absorption of the parent or daughter particles. The cross sections for
absorption may be poorly measured or not even known. This limited knowl-
edge can also lead to a systematic uncertainty in the measurement. Finally,
another source of systematic uncertainty could be backgrounds that mimic
the signal but which are not properly accounted for. Typically, systematic
uncertainties are not well deﬁned and are not straightforward to determine.
They are also usually not Gaussian distributed, and combining systematic
uncertainties from diﬀerent sources is problematic. Since even the meaning
and deﬁnition of systematic uncertainties are diﬃcult to quantify, ideally
one should design an experiment to have a small systematic uncertainty
(compared to the statistical uncertainty), so as not to have to worry about
the details of the treatment and combining of systematic uncertainties.
Further coverage of systematic uncertainties is beyond the scope of this
mini-course. The understanding of systematics is beginning to be better
understood and in some rare cases are even correctly taught.14 However
considerable disagreements are still common.

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

16

3.3. Improving S/B for Open Charm

Although the use of particle identiﬁcation can be powerful in isolating a
signal, it can be seen from Fig. 9 that there is considerable room for im-
provement. This is especially true in hadronic interactions which typically
have higher backgrounds than in e+e− annihilations.

The lifetimes of the open charm particles are in the range 0.1–1 ps, which
is small but ﬁnite and can be used to isolate a signal. Almost all the u- and
d-quark backgrounds have essentially zero lifetime while the backgrounds
from some strange particles decay after a long distance. Thus the signature
of a charm particle is given by its decay a short distance away from the
production point. For example, a 30 GeV D0 travels an average length of
about 2 mm, which is quite small but increases linearly with momentum.
To get a better sense of the scale involved, consider the decay of a
a charm particle produced in a ﬁxed-target experiment as illustrated in
Fig. 10(a). The charm particle is produced and then decays after traveling
a distance LD. To separate the production and decay vertices we need to
measure LD with a resolution of σLD << LD. Since position detectors typ-
ically measure in the dimension transverse to the beam direction, it is more
convenient to transform this essentially longitudinal resolution requirement
into an transverse one. The typical angle that the charm particle is pro-
duced relative to the beam direction is θ
mD/pD, where mD and pD are
the mass and momentum of the charm particle respectively. The mean dis-
tance traveled by the charm particle is LD = βγcτD = cτDpD/mD where
τD is the lifetime of the charm particle. Thus to resolve the production and
decay vertices we need σtrans << θLD, or σtrans << cτD, where σtrans is
the transverse position resolution of the detector (charged) tracking system.
The values of cτD for the D0, D+ and Λ+
c are 123 µm, 312 µm and 60 µm
respectively.

≈

The resolution of the MWPC’s depend on the wire spacing (s), and for
a single detector plane is given by σtrans = s/√12. The minimum wire
spacings are in the range 1–2 mm depending on their cross sectional cover-
age. For s = 2 mm, σtrans = 577 µm, too large to resolve the production
and decay vertices. The spatial resolution can be improved by measuring
the time between a charged particle passing through the detector plane and
when a signal is received in the wire closest to the point of passage. This
is done in Drift Chambers and resolutions as low as σtrans
100 µm have
been obtained in such a tracking system. This is still too large, especially
considering that one typically needs better than 5–10σ vertex separation.

≈

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

17

Figure 10.
K −π+ mass plots from E691 showing the power of a detached vertex requirement.

(a),(b) Illustration of production and decay of a charm particle. (c) Invariant

A tracking system with much better spatial resolution is needed. His-
torically, two detector technologies have been used that can give better
resolutions: photographic emulsions and Bubble Chambers.

Detection using layers of photographic emulsions has been used for a
long time and spatial resolutions of better than 10 µm have been obtained.
Although these have been used relatively recently in DONUT to make the
ﬁrst direct observation of the ντ

15 they are not suitable for high rates.

Bubble Chambers have also been used historically to make important
observations. Typically, the resolution of bubble chambers is not better
than that for Drift Chambers. However, a suﬃciently small bubble chamber,
like the LEBC in the LEBC-EHS experiment, has achieved resolutions of
10 µm, but again such bubble chambers are not suitable for high rates.
∼
The LEBC-EHS experiment reconstructed about 300-500 charm decays.16
What launched the high statistics studies of charm quark physics was
the development and use of the Silicon Microstrip Detector (SMD). The
Fermilab E691 photoproduction experiment included one of the ﬁrst SMD’s
and collected a 10,000 sample of fully reconstructed charm decays, about
two orders of magnitude more than other experiments of that time. Resolu-
10 µm can be obtained and some data from E691
tions as good as σtrans
are shown in Fig. 10(c).17 SMD’s have now been used in many experiments
including those studying bottom and top quarks.

∼

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

18

3.4. Going for Higher Statistics

The road to higher statistics in charm studies is illustrated in Fig. 11, giving
some selected milestones along the route.

Some Charm Experiments

E831/FOCUS

E791

CLEO
e e+ -

p/S

E691

E687

E781/SELEX

8
10

6
10

4
10

2
10

 

s
y
a
c
e
d
m
r
a
h
C
d
e

 

t
c
u
r
t
s
n
o
c
e
r
 
y

l
l

u
F

E87A

MARK
3

e e+ -
n
E400

p/ K/p
E769

E516

1980

1990

Year

2000

Figure 11. Number of fully reconstructed charm decays for diﬀerent experiments as a
function of time.

The charm quark experiments include e+e− and p¯p colliders as well as
ﬁxed-target experiments using photon and hadron beams. While larger
charm data sets could be obtained in e+e− experiments by increasing the
luminosity (e.g. CLEO), the ﬁxed-target experiments needed additional
technological advances – the 8 mm tape for data storage and high power
commodity computing for data processing. For example, using these tech-
nologies and by building a more intense photon beam, the Fermilab FOCUS
photoproduction experiment obtained a sample of 1 million fully recon-
structed charm decays with published physics results one year after the
end of data taking. Using 8 mm tapes with 30 times the capacity of 9-
track tapes, the Fermilab E791 hadroproduction experiment could write
out much more data and collected more charm than previous hadroproduc-
tion experiments. To do this, E791 used a wall of 42 8 mm tape drives in
parallel to record data fast enough. To obtain substantially more statistics,
a revolution in triggering is needed.

3.5. The Trigger System and the Bottom Quark

Typically the particle interactions occur at a high rate and the S/B can be
as low as 10−3–10−8. A trigger system is used to quickly decide whether an

g
g
g
p
g
g
February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

19

interaction contains signal and thus “trigger” the recording of the related
data.

For a charm photoproduction experiment like FOCUS, the photon beam
largely produces e+e− conversion pairs and only about 1 in every 500 pho-
ton interactions would produce hadrons. Only 1 in every 150 of those in-
teractions producing hadrons contains a charm quark. It is relatively easy
to recognize a photon conversion from an interaction producing hadrons.
Since the fraction of hadron producing interactions containing charm is
not too small, one just writes out all hadron producing interactions. For
hadroproduction experiments on-the-other-hand, the fraction of charm is
smaller by another factor of ten, thus either a lot more data must be written
out and analyzed or a better, more intelligent trigger must be used.

Historically the trigger is a system of fast electronics that quickly pro-
cesses special trigger signals produced by the detector and gives an elec-
tronic acceptance decision that is used to “trigger” the readout to save the
data for that interaction. Long cables are used to delay the signals from the
rest of the detector so they do not arrive at the readout before the trigger
decision is made. In addition, the experiment is “dead” and unavailable to
collect more data until the data readout is completed – this dead-time can
be a signiﬁcant fraction of the live-time. In the ﬁrst stage of the FOCUS
trigger, the decision must be made within about 370 ns from the time the
interaction occurs.

Developments have made modern trigger systems much easier. Elec-
tronics are now faster, smaller and cheaper. Also, high speed data links
and computing resources are more powerful. A large amount of memory
is now aﬀordable so that data from the detectors can be stored digitally
while the trigger processing takes place. This eliminates the need for long
signal cables which can degrade analog signals, and it also gives more time
for trigger processing and virtually eliminates dead-time.

The ultimate trigger is if all the data could be recorded and analyzed
before deciding which data to store. One can illustrate what is needed
for such a trigger by using as an example the CDF or D0 experiments
at Fermilab.
In these experiments, protons and antiprotons cross every
106 crossing/s. If it took one second to fully
396 ns, so there are about 2.5
analyze the data from one crossing in a single CPU, we would need 2.5
million CPU’s to not lose data from any crossings. We would also need to
temporarily store at least 2.5 million crossings worth of data. If one needs
300 KB/crossing, then 1 TB (103 GB) is needed. Since the processing time
would have a long tail beyond one second, to be safe one would want about

×

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

20

1000 TB of RAM as well as the 2.5 million CPU’s! Clearly a trigger that
only partially processes the data is needed.

Even if the ultimate trigger cannot yet be realized, the developments
mentioned above have provided the needed ingredients to separate out
charm decays in hadronic collisions by looking for evidence of a detached
decay vertex at the trigger level. Since recognizing bottom quark decays
is similar to that for charm decays, this revolution in triggering has made
possible an experiment that will reconstruct very large samples of charm
and bottom decays.

Already large samples of charm and bottom quark decays are being
collected using the BaBar and Belle e+e− experiments. To do better one
must use hadronic collisions with suﬃcient energy like p¯p annihilations at
the Fermilab Tevatron. The cross section for producing bottom quarks is
much larger than in e+e− annihilations, e.g. about 100 µb compared to
about 1.1 nb at the Υ(4S). The CDF and D0 p¯p experiments can collect
sizable samples of charm and bottom decays, but to get 1000 times more
rate than BaBar or Belle requires a specialized detector and data acquisition
and trigger systems.

The BTeV experiment18 is designed to study bottom and charm decays
at the Tevatron. To maximize the yield for clean ﬂavour-tagged B mesons
for CP violation studies, the detector is placed in the forward direction
allowing a Ring Imaging ˇCerenkov Counter (RICH) for excellent particle
identiﬁcation over a wide momentum range. A PbWO4 crystal calorimeter
provides eﬃcient detection of photons and π0’s with excellent energy reso-
lution. The BTeV experiment includes a silicon pixel detector that makes
possible the recognition of detached vertices at the lowest trigger level.

Although a SMD can provide excellent spatial resolution, a lot of data
processing is typically needed for interactions with many tracks due to the
strip geometry. This is illustrated in Fig. 12(a). A single particle passing
through a plane of strips will give a signal in one strip as illustrated in (i).
The location along the hit strip can be determined by a second plane of
strips oriented at 90◦ to the ﬁrst plane as in (ii). However if, as illustrated in
(iii), two particles pass through the two planes we would get four hit strips
and one cannot tell if the two particles passed through points (A1, A2) or
through (B1, B2). This ambiguity may be resolved by a third plane of strips
at an angle as given in (iv). For a complex event with many particles the
pattern recognition becomes quite complex and requires signiﬁcant CPU
power.

Ideally a trigger algorithm should be close to that used in the data

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

21

Figure 12.
on the lifetime distribution of a detached vertex trigger.

(a) Illustration of pattern recognition in a SMD. (b) Illustration of the eﬀect

analysis but with looser selection criteria. This is because a poorly cho-
sen trigger algorithm can give rise to sizable systematic uncertainties. A
simple example is illustrated in Fig. 12(b). The lifetime distribution of
the B0 meson is shown which is a pure exponential with the B0 lifetime.
Since backgrounds are typically at low lifetimes, an ideal trigger for collect-
ing data to measure the lifetime would select decays with a large enough
lifetime as illustrated in the middle distribution of Fig. 12(b).
In a real
trigger there is typically only time to do partial processing, and thus one
might require only the presence of one or two detached tracks, instead of
reconstructing the production and B0 decay vertices. This could lead to a
lifetime distribution illustrated by the right-most distribution of Fig. 12(b).
Thus a correction function is needed to extract the correct B0 lifetime.

×

The charged track pattern recognition is simpliﬁed in BTeV by the use of
50 µm2 silicon pixels which can locate the position of a passing particle
400
in 3-dimensions by a single hit pixel. Low momentum tracks undergo more
MCS and can give rise to false detached tracks. In BTeV, the pixel detector
is located in a dipole magnet so that tracks with low momentum can be
rejected and not used at the trigger level.

Even with silicon pixels a full reconstruction cannot be done. Custom
electronics using 500 FPGA’s are used to help in processing the 500 GB/s
data rate coming from the detectors. Further data processing and the pat-
tern recognition is done on 500 commercial IBM-G5-equivalent processors.
Two further levels of the trigger running on 1500 commodity CPU’s reduce

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

22

Glossary

the data going to storage to a more manageable 200 MB/s.

The BTeV experiment nicely illustrates the convergence of a number of
technological advances. Years of scientiﬁc progress have enabled such an
experiment to be realized.

Bubble chamber: A historic detector consisting of a cryogenic liquid main-
tained at a pressure above the equilibrium vapour pressure. The bubble
chamber can be expanded to suddenly decrease the pressure so that charged
particles passing through the liquid in a “superheated” condition will create
a track of bubbles. Photographs are taken of the bubbles in multiple views
to reconstruct the particle trajectories.

Calorimeter: A device to measure the energy of particles. The two
distinct types are electromagnetic calorimeters and hadronic calorimeters.
They work by completely absorbing the shower produced by a particle and
producing a signal proportional to its energy. Calorimeters must be cali-
brated to give the absolute particle energy.

ˇCerenkov counter: A detector based on the ˇCerenkov eﬀect (for which
ˇCerenkov shared the 1958 Nobel prize). Particles traveling faster than light
in a given medium emits a cone of ( ˇCerenkov) light. A threshold ˇCerenkov
counter contains a gas, for example, with a well chosen refractive index so
that for a given particle momentum one particle type (e.g. pions) will emit
light while another (e.g. kaons) will not. The angle of the cone of light also
depends on the particle velocity which is used in other forms of ˇCerenkov
counters like the RICH. The amount of ˇCerenkov light emitted is typically
low, about 100 times less intense than scintillation light in a scintillator.

Drift chamber: A wire chamber where one measures the time between
when a charged particle passes through and when a signal in the nearest
signal wire is received. Typically many wires are used to form drift cells
where the electric ﬁeld is tailored to obtain a fairly uniform drift velocity
across the cell. The spatial resolution is better than a MWPC but a drift
chamber is more complex and typically cannot handle as high a rate of
particles.

Electromagnetic calorimeter: A calorimeter for measuring the ener-
gies of photons and e± through their electromagnetic interactions. These
calorimeters can be made from dense crystals like PbWO4 or lead-glass, or

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

23

can be sandwiches made of multiple layers of dense absorber and detection
material.

Emulsions: Usually a layer of photographic emulsion several hundred µm
thick in which a traversing charged particle causes the nearest silver halide
grains to develop. Each grain is typically 0.2 µm in diameter with about
270 developed grains/mm. The emulsion must be scanned to reconstruct
the particle trajectories.

Lead-glass shower counters: A dense glass used to detect photons
and e± and for electromagnetic calorimeters. The detection is based on
ˇCerenkov light.

Magnetic Spectrometer: A detector system used to determine the mo-
mentum of charged particles by measuring the defection of the particles
in a known magnetic ﬁeld. Various magnetic ﬁeld conﬁgurations can be
used e.g. dipole, solenoid, or toroid. Deﬂection of particles are measured
using position detectors, usually wire chambers, but can be e.g. scintillator
hodoscopes, scintillating ﬁbres or a SMD.

Multiwire proportional chamber (MWPC): A wire chamber where
the location of a passing charged particle is determined by the location of
the wire closest to it.

Photomultiplier Tube (PMT): A device to detect a small quantity of
light using the photoelectric eﬀect (for which Einstein received the 1921
Nobel prize). The maximum sensitivity of the photocathode in a typical
PMT is for blue light.

Ring imaging ˇCerenkov counter (RICH): A ˇCerenkov counter where
the angle of the emitted ˇCerenkov light is measured to enable the identiﬁ-
cation of particles over a wide momentum range.

Scintillator: A material that produces light through ﬂuorescence when
a charged particle passes through it. Scintillators used include inorganic
crystals like PbWO4, organic liquids and plastic. A classic plastic scintil-
lator is made of polystyrene that produces light in the UV. The UV light
is shifted to blue with a tiny doping of primary and secondary ﬂuors to
better match the photosensitivity of a PMT. Most of the light comes in
a fast component (few ns) and strong signals are possible with suﬃcient
scintillator thickness.

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

24

Scintillator ﬁbres: Scintillator in the form of long ﬂexible ﬁbres with an
outer acrylic sleeve so that the scintillation light is isolated to the ﬁbre, but
still totally internally reﬂected along the ﬁbre to the ends. Typically a few
mm in diameter they are used for position detectors or in calorimeters as
either the detection material or as a mechanism for readout.

Scintillator hodoscope: A single detector plane made of strips of scin-
tillator. Used to detect the position of a charge particle. Two planes can
be overlapped with the strips in one plane oriented at 90◦ to the other to
locate the particle in both transverse dimensions.

Silicon microstrip detector (SMD): Detection is based on essentially
a silicon semiconductor p-n junction where the depletion region is enlarged
by a bias voltage. The depletion layer can be considered as a solid state
ionization chamber. A charged particle passing through the depletion re-
gion liberates electron-hole pairs which create signals on very thin, closely
spaced readout strips. SMD’s have the detection regions arranged as long
uniformly separated strips. The strip separation can be in the range 10–
300 µm.

Silicon pixels: Similar to the SMD but the active region is in the form of
rectangles so that a “hit” pixel locates a passing particle in both transverse
dimensions. The readout is however more complicated.

Spark wire chamber: A parallel-plate gas chamber in which a high volt-
age pulse is applied immediately after the passage of a passing charged
particle. Sparks form along the trail of ions caused by the charged parti-
cle passing through the gas. This can provide a visualization of the track
useful for public demonstration. High speed readout is typically done mag-
netostrictively or capacitively.

Time-of-ﬂight detector: A system for identifying charged particles based
on measuring their velocity between two points. The time-of-ﬂight between
two points is usually measured using scintillator counters possibly in con-
junction with a measurement of the time of an interaction. The particle
momentum is also measured giving a velocity that can distinguish particle
types through their diﬀering masses. See Sec. 3.1.

Wire chamber: For detection of charged particles through their ionization
of usually noble gas atoms. A high voltage causes ionized electrons to
accelerate and create an avalanche of electrons and positive ions. Detection
of the avalanches in a plane of wires can give the position of the passing

February 21, 2014

18:34 Proceedings Trim Size: 9in x 6in

cheung˙silavae

25

particle. Many types of wire chambers have been used, the original type
(MWPC) was invented by Charpak for which he received the 1992 Nobel
prize.

Acknowledgments

My thanks to Jeﬀ Appel for some helpful suggestions for these lectures
and for a careful reading of this writeup. This work was supported by
the Universities Research Association Inc. under Contract No. DE-AC02-
76CH03000 with the U. S. Department of Energy.

References

1. Some examples of textbooks are: R. Fernow, Introduction to experimental
particle physics, CUP 1986; K. Kleinknecht, Detectors for particle radiation,
2nd Ed., CUP 1998. There are also some excellent articles, e.g. in F. Sauli
(Ed.), Instrumentation in High Energy Physics, World Scientiﬁc, 1992.
2. R. N. Cahn and G. Goldhaber, The experimental foundations of particle

physics, CUP 1989.

3. S. C. C. Ting, Nobel Lecture, 11 Dec. 1976, the full text is available at
http://nobelprize.org/physics/laureates/1976/ting-lecture.pdf.
4. B. Richter, Nobel Lecture, 11 Dec. 1976, the full text is available at
http://nobelprize.org/physics/laureates/1976/richter-lecture.pdf.

5. J. H.Christenson et al., Phys. Rev. Lett. 21, 1523 (1970).
6. L. M Lederman, Nobel Lecture, 8 Dec. 1988, the full text is available at
http://nobelprize.org/physics/laureates/1988/lederman-lecture.pdf.
7. J. J. Aubert et al., Phys. Rev. Lett. 33, 1404 (1974); Nucl. Phys. B89, 1

(1975).

8. M. Ambrogiani et al., Phys. Rev. D64, 052003 (2001).
9. D. C. Horn et al., Phys. Rev. Lett. 36, 1236 (1976).
10. S. W. Herb et al., Phys. Rev. Lett. 39, 252 (1977).
11. G. Danby et al., Phys. Rev. Lett. 9, 36 (1962). See also the Nobel lectures at

http://nobelprize.org/physics/laureates/1988/

12. G. Goldhaber et al., Phys. Rev. Lett. 37, 255 (1976); I. Peruzzi et al., Phys.

Rev. Lett. 37, 569 (1976).

13. Some examples of statistics textbooks are: P. R. Bevington and D. K. Robin-
son, “Data Reduction and Error Analysis for the Physical Sciences”, 2nd
Ed., McGraw-Hill 1992; G. Cowan, “Statistical Data Analysis”, OUP 1998;
D. S. Sivia, “Data Analysis: A Bayesian Tutorial”, OUP 1996.

14. R. Barlow, “Systematic Errors: Facts and Fictions”, hep-ex/0207026.
15. K. Kodama et al., Phys. Lett. B504, 218 (2001).
16. M. Aguilar-Benitez et al., Z. Phys. C40, 321 (1988).
17. J. C. Anjos et al., Phys. Rev. Lett. 58, 311 (1987); K. Sliwa et al., Phys. Rev.

D 32, 1053 (1985); J. R. Rabb et al. Phys. Rev. D 37, 2391 (1988).

18. http://www-btev.fnal.gov/


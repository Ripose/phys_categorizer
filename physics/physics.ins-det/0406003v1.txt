Pattern Recognition and Data Compression
for the ALICE High Level Trigger

4
0
0
2
 
n
u
J
 
1
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
3
0
0
6
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Institutt for fysikk og teknologi

U N I  V   E  R S I T

 

A

 

S

B

 

E R G E   N   S I S

by
Anders Strand Vestbø

May 2004

Universitetet i Bergen

Bergen, Norway

Institutt for fysikk og teknologi

U N I  V   E  R S I T

 

A

 

S

B

 

E R G E   N   S I S

Pattern Recognition and Data Compression
for the ALICE High Level Trigger

by
Anders Strand Vestbø

a thesis submitted to Institutt for fysikk og teknologi,
Universitetet i Bergen,
in partial fulﬁlment of the requirements for
the degree of Doctor Scientiarum

May 2004
Universitetet i Bergen
Bergen, Norway

Acknowledgements

There are several people who deserve acknowledgement for their contribution one way or
another to the work compiled in this thesis.

Most of all, my sincerest thanks goes to my supervisor, Prof. Dieter R¨ohrich, for
excellent guidance and support. His relaxed attitude and detailed insight in a wide range
of topics has provided me with an ideal working environment. Furthermore, I would
like to thank Constantin Albrecht Loizides for all the academic and social interactions
during the last two years. I appreciate all the nice discussions – both fundamental and
shallow, enlightening questions and answers, great parties and valuable comments to the
thesis. I would also like to thank Dr. Ulrich Frankenfeld for a great time shared working
together during his Post. Doc. period in Bergen, and later in various pubs around the
world discussing the crew on German warships etc.

I am grateful to all the people in the Experimental Nuclear Physics Group in Bergen for
maintaining a good environment for both research and friendship. In particular, I would
like to mention Jens Ivar Jørdre, Zhongbao Yin, Jørgen Lien, Are Severin Martinsen,
Gaute Øvrebekk, Kenneth Aamodt and former students Bjørn Tore Knudsen and Espen
Vorland.

Thanks also to Timm Morten Steinbeck and Arne Wiebalck for being excellent hosts
during my visits to Heidelberg, and for making the HLT data-challenge in Paderborn such
an interesting experience. I am also grateful to the STAR L3 group under direction of
Dr. Jens S¨oren Lange, for giving me a boost into the world of High Level Triggers during
my stay at BNL, spring 2000.

I also wish to thank Prof. Bernhard Skaali, the project leader of the Norwegian ALICE
Group, for hiring me as a Dr. Scient. student at the University of Oslo, and for giving me
the opportunity to attend a number of various international conferences and workshops.
Finally, I am deeply in debt to Renate, for encouragement, improving my thesis and

for having absolute conﬁdence in me.

Bergen, March 2004
Anders Strand Vestbø

“In principle it’s easy.”

Contents

Introduction

1

3
3
4
6
8

1 Ultrarelativistic Heavy Ion Collisions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 Quarks and gluons
1.2 Hot and dense nuclear matter . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 The dynamics of heavy ion collisions
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
1.4 The experimental observables

2 The ALICE Experiment at LHC

15
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2 LHC running strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3 Detector layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.4 The TPC detector
2.4.1 Principle of operation . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.4.2 Detector layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4.3 Readout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5 Data volumes and data-acquisition . . . . . . . . . . . . . . . . . . . . . . 24
2.5.1 Data rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.5.2 The trigger system . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.5.3 The DAQ system . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.4 The High Level Trigger . . . . . . . . . . . . . . . . . . . . . . . . . 27

3 The ALICE High Level Trigger System

29
3.1 The necessity of a High Level Trigger . . . . . . . . . . . . . . . . . . . . . 29
3.2 Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.2.1 Trigger mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
. . . . . . . . . . . . . . . . . . . . . . . . 33
3.2.2 Data compression mode
3.3 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

4 Fast Pattern Recognition in the ALICE TPC

39
4.1 Track reconstruction methods . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2 The ALICE tracking environment . . . . . . . . . . . . . . . . . . . . . . . 43
4.2.1 Particle multiplicity and detector occupancy . . . . . . . . . . . . . 43
4.2.2 Magnetic ﬁeld settings . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.2.3 Particle trajectory in a magnetic ﬁeld . . . . . . . . . . . . . . . . . 45
4.3 The AliROOT framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

i

4.3.1 Event simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.3.2
Simulation of detector response . . . . . . . . . . . . . . . . . . . . 47
4.3.3 The Oﬄine reconstruction chain . . . . . . . . . . . . . . . . . . . . 49
4.4 Premises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.5 Sequential tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.5.1 The Cluster Finder . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.5.2 The Track Finder . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.5.3 The Track Fitter
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.5.4 The Track Merger . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.5.5 Data ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.5.6 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Iterative Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.6.1 The Cluster Fitter
. . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.6.2 The Hough Transform . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.6.3 Data ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.6.4 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

4.6

5 TPC Data Compression

93
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.1
. . . . . . . . . . . . . . . . . . . . . . 94
5.2 TPC signal generation and models
5.3 TPC data format and coding
. . . . . . . . . . . . . . . . . . . . . . . . . 95
5.4 Local modeling techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.4.1 Lossless TPC data compression . . . . . . . . . . . . . . . . . . . . 97
5.4.2 Lossy TPC data compression . . . . . . . . . . . . . . . . . . . . . 98
5.4.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
. . . . . . . . . . . . . . . . . . . . . . . . . . 99
Storing cluster data . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.5.1
5.5.2 Track and cluster modeling
. . . . . . . . . . . . . . . . . . . . . . 102
5.5.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

5.5 Global modeling techniques

6 Conclusions and Outlook

117
6.1 Online TPC pattern recognition . . . . . . . . . . . . . . . . . . . . . . . . 117
6.2 Online data compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.3 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

A Track parameterizations

123
A.1 The equations of motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
A.2 Helix parameterizations

B Software and data formats

129
B.1 Analysis software structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
B.2 Compressed data formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

References

135

ii

List of Figures

1.1 Energy density as a function of temperature calculated within lattice QCD.
1.2 Lattice calculations at ﬁnite baryon chemical potential.
. . . . . . . . . . .
1.3 QCD phase diagram.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 The Bjorken space-time scenario for a heavy ion collision. . . . . . . . . . .
1.5 Data and predictions for charged particle multiplicity per unit pseudo-

5
6
7
8

rapidity.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
. . . . . . 11

1.6 Elliptic ﬂow as a function of centrality and transverse momenta.
1.7 Nuclear modiﬁcation factor measured for minimum biased collisions of d–

Au at √sN N =200 GeV compared to central Au–Au collisions.

. . . . . . . 12

1.8 Two-particle azimuthal distributions for high transverse momentum

hadrons measured at RHIC.

. . . . . . . . . . . . . . . . . . . . . . . . . . 13

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.1 The ALICE detectors.
. . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.2 TPC principle of operation.
. . . . . . . . 21
2.3 Deﬁnition of the track inclination angle in the ALICE TPC.
2.4 ALICE TPC schematic layout. . . . . . . . . . . . . . . . . . . . . . . . . . 22

3.1 Distribution of charm meson decay products into pions and kaons. . . . . . 31
. . . . . . . . . . . . . . . . . . 35
3.2 Data ﬂow architecture of the HLT system.

4.1 Simulated occupancy in the ALICE TPC as a function of pad-row number. 44
4.2 Overview of the AliROOT framework.
. . . . . . . . . . . . . . . . . . . . 46
4.3 Flow diagram of the HLT Cluster Finder algorithm. . . . . . . . . . . . . . 51
Illustration of conformal mapping of space points along circular track seg-
4.4
ments.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.5 Flow diagram of the HLT Track Follower algorithm. . . . . . . . . . . . . . 55
4.6 Possible data ﬂow for the sequential track reconstruction chain within the

HLT system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

4.7 Tracking eﬃciencies as a function of pt for the HLT sequential track recon-

struction chain.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

4.8 Contamination as a function pt for the HLT sequential track reconstruction

chain.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Integrated eﬃciency and contamination as a function of multiplicity. . . . . 62

4.9
4.10 Residual distributions for the HLT sequential track reconstruction chain

for dNch/dη = 1000. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

4.11 Residuals as a function of multiplicity for the HLT sequential track recon-

struction chain.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

iii

4.12 Relative transverse momentum resolution for the HLT sequential track

reconstruction chain.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64

4.13 Relative transverse momentum resolution for the HLT track reconstruction

chain as a function of pt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

4.14 Tracking eﬃciency of secondary tracks for the HLT sequential track recon-

struction chain.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
. . 68
4.15 Measured CPU-time for the HLT sequential track reconstruction chain.
4.16 Example of ﬁtting and deconvolution of overlapping clusters. . . . . . . . . 70
. . . . . . . . . . . . . 71
4.17 Flow diagram of the HLT Cluster Fitter algorithm.
. . . . . . . . . . . . . . . 73
4.18 Illustration of the HT applied to a straight line.
. . . . . . . . . . . . . . . . . . . 77
4.19 Deﬁnition of the image space in the HT.
. . . . . . . . . . . . . . . . . 78
4.20 Peak formation in the HT parameter space.
4.21 Example of the HT parameter space.
. . . . . . . . . . . . . . . . . . . . . 82
. . . . 83
4.22 Illustration of the peak ﬁnding method in the HT parameter space.
4.23 Average spread in pseudo-rapidity,∆η, as a function of pt. . . . . . . . . . . 85
4.24 Parameterization of the HT image space. . . . . . . . . . . . . . . . . . . . 86
4.25 Possible data ﬂow for the iterative reconstruction chain within the HLT

system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

4.26 Tracking eﬃciencies as a function of pt for the HLT iterative reconstruction

chain.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

4.27 Tracking eﬃciencies as a function of pt for the HLT iterative track recon-

4.28 Transverse momentum resolution for the HLT iterative track reconstruction

struction chain for pt ≥
chain for dNch/dη = 1000 and 4000. . . . . . . . . . . . . . . . . . . . . . . 90

0.5 GeV. . . . . . . . . . . . . . . . . . . . . . . . . 89

5.1 Plot of the 10-to-8 bit conversion table used. . . . . . . . . . . . . . . . . . 97
. . . . . . . . 98
5.2 Distribution of ADC-values in simulated ALICE TPC-data.
5.3 Space point resolution for diﬀerent encoding sizes of the clusters. . . . . . . 101
5.4 Ratio between cluster data and raw-data as a function of multiplicity.
. . . 103
5.5 Deﬁnition of a residual. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.6 Data compression encoding scheme. . . . . . . . . . . . . . . . . . . . . . . 105
5.7 Flow diagram of the implemented data compress/expand cycle.
. . . . . . 107
5.8
. . . . . . . . . . . . . . . . . . . . . 108
. . . . . . . . . . . . . . . . . . . . . . 108
5.9 Distribution of quantized residuals.
5.10 Eﬃciency loss due to removing the cluster shape information from the data

Impact on the space point resolution.

stream. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

5.11 Impact on tracking eﬃciency from disregarding the remaining clusters in

the compression scheme.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5.12 Impact on the relative momentum resolution.
. . . . . . . . . . . . . . . . 112
5.13 Transverse residual distribution before and after data compression. . . . . . 113
. . . . . . . . . . . . 113
5.14 Impact on the number of assigned clusters per track.
. . . . . . . . . . . . . . . . . . . . . . 114
5.15 Impact on the dip-angle resolution.
. . . . . 115
5.16 Achieved compression ratios and the corresponding eﬃciency loss.

6.1

Integrated tracking eﬃciency and predicted multiplicities. . . . . . . . . . . 118

iv

A.1 Schematic view of the helix parameters. . . . . . . . . . . . . . . . . . . . . 125
A.2 Deﬁnition of the track parameters at the point of DCAO. . . . . . . . . . . 127

B.1 Schematic overview of the data payload communication in the HLT recon-

struction chain.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

B.2 Schematic overview of the interface between HLT analysis code and Ali-

ROOT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

v

vi

List of Tables

. . . . . . . . . . 23
2.1 ALICE TPC design parameters of the readout chambers.
2.2 ALICE TPC design parameters of the gas volume. . . . . . . . . . . . . . . 23
. . . . . 26
2.3 Expected ALICE event and data rates for the diﬀerent LHC runs.

3.1 Expected trigger rates of the di-muon detector.
3.2 Number of HLT detector links per detector and their data payload.

. . . . . . . . . . . . . . . 33
. . . . 35

4.1 Charged particle multiplicity simulated by diﬀerent event generators.
. . . 43
4.2 Space point resolution obtained using the HLT Cluster Finder. . . . . . . . 52
4.3 Comparison of the space point resolution obtained by the HLT Cluster

Fitter and the HLT Cluster Finder on both isolated and overlapping clusters. 72

4.4 Ratio between track candidates found by the HT and tracks reconstructed

by the Cluster Fitter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.5 Measured CPU-time for the diﬀerent processing steps in the HT. . . . . . . 91

5.1 Compression performance for local data modeling techniques on simulated

ALICE TPC data.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.2 Space point data and their required encoding size. . . . . . . . . . . . . . . 102
5.3 Cluster and raw-data parameters and their respective encoding size. . . . . 102
5.4 Cluster and track parameters and their respective size used in the data

compression scheme.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.5 Data compression ratios for the diﬀerent event samples. . . . . . . . . . . . 115
5.6 Properties of the compressed data samples. . . . . . . . . . . . . . . . . . . 116

6.1 Computational demands on the HLT system. . . . . . . . . . . . . . . . . . 120
6.2 Estimated TPC data rate reduction based on obtained TPC data compres-

sion ratios. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

vii

viii

Introduction

The primary objective of high energy physics is to study the fundamental forces and
symmetries which exist in nature and their macroscopic manifestations. Over the last
decades, a detailed theory of elementary particles and their fundamental interactions has
been established in the Standard Model. Still, very little is known about the properties
of nuclear or hadronic matter, i.e. matter that is composed of quarks and bound by the
strong force – one of the fundamental forces in nature. Under normal conditions the
quarks are conﬁned in protons and neutrons, interacting via the nuclear force. At low
energy densities these hadronic bound states are the degrees of freedom of nuclear matter.
At higher energy densities the degrees of freedom are quarks and gluons interacting via
the strong force.

The focus of heavy ion physics is to study and understand the properties of the
diﬀerent phases of nuclear matter. At very high densities and temperatures the nucleons
are expected to dissolve into their constituents and form a plasma consisting of quarks
and gluons, the so-called quark-gluon plasma. According to Big Bang cosmology such a
phase transition from the quark-gluon plasma into hadronic matter took place during the
ﬁrst microsecond after the Big Bang. By colliding heavy ions at very high energies similar
conditions can be generated in the laboratory. This creates instantaneously a partonic
phase which quickly equilibrates into a quark-gluon plasma.

The study of such a phase transition, and the physics of the quark-gluon plasma
state, requires numerous systematic measurements of nuclear collisions with varying initial
conditions. The main challenge of heavy-ion physics is to record and analyze the large
number of particles which emerge from these collisions. The ALICE experiment at the
upcoming Large Hadron Collider (LHC) at CERN will be dedicated to the study of
heavy ion collisions at energies which go far beyond the critical energy density for a phase
transition. At these energies, up to 20 000 particles will be detected in every central
collision, generating a wealth of information which has to be recorded for subsequent
analysis. In order to accumulate enough statistics for a coherent measurement of the wide
range of predicted observables, the experiment has to collect as many events as possible
within the given runtime. The allowed event rate, however, will produce about one order
of magnitude more data than the foreseen data rate to mass storage. This inconsistency
between the available data rate and the limited mass storage bandwidth can be overcome
by introducing a layer in the readout-system which is able to eﬃciently reduce the data
rate by online event selection and data compression. Such a High Level Trigger system
will have to perform real-time analysis of the detector information, requiring fast pattern
recognition in order to reconstruct the particle tracks.

The ALICE High Level Trigger system is designed to accomplish this task. The system

1

entails a large scale generic processing farm of the order of several hundred separate
nodes. The overall architecture of the system follows a hierarchical structure, driven by
the intrinsic parallelism of the data ﬂow from the detectors and the demand for a full
event reconstruction. The system components will be based on commercially available
PCs connected with a high bandwidth, low latency network. A number of nodes will be
equipped with FPGA co-processors for designated pre-processing tasks.

The main processing task of the system is fast parallel detector speciﬁc pattern recog-
nition. Given the large uncertainty of the anticipated particle multiplicity, diﬀerent ap-
proaches to the pattern recognition problem need to be considered. Once the particle
tracks have been reconstructed event selections can be performed on the basis of vari-
ous physics analysis algorithms. Such applications may include event rate reduction by
complete event selection/rejection, or event size reduction by region-of-interest readout
or data compression.

2

Chapter 1

Ultrarelativistic Heavy Ion Collisions

1.1 Quarks and gluons

One of the fundamental assumptions in modern elementary particle physics is the quark
model deﬁned by Gell-Mann and Zweig [1, 2].
It states that all hadrons consist of a
multiple of quarks in a bound state. Most common are the baryons and mesons, with
three quarks (qqq) and a quark and a anti-quark (q ¯q) respectively. In addition, recent
experimental evidence for the so-called pentaquark state (qqqq ¯q) has been reported [3, 4,
5, 6, 7]. It is possible to reconstruct and explain all the properties of the hadrons (charge,
mass, magnetic moment, isospin etc.) from the quantum numbers of the quarks. For
instance, to build a single nucleon one needs at least two diﬀerent types of quarks, which
are designated by up (u) and down (d) and have charge 2/3 and -1/3 charge respectively.
The proton consists of three quarks (uud), resulting in a total charge of +1, while the
neutron contains the combination (udd). Quarks are identiﬁed by the quantum property
ﬂavor. There are in total 6 diﬀerent quarks, here listed with increasing mass: up (u),
down (d), strange (s), charm (c), bottom (b) and top (t).

The interaction binding the quarks into hadrons is called the strong interaction, and
is described by the theory of Quantum Chromodynamics (QCD). Such a fundamental
interaction is, according to the standard model, always connected with virtual particle
exchanges. Analogous to the electromagnetic interaction in which photons are exchanged
between electrically charged particles, gluons are the mediators of the strong force and
couple to a quantum number called color charge. This quantum number can assume three
values (red, blue and green), and each quark of a given ﬂavor carries a color quantum
number. In contrast to the photons which have no charge, the gluons carry simultaneous
color and anti-color. This has the eﬀect that they do not only couple to quarks, but also to
other gluons. As a consequence, the strong coupling constant shows a strong dependence
on the quark-quark separation. For large distances the coupling constant grows towards
inﬁnity, which implies that an inﬁnite amount of energy would be required to separate
two color charges. Consequently, no quark or gluon may exist as “free” particle. This
is reﬂected through the fact the the quarks are always arranged in such a way that all
particles which exist in physical vacuum are colorless. This phenomenon is commonly
referred to as conﬁnement. However, for very small distances the coupling decreases

3

asymptotically. In the high energy limit quarks can be considered to be “free”, and this
is called asymptotic freedom.

1.2 Hot and dense nuclear matter

The asymptotic behavior of QCD at high densities has been predicted to be a phase
transition in nuclear matter [8]. Such a phase transition is expected to occur at extreme
temperatures and energy densities, forcing the nuclear matter to undergo a transition into
a deconﬁned state of quarks and gluons. The new phase is known as the Quark-Gluon
Plasma (QGP), and unlike ordinary nuclear matter where quarks and gluons are conﬁned
in bound states as hadrons, they are now considered as almost “free” particles. Such a
phase transition will consequently lead to a dramatic jump in the energy density of the
state, due to the sudden increase of the number of degrees of freedom. There will be more
spin and color states available to the quarks and gluons when moving freely compared to
the number of states available within the hadrons.

In addition, QCD predicts that in a high temperature phase transition a fundamental
symmetry of the QCD theory, which are valid only at high energy densities, is restored.
This chiral symmetry is spontaneously broken at normal nuclear density, and the current
quark masses originate as a direct consequence of this symmetry breaking mechanism.
During a phase transition into QGP the chiral symmetry is approximately restored and
the quark masses are reduced from the large eﬀective values in hadronic matter to their
small bare ones.

Lattice QCD and the phase diagram

Phase transitions are related to large distance phenomena in a thermal medium, and
go along with long range collective phenomena and the spontaneous breaking of global
symmetries. Thus in order to study such a phase transition within the theory of QCD,
a numerical approach that is capable of dealing with the equilibrium thermodynamics of
the strong interactions is needed. Lattice QCD [9] provides a ﬁrst principle approach that
allows to study large distance, non-perturbative aspects of the strong interaction. This
is done by introducing a discrete space-time lattice, which makes it suited for numerical
calculations.

≈

In the lattice calculations a discontinuity in the energy density as a function of tem-
perature is found at a critical temperature of the order of Tc ≈
170 MeV, corresponding
1 GeV/fm3 [10], Figure 1.1. There are however many un-
to an energy density of ǫ
certainties involved regarding the actual value of this temperature, and the order of the
phase transition. The reason is that both depend on the number of ﬂavors and the bare
quark masses being used in the calculations. In the high temperature and zero baryon
density limit, the phase transition is fully described by the chiral symmetry of the QCD
Lagrangian [11]. This symmetry is a global intrinsic symmetry of the theory which is
exact only in the limit of vanishing quark masses. However, the quarks in nature are not
massless, and in particular the heavy quarks (charm, bottom and top) are too heavy to
play a role in the thermodynamics in the vicinity of the phase transition. However, the
strange quark, whose mass is of the order of Tc, plays a crucial role in deciding about the

4

4

T

/

  8

 16

 14

 12

 10

  6

  4

  2

  0

RHIC  

SB/T4

LHC  

SPS  

3 flavour
2 flavour
2+1-flavour

Tc = (173 ±15) MeV 
c ~ 0.7 GeV/fm3 

100

200

400

500

600

300
T (MeV)

Figure 1.1: Energy density as a function of temperature calculated in lattice QCD at zero
baryon chemical potential with various numbers of degenerate quark ﬂavors [10].

nature of the transition at vanishing baryon density. In the massless limit a three ﬂavor
QCD shows a ﬁrst order phase transition. Recent lattice calculations indicate that the
phase transition for realistic values of the up, down and strange quark masses may even be
a rapid crossover taking place in a narrow temperature interval around Tc ∼
170 MeV [12].
=0) the standard Monte-Carlo sampling tech-
niques used in lattice calculations, Figure 1.1, are no longer applicable. However, recent
theoretical progress has overcome this problem, and consequently extends the lattice sim-
ulations of the QCD phase transition for values up to µb=0.5-0.8 GeV [13], Figure 1.2.
The results show a slight decrease of Tc with increasing µB.

At ﬁnite baryon chemical potential (µB 6

The present experimental and theoretical knowledge about the diﬀerent phases of
strongly interacting matter can be summarized in a generic QCD phase diagram, Fig-
In addition to the phase transition at high temperatures, deconﬁnement is
ure 1.3.
expected at suﬃciently large density (several times normal nuclear matter density) and
low temperature. However, in this case the evidence is less compelling due to the lack of
lattice results for very high values of µB.

Relativistic heavy ion collisions

Relativistic heavy ion collisions oﬀer a unique tool to probe hot and dense nuclear matter
in the laboratory. During the last decades a great number of experiments have been
carried out in order to explore the nuclear state of matter as a function of temperature
and energy density. The main motivation is the search for a QGP phase. At the CERN
SPS accelerator a series of ﬁxed target experiments has collected a wealth of information
about nuclear matter at center-of-mass energies of √s=5-20 A GeV. However, the results
have not ﬁrmly established the existence of the QGP phase yet, as the energy density
obtained only slightly exceeds the critical temperature, Tc.

5

e
e
e
de Forcrand, Philipsen
Fodor, Katz
Allton et al.

180

175

170

165

160

155

V
e
M
T

/

150
0

200

400
B/MeV

600

800

Figure 1.2: Lattice calculations at ﬁnite baryon chemical potential from [13] and references
therein. The lines indicate a rapid crossover transition at low µB which becomes ﬁrst order
above the tri-critical point. The position of this point (indicated by the rectangle on the
ﬁgure) is subject to signiﬁcant uncertainties.

Current heavy-ion experiments at the Relativistic Heavy Ion Collider (RHIC) at
Brookhaven National Laboratories (USA) and scheduled experiments at the Large Hadron
Collider (LHC) at CERN (Switzerland), will generate suﬃciently high energy densities to
form a baryon-free plasma. At the RHIC accelerator, four experiments are dedicated to
the study of Au–Au collisions at center-of-mass energies up to √s=200 A GeV. Further-
more, the LHC will make Pb–Pb nuclei collide at √s=5.5 A TeV which will be studied by
the ALICE experiment. At these energies, nuclear matter is predicted to be transparent
enough to form baryon-free matter, heated well beyond the expected phase transition
temperature.

1.3 The dynamics of heavy ion collisions

Even though relativistic heavy ion experiments in the laboratory may recreate the con-
ditions for a phase transition, direct comparison to lattice QCD calculations is generally
very diﬃcult. The reason is that lattice QCD exclusively describes matter in a ther-
modynamical equilibrium, while the outcome of a heavy ion collision is a ﬁnite, highly
excited and dynamical non-equilibrated system. The correct theoretical treatment of such
a system is therefore not a trivial task, and involves concepts which go far beyond the
capabilities of simple statistical thermodynamics. These models can however be valuable
as they can provide ﬁrst order (quasi-)analytic solutions that can be compared directly
with measured quantities.

The evolution of a heavy ion collision in space and time depends extensively on the
initial conditions of the system. Consequently, heavy ion collisions are generally divided
in two energy domains: Lower energies where the stopping power is suﬃcient to stop

6

m
early universe

quark gluon
plasma

]

V
e
M

 

 

[
T
e
r
u
t
a
r
e
p
m
e
t

250

200

150

100

50

RHIC SPS

Lattice QCD

AGS

SIS

hadron gas

atomic
nuclei

neutron stars

0.2

0.4

0.6

0.8

1

1.2

baryonic chemical potential m

1.4
B [GeV]

Figure 1.3: QCD phase diagram summarizing the present understanding about the struc-
ture of nuclear matter at diﬀerent densities and temperatures. The points marks illus-
trates the results achieved by the diﬀerent ultrarelativistic collider experiments, and the
dashed line represent the lattice QCD calculations.

the colliding nuclear matter, and higher energies, where the colliding baryons initially
penetrate each other. The former case is applicable to the energy range of the AGS and
SPS experiments, and is commonly described within Landau’s ﬂuid-dynamical model. For
the energies which will be obtained at the LHC, the latter scenario is most likely to be
the case, and is often described with the scaling hydrodynamical model of Bjorken [14],
Figure 1.4.
In this picture, the Lorentz contracted nuclei become almost completely
transparent to each other, and the valence quarks maintain their initial rapidities. At their
inter-penetration, however, the partons interact creating a high energy density chromo-
electric ﬁeld between the two nuclei. Within the chromo-electric ﬁeld a system of non-
equilibrated deconﬁned quarks and gluons is created. This matter constitutes the so-called
pre-equilibrium phase, which after a certain formation time might lead to a local thermal
equilibrium provided that there are enough interactions among the constituents. The
initial conditions at which an equilibrium is reached is deﬁned by the proper time, τ . The
proper time is deﬁned as the local time in the rest frame of any ﬂuid element. If all the
particles originate from one point in space-time the proper time can be expressed as

After a formation time τ0 which is likely to be 0.5 to 2 fm/c the system reaches thermal
equilibrium which is characterized by a uniform energy density and temperature. From

τ = t/γ = √t2

z2.

−

7

time

Freeze−out

Hadron gas

Mixed phase

Equilibrated QGP

Deconfined quarks and gluons

z

Beam

Beam

Figure 1.4: The Bjorken space-time scenario for a heavy ion collision. The two colliding
nuclei resembles two ﬂat discs because of the Lorentz contraction in the laboratory frame.
The parabola indicate the constant proper times.

that time on the system is treated using 1+1 dimensional (spatial + time) ideal relativistic
ﬂuid dynamics, where the system expands in the longitudinal direction.

As the system expands, the equilibrated plasma of deconﬁned quarks and gluons
quickly cools down to the temperature where a phase transition into a hadron gas takes
place. Depending on the type of the transition, the system may spend some time in a
mixed phase where the QGP coexists with the hadron gas. Finally, the size of the system
becomes larger than the mean free path of hadrons, in which they undergo a freeze-out
and stream freely towards the detectors. This freeze-out process is most usually treated
as a sudden freeze-out, implying that at a given instant in the space-time all constituents
within the ﬂuid become independent, and ﬁnal interactions and collisions are neglected.

1.4 The experimental observables

In order to establish experimentally the properties of the hot and dense partonic matter
created in heavy ion collisions, a wide range of variables of the system have to be measured.
Due to the short existence and limited spatial extend of the generated plasma, however,
basic properties such as volume, temperature, density of the plasma state and the masses
of the quarks contained in it, cannot be measured directly. Instead, it must be derived
from the remnants of the collision, i.e. the ﬁnal state particles which after the freeze-out-
stage has reached the detectors. Several observables have been suggested and identiﬁed,
which needs to be evaluated individually and/or in combination with other probes.

8

In general, the observables in a heavy ion collisions can be divided into three main

categories:

Hadronic observables.

Electromagnetic observables.

Hard probes.

•

•

•

Each of the observables are characteristic of a certain stage in the collision, but they are
not completely independent of each other. The hadrons emerge only in the ﬁnal stage
of the collision after they freeze-out from the hadron gas, and thus carry information
about the system at the time of freeze-out. The electromagnetic observables on the other
hand will, because of their long mean-free-path relative to the size of the QCD medium,
manage to escape from the system without any further interaction, and thus emerge
predominately from the earlier, hot stage of the collision. Lastly, the initial stage of the
collision is dominated by the collision dynamics of the produced partonic system, and the
study of hard processes enable to probe the very early parton dynamics and evolution of
the initial stage of the system.

In the following the main observables which will be relevant at LHC energies, and
therefore will be measured by the ALICE experiment, will be introduced. These observ-
ables are based on theoretical predictions combined with experimental results from SPS
and RHIC.

Hadronic observables

The hadronic observables are often referred to as soft probes of the heavy ion collision, as
they mostly connect to the non-perturbative aspects of QCD. They deal with the more
global characteristics of the system such as particle production, particle abundances and
spectra and correlations.

Particle multiplicity

One of the most important and fundamental observable in a heavy ion collision is particle
multiplicity. By measuring the number of particles produced in the collision, one can
determine the energy density of the system. From a theoretical point of view, this is
important since it enters the calculation of most other observables. On the experimental
side, the particle multiplicity ﬁxes the detector performance, and thus the accuracy with
which many of the observables can be measured.

The particle multiplicity in heavy ion collisions is very diﬃcult to predict, since it
cannot be calculated from ﬁrst principles. The obvious approach is thus to extrapolate
already measured quantities obtained in lower energy experiments, using diﬀerent theo-
retical extrapolation models. In Figure 1.5 data from RHIC and predictions for center
of mass energies up to LHC levels (√s=5.5 TeV) [15] are shown. At RHIC the charged
particle multiplicity per unit pseudo-rapidity, dNch/dη, is measured to be 700-800 at η=0.
The predictions for LHC show that one should expect a value of about 2200. In [16] the
multiplicity is computed in a two-component soft+semi-hard string model, which gives a
slightly higher density of 2600-3200, depending on the initial assumptions.

9

A=197, central
A=197, 6 % central
A=208, central
A=208, 6 % central
PHOBOS data
PHENIX data
STAR data

5· 103

103

1
<

|

|

]

dh

 
/
 

h
c
N
d

[

2· 102

102

103
(cid:214) s (GeV)

104

Figure 1.5: Data and predictions for charged particle multiplicity per unit pseudo-
rapidity [15].

Particle spectra and correlations

Most of the particles emitted in a heavy ion collision are hadrons which decouple from the
collision region during the hadronic freeze-out stage. Hence, by measuring the diﬀerent
particle spectra, one obtains information about the chemical and kinetically freeze-out
distributions. From these observables, one can derive quantities like the freeze-out tem-
perature and chemical potential, ﬂow velocities within the expanding system, size of
the system etc. Since these distributions are also highly constrained by the dynamical
evolution of the system, they will also yield information about the early stages of the col-
lision [17, 18, 19]. Furthermore, the ﬁnal momentum distributions may provide detailed
information about the time evolution of the collision system [20].

Essential information about the collision system is obtained from studying its evolution
in time and space. The size and expansion results from the work of pressure gradients
within the system, and hence reﬂects directly the underlying equation of state. This can
be obtained directly by particle interferometry or correlations. By these methods one can
measure the ﬁnal size of the ﬁreball, gain insight about its expansion and phase-space
density and provide information about the timing of the hadronization.

Furthermore, the so-called elliptic ﬂow is sensitive to the degree of thermalization
achieved in the system. In general it describes the azimuthal asymmetry of the particle
production, and builds up through re-scattering in the evolving system which converts
the spatial anisotropy into momentum anisotropy. A rapid expansion of the hot system
will destroy the original anisotropy and reduce the following momentum anisotropy. Thus
by measuring the elliptic ﬂow, information about the early stage of the collision is ob-
tained, and in particular whether local thermalization is reached followed by a collective

10

h
hydrodynamic expansion. The observed large elliptic ﬂow measured at RHIC, Figure 1.6,
indicates that the hydrodynamical model is applicable for a wide range of momenta and
particle types.

2

v

0.1

0.08

0.06

0.04

0.02

0
0

2

v

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0
0

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

1.2

1.4

/nchn

max

2

1.6
1.8
 (GeV/c)

tp

Figure 1.6: Measured elliptic ﬂow at RHIC [21]. Left: Elliptic ﬂow as a function of
centrality. The open rectangles show a range of values expected in the hydrodynamical
limit. Right: Elliptic ﬂow as a function of transverse momentum for minimum bias events.

Fluctuations

Like any other physical measured quantities, the observables in a heavy ion collisions are
also subject to ﬂuctuations. These ﬂuctuations can themselves provide useful information
about the collision as they are generally system dependent. One of these observables is
the ﬂuctuation of certain particle ratios, as they give access to information about the
abundance of resonances at the chemical freeze-out [22]. Furthermore, by measuring the
charge ﬂuctuations per unit degree of freedom of the system in a heavy ion collision, one
can gain knowledge whether a QGP phase was created [23]. The argument is that in a
QGP phase the system would consist of quarks and gluons which means that the unit
of charge is 1/3, while in a pure hadronic phase it will be 1. The ﬂuctuation in the net
charge depends on the squares of the charges, and hence are strongly dependent of the
phase it originates from.

Electromagnetic observables

like photons, carry unperturbed information about the
Electromagnetic observables,
source in which the photons have been produced. Since photons are electromagneti-
cally interacting particles, their mean free path in the QCD medium is large enough to
escape the system without any further interaction. These so-called direct photons provide
a powerful probe of the evolution of the collision. However, the experimental feasibility is
dominated by a severe background from the radiative decay of neutral pions (π0
γγ).
Results from WA98 experiment indicates that the task of extracting the direct photons
at SPS-energies is feasible [24]. Recent results from the PHENIX experiment at RHIC
show a direct photon signal above the expected background in central Au–Au events [25].

→

11

r
o
t
c
a
F
 
n
o
i
t
a
c
i
f
i
d
o
M

 
r
a
e
l
c
u
N

1.5

1

0.5

d+Au (MB)

Au+Au (0-10%)

=0

1

2

3
 [GeV/c]

4

Tp

5

Figure 1.7: Nuclear modiﬁcation factor measured for minimum biased collisions of d–Au
at √sN N =200 GeV compared to central Au–Au collisions [26]. The nuclear modiﬁcation
factor is deﬁned as the ratio of the hadron yields in nucleus-nucleus interactions and the
yield in nucleon-nucleon interactions scaled by the equivalent number of binary nucleon-
nucleon collisions to account for the collision geometry.

During the initial non-equilibrated stage of a heavy ion collision at LHC, the dynamics
are dominated by hard processes within the interacting partonic system. The study of
such processes thus probes the very early parton dynamics and the evolution of the QGP
phase. In contrast to the hadronic observables, the hard probes involve only a limited
number of energetic colliding partons, and are theoretically treated by perturbative QCD.

Hard probes

Jet production

During the inter-penetration of two high energetic colliding nuclei, the partons within
the projectiles interact with each other in hard 2 to 2 processes, and the initial parton
momentum is transferred into ﬁnal state partons or photons. Each of these ﬁnal state
partons will then emerge back-to-back from the collision region and radiate energy because
of their color charges before they ﬁnally hadronize into a number of colorless hadrons.
The resulting cluster of particles is commonly referred to as jets.

High transverse energy jets produced in a heavy ion collision are expected to loose
major parts of their initial energy when traversing the collisions region prior to the freeze-
out phase. Studying jet production can thus help to determine the QCD medium eﬀects
acting on a color charge traversing a medium of color charges, in analogy to the Bethe-
Bloch physics of QED. By comparing the cross section for jet production for that in p–p
collisions at the same center of mass energy, one can identify these medium modiﬁcations
of the jet properties which characterize the hot and dense nuclear matter in the initial
stage of the collision region.

Several observables has been proposed as probes for the energy loss of the fast moving

12

h
 h++h-

0.2

d+Au FTPC-Au 0-20%
d+Au min. bias

(a)

p+p min. bias
Au+Au central

(b)

)
φ
∆
(
d
N
d

/

 

r
e
g
g
i
r
t

N
1

/

0.1

0

0.2

0.1

0

0

π/2

π

∆φ (radians)

Figure 1.8: Two-particle azimuthal distributions for high transverse momentum hadrons
measured at RHIC [30]. The distributions in d–Au collisions include a near side (∆φ
0)
peak similar to that seen in p+p and Au–Au collisions and typical of jet production, and
π) peak similar to di-jet events seen in p+p. This second peak is
a back-to-back (∆φ
suppressed in central Au–Au collisions, indicating ﬁnal state interactions with the dense
system generated in the collision.

∼

∼

partons in the medium of deconﬁned color charges [27, 28, 29]. In particular, this energy
loss should be visible as a reduced yield, or quenching, of high momentum hadrons in
central A-A collisions. This eﬀect has indeed been observed at RHIC, Figure 1.7. The
measurements show that central collisions between Au–Au nuclei exhibit a very signiﬁcant
suppression of the high transverse momentum component as compared to nucleon-nucleon
collisions. This observation indicates a substantial energy loss of the ﬁnal state partons
or their hadronic fragments in the medium generated by high energy nuclear collisions.

∼

∼

0 and ∆φ

Furthermore, the production of jets has been demonstrated in angular correlations of
high transverse momentum hadrons through the observation of enhanced correlations at
∆φ
π, Figure 1.8. By comparing the measurements from d–Au collisions
to central Au–Au collisions one observes a suppression of the back-to-back correlation
for central Au–Au collisions, indicating that one of the two jets is no longer present. If
this suppression would be a result of initial-state eﬀects, it should consequently also be
observed in d–Au collisions but no such suppression is observed. This energy imbalance
thus suggests that one of the jets which has a much longer in-medium path-length interacts
with the dense system and looses substantial amounts of its energy, which is in agreement
with the predicted jet quenching in a QGP.

13

Heavy quark production

Heavy quarks like charm and bottom provide a probe which is highly sensitive to the
collision dynamics. Heavy quark production is an perturbative phenomenon which takes
place on a time scale of the order of the inverse quark mass. The relative long lifetime
of the charm and bottom quarks allows them to live through the thermalization phase
of the QGP, and thereby also be aﬀected by its presence. Also, heavy quark-anti-quarks
may form quarkonium states with binding energies comparable to the temperature of the
QGP, implying a large quarkonium break-up and suppression.

Typical observables including heavy quark production are the total production rates,
transverse momentum distributions and kinematic correlations between the heavy quark
and anti-quark. These observables have to be compared to those of p–p and p–A collisions
in order extract information on the properties of the hadronic plasma.

The observables connected to the heavy quark production will become increasingly
important at LHC energies, as the center of mass energy will be suﬃcient to copiously
produce the heavy quarks charm and bottom and their bound states.

14

Chapter 2

The ALICE Experiment at LHC

2.1 Introduction

The LHC accelerator at CERN is scheduled for 2007. As the only experiment build for
the heavy ion program, the A Large Ion Collider Experiment (ALICE) experiment is
optimized for the study of heavy ion collisions at the foreseen center of mass energy of
5.5 A TeV. The main goal of this experiment is to probe in detail the non-perturbative
∼
aspects of QCD such as deconﬁnement and chiral symmetry restoration. Extrapolating
from present results, all parameters relevant to the formation of the QGP phase will
be more favorable, and in particular the energy density and the size and lifetime of the
system should all improve by an order of magnitude compared to SPS and RHIC.

The ALICE detectors are designed to measure most of the observables which is rele-
vant to the formation of a QGP phase. The experimental capabilities to measure these
observables depend both on the performance of the detectors and the number of events
which can be collected.

2.2 LHC running strategy

The heavy ion program foreseen for LHC will mainly consist of two parts [31]: Colliding
Pb–Pb at the highest possible energy, and a more limited systematic study of diﬀerent
collision systems for diﬀerent beam energies.
In addition to A–A systems, both p–p
and various p–A systems will be studied in order to study the system as a function of
energy density and to provide reference data for the Pb–Pb systems. The ALICE running
program has therefore been divided into two phases: An initial phase which is based on
the current theoretical understanding and results from RHIC, and a second phase where
a number of diﬀerent running options will be considered depending on the outcome of
the initial results.

The ﬁrst data that will be taken with ALICE will be from p–p collisions. The LHC
will start running with several months of proton beams, followed by the end of each year
by several weeks of heavy ion collisions. The eﬀective running time per year is expected
to be 107 s for proton and 106 s for heavy ion operation. During the ﬁrst heavy ion
run, Pb–Pb collisions at the highest energy density is foreseen to provide global event
properties and large cross section observables. For low cross section observables, and in

15

Figure 2.1: The ALICE detectors.

particular hard processes which are the main focus of LHC, 1-2 years of Pb–Pb runs at
the highest possibly luminosity are required to collect suﬃcient amount of statistics. In
the later running phase, p–Pb collision will be run in order to provide reference data for
Pb–Pb systems. Further on, energy dependencies will be studied by using lower-mass ion
systems such as Ar–Ar.

2.3 Detector layout

The complete layout of the ALICE detector as proposed initially together with the physics
objectives are described in the ALICE Technical Proposal [32, 33, 34]. Since then, some of
the sub-detectors have been modiﬁed to meet the new experimental goals set by the most
recent results from RHIC and latest theoretical developments. Most of the individual
sub-detectors are described in detail in their respective technical design reports [35, 36,
37, 38, 39, 40, 41, 42, 43, 44, 45]. In the following a brief description of the general ALICE
detector layout and a introduction to the diﬀerent sub-systems, is given.

The experimental setup of the ALICE detectors, Figure 2.1, is mainly composed by

three parts:

•

•

•

The central barrel which is contained in the L3 magnet. The detectors in the central
< 0.9 over the full azimuth
barrel region have an acceptance in pseudo-rapidity of
angle. These detectors will probe hadronic signals, di-electrons and photons.

η

|

|

The forward muon spectrometer for detecting muon pairs from the decay of heavy
quarkonium in the interval 2.5 < η < 4.0.

The forward detectors, η > 4, which will be used to determine the multiplicity.
These detectors will also be used as a fast centrality trigger.

16

The Inner Tracking System

The Inner Tracking System (ITS) is designed and optimized for reconstructing secondary
vertices from hyperon and charmed meson decays, and precision tracking and identiﬁ-
cation of low pt particles. The detector consists of 6 layers of high resolution silicon
detectors, located at innermost radius 4 cm to outermost 44 cm. The diﬀerent layers are
designed to achieve an impact parameter resolution of 100 µm within the expected parti-
cle density. Hence the innermost layers consists of pixel detectors, silicon drift detectors
for the following two, and the two outer layers are equipped with double-sided silicon
micro-strip detectors.

The Time Projection Chamber

The main tracking device in ALICE is a cylindrical Time Projection Chamber (TPC).
Its main purpose is thus to provide charge particle momentum measurement over the
central rapidity region and particle identiﬁcation via dE/dx.
In addition, it will use
information from the ITS, Transition Radiation Detector (TRD) and Time Of Flight
(TOF) detector (see next section) in order to obtain a more accurate vertex determination,
particle identiﬁcation and two track separation. The TPC has an inner radius of 90 cm
which is given by the maximum acceptable hit density, and an outer radius of 250 cm
deﬁned by the length required for a dE/dx resolution of <10%. The overall acceptance
is

< 0.9 and thus matches that of the ITS, TRD and TOF.

η

|

|

Detectors for Particle Identiﬁcation

Particle identiﬁcation (PID) for a large part of the phase space is obtained by a combi-
nation of dE/dx from the ITS and TPC, and time of ﬂight information from the Time of
Flight (TOF) detector.

Electron identiﬁcation above 1 GeV/c is provided by the Transition Radiation Detec-
tor (TRD). The TRD will in conjunction with ITS and TPC provide electron identiﬁcation
in order to measure, in the di-electron channel, the production of light and heavy meson
resonances as well as to study the di-lepton continuum.

For the high momentum PID a Ring Imaging Cherenkov (RICH) detector will be
used. The detector covers 5% of the acceptance of the central detectors, and allows PID
of hadrons up to 5 GeV.

The Photon Spectrometer

The measurement of direct photons, π0 and η is provided by a high-resolution electro-
magnetic calorimeter, the Photon Spectrometer (PHOS). The detector is located on the
bottom of the ALICE experimental assembly, and is built from scintillating lead-tungstate
crystals coupled with photo-detectors. The readout electronics provides both energy and
time information to reject anti-neutrons and trigger for high pt photons.

17

The Muon Arm

The forward muon spectrometer will allow study of vector resonances via the µ+µ− decay
channel. It is placed outside the L3 magnet, and consists of a composite absorber close
to the interaction point in order to reduce the µ background from π and K decays. The
spectrometer magnet is a large dipole magnet with a nominal ﬁeld of 0.7 T. Tracking
is performed within 10 planes of thin multi-wire proportional chambers with cathode
readout.

The Forward Detectors

Several smaller detectors placed in the forward region, η >4, will be used to measure global
event characteristics such as the event reaction plane, multiplicity of charged particles and
precise time of the collision. The multiplicity information is partially used to derive a
trigger.

A set of four small and very dense calorimeters, the Zero Degree Calorimeter (ZDC)

will be used to measure and trigger on the centrality of the collisions.

The Photon Multiplicity Detector (PMD) is a pre-shower detector which is mounted
behind the TPC opposite to the muon arm. It will measure the ratio of photons to charged
particles, the transverse energy of neutral particles, the elliptic ﬂow and the event reaction
plane.

The Forward Multiplicity Detector (FMD) consists of silicon pad detectors organized
in ﬁve disks which is placed on both sides of the central detectors. It will measure the
pseudo-rapidity distribution of charged particles over a large fraction of the phase space.
The T0 counters are 24 Cherenkov radiators and will provide the event time with a
precision of 50 ps. The V0 counters (consisting of scintillators) will be used as the main
interaction trigger and to locate the event vertex.

2.4 The TPC detector

The ALICE TPC detector is the main tracking device. It is placed inside the homoge-
neous magnetic ﬁeld of the L3 magnet. With almost full three dimensional coverage, the
tracking detector can provide information of the complete particle track in addition to the
particles speciﬁc energy loss, dE/dx. Experience from previous experiments show that
TPC detectors can handle high particle multiplicities and high track densities (EOS [46],
NA49 [47] and STAR [48]).

2.4.1 Principle of operation

The TPC detector is a gaseous ionization detector. This group of detectors are sensitive
to the ionization electrons and ions that are produced when charged particles traverse
the gas in the detector.

The TPC detector consists of a large cylindrical chamber ﬁlled with gas, Figure 2.4. A
uniform electric ﬁeld, E, is applied and directed along the detector volume. When charged
particles traverse the gas they will ionize the gas along their trajectory liberating electrons.
The liberated charge is subject to the electric ﬁeld, and electrons will drift opposite the

18

Figure 2.2: TPC principle for detection of charged particle trajectories.

direction of E towards the end-caps of the chamber where their position is detected. This
then yields the two-dimensional position of a space point onto the end-cap plane. The
third coordinate is given by the drift time of the ionization electrons. Since all ionization
electrons created in the sensitive volume of the TPC will drift towards the end-cap, almost
a continuous sample of space points for each track is detected allowing a full reconstruction
of the particle trajectory. Furthermore, the charge which is collected at the end-caps is
proportional to the ionization, and thus the energy loss, of the particle. The signal
amplitudes provide information on dE/dx of the traversing particle. In conjunction with
the measured momentum obtained from the curvature of the trajectory in the magnetic
ﬁeld, this enables particle identiﬁcation.

The detection of the drifting electrons is done by using Multi-Wire Proportional
Chambers (MWPC), Figure 2.2. The primary electrons by themselves do not induce
a suﬃciently large signal for readout. The necessary signal ampliﬁcation is provided by
avalanche creation in the vicinity of anode wires. The readout chambers consist of a grid
of anode wires above a cathode pad plane, a cathode wire grid and a gating grid. A
negative voltage is applied to the cathode wires and cathode pads producing an electric
ﬁeld near the anode wires with a 1/r dependence. When the drifting electrons enters the
region behind the gating grid, they will continue to drift along the ﬁeld lines towards
the nearest anode wire. Upon reaching the high ﬁeld region close to the anode wire,
the electrons will be accelerated to produce an avalanche. The positive ions liberated
in the avalanche process will induce charge on the cathode pads. This signal current is
characterized with a fast rise time and a long tail due to the motion of the positive ions.
The function of the gating grid is to open and close the ampliﬁcation region to the

19

drift volume. When a trigger signal is issued, the gating grid wires are held at the same
potential, admitting the electrons from the drift volume to enter the ampliﬁcation volume.
Then, when absence of a valid trigger, the gating grid is biased with a bipolar ﬁeld which
In addition, the closed
prevents the electrons from drifting into the avalanche region.
gate prevents the positive ions created in the previous event from drifting back into the
drift volume. This is important since escaping ions into the drift volume accumulate, and
can cause severe distortions of the drift ﬁeld.

A precise measurement of the location of the avalanche can be obtained if the induced
signal is distributed over several adjacent readout pads, using an appropriate center-
of-gravity algorithm. The position of the particle track in the drift direction can be
determined by sampling the time distribution of each pad signal. The resulting two-
dimensional pulse height distribution in pad-time space is called a cluster.

Signal shape and position resolution

The intrinsic resolution of a TPC detector is determined by the so-called Pad Response
Function (PRF). This function represent the relative pulse height distribution of signals
induced on adjacent pads by a point-line avalanche. Its distribution is well approximated
by a Gaussian function,

Pi = C

exp

·

(x

xi)2
x !

−
2σ2

,

 

(2.1)

where x is the position of the induced avalanche and xi the respective pads. The width of
the distribution, σx, is not entirely determined by the PRF. The reason is that the drifting
electrons are spread because of diﬀusion when drifting towards the end-caps1. Thus, the
distribution of the primary electrons arriving at the anode wires cannot be considered
point-like. In addition, the ﬁnite track inclination angle with the pad-plane spreads the
ionization such that width of the initial charge distribution represents a projection of
the track segment over the pad-length. The resulting mean cluster width along the pad-
direction can be parameterized as [49],

x = σ2
σ2

P RF + D2
t ·

sdrift +

l2

tan2(β)
12

·

d2

·

+

(tan(α)

tan(ψ))2

−
12

(2.2)

where Dt is the diﬀusion constant of the gas, sdrift is the drift distance of the drifting
electrons, l is the pad-length, d is the distance between two anode wires and β is the
inclination angle of the track, Figure 2.3. The angle between the normal to the anode
wires and the projection of the track, α, is in the ALICE TPC equal to β. The Lorentz-
angle, ψ, is deﬁned as the angle between the electric ﬁeld and the drift-velocity. This
angle applies near the anode wires where the electric and magnetic ﬁelds are no longer
parallel, leading to a displacement of the drifting electrons.

In the longitudinal direction, the width of a pad signal generated by a single electron
avalanche is given by the shaping constant of the readout electronics. The time signal
is obtained by folding the avalanche with a Gaussian shaping function. Also in this
direction the electron distribution suﬀers from diﬀusion and track inclination, and similar

1This spread is partially reduced by the parallel magnetic ﬁeld along the drift direction which conﬁnes

the electrons to helical trajectories about the drift direction.

20

Anode wires

d

Pad length l

Track

Pad width

Figure 2.3: Deﬁnition of the track inclination angle in the ALICE TPC.

to Equation 2.2 the mean cluster width in the longitudinal direction can be parameterized
by

L = σ2
σ2

0 + D2
l ·

sdrift +

l2

tan2(λ)
12

,

·

(2.3)

where Dl is the longitudinal diﬀusion constant, and λ is the inclination angle of the track
in the drift direction.

The cluster widths are subject to ﬂuctuations, which depends on the contribution
of the random diﬀusion and the angular spread, and on the gas gain ﬂuctuation and
secondary ionization. Furthermore, deviations from the Gaussian shape of the clusters
may occur as a result of asymmetric distribution of the electron cluster.

The accuracy in which the centroid of the cluster can be determined is limited by the
spread of the ionization and the subsequent diﬀusion which ampliﬁes this spread. Similar
to the widths of the cluster, the resolution therefore also depends on the track inclination
angles and the drift distance, and is theoretically given by [49]:

x = δ2
δ2

x,0 +

y = δ2
δ2

y,0 +

D2
t ·
l
·
D2
l ·
l
·

sdrift
ne
sdrift
ne

+

+

l2 tan2(β)
neﬀ,pad
12
l2 tan2(λ)
neﬀ,pad
12

·

·

+

d2

(tan(α)

·
12

−
neﬀ,wire

tan(ψ))2
nsense

·

·

Here, ne is the number of ionized electron-ion pairs per cm track length, neﬀ,pad is the
number of primary electrons in the gas “column” below the pad, neﬀ,wire is the number of
primary charged units along an anode wire on which the cluster charge is deposited and
nsense is the number of anode wires crossing the pad.

Occupancy

The performance of a TPC depends highly on the detector occupancy. The occupancy
is a measure of the track density with respect to the intrinsic detector resolution.
In
general, one can deﬁne it as the probability of having a signal above threshold,

(2.4)

(2.5)

O =

Nabove
Nall

,

21

b
Figure 2.4: ALICE TPC schematic layout.

where Nabove is the number of signals above threshold and Nall is the total number of
time-bins. For the TPC, the signals are the bins in pad-row-plane. The number of active
signals is a function of the particle density, F , and on the eﬀective cluster area, seﬀ, and
can be expressed as

O = 1

exp(

F

seﬀ)

−

−

·

(2.6)

Since the occupancy is a function of eﬀective cluster area, an optimization of the detector
parameters in terms of cluster widths is necessary. As described above the cluster widths
in general depends on diﬀusion, response functions and on the pad-length. For a given
gas and drift ﬁeld the diﬀusion is no longer a variable factor, and the cluster size is thus
determined by the geometry of the pad.

2.4.2 Detector layout

∼

250 cm, and a length of

90 cm, an outer radius of

The overall design of the ALICE TPC detector consists of a cylindrical chamber with an
inner radius of
500 cm, Figure 2.4.
∼
A thin high voltage electrode divides the cylinder in two, and provides a uniform electric
drift towards the end-caps. The readout chambers which cover the end-caps of the TPC
cylinder, consist of conventional MWPC with cathode pad readout. The azimuthal seg-
mentation of the readout plane follows that of subsequent ALICE detectors, which leads
18 (both sides of the TPC) trapezoidal sectors, each covering 20◦ in azimuth. The
to 2
< 0.9 for full radial track length, and to
overall acceptance covered by the TPC is
about

|
< 1.5 for reduced track lengths and poorer momentum resolutions.

×

∼

η

η

|

The detector parameters are chosen to minimize the detector occupancy under the
expected high track density. Based on calculations of the PRF for diﬀerent pad and
wire geometry, a rectangular pad shape has been chosen for the ALICE TPC, Figure 2.3.

|

|

22

Pad size
Total number of pad-rows
Total number of pads

×

7.5 mm
63
5504

×

10 mm 6
64
4864

×

15 mm
32
5120

Inner chambers Outer chambers
6

4

Table 2.1: ALICE TPC design parameters of the readout chambers [31].

Detector gas Ne/CO2 (90/10)
Gas volume
Drift length
Drift ﬁeld
Drift velocity
Drift time
Total HV
Diﬀusion

88 m3
2
250 cm
×
400 V/cm
2.84 cm/µs
88 µs
100 kV
220 µm/√cm

Table 2.2: ALICE TPC design parameters of the gas volume [31]. The choice of gas
mixture is currently under discussion, and may also include N2.

Because of the cylindrical volume of the ALICE TPC, the track density has a radial
dependency and is proportional to r−2. This leads to diﬀerent requirements for the
pad sizes and the corresponding readout chambers as a function of distance from the
interaction point. Therefore, the readout is segmented radially into two separate readout
chambers with slightly diﬀerent wire geometry and pad sizes. In total there are 557 568
readout pads of three diﬀerent sizes, Table 2.1. The radial distance of the active area is
from 84.1 cm to 132.1 cm and from 134.6 cm to 246.6 cm for the inner and outer chambers
respectively, while the total area is 32.5 m2.

The drift gas is optimized for drift speed, low diﬀusion, low radiation length and
hence multiple scattering, small space-charge eﬀect and aging properties. The parameters
related to the gas and the drift ﬁeld is listed in Table 2.2.

2.4.3 Readout

The front-end electronics of the detector is responsible for reading out the charge induced
on each of the cathode pads. Each of these readout channels is comprised of three basic
units [50]: A charge sensitive PreAmpliﬁer/ShAper (PASA), a 10-bit Analogue to Digital
Converter (ADC), and a digital signal processing circuit.

The charge induced on a pad is ampliﬁed and integrated by the PASA. A single channel
1000e. Immediately after the PASA, the 10
is designed to have a noise value (r.m.s.)
≤
bit ADC samples the signal at a rate of 5-6 MHz. The digitized signal is then processed
by a set of circuits contained in a single chip named ALTRO (ALice Tpc ReadOut). Each
ALTRO contains 16 channels that operate concurrently to digitize and process the input
signals. Baseline shifts due to signal pile-ups are removed. After the processing, the

23

ALTRO chip performs zero-suppression. Zero-suppression means that a base-line ADC-
value corresponding to 2-3 times the RMS-value above noise is subtracted to correct for
signal baseline instabilities. Hence, ADC-values smaller than a preset constant threshold
value are rejected. In addition, a ﬁlter checks for a consecutive number of samples above
the threshold in order to identify the sequences corresponding to the pulses. The zero-
suppressed data are then formatted into 32-bit words according to a back-linked data
structure. The ALTRO also contain a multiple-event buﬀer for storing trigger-related
data. When a L1 trigger signal is received (Section 2.5.2), the data is stored in memory,
and upon arrival of the second level trigger (L2 accept or reject) the latest event in the
data stream is either frozen in the data memory until complete readout takes place, or
discarded.

The complete readout chain is contained in the Front-End Cards (FEC) plugged into
crates and attached directly to the detector. Each FEC contains 128 channels and is
connected to the cathode readout plane by means of 6 cables. A number of FECs are
controlled by a Read Control Unit, which interface the FECs to the DAQ, the trigger
system and the Detector Control System (DCS). The data is shipped to the DAQ using
optical ﬁbers called Detector Data Link (DDL). Each of the 36 TPC sectors are read out
by 6 RCUs and 6 corresponding DDLs.

2.5 Data volumes and data-acquisition

2.5.1 Data rates

The data rate produced by the detectors is a function of both event rate and event data
size. The event rate is given by the running luminosity, while the event data size is deﬁned
by the granularity of the detectors and the particle multiplicity. The maximum usable
luminosity is limited by both the LHC accelerator and the detector dead times. Given
the amount of readout channels the biggest amount of data is by far produced by the
TPC detector.

Event rates

From a detector point of view, the maximum usable luminosity is limited by the the time
it takes to read out the detectors. In particular, the TPC detector, which is the slowest
detector, needs <90 µs for the electrons to drift to the end-caps. If the luminosity is high
enough, additional events may occur within TPC frame during readout causing several
superimposed events which are shifted in the time direction. These pile-up events will
contribute to the track density and the detector occupancy, and consequently may lead to
a loss in tracking performance. At an average luminosity of 1027 cm−2 s−1, the minimum
bias rate for Pb–Pb is 8 kHz for a hadronic interaction cross section of 8 barn, giving a
probability of having a double event within the TPC frame of 76% [31]. The remaining
“single” minimum biased Pb–Pb event rate is thus limited to 2 kHz, and the central event
rate to 200 Hz.

In the case of p–p runs the situation is diﬀerent. A single p–p event has a very low
multiplicity compared to Pb–Pb, thus the TPC can tolerate several pile-up events without
suﬀering any signiﬁcant loss of tracking performance. In order to keep the pile-up at an

24

acceptable level, the luminosity during p–p runs will be limited to
which corresponds to an interaction rate of
25 piled-up events in the TPC.
be

1030 cm−2 s−1
200 kHz. At this rate, there will on average

∼
Furthermore, the maximum possible event rate for both minimum biased Pb–Pb and
p–p interactions is limited by the maximum TPC gating frequency to approximately
1 kHz. Considering the luminosity, event pile-up conditions and the maximum TPC
gating rate, estimates of the maximum event rates can be obtained, Table 2.3.

3
∼

×

∼

Event sizes

The event sizes for Pb–Pb interactions are directly proportional to the multiplicity pro-
duced in the collision. This makes them very diﬃcult to calculate as the multiplicities
are hard to predict. For Pb–Pb collisions at 5.5 TeV predictions range from 2000 to 8000
particles per unit rapidity for central Pb–Pb collisions at LHC [31], while extrapolation
from RHIC gives values around 2000-3500 (Section 1.4, page 9). During the design of
ALICE the value 8000 was used as a baseline in order to provide a safety margin on the
detector performance.

Simulations [35] indicate that the average TPC occupancy will be about 25% for the
highest multiplicity. Multiplying the number of readout channels with the number of
time-bins and taking into account the 10 bit ADC dynamic range, this leads to an event
size directly at the detector readout of 350 MB. The ADC conversion gain is typically
chosen so that σnoise corresponds to one ADC count. This means that the relative accuracy
increases with the ADC-values, and is not needed for the upper part of the dynamic range.
The ADC-values can therefore be compressed non-linearly from 10 to 8 bits leading to
a constant relative accuracy over the whole dynamic range. By compressing the ADC-
values from 10 to 8 bits, the event size will be reduced to about 290 MB. In addition,
since it is problematic to resolve individual tracks that have a low pt and cross the TPC
volume under small angles relative to the beam axis, a 45◦ cone will be cut out of the data
resulting in the rejection of all particles which are not in the geometrical acceptance of the
outer detectors. This will reduce the data size further by a factor of 40%. Finally, after
75 MB. If running at the central
zero-suppression the raw event size is expected to be
Pb–Pb interaction rate of 200 Hz, this corresponds to a TPC data rate of

15 GB/s.

∼

Regarding p–p interactions, the estimated TPC event size for a single p–p collision
is approximately 60 kB. In this case one also has to take into account the additional
data coming from the pile-up events. The total data volume, including the piled up
events, is estimated to be of the order of 2.5 MB. This event size is estimated assuming a
coding scheme for the TPC data well adapted to a low occupancy and without any data
compression. If running at the foreseen maximum TPC rate of 1 kHz, this would produce
a total data rate
2.25 GB/s. In Table 2.3 the expected event and data rates for the
∼
diﬀerent interactions are summarized.

∼

2.5.2 The trigger system

As described in Section 2.2, the ALICE experiment will operate under diﬀerent beam
conditions. The trigger system is responsible for selecting the diﬀerent types of events
and enable readout of the detector when certain criteria are met. The ALICE trigger

25

Event rate Data rate (approx.)

Collision
p–p
Min. bias Pb–Pb
Central Pb–Pb

1 kHz
1 kHz
200 Hz

2.25 GB/s
22 GB/s
15 GB/s

Table 2.3: Expected ALICE event and data rates for the diﬀerent LHC runs [31].

system is foreseen to operate in three diﬀerent levels [32]: Level 0 (L0), Level 1 (L1) and
Level 2 (L2). These diﬀerent levels correspond to criteria imposed from diﬀerent detectors,
where the selection criteria gets stronger as the trigger number increase. Correspondingly,
the rates at which each trigger level is operated decreases at higher levels.

The L0 and L1 trigger are both ﬁxed-latency triggers, which means that their rate is
constant. The main diﬀerence between the two is that the diﬀerent detectors need trigger
decision to strobe the electronics at diﬀerent times after the interaction. The main task
of the L0 trigger is to signal that an interaction has taken place at the earliest possible
time, which is after about 1.2µs2. This trigger is based exclusively on the information
from the T0 and V0 counters and checks for the following features:

1. The interaction vertex is close the the nominal collision point.

2. The forward-backward distribution of tracks is consistent with a colliding beam

interaction.

3. The measured multiplicity is above a given threshold.

No strong centrality condition is made at L0, as non-central events giving di-muon triggers
are also required. At L1 decision, which is made after about 6.5µs3, more stringent
centrality requirements are made. Its selection is based on information from the muon
system, PHOS, and on the centrality detectors, FMD and ZDC. At this time all the
remaining detectors are strobed. In particular, the TPC gate is opened which leads to
the requirement that the L1 trigger can have a maximum frequency of 1 kHz.

∼

During the drift time of the TPC (

100µs) the L2 decision is made. Based on the data
extracted from the diﬀerent trigger sub-detectors, more selective algorithms are applied
(e.g. a mass cut on the di-muon system). Also, during this time a reset can be issued as a
result of pile-up events (only when running with Pb–Pb interactions). Since the selection
algorithms will diﬀer in processing time, the latency of the L2 trigger is not ﬁxed, but
has an upper bound as deﬁned by the TPC drift time. After the L2 trigger, the data are
all read out from the front-end electronics into the DAQ and High Level Trigger system.

2.5.3 The DAQ system

The data acquisition system [51] is responsible for collecting the data from all the sub-
detectors and assemble the sub-event data blocks into full event before sending the data to

2The L0 latency is an estimate based on the expected transmission time in the cables
3The L1 latency is estimated from the expected time is takes for the muon arm and the ZDC to issue

a trigger signal, including a safety margin of

20%

≥

26

mass storage. The architecture of the system is based on PCs connected by a commodity
network, most likely TCP over Gigabit Ethernet. The data transfer from the front-
end electronics of the detectors are initiated by a L2 trigger accept. The data is then
transferred in parallel from all sub-detectors using special optical links, called Detector
Data Link (DDL), into the Local Data Concentrators (LDC) where the sub-event building
takes place. Parallel to the DAQ, the data is also shipped to the High Level Trigger
system by duplicating the data stream (Figure 3.2). The sub-events prepared by the
LDCs are transferred to one Global Data Concentrator (GDC) where the full event can
be assembled. The event building is managed by the Event Building and Distribution
System (EDBS), which is a protocol running on all the machines (LDCs and GDCs). A
GDC destination for a particular event is determined by the EDBS which communicates
this decision to the LDCs. The fully assembled events are ﬁnally shipped to permanent
storage for archiving and further oﬄine analysis.

The DAQ system is designed to be ﬂexible in order to meet the requirements for the
diﬀerent data taking scenarios. As the p–p interaction produce only data rate of 1/5
relative to Pb–Pb interactions, the requirement on the system is deﬁned by the expected
data rate from the heavy ion runs. In the heavy ion mode, two main types of events have
to be handled. The ﬁrst consists of central Pb–Pb events at a relatively low rate but with
a large event size. The second one concerns the events containing a muon pair which has
been reported by the trigger and is read out with a reduced detector subset, including
the muon arm. Much higher trigger rates are required in the latter case, typically up to
1 kHz.

2.5.4 The High Level Trigger

In the ALICE Technical Proposal [32], the collaboration estimated that a bandwidth
of 1.25 GB/s to mass storage would provide adequate physics statistics. As seen from
Table 2.3 the expected data rate from the detector exceeds this number by an order of
magnitude. This has lead to the proposal and inclusion of the ALICE High Level Trigger
(HLT) system. The task of this system is to reduce the data rate to an acceptable level
in terms of DAQ bandwidth and mass storage costs, and at the same time provide the
necessary event statistics. This is accomplished by performing online processing of the
data, allowing partial or full event reconstruction in order to select interesting events or
sub-events, and/or to compress the data eﬃciently using data compression techniques.
Processing the detector information at a bandwidth of 10-20 GB/s requires a massive
parallel computing system. The functionality and architecture of the HLT system are
topics of Chapter 3.

27

28

Chapter 3

The ALICE High Level Trigger
System

3.1 The necessity of a High Level Trigger

The ultimate goal of the ALICE detector is to detect and investigate the QGP phase of
nuclear matter. This task can only be solved by a coherent measurement of a wide range
of observables from both peripheral to central heavy ion collisions. An essential part of
this measurement is the collection of enough events in order to obtain suﬃcient statistics
for the physics analysis. On the other hand, hard processes such as heavy quarkonium
and jet production corresponds to relatively small cross-sections, and consequently one
needs to consider a large number of events to provide the adequate statistics for these
observables. The systematic analysis of such hard signals therefore calls for running
the detector at the full available luminosity, which makes it necessary to consider all
interactions at the full available central event rate of 200 Hz. However, as described in
Section 2.5.1, the foreseen data rate in such a scenario exceeds the planned mass storage
bandwidth by an order of magnitude. It is therefore necessary to introduce some kind
of online data reduction into the output data stream. Such a reduction should as far as
possible reduce the data readout rate to match the DAQ and mass storage bandwidth,
and at the same time allow ALICE to acquire suﬃcient statistics for the diﬀerent physic
observables. This has lead to the preparation of the ALICE High Level Trigger (HLT)
system, whose prime task will be to enhance event selectivity and/or reduction of the
event size by partial or full online event reconstruction.

Data reduction in ALICE can in general be accomplished in two ways:

3.2 Functionality

– Event rate reduction

– Event size reduction

The ﬁrst method implies that only a fraction of the available events are sent to mass
storage. This option would also be used without any HLT system being present, as the

29

readout rate coming from the detectors would have to be decreased in order to meet the
data rate limitation of the mass storage. However, by introducing the HLT system, data
can be processed online and event selections may be performed on the basis of physics
observables. Thus, the introduction of the HLT system will enable an event rate reduction
and at the same time improve the event statistics needed for the diﬀerent physic programs.
In the latter case, selections of Region of Interest (ROI) and data compression techniques
can be used to reduce the event size itself and thus increase the possible event rate being
sent to mass storage.

In both cases online processing of the data is required, requiring pattern recognition
in order to reconstruct the event. In the following the two cases will be referred to as
running in trigger mode and data compression mode, respectively. In this context trigger
means selection or rejection of events or sub-events on the basis of a speciﬁc physics
analysis. In the data compression mode, the chunk of data representing an (sub)event is
compressed by applying appropriate data compression techniques.

3.2.1 Trigger mode

The HLT trigger running mode can be divided into two subclasses: Complete event
selection/rejection, and Region Of Interest (ROI) readout. Both of them are based on
the online identiﬁcation of some predeﬁned certain physical event. Depending on the
topology of the trigger signals, either full or partial event reconstruction is required for
this mode of operation.

Although the hard probes of QCD are rare, and at the same time require high statistics
for a systematic study, they provide to a large degree the most topologically distinct
tracking signatures in the TPC. Therefore, most of the online HLT trigger algorithms
will be based on online tracking of the TPC data. Further reﬁnement may also result
from using early time information of the ITS and TRD systems. The diﬀerent feasible
trigger modes envisaged to date are described in detail in the HLT Technical Design
Report [51]. A brief summary is given in the following.

Jet trigger

The study of jet production at LHC energies is one of the interesting probes of the strongly
interacting QCD matter (Section 1.4, page 12), but a high number of collected events are
required in order to provide the necessary statistics. Estimations based on scaling from
p–p collisions indicate that around 108 inspected events in the TPC are required to collect
an amount of 104 jet events with Et >100 GeV. One year of Pb–Pb acquisitions result in
108 events at 200 Hz TPC rate, but only a fraction of these events can be written
about 2
to mass storage. Employing online jet-ﬁnder tracking algorithms within HLT, inspection
of all central Pb-Pb events at 200 Hz is however feasible, and will thereby enhance the
yield of jet events by a factor 10.

×

Jets with high transverse energy (Et > 100 GeV) have a on average a unique charged
track topology. Furthermore, they have a suﬃciently charged track multiplicity to stand
over the ﬂuctuating mini-jet background in central Pb–Pb collisions. The stiﬀ nature of
these tracks and their relatively close proximity allow for the implementation of a speciﬁc
and fast local tracking in the TPC.

30

Pions

Kaons

.

u

 
.

a
10

-1

-2

10

-3

10

-4

10

0

1

2

3

4

5
Tp

6
7
 [GeV/c]

0

1

2

3

4

5

Tp

6

7
 [GeV/c]

Figure 3.1: Distribution of charm meson decay products into pions and kaons. The solid
lines represent the signal while the dashed line is the background [51].

Open charm trigger

The measurement of particles carrying open charm (such as D-mesons) provide a probe
which is sensitive to the collision dynamics at both short and long time scales. This
observable will become increasingly important at LHC energies, and its detection and
systematic analysis is one of the main goals of the ALICE experiment. The physics of
open charm cross-section analysis requires 20 Hz of central Pb–Pb for 106 seconds, i.e. one
107 events [52]. If all of these events
month of Pb–Pb acquisition, which amounts to 2
·
850 MB/s (65% of the available DAQ
should be written to tape, this would require
bandwidth) for this observable alone. Thus any means for reducing the required amount
of data is desired to increase the statistics and free bandwidth for other observables.

∼

The open charm meson D0 decays via a weak decay into kaon and a pion with a
branching ratio of 3.83%. The resulting yield in a central Pb-Pb collisions has been
estimated to dN(D0
Kπ)/dy=0.53. The impact parameter of the decay products
is typically about 100 µm. To detect these decays one has to compute the invariant
In order to reduce the
mass of tracks originating from displaced secondary vertices.
combinatoric background various kinematic and secondary vertex topology selections has
to be performed.

→

From a HLT point of view, the foreseen event selection strategy proceeds in two steps:
A momentum ﬁlter which reduces the data volume, and secondly, an impact parameter
analysis rejecting events with no D0 candidate. Figure 3.1 shows the distribution of charm
meson decay products into pions and kaons together with the background (pions and kaons
from the underlying event). For example, if the < pt > of the D0 is 1 GeV/c, the relevant
decay channels, Kππ (for charged D), and Kπ0 (for D0), have a majority of their tracks
above 0.8 GeV/c. Similarly, if the < pt > of the underlying “soft” event is 0.4 GeV/c, the
fraction of tracks with pt > 0.7 GeV/c is about 15% of the total1. Reconstructing all tracks
in the TPC online (with an emphasis on a high eﬃciency at high pt), and keeping only
the raw data along regions revealing high pt trajectories, can reduce the data volume by

1The estimate was obtained from the pt distribution resulting from a simulation of central Pb-Pb

event using the HIJING event generator

.

u

 
.

-1

a
10

-2

10

-3

10

-4

10

31

a factor of 5-6. Applying additional kinematical selection and secondary vertex topology
criteria would further improve the selection of possible D0 events. Simulations show that
signal–to–event of 0.0013 and a background–to–event of 0.0116 [53] should be obtainable
in ALICE. HLT can potentially reduce the data rate needed for the open charm program
by a factor 5-10, thus increasing statistics and at the same time release DAQ bandwidth.

Di-electron trigger

The muon arm measures the J/ψ and Υ spectra via the di-muon channel. A comple-
mentary study of these particles will be performed by reconstructing their leptonic decay
into e+e− and tracking these di-electrons through the TPC, TRD and ITS. The TRD
will trigger on high pt tracks by online reconstruction of particle trajectories in the TRD
chambers, and on the electron candidates by measuring of the total energy loss and the
depth proﬁle of the deposited energy. The true quarkonium trigger rate however is small
10−2 Hz) and the trigger is dominated by the background. De-
(e.g. signal rate of Υ is
pending on the set of cuts being used, a trigger rate of di-electron pairs of 300-700 Hz at
dNch/dy=8000 is expected [36]. The main contributions to the background comes from:

≈

Electron pairs from Dalitz decays of π0, η, ρ, ω, φ and semi-leptonic decays of B
and D mesons.

Electrons or positrons from gamma conversions, Bremsstrahlung, and secondary
interactions.

Pions misidentiﬁed as electrons.

Fake tracks from combinations of clusters from diﬀerent tracks.

The HLT can be used to reject background events by two methods:

Combining TRD tracklets with TPC and ITS tracking. The combined track ﬁt
allows for a more accurate determination of the momentum than by the TRD alone,
and thus HLT will reject secondary electrons by sharpening the momentum cut.

•

Utilizing dE/dx in the TPC. By identifying the particles using dE/dx information
from the TPC, the background from misidentiﬁed pions can be reduced.

Simulations indicates that event rate reduction by a factor of ten can be achieved.

Di-muon trigger

The forward muon arm is designed to detect vector resonances via the µ+µ− decay chan-
nel, and will run at the highest possible rate in order to record all muons with the lowest
possible dead-time. The task of the di-muon trigger system is to select events containing
the di-muon pair from the decay of J/ψ and Υ, where the background is mainly coming
from the muons due to πK decays.

The ﬁrst level of the di-muon trigger consists of a transverse momentum selection
based on the information from two dedicated trigger chambers. The trigger is optimized
for two diﬀerent pt thresholds in order to select low (>1 GeV/c) and high (>2 GeV/c)

•

•

•

•

•

32

Low pt cut
High pt cut

HLT
L0
2000 Hz
500 Hz
550 Hz Few Hz

Table 3.1: Expected trigger rates of the di-muon detector [54]. The two pt -cuts correspond
to the selection of J/ψ and Υ resonances, respectively.

pt muons from the J/ψ and Υ resonances, respectively. However, the coarse-grained
segmentation of these trigger chambers does not allow a sharp pt-cut, resulting in a
rather large background trigger rate. The pt resolution can be improved by performing an
additional tracking step within HLT using information from the muon tracking chambers,
and thus achieve higher trigger selectivity, Table 3.1. The expected background rejection
factor by inclusion of HLT algorithm is 5-100.

Pileup removal in pp

1030 cm−2 s−1 will result
In the case of p–p running, the foreseen running luminosity of 2
in an interaction rate of about 200 kHz. During the TPC drift time of about 90 µs, around
25 superimposed events will be captured in the TPC frame, leading to about 95% overhead
in the data stream. These additional piled-up events will be displaced along the beam
axis, and will not be used during oﬄine analysis. Using HLT to reconstruct all tracks
online, the tracks corresponding to the original triggered event can be identiﬁed while the
tracks belonging to the pile-up events can be disregarded from the readout data stream.
Simulations indicate that an overall event size reduction of
3/25 can be achieved while
retaining an eﬃciency of more than 95% for the primary tracks of the event.

×

∼

3.2.2 Data compression mode

The option to compress the data online provides a method that can improve the physics
capabilities of the experiment in terms of statistics, even without performing selective
readout.
10), the full event rate can in
principle be written to mass storage. Any data compression has to be performed with
caution to assure the validity of the measured physical observables.

If the compression factor is high enough (

≥

The TPC detector produces by far the largest amount of data in terms of event sizes,
and any data compression scheme should therefore be optimized to eﬃciently and reli-
ably compress the TPC data. TPC data are ﬁrst compressed in the the TPC front-end
electronics by the zero suppression, Section 2.4.3. Here, pedestal subtraction (setting
threshold on the ADC values) and identiﬁcation of the sequences in time direction is
performed. The zeros between these sequences are then compressed by Run-Length En-
coding (RLE), which means that the distance between the sequences are stored rather
than storing the zeros themselves.

The data may be further compressed by applying standard data compression tech-
niques such as entropy coding. These algorithms may be directly applied on the RLE
ADC-data and allow bit-by-bit reconstruction of the original data set. Since these tech-

33

niques normally use some form of coding table, they are not very computationally de-
manding, and can even be performed on dedicated hardware such as Field Programmable
Gate Arrays (FPGA). Extensive studies of TPC data in the NA49 experiment, and sim-
ulated TPC data for ALICE, show that compression factors of 2 can be achieved using
these techniques [55]. The most eﬃcient data compression, however, is obtained by using
compression algorithms which are highly adapted to the underlying TPC data. Such
methods exploit the fact that the relevant information is contained in the reconstructed
cluster centroids and the track charge depositions. These parameters can be stored as de-
viations from a model, and if the model is well adapted to the data the resulting bit-rate
needed to store the data will be small. Since the clusters in the TPC critically depends
on the track parameters, the reconstructed tracks and clusters can be used to build such
eﬃcient data models. In contrast to the entropy coding algorithms, such techniques do
not keep the original data unmodiﬁed as the clusters are coded rather than the ADC data.
However, from a data analysis point of view, only the eﬀects on the physics observables
are of importance. Studies carried out in this work indicate that compression factors of
6-10 may be achieved using such a compression scheme. The diﬀerent available TPC data
compression schemes and their performance are discussed in detail in Chapter 5.

3.3 Architecture

The HLT system will have to process an expected data rate of 10-20 GB/s. Given this large
amount of data, and the complexity of the processing task, a massive parallel computing
system is required. The HLT system is therefore planned to consist of a large PC cluster
farm with several hundred up to a thousand separate nodes. The architecture of such a
system is mainly driven by two constraints. Firstly, the data has an inherent granularity
and parallelism deﬁned by the readout segmentation of the detectors. Secondly, HLT is
(in trigger mode) responsible for issuing a trigger decision based on information derived
from a partial or complete event reconstruction. This means that the reconstructed
data ﬁnally has to be collected at a global layer in which the trigger algorithms are
implemented. Both of these requirements demand a hierarchical tree-like topology with
a high degree of connectivity.

In parallel to the DAQ system (Section 2.5.3), the data is received from the front-
end electronics via the DDLs into the receiving nodes of the HLT system, Figure 3.2.
These processors constitute the ﬁrst layer of the HLT system, and are referred to as
the Front-End Processors (FEP). Each DDL is mounted on a HLT Readout Receiver
Card (HLT–RORC) which is a custom designed PCI card hosted by every FEP. Several
HLT–RORCs may be placed in each FEP, depending on the bandwidth and processing
requirements. Every HLT–RORC will be equipped with additional co-processor function-
ality for designated pre-processing steps of the data in order to take load oﬀ the CPUs of
the FEPs. The total number of HLT–RORCs is deﬁned by the readout granularity of the
detectors. For the TPC detector, which is the biggest contributor, the readout is divided
into its respective 36 azimuthal sectors, where each sector is divided into 6 sub-sectors.
6 = 216 DDLs for
Every sub-sector is read out by one DDL, and thus there will be 36
the TPC alone. Taking all detectors into account there will be a total of about 400 DDLs.

×

34

Figure 3.2: Data ﬂow architecture of the HLT system [51]. The detector raw data is
duplicated and simultaneously received by DAQ and HLT. The architecture follows a
hierarchical structure, adapted to the parallelism of the data and the various tasks of the
pattern recognition.

Data-ﬂow

All detectors will ship their data upon a receipt of a L2–accept trigger distributed by the
central trigger processor (Section 2.5.2). Before that time, the data remains within the
domain of the front-end electronics. Table 3.2 gives an overview of the various detector
links and their expected data payload. The associated L2–accept trigger rates for the TPC

Detector Number of Pb–Pb central Pb–Pb per.

Sub-event size per DDL

TPC
TRD
DiMuon
ITS

DDLs
216
18
10
56

(kB)
352
39
15
35

(kB)
90
10

Table 3.2: Number of HLT detector links per detector and their data payload [51].

are limited to 200 Hz for Pb–Pb running and 1 kHz for p–p (Section 2.5.1). The other
detectors can be triggered with 1 kHz, when triggered without TPC coincidence2. This
overview shows that the aggregate HLT input raw data stream amounts to a maximum of
about 20 GB/s. The processing rates will vary as a function of trigger type and running

2With the exception of the Silicon Drift Detectors (SDD).

35

scenarios. For instance in central Pb–Pb collisions the TPC trigger rate will be limited to
200 Hz due to pile-up protection, while for p–p collisions the TPC trigger rate may be as
high as 1 kHz. The maximum HLT output data rate, however, is limited to the maximum
taping rate of 1.25 GB/s. Consequently, both the maximum HLT input and output rates
are deﬁned almost independent of its detailed architecture and requirements.

The detailed data-ﬂow within the HLT system will be deﬁned according to the physics
requirements and operating scenarios. However, the general structure depends to a large
degree on the input and output data ﬂow as stated above, Figure 3.2. The data is
received on the HLT–RORC where the ﬁrst part of the processing will take place using
a Field Programmable Gate Array (FPGA) co-processor. In the case of the TPC this
cluster ﬁnding and/or the
processing consists of local pattern recognition tasks, i.e.
Hough Transform. The data is then transferred into the main memory of the FEP over
the PCI bus, where further processing can be made. After this the data is shipped to a
node at the next level, which consists of as many nodes that are necessary to perform the
processing needed. Here, the processing typically includes track ﬁnding within the TPC
sectors. The output data produced by each level is again shipped to the next level of
nodes until the ﬁnal stage has been reached. In this way, the processing hierarchy follows
a tree-like structure, where successive larger parts of an event are processed and merged
as one comes closer to the root of the tree. At this global level all the necessary data has
been collected into the ﬁnal reconstructed event, i.e. tracks from the diﬀerent sub-sectors
are merged and ﬁtted, and a ﬁnal trigger decision for the event can be taken based on
selection algorithms. The decision and the corresponding data is then passed to the DAQ
system for readout and storage. The interface between DAQ and and HLT will consist
of a number of DDL links between a set of HLT event merger nodes and a number of
DAQ LDCs. The various processing tasks involved in the diﬀerent levels are described in
Chapter 4.

Hardware components

The HLT system will consist of a large scale generic computation farm, with standard
PCs connected with a high bandwidth and low latency network. Given the huge com-
mercial development within this ﬁeld, the project has decided to keep custom hardware
development at a minimum. This will thus enable the system to be very ﬂexible with
respect to selecting the type of processing node and network technology, and also delay
this decision as much as possible due to the continuously drop in prices.

The design of the data ﬂow, and exact processing sequence, determines much of the
architecture and network topology which will be used.
It has also impact on the re-
quirements on the communication between each pair of the HLT nodes. Candidates for
the network technology to be used are not yet ﬁxed, but for the required bandwidth at
least Gigabit Ethernet or a System Area Network (SAN) dedicated to communication in
cluster solutions, is necessary. Possible choices which are considered are among others
ATOLL, SCI and Myrinet.

The HLT–RORC will implement co-processor functionality by usage of Field Pro-
grammable Gate Array (FPGA). The FPGA basically implements a large array of freely
programmable logic gates which can be connected in an arbitrary fashion. The relatively
low clock rate of less than 100 MHz is compensated for by a very high degree of inher-

36

ent parallelism. This makes the FPGA ideal for implementation of algorithm which can
be executed in a highly parallel fashion, such as TPC cluster ﬁnding and the Hough
Transform, Chapter 4.

Communication framework

An essential part of the HLT system is interprocess communication and data transport
within the system. For this purpose a generic communication framework has been devel-
oped [56]. The framework has been designed with an emphasis on three main issues:

Eﬃciency.

Flexibility.

Fault tolerance.

•

•

•

Eﬃciency in this context is primarily the minimization of CPU cycles used for transporting
data, in order to keep as much CPU power as possible available for processing the data.
The requirement for ﬂexibility is a natural consequence from the fact that neither the
topology nor the network interface is deﬁned, and will be postponed as much as possible.
Fault tolerance means that the framework has to be able to handle and recover from errors
as autonomously as possible, and thus should not contain any single points of failure in
which a fault can disable the whole system.

The framework implements an interface between diﬀerent readout steps, by deﬁning
data producers – Publisher – and data consumers – Subscriber. In order to be as eﬃcient
as possible the data is not communicated between the diﬀerent processes, but rather a
descriptor of the data including a reference to the actual data in shared memory is sent. In
this way data is stored in memory as long as possible, avoiding unnecessary copying of the
data. The framework basically consists of a number of independent software components
that can be connected together in an arbitrary fashion. The generic interface allows the
processing modules to have a common interface which is independent of the underlying
network interface.

37

38

Chapter 4

Fast Pattern Recognition in the
ALICE TPC

The main processing task of the High Level Trigger system is online event reconstruction.
Both event selection and eﬃcient data compression needs a preceding pattern recognition
step. The algorithms which have been developed for online reconstruction of the TPC
data, together with a performance evaluation, are presented in this chapter.

4.1 Track reconstruction methods

Every charged particle traversing the detectors leaves a number of discrete hits that allow
spatial allocation of the particle trajectory. The task of the reconstruction algorithms is
to assign these space points to tracks and to reconstruct the particle kinematics. Recon-
structing a particle is equivalent to ﬁnding the parameters that uniquely deﬁne its path
in space. From the reconstructed parameters one can ﬁnd the physical properties of the
particles that passed through the detector.

In general, track reconstruction is a two-folded problem:

•

•

Track ﬁnding.
The input to the track ﬁnder is a list of all hits in selected regions or the complete
detector. The task is to decide which hits are made by a speciﬁc particle, and
group these hits into a number of subsets. All the hits within a subset belong to
the same particle trajectory, and the number of subsets corresponds to the number
of particles traversing the detector.

Track ﬁtting.
The task of the track ﬁtting procedure is to estimate the parameters of the curve
describing the trajectory. The input is thus the positions of all the hits in a subset
provided by the track ﬁnder, whereas the output is a list of particles represented by
an estimate of the track parameters.

Traditionally these two steps are performed separately, but certain applications also com-
bine them.

The increasing complexity in terms of track densities and data sizes in the LHC
experiments, poses high demands on the track reconstruction algorithms. Both real-time

39

applications and oﬄine analysis require fast and robust methods in order to eﬃciently
process the large amount of data produced. A great variety of pattern recognition methods
exist within the ﬁeld of high-energy physics, each with their advantages and disadvantages
with respect to complexity and performance. In the following a brief overview of the main
track ﬁnding and ﬁtting methods is given.

Local track follower

The principle of a track follower is to assign hits to the track candidate under construction
based on the local information of the track. The track candidate is typically initiated
by building short track segments including only a few hits. These track segments are
subsequently extrapolated between the adjacent active detector layers. For every step,
the algorithm tries to assign a new hit to the track, and the choice is being made by
applying χ2 criteria of a temporary track ﬁt. Once a new hit has been assigned, the track
parameters are updated and the search is continued.

The advantage with the local track follower approach is its simplicity with respect
to both data access (since it only needs to access a limited number of hits at the same
time) and the selection criteria for each step. This makes this method eﬃcient when the
hit and track densities are relatively low. However, when the hit density becomes high,
such an algorithm might get confused on the basis of simple χ2 criteria. This will lead
to tracks being split or spliced together in unpredictable ways, and thus eﬀect the track
ﬁnding eﬃciency.

Track-Road methods

The Track-Road methods are similar to the local track follower algorithms. However,
Track-Road methods build tracks by interpolating within possible trajectory roads in
the detector. This is done by choosing hits in the outermost and innermost layers of the
detector, and interpolate between combination of these hits to obtain the complete tracks.
Because of the combinatorial nature of these methods, they are more computationally
demanding than the local track follower algorithms.

Template matching methods

Many of the ambiguities at track crossings in detector layers that arise from local extrapo-
lation algorithms can be overcome with global information of the complete trajectory. The
global information can be obtained by using diﬀerent template matching methods. Here,
a list of predeﬁned trajectories, called templates, are computed based on the equations of
particle motion. The templates are then overlaid on the data from the detector, and a set
of conditions is applied to determine whether a match exists. Originally, the templates
were static, and the resolution of the track parameters were deﬁned by the number and
granularity of the template set being used.
In order to overcome these shortcomings,
diﬀerent adaptive approaches have been developed.

The Elastic Tracking [57, 58] is an approach that deﬁnes the templates in a dynamical
way, by allowing the predeﬁned trajectories to deform via a set of mathematical equations
to match a pattern in the data. This approach has proved to be very eﬃcient in high track
density environments [59]. It also oﬀers the possibility to do tracking on raw detector

40

data without any data pre-processing. However, the high eﬃciency comes at the expense
of very high processing time and memory requirements.

Hough Transform (Histogram method)

Histogram methods are closely related to template matching methods. They transform
particle hits which makes up a pattern into a suitable parameter space for further eval-
uation. Most common is the Hough Transform, which was initially proposed to detect
particle tracks in bubble chamber pictures [60]. A hit or a subset of hits on a particle
trajectory is transformed into curves in a n-dimensional histogram, in which the hits give
a “vote” to all the possible trajectories they can possibly belong to. Once all the hits
on a trajectory have been transformed, the intersection of these curves will correspond
to the parameters of the track. In this way the problem of recognizing global patterns is
reduced to the problem of local peak detection in parameter space.

The advantages of the Hough Transform is its noise robustness and its simplicity with
respect to hardware implementation. However, the transformation is also very computa-
tionally demanding, in particular with respect to the memory requirements.

Kalman ﬁltering

The Kalman ﬁlter method is a method for track ﬁnding and ﬁtting which is very commonly
used in high-energy physics experiments [61, 62, 63]. In the framework of the Kalman
ﬁlter, the change of the parameters of a track along its path is regarded as the dynamical
evolution of a stochastic state vector. The algorithm basically consists of a succession of
alternating prediction and ﬁlter steps. In the prediction step the current state vector is
extrapolated to the next detector layer, taking into account multiple scattering, energy
loss and Bremsstrahlung in the case of electrons. In the ﬁlter step the next hit on the
trajectory is selected based on the predictions of the state vector, and the state vector is
updated accordingly.

The main advantage of the Kalman ﬁlter is that is a method for simultaneous track
recognition and track ﬁtting. It also provides a natural way of extrapolating the tracks
between diﬀerent detectors. On the other hand, the Kalman ﬁlter needs an initial set of
“seeds” which has to be provided by a preceding pattern recognition step.

Neural Networks

The application of neural network methods is known to oﬀer good approximate solutions
to diﬀerent optimization problems, among them track ﬁnding problems in high-energy
physics [64]. The basic idea is to set up an energy function whose minimum corresponds
to the solution of the pattern recognition problem. The dynamical evolution of the
network should then always decrease the energy and rapidly converge to a solution. This
method has proven to give comparable results with other track ﬁnding approaches [65],
and is generally less time consuming. One major drawback, however, is absence of a track
model in the formalism, and the fact that it has to be supplemented with a separate track
ﬁtting procedure.

41

Track reconstruction in the TPC

In case of the TPC, the signals produced by the traversing particles correspond to two-
dimensional charge distributions, called clusters, Section 2.4.1. The centroid of these
distributions corresponds to the three-dimensional space points along the particle tra-
jectories. Pattern recognition in the TPC is often classiﬁed in two main categories; an
sequential and iterative approach.

Sequential pattern recognition

In the sequential case the pattern recognition is performed in two sequential steps. In
the ﬁrst step, the cluster centroids are calculated. A cluster ﬁnder searches the raw ADC
data for local maxima in the two dimensional charge distributions. If an isolated cluster
is found, the centroid position in pad and time is calculated. In the case of overlapping
clusters, the charge distributions have to be separated by unfolding procedures. However,
due to the missing information about the track parameters of the crossing tracks, neither
the shape nor the size of the clusters are known at this stage. Furthermore, the number of
tracks contributing to the distribution are unknown. Therefore, the distributions cannot
easily be unfolded, and the resulting centroids are error-prone and may result in a loss
of tracking performance. Together with the position of the pad-row plane, the centroids
provide the three dimensional coordinates which are interpreted as the particle crossing
point with the pad-row plane. In the second step, the list of space points serve as an
input for the track ﬁnder which connects the space points into track segments. The track
ﬁnding problem is typically solved by a local track follower or a Kalman ﬁlter approach.
This method is the conventional approach for track reconstruction and has been suc-
cessfully been used with TPCs in a relatively low occupancy environment. Examples are
the reconstruction programs for the NA49 [47] and the STAR [48] experiment. In the
ALICE experiment however, the detector occupancy in the TPC may well reach values
of up to 50% (Figure 4.1). In such a scenario the amount of overlapping clusters will be
very high, and it is generally expected that the performance of a simple cluster ﬁnder will
be low. A cluster model for the overlapping charge distributions is likely to improve the
results.

Iterative pattern recognition

In the iterative track ﬁnding approach, the procedures of cluster ﬁnding and track ﬁnding
are not separated into two sequential steps, but rather done in a more parallel or iterative
fashion. In this case parts of the track ﬁnding is solved prior to the cluster ﬁnding. In a
ﬁrst step “track seeds” are found for instance by using a combinatorial approach based on
cluster centroids located at larger radius (where the occupancy is low) or by applying track
ﬁnding algorithms like the Hough Transform directly on the raw ADC data. Based on the
list of found track segments, one returns to the raw-data and reconstruct the clusters along
the trajectories. Since the cluster model is a function of the crossing tracks and detector
speciﬁc parameters and the electronics, the track model provides an estimate of shape
and size of the clusters. Hence the charge distributions can be ﬁtted to a known shape.
Based on the additional knowledge of the tracks passing nearby and therefore possibly

42

contributing charge to the clusters, overlapping clusters can be properly deconvoluted.
This approach was used in the NA49 [47].

4.2 The ALICE tracking environment

The experimental setup of the ALICE experiment consists of the central barrel placed
inside the L3 magnet and the forward detectors (Section 2.3). The main tracking detector
is the TPC detector, which has a coverage of

< 0.9 and full azimuth.

η

|

|

4.2.1 Particle multiplicity and detector occupancy

The ALICE experiment is designed to handle a charged particle multiplicity corresponding
to 8000 charged particles per unit rapidity, dNch/dy=8000 [32]. This estimate is regarded
as an extreme assumption, and is based on a comparison between diﬀerent event generators
for Pb-Pb collisions at the LHC energy of √sN N =5.5 TeV. The predicted charged particle
multiplicity at the mid-rapidity, however, varies strongly, ranging from dNch/dy of about
1500 to 7-8000, depending on the generator used.

In order to study detector performance, it is more convenient to use pseudo-rapidity,
η, instead of rapidity since the pseudo-rapidity is a geometrical unit while the rapidity
depends on particle composition and pt-spectra.
It should however be noted that the
multiplicity per unit rapidity is 10-20% more at mid-rapidity than the multiplicity ex-
pressed in units of pseudo-rapidity [31]. This means that a charged particle density of
dNch/dη=8000 is 10-20% higher than the extreme assumption of dNch/dy=8000.

The ALICE Technical Proposal was written when the highest available nucleon-
nucleon center of mass energy results was √sN N =20 GeV (CERN SPS), i.e. a factor
of 300 less than the LHC energy. Since that time, new data from RHIC at energy up to
√sN N =200 GeV has become available, and the event generators have been correspond-
ingly updated.
In Table 4.1 the results from the most common generators, HIJING
(Heavy-Ion Jet INteraction Generator), DPMJET (Dual Parton Model) and the SFM
(String Fusion Model) are summarized. A detailed description of the processes included

Generator Comments
HIJING 1.36 with quenching

without quenching

DPMJET-II.5 with baryon stopping

SFM

without baryon stopping
with fusion
without fusion

dNch/dη at η=0 (approx.)
6200
2900
2300
2000
2700
3100

Table 4.1: Charged particle multiplicity simulated by diﬀerent event generators [31].

and parameters used for each of these event generators can be found in [31] and refer-
ences therein. Results show that large diﬀerences for dNch/dη still exists, and even in the
framework of a single generator the density is strongly model dependent. Furthermore,

43

none of the current generators reproduce the multiplicities obtained from the theoretical
extrapolations from RHIC described in Section 1.4 on page 9, which indicates values of
about 2000-3000. Because of the large uncertainty in the particle multiplicity and the
30), the value of up to
fact that extrapolation from RHIC results are still quite large (
8000 is still being considered as the design value by the collaboration in order to provide
a absolute safety margin with respect to detector and software design. From a pattern
recognition point of view, tracking algorithms should be developed that are capable of
handling all possible scenarios.

∼

8000
8000
8000
8000
8000
6000
6000
6000
6000
6000
4000
4000
4000
4000
4000
2000
2000
2000
2000
2000
1000
1000
1000
1000
1000

chdN
hd

]

%

[
 
y
c
n
a
p
u
c
c
O

60

50

40

30

20

10

0
0
0

20
20

40
40

60
60

80
80

100
100

120
120

140
140

Padrow
Padrow

Figure 4.1: Simulated occupancy in the TPC as a function of pad-row number. The
occupancy is calculated as the ratio between signals above threshold and the total number
of digits. The steps are due to change in pad-sizes (Table 2.1).

In Figure 4.1 the simulated detector occupancy for the TPC is shown as a func-
tion of radius (represented by the pad-row number) for diﬀerent multiplicities. For
dNch/dη=8000 it reaches
50% for the innermost padrows, and drops to less 20% for
the outermost.

∼

4.2.2 Magnetic ﬁeld settings

The central detectors are placed inside the L3 magnet which provides a solenoidal mag-
netic ﬁeld. In general, the ﬁeld strength selection in a heavy ion experiment is a compro-
mise between low momentum acceptance, momentum resolution and tracking and trigger
eﬃciency. In the ALICE experiment, the desired low momentum cut-oﬀ is determined
by the ability to probe the soft hadronic regime, i.e. collective eﬀects, detection of decay
products of low pt hyperons, and identical particle interferometry. In the high momentum
regime, the resolution has to be suﬃcient to measure the leading particles of high energy
jets, and to measure the decay products of heavy quarkonium. The ideal ﬁeld strength
for hadronic physics, maximizing the reconstruction eﬃciency, is around 0.2 T, while for
the high pt observables the maximum ﬁeld strength the L3 magnet is able to produce,

44

pt =

0.3B
κ
|

|

pz = pt tan λ
p2
t + p2
z

p =

q

0.4-0.5 T will most likely be the best choice [31]. Since the high pt observables are the
ones which are limited by statistics, ALICE will run mostly at the higher magnetic ﬁeld
setting.

4.2.3 Particle trajectory in a magnetic ﬁeld

A charged particle moving in a homogeneous magnetic ﬁeld follows a circular motion in
the plane perpendicular to the ﬁeld. This is commonly referred to as the bending plane of
the track. The momentum component along the magnetic ﬁeld is left unchanged. These
two components thus form a helix path in space. A description of the track model and
helix parameterization can be found in Appendix A. The assumption that a particle
follows a helical trajectory is exact when the magnetic ﬁeld is constant and one neglects
energy loss and multiple scattering. For a particle with unit charge, the relation between
the magnetic ﬁeld B [T], and the curvature of the track, κ [m−1] is given by

[GeV]

(4.1)

Once the pt of the particle has been determined, the remaining components of the particle
momentum can be calculated from

(4.2)

(4.3)

where λ is the dip-angle of the track.

The curvature and the dip-angle of a track segment is determined by ﬁtting the indi-
vidual space points along the trajectory. The accuracy of which the curvature and thus
the momentum can be measured is limited by the space point errors. The relation be-
tween the azimuthal position resolution of a single space point and the relative transverse
momentum error can be parameterized by [66]

∆pt
pt

=

σrφpt
0.3BL2 s

720
N + 4

where L is the total visible track length, N is the number of measured space points on
the track and σrφ is the error of the position measurement. For the ALICE TPC L and B
are given by the overall design parameters of the detector, while N and the space point
resolution are deﬁned by the design of the readout chambers (Section 2.4.2).

In addition to the track measurement errors, the obtained momentum resolution will
be limited by multiple scattering and energy loss. These eﬀects cause the particle trajec-
tory to deviate from a helix. Furthermore, the resolution will also be limited by detector
readout speciﬁc eﬀects such as diﬀusion and angular eﬀects etc. (Section 2.4.1, page 20),
and the noise imposed by the electronic readout chain.

4.3 The AliROOT framework

The complete ALICE detector setup is described and simulated within the AliROOT
framework [67], which is based on the ROOT software package [68]. The framework

45

Figure 4.2: Functionality overview of the AliROOT framework [31].

consists of a set of software tools which mimic the diﬀerent steps of data processing
required in the ALICE experiment. An overview of the functionality of the framework is
shown schematically in Figure 4.2. Data are generated via simulation programs, i.e. event
generators and detector response simulations, and are then transformed into the data
format produced by the detector. The data produced by the event generators contain the
full information about the generated particles, i.e. particle type and kinematics. These
data then serve as input to the detector simulation chain, in which the information is
disintegrated to that generated by particles when traversing the detector. The outcome
of the detector simulation is representative of real data measured in a real detector. This
data can thus be used as input for the various reconstruction and analysis chains, which
can be evaluated by comparing the ﬁnal reconstructed particles with the generated ones.
Each detector is described as an independent module that contains the code for both
simulation and reconstruction.

4.3.1 Event simulation

The theoretical uncertainty with respect to the outcome of heavy ion collisions at LHC
makes it necessary to use and compare several models for the simulation of the ﬁnal state
particles in an event. Consequently, a variety of diﬀerent event generators have been
implemented within the AliROOT framework. In order to evaluate the reconstruction
and analysis algorithms in AliROOT, a generator capable of simulating a typical “back-
ground” event for various multiplicities is needed. For this purpose a parameterization of
the HIJING generator, which is based on parameterized η-density and pt distributions of
charged and neutral pions and kaons was encoded. In this model, the η-distributions have
been obtained from a HIJING simulation of central Pb-Pb collisions, and the pt distribu-
tions of the pions are based on pt measurements at √s=1.8 TeV [69]. The corresponding
kaon pt distribution is obtained from the pion distribution by mt scaling. To simulate a
multiplicity of e.g. dNch/dη = 8000, the η-distribution is scaled in such a way that 8000
<0.5. Lower multiplicity
charged particles per event are produced within the range

η

|

|

46

events are scaled similarly. All the sample events used in this work have been produced
using this parameterization.

4.3.2 Simulation of detector response

Once the collision is simulated by an event generator, the ﬁnal state particles are fed to a
particle transport program. For simulating particle transport in AliROOT the GEANT3
package is currently being used. The particles are transported in the material of the
detector, simulating the interaction and the energy deposition that generates the detector
response. From the energy deposition, the “ideal” detector response resulting from the
traversing particles is generated. This response is subsequently digitized and formatted
according to the output of the detector front-end electronics. Thus, the ﬁnal results are
expected to closely resemble the real data produced by the ALICE detector system.

In the special case of the ALICE TPC detector, a dedicated microscopic simulator has
been implemented in AliROOT [70]. All major physical processes are incorporated in this
simulator, including parameterization of the ionization in the gas, generation of secondary
B eﬀect near the anode wires,
electrons, diﬀusion of electrons, electron attachment, E
and a complete pad and time response determined by the readout-chamber geometry and
electronics parameters.

×

Simulation of the physical processes in the TPC

The ionization in the gas is basically described by the generation of primary and secondary
electrons. The electromagnetic interactions of the initial particle with the TPC gas lead
to the release of the primary electrons. If these electrons have suﬃcient kinetic energy
they will further ionize the gas and produce secondary electrons, creating the electron
cluster. The mean distance, D, between two primary ionization’s can be expressed as

Here Nprim is the number of primary electrons per cm produced by a minimum ionizing
particle (MIP), and f (βλ) is the Bethe-Bloch function. The energy loss function is pa-
rameterized from a ﬁt to energy-loss data for 90% Ar, 10% CH4. The energy loss released
in the primary ionization to atomic electrons has, if one neglects the atomic shell struc-
ture, a close to 1/E2 dependence. For light gases the distribution has a slightly steeper
dependence, and in the simulation a 1/E2.2 parameterization is used.

The total number of secondary electrons, Ntot, created in a cluster is given by,

where Etot is the energy loss in a given collision, Wi is the eﬀective energy required
to produce an electron-ion pair and Ipot is the ﬁrst ionization potential. The simulated
clusters are initially point-like objects, and no distinction between primary and secondary
electrons are done.

D =

Nprim

1
f (βγ)

.

·

Ntot =

Etot

Ipot

−
Wi

+ 1,

47

(4.4)

(4.5)

The produced electrons drift through the gas with an constant eﬀective drift velocity.
During the drift, the electron cloud is subject to diﬀusion, which is described by a three-
dimensional Gaussian distribution,
x0)2

−
2δ2
#
L
(4.6)
where (x0, y0, z0) is the electron creation point, and the transversal, δT , and longitudinal,
δL, diﬀusion are given by the drift length, Ldrift, and gas coeﬃcients, DT and DL

1
√2πδL

1
√2πδT

1
√2πδT

P (x, y, z) =

−
2δ2
T

−
2δ2
T

y0)2

z0)2

exp

exp

exp

"−

"−

"−

(x

(y

(z

#·

#·

,

δT = DT

Ldrift

δL = DL

Ldrift

q

q

(4.7)

(4.8)

(4.9)

During the drift through the gas the electrons can be absorbed in the gas by the
formation of negative ions. This process has been simulated by assuming a probability of
electron capture of 1% per m drift per ppm of O2.

Near the anode wires, the magnetic ﬁeld and electric ﬁelds are no longer parallel,
and because of the Lorentz force the electrons experience a displacement along the wire
direction. If an electron enters the readout chamber at point (x0, y0), it is displaced in
the x-direction (assuming that the wires are placed along the x-axis). The new position
of the electron is given by,

where y is the coordinate of the wire on which an electron is collected, and ωτ is the
tangent of the Lorentz angle.

x = x0 + ωτ (y

y0),

−

Simulation of the signal generation

When the electron cluster enters the readout chambers, the electrons are accelerated in
an increasing electric ﬁeld towards the anode wires. Once the electric ﬁeld is strong
enough, an avalanche is created. The amplitude of this avalanche is determined by the
high voltage applied to the wire, and is subject to ﬂuctuations. The resulting number of
electrons created can be described by an exponential probability distribution function,

P (q) =

exp

1
¯q ·

q
¯q !

,

 −

where ¯q is the mean avalanche amplitude.

An electron which is collected on the anode wire leaves an ion behind which induces
a charge on the pad plane. This charge is integrated over the pad area, and the induced
charge distribution in the pad plane is determined by the Pad Response Function (PRF).
In the simulation, a two-dimensional PRF is computed for the pad-geometries planned
for the ALICE TPC. This calculation also incorporates possible signal measurements
from the neighboring wires (“crosstalk”). The time signal is obtained by convolving the
avalanche with the shaping function of the preampliﬁer/shaper, and sampled at a given
frequency. The generated signal is then superimposed with a random electronic noise.
This noise is described by a Gaussian with RMS equal to 1000 e. The simulated signal
is ﬁnally digitized using the predeﬁned dynamic range of the electronics and by applying
zero suppression.

48

4.3.3 The Oﬄine reconstruction chain

The ALICE Oﬄine reconstruction algorithm is based on the Kalman ﬁltering ap-
proach [63]. The reconstruction chain starts with a cluster ﬁnder in the TPC, which
provides the space points that are used for tracking with the Kalman ﬁlter. The overall
tracking then starts with track seeding in the outermost pad-rows of the TPC. It begins
with a search for all pairs of points which are projecting to the primary vertex. Diﬀerent
combinations of the pad-rows are used with and without a primary vertex constraint, in
order to obtain both primary and secondary tracks. When a reasonable pair of points is
found, parameters of a helix going through these points and the vertex are calculated.
These parameters and the corresponding covariance matrix are then taken as the initial
track candidate which are propagated from the outermost point to the inner point using
the Kalman ﬁlter. If at least half of the possible points between the initial ones were
successfully associated with the current track candidate, it is stored as a valid seed and
the search continues. Each seed is then propagated through the entire TPC.

One of the main shortcomings using the Kalman ﬁlter in ALICE is that it depends
on good seeds to start a stable ﬁltering procedure. In the present approach this is very
computationally demanding, as the search is done in a straightforward combinatorial way.
Another drawback is that the clusters have to be reconstructed prior to the track ﬁnding,
which is a diﬃcult task in regions with high occupancy and cluster overlapping. However,
recent developments deal with this problem utilizing an iterative tracking approach, i.e.
by performing cluster ﬁnding in parallel to track ﬁnding [71]. In this way, information
about the tracks which produce the clusters is available, and deconvolution is more easily
performed.

The pattern recognition algorithms implemented in this work are primarily developed for
the ALICE High Level Trigger system. In this context there are two main considerations:

4.4 Premises

Computing requirements.

•

•

Parallelization.

The computing requirements deﬁne the processing power in terms of processing time
and memory requirements needed to reconstruct a complete event. From the High Level
Trigger point of view, the time available for this task is constrained by the event rate.
The amount of processing power needed for the full system will critically depend on the
processing requirements for the individual processing modules. The algorithms should
therefore be optimized with respect to both eﬃcient data organization and the minimiza-
tion of computing intensive tasks. The latter consideration is related to the fact that
the readout of the detector is done in parallel, Section 3.3. This deﬁnes the ﬁrst level
of the HLT architecture, in which the ﬁrst part of the pattern recognition is performed.
Furthermore, merging the data from diﬀerent parts of the detector implies copying data
over network, and should therefore be avoided as far as possible. Eﬀort has therefore
been made to implement the processing schemes in a highly modular fashion, allowing
for a high degree of parallelization.

49

As part of this work, two diﬀerent TPC pattern recognition approaches were imple-
mented and evaluated on simulated ALICE TPC data. They are in the following referred
to as sequential tracking and iterative tracking, reﬂecting the two main TPC track recon-
struction methods introduced on page 42.

4.5 Sequential tracking

The implemented sequential tracking scheme consists of 4 main successive processing
modules, referred to as Cluster Finder, Track Finder, Track Fitter and Track Merger.
The algorithms used for the Cluster Finder and the Track Finder are based on the re-
construction scheme implemented and used in the STAR L3 trigger [72]. All algorithms,
and their implementation issues are described in the following.

4.5.1 The Cluster Finder

The cluster ﬁnder algorithm is an implementation of a straightforward sequence matching
technique. The main emphasis of the algorithm is to optimize the program with respect
to CPU time and memory access. A FPGA implementation of the algorithm has been
implemented [73] for the purpose of utilizing the co-processor functionality planned for
the Front-End Processors.

The basic functionality of the algorithm is to group sequences in the pad-row-
plane which belong to the the same cluster and calculate the two-dimensional centroid,
(λpad, λtime), by a weighted mean,

λpad =

λtime =

(4.10)

i qipi
i qi

P

P

i qiti
i qi

.

P

P

Here qi is the ADC-value of a given (pi, ti) (pad,time) bin. A ﬂow diagram of the algorithm
is shown in Figure 4.3. The input to the cluster ﬁnder is a list of sequences for each
pad. For every new sequence the centroid position in the time direction is calculated.
This temporary mean is then compared to the sequences in the previous processed pads.
During this step there are two possible outcomes:

– A match is found. This means that there is another sequence on the neighboring
pad that overlaps with the current sequence. The two sequences are then merged
and the mean in both pad and time are calculated.

– No match was found. In this case, the sequence is regarded as the start of a new

cluster.

The essential feature of the algorithm is that it enables the program to handle all
sequences in two distinct lists: The current pad list and the previous pad(s) list. This
also allows the loop over the data only once, while performing all the calculations on-the-
ﬂy. Consequently, it is assumed that the input data stream is ordered in pad and time,
i.e. all sequences on a single pad are received before the successive pad. Thus, the current
pad list contains information about the sequences on the pad which are currently being

50

Sequence loop

Calculate mean

Compare mean with
previous pad

No match

Match

Start new cluster

Merge sequences

Calculate mean in
pad and time

Update cluster lists

Figure 4.3: Flow diagram of the HLT Cluster Finder algorithm.

processed, while the previous pad(s) list contains the clusters on the previous pad(s). A
cluster is considered complete once there are no matching sequences on the current pad.
In the case of overlapping clusters, a simple cluster deconvolution scheme is applied. A
check is performed on each sequence whether there exists a local minimum in the charge
values. If such a minimum exists, the sequence is separated into two sequences by splitting
at the time-bin containing the minimum charge. Following the same procedure in the pad
direction, a check is done whether there is a local minimum in the pad charge for the
cluster under construction. If this is the case, the matching cluster on the previous pad is
not merged, but considered complete, while the sequence on the current pad is considered
as a new cluster.

In addition to the cluster centroids, also the shape of the cluster is needed as additional
information for the estimation of the space point errors, see next section. The shape can
be represented by the widths of the cluster in the two dimensions, and is thus calculated
as the RMS of the distributions.

In Table 4.2 the space point resolution obtained with the algorithm is listed for isolated
clusters. The estimates are obtained by comparing the reconstructed centroids with pad-
row plane crossings of the the simulated particle trajectories. The results are averaged
< 0.9).
over all primary tracks with pt ≥
The values correspond to the standard deviation obtained by performing a Gaussian ﬁt
of the distributions. For reason of comparison, also the results from the Oﬄine cluster
ﬁnder algorithm are shown. The resolutions are in the order of 0.8-1 mm and 1-1.4 mm
for the pad and time direction, respectively. The results for the Oﬄine cluster ﬁnder are

0.1 GeV which cross the entire TPC volume (
|

η

|

51

slightly worse than the HLT cluster ﬁnder, which may be due to a wrong splitting of a
few but large pathological clusters.

Pad direction [mm] Time direction [mm]
HLT
0.99
0.88

Oﬄine
1.50
1.23

Oﬄine
1.10
0.96

HLT
1.33
1.14

Inner chambers
Outer chambers

Table 4.2: Space point resolution obtained using the HLT Cluster Finder on isolated
clusters. For comparison, the results from the Oﬄine cluster ﬁnder algorithm are also
listed.

Space point errors

In addition to the three dimensional space point coordinates of the clusters, an estimate
of the errors of the space points is needed for the track ﬁnding procedure. Similar to the
cluster widths, the errors are determined by the diﬀusion and angular spread of the drifting
electron cloud, and are thus dependent of detector speciﬁc parameters and the track
parameters, Section 2.4.1. However, since the cluster ﬁnder does not have any information
about the tracks, the errors are assumed to be proportional to the calculated RMS-values
of the clusters. The coeﬃcients have been found by comparing the reconstructed space
points with the pad-row crossing points of the simulated tracks, and parameterizing the
errors as a function of the cluster widths for both pad and time direction.

4.5.2 The Track Finder

The track ﬁnding algorithm is a local track follower algorithm. Its main feature is that
it incorporates a transformation on the space points commonly referred to as conformal
mapping. The purpose of this transform is to describe the circular motion of the parti-
cle trajectory in the bending plane by a linear parameterization. Since ﬁtting straight
lines is signiﬁcantly less computationally demanding than ﬁtting circles, the eﬀect of the
transform is to minimize the amount of calculations needed for the ﬁtting procedures of
the track ﬁnding algorithm.

Conformal mapping

The purpose of the conformal mapping is to transform the points on a circular pattern
into a space where it can be described using a linear relation. Denoting the conformal
space by (x′, y′), a space point (x, y) transforms in according to,

x′ =

x

xt

y′ =

−
r2 = (x

yt

−
r2
xt)2 + (y

−
r2
y

−

52

yt)2

−

(4.11)

where the point (xt,yt) is a ﬁxed point on the circle. In order to derive the linear rela-
tionship between points in conformal space, consider two points (x, y) and (xt, yt) on a
circle with center (xc, yc) and radius rc. For both points the following relations are then
valid,

r2
c = (x
−
r2
c = (xt −

xc)2 + (y
−
xc)2 + (yt −

yc)2
yc)2,

and therefore,

Inserting the relations,

x2 + y2 = 2xc(x

xt) + 2yc(y

yt) + x2

t + y2
t .

−

−

and the reverse transformation from Equation 4.11,

x2 = (x
y2 = (y

x2
xt)2 + 2xxt −
t
y2
yt)2 + 2yyt −
t

−
−

x = x′r2 + xt
y =

y′r2 + yt

−

results in,

(x

xt)2 + (y

yt)2 = 2r2[x′(xc −

xt)

y′(y0

yt)],

−

−
−
and together with the deﬁnition of r2 in Equation 4.11 this gives
xc −
yc −

1
2(yc −
Equation 4.12 thus provides a linear parameterization of the circle in conformal space,
Figure 4.4.

= ay′x′ + by′.

y′(x′) =

(4.12)

xt
yt

yt)

x′

−

−

The transformation needs a ﬁxed point, (xt, yt), on the circle. In the case of track ﬁnd-
ing, this point can be replaced by the collision vertex coordinates if the track is assumed
to originate from a primary particle. Alternatively, one can use the ﬁrst point associated
on the track. In that case, one assumes that the track originate from a secondary vertex.

The track follower

The track ﬁnding procedure is based on a follow-your-nose method initially proposed
in [74]. The algorithm consists of building track segments by combining space points
which are co-linear in the conformal space.

The ﬁrst step of the algorithm consists of data organization. An essential part of the
algorithm is to organize the space points in a eﬃcient manner, so that the track ﬁnder has
fast access to all the space points when building the track segments. This is done by as-
signing the space points to special sub-volumes in (r, φ, η), where r, φ and η correspond to
pad-row number, polar angle and pseudo-rapidity, respectively. A sub-volume is denoted
using the corresponding indexes (ir, iφ, iη). Every space point is uniquely associated with

53

250

Y

200

150

100

50

0

-50

-100

-150

-200

-250

0.015

’

Y

0.01

0.005

0

-0.005

-0.01

-0.015

-250 -200 -150 -100 -50

0

50 100 150 200 250

-0.015

-0.01

-0.005

0

0.005

0.01

X

0.015
’X

Figure 4.4: Conformal mapping of space points lying on the circular track segments. Each
point in xy-space (left) is transformed into conformal space (right) using the vertex as a
ﬁxed point on the circle.

one sub-volume, and one sub-volume can of course contain many space points. The sub-
volumes merely acts as containers for the space points, which in the implemented program
correspond to a list of pointers to the respective space points structures in memory.

After transforming the space points into conformal space and organizing them into
sub-volumes, the actual track ﬁnding starts. The tracks are being initiated by building
track segments, and the search is starting at the outermost pad-rows. A space point
is chosen as the starting point, S, in which the corresponding sub-volume is denoted
(iS
η ). During the search, only space points which are in the nearby sub-volumes are
considered. More speciﬁc, this corresponds to the sub-volumes satisfying the conditions:

φ, iS

r , iS

jr ≤
jφ ≤
jη ≤
where N is a tunable parameter of the program. In this context, the distance, d, between
two points is deﬁned as

iS
1
r −
iS
φ + 1
iS
η + 1

iS
r −
iS
φ −
iS
η −

(4.13)

≤
≤

N
1

≤

1

d =

iS
r −

|

ij
r| ×

(

φS

|

φj

+

ηS

−

|

|

ηj

).

|

−

(4.14)

The space points which are closest together are linked, and the search is continued until
a certain number of space points is reached.

Once the track segments are obtained, the corresponding points are ﬁtted to straight
lines in conformal space and in the (s, z) plane. The tracks are then build by associating
new space points to the track which are close to the ﬁt. The search continues from the
outer pad-rows towards the inner pad-rows. When searching for a new space point, the
nearby sub-volumes according to Equation 4.13 are being searched. For every step there
are three possibilities:

54

Organize space
points in
sub−volumes

Space point loop

Space point used

Space point unused

#space points too few

#space points sufficient

Build track candidate

Choose space points
closest in           space

Extend track
Choose space point
closest to track fit

Calculate track
parameters

Acceptable

2

Unacceptable

2

Last layer
not reached

Last layer 
reached

#space points sufficient

#space points too few

Store track

Delete track candiate

Next space point

Figure 4.5: Flow diagram of the Track Follower algorithm.

– There are no points in the sub-volumes.

If the track already has enough space points assigned, the track is considered a
ﬁnished track, and the assigned points are removed from the available space point
list. If the track does not have enough points assigned, the track is rejected and the
space points are kept in the event.

– There is one space point in the sub-volumes.

If the space point gives a updated track ﬁt with acceptable χ2, the point is included
and the track parameters are correspondingly updated. Otherwise the track is
considered a ﬁnal track if the number of already assigned space points is suﬃcient.
Otherwise the track candidate is deleted and the space points made available for
further tracking.

– There are several space points in the sub-volumes.

In this case the program can either search through all the space point candidates
and choose the one which gives the best χ2 of the track ﬁt, or it can search through
the list until a space point which gives good enough ﬁt is found.

A ﬂow diagram of the algorithm is illustrated in Figure 4.5.

The performance of the algorithm on a given event sample is governed by the values
set for the various tracking parameters. These typically include the granularity of the

55

f
-
h
c
c
sub-volumes, maximum allowed χ2 during the track building and minimum number of
hits on a valid track.

Non-vertex tracking

The Track Finder can perform two tracking passes, where the second pass takes the
unused space points from the ﬁrst pass as an input. In this case, the Track Finder does
not apply any vertex constraint when building the tracks, and the conformal mapping is
done relative to the ﬁrst point associated with the track.

Due to the fact that most of the interesting tracks are in fact originating from the area
of the main vertex, this option is in general not applied. Also, secondary tracks, which
have a small impact parameter, can still be detected by the Track Finder if a suﬃciently
high uncertainty is assigned to the location of the primary vertex.

4.5.3 The Track Fitter

Once the Track Finder has recognized the tracks by grouping the space points into subsets,
a subsequent ﬁt of the subsets in space is done in order to obtain the best estimate of
the respective track parameters. The track ﬁtting procedure assumes that the particles
follows helical trajectories. This assumption is valid if the magnetic ﬁeld is suﬃciently
uniform and energy losses are small. The helical motion can be decomposed into a circle
projected on the bending plane and a straight line projected on the non-bending plane
(Appendix A.2). In order to optimize the ﬁtting procedure the track helix parameters
are evaluated in two independent procedures in which the space points are ﬁrst ﬁtted to
a circle in the bending plane and then to straight lines in the non-bending plane.

The circles in the transverse plane are ﬁtted using an algorithm presented in [75]. This
ﬁtting procedure is less computationally expensive than normal χ2 minimization ﬁtting
routines, while at the same time preserving its stability and accuracy. The longitudinal
part of the tracks are ﬁtted to a straight line in the (s, z)-space, which denotes the path
along the helix and the coordinate along the beam-direction, respectively. The linear ﬁt
is a simple least squares ﬁt to a line of the form y = ax + b using linear regression.

4.5.4 The Track Merger

The purpose of the Track Merger is to merge multiple tracks segments belonging to
the same particle trajectory. This is necessary when a track is being reconstructed in
multiple parts corresponding to the diﬀerent sub-detectors. For the TPC this is the case
if track ﬁnding is performed independently in diﬀerent sectors, that later are merged with
neighboring sectors.

The Track Merger is based on a simple matching method of track segments which
might belong to the same trajectory. The input to the Track Merger is thus all the
reconstructed track segments within each individual sector. The tracks are organized
into individual lists corresponding to which sector they are in. In a ﬁrst step, all track
segments within a list are checked to determine if they cross the corresponding sector
boundaries. This is done by taking the intersection of the projected helix in the transverse
plane and the straight lines representing the sector boundaries. Only the tracks for which

56

this is true, are further processed by the Track Merger. The parameters of each track
are calculated at a plane perpendicular to the sector boundary, and in the middle of the
TPC cylinder. Pair of tracks within two neighboring sectors are compared by taking
the diﬀerence between their respective track parameters at this plane. If the diﬀerence
between the parameters are less than a predeﬁned threshold value, the two tracks are
merged into one track, and the two track segments are subsequently removed from the
lists. The track parameters which are evaluated are the two-dimensional crossing point
with the plane, the azimuthal angle of the track momentum, the curvature and the dip
angle.

4.5.5 Data ﬂow

The complete track reconstruction chain described above, has been implemented in a
generic way. This allows for a high degree of parallelization of the diﬀerent processing
components. The ﬁnal processing scheme will be adapted to both the inherit data ﬂow
coming from the detectors into the HLT system, Section 3.3, and the required data
ﬂow from the individual processing components. The potential decomposition of each
processing module depends on the inherit locality of the algorithm with respect to required
input data.

Cluster Finder The Cluster Finder solves a local two-dimensional problem, and does
not need information from more than one pad-row at the same time. Thus the algorithm
could be broken into a number of parallel components corresponding to the total number
of pad-rows in the TPC. This allows the algorithm to be implemented at a very early
stage in the readout scheme.

Track Finder Since the tracks have to be build using space points from diﬀerent
layers, this processing step needs a certain number of successive pad-rows. In principal
only ﬁve points are needed to ﬁt a track to a helix, however, the resolution of the obtained
track parameters depends on the track length and the number of assigned space points.
Also, when dividing the track ﬁnding problem into smaller regions of the detector, the
track segments have to be merged across the boundaries of these regions which again
will introduce an extra processing step. Such an additional track merging step will also
prevent some split tracks from merging, and consequently lead to a certain loss of tracking
eﬃciency. One therefore has to weight the trade-oﬀ between the gain of reduced processing
time with the complexity of the hardware topology and the possible loss of tracking
performance.

Final track ﬁtting The ﬁnal tracks can only be obtained when the information from
the tracking detectors has been collected. In the case of TPC tracking, this corresponds
to all the pad-rows in which the tracks have produced a signal. Hence, the ﬁnal track
ﬁtting of the TPC tracks can only be done when the track and space point information
has been collected from all the TPC sectors, and the track segments have been merged
across the detector boundaries.

Figure 4.6 shows a possible data ﬂow scenario of the sequential tracking approach.
Given the inherit parallelism of the Cluster Finder a natural choice will be to run it
on the Front-End Processors (FEP) of the HLT system, where a copy of the raw-data
is received in a massive parallel fashion. Hence there are six Cluster Finder processes
running in parallel for each TPC sector, each processing the data from one sub-sector.

57

36 TPC Sector

1 TPC Sector

Cluster
Finder

Cluster
Finder

Cluster
Finder

Cluster
Finder

Cluster
Finder

Cluster
Finder

Track Finder

Track Fitter

Track Merger
& Fitter

Sector level

Global level

Figure 4.6: Data ﬂow for the sequential track reconstruction chain.

The output are then collected at the TPC sector level, where the Track Finder takes as
input the list of space points from a complete TPC sector. Once the Track Finder has
found all track segments, they are passed to the Track Fitter which performs a ﬁt of
the tracks to obtain the helix parameters. Finally, the tracks from all 36 TPC sectors
are collected at a global layer, where they are merged across the sector boundaries. In
addition, a ﬁnal track ﬁt is performed on the merged tracks in order to get the best
estimate of the track parameters.

In this way the processing topology follows a tree-like structure, where the output
from one processing level is being merged at a higher level until all tracks are globally
reconstructed. Such a scheme is consistent with a parallel solution implementing a generic
PC cluster of individual nodes, whose number can be adjusted according to the computing
requirements of the individual processing components.

4.5.6 Performance

The implemented sequential reconstruction chain has been evaluated for various particle
multiplicities. The event samples were generated using the HIJING parameterization,
Section 4.3.1, and four diﬀerent multiplicities corresponding to dNch/dη of 1000, 2000,
4000 and 6000. The magnetic ﬁeld settings correspond to standard solenoidal ﬁeld map
of 0.2 T and 0.4 T. All results have been obtained using a data ﬂow scheme as illustrated
in Figure 4.6. For each event sample the results are compared to the results obtained
by the Oﬄine TPC reconstruction chain (Section 4.3.3). In the following the results are
referred to as HLT and Oﬄine respectively. Unless otherwise stated, the standard Oﬄine
algorithm has been used.

58

Deﬁnitions

In order to determine the performance of the track reconstruction chain, certain deﬁnitions
are required. In principle there are no “global” detector independent deﬁnitions to be
used for this purpose. Instead, deﬁnitions which are agreed upon within the collaboration,
are applied. In particular, one has to deﬁne the quantities which enter the equations:

Eﬃciency =

Contamination =

Number of f ound good tracks
Number of generated good tracks
Number of f ound contaminated tracks
Number of generated good tracks

(4.15)

A generated good track refers to a generated particle whose trajectory should be found
by the reconstruction program.
In general this means that the particle trajectory is
contained within the detector acceptance, and simultaneously produces suﬃcient amount
of signals when traversing the detector. For instance, a track which traverse through a
dead-zone of the TPC detector might not produce enough clusters on the pad-rows to be
reconstructed, and thus should not be counted as a generated good track. Furthermore,
a found good track is a track which has been correctly reconstructed from the simulated
raw-data. This usually means that a certain number of space points were assigned to the
track, and that most of these space points were reconstructed from clusters produced by
the generated good track. Similarly, a found contaminated track refers to a track which
had enough assigned space points, but with too many of them wrongly assigned.

In this work the deﬁnitions from the Oﬄine reconstruction framework [35] are used:

•

•

Generated good track – A track which crosses at least 40% of all pad-rows.

Found good track – A track for which the number of assigned space points is at least
40% of the total number of pad-rows. In addition the tracks should have no more
than 10% wrongly assigned space points, and half of the innermost 10% of the space
points must be assigned correctly.

•

Found contaminated track – A track which has suﬃcient number of space points
assigned (

40%), but more than 10% wrongly assigned clusters.

≥

If not otherwise stated only primary tracks were considered in the evaluations.

Tracking eﬃciency

The obtained tracking eﬃciencies as a function of pt are shown in Figure 4.7 for the dif-
ferent event samples. Figure 4.8 shows the corresponding contamination. For a magnetic
ﬁeld strength of 0.2 T the eﬃciency shows no signiﬁcant dependence on the transverse
momentum at lower multiplicities. At dNch/dη = 6000, however, a decrease of the low
momentum eﬃciency is observed. This decrease is not seen in the Oﬄine eﬃciency. For
a ﬁeld strength of 0.4 T, a decrease is seen for the low pt component for both HLT and
Oﬄine for all multiplicities. However, the tendency is more signiﬁcant for the HLT re-
sults when going towards higher multiplicities. Up to dNch/dη = 2000, HLT and Oﬄine
results are similar, while for dNch/dη = 4000 and 6000 the low pt eﬃciency for HLT is
signiﬁcantly lower than corresponding Oﬄine results.

59

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e

i

c

 

i
f
f
e
g
n
k
c
a
r
T

i

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

chdN
hd

=1000,   L3 Field: 0.2T

chdN
hd

=1000,   L3 Field: 0.4T

HLT

Offline

HLT

Offline

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

chdN
hd

=2000,   L3 Field: 0.2T

chdN
hd

=2000,   L3 Field: 0.4T

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

chdN
hd

=4000,   L3 Field: 0.2T

chdN
hd

=4000,   L3 Field: 0.4T

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

chdN
hd

=6000,   L3 Field: 0.2T

chdN
hd

=6000,   L3 Field: 0.4T

3
3
 [GeV]
 [GeV]

tp
tp

HLT

Offline

3
3
 [GeV]
 [GeV]

tp
tp

HLT

Offline

3
3
 [GeV]
 [GeV]

tp
tp

HLT

Offline

3
3
 [GeV]
 [GeV]

tp
tp

HLT

Offline

3
3
 [GeV]
 [GeV]

tp
tp

HLT

Offline

3
3
 [GeV]
 [GeV]

tp
tp

HLT

Offline

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1.4

1.2

1

0.8

0.6

0.4

0.2

0

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e

i

c

 

i
f
f
e
g
n
k
c
a
r
T

i

60

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3
 [GeV]
 [GeV]

tp
tp

3
3
 [GeV]
 [GeV]

tp
tp

Figure 4.7: Tracking eﬃciencies as a function of pt. Results using magnetic ﬁeld strength
of 0.2 T (left) and 0.4 T (right).

i

n
o
i
t
a
n
m
a
t
n
o
C

i

n
o
i
t
a
n
m
a
t
n
o
C

i

n
o
i
t
a
n
m
a
t
n
o
C

i

n
o
i
t
a
n
m
a
t
n
o
C

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

dN
dy

=1000,   L3 Field: 0.2T

dN
dy

=1000,   L3 Field: 0.4T

HLT

Offline

HLT

Offline

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

dN
dy

=2000,   L3 Field: 0.2T

dN
dy

=2000,   L3 Field: 0.4T

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

dN
dy

=4000,   L3 Field: 0.2T

dN
dy

=4000,   L3 Field: 0.4T

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

dN
dy

=6000,   L3 Field: 0.2T

dN
dy

=6000,   L3 Field: 0.4T

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

HLT

Offline

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

HLT

Offline

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

HLT

Offline

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

HLT

Offline

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

HLT

Offline

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

HLT

Offline

i

n
o
i
t
a
n
m
a
t
n
o
C

i

n
o
i
t
a
n
m
a
t
n
o
C

i

n
o
i
t
a
n
m
a
t
n
o
C

i

n
o
i
t
a
n
m
a
t
n
o
C

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

61

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

1.8
1.8
TP
TP

2
2
 [GeV/c]
 [GeV/c]

Figure 4.8: Contamination as a function of pt. Results using magnetic ﬁeld strength of
0.2 T (left) and 0.4 T (right).

y
c
n
e
i
c
i
f
f
e
 
l
a
r
g
e
t
n

I

i

n
o
i
t
a
n
m
a
t
n
o
C

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

Figure 4.9 shows the resulting integral eﬃciency and contamination as a function of
multiplicity for both magnetic ﬁeld strengths. An obvious similarity is seen in the results
obtained with the two tracking approaches for multiplicities
2000 for both ﬁeld settings.
For higher multiplicity, the eﬃciency of HLT algorithms is partially lost to contaminated
tracks whose relative amount reaches 4-5% for dNch/dη = 4000-6000.

≤

L3 Field: 0.2T

L3 Field: 0.4T

HLT

Offline

HLT

Offline

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

L3 Field: 0.2T

L3 Field: 0.4T

chdN
chdN
hd
hd

HLT

Offline

chdN
chdN
hd
hd

HLT

Offline

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

chdN
chdN
hd
hd

chdN
chdN
hd
hd

Figure 4.9: Integral eﬃciency and contamination as a function of multiplicity.

Position resolution

The position resolution can be estimated by calculating the distance, δ, from each space
point on a track to the respective trajectory deﬁned by the track ﬁt. The width of
the resulting distribution of these residuals can be taken as a measure of the position
resolution of the tracks. In Figure 4.10 such a distribution is shown for the transverse
and longitudinal direction for a multiplicity of dNch/dη = 1000. Only tracks with small
5◦) are taken into account for the longitudinal residuals, while all tracks
λ
dip-angle (
|
with pt ≥
0.1 GeV are included for the transverse residuals. The distributions include
both inner and outer sectors of the TPC, and is thus averaged over the three diﬀerent
pad-geometries. The resolution represented by the RMS-values of the distributions is
about 1.37 mm and 1.47 mm for the pad and time direction, respectively. In Figure 4.11
the obtained position resolutions are plotted as a function of multiplicity. Here also the
corresponding results from Oﬄine is shown. The results shows a clear deterioration of

| ≤

y
c
n
e
i
c
i
f
f
e
 
l
a
r
g
e
t
n

I

i

n
o
i
t
a
n
m
a
t
n
o
C

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

62

the resolution in both directions for increasing multiplicities. Similar eﬀect is seen for
Oﬄine, although in this case the dependency on multiplicity is less signiﬁcant.

Entries 
Entries 

 1119200
 1119200

Mean    3.092e-05
Mean    3.092e-05

RMS   
RMS   

 0.1374
 0.1374

10000

s
t
n
u
o
C

8000

Entries 
Entries 

 116697
 116697

Mean  
Mean  

 -0.0006726
 -0.0006726

RMS   
RMS   

 0.1471
 0.1471

-0.8 -0.6 -0.4 -0.2

-0

0.2 0.4

0.6

0.8

1

-0.8 -0.6 -0.4 -0.2

-0

0.2 0.4

0.6

0.8

1

 [cm]
T

 [cm]

L

Figure 4.10: Distribution of transverse (left) and longitudinal (right) residuals for
dNch/dη=1000. The distributions includes clusters from both the inner and outer TPC
readout chambers.

6000

4000

2000

0
-1

 (HLT)
T

 (HLT)

L

 (Offline)
T

 (Offline)

L

1

x10

10000

s
t
n
u
o
C

8000

6000

4000

2000

0
-1

]

m
c
[
 

S
M
R

0.3

0.28

0.26

0.24

0.22

0.2

0.18

0.16

0.14

0.12

0.1

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

chdN
chdN
hd
hd

Figure 4.11: Residuals as a function of multiplicity. The results are obtained from the
RMS-value of the respective distributions.

Momentum resolution

The momentum resolution of the reconstructed tracks can be estimated by comparing the
reconstructed momentum with the momentum of the corresponding simulated particle.
The resolution is a function of pt, Equation 4.3, and the relative pt resolution is deﬁned

63

d
d
d
d
d
d
s
t
n
u
o
C

120

100

chdN
hd

=1000

L3 Field: 0.4T

tp

0.4GeV

Entries 
Entries 

Mean  
Mean  

RMS   
RMS   

2c
2c

 / ndf 
 / ndf 

 1616
 1616

 0.3133
 0.3133

  1.399
  1.399

 94.91 / 58
 94.91 / 58

Constant 
Constant 

 114.9 
 114.9 

 4.108
 4.108

Mean     
Mean     

 0.3354 
 0.3354 

 0.02707
 0.02707

Sigma    
Sigma    

 1.054 
 1.054 

 0.02621
 0.02621

80

60

40

20

0
-10

-8

-6

-4

-2

0

2

4

8
6
 / pt pD

t

10

 [%]

Figure 4.12: Relative transverse momentum resolution for tracks with pt ≈
0.4 GeV. The
distribution is ﬁtted with a Gauss function, that gives a relative width of 1.05%. The
distribution is not completely symmetrical, because the measured momentum is inversely
proportional to the curvature of the track.

as,

∆rpt =

∆pt
pt

=

pt,measured

pt,particle

−
pt,particle

,

(4.16)

where pt,measured is the reconstructed transverse momentum of the track, and pt,particle
is
the transverse momentum of the corresponding simulated particle. In order to measure
this quantity for a given sample of tracks, a Gauss-ﬁt of the distribution is commonly per-
formed and the resulting width is taken as a measure of ∆rpt, see example in Figure 4.12.
The results for the diﬀerent event samples are shown in Figure 4.13 as a function of pt.
For the low multiplicity regime the relative resolution is contained within the interval
1.5-2.5%, with an average of 1.8% for magnetic ﬁeld of 0.2 T over the entire pt range. For
a magnetic ﬁeld of 0.4 T the resolution is centered around 1.2% for the same multiplicity.
The diﬀerence between HLT and Oﬄine results in this regime is small. For higher mul-
tiplicities the diﬀerence between HLT and Oﬄine becomes more signiﬁcant, in particular
in the higher pt range.

Remarks on the observed tracking performance

In summary, the results from these simulations indicates a signiﬁcant loss of tracking
performance in the low momentum regime for higher multiplicities. These eﬃciencies
may be explained by several factors. The main reason for the eﬃciency loss seen for
the HLT reconstruction chain compared to the Oﬄine results is the quality of the recon-
structed clusters. The Cluster Finder algorithm follows a rather simple sequence matching
scheme, and implements a straight-forward center-of-gravity approach to obtain the clus-

64

–
–
–
–
–
–
»
]

%

[
 

p

 
/
 

p

 

t

t

]

%

[
 

p

 
/
 

p

 

t

t

]

%

[
 

p

 
/
 

p

 

t

t

]

%

[
 

p

 
/
 

p

 

t

t

8

7

6

5

4

3

2

1

0

8

7

6

5

4

3

2

1

0

8

7

6

5

4

3

2

1

0

8

7

6

5

4

3

2

1

0

chdN
hd

=1000,   L3 Field: 0.2T

chdN
hd

=1000,   L3 Field: 0.4T

HLT

Offline

HLT

Offline

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

chdN
hd

=2000,   L3 Field: 0.2T

chdN
hd

=2000,   L3 Field: 0.4T

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

chdN
hd

=4000,   L3 Field: 0.2T

chdN
hd

=4000,   L3 Field: 0.4T

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

chdN
hd

=6000,   L3 Field: 0.2T

chdN
hd

=6000,   L3 Field: 0.4T

1.4
1.4
 [GeV]
 [GeV]

tp
tp

HLT

Offline

1.4
1.4
 [GeV]
 [GeV]

tp
tp

HLT

Offline

1.4
1.4
 [GeV]
 [GeV]

tp
tp

HLT

Offline

1.4
1.4
 [GeV]
 [GeV]

tp
tp

HLT

Offline

1.4
1.4
 [GeV]
 [GeV]

tp
tp

HLT

Offline

1.4
1.4
 [GeV]
 [GeV]

tp
tp

HLT

Offline

8

7

6

5

4

3

2

1

0

8

7

6

5

4

3

2

1

0

8

7

6

5

4

3

2

1

0

8

7

6

5

4

3

2

1

0

]

%

[
 

p

 
/
 

p

 

t

t

]

%

[
 

p

 
/
 

p

 

t

t

]

%

[
 

p

 
/
 

p

 

t

t

]

%

[
 

p

 
/
 

p

 

t

t

65

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4
 [GeV]
 [GeV]

tp
tp

1.4
1.4
 [GeV]
 [GeV]

tp
tp

Figure 4.13: Relative transverse momentum resolution as a function of pt. Results ob-
tained using magnetic ﬁeld strength of 0.2 T (left) and 0.4 T (right), respectively.

D
D
D
D
D
D
D
D
ter centroids. Due to the fact that the Cluster Finder has no prior information about
the individual charge distributions, overlapping clusters are not handled very well. As
a consequence, overlapping clusters are split wrongly and the centroid calculations are
biased. This is also reﬂected in the observed deterioration of the space point resolution
as a function of multiplicity (Figure 4.11). In addition, a number of noise clusters may
also be wrongly identiﬁed from very low pt tracks or δ-electrons which produce rather
large areas of continuous signal in the pad-row planes. These areas are interpreted by the
Cluster Finder as many overlapping clusters. Both these eﬀects contribute to the local
track follower getting dis-oriented during cluster assignment. As a result tracks are being
split and contaminated by wrongly assigned clusters.

Furthermore, the track ﬁnding procedure reconstructs tracks based on a simple ﬁt
selection criteria in conformal space, and does not include more advanced correction for
energy loss and multiple scattering. Since the impact of multiple scattering and energy
loss is higher for low energy particles, this eﬀect is more signiﬁcant in the low momentum
regime.

Parts of the eﬃciency loss at low momentum and high ﬁeld setting may also be
explained by the fact that the Track Finder is implemented in a parallel fashion, i.e. the
tracks are reconstructed at the TPC sector level. Consequently, the tracks which cross
the sector boundaries have to be merged by the subsequent Track Merger. If the quality
of the individual track segments are poor and the track density is high, the Track Merger
may not be able to correctly merge the corresponding track segments.

Secondary tracks

The tracking performance discussed above were determined exclusively for primary tracks.
In order to get an estimate of the reconstruction capabilities of secondary particles, a
complementary was done including a selection of secondary particle tracks. In this case the
generated good tracks in Equation 4.15 was deﬁned as particles which are decay products
of K and Λ particles. In addition, the respective tracks are required to be contained within
the TPC acceptance and furthermore cross at least 40% of all padrows (same requirement
as was made for the primary tracks above). The selected particles thus consists of

K : µ, e, π
Λ :

p, π

for the two cases respectively.

In this case, a second pass was done during the track ﬁnding step, where the sec-
ond tracking pass took the unused clusters from the ﬁrst pass as input. As explained
previously, no vertex constraint was imposed on these secondary tracks, and thus the
conformal mapping of the space points was calculated relative to the ﬁrst assigned cluster
to a track. In Figure 4.14 the resulting eﬃciency as a function of pt is shown for events
with multiplicity of dNch/dη = 1000. For reason of comparison, the eﬃciency obtained
with an recently developed version of the Oﬄine chain is also shown. This new Oﬄine
approach consists of an improved Kalman ﬁlter [71] which includes optimization for de-
60% for the low momentum bin, while
tection of secondaries. The tracking eﬃciency is
at higher pt it increases to >80%. The diﬀerence between Oﬄine and HLT is signiﬁcant

∼

66

Secondary tracks from K and 

,   

=1000,   L3 Field: 0.4T

chdN
hd

HLT

Offline (New)

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

1.4

1.2

1

0.8

0.6

0.4

0.2

0
0.1
0.1

0.2
0.2

0.3
0.3

0.4
0.4

0.5
0.5

0.6
0.6

0.7
0.7

0.8
0.8

0.9
0.9
tp
tp

1
1

 [GeV]
 [GeV]

Figure 4.14: Tracking eﬃciency of secondary tracks from decay of K and Λ particles using
HLT (solid line) and an improved Oﬄine chain (dotted line).

at the low momentum, whereas the diﬀerence decreases at higher momentum. It should
be noted that the current HLT algorithm was not tuned for the detection of secondary
tracks with the exception of the second tracking pass adaption.

Computing requirements

The computing requirements of the various processing tasks can be estimated by mea-
suring the CPU time needed for the diﬀerent processing modules. Such measurements
may indicate the amount of computing power needed to implement the algorithm to run
on the High Level Trigger system, and in particular which processing steps represent the
bottlenecks. In the same way as the tracking performance depends on the particle mul-
tiplicity, the required processing time is correlated with the number of space points and
tracks which have to be reconstructed.

The processing time of all the individual process components was measured for various
event multiplicities. For the test, a standard PC consisting of a 800 MHz Twin Pentium
III with a Serverworks Chipset, 256 kB L2 Cache running a Linux kernel v2.4 was used.
All measurements were performed while the data was located within the memory, i.e.
no overhead from any external device access (disk, network interface etc.) was included.
The left plot in Figure 4.15 shows the measured processing times for three diﬀerent
multiplicities. The measurements are resolved with respect to the diﬀerent computing
steps, where the values corresponds to the time needed by a single processing module
as implemented in the scheme shown in Figure 4.6. For instance, for a multiplicity of
dNch/dη = 1000 the Cluster Finder needs on average about 8 ms to process the pad-rows
which is contained within a single sub-sector, while the Track Finder needs about 130 ms
to reconstruct the track segments within a complete TPC sector. The Track Finder step
includes both the transformation of the space points into conformal space and the track
following procedure.

In the right plot in Figure 4.15 measured CPU-time integrated over all the processing

67

L
chdN
hd

=1000

chdN
hd

=2000

chdN
hd

=4000

HLT

Offline

]
s
[
 
e
m

i
t
-

 

U
P
C
d
e
t
a
r
g
e
t
n

I

2

10

10

]
s
m

[
 
e
m

i
t
 

U
P
C

3

10

2

10

10

Cluster Finder
Cluster Finder

Track Finder
Track Finder

Track Fitter
Track Fitter

Track Merger
Track Merger

Gl. Track Fitter
Gl. Track Fitter

1000
1000

1500
1500

2000
2000

2500
2500

3000
3000

3500
3500

4000
4000

4500
4500

1
500
500

chdN
chdN
hd
hd

Figure 4.15: Compute time requirements measured on an 800 MHz processor for three
diﬀerent particle multiplicities. Measured CPU-time for the sequential tracking chain re-
solved with respect to the diﬀerent processing steps (left). Measured CPU-time integrated
over all the processing steps performed sequentially on a single CPU (right). For reason
of comparison the corresponding CPU-time measured for the Oﬄine reconstruction chain
is shown (dotted line).

steps performed sequentially on a single CPU is shown as a function of multiplicity. The
measured CPU-time needed by the Oﬄine reconstruction chain is also shown. For the
Oﬄine reconstruction chain, data loading into memory is included in the measurement,
as no clear separation between loading and processing of data is available in its current
implementation. At dNch/dη = 4000 the HLT chain needs a total of about 30 s to for a
full TPC event reconstruction, while Oﬄine needs about 170 s.

4.6 Iterative Tracking

In the sequential tracking approach described in the previous section, the cluster centroids
are obtained using a straight-forward center-of-gravity calculation. Such an algorithm
has obvious limitations when applied to a high occupancy environment, since the lack
of information about the tracks bias the centroid calculation in the case of overlapping
charge distributions. The main objective of iterative tracking is to provide the track
information prior to the cluster ﬁnding in order to better ﬁt and unfold the overlapping
clusters. From this the correct cluster centroids should be obtained.

In this approach, the pattern recognition scheme consists of two main parts: Track
candidate ﬁnding and cluster ﬁtting.
In a ﬁrst step an implementation of the Hough
Transform is applied to the raw ADC-data in order to obtain a list of track candidates.
These track segments serve as input for the Cluster Fitter, which reconstructs the cluster
centroids along the particle trajectories by ﬁtting the respective charge distributions to a
parameterized shape. Finally, the assigned clusters are ﬁtted to a helix in order to obtain
the best estimate of the track parameters.

68

4.6.1 The Cluster Fitter

The implemented ﬁtting routines were initially developed for TPC data in the NA49
It assumes that the clusters can described using a two-dimensional
experiment [47].
Gauss function.

The cluster model

The shape of the clusters is given by the convolution of the response functions of the
readout pads, Section 2.4.1. Both the pad and time response functions have a close to
Gaussian shape, as well as the spread of the electron clouds due to diﬀusion. The cluster
model can thus to a good approximation be described with a two-dimensional Gauss-
distribution whose widths depend on geometry of the pads and the track parameters. A
total of ﬁve independent parameters are needed to fully describe the model:

The two-dimensional position (p, t) in pad and time direction.

•

•

•

The widths (σpad, σtime), in pad and time.

The amplitude of the distribution.

These parameters vary for each cluster, so without any prior knowledge they would all
have to be ﬁtted for. In the case of overlapping clusters such a ﬁtting procedure would be
a very demanding task as no information about the number of contributing tracks, nor
the shape for each individual charge distributions, is known. The shape of the clusters in
the TPC depends on the drift length (diﬀusion of the drifting electrons in the gas), and
track crossing angles with the pad-row-plane (spread of primary ionization electrons), and
can be parameterized according to [49]:

·

l2

d2

T ·

sdrift +

(tan(α)

P RF + D2

pad = σ2
σ2

time = σ2
σ2

tan2(β)
12
tan2(λ)
12
where the various terms are described in Section 2.4.1, page 20. This parameterization
indicates that any prior knowledge of track parameters could provide the ﬁtting procedure
with an improved initial guess of the parameters, or even reduce the number of parameters
to vary in a ﬁt.

0 + D2
L ·

tan(ψ))2

sdrift +

−
12

(4.17)

l2

+

·

·

Although the average cluster widths can be described with the parameterization in
Equation 4.17, their properties are determined by stochastic processes and are thus also
subject to ﬂuctuations. The ﬂuctuation of the shape depends on the contribution of
the random diﬀusion and angular spread of the ionization, and also on the gas gain
ﬂuctuations and secondary ionization.

Fitting procedure

The implemented procedure ﬁts the clusters to the model

q(p, t) =

Ak exp

K

Xk=1

2

1
2  

pk
p
σpad,k !

−

1
2  

tk
t
σtime,k !

−

−

2







−



69

(4.18)

which is a sum of K two-dimensional Gauss-distributions, with the variables Ak, (pk, tk)
and (σpad,k, σtime,k) denoting the amplitude, position and widths of the individual distri-
butions, respectively. A χ2 ﬁt of the model to the cluster data is done by applying the
Levenberg-Marquardt method [76] to minimize the least square error in the ﬁt1. All ﬁve
parameters may be free to vary in the ﬁt, or only a subset of them may be ﬁtted while
the rest are held ﬁxed at their input values. In either case, the ﬁtting routine returns the
best ﬁt values for the free parameters in the ﬁt. Once each cluster has been ﬁtted to a
Gaussian distribution, the total charge of the cluster can be obtained from the relation

Qtotal,k = 2πσpad,kσtime,kAk.

(4.19)

The procedure can also be extended to other models for the cluster shape, e.g. to ac-
count for asymmetric Gaussian distributions. In that case the two-dimensional Gaussian
given above may be replaced with an alternative model, and the ﬁtting procedure is done
accordingly.

Deconvolution of overlapping clusters

In the case of overlapping clusters, K
2, the corresponding charge distribution is ﬁtted
to a sum of K individual Gauss-distributions. If the track parameters are known from
K parameters may be provided with initial
a preceding pattern recognition step, all 5
×
values. In addition, deconvolution can now be done by ﬁxing the widths of the individual
distributions and only letting the positions and the amplitude parameters vary freely.

≥

An example of deconvolution of two overlapping clusters is shown in Figure 4.16. In

i

n
b
e
m
T

i

440

438

436

434

432

430

Initial position (trajectory crossing)

Position after fit

46

48

50

52

54

56

56
Pad

54

52

50

48

46

440

436
438
b i n
T i m e

Pad

430

432

434

Figure 4.16: Illustrating ﬁtting and deconvolution of overlapping clusters. Left: The
circles mark the initial position given as input to the ﬁtting routine, while the star marks
the best-ﬁt values returned from the ﬁt. Right: Same clusters shown using a lego-plot.

this case the input to the ﬁtting routine are the two sets of 5 initial parameters, each

1The Levenberg-Marquardt method (also called Marquardt method) is a standard algorithm for the

minimization of the least square error in the ﬁt of nonlinear models.

e
u
l
a
v
-
C
D
A

50

40

30

20

10

0

70

Track loop

Find nearest
cluster

Check for
overlaps

Local maxima

No local maxima

Acceptable

c 2

Unacceptable

c 2

Store clusters

Remove clusters

Fit clusters

Next track

Figure 4.17: Flow diagram of the Cluster Fitter algorithm.

consisting of the position in the pad-row-plane (illustrated by the circle markers), the
widths of the distribution in two dimensions and the amplitude. The initial values of
these parameters are determined from the results obtained by the preceding tracking
ﬁnding algorithm. The initial positions are taken as the crossing point of the computed
tracks with the pad-row-plane, while the widths are calculated from Equation 4.17 with
the track inclination angles and initial positions as input. During the ﬁtting procedure,
the widths are kept ﬁxed at their input values, while the remaining parameters are free
to vary. This means that in this case there are 3+3 parameters which are being ﬁtted for.
The ﬁtting routine ﬁnally returns the best-values obtained for each position (illustrated
by the star markers) and the respective amplitudes of the individual distributions.
In
this way the initial positions serve as the prediction of the cluster centroids, while the
returned values from the ﬁt are the corrected, ﬁnal reconstructed cluster centroids. The
improved parameter estimates are subsequently returned to the track model.

The algorithm

The implemented Cluster Fitter algorithm assumes that the track parameters have been
obtained by a preceding track ﬁnding algorithm, and thus takes a list of track candidates
as input. In a ﬁrst step each input track is propagated through the TPC detector and
the intersection between the helical trajectories and the pad-row-planes is calculated and
stored internally for each track candidate.

The algorithm then processes each pad-row separately as illustrated by the ﬂow dia-
gram in Figure 4.17. A loop is performed over all track intersection points with the current

71

K=1
Inner chambers
Outer chambers
K
Inner chambers
Outer chambers

≥

2

Pad direction [mm] Time direction [mm]
Gauss-ﬁt
0.97
0.90

Gauss-ﬁt
1.48
1.27

CoG
1.33
1.14

CoG
0.99
0.88

1.90
1.78

2.90
4.68

2.33
1.92

3.54
4.36

Table 4.3: Comparison of the space point resolution obtained by applying the ﬁtting
procedure and the center-of-gravity (CoG), corresponding to the Cluster Finder, on both
isolated (K=1) and overlapping (K
2) clusters.

≥

pad-row. For each track, a region around its intersection point in the pad-row-plane is
searched for a nearest cluster. In addition, any potential additional tracks contributing to
the given cluster is identiﬁed. If a local maxima is found within the cluster distribution,
the clusters are ﬁtted. A track candidate is only taken into account in the ﬁt if it can
be associated with a local maxima in the distribution. The reason for this criteria is to
avoid taking into account fake tracks, i.e. tracks which may have been falsely identiﬁed
as a track candidate during the preceding track procedure. If the resulting ﬁt returns
an acceptable χ2 the clusters are stored with the values obtained in the ﬁt, otherwise
the clusters are removed. During the ﬁt the widths are always held ﬁxed at their input
values, while the position and amplitude are ﬁtted for.

In Table 4.3 the space point resolutions achieved with the Cluster Fitter are listed for
both isolated (K=1) and overlapping clusters (K
2). The input parameters to the ﬁt
were in this case provided by the respective simulated particle trajectory producing the
cluster. For comparison, the corresponding resolutions achieved using the Cluster Finder
in the sequential tracking scheme (Section 4.5.1) are also listed. The results show that
for isolated clusters the transverse resolution obtained in the two approaches are similar,
while the longitudinal resolution is slightly worse in the ﬁtting approach. In the case of
overlapping clusters, the Cluster Fitter achieves 30-50% better resolution compared to
the Cluster Finder.

≥

4.6.2 The Hough Transform

The Hough Transform (HT) is a widely adapted algorithm in image analysis related ﬁelds.
Originally it was used to recognize linear patterns in binary images [60], but the principle
can easily be implemented to any kind of pattern. A complete review of HT methods and
main developments can be found in e.g. [77] and [78].

The basic idea of the HT is easily understood by considering a straight line, y = ax+b,
where the parameters a and b deﬁne the slope and intercept respectively. The HT uses
the idea that this equation can be reformulated into b =
ax + y. Thus a point (x, y)
is seen to deﬁne a straight line in a two-dimensional parameter space. In the space of
parameters a and b, this line corresponds to all possible straight lines going through the

−

72

y

b

9

8

7

6

5

4

3

2

1

8

6

4

2

0

-2

-4

-6

-8
0

0

0.5

1

1.5

2

2.5

3

3.5

4

0.5

1

1.5

2

2.5

3

3.5

4

4.5

x

5
a

Figure 4.18:
Illustration of a simple HT applied to points lying on the straight line
y = 2x + 1 (left). Each point transforms to a line in the parameter space (right), and the
intersection of these lines deﬁnes the parameters of the line.

point (x, y) in the xy-plane. If several points (xi, yi) are along a straight line in the xy-
plane, the transformed lines in parameter space will intersect at the point (a, b) which
deﬁnes the actual parameters of the line. By identifying this point of intersection, one
has identiﬁed the line itself. Thus the eﬀect of the HT is to reduce the recognition of
global patterns in a image space to a local peak detection in parameter space. This is
illustrated in Figure 4.18.

Formally one can describe the transform as follows. Let X be a point in image space
and Ω a point in parameter space. A set of image points which lie on a curve can then
be deﬁned by the function f such that

f (X, Ω) = 0.

(4.20)

In the case of a straight line one can deﬁne X=(x, y) and Ω = (a, b). The equation for a
straight line then becomes f (X, Ω) = y
b = 0 and maps each value of the parameter
−
combination (a, b) to a set of image points. The mapping is one to many from the space of
possible parameters values to the space of image points. Equation 4.20 can be viewed as
a mutual constraint between image points and parameter points and therefore it can be
interpreted as deﬁning a mapping from a set of image points to a set of possible parameter
values.

ax

−

The HT can be deﬁned for arbitrary parametrically deﬁned image curves. When
considering curves characterized by N parameters, the HT will map out of hyper-surface
in a N-dimensional parameter space. Correspondingly the intersection of these surfaces
marks a possible candidate for the curves in image space. The HT for a set of image
points,

, is deﬁned as

X1, X2, ..., Xn}

{

H(Ω) =

p(Xj, Ω)

(4.21)

n

Xj=1

73

where

p(X, Ω) =

1 if f (X, Ω) = 0
0 otherwise

(

(4.22)

In practical implementations of the HT, the continuous parameter space is usually
considered to be composed of the union of ﬁnite-sized regions. One then makes parameter
space discrete and deﬁnes a counter for each cell in the parameter space. Each cell is
associated with a element in a multi-dimensional array, and the respective counters are
incremented when a hyper-surface from the transform of an image point passes through
the region of parameter space associated with the element. This array is in literature
referred to as the Hough accumulator array. If CΩ denotes a element in the accumulator
array which corresponds to a cell in the parameter space centered at Ω, then consequently
p(X, Ω) is 1 if any curve corresponding to the parameter space cell CΩ passes through
the point, X, in image space. This can be considered as every image point giving “votes”
to all the possible parameter combinations it can possibly belong to. The detection of
the intersection of the surfaces is done by detection of local peaks in the number of
accumulator counts. The size and shape of these peaks depend on several factors, such
as the number of transformed image points, the errors of the image points and the size
of the parameter cells.

The standard Hough transform as described above is a divergent transform, i.e. a
point is mapped onto a curve in parameter space. It is however possible to reformulate the
transform so that it becomes a convergent transform. In this case, instead of transforming
single image points into curves in parameter space, subset of points is mapped. The
advantage of this approach is that each mapping is into a smaller subset of the parameter
space. However, since all combinations of possible subsets of image points has to be
mapped in the transform, the combinatorics of such algorithm makes it very computing
intensive.

Implementation issues

As described in the previous section, the underlying idea of the HT is to search for some
kind of predeﬁned parameterized pattern in an certain image space. In the case of the
ALICE TPC, the image space consists of the digitized charge distributions sampled from
the ionization along the particle trajectory, while the patterns to be parameterized are
deﬁned by the motion of the charged particles in the homogeneous magnetic ﬁeld. The
main implementation issues of such an algorithm consists of deﬁning a proper parame-
terization and its corresponding HT, together with the adaption of a optimal parameter
space. These issues are discussed in the following.

Parameterization and derivation of the HT

A charged particle which moves in a homogeneous magnetic ﬁeld follows a helical trajec-
tory. This corresponds to a circular motion in the plane perpendicular to the magnetic
ﬁeld, and a non-curved motion in the plane along the magnetic ﬁeld. In total ﬁve indepen-
dent parameters are needed to fully describe the helix in three dimensions (Appendix A.2).
Under the assumption that the helix originates from the vertex, the following parameter-

74

ization can be used,

1
κ{
1
κ{−

x(t) =

sin(ψ0 + ht)

sin ψ0

−

}

y(t) =

cos(ψ0 + ht) + cos ψ0

}

z(t) = γt,

(4.23)

where κ is the curvature, ψ0 is the emission angle with the x-axis, h is the sense of
rotation, γ governs the longitudinal translation and t
[0, π]. In this scenario the number
of required parameters are reduced to the 3 independent parameters, (κ, ψ0, γ), where the
two former describe the circle in the transverse plane and the latter the straight line in
the longitudinal plane.

∈

Now let Xi = (xi, yi, zi) denote a point lying on the helix, and Ω = (κ, ψ0, γ) the
parameters of a helix. Following the terminology introduced above, Xi and Ω represent
points in image space and parameter space respectively. Let Mi(Xi, Ω) denote the minimal
squared distance between a helix and a point i,

Mi(Xi, Ω) = M xy

i + M z

i = (xi −

x)2 + (y

yi)2 + (z

zi)2,

−

−

(4.24)

and M z

where M xy
i denote the transverse projection and the longitudinal distance re-
i
spectively. In order to derive the standard HT for this helix parameterization one has to
solve the equation

Mi(Xi, Ω) = 0.

(4.25)

This formula allows the determination of the set of all helix parameters which correspond
to the given point Xi. The transformation deﬁnes a mapping of one point in our image
space into a three dimensional curve in the parameter space span by (κ, ψ0, γ). The
problem can be simpliﬁed by considering the transverse and the longitudinal part of the
helix separately:

(x, y)
(z)

↔
↔

(κ, ψ0)
(γ)

The transform in the transverse plane is then deﬁned by the equation M xy
i =0, and and
maps the transverse projection of a point into a curve in the two-dimensional parameter
space span by (κ, ψ0). In the latter case the transform is deﬁned by M z
i =0 and maps a
point into a one-dimensional parameter space span by γ.
Substituting x and y from Equation 4.23 into M xy
i

in Equation 4.25 gives

Taking the square root and changing to polar coordinates (xi, yi)

(ri, φi) gives

1
κ2

1

(cid:26)

−

q

(κxi + sin ψ0)2 + (κyi −

cos ψ0)2

= 0.

2

(cid:27)

1
1
κ (cid:26)

±

−

q

κ2r2

i + 2κri(cos φi sin ψ0

sin φi cos ψ0) + 1

= 0

−
i + 2κri sin(φi −

κ2r2

ψ0)

= 0

→

(cid:27)

o

1
κ

−

n

75

which ﬁnally gives the expression

κ =

2
ri

sin(φi −

ψ0)

γ =

zi
t

.

Equation 4.26 is thus the standard HT for the parameterization of a helix going through
(0,0) projected into the transverse plane. According to this transform a single image
point will by mapped into a sinusoidal curve in parameter space.

Similarly, substituting z from Equation 4.23 gives

This transform thus corresponds to a mapping into a one-dimensional parameter space
span by γ.

Equations 4.26 and 4.27 deﬁnes the HT for the transverse and longitudinal pattern of
the helix parameterization in Equation 4.23. Using this deﬁnition two separate accumu-
lator arrays are required which can be denoted as H(κ, ψ0) and H(γ) respectively. The
two sets are correlated, i.e. a valid peak in H(κ, ψ0) will have a corresponding peak in
H(γ). This can be utilized by deﬁning subsets in γ, and thereby transforming all points
located within a given subset into H(κ, ψ0). A helix whose points are conﬁned within
the subset, will consequently create a peak in H(κ, ψ0) corresponding to the parameters
of the helix in the transverse plane. The longitudinal parameter of the helix can thus be
determined from the values of the subset being transformed. For the purpose of deﬁning
such sub-volumes, the pseudo-rapidity, η, has been used. This is a geometrical quantity
deﬁned by the angle between the trajectory and the beam axis, θ, as

η = ln(tan

)−1.

θ
2

η =

ln

1
2

r3 + z
z
r3

−

θ is related to the dip-angle of the track, λ, as θ = π
point (x, y, z) is deﬁned as

2 −

λ. The pseudo-rapidity of a image

where r3 = √x2 + y2 + z2. The standard HT (Equation 4.21) for a set of image points,
ǫk =

, is then deﬁned as

X1, X2, ..., Xn}

{

where

and

Hk(κ, ψ0) =

p(xj, yj, κ, ψ0)

p(x, y, κ, ψ0) =

1 if κ = 2
0 otherwise

r sin(φ

−

ψ0)

(

ǫk ∈

[ηk, ηk+1]

ǫk corresponds to the set of points located within a sub-volume in the longitudinal plane,
Figure 4.19. Each sub-volume thus deﬁnes an image space which contains circular patterns
corresponding to the projected tracks in the transverse plane.

n

Xj=1

76

(4.26)

(4.27)

(4.28)

(4.29)

(4.30)

(4.31)

(4.32)

k

k+1

x

z

TPC barrel

y

TPC Sectors

0 t

(r        )
1

1

x

(r        )
2

2

R =

1

t

Center of curvature

Figure 4.19: Deﬁnition of the image space in the HT. The image space (right) is deﬁned
as a sub-volume characterized by the pseudo-rapidity, ηk (left).

On choosing the quantization scheme for H(κ, ψ0)

To compute the HT both the ranges and the quantization steps must be selected for
H(κ, ψ0). The proper choice of these parameters is an essential part of the algorithm
as they directly aﬀect the memory size, processing times and the ﬁnal accuracy of the
obtained track parameters. The optimal number and size of bins in the parameter space
depend generally on aspects like the number and density of the data points, the required
resolution, the noise level and computing requirements. For example, choosing a bin size
which is too big will restrict the achievable resolution on the reconstructed tracks. On the
other hand, if the bin size is too small the image points on a single track will be spread
throughout too many bins and may not create a distinct peak at all.

In order to choose the set of properties for the parameter space, an investigation of
the characteristics of the formed peak is necessary. The shape and size of the peak is
determined by two main factors: The sampling of the parameter space, and the “noise”
In this context noise includes all factors which contribute to the
of the image points.
image points not being perfectly aligned on the circle parameterization, and consequently
would lead to the intersection point in parameter space being smeared over a ﬁnite area.
The intrinsic accuracy of the transform is limited by the sampling intervals chosen for
the accumulator array, H. Moreover, the formation of peaks in the parameter space is
directly inﬂuenced by how the space is quantized. Following the procedure introduced
in [79] one can investigate the eﬀect on formation of the peaks in the parameter space.
Consider an ideal circular trajectory deﬁned by the parameters (κt, ψ0,t) with two points
(r1, φ1) and (r2, φ2) as illustrated in Figure 4.19. These two points will under the HT
deﬁne two sinusoids in the parameter space, H(κ, ψ0) (Figure 4.20, left),

κ1(ψ0) =

sin(φ1

ψ0)

2
r1
2
r2

77

−

−

κ2(ψ0) =

sin(φ2

ψ0),

(4.33)

y
f
k
f
h
h
q
0.02

]

1
-

m
c
[
 

0.01

y(1

)0

0

y(2

)0

-0.01

-0.02

(y  )
1
0

(y  )
2
0

+

1
2

q

t

h

(4.34)

(4.35)

-1

-0.5

0

0.5

1

 [rad]
0

0,k

0,t

0,k+

1
2

q

0

Figure 4.20: Peak formation in the HT parameter space. The transformation of the
points lying in a circular segment deﬁne sinusoids in the parameter space (left). The
curves corresponding to two end-points on the segment are denoted κ1 and κ2. The peak
is spread due to the sampling of the parameter space (right).

with the derivatives

∂κ1
∂ψ0
∂κ2
∂ψ0

=

=

2
r1
2
r2

−

−

cos(φ1

ψ0)

cos(φ2

ψ0),

−

−

In addition, let (r0, φ0) represent a point on the circle between the two points. Assuming
that

< π/2 and r1 < r0 < r2 gives

ψ0

φ

|

−

|

κ1(ψ0) > κ0(ψ0) > κ2(ψ0)
κ1(ψ0) < κ0(ψ0) < κ2(ψ0)

for ψ0 < ψ0,t
for ψ0,t < ψ0

∈

1
2

The peak formed by the transformation exhibits a typical cross shape (Figure 4.20, right),
where the spread will be conﬁned by the the curves of κ1 and κ2. Let now ψ0 be sampled
1
2∆ψ0 and ψ0,k +
with a uniform interval ∆qψ0 and assume that ψ0,t lies between ψ0,k −
1
2∆ψ0 where k

[0, m]. The uncertainty between ψ0,k and ψ0,t can be given by

−

∆qψ0

δψ0,k = ψ0,t −
Each of the accumulators can be regarded as small rectangular window overlaid the
continuous parameter space. Assume that the true peak is located within the window of
size ∆qψ0
[0, n]. The uncertainty between κh and
κt is then

∆qκ with center (ψ0,k, κh) where h

ψ0,k ≤

∆qψ0.

(4.36)

≤

×

∈

1
2

1
2

−

∆qκ

δκh = κt −

κh ≤

≤

1
2

∆qκ.

(4.37)

78

y
k
k
k
y
k
y
k
k
D
k
y
D
y
k
k
Depending on the relative size of ∆qψ0 and ∆qκ, the peak will be distributed or spread
out over several bins in the accumulator array. The total spread in the κ-direction, ∆psκ
at the sampling interval ψ0,k is given by

∆psκ(ψ0,k) = κ1(ψ0,k)

κ2(ψ0,k)

−

(4.38)

Substituting δψ0,k = ψ0,t −

ψ0,k gives

∆psκ(ψ0,k) =

sin(φ1

ψ0,t + δψ0,k)

sin(φ2

ψ0,t + δψ0,k)

2
r1

= cos(δψ0,k)

sin(φ1

ψ0,t)

sin(φ2

ψ0,t)

2
r2

−

−

−

2
r1

2
r1
(cid:26)
κ1(ψ0,t)
{

2
r2

−

2
r2

−

−

−

κ2(ψ0,t)

}

−

cos(φ1

ψ0,t)

cos(φ2

∂κ2
∂ψ0 !ψ0=ψ0,t

∂κ1
∂ψ0 !ψ0=ψ0,t

−  

−

(cid:27)
ψ0,t)

−

(cid:27)

(cid:26)
+ sin(δψ0,k)

= cos(δψ0,k)

+ sin(δψ0,k)




 



In the last equation the ﬁrst term equals zero since κ1(ψ0,t) = κ2(ψ0,t) = κ(ψ0,t) which
gives

∆psκ(ψ0,k) = sin(δψ0,k)

∂κ2
∂ψ0 !ψ0=ψ0,t

∂κ1
∂ψ0 !ψ0=ψ0,t

−  




 

Maximum spreading of the peak occurs when (see Equation 4.37)













max(
|

δψ0,k|

) =

∆qψ0

1
2

which yields

(∆psκ(ψ0,k))max = sin(

∆qψ0)

1
2

∂κ2
∂ψ0 !ψ0=ψ0,t




 

∂κ1
∂ψ0 !ψ0=ψ0,t




−  


Consequently, the following condition on the bin size will allow a maximum collection
votes without peak spreading:



∆qκ = (∆psκ(ψ0,k))max.

(4.43)

Equation 4.43 shows that values of quantization steps combine to inﬂuence the spread-
In

ing of the peak, which indicates that they cannot be selected fully independently.
particular, one can diﬀer between the two cases,

∆qκ < (∆psκ(ψ0,k))max
∆qκ > (∆psκ(ψ0,k))max,

(4.44)

which can be interpreted as under-sampling and oversampling of ψ0 with respect to κ
respectively. The eﬀect of the former case is to split the votes to neighboring accumulators

79

(4.39)

(4.40)

(4.41)

(4.42)

in the κ-direction, while the latter corresponds to the case where the peak is distributed
over several neighboring bins in the ψ0-direction.

This derivation shows that for an ideal circle segment, the resulting peak will be spread
with a characteristic form due to the quantization of the parameter space. The implicit
assumption is that all the image points on the circle are transformed into a single bin if the
proper quantization is chosen. However, the image points, represented by the digitized
charge distribution in the TPC detector, do not lie on an ideal circular trajectory due to
the energy loss and multiple scattering of the particles and the intrinsic detector noise.
As a consequence of the uncertainty of the image points, (ri, φi), on a circular track, the
transformation of these points gives a region in the (κ, ψ0)-space rather than a point. The
uncertainty of the calculated κ can be obtained by taking the Taylor expansion of the
transformation Equation 4.26 around r and φ. The ﬁrst order approximation yields

δκ =

∂κ
∂φ !r,ψ0

δφ +

 

 
2
r

δr

∂κ
∂r !φ,ψ0
2
r2 sin(φ

−

=

cos(φ

ψ0)δφ

−

ψ0)δr.

−

(4.45)

This relation shows that spread of the peak in κ is inﬂuenced by the noise of image points,
but also on the location of the points in the image. Due to the r−2 dependence in the
second term, the error will be dominated by the error in φ.

In general, the characteristics of the formed peaks in the parameter space depend on
both the location of the image points and the track segment in space, and the errors of
the individual image points. For a given input image containing several track segments
it is therefore generally impossible to quantize H(κ, ψ0) optimally such that all peaks
formed by the transform have the same extension and shape. It is therefore desirable
to construct a quantization scheme that as far as possible allows an average peak which
keeps a suﬃcient peak signiﬁcance with respect to the background.

Accumulation

The HT itself is a pure accumulation procedure. The transformation of each image
point consists of incrementing the corresponding counters in a multidimensional array
representing the cells in the parameter space. Depending on the nature and topology
of the image, diﬀerent methods can be applied. Many implementations of the HT have
suggested using weighting factors so that the most prominent or most certain image
features contribute more to the accumulator cells than less certain data. The eﬀect of
such schemes is evident: By assigning higher weights to image points lying closer to the
parameterization, the corresponding bins will receive a higher vote and will consequently
lead to a sharpening of the peaks in parameter space. Formally this is introduced by
modifying Equation 4.21 to

H(Ω) =

w(j)p(Xj, Ω)

(4.46)

where w(j) represents the weighting factor for each image point.

n

Xj=1

80

In the case of TPC data, each image point has an associated ADC-value. These values
represent the charge distributions of the ionized charge in each pad-row-plane, where the
centroid of the distribution denotes the space point of the crossing particle track. In this
context, each image point can be regarded as having a certain weight, corresponding to
its ADC-value. Thus w in Equation 4.46 is replaced by the ADC-value of the time-bin
to be transformed. The implementation of the resulting accumulation algorithm can be
illustrated in pseudo-code as follows:

Initialize H(κ, ψ0) to zero;
for i:=1 to npoints do
for k:=1 to m do
κt = 2
ψk);
sin(φi −
ri
κh = Qntz(κt, ∆qκ);
H[ψk][κh] = H[ψk][κh] + ADC-value;

end for

end for

Qntz() denotes the quantizing function which locates the corresponding bin in the κ-
direction. It is thus deﬁned by

Qntz(κ, ∆qκ) =

κp−1
κp

(

κp−1
if κ
−
otherwise

≤

∆qκ/2

Each of the accumulator arrays, H(κ, ψ0), can be interpreted as a two-dimensional
histogram, Figure 4.21, where each bin in the histogram corresponds to an element in the
accumulator array.

where p

[0, m].

∈

Peak ﬁnding

Once the two-dimensional histograms have been ﬁlled according to the transformation
outlined above, the histograms have to be searched for local peaks which corresponds to
potential track candidates. Among these peaks there will in general also be a certain
number of false peaks which are originating from artiﬁcial features in the image. Such
features may e.g. be noisy data points and tracks which are outside the parameter range
which is searched.
In both cases structural backgrounds are created in the parameter
space.

Prior to the peak search, a threshold operation is applied to the histogram. The reason
for this is twofold. Firstly, the number of bins which have to be searched by the peak
ﬁnder, are reduced. Secondly, such a threshold eliminates the contribution from peaks
that are too small to correspond to a real track segment. In the simplest case a local
peak ﬁnder identiﬁes a single histogram bin which corresponds to a local maxima. The
requirement for such a local maxima is that it has a value that is larger than its immediate
neighbors in the histogram. However, since the shape of the peaks is to the ﬁrst order
known, the peak ﬁnder can take advantage of this knowledge in order to better be able
to exclude the true peaks from the background.

81

]

1
-

m
c
[
 

0.003

0.002

0.001

0

-0.001

-0.3

-0.2

-0.1

0

0.1

0.2

0.3

 [rad]
0

Figure 4.21: Example of the HT parameter space after transforming all points lying along
a track with pt =0.5 GeV.

The goal of the peak ﬁnder is to estimate the coordinates of the crossing point,
(κt, ψ0,t), of the peak shape. The implemented procedure starts by locating the initial
peaks within a sliding window in the κ-direction, Figure 4.22. The length of the window,
nκ, is determined by the expected spread of the peak. The idea is thus to choose nκ large
enough so that it captures the full peak spread in κ near ψ0,t, and at the same time small
enough so that it captures the spread only near ψ0,t. If the value of nκ is too small the
window will not take into account all the transformed points along the track segment,
while a window that is too large will include more of the background and may create
ambiguities. The complete procedure can be outlined as follows:

1. Location of the initial peaks.

For each bin in the ψ0-direction the sum of values within the sliding window in κ is
calculated. If a local maxima is found, this window is stored in a temporary list.

2. Location of the two-dimensional peaks.

Each of the local maxima is compared with the local maxima’s in the neighboring
ψ0-bins in order to validate the peak in the two dimensions. If a suﬃcient number
of matching windows are found, the peak is passed to the next step, otherwise the
window is deleted from the list.

3. Evaluation of the peaks.

The two-dimensional peaks are evaluated by performing a simple check on their
shape. This is done by comparing the sum of values within regions on both sides of
the maximum bin in ψ0. If the shape exhibits a clear asymmetry, the peak is stored
in a ﬁnal peak list.

82

y
k
The bin with the maximum value within the peak-region deﬁnes the ﬁnal location of the
peak.

n k

0

Figure 4.22: Illustration of the peak ﬁnding method.

Choice of parameters

Image boundaries

The critical aspect in choosing the boundaries of the image space is the requirement for
the image to contain enough signals from each of the track segments in order to produce
a clear peak in the parameter space. In particular, the image must contain a suﬃcient
number of pad-rows in which the track produces a signal in order for the accumulator
array to pick up enough votes for the tracks. This is necessary for the formed peaks
to be signiﬁcant compared to the background structures. From a implementation point
of view, the choice of image space deﬁnes the required data volume access. Thus the
segmentation in the transverse direction is a choice based on a trade-oﬀ between two
main factors: Required data ﬂow and performance of the algorithm.

As described in Section 3.3, the input data-ﬂow in the HLT system is deﬁned by the
granularity of the detector. The TPC is read out in sub-sectors, where each sub-sector
corresponds to 1/6 of a complete TPC sector. The data from each sub-sector is transferred
directly into the ﬁrst level nodes of the HLT system (FEPs). In order to minimize the
required connectivity in terms of bandwidth between the HLT nodes, an attempt has
been made to do as much as possible of the processing directly on the FEPs.
In this
approach, the image in the transverse direction will be deﬁned by the boundaries of the
sector readout chambers, Figure 4.24.

The performance of the algorithm depends on the topology of the parameter space,
and the ability to distinguish the local maxima from the background. In this context, the
prime task is the selection of an image space which allows for a suﬃcient accumulation
of hits along the particle trajectory. The inherit spread of the peak in parameter space is
a function both of the sampling intervals and the location of the image points in space.
In particular, the opening angle between the arms of the peak cross shape is dependent
If the
on the distance between the two end-points of the track segment, Figure 4.20.

83

y
k
points are close in space this angle is small, and ∆qκ needs to be chosen correspondingly
small in order to have a sharp peak. Furthermore, ∆qψ0 has to be chosen according to the
conditions in Equation 4.44 in order to avoid under- or over-sampling. On the other hand,
if ∆qκ is signiﬁcantly smaller than the spread caused by the noise of the image points,
the peak will be smeared out over a large number of bins and may loose its signiﬁcance
with respect to the background.

Based on the above analysis, it has proven necessary to include more than one sub-
sector in the HT in order to gain suﬃcient peak signiﬁcance versus background. Since the
HT is a pure accumulator procedure, this can be implemented by adding the accumulator
arrays from the diﬀerent sub-sectors. Hence the HT is performed locally on a sub-sector,
and in subsequent step the accumulator arrays from all successive sub-sectors within a
complete sector are added,

H sector(κ, ψ0) =

H subsector

i

(κ, ψ0)

6

(4.47)

Xi=1
Here, H sector(κ, ψ0) is the resulting accumulator array obtained by adding all the non-zero
entries from the individual sub-sectors, H subsector

(κ, ψ0).

i

Longitudinal segmentation

In the longitudinal direction, the data volume is divided into sub-volumes in pseudo-
rapidity, Figure 4.19. This segmentation of the data serves two purposes: Firstly, it
allows the determination of the parameter governing the longitudinal motion. Secondly,
it reduce the density of tracks within one image. The size of the η-volumes is critical
to the performance of the algorithm.
If the size is too small, the tracks may be split
into several sub-volumes. This splitting causes the signal of a single track to appear in
several neighboring sub-volumes, and may prevent a track with too few hits in a single
image, to produce a clear peak in the corresponding parameter space. On the other hand,
choosing a sub-volume which is too wide leads to a higher track density within an image
and produces a correspondingly higher occupancy in the parameter space. This makes it
diﬃcult to correctly identify the peaks. In addition, a very course segmentation will lead
to a low resolution in determining the longitudinal parameter of the trajectory.

Figure 4.23 shows the average spread in pseudo-rapidity, ∆η, for a given particle as
a function of pt. The spread is obtained from the η-distribution of the signals produced
by a given particle. Such a plot thus gives an estimate of how much of the signal from
a particle will be contained within a given η-sub-volume. ∆η increases below pt of 0.3-
0.4 GeV, while for higher momentum the spread saturates at ∆η
0.001-0.003. This
observed increase at lower momentum is mainly due to the fact that particle energy loss
and multiple scattering becomes more signiﬁcant for lower momenta.

∼

Based on the simulated spread, a uniform segmentation of η with intervals of size 0.01

has been done. This leads to about 100 sub-volumes in the central cone of

< 0.9.

η

|

|

Parameter space

The properties of the parameter space are determined by the choice of image space. In
particular, the range of the histogram axis is deﬁned by the possible values of the track

84

-2

10

-3

10

0.1
0.1

0.2
0.2

0.3
0.3

0.4
0.4

0.5
0.5

0.6
0.6

0.7
0.7

0.8
0.8

0.9
0.9
tp
tp

1
1
 [GeV]
 [GeV]

Figure 4.23: Average spread in pseudo-rapidity, ∆η, of signals produced by a given particle
as a function of pt.

parameters. The parameters consist of the curvature of the track, κ, and the emission
angle with the x-axis, ψ0. The curvature of the track is inversely proportional to the
transverse momentum of the particle, Equation 4.1, and the maximum value of κ is thus
deﬁned by the minimum value of pt:

max =

κ
|

|

0.3B
pt,min

.

(4.48)

(4.49)

In this context, pt,min corresponds to the lower limit of the desired pt range to be mea-
sured. Once the κ-range is deﬁned, the range in ψ0 can be given by taking the inverse of
Equation 4.26,

ψ0

max = φm −

|

|

sin−1

rm
2

(cid:18)

κmax

,

(cid:19)

where (rm, φm) is the polar coordinates of a point on the line which conﬁnes the image
space, Figure 4.24. This line corresponds to the azimuthal angle of the boundary of the
TPC sector. The point is chosen to be in the middle of the TPC barrel, in order to avoid
an unequal distribution of the track segments in neighboring sectors.

The fact that the maximum values of κ are deﬁned by the minimum value of pt,
makes it possible to apply a lower pt-cut in a straight-forward way. Particle tracks with a
higher curvature than the range of the histogram axis will not create a valid peak in the
parameter space, and will thus be excluded from the pattern recognition. This can be
exploited by the algorithm, which can be optimized to search for tracks within a certain
pt-range.

The sampling intervals of the parameter space were chosen according to the average
spread of the formed peaks in the simulations. The relative size of the intervals were
selected in order to avoid possible under- or over-sampling, Equation 4.44.

4.6.3 Data ﬂow

Similar to the sequential tracking code, the iterative reconstruction chain has been im-
plemented in modules in order to be highly conﬁgurable with respect to diﬀerent process

85

h
D
]

m
c
[
 

Y

80

60

40

20

0

-20

-40

-60

-80
0

)mf,m(r

0,max

50

100

150

200

250

X [cm]

Figure 4.24: Parameterization of the HT image space. The parameter range of the HT
parameter space is deﬁned by the minimum pt (maximum curvature, κ) and a point
(rm, φm) on the line which conﬁnes the image space in the transversal direction (this line
corresponds to the TPC sector boundary).

topologies. Each processing step has however certain requirements with respect to locality
and data ﬂow.

HT The HT is a pure accumulation procedure, and consists of incrementing an array of
counters for every input point. The algorithm is parallel by nature, since the transform of
each image point is treated independently. However, the requirement on the image space
with respect to the peak signiﬁcance versus background makes it necessary to include a
minimum number of data volume as input data.

Peak Finder Peak ﬁnding is done independently on each accumulator array. Each

peak array may thus be processed in parallel.

Cluster Fitter The Cluster Fitter closely resembles the Cluster Finder as far as
parallelization is concerned. They are both restricted to the two-dimensional pad-row-
plane without any need of information from the other pad-rows. However, the Cluster
Fitter requires the track parameters as input for the ﬁtting procedure, and thus needs
information on the tracks that crosses the pad-row being processed.

A possible data ﬂow scheme is shown in Figure 4.25.

In this scenario the HT is
performed in parallel on each sub-sector, whereas the resulting accumulator arrays from
each sub-sector within a sector are added according to Equation 4.47. The arrays are
then processed by the peak ﬁnder, whose output is a list of track candidates. The list
of tracks are passed to the Cluster Fitter (Section 4.6.1), which reconstructs the clusters
along the helix trajectories. In order to obtain the optimal track parameters, the collected
space points belonging to each track are ﬁtted to a helix using the Track Fitter algorithm,
Section 4.5.3.

4.6.4 Performance

The complete reconstruction chain, as outlined above, has been evaluated for diﬀerent
particle multiplicities. In order to compare the results with the results from the other

86

y
36 TPC Sectors

1 TPC Sector

HT

HT

HT

HT

HT

HT

Add H(    )

Peak Finder

Cluster Fitter

Track Fitter

Sector level

Global level

Figure 4.25: Data ﬂow for the iterative reconstruction chain.

tracking algorithms, the same deﬁnitions concerning tracking eﬃciency as described on
page 59 have been used. The results are compared to both the sequential tracking scheme
and the standard Oﬄine chain, here referred to as HLT sequential and Oﬄine, respec-
tively.

Tracking eﬃciency

Figure 4.26 shows the tracking eﬃciency as a function of pt for two event samples of mul-
tiplicity dNch/dη = 1000 and dNch/dη = 4000, respectively. All primary particles with
pt ≥
0.15 GeV were included in this evaluation, and the boundaries of the parameter
space were adapted accordingly. At dNch/dη = 1000 the eﬃciency is slightly lower than
for both HLT sequential and Oﬄine. In particular, the eﬃciency drops in the low momen-
0.5 GeV. For higher pt the eﬃciencies shows similar behavior for all
tum regime for pt ≤
approaches. At higher multiplicity, dNch/dη = 4000, the track eﬃciency in the iterative
approach is signiﬁcantly lower than both the HLT sequential and Oﬄine reconstruction
chain.

In Table 4.4 the ratio between the number of track candidates found by the HT, and
the number of tracks reconstructed by the Cluster Fitter are listed for the two event
samples. This ratio provides an estimate of the relative number of false track candidates
which originate from falsely identiﬁed peaks in the HT parameter space. The ratios
indicate that more than 70% and 130% of all the track candidates from the HT failed to
be reconstructed by the Cluster Fitter for the two multiplicities, respectively. The right
column in Table 4.4 shows the corresponding ratios for the case where the parameter

87

W
chdN
hd

=1000,   L3 Field: 0.4T

chdN
hd

=4000,   L3 Field: 0.4T

HLT (Sequential)
HLT (Iterative)
Offline

HLT (Sequential)
HLT (Iterative)
Offline

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

1.4

1.2

1

0.8

0.6

0.4

0.2

0

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

tp
tp

 [GeV]
 [GeV]

tp
tp

 [GeV]
 [GeV]

Figure 4.26: Tracking eﬃciencies as a function of pt for two multiplicity samples. All
tracks with pt ≥

0.15 GeV have been included in the evaluation.

Ratio

pt,min

0.15 GeV pt,min

0.5 GeV

dNch/dη
1000
4000

≥
1.75
2.35

≥
1.39
1.32

Table 4.4: Ratio between track candidates found by the HT and tracks reconstructed by
the Cluster Fitter.

space in the HT has been restricted to only include tracks for pt ≥
the ratio decreased to
fake track candidates originates from the lower pt-range.

0.5 GeV. In this case
1.3 for both multiplicities, indicating that the major part of the

∼

Figure 4.27 shows the tracking eﬃciencies for the same event samples as evaluated
for the sequential track reconstruction chain, Figure 4.7. Due to the problem with fake
tracks at lower momentum, the parameter space in the HT was optimized for tracks with
pt ≥
0.5 GeV were included in
the evaluation. At lower multiplicities (dNch/dη
2000) the tracking eﬃciency of all
approaches are similar for this pt-region. At dNch/dη = 4000, however, the eﬃciency
of the iterative approach is signiﬁcantly lower than for the HLT sequential and Oﬄine
reconstruction chains.

0.5 GeV, and consequently only particle tracks with pt ≥

≤

Momentum resolution

Figure 4.28 shows the transverse momentum resolution for two of the event samples in
Figure 4.27. At the lower (dNch/dη = 1000) multiplicity the iterative approach is slightly
better than the sequential approach. At higher multiplicities (dNch/dη = 4000), however,
the iterative approach is lower than both sequential and Oﬄine.

88

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e

i

c

 

i
f
f
e
g
n
k
c
a
r
T

i

0
0.5
0.5

0
0.5
0.5

1.4

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

0
0.5
0.5

0
0.5
0.5

chdN
hd

=1000,   L3 Field: 0.2T

chdN
hd

=1000,   L3 Field: 0.4T

HLT (Sequential)
HLT (Iterative)
Offline

HLT (Sequential)
HLT (Iterative)
Offline

1
1

1.5
1.5

2
2

2.5
2.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

3
3
 [GeV]
 [GeV]

tp
tp

0
0.5
0.5

chdN
hd

=2000,   L3 Field: 0.2T

chdN
hd

=2000,   L3 Field: 0.4T

1
1

1.5
1.5

2
2

2.5
2.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

3
3
 [GeV]
 [GeV]

tp
tp

0
0.5
0.5

chdN
hd

=4000,   L3 Field: 0.2T

chdN
hd

=4000,   L3 Field: 0.4T

1
1

1.5
1.5

2
2

2.5
2.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3
 [GeV]
 [GeV]

tp
tp

0
0.5
0.5

chdN
hd

=6000,   L3 Field: 0.2T

chdN
hd

=6000,   L3 Field: 0.4T

tp
tp

 [GeV]
 [GeV]

HLT (Sequential)
HLT (Iterative)
Offline

tp
tp

 [GeV]
 [GeV]

HLT (Sequential)
HLT (Iterative)
Offline

3
3
 [GeV]
 [GeV]

tp
tp

HLT (Sequential)
HLT (Iterative)
Offline

HLT (Sequential)
HLT (Iterative)
Offline

HLT (Sequential)
HLT (Iterative)
Offline

HLT (Sequential)
HLT (Iterative)
Offline

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

y
c
n
e

i

c

 

i
f
f
e
g
n
k
c
a
r
T

i

1.4

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

1.4

1.2

1

0.8

0.6

0.4

0.2

89

1
1

1.5
1.5

2
2

2.5
2.5

3
3

1
1

1.5
1.5

2
2

2.5
2.5

3
3

tp
tp

 [GeV]
 [GeV]

0
0.5
0.5

tp
tp

 [GeV]
 [GeV]

Figure 4.27: Tracking eﬃciencies as a function of pt for four diﬀerent multiplicities at both
magnetic ﬁeld settings of 0.2T (left) and 0.4T (right). Only tracks with pt ≥
0.5 GeV/c
were included.

chdN
hd

=1000,   L3 Field: 0.4T

chdN
hd

=4000,   L3 Field: 0.4T

HLT (Sequential)

HLT (Iterative)

Offline

]

%

[
 

p

 
/
 

p

 

t

t

HLT (Sequential)

HLT (Iterative)

Offline

]

%

[
 

p

 
/
 

p

 

t

t

4.5

3.5

5

4

3

2

2.5

1.5

1

0.5

0
0.5
0.5

4.5

3.5

5

4

3

2

2.5

1.5

1

0.5

0
0.5
0.5

0.6
0.6

0.7
0.7

0.8
0.8

0.9
0.9

1
1

1.1
1.1

1.2
1.2

1.3
1.3

0.6
0.6

0.7
0.7

0.8
0.8

0.9
0.9

1
1

1.1
1.1

1.2
1.2

1.3
1.3

1.5
1.5

1.4
1.4
tp
tp

 [GeV]
 [GeV]

1.5
1.5

1.4
1.4
tp
tp

 [GeV]
 [GeV]

Figure 4.28: Transverse momentum resolution for two multiplicities. Only tracks with
pt ≥

0.5 GeV were included.

Remarks on the tracking performance

The results indicate that the implemented iterative reconstruction chain has clear limi-
tations when going to higher multiplicities. The main reason for this shortcoming is the
relative high number of falsely identiﬁed track candidates from the HT. The subsequent
Cluster Fitter has no way of distinguishing these false track candidates from the valid
ones. As a consequence, the input to the ﬁtting procedure of a given cluster distribution
might be wrong, and the ﬁt procedure fails. This eﬀect becomes increasingly signiﬁcant
as the occupancy increases, which is demonstrated by the fact that both the tracking
eﬃciency and transverse momentum resolution deteriorates at higher multiplicities.

The relatively high number of fake tracks is mainly due to the complexity of the
formed structures in the parameter space.
In general, the peak formation depends on
both the location of the points on a track segment in space and the errors of the points.
In addition, energy loss and multiple scattering will cause the trajectories to deviate
from their circular pattern, which leads to an additional smearing of the peaks. All of
these eﬀects complicates the peak ﬁnding, since the structured backgrounds is diﬃcult
the distinguish from the real peak formations. As a consequence, the criteria for selecting
a valid peak needs to be relaxed, and background formations will be wrongly identiﬁed
as track candidates.

Computing requirements

The required processing time of the various steps of the HT has been measured on a
standard PC of 800 MHz Twin Pentium III, 256 KB L2 cache, running a Linux kernel v2.4.
Table 4.5 lists the CPU-time for a multiplicity of dNch/dη = 4000. The measurements are
resolved with respect to the diﬀerent computing steps, and the values thus corresponds
to the time needed by a single processing module when implemented according to the
scheme shown in Figure 4.25. The HT needs about 3.5 s to process the data within a
single sub-sector, resulting in a total HT processing time of 750 s for the complete event.
It should be noted that the algorithm was already optimized by utilizing look-up tables
for the accumulation procedure [51].

90

D
D
Process
HT
Histogram add
Peak ﬁnder

Locality CPU-time [s]
Sub-sector
Sector
Sector

3.5
1.2
1.8

Table 4.5: Measured CPU-time for the diﬀerent processing steps in the HT. The mea-
surement was done for an event sample with multiplicity of dNch/dη = 4000. The locality
refers to where the respective process is running, according to the data ﬂow shown in
Figure 4.25.

The results clearly indicate that the HT is very CPU-ineﬃcient. The reason is that
the processing is particularly I/O-bound as is needs extensive and repetitive access to
large accumulator arrays. Due to their relative large sizes and numbers, the memory
requirements are too large to ﬁt in the cache of the CPU, and thus the timing numbers
will scale poorly with the CPU clock frequency. However, the inherit degree of locality
and parallelism of the HT makes it suited for implementation in FPGA co-processors [51].

4.7 Summary

In this chapter two diﬀerent approaches for pattern recognition in the ALICE TPC for
the High Level Trigger system have been presented. The two approaches have both their
advantages and limitations with respect to complexity, implementation and performance.
Sequential tracking has shown to be eﬃcient for lower multiplicities, both with re-
spect to the quality of the reconstructed tracks and the required processing time. The
achieved tracking performance is at the same level as the Oﬄine reconstruction chain
for multiplicities dNch/dη
2000. At higher multiplicities, the tracking eﬃciency drops,
in particular in the low momentum regime. The reasons for this are well understood.
It is evident that the straight forward Cluster Finder approach fails to reconstruct the
cluster centroids once the occupancy is too high. This is due to its lacking capabilities of
unfolding the overlapping charge distributions.

≤

In the iterative approach, an attempt has been made to improve the cluster reconstruc-
tion by providing an estimate of the track parameters prior to a cluster ﬁtting procedure.
This was done by applying an implementation of the Hough Transform on the raw ADC-
data. Here the located track candidates were used as an input for a two-dimensional clus-
ter ﬁtting and deconvolution of the overlapping clusters. The advantage of this approach
is the simplicity and inherit degree of parallelization of the Hough Transform, which
makes it an clear candidate to be implemented very early on the readout chain within
the HLT system, possibly utilizing the FPGA co-processors planned for the HLT-RORC.
However, the HT produces a very high number of false positives, which complicates the
cluster ﬁtting. As a result, the resulting tracking performance is less than that of the
sequential approach.

Alternative solutions to improve the tracking eﬃciency for higher multiplicities are

discussed in Chapter 6.

91

92

Chapter 5

TPC Data Compression

The option to compress the detector data eﬃciently enables a potential increase of the
event rate to mass storage, even without performing event selection. All physics observ-
ables will beneﬁt from such an application. Applicable data compression schemes and
their expected performance on ALICE TPC data are presented in this chapter.

5.1 Introduction

The ultimate goal of data compression is to reduce the number of bits required to store
information, without any signiﬁcant information loss. From a information theory point
of view, data compression techniques can be divided into two main categories; lossy and
lossless. A lossy data compression does not guarantee a bit-by-bit reconstruction of the
original data set, and may therefore concede a certain loss of accuracy.
In exchange
however they may provide a greatly increased compression factor. Lossy compression
techniques are commonly applied to graphics and digitized sound, as the digitized repre-
sentations of such analogue phenomena includes a certain degree of noise anyhow. Most
lossy algorithms can be adjusted to diﬀerent quality levels, gaining higher accuracy in
exchange for less eﬀective compression. Lossless compression techniques, on the other
hand, guaranty that an exact duplicate of the input data stream is generated after a
compress/expand cycle. These techniques are applied when the loss of even a single bit
can aﬀect the information content signiﬁcantly.

In general data compression consists of taking a stream of symbols and transforming
them into codes.
If the compression is eﬀective, the resulting stream of codes will be
signiﬁcantly smaller than the original data size. The decision to output a certain code for
a certain symbol or data subset is based on a model. The model is a collection of data
and rules to process the input data and determine which codes to output.

The most eﬀective compression results can be achieved if the model is well adapted
to the underlying data. A well-known example is the approach used by the MP3-format
for audio ﬁles, where the results achieved when sound is compressed adapted to human
hearing characteristics are much better than the results from general purpose algorithms.
In case of TPC data the underlying data is the ADC-data, the clusters and tracks, while
all the relevant information is contained in the ﬁnal reconstructed physical observables.

93

Before discussing the various compression algorithms applied, a brief summary of the
characteristics of TPC data with respect to the various signal generation errors is given.

5.2 TPC signal generation and models

An eﬀective data compression scheme has to be well adapted to the underlying data model
and the noise already present in the data. As long as the allowed loss of accuracy does not
exceed the inherit accuracy already present in the data, the compression scheme will in
general not alter the information content. Prior to discussing applicable data compression
techniques for TPC data, the understanding of the signal generation and the resulting
inherit error sources within the original data stream is necessary.

Error sources within the TPC signal generation

The ultimate task of the TPC is to measure the kinematics of the traversing particles
and contribute to the particle identiﬁcation by energy loss measurement. Before the data
is readout out from the detector, several factors contributes to alter the data. These can
be summarized as follows:

•

•

•

•

Elastic and inelastic scattering.
Both prior to entering the sensitive volume of the TPC and within the gas of the
TPC volume, the charged particles interact with the surrounding material, causing
the particle track to deviate from its original trajectory. These eﬀects include both
elastic scattering from nuclei, and inelastic collisions with the atomic electrons of
the material. The energy loss is described by the Bethe-Bloch formula, and typically
follows a Landau distribution.

Diﬀusion.
The drifting electrons diﬀuse when drifting towards the end-caps of the TPC, and
hence inﬂuence the position resolution of the reconstructed space points. The drift-
ing cluster of electrons can be described by e.g. a 3D Gaussian distribution, where
the widths are determined by the diﬀusion constants of the gas (see Section 4.3.2,
page 48).

Electron attachment.
During the drift the electrons can be absorbed in the gas by formation of negative
ions.

Gas ampliﬁcation.
Each of the liberated electrons is subject to gas multiplication when entering the
readout chamber. This ampliﬁcation is described by a exponential probability dis-
tribution. The gas gain properties of the gas has also a strong dependence on the
temperature.

•

×

B-eﬀects.

E
The fact that the E and B ﬁelds are not parallel near the anode wires, leads to a
displacement of the drifting electrons when entering the readout chamber.

94

•

•

Front-end electronics.
When the image charge induced on the readout pads is processed through the front-
end electronics several sources contribute to the noise and distortion of the original
signal. Tail cancellation, pedestal subtraction and zero suppression are all lossy
compression techniques in their manipulation of the original raw data.

10-to-8 bit compression.
The ADC conversion gain is typically chosen so that σnoise corresponds to 1 ADC
count. This means that the relative accuracy increases with the ADC-values, and is
not needed for the upper part of the dynamic range. The ADC-values can therefore
be compressed non-linearly from 10 to 8 bits leading to a constant relative accuracy
over the whole dynamic range (Figure 5.1). The conversion from 10 to 8 bit of
the ADC-values is thus a data volume reduction with some information loss, but
keeping the relative accuracy for the single ADC-value.

All of these eﬀects modify the original data, preventing an exact event reconstruction.
Therefore, the complete readout system must be considered in order to decide to which
extend lossy or lossless data compression should be applied. Also, since the relevant
information in the data lies in the ﬁnal outcome of the analysis, any lossy compression
should be evaluated from its impact on the reconstructed physics observables.

Local and global TPC data models

In the context of TPC data compression, the applicable compression techniques can be
divided into two main categories; local and global modeling. Local modeling techniques
are applied on the scale of raw ADC-data, i.e. after a compress/expand cycle the re-
sulting data will still consist of the same ADC-data format. In the latter case the data
is described within a more global model, the clusters and tracks. In such a compression
scheme, the ﬁnal uncompressed data may not consist of ADC-data, but rather cluster and
track parameters themselves. The diﬀerence between these two approaches is obviously
the elimination of the original raw data from the data stream. However, as the vital
information in the TPC data is contained in the reconstructed particle trajectories, there
is in principle no need to keep the raw ADC-data.

5.3 TPC data format and coding

TPC readout data format

The ALICE TPC raw data format which will be used during the running of the experiment
is deﬁned by the ALTRO chip in the TPC readout-electronics chain, Section 2.4.3. In
addition to the digitization of the input signals, the ALTRO performs the zero-suppression
of the digitized data. This is basically a data compression technique that consists of
detecting the hits in the time-direction and discarding the noise in between by replacing
it with zeros. It is implemented using a sequence detection scheme, which is based on
the rejection of isolated samples with a value smaller than a threshold level. Both the
thresholding and hit-ﬁnding are lossy data compression techniques, as small clusters or
tails of the clusters might be discarded from the data.

95

The zero-suppression results in long sequences of zeros between the hits. Run-Length
Encoding (RLE) subsequently compresses these zeros. The principle of RLE is to replace
a sequence of identical symbols by a certain character or tag, and the length of the
sequence. In case of the TPC the RLE is performed on the zeros. This is thus equivalent
to storing only the hits and their positions. As the RLE does not modify or discard any
data, it is a lossless data compression technique.

The ALTRO data format consists of a list of sequences written in a back-linked struc-
ture [35], where each sequence is described by three ﬁelds: Temporal information (tem-
poral position of the last time-bin in the sequence), sequence length (number of time-bins
in the sequence) and the ADC-values themselves.

Simulation and coding

Simulated data were generated within the AliROOT framework Section 4.3. All events
were generated using the HIJING parameterization for diﬀerent multiplicities, and the
standard TPC slow simulator including the complete detector response simulation pack-
age. The output from the simulation thus correspond to zero-suppressed 10-bit ADC-
data, whose format is deﬁned within special AliROOT data containers. Before any of the
data compression techniques presented in this chapter were applied, the data was further
modiﬁed in three steps:

1. A 45-degree (relative to the beam axis) cone was cut out of the data in order to
remove data from particles which are not in the geometrical acceptance of the outer
detectors. This will be done also during readout of the TPC detector, as the low
pt tracks crossing the TPC under small angles are too problematic to resolve. This
cutout reduces the original data volume by

40%.

∼

2. The simulated 10-bit ADC-data were transformed into 8-bit data by a 10-to-8 bit

table shown in Figure 5.1.

3. The ﬁnal 8-bit ADC-data were then RLE and written to binary ﬁles for subsequent
comparison to the compressed data. The adapted data format uses a coding scheme
whose size resemble the ALTRO-format. Further details about the format used can
be found in Appendix B.2.

All compression ratios in the following are calculated as

Compressed size

Original size ×

100

[%]

where the original size refers to the RLE 8 bit ADC-data.

5.4 Local modeling techniques

In [55] several data compression techniques, based on local data modeling, were applied on
TPC data. Both data from the NA49 experiment, and simulated data from the ALICE
TPC, were studied. A brief introduction to the diﬀerent algorithms applied and the
achieved result is given in the following.

96

e
u
l
a
v
 
t
i
b
 
8

250

200

150

100

50

0
0

200

400

600

800

1000
10 bit value

Figure 5.1: Plot of the 10-to-8 bit conversion table used [80].

5.4.1 Lossless TPC data compression

In general, most lossless compression techniques are based on entropy coding. Such algo-
rithms exploit any possible redundancy within the information message, and the fact that
the diﬀerent symbols within a message are not equally probable. A symbol, which has
a very high probability of occurrence, will be coded using very few bits, while symbols
with a low probability are coded with a larger number of bits. For TPC data, such a
scheme can exploit the fact that diﬀerent ADC-samples are not equally probable: Small
ADC-values occur more often than larger ones, Figure 5.2. The theoretical limit on the
average word size that can be achieved with such a strategy is given by the entropy of
the data source. For TPC data the entropy can be computed as

E =

p(A) log2 p(A).

−

XA∈Ω

(5.1)

Here p(A) is the probability of having a ADC-value, A, in the data, and Ω is the set of all
possible words that are contained in the data source. The diﬀerence between the number
of bits used to represent a single character and its information entropy is the potential
for entropy coding techniques.

Huﬀman Coding One of the most common entropy coding techniques is the Huﬀ-
man Coding [81]. This algorithm has proved to be easily implemented and achieve good
compression results without extensive processing power. The basic idea is to assign each
input signal to a leaf of a binary tree, the so-called Huﬀman Tree. Each branch on this
tree is either assigned the 0 or 1 bit, and the path from the root node to the leaf deﬁnes
the code used for this symbol. By adapting the tree to the probability distribution within
the data sample, symbols with higher probabilities get shorter codes and vice versa.

Arithmetic Coding A somewhat more complicated but potentially more eﬀective
compression technique is Arithmetic Coding. In contrast to Huﬀman coding, this approach
does not produce a code for each symbol, but rather a code for an entire message. It is
therefore not restricted to using integral number of bits per code, and is thus potentially
more eﬀective than the Huﬀman Coding. This compression technique can theoretically
approach the lower limit given by the entropy [82]. The main idea is to assign to every

97

s
t
n
u
o
C

6

10

5

10

4

10

3

10

2

10

0

50

100

150

200

250
ADC-value

Figure 5.2: Distribution of ADC-values in simulated ALICE TPC-data.

symbol an interval between 0 and 1, with a size according to the occurrence probability
of that symbol. For each input symbol the interval is shrunk to the range assigned to the
new symbol. In this way the interval gets smaller and smaller and the input stream of
symbols is replaced by a single ﬂoating point number representing the encoded message.
The main drawback with Arithmetic Coding is that a relatively large number of operations
are needed to encode a single symbol.

Diﬀerentiation A common approach for lossless graphics compression is diﬀerenti-
ation, which exploits the fact that adjacent symbols may be similar. This is particularly
true for TPC-data, where adjacent ADC-values are highly correlated. In this case the
distribution of the derivative of the input has lower entropy than the data itself.

Code table coding A similar approach to diﬀerentiation is predictive encoding. In
this case the next value in the data stream is guessed based on the previous sent value,
and only the diﬀerence between the guess and the actual value is sent. Thus, if the guess
is close to the actual value, the entropy of the diﬀerence is small.

5.4.2 Lossy TPC data compression

The best compression results can always be achieved if small noise-like changes of the
data can be tolerated. The term lossy originates from the fact that these methods do not
allow a bit-by-bit reconstruction of the original data set. Lossy compression techniques
usually introduce some kind of quantization of the data, thereby lowering the required
number of bits needed to store the information. In the simplest form, the data samples
can be quantized into intervals corresponding to the required resolution.

Vector Quantization One of the more sophisticated quantization techniques is Vec-
tor Quantization [83]. Here statistical dependencies between successive data samples are
exploited. Instead of quantizing data samples independently, several samples are grouped
together to form a vector of data samples. In the TPC data, such a vector can typically
be a sequence of ADC-values. The vector is compared to entries in a codebook of vectors,
and the index of the best matching vector in the codebook is stored. In order to achieve
eﬀective compression factors the codebook has to be trained on the statistical properties

98

Entropy Relative event size [%]

Type of encoder
Zero suppressed raw event size
Arithmetic Coding
Code table coding
Huﬀman Coding
Vector Quantization

8
6.4
5.7
5.2
5.1-3.8

100
80
71
65
64-48

Table 5.1: Compression performance for local data modeling techniques on simulated
ALICE TPC data [55]. The entropy is given as the average number of bits used to
encode a sample. For the Vector Quantization the entropies depends on the treatment of
the residuals.

of the data. Since such a codebook is pre-produced, only the given vectors are available
to represent the data. Since this can lead to rather large quantization errors, also the
diﬀerence between the input data and the selected codebook entry, the residuals, can be
stored. These residuals can further be quantized and entropy encoded.

5.4.3 Results

The resulting compression factors obtained with the algorithms described above are sum-
marized in Table 5.1. For the Huﬀman Coding, separate trees were built for the ADC-
values and the header information, i.e. position and length. The Code table coding
implemented a table that included a best guess for each combination of the preceding
values. The results did not improve if the table was dependent on two of the preceding
values. For the Vector Quantization it was shown that the impact on the physics observ-
ables is measurable but small, with the space point resolution being the most sensitive
quantity.

5.5 Global modeling techniques

The standard compression techniques listed in Table 5.1 are all applied on the scale
of ADC samples and sequences. However, the TPC data can be described within a
more global model, the tracks and their corresponding clusters. Such models can be
exploited by compression methods by using it to transform the data into an more eﬃcient
representation, and thereby reducing the redundancy information (entropy) in the input
data. Instead of storing the TPC data on the ADC-level, the information can be stored on
a higher extraction level such as cluster and track parameters. The main diﬀerence of such
models from the local techniques is that they require some form of pattern recognition
to be done prior to the compression scheme. In this context the purpose of the pattern
recognition is diﬀerent from the normal case since the aim is not to extract physics
information about the particle kinematics, but to build a data model.

In general the TPC pattern recognition scheme is a two-folded process, which cor-
responds to reconstructing the space points from the clusters and combining the space

99

points into tracks, Chapter 4. From this point of view, there are two levels in which TPC
data can be represented:

Space point data.

•

•

Space point data relative to their tracks.

The ﬁrst case corresponds to storing only the space points and their properties. Thus,
only a cluster ﬁnding procedure needs to be done prior to the compression. In the latter
case, it is assumed that a zeroth-order tracking step is performed in which the space
points are encoded with respect to their distance to the tracks they belong to. Both of
these methods will eliminate the original raw data from the data stream, as the ﬁnal data
set after decompression in either case will consist of a list of clusters.

5.5.1 Storing cluster data

The TPC clusters represent the three dimensional space points of the particle trajectories.
The information which is needed from an analysis point of view consists of the three
dimensional coordinates, the shape of the cluster and the total charge of the corresponding
cluster. To ﬁrst order each of these values would have to be stored with the required
number of bits to ensure the information content. However, from the way the data is
organized in the detector, some simpliﬁcations can be made.

The space points are reconstructed from the cluster centroids, which is calculated from
the two-dimensional digitized charge distribution is the pad-row-plane. The coordinate
in one dimension is thus given by the radius of the pad-row plane, and is consequently
the same for all the clusters on the same pad-row. The remaining two coordinates can
therefore be encoded with respect to the pad-row they are located on.

The number of bits required to store the space point coordinates is determined by the
inherit resolution within the data. The typical space point resolution of the ALICE TPC
is at the order of 0.08 cm and 0.1 cm for the pad and time direction respectively. With
a pad-width of 0.4 cm and 0.6 cm for the inner and outer TPC chambers respectively
0.1 in pad-coordinates. In order to avoid
(Table 2.1), this corresponds to a precision of
any deterioration of this resolution, this coordinate should thus be stored with at least 2
decimal precisions. Given that the maximum number of pads on a single pad-row is 140,
a total number of 14 bits1 is needed to encode the pad-coordinate. Similar arguments
hold for the time-direction whose absolute value can take up to 512 time-bins. With a
2 decimal precision this would then correspond to a spatial resolution of
0.006 cm and
thus a total number of 16 bits for the encoding size.

∼

∼

Table 5.2 lists the number of bits required to encode the positions using 1, 2 and 3 dec-
imals for both pad and time direction together with their respective spatial resolutions2.
Figure 5.3 shows the impact on the space point resolution of keeping the respective levels
of precision within the data. In the plots also the resolution within the original data, i.e.
1The number is estimated from the assumption that one need to encode numbers up to 140 00,

214=163 84.

2These numbers represent the maximum number of bits required to store the coordinates. E.g. for
the pad-direction, the number of pads varies for each pad-row and the encoding size may be reduced by
a bit on the innermost pad-rows.

100

]

m
c
[
 

d
a
p

0.0915

0.091

0.0905

0.09

0.0895

0.089

0.0885

0.1006

0.1004

0.1002

0.1

0.0998

0.0996

0.0994

0.0992

0.1155

0.115

]

m
c
[
 

e
m

i
t

0.1145

0.114

0.1135

0.113

0.134

0.1335

0.133

0.1325

0.132

10
10

11
11

12
12

13
13

14
14

15
15

12
12

13
13

14
14

15
15

16
16

17
17

18
18

19
19

20
20

16
16
Number of encoding bits
Number of encoding bits

17
17

18
18

19
19

Number of encoding bits
Number of encoding bits

Figure 5.3: Space point resolution for diﬀerent encoding sizes of the cluster centroids.
The upper plots show the result for the outermost TPC chambers, while the inner plots
shows the inner chambers. The shaded area shows the resolution within the original data
set. The results were obtained by comparing the cluster centroids and the position of
the corresponding simulated particle trajectory. The values correspond to the standard
deviation from a Gauss ﬁt.

keeping the full ﬂoating point precision, is shown. As expected there is some impact on
the resolution by keeping only a single decimal precision for both directions. No diﬀer-
ence can however be seen when using 2 and 3 decimals. This indicates that the a total of
14 and 16 bits is needed to store the pad and time coordinate respectively without any
information loss.

In Table 5.3 the cluster parameters together with their required encoding size are
listed. In addition to the two-dimensional centroid, also the cluster shape and the total
charge are needed. In order to store the cluster widths with the same level of precision
as for the centroids, each direction is encoded with 8 bits. The total cluster charge is
stored using 12 bits. In total 58 bits are needed to store the complete cluster using this
representation.

The above storage considerations can now be compared to the required size of storing
the raw-data itself. However, the required encoding size for the raw-data depends in
general on the size of the clusters in terms of number of bins in the pad-row-plane, and
also on the multiplicity. For instance, assuming a single ’ideal’ cluster whose size is 3
3
×
bins, a total of 3
8=72 bits are required to store the ADC-values. Furthermore, header
information is needed to encode the position of the ADC-values in the pad-row-plane. This
typically contains the respective pad-numbers on which the individual ADC-sequences
are located and the relative time-information (Appendix B.2). Each pad-number can be
8=24 additional bits are required for the 3 diﬀerent pad-
encoded using 8 bits, thus 3
8=32 bits are needed for encoding of zero-sequences
numbers. For the time-information 4
and RLE tags. This results in a total of 192 bits, Table 5.3. Comparing with the encoding

3
×

×

×

×

101

d
d
Cluster centroid Size (bits)

Pad centroid

Time centroid

Precision [cm]
Inner Outer
0.04
0.004
0.0004

0.06
0.006
0.0006

0.06
0.006
0.0006

11
14
18
13
16
19

Table 5.2: Space point data and their required encoding size.

Cluster parameters Size (Bit)

Pad coordinate
Time coordinate
σpad
σtime
Total charge
Total

14
16
8
8
12
58

Raw-data parameters Size (Bit)
8=72

3

ADC-values
Pad-numbers
Time information

3
×
3
3

×
×

×
8=24
32=96

Total

192

Table 5.3: Data parameters and their encoding size. Left table: Cluster parameters
and the estimated number of bits required to store the respective parameters within the
intrinsic resolution. Right table: Raw-data parameters and their estimated encoding size
assuming a single isolated cluster of size 3

3 bins.

×

size for the cluster parameters, a reduction of about 70% may be achieved when storing
the cluster parameters instead of the raw-data. This estimate however assumes that only
one isolated cluster is present on a single pad-row, which of course in general is not the
case. The relative size of the header information is strongly dependent on the occupancy,
and will decrease as a function of multiplicity due to the fact that the sequences on a
given pad are encoded according to their internal distance and the pad-number is stored
only once per pad. This is illustrated in Figure 5.4 which shows the relative size of the
cluster data for diﬀerent multiplicities. Here, the HLT Cluster Finder algorithm was
used to reconstruct the clusters, Section 4.5.1. For low multiplicities the ratio is
25%,
30% for higher multiplicities. The fact that the ratio for very low
while it increases to
multiplicity is
5% lower than the value estimated above, can be explained by the fact
that the clusters are in general larger than the assumed 3

3 bins.

∼

∼

∼

×

5.5.2 Track and cluster modeling

Given the resolution of the space points and the size of the detector volume, the space
points will lead to a rather large encoding size of the clusters. However, if the track
reconstruction has been done the clusters are assigned to their respective tracks. Within

102

]

%

[
 
e
z
i
s
 
r
e
t
s
u
l
c
 
e
v
i
t
a
l
e
R

50

45

40

35

30

25

20

15

10

5

0

0
0
100

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

7000
7000

chdN
chdN
hd
hd

Figure 5.4: Ratio between cluster data and raw-data as a function of multiplicity. The
cluster data was stored using the representation listed in Table 5.3.

this data model, the cluster positions can be encoded relative to the track parameters
and thereby lowering the required number of bits needed to store the information.

The basic idea of this compression technique is to encode the data within the context
of the reconstructed tracks and their corresponding clusters. Thus, the pattern recog-
nition has to be solved prior to the data compression. Once the track reconstruction is
completed, the tracks can be represented by helix parameters, and the clusters by their
two-dimensional crossing point with the pad-row-plane. Let (Λpad, Λtime) denote a in-
tersection between a track and the pad-row plane, and (λpad, λtime) the centroid of the
corresponding cluster assigned to the track, Figure 5.5. Further, let the distance between

Reconstructed trajectory

Cluster shape

Padrow−plane

103

Figure 5.5: Deﬁnition of a residual. The residuals are deﬁned as the distance, δ, between
the track crossing point, Λ, and the cluster centroid, λ, of the assigned clusters.

l
L
d
the track crossing point and cluster position, the residuals, be deﬁned as

δpad = Λpad

λpad,

δtime = Λtime

λtime.

(5.2)

−

−

The residuals represent small deviations of the cluster position from the track model, and
are subject to the intrinsic detector resolution. This oﬀers the possibility to quantize the
residuals with a transfer function whose resolution is adapted to the detector noise and
resolution,

δpad

δpad,Q =

→

δpad
ǫpad

,

δtime

δtime,Q =

→

δtime
ǫtime

,

(5.3)

where ǫ is the respective quantization intervals chosen for the two directions. The resulting
numbers can now be stored with a minimum number of bits required to encode the
quantized residuals.

In addition to the cluster positions, also the cluster shape and total cluster charge is
needed for the later oﬄine analysis. As discussed in Section 2.4.1 the shape of a cluster is
determined by the detector speciﬁc variables and the track inclination with the pad-row-
plane. The cluster shape can thus be parameterized as a function of the track parameters,
which allows the cluster shape to be restored without storing the actual shape. Optionally,
the deviation of the cluster widths from the model can be stored.

Regarding the cluster charge, its value is determined by the number of primary col-
lisions by the traversing particle in the gas which is a random variable described by a
Poisson distribution. Due to secondary ionization and gas gain ﬂuctuations the total
charge is described by a Landau distribution with a long tail. Because of this, the repre-
sentation of the cluster charge can not easily be reduced and is thus stored as the original
value as calculated during cluster reconstruction.

Encoding scheme

A compression scheme which stores the track and cluster information in a compressed
format has been implemented, and will be described in the following.

The input to the compression scheme are the reconstructed tracks and their corre-
sponding assigned clusters. For every pad-row-plane the residuals are calculated and
quantized according to Equations 5.2 and 5.3. The encoding scheme assumes the tracks
have the following properties3:

The ﬁrst and last point of the trajectory correspond to the innermost and outermost
pad-row of the TPC respectively.

A track may contain clusters from diﬀerent TPC sectors.

In this scheme there is no need to store the pad-row number for every cluster, as the
initial assumption is that there is a corresponding cluster on every successive pad-row
throughout the TPC radius. Instead, only 1 bit is required to ﬂag if this is not the case,
i.e. to denote whether a cluster is present or not on a particular pad-row. Furthermore,
3This deﬁnition only serves to optimize the compression scheme, and has nothing to do with the actual

tracking procedure being used.

•

•

104

an additional bit is also required to denote a possible change of the TPC sector. This may
happen if the trajectory crosses the boundary between two neighboring TPC sectors.

A given data sample is encoded as illustrated in Figure 5.6. Each track contains a

.
.
.

Charge

Time
Pad
Sector

Charge

Time
Pad

Cluster
presence

Sector
change

.
.
.

1
1
0

.
.
.

1
0

Track
parameters

Figure 5.6: Data compression encoding scheme.

bitmap which encodes the cluster presence. For every pad-row 1 bit is used to ﬂag whether
a cluster has been assigned or not on that pad-row. If no cluster is present, only the ’0’
bit is written, otherwise the cluster information is also stored. A second bitmap is used
to store any change in TPC sector number. If a cluster in the list belongs to a diﬀerent
sector than the previous, this is stored by setting this bit to ’1’. In that case, the new
sector number is stored in addition to the cluster information.

The compressed data sample consists of the track parameters of every track together
with the coded cluster information for the corresponding clusters. The track parameters
correspond to a minimum set of required parameters to describe a helix, Appendix A.2.
These are the curvature of the circular motion, κ, the coordinates of the point at the
distance of closest approach (DCAO), (rDCAO, φDCAO, zDCAO), and the dip-angle of the
track, λ. In Table 5.4 these parameters are listed together with their respective encoding
sizes. The size of the cluster parameters depends on the choice of quantization intervals,
ǫ, in Equation 5.3. The minimum number of bits required to store a quantized residual
δQ is given by

nbits = max[

)

δQ
log(
|
|
log(2)

] + 1

nbits(ǫ)

≡

(5.4)

where max[x] denotes the closest integer larger than x. The extra bit is needed to encode
the sign. Thus for a given data sample the number of bits used to encode the residuals are
calculated from the maximum value in the sample using Equation 5.4. The track–cluster
parameters and their respective sizes are listed in Table 5.4.

105

D
D
D
D
Track parameters Size (Byte)

Track–cluster parameters Size (Bit)

κ
φDCAO
rDCAO
zDCAO
λ

4 (ﬂoat)
4 (ﬂoat)
4 (ﬂoat)
4 (ﬂoat)
4 (ﬂoat)

Cluster presence
Sector / change
Cluster charge
δpad,Q
δtime,Q

1
6 / 1
12
nbits(ǫ)
nbits(ǫ)

Table 5.4: Cluster and track parameters and their respective size used in the compression
scheme. The track parameters are deﬁned at the distance of closest approach to origin
(DCAO). The number of bits used to encode the residuals is determined by the maximum
value of the respective residuals in the sample, and are calculated from Equation 5.4.

The compress/expand cycle

In general, any implementation of a data compression scheme consists of two separate
steps: compression and expansion. In the ﬁrst step the given input data sample is encoded
following the implemented encoding scheme. The compressed data sample can then be
compared to the original in order to evaluate the compression ratio. Then, during the
expansion, the compressed data sample is decompressed and restored into the same data
format as the original data. Finally, the uncompressed data can be compared to the
original data set in order to evaluate any impact on the relevant information content.

Figure 5.7 show a ﬂow diagram of the complete compress/expand cycle which has
been applied. In a ﬁrst step the clusters and tracks are reconstructed from the 10 bit
ADC-data. The resulting clusters and tracks are then encoded according to the scheme
illustrated in Figure 5.6. The remaining clusters in the event which were not assigned to
any tracks may optionally be written in addition to the compressed data, or they may be
discarded all together. Alternatively a special selection of the remaining clusters can be
done using a cluster analyzer (discussed on page 110). The data is then uncompressed
by restoring the cluster centroids and shape from the compressed data, and optionally
merging them with the remaining cluster data. The ﬁnal list of clusters is then given to
the oﬄine track ﬁnder program. Once the tracks have been reconstructed, the resulting
tracking performance can be evaluated in order to check the impact the compression
has on the tracking performance. Furthermore, the compressed data size is compared
to the original raw-data size, represented by the 8 bit ADC-data, to obtain the relative
compression ratios.

On choosing the quantization intervals

The quantization interval, ǫ, in Equation 5.3 determines the accuracy of the compression
scheme. This is an essential parameter in the algorithm as it limits both the accuracy
and the achievable compression factor of the algorithm. Selecting a very coarse quan-
tization interval will in general allow for a greater compression factor as the number of
bits required to encode the residuals will be small. Any such gain in compression factor

106

Cluster
analyzer (opt.)

Store remaining
clusters (opt.)

Compress track and
cluster information

10−to−8 bit
conversion

10 bit
ADC data

Reconstruct
event

Uncompress data

Reconstruct
tracks

Evaluate tracking
performance

Calculate
compression factor

Figure 5.7: Flow diagram of the implemented data compress/expand cycle.

will however come at the expense of a correspondingly lower space point resolution in the
ﬁnal uncompressed data sample.

Figure 5.8 illustrates the impact of the quantization steps on the space point resolution.
The plots show the ﬁnal space point resolution after a compress/expand cycle as a function
of the choice of quantization steps in the pad and time direction, respectively. The event
sample correspond to a low multiplicity event (dNch/dη = 100), i.e. well separated clusters
and tracks, and the events were reconstructed by the HLT sequential track reconstruction
chain, Section 4.5. As one would expect the space point resolution deteriorates as the
quantization intervals increases. For ǫ=0.05 cm, the loss is about 2% and 1% for the pad
and time direction, respectively.

Figure 5.9 shows the distribution of the quantized residuals for ǫ=0.05 cm. The distri-
butions are approximately exponential. The maximum value of the distributions are 104
and 130 for the pad and time distribution respectively, which according to Equation 5.4
means that 8 and 9 bits are required to encode the residuals with a ﬁxed bit-rate. How-
ever, given that values are not equally probable, further compression is possible by entropy
coding. The entropy of the samples has been calculated according to Equation 5.1, and
suggest a theoretical lower limit on the average word size needed to encode these residuals
of 2.4 and 3.6 for the pad and time direction respectively.

Cluster widths

In addition to the cluster centroids, also the space point errors are needed for the track
reconstruction. For the track ﬁnding algorithms, these errors are for practical purposes
calculated from the cluster widths using suitable proportional factors optimized from the
simulated data, Section 4.5.1. As the cluster widths can be parameterized according to

107

]

m
c
[
 

d
a
p

0.115

0.11

0.105

0.1

0.095

0.09

0.085

0.08

0.112

0.11

0.108

0.106

0.104

0.102

0.1

0.098

0.096

0.094

s
t
n
u
o
C

5
10

4
10

3
10

2
10

10

1

0
0

0.02
0.02

0.04
0.04

0.06
0.06

0.08
0.08

0.1
0.1

0.12
0.12

0.14
0.14

0
0

0.02
0.02

0.04
0.04

0.06
0.06

0.08
0.08

0.1
0.1

0.12
0.12

0.14
0.14

0.16
0.16
pad
pad

 [cm]
 [cm]

0.18
0.18

0.16
0.16
time
time

 [cm]
 [cm]

0.18
0.18

Figure 5.8: Impact of the quantization scheme on the space point resolution. The upper
plots show the result for the outer TPC chambers, while the inner plots shows the inner
chambers. The shaded area corresponds to the resolution within the original data sam-
ple. The resolutions were obtained from isolated clusters from all primary tracks with
pt ≥
0.1 GeV. The values correspond to the standard deviation from a Gauss ﬁt of the
respective distributions.

Entropy 

 2.44

Entropy 

 3.65

0

20

40

60

80

100

120

140

160

180

200

0

20

40

60

80

100

120

140

160

180

200

pad,Q

time,Q

Figure 5.9: Distribution of the quantized residuals for a quantization interval of
ǫ=0.05 cm. The data sample used are the same as used in Figure 5.8.

]

m
c
[
 

e
m

i
t

0.122

0.12

0.118

0.116

0.114

0.112

0.11

0.14

0.138

0.136

0.134

0.132

0.13

s
t
n
u
o
C

5

10

4

10

3

10

2

10

10

1

108

˛
˛
d
˛
˛
d
d
»
d
»
the track parameters (Equation 4.17), this can be exploited by the compression scheme
by removing the cluster shape information from the data stream and restoring it from
parameterization during the decompression step.

However, since the cluster generation in the TPC is a result of stochastic processes, the
cluster shape and also the space point errors are subject to ﬂuctuations. As a consequence,
the parameterization of the cluster shape may lead to deviations of the space point errors
from the “true” errors. This can furthermore be a potential source for loss in tracking
performance as e.g. the track reconstruction algorithm may make wrong decisions based
on χ2-criteria if the provided error estimates diﬀer signiﬁcantly from the true ones. Such
loss would in particular be expected to occur if the occupancy is large, in which the space
point assignment critically depends on a good estimate of the individual space point
errors.

Efficiency loss: Stored 

Efficiency loss: Stored 

param

s+

param

true

]

%

[
 
s
s
o

l
 

y
c
n
e
c

i

 

i
f
f
e
g
n
k
c
a
r
T

i

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0
0
0

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

7000
7000

chdN
chdN
hd
hd

Figure 5.10: Integrated tracking eﬃciency loss due to removing the cluster shape informa-
tion from the data stream (σparam). Instead of storing the cluster widths for each cluster,
the widths are parameterized by Equation 4.17. The eﬃciency loss when the original
cluster widths are stored for clusters where the measured widths deviate substantially
from the parameterized ones are also shown, Equation 5.5 (σparam + σtrue).

Figure 5.10 shows the loss in tracking eﬃciency due to removing the cluster shape
information from the data stream as a function of multiplicity. In this case clusters re-
constructed by the Oﬄine cluster ﬁnder was used as input in order to avoid any eﬀects
resulting from diﬀerence in performance between the HLT and Oﬄine cluster ﬁnder al-
gorithms. The loss was estimated by taking the diﬀerence between the integrated Oﬄine
tracking eﬃciency in the case where the cluster shape was parameterized according to the
track parameters and the original data set in which the cluster shape was stored. The
results indicate that the loss increases slightly as a function of multiplicity, and reaches
the maximum of

1.4% for dNch/dη = 6000.

In order to minimize such a loss, the original shape information can be kept by storing
the deviation of the parameterized widths from the true ones. Alternatively, the widths
can be stored only for clusters whose widths diﬀers from the parameterization more than

∼

109

s
s
a certain threshold value. The scenario is also shown in Figure 5.10. In this case the
cluster widths has been stored for clusters where

σparam

σtrue

>

σtrue.

|

−

|

1
2

(5.5)

∼

∼

This selection criteria includes
10-15% of all the clusters in the samples, and results
in no observable eﬃciency loss at dNch/dη = 1000. However, at higher multiplicities,
dNch/dη = 6000, a eﬃciency loss of

0.8% is observed.
The results indicates that the Oﬄine track ﬁnder algorithm is rather sensitive to the
estimation of the cluster shape. This may be explained by the fact that the Oﬄine track
ﬁnder has been highly tuned to the space point errors obtained from the simulated cluster
shape [84]. In order to further optimize the compression scheme in terms of cluster shape
and space point resolutions, a detailed understanding of the TPC response and the impact
on the ﬁnal Oﬄine reconstruction chain is needed. Such a study can however only be done
using real TPC data, as these data will contain other eﬀects as well. The results presented
in the following are obtained using only a parameterization of the cluster widths.

Remaining clusters

After the track ﬁnding procedure, a certain amount of remaining clusters will be present.
These are clusters which were not assigned to any tracks during the track reconstruction
procedure. The are several possible sources of these clusters, and they depend foremost
on the eﬃciency of the track ﬁnding algorithm at hand. In general the most common
source is particles with a very low pt whose tracks could not be reconstructed due to
large multiple scattering and energy loss. This is in particular true for the so-called δ-
electrons, which are produced when particles transfer a relatively large amount of their
energy into a single electron when traversing through the gas. The typical signal from
such particles forms a large continuous cluster area because of their large inclination with
the pad-row-plane. Since these particles are in general not of any interest, they will not
be included in any Oﬄine analysis and can therefore be considered as noise. Thus from
a data compression point of view, such clusters should be removed from the data stream
as they do not contain any relevant information.

The list of remaining clusters may however also include clusters that are not originating
from “noise” particle tracks, but are merely a result of an ineﬃcient track reconstruction
chain. In order to minimize any loss of accuracy and eﬃciency in the output data, the
remaining clusters may be written in addition to the compressed data. These clusters
can however not be encoded with respect to any track model, and thus needs to be stored
with the cluster parameters as proposed in Table 5.3. This will then contribute with an
additional overhead to the compressed data size. On the other hand, if they are completely
disregarded from the data, vital information may be lost as the list may contain “valid”
clusters which were not assigned to any tracks by the track reconstruction algorithm. Such
a loss can be signiﬁcant if the tracking performance of the applied tracking algorithm is
lower than the one that will be used for the later oﬄine analysis.

An option to store the remaining clusters, or a certain selection of them, has been
included in the compression scheme (Figure 5.7). Here, a cluster analyzer can classify
clusters based on a selection scheme, and only store a fraction of the cluster list. The

110

analyzer selects remaining clusters located within the detector region being used for the
seed-ﬁnding procedure in the Oﬄine track ﬁnder algorithm, Section 4.3.3.

The impact on the tracking eﬃciency from removing the remaining clusters is illus-
trated for a given data sample in Figure 5.11. The two plots display the Oﬄine tracking

Removing all remaining clusters

Keeping remaining clusters on padrows for "seed-finding"

Compressed

Original

Compressed

Original

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

1.4

1.2

1

0.8

0.6

0.4

0.2

0

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
T

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

0.5
0.5

1
1

1.5
1.5

2
2

2.5
2.5

3
3

TP
TP

 [GeV]
 [GeV]

TP
TP

 [GeV]
 [GeV]

Figure 5.11: Impact on tracking eﬃciency from disregarding the remaining clusters in
the compression scheme. The data sample corresponds to an event with multiplicity of
dNch/dη = 1000. The plots show the tracking eﬃciency as a function of pt before (original)
and after (compressed) the compress/expand cycle has been applied. Left: All remaining
clusters are removed from the data stream. Right: Only remaining clusters which are
located in the region where track seed-ﬁnding is performed are stored.

eﬃciency as a function of pt both before and after a compress/expand cycle.
In both
cases, the input clusters and tracks to the compression scheme were reconstructed by
the HLT sequential track reconstruction chain, Section 4.5. In the left plot, all the re-
maining clusters have been removed from the data sample. A signiﬁcant loss of tracking
performance is observed, resulting in an integrated loss of about 16%. In the right plot,
the cluster analyzer was applied to the remaining clusters in which the resulting selection
were stored while the rest is removed. In this case, the integrated eﬃciency loss is reduced
to about 1.5%.

5.5.3 Results

In order to investigate the performance of the compression scheme outlined in the previous
sections, the compress/expand cycle was applied to various simulated data samples. The
input to the compression scheme presented in the following are clusters and tracks which
were reconstructed using the implemented HLT sequential tracking approach presented
in Section 4.5. All tracking performance results and impact thereon refer to the standard
Oﬄine track ﬁnder.

Impact on the momentum and dip-angle resolution

One of the observables, which should be sensitive to the compression, is the momentum
resolution. According to Equation 4.3 the relative transverse momentum resolution de-
pends on both the space point resolution and the number of space points assigned to

111

the track. In Figure 5.12 the impact on the relative momentum resolution is shown for
an event with multiplicity dNch/dη = 1000. In the left plot the resolution is shown as a

chdN
hd

=1000,   L3 Field: 0.4T

chdN
hd

=1000,   L3 Field: 0.4T

Compressed

Original

]

%

[
 

T
P

 
/
 

T
P

 

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

]

%

[
 
n
o
i
t
u
o
s
e
r
 

l

m
u
t
n
e
m
o
m
 
e
v
i
t
a
l
e
r
 
d
e
t
a
r
g
r
e
t
n

I

1.02

1

0.98

0.96

0.94

0.92

0.9

0.88
0
0

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

1
1

1.2
1.2

1.4
1.4

1.6
1.6

0.05
0.05

0.1
0.1

0.15
0.15

0.2
0.2

0.25
0.25

0.3
0.3

0.35
0.35

TP
TP

 [GeV]
 [GeV]

 [cm]
 [cm]

Figure 5.12: Impact on the relative momentum resolution from the compression. Left:
Relative momentum resolution as a function of pt for both original data set and the
In the compression scheme the quantization intervals were
compressed data sample.
ǫ=0.05 cm. Right: Integrated relative momentum resolution as a function of the quanti-
zation intervals in the compression scheme. The shaded area corresponds to the resolution
within the original data set.

≤

function of pt for a quantization interval of ǫ=0.05 cm for both the original and the com-
pressed data sample. In the compression the remaining clusters selected by the cluster
analyzer (Figure 5.11) were stored in addition to the compressed data. The plot indicates
a slight improvement of the momentum resolution in the compressed data compared to
the original for all pt -bins shown. In the right plot the integrated resolution is shown as
a function of the quantization intervals used. For ǫ
0.1 cm the momentum resolution is

3% better relative to the original data.

∼

The improvement in the resolution is an eﬀect of the quantization of the residuals.
As a consequence of this quantization, the resulting residual distributions are slightly
narrowed as the residuals which are below the quantization interval are mapped to zero.
This narrowing is illustrated in Figure 5.13. Here the transverse residual distribution
before and after the applied compression scheme is presented, showing that the width
of the distribution (represented by the RMS-value) is smaller for the post-compression
distribution compared to the original data set. This eﬀect leads to a better ﬁt in the
post-compression track ﬁnding and thus a more precise estimate of the curvature and
correspondingly the momentum resolution. Furthermore, the potential loss of clusters
will have a less signiﬁcant impact on the momentum resolution as the number of space
points has a 1/√N dependence compared to the linear dependence on the space point
resolution. Figure 5.14 shows the number of clusters per track before and after the
compression. A slight shift towards smaller values is observed for the compressed data
sample. The mean value of the distribution changes by less than two clusters per track
for the quantization intervals investigated.

Similar improvement of resolution can also be seen for the longitudinal part of the
reconstructed tracks, which is illustrated by the impact on dip-angle resolution in Fig-
ure 5.15. Also here the resolution is shown as a function of the quantization intervals

112

D
˛
˛
Entries 
Entries 

 572576
 572576

Mean  
Mean  

 -0.0003079
 -0.0003079

RMS   
RMS   

 0.1665
 0.1665

Entries 
Entries 

 551497
 551497

Mean  
Mean  

 -0.0009352
 -0.0009352

RMS   
RMS   

 0.1439
 0.1439

s
t
n
u
o
C

45000

40000

35000

30000

25000

20000

15000

10000

5000

0
-1

-0.8 -0.6 -0.4 -0.2

-0

0.2

0.4

0.6 0.8

1

-0.8 -0.6 -0.4 -0.2

-0

0.2

0.4

0.6 0.8

1

 [cm]
T

 [cm]
T

Figure 5.13: Transverse residual distribution before and after data compression. Left:
Transverse residual distribution within the original data set. Right: Transverse residual
distribution in the post-compression data set. The distribution has been obtained from
clusters in the outer TPC chambers, and is averaged over all tracks in the event sample
(dNch/dη = 1000). Note the steps in the distribution which is due to the quantization.

chdN
hd

=1000,   L3 Field: 0.4T

Original

Compressed

s
t
n
u
o
C

60

50

40

30

20

10

0
0

20

40

60

80

100

120

140

Number of hits per track

Figure 5.14: Impact on the number of assigned clusters per track from the compres-
sion. The distributions corresponds to the number of assigned clusters per track before
(original) and after (compressed) data compression has been applied.

s
t
n
u
o
C

45000

40000

35000

30000

25000

20000

15000

10000

5000

0
-1

113

d
d
chdN
hd

=1000,   L3 Field: 0.4T

]
d
a
r
m

[
 

1.04

1.02

1

0.98

0.96

0.94

0.92
0
0

0.05
0.05

0.1
0.1

0.15
0.15

0.2
0.2

0.25
0.25

0.3
0.3

0.35
0.35

 [cm]
 [cm]

≤

Figure 5.15: Integrated track dip-angle resolution, ∆λ, as a function of the quantization
intervals in the compression scheme.

which have been used during the compression scheme. For ǫ

0.1 cm the resolution is

4% better than within the original data.

∼

Compression ratios versus tracking performance

In Figure 5.16 the achieved compression ratios are shown together with the corresponding
loss in eﬃciency for 5 diﬀerent multiplicities, with dNch/dη ranging from 100 to 6000. The
quantization intervals was set to ǫ=0.05 cm for both pad and time direction. For every
data set the compress/expand cycle was applied using three diﬀerent options:

Keeping all the remaining clusters.

Keeping a selection of remaining clusters.

Disregarding all the remaining clusters.

•

•

•

The second option applies the selection option discussed above which includes the clus-
ters located in the region being used for seed-ﬁnding by the Oﬄine track ﬁnder. The
compression results are also summarized in Table 5.5.

It is generally observed that both the eﬃciency loss and the relative event size increases
as a function of multiplicity.
If all remaining clusters are kept in the data stream the
resulting compression ratios range from 18-28%. In this case, a very small loss in eﬃciency
can be observed for all the event samples with a maximum of 3.5% for dNch/dη = 6000.
In the case where only a selection of remaining clusters is stored, compression ratios
of 10-14% are achieved.
In this case, however, some loss of tracking eﬃciency can be
seen, particularly for the higher multiplicity events. For dNch/dη = 1000 the loss is on
average
16%. The third scenario, which
disregards all remaining clusters in the data stream, indicates a signiﬁcant impact for all
16% even
the data samples investigated. In this case, the eﬃciency loss is on average
for the lowest multiplicity of dNch/dη = 100.

1.5%, while for dNch/dη = 6000 it goes up to

∼

≥

∼

114

˛
˛
l
D
100

%

90

80

70

60

50

40

30

20

10

0

Integrated tracking efficiency

Original

Keeping all remaining clusters

Keeping selected remaining clusters

Removing all remaining clusters

Relative event size

Keeping all remaining clusters

Keeping selected remaining clusters

Removing all remaining clusters

0
0
100

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

7000
7000

chdN
chdN
hd
hd

Figure 5.16: Achieved compression ratios and the corresponding eﬃciency loss for the
diﬀerent event samples investigated.

The observed loss in tracking eﬃciency is mainly caused by the shortcomings of the
track reconstruction chain used to model the data, and not the compression scheme itself.
This is also supported by the fact that the loss is insigniﬁcant at lower multiplicities,
both with respect to the tracking eﬃciency and the impact on resolutions. For higher
multiplicities, the applied tracking scheme has clear limitations compared to the approach
used by Oﬄine, which was demonstrated in Section 4.5.6.

In Table 5.6 the properties of the compressed data samples are listed. In particular,
the diﬀerence between the ﬁxed bit-rate used to encode the residuals and their entropies
indicate that the samples can be compressed even further by e.g. Arithmetic Coding. By
simply taking the ratio between the calculated entropies and the number of bits used, an
additional factor of 2-3 is achieved. The potential gain in the total compression ratio will

dNch/dη All remaining Selected remaining No remaining

Relative event size [%]

100
1000
2000
4000
6000

18.0
23.8
25.4
26.9
27.5

10.1
12.7
13.5
13.3
12.8

7.3
9.3
9.3
8.8
7.7

Table 5.5: Data compression ratios for the diﬀerent event samples investigated. The three
columns correspond to the various options of storing the remaining clusters.

115

Bits used

Entropy

Relative size [%]

dNch/dη Pad Time Pad Time Track parameters Cluster parameters

100
1000
2000
4000
6000

7
9
8
9
9

8
9
9
9
9

2.4
2.7
2.7
2.9
3.0

3.6
3.9
3.9
4.1
4.3

6.0
5.9
6.6
6.9
7.3

94.0
94.1
93.4
93.1
92.7

Table 5.6: Properties of the compressed data samples. The bits used refer to the ﬁxed
bit-rate used to encode the residuals, while the entropy is the calculated entropy of the
respective quantized residual distribution in the sample. The relative size gives the relative
size of the track and cluster data in the compressed format.

however be slightly less than this factor, as the compressed data also contain the cluster
charges, header information and the track parameters (Figure 5.6) which are diﬃcult to
compress any further.

5.6 Summary

In this chapter various options for compressing the TPC data as an application for the
High Level Trigger System are presented.

Extensive studies applying local data modeling techniques on both real NA49 TPC
data and simulated ALICE TPC data are published in [55]. The results show that lossless
compression techniques such as Huﬀman and Arithmetic Coding can achieve compres-
50% with a
60%. The lossy Vector Quantizer may achieve ratios of
sion ratios of
measurable, but small, impact on the space point resolution.

≥

∼

By introducing online pattern recognition, the TPC data can be modeled more eﬃ-
ciently by representing the data using cluster and track information. A data compression
scheme which utilize the redundant information content by representing the cluster data
relative to the track model has been implemented. Such a scheme will lower the bit-rate
needed to encode the cluster parameters as the cluster model critically depends on the
track parameters. Additional options to compensate for the potential loss of clusters in
the compression scheme have been implemented in the cluster analyzer. These options
include clusters that should be stored in addition to the compressed data sample. The
results using simulated ALICE TPC data indicate that compression ratios of 10-15% are
achievable. Even lower compressing ratios are possible by utilizing the entropy factor
of the cluster residuals, e.g. using Arithmetic Coding. The entropies of the data sam-
ples indicate an additional compression factor of 2, depending on the overhead from the
remaining clusters.

The relative loss in tracking eﬃciency is small for dNch/dη

2000, and increases for
higher multiplicities. The impact on the relative pt resolution and dip-angle resolutions
was shown to be insigniﬁcant. The eﬃciency loss at higher multiplicities is solely due to
the ineﬃciency of the HLT reconstruction chain and not the compression scheme itself.

≤

116

Chapter 6

Conclusions and Outlook

In summary, two main topics are addressed in this work:

Online TPC pattern recognition.

Online TPC data compression.

•

•

The pattern recognition forms the basis of the HLT system as all the foreseen applications,
both event selection and eﬃcient data compression, relies on a full or partial online event
reconstruction. The latter may be considered as the ultimate HLT-application as an
eﬀective online compression of the TPC data would potentially enable higher statistics
for all the physics observables.

6.1 Online TPC pattern recognition

The critical performance issue both in terms of tracking eﬃciencies and computing re-
quirements is the particle multiplicity. Present predictions for the multiplicity in central
Pb–Pb collisions at LHC range from 2000 to 6000 charged particles per unit rapidity, and
extrapolations from RHIC suggesting multiplicity values of about 2000-3000.

Tracking performance

2200 [15] and dNch/dη

Figure 6.1 shows the integrated tracking eﬃciency of primary particles as a function of
multiplicity obtained with the HLT sequential tracking scheme. Two theoretical predic-
tions, dNch/dη
3200 [16], are marked with the shaded area.
Within this multiplicity range, the simulations indicate that tracking eﬃciencies of 85–
90% are feasible. For the secondary particles, simulations indicate that more than 60%
of the decay products from K and Λ particles within the TPC acceptance can be recon-
structed with the current tracking scheme. With further optimizations of the tracking
parameters this number is likely to improve.

≈

≈

At these relatively low multiplicities the resulting occupancy might be handled sat-
isfactory by the implemented sequential tracking scheme. For higher multiplicities it is
obvious that an iterative tracking approach is required in order to achieve the desired
online tracking eﬃciencies. The implemented algorithms, the Hough Transform and a

117

HLT Sequential tracking

L3 Field: 0.4T

y
c
n
e
i
c
i
f
f
e
 
g
n
i
k
c
a
r
t
 
d
e
t
a
r
g
e
t
n

I

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1000
1000

2000
2000

3000
3000

4000
4000

5000
5000

6000
6000

chdN
chdN
hd
hd

Integrated tracking eﬃciency obtained with the HLT sequential tracking
Figure 6.1:
approach. The shaded area indicates the present theoretical predictions based on extrap-
olations from RHIC [15, 16].

Cluster Fitter, however do not show any improved performance compared to the sequen-
tial chain. The main reason being the relatively high number of falsely identiﬁed peaks
in the HT.

Impact on physics cases

The performance of the online track reconstruction algorithms must eventually be evalu-
ated with respect to the impact on the various trigger applications. Such evaluation can
only be done by extensive studies using the reconstructed tracks as input to the trigger
algorithms. Some preliminary conclusion may however be drawn based on the current
outlined trigger algorithms.

Most of the physics trigger applications such as the jet and open charm trigger relies
on a good tracking eﬃciency for high transverse momentum particles. The jet trigger will
mainly focus on particles with pt ≥
2 GeV, and the open charm trigger will utilize a online
momentum ﬁlter where only tracks with pt ∼
0.5–1 GeV will be included in the search for
D-candidates. In this region the tracking eﬃciency of the sequential approach is compa-
rable to Oﬄine for all multiplicities, as the relative eﬃciency loss is mainly restricted to
lower momentum particle tracks. Also the iterative tracking approach shows promising
performance in this momentum regime. This suggest that both tracking approaches may
be utilized in such trigger applications. Further reﬁnement of the open charm trigger in
terms of secondary vertex analysis can only be done by including information by the ITS
detector.

Suggestions for further work

Based on the current understanding of the ALICE TPC tracking performance, there are
several optimizations and alternative solutions that should be pursued.

The fact that the sequential tracking scheme demonstrates a good performance at
lower occupancies suggests the possibility of using it also for higher multiplicities in regions

118

where the occupancy is suﬃciently low, i.e. at large TPC radius. Reconstruction the
tracks in the outermost pad-rows of the TPC with the Cluster Finder and Track Finder,
these track segments can be extended towards the inner pad-rows by utilizing the Cluster
Fitter. In such a scenario, the tracks reconstructed by the sequential tracking scheme
serve as input to the Cluster Fitter where the charge distributions along the trajectory
at the inner pad-rows are ﬁtted and deconvoluted in order to properly reconstruct and
collect the cluster centroids. The ﬁnal list of assigned clusters may then be ﬁtted to a
helix in order to obtain the track parameters. Since the occupancy is low, the tracking
parameters of the Track Finder should be relaxed in order to recover as many of the
tracks as possible.

The cluster ﬁtting procedure may also be further optimized. One option is to compare
the estimated width of the cluster to be ﬁtted to the calculated RMS-value of the actual
charge distribution. Such a comparison can be used to determine the validity of the input
track, and in particular reject input tracks whose estimated widths deviate substantially
from that of the charge distribution. This may consequently reduce the number of fake
tracks in the ﬁtting procedure. Furthermore, global information about a track may be
utilized in order to reject fake tracks at an early stage in the cluster ﬁtting. For instance,
a simple check may be performed on the input track on whether it points to valid clus-
ters on a certain number of successive pad-rows. If the track is valid, there should be
corresponding charge distributions along its trajectory on all the pad-rows.

Final optimization and tuning of the cluster ﬁtting procedure can only be obtained
using real ALICE TPC data, as a detailed understanding of the TPC response functions
is needed to optimize the parameterization of the cluster shapes.

Computing requirements

The required computing power of the HLT system is directly inﬂuenced by the CPU-
time needed by the individual processing modules in the event reconstruction chain. The
largest amount of computing is by far required by the track reconstruction in the TPC, and
the measured processing times of the TPC reconstruction chain may serve as a indicator
on the amount of processing power needed for the complete system. Such estimates will
however ignore any overhead from the interprocess communication and synchronization
in order to operate the HLT system, and can only be indicative.

Given the rapid rate of increasing CPU performance, the number of single processors
at the time of purchasing the HLT components will scale down relative to current available
CPU performance. According to Moore’s law the processing power of a single processor
will approximately double for every 18-24 months. Such a scaling is however restricted to
applications which have a very low memory access requirements, i.e. it is only approxi-
mately applicable if all the data and code references are in the internal memory-cache of
the CPU. This means that if an algorithm is heavily I/O bound, its processing time will
not scale accordingly.

Table 6.1 lists the CPU-time measured on two diﬀerent CPUs for the sequential re-
construction chain. The benchmark CPUs consist of Pentium III 800 MHz and Pentium
4 2800 MHz, both with 1 GB of RAM and 256 kB and 512 kB L2 cache respectively. Both
architectures were running a Linux kernel v2.4. The measured CPU-time is integrated
over all the processing modules, and is thus equivalent to the processing time needed to

119

Pentium III, 800 MHz Pentium 4, 2800 MHz

dNch/dη CPU-time [s]

CPU-time [s] #CPU

1000
2000
4000
6000

7.5
14.0
29.5
47.3

#CPU
1500
2800
5900
9460

3.4
6.3
13.2
21.2

680
1260
2650
4240

Table 6.1: Computational demands on the HLT system from TPC tracking. The CPU-
time is integrated over all the processing modules in the sequential tracking approach, and
is equivalent to the processing time needed to reconstruct the event on a single CPU. The
number of processors corresponds to the measured CPU-time divided by the available
time-budget of 5 ms assuming event rate of 200 Hz.

reconstruct one event on a single CPU. Based on these numbers one can estimate the
required number of processors needed to process the data within the time-budget of 5 ms
in central Pb–Pb rate of 200 Hz. For instance, assuming a multiplicity of dNch/dη = 2000,
about 6300 ms/5 ms=1260 CPUs (Pentium 4) is required to reconstruct the tracks in the
TPC, while approximately the double amount is needed for dNch/dη = 4000. Comparing
the performance on the two processor types there is a factor of about 2.2 improvement in
processing time, which illustrates that the processing times does not scale with the CPU
clock frequency (3.5).

The Cluster Finder algorithm needs approximately 25% of the measured CPU-time.
This algorithm has been synthesized on a FPGA, and will most likely utilize the FPGA co-
processor functionality planned for the HLT-RORC. Assuming that the Cluster Finder
can be processed in a single FEP, this can potentially reduce the number of required
processors by a factor 4.

Given that the HLT components will be purchased in 2006-2007, an additional factor
of increase of the CPU performance can be expected compared with todays measurements.
How much this will aﬀect the overall requirements of the HLT system, however, has to
be monitored and evaluated during the purchasing procedure.

6.2 Online data compression

The modeling techniques implemented indicate that one can compress the TPC data by
a factor 6-10 with a low impact on the tracking performance. This however assumes that
the online pattern recognition scheme which are used to model the data is comparable to
that of which will be used during the Oﬄine analysis.

Based on the achieved compression ratios one can estimate the potential online data
rate reduction as far as the TPC is concerned. In Table 6.2, the event sizes estimated from
RLE 8 bit ADC data and the corresponding data rate assuming a central event rate of
200 Hz are shown. These event sizes are scaled down using the compression ratios obtained
and the resulting data rate is given accordingly. Given the foreseen bandwidth to mass

120

Compressed track–cluster data
dNch/dη Event size [MB] Data rate [MB/s] Event size [MB] Data rate[MB/s]

RLE 8 bit TPC data

1000
2000
4000
6000

13.8
24.1
44.1
61.6

2760
4820
8820
12320

1.8
3.3
5.9
7.8

360
660
1180
1560

Table 6.2: Estimated TPC data rate reduction based on obtained TPC data compression
ratios. The compression ratios used corresponds to the results where a selection of the
remaining clusters has been stored.

∼

1200 MB/s, the results indicates that up to a multiplicity of dNch/dη=4000
storage of
the data rate can suﬃciently be reduced to write all the measured data to mass storage.
Entropy encoding of the quantized residuals by e.g. Arithmetic Coding may further
improve the compression ratio by a factor of 2. However, also clusters belonging to
secondary tracks must be considered, resulting in an increase of the compressed data
size. With a good tracking eﬃciency they can be modeled and compressed similarly to
what is done for the primaries.
Including data from these tracks is not likely to have
a signiﬁcant impact on the compression ration, as their relative size is small. Their
presence will therefore mainly concern the additional computing power needed for their
reconstruction.

The diﬀerence between the modeling scheme, the general readout scheme and/or us-
ing e.g. entropy coding, is the elimination of the original raw-data from the data stream.
However, the raw-data itself contains a signiﬁcant amount of redundant information as
its characteristics are governed by the detector speciﬁc constants such as the diﬀusion
constants of the gas, electronic response functions etc. Therefore, once the Oﬄine re-
construction and analysis chain has been set up correctly and fully understood, there
will in principle be no reason to return to the raw-data anymore. During the ﬁrst years
of running the experiment, however, zero-suppressed raw-data will have to be recorded
without any further processing in order to ensure a complete understanding of the TPC
detector response. Any data compression scheme exploiting the global characteristics of
the data is therefore likely not to be used during these ﬁrst years of operation.

6.3 Outlook

Although recent results from RHIC indicate a particle multiplicity which is well below
the original ALICE design value of dNch/dy=8000, the uncertainties are still very large.
More eﬀort should therefore be invested in further optimizing the algorithms for the
higher multiplicity regime, possibly utilizing alternative combined tracking approaches
as outlined above. Extensive studies are also needed to investigate the impact of the
tracking performance on the various trigger applications, and adapt the tracking parame-
ters accordingly. Furthermore, the other detectors, in particular the ITS and TRD, need
to be incorporated into the HLT track reconstruction framework in order to enable full

121

event reconstruction functionality. An obvious approach would be to implement an online
version of the Kalman ﬁlter as used within the Oﬄine framework.

Further optimization of the TPC data modeling and compression scheme relies on a
detailed understanding of the TPC response and its data models, and should therefore
primarily be done using real ALICE TPC data. The main impact on its performance is
however strongly correlated with the eﬃciency of the preceding pattern recognition step,
and the achievable compression ratio versus information loss will therefore beneﬁt from
any improvement thereon.

122

Appendix A

Track parameterizations

In general, the motion of a particle within a given detector depends on both the magnetic
and electric ﬁelds, and the interactions with the surrounding material such as multiple
scattering and energy loss. For pattern recognition and track ﬁtting purposes, it is often
practical to assume that the trajectory of the particle is not aﬀected by the material. In
a static uniform magnetic ﬁeld the trajectory of a charged particle is then described by a
helix. In the following, the track model and its helix parameterization commonly used in
collider experiments will be outlined. A detailed description of general track models and
ﬁtting procedures is given by Bock et. al. [85].

A.1 The equations of motion

A charged particle moving in a static magnetic ﬁeld, B, is subject to the Lorentz force,
F, which is given by,

where v is the velocity of the particle and q is the charge. From this equation the equation
of motion can be derived,

F = qv

B

×

mγ

d2x
dt2 = c2Kqv(t)

×

B(x(t))

where K is a proportionality factor, B(x) is the static magnetic ﬁeld as a function of
particle position, x, m is the rest mass, c is the velocity of light and t is the time in the
laboratory rest frame. The relativistic Lorentz factor is given by γ = (1
2 and
β = v/c.

β2)− 1

−

Equation A.2 can be rewritten in the form of geometrical quantities only,

(A.1)

(A.2)

(A.3)

d2x
ds2 =

Kq
P

dx
ds ×

B(x(s))

where s(t) is the distance along the trajectory and P = mγv in the laboratory frame.
The following units are commonly used:

q in multiples of the positive elementary charge (dimensionless).

•

•

x and s in cm.

123

P in GeV/c.

B in Tesla.

•

•

•

K is proportional to the velocity of light and is deﬁned as 0.00299792458 T−1 cm−1.

A.2 Helix parameterizations

By integrating formula A.3 one can obtain a parameterization for the particle trajectory.
There are in total six integration constant in addition to the unknown momentum P .
However, taking the identity

2

dx
ds !

 

+

2

dy
ds !

 

+

2

dz
ds !

 

1

≡

and by choosing a “reference surface” there will be only ﬁve independent parameters
deﬁning the trajectory, three deﬁning the circular projection in the plane orthogonal to
the magnetic ﬁeld direction and two for the linear motion along the direction of the ﬁeld.
By further assuming that the magnetic ﬁeld is parallel to the z-axis, equation A.3 reduces
to,

Kq
dy
P
ds
Kq
P

d2x
ds2 =
d2y
ds2 =
−
d2z
ds2 = 0

dx
ds

B

(A.4)

and the solution is a helix with an axis parallel to z,

x(s) = x0 +

[cos(Φ0 + hsκ cos λ)

cos Φ0]

y(s) = y0 +

[sin(Φ0 + hsκ cos λ)

sin Φ0]

−

−

1
κ
1
κ

z(s) = z0 + s sin λ.

(A.5)

The helix parameters are illustrated in Figure A.1 and are deﬁned as follows:

s is the path length along the helix. It increases when moving in the direction of
the particle’s momentum vector.

(x0, y0, z0) are the coordinates of the starting point of the helix, where s = s0=0.

λ = sin−1(dz/ds) (
as the dip-angle.

−

≤

π/2 < λ

π/2) is the slope of the helix, commonly referred to

κ = 1/R is the curvature of the circular projection in the xy-plane.

q is the charge of the particle in units of positron charge.

•

•

•

•

•

124

•

•

•

h is the sense of rotation of the projected helix in the xy-plane, where h=-
sign(qB)=

1 (=sign(dφ/ds), where φ is the track direction).

±

ψ0 is the azimuthal angle of the track direction at the starting point.

Φ0 is the azimuth angle of the starting point (polar coordinates) with respect to the
helix axis (Φ0 = ψ0

hπ/2).

−

R

0

s > 0

y

(x , y )
i
i

(x , y )
c
c

z

z0

0
(x  , y )
0
0

s < 0

x

p

p
t

pz

s

Figure A.1: Schematic view of the helix parameters. Projection of the helix in the xy-
plane (left). Projection of the helix in the sz-plane (right).

Track ﬁt parameters

The track ﬁt in the transverse plane determines the center of curvature (xc, yx) and the
radius of curvature R, while the linear ﬁt in (s, z)-plane returns the z0 and tan λ. In order
to calculate the various track parameters, one need to determine the reference/starting
point of the helix. This is typically chosen as a point closest to the innermost layer of the
detector, i.e. relative to the innermost assigned space point of the track. If the innermost
assigned space point on the track is (x1, y1), the azimuthal angle can be deﬁned as

The closest point (x0, y0) on the trajectory and the azimuthal angle of the track direction
at that point are then calculated according to

Φ0 = tan−1 y1
x1

yc
xc

.

−
−

x0 = xc +

y0 = yc +

ψ0 = Φ0 + h

cos Φ0
κ
sin Φ0
κ
π
2

125

(A.6)

(A.7)

F
y
l
(A.8)

(A.11)

(A.12)

The remaining track parameters can be calculated as

pt =

KB
κ

pz = pt tan λ

p =

p2
t + p2
z.

q

Helix going through origin

If the helix is going through the origin (0,0,0), the helix parameterization in equation A.5
is reduced to

Deﬁning t = sκ cos λ, and utilizing the relations

x(s) =

[cos(Φ0 + hsκ cos λ)

cos Φ0]

y(s) =

[sin(Φ0 + hsκ cos λ)

sin Φ0]

−

−

1
κ
1
κ

z(s) = s sin λ

(A.9)

cos(Φ0) = cos(ψ0

h

) = h sin ψ0

π
2
π
2

−

−

−

sin(Φ0) = sin(ψ0

h

) =

h cos ψ0

(A.10)

gives

where

x(t) =

1
κ
1
κ
z(t) = γt

y(t) =

[
−

[sin(ψ0 + ht)

sin ψ0]

−

cos(ψ0 + ht) + cos ψ0]

γ =

tan λ
κ

In this case the number of independent parameters describing the helix is reduced from
ﬁve to 3, i.e. two parameters for the circular projection in the transverse plane (κ and
ψ0), and one parameter for describing the longitudinal motion (γ).

Deﬁning the track parameters at DCAO

The reference point of the track can be deﬁned as the point of distance of closest approach
to the coordinate system origin (DCAO), Figure A.2. This is useful if e.g. all tracks should
be deﬁned relative to the same point. In this case, the point (x1, y1) in equation A.6 is
(0,0) and (x0, y0, z0) is the point at DCAO. The helix can now be deﬁned by the parameter
set,

(κ, rDCAO, φDCAO, zDCAO, λ)

126

y

(  ,    )
r

DCAO

0

R

x

0

(x , y )
c
c

Figure A.2: Deﬁning the track parameters at the point of DCAO.

where rDCAO, φDCAO and zDCAO denotes the radius, azimuthal angle and z-coordinate
at the point, respectively. In addition to these ﬁve parameters, two signs are needed to
denote the sense of rotation, h, and the relative distance of the point with respect to the
center of curvature (sign(R
c )). The ﬁve parameters thus completely deﬁne the
helix, and the track parameters can be calculated as

c + y2
x2

−

q

x0 =
y0 =

rDCAO
rDCAO

cos φDCAO
sin φDCAO

Φ0 =

|
|




|
|
φDCAO
φDCAO + π
2
π
2

ψ0 = Φ0 + h



xc = x0

cos Φ0

yc = y0

sin Φ0

1
κ
1
κ

−

−

if R >

if R <

c + y2
x2
c
c + y2
x2
c

q

q

(A.13)

127

f
F
y
128

Appendix B

Software and data formats

B.1 Analysis software structure

All the HLT analysis software has been written in C++. The various processing steps
are implemented in individual modules, allowing various processing topologies to be im-
plemented. The data structures used internally in the framework are simple C-structures
which has been adapted to the format which is likely to be used in the readout chain.
In order to be compatible with the modular communication framework to be used in the
HLT system (page 37), the data payloads between the processing modules are commu-
nicated by data references to the actual data in memory. This means that each module
takes a data pointer as input, and reads the input data from corresponding location in
memory. Similarly, the output data is written back to memory and the corresponding
data pointer is communicated to the next module in the processing chain. This concept
is illustrated in Figure B.1.

ONLINE MODE

OFFLINE MODE

New Event

Shared Memory

Binary files

Memory handler

Read/Write

Subscriber

Processing
module

Publisher

New Event

Input data

Output data

Pointers

Processing module

Memory

d

a

e

R

Input data

Write

Output data

Figure B.1: Schematic overview of the data payload communication in the HLT re-
construction chain.
In the online mode the communication is handled by the Pub-
lisher/Subscriber framework (left), while in oﬄine mode the memory handling is handled
transparently by a dedicated memory handler class (right).

When running the chain on a parallel prototype system (online mode), the inter-
process communication is completely handled within the Publisher/Subscriber frame-
work [56]. This framework is responsible for connecting all the various modules in

129

the chain together, and the data payloads are handled transparent of the underlying
network interface. For code development and tracking performance evaluation pur-
poses/debugging etc, i.e. when running the chain in oﬄine mode, the memory handling
is done by a dedicated memory handler class. In this case, the output data is written to
binary ﬁles for subsequent evaluation. In this way there is no diﬀerence between the two
running modes as far as the processing modules are concerned. This makes it possible
to use the same analysis code both for prototype testing of the HLT system and oﬄine
evaluation within the AliROOT framework.

Interface to AliROOT

The interface between the HLT and AliROOT framework is provided via special functions
incorporated into a derived class of the HLT memory handling class. This functionality
enables reading of the AliROOT data containers and transforming the data into the
format used within the HLT framework, Figure B.2. This furthermore enables storing and

AliROOT
simulation

Performance
evaluation

AliROOT−HLT I/O

HLT
raw−data

HLT
reconstruction chain

Figure B.2: Schematic overview of the interface between HLT analysis code and Ali-
ROOT.

sending the Monte Carlo information from the simulation through the HLT reconstruction
chain in order to evaluate the tracking performance.

The HLT code can be compiled into shared libraries which can be loaded into an
AliROOT interactive session where it can run and evaluated using standard ROOT-
macros. The AliROOT support is controlled via preprocessor options during compiling.

B.2 Compressed data formats

8-bit RLE ADC-data format

In order to enable a simple approach to estimate the raw-data event sizes, a RLE ADC-
data formatting scheme has been implemented. The format basically consists of replacing
the zero-intervals between the sequences by a tag and the length of the intervals, and will
therefore be comparable in size with the ALTRO-format.

Initially the 10 bit ADC-data from the simulation is converted to 8 bit using a 10-to-8
bit conversion table, Figure 5.1. The data is then compressed by the RLE scheme and
written to binary ﬁles. The data is organized by its inherit granularity from the detector
readout scheme, i.e. 1 ﬁle per (sub-)sector. Each ﬁle thus contains a certain number of
pad-rows which is written in the beginning of the ﬁle. The remaining data stream consists
of the data on the successive pad-rows,

NROWS PADROW_0 PADROW_1 ...

130

For every pad-row, PADROW #, the corresponding row number is written together with the
number of pads containing data on that row. (pads which has at least 1 sequence of
time-bins above threshold) Both number are written using a 8 bit word, as the number of
rows and pads on a given pad-row is 159 and 140, respectively. For every pad containing
data the pad number is written, and then the ADC-values on that pad. When a series of
zeros occur, a zero is written followed by the number of zeros. Example:

PAD 0 NZEROS ADC ADC ADC ADC 0 NZEROS ADC ADC ADC 0 0

This pad with number PAD contains two sequences with 4 and 3 consecutive time-bins
respectively. The two zeros at the end is used to mark the end of the data stream on that
pad. Each entry consists of a 8 bit word. If the number of zeros in a sequence is more
than 255, an additional 8 bit word is written.

Bitwise I/O handling

For the implemented data compression schemes, bitwise handling of the data is necessary.
The standard C I/O libraries only accommodates I/O on even byte boundaries, and all
bitwise I/O has to be done using bitwise operators on integer values. In order to enable a
more conventional way of handling bitwise I/O to ﬁles, special routines has been adapted
from [82]. All bitwise I/O operations to ﬁle is done via the structure:

#include <stdio.h>
typedef struct bit_file {

FILE *file;
unsigned char mask;
int rack;

} BIT_FILE;

All data is read/written to ﬁle via a pointer to the normal FILE structure. The bitwise
handling of the data is done with the additional mask and rack elements. The rack
contains the current byte of data either read in from the ﬁle or waiting to be written out
to the ﬁle. mask contains a single bit mask used either to set or clear the current output
bit or to mask in the current input bit. The mask element is initialized to 0x80, and
during the output the ﬁrst write to the BIT FILE will set or clear that bit and the mask
element shifts to the next. Once the mask has shifted to the point at which all the bits
in the output rack have been set or cleared, the rack is written out to the ﬁle, and a new
rack byte is started. Performing input from a BIT FILE is done in a similar fashion.

Four types of I/O routines are deﬁned, which read or write a single bit or multiple

bits at a time:

void
void

OutputBit( BIT_FILE *bit_file, int bit );
OutputBits( BIT_FILE *bit_file,

int
unsigned long InputBits( BIT_FILE *bit_file, int bit_count );

InputBit( BIT_FILE *bit_file );

unsigned long code, int bit_count );

code denotes the value which is read/written using bit count number of bits.

131

Compressed cluster data format

The compressed cluster data format has been used for the estimation of the data size
needed to store the clusters as raw-data-arrays, Section 5.5.1. For every pad-row the
corresponding row-number is written with a 8 bit word. Next the number of clusters
present on the row is encoded with 10 bits. The data stream for a single pad-row then
becomes:

PADROW NCLUSTERS CLUSTER CLUSTER CLUSTER ...

E.g. for a given pad-row the two ﬁrst is written to the output ﬁle by:

OutputBits(bitfile,padrow,8);
OutputBits(bitfile,n_clusters,10);

//Padrow number
//Number of clusters on padrow

where

BIT_FILE *bitfile;

//Pointer set to the to the output file.

A cluster on the current pad-row is now written as

OutputBits(bitfile,pad_centroid,n_pad_bits);
OutputBits(bitfile,time_centroid,n_time_bits);
OutputBits(bitfile,pad_width,n_padwidth_bits);
OutputBits(bitfile,time_width,n_timewidth_bits);//Time width
OutputBits(bitfile,tot_charge,n_charge_bits);

//Pad centroid
//Time centroid
//Pad width

//Cluster charge

Compressed cluster–track data format

This format refers to the format used for the compressed track and cluster information,
Section 5.5.2. It basically consists of the track parameters and the clusters assigned to
the given track. The overall data stream has the following format

TRACK CLUSTER CLUSTER CLUSTER ... CLUSTER _CLEAR_

where TRACK and CLUSTER denotes the track and relative cluster parameters respectively.
CLEAR marks that the current bit-racks in BIT FILE is written to ﬁle and the mask
element is cleared.

The track parameters are deﬁned within a standard C-structure as:

struct AliL3TrackModel {

Float_t fKappa; //curvature
Float_t fPhi;
Float_t fD;
Float_t fZ0;
Float_t fTgl;

//azimuthal angle of DCAO
//radius of DCA0
//z-coordinate of DCA0
//tan of dipangle

};
typedef struct AliL3TrackModel AliL3TrackModel;

For a given track the parameters are written to the output ﬁle:

132

fwrite(&track,sizeof(AliL3TrackModel),1,bitfile->file);

where

BIT_FILE *bitfile;
AliL3TrackModel track; //Structure filled with the track parameters

//Pointer set to the to the output file.

The ﬁrst cluster in the stream is encoded as:

//A single bit to flag cluster presence

OutputBit(bitfile,1);
OutputBits(bitfile,sector,6); //TPC sector number of the first cluster
OutputBit(output,0);
OutputBits(output,abs(delta_time_q),n_time_bits); //Absolute timeresidual value
OutputBit(output,0);
OutputBits(output,abs(delta_pad_q),n_pad_bits);
OutputBits(output,tot_charge,n_charge_bits);

//Sign of pad residual
//Absolute padresidual value
//Cluster charge

//Sign of time residual

The following clusters in the track list are encoded in the same fashion, with the only
diﬀerence that instead of writing the TPC sector number for every cluster, a single bit is
used to denote a possible change of sector. E.g. if there is no change:

OutputBit(bitfile,1); //Cluster present
OutputBit(bitfile,0); //No change of sector
OutputBit(output,0);
...

and if there was a change of sector

//Sign of time residual

OutputBit(bitfile,1); //Cluster present
OutputBit(bitfile,1); //Change of sector
OutputBits(bitfile,sector,6); //New TPC sector number
OutputBit(output,0);
...

//Sign of time residual

Reading/uncompressing the data is done similarly, using the corresponding input routines.

133

134

References

[1] M. Gell-Mann, Phys. Rev. Lett. 8 (1964) 214.

[2] G. Zweig, CERN Report 8419/TH (1964) 412.

[3] T. Nakano et. al., Phys. Rev. Lett. 91 (2003) 012002.

[4] V. V. Barmin et. al., Phys. Atom. Nucl. 66 (2003) 1715-1718.

[5] S. Stepanyan et. al., Phys. Rev. Lett. 91 (2003) 252001.

[6] J. Barth et. al., Evidence for the positive-strangeness pentaquark Θ+ in photoproduc-

tion with the SAPHIR detector at ELSA, Preprint: arXiv:hep-ex/0307083.

[13] F. Scikor et. al., Lattice QCD at non-vanishing density: phase diagram, equation of

state, Preprint: arXiv:hep-lat/0301027.

[7] C. Alt et. al., Phys. Rev. Lett. 92 (2004) 042003.

[8] N. Cabbibo, G. Parisi, Phys. Lett. 59B (1975) 67.

[9] K. G. Wilson, Phys. Rev. D10 (1974) 2445.

[10] F. Karsch et. al., Phys. Lett. B478 (2000) 447.

[11] R. D. Pisarki, F. Wilczek, Phys. Rev. D29 (1984) 338.

[12] F. Karsch, Nucl. Phys. A698 (2002) 199.

[14] J. D. Bjorken, Phys. Rev. D27 (1983) 140-151.

[15] K. J. Eskola et. al., Nucl. Phys. A696 (2001) 715-728.

[16] N. S. Amelin et. al., Eur. Phys. Jour. C22 (2001) 149-163.

[17] U. Heinz, Nucl. Phys. A638 (1998) 357c.

[18] R. Stock, Nucl. Phys. Lett. 456 (1999) 277.

[19] J. Stachel, Nucl. Phys. A654 (1999) 119c.

[20] U. Heinz, Nucl. Phys. A685 (2001) 414c.

135

[21] R. Snellings et. al., Elliptic ﬂow measurements from STAR, Preprint: arXiv:nucl-

ex/0305001.

[22] M. Becattini et. al., Eur. Phys. Jour. C5 (1998) 143.

[23] S. Jeon, V. Koch, Phys. Rev. Lett. 85 (2000) 2076.

[24] M. M. Aggarwal et. al., Phys. Rev. Lett. (1999) 83.

[25] Justin Frantz et. al., Direct Photons in 200 GeV p+p and Au+Au Collisions, Pre-

sented at Quark Matter 2004, Oakland.

[26] I. Arsene et. al., Phys. Rev. Lett. 91 (2003) 072305.

[27] X. -N. Wang, M. Gyulassy, Phys. Rev. D44 (1991) 3501-3516.

[28] X. N. Wang, Phys. Rev. C63 (2001) 054902.

[29] Y. L. Dokshitzer, D. E. Kharzeev, Phys. Lett. B519 (2001) 199-206.

[30] J. Adams et. al., Phys. Rev. Lett. 91 (2003) 072304.

[31] The ALICE Collaboration, ALICE Physics Performance Report, Vol. I, CERN–

LHCC–2003–049.

[32] The ALICE Collaboration, Technical Proposal CERN–LHCC–1995–71.

[33] The ALICE Collaboration, Technical Proposal, Addendum 1, CERN–LHCC–1996–

[34] The ALICE Collaboration, Technical Proposal, Addendum 2, CERN–LHCC–1999–

32.

13.

[35] The ALICE Collaboration, ALICE Technical Design Report for the Time Projection

Chamber, CERN–LHCC–2000–001.

[36] The ALICE Collaboration, ALICE Technical Design Report of the Transition Radi-

ation Detector, CERN–LHCC–2001–021.

[37] The ALICE Collatoration, ALICE Technical Design Report of the Time Of Flight

System, CERN–LHCC–2000–12.

[38] The ALICE Collaboration, Addendum to the ALICE Technical Report of the Time

of Flight System, CERN–LHCC–2002–016.

[39] The ALICE Collaboration, ALICE Technical Design Report of the Photon Multiplic-

ity Detector, CERN–LHCC–1999–32.

[40] The ALICE Collaboration, ALICE Technical Design Report of the Dimuon Forward

Spectrometer, CERN–LHCC–1999–22.

[41] The ALICE Collaboration, Addendum to the ALICE Technical Report for the

Dimuon Forward Spectrometer, CERN–LHCC–2000–046.

136

[42] The ALICE Collaboration, ALICE Technical Design Report of the Inner Tracking

System, CERN–LHCC–1999–12.

[43] The ALICE Collaboration, ALICE Technical Design Report of the Zero Degree

Calorimeter, CERN–LHCC–1999–5.

[44] The ALICE Collaboration, ALICE Technical Design Report of the Photon Spectrom-

eter, CERN–LHCC–1999–4.

[45] The ALICE Collaboration, ALICE Technical Design Report of the Detector for High

Momentum PID, CERN–LHCC–1998–19.

[46] G. Rai et. al., IEEE Trans. Nucl. Sci. NS-37 (1990) 56.

[47] S. Afanasiev et. al., Nucl. Instrum. Meth. A430 (1999) 21.

[48] M. Anderson et. al., Nucl. Instrum. Meth. A499 (2003) 659-678.

[49] T. Lohse, W. Witzeling, The Time Projection Chamber, ALEPH–1991–156.

[50] L. Musa, Nucl. Phys. A715 (2003) 843c-848c.

[51] The ALICE Collaboration, Technical Design Report: Trigger, Data Acquisition,

High-Level Trigger, Control System, CERN–LHCC–2003–062.

[52] G. Paic et. al., Physics Requirement for the ALICE DAQ system, ALICE–INT–2000–

30.

[53] A. Dainese, Charm production and in-medium QCD energy loss in nucleus-nucleus
collisions with ALICE. A performance study., Preprint: arXiv:nucl-ex/0311004.

[54] F. Manso, A ﬁrst algorithm for a dimuon High Level Trigger, ALICE–INT–2002–04.

[55] J. Berger et. al., Nucl. Instr. Meth. A489 (2002) 406.

[56] T. M. Steinbeck, A Modular Fault-Tolerant Data Transport Framework, Ph.D. thesis,

Kirchhoﬀ Institute of Physics, Ruprecht-Karls-University, Heidelberg, 2004.

[57] M. Ohlsson et. al., Comp. Phys. Commun. 71 (1992) 77.

[58] M. Gyulassy, M. Harlander, Comp. Phys. Commun. 66 (1991) 31-46.

[59] B. Lasiuk et. al., Development of an Elastic Tracking Package, Proc. International

Conference on Computing in High Energy Physics, Chicago, USA (1998).

[60] P. V. C. Hough, Machine Analysis of Bubble Chamber Pictures, Proc. International
Conference on High Energy Accelerators and Instrumentation, CERN (1959).

[61] P. Billior, Nucl. Instrum. Meth. 225 (1984) 602.

[62] R. Fr¨uhwirth, Application of Filter Methods to the Reconstruction of Tracks and Ver-
tices in Events of Experimental High Energy Physics, HEPHY–PUB–516–88, Vienna
(1988).

137

[63] B. Batyunya et. al., Kalman Filtering Application for Track Recognition and Recon-

struction in ALICE Tracking System, ALICE–INT–1997–24.

[64] J. J. Hopﬁeld, Proc. Nat. Acad. Sci. USA 79 (1982) 2554-2558.

[65] G. Stimpﬂ-Abele, L. Garrido, Comp. Phys. Commun. 64 (1991) 46-56.

[66] R. L. Gluckstern, Nucl. Instrum. Meth. 24 (1963) 381.

[67] http://AliSoft.cern.ch/offline

[68] http://root.cern.ch

[69] F. Abe et. al., Phys. Rev. Lett. 61 (1988) 1819.

[70] M. Kowalski, ALICE TPC Slow Simulator, ALICE–INT–1996–36.

[71] Y. Belikov et. al., TPC tracking and particle identiﬁcation in high-density environ-

ment, Preprint: arXiv:physics/0306108.

[72] C. Adler et. al., Nucl. Instrum. Meth. A499 (2003) 778.

[73] G. Grastveit et. al., FPGA Co-processor for the ALICE High Level Trigger, Preprint:

arXiv:physics/0307017.

[74] P. Yepes, Nucl. Instrum. Meth. A380 (1996) 582.

[75] N. I. Chernov, G. A. Oskov, Comp. Phys. Commun. 33 (1984) 329-333.

[76] D. W. Marquardt, Journal of the Society for Industrial and Applied Mathemathics

11 (1963) 431-441.

[77] J. Illingworth, J. Kittler, Computer Vision Graphics Image Process. 44 (1988) 87-116.

[78] V. F. Leavers, Computer Vision Graphics Image Process. 58 (1993) 250-264.

[79] W. C. Y. Lam et. al., Patt. Recog. Lett. 15 (1994) 1127-1135.

[80] Marian Ivanov, CERN, Private communication.

[81] D. A. Huﬀman, Proc. IRE 40 (9) (1952) 1098.

[83] R. M. Gray, IEEE ASSP Magazine (1984) 4ﬀ.

[84] Y. Belikov, CERN, Private communication.

[82] M. Nelson, J. L. Gailly, The Data Compression Book, M T Books, New York, 1996.

[85] R. K. Bock et. al., Data Analysis Techniques for High Energy Physics Experiments,

Cambrigde University Press, 1990.

138


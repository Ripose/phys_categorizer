5
0
0
2
 
n
u
J
 
3
2
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 
1
v
6
7
1
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Workload characterization and modelling

2nd February 2008

Workload analysis of a cluster
in a Grid environment

Emmanuel Medernach

Laboratoire de Physique Corpusculaire, CNRS-IN2P3

Campus des C´ezeaux, 63177 Aubi`ere Cedex, France
∗
e-mail: medernac@clermont.in2p3.fr

Abstract. With Grids, we are able to
share computing resources and to provide
for scientiﬁc communities a global trans-
parent access to local facilities. In such an
environment the problems of fair resource
sharing and best usage arise. In this pa-
per, the analysis of the LPC cluster usage
(Clermont-Ferrand, France) in the EGEE
Grid environment is done, and from the
results a model for job arrival is proposed.

1

Introduction

Analysis of a cluster workload is essential to under-
stand better user behavior and how resources are
used [1]. We are interested to model and simulate
the usage of a Grid cluster node in order to com-
pare diﬀerent scheduling policies and to ﬁnd the
best suited one for our needs.

The Grid gives new ways to share resources be-
tween sites, both as computing and storage re-
sources. Grid deﬁnes a global architecture for dis-
tributed scheduling and resource management [2]
that enable resources scaling. We would like to un-
derstand better such a system so that a model can
be deﬁned. With such a model, simulation may
be done and a quality of service and fairness could
then be proposed to the diﬀerent users and groups.
Brieﬂy, we have some groups of users that each
submit jobs to a group of clusters. These jobs
are placed inside a waiting queue on some clusters
before being scheduled and then processed. Each
group of users have their own need and their own
strategy to job submittal. We wish:

1. to have good metrics that describes the group

and user usage of the site.

∗This work was supported by EGEE.

1

2. to model the global behavior (average job wait-
ing time, average waiting queue length, system
utilization, etc.) in order to know what is the
inﬂuence of each parameter and to avoid site
saturation.

3. to simulate jobs arrivals and characteristics to
test and compare diﬀerent scheduling strate-
gies. The goal is to maximize system utiliza-
tion and to provide fairness between site users
to avoid job starvation.

As parallel scheduling for p machines is a hard
problem [3, 4], heuristics are used [5, 6]. More-
over we have no exact value about the duration of
jobs, making the problem diﬃcult. We need a good
model to be able to compare diﬀerent scheduling
strategies. We believe that being able to charac-
terize users and groups behavior we could better
design scheduling strategies that promote fairness
and maintain a good throughput. From this paper
some metrics are revealed, from the job submittal
protocol a detailed arrival model for single user and
group is proposed and scheduling problems are dis-
cussed. We then suggest a new design based on our
observation and show relationship between fairness
issue and system utilization as a ﬂow problem.

Our cluster usage in the EGEE (Enabling Grids
for E-science in Europe) Grid is presented in sec-
tion 2, the Grid middleware used is described.
Corresponding scheduling scheme is shown in sec-
tion 3. Then the workload of the LPC (Laboratoire
de Physique Corpusculaire) computing resource, is
presented (section 4) and the logs are analyzed sta-
tistically. A model is then proposed in section 5
that describes the job arrival rate to this cluster.
Simulation and validation are done in section 6 with
comparison with related works in section 7. Results
are discussed in section 8. Section 9 concludes this
paper.

2

2 Environment

2.1 Local situation

The EGEE node at LPC Clermont-Ferrand is a
Linux cluster made of 140 dual 3.0 GHz CPUs
with 1 GB of RAM and managed by 2 servers with
the LCG (LHC Computing Grid Project) middle-
ware. We are currently using MAUI as our cluster
scheduler [7, 8]. It is shared with the regional Grid
INSTRUIRE (http://www.instruire.org). Our
LPC Cluster role in EGEE is to be used mostly by
Biomedical users∗ located in Europe and by High
Energy Physics Communities. Biomedical research
is one core application of the EGEE project. The
approach is to apply the computing methods and
tools developed in high energy physics for biome-
dical applications. Our team has been involved in
international research group focused on deploying
biomedical applications in a Grid environment.

One pilot application is GATE which is based
on the Monte Carlo GEANT4 [9] toolkit developed
by the high energy physics community. Radiothe-
rapy and brachytherapy use ionizing radiations to
treat cancer. Before each treatment, physicians and
physicists plan the treatment using analytical treat-
ment planning systems and medical images data of
the tumor. By using the Grid environment pro-
vided by the EGEE project, we will be able to
reduce the computing time of Monte Carlo simu-
lations in order to provide a reasonable time con-
suming tool for speciﬁc cancer treatment requiring
Monte-Carlo accuracy.

Another group is Dteam, this group is partly re-
sponsible of sending tests and monitoring jobs to
our site. Total CPU time used by this group is
small relatively to the other one, but the jobs sent
are important for the site monitoring. There are
also groups using the cluster from the LHC exper-
iments at CERN (http://www.cern.ch). There
are diﬀerent kind of jobs for a given group. For ex-
ample, Data Analysis requires a lot of I/O whereas
Monte-Carlo Simulation needs few I/O.

2.2 EGEE Grid technology

In Grid world, resources are controlled by their
owners. For instance diﬀerent kind of scheduling

∗Our cluster represented 75% of all the Biomed Virtual

Organization (VO) jobs in 2004.

2 ENVIRONMENT

policies could be used for each site. A Grid resource
center provides to the Grid computing and/or stor-
age resources and also services that allow jobs to be
submitted by guests users, security services, moni-
toring tools, storage facility and software manage-
ment. The main issue of submitting a job to a
remote site is to provide some warranty of security
and correct execution. In fact the middleware au-
tomatically resubmits job when there is a problem
with one site. Security and authentication are also
provided as Grid services.

The Grid principle is to allow user a worldwide
transparent access to computing and storage re-
sources. In the case of EGEE, this access is aimed
to be transparent by using LCG middleware built
on top of the Globus Toolkit [10]. Middleware acts
as a layer of software that provides homogeneous
access to diﬀerent Grid resource centers.

2.3 LCG Middleware

LCG is organized into Virtual Organizations
(VOs): dynamic collections of individuals and in-
stitutions sharing resources in a ﬂexible, secure and
coordinated manner. Resource sharing is facili-
tated and controlled by a set of services that al-
low resources to be discovered, accessed, allocated,
monitored and accounted for, regardless of their
physical location. Since these services provide a
layer between physical resources and applications,
they are often referred to as Grid Middleware [11].
Bag of task applications are parallel applications
composed of independent jobs. No communications
are required between running jobs. Since jobs from
a same task may execute on diﬀerent sites commu-
nications between jobs are avoided.
In this con-
text, users submit their jobs to the Grid one by
one through the middleware. Our cluster receives
jobs only from the Grid. This means that each
job requests for one and only one processor. Users
could directly specify the execution site or let a
Grid service choose the best destination for them.
Users give only a rough estimation of the maximum
job running time. In general this estimated time is
overestimated and very imprecise [12]. Instead of
speaking about an estimated time, it could be bet-
ter to speak about an upper bound for job duration,
so this value provided by users is more a precision
value. The bigger the value is the more imprecise
the value of the actual runtime could be.

Figure 1 shows the scenario of a job submittal.
In this ﬁgure rounded boxes are grid services and
ellipses are the diﬀerent jobs states. As there is no
communications between jobs, jobs could run inde-
pendently on multiple clusters. Instead of commu-
nicating between job execution, jobs write output
ﬁles to Storage Elements (SE) of the Grid. Small
output ﬁles could also be sent to the UI. Replica Lo-
cation Service (RLS) is a Grid service that allow lo-
cation of replicated data. Other jobs may read and
work on the data generated, forming “pipelines” of
jobs.

The users Grid entry point is called an User In-
terface (UI). This is the gateway to Grid services.
From this machine, users are given the capability
to submit jobs to a Computing Element and to fol-
low their jobs status [13]. A Computing Element
(CE) is composed of Grid batch queues. A Com-
puting Element is built on a homogeneous farm of
computing nodes called Worker Nodes (WN) and
on a node called a GateKeeper acting as a security
front-end to the rest of the Grid.

Users can query the Information System in order
to know both the state of diﬀerent grid nodes and
where their jobs are able to run depending on job
requirements. This match-making process has been
packaged as a Grid service known as the Resource
Broker (RB). Users could either submit their jobs
directly to diﬀerent sites or to a central Resource
Broker which then dispatches their jobs to match-
ing sites.

The services of the Workload Management Sys-
tem (WMS) are responsible for the acceptance of
job submits and the dispatching of these jobs to the
appropriate CEs, depending on job requirements
and on available resources. The Resource Broker is
the machine where the WMS services run, there is
at least one RB for each VO. The duty of the RB
is to ﬁnd the best resource matching the require-
ments of a job (match-making process). (For more
details see [14])

Users are then mapped to a local account on the
chosen executing CE. When a CE receives a job,
it enqueues it inside an appropriate batch queue,
chosen depending on the job requirements, for in-
stance depending on the maximum running time. A
scheduler then proceeds all these queues to decide
the execution of jobs. Users could question about
status of their jobs during all the job lifetime.

3

3 Scheduling scheme

The goal of the scheduler is ﬁrst to enable execu-
tion of jobs, to maximize job throughput and to
maintain a good equilibrium between users in their
usage of the cluster [15]. At the same time sched-
uler has to avoid starvation, that is jobs, users or
groups that access scarcely to available cluster re-
sources compared to others.

Scheduling is done on-line, i.e the scheduler has
no knowledge about all the job input requests but
jobs are submitted to the cluster at arbitrary time.
No preemption is done, the cluster uses a space-
sharing mode for jobs. In a Grid environment long-
time running jobs are common. The worst case is
when the cluster is full of jobs running for days
and at the same time receiving jobs blocked in the
waiting queue.

Short jobs like monitoring jobs barely delay too
much longer jobs. For example, a 1 day job could
wait 15 minutes before starting, but it is unwise if
a 5 minutes job has to wait the same 15 minutes.
This results in production of algorithms classes that
encourage the start of short jobs over longer jobs.
(Short jobs have higher priority [16]) Some other so-
lution proposed is to split the cluster in static sub-
clusters but this is not compatible with a sharing
vision like Grids. Ideal on-line scheduler will maxi-
mize cluster usage and fairness between groups and
users. Of course a good trade-oﬀ has to be found
between the two.

3.1 Local situation

We are using two servers to manage our 140 CPUs,
on each machine there are 5 queues where each
group could send their jobs to. Each queue has
its own limit in maximum CPU Time. A job in
a given queue is killed if it exceeds its queue time
limit. There are in fact two limits, one is the max-
imum CPU time, the other one is the maximum
total time (or Wall time) a job could use. For each
queue there is also a limit in the number of jobs
than can run at a given time. This is done in order
to avoid that the cluster is full with long running
jobs and short jobs cannot run before days. Likely
there is the same limit in number of running jobs
for a given group.

Maui Scheduler and the Portable Batch Sys-
tem (PBS) run on multiple hardware and oper-

4

3 SCHEDULING SCHEME

Virtual organization

User Interface (UI)
Account

Authentication
Proxy

Job submittal

Job description file 

Resource Broker (RB)

User Interface (UI)

Input Sandbox

Resource Broker (RB)

SUBMITTED

Resource Broker (RB)

Query the Information system

BDII

WAIT

Automatic resubmission

Computing Element (CE)

Resource Broker (RB)

Wrapper script

READY

Globus Gatekeeper

Globus Gatekeeper

Grid Batch Queuing

LRMS (PBS for instance)

SCHEDULED

LRMS (PBS for instance)

Scheduling

Resource Broker (RB)

Input Sandbox

Worker Nodes (WNs)

Worker Nodes (WNs)

Worker Nodes (WNs)

RUNNING

Computing Element (CE)

Output Sandbox
Output Sandbox

Resource Broker (RB)

DONE

Resource Broker (RB)

Output Sandbox
Output Sandbox

User Interface (UI)

CLEARED

Figure 1: Job submittal scenario

Queue Max CPU Max Wall Max Jobs

Test
Short
Long
Day
Inﬁnite

(H:M)
00:05
00:20
08:00
24:00
48:00

(H:M)
00:15
01:30
24:00
36:00
72:00

130
130
130
130
130

Table 1: Queue conﬁguration (maximum CPU
time, Wall time and running jobs)

ating systems. MAUI is a scheduling policy en-
gine that is used together with the PBS batch sys-
tem. PBS manages the job reception in queues
and execution on cluster nodes. MAUI is a First-

Come-First-Served backﬁll scheduler with priori-
ties. This means that is checks periodically the
running queues, execution of lower priority jobs is
allowed if it is determined that their running will
not delay jobs higher in the queue [8]. Maui is un-
fortunately not event driven, it polls regularly the
PBS queues to decide which jobs to run. MAUI
allows to add a priority property for each queue.
Our site conﬁguration is that the shorter the queue
allows jobs to run, the more priority is given to that
job. Jobs are then selected to run depending on a
priority based on the job attributes such as owner,
group, queue, waiting time, etc.

4.1 Running time

5

4 Workload data analysis

Workload analysis allows to obtain a model of the
user behavior [17]. Such a model is essential for
understanding how the diﬀerent parameters change
the resource center usage. Meta-computing work-
load [18] like Grid environments is composed of dif-
ferent site workloads. We are interested in mod-
elling workload of our site which is part of the
EGEE computational Grid. Our site receives only
jobs coming from the EGEE Grid and each requests
for only one CPU.

Group Mean

Biomed
Dteam
LHCb
Atlas
Dzero

5417
222
2072
13071
213

Standard Number
of jobs
Deviation
108197
22942.2
94474
3673.6
9709
7783.4
7979
28788.8
1332
393.9

Table 2: Group running time in seconds and total
number of jobs submitted

Traces of users activities are obtained from
there are not much diﬀerences between CPU time
accountings on the server logs. Logs contain in-
and total time, so it means that jobs sent to our
formation about users, resources used, jobs arrival
cluster are really CPU intensive jobs and not I/O
time and jobs completion time. It is possible to use
intensive. Dteam jobs are mainly short monitor-
directly these traces to obtain a static simulation
ing jobs but all Dteam jobs are not regularly sent
or to use a dynamic model instead. Workload
jobs. We have 6784.6 days CPU time consumed by
models are more ﬂexible than logs, because they
Biomed for 108197 jobs (Mean of one hour and half
allow to generate traces with diﬀerent parameters
per jobs, table 2). Repartition of cumulative job
and better understand workload properties [1].
duration distributions for Biomed VO is shown on
Workload analysis allows to obtain a model of
ﬁgure 4. The duration of about 70% of Biomed jobs
users activity. Such a model is essential for un-
are less than 15 minutes and 50% under 10 seconds,
derstanding how the diﬀerent parameters change
there are a dominant number of small running jobs
the resource center usage. Our workload data has
but the distribution is very wide as shown by the
been converted to the Standard Workload Format
high standard deviation compared to the mean in
(http://www.cs.huji.ac.il/labs/parallel/workload/)
table 2.
and made publicly available for further researches.
Workload is from August 1st 2004 to May 15th
2005. We have a cluster containing 140 CPUs since
September 15th. This can be visible in the ﬁgure 2,
3(a) and 3(b), where we notice that the number of
jobs sent increases. Statistics are obtained from the
PBS log ﬁles. PBS log ﬁles are well structured for
data analysis. An AWK script is used to extract
information from PBS log ﬁles. AWK acts on lines
matched by regular expressions. We do not have
information about users Login time because users
send jobs to our cluster from an User Interface (UI)
of the EGEE Grid and not directly.

Table 3: Queue mean running time in seconds, cor-
responding Standard Deviation and Coeﬃcient of
Variation

Standard
Deviation
373.6
1230.5
11881.2
25489.2
30824.5

31.0
149.5
2943.2
6634.8
10062.2

Test
Short
Long
Day
Inﬁnite

12.0
8.2
4.0
3.8
3.0

Queue

Mean

CV

4.1 Running time

During 280 days, our site received 230474 jobs from
which 94474 Dteam jobs and 108197 Biomed jobs
(table 2). For all these jobs there are 23208 jobs
that failed and were dequeued. It appears that jobs
are submitted irregularly and by bursts, that is lot
of jobs submitted in a short period of time followed
by a period of relative inactivity. From the logs,

Users submit

their jobs with an estimated
run length. For relationships between execution
time and requested job duration and its accuracy
see [19]. To sum up estimated jobs duration are es-
sentially inaccurate. It is in fact an upper bound for
job duration which could in reality take any value
below it. Table 3 shows for each queue the mean
running time, its standard deviation and coeﬃcient
of variation (CV) which is the ratio between stan-

6

4 WORKLOAD DATA ANALYSIS

Biomed jobs

Dteam jobs

 10000

01 Aug  01 Sep  01 Oct 

01 Nov  01 Dec  01 Jan  01 Feb  01 Mar 

01 Apr  01 May 

01 Aug  01 Sep  01 Oct 

01 Nov  01 Dec  01 Jan  01 Feb  01 Mar 

01 Apr  01 May 

Date

Date

(a) Number of Biomed jobs received per weeks (from
August 2004 to May 2005)

(b) Number of Dteam jobs received per weeks (from
August 2004 to May 2005)

Figure 2: Number of jobs received per VO and per week from August 2004 to May 2005

s
b
o

j
 
f

o
 
r
e
b
m
u
N

 14000

 12000

 10000

 8000

 6000

 4000

 2000

 0

)

%

(
 
n
o
i
t
a
z

i
l
i
t
u
 
m
e
t
s
y
S

 60

 50

 40

 30

 20

 10

 0

Site utilization

s
b
o

j
 
f

o
 
r
e
b
m
u
N

 8000

 6000

 4000

 2000

 0

)
s
y
a
D

i

(
 
e
m
T
U
P
C

 

 1000

 100

 10

 1

 0.1

 0.01

 0.001

 0.0001

 1e-05

01 Aug  01 Sep  01 Oct  01 Nov  01 Dec  01 Jan  01 Feb  01 Mar  01 Apr  01 May 
Date

01 Aug  01 Sep  01 Oct  01 Nov 01 Dec  01 Jan  01 Feb 01 Mar  01 Apr  01 May 
Date

Biomed CPU Time
Dteam jobs

(a) System utilization per weeks (from August 2004
to May 2005)

(b) CPU consumed by Biomed and Dteam jobs per
weeks (from August 2004 to May 2005)

Figure 3: Cluster utilization as CPU consumed per VO and per week from August 2004 to May 2005

4.2 Waiting time

7

s
b
o

j
 
f

 

o
n
o

i
t
c
a
r
F

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

Dteam Job duration

Biomed Job duration

s
b
o

j
 
f

 

o
n
o

i
t
c
a
r
F

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

10 sec

1 min

5 min 15 min
Duration (hours)

1h

8h

24h 48h

10 sec

1 min

8h

24h 48h

5 min 15 min
Duration (hours)

1h

(a) Dteam job runtime

(b) Biomed job runtime

Figure 4: Dteam and Biomed job runtime distributions (logscale on time axis)

dard deviation and the mean. CV decreases as the
queue maximum runtime increase. This means that
jobs in shorter queues vary a lot in their duration
compared to longer jobs and we can expect that
more the upper bound given is high the more con-
ﬁdence in using the queue mean runtime as a an
estimation we could have.

A commonly used method for modelling duration
distribution is to use log-uniform distribution. Fig-
ures 4(a) and 4(b) show the fraction of Dteam and
Biomed jobs with duration less than some value.
Job duration has been modelled with a multi-stage
log-uniform model in [20] which is piecewise linear
in log space. In this case Dteam and Biomed job
duration could be approximated respectively with
a 3 and a 6 stages log-uniform distribution.

4.2 Waiting time

Table 4 shows that jobs coming from the Dteam
group are the more unfairly treaten. Dteam group
sends short jobs very often, Dteam jobs are then all
placed in queue waiting that long jobs from other
groups ﬁnished. Dzero group sends short jobs more
rarely and is also less penalized than Dteam be-
cause there are less Dzero jobs that are waiting
together in queue before being treated. The best
treated group is LHCb with not very long running
jobs (average of about 34 minutes) and one job

Group

Mean

Stretch

Biomed
781.5
Dteam 1424.1
217.7
LHCb
2332.8
Atlas
90.7
Dzero

0.874
0.135
0.905
0.848
0.701

Standard
Deviation
16398.8
26104.5
2000.7
13052.1
546.3

CV

20.9
18.3
9.1
5.5
6.0

Table 4: Group mean waiting time in seconds, cor-
responding Standard Deviation and Coeﬃcient of
Variation

about every 41 minutes. The best behavior to re-
duce waiting time per jobs seems to send jobs that
are not too short compared to the waiting factor,
and send not too very often in order to avoid that
they all wait together inside a queue. Very long
jobs is not a good behavior too as the scheduler
delay them to run shorter one if possible.

Table 5 shows the mean waiting time per jobs
on a given queue. There is a problem with such
a metric, for example: Consider one job arriving
on a cluster with only one free CPU, it will run
on it during a time T with no waiting time. Con-
sider now that this job is splitted in N shorter jobs
(numbered 0 . . . N − 1) with equal total duration
T . Then the waiting time for the job number i will
be iT /N , and the total waiting time (N − 1)T /2.

8

4 WORKLOAD DATA ANALYSIS

Queue

Mean

Test
Short
Long
Day
Inﬁnite

33335.9
1249.7
535.1
466.8
1753.9

Standard
Deviation
148326.4
27621.8
5338.8
8170.7
24439.8

CV Number
of jobs
45760
81963
32879
19275
49060

4.4
22.1
9.9
17.5
13.9

Table 5: Queue mean waiting time in seconds, cor-
responding Standard Deviation, Coeﬃcient of Vari-
ation and number of jobs

So the more a job is splitted the more it will wait
in total. Another metric that does not depend on
the number of jobs is the total waiting time divided
by the number of jobs and by the total job dura-
tion. Let note
W T this normalized waiting time,
We obtain:

d

T otalW aitingT ime
N Jobs ∗ T otalDuration
M eanW aitingT ime
N Jobs ∗ M eanDuration

W T =

d
W T =

d

(4.1)

Queue
Test
Short
Long
Day
Inﬁnite

d

W T

Group

W T
2.35e-2 Biomed
1.58e-5
d
1.02e-4 Dteam 6.79e-5
1.08e-5
LHCb
5.53e-6
2.23e-5
3.65e-6 Atlas
31.9e-5
3.55e-6 Dzero

Table 6: Queue and Group normalized waiting time

With this metric, the Test queue is still the most
unfairly treated and the Inﬁnite queue has the more
beneﬁts compared to the other queues. Dteam
group is again bad treated because their jobs are
mainly sent to the Test queue. The more unfairly
treated group is Dzero.

4.3 Arrival time

Job arrival daily cycle is presented in ﬁgure 5. This
ﬁgure shows the number of arrival depending on job
arrival hours, with a 10 minutes sampling. Clearly
users prefer to send their jobs at o’clock. In fact
we receive regular monitoring jobs from the VO
Dteam. The monitoring jobs are submitted every

Job arrivals per time of day

d
e
v
i
r
r
a

 
s
b
o

j
 
f

o

 
r
e
b
m
u
N

 4000

 3500

 3000

 2500

 2000

 1500

 1000

 500

 0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24
Hours

Figure 5: Job arrival daily cycle

Group

Biomed
Dteam
LHCb
Atlas
Dzero

Mean

Standard
(seconds) Deviation

223.6
256.2
2474.6
2824.1
5018.7

5194.5
2385.4
39460.5
60789.4
50996.6

CV

23.22
9.31
15.94
21.52
10.16

Table 7: Group interarrival time in seconds, cor-
responding Standard Deviation and Coeﬃcient of
Variation

hour from goc.grid-support.ac.uk. Users are lo-
cated in all Europe, so the eﬀect of sending at work-
ing hours is summed over all users timezones. How-
ever the shape is similar compared to other daily
cycle, during night (before 8am) less jobs are sub-
mitted and there is an activity peak around midday,
2pm and 4pm.

Table 7 shows the moments of interarrival time
for each group. CV is much higher than 1, this
means that arrivals are not Poisson processes and
are very irregularly distributed. For instance we
could receive 10 jobs in 10 minutes followed by
nothing during the 50 next minutes. In this case we
have a mean interarrival time of 6 minutes but in
fact when jobs arrived they arrived every minutes.

Figure 3(a) shows the system utilization of our
cluster during each week. There are a maximum of
980 CPU days consumed each week for 140 CPUs.
We have a highly varying cluster activity.

Frequencies (Biomed)

Frequencies (Dteam)

9

 1

 0.1

n
o

i
t
r
o
p
o
r
P

 0.01

 0.001

 0.0001

 1

 0.1

n
o

i
t
r
o
p
o
r
P

 0.01

 0.001

 0.0001

 1e-05

 0

 1e-05

 0

 5

 10

 20

 15
Jobs sent per 5 minutes

 25

 30

 35

 40

 5

 10

 15

 20

 25

Jobs sent per 5 minutes

(a) Biomed job arrival rate each 5 minutes

(b) Dteam job arrival rate each 5 minutes

Figure 6: Arrival frequencies for Biomed and Dteam VOs (Proportion of occurrences of n jobs received
during an interval of 5 minutes)

4.4 Frequency analysis

but only job arrival.

Job arrival rate is a common measurement for a
site usage in queuing theory. Figures 6(a) and 6(b)
present the job arrival rate distribution. It is the
number of time n jobs are submitted during inter-
val length of 5 minutes. They show that most of the
time the cluster does not receive jobs but jobs ar-
rived grouped. Users actually submit groups of jobs
and not stand-alone jobs. It explains the shape of
the arrival rate: it fastly decreases but too slowly
compared to a Poisson distribution. Poisson dis-
tribution is usually used for modelling the arrival
process but evidences are against that fact [21].

Dteam monitoring jobs are short and regular
jobs, there is no need of a special arrival model
for such jobs. What we observe for other kind of
jobs is that the job arrival law is not a Poisson Law
(see table 7 where CV ≫ 1) as for instance a web
site traﬃc [22]. What really happens is that users
come using the cluster from an User Interface dur-
ing some time interval. During this time they send
jobs to the cluster. Users log to an User Interface
machine in order to send their jobs to a RB that
dispatch them to some CEs. Note that one can
send jobs to our cluster only from an User Inter-
face, it means for instance that jobs running on a
cluster cannot send secondary jobs. On a comput-
ing site we do not have this user login information,

First we look at modelling user arrival and sub-
mission behavior. Secondly we show that the model
proposed shows good results for a group behavior.

5 Model

5.1 Login model

In this section we begin to model user Login/Logout
behavior from the Grid job ﬂow (ﬁgure 1). We
neglect the case where an user has multiple login
on diﬀerent UI at the same time. We mean that
a user is in the state Login if he is in the state of
sending jobs from an UI to our cluster, else he is in
the state Logout.

Markov chains are like automatons with for each
state a probability of transition. One property of
Markov chains is that future states depend only
on the current state and not on the past history.
So a Markov state must contains all the infor-
mation needed for future states. We decided to
model the Login/Logout behavior as a continuous
Markov chain. During each dt, a Logout user has
a probability during dt of λdt to login and a Login
user has a probability during dt of δdt to logout
(see ﬁgure 7). λ is called the Login rate and δ is
called the Logout rate.

10

5 MODEL

1 − λ dt

1 − δ

dt

5.2 Job submittal model

δ dt

Logout

Login

λ dt

Figure 7: Login/Logout cycle

All these parameters could vary over time as we
see with the variation of the week job arrival (ﬁg-
ure 3(a)) or during day time (ﬁgure 5) The model
proposed could be used more accurately with non-
constant parameters at the expense of more calcu-
lation and more diﬃcult ﬁtting. For example, one
could numerically use Fourier series for the Login
rate or for the submittal rate to model this daily
cycle. We use now constant parameters for calcu-
lation, looking for general properties.

We would like to have the probabilities during
time that the user is logged or not logged. Let
PLogin(t) and PLogout(t) be respectively probability
that the user is logged or not logged at time t. We
have from the modelling:

PLogout(t + dt) = (1 − λdt)PLogout(t)

PLogin(t + dt) = (1 − δdt)PLogin(t)

+ δdtPLogin(t)

+ λdtPLogout(t)

(5.1)

(5.2)

At equilibrium we have no variation so

PLogout(t + dt) = PLogout(t) = PLogout
PLogin(t + dt) = PLogin(t) = PLogin

(5.3)
(5.4)

We obtain:

PLogout =

PLogin =

δ
λ + δ
λ
λ + δ

(5.5)

(5.6)

During period when users are logged they could
submit jobs. We model the job submittal rate for
one user as follows: During dt when the user is
logged he has a probability of µdt to submit a job.
With δ = 0 we have a delayed Poisson process,
with µ = 0 no jobs are submitted. The full model
is shown at ﬁgure 8, it shows all the possible out-
comes with corresponding probabilities from one of
the possible state to the next after a small period
dt. Numbers inside circles are the number of jobs
submitted from the start. Login states are below
and Logout states are at the top. We have:

• Pn(t) is the probability to be in the state “User
is not logged at time t and n jobs have been
submitted between time 0 and t.”

• Qn(t) is the probability to be in the state
“User is logged at time t and n jobs have been
submitted between time 0 and t.”

• Rn(t) is the probability to be in the state “n
jobs have been submitted between time 0 and
t.” We have Rn = Pn + Qn.

From the model, we obtain with the same
method as before this recursive diﬀerential equa-
tion:

M =

−λ
λ −(µ + δ)(cid:19)

δ

(cid:18)

= M

P0
Q0(cid:19)

(cid:18)

= M

Pn
Qn(cid:19)

(cid:18)

+

0
µQn−1(cid:19)

(cid:18)

′

P0
Q0(cid:19)
′

Pn
Qn(cid:19)

(cid:18)

(cid:18)

(5.7)

(5.8)

(5.9)

This results to the following recursive equation
(in case the parameters are constants, M is a con-
stant)

Pn
Qn(cid:19)

(cid:18)

= eMt

e−Mx

Z

0
µQn−1(cid:19)

(cid:18)

dx (5.10)

We take a look at the probability of having no
job arrival during an interval of time t which is P0
and Q0. R0 is the the probability that no jobs have
been submitted between arbitrary time 0 and t. So
from the above model, we have:

′

P0
Q0(cid:19)

(cid:18)

=

−λ
δ
λ −(µ + δ)(cid:19) (cid:18)

P0
Q0(cid:19)

(cid:18)

(5.11)

5.3 Model characteristics

11

1 − λ dt

1 − λ dt

1 − λ dt

1 − λ dt

Logout

0

Login

0

λ dt

λ dt

λ dt

λ dt

δ dt

δ dt

δ dt

δ dt

1

1

2

2

3

3

dtµ

dtµ

dtµ

dtµ

1 − (δ + µ)

dt

1 − (δ + µ)

dt

1 − (δ + µ)

dt

1 − (δ + µ)

dt

Figure 8: Markov modelling of jobs submittal

At arbitrary time we could be in the state Login
with probability λ/(λ + δ) and in the state Logout
with the probability δ/(λ + δ). We have from the
results above:

P0(0)
Q0(0)(cid:19)

(cid:18)

=

PLogout
PLogin (cid:19)

=

(cid:18)
R0 = P0 + Q0

1

λ + δ (cid:18)

δ
λ(cid:19)

(5.12)

(5.13)

Finally we obtain the result.

R0(t) = m0

e−m1t − e−m2t
m1 − m2

+

m1e−m2t − m2e−m1t
m1 − m2

Where

m0 =

= µPLogin

λµ
λ + δ

m1 + m2 = λ + δ + µ

m1m2 = λµ

(5.14)

(5.15)

(5.16)

(5.17)
(5.18)

With λ = 0 or µ = 0, we obtain that no jobs
are submitted (R0(t) = 1). With δ = 0, this is
a Poisson process and R0(t) = e−µt. Note that

during a period of t there are in average µPLogint
jobs submitted, we have also for small period t,

R0(t) ≈ 1 − µPLogint

We have also
R′

0(0) = −µPLogin

R′

0(0) = −

Number of jobs submitted
Total duration

R0(t) could be estimated by splitting the arrival
processes in intervals of duration t and estimating
the ratio of intervals with no arrival. The error of
this estimation is linear with t. Another issue is
that the logs precision is not below one second.

5.3 Model characteristics

We have also these interesting properties:

= −µ

R′
0(0) = −µPLogin
R′′
0 (0)
R′
0(0)
0(0)2
R′
R′′
0 (0)
R′′′
0 (0)
R′′
0 (0)

= −(µ + δ)

= PLogin

(5.19)

(5.20)

(5.21)

(5.22)

(5.23)

(5.24)

(5.25)

12

6 SIMULATION AND VALIDATION

Probability distribution of the duration between
two jobs arrival is called an interarrival process.
Interarrival process is a common metric in queuing
theory. We have A(t) = P0(t) + Q0(t) with the
initial condition that user just submits a job. This
implies that user is logged.

P0(0) = 0.0, Q0(0) = 1.0

Another interesting property is the number of
jobs submitted by this model during a Login period.
Let Pn be the probability to receive n jobs during
a Login period. We have:

Pn =

Pn =

∞

(µt)n
n!

Z
0

δ
µ + δ

(

µ
µ + δ

)n

e−µtδe−δtdt

(5.36)

(5.37)

A(t) = µ

e−m1t − e−m2t
m1 − m2

+

m1e−m2t − m2e−m1t
m1 − m2

This is a geometric law. The mean number of jobs
submitted by Login period is µ/δ.

(5.26)

(5.27)

5.4 Group model

Groups are composed of users, either regular users
sending jobs at regular time or users with a Lo-
gin/Logout like behavior. Metrics deﬁned below as
the mean number of jobs sent by Login state, the
mean submittal rate and probability of Login could
represent an user behavior.

p =

µ − m2
m1 − m2

A(t) = pe−m1t + (1 − p)e−m2t

(5.28)

We have µ ∈ [m1; m2] because

(µ − m1)(µ − m2) = µ
(µ − m1)(µ − m2) = −δµ < 0

2 − (λ + δ + µ)µ + δµ

(5.29)

 0.0001

Job submittal rate

So p ∈ [0; 1], and we have an hyper-exponential
interarrival law of order 2 with parameters p =
(µ − m2)/(m1 − m2), m1, m2. This result is co-
herent with other experimental ﬁtting results [23]
Moreover any hyper-exponential law of order 2 may
be modelled with the Markov chain described in ﬁg-
ure 8 with parameters µ = pm1 + (1 − p)m2, λ =
m1m2/µ, δ = m1 + m2 − µ − λ

Let

calculate

the mean interarrival

time.
Probability to have an interarrival time between
θ and θ + dθ is A(θ) − A(θ + dθ) = −A′(θ)dθ. The
mean is

)
s
d
n
o
c
e
s
 
r
e
p
 
s
b
o
J
(
 
e
t
a
r
 
l
a
t
t
i

m
b
u
s
 
b
o
J

 1e-05

 1e-06

 1e-07

 1e-08

 1e-09

 0

−θA′(θ)dθ =

A(θ)dθ (5.30)

∞

Z
0

Figure 9: Users job submittal rates during their
period of activity

 20

 40

 80

 100

 120

 60
User rank 

∞

Z
0

˜A =

˜A =

1
µPLogin

=

λ + δ
λµ

Let compute the variance of interarrival distribu-

tion.

var =

−(θ − ˜A)

2A′(θ)dθ

∞

Z
0

∞

Z
0

2

var = 2

θA(θ)dθ − ˜A2

var
˜A2 = CV
CV

2

= 1 + 2 P 2

Logout

= 1 + 2

δµ
(λ + δ)2
µ
δ

(5.31)

(5.32)

(5.33)

(5.34)

(5.35)

Figure 9 shows the sorted distribution of users
submittal rate (µPLogin). Except for the highest
values it is quite a straight line in logspace. This
observation could be included in a group model.

6 Simulation and validation

We have done a simulation in Scheme [24] directly
using the Markov model. We began by ﬁtting users
behavior from the logs with our model. Like the
frequency obtained from the logs, the model shows

 1

 0.1

y
t
i
l
i

b
a
b
o
r
P

 0.01

 0.001

 0.0001

 1e-05

 0

 1

 0.1

y
t
i
l
i

b
a
b
o
r
P

 0.01

 0.001

 0.0001

 1e-05

 0

13

Biomed user
Simulation (Error = 4.929e-3)
Poisson distribution fitting with the same mean

Biomed user
Simulation (Error = 3.534e-3)

 1

 0.1

y
t
i
l
i

b
a
b
o
r
P

 0.01

 0.001

 0.0001

 1e-05

 0

 1

 0.1

y
t
i
l
i

b
a
b
o
r
P

 0.01

 0.001

 0.0001

 1e-05

 5

 10

 15

 20

 25

 5

 10

 15

 20

 25

Jobs sent per 5 minutes

Jobs sent per 5 minutes

(a) Biomed user 1

(b) Biomed user 2

Biomed user
Simulation (Error = 1.1078e-2)

Biomed user
Simulation (Error =  8.78e-2)

 5

 10

 15

 20

 25

 0

 5

 10

 15

 20

 25

Jobs sent per 5 minutes

Jobs sent per 5 minutes

(c) Biomed user 3

(d) Biomed user 4

Name
Biomed user 1
Biomed user 2
Biomed user 3
Biomed user 4

µ
0.0837
0.0620
0.0832
0.0365

δ
0.02079
0.01188
0.02475
1.4285e-3

λ
2.1e-4
1.2e-4
2.5e-4
1.075e-4

Error
4.929e-3
3.534e-3
1.1078e-2
8.78e-2

Figure 10: Biomed simulation results

14

6 SIMULATION AND VALIDATION

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

 1

 0.99

 0.98

 0.97

 0.96

 0.95

 0.94

 0.93

y
t
i
l
i

b
a
b
o
r
P

y
t
i
l
i

b
a
b
o
r
P

R0 probability for Biomed user 6 at LPC cluster
Hyper-exponential fitting (order 2)

R0 probability for user 3 at NASA Ames
Hyper-exponential fitting (order 2)

 0.65

 0

 5000

 25000

 30000

 5000

 0

 0

 10000

 15000
Interval length (seconds)

 20000

 10000

 15000
Interval length (seconds)

 20000

 25000

 30000

(a) LPC cluster Biomed user

(b) NASA Ames most active user

R0 probability for user 75 at DAS2 fs0 cluster
Hyper-exponential fitting (order 2)

R0 probability for user 35 at SDSC Blue Horizon
Hyper-exponential fitting (order 2)

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

y
t
i
l
i

b
a
b
o
r
P

y
t
i
l
i

b
a
b
o
r
P

 0.92

 0

 2000

 10000

 12000

 5000

 0.65

 0

 4000

 6000
Interval length (seconds)

 8000

 10000

 15000
Interval length (seconds)

 20000

 25000

 30000

(c) DAS2 fs0 cluster most active user

(d) SDSC Blue Horizon most active user

Figure 11: Hyper-Exponential ﬁtting of R0 for a Biomed LPC user and for the most active users at
NASA Ames, DAS2 and SDSC Blue Horizon clusters.

6.1 Other workloads comparison

15

a majority of intervals with no job arrival, possi-
bly followed by a relatively ﬂat area and a fast de-
creases. Some ﬁtting results are presented in ﬁg-
ures 10. Norm used to ﬁt real data is the max-
imum diﬀerence between the two cumulative dis-
tributions. We ﬁtted the frequency data for each
user.

During a period of t there are in average µPLogint
jobs submitted. We evaluate the value of µPLogin
which is the average number of jobs submitted by
seconds. We use that value when doing a set of sim-
ulation in order to ﬁt a known real user probability
distribution. We have two free parameters, so we
vary PLogin between 0.0 and 1.0 and lambda which
the inverse is the average time an user is Logout.
Some results obtained are shown in ﬁgures 10.

µ parameter decides of the frequency length of
the curve. Without the Login behavior we would
have obtained a classic Poisson curve of µ parame-
ter. 1/µ is the mean interarrival time during Login
period. An idea to evaluate µ would be to evaluate
the job arrival rate during Login periods, but we
lack that Login information.

δ and λ are the Logout and Login parameters.
What is really important is the ratio λ/(λ + δ)
which is PLogin. This is the ratio between time
user is active on the cluster and total time. δ and
PLogin are measures of the deviation from a clas-
sic Poisson law. For instance, the mean number
of jobs submitted by Login period is µ/δ and the
mean job submittal rate is µPLogin. For a same
PLogin we could have very diﬀerent scenarios. A
user could be active for long time but rarely logged
and another user could be active for short period
with frequent login. 1/δ is the mean Login time,
1/λ is the mean Logout time.

The R0 probability is essential for studying job
arrival time. 1 − R0(t) is the probability that be-
tween time 0 and t we have received at least one
job. It is easier to ﬁt the R0 distribution for an user
than the interarrival distribution because we have
more points. Figure 11(a) shows a typical graph of
R0 for a Biomed user. It shows for instance that
for intervals of 10000 seconds, this Biomed user has
a probability of about 0.2 to submits one or more
jobs. We have ﬁtted this probability with hyper-
exponential curve, that is a summation of expo-
nential curves. There was too much noises for high
interval time to ﬁt that curve. In fact errors on R0
are linear with t. So we have smoothed the curve

before ﬁtting by averaging near points. R0 for this
user was ﬁtted with a sum of two exponentials.

It seems that more than the Login/Logout be-
havior there is also a notion of user activity. For
example during the preparation of jobs or analysis
phase of the results an user does not use the Grid
and consequently the cluster at all. More than the
Login and Logout state an Inactive state could be
added to the model if needed.

6.1 Other workloads comparison

User number 3 is the most active user from the
NASA Ames iPSC/860 workload †. Figure 11(b)
shows its R0(t) probability,
it is clearly hyper-
exponential of order 2, as other users like number
22 and 23. Other users like number 12 and 15 are
more classical Poissonian users.

DAS2 Clusters (see note †) used also PBS and
MAUI as their batch system and scheduler. The
main diﬀerence we have with them is that they use
Globus to co-allocate nodes on diﬀerent clusters.
We only have bag of tasks applications which in-
teracts together in a pipeline way by ﬁles stored
on SEs. Their fs0 144 CPUs cluster is quite similar
with ours. Figure 11(c) shows the R0(t) probability
for their most active user and corresponding hyper-
exponential ﬁtting or order 2.

SDSC Blue Horizon cluster (see note †) have a to-
tal of 144 nodes. The R0(t) distribution probability
of their most active user was ﬁtted with a hyper-
exponential of order 2 in ﬁgure 11(d).

7 Related works

Our Grid environment is very particular and dif-
ferent from common cluster environment as paral-
lelism involved requires no interaction between pro-
cesses and degree of parallelism is one for all jobs.
To be able to completely simulate the node us-
age we need not only the jobs submittal process but
also the job duration process. Our runtime model

†The workload log from the NASA Ames iPSC/860
The work-
was graciously provided by Bill Nitzberg.
from DAS2 were graciously provided by
load logs
The work-
Hui Li, David Groep and Lex Wolters.
load log from the SDSC Blue Horizon was graciously
and Nancy Wilkins-
provided by Travis Earheart
Diehr.
the Parallel Workload
All are available at
Archives http://www.cs.huji.ac.il/labs/parallel/workload/

16

8 DISCUSSION

is similar with the Downey model [20] for run-
time which is composed of linear pieces in logspace.
There is a strong correlation between successive
jobs running time but it seems unlikely that a gen-
eral model for duration may be made because it de-
pends highly on algorithms and data used by users.
Most other models use Poisson distribution for
interarrival distribution. But evidences, like CV
be much higher than one, demonstrate that ex-
ponential distributions does not ﬁt well the real
data [25, 26]. The need of a detailed model was
expressed in [27]. With constant parameters our
model exhibits a hyper-exponential distribution for
interarrival rate and justify such a distribution
choice. One strong beneﬁt of our model is that
it is general and could be used numerically with
non-constant parameters at the expense of diﬃcult
ﬁtting.

8 Discussion

What could be stated is that job maximum run
times provided by users are essentially inaccurate,
some authors are even not using this information
for scheduling [2]. Maybe a better concept is the
relative urgency of a job. For example on a grid
software managers are people responsible for in-
stalling software on cluster nodes by sending in-
stallation jobs. Software manager jobs may be re-
garded as more urgent than other jobs type. So
sending jobs with an estimated runtime could be
replaced by sending jobs with an urgency param-
eter. That urgency could be established in part
as a site policy. Each site administrator could de-
ﬁne some users classes for diﬀerent kind of jobs and
software used with diﬀerent jobs priorities. For in-
stance a site hosted in some laboratory might wish
to promote its scientiﬁc domain more than other
domain, or some speciﬁc applications might need
quality of services like real time interaction.

Another idea for scheduling is to have some sort
of risk assessment measured during the scheduler
decision. This risk assessment may be based on
blocking probabilities obtained either from the logs
or from some user behavior models. For example,
it could be wise to forbid that a group or an user
takes all the cluster at a given time but instead to
let some few percents of it open for short jobs or
low CPU consuming jobs like monitoring.

Information System shows for a site the num-
ber of job currently running and waiting. But it
is not really the relevant metric in an on-line envi-
ronment. A better metric for a cluster is the com-
puting ﬂow rate input and the computing ﬂow rate
capacity. A cluster is able to treat some amount
of computation per unit of time. So a cluster is
contributing to the Grid with some computation
ﬂow rate (in GigaFLOPS or TeraFLOPS). As with
classical queuing theory if the input rate is higher
than the capacity, the site is overloaded and the
global performances are low due to jobs waiting to
be processed. What happens is that the site receive
more jobs that is is able to treat in a given time.
So the queues begin to grow and jobs have to wait
more and more before being started, resulting in
performance decay. Similarly when the computa-
tion submitted rate is lower than the site capacity
the site is under-used. Job submittal have also to
be fairly distributed according to the site capac-
ity. For example, a site that is twice bigger than
another site have to receive twice more computing
request than the other site. But there is a problem
to globally enforce this submittal scheme on all the
Grid. This is why a local site migration policy may
be better than a central migration policy done with
the RB.

To be more precise there are two diﬀerent kinds
of cluster ﬂow rate metrics, one is the local ﬂow
rate and the other one is the global ﬂow rate. The
local computing ﬂow rate is the ﬂow rate that one
job sees when reaching the site. The global ﬂow
rate is the computing ﬂow rate a group of jobs
see when reaching the site. That global ﬂow rate
is also the main measurement for meta-scheduling
between sites. These two metrics are diﬀerent, for
instance we could have a site with a lot of slow ma-
chines (low local ﬂow rate and big global ﬂow rate)
and another site with only few supercomputers (big
local ﬂow rate and low global ﬂow rate). But the
most interesting metric for one job is the local ﬂow
rate. This means that if each job wants individu-
ally to be processed at the best local ﬂow rate site,
this site will saturate and be globally slow.

As far as all users and groups computation total
ﬂow rate is less than the site global ﬂow rate or
site capacity, there is no real fairness issue because
there is no strife to access the site resources, there
is enough for all. The problem comes when the
sum of all computation ﬂow rate is greater than

17

the site capacity, ﬁrstly this globally reduces the
site performance, secondly the scheduler must take
decision to share fairly these resources. The Grid
is an ideal tool that would allow to balance the
load between sites by migrating jobs [2]. A site
that share their resources and is not saturated could
discharge another heavily loaded site. Some kind
of local site ﬂow control could maintain a bounded
input rate even with ﬂuctuating jobs submittal. For
instance fairness between groups and users could
be maintained by decreasing the most demanding
input rate and distributing it to other less saturated
sites.

Another beneﬁts is that applications computing
ﬂow rates may be partly expressed by users in their
job requirements. Computing ﬂow rate takes into
account both the jobs sizes and their time limits.
Fairness between users is then ensured if whatever
may be ﬂow values asked by each user, part granted
to each penalizes no other one. Computing ﬂow
rate granted by a site to an application may depend
on the applications degree of parallelism, that is for
the moment the number of jobs. For instance it
may be more diﬃcult to serve an application com-
posed of only one job asking for a lot of computing
ﬂow rate than to serve an application asking the
same computing ﬂow rate but composed of many
jobs. Urgency is not totally measured by a com-
puting ﬂow rate. For example a critical medical
application which is a matter of life or death ar-
riving on a full site has to be treated in priority.
Allocating ﬂow rates between users and groups has
to be right and to take under account priority or
urgency issues.

To use a site wisely users have to bound their
computational ﬂow rate and to negotiate it with
site managers. A computing model has to be de-
ﬁned and published. These remarks are impor-
tant in the case of on-line computing like Grids
where meta-scheduling strategy have to take a lot
of parameters into account. General on-line load
balancing and scheduling algorithms [28, 29, 30, 31]
may be applied. The problem of ﬁnding the best
suited scheduling policy is still an open problem. A
better understanding of job running time is nece-
ssary to have a full model.

The LCG middleware allows users to send their
jobs to diﬀerent nodes. This is done by the way of a
central element called a Resource Broker, that col-
lects user’s requests and distributes them to com-

puting sites. The main purpose is to match the
available resources and balance the load of job sub-
mittal requests. Jobs are better localized near the
data they need to use.

We would like to advise instead a peer to peer [32]
view of the Grid over a centralized one.
In this
view computing sites themselves work together
with other computing sites to balance the aver-
age workload. Not relying on dependent services
greatly improves the reliability and adaptability of
the whole systems. That kind of meta-scheduling
have to be globally distributed as stated by Dmitry
Zotkin and Peter J. Keleher [12]:

In a distributed system like Grid, the use of a
central Grid scheduler ‡ may result in a performance
bottleneck and lead to a failure of the whole system.
It is therefore appropriate to use a decentralized
scheduler architecture and distributed algorithm.

gLite [33] is the next generation middleware for
Grid computing.
gLite will provide lightweight
middleware for Grid computing. The gLite Grid
services follow a Service Oriented Architecture
which will facilitate interoperability among Grid
services. Architecture details of gLite could be
viewed in [11]. The architecture constituted by this
set of services is not bound to speciﬁc implemen-
tations of the services and although the services
are expected to work together in a concerted way
in order to achieve the goals of the end-user they
can be deployed and used independently, allowing
their exploitation in diﬀerent contexts. The gLite
service decomposition has been largely inﬂuenced
by the work performed in the LCG project. Ser-
vice implementations need to be inter-operable in
such a way that a client may talk to diﬀerent inde-
pendent implementations of the same service. This
can be achieved in developing lightweight services
that only require minimal support from their de-
ployment environment and deﬁning standard and
extensible communication protocols between Grid
services.

9 Conclusion

So far we have analyzed the workload of a Grid
enabled cluster and proposed an inﬁnite Markov-
based model that describes the process of jobs ar-

‡like the Resource Broker used in LCG middleware

18

REFERENCES

rival. Then a numerical ﬁtting has been done be-
tween the logs and the model. We ﬁnd a very simi-
lar behavior compared to the logs, even bursts were
observed during the simulation.

Acknowledgments

cluster

The
at LPC Clermont-Ferrand was
funded by Conseil R´egional d’Auvergne within
the
INSTRUIRE project
the
(http://www.instruire.org)

framework of

References

[1] Dror G. Feitelson. Workload modeling for perfor-
mance evaluation. In Maria Carla Calzarossa and
Salvatore Tucci, editors, Performance Evaluation
of Complex Systems: Techniques and Tools, pages
114–141. Springer-Verlag, Sep 2002. Lect. Notes
Comput. Sci. vol. 2459.

[2] Darin England and Jon B. Weissman. Costs and
beneﬁts of load sharing in the computational grid.
In Dror G. Feitelson and Larry Rudolph, editors,
Job Scheduling Strategies for Parallel Processing.
Springer-Verlag, 2004.

[3] M. Garey and D.S. Johnson. Computers and
Intractability: A Guide to the Theory of NP-
Completeness. Freeman, San Francisco, CA., 1979.

[4] Stephan Mertens. The easiest hard problem: Num-
ber partitioning. In A.G. Percus, G. Istrate, and
C. Moore, editors, Computational Complexity and
Statistical Physics, New York, 2004. Oxford Uni-
versity Press.

[5] Dror Feitelson School. Parallel job scheduling
— a status report.
In Dror G. Feitelson, Larry
Rudolph, and Uwe Schwiegelshohn, editors, Job
Scheduling Strategies for Parallel Processing, pages
1–16. Springer Verlag, 2004.

[6] Dror G. Feitelson and Larry Rudolph. Parallel job
scheduling:
In Dror G.
Issues and approaches.
Feitelson and Larry Rudolph, editors, Job Schedul-
ing Strategies for Parallel Processing, pages 1–18.
Springer-Verlag, 1995. Lect. Notes Comput. Sci.
vol. 949.

[7] David Jackson, Quinn Snell, and Mark Clement.
Core algorithms of the Maui scheduler. In Dror G.
Feitelson and Larry Rudolph, editors, Job Schedul-
ing Strategies for Parallel Processing, pages 87–
102. Springer Verlag, 2001. Lect. Notes Comput.
Sci. vol. 2221.

[8] Brett Bode, David M. Halstead, Ricky Kendall,
and Zhou Lei. The Portable Batch Scheduler and
the Maui Scheduler on Linux Clusters, USENIX
Association. 4th Annual Linux Showcase Confer-
ence, 2000.

[9] S. Agostinelli et al. Geant 4 (GEometry ANd
Tracking): a Simulation toolkit. Nuclear Instru-
ments and Methods in Physics Research, pages
250–303, 2003.

[10] Ian Foster and Carl Kesselman. Globus: A
metacomputing infrastructure toolkit. The In-
ternational Journal of Supercomputer Applications
and High Performance Computing, 11(2):115–128,
Summer 1997.

[11] EGEE Design

Team.
architecture,

EGEE mid-
EGEE-DJRA1.1-
dleware
476451-v1.0, August 2004.
Also available
as https://edms.cern.ch/document/476451/1.0.

[12] Dmitry Zotkin and Peter J. Keleher. Job-length
estimation and performance in backﬁlling sched-
ulers. In HPDC, 1999.

[13] Antonio Delgado Peris, Patricia M´endez Lorenzo,
Flavia Donno, Andrea Sciab`a, Simone Campana,
and Roberto Santinelli. LCG User guide, 2004.

[14] G. Avellino, S. Beco, B. Cantalupo, A. Maras-
chini, F. Pacini, M. Sottilaro, A. Terracina,
D. Colling, F. Giacomini, E. Ronchieri, A. Gi-
anelle, R. Peluso, M. Sgaravatto, A. Guarise,
R. Piro, A. Werbrouck, D. Kouˇril, A. Kˇrenek,
L. Matyska, M. Mulaˇc, J. Posp´iˇsil, M. Ruda,
Z. Salvet, J. Sitera, J. ˇSkrabal, M. Voc ˙u, M. Mez-
zadri, F. Prelz, S. Monforte, and M. Pappalardo.
The datagrid workload management system: Chal-
lenges and results. Kluwer Academic Publishers,
2004.

[15] Dror G. Feitelson and Larry Rudolph. Toward con-
vergence in job schedulers for parallel supercom-
puters. In Dror G. Feitelson and Larry Rudolph,
editors, Job Scheduling Strategies for Parallel Pro-
cessing, pages 1–26. Springer-Verlag, 1996. Lect.
Notes Comput. Sci. vol. 1162.

[16] Su-Hui Chiang, Andrea Arpaci-Dusseau, and
Mary K. Vernon. The impact of more accurate
requested runtimes on production job scheduling
performance. In Dror G. Feitelson, Larry Rudolph,
and Uwe Schwiegelshohn, editors, Job Scheduling
Strategies for Parallel Processing, pages 103–127.
Springer Verlag, 2002. Lect. Notes Comput. Sci.
vol. 2537.

[17] Maria Calzarossa and Giuseppe Serazzi. Work-
load characterization: A survey. Proc. IEEE,
81(8):1136–1150, 1993.

REFERENCES

19

[28] Yossi Azar, Bala Kalyansasundaram, Serge A.
Plotkin, Kirk Pruhs, and Orli Waarts. On-line
load balancing of temporary tasks. J. Algorithms,
22(1):93–110, 1997.

[29] Yossi Azar, Andrei Z. Broder, and Anna R. Kar-
lin. On-line load balancing. Theoretical Computer
Science, 130(1):73–84, 1994.

[30] A. Bar-Noy, A. Freund, and J. Naor. New algo-
rithms for related machines with temporary jobs.
In E.K. Burke, editor, Journal of Scheduling, pages
259–272. Springer-Verlag, 2000.

[31] Tak-Wah Lam, Hing-Fung Ting, Kar-Keung To,
and Wai-Ha Wong. On-line load balancing of tem-
porary tasks revisited. Theoretical Computer Sci-
ence, 270(1–2):325–340, 2002.

[32] Nazareno Andrade, Walfredo Cirne, Francisco
Brasileiro, and Paulo Roisenberg. OurGrid: An
approach to easily assemble grids with equitable
resource sharing. In Proceedings of the 9th Work-
shop on Job Scheduling Strategies for Parallel Pro-
cessing, June 2003.

Design

of
[33] EGEE
services.
the
EGEE
available
as https://edms.cern.ch/document/487871/1.0.

Team.
middleware

grid
Also

Design

EGEE

JRA1,

2004.

[18] Steve J. Chapin, Walfredo Cirne, Dror G. Feitel-
son, James Patton Jones, Scott T. Leutenegger,
Uwe Schwiegelshohn, Warren Smith, and David
Talby. Benchmarks and standards for the evalua-
tion of parallel job schedulers. In Dror G. Feitelson
and Larry Rudolph, editors, Job Scheduling Strate-
gies for Parallel Processing, pages 67–90. Springer-
Verlag, 1999. Lect. Notes Comput. Sci. vol. 1659.

[19] Walfredo Cirne and Francine Berman. A com-
prehensive model of the supercomputer workload,
2001.

[20] Allen B. Downey and Dror G. Feitelson. The elu-
sive goal of workload characterization. Perf. Eval.
Rev., 26(4):14–29, 1999.

[21] Dror G. Feitelson and Bill Nitzberg. Job charac-
teristics of a production parallel scientiﬁc workload
on the NASA Ames iPSC/860. In Dror G. Feit-
elson and Larry Rudolph, editors, Job Scheduling
Strategies for Parallel Processing, pages 337–360.
Springer-Verlag, 1995. Lect. Notes Comput. Sci.
vol. 949.

[22] Vern Paxson and Sally Floyd. Wide area traf-
ﬁc: the failure of Poisson modeling. IEEE/ACM
Transactions on Networking, 3(3):226–244, 1995.

[23] Hui Li, David Groep, and Lex Wolters. Workload
characteristics of a multi-cluster supercomputer.
In Dror G. Feitelson, Larry Rudolph, and Uwe
Schwiegelshohn, editors, Job Scheduling Strategies
for Parallel Processing. Springer Verlag, 2004.

[24] Richard Kelsey, William Clinger,
Revised5
Jonathan Rees
on the algorithmic language Scheme.
SIGPLAN Notices, 33(9):26–76, 1998.

(Editors).

and
report
ACM

[25] Dror G. Feitelson. Metrics for parallel job schedul-
ing and their convergence. In Dror G. Feitelson and
Larry Rudolph, editors, Job Scheduling Strategies
for Parallel Processing, pages 188–205. Springer
Verlag, 2001. Lect. Notes Comput. Sci. vol. 2221.

[26] Joefon Jann, Pratap Pattnaik, Hubertus Franke,
Fang Wang, Joseph Skovira, and Joseph Riodan.
Modeling of workload in MPPs. In Dror G. Feitel-
son and Larry Rudolph, editors, Job Scheduling
Strategies for Parallel Processing, pages 95–116.
Springer Verlag, 1997. Lect. Notes Comput. Sci.
vol. 1291.

[27] David Talby, Dror G. Feitelson, and Adi Raveh.
Comparing logs and models of parallel workloads
using the co-plot method. In Dror G. Feitelson and
Larry Rudolph, editors, Job Scheduling Strategies
for Parallel Processing, pages 43–66. Springer Ver-
lag, 1999. Lect. Notes Comput. Sci. vol. 1659.


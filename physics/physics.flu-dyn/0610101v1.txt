6
0
0
2
 
t
c
O
 
3
1
 
 
]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[
 
 
1
v
1
0
1
0
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Blow Ups of Complex Solutions of the 3

-Navier-Stokes System

and
Renormalization Group Method

D

by

Dong Li1 and Ya. G. Sinai1 2

Abstract: We consider complex-valued solutions of the three-dimensional Navier-
Stokes system without external forcing on R3. We show that there exists an open
set in the space of 10-parameter families of initial conditions such that for each
family from this set there are values of parameters for which the solution develops
blow up in ﬁnite time.

Keywords: Navier-Stokes system, renormalization group theory, ﬁxed point,
the linearization near the ﬁxed point, spectrum of the linearized group, Hermite
polynomials

1Program in Applied and Computational Mathematics, Princeton University, Princeton, New Jersey,

U.S.A.

1Mathematics Department, Princeton University, Princeton, New Jersey, U.S.A. &
2Landau Institute of Theoretical Physics, Moscow, Russia

1. Introduction.
§

There are many phenomena in nature which can be considered as some manifestation

of blow ups, like hurricanes, tornadoes, sandstorms, etc. If we believe that Navier-Stokes
system describes well enough the motions of real gases and ﬂuids under normal conditions,

then it gives some reasons to expect that blow ups in solutions of this system also exist.

We consider in this paper the 3

-Navier-Stokes system for incompressible ﬂuids moving
without external forcing on R3 with viscosity equal to 1. After Fourier transform it becomes
R3,
a non-local, non-linear equation for a non-known function v(k, t) with values in C 3, k
t > 0. The incompressibility condition takes the form∗

v(k, t), k

= 0 and

D

∈

h

i

v(k, t) = exp

v(k, 0) + i

exp

2

k

t
|
{−

|

}

(t

{−

−

2

s)

k

|

|

}

ds

·

v(k

k′, s), k

Pk v(k′, s) dk′

−

i ·

(1)

t

Z0

h

ZR3

In this expression v(k, 0) is an initial condition and Pk is the orthogonal projection to the
k
subspace orthogonal to k, i.e. Pkv = v
. The formula (1) shows that the Navier-Stokes
system is genuinely inﬁnite-dimensional dynamical system: the value v(k, t) is determined
by the integration over all “degrees of freedom” and previous moments of time.

i ·
k,k
i
h

v,k
h

−

The problem of blow ups in solutions of the Navier-Stokes System(NSS) appeared after
classical works of J. Leray (see [Le 1]) where he proved the existence of the weak solutions

of NSS. O. Ladyzenskaya proved the existence of strong solutions of 2-dim NSS in bounded
domains (see [La 1]). Many important contributions to the modern understanding of the 2-

dim ﬂuid dynamics were done by C. Foias and R. Temam (see [FT]), V. Yudovich (see [Y1]),
Giga ([G1]) and others. However, the situation with the 3-dim NSS remained unclear. The

Clay mathematical institute announced the problem of existence of strong solutions of the
3-dimensional NSS as one of the most important problem in mathematics of the XXI-century
(see [Cl]).

In this paper we omit the condition that v(k, t) is the Fourier transform of a real-valued
vector ﬁeld in the x-space and consider (1) in the space of all possible complex-valued func-
tions with values in C 3.

In this situation the energy inequality does not hold. Detailed

∗Since k

R3, v(k, t)

C3, the order in the inner product is important.

∈

∈

assumptions concerning the initial condition v(k, 0) will be discussed later (see
7). In all
§
cases v(k, 0) will be bounded functions whose support is a neighborhood of some point
(0, 0, k(0)). The incompressibility condition implies that the components v1(k, 0),v2(k, 0) of
v(k, 0) are arbitrary functions of k while v3(k, 0) can be found from the incompressibility
condition

= 0.

v, k

h

i

Various methods (see, for example, [K], [C], [S1]) allow to prove in such cases the existence
and uniqueness of classical solutions of (1) on ﬁnite intervals of time. For these solutions

(see, for example [S2])

v(k, t)

|

| ≤

const exp

const √t

{−

k

, 0

· |

|}

t

≤

≤

t0 .

(2)

Presumably, v(k, t) has an asymptotics of this type but this requires more work. According
to a conventional wisdom, possible blow ups are connected with the violation of (2).

In this paper we ﬁx t and consider one-parameter families of initial conditions vA(k, t) =
Av(k, 0). We show that for some special v(k, 0) one can ﬁnd critical values Acr = Acr(t) such
that the solution vAcr (k, s) blows up at t so that for t′ < t both the energy and the enstrophy
are ﬁnite while at t′ = t they both become inﬁnite. Even more, for t′ < t the solution
t this region expands to
decays exponentially outside some region depending on t. As t′
an unbounded domain in R3.

↑

Our main approach is based on the renormalization group method which is so useful
in probability theory, statistical physics and the theory of dynamical systems. It is rather
diﬃcult to give the exact formulation of our result in the introduction because it uses some

notions, parameters, etc., which will appear in the later sections. Loosely speaking, we show
that in ℓ-parameter families of initial conditions, for ℓ = 10, one can ﬁnd values of parameters

for which the solutions develop blow ups of the type we already described. The meaning of
ℓ is explained in

4,
§

5,
§

6.
§

We thank C. Feﬀerman, W.E, K. Khanin and V. Yakhot for many useful discussions. A
big part of the text was prepared during the visit of the second author of the Mathemat-

ics Department of California Institute of Technology and we thank the Department for its
very warm hospitality. We also thank G. Pecht for her excellent typing of the manuscript.
The ﬁnancial support from NSF Grant DMS 0600996 given to the second author is highly

acknowledged.

2

2. Power Series for Solutions of the 3
§

D

Preliminary Changes of Variables

-Navier-Stokes-Systems and

Our general approach is based upon the method of power series which were introduced

in [S1], [S2]. We write down the solution of (1) in the form:

vA(k, t) = exp

2

k

t
|
{−

|

} ·

A v(k, 0) +

exp

(t

2

s)

k

Ap hp(k, s) ds

(3)

{−

−

|

|

} ·

p>1
X

The substitution of (3) into (1) gives the system of recurrent equations connecting the func-
tions hp:

h1(k, s) = exp

2

k

s
{−

|

|

}

v(k, 0),

h2(k, s) = i

k′, 0) , k

Pk v(k′, 0)

exp

i

k

s
{−

|

−

2

k′

|

s

k′

|

2

|

}

−

d3k′,

·

v(k

h

−

ZR3

s

0
Z

ZR3

(s

hp(k, s) = i

ds2

v(k

k′, 0), k

Pkhp

1(k′, s2)

h

−

i

−

·

exp

k

s
{−

|

−

2

k′

|

−

s2)

k′

2

|

|

}

−

d3k′ + i

s

s

ds1

ds2

0

p1,p2>1 Z
p1+p2=p
X

0
Z

h

ZR3

hp1(k

k′, s1), k

−

i ·

Pkhp2(k′, s2)

exp

(s

{−

−

·

s1)

k

|

−

|

−

2

k′

(s

s2)

k′

|

2

|

}

−

d3k′ +

s

ds1

i
0
Z

ZR3

hp

1(k

−

h

−

k′, s1), k

Pk v(k′, 0)

exp

(s

s1)

k

i

·

{−

−

|

−

|

−

|

|

}

2

k′

2

s

k′

d3k′ .

(6)

Clearly, hp(k, s)

k for every p

1, k

≥

R3.

∈

⊥

It follows from the results of [S2] that the series (3) converges for suﬃciently small s and

gives a classical solution of (1). Make the following change of variables which simpliﬁes (4),
(5), (6). Put ˜k = k√s, ˜k′ = k′√s, introduce relative times ˜s1, ˜s2, s1 = ˜s1s, s2 = ˜s2s and
denote gr(˜k, s) = hr

1. Then

, r

˜k
√s, s

(cid:16)

(cid:17)

≥

g1(˜k, s) = exp

2

˜k

{−|

|

} ·

˜k
√s

v

 

, 0

,

!

(4)

(5)

(4′)

t

Z0

3

g2(˜k, s) = h2

˜k
√s

 

, s

=

!

i
s2

ZR3

˜k

˜k′
−
√s

v

h

 

, 0

, ˜k

!

i ·

P˜kv

˜k′
√s

 

, 0

exp

!

˜k

2

˜k′

2

˜k′

d3˜k′ ,

{−|

−

|

− |

|

}

(5′)

gp(˜k, s) =

1

i
s

Z0

d˜s2

v

h

ZR3

exp

 

˜k

˜k

˜k′
−
√s

, 0

, ˜k

!

i ·

P˜kgp

−

1(˜k

˜s2 , ˜s2s)

{−|

−

|

−

2

˜k′

(1

˜s2)

2

˜k′

|

|

}

−

p
d3˜k′ +

1

1

+ i

d˜s1

d˜s2

gp1((˜k

˜k′)

˜s1, ˜s1s), ˜k

p1+p2=p
X
p1>1,p2>1

Z0

Z0

h

ZR3

−

p

i ·

P˜kgp2(˜k′

˜s2, ˜s2s) exp

(1

˜s1)

{−

−

˜k

|

2

˜k′

−

|

−

(1

˜s2)

2

˜k′

|

|

}

−

d3˜k′

p

1

+

i
s

d˜s1

Z0

ZR3

1((˜k

gp

−

h

−

˜k′))

˜s1 , ˜s1s), ˜k

P˜k v

i

 

˜k′
√s

, 0

! ·

p

exp

(1

{−

−

˜s1)

˜k

|

2

˜k′

|

−

2

˜k′

d3˜k′

− |

|

}

(6′)

support: for small s its values are of order

The function g2(˜k, s) has a singularity at s = 0 even in the case of functions with compact
. This singularity is integrable and all gp(k, s),

1
√s
p > 2, are bounded. The singularity is connected with our choice of the coordinates ˜k, ˜k′.

The formulas (4)-(6) or (4′)-(6′) resemble convolutions in probability theory. For example,
. Therefore it is natural to expect that

if C = supp v(k, 0) then supp hp = C + C +

+ C

· · ·
p times

|

{z

4

}

hp and gp satisfy some form of the limit theorem of probability theory. This question will be
discussed in more detail in the next sections.

Make another change of variables. Assume that we have some p. The terms in (6′) with
p1/2 will be called boundary terms. They will be treated as remainder
p1/2 and p2 ≤
p1 ≤
terms and will be estimated later. Suppose that we have some number ˜k(0) which later will
(r) = (0, 0, r˜k(0)). These will be the
be assumed to be suﬃciently large. Introduce the vector
concentrated,
will
which
points
e
R3. Thus instead of ˜k we have the
p1/2
new variable Y = (Y1, Y2, Y3) which typically will take values O(1). Put ˜κ(0) = (0, 0, ˜k(0)).

p1/2. We write ˜k =

all
(r) + √r

gr
Y, Y

near

be

−

≤

≤

K

K

∈

p

r

·

e
In all integrals over ˜s1, ˜s2 in (6′) make another change of variables 1

˜sj = θj
p2
j
(p2) + √pY ′. We write

, j = 1, 2.

−

Instead of the variable of integration ˜k′ introduce Y ′ where ˜k′ =
˜gr(Y, s) = gr(

(r) + √r Y, s), γ = p1

γ. Then from (6′)

p , p2

p = 1

−

K

e

˜gp(Y, s) = gp( ˜
K

(p) + √p Y, s) = p5/2

i



K

e

1

p2
1

p2
2

dθ1

Z0

dθ2 ·

p2
1 ·

p2
2 ·

˜gp1

ZR3 (cid:28)

(cid:18)

Y

Y ′

−
√γ

,

1
(cid:18)

−

s

θ1
p2
1 (cid:19)

(cid:19)

P˜κ(0) + Y

˜gp2

√p

Y ′

√1

γ

−

(cid:18)

,

1
(cid:18)

−

s

θ2
p2
2 (cid:19)

·

(cid:19)



, ˜κ(0) +

p1,p2>√p
X
p1+p2=p

Z0

Y
√p

·

(cid:29)

exp

θ1

˜κ(0) +

(−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y ′
γ

Y
−
√p
·

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

θ2

−

˜κ(0) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y ′
√p (1

γ)

−

)

d3 Y ′

.

#

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(7)

This is the main recurrent relation which we shall study in the next sections. It is of some
importance that in front of (7) we have the factor p5/2 and inside the sum the factor 1
p2
1 ·
Both are connected with the new scaling inherent to the Navier-Stokes system.

1
p2
2

.

3. The Renormalization Group Equation
§

−→ ∞

As p

the recurrent equation (7) takes some limiting form which will be derived in

this section. All remainders which appear in this way are listed and estimated in

8.
§
The main contribution to (7) comes from p1, p2 of order p. If Y, Y ′ = O(1) then Y

Y ′
√p , Y ′
√p
are small compared to ˜κ(0) = (0, 0, ˜k(0)). Therefore the Gaussian term in (7) can be replaced

−

5

(θ1 + θ2)

by exp
and Y ′ can be done separately. Thus instead of (7) we get a simpler recurrent relation:

, ˜s1 and ˜s2 can be replaced by 1 and the integrations over θ1, θ2

{−

}

|

|

˜k(0)

2

˜gp(Y, s) =

p5/2

i
˜k(0)

|

4

|

Xp1,p2>p1/2
p1+p2=p

1

p2
1 ·

p2
2 ·

ZR3 (cid:28)

(cid:18)

˜gp1

Y

Y ′

−
√γ

(cid:19)

, s

, ˜κ(0) +

Y
√p

·

(cid:29)

In view of incompressibility

P˜κ(0)+ Y

√p

˜gp2

·

 

, s

d3 Y ′ .

γ)

!

Y ′
(1

−

p

˜gp1

Y

Y ′
√γ , s
−

, ˜κ(0) + Y
√p

=

(cid:17)

·

κ(0)

p1 + Y

Y ′

−
√γ

E
√p1, s

·

D
= 1
p1

(cid:16)
gp1

(cid:16)
D
< gp1

= 1
p1

+ 1
p1

< gp1

= 1

√p1 ·

κ(0)p1 + Y
(cid:16)
κ(0) p1 + Y
(cid:16)
< ˜gp1

Y ′
√γ , s
−

Y

(cid:16)
Y ′
√γ , s
−

Y

(cid:17)
, Y ′

Y ′

−
√γ

√p1 , s

·

·

Y ′

−
√γ

(cid:17)

(cid:17)
(γ

, Y

Y ′
√γ >
−

1) +

·

−

+ 1
√p2

< ˜gp1

(cid:16)
Write ˜gp in the form

(1

γ) >

−

(cid:17)

p

, κ(0)p1 + Y

γ√p

·

(cid:17)
, κ(0) p1 + Y

E

Y ′

√γ √p1 > +
−

√p1, s

, Y γ

√p

(Y

Y ′) √p > =

·

−

−

˜gp(Y, s) = (G(p)

1 (Y, s) , G(p)

2 (Y, s) ,

F (p)(Y, s)) .

1
√p

˜k
r

Since ˜k = ˜κ(0)

·

p + Y √p, the incompressibility implies

< gr(˜k, s) , ˜k > = < gr(˜k, s) ,

> = 0

and for Y = O(1)

Y1
√r ·

Y2
√r

G(r)

1 (Y, s) +

G(r)

2 (Y, s) +

F (r)(Y, s) = O

1
r

(cid:18)

(cid:19)

.

˜k(r)
√r ·

6

(8)

(9)

(10)

(11)

(12)

In our approximation we replace (12) by

Y1G(r)

1 (Y, s) + Y2G(r)

2 (Y, s) + F (r)(Y, s) = 0 .

(13)

Thus for given Y1, Y2, Y3 the component Fr can be expressed through G(r)
to be true even if we do not neglect the rhs of (13). Return back to (9). From (13)

1 , G(r)

2 . This remains

< ˜gp1

Y

Y ′
√γ , s
−

, ˜κ(0) + Y

√p >= 1

√p

1

γ
√γ < ˜gp1
−

Y

Y ′
√γ , s
−

, Y

Y ′
√γ >
−

(cid:16)

(cid:17)

h
γ > ] =

+ √1

(cid:17)
γ < ˜gp1

(cid:16)

−

Y

Y ′
√γ , s
−

= 1
√p

γ
1
−
√γ

(cid:16)
Y ′1
√γ G(p1)

−

1

Y1

,

Y ′
√1

−

(cid:17)
Y
Y ′
√γ , s
−

+ Y3

−
√γ

h
Y ′3

(cid:16)
1
√p1 ·

·

F (p1)

(cid:16)
Y
Y ′
√γ , s
−

(cid:17)
+

(cid:16)

(cid:17)

p

+ Y2

Y ′2
√γ G(p1)

−

2

Y

Y ′
√γ , s
−

(1

γ)

−

(cid:16)
Y ′1
√1

−

(cid:16)

(cid:17)

(cid:16)

γ G(p1)

1

Y

Y ′
√γ , s
−

(cid:17)

+ Y ′2
√1
−

γ G(p1)

2

Y

Y ′
√γ , s
−

+ 1
√p2

Y ′3
√1

−

γ F (p1)

Y

Y ′
√γ , s
−

.

(cid:16)
In our approximation the inner product in (14) can be replaced by

(cid:17)

(cid:16)

(cid:17)(cid:17)i

1
√p

γ
1
−
√γ

Y1

Y ′1
√γ G(p1)

−

1

Y

Y ′
√γ , s
−

+ Y2

Y ′2
√γ G(p1)

−

2

Y

Y ′
√γ , s
−

h

(cid:16)

+

1

γ

−

p

√1

γ

−

(cid:18)

(cid:16)
Y ′1

(cid:17)
G(p1)
1

Y

Y ′

−
√γ

, s

(cid:18)

(cid:16)
+

(cid:17)(cid:17)

Y2

G(p1)
2

(cid:19)

√1

γ

−

Y

Y ′

s

−
√γ

(cid:18)

(cid:19)(cid:19)(cid:21)

(14)

(15)

According to the deﬁnition of the projector

P˜κ(0)+ Y

√p

˜gp2

Y ′
√1

γ , s

−

(cid:16)

(cid:17)

= ˜gp2

Y ′
√1

γ , s

−

(cid:16)

(cid:17)

< ˜gp2

−

Y ′
γ , s
√1
−
(cid:17)
< ˜κ(0) + Y

, ˜κ(0) + Y

√p >
√p , ˜κ(0) + Y

·
√p >

(cid:16)

(˜κ(0) + Y

√p)

= ˜gp2

, s

+ O

.

(16)

Y ′

√1

γ

−

(cid:18)

(cid:19)

1
√p2 (cid:19)

(cid:18)

This shows that in the main order of magnitude the projector is the identity operator and

we come to a simpler recurrent relation instead of (8):

7

˜gp(Y, s) = i λ1 ·

p2

p2
1 ·

p2
2 ZR3 (cid:20)

p1/2
Xp1,p2≥
p1+p2=p

1

γ
−
√γ

Y ′1

Y1 −
√γ

·

(cid:18)

Y ′2

Y2 −
√γ

G(p1)
2

Y

Y ′

, s

−
√γ

+ √1

γ

−

(cid:19)(cid:19)

G(p1)
1

Y

Y ′

−
√γ

(cid:18)

Y ′1

√1

γ

−

(cid:18)

G(p1)
1

, s

+

(cid:19)
Y

Y ′

−
√γ

(cid:18)

(cid:18)

Y ′2

√1

γ

−

, s

+

(cid:19)

G(p1)
2

Y

Y ′

, s

−
√γ

(cid:18)

Y ′

˜gp2

·

√1

(cid:18)

−

, s

γ

(cid:19)

(cid:19)(cid:19)(cid:21)

d3Y ′ .

(17)

The main assumption which we shall check below in the next sections concerns the asymptotic
form of ˜gp(Y, s) as p
+ ] on the time axis and some
Λ, positive σ(1), σ(2) and for all r < p

: for some interval S(p) = [S(p)
−

−→ ∞

, S(p)

˜gr(Y, s) = Λr

1r

−

σ(1)
2π

·

exp

σ(1)
2

−

(cid:26)

2 +

Y1|

|

(cid:0)

Y 2
2 |

|

· r

(cid:27)
(cid:1)
2 (Y, s) , δ(r)

σ(2)
2π

exp

σ(2)
2 |

2

Y3|

·

(cid:27)

−

(cid:26)

(18)

(H1(Y1, Y2, Y3) + δ(r)

1 (Y, s), H2(Y1, Y2, Y3) + δ(r)

3 (Y, s))

where δ(r)
sense the convergence to zero takes place. The substitution of (18) into (17) gives

, j = 1, 2, 3. Later we shall explain in more detail in what

j (Y, s)

−→ ∞

0 as r

−→

˜gp(Y, s) =

i
˜k(p)

|

4 ·

|

p

·

Λp

−

2

·

1
p ·

1

γ

2 (1

1
2

γ)

−

γ
1
−
√γ ·

Y ′1

Y1 −
√γ

H1

·

Y

Y ′

−
√γ

(cid:18)

(cid:18)

+

(cid:19)

·

ZR3 (cid:20)

Xγ= p1

p

+

Y ′2

Y2 −
√γ

H2

Y

Y ′

−
√γ

+ √1

γ

−

(cid:19)(cid:19)

Y ′1

√1

γ

−

(cid:18)

H1

(cid:18)

Y

Y ′

−
√γ

+

(cid:19)

(cid:18)

(cid:18)

+

Y ′2

√1

γ

−

H2

Y

Y ′

−
√γ

H

Y ′

√1

(cid:19)(cid:19)(cid:21)

(cid:18)

σ(1)
2πγ ·

exp

σ(1)
2

Y1 −

|

Y ′1|

2 +
γ

−

(cid:26)

(cid:18)

·

γ

(cid:19)

2

Y ′2|

−
Y2 −

|

·

(cid:19)(cid:27)

8

σ(1)

2π(1

γ)

−

exp

−

(cid:26)

σ(1)
2

Y ′1|

|

2 +
1

−
Y3 −
γ

|

2

Y ′2|
|
γ

2

Y ′3|

·

(cid:27)

σ(2)
2πγ

s

exp

σ(2)
2

−

(cid:26)

σ(2)

· s

2π(1

γ)

−

(cid:27)

exp

σ(2)
2

2

Y ′3|
|
γ
1
−

(cid:27)

−

(cid:26)

d3Y ′ .

(19)

Here

Y ′

H

√1

γ

(cid:19)

−

(cid:18)

=

H1

(cid:18)

Y ′1

√1

(cid:18)

−

,

γ

√1

Y ′2

−

,

γ

Y ′3

,

√1

γ

(cid:19)

−

H2

Y ′1

√1

(cid:18)

−

,

γ

√1

Y ′2

−

,

γ

Y ′3

√1

γ

(cid:19)

−

, 0

.

(cid:19)

We do not mention explicitly the dependence of H on s.

The last sum looks like a Riemannian integral sum whose limit takes the form as p

:

−→ ∞

Λ exp

σ(1)
2

2 +

(

Y1|

|

2)

Y2|

|

·

o

σ(1)
2π ·

exp

σ(1)
2πγ

exp

−

(cid:26)

−

(cid:26)

2

σ(2)
Y3|
|
2

(cid:27) r
Y ′1|

Y1 −

|

σ(1)(

σ(2)
2π

2 +
2γ

ZR3

H(Y )

Y2 −

|

Y ′2|

2)

(cid:27)

1

γ

2 (1

1

γ)

2 dγ

−

exp

−

(cid:26)

σ(1)(

2 +

Y ′1|
|
2(1

2)

Y ′2|

|
γ)

σ(2)
2πγ

exp

σ(2)

|

Y3 −
2γ

2

Y ′3|

−

(cid:26)

·

(cid:27)

· s

(cid:27)

−

n
i
˜k(0)

1

4

|

0
Z

=

|

σ(1)

2π(1

γ)

−

σ(2)

2π(1

s

γ)

−

exp

−

(cid:26)

1

γ
−
√γ

−

Y ′1

Y1 −
√γ

H1

Y

Y ′

−
√γ

+

Y ′2

Y2 −
√γ

H2

Y

Y ′

−
√γ

(cid:18)

(cid:19)

(cid:18)

(cid:19)(cid:19)

(cid:27) (cid:20)

2

−
Y ′3|
|
γ)
−

σ(2)
2(1

1

+ γ

2 (1

γ)

−

Y ′1

√1

γ

−

(cid:18)

H1

(cid:18)

Y

Y ′

−
√γ

+

(cid:19)

H2

Y
−
√1

Y ′
γ

(cid:18)

−

(cid:19)(cid:19)(cid:21)

H

·

√1

γ

−

Y ′

√1

γ

(cid:19)

−

(cid:18)

d3Y ′ .

(20)

The integral over Y3 is the usual convolution. Therefore we can look for functions H1, H2
depending only on Y1, Y2, i.e. H1(Y ) = H1(Y1, Y2), H2(Y ) = H2(Y1, Y2). Write down the
equation for H1, H2 which does not contain Y3:

(cid:18)

Y ′2

9

exp

σ(1)
2 |

Y

2

|

−

σ(1)
2π ·

·

o

H(Y ) =

1

dγ

0

Z

ZR2

σ(1)
2πγ ·

exp

σ(1)

Y ′

2

|

|

Y
−
2γ

−

(cid:26)

σ(1)

−

·

2π(1

γ) ·

(cid:27)

exp

σ(1)

−

2(1

γ) · |

−

2

Y ′

|

(1

−

−

γ)3/2

Y1

Y ′1

−
√γ

H1

·

Y

Y ′

−
√γ

+ Y2

Y ′2
√γ H2
−

Y

Y ′

−
√γ

o h

(cid:16)

(cid:17)

(cid:16)

(cid:17)(cid:17)

n

n

1

+ γ

2 (1

γ)

−

Y ′1

√1

γ

−

(cid:18)

Y

Y ′

−
√γ

H1

(cid:18)

(cid:19)

Y ′2

√1

γ

−

H2

(cid:18)

Y

Y ′

−
√γ

H

·

(cid:19)(cid:19)(cid:21)

Y ′

√1

γ

(cid:19)

−

(cid:18)

d2Y ′ .

(21)

(cid:16)

+

Here Y = (Y1, Y2), Y ′ = (Y ′1, Y ′2), H(Y ) = (H1(Y1, Y2), H2(Y1, Y2). This is our main equation
for the ﬁxed point of the renormalization group which we shall analyze in the next section

(see also

7).
§

4. Analysis of the Equation (21)
§

The solutions to the equation (21) have a natural scaling with respect to the parameter
σ = σ(1). Namely, if we solve the equation (21) for σ = 1 and let the corresponding solution
be H(Y ), then the general solution for arbitrary σ is given by the formula

Hσ(Y ) = √σ H(√σY ) .

(22)

This is analogous to the usual scaling of the Gaussian ﬁxed point in probability theory.
Thus, it is enough to consider the equation (21) for σ = 1. We shall show that there exists a
three-parameter family of solutions to the equation (21) for σ = 1. The equation (21) takes a

simpler form if we use expansions over Hermite polynomials. All necessary facts about Her-
mite polynomials are collected in the Appendix 1. For H(Y1, Y2) = (H1(Y1, Y2), H2(Y1, Y2)),
we write

Hj(Y1, Y2) =

h(j)(m1, m2) Hem1(Y1) Hem2(Y2),

j = 1, 2

(23)

where Hem(z) are the Hermite polynomials of degree m with respect to the Gaussian density

m1,m2
X
≥

0

1
√2π

exp

1

2 z2

−

(cid:8)

(cid:9)

. We have (see (42)):

zHem(z) = Hem+1(z) + mHem

1(z) , m > 0

(24)

−

10

and

He0(z) = 1,

zHe0(z) = z = He1(z) .

Also we use the formula (see (43))

Hem1

ZR1

Y

Y ′

−
√γ

(cid:18)

(cid:19)

1
√2π

exp

−

(cid:26)

Y

|

2

Y ′

|

−
2γ

(cid:27)

Hem2

Y ′

√1

γ

(cid:19)

−

(cid:18)

1
√2π

exp

|
2(1

−

(cid:26)

2

Y ′

|
−

γ)

(cid:27)

dY ′ = γ

m1+1
2

(1

m2+1

γ)

2 Hem1+m2(Y )

−

1
√2π

exp

2

|

Y
|
2

(cid:27)

−

(cid:26)

.

(25)

Substituting (23) into (21) and using (24), (25), we come to the system of equations for the
coeﬃcients h(m1, m2) which is equivalent to (21):

h(j)(m1, m2) =

(B1h(1))(m′1, m′2) + (B2h(2))(m′1, m′2)

h(j)(m′′1, m′′2)

J (1)
m′m′′ ·

(cid:8)

+m′′1
=m1
Xm′1
m′2+m′′2 =m2

+J (2)

m′m′′ ·

(cid:8)

where m′ = m′1 + m′2 , m′′ = m′′1 + m′′2 and

h(1)(m′1, m′2) (B1h(j))(m′′1, m′′2) + h(2)(m′1, m′2) (B2h(j))(m′′1, m′′2)

(26)

(cid:9)

(cid:9)

1

−

Z0
1

J (1)
m′m′′

=



m′
2 (1

γ

γ)

m′′+3

2 dγ

−

−

m′+1
2

γ

(1

γ)

m′′+2

2 dγ

J (2)
m′m′′



(B1 h(j)) (m′1, m′2) = h(j) (m′1 −

=

Z0

1, m′2) + (m′1 + 1) h(j)(m′1 + 1, m′2)

(B2 h(j)) (m′1, m′2) = h(j) (m′1, m′2 −

1) + (m′2 + 1) h(j) (m′1, m′2 + 1)

(27)

To simplify the system (26), we shall look for solutions with h(j)(0, 0) = 0, j = 1, 2.
m1,m2 when there is no confusion.

Below we sometimes write h(j)(m1, m2) as h(j)
Similar conventions will be applied to J (j)(m1, m2). For m1 + m2 = 1, we have

m1m2 or h(j)

11

10 = J (1)
h(1)
01 ·
01 = J (1)
h(1)
01 ·
10 = J (1)
h(2)
01 ·
01 = J (1)
h(2)
01 ·

(h(1)

10 + h(2)
01 )

(h(1)

10 + h(2)
01 )

(h(1)

10 + h(2)
01 )

(h(1)

10 + h(2)
01 )

10 + J (2)
h(1)
10 ·
01 + J (2)
h(1)
10 ·
10 + J (2)
h(2)
10 ·
01 + J (2)
h(2)
10 ·

·

·

·

·

(h(1)

10 h(1)

10 + h(2)

10 h(1)
01 )

(h(1)

01 h(1)

10 + h(2)

01 h(1)
01 )

(h(1)

10 h(2)

10 + h(2)

10 h(2)
01 )

(h(1)

01 h(2)

10 + h(2)

01 h(2)
01 )






−

where J (1)

01 =

1/3 and J (2)

10 = 1/6. There are two cases:

Case 1. h(1)

10 + h(2)

01 =

6. In this case (h(1)

10 , h(1)

01 , h(2)

−

10 , h(2)
01 ) only needs to satisfy:
01 h(2)
h(1)

10

(h(1)

10 + 3)2 = 9

−

This is a two parameter family of solutions.

Case 2. h(1)

10 + h(2)
=
01 6
−
10 = h(2)
we have h(1)

01 =

2, h(1)

01 = h(2)

10 = 0.

−

6. In this case (h(1)

10 , h(1)

01 , h(2)

10 , h(2)

01 ) can be uniquely determined and

10 = h(2)
10 = 0. Let us write down the recurrent relations for m1 + m2 = 2, j = 1, 2 :

For the rest of this paper we shall consider only the case 2 for which h(1)
01 = h(2)
h(1)
h(j)
20 =

20 + 4J (1)

02 + 4J (2)

11 )h(j)

(2J (2)

h(2)
11

01 =

2,

−

20 + 2J (1)
11 ·
˙h(j)
01 ·

11 + J (1)

h(j)
10 ·
(2h(1)

20 + h(j)
h(1)
J (1)
10 ·
11 ·
11 ) + J (1)
11 ·

20 + h(2)

11

h(j)
10 ·

(h(1)

11 + 2h(2)
02 )

−



−

(2J (2)

11 )h(j)

h(j)
11 =

20 + 4J (1)

02 + 4J (2)

(2J (2)

h(j)
02 =



h(j)
11 = 0. Solving the recurrent relations for m1 + m2 = 3 gives us:

02 + h(j)
h(2)
01 ·

02 + 2J (1)
11 ·

20 + 4J (1)

02 + 4J (2)

11 )h(j)

h(j)
01 ·

−

It is not diﬃcult to check that the only solution to the above system is h(j)

20 = h(j)

02 =

J (1)
11 ·

h(1)
11

This shows that (h(1)

4,
the recurrent relations for m1 + m2 = p form a linear system of equations for the variables

30 ) can be considered as free parameters. For any p

12 , h(1)

21 , h(1)

≥

30 = 0

h(1)
03 = h(2)
h(1)
12 = h(2)
21 = h(2)
h(1)
30 = h(2)
h(1)

21

03

12





12

p

h(j)
10 only. In principle, they can be
m1,p
{
solved and an explicit expression for the solutions can be found. We emphasize here that if

m1=0 with coeﬃcients depending on h(j)

01 and h(j)

m1}

−

the free parameters take real values then the whole solution is also real.

It is not diﬃcult to check that for any values of (h(1)

m1,m2
4) by using (26). The solution we obtain is formal in the sense that it satisﬁes
. We are now ready to formulate

(m1 + m2 ≥
(26) but hm1,m2 with m1 +m2 = p may not decay as p
the theorem concerning the existence of formal solutions to (26).

30 ), one can ﬁnd all h(j)

12 , h(1)

21 , h(1)

−→ ∞

Theorem 4.1. For any values of (h(1)
the recurrent equation (26).

12 , h(1)

21 , h(1)

30 ), there exists a unique formal solution to

12 , h(1)

21 and h(1)
30 .

Thus, theorem 4.1 claims the existence of a three-parameter family of solutions of (21)
30 are suﬃ-
m1,m2 decay as m1 + m2 = p tends to inﬁnity. Let us say that
introduce the vector h(d) =
4,
d,0)T . The vector h(d) contains all terms of degree d. By

parameterized by h(1)
ciently small, then h(j)
h(j)
m1,m2 has degree d if m1 + m2 = d. For each d
0,d, . . . , h(2)
0,d, h(1)
(h(1)
the recurrent relation (26)

It turns out that if h(1)

21 and h(1)

1, . . . , h(1)

12 , h(1)

d,0, h(2)

≥

1,d

−

C (d) h(d) = b(d)

(28)

where the vector b(d) contains terms of degree

d

1. Also C (d)

R(2d+2)

(2d+2) is a matrix:

×

≤

−

∈

16d

16 + 32k

−

−

(d + 1)(d + 3)(d + 5)

if 1

k = l

d + 1

≤

≤

80d + 80

32k

−

−

(d + 1)(d + 3)(d + 5)

if d + 2

k = l

2d + 2

≤

≤

C (d)

kℓ =

32(d

k + 2)

−
(d + 1)(d + 3)(d + 5)

,

−

if 2

k

d + 1, l = d + k

≤

≤

32(k

d

1)

−
(d + 1)(d + 3)(d + 5)

−

,

−

if d + 2

k

2d + 1, l = k

d

≤

≤

−

all other cases

1

1

0,






,

,

13

It is easy to check that if d

4, then C (d) is nonsingular and as d

, C (d) converges to

the identity matrix. This observation immediately implies the following lemma:

≥

−→ ∞

Lemma 4.2. Let (C (d))−
constant C1 > 0 such that for all d

4

≥

1 be the inverse matrix of C (d) for d

4. There exists an absolute

≥

(C (d))−

1

k

C1 .

k ≤

We are now ready to derive an estimate which gives the decay of solutions of the recurrent

relation (26).

Theorem 4.3. If
C2 > 0, 0 < ρ < 1

h(1)
δ,
12 | ≤
4 , we have

|

h(1)
21 | ≤

|

δ,

h(1)
30 | ≤

|

δ and δ is suﬃciently small, then for some

h(j)
m1,m2

C2

Γ

≤

ρm1+m2
m1+m2+7
2

m1 ≥

0, m2 ≥

∀

0, j = 1, 2.

(cid:0)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
Proof. We begin by noting that h(j)
m1m2 = 0 if m1 + m2 is even. This can be easily proven by
using the recurrent relation (26) and the fact that h(j)
m1,m2 = 0 for m1 + m2 = 2.
Let 0 < ρ1 < 1, ρ1 will be chosen suﬃciently small. We shall use induction on m1 + m2
where m1 + m2 is odd. According to the induction hypothesis

00 = 0 and h(j)

(cid:1)

h(j)
m1,m2| ≤

|

ρm1+m2+2
1
m1+m2+7
Γ
2

g(m1 + m2)

(29)

L is an odd number and L will be chosen later
for every 3
to be suﬃciently large. Also g is a function to be speciﬁed later. We shall comment on the

m1 + m2 ≤

2 where d

−

≤

d

(cid:0)
≥

(cid:1)

choice of L and verify the induction hypothesis for 3
L later. Let us show that
the same inequality holds for m1 + m2 = d. Without any loss of generality, let us consider
j = 1. The case j = 2 is similar. Fix m1 and let b(d)
m1 be the (m1 + 1)th component of the
vector b(d) in the equation (28). We now estimate b(d)
m1 using the induction hypothesis (29)

m1 + m2 ≤

≤

14

and the equation (26):

≤

b(d)
m1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

·

Γ

(cid:0)

Γ

ρm′+3
1
m′+8
2
ρm′+1
(cid:1)
1
m′+6
2
ρm′+2
(cid:0)
1
m′+7
2
ρm′+2
(cid:0)
1
m′+7
2

Γ

Γ

·

·

·

(cid:1)

(cid:1)

·

(cid:0)

Γ

ρm′′+2
1
m′′+7
2
ρm′′+2
(cid:1)
1
m′′+7
2
ρm′′+3
(cid:0)
1
m′′+8
2
ρm′′+1
(cid:0)
1
m′′+6
2

Γ

Γ

·

·

·

(cid:1)

(cid:1)

·

·

·

·

·

2

2

2

d

3

−

J (1)
m′,m′′

2

·

·

Γ

Xm′=2 (cid:12)
(cid:12)
3
d
(cid:12)
−

+

(cid:12)
(cid:12)
(cid:12)
J (1)
m′,m′′

Xm′=4 (cid:12)
(cid:12)
2
d
(cid:12)
−
J (2)
m′,m′′

Xm′=3 (cid:12)
(cid:12)
4
d
(cid:12)
−
J (2)
m′,m′′

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

+

+

Xm′=3 (cid:12)
(cid:12)
(cid:12)
J (1)
2,d
−

2

+ 12

(cid:16) (cid:12)
(cid:12)
(cid:12)

·

(cid:12)
(cid:12)
(cid:12)
+

(cid:12)
(cid:12)
(cid:12)

(m′ + 1)g(m′ + 1)g(m′′)

(m′ + 1)

g(m′

1)g(m′′)

−

·

·

·

(m′ + 1)

(m′′ + 1)

g(m′)g(m′′ + 1)

(m′ + 1)

g(m′)g(m′′

1)

·

−

J (1)
d
−
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:0)
1,1

+

(cid:0)
2,2

+

(cid:1)
J (2)
d
−
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1

(cid:1)
J (2)
1,d
−
(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

ρd
1
d+5
2

·

Γ

(cid:0)

(cid:1)

g(d

2)

−

·

The last term in the rhs of the above inequality comes from the case where hm′1m′2
or hm′′1 m′′2
of degree one since the induction hypothesis holds only for 3
2. Also in the
estimation of the ﬁrst four terms we use the fact that for ﬁxed (m′, m1), there are at most
m′ + 1, m′′ + 1
2 = m2,
min
m′1 + m′2 = m′ and m′′

tuples of (m′, m′′
1 + m′′

2 = m′′. By (27), we have

2) such that m′1 + m′′

1 = m1, m′2 + m′′

m1 + m2 ≤

1, m′2, m′′

−

≤

is

d

{

}

and for some constant C3 > 0

J (1)
m′,m′′

=

Γ

m′+2
2

Γ

(cid:0)
m′+3
2

J (2)
m′,m′′

=

m′′+5
2

Γ
m′+m′′+7
2

(cid:0)

(cid:1)

(cid:1)
m′′+4
2

Γ
m′+m′′+7
2

(cid:1)

(cid:0)

(cid:1)

(cid:1)

(cid:1)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

+

1,1

+

2,2

+

2

J (1)
2,d
−
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

J (1)
d
−
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

C3
d2

1

≤

J (2)
1,d
−
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Γ
(cid:0)

Γ
(cid:0)

(cid:0)

J (2)
d
−
(cid:12)
(cid:12)
(cid:12)

15

Therefore

2ρd+5
1
d+7
Γ
2

≤

d

3

−

Γ

·

Xm′=2
d

m′+2
2
Γ

(cid:0)

b(d)
m1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)
+

+

+

+

2ρd+3
(cid:1)
1
d+7
Γ
2
2ρd+5
(cid:0)
1
d+7
Γ
2
2ρd+3
(cid:0)
1
d+7
Γ
2
ρd+2
(cid:0)
1
d+7
2

Γ

(cid:1)

(cid:1)

(cid:1)

3

−

Γ

·

·

·

·

(cid:0)

(cid:0)

(cid:0)

Xm′=4
2
d
−

Xm′=3
4
d
−

Γ

Γ

Xm′=3
C3
d2 ·

Γ
Γ

ρd+2
(cid:0)
1
d+7
2

≤

Γ

(cid:1)
ρ1 ·

·

C4 ·  

d+7
2
d+5
2
3

−

(cid:0)
d
(cid:0)

Xm′=2

d

2

(cid:0)
−

(cid:1)

Xm′=3

g(m′ + 1)g(m′′)

g(m′

1)g(m′′)

−

(m′′ + 1)

g(m′)g(m′′ + 1)

·

g(m′)g(m′′

1)

−

Γ
Γ

·

m′′+5
2
m′′+7
2

(m′ + 1)

·
m′+8
(cid:1)
2
m′+2
(cid:0)
2
Γ

(cid:1)
(m′ + 1)
·
m′+6
(cid:1)
2
m′+3
(cid:0)
2
Γ

(m′ + 1)

(cid:1)

·
m′+7
(cid:1)
2
m′+3
(cid:0)
2
Γ

·
m′+7
(cid:1)
2
(cid:0)

(m′ + 1)

(cid:1)

(cid:0)
(cid:0)
Γ
·
Γ

Γ

Γ
Γ

·

·

(cid:0)
(cid:0)

(cid:0)

(cid:0)
(cid:0)

·

(cid:1)
m′′+5
(cid:1)
2
m′′+7
2

·

m′′+4
2
Γ

(cid:1)
(cid:1)
·
m′′+8
(cid:1)
2
m′′+4
(cid:0)
2
m′′+6
2

·

(cid:1)

(cid:1)
(cid:1)

12
(cid:1)
ρ2
1 ·

·

g(d

2)

−

(cid:1)
(cid:1)
g(m′ + 1)g(m′′) +

d

3

−

Xm′=4

−

!

g(m′

1)g(m′′)

−

ρd+2
1
d+7
2

Γ

(cid:0)

(cid:1)

C5 ·

·

2)

g(d
d

−
ρ1
·

g(m′)g(m′′ + 1) +

g(m′)g(m′′

1)

+

where C4, C5 are some constants. Now we specify the choice of the function g. Let g(m) be
such that g1 = α and

d

4

−

Xm′=3

m

1

−

p=1
X

d

Xm′=1

·

(cid:18)

ρd+2
1
d+7
2

(cid:0)

(cid:1)

g(m) =

g(p)g(m

p)

for m > 1

−

By the method of formal power series it is not diﬃcult to show that

g(m) =

1
2 ·

(2m

1)!!

−
m!

(2α)m

·

Clearly, we have const

const, and this immediately gives us

g(m+1)
g(m) ≤

≤

b(d)
m1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

ρd+2
1
d+7
2
ρd+2
(cid:0)
1
d+7
2

≤

Γ

≤

Γ

(cid:1)

·

C6 ·

·

g(m′)g(d

m′) +

−

g(d)

C6ρ1 +

C6
ρ1 (cid:19)

·

d

ρd+2
1
d+7
2

Γ

(cid:0)

(cid:1)

C6
ρ1

·

·

d

g(d)

where C6 > 0 is some constant. Now by Lemma 4.2, we obtain that

(cid:1)

(cid:0)

hm1m2| ≤

|

Γ

g(d)

C1 ·

·

C6ρ1 +

(cid:18)

C6
ρ1 (cid:19)

·

d

16

Choose ρ1 so small that C1C6ρ1 < 1
This clearly implies

2 and ρ1 ·

4α < 1

4 . Then take L so large that C1C6

ρ1L < 1
2.

hm1,m2| ≤

|

ρd+2
1
d+7
2

Γ

g(d)

12 , h(1)
We now justify the induction hypothesis (29). Recall that our free parameters are h(1)
(cid:0)
(cid:1)
21
30 . It is easy to check that if we set h(1)
21 = h(1)
12 = h(1)
and h(1)
30 = 0, then hm1m2 = 0 for any
h(1)
h(1)
< δ with
2. Since L is ﬁxed, and 0 <
m1 + m2 ≥
21 |
12 |
suﬃciently small δ, then the induction hypothesis is satisﬁed. A simple estimate on g gives

< δ, 0 <

< δ, 0 <

h(1)
30 |

|

|

|

that

Thus the theorem is proven if one takes ρ = 4αρ1.

g(m)

(4α)m

≤

As it is stated our solutions of (20) are determined by ﬁve parameters σ(1), h(1)

12 , h(1)

21 , h(1)

30 , σ(2).

However, it turns out that these parameters are not independent and σ1 can be expressed
through ( h(1)
30 ,σ(2) (Y ) be the solution of (20). Then
G(σ(1), h(1)

30 ). Namely, let Gσ(1),h(1)
21 , h(1)
30 , σ(2)) (Y ) = G(1, σ(1)(h(1)

12 , h(1)
21 , h(1)

1) + 1, σ(2))(Y ) .

1) + 1 , σ(1)h(1)

21 , σ(1)(h(1)

12 , h(1)

21 ,h(1)

12 ,h(1)

30 −

12 −

This equality is proved at the end of

existence of solutions of (21).

6. We formulate now the ﬁnal result concerning the
§

12 , h(1)
Theorem 4.2. Let σ(1) > 0, σ(2) > 0 and h(1)
exists a solution of (20) which has the following form

21 , h(1)

30 be suﬃciently small. Then there

G(σ(1), h(1)

12 , h(1)

21 , h(1)

30 , σ(2)) (Y1, Y2, Y3) = exp

σ(1)
2

−

(cid:26)

(cid:0)

2 +

Y1|

|

2

Y2|

|

·

(cid:27)
(cid:1)

√σ(1) H (h(1)

12 ,h(1)

21 ,h(1)

30 ) (√σ(1) Y1, √σ(1) Y2) .

σ(1)
2π ·
·
12 ,h(1)

σ(2)
2 |

2

Y3|

exp

−

(cid:26)

(cid:27) r

σ(2)
2π ·

Here H (h(1)

21 ,h(1)

30 ) is the solution of (21) with the given h(1)

12 , h(1)

21 , h(1)
30 .

As it was already mentioned the parameters σ1, h(1)

30 , σ2 are not independent
and actually the set of solutions depends on four independent parameters (see Lemma 6.2).

12 , h(1)

21 , h(1)

From the estimate in Theorem 4.3 and from known asymptotic formulas for the Hermite
30 ) converges for every Y = (Y1, Y2).

21 ,h(1)

12 ,h(1)

polynomials it follows that the series giving H (h(1)
Better estimates are also easily available.

17

5. The Linearization Near Fixed Point
§

Denote h(1)

12 = x(1), h(1)

21 = x(2), h(1)

30 = x(3). Our ﬁxed points have the following form

σ(1)
2π

exp

1 + Y 2
σ(1)(Y 2
2 )
2

−

(cid:26)

·

(cid:27)

G(σ(1),x(1),x(2),x(3),σ(2)) =

σ(2)
2π

· r

exp

−

(cid:26)

σ(2)Y 2
3
2

(cid:27) (cid:16)

H (σ(1), x(1), x(2), x(3))

(Y1, Y2), H (σ(1), x(1), x(2), x(3))

1

2

(Y1, Y2), 0

(30)

(cid:17)

Recall that H (σ(1), x(1), x(2), x(3)) = √σ(1)H (1, x(1), x(2), x(3))(√σ(1)Y1, √σ(1)Y2) and H (1, x(1), x(2), x(3))

are described in

4.
§

The strategy of the proof of the main result is based on the method of renormalization
group. At the p-th step of our procedure, we consider an interval on the time axis S(p) =
S(p)
, S+]
−
h
is an interval of positive length. We want to ﬁnd conditions under which ˜gr(Y, s), s
have a representation

S(p). From our estimates it will follow that

such that S(p+1)

S(p) = [S

, S(p)
+

S(p),

p
T

⊆

∈

i

−

˜gr(Y, s) = Λr

1r

−

exp

H (σ(1),x(1),x(2),x(3))

(Y ) + δ(r)

1

σ(1)
2π

1 + Y 2
σ(1)(Y 2
2 )
2

−

σ(2)
2π

s

exp

(cid:26)

(cid:27)
1 (Y, s), H (σ(1),x(1),x(2),x(3))

2

(cid:26)
(Y ) + δ(r)

σ(2)Y 2
3
2

−

·

(cid:27)

2 (Y, s), δ(r)

3 (Y, s)

2 , δ(r)

(cid:16)
where δ(r)
1 , δ(r)
. The renormalization is based on the crucial
observation (see above) that for large p, the sum over p1 is a Riemannian integral sum for
an integral over γ changing from 0 to 1. Let us write

tend to zero as r

→ ∞

(cid:17)

3

˜gr(Y, s) Λ−

r+1(r−

1 exp

σ(1)(Y 2
1 + Y 2
2 )
2

+

σ(2)Y 2
3
2

2π
σ(1)

(cid:26)

(cid:19) (cid:18)
= H (σ(1),x(1),x(2),x(3)) (Y1, Y2) + δ(r)(γ, Y, s)

(cid:27) (cid:18)

1
2

=

2π
σ(2)

(cid:19)

(31)

where δ(r)(γ, Y, s) =

δ(r)
j (γ, Y, s), 1

≤
as a small perturbation of our ﬁxed point (30).
consider the set of functions
Recall that the third component of H (σ(1),x(1),x(2),x(3)) is zero because of incompressibility and
δ(p)
3

can be found from the incompressibility condition. Clearly,

≤
δ(p)(γ, Y, s)

n

o

{

}

j

3

= δ(p)(γ, Y, s), γ =

It is natural to

r
p

.

δ(p+1)(γ, Y, s) = δ(p)

γ, Y, s

, γ

p
p + 1

.

≤

(cid:19)

p + 1
p

(cid:18)

18

The formula for δ(p+1)(1, Y, s) follows from (21):

exp

σ(1)
2

−

2 +

(

Y1|

|

2)

Y2|

|

−

σ(2)
2 |

2

Y3|

·

(cid:27)

σ(1)
2π r

σ(2)
2π ·

δ(p+1)
j

(1, Y, s)

(cid:26)
1

=

dγ

Z0

ZR3

σ(1)
2πγ · s

σ(1)
2πγ ·

σ(2)

−

2π(1

γ) · s

2π(1

γ) ·

σ(2)

−

exp

−

(cid:26)

σ(1)(

Y1 −

|

2 +
Y ′1|
2πγ

Y2 −

|

Y ′2|

2)

σ(2)

|

Y3 −
2πγ

2

Y ′3|

−

σ(1)(

2 +

Y ′1|
|
2π(1

2)

Y ′2|
|
γ)

−

−

−

σ(2)
|
2π(1

2

Y ′3|
γ)
−

−

3
2

γ)

(1

−

−

(cid:27) (cid:26)(cid:20)

(cid:18)

Y ′1

Y1 −
√γ

H1

Y

Y ′

−
√γ

+

Y ′2

Y2 −
√γ

H2

Y

Y ′

−
√γ

(cid:19)

(cid:18)

(cid:19)(cid:19)

Y ′1

γ

−
Y1 −
√γ

1

+ γ

2 (1

γ)

−

√1

(cid:18)

Y

Y ′

−
√γ

H1

(cid:18)

+

(cid:19)

√1

γ

−

H2

(cid:18)

Y

Y ′

−
√γ

δ(p+1)
j

γ,

1
(cid:18)

−

Y ′

√1

−

, s

γ

(cid:19)

(cid:19)(cid:21)

+

1
(cid:18)

−

(cid:20)

−

3
2

γ)

(cid:18)

Y ′1

δ(p+1)
1

Y

γ,

Y ′

−
√γ

(cid:18)

, s

+

(cid:19)

Y ′2

Y2 −
√γ

δ(p+1)
2

Y

γ,

Y ′

, s

−
√γ

(cid:18)

(cid:19)(cid:19)

1

+ γ

2 (1

γ)

−

Y ′1

δ(p+1)
1

√1

γ

−

(cid:18)

Y

γ,

Y ′

−
√γ

(cid:18)

, s

+

(cid:19)

Y ′2

√1

γ

−

δ(p+1)
2

Y

γ,

Y ′

, s

−
√γ

(cid:18)

(cid:19)(cid:19)(cid:19)(cid:21)

Hj

Y ′

√1

, s

γ

d3Y ′,

j = 1, 2

−

(cid:18)
We did not include in the last expression terms which are quadratic in δ because in this

(cid:19)(cid:27)

(32)

section we consider only the linearized map.

Another way to introduce the semi-group of linearized maps is the following. Take θ > 0
j, j = 0, 1, 2, . . . Our semigroup will act

which later will tend to zero. Denote γj = (1 + θ)−
on the space ∆ of functions δ(γ, Y ) with values in C 3 such that

1. for each γ, 0

γ

1, the function δ(γ, Y ) belongs to the Hilbert space L2 = L2 (R3)

of square-integrable functions with respect to the weight
(Y1, Y2, Y3);

3

2 exp

σ(1)
2π

(cid:16)

(cid:17)

σ(1)Y 2
2

}

{−

, Y =

≤

≤

(cid:18)

Y ′2

19

2. As a function of γ it is a continuous curve in this Hilbert space and max
≤

≤

γ

0

1 k

δ(γ, Y )

kL2 <

.

∞

Deﬁne the linearized map Lθ corresponding to θ as follows:

1. for γj+1 ≤

γ

≤

γj, j = 1, 2, . . .

Lθ(δ(γ, Y )) = δ(γ(1 + θ), Y );

1

2. for

1 + θ ≤

≤

γ

1 the function Lθ(δ(γ, Y )) is given by the formula

Lθ(δ(γ, Y )) = δp1(1, Y, s)

where p1 is found from the relation

= γ.

p1
p

In other words at γ = 1 we use (32) to ﬁnd the new δ(p+1)(1, Y, s). After that we apply

1.

It is easy to see that there exist the limits lim

Ln

θ = At and the operators At constitute

a semi-group. For γ < 1, t > 0 such that γet < 1

θ
nθ

0
t

→
→

Atδ(γ, Y ) = δ(γet, Y )

Let

be the inﬁnitesimal generator of the semi-group At. In
.

the spectrum and eigenfunctions of

A

6 we study in more detail
§

A

Lemma 5.1. The eigenfunctions of the group At have the form

where ˜Φα is a function with values in C 3 satisfying (32).

δ(γ, Y ) = γα ˜Φα(Y )

In more detail, if we take δ(γ, Y ) = γα ˜Φα(Y ) and substitute it into the rhs of (32) we get

in the lhs δ(p+1)(Y ) = ˜Φα(Y ).

20

Proof. If δ(γ, Y ) is an eigenfunction then from the formula for At

Atδ(γ, Y ) = δ(γe−

t, Y ) = e−

αtδ(γ, Y )

Let γ

1. Then

→

Lemma is proven.

δ(e−

t, Y ) = e−

αtΦ(Y ) = γαΦ(Y )

The space ∆ is spanned by the eigenfunctions of

in the sense that for any h

∆

At

{

}

∈

we have the expansion

h(γ, Y ) =

C (α)γαΦα(Y )

α

spec
X
∈

A

The coeﬃcients C (α) are found with the help of the eigenfunctions of the conjugate system
. The form of the conjugate semi-group and its eigenfunctions can be investigated

(A∗)t
{
using the described above discrete approximation. We do not dwell more on this.

}

6. The Spectrum of the Group of Linearized Maps
§

In this section we show that the solutions of (21) studied in

4 have l(u) = 4 unsta-
§
ble eigenvalues and l(n) = 6 neutral eigenvalues. Therefore in the renormalization group
approach we consider 10– parameter families of initial conditions (see below).

As was already mentioned, in the limit p

the linearized maps generate a semi-
R3, j = 1, 2
group of operators acting in the space ∆ of functions f (j)(γ, Y ), 0
which are continuous as functions of γ in the Hilbert space L2. At γ = 1, the functions
f (j)(γ, Y ) satisfy the boundary condition which follows from (32):

−→ ∞

1, Y

≤

≤

∈

γ

exp

σ(1)
2

−

2 +

(

Y1|

|

2)

Y2|

|

−

σ(2)
2 |

2

Y3|

·

(cid:27)

σ(1)
2π r

σ(2)
2π ·

f (j)(1, Y )

(cid:26)
1

=

dγ

Z0

ZR3

σ(1)
2πγ · s

σ(2)
2πγ ·

σ(1)

−

2π(1

γ) · s

2π(1

γ) ·

σ(2)

−

21

exp

−

(cid:26)

σ(1)(

Y1 −

|

2 +
Y ′1|
2πγ

Y2 −

|

Y ′2|

2)

σ(2)

|

Y3 −
2πγ

2

Y ′3|

−

σ(1)(

2 +

Y ′1|
|
2π(1

2)

Y ′2|
|
γ)

−

−

−

σ(2)
|
2π(1

2

Y ′3|
γ)
−

−

3
2

γ)

(1

−

−

(cid:27) (cid:26)(cid:20)

(cid:18)

Y ′1

Y1 −
√γ

H1

Y

Y ′

−
√γ

+

Y ′2

Y2 −
√γ

H2

Y

Y ′

−
√γ

(cid:19)

(cid:18)

1

+ γ

2 (1

γ)

−

√1

(cid:18)

+

1
(cid:18)

−

(cid:20)

−

3
2

γ)

Y ′1

H1

Y ′1

γ

−
Y1 −
√γ

(cid:18)

Y ′1

Y

Y ′

−
√γ

+

(cid:19)

(cid:18)

√1

γ

Y

Y ′

−
√γ

H2

(cid:18)

f (j)

γ,

−

1
(cid:18)

(cid:19)(cid:21)

f (1)

γ,

(cid:18)

Y

Y ′

−
√γ

Y ′2

Y2 −
√γ

f (2)

γ,

(cid:18)

Y

Y ′

−
√γ

(cid:19)(cid:19)

1

+ γ

2 (1

γ)

−

√1

γ

−

(cid:18)

f (1)

γ,

(cid:18)

Y

Y ′

−
√γ

(cid:19)

Y ′2

√1

γ

−

f (2)

γ,

(cid:18)

Y

Y ′

−
√γ

(cid:19)(cid:19)(cid:19)(cid:21)

(cid:18)

Y ′2

−

+

(cid:19)

+

(cid:19)(cid:19)

Y ′

√1

γ

(cid:19)

−

(33)

Y ′

Hj

√1

γ

−

(cid:19)(cid:27)

(cid:18)

d3Y ′,

j = 1, 2

Denote by

Rp the linear operator which transforms

s is a parameter which plays no role in this section. As it was explained in
there exists the limit lim

}
5, for each t
§
p = At so that the operators At constitute a semi-group having
tp
−→∞ R
A

δ(γ, Y, s) = γ ∂δ(γ,Y,s)

an inﬁnitesimal generator

, 0 < γ < 1 and

. In our case

At
−
t

. Here

δ(p+1)(γ, Y, s)

δ(p)(γ, Y, s)

into

A

for γ = 1 the function δ(1, Y, s) satisﬁes the boundary condition (33) in which f (γ, Y ) =
δ(p+1)(1, Y, s).

= lim
0
t
↓

∂γ

}

{

{

p

I

A

If α is an eigenvalue of

, then the corresponding eigenfunction has the form γαΦα,σ(1),σ(2)(Y )
(see Lemma 5.1), where Φα,σ(1),σ(2)(Y ) satisﬁes the equation (33) with f (γ, Y ) = γαΦα,σ(1),σ(2)(Y ).
(α) = 0) then the corresponding eigenvalue is called unstable (neutral). All
If
other eigenvalues are called stable. The subspaces generated by unstable, neutral, stable
eigenvalues are denoted by Γ(u), Γ(n), Γ(s) respectively.

(α) > 0 (

ℜ

ℜ

As before, for Φ(j)

α,σ(1),σ(2)(Y ) the following scaling relation with respect to σ(1), σ(2) is valid:

Φ(j)

α,σ(1),σ(2)(Y )

∝

Φ(j)

α,1,1 (√σ(1)Y1, √σ(1)Y2, √σ(2)Y3)

Therefore it is enough to consider the above equation (33) for σ(1) = σ(2) = 1. We again use
the expansion over Hermite polynomials:

22

Φ(j)

α,1,1(Y ) = Φ(j)

α (Y ) =

f (j)
α (m1, m2, m3) Hem1(Y1) Hem2(Y2)Hem3(Y3)

m1, m2, m3
X

Here j takes values 1, 2, 3. Since in m3 it is the usual convolution and H does not depend
on Y3, it is enough to look for solutions of (33) having the form fm1,m2δm3. Put β = α + m3
2
and f (j)

α (m1, m2)δm3. We come to the linear system of recurrent relations

β (m1, m2) = f (j)

f (j)
β (m1, m2) =

J (1)
m′,m′′+2β

(B1h(1)) (m′1, m′2) + (B2h(2)) (m′1, m′2)

f (j)
β (m′′1, m′′2)

+ m′′1
= m1
Xm′1
m′2 + m′′2 = m2

(cid:0)

(cid:1)

h(1)(m′1, m′2)

(B1f (j)

β )(m′′1, m′′2)

h(2)(m′1, m′2)

(B2f (j)

β )(m′′1, m′′2)

+ J (2)

m′,m′′+2β ·

+ J (2)

m′,m′′+2β ·

+ J (1)

m′+2β,m′′ ·

+ J (2)

m′+2β,m′′ ·

+ J (2)

m′+2β,m′′ ·

·

·

·

·

(B1f (1)

β )(m′1, m′2) + (B2f (2)

β )(m′1, m′2)

h(j)(m′′1, m′′2)

(34)

(cid:16)
f (1)
β (m′1, m′2)

(B1h(j))(m′′1, m′′2)

(cid:17)

f (2)
β (m′1, m′2)

(B2h(j))(m′′1, m′′2)

Introduce the vector

(cid:16)

The vector f (d)
β

f (d)
β =

f (1)
β

(0, d), f (1)
β

(1, d

1) , . . . , f (1)
β

(d, 0) f (2)
β

(0, d), f (2)
β

(1, d

1) , . . . , f (2)
β

(d, 0)

−

−

T

(cid:17)

contains all terms of degree d. The recurrent relation (34) can be written as

where the vector b(d)
Let C (d)

β (k, ℓ) be its (k, ℓ)-entry. Then

β contains terms of degree

d

≤

−

1. Also C (d)

R2(d+1)

2(d+1) is a matrix.

×

β ∈

β f (d)
C (d)

β = b(d)

β

23

,

,

,

,

∗

1

1

−

0,

ℜ






16d + 32β

16 + 32k

−
(d + 2β + 1)(d + 2β + 3)(d + 2β + 5)

−

if 1

k = ℓ

d + 1

≤

≤

80d + 160β + 80

32k

−

−

(d + 2β + 1)(d + 2β + 3)(d + 2β + 5)

if d + 2

k = ℓ

2d + 2

≤

≤

C (d)

β (k, ℓ) =

32(d + 2β

k + 2)

−

−

(d + 2β + 1)(d + 2β + 3)(d + 2β + 5)

≤

≤

if 2

k

d + 1, ℓ = d + k

32(k

d

2β

1)

−
(d + 2β + 1)(d + 2β + 3)(d + 2β + 5)

−

−

if d + 2

k

2d + 1, ℓ = k

d

≤

≤

−

all other cases

Note that d + 2

(β) >

1.

−

Lemma 6.1. Assume
d
for all d

≥
, the matrix C (d)
β

(β)

ℜ

≥

∗

is invertible.

0. There exists an integer d

> 0, independent of β, such that

Proof. As d tends to inﬁnity, C (d)
β
estimate on the diagonal and oﬀ-diagonal entries shows that for all β such that
and suﬃciently large d, the matrix C (d)
existence of the needed d

0
β becomes diagonally dominant. This implies the

tends to the identity matrix if

and its independence of β.

0. A simple

(β)

(β)

≥

≥

ℜ

ℜ

∗

A similar statement holds if we assume that

(β)

A for any given A

0. We

ℜ

≥ −

≤

formulate it as the following lemma.

Lemma 6.1′. For any A
such that for all d

d

≥

0, there exists an integer d

(A) > 0 which depends only on A,

(A) and all β with

(β)

ℜ

≥ −

≥

∗

∗

A, the matrix C (d)
β

is invertible.

By Lemma 6.1, to ﬁnd all eigen-values of

it amounts to solve the equation det(C (d)

β ) = 0.

The eigenvalue α is then found from the relation β = α + m3

2 . Let

a1 =

1

−

(cid:18)

16
(d + 2β + 3)(d + 2β + 5)

32
(d + 2β + 1)(d + 2β + 3)(d + 2β + 5)

(cid:19)

Then a1 is the eigen-value of the matrix ˜C (d)

R2(d+1)

2(d+1) given by:

×

A

(cid:19)(cid:30) (cid:18)

∈

24

k

1,

−

if 1

k = ℓ

d + 1

≤

≤

2d + 2

k,

if d + 2

k = ℓ

2d + 2

≤

≤

˜C (d)(k, ℓ) =

d + 2

k,

if 2

k

d + 1, ℓ = d + k

≤

≤

−

−

k

d

1

−

−

if d + 2

k

2d + 1, ℓ = k

d

≤

≤

−

0,

all other cases






It is not diﬃcult to ﬁnd that the eigen-values of ˜C (d) are 0 and d+1 with algebraic multiplicity
d + 2 and d respectively. Solve the equations a1 = 0 or a1 = d + 1 and use the condition
d + 2

1. The possible values of β are then given by

(β) >

ℜ

−

β =

3

d

−
2

or

√17

4

d

,

−

−
2

d = 1, 2, 3,

· · ·

This fact immediately gives the following lemma.

β )−

1 be the inverse matrix of ˜C (d)
2β
β
2β is an integer. Then there exists an absolute constant C2 > 0 such that for

d∗(β), where d∗(β) = 3

for d

−

≥

Lemma 6.2. Let ( ˜C (d)
or √17
4
−
−
d∗(β)
all d

≥

(34).

( ˜C (d)

β )−

1

k

k ≤

C2 .

We now state our theorem about the properties of the solutions to the recurrent relation

Theorem 6.3. The only possible values of β for which (34) have nonzero solutions f (j)
is given by:

β (m1, m2)

β =

3

m

−
2

or

√17

4
−
2

m

−

, m = 1, 2, 3,

The corresponding solutions f (j)

β (m1, m2) have the following property:

a) β = (√17

4

−
d = m, we have

−

m)/2. In this case f (j)

β (m1, m2) = 0 for any 0

m1 + m2 < m. For

f (1)
β (r, d

r) =

(d

−

−

−

r + 1)f (2)

β (r

−

−

1, d

r + 1),

r = 1, 2,

, d

· · ·

· · ·

≤

25

β (d, 0) are free parameters. f (j)

f (1)
β (0, d), f (2)
determined if the values of the m + 2 free parameters f (1)
and f (2)

β (m1, m2) for m1 + m2 ≥
−

β (m, 0) are speciﬁed.

β (r, m

m+ 1 are uniquely
, m,
r), r = 0, 1,

· · ·

b) β = (3

−

we have f (1)

m)/2. In this case f (j)
β (0, d) = f (2)

β (d, 0) = 0, and

β (m1, m2) = 0 for any 0

m1 + m2 < m. For d = m,

≤

f (1)
β (r, d

r) = f (2)

β (r

are free parameters. f (j)
values of the m free parameters f (1)

−

−

−
β (m1, m2) for m1 + m2 ≥
β (r, m

1, d

r + 1),

r = 1,

, d

· · ·

m + 1 are uniquely determined if the

r), r = 1,

, m are speciﬁed.

−

· · ·

In both case a) and b), the solutions f (j)
Since f (j)

β depends linearly on the free parameters, we have for some C3 > 0, 0 < ρ < 1

4000

β (m1, m2) is zero for m1 + m2 = m + 1, m + 3, . . ..

f (j)
β (m1, m2)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

C3

≤

Γ

ρm1+m2+2β
m1+m2+2β+3
2

(cid:0)

(cid:1)

,

m1 ≥

0, m2 ≥

∀

0, j = 1, 2.

Proof. Property a) and b) are straightforward computations. From recurrent relation
(34), by parity it is obvious that f (j)
β (m1, m2) = 0 for m1 + m2 = m + 1. An easy induction
shows that f (j)
β (m1, m2) = 0 for m1 +m2 = m+3, m+5, . . . We now prove the decay estimate.
The strategy of the proof is the same as in theorem 4.3. From the proof of theorem 4.3, it
is clear that by choosing the parameters (x(1), x(2), x(3)) suﬃciently small, we have

ρm1+m2+2
m1+m2+7
2

≤

Γ

h(j)
m1,m2

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Our induction hypothesis for f (j)

(cid:0)

(cid:1)

β (m1, m2) is

m1 ≥

0, m2 ≥

∀

0, m1 + m2 ≥

3, j = 1, 2.

ρm1+m2+2β
m1+m2+2β+3
2

≤

Γ

f (j)
β (m1, m2)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

m

∀

≤

m1 + m2 < d, j = 1, 2.

β (m1, m2) = 0 for m1 + m2 =
where d
m + 1, m + 3, . . .). We assume that L is a suﬃciently large number and will verify the

m is an even number (note that f (j)

L and d

−

≥

(cid:0)

(cid:1)

26

induction assumption for m

d

L later. Now for m1 + m2 = d, by lemma 6.2, we have

f (j)
β (m1, m2)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

C2 ·

≤

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

+C2 ·

≤

≤

(m′ + 1)

(m′ + 1)

d

m

−

Xm′=2
m
d
−

Xm′=4
J (1)
2,d
−
(cid:12)
(cid:12)
d
(cid:12)

m+1

−

2+2β

4

·

(cid:12)
(cid:12)
(cid:12)
(m′ + 1)

Xm′=3
1
m
d
−
−
(m′ + 1)

1+2β

4

·

·

·

·

·

·

(cid:12)
(cid:12)
(cid:12)

J (1)
m′,m′′+2β
(cid:12)
(cid:12)
(cid:12)
J (1)
m′,m′′+2β
(cid:12)
(cid:12)
(cid:12)
·

(cid:12)
(cid:12)
ρd
(cid:12)
−
d+2β+1
2

2+2β

Γ

·

·

(cid:0)
(cid:1)
J (2)
m′,m′′+2β
(cid:12)
(cid:12)
(cid:12)
J (2)
m′,m′′+2β
(cid:12)
(cid:12)
(cid:12)
Γ

ρd
−
d+2β+2
2

1+2β

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Xm′=3
J (2)
1,d
−
(cid:12)
(cid:12)
(cid:12)

−

d

3

1

Xm′=m
−
3
d
−

Xm′=m+1
J (1)
d
−
(cid:12)
(cid:12)
(cid:12)

−

d

2

Xm′=m
4
d
−

Xm′=m
J (2)
d
−
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

4

·

·

Γ

1+2β,1

(cid:12)
(cid:12)
(cid:12)
(m′′ + 1)

2+2β

(cid:12)
(cid:12)
ρd
(cid:12)
−
d+2β+1
2

(cid:1)
(cid:0)
J (2)
m′+2β,m′′

·

·

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
Γ

(m′′ + 1)

J (2)
m′+2β,m′′

2+2β,2

4

·

·

2+2β

ρd
−
d+2β+1
2

(cid:0)

(cid:1)

·

·

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

2

2

·

·

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

·

·

(m′′ + 1)

J (1)
m′+2β,m′′

2

2(m′ + 1)

ρm′+3
m′+8
2

·

Γ

ρm′′+2β
m′′+2β+3
2

·

Γ

2

·

Γ

ρm′+1
m′+6
2

(cid:0)

·

Γ

ρm′′+2β
(cid:1)
(cid:0)
m′′+2β+3
2

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

ρm′+2
m′+7
2

·

Γ

·

(cid:1)

ρm′+2
(cid:0)
m′+7
2

·

Γ

2

2

·

·

(m′′ + 1)

ρm′′+2β+1
m′′+2β+4
2

·

Γ

ρm′′+2β

−

1

(cid:0)

(cid:1)

·

Γ

m′′+2β+2
2

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ρm′+2β

−

1

(cid:0)

·

Γ

m′+2β+3
2

·

Γ

(cid:1)
ρm′′+2
m′′+7
2

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ρm′+2β
m′+2β+3
2

ρm′′+3
m′′+8
2

·

Γ

·

·

Γ

(m′′ + 1)

ρm′+2β
(cid:0)
m′+2β+3
2

·

Γ

(cid:0)

(cid:1)

(cid:1)

·

Γ

(cid:1)

ρm′′+1
(cid:0)
m′′+6
2

(cid:0)

(cid:1)

(m′′ + 1)

(cid:0)

(cid:1)

J (1)
m′+2β,m′′

2

(m′ + 1)

·

ρm′+2β+1
m′+2β+4
2

·

Γ

ρm′′+2
m′′+7
2

·

Γ

27

C2 ·

≤

Γ

ρd+2β
d+2β+3
2

d

m

−

·  

Xm′=2

8ρ3
d + 2β + 5

+

8ρ
d + 2β + 5

+

8

ρ2

(d + 2β + 5)

·

8ρ3
(cid:1)
d + 2β + 5 ·

2(m′′ + 1)
d + 2β + 3

+

8ρ
d + 2β + 5

+

8
ρ(d + 2β + 5)

d

m

−

Xm′=4
1
m
−
−

d

8ρ3
d + 2β + 5 ·

2(m′ + 1)
d + 2β + 3

+ +

8ρ
d + 2β + 5

+

16
ρ2(d + 2β + 5)

Xm′=3

d

3

−

Xm′=m+1

d

m+1
(cid:0)

−

Xm′=3
3
d
−

1

Xm′=m
−
2
d
−

+

+

+

8ρ
d + 2β + 5

+

16
ρ2(d + 2β + 5)!

16ρ3
d + 2β + 5

+

d

4

−

Xm′=m
C2 ·

Γ

≤

ρd+2β
d+2β+3
2

Xm′=m
2000ρ

·

(cid:1)

(cid:0)

ρd+2β
d+2β+3
2

≤

Γ

(cid:0)

(cid:1)

where in the second last inequality above we have used the fact that d

L and L is suﬃciently

2. It is clear that it suﬃces for us to take L = 2m. To check
large such that d/(d + 2β + 3)
≤
β (m1, m2) depends linearly on
the inductive assumption for m
several free parameters. If we let them be suﬃciently small, then it is clear that the inductive

2m, we recall that f (j)

≤

≤

d

≥

assumption is satisﬁed for m

d

2m. Our theorem is proved.

≤

≤

We now formulate our main theorem about the spectrum of the linearized operator.

Theorem 6.4. The spectrum of the operator

consists of the following eigen-values

A

where λ(1)

m =

m

2 , λ(2)

m = √17
4
−
2

−

, m

1.

−

spec (

) =

1,

, 0, λ(1)

m , λ(2)

m , m

≥

1

.

(cid:27)

A
m

1
2

(cid:26)

≥

The ﬁrst eigen-values have multiplicities ν1 = 1, ν 1

= 3, ν0 = 6. The eigen-values λ(1)
m ,
λ(2)
m correspond to the stable part of the spectrum and also have ﬁnite multiplicities given by:
νλ(1)

= (m+3)(m+4)
2

= m(m+5)
2

νλ(2)

.

,

2

m

m

For each α

spec (

) , the eigenfunctions f (j)

α (m1, m2, m3) have the following property:

∈

A

a) f (j)

α (m1, m2, m3) is compactly supported in the m3 variable, i.e., there exists an integer
m∗3 = m∗3(α) such that

f (j)
α (m1, m2, m3) = 0

if m3 > m∗3

28

b) f (j)

α (m1, m2, m3) decays faster than exponentially, more precisely, there exist constants
C3 = C3(α) > 0 and 0 < ρ < 1

4000 , such that

f (j)
α (m1, m2, m3)

C3

Γ

≤

ρm1+m2+m3+2α
m1+m2+m3+2α+3
2

,

m1, m2, m3 ≥

0

∀

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:0)

(cid:1)

The system of eigenfunctions is complete in the following sense. Let Γ(s) be the stable
(λ) < 0, Γ(u) be the unstable sub-
linear subspace of ∆ generated by all eigenfunctions with
space generated by all eigenfunctions with eigenvalues λ > 0, and Γ(n) be the neutral subspace
generated by all eigenfunctions with eigenvalue λ = 0. Then dim Γ(u) = 4, dim Γ(n) = 6 and

ℜ

∆ = Γ(u) + Γ(n) + Γ(s) .

Proof. By Lemma 6.1, we only need to examine β for which det(C (d)

d

3

2 or √17

−

−
2

4

d

−

β ) = 0. From
. We discuss the spectrum

previous arguments, we have that for d
separately in the following three cases.

≥

1, β =

−

1◦ unstable spectrum:

α = 1, 1/2.

a) α = 1. Since β = α + m3

The eigenspace is one-dimensional with f (1)
100 = 0, f (1)
is a free parameter and the remaining part of all higher degree terms ( f (j)
m1 + m2 ≥

2 , the only possibility is that β = 1, d = 1 and m3 = 0.
100 = f (2)
000 = f (1)
010
m1,m2,0 with

2 ) is uniquely determined once we specify f (1)
100.

000 = f (2)

010 = f (2)

m1,m2,0 = 0 for m1 + m2 ≤

b) α = 1/2. Possible cases are m3 = 0, β = 1/2, d = 0, 2 or m3 = 1, β = 1, d = 1. In the
ﬁrst case we have f (j)
110 are two free
parameters, all other terms of higher degee ( f (j)
3 ) are uniquely
determined once we specify the above four parameters. In the second case we have
f (1)
001 = f (2)
011 is a free parameter and the remaining part of
all higher degree terms ( f (j)
2 ) is uniquely determined once we
specify f (1)
101. Putting two cases together, we see that the dimension of the eigenspace
is 3.

1, f (1)
m1,m2,0 with m1 + m2 ≥

m1,m2,1 with m1 + m2 ≥

101 = 0, f (1)

200 = f (2)

110 = f (2)

011 = f (2)

001 = f (1)

101 = f (2)

020, f (1)

29

This gives dim Γ(u) = 4.

2◦ neutral spectrum:

Here we have α = 0, and three cases.

a) m3 = 2. Then β = 1. The eigenspace is one-dimensional with f (1)

012 =
012 is a free parameter and the remaining part of all higher degree
102. This
∂σ(2) which corresponds to the variation of the parameter

2 ) is uniquely determined once we specify f (1)

002 = f (2)

002 = f (1)

f (2)
102 = 0, f (1)
102 = f (2)
terms ( f (j)
m1,m2,2 with m1 + m2 ≥
eigenvector is connected with ∂
σ(2) of the ﬁxed point.

b) m3 = 1. Then β = 1/2. We have f (j)

m1,m2,1 = 0 for m1 + m2 ≤
are two free parameters, all other terms of higher degee ( f (j)
uniquely determined once we specify the above two parameters. Clearly the eigenspace
is two-dimensional. This eigenspace does not correspond to any change of parameters

021, f (1)
111 = f (2)
m1,m2,1 with m1 +m2 ≥

201 = f (2)
111
3) are

1, f (1)

of the ﬁxed point.

c) m3 = 0. Then β = 0. We have f (j)
210 = f (2)

f (1)
120 = f (2)
degee ( f (j)
three parameters. This eigenspace corresponds to ( ∂

030, f (1)
m1,m2,0 with m1 + m2 ≥

m1,m2,0 = 0 for m1 + m2 ≤

300 = 0,
210 are three free parameters. All other terms of higher
4 ) are uniquely determined once we specify the above
∂
∂x(2) ,

030 = f (2)

300 = f (2)

120, f (1)

∂
∂x(3) ).

2, f (1)

∂x(1) ,

Putting all three cases together, we see that dim Γ(n) = 6.

3◦ stable spectrum:

(α) < 0.

ℜ

There are two cases.

Case 1: α =

m
2 , m

1. Recall that β = α + m3

2 , and m3 satisﬁes 0

≥

−

≤
theorem 6.3, for each such β, the number of free parameters is 3
total multiplicity να is given by

m3 ≤
−

m + 2. By

2β. Then the

να =

3

(

m + m3) =

−

−

(m + 3)(m + 4)
2

m+2

m3=0
X

30

Case 2: α = √17

4

m

−

, m

−
2

1. β = α + m3

2 , and m3 satisﬁes 0

m3 ≤

≤

m

−

1. By theorem

6.3, we have

≥

να =

(m

m3 + 2) =

−

m(m + 5)
2

m

1

−

m3=0
X

It follows easily that the eigenfunctions f (j)
variable. By theorem 6.3, the decay estimate on f (j)

α (m1, m2, m3) is obvious.

α (m1, m2, m3) is compactly supported in the m3

It turns out that the eigenvector corresponding to

∂
∂σ(1) is in the eigenspace spanned by

the eigenvectors ( ∂

∂x(1) ,

∂
∂x(2) ,

∂
∂x(3) ). More precisely we have the following:

Lemma 6.3. Let t1 = x(1)

1, t2 = x(2), t3 = x(3)

1. Then

−

−

˜G(σ(1),t1,t2,t3,σ(2))(Y ) = G(σ(1),x(1),x(2),x(3),σ(2))(Y )

(34)

where G(σ(1),x(1),x(2),x(3),σ(2)) is deﬁned in (30). The function ˜G satisﬁes the following scaling
relation:

˜G(σ(1),t1,t2,t3,σ(2))(Y ) = ˜G(1, σ(1)t1, σ(1)t2, σ(1)t3, σ(2))(Y )

(35)

Proof. Let f (j),0
that

m1,m2,0 correspond to the eigenvector

∂
∂σ(1) , then a simple calculation shows

f (j),0
m1,m2,0 = (m1 + m2 −

1) h(j)

m1m2 + h(j)

m1

2,m2 + h(j)

m1,m2

−

2 .

−

m1,m2,0 correspond to the eigenvectors

∂
∂x(1) ,

∂
∂x(1) , and

∂
∂x(3)

If f (j),1

m1,m2,0, f (j),2

m1,m2,0 and f (j),3
respectively, then clearly we have

This immediately gives

1

−

(cid:1)

(cid:0)

f (j),0
m1,m2,0 =

x(1)

m1,m2,0 + x(2)f (j),2
f (j),1

m1,m2,0 +

x(3)

f (j),3
m1,m2,0

1

−

(cid:1)

(cid:0)

∂

∂σ(1) −

σ(1)

(cid:20)

x(1)

1

−

(cid:0)

(cid:1)

∂

∂

x(2)

∂x(1) −

∂x(2) −

x(3)

1

−

∂
∂x(3)

(cid:21)

(cid:1)

G(σ(1),x(1),x(2),x(3),σ(2)) (Y ) = 0 .

(cid:0)
31

Regarding this as a transport equation in the variables (σ(1), t1, t2, t3), we can easily ﬁnd that
˜G satisﬁes the scaling (35). Lemma is proved.

This lemma actually shows in what sense the parameters σ(1), x(1), x(2), x(3) are dependent.

As was shown in

4, we have the ﬁve-parameter family of ﬁxed points G(σ(1),x(1),x(2),x(3),σ(2)).
§

We use the notation π = (σ(1), x(1), x(2), x(3), σ(2)) and write G(π) instead of G(σ(1),x(1),x(2),x(3),σ(2)).
The spectrum of the linearization of the equation for the ﬁxed point does not depend on π
ℓ(u) = 4 and ℓ(n) = 6
(see
neutral eigenvectors Φ(n)
j′

5) and has ℓ(u) = 4 unstable eigenvectors Φ(u)
§

j (Y1, Y2, Y3), 1

(Y1, Y2, Y3), 1

ℓ(n) = 6.

≤

≤

j′

j

≤

≤

7. The Choice of Initial Conditions and the Initial Part of the Inductive
§
Procedure

The equation (21) for the ﬁxed point which was derived in

3 is non-typical from the
§
point of view of the renormalization group theory because it contains the integration over γ,

γ

≤

≤

0
1. On the other hand, since we consider the Cauchy problem for (1) we are given
only the initial condition v(k, 0) which produces through the recurrent relations (4), (5), (6)
or (4′), (5′), (6′) the whole set of functions hr(k, s) or gr(˜k, s). For large p and r
r
can be considered as depending on γ =
p

p they
and our procedure is organized in such a way that

for γ which are away from zero ˜gr are close to their limits. Therefore the initial part of our
process should be discussed in more detail. This is done in this section.

≤

We take k(0) which will be assumed to be suﬃciently large, introduce the neighborhood

A1 =

k :

k

{

κ(0)

−

≤

D1√k(0)lnk(0)

}

(cid:12)
where κ(0) = (0, 0, k(0)) and D1 is also suﬃciently large. Our initial conditions will be zero
(cid:12)
outside A1. Inside A1 they have the form

(cid:12)
(cid:12)

v(k, 0) =

exp

1
2π

1 + Y 2
Y 2
2
2

4

H (0)(Y1, Y2) +

j Φ(u)
b(u)

j (Y1, Y2, Y3)+

−

(cid:26)

(cid:27)  

j=1
X
(Y1, Y2, Y3) + Φ(Y1, Y2, Y3; b(u), b(n))

1
√2π

!

exp

Y 2
3
2

−

(cid:26)

(cid:27)

b(n)
j′

Φ(n)
j′

6

Xj′=1

32

1 (Y1, Y2), H (0)
In this expression k = k(0) + √k(0)Y , H (0)(Y1, Y2) = (H (0)
2 (Y1, Y2), 0) is the ﬁxed
4) corresponding to the parameters σ(1)
1 =
point of our renormalization group (see
§
1, x1 = x2 = x3 = 0. Also Φ(u)
, Φ(n)
are unstable and neutral eigen-functions for H (0)
j′
b(u)
6, b(u)
ρ1 where ρ1 is
described in
j
j
§
another constant which depends on k(0). Its value will also be speciﬁed later. Each function
b(n)
Φ(Y1, Y2, Y3; b(u), b(n)), b(u) =
j′ }

is small in the sense that they satisfy

are our main parameters,

1 = σ(2)

and b(n)
j′

ρ1 ≤
−

b(u)
j }

, b(n) =

j′ ≤

, b(n)

{

{

j

sup

Φ(Y1, Y2, Y3; ¯b(u), ¯b(n))

(cid:12)
(cid:12)
Φ(Y1, Y2, Y3; b(u), b(n))

sup
Y,b

−

Φ(Y1, Y2, Y3; b(u), b(n))

D2,

≤

(cid:12)
(cid:12)
≤

D2(

¯b(u)

b(u)

+

¯b(n)

b(n)

).

−

−

Due to the presence of b(u), b(n), we have l = l(u) + l(n) = 10-parameter families of initial
(cid:12)
(cid:12)
conditions, due to the presence of Φ we have an open set in the space of such families.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Ar =

k :

{

rκ(0)

k

|

−

| ≤

D1√rk(0) ln rk(0)

}

and the variable Y be such that k = rκ(0) + √rk(0)Y . Assume that for r < p,
D1√ln rk(0)

Y

|

| ≤

hr(rκ(0) + √rk(0)Y, s) = Zp(s)Λr
p

−

1

(s)r˜gr(Y, s)

Let

and

˜gr(Y, s) =

exp

Y 2
1 + Y 2
2
2

−

1
√2π

exp

Y 2
3
2

−

(cid:26)

(cid:27)
1 (Y1, Y2, Y3), H (0)
1 (Y1, Y2) + δ(r)

(cid:26)

H (0)

r

·

(cid:27)

1
2π

·

(cid:18)

2 (Y1, Y2) + δ(r)

2 (Y1, Y2, Y3),

1
√rk(0)

(F (r)(Y1, Y2) + δ(r)

3 (Y1, Y2, Y3))

(cid:19)

where in view of incompressibility

H (0)

1 Y1 + H (0)

2 Y2 + F (r) = 0

(36)

We shall derive a system of recurrent relations for Zp(s) and Λp(s) for p < p0. All δ(r)
j

will be considered as remainder terms.

Outside Ar we assume that

33

hr(rκ(0) + √rk(0)Y, s)

|

1
(rk(0))λ1

| ≤

where λ1 is another constant which depends on C1.

Returning back to (6) take the term with some p1, p2, p1 + p2 = p and introduce the new
pk(0)Y ′. Introduce also the variables θ1, θ2,

variable of integration Y ′ where k′ = p2κ(0) +

0

θ1 ≤

≤

(p1k(0))2, 0

θ2 ≤

≤

(p2k(0))2 where s1 = s

p

1
(cid:18)

−

θ1
(p1k(0))2

(cid:19)

, s2 = s

1
(cid:18)

−

θ2
(p2k(0))2

.
(cid:19)

34

Then from (6)

hp(pκ(0) +

= (pk(0))

p
2 i

5

−

Z0

˜g1((Y

Y ′)

−

(cid:28)

p

+ ip

˜gp1

(cid:28)

p1+p2=p
X
p1,p2>1
Y

Y ′

, s

−
√γ

(cid:18)
Zp(s(1

Zp(s(1

θ2

−

((p

1)k(0))2

1

Λp
p

−

(s(1

θ2

))

(p

1)

Zp(s)Λp(s)

((p

1)k(0))2

·

−

·

pk(0)Y, s) = Zp+1(s)Λp
((p

1)k(0))2

p+1(s)p˜gp(Y, s)

dθ2

exp

κ(0,0) +

θ2|
−

(cid:26)

2

Y ′
1)k(0) |

−

·

(cid:27)

√(p

ZR3

)

·

−
pk(0), 0), κ(0,0) +

−
Y
√pk(0)

−
Pκ(0,0)+ Y

(p1k(0))2

(p2k(0))2

(cid:29)

dθ1

dθ2

√pk(0)

˜gp

1

−

Y ′

(cid:18)

p

1

−

r

p

, s(1

θ2

)

d3Y ′+

−

((p

1)k(0))2

−

(cid:19)

1
p

(pk(0))

5

2 p1p2

(p1k(0))2(p2k(0))2

Z0

Z0

ZR3 (cid:18)

(cid:19)

(cid:27)

3
2

exp

1
2π

−

(cid:26)

(Y1

−

Y ′1 )2+(Y2

Y ′2)2+(Y3

Y ′3 )2

−

, κ(0,0) +

θ1

−

(p1k(0))2

1
(cid:18)

θ1

−

(p1k(0))2

Λp1
p

))

·

(cid:19)(cid:19)
1

−

(s(1

Y
√pk(0)

(cid:29)

θ1

Pκ(0,0)+ Y

˜gp2

√pk(0)
θ2

))

Zp(s(1

Y ′

, s

(cid:18)
))

√1

γ

−
Λp2
p

1

−

1
(cid:18)
(s(1

−

(p1k(0))2

·

−

(p2k(0))2

·

−

(p2k(0))2

−

(p2k(0))2

(cid:19)(cid:19)
))

·

−
2γ

θ2

θ2

exp

(cid:27)

(cid:26)

κ(0,0) +

θ1|
−

Y ′
Y
−
γ√pk(0) |

2

exp

(cid:27)

(cid:26)

κ(0,0) +

θ2|
−

Y ′

γ)√pk(0) |

(1

−

2

+

(cid:27)

3
2

exp

1
2π

(cid:18)

+

(cid:19)
i(pk(0))
((p

−

−

(cid:26)

5

1)

2 (p
1)k(0))2

−

(Y ′1 )2+(Y ′2 )2+(Y ′3 )2

2(1

γ)

−
1)k(0))2

((p

−

Zp(s(1

θ1

−

((p

1)k(0))2

−

Z0

ZR3

1

Λp
p

−

(s(1

)

·

dθ1

exp

κ(0,0) +

θ1|
−

(cid:26)

θ1

Y
√(p

Y ′
−
1)k(0) |

2

−

(cid:27)

))

Zp(s)Λp(s)

−

((p

1)k(0))2

·

−

˜gp

1((Y

−

−

Y ′)

(cid:28)

p

1

−

r

p

, s(1

θ1

), κ(0,0) +

−

((p

1)k(0))2

−

Y
Y ′
−
√pk(0)

Pκ(0,0)+ Y

√pk(0)

(cid:29)

˜g1

Y ′√p, s)

d3Y ′

(cid:0)

(cid:1)

(37)

Here γ =

p1
p

to what we did in
consists of four steps.

and κ(0,0) = (0, 0, 1). Now we shall modify (37) for p1 > 1, p2 > 1 similar
3. Later we discuss the terms with p1 = 1 and p2 = 1. The modiﬁcation
§

Step 1. All terms s

1

are replaced by s.

θ1
(p1k(0))2

,s

1

θ2
(p2k(0))2

−

(cid:17)

(cid:16)

(cid:17)

−

(cid:16)

35

Step 2. Write

5

(pk(0))

2 p1p2
(p1k(0))2(p2k(0))2 =

1
2

(pk(0))
(k(0))2γ(1

γ)

−

Step 3. Consider the inner product

(pk(0))

1
2

Y

Y ′

−
√γ

˜gp1

*

(cid:18)

(cid:19)

, s

, κ(0,0) +

Y
pk(0) +

Up to remainders and from (36) it equals to

p

3
2

exp

−

(cid:26)

1
2π

(cid:19)
Y ′2

(cid:18)
Y2 −
√γ

H (0)
1
(cid:20)

(cid:18)

Y ′1

,

Y1 −
√γ

(Y1 −

Y ′1)2 + (Y2 −
2γ

Y ′2)2 + (Y3 −

Y ′3)2

(cid:19)

3
2

=

exp

Y ′1

1
2π
(cid:19)
Y1 −
√γ
Y2 −
√γ

,

,

(cid:18)
H (0)
1
(cid:20)

(cid:18)
Y ′1
Y1 −
√γ

Y1 + H (0)
2

(cid:18)
(Y1 −

Y ′2

−
(cid:26)
Y2 −
√γ
(cid:19)
Y ′1
Y1 −
γ

Y ′2

(cid:19)

,

Y ′2

Y ′1

Y2 +

Y1 −
Y2 −
1
√γ
√γ
√γ
(cid:19)
Y ′2)2 + (Y3 −
Y ′1)2 + (Y2 −
2γ
Y1 −
Y2 −
√γ
√γ
(cid:18)
Y2 −
Y1 −
√γ
√γ
(cid:18)
Y ′2)2 + (Y3 −
Y ′1)2 + (Y2 −
2γ

Y1 + H (0)
2

H (0)
2

Y ′1

Y ′1

−

,

,

3
2

exp

(Y1 −

(cid:27)

F (p1)

Y ′3)2

(cid:27)
Y2−
Y2 −
γ

Y ′2

Y ′2

(cid:19)

(cid:19)
Y ′3)2

Y ′2

=

(cid:21)

Y

Y ′

−
√γ

(cid:18)

, s

=

(cid:19)(cid:21)

−
(cid:26)
Y2 −
√γ
Y2 −
√γ

Y ′2

(cid:19)
Y ′2

Y ′1

Y1 −
√γ

+ H (0)
2

Y ′1

√1

γ

−

(cid:19)

+ H (0)
2

Y ′1

,

Y ′1

,

Y1 −
√γ
Y1 −
√γ

(cid:18)

(cid:18)

Y2 −
√γ
Y2 −
√γ

(cid:27)
Y ′2

(cid:19)

Y ′2

Y ′2

Y2 −
√γ

+

(cid:21)

Y ′2

√1

γ

−

(cid:21)(cid:27)

(cid:19)

j (Y, s) depend only on Y1, Y2 and s. With respect to Y3 we have

H (0)
1

−

(cid:18)

Y ′1

1

(cid:26)

=

−

H (0)
1

γ
−
√γ

1
2π
(cid:18)
(cid:19)
Y1 −
√γ
Y1 −
H (0)
1
√γ
(cid:20)
Let us stress again that H (0)
the usual convolution.

p

−

+

(cid:18)

(cid:18)

(cid:20)

γ

1

,

Y ′1

,

Step 4. Replace the projection operator by the identity operator. It is not the reduction

to the Burgers system because the incompressibility condition is preserved.

36

Now we shall modify the ﬁrst and the last terms in (37). For the ﬁrst one we can write

((p

1)k(0))2

−

5

(pk(0))
((p

2 (p
1)
−
1)k(0))2

−

n

exp

·

s
−

|

Z0

−

dθ2

exp

κ(0,0) +

θ2|
−

(cid:26)

ZR3

2

Y ′
1)k(0) |

−

·

(cid:27)

√(p

κ(0) + (Y

Y ′)

pk(0)

v(κ(0) + (Y

Y ′)

pk(0), 0), κ(0,0) +

p

2

|

o (cid:28)
Pκ(0)+ Y

·

√pk(0)

−

p
p

˜gp

1

−

Y ′

(cid:18)

p

1

−

r

, s(1

θ2

−

((p

1)k(0))2

−

Y
√pk(0)

·

(cid:29)
d3Y ′

)

(cid:19)

(38)

1) comes from the inductive assumption concerning hp

1. As before, we

The factor (p
−
κ(0,0) +
θ2|
−

replace exp

(cid:26)
Y ′

p

1

and ˜gp
the remainder terms.
(cid:16)

1, s(1

−

−

p

q

√(p

2

Y ′
1)k(0) |

−

θ2
1)k(0))2 )

−

((p

−

(cid:27)

(cid:17)

by exp

θ2}

{−

, Pκ(0)+ Y

√pk(0)

by the identity operator

1, s). All corrections are included in

by ˜gp

1(Y ′

−

p

p

−

q

For the Gaussian term in v(κ(0) + (Y

Y ′)

pk(0), 0) we can write

This shows that typically Y

−
Y ′ = O( 1
√p). For the third component F (1) of v(κ(0) + (Y
pk(0), 0) using the incompressibility condition we can write

(2π)

p

−

n

Y ′)

exp

3
2

.

o
−

2p

Y

|

−

Y ′|
2

p

1
√k(0)

−

(Y1 −

(cid:18)

F (1)(κ(0) + (Y

Y ′)

pk(0), 0) =

−

Y ′1)√pH (0)

1 ((Y

p
Y ′2)√pH (0)
Y ′)√p) + (Y2 −

1 ((Y

−

−

Y ′)√p) + O(

1
√k(0)

)

·

(cid:19)

−

1

For the inner product in (38)

exp

·

−

(cid:26)

p

Y

|

2

Y ′

|

−
2

(cid:27)

pk(0)

v(κ(0) + (Y

Y ′)

pk(0), 0), κ(0,0) +

= exp

p

(cid:28)

−

1 ((Y

H (0)
h

·
Y ′1)√pH (0)

1 ((Y

−

√p

−

(Y1 −
(cid:16)

Y
√pk(0)

p

Y

|

−

−
2

Y ′

2

|

·

(cid:27)

p
Y ′)√p)Y1 + H (0)

1 ((Y

(cid:29)
(cid:26)
Y ′)√p)Y2−

−

Y ′)√p) + (Y2 −

−

Y ′2)√pH (0)

1 ((Y

Y ′)√p)

+ O(

−

1
√k(0)

)

(cid:21)

(cid:17)

The expression in the square brackets grows as √p and therefore

pk(0)

v(κ(0) + (Y

Y ′)

pk(0), 0), κ(0,0) +

p

(cid:28)

−

p
37

Y
√pk(0)

=

(cid:29)

can be replaced by

√p

−

(Y1 −
h

Y ′1)√pH (0)

1 ((Y

Y ′)√p) + (Y2 −

−

Y ′2)√pH (0)

1 ((Y

Y ′)√p)

−

−

1
√p

−

1 ((Y

H (0)
(cid:16)

−

Y ′)√p)Y1 + H (0)

1 ((Y

Y ′)√p)Y2

−

(cid:17)(cid:21)

Further,

exp

s
{−
2sk(0)

|

h

exp

·

{−

κ(0) + (Y

Y ′)

pk(0)

2

= exp

k(0)

2

|
p
Y ′)√p√k(0)

}

−

−

exp

i}

s
{−
|
Y
s
{−

|

−

|
Y ′

}·
2pk(0)

κ(0,0), (Y

The ﬁrst factor takes values O(1), the others can be written as 1 + O(
of magnitude of (38) takes the form

}

|
√k(0) ). The main order

1

p exp

s(k(0))2

{−

(p

1)

−
p

}

1
p 

−

(Y1 −

Y ′1)√pH (0)

1 ((Y

Y ′)√p) + (Y2 −

−

Y ′2)√pH (0)

1 ((Y

Y ′)√p)

+

−

+

1
√p

1 ((Y

H (0)
h

−

ZR3 h


Y ′)√p)Y1 + H (0)

1 ((Y

Y ′)√p)Y2

−

1
2π

·

3
2

exp

Y ′|
|
2(p

2p

1)

−

H (0)

Y ′

Y

|

−

2p

Y ′|
2

·

o

−

n

3

2 exp

p
2π

i (cid:16)
p

p

1

(cid:17)

d3Y ′

#

(cid:27)
A similar expression can be written for the last term in (37). Remark that due to our choice
of the interval S(1) the product s(k(0))2 = O(1).

r

(cid:26)

(cid:19)

(cid:18)

(cid:19)

(cid:18)

−

−

i

Now we derive the recurrent formula for Zp(s) and Λp(s). Since our special ﬁxed point
H (0) is a Hermite polynomial of ﬁrst degree, the convolution of H (0) over Y ′ (see (37)) gives
us simply the product of H (0) and the Gaussian term and a polynomial in γ. The function
H (0) and the Gaussian term can then be taken out of the summation in γ and this gives us
the following recurrent system for Zp(s) and Λp(s):

Zp+1(s)Λp
1
p ·

p+1(s)
i
(k(0))2 ·

=

p1+p2=p
X

(6γ2

10γ + 4)

Zp(s)2

Λp

p(s)

·

·

−

(1

·

−

e−

s(p1k(0))2

)

(1

·

−

e−

s(p2k(0))2

)

(39)

38

where the factor (6γ2
take Zp(s) =

−

i(k(0))2 and write Λp+1(s)

Λp(s) = 1 + ξp+1

p2 , then we have

10γ + 4) comes from the convolution of H (0) with itself. Now if we

−

1 +

(cid:18)

p

=

ξp+1
p2

(cid:19)

1
p ·

γ
X

(6γ2

10γ + 4)

(1

e−

s(p1k(0))2

)(1

e−

s(p2k(0))2

)

−

·

−

−

then it is not diﬃcult to see that there exists bounded ξp+1 (with an bound independent of
p) such that the equality holds. It is an elementary fact that the limit

Λ(s) = lim
→∞

p

Λp+1(s) = Λ1

1 +

∞

Yk=1 (cid:18)

ξk+1
k2

k

(cid:19)

exists.

Now we discuss the behavior of all remainders for p <

k(0)

λ2.

By Φ(u)

, Φ(n)
j′

j

we denote the eigen-vectors of the linearized renormalization group corre-
sponding to the ﬁxed point H (0). For each p we make the following inductive assumption for
δ(r)(Y, s), r < p :

(cid:0)

(cid:1)

4

6

δ(r)(Y, s) =

j,r + β(u)
b(u)

j,r

Φ(u)

j +

j′,r + β(n)
b(n)

j′,r

Φ(n)
j′

+ Φ(st)
r

,

γ =

r

−

p

1

j=1 (cid:16)
X
1)αj b(u)

(cid:17)
j′,r = b(n)

Xj′=1 (cid:16)
, Φ(st)
r

j,r = (p

j γα(u)
where b(u)
subspace of the linearized renormalization group, γ = r
p
−

, b(n)

−

j′

j

1.

(cid:17)

is a function which belongs to the stable

As we go from p

1 to p, the variable γ = r
p
−

−

1 is replaced by γ′ = r

p = γ

p

1
p . Therefore
−

·

j,r + β(u)
b(u)
(cid:16)

j,r

(cid:17)

γαj Φ(u)

j =

(p

1)αj b(u)

−

j + β(u)
(cid:17)
αj

j,r

·

p

p

(cid:18)

−

1

(cid:19)

1

p

(cid:18)
−
β(u)
j,r

·

(cid:19)

·

αj

p

(γ′)αj Φ(u)

j

·

(cid:19)
(γ′)αj Φ(u)

j

.

=

pαj b(u)

j +

(cid:16)

(cid:18)

In the same way for the neutral eigen-functions we have

b(n)
j′
(cid:16)
because αj′ = 0. In the same way one can transform Φ(st).The coeﬃcients β(u)
j′,r are small
compared to the ﬁrst term . An important conclusion is that the projections to the unstable

j,r , β(n)

+ β(n)
j′,r

Φ(n)
j′

(cid:17)

39

directions increase, projections to the neutral directions remain the same and projections
to the stable directions decrease. As was already said, in the case of unstable and neutral
directions the term containing b(u)

is the main term.

or b(n)
j′

j

Now we discuss the form of δ(p)(Y, s). It is the sum of three types of terms.

j

, b(n)
j′

j (γ′)αj Φ(u)

a1). The term which depends linearly on all δ(r)(Y, s). Especially important is the part
Φ(n)
. If we were to have and be in the limiting
j′

which contains all pαj b(u)
regime H (0) then the integral will give pαj b(u)
since
j
γ′ = 1. However, H (r) are slightly diﬀerent from H (0). Therefor we shall have a small
pj1, β(n)
correction which is included in all β(u)
pj′1,
Φ(st)
p,1 . The we have terms which are linear functions of all β(u)
. They
will give us β(u)

. We denote it as β(u)
j′,r and Φ(st)

j = (p + 1)αj b(u)
Φ(u)

j′,p and in Φ(st)

j,r , β(n)

j,p , β(n)

j Φ(u)

1 + 1
p

(cid:17)

(cid:16)

αj

p

r

j

pj2, β(n)

pj′2, Φ(st)
p,2 .

a2). The term which is the sum of all quadratic expressions depending on δ(p1), δ(p2). We

expand it using our basis of Φ(u)

, Φ(n)
j′

j

and all stable eigen-vectors.

a3). The term which contains all corrections which arise during the four steps described

above. We also expand it in the same way as in a2)).

The sum of all terms gives β(u)

p,j , β(n)
p,j′

, Φst
p .

We use this procedure till p = p0 = (k(0))λ2. The procedure for p > p0 is discussed in

9.
§

8. The List of Remainders and Their Estimates
§

In the beginning of

7 we described 10-parameter families of initial conditions which we
§
consider in this paper. We mentioned above that for each p we have an interval S(p) =
S(p)
on the time axis. Actually these intervals are changing when p = pn = (1 + ǫ)n
−
h
where ǫ > 0 is a constant. Therefore we shall write S(n) =
will be no confusion.

and hope that there

, S(pn)
+

S(pn)
−
h

, S(p)
+

i

i

In this and the next section we consider p > (k(0))λ2. Each function ˜gr(Y, s), 3

r < p,

≤

has the following representation:

40

in the domain

Y

|

| ≤

C1√ln rk(0), Y = (Y1, Y2, Y3)

˜gr(Y, s) = Λr

−

1

σ(1)
2π

r

·

·

exp

3
∈ R

σ(1)
2

(cid:26)

2 +

Y1|

|
(cid:0)

+

2

Y2|

|

σ(2)
2π

· s

σ(2)
2 |

2

Y3|

exp

−

(cid:26)

·

(cid:27)

(cid:0)

H (0)(Y1, Y2) + δ(r)(Y, s)

;

(cid:1)

in the domain

> C1

ln(rk(0)) :

(cid:1)(cid:9)
Y

|

|

p

exp

σ(1)
2π

−

(cid:26)

exp

·

Λr

−

−

r

·

(cid:26)
1

·

≤

σ(1)
2
σ(2)
2 |
1
rλ3

1

−

2 +

(

Y1|

|

2)

Y2|

|

σ(2)
2π ·

· s

(cid:27)

2

Y3|

· |

(cid:27)

H (0) (Y1, Y2) + δ(r)(Y, s)

|

for some constant λ3 > 0. We use the formula (7) to get ˜g(p)(Y, s). New remainders appear
in one of the following ways.

Type I. The recurrent relation (7) does not coincide with the equation for the ﬁxed point

and actually is some perturbation of this equation. The diﬀerence produces some
remainders which tend to zero as p
.

Type II. For the limiting equation all eigen-vectors in the linear approximation are multi-
plied by some constant. In the equation (7) it is no longer true and the diﬀerence

generate some remainders. (see also

Type III. The remainders which are quadratic functions of all previous remainders.

→ ∞

9).
§

41

8A. The Remainders of Type I.
§

We call the domain A the set

ln(rk(0))
. The estimates will be done separately in each domain. We in-
{|
clude the ﬁrst, the second and the last two terms in (7) in the remainders. We shall estimate

and the domain B the set

ln(rk(0))

> D1

| ≤

D1

p

{|

Y

Y

}

}

|

p

only the ﬁrst one, the others are estimated in the same way.

Domain A: We have

β(1)
p (Y, s) = (p + 1)

5
2

p2

i
sp2 ·

·

dθ2

< v

k(0) +

Z0

ZR3

(cid:18)(cid:18)

Y

Y ′

−
√s

p + 1 , 0

; b

,

(cid:19)

(cid:19)

p

√s k(0) +

Y
√p + 1

> P

√s k(0)

+

Y
√p+1

˜gp

Y ′,

1
(cid:18)

−

θ2
p2

s

(cid:19)

(cid:19)

exp

√s k(0) + (Y

Y ′) √p + 1

√s k(0) p + Y ′

p + 1

d3Y ′

(cid:18)

θ2
p2

−

2

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

2

(cid:12)
(cid:12)
(cid:12)

(cid:27)

p

Here b means the collection of all parameters of v(k; 0). The the main contribution to the

integral comes from Y

Y ′ = O

. In this domain in the main order of magnitude

−

(cid:26)

(cid:12)
(cid:12)

−

−

1
√p+1

(cid:16)

(cid:17)

p

v(k(0) +

h

Y

Y ′

−
√s

p + 1, 0 ; b), √s k(0)

= O(1)

i

Assuming that v(k(0) + Y
see that the inner product

Y ′

√s √p + 1, 0; b) is diﬀerentiable w.r.t the ﬁrst three variables we

−

v(k(0) +

h

Y

Y ′

−
√s

p

p + 1, 0 ; α) , √s k(0) +

Y
√p + 1i

is of order O(1). For ˜gp we can write using our inductive assumptions

˜gp

Y ′,

(cid:18)

θ2
p2

1
(cid:18)

−

(cid:19)

(cid:19)

s

= Λp

−

1

p

·

·

σ(1)
2π · r

σ(2)
2π ·

exp

−

(cid:26)

σ(1) (

2 + Y2|
Y1|
2

|

2)

(cid:27)

exp

·

−

(cid:26)

2)

σ(2) (
Y3|
|
2

(p)

Y ′,

· H

(cid:27)

(cid:18)

1
(cid:18)

−

θ2
p2

s

.

(cid:19)

(cid:19)

42

Also

exp

θ2
p2

−

(cid:26)

√s k(0) p + Y ′
(cid:12)
(cid:12)
(cid:12)

p

2

(cid:12)
(cid:12)
(cid:12)

(cid:27)

p + 1

= exp

θ2

(−

√s k(0) +
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y ′√p + 1
p

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

and in the main order of magnitude the integration over θ2 does not depend on Y ′. Thus we
can write

β(1)
p (Y, s)

|

| ≤

Λ(p

−

2)

p

exp

·

·

σ(1)
2

−

(cid:26)

2 +

(

Y1|

|

2)

Y2|

|

(cid:27)

exp

·

−

(cid:26)

σ(2)
2 |

2

Y3|

D4
p

·

(cid:27)

(40)

(p + 1)

Here and later various constants whose exact values play no role in the arguments will be
denoted by the letter D with indices. Since in the expression for ˜gp+1 we have the factors
σ(1)
Λp
2π

Y1|
|
is relatively smaller than ˜gp+1 with an order O( 1
o
p). This is good
enough for our purposes. We did not discuss the errors which follow from the fact that the
expressions in the previous formulas depend on θ2.

σ(1)
2 (
−
β(1)
n
p (Y, s)

(40) shows that

, the estimate

σ(2)(s)
2π

Y2|

Y3|

σ(2)
2

2 +

exp

exp

q

−

n

o

|

·

·

·

·

|

|

|

2

2

Domain B: The smallness of β(1)

p (Y, s) in this case follows easily from several inequalities

and arguments.

D4

pk(0) because

k

D5pk(0).

|

| ≤

1◦:

2◦:

3◦:

Y

Y

|

|

If

| ≤

p

Y ′

−

| ≤

Y

|

−

Y ′

| ≤

2s+
√p

then

D6√k(0) because v(k, 0; b) has a compact support.

If

Y

|

Y ′

−

| ≥

2s+
√p

then

−

(cid:26)

(cid:12)
(cid:12)
(cid:12)

exp

√s k(0) + (Y

Y ′)

p + 1

−

p

1

≤

2

(cid:12)
(cid:12)
(cid:12)

(cid:27)

exp

−

n

√s k(0) + (Y
(cid:12)
(cid:12)
(cid:12)

−

p

43

Y ′)

p + 1

exp

≤

2

|

o

s+
4

−

n

Y

Y ′

2

|

−

.

o

(cid:12)
(cid:12)
(cid:12)

4◦:

If

Y ′

|

| ≥

D7 √p then

5◦:

If

Y ′

|

| ≤

D7 √p then

6◦: We have

exp

θ2
p2

−

(cid:26)

√s k(0)p + Y ′
(cid:12)
(cid:12)
(cid:12)

p

2

(cid:12)
(cid:12)
(cid:12)

(cid:27)

p + 1

exp

C8θ2}

{−

≤

θ2
p2

exp

−

(cid:26)

√s k(0) p + Y ′

p + 1

p

exp

2 +

(

Y ′1|

|

2)

Y ′2|

|

−

2

(cid:27)

(cid:12)
(cid:12)
(cid:12)
σ(2)
2 |

1 .

≤

2

Y ′3|

(cid:27)

(cid:12)
(cid:12)
(cid:12)
σ(1)
2

−

(cid:26)

σ(1)
2

= exp

−

(cid:26)

(cid:16)

Y1 −
|

(Y1 −

Y ′1)

|

2 +

Y2 −

|

(Y2 −

Y ′2)

2

|

(cid:17)

σ(2)
2 |

−

Y3 −

(Y3 −

Y ′3)

2

|

= exp

σ(1)
2

−

(cid:26)

2 +

(

Y1|

|

2)

Y2|

|

−

2)

(

Y3|

|

(cid:27)

(cid:27)

σ(2)
2

If

Y

|

Y ′

−

| ≤

exp

·

σ(1) (Y1(Y1 −
(cid:8)
σ(1)(s)
2

Y1 −

|

−

2s+
√p

(cid:0)
then

exp

σ(1) (Y1(Y1 −
(cid:8)

Y ′1) + Y2(Y2 −

Y ′2)) + σ(2) Y3(Y3 −

Y ′3)

2 +

Y ′1|

2

Y2 −

|

Y ′2|

σ(2)
2 |

2

Y3 −

Y ′3|

.

(cid:27)

−

(cid:1)

Y ′1) + Y2(Y2 −

Y ′2)) + σ(2)(s) Y3(Y3 −

Y ′3)

σ(1)
2

−

(

Y1 −

|

Y ′1|

2 +

Y2 −

|

Y ′2|

2)

−

σ(2)
2 |

2

Y3 −

Y ′3|

C8 .

≤

(cid:27)

44

If

Y

Y ′

>

|

−

|
Gaussian factor and

2s+
√p

(p)(Y )

|

|H

then we have an integral of the function which is the product of some

. Direct estimate shows as before that in this case

β(1)
p (Y, s)

|

| ≤

Λ(p

−

1)

p

e−

·

·

σ(1)
2

(

Y1|

|

2 + Y2|

2)

σ(2)
2 |

2

Y3|

e−

·

D8
p

3
2

·

which is also good for us.

In the same way one can estimate terms with relatively small p1 and p

p

−

√p. The remainders will be of order

or p1 ≥
from splitting the integration over θ and Y ′ (see (7) and beginning of
that p1 > √p or p1 < p

√p because other terms were estimated before. Put

1
√p1 ·

p1 (i.e., p1 ≤

√p
−
1
p . The next set of remainders comes
3). We may assume
§

−

˜˜gp+1(Y, s) = i (p + 1)

5
2

p2
1

p2
2

dθ1

Z0

dθ2 ·

1
1p2
p2
2

p1+p2 = p+1
X
p1,p2 > √p

Z0

(Y

Y ′)

−

(1

1
2

)

θ1
p2
−
1
√γ

,

1
(cid:18)

−

θ1
p2
1 (cid:19)

s

, √s k(0) +

Y
√p + 1i

ZR3

h

˜gp1 


P

√s k(0) +

Y
√p + 1

˜gp2 


Y ′(1

θ2
p2
2

−

(1

γ)

−

,

1
(cid:18)

−

θ2
p2
2 (cid:19)

s









1
2

)

p

2

exp

·

θ1 |
−

(cid:26)

√s k(0) +

Y ′

Y
−
√p + 1

·

γ |

−

θ2|

√s k(0) +

Y ′

Y
−
√p + 1γ |

2

.

(cid:27)

Using the inductive assumption we can rewrite the last expression as follows:

˜˜gp+1 (Y, s) = i (p + 1)

p2
1

p2
2

dθ1

dθ2

p1+p2 = p+1
X
p1,p2 > √p

Z0

Z0

Λp1

−

1

Λp2

−

1

·

·

γ(1

γ) ·

1
p + 1

exp

σ(1)
2

Y1 −

|

Y ′1|

2 +
γ

2

Y2 −

|

Y ′2|

1

−

−

(cid:26)

45

σ(2)
2

|

Y3 −
γ

2

Y ′3|

−

−

σ(1)
2

2

2 +
Y ′1|
(1

|

Y ′2|
|
γ)

−

σ(1)
2

·

−

2

Y ′3|
|
γ
1
−

(cid:27)

1

. p

2 <

(p1)

H

Y

Y ′, s

−

(cid:18)

1
(cid:18)

−

θ1
p2
1 (cid:19)(cid:19)

,

√s k(0) +

Y
√p

>

P

·

√s k(0) +

H

Y
√p

(p2)

Y ′, s

(cid:18)

1
(cid:18)

−

θ2
p2
2 (cid:19)(cid:19)

d3Y ′ .

As was explained before, due to incompressibility in the Domain A , the inner product

(p1)

hH

Y

Y ′; s

−

(cid:18)

1
(cid:18)

−

θ1
p2
1 (cid:19)(cid:19)

, √s k(0) +

Y
√p i

takes values O( 1
O( 1

√p). Therefore the product

√p ) because the ﬁrst two components of the vector √s k(0) + Y

√p are of order

(p1)

√p

h H

Y

Y ′, s

−

(cid:18)

1
(cid:18)

−

θ1
p2
1 (cid:19)(cid:19)

, √s k(0) +

Y
√p i

takes values of order O(1).

The remainder can be written in the following form:

β(2)
p

(Y, s) = i

p1+p2 = p+1
X
p1,p2 > √p

1

−

γ(1

γ) ·

1
p ·

p2
1

p2
2

dθ1

dθ2

Z0

Z0

Λp1

−

1

Λp2

−

1

·

·

1
Λp ·

·

exp

−

(cid:26)

R3

Z

σ(1) (

Y1 −

|

2 +
Y ′1|
2γ

Y2 −

|

Y ′2|

2)

σ(2)
2γ ·

|

Y3 −
2γ

2

Y ′3|

−

−

σ(1) (

2)

2 + Y ′2|
Y ′1|
|
γ)
2(1
−

σ(2)
2(1

2

Y ′3|
|
γ)
−

(cid:27)

−

(p1)

<

H

Y

Y ′, s

−

(cid:18)

1
(cid:18)

−

θ1
p2
1 (cid:19)(cid:19)

,

46

√s k(0) +

Y
√p

>

P√s k(0) + Y

·

√p H

(p2)

Y ′,

1
(cid:18)

−

s

θ2
p2
2 (cid:19)

·

(cid:19)

(cid:18)

exp

·

θ1|
−

(cid:26)

√s k(0) +

Y

Y ′
−
√pγ |

2

θ2|

−

√s k(0) +

Y ′
√p (1

2

γ) |

·

(cid:27)

−

1

−

γ(1

γ) ·

1
p ·

p2
1

Z0

i

−

p1+p2 = p+1
X
p1,p2 > 1

exp

θ1s

dθ1

{−

}

exp

θ2s

dθ2

{−

}

p2
2

Z0

exp

−

(cid:26)

ZR3

σ(1) (

Y1 −

|

2 +
Y ′1|
2γ

Y2 −

|

Y ′2|

2)

σ(2)(

|

Y3 −
2γ

2)

Y ′3|

−

σ(1) (

2)

2 + Y ′2|
Y ′1|
|
γ)
2(1
−

σ(2)
2(1

2

Y ′3|
|
γ)
−

(cid:27)

−

−

1
2

p

·

· hH

(p1) (Y

Y ′), √s k(0) +

−

Y
√p i

P

√s k(0) +

H

Y
√p

(p2) (Y ′, s) d3Y ′ .

We did not include the factor Λp
remainder is estimated in the following way.

−

·

1

p because it is a part of the inductive assumption. This

First we consider

R1 =

√s k(0) +

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
As before, consider the domain where

(cid:12)
(cid:12)
(cid:12)
(cid:12)

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

Y

2

Y ′

−
√pγ

s

+

−

!

√s k(0) +

Y ′
√p (1

γ)

−

−

s

!

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y

|

Y ′

−

| ≤

D9

ln (pk(0)),

Y ′

D10

ln (pk(0)) .

|

| ≤

q

q

We write

47

R1 = |

2

Y ′
γ2
1

|

Y
−
p
·

+ |
p

Y ′

2

|
γ2
2

·

+ C11

Y ′

|

Y
−
√p γ

|

(cid:18)

|

+

Y ′
|
√p(1

|
−

.

γ)

(cid:19)

In the Domain A

Therefore

R1| ≤

|

C12 ln(pk(0))
pk(0)

.

R2 = exp

θ1

√s k(0) +

(−

2

Y
Y ′
−
√p γ

θ2

√s k(0) +
(cid:12)
(cid:12)
(cid:12)
θ2s
(cid:12)

−

(cid:12)
(cid:12)
(cid:12)
exp
(cid:12)

{−

}

Y ′
√pγ2

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

exp

θ1s

{−

} ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

= exp

(θ1 + θ2)s)

{−

exp

} · "

(−

√s k(0) +

Y

2

Y ′

−
√pγ

s

−

!

θ1

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
Y ′
√p(1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

#

exp

·

θ2

(−

√s k(0) +

s

γ)

−

!) −

−

 (cid:12)
(cid:12)
(cid:12)
(cid:12)

R2| ≤

|

exp

(θ1 + θ2)s

{−

C13

θ1 ·
√pγ

}

(cid:18)

θ2 ln p

√p(1

γ)

(cid:19)

−

.

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

and in the Domain A

This shows that in the Domain A we can replace the exponent

√s k(0) +

exp

θ1|
−

(cid:26)

Y

Y ′
−
√pγ |

2

θ2|

−

√s k(0) +

Y ′
√p(1

2

γ) |

(cid:27)

−

{−

(θ1+θ2)s(k(0))2

and the remainder will be not more than D14 ln p

by exp
. This is enough for
our purposes. In the Domain B the estimates are similar because again the main contribution
D10√ln p. In other words, in the
D9√ln p,
to the integral comes from
−
Domain B we can replace the product of the Gaussian factors and

(p) by

| ≤

| ≤

Y ′

Y ′

√p

Y

}

|

|

H

exp

1
2

−

(cid:26)

σ(1) (

Y1 −

|

Y ′1|

2 +

Y2 −

|

Y ′2|

2)

−

σ(2)

2

Y3 −

|

Y ′3|

1
2

48

1
2

−

σ(1) (

2 +

Y ′1|

|

2)

Y ′2|

|

−

σ(2)(

2)

Y ′3|

|

1
2

.

(cid:27)

This is also enough for our purpose.

The next remainder of Type I comes from the diﬀerence between the sum over γ and the

corresponding integral. The remainder β(3)

p (Y, s) is the diﬀerence between the sum

i

√γ

(1

γ)

−

·

p1+p2 = p+1
X
p1,p2 > √p

p

1
p ·

ZR3

exp

−

(cid:26)

σ(1) (

Y1 −

|

2 +
Y ′1|
2γ

Y2 −

|

Y ′2|

2)

σ(2)(

|

Y3 −
2γ

)2

Y ′3|

−

σ(1)(

2 +

Y ′1|
|
2(1

2)

Y ′2|

|
γ)

−

−

σ(2)
2(1

2

Y ′3|
|
γ)
−

(cid:27)

−

3
2

3
2

1
2πγ

·

(cid:18)

·

(cid:18)

(cid:19)

1
2π(1

γ)

(cid:19)

−

1
2

p

·

· hH

(p1)((Y

Y ′)) , √s k(0) +

−

Y
√p i

P

√s k(0) +

H

Y
√p

(p2)(Y ′, s) d3Y ′

and the corresponding integral over γ from 0 to 1. It is easy to check that this diﬀerence is
not more than C14
√p .

8B. The Remainders of Type II and III
§

integrals . The functions

All remainders of Type II appear because we use the sums (over p1) instead of the
are deﬁned for all γ. We use a linear interpolation to
C16
deﬁne δ(γ, Y, s) for all γ. From our inductive assumptions it follows that
√p .
(cid:17)
Therefore, the remainders which follow from the diﬀerence between the sum and the integral

δp(γ, Y, s)

| ≤

−
√γ

H

(cid:16)

Y ′

|

Y

also satisfy this estimate.

It remains to consider quadratic expressions of δp(γ, Y, s). The Gaussian density is present

in all these expressions. Therefore, all the remainders are not more than C17
p .

49

9. Final Steps in the Proof of the Main Result
§

In this section we consider our procedure for p > p0. Introduce the sequence pn, pn =
1 = (1 + ǫ)np0, where ǫ > 0 is small (see below). These are the values of p when

(1 + ǫ)pn
we make the renormalization of our parameters. For p

−

= pn, no renormalization is done.

In

7 we explained the choice of our ﬁxed point H (0). The corresponding eigen-functions
§
are denoted by Φ(u)
. Also we have eigen-functions of the stable part of the spectrum.
Consider p, pm < p < pm+1. By induction we assume that we have an interval on the time
, r < p, so that
axis

j and Φ(n)
j′

and s

, S(m)
+

, S(m)
+

S(m)
−
h

S(m)
∈
−
h
˜gr(Y, s) =Λr
−

i

r

1

i

(H (0)(Y ) + δ(r)(Y, s))

·

·
σ(1)
2π

·

·

exp

1 + Y 2
σ(1)(Y 2
2 )
2

σ(2)
2π

exp

σ(2)Y 2
3
2

(cid:27)

−

(cid:26)

· r

(cid:27)

−

(cid:26)

If γ = r
p
−

1 then

δ(r)(Y, s) =

j,p + β(u)
b(u)

j,r

γα(u)

j Φ(u)

j (Y ) +

j′,p + β(n)
b(n)

j′,r

Φ(n)

j (Y ) + Φ(st)

r

(Y, γ).

4

j=1 (cid:16)
X

(cid:17)

6

Xj′=1 (cid:16)

(cid:17)
j′,p, Φ(st)
j,p , b(n)

r

can be written as a

j,r , β(n)

here β(u)
j′,r are small corrections to the main terms b(u)
series w.r.t. the stable eigen-functions. (see Appendix II).

At one step of our procedure p

1 is replaced by p, γ is replaced by γ′ = γ

is replaced by
·
During the whole interval pm < p < pm+1 the variable b(u)

1 + 1
p
−

is replaced by (¯b(u)

j,p + β(u)
j,r )
(cid:16)
j,pm acquires the factor

j,p + ¯β(u)

, b(u)

(cid:16)

(cid:17)

j,r

1

α(u)
j

−
(γ′)α(u)

j

·

p

j

1

−

p and γα(u)
α(u)
j
1 + 1
p
−

1

.

(cid:17)

α(u)
j

1

1 +

p

1

(cid:19)

−

≈ e(1+ǫ)α(u)

j

.

pm<p<pm+1 (cid:18)
Y

A similar statement holds for the stable part of the spectrum. The neutral part remains the
same since α(n)
j′

= 0.

Now we shall discuss δ(p)(Y, s) using (7). As in

7 δ(p)(Y, s) consists of three parts.
§

Part I.

In all δ(r) the main term is the one which contains our basic parameters b(u)
consider terms in (7) which are linear in b(u)

. As it follows from the deﬁnition

, b(n)
j′

. We

j

, b(n)
j′

j

50

6
of the linearized group and its spectrum we get

part we get 1 because α(n)
= 0. We put b(u)
j′
The stable part is transformed accordingly.

(cid:16)
j,p+1 = b(u)
j,p ·

(cid:17)
1 + 1
p

(cid:16)

(cid:17)

α(u)
j

1 + 1
p

b(u)
j,p . For the neutral
α(u)
j

j,p , b(n)
b(u)

j′,p+1 = b(n)
j′,p.

Part II. The term which is the sum of quadratic functions of all δ(r). We expand it using
and the functions from the stable part of the
j′,p and the stable function Φ(st)
j,p , β(n)

the basis of our functions Φ(u)
spectrum. The result is included in β(u)

, Φ(n)
j′

(Y, s).

p

j

Part III. All remainders which arise because the formulas for ﬁnite p are diﬀerent from the

limiting formulas. These remainders were estimated in
a series w.r.t. our basis and the corresponding terms are included in β(u)
the stable part of the spectrum.

6. The result is written as
§
j′,p and

j,p , β(n)

Finally we have

j,p+1 = b(u)
b(u)

j,p

1 +

,

j,p+1 = b(n)
b(n)

j,p

α(u)
j

1
p

(cid:19)

(Y, s). This works for p < pm+1. If p = pm+1, then

(cid:18)
j′,p and Φ(st)
and the formulas for β(u)
p
we introduce new variables (rescaling!)

j,p , β(n)

j,pm+1 = b(u)
b(u)

j,pm+1

1 +

+ β(u)

j,pm+1,

α(u)
j

1
pm+1 (cid:19)
1 + β(n)

.

j′,pm+1

1

−

(cid:18)
= b(n)

j′,pm+1

−

b(n)
j′,pm+1

It is our other inductive assumption that

−
where 0 < ρ1 < 1 but ρ1 is suﬃciently close to 1.

−

ρm
1 ≤

b(u)
j,pm ≤

ρm
1 ,

ρm
1 ≤

b(n)
j,pm ≤

ρm
1

m+1 =

Let ∆(m+1)

and ∆(m+1)
ρm+1
=
m
1
It follows easily from the estimates of β(u)
, then ∆(m)

(cid:3)
∆(m)
m

, ρm+1
1

−

(b(u)
j

, b(n)
j′

(b(u)
j,pm+1, β(n)
n

j,pm, b(n)
ρm+1
j′,pm) :
1 ≤
j′,pm+1 that ∆(m+1)
m

−

b(u)
j,pm+1, b(n)
∆(m)
m .

j′,pm+1 ≤
If ∆(m)

0 =
is a decreasing sequence of closed sets. The

⊆

j′,m)
gives us the values of parameters for which δ(p)

o

∈

0

(cid:2)
j,m, b(n)
) : (b(u)
m ∆(m)

0

n
intersection

→ ∞

as p

.

→ ∞

ρm+1
1

.

o

T

We make also some shortening of the time interval S(m). In the formulas for δ(r) there
are several remainders which appear when we replace in all expressions s′ and s′′ by s. We

51

estimate these remainders using the fact that our functions satisfy the Lipschitz condition
and the Lipschitz constants and the maxima of their values decay as some power of p. We
choose the interval S(m+1)
S(m+1) these remainders do
S(m) so that when we consider s
not violate the basic inclusion ∆(m+1)
m . It is easy to see S(m+1) can be chosen so that
S(m)
m S(m)
is an interval of positive length.

S(m+1) consists of two intervals whose lengths decay exponentially. Therefore

∆(m)

⊂

⊂

∈

\

m

T

j,pm+1, b(n)

The transformation (b(u)

(b(u)
j′,pm) is given by smooth functions and is
suﬃciently close to the identity map. The last step in the renormalization is the replacement
in all δ(r), r < pm+1 the variables b(u)
j′,pm+1.
The form of δ(r) in new variables is the same as before.

j′,pm by their expressions through b(u)

j,pm+1, b(n)

j,pm, b(n)

j,pm, b(n)

j′,pm+1

→

)

The Choice of Constants

The main constants which are used in the construction are the following:

1. k(0) which determines the position of the domain where v(k, 0) is concentrated.

2. D1 is the constant which determines the size of the neighborhood where v(k, 0) is

concentrated.

of v(k, 0).

3. ρ1 determines the size of the neighborhood where the main parameters b(u)

, b(n)
j′

j

vary.

4. D2 is the constant which determines the possible size of perturbations Φ(st) in the form

5. λ1 is the power which gives the estimation of the decay of hr in the domain B.

6. λ2 is the parameter which determines the size of the ﬁrst part of the procedure.

7. ǫ determines the values of p where we make the renormalization.

The value of k(0) should be suﬃciently large. All estimate of the remainders which appear
. They should be so small that the

during the ﬁrst half of the procedure are less than const
(k(0))

1
2

estimates of all β(u)
j′r are much smaller than ρ1. On the other hand, ρ1 should be small
but not too small. It should be small in order to make the quadratic part of our formulas

jr , β(n)

smaller than the linear part. However ρ cannot be too small in order that we could choose

52

−

ρm+1, ρm+1]. This can be achieved by the choice of k(0). The parameter
the next interval [
λ2 should be small. In this case the estimates of all corrections are easier. However, after λ2
is chosen the value of k(0) can be taken suﬃciently large depending on λ2. The parameter
λ1 can be arbitrarily large in order to make the perturbation arbitrarily small. The value of
(k(0))λ1 . We choose D1 so that
D1 determines the estimates in the domain B which decay as
2. The value of ǫ is chosen small so that we can write with a good precision the action

1

λ1 > 1
of the linearized renormalization group.

10. Critical Value of Parameters and Behavior
§
of Solutions near the Singularity Point

We return back to the ﬁrst formulas:

vA(k, t) = exp

A

v(k, 0) +

exp

(t

2

k

t
|
{−

|

}

·

2

s)

k

{−

−

|

|

} ·

Aphp(k, s) ds

or

t

Z0

t

Z0

p>1
X

p>1
X

vA(k, t) = exp

2

k

t
|

|

}

{−

·

A

v(k, 0) +

exp

(t

2

s)

k

{−

−

|

|

} ·

Apgp(k√s, s) ds .

(41)

t

∈

Our construction gives us the interval S =
S we can ﬁnd the values of parameters b(u)
j = b(u)
T
j′

j (t), 1
6 such that we have the representation (31) with δ(r)

4 and b(n)
= b(n)
j′
j′
. It is easy to
1(t). If so then Aphp(k, t) is concentrated in the domain with the center
√t having the size O(√p) and there it takes values O(p). This immediately implies that

n S(n) on the time axis such that for each
(t),

≤
0 as r

1
see that Acr(t) = Λ−
at κ(0)p
at t the energy is inﬁnite.

→ ∞

→

≤

≤

≤

j

Consider t′ < t. It is important to investigate the behavior of E(t′) and the enstrophy
t′. It
C1∆t+ O(∆t)) for some
−
C∆t+o(∆t))p. It is clear
. For p >> ln(∆t)−

Ω(t′) of the same solution with A = Acr(t) when t′ is close to t. Denote ∆t = t
follows easily from the proof of the main result that Λ(t′)/Λ(t) = (1
constant C1. Since Ap
= (1
−
ln(∆t)−
that the terms in (41) are close to each other for p
∆t

Λ(t′)/Λ(t)
O

(Λ(t′))p = Ap

(Λ(t))p

cr ·

the

cr ·

−

∆t

·

p

1

1

(cid:0)

≤

(cid:1)
(cid:16)

(cid:17)

53

product Ap
Therefore it is enough to consider

cr(Λ(t′))p tends exponentially to zero and dominates other terms of the expansion.
and in this domain the solution grows as

O

k

1

3

1

|

k

2 . The factor

k
|
essential contribution to the solution belonging to an interval of the size O(
From this argument it follows easily that E(t′) ∼

2 appears because for any k the values for which the terms in (41) give the
) = O(√p).

and Ω(t′) ∼

k

|

|

.

8

6

1

1

|

|
ln(∆t)−
p
∆t

ln(∆t)−
∆t

|

| ≤

ln(∆t)−
∆t

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

54

Appendix I. Hermite Polynomials and their basic properties

Take σ > 0 and write

He(σ)

n (x) = (

σx2
2

1)ne

−

dn
dxn e−

σx2
2 , n

0.

≥

It is clear that He(σ)

n (x) = σnxn +
the n-th Hermite polynomial.

· · ·

, where dots mean terms of smaller degree. We
0 (x) = 1, He(σ)
1 (x) =
n (√σx). It is easy to

It is clear that He(σ)
2 He(1)
n (x) = σ

n

σ and so on. In general, He(σ)

shall call He(σ)
n
σx, He(σ)
2 (x) = σ2x2
check that

−

σxHe(σ)

n (x) = He(σ)

n+1(x) + σnHe(σ)

n

1(x).

−

m (x)e−

σx2
2

σ

2π is (iλ)me−

λ2
2λ . This implies the formula for

The Fourier transform of He(σ)
convolution:

p

He(σ)

m1(x

−

R1

Z

σ(x

y)2

−
2

y)e−

σ
2π ·

r

He(σ)

m2(y)e−

σy2
2

dy = He(σ)

m1+m2(x)e−

σ
2π

r

σx2
2

σ
2π

r

Take positive γ1, γ2, γ1 + γ2 = 1 and consider the convolution of He(σ)

m1( x
√γ1
. Their Fourier transforms are (iλ√γ1)m1e−

)e−
λ2γ1
2σ

σ
2πγ1

and He(σ)

m2( x
√γ2

)e−

σx2
2γ2

·

σ
2πγ2

respectively. The product of these two functions is γ

m2
2

m1
1 γ
2

2

(iλ)m1+m2e−

λ2
2σ .

λ2γ2
2σ

q
(iλ√γ2)m2e−
Therefore the convolution is γ

q
m1
2

1

·

m2
2

γ

2 He(σ)

m1+m2(x)e−

σx2
2 .

(42)

(43)

σx2
2γ1

·
and

55

References

[C] M. Cannone. Harmonic Analysis Tools for Solving the Incompressile Navier-Stokes

Equations. Handbook of Mathematical Fluid Dynamics, vol. 3, 2002.

[Cl] Clay Mathematical Institute. The Millennium Prize Problems, 2006.

[F-T] C. Foias and R. Temam. Gevrey Classes of Regularity for the Solutions of the Navier-

Stokes Equations. J. of Funct. Anal. 87, 1989, 359-369.

[G] Y. Giga, T. Miyakawa. Navier-Stokes Flow in R3 with Measures as Initial Vorticity

and Morrey spaces. Commu. Partial Diﬀerential Equations, 14, 1989, 577-618.

[K] T. Kato. Strong Lp-solution of the Navier-Stokes Equation in Rm, with Applications

to Weak Solutions. Math. Zeitschrift, 187, 1984, 471-480.

[La] O. Ladyzenskaya. The mathematical theory of viscous incompressible ﬂow. New York:

Gordon and Breach Science Publishers, 1969.

[Le] J. Leray. ´Etude de diverses ´equations int´egrales non lin´eaires et de quelques probl´emes

que pose l’hydrodynamique. J. Math. Pures Appl. 12, 1993, 1-82

[Si 1] Ya. G. Sinai. Power Series for Solutions of the Navier-Stokes System on R3. Journal

of Stat. Physics, vol. 121, No. 516, 2005, 779-804.

[Si 2] Ya. G. Sinai. Diagrammatic Approach to the 3D-Navier-Stokes System. Russian

Math. Surveys, vol. 60, No.5, 2005, 47-70.

[Y] V.I. Yudovich. The Linearization Method in Hydrodynamical Stability Theory. Trans.

Math. Mon. Amer. Math. Soc. Providence, RI 74(1984).

February 2, 2008:gpp

56


5
0
0
2
 
n
u
J
 
2
2
 
 
]
h
p
-
m
o
t
a
.
s
c
i
s
y
h
p
[
 
 
1
v
3
7
1
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

Fundamental Physical Constants:
Looking from Different Angles

Savely G. Karshenboim

Abstract: We consider fundamental physical constants which are among a few of the most
important pieces of information we have learned about Nature after its intensive centuries-
long studies. We discuss their multifunctional role in modern physics including problems
related to the art of measurement, natural and practical units, origin of the constants, their
possible calculability and variability etc.

PACS Nos.: 06.02.Jr, 06.02.Fn

R´esum´e : Nous ... French version of abstract (supplied by CJP)

[Traduit par la r´edaction]

Received 2004. Accepted 2005.

Savely G. Karshenboim. D. I. Mendeleev Institute for Metrology, St. Petersburg, 189620, Russia and Max-
Planck-Institut f¨ur Quantenoptik, Garching, 85748, Germany; e-mail: sek@mpq.mpg.de

unknown 99: 1–46 (2005)

2005 NRC Canada

2

Contents

1

Introduction

unknown Vol. 99, 2005

2 Physical Constants, Units and Art of Measurement

3 Physical Constants and Precision Measurements

4 The International System of Units SI:

Vacuum constant ǫ0, candela, kelvin, mole and other questions
. . . .
4.1
‘Unnecessary’ units .
4.2
‘Human-related’ units . . . .
4.3 Vacuum constant ǫ0 and Gaussian units
4.4

. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .

‘Unnecessary’ units, II

. . .
. . .
.
. . .

. . . .
. . . .

. . . .

. . .

. . . .
. . . .
. . . .
. . . .

. . .
. . .
. . .
. . .

. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .

. . . .

5 Physical Phenomena Governed by Fundamental Constants
. . . .
. . . .
. . . .
. . . .
. . . .

. . . .
5.1 Free particles
. . . .
. . . .
5.2 Simple atoms and molecules . . . .
. . . .
. . . .
5.3 Free compound particles
5.4 Macroscopic quantum phenomena .
. . . .
5.5 Atomistics and discrete classical phenomena . . .

. . .
. . .
. . .
. . .

. . . .

. .

. . . .
. . . .
. . . .
. . . .
. . . .

. . .
. . .
. . .
. . .
. . .

. . . .
. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .
. . . .

.
.
.
.

.
.
.
.
.

6 Fundamental Constants and Renormalization:

Operational philosophy of physics

7 On Calculable Physical Constants

8 Natural Units

9 Deﬁnitions and Mise en Pratique for the SI Units:

A back door for natural units

10 Fundamental Constants and Geometry

11 Constancy of Fundamental Constants

12 Search for Possible Time Variation of Fundamental Constants
. . . .

12.1 Atomic clocks . . . .
. . .
. . . .
12.2 Scaling of different transitions in terms of the fundamental constants .
. . .
12.3 Current laboratory limits . .
. . . .
. . . .
. . .
12.4 Non-laboratory searches for the variations of the constants

. . . .
. . .

. . . .

. . . .

. . . .

. . . .

. . .

. . .

. . . .
. . . .
. . . .
. . . .

. . . .
. . . .
. . . .
. . . .

.
.
.
.

13 Fundamentality of the Constants and the Planck Scale

14 Constants of Cosmology

15 Physics at the Edge

16 Dreaming about New Physics

17 Conclusions

2005 NRC Canada

3

4

7

10
10
11
13
14

15
16
16
18
19
20

21

22

25

27

28

30

35
36
36
39
40

40

41

42

42

44

Savely Karshenboim

1. Introduction

3

‘You needn’t say “please” to me about ’em,’ the Sheep said,
...‘I didn’t put ’em there, and I’m not going to take ’em away.’
L.C.

There is a number of ways to understand Nature. One can approach it with logics, with guesses, with
imagination. A way scientists and, especially, physicists address the problem is based on a comparison
of our ideas and the reality via measurements. Experiment inspires theory and veriﬁes theory. The soul
of the physical approach is neither logics, nor even a quantitative approach, but a common sense. The
latter is based on centuries-long experience in investigation of Nature. It tells us to be sceptical. It tells
us that even complicated phenomena are often based on simple pictures, and most of them allow to
estimate effects in terms of certain fundamental quantities. It tells us that those simple pictures are
good to start with but should involve more and more details once we desire a more accurate agreement
between any of our theories and the measured reality.

Fundamental constants play a crucial role in physics in a few different ways and we consider their
signiﬁcance in this paper. However, to start the subject we need to agree on what the fundamental
constants are. And we discover a great variety of approaches to the problem based on a particular role
of a particular constant in a speciﬁc ﬁeld of physics.

Two polar points of view are related to ‘practical’ and ‘fundamental’ physics.

•

•

The practical view addresses the art of measurement which makes physics to be physics. There is
a number of beautiful laws such as the Maxwell equations or the Dirac equation which pretend to
describe Nature. However, as a quantitative method of exploring the world, physics needs some
quantitative values to be measured. That requires certain parameters enter basic equations. We
also need certain quantities to be used as units to make proper comparisons of different results.
Some of these parameters enter a number of equations from different branches of physics and are
universal to some extent. That is a ‘practical’ way to deﬁne what the fundamental constants are.
A very important property of such constants is that they should be measurable. The fundamental
constants understood in such a way are a kind of an interface to access Nature quantitatively and
apply basic laws to its quantitative description.

However, not every such a constant is truly fundamental. If we, e.g., need a unit, we can consider
a property of such a non-fundamental object as the caesium atom. The approach of fundamental
physics is based on the idea that we can explain the world with a few very basic laws and a few
very basic constants. The rest of the constants should be calculable or expressed in terms of other
constants. Such constants are our interface to really fundamental physics but most of them have
a very reduced value in real measurements, because they are often not measurable. To deduce
their values from experiment, one has to apply sophisticated theories and, sometimes, certain
models.

A good illustration of a difference between these two approaches is the situation with the Rydberg

constant

R∞ =

=

α2mec
2h
e4me
8ǫ2
0h3c

,

which is expressed in a simple way in terms of certainly more fundamental quantities. However, this
exactness is rather an illusion, because the constant is not measurable in a direct way. The most accu-
rately measured transition in hydrogen is the triplet 1s

2s transition (see, e.g., [1])

νH (1s

−

2s, F = 1) = 2 466 061 102 474 851(34) Hz

[1.4

−

−14]

10

×

(1)

(2)

2005 NRC Canada

4

unknown Vol. 99, 2005

and it may be only approximately related to the Rydberg constant (see Fig. 1). To obtain its value, one
has to apply quantum electrodynamics (QED) theory and perform some additional measurements. At
the present time, the fractional accuracy (a number in squared brackets) in the determination of this
constant [2]
R∞ = 10 973 731.568 525(73) m−1

−12]

[6.6

(3)

10

×

is much lower than that of the measurement of νH (1s
Rydberg constant in some other way, e.g.

−

2s) (cf. (2)). One might indeed redeﬁne the

R =

νH (1s

2s) ,

−

4
3 c

(4)

e
making the accuracy of its determination higher. However, a relation with more fundamental constants
would be more complicated and not exactly known. The practical and fundamental approaches cannot
easily meet each other because we can very seldom both exactly calculate and directly measure some
quantity, which has a certain non-trivial meaning. A choice between the practical and fundamental
options is a kind of trade-off between measurability and applicability, on one side, and calculability
and theoretical transparency, on the other.

There is a number of approaches which lie between the two mentioned above. For example, quan-
tum electrodynamics (QED) at the very beginning of its development met a problem of divergencies
in simple calculations. A response to the problem was the idea of renormalization, which states that
QED theory should express observable values (such as, e.g., the Lamb shift in the hydrogen atom or
the anomalous magnetic moment of an electron) in terms of certain observable properties (such as the
charge and mass of an electron). The measurable charge and mass are deﬁnitely more fundamental
than most of ‘practical’ constants such as caesium hyperﬁne constant. However, they are certainly less
fundamental than similar quantities deﬁned at the Planck scale. Such an approach is in formal sense
not an ab initio calculation of a quantity under question (e.g., the Lamb shift), but rather an ab initio
constraint on observable quantities (the energy shifts, the charge and the mass).

The point which deﬁnitely uniﬁes all approaches is that the fundamental constants are such dimen-
sional or dimensionless quantities which are fundamentally important to understand, investigate and
describe our world. However, the importance is often understood differently. In a ﬁnite-size paper it
is not easy to consider the whole range of problems related to the fundamental constants and part of
discussion missing here can be found in another recent review of the author [3] (see also [4]).

2. Physical Constants, Units and Art of Measurement

Speak in French when you can’t think of the English for a thing.
L.C.

When one does a measurement, certain units should be applied to arrive at a quantitative result. A
measurement is always comparison and in a sense we deal with dimensionless quantities only. ‘Math-
ematically’, this point of view is true, however, it is counterproductive. To compare two similar quan-
tities measured separately, we have to go through a number of comparisons. Instead of that, it has been
arranged to separate a certain part of comparisons and use them to introduce units, certain speciﬁc
quantities applied worldwide for a comparison with similar quantities under question. The units (or a
system of units) ﬁnd its endower as a coherent system of certain universally understood and legally
adopted measures and weights which can be used to measure any physical quantity.

One should not underestimate problems of measurements. Access to quantitative properties of Na-
ture is a crucial part of physics and it is a problem of fundamental importance to improve and extend
our accessibility.

Since the very discovery of the world of measurable quantities, we used natural units. But their

degree of naturality was different. We started with values related to our essential life:

2005 NRC Canada

Savely Karshenboim

5

νH (1s − 2s) =

c R∞

1 +

2
(Zα)

+

4
(Zα)

+

6
(Zα)

+ . . .

3
4

(

11
48

h
me
mp

43
384

13
24

−1 −

2
(Zα)

−

4
(Zα)

+ . . .

17
64

851
12288

i

i

+

+

+

+

+

+

+

+

+

+

+

+

+

+

−

2

¶
3

h

¶
h
me
mp

h
me
mp

µ

me
mp
µ
(Zα)3
π
(Zα)3
π

h
me
mp

1 +

(Zα)

+ . . .

2

41
48

i

− 1 + . . .

i
1
(Zα)2 −

8
9

−

ln

7
9
2

7
3

ln

8
3

1
(Zα)2 +
4
9

h
1
(Zα)2 −
ln
1
(Zα)2 +

ln

28
3

4
3

µ
2
(Zα)

−

¶
28
9

(Zα)

h
2 me
mp

2
(Zα)

3
+ α(Zα)

2

h
me
mp

¶
h
log 2 −

+ α(Zα)

−14 log 2 +

µ
14
3
h
3 me
mp
7
8

2

h
2
(Zα)

4
(Zα)

α
π

³

´

2

2989
288

i
2989
96

h
ln

2

1
(Zα)2 +
7
2

π

2

³
ln 2 +

−

i
208
9
70π2
81

³
+ 50.2976

´

·
2 me
mp
2

(Zα)

α
π

2

³
´
4
(Zα)

·
3
(Zα)

56
81

3

ln

³
+

´
−

246337
32400

h
−

385π2
81

µ
+ 147(25)

+ . . .

α
π
α
π

α
π

α
π

α
π

α
π

α
π

³
+

´
1673π2
405

·
2

log

2 +

α(Zα)3
π2

me
mp

2
(Zα)

14
9

−

2

3136
81

·
mecRp
¯h

³

´

+ . . .

)

ln k0(2s) +

ln k0(1s) −

ln 2 −

112
3

805
54

ln k0(2s) −

ln k0(1s) + 112 ln 2 +

+ . . .

i
889
18

i

64
9

32
9

64
3

32
3

log k0(2s) +

log k0(1s) −

log k0(2s) −

log k0(1s) +

266
135
i
14
5

i

−

ln

56
3

1
(Zα)2 −

8
3

64
3

log k0(2s) +

log k0(1s) −

−

ln 2 +

347
90

1

ln

(Zα)2 + 71.626974

+ . . .

+

15253
1944

´
+

21
4

ζ(3)

¸

2

π

ln 2 −

21
2

70π2
27

−

15253
648

−

ζ(3)

63
4

¸

1
27

2

ln

1
(Zα)2 +
1126
135

+

1
(Zα)2
7π2
4

ln 2 −

ln 2 −

2 − 34.845333

ln

248
27

2

ln

1
(Zα)2

¶

14
15

i

i

3

i
2
(Zα)

−

248659831
279936

+

1765757π2
29160

−

11137π4
9720

+

7952
27

ln 2 −

33509π2
324

ln 2

497
81

4

log

2 +

588497
6912

ζ(3) +

ζ(3) −

ζ(5)

+ . . .

847π2
216

595
72

245π2
108

+

14π2
3

ln 2 − 14ζ(3) −

π(Zα) ln

14
9

2

¸
1
(Zα)2

¸

Fig. 1. A relation between the 1s − 2s transition frequency νH (1s − 2s) and the Rydberg constant R∞. A
correction for a difference between the center of gravity of the 1s and 2s hyperﬁne multiplets and their triplet
component is not included.

2005 NRC Canada

6

•

•

•

•

parameters of human beings;

parameters of water, the most universal substance around us;

parameters of Earth itself;

parameters of Earth as a part of the Solar system.

unknown Vol. 99, 2005

This approach had been followed until the introduction of the metric system two centuries ago, and, in
fact, the very metric system was originally based on properties of Earth: the metre1 was deﬁned in such
a way that a length of a quadrant (a quarter of meridian) of a certain meridian was equal to 10 000 km
(exactly); the second was obviously deﬁned by a day and a year; the gram was then understood as a
mass of one cubic centimetre of water and so on.

The metric treaty was signed in 1875 in Paris. Since then we have changed contents of our units but
tried to keep their size. Few changes took place after the SI system was adopted in 1960 (‘SI’ means
Syst`eme International d’Unit´es — International System of Units). The latest version is presented in an
CIPM2 brochure [5]. For example, the SI unit of length, the metre, was originally deﬁned via the size
of Earth, later was related to an artiﬁcial ruler, to a hot optical emission line and now to the hyperﬁne
structure interval in cold caesium atoms

νHFS(133Cs) = 9 192 631 770 Hz

(exactly)

and a ﬁxed value of the speed of light

c = 299 792 458 m/s

(exactly) .

One also has to remember that this unit was introduced as a substitute for numerous units based on
details of the shape of a human body, such as the foot and the yard and takes their magnitude (in a
general scale) from them

1 m

1.1 yd

3.3 ft .

≃

≃
This evolution in the deﬁnition of the metre clearly demonstrates two great controversies of the
SI system: changing stability and advanced simplicity. First, there has been no single SI system at
all. We have seen with time since the very appearance of the metric convention, a number of various,
but similar, systems of units. While the hierarchy and basic relations between the units were roughly
the same all over the time, the units themselves and related standards changed drastically. However,
for practical reasons, the size of the units during those revolutionary redeﬁnitions was kept the same
as close as possible. Secondly, the system of units is indeed a product created ﬁrst of all by non-
physicists for non-physicists. I dare say, we, physicists, even do not care what the SI units actually
are. For example, most of us have learned about the mole in a time, when we could not recognize that
excitations, binding effects in the solid phase, kinetic energy etc would change a mass of the sample
(but indeed not a number of particles). Later, after we have learned about all these effects, we assumed
that the SI deﬁnition is properly adjusted to them. But it is unlikely that most of us checked the SI
deﬁnitions for that. Actually, all SI deﬁnitions come historically from non-relativistic classical physics
and similar to their appearance we have also learned them for the ﬁrst time as classical non-relativistic
stuff. We do not care about actual SI deﬁnitions partly because we do not consider seriously the legal
side of SI and due to that we believe that we may ourselves interpret and correct SI deﬁnitions if
necessary.

1 There are two different spellings for this term: the meter is used in USA, while the metre is used in UK and most of other
English speaking countries and in international literature (see, e.g., [5]). The latter is also traditionally used in metrological
literature.

2 CIPM is the International Committee for Weights and Measures.

2005 NRC Canada

(5)

(6)

(7)

Savely Karshenboim

7

Physicists serve as experts only while decisions are made by authorities. The SI system has been
created for a legal use and trade rather than for scientiﬁc applications. Due to that, crucial features of
the SI convention should be expressed as simply as possible. Meanwhile, these ‘simply deﬁned’ units
should allow to apply the most advanced physical technologies. That makes the SI system to be a kind
of an iceberg with a stable and simple visible part, while the underwater part is sophisticated, advanced
and changes its basic properties from period to period. The changes in the deﬁnition of the SI metre
have demonstrated a general trend in physical metrology: to use more stable and more fundamental
quantities and closely follow progress in physics. Eventually, we want units to be related to quantized
properties of natural phenomena and most of all, if possible, to values of fundamental constants. We
already have natural deﬁnitions of the metre and the second, and are approaching a natural deﬁnition
of basic electric units and, maybe, the kilogram.

Note, however, that a choice of units is not restricted to the International system SI. There is a
number of options. Certain units, such as universal atomic mass unit, are accepted to be used together
with the SI units [5]. There is a number of units such as the Bohr magneton µB = e¯h/2me and
the nuclear magneton µN = e¯h/2mp, which do not need any approval since they are well-deﬁned
simple combinations of basic fundamental constants. A number of quantities are measured in terms of
fundamental constants. For example, the electric charge of nuclei and particles is customarily measured
in that of the positron. Sometimes, instead of introducing units, new values with special normalization
are introduced, such as angular momentum in quantum physics, which is equal to the actual angular
momentum divided by the reduced Planck constant ¯h.

Fundamental constants (as units) play a very important role in precision measurements and in
special cases. The latter corresponds to a situation when conventional methods cannot be applied. For
example, sometimes to determine a temperature we can not use a thermometer properly calibrated
using primary thermodynamical standards. In some cases, when, e.g., the temperature is too high or
too low, or if we have to deal with a remote object, we may rely on the Boltzmann distribution and
measure frequency and the spectral intensity of emitted photons. To interpret the frequency in terms of
temperature, we have to use the values of the Planck constant h and the Boltzmann constant k.

3. Physical Constants and Precision Measurements

‘You needn’t say “exactly,” ’ the Queen remarked: ‘I can
believe it without that. Now I’ll give you something to
believe.’

L.C.

A precision measurement is another case very closely related to fundamental constants. As we
mentioned, those constants are universal and some may appear in measurements in different branches
of physics. That offers us a unique opportunity to verify our understanding of Nature in a very general
sense. We know that any particular theory is an approximation. Our basic approach always involves
certain laws and certain ideas on what the uncertainty of our consideration is. The most crucial test of
the whole approach is to check if values from different areas of physics agree with each other.

This test has been regularly performed by the CODATA task group on fundamental constants, which
publishes its Recommended Values of the Fundamental Constants [2] (see also previous CODATA3
papers [6]). The progress in the determination of the most important fundamental constants for about
30 years (since the establishment of the CODATA task group) is shown in Fig. 2. The responsibility of
the group is to compare results from different ﬁelds and to deliver the most accurate values of constants
important for ‘precision’ measurements of ‘essential’ quantities. Indeed, the ‘precision’ threshold is
different for different quantities. Note that constants related to cosmology, astronomy and some from

3 CODATA is the Committee for Data for Science and Technology of the International Council for Science.

2005 NRC Canada

8

unknown Vol. 99, 2005

-4

10

-6

10

-8

10

-10

10

i

y
t
n
a
t
r
e
c
n
u

 
l

a
n
o

i
t
c
a
r
F

 

 

G

N

A

e

 

h

h•N

A

m

/m

p

e

m

[u]

p

R

1970

1980

1990

2000

Fig. 2. Progress in the determination of fundamental constants: the time dependence of the fractional uncertainty
(see the recent paper [2] and also [6] on earlier results by the CODATA task group). Here, G stands for the
Newtonian gravity constant, NA for the Avogadro constant and mp[u] for the proton mass in the universal atomic
mass units.

particle physics such as the Hubble constant, astronomical unit and Cabibbo angle are traditionally
excluded from the consideration as not being related to the ‘precision’ physics. Meanwhile, certain
properties of light nuclei (deuterium and both stable helium isotopes 3He and 4He) are included.

The most important lesson we have learned from CODATA’s work is not just their recommended
values, but evidence of overall consistency of our approach to quantitative description of Nature. That
is illustrated in Figs. 3 and 4 showing different approaches to the determination of the ﬁne structure
constant α and the Planck constant h [2], which play a central role in the adjustment of the fundamental
constants [2].

Why are the values of these two constants so signiﬁcant for the CODATA adjustment? The answer
is that when we consider physics from the fundamental point of view, the electron and proton are just
certain particles among many others. However, when we do our measurements, we deal not with matter
in general but mainly with atomic substances where electrons and protons are fundamental ‘bricks’. In
such a case electron and proton properties are as fundamental as h and c or even more. In particular,
these properties determine results of spectroscopy of simple atoms and macroscopic quantum electro-
magnetic effects (see Sect. 5). The experiments, results of which are presented in Figs. 3 and 4, involve,
directly or indirectly, such constants as the Rydberg constant, the electron and proton masses, the elec-
tric charge and magnetic moments of an electron and a proton, the Planck constant and the speed of
light.

Let us consider the Planck constant in more detail. The accuracy of the determination of the most

important fundamental constants is summarized in Fig. 2. We note, that the ﬁne structure constant

α =

e2
4πǫ0¯hc

(8)

2005 NRC Canada

Savely Karshenboim

9

Fig. 3. Determination of the ﬁne structure constant α by different methods as discussed in [2]. Among the results:
a free QED value from the anomalous magnetic moment of electron (ae), a bound QED value from the muonium
hyperﬁne structure (Mu), an atomic interferometer value (Cs), a value involving a lattice parameter (n) and values
dealing with calculable capacitor (RK ) and gyromagnetic ratio of proton and helion (γ), measured in SI units with
help of macroscopic electric standards. The grey vertical strip is related to the CODATA-2002 value [2].

 
 

 R

 (LCIE-01)

K

 R

 (NIM-95)

K

 R

 (NPL-88)

K

 R

 (NML-97)

K

 R

 (NIST-97)

K

 

’ (KRISS/VNIIM-98)

h

 

’ (NIST-89)

p

 h/m (n)

 h/m (Cs)

 Mu hfs

 a

e

 

’ (NIST-95)

p

 
 

137.03597

137.03600

137.03603

-1

 
 

 
 

 
 

N

A

F (NIST-80)

’ (NPL-79)

p

’ (NIM-95)

p

K

•R

 (NIST-98]

J

K

2

2

K

•R

 (NPL-90)

J

K

K

 (PTB-91)

J

K

 (NML-89)

J

h [J•s]

6.62606•10

6.62607•10

6.62608•10

-34

-34

-34

Fig. 4. Determination of the Planck constant h by different methods as discussed in [2]. The results come from
the measurement of the Faraday constants F = eNA, the Avogadro constant NA, the gyromagnetic ratio of proton
(γp), volt (KJ ) and watt (KJ · RK ) balances. The grey vertical strip is related to the CODATA-2002 value [2].

2005 NRC Canada

10

unknown Vol. 99, 2005

and the molar Avogadro constant h
NA have been known better that some of their constituents, such
as h, e and NA. That means that an experimental determination of h is equivalent to a determination
of e and NA and thus, from the experimental point of view, the Planck constant h becomes also related
to classical electrodynamics and atomic and molecular physics of substance. In a ‘practical’ sense, h
is even more universal than in a ‘fundamental’ sense. More discussions on this subject can be found in
[3].

·

4. The International System of Units SI:

Vacuum constant ǫ0, candela, kelvin, mole and other questions

From the point of view of fundamental physics, the system SI [5] is unnecessarily complicated. It

has seven basic units:

metre, second and kilogram (which beyond any doubt are crucial units for any system of units
for physical quantities);

ampere (which is already questionable and, in fact, a number of physicists (see, e.g., [7]) strongly
believe that it is much better to set ǫ0 = 1 and to measure electrical quantities in terms of three
basic mechanical units);

kelvin and mole (which are, in a sense, unnecessary units since the thermodynamic energy and
the number of particles can be measured without introducing any special units);

candela (which looks like a worse case — an unnecessary unit for a quantity related to a sensi-
tivity of a human eye, an object, which is rather outside of physics).

4.1. ‘Unnecessary’ units

‘I didn’t say there was nothing better,’ the King replied. ‘I said
there was nothing like it.’

L.C.

Let us start with the ‘unnecessary’ units. From the philosophical point of view, any measurement
is a comparison of two quantities of the same dimension and thus is a relative measurement. However,
as mentioned above, if we do not like to create every time a chain of comparisons, we should introduce
certain units. A measurement in terms of these units, although still a comparison, is a very special
comparison and we qualify such as an absolute measurement. To be more precise, we like to have a
coherent system of units and thus it is not enough to deﬁne any units, we have to deﬁne a certain system
of units with

one unit for each kind of quantity (each dimension);

most units derived from a few basic units (e.g., the newton, a unit of force, is deﬁned through the
metre, the second and the kilogram: 1 N = 1 kg

1 s−2).

1 m

×

×

Meanwhile, in certain areas the relative measurements are so much more accurate (or much more
easier, or have other big advantages) than the absolute measurements, that we face a hard choice: either
to support a minimized coherent system of units, or to introduce some ‘extra’ units (inside or outside
the system). There are several options for a solution. A choice made in the case of temperature and
amount of substance was to extend the system and to introduce new basic units. For the mass of atoms
and molecules, the universal atomic mass unit has been introduced as a unit outside of SI, but ofﬁcially
recognized and recommended for use. Nuclear magnetic moments are customarily measured in units
of the nuclear magneton, which has never been included in any ofﬁcial recommendation of units.

2005 NRC Canada

•

•

•

•

•

•

Savely Karshenboim

One may think that since the kelvin appeared a long time ago4 (before we realized that temperature
is a kind of energy), it is kept now for historic reasons only. That is not correct. An example with the
foot in USA shows how to treat the problem. There is no independent foot there — this traditional unit5
is deﬁned as an exactly ﬁxed part of the metre:

1 ft = 0.3048 m (exactly) .

As a result, for everyday life a use of feet and metres is not quite the same. Meanwhile, for scientiﬁc life
and industrial precision mechanics and electronics, their simultaneous use may be quite confusing, but
it is completely equivalent: the same information, the same accuracy, the same actual basic deﬁnitions.
On the contrary, the use of the kelvin and the joule is not the same – interpreting data from one unit to
the other changes the accuracy of the results.

11

(9)

4.2. ‘Human-related’ units

It was labelled ‘Orange marmalade’, but to her great
disappointment it was empty.
L.C.

The case of candela presents an additional problem, which is not a question of units, but a question
of quantities. Why did the original foot and similar units fail? One of the reasons is that they were ill-
deﬁned. But that is half-truth only. The truth is that they were related to non-physical quantities. The
original foot was ﬁrst related to a size of a particular person (a king/queen), later some approaches were
related to an ‘average’ person. And only eventually, the ‘human’ foot was substituted by an artiﬁcial
ruler.

When we rely on properties of a particular or average human being, we deal with a biological object.
If we now reverse the problem and try to check a value related to the foot in its original sense, we meet
a biological problem. We need to make a decision on the selection of people, to address the problem
that the result may depend on geography etc. Eventually, the ultimate decision will choose either a
‘conventional foot’ (as it is), which is not related anymore to any person, or a ‘conventional person’,
which should be a subject of a real measurement. In other words, even measuring some property in
well-deﬁned units, we may need in certain cases an arbitrary agreement on what this property is. We
qualify this kind of agreement as arbitrary because within certain margins we are free to adopt any
parameters.

In fact, the problem of the average size of a human body is not so important now, however, there is
a number of questions due to ecology, safety, medicine, which involve interactions of certain physical
effects and a human being. We can easily characterize these phenomena by a complete description of
their physical properties. However, for obvious practical reasons, we often need an integral estimation
of the inﬂuence which involves a number of parameters, which values vary in a broad range (such as
frequency), and we certainly know that the human sensitivity depends on frequency and other various
parameters. Such integral characteristics are not of physical nature. To determine them we need to
perform two kinds of measurements on

physical details of the effects (a kind of the spectral distribution);

•

•

spectral sensitivity of a human being.

4 To be more precise, the Celsius temperature scale is meant since Celsius’ and Kelvin’s degrees are the same.
5 Actually there is a number of different versions of the foot. Eq. (9) corresponds to the international foot. There is also the
U.S. survey foot which is equal to 1200/3937 m. The number is chosen in such a way that 1 m = 39.37 in (see Sect. B.6
in [8] for legal detail; historical details can be found in [9]).

2005 NRC Canada

12

unknown Vol. 99, 2005

If we accept the sensitivity as a real quantity, which is determined by effects beyond physics, the
whole integral characteristic becomes not a pure physical but of combined nature: physics+biology. If
we accept a model for the sensitivity, we can do simple calculations within this model and obtain a pure
physical result, which will be related to the model rather than to reality. In other words, the result will
be in well-deﬁned units but for a conventional quantity. In some cases (e.g., in radiology [10]) real and
conventional quantities are clearly distinguished. In others the separation is less clear. But in any case,
quantities, related to a sensitivity of an [average] human eye, cannot be accepted as physical quantities
and it is does not matter how their units are deﬁned.

What is also important is a status of the SI system as an international treaty. Everything related to SI
is a part of this agreement. Otherwise, it is not a part of SI. A unit has to be a unit for a certain quantity.
If a quantity is not well-deﬁned, the unit is also ill-deﬁned. If we deal with a quantity for which an
additional agreement is needed, we have to put it into SI, because it has to be a part of a deﬁnition of
the related unit.

The question of the candela is very doubtful. The candela itself is deﬁned as a part of SI in ‘rigid
physical terms’6. As we mentioned above, there may be a need to have a convention on properties, but
never on all physical quantities of a kind. We deﬁne the metre of SI and we can use it. Length in general
is well-deﬁned and does not need any additional agreement. However, if we like to measure particular
properties of certain objects, which are related to length, we may need an additional agreement on these
properties. For example, when we deal with an average parameter of a human body.

This problem of a ‘conventional’ characteristic or a ‘conventional’ object is not only for human-
related (or life-related) cases. It is due to the peculiarity of classical objects. A number of well-known
not-life-related examples of conventional properties are related to those of Earth such as the ‘standard
acceleration due to gravity’ (gn = 9.806 65 m/s2), adopted by General Conference on Weights and
105 Pa), various conventional days and
Measures, the ‘standard pressure’ (of 1 atm = 101 325
years. From a formal point of view, these values have nothing to do with the real acceleration of free
fall which varies from place to place and during the day. As we mentioned in Sect 2, the metric treaty
ﬁrst relied to properties of Earth (the metre, the second, and, indirectly, the kilogram), which were
believed to be well-deﬁned. Later, it was realized that they are not and the units were redeﬁned via
artiﬁcial objects (which are now partly substituted by natural quantum objects). Earth as a whole also
presents an example of a conventional object, when its shape is simpliﬁed and a number of properties
are ‘projected’ to the sea level. A convention is not necessary related to a peculiar object, a reason for
a convention may be speciﬁc conditions of experiment. A recommendation for the practical realization
of the metre [11] gives a list of accurately measured optical atomic and molecular transitions. However,
some transitions are to be measured under speciﬁc conditions, i.e. their given frequencies are related
to real transition frequencies not necessarily identical to them.

×

The case of the candela and photometry is very speciﬁc and quite different from any other basic
unit. Without accepting, as a part of SI, a convention either on a spectral sensitivity of a human eye7 or
on what are ‘the same sensation’ and ‘an [average] human eye’ we see a very reduced ﬁeld for mea-
surements. Actually, there are two kinds of photometrical quantities: visual and physical. To introduce
the candela as a unit for both, we need both kinds of conventions.

6 The candela is the luminous intensity, in a given direction, of a source that emits monochromatic radiation of frequency

540 × 1012 hertz and that has a radian intensity in that direction of 1/683 watt per radian [5].

7 Appendix 2 of the ofﬁcial SI booklet [5] contains some details of practical realizations of all basic SI units. In the case of
the candela, it reads: The deﬁnition of the candela given on page 98 [of [5]] is expressed in strictly physical terms. The
objective of photometry, however, is to measure light in such a way that the result of the measurement correlates closely with
the visual sensation experienced by a human observer of the same radiation. For this purpose, the International Commission
on Illumination (CIE) introduced two spectral functions V (λ) and V ′(λ)... One of them, V (λ), is applied in photometry.
However, the recommendations of CIPM on practical realizations do never (except of the case of the candela) contain any
information which in needed for the realization. They are supposed to deliver certain information which follows from the
main body of the SI booklet and may be used to simplify the realization (see, e.g., Sect. 5.4 and 9).

2005 NRC Canada

Savely Karshenboim

13

The candela and photometrical quantities were designed to deal with all visible frequencies. How-
ever, at the present time the SI system does not include any convention which allows to go beyond the
1014 Hz at which the candela is deﬁned. That means that the candela deﬁnition as
frequency of 5.4
an SI unit is incomplete and completely compromises it as such, because SI denies any quantity to be
measured in candelas. Within just SI, we cannot measure photometrical quantities related to, e.g., red
light.

×

For really physical quantities (such as electrical current or amount of substance) those deﬁnitions
are rigid, however, for the human-related quantities, the deﬁnitions are quite ﬂexible and need addi-
tional assumptions to be adopted. There may be different opinions on what the best way to treat the
candela is and how to modify SI for that. However, we have to acknowledge that in the current version
of the SI system [5] the candela, as an SI unit is much compromised and can be barely used as an SI
unit for any application. The physical quantities are deﬁned through physical laws and that means that
they are ‘deﬁned by Nature’.

4.3. Vacuum constant ǫ0 and Gaussian units

‘You can call it “nonsense” if you like’, she said, ‘but I’ve
heard nonsense, compared with which that would be as
sensible as dictionary!’

L.C.

Although the candela is the most questionable among the basic SI units, it has never been a sub-
ject of a world-wide discussion as the ampere and the Gaussian units have been. Obviously, that is
because of the signiﬁcance of electromagnetic phenomena in modern physics. There is no doubt that
the Gaussian units, in which ǫ0 = 1, are better for understanding of electrodynamics. However, there is
a number of units very well suited for some classes of phenomena (see, e.g., Sect. 8) and that does not
mean that these units are proper units for general purposes. In this short chapter I try to explain why
the units with ǫ0 = 1 have never been good for the general use.

First, we have to remind that the units are needed mainly to express results of measurements (done
or predicted). If these practical units are not good for theory, we may do calculations in more appropri-
ate units, but at the end we have to present the ﬁnal results in some practical units.

Secondly, we remark that there are some areas and, in particular, a ﬁeld of electrotechnical mea-
surements (of electric potential, current, resistance, inductance and capacity), where relative measure-
ments can be done much simpler and more accurately (in respect to absolute measurements of the same
values). Why is it so? The answer is simple: both the SI and Gaussian deﬁnitions of the basic electro-
magnetic units involve calculations of the magnetic or electric ﬁelds and building of a macroscopic bulk
setup with well controlled values of these ﬁelds. In other words, the absolute measurements deal with a
completely different kind of experiments. The absolute measurements correspond to electrodynamics,
while the relative measurements of quantities listed above are related to electrotechnics.

For this reason, nearly all electric measurements are realized as relative measurements done in spe-
cial ‘electrotechnical’ units. Separate experiments are performed in a limited number of metrological
laboratories to cross-check these units and to calibrate them properly in terms of SI. At earlier times,
the standards were built on classical objects. They were artiﬁcial and in this sense similar to the present
standard of mass. However, in contrast to a prototype of the kilogram, they were much more vulnerable.
There is a number of effects which may affect properties of classical objects and shift them. However,
it is much easier to ‘break’ an electric device than a weight. Thus, the electrical units evolved and their
calibration was not a simple procedure. They were quasi-independent. That produced a strong need to
= 1. A value of ǫ0
have an independent unit for electrical effects and to provide it one has to have ǫ0 6
has been ﬁxed within SI, but it was unknown in practical units and had to be measured.

Now, we have taken an advantage of the application of macroscopic quantum effects (see Sect. 5.4
and 9 for detail) and may be sure that the electrical units do not evolve, but still we need to calibrate

2005 NRC Canada

14

1
c2µ0

ǫ0 =

=

ǫ0 =

e2
4πα¯hc

,

them. While a value of ǫ0 is calculable in terms of SI units

unknown Vol. 99, 2005

107

2 F/m

4π

299 792 458
(cid:0)

(cid:1)
= 8.854 187 817...
×

−12 F/m

10

it is still unknown in practical units (such as, e.g., ohm-1990, Ω90, [12, 13]) and is a subject of mea-
surement. Maybe in future we will decide to reverse a situation accepting quantum deﬁnitions of ohm
and volt. That will upgrade today’s practical units Ω90 [12, 13] and V90 [14] up to the status of SI
units, but will make ǫ0 a measurable quantity and will substitute the prototype of the kilogram by an
electrical balance. In such a scenario the values of the Planck constant h and the elementary charge e
would be ﬁxed and, with a value of the speed of light already ﬁxed, one can see that

(10)

(11)

where the ﬁne structure constant α as a dimensionless constant has an unknown value, which is a
subject of measurement. Thus, ǫ0 becomes a measurable quantity certainly not equal to unity in any
sense. That is why we should not like to set a simple identity ǫ0 = 1 now.

There is one more issue about the constants of vacuum ǫ0 and the ﬁne structure constant α. We
may wonder whether the ﬁne structure constant is calculable or not. We cannot answer this question
now, however, there is a certain constraint on a scheme how α might be predicted. The most expected
scenario is that we would be able to predict α0 (a value of the ﬁne structure constant related to the
Planck scale) as a kind of a geometric factor (see Sect. 7, 10 and 13). In such a case, the electric
charge would not be a new independent property, but a kind of a derivative from mechanical properties
and should be, in principle, measured in mechanical units with ǫ0 = 1. However, if α has a value
chosen due to spontaneous breakdown of symmetry (see Sect. 10 and 11) it may be any. Perhaps, we
should treat the electric charge as a new independent quantity and measure it in separate units with a
dimensional value of ǫ0 as a consequence. A situation with the Coulomb (or Ampere) law is different
from, e.g., that with the Newtonian gravity. For the latter we have already known that the gravitational
charge (i.e., the gravitational mass), due to a deep physical reason (the equivalence principle), is not a
new property, but a derived property completely determined by the inertial mass. Indeed, the question
of calculability of α and thus of the origin of the electric charge will not be answered soon and, indeed,
such a general view would never affect any decision on units, because of practical importance of the
question. The practice of electrical measurements has obviously pointed to the proper choice.

4.4. ‘Unnecessary’ units, II

While discussing ‘necessary’ and ‘unnecessary’ units, we would like to mention a point important
for practical use. When we speak about most of phenomena, we often apply a ‘jargon’ dropping impor-
tant words. In such a case to understand anything properly, redundant information would be helpful.
For example, we often speak about a magnetic ﬁeld not clearly discerning magnetic induction B and
magnetic ﬁeld strength H (which are not simply related in media), or even about a ﬁeld not specifying
whether we mean magnetic or electric ﬁeld. In such a case, the use of different units is very helpful to
understand the practical situation. The same story is with units and their biological equivalents, which
from a theoretical point of view should be rather the same. However, naming the unit we immediately
explain which property of, e.g., radiation we have in mind: their energy or their effect on a human
body. A choice of a unit plays a role of a ﬂag allowing to drop a number of words. Use of four different
units for the electromagnetic ﬁeld (for E, D, B and H) makes theory less transparent and unnecessary

2005 NRC Canada

Savely Karshenboim

15

complicated, however, these four units may be helpful to describe an experiment in a much shorter
way.

5. Physical Phenomena Governed by Fundamental Constants

It’s as large as life and twice as natural!
L.C.

We mentioned above a quantum approach to standards of electrical units. They superseded classical
standards providing universal values which do not depend upon time or location of the measurement8.
It is also very useful that we can determine them from certain experiments not related to electricity.
That is because they are based on fundamental constants. However, fundamental constants, if they
are really fundamental in some sense, should show themselves only at a fundamental level, while any
particular measurements deal with objects and phenomena far from fundamental. How can we access
any fundamental quantity? The obvious answer is that we have to try to ﬁnd a property of a certain
non-fundamental object, which we can calculate. There are two general kinds of such objects.

•

•

First, we can study relatively simple objects, which properties can be calculated by us. The
simplest are particles, and only recently we learned how to study a single particle in a trap. In
earlier times we dealt with beams and clouds of interacting particles trying to eliminate their
interactions. The next in the row of simple objects are simple atoms and simple molecules.

Another option are macroscopic quantum effects, such as, e.g., the Josephson effect. Once we
realized proper conditions, we can see the same result for various samples and the result is simply
expressed in terms of fundamental constants. An important feature of this kind of effects is that
when conditions are not perfect, they often make the effect harder or even impossible to observe,
but seldom affect the basic parameters of the effect. So, certain quantities coming from this kind
of effects are quite immune to the conditions of the experiment.

Before we discuss any application of properties of elementary and compound particles, let us un-
derline that an important detail for the interpretation of such measurements is that particle properties
are the same for each species. A measurement may be even of classical nature aiming to determine the
Avogadro or Faraday constant, however, the output is very different in classical and quantum frame-
work. For example, the Faraday constant

F = e NA

(12)

in the classical consideration is deﬁned for an average charge carried by the Avogadro number of
electrons (correcting for the sign of the charge) or single-charged ions, while NA is in turn an average
number of carbon atoms needed to form 12 gram of a carbon material. In the quantum case, we know
that carbon atoms or electrons are the same and we can drop a word ‘average’. In the quantum case the
electron charge e is certainly a fundamental constant, while from a classical point of view it is about
the same as an average mass of dust particles or rain drops.

Classical physics is unable to deal with identical objects. What does ‘identical’ mean? If there is
no interference, we can always distinguish between two electrons. From the point of view of classical
statistics ‘identical’ means ‘different recognizable objects for which in a particular consideration we
do not care which is which9’. But if we did care, we could always recognize them. If two electrons
have approximately the same mass and charge, classical mechanics cannot check if they are the same

8 We discuss possible time- and space variations of fundamental constants in Sect 11 and 12.
9 Historically, the statistical analysis appeared in an attempt to describe social phenomena dealing with people. Deﬁnitely, that

is just a case when the objects are clearly distinguishable.

2005 NRC Canada

16

unknown Vol. 99, 2005

exactly or approximately, because there is always an experimental uncertainty. It may be in principle
reduced to any level but never removed completely.

Quantum physics introduces interference between particles. Physics of two slightly different elec-
trons and two identical electrons is by far not the same. And what is very important, we do not need
to perform any interferometric experiments. They have already been performed for us by Nature. The
Pauli principle governs the atomic levels and nuclear shells. All electrons are identical as well as all
protons and neutrons. The very existence of lasers showed the identity of all photons as particles.

Identity of objects of the same class makes their properties to be natural constants. However, if we

like them to be really fundamental from a theoretical point of view, we have to study simple objects.

5.1. Free particles

Studying free particles offers relatively limited access to the fundamental constants. We can mea-
sure their masses and magnetic moments. Their electric dipole moments have been searched for (be-
cause of different supersymmetrical theories) but have not yet been detected. Their charge is known in
relative units and seems to be trivial. Sometimes, but very seldom, they have calculable properties. Two
of the most important of them are related to the anomalous magnetic moment of electron and muon.

Brieﬂy speaking, if we like to learn something beyond basic properties (such as mass) of an object,
we have to study interactions. An interaction with a classical ﬁeld is not a good case because we can
hardly provide conﬁgurations of the classical electromagnetic ﬁeld controlled with a high accuracy.
Only one kind of classical ﬁelds is suited for precision measurements, namely, a homogeneous mag-
netic ﬁeld with a ﬁxed, but unknown (in the units of SI) strength. That allows us to compare masses and
magnetic moments. A quantum interaction is under much better control, because its strength is con-
trolled by Nature and not by us. Measuring the mass or the magnetic moment, one determines certain
fundamental parameters directly, while dealing with a calculable interaction and calculable properties,
we access fundamental constants indirectly. A quantum electrodynamical self-interaction allows us to
present the anomalous magnetic moment of an electron ae in terms of the ﬁne structure constant with
a high accuracy [15]

ae =

0.328 478 696

+ 1.181 241

1.737(39)

+ . . .

2

α
π (cid:17)

(cid:16)

3

α
π (cid:17)

(cid:16)

−

4

α
π (cid:17)

(cid:16)

1
2

α
π −

+ near negligible eﬀects of weak and strong interactions .

A measurement of ae [16]

ae = 1.159 652 188 3(42)

−3

10

[3.2

−9]

10

×

×

delivered the most accurate value of α

α

−1
g−2 = 137.035 998 80(52)

−9] .

10

[3.4

×

In Table 1 we list fundamental constants which may be obtained studying particles. An example of
a non-elementary particle is the deuteron. Measuring its binding energy and masses of proton and
deuteron, one can obtain the neutron mass (see Sect. 5.3).

5.2. Simple atoms and molecules

From the very beginning of studies of classical effects, we distinguished kinematics (i.e., a theory
of particle motion at a given force or a given potential ﬁeld) and dynamics (i.e., a theory of forces:
ﬁelds, their sources and their interactions with particles). Classical theory of Maxwell equations is a
dynamics of charged particles and kinematics of photons. Quantum mechanics introduced quantized
properties and identical objects. That, additionally to dynamics and kinematics, opens an opportunity
for a prediction of structure of certain objects. Indeed, classical physics could successfully consider

2005 NRC Canada

(13)

(14)

(15)

Savely Karshenboim

17

Particle
Electron

Muon

Neutron
Deuteron
Caesium

Constant
me
α
aµ
µµ/µp
α
mn
α

Comment
via comparison to proton
via QED
via comparison to proton
via comparison to proton
via comparison to lattice spacing and R∞
via a measurement of binding energy and comparison to proton
via atomic interferometry and Raman scattering

Table 1. Fundamental constants determined through properties of elementary and compound particles

compound objects, which consist of ‘simple’ constituents such as the Solar system formed by the Sun
and the planets. However, there has been no chance for any ab initio calculation, because physics deals
only with particular objects and in classical physics any particular object is a peculiar object: the pa-
rameters are peculiar and the initial conditions are peculiar. Quantum physics introduced fundamental
particles: a few kinds of particles which form everything in our World. We still need to know some their
parameters, but with these parameters determined we can try to calculate everything. We can also re-
verse the problem: predicting the structure properties in terms of fundamental parameters of constituent
particles and determining the properties experimentally, we deduce actual values of the parameters.

If we study a few-particle system, we have a better chance to study interactions of its constituents.
However, different simple atoms are treated within QED in very different ways. The most clear case
is the lepton quantum electrodynamics (electron and muon are the most important leptons in QED). It
needs very few input data: the elementary charge and the lepton masses. Any other property (e.g., the
magnetic moment) can be in principle derived from these few.

Indeed, the proton cannot be treated this way. Generally speaking, QED is a theory of electromag-
netic interactions of leptons, photons and ‘external’ sources. A proton is such a source and we need
a number of parameters to describe it: its charge qp, magnetic moment µp, details of its charge and
magnetic moment distribution as well as more sophisticated parameters. What we only know is that
this approach is consistent — we can measure these parameters and they are the same for very different
kinds of experiments.

Theory of simple atoms involves a few dimensionless parameters (which actually are small param-

eters for a number of important applications and allow various perturbative expansions):

the ﬁne structure constant α, the exponential factor of which indicates how many QED loops are
taken into account;

the Coulomb strength Zα, with the nuclear charge Z changing in a very broad range from Z = 1
to Z

90;

≃

10−3/2A (A is
the mass ratio of the orbiting particle to the nucleus, which is me/Amp ≃
the nuclear mass number) for a conventional (‘electronic’) atom; me/mµ ≃
1/207 for muo-
nium (the nucleus is a positive muon); unity for positronium (the nucleus is a positron); and
mµ/Amp ≃
various parameters related to the nuclear structure.

1/9A for a muonic atom;

•

•

•

•

It is clear that an exact calculation with all these parameters is not possible. Some calculations for
conventional atoms are exact in Zα, while positronium calculations apparently must be done exactly
in the electron-to-nucleus mass ratio. Still, an expansion in other small parameters has to be applied
and an accurate theory is possible not for any simple atoms. A proper estimation of uncalculated terms

2005 NRC Canada

18

unknown Vol. 99, 2005

is sometimes a difﬁcult problem [17]. Various details of theoretical calculations can be found in review
[18].

In light atoms, the perturbative approach is dominant and to demonstrate how far we can go with
theoretical predictions, we summarize in Table 2 crucial (for a comparison with experiment) orders of
QED theory for energy levels in various two-body atoms. We remind that the leading non-relativistic
binding energy is of order of (Zα)2mec2.

Value

Hydrogen, deuterium (gross structure)
Hydrogen, deuterium (ﬁne structure)
Hydrogen, deuterium (Lamb shift)
3He+ ion (2s HFS)

4He+ ion (Lamb shift)
N6+ ion (ﬁne structure)
Muonium (1s HFS)

Positronium (1s HFS)
Positronium (gross structure)
Positronium (ﬁne structure)

Order
[in units of (Zα)2mec2]
α(Zα)5, α2(Zα)4
α(Zα)5, α2(Zα)4
α(Zα)5, α2(Zα)4

α(Zα)5me/M , α(Zα)4m2
α2(Zα)4me/M , (Zα)5m2

e/M 2,
e/M 2

(Zα)5m2

e/M 2,

α(Zα)5, α2(Zα)4
α(Zα)5, α2(Zα)4

e/M 2, α(Zα)4m2
α(Zα)5me/M
α5
α5
α5

Table 2. Crucial (for a comparison of QED theory and experiment) orders of magnitude for corrections to the
energy levels in units of (Zα)2mec2 (see [17] for detail). Here: M stands for the nuclear mass.

Various simple atoms and certain simple molecules can deliver us much more information than free
particles, because we are able to express their properties in terms of such fundamental constants as the
Rydberg constant R∞, the ﬁne structure constant α, various masses (me, mp, mµ, mπ etc.), magnetic
moments (µp, µd, µµ etc.) and some other constants. Working with atoms and molecules we can apply
various spectroscopic methods, which are the most accurate at the moment.

Simple molecules are much more complicated than simple atoms and their use is rather limited.
For example, studies of the hydrogen deuteride (HD) provide us with the most accurate value of µp/µd
[19]. A summary on use of simple atoms and molecules to determine precision values of various
fundamental constants is given in Table 3. More details on simple atoms can be found in [20, 21],
while a popular history of applications of hydrogen to fundamental problems is presented in [22].

5.3. Free compound particles

Free particles, which we can study, are not necessarily elementary particles. We can treat nuclei,
atoms and molecules as compound particles and study their simplest properties (such as the mass or
the magnetic moment). We can also rely on conservation laws. For example, the best value of neutron
mass comes from deuteron studies. The deuteron mass [23]

md = 2.013 553 212 70(35) u

[1.7

−10]

10

×

combined with an accurate value of its binding energy Ed of approximately 2.2 Mev [24] and the
proton mass [25]

mp = 1.007 276 466 89(14) u [1.4

−10] ,

10

×

(16)

(17)

2005 NRC Canada

Savely Karshenboim

19

System
Muonium

Hydrogen

Deuterium

Constant
α
mµ/me
µµ/µp
R∞
µp/µe
R∞
µd/µe
α

Comment
via bound state QED
via bound state QED
via bound state QED and comparison to proton
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED
via bound state QED

Helium
Hydrogen-like carbon me/mp
Hydrogen-like oxygen me/mp
Muonic atoms
mµ/me
Pionic atoms
mπ/me
HD molecule
µd/µp
HT molecule
µt/µp

Table 3. Fundamental constants determined through simple atomic and molecular systems

provides us with such a possibility [23, 2]10
mp −
mn = md −

Ed/c2

= 1.008 664 915 60(55) u [5.5

−10] .

10

×

Another example of an application of compound particles: determination of the ﬁne structure con-
stant α via scattering of photons at the caesium atom and measuring their recoil shift [26]. In this
experiment, one should deal with absorption and stimulated emission, however, dynamical details of
the interaction of the photons and the atom are unimportant. Once we know the direction and frequency
of the photons, we need to know only about the atom as a whole: its energy and momentum. If we treat
atom in such a way as a compound particle, it is ‘simple’ in a sense.

We remind that properties of certain atoms and molecules play a crucial role in the deﬁnition of SI
units: hertz is deﬁned via the hyperﬁne interval in the caesium-133 atom and kelvin temperature via
the triple point of the water. The universal atomic mass unit (a non-SI unit acceptable along with SI)
and the mole are deﬁned via the mass of the carbon-12 atom. However, considering atomic properties
as units, we indeed should not care if they may be calculated.

5.4. Macroscopic quantum phenomena

Macroscopic quantum phenomena offer us certain properties which may be presented in terms of
fundamental quantities. For example, the Meissner effect provides us with a quantized magnetic ﬁeld.
The magnetic ﬂux through a superconductive loop can take only very speciﬁc values such as

(18)

(19)

(20)

Φn = nΦ0 ,

where

Φ0 =

h
2e

the magnetic ﬂux quantum and n is an integer number. As we mentioned, the most important for
applications is to consider macroscopic quantum phenomena which are related to quantized values of

10 This value of the neutron mass also involves data similarly related to other, more complicated, nuclei [23]. Note that the data
on the binding energy may differ from the originally published results because of the recalibration of the lattice parameter.

2005 NRC Canada

20

unknown Vol. 99, 2005

electrotechnical properties. Two such effects play crucial role for practical use. These are the quantum
Hall effect and the Josephson effect. The former offers us a quantized value of the resistance

proportional to the von Klitzing constant

while the latter allows to quantize the voltage related to some frequency ν:

Rn =

RK
n

,

RK =

h
e2 ,

n′
KJ ·

ν ,

Un′ =

where

KJ =

2e
h

(21)

(22)

(23)

(24)

is the Josephson constant and n and n′ are certain integer numbers.

We note that a direct measurement of these constants in SI units is very complicated, and consider

different approaches to the application of these two quantized phenomena in Sect. 9.

5.5. Atomistics and discrete classical phenomena

As already mentioned, even certain classical constants such as the Avogadro and Faraday constant
have quantum origin. First, they originate from the atomic nature of substance, which is a consequence
of quantum mechanics, and, secondly, they receive their meaning because of identity of species of the
same kind.

We collect in Table 4 the most accurately known fundamental constants [2] which have been ob-

tained through studies of calculable objects.

Constant
R∞
mp
mp/me
ae
α−1
µp/µB
mµ/me
µµ/µp
e
F
µp
h
NA
aµ

CODATA 2002 values

10 973 731.568 525(73) m−1

[6.6 × 10−12]

1.007 276 466 88(13) u [1.3 × 10−10]
1 836.152 672 61(85) [4.6 × 10−10]

1.159 652 185 9(38) × 10−3
137.035 999 11(46)

[3.2 × 10−9]

[3.3 × 10−9]

1.521 032 206(15) × 10−3 [1.0 × 10−8]

206.768 283 8(54)
−3.183 345 118(89)

[2.6 × 10−8]
[2.6 × 10−8]

1.602 176 53(14) × 10−19 C [8.5 × 10−8]

96.485 383 3(83) × 1023 C/mol
[8.6 × 10−8]
1.410 606 71(12) × 10−26 J/T [8.7 × 10−8]

6.626 069 3(11) × 10−34 J s
6.022 141 5(10) × 1023 mol−1
1.165 919 81(62) × 10−3

[1.7 × 10−7]
[1.7 × 10−7]

[5.3 × 10−7]

Method
bound state QED
free particles
bound state QED, free particles
QED, free particles
QED, free particles, bound state QED, MQE
bound state QED
bound state QED
bound state QED
MQE, ADN
MQE, ADN
free particles, MQE, ADN
MQE, ADN
MQE, ADN
free particles, QED

Table 4. CODATA 2002 recommended values of fundamental constants [2] and methods applied to achieve their
values. Here: MQE stands for macroscopic quantum effects, ADN for effects due to atomic discrete nature of
substance

2005 NRC Canada

Savely Karshenboim

21

6. Fundamental Constants and Renormalization:

Operational philosophy of physics

And she tried to fancy what the ﬂame of a candle is like
after the candle is blown out, for she could not remember
ever having seen such a thing.
L.C.

Quantum mechanics appeared after a brilliant success of relativity and indeed it was understood that
a non-relativistic quantum theory should be extended to the relativistic case. However, the development
met a problem that perturbative calculations have involved certain divergencies. The problem has been
solved by the introduction of a procedure of renormalization.

The solution of this problem has a philosophical side and before addressing the problem, let us
discuss some philosophical aspects of physics and, ﬁrst of all, answer a question what objectives of
physics are. Let us do that pragmatically. We do not discuss what various sciences pretend to aim,
we like to check what they really do. Both philosophy and physics pretend to understand Nature.
Philosophy picks up the most signiﬁcant questions of the very existence of Nature, however, it does not
care if we have enough data to answer them. And actually, similar to the truly fundamental constants,
the fundamentality never shows itself for measurements. Physics also pretends to understand Nature,
however, in reality it does not care what Nature, matter or any particular object such as a photon and
an electron are. Physics questions not what various objects are, but how they interact to each other.
It studies not what Nature is, but how it operates. We, physicists, certainly believe that something
really exists in an ‘absolute’ sense since the same experiments produce the same results. However, we
cannot say that anything particular exists until we measure it. It is close to the positivistic philosophy.
However, that is not a philosophy of physicists, but a kind of modus operandi in physics. This kind of
double standards is often met in everyday personal and professional life: there is a philosophy, which
provides us with a general view on events and there is an operational scheme, which determines our
reaction to the events. The philosophical views of physicists on Nature may be very different from
each other, while their professional operational scheme is nearly the same for anybody. This scheme is
based on a kind of a ‘short-range’ philosophy. I call the philosophy beyond the operational scheme as
‘operational philosophy’.

Indeed, we may say that matter exists. Or that an electron exists. But that gives us no real piece
of information at all. If we could say that a certain particle with speciﬁc properties exists, that would
contain certain information, and could be correct or not. But to learn that we have to perform an
experiment.

A philosophical breakthrough of special relativity was the idea that the simultaneous events are
unmeasurable. Quantum mechanics said that trajectory is unmeasurable. That we cannot distinguish
between two identical particles. That we cannot measure certain properties simultaneously. That we
cannot do ‘exact’ measurements without certain consequences. After we had learned that, we changed
our view on what exists and what does not.

Non-relativistic quantum mechanics succeeded with a perturbative approach. We start with an un-
perturbed equation with unperturbed parameters and introduce various small perturbations which shift
properties of the result. In quantum mechanics we are able ‘to switch off’ most of perturbations for
real quantum mechanical problems or at least vary their parameters. On the contrary, in quantum elec-
trodynamics (QED) we cannot turn off the self-interaction. It is proportional to a small parameter
1/137, but because of the divergencies the perturbative correction is not small. It cannot be even
α
calculated properly because it involves physics of high momenta. QED said, that since the unperturbed
‘bare’ parameters (such as the electron mass m0 and charge e0) are not measurable, we should not care
if they are ﬁnite or divergent, well-determined or model-dependent. In a sense they do not exist since
they are certain abstract results of our imagination. What we have to care about are only measurable
quantities, i.e. ‘dressed’ (perturbed) parameters. We are able to express measurable energy shifts in

∼

2005 NRC Canada

22

unknown Vol. 99, 2005

terms of the measurable electron mass m and charge e without any divergencies and any needs for
knowledge of physics at the high momentum scale. In this kind of expression one measurable quantity
in terms of others means a QED calculation, a successful QED calculation.

All these examples follow the idea of some equality between the very existence of a quantity and
the possibility for a measurement of its value. This approach is a backbone of physics, its operational
philosophy.

It ﬁnds its realization in the approach of effective potentials, which are used for various problems
in particle physics. It may be an effective phenomenological potential for pions, or an effective quan-
tum ﬁeld theory produced on a way of going down to our energy from the Planck scale or from the
supersymmetry scale. The story is that we believe that for various reasons the fundamental physics is
determined at certain much shorter distances and higher energies and momenta than the ones we deal
with in our experiments. Dealing with low energies, we can see only a certain effective theory. That is
not a true fundamental theory but that all what we have in an experimental sense. We have to be suc-
cessful, otherwise physics would have no sense until we reach the fundamental scale of distances and
energies. We trust that it is enough to determine some parameters at our low energies and any further
calculations can be performed in their terms. In other words, we expect that low-energy physics is com-
plete (in a sense that parameters determined at low energy are enough for the low-energy calculations)
and consistent. If that is not correct, we should interpret that as existence of something unmeasurable
that affects our world in an unpredictable way.

7. On Calculable Physical Constants

There was nothing so very remarkable in that; ... it occurred to
her that she ought to have wondered in this, but at the time it all
seemed quite natural.

L.C.

If we look at the list of the recommended values of the fundamental constants [2], it is unlikely to
ﬁnd there any constant which can be calculated exactly ab initio. We can then assume that there is no
calculable constant which has a practical sense. We have already mentioned in Sect. 1, that consider-
ing the hydrogen atom we either should deal with a calculable quantity (R∞), or with a measurable
one (νH(1s
2s)). If a spectral property can be measured directly, it cannot be calculated ab initio
exactly. We may then conclude that there is no constant which is both exactly calculable and directly
measurable.

−

We note, however, that a reason for this conclusion comes in part from psychology. Let us give an
example of a similar situation. It is well known that a theory of a point-like particle with a non-zero
anomalous magnetic moment is inconsistent. Meanwhile, we believe, that the electron, being point-
like, still possesses an anomalous magnetic moment and its theory is consistent. The only inconsistency
here is in the terminology. We say “the electron has an anomalous magnetic moment”, because it
may be directly measured and because it was ﬁrst measured and next understood theoretically. We
say “the electron is a point-like particle”, because its structure cannot be actually measured in any
straightforward way and because it was ﬁrst calculated and next certain previously measured effects
(such as the Lamb shift in hydrogen atom and helium ion) were understood as consequences of the
internal structure of the electron. From the theoretical point of view the same effects are responsible
for the anomalous magnetic moment of the electron and its internal structure and in a sense we can
speak either about a point-like electron with g = 2 or about an electron which has both the anomalous
moment and the structure. But for historical and psychological reasons we have chosen another way to
express the situation.

A similar problem in terminology is for the calculability of the constants. We know that, e.g., an
internal angular momentum of Earth and Moon could take an arbitrary values and their ratio is a kind of
constant to characterize our Earth–Moon system. If the internal angular moment (spin) were measured

2005 NRC Canada

Savely Karshenboim

23

for quantum objects (such as electrons or atoms) before the appearance of quantum mechanics (still it
is hard to imagine how), we could be surprised that Se/Sp = 1. Quantum mechanics would explain this
constant. However, in reality, ﬁrst, a quantum theory of the angular momentum was created and next
we measured the spin (or rather interpreted some results as a determination of the spin of an electron
and a proton). In time of quantum mechanics the identity Se/Sp = 1 is trivial, and now we do not
consider the ratio of the spins as a fundamental constant.

Another example is the famous Einstein’s identity E0 = mc2. This equation appeared as a result
of special relativity and was ﬁrst seen experimentally through a relativistic correction to the kinetic
energy. There was no way to measure it directly. Now, we can measure the binding energy EB (of
nuclei, such as the deuteron, or even of atoms — see, e.g., [28]) and check whether the mass of a
bound system is the same as a sum of the masses of its composites. And we indeed know and can
now verify experimentally that the mass is reduced by a value of EB/c2. We study the mass and the
binding energy as static properties and do not need to perform any relativistic experiment to check
E0 = mc2. Another possibility to reach E0 = mc2 without any relativistic experiments is to measure
annihilation energy of positronium. The energy is determined as the energy of two gamma-quanta and
the positronium mass is twice the electron mass (with a correction due to the atomic binding energy).
mc2 and
If that was measured before the Einstein’s relativity theory, we would write it as E0 = k1 ·
interpret the theory as a calculation of k1 = 1.

One more example is a comparison of properties of a particle and its antiparticle (like, e.g., their
charges, masses etc.). That is a result of the CPT invariance which is a consequence of the Lorentz
invariance. In early time, even their existence was ﬁrst proved theoretically and next discovered exper-
imentally. We may say that we are able to calculate the electron-to-positron mass ratio and it should be
unity.

As we see from the examples above, very often a question of calculability of a constant is related
to history and psychology: we should ﬁrst recognize a certain property as a constant of Nature and
next calculate it. Generally speaking, the most fundamental constants such as the speed of light c or
the Planck constant h enter a great number of very different equations. If any of these equations were
discovered before the Einstein’s relativity and quantum mechanics, we should introduce a number
of constants c1, c2... and h1, h2... instead of two basic constants c and h. Reducing the numerous
coefﬁcients in different equations to these two, we, in a sense, calculate these constants stating c1 =
c2 = ... = c and h1 = h2 = ... = h (as it is discussed above for k1 = 1). And actually, that is one of
the most likely situations in future for exactly calculable constants.

Perhaps, the most important example of a similar situation is related to the ‘elementary electric
charge’. We accept for practical applications that the absolute value of the electron and proton charges
are the same. For example, the CODATA adjustment [2] does not distinguish between the proton charge
and the positron charge. However, no theory, conﬁrmed by the experiment, implies that. Conservation
of the electric charge only urges that

qe + qp = qn + qν .

(25)

In other words a small disbalance of the electron and proton charges is permitted if the neutron and/or
the neutrino possesses a small electric charge. In earlier time we believed that the neutrino was massless
and thus should be neutral since a massless charged particle would cause certain problems in conven-
tional QED. Now we have learned that the neutrino has a non-zero mass, but it is suspected that this
is the so-called Majorana mass, which also implies neutrality of the neutrino. However, we can say
nothing about the neutron (from a theoretical point of view). Meantime, from experiment we know that
[27]

qe + qp|
|
qp

1.0

≤

×

−21 .

10

(26)

2005 NRC Canada

24

unknown Vol. 99, 2005

/qp ≥

qe + qp|
|

qe + qp|
|

The various limitations on
involve certain assumptions and we have to be very careful with
the results. However, the orders of magnitudes are quite clear. Brieﬂy speaking, when we consider an
interaction of two hydrogen atoms at a long (in a macroscopic sense) distance, the gravitational inter-
action is 37 orders of magnitude weaker than the electromagnetic Coulomb interaction of two protons.
10−18, the electromagnetic H-H interaction would dominate over
That means that, if
the gravity. We know that the interaction of ‘neutral’ bulks of substance apparently is the Newtonian’s
gravitation. If we suggest for simplicity that qν = 0 (what is probably true), then a small value of
qe + qp = qn would effectively produce an 1/r-force coupled to the baryon charge of the bulk matter.
We know (from various tests of the equivalence principle) that this force (if any) is substantially weaker
than Newtonian’s gravity. And that sets a limit on the residual electric charge of the ‘neutral’ hydrogen
atom (and of the neutron) at the level of few orders of magnitude below 10−18qp. We note that the
limitation in (26) is only approximately three orders of magnitude stronger than the limit of 10−18qp
related to the dominance of the gravity in the interaction of the ‘neutral’ particles. That is because
of two reasons: ﬁrst, the measurements are related to the coupling constant, which is proportional to
(qe +qp)2, and secondly, the mass itself is approximately proportional to the baryon charge. Only small
corrections, due to a difference (mp + me)
mn and a nuclear binding energy, violate the equation
Matom = Amn. That considerably weakens use of the equivalence principle.

−

If we believe in a certain uniﬁcation theory (such as, e.g., SO(10)), we can derive

qe + qp = 0 .

(27)

So considering different uniﬁcation theories, we are approaching a calculation of qe + qp, but it is likely
that, once we succeed, we will (for psychological reasons) say again “that is not a calculation since it
is a trivial consequence of the uniﬁcation theory.”

Let us return to the Rydberg constant. Have we calculated anything real? Or did we just give a
special name to a certain experimentally meaningless combination of e, c, h and me? To answer this
question we need to consider one more approach for the ab initio calculation of properties in terms
of the fundamental constants: an approximate calculation. Such a calculation is quite important for
applications since we apply a perturbative approach to numerous problems. Historically, the Rydberg
constant was introduced to describe certain hydrogen energy levels (the Balmer series) and this con-
stant was later calculated. However, with a substantial increase of accuracy of theory and experiment
we arrived at a point when a choice had to be made: to deal with a measured quantity or to introduce
a special value which would be used in perturbative calculations. So, at the present time, the constant
itself is not a real property of any atom, and we can say that we gave a special name to a speciﬁc
combination of the more fundamental values (the Rydberg constant). However, we can say that there
is a constant which describes (without any corrections made) the hydrogen and deuterium energy lev-
els with an uncertainty below a part in 103 and this constant has been calculated. We may introduce
certain corrections and bridge the Rydberg constant and the hydrogen energy levels with much higher
accuracy. But the signiﬁcance of the application of Rydberg constants is ﬁrst of all that this constant
approximately describes a number of transitions and that is an important success of the ab initio calcu-
lations.

A signiﬁcance of this constant is also that it describes the order of magnitude for any gross-structure
transition in any neutral atom and molecule (see Sect. 12.2). A constant characterizing an effect in
general is also an important and non-trivial result. That is one more facet of the calculability of nat-
ural constants. A famous example of such a calculation of order of magnitude is a consideration by
Schr¨odinger of the size of atoms and the life cells [29]. He tried to answer a question, why the atoms
are so small. Indeed, that is rather a question why we are so big. Sch¨odinger considered some reasons
for that. This consideration has also been important to understand the numerical values of the atomic
constants, since the practical units and, in particular, the SI units are deﬁned in such a way that anything
related to a human being should be in a sense of order of unity.

2005 NRC Canada

Savely Karshenboim

25

Other examples: a prediction of the order of magnitude of the electrical voltage which arose from
molecular and atomic phenomena: it is a few volts — that takes its origin from early studies of similar
phenomena and from the mentioned above fact that all molecular and atomic energy levels in the neutral
atoms have the same order of magnitude (related to the Rydberg energy which is approximately 13 eV).
Actually, because of that, the volt is only a natural unit if we mean its order of magnitude.

Ab initio calculations in the leading order, or even a rough approximation, may be also useful when
one looks for a possible time variation of the fundamental constants. In such a case it is necessary to be
able to perform a calculation of the dependence of energy levels on the fundamental constants rather
than the energy levels themselves. We discuss this issue in Sect. 12.

8. Natural Units

And it certainly did seem a little provoking (‘almost as if it
happened on purpose,’ she thought).

L.C.

How many units and standards do we need? Theoretically, we need only the base units of SI, and
even not all of them. In particular, we can reproduce the metre through the second and the ampere
through the kilogram, the metre and the second. However, practically, we need a lot. A measurement is
a comparison, and the most fortunate case is a comparison of a quantity under question with a ‘probe
quantity’ of the same kind. However, when we do different measurements of ‘the same’ kind of quanti-
ties such as, e.g., the distances, we notice a big range of their values. Astronomical and atomic distances
are related to not quite ‘the same’ kind. And indeed, for obvious practical reasons, we measure them
quite differently and like to apply different ‘probe quantities’, i.e., different units.

Indeed, these units cannot be independent and we need to properly calibrate them. However, the
calibration is not always important, because quite seldom we are really interested in a comparison of,
e.g., the mass of a hydrogen atom and the mass of the Sun. And because of that we can leave their
units, which must be related in a formal sense, to be practically unrelated.

Still, for a number of measurements a proper calibration is needed. In the case of classical phe-
nomena we have to perform this calibration regularly, to take care that the unit is unchanged during the
experiment etc. We should also take care that the units in different laboratories are properly compared.
However, quantum physics opens another option. It offers quantum natural units which are stable and
universal.

Actually every dimensional fundamental constant is a kind of a natural unit [7], and a substantial
part of dimensionless constants (and certain dimensional constants such as the Boltzmann constant k)
can be considered as conversion factors11 (see [3]). So, there is a big variety of various natural units
and even natural systems of units. Two kinds of natural systems have been used.

•

•

First, a system can have natural units for any dimensions, such as the atomic units and the Planck
units. Indeed, some systems are incomplete because they do not care about all phenomena. But
every unit, which is really needed for a description of a certain class of phenomena, may be or
may be not related to the fundamental constants. In the ﬁrst kind of the natural units all necessary
units are related.

Secondly, a system can apply natural constants together with other units. In such a case, the
natural parameters or the fundamental constants rather set a certain constraint on units, such as
in the case of systems in which ¯h = c = 1 (relativistic units) or ǫ0 = 1 (e.g., Gaussian units).

Are natural units (or a natural system of units) a good choice? In their ‘complete form’ they are
as good as any other units. However, in physics we widely use various ‘jargons’ [7]. We can indeed

11 We do not mean that these constants are just the conversion factors and nothing else.

2005 NRC Canada

26

unknown Vol. 99, 2005

measure similar (but not the same) quantities in the same units, such as a measurement both of the time
intervals and the distances in the seconds (or or both in the metres). However, we know these are very
different quantities which are measured differently. That means that saying c = 1 we use a jargon, and
in reality, we mean something like c is equal to one light year per year or so. A jargon, as a special kind
of language, differs from a normal language being designed for a special use only. In this special use
(for a special kind of phenomena) it offers a more short and clear description. Meantime, often the very
use of the ‘words’ differs from the normal use and the jargon sentences are ‘wrong’ or meaningless
literary. The same in physics. We like to measure frequency, energy, momentum and mass in different
units in a general case. They are closely related in the case of relativistic quantum physics. However,
in the general case they correspond to very different properties and assume different experimental
techniques to deal with them. They also suggest different modiﬁcations for applications to continuous
media (which is rather unimportant for fundamental physics, but signiﬁcant for experiment). The prac-
tice of the jargons often deal with numerous hidden substitutions for quantities (confusingly keeping
their names) such as

t

m

→

→

x0 = ct
E0 = mc2 .

Such hidden substitutions, which are equivalent to a use of the same units for different quantities (as
the energy units for the mass), would be misleading and not very helpful in a general case because of
destroying advantages of the dimensional analysis method. We like to distinguish the distinguishable
quantities. However, in quantum relativistic physics, where constants c and h may appear in any equa-
tions, we can hardly use the dimensional analysis and it is worth to present all of these quantities in,
e.g., energy units without losing even a bit of information.

Atomic units

ma.u. = me = 9.109 ...
ea.u. = e = 1.602 ...

−31 kg
10
−19 C ,

×
10

×
= 5.291 ...

¯h
αmec

la.u. = a0 =

×
Ea.u. = Eh = α2mec2 = 4.359 ...
×
−17 s ,

= 2.418 ...

ta.u. =

10

−11 m ,

10

−18 J ,

10

¯h
Eh

×

(28)

(29)

which present a very natural, physical and logical coherent system of units, are very well adjusted to
atomic and molecular phenomena and most of quantities there are of the order of unity. However, those
units indeed are not convenient for other phenomena. These units present a case of the ‘theoretically’
natural units. We choose them to simplify theory (see, e.g., Sect. 12.2). The other kind of natural units
are ‘practical’ natural units. Choosing them, we do not care too much about their fundamentality. Our
concern is our ability to apply them. Examples of the ‘practically’ fundamental units are the caesium
HFS interval, the carbon atomic mass, the Bohr and nuclear magnetons, the masses and the magnetic
moments of an electron and a proton, constants of von Klitzing and Josephson. We partly consider a
question of the ‘practically’ fundamental units in the next section (Sect. 9).

A choice on units, we do in physics, is quite simple. We use various ‘theoretically’ natural units
when we do calculations. Some of them are very helpful also for education. The more complicated are
the calculation, the more useful are the related natural units. However, once we refer to a quantity to
be measured, we switch to ‘general’ (SI) or natural ‘practical’ units.

2005 NRC Canada

Savely Karshenboim

27

9. Deﬁnitions and Mise en Pratique for the SI Units:

A back door for natural units

‘When I use a word,’ Humpty Dumpty said in rather a scornful
tone, ‘it means just what I choose it to mean — neither more nor
less.’

L.C.

Currently, practical recommendations issued by CIPM for the most important units [5] such as the

metre [11], the ohm [12, 13] and the volt [14] are based on certain natural units.

Why do we need such recommendations? A problem is that the original SI deﬁnitions [5] cannot be
used in an easy way. As we mentioned, the idea of the units comes once from the fact that a measure-
ment is a comparison and to compare the results of different measurements we need to go through a
chain of comparisons. The introduction of the units means that an essential (and the ‘universal’) part of
the comparisons is separated from the rest and recognized in a very speciﬁc way. It is a responsibility
of metrological institutions around the world to take care about the standards and the units. The output
of this work should be a certain set of quantities, convenient for a further use. Unfortunately, the SI
deﬁnition of certain units is not suited for that. The practical recommendations are designed to cover
the gap between the rigorous SI deﬁnitions and practical accessability by a relatively broad range of
users. However, the recommendations are not a part of SI in a sense: they aim to arrange additional
conventional units and to simplify a measurement in the SI units as long as the users agree with a
reduced accuracy.

Let us give an example of such a recommendation. As we mention in Sect. 5.4, certain macroscopic
quantum effects (the quantum Hall effect and the Josephson effect) may be very helpful to establish
natural units of the resistance and the electric potential. For that one has to know values of the funda-
mental constants RK (the von Klitzing constant) and KJ (the Josephson constant) in the units of SI.
That is not a simple issue and three basic strategies may be applied to take advantage of these effects.
Scenario # 1 suggests, that we ﬁx values of these two constants. Since that is not possible for the

units of SI, that means an introduction of certain conventional units, in which

RK = 25 812.807 Ω90
KJ = 483 597.9 GHz/V90

(exactly)

[12] ,

(exactly)

[14] .

They are not the SI units and that means that if we check the Ampere law with magnetic constant

µ0 = 4π

×

−7 N/A2

10

we should fail. We would also arrive at a discrepancy in the energy measurements since

s .

= J

V2
90/Ω90 6
But for most of applications these mismatches with the SI system are not important and the units
volt-90 and ohm-90 are sufﬁcient for most of measurements, however, not for all.

(32)

·

If we really need to deal with the SI units, we should do something else. Scenario #2 suggests,
that we use the same units based on these two quantum effects, however, we determine two necessary
parameters RK and KJ from additional experiments. For trade and legal applications one may use the
recommendations [13, 14], which suggest an uncertainty of the numerical values of (30) as related to
the SI units

RK = 25 812.807 0(26) Ω [1
×
KJ = 483 597.9(2) GHz/V [4

10

−7]
−7]
10

×

[13] ,
[14] .

Two scenarios above are based on the recommendations. The recommendations, however, as well
as all the legal metrology, are designed not for a scientiﬁc use. What is important for physics is not

(30)

(31)

(33)

2005 NRC Canada

28

unknown Vol. 99, 2005

a subject of any legal agreement. For non-precision absolute measurements, or for relative measure-
ments, one can use the CIPM [12, 13, 14] or CODATA [2] recommendations just for convenience.
Meanwhile, for precision scientiﬁc applications, we should avoid any particular values of RK and KJ
in the SI units at all. Instead, the results should be presented as related to more complicated values,
which contain factors (RK)n(KJ )m, taking into account that we have measured a certain quantity
in the units related to the quantum natural units, determined by RK and KJ . One can see such an
approach in the CODATA adjustment of the fundamental constants [2], which dealt with the most
accurate measurements of the fundamental constants.

10. Fundamental Constants and Geometry

... The Multiplication Table doesn’t signify: let’s try Geography.
L.C.

Speaking about the constants of Nature, we cannot avoid a question if number π is one of them.
Our answer is: in a sense, it is. To present our point of view we address to geometry. We remember
from high school, that geometry is based on twelve axioms, the statements which are above any proofs.
However, in physics we should prove (experimentally) everything. We know that the general relativity
states that space-time is ﬂat if there are no gravity sources around. However, a correct statement is
‘locally ﬂat’. Globally, the universe may have a geometry which does not allow the ﬂat geometry
‘universewide’ (like, e.g., a surface of sphere). We have to check if the actual geometry is ﬂat and
the present point of view is: it is close to being ﬂat within a certain uncertainty. That does not help
much, because according to the inﬂatory model [30, 31] the related parameters should be very close
to the values for the ﬂat case. Topologically, the universe may be either open or closed (or even of a
more complicated topological structure than just a closed 4D space). The closure or openness is closely
related to the density of energy which is known at a few-percent level (see Sect. 14). It is consistent
with the ﬂat value. Now, we may in principle check whether the sum of angles of a triangle studied
in our space (after doing corrections to remove gravity effects) is equal to π. This statement may be
veriﬁed and thus π is a fundamental constant of our space. However, it is a fundamental constant not
in the same way as, e.g., α. The ﬁne structure constant is (at least now) a constant without any relation
to theory (of its origin). We measure α and use its value to develop a QED phenomenology. A value
of α is in a sense not critical. If it were discovered that α = 1/136 or 1/138, that would certainly
change numerically the theoretical predictions, but would not change our general view on fundamental
physics. For the number π or the number of the space dimensions (d = 3) we already have a theory
sensitive to their values. We can indeed say that π or 3 are just mathematical numbers, imbedded into a
ﬂat 3D geometry. However, a physical statement is that this geometry is ours. Actually, to measure the
angles geometrically or trigonometrically means to accept a part of geometrical ideas. In such a sense
any test of the sum of angles to be π is rather a test of the validity of the 3D Euclidean ﬂat geometry
in the application to our world.

The relation between geometry and predictability has a more broad context. In a sense any symme-

try is related to a kind of geometry.

Trying to build relativistic gravity theory, we strengthen signiﬁcance of the equivalence principle
which says that a ratio of the inertial and gravitation masses is a universal parameter (which we set to
unity).

A wish to build a linear equation for a relativistic particle led to a prediction of positrons with

‘calculable’ properties:

−

qe =
qe ,
me = me ,
Ie = Ie ,
µe =

µe .

−

(34)

2005 NRC Canada

Savely Karshenboim

The positron actually has been the ﬁrst ever predicted particle. The Dirac equation (as well as the other
linear equations) suggests that in the leading approximation the g factor of a point-like (i.e. structure-
less) particle is equal to two. However, this value is perturbed. For the free leptons (electrons and muon)
the non-zero anomalous contribution is small and can be perturbatively calculated up to certain accu-
racy and also accurately measured. The present combined results for the anomalous magnetic moments
are [2]

g

g

2
−
2 (cid:19)e
2
−
2 (cid:19)µ

(cid:18)

(cid:18)

= ae = 1.159 652 185 9(38)

−3

10

[3.2

−9] ,

10

×

×

= aµ = 1.165 919 81(62)

−3

10

[5.3

−7] .

10

×

×

29

(35)

When the uniﬁcation theory SU(5) was suggested, one of its successes was an explanation of
the Weinberg angle ΘW . The value was predicted for a certain high energy scale (related to Eun ∼
1014 Gev). A measurable value is related to a much lower energy scale and thus should be renormal-
ized. An accurate theory of its perturbation by the radiative corrections together with accurate exper-
imental data should provide us with a constraint on the uniﬁcation theory. The SU(5) theory happens
to be incorrect since it disagrees with a number of observed effects (such as a ‘too large’ proton life-
time τp, presence of the neutrino oscillations etc.). It is believed, nevertheless, that similar uniﬁcation
schemes will eventually explain the neutrality of the hydrogen atom and the value ΘW .

It is quite likely that most, if not all, calculable constants may be predicted only via symmetrical

and thus geometrical ideas.

The symmetries and conservation laws are very strongly related to the fundamental constants and
their constancy. Due to that we would like to note that additionally to direct violations of symmetries
quantum physics offers additional ways to violate the symmetries in a ‘smooth’ matter: via the quantum
anomaly or the spontaneous breaking.

anomaly Quantum ﬁeld theory suggests that we should substitute the wave function for a ﬁeld operator
Ψ(x), which is quite singular and a quantity J[Γ] = Ψ(x)ΓΨ(x) (where for fermions Γ is an
arbitrary combination of the Dirac gamma-matrices) is ill-deﬁned. The quantity J is signiﬁcant
because any electron or quark current is of such a form. It was discovered that there may be a
special kind of quantum violation of symmetry — the anomaly. It is realized in such a way that:

(i) there are two currents J 1

µ and J 2

µ, which are conserved within classical physics (∂J a

µ/∂xµ =

0), and thus two symmetries are presented at the classical level;

(ii) the currents are singular and thus ill-deﬁned at the quantum-ﬁeld level;

(iii) there is no regularization which supports the conservation of both currents and both sym-

metries.

As a result, one of the symmetries (e.g., related to J 2
µ) is to be violated and the non-conservation
= 0 is proportional to the Planck constant h (see [32] for more detail). A well-
term ∂Jµ/∂xµ 6
known example is the Adler anomaly for the axial current which plays an important role in
properties of the π0-meson.

spontaneous The spontaneous breakdown of symmetry is another example how a classical symmetry can be
broken in quantum ﬁeld theory. Let us suggest that the interactions (potentials) are invariant
in respect to a certain symmetry. There is no symmetrical state with the minimal energy, but
instead there is a family of non-symmetrical minimum-energy states. It is similar to, e.g., the
magnetization of a bulk of iron. The theory is isotropic, however, a minimum of energy is related
to the case with a certain non-zero value of the macroscopic magnetic moment. Any direction of
the moment is related to a minimum of the energy (‘vacuum’), however, only one can take place

2005 NRC Canada

30

unknown Vol. 99, 2005

at any particular case. Indeed, there are domains with different directions, but if our observable
universe is inside such a domain, we would not see the other domains (see [34, 31, 32, 30] for
more detail). We note that it is different from the simplest problems in quantum mechanics.
Quantum mechanics can also deal with such a potential, however, because of the overlap of
the vacuum states there is a ‘ﬁne structure’ and the actual minimum of the energy is related to
a certain superposition of these states (e.g., their symmetric sum). The particular asymmetric
vacuum state is to be presented as a sum of the superposition of compound vacuum states and its
evolution via certain oscillations will lead most probably to the lowest state of this ﬁne structure.
However, with an increase of the phase volume (with increase of the number of degrees of
freedom) the probability of the tunnel transitions between the vacuum states goes down very
fast. The evolution time becomes so long that we can see no evolution et all. E.g., we cannot
detect any oscillation between left-hand and right-hand organic molecules. In the case of the
quantum ﬁeld the characteristic evolution time is inﬁnite because of the huge volume of the
universe.

There is also a speciﬁc kind of phenomena, which may lead to an ‘observational’ violation of such
symmetries as, e.g., the Lorentz invariance. They are related to the fact that it is unlikely to observe
any symmetry directly, but we study certain consequences of the symmetry, and, if we do not know a
complete theory, we can be misled.

Let us explain that with an example of nonrelativistic quantum and classical mechanics. They have
mainly the same symmetries and conservation laws (conservation of momentum, angular momentum
and energy). However, from a classical point of view, conservation means that we can measure, e.g.,
all three components of the angular momentum in two separate moments of time and the result must
the same. Classically, we also expect that we can do two ‘fast’ separate measurements of the energy, as
precise as we like, and that allows to check whether the energy is conserved exactly and in any particu-
lar phenomena or the conservation takes place approximately and/or on average. And ‘fast’ means that
the measurement time may be as short as we like. That is by far not the same in the quantum case. In
both cases, doing ‘classical’ experiments for the conservation of the ‘whole’ angular momentum and
the ‘exact’ conservation of energy, one will fail to conﬁrm the conservation laws once we arrive at the
level of accuracy where quantum effects enter into the game. The symmetries and conservations still
take place, but their observed consequences differ from the naive classical expectations.

We know, that at the Planck scale, the geometry of the space-time quite likely differs from what
we see around us12. We do not know what it really is. Indeed, certain symmetries can be broken there.
However, some most ‘sacred’ symmetries might be realized in such a way that their consequences
alternate from our expectations and, thence, the experimental results might ‘observe’ certain violations
of these symmetries and conservations.

11. Constancy of Fundamental Constants

... They began running when they liked, and left off when they
liked.

L.C.

12 I have often heard this statement and this is one more example of a physical jargon. We, e.g., clearly understand that quantum
mechanics does not change the phase space. In the one-dimensional case, the plain {x − p} is just the same as in the classical
case. However, classical states are rather point-like. We sometimes assign them a ﬁnite volume because of the experimental
uncertainty in our data or because of their statistical treatment which is in classical physics also a result of an uncertainty
in our description. Meanwhile, the volume presents only a spot where the point may be, but any state still is point-like and
the uncertainty is, in principle, avoidable in classical physics. The Heizenberg inequality implies that a quantum state has a
minimal ﬁnite volume determined by the Planck constant ¯h. The same for the 3D angular momentum. The only point-like
quantum state in the 3D angular-momentum space corresponds to the case of zero angular momentum. But nevertheless —
the space is the same, the allowed states are different.

2005 NRC Canada

Savely Karshenboim

31

The fundamental constants, most of them, appear in physics with quantum mechanics. The New-
ton’s constant G came earlier, but only considering the Planck-scale effects we can imagine how fun-
damental it is. They were called ‘constants’ and it was believed that they should be such by default.
To vary them, one should rather expect an exceptional reason. That was the situation, when Dirac and
later Gamov suggested that the ‘constants’ may not be constant.

However, the truth is that there is no strong reason why the ‘constants’ of Nature are constant. We
know that the ratio of the electron and proton spins is unity and cannot vary. If it were possible to switch
off the QED corrections, we should expect that the g factor of an electron is a trivial constant equal to
two. Thus, there may be only one theoretical reason for their constancy — that would be an explanation
of their origin. For the most important constants we have none. The constancy of the constants is merely
an experimental fact and an a priori trust in the domination of symmetry in the nature of Nature. The
former, indeed, can never be ﬁnal and we need to check that again and again with a more broad range
of phenomena and with a higher accuracy. The latter is in a formal sense rather wrong: we recognize
the inﬂation as a basic element of modern cosmology. And the inﬂation [30, 31] had urged the electron
mass and charge to vary in a very remote past. If we accept that the constants were varying once, we
should rather consider them as changing quantities at a default situation, and need a reason for them
not to vary again. Or not to vary fast. An once non-constant is forever not a ‘trusted’ constant.

We recognize the existence of the dark matter which may interact ‘very’ weakly with our matter.
We do not know what the dark matter is and how weak may be this ‘very’ weak interaction. Due to a
number of such unclear phenomena, we need to distinguish between

effects such as a violation of the local position invariance (and in particular a violation of the
local time invariance)

•

•

and a variation of the constants.

One may expect that a violation of the local invariance means that results of measurements would de-
pend on time and location upon the measurement and that is the same as a time- and space- dependence
of the fundamental constants. However, these two situations are not quite the same.

The results of an experiment may be affected by an environment. In earlier times, an ‘environment’
for a laboratory-scale experiment was also laboratory-scaled. An exception was gravitational and mag-
netic ﬁeld of Earth. However, they were not signiﬁcant: since the former was nearly a constant (which
did not depend on the location at the level of then achievable accuracy) and from the latter there may
be a shield. Now, doing high-precision balance experiments, one can clearly see effects of the motion
of Sun and Moon in this scale of experiments. Indeed, the existence of the surf has been known for
centuries. But the surf is a result of an accumulation of these effects over a ‘big’ detector, which is
of the Earth scale. Until the very recent time it was not possible to see such effects with the ‘small’
detectors.

Now, we are sensitive to the environment at a very large scale. We know that we live in a chang-
ing universe (the environment item number one), going through a bath of 2.7-K cosmic microwave
background and a similar background radiation of known (neutrino) and, maybe, unknown massless
particles (the environment item number two) and dark matter and dark energy (the environment item
number three) etc. We would never qualify effects of interactions with them as a real violation of
Lorentz symmetry, but we may want to qualify a variation of certain natural parameters induced by
them as a variation of the constants. In principle, we can say that there was no variation of truly fun-
damental constants during the inﬂation, but only ‘environmental effects’, caused by cooling of the
Universe. However, we prefer to say that the electron mass has changed.

As we mentioned, the Earth gravity ﬁeld is nearly a constant and the free fall acceleration g was
considered for a while as an universal and fundamental constant. Now we know it is neither constant
nor universal and fundamental.

2005 NRC Canada

32

unknown Vol. 99, 2005

Kepler found that any planetary orbit satisﬁes a condition

R3
T 2 = [Kepler’s] constant
with the same universal constant for any planet. We now know that the Kepler’s ‘universal’ constant,
which governs the motion of all planets, is a speciﬁc constant related to our solar system only and
nothing more.

(36)

These two examples shows how important it is to understand the nature of the constants. We now
have a great number of fundamental parameters, origin of which is unclear: the Yukawa Higgs coupling
constants, the Cabibbo-Kobayashi-Maskawa matrix (CKM) parameters, parameters of a lepton analog
of CKM, cosmological constants etc. These constants have been observed and studied. There are also
a number of important constants which have not yet been detected, but strongly expected as, e.g., the
mass of the Higgs particle.

Albert Einstein believed that all the constants are, in principle, calculable. That should be expected
in a world, where the equations determine everything. But that apparently is not ours. We know, that
some symmetries of our world have been spontaneously broken. That happens, when the symmetric
state is unstable, while a family of non-symmetric states has the same minimal energy. The vacuum
falls to one of these minimum energy states. We know a number of examples in classical physics. For
example, we already mentioned a bulk piece of iron with a non-zero value of the residual magnetic
moments. A massive piece used to consist of domains — numerous smaller pieces, in which there is
a non-vanishing macroscopic magnetic moment. Hamiltonian and all equations which describe any
domain are isotropic. However, the state with zero macroscopic magnetic moment is unstable. Stable
states are those with a magnetic moment directed to somewhere. Where? We cannot predict. It may
be any direction and in fact the directions in different domains are different. One may think that a
direction is not as important for observable quantities as the magnitude (indeed until we do not look
for a violation of the isotropy — which is not important if we have in mind not our space but a certain
functional space like e.g. of the isotopic spin). However, there is a simple example how to transform
the direction into a magnitude. It is enough to imagine a situation when there are two independent
values similar to the iron’s magnetic ﬁeld, completely independent for the vacuum states, but coupled
together to the same matter ﬁeld. In such a case the angle between them is related to a scalar which can
affect a value of the energy.

This example shows, that certain properties cannot be predicted through the equations. We ac-
knowledge a spontaneous breakdown of symmetry in the Standard model of the electroweak interac-
tions [34, 31, 32]. We expect that our world has a larger symmetry group than we actually observe. And
a non-observed part of the symmetry has been destroyed by one or few spontaneous breakdowns. It
may happen that certain ‘fundamental’ parameters of our world are a direct result of such breakdowns
and they could take, in principle, different values in another place or another version of the evolution
of the universe.

If that is the case, certain parameters are not predictable and discussing them we approach a frame-
work which is the so-called anthropic principle. There has been a number of various modiﬁcations of
it, including not only physical, but also philosophical ideas. We are not very enthusiastic about these
ideas. However, a ‘minimal’ physical part of the principle is the selection principle: we observe only
what we can observe and the very presence of our species, as the observers, sets a certain constraint on
the observable properties. That is like the second Kepler’s law: if we would learn neither the Newto-
nian gravity theory, nor data about planets outside of our solar system, we should consider the Kepler’s
constant in Eq. (36) as a universal constant — the universal constant for all observable planets.

Once we allow variations of the constants of Nature, we remark that the units are also vulnerable.

From the ﬁrst glance, we should prefer to speak about dimensionless constants such as

α =

e2
4πǫ0¯hc

.

(37)

2005 NRC Canada

Savely Karshenboim

33

They are clearer to discuss phenomenologically and easier to detect. Sometimes, it is even stated that
we can only look for a variation of dimensionless quantities.

Indeed, variations of the dimensional constants may be also detected, but the experiments are much
more complicated, because they should directly address time- and space- gradients of such constants
[35, 4]. A well-known example is the famous Michelson-Morley experiment, which checked if the
dimensional constant (the speed of light) in the same in any directions.

Let us leave the general discussion on a search for the variability of the constants at this point and
ﬁrst look how we can describe their variations. If one tries to seriously consider the varying constants,
we have to introduce changes from the very beginning. It is not enough just to accept the equations
derived under a conventional assumption of the constancy of the natural constants and then allow them
to vary slowly. We can easily arrive at a contradiction. Let us consider a simple example: a situation
when, in a speciﬁc inertial frame, the Planck constant is slowly changing globally with the time, so
h = h(t). Meantime, the laws of physics are still isotropic and in particular ∂h/∂xi = 0. However,
the angular momentum13 is quantized

Lz = ¯h

lz .

·

(38)

We arrive at an obvious contradiction: the angular momentum should be conserved (because of the
isotropy), while lz is also not changing (being an integer or semi-integer number) and, meantime, their
ratio, the Planck constants ¯h, is changing. This inconsistency comes directly from the assumption that
we can accept the known equations and allow their constants to slowly vary.

There is no straightforward way to deal with the variable constants. First of all, when the constants

are constant, we can redeﬁne operators via their effective renormalization as, e.g.,

1
e

′
µν

F

Fµν →
etc. If we like to introduce slowly varying constants into the basic equations, we even do not know
into which. Because of the ‘renormalizations’, such as in (39), we have not a single set of the basic
equations but quite a broad family of the equations which are equivalent as long as the ‘constants’ are
constant.

(39)

We used to describe most of objects which are calculable with help of such equations. However, in
our opinion, the starting point to adjust our basic phenomenology to the case of the variable constants
is the path integral (the functional integral over ﬁeld conﬁgurations). The conventional approach reads
that the path integral

′

−iS

e

Z =

Z

presents a matrix element of the evolution operator [33, 32]. To study a particular evolution we should
integrate over all available conﬁgurations (‘trajectories’) with proper initial and ﬁnal conditions. The
action S′ is normalized to be dimensionless. This operator has a transparent physical sense: we have
to sum over all possible trajectories and we also know that in most of cases the dominant trajectory
(trajectories) is related to the least action. The least-action trajectory for quantum mechanics is the
classical trajectory. When we study quantum-ﬁeld ‘trajectories’ in the functional space the least-action
trajectory is related to the ﬁeld equation such as the Maxwell equations for the photon’s ﬁeld and the
Dirac equation for the electron’s ﬁelds. Far from the minimizing trajectory the phase (which is the
action S′) is changing fast and the contributions cancel each other. Close to the minimizing trajectory
the phase is nearly unchanged and the contributions are enhanced.

13 We consider here the classical angular momentum L, which is dimensional, and the quantum angular momentum l which is

dimensionless.

2005 NRC Canada

34

unknown Vol. 99, 2005

Now, we can generalize the action (e.g., for quantum electrodynamics, which is in a narrowed

sense, substantially, a theory of electrons and photons — see Sect. 5.2) to the form

S

′
QED =

d4x

ξ3(x)ψ

gµνγµ

Z

(cid:26)

(cid:20)

∂
∂xν

i
(cid:18)

+ ξ4(x)Aν

ξ5(x)
(cid:21)

ψ

−

(cid:19) −

1
4

ξ6(x)gµν gρλF µρF νλ

, (40)

(cid:27)

where the metric tensor at a particular preferred frame14 is deﬁned as

gµν =







ξ1(x)
0
0
0

−

0
ξ2(x)
0
0

0
0
ξ2(x)
0

−

0
0
0
ξ2(x)

−







and the electromagnetic ﬁeld tensor as

F µν =

∂Aν
∂xµ −

∂Aµ
∂xν

.

The functions ξn(x) obviously violate several important symmetries (including the gauge invari-
ance, the local position invariance, the local Lorentz invariance) and allow time- and space- variations
of the dimensional fundamental ‘constants’. Here, the functions ξn(x) are dimensional, but we can
indeed introduce factors h(0), c(0), e(0) and m(0) in a proper way and make the ξ functions dimension-
less. These four factors are related to the Planck constant, the speed of light, the elementary charge and
the electron mass as measured in a particular point x(0). More complicated models are also possible
with, e.g., a less trivial metric tensor or the electromagnetic ﬁeld tensor, or with appearance of small
terms which directly violate various symmetries.

When should we use Eq. 40 and when just put time- and space- dependence inside conventional

equations? It depends on a problem. Basically, there are three kinds of related measurements:

•

•

•

One may perform a series of ‘fast’ measurements separated in time by a ‘large’ interval T (for
simplicity we speak here about the time variations only). That is, in particular, a case of atomic,
molecular and nuclear spectroscopy. Additional terms with derivatives of the ξ(x) functions
should be integrated over the ‘short’ time of the measurements τ and the value of τ ∂ξ/∂t may
τ ).
be neglected in comparison with a difference ξ(t+T )

ξ(t) over the ‘large’ separation (T

−

≫

‘Long’ measurements can be performed, e.g., as a result of continuous monitoring of the motion
of planets etc. In this case the effect of the integration of ∂ξ/∂x is comparable to the effect of
the adiabatic change of ξ(x) in the conventional equations.

And indeed, one can try to deal directly with derivatives performing ∂ξ/∂x-sensitive experi-
ments. It is quite probable that it is easier to do that for space- rather than for time- gradients.
For example, we can try to perform precision measurements in space similar to the GPS mea-
surements in the atmosphere. In the case of space-gradient terms in the law of the propagation
of light we should ‘observe’ a non-ﬂat geometry after interpreting the light propagation time
intervals as the effective distances.

14 There are two natural options for such a frame: the ﬁrst is related to the one where the cosmic microwave background (CMB)
radiation is isotropic, the other corresponds to the frame which is determined by the dark matter and it is indeed not worldwide
universal.

2005 NRC Canada

Savely Karshenboim

35

12. Search for Possible Time Variation of Fundamental Constants

– ...‘one can’t believe impossible things.’
– ‘I daresay you haven’t had much practice.’
L.C.

The easiest and most transparent kind of experiments to search for a variation of the constants is
indeed to measure the same quantity twice. If these measurements are ‘fast’ and have a long separation,
we can use the description with non-varying constants and not care about possible additional terms
and gradients. The validity of this approach is obvious for atomic physics: we do a series of short
measurements, for which the gradients of constants are negligible for atomic time and space scale
and cannot affect the result of measurements. A different situation is for a search of a variation of
the Newtonian constant G. Most of measurements are related to continuous monitoring during a long
period. Instead of large series of short atomic measurements, the gravity searches deal with a number
of long measurements. In such a case, contributions of the gradients should be important.

Due to that we concentrate our attention on atomic measurements. Still there are three kinds of

them discussed in literature.

•

•

•

Astrophysical comparisons deal with a relatively low fractional accuracy, but take advantage of
a very big time separation (up to 1010 yr). This kind of observations is by far not transparent
and suffers from necessary statistical evaluations in a situation when certain correlations may be
present.

Clocks based on different transitions have been developed and comparing them one may hope to
learn about relative variations of their transition frequencies. However, the clock is just a device,
built for a purpose, and it is not necessary that its properties are related to the atomic or molecular
transition frequency in an exact sense. There may be a number of drifts and certain parameters of
the clock, which determines drifts and which are determined in turn by an artiﬁcial environment
in a non-controlled way. An example of such a clock is the hydrogen maser. Its frequency drift
is purely due to an environmental problem (a so called wall-shift).

Still, the frequencies of certain clocks follow the atomic transition frequencies. Such clocks are
similar in this sense to the primary caesium standard. The very basic principle of the primary
caesium clock is that it should reproduce the caesium HFS transition frequency, which is related
to the SI second. To deal with this kind of the clocks is the same as to measure an atomic or
molecular frequency with high accuracy.

Below we consider in detail this kind of the ‘near primary’ frequency standards (and we note that
maybe in future one of them will give us a new SI second15).

Signiﬁcance of these clock-based experiments is in a great accuracy of precision frequency mea-
surements. Presently and for a while (maybe even forever) the frequency measurements are the most
accurate.

Here we consider certain detail of laboratory searches. Different aspects of the possible variations

of the fundamental constants and their searches are discussed in [36].

15 Because of a certain conservativeness of CIPM, which should necessarily take place, and because of a variety of competitive
optical candidates, we expect in near future not a change of the deﬁnition of the SI second, but, ﬁrst of all, CIPM recommen-
dations. At the ﬁrst stage, they could recommend values of certain optical and microwave transitions which would be advised
for a practical realization of the second (compare, e.g., to the CIPM recommendation on the metre [11]). At the second stage,
after the accuracy of a comparison of certain optical transitions to each other will supersede the accuracy of the caesium
standards, a conventional second (e.g., the second-2015) could be introduced.

2005 NRC Canada

36

12.1. Atomic clocks

unknown Vol. 99, 2005

In this chapter we consider rather frequency standards than clocks. In principle, a clock is a time
standard. Indeed, the frequency and time intervals are closely related, however, the time measurement
may be ‘absolute’, i.e., related to the conventional ‘beginning of time’. That involves two metrological
problems for keeping the time scale: the realization of the time-interval unit and of the ‘zero point’. The
speciﬁcs of time keeping requires that ‘true’ clocks operate continuously, otherwise the information on
the initial moment would be lost. A real time standard is actually not a single device but a set of
various related standards. Still, with a peripheral part of the clock operating around it, the very heart
of any clock is a certain frequency standard. Presently, that is either a caesium standard or a standard
calibrated against the caesium.

The best modern clocks pretend to deliver certain reference frequencies with an uncertainty at the
level of a part in 1014 and even less. If we check a value of the linear Doppler shift related to this level,
the speed of an atom is to be 3 µm/s. For a hydrogen atom a temperature of 1 K is related to a speed
of approximately 100 m/s, i.e., eight orders of magnitude higher. Heavier atoms at this temperature are
slower by a factor of √A, where A is the atomic mass number. That means that for an accurate clock
we have to solve the problem of the linear Doppler effect.

In different clocks the problem of the linear Doppler effects is solved differently (see [37] for more
detail). In the clocks with neutral atoms, the atoms are cooled down to the level much much lower than
1 K. Ions are trapped and that eliminates the Doppler effect — a localized trapped particle cannot have
a non-zero momentum. One more approach is to study two-photon transitions, which are not sensitive
to the linear Doppler effect.

If the constants are changing, not only theory should be reconsidered, but also experiment. First of
all, we need to acknowledge that if certain natural constants are changing, our units and, in particular,
the SI second are changing as well. For this reason, the most simple and hopeful way is to look for
variation of dimensionless quantities. However, since any measurement is a comparison, we can also
deal with dimensional quantities, properly specifying the units. The interpretation of the variation of
the numerical values of the constants differs drastically from the interpretation of a search for their
direct variation. In particular, we will speak about a variation of the numerical value of the Rydberg
constant, which is closely related to properties of the caesium atom. In fact, it is equal to

2c

R∞
νHFS(133Cs) ×

·

νHFS(133Cs)
2c

1

9 192 631 770

×

2

299 792 458

a.u.

×

(exactly)

R∞

{

}

=

=

νHFS(133Cs)
(cid:1)
(cid:0)
νHFS(133Cs)
a.u.
(cid:1)
(cid:0)

9 192 631 770

2

299 792 458

×

is an artifact of the SI system.

= 15.331 659 494 249 183 8 . . .

where

is the caesium HFS interval in the atomic units and the number

(41)

12.2. Scaling of different transitions in terms of the fundamental constants

With help of the accurate clocks we can compare the frequencies of different transitions. What can

First of all, let us look at expressions for different atomic transitions in the simplest case, i.e. for

we learn from them?

the hydrogen atom,

f (2p

1s)

→

≃

3
4 ·

cR∞ ,

2005 NRC Canada

37

(42)

(43)

Savely Karshenboim

Transition
Gross structure
Fine structure
Hyperﬁne structure

Energy scaling
cR∞
α2cR∞
α2(µ/µB )cR∞

Table 5. Scaling behavior of the atomic energy intervals as functions of the fundamental constants. µ stands for
the nuclear magnetic moment.

f (2p3/2 −

2p1/2)

fHFS(1s)

α2

cR∞ ,

1
16 ·
4
3 ·

α2

·
µp
µB ·

·

cR∞ .

≃

≃

Indeed, there are various corrections and, in particular, the relativistic and the ﬁnite-nuclear-mass cor-
rections but they are small.

The ﬁrst interval is related to the gross structure, the second is for the ﬁne structure and the last is
for the hyperﬁne splitting. So, we note that if we would measure them, we can learn about variations
of cR∞, α and µp/µB. To be more precise, when one measures a frequency, the result may be either
absolute or relative. The latter case, when two ratios are measured for three transitions, will tell us
nothing about the Rydberg constant. Measuring the intervals absolutely, i.e. in certain units, we can
consider a variation of a value of the Rydberg frequency cR∞ in these units. In the previous subsection
we explained about the physical meaning of a value of the Rydberg constant in the SI units.

What happens if we consider a more complicated atom? First, let us re-write the equations above

in atomic units

f (2p3/2 −

f (2p

→

1s)
a.u. ≃
(cid:12)
(cid:12)
(cid:12)
2p1/2)
a.u. ≃
(cid:12)
(cid:12)
(cid:12)
fHFS(1s)
(cid:12)
(cid:12)
(cid:12)

a.u. ≃

,

3
8
1
32 ·
2
3 ·

α2 ,

α2

µp
µB

·

.

The gross structure is of order of unity. The ﬁne structure is a relativistic effect, proportional to the
factor of (v/c)2 and thus to α2. The hyperﬁne structure is also a relativistic effect, but it suppressed by
a small value of the nuclear magnetic moment in atomic units.

If we have a more complicated atom, nothing will change except for the numerical coefﬁcients.
There is no additional small or big parameter when calculating the gross structure and it still should
be of order of unity. The electron speed is always proportional to αc. It may also be a value of the
nuclear charge Z involved, but it does not change with time. So, we conclude that the scaling behavior
of the atomic transitions with changes of the constants is the same as in hydrogen (see Table 5). The
importance of these scalings for a search of the variation of the constants was ﬁrst pointed out in [38]
and discussed there for astrophysical searches.

Molecular spectra are more complicated than atomic. The biggest energy intervals are related to
the electron transitions and they are completely similar to the atomic gross structure. Two other kinds
of the intervals (vibrational and rotational) are due to the nuclear motion.

Let us consider a diatomic molecule. In the so-called Born-Oppenheimer approximation (see, e.g.,
[39]) we can consider the energy of the electronic states as a solution of a problem of the electrons
in the ﬁeld of two Coulomb centers with the inﬁnite masses separated by the distance R. The result
depends on this distance (E(R)) and the next step is to ﬁnd a value of the distance R0 which minimizes
the energy. Let us now take into account nuclear motion. In the leading approximation the Hamiltonian

2005 NRC Canada

38

unknown Vol. 99, 2005

Transition
Electronic structure
Vibrational structure
Rotational structure

Energy scaling
cR∞
(me/M )1/2cR∞
(me/M )cR∞

Table 6. Scaling behavior of the molecular energy intervals as functions of the fundamental constants. M stands
for an effective nuclear mass (equal to reduce nuclear mass for diatomic molecules), but for most of applications
may be substituted for the proton mass mp.

is of the form

H =

+ E(R)

P2

2M
P2

≃

2M −

k
2

(R0 −

R)2 + E(R0) ,

(44)

∼

E(R)/R2

where we note that R0 is about unity in atomic units, the binding energy E(R) is also about unity and
0 is about unity as well. All of them do not depend on the fundamental constants
thus k
(in the atomic units). M is the nuclear reduced mass. The equation is for a harmonic oscillator (in
the leading approximation) and we know all parameters (at least their dependence on the constants
1/M in the atomic units or
in the atomic units). We ﬁnd that the vibrational quantum of energy is
(me/M )1/2cR∞ in the SI units. Here M is a characteristic nuclear mass, but for the most of applica-
tions we can neglect the binding energy and the difference between the proton and neutron masses and
set M = Amp. For diatomic molecules the effective atomic number is related to the reduced mass

p

AR =

A1A2
A1 + A2

.

Being a number, it cannot vary with time and can be dropped out from any scaling equations applied
for interpretation of search-for-variation experiments.

Estimation of the rotational energy is also simple. The energy is of order L2/I where L is the
(dimensional) orbital momentum and I is the moment of inertia. In atomic units L is of order of unity
M . Finally we ﬁnd that the rotational energy scales as (1/M ) in the atomic units or
and I
0 ∼
(me/M )cR∞ in the SI units.

M R2

∼

The importance of different scaling of the molecular transitions was pointed out in [40] due to
astrophysical applications. We summarize the scaling behavior of various molecular transitions in Ta-
ble 6.

This evaluation shows a great convenience of the atomic units for atomic and molecular calculations
and demonstrates that a calculation of the order of magnitude of an effect and its rough detail is an
important issue and a part of ‘calculable’ properties.

A successful deduction of constraints on a possible time variation of

At the present time all these scalings are not very hopeful for laboratory searches since only two
kinds of atomic transitions, optical and HFS, are studied with a high accuracy. It should have a very
reduced application, if the non-relativistic scaling in Tables 5 and 6 would be the only method we have.
and α is possible
because of the relativistic corrections, which are responsible for a different sensitivity to the α-variation
for various transitions of the same kind (e.g., for various gross-structure optical transitions). That was
ﬁrst pointed out in [41] and later successfully developed and applied to various atomic systems in [42].
Most of standards deal with neutral atoms and single-charged ions. The valent electron(s) spends
most of time outside of a core created by the nucleus and the closed shells. The core charge for atoms
used in the actual clocks is from one to three. The relativistic correction is of order of (Z ′α)2. If Z ′ is

cR∞

{

}

2005 NRC Canada

Savely Karshenboim

39

Constants (X)
α
{cR∞}
µCs/µB
µRb/µCs
µYb/µCs
me/mp
µp/µe
gp
gn

Variation rate (∂ ln X/∂t)
(−0.3 ± 2.0) · 10−15 yr−1
(−2.1 ± 3.1) · 10−15 yr−1
(3.0 ± 6.8) · 10−15 yr−1
(−0.2 ± 1.2) · 10−15 yr−1
(3 ± 3) · 10−14 yr−1
(2.9 ± 6.2) · 10−15 yr−1
(2.9 ± 5.8) · 10−15 yr−1
(−0.1 ± 0.5) · 10−15 yr−1
(3 ± 3) · 10−14 yr−1

Table 7. Current laboratory constraints on the possible time variations of natural constants [37]. The results above
the horizonal line are model-independent, while the validity of the results below the line depends on the applica-
bility of the Schmidt model. The uncertainty of this application is not shown.

the core charge, it is still very small. However, the relativistic corrections are singular and a contribution
of the short distances, where an electron sees the whole nuclear charge Z, is enhanced. The dominant
part of the relativistic corrections comes from the short distances where the electron sees the whole
charge of a bare nucleus and thus in the atoms under question the correction can be really big.

12.3. Current laboratory limits

Optical measurements delivered us data on a few elements (mercury ion [43], hydrogen [44], ytter-
bium ion [45] and calcium [46]) and there are also promising results on strontium ion [47] and neutral
strontium [48] and more data on strontium and other transitions are expected. The already available
data [43, 44, 45, 46] are related to transitions with very different relativistic corrections and that is
enough to derive strong limitations on the time variation of several constants. The model-independent
constraints achieved this way are collected in Table 7 (in the top part) [37]. The HFS results were also
applied to obtain constraints on the variation of the magnetic moments [49, 50]. To derive results on the
more fundamental quantities than the nuclear magnetic moments of few particular nuclei, we applied
the Schmidt model (see, e.g., [52]). Its importance for the interpretation of results on the variation of
the fundamental constants was pointed out in [51]. The results are shown in the bottom part of Table 7
[37].

Is it possible to reach a constraint on the time variation of of me/mp from atomic spectroscopy?
Yes, it may be done in the following way. First, we extract a limitation on a variation of cR∞ without
any use of the hydrogen data and next we can compare it to a variation of the hydrogen 1s
2s
frequency (which is proportional to cR∞(1

me/mp)). The constraint on the variation is

−

−

∂ ln(mp/me)
∂t

= (

0.4

1.3)

−

±

·

−11 yr

−1 ,

10

(45)

which is more than three orders of magnitude weaker than the model dependent constraint in Table 7.
Stronger limitations should appear rather from molecular spectroscopy.

It is signiﬁcant that we can eventually constrain the variability of the fundamental constants. Results
on variations of such non-fundamental objects as the atomic transition frequencies should be rather
doubtful since such a level of accuracy has been never achieved before and various details of the
experiments may need an additional examination. Expression such results in terms of fundamental
constants allows a cross-comparison and makes the results more reliable.

2005 NRC Canada

40

unknown Vol. 99, 2005

12.4. Non-laboratory searches for the variations of the constants

... but it all came different!
L.C.

The laboratory limitations are not the strongest, but the most reliable. Astrophysical [53] results un-
fortunately contradict each other, as well as and geochemical [54] constraints. Studies by both methods
involve various systematic sources and the access to data is limited. One should deal with observations,
not with experiments.

13. Fundamentality of the Constants and the Planck Scale

... and noticed that what can be seen from the old room was quite
common and uninteresting, but that all the rest was as different
as possible.

L.C.

If physics is governed by the most fundamental constants and if the ultimate theory includes quan-

tum properties of the space-time, then the fundamental scale is determined by the Planck units

MPl =

= 2.176 45(16)

1/2

¯hc
G (cid:19)

(cid:18)

= 1.220 90(9)

lPl =

= 1.616 24(12)

−8 kg

10

×

1019 Gev/c2 ,
−35 m ,

×
10

tPl =

= 5.391 21(40)

−44 s ,

10

×

TPl =

= 1.416 79(11)

1032 K .

×

×

¯h
MPl c
lPl
c
MPlc2
k

(46)

At the scale determined by these units the laws of Nature should take the simplest form and if any
observable fundamental constant is calculable, it should be calculable there.

Due to success of the renormalization approach it is commonly believed that physics from the
Planck scale does not affect our ‘low-energy’ world. That is true only in part. To be accurate, the idea
of the renormalization reads that all what we need from the higher-energy physics can be successfully
measured at our low energies. Still, what we measure at our energies comes from the higher scale (such
as, e.g., the Planck scale, or a scale of the spontaneous violation of a larger symmetry related to the
uniﬁcation). Presently, we do not have any theory related to the higher energy scale. If the Planck or
another high-energy scale has no dynamics, we have not much hope to understand about the high-
energy physics from our low-energy experiments. We can learn nothing from measured numbers until
we are able to proceed with a theory from the scale of, let us say, the Z boson mass, to a really high
energy. Any corrections beyond that are small as (m/MPl)2 where m is a characteristic mass scale we
deal with. However, if the Planck-scale physics has a dynamics, e.g., a variation of certain parameters,
that is not true anymore. First of all, if the bare constants (such as the bare electron charge e0 and the
the electron mass m0) determined at the Planck scale can vary, we should be able to detect that. There
is a chance, that the ﬁne structure constant at the Planck scale is calculable and, e.g., α0 = 1/π4,
however, but there is no chance that any numerical exercises, such as done in the past, may succeed
to express actual α in an simple way. Neither it is likely that m0 is calculable in a simple matter.
Nevertheless, if the bare constants do not vary, we still can expect that the dressed constant (i.e. the
actual renormalized constants which we measure) show a certain detectable variation, which could
appear via the renormalization.

How easily can we see such a dynamics, induced by the renormalization? A question for α variation
is whether the divergencies are cut by the Planck scale, or a certain supersymmetry enters into the game

2005 NRC Canada

41

(47)

(48)

Savely Karshenboim

at an intermediate scale MSS ≪
bare electron mass vary, and if it does whether the ratios me/MPl and MSS/MPl vary.

MPl and cut the divergencies off. The other question is whether the

In the case of ultraviolet divergencies going up to MPl, the result is

while in the case of the supersymmetrical cut off, the variations are of a quite reduced value:

1
α

∂α
∂t ∼

α
π

1
MPl

∂MPl
∂t

,

1
α

∂α
∂t ∼

α
π (cid:18)

MSS
MPl (cid:19)

1
MPl

∂MPl
∂t

.

2

Concerning the bare electron mass, a problem is much more complicated, because detail of the
so-called Higgs sector are unclear and connection of the Higgs parameters with more fundamental
quantities are to be clariﬁed. It may happen that a dynamics at the Planck scale urges certain variations
at the Higgs sector and afterwards a variation of the Higgs vacuum average v and consequently of
the bare value of the electron mass m0. We note, that such a variation may in principle keep the ratio
m0/MPl constant and reduce the value of the α variation via the renormalization which mentioned
above. A detection of a variation of the electron mass is even less clear. A variation of me is not the
question from the experimental point of view, the question is a variation of me/mp. Since the origin
of the electron and the proton masses are very different it is hard to understand what can happen with
their ratio.

Still with a number of problems to be solved the fundamental constants give us a chance to study

physics of a higher-energy scale not available anyhow else.

14. Constants of Cosmology

It’s a poor sort of memory that only works backwards.
L.C.

Constants of our universe as a whole give us another questionable chance to study physics beyond
our essential world. Such constants as listed in Table 8 may offer us a unique opportunity to learn about
the early time of the universe and thus perhaps about a very-high-energy physics.

Constant
Ωtot − 1
Ωbm
Ωdm
ΩΛ
TCMB
H
nγ /nB

Value
0.02(2)
0.044(4)
0.22(4)
0.73(4)
2.725(1) K
1.02(5) × 10−10 yr−1
1.64(5) × 109

Table 8. Fundamental constants of cosmology. The results are taken from [27].

In particular, these constants characterize the density of the bright matter (i.e. a visible matter), the
dark matter (a matter recognized because of its gravitational effects) and the dark energy (recognized
due to its cosmological consequences and related to the Einstein’s Λ-term) in the units of the critical
density

Ωi =

ρc =

,

ρi
ρc
3
8π

H 2
G

,

(49)

(50)

2005 NRC Canada

42

unknown Vol. 99, 2005

where H is the Hubble constant. The critical density is a separation mark between the closed (Ωtot > 1)
and open (Ωtot < 1) universe. The between case (Ωtot = 1) is the ﬂat universe. The present result for
the total density is close to the ﬂat value.

One more parameter, a ratio of the number of the microwave background photons and the baryons

(nγ/nB) is important to learn about a moment when the light split from the baryon matter.

Meanwhile, we have to remember about the evolution in understanding of such important ‘con-
stants’ as the free fall acceleration g and the water density. The constants of our world are not necessar-
ily the truly fundamental constants since their values might be taken by chance with the spontaneous
breakdown of certain symmetries.

15. Physics at the Edge

... As she couldn’t answer either question, it didn’t much matter
which way she put it.

L.C.

A question of the constancy and the fundamentality of the fundamental constants is certainly a
question of new physics. Studying the problem experimentally via, e.g., searching for the variability of
the natural constants we address this new physics. Maybe that is not the best way to do that, however,
we are extremely limited now in what we can do. Never from the Newton’s time, we have been so
badly suited for going forward. The physics is in a deep crisis, despite that it looks like as a success.
We are able to explain nearly everything we can deal with. Yes, we have some problems, but that is
either because some objects are too complicated, or because the involved interaction is strong and we
are not able to go from Hamiltonian to the observable quantities. But that is normal. In a sense, that
is not a problem of fundamental physics, but of technology to apply it, which indeed is also of great
importance.

We have access to only a few problems related in different ways to the fundamentally new physics:

on details of the Higgs sector;

on the extension of symmetry from the Standard model to a certain uniﬁcation theory which,
probably, involves a supersymmetry;

•

•

•

•

•

on the dark matter;

on the dark energy;

on quantum gravity and physics at the Planck scale.

To address them, we suffer from extreme shortage of information and do not see a feasible way to reach
more data soon. To illustrate the problem we collect in Table 9 data, important for new physics.

16. Dreaming about New Physics

And here I wish I could tell you half the things Alice used to
say, beginning with her favourite phrase ‘Let’s pretend.’

L.C.

What should scientists do with the obvious lack of the data? Different people do different things.
Some develop a ‘real sector’ of physics, where certain problems are important, sometimes very impor-
tant, but not ‘fundamentally’ important. Some develop tools and technologies which are needed to go
further. Some search for new physics, but the lack of information does not allow to understand where it
is better to look for. So this search is a kind of a search for a treasure which does not necessarily exist.

2005 NRC Canada

43

Savely Karshenboim

Constant
mH
CKM matrix
lepton analog of CKM

Comment
should be within certain margins if the Higgs particle is elementary
unitarity would conﬁrm the minimal Standard Model
would constrain physics beyond the Standard Model;
the structure of both matrices could give a hint to the ﬂavor symmetry
should be zero in the case of uniﬁcation theories
would constrain uniﬁcation theories⋆
would constrain uniﬁcation theories⋆
would constrain uniﬁcation theories⋆
would constrain uniﬁcation theories†
would constrain physics beyond the Standard Model;
the Majorana mass would conﬁrm that qν = 0
if the value is positive, the universe is closed,
if negative, it is open, if zero, it is Euclidean;
a small value is expected due to the inﬂation model (IM); it will constrain IM
important that the dark matter exists∗
important that the dark energy exists∗
would constrain cosmological theory

|qe + qp|
τp
de
dn
sin ΘW
mν

Ωtot − 1

Ωdm
ΩΛ
∂H/∂t

Table 9. Fundamental constants of new physics. ⋆ – the experimental level is already within theoretical margins
and any improvement of the limits is important; † – uniﬁcation theory must predict its value at a certain high
energy scale; radiative corrections are needed to go down to low energy for a comparison with experiment; ∗ – the
exact value is not important for a moment.

Some dream. It is hard to qualify differently the theoretical studies without any connection with
experiment, i.e., with reality. Dreamers existed at all times. Sometimes doers and thinkers put the
dreamers into shadow, but they existed.

The shortage of the experimental data makes us to wonder whether ‘purely’ theoretical progress is
possible. Our opinion is rather negative. The creation of the Einstein’s special relativity is sometimes
believed to be a perfect example of such a progress. However, there is certain confusion in use of the
word ‘theoretical’. A ‘theory’ may be a model, a hypothesis or a framework of certain calculations
completely supported by experiment, i.e., a high-level kind of ﬁtting.

The famous inconsistency of the Newton’s mechanics and the Maxwell’s theory of electromag-
netism was not a ‘purely theoretical’ problem. That was a conceptual disagreement between two ‘the-
oretical ﬁts’ of a huge amount of experimental data. The relativity principle in the former form was a
part of the Newton’s mechanics. The Einstein’s solution reproduced both theories: Maxwell’s (exactly)
and Newton’s (as an approximation at v/c
1). It was also immediately conﬁrmed by numerous
experiments. Later, Einstein tried to solve an inconsistency between two other pieces of the theoretical
description of then existed data — his fresh-backed relativity and the old-fashion Newton’s gravity. In
contrast to special relativity, it was difﬁcult to conﬁrm general relativity accurately and in detail. The
progress in the ﬁeld had been quite slow for a long period until vitalized by appearence of new data.

≪

The ﬁrst important steps of quantum mechanics were directly inspired by various experimental
data (for that time this theory was too crazy to appear as a result of a ‘purely theoretical’ development)
and its crucial statements were immediately checked experimentally. When the immediate experiment
was not possible, the theoretical ideas were sometimes gloriously correct, but sometimes completely
wrong. An example of the wrong ideas was an expectation that the proton should have the Dirac’s value
of the g-factor, i.e., gp = 2 (see, e.g., [22]). There was no other way but experiment to check if the idea
is wrong or correct and they had to wait until the idea was conﬁrmed.

Sometimes this kind of studies is just a waste of time, sometimes an important step to future theo-

2005 NRC Canada

44

unknown Vol. 99, 2005

ries. We can never know. Such important conceptions as antiparticles, the Majorana mass, the Kaluza-
Klein theory, and the Yang-Mills gauge ﬁeld appeared as purely theoretical constructions. The positron
was discovered shortly after its prediction by Dirac. The Majorana mass perhaps will now ﬁnd its ap-
plication to neutrino. The Yang-Mills gauge ﬁeld theory is now a way to describe weak and strong
interactions and likely other interactions which should appear due to the uniﬁcation. Speculations on
Kaluza-Klein theories have inspired a lot of works on uniﬁcations of all interactions with gravity, but,
maybe, this problem will be solved in another way.

17. Conclusions

‘Are you animal – or vegetable – or mineral?’ he said, yawning
at every word.

L.C.

In this short paper we have tried to present an overview of a problem of the fundamental constants
and various related questions, including practical metrology and realizations of the units, simple atoms
and macroscopic quantum phenomena, variations of the constants and physics at the Planck scale.

In the beginning of the paper we introduced the fundamental constants as certain universal param-
eters of the most basic equations. We have noted afterwards that we hardly understand their origin.
These parameters play nearly a mystic role. Such equations as Maxwell’s or Dirac’s appeared as then
top-fundamental summary of our understanding of Nature. Clearly, such equations interpret the behav-
ior of certain objects in terms of the input parameters c, h, e, me, G etc, which as any input parameters
should come from outside the equations. The equations happen to be a benchmark between understood
(the shape of the equations) and non-understood (the fundamental constants inside the equations). In a
sense the truly fundamental constants are the least understood part of the best understood physics.

We still do not know where the ﬁne structure constant α comes from and what should its value be;
we still wonder what the reason that the gravity is so much weaker than the electromagnetic interaction
is. The trace of the origin of the fundamental constants is lost somewhere in Wonderland, which we
can, perhaps, see through the looking glass and try to guess about the unseen part of the room...

Acknowledgements

This work was supported in part by the RFBR under grants 03-02-04029 and 03-02-16843, and
by DFG under grant GZ 436 RUS 113/769/0-1. The author gratefully acknowledges stimulating dis-
cussions with L. B. Okun, Z. Berezhiani, S. I. Eidelman, V. Flambaum, L. Hollberg, O. Kancheli, D.
Kleppner, C. L¨ammerzahl, E. Peik, D. Pritchard, and V. A. Shelyuto.

References

1. M. Niering, R. Holzwarth, J. Reichert, P. Pokasov, Th. Udem, M. Weitz, T. W. H¨ansch, P. Lemonde, G.

Santarelli, M. Abgrall, P. Laurent, C. Salomon, and A. Clairon, Phys. Rev. Lett. 84, 5496 (2000);
M. Fischer et al., Phys. Rev. Lett. 92, 230802 (2004).

2. P. J. Mohr and B. N. Taylor, Rev. Mod. Phys. 77, 1 (2005). See also http://physics.nist.gov/constants.
3. S. G. Karshenboim, Physics-Uspekhi, 48, 255 (2005).
4. S. G. Karshenboim and E. Peik, In [36], p. 1.
5. The International System of Units (SI). BIPM, S`evres, 1998. We quote here English version, however,

the ofﬁcial text is French.

6. P. J. Mohr and B. N. Taylor, Rev. Mod. Phys. 72, 351 (2000); E. R. Cohen and B. N. Taylor, Rev. Mod.

Phys. 59, 1121 (1987);
E. R. Cohen and B. N. Taylor, J. Phys. Chem. Rev. Data 2, 663 (1973).

7. L. B. Okun, in [36]; physics/0310069.
8. B. N. Taylor, Guide for the Use of the International System of Units (SI). NIST Spec. Pub. 811. NIST,

1995.

2005 NRC Canada

Savely Karshenboim

45

9. D. Fenna. A Dictionary of Weights, Measures and Units (Oxford University Press, 2002).
10. ICRP Publication 74. Conversion Coefﬁcients for use in Radiological Protection against External

Radiation. Pergamon. 1995.

11. T. J. Quinn, Metrologia 40, 103 (2003).
12. T. J. Quinn, Metrologia 26, 69 (1989) (see p. 70).
13. T. J. Quinn, Metrologia 38, 89 (2001) (see p. 91).
14. T. J. Quinn, Metrologia 26, 69 (1989) (see p. 69).
15. T. Kinoshita and M. Nio, Phys. Rev. Lett. 90, 021803, (2003).
16. R. S. Van Dyck, Jr., P. B. Schwinberg and H. G. Dehmelt, Phys. Rev. 59, 26 (1987).
17. S. G. Karshenboim, in Ref. [21], p. 141; Int. J. Mod. Phys. A19, 3879 (2004).
18. M. I. Eides, H. Grotch and V. A Shelyuto, Phys. Rep. 342, 63 (2001).
19. Yu.I. Neronov and S.G. Karshenboim, Phys. Lett. A318, 126 (2003);

S. G. Karshenboim, V. G. Ivanov, Yu. I. Neronov, B. P. Nikolaev and Yu. N. Tolparov to be published,
Can. J. Phys,.

20. S. G. Karshenboim, F. S. Pavone, F. Bassani, M. Inguscio and T. W. H¨ansch (Eds). The Hydrogen Atom:

Precision Physics of Simple Atomic Systems. Springer-Verlag, Berlin, Heidelberg, 2001.

21. S. G. Karshenboim and V. B. Smirnov (Eds). Precision Physics of Simple Atomic Systems. Springer-

Verlag, Berlin, Heidelberg, 2003.

22. J. S. Rigden. Hydrogen: The Essential Element (Harvard University Press, 2002).
23. A. H. Wapstra, G. Audi and C. Thibault, Nucl. Phys. A 729, 129 (2003);
G. Audi, A. H. Wapstra and C. Thibault, Nucl. Phys. A 729, 337 (2003).

24. E. G. Kessler, Jr., M. S. Dewey, R. D. Deslattes, A. Henins, H. G. Brner, M. Jentschel, C. Doll, and H.

Lehmann, Phys. Lett. A 255, 221 (1999)

25. R. S. Van Dyck, Jr., D. L. Farnham, S. L. Zafonte, and P. B. Schwinberg, in Trapped Charged Particles

and Fundamental Physics, ed. by D. H. E. Dubin and D. Schneider (AIP, 1999), p. 101.

26. B. Young, M. Kasevich, and S. Chu, in Atom Interferometry, ed. P. R. Berman, New York: Academic

Press, p. 363 (1997);
A. Wicht, J. M. Hensley, E. Sarajlic, and S. Chu, Phys. Scr. T102, 82 (2002).

27. S. Eidelman et al., The Review of Particle Physics, Phys. Lett. B592, 1 (2004).
28. D. Pritchard, an invited talk at ICAP’2004, to be published.
29. E. Sch¨odinger. What Is Life? (Cambridge University Press, 1967.
30. A. D. Linde, Inﬂation and Quantum Cosmology (Academic Press, 1990)
31. L.B. Okun, Leptons and Quarks (Elsevier Science Pub Co., 1987)
32. C. Itzykson and J.-B. Zuber, Quantum Field Thoery (McGraw-Hill Inc., 1985);

S. Weinberg, The Quantum Theory of Fields, Vol. 1, 2 (Cambridge University Press, 1995, 1996).

33. P. Ramond, Field Theory. A Modern Primer, (The Benjamin/Cumming Publ. Co., 1981)
34. L. B. Okun. Particle physics: The quest for the substance of substance. Harwood Ac. Publ., 1985.
35. S. G. Karshenboim, Eprints physics/0306180 and physics/0311080, to be published. Add references.
36. S. G. Karshenboim and E. Peik (Eds.). Astrophysics, Clocks and Fundamental Constants, (Springer,

Berlin, Heidelberg, 2004), Vol. 648.

37. S. G. Karshenboim, V. Flambaum, and E. Peik, to be published in Handbook on Atomic, Molecular and

Optical Physics; physics/0410074.
38. M. P. Savedoff, Nature 178, 688 (1956).
39. L. Pauling and E. W. Wilson, Jr., Introduction to Quantum Mechanics with Applications to Chemistry,

Dover, 1985;
M. Karplus and R.N. Porter, Atoms and Molecules. An Introduction For Students of Physical Chemistry,
Benjamin, New York 1970.

40. R. I. Thompson, Astrophys. Lett. 16, 3 (1975).
41. J. D. Prestage, R. L. Tjoelker, and L. Maleki, Phys. Rev. Lett. 74, 3511 (1995).
42. V. A. Dzuba, V. V. Flambaum, and J. K. Webb, Phys. Rev. Lett. 82, 888 (1999); Phys. Rev. A 59, 230

(1999).

43. T. Udem, S. A. Diddams, K. R. Vogel, C. W. Oates, E. A. Curtis, W. D. Lee, W. M. Itano, R. E.

Drullinger, J. C. Bergquist, and L. Hollberg, Phys. Rev. Lett. 86, 4996 (2001);
S. Bize, S. A. Diddams, U. Tanaka, C. E. Tanner, W. H. Oskay, R. E. Drullinger, T. E. Parker, T. P.

2005 NRC Canada

46

unknown Vol. 99, 2005

Heavner, S. R. Jefferts, L. Hollberg, W. M. Itano, D. J. Wineland, and J. C. Bergquist, Phys. Rev. Lett.
90, 150802 (2003).

44. M. Niering, R. Holzwarth, J. Reichert, P. Pokasov, Th. Udem, M. Weitz, T. W. H¨ansch, P. Lemonde, G.
Santarelli, M. Abgrall, P. Laurent, C. Salomon, and A. Clairon, Phys. Rev. Lett. 84, 5496 (2000); M.
Fischer et al., Phys. Rev. Lett. 92, 230802 (2004).

45. J. Stenger, C. Tamm, N. Haverkamp, S. Weyers, and H. R. Telle, Opt. Lett. 26, 1589 (2001);

E. Peik, B. Lipphardt, H. Schnatz, T. Schneider, Chr. Tamm, S. G. Karshenboim, Phys. Rev. Lett. 93,
170801 (2004).

46. G. Wilpers et al. , Phys. Rev. Lett. 89, 230801 (2002); F. Riehle et al. in Ref. [36], p. 229.
47. H. S. Margolis, G. P. Barwood, G. Huang, H. A. Klein, S. N. Lea, K. Szymaniec and P. Gill, Science

306, 1355 (2004).

48. T. Ido, T. H. Loftus, M. M. Boyd, A. D. Ludlow, K. W. Holman and J. Ye, eprint physics/0410110.
49. H. Marion, F. Pereira Dos Santos, M. Abgrall, S. Zhang, Y. Sortais, S. Bize, I. Maksimovic, D.
Calonico, J. Gruenert, C. Mandache, P. Lemonde, G. Santarelli, Ph. Laurent, A. Clairon, and C.
Salomon, Phys. Rev. Lett. 90, 150801 (2003).

50. P. T. Fisk et al., IEEE Trans. UFFC 44, 344 (1997); P. T. Fisk, Rep. Prog. Phys. 60, 761 (1997); R.
B. Warrington, P. T. H. Fisk, M. J. Wouters, and M. A. Lawn, in Proceedings of the 6th Symposium
Frequency Standards and Metrology, edited by P. Gill (World Scientiﬁc, 2002), p. 297.

51. S. G. Karshenboim, Can. J. Phys. 78, 639 (2000).
52. J. Blatt and V. F. Weisskopf, Theoretical Nuclear Physics. Dover, 1991.
53. J. K. Webb, V. V. Flambaum, C. W. Churchill, M. J. Drinkwater, and J. D. Barrow, Phys. Rev. Lett. 82,

884 (1999);
J. K. Webb, M. T. Murphy, V. V. Flambaum, V. A. Dzuba, J. D. Barrow, C. W. Churchill, J. X.
Prochaska, and A. M. Wolfe Phys. Rev. Lett. 87, 091301 (2001);
R. Srianand, H. Chand, P. Petitjean, and B. Aracil, Phys. Rev. Lett. 92, 121302 (2004).

54. Y. Fujii, In [36], p. 167;

S. K. Lamoreaux and J. R. Torgerson, Phys. Rev. D 69, 121701 (2004)

2005 NRC Canada


Markov Chains of Inﬁnite Order and Asymptotic

Satisfaction of Balance: Application to the Adaptive

Integration Method

David J. Earl and Michael W. Deem

Departments of Bioengineering and Physics & Astronomy

Rice University

6100 Main Street—MS 142

Houston, TX 77005–1892

June 6, 2011

Corresponding author: M. W. Deem, mwdeem@rice.edu, fax: 713–348–5811.

4
0
0
2
 
v
o
N
 
7
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
0
5
1
1
1
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

Abstract

Adaptive Monte Carlo methods can be viewed as implementations of Markov chains

with inﬁnite memory. We derive a general condition for the convergence of a Monte

Carlo method whose history dependence is contained within the simulated density

distribution. In convergent cases, our result implies that the balance condition need

only be satisﬁed asymptotically. As an example, we show that the adaptive integration

method converges.

1 Introduction

Adaptive Monte Carlo methods that change the sampling strategy based upon statis-

tics collected on the ﬂy have been shown to be very powerful in a number of interesting

applications.1–5 Typically these adaptive methods use the statistics collected during a run

to construct an importance sampling potential that is intended to remove the most signiﬁ-

cant barriers to sampling in the problem. These methods, however, have been criticized by

various authors due to their lack of satisfying detailed balance. Although the use of adap-

tive simulation methods is growing, and their success has been demonstrated in a number

of cases,4 widespread acceptance of the correctness of the approach within the simulation

community has been hampered by these questions of detailed balance and so of convergence

of the simulated distribution in phase space. In most of these methods, it is clear that there

is at least one ﬁxed point of the distribution, the Boltzmann, possibly modiﬁed by an impor-

tance sampling factor that is itself a functional of the Boltzmann distribution. As the Monte

Carlo algorithm will be started from an arbitrary initial condition, it is of interest to know

2

whether these algorithms will converge to the Boltzmann, or any other, ﬁxed point. Such

ﬁxed point analysis has not been performed for this class of algorithms and is the subject of

this paper.

As an archetypal example of these adaptive Monte Carlo methods, we consider the

adaptive integration scheme of Fasnacht, Swendsen, and Rosenberg.6 In this method, we

focus on one order parameter, λ, of the system that leads to the most signiﬁcant barriers.

We construct an estimate of the probability distribution of λ, and we use the inverse of this

probability as the importance sampling function:

P (λ0) =

δ(λ(x)

h

λ0)

=

i

−

R

dxe−βU (x)δ(λ(x)
dxe−βU (x)

λ0)

,

−

R

where β is the inverse temperature, and x is a vector in 3n dimensions. We deﬁne

e−βF (λ0) =

dxe−βU (x)δ(λ(x)

λ0) .

−

Z

Then the probability distribution is given by

where the conﬁgurational integral is Z =

dxe−βU (x). Now, consider

P (λ0) =

e−βF (λ0)
Z

,

d ln P (λ0)
dλ0

=

=

λ0)

dxe−βU (x) d

dλ0 δ(λ(x)

dxδ(λ(x)
R

dxe−βU (x)δ(λ(x)
λ0) d
−
dxe−βU (x)δ(λ(x)

−
λ0)
−
dλ0 e−βU (x)
λ0)

R

R

−

R

R

3

(1)

(2)

(3)

=

=

dxe−βU (x) dU

dλ0 δ(λ(x)

dxe−βU (x)δ(λ(x)
dU
R
dλ0 +λ0

,

β

−

R

β

−

*

λ0)

−
λ0)

−

where we have used the fact that both λ and U are functions of x, and dU/dλ really means

U

dx/dλ. Thus, we come to the thermodynamic integration formula for the free energy

∇

·

as a function of λ:

F (λ0) =

dλ

dU
dλ +

*

Z

λ0

λmin

Z

dλ′

=

R

R

dxe−βU (x) dU
dλ δ(λ(x)
dxe−βU (x)δ(λ(x)

λ′)
−
λ′)

.

−

We desire the observed distribution in the Monte Carlo scheme to be

ρ(x) = (const)e−βU(x)/P(λ(x)) = (const)e−βU(x)+βF(x) .

(6)

As the simulation is performed, the estimated value of importance sampling free energy,

ˆF (λ), is constructed from Eqn. 6 and used in Eqn. 5. For example, if the distribution to

date is ρ(x), then we have

ˆF (λ0) =

λ0

dλ′

λmin

Z

dxρ(x) dU
dλ δ(λ(x)
dxρ(x)δ(λ(x)

λ′)
−
λ′)

.

−

R

R

In the context of a Metropolis algorithm, the acceptance criterion would be

acc(o

n) = min

→

1, e−βU (xn)+βU (xo)+β ˆF (λ(xn))−β ˆF (λ(xo))
n

o

.

4

(4)

(5)

(7)

(8)

Although we have written transition probabilities that satisfy detailed balance in Eq. 8,

our analysis is equally applicable to transition probabilities that satisfy only balance.7 Since

the importance sampling function is changing with step number, due to the updates to

density and so to ˆF , this adaptive algorithm does not satisfy detailed balance. It is clear,

however, that if the density has converged to the Boltzmann, ρ(x) = (const)e−βU (x)+βF (x),

then the estimation in Eqn. 7 is exact, and the acceptance criterion in Eqn. 8 is exact

and constant for future steps. Thus, the desired exact answer, Eqn. 6, is a ﬁxed point of

this algorithm. We also note that if the observed density distribution is not exact, but if

ρ(x)/ρ(y) = e−βU (x)+βU (y) whenever λ(x) = λ(y), then the estimated importance sampling

function, Eqn. 7, is also exact. This property will prove to be useful.

The Markov process underlying a Monte Carlo algorithm with acceptance probabili-

ties such as Eqn. 8 has memory, because the process depends on all past states through

the observed density distribution ρ(x). Technically, this is a Markov chain of inﬁnite order.8

Markov chains of inﬁnite order have a solid body of convergence results when the dependence

on the past, essentially, decays exponentially fast (technically, when they are continuous with

respect to the past).9 The Markov process in adaptive integration is dramatically discontinu-

ous with respect to the past: the ﬁrst point observed is just as important as the most recent

point observed in the measured density distribution.

We here consider the inﬁnite order Markov process that occurs in adaptive Monte Carlo.

In Sec. 2, we derive a general condition on the transition matrices for adaptive Monte Carlo

that guaranties convergence to a unique distribution. Although we often use continuous

notation, technically we limit discussion to ﬁnite, discrete spaces, in the belief that the

continuous limit exists as the grid spacing goes to zero. In Sec. 3, we examine the special

5

case of adaptive integration Monte Carlo, showing convergence to a unique distribution

occurs. We discuss our ﬁndings in Sec. 4.

2 Theory

We wish to ﬁnd the conditions under which a Markov chain of inﬁnite order converges to

a unique limiting distribution. We consider the chain to possess a transition probability that

depends on the current and future state and on the density constructed from all previously

observed states, as in Eqns. 7–8. We assume that the transition probabilities satisfy a

generalization of the irreducibility condition: we assume there is c > 0 such that

Prob[x(t), ρ(t)

y(t + m)] > c

→

(9)

for all x, y, ρ, and t for some ﬁxed m. This is a precise statement of our desire that the process

be ergodic, with mixing time m. Thus, we have Prob[x(t)] > c for all times t > m, because

we can apply Eqn. 9 to each of the initial states 1, . . . , m

1, and then iterate to conclude

Prob[x(m′ +m)] =

x′ Prob[x′(m′), ρ(m′)

x(m+m′)]Prob[x′(m′)]

x′ Prob[x′(m′)] =

P

→

c

≥

P

c. In fact, we consider a larger value of m, so that any given Markov chain has an observed

density that equals the expected probability distribution to O(1/√m).10 Now consider a

Markov chain that has run for M Monte Carlo steps, M

m. For this process, it will be

−

≫

true that

ρ(M + m) = ρ(M) + O(m/M) ∼= ρ(M) .

(10)

6

Thus, the transition matrix A[ρ(M)] will be roughly constant during this time interval, since

the density itself is not changing much, and assuming the transition matrix depends smoothly

on the density. The distribution of new states during this time period will converge to

Prob[x(M + m)] = A[ρ(M + m

1)]A[ρ(M + m

2)] . . . A[ρ(M)]ρ(M)

−

−

= A[ρ(M)]mρ(M) + O(m2/M)

= ρ∗[ρ(M)] + O(rm, m2/M) .

(11)

Here r < 1 is the second largest generalized eigenvalue of A[ρ(M)].11 The probability

distribution is driven to the limiting distribution of the transition matrix for large m:

Prob[x(M + m)]

ρ∗[ρ(M)]. By the Frobenius-Perron theorem,11 this limiting distribution

→

depends on the measured density, but not on the state at M: ρ∗ = limm→∞ Am[ρ(M)]ρ(M) =

limm→∞ Am[ρ(M)]ρ for any ρ. By the central limit theorem of Markov processes,10 any likely

instance of this probability distribution will be accurate to O(1/√m). Thus, the contribution

of these m steps to the history-dependent density is

ρ(M + m) =

ρ(M) +

M
M + m

m
M + m

ρ∗[ρ(M)] +

m
M + m

O(

m2
M

, rm, 1/√m) ,

(12)

Since we consider the limit of 1

m2

≪

≪

M, we may drop the error terms. We let u = m

M +m.

Then by the contraction mapping theorem on compact spaces12 there will be a ﬁxed point

to Eqn. 12 if there is a metric, D, such that

D [(1

−

u)ρ1 + uρ∗[ρ1], (1

u)ρ2 + uρ∗[ρ2]]

−

(13)

7

is initially decreasing as u increases from 0, for any arbitrary ρ1 and ρ2. If this condition is

satisﬁed, the ﬁxed point exists and is unique for our ﬁnite, discrete system.12 We note that

if the following is satisﬁed for arbitrary ρ1 and ρ2, then Eqn. 13 is automatically satisﬁed for

D [ρ∗[ρ1], ρ∗[ρ2]] < D[ρ1, ρ2]

(14)

Alternatively, we can consider the uniqueness and existence of the mapping ρn+1 = ρ∗(ρn)

small u:

for arbitrary ρ0.

3 Application to Adaptive Monte Carlo

The general condition, Eqn. 13, seems diﬃcult to check for an arbitrary functional

dependence on the measured density, ρ∗[ρ]. We, thus, specialize consideration to the adaptive

integration method. We rewrite Eqn. 12 as

ρ(t + ∆t) = (1

)ρ +

∆t
t

−

∆t
t

ρ∗[ρ(t)]

where ∆t = m. Assuming that this diﬀerence equation is well-approximated by a diﬀerential

equation, we ﬁnd

(15)

(16)

(17)

(18)

dρ
dt

∆t =

∆tρ
t

−

+

∆t
t

ρ∗[ρ]

dρ
dt

=

(ρ∗[ρ]

1
t

ρ)

−

dρ
d ln t

= ρ∗[ρ]

ρ

−

8

Letting α = ln t,

dρ
dα

= ρ∗[ρ]

ρ

−

(19)

We note that for ρ < ρ∗, ρ increases, whereas for ρ > ρ∗, ρ decreases. Therefore ρ = ρ∗

informally appears to be a stable ﬁxed point.

We now consider more carefully the function ρ∗[ρ]. Letting t

M, we ﬁnd

≫

ρ(t) =

(arbitrary initial ρ) + (1

(20)

M
t

t/m

M
t

)

−

(t

Xi=M/m

−

ρi
M)/m

where the density at time t = i∆t, ρi = ρ∗

i = e−U /Pi(λ), is correct for a given λ, ρi(x)/ρi(y) =

e−βU (x)+βU (y), but for which Pi(λ) has not converged to Eqn. 1. This result for the ratio of

the density follows from Eqns. 6–8. Thus, for a given λ,

Eqns. 6–8 in the limit 1

m2

M

≪

≪

≪

t thus imply that ˆF and ρ are becoming exact:

ρ(t)(x) = (const) exp−U (x) +O(

M
t

) .

ρ∗[ρ] = ρ∗ +

δρ∗
δρ
= ρ∗ + O(M/t) .

δρ

Thus, Eqn. 19 becomes

dρ
dα

= ρ∗[ρ∗ + δρ]

ρ + O(M/t, m2/t, rm, 1/√m)

−

= ρ∗

ρ + O(Me−α, m2e−α, rm, 1/√m) ,

−

9

(21)

(22)

(23)

where we have reinserted the additional error terms from Eqn. 12. Thus,

ρ∗ + O(Me−α, m2e−α, rm, 1/√m)

ρ

→

exponentially fast in α. Thus, in the limit 1

m2

M

≪

≪

≪

t, we ﬁnd

ρ(t)

→

ρ∗ + O(M/t, m2/t, rm, 1/√m, 1/t)

(24)

(25)

as t

→ ∞

, where the errors in Eqn. 25 are respectively from the error on ˆF , the change of the

density during ∆t, the Monte Carlo convergence for a given ˆF , the stochastic convergence

of the density to the distribution, and the convergence of the diﬀerential equation.

4 Discussion

Our analysis of the convergence of the adaptive integration Monte Carlo scheme gives

insight into why the convergence of the method is so rapid. That is, the adaptive method

converges as fast as the underlying Monte Carlo method converges for density distributions

with a given value of λ. Once these estimates have converged, then the sampling over

diﬀerent values of λ is typically exponentially accelerated by the importance sampling bias

introduced by the P (λ) factors in Eqn. 6, so that a simple random walk in the λ order

parameter may be achieved without any barriers to sampling. Thus, if the most signiﬁcant

barriers in the problem are fully captured by the λ order parameter, then the adaptive

integration dramatically speeds up the convergence of the underlying Monte Carlo algorithm.

In the context of the above analysis, an improved importance sampling estimate dramatically

10

reduces the number of steps, m, that it takes to reach equilibrium for a given value of λ.

In the above analysis, full convergence was shown. That is, convergence of both the

density distribution for a given value of λ and the distribution of λ was shown. Since adap-

tive integration Monte Carlo is a form of iteration, we see the linear convergence, as O(1/t),

in Eqn. 25. In the analysis of the Monte Carlo data, we will remove the bias introduced

by the P (λ). That is, we will adjust the observed densities, scaling out the P (λ) depen-

dence by histogram reweighting. Incidentally, we note from Eqn. 20, that when reweighting

the histograms, one should use the the average of the calculated importance sampling fac-

tors,

1/Pi(λ)

, rather than the instantaneous importance sampling factor, 1/Pt(λ). Such a

h

i

reweighting procedure implies by Eqns. 6–7 that once the simulated density has converged

within a given value of λ (in O(m) steps), the reweighted density has also converged for

all λ. So, the slow linear convergence observed in Eqn. 25 is actually not a great concern.

Conversely, our major algorithmic interest is in the exponential reduction of the sampling

time, m, within a given value of λ, which is already induced by an only roughly converged

importance sampling bias, P (λ).

In conclusion, detailed balance, or balance,7 need not be satisﬁed at all times in a

Monte Carlo simulation. Balance need only be satisﬁed asymptotically. Indeed, the desire

to maintain balance comes only from the Markov chain theory that shows such an approach

converges to a unique limiting distribution. Any Monte Carlo method that converges to the

speciﬁed unique limiting distribution will be equally valid. Given the success of the adaptive

Monte Carlo methods, it would appear that the importance of detailed balance is over-rated.

11

The authors thank David J. Aldous for pointing out the relevance of the contraction

condition, Eqn. 13. This work was supported by the U.S. Department of Energy Oﬃce of

Acknowledgments

Basic Energy Sciences.

References

[1] Mezei, M. J. Comput. Phys. 1987, 68, 237–248.

[2] Smith, G. R.; Bruce, A. D. J. Phys. A 1995, 28, 6623–6643.

[3] Engkvist, O.; Karlstrom, G. Chem. Phys. 1996, 213, 63–76.

[4] Wang, F.; Landau, D. P. Phys. Rev. Lett. 2001, 86, 2050–2053.

[5] Wang, J. S.; Swendsen, R. H. J. Stat. Phys. 2002, 106, 245–285.

[6] Fasnacht, M.; Swendsen, R. H.; Rosenberg, J. M. Phys. Rev. E 2004, 69, 056704.

[7] Manousiouthakis, V. I.; Deem, M. W. J. Chem. Phys. 1999, 110, 2753–2756.

[8] Harris, T. E. Paciﬁc Journal of Mathematics 1955, 5, 707-724.

[9] Fern´andez, R.; Maillard, G. Electronic Journal of Probability 2004, 9, 145-176.

[10] Norris, J. R. Markov Chains; Cambridge University Press: New York, 1997 Convergence

in every possible state may require m much larger than the total number of states.

[11] Gantmacher, F. R. Matrix Theory; volume 1 Chelsea: New York, 1959.

12

[12] Lax, P. D. Functional Analysis; John Wiley & Sons: New York, 2002.

13


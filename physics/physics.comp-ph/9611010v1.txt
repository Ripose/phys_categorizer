6
9
9
1
 
v
o
N
 
2
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
0
1
1
6
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Quasi-Monte Carlo, Discrepancies
and Error Estimates1

Jiri Hoogland2
NIKHEF, Amsterdam, The Netherlands

Fred James3
CERN, Geneva, Switzerland

Ronald Kleiss4
University of Nijmegen, Nijmegen, The Netherlands

Abstract

We discuss the problem of deﬁning an estimate for the error in Quasi-Monte
Carlo integration. The key issue is the deﬁnition of an ensemble of quasi-
random point sets that, on the one hand, includes a suﬃciency of equivalent
point sets, and on the other hand uses information on the degree of uniformity
of the point set actually used, in the form of a discrepancy or diaphony. A
few examples of such discrepancies are given. We derive the distribution of
our error estimate in the limit of large number of points.
In many cases,
Gaussian central limits are obtained. We also present numerical results for
the quadratic star-discrepancy for a number of quasi-random sequences.

1 The error problem

We discuss the problem of integration of a square integrable function over
the s-dimensional unit hypercube K = [0, 1)s, using a set of N points xk,
1presented at the 2nd International Conference on Monte Carlo and Quasi-Monte Carlo

Methods in Scientiﬁc Computing, Salzburg, Austria, july 9-12,1996

2e-mail: t96@nikhefh.nikhef.nl, research supported by Stichting FOM.
3e-mail: james@mail.cern.ch
4e-mail: kleiss@sci.kun.nl,research supported by Stichting FOM.

1

k = 1, 2, . . . , N. The actual integral is J =
estimate is given by

K dxf (x), and its numerical
R

S =

N

1
N

f (xk) .

(1)

Xk=1
Depending on the way in which the points xk are chosen, we distinguish
diﬀerent integration methods: if the points come from some predetermined,
deterministic scheme we have a quadrature rule, if they are considered to be
iid uniform random numbers, we have Monte Carlo. An intermediate case is
that of Quasi-Monte Carlo, where the points are considered to be part of a
low-discrepancy sequence, but share the ergodic properties of a truly random
sequence5. The integration error is deﬁned as η
J. Good integration
methods are characterized by the fact that they typically lead to a small
value of η, but, more importantly, allow one to obtain a good estimate of
η. In the case of Monte Carlo, η is a stochastic variable, and hence has a
probability density P (η). For quasi-random point sets used in Quasi-Monte
Carlo, we may (as we shall specify more precisely later on) also interpret η as
having such a probability density: its form is the subject of this contribution.

≡

−

S

In true Monte Carlo, the error distribution P (η) is obtained by viewing
as a typical member of an ensemble of random
the point set
point sets, governed by the obvious Cartesian combined probability density

x1, x2, . . . , xN }

{

PN (x1, x2, . . . , xN ) = 1 ,

(2)

so that the xk are indeed iid uniform random numbers. The error η is then
a random variable over this ensemble, with the following well-known results.
In the ﬁrst place, S is an unbiased estimator of J in the sense that
= 0,
where the average is over the ensemble of points sets. In the second place,
for large N, P (N) approaches a normal distribution according to the Central
Limit Theorem. Finally, the variance of this distribution is given by
=
Var(f )/N, where Var denotes the variance. Note that, since we average
over the integration points, the error distribution can depend only on the
integrand itself.

η2
h

η

i

h

i

The conceptual problem in the use of quasi-random rather than truly
random point sets is the following: a quasi-random point set is not a ‘typical’
set of random points! Indeed, quasi-random point sets are very special, with
carefully built-in correlations between the various points so that each new
point tends to ‘ﬁll a gap’ left by the previous ones. The usual Monte Car-
lo error estimate is therefore not really justiﬁed in Quasi-Monte Carlo. On
the other hand, many diﬀerent error bounds assure us that small errors are
possible, and indeed likely when we apply low-discrepancy point sets. In the
following, we shall discuss two approaches to a solution of this conundrum.
Obviously, we can only summarize the main results here: technical details
and pictures can be found in the references.

5We shall not discuss the case of point sets with ﬁxed, predetermined N .

2 The Bayesian approach

The ﬁrst way around the aforementioned conceptual problem is to inter-
change the rˆoles of integrand and point set: we view the integrand f (x) as
a typical member of some underlying class of functions and average over
this class, so that the error depends only on a property of the point set. In
practice, the choice of function class often entails a good deal of idealism or
pot luck, as usual in a Bayesian approach to probability. We discuss several
examples, in which we denote by
hif an average over the probability measure
governing the function class.

2.1 The Wo´zniakowski Lemma

Let the integrand f (x) be chosen according to the Wiener (sheet) measure
in s dimensions. This measure is Gaussian, with

f (x)

if = 0 ,

h

h

f (x), f (y)

if =

min(xµ, yµ) ,

(3)

s

Yµ=1

where the index µ labels the coordinates. We may then quote the Lemma
from [1]:

η

if = 0 ,

h

η2

f

E

D

= D2(x∗

1, x∗

2, . . . , x∗

M ) ,

(4)

where D2 stands for the L2 norm of the well-known star-discrepancy, and
the x∗
xµ. In [2] it is shown,
moreover, that the distribution P (η) in this case is a Gaussian.

k denotes the ‘reﬂected’ point, with (x∗

k)µ = 1

−

We have here the interesting general fact that the choice of a particular
function class induces its own particular discrepancy. On the other hand, in
many cases (such as in particle physics phenomenology) the Wiener measure
is certainly not appropriate since it is dominated by integrands that are
not locally smooth. In [3], folded Wiener sheet measures are studied with
analogous results, but then again these describe functions that are much too
smooth.

2.2

Induced discrepancies

In [2], we established the following general result. Let the measure on the
function class be such that

f (x1)

if = 0 ,

f (x1)f (x2)
h

if =

h

Z

dy h(x1, y)h(x2, y)

(5)

for all x1,2 in K, for some h(x, y). There is then an induced quadratic dis-
crepancy, deﬁned as follows:

η2

f
E

D

Z

=

dy g(y)2

,

g(y) =

h(xk, y)

dx h(x, y) .

(6)

1
N

N

Xk=1

− ZK

Note that h is not necessarily in the same function class as the f , and indeed
y may be deﬁned in a space quite diﬀerent from K. Note that, whenever
the function class measure is Gaussian, then P (η) will also be Gaussian.
Generalizations to higher moments can be found in [2, 4].

2.3 Orthonormal function bases

(7)

(8)

As an example in s = 1, let un(x) be an orthonormal set of functions on K,
as follows:

u0(x) = 1 ,

dx um(x)un(x) = δm,n ,

ZK

with m, n = 0, 1, 2, . . .. Let f (x) admit of a decomposition

f (x) =

vnun(x) ,

Xn≥0
and choose the measure such that the vn are normally distributed around
zero, with variance σ2
is then
deﬁned6 as

n. The induced quadratic discrepancy Dorth

2

=

f

1
N

η2
D

Dorth
2

, Dorth

2 =

un(xk)un(xl) .

(9)

1
N

N

σ2
n

Xk,l=1
A special case is that of the Fourier class, which has

Xn>0

E

u2n = √2 cos(2πnx) , u2n−1 = √2 sin(2πnx) , n = 1, 2, 3, . . .

(10)

The physically reasonable requirement that the phase of each mode n (made
up from u2n−1 and u2n) is uniformly distributed forces us to have the Gaus-
sian measure, with in addition σ2n−1 = σ2n, which property corresponds to
translational invariance. We then have

2

Dorth
2

=

2
N

N

σ2
2n

exp(2iπnxk)

.

(11)

(cid:12)
Xk=1
(cid:12)
(cid:12)
(cid:12)
Obviously, other orthonormal function bases are also possible, such as the
(cid:12)
system of Walsh functions; a further discussion, including the straightforward
generalization to higher dimension, can be found in [4, 5, 6, 7]. Note that all
such quadratic discrepancies are nonnegative by construction.

Xn>0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

3 The discrepancy-based approach

Another way of establishing integration error estimates, which in our opinion
does more justice to the spirit of Monte Carlo, is the following.
Instead
of considering all point sets of N truly random points, with the Cartesian
probability density (2), we restrict ourselves to those point sets that have a
given value of discrepancy, for some predeﬁned type of discrepancy. In this
way, information on the discrepancy performance of one’s favorite quasi-ran-
dom number sequence can be incorporated in the error estimate.

3.1 Non-Cartesian distribution of points

We have then, instead of (2), a combined probability density PN for the N
points as follows. Let DN (x1, . . . , xN ) be some discrepancy deﬁned on sets
of N points in K, and suppose its value for the actual point set that is used
in the integration be w. Then,

H(w) =

dx1

dxN δ (DN (x1, . . . , xN )

w) ,

−

· · ·

ZK

1
H(w)
1
N

−

PN (w; x1, . . . , xN ) =

δ (DN (x1, . . . , xN )

w)

−

= 1

FN (w; x1, . . . , xN ) ,

(12)

so that FN measures the deviation from Cartesian (iid) uniformity. The
quantity H(w) is of course just the probability density for the discrepancy
over the iid uniform random numbers, an object interesting in its own right.
Let us also deﬁne marginal deviations as

Fk(w; x1, . . . , xk) =

dxk+1

dxN FN (w; x1, . . . , xN ) .

(13)

ZK

· · ·

We can then simply establish, for instance, that, provided F1(w; x) vanishes
for all x,

η2
D

E
η

=

1
N 

= 0 ,

N

1
−
N ZK

−

Var(f )

dxdy f (x)f (y)F2(w; x, y)

,





(14)

h

hi

i
now denotes averaging with respect to PN (w; .). It is seen that we
where
may expect a reduced error if F2 is positive when x and y are ‘close’ together
in some sense, i.e. if the points in the point set ‘repel’ each other. Note that
(1/N), deviation from uniformity is suﬃcient.
only a small,

O

3.2 Error probability distribution

In many cases,it is actually possible to compute the F2 mentioned above.
In fact, especially in the case of discrepancies deﬁned using orthonormal
function bases, we can do much more. Using a Feynman-diagram technique
described in detail in [6, 8], we can establish results for P (η) as an asymptotic
expansion in 1/N. To leading order, we have

N/2π

+i∞

P (η) = q

2πiH(w)

1

dz

exp

A(z)

B(z)

"

η2N
2B(z) #

−

,

Z−i∞

Xn>0

1
2
v2
n
2zσ2

,

A(z) =

wz

−

−

B(z) =

1

X

−

q
log(1

2zσ2

n) ,

−

(15)

where the z integral runs to the left of any singularities. This result holds, for
N asymptotically large, for any discrepancy measure based on orthonormal
functions as discussed above, and, moreover, for any reasonable f , even if
it is not in the function class based on these orthonormal functions. The
1/N corrections are fully calculable, although we have not done so yet. Two
corollaries follow immediately. In the ﬁrst place,

∞

Z0

dw H(w)P (η) =

N/2πV e−η2N/2V

, V

v2
n = Var(f ) ,

(16)

q

≡

Xn>0

which recovers the Central Limit Theorem valid over the whole ensemble of
N-point point sets with any w. In the second place, we obtain an integral
representation for H(w) by insisiting that P (η) be normalized to unity:

H(w) =

1
2πi

+i∞

Z−i∞

dz exp

zw

"−

−

log(1

2zσ2
n)

.

#

−

(17)

1
2

Xn>0

Generalizations of these results only aﬀect the sums over n.

3.3 Application 1: equal strengths

A simple model for a discrepancy is obtained by taking σn = 1/2M for n =
1, 2, . . . , 2M, and zero otherwise (with trivial extension to more dimensions).
Let us then decompose the variance of the integrand as follows:

V = Var(f ) =

v2
n = V1 + V2 , V1 =

v2
n , V2 =

v2
n ,

(18)

Xn>0

Xn>2M

so that V1 contains that part of the variance to which the discrepancy is
sensitive (the ‘covered’ part) and V2 the rest (the ‘uncovered’ part). We then
have

2M

Xn=1

H(w) =

wM −1e−M w

exp

M M
Γ(M)

P (η)

N
2π(wV1 + V2) !

∼  

exp

 −

∼

1/2

,

1)2

(cid:18)−

−

(w

M
2
η2N
2(wV1 + v2) !

(cid:19)

,

(19)

where the approximations are valid for large M. We see that a new central
limit theorem holds, where the variance of f has been modiﬁed so that its
covered part is reduced by a factor w, according to intuition.

3.4 Application 2: harmonic model in one dimension

Let us concentrate on the case s = 1, and take σ2n−1 = σ2n = 1/n, so that
f is, on the average, square integrable, but its derivative is not. In that case
we have,

1)m−1m2e−wm2/2 ,

(20)

H(w) =

(

−

X

which is, apart from a trivial scaling, precisely the probability density of
the Kolmogorov-Smirnov statistic. This is somewhat surprising since that
statistic is based on the L∞ norm of the standard star-discrepancy, a to-
tally diﬀerent object. In addition, we conjecture, that for values of w small
compared to its expectation value

= π2/3, we shall have

w

P (η)

exp

∝

"−

i

h
η2N
2C #

, C = Var(f )

;

(21)

w
w
h

i

this would again indicate a reduction of the error estimate for small w. To
date, we have not yet been able to prove this assertion.

3.5 The quadratic star-discrepancy

We have also obtained some results for the quadratic form of the standard
star-discrepanc [4, 5]. Although this is not based on orthonormal functions
and the analysis is hence more complicated, we have obtained the moment-
for asymptotically large N, where w now
generating function G(z) =
stands for N times the quadratic star-discrepancy. More precisely, we have

ezw

h

i

1) log(1

2zn) ,

G(z) = exp(ψ(z))/

χ(z) ,

q
Qs(2n

−

Xn>0

ψ(z) =

χ(z) =

1
2
−
2s
2z

Xn>0
zn = (4/π2)s

Qs(2n

1)

1

−

2z

(2n

1)2 .

−

,

zn

−

zn

−
Here, Qs(m) is the number of ways in which an odd positive integer m can
be written as a product of s positive integers, including 1’s. The function
H(w) can now be computed numerically for diﬀerent s values. We have done
so, and ﬁnd that H(w) very slowly approaches a Gaussian distribution as s
increases. Indeed, the skewness of H(w) is, for large s, approximately given
by (216/225)s/2 so that the approach to normality is indeed slow.

(22)

4 Numerical results for the quadratic star-

discrepancy

Since we now have H(w) for the quadratic star-discrepancy, we can reliably
judge how well quasi-random number generators perform, since we can com-
pare the discrepancy of their output with the behaviour of truly random
points. Space does not permit us to show pictures, which can be found in
[4, 5]. Here, we just describe the results. We have computed the quadratic
star-discrepancy for a good pseudorandom generator (RANLUX), and for the

q

w

−

−h

)/
i

exactly, and by Monte Carlo. The latter method is actually faster for N
larger than about 50,000 if we ask for 5% accuracy. We made runs of up to
N = 150, 000, and considered the lowest and highest discrepancy value in
subsequent intervals of 1000. These we compared with the expected value
3−s)/N for truly random points, and also plotted the standardized7
(2−s
form ξ(w) = (w
Var(w). We considered dimensions from 1 up to 20.
In all dimensions, RANLUX appears to mimic a truly random sequence quite
well. The quasi-random generators perform very well in low dimensions, and
generally the discrepancy falls further and further below than of a random
sequence as N increases. There are exceptions, however:
for instance, the
Sobol’ sequence for s = 11 degrades, and is actually worse than random
for N
60, 000, again rapidly improving for larger N. Apart from taking
this as a warning, we have not investigated the reason for such behaviour
in detail. The biggest surprise was when we plotted the variable ξ(w) (for
instance, for N = 150, 000) as a function of s. It appears that, as measured
in this way, the performance of all quasi-random generators improves with
increasing s! For s larger than 15 or so, all generators become rather bad,
which is of course due to the fact that the onset of the asymptotic regime
occurs for larger N as s increases. But what is more striking is the fact that
the old-fashioned and simple Richtmyer generator performs as well as the
modern, sophisticated Sobol’ and Niederreiter sequences. We take this as an
indication that the Richtmyer generator deserves more study, in particular
since we have not attempted any optimization of its ‘lattice point’.

∼

References

[1] H. Wo´zniakoski, Bull. AMS 24(1991)185.

[2] R. Kleiss, Comp. Phys. Comm. 71(1992)39.

[3] S. Paskov, J. Complexity 9(1993)291.

[4] J.K. Hoogland, Ph.D. thesis, University of Amsterdam, 1996.

[5] F. James, J. Hoogand and R. Kleiss, Multidimensional sampling for
simulation and integration: measures, discrepancies and quasi-random
numbers, to appear in Comp. Phys. Comm.

[6] J. Hoogland and R. Kleiss, Discrepancy-based error estimates for Quasi-
Monte Carlo. I: General formalism, Comp. Phys. Comm. 98 (1996) 111.

[7] J. Hoogland and R. Kleiss, Discrepancy-based error estimates for Quasi-
Monte Carlo. II: Results for one dimension, Comp. Phys. Comm.
98(1996)128.

[8] J. Hoogland and R. Kleiss, Discrepancy-based error estimates for Quasi-
Monte Carlo. III: Error distributions and central limits, in preparation.


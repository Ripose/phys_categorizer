0
0
0
2
 
v
o
N
 
1
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
3
5
0
1
1
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Faster Evaluation of Multidimensional
Integrals

A. Papageorgiou
J.F. Traub

Department of Computer Science
Columbia University
New York, NY 10027
June 1997

Abstract

In a recent paper Keister proposed two quadrature rules as alternatives to Monte
Carlo for certain multidimensional integrals and reported his test results. In earlier
work we had shown that the quasi-Monte Carlo method with generalized Faure points
is very eﬀective for a variety of high dimensional integrals occuring in mathematical
ﬁnance. In this paper we report test results of this method on Keister’s examples of
dimension 9 and 25, and also for examples of dimension 60, 80 and 100.

For the 25 dimensional integral we achieved accuracy of 10−2 with less than 500
points while the two methods tested by Keister used more than 220, 000 points. In all of
our tests, for n sample points we obtained an empirical convergence rate proportional
to n−1 rather than the n−1/2 of Monte Carlo.

1 Introduction

Keister [1] points out that multi-dimensional integrals arise frequently in many branches
of physics. He rules out product rules of one-dimensional methods because the number of
integrand evaluations required grows exponentially in the number of dimensions. He observes
that although Monte Carlo (MC) methods are desirable in high dimension, a large number,
n, of integrand evaluations can be required since the expected error decreases as n−1/2.

This motivates Keister to seek non-product rules for a certain class of integrands deﬁned
below. He proposes two quadrature rules, one by Mc Namee and Stenger (MS) [2], and a
second due to Genz and Patterson (GP) [3],[4], which he tests on a speciﬁc example of his
class of integrands.

In this paper we report test results on Keister’s example using quasi-Monte Carlo (QMC)
methods. QMC methods evaluate the integrand at deterministic points in contrast to MC

1

methods which evaluate the integrand at random points. The deterministic points belong to
low discrepancy sequences which, roughly speaking, are uniformly spread as we will see in the
next section. Niederreiter [5] is an authoritative monograph on low discrepancy sequences,
their properties, and their applications to multi-dimensional integration.

The Koksma-Hlawka inequality (see the next section for a precise statement) states that
low discrepancy sequences yield a worst case error for multivariate integration bounded by
a multiple of (log n)d/n, where n is the number of evaluations and d is the dimension of
the integrand. A similar bound on the average error is implied by Wo´zniakowski’s theorem
[6]. The proof of this theorem is based on concepts and results from information-based
complexity [7].

For d ﬁxed and n large, the error (log n)d/n beats the MC error n−1/2. But for n ﬁxed
and d large, the (log n)d/n factor looks ominous. Therefore, it was believed that QMC
methods should not be used for high-dimensional problems; d = 12 was considered high
[8, p. 204]. Traub and a then Ph.D. student, Paskov, decided to test the eﬃcacy of QMC
methods for the valuation of ﬁnancial derivatives. Software construction and testing of QMC
methods for ﬁnancial applications was began in Fall 1992. The ﬁrst tests were run on a very
diﬃcult ﬁnancial derivative in 360 dimensions, which required 105 ﬂoating point operations
per evaluation. Surprisingly, QMC methods consistently beat MC methods.

The ﬁrst published announcement was in January 1994 [9]. Details appeared in [10], [11],
[12]. Tests by other researchers [13], [14] lead to similar conclusions for the high-dimensional
problems of mathematical ﬁnance.

These results are empirical. A number of hypotheses have been advanced to explain the
observed results. One of these is that, due to the discounted value of money, the ﬁnancial
problems are highly non-isotropic with some dimensions far more important than others.
Perhaps the QMC methods take advantage of this. A generally accepted explanation is not
yet available.

Since Keister’s test integral is isotropic it provides an example which is very diﬀerent
than the examples from mathematical ﬁnance. To our surprise the QMC method beat both
MC and two other methods tested by Keister by very convincing margins.

The problems in [1] require the computation of a weighted multi-dimensional integral

where d is the dimension of the problem, f : Rd
ρ(x), x

Rd satisﬁes

→

∈

R is a smooth function, and the weight

with η(
and changes of sign of the variables. The example in [1] (see also [15]) is

xj) = η(xj), xj ∈
−

R. Thus, the weight is symmetric with respect to permutations

f (x)ρ(x) dx,

ZRd

ρ(x) =

η(xj),

d

Yj=1

cos(

x
k

k

ZRd

)e−||x||2

dx,

2

(1)

(2)

(3)

where

k · k

denotes the Euclidean norm in Rd.

The integral in (3) can be reduced, via a change of variable, to a one-dimensional integral
which can be analytically integrated. As we will see, the QMC method takes advantage of
the dependence on the norm automatically and provides a numerical solution with error
similar to a one-dimensional integral.

The QMC method that we test in this paper uses points from the generalized Faure
sequence, which was constructed by Tezuka [16]. We will refer to it as QMC-GF. This
sequence has been very successful in solving problems of mathematical ﬁnance [12], [14].

The performance of QMC-GF on the integral (3) is most impressive. For example, for
the 25-dimensional integral it achieves error 10−2 using less than 500 points, far superior to
all the other methods. Its error over the range we tested, which was up to 106 points, was
n−1, with c < 110, d = 9, 25, 80, 60, 100. That may be compared with the MC method
c
whose error was proportional to n−1/2.

·

We summarize the remainder of this paper.

In the next section we provide a brief
introduction to low discrepancy sequences. Test results are given in the third section. A
summary of our results and future research concludes the paper.

2 Low Discrepancy Sequences

Discrepancy is a measure of deviation from uniformity of a sequence of real numbers. In
particular, the discrepancy of n points x1, . . . , xn ∈
A(E; n)
n

1, is deﬁned by

[0, 1]d, d

λ(E)

D(d)

(4)

≥

,

−

n = sup
E (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the supremum is taken over all the subsets of [0, 1]d of the form E = [0, t1)
[0, td),
d, λ denotes the Lebesgue measure, and A(E; n) denotes the number
0
of the xj that are contained in E. A detailed analysis of low discrepancy sequences can be
found in [5] and in the references therein.

tj ≤

×· · ·×

1, 1

≤

≤

≤

j

A sequence x1, x2, . . . of points in [0, 1]d is a low discrepancy sequence iﬀ

D(d)

n ≤

c(d)

(log n)d
n

,

∀

n > 1,

where the constant c(d) depends only on the dimension d. Neiderreiter, see [5], gives a general
method for constructing (t, d)-sequences, t
0, which are low discrepancy sequences. The
discrepancy of the ﬁrst n points in a (t, d)-sequence is given by

≥

D(d)

n ≤

c(t, d, b)

(log n)d
n

+ O

(log n)d−1
n

,

(cid:19)

(cid:18)

where b
bt/d!

≥

·

2 is an integer parameter, upon which the sequence depends, and c(t, d, b)

(b/2 log b)d. Hence, the value t = 0 is desirable.

(5)

≈

3

The generalized Faure sequence [16] is a (0, d) sequence and is obtained as follows. For

a prime number b

d and n = 0, 1, . . . , consider the base b representation of n, i.e.,

≥

[0, b) are integers, i = 0, 1, . . . . The j-th coordinate of the point xn is then

where ai(n)
given by

∈

where

n =

ai(n)bi,

∞

Xi=0

x(j)
n =

x(j)
nk b−k−1, 1

j

d,

≤

≤

∞

Xk=0

x(j)
nk =

c(j)
ks as(n).

∞

Xs=0

j

−

The matrix C (j) = (c(j)
ks ) is called the generator matrix of the sequence and is given by
C (j) = A(j)P j−1, where A(j) is a nonsingular lower triangular matrix and P j−1 denotes the
j

1 power of the Pascal matrix, 1
We conclude this section by stating the Koksma-Hlawka inequality which establishes the
relationship between low discrepancy sequences and multivariate integration, see [5]. If f
is a real function, deﬁned on [0, 1]d, of bounded variation, V (f ), in the sense of Hardy and
Krause, then for any sequence x1, . . . , xn ∈
1
n

[0, 1)d we have

V (f )D(d)
n .

f (x) dx

d.

≤

≤

n

≤

−

(cid:12)
Z[0,1]d
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1

f (xi)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

3 Methods and Test Results

We transform the integral (3) to one over the cube [0, 1]d. We have

Id(cos) =

cos(

x
k

k

ZRd

)e−kxk2

dx = 2−d/2

cos(

y

k

k

ZRd

/√2)e−kyk2/2 dy

(6)

= πd/2

cos(

y

/√2)

ZRd

k

k

e−kyk2/2
(2π)d/2 dy

= πd/2

Z[0,1]d

d

cos 

v
u
u
t



Xj=1

(φ−1)2(tj)/2

dt,



where φ is the cummulative normal distribution function with mean 0 and variance 1,

φ(u) =

1
√2π Z

u

−∞

e−s2/2 ds, u

,

[
−∞

∈

∞

].

4

We obtain the n deterministic sample points xi = (xi1,...,xid)

xij = φ−1(tij), where ti = (ti1, . . . , tid)
low discrepancy sequence. Our method, Id,n, is deﬁned by:

∈

Rd, i = 1, . . . , n, by setting
[0, 1]d, i = 1, . . . , n, are n consecutive terms of a

∈

πd/2
n

n

Xi=1

d

v
u
u
t

Xj=1

Id,n(cos) =

cos 

(φ−1)2(ti,j)/2

.

(7)


Our test problem could be reduced to a one-dimensional integral. We did not do this
because we wanted test how QMC methods perform on d-dimensional integrals. As we
will see, the empirical rate of convergence of QMC-GF is n−1 which suggests that this
method takes advantage of the dependence on the norm automatically without a dimension
reducing transformation. A method corresponding to (7) can be derived for the more general
integration problem (1) with weight function satisfying (2).



We report test results. We used the generalized Faure1 low discrepancy sequence [16]
to derive the sample points for the QMC method. We remind the reader that we call this
the QMC-GF method. We compared this method to the McNamee-Stenger (MS) and Genz-
Patterson (GP) [2], [3], [4] methods. We also tested using a Monte Carlo method of the form
(7), i.e., using randomly generated points ti,j. Hence, we use the same change of variable for
QMC-GF and MC.

−

·
·

The value, Id(cos), of the integral (6) is, see [1], I9(cos) =

106. We used Mathematica to compute I60(cos) = 4.89052986
1019 and I100(cos) = 4.57024396

71.633234291 and I25(cos) =
1014, I80(cos) =
1.356914
−
1024. We measure the accuracy of an approxi-
6.78878724
·
mation by computing its relative error (fractional deviation). We observe the least number
of sample points required by an method to achieve and maintain a relative error below a
speciﬁed level, e.g. 10−3, until the end of the simulation. We introduced this more conser-
vative way of assessing the performance of a method in [12]. Thus, we study the error of
an method throughout a simulation. We believe that this has advantages over performance
reports that are based only on values at the end of a simulation.
We summarize our ﬁndings and then provide some details.

·

The QMC-GF method outperforms the MS and GP methods for d = 25.

The MS and GP methods are sensitive to the dimension. They perform quite well
for d = 9 and very poorly for d = 25. For example, for d = 25 and for accuracy of
the order 10−2 these methods use some 220, 000 points while the QMC-GF method
uses less than 500 points. Therefore, they should only be used when the dimension is
relatively low.

The QMC-GF method performs well for d = 9, 25, 60, 80 and 100.

The relative error of the QMC-GF method is bounded by

≤
1 The generalized Faure and the Sobol’ low discrepancy sequences are included in FINDER, a Columbia

n−1, cd < 110, n

106, d = 9, 25, 60, 80, 100.

(8)

cd ·

University software system, and are available to researchers upon request by writing the authors.

•

•

•

•

5

Note that this is an empirical conclusion. We write cd to suggest that, in principle,
this constant depends on d although we did not see a strong dependence in our tests.

The QMC-GF method achieves relative error 10−2 using about 500 points.

•

The relative error of the MC method is bounded by β

n−1/2 as predicted by the theory.

•
First we consider the case d = 9. The performance of the QMC-GF, and the GP methods
is comparable for accuracy less than 10−4. The relative error of the MS method ﬂuctuates
about the value 10−4 for sample sizes between 36, 967 and 96, 745 points, see [1, Table I], and
is slower than the QMC-GF method since it requires at least four times as many function
evaluations. (For this level of accuracy the MC method requires more than 106 points).

·

For d = 25 the results are striking. The MS and GP methods require about 220, 000
points for accuracy of order 10−2 while the QMC-GF method requires less than 500 points.
Table I is from [1, Table II] and exhibits the performance of the MS and GP methods.

Method Number of Points
1, 251
19, 751
20, 901
227, 001
244, 101
Table I. Comparison of MS and GP methods, d=25

Relative Error
2.00
0.40
0.75
0.06
0.07

GP and MS
GP
MS
GP
MS

Table II summarizes the performance of the QMC-GF method.

Method Number of Points
500
QMC-GF
1, 200
QMC-GF
14, 500
QMC-GF
214, 000
QMC-GF
Table II. The Quasi-Monte Carlo method, d=25

Relative Error
10−2
10−3

10−4
10−5

5
5

·
·

As we mentioned above, we are using a very conservative criterion when we report relative
error. It takes about 219, 000, 490, 000, and many more than 106 points for the MC method
to reach accuracies of 10−3, 5

10−5, respectively.

10−4, and 5

Figure 1 exhibits the relative error of the QMC-GF method for d = 25. The horizontal
axis shows the sample size n, while the vertical axis shows the relative error. The horizontal
lines depict the accuracy.

Figure 2 shows the convergence rate of the QMC-GF method. We plot the logarithm of
the relative error as a function of the logarithm of the sample size for d = 25 and obtain the
linear convergence summarized in (8).

·

·

Recently, Keister [17] obtained good results using a public domain version of the Sobol’

low discrepancy sequence.

Keister [1] did not perform tests for d > 25. We tested the QMC-GF method for d = 60,
80 and 100 and we found that its performance is comparable to that of the lower values of

6

d. We did not ﬁnd evidence suggesting that its performance suﬀers as the dimension grows.
This is shown in the empirical error equation (8) and is further demonstrated in Figure 3,
which shows the convergence of QMC-GF for d = 100. In particular, in Figure 3 we plot the
logarithm of the relative error as a function of the logarithm of the sample size.

4 Summary and Future Research

We have shown that the QMC-GF method beats MC methods and the MS and GP meth-
ods by a wide margin for Keister’s 25-dimensional example. We have also shown that its
good performance is maintained when the dimension takes much higher values. Other high
dimensional problems motivated by applications to physics should be tested.

Extensive testing on a variety of high-dimensional integrals which occur in mathematical
ﬁnance also ﬁnd QMC methods consistently beating the MC method. Preliminary results
from our tests on high-dimensional integrals arising from several very diﬀerent applications
again point to the superiority of QMC over MC.

The results are empirical. There is currently no theory which explains why, for a variety
of applications, QMC methods are much better than one would expect from the Koksma-
Hlawka inequality or from Wo´zniakowski’s theorem. Finding the theoretical justiﬁcation
for the superiority of QMC methods for certain classes of integrands is a most important
direction of future research.

Acknowledgments

We thank Bradley Keister for his comments on a draft of this paper. We are grateful to
Richard Palmer for directing us to Bradley Keister’s paper, and to Henryk Wo´zniakowski
for his comments on the manuscript.

References

119–122, 1996.

[1] Keister, B.D., Multidimensional Quadrature Algorithms, Computers in Physics, 10:20,

[2] Mc Namee, J., and Stenger, F., Construction of Fully Symmetric Numerical Integration

Formulas, Numer. Math., 10, 327–344, 1967.

[3] Genz, A., A Lagrange Extrapolation Algorithm for Sequences of Approximations to

Multiple Integrals, SIAM J. Sci. Stat. Comput., 3, 160–172, 1982.

[4] Patterson, T.N.L., The Optimum Addition of Points to Quadrature Formulae, Mathe-

matics of Computation, 22, 847–856, 1968.

7

[5] Niederreiter, H., Random Number Generation and Quasi-Monte Carlo Methods, CBMS-

NSF Regional Conference Series in Applied Math. No. 63, SIAM, 1992.

[6] Wo´zniakowski, H., Average case complexity of multivariate integration, Bulletin of the

American Mathematical Society, 24, 185–194, 1991.

[7] Traub, J.F., Wasilkowski, G.W., and Wo´zniakowski, H., Information-Based Complexity,

Academic Press, New York, 1988.

[8] Bratley, P., Fox, B.L., and Niederreiter, H., Implementation and Tests of Low-
Discrepancy Sequences, ACM Trans. on Modeling and Computer Simulation, 2:3, 195–
213, 1992.

[9] Traub, J.F. and Wo´zniakowski, H., Breaking Intractability, Scientiﬁc American, 270,

102–107, 1994.

[10] Paskov, S.H. and Traub, J.F., Faster Valuation of Financial Derivatives, The Journal

of Portfolio Management, 113–120, Fall 1995.

[11] Paskov, S.H., New Methodologies for Valuing Derivatives, in Mathematics of Deriva-
tive Securities, S. Pliska and M. Dempster eds., Isaac Newton Institute, Cambridge
University Press, Cambridge, UK, 1997.

[12] Papageorgiou, A., and Traub, J.F., Beating Monte Carlo, Risk, 9:6, 63–65, 1996.

[13] Joy, C., Boyle, P.P., and Tan, K.S., Quasi-Monte Carlo Methods in Numerical Finance,
working paper, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1, 1995.

[14] Ninomiya, S., and Tezuka, S., Toward real-time pricing of complex ﬁnancial derivatives,

Applied Mathematical Finance, 3, 1–20, 1996.

[15] Capstick, S., and Keister, B.D., Multidimensional quadrature algorithms at higher de-

gree and/or dimension, Journal of Computational Physics, 123, 267–273, 1996.

[16] Tezuka, S., Uniform Random Numbers: Theory and Practice, Kluwer Academic Pub-

lishers, Boston, 1995.

[17] Keister, B.D., Private communication, 1997.

8

0.0025

0.002

0.0015

0.001

0.0005

0

0.1

0.01

0.001

0.0001

1e-05

1e-06

1e-07

1e-08

5000 10000 15000 20000 25000 30000 35000 40000 45000 50000

Figure 1. QMC-GF, relative error as a function of the sample size, d=25

1000

10000

100000

1e+06

Figure 2. QMC-GF, log(relative error) as a function of log(sample size), d=25

9

0.1

0.01

0.001

0.0001

1e-05

1e-06

1e-07

1e-08

1000

10000

100000

1e+06

Figure 3. QMC-GF, log(relative error) as a function of log(sample size), d=100

10


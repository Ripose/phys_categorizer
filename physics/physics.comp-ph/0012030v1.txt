0
0
0
2
 
c
e
D
 
4
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
0
3
0
2
1
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

MONTE CARLO METHODS: APPLICATION TO HYDROGEN GAS
AND HARD SPHERES

BY

MARK DOUGLAS DEWING

B.S., Michigan Technological University, 1993
M.S., University of Illinois at Urbana-Champaign, 1995

THESIS
Submitted in partial fulﬁllment of the requirements
for the degree of Doctor of Philosophy in Physics
in the Graduate College of the
University of Illinois at Urbana-Champaign, 2001

Urbana, Illinois

MONTE CARLO METHODS: APPLICATION TO HYDROGEN GAS
AND HARD SPHERES

Mark Douglas Dewing, Ph.D.
Department of Physics
University of Illinois at Urbana-Champaign, 2001
David M. Ceperley, Advisor

Quantum Monte Carlo (QMC) methods are among the most accurate for computing
ground state properties of quantum systems. The two major types of QMC we use are Vari-
ational Monte Carlo (VMC), which evaluates integrals arising from the variational princi-
ple, and Diffusion Monte Carlo (DMC), which stochastically projects to the ground state

from a trial wave function. These methods are applied to a system of boson hard spheres
to get exact, inﬁnite system size results for the ground state at several densities.

The kinds of problems that can be simulated with Monte Carlo methods are expanded
through the development of new algorithms for combining a QMC simulation with a classi-

cal Monte Carlo simulation, which we call Coupled Electronic-Ionic Monte Carlo (CEIMC).
The new CEIMC method is applied to a system of molecular hydrogen at temperatures
ranging from 2800K to 4500K and densities from 0.25 to 0.46 g/cm3.

VMC requires optimizing a parameterized wave function to ﬁnd the minimum energy.
We examine several techniques for optimizing VMC wave functions, focusing on the ability
to optimize parameters appearing in the Slater determinant.

Classical Monte Carlo simulations use an empirical interatomic potential to compute

equilibrium properties of various states of matter. The CEIMC method replaces the empir-
ical potential with a QMC calculation of the electronic energy. This is similar in spirit to
the Car-Parrinello technique, which uses Density Functional Theory for the electrons and
molecular dynamics for the nuclei.

The challenges in constructing an efﬁcient CEIMC simulation center mostly around the
noisy results generated from the QMC computations of the electronic energy. We introduce
two complementary techniques, one for tolerating the noise and the other for reducing

it. The penalty method modiﬁes the Metropolis acceptance ratio to tolerate noise without
introducing a bias in the simulation of the nuclei. For reducing the noise, we introduce
the two-sided energy difference method, which uses correlated sampling to compute the
energy change associated with a trial move of the nuclear coordinates. Unlike the standard

reweighting method, it remains stable as the energy difference increases.

iii

Acknowledgments

First I would like to thank my advisor, David Ceperley, for supporting me in this research,

for teaching these Monte Carlo methods to me, and for being available and patient when
answering questions.

I would also like to thank the graduate students and postdocs in the group who have
helped me and provided useful and interesting discussions. And special thanks to Tadashi

Ogitsu for the use of his DFT code.

I am grateful to my parents for their support during my college and graduate school
pursuits. I enjoyed the refreshing summer visits to their farm. Thanks to my brother Luke,

whose living in Colorado was convenient for ski trips.

During my time here, I have beneﬁted greatly from friendships and relationships with
people in Graduate Intervarsity Christian Fellowship, Grace Community Church, and Illini
Life Christian Fellowship. They have given me a great deal of strength and encouragement

when I needed it.

ing this work.

My work was supported by the computational facilities at NCSA, by a Graduate Re-
search Trainee fellowship NSF Grant No. DGE93-54978, and by NSF Grant No. DMR
98-02373.

And ﬁnally, apologies to my cat, Lucy, for not giving her enough attention while ﬁnish-

iv

Contents

1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
. .

1.1 Thesis Overview . .

. .

. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.

.

.

.

.

2 Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .

2.1 Basic Monte Carlo Integration . .

. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.

.

. .
2.2 Metropolis Sampling .
2.3 Classical Monte Carlo .
. .
2.4 Variational Monte Carlo . .

.
.
.
2.4.1 Two Level Sampling .

.
.

2.5 Diffusion Monte Carlo .
.
.

Fermions . .
. .

2.6 Statistical Errors .

2.5.1

.
.

. .
. .
. .

.
.
.

. .
. .
. .
. .

. .
. .
. .

2.7 Wave Functions .
. .
2.8 Periodic Boundary Conditions . .

. .

. .

.

.

.

.
.
.
.

.
.
.

.
.

.
.
.
.

.
.
.

.
.

. .
. .
. .
. .

. .
. .
. .

. .
. .

.
.
.
.

.
.
.

.
.

. .
. .
. .
. .

. .
. .
. .

. .
. .

1
3

4
4

5
7
7
8

. .
. .
. .
. .

. .
9
. . 14
. . 15

. . 16
. . 16

. . 22
. . 23
. . 25
. . 27

. . 28

.
.
.
.

.
.
.

.
.

.
.
.
.

.

.
.
.
.

.
.
.

.
.

.
.
.
.

.

. .
. .
. .
. .

. .
. .
. .

. .
. .

. .
. .
. .
. .

. .

.
.
.
.

.
.
.

.
.

.
.
.
.

.

.
.
.
.

.
.
.

.
.

.
.
.
.

.

. .
. .
. .
. .

. .
. .
. .

. .
. .

. .
. .
. .
. .

. .

.
.
.
.

.
.
.

.
.

.
.
.
.

.

. .
. .
. .
. .

. .
. .
. .

. .
. .

. .
. .
. .
. .

. .

.
.
.
.

.
.
.

.
.

.
.
.
.

.

.
.
.
.

.
.
.

.
.

.
.
.
.

.

3 Energy Difference Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
. . 20
.
. . 21
.

3.1 Direct Difference . .
. .
3.2 Reweighting . .

. .
. .

. .
. .

. .
. .

. .
. .

. .
. .

. .
. .

. .
. .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

3.3 Bennett’s Method for Free Energy Differences .
. .
3.4 Two-Sided Sampling .
. .
.
3.5 Examples .
. .

. .
. .
.
3.5.1 Diffusion Monte Carlo . .

. .
. .
. .

. .
. .

.
.
.

.
.
.

.
.
.

. .

. .

.
.

.
.

3.5.2 Binding Energy .

. .

.

. .

.

.

. .

.

. .

4 Wave Function Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
. . 32
. . 32
. . 33

4.1 Energy vs. Variance Minimization .
.
4.2 Fixed Sample Reweighting .
.
. .
4.3 Newton Method .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

. .

.
.

.

.

4.4 Stochastic Gradient Approximation .

. .

.

. .

.

.

. .

.

.

. .

.

. .

.

.

. . 36

v

5 Coupled Simulation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4.5 Gradient Biased Random Walk . .
. .
4.6 Comparison of methods .
. .
.
4.7 Future Work . .

. .
. .

. .

.
.

.

.

5.1 Penalty Method .

.

. .
. .
5.1.1 Other methods
. .
5.1.2 Handling noisy data
. .

. .

.
.

.

.

.

5.2 Pre-rejection . .

5.3 Trial Moves . .
. .
5.4 Single H2 molecule .

.

.
.

.
.

. .
. .

.
.
.
.

.
.

. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.

.
.

.
.
.

.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.

.
.

.
.
.

.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.

.
.

.
.
.

.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.

.
.

.
.
.

.
.
.
.

.
.

. . 40
. . 40
. . 42

. . 43
. . 45
. . 45
. . 46

. . 48
. . 50

6 Hard Spheres . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
. . 52
.
. . 55
.

. .
6.1 Wave function .
6.2 Finite Size Effects . .

. .
. .

. .
. .

. .
. .

. .
. .

. .
. .

. .
. .

. .
. .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

6.3 Distribution Functions and Condensate Fraction .

.

. .

.

.

. .

.

. .

.

.

. . 59

7 Hydrogen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
. . 62
.
. . 63
.
. . 64
.

7.1 Experiment
.
.
.
.
7.2 Theory .
7.3 Pressure and Kinetic Energy .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .
. .

. .
. .

. .
. .

. .
. .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

7.4
. .
7.5 Results .
7.6 Simulation analysis .

Individual Conﬁgurations . .
. .
. .
. .

.
.

.
.

.

.

.

7.7 Future Work . .

.

. .

.

.

. .

.
.
.

.

. .
. .
. .

. .

.
.
.

.

.
.
.

.

. .
. .
. .

. .

.
.
.

.

. .
. .
. .

. .

.
.
.

.

.
.
.

.

. .
. .
. .

. .

.
.
.

.

.
.
.

.

. .
. .
. .

. .

.
.
.

.

. .
. .
. .

. .

.
.
.

.

.
.
.

.

. . 64
. . 65
. . 68

. . 69

8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

A Determinant Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

B Elements of the Local Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

C Cusp Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

Vita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

vi

List of Tables

2.1 Timings for Li2 molecule using the standard sampling method. All times

are in seconds on an SGI Origin 2000.

. .

. .

. .

. .

. . 10

2.2 Timings for Li2 molecule using the two level sampling method. All times

are in seconds on an SGI Origin 2000.

. .

. .

. .

. .

. . 10

2.3 Timings for the system of 32 H2 molecules in a periodic box using the
standard sampling method. All times are in seconds on a Sun Ultra 5.
2.4 Timings for a system of 32 H2 molecules in a periodic box using the two

.

. . 10

level sampling method. All times are in seconds on a Sun Ultra 5.

.

. . 11

.

.

.

.

.

.

. .

. .

2.5 Comparison of energies and variances for various forms for orbitals and

. .
Jastrow factors for a single H2 molecule.
. .
2.6 Values of variational parameters for H2.
2.7 Values of variational parameters for wave function E .

. .
. .

.
.

.
.

.
.

.

. .
. .
. .

. .
. .
. .

. . 18
. . 19
. . 19

.

.

.

.
.
.

.

.

.

.
.
.

.

.

.
.
.

.

.

.
.
.

.

.

.
.
.

5.1 Efﬁciency of classical Monte Carlo for moving several particles at once.
The table on the left is for low density system at rs = 3.0 and T=5000K.
The table on the right is for a high density system at rs = 1.8 and T=3000K.
The largest values of the efﬁciency are shown in boxes.

.
5.2 Results of CEIMC for isolated H2 molecule at T=5000K.

. .

. .

. .

. .

.

.

.

.

.

.

.

.

. . 49

. . 50

6.1 Variational parameters for hard sphere gas .
6.2 Energy extrapolated to inﬁnite system size (in units of

. .

.

.

. .

6.3 Condensate fraction

.

.

. .

.

. .

.

.

. .

.

. .

.

.

. .

.
.
¯h2
ms 2 )
.
.

. .
.

. .

.
.

.

. .
. .

. .

.
.

.

.
.

.

. . 53
. . 58

. . 61

7.1 Pressure from simulations and shock wave experiments .
7.2 Energy from simulations and models, relative to the ground state of an iso-
lated H2 molecule. The H2 column is a single thermally excited molecule
.
.
plus the quantum vibrational KE.

. .

. .

. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.

.

.

.

. . 67

. . 67

7.3 Average molecular H2 bond length. The H2 column is a single thermally
. .

excited molecule in free space.

. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.

.

. . 67

vii

7.4 Simulation quantities ordered according to average noise level, b

. The
time column is the time for a single quantum step in minutes on an SGI

Origin 2000. N is the number of molecules in the simulation.

.

. .

.

.

. . 70

viii

s
List of Figures

. . 11

. . 15

. . 26

. . 26
. . 27
. . 29

. . 30

. . 36

is for 32 H2 molecules.

2.1 Efﬁciency of VMC. The graph on the left is for Li2. The graph on the right
. .
2.2 Examples of statistical data analysis using reblocking. The error in the
graph on the left has converged, while the error in the graph on the right
.
.
has not.

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

2.3 Optimized electron-electron Jastrow factor for different forms.

.

. .

.

.

. . 18

.

.

.

.

. .

. .

3.1 Two H2 molecules in a parallel conﬁguration .
3.2 Energy difference (left) and the estimated statistical error (on logscale)
(right) for two H2 molecules in a parallel conﬁguration, starting from d=2.5
.
.
Bohr.

. .
.
.
.
.
3.3 Finite sample size bias in the energy difference of Li2.
3.4 Error in energy difference of Li2 using DMC (top) and VMC (bottom) .
.
3.5 Error in VMC binding energy of H2-H2 system .

. .
. .

. .
. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

4.1 Examples using the Newton iteration with varying amounts of noise.
4.2 Examples of SGA. The graph on the top shows the convergence of one
variational parameter for several SGA algorithms. The graph on the bottom

.

shows the resulting energy.

. .
4.3 Optimization methods applied to (a) Single H2 (b) 8 H2’s (c) 16 H2’s (d)
. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

. .

32 H2’s

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . 39

. . 41

5.1 CEIMC program outlines. Boxes indicate quantum computations. The
dashed box indicates a quantity saved from a previous computation. The
top algorithm is incorrect. The bottom algorithm is correct.

5.2 Examples on a Lennard-Jones potential with synthetic noise.
5.3 H2 bond length distribution.

. .

. .

. .

. .

.

.

.

.

.

.

.

.

. .

6.1 Time step error for (a) r = 0.01 (b) r = 0.05 (c) r = 0.1 (d) r = 0.2
6.2 S(k) for (a) r = 0.05 (b) r = 0.2 .
.
. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.
.

.
.

.

. .
. .

. .

.
.

.

.
.

.

.

. . 47
. . 48

. . 50

. . 54
. . 55

ix

6.3 VMC ﬁnite size effects for (a) r = 0.01 (b) r = 0.05 (c) r = 0.1 (d) r = 0.2 56
6.4 DMC ﬁnite size effects for (a) r = 0.01 (b) r = 0.05 (c) r = 0.1 (d) r = 0.2 57
. . 59
.
6.5 Energy vs. density .
. .
.
.
. . 60
6.6 Pair distribution function for several densities .
.
. . 60
6.7 Single particle density matrix for several densities
. . 61
.
6.8 Condensate fraction vs. density

. .
. .
. .
. .

. .
. .
. .
. .

. .
. .
. .
. .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

. .

. .

. .

. .

. .

.
.

.

.

.

.

.

.

.

.

.

7.1 Electronic energy for several conﬁgurations computed by several methods.

The energy is relative to an isolated H2 molecule.

.

. .

.

.

. .

.

. .

.

.

. . 66

7.2 Proton pair distribution function g(r) for (a) rs = 2.1 and T=4530 K (b)
. .

. . 67
.
.
7.3 The proton pair distribution function, g(r), close to rs = 1.8 and T = 3000K. 68

rs = 2.202 and T=2820 K .

. .

. .

. .

. .

. .

.

.

.

.

.

.

.

.

.

x

Guide to Notation

CEIMC

Coupled Electronic-Ionic Monte Carlo

DMC

Diffusion Monte Carlo

DFT

Density Functional Theory

GBRW

Gradient Biased Random Walk

PIMC

Path Integral Monte Carlo

QMC

Quantum Monte Carlo

SGA

Stochastic Gradient Approximation

VMC

Variational Monte Carlo

LDA

Local Density Approximation (in Density Functional Theory)

a

a0

A(s

s′)

→

General variational parameter.

Instantaneous acceptance probability in penalty method.

Bohr radius, unit of length in atomic units.
9 m.
1 a0 = 5.29

10−

×

Acceptance probability in Metropolis method.

Inverse temperature,

1
kBT .

QMC estimate of an energy difference.

Exact energy difference. Also the sampling box size in the Metropo-
lis algorithm.

d

Bond length.

xi

a
b
d
D
D

Slater determinant.

A variational parameter in H2 orbitals.

A trial wave function (also denoted y T ). Also the relative noise
parameter.

f

EL

q v

h

H

Ha

kB

K

n0

N

P

s′)

Q

rs

ri j

Hartree, unit of energy in atomic units. 1 Ha = 27.21 eV.

G(R

→

R′, t )

Green’s function propagator in DMC.

Additional noise rejection ratio.

The local energy of a trial wave function.

Vibrational temperature.

Step size parameter in SGA and GBRW.

A many-body Hamiltonian.

Boltzmann factor.

Kinetic energy.

¯h2/2m, where m is the particle’s mass.

Condensate fraction.

The number of particles in a system.

p (s)

Probability distribution to be sampled in a Markov process.

Pressure, or a probability distribution in the two-sided method.

P (s

→

Transition probability in a Markov chain.

Normalization integral .

Density.

r 1(r)

Single particle density matrix.

1/3

3
4p n

, where n is the electron number density.

(cid:0)
The separation between particle j and particle i.

(cid:1)

xii

z
h
l
r
R

The coordinates of all the particles in a many-body system.

Noise level (variance or standard error). Also the hard sphere

diameter.

State in conﬁguration space.

DMC time step .

Temperature.

T (s

s′)

→

Sampling distribution in Metropolis method.

u(r)

Jastrow factor in a many-body wave function.

Volume of the simulation cell, or potential energy.

Alternate notation for potential energy (for formulas that have
both potential energy and volume).

Weight factor in correlated sampling methods.

Width of H2 orbital.

A trial wave function (also denoted f ).

The exact many body ground state wave function.

A single particle orbital.

s

T

V

V

w

wl

y T

f 0

xiii

s
t
f
Chapter 1

Introduction

The ﬁrst computer simulations of a condensed matter system used the simplest potential,
the hard sphere (Metropolis et al., 1953). As computers and simulations progressed, more

sophisticated and realistic potentials came into use. These potentials are parameterized
and then ﬁt to reproduce various experimental quantities. Both Molecular Dynamics (MD)
and Monte Carlo (MC) methods are used to generate ensemble averages of many-particle

systems.

These potentials originate from the microscopic structure of matter, described in terms
of electrons, nuclei, and the Schr¨odinger equation. But the many-body Schr¨odinger equa-
tion is too complicated to solve directly, so some approximations are needed. The one

electron approximation is a successful approach, where a single electron interacts with an
external potential (ie, the nuclei) and with a mean ﬁeld generated by all the other electrons.
This is done by Hartree-Fock (HF) or with Density Functional Theory (DFT) (Parr and
Yang, 1989). DFT is in principle exact, but contains an unknown exchange and correla-

tion functional that must be approximated. The most common one is the Local Density
Approximation (LDA).

These ﬁrst principles calculations are used in ﬁtting the potentials, which are then used

in an MC or MD computation. But the problem of transferability still remains. Empirical
potentials are only valid in situations for which they have been designed and ﬁtted.

In 1985, Car and Parrinello introduced their method, which replaced the empirical po-
tential with a DFT calculation done ‘on the ﬂy’ (Car and Parrinello, 1985). They did a

molecular dynamics simulation of the nuclei of liquid silicon and then computed the den-
sity functional energy of the electrons at every MD step. To improve the efﬁciency of the
computation of the DFT energy, they introduced a new iterative method for solving the

DFT equations. It has been a very successful method, with the original paper being cited
over 2300 times since its publication.

1

Previously, the DFT equations had been solved by eigenvalue methods. But eigenvalue
problems can also be regarded as optimization problems, where an energy functional is
minimized. Car and Parrinello used an idea similar to simulated annealing, but they used

molecular dynamics to move through parameter space, rather than Monte Carlo. This had
the effect of making the equations of motion similar between the electronic problem and the
nuclear problem, with the only difference being in the relative masses. Since the electronic

problem was not real electron dynamics, the electron mass does not correspond to any
physical quantity, and is only a parameter controlling the convergence of the electronic part
of the simulation. Since then, other iterative methods have been introduced, usually based
on the Conjugate-Gradient method (Payne et al., 1992).

A brief review of applications of the Car-Parrinello method to liquid problems is given
by Sprik (2000). This review also mentions that LDA and some other functionals are not
good enough to accurately simulate water (there are improved functionals that are accept-

able). Another review of molecular dynamics by Tuckerman and Martyna (2000) includes
material on treating the nuclei classically and also using path integrals to treat the nuclei
quantum mechanically, done by Marx and Parrinello (1996).

Quantum Monte Carlo (QMC) methods have developed as another means for accurately

solving the many body Schr¨odinger equation (Hammond et al., 1994; Anderson, 1995;
Ceperley and Mitas, 1996). The success of QMC lies partly in the fact these methods
explicitly include correlation among the electrons, which can not be done directly with the
one electron methods. Particularly with the Local Density Approximation (LDA), DFT has

known difﬁculties in handling electron correlation (Grossman et al., 1995).

In the spirit of the Car-Parrinello method, we integrate a Classical Monte Carlo sim-
ulation of the nuclei with a QMC simulation for the electrons. This we call Coupled

Electronic-Ionic Monte Carlo (CEIMC). There are some challenges in constructing an ef-
ﬁcient method.

The ﬁrst problem we encounter is that the results of a QMC simulation are noisy. The
QMC energy has some uncertainty associated with it, and it could bias the classical part of

the simulation. We could run the QMC simulation until the noise is negligible, but that is
very time-consuming. A better way is use the penalty method, which modiﬁes the usual
MC formulas to be tolerant of noise.

The electrons are assumed to be in their ground state, both in the Car-Parrinello method

and in our CEIMC method. There are two internal effects that could excite the electrons
- coupling to nuclear motion and thermal excitations. In the ﬁrst case, we make the Born-
Oppenheimer approximation, where the nuclei are so much more massive than the electrons

that the electrons are assumed to respond to nuclear motion instantaneously, and so stay in

2

their ground state. We neglect any occupation of excited states of the electrons due to
coupling to nuclear motion.

In the case of thermal excitation, let us examine several relevant energy scales. If we
consider a gas of degenerate electrons at a density of n = 0.0298 electrons per cubic Bohr
(i.e. rs =
= 2.0), the Fermi temperature is about 140,000K. The gap between the
ground state and the ﬁrst excited state of a hydrogen molecule at equilibrium bond distance

3
4p n

1/3

(cid:0)

(cid:1)

is about 124,000K. As long as our temperatures are well below this (and they are), and we
are not at too high pressures (pressure decreases the gap), the thermal occupation of excited
states can be neglected.

Hydrogen is the most abundant element in the universe, making an understanding of its

properties important, particularly for astrophysical applications. Models of the interiors of
the gas giant planets depends on a knowledge of the equation of state of hydrogen (Hubbard
and Stevenson, 1984; Stevenson, 1988). Hydrogen is also the simplest element, but it still

displays remarkable variety in its properties and phase diagram. It has several solid phases
at low temperature, and the crystal structure of one of them (phase III) is not fully known
yet. At high temperature and pressure the ﬂuid becomes metallic, but the exact nature of
the transition is not known.

Computer simulation can also be used to obtain results on model systems. We will
examine the hard sphere Bose gas, a simple and important model. For this model, all
the approximations we make are controllable, and we will look at how to deal with those
approximations and obtain exact results for this model.

1.1 Thesis Overview

Chapter 2 is an introduction to the basic classical and quantum Monte Carlo techniques
we will be using. Chapter 3 presents an improved QMC method for computing the energy

difference between two systems. Chapter 4 is an examination of parameter optimization,
which is essential in VMC. We present various methods for minimizing the energy, and
give some comparisons between them.

Successful CEIMC simulations are based on the penalty method for tolerating noise in
the Metropolis method, which is detailed in Chapter 5. Some additional details are dis-
cussed, and an example of CEIMC applied to a single H2 molecule is given. The results of
computations of the ground state energy of the boson hard sphere model are presented in

Chapter 6. In Chapter 7, the CEIMC simulation method is applied to ﬂuid molecular hy-
drogen. We present data for a few state points and perform some analysis of the simulation
itself.

3

Chapter 2

Monte Carlo Methods

Monte Carlo integration methods are very useful for evaluating the basic integrals of sta-
tistical and quantum physics. In a system with Np particles, these integrals have the form

dR p (R)O(R)
dR p (R)

O

=

h

i

R

(2.1)

where R is a 3Np dimensional vector, p (R) is a probability distribution, and O(R) is the
R
observable or quantity of interest. These integrals have two important characteristics: high

dimensionality and the integrands are sharply peaked - only small parts of phase space
contribute signiﬁcantly to the integral.

The high dimensionality makes a grid based scheme impractical in two ways. First, sup-

pose we have a 300 dimensional integral (100 particle simulation), and want 10 grid points
in each dimension. Even this crude integration requires function evaluations at 10300 grid
points! Second, consider the trapezoidal rule (as a concrete example) in d dimensions. The
error using N samples will go as O (N−
2/d). As we will show, the error in Monte Carlo
integration goes as O (N−
1/2). The Monte Carlo error is independent of the dimensional-
ity whereas the grid based method depends on it strongly. For these high dimensionality
problems, Monte Carlo is only practical choice.

2.1 Basic Monte Carlo Integration

Consider an integral of the form

To evaluate by Monte Carlo, compute f (x) at N points sampled uniformly from [0, 1]. An
approximation to I is given by

(2.2)

(2.3)

I =

f (x)dx.

1

0

Z

¯f =

I

≈

f (xi)

N

i=1

1
N

4

(cid:229)
The estimate of the statistical error in ¯f will be

I = s

f /√N

where s 2

f is the variance, and is given by

s 2

f =

1

−

(N

1)

N

i=1

(cid:0)

f (xi)

−

2

¯f

(cid:1)

Thus the error goes as O (N−

1/2).

The error bounds can be improved by sampling more points, or by reducing the vari-
f . The latter can be accomplished with importance sampling. Consider some prob-

ance, s 2
ability, P(x), that is an approximation to f (x). Write Eq. (2.2) as

The estimate of I is obtained by sampling N points from P(x) and computing

I =

P(x)

1

0

Z

f (x)
P(x)

dx.

N

I

≈

i=1

f (xi)
P(xi)

If P is a good approximation to f , then the variance of the sum in Eq. (2.7) is much less
than the variance of the sum in Eq. (2.3).

The fact that the integrands of interest are sharply peaked, as mentioned previously,
makes importance sampling a necessity. The most useful type of importance sampling for
these problems is the Metropolis method.

2.2 Metropolis Sampling

The Metropolis method (Metropolis et al., 1953) uses a Markov process to generate sam-
ples from a normalized probability distribution, p (R)/
dR p (R). These samples are then
used to estimate Eq. (2.1) by

R

For generality in the following section, we will denote the state of the simulation by s.

A Markov process takes a transition probability between states, P (s

s′), and con-
structs a series of state points s1, s2, ... (called a chain). An important characteristic of a
Markov process is that the choice of the next state point in the chain depends only on the
current state point, not any previous state points.

→

¯O =

1
N

i

O(Ri)

5

(2.4)

(2.5)

(2.6)

(2.7)

(2.8)

s
(cid:229)
(cid:229)
(cid:229)
The Metropolis method constructs a transition probability such that generated state
points are sampled from the desired distribution. For this to work, the transition probability
must satisfy ergodicity. This means the Markov chain must eventually be able to reach any

state in the system. A sufﬁcient condition for satisfying ergodicity is detailed balance,

p (s)P (s

s′) = p (s′)P (s′

→

s).

→

The generalized Metropolis method breaks the transition probability into the product
s′) and an acceptance probability

of two pieces - an a priori sampling distribution T (s
A(s

s′). The Metropolis choice for the acceptance probability is

→

→

A(s

s′) = min

1,

→

(cid:20)

p (s′)T (s′ →
s)
p (s)T (s
s′)
→

(cid:21)

The procedure is to sample a trial state, s′, according to T (s

s′) and evaluate Eq.
(2.10). The acceptance probability is compared with a uniform random number on [0, 1].
If A is greater than the random number, the move is accepted, s′ becomes the new s and is
used in the average in Eq (2.8). Otherwise the move is rejected, s is not changed, and is
reused in the average.

→

The original Metropolis procedure chooses a trial position uniformly inside a box cen-

tered around the current point,

s′ = s + y

(2.11)

where y is a uniform random number on [
In this case, T is uniform and will cancel out of Eq. (2.10).

−

D /2, D /2], with D being an adjustable parameter.

An important measure of the Metropolis procedure is the acceptance ratio - the ratio
of accepted moves to the number of trial moves. It can be adjusted by the choice of D
.
If the acceptance ratio is too small, state space will be explored very slowly because very

few moves are accepted. If the acceptance ratio is high, it is likely that the trial moves
are too small and once again, diffusion through state space will be very slow. Balancing
these considerations leads to the standard rule of thumb that the optimal acceptance ratio is
around 50%.

A better consideration is maximization of the efﬁciency,

(2.9)

(2.10)

(2.12)

where T is the computer time taken to get an error estimate of s

.

x =

1
s 2T

6

2.3 Classical Monte Carlo

The probability distribution we wish to sample is the Boltzmann distribution

p (s) (cid:181)

exp[

V (s)/kT ].

−

(2.13)

The ﬁrst simulations of this type were done with the hard sphere potential (Metropolis
et al., 1953; Wood and Jacobson, 1957). Later simulations used Lennard-Jones potentials,

and then other types of empirical potentials.

The Metropolis procedure samples only the normalized p (s). Averages over this distri-
bution are readily computed, but quantities that depend on the value of the normalization
are difﬁcult to compute. In classical systems, this includes quantities such the entropy and

the free energy. There are techniques, however, for computing the free energy difference
between two systems.

2.4 Variational Monte Carlo

Variational Monte Carlo (VMC) is based on evaluating the integral that arises from the
variational principle. The variational principle states that the energy from applying the
Hamiltonian to a trial wave function must be greater than or equal to the exact ground

state energy. Typically the wave function is parameterized and then optimized with respect
to those parameters to ﬁnd the minimum energy (or minimum variance of the energy).
Monte Carlo is needed because the wave function contains explicit two (or higher) particle
correlations and this results in a non-factoring high dimensional integral.

The energy is written as

dRy T (R)Hy T (R)
dRy T (R)2

E =

R

=

R

dR

y T (R)
|
dR

2 EL(R)
|
2
y T (R)
|

|

(2.14)

R

R
, and is called the local energy. Other diagonal matrix elements can be

where EL = Hy T
y T
evaluated in a similar fashion. Off diagonal elements can also be evaluated, but with more
effort. McMillan (1965) introduced the use of Metropolis sampling for evaluating this
integral.

A typical form of the variational wave function is a Jastrow factor (two body correla-

tions) multiplied by two Slater determinants of one body orbitals.

y T = exp

u(ri j)

Det

S↑

Det

S↓

"−

i< j

#

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(2.15)

7

(cid:229)
The Jastrow factor, u, will contain electron-nucleus and electron-electron correlations. Ap-
pendix B has details on the derivatives that enter into the local energy.

As two electrons or an electron and a nucleus get close, there is a singularity in the

Coulomb potential. That singularity needs to be canceled by kinetic energy terms in the
wave function. This requirement is known as the cusp condition. Details are given in
Appendix C.

Techniques for the efﬁcient handling of the determinants were developed by Ceperley
et al. (1977). The VMC algorithm is implemented so that only single electron trial moves
are proposed. This causes a change in only one column of the Slater matrix. The new
determinant and its derivatives can be computed in O (N) operations, given the inverse of
the old Slater matrix. This inverse is computed once at the beginning of the simulation and
then updated whenever a trial move is accepted. The update takes O (N2) operations. By
comparison, computing the determinant directly takes O (N3) operations. This technique
creates a situation where there is more work done for an acceptance than for a rejection. A
lower acceptance ratio will be faster, since fewer updates need to be performed. Details of
the updating procedure and some other properties of determinants are given in Appendix A.
Optimization of the parameters in the wave function is a large topic, so we will defer
the discussion until a later chapter. However, we will make one observation here. If y T is
an eigenstate, the local energy becomes constant and any MC estimate for the energy will
have zero variance. This zero-variance principle allows searching for optimum parameters
by minimizing the variance rather than minimizing the energy. In principle this is true for

any eigenstate, not just the ground state.

2.4.1 Two Level Sampling

A multilevel sampling approach can be used to increase the efﬁciency of VMC (Dewing,

2000). Multilevel sampling has been used extensively in path integral Monte Carlo (Ceper-
ley, 1995). The general idea is to use a coarse approximation to the desired probability
function for an initial accept/reject step. If it is accepted at this ﬁrst level, a more accurate
approximation is used, and another accept/reject step is made. This continues until the

move is rejected or until the most detailed level has been reached. This method increases
the speed of the calculation because the entire probability function need not be computed
every time.

Consider splitting the wave function into two factors - the single body part (D) and
U ). Treat the single body part as the ﬁrst level, and the whole wave

the two body part (e−
function as the second level.

8

First, a trial move, R′, is proposed and accepted with probability

(cid:20)
If it is accepted at this stage, the two body part is computed and the trial move is accepted
with probability

(cid:21)

A1 = min

1,

D2(R′)
D2(R)

A2 = min

1,

exp[
exp[

2U (R′)]
2U (R)]

−
−

(cid:21)

(cid:20)

(2.16)

(2.17)

It can be veriﬁed by substitution that this satisﬁes detailed balance in Eq. (2.9). After
an acceptance at this second level, the inverse Slater matrices are updated, as described

previously.

We compared the efﬁciency between the standard sampling method and the two level
sampling method on two test systems: a single Li2 molecule in free space, and a collection
of 32 H2 molecules in a periodic box of side 19.344 a.u. (rs = 3.0). The wave functions are
taken from Reynolds et al. (1982), and will be described in Section 2.7.

The step size, D

, is the obvious parameter to adjust in maximizing the efﬁciency. But
we can also vary the number of steps between computations of EL. The Metropolis method
produces correlated state points (see more on serial correlations in Section 2.6), so succes-
sive samples of EL do not contain much new information. In these tests we sampled EL
every ﬁve steps.

Results for the different sampling methods with Li2 are shown in Tables 2.1 and 2.2.
The Determinant Time and Jastrow Time columns include only the time needed for com-
puting the wave function ratio in the Metropolis method, and not the time for computing the
local energy. The total time column does include the time for computing the local energy.

The efﬁciency is also shown on the left in Figure 2.1.

For the two level method with Li2, the second level acceptance ratio is quite high,

indicating the single body part is a good approximation to the whole wave function.

Results for the collection of H2 molecules are given in Tables 2.3 and 2.4. The efﬁ-

ciency is also shown on the right graph in Figure 2.1.

Comparing the maximum efﬁciency for each sampling method, two level sampling is
39% more efﬁcient than standard sampling for Li2, and 72% more efﬁcient for the collec-
tion of H2’s.

2.5 Diffusion Monte Carlo

Diffusion Monte Carlo (DMC) is a method for computing the ground state wave function.

It typically takes an order of magnitude more computing time than VMC, and is most
efﬁcient when used in conjunction with a good VMC trial function.

9

Table 2.1: Timings for Li2 molecule using the standard sampling method. All times are in
seconds on an SGI Origin 2000.

Acceptance Determinant

Jastrow Total

Time

Time

Time

Ratio

0.610
0.491

0.407
0.349
0.307

1.0
1.5

2.0
2.5
3.0

48.3
48.1

48.2
48.2
48.2

340
340

340
339
339

516
508

503
499
496

1190
1680

1460
1070
800

Table 2.2: Timings for Li2 molecule using the two level sampling method. All times are in
seconds on an SGI Origin 2000.

First Level Second Level Total Acc.
Acc. Ratio
Acc. Ratio

Ratio

1.0
1.5

2.0
2.5
3.0

0.674
0.543

0.447
0.379
0.331

0.899
0.894

0.897
0.902
0.906

Time

400
347

304
276
256

1580
2430

2340
1910
1400

0.606
0.485

0.401
0.342
0.300

Table 2.3: Timings for the system of 32 H2 molecules in a periodic box using the standard
sampling method. All times are in seconds on a Sun Ultra 5.

Acceptance Determinant

Ratio

0.606
0.455

0.338
0.250
0.185
0.139

2.0
3.0

4.0
5.0
6.0
7.0

Jastrow Total
Time
Time

1089
1085

1084
1080
1080
1084

2015
1891

1794
1722
1668
1629

0.61
1.22

1.23
1.06
1.02
0.76

Time

167
167

166
166
164
162

10

D
x
D
x
D
x
Table 2.4: Timings for a system of 32 H2 molecules in a periodic box using the two level
sampling method. All times are in seconds on a Sun Ultra 5.

First Level Second Level Total Acc. Total
Time
Acc. Ratio

Acc. Ratio

Ratio

2.0

3.0
4.0
5.0
6.0

7.0

0.740

0.598
0.468
0.357
0.370

0.204

0.795

0.728
0.681
0.649
0.627

0.609

0.589

0.436
0.319
0.232
0.169

0.124

1804

1421
1185
994
849

740

0.59

1.77
2.11
1.55
1.87

1.46

standard sampling
two level sampling

2800

2400

2000

1600

1200

800

1

1.5

2

2.5

3

2

3

4

5

6

7

Figure 2.1: Efﬁciency of VMC. The graph on the left is for Li2. The graph on the right is
for 32 H2 molecules.

standard sampling
two level sampling

2.2

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

11

D
x
x
D
x
D
(2.18)

(2.19)

(2.20)

Formally, DMC and related methods work by converting the differential form of the
Schr¨odinger equation into an integral equation and solving that integral equation by stochas-
tic methods. From another point of view, the Schr¨odinger equation in imaginary time and

the diffusion equation are very similar, enabling one to use a random process to solve the
imaginary time Schr¨odinger equation. This similarity was recognized early and was pro-
posed as a computational scheme in the early days of computing (Metropolis and Ulam,

1949). Unfortunately, without importance sampling, it is very inefﬁcient computationally.

The ground state wave function can be obtained by the projection

where E0 is the ground state energy. This can be seen by expanding y T in energy eigen-
states,

f 0 = lim
→

t (H

e−

E0)y T
−

t (H

e−

E0)y T = e−
−

t (H

E0) (cid:229)
−

i

i
E0)f n.
−

= (cid:229)

t (En

e−

n

At large t , the contribution from the excited states will decay exponentially, and only the
ground state will remain. To make a practical computation method, we write the projection
in the position basis as

y (R′,t + t ) =

dRy (R,t)G(R

R′; t )

→

Z

t H

R

e−

R′|

and f (R,t) is the wave function after some time t. This equation
where G =
is iterated to get to the large time limit. The fully interacting, many-body Green’s function
is too hard to compute, so the various methods differ in how they approximate the full

i

h

|

projector. In particular, DMC makes a short time approximation, and the resulting pieces
have natural interpretations in terms of a diffusion process with branching. The name
Projector Monte Carlo or Green’s Function Monte Carlo is often applied to these methods.
Perhaps unfortunately, the name Green’s Function Monte Carlo (GFMC) is also applied to

a speciﬁc technique that uses a spatial domain decomposition for the Green’s function.

For a more detailed presentation of DMC, with importance sampling, we mostly follow

Reynolds et al. (1982). We start with the Schr¨odinger equation in imaginary time

(R,t)
¶ t

=

−

−

(cid:2)

−

(cid:3)

2 +V (R)

ET

f (R,t)

(2.21)

where l = ¯h2/2m. Importance sampling is added by multiplying Eq. (2.21) by a known trial
function y T . The result, written in terms of the “mixed distribution” f (R,t) = f (R,t)y T(R),
is

f (R,t)
¶ t

=

−

−

2 f + (EL(R)

ET ) f + l

−

( f FQ(R))

·

(2.22)

12

t
¥
f
¶
f
l
(cid:209)
¶
l
(cid:209)
(cid:209)
where EL is the local energy, as in VMC, and FQ = 2(cid:209)

T /y T (called the quantum force).

Once again, the solution for f in terms of a Green’s function is

f (R′,t + t ) =

dR f (R,t)G(R

R′; t )

→

Z

(2.23)

For sufﬁciently short times, we can ignore the non-commutivity of the kinetic and potential
t T et V . The explicit form for the short time Green’s
terms in the Hamiltonian, e−
function in the position basis is

e−

t H

≈

G(R

Gbranch(R

Gdrift(R

→

→

→

R′; t ) = (4p
R′; t ) = exp [
R′; t ) = exp

)−

3N/2Gbranch(R
¯E
ET

→

}

]
t FQ(R)

2

/4l

{
R′

−

−

R

−

−

−
h

(cid:8)

(cid:9)

i

R′; t )Gdrift(R

R′; t )

→

(2.24)

(2.25)

(2.26)

where ¯E = [EL(R) + EL(R′)] /2.

The algorithm is started by generating a collection of conﬁgurations (“walkers”), usu-
ally sampled from y T . Equation (2.23) proceeds by applying a drifting random walk to
each particle. The new position of the ith particle is given by

~r′i =~ri + l

t ~FQ(R) + √2l

(2.27)

is a normally distributed random variable with zero mean and unit variance. In a

where c
simple interpretation of Eq. (2.23), this would always be the new position. But consider
the case if y T becomes the true ground state, f 0. The branching term is then constant
and the algorithm becomes similar to VMC. In this case we want to sample the correct
distribution for any t . This is done by adding a Metropolis rejection step, where the trial
move is accepted with probability

A = min

1,

y T (R′)2G(R
→
y T (R)2G(R′ →

R′, t )
R, t )

(cid:21)

(cid:20)

(2.28)

Each conﬁguration is then weighted by Gbranch. Because of rejections in the previous step,
the time, t , in Eq. (2.25) should be replaced by t eff, which is

r2
acceptedi
r2
totali
h
is the mean square displacement of the accepted electron moves and

t eff = h

where
is the mean square displacement of all the proposed electron moves.

r2
acceptedi

h

(2.29)

r2
totali

h

The weighting is done by adding or removing conﬁgurations from the collection (branch-
ing). This is done by computing the multiplicity M = int(Gbranch + y), where y is a random
number on [0, 1]. This multiplicity is the number of copies of this conﬁguration that should

13

y
l
t
t
l
t
t
c
t
be retained in the collection of walkers. If it is zero, the conﬁguration is deleted from the
collection. If it is one, the conﬁguration remains as is. If it is greater than one, additional
copies of this conﬁguration are added to the collection.

The number of walkers in the collection is kept roughly constant by adjusting ET . In

particular, the trial energy is adjusted according to

ET = E0 + k

ln(P∗/P)

(2.30)

where E0 is the best guess for the ground state energy, P is the current population, P∗ is the
desired population, and k

is a feedback parameter.

The energy is computed by averaging the local energy over the distribution of walkers.

Once the transients have decayed away, subsequent steps are part of the ground state dis-
tribution. The program is then run for however long is necessary to gather statistics for the
energy and other estimators.

There is a problem with DMC for estimating quantities other than the energy. The

expectation value is not averaged over the ground state, but over the mixed distribution
f 0y T . This can be partly corrected by using the extrapolated estimator,

f 0

A

f 0

|

|

h

i ≈

2

h

f 0

A

y T

|

|

y T

A

y T

|

|

i

i − h

(2.31)

.

Getting the correct estimator (also called a pure estimator) requires ”forward walking”,
so named because the weight needed, f 0/y T , is related to the asymptotic number of chil-
dren of each walker (Liu et al., 1974). This can be implemented by storing the value of
the estimator and propagating it forward with the walker for a given number of steps (Ca-
sulleras and Boronat, 1995).

2.5.1 Fermions

In all these methods, some quantity is treated as a probability, which requires that it be
2, which is always positive. For DMC, we sample
positive. In VMC this quantity is
|
from y T f 0, which can be negative if the fermion nodes of y T are not the same as the nodes
of f 0. The simplest cure is to ﬁx the nodes of the ground state to be the same as y T . This
is known as the ﬁxed-node approximation. It is implemented in the DMC algorithm by
rejecting moves that would change the sign of the determinant of y T .

y T
|

14

0.009

0.008

0.007

0.006

0.005

0.004

0.003

0.002

0.001

0.035

0.03

0.025

0.02

0.015

0.01

0.005

0

0

2

4

6

8

10

12

14

0

1

2

3

4

5

6

7

8

Blocking Level

Blocking Level

Figure 2.2: Examples of statistical data analysis using reblocking. The error in the graph
on the left has converged, while the error in the graph on the right has not.

2.6 Statistical Errors

The formula for the variance given in Eq. (2.5) assumes that there are no serial correlations
in the data. However, the Metropolis sampling method produces correlated data, which
must be considered when estimating the statistical error.

Correlations in data are quantiﬁed by the autocorrelation function, deﬁned for some

estimator, E, as

C(k) =

1

−

(N

k)

N

k
−

i=1

(Ei

−

¯E)(Ei+k

¯E)

−

The autocorrelation time, k

, is computed as

(2.32)

(2.33)

k = 1 +

2
s 2

cutoff

k=1

C(k)

This sum tends to be quite noisy. As a heuristic strategy, we can approximate k by the ﬁrst
place where the autocorrelation function drops below 10%. The true variance of the mean
is the simple variance of the individual data points multiplied by k

.

Another way to estimate the true error is by reblocking (Flyvbjerg and Petersen, 1989;

Nightingale, 1999). At the second level, take the average of every 2 data points. Now com-
pute the variance of this set of data that has N/2 points. Continue this procedure recursively
until the variance stops changing. Nightingale (1999) gives a well-deﬁned procedure for

computing when that occurs. Figure 2.2 shows some example plots of error vs. reblocking
level. On the left hand graph we see the expected plateau in the error estimate. On the right
hand graph there is no plateau, indicating that there is not enough data to reliably estimate
the error.

15

s
s
(cid:229)
(cid:229)
2.7 Wave Functions

For our studies of molecular hydrogen, we started with the wave function y
(Reynolds et al., 1982). The Jastrow factors are

III from Reynolds

uee =

−
une = (cid:229)

aeri j
1 + beeri j
Za anria
1 + benria

i< j

i,a

(2.34)

(2.35)

(2.36)

where Z is the nuclear charge and b is the variational parameter. The cusp conditions are
satisﬁed by setting an = 1 and ae = 1/2. As noted in Appendix C, having the correct cusp
condition for parallel spins does not affect the energy much, so the same value for ae is used
for parallel and antiparallel electron spins. The b′s from the two types of Jastrow factors
are folded into a single parameter, b = a/b2.

The orbitals are ﬂoating Gaussians, with the form

l(r) = exp

−

(r

cl)2

−
w2
l

(cid:20)
where cl is the center of orbital, and wl is a free parameter. In molecular hydrogen, cl will
be ﬁxed at the bond center.

(cid:21)

The orbitals can be generalized to be anisotropic,

l(r) = exp

(r

−

cl)T

RT G R

(r

cl)

·

·

−

−
(cid:2)

where G
is a diagonal tensor and R is a rotation matrix. There are two parameters - the
width along the bond direction (rotated so as to be the z-axis), and the width perpendicular
to the bond direction. The elements of G are deﬁned to be (1/w2

xy, 1/w2

xy, 1/w2
z ).

(cid:3)

Finally, additional energy reduction was found for the isolated H2 molecule by multi-

plying the orbital by (1 + z

), where z

r
|

cl

|

−

is a variational parameter.

2.8 Periodic Boundary Conditions

The effects of an inﬁnite system can be approximated by imposing periodic boundary con-
ditions on a ﬁnite system. Every particle in the system then has an inﬁnite number of im-
ages. Inter-particle distances are calculated using the minimum image convention, which
uses only the distance to the closest image.

Care needs to be taken with the wave function when using the minimum image conven-
tion. As the inter-particle distance crosses over from one image to another there can be a

16

(cid:229)
f
f
discontinuity in the derivative of the wave function, leading to a delta function spike in the
energy. If this is not accounted for, the VMC energy can become lower than the true ground
state because this delta function term in the energy has been neglected. Additionally, the

Gaussian orbitals can lower their energy by having a width comparable to or larger than the
box size. Then sections of the orbital with large kinetic energy are outside L/2, and do not
get counted in the integral. This can be ﬁxed by summing over images, or by insuring the

wave functions have the correct behavior at

L/2. We use the latter solution.

The orbitals are multiplied by a cutoff function that ensures its value and ﬁrst derivative

are zero at the box edge. The function we use is

fc(r) = 1

exp

−

g c(r

rm)2

−

(2.37)

where rm is ﬁxed at L/2 and g c is a variational parameter.

(cid:3)

The Jastrow factors are constructed so that they obey the correct cusp conditions as
L/2. The simplest
0 and so that the ﬁrst and second derivatives are zero at rm

r
function that satisﬁes these conditions is a cubic polynomial. Let y = r/rm. Then

→

≤

±

−

(cid:2)

u(y) = a1y + a2y2 + a3y3,

(2.38)

rm, a2 =

a1, and a3 = a1/3. Variational freedom is gained
where a1 = (cusp value)
by varying rm, and by adding a general function multiplied by y2(y
1)3 to preserve the
boundary conditions. We choose a sum of Chebyshev polynomials as the general function
(Williamson et al., 1996). The full Jastrow factor is then

−

−

∗

u(y) = a1(y

y2 +

y3) + y2(y

1
3

−

1)3 (cid:229)

−

i

biTi(2y

1),

−

(2.39)

where rm and the bi are variational parameters. We use ﬁve Chebyshev polynomials for the
electron-electron part and another ﬁve for the electron-nuclear part. We optimized one set
of rm and bi parameters for all electron-electron pairs in any particular system, and another
set of parameters for all electron-nuclear pairs.

Comparisons of the energy and variance of various combinations of forms for the orbital
and Jastrow factors are shown in Table 2.5. The variational parameters are given in Tables
2.6 and 2.7. A comparison of the electron-electron Jastrow factors is shown in Figure 2.3.

Their short range behavior is similar, but the long range behavior differs between the types
of Jastrow factors.

The quality of wave functions is often measured by the percent of correlation energy
recovered. For H2, the HF (no correlation) energy is
1.1336 Hartrees and the exact (full
1.17447 Ha. Sun et al. (1989) compared a number of forms for
correlation) energy is
electron correlation functions. Their best value recovered 80% of the correlation energy.

−

−

17

)
r
(

e
e
u

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

Pade
cubic
cubic + Chebyshev

Figure 2.3: Optimized electron-electron Jastrow factor for different forms.

1

2

3

5

6

7

8

4
r

Table 2.5: Comparison of energies and variances for various forms for orbitals and Jastrow
factors for a single H2 molecule.

Orbital

Jastrow

Energy

Variance % CE

A

Isotropic

B
Anisotropic
C Anisotropic + z
D Anisotropic + z
E Anisotropic + z

simple Pad´e

simple Pad´e
simple Pad´e
cubic

cubic+Chebyshev

1.1598(4)
1.1643(2)
1.1653(2)
1.1688(2)
1.1702(2)

−

−
−
−

−

0.046

0.040
0.033
0.039

0.046

64.0(9)

75.0(6)
77.5(5)
86.1(6)

89.6(5)

Using one of these forms, but with better optimization, Huang et al. (1990) recovered 84%

of the correlation energy. Snajdr et al. (1999) obtained 93% of the correlation energy using
a Linear Combination of Atomic Orbitals (LCAO) form with 1s, 2s and 2p orbitals, and
using the Jastrow factors of Schmidt and Moskowitz (1990).

The variance is higher with those wave functions involving the cubic polynomial, even
though the energy is lower. I believe this is mostly likely because the cubic polynomial
does not have the correct 1/r behavior at large r, but the simple Pad´e form does. This
long range behavior contributes little to the average of the energy, but it contributes more

signiﬁcantly to the variance.

18

Y
Table 2.6: Values of variational parameters for H2.

Orbital parameters

Jastrow parameters
b = 9.913
A w = 2.74
b = 10.002
B wxy = 2.514, wz = 2.977
C wxy = 2.416, wz = 2.833, z = 0.0445 b = 9.958
D wxy = 2.357, wz = 2.628, z = 0.248

e-e rm = 5.404
e-n rm = 5.376

Table 2.7: Values of variational parameters for wave function E

Component

Orbital

Parameters

wxy = 2.299 wz = 2.515

Electron-electron Jastrow rm = 6.281
b2 = 0.619
rm = 5.329
b2 = 0.952

Electron-nuclear Jastrow

z = 0.301
b0 = -1.012 b1 = 0.193
b3 = 0.025
b4 = 0.138
b0 = -2.084 b1 = 0.153
b4 = 1.027
b3 = 1.217

19

Y
Chapter 3

Energy Difference Methods

Very often it is the difference in energy between two systems that is of interest, and not
the absolute energy of a single system. For a quantity such as the binding energy, we want

the difference between the energy of the molecule and the energy of the free atoms. In our
CEIMC simulations, we want the change in energy from moving a few nuclei. In VMC
optimization, we want to know the change in energy from modifying some of the wave

function parameters.

Correlated sampling methods can provide a more efﬁcient approach to computing these
energy differences. But the widely used reweighting method has some drawbacks. We
will introduce a new method that alleviates some of the drawbacks of reweighting while

retaining its advantages.

3.1 Direct Difference

The simplest, and most straightforward way of computing the difference in energy between

two systems is to perform independent computations of the energy for each system. Then
the energy difference and error estimate are simply

D E = E1

s (D E) = =

E2
−
1 + s 2
s 2
2

q

(3.1)

(3.2)

This method is simple and robust, but has the drawback that the error is related to the
error in computing a single system. If the systems are very similar, either in variational

parameters or in nuclear positions, the energy difference is likely to be small and difﬁcult
to resolve, since s 1 and s 2 are determined by the entire system. Similarities between the
systems can be exploited with correlated sampling.

20

3.2 Reweighting

Reweighting is the simplest correlated sampling method.

D E = E1

E2
−
dR y 2

1 EL1

dR y 2

2 EL2

dR y 2

1 − R

dR y 2
2

=

=

R

R

R

R
dR y 2

1 EL1

dR y 2

1 − R

dR y 2
R
1

y 2
2
y 2
1

(cid:16)
dR y 2
1

EL2

(cid:17)
y 2
2
y 2
1

(cid:16)

(cid:17)

D E

1
N

≈

EL1(Ri)

w(Ri)EL2(Ri)
i w(Ri)

−

(cid:21)

y 2

1 (cid:20)

Ri

∈

An estimate of D E for a ﬁnite simulation is

R

where w = y 2

2/y 2

1. The same set of sample points is used for evaluating both terms.

Reweighting works well when y 1 and y 2 are not too different, and thus have large
overlap. As the overlap between them decreases, reweighting gets worse due to large ﬂuc-
tuations in the weights. This effect can be quantiﬁed by computing the effective number of

points appearing in the sum in Eq. (3.4), which is

Neff =

((cid:229)

i w2
i
i wi)2

Eventually, one or a few large weights will come to dominate the sum, and the effective
number of points will be very small, and the variance in D E will be very large.

Particularly pernicious is the case when the nodes differ between the two systems. The
denominator of the weight can easily be very small, causing a very large weight value. This
is encountered when using reweighting to optimize orbital parameters in VMC (Barnett

et al., 1997).

In Eq. (3.4) we derived reweighting by drawing points from y 1 and computing the
properties of both systems from them. It could also be derived by drawing points from y 2
as well. We can compute the energy difference both ways and take the average. This gives

us the symmetrized reweighting method,

D E =

1
2N

1
2N

"

y 2
1

Ri

∈

"

y 2
2

Ri

∈

EL1(Ri)

wx(Ri)EL2(Ri)

−

1
N

i wx(Ri) #

wy(Ri)EL1(Ri)

1
N

i wy(Ri) −

EL2(Ri)

#

+

where wx = y 2

2/y 2

1 and wy = y 2

1/y 2
2.

21

(3.3)

(3.4)

(3.5)

(3.6)

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
3.3 Bennett’s Method for Free Energy Differences

First let us digress to discuss computation of the normalization integral. It was mentioned

earlier that the Metropolis sampling method makes it difﬁcult to extract information about
the normalization integral, which the partition function in the classical case. Bennett (1976)
demonstrated a method for ﬁnding the free energy difference between two systems. We will
describe his method in terms of a ratio of normalizations.

We can compute the ratio of two normalizations, Q1 and Q2, in a fashion very similar

to reweighting.

Q1/Q2 =

dR y 2

1(R)

dR y 2

2(R)

. Z
y 2
y 2

1(R)
2(R)

. Z

Z

Z

dR y 2

2(R)

y 2
y 2

1(Ri)
2(Ri)

y 2

Ri

∈

=

≈

dR y 2

2(R)

This is a one-sided estimate, because it only uses samples from system two to compute
properties of system one. Note that this sum is the same as the sum of the weights used in
reweighting in Eq. (3.4).

Bennett improved on this one-sided estimate, starting with an identity written as

R
where W is an arbitrary weight function. He found the optimum W by minimizing the
variance of the free energy difference,

Q1/Q2 =

Q1
Q2

R

dR y 2
dR y 2

2 W y 2
1
1 W y 2
2

W (cid:181)

1
2 + Q2y 2
1

Q1y 2

1 =

dR

R

dR

y 2
2
Q2
y 2
1
Q1

y 2
1/
y 2
(cid:0)
2/

2 + y 2
y 2
Qy 2

1/Q
2 + y 2
1

(cid:1)

Let Q = Q1/Q2. Inserting Eq. (3.11) into Eq. (3.10), we get

(cid:0)
Let x represent the conﬁgurations sampled from y 1 and y the conﬁgurations sampled from
y 2. The ﬁnite sample version of this equation is

(cid:1)

R

Qy 2
2(xi)
1(xi) + Qy 2
y 2

2(xi)

i

= (cid:229)

y 2
1(yi)
1(yi) + Qy 2
y 2

2(yi)

i

22

(3.7)

(3.8)

(3.9)

(3.10)

(3.11)

(3.12)

(3.13)

(cid:229)
(cid:229)
The value of Q can be found by a simple iteration

Qn+1 = Qn 

y 2
1(yi)
1(yi)+Qny 2
y 2
Qy 2
2(xi)
1(xi)+Qny 2
y 2

y

x

2(yi)

2(xi)









(3.14)

The iteration is started with Q0 = 1 and stopped when the correction factor in brackets is
sufﬁciently close to one. Typically convergence takes less that ten iterations, but if Q is
much larger or smaller than one it can take more iterations.

We have written these formulas assuming that the number of sample points from each

system is the same. Bennett derived them for case with differing numbers of samples in
each sum, and found the best variance was usually very near an equal ratio of computer
time spent on each system. In our case the systems are of equal complexity, so this means
using equal numbers of points is optimal, or very nearly so.

By properly combining information from both systems, we can get a much better (lower
variance) estimate of the ratio of their normalizations than if we had used information from
only a single system (one-sided sampling).

3.4 Two-Sided Sampling

We can apply this notion to computing the energy difference between two quantum sys-
tems. Consider sampling from some distribution that contains information about both y 1
and y 2. The simplest such distribution is

(cid:20)
The energy difference can be written as

P =

1
2

y 2
1
Q1

+

y 2
2
Q2 (cid:21)

In the ﬁnite case, we have

D E =

1EL1

dR y 2
Q1

2EL1

dR y 2
Q2

=

dR P

R

Z

y 2

− R
1EL1
Q1P

(cid:18)

dR P

y 2

2EL2
Q2P

(cid:18)

(cid:19)

−

Z

(cid:19)

D E

1
2N

≈

Ri

x,y

∈

y 2

1(Ri)EL1(Ri)
Q1P

−

y 2

2(Ri)EL2(Ri)
Q2P

(3.15)

(3.16)

(3.17)

It is important to note that the sum covers samples taken from both y 1 and y 2. The sum
includes both “direct” terms (eg. y 1 and EL1 evaluated at conﬁgurations sampled from y 1)
and “cross” terms (eg. y 1 and EL2 evaluated at conﬁgurations sampled from y 2).

23

(cid:229)
(cid:229)
(cid:229)
The denominator of the ﬁrst term in Eq. (3.17) is
Q1
Q2

Q1P =

1 +

1
2

y 2

y 2
2

(cid:21)

(cid:20)

(3.18)

The ratio Q = Q1/Q2 is computed by the Bennett method. The denominator of the second
term can be computed similarly.

One major feature of the two-sided method is that it reproduces reweighting in the
large overlap regime, and the direct method in the low overlap regime. In the intermediate
regime, it smoothly joins the two limits.

To show this, ﬁrst consider the case where the two systems are very different and the
1(yi) and y 2
2(xi) will be small. Expand Eq. (3.17)

wave functions have low overlap. Here y 2
into its four terms

D E =

1
2N

+

1
|
2N

y 2
1(xi)EL1(xi)
1(xi) + Qy 2
y 2

2(xi)

1
2N

−

y 2
1(yi)EL1(yi)
1(yi) + Qy 2
y 2

2(yi)

(cid:3)

direct
1
2N

{z
−

1
2

x

(cid:2)

1
2

y

y 2
2(yi)EL2(yi)
1(yi)/Q + y 2
y 2

2(yi)

y 2
2(xi)EL2(xi)
1(xi)/Q + y 2
y 2

2(xi)

(cid:3)

}

1
2

y

(cid:2)

1
2

x

(3.19)

(cid:3)

(cid:2)

|

cross
(cid:2)
2(yi)) and one small term (y 2
1(xi) or y 2
Each denominator will have one large term (y 2
1(yi)
{z
or y 2
2(xi)). The value of Q is moderate compared to the wave functions, so it will not affect
the relative sizes of these terms. Always having one large term in the denominator means
there will never be any excessively large contributions to the sum resulting from division
by a small value, as happens in reweighting. The cross terms have a small value (y 2
1(y) or
y 2
2(x)) in the numerator, and so vanish. The large terms in the denominators in the direct
terms cancel the y 2’s in the numerator, and we are left with

}

(cid:3)

D E

1
N

≈

x

EL1(xi)

EL2(yi)

1
N

−

y

(3.20)

which is just the direct method.

Now for the case where the systems are very similar and have large overlap. Recall

from Eq. (3.9) that we can write one-sided estimates for Q as

Q =

wy(yi) = 1

wx(xi)

(3.21)

1
N

y

1
N

x

.

1(yi)/y 2

2(yi) and wx = y 2

2(xi)/y 2

1(xi). Write the four terms of Eq. (3.17) in a

where wy = y 2
different order

D E =

x

1
N
1
QN

EL1(xi)
Q
1 + Qwx(xi) −
N
wy(y1)EL1(yi)
1 + wy(yi)/Q −

y

wx(xi)EL2(xi)
1 + Qwx(xi)
EL2(yi)
1 + wy(yi)/Q

y

x

1
N

+

(3.22)

24

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
Approximate the denominator of each term by two, replace the leading Q’s with the appro-
priate one-sided approximation, and we get

D E

≈

1
2N

1
2N

EL1(xi)

x "

wx(xi)EL2(xi)

−

1
N

x wx(xi) #

wy(yi)EL1(yi)

y "

1
N

x wy(yi) −

EL2(yi)

#

+

(3.23)

which is the symmetric version of reweighting given in Eq. (3.6).

Due to computational considerations, it is useful to divide Eqns (3.13) and (3.17) by y 2
1
2 as appropriate, and work with the resulting ratios wx = y 2
or y 2
2, as
was done in Eq (3.22). The values of the wave functions can easily over or under ﬂow dou-
ble precision variables. It is best to use the log of the wave function, take differences, and

1 and wy = y 2

2/y 2

1/y 2

then exponentiate. Furthermore, an arbitrary normalization of the wave functions makes
no physical difference, but can result in very large or small numbers, even after taking the
difference of the logarithms. This problem is ameliorated by subtracting the average value

of the log of the wave function from the individual values. Sometimes this is not enough,
however, and the value of the energy difference exceeds the range representable in a double
precision variable, indicated by NaN (Not a Number). In this case, the overlap is clearly
very small and the two-sided method should give the same results as the direct method.

The program checks for the energy difference being NaN, and if so, it substitutes the di-
rect method result (the data collected for the two-sided method is a superset of that needed
for the direct method). Having done this, the subroutine computing the two-sided energy
difference will always return a reasonable answer, an important consideration for a core

routine in a program.

3.5 Examples

The ﬁrst example is of two H2 molecules in a parallel orientation as shown in Figure 3.1.
The bond lengths are at equilibrium, 1.4 Bohr, and the starting separation between the
molecules is d = 2.5 Bohr.

The energy difference between that conﬁguration and conﬁgurations with other inter-

molecular distances was computed using the direct method, the two-sided method, and
reweighting. The resulting energy differences are shown on the left in Figure 3.2. Note
that reweighting gets the wrong answer at large separations. This is most likely due to a
ﬁnite sample size bias. More important is the error in that energy difference, shown on the

right in Figure 3.2. Note that both reweighting and the two-sided method have errors that

25

(cid:229)
(cid:229)
(cid:229)
(cid:229)
y

1

E

 

2

1.5

0.5

0

1.4

x

d

Figure 3.1: Two H2 molecules in a parallel conﬁguration

Direct
Reweighting
Two-sided

0.01

 
y
g
r
e
n
E

0.001

0.0001

Direct
Reweighting
Two-sided

-2

-1

0

1

2

3

4

-2

-1

0

1

2

3

4

Move distance

Move distance

Figure 3.2: Energy difference (left) and the estimated statistical error (on logscale) (right)
for two H2 molecules in a parallel conﬁguration, starting from d=2.5 Bohr.

drop to zero as the overlap increases. This graph also clearly shows the properties of the
two-sided method mentioned previously, behaving like reweighting at small changes in the

separation (large wave function overlap), and smoothly crossing over to the direct method
for at large changes in the separation (small wave function overlap).

Reweighting and the two-sided method may give biased results because there are a
ﬁnite number of sample points in the sums in Eqns. (3.6) and (3.17). To test for this, a sum

of a given length is repeated many times and the average energy difference for that length
computed. The test for a bias was performed on a Li2 molecule. The energy difference
was computed between a bond length of 4.5 Bohr and the equilibrium bond length of 5.05

Bohr. Figure 3.3 shows the results for different numbers of points in the sum. Reweighting
shows a much larger ﬁnite sample size bias than the two-sided method, which has almost
none.

26

D
s
)
5
0
.
5
=
d
(
E

 
-
 
)
5
.
4
=
d
(
E

0.01

0.008

0.006

0.004

0.002

0

-0.002

-0.004

-0.006

-0.008

-0.01

-0.012

Direct
Reweighting
Two-sided
Correct

0

20

40

60

80

100
Number of points in sum

120

140

160

180

200

Figure 3.3: Finite sample size bias in the energy difference of Li2.

3.5.1 Diffusion Monte Carlo

Using the two-sided method (or reweighting, for that matter) with DMC is slightly more
complicated. The reweighting transformation applied to the basic DMC iteration gives

f1(R′;t + t ) =

dR f (R;t)G1(R

R′; t )

=

dR f (R′;t)G2(R

Z

Z

→

→

R′; t )

G1(R
G2(R

R′; t )
R′; t )

→
→

(3.24)

(3.25)

The weight w = G1/G2 must be computed over several iterations. The ﬁnal weight used in
the correlated sampling formulas is a product of the weights of every iteration.

The weight factor is not quite right, due to the rejection step. Since the rejection ratio for
DMC is very small (< 1%), ignoring the issue should not introduce a large error. Umrigar
and Filippi (2000) give a more sophisticated method for dealing with rejections.

The ﬁxed-node condition also has to be obeyed, and conﬁgurations that cross a node

while projecting have their weight set to zero.

A version of reweighting was implemented by Wells (1985) as the differential Green’s
function Monte Carlo method (actually DMC). He used the response to an external ﬁeld to
determine the dipole moment of LiH. The same trial function was used, so the drift term
was the same between both systems. Only the branching term was different; that entered

as a weight factor. In our case, the trial function and the nuclear positions may be different
between the two systems.

The top of Figure 3.4 shows the difference in DMC energies using the various methods.

27

The energy difference was computed starting from the equilibrium bond length of 5.05
Bohr. Partly because of the need to project for several DMC steps, the two-sided method
has a fairly small range where it does better than the direct method (compared with the

range for VMC). For comparison, the VMC results are shown at the bottom of Figure 3.4.
There are two lines in the DMC graph for the direct method. The implementation had a
limitation where only one projection to accumulate the weights would occur at a time. We

used 30 steps in the projection, and consequently could only get the weights once every 30
steps. The limited data line is computed from data collected once every 30 steps (the same
amount of data available to the correlated methods) and is the line the two-side method
joins on to. The full data line used all the data available in the simulation and so has a

lower statistical error.

3.5.2 Binding Energy

To compute the binding energy, let the non-interacting system be y 2, and the fully interact-
ing system be y 1. The nuclear positions are the same for both systems, and the appropriate
interaction terms are set to zero for the non-interacting system.

A pair of H2 molecules in a parallel conﬁguration was used as the test system. The
binding energy we are interested in is that of the interacting molecules minus separate H2
molecules (and not the separate atoms).

EB = E ((H2)2)

2E(H2)

−

(3.26)

There is a problem in that the electrons in the fully interacting system can switch
molecules and have no effect on the computation, but these conﬁgurations are very unlikely
in the non-interacting system. This leads to an artiﬁcially small overlap. The solution in
this symmetric case is to restrict the domain of integration. The electron coordinates are
ordered along the x-axis so that x1 < x2 and x3 < x4.

To see that this restriction is exact, consider the integral

−

Z

dx1

dx2 f (

x1
|
where f corresponds to the electron-electron Jastrow factor and g is symmetric under the
interchange of x1 and x2 and corresponds to the electron-nucleus Jastrow factor and the
x2.
square of the Slater determinant. Change variables to R = (x1 + x2)/2 and r = x1
Now we have

)g(x1, x2)
|

(3.27)

x2

−

−

Z

−

dR

dr f (

)g(R + r/2, R

r/2)

(3.28)

|
The integral over r is even, and we only need to integrate over half of the interval (r < 0 or
r > 0), which corresponds to the restrictions x1 < x2 or x1 > x2.

−

Z

Z

−

−

r
|

28

¥
¥
¥
¥
¥
¥
¥
¥
direct, full data
direct, limited data
reweighting
two-sided

4.2

4.3

4.4

4.8

4.9

5

4.5

4.6
Bond Length

4.7

direct
reweighting
two-sided

1

0.1

0.01

0.001

0.0001

0.01

0.001

e
c
n
e
r
e
f
f
i

D
 
y
g
r
e
n
E
 
n
i
 
r
o
r
r

E

e
c
n
e
r
e

f
f
i

D
 
y
g
r
e
n
E
n

 

i
 
r
o
r
r

E

0.0001

4.2

4.3

4.4

4.5

4.6
Bond Length

4.7

4.8

4.9

5

Figure 3.4: Error in energy difference of Li2 using DMC (top) and VMC (bottom)

29

Two-sided, Restricted
Reweighting, Restricted
Two-sided, Unrestricted
Reweighting, Unrestricted
Direct

1

0.1

0.01

0.001

0.0001

i

y
g
r
e
n
E
 
g
n
d
n
B
 
n
i
 
r
o
r
r

i

E

0

2

4

12

14

16

6
10
8
H2 - H2 distance

Figure 3.5: Error in VMC binding energy of H2-H2 system

Figure 3.5 shows the error in the VMC binding energy for various intermolecular dis-
tances. Without restricting the domain of integration, reweighting performs poorly, and the
two-sided method reproduces the results of the direct method. With the restricted domain,

the correlated methods perform quite well.

30

Chapter 4

Wave Function Optimization

Variational Monte Carlo (VMC) depends crucially on the optimization of parameters in the
wave function to ﬁnd the minimum energy. The general problem of function optimization

is a well-studied area. For a general introduction to various optimization techniques, see
Press et al. (1992). For more in-depth work, consult Polak (1997), Dennis and Schnabel
(1983), or Ortega and Rheinboldt (1970).

The main difﬁcultly in applying these techniques to optimizing VMC wave functions
is noise - we only get stochastic estimates for function values or gradients. Glynn (1986)
describes several strategies for optimization in the presence of noise. We will divide these
into three categories.

The ﬁrst strategy is to convert the problem into a nearby smooth, non-noisy problem,
and solve that problem instead. Fixed sample reweighting takes this approach by sampling
some set of conﬁgurations and optimizing with just these conﬁgurations.

The second approach is to reduce the noise to negligible levels, and proceed with reg-

ular optimization techniques. This is possible with a Newton method, where the ﬁrst and
second derivatives of the function are computed, and the number of iterations needed for
convergence hopefully is small.

The third approach is to use a method tailored to handle noise. The Stochastic Gradient
Approximation (SGA) is such a method. Also somewhat in this category, we will examine
a method that is essentially a biased random walk, and the moves are accepted or rejected
based on the whether or not the energy decreases.

These approaches will be compared on several problems of different sizes to see how
they scale. We will use a single H2 molecule, and collections of 8,16, and 32 H2 molecules
in a box as trial problems.

31

4.1 Energy vs. Variance Minimization

There is a choice of objective functions - either the energy or the variance of the energy

can be minimized. Under certain circumstances, variance minimization is more stable
that energy minimization. For the reweighting method, this is deﬁnitely true, but it may
not be the case for the other optimization techniques. It is generally held that variance
optimization would produce better values for observables other than energy (Williamson

et al., 1996), but this may not always be the case (Snajdr et al., 1999; Snajdr and Rothstein,
2000). The argument is that the variance is more sensitive to parts of the wave function
that do not contribute to the energy. As we have seen in Chapter 2, having incorrect long
range behavior in the H2 wave function does not affect the energy much, but does cause
the variance to rise. In other words, variance minimization should yield a “smoother” wave
function, which should then have better non-energy properties.

4.2 Fixed Sample Reweighting

Fixed sample reweighting with minimization of the variance was popularized by Umrigar
et al. (1988), and has been used extensively since then. The current state of the art is
described by Kent (Kent et al., 1999; Kent, 1999).

The core of the method is the single sided reweighting method described in Chapter

3. A number of conﬁgurations are sampled from a distribution with variational parameters
a 0. The energy at an arbitrary value of the variational parameter, a

, is computed by

where w(Ri; a ) = y 2(Ri; a )/y 2(Ri; a 0). Alternatively, one could compute the variance by

E(a ) = (cid:229)

w(Ri; a )EL(Ri; a )/(cid:229)

w(Ri; a )

i

i

A(a ) = (cid:229)

w(Ri; a )(EL(Ri; a )

ET )2/(cid:229)

w(Ri; a )

i

−

i

(4.1)

(4.2)

where ET can either be the weighted average energy (4.1) or it could be a guess at the
desired energy.

The weights in these expressions can get very large when the variational parameters
move far from the sampled value a 0, and especially when the parameters that affect the
nodes are adjusted. Then just a few conﬁgurations will dominate the sum, and the energy
estimator can often give meaningless low values. The variance estimator, however, will
remain more stable in this situation. For either estimator, the best ﬁx is to regenerate the
conﬁgurations being used when the parameters move too far away from a 0. This can be
used in conjunction with the enhancements described below.

32

A second advantage of the variance estimator is that the weights can be modiﬁed with-
out changing the location of the minimum (Kent et al., 1999). The same is not true for the
energy. The problem of a few large weights can be solved by limiting them to a maximum

value (Filippi and Umrigar, 1996), or more simply by just setting them all to one (Schmidt
and Moskowitz, 1990). Barnett et al. (1997) tame the ﬂuctuating weights by sampling
from a positive deﬁnite guiding function. In this cases, ET should be set to a best guess, or
slightly below, because the energy estimator will not be reliable.

Further increases in stability can be gained by limiting outliers in Eq. (4.2). Large
outlying values have a disproportionately large effect on the variance, but their contribution
is not that meaningful. Kent et al. (1999) gives a procedure choosing a cutoff that will

reduce its effect as the number of samples increases. We used a simpler approach, removing
from the sum any values greater than 5 standard deviations from the average.

Another efﬁciency improvement can be exploited when the Jastrow U is linear in the

variational parameters. Then the variational parameters can be factored out of the sum over
interparticle distances, and the value of that sum can be stored. Fixed sample reweighting
has been applied mostly to optimizing Jastrow factors, and not parameters in the Slater
determinant, so this results in a dramatic time savings. In our case we have both Jastrow

and determinantal parameters, and the code spends about 40% of its time computing the
Jastrow factor. This percentage will decrease as the system size increases, since the Jastrow
computation is O (N2) but the determinantal part requires matrix work that is O (N3). We
did not implement this improvement, so bear in mind when perusing the results that the

reweighting time could be reduced, probably by 30%.

An additional advantage of reweighting is that, since it is solving a smooth problem,
an off-the-shelf minimizer can be used. We used the DSMNF general minimizer from

the PORT library, which uses only the function values and does not need any derivatives.
Routines to minimize sums of squares are also available, but we did not try them. The
ﬁxed sample reweighting algorithm is then: Generate a set of conﬁgurations and minimize
the variance with this set. Generate a new set of conﬁgurations using the new variational

parameters and ﬁnd the minimum variance again. Repeat for several steps to ensure con-
vergence.

4.3 Newton Method

The Newton method makes use of the ﬁrst and second derivatives. We can approximate a
function near its minimum as a quadratic surface

f (x)

f (x0) + (x

x0)T

b + (x

x0)T

≈

−

−

A

(x

·

·

−

x0)

(4.3)

·

33

where bi =
given by

f
¶ x and Ai j =

¶ 2 f
¶ xi¶ x j

is the Hessian matrix. The location of the minimum is then

−
Since we are likely to start in a region where f is not quadratic, this step is iterated several

·

x0 = x

1
A−

b

(4.4)

times.

This procedure, along with analytic evaluation of the derivatives, was applied to VMC
energy minimization by Lin et al. (2000). Analytic derivatives of the local energy with
respect to determinantal parameters are given by Bueckert et al. (1992), but these were

used in the context of a reweighting minimization.

Recall the VMC energy is computed by

E =

EL

=

h

i

Z

(cid:30)Z

dR y 2(a )EL(a )

dR y 2(a )

(4.5)

We want the derivatives of E with respect to various variational parameters, a
be computed with ﬁnite differences and reweighting, but it is better to do some analytical
work on this expression ﬁrst.

. These could

Lin et al. (2000) use some Green’s relations to eliminate explicit derivatives of the local

energy (Ceperley et al., 1977), and derive the following expression for the gradient,

¶ E

m

= 2

EL y

h
(cid:2)

′ln,mi − h

ih

′ln,mi

EL

(cid:3)

ln y

1

=

′ln,m =

.

m

m
They also give the expression for the Hessian,

where

where

and

¶ 2E
m¶

n

= 2

EL y

h
(cid:8)
+2

EL y

h
(cid:2)

EL

′′ln,m,ni − h
′ln,m
¶ E

ih
′ln,ni − h

′′ln,m,ni
EL
ih
¶ E

n − h

′ln,ni

m

−h
+
h

′ln,mi
′ln,mE′L,ni

′ln,m

′ln,ni

(cid:3)

(cid:9)
¶ EL
n

E′L,n =

′′ln,m,n =

¶ 2 ln y
m¶

.

n

34

(4.6)

(4.7)

(4.8)

(4.9)

(4.10)

¶
¶
a
y
y
¶
¶
a
y
¶
y
¶
a
¶
a
a
y
y
y
y
y
¶
a
y
¶
a
y
¶
a
y
¶
a
a
Computing the ﬁrst derivatives of the wave function analytically is relatively easy.
Computing the second derivatives with respect to parameters in the Jastrow factor is also
easy analytically. For parameters that appear in the determinant, however, second deriva-

tives are more difﬁcult. For this reason we compute most of the ﬁrst derivatives analytically,
and use these to compute the second derivatives with a simple ﬁnite difference scheme. The
ﬁrst derivative of the local energy was computed with ﬁnite differences. The derivative of

the orbital cutoff parameter, which is the same for all the orbitals, was also computed with
ﬁnite differences.

An advantage of the Newton approach over the gradient-only approaches is that it has
information about how big of step should be taken, whereas the step size is a parameter that

must be tuned in the gradient-only methods. The drawback, though, is a greater sensitivity
to noise. The gradient and Hessian must be sufﬁciently accurate, or the Newton iteration
will get wildly wrong results. More precisely, it is the non-linear process of taking the

inverse in Eq. (4.4) that causes the problem. Furthermore, this sensitivity to noise increases
with the number of parameters.

Another problem is parameter degeneracy, or near degeneracy. This will make the
Hessian singular, or nearly so. Even if it not exactly singular, being nearly singular is the

equivalent of dividing by a small number, which will also greatly magnify the effects of
noise. The usual solution is use of the Singular Value Decomposition (SVD). See Press
et al. (1992) or Kincaid and Cheney (1991) for a description of the algorithm. A more
detailed look at ”regularization” (of which the SVD is one method) is in Hansen (1998).

The SVD starts by decomposing a matrix as

A = PDQ

(4.11)

where P and Q are unitary matrices and D is a diagonal matrix. The elements of D are the
eigenvalues of AT A. For our square, symmetric matrix, these are are the squares of the
eigenvalues of A. We can also take P and Q to be the eigenvectors of A. The utility of the
SVD is seen when we write the inverse of A as

A−

1 = QT D−

1PT

(4.12)

If A is singular, then at least one of its eigenvalues is zero. In this case, zero eigenvalues
also indicate parameter degeneracy, so it’s not really necessary to move in the directions

corresponding to the zero eigenvalues. To avoid moving in these these directions, and to
stabilize the inverse, set 1/di in Eq. (4.12) to zero when di is smaller than some cutoff.

With the eigenvalue decomposition we have an additional technique - negative eigen-
values correspond to uphill directions and mean we are at a saddle point or are far from a

35

Ns=4000
Ns=8000
Ns = 8000, no SVD
Ns=16000
Ns=32000
Ns = 32000, no SVD

l

l

)
e
u
c
e
o
m
/
a
H

(
 
y
g
r
e
n
E

-1.1

-1.11

-1.12

-1.13

-1.14

-1.15

-1.16

0

0.5

1

1.5

2

Time (Hours)

Figure 4.1: Examples using the Newton iteration with varying amounts of noise.

region where the quadratic approximation is good. 1 The simplest way of handling this is
to ignore negative eigenvalues. So we remove small positive and all negative eigenvalues

when solving Eq. (4.4).

Some examples of this Newton iteration with 8 H2 molecules are shown in Figure 4.1.
Ns is the number of samples used in computing the gradient and Hessian. Unless otherwise
noted, the SVD method for solving Eq. (4.4) was used with removal of eigenvalues less
than 0.01. Other runs without using regularization are not shown because they diverge very
drastically.

4.4 Stochastic Gradient Approximation

The Stochastic Gradient Approximation (SGA) was designed by Robbins and Munro (1951)
to handle optimization with noisy gradients. It was ﬁrst applied to VMC optimization by
Harju et al. (1997).

The SGA iteration can be written as

i = a

i

1
−

−

hg i(cid:209)

a E(a

1).

i

−

(4.13)

where h is a step size parameter and g i is some specially chosen series.

1Much of the complexity of current Newton and quasi-Newton optimization methods is in deciding how to
move the parameters when the quadratic approximation is not good. Typically it involves a line minimization
in the gradient direction or some sort of back tracking.

36

a
There are some conditions on g i that must be satisﬁed in order for this iteration to

converge. They are

The condition given by Eq. (4.15) allows the iteration to reach anywhere in parameter

space. The condition in Eq. (4.16) is needed so the effects of noise will eventually be
damped out. An obvious choice for g i is 1/i. For more discussion on these conditions and
for some conditions on the objective function, see Young (1976) and Tsypkin (1971).

We can analyze the convergence in the limiting case of no noise in one dimension. First

let us make a continuous version of Eq. (4.13) by letting g (t) = dt/t and da = a (t)
dt). Then in the dt

0 limit, the SGA iteration is

a (t

−

−

→

Now let us assume that f has a quadratic form, f (a ) = 1

2 Aa 2 + Ba + f0, with a mini-

mum at a =

B/A. Now Eq. (4.17) is

−

The solution is

then

The solution is

We see that the smaller d
indeed be the best choice.

Eq. (4.20) is

where a0 is a constant of integration. So we see that it will converge to the solution at
t

, with a rate that is controlled by the curvature of the potential and our choice of h.
Now consider generalizing to the case where g (t) = dt/td . Our continuous equation is

→

a (t) =

B/A + a0 exp

hAt1
−

/(1

d )

−

−
h

−

i

is, the faster the convergence. If there were no noise, d = 0 would

Now let us represent the noise in the gradient with an additive noise term, h (t). Then

g i > 0

g i = ¥

i=1

i=1

i < ¥
g 2

da
dt

=

h
t

−

f (a (t))

da
dt

=

h
t

−

[Aa + B]

a (t) =

B/A + a0t−

hA

−

da
dt

=

h
td

−

[Aa + B]

da
dt

=

h
td

−

[Aa + B + h (t)]

37

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

(4.19)

(4.20)

(4.21)

(4.22)

¥
(cid:229)
¥
(cid:229)
(cid:209)
a
¥
d
Previously we considered case where noise was negligible. Now consider the case where
the noise dominates, so Eq. (4.22) becomes

(4.23)

(4.24)

(4.25)

(4.26)

The solution is the integral

da
dt

h (t)
td

=

−

a (t) =

−

Z

T

dt

h (t)
td

To look at convergence, we need to compute the variance of a
Take the noise to have a probability distribution P(x,t) with zero mean and variance s
variance of a

is then

integrated over the noise.

. The

−
If we take P(x,t) to have no dependence on t, the integrals factor and we get

Z

Z

−

s 2(a ) =

T

dx

dt P(x,t)

x2
t2d

s 2(a ) =

s 2
2d

−

−

1

1
T 2d

1
−

Here we see that larger values of d

lead to faster convergence of the noise. Since smaller

lead to faster convergence of the non-noisy problem, we need an intermediate

values of d
value of d

to balance these effects.

One variation, suggested by Nemirovksy and Yudi (1983), is to use d = 1/2 and use the
cumulative average of the variational parameters. This value of d violates the condition in
Eq. (4.16), but this condition is there to insure the noisy part converges. Instead we use the
cumulative averaging process to remove the noise.

Another acceleration technique involves monitoring the sign of the gradient (Tsypkin,
1971). Far from the minimum the gradient will not often change sign between successive

steps. Close to the minimum, the noise will eventually dominate, and the gradient will
change sign more often. The acceleration procedure is to only update g i when the sign of
the gradient changes. This also has the advantage of adjusting the convergence of each

parameter separately.

In practice, starting the series at g 1 = 1 tends to make the ﬁrst steps have a dramatically
larger effect on the parameters than subsequent steps. Often, the ﬁrst few steps would move
the parameters very far from the minimum, and then the iteration will take a long time to
converge. In this work we started the series at i = 10 to minimize this effect.

We tried several of these SGA variants on the box of 8 H2 molecules. We used h = 3
when g i = 1/√i and h = 10 when g i = 1/i. This way the initial step sizes (give by hg i) were
similar. Figure 4.2 shows the convergence of one of the variational parameters (rm for the
electron-electron Jastrow). The convergence of the energy is also shown. We see that the
two accelerated methods converge faster than the simple SGA.

38

¥
¥
g  = 1/i
g  = 1/i1/2
g  = 1/i1/2,cumulative average
g  = 1/i, accelerated 

100

10000

100000

1000
i

g  = 1/i
g  = 1/i1/2,cumulative average
g  = 1/i, accelerated

m

r
 
e
-
e

y
g
r
e
n
E

5

4.8

4.6

4.4

4.2

4

10

-9.25

-9.255

-9.26

-9.265

-9.27

-9.275

100

1000

10000

100000

i

Figure 4.2: Examples of SGA. The graph on the top shows the convergence of one varia-
tional parameter for several SGA algorithms. The graph on the bottom shows the resulting
energy.

39

4.5 Gradient Biased Random Walk

We introduce a new method that is made possible by the two-sided energy difference

method in Chapter 3. Using this, it is relatively easy to determine whether a change of
the trial parameters lowers the energy or not. This determination can be ﬁtted onto a num-
ber of methods, even a random walk. We evaluate the gradient and make a trial move in the
gradient direction, similar to the SGA. Unlike the SGA, the move is accepted only if it low-

ers the energy. Since the gradient is noisy, we are effectively making a random walk that is
biased in the gradient direction, hence the name Gradient Biased Random Walk (GBRW).

The trial move is

a T = a

i

1
−

h(cid:209)

a E(a

1).

i

−
where h is randomly chosen from [0, hmax]. To provide some simple adaptivity, hmax is
adjusted during the run. If a trial move is rejected, hmax is decreased via multiplication by
some factor, usually 0.5 or 0.6. If a trial move is accepted, it is increased by multiplying by
the reciprocal of that same factor.

−

(4.27)

Currently the level of convergence of this method is controlled by how well the energy
difference is computed. In other words, once the energy differences are of the same size
as the estimated error, it simply ﬂuctuates. There are several possibilities for making a

convergent method. The ﬁrst is to take the cumulative average of the parameters, or add
a damping parameter as in the SGA. The second is to increase the number of samples to
compute the energy difference (and so decrease the noise) at each iteration.

4.6 Comparison of methods

We test the various optimization methods and compare their run times. The test systems
are an isolated H2 molecule, and 8,16, and 32 molecules in a box at rs = 3.0, a fairly
low density. Each system has 12 parameters in the Jastrow factor, and 3 determinantal

parameters per molecule, plus one more for the box cutoff (which is the same for all the
orbitals). Thus we have 15, 37, 61, and 109 variational parameters, respectively. For the
starting parameters,we set the Jastrow cutoff to rm = 4.0, the orbital widths to 2.0, the
orbital box cutoff to 1.0, and all the other parameters to zero.

The Newton method used the regularization method with a cutoff of 0.01 for N = 8, 16
and a cutoff of 0.1 for N = 32. No regularization was used for N = 1. The SGA method
used g i = 1/√i and parameter averaging. Reweighting used 16000 conﬁgurations for N = 1
and 1000 conﬁgurations for N = 8 and 16. We did not attempt reweighting on the largest
system.

40

-1.161

-1.163

-1.165

-1.167

-1.169

-1.171

-1.1

-1.11

-1.12

-1.13

-1.14

-1.15

-1.16

-1.17

y
g
r
e
n
E

l

l

)
e
u
c
e
o
m
/
a
H

(
 
y
g
r
e
n
E

SGA
GBRW
Reweighting
Newton

SGA
GBRW
Reweighting
Newton

0

0.2

0.4

0.6

0.8

1

0

1

2

3

4

5

6

7

8

9

Time (hr)

(a)

SGA
GBRW
Reweighting
Newton

Time (hr)

(b)

SGA
GBRW
Newton

l

l

)
e
u
c
e
o
m
a
H

/

(
 
y
g
r
e
n
E

l

l

)
e
u
c
e
o
m
/
a
H

(
 
y
g
r
e
n
E

-1.144

-1.146

-1.148

-1.15

-1.152

-1.154

-1.156

-1.158

-1.16

-1.1

-1.11

-1.12

-1.13

-1.14

-1.15

-1.16

-1.17

0

5

10

15

20

25

0

20

40

60

80

100

120

140

Time (hr)

(c)

Time (hr)

(d)

Figure 4.3: Optimization methods applied to (a) Single H2 (b) 8 H2’s (c) 16 H2’s (d) 32
H2’s

The best way to compare these methods would be to run them all many times starting
from different random number seeds. The average of the resulting distribution would give

the average quality of each method, and the spread of the distribution would indicate the
stability. However, this is time-consuming and instead, as a ﬁrst approximation we present
the results for a single run of each method in Figure 4.3. The times are in hours on an AMD
Duron 600 Mhz (which is approximately 1/2 to 2/3 the speed of a 195 Mhz R10000 in an

SGI Origin).

For the single molecule, it is clear that the Newton method is the best method. The
reweighting method also performs well, and the two gradient methods take longer to con-

verge. As the system size increases, however, the gradient methods do better, with the SGA
method doing the best.

The Newton method in particular has difﬁculty with stability as the system size in-
creases. It needs to be run long enough so the noise is small enough that it does not affect

41

the results.

The reweighting method performs surprisingly poorly on the larger systems. Looking
more closely at the results of reweighting for the N = 8 case, we get a total energy of
9.275(3) Ha and a
−
−
9.268(2) Ha and a variance of 0.36. It
variance of 0.42. From the GBRW we get E =
appears that the problem is with the variance minimization and we have a case where the

9.244(2) Ha and a variance of 0.30. From the SGA we get E =

−

minimum variance solution is not the lowest energy solution. Although on the scale of the
total energy, the difference between reweighting and the SGA is only 0.3%. On the scale
of the correlation energy in the isolated molecule, this difference is about 10%.

4.7 Future Work

We have compared a few basic methods for VMC parameter optimization. Many more
improvements and modiﬁcations could be conceived and tried.

Currently we ran these with set numbers of iterations and numbers of samples, then

looked at the results, and perhaps made adjustments and tried again. What would be very
helpful is some sort of adaptivity - adjusting the number of samples or even the type of
method as the optimization proceeds in order to ensure convergence.

So far the gradient-only methods seem to have the advantage, but have the disadvan-
tage that they require a step size be set manually. In order to generate a trial step size
automatically, a secant updating method could be tried, where successive gradient evalu-
ations are used to build up an approximate inverse Hessian (Dennis and Schnabel, 1983).

These methods are often superior to using the actual Hessian (Press et al., 1992) , but it is
not clear how the presence of noise will affect the algorithm.

Finally, it would be instructive to perform these comparisons on systems containing

atoms with higher atomic number.

42

Chapter 5

Coupled Simulation Methods

There are several issues we have to deal with when constructing an efﬁcient CEIMC simula-
tion. The ﬁrst is noise from the QMC evaluation of the energy. We will discuss a modiﬁca-

tion to the Metropolis acceptance ratio, called the penalty method, that will accommodate
noise. Next we will examine some of the details involved in a CEIMC simulation, and
ﬁnally give results for a single H2 molecule.

5.1 Penalty Method

The Metropolis acceptance ratio, from Chapter 2, is min [1, exp(
V (s)]. The QMC simulation will yield a noisy estimate for D

−

D )], where D = b [V (s′)

−

, which we denote as d
= exp(

exp(

d )

The exponential in the acceptance ratio is nonlinear, so that

).
The noise will introduce a bias into our acceptance ratio formula. To avoid this bias in our
simulations, we can either run until the noise is negligible, or we can try ﬁnd a method that
tolerates noise.

h−

−

i 6

h

i

.05
Typical energy differences for moves in our simulations are on the order of .01
Ha. If we want an error level of 10% (statistical error of .001 Ha) 1 it would take about 7
hours of computer time for a system of 16 H2 molecules. We need to perform hundreds
of these steps as part of the classical simulation, so clearly a method that could tolerate
higher noise levels would be very beneﬁcial. The penalty method of Ceperley and Dewing
(1999) does this, and our simulations run with noise levels on the order of .01 Ha, which
only takes about 4 minutes of computer time.

−

In the penalty method, we start with detailed balance, written as

→
1The usual error level considered chemical accuracy is 1 kcal/mol = .0016 Ha

→

−

A(s

s′) = A(s′

s) exp [

] .

(5.1)

43

d
D
To deal with noise, we would like to satisfy detailed balance on average, We introduce
an instantaneous acceptance probability, a(d ), that is a function of the estimated energy
difference. The average acceptance probability is the instantaneous one averaged over the

noise,

The detailed balance equation we would like to satisfy is then

A(s

s′) =

→

Z

−

dd P(d ; s

s′)a(d )

→

dd P(d ; s

e−

a(

d )

= 0

−
Suppose the noise is normally distributed with variance, s

→

−

Z

−

i
. Then

s′)

a(d )
h

A simple solution to Eq. (5.3) is

P(d ) = (2s 2p )−

1/2 exp

(d

D )2

−
2s 2

(cid:21)

−

(cid:20)

a(d ) = min

1, exp(

(cid:20)

s 2

)

(cid:21)

−

−

2

s 2/2 term causes addition rejections of trial moves due to noise. For this reason

The extra
it is called the penalty method.

−

To verify that the solution in Eq. (5.5) satisﬁes detailed balance (5.3), let us compute

the average acceptance probability

A(D ) =

=

=

D )2/2s 2

min

−

−

−

Z

s 2/2

(d
dd e−

(d
dd e−

1
√2s 2p
1
√2s 2p
1
√2s 2p
1
erfc((s 2/2 + D )/2s 2) +
2

′ e−

s 2/2

dd

Z

Z

−

−

−

−

−

D )2/2s 2

2/2s 2

′

s 2/2

i

−

1, e−
h
+

1
√2s 2p
1
√2s 2p
Z
erfc((s 2/2

+

s 2/2

Z

−

=

1
2
where we have made the substitutions d
′ = d
A(D ) will satisfy detailed balance, A(D ) = e−

e−

−
D A(
In practice, both the energy difference and the error are being estimated from a ﬁnite
set of data. Assume we have n estimates for the energy difference, y1, ..., yn. Estimates for
the mean and variance are given by

−

′ + s 2. This expression for

−
D )/2s 2)

s 2/2

−
′′ = d

and d
D ).

(d
dd e−

−

D )2/2s 2

e−

−

s 2/2

dd

′′ e−

e−

2/2s 2

′′

(5.2)

(5.3)

(5.4)

(5.5)

(5.6)

(5.7)

1
n

n

yi

i=1
1

d =

c 2 =

n

d )2

(yi

−

n(n

1)

−

i=1

44

¥
¥
¥
¥
D
d
¥
¥
d
¥
¥
d
D
¥
d
¥
D
D
d
D
D
(cid:229)
(cid:229)
and we have D =

and s 2 =

c 2

.

i

h

h

i

The average acceptance ratio can be written as integral over d and c 2. The probability
distribution for the estimated error is a chi-squared distribution. An asymptotic solution can
be formed by expanding a(d , c 2) and performing the integrals to get the average acceptance
s 2/2), and by matching powers of s we
ratio. This is set equal to a power series for exp(
get the coefﬁcients for the original series for a(d , c 2). This series can by summed to get a
Bessel function, hence we call it the Bessel acceptance formula. It is convenient to expand
the log of the Bessel acceptance formula in powers of c 2/n. The Bessel acceptance formula
is then

−

where

a(d , c 2, n) = min [1, exp(

uB)]

−

−

uB =

+

c 2

2

c 4

+

4(n + 1)

3(n + 1)(n + 3)

c 6

+

c 8(5n + 7)
8(n + 5)(n + 3)(n + 1)2)

+

· · ·

(5.8)

(5.9)

Note that as n gets large, only the ﬁrst term is important, which is just the regular penalty
method.

5.1.1 Other methods

There is another method for handling noise, originally proposed by Kennedy, Kuti, and
Bhanot (Kennedy and Kuti, 1985; Bhanot and Kennedy, 1985), that uses a power series
d ] to construct an unbiased acceptance ratio. It has an advantage over
expansion of exp[
the penalty method in that it does not assume any particular distribution for the noise. The
method has a major drawback in that it depends on the value of d not becoming too large,
and not just the error estimate for d . This could severely restrict the maximum steps sizes
for moving the nuclei in our simulations. Methods for dealing with this restriction has

−

recently been addressed by Lin et al. (1999) and Bakeyev and de Forcrand (2000), but we
did not explore these extensions.

5.1.2 Handling noisy data

Using noisy data requires care in handling. Particularly, inappropriate reuse of any single

estimated value can lead to biased results. For instance, in a classical simulation the en-
ergy difference would be computed, and one of the two energies involved would be used
in accumulating the average energy. See the top of Figure 5.1 for an outline of such a sim-

ulation. However, this leads to a bias when noisy energies are involved. This can be seen
by considering a negative ﬂuctuation in the energy of the trial move. This will make the

45

d
d
energy difference smaller (or more negative) and hence more likely to be accepted. Thus
the negative ﬂuctuations would be preferentially added to the accumulated average, and
bias the result downward.

This program outline could be corrected by computing a new value for the energy in
the average. However, there is another arrangement that is more amenable to the energy
difference methods of Chapter 3. The computation of the energy used in the average is the

same quantity needed for the old energy in the next iteration. So that computation can be
moved to the next iteration, as shown on the bottom in Figure 5.1.

Several points are illustrated with a system of two particle interacting via a Lennard-
Jones potential with e = 0.1 and s = 1.5 2. The temperature of the system was 3160K (b =
100). Noise was simulated by adding a Gaussian random variable with known variance
to every energy computation. Results for several algorithms versus noise level are shown
in Figure 5.2. The top curve shows the bias that results from having no penalty method.

The middle curve is the correct method, which we see is independent of the noise level.
The bottom curve demonstrates the bias from reusing the energy involved in making the
accept/reject decision.

The noise level of a system can be characterized by the relative noise parameter, f =
(b
)2t/t0, where t is the computer time spent reducing the noise, and t0 is the computer
time spent on other pursuits, such as optimizing the VMC wavefunction or equilibrating
the DMC runs. A small f means little time is being spent on reducing noise, where a large
f means much time is being spent reducing noise. The efﬁciency of a CEIMC simulation

s = 1

can be written in terms of this parameter. Our paper gives an example of a double well
potential, and ﬁnds the noise level that gives the maximum efﬁciency. Generally it falls
around b
2, with the optimial noise level increasing as the relative noise parameter
increases. The one exception occurs when computing the ﬁrst moment, which is sensitive to
crossing the barrier between the double wells. These crossings are assisted by an increased
noise level, hence the optimal noise level is much higher.

−

5.2 Pre-rejection

We can apply multi-level sampling ideas (see section 2.4.1 for an application to VMC) to
our CEIMC simulations as well. The idea is to use an empirical potential to ”pre-reject”
moves that would cause particles to overlap and be rejected anyway.

In this case, the trial move is proposed and accepted/rejected based on a classical po-

2Here s

is the length scale for the LJ potential

46

s
Compute old Energy

loop over Classical steps

loop over Number of molecules

make trial move (translation and rotation of H2 molecule)
Compute trial Energy

1, exp

acceptance probability = min
accept/reject trial move
(cid:0)
if accept, set old Energy = trial Energy
Use updated old Energy in average

(cid:2)

D E

(b

−

−

)2/2

(cid:1)(cid:3)

end loop

end loop

loop over Classical steps

loop over Number of molecules

make trial move (translation and rotation of H2 molecule)
Compute old Energy Compute trial Energy
D E

1, exp

)2/2

(b

acceptance probability = min
accept/reject trial move

−

−

(cid:0)

(cid:2)

(cid:1)(cid:3)

Use old Energy in average

end loop

end loop

Figure 5.1: CEIMC program outlines. Boxes indicate quantum computations. The dashed
box indicates a quantity saved from a previous computation. The top algorithm is incorrect.
The bottom algorithm is correct.

47

b
s
b
s
w/o penalty
w/ penalty, reusing energy
w/ penalty, new energy

-0.08

y
g
r
e
n
E

-0.075

-0.085

-0.09

-0.095

-0.1

0

0.5

1

1.5

3

3.5

4

4.5

2

2.5
Noise level (b

)

Figure 5.2: Examples on a Lennard-Jones potential with synthetic noise.

(5.10)

(5.11)

tential

A1 = min

1,

T (R
→
T (R′ →

R′)
R)

exp(

D Vcl)

−

(cid:21)

(cid:20)

where D Vcl = Vcl(R′)
at this ﬁrst level, the QMC energy difference is computed and accepted with probability

Vcl(R) and T is the sampling probability for a move. If it is accepted

−

A2 = min [1, exp(

D VQMC

uB) exp(b

D Vcl)]

−

−

where uB is noise penalty.

Compared to the cost of evaluating the QMC energy difference, computing the classical
energy difference is free. Reducing the number of QMC energy difference evaluations is
valuable in reducing the computer time required.

In Chapter 7, using the pre-rejection technique with a CEIMC-DMC simulation results
in a ﬁrst level (classical potential) acceptance ratio of 0.43, and a second level (quantum
potential) acceptance ratio of 0.52. The penalty method rejects additional trial moves be-
cause of noise. If these rejections are counted as acceptances (ie, no penalty method or no

noise), then the second level acceptance ratio would be 0.71. The classical potential is a
fairly good representation for the DMC potential, and we can use that to reduce the number
of DMC energy difference evaluations needed.

5.3 Trial Moves

Molecular moves are separated into translation, rotation and bond length changes.

48

s
b
b
Table 5.1: Efﬁciency of classical Monte Carlo for moving several particles at once. The
table on the left is for low density system at rs = 3.0 and T=5000K. The table on the right
is for a high density system at rs = 1.8 and T=3000K. The largest values of the efﬁciency
are shown in boxes.

Nm
1

2
4
8

16
32

0.8

1.4

2.4
5.6
6.9

9.5
11.9

1.6

4.9

8.1
12.5
14.8

22.2
14.8

3.0

9.6

15.3
16.9
18.0

7.4
2.7

4.0

11.6

17.4
17.7
14.8

1.6

236

121
66

2.0

191

114
23

Nm
1

2
4
8

16
32

0.4

74

43
118
172

155
39

1.2

99

179
141
24

0.8

134

149
170
128

52

The Silvera-Goldman potential is used as the empirical potential for pre-rejecting trans-
lational moves. Anisotropic potentials were tried for pre-rejecting rotational moves, but

they did not work very well. It is not clear whether this was from the the potentials being
derived for isolated H2-H2 interaction, or from inaccuracy in the trial wave function.

Bond stretching moves were pre-rejected using the, essentially exact, H2 intramolecular
potential of Kolos and Wolniewicz (1964). The new bond length is sampled uniformly from
a box of size D b around the current position. Because of phase space factors we need to
include a sampling probability of T (R) = 1/R2 in the acceptance formula.

The trial move for classical Monte Carlo is usually presented as either moving one

particle at a time, or all of the particles at once. However, we can move other numbers of
particles as well. Table 5.1 shows the efﬁciency for a classical system with 32 H2 molecules
for two densities and temperatures. On the left is a low density system with rs = 3.0 and
at a temperature of 5000K, and on the right higher density system with rs = 1.8 and a
temperature of 3000K. For the lower density system, the highest efﬁciency occurs when
moving half the molecules at a time. Relatively high efﬁciency can also be found moving
2, 4 or 8 at a time as well. For the higher density system, the most efﬁcient regime shifts
towards smaller step sizes and fewer number of particles moved at a time. 3

3These results are not generally applicable to classical MC simulations, since much more efﬁcient imple-

mentations are possible for systems interacting with a two body potential.

49

D
D
Table 5.2: Results of CEIMC for isolated H2 molecule at T=5000K.

Energy

Virial

r

0.0

i
h
1.57
-0.009(6) 1.56(2) 1.58(2)

i

r2
h
1.60
p

exact
-1.1630
VMC -1.159(1)

DMC -1.163(2)

-0.015(6) 1.58(2) 1.60(2)

exact
VMC
DMC

1

0.8

0.6

0.4

0.2

0

1

1.5

2.5

3

2

R

Figure 5.3: H2 bond length distribution.

5.4 Single H2 molecule

The CEIMC method was applied to a single H2 molecular in free space, at a temperature
of 5000K. Exact results are obtained by integrating the potential of Kolos and Wolniewicz

(1964). Results for the energy, pressure, and ﬁrst and second moments of the bond length
are given in Table 5.2. The Virial column is computed by

Virial = [2

K

+

h

i

V

h

]

i

(5.12)

This is related by the virial theorem to the force on the nuclei, which should be zero for an
isolated molecule. In Chapter 7, we will see this expression used to compute the pressure.
As we would expect, the VMC energy is higher than the exact energy. The other quan-

tities are close to their expected values. Histograms of the bond length distribution are
shown in Figure 5.3. Here again, both VMC and DMC reproduce the exact distribution
well.

50

Chapter 6

Hard Spheres

A system of hard spheres is the simplest non-ideal many body system to study. Statis-
tical Monte Carlo techniques and molecular dynamics were ﬁrst applied to hard spheres

(Metropolis et al., 1953; Alder and Wainwright, 1957). Additionally, some of the ﬁrst ap-
plications of ﬁeld theoretic methods to condensed matter systems were on hard spheres.
More recently, the achievement of BEC in trapped atomic gases has renewed interest in the

theory of the hard sphere Bose gas (Dalfovo et al., 1999).

This chapter is a VMC and DMC study of a homogeneous, boson hard sphere ﬂuid.
Because they are bosons, there is no ﬁxed node approximation in DMC, and the result can
be made essentially exact. The approximations we must control are ﬁnite size effects and

timestep error in DMC.

sion,

Applying perturbation theory to the Bose hard sphere gas yields the low density expan-

E = 2p

[1 +C1√r +C2r

ln r +C3r + ...]

(6.1)

where C1 = 128/(15√p ) and C2 = 8(4p /3
√3). Mean ﬁeld theory gives the linear term.
−
Straight forward application of perturbation theory yields the C1 term, as was done by
Huang, Yang, and Lee (Lee et al., 1957; Huang and Yang, 1957). The next higher or-

der of perturbation theory diverges, but this was solved by including the depletion of the
condensate by Beliaev (1958) to get C2. Wu (1959) obtained the same results via a resum-
mation technique. Hugenholtz and Pines (1959) also obtained the logarithmic term. They
also obtained the functional form for the series, which includes terms of the form r n/2 and
r n/2 log(r ).

Renormalization group techniques have recently been applied to examine this diver-
gence (Castellani et al., 1997; Braaten and Nieto, 1997a,b). In addition, Braaten and Nieto
have calculated C3 (Braaten and Nieto, 1997a,b). This term is also ﬁrst term that depends
on more that the two body s-wave scattering length. It would require a solution to the three

51

r
body scattering problem, which makes an explicit computation of that coupling constant
difﬁcult.

Hansen et al. (1971) used VMC on an 864 particle system to ﬁnd the ﬂuid-solid tran-

sition density. Kalos et al. (1974) used the more accurate Green’s Function Monte Carlo
(GFMC) to calculate the energy of the solid and liquid phase near freezing to determine
the freezing density. They used 256 particles. In the liquid state they computed four points

ranging in density from 0.16 to 0.27.

Recently Giorgini et al. (1999) did DMC calculations on the homogeneous Bose gas,
with various potentials, including the hard sphere potential. They used 500 particles and
there was no mention of what DMC time step was used. These calculations were also used
to make a ﬁtting to the extended form of Eq. (6.1) (using terms up to r 5/2) by Boronat et al.
(2000).

There have been other attempts to get an equation of state by using various ﬁtting tech-

niques to combine the low density results and the GFMC results (Aguilera-Navarro et al.,
1987; Keller et al., 1996). As noted by Keller et al. (1996), the earlier work had an error
and used only half the energy of the actual GFMC results. A new attempt at ﬁtting the
various functional forms was not done, but a new value for C3 was estimated.

The Hamiltonian for this system is

H =

1
2

−

i + (cid:229)
(cid:209) 2

i

i< j

v(

ri

|

−

r j

)

|

where

r < s
r > s
We have set ¯h = m = s = 1 in all these calculations.

v(r) =

(cid:26)

0

6.1 Wave function

An approximate wave function for the boson ground state that we use for a trial function is

The individual functions are very similar to the ones used for hydrogen. The correlation
function has a maximum range, rmax, beyond which f is constant. In order to be compatible
L/2. The “cusp” condition is that
with periodic boundary conditions, we require rmax
s ).
the wave function must vanish linearly when two spheres get close, f (r
(Unlike the electronic case, the slope is not ﬁxed.)

s ) (cid:181)

→

(r

−

≤

y = (cid:213)

f (ri j)

i< j

52

(6.2)

(6.3)

(6.4)

(cid:229)
¥
b0

Table 6.1: Variational parameters for hard sphere gas
b2
0.5624
0.8147
-0.12

xmax
1.92
2.68
2.68

-0.3209 0.25395
0.9173
-0.33
-1.95

.2
.1
.05
.01 5.9152

1.995
0.674

b1

b3
0.0145
0.2269
0.056

b4
-.05123
0.0345
0.0

0.86267 -1.2982 -0.08135 -0.3152

A change of variables will simplify these expressions. Let x = r

, xmax = rmax

−

−

and y = x/xmax. Now y lies in the range [0, 1]. In these variables, the boundary conditions
on f are

f (y = 0) = 0

f (y = 1) = 1

f ′(y = 1) = f ′′(y = 1) = 0

and the wave function is

f (y) = 3(y

y2) + y3 + y(y

1)3

−

−

biTi(2y

1)

−

4

i=0

where Ti are Chebyshev polynomials, and bi are variational parameters.

The parameters for each density are given in Table 6.1. They were obtained from op-
timization of the smallest system (N = 40). Then those same parameters were used for all
system sizes at a particular density.

−

The cost of computing of the wave function and local energy is dominated by calculat-
ing the N(N
1)/2 interparticle distances. There are techniques for improving the scaling
of computations of short range interactions to achieve O (N). We used the cell method
(Allen and Tildesley, 1987). The simulation box is divided into cubic cells and a list is
made of all the particles in each cell. For simplicity, consider the case where each cell is
larger than the cutoff distance, rmax. Then a particle in a cell will have a non-zero inter-
action only with the particles in the same cell and with particles in the neighboring cells.

Particles in cells further away can be ignored. There is an overhead in computing and
maintaining these lists. We used the cell method on systems with 500 particles and larger.
There is a deﬁciency with the trial wave function that leads to undersampling when three
particles are in close proximity. In DMC, this leads to a large number of branching walkers

to compensate for the undersampling, which invariably causes problems with maintaining
a stable population. One solution is to use a guiding function, which differs from the trial

53

(6.5)

(6.6)

(6.7)

(6.8)

r
s
s
(cid:229)
0.08936

0.08932

y
g
r
e
n
E

0.08928

0.08924

0.0892

0.08916

y
g
r
e
n
E

1.803

1.802

1.801

1.8

1.799

1.798

1.797

(a)

(c)

extrapolated to t =0

extrapolated to t =0
GFMC

0

0.02

0.04

0.06

0.08

0.1

0

0.01

0.02

0.03

0.04

0.05

extrapolated to t =0

extrapolated to t =0

y
g
r
e
n
E

y
g
r
e
n
E

0.666

0.6658

0.6656

0.6654

0.6652

0.665

0.6648

0.6646

5.665

5.66

5.655

5.65

5.645

5.64

(b)

(d)

0

0.002

0.004

0.006

0.008

0.01

0

0.002

0.004

0.006

0.008

Figure 6.1: Time step error for (a) r = 0.01 (b) r = 0.05 (c) r = 0.1 (d) r = 0.2

wave function, for the diffusion and branching. Then a weight, y T /y G, is associated with
each sample point. In this case the simplest guiding function is to use is y G = y
T . We
found that a = 0.9 was sufﬁcient to make the population of walkers stable.

The DMC timestep errors should be local, and hence the same for all system sizes.
We did timestep extrapolation on systems with N = 40 particles. The timestep errors were
found to be linear in t . The extrapolations to t = 0 are shown in Figure 6.1.

Green’s Function Monte Carlo (GFMC) uses a different decomposition of the Green’s
function that DMC. The principle advantage of GFMC is that is has no time step error. We
ran GFMC at r = .05 and N = 40 to verify the timestep errors. The GFMC data point is
shown at t = 0 in Figure 6.1. The importance sampling in GFMC is not as effective as in
DMC, hence the variance is larger, and GFMC is less efﬁcient than DMC. GFMC has an
efﬁciency of about 7, whereas DMC has an efﬁciency of 240 at t = .002 and an efﬁciency
of 570 at t = .007. Even with computing at several timesteps to extrapolate to zero, DMC
is more efﬁcient that GFMC.

54

t
t
t
t
a
)
k
(
S

1.2

1

0.8

0.6

0.4

0.2

0

0

VMC
DMC, mixed est
DMC, extrapolated est

)
k
(
S

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1.5

k

(b)

VMC
DMC, mixed est
DMC, extrapolated est

1.5

k

(a)

0.5

1

2

2.5

3

0.5

1

2

2.5

3

Figure 6.2: S(k) for (a) r = 0.05 (b) r = 0.2

6.2 Finite Size Effects

The main contribution to ﬁnite size effects in the energy is the long wavelength phonons.
The functional form for their contribution depends on the small k behavior of the structure
factor, S(k). The energy can be written as

E = 4p

k2dk e (k)

(6.9)

Zkb
where kb = 2p /L is the small k cutoff due to the ﬁnite box size. The energy of the phonon
excitations at small k is proportional to S(k) (Feynman and Cohen, 1956). For a classical
liquid, S(k) (cid:181) 1 + O (k2). For a Bose ﬂuid, S(k) should be proportional to k and S(k
0) =
0.

→

The VMC wave function has no long range part, and so we expect it to behave like a
classical ﬂuid at small k. Integrating Eq (6.9), we get E (cid:181) k3
b, which is the same as scaling by
1/N for a ﬁxed density. A more rigorous derivation of this scaling is given by Lebowitz and
Percus (1961). The DMC algorithm should pick up the correct long wavelength behavior,
leading to an S(k) that is linear in k. Integrating Eq. (6.9) we get E (cid:181)
k4
b. This then gives
us 1/N4/3 scaling.

The small k behavior for S(k) can be seen nicely for r = 0.05, shown in Figure 6.2a. The
graph shows that the VMC structure factor appears quadratic as expected. The DMC mixed
estimator shows the S(k) behaving linearly, but still not headed to zero. The extrapolated
estimator looks like it over corrects and lowers S(k) too much.

We did calculations for systems with 40, 108, 256, and 500 particles. For r = .2,
additional VMC runs with N = 103 and N = 104 particles were done, and they are shown
on the graph. The VMC energy nicely ﬁts the 1/N behavior, as shown in Figure 6.3.

55

extrapolated to N=¥

extrapolated to N=¥

0

0.005

0.01

0.015

0.02

0.025

0

0.005

0.01

0.015

0.02

0.025

extrapolated to N=¥

extrapolated to N=¥

y
g
r
e
n
E

y
g
r
e
n
E

0.0916

0.0914

0.0912

0.091

0.0908

0.0906

0.0904

0.0902

0.09

1.876

1.874

1.872

1.87

1.868

1.866

1.864

1.862

1.86

1/N

(a)

1/N

(c)

0

0.005

0.01

0.015

0.02

0.025

0

0.005

0.01

0.015

0.02

0.025

Figure 6.3: VMC ﬁnite size effects for (a) r = 0.01 (b) r = 0.05 (c) r = 0.1 (d) r = 0.2

1/N

(b)

1/N

(d)

0.692

0.69

y
g
r
e
n
E

0.688

0.686

0.684

0.682

y
g
r
e
n
E

6.06

6.05

6.04

6.03

6.02

6.01

56

extrapolated to N=¥

extrapolated to N=¥

0

0.002

0.004

0.006

0.008

0

0.002

0.004

0.006

0.008

extrapolated to N=¥

extrapolated to N=¥

y
g
r
e
n
E

y
g
r
e
n
E

0.09

0.0898

0.0896

0.0894

0.0892

1.812

1.81

1.808

1.806

1.804

1.802

1.8

1.798

1/N4/3

(a)

1/N4/3

(c)

0

0.002

0.004

0.006

0.008

0

0.002

0.004

0.006

0.008

Figure 6.4: DMC ﬁnite size effects for (a) r = 0.01 (b) r = 0.05 (c) r = 0.1 (d) r = 0.2

1/N4/3

(b)

1/N4/3

(d)

y
g
r
e
n
E

y
g
r
e
n
E

0.671

0.67

0.669

0.668

0.667

0.666

0.665

5.7

5.69

5.68

5.67

5.66

5.65

5.64

57

Table 6.2: Energy extrapolated to inﬁnite system size (in units of

VMC

6.0546(6)
1.8744(4)
0.6917(1)

.2
.1
.05
.01 9.144(2)

DMC

5.67(1)
1.809(1)
0.6690(4)

¯h2
ms 2 )
Giorgini et al. (1999)

1.8130(35)
0.6690(5)

2
10−

×

8.9896(8)

2
10−

×

8.980(5)

2
10−

×

The DMC ﬁnite size extrapolation is shown in Figure 6.4. At higher densities, the DMC
energy does not appear to have a 1/N4/3 dependence (or even 1/N dependence). However,
the data is too sparse and noisy to make a good determination as to what the functional
form should be, so we ﬁt it to 1/N4/3. The ﬁnal inﬁnite system results are given in Table
6.2.

The long wavelength excitations will take a long time to sample, and their effect on
the energy (and S(k)) may only be apparent with very long runs. And the time needed
to sample them will increase with box size. The larger box sizes may be insufﬁciently
converged, causing the energy to be too high. This may explain the apparent curvature in

the wrong direction.

There are several approaches for resolving the problem at larger box sizes. The ﬁrst
is simply to perform even longer runs to see if the energy drops. Similarly, data for more
system sizes would be helpful in outlining the functional form of the ﬁnite size dependence.

Finally, explicit long range correlations could be added to the wave function, of the form
proposed by Chester and Reatto (1966).

The energy versus density is shown in Figure 6.5, relative to the ﬁrst order term , which
is linear in the density. The low density exansion up to the C1 term is also shown, as well
as up to C3, using the ﬁtted value of 73.296 (Keller et al., 1996). It is clear from the graph,
and was noted by Hugenholtz and Pines (1959), that these additional terms by themselves
do not help the expansion.

Boronat et al. (2000) treated C2 and C3 as adjustable parameters and added two addi-
tional terms, r 5/2 log(x) and r 5/2. They get a very good ﬁt to the high density data, as seen
in Figure 6.5.

The results of Giorgini et al. (1999) are also given in Table 6.2 for the three common

densities computed. Their data and ours agree within the error bars.

58

r
low density expansion up to C1
low density expansion up to C3
fit of Boronat et al.
this work

6

5

4

3

2

1

 

2p
/
E

0
0.001

Figure 6.5: Energy vs. density

0.01

3

0.1

6.3 Distribution Functions and Condensate Fraction

The two particle correlation function for all densities was calculated with a system size of
256 particles. The results for g(r) using the extrapolated estimator are shown in Figure 6.6.
We see the liquid shell structure developing as the density increases.

The single particle density matrix is the projection of a many-body wave function on to

a single particle space. It is deﬁned by

r 1(r, r′) =

dr2...drN y (r, r2, ..., rN)y ∗(r′, r2, ..., rN)

(6.10)

Z

In the homogeneous case, r 1 only depends on the distance between r and r′. The large
r behavior of r 1 (or the k = 0 behavior of its Fourier transform, n(k)) is the condensate
fraction. At zero temperature, the many body wave function is in the ground state. Because
of interations, not all the particles are in the single body zero momentum state (k = 0 plane
wave state in this case).

We used a method for sampling r 1(r) which was given by McMillan (1965). The con-
densate fraction was obtained by integrating the single particle density matrix for distances
greater than some cutoff, rc, chosen to be where r 1(r) had reached a plateau.

The condensate fraction is given in Table 6.3 and shown in Figure 6.8. Also shown in
the ﬁgure is the GFMC result of n0 = 0.095(1) at r = 0.2. The low density expansion is
given by

(6.11)

n0 = 1

8
3√p

−

r 1/2

59

r
r
s
)
r
(
g

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

1.2

1

0.8

0.4

0.2

0

0

)
r
(

1

0.6

0.5

1

2

2.5

3

1.5

r r 1/3

Figure 6.6: Pair distribution function for several densities

r =0.2
r =0.1
r =0.05
r =0.01

r =0.2
r =0.1
r =0.05
r =0.01

Figure 6.7: Single particle density matrix for several densities

1

2

3

4

5

r r 1/3

60

r
Table 6.3: Condensate fraction

VMC

DMC (mixed) Extrapolated

.2

0.1009(5) 0.0876(3)
0.2960(5)
.1
0.307(2)
0.5401(4)
.05 0.563(2)
0.826(1)
.01 0.834(1)

0.0743(8)
0.285(2)
0.517(8)
0.818(2)

This work
Low density expansion
GFMC
fit of Boronat et al

0
n

0.5

1

0.9

0.8

0.7

0.6

0.4

0.3

0.2

0.1

0

0

Figure 6.8: Condensate fraction vs. density

0.05

0.15

0.2

0.1

3

Similar to their treatment of the energy, Boronat et al. (2000) added two additional terms, r
and r 3/2. Their ﬁt does a good job at higher densities, where, as expected, the low density
expansion misses the full extent of the depletion.

61

r
r
s
Chapter 7

Hydrogen

Hydrogen has been the subject of many experimental and theoretical studies. Theoretically,
its simple electronic structure make it a favorable ﬁrst target for various methods. Exper-

imentally, hydrogen has been compressed by shock waves and also with a diamond anvil
cell. We will present some CEIMC simulations and compare the results with those from
one of the gas gun shock wave experiments.

7.1 Experiment

The high pressure experiments fall into two categories - transient compression from a shock
wave or static compression from a diamond anvil cell. The shock wave experiments reach

higher temperatures and pressures, but obtain more limited data. A high-velocity projectile
hits a stationary target, inducing a shock wave in the target. The target is analyzed by
the Hugoniot relations, derived by treating the shock wave as an ideal discontinuity and
applying conservation of mass, momentum, and energy across it (Zel’dovich and Raizer,

1966). The relations are then

P

−

E

−

P0 = r ousup
r = r 0us/(us
1
2

E0 =

(V0

−

up)

−

V )(P + P0)

(7.1)

(7.2)

(7.3)

where E0, P0, V0, and r 0 are the initial energy, pressure, volume, and density, respectively.
The velocity of the shock wave is us and up is the velocity of the projectile driving the
shock.

There are a number of methods for accelerating a projectile (Cable, 1970), but the two
most prominent methods for hydrogen targets are the two stage light gas gun (Nellis et al.,

62

1983; Holmes et al., 1995; Weir et al., 1996; Nellis et al., 1999) and a large laser (Silva
et al., 1997; Collins et al., 1998; Celliers et al., 2000).

Recent advances make it possible to measure the temperature by light emission from
the samples during compression (Holmes et al., 1995). Measurement of the conductivity is
also possible, used in recent experiments to detect metallic hydrogen (Weir et al., 1996).

The diamond anvil cell (DAC) is used to generate large static pressures. It has been
used to study the ﬂuid phase and several solid phases (Mao and Hemley, 1994). It has also
been used to determine the melting curve for hydrogen up to 500K (Diatschenko et al.,
1985; Datchi et al., 2000).

7.2 Theory

Free energy models are typically based on the chemical picture, where molecules, atoms
and various types of ions are all treated as different species of particles. Solving the phys-

ical picture, where the only fundamental particles are electrons and protons, is much more
difﬁcult (see Path Integral Monte Carlo below). The free energy of the various phases is
constructed from a variety of ﬁts to experimental data, equation of state data from refer-

ence systems (Lennard-Jones and hard sphere), and empirical and theoretical interaction
potentials.

One of the best known models is that of Saumon and Chabrier (Saumon and Chabrier,
1991, 1992). Extensive tables for astrophysical use were published by Saumon et al.

(1995). Another model was developed by Kitamura and Ichimaru (1998), to study the
plasma (metal-insulator) transition.

The Path Integral Monte Carlo (PIMC) method is in principle the best method for sim-
ulations, since it treats both the electrons and protons quantum mechanically at non-zero

temperature (Pierleoni et al., 1994; Magro et al., 1996; Militzer and Ceperley, 2000; Mil-
itzer, 2000). The only major uncontrolled approximation is the location of the electron
nodes, with problems and a solution similar to the ﬁxed node method in DMC. Militzer

and Pollock (2000) have made progress in improving the nodal structure used in these cal-
culations. PIMC is based on breaking up a thermal density matrix into a product of high
temperature components, and consequently it works well at high temperature and becomes
less efﬁcient as the temperature decreases. About 5000K is currently the lower limit for

PIMC calculations. Our CEIMC simulation technique should make a nice complement to
the PIMC method.

There have also been path integral studies using empirical potentials, in order to exam-
ine the quantum effects of the nuclei on the system (Wang et al., 1996, 1997; Cui et al.,

63

1997; Chakravarty, 1999).

The Car-Parrinello method has been used to simulate this system (Hohl et al., 1993;
Kohanoff et al., 1997; Pfafenzeller and Hohl, 1997; Galli et al., 2000). At low temperature,

it it necessary to treat the nuclei with path integrals (Biermann et al., 1998; Kitamura et al.,
2000). Some studies used LDA with the G point approximation (using only one k-point
for the integral over the Brillouin zone), which is not sufﬁcient to converge the anisotropic

behavior of the potential (Mazin and Cohen, 1995), and gives rise to unphysical planar
structures (Kohanoff et al., 1997).

7.3 Pressure and Kinetic Energy

The pressure is computed by a virial estimator based on the potential and kinetic energies

P =

[2

K

+

V

]

1
3V

(7.4)

h
where V is the volume and V is the potential energy. In these MC simulations, only the
kinetic energy of the electrons is explicitly computed. The kinetic energy of the nuclei must
also be added.

i

h

i

We are only considering hydrogen in the molecular state, and further assume that rota-
tional and vibrational motion can be separated. The characteristic temperature for quantum
effects for rotational motion is about 85 K for H2 (Landau and Lifshitz, 1980). At our sim-
ulation temperatures, we can use the classical expression for the rotational kinetic energy,
Erot = 2kT .

The characteristic vibrational temperature for H2 is q v = 6100 K, so it is necessary to

use the quantum expression for the vibrational kinetic energy. It is

Evib =

q v
q v/T

e−

1

−

(7.5)

For D2, the characteristic temperature should be a factor of √2 lower.

Of course, these expressions are only valid for a free molecule. To truly treat the kinetic
energy of the nuclei correctly in the interacting system, path integrals should be used for
the nuclei.

7.4 Individual Conﬁgurations

We took several conﬁgurations from PIMC simulations at 5000K at two densities (rs = 1.86
and rs = 2.0), and compared the electronic energy using VMC, DMC, DFT-LDA, and some

64

empirical potentials. The DFT-LDA results were obtained from a plane wave code using
an energy cutoff of 60 Rydbergs, and using the G point approximation (Ogitsu, 2000).

The empirical potentials are the Silvera-Goldman (Silvera and Goldman, 1978) and

the Diep-Johnson (Diep and Johnson, 2000a,b). To these we added the energy from the
Kolos (Kolos and Wolniewicz, 1964) intramolecular potential to get the energy as a function
of bond length variations. The Silvera-Goldman potential was obtained by ﬁtting to low

temperature experimental data, with pressures up to 20 kbar, and is isotropic. The Diep-
Johnson potential is the most recent in a number of potentials for the isolated H2-H2 system.
It was ﬁt to the results of accurate quantum chemistry calculations for a number of H2-H2
conﬁgurations. It is an anisotropic potential.

The energies relative to an isolated H2 molecule are shown in Figure 7.1. The ﬁrst
thing we notice is that the classical potentials are more accurate than VMC or DFT. The
Silvera-Goldman mostly does a good job of reproducing the DMC results. 1 Some of the
failures of the SG potential can be attributed to the lack of anisotropy. The isolated H2-H2
potential (Diep-Johnson) has much weaker interactions, compared with interactions in a
denser system.

The PIMC method itself gives an average energy of about 0.07(3) Ha for both densities.
Improvements in the fermion nodes appear to lower the energy (Militzer and Pollock, 2000;
Militzer and Ceperley, 2000), although the error bars are still quite large. There are also
corrections to some internal approximations that lower the energy by an additional 0.02
Ha (Militzer, 2000). These effects combined seem to bring the PIMC energy in rough

agreement with the DMC energy.

We used the Silvera-Goldman potential for pre-rejection. As seen in the Figure 7.1,
it resembles the DMC potential even though it lacks anisotropy. A hybrid potential was

created by Cui et al. (1997), taking the isotropic part from a potential that was ﬁt to high
density, and combining that with the anisotropic part from one of the isolated H2-H2 poten-
tials. We did not pursue this approach for constructing a better potential for pre-rejection.

7.5 Results

We obtained results from simulations at three state points, two of which can be compared
with the gas gun data of Holmes et al. (1995). The pressure is given in Table 7.1, with
results from the gas gun experiments, the Saumon-Chabrier model, from simulations using

the Silvera-Goldman potential, and from our CEIMC simulations. These state points are in
the ﬂuid molecular H2 phase. For the gas gun experiments, the uncertainties in the mear-

1It should be noted that we are taking the SG potential far from the temperature range it was ﬁt to.

65

DMC
DFT-LDA
VMC
Silvera-Goldman
Diep-Johnson

0.09

0.08

0.07

0.06

0.05

0.04

0.03

l

l

)
e
u
c
e
o
m
/
a
H

(
 
y
g
r
e
n
E

0.02

0

2

6

4
1-6, rs=1.86        7-12, rs=2.0

8

10

12

Figure 7.1: Electronic energy for several conﬁgurations computed by several methods.
The energy is relative to an isolated H2 molecule.

sured temperatures are around 100-200K. The experimental uncertainties in the volume

and pressure were not given, but previous work indicates that they are about 1-2% (Nellis
et al., 1983).

We did CEIMC calculations using VMC or DMC for computing the underlying elec-
tronic energy, which are the ﬁrst such QMC calculations in this range. The simulations at
rs = 2.1 and rs = 1.8 were done with 32 molecules, and the simulations at rs = 2.202 were
done with 16 molecules. We see that the pressures from VMC and DMC are very similar,
and that for rs = 2.1 we get good agreement with experiment.

There is a larger discrepancy with experiment at rs = 2.202. The ﬁnite size effects are
fairly large, especially with DMC. We also did simulations at rs = 2.1 with 16 molecules
and obtained pressures of 0.264(3) Mbar for CEIMC-VMC and 0.129(4) Mbar for CEIMC-
DMC. The Silvera-Goldman potential showed much smaller ﬁnite size effects than the

CEIMC simulations, so we that the electronic part of the simulation is largely responsible
for the observed ﬁnite size effects.

The energies for all these systems are given in Table 7.2. The energy at rs = 2.1 with
16 molecules for CEIMC-VMC is 0.0711(4) Ha and for CEIMC-DMC is 0.0721(8) Ha.
The average molecular bond length is given in Table 7.3, and we see the bond length is
compressed relative to the free molecule. The proton-proton distribution functions com-
paring CEIMC-VMC and CEIMC-DMC are shown in Figure 7.2. The VMC and DMC
distribution functions look similar, with the ﬁrst large intramolecular peak around r = 1.4
and the intermolecular peak around r = 4.5.

66

Table 7.1: Pressure from simulations and shock wave experiments
Pressure (Mbar)

V(cc/mol) T(K)

rs

Gasgun

S-C

S-G CEIMC-VMC CEIMC-DMC

2.100
2.202
1.800

6.92
7.98
4.36

4530
2820
3000

0.234
0.120
-

0.213 0.201
0.125 0.116
0.528

-

0.226(4)
0.105(6)
-

0.225(3)
0.10(5)
0.433(4)

Table 7.2: Energy from simulations and models, relative to the ground state of an isolated
H2 molecule. The H2 column is a single thermally excited molecule plus the quantum
vibrational KE.

rs

V(cc/mol) T(K)

Energy (Ha/molecule)

H2

S-C

S-G

CEIMC-VMC CEIMC-DMC

2.100
2.202
1.800

6.92
7.98
4.36

4530 0.0493 0.0643 0.0689
2820 0.0290 0.0367 0.0408
0.0722
3000 0.0311

-

0.0663(8)
0.0305(8)
-

0.0617(2)
0.0334(9)
0.055(1)

Table 7.3: Average molecular H2 bond length. The H2 column is a single thermally excited
molecule in free space.

rs

T(K)

Average bond length (Bohr)

H2

CEIMC-VMC CEIMC-DMC

2.100 4530 1.550

2.202 2820 1.486
1.800 3000 1.492

1.431(1)

1.443(1)
-

1.413(3)

1.429(6)
1.410(1)

DMC
VMC

DMC
VMC

)
r
(
g

4.5

3.5

2.5

1.5

4

3

2

1

0

0.5

0

1

2

3

4

5

6

7

0

1

2

4

5

6

r

(a)

3

r

(b)

Figure 7.2: Proton pair distribution function g(r) for (a) rs = 2.1 and T=4530 K (b) rs =
2.202 and T=2820 K

)
r
(
g

4.5

3.5

2.5

1.5

0.5

4

3

2

1

0

67

CEIMC-DMC
Hohl, et al.

)
r
(
g

1.5

3.5

2.5

3

2

1

0.5

0

0

Figure 7.3: The proton pair distribution function, g(r), close to rs = 1.8 and T = 3000K.

1

2

4

5

6

3
r

The CEIMC-VMC simulations at rs = 1.8 and 3000 K never converged. Starting from
a liquid state, the energy decreased the entire simulation. Looking at the conﬁgurations re-

vealed they were forming a plane. It is not clear whether it was trying to freeze, or forming
structures similar to those found in DFT-LDA calculations with insufﬁcient Brillouin zone
sampling (Hohl et al., 1993; Kohanoff et al., 1997). The CEIMC-DMC simulations did not

appear to have any difﬁculty, so is seems the VMC behavior was due to inadequacies of the
wave function.

Hohl et al. (1993) did DFT-LDA simulations at rs = 1.78 and T=3000K, which is very
close to our simulations at rs = 1.8. The resulting proton-proton distribution functions are
compared in Figure 7.3. The discrepancy between CEIMC and LDA in the intramolecular
portion of the curve has several possible causes. On the CEIMC side, it may be due to
an insufﬁciently long run or due to the molecular nature of the wave function, which does
not allow dissociation. The deﬁciencies of LDA may account for it preferring fewer and

less tightly bound molecules. LDA is known to overestimate the bond length of a free
hydrogen molecule (Hohl et al., 1993), which would account for the shifted location of the
bond length peak.

7.6 Simulation analysis

We also recorded some diagnostic information about the workings of the simulation, such
as the average noise level, the relative noise parameter f = (b
)2t/t0, and a quantity called

68

s
the additional noise rejection ratio, h
. When a move is rejected with the penalty method, it
is useful to recompute the acceptance decision with the same random number and without
If the move would have been accepted without the noise penalty, it
the noise penalty.

is considered a rejection due to noise (as opposed to a rejection due to a trial move that
increases the energy). This can be used to monitor the effects of the noise on the simulation.
The additional noise rejection ratio is deﬁned as

h =

Nnoise rej
Nnoise rej + Naccept

(7.6)

is small, the noise is causing few additional rejections. If h

If h
as many moves to be rejected as accepted. As h
be rejected.

→

is 1/2, the noise is causing
1, the noise is causing many moves to

Table 7.4 show the noise level (b

), the relative noise parameter, f , the additional noise
rejection ratio, a ratio of the error level for the direct method and the two-sided method,
and the time for a single quantum step. Looking at f , we see it is small for VMC and large
for DMC. This is because of VMC optimization takes a proportionately larger amount of

time in the VMC run than in the DMC run.

We used the two-sided method for computing energy differences of trial moves with
VMC, but only used the direct method with DMC. The column headed s 2
ts shows
how the efﬁciency of computing the energy difference is improved by using the two-sided

d/s 2

method. This improvement is only in the energy difference part of the total time, the op-
timization time is unaffected (and is a large part of the run time, since the f parameter is
small). In Chapter 3, there was an example showing that the two-sided method was not as

effective for DMC as for VMC. But in these simulations the DMC runs have a much larger
f parameter, so even small reductions in the noise level would have an impact on the run
time.

Some of DMC energy differences had values of noise many times greater than the

average, which may be due to an instability in the DMC algorithm. We removed these
outliers in computing the average noise level.

We tried the method for even lower temperatures with a simulation at T=800 K and
rs = 1.8 and it had a promising start, but after a while the acceptance ratio dropped and we
were unable to get any usable data.

7.7 Future Work

The ﬁnite size effects in DMC need to be resolved. Using Ewald sums for computing the
Coulomb interaction might help alleviate some of the ﬁnite size effects. Extending the

69

s
Table 7.4: Simulation quantities ordered according to average noise level, b
. The time
column is the time for a single quantum step in minutes on an SGI Origin 2000. N is the
number of molecules in the simulation.
T(K) N QMC b

time (min)

f

rs

2.100 4530 16 VMC 0.68 0.17 0.11

2.202 2820 16 VMC 0.70 0.27 0.13
2.100 4530 32 VMC 0.90 0.29 0.16
1.800 3000 32 VMC 0.91 0.30 0.15

2.100 4530 16 DMC 1.62 2.28 0.28
2.100 4530 32 DMC 1.74 5.30 0.29
2.202 2820 16 DMC 2.02 5.33 0.40
1.800 3000 32 DMC 2.42 13.1 0.42

d/s 2
2
ts
2.2

3.2
2.3
7.7

-
-
-
-

18

21
70
89

76
440
92
510

wave function to allow for dissociated molecules and to provide for ionization would help
make the simulations more accurate, particularly at higher temperatures and pressures.

70

s
s
h
s
Chapter 8

Conclusions

In this work we have developed new methods for increasing the scope of QMC calcula-
tions, and for increasing their efﬁciency. Variational Monte Carlo depends on optimizing

parameters, but the presence of noise makes it difﬁcult. We have examined several different
kinds of optimization approaches and compared them. Further work should improve these
methods even more.

The boson hard sphere model is an important theoretical model. We have performed
“computational experiments” to obtain the ground state energy of this model. The effects
of long range correlation on the energy are masked by the current uncertainty in the inﬁnite
system size results. However, if more accurate results are desired, the nature of the long

range correlations and their effect on the energy will need to be more clearly resolved.

As a method for including increasingly more detailed and accurate physical effects in
our simulations, we have developed the Coupled Electronic-Ionic Monte Carlo method.
The central idea is simple, but several supporting developments were needed to make it

computationally feasible. The penalty method enables use of energy differences with a
noise level of approximately kBT , rather than needing noise smaller than some fraction
of kBT to avoid bias. The two sided energy difference method can stably compute these
energy differences.

The CEIMC method was applied to a system of molecular hydrogen at a few state

points. It shows promise for generating accurate simulation results.

71

Appendix A

Determinant Properties

The elements of the Slater matrix are Di j = f

j(ri). The Slater determinant looks like

f 1(r1)
...
f 1(rn)

. . .
. . .
. . .

f n(r1)
...
f n(rn)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

We assume that the single particle orbitals depend only on a single coordinate (ie, no back-
ﬂow).

The determinant of a matrix can be computed using the expansion by cofactors. This

expands the determinant of an N
×
(N
1) matrices. As a recursive algorithm for computing the determinant, it is not very
efﬁcient, but for theoretical analysis, it is very useful for isolating the inﬂuence of a single

N matrix into a sum of N determinants of (N

1)

−

×

−

row or column.

Deﬁne the cofactors of a matrix M to be

(cid:12)
(cid:12)
−
1) matrix formed by removing row i and column j from A. The determinant of

where the matrix formed by ci j is called the cofactor matrix. The matrix Mi j is an (N
1)
A can then be written as

(N

−

×

(cid:12)
(cid:12)

ci j = (

1)i+ j

Mi j

−

= (cid:229)

A

|

|

j

ak jck j = (cid:229)

aikcik

i

for k = 1 . . . N. The transpose of the cofactor matrix is called the adjoint of A. Now the
adjoint is related to the inverse by

To compute the ratio of determinants, expand the determinant of D(r′k) in cofactors

about the kth row. Note that then the cofactors have no dependence on r′k.

adj A =

1
A−

A
|

|

72

(A.1)

(A.2)

(A.3)

(A.4)

D(r′k)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

D(r′k)
D(rk)
(cid:12)
|
(cid:12)

(cid:12)
|
(cid:12)

= (cid:229)

= (cid:229)

= (cid:229)

i

i

i

i(r′k)cki

i(r′k)

D(rk)
|

|

(D−

1(rk))ik

i(r′k)(D−

1(rk))ik

If a move is accepted, the inverse matrix can be updated in O(N2) time (rather than
O(N3) for recomputing the inverse). The formula for updating an inverse if only a sin-
gle row (or column) changes was given by Sherman and Morrison (1951). Let q be the

ratio of determinants given above. Row k merely needs to be updated to reﬂect the new
1
1
determinant, D−
k j = D−
k j /q . The other rows are updated as

1
1
i j = D−
D−
i j −

1
D−
ik
q

l

1
D−
l j

l(r′k)

i

= k

(A.5)

73

f
f
f
(cid:229)
f
6
Appendix B

Elements of the Local Energy

The wave function has the form

where D is the product of a spin up and a spin down Slater determinant and

y T = D exp [

U ]

−

U = (cid:229)

u(ri j).

i< j

The local energy is then

EL =

(cid:209) 2U

(cid:209) U

(cid:209) U

1
2

1
2

1
2

(cid:209) 2D
D

+

(cid:209) D
D

−
In Diffusion Monte Carlo, we need the quantum force, FQ = (cid:209)

−

(cid:19)

(cid:18)

(cid:18)

·

·

(cid:19)
ln

2.

|

|

(cid:209) U +V

(B.3)

The derivatives of the Jastrow factors are

FQ = 2

(cid:209) D
D

(cid:18)

−

(cid:19)

2(cid:209) U

kUee = (cid:229)

u′ee(rik)

kUne =

u′ne(rka )

ri

rk

−
rik

rk

Ra

−
rka

kUee = (cid:229)
(cid:209) 2

u′ee(rik) + u′′ee(rik)

i

=k

M

a =1

2
rik

2
rka

i

=k

M

a =1

(cid:209) 2

kUne =

u′ne(rka ) + u′′ne(rka )

= (cid:229)

[(cid:209)

kf

1
i(rk)] d−
ik

d

|

k
|
d
|
|

i

74

For the gradient with respect to particle k, expand the determinant by cofactors about row
k. Then the cofactors have no rk dependence.

(B.1)

(B.2)

(B.4)

(B.5)

(B.6)

(B.7)

(B.8)

(B.9)

y
(cid:209)
6
(cid:209)
(cid:229)
6
(cid:229)
(cid:209)
d

(cid:209) 2
k |
d
|
|

|

= (cid:229)

(cid:209) 2
k

i(rk)

1
d−
ik

i

(cid:2)

(cid:3)

(B.10)

75

f
Appendix C

Cusp Condition

When two Coulomb particles get close, the potential has a 1/r singularity. The wave func-
tion must have the correct form to cancel this singularity. First, consider an electron and a

nucleus. The relevant part of the Schr¨odinger equation is

(C.1)

(C.2)

(C.3)

1
2M

(cid:209) 2

n −

1
2

(cid:209) 2

e −

Ze2
r

y = Ey

−

(cid:20)

(cid:21)

(cid:1)

1
2

−

1
r

′′

−

Ze2y + y

= Ey

′

(cid:0)

1

′ =

Ze2

−

where M is the nuclear mass and Z is the nuclear charge. Assume that M

me, so the ﬁrst

term can be ignored. Write the second term in spherical coordinates and we get

≫

In order for the singularity to cancel at small r, the term multiplying 1/r must vanish. So
we have

If y = e−

cr we must have c = Ze2.

For the case of two electrons, the Schr¨odinger equation is

(cid:20)

−

(cid:209) 2

(cid:209) 2

1
2

1
2

2 +

y = Ey

1 −
Switching to relative coordinates r12 = r1

e2
r12 (cid:21)
r2 gives us
e2
r12 (cid:21)
Electrons with unlike spins (no antisymmetry requirement) have an extra factor of 1/2 in
the cusp condition compared with the electron-nucleus case. So we have c =

y = Ey

e2/2.

(C.5)

(C.4)

12 +

(cid:209) 2

−

−

(cid:20)

In the antisymmetric case, the electrons will be in a relative p state, reducing the cusp
e2/4. Having the correct cusp for like spin electrons gains very
condition by 1/2, so c =
little in the energy or the variance, since the antisymmetry requirement keeps them apart
anyway.

−

−

76

y
y
y
References

Aguilera-Navarro, V. C., S. Ho and M. de Llano, “Low-density expansions, computer ex-
periments, and the ground state energy of hard-sphere ﬂuids”, Phys. Rev. A 36, 12, 5742
(1987).

Alder, B. J. and T. E. Wainwright, “Phase transition for hard sphere system”, J. Chem.

Phys. 27, 1208 (1957).

New York, 1987).

Allen, M. P. and D. J. Tildesley, Computer Simulation of Liquids (Oxford University Press,

Anderson, J. B., “Exact quantum chemistry by Monte Carlo methods”, in “Quantum Me-
chanical Electronic Structure Calculations with Chemical Accuracy”, edited by S. R.
Langhoff, p. 1 (Kluwer Academic, 1995).

Bakeyev, T. and P. de Forcrand, “Noisy Monte Carlo revisited”, arXiv:hep-lat/0008006

(2000).

Barnett, R. N., Z. Sun and J. W. A. Lester, “Fixed-sample optimization in quantum Monte

Carlo using a probability density function”, Chem. Phys. Lett 273, 321 (1997).

Beliaev, S., “Application of the methods of quantum ﬁeld theory to a system of bosons”,

Sov. Phys. JETP 7, 289 (1958).

Comp. Phys. 22, 245 (1976).

157B, 70 (1985).

Bennett, C. H., “Efﬁcient estimation of free energy differences from Monte Carlo data”, J.

Bhanot, G. and A. D. Kennedy, “Bosonic lattice guage theory with noise”, Phys. Lett.

Biermann, S., D. Hohl and D. Marx, “Quantum effects in solid hydrogen at ultra-high

pressure”, Solid State Comm. 108, 337 (1998).

Boronat, J., J. Casulleras and S. Giorgini, “Quantum hard spheres as a model for a homo-

geneous Bose gas”, Physica B 284, 1 (2000).

Braaten, E. and A. Nieto, “Quantum corrections to the ground state energy of a homoge-

neous Bose gas”, arXiv:cond-mat/9712041 (1997a).

Braaten, E. and A. Nieto, “Renormalization effects in a dilute Bose gas”, Phys. Rev. B 55,

13, 8090 (1997b).

77

Bueckert, H., S. M. Rothstein and J. Vrbik, “Optimization of quantum Monte Carlo wave-

functions using analytical derivatives”, Can. J. Chem. 70, 366 (1992).

Cable, A. J., “Hypervelocity accelerators”, in “High velocity impact phenomena”, edited

by R. Kinslow, p. 1 (Academic Press, 1970).

Car, R. and M. Parrinello, “Uniﬁed approach for molecular dynamics and density-

functional theory”, Phys. Rev. Lett. 55, 2471 (1985).

Castellani, C., C. D. Castro, F. Pistolesi and G. C. Strinati, “Infared behavior of interacting

bosons at zero temperature”, Phys. Rev. Lett. 78, 9, 1612 (1997).

Casulleras, J. and J. Boronat, “Unbiased estimators in quatnum Monte Carlo methods:

Application to liquid 4He”, Phys. Rev. B 52, 3654 (1995).

Celliers, P. M., G. W. Collins, L. B. D. Silva, D. M. Gold, R. Cauble, R. J. Wallace, M. E.
Foord and B. A. Hammel, “Shock-induced transformation of liquid deuterium into a
metallic ﬂuid”, Phys. Rev. Lett. 84, 5564 (2000).

Ceperley, D. M., “Path integrals in the theory of condensed helium”, Rev. Mod. Phys. 67,

279 (1995).

Ceperley, D. M., G. V. Chester and M. H. Kalos, “Monte Carlo simulation of a many-

fermion system”, Phys. Rev. B 16, 3081 (1977).

Ceperley, D. M. and M. Dewing, “The penalty method for random walks with uncertain

energies”, J. Chem. Phys. 110, 9812 (1999).

Ceperley, D. M. and L. Mitas, “Quantum Monte Carlo methods in chemistry”, in “Advances
in Chemical Physics”, edited by I. Prigogine and S. A. Rice (Wiley and Sons, 1996).

Chakravarty, C., “Isothermal-isobaric ensemble simulations of melting in quantum solids”,

Phys. Rev. B 59, 3590 (1999).

Chester, G. V. and L. Reatto, “The ground state of liquid helium four”, Phys. Lett. 22, 276

(1966).

Collins, G. W., L. B. D. Silva, P. Celliers, D. M. Gold, M. E. Foord, R. J. Wallace, A. Ng,
S. V. Weber, K. S. Budil and R. Cauble, “Measurements of the equation of state of
deuterium at the ﬂuid insulator-metal transition”, Science 281, 1178 (1998).

Cui, T., E. Cheng, B. J. Alder and K. B. Whaley, “Rotational ordering in solid deuterium
and hydrogen: A path integral Monte Carlo study”, Phys. Rev. B 55, 12253 (1997).

Dalfovo, F., S. Giorgini, L. P. Pitaevskii and S. Stringari, “Theory of Bose-Einstein con-

densation in trapped gases”, Rev. Mod. Phys. 71, 463 (1999).

Datchi, F., P. Loubeyre, and R. LeToullec, “Extended and accurate determination of the
melting curves of argon, helium, ice (H2O), and hydrogen (H2)”, Phys. Rev. B 61, 6535
(2000).

78

Dennis, J. E. and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and

Nonlinear Equations (Prentice-Hall, 1983).

Dewing, M., “Improved efﬁciency with variational Monte Carlo using two level sampling”,

J. Chem. Phys. 113, 5123 (2000).

Diatschenko, V., C. W. Chu, D. H. Liebenberg, D. A. Young, M. Ross and R. L. Mills,
“Melting curves of molecular hydrogen and molecular deuterium under high pressures
between 20 and 373 k”, Phys. Rev. B 32, 381 (1985).

Diep, P. and J. K. Johnson, “An accurate H2-H2 interaction potential from ﬁrst principles”,

J. Chem. Phys. 112, 4465 (2000a).

Diep, P. and J. K. Johnson, “Erratum: An accurate H2-H2 interaction potential from ﬁrst

principles”, J. Chem. Phys. 113, 3480 (2000b).

Feynman, R. P. and M. Cohen, “Energy spectrum of the excitations in liquid helium”, Phys.

Rev 102, 1189 (1956).

Filippi, C. and C. J. Umrigar, “Multiconﬁguration wave functions for quantum Monte Carlo

calculations of ﬁrst-row diatomic molecules”, J. Chem. Phys. 105, 213 (1996).

Flyvbjerg, H. and H. G. Petersen, “Error estimates on averages of correlated data”, J. Chem.

Phys. 91, 461 (1989).

Galli, G., R. Q. Hood, A. U. Hazi and F. Gygi, “Ab initio simulations of compressed liquid

deuterium”, Phys. Rev. B 61 (2000).

Giorgini, S., J. Boronat and J. Casulleras, “Ground state of a homogeneous Bose gas: A

diffusion Monte Carlo calculation”, Phys. Rev. A 60, 5129 (1999).

Glynn, P. W., “Optimization of stochastic systems”, in “Proceedings of the 1986 Winter
Simulation Conference”, edited by J. Wilson, J. Henriksen and S. Roberts, p. 52 (1986).

Grossman, J. C., L. Mitas and K. Raghavachari, “Structure and stability of molecular car-

bon: Importance of electron correlation”, Phys. Rev. Lett. 75, 3870 (1995).

Hammond, B. L., J. W. A. Lester and P. J. Reynolds, Monte Carlo Methods in Ab Ini-
tio Quantum Chemistry, World scientiﬁc lecture and course notes in chemistry (World
Scientiﬁc, 1994).

Hansen, J. P., D. Levesque and D. Schiff, “Fluid-solid transition of a hard sphere Bose

system”, Phys. Rev. A 3, 776 (1971).

Hansen, P. C., Rank-deﬁcient and discrete ill-posed problems (SIAM, 1998).

Harju, A., B. Barbiellini, S. Siljamaki, R. M. Nieminen and G. Ortiz, “Stochastic gradient
approximation: An efﬁcient method to optimize many body wave functions”, Phys. Rev.
Lett. 79, 1173 (1997).

79

Hohl, D., V. Natoli, D. M. Ceperley and R. M. Martin, “Molecular dynamics in dense

hydrogen”, Phys. Rev. Lett. 71, 541 (1993).

Holmes, N. C., M. Ross and W. J. Nellis, “Temperature measurements and dissociation of
shock-compressed liquid deuterium and hydrogen”, Phys. Rev. B 52, 15835 (1995).

Huang, K. and C. Yang, “Quantum mechanical many body problem with hard-sphere in-

teractions”, Phys. Rev. 105, 767 (1957).

Huang, S., Z. Sun and J. W. A. Lester, “Optimized trial functions for quantum Monte

Carlo”, J. Chem. Phys. 92, 597 (1990).

Hubbard, W. B. and D. J. Stevenson, “Interior structure”, in “Saturn”, edited by T. Gehrels

and M. S. Matthews (University of Arizona Press, 1984).

Hugenholtz, N. M. and D. Pines, “Ground state energy and excitation spectrum of a system

of interaction bosons”, Phys. Rev. 116, 489 (1959).

Kalos, M. H., D. Levesque and L. Verlet, “Helium at zero temperature with hard-sphere

and other forces”, Phys. Rev. A 9, 5, 2178 (1974).

Keller, C., M. de Llano, S. Z. Ren, M. A. Solis and J. George A. Baker, “Quantum hard

sphere system equations of state revisited”, Annals of Physics 251, 64 (1996).

Kennedy, A. D. and K. Kuti, “Noise without noise: A new Monte Carlo method”, Phys.

Rev. Lett. 54, 2473 (1985).

versity of Cambridge (1999).

Kent, P. R. C., Techniques and Applications of Quantum Monte Carlo, Ph.D. thesis, Uni-

Kent, P. R. C., R. J. Needs and G. Rajagopal, “Monte Carlo energy and variance mini-
mization techniques for optimizing many body wave functions”, Phys. Rev. B 59, 12344
(1999).

Kincaid, D. and W. Cheney, Numerical Analysis (Brooks/Cole Publishing Company, 1991).

Kitamura, H. and S. Ichimaru, “Metal-insulator transitions in dense hydrogen: Equations
of state, phase diagrams and interpretation of shock-compression experiments”, J. Phys.
Soc. Japan 67, 950 (1998).

Kitamura, H., S. Tsuneyuki, T. Ogitsu and T. Miyake, “Quantum distribution of protons in

solid molecular hydrogen at megabar pressures”, Nature 404, 259 (2000).

Kohanoff, J., S. Scandolo, G. L. Chiarotti and E. Tosatti, “Solid molecular hydrogen: The

broken symmetry phase”, Phys. Rev. Lett. 78, 2783 (1997).

Kolos, W. and L. Wolniewicz, “Accurate adiabatic treatment of the ground state of the

hydrogen molecule”, J. Chem. Phys. 41 (1964).

Landau, L. D. and E. M. Lifshitz, Statistical Physics, 3rd edition (Pergamon Press, New

York, 1980).

80

Lebowitz, J. L. and J. K. Percus, “Long range correlations in a closed system with applica-

tions to nonuniform ﬂuids”, Phys. Rev. 122, 1675 (1961).

Lee, T., K. Huang and C. Yang, “Eigenvalues and eigenfunctions of a bose system of hard

spheres and its low temperature properties”, Phys. Rev. 106, 1135 (1957).

Lin, L., K. F. Liu and J. Sloan, “A noisy Monte Carlo algorithm”, arXiv:hep-lat/9905033

(1999).

Lin, X., H. Zhang and A. M. Rappe, “Optimization of quantum Monte Carlo wave functions

using analytical energy derivatives”, J. Chem. Phys. 112, 2650 (2000).

Liu, K. S., M. H. Kalos and G. V. Chester, “Quantum hard spheres in a channel”, Phys.

Rev. A. 10, 303 (1974).

Magro, W. R., D. M. Ceperley, C. Pierleoni and B. Bernu, “Molecular dissociation in hot,

dense hydrogen”, Phys. Rev. Lett. 76, 1240 (1996).

Mao, H. and R. J. Hemley, “Ultrahigh-pressure transitions in solid hydrogen”, Rev. Mod.

Phys. 66, 671 (1994).

Marx, D. and M. Parrinello, “Ab initio path integral molecular dynamics: Basic ideas”, J.

Chem. Phys. 104, 4077 (1996).

Mazin, I. I. and R. E. Cohen, “Insulator-metal transition in solid hydrogen: Implications of
electronic structure calculations for recent experiments”, Phys. Rev. B 52, R8597 (1995).

McMillan, W. L., “Ground state of liquid He4”, Phys. Rev. 138, 442 (1965).

Metropolis, N., A. W. Rosenbluth, M. Rosenbluth, A. H. Teller and E. Teller, “Equation of

state calculations by fast computing machines”, J. Chem. Phys. 21, 1087 (1953).

Metropolis, N. and S. Ulam, J. Am. Stat. Assoc. 44, 335 (1949).

Militzer, B., Path Integral Monte Carlo Simulations of Hot Dense Hydrogen, Ph.D. thesis,

University of Illinois at Urbana-Champaign (2000).

Militzer, B. and D. M. Ceperley, “Path integral monte carlo calculation of the deuterium

hugoniot”, Phys. Rev. Lett. 85, 1890 (2000).

Militzer, B. and E. L. Pollock, “Variational density matrix method for warm, condensed

matter: Application to dense hydrogen”, Phys. Rev. E 61, 3470 (2000).

Nellis, W. J., A. C. Mitchell, M. van Theil, G. J. Devine, R. J. Trainor and N. Brown,
“Equation of state data for molecular hydrogen and deuterium at shock pressures in the
range 2-76 gpa (20-760 kbar)”, J. Chem. Phys. 79, 1480 (1983).

Nellis, W. J., S. T. Weir and A. C. Mitchell, “Minimum metallic conductivity of ﬂuid

hydrogen at 140 gpa (1.4 mbar)”, Phys. Rev. B 59, 3434 (1999).

81

Nemirovksy, A. S. and D. B. Yudi, Problem complexity and method efﬁciency in optimiza-

tion (1983).

Nightingale, M. P., “Basics, quantum Monte Carlo and statistical mechanics”, in “Quantum
Monte Carlo Methods in Physics and Chemistry”, edited by M. P. Nightingale and C. J.
Umrigar, vol. 525 of Nato Science Series C: Mathematical and Physical Sciences, pp.
1–36 (Kluwer Academic, Dordrecht, The Netherlands, 1999).

Ogitsu, T.,

(multiple parallel density funtional
http://www.ncsa.uiuc.edu/Apps/CMP/togitsu/MPdft.html (2000).

theory) code”, URL

“Mp-dft

Ortega, J. M. and W. C. Rheinboldt, Iterative solution of nonlinear equations in several

variables (Academic Press, 1970).

Parr, R. G. and W. Yang, Density Functional Theory of Atoms and Molecules (Oxford,

1989).

1997).

Payne, M. C., M. P. Teter, D. C. Allan, T. A. Arias and J. D. Joannopoulos, “Iterative
minimization techniques for ab initio total-energy calculations: molecular dynamics and
conjugate gradients”, Rev. Mod. Phys 64, 1045 (1992).

Pfafenzeller, O. and D. Hohl, “Structure and electrical conductivity in ﬂuid high-density

hydrogen”, J. Phys: Condens. Matter 9, 11023 (1997).

Pierleoni, C., D. M. Ceperley, B. Bernu and W. R. Magro, “Equation of state of the hydro-

gen plasma by path integral Monte Carlo simulation”, PRL 73, 2145 (1994).

Polak, E., Optimization: Algorithms and Consistent Approximations (Springer-Verlag,

Press, W. H., S. A. Teukolsky, W. T. Vetterling and B. P. Flannery, Numerical Recipes in

Fortran. Second Edition (Cambridge University Press, 1992).

Reynolds, P. J., D. M. Ceperley, B. J. Alder and W. A. Lester, “Fixed-node quantum Monte

Carlo for molecules”, J. Chem. Phys. 77, 5593 (1982).

Robbins, H. and S. Munro, “A stochastic approximation method”, Annals of Math. Stat.

22, 400 (1951).

Rev. A 44, 5122 (1991).

Rev. A 46, 2084 (1992).

Saumon, D. and G. Chabrier, “Fluid hydrogen at high density: Pressure dissociation”, Phys.

Saumon, D. and G. Chabrier, “Fluid hydrogen at high density: Pressure ionization”, Phys.

Saumon, D., G. Chabrier and H. M. Van Horn, “An equation of state for low-mass stars and

giant planets”, Astrophys. J. Sup. 99, 713 (1995).

Schmidt, K. E. and J. W. Moskowitz, “Correlated Monte Carlo wave functions for the atoms

he through ne”, J. Chem. Phys. 93, 4172 (1990).

82

Sherman, J. and W. J. Morrison, “Adjustment of an inverse matrix corresponding to a

change in one element of a given matrix”, Annals of Math. Stat. 21, 124 (1951).

Silva, L. B. D., P. Celliers, G. W. Collins, K. S. Budil, N. C. Holmes, J. T. W. Barbee,
B. A. Hammel, J. D. Kilkenny, R. J. Wallace, M. Ross, R. Cauble, A. Ng and G. Chiu,
“Absolute equation of state measurements on shocked liquid deuterium up to 200 gpa (2
mbar)”, Phys. Rev. Lett. 78, 483 (1997).

Silvera, I. F. and V. V. Goldman, “The isotropic intermolecular potential for H2 and D2 in

the solid and gas phase”, J. Chem. Phys. 69 (1978).

Snajdr, M., J. R. Dwyer and S. M. Rothstein, “Histogram ﬁltering: A technique to optimize
wave functions for use in Monte Carlo simulations”, J. Chem. Phys. 111, 9971 (1999).

Snajdr, M. and S. M. Rothstein, “Are properties derived from variance-optimized wave-
functions generally more accurate? Monte Carlo study of non-energy-related properties
of H2, He and LiH”, J. Chem. Phys. 112, 4935 (2000).

Sprik, M., “Ab initio molecular dynamics simulation of liquids and solutions”, J. Phys.:

Condens. Matter 12, A161 (2000).

Stevenson, D. J., “The role of high pressure experiment and theory in our understanding of
gaseous and icy planets”, in “Shock waves in condensed matter”, edited by S. C. Schmidt
and N. C. Holmes, p. 51 (Elsevier, 1988).

Sun, Z., P. J. Reynolds, R. K. Owen and J. W. A. Lester, “Monte Carlo study of electron

correlation functions for small molecules”, Theor. Chim. Acta 75, 353 (1989).

Tsypkin, Y. Z., Adaptation and learning in automatic systems (Academic Press, 1971).

Tuckerman, M. E. and G. J. Martyna, “Understanding modern molecular dynamics: Tech-

niques and applications”, J. Phys. Chem. B 104, 159 (2000).

Umrigar, C. J. and C. Filippi, “Correlated sampling in quantum Monte Carlo: A route to

forces”, Phys. Rev. B 61, R16291 (2000).

Umrigar, C. J., K. G. Wilson and J. W. Wilkins, “Optimized trial wave functions for quan-

tum Monte Carlo calculations”, Phys. Rev. Lett. 60, 1719 (1988).

Wang, Q., J. K. Johnson and J. Q. Broughton, “Thermodynamic properties and phase equi-
librium of ﬂuid hydrogen from path integral simulations”, Mol. Phys. 89, 1105 (1996).

Wang, Q., J. K. Johnson and J. Q. Broughton, “Path integral grand canonical Monte Carlo”,

J. Chem. Phys 107, 5108 (1997).

Weir, S. T., A. C. Mitchell and W. J. Nellis, “Metallization of ﬂuid molecular hydrogen at

140 gpa (1.4 mbar)”, Phys. Rev. Lett. 76, 1860 (1996).

Wells, B. H., “The differential Green’s function Monte Carlo method. the dipole moment

of LiH”, Chem. Phys. Lett. 115, 89 (1985).

83

Williamson, A. J., S. D. Kenny, G. Rajagopal, A. J. James, R. J. Needs, L. M. Fraser,
W. M. C. Foulkes and P. Maccullum, “Optimized wave functions for quantum Monte
Carlo studies of atoms and solids”, Phys. Rev. B 53, 9640 (1996).

Wood, W. W. and J. D. Jacobson, “Preliminary results from a recalculation of the Monte

Carlo equation of state of hard spheres”, J. Chem. Phys. 27, 1207 (1957).

Wu, T., “Ground state of a Bose system of hard spheres”, Phys. Rev. 115, 1390 (1959).

Young, P., “Optimization in the presense of noise”, in “Optimization in Action”, edited by

L. C. W. Dixon (Academic Press, London, 1976).

Zel’dovich, Y. B. and Y. P. Raizer, Physics of Shock Waves and High-Temperature Hydro-

dynamic Phenomena (Academic Press, 1966).

84

Vita

Mark Dewing was born on May 18, 1971 in San Diego, California. He received a B. S. in

physics from Michigan Technological University in 1993. He received an M. S. in physics
from the University of Illinois at Urbana-Champaign in 1995.

85


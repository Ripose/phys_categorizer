8
9
9
1
 
c
e
D
 
7
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
1
1
0
2
1
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

MZ-TH/98-54

Parallelization of adaptive MC integrators

Reent pvegas developments

Rihard Krekel

∗

Deember 7, 1998

Abstrat

der to redue the sequential fration in Amdahl's law,

This paper shortly desribes some important hanges to the

(cid:29)utuations. Although these (cid:29)utuations are nothing

pvegas-ode sine its (cid:28)rst publiation. It proeeds with a

new (merely representing the statistial nature of MC

report on the saling-behavior that was found on a wide

integration) they an nevertheless trouble.

It turned

range of urrent parallel hardware and disusses some issues

out to be impossible to obtain exatly the same output

of optimization that may be thrown up.

if more than one proessor was used. To get around this

the numerial output is sub jet to additional random

Sine the (cid:28)rst publi announement of our parallel

reproduible results if the user wishes to.

If enabled,

problem the ode was restrutured in order to allow for

version of G. P. Lepage's vegas-algorithm [1℄ for mul-

this feature will initialize all RNGs identially and sub-

tidimensional numerial integration in Otober 1997 [2℄

sequently an algorithm will deide how muh to advane

work has been done to improve the pvegas-ode

by

1

eah RNG suh that exatly the same D-dimensional

adding new features and making it more portable suh

sample-points are evaluated. We will all this feature

that nearly all present-day parallel hardware is sup-

ausal random-number generation. Thus the output is

ported.

We feel that these improvements ought to be

2

independent from the number of proessors p and an

published to the ommunity of pvegas-users now es-

easily be heked by a mahine, using diff for instane.

peially sine we (cid:28)nally onsider this work ompleted

It is self-evident that this slows down the program and

due both to positive reports by users and a onver-

should be used for the purpose of debugging only and

gene to the widely aepted standards Posix 1003.1

not in prodution runs.

3

and MPI [3℄.

The seond innovation is an implementation of

Our original approah to parallelization based on

pvegas using the MPI Message-Passing Interfae Stan-

dimensional parallel subspae and its orthogonal om-

ble of running pvegas substantially, inluding massive

dard [3℄. This enlarges the spetrum of mahines apa-

splitting up the D-dimensional spae into a Dk -

plement (the D⊥ = D − Dk -dimensional orthogonal

spae) and using the strati(cid:28)ation-grid for deomposi-

alike. This MPI-ode also allows ausal random number

parallel mahines and networks of workstations (NOWs)

tion as outlined in [2℄ remains untouhed. Setion 1

generation.

desribes the most important additions.

Simultaneously pvegas has been applied by numer-

ous researhers both in industry and universities.

Its

2 New platforms sine 1996

saling-behavior has been probed in pratie on a wide

spetrum of hardware. The remaining two setions are

Several new hardware-platforms have beome available

devoted to a disussion of platforms.

sine our (cid:28)rst tests of pvegas in Winter 1996/97 [2℄.

While the very high-end of superomputers remains

a domain of massively parallel mahines [8℄, SMP-

1 Code improvements

workstations with two or more CPUs are quikly (cid:28)nd-

Sine the original pvegas features an independent ran-

dom number generator (RNG) for eah proessor in or-

ing their way into labs or even on the desktops of

individual researhers. Sine these mahines feature

the programming-paradigm of shared-memory and in

∗

1

2

Inst. f. Physik, Johannes Gutenberg-Univ. Mainz, Germany,

most ases support Posix-threads as de(cid:28)ned by IEEE

email: rihard.krekeluni-mainz.de

The

soures

are

available via anonymous-FTP from

in Posix Setion 1003.1, they are well-suited for run-

ftp://ftpthep.physik.uni-mainz.de/pub/pvegas/.

ning the multi-threaded pvegas and we report on some

Independently from us, Sini²a Veseli has worked on an imple-

of them in the present paper.

In addition, the newer

mentation of vegas for mahines with distributed memory using

the PVM-library [6℄ and Thorsten Ohl has prepared a parallel ver-

Sine the ode for this feature is ompletely hash-de(cid:28)ned,

sion for MPI-based systems that makes an attempt to overome

absolutely no overhead is introdued when it is swithed o(cid:27), thus

vegas' inherent problems with non-fatorizable singularities [7℄.

guaranteeing the usual performane.

3

1

Vendor:

Arhiteture:

CPU:

MHz: OS:

pmax : Model: omment:

Convex

SPP-1200

PA-7200

120

SPP-UX 4.2

46 CPS

su(cid:27)ers from a looping

HP

X-lass

PA-8000

180

SPP-UX 5.2

46 CPS/

dto., but only if

main thread of exeution

Posix

CPS-threads are used

Cray

T3D

EV4

150

Unios Max

256 MPI

dto., sine MPI (with

1.3.0.3

expliit master-proess)

Siemens-

Solaris-NOW

Pentium-II 300

SunOS 5.6

31 MPI

dto. (prototype of

Sali-Dolphin

ommerial produt)

DEC

AlphaServer 8400 EV5

300

D.U. 4.0

6 Posix

((cid:16)Turbo-Laser(cid:17))

SGI

Origin 200

R10000

180

IRIX 6.4

4 Posix

Sun

E3000

UltraSpar 250

SunOS 5.5.1

4 Posix

self-made

Linux-NOW

AMD K6

233

GNU/

5 MPI

plus one additional

system

Linux 2.0

dediated master-mahine

Table 1: Overview of Hardware tested. pmax refers to the number of CPUs whih were used in our tests, not

neessarily the number of CPUs installed.

MPI version using the same deomposition method as

in eah dimension. Thus 2 · 215 = 8 168 202 points are

the approved pvegas has been tested on Cray-systems

being evaluated.

and NOWs. These results will also be reviewed in the

This integration idealizes a realisti alulation in el-

following setion.

ementary partile physis and it turned out to be quite

We hallenge the seletion of mahines in Table 3

able to probe the hardware struture and unover prob-

with the omputational problem already familiar from

lems in on(cid:28)guration and optimization. This is the rea-

our original publiation [2℄. (For this artile we inlude

son why we ontinued using it for measurements of ef-

only mahines that have at least 4 CPUs.) The list in-

(cid:28)ieny.

ludes ommerially available produts (the SMP ma-

In Fig. 1 we hoose to normalize all measured e(cid:30)-

hines AlphaServer 8400 and E3000, the mixed arhi-

ienies with respet to one proessor on the Convex

tetures SPP-1200, X-lass and Origin 200 and massive

SPP-1200 whih took about 40 minutes to omplete the

parallel systems like the T3D) as well as systems built

task. Fig. 2 shows absolute runtimes on the hardware

from ommodity hardware like the Paderborn luster of

tested plotted double-logarithmially.

32 Dual Pentium-II with a Salable Coherent Interon-

Both (cid:28)gures demonstrate the rather good overall sal-

net (SCI) and a quikly assembled Linux-NOW run-

ability. Several aspets deserve speial mention:

ning MPI over ordinary Ethernet.

3 Comparison

This is to be expeted by the nature of the test.

All urves are modulated by a visible grain-size ef-

fet at p & 32 as an best be seen at the X-lass.

To reapitulate, the hallenge onsisted of integrating a

A slight drop-o(cid:27) at the boundary of hypernodes is

normalized test funtion whih demanded evaluation of

apparent in the ase of the X-lass (multiples of

8 Dilogarithms omputed with a method outlined in [4℄.

16) and an also be found in the runtimes of the

(This ase resembles a typial situation in xloops [5℄.)

SPP-1200 (multiples of 8).

One must, however, be areful when deduing any-

thing for other integrals(cid:22)before embarking on large-

The SCI-based luster of PCs' saturation is due

sale omputations one should always onsider measur-

to problems with sub-optimal hoies of SCI pa-

ing the behavior of one's mahine.

rameters in this prototype mahine(cid:22)a prodution-

The D = 5 -dimensional problem is split up into
a Dk = 2 -dimensional parallel spae and a D⊥ = 3

mahine an be expeted to perform better. Tests

have shown that on this mahine the saturation an

-dimensional orthogonal spae. Trying to integrate

in priniple be avoided by using larger problem-

8.2 · 106

sample-points results in a grid with 21 slies

sizes.

•

•

•

2

Convex SPP-1200
HP X-Class
DEC 8400
SGI Origin 200
Sun E3000
SCI based PC-Cluster
Linux-NOW
Cray T3D

y
c
n
e
i
c
i
f
f
e

7

6

5

4

3

2

1

0

8

16

24

40
32
number of processors p

48

56

64

Figure 1: Relative e(cid:30)ieny of pvegas on di(cid:27)erent arhitetures, normalized to the SPP-1200.

•

The T3D's drop-o(cid:27) an almost purely be explained

the outlined transformation, a sum of Taylor series, in-

by the grain-size e(cid:27)et. Even a derease of grain-

volves omplex multipliations, a ode optimized muh

size by inreasing Dk from 2 to 3 still showed nearly
perfet saling up to p = 256.

better by the IRIX ompiler than on all the other ones

(e.g. e(cid:27)etively four yles per omplex multipliation

in ontrast to 27 on the Turbo-Laser).

The lesson from

4

The good salar performane of the SGI Origin 200

and the X-lass is somewhat surprising.

It resembles

all this is that one must be extremely areful when try-

most benhmarks (e.g. [8℄, based on Linpak [9℄) only

ing to predit the performane of any nonlinear ode.

partially. Of ourse these di(cid:27)erenes an diretly be

With respet to pvegas we should of ourse not use the

traed bak to the numerial e(cid:27)ort of our integrand.

presented absolute numbers to rate the genuine pvegas-

Inspetion of the ode with the pro(cid:28)ling-tool available

performane.

on eah individual system reveals some soures for the

strikingly di(cid:27)erent performanes(cid:22)the ability of the pro-

essor to deal with ode full of jumps together with the

4 Conlusion

ompiler's performane at optimization.

The nearly onstant saling of the multi-threaded

To understand this we need to have a look at the

pvegas found in our original work [2℄ on the SPP-1200

benhmark integrand. The series for alulating Dilog-

turns out to be a ase reahed with most urrent hard-

arithms suggested in [4℄ and used here is enhaned by

ware. This was reported independendly by a wide va-

applying some relations holding between Dilogarithms

riety of researhers. While the old SPP-1200 disap-

in order to evaluate the series only where it onverges

points by a lak of Posix-threads (resulting in the need

(within the omplex unit-irle) and further where it

of a somewhat lumsy ode that respets two di(cid:27)er-

onverges fast (within a retangle inside the unit-irle

ent thread-models) the X-Class provides both of them.

that not even touhes the irle itself ). This transfor-

mation with many onditional statements and even o-

Only the vendor's ompilers with aggressive optimization-

asional reursions is in itself the (cid:28)rst soure of possible

settings were used on these platforms:

performane losses.

It learly probes the proessor's

branh-predition and branh-penalty together with its

X-lass: Exemplar 89 C-ompiler V 1.2.1

Origin 200: MIPSpro C-ompiler V 7.20

E3000: Sun WorkShop C-ompiler V 4.2

potential in doing out-of-order exeution(cid:22)the latter is

Turbo-Laser: DEC C V5.2-038

probably the ause for the relatively poor salar perfor-

GNU g V 2.8.1 was used on the PC-luster sine it turned out

mane of the Ultra-SPARC proessors. The result of

to be the fastest ompiler for that partilular ode.

4

•
•
•
•

3

Convex SPP-1200
HP X-Class
DEC 8400
SGI Origin 200
Sun E3000
SCI based PC-Cluster
Linux-NOW
Cray T3D

1000

s
/
t

100

10

1

2

4

64

128

256

8

32
16
number of processors p

Figure 2: Runtimes of pvegas on di(cid:27)erent arhitetures in seonds.

Some small and less ostly SMP-mahines also do a rea-

[3℄ MPI: A Message-Passing Interfae Standard ; Uni-

sonable job, thus beoming a very attrative tool for the

versity of Tennessee, Knoxville, Tennessee, (1995)

numerially demanding researher.

In the meantime, using an expliit farmer-worker

[4℄ G. 't Hooft, M. Veltman: Salar one-loop integrals ;

model and the MPI standard for message-passing was

Nul. Phys. B 153 (1979) 365

found to deliver very satisfying performane if the prie

[5℄ L. Brüher, J. Franzkowski, A. Frink, D. Kreimer:

of an idling master CPU an be paid. These results

Introdution to xloops ; hep-ph/9611378

depend, of ourse, strongly on the latenies of the un-

derlying network or message-passing hardware.

[6℄ S. Veseli: Multidimensional integration in a hetero-

geneous network environment ; FERMILAB-PUB-

Aknowledgements:

I wish to thank the Edinburgh

97/271-T; physis/9710017

Parallel Computing Centre for hospitality during a visit

[7℄ T. Ohl: Vegas Revisited: Adaptive Monte Carlo

in winter 1998 and their invaluable support of part

Integration Beyond Fatorization ; hep-ph/9806432

of this work.

I am also grateful to GIP Mainz and

to Paderborn Center for parallel Computing for their

[8℄ J. J. Dongarra, H. W. Meuer, E. Strohmaier: Top

feedbak as well as aess to their mahines. Again,

500 Superomputer Sites ; Editions 1-12, Univer-

Markus Take has ontributed some very fruitful om-

sity of Mannheim, Germany, June and November,

ments about issues of optimization.

(1993-1998)

Referenes

Tennessee, CS-89-85, (1994)

[9℄ J. J. Dongarra: Performane of various omputers

using standard linear equations software ; Univ. of

[1℄ G. P. Lepage: A New Algorithm for Adaptive Mul-

tidimensional Integration ; J. Comput. Phys. 27

(1978) 192-203

[2℄ R. Krekel: Paral lelization of adaptive MC Inte-

grators ; Comput. Phys. Commun. 106 (1997) 258;

physis/9710028

4


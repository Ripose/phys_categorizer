8
9
9
1
 
c
e
D
 
8
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
5
3
0
2
1
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The Penalty Method for Random Walks with Uncertain Energies

D. M. Ceperley and M. Dewing
Department of Physics and NCSA
University of Illinois at Urbana-Champaign, Urbana, IL 61801
(September 15, 2013)

We generalize the Metropolis et al. random walk algorithm to the situation where the energy
is noisy and can only be estimated. Two possible applications are for long range potentials and
for mixed quantum-classical simulations. If the noise is normally distributed we are able to modify
the acceptance probability by applying a penalty to the energy diﬀerence and thereby achieve exact
sampling even with very strong noise. When one has to estimate the variance we have an approximate
formula, good in the limit of large number of independent estimates. We argue that the penalty
method is nearly optimal. We also adapt an existing method by Kennedy and Kuti and compare to
the penalty method on a one dimensional double well.

I. INTRODUCTION

As Metropolis et al. showed in 19531, Markov random walks can be used to sample the Boltzmann distribution
thereby calculate thermodynamic properties of classical many-body systems. The algorithm they introduced is one of
the most important and pervasive numerical algorithms used on computers because it is a general method of sampling
arbitrary highly-dimensional probability distributions. Since then many extensions have been developed2. In addition
to the sampling of classical systems, many Quantum Monte Carlo algorithms such as Path Integral Monte Carlo3,
variational Monte Carlo4 and Lattice Gauge Monte Carlo use a generalization of the random walk algorithm.

s
{
s′), thus generating a random walk through state space,

In a Markov process, one changes the state of the system
(s

randomly according to a ﬁxed transition rule,
. The transition probabilities often
s0, s1, s2 . . .
P
}
satisfy the detailed balance property (a suﬃcient but not necessary condition). This means that the transition rate
from s to s′ equals the reverse rate:

}
{

→

π(s)

(s

P

→

′

s

) = π(s

′

(s

′

)
P

→

s).

exp(

Here π(s) is the desired equilibrium distribution which we take for simplicity to be the classical Boltzmann distribution:
s′)
π(s)
}
s′) is ergodic, then the random walk will eventually converge to π. For more
satisfy detailed balance and if
details see Refs. [ 5,6].

V (s)/(kBT )) where T is the temperature and V (s) is the energy. If the pair of functions

π(s),

→

→

(s

(s

∝

−

P

P

{

In the particular method introduced by Metropolis one ensures that the transition rule satisﬁes detailed balance by
s′) (a probability distribution that can be directly sampled
1.

splitting it into an “a priori” sampling distribution T (s
such as a uniform distribution about the current position) and an acceptance probability a(s
The overall transition rate is:

s′) with 0

→

→

≤

≤

a

(s

P

→

′

s

) = T (s

′

s

)a(s

′

s

).

→

→

Metropolis et al.1 made the choice for the acceptance probability:
′

′

aM (s

s

) = min [1, q(s

s)] ,

→

→

where

′

s

) =

q(s

→

π(s′)T (s′
π(s)T (s

s)
s′)

→
→

= exp(

(V (s

)

V (s))/(kBT )).

′

−

−

Here we are assuming for the sake of simplicity that T (s′
s′). The random walk does not simply
proceed downhill; thermal ﬂuctuations can drive it uphill. Moves that lower the potential energy are always accepted
but moves that raise the potential energy are often accepted if the energy cost (relative to kBT = 1/β) is small. Since
asymptotic convergence can be guaranteed, the main issue is whether conﬁguration space is explored thoroughly in a
reasonable amount of computer time.

s) = T (s

→

→

What we consider in this article is the common situation where the energy, V (s) needed to accept or reject moves,

is itself uncertain. This can come about because of two related situations:

(1)

(2)

(3)

(4)

1

•

•

The energy may be expressed as an integral: V (s) =
need to perform the integral with another subsidiary Monte Carlo calculation.

R

dxv(x, s). If the integral has many dimensions, one might

N
The energy may be expressed as a ﬁnite sum: V (s) =
k=1 ek(s) where is N is large enough that performing
the summation slows the calculation. It might be desirable for the sake of eﬃciency to sample only a few terms
P
in the sum.

A. Mixed Quantum-Classical Simulation

First, consider the typical system in condensed matter physics and chemistry, composed of a number of classical
nuclei and quantum electrons. In many cases the electrons can be assumed to be in their ground state and to follow
the nuclei adiabatically. To perform a simulation of this system, we need to accept or reject the nuclear moves based
on the Born-Oppenheimer potential energy VBO(s), deﬁned as the eigenvalue of the electronic Schrˆodinger equation
with the nuclei ﬁxed at position s. In most applications, this potential is approximated by a semi-empirical potential
typically involving sums over pair of particles. More recently, in the Car-Parrinello molecular dynamics method7, one
performs a molecular dynamics simulation of the ions simultaneous with a solution of the electronic quantum wave
equation. To be feasible one uses a mean ﬁeld approximation to the full many-body Schr¨odinger equation using the
local density functional approximation to density functional theory or a variant. Others have proposed coupling a
nuclear Monte Carlo random walk to an LDA calculation8. Although mean-ﬁeld methods such as LDA are among
the most accurate methods fast enough to be useful for large systems, they also have known deﬁciencies9.

We would like to use a quantum Monte Carlo (QMC) simulation to calculate VBO(s) during the midst of a classical
MC simulation (CMC)10. QMC methods, though not yet rigorous because of the fermion sign problem, are the most
accurate methods useful for hundreds of electrons. But QMC simulation will only give an estimate of VBO(s) with
some statistical uncertainty. It is very time consuming to reduce the error to a negligible level. We would like to take
into account the statistical error without having to reduce it to zero.

Note that we do not wish the new Monte Carlo procedure to introduce uncontrolled approximations because the
goal of coupling the CMC and QMC is a robust, accurate method. We need to control systematic errors. It has
been noticed by Doll and Freeman11, after studying a simple example, that CMC is robust with respect to noise but
recommend using small noise levels and small step sizes to minimize the systematic errors. However, this can degrade
the overall eﬃciency. If we can tolerate higher noise levels without introducing systematic errors, the overall computer
algorithm will run faster and more challenging physical systems can be investigated, e.g. more electrons and lower
temperatures

B. Long-range potentials

∆V (r′

k) =

[v(r

′
kj )

v(rkj )].

−

N

Xj=1

In CMC with a pair potential, to compute the change in energy when particle k is moved to position r′

k, one needs

to compute the sum

This is referred to as an order N 2 algorithm since the computer eﬀort to move all particles once is proportional to
N 2. If the interaction has a ﬁnite range, neighbor tables12 will reduce this complexity to order N . However charged
systems with Coulomb interactions are not amenable to this treatment. Usually the Ewald image method is used to
handle the long-range potentials with a complexity13 of order N 3/2. The fast multipole method14, which scales as N
for the Coulomb interaction is not applicable to Monte Carlo since that method computes the total energy or force
and in MC we need the change in potential as a single particle is moved.

The challenge is to come up with an order N Monte Carlo method for charged systems. In the Ewald method, the

potential is split into a short-range part and a long-range part:

The short ranged part is a ﬁnite ranged and can be handled with neighbor tables, the long range part is usually
expanded in a Fourier series, at least in periodic boundary conditions and is bounded and slowly varying. We suggest
that it is possible to estimate the value of vl(r) by sampling either particles at random, or terms in its Fourier
expansion. The question that arises is how to compensate for the noise of the estimate in ∆V .

v(r) = vs(r) + vl(r).

2

(5)

(6)

In both of these examples one could simply ignore the eﬀect of ﬂuctuations in the estimate of ∆V (s). If the errors
are small then clearly the sampled distribution will be changed only a little. If the acceptance ratio as a function of
∆V (s) were a linear function there would be no bias, but because it is non-linear, ﬂuctuations will bias the asymptotic
distribution. In this paper we will make a conceptually simple generalization of Metropolis algorithm, by adjusting
the acceptance ratio formula so that the transition probabilities are unaﬀected by the ﬂuctuations in the estimate of
∆V (s). We end up with a completely rigorous formula in the sense that if one averages long enough, one will get
the exact distribution, even if the noise level is large. The only assumption is that the individual energy estimates
are independently sampled from a normal distribution whose mean value is ∆V (s). One complication is that the
estimates of the variance of ∆V (s) are also needed. We show how to treat that case as well.

Kennedy, Kuti and Bhanot15,16 introduced an algorithm with many of the same aims as the present work but for
computations in lattice gauge theory. We will describe their method and compare it to the new method later in the
paper.

II. DETAILED BALANCE WITH UNCERTAINTY.

From the two examples discussed above, let us suppose that when a move from s to s′ is made, an estimate of the
s′). (We take units with kBT = 1 hereafter.) By V (s) we
s′) be a modiﬁed acceptance probability; we assume that it depends only
s′)dδ be the probability for obtaining a value δ. Then the

diﬀerence in energy is available, which we denote δ(s
mean the true potential energy. Let a(s
on the estimate δ of the energy diﬀerence. Let P (δ; s
average acceptance ratio from s to s′:

→

→

→

′

s

) =

A(s

→

∞

Z

−∞

dδP (δ; s

′

s

)a(δ).

→

The detailed balance equation is:

Deﬁning:

−V (s)/kB T T (s

e

′

s

)A(s

′

s

) = e

′
−V (s

)/kB T T (s

′

′

s)A(s

→

s)

→

→

→

−
we can rewrite the detailed balance equation as:

→

∆(s

s

) = [V (s

)

V (s)]/kBT

′

′

′

ln[T (s

−

s)/T (s

→

′

s

)]

→

→
If the process to estimate δ is symmetric in s and s′ then P (δ; s′
requires:

→

A(s

′

s

) = e

−∆A(s

′

s).

s) = P (

δ; s

−

→

→

s′). Then detailed balance

∞

Z

−∞

dδP (δ; s

′

s

)[a(δ)

−∆a(

e

δ)] = 0.

→

−

−

In addition, we must have that 0

a(δ)

1 since a is a probability17.

≤

≤

The diﬃculty in using these formulas is that during the MC random walk, we do not know either P (δ; s

∆. Hence we must ﬁnd a function a(δ) which satisﬁes Eq. (11) for all P (δ) and ∆.

To make progress we assume a particular form for P (δ; s

s′). In many interesting cases, the noise of the energy
diﬀerence will be normally distributed. In fact the central limit theorem guarantees that the probability distribution
of δ will approach a normal distribution if the variance of the energy diﬀerence exists and one averages long enough.
= ∆, then the probability of getting a particular value of δ is:
If the noise is unbiased:

→

s′) or

→

δ
h

i

P (δ) = (2σ2π)

−1/2 exp(

(δ

−

−

∆)2/(2σ2)).

In this section only, we will assume that we know the value of σ, that only ∆ is unknown. We will discuss relaxing
this assumption in Sec. IV.

In the case of a normal distribution with known variance σ we have found a very simple exact solution to Eq. (11):

(7)

(8)

(9)

(10)

(11)

(12)

(13)

aP (δ; σ) = min(1, exp(

δ

σ2/2))

−

−

3

σ2/2) for
σ2/2. We refer to the quantity u = σ2/2 as the noise penalty. Clearly, the formula reverts to the usual

The uncertainty in the action just causes a reduction in the acceptance probability by an amount exp(
δ >
Metropolis formula when the noise vanishes.

−

−

To prove Eq. (13) satisﬁes Eq. (10), one does the integrals in Eq. (7) to obtain:

A(∆) =

−∆erfc(c(σ2/2

[e

∆)) + erfc(c(σ2/2 + ∆))]

−

1
2

(14)

where erfc(z) is the complimentary error function and c = 1/√2σ2.

Below we apply Eq.

(13) to several simple problems and ﬁnd that it indeed gives exact answers to statistical
precision. The remainder of the paper concerns considerations of eﬃciency, a comparison to other methods and the
more diﬃcult problem of estimating σ.

The chief motivation for studying the eﬀect of noise on a Markov process is for reasons of eﬃciency. If computer
time were not an issue, we could average enough to reduce the noise level to an insigniﬁcant level. In this section we
are concerned with the question of how to optimize the acceptance formula and the noise level.

III. OPTIMALITY

A. Acceptance ratio

We ﬁrst propose a measure of optimality of an acceptance formula and relate that to a linear programming problem.
It is clear that Eq.(11) can have multiple solutions; its solution set is convex. For example, if a(δ) is a solution then
so is λa(δ) for 0 < λ < 1. Even in the noise-less case, several acceptance formulas have been suggested in the
literature18,19. To choose between various solutions we now discuss the eﬃciency of the Markov process, namely the
computer time needed to calculate a property to a given accuracy. It is a diﬃcult problem20 to determine the eﬃciency
of a Markov chain but Peskun21 has shown that given two acceptance rules, a1(x) and a2(x), if a1(∆)
a2(∆) for
all ∆
= 0, then every property will be computed with a lower variance using rule 1 versus rule 2. Hence the most
eﬃcient simulation will have the maximum value of λ. Very roughly what Peskun has shown is that it is always better
to accept moves, other considerations being equal.

≥

We propose to call an optimal acceptance formula, one where the average probability of moving is as large as
0.) In our

possible. Let W (δ)dδ be the probability density of attempting a move with a change in action δ, ( W (δ)
deﬁnition an “optimal” formula will maximize:

≥

∞

Z

−∞

ζ =

dδW (δ) (a(δ)

aM (δ)) .

−

(15)

1 leads to the solution a(δ) = 1 if δ

0.

It is likely that the optimal functions are, to a large part, independent of W and so we set W (x) = 1. We subtracted
aM (x), the Metropolis formula, so the integral would be convergent. Note that for the solution for a normal distribution
aP (δ) we have: ζP =

σ2/2.

In the noise-less case one can easily show21 that the Metropolis formula is optimal. Without uncertainty, Eq.
δ). For each δ > 0, one needs to maximize: W (δ)a(δ) +

−

(10) only couples values with the same
δ). This and the constraint 0
W (

δ)a(

: a(δ) = e−δa(
δ
|
|
a(x)
≤

≤

−

−

−
We conjecture that the formula Eq. (13) is nearly optimal; one argument is based on an analysis of the large and
σ2. We
small δ limits: the other is numerical. First, consider moves which are deﬁnitively uphill or downhill δ2
expect downhill moves will always be accepted for an optimal function, so A(∆) = 1; this is its maximum value.
Then from Eq. (10) A(∆) = e−∆ for ∆
σ. Now we must invert Eq. (7). The unique continuous solution is
a(δ) = exp(
σ the solution is optimal in the class of continuous
functions22.

σ. Hence, in the region

σ2/2) for δ

| ≫

δ
|

≫

≫

≫

≤

−

−

δ

Another approach to ﬁnding the optimal solution to Eq. (11) is numerical. We wish to maximize Eq. (15) subject
to equality constraints and the inequality constraints that a(δ) be a probability. This is an inﬁnite dimensional
linear programming (LP) problem, a well-studied problem in optimization theory for which where exist methods to
determine the globally optimal solution. To ﬁnd such a solution, we represent a(δ) on a ﬁnite basis. We used a
y to y and assumed that outside the range a(δ) had the asymptotic form derived above.
uniform grid in the range
i Kijai + cj where cj
In this discrete basis A(∆) is a linear transformation of the acceptance probability: Aj =

−

P

4

6
represents the contribution coming from
inequalities: 0

δ
|
1 and the equalities:

a

|

≤

≤

> y. The problem is to ﬁnd a solution maximizing

i ai subject to the

P

(16)

[Ki,j −

Xi

−xj Ki,−j]ai = e

e

−xj c−j −

cj.

Fig. 1 shows the LP solution, for σ = 1 compared with aP (δ). Note that it is not a continuous function, but
for the most part consists of regions with ai = 1 alternating with regions with ai = 0. The LP solution is a very
accurate solution to the problem posed, with errors of less than 10−5. The discontinuous nature of the LP solution
is to be expected since the solution must lie on the vertices of the feasible region, determined by the equalities and
inequalities. To obtain the solution to this diﬃcult ill-conditioned problem, we discretized the values of δ on a grid
with spacing 0.01. However we only demanded that Eq. (10) be satisﬁed on a grid ∆ with a spacing of 0.2. This
implies that there were 40 times as many degrees of freedom as equality constraints and thus most variables were free
to reach the extreme values of 0 and 1.

−

The optimal LP function has a slightly larger value of ζ, roughly about ζLP ≈ −
0.5σ2. As far as we can determine, the LP solutions survive in the limit dr
→

0.45σ2 versus the value for ap of
0 and are slightly more optimal
ζP =
than aP . However, given the inconvenience of determining and programming the LP solutions, and the very limited
improvement in ζ, we see little reason23 to prefer such solutions. When we added a factor to penalize discontinuities
ai−1)2 (this makes it a quadratic programming problem)
in a(δ) to the objective function proportional to
then the solution converged to aP (δ).

i(ai −

P

B. Noise level

Now let us consider how to optimize the noise level σ. An energy diﬀerence with a large noise level can be computed
quickly, but because of the penalty in Eq. (13) it has a low acceptance ratio, reducing the overall eﬃciency of the
simulation. We should pick σ to minimize the variance of some property with the total computer time ﬁxed. The
computer time can be written as T = m(nt + t0) where t is the time for an elementary evaluation of a given energy
diﬀerence, n is the number of evaluations of δ before an acceptance is tried, m is the total number of steps of the
random walk and t0 is the CPU time in the noise-less part of the code. But the error in any property converges as
ǫ = c(σ)m−1/2 where c is some function of σ and the noise level converges as σ = dn−1/2. Eliminating the variables
m and n, we write the MC ineﬃciency:

−1 = T ǫ2 = t0c(σ)2

ζ

−2 + 1

.

f σ
(cid:2)

(cid:3)

Here f = d2t/t0, the relative noise parameter, is the CPU time needed to reduce the variance of the energy diﬀerence
relative to the CPU time used in the noise-less part of the code: for f
1 computation
of the noisy energy diﬀerence dominates the computer time.

1 noise is unimportant, for f

≪

≫

To demonstrate how important this optimization step is, we consider a one dimensional double well with a potential

given by:

(17)

(18)

kBT V (s) = a1s2 + a2s4.

−

0.288 and a2 = 0.009. We used a uniform transition probability (T (s

4 and the height of the central peak is π(0)/π(4) = 0.1, which
We picked parameters such the two minima are at s =
s′)) with a maximum
corresponds to a1 =
move step of 0.5. This means overcoming the barrier requires multiple steps, typical of an application which has a
probability density with several competing minima. To measure the eﬃciency, we computed the error on the average
value of moments s on Markov chains with 107 steps and values of noise in the range 0
6. We also calculated
the density and compared to the exact values obtained by deterministic integration. Shown in Fig. 2 is the acceptance
σ2/8]) is the
ratio versus σ. We see that it decreases to zero rapidly at large noise levels. The dotted line (
asymptotic form for large σ.

exp[

→

≤

±

≤

∝

−

σ

Fig. 3 shows an example of the density obtained when the noise in the energy was σ = 1. It is seen that ignoring
the noise leads to a much smoother density than the exact result. Using the acceptance formula aP (δ) we recover the
exact result within statistical errors.

Figure 4 shows the ineﬃciency (relative to its value when the noise is switched oﬀ) versus σ and f . In general, as
the diﬃculty of reducing the noise (as measured by f) increases, the calculation becomes less eﬃcient, and the optimal
value of σ increases. The two panels show the eﬃciency of computing
; the behavior of the error is quite
i
diﬀerent for even and odd moments of s because the error in the ﬁrst moment is sensitive to the rate at which the

s2
h

and

s
h

i

5

walk passes over the barrier, while the second is not. The ﬂat behavior at large noise level of the ﬁrst moment occurs
because the noise actually helps passage over the barrier: for f > 3 a ﬁnite optimal value of σ ceases to exist.

On this example, we ﬁnd that c(σ)

exp(ασ2) with α

0.09 for even moments and α

0.025 for odd moments.

With this assumption the optimal value of the noise level equals:

∝

≈

≈

∗2 = (f /2)[

σ

1 + 2/(f α)

1].

p

−

Although this formula is approximate (because of the assumption on c(σ)) it does give reasonable values for the
optimal σ.

As this example demonstrates, it is much more eﬃcient to perform a simulation at large noise levels. One can
quickly try very many moves even if most of them get rejected instead of just a few ones where the energy diﬀerence
has been accurately computed. However, there are practical problems with using large σ as will be discussed next.

IV. UNCERTAIN ENERGY AND VARIANCE

Unfortunately there is a serious complication: the variance needed in the noise penalty is also unknown. Both
the change in energy and its variance need to be estimated from the data. The variance in general will depend on
s′); we cannot assume it is independent of the conﬁguration of the walk. Precise
the particular transition: (s
estimates of variance of the energy diﬀerence are even more diﬃcult to obtain than of energy diﬀerence itself since
the error is the second moment of the noise and will ﬂuctuate more. In Fig.(3) is shown the eﬀect on the double well
example of using an estimate of the variance in the penalty formula instead of the true variance. The systematic error
arises because the acceptance rate formula is a non-linear function of the variance. We will see that we must add an
additional penalty for estimating the variance from the data.

→

Let us suppose we generate n estimates of the change in action:

where each yk is assumed to be an

y1, . . . , yn}

{

independent normal variate with mean and variance:

Unbiased estimates of ∆ and σ2 are:

By construction
= ∆ and
energy diﬀerence comes from a diﬀerent processor.

δ
h

i

i

χ2
h

= σ2. This can be done on a parallel computer if an independent estimate of the

The joint probability distribution function of δ and χ2 is the product of a normal distribution for the mean and a

chi-squared distribution for the variance:

where P (δ

∆, σ)is given in Eq.(12) and

−

−

with µ = (n

1)/2 and

P (δ, χ2; ∆, σ) = P (δ

∆, σ)Pn−1(χ2; σ)

−

Pn−1(χ2; σ) = cnχn−3e

2
−µχ

/σ

2

The generalization from the previous section is straightforward. The acceptance probability can only depend on

the estimators δ and χ2. The average acceptance probability is:

= ∆

yii
h

(yi −
h

∆)2

= nσ2.

i

δ =

n
i=1 yi
n

P

χ2 =

n

i=1(yi −
1)
n(n
−

δ)2

.

P

cn =

(µ/σ2)µ
Γ(µ)

.

6

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

Detailed balance requires:

A(∆, σ) =

dχ2P (δ, χ2; ∆, σ)a(δ, χ2).

∞

∞

dδ

Z

−∞

Z

0

A(∆, σ) = exp(

∆)A(

∆, σ)

−

−

for all values of ∆ and σ
≥
dimensional homogeneous integral equation for a(δ, χ2).

0. We have two parameters to estimate and average over instead of one and a two

In the limit of enough independent evaluations we recover the one parameter equation since limn→∞ Pn−1(χ2) =

δ(χ2

σ2) and the equations for diﬀerent σ’s decouple.

−

Asymptotic Solution

We can do the same type of analysis at large

∆
|
|
χ2. Assume there exists a solution with A(∆, σ) = 1 for ∆
σ. Assume this solution can be expanded in a power series in χ2, a(δ, χ2) =

as we did when σ was known. A move is deﬁnitely uphill or
∆)
k=0 bkχ2ke−δ. Explicitly

σ. Then A(∆, σ) = exp(

≪ −

downhill if δ2
for ∆
performing the integrals we obtain:

≫

≫

−

∞

P

Matching terms in powers of σ2 we obtain bk. The expansion can be summed to obtain a Bessel function:

exp(

σ2/2) =

cnbkΓ(µ + k)(σ2/µ)µ+k.

−

Xk

a(δ, χ2) = Γ(µ)e

−δ

(µ−1)/2

2
µχ2 (cid:21)

(cid:20)

Jµ−1(χ

2µ).

p

This function is positive for χ2 < n/4. For larger values of χ2 either the assumption of A(∆, σ) = 1 is wrong or no
smooth solution exists.

Taking the logarithm of the power series expansion, we obtain a convenient asymptotic form for the penalty in

powers of η = χ2/n:

The “Bessel” acceptance formula is:

uB =

+

χ2
2

χ4
4(n + 1)

+

χ6
3(n + 1)(n + 3)

+ . . .

aB(δ, χ2, n) = min(1, exp(

δ

uB))

−

−

.

The ﬁrst term χ2/2, is the penalty in the case where we know the variance. The error in the error causes an
additional penalty equal, in lowest order, to χ4/(4n). This asymptotic form should only be used for small values of η
since the expansion is not convergent for η
1/4. In Fig. 5 we show errors in the detailed balance ratio as a function
of ∆ and σ for n = 128. It is seen that the errors are small but rapidly increasing as a function of σ. We ﬁnd that the
maximum relative error in the detailed balance ratio approximately equal to 0.15η2. Good MC work will have the
error less than 10−3 requiring η < 0.1 Very accurate MC work with errors of less that 10−4 requires a ratio η < 0.02.
This is a limitation on the noise level.

≥

Figure 4 shows the eﬀect on the eﬃciency of the additional noise penalty. While the eﬀect on the even moments
is small, the eﬃciency of the ﬁrst moment dramatically increases for noise levels σ > 2, perhaps because rejections
for large dispersions of the energy diﬀerences cause diﬃculty in crossing the barrier. The eﬃciency becomes more
sensitive to σ.

We have not found an exact solution for Eq. (28). From numerical searches it is clear that much more accurate
solutions exist than the asymptotic form. We have found such piecewise exponential forms. But the Bessel formula
is a practical way of achieving detailed balance if one can generate enough independent normally distributed data.

7

V. DEVIATIONS FROM A NORMAL DISTRIBUTION

We have assumed that δ is normally distributed. In the case the noise is independent of position but otherwise

completely general, we can perform the asymptotic analysis. Let us assume that:

A(∆) =

dδP (δ

∆)a(δ)

Z

−

a(δ) = exp(

δ

u).

−

−

and that A(∆) = 1 for suﬃciently negative values of ∆. Then for large values of ∆ the unique continuous solution is:

The penalty u has an expansion in terms of the cumulants of P (δ):

∞

u =

Xn=2,4,...

κn/n! =

ln(

−

Z

−∞

∞

dxP (x)e

−x).

The odd cumulants vanish because P (x) = P (
x). For the normal distribution this reduces to Eq. (13) and the
penalty form is exact. The contribution of higher order cumulants could be either positive or negative leading to
positive or negative penalties.

−

Eq. (35) illustrates a limitation of the penalty method: one can not allow the energy diﬀerence to have a long
tail of large values. It is important that the energy diﬀerence be bounded because a penalty can be deﬁned only if
limx→∞ exP (x) = 0 so the integral will exist. Suppose the energy diﬀerence in Eq. (5) is a sum of an inverse power
and that r is sampled uniformly. Then we ﬁnd (in 3 dimensions)
of the distances to the other particles ∆ =
δ−3/m. For any positive value of m the higher order cumulants and the penalty will not exist
that at large δ: P (δ)
even though the mean and variance of δ exist under the weaker condition: m < 3/2. We must arrange things so
that large deviations of the energy diﬀerence from the exact value are non-existent or exponentially rare, perhaps by
bounding the energy error.

−m
j r
j

P

∝

VI. COMPARISON WITH OTHER METHODS

A. Method of Kennedy, Kuti, and Bhanot

Kennedy, Kuti and Bhanot15,16 (KKB) have introduced a noisy MC algorithm for lattice gauge theory. We adapted
that method for the present application by using energy diﬀerences with respect to an approximate potential, w(s),
that can be determined quickly and exactly. Proposed moves are “pre-rejected” using w(s) and then the more
expensive computation of an estimate of v(s) is done. Let us suppose that the deviation between these potentials can
be bounded: max
ǫ for some ǫ. We determine an unbiased estimate of the ratio q needed in Eq. (4)
δw(s)
|
by using the power series expansion:

δv(s)

| ≤

−

′

s

) = e

−δ =

δ)n/n!

q(s

→

∞

(
−
Xn=0

a = (1 + q)/(2 + ǫ).

s′) = v(s′)

w(s′)

where δ(s
v(s) + w(s). With a predeﬁned probability we sample terms in the power series
up to order n and obtain an estimate of q; this is a variant of the von Neumann-Ulam method We ﬁnally accept the
move with probability

→

−

−

For an appropriate choice of parameters, a is in the range 0
given by the following pseudo-code:

a

≤

≤

1 most of the time. The revised KKB method is

(33)

(34)

(35)

(36)

(37)

Sample s′ from T (s
If (exp [

→
w(s′) + w(s)] < prn) then

s′)

−
reject move

else

q0 = t0 = 1

8

do n = 1,

∞
pn = min(γ/n, 1)
if (pn < prn) exit loop
sample xn =
tn = tn−1xn/(npn)
qn = qn−1 + tn

−

end do

v(s′) + w(s′) + v(s)

w(s)

−

if [(1 + qn)/(2 + ǫ) > prn] then accept move

In this procedure γ > 0 is a parameter which controls the number of terms sampled. For γ

1 the average
number of evaluations of x per step is ne(eγ
1), where ne is the acceptance ratio of the preliminary rejection step.
Each sample of x must be uncorrelated with previous samples. As ǫ
0, one recovers the Metropolis algorithm.
The sampling distribution, γ, and ǫ have to be ﬁxed to ensure that a is in the interval [0, 1] almost all of the time.
Violations for which a < 0 put a limit on the size of the noise and the size of the sampling step, while ǫ can be
made arbitrarily large to remove violations where a > 1. This will, however, aﬀect the eﬃciency. A recent preprint24
proposed to solve the problem of violating the constraints on the acceptance probabilities by introducing negative
signs into the estimators. We have not explored this possibility.

→

≤

−

We made a comparison to the penalty method with the double well potential, using w(s) = a2s4 as the approximate
potential. (It conﬁnes the random walk but does not have the central barrier.) For a violation level of 10−4, the
maximum noise was σ = 0.4. This is a much smaller noise level than is optimal in the penalty method. For this
noise, a transition step of 0.45 was optimal. To optimize γ and ǫ, we ﬁrst adjusted γ until the half the desired number
violations occurred for a < 0. Then we adjusted ǫ until the total number of violations equaled 10−4. The errors in
the ﬁrst and second moments are given in Table I, along with the parameters used in the KKB algorithm. We ﬁnd
that the KKB method is 2.3 times slower for the ﬁrst moment and 3.5 times slower for the second moment than the
penalty method (run at the same noise level, with the same transition step size and computing the variance with
n = 32 points). This comparison was done assuming f is suﬃciently small that we do not have to take into account
the multiple evaluations of the energy diﬀerences. Taking that into account would raise the ineﬃciency of the KKB
method by another factor of 2.74, the average number of function evaluations.

We also tested the KKB method with w(s) = v(s) (i.e., the argument of the exponential was only noise). The
data for this case is also given in Table I. The maximum value of allowable noise was still σ = 0.4. For σ < 0.2,
the average number of function evaluations was less than one, making the method more eﬃcient than the penalty
method, for a ﬁxed noise level. For the ﬁrst moment, KKB was 3.4 times more eﬃcient for σ = 0.1 and 1.3 times
more eﬃcient for σ = 0.2. However, if we consider optimizing σ as in Sec. III B, the KKB method is less eﬃcient
than the penalty method. To be eﬃcient at large values of f , larger values of σ must be used, and there the KKB
method is less eﬃcient. At small values of f , the last term in Eq. (17) dominates, and the lesser number of function
evaluations yields no advantage for the KKB method.

The KKB method requires taking enough samples to lower the noise to an acceptable value.

In contrast, the
penalty method requires taking enough samples to ensure the distribution is normal. Also, for this problem, the
penalty method could have an even higher eﬃciency because it could use larger sampling steps sizes (the maximum
KKB sampling step size depends on the quality of the approximate function, w). The advantage of the KKB method
is that it makes no assumptions about the normalcy of the noise; the disadvantage is that one cannot guarantee that
a is in the range [0, 1]. Knowledge that the noise is normally distributed allows one to use a much more eﬃcient
method.

Another alternative noisy MC method is to combine the stochastic evaluation of an exponential with the reweighting
. Then an exact average can be

method. One can perform a simulation with w(s), generating a random walk
generated by reweighting:

si}

{

(38)

where Qi exp(
w(si))/kBT ). As discussed above an estimate of Qi can be generated with the von Neumann-
Ulam procedure by stochastically summing the power series expansion of the exponential. In this case we do not care
whether the exponential is between 0 and 1, only its variance is important. The diﬃculty is that the exponent of the

(v(si)

−

−

B. Reweighting

=

O
h

i

P

i O(si)Qi
i Qi

P

9

weight increases linearly with the size of the system, i.e.
will increase
exponentially with the size of the system. This method is only appropriate for small systems, but no assumptions are
w(s). The advantage of including the noise in the random walk rather than
made about the distribution of v(s)
reweighting the visited states is that one works with energy diﬀerences only and it is possible to make the ﬂuctuations
of diﬀerences independent of the size of the system.

N . Hence the variance of

(v(s)
h

O
h

i ∝

−

−

i

w(s))2

VII. CONCLUSION

We have shown a small modiﬁcation of the usual random walk method by applying a penalty to the energy diﬀerence
can compensate for noise in the computation of the energy diﬀerence. If the noise is normally distributed with a known
variance, the compensation is exact. If one estimates the variance from n data points, we show that it suﬃces to have
χ2
0.1n and apply an additional penalty. On a double well potential we found that the the optimal noise level is
typically kBT

3kBT.

≤

σ

The penalty method utilizes the power of Monte Carlo: one can choose the transition rules to obey detailed
balance and to optimize convergence and use only well-controlled approximations. We can generalize to other noise
distributions by using numerical solutions to the detailed balance equations as we have shown. We have adapted a
method introduced by Kennedy et al.15 but found it to be much slower once the noise level becomes high.

≤

≤

We now plan to apply the algorithm to a serious application. As we have shown, very large gains in eﬃciency
are sometimes possible. However, the problem remains of ensuring that the estimates of the energy diﬀerences are
statistically independent and normally distributed.

Codes used in calculations reported here are available at: [http://www.ncsa.uiuc.edu/Apps/CMP/index.html ]

This work was initiated at the Aspen Center for Physics and has been supported by the NSF grant DMR 98-02373
and computational facilities at NCSA. DMC acknowledges useful discussions with J. Kuti, A. Kennedy and A. Mira.

ACKNOWLEDGMENTS

1 N. Metropolis, A. W. Rosenbluth, M. N . Rosenbluth, A. H. Teller, E. Teller, J. Chem. Phys. 21, 1087 (1953).
2 D. Ferguson, J. Siepmann, and D. Truhlar, Adv. Chem. Phys. 105 (1998).
3 D. M. Ceperley, Rev. Mod. Phys. 67, 279 (1995).
4 D. M. Ceperley, G. V. Chester and M. H. Kalos, Phys. Rev. B 16, 3081 (1977).
5 J. M. Hammersley and D. C Handscomb, Monte Carlo Methods (Chapman and Hall, London 1964).
6 P. A. Whitlock and M. H. Kalos Monte Carlo Methods (Wiley, 1986).
7 R. Car and M. Parrinello, Phys. Rev. Lett. 55, 2471 (1985).
8 R. O. Weht, J. Kohanoﬀ, D. A. Estrin, C. Chakravarty, J Chem. Phys. 108, 8848 (1998).
9 J. C. Grossman, L. Mitas, and K. Raghavachari, Phys. Rev. Lett. 75, 3870 (1995).
10 The nuclei do not have to be classical: one can treat quantum eﬀects with imaginary time path integrals. For simplicity, we

do not discuss that case.

11 J. D. Doll and D. L. Freeman, Chem. Phys. Lett. 227 436 (1994).
12 M. P. Allen and D. J. Tildesley, Computer Simulation of Liquids (Oxford University Press, Oxford, 1987).
13 V. Natoli and D. M. Ceperley, J. Comp. Phys. 117, 171 (1995).
14 L. Greengard and V. Rokhlin, J. Comput. Phys. 60, 187 (1987).
15 A. D. Kennedy and J. Kuti, Phys. Rev. Lett. 54, 2473 (1985).
16 G. Bhanot and A. D. Kennedy, Phys. Lett. B 157, 70 (1985).
17 If the sampling probability were also to be only estimated, in particular if the logarithm of T was normally distributed, that

would simply add to σ and the discussion carries through.

18 Wood., W. W. and J. D. Jacobson, Proceedings of the Western Joint Computer Conference, 261 (1959).
19 A. A. Barker, Austral. J. Phys. 18, 119 (1965).
20 A. Mira, PhD thesis, University of Minnesota, (1998). [http://www.stat.umn.edu/∼mira]
21 P. H. Peskun, Biometrika, 60, 607 (1973).
22 However simply requiring that A(∆) = 1 for ∆ ≪ −σ does not uniquely specify a(δ). Numerically we have found other

solutions with these boundary conditions.

10

23 A possible reason would be that one does not have to compute an exponential or random number with such an acceptance

formula.

24 L. Lin, K. F. Liu and J. Sloan, Lattice 98 Conference Proceedings, 1998 (xxx.lanl.gov hep-lat/9809168).

FIG. 1. The optimal acceptance formula computed using the linear programming method (solid line) and using the penalty

method (dotted line). Both are for σ = 1. The accuracy of the LP solution is better than 1 part in 10

−5.

11

0.2
0.2

0.15
0.15

0.1
0.1

0.05
0.05

0
0

-5
-5

0
0

s
s

5
5

FIG. 2. The logarithm of the acceptance ratio as a function of σ2 for the double well potential. The dashed line is proportional

to exp(−σ2/2) and the dotted line to exp(−σ2/8)

12

FIG. 3. The density as computed using the the Metropolis formula (solid line), the direct penalty, dashed line, and with

Bessel penalty (solid line). In all cases the noise level was σ = 1.

13

FIG. 4. The relative in-eﬃciency of penalty MC as a function of σ and the noise level, f. From bottom to top the values
of f are 0.5, 1, 2, 4. The solid lines are assuming the noise is known, the dashed lines are using the Bessel formula with 64
independent evaluations. Fig. (a) is the ﬁrst moment, (b) is the second moment.

(a)

14

(b)

15

0.0004

0.0003

0.0002

0.0001

0

0

2

4

6

FIG. 5. The log of the detailed balance ratio versus ∆ for m=128. From top to bottom the curves are with a noise level of

σ = 2, 1.8, 1.6, 1.4, 1.2, 1.0.

16

TABLE I. Computed MC ineﬃciencies for the modiﬁed KKB method and the penalty method. n is the average number of

function evaluations per step. The Bessel penalty method uses 32 data points and a transition step of 0.45.

c(σ)

KKB parameters

σ

0.0
0.1
0.2
0.3
0.4

0.1
0.2
0.3
0.4

0.0
0.1
0.2
0.3
0.4

x

273
270
275
282
270

212
209
221
215

175
174
184
183
178

γ

KKB, w(s) = a2s4
1.07
1.1
1.2
1.2
2.1

KKB, w(s) = a1s2 + a2s4

0.20
0.59
0.75
1.35

Penalty

x2

154
152
151
155
145

92
89
98
98

74
78
76
76
78

ǫ

6.0
6.0
6.0
6.0
4.9

2.0
1.8
2.3
2.1

n

1.49
1.52
1.61
1.82
2.74

0.20
0.59
1.02
1.93

17


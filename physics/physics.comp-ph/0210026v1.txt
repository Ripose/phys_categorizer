2
0
0
2
 
t
c
O
 
7
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
6
2
0
0
1
2
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A method for solving systems of non-linear diﬀerential equations
with moving singularities

S. S. Gousheh1∗, H. R. Sepangi1,2 and K. Ghafoori-Tabrizi1
1Department of physics, Shahid Beheshti University, Evin, Tehran 19839, Iran
2 Institute for Studies in Theoretical Physics and Mathematics, P.O. Box 19395-5746, Tehran, Iran

July 25, 2013

Abstract

We present a method for solving a class of initial valued, coupled, non-linear diﬀerential equa-
tions with ‘moving singularities’ subject to some subsidiary conditions. We show that this type
of singularities can be adequately treated by establishing certain ‘moving’ jump conditions across
them. We show how a ﬁrst integral of the diﬀerential equations, if available, can also be used for
checking the accuracy of the numerical solution.

PACS: 02.60.Cb, 02.60.Lj
Keywords: Diﬀerential equations, Moving singularities

1

Introduction

When solving a physical problem, one usually encounters a set of coupled nonlinear diﬀerential
equations, called dynamical equations or equations of motion, describing the dynamics of the system.
This set of diﬀerential equations usually emanate from a general physical principle, and might include
some subsidiary equations which can be categorized in two distinct classes, constraints or integrals
of motion. The degrees of these subsidiary equations are usually at least one order lower than the
dynamical equations. At the practical level these two classes of subsidiary conditions are treated very
diﬀerently. The ﬁrst class has to be solved simultaneously with the rest of the dynamical equations.
However, the integrals of motion need only be used to put some constraints on the initial conditions.
Moreover they can be used as a consistency check on the solutions.

For physically interesting cases, usually time evolution problems appear as initial valued ones,
and static problems as boundary valued ones. While solving these problems, one often encounters
various kinds of singularities. These singularities are usually indications of some profound physical
laws or processes with signiﬁcant implications. The most common type of singularities are those in

∗Email address: ss-gousheh@cc.sbu.ac.ir

1

which the coeﬃcient functions of the diﬀerential equations have singularities at some ﬁxed points,
for example δ-functions. These can be called the ﬁxed type of singularities. In a previous work [1]
we discussed several general methods to make an eﬃcient numerical algorithm for boundary valued
problems of this type.

In this paper we present a method for handling a class of initial valued, coupled, non-linear
diﬀerential equations, whose solutions contain moving singularities. These singularities have the
property that their positions and severities are apriori unknown and depend on the solutions yet to
be obtained. Singularities of this type have been noted in such diverse ﬁelds as celestial mechanics, in
particular the classical Kepler problem [2], and in the study of tensor ﬁelds deﬁned on moving surfaces
[3]. We show that this type of singularity can be adequately treated numerically by establishing
certain moving jump conditions across them. We establish the accuracy of our numerical solutions
by showing that the equation representing the integral of motion is satisﬁed at all values of the
independent variable including at the positions of the moving singularities.

For the integration algorithm we use the basic fourth-order Runge-Kutta method. It is worth
mentioning that more accurate integration algorithms exist. For example, there are exponential and
Bessel ﬁtted variable step method of order 6 due to Raptis and Cash [4], and also a variable step P-
stable method of order 6 and 8 with a phase lag of the same order due to Simos [5]. For the embedded
Runge-Kutta, formulae of order 6(5) and 8(7) have been developed by Prince and Dormand [6] and
formulae of order 8(6) and 12(10) have been developed by Dormand et. al.
[7]. These integration
algorithms should be more eﬃcient for higher accuracy. Our choice of the integration algorithm is
based on the following reasons. First, our main objective has been to ﬁnd a solution to the problem
of moving singularities and not the eﬃciency of the integration algorithm itself. Second, it turns out
that in this problem the cumulative error of even the fourth order Runge-Kutta integration algorithm
is negligible compared to the error introduced to the solutions at each jump across the singularities.
The set of equations that we discuss results from a classical model of gravitation in Robertson-
Walker cosmology in which the signature of the metric undergoes a transition from a Euclidean to a
Lorentzian domain. In section 2 we brieﬂy discuss the physical origin of the problem and show its
reduction to a set of ordinary diﬀerential equations. There, one sees an example in which this set
automatically includes a subsidiary equation which is an integral of motion, along with the dynamical
equations. It is worth mentioning that if an integral of motion is not directly included in the set of
equations of motion, it can some times be derived directly from the dynamical equations. Also we
employ a reparameterization transformation which allows one to seek continuous solutions across the
hypersurface of signature change. Moreover, we employ a set of transformations which reduces the
degree of severity of the moving singularities. The reader who is interested only in the numerical
methods can skip to section 3, without loss of continuity.

2 Derivation of dynamical equations

Traditionally, one of the features of classical gravity is that the signature of the metric is usually
considered as ﬁxed. If one relaxes this condition, one may ﬁnd solutions to the ﬁeld equations which
exhibit a signature transition [8, 9]. In the model that we study here a real scalar ﬁeld is taken as the
matter source interacting with gravity and itself in a Robertson-Walker geometry whose signature
evolution is controlled by a preferred coordinate. In this model, we seek solutions to the dynamical
equations which are smooth and continuous across the hypersurface of signature transition, where the
metric is degenerate. The alternative would have been to ﬁnd solutions by solving Einstein’s equations
in disjoint regions next to the hypersurface, and then ﬁnding jump conditions to match them [10]. For

2

the spatially ﬂat universes, the ﬁrst approach yields exactly solvable Einstein’s equations [8]. Here,
we discuss the general case which includes the spatially ﬂat as well as non-ﬂat cases and solve the
resulting dynamical equations numerically. For more details of the physical basis and signiﬁcance of
the problem, we refer the interested reader to the reference [9].

Consider gravity coupling to a scalar ﬁeld through Einstein’s equation,

where the scalar ﬁeld φ is a solution of the Klein-Gordon equation,

Gµν = κTµν [φ],

∆φ

∂U
∂φ

−

= 0.

Here, Gµν is the Einstein tensor constructed from torsion-free connections compatible with the metric,
and U (φ) is the scalar potential for the real scalar ﬁeld φ, which interacts with itself and gravity
through the stress-energy tensor T [φ].

The above coupled equations are to be solved in a domain that would lead to Robertson-Walker
cosmologies with Lorentzian signature. However, if the metric is suitably parametrized, one expects
to see continuous transition to a Euclidean domain. As in [8], we adopt a chart with coordinate
functions
where the hypersurface of signature change would be located at β = 0. The
}
metric can be parametrized to take the form

β, x1, x2, x3
{

g =

βdβ

dβ +

−

⊗

R2(β)
[1 + (k/4)r2]2

dxi

dxi,

⊗

Xi

where r2 =
i xixi. We seek solutions of the form R = R(β) and φ = φ(β). Now, it is apparent that
the sign of β determines the geometry, being Lorentzian if β > 0 and Euclidean if β < 0. For β > 0,
the traditional cosmic time can be recovered by the substitution t = (2/3)β3/2. Adopting the chart
t, xi
{

and using equations (1) through (3) with units in which κ = 1, one ﬁnds

P

}

3

−
¨R
R

2

+

¨φ + 3

3

˙R2
R2 −
˙R2
R2 +
˙R
˙φ +
R

k
R2 +
k
R2 =
∂U
∂φ

= 0,

˙φ2
2

˙φ2
2

−

+ U (φ) = 0,

+ U (φ),

(1)

(2)

(3)

(4)

(5)

(6)

. Now a
where a dot represents diﬀerentiation with respect to t and
solution to the problem is furnished by ﬁnding R(t) and φ(t), for a given U (φ). Note that these
equations are not all independent. For example equation (5) can be obtained by combining equations
(4) and (6). Upon a closer inspection we recognize that this is due to the fact that equation (4)
is not a dynamical equation, rather it is actually an integral of motion representing a zero energy
condition. That is, any solution of the dynamical equations (5) and (6) would yield a constant total
energy (equation (4)). However, Einstein’s equations demand zero energy solutions only.

< φ <

R <

−∞

, 0

∞

∞

≤

As is apparent from the dynamical equations, we have moving singularities at all times for which
R = 0. These moving singularities are potentially very severe and, as we shall see later, φ actually
diverges there. We can get an indication on the divergence of φ from equations (4,5). These equations
indicate that U (φ) has to cancel the divergence of the k/R2 terms, and for all physically relevant
potentials, this implies that φ has to diverge. We therefore need to use a set of transformations to

3

reduce the severity of the divergence of solutions. We expect the following transformations to render
the solutions more manageable, since it is formed of products of factors which go to zero and inﬁnity
at about the same strength,

X = R3/2 cosh(αφ),
Y = R3/2 sinh(αφ),

(7)

(8)

(9)

(10)

(11)

where α2 = 3
8 .

The above equations are considerably simpliﬁed if we take the potential to be

2α2(X 2

Y 2)U (φ(X, Y )) = a1X 2 + a2Y 2 + 2bXY,

−

where a1, a2 and b are adjustable parameters. This choice for the potential stems from the fact
that the left hand side of equation (9) directly appears in the Lagrangian, from which the dynamical
eqations can be derived. The features of this potential and the physics involved in the choice of its
parameters have been discussed in [8, 9].

The dynamical equations (5) and (6) in terms of X and Y and the evolution variable β now

become,

′′

Y

=

′′

X

=

(cid:18)

1
2
1
2 (cid:18)

(cid:19)

1
β
1
β (cid:19)

′

Y

′

X

3
4
3
4

−

−

βkY (X 2

−2/3

Y 2)

β(a2Y + bX),

−

βkX(X 2

Y 2)

−2/3 + β(a1X + bY ),

−

−

subject to the subsidary ‘zero energy condition’, equation (4), which can be written in terms of the
new variables as

1
β (cid:19)

(cid:18)

X

(
−

′ 2

+ Y

′ 2

)

9
4

−

−

k(X 2

Y 2)1/3 + (a1X 2 + a2Y 2 + 2bXY ) = 0.

(12)

Here, a prime represents diﬀerentiation with respect to β. Note that this equation does not contain
any singularity, and equations (10) and (11) are actually less singular than equations (5) and (6).
The coupled equations (10) and (11) must now be solved and, as explained before, equation (12) is
merely a restriction on the initial conditions. However, it can also be used as a consistency check on
the analytical or numerical solutions. These equations do not seem to have a closed form solution so
a numerical treatment is necessary.

3 The numerical method

The dynamical equations that we have to solve are equations (10, 11). As mentioned in the last
section, equation (12) is an integral of motion. That is, any true solution to equations (10, 11)
automatically satisﬁes equation (12) for all values of the independent parameter, if it satisﬁes it at
any one point. Therefore, we can use equation (12) to put a restriction on the initial conditions.
More importantly, one can check the accuracy of the solutions by seeing how well equation (12) is
satisﬁed, as the algorithm integrates the dynamical equations.

As a ﬁrst step towards a numerical solution to the equations, we should study the restrictions
imposed by the set of diﬀerential equations on the initial conditions. These restrictions are the result
of the requirement of consistency of the initial conditions with the dynamical equations. However we
can accomplish a more complete task by ﬁnding the general form of the analytic solutions close to

4

the initial point. These solutions certainly include the complete information on the allowed set of the
initial conditions1. Moreover, the knowledge on the analytic solutions help with the ﬁrst few steps of
the integration algorithm.

3.1 Analytic solutions close to the initial point

In order to ﬁnd analytic solutions which are valid near the initial point (β = 0), we ﬁrst study the
restrictions imposed by equations (10)–(12) on the initial conditions. This is done by noting that in
order to have well behaved solutions close to β = 0, the ﬁrst term of equation (12) shows that we must
either have X ′(β)
Y ′(0)
βny , where nx, ny ≥
. However, the
∼
∼
|
|
ﬁrst terms on the right hand sides of equations (10) and (11) impose a more severe restriction. These
two equations admit solutions X ′(β)
β1/2 and Y ′(β)
β1/2 close to β = 0, however, this class of
solutions does not admit real or C 2 solutions across β = 0. One can show that regular solutions close
to β = 0 are of the form

βnx and Y ′(β)

X ′(0)
|
|

1/2, or

∼

∼

=

X(β) = Axβ3 + X0,

where Ax =

Y (β) = Ayβ3 + Y0,

where Ay =

kX0
0 )2/3 + a1X0 + bY0
Y 2

,

#

(X 2

3
4

2
9 "−
2
3
4
9 "−

0 −
kY0
Y 2
0 )2/3 −

(X 2

0 −

a2Y0

bX0

,

−

#

X(0), etc. Therefore, the initial conditions on the ﬁrst and second derivatives must satisfy

with X0
the relations

≡

′

′

X

(0) = Y

(0) = 0 and X

(0) = Y

(0) = 0.

′′

′′

Strictly speaking the conditions on the second derivatives are not initial conditions but rather
consistency checks, since we have coupled second order equations. Therefore, the initial values for
the functions X and Y must now satisfy, c.f. equation (12),

9
4

−

k(X 2

0 −

0 )1/3 + (a1X 2
Y 2

0 + a2Y 2

0 + 2bX0Y0) = 0.

The contour plots of equation (16) for k =
1 are given in ﬁgure 1. Along the contours, one ﬁnds the
possible initial values for X and Y . Although equation (16) is equivalent to a sixth order algebraic
equation which cannot be directly solved analytically, we can solve it by going back to the original
variables R and φ. The solutions are either R(0) = 0 giving φ(0) =
, which we exclude because
we have been seeking continuous solutions across β = 0, or R(0)

= 0 (it is a free parameter) with

±∞

±

(13)

(14)

(15)

(16)

(17)

where

φ(0) =

1
2α

−1

cosh

DB

"

±

b√D2
B2

−
b2

−

B2 + b2

,

#

D =

9k
4R(0)2 −

a1

a2

−
2

and B =

a1 + a2
2

=

m2
2

.

Therefore, the acceptable values of X0 and Y0 ( X0 >
equation (17).

Y0
|

) can also be obtained analytically from
|

1

The question of the allowed set of the initial conditions, though interesting enough in its own right in all problems
of this type, is of crucial importance for the problem at hand, as the determination of the correct initial conditions is
an open problem in cosmology.

5

6
k = 1

k = -1

 

4  

2  

Y

0  

-2  

-4  

 

4  

2  

Y

0  

-2  

-4  

-4

-2

0

-4

-2

0

X

2

 

4

X

2

 

4

Figure 1: The contour plots of the allowed initial values of X and Y , satisfying the equation of constraint (16) for
k = ±1. The point (0,0) is a solution and the curves approaching this point actually pass through it, although this is
not shown on the plots due to the limitations on the numerical accuracy.

3.2 Integration algorithm

Y .
The important feature of equations (10) and (11) is that they are singular for all β at which X =
±
At these critical values of β (βc), the original variables take the values R(βc) = 0 and φ(βc) =
.
±∞
We can directly infer from the diﬀerential equations that the solutions for the new variables and
their ﬁrst derivatives have to be continuous across the singularities. However, the second and higher
derivatives will be singular at βc. That is, the singularities of the new variables are considerably
milder than those of the original variables.

Although the solutions and their ﬁrst derivatives are continuous across βc, they cause problems
for the integration algorithm. Any attempt in solving these equations involves handling these moving
singularities, as one encounters them when integrating the coupled equations. To proceed, we ﬁrst
establish jump conditions across these singular points as follows: close to βc we assume that the
solutions have the following linear forms

X± = a± + b±β,

Y± = c± + d±β,

±

refers to the right or left hand sides of the singularity, respectively. Substituting the above
where
equations in (10) and (11) and dropping all non-singular terms, one can integrate these equations
ǫ, βc + ǫ, where 2ǫ is the distance across the jump. For the integration we have
in the interval βc −
dropped all terms which would give rise to contributions

(ǫ4/3). One ﬁnds at Yc =

Xc

O

±

b+

b− =

−

9
4

k

(2Xcǫ)1/3
(b−

d−)2/3 βc =

−

(d+

d−),

∓

−

∓

where ǫ can be taken as small a value as is desired for any required accuracy. Equation (20), together
with the requirement of continuity of X and Y , establish our jump condition for handling the sin-
gularities of the diﬀerential equations. It is apparent from equation (20) that the slopes X ′(β) and
Y ′(β) are continuous at βc.

Writing an actual algorithm for handling these singularities requires some care. Let us ﬁrst write

the original variable R(β) in terms of the new variables,

(18)

(19)

(20)

R = (X 2

Y 2)1/3.

−

6

Y

-2

X

10

7.5

5

2.5

-2.5

-5

-7.5

-10

k = 1

k = -1

2

4

6

8

beta

10

2

4

6

8

beta

10

Y

-2

X

10

7.5

5

2.5

-2.5

-5

-7.5

-10

Figure 2: Solutions for X(β) (broken curve) and Y (β) (solid curve), for k = ±1. The values of the parameters are
b = 2, λ = 0, m2

= 4.5.

|

We recall that the diﬀerential equations become singular when R = 0. As we integrate them, when
becomes small (less than 1) we reduce the step size by one order of magnitude since the crossing of
R
|
becomes smaller than 0.1,
R(β) through zero at βc is rather steep. Then at the ﬁrst instant when
henceforth called ‘the ﬁxed point’, the algorithm records all the relevant values (X, Y, X ′, Y ′, R, β)
and continues integrating towards the singular point with yet ﬁner steps. Past the ﬁxed point, the
singular terms in the diﬀerential equations become too large and no integration algorithm can give
reliable values for X and Y . However, we can use the information obtained past this point to pinpoint
βc as follows: We record the last two values of β and R right before the instant when the sign of
βc)1/3 (which is consistent with equations (18) and (19)) one
R changes. Then assuming R
obtains

R
|

(β

−

∝

|

R3

2β1
R3

1β2

R3
R3
1

|

.

(21)

βc =

−

±

increases beyond 1 and with regular steps until it approaches the next singularity.

−
2 −
βc and use a linear extrapolation to obtain Xc and
Having obtained βc, we can calculate ǫ = βﬁxed
Yc and see whether Yc =
Xc as a consistency check. We can then calculate the values of the slopes
(X ′, Y ′) on the other side of the singularity (at βover = βc + ǫ) using the jump conditions (equation
(20)) and then using linear extrapolation on both sides of the singularity, the values of the functions
can be calculated at βover. The algorithm then continues integrating with ﬁne steps until the value
of

R
|
We use a set of parameters (b = 2, λ = 0, m2 = 4.5) in equations (10-12) which are physically
relevant and choose our initial conditions consistent with equations (15) and (16). Recall that since
equation (12) is a constant of motion, if it is satisﬁed at β = 0, for a true solution it will be satisﬁed
at all other values of β. Therefore if the initial values for the functions X and Y satisfy equation (16)
at β = 0, equation (12) should always be satisﬁed. For integrating equations (10) and (11), we have
used the fourth order Runge-Kutta method. The resulting solutions for k =
1 are shown in ﬁgure
2. It is apparent from ﬁgure 2 that at the singular points X =
Y , the solutions are continuous and
the singularities are very mild. As a measure of the accuracy of the solutions, we have computed
the ‘zero energy condition’, equation (12), as a function of β for k =
1 which are shown in ﬁgure
3. It is evident from ﬁgure 3 that the values of ‘total energy’ stay very close to zero, thus indicating
the validity of the numerical solution. In ﬁgure 4 the variations of the original variables φ and R
are shown. As can be seen, φ actually diverges at the singular points. As a further check, we have
numerically recovered the analytic solutions presented in [8] for k = 0 in every detail.

±

±

±

7

0.01 

0.0075

0.005

0.0025

-0.0025

-0.005

-0.0075

-0.01

k = 1

k = -1

2

4

6

8

beta

10

2

4

6

8

beta

10

0.01 

0.0075

0.005

0.0025

-0.0025

-0.005

-0.0075

-0.01

Figure 3: The graph of the total energy deﬁned by equation (12) for k = ±1. As is apparent from the graphs, the
zero energy condition is satisﬁed to a high accuracy. The small jumps in the graphs are at the critical values of β where
there are singular points: Y (βc) = ±X(βc).

-2

-1.5

-1

-0.5

0.5

1

1.5

beta

phi

phi

R

R

4

3

2

1

-1

-2

-3

-4

8

Figure 4: Graphical representation of the original variables φ(β) and R(β), for b = 2, λ = 0, m2 = 4.5, and k = 1
(broken curves) and k = −1 (solid curves).

4 Conclusions

We have shown how a particular class of initial valued coupled non-linear ordinary diﬀerential equa-
tions with moving singularities can be numerically solved. The main obstacle of having moving
singularities can be overcome by establishing a set of jump conditions across them. These conditions
are obtained by approximating the form of the solutions close to the singular points and directly
integrating the diﬀerential equations in the neighbourhood of these points. We have found that a
ﬁrst order approximation close to these points is suﬃciently accurate. Also since the main source
of error in the solutions eminates from the jump conditions, we have found that the fourth order
Runge-Kutta is suﬃcient for the integration algorithm.

References

[1] S. S. Gousheh, J. Comput. Phys. 123, 162 (1996).

[2] M. Tajdari, SIAM J. Appl. Math. 44, 881 (1984);
M. Tajdari, SIAM J. Appl. Math. 50, 1764 (1990);
M. Tajdari, SIAM J. Appl. Math. 56, 1363 (1996).

[3] R. S. D. Thomas, Utilitas Mathematica 43, 7 (1993).

[4] A. D. Raptis and J. R. Cash, Comput. phys. Commun. 44, 95 (1987);
A. D. Raptis and J. R. Cash, Comput. phys. Commun. 36, 113 (1985).

[5] T. E. Simos, J. Comput. Phys. 108, 175 (1993).

[6] P. J. Prince and J. R. Dormand, J. Comput. Appl. Math. 7, No. 1, 68 (1981).

[7] J. R. Dormand, M. E. A. El-Mikkawy and P. J. Prince, IMA J. Numer. Anal. 7, 423 (1987).

[8] T. Dereli and R. W. Tucker Class. Quantum Grav. 10, 365 (1993)

[9] K. Ghafoori-Tabrizi, S. S. Gousheh and H. R. Sepangi Int. J. Mod. Phys. A 15, 1521 (2000).

[10] C. Hellaby and T. Dray Phys. Rev. D 49, 5096 (1994).

9


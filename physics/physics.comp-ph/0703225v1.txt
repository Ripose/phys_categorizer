7
0
0
2
 
r
a

M
 
6
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
5
2
2
3
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Entropic eﬀects in large-scale Monte Carlo simulations

Cristian Predescu∗
Department of Chemistry and Kenneth S. Pitzer Center for Theoretical Chemistry,
University of California, Berkeley, California 94720
(Dated: February 21, 2014)

The eﬃciency of Monte Carlo samplers is dictated not only by energetic eﬀects, such as large
barriers, but also by entropic eﬀects that are due to the sheer volume that is sampled. The latter
eﬀects appear in the form of an entropic mismatch or divergence between the direct and reverse trial
moves. We provide lower and upper bounds for the average acceptance probability in terms of the
R´enyi divergence of order 1/2. We show that the asymptotic ﬁnitude of the entropic divergence is
the necessary and suﬃcient condition for non-vanishing acceptance probabilities in the limit of large
dimensions. Furthermore, we demonstrate that the upper bound is reasonably tight by showing that
the exponent is asymptotically exact for systems made up of a large number of independent and
identically distributed subsystems. For the last statement, we provide an alternative proof that relies
on the reformulation of the acceptance probability as a large deviation problem. The reformulation
also leads to a class of low-variance estimators for strongly asymmetric distributions. We show that
the entropy divergence causes a decay in the average displacements with the number of dimensions
n that are simultaneously updated. For systems that have a well-deﬁned thermodynamic limit, the
−1/6 for Smart Monte Carlo
−1/2 for random-walk Monte Carlo and n
decay is demonstrated to be n
(SMC). Numerical simulations of the LJ38 cluster show that SMC is virtually as eﬃcient as the
Markov chain implementation of the Gibbs sampler, which is normally utilized for Lennard-Jones
clusters. An application of the entropic inequalities to the parallel tempering method demonstrates
that the number of replicas increases as the square root of the heat capacity of the system.

PACS numbers: 02.70.Tt, 05.10.Ln, 05.40.Jc
Keywords: Metropolis-Hastings, large deviation, R´enyi divergence, relative entropy, Smart Monte Carlo,
random-walk Monte Carlo, Brownian motion, diﬀusion, Lennard-Jones cluster

I.

INTRODUCTION

Traditionally, the line of thought in designing Monte
Carlo algorithms [1, 2] has been that of overcoming large-
energy barriers. With few exceptions, the algorithms
constructed according to the Metropolis-Hastings pre-
scription are local in nature. Unless large jumps directly
over the barrier are attempted, the rate of convergence
β∆E), where β is the inverse
is proportional to exp(
temperature and ∆E is the energy barrier. In physical
terms, the underlying stochastic dynamics becomes an
activated diﬀusive process, akin to the Smoluchowski dy-
namics. Then, what determines the size of the jumps?
Of course, one factor is the presence of energetic barriers.

−

A second factor is the number of variables that are
simultaneously updated.
It is experimental knowledge
that updating many particles simultaneously results in a
decrease of the average displacements. The typical justi-
ﬁcation is that the likelihood that the particles collide is
increased. Owing to the ensuing increase in the energy,
the move is more likely to be rejected. It was recently
realized that this explanation is erroneous and that the
inﬂuence of the dimensionality on the eﬃciency of sam-
plers is mostly an entropic eﬀect [3]. Numerical exper-
iments have shown that the eﬀect features a behavior
similar to that predicted for systems made up of inde-

∗Electronic address: cpredescu@comcast.net

pendent particles, which particles cannot collide. In the
original work, the eﬀect has not been properly quanti-
ﬁed. Given the importance of the rejection-based Monte
Carlo algorithms in the physical sciences, the author feels
that a proper analysis is highly desirable. More gener-
ally, acquiring tools necessary to quantify the eﬀect of
dimensionality is of deﬁnite importance, as Monte Carlo
methods are techniques intended to cope speciﬁcally with
large dimensional systems.

As mathematical technique, the starting point are
some simple yet strong inequalities. Consider a d-
dimensional system described by a probability density
ρ(x), which we shall cast in the normalized Boltzmann
βV (x)/Q(β). The Metropolis-Hastings sampler
form e−
utilizes the trial distribution T (y
x). The average accep-
|
tance probability is given by the formula

=

dx

dyρ(x)T (y

A

Z

Z
In Section II, we show that

x) min
|

ρ(y)T (x
y)
|
x)
ρ(x)T (y
|

(cid:27)

1,

(cid:26)

.

(1)

1
2

exp

D1/2

exp

D1/2

.

(2)

≤ A ≤

−
(cid:18)
(cid:0)
0 is the R´enyi entropy divergence of order

−

(cid:19)

(cid:1)

1
2

Here, D1/2 ≥
1/2 deﬁned by the expression [4]

D1/2 =

2 log

−

dx

dy [ρ(x)T (y

y)]1/2
x)ρ(y)T (x
|
|

(cid:18)Z

Z

.

(cid:19)
(3)

.

· · ·

n
2

x1)

D1/2

An ≤

T (yn|

The divergence D1/2 measures the mismatch between
the direct sampling distribution ρ(x)T (y
x) and the re-
|
y).
verse distribution ρ(y)T (x
It is zero if and only if
|
detailed balance is already achieved, an unlikely scenario
in practical applications. As an extensive entropy, D1/2
is additive for independent probability distributions. As
such, if we sample together a system made up of n in-
dependent replicas from the product trial distribution
T (y1|
xn), then the acceptance probability
An satisﬁes the inequality
exp

−
We have already presented some of the mathematical
results so that to entice the reader’s attention. The ex-
ponential decay predicted by Eq. (4) automatically im-
plies that the entropic divergence D1/2 cannot be left
unchanged. It must be decreased at least as fast as 1/n.
This can be achieved by tuning some parameters. For ex-
ample, we may decrease the average displacements for the
random-walk Metropolis algorithm or the ratio between
consecutive temperatures for parallel tempering. Details
are left for Section IV. They prove to be consistent with
prior analysis, wherever such analysis is available.
In
particular, the results for the random walk Metropo-
lis algorithm and the Smart Monte Carlo method agree
with those obtained by Roberts and coworkers [5, 6, 7],
whereas the results for parallel tempering simulations
agree with those of Kofke [8] and Predescu et al [9].

(4)

(cid:16)

(cid:17)

A great deal of care is dedicated to the systems made
up of a large number n of identical and independent sub-
systems. Indeed, an analysis of the dependence with the
dimensionality requires some form of homogeneity of the
system in the thermodynamic limit. In addition, the as-
sumption of independence is a model for the physical
reality that suﬃciently large subsystems interact weakly.
More precisely, various properties become extensive in
the thermodynamic limit because the contribution of the
frontier interactions to these properties becomes compar-
atively small.
In Section II, we demonstrate that the
inequality given by Eq. (4) is reasonably tight. More
x) utilized for sam-
precisely, if the trial distribution T (y
|
pling a subsystem is left unchanged as the dimensionality
of the overall system is increased, then

lim
n
→∞

−

n−

1 log (

An) =

1
2

D1/2.

This last identity is the correct statement for the bad
sampling theorem of Ref. 3, which incorrectly asserted
the law

lim
n
→∞

−

n−

1 log (

An) = D1,

with D1 being the relative Shannon entropy

D1 =

dx

dyρ(x)T (y

−

Z

Z

x) log
|

y)
ρ(y)T (x
|
x)
ρ(x)T (y
|

(cid:20)

(cid:21)

.

(7)

Save for this correction, all predictions made in Ref. 3
with respect to the behavior of the average displacements

(5)

(6)

2

as a function of dimensionality remain the same. The rea-
son is that the asymptotic ﬁnitude of the relative Shan-
non entropy D1 (also called the Kullback–Leibler diver-
gence or the R´enyi divergence of order 1) is a suﬃcient
condition for non-vanishing acceptance probabilities.

In Section III, we recast the evaluation of the average
acceptance probability as a large deviation problem. We
then utilize Cram´er’s large deviation theorem to provide
a second demonstration of Eq. (5). The rare events as-
sociated with large deviation problems are a source of
exponentially increasing variances for quantities such as
acceptance probabilities or ratios of partition functions.
To alleviate the overlap problem, a class of low-variance
estimators for strongly asymmetric distributions is in-
troduced, along with a demonstration of their statistical
eﬃciency.

In Section V, we ﬁnd that the Smart Monte Carlo
method of Rossky, Doll, and Friedman [10] features an
1/6. Numerical veriﬁ-
improved theoretical scaling of n−
cation recovers the predicted scaling for correlated sys-
tems, namely embedded Lennard-Jones clusters. The
ﬁndings are in agreement with the results of Roberts and
1/6 scaling
Rosenthal [6], who have obtained the same n−
for systems made up of statistically independent subsys-
tems. A simulation of the LJ38 cluster shows that Smart
Monte Carlo is roughly as eﬃcient as the Metropolis al-
gorithm with single-particle updates, which is the tech-
nique normally utilized for clusters. For more expensive
potentials, it is argued that Smart Monte Carlo will likely
be more eﬃcient provided that the number of atoms up-
dated simultaneously is in the range of tens to hundreds.
Section VI contains the conclusions of the paper, which
are mostly recommendations on the design of Monte
Carlo algorithms so that to minimize the negative im-
pact of the entropic eﬀects.

II. PROOF OF THE MATHEMATICAL CLAIMS

For the reminder of this paper, the notation π(x, y) =
ρ(x)T (y
x) turns out to be convenient. Notice that the
|
trial distribution π(x, y) is normally strongly asymmet-
ric and that its symmetry is equivalent to the detailed
balance condition.
In addition, we shall also need the
antisymmetric function

X(x, y) = log [π(y, x)/π(x, y)] ,

(8)

which we regard as a random variable with respect to the
probability distribution π(x, y). The expected value of
some random variable Y , that is, the quantity

dxdyπ(x, y)Y (x, y),

(9)

Z Z

will be denoted by E(Y ) or simply EY in order to avoid
the more cumbersome integral notation.

The average acceptance probability reads

= E min

1, eX

= EeX/2 min

e−

X/2, eX/2

.

(10)

A

(cid:8)

(cid:9)

n

o

o

≤

Observe the equality

It follows that

n

min

e−

X/2, eX/2

= e−|

X

/2

|

1.

≤

(11)

= EeX/2e−|

X

/2

|

EeX/2 = e−

2 D1/2 ,

1

(12)

A

which is the upper-bound inequality in Eq. (2).

To establish the other inequality, recall that 1/x is a
). By virtue of Jensen’s

[0,

convex function on x
inequality, we have

∈

∞

= EeX/2

A

EeX/2e−|
X
EeX/2

/2

|

≥

EeX/2

EeX/2e|
X
EeX/2

/2

|

(cid:18)

1

−

.

(cid:19)

(13)

A helpful identity is

Ee(X+

|

X

)/2 = E max

|

1, eX

E

1 + eX

= 2.

(14)

(cid:0)
Combining with Eq. (13), we obtain

(cid:8)

(cid:9)

A ≥

2

EeX/2

(cid:16)

(cid:17)

.

Ee(X+

|

X

)/2

|

EeX/2

=

e−

1
2

D1/2,
(15)

(cid:16)

(cid:17)

which is the lower-bound inequality in Eq. (2).

Given their relevance for Monte Carlo algorithms, it is
worthwhile to recall the deﬁnitional features of the R´enyi
divergences. For α
0, the R´enyi divergence of order α
for continuous distributions p(x) and q(x) is given by the
expression [4]

≥

Dα(p

q) =

k

α

1

log

(cid:20)Z

1

−

p(x)αq(x)1

−

(16)

.

αdx
(cid:21)

For α = 1, the R´enyi divergence is deﬁned by continuity,
as the limit α
1. The result, which can be obtained
by means of l’Hˆopital’s rule, is the Kullback–Leibler di-
vergence (or the relative Shannon entropy)

→

≤

1
2

≥

(cid:1)

2

D1(p

q) =

k

−

Z

p(x) log [q(x)/p(x)] dx.

(17)

The R´enyi divergences are non-negative quantities. They
are zero if and only if p(x) and q(x) are identical ex-
cept for a set of measure zero. As expected of entropies,
the R´enyi divergences are additive for independent dis-
tributions. More precisely, if p(x, x′) = p1(x)p2(x′) and
q(x, x′) = q1(x)q2(x′) then

Dα(p

q) = Dα(p1k
a relation we shall utilize in the following section.

q1) + Dα(p2k

q2),

k

(18)

or

As far as the theory developed in the present paper is
concerned, of importance is the R´enyi divergence of order
1/2, which provides a measure of the mismatch between
y) sam-
x) and the reverse ρ(y)T (x
the direct ρ(x)T (y
|
|
pling probabilities. In words, the divergence quantiﬁes
the lack of detailed balance in a way that relates directly
to the values of the acceptance probabilities, by virtue

3

of the inequalities we have established. Notice that the
divergence of order 1/2 is symmetric under the permu-
tation of p and q. The common value is bounded from
above by either of the Kullback–Leibler divergences

k

D1/2(p

q) = D1/2(q

.
p)
}
(19)
This follows from Jensen’s inequality and the convexity
of the function

q), D1(q

log(x).

D1(p

min

p)

≤

k

{

k

k

A diﬀerent lower bound for the acceptance probability
helps demonstrate the limiting result given by Eq. (5),
for independent systems. Let us denote by Es(Y ) the ex-
pected value of some random variable against the sym-
metric probability distribution πs(x, y) deﬁned by the
expression

−

π(x, y) exp[X(x, y)/2]
E exp(X/2)

=

eD1/2π(x, y)π(y, x)

1/2

.

(20)
(cid:2)
With this notation, the expression for the acceptance
probability [see the left-hand side of Eq. (13)] becomes

(cid:3)

=

EeX/2

Ese−|

X

/2

|

= e−

2 D1/2Ese−|

X

/2.

|

(21)

1

(cid:17) (cid:16)

(cid:17)

x is a convex function, Jensen’s inequality pro-

A

(cid:16)
Since e−
duces

1

Es|
2 D1/2 exp (
X
A ≥
−
Cauchy’s inequality implies Es|
X

| ≤

e−

/2) .
|
(EsX 2)1/2 and so,

(22)

exp

A ≥

−

(cid:20)

1
2

D1/2 −

1
2

EsX 2

1/2

.

(cid:21)

(cid:0)

(cid:1)

(23)

It is worth noting that (EsX 2)1/2 is in fact a stan-
dard deviation. The function X(x, y) is antisymmetric
and, consequently, its expectation against the symmetric
measure πs(x, y) is zero. To see the relevance of this ob-
servation, assume that we sample together a large num-
ber n of identical and statistically independent systems
described by the same probability distribution ρ(x). As-
sume also that we attempt to utilize a same trial distri-
x), independent of n. The formula for the
bution T (y
|
average acceptance probability reads

An =

· · ·

Z

Z

dx1dy1 · · ·

dxndynπ(x1, y1)

· · ·

π(xn, yn) min

1,

×

(

n

i=1
Y

π(yi, xi)
π(xi, yi) )

,

(24)

An = EeSn/2Ese−|

Sn|

/2.

For the last identity, we have employed Eq. (21), with Sn
deﬁned accordingly by

Sn(x1, y1, . . . , xn, yn) = log

n

"

i=1
Y

π(yi, xi)
π(xi, yi) #

.

(25)

(26)

4

The identity

n

Sn =

log

π(yi, xi)
π(xi, yi)

i=1
X

(cid:20)

(cid:21)

i=1
X

n

and let IAc(x, y) be the indicator function of the comple-
ment of A. It follows that

=

X(xi, yi)

(27)

dxdy [π(x, y)IA(x, y) + π(y, x)IAc (x, y)] .

shows that Sn is a sum of independent and identically
distributed random variables Xi. By independence, iden-
tical distribution, and the vanishing expectation of each
Xi,

Let B be the set of points such that π(x, y) = π(y, x).
Use of symmetry in Eq. (33) produces IAc (x, y) =
IA(y, x) + IB(x, y). As such,

dxdy[π(x, y)IA(x, y) + π(y, x)IA(y, x)

EsS2

n =

EsXiXj = nEsX 2.

(28)

+π(x, y)IB (x, y)] = 2

dxdyπ(x, y)IA(x, y) (34)

=

A

Z Z

=

A

Z Z

n

i,j=1
X

On the other hand, the additivity of the R´enyi diver-
gence of order 1/2 for independent distributions implies

Ee−

Sn/2 = e−

(n/2)D1/2.

(29)

We obtain

and conclude

(n/2)D1/2

e−

≥ An ≥

(n/2)D1/2

e−

−

(nEsX

2

1/2

)

/2

(30)

≤

1
2

1
2

1
n

log(

An)

D1/2 +

D1/2 ≤ −

1
2n1/2 (EsX 2)1/2. (31)
This sequence of inequalities produces Eq. (5) upon let-
. We shall construct another proof for Eq. (5)
ting n
in the next section, by means of Cram´er’s large deviation
theorem.

→ ∞

III. THE ACCEPTANCE PROBABILITY AS A
LARGE DEVIATION PROBLEM

The basic task of this section is to reformulate the eval-
uation of the acceptance probability as a large deviation
problem. By doing so, we obtain a better understand-
ing of the source of the exponential decrease in the ac-
ceptance probability for independent systems. We will
ﬁnd that the source is the rare-event sampling normally
associated with large deviation problems.
In fact, we
shall construct a diﬀerent proof for Eq. (5) by means of
Cram´er’s large deviation theorem [11], which quantiﬁes
the frequency of such rare events. In addition, we are led
to the consideration of special low-variance estimators for
the acceptance probability, which can be generalized to
other scenarios, as done in Appendix I.

We start with the following formula for the acceptance

probability

=

A

Z Z

dxdy min

π(x, y), π(y, x)
}
{

.

(32)

Let IA(x, y) be the indicator function of the set

A =

(x, y) : π(x, y) < π(y, x)
}

{

,

(33)

+

dxdyπ(x, y)IB (x, y).

Z Z

Z Z

Notice that π(x, y) is symmetric if and only if X(x, y) =
0 and that π(x, y) < π(y, x) if and only if X(x, y) >
0.
is the
probability that X(x, y) = 0 plus twice the probability
that X(x, y) > 0. That is,

If follows that the acceptance probability

A

= P (X = 0) + 2P (X > 0).

(35)

A

For a system made up of a large number n of inde-
pendent and identically distributed subsystems sampled
together, Eq. (35) becomes

An = P (Sn/n = 0) + 2P (Sn/n > 0).
Recall the deﬁnition of Sn given by Eq. (27). The division
by n does not change the equalities or inequalities in
Eq. (35) and is a formality that arranges Eq. (36) in the
form typical of large deviation problems.

(36)

−

By the strong law of large numbers, Sn/n converges
to minus the Kullback–Leibler divergence D1 given by
If the detailed balance is not satisﬁed almost
Eq. (7).
D1 < 0. Naturally, this implies
everywhere, then
P (Sn/n = 0)
. For
large deviation problems, the decay of the last probabil-
ities is exponentially fast. For the simple case discussed
here, where Sn is a sum of identical and statistically in-
dependent random variables, the law of the exponential
decay is exactly known and is given by Cram´er’s theorem,
the application of which leads again to

0 and P (Sn/n > 0)

0, as n

→ ∞

→

→

lim
n
→∞

−

n−

1 log(

An) =

1
2

D1/2,

(37)

with D1/2 deﬁned by Eq. (4). The details of the proof
are given in Appendix II.

Eq. (34) provides two diﬀerent estimators for the ac-
ceptance probability, the ﬁfty-ﬁfty average of which is
that implied by Eq. (1). The ﬁrst one is given by Eq. (35)
and is only adequate for large values of the acceptance
probability (larger than 1/2). The second can be deduced
by a similar reasoning, save for changing the sense of the
inequality in the deﬁnition of the set A. It is given by

= P (X = 0) + 2

dxdyπ(x, y) exp[X(x, y)].

A

Z ZX<0

(38)

In words, we ﬁrst test if the move is likely to be rejected.
If the answer is positive, then we accumulate twice the ra-
x). We accumulate 1 if detailed
y)/ρ(x)T (y
tio ρ(y)T (x
|
|
balance is already satisﬁed and 0 otherwise. The second
estimator is much better behaved than the ﬁrst if the ac-
ceptance probability is small, which happens when many
moves are likely to be rejected. In this case, the proba-
bility for the event X < 0 is large and we get adequate
statistics.
1/2,
so that P (X
1/2, meaning that the number of
points necessary for the implementation of Eq. (38) is al-
ways more than half the total number. In addition, the
variance of the estimator is always more favorable, since
exp[X(x, y)] is smaller than 1 whenever X < 0. In many
cases, smaller means signiﬁcantly smaller and the stan-
dard deviation of the estimator turns out to be roughly
proportional to the value of the acceptance probability
itself.

In fact, Eq. (35) implies P (X > 0)

0)

≤

≤

≥

The strategy presented in the preceding paragraph has
been utilized before by the present author and collabo-
rators in the context of replica exchange methods for the
design of partition function estimators that alleviate the
overlap problem [12].
In fact, the source of the over-
lap problem is a poor statistics related to the fact that
P (X > 0) can be exponentially small. Motivated by
these examples, we present and justify the general form
of such estimators in Appendix I.

The practical relevance of Eq. (38) is for tuning the
various parameters controlling a simulation. There, good
accuracy over few Monte Carlo cycles is required in con-
ditions in which most attempted moves are rejected. On
a computer, where the equality X = 0 cannot be tested
exactly in ﬂoating-point arithmetic owing to inherent nu-
merical errors, the function

emin
{

0,X(x,y)

}

0,
1,
2,

if X(x, y) > ǫ,
X(x, y)
if
|
if X(x, y) <

ǫ,
ǫ

| ≤
−

× 


(39)

can be accumulated instead. The estimator is valid for
arbitrary ǫ
0 but is to be employed with a small value
of ǫ, say the square root of the machine precision.



≥

IV. SCALING OF TUNING PARAMETERS
WITH THE DIMENSIONALITY: EXAMPLES

Eq. (2) produces the necessary and suﬃcient condi-
tion for the acceptance probabilities to remain ﬁnite upon
large changes in various parameters, such as the dimen-
sionality of the system. Let D1/2(d, α) denote the R´enyi
divergence of order 1/2 for a d-dimensional sampler uti-
lizing a trial distribution additionally characterized by
the parameter or the family of parameters α. The neces-
sary and suﬃcient condition for nonvanishing asymptotic
acceptance probabilities is the existence of values αd such
that

5

In words, the sequence D1/2(d, αd) must be bounded. If
D1/2(d, αd) has a subsequence increasing to inﬁnity, then
the corresponding acceptance probabilities will converge
to zero, by the upper-bound inequality in Eq. (2). This
establishes the necessity. Conversely, if D1/2(d, αd) is
bounded from above by M , then the acceptance prob-
M /2, by the lower-bound
abilities cannot fall below e−
inequality.

Albeit the index d does not have to be the dimension,
the dependence with the dimensionality is the natural
problem to study, owing to the additivity of the entropy
divergence for independent systems and trial distribu-
tions. The necessity for tuning the parameters α comes
from an expected unlimited increase in the entropic di-
vergence if these parameters are kept constant. For deﬁ-
niteness, assume that we are given a chemically homoge-
neous system. For proposals T (y, x) that are products of
dimension-independent distributions, we expect the en-
tropic divergence to increase linearly with the number of
dimensions. Indeed, the proposal is already conditionally
independent, whereas the particles making up a physical
system must decorrelate in the thermodynamic limit. A
better insight is provided by the Kullback-Leibler diver-
gence, which, according to Eq. (19), bounds the R´enyi
divergence of order 1/2 from above. The behavior of
D1 closely mimics the behavior of the Boltzmann-Gibbs
entropy except for the case where the proposal distribu-
tion introduces additional correlation. (Recall that the
Boltzmann-Gibbs entropy and the Shannon entropy are
given by the same formula, except for the multiplicative
Boltzmann constant). Moreover, D1 is easier to evaluate
numerically if veriﬁcation of linearity is desired in order
to test for ﬁnite-size eﬀects.

In the reminder of the section, we apply the condi-
tion to the random-walk Metropolis algorithm and the
parallel tempering method in order to demonstrate the
versatility of the entropic inequalities in predicting the
scaling behavior in various situations. The random-walk
Metropolis algorithm is constructed from a proposal dis-
tribution of the form

x) = α−
T (x + z
|

1f (α−

1z).

(41)

7→ −

Here, f (z) is a normalized distribution invariant under
the transformation z
z. For simplicity, we take f (z)
to be symmetric in each of the variables zi. In addition,
we require that f (z) be a product of low-dimensional
distributions, so that the entropic divergence does not
increase faster than linearly. Typically, the scaling factor
α > 0 is tuned such that the acceptance probability lies
somewhere between 20% and 50%. The entropic diver-
gence D1/2(d, α) is

2 log

1

α−

dx

dzf (α−

1z) [ρ(x)ρ(x + z)]1/2

−

(cid:18)

Z

=

2 log

−

Z
dx

(cid:18)Z

Z

dz [ρ(x)ρ(x + αz)]1/2 f (z)

.

(cid:19)

(cid:19)

D1/2(d, αd)

M <

≤

,
∞

d

∀

≥

1.

(40)

For given d, the entropic divergence converges to zero as

0. In the same limit, the trial distribution shrinks

α
to a delta function.

→

For small α, the leading term in the entropic diver-
gence, as follows from a Taylor expansion around α = 0,
is the Fisher entropy. More precisely, we have

α2
4

d

σ2
i

i=1
X

Z

1
ρ(x)

2

∂ρ(x)
∂xi (cid:21)

(cid:20)

dx + O(α4d),

(42)

D1/2(d, α) =

where

σ2
i =

f (z)z2

i dz,

1

i

≤

≤

d

Z

are the second-order moments of the kernel f (z). Un-
der the assumptions we made about f (z), the ﬁrst-order
moments and the cross second-order moments are zero.
The error has an implied linear-only dependence with the
dimension owing to the assumed decorrelation of the par-
ticles in the thermodynamic limit. Eq. (40) implies that
1/2 (more generally, as the
αd must decrease as fast as d−
inverse square root of the Fisher entropy). Since αdσi
is the standard deviation in the direction i of the trial
distribution, we see that the average displacements are
proportional to the inverse square root of the number of
dimensions sampled together. That the scaling is correct
has been veriﬁed numerically for Lennard-Jones clusters
[3] and found to hold true. In addition, the mathematical
analysis performed by Roberts et al [5] for independent
systems leads to the same conclusion.

The ﬁnitude requirement for the second-order mo-
ments of the function f (z) sets some constraints on the
length of the tails for proposal rules that are products
of low-dimensional distributions. Normally, we would
like to utilize such products in order to avoid introduc-
ing artiﬁcial correlation between distant particles. Also,
functions f (z) with long tails seem more adequate for
potentials with rough topologies. A good compromise is
provided by products of coordinate-scaled versions of the
one-dimensional function

f (z) = (1 + z2)−

3/2/2.

(43)

−

3/2

Albeit f (z) does not have a ﬁnite variance, its second mo-
ment is only mildly divergent and the distribution seems
appropriate up to a few tens of variables updated simul-
taneously. The advantage over a ﬁnite-variance function
of the form (1 + z2)−
ǫ is that random numbers dis-
tributed according to f (z) can be cheaply generated by
ξ)1/2), with
means of the identity ζ = (ξ
ξ uniformly distributed on (0, 1). The trial distribution
just described appeared to be superior to the standard
ﬂat distribution in the Monte Carlo sampling of a path-
integral implementation of the Onsager-Machlup formula
[13]. Nevertheless, any clear advantage of a long-tail dis-
tribution is perhaps limited to rough potential surfaces.
The second example we work out is for parallel temper-
ing [2, 14], where swaps are attempted between two large

1/2)/(ξ(1

−

−

6

(44)

dimensional systems characterized by the inverse tem-
peratures β′
β [9]. The entropic divergence is given
by

≥

D1/2(d, R) =

2 log

−

Q( ¯β)2
Q(β)Q(β′)

,

(cid:21)

(cid:20)

where ¯β = (β + β′)/2 and R = β′/β
of Ref. 9 lead to

≥

1. Eqs. 10 and 20

D1/2(d, R) = 2

R
1
−
R + 1

2

(cid:19)

(cid:18)

C(d)
V ( ¯β) + O(d
R
|

1

3),
|

−

where C(d)
capacity of the physical system.

V ( ¯β) is the potential contribution to the heat

Again, we can compensate the linear increase charac-
teristic of the heat capacity by letting the ratio R get
close to 1 suﬃciently fast. We must choose a dimension-
dependent value Rd such that

C(d)
V ( ¯β)(Rd −

1)2

≤

M <

,
∞

(45)

The last inequality predicts that the temperatures β and
1/2, a con-
β′ must get close one to another as fast as d−
dition that is also suﬃcient. That the heat capacity is
the main cause for the decrease in the temperature ra-
tios has been demonstrated before by Kofke [8]. The
1/2 scaling has been conﬁrmed by numerous simula-
d−
tions [9, 15] and also subjected to speciﬁc mathematical
analysis [8, 9].

V. ANALYSIS OF THE SMART MONTE
CARLO METHOD

The force-biased Monte Carlo methods [16] attempt to
improve their eﬃciency by sampling from a more suitable
local distribution constructed with the help of the ﬁrst-
order derivatives of the potential.
In one-dimensional
notation, we have

e−

βV (y) = e−

βV (x)

βV

(x)(y

′

−

−

x) + O(
y
|

2).
x
|

−

(46)

The right-hand side must be multiplied by a windowing
function that eventually determines the lengthscale on
which the Taylor approximation is valid. The windowing
function must also decay fast enough to inﬁnity, so that
to make the right-hand side integrable. It was originally
taken to be constant on a symmetric interval centered
about x. If, for ease of computation, it is taken to be a
Gaussian, then

e−

βV (y) = e−

βV (x)

βV

(x)(y

x)

(y

x)

/2σ

−

−

−

−

′

2

2

y
+ O(
|

−

2).
x
|

Completing the square, we deduce that a suitable pro-
posal distribution is

T (y

x) =
|

1
√2πσ2

exp

y
(− (cid:2)

−

x + βσ2V ′(x)

2

2σ2

. (47)

)

(cid:3)

−

We notice that the trial move is the transition kernel for a
Smoluchowski process (or generalized Brownian motion),
and this is clearly true if we only care to set σ2 = 2Dt,
with D being the diﬀusion constant and t being the
(short) transit time. Nevertheless, the equilibrium tem-
perature is wrong, namely exp[

2βV (x)].

The Smart Monte Carlo method was introduced by
Rossky, Doll, and Friedman [10] not as a force-biased
technique, but as a Metropolis-Hastings correction to a
Smoluchowski process. The idea is that the latter process
is already an ergodic sampler for the Boltzmann distri-
bution. However, the short-time transition kernels that
are numerically available are only asymptotically accu-
rate. The transition kernel described by Eq. (47), with
the inverse temperature β replaced by β/2, is character-
istic of the stochastic Euler’s integrator and has order
of convergence 1. Rao and Berne [17] have subsequently
modiﬁed the force-biased method and introduced a tun-
ing parameter θ to control the force bias.

We follow their approach and deﬁne a class of transi-
tion kernels depending on a parameter θ and having the
form

2

.

d

p

exp

i=1
Y

Tθ(y

(− (cid:2)

yi −

x) =
|

1
2πσ2
i

xi + θβσ2
2σ2
i

i ∇iV (x)
)
(cid:3)
(48)
Notice that for multidimensional systems the transition
kernel is a product of one-dimensional distributions. As
discussed in the preceding section, we expect the en-
tropic divergence to behave linearly in the thermody-
namic limit, because the proposal does not introduce
additional particle correlation over the Boltzmann dis-
tribution. Let us ﬁnd the value of θ that maximizes the
standard deviations σi for large dimensional systems. In
Appendix III, we show that the relative entropy D1/2(θ)
is given by the formula

d

1/2)2

D1/2(θ) = β2 (θ

[σi∇iV (x)]2

+ O(d
k

σ

4).

−

*

+

i=1
X

k
(49)
The brackets denote an average against the Boltzmann
distribution and the quantity itself is again the Fisher
entropy (of course, up to some scaling or multiplicative
constants).
Assume θ

= 1/2. From the asymptotic requirement

β2 (θ

1/2)2

−

[σi∇iV (x)]2

M <

,
∞

+ ≤

d

*

i=1
X

we learn that the maximal standard deviations scale as

σi,d ∼

i /d1/2,
σ0

(50)

if the acceptance probability is to converge to a non-
. Even more, the
vanishing value in the limit d
dominant term in Eq. (49) has the same value whether
θ = 0 or θ = 1.
It follows that, for suﬃciently large
systems, the original force-biased Monte Carlo method

→ ∞

7

we considered in Eq. (47) exhibits essentially the same
average displacements as the unbiased method.

The Smart Monte Carlo method corresponds to the
choice of parameter θ = 1/2, which is the only value
that cancels the dominant Fisher entropy. As argued in
Appendix III, for θ = 1/2, the decay of the entropic di-
6, but in general not faster.
vergence is as fast as d
k
k
Again, the factor d is always linear, albeit it needs not
be so for systems that do not have a well-deﬁned ther-
modynamic limit. The scaling of the standard deviations
with the dimensionality improves to

σ

σi,d ∼

σ0
i /d1/6,

(51)

where σ0
i are some asymptotic constants. This result
has been obtained before by Roberts and Rosenthal [6]
for independent systems and suggested by Kennedy and
Pendleton [18].

k

σ

Incidently, the asymptotic expansion worked out in
Appendix III shows that the cancellation of the term
4 is due to the special values of the moments of the
k
Gaussian window. The force-biased method can also
1/6 scaling provided that the original rect-
achieve the d−
angular window is modiﬁed so that to match the ﬁrst
5 moments of a Gaussian. The simplest replacement is
furnished by scaled products of

(2π)−

1/2e−

z

2

/2

2
5

≈

3
5

−

δ(z) +

I[

√5,√5](z).

(52)

The ﬁrst term is Dirac’s delta function, whereas the sec-
√5, √5].
ond is the indicator function for the interval [
The utilization of the delta function does not pose pro-
gramming problems. By its symmetry, the window is
only part of the proposal step and conveniently cancels in
the acceptance/rejection step, save for the normalization
coeﬃcient. A compactly supported window may allevi-
ate some of the numerical issues associated with Euler’s
integrator, as the particles are prevented from moving
arbitrarily far under harsh gradients.

−

In the remainder of the section, as an illustration,
we determine numerically the scaling of the tuned stan-
dard deviations σi,d for Lennard-Jones clusters. As well-
known, the potential energy is given by

Vtot(x) =

VLJ(rij ) +

Vc(ri),

(53)

Np

i<j
X

Np

i=1
X

with Np being the number of particles. As usual, VLJ(rij )
is the Lennard-Jones potential describing the interaction
between the particles i and j

12

"(cid:18)

VLJ(rij ) = 4ǫLJ

σLJ
rij (cid:19)

σLJ
rij (cid:19)
Vc(ri) is a conﬁning potential of the form
ri −
if
|
/Rc)3 , otherwise.
|

103ǫLJ (
ri −
|

Rcm

0,
5

−

(cid:26)

(cid:18)

·

6

.

#

Rcm

< Rc,

|

(54)

(55)

6
0.20

]

Å

[
4
/
1

0.15

= 1 / 2

= 10 K

= 5.7 K

8

T = 10 K

n = 11

n = 51

0.06

]

Å

[

0.04

0.02

n = 91

0.10
1

3 1

6 1

9 1

0.00

0 .0

0 .5

1 .0

FIG. 1: Scaling of the standard deviations with the number
of particles n updated simultaneously for the LJ91 cluster at
diﬀerent low temperatures. A conﬁning radius of 4σLJ is
utilized. The updated particles are chosen randomly. The
−1/4 behavior is caused by surface eﬀects, which
observed n
induce an artiﬁcial increase in the standard deviations for
small n. The scaling is expected to improve to n−1/6, in the
thermodynamic limit.

FIG. 2: Dependence of the standard deviations with the pa-
rameter θ for various numbers of particles updated simulta-
neously. The updated particles are those closest to the center
of mass of a 500-atom cluster. A conﬁning radius of 5.5σLJ
has been utilized. The θ-dependence expressed by Eq. (49)
implies that the plotted curves become symmetric in the ther-
modynamic limit and feature a |θ − 1/2|

−1 singularity.

Thus, the cluster is conﬁned to its center of mass Rcm.
The values of the Lennard-Jones parameters σLJ and ǫLJ
used are 2.749 ˚A and 35.6 K. They are characteristic of
the Ne atoms.

The conﬁning potential has been utilized before in
Ref. [13] for diﬀusion processes and found to be more
adequate than a steeply-increasing high-order polyno-
mial. The latter introduces numerical instabilities owing
to the high gradients experienced by the atoms reach-
ing the frontier. To overcome the tendency of SMC to
stall if high gradients are met, we have coupled the sam-
pler with a standard random-walk Metropolis algorithm.
The latter has been randomly utilized 25% of the time.
The coupling with a random-walk Metropolis algorithm
also serves a second purpose. Christensen et al [7] have
pointed out that SMC is slow to attain stationarity if
1/4 scaling is more like to be
run by itself and that an n−
observed in the transient regime.

The typical strategy is to simulate a cluster having a
large number of particles Np, while utilizing updates of
the whole system. In the second phase of the simulation,
the underlying all-particle sampler is kept running. A
smaller number of particles n are attempted to be up-
dated and the acceptance probability for such moves is
accumulated by means of the estimator discussed in Sec-
tion III. Because the standard deviations for the proposal
distribution keep changing, the attempted moves are not
accepted, as this would alter the detailed balance. The
computed acceptance probabilities are utilized to tune
the standard deviations for the hypothetical n-particle
sampler so that the ﬁnal acceptance probability lies be-
tween 39% and 41%. Since the moves are not accepted,
we can choose those n particles that are the closest to
the center of mass. The coating provided by the exterior

atoms minimizes the surface eﬀects. Otherwise, if the n
particles are chosen randomly, then the standard devia-
tions for small n are artiﬁcially increased, owing to the
larger mobility of the atoms at the surface. As shown
in Fig. 1, the surface eﬀects create the appearance of an
n−

1/4 scaling.
Fig. 2 shows the common standard deviation of the
trial distribution for diﬀerent numbers n of embedded
particles that are updated simultaneously and for diﬀer-
ent values of the parameter θ. As n increases, the value
θ = 1/2 becomes the optimal one, in agreement with the
theoretical predictions. For n = 51 and n = 91, it is also
apparent that the standard deviations for the unbiased
(θ = 0) and fully biased (θ = 1) Monte Carlo techniques
are almost equal, again, in agreement with the theory.
For small n, the plots are biased toward larger θ, which
suggests that the guidance provided by the force bias is
energetically favorable, yet eventually hindered by the
entropic eﬀects.

Despite signiﬁcant computational eﬀort, we were not
able to evaluate the standard deviations with suﬃcient
1/6 scaling.
accuracy for a proper demonstration of the n−
The reason is as follows. Even if we increase the num-
ber of particles updated simultaneously in an exponential
fashion, say n = 2k, the standard deviations for succes-
sive n diﬀer by a meager 12.2%. To correctly pinpoint
the scaling, the relative error in the standard deviations
needs to be about 2%. This is diﬃcult to achieve by
tuning. Many blocks are necessary, and the blocks must
contain suﬃciently many steps that the block averages
for the acceptance probabilities have adequate statistical
errors. However, a diﬀerent approach is equally valid and
computationally less expensive. We optimize the scaling
parameters for the largest number of particles we plan
to update (here, n = 91), and then decrease the num-

9

= 1 / 2

= 10 K

= 5.7 K

1.0

0.5

0.0
1

6.0

5.0

3.0

/

4.0

3 1

6 1

9 1

 0

 2

 4

 8

 10

 12

 6
 [K]

FIG. 3: Dependence of the observed acceptance probabilities
with the number of particles n updated simultaneously, for
−1/6 law. As for
standard deviations scaled according to the n
Fig. 2, the updated particles are those closest to the center of
mass of a 500-atom cluster. In both cases, the Lennard-Jones
potentials have been cut oﬀ beyond 3σLJ . The error bars are
given by the second signiﬁcant digit.

FIG. 4: The evolution of the heat capacity proﬁles is similar
to that from Fig. 1 of Ref. 19. Each of the six curves has been
obtained by collecting averages for groups of 175 million steps.
Early in the simulation, the heat capacity develops a fake
maximum, which is subsequently transformed in a “shoulder.”
The last four curves ﬂuctuate around this shoulder, with the
ﬂuctuations providing an estimate for the errors.

ber of atoms, say by 10. For n = 81, the theory says
that the acceptance probability remains constant if the
standard deviations for n = 91 are increased by a factor
of [(n + 10)/n]1/6. The procedure is repeated down to
n = 1. To conﬁrm the scaling, it suﬃces to check that the
acceptance probabilities do remain constant. Since the
standard deviations are kept constant for a given n, the
many block estimates utilized for tuning can now be av-
eraged to produce accurate estimates for the acceptance
probabilities. The results presented in Fig. 3 adequately
conﬁrm the theoretical scaling.

We have utilized the Smart Monte Carlo method to
sample the LJ38 cluster, which is a notoriously diﬃ-
cult problem, especially when a large conﬁning radius
Rc = 3.612σLJ is utilized. We have implemented SMC
in the all-particle version and run it for a number of 1.25
billion steps. The number of steps has been chosen so
that the computational cost is the same as for the simu-
lation performed in Ref. 19. There, a standard Metropo-
lis sampler has been run for 360 million sweeps, each
sweep being composed of 38 single-particle Metropolis
moves. Even though parallel tempering is utilized, the
LJ38 cluster exhibits excessively long equilibration times.
Heat capacities are overly sensitive to the lack of equili-
bration and serve as a good indicator for the progress of
the simulation. The six curves shown in Fig. 4 have been
evaluated by accumulating data over 175 million SMC
steps. They follow the same history as the curves pre-
sented in Fig. 1 of Ref. 19. Comparison shows that the
Smart Monte Carlo method is marginally faster. There-
fore, the simulation demonstrates the capability of the
SMC method to sample diﬃcult systems in the range of
tens of atoms. For more complicated potential energies,
the evaluation of the forces may be signiﬁcantly faster

than the evaluation of the potential diﬀerences for single
particle moves, making the SMC method more advanta-
geous (a non-trivial advantage is also the simplicity of
the ensuing codes and the parallelization opportunity of-
fered by the force-ﬁeld). A more elaborate discussion is
provided in the following section.

VI. CONCLUSIONS

The present study allows us to make several general
recommendations on the design of algorithms. We begin
by considering the case of a single-particle sampling strat-
egy. Such techniques can be regarded as Monte Carlo
implementations of a Gibbs sampler [2]. The latter is
an idealized technique whereby the new coordinates for a
particle are proposed directly from the conditional distri-
bution of the particle. Obviously, a Gibbs sampler has a
built-in correlation that sets a limit on the performance
of Metropolis algorithms that update only a few vari-
ables at a time. As such, we cannot indeﬁnitely improve
the eﬃciency of the algorithms by increasing the average
displacements for the trial distribution. For example, in
the case of a Lennard-Jones cluster, a particle essentially
moves in a cage created by its environment. Even if we
sample independently from the conditional distribution
of the particle, we still propose moves mostly in this cage,
which is the statistically important region.

Coping with the Gibbs correlation requires updating
more than one particle at a time. To set things in
perspective, in the limit that all particles are updated
and that the displacements are comparable with the size
of the cluster, the sampling becomes almost indepen-
dent. Unfortunately, by comparison with the Monte

Carlo Gibbs-like sampler described in the preceding para-
graph, the displacements are further decreased by the
In the limit of small displacements,
entropic eﬀects.
the Metropolis sampler is essentially diﬀusive. Conse-
α, we
quently, if the decrease in the displacements is n−
will roughly need n2α steps to cover distances comparable
to those for a Gibbs-like sampler.

Despite its intrinsically local nature, an all-particle
Metropolis sampler can be more eﬃcient than a Gibbs
sampler if the paths followed by the latter involve en-
ergetic barriers that are signiﬁcantly larger than those
characteristic of paths close to the minimum energy path.
The latter paths are available to an all-atom sampler and
more so to the Smart Monte Carlo method, whereas a
Gibbs sampler may have little choice in eﬀectuating a
rare transition other than placing the particles succes-
sively on the edges of the conditional cages. In case of
high cooperativeness for the minimum energy path [20], a
large number of particles must be correctly positioned in
order to cause a successful transition. Unfortunately, the
transition probability is exponentially decreasing with
the aforementioned number of particles, because prob-
abilities for individual particles multiply.

Which strategy must be utilized depends also on the
computational cost to implement a given step. For gen-
eral potentials, there might be the case that updating a
single particle costs the same as updating all particles. If
this is true, then it seems more advantageous to utilize
Indeed, we need
the all-particle Metropolis algorithm.
n calls to the potential function to defeat the entropic
eﬀects and span reasonable distances. The Gibbs-like
sampler makes n calls to the potential function to com-
plete a sweep. However, the ﬁrst technique is in principle
not limited by the formation of cages. Nevertheless, in
practice, the situation is more complicated because up-
dating a single particle rarely requires a full evaluation
of the potential. For example, a Lennard-Jones system
can be updated one particle at a time in such a way that
the cost for a complete sweep is twice the cost for an
all-particle move. The gain in computational eﬃciency is
large enough that the Monte Carlo implementation of the
Gibbs sampler is the default sampler for Lennard-Jones
clusters or liquids. Owing to its superiority in dealing
with the entropic eﬀects, the Smart Monte Carlo method
is a good candidate for sampling subsystems containing
tens or hundreds of atoms.

To summarize, the ideal sampling strategy would di-
vide the system in small subsystems (which may overlap)
in a way that optimally compromises on the following re-
i) the correlation between the subsystems
quirements:
be as small as possible and ii) the dimensionality of each
subsystem be as small as possible. An eﬃcient implemen-
tation may require the utilization of variables other than
Cartesian ones, as the subsystems may become nearly un-
correlated in such coordinates. Each subsystem, which
is described by strongly correlated degrees of freedom,
is sampled utilizing an all-particle strategy or the Smart
Monte Carlo method, if forces are easily computable. Fi-

10

nally, if techniques of reducing the dimensionality of the
system are not available, then the designer should intro-
duce tuning parameters that can arbitrarily increase the
similarity between the direct and reverse sampling dis-
tributions. A case is constituted by the replica-exchange
techniques. If tuning parameters are not provided, then
one unavoidably ends up with an algorithm that sud-
denly stops working for systems of suﬃciently large di-
mensionality, owing to the severe exponential decay in
the acceptance probability.

Acknowledgments

This work was supported in part by the National Sci-
ence Foundation Grant No. CHE-0345280 and by the
Director, Oﬃce of Science, Oﬃce of Basic Energy Sci-
ences, Chemical Sciences, Geosciences, and Biosciences
Division, U.S. Department of Energy under Contract No.
DE-AC02-05CH11231. NERSC has provided the com-
puting resources.

APPENDIX A: LOW-VARIANCE ESTIMATORS
FOR STRONGLY ASYMMETRIC
DISTRIBUTIONS

Sometimes, a sampling problem appears naturally in

the form

dxdyf (x, y)
dxdyπ(x, y)

,

R R
R R

(A1)

with f (x, y) symmetric and π(x, y) positive. The dis-
tribution at the denominator is assumed unnormal-
ized for the sake of generality. The default estimator
f (x, y)/π(x, y) might register large contributions to the
variance arising from the pairs (x, y) where π(x, y) is
small. This is apparent from the formula for the second
moment, which is

dxdyf (x, y)2 /π(x, y)
dxdyπ(x, y)

.

(A2)

R R

R R

Nevertheless, for strongly asymmetric sampling distribu-
tions, π(y, x) might be large for such pairs.
It turns
out that it is possible to construct estimators for which
the variance behaves as if the sampling distribution were
replaced by max
and which incur little
penalty over the default estimator if the sampling distri-
bution is not strongly asymmetric.

π(x, y), π(y, x)
}

{

Recall

X(x, y) = log [π(y, x)/π(x, y)]

(A3)

and let ǫ

0. Deﬁne the sets

≥

Aǫ =

(x, y) : X(x, y) <
{

ǫ

−

}

(A4)

Z Z

Z Z

+

+

Z Z

Z Z

Z Z

and

Notice that

−

Z Z

Bǫ =

(x, y) :
{

X(x, y)
|

| ≤

.

ǫ

}

(A5)

IAc

ǫ (x, y) = IAǫ (y, x) + IBǫ (x, y),

(A6)

a property that stems from the antisymmetry relation
X(y, x). Therefore, we can write
X(x, y) =

dxdyf (x, y) =

dxdyf (x, y)IAǫ (x, y)

dxdyf (x, y)IAǫ (y, x) (A7)

dxdyf (x, y)IBǫ (x, y).

The symmetry of f (x, y) implies that the ﬁrst two terms
of the right-hand side are equal, so that

dxdyf (x, y) = 2

dxdyf (x, y)IAǫ (x, y)

Z Z

+

dxdyf (x, y)IBǫ (x, y). (A8)

This leads to the estimating function

f (x, y)
π(x, y) × 


0,
1,
2,

if X(x, y) > ǫ,
X(x, y)
if
|
if X(x, y) <

| ≤

ǫ,
ǫ.

−

(A9)



The estimator given by Eq. (39) is obtained by set-
x) and f (x, y) equal to
ting π(x, y) equal to ρ(x)T (y
|
min
. The partition function estimator
{
given by Eq. (12) of Ref. 12 is obtained by setting π(x, y)
equal to ρi(x)ρj (y) and f (x, y) equal to ρj(x)ρj (y).

π(x, y), π(y, x)
}

We claim that Eq. (A9) must be utilized with an ǫ
as small as possible, ideally zero, if not for the numer-
ical issues mentioned in Section II. For ǫ = 0, we have
π(x, y)
π(y, x) wherever the estimating function is not
zero. Therefore, the square of the estimating function is
smaller than

≥

2f (x, y)2

π(x, y) max

π(x, y), π(y, x)
}

{

× 


the expected value of which is



0,
1,
2,

if X(x, y) > 0,
X(x, y)
if
|
|
if X(x, y) < 0,

= 0,

(A10)

dxdyf (x, y)2 /max
{

π(x, y), π(y, x)
}

.

(A11)

dxdyπ(x, y)

2

R R

R R

This value can be at most twice larger than the second
moment of the default estimator. However, it may actu-
ally be signiﬁcantly smaller for strongly asymmetric dis-
tributions, for which π(y, x) may often be substantially
larger than π(x, y). Notice that Eq. (35) is still valid and
1/2. Thus, the penalty of at most
implies P (X
2 has a statistical origin. For nearly symmetric distribu-
tions, we may end up utilizing only half (but not less) of
the total number of points sampled.

0)

≥

≤

11

APPENDIX B: PROOF OF EQ. (37) BY
CRAM´ER’S THEOREM

As otherwise the statement of Eq. (37) is trivially true,
we shall assume that the set of points for which the de-
tailed balance condition is violated

ρ(x)T (y

x)
|

y)
= ρ(y)T (x
|

(B1)

has nonvanishing probability. The rejection mechanism
in the Metropolis-Hastings method has the purpose of
correcting the lack of detailed balance of the trial dis-
tribution. Owing to Eq. (B1), we have D1 > 0. Recall
Eq. (8) and consider the moment generating function

φ(θ) = E exp(θX) =

π(x, y)

Z Z

Observe that the ﬁrst derivative

π(y, x)
π(x, y)

θ

(cid:21)

(cid:20)

dxdy.

(B2)

φ′(θ) =

π(x, y)

π(y, x)
π(x, y)

θ

log

π(y, x)
π(x, y)

dxdy

(cid:20)

(cid:20)

(cid:21)

−

Z Z

In fact φ′(0) =

(cid:21)
is deﬁned on the interval [0, 1].
D1,
φ′(1) = D1, and φ′(1/2) = 0. The last relation follows
easily by symmetry. If a is such that
D1 < a < D1,
then the equation a = φ′(θa)/φ(θa) has a unique solution
in the interval (0, 1), owing to the fact that φ′(θ)/φ(θ) is
increasing. The reader can look at the moment generat-
ing function and see that the derivative of φ′(θ)/φ(θ) is a
“heat capacity.” Moreover, Eq. (B1) demands that this
heat capacity be continuous and non-zero on (
D1, D1).
From the implicit mapping theorem, it follows that the
solution θa is continuous in a. Of note is that θ0 = 1/2.
According to Cram´er’s theorem, which is stated at the

−

−

end of the appendix, we have

log P (Sn/n

a)

aθa + log φ(θa),

≥

→ −

for all a

D1, D1). If a > 0, then, by Eq. (36),

1
n

lim
n
→∞

(
∈
−
2P (Sn/n

0)

≥

≥ An ≥

2P (Sn/n

a),

≥

from where it follows that

log φ(1/2)

lim inf
n
→∞

≥

≥
1
n

log

1
n

log

An

lim sup
n

→∞

An ≥ −

aθa + log φ(θa).

Letting a
ity, we obtain the desired result, namely

0, by means of the aforementioned continu-

→

1
n

lim
n
→∞

log

An = log φ(1/2) =

−

D1/2 < 0.

(B3)

1
2

To see that D1/2 > 0, observe that the function
f (x) = x1/2 is concave. Jensen’s inequality as applied to
Eq. (B2) produces φ(1/2)
1, with equality if and only
≤
if π(x, y) = π(y, x) holds almost surely, which is contrary

6
to our assumption. Nevertheless, if the last equality holds
almost surely, then Eq. (37) is trivially true, since
An = 1
and D1/2 = 0.
To help the reader understand the proof, we give the

statement of Cram´er’s large deviation theorem [11].

n

{

≥

Xn; n

Theorem 1 Let
be a sequence of indepen-
1
}
dent and identically distributed random variables and let
i=1 Xi be the partial sums. Let µ = EXi and
Sn =
pick a > µ. Suppose φ(θ) = E exp(θXi) is deﬁned on
the interval [0, θ+) and that the distribution of Xi is not
(0, θ+) so
a point mass. Assume also that there is θa ∈
that a = φ′(θa)/φ(θa). Then as n

P

,
→ ∞

n−

1 log P (Sn ≥

na)

→ −

aθa + log φ(θa).

APPENDIX C: PROOF OF EQ. (49)

We ﬁrst consider the case d = 1. Write D1/2(θ) in the
2 log S(θ). Following the substitution

form D1/2(θ) =
y = x + σz, S(θ) is given by the expression

−

S(θ) =

1
Q(β)√2π

dx

dze−

β[V (x)+V (x+zσ)]/2

R
Z
e−[z+θβσV

R
Z
(x)]2
/4e−[z

′

−

×

θβσV

′

(x+zσ)]2

/4. (C1)

By interchanging the order of integration and performing
the substitution x′ = x + zσ/2, S(θ) is seen to equal

1
Q(β)√2π

Z
e−[z+θβσV

dx

R

′

(x

−

×

R
Z
zσ/2)]2

dze−

β
2 [V (x

−

zσ/2)+V (x+zσ/2)]

/4e−[z

−

θβσV

′

(x+zσ/2)]2

/4. (C2)

The integrand in this equation is symmetric with respect
to the variable z. Rearrangement of the exponent pro-
duces

S(θ) =

1
Q(β)

R

Z
β[V (x

e−
eθβzσ[V
σ
θ

β

2

2

e−

×
×

×

βV (x) 1

dxe−

√2π

Z
zσ/2)+V (x+zσ/2)

R

−

−
′
(x+zσ/2)

′

V

(x

−

−

zσ/2)]/2

2

z

/2

dze−

2V (x)]/2

2

′

[V

(x

2

′

zσ/2)

+V

(x+zσ/2)

−

2

]/4.

(C3)

The reason we write S(θ) in this form is that σ appears
either in the form σ2 or as a product zσ. Since the odd
moments of the Gaussian measure vanish, it follows that
the Taylor expansion at σ = 0 contains only terms of the
form σ2k. Evaluating the ﬁrst two nonvanishing terms
explicitly, we obtain as an intermediate result

S(θ) = 1

1
Q(β)

−

dxe−

βV (x)

V ′(x)2

θ2β2σ2
2

(cid:20)

R
Z
βσ2
2

+

1
4 −

θ

V ′′(x)

+ O(σ4). (C4)

(cid:18)

(cid:19)

(cid:21)

12

dxe−

βV (x)V ′′(x) =

dxe−

βV (x)βV ′(x)2.

Integration by parts produces

R

Z

Eq. (C4) reduces to

β2σ2(θ

1/2)2

S(θ) = 1

−

−
2Q(β)

R

Z

R

Z

dxe−

βV (x)V ′(x)2 + O(σ4).

(C5)

Since D1/2(θ) =

2 log S(θ), we obtain

−

−

D1/2(θ) = β2σ2(θ

1/2)2

V ′(x)2

β + O(σ4),

(C6)

(cid:10)

(cid:11)

where we have utilized the bra-ket notation to denote
thermodynamic averages for the Fisher entropy. For the
d-dimensional case, the reader can reduce the problem to
the form speciﬁed by Eq. (C3), perform the expansion,
and then notice that all averages involving cross terms
zizj with i
= j vanish. One eventually obtains the form
speciﬁed by Eq. (49).

For θ = 1/2, we can actually show that the decay of
the entropy divergence is faster than the quartic rate im-
6), but
plied by Eq. (C6). The decay is as fast as O(
k
in general not faster. Expensive yet straightforward cal-
culations show that the quartic term is given by the ther-
modynamic average of the sum over 1
d of the
terms

i, j

≤

≤

σ

k

i σ2
σ2
j
64

−

−

3β∂iijj V (x) + β2 [

2∂ijV (x)∂ij V (x)

−
4∂iV (x)∂ijj V (x)]

+∂iiV (x)∂jj V (x)

(cid:8)

−
2β3[∂iV (x)]2∂jj V (x) + β4[∂iV (x)]2[∂jV (x)]2

.

(C7)

Again, we utilize integration by parts to replace the in-
tegrands with equivalent ones. The goal is to remove the
integrands containing high order derivatives. By means
of the identities

(cid:9)

∂iijj V (x)
h
= β2

iβ = β
[∂iV (x)]2∂jj V (x)

∂iV (x)∂ijj V (x)
h

β

iβ

β −

∂iiV (x)∂jj V (x)
h

iβ ,

(cid:11)
we can replace Eq. (C7) with

(cid:10)

2β2∂ij V (x)∂ij V (x)

i σ2
σ2
j
64
+3β3[∂iV (x)]2∂jj V (x)

(cid:8)

−

2β2∂iiV (x)∂jj V (x)

−
β4[∂iV (x)]2[∂jV (x)]2

.

Furthermore, the equality

(cid:9)

[∂iV (x)]2∂jj V (x)

[∂iV (x)]2[∂jV (x)]2

β

β = β
2
(cid:11)
−

∂iV (x)∂ij V (x)∂j V (x)
h

(cid:10)

(cid:11)
iβ

(cid:10)

leads to the equivalent integrand

β2∂ijV (x)∂ij V (x)

i σ2
σ2
j
32
+β3[∂iV (x)]2∂jj V (x)

(cid:8)

−

β2∂iiV (x)∂jj V (x)

−
β3∂iV (x)∂ij V (x)∂j V (x)

.

(cid:9)

6
Letting φ(x) = exp[

to show that the term of order
1

d of the terms

i, j

k

k

βV (x)/2], it is straightforward
4 is the sum over all

σ

−

≤

≤
i σ2
σ2
j
8Q(β)

Rd

Z

(cid:8)

∂iiφ(x)∂jj φ(x)

[∂ij φ(x)]2

dx.

(C8)

−

However, the terms are equal to zero because of the iden-
tities

(cid:9)

∂iiφ(x)∂jj φ(x)dx =

φ(x)∂iijj φ(x)dx

Rd

Z

Rd

Z

Rd

Z

=

∂ij φ(x)∂ij φ(x)dx,

which follow by integration by parts.

13

σ

We conclude that, if θ = 1/2, the dominant term in
the expansion of the divergence D1/2(θ) in powers of σ
6). For the quadratic potential V (x) = 2x2 and
is O(
k
for β = 1, numerical results show that D1/2(θ)/σ6
1,
as σ
0. Thus, in general, we cannot hope for a better
→
scaling.

→

k

[1] M. Kalos and P. Whitlock, Monte Carlo Methods (Wiley-

[11] R. Durrett, Probability: Theory and Examples, 2nd ed.

Interscience, New York, 1986).

(Duxbury, New York, 1996), Th. (9.5), p. 74.

[2] J. S. Liu, Monte Carlo Strategies in Scientiﬁc Computing

[12] S. Chempath, C. Predescu, and A. T. Bell, J. Chem.

(Springer-Verlag, New York, 2001).

[3] C. Predescu, Phys. Rev. E 71, 045701(R) (2005); Phys.

Phys. 124, 234101 (2006).
III

Miller

F.

[13] T.

and

C.

Predescu,

Rev. E 71, 046707 (2005).

[4] A. R´enyi, On Measures of Entropy And Information; in
Proceedings of the 4th Berkeley Symposium on Mathe-
matical Statistics and Probability, ed. J. Neyman, (Uni-
versity of California Press, Berkeley, 1961) pp. 547–561.
[5] G. O. Roberts, A. Gelman, W. R. Gilks, Ann. Appl.

http://arxiv.org/abs/cond-mat/0610597.

[14] C. J. Geyer, in Computing Science and Statistics: Pro-
ceedings of the 23rd Symposium on the Interface, ed.
E. M. Keramigas (Interface Foundation, Fairfax, 1991);
p. 156.

[15] H. Fukunishi, O. Watanabe, and S. Takada, J. Chem.

[6] G. O. Roberts and J. S. Rosenthal, J. R. Statist. Soc. B

[16] C. Pangali, M. Rao, and B. J. Berne, Chem. Phys. Lett.

Phys. 116, 9058 (2002).

55, 413 (1978).

[7] O. F. Christensen, G. O. Roberts, and J. S. Rosenthal,

J. R. Statist. Soc. B 67, 253 (2005).

[17] M. Rao and B. J. Berne, J. Chem. Phys. 71, 129 (1979).
[18] A. D. Kennedy and B. Pendleton, Nucl. Phys. B, suppl.,

[8] D. A. Kofke, J. Chem. Phys. 117, 6911 (2002); J. Chem.

20, 118 (1991).

[9] C. Predescu, M. Predescu, and C. V. Ciobanu, J. Chem.

J. Chem. Phys. 122, 154305 (2005).

[19] C. Predescu, P. A. Frantsuzov, and V. A. Mandelshtam,

[20] S. A. Trygubenko and D. J. Wales, J. Chem. Phys. 121,

[10] P. J. Rossky, J. D. Doll and H. Friedman, J. Chem. Phys.

6689 (2004).

Probab. 7, 110 (1997).

60, 255 (1998).

Phys. 120, 10852 (2004).

Phys. 120, 4119 (2004).

69, 4628 (1978).


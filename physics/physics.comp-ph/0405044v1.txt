4
0
0
2
 
y
a
M
 
0
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
4
4
0
5
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Least Dependent Component Analysis Based on Mutual Information

Harald St¨ogbauer, Alexander Kraskov, Sergey A. Astakhov, and Peter Grassberger
John-von-Neumann Institute for Computing, Forschungszentrum J¨ulich, D-52425 J¨ulich, Germany
(Dated: February 21, 2014)

We propose to use precise estimators of mutual information (MI) to ﬁnd least dependent compo-
nents in a linearly mixed signal. On the one hand this seems to lead to better blind source separation
than with any other presently available algorithm. On the other hand it has the advantage, com-
pared to other implementations of ‘independent’ component analysis (ICA) some of which are based
on crude approximations for MI, that the numerical values of the MI can be used for:
(i) estimating residual dependencies between the output components;
(ii) estimating the reliability of the output, by comparing the pairwise MIs with those of re-mixed
components;
(iii) clustering the output according to the residual interdependencies.
For the MI estimator we use a recently proposed k-nearest neighbor based algorithm. For time
sequences we combine this with delay embedding, in order to take into account non-trivial time
correlations. After several tests with artiﬁcial data, we apply the resulting MILCA (Mutual Infor-
mation based Least dependent Component Analysis) algorithm to a real-world dataset, the ECG of
a pregnant woman.

I.

INTRODUCTION

‘Independent’ component analysis (ICA) is a statistical
method for transforming an observed multi-component
data set x(t) = (x1(t), x2(t), ..., xn(t)) into components
that are statistically as independent from each other as
possible [1]. In theoretical analyses one usually assumes a
certain model for the data for which a decomposition into
completely independent components is possible, but in
real life applications the latter will in general not be true.
Depending on the assumed structure of the data, one
typically makes a parametrized guess about how they can
be decomposed (linearly or not, using only equal times
or using also delayed superpositions, etc.) and then ﬁxes
the parameters by minimizing some similarity measure
between the output components.

Using mutual information (MI) would be the most nat-
ural way to solve this problem. But estimating MI from
statistical samples is not easy. Most existing algorithms
are either very slow or very crude. Also, the more so-
phisticated estimates usually do not depend smoothly on
transformations of the data, which slows down minimum
searches. In the ICA literature mostly very crude approx-
imations of MI are used, or MI is completely disregarded
in favor of diﬀerent approaches [1, 2]. In particular, we
are aware of only very few attempts to pay attention to
the actual values of the similarities / (in)dependences
obtained by ICA. Of course it has been recognized sev-
eral times that even the best decomposition with a given
linear and instantaneous) may
class of algorithms (e.g.
not lead to strictly independent components, but then
typically it is proposed to use a decomposition algorithm
within this class which is diﬀerent from that for truly
independent sources [3, 4]. An exception is the ‘multidi-
mensional ICA’ of [5] where the author points out that
one can use standard decomposition algorithms even in
case of non-zero dependencies, but also there most of the
attention is payed to whether components are indepen-

dent, but not on how dependent they are. The latter can
be useful for clustering the output, but also for reliability
and stability testing: A blind source separation into inde-
pendent components will be the more robust, the deeper
are the minima of the dependences. In [6, 7, 8] such relia-
bility tests have been proposed based on resampling and
noise injection. We believe that looking at the depen-
dence landscape is more direct and conceptually simpler.
In the present paper we propose to use a recently intro-
duced MI estimator based on k-nearest neighbor statis-
tics [9]. It resembles the Vasicek estimator [10] for diﬀer-
ential entropies which has been applied recently to ICA
[11, 12] and which is also based on k-nearest neighbor
statistics. But while the Vasicek estimator exists only for
1-dimensional distributions and can not therefore be used
to estimate dependencies via MI, our estimator is based
on the Kozachenko-Leonenko [13] estimator for diﬀeren-
tial entropies and works in any dimension. In addition,
it seems to give the most precise blind source separation
algorithm for 2-d distributions known at present.

Throughout the paper we will only discuss the simplest
case of linear superpositions. While MI can be applied in
principle also to nonlinear mixtures, this would be much
more diﬃcult.

The paper is organized as follows. In Sec. II we recall
basic properties of MI and present the MI estimator of
[9]. The basic version of MILCA is described in Sec. III
where we also give ﬁrst applications to toy models, and
where we will also discuss the reliability of the decompo-
sitions. In Sec. IV we deal with the case where only some
groups of output components are independent, with non-
zero interdependencies within the groups. In this case it
is natural to cluster the components. We propose to use
again MI for that purpose, in the form of the mutual
information based clustering (MIC) algorithm presented
recently in [14]. In Sec. V we discuss how MILCA (and
other ICA algorithms) can be combined with time delay
embedding, in order to take into account non-trivial time

structure (in case the data to be decomposed form a time
series). A thorough discussion of our method and of its
relations to previous work is given in Sec. VI. Conclusions
are drawn in the last section, Sec. VII.

II. MUTUAL INFORMATION

A. General Properties of MI

Assume that X and Y are continuous random variables
with joint density µ(x, y) and marginal densities µx(x) =

dyµ(x, y) and µy(y). Then MI is deﬁned as [15]

R

I(X, Y ) =

dxdy µ(x, y) ln

Z Z
In terms of the diﬀerential entropies

µ(x, y)
µx(x)µy(y)

.

(1)

H(X) = −

dxµx(x) ln µx(x),

H(Y ) = −

dyµy(y) ln µy(y),

Z

Z

ZZ

and

H(X, Y ) = −

dxdy µ(x, y) ln µ(x, y)

(4)

it can be written as I(X, Y ) = H(X) + H(Y ) − H(X, Y ).
The most important property of MI is that it is al-
ways non-negative, and is zero if and only if X and Y
are independent. Another important feature of MI is
its invariance under homeomorphisms of X and Y .
If
X ′ = F (X) and Y ′ = G(Y ) are smooth and uniquely
invertible maps, then

′

′

I(X

, Y

) = I(X, Y ).

(5)

Notice that this is not the case for diﬀerential entropies.
Just as Gaussian distributions maximize the diﬀerential
entropy, giving thereby an upper bound on the entropy
in terms of the variance of the distribution, Gaussians
minimize MI [9]. This gives a lower bound on MI in
terms of the correlation coeﬃcient

r =

hX.Y i
[hX 2ihY 2i]1/2 ,

I(X, Y ) ≥ −

ln(1 − r2) .

1
2

This might suggest that MI can be decomposed into a
‘linear’ part [the r.h.s. of Eq.(7)] plus a non-linear part.
While such a decomposition is of course always possible,
it is in general not useful. For example, it would also
suggest that the minimum of MI under linear transfor-
mations (X ′, Y ′) = A (X, Y ) is always reached when X ′

(2)

(3)

(6)

(7)

2

and Y ′ are linearly uncorrelated (in which case r = 0 and
the r.h.s. of Eq.(7) is zero). But it is easy to give coun-
terexamples for which this is not true (see appendix).

This is important for MILCA, since it is standard prac-
tice in ICA to make ﬁrst a “prewhitening” (principle
component transformation plus rescaling, so that the co-
variance matrix is isotropic), and to restrict the actual
minimization of the contrast function to pure rotations
[1]. If one is sure that the sources are really independent,
this is justiﬁed: For the correct sources both MIs and
covariances are zero. But it is not justiﬁed, if there are
no strictly independent sources and we want to ﬁnd the
least dependent sources.

For any number M of random variables, the MI (or

‘redundancy’, as it is often called) is deﬁned as

M

m=1
X

I(X1, X2, . . . XM ) =

H(Xm) − H(X1, X2, . . . XM ).

(8)
Notice that this is the appropriate deﬁnition for ICA or
MILCA, since it is this diﬀerence which one wants to
minimize. In the literature outside the ICA community
usually a diﬀerent construct is called MI [15], but we shall
in the following only use Eq.(8).

The M -dimensional MI shares with I(X, Y ) the invari-
ance under homeomorphisms for each Xm, and the fact
that it is bounded by the value obtained for a Gaussian
with the same covariance matrix [9]. The next important
property is the grouping property [9]

I(X, Y, Z) = I((X, Y ), Z) + I(X, Y ) .

(9)

Here, I((X, Y ), Z) is the MI between the two variables
Z and (X, Y ), and we have used the fact that a ran-
dom variable need not be a scalar. Indeed, anything we
said so far holds also if X, Y, . . . are multicomponent ran-
dom variables (except Eq.(7) which has to be suitably
modiﬁed). Therefore, if we have more than 3 random
variables, Eq.(9) can be iterated. For any set of random
variables and any hierarchical clustering of this set into
disjoint groups, the total MI can be hierarchically de-
composed into MIs between groups and MIs within each
group. This will become important in Sec.V where we
discuss clustering based on MI.

Intuitively, one might expect that I(X, Y, Z) = 0 if
I(X, Y ) = I(X, Z) = I(Y, Z) = 0. Pairwise strict inde-
pendence would then imply global independence. That
this is not true is demonstrated in the appendix with
a simple counter example.
It becomes important for
chaotic deterministic systems. If x1, x2, . . . xN is a uni-
variate signal produced by a strange attractor with di-
mension d, then any d-tuple of consecutive xt values will
be weakly dependent, while any m-tuple with m > d will
be strongly dependent.

The last property to be discussed here is related to
homeomorphisms involving a pair of variables (X, Y ), i.e.
(X ′, Y ′) = F (X, Y ). Using the grouping property and
the invariance under homeomorphisms of a single variable

we obtain [9]

H(Y ) and H(X, Y ) separately and using

′

′

′

′

, Y

, Y

I(X

, Z, . . .) = I(X, Y, Z, . . .) + [I(X

) − I(X, Y )] .
(10)
This is important if we want to minimize the MI with re-
spect to linear transformations. Since any such transfor-
mation in M dimensions can be factorized into pairwise
transformations, this means that we only have to com-
pute pairwise MIs for the minimization. To ﬁnd the ac-
tual value of the minimum, we have of course to perform
one calculation in all M dimensions. We also have to
estimate higher order MIs directly, if we want to use the
method of Sec. V.A with embedding dimension m > 2.

B. MI Estimation

Assume that one has a set of N bivariate measure-
ments, (xi, yi), i = 1, . . . N , which are assumed to be iid
(independent identically distributed) realizations of the
random variable Z = (X, Y ). Our task is to estimate
MI, with or without explicit estimation of the unknown
densities µ(x, y), µx(x), and µy(y).

Two classes of estimators were given in [9].

In con-
trast to other estimators based on cumulant expansions,
entropy maximalization, parameterizations of the densi-
ties, kernel density estimators or binnings (for a review
of these methods see [9]), the algorithms proposed in [9]
are based on entropy estimates from k-nearest neighbor
distances. This implies that they are data eﬃcient (with
k = 1 we resolve structures down to the smallest possi-
ble scales), adaptive (the resolution is higher where data
are more numerous), and have minimal bias. Numer-
ically, they seem to become exact for independent dis-
tributions, i.e. the estimators are completely unbiased
(and therefore vanish except for statistical ﬂuctuations)
if µ(x, y) = µ(x)µ(y). This was found for all tested distri-
butions and for all dimensions of x and y. It is of course
particularly useful for an application where we just want
to test for independence.

In the following we shall discuss only one of these
two classes, the one based on rectangular neighborhoods
called ˆI (2)(X, Y ) in [9].

C. Formal Developments

We will start from the Kozachenko-Leonenko estimate

for Shannon entropy [9, 13, 16, 17, 18]:

ˆH(X) = −ψ(k) + ψ(N ) + log cd +

log ǫ(i)

(11)

d
N

N

i=1
X

where ψ(x) is the digamma function, ǫ(i) is twice the dis-
tance from xi to its k-th neighbor, d is the dimension of x
and cd is the volume of the d-dimensional unit ball. Mu-
tual information could be obtained by estimating H(X),

3

I(X, Y ) = H(X) + H(Y ) − H(X, Y ) .

(12)

But for any ﬁxed k, the distance to the k-th neighbor in
the joint space will be larger than the distances to the
neighbors in the marginal spaces. Since the bias from the
non-uniformity of the density depends of course on these
distances, the biases in ˆH(X), ˆH(Y ), and in ˆH(X, Y )
would not cancel.

To avoid this, we notice that Eq.(11) holds for any
value of k, and that we do not have to choose a ﬁxed
k when estimating the marginal entropies (this idea was
used ﬁrst, somewhat less systematically, in [19]). So let us
denote by ǫx(i) and ǫy(i) the edge lengths of the smallest
rectangle around point i containing k neighbors, and let
nx(i) and ny(i) (the number of points with ||xi − xj|| ≤
ǫx(i)/2 and ||yi − yj|| ≤ ǫy(i)/2) as the new number of
neighbors in the marginal space. The estimate for MI is
then:

ˆI(X, Y ) = ψ(k) − 1/k − hψ(nx) + ψ(ny)i + ψ(N ). (13)

We denote by h. . .i averages both over all i ∈ [1, . . . N ]
and over all realizations of the random samples.

Here we will show results of ˆI(X, Y ) for Gaussian dis-
tributions (cf. Fig. 1). Let X and Y be Gaussian signals
with zero mean and unit variance, and with covariance
r. In this case I(X, Y ) is known exactly,

IGauss(X, Y ) = −

log(1 − r2)

(14)

1
2

Apart from the fact that indeed ˆI(X, Y )−IGauss(X, Y ) →
0 for N → ∞, the most conspicuous feature is that the
systematic error is compatible with zero for r = 0. This is
a property which makes the estimator particular interest-
ing for ICA because there we are looking for uncorrelated
signals. For non-Gaussian signals, our estimator still has
a smaller systematic error than other estimators in the
literature [9].

Using the same arguments for n random variables
X1, X2, . . . Xm, the MI estimate for I(X1, X2, . . . Xm),
is [9]:

ˆI(X1, X2, . . . Xm) = ψ(k) − (m − 1)/k + (m − 1)ψ(N )

− hψ(nx1 ) + ψ(nx2 ) + . . . ψ(nxm)i

D. Practical considerations

By choosing proper values for k, the algorithm allows
to minimize either the statistical or the systematic errors.
The higher is k, the lower is the statistical error of ˆI.
The systematic error shows exactly the opposite behav-
ior. Thus, to keep the balance between these two errors,
the best choice for k would lie in the middle range. But

r = 0.9
r = 0.6
r = 0.3
r = 0.0

)

2

r
-
1
(
 
g
o
l
 
2
/
1
 
 

+

 
 
)

Y
X

,

(
I

 0.01

 0.005

 0

-0.005

-0.01

-0.015

 0

 0.01

 0.02

 0.03

 0.04

 0.05

1 / N

FIG. 1: Estimates of average values of I(X, Y )
Iexact(X, Y )
for Gaussian signals with unit variance and covariances r =
0.9, 0.6, 0.3, and 0.0 (from top to bottom), plotted against
1/N . In all cases k = 1. The number of realizations is > 2
106 for N <= 1000, and decreases to
Error bars are smaller than the sizes of the symbols.

×
105 for N = 40, 000.

−

≈

for some cases it makes sense to deviate from this, e.g.
when we want to ﬁnd most independent signal sources.
There the true values of the MI are small, and thus also
the systematic errors for all k. In this case it is better
to use large k in order to reduce statistical errors. On
the other hand, when the data ﬁles are very long we do
not have to worry about statistical errors and we should
choose k small.

Most of the CPU time for estimating MI with our new
estimator is used for neighbor searching. In [9] we pre-
sented three implementations which ranged from very
simple but slow to sophisticated and fast.
In the fol-
lowing we shall always use the fastest implementation
which uses grids to achieve a CPU time ∼ N ln N for N
points. We will not use rank ordering (as also discussed
in [9]), but we will add small Gaussian jiggles (amplitude
≈ 10−8) to all measured values in order to break any
degeneracies due to quantization in the analog-to-digital
conversion [9].

III. MILCA WITHOUT USING TEMPORAL
STRUCTURES

A. Basic Algorithm

In this Subsection we will show how the linear instan-
taneous ICA problem is solved using the new MI estima-
tor. We will apply this then to several artiﬁcial data sets
which are constructed by superimposing known indepen-
dent sources, and we will compare the results with those
from several other ICA algorithms.

In the simplest case x(t) is an instantaneous lin-
ear superposition of n independent sources s(t) =

4

(s1(t), s2(t), ..., sn(t)),

x(t) = A s(t) ,

(15)

where A is a non-singular n × n ‘mixing matrix’. This
means that the number of sources is equal to the number
of measured components. In this case, we know that a
decomposition into independent components is possible,
since the inverse transformation

ˆs(t) = Wx(t) with W = A

(16)

−1

does exactly this. If Eq.(15) does not hold then no de-
composition into strictly independent components is pos-
sible by a linear transformation like Eq.(16), but one can
still search for least dependent components.

But even if Eq.(15) does hold, the problem of blind
source separation (BSS), i.e., of ﬁnding the matrix W
without explicitly knowing A, is not trivial. Basically,
it requires that x is such that the components of any
superposition s′ = W′x with W′ 6= W are not indepen-
dent. Since linear combinations of Gaussian variables are
also Gaussian, BSS is possible only if the sources are not
Gaussian. Otherwise, any rotation (orthogonal transfor-
mation) s′ = Rs would again lead to independent com-
ponents, and the original sources s could not be uniquely
recovered. Since any ICA algorithm will ﬁnd a more or
less meaningful solution, we need a reliability test for the
obtained components. This is given in subsection C.

As a ﬁrst step, the matrix W is usually decomposed
into two factors, W = RV, where the prewhitening V
transforms the covariance matrix into C′ = VCVT = 1,
and R is a pure rotation. Prewhitening is just a princi-
pal component analysis (PCA) together with a rescaling.
The ICA problem proper reduces then to ﬁnding a suit-
able rotation for the prewhitened data.

The motivation for this is that any reasonable contrast
function used for the ICA will give least dependent com-
ponents which are also uncorrelated. In Sec. II we have
seen that this is not always the case, but that it is true
whenever the components are really independent. One
can take now several diﬀerent attitudes. The most rad-
ical is to abandon prewhitening altogether (for diﬀerent
reasons for not to use prewhitening, see [20]). But this
slows down the algorithm considerably. Also, prewhiten-
ing can be detrimental only when there are residual de-
pendencies between the optimal components, and it is
not clear what is the signiﬁcance of such components. In
the following we shall always use prewhitening unless we
say explicitly the opposite. We shall always assume that
the prewhitening step has already been done, and we will
restrict the proper ICA (or rather LCA) transformations
to pure rotations. As a third alternative one could ﬁrst
use prewhitening, but try at the end whether some non-
orthogonal transformations improve the results further.
We have not yet studied this strategy.

The aim of ICA is now to minimize I(X1 . . . Xn) un-
der a pure rotation R. Any rotation can be represented
as a product of rotations which act only in some 2 × 2

pdfs
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
mean

x
e
d
n

i
 

e
c
n
a
m
r
o

f
r
e
p

0.25

0.2

0.15

0.1

0.05

0

FastICA
4.4
5.8
2.3
6.4
4.9
3.6
1.8
5.1
10.0
6.0
5.8
11.0
3.9
5.3
4.4
3.7
19.0
5.8
6.1

Jade
3.7
4.1
1.9
6.1
3.9
2.7
1.4
4.1
6.8
4.5
4.4
8.3
2.8
3.9
3.3
2.9
15.3
4.3
4.7

Imax
1.8
3.4
2.0
6.9
3.2
1.0
0.6
3.1
7.8
50.6
4.2
9.4
3.9
32.1
4.1
8.2
43.3
5.9
10.6

KCCA
3.7
3.7
2.7
7.1
1.7
1.7
1.5
4.6
8.3
1.4
3.2
4.9
6.2
7.1
6.3
3.6
5.2
4.1
4.3

KGV
3.0
2.9
2.4
5.7
1.5
1.5
1.4
3.6
6.4

1.3
2.8
3.8
4.7
3.0
4.5
2.8
3.6
3.7
3.3

RADICAL
2.1
2.7
1.2
5.3
0.9
1.0
0.6
3.7
8.3
0.8
2.7
4.2
1.0
1.8
3.4
1.1
2.3
3.2
2.6

MILCA
2.7
2.9
1.5
7.0
0.9
0.9
0.6
3.4
7.9
0.7
2.4
4.1
1.0
2.0
3.4
1.6
2.9
3.5
2.7

5

MILCA(augmented)
2.4
2.5
1.0
4.3
1.0
0.9
0.6
3.3
8.0
0.8
2.3
3.3
0.8
1.6
2.9

1.2
1.9
2.7
2.3

TABLE I: Performance indices (multiplied by 100) for two-component blind source separation, test problem (A). The results in
the ﬁrst 6 columns (FastICA, Jade, Imax, KCCA, KGV, and RADICAL) are taken from Ref.[12], where also references to these
algorithms are given and where the probability distribution functions (pdfs) ‘a’ - ‘r’ are deﬁned. The last two columns show
the results of MILCA, ﬁrst in its simplest version (column 7) and then with data augmentation as proposed in [12] (column 8).
Each performance index is an average over 100 replicas, each replica consisting of 1000 pairs of numbers drawn randomly from
the pdfs. For MILCA we used k = 10, and we ﬁtted ˆI(φ) by Fourier sums with 3 (MILCA) and 5 terms (augmented MILCA),
respectively.

n
o

i
t

a
m
r
o

f

n

i
 
l

a
u

t

u
m

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

n
o
i
t
a
m
r
o
f
n
i
 
l
a
u
t
u
m

0.88

0.86

0.84

0.82

0.8

0.78

0.76

0.74

0.72

0.7

1

2

3

4

5

6

7

F

a

J

A

T

D

s

tI

C

A

D

E

S

E

P

M

I

L

C

A

F

a

J

A

T

D

s

tI

C

A

D

E

M

I

L

S

E

P

C

A

F

a

J

A

T

D

s

tI

C

A

D

E

S

E

P

M

I

L

C

A

1

2

3

4

5

6

7

FIG. 2: Test problem (B), consisting of 5 input channels. Left
panel: Averaged performance index Perr from the output of
FastICA [1] (parameters with lowest MI), JADE [24], TD-
SEP (same parameters as in [6]), and MILCA (k=30). Right
panel: same as left side, but with total MI ˆI (k=3) used as
performance measure.

FIG. 3: Test problem (C), with 7 input channels. Left panel:
Averaged ˆI(ˆs1 . . . ˆsn) (k=3) from the output of FastICA [1]
(parameters with lowest MI), JADE [24], TDSEP (same pa-
rameters as in [6]), and MILCA (k=30). The horizontal line
indicates the true MI of the input channels. Right panel:
Pairwise MI estimates ˆI between all channel combinations,
for the MILCA output components shown in Fig. 4 (diagonal
is set to zero).

subspace, R =

i,j Rij(φ), where

Q

with

(17)

′
′
Rij(φ)(x1 . . . xi . . . xj . . . xn) = (x1 . . . x
j . . . xn)
i . . . x

′
i = cos φ xi + sin φ xj,
x

′
j = − sin φ xi + cos φ xj .
x

4

3

2

1

0

(18)

1

2

3

4

5

6

7

1000

2000

3000

4000

FIG. 4: Output of the MILCA algorithm, test problem (C).

For such a rotation one has (see Eq.(10))

I(Rij(φ)X) − I(X) = I(X

′
i, X

′
j) − I(Xi, Xj) ,

(19)

i.e., the change of I(X1 . . . Xn) under any rotation can
be computed by adding up changes of two-variable MIs.
This is an important numerical simpliﬁcation.

i, X ′

To ﬁnd the optimal angle φ in a given (i, j) plane, we
calculated ˆIij (φ) = ˆI(X ′
j) for typically 150 diﬀerent
angles in the interval [0, π/2], ﬁtted these values by typ-
ically 3-15 Fourier components, and took then the mini-
mum of the ﬁt. The latter is useful because ˆI(φ) is not
smooth in φ, for essentially the same reasons as discussed
in [12]. We also tried the augmentation proposed in [12]
to smoothen ˆI(X ′, Y ′). It worked equally well, by and
large, as the Fourier ﬁltering, but it was much slower.

Now the resulting MILCA-algorithm can be summa-

rized:

the data;

1. Preprocess (center, ﬁlter, detrend, ...) and whiten

2. For each pair (i, j) with i, j = 1 . . . n ﬁnd the an-
gle φ which minimizes a smooth ﬁt to ˆIij (φ) =
ˆI(X ′

i, X ′
3. If ˆI(X ′

j);
1 . . . X ′

step 2. Else, ˆsi = X ′
sources.

n) has not yet converged, go back to
i are the estimates for the

The order of choosing the sequence of pairs in point 2
is not essential. In our numerical simulations the conver-
gence speed did not diﬀer signiﬁcantly whether we went
through the pairs (i, j) systematically or randomly.

B. Numerical Examples and Performance Tests

(A) As a ﬁrst test we study the set of 18 problems
proposed by Bach & Jordan [21] and studied also in [12].

6

Each problem corresponds to a 1-d probability distri-
bution p(x). One thousand pairs of random numbers
x and y, each drawn iid from p(x)p(y), are mixed as
x′ = x cos φ + y sin φ, y′ = −x sin φ + y cos φ with random
angle φ common to all pairs (i.e., A is a pure rotation).
Using MILCA, we obtained then the estimate ˆA. This is
repeated 100 times with diﬀerent angles φ and with dif-
ferent random sets of pairs (x, y). To assess the quality
of the estimator ˆA (or, equivalently, of the back transfor-
mation ˆW = ˆA−1), we use the Amari performance index
Perr [22]

Perr =

1
2N

N

i,j=1
X

where pij = ( ˆA−1A)ij .

(

|pij |
maxk |pik|

+

|pij|
maxk |pkj|

) − 1

(20)

Results are given in Table 1 (column ‘MILCA’) and
compared there to the results of previous algorithms
given in [12]. They are excellent in average and surpassed
only by the RADICAL algorithm proposed in [12] which
also uses an entropy estimate based on neighbor dis-
tances, but for the diﬀerential Shannon entropies H(x′)
and H(y′). Another feature used in [12] is data aug-
mentation: To obtain a more smooth dependence on the
angle φ, each data vector (x, y) is replaced by an R-tuple
(with R = 30) of near-by points. The same augmenta-
tion trick can be used also for MILCA, and improves the
results for very similar reasons. Indeed, our results ob-
tained with MILCA and with data augmentation, given
in the last column of Table 1, are even better than those
of RADICAL. In the following tests we did not use data
augmentation, because it is rather time consuming.

(B) As a second test we study an example taken from
[6]. In involves ﬁve input sources (a sine wave, two dif-
ferent speech signals [the ﬁrst half of “Houston, we have
a problem” and “parental guidance is suggested” from
[23]], one white Gaussian noise, one uniformly distributed
white noise) (5000 data points each) which are linearly
mixed with a 5 × 5 matrix A to form ﬁve output sig-
nals. In mixing these components, no time delay is used,
i.e. the superpositions are strictly local in time. For this
example it is possible to ﬁnd the inverse transformation
W = A−1 up to a permutation and up to scaling factors,
because all sources are independent from each other and
only one has a Gaussian distribution. To assess the qual-
ity of this back transformation we again use the Amari
performance index.

The results obtained with two hundred diﬀerent ran-
dom mixtures of the sources (with uniformly distributed
mixing matrices and with diﬀerent realizations of the
random channels for each mixture) are compared in the
left panel of Fig. 2 with three standard algorithms: Fas-
tICA [1], JADE [24], and TDSEP [6]. We found that
FastICA sometimes gets stuck in a local minimum, and
runs diﬀering only in the initial conditions can produce
diﬀerent results. The error bars shown in Fig. 2 indi-
cate the resulting uncertainty of the performance mea-
sure, estimated from 20 realizations that diﬀer only in

initial conditions. The errors of JADE and FastICA are
mainly due to their diﬃculty to separate one of the au-
dio channels from the Gaussian noise. TDSEP is not
able to decompose the two noise channels, since it is also
not designed for this purpose (it uses time structures to
separate signals). Very good results for all 200 mixtures
are obtained by MILCA, although the audio signals are
quit noisy and have nearly Gaussian distributions. The
performance of JADE and FastICA compared to MILCA
becomes better when the quality of the acoustic signals
improves.

In addition to the Amari index, another (more direct)
way to judge the accuracy of the source estimates is to
look at the estimated MIs. If and only if the sources were
estimated correctly, the MI should be zero. In the follow-
ing we propose to use both the matrix of pairwise esti-
mators ˆI(ˆsi, ˆsj) and the estimated total MI ˆI(ˆs1 . . . ˆsn).
The important advantage over the Amari index is that
they can also be used when the exact sources are not
known. Low values of the MI indicate that both the data
is a mixture of independent components, and the separa-
tion algorithm worked well in producing some indepen-
dent components. Notice that it cannot be expected in
general that the components found are identical to the
sources, e.g., if some of them are Gaussians. In Fig. 2
again MILCA shows the best performances.

Notice the very big diﬀerence between FastICA/JADE
and TDSEP in the right panel of Fig. 2, which is much
bigger than that measured with the Amari index. The
ﬁrst two have problems in separating one of the acous-
tic signals (signal #4 in Fig. 4) from the Gaussian, be-
cause it has a nearly Gaussian amplitude distribution,
but for the same reason this is not punished by a large
MI between the outputs (improved performance index
see later in Fig. 14). TDSEP, using time information,
has no problem with this, but cannot separate uniform
from Gaussian noise – and is heavily punished for that by
MI. In Sec. V we will show how to improve MILCA such
that it can better separate components which have nearly
Gaussian amplitude distributions but diﬀerent time cor-
relations. Using that improved MILCA will give much
bigger performance diﬀerence with algorithms like Fas-
tICA/JADE.

(C) Next we want to investigate the case where the
decomposition is neither perfectly nor uniquely possible.
Such an example can be constructed by simply adding
one cosine with the same frequency as the sine and one
more Gaussian channel to the last test case. This now
violates the assumption of independent sources, because
the sine and cosine are strongly dependent. The theoret-
ical value for the MI would be inﬁnite, but a numerical
estimator from a ﬁnite data sample gives a ﬁnite value,
in our case I(S1 . . . Sn) = 0.72 [25]. But for this example,
perfect blind source separation is impossible also because
the two Gaussians are not uniquely decomposable. We
want to know how an ICA algorithm performs in view of
such problems. It should still be able to separate those
components which can be separated.

7

FIG. 5: Performance index distributions over 7000 triples of
three-component mixtures. For histograms (a),(b) the origi-
nal spectra were decomposed, for (c)-(e) their second deriva-
tives.

The total output MI is shown in the left panel of Fig. 3.
We see that for all algorithms the MI is higher than
the MI between the input channels, which serves essen-
tially as a consistency test. The diﬀerence is smallest
for MILCA. The MIs between all pairwise channel com-
binations obtained with MILCA are shown in the right
panel of Fig. 3. They show again that MILCA has done
a perfect job: All components are independent except
for those which should not be. MILCA output is shown
directly in Fig. 4. Although we do not show the input,
it is clear that the separation has been as successful as
possible.

(D) There are a number of blind source separation
problems in the ﬁeld of analytical spectroscopy, where
quantitative spectral analysis of chemical mixtures is for-
mulated as multivariate curve resolution (for recent re-
views see [26, 27, 28] and as an ICA problem [29, 30, 31]).
Assuming Beer’s law, the spectrum of a mixture of pure
constituents with spectra si(ν) and concentrations Ai is
i Aisi(ν). Given a set of N mixtures and N
x(ν) =
pure components, we can then write this in vector nota-
tion as x(ν) = As(ν), analogous to Eq.(15). The task is
to obtain estimates ˆs(ν) for the pure components. This
is the instantaneous linear ICA problem, except that in
most applications of interest the spectral sources are not
independent but have overlapping bands. This happens
when chemical compounds in a mixture share several
common or similar structural groups that demonstrate
nearly the same spectral patterns.

P

This diﬃculty makes mixture decomposition quite
nontrivial for many BSS techniques used in chemomet-
rics, unless interactive band selection (e.g. SIMPLISMA
[32], IPCA [33], BTEM [34]) is employed to avoid using
those parts of the signals where severe overlaps reduce
the quality of decomposition. Such preprocessing made
by hand is, of course, a bit of an art, because these un-
safe bands can not be known a priori in a blind problem.
Since the focus here is rather on developing general pur-
pose algorithms, we aim at using MILCA without inter-
active preprocessing in order to estimate its pure overall

eﬃciency in cases when residual dependencies play a role.
To test the performance of MILCA on typical spectral
data we collected a pool of 62 experimental molecular
infrared absorption spectra in the range 550-3830 cm−1
(822 data points each) taken from the NIST database
[35]. This test set was selected to contain organic com-
pounds with common structural groups (benzene deriva-
tives, phenols, alcohols, thiols) so that their spectra have
multiple overlapping bands and, thereby, are mutually
dependent. Then a sample of 7000 triples of three-
component mixtures was constructed by choosing spec-
tra randomly from the pool and applying random mix-
ing matrices A [36]. For each decomposition the Amari
performance index was computed. Fig. 5 compares its
distributions for several diﬀerent ICA algorithms includ-
ing FastICA [1], RADICAL [12] and Nonnegative PCA
(NNPCA) [37]. The latter uses the fact that pure spec-
tra are non-negative and the same should hold for the
estimates, so the nonnegativity is imposed as a soft con-
straint on the estimates ˆsi (ν) in an optimization proce-
dure. But our simulations showed that this constraint
is often not fulﬁlled, and in some cases the output of
NNPCA (as well as that of other algorithms) is nega-
tive. To a large part this is due to dependencies be-
tween the sources. Already prewhitening (i.e. PCA
and rescaling) sometimes leads to decorrelated compo-
nents which cannot be made nonnegative by any subse-
quent rotation. Trying to enforce nonnegativity neglect-
ing other aspects might then be counterproductive, and
this might partly explain the relatively poor performance
of NNPCA (Fig. 5a).

NNPCA has to be applied to the original spectra, while
it is well known that using derivatives of spectroscopic
signals with respect to frequency can improve the results
(see, e.g., [29, 31]). Taking such derivatives extracts the
spectral information which is more independent between
the sources [2].
In our numerical experiments, second
order derivatives approximated by ﬁnite diﬀerences

∼ x(νi−1) − 2x(νi) + x(νi+1).

(21)

gave best performance [38]. This is clearly seen in the ex-
ample of FastICA (compare distributions (b) and (c) on
Fig. 5). But MILCA (e) and RADICAL (d) with second
derivative data perform better than FastICA (c), and
are almost equally good when compared to each other.
Furthermore, our numerical results conﬁrmed that non-
negativity is satisﬁed whenever the decomposition is suc-
cessful (Amari index below 0.05) (see also the discussion
in [39]). But whether this is fulﬁlled depends primarily
on the dependencies between the original signals, and less
on the algorithm employed.

A more detailed study of the potential of MILCA in
multivariate spectral curve resolution will be given in
a forthcoming publication [40] which will focus on the
analysis of experimental mixtures and, in particular, on
the comparison with recently developed interactive algo-
rithms such as BTEM [34].

d2x(ν)
dν2

νi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

2

3

4

5

6

7

8

0.25

0.2

0.15

0.1

0.05

0

1

2

3

4

5

6

7

FIG. 6: Square roots of variabilities √σij of I(R(Xi, Xj ))
(with k = 6) from MILCA output for test problem (C)
(Fig. 4). Elements on the diagonal have been put to zero.

C. Reliability and Uniqueness of the ICA Output

Obtaining the most independent components from a
mixture is only the ﬁrst part of an ICA analysis. Check-
ing the actual dependencies between the obtained com-
ponents should be the next task, although it is most often
ignored. We have seen that it becomes easy and natural
with MILCA, which was indeed one of our main moti-
vations for MILCA. The next task after that is to check
the reliability, uniqueness, and robustness of the decom-
position. We have already discussed this in the last sub-
section for test example (C), but not very systematic. A
systematic discussion will be given now.

Recently proposed reliability tests [6, 7, 8] are based
on bootstrap methods or noise injection. We here present
an alternative procedure which again makes use of the
fact that MILCA gives reliable estimates of the actual
(in-)dependencies: We test how much the estimated de-
pendencies change under re-mixing the outputs.

In the simplest case, a multivariate signal with n com-
ponents is an instantaneous linear mixture of n indepen-
dent sources. This was the model we started with in sub-
section A. We assume it to apply when (i) all estimated
pairwise MIs between all ICA components fall below a
deﬁned threshold, ˆI(ˆsi, ˆsj) < Dmax for all i, j = 1, ..., n
and i 6= j, and (ii) the overall MI ˆI(ˆs1 . . . ˆsn) is below
another threshold. Notice that the ﬁrst criterion alone is
not suﬃcient, see the appendix.

In real-world data, however, we are usually confronted
with deviations from this simple model. The next sim-
ple possibility is that some pairwise MIs are still exactly
zero, but others are not. Let us draw a graph where
each of the n output channels is represented by a ver-
tex, and each pair (i, j) of vertices is connected by an
edge if ˆI(ˆsi, ˆsj) > Dmax. This gives a partitioning of
the set of output components into connected clusters
C1, . . . Cm with m ≤ n. If, in addition, the MI between
these clusters, ˆI(C1, . . . Cm), is below another suitably
chosen threshold, we consider each cluster to be indepen-
dent (notice that we do not require all channels within a
cluster to have a MI above the threshold Dmax). This
is essentially our version of multidimensional ICA [5].
It uses exactly the same basic MILCA algorithm as de-

ﬁned above, and is thus much simpler conceptually than
the ‘tree-dependent component analysis’ of [3]. Its main
drawback is that it is not sensitive to the actual strengths
of the non-zero interdependencies. A better algorithm
which does take them into account will be discussed in
Sec.IV.

In addition to this ﬁrst step of an ICA output analy-
sis, we have to test for the uniqueness of the components.
For this purpose, we check whether the (one- or multi-
dimensional) sources obtained by the ICA algorithm in-
deed correspond to distinct minima of the contrast func-
tion or whether other linear combinations exist which
show approximately the same overall dependencies. An
example for the latter case is given by two uncorrelated
Gaussian signals. They remain independent under rota-
tion [41].

A good estimator for the uniqueness of the ICA output
is the variability of the pairwise MI under remixing, i.e.
under rotations in the two-dimensional plane:

σij = I(Xi, Xj) − ˆIij (φmin)

for i 6= j

(22)

where the global minimum of ˆI is at φ = φmin, and

I(Xi, Xj) =

π/2

dφ ˆIij (φ)

2
π

(23)

0
Z
(notice that Iij (φ) is periodic in φ with period π/2). For
unique solutions the MI will change signiﬁcantly (large
σij ), but it will stay almost constant for ambiguous out-
puts (small σij ).

Results for the MILCA output of test problem (C) are
shown in Fig. 6. (to aid in the interpretation, the actual
output signals were shown in Fig. 4). The basic ICA
model is violated both in the Gaussian noise subspace
and the sin/cos subspace. In the Gaussian subspace the
components are independent, but it should be impossible
to ﬁnd a unique decomposition. Indeed, σ5,6 ≈ 0 (Fig 6)
and ˆI5,6 ≈ 0 (Fig. 3). For the dependent components
(sin/cos subspace) the situation is diﬀerent. We expect
to have σ = 0 also here, corresponding to the isotropy
of the distribution in this subspace. But ˆI should be
much larger than zero, because the two signals are not
independent. Indeed, we see σ1,2 ≈ 0 and ˆI1,2 ≫ 0. In
general, it depends on the speciﬁc application whether
one should attribute any meaning to σij when compo-
nents i and j are not independent. Finally, we conclude
from Fig. 3(right) and Fig. 6 that the channels 3, 4 (au-
dio signals), and 7 (uniformly distributed noise) are one
dimensional sources, because they are independent from
any channel, ˆI3,i ≈ ˆI4,i ≈ ˆI7,i ≈ 0, and are reliable,
σ3,i ≈ σ4,i ≈ σ7,i ≫ 0.

D. Noisy Signals

9

0.25

0.2

0.15

0.1

0.05

0

0

0.5

0.4

0.3

0.2

0.1

0
13

π/8

π/4

3π/8

π/2

FIG. 7: Unsmoothened estimates of ˆI(φ) for two randomly
mixed uniform distributions, corrupted with isotropic Gaus-
sian measurement noises with diﬀerent signal-to-noise ratios
, 13, 7, 4, 1 (from top to bottom), plotted against φ.
SN R =

∞

11

9

SNR

7

5

3

1

FIG. 8: Averaged Amari index against the signal-to-noise ra-
tio. The condition number of the mixing matrices is 6. The
upper curve (in the SNR range from 7 to 3) is for standard
MILCA, the lower for n-MILCA.

In the literature there exist several algorithms which
are specially tailored to this problem (see e.g. Ref. [1],
chapter 15). Typically, in order to obtain optimal per-
formance, the noise is assumed to satisfy very special
properties like being additive, uncorrelated, isotropic and
Gaussian. Below we will present a modiﬁed MILCA al-
gorithm which assumes that we have measurement noise
with exactly these properties.

Alternatively, one can take just a standard ICA al-
gorithm (in our case MILCA as described above), and
analyze how its output depends on the noise level.
In
the following we will compare both approaches.

We start with two uniformly distributed variables and
mixed them with a random 2 × 2 matrix with a ﬁxed
condition number. After that, iid Gaussian noises are
added to each of the two mixtures. The amplitudes in
both channels are the same,

xi(t) =

Aij sj(t) + ηi(t)

(24)

2

j=1
X

Because our aim is to apply MILCA to real world data,
we have to discuss the inﬂuence of measurement noise.

with hηi(t)ηj(t′)i = rδij δtt′ . For the case were we do

10

0.3

0.25

0.2

0.15

0.1

0.05

1

2

3

4

1

2

3

4

5

6

7

8

1
2
3
4
5
6
7
8

not use any information of the measurement noise sig-
nals xi(t) are then simply used as input in MILCA. In
Fig. 7 we show ˆI(φ) for the same mixing matrix but dif-
ferent signal-to-noise ratios SN R = var(si(t))/r. We see
that ˆI becomes ﬂatter (the variability with respect to the
mixing angle decreases) with decreasing SNR [42]. The
presence of noise leads also to a shift of the minimum.
Both eﬀects introduce errors in estimating the original
mixing matrix. The upper curve in Fig. 8 shows the av-
eraged Amari index over 100 realizations with diﬀerent
noise and mixing matrices.

To reduce this error we modify MILCA to n-MILCA
(noisy MILCA). At ﬁrst we do a ’quasiwhitening’ with
the estimated covariance matrix V = (Cx − r1)−1/2 of
the pure signals (see, e.g., [1], chapter 15) to decorrelate
the original sources. As a consequence of this, the noise
will become now correlated, and with it also the entire
’quasiwhitened’ signal. Because of this we should not
minimize ˆI(φ), since in this way we would introduce a
bias as seen in Fig. 7 towards wrong values of φ. Instead
we minimize ˆI(φ) + 1
2 ln(1 − Cij(φ)2) where we have sub-
tracted the ‘linear’ contribution (see Eq.(7)). In Fig. 8
we show again the averaged Amari index for the same
realizations as used before. Making use of detailed in-
formation on the noise clearly improved the results, ex-
cept for very small SNR. The amount by which it im-
proves depends on the condition number of the mixing
matrix. For matrices far away from singularity (low con-
dition number) the quasiwhitening has little eﬀect and
there is hardly any diﬀerence, while for large condition
numbers the two mixtures are nearly the same and it is
impossible to obtain good results with either algorithms.
Finally, before leaving this subsection, let us say a few
words about outliers. Outliers are just a special case
of noise. Because our MI estimator is based on the k-
nearest neighbor distribution, outliers make less diﬃcul-
ties Ref.[12] than e.g. in kurtosis based algorithms.

E. A Real-World Application

Finally, let us apply MILCA to a fetal ECG recording
from the abdomen and thorax of a pregnant woman (8
electrodes, 500 Hz, 5 seconds). We chose this data set
because it was analyzed several times with diﬀerent ICA
algorithms [5, 6, 9, 43] and is available on the web [44].
The output components of MILCA are shown in Fig. 9
[45]. We used k = 30 neighbors for estimating MI, and
to obtain the minima of ˆIij (φ) we ﬁtted with 3 Fourier
components. The success of the decomposition is already
seen by visual inspection. Obviously, channels 1-2 are
dominated by the heartbeat of the mother, and chan-
nel 5 by that of the child. Channels 3, 4, and 6 still
contain heartbeat components (of mother and child, re-
spectively), but look much more noisy. Channels 7 and 8
seem to be dominated by noise, but with rather diﬀerent
spectral compositions.

In order to verify this also formally (which would be

1

2

3

4

sec

FIG. 9: MILCA output:
I(X1 . . . X8) for the heart beat example of Sec. III.E.

components after minimizing

0.4

0.3

0.2

0.1

0

1
2
3
4
5
6
7
8

1 2 3 4 5 6 7 8

1 2 3 4 5 6 7 8

FIG. 10: Left panel: ˆI between the all pairwise combinations
of the signals shown in Fig. 9. Right panel: Square roots of
variabilities σij of ˆIij (φ). In both panels the values on the
diagonal are set to zero.

essential in any automatic real-time implementation) we
ﬁrst show in Fig. 10 (left panel) the pairwise MIs. We see
that most MIs are indeed small, except the one between
the ﬁrst two components. This indicates again that the
ﬁrst two components belong to the same source, namely
the heart of the mother. But some of the other MIs seem
to be deﬁnitely non-zero, even if they are small. This
indicates that the decomposition is not perfect, as is also
seen by closer inspection of Fig. 9.

Finally, we show in the right panel of Fig. 10 the vari-
abilities under re-mixing. They conﬁrm our previous
ﬁndings. In contrast to the sine/cosine pair in test exam-
ple (C), the ﬁrst two components have non-zero σ, show-
ing that the distribution in this subspace is not isotropic
and that one can minimize the interdependence in it by
a suitably chosen demixing. Apart from that, the biggest
values of σ are for channels 1, 2, and 5, showing that these
channels are most reliably and uniquely reconstructed.
They are just the channels dominated most strongly by
heart beat.

n
o

i
t

a
m
r
o

f

n

i
 
l

t

a
u
u
m

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

2

7

4

3

5

6

8

FIG. 11: Dendrogram for Fig. 9. The height of each cluster
(ij) corresponds to ˆI(Xi, Xj ) (k=6).

IV. CLUSTER ANALYSIS

We pointed already out that the usual assumption of
independent one-dimensional sources as in Eq.(15) is of-
ten unrealistic. Take e.g. the ECG discussed in the pre-
vious subsection, and assume that both hearts – the one
of the mother and the one of the fetus – are indepen-
dent chaotic dynamical systems. A chaotic system with
continuous time must have at least 3 excited degrees of
freedom [48]. With any generic placement of the elec-
trodes, we should then expect to pick up ≥ 3 diﬀerent
components from each heart. These components must
be strongly dependent on each other, even after having
been whitened [49]. Thus each heart must contribute to
at least 3 output components in any linear ICA scheme.
For the mother heart we have indeed found 2 compo-
nents. The fact that we have not clearly identiﬁed more
dependent components in the output should be consid-
ered as failure of the instantaneous linear algorithm and
will be dealt with more systematically in Sec. V.

In any case, in view of this we have to expect that
outputs in real-world applications are not independent
but come in connected clusters. Moreover, we should ex-
pect that even within one cluster there are more or less
strongly connected substructures. We have already dis-
cussed in Sec. III.c a simple way how to identify these
clusters. In the present section we present a more sys-
tematic analysis.

Our strategy is to estimate a proximity matrix from the
MIs, and then to use a hierarchical clustering algorithm
to obtain a dendrogram. No thresholds are used in con-
structing the dendrogram, i.e. it is constructed without
making any decision about which MILCA output chan-
nels are independent or not. Only after its construction
we decide, usually on heuristic reasons and based on ar-
guments of practicality and usefulness, which channels
are actually grouped together. This is more convenient,
usually, than the algorithms of [3, 4, 50] where this de-
cision stands at the starting point of the algorithm or is
an essential part of it.

A ﬁrst technical problem concerns the choice of the

11

proximity matrix. One might be tempted to use MI di-
rectly. But we want to include the possibility that some
of the channels to be grouped together are already multi-
dimensional by themselves. In this case, using MI would
introduce a bias: multivariate channels not only tend to
carry more information than univariate ones, they also
will have larger MIs. Therefore we propose to use as a
similarity measure [14]

Pij =

ˆI(ˆsi, ˆsj)
dim(ˆsi) + dim(ˆsj)

,

(25)

where dim(x) is the dimension of the variable x, i.e. the
number of its components.

In most cluster algorithms the proximity matrix P is
In the subsequent steps,
used only for the ﬁrst step.
proximities for clusters are derived from it in some re-
cursive way [51].
In the present paper we propose to
use ‘MI-based Clustering’ (MIC) [14] which is based on
the grouping property Eq.(9). Thus, a cluster of output
channels is just characterized by the multivariate signal
formed by the tuple of its individual channels, and the
proximity measure is still given precisely by Eq.(25) at
each level of the hierarchy.

In summary, our cluster algorithm is as follows. We
start with n (usually univariate) MILCA output chan-
nels ˆsi ,
i = 1, . . . n, and we compute Pij according to
Eq.(25). After that, we enter the following recursion:

1. Find the pair with minimum distance in the matrix,

say clusters i and j;

2. Combine the clusters i and j to a new cluster (ij)
with multivariate data ˆsij, and attribute to it a
height ˆI(ˆsi, ˆsj) in the dendrogram. Thereby the
total number of clusters is reduced by one, n ←
n − 1;

3. If the new value of n is 1 then exit; else

4. update the proximity matrix Pij and go to 1.

The dendrogram obtained in this way for the ECG data
of Sec. III.E is shown in Fig. 11. In this ﬁgure two clusters
are clearly distinguishable, the mother cluster containing
channels (1, 2, 3, 4, 7) and the fetus cluster formed by
channels 5 and 6. This agrees perfectly with the inter-
pretation given in Sec. III.E. One can of course debate
whether, e.g., channel 7 belongs to the mother cluster or
not, but this can be decided as it seems most convenient,
and it will in general have little eﬀect on any conclusions.
One way to make use of such a clustering is in cleaning
the data and separating the individual sources. For that,
one prunes everything except the wanted cluster, and re-
constructs the original channels by applying the inverse
of the matrix W. Results obtained in this way will be
shown in the next section, after having discussed how to
take into account temporal structures.

s
s
u
a
G
 
e
t
i
h
w

3

2

1

0

−1

−2

−3

0.5

0.4

0.3

0.2

0.1

n
o
i
t
a
m
r
o
f
n
i
 
l
a
u
t
u
m

0
0

−3 −2 −1

0
1
red Gauss

2

3

50

100
sample points

150

200

FIG. 12: Left panel: scatter plot of the two Gaussian sources
with diﬀerent spectra. Right panel: The ﬁrst 200 sample
points of the output of the modiﬁed MILCA algorithm with
τ = 1 and m = 2.

π/8

π/4

3π/8

π/2

FIG. 13: Change of ˆI under rotation, for the Gaussian model
shown in Fig. 12. The nearly horizontal curve shows the be-
havior without, the sinusoidal one the result with using delay
embedding. Here the actual mixing angle is 0.

12

x
e
d
n
i
 
e
c
n
a
m
r
o
f
r
e
p

0.25

0.2

0.15

0.1

0.05

0

n
o
i
t
a
m
r
o
f
n
i
 
l
a
u
t
u
m

0.3

0.25

0.2

0.15

0.1

0.05

0

F

a

J

A

T

D

s

tI

C

A

D

E

S

E

M

M

I

L

I

L

C

C

P

A

A

*

F

a

J

T

A

D

s

tI

C

A

D

S

E

E

M

M

I

L

I

L

C

C

P

A

A

*

FIG. 14: Test problem (B) of Sec.III, consisting of 5 input
channels, compare with Fig.2. Algorithm “MILCA
” now
refers to the minimization of Eq.(28). The black bars on the
rhs panel show the full MI given in Eq.(28). The embedding
parameters are m = 2, τ = 1.

∗

t by giving not its value at t itself, but at m previous
times. This makes of course sense only when there is
any time structure in the signal. Similarly we can also
embed multivariate signals. For n measured channels one
obtains thereby an n × m ‘delay matrix’

X(t) = [x1(t), . . . xn(t)].

(27)

To decompose an instantaneous linear mixture of n
signals with either non-Gaussian statistics or with non-
trivial time structure, we propose to simply minimize the
MI,

V. USING TEMPORAL STRUCTURES

ˆI(s1(t), . . . sn(t)) != min .

(28)

A.

Instantaneous Demixing that Minimizes

Delayed Mutual Informations

Until now we have not used any time structure in
In the following we shall assume the sig-
the signals.
nals to be stationary with ﬁnite autocorrelation times.
ICA-algorithms in the literature either use no time infor-
mation at all (JADE[24], FastICA[1], INFOMAX[52], ...)
or, if they do use it, they use only second order statistics
(AMUSE [53], TDSEP [6],...). The ﬁrst group is not able
to decompose two Gaussian signals with diﬀerent spec-
tra, while the second group is not able to separate two
temporally white signals with diﬀerent amplitude distri-
butions. Obviously one has to make use of time structure
and higher order statistics, to obtain optimal results in
general [2, 54]. This is precisely what we will do in this
subsection.

Normally the ﬁrst step in nonlinear time series anal-
ysis of univariate signals is delay embedding [47]: One
constructs a formally m-variate signal, for any m > 1,
by simply forming m-dimensional ‘delay vectors’ with a
suitably chosen delay τ ,

x(t) = [x(t − τ ), x(t − 2τ ), . . . x(t − mτ )]T .

(26)

Thus one characterizes the “state” of a signal at time

Notice that we have here considered the delay vectors
as joint entities, i.e. we do not include in Eq.(28) the
MIs between the diﬀerent delays of the same xi. More
explicitly [55],

I(x1(t), . . . xn(t)) = I(x1(t − τ ), . . . x1(t − mτ ),
x2(t − τ ), . . . x2(t − mτ ), . . .
xn(t − τ ), . . . xn(t − mτ ))
n

(29)

I(xi(t − τ ), . . . xi(t − mτ ))

H(xi(t)) − H(x1(t), . . . xn(t))

To minimize this, we proceed again as in Sec.III,
i.e. we decompose the rotation needed to minimize
ˆI(x1(t), . . . xn(t)) into rotations within each of the n(n −
1)/2 coordinate planes. Each of the latter rotations still
involves rotations of m delay coordinate pairs, but this
can be further decomposed into m rotations where only
one delay coordinate pair is rotated. We thereby obtain

′
′
i(t), . . . x
I(. . . x
j(t) . . .) − I(. . . xi(t), . . . xj(t) . . .)
′
′
j (t)) − I(xi(t), xj (t))
i(t), x
= I(x

−

=

i=1
X
n

i=1
X

13

1

2

3

4

5

6

1

2

3

4

sec

FIG. 15: Upper panel: Two channels of the ECG of a preg-
nant woman. Lower panel: MILCA output from these two
channels.

= I(xi(t − τ ), . . . xi(t − mτ ))
+I(xj (t − τ ), . . . xj(t − mτ ))
′
′
i(t − mτ ))
i(t − τ ), . . . x
−I(x
′
′
−I(x
j (t − τ ), . . . x
j(t − mτ ))
′
′
j (t)) − I(xi(t), xj (t))] ,
i(t), x
+m [I(x

(30)

i(t), x′

where we have used in the last term the fact that
I(x′
j (t)) is independent of t due to stationarity. If
m = 2, this is again a sum of pairwise MIs. If m > 2, we
have to estimate m-dimensional MIs directly.

To illustrate this on a simple example, let us assume
two channels where x1(t) and x2(t) are instantaneous
mixtures of two Gaussian signals with the same ampli-
tude distribution but with diﬀerent spectra: x1 is white
(iid), while x2 is red and was obtained by ﬁltering with
a Butterworth ﬁlter of order 6 and with cutoﬀ frequency
0.3. For simplicity we assume the mixing to be a pure ro-
tation. Then a scatter plot of the vectors (x1(t), x2(t)) is
completely featureless, see Fig. 12(left), and will not al-
low a unique decomposition. But using delay embedding
with m = 2 is suﬃcient to obtain the original sources
(Fig. 12, right panel).

Similarly good results were obtained with the less triv-
ial examples of previous sections. In particular, we tested
the algorithm on test problem (B) of Sec. III.B (Fig. 14).
The performance of MILCA is improved substantially,
even with m = 2. The delayed MI (Eq.(28)) which make
use of the time structure serves as a better performance
value (Fig. 14 (right)). Now JADE and FastICA are
also heavily punished for not separating one audio signal
from Gaussian noise (as one can see the MI for TDSEP
is nearly unchanged because the time correlation in the
output is minimal).

B. Demixing with Delays

The most general linear demixing ansatz for a station-
ary system assumes superpositions of the observed signals

1

2

3

4

sec

FIG. 16: MILCA output from the delay embedded two chan-
nel ECG with embedding dimension m = 3.

with delays. Using up to m delays τ, 2τ, . . . mτ , we thus
make the ansatz (see e.g. Ref. [1], chapter 19)

ˆsi(t) =

wk

ij xj (t − kτ )

N

m

j=1
X
N

Xk=1

j=1
X

=

wijxj (t) ,

wij = [w1

ij . . . wk

ij ].

(31)

(32)

where xj(t) is a delay vector as deﬁned in Eq.(26) and

Since we have now linear superpositions of n × m mea-
surements xj (t − kτ ) on the right hand side, we can also
determine the same number of ˆsi(t) for each value of t,
i.e. the index i in Eq.(31) runs from 1 to nm.

This ansatz is obviously more appropriate than instan-
taneous mixing, if the signals xi(t) are themselves su-
perpositions of delayed sources. If they involve a ﬁnite
number of delays,

′

m

xi(t) =

ak
ijsj(t − kτ ),

(33)

j
X

Xk=1
Eq.(31) with ﬁnite m would not give the exact demixing,
since inverting Eq.(33) would require an inﬁnite number
of delay terms. Also, Eq.(31) in general does not corre-
spond to the inverse of Eq.(33), because its solutions are
in general not components of any delay vectors. But it
should deﬁnitely be a better ansatz than the instanta-
neous Eq.(15).

Apart from that, we would anyhow not expect Eq.(33)
to be the correct model in most applications. The main
reason why we believe that Eq.(31) is useful in many ap-
plications is that it can cope much better with the situa-
tion discussed at the beginning of Sec. IV. Assume for the

14

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

moment that there is a single source. Diﬀerent sensors
(as, e.g., diﬀerent ECG contacts) typically see diﬀerent
projections of this source, and the signals xi(t) can there-
fore be considered as diﬀerent coordinates describing its
dynamics. As pointed out by Takens [47], delayed values
of one single signal can also be considered as diﬀerent
coordinates. Our demixing ansatz basically reﬂects the
hope that suitable superpositions of delayed values of xi,
say, can mimic any other signal xj .

To illustrate this, we consider again the above ECG
recording. We assume for the moment that only the two
channels with the most pronounced fetus heartbeat are
available and try to decompose them into mother and fe-
tus heartbeat. These two channels are shown in Fig. 15
(top). They are still dominated by the mother heartbeat.
But the R peak of the mother has a very diﬀerent shape
in both channels: In the lower trace it is mainly posi-
tive, while it has both positive and negative components
in the upper. It is therefore clear that there cannot ex-
ist an instantaneous superposition to which the mother’s
heartbeat does not contribute. Instantaneous ICA must
fail for this case, as is indeed seen in the lower two traces
of Fig. 15.

In order to obtain the least dependent components ob-
tainable with Eq.(31), we minimize again the MI. But
now, in contrast to the previous subsection, the output
variable si(t) are not delay coordinates of any sources,
and therefore we must minimize the full MI between all
si(t),

ˆI(s1(t), . . . snm(t)) != min .

(34)

The minimization is done again, as in all previous
cases, by performing successive transformations in 2-
dimensional subspaces and by using Eq.(10).
In terms
of the actual algorithm, the only diﬀerence to the pre-
vious subsection is that we now make rotations in all
subspaces.

In our application to the fetal ECG we use embed-
ding dimension m = 3 and the smallest possible delay,
τ = 1/500 s−1. Results for the two channels shown in
Fig. 15 are now shown in Fig. 16. The separation is
now improved. Although we still have one output chan-
nel where mother and fetus are strongly mixed (channel
#4), channel #6 is now practically pure fetal heartbeat.
Finally, we applied this method to all 8 channels of
the ECG. Using again m = 3 gives altogether 24 output
channels. They are shown in (Fig. 17), and we can clearly
see which ones are dominated by the mother heartbeat,
which by the fetus, and which by noise. In order to do
this more objectively, we again apply the cluster algo-
rithm of Sec. IV, with the result shown in Fig. 18. There
one can clearly see two big clusters corresponding to the
mother and to the fetus. There are also some small clus-
ters which should be considered as noise.

For any two clusters (tuples) X = X1 . . . Xp and
Y = Y1 . . . Yq one has I(X1, . . . Yq) ≥ I(X) + I(Y ). This
guarantees, if the MI is estimated correctly, that the tree
is drawn properly, i.e. each parent node is above the two

0

1

2

3

4

sec

FIG. 17: MILCA output from the embedded eight channel
ECG (k=100, m=3)

 
r
e
h
t
o
m

 
d

l
i

h
c

1.5

n
o
i
t
a
m
r
o
f
n
i
 
l
a
u
t
u
m

0.5

2

1

0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
channel nr.

FIG. 18: Dendrogram for Fig. 17. Heights of each cluster
correspond to I(Xi, Xj ) of the cluster ij (k=3).

daughter nods. The two slight glitches (when clusters (1
- 14) and (15 - 18) join, and when (21 -22) is joined with
23) result from small errors in estimating MI. They do
not aﬀect our conclusions.

In Fig. 19 we show the matrices of pairwise MIs (left
panel) and of pairwise variabilities (right). They are
as expected, and they show much more pronounced
structures than the matrices without delay embedding
(Fig. 10). For the MIs one can see a clear block struc-
ture, i.e. the mother and fetus components are now in-
deed more independent, as suggested also from the traces
themselves. From the right panel we see that the main
mother channels (1-4) and the fetus channels (7-8) are
very stable. The rest is mostly noise, and is not stable as
indicated by the very small variabilities.

5

10

15

20

a

b

c

d

e

The ﬁnal result of MILCA is obtained by pruning ev-

erything not belonging to the cluster of interest,

ˆsi(t) → PC ˆsi(t) ≡

ˆsi(t)
0

(

i ∈ cluster C
else

(35)

and performing the back transformation. At this stage
there arises the problem that the reconstructed signals

ˆxj,k(t; C) = W

−1
(j,k),iPC ˆsi(t) , Wi,(j,k) = wk

ij

(36)

are in general not delay vectors, i.e.

ˆxj,k+1(t; C) 6= ˆxj,k(t − τ ; C) .

(37)

In view of this, one has to make some heuristic decision
what to use as a cleaned signal. We use simple averages,

m

1
m

ˆxj(t; C) =

ˆxj,k(t + kτ ; C) .

(38)

Xk=1
We do not show all 8 full traces for the mother and fetus,
because this would not be very informative: The results
are too clean to be judged on this scale. Instead we show
in Fig. 20 blow-ups of one of the original traces and the
contributions to it from the mother and from the fetus.
The separation is practically perfect.

Before leaving this section, we should point out that
one can, in principle, also construct algorithms in be-
tween those of the last two subsections. In subsection A
we had used delays to minimize the lagged MI, but we
had not used the delays in the demixing. In the present
subsection, we have used the same delays both for min-
imizing MI and for demixing. A generalization consists
in using m delays in the demixing, but minimizing the
MI with additional m′ delays. Thus we make the same
demixing ansatz Eq.(31) as above, but we minimize

ˆI(s1(t), . . . snm(t)) != min .

(39)

where we have used the deﬁnition of I(s1(t), . . .) given in
Eq.(30), and ˆsi(t) = [ˆsi(t−τ ), ˆsi(t−2τ ), . . . ˆsi(t−m′τ )]T .
Up to now, we have not yet applied this to any problem.

VI. DISCUSSION

There is by now a huge literature on independent com-
ponent analysis. Therefore, most of our treatment is re-
lated in some form to previous work. One of our basic
premises was that we did not so much care about speed,
but we wanted as precise a dependency measure as possi-
ble. Our claim that this is provided in principle by MI is
of course not new. But we believe that our estimator via
k-nearest neighbor statistics is new and provides the most
precise mutual information estimate. It is closely related
to similar estimators for diﬀerential entropies which had
been used in [11, 12], and the quality of our results in

15

0.5

0.4

0.3

0.2

0.1

5

10

15

20

0.3

0.2

0.1

0

5

10

15

20

5

10

15

20

FIG. 19: Left panel: Pairwise MIs between the estimated
components shown in Fig. 17. Right: Square roots of vari-
abilities √σij of I(Xi, Xj ) (with k = 6). Elements on the
diagonal have been put to zero.

1 sec

2 sec

FIG. 20: Short segment from the original ECG (a), of the
mother and fetus contributions estimated without delay em-
bedding (b,c), and of the two contributions estimated with
delay embedding (d,e).

the most simple 2-d blind source separation problem is
very similar to the one in [12]. The main virtue of our
MI estimator, compared to all previous MI estimators,
is the numerical fact that it becomes unbiased when the
two distributions are independent.

While using diﬀerential entropies instead of MI would
give the same quality and somewhat simpler codes for
the basic blind separation problem, using MI has other
advantages: with it we can estimate the residual de-
pendencies between the output components. Our use of
this knowledge for estimating the output uniqueness and
robustness, by measuring how the dependencies change
under re-mixing, seems to be new. Previous authors
used for this problem resamplings and/or noise addition
[6, 7, 8].

In addition to this, we used the MIs between the out-
puts to cluster them, and we then used this clustering to
obtain the contributions of the individual (multidimen-
sional) sources to the measured signals. The observation
that “independent” component analysis will in general,
when applied to real world data, not give independent
components is not new either [3, 4, 5]. We stress it
by calling our approach a “least dependent” component
analysis. Our detailed implementation of this idea seems
to be new, not the least because our clustering algorithm
is novel and uses a speciﬁc property of MI not shared by
other contrast functions.

Although the extension of our algorithm to data with
time structure discussed in Sec.V.A seems straightfor-
ward, this strategy of combining in the contrast function
deviations from Gaussianity both at equal times and at
non-equal times has been considered in very few papers
only [2, 54]. The present paper is the ﬁrst which uses
directly MI for combining these two aspects. In Sec. V.
was shown that this can substantially improve the sepa-
ration e.g. of audio signals.

Both the ansatz of Sec.V.A and the method of demix-
ing with delays in Sec.V.B are entirely based on MI, and
use essentially the same algorithm. Therefore, also the
generalization mentioned at the end of Sec.V.B uses es-
sentially the same basic algorithm. This last general-
ization was never considered before, but demixing with
delays is of course a very widely treated concept (see e.g.
[1]).
In our
presentation we stressed several features which are typi-
cally overlooked. One is that the ‘convolutive’ demixing
ansatz Eq.(31) is in general, when the sources si(t) are
not strictly independent, not equivalent to a convolutive
mixing ansatz, because the sources then will not be com-
ponents of delay vectors. This is also the reason why we
avoided the term ‘convolutive mixing’.

It is usually called ‘convolutive mixing’.

Just as ICA may be considered as a generalization
of principle component analysis (PCA) to non-Gaussian
contrast functions, mixing with delays is a generalization
of multivariate singular source analysis (SSA) [57, 58]
to include non-Gaussianity. Univariate SSA, see e.g.
[49, 59], is often considered as an alternative to Fourier
decomposition and has found many applications, while
multivariate SSA was mainly used in geophysics. Indeed,
we consider blind source separation algorithms based on
temporal second order statistics (AMUSE, TDSEP) as
more closely related to multivariate SSA than to other
ICA methods based on nonlinear contrast functions.

While we discussed also a number of other applications
and test models, our main test problem was the ECG of
a pregnant woman, and the task was mainly to extract
a clean fetal ECG. We have chosen this partly because
this ECG was already used in previous ICA analyses
[5, 6, 43]. We believe that our method clearly outper-
formed these and gives nearly perfect results, although
we should admit that the signals to start with were al-
ready exceptionally clean. It would be of interest to see
how our method performs on more noisy (and thus more
typical) ECGs. Obtaining fetal ECGs should be of con-
siderable clinical interest, although it is not practiced at
presence, mainly because of the formidable diﬃculties to
extract them with previous methods. In this respect we
should mention the seminal work of [60, 61] where fetal
ECGs were extracted even from univariate signals using
locally nonlinear methods. It would be interesting to see
how our method compares with such a nonlinear method
when the latter is used for multivariate signals.

Throughout the paper, we used total MI as a contrast
function. One might a priori think that the sum of all
pairwise MIs would be easier to estimate, and could be

16

as useful as the total MI. Neither is true. One reason
for the the eﬃciency of our algorithm is that changes of
the total MI under linear remixings can be estimated by
computing only pairwise MIs (except for the method of
Sec. V.A with embedding dimension m > 2). Thus one
needs to compute the full high-dimensional MI only once.
For all changes during the minimization, computing pair-
wise MIs is suﬃcient. But this does not mean that total
MI is essentially a sum of pairwise MIs. We showed in
the appendix that this can be very wrong. And we found
in more realistic applications that the sum over all pair-
wise MIs sometimes increases when we minimize total
MI. Therefore we consider the sum over all pairwise MIs
as a very bad contrast function.

This is somewhat surprising if one considers ICA as a
generalization of PCA. PCA can be viewed as minimal-
ization of the sum over all squared pairwise covariances.
But we believe that this close relation between ICA and
PCA is somewhat misleading anyhow. It is usually based
on this analogy that the data are ﬁrst pre-whitened, be-
fore the ICA analysis proper is made, which is then re-
stricted to pure rotations. We showed by means of a
counter example that this can lead to a solution which
does not have minimal MI. This was a rather artiﬁcial
example, and the problem might not be serious in prac-
tice (all our results were obtained, for simplicity, with
prewhitening). But one should keep it in mind in future
applications.

Finally we should point out that Eqs. (9,10) hold for
the exact MI, but are only approximately true for our es-
timators. Therefore, working directly on higher dimen-
sional MIs, without breaking their changes down to 2-
dimensional contributions, can give slightly diﬀerent re-
sults. We found no big systematic trends, although we
expect in general that estimates using the smallest di-
mensions are most reliable. The reason is that they are
based on smaller distances for ﬁxed k, or use larger k
when using the same distances. The ﬁrst reduces sys-
tematic errors, the second statistical ones. The decrease
in CPU time when using Eq. (10) to decrease the eﬀective
dimensionality is a further important point.

VII. CONCLUSION

In the ﬁrst part of the paper we discussed the classical
linear instantaneous ICA model and introduced a new
algorithm which shows better results than conventional
ICA-algorithms. Our algorithm should be particularly
useful for real world data, since it works with actual de-
pendencies between reconstructed sources (as measured
by mutual informations), and thus easily allows to study
the question how independent and unique are the found
components.

In the following we discussed the case where outputs
can be grouped together for a meaningful interpretation.
We again saw that MI has some properties which makes
it the ideal contrast function, also for this purpose.

17

Finally, when we included time domain structures, we
again could use the same estimates of MI, with basically
the same algorithms. This – and the excellent results
when applied to a fetal electrocardiogram – suggest that
our method of basing independent component analysis
systematically on highly precise estimates of MI is very
promising. It is true that our method is slower than ex-
isting algorithms like FastICA or JADE, but we believe
that the improved results justify this eﬀort in many situ-
ations, in particular in view of the ever-increasing power
of digital computers.

Appendix

In this appendix we give two counter examples showing
somewhat counterintuitive features of the MI. In the ﬁrst
example we have two continuous variables, and the joint
density is constant in an L-shaped domain

D = {[0, lx] × [0, ǫ] ∪ [0, ǫ] × [0, ly]}.

(40)

It is zero outside D. It is easily seen that I(X, Y ) → h in
the limit ǫ → 0, with h = p ln p+(1−p) ln(1−p) and with
p = lx/(lx + ly). In this limit, the marginal distributions
are superpositions of a delta peak at x or y equal to zero,
and a uniform distribution on [0, 1]. The components

have relative weights lx : ly. The only information about
y learned by ﬁxing x is on which arm the pair (x, y) is
located, and for this h bits are suﬃcient.

On the other hand, any linear transformation applied
to the (x, y)-plane would give an L-shaped ﬁgure with at
least one oblique arm. For such a distribution knowing x
would specify y with an accuracy ∼ ǫ, and thus I(X, Y ) ∼
− ln ǫ → ∞ for ǫ → 0. But the covariance between X and
Y is not zero, hence the minimal MI is reached (for small
ǫ) when the correlation coeﬃcient r is non-zero. A more
detailed analysis shows that I(X, Y ) of the distribution
rotated by an angle φ is not symmetric under φ → −φ,
if lx 6= ly.

The second example is one of three random variables
X, Y , and Z which are pairwise strictly independent, but
globally dependent. For simplicity, the example uses dis-
crete and indeed binary variables. We have thus 8 prob-
abilities p(x, y, z) for each variable being either 0 or 1,
and we chose them as p(0, 0, 0) = p(1, 1, 0) = p(0, 1, 1) =
p(1, 0, 1) = 1/8+ǫ and p(0, 0, 1) = p(0, 1, 0) = p(1, 0, 0) =
p(1, 1, 1) = 1/8 − ǫ. For this choice all pairwise probabil-
ities are 1/4, but I(X, Y, Z) 6= 0.

Acknowledgements: We thank Drs. Ralph Anrzejak,
Thomas Kreuz, and Walter Nadler for numerous discus-
sions. H.S. thanks also Andreas Ziehe for invaluable dis-
cussions and comments.

[1] A. Hyv¨arinen, J. Karhunen, and E. Oja, Independent

Component Analysis (Wiley, New York 2001).

[2] A. Cichocki, and S. Amari, Adaptive Blind Signal and
Image Processing: Learning Algorithms and Applications
(Wiley, 2002).

[3] F.R. Bach, M.I. Jordan, J. Machine Learning Res. 4,

[4] A. Hyv¨arinen, P.O. Hoyer and M. Inki, Neural Compu-

1205, (2003).

tation 13, 1525 (2001).

[5] J.-F. Cardoso, “Multidimensional independent compo-

nent analysis”, Proceedings of ICASSP ’98 (1998).

[6] F. Meinecke, A. Ziehe, M. Kawanabe, and K.-R. M¨uller,

IEEE Trans. Biomed. Eng. 49, 1514 (2002).

[7] Stefan Harmeling, Frank Meinecke, and Klaus-Robert
M¨uller, Analysing ICA component by injection noise, In
Proc. Int. Workshop on Independent Component Analy-
sis (ICA 2003), 2003.

[8] J. Himberg and A. Hyv¨arinen. Icasso: software for in-
vestigating the reliability of ICA estimates by clustering
and visualization. Proceedings of the Workshop on Neu-
ral Networks and Signal Processing (NNSP’03), 2003
[9] A. Kraskov, H. St¨ogbauer and P. Grassberger, Phys. Rev.

E, in press.

38, 54 (1976).

[10] O. Vasicek, Journal of the Royal Statist. Soc., Series B,

[11] D.T. Pham, IEEE Trans. Signal Process. 48, 363 (2000).
[12] E.G. Learned-Miller and J.W. Fisher III, J. Machine

Learning Res. 4, 1271 (2003).

[13] L.F. Kozachenko and N.N. Leonenko, Probl. Inf. Transm.

23, 95 (1987).

[14] A. Kraskov, H. St¨ogbauer, R. Andrzejak and P. Grass-
berger, preprint arXiv.org/abs/q-bio.QM/0311039; sub-
mitted to Bioinfomatics (2003).

[15] T.M. Cover and J.A. Thomas, Elements of Information

Theory (Wiley, New York 1991).

[16] P. Grassberger, Phys. Lett. 107 A, 101 (1985).
[17] R.L. Somorjai, “Methods for Estimating the Intrinsic Di-
mensionality of High-Dimensional Point Sets”, in Dimen-
sions and Entropies in Chaotic Systems, G. Mayer-Kress,
Ed. (Springer, Berlin 1986).

[18] J.D. Victor, Phys. Rev. E 66, 051903-1 (2002).
[19] A. Kaiser and T. Schreiber, Physica D 166, 43 (2002).
[20] A. Ziehe, P. Laskov, G. Nolte, and K.-R. M¨uller, A fast
algorithm for joint diagonalization with application to
blind source separation, BLISS technical report (2003).

[21] F.R. Bach, M.I. Jordan, J. Machine Learning Res. 3, 1

(2002).

[22] S. Amari, A. Cichocki and H.H. Yang, A new learning
algorithm for blind source separation, in D.S. Touretzky
et al. eds., Advances in Neural Information Processing
8 (Proc. NIPS’95), pp. 757-763 (MIT Press, Cambridge,
MA, 1996).

[23] http://www.jokes.thefunnybone.com/waves/
[24] J.-F. Cardoso and A. Souloumiac, IEE Proceedings F

140, 362 (1993).

[25] Notice that this value is much lower than the pairwise
estimate ˆI(ˆs1, ˆs2)
4 shown in Fig. 3, which seems to
contradict the claim that this MI is dominated by the de-
pendence between the ﬁrst two sources. This is explained
by the fact that ˆI(ˆs1, ˆs2) is estimated from much closer

≈

[34] E. Widjaja, C. Li and M. Garland, Anal. Chem. 75, 4499

[48] E. Ott, Chaos in Dynamical Systems (Cambridge Univ.

neighbors (working in 2 dimensions only), and thus is
able to resolve much ﬁner details.

[26] J.-H. Jiang and Y. Ozaki, Appl. Spectrosc. Rev. 37(3),

[27] P. Geladi, Spectrochim. Acta B 58, 767-782 (2003).
[28] A. de Juan and R. Tauler, Anal. Chim. Acta 500, 195

[29] J. Chen and X.Z. Wang, J. Chem. Inf. Comput. Sci. 41,

321 (2002).

(2003).

992 (2001).

[30] J.Y. Ren, C.Q. Chang, P.C.W. Fung, J.G. Shen, and
F.H.Y. Chan, J. Magn. Res. 166(1), 82-91 (2004).
[31] E. Visser and T.W. Lee, Chemometr. Intell. Lab. Syst.

70(2), 147-155 (2004).

[32] W. Windig and J. Guilment, Anal. Chem. 63, 1425

[33] D.S. Bu and C.W. Brown, Appl. Spectrosc. 54, 1214

(1991).

(2000).

(2003).

[35] NIST Mass Spec Data Center, S.E. Stein, director, ”In-
frared Spectra” in NIST Chemistry WebBook, NIST
Standard Reference Database Number 69, Eds. P.J. Lin-
strom and W.G. Mallard, March 2003, National Institute
of Standards and Technology, Gaithersburg MD, 20899
(http://webbook.nist.gov).

[36] The matrix elements Aij were uniformly chosen from the
interval [0, 1]. Thus they are not normalized and give only
relative concentrations, but this is irrelevant.

[37] M.D. Plumbley and E. Oja, IEEE T. Neural Networ.

15(1), 66 (2004).

[38] In case of noisy and less smooth spectra, the derivatives
might be taken using more sophisticated approximations,
e.g. A. Savitzky and M.J.E. Golay, Anal. Chem. 36(8),
1627 (1964).

[39] A. Cichocki and P. Georgiev, IEICE T. Fund. Electr. EA

E86A(3), 522 (2003).

[40] S.A. Astakhov et al., to be published.
[41] Although the generalizations to multidimensional ones
would be in principle straightforward, we shall in the
following discuss only the uniqueness of 1-d output com-
ponents. In particular, we shall treat even those output
channels as 1-dimensional which are not independent.
This might seem a bit schizophrenic, but it is easier to
discuss and nothing is lost in comparison with the case
where only independent clusters are checked for unique-
ness.

[42] The curves shown in Fig. 7 seem to be nearly symmetric
around the minimum. But we show in the appendix that
this need not be the case in general.

[43] Lathauwer, L.D., Moor, B.D., Vandewalle, J.: Fetal elec-
trocardiogram extraction by source subspace separation.
In: Processing of HOS, Aiguabla, Spain (1995)

18

[44] B.L.R. De Moor (ed), “Daisy: Database for the identi-
ﬁcation of systems”, www.esat.kuleuven.ac.be/sista/daisy
(1997).

[45] We are aware of the fact that the beat rate seen in Fig. 9
sems much too high, suggesting that the ECG was in-
deed sampled with
200 Hz. But the value of 500 Hz
was conﬁrmed by the authors maintaining the URL [44].
Anyhow, our conclusions are independent of the actual
sampling rate.

≈

[46] A. Ziehe, and K.-R. M¨uller. “TDSEP - an eﬃcient algo-
rithm for blind separation using time structure”, in L.
Niklasson et al., eds., Proceedings of the 8th Int’l. Conf.
on Artiﬁcial Neural Networks, ICANN’98, p. 675 (Berlin,
1998).

[47] H. Kantz and T. Schreiber, Nonlinear Time Series Anal-
ysis (Cambridge Nonlinear Science Series No. 7, Cam-
bridge University Press, 1997).

Press, Cambridge, UK, 1993).

[49] D.S. Broomhead and G.P. King, Physica D 20, 217

[50] A. Hyv¨arinen and P.O. Hoyer, Neural Computation 12,

[51] A.K. Jain and R.C.Dubes, Algorithms for Clustering
Data (Prentice Hall, Englewood Cliﬀs, NJ, 1988).
[52] A.J. Bell and T. Sejnowski, Neural Computation 7, 1129

(1986).

1705 (2000).

(1995).

[53] L. Tong, R.-W. Liu, V.C. Soon, and Y.-F. Huang, IEEE

Trans. on Circuits and Systems 38, 499 (1991).

[54] K.-R. M¨uller, P. Philips, and A. Ziehe, “JADETD: Com-
bining higher-order statistics and temporal information
for blind source separation (with noise)”. In J.F. Car-
doso, Ch. Jutten, and Ph. Loubaton, editors, ICA ’99,
pages 87-92 (Aussois, 1999).

[55] When estimating the individual MIs on the right hand
side, one should pay attention that the same neighbors
are used, i.e. one should not use the same value of k in
each term (see also footnote [25]).

[56] L. Molgedey and H.G. Schuster, Phys. Rev. Lett. 72,

[57] C.L. Keppenne and M. Ghil, Intl. J. Bifurcations and

3634 (1994).

Chaos 3, 625 (1993).

[58] M. Ghil et al., Reviews of Geophysics 40(1), 1003,

doi:10.1029/2000RG000092 (2002).

[59] R. Vautard and M. Ghil, Physica D 35, 395 (1989); 58,

[60] T. Schreiber and D. T. Kaplan, Phys. Rev. E 53, 4326

95 (1992).

(1996).

[61] M. Richter, T. Schreiber, and D. T. Kaplan, IEEE Trans.

Bio-Med. Eng. 45, 133 (1998).


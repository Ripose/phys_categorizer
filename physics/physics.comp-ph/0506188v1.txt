5
0
0
2
 
n
u
J
 
4
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
8
8
1
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Genetic algorithms for the numerical solution of
variational problems without analytic trial functions

Carlos D. Toledo
Center for Intelligent Systems, Monterrey Institute of Technology
Campus Monterrey, Monterrey, Nuevo Le´on 64849, M´exico
(Dated: June 24, 2011)

A coding of functions that allows a genetic algorithm to minimize functionals without analytic
trial functions is presented and implemented for solving numerically some instances of variational
problems from physics.

I.

INTRODUCTION

The genetic algorithm (GA)1,2,3 has become popular as
as a reliable computerized method for solving problems
from a wide range of domains, such as function optimiza-
tion, handling them even in nonlinear, multidimensional
search spaces. A conventional GA is a stochastic search
method inspired in natural selection and the darwinian
idea of the survival of the ﬁttest in which the possible
solutions of a problem (metaphorically the points in the
search space) are coded as ﬁxed-length strings of char-
acters of an alphabet (usually binary) that resemble the
chromosomes of alive beings. A GA evolves a population
of search “points” chosen randomly appliying iteratively
on them operators of selection, crossover and mutation
for creating new populations (generations).

Selection consists in giving a proportionally bigger
number of oﬀspring to the ﬁtter individuals so the char-
acteristics that make them better prevail. The combina-
tion of this characteristics for generating new individuals
is achieved through crossover, that is the interchange of
portions of the strings of characters of two individuals
paired randomly, giving birth to two new individuals for
the next generation.
In its simplest form in a GA all
individuals are removed (die) after reproduction.

The last iterative step consists in making random
changes to the strings of individuals chosen with a small
probability, which is named mutation after the natural
process that it resembles. After some generations the in-
dividuals tend to concentrate around the ﬁttest “points”
in the search space, so it can be said that all of the pro-
cess was a way of optimizing the function employed to
determine the ﬁtting.

The predominant kind of optimization problems at-
tacked with GAs have been those in which the strings
code literally points in a multidimensional space, where
each dimension represents a parameter or variable of in-
terest. When the potential solutions of a problem are
functions (as it is the case for variational problems4),
not points, the most popular GA approach developed
to date has been that of choosing a set of analytic trial
functions and combining them in the ﬁttest way. There
are two main ways for doing so: weightening them, case
where a string of weights is an individual, or using ge-
netic programming (GP)5 where the trial functions and
the mathematical operators needed for combining them

are the alphabet that gives shape to each member of the
population.

In this paper a way to directly represent numerical
functions as strings (individuals) of a GA is presented,
followed by its successful implementation on some in-
stances of variational problems from physics.

II. ANGULAR GENES ARE NOT REAL GENES

Lets take G as the alphabet chosen to code individuals.
In the literature of GAs (without taking GP into account)
two main alphabets are usually discussed, the so called
binary genes G = 0, 1 and the real genes G = ℜ. Even
when there is not any special restriction on G in the
deﬁnition of the GA, only the implicit warning that it
must facilitate the heredity of the ﬁttest characteristics
for obtaining acceptable results, it is curious how the
attention in the ﬁeld has been biased toward the binary
and real alphabets. One of the goals of this paper is to
emphazise the importance of focusing attention in other
alphabets, in the extra information that is possible to get
from them, speciﬁcally in an angular one, which will be
called from now on angular genes. Aren’t angular genes
just real genes? The distinction made is based in the
commonly forgotten fact that angles are not numbers,6
they are an entity by themselves.

The angular genes code piecewise functions as a string
α of the angles between each consecutive pair of linear
segments. For any combination of angles it is possible
to scale and rotate the collection to ﬁt the initial and
ﬁnal desired values. Taking y1 = y(x1) and yN = y(xN )
as the initial and ﬁnal values of the piecewise function
yk = y(xk) to be represented in the range (x1, xN ) with
N − 1 linear segments and k = 2, 3, . . . , N , the coding is
deﬁned as follows:

R =

(yN − y1)2 + (xN − x1)2

p

β = tan

−1[(yN − y1)/(xN − x1)]

r2 =

si−1 cos

αj

si−1 sin

αj

i−1





j=1
X

2

+

N





i=2
X









i−1





j=1
X

2









N





i=2
X

yk =

R/r

si−1 sin

i=2 (cid:16)
X

(cid:17)

αj − γ + β

+ y1

(2)

A. Curve of shortest distance in the Euclidian
plane

















γ = tan

−1

si−1 cos

αj

N

i=2
P





i−1

 

j=1
P

!,

N

i=2
P

si−1 sin

αj

i−1

 

j=1
P

!





xk =

R/r

si−1 cos

i=2 (cid:16)
X

(cid:17)

αj − γ + β

+ x1

(1)

k

k

i−1





j=1
X

i−1





j=1
X

Where −σ ≤ αj ≤ σ and s is a string of real numbers
0 < si ≤ 1 that codes the relative length of each linear
segment and together with α forms an individual. Having
(1) and (2) we can further deﬁne:

∆xi =

R/r

si cos

αj − γ + β





j=1
X

i

i





j=1
X

(cid:16)

(cid:17)

R/r
(cid:16)

(cid:17)

∆yi =

si sin

αj − γ + β

′
i = tan

y

αj − γ + β

i





j=1
X





′′
i = tan(αi+1)(1 + y

′
iy

′
i+1)/∆xi

y

(3)

(4)

Taking Θi = Θ(xi) as the evaluation in xi of the func-
tion that minimizes the functional, for the best found
individual we have:

αi+1 − tan

−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Θ′
1 + Θ′

i+1 − Θ′
iΘ′

i

(cid:18)

i+1 (cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Equation (5) is a measure of the error of the best ap-
proximation found that clariﬁes the inﬂuence of a proper
choice of σ according to the problem. If σ is too small
the error can be surely surpassed but if it is too big the
search space grows.

Ignoring the diﬀerences in a tenth of σ and 1 the search
space explored has a size of approximately 10N −1 ×
20N −1.

III. EXAMPLES

2

The presented coding was used to solve instances of
four well known variational problems from physics.
In
each case the population used had a size of 100 with N =
101, probability of mutation of 0.05 and 500 generations.
The number of runs made for all cases was ten. For
the three ﬁrst cases it was not needed the help of s, so
si = 1, but not for the last where 0 < si ≤ 1. The
algorithm was written in MATLAB and implemented in
a personal computer with Pentium(R)4 CPU, 2.4GHz,
448 MB RAM.

The functional to minimize is

J =

(dx)2 + (dy)2

xN ,yN

x1,y1

Z

p
whose known solution4 is the straight line y = ax + b.
For the case x1 = y1 = yN = 0, xN = 1 with minimum
J = 1, the average solution found by the algorithm with
σ = 0.005π was J = 1 + 1.66 × 10−5 with standard
deviation of 1.2 × 10−6.

B. Curve of minimum revolution area

Considering two paralel coaxial wire circles to be con-
nected by a surface of minimum area that is generated by
revolving a curve y(x) about the x-axis, the functional to
minimize is

J =

2πy

(dx)2 + (dy)2

xN ,yN

x1,y1

Z

p

whose known solution4 is the catenoid y = cosh(ax +
b)/a. For the case −x1 = xN = 0.5, y1 = yN = 1 with
minimum J = 5.9917, the average solution found by the
algorithm with σ = 0.005π was J = 5.9919 with standard
deviation of 1.4 × 10−5.

C. Fermat’s principle

< ε

(5)

path y(x) for which

According to Fermat’s principle light will follow the

J =

n(x, y)

(dx)2 + (dy)2

xN ,yN

x1,y1

Z

p

is minimum when n is the index of refraction. When
n = ey the solution is y = ln(a/ cos(x + b)). For the
case −x1 = xN = 1, y1 = yN = 1 with minimum J =
4.5749, the average solution found by the algorithm with
σ = 0.01π was J = 4.5752 with standard deviation of
5.2 × 10−5.

3

D. The energies of the hydrogen atom

solution of E3 = −1.5103 eV with standard deviation of
0.001 eV. In the three cases σ = 0.005π.

The hydrogen atom7 is the quantum system made of
a proton and an electron whose energies, without taking
into account the degeneracies, can be found minimizing
the functional

En =

∞

1
c

0
Z

~2

2µ

(u

′
n)2 +

n(n − 1)~2
2µr2

−

q2
4πε0r

u2
n

dr

(cid:21)

(cid:18)

(cid:20)
∞
0 u2
R

(cid:19)
ndr, un(r) = rRn, un(0) = 0, R2

with c =
n is the
probability distribution for the radial location of the elec-
tron, q is its charge, µ = memp/(me + mp) the reduced
mass of the system, ε0 the permitivity of free space and ~
the Planck’s constant divided by 2π. The energies of the
system are ruled by the equation En = −13.6052 eV/n2.
The algorithm was used to ﬁnd the three ﬁrst ener-
gies of the system. In this case the result found by each
run depends strongly in a right choice of rN such that
un(rN )2 ≈ 0. For the ground state E1 = −13.6052 eV
the best found was about eight times the Bohr radius,
the average solution found by the algorithm was E1 =
−13.5987 eV with a standard deviation of 0.028 eV. For
n = 2, E2 = −3.4014 eV, the best choice for rN made
was about ﬁfteen times the Bohr radius and the average
solution found by the algorithm was E2 = −3.42467 eV
with standard deviation of 0.005 eV. For n = 3, E3 =
−1.5117 eV, the best choice for rN made was about
twenty ﬁve times the Bohr radius, reaching an average

IV. CONCLUSIONS AND FUTURE WORK

The examples shown were chosen with demostrative
purposes. Better aproximations for speciﬁc cases can be
reached increasing N and improving the choice of σ, with
the extra computational eﬀort it implies. Even thought
that it was showed the eﬃciency of the coding to mini-
mize the functionals presented it will be necessary the de-
velopment of a theory of diﬃculty to give a more concise
explanation of the kind of problems that could be hard to
solve using it, like those already existent for binary genes
like deception 1,2 and NK landscapes 8. Another useful fu-
ture development will be that of general ways of handling
problems with constraints. An important potential ap-
plication of the kind of genetic algorithm presented would
be in those cases where there are not analytic solutions
available, like in many quantum systems.

Acknowledgments

I wish to acknowledge Dr. Hugo Alarc´on for introduc-
ing me to the idea of solving variational problems using
genetic algorithms.

1 D. E. Goldberg, Genetic Algorithms in Search, Optimiza-

tion and Machine Learning (Addison Wesley, 1989).

2 D. E. Goldberg, The Design of Innovation. Lessons from
and for Competent Genetic Algorithms (Kluwer Academic
Publishers, 2002).

3 D. Whitley, Tech. Rep. CS-93-103, Colorado State Univer-

sity (1994).

4 G. B. Arfken and H. J. Weber, Mathematical Methods for

Physicists (Hartcourt Academic Press, 2001).

5 J. R. Koza, Genetic Programming: On the Programming of

Computers by the Means of Natural Selection (MIT Press,
Cambridge, MA, 1992).

6 G. Lakoﬀ and R. E. N´u˜nez, Where Mathematics Comes
From: How the Embodied Mind Brings Mathematics into
Being (Basic Books, 2000).

7 C. Cohen-Tannoudji, Quantum Mechanics (Wiley, 1977).
8 S. A. Kauﬀman, The Origins of Order. Self-Organization
and Selection in Evolution (Oxford University Press, 1993).


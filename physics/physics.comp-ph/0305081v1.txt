2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003 1

AMANDA - ﬁrst running experiment to use GRID in production

T. Harenberg, K.-H. Becker, W. Rhode, C. Schmitt
University of Wuppertal, Fachbereich Physik, 42097 Wuppertal, Germany

3
0
0
2
 
y
a
M
 
9
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
1
8
0
5
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The Grid technologies are in ongoing development. Using current Grid toolkits like the Globus toolkit [1] gives
one the possibility to build up virtual organizations as deﬁned in [2]. Although these tookits are in still under
development and do not feature all functionality, they can already now be used to set up an eﬃcient computing
environment for physics collaborations with only moderate work. We discuss in this paper the use of such a
computing structure in two running experiments - the AMANDA (AMANDA = Antarctic muon and neutrino
detector array) neutrino telescope and the DØ experiment at Tevatron, Fermilab. One of the main features of our
approach is to avoid reprogramming of the existing software which is based on several programming languages
(FORTRAN, C/C++, JAVA). This was realized with software layers around the collaboration software taking
care about in- and output, user notiﬁcation, tracking of running jobs, etc. A further important aspect is the
resolution of library dependencies, which occur when a user runs self-compiled jobs on machines, where these
libraries are not installed. These dependencies are also resolved with this layers.

1. Introduction

AMANDA is a running neutrino telescope situated
at the south pole.
It’s collaboration members are
spread throughout the world, mainly in North Amer-
ica and Europe. Our aim was to help this collab-
oration to create a user-friendly, uniﬁed access to
the standard software repository using exiting GRID
toolkits. Furthermore, the spreaded computing power
of the participating institutes should be not united,
but the access to foreign resources should be uniﬁed,
to give any single physicists within the collaboration
the possibility to have appropriate computing power
available when needed.

Like in other experiments, the standard simula-
tion software within AMANDA has a grown structure
and consists of many parts written by several people
and in several programming languages (FORTRAN,
C/C++, JAVA). To use the GRID software struc-
tures, some reprogramming would be needed, which
is diﬃcult in running experiments. We show, how we
solved this with an approach, which required no re-
programming of the existing software.

At DØ the situated is diﬀerent: the experiment has
a data access system SAM1[3] which implements some
of the basic GRID ideas. Our emphasis here was to
show that for some analyses our system can be used
as a queuing system to prove that GRID- and non-
GRID-parts work smoothly together.

In this paper we start by brieﬂy summerizing the
basic idea of the GRID and its diﬀerent layers and of
the diﬀerent protocols used in such an environment.
After that, we describe the GRID system which has
been build up at our institute and extended to further
collaboration members.

Afterwards, the parts of the collaboration software
are identiﬁed which have to be rewritten to use this

1SAM = Sequential data Access via Meta-data

MOAT010

GRID infrastructure. To minimize the changes we
use a diﬀerent approach at AMANDA, which is being
presented. And we show that also within DØ our
system can be used. Finally, we present the graphical
user interface (GUI) which has been developed to give
the physicists an easy access to the GRID.

2. An Introduction to GRID

This basic idea of “The Grid” is deﬁned in the
Book The Grid: Blueprint for a New Computing In-
frastructure by I. Foster and C. Kesselman[4] as: In-
frastructure that enables the integrated, collaborative
use of high-end computers, networks, databases, and
scientiﬁc instruments owned and managed by multi-
ple organizations. One may compare the GRID to
the electrical power grid, where the the power plants
and the consumers are connected via a large network.
The single consumer is not interested where his en-
ergy comes from. This is one of the main topics of
building up a grid: the user should gain access to all
kind of computer resources (CPU, storage, data, . . . )
without having to care about the underlying access
procedures. The grid provides him with a uniform
access mechanisms to all this kind of resources.

To achieve this, the Globus toolkit introduces a soft-
ware structure, the so called “middleware”, which is
a software layer in between the user application and
all kinds of resources, as shown in Figure 1. Our work
is based on the Globus toolkit [1], which is a common
toolkit for GRID developments.

The “middleware” introduces

some protocols,
which are used by the participating computers to com-
municate. These protocols deal with the authentica-
tion of users, the data transfer, and the information
interchange to allocate free resources. The follow-
ing table I gives an overview of these protocols and
some of its non-GRID equivalents. Note that this ta-
ble is not complete and due to the ongoing develop-
ments things may change quickly. The last column

2 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003

Collaboration
      Tools

r
e
s
U

n
o
i
t
a
c
i
l

p
p
A

I

D
R
G

e
r
a
w
e
l
d
d
M

i

e
r
a
w
d
r
a
H

(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)

Information
Services

Resource
Management

Fault
Detection

. . .

Remote Access

Remote Monitor

• the user should be able to use the batch queuing
systems in its own institute and in other insti-
tutes participating in AMANDA without hav-
ing to care about the local infrastructure, the
access policies and the network infrastructure
(Firewalls),

Network

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:1)(cid:1)

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)

• the in- and output ﬁles of the software should
be transparent to the user, which means that for
him it makes no diﬀerent where the code runs,

Meta Data

Core Data

Storage

Figure 1: The Role of GRID Services (aka Middleware)
and Tools

shows some commands which are implemented in the
Globus toolkit to access these protocols from the shell.
Within our project all protocols are used with the ex-
ception of the Heartbeat Monitor.

Task

Standard Pro-
tocol

Grid equiva-
lent

command-line
tool(s)

Access to ma-
chines

telnet, rlogin,
rsh, ssh, . . .

GRID Secu-
rity
Infras-
tructure [5]

to

[6],

Data transfer ftp, scp, . . . Globus
Access
Secondary
Storage
(GASS)
GridFTP [7]
Metadata
Directory
Service
(MDS) [8]
Heartbeat
Monitor
(HBM) [9]

N/A (external
software
like
OpenLDAP)

N/A (exter-
nal watchdog
software)

Resource ﬁnd-
ing

Computer
monitoring

N/A

Table I Overview over Grid protocols

globusrun,
globus-job-
run, . . .
globus-url-
copy

sions:

3. A GRID for AMANDA and for DØ

Although current GRID toolkits like the used
Globus Toolkit 2 have all basic features needed to
build up a GRID included, not all ideas could be
implemented without major programming Globus
Toolkit itself.

For AMANDA, we focussed the following:

• the user should have access to a central software
repository, where the standard oﬄine software
is “ready to use”,

MOAT010

• a list of running and ﬁnished jobs should be
available to the user, this is a job not covered
by the Globus Toolkit yet,

• for mass production the generated data should
be available at the centralized data storage,

• besides standard software, “own code” should
also be possible to run within the GRID envi-
ronment, ﬁle transfer should be provided and

• the software should care that own user code
should run on remote sides even if the binary
has been dynamically linked to libraries which
are may be not installed at the remote side.

These features has been postponed for later ver-

• the system does not search an appropriate batch
system itself, as the needs of the software cannot
be guessed from the binary. This is especially
true for software programmed by the user,

As mentioned in the introduction, the software
within AMANDA shows a grown structure with a
variety of programs written in many diﬀerent pro-
gramming languages (C, JAVA, FORTRAN, . . . ). Al-
though a JAVA port of the Globus toolkit exists, en-
abling FORTRAN code to use the Globus protocols
seems to be unfeasable.

Therefore, some code has been written around the
standard software which takes care of the in- and out-
put using the GRID protocols. The standard software
runs in a kind of sandbox, where all the necessary li-
braries and ﬁles are provided, and after the end of a
job, the output is transfered back. For this reason, we
create a software server, serving the standard software
together with the code around as a bundle. Every
time a user requests a run, this software is transfered
to the executing node by the queuing system and then
executed in a temporary ﬁle space. After job termina-
tion, all ﬁles are cleared up. This is explained in more
detail in the following chapter 3.1. And we show that
we also can use our system without the Grid compo-
nents, therefore we present an example where we used
it as a queuing system for DØ.

ldapsearch

• ports to other operating systems

2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003 3

3.1. Thoughts towards a GRID in
Wuppertal

This chapter ﬁrst introduces to the situation at the
Physics Department in Wuppertal to enhance, what a
GRID should achieve. After that, the installation of a
GRID system in Wuppertal is explained and the Grid-
navigator program, which has been developed here, is
presented.

The groups in Wuppertal involved in AMANDA
and DØ experiments have a well equipped computer
infrastructure, but no centralized INTEL-CPU based
computing cluster. The aim of our work was to make
the CPU power usable, which is available in desktop
PCs, which are of Pentium-III 1 GHz class. The de-
velopment of our software was done on several PCs
running diﬀerent ﬂavors of Linux (SuSE Linux Ver-
sions from 7.3 to 8.1 and RedHat 7.x). Porting to
other platforms as for example DEC alpha has been
postponed.

To explain the diﬀerence between a GRID system
and a conventional batch queuing system we ﬁrst take
a look, how a typical conventional approach to set up
a queuing system looks like:

The machines are connected via a (fast) local area
network (LAN). One machine acts as a central server
which holds the disks and a central account service.
This disk space is - together with the account informa-
tion - exported to the cluster machines using protocols
like the Network File System (NFS) and the Network
Information Service (NIS).

The computer program is executed at one of the
cluster machines, but doing so every ﬁle (executable,
library, program data) is transfered at the moment it
is needed via the network to the executing machine.
Furthermore, every ﬁle created or changed by the soft-
ware has to be transfered “online” back to the server,
as illustrated by ﬁg. 2. This is normally not a prob-
lem in fast LANs, but on slower and less reliable wide
area networks (WANs), this structure may result in
slow job execution, high network load and may com-
pletely hold when the network is down for even a short
amount of time.

Our approach is diﬀerent and shown in ﬁg. 3.
We use the GRID protocols to gain access to the
machines and to transfer data. We bundle all the
software together with the sandbox software, as in-
troduced in chapter 3. This software is stored on a
central server which can be unique for every institute.
A central data server is set up - this is a machine with
a large RAID disk array to store the data in mass pro-
duction. After the machine which should run the soft-
ware is chosen by a queuing system (we chose Condor
for reasons described later), the software together with
all needed libraries is transfered once, so is the user
data. Then the program is executed in a temporary
ﬁle space and after the end of the run, the produced
data is transfered back to the user’s machine or the

MOAT010

conventional approach

(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)

(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)

central server

/user
/cern
/scratch

NFS, NIS, ...

(cid:1)
(cid:0)
(cid:0)
(cid:1)

(cid:1)
(cid:0)
(cid:0)
(cid:1)

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

/user/amasim.exe
/cern/pro/lib/libXX.so

Figure 2: A conventional approach to set up a batch
queuing system

our approach

central data server

(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)

(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)
(cid:0)
(cid:0)
(cid:1)
(cid:1)

(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)(cid:1)

central software server
bundles of software
AMASIM, MMC, CORSIKA, ...

CORSIKA
bundle

User data

(cid:1)
(cid:0)
(cid:0)
(cid:1)

possible institute boundaries

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

temp. Filespace

Program
Libraries
User data

data Inform.

Figure 3: The Wuppertal approach to build up a GRID

the central data server. The red lines shows the possi-
ble institute boundaries in this scenario. As the GRID
protocols gives a uniform access to the resources, the
access is the same no matter in which institute the
resource is, but the software server should be in the
same domain as the executing machine to prevent un-
necessary data transfer over WANs.

For us, the main aim of our work is to use the ex-
isting toolkits and the existing collaboration software
together and to exploit the full capacity of all the PCs
in our institute. In addition we want to use this as a
testbed to understand the gains and potential prob-
lems of such a GRID.

The Globus toolkit itself doesn’t come with a queu-
ing system, which is needed to choose a machine to
run a software bundle. But several external queuing
systems are already supported by Globus, for example

4 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003

LSF, PBS and Condor. Supported means here that
Globus knows in principle how to access these batch
systems.

For the choice of the batch system we particularly
aimed at the optimal use of our desktop PCs in our
department. This means that

• during working hours the PCs shouldn’t run any

jobs when the user is working,

• the system should care of PCs, which are
switched oﬀ for any reason without any inter-
action of the system operators,

• the system should choose a free PC on its own.

With our preconditions, the Condor queuing sys-
tem [10] seems to be the best choice. The basic Con-
dor system is a high-throughput-computing environ-
ment, whose task is to balance the jobs between ma-
chines in a way, that the idle times are minimized.
It supports a mechanism, which suspends jobs when
a running machine gets load or interactive work and
releases the job again when idle, so it ﬁts our needs.
With Condor-G [11] a GRID enabled version of Con-
dor is available, but these enhancements are not used
here. The GRID enhancements to Condor were writ-
ten to track the users’ GRID jobs – a task which
is done here in Wuppertal by our own system. As
Condor is not available as Open-Source software, one
would rely otherwise on the command line tools, which
may change.

Furthermore, Condor has a ﬁle transfer mecha-
nism included, but this requires that the job is linked
against a special library. This mechanism is not used
here, instead the GRID protocols are used to transfer
ﬁles. This has several advantages:

• using plain Condor requires the binary executa-
bles to be linked to the Condor library. This is
only possible if one has at least access to the ob-
ject ﬁles. So one has two distribute two versions
of the software, which can be a disadvantage in
a big collaboration.

• the Condor traﬃc has not to be tunneled seper-

atly through the ﬁrewalls,

Although Globus supports Condor as a batch queu-
ing system, some small modiﬁcations had to be ap-
plied to the Globus code to get access to our Condor
queue and to optimize the co-operation of these two
software packages.

3.2. The GRID system in Wuppertal

Based on the thoughts in the previous chapter, we
developed our GRID in Wuppertal in the following
way:

• besides the Globus Toolkit 2, Condor is installed

on every participating PC,

• we have one machine set up as a central software
software. There are no special requirements to
this machine,

• one machine with Globus installed and appro-
priate disk space acts as a central data server.
This machine does not need to have Condor in-
stalled, to prevent this important machine to get
high load from batch jobs,

• one machine out of the normal machine acts as
Condor server. On this machine, Globus was
conﬁgured to access the Condor queue(s).

The Condor system itself can be used by users
who don’t want or can use the GRID, but only
want to use the Condor queuing system. Both
work smoothly together. We tested this with
an example DØ t¯t crossection using the root-
Analysis-Framework [12].

Using the GRID, one gains a uniform access to
the queuing systems, so extensions to other in-
stitutes (even with queuing systems other then
Condor). That means that from the user side
of view, he can submit jobs with the same com-
mand (or within the GUI) without having to
care about how the exact queueing mechanism
on the target cluter looks like. We extended
our system to a machine at the Aachen Techni-
cal University and successfully tested the inter-
institute communication. These tests shows,
that in general modiﬁcations to existing Fire-
walls are needed. Although the range of used
TCP ports can be limited in the Globus toolkit,
it seems that in this stage of the toolkit all
ports above 1024 have to be opened. But im-
plementation of a virtual private network with
a third party software (i.e. FreeS/WAN [13])
seems however to be unnecessary. All commu-
nication between the nodes are encrypted using
the OpenSSL library [14] and access control is
only granted by presenting a valid and signed
X.509 certiﬁcate. Furthermore, the access to the
systems is always controlled by the local admin-
istrator.

4. The Gridnavigator Program

This chapter introduces the Gridnavigator soft-
ware developed in Wuppertal. The main two
goals of this program is to develop software lay-
ers (“sandboxes”) about the existing AMANDA
collaboration software and to simplify the use
of the GRID, which is quite complicated using

MOAT010

2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003 5

the standard commands given by the Globus
Toolkit.

The Gridnavigator is very modular and con-
sists of two main parts: the already introduced
sandbox software and a graphical user inter-
face (GUI). The latter was written to make all
the GRID software structure usable to physicist,
which do not have detailed knowledge about the
GRID terminology, while the sandbox software
is a layer around the standard AMANDA soft-
ware which takes care about the ﬁle transfer,
executes the AMANDA software with all the
needed libraries and informs the user about the
job status. The approach of the sandboxes has
the following advantages:

– the people developing the standard soft-
ware can keep their way of handling in- and
output, so that

– modiﬁcations (i.e. new versions) can be im-

plemented fast and

– the institutes not participating in the
GRID have the usual software structure.

– Non-default

libraries needed

the
AMANDA software are included to min-
imize installation eﬀort on the desktop
machines.

by

– And - most important - this was much less
time consuming than reprogramming parts
of the existing software.

These sandboxes are realized so far for the fol-
lowing programs:

– dCORSIKA [15] - an air shower generator

(mainly written in FORTRAN)

– MMC [16] - a code for muon propagation

in matter (written in JAVA)

– AMASIM [17]

for
AMANDA (written in C and FORTRAN)

- a simulation tool

The complete program - the sandbox and the
GUI - is written with the Python programming
language [18]. As python compiles the program
in a byte-code like JAVA [19], the program may
be easily ported to other operation systems. The
graphics are produced with the tkinter program
library [20], which is the standard library for
writing graphical user interfaces in Python and
is also available for many platforms.

The JAVA code was compiled using the new fea-
tures of the GNU C compiler gcc [21] from ver-
sion 3 on [22]. This compiles JAVA into native
machine code, which makes it unnecessary to
have a JAVA Runtime Environment (JRE) in-
stalled on every machine. We also implemented

MOAT010

Figure 4: AMANDA standard software dialog

successfully a sandbox with a JRE, but this ap-
proach makes the bundles much bigger.

Due to the modularity of the Gridnavigator soft-
ware, the user can start a software run now via
the command line or with the GUI. Picture 4
shows the input screen of the GUI to run the
AMANDA software.

The user has the option to get the output back
to his home directory or to a centralized direc-
tory at his institute (or to any other GRID re-
source he has access to). To get access to the
home directory, a local GASS server2 is started
by the Gridnavigator program. Furthermore,
the user is informed via an email about the start
and ﬁnish of his run if an email address was en-
tered. Error messages are also send being this
way.

All needed parameters like the name and direc-
tory of the local data server, the local mail server
to deliver the mails, etc. are set for each domain
seperatly in the Gridnavigator software, to en-
sure that software may run at any resource it
has been conﬁgured for - and of course to which
the user has access to.

The sandboxes can be used without the GUI,
but to GUI itself uses the sandboxes. Some op-
tions of the sandboxes are not accessible via the
GUI, but these options are not widely used. The
separation in the two parts has the advantage,
that in case of heavy mass production, the jobs
can be submitted to the batch queues with a
script or another program.

2See chapter 1 for an explanation of GASS

6 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003

In addition the AMANDA standard software,
own code is also supported, which is in gen-
eral not GRID-enabled. For this reason, another
sandbox tool has been written which transfers
the in- and output like it does with the stan-
dard software. In this case, the user has to pro-
vide the program name, and the names of the
in- and output ﬁles. See picture 6 for the in-
put screen. The Gridnavigator then takes care
about the needed libraries of this executable.
For this reason, a list of all needed libraries is
created on the submitting machine. On the re-
mote host, the sandbox code then tries to resolve
these dependencies and in case not all libraries
are present at this machine, the missing ones
are transfered directly from the submitting ma-
chine using the GASS protocol. This even works
for libraries which are not installed system-wide
on the submitting machine. See ﬁgure 5 for a
scheme of the complete mechanism. Note that
this can be more complex in special cases, where
a third machine is involved.

submitting machine

executing machine

2.) transfer executable and library information

transfer:

Executable

Information

Job:
ownanalysis.exe

needed libraries:
− libc.so
− ld−linux.so.2
− libm.so.6
− libroot.so

GASS URL

Job:
ownanalysis.exe

needed libraries:
− libc.so
− ld−linux.so.2
− libm.so.6
− libroot.so

(cid:1)(cid:1)(cid:1)
(cid:0)(cid:0)(cid:0)
(cid:1)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:1)(cid:1)

3.) check for needed libraries on remote machine

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

(cid:1)(cid:1)
(cid:0)(cid:0)
(cid:0)(cid:0)
(cid:1)(cid:1)

Information about
missing libraries

check for libraries:
− libc.so
− ld−linux.so.2
− libm.so.6
− libroot.so

1.) GASS server started
to access local Filesystem

4.) transfer missing libraries, input files and execute

the binary

libroot.so

input files

5.) transfer output files back

6.) stop GASS server

libroot.so

output files

GASS URL

Figure 5: Scheme of the library resolving mechanism and
data transfer mechanism. Special cases not illustrated.

This mechanism is in particular a big improve-
ment if one works accross boundaries of just one
institute, as it makes it unnecessary to install ev-
ery library needed by the collaboration (maybe
even only by one member of the collaboration)
at every institute and make the GRID usable
even in the case, where a user wrote his own code
using libraries not installed anywhere else then
on his local machine. Furthermore, this mech-
anism can help keeping the software repository
up-to-date, as new versions of the collaboration
software can be integrated very easy. We chose
to bundle the libraries of the standard software
for now to make the system more fault tolerant.
For his own code, the user has speciﬁed the in-
formation about in- and output ﬁles once, but

MOAT010

Figure 6: Dialog to run own code in the Grid
environment

then he gains the advantage of having his code
running even at remote side(s) without any fur-
ther modiﬁcations.

Besides this, one more part are covered by the
Gridnavigator GUI:

it tracks all the submitted jobs and informs the
user about the status of his jobs. See picture 7
for this. As the Globus toolkit can access diﬀer-
ent queuing systems, it cannot provide simply a
list of all running jobs. Instead it provides the
user with a URL3, with can be used to resolve
the status of the job. Therefore, a list of these
job-URLs is stored together with other informa-
tion by the Gridnavigator program. The user
may cancel one or more jobs.

The program also takes care about the creation
of the necessary proxy, this is a time-limited
cryptographic certiﬁcate which enables the user
to access the Grid resources. See [23] for details
of this mechanism. The user can “log-in” to the
Grid using the GUI and may specify the lifetime
of the proxy.

5. Experience and Advantages

We installed this system on 18 PCs in our insti-
tute. These have several versions of the LINUX

3URL = Uniform Resource Locator. The URL have the

same format as widely used in the worldwide web (WWW)

2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003 7

opened all TCP ports above 1024 on the in-
stitutes’ ﬁrewalls. The Certiﬁcation Authority
could be installed, so that users from one side
had automatically access to the GRID resources
on the other side. We could successfully test
our software layers (“sandboxes”), so AMANDA
software could run in Aachen, although it was
not seperatly installed. This system served 4
AMANDA diploma students in Wuppertal as
basis for their Monte Carlo production.

The example DØ analysis using the plain Con-
dor environment archived 2000 grid points per
hour.

We tested successfully the library resolving
mechanism using some software of the DEL-
PHI collaboration. DELPHI is one of the for-
mer experiments operated at the LEP electron-
positron accellarator at CERN, Geneva.

We presented a way how a existing GRID toolkit
- the Globus Toolkit 2 in our case - can be used
to establish and run a GRID system within a
physics collaboration and how the physicists can
proﬁt from these software structures.

We established successfully such a system in
Wuppertal using the Globus toolkit and the
Condor queuing system. We presented our ap-
proach to build up a GRID with a software
server and a data storage server.

We extended the system with success to other
institutes.

We showed an approach of how to run exist-
ing collaboration software, which is not GRID-
enabled yet, in such a GRID environment. We
presented the “Gridnavigator” software, which
was developed in Wuppertal. This software has
to parts: some “sandboxes” and a graphical user
interface.

The “sandboxes” are software layers around the
existing collaboration software, which takes care
about the needed libraries, the ﬁle transfer, the
user notiﬁcation, etc. A similar software was
written to make user code usable, this is code,
which has not been bundled by anybody to run
in a GRID environment and which is not GRID-
enabled. This sandbox cares - like in the previ-
ous case - about the ﬁle transfer, the execution
in a temporary ﬁle space and the user notiﬁ-
cation. Furthermore, it resolves library depen-
dencies by transferring those libraries, who are
not installed on the target system, from the ma-
chine where the job was submitted from. This

Figure 7: The main screen of the Gridnavigator program

6. Summary

Operating Systems (SuSE Linux Versions 7.3,
8.0 and 8.1 and RedHat 7.x) and diﬀerent con-
ﬁgurations concerning disk space, memory and
CPU. No general problems occurred.

One problem occurred when trying to run the
AMAsim software package. This package re-
quires quite a lot amount of memory (ap-
prox. 512 MB per instance). When such a job
was started on a PC which less then RAM, then
this disturbs the normal operation quite a lot,
due to the swapping activity of the LINUX oper-
ating system. We solved this situation by deﬁn-
ing two Condor queues, one consists of all PCs,
one only included the well-equipped ones. On
the Condor server, another jobmanager was de-
ﬁned with special parameters passed t the Con-
dor queuing system. A jobmanager is a gateway
between the GRID and a queuing system in the
Globus Toolkit. Sending AMAsim jobs to the
one queue with the special parameters and all
others to the general queue solved now our prob-
lem.

With this system, we achieved in average 200
million primary CORSIKA events per week with
one mouse click. The situation before was run-
ning CORSIKA on several Sun UltraSPARC II
workstations, where we could get around 15 mil-
lion primary events per week with a huge over-
load in administration (connect to every single
machine, start the program, transfer back the
output, etc.).

We extended the system to the Technical Uni-
versity of Aachen, where we had thanks to
Dr. Rolf Steinberg the possibility to test inter-
institute communication. This succeeded when

MOAT010

8 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California, March 24 – 28, 2003

is in most cases the machine where the software
was build on. This feature is very important
nowadays, where most of the existing software
has not been rewritten to be used in a GRID
environment.

For an easy access to the system, we presented
the Gridnavigator program, a graphical user in-
terface to our Grid system. Using that, the user
can submit bundled and own software without
having to care about the underlying GRID soft-
ware structure. Furthermore, it keeps track of
submitted jobs and gives the user the possibility
to cancel one or more of his jobs.

We tested the “Gridnavigator” with the three
main parts of the AMANDA oﬄine software
chain. These work smoothly within the GRID
environment. Furthermore, we tested the sup-
port of own user software by running code from
the DELPHI collaboration within the GRID.
And we tested, that the Condor queuing sys-
tem itself can be used without using the GRID
by running a DØ analysis run.

Our work now gives the AMANDA collaboration
the possibility to build up a “virtual organiza-
tion” by connecting the participating institutes
with the Globus toolkit and use our program to
access the GRID without knowledge of the un-
derlying GRID software layer. Jobs can be sub-
mitted to foreign institutes when local resources
does not ﬁt the needs of the user. Access to
produced mass production data can be uniﬁed.

References

[1] http://www.globus.org.
[2] I. Foster, C. Kesselman, and S. Tuecke. The
Anatomy of the Grid: Enabling Scalable
Virtual Organizations. International J. Su-
percomputer Applications, 15(3), 2001.
[3] Andrew Baranovski, Diana Bonham,
Gabriele Garzoglio,
Jozwiak,
Lauri Loebel Carpenter, Lee Lueking,
Carmenita Moore, Ruth Pordes, Heidi
Igor Terekhov, Matthew
Schellman,
Vranicar, Sinisa Veseli, Stephen White,
and Victoria White. SAM Managed Cache
and Processing for Clusters in a Worldwide
Grid-Enabled System, Proceedings of the
Large Cluster Computing Workshop 2002.

Chris

[4] Ian Foster and Carl Kesselman. The Grid:
Blueprint for a New Computing Infrastruc-
ture. Morgan Kaufmann Publishers, July
1998.

[5] I. Foster, C. Kesselman, G. Tsudik, and
S. Tuecke. A security architecture for com-
in ACM Conference on
putational grids.
Computers and Security, pages 83-91. ACM
Press, 1998.
[6] J. Bester,

I. Foster, C. Kesselman,
J. Tedesco, and S. Tuecke. GASS: A Data
Movement and Access Service for Wide
Area Computing Systems. Sixth Workshop
on I/O in Parallel and Distributed Systems,
May 1999.

[7] W. Allcock, J. Bester, J. Bresnahan,
A. Chervenak, L. Liming, S. Meder, and
S. Tuecke. GGF GridFTP Working Group
Document, September 2002.

[8] S. Fitzgerald,

I. Foster, C. Kesselman,
G. von Laszewski, W. Smith, and S. Tuecke.
A Directory Service for Conﬁguring High-
Performance Distributed Computations.
Sixth IEEE Symp. on High-Performance
Distributed Computing, 1997.

[9] P. Stelling, I. Foster, C. Kesselman, C.Lee,
and G. von Laszewski. A Fault Detection
Service for Wide Area Distributed Compu-
tations. Proc. 7th IEEE Symp. on High
Performance Distributed Computing, pages
268-278, 1998.

[10] http://www.cs.wisc.edu/condor.
[11] http://www.cs.wisc.edu/condor/condorg.
[12] http://root.cern.ch.
[13] http://www.freeswan.org.
[14] http://www.openssl.org.
[15] http://amanda.uni-

wuppertal.de/˜dima/work/CORSIKA.

[16] http://amanda.uni-

wuppertal.de/˜dima/work/MUONPR.

[17] http://www.ps.uci.edu/˜hundert/aman-

da/amasim/amasim.html.

[18] http://www.python.org.
[19] http://java.sun.com.
[20] http://www.python.org/topics/tkinter/.
[21] http://gcc.gnu.org.
[22] http://gcc.gnu.org/gcc-3.0/features.html.
[23] http://www.globus.org/Security.

MOAT010


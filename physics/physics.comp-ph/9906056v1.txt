Version of February 2, 2008

Practical Guide to Monte Carlo

S. Jadach
Institute of Nuclear Physics, ul. Kawiory 26a, Krak´ow, Poland
and
DESY, Theory Group, Notkestrasse 85, D-22603 Hamburg, Germany

Abstract

I show how to construct Monte Carlo algorithms (programs), prove
that they are correct and document them. Complicated algorithms are
build using a handful of elementary methods. This construction process
is transparently illustrated using graphical representation in which com-
plicated graphs consist of only several elementary building blocks. In
particular I discuss the equivalent algorithms, that is diﬀerent MC algo-
rithms, with diﬀerent arrangements of the elementary building blocks,
which generate the same ﬁnal probability distribution. I also show how
to transform a given MC algorithm into another equivalent one and
discuss advantages of the various “architectures”.

To be submitted somewhere, sometime (or may be not)

9
9
9
1
 
n
u
J
 
9
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
6
5
0
6
0
9
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1 Introduction

The aim of this report is to provide:

• Elementary description of elementary Monte Carlo (MC) methods for a
graduate student in physics, who is supposed to learn them in a couple
of days,

• Methods of transparent documenting of the MC programs,

• Reference in publications where there is no space for description of the

elementary MC methodology.

In my opinion there is certain gap in the published literature on the MC
methods. The elementary MC methods like rejection according to weight,
branching (multichannel method) or mapping of variables are so simple and
intuitive that it seems to be not worth to write anything on them. On the other
hand in the practical MC applications these methods are often combined in
such a complicated and baroque way that sometimes one may wonder if the
author is really controlling what he is doing, especially if the documentation
is incomplete/sparse and we lack commonly accepted terminology graphical
notation for describing MC algorithms. There are also many mathematically
oriented articles and textbooks on the MC methods which in my opinion seem
to have very little connection with the practical every day work of someone
constructing MC program. The aim of this report is to ﬁll at least partly
this gap. This report is extension of a section in ref. [1]. I would like also to
recommend the classical report [2] of James on the elementary MC methods.
Section 1 describes elementary MC methods of the single and multiple
level rejection (reweighting), including detailed description of the weight book-
keeping and recipes for keeping correct normalisation for the total integrand
and the diﬀerential distributions (histograms). Section 2 introduces branching
(multi-channel) method and section 3 demonstrates the simplest combinations
of the rejection and branching. Section 4 and 5 reviews more advanced as-
pects of combining rejection and branching, in particular I show examples of
“equivalent” algorithms, i.e. diﬀerent algorithm which provide the same distri-
butions, pointing out advantages of certain arrangements of the the rejection
and branching. Another common method of the variable mapping is discussed
in section 5, again in the context of various arrangement of the rejection and
branching methods.

1

2 Rejection, compensating weights

We intend to generate randomly events, that is points x = (x1, x2, x3, ..., xn),
according to a distribution

within certain domain Ω and, simultaneously, we want to calculate (estimate)
the integral

ρ(xi) =

dnσ
dxn (xi)

σ =

ρ(xi) dxn.

Z
Ω

ρ(yi) =

ρ(xi).

∂x
∂y (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

w(x) =

ρ(xi)
ρ(1)(xi)

=

dnσ
dnσ(1) .

σ(1) =

ρ(xi)(1) dxn.

Z
Ω

as precisely as possible. In our notation, change of integration variables induces
in ρ-density a Jacobian factor1

The normalised to unity probability density is simply

1
σ

dnp =

dnσ,

dnp =

Z
Ω

Z
Ω

dnp
dxn dxn = 1.

In ﬁg. 1(a) I show the simple single-line algorithm with rejection according

to a weight2 deﬁned as a ratio of an exact distribution ρ to approximate ρ(1)

We assume that we are able to generate randomly events according to ρ(1) and
we know the numerical value of the integral

(1)

(2)

(3)

(4)

(5)

(6)

1 I assume that the reader is familiar only with the elementary calculus methods, and

purposely avoid to call ρ a measure.

2 I assume that the distribution ρ may be quite singular, in particular it may include
Dirac δ functions. The weight I expect to be analytical almost everywhere. It may, however,
include discontinuities.

2

(a)

(cid:27)

?
ρ(1)(xi)

ρ(1)(xi)
?
(cid:27)(cid:24)w
(cid:26)(cid:25)
ρ(xi)
?

6

-

(b)

?
ρ(1)(xi)

ρ(1)(xi)
?
(cid:27)(cid:24)w
(cid:26)(cid:25)
ρ(xi)
?

Figure 1: Single-line MC algorithm: versions (a) constant-weight events and (b)
variable-weight events.

The box ρ(1)(xi) represents part of the algorithm (part of computer code)
which provides us events according to ρ(1) and the value of σ(1). The content
of the box ρ(1)(xi) can be a complicated algorithm (computer code) and we
treat it as a “black box”, i.e. we may know nothing about its content.
In
particular it can be taken from ready-to-use library of the programs generating
standard distributions, or even a physical process providing “random events”.
The circle with the return line depicts rejection method. For each event leaving
ρ(1)(xi) we calculate weight w and we accept event (downward arrow) if

rW < w,

(7)

where W is a maximum weight and r is a uniform random number 0 < r < 1.
Otherwise event is rejected (return arrow in the graph). It is easy to see that
events exiting our algorithm are generated according to density ρ. Probability
density of accepted events at the point xi is equal to product of probability
dnp(1) of events produced in the box ρ(1) times the probability paccept = w(x)/W

3

of accepting an event

dnp(x) = N dnp(1) paccept = N

dnσ(1)(x)
σ(1)

w(x)
W

,

where N is normalisation factor. Substituting the deﬁnition of the weight
dnp(x) = 1 we
w(x) = dnσ/dnσ(1) and imposing normalisation condition
ﬁnd

R

dnp(x) =

N
W

dnσ(x)
σ(1)

,

N =

W σ(1)
σ

and as a result we obtain

dnp(x) =

dnσ(x)
σ

as desired. The dashed box ρ(xi) can be used as part in a bigger algorithm (box
in a bigger graph) because it provides events generated according to density
ρ(xi). The question is whether within the dashed box we are able to estimate
the integral σ. In fact we can, and there are even two ways to do it. In the
ﬁrst method we use the ratio on accepted events N to the total number N (1)
of the events generated in the box ρ(1)(xi) . The number of accepted events
is, on the average, proportional to probability of generating an event event
dnσ(1)/σ(1) times probability of accepting an event

averaged all over the points xi in the entire integration (generation) domain Ω

N = N (1)

Z
Ω

dnσ(1)
σ(1)

¯w = N (1) σ

¯σ(1) ,

¯σ(1) = W σ(1).

(12)

The above relation can be used to calculate the unknown integral σ as follows

using known σ(1) and counting accepted events N. Of course, the error of the
above estimator of σ is given by the usual statistical error from the binomial

¯w =

w(x)
W

,

σ = ¯σ(1) N

N (1) .

4

(8)

(9)

(10)

(11)

(13)

(14)

(15)

(16)

(17)

(18)

distribution. In the second method we calculate the average weight where the
averaging is done over all accepted and rejected events

The above gives us second equivalent estimator of the unknown integral σ in
terms of the known σ(1) and the measured average weight

< w >=

Z
Ω

dnσ(1)
σ(1) w(x) =

σ
σ(1) .

σ = σ(1) < w >= ¯σ(1) < ¯w > .

Another often asked question is: how to calculate the integral ∆σ over a
subdomain ∆Ω, which is for instance a single bin in a histogram? The following
formula can be easily derived

where ∆N is number of events falling into subdomain ∆Ω. A particular case
is the proper normalisation of the histogram. Let us take the one dimensional
distribution

∆σ = σ

∆N
N

= ¯σ(1) ∆N
N (1)

=

dnσ δ(z − z(xi))

dσ
dz

Z
Ω

which we estimate/calculate by means of collecting generated events in a his-
togram with nb equal bins within a (zmin, zmax) range. The relevant formula
reads

dσ
dz

≃

σ∆N
∆zN

=

nbσ∆N
(zmax − zmin)N

=

nb¯σ(1)∆N
(zmax − zmin)N (1)

In ﬁg. 1(b) I show the same algorithm for variable-weight events. In this
case we do not reject events but we associate the weight w with each event.
For the total integral over entire Ω I may use the same formula of eq. (15) as
for the constant-weight algorithm. In the case of the histogram we accumulate
w. The properly normalised distribution is
in each bin a sum of weights

obtained as follows3

Pz∈bin

dσ
dz

=

nbσ(1)
(zmax − zmin)N (1)

w =

nb¯σ(1)
(zmax − zmin)N (1)

Xz∈bin

Xz∈bin

¯w

(19)

5

(a)

(cid:27)
(cid:27)
(cid:27)
?
ρ(n)(xi)

6

6

6

?

(n−1)

(cid:27)(cid:24)
w

-

(cid:26)(cid:25)

ρ(n−1)(xi)

...

?
(cid:27)(cid:24)
w(1)

-

(cid:26)(cid:25)

ρ(1)(xi)

?
(cid:27)(cid:24)
w(0)

-

(cid:26)(cid:25)

ρ(0)(xi)

?

(b)

?
ρ(n)(xi)

?

(n−1)

(cid:27)(cid:24)
w

(cid:26)(cid:25)

...

?
(cid:27)(cid:24)
w(1)

(cid:26)(cid:25)

?
(cid:27)(cid:24)
w(0)

(cid:26)(cid:25)

?

Figure 2: Single-line MC algorithm: versions (a) with constant-weight events, nested
rejection loops, and (b) with variable-weight events.

In ﬁg. 2(a) I show the simple single-line algorithm with several nested
rejection loops. The meaning of the graph is rather obvious. The original
distribution ρ0 goes through n-step simpliﬁcation procedure

ρ(0) → ρ(1) → ρ(2) · · · → ρ(n)

(20)

3Operationally this formula is identical for the case of variable-weight and constant-
weight events and is therefore handy in practical calculations [3]. The values of ¯σ(1) and
N (1) can be accumulated using a dedicated, one-bin-histogram. This arrangement facilitates
dumping all MC results on the disk and restarting MC run at the later time.

6

and the compensation weights

w(k) =

ρ(k)
ρ(k−1)

(21)

are used for rejections “locally” in a standard way: each weight w(k) is com-
pared with rW (k) where 0 < r < 1 is uniform random number, and if w(k) <
rW (k) the event is accepted (down-ward arrow), otherwise rejected (return
loop). The average weights < w(k) > are calculated for each rejection loop.
The most inward box ρ(n)(xi) represents generation algorithm of the points xi
according to maximally simpliﬁed (crude) distribution ρ(n) for which we know
the integral σ(n) =
ρ(n) analytically. The integral of the original distribution
ρ is obtained from the crude integral and the average weights

R

σ(0) =

ρ(xi) dxn = σ(n)

< w(i−1) >= ¯σ(n)

< ¯w(i−1) >

(22)

Z

n

Yi=1

n

Yi=1

The above is completely standard and can be found in ref. [1]. Note also that
all n rejection loops may be combined into single rejection loop with the weight
being product of all weights along the line

w =

w(i−1).

n

Yi=1

(23)

Usually, the version with nested loops is more eﬃcient and the corresponding
program is more modular. The weights for the internal loops are related to
more technical aspects of the MC algorithm (Jacobians) and do not evolve
quickly (during the development of the program) while external weights cor-
respond to physics model and may change more frequently.
It is therefore
proﬁtable to keep in practice several levels of the weights.

In
Finally, we may decide to perform calculation for weighted events.
ﬁg. 2(b) I show the version of the simple single-line MC algorithm with variable-
weight events. The event at the exit of the graph gets associated weight w
which is the product of all weights along the line.

3 Branching

In ﬁg. 3 I show the general MC algorithm with branching into n branches.
This kind of algorithm is used when the distribution to be generated ρ can be

7

?

Pk

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

k = 1, ..., n

}

HHHHHH

ρ1(xi)

ρ2(xi)

. . .

ρn(xi)

?

?

HHHHHH

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

?

ρ(xi)

?

Figure 3: Pure branching.

ρ(xi) =

ρi

n

Xi=1

ρk

ρi

=

σk
σ

,

Pk = R
Pi R

split into sum of several distinct (positive) subdistributions

and we are able, one way or another, to generate each ρi separately. Usu-
ally each ρi contains single peak or one class of peaks in the distribution ρ.
In the beginning of the algorithm (black disc) we pick randomly one branch
(subdistribution) according to probability

(24)

(25)

i.e. we have to know in advance the integrals σi =
ρi analytically or numer-
ically. Typically, in each branch one uses diﬀerent integration variables xi to
parameterise the integral. The particular choice of variables will be adjusted
to leading “singularities” in the branch distribution.

R

Let us give a formal proof of the correctness of the branching method.

dnp(x) =

Pkdnpk(x) =

Xk

σk
σ

dnσk(x)
σk

=

1
σ Xk

Xk

dnσk(x) =

dnσ(x)
σ

(26)

8

Finally, note that at the exit of the branched MC algorithm (exit of the
graph in ﬁg. 3), we may be forced for various reason (saving computer mem-
ory), trash all information on the origin of an event, consequently, we may be
not able to use any information speciﬁc to the branch from which the event
has came. This is rather important practical aspect to be kept in mind.

4 Branching and internal rejection

(a)

(cid:27)

?

?

w2

?

?

(b)

(cid:27)

?

?

?
?

?

6

¯P (1)
k

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

k = 1, ..., n

}

HHHHHH

ρ(1)
1 (xi)

ρ(1)
2 (xi)

. . .

ρ(1)
n (xi)

?

(cid:27)(cid:24)-

wn

(cid:27)(cid:24) -

(cid:26)(cid:25)

w2

?

Pk

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

k = 1, ..., n

}

HHHHHH

6

6

6

ρ(1)
1 (xi)

ρ(1)
2 (xi)

. . .

ρ(1)
n (xi)

(cid:27)

?

?

w1

?

(cid:27)

?

?

wn

?

ρ(xi)

(cid:27)(cid:24)-

(cid:27)(cid:24)-

(cid:27)(cid:24)-

(cid:27)(cid:24)

(cid:26)(cid:25)

(cid:26)(cid:25)

(cid:26)(cid:25)

(cid:26)(cid:25)

HHHHHH

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

(cid:26)(cid:25)

?

HHHHHH

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

?

w1

-

?
?

ρ(xi)

Figure 4: Branching and weights. Two equivalent generation methods with individual
compensation weight for each branch. The case (a) with local return loop for each
branch, (b) with common return point before branching. See text for more explanations
how to to transform algorithm (a) into (b).

In ﬁg. 4(a) I show the simplest combination of the branching algorithm with
the standard rejection method. This type of the algorithm is potentially very
eﬃcient but is used not so often in the MC event generators because it requires

9

that we know in advance branching probabilities Pk. In most of situations we
do not know them analytically. In principle, they can be calculated numerically
using σk = ¯σ(1)
k < ¯wk > but this is not very handy because it requires two MC
runs – ﬁrst run which determines average weights < ¯wk > an second run with
Pk calculated from < ¯wk > from the ﬁrst run.

The solution to the above problem is the variant of the algorithm presented

in ﬁg. 4(b) where

¯Pk =

¯σ(1)
k
¯σ(1) = R
Pl R

ρ(1)
k
ρ(1)
l

(27)

and all rejection returns are done to a point before the branching point. Let
4(b). The probability den-
us check correctness of the algorithm in ﬁg.
sity dnp(xi) at the point xi at the exit of the algorithm (graph) is propor-
tional to product of probability of getting event in the box ρ(1)
k (xi) equal
dnp(1)
times probability of accepting event being ¯wk(x) =
dnσk(x)/dn¯σ(1)
k (x), all that averaged over branches with probabilities ¯Pk. The
same statement is expressed mathematically as follows:

k (x) = dnσ(1)

k (x)/σ(1)

k

dnp(xi) = N

¯Pk dnp(1)

k (x) ¯wk(x)

Xk

Xk

= N

¯σ(1)
k
¯σ(1)

dnσ(1)
k (x)
σ(1)
k

dnσk(x)
dn¯σ(1)
k (x)

= N

dnσ(x)
¯σ(1)

.

(28)

Normalisation N = ¯σ(1)/σ is determined from the condition
Finally we obtain dnp(xi) = dnσ(xi)/σ, as expected.

Ω dnp(xi) = 1.
R

The total integral is a sum over integrals from all branches σ =

k σk
k < wk > (see eq. (15)).

P

where for each branch we may use the formula σk = σ(1)
Slightly rearranged formula

σ =

σ(1)
k < wk >= ¯σ(1)

¯Pk < ¯wk >= ¯σ(1) < ¯w >,

(29)

Xk

Xk

where in < ¯w > we average also over branches, is a straightforward general-
isation of eq. (15). We can also generalised formula of eq. (13) for the total
integral based on the number of accepted events

σ = ¯σ(1) N

N (1) = ¯σ(1)

k Nk
k N (1)

k

P

P

10

(30)

Proof: number N of events accepted in all branches is

N =

Nk =

Xk

Xk

N (1)
k

σk
¯σ(1)
k

(31)

where N (1)
k
see also eq. (12). Inserting N (1)
eq. (30).

is total number of events in a given branch (that is before rejection),
k = N (1) ¯Pk we get N = σ/¯σ(1) and therefore

Summarising, we see that the two algorithms in ﬁg. 4 are equivalent, ie.
they provide the same distribution of events and the same total integral. The
algorithm (a) is probably slightly more eﬃcient but also more diﬃcult to realize
because it requires precise knowledge of the branching probabilities Pk. The
algorithm (b) is usually less eﬃcient but the branching probabilities ¯P (1)
are
k
easier to evaluate because they correspond to simpliﬁed distributions ρ(1)
k .

Let us now consider the case of variable-weight events for which all return
loops in ﬁg. 4 are removed and the two cases (a) and (b) are identical. The
event at the exit of the algorithm carries the weight from one of the branches!
For the calculation of the total integral we may use the same formulas of
eqs. (30) and (15) as for the constant-weight method. Let us check whether
we may proceed as usual for the calculation of the integrals in the subdomain
∆Ω being single bin in any kind of the diﬀerential (or multi-diﬀerential) dis-
tribution. Let us generate long series of N weighted events and accumulate
sum of the weights which fall into ∆Ω. Of course, in the sum of accumulated
weights we have contributions from all branches

¯w(xi) =

dnNk ¯wk =

dnp(1)
k

¯wk

(32)

N ¯P (1)

k Z
∆Ω

Xk

Xxi∈∆Ω

Z
∆Ω
Substituting deﬁnitions for ¯P (1)

Xk

k

¯w(xi) = N

Xxi∈∆Ω

and of dnp(1)

¯σ(1)
k
¯σ(1) Z
∆Ω

Xk

k we get
dn¯σ(1)
k (x)
¯σ(1)
k

dnσk(x)
dn¯σ(1)
k (x)

=

N
¯σ(1)

Xk

Z
∆Ω

dnσk(x) =

N
¯σ(1) ∆σ

(33)

Reverting the above formula we get an estimate of the integrated or (multi-)
diﬀerential distribution in terms of sum of the weights

∆σ ≡

dnσk =

Z
∆Ω

¯σ(1)
N Xxi∈∆Ω

σ(1)
N Xxi∈∆Ω

¯w(xi) =

w(xi)

(34)

11

Let us ﬁnally discuss the role of the maximum weight Wk and the appar-
ently unnecessary complication of keeping two kinds of crude distributions σ(1)
and ¯σ(1). For variable-weight events without branching, W is merely a scale
factor which cancels out completely among < ¯w > and ¯σ(1) in the overall
normalisation. Its only role is to keep weights in certain preferred range, for
example it is often preferred to have weights of order 1. In the case of the
variable-weights with branching the relative values of Wk start to play certain
role. Although, for inﬁnite number of events, ﬁnal results (distributions and
integrals) do not depend on Wk the eﬃciency (convergence) of the calculation
depends on the relative ratios of Wk [4]. The maximum weights Wk are more
important/useful for constant-weight algorithm. They are chosen in such a
way that ¯w < 1. The rejection method does not work if this condition is not
fulﬁlled. In most cases we do not know analytically the maximum weight for
a given approximate ρ(1) and the maximum weight W is adjusted empirically.
Of course, the same adjustments can be done by scaling (multiplying by a
constant) the entire ρ(1) but the long-standing tradition tells us to keep ρ(1)
unchanged and rather introduce an explicit adjustment factor W . In the case
of the constant-weight algorithm the values of Wk determine the eﬃciency (re-
jection rate) in each branch. Let us stress again that it is always possible to
enforce Wk = 1 and the presence of Wk is in fact pure conventional.

5 Branching and external rejection

In ﬁg. 5 we transform our algorithm one step further. In ﬁg. 5(a) we repeat
essentially the algorithm of ﬁg. 4(b) while in ﬁg. 5(b) we have single rejection
outside branched part. The weights wk and w are related such that two algo-
rithms are equivalent, that is both algorithms provide the same distributions
and calculate the same integral. The relations is very simple

w =

¯p(1)
k (x) wk(x) =

Xk

¯ρ(1)
k (x)
¯ρ(1)(x)

Xk

wk(x) =

ρ(x)
ρ(1)(x)

= Pk

ρk(x)

ρ(1)
k (x)

Pk

.

(35)

The algorithm of ﬁg. 5(b) can be also obtained independently by combining
in a straightforward way the rejection and branching methods. We proceed as
follows: ﬁrst we simplify

ρ(x) → ρ(1)(x)

(36)

12

(a)

(cid:27)

?

?

6

¯P (1)
k

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

k = 1, ..., n

}

HHHHHH

ρ(1)
1 (xi)

ρ(1)
2 (xi)

. . .

ρ(1)
n (xi)

?

(cid:27)(cid:24)-

wn

-

ρn(xi)
?

(cid:27)(cid:24) -

(cid:26)(cid:25)

w2

?

w1

(cid:27)(cid:24)

(cid:26)(cid:25)

(cid:26)(cid:25)
ρ1(xi)
?

ρ2(xi)
?

HHHHHH

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

?

ρ(xi)

(b)

(cid:27)

?

6

¯P (1)
k

k = 1, ..., n

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

}

HHHHHH

ρ(1)
1 (xi)

ρ(1)
2 (xi)

. . .

ρ(1)
n (xi)

?

?

HHHHHH

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

?

ρ(1)(xi)

(cid:27)(cid:24)

-

(cid:26)(cid:25)

ρ(xi)

?

w

?

Figure 5: Branching and weights. Two equivalent generation methods:
(a) with
compensation weight for each branch. (b) with common compensation weight for all
branches, See text for more explanations how to to transform algorithm (a) into (b).

and this simpliﬁcation is compensated by the weight

w =

ρ(x)
ρ(1)(x)

(37)

and for the internal part ρ(1)(x) we apply the branching method as described
in section 3. Consequently, we may apply all standard formulas for the calcu-
lation of the total integral, for instance σ = σ(1) < w >, and we do not need to
worry about additional proofs of the correctness of the algorithm of ﬁg. 5(b);
we already know that it generates properly the distribution ρ(xi).

Note that the algorithm of ﬁg. 5(b) looks more general than that of ﬁg. 5(a)
in the following sense: the simpliﬁed distribution ρ(1) can be written a sum
ρ(1)
from contributions from all branches ρ(1) =
k and the same is true for ρ

Pk

13

in the case (a) while, in general, it needs not be true in the case (b). In other
words algorithm (a) can be transformed into (b) but the transformation in the
opposite direction is less obvious. There is always a trivial transformation of
(b) into (a) in which we set wk ≡ w. In other words, if in the graph (a) all
weights wk are the same then we are allowed to contract all rejection loop into
a single one as in graph (b), and vice versa. This sounds trivial but may be
useful in the case of the several levels of the compensation/rejection weights.
In the case of the variable-weight we simply omit the rejection return-loops
and sum up weights of the events. Again, since ﬁg. 5(b) is a direct superposition
of the standard weighting and branching methods all standard rules apply. It
is amusing to observe that in spite of the fact that the the weights in the two
algorithm of ﬁgs. 5 are diﬀerent, the two algorithm provide exactly the same
distributions and integrals – only eﬃciency may diﬀer. Which one is more
convenient or eﬃcient depends on the details of a particular problem. In the
next section we shall elaborate on advantages and disadvantages of the two.

6 Branching, compensating weights and map-

ping

Branching is a very powerful tool in the case of the distribution with many
peaks. Usually, we are able to split

ρ(x) =

ρk(x)

Xk

(38)

in such a way that each ρk(x) contains one kind of a spike in the distribution.
In each branch we generate diﬀerent spike with help of the dedicated change
of the variables xi → y(k)

such that in

i

ρk(xi) = ρk(y(k)

∂x
∂y(k) (cid:12)
(cid:12)
(cid:12)
(cid:12)
) is completely ﬂat and the whole spike is located in

(39)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

i

new distribution ρk(y(k)
the Jacobian function |∂y(k)/∂x|. In the following approximation

i

ρk(xi) → ρ(1)

k (xi) = r(1)

k

(40)

∂x
∂y(k) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

14

(41)

(42)

the ρk(y(k)
pensating weight reads as follows

i

) is simply replaced by the constant residue r(1)

k . The relevant com-

wk =

ρk(xi)
ρ(1)
k (xi)

=

ρk(xi)
r(1)
k

∂y(k)
∂x (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

The approximate cross sections needed for branching probabilities are

σ(1)
k =

dnxρ(1)

k = r(1)

dny(k) = r(1)

k V (Ωk)

Z
Ω

k Z
Ωk

where Ωk is the integration domain expressed in the variables y(k) and V (Ωk)
is simply the Cartesian volume of the domain Ωk in the y-space.

Now comes the interesting question: Which of the two algorithms of ﬁg. 5
is more convenient and/or eﬃcient. Finally, the answer will always depend on
the individual properties of a given distribution ρ. Nevertheless, let us point
out some general advantages of the case (a). In the algorithm of ﬁg. 5(a) we
need a single weight wk, for the k-th branch from which an event originates.
The distribution ρ(1)
k might be a simple function directly expressed in terms
of xi but it may happen that the Jacobian |∂y(k)/∂x| is a more complicated
function which requires the knowledge of y(k)
and the whole transformation
y(k)
i → xi. Of course, it is not the problem for the single branch, as in the case
(a), since in the process of generating an event we calculate primarily (generate
randomly) the point y(k)
and we transform it into xi; the calculation of this
Jacobian is usually a byproduct of the generation process. The situation in
the algorithm of ﬁg. 5(b) might be worse because in this case the global weight

i

i

(43)

w =

ρ(x)
ρ(1)(x)

= Pk

ρk(x)

ρk(x)

ρ(1)
k (x)

= Pk
r(1)
k

∂y(k)
∂x (cid:12)
(cid:12)
(cid:12)

Pk

Pk

(cid:12)
(cid:12)
(cid:12)
contains in the denominator ρ(1)
k (x) (or Jacobians) for all branches. Conse-
quently, in some cases we may be forced to perform for each event, (often
quite complicated) transformations xi → y(k)
and calculate Jacobians for all
branches. This is cumbersome, especially if we have large number of branches.
It may also consume a lot of computer time. Just imagine that due to per-
mutation symmetry we have N! branches – even if N is some moderately
high number the summation over all branches might consume almost inﬁnite
amount of computer time. The procedure of summation over branches might

i

15

be also numerically instable in the case of very strong spikes in ρ(1)
k (x) because
computer arithmetic is usually optimised for a given single spike in a given
branch and it might break down for other branch, unless special labour-hungry
methods are employed.

We conclude that the algorithm in ﬁg. 5(a) seems to have certain advantages
over the algorithm in ﬁg. 5(b) although in many cases the diﬀerence might be
unimportant and one might ﬁnd algorithm (b) more simple (it is perhaps easier
to explain and document).

6

6

(cid:27)
(cid:27)

?

?

¯P (2)
k

k = 1, ..., n

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

}

HHHHHH

ρ(2)
1 (xi)

ρ(2)
2 (xi)

. . .

ρ(2)
n (xi)

?

(cid:27)(cid:24)-
w(1)
n
(cid:26)(cid:25)

(cid:27)(cid:24) -
w(1)
2
(cid:26)(cid:25)

-

?

(cid:27)(cid:24)
w(1)
1
(cid:26)(cid:25)
ρ(1)
1 (xi)
?

ρ(1)
2 (xi)
?

ρ(1)
n (xi)
?

HHHHHH

(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)

ρ(1)(xi)

-

(cid:27)(cid:24)
w(0)

(cid:26)(cid:25)
ρ(xi)

?

?

Figure 6: Branching and weights. Practical example.

In the two examples of ﬁg. 5 we have required that ρ(x) can be split im-
mediately into a sum of singular terms, each of them generated in a separate
branch. In the real life it is often not true and ﬁg. 6 illustrates more realistic

16

(44)

(45)

(46)

(47)

scenario. Here, before branching can be applied, we make simpliﬁcation

compensated by the weight

ρ(xi) → ρ(1)(xi)

w(0) =

ρ(xi)
ρ(1)(xi)

=

ρ(xi)
ρ(1)
k (xi)

.

Pk

This simpliﬁcation removes ﬁne details from ρ(x) (for example quantum me-
chanical interferences) which are numerically unimportant and prevent us from
writing ρ(x) as a sum of several positive and relatively simple singular terms.
(Note that in w(0) we still do not have any Jacobians and we know nothing
about transformation to variables y(k)
!) The branching is done in the next
step for ρ(1)(xi) and the weights

i

w(1)

k =

ρ(1)
k (xi)
ρ(2)
k (xi)

.

k (xi), see discussion above. As in the example of ﬁg. 5, w(1)

compensate for the fact that the Jacobian |∂y(k)/∂x| do not match exactly
the ρ(1)
involves
elements of the calculation for a single branch only (Jacobian!). The branching
probabilities ¯P (2)

are easily calculated using known integrals ¯σ(2)
k .

k

k

The total weight

w = w(0)w(1)

k =

ρ(xi)
ρ(1)
k (xi)

ρ(1)
k (xi)
ρ(2)
k (xi)

=

ρ(xi)
ρ(1)
k (xi)

Pk

Pk

ρ(1)
k (xi)
∂y(k)
r(1)
k
∂x (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

consists of global component w(0) which knows nothing about technicalities
of generation in the individual branch k and the local component w(1)
k which
bears responsibility for technicalities of generation in the individual branch k
(in particular it may encapsulate cumbersome Jacobian functions). The lack
of sum over k in eq. (47) is not a mistake – the local part of the weight is
calculated only for a SINGLE k-th branch!!! This is a great practical advantage
and such an arrangement of the weights is generally much more convenient
contrary to the algorithm being the straightforward extension of the algorithm
in ﬁg 5(b) for which the weight

w = w(0)w(1) =

ρ(xi)
ρ(1)
k (xi)

Pk

Pk

Pk

ρ(1)
k (xi)

ρ(2)
k (xi)

=

ρ(xi)
ρ(1)
k (xi)

Pk

ρ(1)
k (xi)

Pk

r(1)
k

Pk

∂y(k)
∂x (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(48)

17

does include sum all over branches for the local part w(1) of the weight. The
eﬃciency of the two algorithms depends on the details of the distribution and
in order to see which of the above of two algorithms is more eﬃcient has to be
checked case by case.

Another important advantage of the algorithm of eq. (47) is that the part of
the algorithm generating ρ(1)
k (xi) can be encapsulated into single subprogram
which generated xi according to k-th crude distribution ρ(1)
k (xi) and provides to
the outside world the weight w(1)
k . The main program does not to need to know
more about any details of the algorithm encapsulated in the subprogram. The
rather annoying feature of the algorithm of eq. (48) is that for the construction
of the total weight in the main program we need to know all nuts and bolts of
the sub-generator for ρ(1)
k (xi), thus encapsulation cannot be realized, leading
to cumbersome non-modular program.

Finally let us note that the total integral is calculated with the usual for-

mula

σ = ¯σ(2) < w(1) w(0) >,

¯σ(2) =

(49)

¯σ(2)
k

Xk

where we understand that for < w(1) > in eq. (47) the average is taken over
all branches. For variable-weight events the the weight is w = w(1)w(0) where
w(1) = w(1)
k

for the actual k-th branch.

7 Conclusions

I have described how to combine three elementary Monte Carlo methods re-
jection, branching and change of variables in the diﬃcult task of generating
multi-dimensional distributions. I have spend some time giving formal math-
ematical proofs of these methods, thus providing useful reference for papers
describing MC event generators, where usually authors lack space/time to
discuss such proofs.
I have also discussed in quite some detail advantages
and disadvantages various combinations of branching and rejection methods.
Again, although these aspects may be known to authors of various MC pro-
grams they are practically never discussed. The most important for practical
applications is probably the discussion on the advantages and disadvantages
of the two arrangements of rejection and branching in ﬁg. 5.

References

18

[1] S. Jadach, Acta. Phys. Polon. B16, 1007 (1985).

[2] F. James, Rep. Prog. Phys. 43, 1145 (1980).

[3] S. Jadach, E. Richter-W¸as, B. F. L. Ward, and Z. W¸as, Comput. Phys.

Commun. 70, 305 (1992).

[4] R. Kleiss and R. Pittau, Comput. Phys. Commun. 83, 141 (1994).

19


3
0
0
2
 
n
u
J
 
7
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
0
6
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

1

The CDF Data Handling System

Dmitry O. Litvintsev∗
FNAL, CD/CDF, Batavia, IL 60510, USA

The Collider Detector at Fermilab (CDF) records proton-antiproton collisions at center of mass energy of 2.0
TeV at the Tevatron collider. A new collider run, Run II, of the Tevatron started in April 2001. Increased
luminosity will result in about 1 PB of data recorded on tapes in the next two years. Currently the CDF
experiment has about 260 TB of data stored on tapes. This amount includes raw and reconstructed data and
their derivatives. The data storage and retrieval are managed by the CDF Data Handling (DH) system. This
system has been designed to accommodate the increased demands of the Run II environment and has proven
robust and reliable in providing reliable ﬂow of data from the detector to the end user. This paper gives an
overview of the CDF Run II Data Handling system which has evolved signiﬁcantly over the course of this year.
An outline of the future direction of the system is given.

1. INTRODUCTION

2. THE DATA ACCESS ORGANIZATION

The Collider Detector at Fermilab (CDF) is a gen-
eral purpose detector at the Fermilab Tevatron [1].
The Tevatron, the world largest p¯p collider with the
c.m.s. energy of about 2 TeV, has undergone a ma-
jor upgrade for Run II that started April 2001. The
CDF detector has been equipped with a new tracking
system, a TOF system, a new plug calorimeter and
luminosity counters. The muon system coverage has
been extended. The CDF trigger and DAQ systems
have been upgraded to accommodate 10× increase in
luminosity.

The experimental program at CDF includes search
for Higgs boson, precision measurements of elec-
troweak parameters, study of t-quark properties,
QCD at large Q2, heavy ﬂavor physics and search for
phenomena beyond Standard Model.

During the ﬁrst years of Run II the CDF collabora-
tion plans to record about 1 PB of data. This volume
is more than 20 times the volume of the previous data
recording, Run I. The sheer volume of data and in-
creased analysis activity due to collaboration growth
and extended physics potential constitute serious chal-
lenges for a data handling system. All data is accessed
multiple times during several years of active analysis.
Direct access storage devices (DASD) are not aﬀord-
able for this data volume. Instead the data is archived
to sequential media. Any of the data can be retrieved
onto DASD into available space on a modiﬁed least
recently used (LRU) basis. Users access the data only
from disk.

The purpose of the DH system is to collect, orga-
nize, archive and then make available the data to user
analysis job.

The CDF Run II data handling strategy is essen-
tially an evolution of the Run I approach of suc-
cessive ﬁltering of events of interest from huge pri-
mary datasets produced at CDF production farm into
smaller sub-samples relevant for individual analyses.
The process of ﬁltering of events and information
stored per each event continues until the samples used
to produce ﬁnal physics results are obtained. These
ﬁnal samples could be small enough to be held on disk.
CDF has adopted a hierarchical data organization
with the dataset at the highest level and runsection
at the lowest level of the structure. The dataset is the
collection of events passing pre-deﬁned set of Level-3
paths (primary and secondary datasets) or other se-
lection criteria relevant for particular physics analysis
(tertiary or derived datasets). Level-3 path is deﬁned
as AND of Level-1, Level-2 and Level-3 triggers.

Dataset

n

Tape

n

Fileset

n

n

File

n

Runsection

Figure 1: Data access hierarchy

There are 50 pre-deﬁned primary datasets at CDF.
During the data taking events that belong to simi-
lar datasets are grouped into 8 streams. Grouping of
datasets into the streams is done in such a way that
event overlap between the streams is minimized and

2

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

CDF Run II Data Logging March 2001 - March 2003

All (raw+processed) data

raw data only

ing and contains on average 3,000 events. Events are
written to ﬁles of about 1 GB in size. No runsection
is split between two ﬁles. Groups of 10 GB or more
worth of ﬁles form ﬁlesets to optimize tape I/O.

At the beginning of the Run II the CDF was writing
data to partitioned AIT-2 tapes, one ﬁleset per par-
tition. The individual data unit existing in the Data
Handling system was therefore a ﬁleset and not a ﬁle.
Users access data by datasets following the datasets
→ ﬁlesets → ﬁles → runsections hierarchy. At the
same time data handling access to data followed
tape→ﬁlesets→ﬁles hierarchy. These two interlacing
access patterns are shown in Figure 1.

]
]

 
 

B
B
T
T
[
[
 
 
s
s
e
e
p
p
a
a
t
t
 
 
n
n
o
o
d
d
e
e
r
r
o
o
t
t
s
s
 
 
e
e
m
m
u
u
o
o
v
v
 
 
a
a
t
t
a
a
D
D

l
l

250
250

200
200

150
150

100
100

50
50

0
0

2.1. Data Flow

During data taking, the Consumer-Server/Logger
(CSL) [2] receives events from the Level-3 PC farm
at 20 MB/sec (75 Hz×250 kB/event) and logs ﬁles to
dual ported SCSI RAID array disks at 20 MB/sec.
These functions are performed on b0dau32, an SGI
2200 server dedicated to the CSL and located in CDF
assembly building. A ﬁleset-tape daemon running
on another identical SGI 2200, fcdfsgi1, located in
Feynman Computer Center, forms ﬁles into ﬁlesets
and logs them to Mass Storage System (MSS) using
the Enstore [4] interface layer that provides access to
network-attached tape drives in the STK robotic tape
library.

Once the data are on tape and the calibrations are
deﬁned, the raw data are fed to the CDF Production
Farm [3] where they are reconstructed. After produc-
tion, the data are split into the 50 primary datasets.
These datasets are written to tapes. The average Pro-
duction Farm I/O throughput is about 30 MB/sec.

The primary datasets are split into secondary
datasets of interest for the physics analysis groups.
Users create tertiary datasets or n-tuples using sec-
ondary datasets as inputs. Figure 2 shows the amount
of raw and produced data logged to tape at CDF since
the beginning of Run II. Some tapes containing older
produced data were recycled to free up media.

2.2. Data and Software Characteristics

/
/
/
/

1
1
1
1
0
0
0
0
8
8
8
8
1
1
1
1
3
3
3
3
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
5
5
5
5
1
1
1
1
4
4
4
4
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
3
3
3
3
1
1
1
1
5
5
5
5
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
0
0
0
0
1
1
1
1
6
6
6
6
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
8
8
8
8
0
0
0
0
7
7
7
7
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
5
5
5
5
0
0
0
0
8
8
8
8
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
2
2
2
2
0
0
0
0
9
9
9
9
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
0
0
0
0
3
3
3
3
9
9
9
9
0
0
0
0

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
8
8
8
8
2
2
2
2
0
0
0
0
1
1
1
1

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
5
5
5
5
2
2
2
2
1
1
1
1
1
1
1
1

/
/
/
/

/
/
/
/

1
1
1
1
0
0
0
0
3
3
3
3
2
2
2
2
2
2
2
2
1
1
1
1

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
0
0
0
0
2
2
2
2
1
1
1
1
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
7
7
7
7
1
1
1
1
2
2
2
2
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
7
7
7
7
1
1
1
1
3
3
3
3
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
4
4
4
4
1
1
1
1
4
4
4
4
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
2
2
2
2
1
1
1
1
5
5
5
5
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
9
9
9
9
0
0
0
0
6
6
6
6
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
7
7
7
7
0
0
0
0
7
7
7
7
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
4
4
4
4
0
0
0
0
8
8
8
8
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
1
1
1
1
0
0
0
0
9
9
9
9
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
9
9
9
9
2
2
2
2
9
9
9
9
0
0
0
0

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
7
7
7
7
2
2
2
2
0
0
0
0
1
1
1
1

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
4
4
4
4
2
2
2
2
1
1
1
1
1
1
1
1

/
/
/
/

/
/
/
/

2
2
2
2
0
0
0
0
2
2
2
2
2
2
2
2
2
2
2
2
1
1
1
1

/
/
/
/

/
/
/
/

3
3
3
3
0
0
0
0
9
9
9
9
1
1
1
1
1
1
1
1
0
0
0
0

/
/
/
/

/
/
/
/

3
3
3
3
0
0
0
0
6
6
6
6
1
1
1
1
2
2
2
2
0
0
0
0

/
/
/
/

/
/
/
/

3
3
3
3
0
0
0
0
6
6
6
6
1
1
1
1
3
3
3
3
0
0
0
0

/
/
/
/

time [weeks]
time [weeks]

Figure 2: Raw and produced data at CDF since the
beginning of Run II

DB interface

DataFileDB

Data retrieval

Disk Cache

Manager

Forms request

DHInput/
DHOutput
Modules

Data Logging

Daemon

Data logging

Data File Catalog

Bookkeeping

MSS

Data archival

Figure 3: The components of the CDF Data Handling
system

• Typical dataset size is 107 events

• Typical analysis job runs at 5 Hz on a 1 GHz
PIII corresponding to few MB/sec input rate

• Analysis jobs are CPU rather than network or

I/O bound over Fast Ethernet

Some characteristics of the CDF data and analysis

software:

3. CDF DATA HANDLING
INFRASTRUCTURE

• ROOT I/O as persistency mechanism

• Typical raw event size is 250 kB

• Typical produced event size is 350 kB, in DST

format (raw banks are kept)

The DH system at CDF has several distinct com-
ponents (Figure 3) each of which has a well-deﬁned
interface.

User speciﬁes a request for data by dataset or other
selection criteria based on meta-data information as-
sociated with the dataset, ﬁleset or ﬁle via talk-to to a

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

3

provided by DataFileDB package. The list of ﬁlesets
is passed to a client API of the disk cache manager.
The client software contacts the disk cache manager
server process to query the status of ﬁlesets in the list.
The ﬁlesets available in the disk cache are processed
while separate staging jobs are launched to copy miss-
ing ﬁlesets from MSS to disk.

Raw and produced data are put to tape by data log-
ging daemons that currently write directly into MSS
using Enstore interface.

3.1. DH I/O modules

An object oriented analysis framework, AC++ [5]
jointly developed by BaBar and CDF provides hooks
to plug in high-level user interfaces to Data Handling
system – DHInput/DHOutput modules. The low-
level API, the means to open, read and deﬁne internal
structure of the data ﬁles is provided by the Event
Data Model [6]. The DHInput provides fast naviga-
tion through input data made possible by the direct
access ROOT format of the CDF data.

The DHOutput module writes out ANSI named
ﬁles furnished with begin run record, empty runsec-
tions records, necessary for luminosity calculations,
and makes entries in the Data File Catalog.

3.2. Data File Catalog

The Data File Catalog is a relational database
that contains information about CDF datasets [7]. It
also contains physics-related bookkeeping information
such as data quality, triggers and ﬁlters used, average
and integrated luminosity, dynamic pre-scales values,
etc. The data access granularity of the DFC is a run-
section, i.e. a group of events rather than individual
events. The DFC table structure reﬂects CDF data
hierarchy depicted in Figure 1.

The DFC tables store information about physical
entities existing in the Data Handling system. Such
entities are Dataset, Tape, Fileset, File and Runsec-
tion. Each entity has a corresponding primary table.
The primary DFC tables are shown in Table I

Entity

Database table

Dataset
Tape
Fileset
File
Runsection CDF2_RUNSECTIONS

CDF2_DATASETS
CDF2_TAPES
CDF2_FILESETS
CDF2_FILES

Table I Primary Data File Catalog tables

they keep history of changes, so called history tables,
or they keep auxiliary entities like ranges or statuses
or descriptions. There are 10 such tables (See Table
II)

Purpose

table name

CDF2_DATASET_REGISTRIES
CDF2_PARENT_DATASETS
CDF2_DATASET_STATUSES
CDF2_PROD_VERSION_DESCS

used to book dataset
dataset parentage
status table
production version
used to create dataset
runsection ranges
in a ﬁle
trigger prescales
dynamic trigger prescales
luminosity history
data quality bit description CDF2_DATA_QUALITY_DESCS
tape pool

CDF2_FILE_LIVETIMES
CDF2_RUNSECTION_LIVETIMES
CDF2_RUNSECTION_LUMINOSITIES

CDF2_RUNSECTION_RANGES

CDF2_TAPEPOOLS

Table II Secondary Data File Catalog tables

Data File Catalog deﬁnes the following primary re-

lationships among the entities:

• a dataset has zero or more parent datasets

• a dataset contains zero or more ﬁlesets

• a dataset is contained on one or more tapes

• tape pool may have zero or more tapes

• a tape has one parent tape pool

• a ﬁleset has one parent dataset

• a ﬁleset contains one or more ﬁles

• a ﬁle has one parent ﬁleset

• a ﬁle has one parent dataset

• a ﬁle contains one or more runsection range

• a ﬁle contains zero or more average prescales

• a runsection contains zero or more dynamic

prescales (livetimes)

There exists an Oracle implementation of the DFC
tables used at Fermilab to keep track of centrally pro-
duced as well as secondary user data. There is also
mSQL implementation of the DFC that can be set up
and run at remote institutions if for some reasons ei-
ther use of DFC located at Fermilab via network or
installation and maintenance of Oracle DFC replica

4

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

constraints and the functionality of triggers are em-
bedded in the DataFileDB library, a C++ DB access
API. Recently the support for mySQL implementation
of the DFC has been added.

3.3. DataFileDB API

The information stored in the DFC is presented to
the data processing algorithms as transient objects
which are retrieved using compound keys. The man-
agement of mapping of persistent data to transient
objects is provided by the common database interface
manager layer[8]. This layer exists between the al-
gorithm code and the code which reads directly from
database tables. At the persistent storage end, it al-
lows multiple back-end mapping classes to be plugged
in and identiﬁed as data sources by character string
it provides a
at the run time. At the user end,
put/get/update/delete interface on top of a transient
class for storage/retrieval/change/removal of objects
of this class using a key.

The mapping object creates a transient object from
the data stored in the DFC using the key. Objects are
cached by keys to prevent multiple database accesses
from diﬀerent algorithms for the same data.

is an abstract base class.

The DBManager[8] has two APIs. The back-
end persistent-to-transient mapping API, IOPack-
age,
In order to create
new persistent-to-transient mapping class, or Map-
per class, one has to derive this class from IOPack-
age. The user code sees the front-end API as template
based. Transient classes are used as the template in-
stantiation parameters of the Manager<object,key>
class. The Manager<object,key> class has methods
such as put, get, update and remove to manipulate
transient objects.

The DataFileDB [7] package is built on the top of
DBManager and provides all the code needed to ma-
nipulate the DFC from a C++ program. It contains
an implementation of the transient object classes,
their associated keys and Mapper classes. There are
three Mapper classes for each transient class corre-
sponding to the three supported underlying relational
database implementations – Oracle (using OCI and
OTL libraries), mSQL and mySQL. Information from
any supported database implementation can be ma-
nipulated without code changes, the source database
can be selected at the run time.

There are six classes representing rows of the DFC
tables. Each of these classes, so called row-classes,
has an interface that allows users to view the infor-
mation held inside an object of the class. Many of
these objects contain the data collected from the sec-
ondary DFC tables like parent datasets or runsection

back-end implementation for the corresponding tran-
sient object.

Some of the row-objects have hierarchical views as-
sociated with them. The hierarchical views make calls
down to DBManager classes which perform connec-
tions to the database to do put, get, update or re-
move queries. E.g., the code to retrieve all the ﬁles
belonging to the dataset, identiﬁed by dataset name
identiﬁer aphysr looks like this:

// make connection to database identified
// by key "prd_dfc"

DFCFileCatalogNode fc("dfc_prd");

// key class associated with file

DFCFileKey key;
key.setDatasetNameID("aphysr");

// typedef std::vector<DFCFile> DFCFiles;
// typedef Handle<DFCFiles> DFCFiles_var;

DFCFiles_var files;
fc.findFiles(key,files);

The findFiles method of hierarchical view
DFCFileCatalogNode instantiates Manager provided
by DBManager API with appropriate template param-
eters and arguments:

DFCFiles_mgr m("dfc_prd","DFCFiles");
m.get(key,files);

The front-end API is in DFCFiles_mgr class which
typedef Manager< DFCFiles,DFCFileKey >.
is
The argument ”dfc prd” identiﬁes the entire set of
classes such as OTL or mSQL or mySQL Mappers
to be used to perform data base operation. An
ASCII text conﬁguration ﬁle associates this string
with the real database instance by including user,
password, node name and class set name. The
second argument instructs IOPackage factory which
particular Mapper sub-class to instantiate.
The
object returned from the API is managed by a smart
pointer (Handle<DFCFiles>).

3.4. Disk Cache Management

One of the central components of the CDF DH sys-
tem is the Disk Inventory Manager (DIM) [9]. It acts
as read/write cache in front of the MSS. The primary
function of the DIM is to cache data from the tape
library onto a large collection of shared disks, sec-
ondary functions include automating the writing of
new ﬁlesets onto tape, handling quotas for space and
reservation management.

The unit of space management is a ﬁleset. DIM
keeps track of ﬁleset status and has no knowledge of
datasets as such. This serves to decouple the manager

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

5

Data Volume Staged From Tape per Day

Total Data Volume Staged from Tape

y
a
d
B
T

/

5

4

3

2

1

0

22.9 MB/s

13.9 MB/s

6.5 MB/s

60

s
/
B
M

50

500

B
T

40

30

20

10

0

400

300

200

100

0

2
2
2
0
0
0
-
-
-
g
g
g
u
u
u
A
A
A
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

2
2
0
0
-
-
g
g
u
u
A
A
-
-
1
1
0
0
0
0
2
2

1
1
0
0
-
-
t
t
c
c
O
O
-
-
1
1
0
0
0
0
2
2

1
1
0
0
-
-
v
v
o
o
N
N
-
-
1
1
0
0
0
0
2
2

1
1
0
0
-
-
c
c
e
e
D
D
-
-
1
1
0
0
0
0
2
2

3
3
0
0
-
-
n
n
a
a
J
J
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
b
b
e
e
F
F
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
r
r
a
a
M
M
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
r
r
p
p
A
A
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
y
y
a
a
M
M
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
n
n
u
u
J
J
-
-
2
2
0
0
0
0
2
2

2
2
1
1
-
-
p
p
e
e
S
S
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
t
t
c
c
O
O
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
v
v
o
o
N
N
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
c
c
e
e
D
D
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
n
n
a
a
J
J
-
-
3
3
0
0
0
0
2
2

1
1
0
0
-
-
b
b
e
e
F
F
-
-
3
3
0
0
0
0
2
2

3
3
0
0
-
-
r
r
a
a
M
M
-
-
3
3
0
0
0
0
2
2

days 

→

1
1
1
0
0
0
-
-
-
t
t
t
c
c
c
O
O
O
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
v
v
v
o
o
o
N
N
N
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
c
c
c
e
e
e
D
D
D
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

3
3
3
0
0
0
-
-
-
n
n
n
a
a
a
J
J
J
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
b
b
b
e
e
e
F
F
F
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
r
r
r
a
a
a
M
M
M
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
r
r
r
p
p
p
A
A
A
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
y
y
y
a
a
a
M
M
M
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
n
n
n
u
u
u
J
J
J
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

2
2
2
1
1
1
-
-
-
p
p
p
e
e
e
S
S
S
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
t
t
t
c
c
c
O
O
O
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
v
v
v
o
o
o
N
N
N
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
c
c
c
e
e
e
D
D
D
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
n
n
n
a
a
a
J
J
J
-
-
-
3
3
3
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
b
b
b
e
e
e
F
F
F
-
-
-
3
3
3
0
0
0
0
0
0
2
2
2

3
3
3
0
0
0
-
-
-
r
r
r
a
a
a
M
M
M
-
-
-
3
3
3
0
0
0
0
0
0
2
2
2

days 
days 

→
→

Figure 4: Data volumes staged from tape on legacy DH system vs time

The server daemon is a multi-threaded program
written to the POSIX C API using the worker pool
model. For better scalability it uses a dynamic thread-
ing system , starting more threads under load and
eliminating them when the load is reduced. The
caching mechanism scores each ﬁleset thet is not being
reserved by users, just arrived or marked static based
on reservations, time on disk and time since last us-
age. It deletes the ﬁleset with the lowest score to make
room for newly requested ﬁleset, an LRU algorithm.
The initial design of the central analysis system was
based on the idea of tight storage and CPU connec-
tion. The large computing machine, a SGI Origin 2000
with 128 300 Mhz MIPS R12000 CPUs, fcdfsgi2, has
been purchased and commissioned at the end of 1999.
A 12 TB disk cache pool of Fiber Channel SCSI disk
RAID arrays is managed by the DIM.

The DIM has originally been designed to work
with attached disks on a large SMP. The SMP-centric
model was ﬁrst envisioned for Run II in 1998. In later
revisions the model’s inﬂexibility, cost/performance
considerations and single source upgrades were ac-
knowledged and an extension towards incorporating
commodity PCs and network distributed IDE RAID
based disk storage was adopted

While network nature of DIM allows it to handle
network-attached storage, it has never been developed
to fully support distributed caching.

The CDF DH group evaluated an eﬀort required to
support DIM in distributed environment and decided

MSS. Originally conceived at DESY, dCache has been
jointly developed by DESY and Fermilab Computing
Division (CD) for several years now. dCache uses net-
work mounted disks to implement distributed data
caches with user authentication. dCache provides ﬁle-
based staging, making concept of ﬁleset obsolete.

A client, requesting a ﬁle, contacts a dCache admin
node and is authenticated. If the ﬁle is in any of the
cache pools managed by the dCache admin server, the
client is redirected to the pool containing the ﬁle. If
the ﬁle is not on any of the pools is is staged from
tape to pool with available disk space and client is
redirected to that pool.

3.5. Mass Storage System

CDF has started Run II with a MSS based on
an AML-2 robotic tape library with cheap commod-
ity AIT-2 tape drives, SONY-CDX500C, directly at-
tached to main CDF data logger and central analysis
SMP, fcdfsgi2.
Interface to MSS was written using
CDF-speciﬁc software. The choice of tape technology
turned out to be more diﬃcult than anticipated.

In May 2002, this tape system has been replaced
with a Enstore Mass Storage System with dual STK
Powderhorn 9310 robotic libraries equipped with 10
network-attached data center quality tape drives STK
T9940A, 10 MB/sec read/write rate each [4]. CDF
Enstore MSS is called CDFEN.

6

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

The

This year, STK T9940A are being replaced
with STK T9940B tape drives.
total
I/O rate of CDF tape system system becomes
10x30 MB/sec=300 MB/sec. This bandwidth is
shared between raw data logging, Production Farms
and a 600 CPU Central Analysis Farm (CAF) [11] for
user analysis and legacy SMP system. Total data ca-
pacity is about 2 PB with 200 GB/tape cartridges.
The tape system I/O rate and volume capacity are
suﬃcient for the Run IIa luminosity goals.

Data delivery became stable and reliable. Figure 4
illustrates increased read rate from Enstore on legacy
DH system.

4. CURRENT DEVELOPMENT

Having achieved operational stability and reliable
data delivery at Fermilab, the CDF Data Handling
group is looking forward to further development of the
Data Handling system towards providing data ﬂow to
world-wide distributed computing resources.

The CDF has evaluated the D0 Data Handling sys-
tem based on Sequential Access through Meta Data
(SAM) system [13]. SAM is a complete Data Handling
system that provides:

• Meta-data ﬁle catalog,
• Clustering the data onto tertiary storage in the

manner corresponding to access pattern

• Caching frequently accessed data on disk or

• Organization of data request to minimize tape

tape,

mounts

• A resource manager that estimates resources re-
quired for the ﬁle requests before they are sub-
mitted and, with this information, makes ad-
ministrative decisions concerning data delivery
priorities

The SAM infrastructure consists of a central data
repository and a number of SAM-stations. A station
consists of one or more computer nodes that share
a data cache managed by one or more station mas-
ter nodes and accessible to consumer/producer nodes.
Users submit jobs to computer in a stations specifying
the project their job will access. Using the ﬁle catalog
meta-data, SAM translates the request into the list of
ﬁles. If the requested ﬁles are available in the station
cache they are sent to the requesting job. If some ﬁles
are missing in the local cache, a station, following a set
of rules will request the data from central repository
or from neighboring stations.

By design, the SAM is inherently a scalable, ﬂexi-

into CDF software [12] resulted in creation of CDF
SAM project in the framework of joint D0/CDF/CD
project.

The outcome of this project was a modiﬁed ﬁle cat-
alog SAM schema that would allow to absorb the CDF
Data File Catalog into SAM and interfacing of SAM
data access layer with dCache for reading and writing
data.

5. CONCLUSION

The CDF Collaboration has changed signiﬁcantly
its computing analysis system towards using globally
distributed commodity CPU resources and network
accessible IDE RAID arrays for disk caching.

The choice of associated software and hardware

components seems to be paying oﬀ remarkably well:

• The Enstore generic interface to tape system
that utilizes data center quality drives allowed
to achieve stable and robust operations in a very
short period of time

• The network disk caching management layer,
dCache, runs successfully on commodity Linux
ﬁle servers managing about 100 TB of dis-
tributed cache pools achieving unprecedented
TB/hour data delivery rate to analysis jobs run-
ning on CAF.

• An adaptation of the SAM as the ﬁrst step to-
wards GRID for CDF that ultimately allows
oﬀ-site users to fully utilize their computing re-
sources

Most importantly CDF has a Data Handling strat-
egy that allows scaling with accumulated luminosity
and increasing number CPUs and disks resources.

The CDF Detector has achieved stable data taking.
Stable detector operation complemented by a reliable
Data Handling system will result in high quality and
timely physics results. The ﬁrst CDF Run II paper is
in print, CDF is back in business.

Acknowledgments

In conclusion the author wishes to thank members
of CDF Data Handling group and Fermilab Comput-
ing Division for their signiﬁcant contribution to de-
sign, implementation and operational support of the
CDF Data Handling system.

References

Computing in High Energy and Nuclear Physics 2003, La Jolla, CA, March 24-28, 2003

7

nical Design Report”, FERMILAB-Pub-96/390-
E(1996)

[2] B. Kilminster et al., ”The CDF Consumer-
Server/Logger system for Run II at the Teva-
tron”, CHEP 2001, Beijing, September 2001
[3] J. Antos et al., ”The CDF Run II Oﬄine Com-
puter Farms”, CHEP 2001, Beijing, September
2001
[4] Enstore

homepage

http://hppc.fnal.gov/enstore/

[5] E. Sexton-Kennedy et al., ”The Physical De-
sign of the CDF Simulation and Reconstruction
Framework”, CHEP 2000, Padova, February 2000
[6] R. Kennedy et al., ”The CDF Run II Event Data
Model”, CHEP 2000, Padova, February 2000
[7] D. Litvintsev et al. ”CDF Run II Data File Cat-
alog”, CHEP 2001, Beijing, September 2001

[8] K. Kowalkowski et al., ”Mapping Auxiliary Per-
sistent Data to C++ Objects in the CDF Re-
construction Framework”, CHEP 2000, Padova,
February 2000

[9] S. Lammel et al., ”The CDF Run II Disk Inven-
tory Manager”, CHEP 2001, Beijing, September
2001

[10] M. Ernst et al., ”dCache, a Distributed Stor-
age Data Caching System”, CHEP 2001, Beijing,
September 2001

[11] F. Wuerthwein, ”The CDF Central Analysis

Farm”, these proceedings

[12] G. Garzoglio et al., ”Implementation of SAM in

CDF”, these proceedings

[13] SAM homepage http://d0db.fnal.gov/sam

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

1

The CDF Data Handling System

Dmitry O. Litvintsev(cid:3)
FNAL, CD/CDF, Batavia, IL 60510, USA

The Collider Detetor at Fermilab (CDF) reords proton-antiproton ollisions at enter of mass energy of 2.0

TeV at the Tevatron ollider. A new ollider run, Run II, of the Tevatron started in April 2001.

Inreased

luminosity will result in about 1 PB of data reorded on tapes in the next two years. Currently the CDF

experiment has about 260 TB of data stored on tapes. This amount inludes raw and reonstruted data and

their derivatives. The data storage and retrieval are managed by the CDF Data Handling (DH) system. This

system has been designed to aommodate the inreased demands of the Run II environment and has proven

robust and reliable in providing reliable (cid:13)ow of data from the detetor to the end user. This paper gives an

overview of the CDF Run II Data Handling system whih has evolved signi(cid:12)antly over the ourse of this year.

An outline of the future diretion of the system is given.

1. INTRODUCTION

2. THE DATA ACCESS ORGANIZATION

The CDF Run II data handling strategy is essen-

tially an evolution of the Run I approah of su-

The Collider Detetor at Fermilab (CDF) is a gen-

essive (cid:12)ltering of events of interest from huge pri-

eral purpose detetor at the Fermilab Tevatron [1℄.

mary datasets produed at CDF prodution farm into

The Tevatron, the world largest p(cid:22)p ollider with the

smaller sub-samples relevant for individual analyses.

.m.s. energy of about 2 TeV, has undergone a ma-

The proess of (cid:12)ltering of events and information

jor upgrade for Run II that started April 2001. The

stored per eah event ontinues until the samples used

CDF detetor has been equipped with a new traking

to produe (cid:12)nal physis results are obtained. These

system, a TOF system, a new plug alorimeter and

(cid:12)nal samples ould be small enough to be held on disk.

luminosity ounters. The muon system overage has

CDF has adopted a hierarhial data organization

been extended. The CDF trigger and DAQ systems

with the dataset at the highest level and runsetion

have been upgraded to aommodate 10(cid:2) inrease in

at the lowest level of the struture. The dataset is the

luminosity.

olletion of events passing pre-de(cid:12)ned set of Level-3

The experimental program at CDF inludes searh

letion riteria relevant for partiular physis analysis

paths (primary and seondary datasets) or other se-

for Higgs boson, preision measurements of ele-

(tertiary or derived datasets). Level-3 path is de(cid:12)ned

troweak parameters, study of t-quark properties,

as AND of Level-1, Level-2 and Level-3 triggers.

QCD at large Q

, heavy (cid:13)avor physis and searh for

2

phenomena beyond Standard Model.

During the (cid:12)rst years of Run II the CDF ollabora-

tion plans to reord about 1 PB of data. This volume

is more than 20 times the volume of the previous data

reording, Run I. The sheer volume of data and in-

reased analysis ativity due to ollaboration growth

Tape

n

and extended physis potential onstitute serious hal-

lenges for a data handling system. All data is aessed

multiple times during several years of ative analysis.

Diret aess storage devies (DASD) are not a(cid:11)ord-

able for this data volume. Instead the data is arhived

Dataset

n

Fileset

n

n

File

n

Runsection

to sequential media. Any of the data an be retrieved

Figure 1: Data aess hierarhy

onto DASD into available spae on a modi(cid:12)ed least

reently used (LRU) basis. Users aess the data only

There are 50 pre-de(cid:12)ned primary datasets at CDF.

from disk.

During the data taking events that belong to simi-

The purpose of the DH system is to ollet, orga-

lar datasets are grouped into 8 streams. Grouping of

nize, arhive and then make available the data to user

datasets into the streams is done in suh a way that

analysis job.

event overlap between the streams is minimized and

(cid:3)

for the CDF Data Handling group

a runsetion is de(cid:12)ned every 30 seonds of data tak-

the fration of the stream in any dataset is not small.

Runsetions are the time intervals of data taking for

whih integrated luminosity is alulated. Typially,

THKT005

Forms request

DHInput/
DHOutput
Modules

Data Logging

Daemon

Data logging

2

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

ing and ontains on average 3,000 events. Events are

CDF Run II Data Logging March 2001 - March 2003

All (raw+processed) data

raw data only

written to (cid:12)les of about 1 GB in size. No runsetion

is split between two (cid:12)les. Groups of 10 GB or more

worth of (cid:12)les form (cid:12)lesets to optimize tape I/O.

At the beginning of the Run II the CDF was writing

data to partitioned AIT-2 tapes, one (cid:12)leset per par-

tition. The individual data unit existing in the Data

Handling system was therefore a (cid:12)leset and not a (cid:12)le.

Users aess data by datasets following the datasets

! (cid:12)lesets ! (cid:12)les ! runsetions hierarhy. At the

same time data handling aess to data followed

tape!(cid:12)lesets!(cid:12)les hierarhy. These two interlaing

aess patterns are shown in Figure 1.

]
]

 
 

B
B
T
T
[
[
 
 
s
s
e
e
p
p
a
a
t
t
 
 
n
n
o
o
d
d
e
e
r
r
o
o
t
t
s
s
 
 
e
e
m
m
u
u
o
o
v
v
 
 
a
a
t
t
a
a
D
D

l
l

250
250

200
200

150
150

100
100

50
50

0
0

2.1. Data Flow

1
1
1
1
0
0
0
0
/
/
/
/
8
8
8
8
1
1
1
1
/
/
/
/
3
3
3
3
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
5
5
5
5
1
1
1
1
/
/
/
/
4
4
4
4
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
3
3
3
3
1
1
1
1
/
/
/
/
5
5
5
5
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
0
0
0
0
1
1
1
1
/
/
/
/
6
6
6
6
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
8
8
8
8
0
0
0
0
/
/
/
/
7
7
7
7
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
5
5
5
5
0
0
0
0
/
/
/
/
8
8
8
8
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
2
2
2
2
0
0
0
0
/
/
/
/
9
9
9
9
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
0
0
0
0
3
3
3
3
/
/
/
/
9
9
9
9
0
0
0
0

1
1
1
1
0
0
0
0
/
/
/
/
8
8
8
8
2
2
2
2
/
/
/
/
0
0
0
0
1
1
1
1

1
1
1
1
0
0
0
0
/
/
/
/
5
5
5
5
2
2
2
2
/
/
/
/
1
1
1
1
1
1
1
1

1
1
1
1
0
0
0
0
/
/
/
/
3
3
3
3
2
2
2
2
/
/
/
/
2
2
2
2
1
1
1
1

2
2
2
2
0
0
0
0
/
/
/
/
0
0
0
0
2
2
2
2
/
/
/
/
1
1
1
1
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
7
7
7
7
1
1
1
1
/
/
/
/
2
2
2
2
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
7
7
7
7
1
1
1
1
/
/
/
/
3
3
3
3
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
4
4
4
4
1
1
1
1
/
/
/
/
4
4
4
4
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
2
2
2
2
1
1
1
1
/
/
/
/
5
5
5
5
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
9
9
9
9
0
0
0
0
/
/
/
/
6
6
6
6
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
7
7
7
7
0
0
0
0
/
/
/
/
7
7
7
7
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
4
4
4
4
0
0
0
0
/
/
/
/
8
8
8
8
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
1
1
1
1
0
0
0
0
/
/
/
/
9
9
9
9
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
9
9
9
9
2
2
2
2
/
/
/
/
9
9
9
9
0
0
0
0

2
2
2
2
0
0
0
0
/
/
/
/
7
7
7
7
2
2
2
2
/
/
/
/
0
0
0
0
1
1
1
1

2
2
2
2
0
0
0
0
/
/
/
/
4
4
4
4
2
2
2
2
/
/
/
/
1
1
1
1
1
1
1
1

2
2
2
2
0
0
0
0
/
/
/
/
2
2
2
2
2
2
2
2
/
/
/
/
2
2
2
2
1
1
1
1

3
3
3
3
0
0
0
0
/
/
/
/
9
9
9
9
1
1
1
1
/
/
/
/
1
1
1
1
0
0
0
0

3
3
3
3
0
0
0
0
/
/
/
/
6
6
6
6
1
1
1
1
/
/
/
/
2
2
2
2
0
0
0
0

3
3
3
3
0
0
0
0
/
/
/
/
6
6
6
6
1
1
1
1
/
/
/
/
3
3
3
3
0
0
0
0

time [weeks]
time [weeks]

Figure 2: Raw and produed data at CDF sine the

beginning of Run II

During data taking, the Consumer-Server/Logger

(CSL) [2℄ reeives events from the Level-3 PC farm

at 20 MB/se (75 Hz(cid:2)250 kB/event) and logs (cid:12)les to

dual ported SCSI RAID array disks at 20 MB/se.

These funtions are performed on b0dau32, an SGI

2200 server dediated to the CSL and loated in CDF

assembly building. A (cid:12)leset-tape daemon running

DB interface

DataFileDB

Data retrieval

Disk Cache

Manager

on another idential SGI 2200, fdfsgi1, loated in

Feynman Computer Center, forms (cid:12)les into (cid:12)lesets

and logs them to Mass Storage System (MSS) using

the Enstore [4℄ interfae layer that provides aess to

network-attahed tape drives in the STK roboti tape

library.

One the data are on tape and the alibrations are

de(cid:12)ned, the raw data are fed to the CDF Prodution

Farm [3℄ where they are reonstruted. After produ-

tion, the data are split into the 50 primary datasets.

Data File Catalog

Bookkeeping

MSS

Data archival

These datasets are written to tapes. The average Pro-

Figure 3: The omponents of the CDF Data Handling

dution Farm I/O throughput is about 30 MB/se.

system

The primary datasets are split into seondary

datasets of interest for the physis analysis groups.

Users reate tertiary datasets or n-tuples using se-

(cid:15) Typial dataset size is 10

events

7

ondary datasets as inputs. Figure 2 shows the amount

of raw and produed data logged to tape at CDF sine

the beginning of Run II. Some tapes ontaining older

(cid:15) Typial analysis job runs at 5 Hz on a 1 GHz

PIII orresponding of few MB/se input rate

produed data were reyled to free up media.

(cid:15) Analysis jobs are CPU rather than network or

2.2. Data and Software Characteristics

I/O bound over Fast Ethernet

Some harateristis of the CDF data and analysis

software:

3. CDF DATA HANDLING
INFRASTRUCTURE

(cid:15) ROOT I/O as persisteny mehanism

The DH system at CDF has several distint om-

(cid:15) Typial raw event size is 250 kB

ponents (Figure 3) eah of whih has a well-de(cid:12)ned

(cid:15) Typial produed event size is 350 kBt, in DST

User spei(cid:12)es a request for data by dataset or other

format (raw banks are kept)

seletion riteria based on meta-data information as-

interfae.

(cid:15) PAD format or mini-DST format, ultimate event

soiated with the dataset, (cid:12)leset or (cid:12)le via talk-to to a

size 50-100 kB

speial DHInput module. The DHInput module trans-

lates the request into a list of (cid:12)lesets using information

(cid:15) N-tuple format 10-30 kB/event

available in the Data File Catalog via DB aess layer

THKT005

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

3

provided by DataFileDB pakage. The list of (cid:12)lesets

they keep history of hanges, so alled history tables,

is passed to a lient API of the disk ahe manager.

or they keep auxiliary entities like ranges or statuses

The lient software ontats the disk ahe manager

or desriptions. There are 10 suh tables (See Table

server proess to query the status of (cid:12)lesets in the list.

II)

The (cid:12)lesets available in the disk ahe are proessed

while separate staging jobs are launhed to opy miss-

Purpose

table name

ing (cid:12)lesets from MSS to disk.

Raw and produed data are put to tape by data log-

used to book dataset

CDF2_DATASET_REGISTRIES

ging daemons that urrently write diretly into MSS

dataset parentage

CDF2_PARENT_DATASETS

using Enstore interfae.

status table

CDF2_DATASET_STATUSES

prodution version

CDF2_PROD_VERSION_DESCS

3.1. DH I/O modules

used to reate dataset

runsetion ranges

CDF2_RUNSECTION_RANGES

An ob jet oriented analysis framework, AC++ [5℄

in a (cid:12)le

jointly developed by BaBar and CDF provides hooks

trigger presales

CDF2_FILE_LIVETIMES

to plug in high-level user interfaes to Data Handling

dynami trigger presales

CDF2_RUNSECTION_LIVETIMES

system { DHInput/DHOutput modules. The low-

luminosity history

CDF2_RUNSECTION_LUMINOSITIES

level API, the means to open, read and de(cid:12)ne internal

data quality bit desription CDF2_DATA_QUALITY_DESCS

struture of the data (cid:12)les is provided by the Event

tape pool

CDF2_TAPEPOOLS

Data Model [6℄. The DHInput provides fast naviga-

tion through input data made possible by the diret

aess ROOT format of the CDF data.

Table II Seondary Data File Catalog tables

The DHOutput module writes out ANSI named

(cid:12)les furnished with begin run reord, empty runse-

Data File Catalog de(cid:12)nes the following primary re-

tions reords, neessary for luminosity alulations,

lationships among the entities:

and makes entries in the Data File Catalog.

(cid:15) a dataset has zero or more parent datasets

3.2. Data File Catalog

(cid:15) a dataset ontains zero or more (cid:12)lesets

The Data File Catalog is a relational database

(cid:15) a dataset is ontained on one or more tapes

that ontains information about CDF datasets [7℄. It

(cid:15) tape pool may have zero or more tapes

also ontains physis-related bookkeeping information

suh as data quality, triggers and (cid:12)lters used, average

(cid:15) a tape has one parent tape pool

and integrated luminosity, dynami pre-sales values,

et. The data aess granularity of the DFC is a run-

(cid:15) a (cid:12)leset has one parent dataset

setion, i.e. a group of events rather than individual

events. The DFC table struture re(cid:13)ets CDF data

(cid:15) a (cid:12)leset ontains one or more (cid:12)les

hierarhy depited in Figure 1.

(cid:15) a (cid:12)le has one parent (cid:12)leset

The DFC tables store information about physial

entities existing in the Data Handling system. Suh

(cid:15) a (cid:12)le has one parent dataset

entities are Dataset, Tape, Fileset, File and Runse-

tion. Eah entity has a orresponding primary table.

(cid:15) a (cid:12)le ontains one or more runsetion range

The primary DFC tables are shown in Table I

(cid:15) a (cid:12)le ontains zero or more average presales

Entity

Database table

(cid:15) a runsetion ontains zero or more dynami

Dataset

CDF2_DATASETS

presales (livetimes)

Tape

CDF2_TAPES

Fileset

CDF2_FILESETS

There exists an Orale implementation of the DFC

tables used at Fermilab to keep trak of entrally pro-

File

CDF2_FILES

dued as well as seondary user data. There is also

Runsetion CDF2_RUNSECTIONS

mSQL implementation of the DFC that an be set up

Table I Primary Data File Catalog tables

ther use of DFC loated at Fermilab via network or

and run at remote institutions if for some reasons ei-

Besides these primary tables there are seondary ta-

are not possible. Orale and mSQL implementations

installation and maintenane of Orale DFC replia

bles. These tables either keep ertain relation between

are idential with the exeption of latter not having

units, like for example parentage of the datasets or

integrity onstraints and database triggers. Integrity

THKT005

4

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

onstraints and the funtionality of triggers are em-

bak-end implementation for the orresponding tran-

bedded in the DataFileDB library, a C++ DB aess

sient ob jet.

API. Reently the support for mySQL implementation

Some of the row-ob jets have hierarhial views as-

of the DFC has been added.

soiated with them. The hierarhial views make alls

3.3. DataFileDB API

down to DBManager lasses whih perform onne-

tions to the database to do put, get, update or re-

move queries. E.g., the ode to retrieve all the (cid:12)les

belonging to the dataset, identi(cid:12)ed by dataset name

The information stored in the DFC is presented to

identi(cid:12)er aphysr looks like this:

the data proessing algorithms as transient ob jets

whih are retrieved using ompound keys. The man-

// make onnetion to database identified

agement of mapping of persistent data to transient

// by key "prd_df"

ob jets is provided by the ommon database interfae

DFCFileCatalogNode f("df_prd");

manager layer[8℄. This layer exists between the al-

// key lass assoiated with file

gorithm ode and the ode whih reads diretly from

DFCFileKey key;

database tables. At the persistent storage end, it al-

key.setDatasetNameID("aphysr");

lows multiple bak-end mapping lasses to be plugged

// typedef std::vetor<DFCFile> DFCFiles;

in and identi(cid:12)ed as data soures by harater string

// typedef Handle<DFCFiles> DFCFiles_var;

at the run time. At the user end,

it provides a

DFCFiles_var files;

put/get/update/delete interfae on top of a transient

f.findFiles(key,files);

lass for storage/retrieval/hange/removal of ob jets

of this lass using a key.

The findFiles method of hierarhial view

The mapping ob jet reates a transient ob jet from

DFCFileCatalogNode instantiates Manager provided

the data stored in the DFC using the key. Ob jets are

by DBManager API with appropriate template param-

ahed by keys to prevent multiple database aesses

eters and arguments:

from di(cid:11)erent algorithms for the same data.

The DBManager[8℄ has two APIs. The bak-

DFCFiles_mgr m("df_prd","DFCFiles");

end persistent-to-transient mapping API, IOPak-

m.get(key,files);

age,

is an abstrat base lass.

In order to reate

The front-end API is in DFCFiles_mgr lass whih

new persistent-to-transient mapping lass, or Map-

is

typedef Manager< DFCFiles,DFCFileKey >.

per lass, one has to derive this lass from IOPak-

The argument "df prd" identi(cid:12)es the entire set of

age. The user ode sees the front-end API as template

lasses suh as OTL or mSQL or mySQL Mappers

based. Transient lasses are used as the template in-

to be used to perform data base operation. An

stantiation parameters of the Manager<objet,key>

ASCII text on(cid:12)guration (cid:12)le assoiates this string

lass. The Manager<objet,key> lass has methods

with the real database instane by inluding user,

suh as put, get, update and remove to manipulate

password, node name and lass set name. The

transient ob jets.

seond argument instruts IOPakage fatory whih

The DataFileDB [7℄ pakage is built on the top of

partiular Mapper sub-lass to instantiate.

The

DBManager and provides all the ode needed to ma-

ob jet returned from the API is managed by a smart

nipulate the DFC from a C++ program. It ontains

pointer (Handle<DFCFiles>).

an implementation of the transient ob jet lasses,

their assoiated keys and Mapper lasses. There are

three Mapper lasses for eah transient lass orre-

3.4. Disk Cache Management

sponding to the three supported underlying relational

database implementations { Orale (using OCI and

One of the entral omponents of the CDF DH sys-

OTL libraries), mSQL and mySQL. Information from

tem is the Disk Inventory Manager (DIM) [9℄. It ats

any supported database implementation an be ma-

as read/write ahe in front of the MSS. The primary

nipulated without ode hanges, the soure database

funtion of the DIM is to ahe data from the tape

an be seleted at the run time.

library onto a large olletion of shared disks, se-

There are six lasses representing rows of the DFC

ondary funtions inlude automating the writing of

tables. Eah of these lasses, so alled row-lasses,

new (cid:12)lesets onto tape, handling quotas for spae and

has an interfae that allows users to view the infor-

reservation management.

mation held inside an ob jet of the lass. Many of

The unit of spae management is a (cid:12)leset. DIM

these ob jets ontain the data olleted from the se-

keeps trak of (cid:12)leset status and has no knowledge of

ondary DFC tables like parent datasets or runsetion

datasets as suh. This serves to deouple the manager

ranges. Eah transient ob jet lass has an assoiated

from database system. The DIM is a lient-server ap-

key ob jet lass. The key ob jet de(cid:12)nes the WHERE

pliation with the ommuniation between lient and

lause of the SQL statement emitted by the Mapper

server via TCP/IP sokets.

THKT005

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

5

Data Volume Staged From Tape per Day

Total Data Volume Staged from Tape

y
a
d
B
T

/

5

4

3

2

1

0

22.9 MB/s

13.9 MB/s

6.5 MB/s

60

s
/
B
M

50

500

B
T

40

30

20

10

0

400

300

200

100

0

2
2
2
0
0
0
-
-
-
g
g
g
u
u
u
A
A
A
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

2
2
0
0
-
-
g
g
u
u
A
A
-
-
1
1
0
0
0
0
2
2

1
1
0
0
-
-
t
t
c
c
O
O
-
-
1
1
0
0
0
0
2
2

1
1
0
0
-
-
v
v
o
o
N
N
-
-
1
1
0
0
0
0
2
2

1
1
0
0
-
-
c
c
e
e
D
D
-
-
1
1
0
0
0
0
2
2

3
3
0
0
-
-
n
n
a
a
J
J
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
b
b
e
e
F
F
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
r
r
a
a
M
M
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
r
r
p
p
A
A
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
y
y
a
a
M
M
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
n
n
u
u
J
J
-
-
2
2
0
0
0
0
2
2

2
2
1
1
-
-
p
p
e
e
S
S
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
t
t
c
c
O
O
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
v
v
o
o
N
N
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
c
c
e
e
D
D
-
-
2
2
0
0
0
0
2
2

1
1
0
0
-
-
n
n
a
a
J
J
-
-
3
3
0
0
0
0
2
2

1
1
0
0
-
-
b
b
e
e
F
F
-
-
3
3
0
0
0
0
2
2

3
3
0
0
-
-
r
r
a
a
M
M
-
-
3
3
0
0
0
0
2
2

days 

→

1
1
1
0
0
0
-
-
-
t
t
t
c
c
c
O
O
O
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
v
v
v
o
o
o
N
N
N
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
c
c
c
e
e
e
D
D
D
-
-
-
1
1
1
0
0
0
0
0
0
2
2
2

3
3
3
0
0
0
-
-
-
n
n
n
a
a
a
J
J
J
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
b
b
b
e
e
e
F
F
F
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
r
r
r
a
a
a
M
M
M
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
r
r
r
p
p
p
A
A
A
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
y
y
y
a
a
a
M
M
M
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
n
n
n
u
u
u
J
J
J
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

2
2
2
1
1
1
-
-
-
p
p
p
e
e
e
S
S
S
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
t
t
t
c
c
c
O
O
O
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
v
v
v
o
o
o
N
N
N
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
c
c
c
e
e
e
D
D
D
-
-
-
2
2
2
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
n
n
n
a
a
a
J
J
J
-
-
-
3
3
3
0
0
0
0
0
0
2
2
2

1
1
1
0
0
0
-
-
-
b
b
b
e
e
e
F
F
F
-
-
-
3
3
3
0
0
0
0
0
0
2
2
2

3
3
3
0
0
0
-
-
-
r
r
r
a
a
a
M
M
M
-
-
-
3
3
3
0
0
0
0
0
0
2
2
2

days 
days 

→
→

Figure 4: Data volumes staged from tape on legay DH system vs time

The server daemon is a multi-threaded program

MSS. Originally oneived at DESY, dCahe has been

written to the POSIX C API using the worker pool

jointly developed by DESY and Fermilab Computing

model. For better salability it uses a dynami thread-

Division (CD) for several years now. dCahe uses net-

ing system , starting more threads under load and

work mounted disks to implement distributed data

eliminating them when the load is redued. The

ahes with user authentiation. dCahe provides (cid:12)le-

ahing mehanism sores eah (cid:12)leset thet is not being

based staging, making onept of (cid:12)leset obsolete.

reserved by users, just arrived or marked stati based

A lient, requesting a (cid:12)le, ontats a dCahe admin

on reservations, time on disk and time sine last us-

node and is authentiated. If the (cid:12)le is in any of the

age. It deletes the (cid:12)leset with the lowest sore to make

ahe pools managed by the dCahe admin server, the

room for newly requested (cid:12)leset, an LRU algorithm.

lient is redireted to the pool ontaining the (cid:12)le. If

The initial design of the entral analysis system was

the (cid:12)le is not on any of the pools is is staged from

based on the idea of tight storage and CPU onne-

tape to pool with available disk spae and lient is

tion. The large omputing mahine, a SGI Origin 2000

redireted to that pool.

with 128 300 Mhz MIPS R12000 CPUs, fdfsgi2, has

been purhased and ommissioned at the end of 1999.

A 12 TB disk ahe pool of Fiber Channel SCSI disk

3.5. Mass Storage System

RAID arrays is managed by the DIM.

The DIM has originally been designed to work

CDF has started Run II with a MSS based on

with attahed disks on a large SMP. The SMP-entri

an AML-2 roboti tape library with heap ommod-

model was (cid:12)rst envisioned for Run II in 1998. In later

ity AIT-2 tape drives, SONY-CDX500C, diretly at-

revisions the model's in(cid:13)exibility, ost/performane

tahed to main CDF data logger and entral analysis

onsiderations and single soure upgrades were a-

SMP, fdfsgi2.

Interfae to MSS was written using

knowledged and an extension towards inorporating

CDF-spei(cid:12) software. The hoie of tape tehnology

ommodity PCs and network distributed IDE RAID

turned out to be more diÆult than antiipated.

based disk storage was adopted

In May 2002, this tape system has been replaed

While network nature of DIM allows it to handle

with a Enstore Mass Storage System with dual STK

network-attahed storage, it has never been developed

Powderhorn 9310 roboti libraries equipped with 10

to fully support distributed ahing.

network-attahed data enter quality tape drives STK

The CDF DH group evaluated an e(cid:11)ort required to

T9940A, 10 MB/se read/write rate eah [4℄. CDF

support DIM in distributed environment and deided

Enstore MSS is alled CDFEN.

in favor of adopting di(cid:11)erent ahe layer { dCahe

All existing data were opied from AIT-2 tapes to

[10℄.

STK artridges using the DH system running on the

dCahe is a front-end disk ahe for the large

entral analysis SMP.

THKT005

6

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

This year, STK T9940A are being replaed

into CDF software [12℄ resulted in reation of CDF

with STK T9940B tape drives.

The

total

SAM pro jet in the framework of joint D0/CDF/CD

I/O rate of CDF tape system system beomes

pro jet.

10x30 MB/se=300 MB/se. This bandwidth is

The outome of this pro jet was a modi(cid:12)ed (cid:12)le at-

shared between raw data logging, Prodution Farms

alog SAM shema that would allow to absorb the CDF

and a 600 CPU Central Analysis Farm (CAF) [11℄ for

Data File Catalog into SAM and interfaing of SAM

user analysis and legay SMP system. Total data a-

data aess layer with dCahe for reading and writing

paity is about 2 PB with 200 GB/tape artridges.

data.

The tape system I/O rate and volume apaity are

suÆient for the Run IIa luminosity goals.

Data delivery beame stable and reliable. Figure 4

5. CONCLUSION

illustrates inreased read rate from Enstore on legay

DH system.

The CDF Collaboration has hanged signi(cid:12)antly

4. CURRENT DEVELOPMENT

its omputing analysis system towards using globally

distributed ommodity CPU resoures and network

aessible IDE RAID arrays for disk ahing.

The hoie of assoiated software omponents seems

Having ahieved operational stability and reliable

to be paying o(cid:11) remarkably well:

data delivery at Fermilab, the CDF Data Handling

group is looking forward to further development of the

(cid:15) The Enstore generi interfae to tape system

Data Handling system towards providing data (cid:13)ow to

that utilizes data enter quality drives allowed

world-wide distributed omputing resoures.

to ahieve stable and robust operations in a very

The CDF has evaluated the D0 Data Handling sys-

short period of time

tem based on Sequential Aess through Meta Data

(SAM) system [13℄. SAM is a omplete Data Handling

system that provides:

(cid:15) The Network disk ahing management layer,

dCahe, runs suessfully on ommodity Linux

(cid:12)le servers managing about 100 TB of dis-

(cid:15) Meta-data (cid:12)le atalog,

tributed ahe pools ahieving unpreedented

(cid:15) Clustering the data onto tertiary storage in the

ning on CAF.

manner orresponding to aess pattern

TB/hour data delivery rate to analysis jobs run-

(cid:15) Cahing frequently aessed data on disk or

(cid:15) An adaptation of the SAM as the (cid:12)rst step to-

wards GRID for CDF that ultimately allows

o(cid:11)-site users to fully utilize their omputing re-

(cid:15) Organization of data request to minimize tape

soures

tape,

mounts

(cid:15) A resoure manager that estimates resoures re-

egy that allows saling with aumulated luminosity

Most importantly CDF has a Data Handling strat-

quired for the (cid:12)le requests before they are sub-

and inreasing number CPUs and disks resoures.

mitted and, with this information, makes ad-

The CDF Detetor has ahieved stable data taking.

ministrative deisions onerning data delivery

Stable detetor operation omplemented by a reliable

priorities

Data Handling system will result in high quality and

The SAM infrastruture onsists of a entral data

timely physis results. The (cid:12)rst CDF Run II paper is

repository and a number of SAM-stations. A station

being published, CDF is bak in business.

onsists of one or more omputer nodes that share

a data ahe managed by one or more station mas-

ter nodes and aessible to onsumer/produer nodes.

Users submit jobs to omputer in a stations speifying

Acknowledgments

the pro jet their job will aess. Using the (cid:12)le atalog

In onlusion the author wishes to thank members

meta-data, SAM translates the request into the list of

of CDF Data Handling group and Fermilab Comput-

(cid:12)les. If the requested (cid:12)les are available in the station

ing Division for their signi(cid:12)ant ontribution/

ahe they are sent to the requesting job. If some (cid:12)les

are missing in the loal ahe, a station, following a set

of rules will request the data from entral repository

References

or from neighboring stations.

By design, the SAM is inherently a salable, (cid:13)exi-

[1℄ F. Abe et al., Nul. Instr. and Meth. A 271 387

ble Data Handling solution spei(cid:12)ally tailored to a-

(1988); CDF Collaboration, "The CDF II Teh-

ommodate distributed omputing resoures. This re-

nial Design Report", FERMILAB-Pub-96/390-

alization and the initial suess of integrating SAM

E(1996)

THKT005

Computing in High Energy and Nulear Physis 2003, La Jol la, CA, Marh 24-28, 2003

7

[2℄ B. Kilminster et al., "The CDF Consumer-

sistent Data to C++ Ob jets in the CDF Re-

Server/Logger system for Run II at the Teva-

onstrution Framework", CHEP 2000, Padova,

tron", CHEP 2001, Beijing, September 2001

February 2000

[3℄ J. Antos et al., "The CDF Run II O(cid:15)ine Com-

[9℄ S. Lammel et al., "The CDF Run II Disk Inven-

puter Farms", CHEP 2001, Beijing, September

tory Manager", CHEP 2001, Beijing, September

2001

2001

[4℄ Enstore

homepage

[10℄ M. Ernst et al., "dCahe, a Distributed Stor-

http://hpp.fnal.gov/enstore/

age Data Cahing System", CHEP 2001, Beijing,

[5℄ E. Sexton-Kennedy et al., "The Physial De-

September 2001

sign of the CDF Simulation and Reonstrution

[11℄ F. Wuerthwein, "The CDF Central Analysis

Framework", CHEP 2000, Padova, February 2000

Farm", these proeedings

[6℄ R. Kennedy et al., "The CDF Run II Event Data

[12℄ G. Garzoglio et al., "Implementation of SAM in

Model", CHEP 2000, Padova, February 2000

CDF", these proeedings

[7℄ D. Litvintsev et al. "CDF Run II Data File Cat-

[13℄ SAM homepage http://d0db.fnal.gov/sam

alog", CHEP 2001, Beijing, September 2001

[8℄ K. Kowalkowski et al., "Mapping Auxiliary Per-

THKT005


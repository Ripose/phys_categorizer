1996 International Symposium on Nonlinear Theory and its Applications
NOLTA ’96, Katsurahama-so, Kochi, Japan, Oct. 7–9, 1996, pp. 249–252

6
9
9
1
 
t
c
O
 
8
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
9
1
0
0
1
6
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

A Fast Algorithm for High-Dimensional Markov
Processes with Finite Sets of Transition Rates

Hans Ekkehard Plesser1 and Dietmar Wendt2

1Laboratory for Neural Modeling, FRP, RIKEN, 2-1 Hirosawa,
Wako-shi, Saitama 351-01, Japan (plesser@yugiri.riken.go.jp)
2Institut f¨ur Theoretische Physik, RWTH Aachen, 52056 Aachen,
Germany (D.Wendt@physik.rwth-aachen.de)

Abstract

The discrete class algorithm presented in this paper
is an eﬃcient simulation tool for stochastic processes
governed by a reasonably small set of transition rates.
The algorithm is presented, its performance compared
to prevailing methods and applications to epitaxial
growth and neuronal models are sketched.

I. Introduction

Stochastic processes play a crucial role in many ﬁelds
of science and technology and have received much
attention ever since the ground-breaking work by
Einstein, Smoluchowski and others at the beginning
of the century [1, 2]. While many low-dimensional
stochastic processes can be treated analytically [3, 4],
this is no longer the case for spatially extended,
high-dimensional systems, such as diﬀusion-limited
reaction-diﬀusion systems, epitaxial growth, popula-
tion dynamics or neuronal interactions [5, 6, 7, 8].

The unifying feature of all these systems is that their

development in time is given by a master equation

∂
∂t

P(cid:0)

n, t (cid:12)
(cid:12)

n(0)

(cid:1) =

Wn′→nP(cid:0)

X
n′

n(0)

n′, t (cid:12)
(cid:12)

(cid:1) − Wn→n′ P(cid:0)

n, t (cid:12)
(cid:12)

n(0)

(cid:1)

for the probability of the system to be in state n
at time t if it was in state n(0) at time t(0). Here,
Wn→n′ are transition rates and n is a vector in an
m-dimensional discrete state space. Since the mas-
ter equation can rarely be solved analytically, eﬃcient
simulation methods for the generation of trajectories
obeying the equation are of tantamount importance.
In the next section, we present the highly eﬃcient
discrete class algorithm (DCA) for the simulation of
systems governed by a reasonably small set of diﬀerent

transistion rates. In section 3 we apply the algorithm
to a simple model of epitaxial growth and demonstrate
the speed-up compared to prevailing methods. Finally,
in section 4, we show how the DCA can be used to
study large neural networks.

II. The Discrete Class Algorithm

The discrete class algorithm is an extension of the min-
imal process method [9] introduced by Gillespie [10].
This elegant algorithm proceeds from state n at t to
n′ at t′ = t + τ as follows:

1. Calculate the total rate for leaving state n:

Wn = Pn′ Wn→n′ .

2. Determine the (exponentially distributed) time

step τ = − ln rnd(0, 1 ]/Wn.

3. Choose a new state n′ with probability

pn(n′) = Wn→n′ /Wn.

The eﬃciency of this algorithm hinges on the ef-
ﬁcient implementation of step 3, i.e. the selection of
the new state n′ from the probabililty distribution
pn(n′) depending on the current state n. For an m-
dimensional problem, the number of states n′ acces-
sible from n (Wn→n′ > 0) will be M ∼ O(m). To
see this, consider a reaction-diﬀusion system with two
particle species A and B and one chemical reaction
A + B → ⊘, modelled on a grid of L × L cells. Then,
the state space is m = 2L2-dimensional and as long
as all cells contain at least one A and one B particle
each, at every time step one out of M = (4 + 4 + 1) · L2
possible events has to be chosen: From any one of the
L2 cells, either an A or a B particle diﬀuses to any one
of the four nearest neighbors or a reaction occurs in it.
Therefore, linear selection schemes requiring an eﬀort
of O(M ) additions per time step are utterly unsuitable
for large systems, as are rejection methods, which are

eﬃcient only if pn(n′) is restricted to a small inter-
val [11]. Methods employing binary trees for step 3
still require an eﬀort of O(log2 M ) per time step [12].
A sophisticated method using a logarithmic classi-
ﬁcation scheme for step 3 has been introduced a few
years ago [13, 14]. This method yields a computational
eﬀort for step 3 independent of M , i.e. independent of
the size of the system, provided the transition rates
Wn→n′ are independent of M (for a more detailed dis-
cussion see [11]). The algorithm involves some over-
head though, and for systems spanning a very large
range of transition rates (> O(1014)), the algorithm
slows down slightly due to computational precautions
required to avoid round-oﬀ errors.

The discrete class algorithm is similar in spirit to the
logarithmic classes, but speciﬁcally aimed at systems
with a reasonably small, discrete set of transition rates,
i.e.

Wn→n′ ∈ {r1, . . . , rK} , K . 50 .

While this may seem a to be a strong restriction at
ﬁrst, models of epitaxial growth [15] fulﬁll these re-
strictions as well as some neuronal models.

The DCA implements step 3 of the minimal pro-
cess method as follows. Each possible transition event
n → n′ is assigned to one of K classes according to its
rate

Dν = {n → n′ | Wn→n′ = rν } ,

ν ∈ 1, . . . , K .

Thus the rate of events in class Dν is given by

Rν = X
n′∈Dν

Wn→n′ = kDνk rν ,

where kDνk is the number of events in Dν, while the
total rate of events is given by

Wn =

Rν .

K

X
ν=1

Furthermore, within a class each event occurs with
equal probability. The selection step 3 is thus split
into two substeps:

3a. Choose a class Dν with probability Rν/Wn by
linear selection, i.e. for a ρ = rnd[ 0, Wn) select
that class ν for which

ν−1

X
i=1

Ri ≤ ρ <

Ri .

ν

X
i=1

3b. Select the new state n′ from class Dν at random.

The linear selection in step 3a requires drawing
a single, uniformly distributed random number and

discrete classes
logarithmic classes
binary trees

]
c
e
s
[
 
p
e
t
s
 
r
e
p
 
e
m

i
t
 

U
P
C

50e-6

40e-6

30e-6

20e-6

10e-6

10

1000
Length of system L [sites]

100

Figure 1: CPU time required per time step for
diﬀerent simulation algorithms. All simulations
were performed on an SGI Indy workstation with
192 megabyte of memory.

O(K) additions, independent of the size of the system,
while step 3b requires drawing another uniformly dis-
tributed random number. Thus, the eﬃciency of the
selection algorithm does not depend on the size of the
system under study. As the number of classes is as-
sumed to be small, the total rate Wn can be calculated
at every step, keeping numeric inaccuracies to a mini-
mum.

III. Comparison with other methods

To demonstrate the performance of the DCA com-
pared to the logarithmic classes [5, 11] and other state-
of-the art methods such as binary trees [12], let us con-
sider a simple model of epitaxial growth based on [6]:

• the substrate is an L × L lattice;

• each lattice site is occupied by one adatom or

none;

• in an initial phase, N < L × L adatoms are de-

posited, which cannot evaporate;

• an adatom with all four next neighbor sites oc-

cupied cannot move;

• all other adatoms diﬀuse to next neighbor sites

with rates

wn = 2kB T

h e−ES/kB T e−nEN /kB T .

Here, n is the number of occupied next-neighbor sites,
h, kB are Planck’s and Boltzmann’s constants, T is
temperature and ES, EN are material-dependent en-
ergies characterizing adatom-substrate and adatom-
adatom interactions, respectively; both are on the or-
der of 1eV. Thus, after the deposition phase, the sys-
tem is determined by just ﬁve diﬀerent rates which

• All input f (j)

, f (j)
p

s

consists of delta-spikes, i.e. an
input event at time T corresponds to the transi-
tion vj(T −) → vj (T +) = vj(T −) + 1.

• f (j)

• f (j)
s

p (t) is Poissonian noise with rate 1/τp.

(t) is synaptic input from other neurons.

• As the potential reaches a threshold, vj(t) = Θ,
neuron j ﬁres a spike after an average waiting
time τf , which is transmitted to all kj neurons
receiving input from j; neuron j is reset to an
absolute refractory state.

• All input is ignored in the refractory state and
the neuron returns to the resting state vj(t) = 0
with rate 1/τr.

This model is obviously very well suited for the DCA,
since it is governed by only three diﬀerent rates: 1/τp,
1/τf and 1/τr. In order to model spontaneous retinal
waves as have been observed in newborn ferrets [18],
we have simulated a grid of neurons with strongly
localized synaptic connections. The network studied
had 512 × 512 neurons and some 6.7 · 106 synapses,
the threshold was set to Θ = 7. The simulation was
stopped after 1.5 million spikes had been generated,
which required only 80 seconds of CPU time on an
SGI Indy workstation. Figure 2 shows a typical state
of activity.

V. Conclusions

The DCA algorithm presented here is a powerful tool
for the study of a large class of stochastic systems and
should foster research in these ﬁelds. The extension of
the epitaxial model towards more complex phenomena
is straightforward.

The neuron model presented above is most likely
too simplistic to further our understanding of real
neuronal systems, but we are presently working on
a faithful implementation of Stein’s model. Prelimi-
nary results indicate that leak currents and inhibitory
inputs can be included. Inclusion of arbitrary synap-
tic weights, though, might necessitate recourse to the
more generally applicable logarithmic class algorithm.
Note that the eﬀective implementation of the DCA
requires sophisticated data structures, similar to those
described in [11]. Source code that can be integrated
in simulation software via an easy to use interface is
available from the authors upon request.

Acknowledgement

Hans E. Plesser acknowledges partial ﬁnancial support
by Studienstiftung des deutschen Volkes.

References

[1] A. Einstein, Untersuchungen ¨uber die Theorie
der ‘Brownschen Bewegung’, R. F¨urth ed., Aka-
demische Verlagsanstalt, Leipzig, 1922

see retina.gif

Figure 2: Propagation of waves on the retina.
Grey level indicates time since last ﬁring, black
being most recent.

are at T = 600K: w0 = 3.0 · 102, w1 = 1.2 · 10−6,
w2 = 4.8 · 10−15, w3 = 1.9 · 10−23 and w4 ≡ 0.

Figure 1 shows the CPU time required per step for
the simulation of this model for diﬀerent lattice sizes
using the discrete class, the logarithmic class and the
binary tree algorithm. This demonstrates clearly the
superiority of the discrete classes to the other algo-
rithms in terms of absolute times as well as the size-
independence of eﬃciency. The minuscule increase in
CPU time for the DCA at very large lattices is due to
cache eﬀects, i.e. shortcomings of the hardware; for a
detailed discussion, see [16].

IV. A Neuronal Model

A crucial problem in modeling the signal processing by
neuronal networks is the enormous number of neurons
involved even in simple tasks. Typically, though, only
a small number of neurons will respond to any one
stimulus presented e.g. to the eye or the ear. The
DCA is well suited for the simulation of such largely
“dormant” systems, since it automatically “focuses”
on active regions of the system under study.

To demonstrate the applicability of the DCA to neu-
ronal studies, we have formulated the following model,
which is essentially a simpliﬁed type of Stein’s model
neuron [17].

• At t = 0, each neuron j has the resting mem-

brane potential vj(0) = 0.

• The membrane potential vj is governed by the

equation dvj /dt = f (j)

s (t) + f (j)

p (t).

[2] M. v. Smoluchowski, Abhandlungen ¨uber die
Brownsche Bewegung und verwandte Erschei-
nungen, R. F¨urth ed., Akademische Verlagsan-
stalt, Leizig, 1923

[3] C. W. Gardiner, Handbook of Stochastic Methods
for Physics, Chemistry and the Natural Sciences,
2nd ed., Springer, Berlin, 1985

[4] N. G. van Kampen, Stochastic Processes in
Physics and Chemistry, 2nd ed., North–Holland,
Amsterdam, 1992

[5] D. Wendt, T. Fricke, J. Schnakenberg, Z. Phys. B

96:541 (1995)

[6] C. Ratsch et al., Phys. Rev. Lett. 72:3194 (1993)

[7] R. Engbert, F. R. Drepper, Chaos, Solitons &

Fractals 4:1147 (1994)

[8] T. Ohira, J. D. Cowan, Phys. Rev. E 48:2259

(1993)

(1995)

(1991)

(1988)

[9] S. Karlin, H. M. Taylor, A First Course in
Stochastic Processes, 2nd ed., Academic Press,
San Diego, 1975

[10] D. T. Gillespie, J. Comp. Phys. 22:403 (1976)

[11] T. Fricke, D. Wendt, Int. J. Mod. Phys. C 6:277

[12] J. L. Blue, I. Beichl, F. Sullivan, Phys. Rev. E

51:R867 (1995)

[13] T. Fricke, J. Schnakenberg, Z. Phys. B 83:277

[14] Y. Matias, J. S. Vitter, W.–C. Ni, Proceedings of
the 4th Annual SIAM/ACM Symposium on Dis-
crete Algorithms (SODA ’93), Austin, TX, 1993

[15] P. A. Maksym, Semicond. Sci. Technol. 3:594

[16] H. E. Plesser, Untersuchungen ¨uber die An-
wendbarkeit stochastischer Verfahren zur L¨osung
partieller Diﬀerentialgleichungen, diploma thesis,
RWTH Aachen, 1995

[17] H. C. Tuckwell, Stochastic Processes in the Neu-

rosciences, SIAM, Philadelphia, 1989

[18] M. B. Feller et al., Science 272, 1182 (1996)

This figure "retina.gif" is available in "gif"(cid:10) format from:

http://arXiv.org/ps/physics/9610019v1


5
0
0
2
 
t
c
O
 
9
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
6
7
1
0
1
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Numerical diﬀerentiation: local versus global
methods

Karsten Ahnert, Markus Abel

Institute of Physics, University of Potsdam, 14415 Potsdam, Germany

Abstract

In this article, we present investigations on several techniques for numerical diﬀer-
entiation of data. Local techniques are confronted with global approaches and the
diﬀerences are discussed in detail. Two basic quantities are used for characterization
of results: The variance of the diﬀerence of the true derivative and its estimate, and
the smoothness of the estimate. We apply the diﬀerent techniques to numerically
produced data and demonstrate the application to data from an aeroacoustic exper-
iment. As a result, we ﬁnd that global methods are generally preferable if a smooth
process shall be considered. For rough estimates local methods are acceptable.

Key words: numerical diﬀerentiation, data analysis, ﬁltering methods, smoothing
splines, nonparametric regression

1 Introduction

The estimation of derivatives from numerical data is a classical problem which
occurs in many problems of data analysis [1]. Applications range from biology
[2], chemistry [3], and mathematics [4] to a variety of problems in physical ap-
plications [5,6]. Surprisingly, experimenters and data analysts often use very
crude techniques for the estimation of derivatives and the topic is not discussed
in depth in the physics community. Despite the relatively simple strategies to
enhance results for numerical diﬀerentiation, improved techniques are rarely
used. Due to this fact, we felt obliged to present typical techniques and algo-
rithms in a computation-oriented way, referring to the literature for mathe-
matical details.

Email addresses: kahnert@stat.physik.uni-potsdam.de (Karsten Ahnert),

markus@stat.physik.uni-potsdam.de (Markus Abel).

Preprint submitted to Computational Physics

9 February 2014

Whereas mathematically, the derivative of a function is obtained by a limit
process involving inﬁnitesimal calculus, this can never be realized for data
measured by digital equipment due to intrinsic discretization of the data. In
addition, real measurements yield data with noise either due to the intrinsic
device properties or due to ﬁnite resolution, e.g., when sampling the data. An
estimate for a derivative of some data y(x) with respect to the argument x
has to cope with these two restrictions. Noise and ﬁnite resolution yield errors
in the data y, the ﬁnite sampling interval does not allow the limit ∆x
0
and thus produces numerical errors, too.

→

Typically, to obtain an estimate for the derivative y′(x), one tries to approxi-
mate the measured data y best (in an exactly speciﬁed sense). One then hopes
that the derivative is found from this approximation as well “best”. We divide
the existing techniques roughly into local and global strategies. For the ﬁrst,
one ﬁts or interpolates a function through the data points locally, as for a
running window; in the second case, the ﬁt or interpolation is global as in the
case of the well-known spline interpolation. Physical variables very often are
known or required to be smooth. Taken into account this criterion, diﬀerences
between methods can be clearly noticed. Below, we quantify the quality of
the estimate by the least-squares error and the smoothness. A consistent for-
mulation including a smoothness parameter yields a minimization problem,
mathematically known as Tikhonov regularization [4,7].

In this article, we compare four diﬀerent methods: ﬁnite diﬀerencing, the
Savitzky-Golay ﬁlter, smoothing splines and a spectral technique. Whereas the
ﬁrst two are local methods, the latter are global ones. Local methods typically
produce non smooth functions, whereas for global methods the smoothness is
controlled in a well-deﬁned way. We compare the methods by two numerically
produced data sets where the noise in the data can be controlled and the
derivative is known either analytically or numerically with high accuracy. Fi-
nally, we apply the methods to a set of data from an aeroacoustic measurement
[8].

This article is organized as follows: In Section 2, we give a brief overview of the
topic and details on local and global methods. Results for the implementation
of the methods are given in Section 3 for the three above mentioned examples.
The article concludes with Section 4.

2 Estimation of Derivatives

The problem of the estimation of derivatives from data has been discussed
under numerical aspects in [4,9]. Here, we consider a data series y(x), measured
at N points (yi, xi) (i = 1, ..., N). Due to measurement accuracy and noise

2

sources the data have errors and we assume that our measured values consist
of a deterministic part f (x) and a noisy part η(x), such that the measured
data are y(x) = f (x) + η(x). We furthermore assume that f is a smooth
function and that the data series y(x) = yi(xi) is given on a uniform grid
xi = a + i∆x, i = 0, 1, 2, . . . of length L = b
b.
Now, we are interested in the best estimate for the derivatives f ′(x) = df /dx
from the measured data. Throughout this article we will denote the estimate
of the derivative ˜f ′(x) and accordingly the estimate of the function by ˜f(x).

a in the interval a

xi ≤

≤

−

f

||

||

−

−

˜f ′

f ′

˜f
||

min, with

= min, then

One can distinguish the techniques commonly used as local or global approx-
imation methods. The basic idea in all the methods is to approximate the
function f (x) with the hope that then the derivative can be estimated. More
explicitly one assumes the following: if the approximation ˜f is close to the
original function f , then the derivative ˜f ′ is also close to f ′. In mathematical
a suitable norm. The
terms: if
|| ≈
estimation is best in the sense of the applied norm. Usually, the L2 norm is
chosen and one has to solve a least-squares problem. Common methods yield
functions ˜f either obtained by local or global interpolation or a ﬁt [9]. In in-
terpolation the resulting function is required to go through the data points,
= min.
i.e. f (xi) = yi; a ﬁt produces the function such that
Implicitly that means f (xi)
= yi, necessarily. A local method acts on a subset
k, i + k)), a global one on all available data. Most of the
of the data (j
methods can be extended to give approximated function values or derivatives
not only at the points xi but on the whole interval a
b. At the bound-
aries some methods are problematic because, e.g., the statistic changes, or the
problem is no longer well posed.

f (xj)

yjk

|| · ||

j k

−

−

≤

≤

P

(i

∈

x

2.1 Local Methods

∋

Local methods work by ﬁtting or interpolating for each xi a function ˜f(x)
on some sub–interval I
x of the domain. Obviously, this does not guaran-
tee that the function is smooth, because the measurement errors can yield
jumps (from interval to interval). The probably best known method is given
by ﬁnite diﬀerences; it results in a natural way when diﬀerential operators
are discretized. This technique has turned to a huge ﬁeld in connection with
numerical integration of partial diﬀerential equations [10]. Depending on the
problem, diﬀerent schemes can be used and the choice of the right diﬀerence
scheme can have enormous impact on the result of an integration [11]. The
basic idea is simple: the function f is interpolated by a polynomial of order m.
For symmetric diﬀerencing, it is put through the points xi−k, . . . , xi, . . . , xi+k,
where m = 2k is a positive integer, asymmetric schemes, like the well-known
upwind schemes work in a similar way. The derivative at the point x is then
the derivative of the interpolated polynomial. Finite diﬀerencing is local, be-

3

6
(1)

(2)

(3)

cause the approximation of derivative depends only on the 2k + 1 points in the
neighborhood of xi. The polynomial can be obtained from Taylor expansion,
Pad´e approximation or similar schemes [11]. The ﬁnite diﬀerence estimator
for a Taylor expansion has the form [10]

∂ ˜f
∂x

i

(cid:17)

(cid:16)

=

k

Xj=1

αj

yi−j

yi+j −
2j∆x

,

with coeﬃcients

αj = 2(

1)j+1

−

k

 

j ! , 

k !

k + j

.

k

−

∈
˜f ′

f ′

For other schemes, diﬀerent coeﬃcients are used. Then (2) or the right hand
side of (1) can contain derivatives. It is also possible to vary the step width
IN) and j by j′ = lj in (1).
∆x. To do so one replaces ∆x by δx = l∆x (l

k

k

−

in (1) is of order O(∆xm).
If the data are noise-free, the error e =
This means, with a ﬁne sampling one can arrive at accurate approximations.
Numerically, however, one is faced with the unavoidable problem of accuracy
loss in numeric addition (or subtraction) due to the cancellation of digits [12].
Addition of two ﬂoating point numbers is ill conditioned; if the absolute value
of two numbers is approximately equal, but the sign diﬀerent [12]. This is
typical for the scheme (1). Especially if measurement noise is present, it can
easily travel to the leading digits and render the results meaningless. So, one
has a trade-oﬀ of numerical inaccuracy due to addition and analytical need
for small values of ∆x for the approximation (1) to hold suﬃciently well.

A consequent error analysis including the data accuracy δ =
V AR(η), with
η the measurement noise process yields the error of a ﬁrst-order symmetric
ﬁnite diﬀerence scheme (k = 1) in Eq. (1) [4]:

q

e

δx + δ/δx .

∼

≃

≪

2√δ is found for δx

√δ. With given data of sampling
A minimal error e
√δ one needs to discard some points to achieve the minimum
interval ∆x
and uses δx = l∆x This is not satisfying, because the information contained
in the left-out points in the interval yi−l, .., yi+l is thrown away. One would
like to use a scheme which has minimal error, but uses all points with the
intention to go beyond the bound 2δ for the error of the estimate.

∼

This can be achieved by using a ﬁt of a polynomial of order m < 2k through
all data in the interval (xi−k, xi+k). This method is known as Savitzky-Golay-
ﬁltering and is widely used in data analysis. The domain has not to be sym-

4

−

metric (as in generalized ﬁnite diﬀerencing), one deﬁnes a neighborhood by an
interval (i
nl, i + nr) with nl + nr + 1 points, and nl not necessarily equal to
nr. A linear regression is used to ﬁnd the best polynomial ﬁt of order m to the
data, and the derivative is obtained from the coeﬃcients of the polynomial.
This procedure is repeated for every data point, like for a moving window.
Smoothness is, however, not guaranteed and the derivative can be discontinu-
ous, which is not desirable for an estimate useful in physical problems where
f is typically required to be smooth.

2.2 Global Methods

≤

Global methods yield an estimation ˜f (x), deﬁned on the whole interval a
x
a representation by some orthogonal basis functions, φi(x), i

≤
b. Since the function is not known beforehand, it makes sense to use
IN: f (x) =
j ajφj(x). The estimation shall be best in the least squares sense, but as
well smooth. Consequently a minimization problem with a side condition for
P
smoothness is formulated. The coeﬃcients aj are determined accordingly. The
choice of the basis depends on the properties one imposes on f . For instance,
C 2 is smooth (the second
it might be clear from the experiment that f
derivative exists), or continuous only, or periodic on the interval, or has other
restrictions which are desired by the modeller.

∈

∈

The global smoothness, s of a function f is given by the second derivative [13]:

s(f ) =

b

k

Za

f ′′(x)

2 dx,

k

The function estimate ˜f (x) is then determined by the usual least-squares
minimization problem with the additional smoothness constraint. The amount
of smoothing is controlled by the smoothing parameter λ, which enters the
minimization problem:

χ2 =

yi −

Xi {

˜f (xi)

2 + λ

}

b

{

Za

˜f ′′(x)

2 dx != min.

}

The ﬁrst term measures the least squares error of the ﬁt, while the second term
penalizes curvature in the function. This is a typical bias-variance problem [14],
and the best choice of the smoothing parameter is nontrivial. Numerically, it
can be determined using cross-validation [15].

For a concrete application, the representation of ˜f can be written as super-
position of some basis functions with according coeﬃcients, aj. This is then

5

(4)

(5)

inserted into (5). The minimizing coeﬃcients, aj, are determined by a vari-
ational principle. As a consequence the conditions ∂χ2/∂aj = 0 have to be
fulﬁlled and the resulting set of equations needs to be solved (see App. A).
C 2 to be twice diﬀerentiable, natural cubic splines are an
If we require f
C ∞, a spectral representation
obvious choice. For periodic functions, or f
suits well. In other situations, other basis systems might be favorable. In the
following we consider smoothing splines and Fourier representation.

∈

∈

In [4], it has been shown rigorously that the minimizer of (5) is a natural cubic
spline, provided the function f is square integrable. The spline representation
used throughout this paper reads

f (x) =

γjBj(x),

n+2

Xj=0

where γj are the coeﬃcients of the cubic B-spline basis functions Bj(x) and n is
the number of knots for constructing the smoothing spline. After the solution
of the minimization problem one calculates the derivative analytically from the
basis functions. The property, important from a fundamental point of view is
the smoothness of the splines. As shown in [4], e
0, which
is superior to the accuracy (3) for ﬁnite diﬀerence methods, especially in the
case of very ﬁne sampling.

√δ for ∆x

→

∼

In the case of a spectral estimate, one writes

˜f (x) =

ckei2πkx/L

n/2

Xk=−n/2

N and λ

to be inserted into (5). As a result, a system of equations is obtained for the
C. If n = N and λ = 0, the data are exactly interpolated,
coeﬃcients ck ∈
for n
= 0, a spectral smoothing problem results. This can be
≤
solved (see App. A), but we will not consider this case. Instead of solving
the complete smoothing problem in spectral space we follow a slightly diﬀer-
ent strategy: many experiments suggest that the noise sits predominantly in
the high frequencies. Then a low-pass ﬁlter can be applied. A well-behaved
standard ﬁlter is the Butterworth ﬁlter [16], which reads for the m-th order:

(6)

(7)

(8)

B(k, k0) =

1 +

2m ,

1

k
k0

(cid:16)

(cid:17)

where k is the frequency and k0 is the cutoﬀ-frequency. So one simply performs
a Fourier transformation, applies the Butterworth ﬁlter in spectral space and

6

6
transforms back. The derivative is obtained in spectral space by multiplica-
tion with ik/2πL (remember that y′(x) =
L ckei2πkx/L). The complete
representation of ˜f is then

i2πk

k

˜f (x) =

ckB(k, k0)ei2πkx/L =

ˆckei2πkx/L,

(9)

k=N/2

Xk=−N/2

P

k=N/2

Xk=−N/2

where ck are the coeﬃcients obtained by Fourier transformation and ˆck =
ckB(k, k0). It should be mentioned here that this representation also leads to
a smoothing problem (5), cf. App. A. But instead of the determination of a
complete set of coeﬃcients aj one have to determine the cut-oﬀ frequency k0
optimal for a given λ, since k0 is the only parameter to be varied. This means
that λ and k0 are equivalent and directly related as is shown in Appendix A.

Both ways to solve Eq. (5), spline and spectral method, yield n equations for
n unknowns (n being the number of knots for the smoothing splines or the
number of basis function in the spectral method). The equations are overde-
n data points are available. This is resolved by the sum
termined, since N
in the minimization procedure which eventually yields an n
n matrix. The
procedure is similar to the usual linear regression [13,9]. If a Butterworth ﬁl-
ter is applied, the matrices are reduced to 1 dimension. A great advantage
of spectral estimation is the relatively unproblematic estimation of lth-order
derivatives, which are easily obtained by multiplication in spectral space by
(ik/2πL)l.

≥

×

3 Numerical Results

We compare the above methods by three examples, two using numerical data,
one using experimental data: 1) the sine function, 2) the Lorenz system [17]
in a chaotic state and 3) data from an acoustic measurement [8]. We quantify
our results with the mean square error of the estimate of the ﬁrst derivative
and the smoothness. The dependence on the parameter of the methods are
discussed in detail.

The variance

∆ =< ( ˜f ′(xi)

f ′(xi))2 >

−

(10)

is given as an overall measure for the result. The derivative is known exactly
for the numerical data. As a second measure we use the smoothness s(f ) from

7

(11)

(12)

(4) or the diﬀerence

S =

s(f )

−

(cid:16)

(cid:17)

s( ˜f )

2

=

˜f ′′(x)

2dx

b

f ′′(xi)

2dx

|

|

−

Za

|

b

|

Za





2

.





Other measures can be used, e.g., correlations or a norm diﬀerent from L2.

For the ﬁnite diﬀerences we choose a second order method, where the pa-
rameter to be varied is the width δx. The approximation of the derivative
reads

∂ ˜f
∂x

i

(cid:17)

(cid:16)

=

yi−l

4
3

yi+l −
2δx

1
3

yi+2l −
4δx

yi−2l

,

−

where l is a positive integer determining the step width and δx = l∆x. The
Savitzky-Golay ﬁlter was of fourth order; parameter dependence on the win-
dow size nl = nr = n has been investigated. The spectral estimator has as
parameters the cut-oﬀ k0 and the number of basis functions, which is set here
to n = N. The splines have the number of knots, n, used as parameter. Ad-
ditionally, for splines and spectral method, one has the smoothing parameter
to be varied. In the spectral case the variation of λ is equivalent to the vari-
ation of k0. According to the above, we approximate the function and then
determine the derivative by one of the methods under consideration.

A comparison of the used methods requires a scaling of the parameters. For
the ﬁnite diﬀerence method we use the quantity wF D = 4l∆x = 4δx the
distance between the most left and the most right point in the considered
domain. For Savitzky-Golay ﬁlter we choose wSG = (2n + 1)∆x. wSG is the
window size and corresponds directly to wF D. The spectral method has the
cut-oﬀ frequency k0, since n = N. One then uses the corresponding length
wS = L/k0. This quantity is the wavelength of the cut-oﬀ. For the smoothing
splines, wSM = L/n, the distance between two knots.

3.1 Sine Function

x

≤

≤

As a ﬁrst example we use the function f (x) = sin(x) deﬁned on the interval
2π with added Gaussian white noise ηi with standard deviation σ
0
and zero-mean. The data set consists of 500 Points yi = f (xi) + ηi, so that
∆x = 2π/500. In Fig. 1 the result for the derivative estimate is displayed
(σ = 0.5). The ﬁnite diﬀerence (δx = 1.55, wF D = 6.18) yield an unacceptable
result with extreme ﬂuctuations of the order of 0.1. Also, the result for the
Savitzky-Golay ﬁlter (n = 143, wSG = 3.61) is not smooth and deviates heavily

8

’
f

∆

 7

 6

 5

 4

 3

 2

 1

 0
-1 0

 1

 2

 4

 5

 6

 3

x

−

Fig. 1. Diﬀerence ∆f ′(xi) = ˜f ′(xi)
f ′(xi) of the estimate and the exact derivative
f ′(x) = cos(x). Measurement noise was simulated by a Gaussian white noise process
with standard deviation (σ = 0.5). From bottom to top: 1. second order ﬁnite
diﬀerence method (δx = 1.55, wF D = 6.18) 2. Savitzky-Golay-ﬁlter (n = 143,
wSG = 3.61), 3. spectral method (k0 = 2.99, wS = 2.09) and 4. smoothing splines
(n = 7, λ = 0.13, wSM = 0.89). Parameters for the methods were chosen such
that they were optimal in the sense of Eq. (10). The oﬀsets 0, 2, 4, 6 are added
respectively for better visibility.

at the boundaries. Spectral (k0 = 2.99, wS = 2.09) and spline (n = 7, λ = 0.13,
wSM = 0.89) method, apparently work better.

Up to now, we showed the result for a speciﬁc set of parameters for each
method. For a complete analysis, the dependence of the diﬀerent methods on
their parameters needs to be investigated. We varied the parameters over a
wide range and considered the mean square error (10), cf. Fig. 2. For every
method a minimum occurs at the point of optimal approximation in the least-
squares sense.

For ﬁnite diﬀerence method we varied the width δx. The optimal width is
δx = 1.55 (wF D = 6.18), nearly 1/3 of the domain. Smaller spacing results
in larger ﬂuctuations, whereas an increasing spacing will not approximate the
desired derivative. It is clearly visible from Fig. 1 that ﬁnite diﬀerences show
the strongest ﬂuctuations among all considered techniques. In practice ﬁnite
diﬀerences should not be the ﬁrst choice.

For the Savitzky-Golay-ﬁlter we vary the window size to ﬁnd an optimum at
143 (wSG = 3.61). For a larger window the ﬁlter smoothes the function too
n

≈

9

∆

 10000

 1000

 100

 10

 1

 0.1

 0.01

 0.001 0

∆
∆

 100
 10
 1
 0.1
 0.01
 0.001

 2

 4

 6

 8

 10

 12

 0.0001

wFD, wSG, wS

 0.01
λ

 1

 10

 100

 1

 0.1

wSM

(a)

(b)

Fig. 2. Parameter dependence of the mean square error for the derivative of sin(x)
(a) thick solid line – dependency of the mean square error of wSG for the Sav-
itzky-Golay-ﬁlter, dashed line – mean square error against wS for spectral method,
third line – mean square error against wF D for the ﬁnite diﬀerence method. (b)
mean square error for the smoothing splines in dependency of wSM and λ.

much, and the result tends to a constant. For a smaller window the inﬂuence
of noise becomes locally more important.

For spectral diﬀerentiation, the ﬁlter cut-oﬀ k0 can be varied from 0-250 to
3 (wS = 2.09). The original func-
determine the optimal cut-oﬀ. We ﬁnd k0 ≈
tion, sin(x) implies that only k = 1 is active in spectral space; for a top-hat
ﬁlter with sharp edge this would result in k0 = 1. Because B(k, k0) is smooth a
slightly larger cut-oﬀ is found. For higher cut-oﬀ values the approximation in-
creasingly oscillates around the optimal solution. For spectral diﬀerentiation,
in general, problems near boundaries occur, if the data are not perfectly peri-
odic. In this case some data points close to the boundaries should be discarded
after the determination of the derivative, but it might be better to switch to
splines or other basis functions.

≈

For smoothing splines, the mean square error depends on the number of used
0.13 and
knots and the smoothing parameter. The optimal values are λ
k = 7 (wSM = 0.89). When the smoothing parameter is increased the estimate
consistently tends to a constant, when it is decreased the estimate represents
a bigger part of the noisy ﬂuctuations. The meaning of the number of used
knots for the smoothing splines can be understood as the degree of freedom of
the smoother; if there is, e.g., one oscillation in the data, one needs minimal 3
knots for approximation. More oscillations require more knots, corresponding
to a higher resolution, e.g., 10 oscillations can not be resolved with a smoothing
spline of 10 knots. For details see [18]. In this sense the degree of freedom of
the smoother can be understood as ability to ﬁt a given number of oscillations
in the data. If the number of knots exceeds the number of oscillations, one has
the bias-variance problem that all methods have in common.

For a direct comparison of the investigated methods for numerical diﬀerenti-

10

 1

 0.1

 0.01

 0.001

 0.0001

 1e-05

∆

S

 1e+12
 1e+10
 1e+08
 1e+06
 10000
 100
 1
 0.01
 1e-04
 1e-06
 1e-08

 0.1
σ

(a)

◦

 0.01

 1

 0.01

 0.1
σ

 1

(b)

Fig. 3. Characterization of the estimation for one period of a sine. a) least squares
error, b) smoothness S. On the x-axis the noise level of the white Gaussian noise
– ﬁnite diﬀerence method, (cid:4) – Savitzky-Golay-ﬁlter,
added to the signal is plotted.
N – smoothing splines,

– spectral method.

•

ation we studied the dependence of the mean square error (10) on the noise
level σ for each method. Results are shown in Fig. 3(a). Finite diﬀerences
have an error about 1 order of magnitude larger than the other methods. The
spectral method works very well for a small noise level, with an error of one or-
der smaller than the Savitzky-Golay-ﬁlter and smoothing splines. For a higher
noise contamination, however, smoothing splines are the best choice, whereas
spectral methods and the Savitzky-Golay-ﬁlter are comparable by means of
(10). Nevertheless, from a mathematical point of view, the Savitzky-Golay
method is a local approximation, with no smooth relation between ˜y′(xi) and
˜y′(xi+1). This is also visible in Fig. 1 where at several points jumps can be
observed.

|

|

˜f ′′(x)

To quantify the intuition on smoothness from the graphs (cf. Fig. 1), we stud-
ied the dependence of the smoothness S on the standard deviation of the
noise, cf. Eq. (11). The estimate was interpolated by a spline and then S( ˜f )
2 over each interpolated data
has been computed from (4) by averaging
point. The interpolation is necessary to determine the second derivative ˜f ′′.
s(f ) is obtained analytically, so that one can easily determine S. The results
are shown in Fig. 3(b). The smoothness of the Savitzky-Golay-ﬁlter is some or-
ders of magnitude worse than the spectral method and the smoothing splines.
Finite diﬀerence methods are worse than Savitzky-Golay-ﬁlter, so that we did
not considered this method here. The main diﬀerence between the methods is
the diﬀerent behavior of the smoothness. While for the Savitzky-Golay-ﬁlter
one can clearly see a linear relation between noise and smoothness, for the
spectral method and the splines the relation is roughly a constant, up to a
certain, parameter-dependent noise level. One recognizes large ﬂuctuations of
S, resulting from the fact that s( ˜f) can be smaller than the mean value of
s(f ) of the original function.

11

 100000

 10000

∆

 1000

 100

 10

 0

∆
∆

 10000

 1000

 100

 10

 0.5

 1

 1.5

 2

 2.5

 3

wFD, wSG, wS

 0 0.05 0.1 0.15 0.2 0.25 0.3  1e-07

 1e-05

wSM

(a)

(b)

 10

 0.1
 0.001
λ

Fig. 4. Parameter dependence of least squares error for x-component of the Lorenz
system with additive white noise σ = 2.0 (a) dependence of the least squares error of
the inverse window size for the diﬀerent methods. Solid line: Savitzky-Golay-ﬁlter,
dashed line: spectral method, dotted line: ﬁnite diﬀerences. (b) least squares error
for the smoothing splines in dependence on the number of knots k and the smoothing
parameter λ.

3.2 Lorenz System

As a second example we analyzed the x-component of the Lorenz system,

˙x = σ(y
˙y = Rx
˙z =

x)
y
−
bz + xy.

−
−

xz

−

(13)
(14)
(15)

We integrate the system numerically by a Runge-Kutta algorithm of fourth
order with timestep 0.01 and parameters σ = 10, R = 28, b = 8/3. The
integration was performed over 500 steps only. We also added Gaussian white
noise to the data. The results for the dependence of the mean square error on
the level of noise are similar to the previous case, f (x) = sin(x) (see Fig. 5(a)).
For small noise the spectral method provides the smallest approximation error.
Finite diﬀerences, here are of the same order of magnitude.

Because in a chaotic time series, many scales are present with a broad spec-
trum, the Lorenz system is a good candidate for a study of the parameter
dependence of the estimates under the aspect of a scaling system. We show
the comparison of the methods in Fig. 4. The optimal parameters found here
are diﬀerent from the values for the sine. This is explained by the diﬀerent
number of oscillations. The data set for sin(x) contains exactly one oscillation,
whereas the data for the Lorenz system contains 6 oscillations. As mentioned
above the parameters must be chosen to encounter the bias-variance trade-oﬀ.

12

 10000

 1000

 100

 10

 1

 0.1

∆

S

 1e+18
 1e+16
 1e+14
 1e+12
 1e+10
 1e+08
 1e+06
 10000
 100
 1
 0.01

 0.01

 0.01

 0.1

 1

 10

σ

(a)

 0.1

 1

 10

σ

(b)

Fig. 5. Dependence of least squares error and smoothness on the additive noise
standard deviation for the x-component of the Lorenz system, Eq. (13)-(15). a) least
squares error, b) smoothness S. On the x-axis the noise level of the white Gaussian
noise added to the signal is plotted. (cid:4) – Savitzky-Golay-ﬁlter, N – smoothing splines,
– spectral method. The parameters for the ﬁt are chosen to be optimal, cf. Fig. 4.

◦
For the Savitzky-Golay-ﬁlter, wSG = 3.61 for sin(x) and wSG = 0.47 for the
Lorenz system. This diﬀerence can be explained by the fact that here large
window sizes will use many points for ﬁtting a polynomial of 4th order to
each data point. For data sets with many oscillations this will result in bad
approximation of the derivative or the function. For the smoothing splines
wSM = 0.89 for sin(x), whereas for the Lorenz system wSM = 0.13. To resolve
many oscillations, obviously more knots are needed. In the spectral case the
dependency of the cut-oﬀ wS = 2.09 for sin(x) and wS = 0.26 for the Lorenz
system. Summarizing this subsection, the results for the much more compli-
cated chaotic time signal match the ones for the quite simple sine signal. This
can be interpreted as a sign for the generality of the results

3.3 Experimental Data

As the last example we analyzed experimental data from the acoustical signal
emitted from the mouth of an organ pipe. This measurement is needed, if the
complex acoustical system of an organ pipe shall be modeled very roughly
as a nonlinear oscillator [19] to be found numerically by nonparametric data
analysis [20,21]. Basically, the physics of the measured data it is not very
important for our purposes and we will not comment further on the origin
of our data, details are found elsewhere [8]. The time series y(t) consists of
500 points, the sampling rate is ∆t = 1/44100 s. In contrast to the previous
examples, we do not have separate access to the derivatives. So, one needs to
estimate the optimal parameters. In principle, there are methods like cross-
validation dealing with this problem [13]. We will now explain brieﬂy the
functioning of cross-validation to have a complete presentation, an explicit

13

application to our data lies beyond the scope of this article.

Cross validation works by dropping one (or, in general M) point (yi, xi) from
the data, the estimate is then based on the remaining N
1 points. The
cross-validation is constructed by sum of squares

−

CV (λ) =

1
N

N

Xi=1{

yi −

˜f −i
λ (xi)

2.

}

(16)

˜f −i
λ is the estimate for f (x) if the point (yi, xi) is omitted. The optimal param-
eter is calculated from CV for a number of values of λ over a suitable range,
then the minimizing λ is selected. Note that this is done for the estimated
function, not the derivative.

Results for the estimates of function and derivative of the experimental data
are shown in Fig. 6. All methods approximate the function quite well in terms
of the least-squares criterion. As well the derivative seems well estimated, but
slight diﬀerences are recognized. First, one notices the weakness of the spectral
estimator at the boundaries; as well splines and Savitzky-Golay ﬁlter should
be considered with great care in this region, as we already saw in the previous
sections. By eye, all three estimates appear indistinguishable. If we, however,
calculate the smoothness of the estimated curve, we ﬁnd great diﬀerences. For
a comparison, we calculated the Savitzky-Golay estimate with a least-squares
1015, the window
error of e = 2.04, the corresponding smoothness was s = 2.05
10−3. Then, we ran the spectral and spline smoother
size was wSG = 1.61
for many parameter values and selected the runs with very close least squares
error. For the spectral estimator, we obtained e = 2.03 with a smoothness of
10−4. The spline estimator yield an
s = 6.16
10−4. As
error e = 2.05 with s = 5.52
a result we ﬁnd again that the global methods are smoother by two orders of
magnitude in comparison with the Savitzky-Golay method. Obviously, as far
as a smooth curve is concerned the global smoothers are superior to the local
one under consideration. If smoothness is not relevant, all three methods are
equivalent if the boundaries are neglected.

1013 and a window size of wSM = 3.23

1013 at a window of wS = 8, 76

·

·

·

·

·

·

4 Conclusion

We presented a qualitative and quantitative comparison of local and global
methods for the numerical estimation of derivatives. Whereas local methods
are appealing due to their simplicity and easy implementation, we advocate
for global methods, because the properties of the functions can be deﬁned
in a proper way. We focused on the important constraint of smoothness and

14

 30
 20
 10
 0
-10
-20

8
6

3

0

)
t
(
y

4

0
1
 
/
 
)
t
(
’
y

-3

-6

 0

 0.002  0.004  0.006  0.008

 0.01

 0.012

t[s]

Fig. 6. Upper plot: data points of the organ pipe measurements and ﬁtted function
using Savitzky-Golay ﬁlter. The other methods are indistinguishable by eye. Lower
plot: ﬁtted derivatives. Solid line: Savitzky-Golay ﬁlter, dashed line: spectral ﬁlter-
ing, dotted line: smoothing splines. The inset shows a part of the ﬁtted derivative in
the interval 0.04s
0.05s Without quantitative analysis it is hard to say which
curve is best, diﬀerences are however recognizable. The parameter used for ﬁtting
10−4s for the spectral
are wSG = 1.6
method and wSM = 3.2

≤
10−3s for Savitzky-Golay ﬁlter, wS = 8.7

·
10−11 for the smoothing splines.

10−4 and λ = 2.6

≤

t

·

·

·

showed how a corresponding minimization problem is solved. Furthermore, we
demonstrated that global methods are superior to local ones if high-precision
estimates are needed or measurement noise is large. If no beforehand infor-
mation on derivatives is given, one has no access to any bounds, when using
methods like ﬁnite diﬀerences or the Savitzky-Golay ﬁlter. We did not want
to consider in detail the computational cost. But it shall be mentioned that
a global estimate can be expensive if the number of points exceeds 105. Then
programming skill (or large memory) is required to encounter the problem of
large matrices to be multiplied.

We compare in this article -ﬁnite diﬀerences, Savitzky-Golay ﬁltering, smooth-
ing splines and smoothing spectral estimators. To compare the methods, the
dependence on parameters has been investigated, optimal parameters could
be determined. The dependence on additive noise has been studied in detail
and we found enormous diﬀerences in the methods. One result is that ﬁnite
diﬀerences are orders of magnitudes oﬀ in comparison to the other methods.
It is used as a worst case demonstration in this article. In terms of the least-
squares error the remaining three methods are comparable in the sense that
they are of the same order of magnitude. However in terms of smoothness, the
Savitzky-Golay ﬁlter fails by some orders of magnitude, and only the global
methods work well. It is remarkable that we can compare the methods on a

15

logarithmic scale, i.e., techniques diﬀer really to a huge extent. A more detailed
look shows that spectral estimators work very well for small noise levels. For
high noise levels, smoothing splines yield better results for the investigated
systems.

Under a more general perspective, we showed how one can choose among
some set of basis functions for the representation of the estimate. Obviously,
for periodic functions a spectral representation is natural, similarly if one is
interested in higher derivatives. If other boundary conditions are required,
other function systems might be favorable. Smoothing splines are an optimal
choice if twice diﬀerentiability (or smoothness) is required, and no further
information is available. For all basis systems the general procedure described
above applies, formulated as a minimization problem. In principle imagine
further constraints like minimal variance of the ﬁrst derivative or other criteria.
Those conditions can be easily built into the method as additional Lagrange
multiplier.

From a practical point of view one has to decide how important it is to obtain
smooth functions to a reasonable accuracy. For a rough guess, a local ﬁlter
might do, for any high-precision analysis the implementation of the minimiza-
tion, or smoothing problem does pay oﬀ. E.g., if one wants to process further
the obtained derivatives, small diﬀerences can yield enormous changes in the
ﬁnal results. Our interest started with an application in some reconstruction
techniques [22], where local methods are by far too inexact. Similar holds for
prediction problems where the integration of functions based on the estimate
of the derivatives is important [23].

We thank M. Rosenblum and M. Hanke-Bourgeois for helpful discussion. M.
Abel and K. Ahnert acknowledge support by the DFG (German research foun-
dation) (Proj. Nr. AB143/3). We are very grateful to S. Bergweiler for pro-
viding the experimental data.

Acknowledgments

References

[1] H. Kantz and T. Schreiber. Nonlinear time series analysis. Cambridge

University Press, 1997.

[2] C. Sch¨afer, M.G. Rosenblum, H.-H. Abel, and J. Kurths. Synchronization in
the human cardiorespiratory system. Physical Review E, 60:857–870, 1999.

16

[3]

I. Z. Kiss, Q. Lv, and J. L Hudson. Synchronization of non-phase-coherent
chaotic electrochemical oscillations. Physical Review E, 71(035201), 2005.

[4] M. Hanke and O. Scherzer. Inverse problems light: Numerical diﬀerentiation.

Amer. Math. Monthly, 108:512–521, 2001.

[5] Markus Raﬀel, Chris Willert, and J¨urgen Kompenhans.

Particle Image
Velocimetry. Springer Series on Experimental Fluid Mechanics. Springer, Berlin,
2001 ( 2nd edition).

[6] G. Conway. priv. comm.

[7] C.W. Groetsch. The Theory of Tikhonov Regularization for Fredholm Equations

of First Kind. Pitman, London, 1984.

[8] M. Abel, S. Bergweiler, and R. Gerhard-Multhaupt. Synchronization of organ
pipes by means of air ﬂow coupling: experimental observations and modeling.
J. Acoust. Soc. Am., 2005. submitted.

[9] S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes in
C: The Art of Scientiﬁc Computing. Cambridge University Press, Cambridge,
2nd edition, 1993.

[10] Lloyd N. Trefethen. Finite Diﬀerence and Spectral Methods for Ordinary
at

and Partial Diﬀerntial Equations.
available
http://web.comlab.ox.ac.uk/oucl/work/nick.trefethen/pdetext.html, 1996.

unpublished text,

[11] J. H. Ferziger and M. Peri´c. Computational Methods for Fluid Dynamics. 3rd

ed. Springer, Berlin, 2002.

[12] D. Godberg. What every computer scientist should know about ﬂoating point

arithmetic. ACM Computing Surveys (CSUR), 23(1):5–48, 1991.

[13] T.J. Hastie and R.J. Tibshirani. Generalized Additive Models. Chapman and

Hall, London, 1990.

[14] J. Honerkamp. Stochastic dynamical systems. VCH, New York, 1994.

[15] C. Gu and G. Wahba. Minimizing gcv/gml scores with multiple smoothing
parameters via the newton method. SIAM J. Sci. Statist. Comput., 12(2):383–
398, 1991.

[16] Steven Smith. The Scientist and Engineer’s Guide to Digital Signal Processing.

California Technical Publishing, 1997.

[17] E.N. Lorenz. Deterministic nonperiodic ﬂow. J. Athmos. Sci., 20:130, 1963.

[18] G. Wahba.

Spline Models for Observational Data. CBMS-NSF Regional

Conference Series in Applied Mathematics 59. SIAM, Philadelphia, 1990.

[19] B. Fabre and A. Hirschberg. Physical modeling of ﬂue instruments: A review

of lumped models. Acustica - Acta Acustica, 86:599–610, 2000.

[20] H. U. Voss, M. Buenner, and M. Abel.

Identiﬁcation of continuous spatio-

temporal systems. Phys. Rev. E, 57(3):2820–2823, 1998.

17

[21] M. Abel. Nonparametric modeling and spatio-temporal data analysis. Int. J.

Bif. Chaos, 14(6):2027, 2004.

[22] M. Abel, K. Ahnert, S. Mandelj, and J. Kurths. Additive nonparametric
reconstruction of dynamical systems from time series. Phys. Rev. E, 71:15203,
2005.

[23] H. D. I. Abarbanel, M. E. Gilpin, and M. Rotenberg. Analysis Of Observed

Chaotic Data. Springer, New York, 1997.

A Appendix: Spectral smoother

In the following, we assume that x
1, . . . , N

∈

[0, 1] for the measured data (xn, yn); n

∈

The spectral representation of a function reads

f (x) =

ckei2πkx .

N −1

Xk=0

and the smoothing term in Eq. (5) reads

1

λ

|

Z0

|

1

Z0

f ′′(x)

2dx = λ

f ′′(x)f ′′(x)∗dx

= 16λπ4

ckc∗

l k2l2

ei2π(k−l)xdx

= 16λπ4

ckc∗

kk4 ,

1

where
we insert the above into

0 ei2π(k−l)xdx = δkl is used. To solve the minimizing problem Eq. (5),
R

N

Xn=0 |

N

Xn=0 (cid:16)

=

χ2 =

yn −

f (xn)

2 + λ

|

f ′′(x)

2dx != min. ,

yn −

N −1

Xk=0

ckei2πkxn

yn −

(cid:17)(cid:16)

ke−i2πkxn
c∗

+ 16λπ4

ckc∗

kk4 .

(cid:17)

N −1

Xk=0

By variation of the coeﬃcients we obtain the conditions

∂χ2
∂ck

= 0 ,

= 0 .

∂χ2
∂c∗
k

N −1

Xk,l=0
N −1

Xk=0

1

|

Z0

1

Z0

|

N −1

Xk=0

18

(A.1)

(A.2)

(A.3)

(A.4)

This yields the equations

N

Xn=1
N

0 =

−

ei2πkxn

yn −

 

0 =

−

Xn=1

e−i2πkxn

yn −

 

N −1

Xl=0
N −1

Xl=0

l e−i2πlxn
c∗

+ 16π4λc∗

kk4

clei2πlxn

+ 16π4λckk4

!

!

(A.5)

(A.6)

These are 2N linear equations for the 2N unknowns
solved by usual algebraic manipulation.

ck, c∗
k}

{

which can be

In the case of using the Butterworth ﬁlter (8), one determines the Fourier
coeﬃcients ck in the conventional way [9]. Then the ﬁlter is applied. The only
possible variation is in k0 and a single equation results. For simpliﬁcation we
write B(k, k0) = Bk, ∂B(k, k0)/∂k0 = B′

k and ckei2πkxn = Ckn and obtain:

∂χ2
∂k0

= 0

=

N

N −1

−

Xn=1 (cid:16)
+32π4λ

Xk=0
N −1

Xk=0
N −1

N

=

−

Xn=1 (cid:16)

Xk=0
N −1

+32π4λ

Xk=0

BkB′

kk4ckc∗

k

BkB′

kk4ckc∗

k

B′
kCkn(yn −

BlC ∗

ln) +

B′

kC ∗

kn(yn −

BlCln)

+

N −1

Xl=0

N −1

Xk=0

N −1

Xl=0

B′

kyn(Ckn + C ∗

kn)

B′

kBl(CknC ∗

ln + C ∗

knCln)

(A.9)

N −1

−

Xk,l=0

(A.7)

(cid:17)

(A.8)

(cid:17)

(A.10)

Using the deﬁnition of the Fourier components ck =
mula can be written as

N

n=1 yne−i2πkxn this for-

∂χ2
∂k0

=

2N

−

N −1

Xk=0

(B′

kckc∗

k + B′

kBkckc∗

k) + 32π4λ

BkB′

kk4ckc∗
k,

(A.11)

P
N −1

Xk=0

N

n=1 ei2π(k−l)xn = δkl, 1/N

where we use 1/N
the Kronecker delta. The factor N can be avoided, if one scales k0 7→
F + λG. Then a simple relation
We write the minimum condition as 0 =
λ = F/G results, relating lambda to k0. The inversion of this formula yields
k0(λ).

N −1
k=0 ei2πk(xn−xm) = δnm and δnm
2π/Nk0.

−

P

P

19


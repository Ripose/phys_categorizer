3
0
0
2
 
n
u
J
 
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
3
1
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

CHEP’03, La Jolla, California, 24. - 28. March, 2003

1

Atlas Data-Challenge 1 on NorduGrid

P. Eerola, B. K ´onya, O. Smirnova
Particle Physics, Institute of Physics, Lund University, Box 118, 22100 Lund, Sweden
T. Ekel ¨of, M. Ellert
Department of Radiation Sciences, Uppsala University, Box 535, 75121 Uppsala, Sweden
J. R. Hansen, J. L. Nielsen, A. W ¨a ¨an ¨anen
Niels Bohr Institutet for Astronomi, Fysik og Geofysik, Blegdamsvej 17, Dk-2100 Copenhagen Ø,
Denmark
S. Hellman
Department of Physics, Stockholm University, SCFAB, SE-106 91 Stockholm, Sweden
A. Konstantinov
University of Oslo, Department of Physics, P. O. Box 1048, Blindern, 0316 Oslo, Norway and Vilnius University,
Institute of Material Science and Applied Research, Saul ˙etekio al. 9, Vilnius 2040, Lithuania
T. Myklebust, F. Ould-Saada
University of Oslo, Department of Physics, P. O. Box 1048, Blindern, 0316 Oslo, Norway

The ﬁrst LHC application ever to be executed in a computational Grid environment is the so-called ATLAS
Data-Challenge 1, more speciﬁcally, the part assigned to the Scandinavian members of the ATLAS Collaboration.
Taking advantage of the NorduGrid testbed and tools, physicists from Denmark, Norway and Sweden were able
to participate in the overall exercise starting in July 2002 and continuing through the rest of 2002 and the ﬁrst
part of 2003 using solely the NorduGrid environment. This allowed to distribute input data over a wide area,
and rely on the NorduGrid resource discovery mechanism to ﬁnd an optimal cluster for job submission. During
the whole Data-Challenge 1, more than 2 TB of input data was processed and more than 2.5 TB of output data
was produced by more than 4750 Grid jobs.

1. Introduction

In order to prepare for data-taking at the LHC
starting 2007, ATLAS has planned a series of com-
puting challenges of increasing size and complexity
[1]. The goals include a test of its computing model
and the complete software suite and to integrate grid-
middleware as quickly as possible. The ﬁrst of these
Data-Challenges, the Atlas Data-Challenge 1, has run
in several stages in the second half of 2002 and in the
ﬁrst half of 2003. For the ﬁrst time, a massive world-
wide production of many diﬀerent physics samples was
run in a total of more than 50 institutes around the
world. NorduGrid was the Scandinavian contribution
to this Data-Challenge.

In the following, we will describe the NorduGrid
involvement in this Atlas Data-Challenge 1. We start
out by giving a description of the NorduGrid testbed
and then move on to describe its tools seen from a
user’s perspective with some emphasis on how Atlas
jobs are run on NorduGrid. Then we describe the
diﬀerent stages in the Atlas Data-Challenge 1 and the
achievements of NorduGrid. A detailed description of
the NorduGrid architecture will be given in a separate
article [2].

2. The NorduGrid testbed

The aim of the NorduGrid project has from the
start been to build and operate a production Grid
in Scandinavia and Finland. The project was started

MOCT011

in May 2001 and has since May 2002 been running
a testbed based on the architecture discussed in [3].
Since July 2002, it has been used for production by dif-
ferent groups as for example described in this article.
The NorduGrid software is a light-weight and portable
grid-middleware that requires minimal changes to al-
ready existing non-grid resources from the correspond-
ing system administrators. This has allowed Nordu-
Grid to get adopted by many existing supercomputer
centres in the Nordic countries. The NorduGrid soft-
ware runs on clusters with such diﬀerent Linux dis-
tributions as Redhat 6.2, Redhat 7.2, Mandrake 8.0,
Debian 3.0 (Woody) and others.

By now, the NorduGrid resources range from the
original small test-clusters at the diﬀerent physics-
institutions to some of the biggest supercomputer
clusters in Scandinavia. It is one of the largest oper-
ational Grids in the world with approximately 1000
CPU’s available 24 hours a day, 7 days a week. The
list of clusters currently available on NorduGrid can
at anytime be seen on the NorduGrid GridMonitor
(http://www.nordugrid.org/monitor/loadmon.php).
See also ﬁgure 1.

3. The NorduGrid User Interface

The NorduGrid User Interface consists of a set
of command-line tools used for submitting jobs, for
querying the status of jobs, for fetching the output of
jobs (if any), for killing and deleting jobs, for mov-
ing and copying ﬁles between Storage Elements and

2

CHEP’03, La Jolla, California, 24. - 28. March, 2003

Figure 1: A typical screenshot of the GridMonitor during production. The number of CPU’s for each cluster is given to
the right of each cluster name. The number of running grid-jobs for each cluster is given by the red bar and the left
number in the middle of the bar. The number of locally submitted (non-grid) jobs is given by the grey bar and the
right number in the bar. The total number of clusters, CPU’s, running grid-job and locally submitted job are given at
the bottom.

Replica Catalogs and so on. The complete list of com-
mands can be seen in table I and detailed information
about these can be found in the NorduGrid User In-
terface manual [4].

To submit a job,

the user has to specify his
job-requirements in the so-called eXtended Resource
Speciﬁcation Language, xRSL [5]. xRSL is an exten-
sion of the Globus job-description language RSL [6]
and is used to describe in a set of attributes all the
information needed to run the job — name of exe-
In addi-
cutable, arguments, input-ﬁles and so on.
tion, the user can specify a set of requirements that
a given cluster must satisfy to be able to run the job.
These requirements include e.g. that a certain amount
of disk space is available for the job on the cluster.

One of the most important requirements is that
speciﬁed by the runTimeEnvironment-attribute. This
attribute provides support for general software pack-
ages runtime environment conﬁguration. When a
site-administrator installs a given software package
on his cluster, he can advertise this in the Infor-
mation System by creating a bash-script with the
name of the package in the so-called runtimedir on
his cluster containing the necessary environment setup
for the software package. This script is then run
before job-execution for each job that requests this
runtime-environment with the runTimeEnvironment-

attribute. A simple example of a runtime environ-
ment conﬁguration script for the Atlas-rpms provided
by NorduGrid is shown later in ﬁgure 2.

The user submits his grid-job to NorduGrid using
the command ngsub. During job-submission, the User
Interface queries the NorduGrid Information System
for available resources and clusters and then queries
each cluster to perform the check of the requirements
speciﬁed by the user. If the xRSL speciﬁes that cer-
tain input-ﬁles should be downloaded from a Storage
Element or Replica Catalog, the User Interface con-
tacts these to obtain information about the input-ﬁles.
Finally the User Interface performs the brokering ac-
cording the information gathered and submits the job
to the chosen cluster.

From here on, the NorduGrid Grid Manager auto-
matically takes care of the job — handling the pre-
processing (e.g. downloading input-ﬁles and so on),
job-submission to the local PBS-system and post-
processing (e.g. automatically uploading output-ﬁles
to Storage Elements). The user can at anytime check
the status of his submitted job using the command
ngstat. A job can be in several diﬀerent states during
job-processing. A list of possible states with a short
explanation is shown in table II.

When a job has ﬁnished, the user can retrieve those
output-ﬁles that have not already been uploaded au-

MOCT011

CHEP’03, La Jolla, California, 24. - 28. March, 2003

3

tomatically to a Storage Element using the command
ngget. Furthermore the user can at anytime get the
standard output of the job displayed using ngcat. This
can be useful for checking whether a job is running as
intended. If it’s not, the user can kill it using ngkill
and try again.

#!/bin/sh

ATLAS_ROOT=/usr/local/atlas/6.0.3
ROOTSYS=/usr/local/root/3.04.02
G4INSTALL=/usr/local/geant4/4.5.0.ref02
G4SYSTEM=Linux-g++

PATH=$ATLAS_ROOT/bin:$ROOTSYS/bin:$PATH

4. Atlas-software

The diﬀerent releases of the Atlas software are in-
stalled at CERN in a public afs-directory under the
ATLAS tree. Sites with afs-connection can use these
Atlas releases directly but this solution is far from
ideal especially since the releases has for a long period
been very Redhat 6.1 speciﬁc.

For Data-Challenge 1, oﬃcial Atlas-rpms for the dif-
ferent releases were distributed. These rpms were used
by many sites with satisfaction. But primarily be-
cause of the diversity of diﬀerent Linux-distributions
on NorduGrid and the requirements of the diﬀerent
site-administrators, NorduGrid decided to produce its
own set of Atlas-rpms that one was able to build
from scratch starting from a generic vanilla Linux-
distribution. Indeed the NorduGrid Atlas-rpms

• are buildable from scratch on many diﬀerent
Linux distributions from the provided SRPMS.

• are trivially relocatable.

• have an install-area that allows a very simple

Atlas-runTimeEnvironment to be setup.

• come with an automatic install-script.

With the eﬀorts of the diﬀerent site-administrators
on NorduGrid, these Atlas-rpms have been built and
installed on many of the NorduGrid sites. Lately these
Atlas-rpms have also been used in Chicago in phase 3
of Data-Challenge 1 in connection with the Chimera
project (see http://grid.uchicago.edu/atlaschimera/).
In ﬁgure 2 a typical runtime-environment script for
Atlas-release 6.0.3 is shown. The simplicity of this
script is due to the install-area that these Atlas-rpms
are equipped with.

Additionally for each Atlas-release, validation pro-
cedures for the clusters were constructed. These con-
sisted of a set of standard validation grid-jobs to test
the Atlas-software-installation at each cluster. The
validation jobs were run on each cluster, before the
site-administrators were allowed to advertise the cor-
responding Atlas-runTimeEnvironment in the Infor-
mation System. This assured that the production
would only be performed on validated clusters.

MOCT011

LD_LIBRARY_PATH=$ATLAS_ROOT/lib:$ROOTSYS/lib:\
$G4INSTALL/lib/$G4SYSTEM:$LD_LIBRARY_PATH

export PATH LD_LIBRARY_PATH ATLAS_ROOT ROOTSYS\
G4INSTALL G4SYSTEM

Figure 2: Example of runtimeenvironment script for
Atlas release 6.0.3 for the rpms provided by NorduGrid.
Local adaptions of this script is run locally on the
clusters before each Atlas-job.

5. Data-Challenge 1

The simulations performed in Data-Challenge 1 was
based on requests from the High-Level-Trigger com-
munity and diﬀerent physics communities. These re-
quests were divided into several datasets and each
dataset was assigned either fully or in parts to dif-
ferent institutes participating in the Data-Challenge.
The ﬁrst phase, the simulation phase, running from
July to August 2002 consisted of large-scale ATLAS
detector simulations using the ATLAS physics simula-
tion program, atlsim, from Atlas release 3.2.1. In the
second phase, running from December 2002 to Jan-
uary 2003, the already simulated events had to be
piled up with minimum-bias events. This was also
done in atlsim, with Atlas release 4.0.1, using a new
highly eﬃcient event mixing procedure. The partici-
pating institutes were assigned the same partitions as
they had already simulated in phase 1. The third
phase consisted of large-scale reconstruction of the
produced data from the ﬁrst two phases — using Atlas
releases 6.0.2 and 6.0.3.

In the ﬁrst and second phase, NorduGrid was as-
signed 300 partitions of the dataset 2000 — corre-
sponding to 10 percent of the full dataset — and the
total dataset 2003 consisting of 1000 partitions. Be-
fore and during phase 3 additional partitions from
dataset 2000 and dataset 2001 were assigned and
moved to NorduGrid for reconstruction.
In the fol-
lowing, we will describe in some detail the diﬀerent
phases.

5.1. Phase 1 – Simulation

As mentioned NorduGrid was assigned 300 parti-
tions of the dataset 2000. The corresponding input-
ﬁles containing the generated events to be simulated
in the Atlas physics simulation program were 15 ﬁles

4

CHEP’03, La Jolla, California, 24. - 28. March, 2003

Table I The NorduGrid User Interface commands.

command
ngsub
ngstat
ngcat
ngget
ngkill
ngclean
ngsync
ngcopy Copy ﬁles to, from and between Storage Elements and replica catalogues

action
Job submission
Show status of jobs
Display standard output of running jobs
Retrieve output from ﬁnished jobs
Kill running jobs
Delete jobs from a cluster
Update the User Interface’s local information about running jobs

ngremove

Delete ﬁles from Storage Elements and Replica Catalogs

state

meaning

ACCEPTED The job has been submitted to a cluster.

— it is waiting to be processed by the Grid Manager.

PREPARING The Grid Manager is preparing the job

— e.g. downloading input-ﬁles speciﬁed by the job-description.

INLRMS: Q The job has been submitted to the local PBS-system and is queued.
INLRMS: R The job has been submitted to the local PBS-system and is running.
FINISHING The job has ﬁnished running in the local PBS-system.

— the Grid Manager is doing the post-processing.

FINISHED The job has ﬁnished. The output of the job can be retrieved.

Table II Possible states of a NorduGrid grid-job.

each of a size of about 1.7 GB. Before the simula-
tions, these input-ﬁles were distributed to the clusters
participating in this ﬁrst phase and a corresponding
runTimeEnvironment DC1-ATLAS deﬁning the envi-
ronment setup for Atlas release 3.2.1 together with
the location of the input-ﬁles was put in place for each
cluster.

A typical production xRSL for phase 1 dataset 2000
is seen in ﬁgure 3. The executable-attribute speciﬁes
In this case, the script ds2000.sh,
the executable.
after linking in some data-ﬁles, calls atlsim, which
performs the actual detector simulations. The script
ds2000.sh is downloaded from the URL given under
inputFiles and take the input-partition (in this case
1145) as the argument. The xRSL requires as dis-
cussed above the runTimeEnvironment DC1-ATLAS to
ensure that the job is only submitted to those clusters
which have the Atlas release 3.2.1 installed and the
input-ﬁles present.

The standard output of the job will go into the ﬁle
speciﬁed by the stdout-attribute and the join at-
tribute requests that the standard error be merged
with the standard output. Furthermore the name of
the job is speciﬁed using the jobName-attribute. The
xRSL also requests a certain amount of CPU time and
diskspace through the CPUTime and Disk attributes.
The CPUTime attribute ensures e.g. that the User In-
terface chooses the right PBS-queue to submit the job
to on the chosen cluster and the job therefore is not

killed when a certain amount of CPU time has been
spent.

Each job produces a set of output-ﬁles. These are
speciﬁed under outputFiles and will at the end of the
job be automatically uploaded by the Grid Manager
to the location speciﬁed under outputFiles. In this
case, the output-ﬁles speciﬁed will be uploaded to a
physical location registered in the NorduGrid Replica
Catalog deﬁned by the replicaCollection attribute,
so that on request from the Grid Manager, the
Replica Catalog resolves e.g. rc://dc1.uio.no/log
to gsiftp://dc1.uio.no/dc1/2000/log and up-
loads the ﬁle to that Storage Element. Similarly with
the other ﬁles.

The 300 jobs for dataset 2000 in this ﬁrst phase used
around 220 CPU-days and produced 5 × 300 output-
ﬁles with a total size of about 320 GB. All output-
ﬁles were uploaded to a NorduGrid Storage Element
as discussed above. A web page querying the Replica
Catalog for output-ﬁles was setup to allow for an easy
browsing and check of log-ﬁles, output-sizes, error-
codes and so on. This provided a very easy check
of whether something had gone wrong during the job-
processing. A screen-shot of this is shown in ﬁgure 4
For dataset 2003, the strategy with the input-ﬁles
was somewhat diﬀerent. The input data to be pro-
cessed consisted of 100 ﬁles with a total volume of
158 GB. Not all sites in NorduGrid was able to ac-
commodate such a big amount of input-ﬁles, and it

MOCT011

CHEP’03, La Jolla, California, 24. - 28. March, 2003

5

&
(executable="ds2000.sh")
(arguments="1145")
(stdout="dc1.002000.simul.01145.hlt.pythia_jet_17.log")
(join="yes")
(inputFiles=("ds2000.sh" "http://www.nordugrid.org/applications/dc1/2000/dc1.002000.simul.NG.sh"))
(outputFiles=

("atlas.01145.zebra"

("atlas.01145.his"

"rc://dc1.uio.no/2000/zebra/dc1.002000.simul.01145.hlt.pythia_jet_17.zebra")

"rc://dc1.uio.no/2000/his/dc1.002000.simul.01145.hlt.pythia_jet_17.his")

("dc1.002000.simul.01145.hlt.pythia_jet_17.log"

"rc://dc1.uio.no/2000/log/dc1.002000.simul.01145.hlt.pythia_jet_17.log")

("dc1.002000.simul.01145.hlt.pythia_jet_17.AMI"

"rc://dc1.uio.no/2000/ami/dc1.002000.simul.01145.hlt.pythia_jet_17.AMI")

("dc1.002000.simul.01145.hlt.pythia_jet_17.MAG"

"rc://dc1.uio.no/2000/mag/dc1.002000.simul.01145.hlt.pythia_jet_17.MAG")

)
(jobName="dc1.002000.simul.01145.hlt.pythia_jet_17")
(runTimeEnvironment="DC1-ATLAS")
(replicacollection="ldap://grid.uio.no:389/lc=ATLAS,rc=Nordugrid,dc=nordugrid,dc=org")
(CPUTime=2000)
(Disk=1200)

Figure 3: A complete submission xrsl-script for dataset 2000 (phase 1)

was therefore decided to distribute only subsets of
these. All the distributed input sets were then reg-
istered into the Replica Catalog so that at the job-
submission stage, the broker could query the Replica
Catalog for clusters having the necessary input-ﬁle for
that job. The job would then preferably be submit-
ted to such clusters. However, if all clusters with the
required input-ﬁle were full, the job would be submit-
ted somewhere else, and the Grid Manager would use
the information in the Replica Catalog and proceed to
download the input-ﬁle into the local temporary cache
on the cluster. A snapshot of the Replica Catalog with
the dataset 2003 input-ﬁles registered is shown in ﬁg-
ure 5.

Otherwise the simulation were in principle com-
pletely similar to dataset 2000. For this part, there
were 1000 jobs using about 300 CPU-days and produc-
ing approximately 442 GB of output. All output-ﬁles
were uploaded to the designated NorduGrid Storage
Element and registered into the Replica Catalog. In
total for phase 1, NorduGrid produced 780 GB of data
in a total of 1300 jobs.

5.2. Phase 2 – Pileup

From a grid point of view, all 3 phases were very
similar — in principle only the application changed.
The biggest diﬀerence from phase 1 to phase 2 was
that the input-ﬁles of phase 2 were the output-ﬁles
of phase 1. This meant that the Grid Manager dur-
ing job-preparation had to download each input-ﬁle
from the designated NorduGrid Storage Element. It

MOCT011

turned out that this did not pose any signiﬁcant prob-
lems. The minimum-bias ﬁles were distributed as in
phase 1 to the diﬀerent sites before the simulations.
Again – if a chosen site did not have all the required
minimum-bias ﬁles, the Grid Manager would proceed
to download these as well into the local cache.

In this phase 1300 jobs were processed having about
780 GB of input-ﬁles and producing about 1080 GB
of output-ﬁles. All output-ﬁles were again automati-
cally uploaded to a designated Storage Element and
registered into the Replica Catalog.

5.3. Phase 3 – Reconstruction

During phase 3, NorduGrid was assigned addition-
ally 400 partitions from dataset 2000 and 750 parti-
tions from dataset 2001 to be reconstructed together
with the 300 + 1000 partitions from dataset 2000 and
2003 already present on NorduGrid. The input-ﬁles
for these were transferred directly from Russia and the
US to a NorduGrid Storage Element using the ngcopy-
program and used in the simulations from there.

In the reconstruction phase, an external mysql
noise database situated either at CERN or at BNL
was used for the ﬁrst time to provide noise for the
LArCalorimeter-system. This initially posed a num-
ber of problems since most clusters in NorduGrid
do not allow for internet-connection from the worker
nodes and a reconstruction job would thus die because
it could not contact the external database. This was
quickly solved though so that a job could download
a standalone version of the database instead and use

6

CHEP’03, La Jolla, California, 24. - 28. March, 2003

Figure 4: A snapshot of the webpage querying the Replica Catalog for output-ﬁles of dataset 2000 during phase 1. For
each partition, the production node, the number of fully simulated events, the CPU time spent per event, the
submission time, the output sizes of the zebra- and histogram-ﬁles and the logﬁle are be shown. A red exclamation
mark is shown for bad partitions.

Figure 5: A snapshot of the Replica Browser showing the dataset 2003 input-ﬁles as registered in the Replica Catalog.

MOCT011

CHEP’03, La Jolla, California, 24. - 28. March, 2003

7

that in the reconstruction.

With this, no further problems were encountered
and the corresponding partitions have now all been
reconstructed. In this phase the NorduGrid share has
been between 15 and 20 percent.

and Bj¨orn Lundberg of Lund University, Bj¨orn Nils-
son of NBI Copenhagen, Niclas Andersson and Leif
Nixon of NSC Link¨oping, ˚Ake Sandgren of HPC2N
Ume˚a and Jacko Koster of Parallab, Bergen.

6. Conclusion

NorduGrid, as the Scandinavian contribution, has
contributed substantially to the Atlas Data-Challenge
1 in all 3 phases. Important lessons about the Nordu-
Grid middleware has been learned during these stress
tests which has been used to extend the stability, ﬂexi-
bility and functionality of the software and NorduGrid
itself. The software is now being adopted by several
computer centres throughout the Scandinavian coun-
tries which shows exciting prospects for NorduGrid in
Scandinavia in the future.

Acknowledgments

The NorduGrid project was funded by the Nordic
Council of Ministers through the Nordunet2 pro-
gramme and by NOS-N. The authors would like to
express their gratitude to the system administrators
across the Nordic countries for their courage, patience
and assistance in enabling the NorduGrid environ-
ment. In particular, our thanks go to Ulf Mj¨ornmark

References

[1] “ATLAS Data Challenges,” Document pre-
the ATLAS Executive Board,

sented
http://atlasinfo.cern.ch/Atlas/GROUPS/
SOFTWARE/DC/doc/AtlasDCs.pdf

to

[2] P. Eerola et. al., “The NorduGrid architecture
and tools,” in Proc. of the CHEP 2003, PSN:
MOAT003 (2003)

[3] M. Ellert et. al., “The NorduGrid project: Using
Globus toolkit for building Grid infrastructure,”
Nucl. Instr. and Methods A 502 (2003), pp. 407-
410, 2003.

[4] M.

Ellert,
interface,

toolkit
user
[Online],
http://www.nordugrid.org/documents/Nordu-
Grid-UI.pdf

NorduGrid
“The
User’s manual,”

[5] O.

Smirnova,

“Extended Resource

ﬁkation
www.nordugrid.org/documents/xrsl.pdf

Language,”

[Online],

Speci-
http://

[6] K. Czajkowski et. al., “A Resource Management
Architecture for Meta-computing Systems,” in 4th
Workshop on Job Scheduling Strategies for Parallel
Processing. Springer Verlag, 1998.

MOCT011


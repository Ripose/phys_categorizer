Optimizing at the Ergodic Edge

Stefan Boettcher and Martin Frank1

1Physics Department, Emory University, Atlanta, Georgia 30322, USA

(Dated: June 27, 2011)

Abstract

Using a simple, annealed model, some of the key features of the recently introduced extremal

optimization heuristic are demonstrated.

In particular, it is shown that the dynamics of local

search possesses a generic critical point under the variation of its sole parameter, separating phases

of too greedy (non-ergodic, jammed) and too random (ergodic) exploration. Comparison of various

local search methods within this model suggests that the existence of the critical point is essential

for the optimal performance of the heuristic.

PACS number(s): 02.60.Pn, 05.40.-a, 64.60.Cn, 75.10.Nr.

5
0
0
2
 
g
u
A
 
1
3
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
1
0
0
9
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

I.

INTRODUCTION

Many situations in physics and beyond require the solution of NP-hard optimization

problems, for which the typical time needed to ascertain the exact solution apparently grows

faster than any power of the system size [1]. Examples in the sciences are the determination

of ground states for disordered magnets [2, 3, 4, 5, 6, 7] or of optimal arrangements of atoms

in a compound [8] or a polymer [9, 10, 11]. With the advent of ever faster computers, the

exact study of such problems has become feasible [12, 13]. Yet, with typically exponential

complexity of these problems, many questions regarding those systems still are only acces-

sible via approximate, heuristic methods [14]. Heuristics trade oﬀ the certainty of an exact

result against ﬁnding optimal or near-optimal solutions with high probability in polynomial

time. Many of these heuristics have been inspired by physical optimization processes, for

instance, simulated annealing [15, 16] or genetic algorithms [17].

Extremal optimization (EO) was proposed recently [2, 18], and has been used to treat a

variety of combinatorial [19, 20, 21] and physical optimization problems [7, 22, 23]. Com-

parative studies with simulated annealing [18, 19, 24] and other Metropolis [25] based

heuristics [26, 27] have established EO as a successful alternative for the study of NP-

hard problems. EO has found a large number of applications, for instance, in pattern

recognition [28, 29], signal ﬁltering [30], transport problems [31] molecular dynamics sim-

ulations [32], artiﬁcial intelligence [33, 34], social modeling [35], and 3d spin glass mod-

els [26, 36, 37]. There are also a number of studies that have explored basic features of the

algorithm [27, 38], extensions [39, 40, 41], and rigorous performance properties [42, 43].

In this article, we will use a simple, annealed model of a generic combinatorial opti-

mization problem, introduced in Ref. [38], to compare analytically certain variations of local

search with EO, and of Metropolis algorithms such as Simulated Annealing (SA) [15, 16].

This comparison aﬃrms the notion of “optimization at the ergodic edge” that motivated the

τ -EO implementation [7, 18]. This implementation possesses a single tunable parameter,

τ , which separates a phase of “greedy” search from a phase of wide ﬂuctuations, combin-

ing both features at the phase transition into an ideal search heuristic for rugged energy

landscapes [44]. The model helps to identify the distinct characteristics of diﬀerent search

heuristics commonly observed in real optimization problems. In particular, revisiting the

model with a “jammed” state from Ref. [38] proves the existence of the phase transition to

2

be essential for the superiority of EO, at least within a one-parameter class of local search

heuristics. At the phase boundary, EO descends suﬃciently fast to the ground state with

enough ﬂuctuations to escape jams.

This article is organized as follows: In the next section, we introduce the annealed opti-

mization model, followed in Sec. III by a short review of the local search heuristics studied

here, in particular, of one-parameter variations of EO and of Metropolis-based search. Then,

in Sec. IV we compare our analytical results for each heuristic in the annealed model. In

Sec. V we show why versions of EO lacking a phase transition fail to optimize well. We

summarize our results and draw some conclusions in Sec. VI.

II. ANNEALED OPTIMIZATION MODEL

As described in Ref. [38], we can abstract certain combinatorial optimization problems

into a simple, analytically tractable model. To motivate this model we imagine a generic

optimization problem as consisting of a number of variables 1

n, each of which

i

≤

≤

contributes an amount

λi to the overall cost per variable (or energy density) of the system,

−

(The factor 1/2 arises because local cost are typically equally shared between neighboring
variables.) We call λi ≤
λi = 0 is optimal for each variable. Correspondingly, ǫ = 0 is the (optimal) ground state

0 the “ﬁtness” of the variable, where larger values are better and

of the system.

In a realistic problem, variables are correlated such that not all of them

could be simultaneously of optimal ﬁtness. But in our annealed model, those correlations

A concrete example for the above deﬁnitions is provided by a spin glass with the Hamil-

are neglected.

tonian

(1)

(2)

with some quenched random mix of bonds Ji,j =

1, 0, +1

and spin variables σi =

1 [7].

∈ {−

}

±

, counting (minus) the number of violated bonds of each

With λi = σi

j Ji,jσj −

Ji,j|

j |

spin i (among its αi non-zero bonds), it is ǫ = H/n+ǫ0, where ǫ0 is an insigniﬁcant constant.

P

P

ǫ =

1
2n

−

n

λi.

i=1
X

H =

Ji,jσiσj

−

i,j
X

3

We will consider that each variable i is in one of α + 1 (αi = α constant) diﬀerent ﬁtness

states λi. We can specify occupation numbers na, 0

α, for each state a, and deﬁne

a

≤

≤

occupation densities ρa = na/n (a = 0, . . . , α). Hence, any local search procedure [14] with

single-variable updates, say, can be cast simply as a set of evolution equations for the ρa(t),

i. e.

˙ρb =

Tb,aQa.

a
X

The Qa are the probabilities that a variable in state a gets updated; any local search process

(based on updating a ﬁnite number of variables) deﬁnes a unique set of Qa, as we will see

below. The matrix Tb,a speciﬁes the net transition to state b given that a variable in state a is

updated. This matrix allows us to design arbitrary, albeit annealed, optimization problems.

Both, T and Q generally depend on the ρa(t) as well as on t explicitly.

We want to consider the diﬀerent ﬁtness states equally spaced, as in the spin glass example

above, where variables in state a contribute a∆E to the energy to the system. Here ∆E > 0

is an arbitrary energy scale. Thus minimizing the “energy” density

ǫ =

1
2

aρa ≥

0,

a
X

deﬁnes the optimization problem in this model. Conservation of probability and of variables

implies the constraints

Qa = 1,

Ta,b = 0

(0

b

α),

≤

≤

ρa(t) = 1,

˙ρa = 0.

a
X

a
X

While this annealed model eliminates most of the relevant features of a truly hard opti-

mization problem, such as quenched randomness and frustration [45], two basic features of

the evolution equations in Eq. (3) remain appealing: (1) The behavior of a system with a

large number of variables can be abstracted into a relatively simple set of equations, describ-

ing their dynamics with a small set of unknowns, and (2) the separation of update process,

T, and update preference, Q, lends itself to an analytical comparison between diﬀerent

heuristics.

(3)

(4)

(5)

(6)

a
X

a
X

4

III. LOCAL SEARCH HEURISTICS

The annealed optimization model is quite generic for a class of combinatorial optimization

problems. But it was designed in particular to analyze the “Extremal Optimization” (EO)

heuristic [38], which we will review next. Then we will present the update probabilities Q

through which each local search heuristic enters into the annealed model in Sec. II. Finally,

we also specify the update probabilities Q for Metropolis-based local searches, such as SA.

A. Extremal Optimization Algorithm

Here we only give a quick review of the EO heuristic as we will use it below. More

substantive discussions of EO can be found elsewhere [2, 7, 18]. EO is simply implemented

as follows: For a given conﬁguration

n
i=1, assign to each variable σi an “ﬁtness”

σi}

{

λi =

0,

1,

−

−

−

2, . . . ,

α

−

(7)

(e. g. λi =

#violated bonds
}

−{

in the spin glass), so that Eq. (1) is satisﬁed. Each variable

falls into one of only α + 1 possible states. Say, currently there are nα variables with the

worst ﬁtness, λ =

α, nα−1 with λ =

(α

1), and so on up to n0 variables with the

−

best ﬁtness λ = 0. (Note that n =

≤
distribution, preferably with a bias towards lower values of k. Determine 0
P

≤

−

−
i ni.) Select an integer k (1

k

n) from some

a

≤

≤

α such

that

α
b=a+1 nb < k

α
b=a nb. Note that lower values of k would select a “pool” na with
larger value of a, containing variables of lower ﬁtness. Finally, select one of the na variables

≤

P

P

in state a and update it unconditionally. As a result, it and its neighboring variables change

their ﬁtness. After all the eﬀected λ’s and n’s are reevaluated, the next variables is chosen

for an update, and the process is repeated. The process would continue to evolve, unless an

extraneous stopping condition is imposed, such as a ﬁxed number of updates. The output

of local search with EO is the best conﬁguration, with the lowest ǫ in Eq. (1), found up to

the current update step.

Clearly, a random selection of variables for such an update, without further input of

information, would not advance the local search towards lower-cost states. Thus, in the

“basic” version of EO [18], each update one variable among those of worst ﬁtness would be

made to change state (typically chosen at random, if there is more than one such variable).

5

This provides a parameter-free local search of some capability. But variants of this basic

elimination-of-the-worst are easily conceived. In particular, Ref. [18] already proposed τ -EO,

a one-parameter (τ ) selection with a bias for selecting variables of poor ﬁtness on a slowly

varying (power-law) scale over the ranking 1

n of the variables by their λi. In detail,

k

≤

≤

τ -EO is characterized by a power-law distribution over the ﬁtness-ranks k,

Pτ (k) =

1

−

n1−τ k−τ

τ

−

1

(1

k

≤

≤

n).

It is a major point of this paper to demonstrate the usefulness of this choice. Hence, we

will compare the eﬀect of this choice with a plausible alternatives, µ-EO, which uses an

exponential scale,

Pµ(k) =

1

−

e−µn e−µk

eµ

1

−

(1

k

≤

≤

n).

In fact, we show that the exponential cut-oﬀ µ in µ-EO, which is ﬁxed during a run, provides

inferior results to τ -EO. Unlike τ -EO, µ-EO does not have a critical point aﬀecting the

behavior of the local search.

Although Ref. [42] has shown rigorously, that an optimal choice is given by using a sharp

threshold when selecting ranks, the actual value of this threshold at any point in time is

typically not obvious (see also Ref. [43]). We will simulate a sharp threshold s (1

Ps(k)

1
1 + er(k−s)

∝

(1

k

≤

≤

n)

for r

. Since we can only consider ﬁxed thresholds s, which gives results similar in

character to µ-EO, it is not apparent how to shape the rigorous results into a successful

via

→ ∞

algorithm.

B. Update Probabilities for Extremal Optimization

As described in Sec. III A (and in Ref. [38]), each update of τ -EO a variable is selected

based on its rank according to the probability distribution in Eq. (8). When a rank k(

has been chosen, a variable is randomly picked from state α, if k/n

ρα, from state α

≤

if ρα < k/n

ρα + ρα−1, and so on. We introduce a new, continuous variable x = k/n, for

≤

large n approximate sums by integrals, and rewrite P (k) in Eq. (8) as

(8)

(9)

s

n)

≤

≤

(10)

n)

1,

≤

−

(11)

p(x) =

τ
−
nτ −1

1

−

1

x−τ

1
n ≤

(cid:18)

x

1

,

≤

(cid:19)

6

where the maintenance of the low-x cut-oﬀ at 1/n will turn out to be crucial. Now, the

average likelihood in EO that a variable in a given state is updated is given by

Qα =

p(x)dx =

ρα

1/n

Z

ρα+ρα−1

Qα−1 =

p(x)dx =

. . .

ρα

Z

1

1−ρ0

Z

Q0 =

p(x)dx =

1
nτ −1

1

−

1

−

1
nτ −1

1

−

ρ1−τ
α −

nτ −1

,

(cid:1)
(ρα−1 + ρα)1−τ

(cid:0)
1
nτ −1

(cid:2)

1

(1

−

−

ρ0)1−τ

,

(cid:2)

(cid:3)

ρ1−τ

α

,

−

(cid:3)

where in the last line the norm

a ρa = 1 was used. These values of the Q’s completely

describe the update preferences for τ -EO at arbitrary τ .

P

Alternatively, if we consider the µ-EO algorithm introduced in Eq. (9), we have to replace

the power-law distribution in Eq. (11) with an exponential distribution:

p(x) =

1

−

µ

e−µ(1−1/n) e−µ(x−1/n)

1
n ≤

(cid:18)

x

1

,

≤

(cid:19)

Hence, for µ-EO we have

Qα =

Qα−1 =

. . .

Q0 =

e−µ(ρα−1/n)
e−µ(1−1/n) ,

1
−
1
−
e−µ(ρα−1/n)

e−µ(ρα+ρα−1−1/n)

,

−
e−µ(1−1/n)

1

−

e−µ(1−ρ0−1/n)
1

−
e−µ(1−1/n)

e−µ(1−1/n)

.

−

ps(x)

1
1 + er(nx−s)

∝

(

1
n ≤

x

1),

≤

Similarly, we can proceed with the threshold distribution in Eq. (10) to obtain

with some proper normalization. While all the integrals to obtain Q are elementary, we do

not display the rather lengthy results here.

Note that all the update probabilities in each variant of EO are independent of T (i. e.

any particular model), which remain to be speciﬁed. This is quite special, as the following

case of Metropolis algorithms shows.

C. Update Probabilities for Metropolis Algorithms

It is more diﬃcult to construct Q for Metropolis-based algorithms [25] like simulated

annealing [15, 16]. Let’s assume that we consider a variable in state a for an update.

7

(12)

(13)

(14)

(15)

Certainly, Qa would be proportional to ρa, since variables are randomly selected for an
update. The Boltzmann factor e−β∆Ea for the potential update from time t

t + 1 of a

→

variable in a, aside from the inverse temperature β(t), only depends on the entries for Ta,b:

∆Ea = n∆ǫa,

bρb(t + 1)

bρb(t)

,

−

Xb

#a

=

∼

=

=

n
2 "

n
2 "

n
2 "
n
2

Xb

Xb

b ˙ρb(t)

,

#a

b

Tb,cQc

,

#a

Xb

c
X
bTb,a,

Xb

(16)

(17)

where the subscript a expresses the fact that it is a given that a variable in state a is

considered for an update. Hence, we ﬁnd for the average probability of an update of a

variable in state a

Qa =

ρamin

1, exp

(

1

N

n
2

β
"−

bTb,a

,

#)

Xb

where the norm

is determined via

a Qa = 1. Unlike for EO, the update probabilities

for SA are model-speciﬁc, i. e. depend on T.

P

N

IV. COMPARISON OF LOCAL SEARCH HEURISTICS

To demonstrate the use of these equations, we consider a simple model of an energetic bar-

rier with only three states (α = 2) and a constant ﬂow matrix Tb,a = [

δb,a +δ(2+b mod 3),a]/n,

−

depicted in Fig. 1. Here, variables in ρ1 can only reach their lowest-energy state in ρ0 by

ﬁrst jumping up in energy to ρ2. Eq. (3) gives

˙ρ0 =

Q0 + Q2) ,

˙ρ1 =

Q1) ,

˙ρ2 =

(18)

1
n

(Q0 −

1
n

(Q1 −

Q2) ,

1
n

(

−

with Q discussed in Sec. III B for the variants of EO.

Given T, we can now also determine the update probabilities for Metropolis according

to Eqs. (17). Note that for a = 2 we can evaluate the min as 1, since

b bTb,a < 0 always,

P

8

1−θ+ρ

1

1/2

θ−ρ

1

1/2

1/2

1/2

ρ
2

ρ
1

ρ
0

Flow jam

FIG. 1: Flow diagram with energetic barri-

FIG. 2: Same as Fig. 1, but with a model

ers. Arrows indicate the average number of

leading to a jam. Variables can only transfer

variables transfered, nTb,a, from a state a to

from ρ2 to ρ0 through ρ1, but only if ρ1 < θ.

a state b, given that a variable in a gets up-

Once ρ1 = θ, ﬂow down from ρ2 ceases until

dated. Diagonal elements Ta,a correspond-

ρ1 reduces again.

ingly are negative, accounting for the out-

ﬂow. Note that variables transferring from

ρ1 to ρ0 most ﬁrst jump up in energy to ρ2.

while for a = 0, 1 the min always evaluates to the exponential. Properly normalized, we

obtain

Q0 =

Q2 =

(1

(1

−

−

ρ0e−β/2
e−β/2)ρ2 + e−β/2 , Q1 =
e−β/2)ρ2 + e−β/2 .

ρ2

(1

−

ρ1e−β/2
e−β/2)ρ2 + e−β/2 ,

(19)

It is now very simple to obtain the stationary solution: For ˙ρ = 0, Eqs. (18) yields

Q0 = Q1 = Q2 = 1/3, and we obtain from Eq. (12) for τ -EO:

ρ0 = 1

nτ −1 +

,

ρ2 =

nτ −1 +

,

ρ1 = 1

ρ0 −

−

ρ2,

(20)

1
1−τ

2
3

(cid:19)

1
1−τ

1
3

(cid:19)

ρ0 =

ln

1
µ

2
3

(cid:20)

+

eµ(1−1/n)

,

ρ2 = 1

−

(cid:21)

+

eµ(1−1/n)

,

ln

1
3

(cid:20)

2
3

(cid:21)

ρ1 = 1

ρ0 −

−

ρ2,

(21)

for µ-EO:

−

(cid:18)

1
3

1
3

2
3

(cid:18)

1
µ

9

and for Metropolis:

ρ0 =

1
2 + e−β/2 ,

ρ1 =

1
2 + e−β/2 ,

ρ2 =

e−β/2
2 + e−β/2 .

For EO with threshold updating, we obtain

ρ0 =

1
3 −

ρ2 =

+

1
3

1
3n −
1
nr
2
3n
1
nr

+

+

+

s
n −

ln

1
3nr
(cid:2)
(enr + ers)

ln

h
s
n −

ln

2
3nr
(cid:2)
(enr + ers)

ln

h

1 + er(n−s)

(cid:3)
1 + er(1−s)

1

3 + e

r
3 (2n+1)

1 + er(n−s)

1
3

,

(cid:0)
1 + er(n−s)

(cid:1)

(cid:0)

(cid:3)
1 + er(1−s)

2

3 + e

r
3 (n+2)

1 + er(n−s)

(cid:1)

2
3

i

,

i

(cid:1)

and, assuming a threshold anywhere between 1 < s < n, for r

(cid:0)

(cid:1)

ρ0 = 1

2s + 1
3n

,

−

ρ2 =

s + 2
3n

,

ρ1 = 1

−

:

(cid:0)
→ ∞
ρ0 −

ρ2

(22)

(23)

(24)

Therefore, according to Eq. (4), Metropolis reaches its best, albeit sub-optimal, cost

ǫ = 1/4 > 0 at β

, due to the energetic barrier faced by the variables in ρ1, see Fig. 1.

→ ∞

(Since ﬂuctuations from the mean are suppressed in this model, even a slowly decreasing

temperature schedule as in Simulated Annealing would not improve results.) In turn, µ-EO

does reach optimality (ρ0 = 1, hence ǫ = 0), but only for µ

. Note that in this limit,

→ ∞

µ-EO reduces back to the “basic” version of EO discussed in Sec. III A. The result for

threshold updating in EO are more promising: near-optimal results are obtained, to within

O(1/n), for any ﬁnite threshold s. But again, results are best for small s

1, in which

→

limit we revert back to “basic” EO.

The result for τ -EO is most remarkable: For n

at τ < 1 EO remains sub-optimal,

→ ∞

but reaches the optimal cost for all τ > 1! As discussed in Ref. [38], this transition at τ = 1

separates an (ergodic) random walk phase with too much ﬂuctuation, and a greedy descent

phase with too little ﬂuctuation, which would trap τ -EO in problems with broken ergodicity

[46]. This transition derives generically from the scale-free power-law in Eq. (8), as was

already argued on the basis of numerical results for real NP-hard problems in Refs. [18, 20].

V. JAMMING MODEL FOR µ-EO

In this section, we revisit the “jammed” model treated in Ref. [38] for τ -EO and repeat

that calculation for µ-EO. As in the example in Sec. IV, µ-EO proves inferior to τ -EO:

10

Lacking the phase of optimal performance in the τ -parameter space, the required ﬁne-tuning

of µ does not succeed in satisfying the conﬂicting constraints imposed on the search.

Naturally, the range of phenomena found in a local search of NP-hard problems is not

limited to energetic barriers. After all, so far we have only considered constant entries for

Tb,a. Therefore, in our next model we want to consider the case of T depending linearly on

the ρi discussed in Ref. [38] for τ -EO. This model highlights signiﬁcant diﬀerences between

the τ -EO and the µ-EO implementation.

From Fig. 2, we can read oﬀ T and obtain for Eq. (3):

˙ρ0 =

Q0 +

Q1

,

1
2

(cid:21)

1
n
1
n

−
1
2

(cid:20)

(cid:20)

˙ρ1 =

Q0 −

Q1 + (θ

ρ1)Q2

,

−

(cid:21)

(25)

. and ˙ρ2 =

˙ρ0 −
−

˙ρ1 from Eq.(6). Aside from the dependence of T on ρ1, we have also

introduced the threshold parameter θ. In fact, if θ

1, the model behaves eﬀectively like

the previous model, and for θ

0 there can be no ﬂow from state 2 to the lower states

≥

≤

at all. The interesting regime is the case 0 < θ < 1, where further ﬂow from state 2 into

state 1 can be blocked for increasing ρ1, providing a negative feed-back to the system. In

eﬀect, the model is capable of exhibiting a “jam” as observed in many models of glassy

dynamics [47, 48, 49], and which is certainly an aspect of local search processes. Indeed, the

emergence of such a “jam” is characteristic of the low-temperature properties of spin glasses

and real optimization problems: After many update steps most variables freeze into a near-

perfect local arrangement and resist further change, while a ﬁnite fraction remains frustrated

(temporarily in this model, permanently in real problems) in a poor local arrangement [50].

More and more of the frozen variables have to be dislodged collectively to accommodate

the frustrated variables before the system as a whole can improve its state. In this highly

correlated state, frozen variables block the progression of frustrated variables, and a jam

emerges.

Inserting the set of Eqs. (14) for α = 2 into the model in Eqs. (25), we obtain

˙ρ0 =

˙ρ1 =

1
n
1
n

1

1

eµ(1−1/n)

1

1

(cid:20)

−

eµ(1−1/n)

1

−

(cid:20)

−

3
2

−
1
2

3
2

1
2

−

eµρ0 +

eµ(ρ0+ρ1)

,

(cid:21)

+

eµρ0

eµ(ρ0+ρ1) + (θ

ρ1)

eµ(1−1/n)

eµ(ρ0+ρ1)

, (26)

−

(cid:0)

−

(cid:21)

(cid:1)

At large times t, the steady state solution,

˙ρ = 0, yields for ρ0 after eliminating ρ1 the

11

implicit equation

per variable as

0 =

3
2 −

3
2

1
µ

−

(cid:20)

eµρ0 +

θ

ln

3

2e−µρ0

eµ(1−1/n)

3eµρ0

,

(27)

−

(cid:0)

(cid:21)

(cid:1)

(cid:0)

−

(cid:1)

and according to Eq. (4), again eliminating ρ1 and ρ2 in favor of ρ0, we can express the cost

ln

3

2e−µρ0

,

ǫ = 1

−

ln

1
µ

ρ0 −
√3
(cid:20)

1
2µ

(cid:18)

1 +

(cid:0)
1
2θ

−

(cid:19)(cid:21)

∼

(µ

1),

(cid:1)
≫

(28)

Unlike the corresponding equations in Ref. [38], which had a phase transition similar to the

solution for τ -EO in Sec. IV, Eqs. (27-28) have no distinct features. In fact, as shown in

Fig. 3, ǫ(µ) behaves similar to the solution for µ-EO in Sec. IV: The relation is independent
, ρ0 →

of n to leading order and only for µ

1 and ǫ

→ ∞

→

0.

While the steady state (t

) features of this model do not seem to be much diﬀerent

→ ∞

from the model in Sec. IV, the dynamics at intermediate times t is more subtle. In particular,

as was shown in Ref. [38], a “jam” in the ﬂow of variables towards better ﬁtness may ensue

under certain circumstances. The emergence of the jam depends on initial conditions, and

its duration will prove to get longer for larger values of µ. If the initial conditions place

a fraction ρ0 > 1

θ already into the lowest state, most likely no jam will emerge, since

−

ρ1(t) < θ for all times, and the ground state is reached in t = O(n) steps. But if initially

ρ0 > θ, and µ is suﬃciently large, µ-EO will drive the system to a situation

ρ1 + ρ2 = 1
where ρ1 ≈
becomes extremely slow, delayed by the µ-dependent, small probability that a variable in

−
θ by preferentially transferring variables from ρ2 to ρ1. Then, further evolution

state 1 is updated ahead of all variables in state 2.

Clearly, this jam is not a steady state solution of Eq. (26). It is not even a meta-stable

solution since there are no energetic barriers. For instance, simulated annealing at zero

temperature would easily ﬁnd the solution in t = O(n) without experiencing a jam.

In

reality, a hard problem would most certainly contain combinations of jams, barriers, and

possibly other features.

make the Ansatz

To analyze the jam, we consider initial conditions leading to a jam, ρ1(0) + ρ2(0) > θ and

ρ1(t) = θ

η(t)

−

12

(29)

n=10
n=100
n=1000
n=10000
Steady-State

>
ε
<

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

10

20

40

50

60

30

µ

FIG. 3: Plot of the energy

averaged over many µ-EO runs after tmax = 100n updates with

ǫ
h

i

diﬀerent initial conditions as a function of µ for n = 10, 100, 1000, and 10000 and θ = 1/2. Plotted

also is ǫ =

a aρa/2 as a function of µ resulting from the jam-free steady-state solution (t

) of

→ ∞

Eqs. (27-28) n =

P

. The plots show little variability with system size for large n, and remain quite

sub-optimal for ﬁnite µ. As for τ

in Ref. [38],

→ ∞

ǫ
h

i →

2/3

θ/2

−

−

θ3/6 = 19/48

0.396 [51]

≈

∞

for µ

.

→ ∞

with η

1 for t <
∼

≪

corrections.

tjam, where tjam is the time at which ρ0 →

1. To determine tjam, we apply

Eq. (29) to the evolution equations in (26) to get

˙ρ0 ∼

1
n

1

eµ(1−1/n)

3
2

1

1
(cid:20)

−

−

eµρ0 +

eµ(ρ0+θ)

,

1
2

(cid:21)

(30)

where the relation for ˙ρ1 merely yields a self-consistent equation to determine sub-leading

We can now integrate Eq. (30) from t = 0 (assuming that any jam emerges almost

13

instantly) up to tjam, where ρ0 = 1:

ρ0(tjam)=1

tjam ∼

n (eµ

1)

−

ρ0(0)

Z

dξ
1
2eµθ

eµξ

1

−

3
2 −

The integral is easily evaluated, and we ﬁnd for large values of µ:

(cid:0)

(cid:1)

tjam ∼

2n
µ

eµ(1−θ−ρ0(0))

(µ

1).

≫

(31)

(32)

Instead of repeating the lengthy calculation in Ref. [38] for the ground state energy averaged
over all possible initial conditions for ﬁnite runtime tmax ∝
with a few obvious remarks: A ﬁnite fraction of the initial conditions will lead to a jam, hence
will require a runtime tmax ≫
say, ǫ
1/n, would require µ
∼
resolve the jam would grow exponentially with system size n, since from Eq. (32) tjam ∼
with c = 1

tjam to reach optimality. Yet, to reach a quality minimum,

1 according to Eq. (28). Thus, the require runtime to

ρ0(0) > 0, by deﬁnition of the jam above.

n, we can content ourselves here

ecn

≫

∼

n

θ

−

−

In conclusion, µ-EO can never quite resolve the conﬂicting demands of pursuing quality

ground states with a strong bias for selecting variables of low ﬁtness (i. e. µ

1) and the

ensuing lack of ﬂuctuations required to break out of a jam, which drives up tjam. Simulations

of this model with µ-EO in Fig. 3 indeed show that the best results for

are obtained

≫

ǫ
i

h

at intermediate values of µ, which converge to a large, constant error for increasing n. In
contrast, τ -EO provides a range near τopt −
∼
out of any jam in a time near-linear in n while still attaining optimal results as it does for

1/ ln(n) [38] with small enough τ to ﬂuctuate

1

any τ > 1, see e. g. Sec. IV.

VI. CONCLUSION

We have presented a simple model to analyze the properties of local search heuristics.

The model with a simple energetic barrier demonstrates the characteristics of a number of

these heuristics, whether athermal (EO and its variants) or thermal (Metropolis) [27]. In

particular, it plausibly describes a number of real phenomena previously observed for τ -EO

in a tractable way. Finally, in a more substantive comparison on a model with jamming,

the exponential distribution over ﬁtnesses, µ-EO proves unable to overcome the conﬂicting

constraints of resolving the jam while ﬁnding good solutions. This is in stark contrast with

the identical calculation in Ref. [38] using a scale-free approach with a power-law distribution

14

over ﬁtnesses in τ -EO. In this approach, a sharp phase transition emerges generically between

an expansive but unreﬁned exploration on one side (“ergodic” phase), and a greedy but

easily trapped search on the other (“non-ergodic” phase), with optimal performance near

the transition.

[1] M. R. Garey and D. S. Johnson, Computers and Intractability, A Guide to the Theory of

NP-Completeness (W. H. Freeman, New York, 1979).

[2] New Optimization Algorithms in Physics, Eds. H. Rieger and A. K. Hartmann, (Springer,

Berlin, 2004).

[3] K. F. Pal, Physica A 223, 283-292 (1996), and 233, 60-66 (1996).

[4] A. K. Hartmann, Phys. Rev. B 59, 3617-3623 (1999), and Phys. Rev. E 60, 5135-5138 (1999).

[5] M. Palassini and A. P. Young, Phys. Rev. Lett. 85, 3017 (2000).

[6] J. Houdayer and O. C. Martin, Phys. Rev. Lett. 83, 1030 (1999).

[7] S. Boettcher and A. G. Percus, Phys. Rev. Lett. 86, 5211-5214 (2001).

[8] K. K. Bhattacharya and J. P. Sethna, Phys. Rev. E 57, 2553 (1998).

[9] E. Tuzel and A. Erzan, Phys. Rev. E 61, R1040 (2000).

[10] H. Frauenkron, U. Bastolla, E. Gerstner, P. Grassberger, and W. Nadler, Phys. Rev. Lett. 80,

3149-3152 (1998).

[11] T. Prellberg and J. Krawczyk, Phys. Rev. Lett. 92, 120602 (2004).

[12] R. G. Palmer and J. Adler, Int. J. Mod. Phys. C 10, 667 (1999).

[13] C. Desimone, M. Diehl, M. J¨unger, P. Mutzel, G. Reinelt, G. Rinaldi, J. Stat. Phys. 80,

[14] Modern Heuristic Search Methods, Eds. V. J. Rayward-Smith, I. H. Osman, and C. R. Reeves

487-496 (1995).

(Wiley, New York, 1996).

[15] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, Science 220, 671-680 (1983).

[16] P. Salamon, P. Sibani, and R. Frost, Facts, Conjectures, and Improvements for Simulated

Annealing (Society for Industrial & Applied Mathematics, 2002).

[17] J. Holland, Adaptation in Natural and Artiﬁcial Systems (University of Michigan Press, Ann

Arbor, 1975).

[18] S. Boettcher and A. G. Percus, Artiﬁcial Intelligence 119, 275-286 (2000).

15

[19] S. Boettcher and A. G. Percus, in GECCO-99: Proceedings of the Genetic and Evolutionary

Computation Conference (Morgan Kaufmann, San Francisco, 1999), 825-832.

[20] S. Boettcher and A. G. Percus, Physical Review E 64, 026114 (2001).

[21] S. Boettcher and A. G. Percus, Phys. Rev. E 69, 066703 (2004).

[22] S. Boettcher, Phys. Rev. B 67, R060403 (2003).

[23] S. Boettcher, Extremal Optimization for Sherrington-Kirkpatrick Spin Glasses, Euro. Phys.

J. B (in press), condmat/0407130.

[24] S. Boettcher, J. Math. Phys. A 32, 5201-5211 (1999).

[25] N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller and E. Teller, Equation of

state calculations by fast computing machines, J. Chem. Phys. 21 (1953) 1087–1092.

[26] J. Dall and P. Sibani Comp. Phys. Comm. 141, 260-267 (2001).

[27] S. Boettcher and P. Sibani, Euro. Phys. J. B 44, 317-326 (2005).

[28] S. Meshoul and M. Batouche, Lecture Notes in Computer Science 2449, 330-337 (2002).

[29] S. Meshoul and M. Batouche, Int. J. Pattern Rec. and AI 17, 1111-1126 (2003).

[30] E. Yom-Tov, A. Grossman, and G. F. Inbar, Biological Cybernatics 85, 395-399 (2001).

[31] F. L. Sousa, V. Vlassov and F. M. Ramos, Heat Transf. Eng. 25, 34-45 (2004).

[32] T. Zhou, W.-J. Bai, L.-J. Cheng, and B.-B. Wang, Phys. Rev. E 72, 016702 (2005).

[33] M. E. Menai and M. Batouche, Lecture Notes in Computer Science 2718, 592-603 (2003).

[34] M. E. Menai and M. Batouche, in Proceedings of the International Conference on Artiﬁcial

Intelligence, IC-AI2003, Eds. H. R. Arabnia et. al., 257-262 (2003).

[35] J. Duch and A. Arenas, Community detection in complex networks using Extremal Optimiza-

tion, cond-mat/0501368.

[36] J.-S. Wang and Y. Okabe, J. Phys. Soc. Jpn. 72, 1380 (2003).

[37] R. N. Onody and P. A. de Castro, Physica A 322, 247-255 (2003).

[38] S. Boettcher and M. Grigni, J. Phys. A. 35, 1109 (2002).

[39] A. A. Middleton, Phys. Rev. E 69, 055701 (R) (2004).

[40] F. L. de Sousa and V. Vlassov and F. M. Ramos, Lecture Notes in Computer Science 2723,

[41] F. L. de Sousa, F. M. Ramos, R. L. Galski, and I. Muraoka, in Recent Developments in

Biologically Inspired Computing, Eds. L. N. De Castro and F. J. Von Zuben (Idea Group Inc.,

375-376 (2003).

2004).

16

[42] F. Heilmann, K.-H. Hoﬀmann, and P. Salamon, Europhys. Lett. 66, 305-310 (2004).

[43] K.-H. Hoﬀmann, F. Heilmann, and P. Salamon, Phys. Rev. E 70, 046704 (2004).

[44] Landscape Paradigms in Physics and Biology, Ed. H. Frauenfelder (Elsevier, Amsterdam,

1997).

Singapore, 1987).

[45] M. M´ezard, G. Parisi, and M. A. Virasoro, Spin Glass Theory and Beyond, (World Scientiﬁc,

[46] F. T. Bantilan and R. G. Palmer, J. Phys. F: Metal Phys. 11, 261-266 (1981).

[47] H. M. Jaeger, S. R. Nagel, R. P. Behringer, Rev. Mod. Phys 68 1259-1273 (1996).

[48] E. Ben-Naim, J. B. Knight, E. R. Nowak, H. M. Jaeger, and S. R. Nagel, Physica D 123,

380-385 (1998).

[49] F. Ritort, Phys. Rev. Lett. 75, 1190-1193 (1995).

[50] R. G. Palmer, D. L. Stein, E. Abrahams, and P. W. Anderson, Phys. Rev. Lett. 53, 958

(1984).

P
for

ǫ
h

i

[51] Ref. [38] has in error in Eq. (28): The general expression for the energy ǫ in the integrand,

2
i=0 iρi/2 = ρ1/2 + ρ2, should be replaced by θ/2 + ρ2 in the jam, which leads to this value

for large τ or µ, instead of 7/16

0.44 quoted there.

≈

17


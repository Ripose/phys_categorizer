3
0
0
2
 
n
u
J
 
2
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
7
9
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

1

Architecture of the ATLAS High Level Trigger Event Selection Software

S. Armstrong, K.A. Assamagan, J.T. Baines, C.P. Bee, M. Biglietti, A. Bogaerts, V. Boisvert, M.
Bosman, S. Brandt, B. Caron, P. Casado, G. Cataldi, D. Cavalli, M. Cervetto, G. Comune, A.
Corso-Radu, A. Di Mattia, M. Diaz Gomez, A. dos Anjos, J. Drohan, N. Ellis, M. Elsing, B. Epp, F.
Etienne, S. Falciano, A. Farilla, S. George, V. Ghete, S. Gonzalez, M. Grothe, A. Kaczmarska, K.
Karr, A. Khomich, N. Konstantinidis, W. Krasny, W. Li, A. Lowe, L. Luminari, H. Ma, C. Meessen,
A.G. Mello, G. Merino, P. Morettini, E. Moyse, A. Nairz, A. Negri, N. Nikitin, A. Nisati, C. Padilla, F.
Parodi, V. Perez-Reale, J.L. Pinfold, P. Pinto, G. Polesello, Z. Qian, S. Rajagopalan, S. Resconi, S.
Rosati, D.A. Scannicchio, C. Schiavi, T. Schoerner-Sadenius, E. Segura, T. Shears, S. Sivoklokov,
M. Smizanska, R. Soluk, C. Stanescu, G. Stavropoulos, S. Tapprogge, F. Touchard, V. Vercesi, A.
Watson, T. Wengler, P. Werner, S. Wheeler, F.J. Wickens, W. Wiedenmann, M. Wielers, H. Zobernig
The ATLAS High Level Trigger Group∗

Presented by Monika Grothe
European Organization for Nuclear Research, CERN, 1211 Geneva 23, Switzerland

The ATLAS High Level Trigger (HLT) consists of two selection steps: the second level trigger and the event
ﬁlter. Both will be implemented in software, running on mostly commodity hardware. Both levels have a
coherent approach to event selection, so a common core software framework has been designed to maximize
this coherency, while allowing suﬃcient ﬂexibility to meet the diﬀerent interfaces and requirements of the two
diﬀerent levels. The approach is extended further to allow the software to run in an oﬀ-line simulation and
reconstruction environment for the purposes of development. This paper describes the architecture and high
level design of the software.

1. INTRODUCTION

The Large Hadron Collider LHC, currently under
construction at CERN and scheduled to start data-
taking in 2007, will collide protons on protons at a
center-of-mass energy of 14 TeV. At the design lumi-
nosity of 1034cm−2s−1, each bunch crossing, occurring
at 25 ns intervals, will result in 23 collisions on aver-
age.

The trigger of the ATLAS experiment, one of the
multi-purpose detectors at the LHC, must be able to
reduce the interaction rate of 109 Hz to below the
maximum rate that can be processed by the oﬀ-line
computing facilities, O(102) Hz. In addition, the AT-
LAS trigger must be able to handle the full data vol-
ume of a detector of the complexity and size of AT-
LAS, with O(108) read-out channels. It is under these
constraints that the ATLAS trigger must retain the
capability of identifying previously undetected and
rare physics processes. A Standard Model Higgs par-
ticle with a mass of 120 MeV, decaying into two pho-
tons, is for example expected to occur at a rate of
10−13 of the interaction rate, the proverbial pin in the
haystack.

Three distinct trigger steps are foreseen. While the
ﬁrst step (Level-1 trigger) is implemented as a hard-
ware trigger, the second and third steps, Level-2 trig-

∗http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG/
HLT/AUTHORLISTS/chep2003.pdf

WEPT004

ger and Event Filter, are software triggers and are
usually referred to as the ATLAS High Level Trigger
(HLT).

This article describes the current status of the high-
level design and the implementation of the HLT event
selection software. The validation of the software is
on-going and aims at the imminent goal of the HLT
Technical Design Report (TDR), due in summer 2003,
as well as at preparing for the ﬁrst stage of ATLAS
commissioning.

After a short overview of the ATLAS trigger, the
general strategy of the HLT event selection software
and its main software components are described. Sub-
ject of this article is the part of the code that provides
the infrastructure in which to run the HLT selection
algorithms. Speciﬁc HLT event selection algorithms
are not discussed. Using the same software environ-
ment to develop and maintain (oﬀ-line) on the one
hand and on the other hand to deploy (on-line) the
HLT selection software is a central HLT design aim.
This article also discusses our experience with appro-
priating oﬀ-line code for on-line use.

2. THE ATLAS TRIGGER

Data at the LHC are produced with the bunch
crossing rate of 40 MHz. The ATLAS trigger (see
Fig. 1) has the task to reduce this rate to an output
rate of O(200) Hz after the Event Filter. The Level-
1 trigger [1] reduces the initial 40 MHz to less than
75 kHz in less than 2.5 µs, the maximum output rate

2 CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

Level 1
Level 1
trigger
trigger

 
f
o
 
n
o
i
g
e
R

I
o
R

 
t
s
e
r
e
t
n
I

Level 2
r Level 2
trigger
trigger

r
e
e
g
g
g
g
i
i
r
r
T
T

 
 
l
l
e
e
v
v
e
e
L
L
 
 
h
h
g
g
i
i
H
H

Event
Event
Filter
Filter

Figure 1: Overview of the ATLAS trigger

and latency the trigger hardware can tolerate. In the
HLT [2], where the boundary between the two trigger
steps is purposefully kept ﬂexible, the Level-2 trigger
will reduce the rate to O(2) kHz and the Event Filter
further to O(200) Hz. The available average latency
of the two steps is substantially diﬀerent, with ∼10 ms
for the Level-2 trigger and ∼1 s for the Event Filter.
Central to the ATLAS trigger design is the Region-
of-Interest (RoI) concept. The Level-1 trigger looks
for regions of potentially interesting activity in the
Calorimeters and the Muon Spectrometer that may
correspond to candidates for high pT objects (electro-
magnetic, muon, tau/hadronic and jet clusters). The
Level-2 trigger selection uses the RoI information (its
type, position and the pT of the highest trigger thresh-
old passed) of the Level-1 trigger as seeds for its pro-
cessing. This strategy keeps the amount of raw data
to be passed to the Level-2 trigger for processing at
only a few per cent of the full event information.

While the raw data of the ATLAS subdetectors are
held in readout buﬀers (ROB), the Level-2 trigger
processes RoI information and raw data on a Level-2
Linux PC farm. On each node of the Level-2 farm, a
Processing Application executes the Level-2 event se-
lection software. Specialized, speed-optimized Level-
2 trigger algorithms carry out the feature extraction
in the region-of-interest indicated by the Level-1 trig-
ger. While the Level-1 trigger does not combine the
information from diﬀerent subdetectors, the Level-2
trigger covers all subdetectors sequentially and gains
additional information from combining their data.

Once an event has been accepted by the Level-2
trigger, the raw data of the full event are passed to the
Event Builder, which in turn passes on the fully-built
event to the Event Filter Linux PC farm. On the farm
nodes, independent Processing Applications execute
oﬀ-line-type selection algorithms that have access to
the full event data, including the latest calibration and

WEPT004

alignment information.

Though the type of selection algorithms used by
the Level-2 trigger and the Event Filter are diﬀerent,
the software infrastructure that the algorithms use is
kept as similar as possible between the two trigger lev-
els. In order to achieve a high level of ﬂexibility, it is
desirable to retain to some degree the possibility to
deploy an HLT algorithm either in the Level-2 trigger
or in the Event Filter. This article describes the com-
mon software infrastructure that is used throughout
the HLT. Given the much more stringent performance
constraints of the Level-2 trigger, this approach meets
limits that will be discussed as well.

3. THE HIGH LEVEL TRIGGER EVENT
SELECTION SOFTWARE

The HLT event selection software has four main
components: the HLT algorithms that perform event
reconstruction and feature extraction, the HLT Steer-
ing that calls a certain subset of the available HLT al-
gorithms in a certain sequence depending on the types
of RoI received from the Level-1 trigger, the HLT
Data Manager that gives access to the information
contained in the raw data, and the HLT raw Event
Data Model that speciﬁes the object representation of
the raw data to be used by the HLT algorithms. This
section describes how the four components work to-
gether and discusses the design and current implemen-
tation of the three components, HLT Steering, HLT
Data Manager, HLT Event Data Model, that provide
the infrastructure for the HLT algorithms both in the
Level-2 trigger and in the Event Filter.

3.1. Working principle

The following example illustrates the working prin-
ciple of the HLT event selection software: The Level-1
trigger ﬁnds two isolated electromagnetic clusters with
pT > 20 GeV each. This is a possible signature for
the decay Z → e+e− for which the ATLAS trigger ac-
cepts an event, if both electrons are isolated and have
a minimum pT of 30 GeV.

The HLT validates this hypothesis in a step-by-step
process. Intermediate signatures are produced as re-
sult of the algorithmic processing of a given step, and
each intermediate signature is examined in order to
be able to reject the hypothesis at the earliest possi-
ble moment. This procedure is managed by the HLT
Steering package.

The validation procedure starts from the Level-1
RoI information as seed, as shown in Fig. 2. In this
example, there are two isolated electromagnetic RoIs
with pT > 20 GeV (“EM20i” in Fig. 2). The HLT
Steering calls an HLT algorithm (“Cluster Shape”) to

CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

3

STEP 4

e30i  

+

e30i  

Signature

m

STEP  3

Iso
lation

Iso
lation

e30

+

e30

e30

+

e30

STEP 2

pT>
30GeV

pT>
30GeV

pT>
30GeV

pT>
30GeV

e   

+

e   

e   

+

e   

e   

+

e   

STEP 1

track
finding

track
finding

track
finding

track
finding

track
finding

track
finding

ecand

+

ecand

ecand

+

ecand

ecand

+

ecand

ecand

+

ecand

Cluster 
shape 

Cluster 
shape 

Cluster 
shape 

Cluster 
shape 

Cluster 
shape 

Cluster 
shape 

Cluster 
shape 

Cluster 
shape 

EM20i

+

EM20i

EM20i

+

EM20i

EM20i

+

EM20i

EM20i

+

EM20i

Signature
m
(intermediate)

Signature
m
(intermediate)

Signature
m
(intermediate)

Level1 seed 

m

Figure 2: Example illustrating the working principle of the HLT selection software. The diﬀerent steps follow each
other in time. Each step is shown with the complete processing chain preceding it. Hence for step 4, the full chain of
processing steps is shown, with time increasing from bottom to top.

determine the cluster shape of the ﬁrst seed. The clus-
ter shape is found consistent with the electron hypoth-
esis and the result is an electron candidate (“ecand”).
The HLT Steering proceeds in the same way with the
second seed and obtains a second electron candidate.
These two electron candidates are considered as an
intermediate signature (“ecand + ecand”). Since the
event after this ﬁrst step is still compatible with the
Z → e+e− signature, the HLT Steering starts the sec-
ond step of the validation procedure. In this second
step, the two electron candidates provide the seeds for
the algorithmic processing. The HLT Steering calls
a track ﬁnding HLT algorithm (“track ﬁnding”) for
each of the two electron candidates. If for each of the
electron candidates a track pointing to them is found,
this second step results in two electrons that consti-
tute another intermediate signature (“e + e”). Using,
in the manner described above, the output of one step
of the validation procedure as input to the next, the
HLT Steering calls the appropriate HLT algorithms in
the appropriate sequence to verify that the two elec-
trons each have pT > 30 GeV and are both isolated.
When ﬁnally, after the fourth step, the signature “e30i
+ e30i” has been reached, the ATLAS trigger accepts
the event as a candidate for the decay Z → e+e− with
the desired electron characteristics.

To guarantee early rejection, the HLT Steering can
reject an event after any step during the validation
procedure. If the ﬁrst step in the example above had

WEPT004

not resulted in two electron candidates with the ap-
propriate cluster shapes, the HLT Steering would have
stopped the validation of the speciﬁc Z decay signa-
ture under scrutiny without passing to step 2. Note
also that the HLT processing is not organized verti-
cally (carry out the full sequence of algorithms to ar-
rive ﬁrst at a reconstructed electron for the ﬁrst RoI
and only then do the same for the second) but hori-
zontally (carry out only the reconstruction foreseen in
step x for the ﬁrst and then the second seed and use
the output as seeds for the next step).

3.2. Steering

In this section, the main ingredients of the HLT
Steering package, Trigger Menus, Sequence Tables and
Trigger Elements, are discussed brieﬂy. For a detailed
description, see [3].

3.2.1. Trigger Menu and Sequence Table

In the example above, only one signature (hypoth-
esis) and for this signature only one sequence of al-
gorithms and intermediate signatures was considered.
Diﬀerent from this simple example, the ATLAS HLT
accepts an event if it is consistent with at least one
signature (hypothesis) in a whole catalog of possible
ones. Also, depending on the Level-1 RoIs, there may
be more than one possible sequence of algorithms and

4 CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

Implementation

Flow

HLT algorithm reconstruction realm
HLT algorithm reconstruction realm
Supporting information
Supporting information

Steering realm
Steering realm
Hypotheses
Hypotheses

etc.

etc.

etc.

DataObject
cluster  

usesuses

DataObject

1 ..*

track

DataObject
CaloCells

usesuses

1 ..*

DataObject
CaloCluster

TriggerElement
“e” 

seeded   by
seeded   by

TriggerElement
“ecand”

seeded    by
seeded    by

usesuses

usesuses

usesuses

DataObject
Level1 EMRoI

TriggerElement
“EM20i”

etc.

e

track
finding

ecand

Cluster
shape

EM20i

Figure 3: Example illustrating the way Trigger Elements are used for communication between the HLT Steering and
the HLT algorithms. Time increases from bottom to top.

intermediate signatures per signature. But as in the
example above, the HLT processing is organized hori-
zontally, not vertically. This means also that the HLT
does not examine one signature and only then the
Instead, for each signature, the processing is
next.
broken up in single steps and the HLT Steering car-
ries out the processing per step, not per signature.

Each step is accompanied by one Trigger Menu and
one Sequence Table. A Trigger Menu is a list of all
intermediate signatures in its step for which an event
can be accepted. The Sequence Table speciﬁes which
HLT algorithms are to be executed, given the seeds as
input to the step.

For more details, see [4].
The HLT Steering deploys algorithms in a very spe-
ciﬁc way. The task of an HLT algorithm is not to per-
form a full scan for, e.g., all muon candidates in the
event, independent of their location in the detector.
Instead, the algorithm has to carry out a localized
reconstruction, guided by a seed, limited to the area
in the detector indicated by the seed. Hence, in the
same event the HLT Steering calls the relevant algo-
rithm once per seed, i.e. depending on the number of
seeds possibly several times.

3.2.2. Trigger Elements

Communication between the HLT Steering and the
HLT algorithms happens with the help of Trigger El-
ements. The HLT Steering hands over the seeds to

a HLT algorithm by means of Trigger Elements. A
HLT algorithm returns the result of its processing to
the HLT Steering in the form of a Trigger Element.

A Trigger Element has a label, but does not hold
any data content. It is related to Data Objects (RoIs,
tracks, clusters, etc.) and other Trigger Elements in
a navigable way. The signatures in a Trigger Menu
consist of logical AND/OR combinations of Trigger
Elements.

The following example (see Fig. 3), simpliﬁed as
compared to the example in Fig. 2, illustrates how the
HLT Steering uses Trigger Elements to communicate
with HLT algorithms. The Level-1 trigger ﬁnds one
isolated electromagnetic cluster with pT > 20 GeV.
Assume for the sake of the argument that the ATLAS
trigger accepts events with at least one isolated elec-
tron with pT > 30 GeV. Then the RoI found by the
Level-1 trigger is a possible signature for an event of
this type.

In step 1, the electromagnetic RoI found by the
Level-1 trigger is the seed for the HLT cluster shape
algorithm. The HLT Steering creates a Trigger Ele-
ment with label “EM20i” that has a navigable “uses”
relationship to the Level-1 RoI as Data Object. The
output of the HLT cluster shape algorithm is an elec-
tron candidate that the algorithm hands back to the
HLT Steering as a new Trigger Element with label
“ecand”. This Trigger Element has a navigable “uses”
relationship to the calorimeter cluster that was the re-
sult of the algorithmic processing. Since the HLT clus-

WEPT004

CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

5

Level1
Level1

Level2
Level2

EventFilter
EventFilter

s
B
O
R
 
0
0
6
1
~

Level1 Result

Level2
Algorithm

Level2 Result

r
e
d

l
i

u
B

t
n
e
v
E

EventFilter
Algorithm

bytestream

bytestream
on demand

Figure 4: Overview of where in the ATLAS trigger raw data are sent to the HLT in bytestream format.

ter shape algorithm was seeded by the Level-1 RoI,
its corresponding output Trigger Element, “ecand”,
has a navigable “seeded by” relationship with its in-
put Trigger Element, “EM20i”. Navigable “uses” re-
lationships are also possible between Data Objects.
This allows the HLT track ﬁnding algorithm in step
3 to access the calorimeter cluster on which its seed
is based, and to access the calorimeter cells on which
the calorimeter cluster is based. In addition, via the
“seeded by” relationship between its input Trigger El-
ement and the input Trigger Element of the preceding
step, the algorithm has also access to the underlying
Data Objects of that previous step.

In the design, the Trigger Elements represent the
intermediate hypotheses that correspond to the inter-
mediate signatures that the HLT Steering examines
to decide whether to continue with the next step of
the algorithmic processing. The Data Objects, on
the other hand, represent the information support-
ing these hypotheses. Diﬀerentiating between Trigger
Elements and Data Objects has the advantage of dif-
ferentiating clearly between the HLT Steering realm,
containing the hypotheses, and the HLT algorithm re-
construction realm, containing the supporting infor-
mation.

3.3. Raw Event Data Model

Unlike the typical oﬀ-line situation where the data
are stored in a convenient format, e.g. objects in a ﬁle
or database, the HLT event selection software running
on the HLT Linux farms can access data only in their
raw format and needs to invoke the ATLAS Data Flow
system [5, 6, 7, 8] in order to do so.

The Data Flow system transfers raw data to the
HLT in bytestream format. Figure 4 illustrates where
in the ATLAS trigger system raw data is exchanged
In the event of a positive Level-1
via bytestreams.

WEPT004

trigger decision, the Level-2 trigger receives the Level-
1 trigger result, in particular the RoIs, by means of a
bytestream. In addition, the Level-2 trigger can de-
mand the bytestream fragment from any of the ∼1600
ROBs that each hold the raw data of a part of the
ATLAS detector. When the Level-2 trigger accepts
an event, the Event Builder receives the Level-2 trig-
ger result and the bytestream from all ∼1600 ROBs
and it passes the fully-built event by way of another
bytestream to the Event Filter.

3.3.1. Raw Data Objects and Detector Elements

In order to allow the HLT algorithms access to the
raw data information in these bytestreams, this infor-
mation has to be represented in object form, as Raw
Data Objects (RDOs). The HLT algorithms access
the raw data information by means of the RDOs and
the transient event store, where the RDOs are stored.
The Raw Event Data Model (Raw EDM) is part of
the HLT EDM and speciﬁes the design of the RDOs
for each ATLAS subdetector and each bytestream in
the Data Flow system. The Raw EDM has to map
onto one another data of very diﬀerent granularity.
The Data Flow system provides data with a minimum

Figure 5: Deﬁnition of a DetectorElement for the
example of the ATLAS Muon Spectrometer.

DetectorElement

6 CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

Transient Event Store

Container

ID

ID

ID

DetElem

DetElem

DetElem …   etc

O
D
R

O
D
R

O
D
R

c
t
e

O
D
R

O
D
R

O
D
R

c
t
e

…

Figure 6: Organization of raw data information in the
transient event store by means of Detector Element
objects of which each is a collections of RDO objects.

granularity of one ROB, while a HLT algorithm typi-
cally needs data organized by geometric detector unit.
For example, in the ATLAS Muon Spectrometer, a
geometric detector unit of interest for a HLT selec-
tion algorithm is a chamber. A chamber is a physical
unit, for example of Monitored Drift Tubes (MDT),
that share the same physical support structure, are
installed as a whole and may be described by a single
set of condition (e.g. alignment, etc.) constants. The
Raw EDM speciﬁes for all ATLAS subdetectors the
object representation of the relevant geometric detec-
tor units, the Detector Elements (see Fig. 5).

In the case of the MDT chambers of the Muon
Spectrometer, the Detector Element corresponds to
a chamber.
In the same way as a chamber can be
viewed as a collection of readout channels, a Detector
Element object is a collection of RDO objects which
each hold the information of one readout channel.

RDOs are stored in the transient event store ordered
according to Detector Elements. The transient store
contains for each subdetector a container of Detector
Elements (see Fig. 6). Each Detector Element has
a unique identiﬁer [9] by which it can be retrieved
from the store, thus allowing with one request to the
transient store to retrieve collections of RDOs, which
respresent collections of readout channels in geometric
detector units.

3.4. Bytestream converters

The raw data in the bytestream format in which
they are held by the ROBs can be viewed as a repre-
sentation of the raw data information in an event. The
RDOs, organized by Detector Element, are another
representation of this information. The conversion
from bytestream representation to RDO representa-
tion is the task of Bytestream Converters. They carry
out the conversion on demand of an HLT algorithm.
Their implementation consists of a general infrastruc-
ture part that uses subdetector speciﬁc methods for
decoding the information from bytestream format to
RDO format. The general part is, for example, re-

sponsible for storing the RDOs in the correct Detector
Element order in the transient store.

Raw data access via Bytestream Converters pro-
ceeds in the following way (see Fig. 7). The HLT
Steering calls an HLT algorithm by passing to it a
Trigger Element with a navigable link to a RoI Data
Object that contains the RoI position. This RoI can,
for example, be located in the Muon Spectrometer.
The HLT algorithm typically needs access only to
the raw data in those Muon Spectrometer chambers
within a certain region around the RoI position (see
Fig. 8). These chambers correspond to a certain set
of Detector Elements. The task of identifying this
set of Detector Elements, given a region in the pseu-
dorapidity η and the polar angle φ, is performed by
the Region Selector [10]. The Region Selector returns
to the calling HLT algorithm the list of identiﬁers of
the appropriate set of Detector Elements. The HLT
algorithm requests the Detector Elements from the
transient store with the help of this identiﬁer list.

At this stage, two situations can occur. If the re-
quested Detector Elements are already available in the
transient store, the store simply returns them. If they
are not available, however, the corresponding raw data
information ﬁrst has to be requested from the Data
Flow system. In this case, the transient event store
invokes the correct Bytestream Converter, which in
turn requests the raw data in bytestream format from
the appropriate ROBs. From the succession of 32-bit
words sent by each ROB, the Bytestream Converter
extracts the relevant information and uses it to ini-
tialize the collections of RDOs. The thus obtained
collections of RDOs, or Detetcor Elements, are stored
in the transient store, which passes them on to the
HLT algorithm.

The extraction of raw data information into RDOs
can be supplemented by additional data preparation
algorithms, e.g. cluster ﬁnders for the Silicon Track-
ers. The object representation of extracted and pre-
pared raw data are called Reconstruction Input Ob-
jects. Their deﬁnition is part of the HLT EDM. They
are organized inside the transient store in collections
as are RDOs, and can be requested by an HLT algo-
rithm in the same way as RDOs.

4. OFF-LINE CODE IN ON-LINE
CONTEXT

A high level of integration of the code designed for
oﬀ-line use and the code designed for on-line use is
clearly a desirable design goal. Ideally, it should be
possible to develop and maintain on-line software on
simulated and real data in a well-tested, ﬂexible and
user-friendly environment as is oﬀered by the oﬀ-line
software. This does not only apply to the HLT algo-
rithms, but also to the HLT infrastructure code used

WEPT004

CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

7

Algorithm

Region
Selector

Transient
EventStore

Data Access

Trans.
Event
Store

Byte
Stream
Converter

Data source
organized 
by ROB

HLT 
Algorithm

Region
Selector

region 

list DetElem IDs

list DetElem IDs

list DetElem IDs

DetElems

ROB ID

raw event 
data

DetElems

Figure 7: HLT algorithm access to raw data information by means of Bytestream Converters and the transient event
store.

Example: Muon Spectrometer

x

RoI position

Figure 8: Example illustrating the task of the Region
Selector.

by them. Furthermore, it clearly enhances the HLT
ﬂexibility in dealing with changes in the LHC run-
ning conditions when it is possible to migrate software
freely between oﬀ-line environment and Event Filter
and between Event Filter and Level-2 trigger.

In the previous sections, the common software in-
frastructure of Level-2 trigger software and Event Fil-
ter software has been described. In order to use this
HLT software infrastructure and the HLT algorithms
in an oﬀ-line environment, the HLT software has to
comply with the basic design principles of the ATLAS
oﬀ-line framework.

The ATLAS oﬀ-line framework, Athena [11],
is
based on a GAUDI [12] (developed by the LHCb
experiment) core. As a consequence, Athena has
adopted a number of the basic principles of the
GAUDI design. Examples are the separation of the

WEPT004

data from the algorithms producing and consuming
the data, separation of the transient and the persis-
tent representation of data and the use of converters
that convert one data representation into another.

For the HLT event selection software this means, for
example, that HLT algorithms are forced to commu-
nicate indirectly, via the transient event store. This
requirement is met by the use of Trigger Elements,
as described in Sec. 3.2.2.
It also means that per-
sistent data can be made available to the HLT event
selection software only by means of converters that
convert them into a transient representation that is
then stored in the transient event store, from where
HLT algorithms can request data. The Bytestream
Converters discussed in Sec. 3.4 and their use as illus-
trated in Fig. 7 comply with this requirement. Fur-
thermore, using the HLT event selection software in
Athena implies at the very least common interfaces to
general services, and may even mean using the very
same services, e.g. database access tools, geometry
service etc., by Athena and by the on-line framework.

The HLT event selection software also needs to
comply with the much more stringent on-line perfor-
mance requirements concerning speed and robustness.
Hence, also any oﬀ-line-imposed elements or code el-
ements imported from the oﬀ-line environment must
meet the on-line performance requirements. They are
considerably more stringent than would be otherwise
needed for pure oﬀ-line use.

The following section describes the current oﬀ-line
dependencies introduced into the HLT software for the
above mentioned reasons. The experience gained so
far in the attempt of using oﬀ-line code in the on-line
context of the trigger is discussed.

8 CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

HLT DataFlow
software
Event Filter

HLTSSW

HLT selection software

Processing 
Application

Level2

Processing 
Application

ROBData
Collector

HLT Core Software

Steering

Data
Manager

EventData
Model

HLT Algorithms

HLT
Algorithms

1..*

Monitoring 
Service

1..*

MetaData 
Service

<<import>>

<<import>>

<<import>>

<<import>>

Athena/
Gaudi

StoreGate

Offline 
EventDataModel

Offline 
Reconstruction
Algorithms

Offline Architecture &
Core Software

Offline Reconstruction

Package
Interface
Dependency

Figure 9: The four components of the HLT selection software and their dependencies on oﬀ-line software.

4.1. Current Off-Line Dependencies

A high level of integration between oﬀ-line and on-
line code can be achieved in diﬀerent ways. One pos-
sibility consists in choosing the interfaces in the oﬀ-
line and in the on-line code such that on-line software
can be used in the oﬀ-line framework and vice versa.
Another possibility is to use the same framework for
oﬀ-line and on-line purposes. In the concrete ATLAS
situation, this means using Athena as on-line frame-
work as well. In addition, any combination of these
two extremes is possible.

In the current implementation of the HLT soft-
ware, Athena is used as framework in the Event Fil-
ter. Level-2 employs a specialized framework that
uses interfaces compatible with those in Athena, so
that any Level-2 infrastructure and algorithm code
can also be used within Athena (see next section). The
HLT infrastructure code (e.g. transient event store,
Bytestream Converters, etc.) currently re-uses code
developed in Athena.

Figure 9 depicts the current oﬀ-line dependencies
in the HLT event selection software. The HLT frame-
work depends on the oﬀ-line framework. For the
Event Filter this means re-use of Athena as frame-
work,
for the Level-2 trigger this means use of a
slightly modiﬁed version (see next section) of the
Athena core software, based on GAUDI. The Data
Manager in the current implementation is the oﬀ-line
transient event store, StoreGate [13]. The HLT EDM

re-uses the oﬀ-line EDM. In the Event Filter, oﬀ-line
algorithms are re-used as selection algorithms. In the
Level-2 trigger, specialized Level-2 algorithms are em-
ployed which, however, are set up such that they can
also be run in the Athena framework.

The execution of the HLT event selection software
is controlled by a Processing Application that is part
of the HLT Data Flow and Data Acquisition software.

4.2. The Special Level-2 Environment

The performance constraints imposed by the HLT
are most stringent for the Level-2 trigger. There, the
average latency available to the HLT selection soft-
ware for accepting or rejecting an event is ∼10 ms.
The Level-2 software needs to be able to handle a
data input rate of up to 75 kHz and to sustain a data
output rate of O(2) kHz error-free and deadtime-free
for extended periods of time. Requirements of such
stringency with respect to speed and robustness are
usually not met by software designed for pure oﬀ-line
use. An additional complication arises from the need
for multi-threading in the Level-2 trigger.

In order to keep the idle time of the CPUs in the
Level-2 Linux PC farm to a minimum, each CPU pro-
cesses in parallel three diﬀerent events, i.e. each Level-
2 CPU carries out at the same time three execute
loops. This is in marked diﬀerence to the situation
foreseen in Athena, where the initialization phase is

WEPT004

CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

9

Athena

Initialize 

Execute 

Finalize 

Level2

Initialize 

Finalize 

Execute

Execute

Execute

Figure 10: Diﬀerence between the oﬀ-line framework
Athena and the framwork needed for the Level-2 trigger,
where multi-threading is required.

followed by a single execute loop that ends in a ﬁ-
nalization phase (see Fig. 10).
In the case of run-
ning inside Athena, Athena controls the event execute
loop; in the multi-threaded Level-2 software the con-
trol needs to be with the HLT Data Flow software. In
addition, in the Level-2 software, all algorithms and
services have to be conﬁgured and initialized strictly
outside of the event execute loop. Hence Athena can-
not be used as HLT framework for the Level-2 trigger.
In order to comply nonetheless with the design
goal that the Level-2 event selection software be us-
able with the Athena framework for development and
maintenance purposes, an interface layer between the
HLT selection and the HLT Data Flow software was
implemented, the so-called Steering Controller [14].
The Steering Controller uses only a minimal set of
it uses a modiﬁca-
In addition,
GAUDI features.
tion to the GAUDI base libraries that allows thread-
speciﬁc instantiation. The standard GAUDI com-
mand EventLoopM gr → executeEvent() is, for ex-
ample, replaced by EventLoopM gr (threadID) →
executeEvent(), where threadID corresponds to
the number of the thread (1, 2, 3).
Accord-
ingly, GAUDI services are called with the com-
mand AnyServiceN eededByAlgo (threadID) and al-
gorithms with AnyHLT ALgorithm (threadID), etc.
The possibility of thread-speciﬁc instantiation has
since been included in GAUDI as a standard feature.
The Steering Controller can be used within the oﬀ-
line framework Athena because in a single-thread en-
vironment the (threadID) suﬃx collapses to a blank.
Beyond this the Steering Controller uses only stan-
dard GAUDI features, which are all contained in the
GAUDI-based Athena.

The special Level-2 feature of running in multi-
thread mode may cause problems when using stan-
dard software tools as, e.g., the Standard Template Li-
brary (STL), even when the software components used
are in principle thread-safe. An unexpected prob-
lem of this kind arose with the speciﬁc implementa-
tion of STL containers in use in the HLT code. The
container implementation, though thread-safe, proved
highly thread-ineﬃcient by setting unnecessary locks.
The problem was overcome by modifying by hand the
container allocator implementation [15].

It should be noted that the problems arising from

WEPT004

the use of multiple threads in the Level-2 trigger soft-
ware are not present for the Event Filter. The Event
Filter uses multiple threads, but the Data Flow and
HLT event selection software are set up in a diﬀerent
way. As a result, it is possible to use Athena as is as
the framework for running HLT selection algorithms
in the Event Filter [16]. However, when Athena was
used for the ﬁrst time in the Event Filter, it became
aparent that an inacceptable 30% of the total process-
ing time per event was spent in the oﬀ-line Raw EDM
available at that time. This problem has since been
remedied, but illustrates the need for a thorough eval-
uation of the suitability of oﬀ-line code for use in the
HLT software. It also illustrates the need for a close
and continuous collaboration between the HLT (“on-
line”) and the “oﬀ-line” communities in order to adopt
successfully “oﬀ-line” software for “on-line” use.

5. CONCLUSIONS AND OUTLOOK

The architecture of the HLT selection software de-
scribed in this article is currently being validated
by implementing the software for full vertical trigger
slices. Two vertical slices are currently under develop-
ment, an electron/gamma slice and a muon slice. The
vertical trigger slices comprise the software to simu-
late the full chain of trigger decisions, starting with
the Level-1 trigger, that leads to identifying an event
as containing a single electron, gamma or muon. Inte-
gration of the HLT selection software with the other
components of the HLT software is on-going with the
goal of running the electron/gamma and muon verti-
cal slices in testbeds of the foreseen Linux PC farms.
Detailed measurements of timing and physics perfor-
mance of each part of the HLT software are under way
and will be reported in the HLT Technical Design Re-
port in summer 2003.

A central goal of the current implementation of the
HLT selection software is a high level of integration be-
tween oﬀ-line and on-line code. This goal has sparked
oﬀ a very fruitful collaboration between the HLT and
the on-line software developer communities. The at-
tempt of adapting and re-using oﬀ-line code in the
HLT software has been carried furthest in the speciﬁc
way of accessing raw data in the HLT by means of
Bytestream Converters that are hidden behind a call
to the oﬀ-line transient event store, StoreGate.

Given the ever increasing available CPU speed, al-
lowing an ever higher level of abstraction, the concept
of re-using oﬀ-line code in the on-line context appears
a logical development. However, given the stringent
constraints and performance requirements of the HLT,
the present implementation of the HLT selection soft-
ware and the heavy re-use it makes of oﬀ-line software
is clearly experimental. This is in particular true for
the Level-2 trigger. The results of the on-going valida-
tion eﬀort will show if the chosen ansatz is sustainable.

10 CHEP03 – Conference for Computing in High Energy and Nuclear Physics, La Jolla, CA, USA, March 2003

6. ACKNOWLEDGMENT

We would like to acknowledge the help and support
of the ATLAS Data Acquisition Group and the AT-
LAS Oﬀ-Line and Detector software groups.

References

[1] ATLAS collab, “ATLAS Level-1 Trigger: Tech-
nical Design Report”, CERN-LHCC-98-014,
ATLAS-TDR-12, June 1998.

[2] ATLAS collab., “ATLAS High-Level Triggers,
DAQ and DCS: Technical Proposal”, CERN-
LHCC-2000-017, March 2000;
ATLAS collab., “ATLAS High-Level Triggers,
DAQ and DCS: Technical Design Report”,
in
preparation.

[3] G.Commune, “The Algorithm Steering and Trig-
ger Decision mechanism of the ATLAS High Level
Trigger”, these proc.

[4] M.Elsing, “Conﬁguration of event-selection crite-
ria in the ATLAS trigger system”, these proc.
[5] G.Lehmann, “The DataFlow system of the AT-

LAS Trigger and DAQ”, these proc.

[6] S.Stancu, “The use of Ethernet in the DataFlow
of the ATLAS Trigger and DAQ”, these proc.

[7] S.Wheeler, “Supervision of the ATLAS High

Level Trigger System”, these proc.

[8] S.Kolos, “On-Line Monitoring software frame-
work in the ATLAS experiment”, these proc.
[9] R.D.Schaﬀer, “Use of a generic identiﬁcation
scheme connecting events and detector descrip-
tion in the ATLAS experiment”, these proc.
[10] V.Boisvert, “The Region of Interest Strategy for

the ATLAS Second Level Trigger”, these proc.

[11] A.Bazan et al., “The Athena Data Dictionary and

Description Language”, these proc.;
V.Fine, H.Ma, “Root Based Persistency in
Athena (ATLAS)”, these proc.

[12] P.Mato (ed.), “GAUDI - Architecture design doc-

ument”, LHCb-98-064 COMP, Nov 1998.

[13] P.Calaﬁura, “The StoreGate: a Data Model for

the ATLAS Software Architecture”, these proc.

[14] S. Gonzalez et al., “Use of Gaudi in the LVL2
Trigger: The Steering Controller”, ATL-DAQ-
2002-012, Jun 2002.

[15] S.Gadomski, “Experience with multi-threaded
C++ applications in the ATLAS DAQ system”,
these proc.

[16] C.Bee et al., “HLT Validation of Athena”, ATL-

DAQ-2002-005, Jan 2002.

WEPT004


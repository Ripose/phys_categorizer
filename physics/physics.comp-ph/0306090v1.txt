3
0
0
2
 
n
u
J
 
2
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
0
9
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

1

Lattice QCD Calculations on Commodity Clusters at DESY

A. Gellrich, D. Pop, P. Wegner, H. Wittig
Deutsches Elektronen-Synchrotron (DESY)
e-mail: Andreas.Gellrich@desy.de, Peter.Wegner@ifh.de
M. Hasenbusch, K. Jansen
John von Neumann Institut
e-mail: Karl.Jansen@ifh.de

f ¨ur Computing (NIC) and DESY

Lattice Gauge Theory is an integral part of particle physics that requires high performance computing in the
multi-Tﬂops regime. These requirements are motivated by the rich research program and the physics milestones
to be reached by the lattice community. Over the last years the enormous gains in processor performance, mem-
ory bandwidth, and external I/O bandwidth for parallel applications have made commodity clusters exploiting
PCs or workstations also suitable for large Lattice Gauge Theory applications. For more than one year two
clusters have been operated at the two DESY sites in Hamburg and Zeuthen, consisting of 32 resp. 16 dual-CPU
PCs, equipped with Intel Pentium 4 Xeon processors. Interconnection of the nodes is done by way of Myrinet.
Linux was chosen as the operating system. In the course of the projects benchmark programs for architectural
studies were developed. The performance of the Wilson-Dirac Operator (also in an even-odd preconditioned
version) as the inner loop of the Lattice QCD (LQCD) algorithms plays the most important role in classifying
the hardware basis to be used. Using the SIMD Streaming Extensions (SSE/SSE2) on Intel’s Pentium 4 Xeon
CPUs give promising results for both the single CPU and the parallel version. The parallel performance, in
addition to the CPU power and the memory throughput, is nevertheless strongly inﬂuenced by the behavior
of hardware components like the PC chip-set and the communication interfaces. The paper starts by giving
a short explanation about the physics background and the motivation for using PC clusters for Lattice QCD.
Subsequently, the concept, implementation, and operating experiences of the two clusters are discussed. Finally,
the paper presents benchmark results and discusses comparisons to systems with diﬀerent hardware components
including Myrinet-, GigaBit-Ethernet-, and Inﬁniband-based interconnects.

1. Introduction

Lattice ﬁeld theory has established itself as an in-
tegral part of high energy physics by providing im-
portant, non-perturbatively obtained results for many
It complements standard ap-
physical observables.
proaches of theoretical particle physics such as pertur-
bation theory and phenomenology, becoming an indis-
pensable method to allow for ﬁrst principles interpre-
tation of experimentally obtained data. The aim of
lattice ﬁeld theory is to understand the structure of
ﬁeld theories, to test these theories against experiment
and in this way to ﬁnd physics behind the standard
model of which we know that it has to be there, but
not at what energy scale it should appear.

Another important aspect of lattice ﬁeld theory is
its high computational needs. A distinctive feature
of the numerical computations in lattice ﬁeld theory
is the required performance of several Tﬂops and this
performance is needed “in one piece”. Such a require-
ment can only be fulﬁlled with massively parallel ar-
chitectures having many processors that are connected
via a very fast interconnecting network and work si-
multaneously on the same problem as a single ma-
chine. This distinguishes the computational needs in
lattice ﬁeld theory from the farming concepts usually
employed in grid computing of experimental high en-
ergy physics.

The combination of high computational needs and
the motivation for this in high energy physics as de-
scribed above, makes a conference such as CHEP an

TUIT001-003

ideal place to present the status and the perspectives
of lattice ﬁeld theory. What strengthens the con-
nection even more is that, at least, a data grid is
also needed in lattice ﬁeld theory to exchange expen-
sive data, the so-called conﬁgurations and propagators
that are generated. An international lattice Data Grid
initiative has been started [1].

The computational requirements with high perfor-
mance in the multi-Tﬂops regime and ﬁne grained
communication can be realized on commercial super-
computers like Hitachi [2] or IBM [3], with specialized
machines such as APE (Array Processor Experiment)
[4] or QCDOC (QCD on Chip) [5], or with PC clus-
ters as they are discussed in this article. Although
commercial supercomputers can conveniently be used,
since they are maintained by the corresponding com-
puter centers, their price is normally very high, the
eﬃciency of lattice ﬁeld theory code is often not opti-
mal and they have many users from diﬀerent applica-
tion ﬁelds leading consequently to a situation that a
single user will not get much computer time.

Specialized,

custom-made machines

like APE
and QCDOC are cost-eﬀective and oﬀer the best
price/performance value for multi-Tﬂops installations.
On the other hand, a lot of work has to be spent by
the physicists themselves in order to develop, build
and maintain these machines. A compromise between
supercomputers and custom-made machines might
therefore be commercial PC clusters and in this ar-

2

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

ticle1 we will concentrate on the experiences we have
gained with these kind of machines at DESY. We also
refer to [6] for a thorough discussion on the above
point.

One observation in lattice ﬁeld theory is that peo-
ple gather in larger and larger collaborations. This
results ﬁrst of all from the huge computer resources
required.
It is aimed to use these resources wisely,
not duplicating results and ﬁnd the best strategies for
solving the physics problems. One example of such
an eﬀort is SciDAC [7] in the US. Another one is the
Lattice Forum (LATFOR) initiative in Germany [8].
Associated to LATFOR are also researchers in Aus-
tria and Switzerland.
In this initiative groups from
many universities and research institutes who work
on lattice ﬁeld theory combine their eﬀorts to reach
physics milestones in diﬀerent areas of lattice ﬁeld the-
ory. They try to coordinate their physics program, to
develop and share software and share conﬁgurations
and propagators, which play the role of very expensive
raw data for lattice ﬁeld theory computations.
The research areas of LATFOR are broad and cover

• ab initio calculations of QCD with dynamical

quarks

– Hadron spectrum and structure functions

– fundamental parameters of QCD, i.e. the
running strong coupling αs(µ) and the
quark masses ¯m(µ)

– B-physics

• matter under extreme conditions

– QCD thermodynamics

– QCD at non-vanishing baryon density

• Non-QCD physics

– Electroweak standard model

– Supersymmetry

• Conceptual developments

– exact chiral symmetry on the lattice

– acceleration of the continuum limit

– non-perturbative renormalization

– ﬁnite size eﬀects

– algorithm development

It would be too demanding (and it is not the pur-
pose of this article) to discuss these topics in detail.
The main target of lattice calculations is certainly
QCD and we will therefore give in the next section
a –presumably too short– introduction to QCD and
discuss a few examples of the results that can be ob-
tained.

2. Physics Motivation

2.1. Quantum Chromodynamics on the
lattice

In lattice ﬁeld theory [12], the continuum space-
time is replaced by a 4-dimensional, euclidean grid
with a lattice spacing a, measured in fm. The advan-
tage of this procedure is that now the theory can be
simulated on a computer. In order to obtain back the
desired results in the continuum, the values of observ-
ables obtained at non-vanishing values of the lattice
spacing have to be extrapolated in a continuum limit
to zero lattice spacing. At this stage, the compari-
son to experiments becomes possible and a test of the
validity of the model considered can be performed.

On the pure computational side we are dealing with
two kind of ﬁelds. The ﬁrst one represents the quarks
and are given by complex vectors

Ψ(x)α,a,nf , 


α = 1, 2, 3
color index
a = 1, 2, 3, 4 Dirac index
nf = 1, . . . , 6 ﬂavor index

.

(1)



These ﬁelds live on the 4-dimensional space time point
x which, on the computer, is represented by integer
numbers, x = (x1, x2, x3, x4) = a · (i, j, k, l) , 1 ≤
i, j, k, l ≤ N . A second set of ﬁelds represents the glu-
ons of QCD and are given by SU(3) matrices U , 3 × 3
complex matrix with unit norm. The ﬁelds U (x, µ)α,β
carry again color indices through which they interact
with the quark ﬁelds. The gluon ﬁelds live on the
links of the lattice that connect points x and x + µ in
direction µ = 1, 2, 3, 4. The interaction is described
by the action2

S = ¯ΨM

−1Ψ .

(2)

The action of eq. (2) requires the inverse of the so-
called fermion matrix M , or, to be more speciﬁc, the
vector X = M −1Ψ. Without giving the exact deﬁni-
tion of the matrix M , the problem is that the fermion

1This paper covers three talks [9, 10, 11] given during the
parallel session on Lattice Gauge Computing at Computing in
High Energy Physics, UCSD, La Jolla, USA, March 24-28, 2003.

2Of course, in QCD the quark ﬁelds are represented as Grass-
mann variables. We discuss here the bosonized form of the ac-
tion as it is used in simulations.

TUIT001-003

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

3

matrix is high dimensional O(106) ⊗ O(106) and the
numerical solution of the linear set of equations

M · X = Ψ

(3)

employing such a matrix is clearly very demanding.
It helps, however, that the matrix is sparse. For such
a case a vast literature for solving eq. (3) exists [13].
What is important is that the algorithms that can be
employed are self-correcting and many of them can be
proven to converge in at most a number of steps that
corresponds to the dimension of the matrix. Thus we
are left with a well posed and regular numerical prob-
lem. Of course, in practice the number of iterations
is much smaller and typical numbers of iterations to
solve eq. (3) are 100 − 1000. Note that in each of these
iterations the matrix M has to be applied to a vector
of size O(106).

In order to give a feeling about the computational
demand, let us give an example. Let us consider a lat-
tice of size 323 · 64 as is realistic in todays calculations
in the quenched theory, where internal quark loops are
neglected. Then we would need to solve eq. (3) twelve
times per conﬁguration for each color and Dirac com-
ponent. Each solution needs O(200) iterations and we
want to perform this on typically O(1000) conﬁgura-
tions. Since one application of the matrix M on such a
lattice needs 2.8 Gﬂop, we are left with approximately
6.6 Pﬂop for obtaining only one physical result for a
single set of parameters, i.e. the bare coupling and
the bare quark masses. On your standard PC which
might run with 500 Mﬂops sustained for this problem,
you would hence need about ﬁve months.
In order
to reach control over the ﬁnite size eﬀects, the chiral
extrapolation and the continuum limit, simulations at
many values of the bare parameters have to be per-
formed.

The numbers above hold for the quenched case,
where the quark ﬁelds are left out as dynamical de-
grees of freedom in the simulation.
If they are in-
cluded, the cost of the simulations becomes at least
a factor 100 more and a single physical result would
need about 40 years on your PC. Clearly, better com-
puters are needed such as the ones discussed in the
introduction.

Of course, relying only on progress in the develop-
ment of computers would be too risky and not enough.
Lattice ﬁeld theory has seen a number of conceptual
improvements [14, 15] in the last years that allowed
to accelerate the simulations themselves.
In addi-
tion, many improvements in the algorithms used were
found. Although each algorithmic improvement by it-
self was only a relatively small step [16], all in all a fac-
tor of 20 acceleration through algorithm improvement
alone could be obtained in the last 15 years. Still, the
development of machines were much faster in this pe-
riod. The situation is illustrated in ﬁg. 1 (taken from a
LATFOR paper). Here we show the speedup obtained
relative to the status in the year 1987. This year is

TUIT001-003

Figure 1: The performance gain of numerical
simulations in lattice ﬁeld theory in the last years
relative to the –normalized to one– situation in 1987.
Algorithm development (Algo) alone reached a gain of a
factor 20. However, the performance gain through
computer development appears to be orders of magnitude
higher. We show this development at the example of the
supercomputers exploited at the research center in J¨ulich
(HLR) and of the APE computers (APE). As a
comparison we also show other architectures (others) as
used worldwide for lattice ﬁeld theory computations.
(taken from LATFOR).

special in that at that time the ﬁrst exact algorithm
for simulations of dynamical fermions was developed
and used [17]. We see in the ﬁgure that since then the
speedup resulting from better computer techniques,
despite the impressive improvements by algorithmic
developments, is orders of magnitude larger than the
algorithm improvement. The ﬁgure also shows that
special hardware,
in this case the APE computer,
shows the same scaling law as commercial supercom-
puters, in this case the CRAY. Other computers and
their performance, relative to the status in 1987, used
throughout the world for lattice ﬁeld theory is also
shown.

2.2. Some selected physics results

The machine, algorithmic and conceptual develop-
ments in lattice ﬁeld theory allowed to compute a
number of important physical quantities in the last
years, despite the aforementioned very large compu-
tational requirements. An important aspect of the
developments is that we understand not only the sta-
tistical errors of the numerical calculations, but also
the systematic ones. For a number of quantities fully
non-perturbatively renormalized results in the contin-
uum limit could be obtained. The only restriction of
these calculations is that they are still done in the

4

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

quenched approximation. However, there is no partic-
ular reason why the calculation should not be doable
in a completely analogous way for the full theory. The
advent of the next generation of machines in the multi-
Tﬂops regime will then open the door to this exciting
perspective.

Let us just discuss two examples of physics results
to illustrate what we just said. One is the running
coupling [18] and the other are moments of parton
distribution functions in deep inelastic scattering. In
a quantum ﬁeld theory such as QCD, there is a steady
generation of virtual particles that shield (or anti-
shield) the charges of the elementary particles. This
leads to renormalization eﬀects. By changing the en-
ergy at which experiments are performed, the scale at
which we look at the, say, color charge is altered, and,
correspondingly the value of the charge itself depends
on this energy scale.

This scale dependence (the running) of the coupling
can be computed in lattice ﬁeld, starting from the
QCD Lagrangian alone, without any further assump-
tions. The trick is to use a suitable lattice renormal-
ization scheme, the so-called Schr¨odinger functional
(SF) scheme, that is deﬁned in a ﬁnite volume and
hence ideally suited for numerical simulations. By
going to very high energies, contact to perturbation
theory can safely be established and renormalization
group invariant quantities can be extracted.
In the
case of the running strong coupling this corresponds
to the Λ-parameter of QCD. The advantage of the
knowledge of renormalization group invariant quan-
tities is that they can be translated to any preferred
renormalization scheme. In this way it becomes possi-
ble to translate results for the running coupling from
lattice simulations to continuum results in the, say,
MS-scheme as it is conventionally used in perturba-
tion theory.

In ﬁg. 2 we show an example of such a calculation.
The results are already in the continuum. They cover
a broad energy range and are very precise (the error
bars of the simulation points are well below the size of
the symbols). In the plot, also a comparison to per-
turbation theory is shown and a good agreement down
to surprisingly small energy scales is found. Another
example is a moment of a parton distribution func-
tion as they can be extracted from global analyses of
experimental data. Such moments can be expressed
as expectation values of local operators and are hence
computable in lattice simulations. The renormaliza-
tion procedure of such moments follow the general
strategy of using the ﬁnite volume SF renormalization
scheme discussed above for the running coupling. We
show in ﬁg. 3 an example of the continuum limit of the
ﬁrst moment hxi of a twist-2, non-singlet operator in a
pion [19]. In the plot two diﬀerent lattice formulations
of QCD were used. It is reassuring that in the limit
that the lattice spacing is sent to zero both unphysical
lattice versions of QCD extrapolate to the same num-

TUIT001-003

Figure 2: The running strong coupling constant in the
continuum as function of the energy scale. The quenched
approximation is used.

ber. As a ﬁnal result for this study it is found that
the lattice gives a preliminary value of hxi = 0.30(3)
while the experimental number reads hxi = 0.23(3).
This lattice number has to be taken with care since
it is obtained in the quenched approximation. But,
again, there is no other reason than missing computer
power to repeat this calculation also for the full theory.
If in this case the results just mentioned were stable,
a really interesting situation would have emerged.

The results that we have just discussed were actu-
ally obtained on APE machines. There are, however,
a number of physics problems, where PC clusters were
used extensively. The simulations concern in particu-
lar the recently discovered chirally invariant formula-
tions of QCD on the lattice. Examples for results on
the PC clusters that are installed at DESY are given
in [20]. The next sections are devoted to a discussion
on PC cluster systems that are installed at DESY.

3. Commodity Clusters at DESY

3.1. Conceptional considerations

Suﬃcient computing power to perform Lattice QCD
(LQCD) calculations as described above can obviously
not be drawn from a single processor. The solution is
to parallelize the physical problem in order to concur-
rently deploy many CPUs. As a consequence massive

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

5

with good price/performance ratios. Interconnectiv-
ity is provided by exploiting modern network tech-
nologies. Such a cluster must:

• deliver suﬃcient CPU (FPU) performance and

memory throughput,

• provide good connectivity between CPUs (band-

width as well as latency),

• be scalable,

• be reliable,

ministration,

applications,

• incorporate tools for easy installation and ad-

• provide a usable software environment for the

• be connected to backup and archiving facilities,

• ﬁt boundary conditions such as space, cooling

and power supply capacities.

In clusters, a set of main building blocks can be iden-
tiﬁed:

• The computing nodes which actually provide
the computing power, optionally with local disk
space,

• a high-speed, low latency network for the

parallelized physics application,

• an auxiliary network to remotely control and

administer the nodes,

• a host system for login, compiling,

linking,

batch job submission, and central disk space,

Figure 3: Continuum limit of the second moment of a
twist-2 operator in a pion. Two versions of lattice QCD
are used, ordinary Wilson fermions (Wilson) and
O(a)-improved fermions (NP improved).

intercommunication between the CPUs is needed.
Typical High Performance Computing (HPC) or Su-
percomputing applications are characterized by high
demands on:

• CPU (especially Floating Point Unit (FPU))

performance,

• memory throughput,

• interconnectivity (bandwidth and latency) be-

• optionally, a slow control network, e.g. based

tween nodes.

on a ﬁeld bus.

One example for so-called supercomputers are Sym-
metric Multi-Processing (SMP) machines with up to
hundreds of processors. They appeared as single ma-
chines and are optimized for memory access and in-
terconnectivity between the CPUs.
In the LQCD area custom-made special purpose ma-
chines such as APE (Array Processor Experiment) [4]
or QCDOC (QCD on Chip) [5] have been developed.
In the last years PCs, exploiting processors with
competitive computing power, hit the commodity
market. Concurrently, modern network technologies
have achieved performances, which allow for high-
speed low latency interconnects between PCs. These
developments paved the way to build PC clusters [21].
Commodity clusters draw computing power from up
to hundreds or even thousands of in principle indepen-
dent PCs with one or two CPUs, called nodes. Those
clusters beneﬁt from their scalability and the possi-
bility to deploy components of the commodity market

A schematic view is shown in ﬁg. 4. The high speed
network can either be organized as a switched network
(e.g. the DESY clusters using Myrinet-switches) or
by a n-dimensional mesh to allow for nearest-neighbor
communication, see ﬁg. 5. In [22] the network is or-
ganized as a 2-dimensional GigaBit-Ethernet mesh.

In scientiﬁc computing Unix-like operating systems
have always played the dominant role. The develop-
ment of Linux along with the triumphal procession of
PCs into the scientiﬁc world made Linux-PCs the sys-
tems of choice in most universities, physics institutes,
and laboratories. In order to actually operate Linux-
PC clusters, further system aspects must be taken into
account:

• Installation and administration of the operat-

ing system Linux,

• security issues (login, open ports, private net-

work),

TUIT001-003

6

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

3.2.1. Hardware

At both DESY sites in Hamburg and Zeuthen com-
modity clusters are operated since January 2002 and
December 2001 respectively. See tab. I for the set-ups.

Table I DESY’s PC clusters.

Item
Nodes
CPUs/node
CPUs/1.7 GHZ 2 × 16
CPUs/2.0 GHZ 2 × 16

Hamburg Zeuthen
32
2

16
2
2 × 16

The cluster nodes as well as the host system are
equipped with high-end commodity components (see
tab. II).

Table II Cluster Hardware.

Item

Implementation

Chassis
Main-board
Processors
Chip-set
Memory
Disk

Chassis
Main-board
Processors
Chip-set
Memory
Disk
Uplink
Downlink

Computing Nodes

rack-mounted 4U module
SuperMicro P4DC6
2 Intel Pentium 4 Xeon 1.7/2.0 GHz
Intel i860
4 × 256 MB RDRAM
18 GB SCSI IBM IC35L018UWD210-0

Host System

rack-mounted 4U module
SuperMicro P4DC6
2 Intel Pentium 4 Xeon 1.7 GHz
Intel i860
Rambus 4 × 256 MB RDRAM
36 GB SCSI IBM DDYS-T36950N
Intel EtherExpress PRO 1000 F
Intel EtherExpress PRO 1000 T

High-speed Network

Interface cards Myrinet M3F-PCI64B-2
Chassis
Line cards
Mngmnt card Myrinet M3-M

Myrinet M3-E32 5 slot
Myrinet M3-SW16-8F

Auxiliary Network

Interface Card on-board Intel 82557 100Base T
Compu-Shack GIGALine 2024M
Switch
48-port 100Base T
Module 1000Base T

Uplink

Figure 4: Schematic view of a cluster.

Figure 5: Schematic view of a mesh cluster.

• user administration,

• application software installation,

• backup and archiving issues,

• monitoring and alarming.

3.2. Implementation

The recently improved PC architectures are well

suitable for Supercomputing by exploiting:

• Increasing CPU clock rates following Moore’s

Law now extending the 2 GHz border,

• larger caches at full processor speed,

• vector units (SSE, SSE2),

• cache pre-fetch,

• fast memory interfaces,

• PCI-Bus at 66 MHz/64-bit,

• high external bandwidth (Myrinet, GigaBit-
partly with very low latency

Ethernet),
(Myrinet).

Intel Pentium 4 Xeon processors [23], which be-
came available at the end of 2001, showed much en-
hanced performance compared to Pentium III Tuala-
tine CPUs due to features such as vector units, and

TUIT001-003

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

7

larger caches (see the section 4 on benchmarking re-
sults). The SuperMicro motherboard P4DC6 with the
Intel i860 chip-set was the only possible combination
until the end of 2002. It provides PCI-Bus support.
For the memory Rambus modules were chosen. The
communication between the nodes in the physics ap-
plication is done by means of Myrinet [24]. It provides
bandwidths up to 240 MB/s with very low latencies in
the order of a few µs. For administration purposes and
to actually submit jobs and copy data the nodes are
interconnected via Fast-Ethernet in a private subnet.
The host system is connected to the central switch
by a copper GigaBit-Ethernet link and has a separate
ﬁber GigaBit-Ethernet link to the outside. Each node
consists of a PC with two CPUs and is housed by a
4U rack-mounted chassis. Up to 9 nodes (18 CPUs)
are installed in a cabinet.

3.2.2. Software

The basic installation plus most of the software sup-
port was purchased with the hardware from the Ger-
man company MEGWARE [25].
For all software related aspects the host system is used
as a server for the nodes.
For the Linux operating system a S.u.S.E. 7.2 distri-
bution [26] was chosen, which contains the kernel ver-
sion 2.4.17 with SMP capabilities. Temperature and
fan sensors are read out by the kernel module lmsen-
sors [27].
The nodes are booted via DHCP, TFTP, and Intel’s
[30] Pre-boot Execution Environment (PXE).
The nodes are operated in a private network
(192.168.1.0) behind the server. For security reasons,
external user login is only possible to the host sys-
tem via ssh.
Individual login from the host system
to the nodes can be done over the auxiliary network
by means of rsh. Users are registered on the host sys-
tem which exports the home directories (/home) and a
data partition (/data) to the nodes. For the user ad-
ministration standard Unix tools are used (useradd).
The necessary ﬁles (/etc/groups, /etc/passwd,
/etc/shadow) are manually distributed to the nodes.
Source code is compiled and linked on the host sys-
tem. Software is mostly written in C/C++ but also
compilers for Fortran77/90 are required. On the host
system in addition to GNU compilers of the Portland
Group [28], KAI [29], and Intel [30] are available.
The parallelization of the computation is done within
the application by means of the Message Passing In-
terface (MPI). Since MPI is running over Myrinet,
a special library which uses low level Myricom com-
munications is installed (MPICH-GM [31]). For the
Myrinet network a static routing table is used.
Zeuthen uses the open source Portable Batch System
(OpenPBS) for job submission.
In Hamburg nodes
are manually distributed to users on good-will basis.
Time synchronization is done via XNTP. The nodes

TUIT001-003

synchronize with respect to the host system which gets
the correct time from DESY’s central server.

Backup and archiving are basically diﬀerent items:
Regular backups are done to provide security against
loss of system data and home directories. Under nor-
mal conditions backups will never be retrieved. For
the Hamburg cluster DESY’s standard backup en-
vironment based on IBM’s Tivoli Storage Manager
(TSM) is used. It automatically creates incremental
backups of the disk of the host system and a regular
basis. At DESY Zeuthen a copy of the host system’s
disk is stored on a second disk.
Archiving tools allow users to arbitrarily store and
retrieve large amounts of data. DESY uses dCache
which provides a simpliﬁed and uniﬁed tool to access
It provides a unique view
the tertiary storage [32].
into the storage repository, hiding the physical loca-
tion of the ﬁle data, cached or tape only. Dataset
staging and disk space management is performed in-
visibly to the data clients. Currently around 1 TB
of data are stored. The archiving system is mainly
used to store temporary check-points and ﬁnal results
of long time computing jobs, so-called conﬁgurations,
which can be used for further analyses.
The backup and archiving scheme is shown in ﬁg. 6.

Figure 6: The backup and archiving scheme.

LQCD calculations require the availability of all
nodes dedicated to the problem for the entire run-
time of the job. The failure of only one node spoils the
entire job. Therefore, stability of the cluster in terms
of availability and sustained performance is a crucial
credential of the clusters. Usually not all nodes are
used for one single job. Many calculations are done
on a limited number (2 − 8) nodes. This allows to
(manually) restart jobs using the check-points on dif-
ferent nodes in case of failures. In order to use the
resources of the clusters most eﬃciently a well-deﬁned
monitoring and alarming scheme is needed.
The company MEGWARE delivered with the software
installation a monitoring package called clustware. It
provides a snapshot of all relevant properties of the
cluster nodes in a graphical user interface, including
CPU usage, load, I/O, and temperature. A long term

8

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

history (> 1min) is not shown.
Alternatively, a DESY in-house development called
ClusterMonitor (CluMon) [33] is in use. Every node
runs a simple Perl-written daemon which periodically
dumps status information of relevant node proper-
ties such as uptime, load, CPU usage, memory us-
age, swap usage, and all temperatures into a node-
speciﬁc ﬁle on the host system. The host system runs
an Apache web server, which allows to remotely ac-
cess status information of the cluster with any web
browser. History of the quantities is kept by means
of the MRTG package [34] and is available from the
web page. In addition, CluMon provides alarming by
e-mail based on the time period since the last update
of the status ﬁle.

3.3. Operational experiences

The two clusters in Hamburg and Zeuthen are lo-
cated in the computer centers. The clusters are oper-
ated and administrated in cooperation by members of
the computer centers and the DESY theory and NIC
groups.
Of around twenty registered users per cluster just a
handful can be classiﬁed as Power Users, running reg-
ularly resource consuming jobs (see tab. III).

Table III Accounts and users.

Site
Hamburg 20
Zeuthen 18

Accounts Power Users Strategy

7
6

good-will basis
batch system

The two clusters in Hamburg and Zeuthen deploy
in total 52 dual-CPU PCs: 32 nodes (Hamburg), plus
16 nodes (Zeuthen), plus 2 spare nodes, plus 2 servers.
During the almost 17 month of operation quite a num-
ber of problems occurred, so usually not all nodes
have been available at all times. Tab. IV lists all ma-
jor hardware problems.

Taking into account that the clusters exploit com-
modity hardware components and oﬀer considerably
better price/performance ratios than big mainframe
SMP-machines, failures of certain components such as
disks and power supplies were expected. The stability
of the PC hardware after replacing the obviously sys-
tematically misbehaving IBM disks was reasonable.
Some annoyance was caused the repeating failures of
the Myrinet interface and line cards, which was also
seen at Fermilab [35] and traced back to broken op-
tical receivers. Nevertheless, the general opinion of
the users on the performance and the stability of the
clusters is very positive.

Table IV List of failures.
Component

Faults Total

PC

1
Motherboards
CPUs
0
Memory Modules 0
1
Power Supplies
35
Disks
0
Ethernet Chips
0
CPU Fans
0
Chassis Fans
Myrinet
1
0
4
6

Fibers
Slot Chassis
Line Cards
Interface Cards

Infrastructure

Cabinet Fans

3

52
104
208
52
52
52
104
52

48
2
10
48

24

3.4. Future developments

The considerations in section 2 require cluster sizes
of O(1000) nodes to approach the Tﬂops regime. Even
more, in order to actually deliver a few Tﬂops sus-
tained for hours, days or even weeks, all nodes would
need to run at the same time. As discussed earlier the
failure of just one node would spoil the entire calcu-
lation. Accepting the experiences so far, this seems
not be possible within the current concept3.
Recent tests have shown that GigaBit-Ethernet might
be an interesting alternative to Myrinet. Benchmark
showed bandwidths of 2 × 1 Gbit/s bidirectional with
special switches which would imply a much better
price/performance ratio than Myrinet. Since GigaBit-
Ethernet is widely used now, one might also expect
more stability and reliability compared to the niche
product Myrinet (see section 4).
Disks –even SCSI– are the most likely components to
break in PCs. Though the replacement of disks is easy,
the aﬀected node is down and needs to be re-installed
completely afterwards. This could be avoided by run-
ning the nodes disk-less. Such a concept would require
a stable and reliable server which could be achieved
by setting up one or more RAID-systems in a tree-like
architecture to distribute load. The server could also
provide the boot image. In another scenario booting
could be done from a local block device such as an
EPROM or a memory stick.
The current set-up relies on one single server which

31 broken node per week in a 50 node cluster is equivalent to
a maximal lifetime of a complete 1000 node cluster of 8 hours.

TUIT001-003

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

9

exports /home and /data directories to the nodes. It
also serves as a login host for the users and is used
for code development, compilation, linking, and job
submission. This machine is clearly a single point of
failure. In a bigger system one would opt for redun-
dancy in the server arrangement by distributing diﬀer-
ent functionalities to diﬀerent machines. In particular
a separate ﬁle server for the exported directories is
needed.
Space, power consumption, and cooling will become a
major issue when planning for thousands of nodes.
Recent developments of so-called blades place the
motherboards vertically to improve the air-ﬂow for
cooling in order to increase CPU densities.
Software installation, administration, and monitoring
of thousands of nodes is a challenge which requires
a very careful choice of appropriate tools. Remote
administration could be enabled by exploiting the se-
rial consoles of the PCs. They could be connected
to a dedicated terminal server or –in a much cheaper
scenario– subsequently from node to node.

4. Benchmarks

The hardware of commodity PCs has been ex-
tremely improved over the last years. The perfor-
mance increase inside the CPU is due to higher clock
rates and enhanced building blocks (e.g. SSE1/SSE2
instructions) following Moore’s Law which predicts
a performance doubling all 18 month. Moore’s Law
gives a technology estimation of mainly the CMOS
density or number of transistors which can be inte-
grated on a chip of a given size.
It does not work
well for the other interacting PC components like the
memory interface and external busses. On the other
hand also a big step forward in the development of fast
memory architectures like Rambus and DDR RAM
and a series of high bandwidth PCI bus based inter-
connects like Myrinet and QSNet is going on. There-
fore PC clusters are becoming more and more attrac-
tive for classical high performance parallel computing
and therefore also as a hardware basis for LQCD ap-
plications.

4.1. Benchmark systems

Apart from the DESY clusters described above,
the following systems were used in order to test the
ability of PC clusters for LQCD applications:
Mellanox: Blade dual Pentium 4 Xeon cluster
connected via Inﬁniband,
running MPICH for
VIA/Inﬁniband with patch from Ohio State Univer-
sity [37],
ParTec: Dual Pentium 4 Xeon cluster connected via
Myrinet running ParaStation MPI [36],

TUIT001-003

MEGWARE: Dual Pentium 4 Xeon cluster con-
nected via Myrinet
running MPICH-GM from
Myricom [31],
Leibniz-Rechenzentrum Munich:
(single CPU
tests) Pentium 4 and dual Xeon PCs with CPUs with
clock rates between 2.4 and 3.06 GHz,
University of Erlangen: GigaBit-Ethernet dual
Pentium 4 Xeon cluster.

4.2. Benchmarks and results

Representative benchmarks for the evaluation of
diﬀerent PC systems have been developed. Already
in the year 2000 a ﬁrst benchmark of M. L¨uscher
(CERN) has shown the potential in using the SSE1
and SSE2 instructions for the Wilson-Dirac opera-
tor [38]. This program takes heavily advantage of
the Pentium 4 memory-to-cache pre-fetch capabilities
and the SSE registers and instructions which are im-
plemented by using assembly in-line code, compatible
to the gcc and Intel compilers. Fig. 7 shows on the
left hand side the performance gain of the highly opti-
mized 32-bit and 64-bit Dirac-Operator kernel which
linearly follows the evolution of the CPU performance
expressed by their clock rate. The value of 1.5 Gﬂops
for the 32-bit implementation or 0.8 Gﬂops for the 64-
bit implementation respectively was unexpected high
for a PC in the year 2000 and encouraged groups work-
ing on LQCD algorithms on PC clusters also to use
the Pentium 4 capabilities to improve their algorithms
on PC clusters.
The Wilson-Dirac operator Benchmarks are accompa-
nied by two tests called add assign ﬁeld (similar to the
BLAS daxpy) and square norm which are represent-
ing the linear algebra part of the benchmark. Both
parts are strongly memory bound which means that
they cannot beneﬁt from the SSE-environment. This
results in a relative small improvement shown on the
right hand site of ﬁg. 7 which also gives an impression
of the slowly evolving memory interface architectures
since the introduction of the dual channel 800 MHz
Rambus.
Another version of such a single node benchmark
was developed by M. Hasenbusch (DESY) for the
even-odd preconditioned Wilson-Dirac operator [39].
Meanwhile (using recent FSB800 based PCs equipped
with a 3.06 MHz Pentium 4) a performance of about
2.6 Gﬂops for the 32-bit implementation and about
1.4 Gﬂops for the 64-bit implementation can be ob-
served.
To evaluate the behavior of PC cluster interconnects a
1-dimensional parallel even-odd preconditioned Dirac
Operator Benchmark on a 2 × 163 lattice (also writ-
ten by M. Hasenbusch) was used (see Appendix). The
aim of the parallel benchmark was to compare dif-
ferent parallel PC based architectures against each

10

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

other rather than achieving the best performance for a
given system. Fig. 8 shows the results on clusters with
diﬀerent numbers of nodes. In addition to the CPU
power and the memory interface the throughput of the
external PCI-Bus depending on the given chip-set and
the interconnecting interface itself are dominating the
entire performance. Both early Intel Pentium 4 Xeon
based clusters at DESY are using the i860 chip-set
which came with a relative poor 33 MHz/64-bit PCI-
Bus performance. A bus-read (send) of 227 MB/s and
a bus-write (recv) of 315 MB/s of maximal 528 MB/s
and expected 450 − 460 MB/s was measured. This
ends up in an external unidirectional bandwidth of
about 160 MB/s of maximal 240 MB/s. In the bidirec-
tional case we measured (90 + 90) MB/s. Meanwhile
more advanced chip-sets like the E7500 were available
for the Pentium 4 Xeon which are providing a PCI
throughput close to the expected numbers. The inﬂu-
ence of the chip-set is dominating the results shown
in ﬁg. 8, whereas in the left hand side on CPU per
node and an the right hand side two CPUs per node
communicating via shared memory MPI are used.
Inﬁniband is a new promising communication tech-
nology especially designed for cluster interconnections
ﬁg. 9. Beside the high throughput shown in ﬁg. 10
the latency at relative small buﬀer size in the order of
2 kB is signiﬁcantly higher then using Myrinet which
could be an advantage for applications using small lo-
In ﬁg. 11 the performance of a 4 node
cal lattices.
partition of the DESY Myrinet Pentium 4 Xeon clus-
ter is compared to the performance of a corresponding
Inﬁniband cluster from Mellanox using a 2-dim par-
allel Wilson-Dirac Operator Benchmark developed at
DESY Hamburg by M. L¨uscher. Due to some prob-
lems using the assembly in-lines within the 2.96 gcc
compiler coming with the RedHat Linux system on
the Mellanox cluster during the short time in which
the cluster was available for testing a code without
the SSE optimizations was used. The Inﬁniband clus-
ter performed approximately 1.8 times better in the
32-bit case and approximately 1.6 times better in the
64-bit case as the Myrinet cluster.
The 1-dimensional (non-optimized) parallel even-
odd preconditioned Dirac Operator Benchmark was
modiﬁed to test whether an improvement using
non-blocking MPI communication functions can be
achieved. No eﬀect was seen in using MPICH over
GM provided by Myricom. Tests on a 4 node cluster
which runs the MPI version of the ParaStation soft-
ware results in an performance increase of about 20%
(see tab. V).

Fig. 12 gives a summary of the communication be-
havior of the diﬀerent architectures resulting from the
parallel benchmarks. The eﬃciency number is the ra-
tio between the performance of the same benchmark
with and without communication. Included is also a
test on a 4 node GigaBit-Ethernet cluster connected

Table V Parastation3 non-blocking I/O support
(non-SSE).

MPI blocking I/O MPI non-blocking I/O
308 Mﬂops

367 Mﬂops

via a non-blocking GigaBit-Ethernet switch. The eﬃ-
ciency number for GigaBit-Ethernet in the rightmost
column of ﬁg. is 12 even better than one can achieve
using Myrinet on a system with a chip-set which pro-
vides a slow PCI-bus throughput. The positive inﬂu-
ence of non-blocking communication support (ParaS-
tation) and fast communication support (Inﬁniband)
is shown by the eﬃciency numbers.
Compared to the single node benchmarks the results
of the parallel benchmarks imply that that the capac-
ity of bandwidth is crucial for the eﬃcient use of PC
clusters as a scalable platform for LQCD applications.

MFLOPS

Dirac operator

Linear Algebra

1.4GHz

2.4GHz

2.53GHz

3.06GHz

3000

2500

2000

1500

1000

500

0

Wilson-Dirac-O perator 32-Bit

Wilson-Dirac-O perator 64-bit

add_ass_field 32-bit

add_ass_field 64-bit

square_norm 32-bit

square_norm 64-bit

Figure 7: Single node Wilson-Dirac operator and linear
algebra benchmark.

MFLOPS/CPU
500

450

400

350

300

250

200

150

100

50

0

Myrinet2000
i860:
90 MB/s

1.7 GHz, i860

2.0 GHz, i860

2.4 GHz, E7500

E7500:
190 MB/s

4 single
(4 CPUs)

8 single
(8 CPUs)

2 dual (4
CPUs)

4 dual (8
CPUs)

8 dual (16
CPUs)

16 single
(16
CPUs)

16 dual
(32
CPUs)

Figure 8: Parallel (1-dim) Wilson-Dirac Operator
Benchmark (SSE), even-odd preconditioned, 2 × 163
lattice, Xeon CPUs, single CPU performance.

TUIT001-003

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

11

Infiniband interconnect

up to 10GB/s
Bi-directional

CPU

CPU

Host Bus

Sys
Mem

Mem
Cntlr

Link:
High Speed Serial
1x, 4x, and 12x

Switch:
Simple, low cost, 
multistage network

CPU

CPU

s
u
B

 
t
s
o
H

Mem
Cntlr

Sys
Mem

HCA

Link
Link

Link
Link

A I/O
C
Cntlr
T

HCA

k
n
L

i

k
n
L

i

Switch

k
n
L

i

k
n
L

i

TCA

Target Channel Adapter:
Interface to I/O controller
SCSI, FC-AL, GbE, ...

I/O
Cntlr
Host Channel Adapter:
•Protocol Engine
•Moves data via messages 
queued in memory

ht t p:/ / www.inf inibandt a.org

Chips : IBM, Mellanox

PCI-X cards: Fujitsu, Mellanox,
JNI,  IBM

Figure 9: Inﬁniband interconnect.

Infiniband vs Myrinet performance  (MFLOPS):

XEON 1.7 GHz Myrinet,
i860 chipset

XEON 2.2 GHz Infiniband,
E7500 chipset

8x83 lattice,
2x2 processor grid

16x163 lattice,
2x4 processor grid

32-Bit

64-Bit

32-Bit

64-Bit

370

281

697

477

338

299

609

480

Figure 11: Inﬁniband and Myrinet performance
comparisons using a parallel (2-dim) Wilson-Dirac
Operator Benchmark on 4 node Pentium 4 Xeon clusters,
single CPU performance, without SSE optimization, the
local lattice size is 42x88 for the 8x83, and 8x4x162 for
the global 16x163 lattice.

Maximal Efficiency  of external I/O

MFLOPs
(without
communication)

MFLOPS
(with
communication)

Maximal
Bandwidth

Efficiency

Myrinet (i860),
SSE

Myrinet/GM
(E7500), SSE

Myrinet/
Parastation (E7500), SSE

Myrinet/
Parastation (E7500),
non-blocking, non-SSE

Gigabit,  Ethernet, 
non-SSE

Infiniband
non-SSE

579

631

675

406

390

370

307

432

446

368

228

297

90 + 90

0.53

190 + 190

0.68

181 + 181

0.66

hidden

0.91

100 + 100

0.58

210 + 210

0.80

be expected further in the case of the CPUs.
The performance of
the 1-dimensional and 2-
dimensional parallel implementations of the Dirac-
Wilson operator depends on the behavior of the ex-
is mainly dependent on the
ternal interconnects, i.e.
PCI-bus throughput coming given by the chip-set and
the interface card itself. Results coming from PC clus-
ters consisting of diﬀerent components have shown an
enhancement in both the quality of the chip-sets (e.g.
E7500)and the throughput of the communication in-
terfaces (e.g. Inﬁniband).
Non-blocking MPI communication can improve the
performance by using adequate MPI implementations
(e.g. ParaStation).

In summary, it might be envisaged, as done by e.g.
LATFOR, that heterogeneous computer landscapes
will be available to the user with centers that host
machines in the multi-Tﬂops regime, still enabled by
specialized machines, and many smaller installations
at universities as well as research centers in the few
hundred to 1 Tﬂops range realized by PC clusters.

Figure 10: Inﬁniband bandwidth compared to Myrinet
and QSNet (source Mellanox).

5. Conclusions

Figure 12: I/O Eﬃciency.

We have discussed the usage of commodity clusters
based on PCs for LQCD calculations. Such instal-
lations would not have been considered competitive a
few years ago. However, our experience with such kind
of machines at NIC and DESY adds further evidence
that for problems in QCD that require below, say, 1
Tﬂops computer power, PC clusters are a valuable
and cots-eﬀective tool for computing physics results
in LQCD. In Hamburg and Zeuthen clusters with 64
and 32 CPUs are successfully in operation for more
than one year.
Investigations using representative benchmarks on the
DESY clusters and also other architectures were car-
ried out with promising results.
Applying the SSE/SSE2 (SIMD+pre-fetch) instruc-
tions on Pentium 4 like CPUs, the single node perfor-
mance of the Wilson-Dirac operator is increasing ac-
cording to the clock rate improvements of those CPUs
used in commodity PCs.
The performance of memory bounded parts of the
LQCD algorithms, especially the linear algebra rou-
tines, depends strongly on the throughput of the mem-
ory interfaces. Those interfaces did not show the same
level of enhancements as it has been observed and will

TUIT001-003

12

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

(5)

(6)

(7)

Acknowledgments

The authors would like to thank Martin L¨uscher
(CERN) for the benchmark codes and the fruitful dis-
cussions about PCs for LQCD, and Isabel Campos
Plasencia (Leibnitz-Rechenzentrum Munich), Ger-
hard Wellein (Uni Erlangen), Holger M¨uller (MEG-
WARE), Norbert Eicker (ParTec), Chris Eddington
(Mellanox) for the opportunity to run the benchmarks
on their clusters.
The authors also wish to thank the computer centers
of DESY Hamburg and Zeuthen.

In our blocking implementation, the communication
of the data and the computation is performed in a
consecutive way. First the spinor-ﬁelds are exchanged
using the MPI Sendrecv function. This is followed by
the application of Heo or Hoe on the single nodes.

Both times required for communication tcomm and
calculation tcalc are measured separately. The eﬀec-
tive bandwidth is computed as:

Bandwidth =

96 × L3/2 Byte
tcomm

The performance per node without communication is
computed as:

Appendix: Discussion of the even-odd
benchmark

P0 =

t × L3/2 × 1392 ﬂop
tcalc

The benchmark program applies the even-odd pre-
conditioned Wilson-Dirac matrix that is deﬁned on a
L3 × T lattice to a spinor-ﬁeld. The program is imple-
mented in C plus some in-lined SSE2 extensions. For
parallelization we have used the MPI message passing
library. The code is derived from Martin L¨uscher’s
benchmark code presented at the lattice conference
2001 [38]. Even with communication switched oﬀ, the
present code performs worse then the one of Martin
L¨uscher (579 Mﬂops vs. 880 Mﬂops on 1.7 GHz Pen-
tium 4). The reason is twofold:

• Less variables that reside in the cache can be

reused than in the standard case.

• We have skipped the cache-optimized order of
the lattice-points to simplify the parallelization.

Strategy of the parallelization

The Wilson-Dirac matrix is a sparse matrix. The
hopping part of the matrix only connects nearest
neighbor sites on the lattice. Therefore, for paral-
lelization, it is natural to divide the lattice in sub-
blocks of size t × lx × ly × lz. Each of the MPI-
processes takes one such sub-block. For simplicity we
have done the parallelization only in one direction:
lx = ly = lz = L and tnp = T , where np is the num-
ber of processes.

The hopping part of the Wilson-Dirac matrix con-
nects nearest neighbor sites on the lattice. Therefore
each application of Heo or Hoe (Hoe connects even
with odd sites and Heo vice versa.) the spinor-ﬁelds
at the right boundary of the left neighbor and the
spinor-ﬁelds at the left boundary of the right neighbor
of each of the processes has to be sent and received.

In the case of even-odd pre-conditioning the spinor-
ﬁeld only resides on the even (or odd) sites. Therefore
L3/2 spinors have to be sent and received. A single
data package has the size

24 × 8 × L3/2 Byte = 96 × L3 Byte

(4)

Correspondingly, the performance including the com-
munication of the data is given by:

P =

t × L3/2 × 1392 ﬂop
tcomm + tcalc

In the case of the non-blocking communication, we
had to divide Hoe (or Heo) into a part that only acts
on spinors that reside on the local lattice and a part
that acts on spinors that reside on the neighbors:

• Initialize send and receive (MPI Isend and

MPI Irecv),

• perform the calculation for the local part of Hoe,

• Wait

for

the

communication

to

ﬁnish

(MPI Wait),

• do the rest of Hoe.

References

[1] http://www.lqcd.org/.
[2] http://www.lrz-muenchen.de/services/compute/hlrb/.
[3] http://www.ibm.com/redbooks/.
[4] R. Alﬁeri et al. (apeNEXT-collaboration),

hep-lat/0102011;
R. Ammendola et al. (apeNEXT-collaboration),
hep-lat/0211031.

[5] D. Chen et al., hep-lat/0011004,

P.A. Boyle et al., hep-lat/0110124,
P.A. Boyle et al., hep-lat/0210034.

[6] M. Hasenbusch, K. Jansen, T. Lippert, H. St¨uben,

P. Wegner, T. Wettig, and H. Wittig,
Evaluating Supercomputer platforms for lattice
QCD applications, in preparation.
[7] http://www.osti.gov/scidac/henp/index.htm.
[8] http://www-zeuthen.desy.de/latfor/.
[9] K. Jansen, “Lattice Gauge Theory and High Per-
formance Computing: The LATFOR initiative in
Germany”, Talk given at CHEP03.

TUIT001-003

Computing in High Energy and Nuclear Physics (CHEP03), La Jolla, Ca, USA, March 24-28, 2003

13

[10] A. Gellrich, “A Commodity Cluster for Lattice
QCD Calculations at DESY”, Talk given at
CHEP03.

R. Petronzio, A. Shindler,
hep-lat/0303012;
K. Jansen, hep-lat/0010038.

and

I. Wetzorke,

[11] P. Wegner, “LQCD Benchmarks on Cluster Ar-

[20] M. Hasenbusch, Phys.Lett. B519 (2001) 177;

chitectures”, Talk given at CHEP03.

[12] H.J. Rothe, Lattice Gauge Theories, World Scien-

tiﬁc Lecture Notes in Physics, Vol. 43,
(World Scientiﬁc, Singapure, 1992);
Montvay and G. M¨unster, Quantum Fields on a
Lattice, Cambridge Univ. Press, 1994;
J. Smit, Introduction to Quantum Fields on a Lat-
tice, Cambridge Lecture Notes in Physics,
Cambridge Univ. Press, 2002.

[13] Y. Saad, Iterative Methods for Sparse Linear Sys-
tems, PWS Publishing Company, Boston, 1996.
[14] G. Rossi, Nucl.Phys.Proc.Suppl. 53 (1997) 3;

R. Sommer, Schladming Lectures 1997,
hep-ph/9711243;
M. L¨uscher, Lectures given at Les Houches Sum-
mer School, 1997, hep-lat/9802029.

[15] P.H. Ginsparg and K.G. Wilson, Phys. Rev. D25

(1982) 2649;
D.B. Kaplan, Phys.Lett. B288 (1992) 342;
P. Hasenfratz, Nucl.Phys. B (Proc.Suppl.) 63A-
C (1998) 53;
P. Hasenfratz, V. Laliena, and F. Niedermayer,
hep-lat/9801021;
M. L¨uscher, Phys.Lett. B428 (1998) 342.

[16] K. Jansen, Nucl.Phys.Proc.Suppl. 53 (1997) 127;
M. Peardon, Nucl.Phys.Proc.Suppl. 106 (2002) 3,
hep-lat/0201003.

[17] S. Duane, A.D. Kennedy, B.J. Pendleton, and

D. Roweth, Phys.Lett. B195 (1987) 216.
[18] R. Sommer and H. Wittig, physics/0204015.
[19] M. Guagnelli,

K. Jansen,

F. Palombi,

M. Hasenbusch and K. Jansen,
hep-lat/0210036, hep-lat/0211042;
K. Jansen and C. Urbach, in preparation;
L. Giusti,
Ch. Hoelbling, M. L¨uscher,
H. Wittig, hep-lat/0212012.

and

[21] G.F. Pﬁster, In Search of Clusters,

Prentice Hall PTR, 2nd Edition, 1998.

[22] Z. Fodor, S.D. Katz, and G. Papp,

hep-lat/0202030.
[23] http://www.intel.com/.
[24] http://www.myricom.com/.
[25] http://www.megware.com/.
[26] http://www.suse.de/.
[27] http://secure.netroedge.com/ lm78/.
[28] http://www.pgroup.com/.
[29] http://www.kai.com/.
[30] http://www.intel.com/software/products/.
[31] http://www.myri.com/.
[32] http://dcache.desy.de/.
[33] http://www.desy.de/∼gellrich/clumon/.
[34] http://people.ee.ethz.ch/∼oetiker/webtools/mrtg/.
[35] D. Holmgren, private communications, CHEP03.
[36] http://www.par-tec.com/.
[37] http://www.mellanox.com/.
[38] M. L¨uscher, hep-lat/0110007,

Nucl.Phys.Proc.Suppl. 106 (2002) 21.

[39] M. Hasenbusch,

http://www.theorie.physik.uni-
wuppertal.de/Cluster2002/Talks/hasenbusch.ps.

TUIT001-003


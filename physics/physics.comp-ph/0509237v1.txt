Exact numerical simulation of power-law noises

Dipartimento di Fisica, Universit`a di Udine and I.N.F.N. – Sezione di Trieste

Via delle Scienze, 208 – I-33100 Udine, Italy

Edoardo Milotti∗

(Dated: February 2, 2008)

Abstract

Many simulations of stochastic processes require colored noises: I describe here an exact numer-

ical method to simulate power-law noises: the method can be extended to more general colored

noises, and is exact for all time steps, even when they are unevenly spaced (as may often happen

for astronomical data, see e.g. N. R. Lomb, Astrophys. Space Sci. 39, 447 (1976)). The algorithm

has a well-behaved computational complexity, it produces a nearly perfect Gaussian noise, and its

computational eﬃciency depends on the required degree of noise Gaussianity.

PACS numbers: 02.50.Ey,05.40.Ca,02.70.Uu

5
0
0
2
 
p
e
S
 
8
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
7
3
2
9
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗Electronic address: milotti@ts.infn.it

1

In recent years colored noise sources have been considered in many disparate applications,

that range from stochastic resonance [1], to biophysics [2, 3] and beam dynamics in particle

accelerators [4, 5]. The analytical approach to some of these processes is often diﬃcult,

and sometimes impossible, and numerical experiments are commonly used to support the

analytical conclusions, or as an aid to discover new results. For this reason, algorithms

that produce colored noise have acquired an ever increasing importance. This widespread

interest spans diﬀerent scientiﬁc communities, and the existing algorithms reﬂect the vari-

ety of approaches to the understanding of stochastic processes in diﬀerent contexts. There

are physics-inspired algorithms that rely mostly on equations of the Langevin type, FFT-

based and autocorrelation function methods that use the spectral or correlation properties

of colored noise, and time-series methods that produce colored noise from diﬀerent ﬁltering

approaches. The review paper by Kasdin [6] provides a long list of references until 1995,

centered mostly on linear processes and FFT methods. More recently, Greenhall wrote a

review paper on FFT-based methods [7], and reference [8] is another very clear paper on the

same topic. I describe here an exact numerical simulation of power-law noises that can be

extended to more general colored noises, and which is based on the classical argument pro-

posed long ago by Bernamont to model 1/f α noise as a superposition of Ornstein-Uhlenbeck

processes [9]. The synthesis of colored noise from a point process is clearly not new, because

this kind of modeling dates as far back as 1909, to the work of Campbell [10] (see also the

famous paper by Rice [13]); more recently Teich, Lowen and collaborators have carried out

extensive studies on point processes with long-tail pulse response functions [14, 15], and

others have studied the synthesis of power-law spectra from nonlinear processes (see, e.g.,

[16] for a model based on a multiplicative point process). The simulation methods described

in [6, 7] assume evenly distributed sampling steps, and the extension to uneven sampling is

not trivial: however noneven sampling has many important applications (see, e.g. the classic

papers by Lomb and Scargle [17, 18] on period analysis for irregularly sampled astronom-

ical data, and two more recent references [19, 20]), and Gillespie discussed a method valid

for the Ornstein-Uhlenbeck process and based on the Langevin equation in 1996 [21]. The

algorithm proposed here is very general (it is not limited to the OU process), it is very easy

to implement, it is valid for all time steps, it has a well-behaved computational complexity,

and produces a nearly perfect Gaussian noise, and its computational eﬃciency depends on

the required degree of noise Gaussianity.

2

Here we take a signal x(t) that originates from the linear superposition of many random

pulses, i.e., pulses that are random in time and can be described by a memoryless process

with a Poisson distribution, have random amplitude A drawn from a distribution with ﬁnite

variance and probability density gA(A), and such that their pulse response function is

h(t, λ) =

exp(

λt) if t

0

−
0

≥
if t < 0

x(t) =

Akh(t

tk, λk)

−






Xk

with a decay rate which is drawn from a distribution with probability density gλ(λ), so that

where tk is the time at which the k-th pulse occurs, Ak is its amplitude and λk is the decay

rate of its pulse response function.

If n is the pulse rate, then on average there are n [gA(A)dA] [gλ(λ)dλ] dt pulses in the
time interval (t′, t′ + dt) and in the amplitude-λ range dAdλ; the number of pulses follows

a Poisson distribution and therefore the variance of the number of detected pulses is also

equal to n [gA(A)dA] [gλ(λ)dλ] dt. This means that the mean and the variance of the output

signal at time t are given by the integrals

t

−∞

Z

t

−∞

Z

λmax

Amax

gλ(λ)dλ

gA(A)dA

=

x
i

h

λmin

Z

Amin

Z

dt′n [Ah(t

t′, λ)]

−

(∆x)2

=

i

h

λmax

Amax

gλ(λ)dλ

gA(A)dA

λmin

Z

Amin

Z

dt′n [Ah(t

2
t′, λ)]

−

If we assume that the amplitude A is ﬁxed, we take the pulse response function (1), and

rearrange the time integration, the integrals (3) and (4) simplify to

= nA

x
i

h

gλ(λ)dλ

dt [h(t, λ)] = nA

dλ = nA

λmax

gλ(λ)
λ

λmin

Z

1
λ

(cid:28)

(cid:29)

and

and

(∆x)2

= nA2

h

i

gλ(λ)dλ

2
dt [h(t, λ)]

= nA2

λmax

gλ(λ)
2λ

dλ =

λmin

Z

nA2
2

1
λ

(cid:28)

(cid:29)

Now let H(ω, λ) be the Fourier transform of h(t, λ), then from the causality constraint on

h(t, λ) and Parseval’s theorem we ﬁnd that the variance (6) can be trasformed into

(∆x)2

=

i

h

nA2
2π

λmax

λmin

Z

gλ(λ)dλ

dω

H(ω, λ)

∞

−∞

Z

|

2 =
|

nA2
2π

∞

λmax

dω

−∞

Z

λmin

Z

gλ(λ)dλ

H(ω, λ)
|

λmax

λmin

Z

λmax

λmin

Z

0
Z

∞

∞

0
Z

3

(1)

(2)

(3)

(4)

(5)

(6)

2
|
(7)

The right-hand expression in equation (7) shows that the spectral density is

S(ω) =

nA2
2π

λmax

λmin

Z

gλ(λ)dλ

H(ω, λ)
|

2
|

H(ω, λ)

= (ω2 + λ2)−1 for the exponential pulse response function (1), we obtain

and since

|
eventually

2
|

λmin
We consider now three special, important cases: if there is just a single decay rate λ, the

Z

S(ω) =

nA2
2π

λmax

gλ(λ)
ω2 + λ2 dλ

spectral density has the usual Lorentzian shape

which, for ω

λ has a 1/f 2 behavior, so that we can approximate a 1/f 2 spectrum in an

≫

actual process by choosing a λ smaller than the lowest observed frequency. With a careful

choice of the distribution gλ(λ) we can synthesize many diﬀerent spectra, but there are two

special choices for gλ(λ): we can take a uniform distribution or a range-limited power-law

distribution. If we assume a uniform distribution of decay rates, between λmin and λmax,

i.e.

then the average

1/λ

that determines the mean level (5), and the variance (6) is

h

i

S(ω) =

nA2
2π

1
ω2 + λ2

gλ(λ) =

1
λmax −

λmin

1
λ

(cid:28)

(cid:29)

=

ln(λmax/λmin)
λmin
λmax −

and using equation 9, the spectral density is easily shown to be

S(ω) =

nA2
2π(λmax −
ω

λmin)

1
ω

(cid:18)

arctan

arctan

λmax

ω −

λmin
ω

(cid:19)

and in the range λmin ≪
we take a range-limited power-law distribution

≪

λmax this spectral density has a 1/f behavior. Similarly, if

then the average

1/λ

is

h

i

gλ(λ) =

1
λ1−β

max

 

−

−

β
λ1−β
min !

λ−β

1

β

1
λ

(cid:28)

(cid:29)

=

−

(cid:18)

λ−β
max −
λ1−β
max
−

−β
λ
min
λ1−β

min

(cid:19)

−
β

4

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

and the spectral density is

S(ω) =

max

(λ1−β
−
λ1−β
minF

1
λ1−β
min)ω2
β
1

−
2

(cid:20)

, 1;

λ1−β
maxF

1

β

−
2

1

(cid:18)

; −

β

−
2
λ2
min
ω2

−
which has a 1/f 1+β behavior in the range λmin ≪

(cid:18)

ω

≪

(cid:19)(cid:21)
λmax.

, 1;

1

β

−
2

; −

λ2
max
ω2

(cid:19)

(16)

Now we follow the lead provided by these considerations, and we take, e.g., the case where

there is just a single decay rate λ, so that the spectral density has the Lorentzian shape (10).

Since the probability density of the time intervals ∆tk between Poisson events is well-known

to be dP (∆t) = n exp(

n∆t)d∆t, we can generate a sequence of ∆t’s from an exponential

−

distribution, and we can thus generate the sequence

(with tk+1 = tk + ∆tk) required to

tk}

{

evaluate a realization of x(t) as in equation (2): ﬁgure 1 shows an example where the single

decays are clearly visible. Figure 1 also shows that, although the process has the desired

spectral density, it is quite obviously non-Gaussian and therefore this generation method

seems to be of marginal utility, as most of the actual physical processes are Gaussian and

Gaussianity is usually a required property of a good noise generator (see, e.g., the recent

paper [11] that describes a hardware-based Gaussian white noise simulator and contains a

list of relevant references; notice also that Gaussianity is sometimes a weakness rather than

a strength, see, e.g.

[12]). The Gaussianity in shot noise processes has been studied at

length since the paper by Rice [13] and here we strictly limit the discussion to the special

processes considered in this paper. The single exponential spikes in ﬁgure 1 stand out more

clearly when the average rate n of the Poisson process is smaller than the decay rate λ; by

contrast, when n

λ, at any time there are many pulses of comparable size and the sum

≫

has a nearly Gaussian behavior. We can gain further insight in this generation method by

using the mean moment generating function (mmgf) for a Poisson process with average rate

a:

exp [it(k

h

k

)]
i

i

− h

=

∞

(k

k

)m

i

− h

m=0h
X
i2
= 1 +
2!
(10a2 + a)t5 +

i
i3
3!

at2 +

+

i5
5!

(it)m
m!

= exp

(eit

1)a

ita

−

−

(cid:3)

(cid:2)
(3a2 + a)t4

i4
at3 +
4!
i6
(15a3 + 5a2 + a)t6 + O(t7)
6!

(17)

(18)

Now we use the mmgf to compute the higher-order moments: as already discussed in the

derivation of equations (3) and (4), the process x(t) is the sum of Poisson variates with

5

diﬀerent amplitudes; on the other hand the mmgf of the weighted sum αk + βj of two

independent Poisson variates k and j, both with rate a, is

exp

h

it [(αk + βj)
{

− h

αk + βj

]
}i

i

exp [itβ(j

j

]
i

i

− h

(19)

=

k

− h

]
ih

i
(αt)2 + (βt)2

exp [itα(k
i2
2!
(αt)3 + (βt)3

a

(cid:2)

h
= 1 +
i3
3!

+

(cid:2)

(cid:3)

(cid:3)
+ O(t4)

(20)

Using the mmgf’s given above we could proceed as in standard texts on probability theory,

and show that for large a the process approaches an exact Gaussian distribution (the usual

proof of the Central Limit Theorem), but the purpose here is giving a quantitative estimate of

the deviation from Gaussianity: from the expansion (20), we see that, just like the variance,

the third moment about the mean of the weighted sum of independent Poisson variates is

the weighted sum of the third moments of the individual variates (this is not true for the

fourth and the higher moments), and therefore we can write a simple expression for the third

moment about the mean

(21)

(22)

(23)

(∆x)3

= nA3

h

i

λmax

λmin

Z

gλ(λ)dλ

dt [h(t, λ)]3

∞

0
Z

and we can use this expression to compute the skewness of the frequency distribution

skewness = h

(∆x)3
(∆x)2

i

h

3/2 =
i

λmax
λmin

√n

R

λmax
λmin

nR

∞

gλ(λ)dλ

0 dt [h(t, λ)]3
0 dt [h(t, λ)]2
R
gλ(λ)dλ
R

∞

o

3/2

With the pulse response function (1) the integrals are easily evaluated, so that

skewness =

λmax
λmin

[gλ(λ)/(3λ)] dλ

√n

R

λmax
λmin

[gλ(λ)/(2λ] dλ

3/2 =

23/2
3

1

1/λ

n
h

i

o

p

nR

From equations (22) and (23), we see that the skewness is small when n
h

i

1/λ

is large (as it

should be for a Gaussian distribution) and that the actual amount of skewness depends on

the adimensional product n
h

1/λ

, as expected; as a rule of thumb one might take n
h

i

1/λ

> 10

i

for good Gaussianity.

The previous considerations apply to the noise process x(t) without any reference to

sampling, however the simulation of noisy physical systems usually implies evaluating the

noise process at evenly spaced sampling times, so we take now a sequence of sampling times
sj}

. At each sampling time only the recent pulses

with average sampling interval

∆s

{

h

i

6

actually contribute, while the older pulses quickly fade away, for instance the average total

contribution of pulses that are older than Ndecay/λ is

δx
i

h

= nA

t−Ndecay/λ

dt′h(t

t′, λ)

−

gλ(λ)dλ

Z

−∞
∞

λmax

λmin

Z

λmax

λmin

Z

1
λ

(cid:28)

(cid:29)

= nA

gλ(λ)dλ

h(t′′)dt′′ = nA

e−Ndecay

(24)

Ndecay/λ

Z

δx
i

h

/

x
i

h

which is just a small fraction of the mean value:

= e−Ndecay , and for this reason as we

proceed forward in time, we can just forget the older transitions. In an actual implementation

we ﬁx Ndecay, but because of random event clustering we cannot know a priori how many

transitions times must actually be kept in memory: for this reason the Poisson-distributed

transition times should not be stored in an array, but in a linked list [22]; the linked list

must also store the decay rates that correspond to each transition event. At each sampling

step the list is updated ﬁrst by generating as many transition times (and the associated

decay rates, which are drawn from a given decay rate distribution) as needed to reach (and

i

1/λ

tk > Ndecay/λ (see ﬁgure 2). The mean list length is just

, and the processing time is proportional to the number of list elements. At

possibly surpass) the actual sampling time sj, and then by discarding those events with an
occurrence time tk such that sj −
nNdecayh
startup the list is empty, and the ﬁrst Ndecay/λminh
and afterwards discarded as the algorithm ﬁlls the list up to the average level, and thus,
for a desired number of samples Ns, we must generate a total of Ns + Ndecay/(λminh
i
samples. The time-complexity of the algorithm is thus proportional to the sum of the total

samples must be used for initialization

∆s

∆s

i

)

number of generated transitions plus the total number of operations used for the list scans,

i.e.,

complexity =

Ns +

Ndecay
∆s

i
h
Ns +

C1n
h

1
λmin (cid:19) (cid:18)
Ndecay
1
λmin (cid:19) (cid:18)
∆s

∆s

+ C2nNdecay

i

C1 + C2

Ndecay
∆s

1
λ
(cid:28)
1
λ

(cid:18)
= n
h

∆s

i

(cid:18)

(cid:29)(cid:19)

i
The algorithm described above is easily implemented; ﬁgures 3 to 8 show the results
obtained in a simulation of Ns = 218 = 262144 transitions, with a single decay rate λ = 0.001,

h

h

i (cid:28)

(cid:29)(cid:19)

(25)

and a Poisson transition rate n = 1 (here and in all the following discussions the system

of units is arbitrary); moreover Ndecay = 20, so that the average relative error due to the
10−9. With

past transitions that have been discarded is

h
these parameters we expect an average list length nNdecay/λ = 20000, and a corresponding

Ndecay)

= exp(

δx
i

x
i

−

≈

2

/

h

·

7

ﬁlling time Ndecay/(λ∆s) = 20000: ﬁgure 3 shows the list length, which behaves exactly

as expected. Figures 4 and 5 show the normalized signal amplitude (x(t)

)/σ, where

x
i

− h

σ is the standard deviation of the amplitude, i.e., the square root of the variance (6): at

the beginning the linked list which contains the process memory is empty, and the signal

is very far oﬀ the predicted average, but as the list ﬁlls up to level, the signal quickly

reaches the predicted average. Figure 6 is the histogram of the normalized signal amplitude

obtained from 262144 samples, after the list ﬁll-up; the continuous curve superimposed on the

histogram is a Gaussian with the mean and standard deviation estimated from the samples,

and we see that there is no visible skewness, because in this simulation run λ/n = 0.001,

which corresponds to a very low skewness (23), but there are multiple peaks, which are
due to the nonstationarity of a true 1/f 2 process (which is well approximated here), and

which require an extremely long observation time to establish the Gaussianity of the process

[23]. Finally ﬁgures 7 and 8 show the Discrete Fourier Transform (DFT) spectrum of the

normalized signal amplitude, which reproduces quite well the expected shape (10).

The next set of ﬁgures shows the results of a simulation with a decay rates that are

uniformly distributed between λmin = 0.0001 and λmax = 1. Just as before, the simulation
contains Ns = 218 = 262144 samples, with a transition rate n = 10, a sampling interval

∆s = 1 and Ndecay = 20: from these parameters we obtain the average
1/λ
that the ﬁll-up time is Ndecay/λmin = 200000, the ﬁll-up length is n(Ndecayh
and the number of samples required for the initial ﬁll-up is (Ndecay/λmin)/∆s = 200000.

9.211, so

i ≈
)

1842.,

1/λ

≈

h

i

Figure 9 shows the linked list length, which in this case does not reach the average ﬁlling

level with a linear growth law, but with a smoothed curve. Figure 10 shows the initial part of

the simulated signal, and ﬁgure 11 shows the histogram of the normalized signal amplitude:

in contrast to the histogram in ﬁgure 6, now the amplitude distribution is slightly skewed,

because in this simulation run 1/(n
h

i

≈

1/λ

)

0.109, much higher than the calculated skewness

for ﬁgure 6. Finally the averaged DFT spectrum is shown in ﬁgure 12: the spectrum mimics

quite well the behavior of a true 1/f spectrum over more than three frequency decades.

The 13 shows the average spectrum obtained in a long simulation run with a diﬀerent
power-law noise: Ns = 220 = 1048576 samples have been generated with A = 1 and a range-

limited power-law distribution of decay rates (14) with β = 0.2, in the range λmin = 0.0001,

λmax = 1. Here too the spectral resolution ∆ω
0.00038 is larger than the minimum decay
rate λmin, and the noise samples reproduce the behavior of a true 1/f 1.2 spectrum (dashed

≈

8

line), over more than three frequency decades.

This generator can be used to test a standard hypothesis that is commonly used with

FFT-based colored noise generators, in analogy to the well-known behavior of white noise,

namely that the standard deviation of the real and imaginary parts of the discrete Fourier

components Fk of a colored noise process is proportional to the square root of the noise

spectrum Sk [8]. Figure 14 shows the ratio var[

(Fk)]/Sk for a simulated 1/f noise: the

ℜ

average ratio is constant and thus the simulation does not disprove the standard assumption,

at least for this particular noise process.

In all the examples described above the sampling interval ∆s is ﬁxed, but the method is

in no way limited to constant sampling intervals. And indeed this is probably the greatest

strength of this noise generator, its ability to work also with uneven sampling intervals: this

is not true for the other common generators [6].

To conclude, in this paper I have described a generator of colored noise that is exact, is

not limited to evenly distributed samples, has a well-behaved complexity O(Ns) (in contrast

to many other generators that have a O(Ns log Ns) complexity), and is not troubled by

hidden periodicity issues, like the FFT-based generators.

[1] L. Gammaitoni, P. H¨anggi, P. Jung, and F. Marchesoni, Rev. Mod. Phys. 70, 223 (1998).

[2] B. Lindner, Phys. Rev. E 69, 022901 (2004).

[3] G. Wenning, T. Hoch, and K. Obermayer, Phys. Rev. E 71, 021902 (2005).

[4] C. L. Bohn and I. V. Sideris, Phys. Rev. Lett. 91, 264801 (2003).

[5] I. V. Sideris and C. L. Bohn, Phys. Rev. STAB 7, 104202 (2004).

[6] N. J. Kasdin, Proc. IEEE 83, 802 (1995).

[7] C. A. Greenhall: FFT-based methods for simulating ﬂicker FM, 34th Annual Precise Time

and Time Interval (PTTI) Meeting, pp. 481-491, Reston, Virginia, 3-5 December, 2002.

[8] J. Timmer and M. K¨onig, Astron. Astrophys. 300, 707 (1995).

[9] J. Bernamont, Ann. Phys. (Leipzig) 7, 7 (1937).

[10] N. Campbell, Proc. Cambr. Phil. Soc. 15, 117 (1909).

[11] D.-U. Lee et al., IEEE Trans. on Computers 53, 1523 (2004).

[12] V. Y. Kontorovich and V. Z. Lyandres, IEEE Trans. on Signal Processing 43, 2372 (1995).

9

[13] S. O. Rice, Bell Syst. Tech. J. 23, 282 (1944).

[14] S. B. Lowen and M. C. Teich, IEEE Trans. on Information Theory 36, 1302 (1990).

[15] S. Thurner et al., Fractals 5, 565 (1997).

[16] B. Kaulakys, V. Gontis, and M. Alaburda, Phys. Rev. E 71, 051105 (2005).

[17] N. R. Lomb, Astrophys. Space Sci. 39, 447 (1976).

[18] J. D. Scargle, Astrophys. J. 263, 835 (1982).

[19] D. Heslop and M. J. Dekkers, Phys. of the Earth and Planetary Interiors 130, 103 (2002).

[20] Q. Tao, Y.-y. Wang, and W.-q. Wang, IEEE Signal Proc. Lett. 11, 764 (2004).

[21] D. T. Gillespie, Phys. Rev. E 54, 2084 (1996).

[22] nowadays linked lists are a standard tool of computer science, and they are incorporated in

many computer languages, but it is fair to mention that they arose as an important byproduct

of the research of A. Newell, H. A. Simon and C. Shaw in the ﬁeld of artiﬁcial intelligence,

see, e.g., A. Newell and H. A. Simon, IRE Trans. Inf. Theory IT-2, 61 (1956).

[23] W. Feller, An Introduction to Probability Theory and Its Applications, 3rd ed., pp. 342-371,

(Wiley, New York, 1970).

10

FIG. 1: A realization of the random process x(t) (eq. 2) with A = 1, n = 1, and with ﬁxed decay

rate λ = 0.5 (all quantities are given in arbitrary units; λ is given in inverse time units). The single

exponential decays are clearly visible, and the random process is obviously non-Gaussian.

11

FIG. 2:

Structure and update dynamics of the linked list that holds the Poisson-distributed

transition events: a.

structure of the list at the j-th sampling time sj; each node contains a

variable that points to the next node (the end of the list is marked by a null pointer), and stores

the time tk of the transition and the decay rate λk of the associated pulse response function. The

list contains only nodes such that sj −
k exp[
from the sum

λk(sj −

−

tk ≤

Ndecay/λk. The response of the system is computed

tk)], where the index k ranges over all the list elements such that

tk ≥

0 (the list head is usually excluded). b. if the (j + 1)-th sampling time is greater than

sj −
the time stored in the list head (as is usually the case), the program generates as many transition

P

times as needed to reach (and possibly overcome) the (j + 1)-th sampling time (light-gray boxes

in the ﬁgure, primed quantities), and next it scans the list to discard all the nodes such that

sj+1

tk > Ndecay/λk (dark-gray boxes). At this point the program computes the new response

−

and steps to the next sampling step.

12

FIG. 3: Length of the linked list in a simulation with A = 1 and a single decay rate λ = 0.001:

the linked list is initially empty, at it ﬁlls up at a constant rate. In this case n = 1, Ndecay = 20

and ∆s = 1 and therefore the ﬁll-up time is (Ndecay/λ) = 20000 the ﬁll-up length is n(Ndecay/λ) =

20000, and the number of samples required for the initial ﬁll-up is (Ndecay/λ)/∆s = 20000. After

the initial ﬁll-up the length of the linked list ﬂuctuates about the average ﬁlling level.

FIG. 4: Plot of the normalized signal amplitude (x(t)

)/σ (σ is the standard deviation of
x
i

− h

the amplitude, i.e., the square root of the variance (6) ) in the simulation run described in ﬁgure

3 and in the text. At the beginning the linked list which contains the process memory is empty,

and the signal is very far oﬀ the predicted average; as the list ﬁlls up to level, the signal quickly

reaches the predicted average.

13

FIG. 5: Detail of the normalized signal amplitude shown in the ﬁgure 4, just after the list has

ﬁlled up to the average level. This signal displays the large characteristic upward an downward

swings that are well-known in the theory of random walks [23].

FIG. 6: The histogram shows the amplitude distribution of 262144 samples from the realization of

the random process x(t) shown in ﬁgure 4, after the list ﬁll-up. The continuous curve is a Gaussian

with the mean and standard deviation estimated from the samples. There is no visible skewness,

because in this simulation run λ/n = 0.001, which corresponds to a very low skewness (23), but

there are multiple peaks, which are due to the nonstationarity of a true 1/f 2 process (which is

well approximated here), and which require an extremely long observation time to establish the

Gaussianity of the process [23].

14

FIG. 7: DFT spectrum obtained from 262144 samples from the realization of the normalized

signal amplitude (x(t)

)/σ shown in ﬁgure 4. The continuous curve shows the theoretical
x
i

− h

power spectral density (10). Because of sampling without low-pass ﬁltering there is some aliasing

and the DFT spectrum shows a slight upward bend at high frequency. Since the sampling interval

is ∆s = 1, the Nyquist (angular) frequency is just ωN yquist = π (here and in the following spectra

time is measured in arbitrary units as in the previous ﬁgures, and frequency units are deﬁned

accordingly). The arrow marks the position of the single decay rate in this simulation λ = 0.001.

15

FIG. 8: Averaged DFT spectrum obtained from the same 262144 samples as the spectrum in

ﬁgure 7, split in 32 blocks of 8192 samples each. The continuous curve shows the theoretical power

spectral density (10), and now it includes also the ﬁrst-order correction to aliasing. Because of the

low-frequency correlation between the blocks (that have been obtained from the same simulation

record), the average spectrum is a bit higher than expected and the theoretical prediction has been

globally shifted 20% higher to ﬁt the average spectrum; this artifact is absent in the analysis of

the whole record (the low-frequency plateau of the spectrum in ﬁgure 7 ﬁts the theoretical curve

exactly as expected). As in ﬁgure 7, the arrow marks the position of the single decay rate in this

simulation λ = 0.001: because of the shorter record length used for DFT analysis, the frequency

resolution is poorer here, and the spectrum mimics quite well the behavior of a true 1/f 2 spectrum,

over about three frequency decades.

16

FIG. 9: Length of the linked list in a simulation with A = 1 and a uniform distribution of decay

rates in the range λmin = 0.0001, λmax = 1: the linked list is initially empty, at it ﬁlls up with

a variable rate that depends on the distribution of decay rates. In this case n = 10, Ndecay = 20

and ∆s = 1 and

9.211, and therefore the ﬁll-up time is Ndecay/λmin = 200000 the ﬁll-

up length is n(Ndecayh
(Ndecay/λmin)/∆s = 200000. After the initial ﬁll-up the length of the linked list ﬂuctuates about

1842., and the number of samples required for the initial ﬁll-up is

)
i

≈

1/λ
h

i ≈
1/λ

the average ﬁlling level.

FIG. 10: Detail of the normalized signal amplitude in the simulation of ﬁgure 9, just after the list

has ﬁlled up to the average level.

17

FIG. 11: The histogram shows the amplitude distribution of 262144 samples from the realization

of the random process x(t) shown in ﬁgure 10, after the list ﬁll-up. The continuous curve is a

Gaussian with the mean and standard deviation estimated from the samples. In contrast to the

histogram in ﬁgure 6, now the amplitude distribution appears slightly skewed, because in this

simulation run 1/(n

0.0109, noticeably higher than the corresponding value for ﬁgure 6.

1/λ
h

)
i

≈

18

FIG. 12: Averaged DFT spectrum obtained from the 262144 samples in the simulation of ﬁgure

9, split in 32 blocks of 8192 samples each. The continuous curve shows the theoretical power

spectral density (13), which includes also the ﬁrst-order correction to aliasing. The arrows mark

the positions of the extreme decay rates λmin = 0.0001 and λmax = 1. The spectral resolution

∆ω

0.0015 is larger than the minimum decay rate λmin, and the spectrum mimics quite well the

≈

behavior of a true 1/f spectrum (dashed line), over more than three frequency decades.

19

FIG. 13: Averaged DFT spectrum obtained from 220 = 1048576 samples with A = 1 and a
range-limited power-law distribution of decay rates λ−β, with β = 0.2, in the range λmin = 0.0001,

λmax = 1, split in 32 blocks of 32768 samples each. The continuous curve shows the theoretical

power spectral density (16), which includes also the ﬁrst-order correction to aliasing. The arrows

mark the positions of the extreme decay rates λmin = 0.0001 and λmax = 1. The spectral resolution

∆ω

0.00038 is larger than the minimum decay rate λmin, and the spectrum mimics quite well

≈

the behavior of a true 1/f 1.2 spectrum (dashed line), over more than three frequency decades.

20

FIG. 14: Ratio var[

(Fk)]/Sk obtained from 220 = 1048576 samples with A = 1 and a uniform

ℜ

distribution of decay rates, in the range λmin = 0.0001, λmax = 1, averaged over the DFT results

obtained from 128 blocks of 8192 samples each. The average ratio ﬂuctuates about constant level,

and this is in line with the usual hypothesis that var[

(Fk)]

Sk (see, e.g., [8])).

ℜ

∝

21


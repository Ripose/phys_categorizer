Computing in High Energy and Nuclear Physics, La Jolla, California, USA, March 24-28 2003

1

A New Implementation of the Region-of-Interest Strategy for the ATLAS Second
Level Trigger

3
0
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
1
0
1
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

S. Armstrong, V. Boisvert ∗, S. Brandt, M. Elsing
European Laboratory for Particle Physics, CERN, Geneva, Switzerland

J.T. Baines, W. Li
Rutherford Appleton Laboratory, Chilton, Didcot, UK

S. George
Department of Physics, RHBNC, University of London, Egham, UK

A.G. Mello
Universidade Federal do Rio de Janeiro, COPPE/EE/IF, Rio de Janeiro, Brazil

On behalf of the Atlas High Level Trigger Group [1]

Among the many challenges presented by the future ATLAS detector at the LHC are the high
data taking rate and volume and the derivation of a rapid trigger decision with limited resources. To
address this challenge within the ATLAS second level trigger system, a Region-of-Interest mechanism
has been adopted which dramatically reduces the relevant ﬁducial volume necessary to be readout
and processed to small regions guided by the hardware-based ﬁrst level trigger. Software has been
developed to allow fast translation between arbitrary geometric regions and identiﬁers of small
collections of the event data. This facilitates on-demand data retrieval and collection building.
The system is optimized to minimize the amount of data transferred and unnecessary building of
complex objects. Details of the design and implementation are presented along with preliminary
performance results.

I.

INTRODUCTION

ATLAS (A Toroidal LHC ApparatuS) is one of the
four detectors currently being built around the LHC
(Large Hadron Collider) at the European Organiza-
tion for Nuclear Research (CERN) in Geneva, Switzer-
land. The LHC will collide protons at a centre-of-mass
energy of 14TeV. ATLAS is a multipurpose detector
designed to have a 4π hermeticity around the interac-
tion region. The physics goals of ATLAS are numer-
ous: from Higgs searches, to Top physics, from SUSY
searches to QCD physics, not forgetting B physics as
well as exotic searches. In addition, part of the ex-
citement for reaching this energy frontier is the dis-
covery potential associated with this unchartered ter-
ritory. These physics requirements combined with a
bunch crossing rate of 40 MHz in addition to an aver-
age of 25 inelastic proton-proton underlying interac-
tions in each bunch crossing (at a design luminosity
of 1034cm−2s−1) put very stringent constraints on the
data acquisition and trigger system.

The overall architecture of the three-level ATLAS
trigger system is shown in ﬁgure 1 [2]. It is designed
to reduce the nominal 40 MHz bunch crossing rate
to a rate of about 200 Hz at which events, that will

∗Presenter at the conference

TUGT008

have a size of about 1.6 MB on average, will be writ-
ten to mass storage. The ﬁrst stage of the trigger,
LVL1, is hardware-based and it reduces the rate to
about 75KHz. Using the fast calorimeter and muon
sub-detectors, it has a latency (time taken to form
and distribute the LVL1 trigger decision) of about
2.5 µs. During that time, the data from all the sub-
detectors (about 108 electronic channels) are kept in
pipeline memories. After the LVL1 decision, selected
data fragments are transferred to the Readout Drivers
(RODs) and then to the Readout Buﬀers (ROBs). It
is foreseen that there will be a total of about 1700
ROBs. The second stage of the trigger system, LVL2,
is software-based and it reduces the rate to about
2KHz. Making use of the so called Region-of-Interest
mechanism the average latency is about 10ms. In or-
der to achieve this goal, the main characteristic of
this stage is a fast rejection achieved by optimized
trigger algorithms. The last stage of the trigger sys-
tem, the Event Filter, occurs after the event building
process. At this stage, the average latency is about
1s. The goal of the Event Filter is both to reduce
the rate to about 200Hz, necessary for mass storage,
but also to classify the events. Hence full calibration
and alignment information is available at this stage.
The trigger algorithms used for this stage have much
in common with the oﬄine algorithms and massive
reuse of those is foreseen. The LVL2 and the Event
Filter stages are commonly referred to as the High

2

Computing in High Energy and Nuclear Physics, La Jolla, California, USA, March 24-28 2003

Level Trigger (HLT).

A. The Region-of-Interest Mechanism

An important piece of the strategy of the ATLAS
trigger relies on the Region-of-Interest (RoI) mecha-
nism for which the LVL2 trigger makes use of infor-
mation provided by the LVL1 trigger in localized re-
gion of the calorimeter and muon sub-detectors. This
process is shown schematically in ﬁgure 2. The in-
formation contained in the RoI typically include the
position (η and φ) and the pT of the candidate objects
as well as energy sums. Candidate objects selected by
the LVL1 can be high-pT muons, electrons or photons,
hadrons or taus, and jets. The energy sums include
the missing-ET vector and the scalar ET value. For all
selected LVL1 events, the RoI information is sent to
the LVL2 using a dedicated data path. Making use of
this RoI information, the LVL2 algorithms only trans-
fer the necessary ROBs in order to arrive quickly at
a LVL2 decision. It is important to note that all the
data from all the sub-detectors with full granularity is
available for the LVL2 algorithms if necessary. How-
ever, typically only a small fraction of the detector,
centred around the RoI information selected by the
LVL1, is needed by the LVL2 algorithms. On average
there are a few RoI per event and as a consequence to
this mechanism only a few percent of the total event
data is required at the LVL2 stage. On the other hand,
even though the bandwidth requirements on the Data
Acquisition system is much reduced, it makes for a
more complex system.

B. The Data Access

As discussed above, the Region-of-Interest mecha-
nism allow for an optimized retrieve of the data neces-
sary to perform the LVL2 decision. Figure 3 shows in
more details the sequence associated with the data ac-
cess. The data is stored in collections within the Tran-
sient Data Store (TDS) [3]. These collections are or-
ganized inside a ContainerWithInfrastructure for easy
retrieval and sorting. The collections are identiﬁed
uniquely via an oﬄine identiﬁer [4]. We can see from
the diagram that the HLT algorithm ﬁrst ask the Re-
gionSelector tool for the list of collection identiﬁers
associated with a particular region, that could corre-
spond for example to an ηφ region that would come
from LVL1. With the list of collection identiﬁers in
hand, the HLT algorithm request the associated data
to the TDS. If the data is already cached within the
TDS, the requested collections are returned.
If the
data is not cached, the TDS launches the ByteStream-
Converter which goal is to ﬁll the collections in the
TDS using the data in ByteStream format. To get
a hold of this data, the ByteStreamConverter must

request speciﬁc ROBs to the ROBDataCollector. Fi-
nally, at LVL2 there is the possibility to do some data
preparation from within the ByteStreamConverter,
which leads to a faster execution of the LVL2 trig-
ger algorithms. One consequence from this sequence
is the fact that the RegionSelector tool plays a central
role within the trigger chain, since every Trigger Algo-
rithm that needs access to the data in a certain region
will have access to this tool. Another remark that
stems from this data access sequence is the need for
an optimization of the collection granularity. There
needs to be a trade oﬀ between a useful navigation
for the trigger algorithms and a minimization of the
data requests. Finally, we mentioned that the col-
lection request was made using the oﬄine identiﬁers,
rather than the online identiﬁers. This choice comes
primarily from two issues, one is the fact that the
current design for the trigger architecture calls for the
use of oﬄine code in the online environment. That is
to say, both the trigger algorithms and the architec-
ture in which they run can be developed in an oﬄine
environment and be directly ported to the online en-
vironment. The beneﬁts are numerous: this will fa-
cilitate the development of algorithms; this will allow
the study of the boundary between LVL2 and Event
Filter and it will lead to easy performance studies for
physics analysis. The second issue related with the
use of oﬄine identiﬁers for the collection and the Re-
gionSelector comes from the fact that a possible region
for which a trigger algorithm could require data is the
InnerDetector sub-detectors, for which there are no
LVL1 online identiﬁers.

II. THE REGION SELECTOR TOOL

Having described the usefulness and environment
surrounding the RegionSelector tool we now turn to
its requirements and implementation.

A. Requirements

As we have just seen, the RegionSelector tool is
central to any LVL2 trigger algorithm, hence its fore-
most requirement is that it should be fast and use up
only a fraction of the available latency at LVL2. An-
other requirement imposed on the RegionSelector is
the fact that it should translate an arbitrary geomet-
rical region into a list of collection identiﬁers. Such
a region can be a simple cone that span the various
sub-detectors.
It can also be a more complex cone
which accounts for the uncertainty in the z position
of the primary vertex, coming from the beam spread.
In that case, ∆η has a radial dependence. Finally,
another geometrical region of interest is that of a he-
lical road, which could correspond for example to a
reconstructed track in need of conﬁrmation by a more

TUGT008

Computing in High Energy and Nuclear Physics, La Jolla, California, USA, March 24-28 2003

3

CALO MUON TRACKING

Interaction rate
~1 GHz
Bunch crossing
rate 40 MHz

LEVEL 1
TRIGGER

< 75 (100) kHz

Regions of Interest

LEVEL 2
TRIGGER

~ 2 kHz

EVENT FILTER

~ 200 Hz

Pipeline
memories

Derandomizers

Readout drivers
(RODs)

Readout buffers
(ROBs)

Full-event buffers
and
processor sub-farms

Event builder

Data recording

FIG. 1: Block diagram of the Trigger/DAQ system.

reﬁned Event Filter algorithm which has access to cal-
ibration and alignment constants. In the current im-
plementation of the RegionSelector tool the innermost
sub-detectors take into account the z direction spread
while the outermost sub-detectors follow a cone.

B.

Implementation

In ﬁgure 4 we show the sequence diagram associated
with the RegionSelector tool. We can distinguish two
main parts, the top one represents the initialization
phase, and the lower part shows the execution phase.
At initialization two maps are ﬁlled for each layer of
each sub-detector: a map between the φ index and a
set of identiﬁers, and a second map between an iden-
tiﬁer and a vector of a range in η. Figure 5 shows
schematically the concept of a set of identiﬁers cor-
responding to a value in φ and a range in η. During
the execution phase, the algorithm asks for the list of
identiﬁers corresponding to a range in φ and a range in
η, for a speciﬁed sub-detector. There is an outer loop
over the layers, then there is a loop over the φ range
to get the associated sets of identiﬁers. Following this

step there is also a loop over the identiﬁers to check
if the η value is within the required range. The built
identiﬁer list is then returned to the algorithm. This
procedure ensures that there are no duplicate identi-
ﬁer within the list. Care must be taken regarding the
φ compact boundary.

A key ingredient to the RegionSelector tool are
mappings made by the sub-detector communities that
tie together a range in η and φ and a collection iden-
tiﬁer. Those mappings are then used to make the in-
ternal maps during the initialization period of the Re-
gionSelector. As mentioned in section I B, there needs
to be an optimization performed for each sub-detector
concerning the granularity of the collections. In the
current implementation table I shows the current col-
lection granularity for each sub-detector, as well as the
total number of ROBs for this sub-detector. For the
Pixel sub-detector a collection corresponds to a mod-
ule, which is a single silicon wafer. For the silicon strip
detector (SCT) the collection granularity corresponds
to a side of a module which is a bonded pair of wafers
whose strips are oriented in the same direction, either
axial or stereo. For the Transition Radiation Tracker
(TRT) sub-detector, the collection granularity corre-

TUGT008

4

Computing in High Energy and Nuclear Physics, La Jolla, California, USA, March 24-28 2003

TABLE II: Timing measurements of RegionSelector in ms.

∆η
0.1 0.2 0.5

∆φ

Pixel (σ = 0.06)
0.20 0.22 0.23
SCT (σ = 0.11)
0.56 0.59 0.62
TRT (σ = 0.23)
1.05 1.12 1.21
0.1 LAr (σ = 0.06)
0.33 0.33 0.35
Tile (σ = 0.008)
0.03 0.03 0.03
MDT (σ = 0.038) 0.06 0.06 0.06
RPC (σ = 0.009) 0.05 0.05 0.06
0.22 0.22 0.23
Pixel
0.60 0.61 0.63
SCT
1.13 1.15 1.23
TRT
0.33 0.34 0.35
0.5 LAr
0.03 0.03 0.03
Tile
0.06 0.06 0.07
MDT
0.05 0.05 0.06
RPC

FIG. 2: Block diagram of the Region-of-Interest mecha-
nism.

TABLE I: Current collection granularity and number of
ROBs for each sub-detector.

Collection Number Num. ROBs

Pixel
SCT
TRT
LAr
Tile
muon MDT
muon CSC
muon RPC
muon TGC

module
side of module
straw layer
Trigger Tower
module
chamber
chamber
chamber
chamber

1744
8176
19008
7168
256
1168
32
574
1584

81
256
256
768
32
192
32
32
32

sponds to a radial straw layer in the barrel and to
1/32 in rφ in the endcap wheels. For the Liquid Argon
calorimeter the collection granularity is that of a Trig-
ger Tower while for the barrel hadronic Tile calorime-
ter the collection granularity is a phi wedge. Finally
for the muon spectrometer there are four technologies
used. Two types of chambers are used for trigger pur-
poses: the Resistive Plate Chambers (RPC) and the
Thin Gap Chambers (TGC); and two types of cham-
bers are used for precision measurements: the Moni-
tored Drift Tube chambers (MDT) and the Cathode
Strip Chambers (CSC). For all of the muon spectrom-
eter the collection granularity corresponds to a single
chamber. For more information on each sub-detector
see for example [5].

III. TIMING MEASUREMENTS

As mentioned earlier, the main requirement for the
RegionSelector tool is that it should only use up a
small fraction of the available latency at LVL2. Pre-
liminary timing measurements were performed on a
1GHz Pentium III machine using the TAU (Tuning
and Analysis Utilities) [6] timing tool. Table II shows
the timing measurements for various sub-detectors for
diﬀerent ranges in η and φ.

We can see from the results that the timing is
mainly independent on the extent of the η and φ
range. If one looks at the LAr calorimer timing and
arbitrarily divide by 3 to account for a 4GHz machine,
one gets a timing of the Region Selector of about
0.11ms. Since preliminary timing measurements of
LVL2 calorimeter algorithms are of the order of 1ms
on a 1GHz machine, we get a combined RegionSelector
and algorithm timing of less than 0.5ms, extrapolated
to a 4GHz machine. This number is well below the ex-
pected 10ms average latency allowed at LVL2. Note
that the timing for the data access is not included in
this number.

IV. CONCLUSION

We have shown an implementation of a tool used
to translate a region into a list of collection identi-
ﬁers. This tool is used by all algorithms that request
data access in a given region of the ATLAS detector.
Although this tool is of crucial importance at LVL2,
specially when combined with the Region-of-Interest
mechanism, it can also be of used in the Event Filter
and in oﬄine reconstruction whenever an algorithm

TUGT008

Computing in High Energy and Nuclear Physics, La Jolla, California, USA, March 24-28 2003

5

HLT 
Algorithm

Region 
Selector

TransientDataStore

ContainerWith
Infrastructure

ByteStream 
Converter

ROB 
Data 
Collector

Data 
Preparation

get(Region): 
list<identifier>

get(key): 
Container

get(list<HashId>): 
Collections

get(key): 
Collections

get(key1): 
Collections

get(ROBID): 
list<ROBData>

get(ROBData): 
Collections

FIG. 3: Schematic sequence diagram of the data access.

:Region 
Selector

:Detector 
Description

create(phiMap)

:phiMap

create(etaMap)

loop<Detectors>

:etaMap

getpos(Detectors): 
pos(r, eta, phi)

loop<Detectors>

getpos(Detectors): 
pos(r, eta, phi)

HLT 
Algorithm

create(GeoRegion)

:GeoRegion

get(GeoRegion):list<HashId>

loop<layers>

get(r): 
extent(phi,eta)

get(phi): 
list<HashId>

loop<phi>

get(HashId): 
list<eta>

loop<HashId>

Aggregate<HashId>: 
<HashId>

FIG. 4: Sequence diagram of the Region Selector.

is interested in a particular seed, be it a simple sub-
detector region or an object spanning a sub-detector
region. Already the timing budget of this tool is
within the accepted latency, scaled to a representative
machine speed for 2007. It is foreseen that continuous

improvements will be achieved in the implementation
and that more complex geometrical regions will be
supported.

TUGT008

6

Computing in High Energy and Nuclear Physics, La Jolla, California, USA, March 24-28 2003

List of Identifiers

Acknowledgments

φ1 

φ

∆η

η

IdentifierHash

FIG. 5: Diagram showing schematically the η φ map.

The authors wish to thank the ATLAS PESA core
software group. The authors also wish to thank the
various sub-detector communities for their assistance
in providing the necessary mappings, central to the
implementation of the RegionSelector tool: K. Assam-
agan, S. Goldfarb, G. Gorﬁne, F. Luehring, H. Ma, S.
Sivoklokov, and S. Solodkov.

Level

[1] The

ATLAS

High
http://atlas.web.cern.ch/Atlas/GROUPS/
DAQTRIG/HLT/AUTHORLISTS/chep2003.pdf [7]
[2] ATLAS HLT/DAQ/DCS Groups, ATLAS High-
Level Triggers, DAQ and DCS: Technical Proposal,
CERN/LHCC/2000-17, March 2000

Trigger

group

[3] P. Calaﬁura, StoreGate: a Data Model Toolkit for
the Atlas Software Architecture, Proceedings CHEP03,
MOJT008

[4] S. Goldfarb and A. Schaﬀer (editors), Deﬁnition of Of-
ﬂine Readout Identiﬁers for the ATLAS detector, AT-
LAS Internal Note ATLAS-SOFT-2001-004

[5] ATLAS
physics
CERN/LHCC/99-14, May 1999

and
ATLAS
collaboration,
performance Technical Design Report,

detector

[6] http://www.cs.uoregon.edu/research/paracomp/tau/
[7] S. Armstrong, J.T. Baines, C.P. Bee, M. Biglietti,
A. Bogaerts, V. Boisvert, M. Bosman, S. Brandt, B.

Caron, P. Casado, G. Cataldi, D. Cavalli, M. Cervetto,
G. Comune, A. Corso-Radu, A. Di Mattia, M. Diaz
Gomez, A. dos Anjos, J. Drohan, N. Ellis, M. Elsing,
B. Epp, F. Etienne, S. Falciano, A. Farilla S. George,
V. Ghete, S. Gonz´alez, M. Grothe, A. Kaczmarska, K.
Karr, A. Khomich, N. Konstantinidis, W. Krasny, W.
Li, A. Lowe, L. Luminari, H. Ma, C. Meessen, A.G.
Mello, G. Merino, P. Morettini, E. Moyse, A. Nairz,
A. Negri, N. Nikitin, A. Nisati, C. Padilla, F. Parodi,
V. Perez-Reale, J.L. Pinfold, P. Pinto, G. Polesello,
Z. Qian, S. Rajagopalan, S. Resconi, S. Rosati, D.A.
Scannicchio, C. Schiavi, T. Schoerner-Sadenius, E. Se-
gura, T. Shears, S. Sivoklokov, M. Smizanska, R. Soluk,
C. Stanescu, S. Tapprogge, F. Touchard, V. Vercesi,
A. Watson, T. Wengler, P. Werner, S. Wheeler, F.J.
Wickens, W. Wiedenmann, M. Wielers, H. Zobernig

TUGT008


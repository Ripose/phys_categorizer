3
0
0
2
 
n
u
J
 
1
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
2
1
0
6
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

CHEP03, La Jolla, California, March 24-28, 2003

1

New vertex reconstruction algorithms for CMS

R. Fr ¨uhwirth, W. Waltenberger∗
Institut f ¨ur Hochenergiephysik der ¨OAW, Vienna, Austria
K. Prokoﬁev, T. Speer
Physik-Institut der Universit ¨at Z ¨urich, Switzerland
P. Vanlaer†
Interuniversity Institute for High Energies, ULB, Belgium
E. Chabanat, N. Estre
Institut de Physique Nucl ´eaire de Lyon, Villeurbanne, France

The reconstruction of interaction vertices can be decomposed into a pattern recognition problem (“vertex
ﬁnding”) and a statistical problem (“vertex ﬁtting”). We brieﬂy review classical methods. We introduce novel
approaches and motivate them in the framework of high-luminosity experiments like at the LHC. We then show
comparisons with the classical methods in relevant physics channels.

1. INTRODUCTION

Vertex reconstruction algorithms face new chal-
lenges in high-luminosity scenarios such as the LHC
experiments. Vertex ﬁnding algorithms have to be
able to disentangle the tracks of vertices in diﬃcult
topologies, such as from decay vertices which are very
close to the primary vertex or decay chains with very
small separations between the vertices. Vertex ﬁtters
will need to be robustiﬁed, since outliers and non-
Gaussian tails in the distributions of the errors of the
track parameter will occur frequently.

We pursue extensive studies of vertex reconstruc-
tion algorithms that are capable of dealing with am-
biguities and track mis-reconstructions. Section 2
discusses robustiﬁcations of vertex ﬁtting algorithms.
Section 3 presents novel approaches to the vertex ﬁnd-
ing problem, derived from the clustering literature.

2. VERTEX FITTING

Robustiﬁed vertex ﬁtting has already been dis-
cussed in [2]; we shall only brieﬂy review this topic
here.

The classical methods in this ﬁeld are least-square
methods. The breakdown point of LS estimators is
zero, which means that even a single outlier track can
bias the resulting ﬁt signiﬁcantly. For noisy environ-
ments such as the LHC experiments robustiﬁcations of
the classic LS methods were investigated. We suggest
three new methods:

∗Supported by the Fonds zur F¨orderung der wissenschaftlichen
Forschung, Project 15177. Corresponding author.
†Supported by the Belgian Federal Oﬃce for Scientiﬁc, Techni-
cal and Cultural aﬀairs through the Interuniversity Attraction
Pole P5/27.

TULT013

• Adaptive method:

Instead of minimizing the
sum of residuals we minimize a weighted sum
of squared residuals:

n

ˆ~xAdaptive = argmin

~x

Xi=1 (cid:0)

wi · r2

i (~x)
(cid:1)

Outliers are not discarded but downweighted ac-
cording to the weight function

wi =

1
1 + e(r2
i −r2

c )β

(1)

Here rc denotes a cutoﬀ parameter, while β ≡
1/(2T ) introduces a temperature that is reduced
in each iteration step in a well-deﬁned annealing
schedule. An iterative weighted LS procedure is
used to ﬁnd this minimum.

• Trimming method: We minimize only a user-
deﬁned fraction of the sum of the squared resid-
uals:

ˆ~xTrimming = argmin

r2
i (~x)

h<n

~x

Xi=1

A fast method that ﬁnds this minimum is de-
scribed in [3].

• LMS: We minimize the median of squared resid-

uals:

ˆ~xLMS = argmin

med

~x

r2
i (~x)
(cid:1)
(cid:0)

Only a simpliﬁed algorithm has so far been
found that is compatible with our CPU con-
straints. This algorithm works separately on
each coordinate of the points of closest approach
of the tracks with respect to a vertex candidate.
This ignores the spatial structure of the data.
A full 3d method that works within our CPU
requirements is still searched for.

2

CHEP03, La Jolla, California, March 24-28, 2003

The conclusions that we draw are as follows [4]:

• the adaptive method should be considered a
good default method; it deals with a great many
diﬀerent situations in an optimal or nearly op-
timal way.
It leaves clean vertices almost un-
aﬀected, while at same time it is a very robust
algorithm.

• The trimming vertex ﬁtter may be interesting if
the number of outliers is known in advance. In
any other situation it is inferior to the adaptive
method.

• The LS ﬁt is the fastest 3D ﬁt, and optimal for

extremely pure data.

• The coordinate-wise LMS ﬁt is the fastest
method but it is very unprecise (a few hundred
microns compared to a few tens for primary ver-
tices). It can nevertheless be used to provide a
ﬁrst guess of the vertex position.

3. VERTEX FINDING

We categorise the set of vertex ﬁnding algorithms
into hierarchic and non-hierarchic methods. Hierar-
chic methods are algorithms whose workings can be
visualised with a dendrogram. Hierarchic methods
can further be split into divisive and agglomerative
methods.

Divisive methods start with one cluster that con-
tains all tracks; after each iteration a certain subset
of tracks is split oﬀ from the cluster into its own clus-
ter, which may in turn itself be split into sub-clusters.
All algorithms stop until a certain formal criterion is
met.

Agglomerative methods start assigning a singleton
cluster to every single track. The most compatible
clusters are then merged in every iteration step. Again
the procedure is stopped when a formal condition is
met. The most decisive factor in these methods is the
metric that is employed to compute the compatibility
between two clusters.

Let α and β denote two clusters. Let further s be
the set of all minimum distances between track pairs
with one track in cluster α and the other in cluster β.
We can now choose as the metric e.g.:

d(α, β) = min(s), max(s), ¯s, median(s), . . .

(2)

The choice d(α, β) = min(s) implements a single
linkage or minimum spanning tree procedure, whereas
d(α, β) = max(s) is often referred to as a complete
linkage.

The following theorem signiﬁcantly reduces the

number of reasonable choices:

A

C

d

V1

B

V2

Figure 1: Schematic description of how the triangle
inequality is violated in the track clustering problem.

Theorem: The triangle inequality does not gener-
ally hold for the minimum distances between a set of
n tracks.

Proof: Let A, B, C denote three tracks. Let A and
B share one common vertex V1; let further B and C
also share one common vertex V2. Then:

AB = ǫ, BC = ǫ, AC = d ≫ ǫ
→ AB + BC ≪ AC q.e.d.

(3)

This means that the choice d(α, β) = min(s) would
cluster A, B and C into a single vertex. We can
therefore safely discard single linkage from the list of
promising algorithms.

Until now the best results were obtained with the
choice d(α, β) = max(s), i.e. with a complete linkage
procedure.

An alternative to the above metrices is of course to
ﬁt vertices for each cluster with more than one track,
and use these vertices as “representatives” of the clus-
ter.

3.1. Finding-Thru-Fitting

The most mature algorithm in CMS is the “princi-
pal vertex reconstructor”, also known as the “ﬁnding-
thru-ﬁtting” method. It is a divisive method that in-
ternally uses a ﬁtter and a track-to-vertex compati-
bility estimator to decide which tracks are to be dis-
carded at each iteration step. The maturity of the
implementation and the algorithmic simplicity make
it an ideal baseline for performance evaluation.

3.2. Apex points

In order to overcome the topological problems de-
scribed in section 3, we conceived another approach:
the apex point formalism. The main concept is that
the tracks are substituted by representative points,
the apex points. These points should fully represent

TULT013

CHEP03, La Jolla, California, March 24-28, 2003

3

the tracks with respect to the vertex ﬁnding prob-
lem at hand. The space that the apex points are
deﬁned in can have any dimension; it must only be
equipped with a proper metric fulﬁlling the trian-
gle inequality. Our current implementation produces
three-dimensional points in a Euclidean space, to-
gether with a 3x3 error matrix. Note that the apex-
point-to-track mapping needs not be unique; it may
very well be necessary that n apex points, n > 1, rep-
resent one track.

One can of course also formulate hierarchic cluster-

ing methods on top of the apex points.

3.3. Apex point ﬁnders

An algorithm that searches for such representative
“apex” points is called an apex point ﬁnder. Since
these ﬁnders operate on the points of closest approach,
they can be formulated as generic pattern recogni-
tion problems in one dimension (i.e. along the tracks).
Thus the set of potential apex point ﬁnding algorithms
is huge; a systematic eﬀort to choose algorithms that
satisfy our needs is an ongoing process. So far we have
only investigated a few simple methods:

• The HSM (half sample mode) ﬁnder iteratively
calls an LMS estimator on the points of closest
approach.

• The MTV (minimal two values) ﬁnder looks
for the two adjacent points of closest approach
whose sum of distances to their counterparts on
the other tracks is minimal.

• The MAMF (minimum area mode) ﬁnder looks
for the two adjacent points whose sum of dis-
tances to their counterparts times their distance
is minimal.

Future research will try more sophisticated al-
[8]

gorithms such as a deterministic annealing [7],
method, or a gravitational clusterer [9].

3.4. Global association criterion

The weights that have been introduced for the adap-
tive ﬁtting method (see section 1) can also be used to
deﬁne a global “plausibility” criterion of the result of
a vertex reconstructor. With m being the total num-
ber of tracks and n being the number of vertices we
deﬁne the global association criterion (GAC) by:

p =

1
n · m

m

n

pij

Xi=1

Xj=1

pij =

1 − wij
wij

if i ∈ j
otherwise

(cid:26)

where

TULT013

Figure 2: Two apex point ﬁnders at work.

and wij is the weight wi (1) of track i with respect to
vertex j.

The most important open question with respect to
this criterion is how it relates to the “Minimum Mes-
sage Length” [10]. Can the information theoretic limit
of the vertex ﬁnding task be formulated in terms of
the GAC?

The potential uses of such a criterion are manifold:
• Exhaustive vertex ﬁnding algorithm. All combi-
nations of track clusters could at least in princi-
ple be iterated through, then one can decide for
the smallest GAC found.

• Stopping condition. The GAC could also serve
as a stopping condition in a wide range of algo-
rithms.

• Super ﬁnder algorithms. One could also use it
to resolve ambiguities. More than one vertex
reconstructors could be used on one event, the
GAC could then decide for the “better” solution.

Clearly, some more research in this direction will

have to be done.

3.5. Learning algorithms

“Learning algorithms” can easily be formulated on
top of the apex points; some can also work on the
distance matrix itself. Good candidates for such algo-
rithms are:

• Vector quantisation or the k-means algorithm,
which have dynamic vertex candidates (“proto-
types”) that are attracted by the apex points.

• Potts neurons [11] or the super-paramagnetic
clustering algorithm [12]; these algorithms at-
tribute a spin-state or a mathematical equiva-
lent to every apex point. Spin-spin correlations

4

CHEP03, La Jolla, California, March 24-28, 2003

together with an annealing schedule will then
make sure that similar apex points are described
by the same spin vector.

• Deterministic annealing [7]; this method for-
mulates the clustering problem as a thermody-
namic system with phase transitions, each tran-
sition introducing a new separate cluster of apex
points.

3.6. Simulation experiment

We compared one of our algorithms with two stan-
dard vertex ﬁnding procedures: the PVR (see sec-
tion 3.1) and the so-called D0Phi method [5], [6] –
a special purpose algorithm based on the impact pa-
rameters of the tracks at the beamline. As a novel
method to compare with we chose an agglomerative
clusterer with a vertex ﬁts as “representative points”,
as it was explained in the last paragraph of section 3.
Our testing was done with 1000 Monte Carlo 50 GeV
b¯b events, generated with the CMSIM simulation pro-
gram [13]. Before the actual comparison all algorithms
were automatically ﬁne-tuned to maximize the follow-
ing “score function”:

Score ≡ 10 · EﬀPrim · EﬀSec · Pur0.25

AssEﬀ0.25

Prim · AssEﬀ0.25

Prim · Pur0.25
Sec ·
Sec · (1 − Fake)0.5

“Eﬀ”, “Pur”, “AssEﬀ”, and “Fake” denote the per-
formance estimators described in [1]; “Prim” denotes
primary vertices, “Sec” stands for secondardy vertices.

In the inclusive secondary vertex ﬁnding scenario,
the agglomerative ﬁtting procedure ﬁnds up to 80 per-
cent of the secondary vertices, as opposed to the 50 –
60 percent found by older algorithms. Note that the
D0Phi algorithm is not intended to ﬁnd any primary
vertices, hence the total score parameter is meaning-
less. See ﬁgure 3 for the complete comparison.

4. CONCLUSIONS

We have reached a good understanding of the ro-
bustiﬁcation methods of the classical LS vertex ﬁtters.
We suggest that the adaptive method be used as the
new default ﬁtting procedure for CMS and possibly
other experiments as well. Surely, we still lack such
an exhaustive understanding in the case of the much
more complex task of vertex ﬁnding, although here
we were able to exclude certain classes of algorithms
on the basis of purely theoretic considerations. Our
ﬁrst results are most promising; we can quite clearly
demonstrate that with respect to e.g. secondary vertex

TULT013

Figure 3: Analysis of performance: one novel
agglomerative ﬁnder compared to older vertex ﬁnding
algorithms. The agglomerative method has a much
higher secondary vertex ﬁnding eﬃciency, while it reports
about the same fake rate.

ﬁnding classical methods such as the “ﬁnding-thru-
ﬁtting” algorithm can be surpassed by far. Future
research will mainly focus on three areas:

• apex point ﬁnding algorithms,

• “learning” algorithms,

tions.

References

[1] T. Boccali et al., Vertex reconstruction framework
and its implementation for CMS, these proceed-
ings.

[2] R. Fr¨uhwirth et al., New developments in vertex
reconstruction for CMS, NIMA 502 (2003) 699–
701.

[3] K. Van Driessen and P.J. Rousseuw, Computing
LTS regression for large data sets (1999) Univer-
sity of Antwerp.

[4] J. D’Hondt, R. Fr¨uhwirth, P. Vanlaer, and W.

Waltenberger, CMS Note, in preparation.

[5] G. Segneri, F. Palla, Lifetime based b-tagging with

CMS, CMS Note 2002/046.

[6] F.Abe et al. [CDF Collaboration], Evidence For
Top Quark Production In Anti-P P Collisions At
S**(1/2) = 1.8 TeV, Phys. Rev. D50 (1994) 2966.
[7] K. Rose, Deterministic annealing for clustering,
compression, classiﬁcation, regression and related

3.7. Results

• the global association criterion and its implica-

CHEP03, La Jolla, California, March 24-28, 2003

5

optimization problems, Proc. IEEE, vol. 86, num.
11 (1998) 2210–2239.

[8] K. Rose, E. Gurewitz, and G. C. Fox, Statistical
mechanics and phases transitions in clustering,
Phys. Rev. Lett., vol. 65, num. 8 (1990) 945–948.
[9] S. Kundu, Gravitational clustering: a new ap-
proach based on the spatial distribution of the
points, Pattern Recognition 32 (1999) 1149–1160.
[10] C. S. Wallace, and D. L. Dowe, Minimum Mes-
sage Length and Kolmogorov Complexity, Com-
puter Journal, vol. 42, Issue 4 (1999) 270–283.

[11] M. Bengtsson and P. Roivainen, Using the Potts
glass for solving the clustering problem, Interna-
tional Journal of Neural Systems, vol. 6, num. 2
(1995) 119–132.

[12] M. Blatt, S. Wiseman and E. Domany, Super-
paramagnetic clustering of data, Phys. Rev. Lett.
76 (1996) 3251–3254.

[13] C. Charlot et al., CMS Simulation Facilities,

CMS TN 93-63, CERN, Geneva, 1993.

TULT013


3
0
0
2
 
g
u
A
 
1
3
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
0
9
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Non-deterministic random bit generator based on electronics noise

1

Mario Stipˇcevi´c1, ∗
Rudjer Boˇskovi´c Institute, Bijeniˇcka 54, P.O.B. 180, HR-10002 Zagreb, Croatia
(Dated: November 19, 2012)

Non-deterministic random bits are needed in many scientiﬁc ﬁelds. Unfortunately today’s com-
puters are very limited in ability to produce them. We present here a method for extraction of
non-deterministic random bits from random physics processes, and one practical realization of a
physical generator based on it. The method is shown to deliver increasingly good randomness in
the limit of slow sampling. A sample of approximately 109 bits produced by the physical generator
prototype is subjected to a series of well-known statistical tests showing no weaknesses.

PACS numbers: 05.40.-a, 02.50.Ng, 03.67.Dd

1. MOTIVATION

results in a sequence of numbers:

Today’s computers are Turing machines governed
It has been noticed that such
by deterministic laws.
machines can not solve certain class of problems because
of their inability to produce random numbers. For
example, it is impossible to simulate a simple radioactive
decay. A computer program that would be able to do
this must be able to produce a sequence of times of
decays which is by no means deterministic.

Consequently, a complete machine that is believed
to be the ultimate universal computing machine is
referred to as ”Turing machine with a random number
generator” or ”probabilistic Turing machine” [1, 2].

Since computers play an increasingly important
role in modern science and life, it is also increasingly
important to develop ”good” random number generators.

The classical approach is to approximate truly ran-
dom, non-deterministic generator of random numbers
by a carefully chosen mathematical function which pro-
duces approximately random numbers. Such a function
of course can be calculated on Turing machines. Without
loss of generality we will only consider random bit gener-
ators (RBG). The basic technique is as follows. First, one
chooses at will an integer number x0 (so called seed) from
a large set of numbers that are known to be valid seeds
for a given function f . (Good coverage of the theory and
practice of pseudo-random generators may be found in
[3, 4, 5, 6, 7, 8, 9, 10, 11].) Then one calculates:

x1 = f (x0)

(1)

Iterating this as follows:

xi+i = f (xi), i = 1, 2, 3 . . .

(2)

x1, x2, x3, ...

(3)

These numbers converted to bits and put together
side-by-side form a long sequence of pseudo-random bits.
Sometimes, not all bits from the numbers xi are used
to form the sequence but only a pre-selected part (for
example the most signiﬁcant half).

Variants of this basic technique exists but the under-
lying property of all pseudo-random generators is that
they must accept a seed, a form of mathematical initial
state that completely determines the sequence of bits
produced thereafter. The idea behind all this is that to
someone who does not know the seed and/or function
f (or does not care about them), the sequence of bits
produced appears to be random.

Pseudo-random numbers can be produced eﬃciently
and are used a lot in simulations of stochastic pro-
cesses like passage of particles through matter [12]
or speeding up of calculation of exact problems such
as multi-dimensional integration or primality testing [13].

While good generator

functions which approxi-
mate true randomness quite well
for most purposes
[6, 7, 8, 11], it is absolutely clear that the entropy of the
whole pseudo-random sequence, no matter how long, is
equal to the entropy of the seed only, which is usually
not more than 16 to 64 bits. This is most clearly seen
by noting that the optimal compression of a sequence
of type (3) would result only in the seed x0 and a
decompression routine that equals f . Since f is usually
publicly known it does not represent a useful source of
entropy, thus the only entropy left is that of the seed.
And even this tiny entropy, namely the entropy of the
seed, has to be provided by somebody or something that
has nothing to do with the pseudo-random generator
itself. Therefore the conclusion that pseudo-random
generators do not generate entropy seems unavoidable.

∗Electronic address: Mario.Stipcevic@irb.hr

For applications in cryptography this may be a

killing property. For example provability of information-
theoretic security of Quantum cryptography protocols
such as BB84 [14], or Maurer’s SKAPD protocol [15]
assumes existence of generators of truly random (non-
deterministic) numbers. Even much less complicated
applications such as PIN or TAN number generators
may not be proven secure if the numbers generated are
based on a deterministic procedure. Cases are known
where seeding by a low-entropy source (such as clock) led
to serious compromise of the subsequent cryptographic
protocol. An example is attack to the Netscape’s 40 bit
RC4-40 [16] challenge data and encryption keys which
could be revealed in a minute or so [17]. The authors of
that article suspect that 128 bit version RC4-128 would
not be much harder to break either if seeding is done
(This example also shows how a
in a similar fashion.
perfectly good cryptographic method may be ruined by
a non-educated or malicious implementator.)

All these cases boil down to the fact that it is hard to
get truly random data form today’s computing machines
(usually PC’s). Some ad-hoc methods exist [18, 19, 20]
to extract a handful of random bits per second, but
the only way to get a lot of random bits of guaranteed
quality in a short time is to add a non-deterministic
random bit generator to the computer, thus eﬀectively
realizing the probabilistic Turing machine.

Problem, however,

is to construct a good enough
generator, whose output can be for all practical purposes
considered as being truly unpredictable and random.

We describe, in this article, a theory and one practical
realization of a reliable and fast random bit generator
which should not be expensive to produce.

2. THE METHOD

Since it is obvious that only physical reality may pro-
vide true randomness, the non-deterministic RBG should
be based on a repeated measurements of identical, in-
dependent physics processes and associated method for
extraction of random bits from these measurements. An
RBG should be characterized by the following:

1. its output can not be predicted regardless of the
amount of knowledge about it (just as it would be
impossible to predict outcome of ﬂipping a fair coin
by knowing exactly how it looks);

2. two identical generators may not be synchronized

to produce the same sequence of bits.

The second requirement could well be regarded as the
deﬁnition of a non-deterministic generator. This require-
ment may be reformulated so that a non-deterministic

2

generator may not accept an initial state. This is oppo-
site to pseudo-random generators which must accept an
initial state.

Since we do not expect that measuring of independent
physics processes, that is making independent identical
experiments whose outcome is random, may form any
pseudo-random pattern we are left with only two prob-
lems associated with non-deterministic random bit gen-
erators. These are:

1. statistical bias, deﬁned as:

b = p(1) − 0.5

(4)

2. correlations among bits, among which the most im-
portant serial correlation coeﬃcient is deﬁned as
[5]:

a =

P
n

n

n−1
i=0 bib(i+1) mod n −

n−1
i=0 b2
i

−

(cid:16)P

(cid:17)

(cid:16)P

2

n−1
i=0 bi
2

(cid:17)

(cid:16)P
n−1
i=0 bi

(cid:17)

(5)

Both are measures of imperfections that are inevitable
in practical realizations of generators. Correlations
appear if the measured physics processes (experiments)
are not completely statistically independent of each
other, whereas statistical bias is mainly associated
with imperfections in the measuring equipment (eg.
electronics). Serial correlation coeﬃcient is a measure of
the extent to which a bit (in a sequence) depends on a
previous bit ([5]), and takes on a value between -1 and 1.
We suppose that correlations between bits further apart
(corresponding to experiments further apart in time) are
smaller if not negligible. For truly random sequences
of bits, of course, both statistical bias and correlations
tend to zero, when the length of the sequence goes to
inﬁnity.

The method presented here solves both problems. It
consists of counting events which are result of measure-
ment of some random physical process. For example
events may be a radioactive decay or taping of rain drops
on a tin roof. These events appear at random but mea-
sured for a long time they have some mean period, µ. The
state of the counter is examined at regular time intervals
with the period T ≫ µ. If the counter state is found to
be odd then the output from the generator is ”1”, other-
wise it is ”0”. Unlike some other methods which require
exponential distribution of time intervals between events
[21] or sampling of an analog white noise source [22], we
will show that this method, in the limit of slow sampling
(T /µ → ∞) leads to vanishing bias and correlations re-
gardless of the distribution of the random process being
measured. Therefore it may used to extract random bits
from a large variety of processes.

3. PRACTICAL REALIZATION OF A RANDOM
BIT GENERATOR

properties of unpredictability needed for a truly random
noise voltage source.

3

Useful non-deterministic random bit generator built in

hardware should satisfy several requirements:

1. Bit sampling method must not rely strongly on any
property of the measured physical process other
than its randomness, at least in some easily achiev-
able limit;

2. Generator should withstand reasonable tolerance in
components and operating conditions (eg. supply
voltage, temperature, EM noise) without the need
for (re)calibration or compensation;

3. Possible malfunctions during the lifetime of the
generator should be foreseen and checked for at
each generator restart (for example like recom-
mended in [23]) or even continuously;

4. Sequences of bits produced by the generator should
pass, with a high probability, any known statistical
randomness test.

In the generator described here physical processes in
Zener diode serve as a source of randomness. The noise
voltage at terminals of the diode is ”measured” by a spe-
cial electronic circuit at regular time intervals. Each mea-
surement results in a random bit.

The circuit presented in Fig. 1 follows the method of
operation described in the previous section. It consists
of the following ﬁve blocks:

1. A source of electric noise;

2. a DC decoupling capacitor C;

3. a circuit for digitizing the noise voltage consisting
of a comparator controlled with a rough automatic
zero-bias correction circuitry;

4. a counting circuit (JK ﬂip-ﬂop);

5. a sampling circuit which delivers a bit upon an ex-

ternal request.

It is a well known fact that a Zener diode operating
in a reverse polarity and the current strength near the
knee, can serve as a noise generator. For example, a
6.8 Volt commercial Zener diode can produce a noise
ﬂuctuation of an amplitude of 30 to 50 mV (peak to
peak) with a mean frequency of zero crossings of the
order of 10 MHz [24].

It is also well known that a Zener diode with a
knee voltage of less than 6.2 V operates mainly in the
Quantum Mechanical (tunneling) regime, while the
diodes with the knee above that operate mainly in the
micro plasma regime [24]. Both regimes have ideal

The best temperature stability of the noise amplitude
is also obtained in the Zener diodes with the knee voltage
of approx. 6.2 V, because in this case the two regimes
with opposite temperature coeﬃcients are in equilibrium
[24]. The same condition is also optimal from the point
of the long term stability.

The generator functions in the following way. Noisy
voltage from the Zener diode is AC decoupled from the
digitizing stage which consists of a comparator COMP
controlled with a rough automatic zero-bias feedback
control. The role of the comparator is to convert tiny
analog noise to the digital signal suitable for further
processing.

The capacitor C in the series with the output resistance
R of the noise source and the eﬀective input resistance
Ri of the comparator forms a major contribution to an
unwanted ”memory”. The timely persistence τe of this
memory is equal to:

τe = C(R + Ri).

(6)

Voltage amplitudes of any two noise variations which
happen within the period τe will be mutually correlated
because of the electric charge in the capacitor C which
has no time to discharge through the resistance in the
system. If physical events are not completely indepen-
dent of each other there will be another persistence τp
giving rise to the total memory:

τ = τe + τp

(7)

Luckily, this ”memory” eﬀect dies oﬀ exponentially with
the time distance between the two variations, thus one
can conclude that any two variations that are distant
enough in time may be considered as statistically inde-
pendent. Whenever this applies, our method is valid, as
will be explained later. Nevertheless, the ”memory” of
the circuit limits frequency bandwidth of the noise and
sets an absolute upper limit to the bit extraction rate
from the generator, which limit is independent of the

+Vcc

Z

R

Ri

VDD

Vr

C

COMP

Ic

LP

V1/2

OPA

J
CP
K

Q

Q

D
CP

Q

Q

Sample

Request

FIG. 1: Schematic diagram of the non-deterministic random
bit generator

latter bit extraction method.

The negative input of the comparator COMP is
connected to a suitable DC reference voltage Vr. Be-
tween the two inputs of the comparator (positive and
negative) a small DC ”oﬀset” voltage can be induced
by virtue of the resistor Ri and the control current
Ic that ﬂows through it. As the result, the positive
input of the comparator ”sees” the sum of the oﬀset
voltage and the noise voltage. Whenever the sum
exceeds the reference voltage Vr, the output of the
comparator goes into the high logical state ”1”, whereas
when the sum goes below the Vr, the output goes into
the low logical state ”0”. This is illustrated in the Fig. 2

By setting the control current Ic a little lower or
higher, the output of the comparator COMP spends
respectively a little more or less time in the logical
state ”1”. At certain strength of the control current
Ic, one can achieve that the comparator COMP spends

FIG. 2: Signals at diﬀerent points of the hardware generator:
a) analog diode noise, b) output of the comparator COMP, c)
output Q of the JK ﬂip-ﬂop, d) ﬁnal output of the generator
(D ﬂip-ﬂop).

4

2 VDD.

approximately equal amount of time in either of the
logical states, the point at which the comparator has
the highest eﬃciency of detecting voltage variations. In
other words, the duty cycle of the comparator COMP
would then on average be close to 0.5. This is ensured
by setting V1/2 = 1
If, for any reason, the duty
cycle ever gets changed, the feedback network closed
by comparator COMP, operational ampliﬁer OPA and
low-pass ﬁlter LP (Fig. 1) will respond by changing the
control current Ic so that the duty cycle of 0.5 will be
restored. This behavior eﬀectively solves the require-
ment 2. Already here, at the output of the comparator
COMP, there is an approximately equal chance that the
state is at logical ”0” or at logical ”1”, i.e. the bias of
the sampled output would be close to zero. However,
due to technical reasons it is quite diﬃcult to keep the
bias below 1/1000 for long term and even this would be
not possible without some ﬁne-tuning. Tuning the bias
consumes a lot of time (due to its statistical nature) and
would therefore be unfavorable for a mass production of
the generator.

Present method for extracting random bits eliminates
the need to tune the bias to zero value, and actually
allows to achieve bias as low as desired, without the
need to do any modiﬁcations to the circuit.

Event counting is the crucial point of the method.
Namely, the JK-type ﬂip-ﬂop (Fig.
1) performs a
continued counting of the events, but keeps track only
whether the count is even or odd. The result of the
counting appears at the output Q of the JK ﬂip-ﬂop
and represents a new random bit sequence with highly
suppressed bias and serial correlation. To understand
how this works it is important to keep in mind that the
output Q of the JK ﬂip-ﬂop shall be sampled periodically
in time. This is a condition for good operation of this
generator. The sampling is done by the D-type ﬂip-ﬂop.

Without the loss of generality, let us suppose that at
the time zero (t = 0) the Zener diode Z exhibits a voltage
breakdown. (Voltage breakdown is only one of the pro-
cesses that may cause ﬂuctuations of the voltage across
the diode that are manifested as noise. But to simplify
the language we will refer to ”voltage breakdown” as a
synonym for a sudden positive jump of the voltage across
the Zener diode.) This causes the output of the compara-
tor COMP to exhibit a positive going transition (and
later a negative one). Every such event triggers the JK
ﬂip-ﬂop to reverse its state at the output Q. This is il-
lustrated in the Fig. 2.b and 2.c.

Let us suppose that the voltage breakdowns happen
at times t0 = 0, t1, t2, t3, t4, etc. Tiny intervals of time
between neighboring voltage breakdowns ∆tk = tk −tk−1
where k = 1, 2, 3... are distributed according to some
statistical distribution. The mean value µ of duration of

this tiny intervals is deﬁned as:

µ = lim
N→∞

1
N

N

Xk=1

tk

For example, in the pure Quantum Mechanical regime
of the Zener diode, intervals between neighboring voltage
breakdowns ∆tk follow the Exponential distribution.
Mixing of the Quantum Mechanical model with the
plasma noise, noise sources other than a Zener diode
or other models including eﬀective memory and/or
ﬁltering of the noise signal prior to digitization, may
lead to more centered distributions such as Poisson
or even Uniform-like distribution. The power of this
method lies in the fact that the knowledge of the actual
distribution of the time intervals between neighboring
voltage breakdowns is irrelevant. The only important
thing is that the shape of the distribution stays stable
over a period of time substantially larger than the
sampling period T .

The method presented in this work consists
in
sampling (i.e. reading out) of the output Q of the JK
ﬂip-ﬂop periodically, at times T , 2T , 3T . . . and generally
at times i × T with i = 1, 2, 3 . . . . The sampling is
made by a D-type ﬂip-ﬂop (Fig.
1). Namely, when
a positive-going edge of the sampling signal (that is
transition from ”0” to ”1”) appears at the ”Request”
input, state that is present at the input D gets sampled.
Sampled value is memorized (frozen) and displayed at
the output Q of the D-type ﬂip-ﬂop, and stays there
until a new positive-going edge appears. According to
the deﬁnition of the D-type ﬂip-ﬂop, the sampling is
done almost instantaneously so its output stays frozen
for almost the whole sampling period T which makes
possible for other devices (such as a computer) to read
it safely.

Let us now suppose that the sampling period T is cho-
sen large enough so that many voltage breakdowns occur
during each sampling period. In another words, we sup-
pose:

T ≫ µ.

(9)

This situation is illustrated in the Fig. 2 where several
voltage breakdowns in the noise signal (FIG.2.a) happen
during any sampling period of length T (FIG 2.d).

Furthermore,

let us denote that the JK ﬂip-ﬂop
changes its state at the output Q: N0 times from t = 0
till t = T , N1 times from t = T till t = 2T , and
generally Ni times from t = iT , till t = (i + 1)T , where
i = 1, 2, 3, . . . .

Now we can conclude that every sampling period (of
length T ) approximately equals the sum of many tiny
intervals ∆tk:

5

(8)

T ≈

N0

∆tk

Xk=1
N0+N1

Xk=N0+1

T ≈

∆tk

etc.

(10)

The above equalities hold only approximately because
the ﬁrst and the last of the tiny intervals in a given sum
may be only partially happening during the respective
sampling period T .
In such a case, a part of the ﬁrst
tiny interval may have actually happened during the
previous period, while a part of the last tiny interval
may have actually happened during the next period
(sum). However, approximation can be made arbitrarily
good by setting the large enough sampling period T .

The Central limit theorem of the Statistics states that
the sum of a large number (say N ) of independent ran-
dom variables (for example ∆tk), which follow some
(any) distribution, approaches Normal distribution in the
limit of large N . This means that the statistical variable
x deﬁned as:

x =

∆tk,

N

Xk=1

(11)

follows approximately the Normal distribution for
large N . However, in our case, deﬁned by equations (10),
the ”variable” x is ﬁxed (being just T ), and the relevant
statistical random variable becomes the number of sum-
mands, N deﬁned as:

N

Xk=1

N +1

Xk=1

∆tk ≤ x <

∆tk.

(12)

In the special case mentioned above, when the tiny in-
tervals ∆tk are distributed according to the Exponential
distribution, random variable N follows, by deﬁnition,
the Poisson distribution:

P (N ) =

exp(−T /µ).

(13)

(T /µ)N
N !

Quite generally, regardless of the distribution function
of tiny intervals ∆tk, it can be shown [25] that the in-
teger random variable N is distributed according to the
Binomial distribution B(N ; 2T /µ, p), where p quickly ap-
proaches 0.5 as T /µ tends to inﬁnity. Given the general
expression for the Binomial distribution:

B(r; n, p) =

pr(1 − p)n−r.

(14)

n
r (cid:19)

(cid:18)

distribution of the variable N can be written as:

B(N ; 2T /µ, p) =

pN (1 − p)2T /µ−N .

(15)

2T /µ

(cid:18)

N (cid:19)

This is not in a contradiction with the previous result
because the Poisson distribution (13) becomes equal to
the Binomial distribution (15) in the limit T /µ → ∞.

Even without a rigorous proof it is easy to understand
why N follows symmetric Binomial distribution (15) in
the limit of slow sampling or, equivalently, large N . Fol-
lowing eq. (12), for large N we can write:

Approximating the deﬁnition (8) with:

x ≈

∆tk.

N

Xk=1

µ ≈

1
N

N

Xk=1

∆tk

N ≈

x.

1
µ

(16)

(17)

(18)

to eliminate the sum in (16) we may conclude:

Since x follows Normal p.d.f. so should N . But since
N is an integer number it actually follows a discrete
version of the Normal p.d.f., i.e.
symmetric Binomial
distribution with p=0.5, Q.E.D. This is a very important
result because it means that (15) holds regardless of
the distribution of tiny intervals ∆tk. This solves the
requirement 1.

It can be shown [25] that as a special consequence of
the equation (15) the probability that the random vari-
able N is even becomes equal to the probability that it
is odd, in the limit T /µ → ∞. But, when the JK ﬂip-
ﬂop changes its state an even number of times its output
Q returns to the initial state (say ”0”) whereas when it
changes an odd number of times it returns the comple-
mentary state (say ”1”). Thus, in the limit of slow sam-
pling, the probability of producing ”0” would be equal to
the probability of producing ”1” at the output Sample
(Fig. 1). This fact can be expressed like this:

T /µ → ∞ ⇒ b → 0.

(19)

Thus it has been shown that the generator and the
corresponding method of sampling random phenomena
solve the ﬁrst problem stated in the section 2, i.e. that
the bias should vanish in some limit.

The second problem, namely the statistical indepen-
dence of the logical states (bits) at the output of the
generator, is solved as follows. First we realize that the

6

subsequent voltage breakdowns in the Zener diode are
statistically independent because the breakdown itself is
an unpredictable physical phenomenon. Secondly, the
internal capacitive memory that appears in the circuit
(which for example may be introduced by imperfections
in electronics design, or be a part of physical process
governing the alternate current noise source) ”dies oﬀ”
exponentially with the half-life τ (Eq. (7)). Eﬀect of the
memory on correlations can be made as small as desired
in the limit of slow sampling, that is by making T/τ
large enough. Namely, in the limit T /τ → ∞ sums of
the type (10) become mutually statistically independent.
The statistical independence of two subsequent sums,
in that limit, takes place because the two sums contain
large portion of summands that are ”out of the range”
of the memory eﬀect of each other. If we consider two
sums that are not subsequent, then the memory eﬀects
are even smaller. This means that a bit corresponding to
a sum becomes statistically independent of neighboring
and even more all other bits, in the limit of slow sam-
pling.
It can be concluded that the generator and the
corresponding method of sampling random phenomena
also solve the second problem stated in the section 2.

Functioning of the whole circuit in Fig.

1 can be
best understood by looking at signals shown in the Fig.
2a-e. One can see that with each positive-going edge
at the output of the comparator COMP (Fig. 2.b), the
JK ﬂip-ﬂop changes state at its output Q (Fig. 2.c).
The sampling signal (Fig. 2.d) isn’t, of course, in any
synchronization with these signals. Whenever it exhibits
a positive-going transition, the output Q of the JK
ﬂip-ﬂop gets sampled and the state thus obtained is the
output bit from the generator (Fig. 2.e).

It is important to understand that the output Q of the
JK ﬂip-ﬂop always produces ”1” after ”0” and ”0” after
”1”, therefore its output is not random at all. However
a duration of zeros and ones varies randomly (Fig. 2c)
and periodic sampling of the said output by the D-type
ﬂip-ﬂop provides a good quality random bits, in the limit
of slow sampling (T ≫ µ and T ≫ τ ).

4. SIMULATIONS

In order to check the theory of operation described
above, a series of simulations of the circuit in Fig. 1 were
made. Simulations should provide an insight on how the
type of distribution of tiny intervals between subsequent
physical events and ratios T /τ and T /µ inﬂuence the
quality of generated bits. The circuit was simulated by a
speciﬁcally designed computer program which simulates
a random digital signal obeying a speciﬁed distribution
which would be present at the output of the comparator
COMP, and assuming that this signal gets processed
by ideal ﬂip-ﬂops. The approximation of considering

the two ﬂip-ﬂops to be ideal is very good because their
fall and rise times (of approximately 7 nanoseconds)
are much shorter than the changes in the noise being
processed.

Two distributions of tiny intervals were simulated:
Exponential distribution
Exponential and Uniform.
is natural distribution for many processes. Uniform
distribution assumes that the intervals between physical
events may be between zero and 1 (in arbitrary time
units, for example microseconds) and evenly distributed
between the two limits. The two distributions are chosen
because they are very diﬀerent: for example Exponential
d. is completely non-symmetric and peaked at one value
(zero) whereas Uniform d.
is symmetric and has no
peak, etc. Most processes should obey distributions that
are somewhere between those two extremes.

The eﬀect of memory was simulated as a pile up: any
two breakdowns closer (in time) than τ were considered
as being just one breakdown happening at the end of
the last breakdown. This in eﬀect may chain up more
than two events to be seen as one. This is a very
faithful model for a noise source which we use, and a
reasonable approximation for any other noise source
because memory (by deﬁnition) always piles up events in
a way that events which happen to close to one another
get piled up and be detected as just one event.

The method for extracting bits does not prefer zeros
nor ones, therefore we do not expect the bias to be
signiﬁcant even in the case when both ratios T /τ and
T /µ are small. What happens however is that when
sampling is being made too fast there is an excess of
longer patterns of ones or zeros. This in turn means that
the probability of single ones (pattern ”010”) a well as
that of single zeros (pattern ”101”) will be smaller than
expected (expected vales are: p(010) = p(101) = 1/8).

For each choice of distribution, ratio T /τ and ratio
τ /µ,
three
the program calculates (by simulation)
output values: probability of ones p(1), probability of
single ones p(010) and serial correlation coeﬃcient a.
Results of simulations for the two distributions and a
range of ratios T /τ and τ /µ are shown in the tables
I-VI. Values shown are bias (b = p(1) − 0.5), second
order bias (b010 = p(010) − 0.125), and serial correlation
coeﬃcient. Number of bits generated in each simulation
is 6,250,000. According to that statistics, 1 sigma
errors are: 0.00020 for b, 0.00007 for b010 and 0.00040
for a. As a source of random numbers the program
used semi-hardware ”/dev/urandom” Linux kernel
random bit generator. The result were cross-checked
using the standard implementation of ”secure” software
random number generator rand() that is part of the
Linux C compiler (we used gcc version 2.95). No discrep-
ancies were found in the results within the error margins.

7

T /µ
2
3
4
6
9
12

0

0.5

0.2

0.75

0.33

1
0.00013 0.00010 -0.00007 0.00030 -0.00029 0.00004
-0.00030 0.00028 -0.00010 0.00026 -0.00013 0.00041
-0.00005 0.00019 0.00022 -0.00020 -0.00003 -0.00030
-0.00000 0.00036 -0.00015 0.00001 0.00020 0.00013
0.00011 -0.00005 0.00012 0.00003 -0.00005 -0.00029
-0.00013 0.00010 -0.00013 -0.00027 0.00037 -0.00013

TABLE I: Uniform distribution, bias

T /µ
2
3
4
6
9
12

0

0.5

0.2

0.75

0.33

1
0.00955 0.00442 0.00383 0.01479 0.06978 0.10538
-0.01351 -0.01744 -0.02429 -0.04041 -0.05231 -0.02093
0.00701 0.01220 0.01852 0.02598 -0.00794 -0.03638
-0.00100 -0.00121 -0.00227 -0.01113 -0.01609 0.01694
0.00028 -0.00001 0.00015 -0.00224 -0.00261 0.00509
0.00006 0.00000 0.00015 -0.00038 0.00017 -0.00049

TABLE II: Uniform distribution, b(010)

τ /µ

τ /µ

τ /µ

T /µ
2
3
4
6
9
12

0

0.5

0.2

0.75

0.33

1
-0.05748 -0.04355 -0.05233 -0.11648 -0.31369 -0.40496
0.06358 0.08214 0.11607 0.20639 0.27048 0.06288
-0.03136 -0.05413 -0.08151 -0.11538 0.01226 0.17930
0.00477 0.00503 0.01013 0.05077 0.07227 -0.07653
-0.00004 0.00024 -0.00057 0.00963 0.01214 -0.02286
0.00044 0.00017 -0.00077 0.00152 -0.00170 0.00254

TABLE III: Uniform distribution, serial correlation

As expected, bias is always consistent with zero. Ab-
solute values of the second order bias and the serial cor-
relation coeﬃcient quickly drop to zero (within the er-
ror margins) when the sampling period becomes large
enough and memory eﬀects small enough. The Expo-
nential distribution seems slightly more favorable in this
respect than the Uniform. From these results it can be
concluded that in the worst case good bits are achieved
by generators with the memory eﬀect τ /µ < 0.33 and the
ratio T /µ > 9. This means that using a standard Zener
diode with 3 MHz mean frequency of voltage turnovers it
should be possible to build a generator that is capable of
generating up to several hundred kilobits of good-quality
random bits per second.

5. PROTOTYPE TESTING

We have constructed a prototype based on the block-
diagram in Fig. 1 using a Zener diode, standard analog
chips and CMOS logic. Bits were produced at 300
kbit/sec. We built four identical circuits and fed their
outputs to a PC computer, thus obtaining a total of 1.2
Mbit/sec. Thanks to such a high speed we were able to
easily produce long sequences of bits for the subsequent
testing. The sequences of bits produced by the physical

T /µ
2
3
4
6
9
12

0

0.5

0.2

0.75

0.33

1
0.00003 0.00023 -0.00025 0.00013 0.00033 0.00005
0.00018 -0.00013 0.00003 -0.00016 0.00034 0.00006
-0.00010 -0.00033 -0.00013 -0.00014 -0.00013 0.00005
0.00002 -0.00013 -0.00013 0.00022 -0.00013 -0.00016
-0.00013 0.00029 0.00014 -0.00011 -0.00016 0.00018
-0.00021 -0.00001 0.00002 -0.00000 0.00032 0.00024

TABLE IV: Exponential distribution, bias

T /µ
2
3
4
6
9
12

0

0.5

0.2

0.75

0.33

1
-0.00398 -0.00101 0.00168 0.01385 0.04992 0.04163
-0.00080 0.00008 -0.00016 -0.00299 0.00777 0.04519
0.00016 -0.00015 0.00014 -0.00013 -0.00939 0.00454
0.00008 -0.00001 0.00005 -0.00011 0.00209 -0.00742
-0.00017 0.00000 -0.00002 -0.00007 -0.00029 0.00139
0.00002 -0.00006 -0.00003 -0.00008 -0.00020 -0.00045

TABLE V: Exponential distribution, b(010)

τ /µ

τ /µ

τ /µ

T /µ
2
3
4
6
9
12

0

0.2

0.5

0.75

0.33

1
0.01869 0.00423 -0.00729 -0.06256 -0.20970 -0.20169
0.00306 0.00016 0.00042 0.01389 -0.03947 -0.19117
-0.00001 0.00033 -0.00016 0.00084 0.04353 -0.02615
-0.00058 -0.00029 -0.00023 0.00021 -0.00972 0.03569
0.00052 -0.00021 0.00020 0.00022 0.00095 -0.00623
0.00033 0.00032 0.00021 0.00006 0.00015 0.00125

TABLE VI: Exponential distribution, serial correlation

generator were tested by three batteries of statistical
tests: Walker’s ENT [26], Marsaglia’s DIEHARD [27]
and NIST’s Statistical Test Suite [28].

The ENT consists of battery of standard tests:
entropy, Chi-square, mean value, Monte Carlo pi value
test and the serial correlation test. These tests may be
evaluated for short sequences thus making possible a
fast check of the prototype in the development stage.
The most important of them are the mean value and the
serial correlation test because they best grasp problems
in hardware. The mean value addresses the inequality
between number of ones and zeros which is very hard
to keep close to zero with imperfect hardware. Non
zero serial correlation reﬂects existence of a short-term
memory in the system, as explained in the section 3.
Indeed, at the development stage of the prototype a
slight consistent bias (positive, of the order of 10−4) has
been noticed on all four gadgets which was attributed to
the fact that comparator had quite a diﬀerent rise time
from the fall time at its output. This was corrected by
taking another comparator after which the bias became
undetectable.

Once the generator was debugged and ENT tests
passed well, we proceeded by generating long sequences
(approx. 108 bits) needed by the DIEHARD, probably

the stringiest battery of tests known today. Table VII
shows a typical result of testing of 8 × 107 bits (10
megabyte) of random data obtained from the generator.

8

Test name
p-value
0.6562
BIRTHDAY SPACINGS
OVERLAPPING 5-PERMUTATION 0.4666
OVERLAPPING 5-PERMUTATION 0.5396
BINARY RANK 31x31
0.4256
0.0246
BINARY RANK 32x32
BINARY RANK 6x8
0.1513
0.5340
BITSTREAM TEST
OPSO
0.2728
OQSO
0.6959
0.8619
DNA
COUNT THE 1’s
0.6046
0.3398
COUNT THE 1’s
PARKING LOT
0.7513
0.2088
MINIMUM DISTANCE
3D SPHERES
0.2800
SQUEEZE
0.9366
0.9472
OVERLAPPING SUMS
RUNS UP
0.3664
0.8111
RUNS DOWN
RUNS UP
0.4229
0.5369
RUNS DOWN
CRAPS (wins)
0.2463
0.1541
CRAPS (throws)

TABLE VII: Typical result of the DIEHARD tests for the
hardware non-deterministic generator

sequence has

According to Marsaglia [27]

failed
a test if the p-value of that test is very close to 1
(p > 0.999999), otherwise the sequence has passed the
test. There are 15 diﬀerent tests in the DIEHARD, but
some are run more than once with diﬀerent parameters
(eg. binary rank test) or on independent parts of the
sequence under test (eg. count the 1’s). Results in the
table VII indicate a random sequence that has passed
all the tests.

It is interesting to note that eﬀorts to standardize
requirements for random bit generators for use with
cryptographic software have been taken, notably the
recommendations of NIST [23]. A sequence of 109
bits divided in 100 equally long pieces (ﬁles) has been
subjected to the NIST’s battery of randomness tests,
version sts-1.50 [28]. We used the recommended test
parameters. Results are summarized in the Table VIII.

According to the NIST analysis program, the mini-
mum pass rate for each statistical test with the exception
of the random excursion (variant) test is approximately
0.9602 whereas the minimum pass rate for the random
excursion (variant) test is approximately 0.9527. Taking
this into account we conclude that all 16 tests were
passed with excellent marks. Passing all three batteries
of tests shows that this generator conforms also to the

Test# p-value Pass rate Statistical test
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.

0.9900 Frequency
0.9900 Block-Frequency
0.9900 Cusum
1.0000 Runs
0.9900 Long-Run
0.9900 Rank
0.9900 FFT
1.0000 Aperiodic-Template
0.9900 Periodic-Template
0.9900 Universal
0.9800 Apen
1.0000 Random-Excursion
0.9844 Random-Excursion-V
0.9900
0.9900 Lempel-Ziv
0.9900 Linear-Complexity

0.9240
0.8165
0.8343
0.0456
0.3041
0.7197
0.3504
0.0909
0.7791
0.2022
0.9240
0.8623
0.7727
0.8831
0.2022
0.8165

Serial

TABLE VIII: Test of randomness extracted from heartbeats

general requirement 4 stated in the section 3.

As one more check of the sampling method itself bits
were generated from a cardiogram data of a healthy pa-
tient. A record of 1190 consequent heartbeats was made
in course of an ergometric stress test. During the test pa-
tient is required to walk on a treadmill whose speed varies
according to a standardized procedure. Test lasts ap-
proximately 12 minutes. The times at which main heart-
beat peaks occur were considered as physical ”events”.
Taking T /µ ≈ 5 we obtained a sequence of 216 bits us-
ing our method. Results of ENT test shown in the table
IX indicate excellent randomness. This demonstrates the
power of the method which is able to extract good ran-
dom bits form a sequence of heartbeats which are nearly
periodic but have only a slight random jitter.

9

TABLE IX: Test of randomness extracted from heart beats

The conclusion is that heartbeat timings may very well
be used as a base for a non-deterministic random gener-
ator !

6. CONCLUSION

A method for extraction of non-deterministic random
bits from random physics processes (such as nuclei decay
or noise voltage variations) is presented. The method
is characterized by the fact that it ensures a good qual-
ity random bits in an easily obtainable limit, that is the
limit of slow sampling, regardless of the distribution of
times between adjacent processes. A physical random
bit generator which makes use of the method and Zener
diode noise was built and successfully tested. The gen-
erator is shown to conform to general requirements for
non-deterministic generators stated in section 3. A sam-
ple of approximately 109 bits produced by the physical
generator prototype is subjected to a series of well-known
statistical tests showing no weaknesses.

7. ACKNOWLEDGMENTS

The heartbeat recording was provided by courtesy
of M. Martinis from Rudjer Boˇskovi´c Institute and the
Institute for cardiac deseases, Zagreb and These data
were taken with approval of the patient.

Bit entropy Chisquare Mean value Serial correlation
0.67/50% 0.528 ± 0.034 −0.003 ± 0.069
0.9977

Parts of the method and apparatus for producing ran-

dom bits are subjected to a patent procedure.

[1] M.A. Nielsen, I.L. Chuang, Quantum Computation and
Quantum Information, (Cambridge University Press,
Cambridge, 2000) p. 2.
free

encyclopedia,
http://www.wikipedia.org/wiki/Probabilistic Turing Machine

Internet URL:

[2] Wikipedia,

the

46(1998) 485.

18(1992) 79.

[3] M. J. Atallah, Algorithms and Theory of Computation
Handbook, (CRC Press LLC, Boca Raton, 1998) p. 29.
[4] W. H. Press, B. P. Flannery, S. A. Teukolsky, W. T. Vet-
terling, Numerical Recipes in C: The Art of Scientiﬁc
Computing, (Cambridge University Press, New York,
1992)

[5] D. E. Knuth, The art of computer programming, Vol. 2,

Third edition, (Addison-Wesley, Reading, 1997)
[6] M. Luescher, Comp. Phys. Comm, 79(1994) 100.
[7] F. James, Comp. Phys. Comm. 79(1994) 111.
[8] P. Hellekalek, Mathematics & Computers in Simulation

[9] W. E. Sharp, C. Bays, Computers & Geosciences,

[10] A.J. Miller, P. Mars, Mathematics & Computers in Sim-

ulation, 19(1997) 198.

[11] G. Marsaglia, Statistics & Probability Letters, 9(1990)

[12] S. Agostinelli et. al, Nucl. Instr. and Meth. A 506(2003)

345.

250.

[13] M. O. Rabin, J. Number Th. 12(1980) 128.
[14] C. H. Bennet, G. Brassard, Quantum cryptography:
Public key distribution and coin tossing, Proceedings of
IEEE International Conference on Computers, Systems
and Signal Processing, (IEEE, New York, 1984) p. 175.
[15] U. Maurer, IEEE Transactions on Information Theory,

39 (1993) 733.

10

[16] R. L. Rivest, The RC4 Encryption Algorithm, RSA Data

Security Inc., Mar. 12, 1992.

[17] I. Goldberg, D. Wagner, Dr. Dobb’s, January 1996
[18] D. Davis, R. Ihaka, P. Fenstermacher, Cryptographic
Randomness from Air Turbulence in Disk Drives in: Lec-
ture Notes in Computer Science Springer Verlag, Berlin,
1984) p. 839.

[19] D. Eastlake, S. Crocker, J. Schiller, Randomness Recom-
mendations for Security, Internet RFC 1750, December
1994.

[20] T. Ts’o, private communication. See also Linux man page
urandom(4) and source code of Linux kernel ver. 2.4.18,
ﬁle random.c

[21] C. H. Vincent, J. Phys. E 3(1970) 594.
[22] V. Bagini, M. Bucci, A design of Reliable True Ran-
dom Number Generator for Cryptographic Applications,
in: Proceedings of CHES’99 Workshop, (Springer Verlag,
Berlin ,2000) p. 204.

[23] Security Requirements for Cryptographic Modules, Fed-

eral Information Processing Standards Publication 140-1,
(FIPS, Gaithersburg, 1994)

[24] P. I. Somlo, Electronics Letters, 11 (1975) 290.
[25] M. Stipˇcevi´c, Some properties of inverted distributions
and their application in sampling random phenomena, to
appear in the Cryptology ePrint Archive, Internet URL:
http://eprint.iacr.org

[26] ENT
Test
http://www.fourmilab.ch/random/

- A Pseudorandom Number
Program,

J. Walker,

Internet

Sequence
URL:

[27] DIEHARD battery of stringent randomness tests (vari-
ous articles and software), G. Marsaglia, Internet URL:
http://stat.fsu.edu/~geo/diehard.html, also avail-
able on CDROM online

[28] A. Rukhin et. al., A Statistical Test Suite for Ran-
dom and Pseudorandom Number Generators for Crypto-
graphic Applications, NIST Special Publication, (NIST,
Gaithersburg, 2001)


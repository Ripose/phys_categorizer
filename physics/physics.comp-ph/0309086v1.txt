3
0
0
2
 
p
e
S
 
0
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
6
8
0
9
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Modiﬁed conjugated gradient method for diagonalising large matrices

Quanlin Jie∗ and Dunhuan Liu
Department of Physics, Wuhan University, Wuhan 430072, P. R. China
(Dated: February 2, 2008)

We present an iterative method to diagonalise large matrices. The basic idea is the same as the
conjugated gradient (CG) method, i.e, minimizing the Rayleigh quotient via its gradient and avoid-
ing reintroduce errors to the directions of previous gradients. Each iteration step is to ﬁnd lowest
eigenvector of the matrix in a subspace spanned by the current trial vector and the corresponding
gradient of the Rayleigh quotient, as well as some previous trial vectors. The gradient, together
with the previous trail vectors, play a similar role of the conjugated gradient of the original CG
algorithm. Our numeric tests indicate that this method converges signiﬁcantly faster than the orig-
inal CG method. And the computational cost of one iteration step is about the same as the original
CG method. It is suitably for ﬁrst principle calculations.

PACS numbers: 02.70.-c, 31.15.Ar, 31.15.-p, 95.75.Pq

I.

INTRODUCTION

In ﬁrst principle calculations, such as band struc-
ture calculations, atomic and molecular structure cal-
culations, one of the basic tasks is to ﬁnd several low-
est eigenvalues and the corresponding eigenvectors iter-
atively (and very often self-consistently) of the eﬀective
Hamiltonian [1]. The matrix dimension of the Hamilto-
nian may range from tens of thousands to several mil-
lions, and one may need up to several thousands of the
lowest eigenvectors of the eﬀective Hamiltonian. Diag-
onalising of matrices in such a scale needs considerable
CPU time and memories. It is one of the major numerical
costs in the ﬁrst principle calculations, and the eﬃciency
of the algorithm is crucial for the performances of the
whole program. There are many eﬀorts to improve the
algorithm [2, 3, 4, 5, 6].

Among widely used algorithms, such as Lanczos [5, 7],
Dividson [8], relaxation method [4, 9], DIIS ( Direct In-
version in the Iterative Subspace, which minimizes all
matrix elements between all trial vectors) [10], and its
later version RMM-DIIS [2, 11] (RMM stands for resid-
ual minimization, i.e., minimizing the norm of the resid-
ual vector in iterative subspace), the conjugated gradient
(CG) method [1, 2] is a valuable tool to ﬁnd a set of low-
est eigenvectors of a large matrix. Brieﬂy speaking, to
obtain the lowest eigenvector of a matrix H for the gen-
eral form eigenvalue problem

the CG method iteratively minimizes the Rayleigh quo-
tient

H

ψ
|

i

= ES

ψ
|

,
i

H
φn
En = h
|
S
φn
|
h

φn
|
φn
|

i
i

,

(1)

(2)

mization point in the direction of the conjugated gradient
which is a combination of the current gradient and pre-
vious conjugated gradient. One can obtain higher eigen-
vectors in the same way, provided to keep the trial vector
orthogonal to the lower eigenvectors. In practical calcu-
lations, the CG method is stable and reasonably eﬃcient
in many cases, and it is easy to implement. The itera-
tion procedure needs only to store the trial vector and its
gradient, as well as one previous conjugated gradient.

−

Conjugated gradient method is originally designed to
minimize positive deﬁnite quadratic functions iteratively.
In n-th step of iteration, The CG method is equivalent to
ﬁnd a minimum in a n-dimensional subspace spanned by
the initial trial vector and the subsequent n
1 gradients
of the quadratic function. Due to special properties of
a quadratic function, one needs only do the minimiza-
tion in a two dimensional space spanned by current state
and the conjugated gradient which is a combination of
current gradient and last step’s conjugated gradient. In
principle, one needs at most N steps to obtain ﬁnal so-
lution in a N dimensional space. Practical calculations
usually needs more steps due to round oﬀ errors. The
conjugated gradient method is virtually the most eﬀec-
tive method to minimize a quadratic function iteratively.
And it is a formally established algorithm to solve the
linear algebraic equation.

For general functions, such as Rayleigh quotient, there
are several ways to deﬁne the conjugated gradient, and
the behaviors of conjugated gradient algorithm are un-
clear. However, near an exact minimum point, any func-
tion behaves like a quadratic function. If one starts with
a good guess, one may ﬁnd solution very quickly. This
partially explains the successes of the conjugated gradi-
ents method in diagonalising a large matrix.

where S is the overlap matrix, and
is a reﬁned trial
vector at step n. Each iteration step is to search the mini-

φn
|

i

II. THE MODIFIED CONJUGATED
ALGORITHM

∗E-mail: qljie@whu.edu.cn

Our method is based on the following two observations:
Firstly, each iteration step of minimizing the Rayleigh

Fn
|

quotient by CG algorithm is equivalent to ﬁnd lowest
eigenvector in a two dimensional subspace. The subspace
, and the
at n-th step is spanned by the current state
i
conjugated gradient
. Note that the conjugated gra-
Fn
i
|
is a combination of the gradient of n-th step’s
dient
Rayleigh quotient, and the (n
1)-th step’s conjugated
. One may expect a better result at n-th
gradient
i
iterative step by ﬁnding lowest eigenvector in a three di-
mensional subspace spanned by
,
φn
i
|
where
is the gradient of the Rayleigh quotient at
Gn
|
n-th step.

, and
i

Fn−1
|

Fn−1
|

Gn
|

φn
|

,
i

−

i

i

i

,
i

and

φn
|

Gn
|

φn−1
|

Secondly, we note that, within the CG algorithm,

φn
i
|
. Thus the three
Fn−1
is a combination of
i
|
Fn−1
, and
Gn
dimensional subspace spanned by
,
φn
|
i
i
|
i
|
is the same as the subspace spanned by
, and
Gn
,
φn
i
|
i
|
. This means that one may obtain a better result
φn−1
i
|
at n-th step by replace the n-step iteration of CG
φn+1
|
i
algorithm with ﬁnding lowest eigenvector at the three
dimensional subspace spanned by
.
i
Of course, the result will further improved if one ﬁnds the
lowest eigenvector in a larger subspace spanned by
,
i
Gn
|

The above observations indicate that one may improve
the eﬃciency of the CG algorithm by replacing each iter-
ation step of the CG algorithm with ﬁnding lowest eigen-
vector in a small subspace spanned by the current vec-
tor
, as well as
Gn
|
i
some previous vectors
. In our numeric
,
i
tests, the eﬀect is signiﬁcant in many cases. Since diago-
nalising a small matrix of several dimension is very cheap
numerically, each step’s numeric cost of the modiﬁed ver-
sion is about the same as the original CG algorithm.

and the corresponding gradient
φn−1
|

φn−m+2
|

, and
i

φn−1
|

φn−1
|

φn−2
|

φn
|

φn
|

· · ·

· · ·

.
i

,
i

,
i

,
i

i

,

Practical implementation of the modiﬁed conjugated
gradient method is similar to that of original CG method.
For ﬁnding one single lowest eigenvalue and its corre-
sponding eigenvector, it goes through the following steps:

1. Choose the dimension M of the iteration subspace,
and the maximum iteration step Nmax.
In our
numerical test, it is enough to set the dimension
M
10. In many case, M = 3 works quite well.
In this case, the 3 dimensional subspace is spanned
, the corresponding gra-
φn
by current trial vector
i
|
dient
ob-
and one previous trial vector
tained in the last step.

φn−1
|

Gn
|

≤

i

i

2. Choose an initial normalized trial vector

φ0
h
(Rayleigh quotient) E0 =

φ0
|

S
|

i

φ0
|

,
i
= 1; and calculate the expectation value
H
|

.
i
, Nmax, do the following itera-
,
i

φ0
h

φ0
|

φ1
|

φ0
|

· · ·

to

i

tion loop to reﬁne the trial vector from
φ2
|

· · ·

,
i

:

3. For n = 0, 1, 2,

(a) Calculate the gradient of the Rayleigh quo-

tient

i

= H

Gn
|

.
φn
i
|
Here the reﬁned trial function
φn
|
ized at the end of each iteration.

φn
|

i −

En

i

(3)

is normal-

i

,
i

,
i

ψ3
|

φn
|

φn−1
|

=
Gn
|
=

(b) In the m dimensional subspace spanned by
ψ2
,
=
=
i
|
· · ·
, calculate the matrix ele-
φn−m+2
i
|
, and
ψi
i
h
ij =
S
. Here, the dimension m = n + 1 if
ψj
i
|
M , otherwise m = M , i.e., in the ﬁrst
≤
1 loops, the subspace has only n + 1 basis

ψ1
i
|
ψm
|
i
ments of the matrix H,
ψj
|
the overlap matrix of the basis vector
S
ψi
h
|
n + 1
M
−
vector.

H
|

ij =

,
i

H

(c) ﬁnd the lowest eigenvalue ǫ and eigenvector ϕ
for the general form eigenvalue problem

(d) From the above eigenvector ϕ, construct the

ϕ = ǫ

ϕ.

S

H

reﬁned trial vector

φn+1
|
m

,
i

φn+1
|

i

=

ϕi

ψi
|

,
i

X
i=1

2

(4)

(5)

and calculate the expectation value En+1 =
φn+1
h
(e) If

is less than a required value or
n > Nmax, stop the iteration loop, otherwise
continue the iteration loop.

H
|
En+1
|

φn+1
|
En

.
i

−

|

Impose a maximum iteration step is necessary in many
cases. For example, in self-consistent calculations, one
needs to update the Hamiltonian after some steps of it-
erations. The trial vector can be chosen, in principle,
arbitrarily, provided it is not orthogonal with the lowest
eigenvector. However, even if the initial trial vector does
accidently orthogonal to the lowest eigenvector, due to
the numeric round oﬀ errors in the iterations, one can
always arrive the lowest eigenvector.

Check the convergence is usually testing the diﬀerence
between the trial vector and its reﬁned version after an
iteration. In our numeric tests, check the diﬀerence be-
tween two consecutive trial vectors’ Rayleigh quotients
also works well. And it is numerically faster.

H

For large matrices, calculation of the gradient is a main
numeric task in each loop of iteration. It involves a mul-
tiplication of matrix and vector. Other numeric costs are
ij and
mainly the calculation of the matrix elements
ij in the small subspace , as well as the combination of
S
the gradient and previous trial vectors to form a reﬁned
trial vector. The numeric cost of diagonalising the small
matrix
is almost nothing as compared with other op-
erations. In each loop of iteration, the subspace changes
replaces
two basis vectors, i.e., the current gradient
the previous one
φn
Gn−1
i
|
|
. One needs only to calcu-
replace the old one
i
late the matrices elements
ij related to the two
ij and
H
vectors in each iteration loop. If the subspace is three
dimensional, the numerical cost of one iteration loop is
about the same as that of original CG method.

, and the reﬁned trial vector
i

φn−m+2
|

Gn
|

H

S

i

After ﬁnding the lowest eigenvector, one can ﬁnd the
second lowest one in a similar way. One starts with a

trial vector orthogonal to the lowest eigenvector, and in
following iterations, gradients of the Rayleigh quotient,
as well as the updated trial vectors, must be kept orthog-
onal to the lowest eigenvector. Similarly, after working
out k lowest eigenvectors, the k + 1 eigenvector can be
worked out by maintaining the orthogonality with k lower
eigenvectors.

In this strict sequential procedure, the accuracy of
lower eigenvectors aﬀect the higher ones. A remedy to
this problem, according to Ref. [2], is re-diagonalising the
matrix in the subspace spanned by the reﬁned trial vec-
tors, which is referred as subspace rotation in [2]. After
this subspace rotation, one can use these resulted vec-
tor as trial vectors for further iteration to improve the
accuracy. In practical implementations, we only iterate
every trial vector for some steps, then perform a subspace
rotation. The convergence check is to test the eigenval-
ues diﬀerences between two consecutive subspace rota-
tion. This procedure improves the overall eﬃciency. In
Ref. [2], there is a detailed discussion on the role of the
subspace rotation.

III. NUMERICAL RESULTS

We test the eﬃciency of the above outlined algorithm
by comparing its performance with other algorithms for
various matrices. In all cases, the modiﬁed CG algorithm
outperforms the original CG algorithm. We observe sig-
niﬁcant improvement to the convergence rate in many
cases.

−

As an illustration, we show in ﬁgure 1 a typical result
for a banded matrix with bandwidth 2L. The matrix’s
diagonal element is aii = 2√i
a, and its oﬀ-diagonal
elements within the band width is a constant aij = a.
Due to its simple form and its relation with Hamiltonian
describing the pairing eﬀects, this matrix has been in-
vestigated by some other authors, see, e.g. [4]. Here we
choose the matrix’s dimension N = 200000 with half-
bandwidth L = 300, the parameter a is set to be 20.
For ﬁnding ﬁrst 8 lowest eigenvectors, the modiﬁed CG
algorithm converges within 100 steps with an accuracy
of machine’s precision limit. It is more than three times
faster than the original CG algorithm. As a comparison,
we also show the result for the block Lanczos method [7],
as well as the steepest decent method. In Figure 1, the
convergence rate of one iteration step is deﬁned as the
relative error of the two consecutive Rayleigh quotients,
(En
En−1)/[(En + En−1)/2], where En−1 and En are
two consecutive Rayleigh quotients. When every eigen-
value reaches the required accuracy, we perform a sub-
space rotation and repeat the iteration. Convergence is to
test the corresponding relative error for every eigenvalue
between two consecutive rotations. In our implementa-
tion, the maximum iteration number Nmax = 500, i.e.,
we go at most 500 steps of iteration for each trial vector
before performing a subspace rotation.

−

In the above calculations, we use a 3-dimensional iter-

 

 Generalized CG

 CG

 Steepest descent

 Block Laczos

3

 

t

e
a
r
 
e
c
n
e
g
r
e
v
n
o
C

2

10

0

10

-2

10

-4

10

-6

10

-8

10

-10

10

-12

10

-14

10

0

50

100

150

200

250

300

350

400

450

500

Iteration numbers

FIG. 1: Convergence rate of the modiﬁed conjugated gradient
method in comparing with other algorithms.

ation subspace for the modiﬁed CG algorithm, i.e., the
subspace is constitute of the current trial vector, its cor-
responding gradient, as well as one previous trial vector.
In such case, each iteration step needs to calculate one
gradient, and some combinations of the three vectors,
as well as solving a 3-dimensional eigenvalue problem.
From the above argument, when the iteration subspace
is 3-dimensional, the numeric cost of each iteration step
is almost the same as that of the original CG method.
For the block Lanczos algorithm, however, to ensure a
reasonable convergence rate, the iteration subspace is 50
dimensional, i.e., one needs to calculate 50 gradients for
each iteration step. To our experience, on step of Lanc-
zos iteration needs longer CPU time than 50 steps of the
modiﬁed CG method. Thus, one Lanczos step is counted
as 50 steps in Figure 1.

i}

,
i

,
i

φn
|

Gn
|

φn−1
|

φn−1
|

the gradient vector

In the 3-dimensional iteration subspace spanned by
, together
Gn
i
{|
, play the same role
with the previous trial vector
i
as that of the conjugate gradient in the minimization of a
quadratic function. This is especially the case when the
Rayleigh quotient closes to the minimum point, i.e., it is
approximately a quadratic function of the iteration trial
,
vector. In fact, without the previous trial vector
i
the lowest eigenvector obtained in the 2-dimensional sub-
space spanned by
is just the result of steep-
i}
est descent method. By including one previous trial vec-
tor which contains information about previous gradients,
one is able to prevent reintroduction of errors to the re-
ﬁned trial vector in the direction of previous gradients.
This is the reason we call this method as modiﬁed CG
algorithm.

φn−1
|

φn
|

Gn

,
i

{|

On the other hand,

in the sense of relaxation al-
gorithm for ﬁnding lowest eigenvector [4, 9], the re-
ﬁned trial vector
is an approxi-
at
mation to the lowest eigenvector of the matrix in
,
,
G1
,
φ0
the subspace spanned by
|
{|
i}
i
i
subspace spanned by
to the
which is
. According to the
φn
,
φn−1
φ1
,
{|
|
i
|
|
i
relaxation algorithm, to ﬁnd the lowest eigenvector in

equivalent
,
i

step n,

Gn
|

G2
|

φn
|

φ2
|

· · ·

· · ·

,
i

,
i

φ0

i}

i

,

,

ψ0
|

the subspace spanned by the above basis vectors, one
starts from an initial trial vector
, and minimizes
i
the Rayleigh quotient iteratively. Each iterative step is
to minimize the Rayleigh quotient in a two dimensional
subspace spanned by the (updated) trial vector, and one
basis vector. The basis vector can be chosen consecu-
tively from the ﬁrst one to the last one. After going
through all basis vectors, one continues the next round
of iteration by choosing the ﬁrst basis vector as next ba-
sis vector. This iteration will converge after goes through
all basis vectors several rounds. Note that, if one starts
as initial trial vector, in
φ0
with the ﬁrst basis vector
|
the two dimensional subspace spanned by two consecu-
tive basis vector
, the second basis vec-
i
tor
minimizes the Rayleigh quotient. After going
i
through all basis vector for one round, the reﬁned trial
vector is
, which represents an approximate lowest
i
eigenvector in the above subspace.

, and
i

φi+1
|

φi+1
|

φn
|

φi
|

i

The above two factors explain the rapid convergence of
the modiﬁed CG algorithm. One consequence from the
above arguments is that, if we increase the dimension of
the iteration subspace by including more previous trial
vectors, the convergence rate will not increase too much.
In other words, one needs only do the modiﬁed CG algo-
rithm in a small iteration space. To our experience, one
needs at most 5-dimensional iteration subspace. In most
cases, it is enough to do the iteration in the 3-dimensional
iteration subspace. Figure 2 shows our numeric result
to conﬁrm this property of the modiﬁed CG algorithm.
Here we do the same calculation using diﬀerent iteration
subspace. The ﬁlled circle connected line is the same as
ﬁgure 1 with 3-dimensional iteration subspace, and the
ﬁlled square and triangle are results for 6 dimensional and
12 dimensional iteration subspace respectively. There is
almost no diﬀerence within 50 steps where the the con-
vergence rate is about 10−8. One needs almost the same
iteration steps to arrive the ﬁnal precision. However, the
3-dimensional iteration runs faster for each iteration step
since it involves less combination and production of the
basis vectors that span the iteration subspace.

 

 M=3

 M=6

 M=12

 

t

e
a
r
 
e
c
n
e
g
r
e
v
n
o
C

0

10

-2

10

-4

10

-6

10

-8

10

-10

10

-12

10

-14

10

0

20

40

60

80

100

120

Iteration number

FIG. 2: Convergence rates of the modiﬁed CG algorithm for
diﬀerent dimensions of the iteration subspace.

4

,

i

{|

i}

φ0

,
i

,
i

· · ·

φ1
|

φn
|

φn
|

Gn
|

φn−1
|

For some matrices or some properly chosen initial
trial vectors, the Rayleigh quotient are approximately
In such cases,
quadratic functions of the trial vectors.
the modiﬁed CG algorithm converges in almost the same
rate as the original CG algorithm. And a trial vector
φn at step n, is an almost exact minimum in the sub-
. We have
,
space spanned by
i
encountered such cases in our numeric tests. In fact, near
a minimum, any function behaves like a quadratic func-
tion. Some matrices with special structures also make
the Rayleigh quotient like a quadratic function in a quite
large region of the vector space. For such matrices, the
CG method is indeed a very eﬃcient method. Of cause,
in any cases the modiﬁed CG method always outperforms
the original CG method.
The reﬁned trial vector

becomes closer and closer
to the previous step’s trial vector
when iteration
closes to ﬁnal solution. In higher dimensional iteration,
one may encounter (numerical) degeneracy of basis vec-
tors that span the iteration subspace. This problem is
easy to solve. One simple solution is to replace this step
by an steepest descent’s step. Other more sophisticated
way is to choose some independent vectors from the ba-
sis vectors and do this step in a small subspace. Both
methods are easy to implement. In fact, one can detect
the degeneracy when solving the general form eigenvalue
problem (4) which can be conveniently solved by the con-
ventional Choleski-Householder procedure [12]. If there
is a degeneracy, the Choleski decomposition of the over-
lap matrix
returns an error code. When this happens,
one can simply redo this step with a steepest descent step.
Alternatively, one can use a more sophisticated Choleski
decomposition program that automatically chooses inde-
pendent basis vector. In doing so, one must adjust the
the matrix element of
simultaneously. This two meth-
ods need almost the same numerical cost. Of course,
the ﬁrst method is easy to implement. In our numerical
tests, there is almost no degeneracy in the 3-dimensional
iteration subspace.

H

S

i

It is straightforward to implement preconditioning
treatment for the modiﬁed CG algorithm. Precondition-
ing treatment can signiﬁcantly improve the convergence
rate for matrices with large diﬀerence between lowest
and highest eigenvalues. Due to the fact that there is
no need to construct explicitly the conjugated gradient
in the modiﬁed CG algorithm, it is easier to implement
the preconditioning treatment by direct modifying each
step’s gradient. Since preconditioning treatment depends
on speciﬁc system, we don’t go into more details about
such topic.

The modiﬁed CG algorithm shares a common feature
with many other iterative methods of diagonalising ma-
trices, such as Lanczos, Dividson, RMM-DIIS, and re-
laxation method. In all these algorithms, one reﬁnes the
trial vector in iterative subspaces. What makes the mod-
iﬁed CG algorithm diﬀerent from other algorithms is that
the iteration subspaces are spanned by the trial vectors of
previous iteration steps, as well as the latest trial vector

and its gradient. The trial vectors of previous steps are
already prepared, one needs only calculating one gradi-
ent vector (and possibly does some preconditioning treat-
ment) to construct the basis vectors of the iterative sub-
space. Only two basis vectors of the iterative subspace
are diﬀerent from previous one, it needs only update two
columns of the matrix elements in the iteration subspace.
By including previous trial vectors into the iterative sub-
space, one avoids reintroduces errors to the trial vectors
in the previous directions of gradients. These properties
of the iterative subspace make the modiﬁed CG algo-
rithm numeric eﬃcient. And the common feature of the
algorithm makes it easy to implement.

It is easy to formulate block algorithm for the modiﬁed
CG algorithm to ﬁnd several lowest eigenvectors simulta-
neously. For this end, one reﬁnes several trial vectors at
each iteration step. Here the iteration subspace includes
all current trial vectors, their gradients, and all trial vec-
tors of some previous steps. In this implementation, one
needs to ﬁnd several eigenvectors by solving the general
form eigenvalue problem (4). Trial vectors obtained in
this way are automatically orthogonal with each other,
and one needs no additional subspace rotation.

.

≤

H

· · ·

However, one step of block algorithm usually needs
more ﬂoating point operations than sequentially process-
ing each trial vector and maintaining orthogonality be-
tween trial vectors by Schmidt orthogonalization method.
This is mainly because the block algorithm needs more
and the cor-
ﬂops to form the matrix elements of
responding overlap matrix
If one needs n0 lowest
S
eigen-solutions for N dimensional matrix, the block al-
gorithm’s iterative subspace is M = mn0 dimensional
. Each step of block algorithm needs
with m = 3, 4,
the following ﬂoating point operations: (a) n0N L ﬂops
for n0 matrix multiplying vector operations to obtain n0
gradients, where L
N is the band width of the ma-
trix; (b) 2(mn0)2N ﬂops for the formation of the matrix
in the iterative subspace and the corre-
elements of
H
; (c) An O (cid:0)(mn0)3
sponding overlap matrix
(cid:1) ﬂoating
point operations for solving the general form eigen-value
problem (4); (d) 2mn2
0N ﬂops for combination the mn0
basis vectors to form n0 reﬁned trial vectors. Here, the
ﬂops in step (c) is negligible when n0 << N . The to-
tal ﬂops of one step block algorithm is σ(m, n0, N ) =
n0N L + 2(mn0)2N + 2mn2
0N . If n0 = 1, the above ﬂoat-
ing point operations σ(m, 1, N ) = N L + 2m2N + 2mN
is the ﬂops for processing one trial vector in sequential
algorithm. One the other hand, sequentially processing
each trial vector one round needs n0σ(m, 1, N ) + 4n2
0N
ﬂops. Here the second term is the ﬂops to maintain the
orthogonality of trial vectors, including making gradi-
ents orthogonal to previous trial vectors. Even including
subspace rotation which is performed after some rounds
of sequential steps, the sequential implementation needs
less ﬂoating point operations than the block algorithm.
If n0 is small, e.g., n0 < 10, the diﬀerence of ﬂops be-
tween block and sequential algorithm is small. The block
algorithm may be one choice in such cases. Like the block

S

5

Lanczoz [7], and block Dividson [8], there are some other
ways to form the iterative subspace to implement the
block version of modiﬁed CG algorithm. For example,
the iterative subspace may contain only one gradient,
plus all the current trial vectors and some previous trial
vectors. The choice of iterative subspace aﬀects the con-
vergence properties which needs further investigations.
For large n0, e.g., n0 > 100, to our experiences, block
algorithm need more numeric cost and is less eﬃcient
as compared with the above sequential implementation.
The dimension of the iteration subspace grows quickly
with the number of needed eigenvectors, and one needs
more memory to store the basis vectors and much more
CPU time to solve the general from eigenvalue problem
(4) which increases drastically with the dimension of the
iterative subspace. Since lowest eigenvector usually con-
verges faster that higher ones, the number of iteration
steps in a block algorithm is determined by the the vec-
tor with slowest convergence rate.

IV. CONCLUSIONS

In summary, in the sense of conjugated gradient al-
gorithm, we formulate an iterative method to ﬁnd a set
of lowest eigenvalues and eigenvectors of a matrix. This
method minimizes the Rayleigh quotient of a trial vector
via the gradient of the Rayleigh quotient, and at the same
time, prevents reintroduce errors in the direction of pre-
vious gradients. We realize such idea by reﬁning the trial
vectors in a special kind of iteration subspaces. Each it-
eration subspace is spanned by the latest trial vector and
the gradient of its Rayleigh quotient, as well as some trial
vectors of previous steps. Each iteration step is to ﬁnd
lowest eigenvector in the iteration subspace. The gradi-
ent, together with the previous trial vector, play the role
of the conventional conjugated gradient. In our numeri-
cal test, it is usual enough to include only one previous
trial vector, i.e., one needs only reﬁning the trail vector
in a 3-dimensional subspace. As compared to the conven-
tional conjugated gradient algorithm, which is designed
to minimizes a general function, the current method ex-
ploits special properties of eigenvalue problems, and thus
converges much faster in many cases. During iterations,
the trial vector at the step n, is an approximately lowest
eigenvector in the subspace spanned by the initial trial
vector and n subsequent gradient vectors. This is the
reason of rapid convergence rate. The easy implementa-
tion of this algorithm makes it suitable for ﬁrst principle
calculations.

This work is supported in part by the National Natural
Science Foundation, the Research Fund of the State Ed-
ucation Ministry of China, and the Research Fund of the
Wuhan University. We thanks helpful discussions with
Prof. W. Wang.

6

[1] M. C. Payne, M. P. Teter, D. C. Allan, T. A. Arias, and
J. D. Joannopoulos, Rev. Mod. Phys., 64, 1045 (1992);
M. P. Teter, M. C. Patne, and D. C. Allan, Phys. Rev.
B 40, 12255 (1989).

[2] G. Kresse, and J. Furthm ¨mller, Phys. Rev. B 54, 11169

[3] D. M. Bylander, L. Kleinman, and S. Lee, Phys. rev. B

(1996).

42, 1394 (1990).

35, L61, (2002).

(1993).

[6] N. Wijesekera,

G.
arXiv:cond-mat/0304374.

[7] J. K. Cullum, R. A. Willoughby,Lanczos Algorithms for
Large Symmetric Eigenvalue Computations, (Birkh¨auser
Boston, Inc, 1985).

[8] E. R. Davidson, J. Computat. Phys. 17, 87 (1975); B.
Liu, in Report on the Workshop Numerical Algorithms in
Chemistry: Algebraic Methods, edited by C. Moler and I.
Shavitt (University of California, Berkley, 1978), p. 49.
[9] I. Shavitt, C. F. Bender, A. Pipano, and R. P. Hosteny,

[10] P. Pulay, Chem. Phys. Lett. 73, 393 (1980).
[11] D. M. Wood and Z. Zunger, J. Phys. A 18, 1343 (1985).
[12] G. H. Golub, and C. F. Van Loan, Matrix Computation,

[4] F. Andreozzi, A. Porrino, and N. L. Iudice, J. Phys. A

J. Comput. Phys. 11, 90 (1973).

[5] H. Q. Lin, J. E. Gubernatis, Computers in Phys. 7, 400

Feng,

T.

L.

Beck,

(The Johns Hopkins University Press, 1996).


8
9
9
1
 
n
u
J
 
2
2
 
 
]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[
 
 
1
v
4
3
0
6
0
8
9
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The Hopgrid algorithm: multilevel synthesis of multigrid and
wavelet theory

D. Ye¸silleten and T. A. Arias
Department of Physics
Massachusetts Institute of Technology
Cambridge, Massachusetts 02139

August 8, 1996

Abstract

The multigrid algorithm is a multilevel approach to
accelerate the numerical solution of discretized dif-
ferential equations in physical problems involving
long-range interactions. Multiresolution analysis of
wavelet theory provides an eﬃcient representation
of functions which exhibit localized bursts of short
length-scale behavior. Applications such as comput-
ing the electrostatic ﬁeld in and around a molecule
should beneﬁt from both approaches. In this work,
we demonstrate how a novel interpolating wavelet
transform, which in itself is the synthesis of ﬁnite
element analysis and wavelet theory, may be used
as the mathematical bridge to connect the two ap-
proaches. The result is a specialized multigrid algo-
rithm which may be applied to problems expressed
in wavelet bases. With this approach, interpolation
and restriction operators and grids for the multigrid
algorithm are predetermined by an interpolating mul-
tiresolution analysis. We will present the new method
and contrast its eﬃciency with standard wavelet and
multigrid approaches.

1

Introduction

Many applications such as electronic structure calcu-
lations require an eﬃcient method to solve discretized
linear diﬀerential equations which are generators of
long range interactions, such as Poisson’s equation.
In modern electronic structure calculations, except
those using planewaves, solving Poisson’s equation
takes up a signiﬁcant amount of computational time.
It is essential to produce an algorithm that has the
capacity to solve eﬃciently problems involving such
long range interactions.

The multigrid algorithm is the state of the art in

solving discretized linear diﬀerential equations de-
It uses a basic it-
scribing long range interactions.
erative method, such as the weighted Jacobi method,
over a sequence of scales to iterate faster toward the
exact solution.
In some cases, even multigrid does
not produce suﬃciently rapid convergence, particu-
larly in calculations using wavelet bases.

Multiresolution analysis, from wavelet theory, pro-
vides the ability to carry out calculations on non-
uniform grids,
focusing resolution only in regions
where it is needed. It is especially useful if the local
region which requires high resolution moves through
space during the calculation, as when the atoms move
in a molecule in an electronic structure calculation.
However, a basis set alone does not provide compu-
tational eﬃciency.

Often, a ﬁnite element basis set provides the ana-
lytic framework for multigrid calculations. Combin-
ing the advantages of ﬁnite element analysis and the
multigrid algorithm with wavelet theory, could pro-
vide a very powerful method to solve discretized lin-
ear diﬀerential equations. This present research aims
to unify these approaches for use in a wide range of
applications, especially in electronic structure calcu-
lations, where all of the aforementioned approaches
prove to be very beneﬁcial in various stages of the
calculations individually.

A basis set that combines wavelet theory with ﬁ-
nite element analysis exists and is known as the in-
terpolet basis (Interpolets is based upon Deslauriers
and Dubuc scaling functions [4], which play the role
of both scaling functions and interpolets in the in-
terpolet basis. See also [5]). The present research
forms a natural bridge in the synthesis of the multi-
grid algorithm with this basis, thereby adding the
power of multiresolution analysis to the multigrid al-
gorithm. In particular, we introduce this new Hop-
grid algorithm for use in problems where an under-

1

lying wavelet or interpolet basis is necessitated by
the problem at hand and application of multigrid al-
gorithm could provide very useful in solving linear
diﬀerential equations in the problem.

matrix composed of the diagonal elements of A, and
r(n) is the residual at step n, deﬁned as r(n) = f −
Av(n). We refer to application of this recursion as
relaxation.

In Section 2 of the paper, to establish a common
notation, we give a brief overview of the traditional
multigrid algorithm and the iterative methods that
form the framework for this algorithm. Section 3 in-
troduces the interpolet theory. Section 4 includes the
description of non-orthogonal multiresolution analy-
sis from wavelet theory in the interpolet basis. Sec-
tion 5 presents the basic ideas behind expressing the
multigrid algorithm in the interpolet basis, and de-
scribes how such a union becomes more eﬃcient by
using multiresolution analysis. Section 6 introduces
the new algorithm formed by a more impact synthesis
of the multigrid algorithm and the interpolet basis,
the Hopgrid Algorithm. The last section expresses
the results obtained using this new algorithm. The
appendix of this paper aims to familiarize the reader
with the essentials of the multigrid algorithm with
a thorough discussion of the basic iterative methods
and the framework of this algorithm.

2 Multigrid Algorithm

This section brieﬂy summarizes the multigrid algo-
rithm [2] and the interpolation and restriction oper-
ators used in this algorithm. A more detailed de-
scription of the multigrid algorithm and these opera-
tors, aimed for the physicist, may be found in Section
9. Throughout this section, superscripts are used to
refer to the scale on which a particular level of the
problem is solved.

Multigrid exploits this basic iterative method in a
multiscale fashion through the following procedure.
First, α relaxations are performed on the scale on
which the problem is deﬁned. Then, the residual
r(0) is transfered up to a spatially coarser scale, us-
ing a linear restriction operator, r(1) = R(1)
(0)r(0). On
this coarser scale, one then solves the error equation
A(1)e(1) = r(1), relaxing again α times. (Note that,
as discussed in the appendix, the exact solution to
the error equation, when summed with the current
iterative solution v, yields the exact solution to the
problem u.) The procedure of passing up the residual
to successively coarser scales continues until a prede-
termined coarsest scale n is reached. At this coarsest
scale n, one then transfers the solution vector v(n)
down to the next ﬁner scale n− 1 using a linear inter-
polation operator I (n−1)
. The error equation is then
relaxed on this scale β times, using v(n−1)+I (n−1)
v(n)
as the initial guess. This procedure is followed down
to the ﬁnest scale, relaxing β times at each scale i
with the initial guess v(i) + I (i)
(i+1)v(i+1), where ﬁnally
one relaxes β times on the original linear equation to
obtain an approximate solution to the overall prob-
lem.

(n)

(n)

The procedure described above is the V-cycle of
the multigrid algorithm. V-cycles may be repeated in
succession to produce a solution of any desired accu-
racy.

2.2 Theory for Interpolation and Re-

2.1 Full V-cycle Multigrid Algorithm

striction Operators

The multigrid algorithm is used to solve discretized
linear diﬀerential equations of the form,

Au = f,

The interpolation and restriction operators should
obey two conditions, known as the variational prop-
erties. The ﬁrst condition, the Galerkin condition,
requires that

(1)

where A is the linear operator, u is the solution vec-
tor, and f is the source vector. Multigrid is based
upon a basic iterative method, which is then applied
over a sequence of scales in order to improve the con-
vergence rate. One such iterative method used for
this purpose is the weighted Jacobi method, which
provides the following recursion,

v(n+1) = v(n) + wD

−1r(n).

(2)

Here, w is an appropriate weight (usually chosen to be
w = 2/3), v(n) is the initial guess, D is the diagonal

A(n+1) = R(n+1)

(n) A(n)I (n)

(n+1).

It is a recipe for the linear operator to be used in
relaxations on the next coarser scale. The second
condition states that

(3)

(4)

(n) = c(I (n)
R(n+1)

(n+1))T ,

where c is a scalar constant. This is the recipe for the
restriction operator, up to an overall scalar constant,
given the interpolation operator.

2

3

Interpolet Theory

In this section, we brieﬂy review interpolet theory
based on the discussion in Lippert, Arias and Edel-
man [1].

In interpolet theory, functions varying slowly over
integer length scales can be closely approximated as
linear combinations of interpolating functions,

f (x) =

fnI(x − n),

(5)

Xn

where the fn are the expansion coeﬃcients and the
I(x − n) are functions with compact support, also
known as interpolets.

Interpolets are a basis set combining wavelet the-
ory with ﬁnite element analysis. In place of the or-
thonormality condition common in wavelet theory,
interpolets have cardinality and interpolation from
ﬁnite element analysis as their characteristic proper-
ties. We will ﬁrst discuss the properties which in-
terpolets share with ﬁnite elements and then we will
describe those properties which they share with tra-
ditional wavelets.

Cardinality means that the values of a function are

zero at all integers except for zero,

I(n) = δn0,

for all integers n.

(6)

As a consequence of this condition, the function
formed by the linear expansion in Eq. (5) will match
exactly the value of the original continuous function
at the integer grid points when the expansion coeﬃ-
cients are taken to be the values of f (x) at the inte-
gers, fn ≡ f (n),

f (n) =

fmI(n − m) =

fmδnm = fn.

(7)

Xm

Xm

Interpolation is the further condition that Eq.
reproduces any polynomial, up to order L for all x,

(5)

xk =

nkIL(x − k), k = 0, 1, ..., L,

(8)

Xn

where subscript L denotes the order of interpolation.
Given the values of a function at the integers, inter-
polets then provide a compact estimate correct to Lth
order of the values of the function at the half integers
through

f (

) =

fmIL(

− m).

(9)

x
2

Xm

x
2

The matrix representations for the interpolation op-
erators deﬁned by this relation for both ﬁrst order in-
terpolets (L = 1) and third order interpolets (L = 3)

are given below. Note that the L = 1 case gives the
familiar linear interpolation procedure. and that the
L = 3 case is distinct from the interpolation given
by traditional cubic B−splines.
Interpolet theory
thereby provides guidance in selecting natural inter-
polation operators for use in the multigrid algorithm.

. . .

1
2
1
1
2

1
2
1
1
2

IL=1 =






















. . .

,























1
2
1
1
2

1
2
1
1
2

. . .

































− 1
16
0
16 − 1
9
16
0
1
9
16 − 1
9
16
16
0
1
0
16 − 1
− 1
9
9
16
16
16
0
1
0
9
9
− 1
16
16
16
1
0
9
− 1
16
16
0
− 1
16

































. . .

IL=3 =

In addition to properties from ﬁnite element analy-
sis, interpolets satisfy the central condition of wavelet
theory, the two-scale relation. This relation states
that every coarse scale interpolet is expressible as a
linear combination of ﬁner scale interpolets. Interpo-
lets therefore interpolate themselves exactly,

IL(x) =

cnIL(2x − n).

(10)

Xn

Here, the IL(x) is the coarse scale interpolet, the
IL(2x−n) are ﬁne scale interpolets, and the cn are the
expansion coeﬃcients. (Note that from the cardinal-
ity property, we have that the cn are just the values
of the interpolet at the half-integers cn = IL(n/2).)
The signiﬁcance of this condition is to ensure that a
basis made from interpolets of varying scales always

3

x
l=1
l=3

-2

0

- 3
2

- 1
16

-1
0
0

- 1
2
1
2
9
16

0
1
1

1
2
1
2
9
16

1
0
0

3
2

- 1
16

2

0

Table I: Values of interpolets of orders l = 1, 3 at var-
ious points x within their compact support. Values
not shown are zero.

a)

b)

c)

d)

Basis spanning the one dimensional space (Vo), whose resolution we want to double.

Basis with doubled spatial resolution spanning the space (V1), direct representation.

Basis with doubled spatial resolution spanning the space  (Vo + Wo), MRA representation.

O

O

O

O

O

O

O

Basis with resolution improved by a factor of 2   spanning a one dimensional space

2

Figure 1: This ﬁgure illustrates pictorially how an
MRA basis is created. The bigger circles represent
the coarse scale interpolets, while the smaller circles
are for the ﬁne scale interpolets.

provides a very uniform description of space. This
point is described in detail the next section.

The self-interpolation property may be applied re-
cursively to express an interpolet in terms of interpo-
lets on arbitrary ﬁner scales,

IL(x) =

...

cncm ... ck

(11)

Xn Xm
× IL(2ax − 2a−1n − 2a−2m ... − k)

Xk

=

IL(k/2a)IL(2ax − k).

Xk

where a is a positive integer that determines the ﬁne
scale on which the original interpolet is expanded.
This relation gives a procedure for determining the
values of any interpolet recursively once given the
values of the cn. Table I presents these expansion
coeﬃcients from which the interpolets may be con-
structed through the two-scale relation.

4 Non-orthogonal Multiresolu-

tion Analysis

In this section, we present a brief discussion of mul-
tiresolution analysis (MRA) of interpolet and wavelet

theories [1],[3], and the application of MRA to non-
uniform grids such as the ones employed in electronic
structure problems.

Assume that there is a basis spanning a space V0 as
in Figure 1a. We refer to this space V0 as the coarse
real space and this representation as the direct repre-
sentation. In this space, any function f (x), varying
slowly over the given grid spacing, can be approxi-
mated with a linear combination of interpolets of the
appropriate scale as explained in the previous section,

f (x) =

fnIL(x − n).

(12)

Xn

To double the resolution of this basis, we can double
the number of grid points and decrease the scale of
the basis functions by a factor of two, as Figure 1b
illustrates. In this new space, V1, which we refer to as
the ﬁne real space, any function f (x) varying slowly
over the new grid spacing can be approximated with
a linear combination of ﬁne scale interpolets with half
the compact support as

f (x) =

˜fnIL(2x − n).

(13)

Xn

General wavelet theory provides another tool to en-
hance the spatial resolution of the basis in Figure 1a,
locally. This tool is known as multiresolution analy-
sis (MRA). MRA states that to double the resolution
of a given basis, we may add to the existing basis a
new set of functions spanning a space W0, such that
V0 + W0 = V1. For this condition to hold, we need
to prove the two constraints that these spaces should
satisfy, namely V0+W0 ⊃ V1 and V0+W0 ⊂ V1. In or-
der for the ﬁrst condition to hold, it is suﬃcient that
the interpolets satisfy the two-scale relation, Eq. (10)
of Section 3. In order to prove the second condition,
one must show that any interpolet IL(2x − n) ∈ V1
can be written as a linear combination of interpolets
in the space V0 + W0. (This is proven for interpolets
in [1].) The new basis spanning the space V0 + W0 is
illustrated in Figure 1c. Henceforth, we will refer to
such a mixed basis space as the MRA space and such
a representation as the MRA representation.

The MRA basis can be extended to include not
only the next ﬁner scale but other scales of increas-
ing ﬁneness, as Figure 1d illustrates. Adding Wi’s
(i = 0, 1, 2, ..., N ) to V0 improves the resolution of
the original space by a factor of 2N +1,

VN = V0 + W0 + W1 + ... + Wi + ... + WN −1.

Multiresolution analysis is especially useful when
the problem requires only local regions of high res-
olution and only a small fraction of the expansion

4

coeﬃcients for the MRA basis are signiﬁcant. Un-
der these circumstances, one needs to employ only a
small subset of the full MRA basis. We shall refer
to the points in space associated with the remaining
basis functions as the non-uniform grid for the prob-
lem. In electronic structure problems, for instance,
only a small spherical region around the atomic core
requires high resolution to describe the most rapid
oscillations in the electronic wave functions. Such
problems are best addressed using non-uniform grids
where high densities of grid points are concentrated in
concentric spheres with diﬀering resolution surround-
ing the atomic nuclei. The advantage of using such
an MRA basis appears when the nuclei move. New
grids do not have to be generated, and associated
Pulley forces calculated, as the nuclei move. Rather,
high resolution MRA basis functions may be simply
turned on and then oﬀ as nuclei pass by. [1] provides
a more detailed explanation of this application.

5 Combining the Multigrid Al-
gorithm with Interpolets and
MRA

We have so far described the multigrid algorithm and
the interpolet basis. In this section, we discuss the
rationale for combining the multigrid algorithm with
interpolets. The result of Section 5.1 will be an al-
gorithm in the direct representation. We will discuss
multiresolution algorithms in Section 5.2.

5.1 Choice of Interpolation and Re-

striction Operators

Interpolets provide a natural prescription for an in-
terpolation operator to be used in the multigrid algo-
rithm. However, multigrid also requires an appropri-
ate restriction operator so that both variational prop-
erties are satisﬁed. In this section we will see that
the appropriate restriction operator is just the trans-
pose of the interpolet interpolation operator. This
comes about because of the form of linear operators
generated by applying the Galerkin technique to the
interpolet basis.

Let the following be the d-dimensional linear equa-

tion to be solved,

ρ(x) are then expanded as,

φ(x) =

˜φnIL(x − n), ρ(x) =

˜ρnIL(x − n). (15)

Xn

Xn

Substituting these expansions into the linear equation
and applying

dx IL(x − m) to both sides yields

R

Xn

A(n)
mn

˜φn =

U (n)

mn ˜ρn,

Xn

(16)

where

A(n)

mn ≡

Z

dx IL(x − m) ˆOIL(x − n)

(17)

is the interpolet form for the linear operator on scale
n, and

U (n)

mn ≡

Z

dx IL(x − m)IL(x − n).

(18)

Using the two scale relation, we may relate this
operator to the linear operator on the next coarser
scale A(n+1),

A(n+1)
mn

dx IL(x/2 − m) ˆOIL(x/2 − n)

≡

=

=

Z

Xkl

ckcl Z
ckA(n)

2m+k,2n+lcl

Xkl
L A(n)IL.
A(n+1) = IT

In the ﬁnal line, we convert the relationship between
the operators on the two scales into a matrix equa-
tion. From Eq. (19), we may construct a set of opera-
tors that automatically satisfy both variational prop-
erties, the Galerkin condition and the full weighting
condition. The appropriate set of operators are as
follows. First, the linear operators A(n) representing
ˆO will be those generated from the interpolets using
the standard Galerkin procedure. Second, the inter-
polation operators IL will be those generated from
the interpolets as discussed in Section 3. Finally, this
analysis shows that to complete the set, the restric-
tion operators must be R = IT
L .

Implementing the interpolet basis in the multigrid
algorithm is then using the linear operators A(n) to
solve Eq. (1) and the error equation, and using the
matrices IL and IT
L as the interpolation and the re-
striction operators, respectively.

ˆOφ(x) = ρ(x),

where ˆO is a linear operator and φ(x) and ρ(x) are
functions of the d-dimensional variable x. If we are
to solve this equation in an interpolet basis, φ(x) and

(14)

5.2 Synthesis of MRA into the Inter-

polet Multigrid Algorithm

In this section, we discuss how we may generalize the
algorithm that combines the interpolet basis with the

dx IL(x − 2m − k) ˆOIL(x − 2n − l)

(19)

5

)
)
r
o
r
r
e
(
s
m

r
(
0
1
g
o

l

−2

−3

−4

−5

−6

−7

−8

−9

−10
0

)
)
r
o
r
r
e
(
s
m

r
(
0
1
g
o

l

−2

−3

−4

−5

−6

−7

−8

−9

−10

−11

−12
0

n
o
i
t
c
i
r
t
s
e
R

a)

b)

c)

d)

Original basis in the MRA representation.

Direct representation for the original basis.

Direct representation of the restricted basis.

MRA representation of the restricted basis.

I
n
t
e
r
p
o
l
a
t
i
o
n

Interpolation and restriction operations
Figure 2:
carried out in the MRA representation. The arrows
indicate the direction to follow to carry out interpo-
lation and restriction. The larger (smaller) circles
represent the coarser (ﬁner) scale interpolets.

multigrid algorithm to multiresolution bases. We also
discuss brieﬂy how this multigrid algorithm in the
MRA bases may be applied to non-uniform grids in
an eﬃcient manner.

We will now discuss two approaches for carrying
out interpolation and restriction for data in the MRA
representation illustrated in Figure 2a. The ﬁrst ap-
proach is a direct approach and is equivalent to the
method explained in the previous section. To carry
out restriction in this approach, one ﬁrst changes
from the MRA representation to the direct represen-
tation (the process carrying data from Figure 2a to
Figure 2b. One then carries out the restriction op-
eration in the usual way by applying the previously
described linear operator IT
L (making the transfor-
mation Figure 2b→2c). Finally, one converts back
from the direct representation to the MRA represen-
tation (2c→2d). Interpolation within this approach
also consists of three steps. First, we change from
the MRA representation to the direct representation
(2d→2c), then we carry out interpolation with the
usual operator IL (2c→2b), and ﬁnally we convert
back from the direct representation to the MRA rep-
resentation (2b→2a). We refer to this approach of
changing from the MRA representation to the direct
representation in order to carry out the interpolation
and the restriction operations as the operator method.
The second approach is an improvement upon the
operator method. This approach uses the properties
of the MRA basis to carry out the operations of in-
terpolation and restriction entirely within the MRA
representation. We refer to this second method as the
basis method. It uses the general physical meaning of
restriction and interpolation to exploit the interpolet

6

(o):operator method

(x):basis method

1

2

3

7

8

9

10

4
6
5
# of V cycles

(o):operator method

(x):basis method

50

WU (work units)

100

150

Figure 3: The operator method compared to the basis
method. The ﬁrst ﬁgure represents the convergence
rate vs the number of V-cycles and the second ﬁgure
shows that the convergence rate of the basis method
is computationally more eﬃcient when the number
of work units are considered. The above graphs are
obtained by solving the Poisson’s equation on a grid
of 128 points, using ﬁrst order interpolets.

basis. Restriction is the elimination of the ﬁne in-
formation from a given basis and mapping it onto a
coarser one. In the interpolet basis, this ﬁne infor-
mation corresponding to the high frequency modes is
carried by the ﬁnest interpolets. Therefore, restric-
tion in this basis corresponds to the obviation of all
the coeﬃcients for the ﬁnest interpolets. Interpola-
tion maps coarse information onto a ﬁner grid.
In
the interpolet basis, the coarse information is repre-
sented by the coarse interpolets. Thus, interpolation
leaves the coarse interpolets unaﬀected, padding the
coeﬃcients for all ﬁner interpolets with zeros.

The operators in this new method satisfy the
Galerkin condition since the interpolation operator in
the MRA representation consists of a block identity
matrix and a block zero matrix for the coarse and ﬁne
informations respectively. Figure 3a illustrates that
the operator and the basis methods are equivalent
with the same convergence rates per V-cycle.

Note that the basis basis method has three distinct
advantages over the operator method. First, the ba-
sis method requires no matrix-vector multiplications
to implement interpolation and restriction. Figure
3b illustrates that the basis method is superior to the
operator method on a per ﬂop basis. Second, the ba-
sis method provides the abilities to interpolate and
to restrict over many scales in a single step. Finally,
the basis method is easily generalized to problems
on non-uniform grids. On non-uniform grids, restric-
tion is merely obviation of ﬁner scale coeﬃcients and
interpolation is padding ﬁner scale coeﬃcients with
zeros.

6 The Hopgrid Algorithm

In this section we introduce the Hopgrid algorithm,
which is the synthesis of the multigrid algorithm and
the interpolet basis in the MRA representation. We
present Hopgrid as an algorithm to solve discretized
linear diﬀerential equations on interpolet bases in
MRA representation.

The eﬃciency of using the interpolet MRA basis
in the multigrid algorithm appears when we look at
how the error behaves at each step. The error vector
at iteration step n is

e(n) = A

−1f − v(n).

(20)

Substituting the deﬁnition of the residual, r = f −
Av and Eq. (2) into Eq. (20) returns the following
expression, which states that error at each step is
multiplied by a convergence operator

e(n+1) = ˆCe(n)

(21)

7

Eigenvalues of Convergence Operator

slow convergence

medium convergence

high convergence

traditional weighted
jacobi conv. operator

conv. operator
in interpolet basis

l

e
u
a
v
n
e
g
e

i

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1
0

50

100

150

mode number

200

250

Figure 4: Eigenvalue spectrum of the convergence op-
erators for the weighted Jacobi method in the tradi-
tional multigrid basis and the interpolet MRA basis
for a problem of 256 grid points. For the eigenvalue
spectrum of the traditional weighted Jacobi operator,
w=2/3, and for the weighted Jacobi operator in the
interpolet MRA basis, w=1.

where ˆC ≡ 1 − wD−1A. The eigenvalues of this op-
erator determine how the error behaves at each iter-
ative step. Figure 4 illustrates the eigenvalue spec-
trum of the convergence operators for both the tra-
ditional weighted Jacobi method and the weighted
Jacobi method in the interpolet MRA basis.

The convergence bands shown in Figure 4 are cho-
sen for convenience to describe the eﬀects of the
eigenvalues on the error modes. The high convergence
band is deﬁned to be the region in which the choice of
w for the weighted Jacobi recursion guarantees that
about half of the eigenvalues of the convergence oper-
ator of the traditional weighted Jacobi method lie in
the vicinity of zero. The errors which are multiplied
by these eigenvalues approach zero at a high rate.
The medium convergence band and the slow conver-
gence band are the regions where the convergence of
the error to zero is slower. The distinction between
these two bands in Figure 4 is chosen for purposes of
illustration.

First, we will consider the convergence of the tradi-
tional weighted Jacobi method. The choice of weight
w in the method aﬀects the number of eigenvalues
that lie in each of the bands; however, the operator
D−1A always has eigenvalues approaching zero, and
so the convergence operator always includes eigenval-
ues near unity and modes which converge very slowly.
On the other hand, most of the eigenvalues of the

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

)

U
W
/
s
t
i

i

g
d
(
 
e

t

a
r
 
.
v
n
o
c
 
f

o

 

l

e
u
a
v
 
.
s
b
a

0.35
0

(*):hopgrid algorithm

(o):weighted Jacobi in MRA

500

1000 1500 2000 2500 3000 3500 4000

problem size (# of grid points)

Figure 5: Comparison between the weighted Jacobi
method in the MRA representation and the Hopgrid
algorithm for the solution of Poisson’s equation on
grids of several diﬀerent sizes.

convergence operator in the interpolet MRA basis are
clustered in the high convergence band. The reason
for this behavior of the eigenvalues is the fact that
the ﬂow of information on the interpolet MRA ba-
sis is stronger than the ﬂow of information in the
basis which the traditional multigrid algorithm uses.
The superiority in communication in the MRA ba-
sis comes from the fact that the MRA basis includes
functions with support spanning the entire spatial ex-
tent of the problem.

The information given in Figure 4 shows that af-
ter one weighted Jacobi relaxation in the interpolet
MRA basis, most of the error modes are eliminated.
Only the modes that correspond to the eigenvalues
at the edges of the spectrum survive after relaxation.
Numerical experiments show that these surviving er-
ror modes have low spatial frequencies and so may
be eliminated very eﬃciently, using the basic ideas
underlying the traditional multigrid algorithm.

We now exploit these observations to produce a
new algorithm. The basic outline of the algorithm
is to ﬁrst use weighted Jacobi in the MRA represen-
tation to eliminate the error modes that correspond
to the high frequency eigenvalues in the high conver-
gence band, and then to restrict the residual to a very
coarse spatial scale where the error equation is solved
exactly, eliminating the remaining error modes. The
fact that we hop over many scales in order to produce
the error vector gives this new algorithm its name,
the Hopgrid Algorithm. Below we shall refer to the
scale on which we solve the error equation as the hop

8

scale, h. Figure 5 shows the comparison between the
convergence rates in digits/WU (work unit) vs prob-
lem size for the Hopgrid algorithm and the weighted
Jacobi method in the MRA representation. As this
ﬁgure illustrates, the hopping over many scales to
calculate the error improves the convergence rate, es-
pecially with increasing problem size.

The detailed procedure for the Hopgrid algorithm
is as follows. First, one applies weighted Jacobi in
MRA representation once on the ﬁnest scale (α=1)
to ﬁnd an approximate solution vector v(0). Then, to
reduce the computational overhead of the algorithm,
instead of calculating all the entries of the residual
r(0) = f − Av(0), we calculate only the 2h entries
which correspond to the restricted residual on the
hop scale h. Furthermore, because these entries de-
pend only weakly on the high frequency components
of v(0), we compute them only with the 2h+2 entries
of v(0) corresponding to the hop scale with two levels
of reﬁnement. After hopping over many scales, the
number of grid points on h is very small compared
to the number of points on the ﬁnest scale, so that,
now, solving the error equation, A(h)e(h) = r(h) ex-
actly becomes a negligible part of the computation.
Finally, we hop again over many scales and interpo-
late the resulting e(h) back to the ﬁnest scale, again
just padding with zeros. The sum of the approximate
solution before hopping v(0) and the interpolated er-
ror from scale n gives the improved solution vector
v = v(0) + I (0)
(n)e(n). The hopping procedure may then
be applied recursively to reach any desired level of
accuracy.

Numerical experiments show that the best conver-
gence rates are obtained when the hop scale h is cho-
sen so that h ≤ N
2 , where N is deﬁned such that the
number of grid points for the original problem is 2N .

7 Results

In this section we compare the eﬃciency of the Hop-
grid algorithm with the traditional multigrid algo-
rithm. We use two diﬀerent measures of convergence
rate for comparison. The ﬁrst measure is the conven-
tional measure in terms of digits per WU [2]. Here, we
deﬁne one work unit to be the number of ﬂops it takes
to multiply a vector by the matrix representing the
linear operator in the corresponding basis. We will
see that in terms of this measure the present algo-
rithm is signﬁcantly superior. The MRA representa-
tion loses part of its advantage in problems requiring
uniform resolution, as on a uniform grid, MRA op-
erators are far denser, having fractal dimension 1.5,
than their single scale counterparts. Even in this ex-

For Amn, values for | m − n | :

0

3

4

5

analytic solution 

First order interpolet:

-2

1

1

2

0

0

Third order interpolet:

20
9

- 9
8

1
72

0

0

Table II: Non-zero matrix elements for the linear op-
erator Amn for the ﬁrst and the third order interpo-
lets. The matrix elements for the ﬁfth order inter-
polets can be calculated in a straight forward manner
but are not listed here.

source vector

0

1/3

2/3

1

Figure 6: Source vector and the exact solution to
Poisson’s equation that is used in ﬁnding the results
shown in this section.

treme case, the Hopgrid algorithm overcomes the ap-
pearant disadvantage of the density of the MRA op-
erators. To demonstrate this we also compare the
convergence rates of the two algorithms on a per ﬂop
basis. Note that we envision applying the Hopgrid al-
gorithm to non-uniform problems expresses in MRA
representations and the ﬂop count for the uniform
case represents a worse-case scenario.

The results presented in this section all use third
order interpolets. The diﬀerernatial equation we
solve for our tests is the Poisson Equation in one di-
mension on the unit interval with periodic boundary
conditions,

−

∂2
∂x2 φ(x) = ρ(x).

(22)

0.2

(o):Standard Multiply

(*):Nonstandard Multiply

s
P
O
L
F
M

 
f
o
 
r
e
b
m
u
N

0.18

0.16

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0

As Figure 6 illustrates, we use for testing purposes
two δ-functions of weight one with opposite signs as
the source vector ρ(x). Given this source vector, the
exact (both analytic and numeric) solution to Pois-
son’s equation, φ(x), is the piece-wise linear function
as given in Figure 6.

Following the procedure given in Section 5.1, we
substitute the interpolet basis expansions for φ(x)
and ρ(x) into Eq. (22). In the direct representation,
this procedure returns the matrix equation

Amn ˜φn =

Umn ˜ρn,

Xn

Xn

(23)

R

dx d

dx IL(x − m) d

dx IL(x − n) and
where Amn =
Umn =
dx IL(x − m)IL(x − n). These are the ma-
trix elements employed in the traditional multigrid
calculations, numerical values computed by recursive
application of the two scale relation as described in
[1], are provided in Table II.

R

9

200

400

600

800

1400

1600

1800

2000

1200

1000
Matrix size

Figure 7: The standard matrix vector multiplication
by the linear operator in the MRA basis vs the non-
standard multiplication.

10

20

30

40

50

60

70

WU (work units)

500

1000 1500 2000 2500 3000 3500 4000

problem size (# of grid points)

(*):hopgrid convergence rate

(o):multigrid convergence rate

(o):multigrid convergence rate

(*):hopgrid convergence rate

(*):hopgrid algorithm

(o):multigrid algorithm

−3

−4

−5

−6

−7

−8

−9

)
)
r
o
r
r
e
(
s
m

r
(
0
1
g
o

l

−10

−11
0

0

−2

−4

−6

−8

−10

−12

−14

−16
0

)
)
r
o
r
r
e
(
s
m

r
(
0
1
g
o

l

(*):hopgrid algorithm

(o):multigrid algorithm

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Number of Mflops

500

1000 1500 2000 2500 3000 3500 4000

problem size (# of grid points)

Figure 8: The convergence of the traditional multi-
grid algorithm versus the convergence of the Hopgrid
algorithm for the solution of the Poisson’s equation
on a grid of 1024 points.

Figure 9: Comparison between the convergence rates
of the traditional multigrid algorithm and the Hop-
grid algorithm for increasing problem size.

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)

U
W
/
s
t
i

i

g
d
(
 

e

t

a
r
 
.
v
n
o
c
 
f
o
 
e
u
a
v
 
.
s
b
a

l

0
0

50

45

40

35

30

25

20

15

10

)
s
p
o
l
f

i

M
/
s
t
i
g
d
(
 
e
t
a
r
 
.
v
n
o
c
 
f
o
 
e
u
a
v
 
.
s
b
a

l

5

0
0

10

The Hopgrid calculations require matrix elements
of the linear operator in the MRA represenation.
These elements are

Amra

mn =

dx

Z

d
dx

IL(2sm x − m)

IL(2sn x − n), (24)

d
dx

where si is the scale of basis function i. These values
may be computed from the previous Amn by appro-
priate change of integration variable and application
of the two scale relation.

Multiplying a vector by this MRA matrix may be
performed using one of two diﬀerent methods. The
ﬁrst is straight forward multiplication by the matrix
Amra. Because of the density of the MRA matrix
mentioned above, the number of ﬂops required to
apply the operator within the method grows faster
than linearly with the size of the problem. The
second approach uses the two-scale relation to re-
duce the number of operations so that the eﬀort in
the matrix multiplication scales only linearly with
the size of the problem [1]. Figure 7 compares the
number of M f lops needed by the standard and the
non-standard multiplications as a function of prob-
lem size. We use this superior approach in all of our
comparisons.

Figure 8 shows how the convergence of the Hop-
grid algorithm to the exact solution compares with
the convergence of the traditional multigrid algorithm
for a problem of 1024 grid points. In Figure 8a, the
convergence to the exact solution is plotted against
work units (WU). Using this comparison the Hop-
grid proceesure is superior to the traditional multi-
grid algorithm. When the convergence rates of these
two algorithms are compared in terms of digits per
Mﬂops, as in Figure 8b, we ﬁnd that despite the in-
creased density of the MRA matrix (by about a factor
of ten), the two methods give comparable results on
a uniform grid.

Next we investigate the dependence of these con-
vergence rates on problem size.
In terms of digits
per W U , Figure 9a shows that although the conver-
gence rate of the traditional multigrid algorithm re-
mains nearly constant, the convergence rate of the
Hopgrid algorithm increases with increasing problem
size. Figure 9b shows that even for the uniform case,
the eﬃciency of the Hopgrid algorithm overcomes the
increased complexity of the MRA matrix and results
in nearly the same convergence rate as the traditional
method in terms of a direct ﬂoating point operations
comparison. It is in the case of non-uniform grids,
where the application of the MRA matrix requires
far fewer operations, where we expect the maximum
from the new algorithm,

8 Conclusion

In this research, our aim was to develop a new algo-
rithm that beneﬁts from the best combination of a va-
riety of approaches, in particular the multigrid algo-
rithm, multiresolution analysis from wavelet theory,
and ﬁnite element analysis. The application for the
new algorithm which we have in mind is the numer-
ical solution of problems involving long-range inter-
actions where an underlying interpolet basis with its
multiresolution properties is needed. We have shown
that the interpolet theory, based upon multiresolu-
tion analysis from wavelet theory and interpolation
properties from ﬁnite element theory, provides a nat-
ural and unique choice of interpolation and restriction
operators and an underlying grid for the multigrid
algorithm. Then we showed how the operations of
restriction and interpolation may be carried out ef-
ﬁciently, without the application of linear operators,
by exploiting the properties of an MRA basis. Fi-
nally, we introduced the Hopgrid algorithm, which
draws upon these results to produce a multigrid-like
method to solve discretized linear equations in prob-
lems expressed in multiresolution wavelet bases. We
have seen that the convergence rate of the new algo-
rithm in terms of work units is superior to that of the
traditional multigrid apprach and that, even in the
worse-case scenario of a uniform problem, the new al-
gorithm performs as well as the traditional approach
on a direct ﬂoating point operation basis despite the
increase complexity of MRA matrices.

9 Appendix: Description of the

Multigrid Algorithm

In this section, we give a detailed discussion of the
traditional multigrid algorithm. This introduction to
multigrid is a review of the technique as given in A
Multigrid Tutorial by William Briggs [2].

9.1 Basic Iterative Methods And the

Coarse Grid Correction Scheme

We ﬁrst present the weighted Jacobi basic iterative
method and the coarse grid correction scheme which
are the core ideas in the multigrid algorithm.

Assume that the discretized diﬀerential equation
we wish to solve may be represented in matrix form
as

Au = f,

(25)

where A is the linear operator, u is the solution vec-
tor, and f is the source vector. Given an approxima-

11

tion to the solution, which we will refer to as v, the
error can be expressed as e = u−v. Another measure
of error is the residual r,

With this deﬁnition,

r ≡ f − Av.

Ae = r

(26)

(27)

is equivalent to Eq. (25). Henceforth, we shall refer
to Eq. (27) as the error equation.

Decomposing the initial matrix A as A = D−L−U ,
where D, −L, −U are the diagonal, lower triangular
and upper triangular elements of A respectively, Eq.
(25) becomes

u = D

−1(L + U )u + D

−1f.

(28)

This equation may be solved iteratively with the fol-
lowing recursion,

v(n+1) = D

−1(L + U )v(n) + D

−1f.

(29)

This recursive algorithm can be improved by choosing
an appropriate weight w, and taking a weighted av-
erage of the initial guess, v(n), and the solution from
Eq. (29),

v(n+1) = [(1 − w)I + wD

−1(L + U )]v(n) + wD

−1f. (30)

Solving Eq. (26) for f and substituting into Eq. (30)
gives a simpliﬁed form for this recursion, in terms of
the residual alone,

v(n+1) = v(n) + wD

−1r(n),

(31)

where r(n) is the residual at step n. This simpliﬁed
recursion is the core of the basic iterative method
known as the weighted Jacobi method. The applica-
tion of this method for solving a problem is referred
to as relaxation.

The weighted Jacobi method and similar basic it-
erative methods rapidly eliminate high frequency er-
rors while leaving the smooth components of the error
mostly unaﬀected. This is called the smoothing prop-
erty. The reason for this behavior can be explained
when we look at the eigenvalues of the convergence
operator ˆC ≡ 1 − wD−1A. The eigenvalues of this
operator determine how fast the error converges to
zero. Fig. 10 shows that the eigenvalues of the con-
vergence operator for the weighted jacobi method lie
in various ranges. The bands are chosen for illustra-
tion purposes and an explanation for these choices
are given in the main body of the paper.

12

Eigenvalues of Convergence Operator

slow convergence

medium convergence

high convergence

l

e
u
a
v
n
e
g
e

i

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1
0

50

100

150

mode number

200

250

Figure 10: Convergence operator for the weighted Ja-
cobi method.

varying slowly in space. Because the convergence op-
erator ˆC does not eliminate eﬃciently most of these
low frequency errors, this basic iterative method ap-
proach the exact solution very slowly. A more eﬃ-
cient method would eliminate all the error modes at
equal rates. The multigrid algorithm has this feature
and produces a solution that approaches the analytic
solution with a higher convergence rate.

The transition from the weighted Jacobi method to
the multigrid algorithm is made by improving upon
the basic Jacobi method. One way to improve the eﬃ-
ciency of weighted Jacobi relaxations is to start with a
better initial guess. Conceptually, one can do this by
solving the problem on a spatially coarser scale and
using this solution as the initial guess for the origi-
nal problem. This procedure is referred to as coarse
grid correction (CGC). On the coarse scale, there are
fewer grid points, and on this grid the smooth er-
ror modes from the ﬁne scale now appear higher fre-
quency modes. Replacing the original problem with a
smaller one of mostly high frequency errors allows the
application of the weighted Jacobi method to solve
the problem more eﬃciently. This way, the conver-
gence rate for the total error is accelerated since both
the high and the low frequency error modes are elim-
inated on the ﬁne and the coarse scales.

Figure 11 illustrates the accelerated convergence
rate of coarse grid correction scheme compared to the
weighted Jacobi method.

9.2 Full V-cycle Multigrid Algorithm

The errors that are multiplied by the eigenvalues in
the slow convergence band correspond to eigenvectors

The multigrid algorithm simply extends the idea of
CGC over a sequence of scales. Throughout this sec-

)
)
r
o
r
r
e
(
s
m

r
(
0
1
g
o

l

)
)
r
o
r
r
e
(
s
m

r
(
0
1
g
o

l

−1.19

−1.195

−1.2

−1.21

−1.215
1

−1.19

−1.195

−1.2

−1.21

−1.215
0

−1.205

(o):weighted Jacobi

(*):coarse grid correction

2

3

4

5
# of iterations

6

7

8

9

−1.205

(o):weighted Jacobi

(*):coarse grid correction

0.005

0.01

0.015

0.02

0.03

0.035

0.04

0.045

0.05

0.025
# of Mflops

Figure 11: This graph shows the diﬀerence be-
tween the eﬃciencies of a pure basic iterative method
(weighted jacobi) and the coarse grid correction in
ﬁnding the solution for the Poisson’s equation on a
grid of 128 points.

tion, superscripts are used to refer to the scale on
which that particular level of the problem is solved.
In order to ﬁnd an approximation to the solution
of the linear equation Au = f , we ﬁrst apply the
weighted Jacobi method as in Eq. 31 α times (do α
relaxations) on the ﬁnest scale, to obtain an approx-
imate solution v(0) and a residual r(0). The residual
calculated on this scale is then transferred to the next
coarser scale using a linear operator R(1)
(0), known as
the restriction operator.
takes a
vector from a ﬁne scale i, and transfers it to the next
coarse scale i + 1. (This operator is discussed later
in the appendix.) For the restriction step, we assume
that the relaxation on the ﬁnest scale has eliminated
most of the high frequency error components, which
lie in the null-space of restriction, so that little in-
formation is lost in the restriction process. The re-
stricted residual is now

In general, R(i+1)

(i)

Next we solve the residual equation

r(1) = R(1)

(0)r(0).

A(1)e(1) = r(1),

(32)

(33)

where A(1) is the linear operator on the coarse scale,
performing α relaxations, using e(1) = 0 as an initial
guess. This relaxation produces an approximation to
the error on the ﬁne scale. We could add this er-
ror to v(0) to obtain an approximate solution for the
ﬁne scale problem, since u = v + e, as previously
explained. However, the multigrid algorithm contin-
ues to go down to coarser scales and uses CGC to
ﬁnd solutions for the error equations at each coarser
scale to improve the convergence rate. Following this
prescription, we compute the residual on scale 1 and
transfer it to the next coarser scale, using the restric-
tion procedure described. Repetition of such relax-
ation and restriction steps continues until the pre-
determined coarsest scale n is reached. During each
step leading up to the coarsest scale, the generated
approximate solution vectors, the v(i), and the resid-
uals, the r(i), are stored to be used later in the algo-
rithm. Although the notation used for the result of
the relaxations at each scale is v, it is important to
realize that except for the ﬁnest scale on which the
original problem is deﬁned, all the vectors v(i) are
approximations to the error since the error equation
is used during relaxations.

At the coarsest scale, another linear operator, an
interpolation operator I (n−1)
transfers the approx-
(n)
imate solution vector v(n) down to the next ﬁner
(i+1),
scale.

In general, the interpolation operator I (i)

13

(n)

(n)

takes a vector from a coarse scale i + 1, and trans-
fers it to the next ﬁne scale i.
(This operator is
also discussed later.) The interpolated approximate
solution vector, I (n−1)
v(n), is the approximation to
the error for scale n − 1. The sum of the approxi-
mate solution v(n−1) and I (n−1)
v(n), give the approx-
imate solution on scale n − 1. However, the inter-
polation process may introduce high frequency errors
into this solution. We eliminate these errors by relax-
ing β times on the residual equation using the initial
guess v(n−1)
v(n) and the residual
(n)
r(n−1) stored from the previous restriction steps as
the source vector. This relaxation produces the ﬁnal
v(n−1) which is then interpolated to the next ﬁner
scale. This procedure is followed down to the ﬁnest
scale. On the ﬁnest scale, we relax β times on the
(25), using the initial guess
original equation, Eq.
v(0)
∗ = v(0) + I (0)
(1) v(1) and f as the source vector. The
solution obtained at this ﬁnal step is the result of
one iteration of the V-cycle of multigrid algorithm
and gives an approximate solution to the problem.

= v(n−1) + I (n−1)

∗

It is a common procedure to carry out only one
relaxation per each scale. All the results presented
here are done with α = β = 1, and w = 2/3.

9.3 Interpolation and Restriction Op-

erators

The interpolation and the restriction operators in the
multigrid algorithm obey two constraints, known as
the variational properties. The ﬁrst of these con-
straints is the Galerkin condition, which speciﬁes the
form of the linear operator on a coarser scale. As-
sume that the error vector on scale n is in the range
of interpolation, e(n) = I (n)
(n+1)e(n+1), for some vector
e(n+1) on scale n + 1. The residual equation at scale
n, A(n)e(n) = r(n), then reads

A(n)I (n)

(n+1)e(n+1) = r(n).

(34)

(36) with A(n+1) as the linear operator and the re-
stricted residual r(n+1), we determine the error vector
on scale n + 1 which when interpolated becomes the
error vector we seek, e(n).

The second variational property speciﬁes the re-
striction operator upto a scalar constant, given the
interpolation operator, R(n+1)
(n+1))T , where c
is a scalar constant. This condition gives the restric-
tion operator the full weighting property. The full
weighting restriction involves taking some weighted
average of values with neighboring points, instead of
simply down sampling, a process known as injection.

(n) = c(I (n)

References

[1] Ross A. Lippert, Tom´as Arias, and Alan Edel-
man, Journal of Computational Physics 140, 278
(1998).

[2] W.L. Briggs; A Multigrid Tutorial (Lancaster

Press, Lancaster, Penssylvania, 1987)

[3] I. Daubechies; Ten Lectures on Wavelets (Siam

Press, Philadelphia, 1992)

[4] G. Deslauriers, S. Dubuc; Fractals, dimensions
(Masson, Paris,

non entieres et applications,
1987).

[5] D.L. Donoho; Interpolating Wavelet Transforms,
(Stanford Dept of Statistics Technical Report 408,
Nov 1992).

[6] M.B. Ruskai et al; Wavelets and their Applica-

tions, (Jones and Bartlett, Boston, 1992)

[7] C.K. Chui; An Introduction to Wavelets (Aca-

demic Press, Boston, 1992)

[8] C.K. Chui, ed; Wavelets: A Tutorial in Theory
and Applications (Academic Press, Boston, 1992)

[9] S.F. McCormick; Multigrid Methods (Siam Press,

Restricting both sides, we ﬁnd

Philadelphia, 1987)

(n) A(n)I (n)
R(n+1)

(n+1)e(n+1) = R(n+1)

(n)

r(n),

(35)

[10] R.D. Cook; Concepts and Applications of Finite

Element Analysis (Wiley, New York, 1992)

or

where

A(n+1)e(n+1) = r(n+1)

A(n+1) ≡ R(n+1)

(n) A(n)I (n)

(n+1).

This is precisely the Galerkin condition for the lin-
ear operator on the next coarse scale. By solving Eq.

(36)

(37)

14


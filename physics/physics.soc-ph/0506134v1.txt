5
0
0
2
 
n
u
J
 
5
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
4
3
1
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Forecasting Financial Time-Series using
Artiﬁcial Market Models

Nachi Gupta1,⋆, Raphael Hauser1, and Neil F. Johnson2

1 Oxford University Computing Laboratory
{nachi,hauser}@comlab.ox.ac.uk

Department of Physics, Oxford University

2

n.johnson@physics.ox.ac.uk

Abstract. We discuss the theoretical machinery involved in predicting
ﬁnancial market movements using an artiﬁcial market model which has
been trained on real ﬁnancial data. This approach to market prediction -
in particular, forecasting ﬁnancial time-series by training a third-party or
‘black box’ game on the ﬁnancial data itself – was discussed by Johnson
et al. in [10] and [13] and was based on some encouraging preliminary
investigations of the dollar-yen exchange rate, various individual stocks,
and stock market indices (see [12] for more details also). However, the
initial attempts lacked a clear formal methodology. Here we present a de-
tailed methodology, using optimization techniques to build an estimate
of the strategy distribution across the multi-trader population. In con-
trast to earlier attempts, we are able to present a systematic method
for identifying ‘pockets of predictability’ in real-world markets. We ﬁnd
that as each pocket closes up, the black-box system needs to be ‘reset’
- which is equivalent to saying that the current probability estimates of
the strategy allocation across the multi-trader population are no longer
accurate. Instead, new probability estimates need to be obtained by iter-
ative updating, until a new ‘pocket of predictability’ emerges and reliable
prediction can resume.

1 Introduction

Judging from the literature, in particular the wide range of popular ﬁnance
books, the possibility of predicting future movements in ﬁnancial markets ranges
from signiﬁcant (see, for example, the many books on chartism) to impossible
(see, for example, [14]). Another scenario of course does exist - that ﬁnancial mar-
kets may neither be predictable or unpredictable all the time, but may instead
have periods where they are predictable (i.e. non-random) and periods where
they are not (i.e. random). Evidence for such ‘pockets of predictability’ were
found several years ago, by Johnson et al. [10]. A similar study was reported
subsequently by Sornette et al. [2]. However, a formal report of a theoretical
framework for identifying such periods of predictability has not appeared in the
literature to date.
⋆ This author would like to thank the Clarendon Bursary for support.

The rationale behind our initial proposal to predict ﬁnancial markets using
artiﬁcial market models, is as follows. Financial markets produce time-series, as
does any dynamical system which is evolving in time, such as the ambient air-
temperature, or the electrical signals generated by heart-rhythms. In principle,
one could use any time-series analysis technique to build up a picture of these
statistical ﬂuctuations and variations - and then use this technique to attempt
some form of prediction, either on the long or short time-scale. One example
would be to use a multivariate analysis of the prices themselves in order to build
up an estimate of the parameters in the multivariate expansion, and then run
this model forwards. However, such a multivariate model may not bear a relation
to any physical representation of the market itself. Instead, imagine that we are
able to identify an artiﬁcial market model which seems to produce the aggre-
gate statistical behavior (i.e. the stylized facts) observed in the ﬁnancial market
itself. It now has the additional advantage that it also mimics the microscopic
structure of the market, i.e. it contains populations of artiﬁcial traders who use
strategies in order to make decisions based on available information, and will
adapt their behavior according to past successes or failures. All other things
being equal, we believe that such a model may be intrinsically ‘better’ than a
purely numerical multivariate one - and may even be preferable to many more
sophisticated models such as certain neural network approaches, which also may
not be correctly capturing a realistic representation of the microscopic details
of a physical market. The question then arises as to whether such an artiﬁcial
market model could be ‘trained’ on the real market in order to produce reliable
forecasts.

Although in principle one could attempt to train any artiﬁcial market model
on a given ﬁnancial time-series, each of the model parameters will need to be
estimated - and if the model has too many parameters, this will become prac-
tically impossible as the model will become over-determined. For this reason,
our own (and Sornette et al.’s [2]) attempts have focused on training a minimal
artiﬁcial market model. We had already shown in [10] that a minimal model
could be built from a binary multi-agent game in which agents are allowed to
not participate if they were not suﬃciently conﬁdent.

Here we focus on the basic Minority Game where all agents trade at every
time-step, since we are interested in describing in detail the parameter estimation
process as opposed to creating the best possible market prediction model. In
particular, there is no reason to expect that (i) the Minority Game’s pay-oﬀ
structure whereby only the minority group gets rewarded, or (ii) the Minority
Game’s traditional feature whereby all agents have the same memory time-scale
m, are either realistic or optimal. For a more complete discussion of suitable
pay-oﬀ structures in artiﬁcial ﬁnancial markets, see Chapter 4 in [9] (also see
[8]). For the present discussion, we retain both these features since they do not
aﬀect the formalism presented - however we note that recent work by Mitman
et al. [16] and subsequent work by Guo [7] have shown that allowing agents to
have multiple memory values does indeed lead to improved performance both
overall and for speciﬁc individual traders.

2

Binary Agent Resource games, such as the Minority Game and its many
extensions, have a number of real-world applications in addition to ﬁnancial
markets. For example, they are well-suited to studying traﬃc ﬂow in the fairly
common situation of drivers having to choose repeatedly between two particular
routes from work to home. In these examples, and in particular the ﬁnancial
market setting, one would like to predict how the system will evolve in time
given the current and past states. In the context of the artiﬁcial market cor-
responding to the Minority Game and its generalizations, this ends up being a
parameter estimation problem. In particular, it comes down to estimating the
composition of the heterogeneous multi-trader population, and speciﬁcally how
this population of traders is distributed across the strategy space. Here we in-
vestigate the use of iterative numerical optimization schemes to estimate these
quantities, in particular the population’s composition across the strategy space
- we will then use these estimates to make forecasts on the time-series we are
analyzing. Along with these forecasts, we also need to ﬁnd a covariance matrix
in order to determine the certainty with which we believe our forecast is correct.
Such a covariance matrix is important for a number of reasons including risk
analysis.

Given the forecast and its associated covariance matrix, we will also need
to decide whether to use the forecast or throw it away based on the covari-
ance matrix (which represents the expected errors on the forecast). We discuss
this point, and in so doing we will see that the system can fall into ‘pockets of
predictability’ during which the system becomes predictable over some signiﬁ-
cant time-window. In the rest of this paper, we discuss these ideas and apply
them to simulated and real ﬁnancial market data, in order to identify pockets of
predictability based on the model.

2 Parameterizing the Artiﬁcial Market Model

2.1 A Binary Agent Resource Game

In a Binary Agent Resource game, a group of N agents compete for a limited
resource and each of them takes a binary action. Let’s denote this action at
time step k for agent i by ai(Ωk−1), where the action is in response to a global
information set deﬁned by Ωk−1 consisting of all information up to time step
k−1 available to all agents. For each time step, there will exist a winning decision
wk based on the action of the agents. This winning decision wk will belong to
the next global information set Ωk and will be available to each agent in the
future.

2.2 The Minority Game

A particularly simple (indeed, possibly over-simpliﬁed) example of a Binary
Agent Resource game is the Minority Game, which was proposed in 1997 by
Challet and Zhang as a very speciﬁc game which highlights the competitive ef-
fects inherent in many complex adaptive systems. Since then, many variants of

3

this game have been posed with slight modiﬁcations to the original. Here we
focus on the original Minority Game for the purposes of our examples, though
we note that the optimization formalism which we present also applies to other
variants of the game.

Let us ﬁrst provide the motivation for using the Minority Game to forecast
ﬁnancial markets. Essentially, in ﬁnancial markets, agents compete for a limited
resource, which gives us the minority nature we are interested in. For example,
if the minority group is selling, the majority group will force the price up at the
following time step because of the greater demand. At the exact same time, the
minority group will sell at the overvalued price and gain proﬁt.

We start the game with a group of N agents, each of which holds an assigned
strategy. Let a given strategy have memory size m meaning it contains a binary
decision for each of the 2m possible binary strings of length m. An example of a
strategy with m = 2 is given in Table 2.1.

Memory Decision

-1, -1
-1, 1
1, -1
1, 1

1
-1
1
1

2
Table 2.1. The left column shows the 2
column shows the strategy’s response to each memory string.

= 4 possible memory strings and the right

A given strategy will look back at the m most recent bits of the winning
decisions. At time step k, this would be (wk−m, . . . , wk−1). The strategy will then
pick the corresponding binary decision. Using our example strategy in Table 2.1,
if wk−2 = −1 and wk−1 = 1, the strategy would make the decision −1. Before
we can explain how the strategy works, we must also deﬁne the time horizon, a
bit string consisting of the T most recent bits of the winning decisions where T
is signiﬁcantly larger than m. For example, at the beginning of time step k, the
time horizon would be (wk−T , . . . , wk−1).

In order for the game to be interesting, we assume some agents hold more
than 1 strategy in what we call their strategy set. We now need to deﬁne how the
agents should choose amongst their strategies. Each agent scores each of their
strategies over the time horizon by giving it one point if it would have predicted
correctly and taking one away if it would have predicted incorrectly at each
time step in the past. For example, if the time horizon was (−1, 1, −1, −1, 1),
we would assign +1 − 1 + 1 = +1 points to our strategy in Table 2.1 since the
ﬁrst decision on (−1, 1) to choose −1 would have been correct, while the second
decision would have been incorrect and the third one correct again. In this way,
we could score all of the agents’ strategies, and the agent will simply pick the
highest scoring strategy to play. The winning decision at time step k, wk, is the
minority decision made by the agents as a whole.

4

For ties between the scores of an agent’s strategies, the agent will simply toss
a fair coin to decide. Further, if there is a tie in the winning decision over all
agents (i.e. an equal number of agents picked -1 and 1), we can again toss a fair
coin to decide. Both of these, together, inject stochasticity into the system. As
mentioned in the introduction, there are a number of variations on ways to score
strategies that can be looked at. In this paper, we stick to the basic Minority
Game structure.

We are now interested in the time-series generated by the aggregate actions
of the agents. We start the series at r0 = 0, where r stands for returns, and allow
i ai(Ωk−1), where ai(Ωk−1) denotes the response
rk to evolve by rk = rk−1 +
of agent i at time k to global information set Ωk−1. For the Minority Game, this
response is simply the decision made by agent i, and Ωk−1 consists only of the
time horizon at time k − 1. Again, agent i makes this decision by choosing the
highest scoring strategy over the time horizon as explained above.

P

P

Further, let’s deﬁne the diﬀerence series of the returns series as z0 = 0 with
zk = rk − rk−1 =
i ai(Ωk−1). The diﬀerence series is the series we will estimate
throughout this paper. It is trivial to ﬁnd the returns series given the diﬀerence
series. In terms of the diﬀerence series, we can also deﬁne the winning decisions
and the time horizon, which we provide here for completeness. The winning
decision at time step k, wk can be deﬁned by wk = -sgn(zk), where sgn(k)
represents the sign function, which is 1 for positive k, −1 for negative k, and 0
otherwise. This simply states that when zk is positive, the minority chose −1
and vice versa. Note that zk will never be 0 since we toss a fair coin for ties. As
before, the time horizon is deﬁned by the winning decisions. At time step k, this
would simply be (wk−T , . . . , wk−1).

For a more thorough introduction to the Minority Game and other variations

and extensions, please refer to [1] or [4].

2.3 The Parameter Estimation Problem for Forecasting

Generally speaking, we would expect that the types of data we might be able to
forecast using the Minority Game would have also been generated by something
similar to a Minority Game. If the real market in question corresponds to a mixed
majority-minority game, for example, then clearly it makes sense to attempt
matching the real-data to such a mixed version of the game. This point will
be explored in another publication. Here we focus on the Minority Game as a
speciﬁc example.

There are many parameters in the Minority Game, so we have to choose a
way to parameterize the game. In this paper, we ﬁx m and T for all agents. We
also say each agent possesses exactly 2 strategies. Next, we remove the parameter
N specifying the number of agents, by allowing this to tend towards inﬁnity and
instead looking at the probability distribution over all possible strategy sets with
2 strategies of memory size m. For example, if m = 1, there are 22
= 4 strategies
with a memory size of 1 (there are 2m possible bit strings of length m and 2
responses to each bit string). So there are
= 6 distinct pairs of strategies
with memory size m = 1. The parameter space we would like to estimate is the

1

4
2
(cid:0)
(cid:1)

5

probability distribution over these 6 possible strategy sets. Notice that we make
a number of assumptions here to decide which parameters to estimate. Some
of these assumptions can be relaxed a bit to create a larger parameter space if
desired.

In this paper, we will provide a mechanism to estimate the probability dis-
tribution over a set of strategies. We will call this probability distribution the
state xk at time step k. In the previous paragraph, we mentioned a scheme to
estimate the 6 pairs of m = 1 strategies. In this case, we were assuming all 6
are strategy sets that were played when generating the time-series. However, we
could also choose to estimate a strategy set with more than 2 strategies per agent
(or maybe even just 1) and each of the strategies in the set can have diﬀerent
memory lengths if desired (note that we may like to modify the scoring scheme
for mixed memory sizes). In this case, we would assume that this set of N mixed
memory and mixed strategy length strategy sets were played when generating
the time-series.

Notice that this estimation problem is an inequality constrained problem
with the constraints that each probability of playing a given strategy set must
be greater than or equal to 0 and the probabilities must sum up to 1.

Section 3 and Section 4 discuss some of the technicalities of the optimization

problem. Section 5 and Section 6 provide some examples with results.

3 Iterative Optimization Methods to Solve Parameter

Estimation Problems

We will now look at iterative (recursive) schemes to solve time-dependent param-
eter estimation problems. We desire iterative schemes for a number of reasons.
They provide a forecast in only one pass of the data. This means the algorithm
can be online and can quickly make forecasts as new measurements are observed.
The iterative schemes we discuss also provide us with error bounds on forecasts
so we can determine when we are in a state of high predictability (or if we ever
attain such a state). This is important for ﬁnancial data since risk is always an
important factor. The ﬁrst method we shall discuss is the Kalman Filter.

3.1 Kalman Filter

A Kalman Filter is simply an iterative least-squares scheme that attempts to
ﬁnd the best estimate at every iteration for a system governed by the following
model:

xk = Φk,k−1xk−1 + uk,

uk ∼ N (0, Qk,k−1)

zk = Hkxk + vk,

vk ∼ N (0, Rk)

(3.1.1)

(3.1.2)

Here xk represents the true state of the underlying system. Φk,k−1 represents
the matrix used to make the transition from state xk−1 to xk. The variable zk
represents the measurement (or observation). Hk is the matrix that takes the

6

state into measurement space. The variables uk and vk are both noise terms
which are normally distributed with mean 0 and variances Qk,k−1 and Rk, re-
spectively.

The Kalman Filter will at every iteration make a prediction for xk which we
denote by ˆxk|k−1. We use the notation k|k − 1 since we will only use measure-
ments provided until time step k − 1 to make the prediction at time k. We can
deﬁne the state prediction error ˜xk|k−1 as the diﬀerence between the true state
and the state prediction.

˜xk|k−1 = xk − ˆxk|k−1

(3.1.3)

In addition, the Kalman Filter will provide a state estimate for xk given all
the measurements provided up to and including time step k. We denote these
estimates by ˆxk|k. We can similarly deﬁne the state estimate error by

˜xk|k = xk − ˆxk|k

(3.1.4)

Since we assume uk is normally distributed with mean 0, we make the state

prediction simply by using Φk,k−1 to make the transition. This is given by

ˆxk|k−1 = Φk,k−1 ˆxk−1|k−1

(3.1.5)

We can also calculate the associated covariance for the state prediction, which
we call the covariance prediction. This is actually just the expectation of the
outer product of the state prediction error with itself. This is given by

Pk|k−1 = Φk,k−1Pk−1|k−1Φ′

k,k−1 + Qk,k−1

(3.1.6)

Notice that we use the prime notation on a matrix throughout this paper to
denote the transpose of that matrix. Now we can make a prediction on what we
expect to see for our measurement, which we call the measurement prediction
by

ˆzk|k−1 = Hk ˆxk|k−1

(3.1.7)

The diﬀerence between our true measurement and our measurement predic-
tion is often times called the innovation (or measurement residual). We will use
the term innovation throughout this paper, and we calculate this by

We can also calculate the associated covariance for the innovation, which we

call the innovation covariance, by

νk = zk − ˆzk|k−1

Sk = HkPk|k−1H ′

k + Rk

(3.1.8)

(3.1.9)

Next, we will calculate the Kalman Gain, which lies at the heart of the
Kalman Filter. This essentially tells us how much we prefer our new measurement
over our measurement residual. We calculate this by

7

Kk = Pk|k−1H ′

kS−1
k

(3.1.10)

Using the Kalman Gain and the innovation, we update the state estimate. If
we look carefully at the following equation, we are essentially taking a weighted
sum of our state prediction with the Kalman Gain multiplied by the innovation.
So the Kalman Gain is telling us how much to “weight in” information contained
in the new measurement. We calculate the updated state estimate by

ˆxk|k = ˆxk|k−1 + Kkνk

(3.1.11)

Last but not least, we calculate the updated covariance estimate. This is
actually just the expectation of the outer product of the state error estimate
with itself. Here we will give the most numerically stable form of this equation,
as this form prevents loss of symmetry and best preserves positive deﬁniteness

Pk|k = (I − KkHk)Pk|k−1(I − KkHk)′ + KkRkK T
k

(3.1.12)

The covariance matrices throughout the Kalman Filter give us a way to mea-
sure the uncertainty of our state prediction, state estimate, and the innovation.
Also, notice that the Kalman Filter is recursive, and we require an initial esti-
mate ˆx0|0 and associated covariance matrix P0|0. Here we simply provided the
equations of the Kalman Filter without derivation. For a more thorough under-
standing of the Kalman Filter, please refer to [3].

3.2 Constrained Iterative Optimization Methods

A Kalman Filter would certainly be the correct tool for the parameter estimation
problem described in 2.3 if we were interested in an iterative solution and did
not have any equality and inequality constraints. However, note that we have the
following constraints on our states at each time step that make this a constrained
problem:

ˆxi,k|k = 1 and ˆxi,k|k ≥ 0, ∀i

(3.2.1)

Xi

Here ˆxi,k|k is the i-th element of ˆxk|k, which represents the single probability
of using a certain strategy set at time step k. Since we would like to use an iter-
ative scheme, we must now think of a diﬀerent method which acts as a Kalman
Filter but allows for equality and inequality constrained optimization. In Section
3.3, we will introduce a method for solving equality constrained problems itera-
tively in a Kalman Filter like manner. From here we will make the extension to
inequality constrained problems in Section 3.4.

3.3 Nonlinear Equality Constraints

Let’s add to our model given by equations (3.1.1) and (3.1.2) the following
smooth nonlinear equality constraints

8

ek(xk) = 0

(3.3.1)

Notice that our constraints provided in equation (3.2.1) are actually linear.
We present the nonlinear case for further completeness here. We now rewrite
the problem we would like to solve where we use the superscript c to denote
constrained. We should also rephrase the problem we would like to solve now.
We are given the last prediction and its covariance, the current measurement
and its covariance, and a set of equality constraints and would like to make the
current prediction and ﬁnd its covariance matrix.
Let’s write the problem we are solving as

k = hc
zc

k(xk) + vc
k,

k ∼ N (0, Rc
vc
k)

(3.3.2)

Here zc

k, hc

k, and vc

k are all vectors, each having three distinct parts. The
ﬁrst part will represent the prediction for the current time step, the second
part is the measurement, and the third part is the equality constraint. zc
k eﬀec-
tively still represents the measurement, with the prediction treated as a ”pseudo-
measurement” with its associated covariance.

The matrix hc

k takes our state into the measurement space as before

Φk,k−1 ˆxk−1|k−1
zk
0





zc
k = 



hc
k(xk) = 

xk
Hkxk
ek(xk)







Notice that by combining equations (3.1.3) and (3.1.4), we can rewrite the

state error prediction as

˜xk|k−1 = Φk,k−1 ˜xk−1|k−1 + uk−1

(3.3.5)

Now we can deﬁne vc

k again as the noise term using equation (3.3.5).

vc
k = 



−Φk,k−1 ˜xk−1|k−1 − uk−1
vk
0





And vc

k will be normally distributed with mean 0 and variance Rc

k. The
diagonal elements of Rc
k. We deﬁne
the covariance of the state estimate error at time step k as Pk|k. Notice also that
Rc

k represent the variance of each element of vc

k contains no oﬀ diagonal elements.

Φk,k−1Pk−1|k−1Φk,k−1

′ + Qk,k−1 0 0
Rk 0
0 0





Rc

k = 



0
0

9

(3.3.3)

(3.3.4)

(3.3.6)

(3.3.7)

This method of expressing our problem can be thought of as a fusion of the
state prediction and the new measurement at each iteration under the given
equality constraints. Much like when we showed the Kalman Filter, we will sim-
ply write the solution here, and refer the reader to [5, 19] for more information.

ˆxk|k,j =

0 I

(cid:2)

(cid:20)

(cid:3)

+

Rc
H c

k H c
k,j
′
0 (cid:21)
k,j

(cid:20)

k − hc
zc

k(ˆxc

k|k,j−1) + H c
0

k,j ˆxc

k|k,j−1

(cid:21)

(3.3.8)

Notice the we use the + notation on a matrix throughout this paper to de-
note the pseudo-inverse of that matrix. This method signiﬁcantly diﬀers from
a Kalman Filter. In this method we are iterating over a dummy variable j
within each time step until we fall within a predetermined convergence bound
ˆxk|k,j − ˆxk|k,j−1
≤ ck or hit a chosen number of maximum iterations. We ini-
tialize our ﬁrst iteration as ˆxk|k,0 = ˆxk−1|k−1 and use the ﬁnal iteration as
(cid:12)
(cid:12)
ˆxk|k = ˆxk|k,J where J represents the ﬁnal iteration.
k,j = ∂hc

Also, notice that we allowed the equality constraints to be nonlinear. As a
∂xk (ˆxk|k,j−1) which gives us a local approximation to

(cid:12)
(cid:12)

k

result, we deﬁne H c
the direction of hc
k.

In [5, 19], we will actually ﬁnd a stronger form for this solution, where Rc
k will
reﬂect the tightening of the covariance for the state prediction based on the new
estimate at each iteration of j. We do not tighten the covariance matrix within
these iterations here, since in our form, we can actually change the number of
equality constraints between iterations of j. We will ﬁnd this useful in the next
section. Not tightening the covariance matrix in this way is reﬂected in a larger
covariance matrix for the estimate as well. This covariance matrix is calculated
as

Pk|k,j = −

0 I

(cid:2)

(cid:20)

(cid:3)

+

Rc
H c

k H c
k,j
′
0 (cid:21)
k,j

0
I (cid:21)

(cid:20)

(3.3.9)

Notice that for faster computation times, we need only calculate Pk|k,j for the
ﬁnal iteration of j. Further, if our equality constraints are in fact independent
of j, we can calculate H c
k,j only once for each k. This would also imply the
pseudo-inverse in equation (3.3.8) can be calculated only once for each k.

This method, while very diﬀerent from the Kalman Filter presented earlier,
provides us with an estimate ˆxk|k and a covariance matrix for the estimate Pk|k
at each time step similar to the Kalman Filter. However, this method allowed
us to incorporate equality constraints.

3.4 Nonlinear Inequality Constraints

We will now extend the equality constrained problem to an inequality con-
strained problem. To our system given by equations (3.1.1), (3.1.2), and (3.3.1),
we will also add the smooth inequality constraints given by

lk(xk) ≥ 0.

10

(3.4.1)

Our method will be to keep a subset of the inequality constraints active
at any time. An active constraint is simply a constraint that we treat as an
equality constraint. An inactive constraint we will relax (ignore) when solving our
optimization problem. After, solving the problem, we then check if our solution
lies in the space given by the inequality constraints. If it doesn’t we start from the
solution in our previous iteration and move in the direction of the new solution
until we hit a set of constraints. For the next iteration, this set of constraints
will be the new active constraints.

We formulate the problem in the same way as before keeping equations
(3.3.2), (3.3.3), (3.3.6), and (3.3.7) the same to set up the problem. However, we
replace equation (3.3.4) by

hc
k(xk) =

xk
Hkxk
ek(xk)
la
k,j(xk)













(3.4.2)

la
k,j represents the set of active inequality constraints. Notice that while we
keep equations (3.3.3), (3.3.6), and (3.3.7) the same, these will need to be padded
by additional zeros appropriately to match the size of la
k,j. Now we solve the
equality constrained problem consisting of the equality constraints and the active
inequality constraints (which we treat as equality constraints) using equations
(3.3.8) and (3.3.9). However, let’s call the solution from equation (3.3.8) ˆx∗
k|k,j
since we have not checked if this solution lies in the inequality constrained space
yet. In order to check this, we ﬁnd the vector that we moved along to reach
ˆx∗
k|k,j . This is simply

d = ˆx∗

k|k,j − ˆxk|k,j−1

(3.4.3)

We now iterate through each of our inequality constraints to check if they
are satisﬁed. If they are all satisﬁed, we choose tmax = 1, and if they are not, we
choose the largest value of tmax such that ˆxk|k,j−1 + tmaxd lies in the inequality
constrained space. We choose our estimate to be

ˆxk|k,j = ˆxk|k,j−1 + tmaxd

(3.4.4)

We also would like to remember the inequality constraints which are being
touched in this new solution. These constraints will now become active for the
next iteration and lie in la
k−1,J , where J represents the
ﬁnal iteration of a given time step.

k,j+1. Note that la

k,0 = la

Note also that we do not perturb the error covariance matrix from equation
(3.3.9) in any way. Under the assumption that our model is a well-matched
model for the data, enforcing inequality constraints (as dictated by the model)
should only make our estimate better. Having a slightly larger covariance matrix
is better than having an overly optimistic one based on a bad choice for the
perturbation. This idea is also touched upon in [18]. Perturbing this covariance
matrix correctly may be investigated in the future.

11

4 Covariance Matching Techniques

In many applications of Kalman Filtering, the process noise Qk,k−1 and mea-
surement noise Rk are known. However, in our application we are not provided
with this information a priori so we would like to estimate them. These can often
times be diﬃcult to approximate especially when there is a known model mis-
match. We will present one possible method to approximate these. The method
we choose is a technique similar to those provided in [15]. We choose to match the
process noise and measurement noise to the past innovation (residual) process.

4.1 Determining the Process Noise and Measurement Noise

In addition to estimating Qk,k−1 and Rk, we will also estimate the innovation
covariance Sk. We can actually determine the innovation covariance from equa-
tion (3.1.9), but estimating it using covariance matching to the past innovation
process can provide us with a more accurate innovation covariance.

We estimate Sk by taking a window of size Nk (which is picked in advance
for statistical smoothing) and time-averaging the innovation covariance based
on the innovation process. This is simply the average of all the outer products
of the innovations over this window.

ˆS∗

k =

1
Nk − 1

k−1

Xj=k−Nk

′

νjνj

(4.1.1)

Next, let’s estimate Rk. This is done similarly. If we refer back to equation

(3.1.9), we can simply calculate this by

ˆR∗

k =

1
Nk − 1

k−1

Xj=k−Nk

νjνj

′ − HjPj|j−1H ′
j

(4.1.2)

We can now use our choice of Rk along with our innovation covariance Sk to

estimate Qk,k−1. Combining equations (3.1.6) and (3.1.9) we have

Sk = Hk(Φk,k−1Pk−1|k−1Φk,k−1

′ + Qk,k−1)Hk + Rk

(4.1.3)

Bringing all Qk,k−1 terms to one side leaves us with

HkQk,k−1Hk

′ = Sk − HkΦkPk−1|k−1Φk

′Hk

′ − Rk

(4.1.4)

And solving for Qk,k−1 gives us

ˆQ∗

k,k−1 =

Hk
(cid:0)

′Hk

′

+Hk
(cid:1)

Sk − HkΦkPk−1|k−1Φk
(cid:0)

Note that it may be desirable to keep ˆQ∗

(4.1.5)
(cid:1)
(cid:1)
k,k−1 diagonal if we do not believe
the process noise has any cross-correlation. It is rare that you would expect
a cross-correlation in the process noise. In addition, keeping the process noise

′Hk

′ − Rk

Hk

+

′Hk

Hk
(cid:0)

12

diagonal has the eﬀect of making our covariance matrix “more positive deﬁnite.”
This can be done simply by setting the oﬀ diagonal terms of ˆQ∗
k,k−1 equal to 0.
It is also important to keep in mind that we are estimating covariance ma-
trices here which must be symmetric and positive semideﬁnite (note that the
diagonal elements should always be greater than or equal to zero as these are
variances).

4.2 Upper and Lower Bounds for Covariance Matrices

We might also like to denote a minimum and maximum number we are willing
to accept for each element of our covariance matrices. The motivation for main-
taining a minimum is that we may not want to become overly optimistic. If the
covariances drop to zero, we will assume the random variable has perfect knowl-
edge. The reason for maintaining a maximum is in case we believe the covariance
actually is upper bounded. Let us denote these matrices by Smin
, Qmin
,
k
Smax
. We apply these by
k

, and Qmax

, Rmax
k

, Rmin
k

k

k

k−1

ˆSk =

1
Nk − 1

min

Smax
j
(cid:0)

, max

Smin
j
(cid:0)

, νjνj

′

,

(cid:1)(cid:1)

Xj=k−Nk

(4.2.1)

k−1

ˆRk =

1
Nk − 1

min

Rmax
j
(cid:0)

, max

Rmin
j
(cid:0)

Xj=k−Nk

, νjνj

′ − HjPj|j−1H ′
j

,

(4.2.2)

(cid:1)(cid:1)

and, using equation (4.1.5),

ˆQk = min

Qmax
k

, max

Qmin
k

, ˆQ∗
k

(4.2.3)

(cid:16)

(cid:16)
Again, keep in mind that the diagonal elements of Smin

, and Qmin
must all be greater than or equal to zero. This is a very simple way of lower and
upper bounding these matrices. There will certainly be a number of ways we
could approach this problem some of which might be much better at preserving
the original information. Our hope for the application mentioned in this paper
is that the bounds are rarely touched if ever.

, Rmin
k

(cid:17)(cid:17)

k

k

5 Application of Inequality Constrained Iterative

Optimization Methods to a Simulated Minority Game

In this section, we will apply the discussed methods to a simulation of the Mi-
nority Game. In the next section, we will apply these methods to real ﬁnancial
data.

13

5.1 Generating Simulation Data

We choose parameters m = 1 for the memory size and allow two strategies per
agent resulting in 6 overall combinations as described in Section 2.3. Also, we
choose the time horizon size to be T = 50. We randomly choose a bit string of
length 50 to serve as the initial time horizon, and we also randomly choose a
probability distribution over the 6 possible strategy sets. We run the simulation
over 150 time steps. This results in a returns series rk from which we can extract
the diﬀerence series zk.

5.2 Predicting the Simulated Market Data

Forming the Estimation Problem To track the diﬀerence series zk, we set the
problem up similar to how it was generated with m = 1 giving us 6 probabilities,
and we choose the time horizon size to be T = 50.

We also make the assumption that the estimate for the probability distribu-
tion at time k will also be the prediction at time k + 1 since we hope that our
estimate at time k will be well matched to the data locally. For the simulated
case, the probability distribution is actually ﬁxed over all k. This boils down
to choosing the identity matrix as the transition matrix (Φk,k−1 for all k in the
notation of Section 3.1).

We create the time horizon at each time step by looking back T time steps
and checking where the minority would have lied at each time step as described
near the end of Section 2.2 using the diﬀerence series zk.

Finally, we score each strategy in each of our strategy sets over the time
horizon similar to what we described in Section 2.2. This will result in a set of
winning strategies. We determine the set of predictions of each of the winning
strategies and this forms the measurement matrix Hk (it is actually a vector of
±1 since the measurements are scalars). Multiplying this by the state gives us
the prediction based on the probability of being in a certain strategy set and
what the set would pick as its forecast.

Notice that in most tracking applications, the transition matrix Φk,k−1 drives
the system evolution through time and the measurement matrix Hk describes
a ﬁxed coordinate transform. Here Hk changes signiﬁcantly based on the state
estimate ˆxk−1|k−1 of the system. In fact, the process noise Qk,k−1 and measure-
ment noise Rk, found by covariance matching techniques in our case, combined
with Hk are really what drive the system evolution through time.

Choosing the minimum and maximum acceptable covariances For the
minimum and maximum acceptable covariances used in the covariance matching
scheme of Section 4, we choose

k = Rmin
Smin

k = 0 and Smax

k = Rmax

k = 1

(5.2.1)

Note that both the measurements and the innovations must lie in [−1, 1]. This
is true because in the extreme situations, all the strategies can choose either −1

14

or 1. It is also known that the variance of the distribution with half of its weight
at a and the other half at b is given by (b−a)
. Applying this formula gives us the
4
maximum acceptable variances of 1 in equation (5.2.1). We use a similar idea to
choose the minimum and maximum acceptable process noise.

2

Qmin

k =

0 0 · · · 0
0 0 · · · 0
...
...
...
. . .
0 0 · · · 0















Qmax

k =

.25 0 · · · 0
0 .25 · · · 0
...
...
...
. . .
0 · · · .25
0















and

and

We choose .25 for the diagonal terms of Qmax

by our previous logic since each
element of the probability distribution must lie in [0, 1]. We force the diagonal
elements to be 0 in order to keep our covariances more positive deﬁnite.

k

In addition, we state the following deﬁnition

cov(x, y) = cor(x, y)σxσy

(5.2.4)

Noticing that the cor(x, y) takes its most extreme values at ±1 and σx and

σy both take their largest values at b−a

2 , we can state

|cov(x, y)| ≤

b − a

2

(cid:18)

2 (cid:19)

We may also be interested in bounding our state prediction error and state
estimate error covariances since we know we are estimating a probability distri-
bution. Using equation (5.2.5) for the oﬀ diagonal terms, we can bound these
by

P min
k|k−1 = P min

k|k =

0 −.25 · · · −.25
· · · −.25
−.25
0
...
...
...
. . .
−.25 −.25 · · ·
0















P max
k|k−1 = P max

k|k =

.25 .25 · · · .25
.25 .25 · · · .25
...
...
...
. . .
.25 .25 · · · .25















15

(5.2.2)

(5.2.3)

(5.2.5)

(5.2.6)

(5.2.7)

We did not provide a very rigorous explanation for our choice of covariance
bounds here. However, in our example, these are the choices we made for the
reasons provided above.

For the covariance matching, we also choose Nk in equations (4.2.1) and
(4.2.2) to be equal to T = 50. Notice that while we have less than 50 innova-
tions, we choose Nk to be the number of innovations we have. When we have 0
innovations (at the initial point), we choose Rk = 0 so we can heavily trust the
ﬁrst measurement to strengthen our initialization.

Initial Parameters We choose our initial state ˆx0|0 = 1
s 1s, where s is the
number of strategy sets (in our case 6) and 1s is a column vector of size s full of
1’s. This is essentially starting with a uniform distribution. We also choose our
initial covariance P0|0 = .25Is×s, where Is×s represents the s × s identity matrix.
We choose .25 again for the same reason as before. Note that we will actually
start the optimization problem at time step T + 1 since we use the ﬁrst T data
points to initialize the time horizon.

Using the methods described in Section 3.4 and Section 4, we can now make
predictions on this system. After making predictions, we need a system by which
to decide which predictions to accept with greater certainty. We discuss this in
the next section.

5.3 Eﬀective Forecasting

The last question we would like to address here is: when is the forecast produced
by this method good and how good? We could base this on the innovation
covariance Sk which is an estimate of the errors of the innovation (residual)
process. Note that we can either use equation (3.1.9) along with equation (3.1.6)
or we can use equation (4.1.1) to calculate Sk. The residual-based estimate given
by equation (4.1.1) will generally provide a smoother function through k which
might be desirable to ﬁnd pockets of predictability (where we can predict well
for a while).

There are various ways of using Sk to decide when we would like to make a
prediction. We look at a very simple method, where we simply take a threshold
value tk such that we choose k in our set of prediction times if Sk ≤ tk.

5.4 Results of simulation

We show the results from the simulated data in Figure 5.1. We choose the thresh-
old value tk in this plot to be 10−3 for all k. Notice that in our case, tk is a scalar
since the measurements are scalars. As we can see in the plot, we are able to
make good forecasts at over 30 points (where our innovations lie within the in-
novation standard deviation). Notice that we only attempt to make forecasts at
the last 100 points of the 150 generated data points (we use the ﬁrst 50 points to
generate the initial time horizon as mentioned earlier). However, we choose only
to make a prediction at 34 of the data points. It happens to be that these 34 data

16

points are the ﬁrst 34 that we attempt to forecast. We have 2 bad predictions at
the end of the plot. After the bad predictions, we never recover to a good pre-
diction since the covariance matching scheme drives the estimated process noise
ˆQk,k−1, estimated measurement noise ˆRk, and estimated innovation covariance
ˆSk up due to the large spike in the single residual which aﬀects the statistical
smoothing for 50 time steps. Since the covariance of the state remains tight and
the covariance of the measurements is relatively large, new measurements aren’t
trusted and given much weight for creating forecasts.

At the same time, because of the transient by the statistical smoothing,
we continue to make forecasts immediately after the ﬁrst false prediction. We
might choose to incorporate a scheme to not make predictions for some length
of time immediately after a false prediction to allow ˆQk,k−1, ˆRk, and ˆSk to
respond to the shock caused by the false prediction. Further, we might like to
signiﬁcantly increase the process noise after a false prediction to eﬀectively cause
new measurements to have a stronger weight in forming estimates.

6 Application of Inequality Constrained Iterative

Optimization Methods to Real Foreign Exchange Data

Finally, let’s apply the ideas in this paper to real ﬁnancial data. We choose
hourly USD/YEN foreign exchange rate data from 1993 to 1994 provided by Dr.
J. James of Bank One in London.

6.1 Setting up the Problem

Scaling the diﬀerence series We ﬁrst ﬁnd the diﬀerence series from the
returns series as before. For the time being, let’s call this z∗
k = rk − rk−1. Since
our algorithm only makes forecasts in [−1, 1], we might like to scale our inputs
to this domain as best as possible. Assuming that we have measurements a priori
up to time step K, we estimate the scaling based on these measurements. Let’s
denote the set of all measurements up to time K by z∗
K where z∗
K in our case is
a vector of scalar measurements. We choose the following method:

Let’s denote the minimum and maximum elements of vector V by the func-
tions min(V ) and max(V ), respectively. And let’s denote the minimum and max-
imum elements of z∗
K , respectively. We ﬁrst proportionally scale
the spacing between elements of z∗
k so the diﬀerence between the minimum el-
ement and the maximum element is 2 (the size of [−1, 1]). We denote this by
z∗∗
k

K and zmax

K by zmin

z∗∗
k =

2z∗
k
zmax
K − zmin

K

(6.1.1)

Next, we scale the elements so the minimum element is at −1. This will

automatically place the maximum element at +1.

zk = z∗∗

k − (min(z∗∗

k ) + 1)

(6.1.2)

17

-6

x 10

1

Simulated Data

s
n
o
i
t
a
v
o
n
n
I

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

-1

5

10

15

20

25

30

35

Prediction Times

Fig. 5.1. In the above plot, the solid line represents the innovation (measurement
residual) process and the dashed line represents one standard deviation about zero
based on the predicted innovation variance. We select “Prediction Times” in this plot
as times when the data is in a predictable state based on the innovation covariance.
These times need not be consecutive in the original data although often times are.

K and zmax

We do the calculation for zmin

K once with all of the a priori infor-
mation we have. Then we can use equations (6.1.1) and (6.1.2) to scale for any
time step k. Our hope is that based on the a priori information, our choice of
zmin
K and zmax
K will reﬂect the true spacing. If we know true values for these, we
can use them instead. Notice also that we can still ﬁnd the returns series rk from
this deﬁnition for the measurements simply by inverting the process.

There will certainly be a number of diﬀerent ways to do this scaling as well.

Forming the Estimation Problem For the rest of this estimation problem,
we actually do the setup exactly the same as in Section 5.2 and we follow the
eﬀective forecasting scheme exactly as in Section 5.3 choosing threshold value tk
to be 10−3 again for all k.

18

6.2 Results on the Real Data

Here we had over 4000 data points. We chose to make predictions at about 100
data points of which over 90 we accept. Again these are the ﬁrst data points we
attempt to make a forecast on. And again we see some false predictions near the
end. Incorporating a scheme to not make predictions immediately after a false
prediction as mentioned in Section 5.4 would leave us with only 1 false prediction
and over 90 good predictions.

-6

x 10

2.5

Hourly USD/YEN FX-rate from 1993 to 1994

s
n
o
i
t
a
v
o
n
n
I

1.5

0.5

2

1

0

-0.5

-1

-1.5

-2

-2.5

10

20

30

70

80

90

100

40
50
60
Prediction Times

Fig. 6.1. In the above plot, the solid line represents the innovation (measurement
residual) process and the dashed line represents one standard deviation about zero
based on the predicted innovation variance. We select “Prediction Times” in this plot
as times when the data is in a predictable state based on the innovation covariance.
These times need not be consecutive in the original data although often times are.

6.3 Extension to Other Games

Note that we chose the Minority Game as the game we thought best exhibits the
dynamics of the ﬁnancial time-series we analyzed. The method for forecasting

19

we describe in this paper can be used with a number of diﬀerent models (or
games). We require the following of our model and forecast scheme:

1) We can parameterize the problem into quantities we would like to estimate
iteratively.

2) We have a way to estimate the transition dynamics for the parameters at each
iteration and this function will always lie in the class of continuously diﬀeren-
tiable functions.

3) We have a way to estimate the measurement function which takes the param-
eter space into the measurement space at each iteration and this function will
always lie in the class of continuously diﬀerentiable functions.

4) We have a way to estimate the process noise and measurement noise at each
iteration.

7 Conclusion

We have shown a way to forecast time-series by parameterizing an artiﬁcial mar-
ket model such as the Minority Game, using an iterative numerical optimization
technique. In our technique we also describe how to follow an eﬀective forecasting
scheme which leads to pockets of predictability.

This paper is meant only to serve as an introduction to such methods of
forecasting. In particular, we have made a number of assumptions here which
can be relaxed in order to pose a more complete problem.

References

1. http://www.unifr.ch/econophysics/.
2. Jørgen Vitting Andersen and Didier Sornette. A mechanism for pockets of pre-
dictability in complex adaptive systems. Europhysics Letters, 70(5):697–703,
November 2005.

3. Yaakov Bar-Shalom, X. Rong Li, and Thiagalingam Kirubarajan. Estimation with

Applications to Tracking and Navigation. John Wiley and Sons, Inc., 2001.

4. Damien Challet, Matteo Marsili, and Yi-Cheng Zhang. Minority Games. Oxford

University Press, 2005.

5. Y. T. Chiang, L. S. Wang, F. R. Chang, and H. M. Peng. Constrained ﬁltering
method for attitude determination using gps and gyro. IEE Proceedings - Radar,
Sonar, and Navigation, 149(5):258–264, October 2002.

6. Jan De Geeter, Hendrik Van Brussel, and Joris De Schutter. A smoothly con-
strained kalman ﬁlter. IEEE Transactions on Pattern Analysis and Machine In-
telligence, 19(10):1171–1177, October 1997.

7. Chengling Gou. Predictability of shanghai stock market by agent-based mix-game

model. e-print cond-mat/0505180 at xxx.lanl.gov, May 2005.

20

8. Paul Jeﬀeries and Neil F. Johnson. Designing agent-based market models. e-print

cond-mat/0207523 at xxx.lanl.gov, July 2002.

9. Neil F. Johnson, Paul Jeﬀeries, and Pak Ming Hui. Financial Market Complexity.

Oxford University Press, 2003.

10. Neil F. Johnson, David Lamper, Paul Jeﬀeries, Michael L. Hart, and Sam D. How-
ison. Application of multi-agent games to the prediction of ﬁnancial time-series.
Physica A: Statistical Mechanics and its Applications, 299(1–2):222–227, October
2001.

11. Thomas Kailath, Ali H. Sayed, and Babak Hassibi. Linear Estimation. Prentice

Hall, March 2000.

12. David Lamper. Problems in Mathematical Finance : Market Modelling and Deriva-

tive Pricing. PhD thesis, University of Oxford, 2002.

13. David Lamper, Sam D. Howison, and Neil F. Johnson. Predictability of large
future changes in a competitive evolving population. Physical Review Letters,
88(1), January 2002.

14. Burton G. Malkiel. A Random Walk Down Wall Street: Completely Revised and

Updated Eighth Edition. W. W. Norton and Company, 2003.

15. Peter S. Maybeck. Stochastic Models, Estimation and Control, volume 2. Academic

Press, Inc., 1982.

16. Kurt E. Mitman, Sehyo Charley Choe, and Neil F. Johnson. Competitive advantage
for multiple-memory strategies in an artiﬁcial market.
In Proceedings of SPIE:
Noise and Fluctuations in Econophysics and Finance, volume 5848, pages 225–
232. The International Society for Optical Engineering, May 2005.

17. Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer-Verlag,

Inc., 1999.

18. Dan Simon and Donald L. Simon. Kalman ﬁltering with inequality constraints for
turbofan engine health estimation. Technical Report A491414, National Aeronau-
tics and Space Administration, John H. Glenn Research Center at Lewis Field,
February 2003.

19. L. S. Wang, Y. T. Chiang, and F. R. Chang. Filtering method for nonlinear systems
with constraints. IEE Proceedings - Control Theory and Applications, 149(6):525–
531, November 2002.

20. Y. Yang and F. Ma. Constrained kalman ﬁlter for nonlinear structural identiﬁca-

tion. Journal of Vibration and Control, 9(12):1343–1357, December 2003.

21


5
0
0
2
 
p
e
S
 
7
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
0
5
0
9
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Analysis of on-line learning

when a moving teacher goes around a true teacher

Seiji MIYOSHI∗ and Masato OKADA†

∗Department of Electronic Engineering, Kobe City College of Technology,

8–3 Gakuenhigashimachi, Nishi-ku, Kobe-shi, 651–2194 Japan

†Division of Transdisciplinary Sciences, Graduate School of Frontier Sciences,

The University of Tokyo, 5–1–5 Kashiwanoha, Kashiwa-shi, Chiba, 277–8561 Japan

RIKEN Brain Science Institute, 2–1 Hirosawa, Wako-shi, Saitama, 351–0198 Japan

JST PRESTO

December 20, 2013

Abstract

In the framework of on-line learning, a learning machine might move around a teacher due to the

diﬀerences in structures or output functions between the teacher and the learning machine or due to

noises. The generalization performance of a new student supervised by a moving machine has been

analyzed. A model composed of a true teacher, a moving teacher and a student that are all linear

perceptrons with noises has been treated analytically using statistical mechanics. It has been proven

that the generalization errors of a student can be smaller than that of a moving teacher, even if the

student only uses examples from the moving teacher.

Key-words: on-line learning, generalization error, moving teacher, true teacher, unlearnable case

1

Introduction

Learning is to infer the underlying rules that dominate data generation using observed data. The observed

data are input-output pairs from a teacher. They are called examples. Learning can be roughly classiﬁed

into batch learning and on-line learning [1]. In batch learning, some given examples are used repeatedly.

In this paradigm, a student becomes to give correct answers after training if that student has an adequate

degree of freedom. However, it is necessary to have a long amount of time and a large memory in which

many examples may be stored. On the contrary, examples used once are discarded in on-line learning.

In this case, a student cannot give correct answers for all examples used in training. However, there are

1

some merits, for example, a large memory for storing many examples isn’t necessary and it is possible to

follow a time variant teacher.

Recently, we [6, 7] have analyzed the generalization performance of ensemble learning [2, 3, 4, 5] in a

framework of on-line learning using a statistical mechanical method [1, 8]. In that process, the following

points are proven subsidiarily. The generalization error doesn’t approach zero when the student is a simple

perceptron and the teacher is a committee machine [11] or a non-monotonic perceptron [12]. Therefore,

models like these can be called unlearnable cases [9, 10]. The behavior of a student in an unlearnable case

depends on the learning rule. That is, the student vector asymptotically converges in one direction using

Hebbian learning. On the contrary, the student vector doesn’t converge in one direction but continues

moving using perceptron learning or AdaTron learning.

In the case of a non-monotonic teacher, the

student’s behavior can be expressed by continuing to go around the teacher, keeping a constant direction

cosine with the teacher.

Considering the applications of statistical learning theories, investigating the system behaviors of

unlearnable cases is very signiﬁcant since real world problems seem to include many unlearnable cases.

In addition, a learning machine may continue going around a teacher in the unlearnable cases as mentioned

above. Here, let us consider a new student that is supervised by a moving learning machine. That is,

we consider a student that uses the input-output pairs of a moving teacher as training examples and we

investigate the generalization performance of a student with a true teacher. Note that the examples used

by the student are only from the moving teacher and the student can’t directly observe the outputs of

the true teacher. In a real human society, a teacher that can be observed by a student doesn’t always

present the correct answer. In many cases, the teacher is learning and continues to vary. Therefore, the

analysis of such a model is interesting for considering the analogies between statistical learning theories

and a real society.

In this paper, we treat a model in which a true teacher, a moving teacher and a student are all linear

perceptrons [6] with noises, as the simplest model in which a moving teacher continues going around

a true teacher. We calculate the order parameters and the generalization errors analytically using a

statistical mechanical method in the framework of on-line learning. As a result, it is proven that a

student’s generalization errors can be smaller than that of the moving teacher. That means the student

can be cleverer than the moving teacher even though the student uses only the examples of the moving

teacher.

2 Model

Three linear perceptrons are treated in this paper: a true teacher, a moving teacher and a student. Their

connection weights are A, B and J , respectively. For simplicity, the connection weight of the true teacher,

that of the moving teacher and that of the student are simply called the true teacher, the moving teacher,

2

and the student, respectively. The true teacher A = (A1, . . . , AN ), the moving teacher B = (B1, . . . , BN ),
the student J = (J1, . . . , JN ), and input x = (x1, . . . , xN ) are N dimensional vectors. Each component
Ai of A is drawn from

(0, 1) independently and ﬁxed, where

N

N

(0, 1) denotes the Gaussian distribution
i of the initial values of B, J

i , J 0

with a mean of zero and a variance unity. Each of the components B0

are drawn from

(0, 1) independently. Each component xi of x is drawn from

(0, 1/N ) independently.

N

N

Thus,

Ai
h
B0
i
(cid:10)

i

(cid:11)

J 0
i
(cid:10)

(cid:11)
xi
h

i

= 0,

= 0,

= 0,

= 0,

(cid:11)
i )2

(Ai)2
(cid:10)
(B0
(cid:10)
(J 0
(cid:10)
(xi)2
(cid:10)

i )2

(cid:11)

(cid:11)

(cid:11)

= 1,

= 1,

= 1,

=

1
N

,

xm,

ym = A

·
B = Bm

vmlm

xm,

umlm

J = J m

xm,

·

·

nm
A ∼ N
nm
B ∼ N
nm
J ∼ N

(0, σ2

A),

(0, σ2

B),

(0, σ2

J ).

3

where

denotes a mean.

h·i

In this paper, the thermodynamic limit N

is also treated. Therefore,

→ ∞

A
k

k

= √N ,

B0

k

k

= √N ,

J 0

k

k

= √N ,

x

k

k

= 1,

where

denotes a vector norm. Generally, norms

B

and

J

of the moving teacher and the student

k · k

k
change as the time step proceeds. Therefore, the ratios lB and lJ of the norms to √N are introduced

k

k

k

and are called the length of the moving teacher and the length of the student. That is,

B

k

k

= lB√N C

J

k

= lJ √N .

k
The outputs of the true teacher, the moving teacher, and the student are ym + nm

A , vmlm

B + nm

B , and

umlm

J + nm

J , respectively. Here,

and

where m denotes the time step. That is, the outputs of the true teacher, the moving teacher and the

student include independent Gaussian noises with variances of σ2

J , respectively. Then, the
ym, vm, and um of Eqs.(6)–(8) obey the Gaussian distributions with a mean of zero and a variance unity.

B, and σ2

A, σ2

In the model treated in this paper, the moving teacher B is updated using an input x and an output

of the true teacher A for the input x. The student J is updated by using an input x and an output of

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

the moving teacher B for the input x. Let us deﬁne an error between the true teacher and the moving

teacher by the squared error of their outputs. That is,

The moving teacher is considered to use the gradient method for learning. That is,

ǫm
B ≡

1
2

(ym + nm

vmlm

B −

A −

B )2 .
nm

Bm+1 = Bm

∂ǫm
B
∂Bm

ηB

−

= Bm + ηB (ym + nm

A −

vmlm

B −

B ) xm,
nm

where, ηB denotes the learning rate of the moving teacher and is a constant number.

In the same manner, let us deﬁne an error between the moving teacher and the student by the squared

error of their outputs. That is,

ǫm
BJ ≡

1
2

(vmlm

B + nm

B −

umlm

J −

J )2 .
nm

The student is considered to use the gradient method for learning. That is,

J m+1 = J m

ηJ

∂ǫm
BJ
∂J m

−
= J m + ηJ (vmlm

B + nm

umlm

J −

B −

J ) xm,
nm

where, ηJ denotes a learning rate of the student and is a constant number.

Generalizing the learning rules, Eqs.(14) and (17) can be expressed as

Bm+1 = Bm + g (ym + nm

A , vmlm

B + nm

B ) xm,

J m+1 = J m + f (vmlm

B + nm

B , umlm

J + nm

J ) xm,

Let us deﬁne an error between the true teacher and the student by the squared error of their outputs.

ǫm
J ≡

1
2

(ym + nm

umlm

J −

A −

J )2 .
nm

respectively.

That is,

3 Theory

3.1 Generalization Error

One purpose of a statistical learning theory is to theoretically obtain generalization errors. Since a

generalization error is the mean of errors for the true teacher over the distribution of the new input and

noises, the generalization error ǫBg of the moving teacher and ǫJg of the student are calculated as follows.

The superscripts m, which represent the time steps, are omitted for simplicity.

ǫBg =

dxdnAdnBP (x, nA, nB)ǫB

Z

(21)

4

=

dydvdnAdnBP (y, v, nA, nB)

(y + nA

vlB

−

−

nB)2

=

2RBlB + (lB)2 + 1 + σ2

A + σ2
B

,

ǫJg =

dxdnAdnJ P (x, nA, nJ )ǫJ

=

dydudnAdnJ P (y, u, nA, nJ )

(y + nA

ulJ

−

−

nJ )2

=

2RJ lJ + (lJ )2 + 1 + σ2

A + σ2
J

.

Z

1
2

×
1
2 (cid:0)−

Z

Z

1
2

×
1
2 (cid:0)−

(cid:1)

(cid:1)

In addition, let us calculate the mean ǫBJg of the error between the student and the moving teacher

as follows:

ǫBJg =

dxdnBdnJ P (x, nB, nJ )ǫBJ

Z

Z

1
2

×
1
2 (cid:0)−

=

dvdudnBdnJ P (v, u, nB, nJ )

(vlB + nB

ulJ

nJ )2

−

−
2RBJlBlJ + (lJ )2 + (lB)2 + σ2

=

Here, the integration has been executed using the following: y, v and u obeys

(0, 1). The covariance

between y and v is RB, between y and u is RJ , and between v and u is RBJ , where

RB

≡

, RJ

≡

, RBJ

≡

A
A

B
B

·
kk

A
A

J
J

·
kk

k
Eq.(30) means that RB, RJ , and RBJ are direction cosines. nA, nB, and nJ are all independent with other

k

k

k

k

k

probabilistic variables. The true teacher A, the moving teacher B, the student J , and the relationship

among RB, RJ , and RBJ are shown in Fig.1.

B + σ2
J

.

(cid:1)

N

B
B

J
J

·
kk

.

3.2 Diﬀerential equations of order parameters and their analytical solutions

To make analysis easy, the following auxiliary order parameters are introduced:

Simultaneous diﬀerential equations in deterministic forms [8] have been obtained that describe the

dynamical behaviors of order parameters based on self-averaging in the thermodynamic limits as follows:

rB

rJ

RBlB,

RJ lJ ,

≡

≡

≡

rBJ

RBJ lBlJ .

drB
dt

=

gy
h

,
i

5

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

Figure 1: True teacher A, moving teacher B and student J . RB, RJ , and RBJ are direction cosines.

Since linear perceptrons are treated in this paper, the sample averages that appeared in the above

equations can be calculated easily as follows:

drJ
dt
drBJ
dt
dlB
dt
dlJ
dt

=

f y
h

,
i

= lJ

gu
h

i

+ lB

f v
h

i

+

gf
h

,
i

=

=

+ h

gv
h

i

f u
h

i

+ h

,

g2
i
2lB
f 2
i
2lJ

.

gu
h

i

f v
h

i

gf
h

i

f y
h

i

gy
h

i

gv
h

i

g2
h

i

f u
h

i

f 2
h

i

l2
B + rBJ

σ2
B),

−

= ηB(rJ

rBJ )/lJ ,

−

= ηJ (lB

−
= ηBηJ (rB

rBJ /lB),

−

rJ

−
rJ ),

= ηJ (rB

= ηB(1

−
rB),

−

= ηB(rB/lB

lB),

−
A + σ2

−
J + σ2

= η2

B(1 + σ2

B + l2

B −

2rB),

= ηJ (rBJ /lJ

lJ ),

= η2

J (l2

B + l2

B + σ2

J −

2rBJ ).

R0

B = R0

J = R0

BJ = 0.

N

6

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

(48)

Since each components of the true teacher A, the initial value of the moving teacher B, and the

initial value of the student J are drawn from

(0, 1) independently and because the thermodynamic

limit N

is also treated, they are all orthogonal to each other in the initial state. That is,

→ ∞

In addition,

cally as follows:

By using Eqs.(39)–(49), the simultaneous diﬀerential equations Eqs.(34)–(38) can be solved analyti-

l0
B = l0

J = 1.

where

−

ηJ

ηB

ηJ

−

−ηB t,

e

rB = 1

−
rJ = 1 +

e

−ηB t,
ηB

ηJ

−

rBJ =

−ηJ t

e

ηB
D
ηJ
ηB
−
−ηBt +

e

ηJ
l2
B = 3

−
C

ηB

l2
J =

−

−
G
ηJ (ηJ

−

−

2)
F

ηBηJ

−
2ηJ
ηB

−
−
ηJ

−
ηB
ηJ

ηB

−ηJ t

e

ηJ

ηB
CeηB (ηB −2)t + Ee(ηB ηJ −ηB −ηJ )t,

−

−ηB t + CeηB (ηB −2)t,

2e

eηB (ηB −2)t

ηB(ηB
2ηB

2)

−
−ηJ t

−
e

ηJ (ηJ

2)

−
2ηJ

−ηB t

e

−

ηJ

ηB
e(ηB ηJ −ηB −ηJ )t + HeηJ (ηJ −2)t,

−

ηJ

ηB

−
2ηJ E
ηJ

ηB

−

+

+

+

+

−

(σ2

A + σ2

B),

C = 2

−

2

D = ηB(1

ηB

−

ηB
ηJ σ2

−

E =

B) + ηJ (1
η2
BηJ

−

ηB) (3

C) ,

−
A + σ2
(σ2
B)

ηB

ηJ )
B)
ηJ

−
ηJ σ2
ηB

,

−

−
ηB(1
ηBηJ
2

−
−

C,

−
ηB)(ηBηJ

(ηJ

−
2ηB

+

ηB
−
ηB + ηJ
ηB

−
ηJ

−
B + σ2
3 + σ2
(cid:0)

−

ηJ

F = η2
J

G = η2
J

H = 3

+

−

ηB(ηB
G
ηJ (ηJ

2)

−
+

−

2)

ηJ (ηJ

2)

−
2ηJ

−

E.

ηB

ηJ

−

C

(cid:1) −

2ηJ (1
ηBηJ

ηJ )D
ηJ

−
ηB

,

−

−

J −
F

(49)

(50)

(51)

(52)

(53)

(54)

(55)

(56)

(57)

(58)

(59)

(60)

4 Results and discussion

The dynamical behaviors of the generalization errors ǫBg, ǫJg and ǫBJg have been obtained analytically

by solving Eqs.(23), (26), (29), (31)–(33) , and (50)–(60). Figures 2 and 3 show the analytical results

and the corresponding simulation results, where N = 103. In the computer simulations, ǫBg, ǫJg, and

7

ǫBJg have been obtained by averaging the squared errors for 104 random inputs at each time step. The

dynamical behaviors of R and l are shown in Figs.4 and 5. In these ﬁgures, the curves represent the

theoretical results. The dots represent the simulation results. Conditions other than ηJ are common:

ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4. Figures 2 and 4 show the results in the case of ηJ = 1.2.

Figures 3 and 5 show the results in the case of ηJ = 0.3.

 0

 5

 20

 10

t=m/N

Figure 2: Generalization errors ǫJg, ǫBg, and ǫBJg in the case of ηJ = 1.2. Theory and computer

simulation. Conditions other than ηJ are ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

r
o
r
r
E
 
n
o
i
t
a
z
i
l
a
r
e
n
e
G

r
o
r
r
E
 
n
o
i
t
a
z
i
l
a
r
e
n
e
G

 2.4

 2.2

 2

 1.8

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 1.6

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

B - J

J

B

 15

B - J

B

J

 15

 0

 5

 20

 10
t=m/N

Figure 3: Generalization errors ǫJg, ǫBg, and ǫBJg in the case of ηJ = 0.3. Theory and computer

simulation. Conditions other than ηJ are ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

Figure 2 shows that the generalization error ǫJg of the student is always larger than the generalization

error ǫBg of the moving teacher when the learning rate of student is relatively large, such as ηJ = 1.2. In

8

addition, the mean ǫBJg of the error between the moving teacher and the student is still larger than ǫJg.

Figure 4 shows that the direction cosine RJ between the true teacher and the student is always smaller

than the direction cosine RB between the true teacher and the moving teacher.

 0

 5

 15

 20

 10

t=m/N

Figure 4: R and l in the case of ηJ = 1.2. Theory and computer simulation. Conditions other than ηJ

are ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

l_J
l_B

l_B

l_J

 2
 1.8
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0

 1.4

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

l
 
,

R

l
 
,

R

R_B

R_J

R_BJ

R_J
R_B

R_BJ

 0

 5

 15

 20

 10

t=m/N

Figure 5: R and l in the case of ηJ = 0.3. Theory and computer simulation. Conditions other than ηJ

are ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

On the contrary, Fig.3 shows that when the learning rate of the student is relatively small, that is

ηJ = 0.3. Although the generalization error ǫJg of the student is larger than the generalization error ǫBg

of the moving teacher in the initial stage of learning, as in the case of ηJ = 1.2, the size relationship is

reversed at t = 4.4, and after that ǫJg is smaller than ǫBg. This means the performance of the student

9

becomes higher than that of the moving teacher.

In regard to the direction cosine, Fig.5 shows that

though the direction cosine RJ between the true teacher and the student is smaller than the direction

cosine RB between the true teacher and the moving teacher in the initial stage of learning, the size

relationship is reversed at t = 5.2, and after that, RJ grows larger than RB. This means that the student

gets closer to the true teacher than the moving teacher in spite of the student only observing the moving

teacher. The reason why the size relationship reverses at diﬀerent times in Fig.3 and Fig.5 is that the

generalization error depends on not only the direction cosines RB, RJ , and RBJ but also the lengths lB

and lJ as shown in Figs.(23), (26), and (29) since linear perceptrons are treated and the squared error is

adopted as an error in this paper. In any case, these results show that the student can have higher level

of performance than the moving teacher. It depends on the learning rate ηJ of the student. This is a

very interesting fact.

In addition, both Figs. 4 and 5 show that the direction cosine RBJ between the moving teacher and

the student takes a negative value in the initial stage of learning. That is, the angle between the moving

teacher and the student once becomes larger than in the initial condition. This means that the student

is once delayed. This is also an interesting phenomenon.

Figures 2 – 5 show that ǫBg, ǫJg, ǫBJg, R, and l almost seem to reach a steady state by t = 20. The

macroscopic behaviors of t

can be understood theoretically since the order parameters have been

→ ∞

obtained analytically. Focusing on the signs of the powers of the exponential functions in Eqs.(50)–(54),

we can see that ǫBg and ǫBJg diverge if 0 > ηB or ηB > 2, and ǫBJg and ǫJg diverge if 0 > ηJ or ηJ > 2.

The steady state values of ǫBg, ǫJg, ǫBJg, R, and l in the case of 0 < ηB, ηJ < 2 can be easily obtained

by substituting t

in Eqs.(50)–(54). The relationships that are obtained by this operation, between

→ ∞

the learning rate ηJ of the student and ǫBg, ǫJg, ǫBJg, R, and l, are shown in Figs. 6, 7, and 8. The

conditions other than ηJ are ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4 that are the same as Figs. 2– 5.

The values on t = 50 are plotted for the simulations. The values are considered to have already reached

a steady state.

These ﬁgures show the following: though the steady generalization error of the student is larger than

that of the moving teacher if ηJ is larger than 0.58, the size relationship is reversed if ηJ is smaller than

0.58. This means the student has higher level of performance than the moving teacher when ηJ is smaller

than 0.58. In regard to the steady R and the steady l, the size relationships are reversed when ηJ = 0.70.

In the limit of ηJ
student J coincides with the true teacher A in both direction and length when ηJ

0, lJ approaches unity, RBJ approaches RB, and RJ approaches unity. That is, the

0. Note that the

→

→

reason why the generalization error ǫJg of the student isn’t zero in Fig. 6 is that independent noises are

added to the true teacher and the student. The phase transition in which RJ and RBJ become zero and

lJ , ǫBJg, and ǫJg diverge on ηJ = 2 is shown in Figs. 6–8.

10

r
o
r
r
E
 
n
o
i
t
a
z
i
l
a
r
e
n
e
G

R

 10

 1

 0.1

 0

 1

 0.8

 0.6

 0.4

 0.2

 0

B - J

J

B

 0.5

 1.5

 2

 1

Eta_J

Figure 6: Steady value of generalization errors ǫBg, ǫJg and ǫBJg. Theory and computer simulation.

Conditions other than ηJ are ηB = 1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

 0

 0.5

 1.5

 2

 1
Eta_J

Figure 7: Steady value of R. Theory and computer simulation. Conditions other than ηJ are ηB =

1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

R_B

R_J

R_BJ

11

 2.5

l

 4

 3.5

 3

 2

 1

 1.5

l_J

l_B

 0

 0.5

 1.5

 2

 1

Eta_J

Figure 8: Steady value of l. Theory and computer simulation. Conditions other than ηJ are ηB =

1.0, σ2

A = 0.2, σ2

B = 0.3, and σ2

J = 0.4.

5 Conclusion

The generalization errors of a model composed of a true teacher, a moving teacher, and a student that

are all linear perceptrons with noises have been obtained analytically using statistical mechanics. It has

been proven that the generalization errors of a student can be smaller than that of a moving teacher,

even if the student only uses examples from the moving teacher.

This research was partially supported by the Ministry of Education, Culture, Sports, Science, and Tech-

nology, Japan, with a Grant-in-Aid for Scientiﬁc Research 14084212, 14580438, 15500151 and 16500093.

Acknowledgments

References

[1] Saad, D. (ed.), On-line Learning in Neural Networks, Cambridge University Press, (1998)

[2] Freund, Y. and Schapire, R.E., “A short introduction to boosting,” Journal of Japanese Society for

Artiﬁcial Intelligence, 14(5), 771–780 (1999) (in Japanese, translation by Abe, N.)

[3] http://www.boosting.org/

(1997).

[4] Krogh, A. and Sollich, P., “Statistical mechanics of ensemble learning,” Phys. Rev. E, 55(1), 811–825

[5] Urbanczik, R., “Online learning with ensembles,” Phys. Rev. E, 62(1), 1448–1451 (2000).

12

[6] Hara, K. and Okada, M., “Ensemble learning of linear perceptron; Online learning theory”,

cond-mat/0402069.

[7] Miyoshi, S., Hara, K. and Okada, M., “Analysis of ensemble learning using simple perceptrons based

on online learning theory”, Phys. Rev. E, 71, 036116. March 2005.

[8] Nishimori, H., “Statistical Physics of Spin Glasses and Information Processing: An Introduction,”

[9] Inoue, J. and Nishimori, H., “On-line AdaTron learning of a unlearnable rules,” Phys. Rev. E, 55(4),

Oxford University Press, (2001)

4544–4551 (1997).

cond-mat/9708096 (1997).

[10] Inoue, J., Nishimori, H. and Kabashima, Y., “A simple perceptron that learns non-monotonic rules,”

[11] Miyoshi, S., Hara, K. and Okada, M., “Analysis of ensemble learning for committee machine teacher”,

Proc. The Seventh Workshop on Information-Based Induction Sciences, pp.178–185, (2004) (in

Japanese).

[12] Miyoshi, S., Hara, K. and Okada, M., “Analysis of ensemble learning for non-monotonic teacher”,

IEICE Technical Report, NC2004-214, pp.123–128, (2005) (in Japanese).

13


5
0
0
2
 
n
u
J
 
9
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
6
8
0
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Economics: the next physical science?

J. Doyne Farmer,1 Martin Shubik,2 and Eric Smith1
1Santa Fe Institute, 1399 Hyde Park Rd., Santa Fe NM 87501
2Economics Department, Yale University, New Haven CT
(Dated: February 2, 2008)

We review an emerging body of work by physicists addressing questions of economic organization
and function. We suggest that, beyond simply employing models familiar from physics to economic
observables, remarkable regularities in economic data may suggest parts of social order that can
usefully be incorporated into, and in turn can broaden, the conceptual structure of physics.

Contents

I. Physics and economics

II. Data analysis and the search for empirical regularities2

III. Modeling the behavior of agents

IV. The quest for simple models of non-rational choice4

V. The El Farol bar problem and the minority game5

VI. Entropy methods

VII. Future directions

A. Income distributions (BOX)

B. Option pricing (BOX)

References

I. PHYSICS AND ECONOMICS

In the last decade or so physicists have begun to
do academic research in economics in a newly emerg-
ing ﬁeld often called “econophysics”. Perhaps a hun-
dred people are now actively involved, with two new
journals1 and frequent conferences. At least ten books
have been written on econophysics in general or spe-
ciﬁc subtopics (We are restricted by citation limits
here, but an extensive bibliography of the books and
archived articles is maintained on the econophysics web-
site, http://www.unifr.ch/econophysics). Ph.D. theses
are being granted by physics departments for research in
economics, and in Europe there are several professors in

1 The new journals are The International Journal of Pure and Ap-
plied Finance and Quantitative Finance. Quantitative Finance
was started by the Institute of Physics and later sold to a com-
mercial ﬁrm.

1

4

6

6

7

8

8

physics departments specializing in econophysics. There
is even a new annual research prize, titled the “Young
Scientist Award for Social and Econophysics”.
Is this
just a fad, or is there something more substantial here?
If physicists want to do research in economics, why
don’t they just get degrees in economics in the ﬁrst place?
Why don’t the econophysicists retool, ﬁnd jobs in eco-
nomics departments and publish in traditional economics
journals? Perhaps this is just a temporary phenomenon,
driven by a generation of physicists who made a bad ca-
reer choice.
Is there any reason why research in eco-
nomics should be done in physics departments as an on-
going activity, or why economics departments should pay
heed to the methods of physics? What advantage, if any,
is conferred by a background in physics? And most im-
portant, how does econophysics diﬀer from economics,
and what unique contribution can it make, if any?

One is naturally suspicious that the emergence of
econophysics is just a reﬂection of a depressed job mar-
ket. It is certainly true that during the last two decades
a large number of physicists have been lured to Wall St,
and that this has been an important stimulus. But this
is not the main focus. Econophysics is primarily an aca-
demic endeavor, whose participants are employed by uni-
versities. It oﬀers no special advantages in the job market
– in fact, quite the opposite: It is even more compet-
itive than mainstream ﬁelds of physics. No permanent
positions in econophysics have ever been oﬀered in an
American university. Papers submitted to Physical Re-
view Letters require special justiﬁcation concerning their
relevance to physics. While the situation in Europe is a
little better than in the U. S., jobs are still very scarce.
The tenuous existence of econophysics relies on senior
professors who have redirected their interests from other
areas, as well as a few bold students and postdocs.

The involvement of physicists in social science has a
long history, going back at least to Daniel Bernoulli, who
in 1738 introduced the idea of utility to describe people’s
preferences. In his Essai philosophique sur les probabilites
(1812), Laplace pointed out that events that might seem
random and unpredictable, such as the number of letters
that end up in the Paris dead-letter oﬃce, can in fact be
quite predictable and can be shown to obey simple laws.
These ideas were further ampliﬁed by Quetelet, a student
of Fourier, who in 1835 coined the term “social physics”,
and studied the existence of patterns in data sets rang-

ing from the frequency of diﬀerent methods for commit-
ting murder to the chest size of Scottish men. Analogies
to physics played an important role in the development
of economic theory through the nineteenth century, and
some of the founders of neoclassical economic theory2,
such as Irving Fisher, a student of Willard Gibbs, were
originally trained as physicists. Ettore Majorana in 1938
presciently outlined both the opportunities and pitfalls
in applying statistical physics methods to the social sci-
ences.

The range of topics that have been addressed spans
many diﬀerent areas of economics. Finance is particu-
larly well represented; sample topics include the empiri-
cal observation of regularities in market data, the dynam-
ics of price formation, the understanding of bubbles and
panics, methods for pricing derivatives such as options,
the construction of optimal portfolios, and many other
subjects. Broader topics in economics include the distri-
bution of income, theories of how money emerges, and
implications of symmetry and scaling to the functioning
of markets.

Despite their long history of association, we see the
substantial contributions of physics to economics as still
in an early stage, and ﬁnd it fanciful to forecast what will
ultimately be accomplished. Almost certainly, “physi-
cal” aspects of theories of social order will not simply
recapitulate existing theories in physics, though already
there appear to be overlaps. The development of so-
cieties and economies is potentially contingent on acci-
dents of history, and at every turn hinges on complex
aspects of human behavior. Nonetheless, striking empir-
ical regularities such as those we survey below suggest
that at least some social order is not historically con-
tingent, and perhaps is predictable from ﬁrst principles.
The role of markets as mediators of communication and
distributed computation, and the emergence of the so-
cial institutions that support them, are quintessentially
economic phenomena. Yet the notions of their computa-
tional or communication capacity, and how these account
for their stability and historical succession, may naturally
be parts of the physical world as it includes human social
dynamics. In the context of human desires, markets and
other economic institutions bring with them notions of
eﬃciency or optimality in satisfying those desires. While
intuitively appealing, such notions have proven hard to
formalize, and the examples below show some progress in
this area. As with most new areas of physical inquiry, we

2 “Neoclassical economics” refers to a representation of individual
decision making in terms of scalar “utility” functions, whose gra-
dients are imagined to be like forces directing people to trade, and
from which economic equilibria arise as a kind of “force balance”
among diﬀerent people’s trading wishes. From the economist’s
point of view neoclassical economics clariﬁed and extended the
work of the classical economists, Smith, Mill, Ricardo and oth-
ers by formalizing the notions of competition, marginal utility
and rent, as well as producing separate theories of the ﬁrm and
consumer.”

2

expect that the ultimate goals of a “physical economics”
will be declared with hindsight, from successes in identi-
fying, measuring, modeling and in some cases predicting
empirical regularities.

II. DATA ANALYSIS AND THE SEARCH FOR
EMPIRICAL REGULARITIES

Economists are typically better trained in statistical
analysis than physicists, so this might seem to be an area
where physicists have little to contribute. However, dif-
ferences in goals and philosophy are important. Physics
is driven by the quest for universal laws. In part, because
of the extreme complexity of phenomena in society, in the
postmodern world where relativist philosophies of science
enjoy disturbingly widespread acceptance, this quest has
been largely abandoned. Modern work in social science
is largely focused on documenting diﬀerences. Although
this trend is much less obvious in economics, a typical
paper in ﬁnancial economics, for example, might study
the diﬀerence between the NYSE and the NASDAQ stock
exchanges, or the eﬀect of changing the tick size of prices
(the unit of the smallest possible price change). Physi-
cists have (perhaps naively) entered with fresh eyes and
new hypotheses, and have looked at economic data with
the goal of ﬁnding pervasive regularities, emphasizing
what might be common to all markets rather than what
might make them diﬀerent. This work has been oppor-
tunistically motivated by the the existence of large data
sets such as the complete transaction histories of major
exchanges over timespans of years, which in some cases
contain hundreds of millions of events.

Much of the work by physicists in economics concerns
power laws. A power law is an asymptotic relation of the
form f (x) ∼ x−α, where x is a variable and α > 0 is a
constant. In many important cases f is a probability dis-
tribution. Power laws have received considerable atten-
tion in physics because they indicate scale free behavior
and they are characteristic of critical or nonequilibrium
phenomena. In fact, the ﬁrst power law distribution (in
any ﬁeld) was observed in economics (see Box A), and
the existence of power laws in economics has been a mat-
ter of debate ever since. In 1963 Benoit Mandelbrot [1]
observed that the distribution of cotton price ﬂuctua-
tions follows a power law. Further observations of power
laws in price changes were subsequently noted by Rosario
Mantegna and Eugene Stanley [2], who coined the term
“econophysics”. Fig. 1 shows the striking ﬁdelity often
found in economic power laws. The existence of power
laws in price changes is interesting from a practical point
of view because of its implications for the risk of ﬁnancial
investments, and from a theoretical point of view because
it suggests scale independence and possible analogies to
nonequilibrium behavior in the processes that generate
ﬁnancial returns.

Since then many other power laws have been discov-
ered by physicists. These include the variance in the

)
x
 
>

 
|
 

 

n
r
u
t
e
r
|
 
(
P
n
o
i
t
c
n
u
f
 
n
o
i
t
u
b

i
r
t
s
i
D

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

100

101

102

x (Units of standard deviation)

FIG. 1: The distribution of 15 minute price movements for
1000 stocks from the NYSE and NASDAQ, from ref. [3]. A
movement in price p is deﬁned as log p(t + τ ) − log p(t). The
price movements for each stock were normalized by dividing
by the standard deviation for that stock. There is a total of 15
million events. The straight line on the right side of the curve
indicates a power law, over about two orders of magnitude.

growth rates of companies as a function of their size [4],
the distribution function for the number of shares in a
transaction, the distribution for the number of trading
orders submitted at a price x away from the best price
oﬀered, the size of the price response to a trade as a
function of the size C of the company being traded, and
many other examples. The question of why power laws
are so ubiquitous in ﬁnancial markets has stimulated a
great deal of theoretical work.

One of the most famous and most used models of prices
is the random walk. The random walk was originally in-
troduced for prices in 1900 by Louis Bachelier, a student
of Poincar´e, ﬁve years before Einstein introduced it to
describe Brownian motion. This forms the basis for the
Black-Scholes theory of option pricing [5], which won a
Nobel prize in economics in 1997 (see Box B). One of the
interesting and surprising properties of the random walk
of real prices is that its diﬀusion rate is not constant. The
size of price changes is strongly positively autocorrelated
in time, a phenomenon called clustered volatility 3. The
autocorrelation of the size of price changes decays as a
power law of the form τ −γ. Since 0 < γ < 1, this is
a long-memory process. Long-memory processes display
anomalous diﬀusion and very slow convergence of statis-

3 The autocorrelation function of a time series x(t) is deﬁned as
C(τ ) = h˜x(t)˜x(t − τ )i, where ˜x = (x − hxi)/σx, h i denotes a time
average and σx the standard deviation. “Volatility” is the term
used in ﬁnance to refer to the variance or size of price shifts.

3

tical averages.

Physicists have recently discovered that volatility is
just one of several long-memory-processes in markets.
One of the most surprising concerns ﬂuctuations in sup-
ply and demand [6]. If one assigns +1 to an order to buy
and −1 to an order to sell, the resulting series of numbers
has a positive autocorrelation function that decays as a
power law with an exponent γ ≈ 0.6, which persists at
statistically signiﬁcant levels across tens of thousands of
orders, for periods of time lasting for weeks. This implies
that changes in supply and demand obey a long-memory
process. This has interesting implications.

One of the most fundamental principles in ﬁnancial
economics is called market eﬃciency. This principle
takes many forms: A market is informationally eﬃcient
if prices reﬂect all available information; it is arbitrage
eﬃcient if it is impossible for investors to make “excess
proﬁts”, and it is allocationally eﬃcient if prices are set
so that they in some sense maximize everyone’s welfare.
One of the consequences of informational eﬃciency is
that prices should not be predictable. In reality this is
not a bad approximation; even the best trading strategies
exploit only very weak levels of predictability.

The coexistence of the long-memory of supply and de-
mand with market eﬃciency creates an interesting and
as yet unresolved puzzle. Long-memory processes are
highly predictable using a simple linear algorithm. Since
the entrance of new buyers tends to drive the price up,
and the entrance of new sellers tends to drive it down,
this naively suggests that price changes should also be
long-memory, which would violate market eﬃciency. To
prevent this from happening, the agents in the market
must somehow collectively adjust their behavior to oﬀ-
set this, for example by creating an asymmetric response
of prices, so that when there is an excess of new buyers
the price response to new buy orders is smaller than it
is to new sell orders. How this comes about, and why it
comes about, remains a mystery. This may be related to
the cause of clustered volatility.

There is a great deal of other empirical work using
methods and analogies from physics that we do not have
the space to describe in any detail. For example, ran-
dom matrix theory [7] (developed in nuclear physics)
and the use of ultrametric correlations have proved use-
ful for understanding the correlation between the move-
ment of prices of diﬀerent companies. An analogy to
the Omori law for seismic activity after major earth-
quakes has proved to be useful for understanding the
aftermath of large crashes in stock markets, and other
analogies from geophysics has led to a controversial hy-
pothesis about why markets crash [8]. The statistics of
price movements have been noted to bear a striking re-
semblance to those of turbulent ﬂuids, which has led to
what may now be the best empirical models available for
predicting clustered volatility. Such examples speak to
the universality of mathematics in its applications to the
world.

III. MODELING THE BEHAVIOR OF AGENTS

The most fundamental diﬀerence between a physical
system and an economy is that economies are inhab-
ited by people, who have strategic interactions. Because
people think, plan, and make decisions based on their
plans, they are much more complicated to understand
than atoms. This is a problem that physics has never
coped with, and it has caused the mathematical tech-
niques and modeling philosophy in economics to diverge
from those in physics. While this is clearly necessary,
many physicists would argue that the gap is wider than
it should be.

The central approach to the problem of strategic inter-
actions in neoclassical economics is the theory of rational
choice. The economists’ stylized version of individual ra-
tionality is to maximize some measure of one’s personal
(usually material) welfare, having perfect knowledge of
the world and of other agents’ goals and abilities, and
the ability to perform computations of any complexity.
When agent A considers any strategy, agent B knows
that A is considering that strategy, and A knows that
B knows that A knows, and so on. This inﬁnite regress
appears very complicated. However, a key simplifying
result is that in any game there exists at least one Nash
equilibrium, which is a set of strategies with the property
that each is the optimal response to all of the others.

The Nash equilibrium is a ﬁxed point in the space of
strategies, which circumvents the inﬁnite regress prob-
lem by imposing self-consistency as a deﬁning criterion.
Subject to several caveats, rational players who are not
cooperating with each other will choose a Nash strategy.
This is the operational meaning of rational choice. The
assumption that decisions of real human beings can be
approximated in this way dominated economic thinking
about individual choices (called microeconomics) from
1950 until the mid-1980s, though it is clearly implau-
sible for all but the simplest cognitive tasks.
It also
leaves unaddressed the problem of aggregation of individ-
ual choices and the behavior of large populations (called
macroeconomics).

IV. THE QUEST FOR SIMPLE MODELS OF
NON-RATIONAL CHOICE

In the last twenty years economics has begun to chal-
lenge the assumptions of rational choice and perfect mar-
kets by modeling imperfections such as asymmetric infor-
mation, incomplete market structure, and bounded ra-
tionality. Several new schools of thought have emerged.
The behavioral economists attempt to take human psy-
chology into account by studying people’s actual choices
in idealized economic settings. Another school uses ide-
alizations of problem solving and learning ranging from
standard statistical methods to artiﬁcial intelligence to
address the problem of bounded rationality. Agent-based
modeling makes computer simulations based on idealiza-

4

tions of human behaviors and focuses on the complexity
of economic interactions. Yet another approach assumes
that some agents (called noise traders) have extremely
limited reasoning capabilities while others are perfectly
rational. Physicists have joined with many economists in
seeking new theories of non-fully-rational choice, bring-
ing new perspectives to bear on the problem.

An early eﬀort using both agent based modeling and
artiﬁcial intelligence is called the Santa Fe Stock Market.
This grew out of a conference in 1986 organized by Ken
Arrow, Phil Anderson and David Pines [9] that brought
together physicists and economists. Presaging the mod-
ern move toward behavioral economics, the physicists all
expressed disbelief in theories of rational choice and sug-
gested that the economists should take human psychol-
ogy and learning more into account. The Santa Fe Stock
Market was a collaboration between economists, physi-
cists and a computer scientist that grew out of this con-
ference. It replaced the rational agents in an idealized
market setting with an artiﬁcial intelligence model [10].
It showed that this leads to qualitative modiﬁcations of
the statistics of prices, such as fat tailed distributions of
price change and clustered volatility, and suggested that
non-rational behavior plays an important part in gener-
ating these phenomena.

The problem with this approach is that it is compli-
cated, and while it captures some qualitative features of
markets, the path to more quantitative theories is not
clear. Agent based models tend to require ad hoc as-
sumptions that are diﬃcult to validate. The hypothesis
of rational choice, in contrast, has the great virtue that it
is parsimonious, making strong predictions from simple
hypotheses. From this point of view it is more like theo-
ries in physics. This perspective has inspired the search
for other simple parsimonious alternatives. One such ap-
proach is often called zero intelligence. This amounts
to the assumption that agents behave more or less ran-
domly, subject to constraints such as their budget. Zero
intelligence models can be used to study the properties of
market institutions, and to determine which properties
of a market depend on intentionality and which don’t.
This provides a benchmark to avoid getting lost in the
large space of realistic human behaviors. Once a zero
intelligence model has be made, it can be modiﬁed by
incorporating more realistic assumptions, adding a little
intelligence based on empirical observations or models of
learning. Where rational choice enters the wilderness of
bounded rationality from the top, zero intelligence enters
it from the bottom.

The zero intelligence approach can be traced back to
the work of Herbert Simon, a Nobel laureate in economics
and pioneer in artiﬁcial intelligence.
Its main champi-
ons in recent years have been physicists, who have used
analogies to statistical mechanics to develop new mod-
els of markets. A good example is the work of Per Bak,
Maya Paczuski, and Martin Shubik (two physicists and
an economist), who studied the impact of random trading
orders on prices within an idealized model of price for-

mation. They assumed that traders simply place orders
to buy or sell at random above or below the prices of the
most recent transactions. They then modify their orders
from time to time, moving them toward the middle until
they generate a transaction. The result is mathematically
analogous to a reaction diﬀusion model for the reaction
A + B → 0 that was developed by the physicist John
Cardy. While this model is highly unrealistic, with a few
modiﬁcations it produces some qualitative features, such
as heavy tailed price distributions, that resemble their
counterparts in real markets.

FIG. 2: Nondimensionalized market impact as a function
of trading size for eleven stocks from the London Stock Ex-
change. The market impact is deﬁned as the price shift in-
duced immediately upon arrival of a trading order causing a
transaction. Price is deﬁned as the mean of the best quoted
prices to buy and to sell. Nondimensionalization of price and
size is based on a theory that treats arrival and cancellation
of trading orders as a stochastic queuing process, as described
in ref. [11]. The pool is an average of the nondimensionalized
data for all eleven stocks. The collapse seen here indicates
that after appropriate scaling all stocks have a common price
response.

Many variations of the BPS model have now been ex-
plored. One variation simply assumes that agents place
and cancel trading orders at random. After perform-
ing a dimensional analysis based on prices, shares and
time, the resulting model can be shown to obey simple
scaling laws that relate statistical properties of trading
order placement to statistical properties of prices. These
laws are restrictions on state variables similar to the ideal
gas law, but in this case the variables on one side are
properties of trading orders, such as the rates for or-
der placement and cancellation, and the variables on the
other side are statistical properties of prices, such as the
diﬀusion rate in Bachelier’s random walk model. These
scaling laws have been tested on data from the London

5

Stock Exchange and have been shown to be in surpris-
ingly good agreement with it [11]. The model also gives
insight into the shape of supply and demand curves, as
shown in Fig 2.

V. THE EL FAROL BAR PROBLEM AND THE
MINORITY GAME

Another alternative approach is to develop highly sim-
pliﬁed models of strategic interaction that do a better
job of capturing the essence of the collective behavior in
a ﬁnancial market. Brian Arthur’s El Farol bar problem
provides an alternative to conventional game theory. The
name El Farol comes from a bar in Santa Fe that is often
crowded. Each day agents decide whether or not to go
hear music; if there is room in the bar they are happy,
and if it is too crowded they are disappointed. By deﬁ-
nition only a minority of the people can be happy, which
leads to a phenomenon analogous to frustration - some
desires are necessarily unsatisﬁable and as a result an as-
tronomically large number of equilibria can emerge. The
El Farol model was simpliﬁed and abstracted by Challet
and Zhang as the minority game [12], in which an odd
number N of agents repeatedly choose between two alter-
natives, which can be labeled 0 or 1. Their decisions are
made independently and simultaneously. Agents whose
choice is the minority value are rewarded (awarded “pay-
oﬀs”, in game-theoretic terminology). Typically agents
are capable of remembering the outcomes of M prior
rounds of play, and maintain an inventory of s strate-
gies (random lookup tables) dictating a next move for
each history. For example, for M = 2 a possible lookup
table would be 00 → 1, 01 → 0, 10 → 1, 11 → 1, meaning
in the ﬁrst case, “If the previous majority choices were
0 in both previous rounds of the game, choose 1 on the
next round”. The strategy chosen is that with the best
cumulative performance.

Minority games exhibit phase transitions for s ≥ 2,
in the ratio z = 2M /N , of the number of resolvable
pasts to the number of agents, as shown in Fig. 3. For
z < zc a critical value, the population is in a symmetric
phase, where the outcome of the next move is unpre-
dictable from the history of play. For z > zc, in con-
trast, N agents sparsely sample the space of strategies,
the next outcome is predictable, and the population is in
a symmetry-broken phase that can be understood analyt-
ically with replica methods. The variance in the number
of winners about the optimum, (N − 1) /2, measures the
failure of “allocative eﬃciency”, and is minimized at zc.
The minority game is readily extended to incorporate
more features of real ﬁnancial markets, such as payoﬀs
that increase as the size of the winning group gets smaller
(much as buyers or sellers of stocks can reap larger proﬁts
when they are providing the more scarce of supply or de-
mand), or the “grand canonical” version in which players
are permitted to enter and leave. With these enhance-
ments the game self-organizes around the critical point

Figure 6: s=2

N=11    

25      

101     

1001    

slope=−1

102

101

/

N
2
^
v
e
d
S

t

100

10−1

10−2

10−3

10−2

10−1

100

101

102

103

104

z

FIG. 3: The variance of the number of winners in the minority
game is plotted against an order parameter z = 2
/N . For
z < zc the variance is high, indicating that the game is highly
ineﬃcient in the sense that the minority group is much smaller
than it needs to be. The variance reaches a minimum at
z = zc ≈ 0.4, and when z is large it approaches the value it
would have if all agents make random choices (from Savit et
al. - see reference [12]).

M

zc, the payoﬀ series exhibits ﬂuctuations that display
clustered volatility, and they have a distribution with a
power law tail, reminiscent of a real market. The minor-
ity game provides a fascinating example of how a very
simple game can display a rich set of properties as soon
as one moves away from the rational choice paradigm.

VI. ENTROPY METHODS

Finance is not the only area of economics where physi-
cists are active.
In economics as in physics it is tra-
ditional to distinguish open from closed systems, which
give rise to diﬀerent notions of equilibrium. Markets con-
sidered merely as conduits for goods produced or con-
sumed elsewhere are described with theories of “partial
equilibrium”, largely speciﬁed by open-system boundary
conditions. Financial markets are open in this sense.
Economists also try to determine the “general equilibria”
of whole societies, taking into account not only trade, but
production, consumption, and to some extent regulation
by government.

6

The understanding of relaxation to equilibrium,
in-
cluding when equilibria are possible and whether they are
unique, has grown in economics and in physics together.
In both ﬁelds mechanical models were used ﬁrst, followed
by statistical explanations [13]. Some recent work [14]
has shown which subset of economic decision problems
have an identical structure to that of classical thermo-
dynamics, including the emergence of a phenomenolog-
ical principle equivalent to entropy maximization, while
the more general equilibration problems usually consid-
ered by economists correspond to physical problems with
many equilibria, such as granular, glassy, or hysteretic
relaxation. The idea that equilibria correspond to sta-
tistically most-probable sets of conﬁgurations has led to
attempts to deﬁne price formation in statistical terms.
A related observation, that income distribution seems
consistent with various forms of entropy maximization,
recasts the problem of understanding income inequality,
and interpreting how much it really tells about the social
forces aﬀecting incomes (see box A).

We expect that maximum-ignorance principles will
grow into a conceptual foundation in economics as they
have in physics, and that with this change, the roles of
symmetry, conservation laws, and scaling will become in-
creasingly important [15]. Eﬀorts to explain which as-
pects of market function or regulatory structure converge
on predictable forms, relatively free of historical contin-
gency, are likely to require characterization in these more
basic terms.

VII. FUTURE DIRECTIONS

Within the next few years we expect that in some
physics and economics departments a basic course teach-
ing the essential elements of both physics and economics
will be designed (much as in biophysics; see Physics
Today March 2005). We believe physics will continue
to contribute to economics in a variety of diﬀerent di-
rections, ranging from macroeconomics to market mi-
crostructure, and that such work will have increasing im-
plications for economic policy making.

One area of opportunity, where the applicability of
physics might not be at all obvious a priori, concerns
the construction of economic indices, such as the Con-
sumer Price Index or the Dow Jones Industrial Average.
Though these indices provide only crude one dimensional
summaries of very complex phenomena, they play an im-
portant role in economic decision making. For example,
pension and wage payments are referenced to the CPI.
Such indices are currently constructed using essentially
ad hoc methods. We believe that the accuracy of such
indices could be improved by careful thinking in terms of
dimensional analysis, combined with better data analy-
sis correlating prices and other factors to the phenomena,
such as wages and pensions, for which the indices are de-
signed. This is ultimately related to the question of why
the economy exhibits so many scale free behaviors, such

7

US
Japan

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

y
t
i
l
i

b
a
b
o
r
p
 
e
v
i
t
a
u
m
u
C

l

−8

10

3
10

as the distribution of wealth or the size of ﬁrms. To
shed light on this we need a better understanding of the
natural dimensions of economic life, and the use of sys-
tematic dimensional analysis is likely to be very useful
in revealing this. Dimensional and scaling methods were
a cornerstone in the understanding of complex phenom-
ena like turbulence in ﬂuids, and all the constituents that
make ﬂuid ﬂow complex – long time correlations, nonlin-
earity, and chaos – are likely to be even greater factors
in the economy.

At the other end of the spectrum, ideas from statistical
mechanics could make practical contributions to prob-
lems in market microstructure. For market design, for
example, some physics-style models suggest that chang-
ing the rules to create incentives for patient trading or-
ders vs. those that demand immediate transactions could
lower the volatility of prices. A related practical prob-
lem concerns the optimal strategy for market makers,
i.e. agents that simultaneously buy and sell, and make
a proﬁt by taking the diﬀerence. Though markets are
increasingly electronic, the design of automated market
makers is still done in a more or less ad hoc manner. The
opportunity is ripe to create a theory for market making
based on methods from statistical mechanics. This could
result in lowering transaction costs and generally making
markets more eﬃcient.

We are reminded that several key ideas in physics
are actually of economic origin. A prejudice that the
books should balance was likely responsible for Joule’s
accounting for the energy content of heat before it was
well-supported by data. The concept of a “currency”,
which we still think of primarily in economic metaphor,
guides our understanding of the role of energy in complex
systems and particularly in biochemistry. Understand-
ing the dynamics and statistical mechanics of agency
promises similarly to expand the conceptual scope of
physics.

APPENDIX A: INCOME DISTRIBUTIONS (BOX)

The ﬁrst identiﬁcation of a power law distribution –
in any ﬁeld – was made by Vilfredo Pareto in 1897 for
the distribution of income among the highest earning few
percent of inhabitants of the UK, and all income distri-
butions asymptotically of this form are known in eco-
nomics as “Pareto distributions”. Subsequent studies by
Pareto for Prussia, Saxony, Paris, and few Italian cities
conﬁrmed these results, which continue to hold up very
well (see Fig. 4). More recent studies [16] have shown
that not only is the income of the wealthy regular, but
so is the income of the majority of wage earners, and the
two groups follow diﬀerent distributions. The low- and
medium-income body of the distribution is either expo-
nential or lognormal (variable among data sets), with a
transition to the Pareto law for large incomes, at a level
that varies with time, tax laws, and other factors, as yet
unknown.

4
10

5
10

6
10

7
10

8
10

Income in 1999 US dollars

FIG. 4: Modern examples of the Pareto distribution of in-
comes, from Ref. [16]. Data are collected from federal income
tax reporting sources, and are more complete for high incomes
in Japan than in the U. S. A systematic diﬀerence in recent
data is that Japanese low incomes are better ﬁt by lognor-
mal distribution, while U. S. low incomes are better ﬁt by
exponential.

As striking as the fact that the large-income distribu-
tion is scale free, is the fact that the overall distribution
is so featureless, being described by four (or ﬁve) param-
eters: mean income, Pareto exponent, transition point
between low- and high-income ranges, and the exponen-
tial constant (or mean and variance of the lognormal) in
the low range. Pareto, lognormal, and exponential dis-
tributions are all limiting distributions of simple random
processes, and can also be derived as maximum-entropy
distributions for either income or its logarithm, subject
to appropriate boundary conditions on the (arithmetic or
geometric) mean income [17].

Income distribution is a hot topic economically and
politically, because it lies at the heart of a society’s
notions of egalitarianism, opportunity, or social insur-
ance. Not surprisingly, causes of income inequality are
asserted, such as distinctions between capital owner-
ship and wage labor, with major policy implications.
Maximum-entropy interpretations of income distribution
place conceptual as well as quantitative bounds on these
arguments. They suggest that the many detailed features
of a society that could in principle aﬀect incomes some-
how average so that their individual characteristic scales
are not imprinted on the aggregate distribution; the ulti-
mate constraints may be conservation laws or boundary
conditions reﬂected in at most a few parameters. Such
featureless averaging, like the scaling relations we have
noted above, may suggest that a form of universality clas-
siﬁcation is fundamental to understanding economics, as
it has been to thermodynamics and ﬁeld theory.

APPENDIX B: OPTION PRICING (BOX)

Bachelier’s random walk was a triumph of quantita-
tive ﬁnance, and became the basis of modern portfolio
analysis, and later the Black-Scholes model for option
pricing [5]. The α and β coeﬃcients published in ev-
ery security analysis are mean and covariance coeﬃcients
from ﬁts to a random walk. However, the heavy tails of
real price ﬂuctuations, under-predicted by the Gaussian
distribution resulting from accumulation of an uncorre-
lated random walk, can lead to disastrous mis-estimates
of risk. This has been an important problem in ﬁnancial
mathematics which has received a great deal of attention.
More recent work by physicists extends analytic meth-
ods for pricing options to take the heavy tails and volatil-
Inspired by the
ity bursts of real prices into account.
work of Constantino Tsallis on non-extensive statistical
mechanics [18], (see Fig. 5) Lisa Borland [19] has de-
veloped a new pricing formula that corrects the stan-
dard Black-Scholes model. For three decades option-
pricing practitioners have recognized that random-walk
estimates were too conservative, and compensated by al-
tering the parameters of the Black-Scholes model depend-
ing on the strike price of the option. The “implied volatil-
ity” assigned in this way makes a well-known “smile”
when plotted versus the strike prices (see Fig. 6). The
Borland option-pricing formula provides a rational ba-
sis for pricing of rare events, and nicely reproduces the
volatility smile. While there are a large number of other
generalizations of the Black-Scholes theory that address
the problem of the smile, Borland’s has the signiﬁcant
advantage that it gives a closed form solution.

The self-consistency condition on which all rational op-
tion pricing is based – arbitrage-free hedging of risk –
is a classically reductionist principle relating derivatives
to their underlying assets.
It is noteworthy that eco-
nomic practice has chosen to adhere to the speciﬁc Black-
Scholes formula based on an empirically invalid model of

y
t
i
s
n
e
D
 
y
t
i
l
i

b
a
b
o
r
P

100

10-1

10-2

10-3

10-4

10-5

10-6

10-7

8

the underlying, and to introduce the phenomenological
curve of “implied volatility” to bring the formula in line
with data. The Borland construction avoids such mixing
of reductionism and phenomenology; its single additional
parameter describes the observed ﬂuctuations in the un-
derlying asset price, separating the problem of explaining
these from that of pricing their derivatives.

10 NASDAQ stocks, 1 min, 2001

q = 1.43

-20

-10

0
Normalized Return

10

20

FIG. 5: Distributions of log returns for 10 Nasdaq high-
volume stocks. Returns are calculated as log p(t+τ )−log p(t),
where p is the transaction price and τ is one minute, and are
normalized by the sample standard deviation. Also shown is
the student distribution (solid line) which provides a good ﬁt
to the data (from ref. [19]).

[1] B. Mandelbrot.

Fractals and Scaling in Finance.

Springer-Verlag, New York, 1997.
[2] R. N. Mantegna and H. E. Stanley.

Introduction to
Econophysics: Correlations and Complexity in Finance.
Cambridge University Press, Cambridge, 1999.

[3] Gabaix, Xavier, Parameswaran Gopikrishnan, Vasiliki
Plerou, H. Eugene Stanley ”A Theory of Power Law
Distributions in Financial Market Fluctuations”, Nature,
2003, vol. 423, p. 267-70.

[4] M. H. R. Stanley, L. A. N. Amaral, S. V. Buldyrev, S.
Havlin, H. Leschhorn, P. Maass, M. A. Salinger, and H.
E. Stanley, ”Scaling Behavior in the Growth of Compa-
nies,” Nature 379, 804-806 (1996).

[5] J-P. Bouchaud and M. Potters. Theory of Financial
Risk:From Statistical Physics to Risk Management. Cam-
bridge University Press, Cambridge, 2000.

[6] J-P. Bouchaud, Y. Gefen, M. Potters, and M. Wyart,
“Fluctuations and response in ﬁnancial markets: The

subtle nature of “random” price changes”, Quantitative
Finance 4(2): 176-190 (2004); F. Lillo and J. D. Farmer,
“The long memory of the eﬃcient market”, Studies in
Nonlinear Dynamics and Econometrics 8 (3), Article
1(2004).

[7] Z. Burda, J. Jurkiewicz, and M. A. Nowak, “Is Econo-

physics a Solid Science?”, cond-mat 0301096.

[8] Didier Sornette. Why Stock Markets Crash: Critical
Events in Complex Financial Systems. Princeton Uni-
versity Press, Princeton, 2002.

[9] P. W. Anderson, K. J. Arrow, and Pines D., editors.
The Economy as an Evolving Complex System. Addison-
Wesley, Redwood City, 1988.

[10] W. B. Arthur, J. H. Holland, B. LeBaron, R. Palmer,
and P. Tayler. Asset pricing under endogenous expec-
tations in an artiﬁcial stock market. In W. B. Arthur,
S. N. Durlauf, and D. H. Lane, editors, The Economy as
an Evolving Complex System II, pages 15–44. Addison-

9

Wesley, Redwood City, 1997.

[11] J. D. Farmer, P. Patelli, and Ilija Zovko. The predictive
power of zero intelligence in ﬁnancial markets. Proceed-
ings of the National Academy of Sciences of the United
States of America, 102(6):2254–2259, 2005.

[12] Damien Challet, Matteo Marsili, and Yi-Cheng zhang.
Minority Games. Oxford University Press, Oxford, 2005;
(if we have to discard one it should be the following) N. F.
Johnson, P. Jeﬀries, and P. M. Hui. Financial Market
Complexity. Oxford University Press, Oxford, 2003.
[13] P. Mirowski. More Heat than Light: Economics as Social
Physics, Physics as Nature’s Economics. Historical Per-
spectives on Modern Economics. Cambridge University
Press, Cambridge, 1989.

[14] E. Smith and D. K. Foley, “Is utility theory so diﬀer-
ent from thermodynamics?”, SFI preprint # 02-04-016
(2004).

[15] E. Smith and M. Shubik “Strategic Freedom, Constraint,
and Symmetry in One-Period Markets with Cash and
Credit Payment”, Economic Theory 25, 513-551 (2005);
M. Shubik and E. Smith, “The physics of time and di-
mension in the economics of ﬁnancial control”, Physica
A 340, 656-667 (2004).

[16] M. Nirei and W. Souma, “Income Distribution and
Stochastic Multiplicative Process with Reset Events”, in
Gallegati, Kirman, Marsili eds, The Complex Dynam-
ics of Economic Interaction, Springer, 2003; Figure from
M. Nirei and W. Souma, “Two factor model of income
distribution dynamics”, SFI preprint # 04-10-029.
[17] A. Dragulescu and V. M. Yakovenko, “Statistical me-

chanics of money”, Eur. Phys. J. B 17, 723-729 (2000).

[18] M. Gell-Mann and C. Tsallis. Nonextensive Entropy-
Interdisciplinary Applications. Oxford University Press,
New York, 2004.

[19] L. Borland, “A theory of non-Gaussian option pricing,

Quant. Fin. 2, 415-431 (2002).

FIG. 6: A typical example of the ”smile” of the implied
volatility (sigma) needed in the Black-Scholes pricing formula
to correctly price options, versus the strike price. The exam-
ple shown here corresponds to that of June 2001 call options
on S&P 100 futures (OX), with 10 days to expiration (sym-
bols). This is well-ﬁt by the theory developed by Borland
(line).


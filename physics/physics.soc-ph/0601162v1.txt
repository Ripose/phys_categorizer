6
0
0
2
 
n
a
J
 
1
2
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
2
6
1
1
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Statistical Mechanics of Online Learning for Ensemble Teachers

Seiji MIYOSHI∗and Masato OKADA†

January 11, 2014

Abstract

We analyze the generalization performance of a student in a model composed of linear perceptrons: a
true teacher, ensemble teachers, and the student. Calculating the generalization error of the student
analytically using statistical mechanics in the framework of on-line learning, it is proven that when
learning rate η < 1, the larger the number K and the variety of the ensemble teachers are, the smaller
the generalization error is. On the other hand, when η > 1, the properties are completely reversed. If
the variety of the ensemble teachers is rich enough, the direction cosine between the true teacher and the
student becomes unity in the limit of η

.
→ ∞
keywords: ensemble teachers, on-line learning, generalization error, statistical mechanics, learning

0 and K

→

rate

1

Introduction

Learning is to infer the underlying rules that dominate data generation using observed data. Observed
data are input-output pairs from a teacher and are called examples. Learning can be roughly classiﬁed
into batch learning and on-line learning [1]. In batch learning, given examples are used repeatedly. In this
paradigm, a student becomes to give correct answers after training if the student has adequate freedom.
However, it is necessary to have a long amount of time and a large memory in which to store many
examples. On the contrary, in online learning examples used once are discarded. In this case, a student
cannot give correct answers for all examples used in training. However, there are merits, for example,
a large memory for storing many examples isn’t necessary, and it is possible to follow a time variant
teacher.

Recently, we [5, 6] analyzed the generalization performance of ensemble learning [2, 3, 4] in a framework
of on-line learning using a statistical mechanical method [1, 8]. Using the same method, we also analyzed
the generalization performance of a student supervised by a moving teacher that goes around a true
teacher[7]. As a result, it was proven that the generalization error of a student can be smaller than a
moving teacher, even if the student only uses examples from the moving teacher. In an actual human
society, a teacher observed by a student doesn’t always present the correct answer. In many cases, the
teacher is learning and continues to change. Therefore, the analysis of such a model is interesting for
considering the analogies between statistical learning theories and an actual human society.

On the other hand, in most cases in an actual human society a student can observe examples from
two or more teachers who diﬀer from each other. Therefore, we analyze the generalization performance of
such a model and discuss the use of imperfect teachers in this paper. That is, we consider a true teacher
and K teachers called ensemble teachers who exist around the true teacher. A student uses input-output
pairs from ensemble teachers in turn or randomly. In this paper, we treat a model in which all of the true
teacher, the ensemble teachers and the student are linear perceptrons[5] with noises. We obtain order

∗Department of Electronic Engineering, Kobe City College of Technology, 8–3 Gakuen-higashimachi, Nishi-ku, Kobe-shi,

651–2194 E-mail address: miyoshi@kobe-kosen.ac.jp

†Division of Transdisciplinary Sciences, Graduate School of Frontier Sciences, The University of Tokyo, 5–1–5 Kashi-
wanoha, Kashiwa-shi, Chiba, 277–8561, RIKEN Brain Science Institute, 2–1 Hirosawa, Wako-shi, Saitama, 351–0198 JST
PRESTO, 5–1–5 Kashiwanoha, Kashiwa-shi, Chiba, 277–8561

1

parameters and generalization errors analytically in the framework of on-line learning using a statistical
mechanical method. As a result, it is proven that when student’s learning rate η < 1, the larger the
number K and the variety of the ensemble teachers are, the smaller the student’s generalization error
is. On the other hand, when η > 1, the properties are completely reversed. If the variety of ensemble
teachers is rich enough, the direction cosine between the true teacher and the student becomes unity in
the limit of η

0 and K

→

.
→ ∞

2 Model

In this paper, we consider a true teacher, K ensemble teachers and a student. They are all linear
perceptrons with connection weights A, Bk and J , respectively. Here, k = 1, . . . , K. For simplicity, the
connection weight of the true teacher, the ensemble teachers and the student are simply called the true
teacher, the ensemble teachers and the student, respectively. True teacher A = (A1, . . . , AN ), ensemble
teachers Bk = (Bk1, . . . , BkN ), student J = (J1, . . . , JN ) and input x = (x1, . . . , xN ) are N dimensional
vectors. Each component Ai of A is drawn from
(0, 1) denotes
Gaussian distribution with a mean of zero and variance unity. Some components Bki are equal to Ai
Ai is independent from
multiplied by –1, the others are equal to Ai. Which component Bki is equal to
(0, 1). Bki is also ﬁxed. The direction cosine between Bk and
the value of Ai. Hence, Bki also obeys
A is RBk and that between Bk and Bk′ is qkk′ . Each of the components J 0
i of the initial value J 0 of J
(0, 1) independently. The direction cosine between J and A is RJ and that between J
are drawn from
and Bk is RBkJ . Each component xi of x is drawn from

(0, 1) independently and ﬁxed, where

(0, 1/N ) independently. Thus,

N

N

N

N

−

where

denotes a mean.

h·i

direction cosines qkk′ , RBk, RJ and RBkJ .

Figure 1 illustrates the relationship among true teacher A, ensemble teachers Bk, student J and

In this paper, the thermodynamic limit N

is also treated. Therefore,

A
k

k

= √N ,

Bk

k

k

J 0

k

k

= √N ,

x
k

k

= 1.

→ ∞
= √N ,

J

Generally, norm
√N are introduced and called the length of the student. That is,
time step.

of the student changes as time step proceeds. Therefore, ratios lm of the norm to
= lm√N , where m denotes the

J m

k

k

k

k

The outputs of the true teacher, the ensemble teachers, and the student are ym + nm

A , vm

k + nm

Bk and

umlm + nm

J , respectively. Here,

Ai
h
Bki
h

i

i

J 0
i

(cid:11)
(cid:10)
xi
i
h

RBk =

RJ =

N

= 0,

(Ai)2

= 1,

= 0,

D

E
(Bki)2

= 1,

= 0,

D

J 0
i

2

E

= 1,

= 0,

E

=

1
N

,

E
, qkk′ =

k
, RBkJ =

D(cid:0)
(cid:1)
(xi)2
Bk
D
·
Bk
kk
J
·
J
kk

k

A
A
A
A

k

k

,

Bk
Bk
Bk
Bk

Bk′
Bk′
J
J

k

·
kk
·
kk

k

k

k
,

ym = A
·
k = Bk
vm
umlm = J m

nm
A ∼ N
nm
Bk ∼ N
nm
J ∼ N

·

xm,
xm,
xm,
·
0, σ2
A
0, σ2
(cid:0)
0, σ2
(cid:0)
J

(cid:0)

(cid:1)

,

.
(cid:1)

(cid:1)
Bk

,

2

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)
(11)

(12)

(13)

Figure 1: True teacher A, ensemble teachers Bk and student J . qkk′ , RJ , RBk and RBkJ are direction
cosines.

That is, the outputs of the true teacher, the ensemble teachers and the student include independent
J , respectively. Then, ym, vm, and um of Eqs. (8)–(10)
Gaussian noises with variances of σ2
obey Gaussian distributions with a mean of zero and variance unity.

Bk, and σ2

A, σ2

Let us deﬁne error ǫBk between true teacher A and each member Bk of the ensemble teachers by the

squared errors of their outputs:

A −
In the same manner, let us deﬁne error ǫBkJ between each member Bk of the ensemble teachers and

ǫm
Bk ≡

1
2

(ym + nm

vm
k −

Bk)2 .
nm

student J by the squared errors of their outputs:

Bk −
Student J adopts the gradient method as a learning rule and uses input x and an output of one of

−

ǫm
BkJ ≡

1
2

(vm

k + nm

umlm

J )2 .
nm

the K ensemble teachers Bk in turn or randomly for updates. That is,

J m+1 = J m

η

−
= J m + η (vm

∂ǫm
BkJ
∂J m
k + nm

Bk −

umlm

J ) xm,
nm

−

where η denotes the learning rate of the student and is a constant number. In cases where the student
uses K ensemble teachers in turn, k = mod (m, K) + 1. Here, mod (m, K) denotes the remainder of m
divided by K. On the other hand, in random cases, k is a uniform random integer that takes one of
1, 2, . . . , K.

Generalizing the learning rules, Eq. (17) can be expressed as

J m+1 = J m + fkxm
= J m + f (vm

k + nm

Bk, umlm + nm

J ) xm,

where f denotes a function that represents the update amount and is determined by the learning rule.

In addition, let us deﬁne error ǫJ between true teacher A and student J by the squared error of their

outputs:

ǫm
J ≡

1
2

(ym + nm

umlm

A −

J )2 .
nm

−

(14)

(15)

(16)

(17)

(18)
(19)

(20)

3

3 Theory

3.1 Generalization error

One purpose of a statistical learning theory is to theoretically obtain generalization errors. Since gener-
alization error is the mean of errors for the true teacher over the distribution of new input and noises,
generalization error ǫBkg of each member Bk of the ensemble teachers and ǫJg of student J are calcu-
lated as follows. Superscripts m, which represent the time steps, are omitted for simplicity unless stated
otherwise.

ǫBkg =

dxdnAdnBkP (x, nA, nBk) ǫBk

dydvkdnAdnBkP (y, vk, nA, nBk)

(y + nA

vk

1
2

nBk)2

−

−

2RBk + 2 + σ2

A + σ2
Bk

−
(cid:1)
(cid:0)
dxdnAdnJ P (x, nA, nJ ) ǫJ

,

ǫJg =

dydudnAdnJ P (y, u, nA, nJ )

(y + nA

ul

−

−

nJ )2

2RJ l + l2 + 1 + σ2

A + σ2
J

.

−
(cid:0)

1
2

(cid:1)

Z

Z
1
2

Z

Z
1
2

=

=

=

=

Here, integrations have been executed using the following: y, vk and u obey

(0, 1). The covariance
between y and vk is RBk, that between vk and u is RBkJ , and that between y and u is RJ . All nA, nBk,
and nJ are independent from other probabilistic variables.

N

3.2 Diﬀerential equations for order parameters and their analytical solutions

To simplify analysis, the following auxiliary order parameters are introduced:

Simultaneous diﬀerential equations in deterministic forms [8], which describe the dynamical behaviors
of order parameters, have been obtained based on self-averaging in the thermodynamic limits as follows:

Here, dimension N has been treated to be suﬃciently greater than the number of ensemble teachers
K. Time t = m/N , that is, time step m normalized by dimension N . Note that the above diﬀerential
equations are identical whether the K ensemble teachers are used in turn or randomly.

Since linear perceptrons are treated in this paper, the sample averages that appeared in the above

equations can be easily calculated as follows:

rJ
rBkJ

RJ l,
RBkJ l.

≡

≡

K

drBkJ
dt

drJ
dt

dl
dt

=

=

=

1
K

1
K

1
K

,
i

fk′ vk
h
k′=1
X
K

fky
h

,
i

k=1
X
K

fku
h
k=1 (cid:18)
X

i

+

1
2l h

f 2
k i

.

(cid:19)

fku
h

i

f 2
k i
h
fky
i
h

= η

rBkJ

l

,

l −

(cid:16)
l2
= η2
−
= η (RBk

(cid:17)

2rBkJ + 1 + σ2
rJ ) ,

(cid:0)

−

Bk + σ2
J

,

(cid:1)

4

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

K

1
K

fk′ vk
h
k′=1
X
Since all components Ai, J 0

i

= η

rBkJ +

 −

K

1
K

qkk′

.

!

k′=1
X
i of true teacher A, and the initial student J 0 are drawn from

(0, 1)
is also treated, they are orthogonal to each

N

independently and because the thermodynamic limit N
other in the initial state. That is,

→ ∞

RJ = 0 when t = 0.

l = 1 when t = 0.

By using Eqs. (32)–(37), simultaneous diﬀerential equations Eqs. (29)–(31) can be solved analytically

In addition,

as follows:

where

rBkJ =

qkk′

1

−ηt

e

,

,

(cid:1)

RBk

1

−ηt

e

K

k′=1
X
K

1
K

1
K

k=1
X
1

2

η

−
1 +

(cid:20)

2 (1

(cid:2)

1

2

η

−

−

−

(cid:0)

(cid:0)

−

(cid:0)

(cid:0)

rJ =

l2 =

+

(cid:1)
η) ¯q + η

1 + ¯σ2

B + σ2
J

η

1 + ¯σ2

(cid:0)
B + σ2
J

(cid:1)(cid:3)
eη(η−2)t

2¯qe

−ηt,

−

2¯q

−

(cid:21)
(cid:1)

(cid:1)

4 Results and Discussion

In this section, we treat the case where direction cosines RBk between the ensemble teachers and the true
teacher, direction cosines qkk′ among the ensemble teachers and variances σ2
Bk of the noises of ensemble
teachers are uniform. That is,

In this case, Eqs. (41) and (42) are expressed as

The dynamical behaviors of generalization errors ǫJg have been analytically obtained by solving Eqs.
(26), (27) and (38)–(47). Figure 2 shows the analytical results and the corresponding simulation results,
where N = 2000. In computer simulations, K ensemble teachers are used in turn. ǫJg was obtained by
averaging the squared errors for 104 random inputs at each time step. Generalization error ǫBg of one of
the ensemble teachers is also shown. The dynamical behaviors of R and l are shown in Fig. 3.

¯q =

qkk′ ,

1
K 2

1
K

K

K

k=1
X
K

k′=1
X
σ2
Bk.

k=1
X

¯σ2
B =

RBk = RB, k = 1, . . . , K,

= k′,
q, k
1, k = k′,

qkk′ =

(cid:26)
Bk = σ2
σ2
B.

q

,

1

−
K

¯q = q +

B = σ2
¯σ2
B.

5

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

6
eg of J (q=1.00)
eg of J (q=0.80)
eg of J (q=0.60)
eg of J (q=0.49)
eg of B

r
o
r
r
E
 
n
o
i
t
a
z
i
l
a
r
e
n
e
G

 1.2

 1.1

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

s
r
e
t
e
m
a
r
a
P
 
r
e
d
r
O

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 0

 5

 15

 20

 10

t=m/N

Figure 2: Dynamical behaviors of generalization errors ǫJg. Theory and computer simulations. Conditions
other than q are η = 0.3, K = 3, RB = 0.7, σ2

B = 0.1 and σ2

A = 0.0, σ2

J = 0.2.

In these ﬁgures, the curves represent theoretical results. The dots represent simulation results. Con-
ditions other than q are common: η = 0.3, K = 3, RB = 0.7, σ2
J = 0.2. Figure 2
shows that the smaller q is, that is, the richer the variety of the ensemble teachers is, the smaller gener-
alization error ǫJg of the student is. Especially in the cases of q = 0.6 and q = 0.49, the generalization
error of the student becomes smaller than a member of the ensemble teachers after t
5. This means
that the student in this model can become more clever than each member of the ensemble teachers even
though the student only uses the input-output pairs of members of the ensemble teachers. Figure 3 shows
that the larger the variety of the ensemble teachers is, the larger direction cosine RJ is and the smaller
length l of the student is. The reason minimum value 0.49 of q is taken as the squared value of RB = 0.7
in Figs. 2 and 3 is described later.

B = 0.1 and σ2

A = 0.0, σ2

≈

l (q=1.00)
l (q=0.80)
l (q=0.60)
l (q=0.49)
R_J (q=1.00)
R_J (q=0.80)
R_J (q=0.60)
R_J (q=0.49)

 5

 15

 20

 10

t=m/N

Figure 3: Dynamical behaviors of RJ and l. Theory and computer simulations. Conditions other than q
are η = 0.3, K = 3, RB = 0.7, σ2

B = 0.1 and σ2

A = 0.0, σ2

J = 0.2.

In Figs. 2 and 3, ǫJg, RJ and l almost seem to reach a steady state by t = 20. The macroscopic

6

behaviors of t
can be understood theoretically since the order parameters have been obtained
analytically. Focusing on the signs of the powers of the exponential functions in Eqs. (38)–(40), we can
see that ǫJg and l diverge if η < 0 or η > 2. The steady state values of rBkJ , rJ and l2 in the case of
0 < η < 2 can be easily obtained by substituting t

in Eqs. (38)–(40) as follows:

→ ∞

→ ∞

q

,

1

−
K

rBkJ

rJ

l2

q +

RB,
1

2

→

→

→

−
= q +

2 (1

η)

q +

η
1

(cid:18)

−
K

q

+

−

(cid:18)

η

−

2

η

(cid:18)

1

q

−
K

(1

−

(cid:19)
q)(K
K

(cid:0)
1)

−

+ η

1 + σ2

B + σ2
J

+ σ2

B + σ2
J

(cid:19)
(cid:1)

.

(cid:19)

(48)

(49)

(50)

(51)

Equations (26), (27) and (48)–(51) show the following: in the case of η = 1, the steady value of length
l is independent from the number K of teachers and direction cosine q among the ensemble teachers.
Therefore, the steady value of generalization error ǫJg and direction cosine RJ are independent from K
and q in this case. In the case of 0 < η < 1, the smaller q is or the larger K is, the smaller the steady
values of l and ǫJg are and the larger the steady value of RJ is. In the case of 1 < η < 2, on the contrary,
the smaller q is or the larger K is, the larger the steady values of l and ǫJg are and the smaller the steady
value of RJ is. That is, in the case of η < 1, the more teachers exist and the richer the variety of teachers
is, the more clever the student can become. On the contrary, in the case of η > 1, the number of teachers
should be small and the variety of teachers should be low for the student to become clever.

In addition, since l

In the right hand side of Eq. (51), since the second and the third terms are positive, the steady
value of l is larger than √q.
, Eqs.
√q in the limit of η
RB/√q. On the other hand, when S and T are generated independently
(27) and (49) show RJ
under conditions where the direction cosine between S and P and between T and P are both R0,
where S, T and P are high dimensional vectors, the direction cosine between S and T is q0 = R2
0,
as shown in the appendix. Therefore, if ensemble teachers have enough variety that they have been
generated independently under the condition that all direction cosines between ensemble teachers and
the true teacher are RB, RB/√q = 1, then direction cosine RJ between the student and the true teacher
approaches unity regardless of the variances of noises in the limit of η

0 and K

0 and K

→ ∞

→

→

→

Figures 4–7 show the relationships between learning rate η and ǫJg, RJ . In Figs 4 and 5, K = 3 and
is ﬁxed. In Figs 6 and 7, q = 0.49 and is ﬁxed. Conditions other than K and q are σ2
J = 0.0
and RB = 0.7. Computer simulations have been executed using η = 0.3, 0.6, 1.0, 1.4 and 1.7. The values
on t = 20 are plotted for the simulations and considered to have already reached a steady state.

B = σ2

A = σ2

→

.
→ ∞

These ﬁgures show the following: the smaller learning rate η is, the smaller generalization error ǫJg
is and the larger direction cosine RJ is. Needless to say, when η is small, learning is slow. Therefore,
residual generalization error and learning speed are in a relationship tradeoﬀ. The phase transition in
which ǫJg diverges and RJ becomes zero on η = 2 is shown. In the case of η < 1, the larger K is or the
smaller q is, that is, the richer the variety of ensemble teachers is, the smaller ǫJg is and the larger RJ
is. On the contrary, the properties are completely reversed in the case of η > 1.

As described above, learning properties are dramatically changed with learning rate η. It is diﬃcult to
explain the reason qualitatively. Here, we try to explain the reason intuitively by showing the geometrical
meaning of η. Figures 8(a)–(c) show the updates of η = 0.5, η = 1 and η = 2, respectively. Here, the
noises are ignored for simplicity. Needless to say, teacher Bk itself cannot be observed directly and only
In addition, since the projections from J m+1
output v can be observed when student J is updated.
to xm and from Bk to xm are equal in the case of η = 1, as shown in Fig. 8(b), η = 1 is a special
condition where the student uses up the information obtained from input xm. In the case of η < 1, the
update is short. Since in a sense this fact helps balance the information from the ensemble teachers, the
generalization error of the student is improved when the number K of teachers is large and their variety
is rich. On the other hand, the update is excessive when η > 1. Therefore, the student is shaken or
swung, and its generalization performance worsens when K is large and the variety is rich. In addition,
the reason that learning diverges if η < 0 or η > 2 can be understood intuitively from Fig. 8: distance
, measured by the projections to xm between student J m+1 after the update

umlm)xm

1)(vm

(η

k

−

−

k

7

q=1.00
q=0.80
q=0.60
q=0.49

r
o
r
r
E
 
n
o
i
t
a
z
i
l
a
r
e
n
e
G

R

 10

 1

 0.1

 0

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

q=1.00
q=0.80
q=0.60
q=0.49

 1

Eta

8

 0.5

 1.5

 2

 1

Eta

Figure 4: Steady value of generalization error ǫJg in the case of K = 3. Theory and computer simulations.
Conditions other than K and q are σ2

J = 0.0 and RB = 0.7.

B = σ2

A = σ2

 0

 0.5

 1.5

 2

Figure 5: Steady value of direction cosine RJ in the case of K = 3. Theory and computer simulations.
Conditions other than K and q are σ2

J = 0.0 and RB = 0.7.

B = σ2

A = σ2

K=1
K=2
K=3
K=10
K=100

 10

 1

 0.1

 0.01

 0

r
o
r
r
E
 
n
o
i
t
a
z
i
l
a
r
e
n
e
G

R

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

K=1
K=2
K=3
K=10
K=100

 1

Eta

9

 0.5

 1.5

 2

 1

Eta

Figure 6: Steady value of generalization error ǫJg in the case of q = 0.49. Theory and computer simula-
tions. Conditions other than K and q are σ2
B = σ2

J = 0.0 and RB = 0.7.

A = σ2

 0

 0.5

 1.5

 2

Figure 7: Steady value of direction cosine RJ in the case of q = 0.49. Theory and computer simulations.
Conditions other than K and q are σ2

J = 0.0 and RB = 0.7.

B = σ2

A = σ2

and teacher Bk, is larger than distance
teacher Bk in the case of η < 0 or η > 2. Therefore, the learning diverges.

umlm)xm

(vm

−

k

k

between student J m before the update and

Figure 8: Geometric meaning of learning rate η

5 Conclusion

We analyzed the generalization performance of a student in a model composed of linear perceptrons: a
true teacher, ensemble teachers, and the student. The generalization error of the student was analytically
calculated using statistical mechanics in the framework of online learning, proving that when learning
rate η < 1, the larger the number K and the variety of the ensemble teachers are, the smaller the
generalization error is. On the other hand, when η > 1, the properties are completely reversed. If the
variety of ensemble teachers is rich enough, the direction cosine between the true teacher and the student
becomes unity in the limit of η

0 and K

→

.
→ ∞

Acknowledgments

This research was partially supported by the Ministry of Education, Culture, Sports, Science, and Tech-
nology of Japan, with Grants-in-Aid for Scientiﬁc Research 14084212, 15500151 and 16500093.

A Direction cosine q among ensemble teachers

Let us consider the case where S and T are generated independently satisfying the condition that direction
cosines between S and P and between T and P are both R0, as shown in Fig. 9, where S, T and P are
N dimensional vectors. In this ﬁgure, the inner product of s and t is

·

s

S

t =

S
R0 k
k
P
k
k
q0 −
(cid:0)
where s and t are projections from S to the orthogonal complement C of X and from T to C, respectively.
q0 denotes the direction cosine between S and T .

R0 k
k

Incidentally, if dimension N is large and S and T have been generated independently, s and t should

(cid:19)
R2
0

−
T

(cid:18)
S

(cid:18)
,

(52)

(53)

T
P

k
k

kk

−

=

(cid:19)

P

P

T

k

k

(cid:1)

·

be orthogonal to each other. Therefore, q0 = R2
0.

10

Figure 9: Direction cosine among ensemble teachers

References

1998).

[1] D. Saad, (ed.): On-line Learning in Neural Networks (Cambridge University Press, Cambridge,

[2] Y. Freund and R. E. Schapire: Journal of Japanese Society for Artiﬁcial Intelligence, 14 (1999) 771

[in Japanese, translation by N. Abe].

[3] A. Krogh and P. Sollich: Phys. Rev. E 55 (1997) 811.

[4] R. Urbanczik: Phys. Rev. E 62 (2000) 1448.

[5] K. Hara and M. Okada: J. Phys. Soc. Jpn. 74 (2005) 2966.

[6] S. Miyoshi, K. Hara and M. Okada: Phys. Rev. E 71 (2005) 036116.

[7] S. Miyoshi and M. Okada: J. Phys. Soc. Jpn. 75 (2005) in press.

[8] H. Nishimori: Statistical Physics of Spin Glasses and Information Processing: An Introduction

(Oxford University Press, Oxford, 2001).

11


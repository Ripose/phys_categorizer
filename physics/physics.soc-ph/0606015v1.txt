6
0
0
2
 
n
u
J
 
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
5
1
0
6
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

On the Feasibility of Portfolio Optimization under Expected Shortfall

Stefano Ciliberti,1 Imre Kondor,2 and Marc M´ezard1
1CNRS; Univ. Paris Sud, UMR8626, LPTMS, ORSAY CEDEX, F-91405
2Collegium Budapest, 1014 Budapest, Szenthromsg u. 2

We address the problem of portfolio optimization under the simplest coherent risk measure, i.e.
the expected shortfall. As it is well known, one can map this problem into a linear programming
setting. For some values of the external parameters, when the available time series is too short,
the portfolio optimization is ill posed because it leads to unbounded positions, inﬁnitely short on
some assets and inﬁnitely long on some others. As ﬁrst observed by Kondor and coworkers, this
phenomenon is actually a phase transition. We investigate the nature of this transition by means
of a replica approach.

I.

INTRODUCTION

Among the several risk measures existing in the context of portfolio optimization, expected shortfall (ES) has
certainly gained increasing popularity in recent years. In several practical applications, ES is starting to replace the
classical Value-at-Risk (VaR). There are a number of reasons for this. For a given threshold probability β, the VaR
is deﬁned so that with probability β the loss will be smaller than VaR. This deﬁnition only gives the minimum loss
one can reasonably expect but does not tell anything about the typical value of that loss that can be measured by
the conditional value-at-risk (CVaR, which is the same as ES for continuous distributions that we consider here [13]
).We will be more precise on these deﬁnitions below. The point we want to stress here is that the VaR measure,
lacking the mandatory properties of subadditivity and convexity, is not coherent [1]. This means that summing VaR’s
of individual portfolios will not necessarily produce an upper bound for the VaR of the combined portfolio, thus
contradicting the holy principle of diversiﬁcation in ﬁnance. A nice practical example of the inconsistency of VaR in
credit portfolio management is reported in Ref. 2. On the other hand, it has been shown [3] that ES is a coherent
measure with interesting properties [4]. Moreover, the optimization of ES can be reduced to linear programming [5]
(which allows for a fast implementation) and leads to a good estimate for the VaR as a byproduct of the minimization
process. To summarize, the intuitive and simple character, together with the mathematical properties (coherence)
and the fast algorithmic implementation (linear programming), are the main reasons behind the growing importance
of ES as a risk measure.

In this paper, we will focus on the feasibility of the portfolio optimization problem under the ES measure of risk.
The control parameters of this problem are (i) the imposed threshold in probability, β, and (ii) the ratio N/T between
the number N of ﬁnancial assets making up the portfolio and the time series length T used to sample the probability
distribution of returns. (It is curious that, albeit trivial, the scaling in N/T had not been explicitly pointed out
before [6].) It has been discovered in [7] that, for certain values of these parameters, the optimization problem does
not have a ﬁnite solution because, even if convex, it is not bounded from below. Extended numerical simulations
allowed these authors to determine the feasibility map of the problem. Here, in order to better understand the root
of the problem and to study the transition from a feasible regime to an unfeasible one (corresponding to an ill-posed
minimization problem) we address the same problem from an analytical point of view.

The paper is organized as follows. In Section II we brieﬂy recall the basic deﬁnitions of β-VaR and β-CVaR and we
show how the portfolio optimization problem can be reduced to linear programming. We introduce a “cost function”
to be minimized under linear constraints and we discuss the rationale for a statistical mechanics approach. In Section
III we solve the problem of optimizing large portfolios under ES using the replica approach. Our results and the
comparison with numerics are reported in Section IV, and our conclusions are summarized in Section V.

II. THE OPTIMIZATION PROBLEM

We consider a portfolio of N ﬁnancial instruments w =

budget constraint ﬁxes the sum of these numbers: we impose for example

w1, . . . wN }

{

, where wi is the position of asset i. The global

We do not stipulate any constraint on short selling, so that wi can be any negative or positive number. This is, of
course, irrealistic for liquidity reasons, but considering this case allows us to show up the essence of the phenomenon.

wi = N .

N

i=1
X

(1)

1

β

)
α

;

w

(
<

P

 0

β-VaR(w)

α

<(w) is the probability of a loss associated to the portfolio w
FIG. 1: Schematic representation of the VaR measure of risk.
P
being smaller than α. The conditional VaR β-CVaR (or ES) is the average loss when this is constrained to be greater than the
β-VaR.

If we imposed a constraint that would render the domain of the wi bounded (such as a ban on short selling), this
would evidently prevent the weights from diverging, but a vestige of the transition would still remain in the form of
large, though ﬁnite, ﬂuctuations of the weights, and in a large number of them sticking to the “walls” of the domain.
, and we will assume an underlying normal distribution
N
i=1 wixi, and the probability of that loss being

We denote the returns on the assets by x =

i /2). The corresponding loss is ℓ(w

x1, x2, . . . xN }

N x2

{

p(x)
−
smaller than a given threshold α is

i exp(

∼

x) =
|

−

P

Q

(cid:0)
where θ(
) is the Heaviside step function, equal to 1 if its argument is positive and 0 otherwise. The β-VaR of this
·
portfolio is formally deﬁned by

(cid:1)

P<(w, α) =

dx p(x)θ

α

ℓ(w

x)
|

−

,

Z

β-VaR(w) = min
{

α :

P<(w, α)

,

β

}

≥

(see Fig. 1), while the CVaR (or ES, in this case) associated with the same portfolio is the average loss on the tail of
the distribution,

β-CVaR(w) = Z

dx p(x)ℓ(w

x)θ
|
dx p(x)θ

(cid:0)
ℓ(w

ℓ(w

x)
|

x)
|

−

β-VaR(w)

−
β-VaR(w)

=

(cid:1)

1

β

1

−

Z

Z

(cid:0)

dx p(x)ℓ(w

x)θ
|

ℓ(w

x)
|

−

β-VaR(w)

.

(4)

(cid:0)

(cid:1)

The threshold β then represents a conﬁdence level.
β = 0.90, 0.95, and 0.99, but we will address the problem for any β
probability” in some previous literature would correspond here to (1

(cid:1)
In practice, the typical values of β which one considers are
[0, 1]. What is usually called “exceedance

As mentioned in the introduction, the ES measure can be obtained from a variational principle [5]. The minimization

∈
β).

−

of a properly chosen objective function leads directly to (4):

β-CVaR(w) = min

Fβ(w, v) ,

v

Fβ(w, v)

v + (1

1

β)−

dx p(x)

ℓ(w

≡

−

+

.

v

x)
|

−

Z

(cid:2)

(cid:3)

Here, [a]+
)/2. The external parameter v over which one has to minimize is claimed to be relevant in
|
itself [5], since its optimal value may represent a good estimate for the actual value-at-risk of the portfolio. We will

(a +

a
|

≡

2

(2)

(3)

(5)

(6)

come back to this point as we discuss our results. We stress here that minimizing (6) over w and v is equivalent to
optimizing (4) over the portfolio vectors w.

Of course, in practical cases the probability distribution of the loss is not known and must be inferred from the
past data. In other words, we need an “in-sample” estimate of the integral in (6), which would turn a well posed
(but useless) optimization problem into a practical approach. We thus approximate the integral by sampling the
probability distributions of returns. For a given time series x(1), . . . x(T ), our objective function becomes simply

ˆFβ(w, v) = v +

1
β)T

(1

−

T

τ =1
X

ℓ(w

x(τ ))
|

−

v

+

= v +

1
β)T

(1

−

T

N

v

−

τ =1 "−
X

i=1
X

+

#

wixiτ

,

(7)

(cid:3)
where we denote by xiτ the return of asset i at time τ . Optimizing this risk measure is the same as the following
linear programming problem:

(cid:2)

given one data sample, i.e. a matrix xiτ , i = 1, . . . N , τ = 1, . . . T ,

minimize the cost function

Eβ

v,

,

wi}

{

;

uτ }

{

xiτ }

{

= (1

β)T v +

uτ ,

over the (N + T + 1) variables Y

w1, . . . wN , u1, . . . uT v

under the (2T + 1) constraints

(cid:2)

≡ {

(cid:3)

−

,

}

•

•

•

•

uτ ≥

0 ,

uτ + v +

xiτ wi ≥

0

∀

τ , and

wi = N .

N

i=1
X

T

τ =1
X

N

i=1
X

→ ∞

Since we allow short positions, not all the wi are positive, which makes this problem diﬀerent from standard linear
W , where W is a very large
programming. To keep the problem tractable, we impose the condition that wi ≥ −
cutoﬀ, and the optimization problem will be said to be ill-deﬁned if its solution does not converge to a ﬁnite limit
when W
. It is now clear why constraining all the wi to be non-negative would eliminate the feasibility problem:
a ﬁnite solution will always exists because the weights are by deﬁnition bounded, the worst case being an optimal
portfolio with only one non-zero weight taking care of the total budget. The control parameters that govern the
problem are the threshold β and the ratio N/T of assets to data points. The resulting “phase diagram” is then a line
in the β-N/T plane separating a region in which, with high probability, the minimization problem is not bounded and
thus does not admit a ﬁnite solution, and another region in which a ﬁnite solution exists. These statements are non-
deterministic because of the intrinsic probabilistic nature of the returns. We will address this minimization problem
in the non-trivial limit where T
, while N/T stays ﬁnite. In this “thermodynamic” limit, we shall
assume that extensive quantities (like the average loss of the optimal portfolio, i.e. the minimum cost function) do
not ﬂuctuate, namely that their probability distribution is concentrated around the mean value. This “self-averaging”
property has been proven for a wide range of similar statistical mechanics models [8]. Then, we will be interested
in the average value of the min of (8) over the distribution of returns. Given the similarity of portfolio optimization
with the statistical physics of disordered systems, this problem can be addressed analytically by means of a replica
approach [9].

→ ∞

→ ∞

, N

III. THE REPLICA APPROACH

We consider one given sample, i.e. a given history of returns xiτ drawn from the distribution

iτ
Y
In order to compute the minimal cost, we introduce the partition function at inverse temperature γ. Recalling that
Y is the set of all variables, the partition function at inverse temperature γ is deﬁned as

p(
{

xiτ }

)

∼

e−

N x2

iτ /2 .

Zγ[
{

xiτ }

] =

ZV

dY exp

γEβ[Y;

−

h

]

xiτ }

{

i

3

(8)

(9)

(10)

(11)

where V is the convex polytope deﬁned by (9). The intensive minimal cost corresponding to this sample is then
computed as

ε[
{

xiτ }

] = lim
N
→∞

min E[
{
N

]

xiτ }

= lim
N
→∞

lim
γ
→∞

1
−
N γ

log Zγ[
{

xiτ }

] .

Actually, we are interested in the average value of this quantity over the choice of the sample. Equation (12) tells us
that the average minimum cost depends on the average of the logarithm of Z. This diﬃculty is usually circumvented
by means of the so called “replica trick”: one computes the average of Z n, where n is an integer, and then the average
of the logarithm is obtained by

log Z = lim
0
→
thus assuming that Z n can be analytically continued to real values of n. The overline stands for an average over
diﬀerent samples, i.e. over the probability distribution (10). This technique has a long history in the physics of spin
glasses [9]: the proof that it leads to the correct solution has been obtained [10] recently.

(13)

n

,

The partition function (11) can be written more explicitly as

∂Z n
∂n

+

∞

T

+

∞

N

+

∞

+i

∞

N

dwi

dλ exp

λ

Zγ[
{

xiτ }

] =

dv

T

0
Z

Z

−∞
+

∞

duτ

τ =1
Y
+i

T

∞

Z

−∞

i=1
Y

i

Z

−

∞

T

×

0
Z

dµτ

dˆµτ exp

ˆµτ

uτ + v +

τ =1
Y

i

Z

−

∞

τ =1
Y

"

τ =1
X

 

"

 
N

i=1
X

i=1
X
xiτ wi −

wi −

N

!# ×

µτ

exp

γ(1

β)T v

γ

!#

"−

−

−

uτ

(14)
,
#

T

τ =1
X

where the constraints are imposed by means of the Lagrange multipliers λ, µ, ˆµ. In view of applying the trick in (13),
we introduce n identical replicas of the system corresponding to the same history of returns
, and write down
Z n
]. After this, the average over the samples can be performed and allows one to introduce the overlap matrix
γ [
{

xiτ }

xiτ }

{

Qab =

wa

i wb
i ,

a, b = 1, . . . n ,

(15)

1
N

N

i=1
X

as well as its conjugate ˆQab (the Lagrange multiplier imposing (15)). Here, a and b are replica indexes. After (several)
Gaussian integrals, one is left with

Z n
γ [
{

xiτ }

]

∼

n

+

∞

dva

+

∞

+i

∞

dQab

Z

−∞

a=1
Y

Z
−∞ Ya,b

i

Z

−

∞ Ya,b

T n log γ + T log ˆZγ

−

va

,

Qab

}

{

}

−

Tr log Q

−

{
(cid:0)

where

d ˆQab exp

N

Qab ˆQab

N

−

ˆQab

γ(1

β)T

−

−

(

Xa,b
N
2

Xa,b

Tr log ˆQ

nN
2

−

log 2

,

)

va

a
X

(16)

n

+

∞

ˆZγ

va

,

Qab

}

{

}

≡

{
(cid:0)

Z

−∞

a=1
Y

(cid:1)

dya exp



−


We now write T = tN and work at ﬁxed t while N

(Q−

1)ab(ya

va)(yb

vb) + γ

yaθ(

ya)

.

(17)

−

−

n

a=1
X

−





The most natural solution is obtained by realizing that all the replicas are identical. Given the linear character of
the problem, the symmetric solution should be the correct one. The replica-symmetric solution corresponds to the
ansatz

T
2

n

(cid:1)

1
2

Xa,b=1
.
→ ∞

Qab =

q1
q0

(

if a = b
= b
if a

;

ˆQab =

ˆq1
ˆq0

(

if a = b
= b
if a

,

and va = v for any a. As we discuss in detail in appendix A, one can show that the optimal cost function, computed
as from eq. (12) but with the average of the log, is the minimum of

ε(v, q0, ∆) =

+ ∆

t(1

β)v

1
2∆

−

−

(cid:20)

q0
2

+

t
2√π

+

∞

Z

−∞

s2

dse−

g(v + s

,

2q0)
(cid:21)

p

4

(12)

(18)

(19)

6
6
105

104

102

101

0.6

0.5

0.4

0.2

0.1

0

0

β = 0.95
β = 0.90
β = 0.80
1/√x

non-feasible

N T

0.3

∆

103

feasible

β

0.2

0.4

0.6

0.8

1

10-8

10-7

10-6

10-4

∗

10-3

10-2

10-5
N
T

(cid:18)

N
T −

(cid:20)

(cid:19)

(cid:21)

FIG. 2: Left panel: The phase diagram of the feasibility problem for expected shortfall. Right panel: The order parameter ∆
diverges with an exponent 1/2 as the transition line is approached. A curve of slope

1/2 is also shown for comparison.

−

where ∆

limγ

≡

) is deﬁned as
γ∆q and the function g(
·

→∞

Note that this function and its derivative are continuous. Moreover, v and q0 in (19) are solutions of the saddle point
equations



g(x) = 


0
x2

−

2x

−

x

0 ,

≥
1
−
1 x <

x < 0 ,
1 .

≤
−

1

β +

−

1
2√π

1 +

−

t
√2πq0 Z

Z
dse−

s2

s2

dse−

g′(v + s

2q0) = 0 ,

s g′(v + s

2q0) = 0 .

p

p

We require that the minimum of (19) occur at a ﬁnite value of ∆. In order to understand this point, we recall the

meaning of ∆ (see also (18)):

∆/γ

∆q = (q1 −

∼

q0) =

N

1
N

2

w(1)
i

1
N

−

i=1
X

(cid:0)

(cid:1)

N

i=1
X

i w(2)
w(1)

i ∼

w2

−

w2 ,

where the superscripts (1) and (2) represent two generic replicas of the system. We then ﬁnd that ∆ is proportional
to the ﬂuctuations in the distribution of the w’s. An inﬁnite value of ∆ would then correspond to a portfolio which is
inﬁnitely short on some particular positions and, because of the global budget constraint (1), inﬁnitely long on some
other ones.

Given (19), the existence of a solution at ﬁnite ∆ translates into the following condition:

which deﬁnes, along with eqs. (21) and (22), our phase diagram.

t(1

β)v

−

q0
2

+

t
2√π

−

+

∞

Z

−∞

s2

dse−

g(v + s

2q0)

0 ,

≥

p

We can now chart the feasibility map of the expected shortfall problem. We will use as control parameters N/T

1/t
1 can be worked out analytically and one can show that the critical value t∗ is given

≡

and β. The limiting case β
by

→

IV. THE PHASE DIAGRAM

1
t∗

=

1
2 − O

(1

−

h

β)3e−(4π(1
−

β)2)−1

.

i

5

(20)

(21)

(22)

(23)

(24)

(25)

6

(26)

(27)

(28)

This limit corresponds to the over-pessimistic case of maximal loss, in which the single worst loss contribute to the
risk measure. The optimization problem is the following:

min
w

max
1,...T
∈{

τ

(cid:20)

−

wixiτ

.

(cid:17)(cid:21)

i
X

} (cid:16)
A simple “geometric” argument by Kondor et al. [7] borrowed from a random geometry context [12] leads to the
critical value 1/t∗ = 0.5 in this extreme case. The idea is the following. According to eq. (26), one has to look for
the minimum of a polytope made by a large number of planes, whose normal vectors (the xiτ ) are drawn from a
symmetric distribution. The simplex is convex, but with some probability it can be unbounded from below and the
optimization problem is ill deﬁned. Increasing T means that the probability of this event decreases, because there are
more planes and thus it is more likely that for large values of the wi the max over t has a positive slope in the i-th
direction. The exact law for this probability can be obtained by induction on N and T [7] and, as we said, it jumps
in the thermodynamic limit from 1 to 0 at N/T = 0.5. Given that the corrections to this limit case are exponentially
small (eq. (25)), the threshold 0.5 can be considered as a good approximation of the actual value for many cases of
practical interest (i.e. β & 0.9).

For ﬁnite values of β we have solved numerically eqs. (21), (22) and (24) using the following procedure. We ﬁrst
solve the two equations (21) and (22), which always admit a solution for (v, q0). We then plot the l.h.s. of eq. (24) as
a function of 1/t for a ﬁxed value of β. This function is positive at small 1/t and becomes negative beyond a threshold
1/t∗. By keeping track of 1/t∗ (numerically obtaining via linear interpolations) for each value of β we build up the
phase diagram (Fig. 2, left). We show in the right panel of Fig. 2 the divergence of the order parameter ∆ versus
1/t

1/t∗. The critical exponent is found to be 1/2:

−

∆

∼

(cid:18)

1
t −

1
t∗(β)

(cid:19)

1/2

−

.

, again in agreement with the scaling found in [7]. We have performed extensive numerical simulations in order to
check the validity of our analytical ﬁndings. For a given realization of the time series, we solve the optimization
W ,
problem (8) by standard linear programming [11]. We impose a large negative cutoﬀ for the w’s, that is wi >
and we say that a feasible solution exists if it stays ﬁnite for W
. We then repeat the procedure for a certain
number of samples, and then average our ﬁnal results (optimal cost, optimal v, and the variance of the w’s in the
optimal portfolio) over those of them which produced a ﬁnite solution. In Fig. 3 we show how the probability of
ﬁnding a ﬁnite solution depends on the size of the problem. Here, the probability is simply deﬁned in terms of the
frequency. We see that the convergence towards the expected 1-0 law is fairly slow, and a ﬁnite size scaling analysis
is shown in the right panel. Without loss of generality, we can summarize the ﬁnite-N numerical results by writing
the probability of ﬁnding a ﬁnite solution as

→ ∞

−

p(N, T, β) = f

1
t −

1
t∗(β)

·

(cid:19)

(cid:20)(cid:18)

N α(β)

,

(cid:21)

→

≫

→

0 if x

1 if x

1 and f (x)

where f (x)

1, and where α(1) = 1/2.
In Fig. 4 (left panel) we plot, for a given value of β, the optimal cost found numerically for several values of the size
1/t∗)1/2.
N compared to the analytical prediction at inﬁnite N . One can show that the cost vanishes as ∆−
The right panel of the same ﬁgure shows the behavior of the value of v which leads to the optimal cost versus N/T ,
for the same ﬁxed value of β. Also in this case, the analytical (N
limit) is plotted for comparison. We note that
this quantity was suggested [5] to be a good approximation of the VaR of the optimal portfolio: We ﬁnd here that
vopt diverges at the critical threshold and becomes negative at an even smaller value of N/T .

→ ∞

(1/t

≪

∼

−

1

V. CONCLUSIONS

We have shown that the problem of optimizing a portfolio under the expected shortfall measure of risk by using
empirical distributions of returns is not well deﬁned when the ratio N/T of assets to data points is larger than
a certain critical value. This value depends on the threshold β of the risk measure in a continuous way and this
deﬁnes a phase diagram. The lower the value of β, the larger the length of the time series needed for the portfolio
optimization. The analytical approach we have discussed in this paper allows us to have a clear understanding of this
phase transition. The mathematical reason for the non-feasibility of the optimization problem is that, with a certain
probability p(N, T, β), the linear constraints in (9) deﬁne a simplex which is not bounded from below, thus leading
1
to a solution which is not ﬁnite (∆q

in our language), in the same way as it happens in the extreme case β

→ ∞

→

7

 0.45

 0.55

 0.6

 0.5
N
T

-0.5
N
T −

(cid:20)

∗

 0
N
T

(cid:18)

(cid:19)

(cid:21)

·

 0.5
N 0.5

 1

 1.5

FIG. 3: Left: The probability of ﬁnding a ﬁnite solution as obtained from linear programming at increasing values of N and
with β = 0.8. Right: Scaling plot of the same data. The critical value is set equal to the analytical one, N/T = 0.4945 and the
critical exponent is 1/2, i.e. the one obtained in [7] for the limit case β
1. The data do not collapse perfectly, and better
results can be obtained by slightly changing either the critical value or the exponent.

→

 1

 0.8

 0.6

 0.4

 0.2

y
t
i
l
i

b
a
b
o
r
p

N = 32
N = 64
N = 128

N = 256
N = 512
N = 1024

 0

 0.4

 1

 0.8

 0.6

 0.4

 0.2

n
o
i
t
c
n
u
f

t
s
o
c

 0
 0.25

y
t
i
l
i

b
a
b
o
r
p

t
p
o
v

 1

 0.8

 0.6

 0.4

 0.2

 0.4

 0.2

 0

-0.2

-0.4

-0.6

-0.8

-1

-1.2

-1.4

N = 32
N = 64
N = 128

N = 256
N = 512
N = 1024

 0

-1.5

-1

N = 32
N = 64

N = 128
N = 256
analytic

N = 32
N = 64

N = 128
N = 256
analytic

 0.3

 0.35

 0.4

 0.45

 0.5

 0.25

 0.3

 0.35

 0.4

 0.45

 0.5

N
T

N
T

FIG. 4: Numerical results from linear programming and comparison with analytical predictions at large N . Left: The minimum
cost of the optimization problem vs N/T , at increasing values of N . The thick line is the analytical solution (19). Here β = 0.7,
(N/T )
0.463. Right: The optimal value of v as found numerically for several values of N is compared to the analytical
solution.

≃

∗

discussed in [7]. From a more physical point of view, it is reasonable that the feasibility of the problem depend on
the number of data points we take from the time series with respect to the number of ﬁnancial instruments of our
portfolio. The probabilistic character of the time series is reﬂected in the probability p(N, T, β). Interestingly, this
probability becomes a threshold function at large N if N/T

1/t is ﬁnite, and its general form is given in (28).

These results have a practical relevance in portfolio optimization. The order parameter discussed in this paper
is tightly related to the relative estimation error [6]. The fact that this order parameter has been found to diverge
means that in some regions of the parameter space the estimation error blows up, which makes the task of portfolio
optimization completely meaningless. The divergence of estimation error is not limited to the case of expected shortfall.
As shown in [7], it happens in the case of variance and absolute deviation as well [14], but the noise sensitivity of
expected shortfall turns out to be even greater than that of these more conventional risk measures.

There is nothing surprising about the fact that if there are no suﬃcient data, the estimation error is large and we
cannot make a good decision. What is surprising is that there is a sharply deﬁned threshold where the estimation
error actually diverges.

≡

For a given portfolio size, it is important to know that a minimum amount of data points is required in order to

perform an optimization based on empirical distributions. We also note that the divergence of the parameter ∆ at
the phase transition, which is directly related to the ﬂuctuations of the optimal portfolio, may play a dramatic role
in practical cases. To stress this point, we can deﬁne a sort of “susceptibility” with respect to the data,

χτ

ij =

∂
wj i
h
∂xiτ

,

and one can show that this quantity diverges at the critical point, since χτ
∆. A small change (or uncertainty) in
xiτ becomes increasingly relevant as the transition is approached, and the portfolio optimization could then be very
unstable even in the feasible region of the phase diagram. We stress that the susceptibility we have introduced might
be considered as a measure of the eﬀect of the noise on portfolio selection and is very reminiscent to the measure
proposed in [6].

ij ∼

In order to present a clean, analytic picture, we have made several simplifying assumptions in this work. We have
omitted the constraint on the returns, liquidity constraints, correlations between the assets, nonstationary eﬀects, etc.
Some of these can be systematically taken into account and we plan to return to these ﬁner details in a subsequent
work.

Acknowledgments. We thank O. C. Martin, and M. Potters for useful discussions, and particularly J. P. Bouchaud
for a critical reading of the manuscript. S. C. is supported by EC through the network MTR 2002-00319, STIPCO,
I.K. by the National Oﬃce of Research and Technology under grant No. KCKHA005.

APPENDIX A: THE REPLICA SYMMETRIC SOLUTION

We show in this appendix how the minimum cost function corresponding to the replica-symmetric ansatz is obtained.
The ‘TrLogQ’ term in (16) is computed by realizing that the eigenvalues of such a symmetric matrix are (q1 + (n

1)q0) (with multiplicity 1) and (q1 −

q0) with multiplicity n

1. Then,

−

Tr log Q = log detQ = log(q1 + (n

1)q0) + (n

−

1) log(q1 −

−

q0) = n

log ∆q +

(cid:18)

q1
∆q

(cid:19)

+

(n2) ,

O

where ∆q

q1 −

≡

q0. The eﬀective partition function in (17) depends on Q−

1, whose elements are:

By introducing a Gaussian measure dPq0 (s)

(Q−

1)ab =

(∆q

q0)/(∆q)2 +
(n)

q0/(∆q)2 +

−

(n)

O

if a = b
= b
if a

O

e−

s2/2q0 , one can show that

(

−

ds
√2πq0

≡

1
n

log ˆZ(v, q1, q0) =

dxae−

1
2∆q

a(xa)2+γ

a(xa+v)θ(

xa

−

−

v)

dPq0 (s)e

s
∆q

a xa

P

P

Z

P

)

1
n

log

q0
2∆q

a

(Z Y
+

Z

=

dPq0 (s) log Bγ(s, v, ∆q) +

(n)

O

where we have deﬁned

Bγ(s, v, ∆q)

dx exp

+ γ(x + v)θ(

x

v)

.

≡

Z

(x

s)2

−
2∆q

−

(cid:18)

−

−

(cid:19)

The exponential in (16) now reads expN n[S(q0, ∆q, ˆq0, ∆ˆq) +

(n)], where

S(q0, ∆q, ˆq0, ∆ˆq) = q0∆ˆq + ˆq0∆q + ∆q∆ˆq

∆ˆq

γt(1

β)v

t log γ + t

dPq0 (s) log Bγ(s, v, ∆q)

−

−

−

O

−
t
2

−

log ∆q

log ∆ˆq +

Z

1
2

(cid:18)

−

ˆq0
∆ˆq

−

(cid:19)

log 2
2

.

The saddle point equations for ˆq0 and ∆ˆq allow then to simplify this expression. The free energy (
limn

γ /∂n is given by

0 ∂Z n

−

γ)fγ =

→

−

γfγ(v, q0, ∆q) =

t log γ +

log ∆q +

γt(1

β)v + t

dPq0 (s) log Bγ(s, v, ∆q) ,

(A6)

1
2 −

1

t

−
2

1
q0 −
2∆q −

−

Z

8

(29)

−

(A1)

(A2)

(A3)

(A4)

(A5)

6
where the actual values of v, q0 and ∆q are ﬁxed by the saddle point equations

∂fγ
∂v

=

∂fγ
∂q0

=

∂fγ
∂∆q

= 0 .

A close inspection of these saddle point equations allows one to perform the low temperature γ
→ ∞
that ∆q = ∆/γ while v and q0 do not depend on the temperature. In this limit one can show that

limit by assuming

1
γ

lim
γ
→∞

log Bγ(s, v, ∆/γ) = 


−
0

s + v + ∆/2

(v + s)2/2∆

∆
s <

−
≤

v

−

s <
v

−
s

−
−
≥ −

v
∆
v

If we plug this expression into eq. (A6) and perform the large-γ limit we get the minimum cost:



E = lim
γ
→∞

fγ =

1

q0 −
2∆

−

+ t(1

β)v

−

t

−

∆

−

dx
√2πq0

Z

−∞

(x−v)2
2q0

e−

x +

(cid:18)

∆
2

(cid:19)

+

t
2∆

0

dx
√2πq0

∆

Z

−

(x−v)2
2q0 x2 .

e−

(A9)

We rescale x

x∆, v

→

v∆, and q0 →

→

q0∆2, and after some algebra we obtain eq. (19).

9

(A7)

(A8)

[1] see P. Artzner, F. Delbaen, J. M. Eber, and D. Heath, Mathematical Finance 9, 203–228 (1999), for an axiomatic deﬁnition

[2] R. Frey, and A. J. McNeil, Journal of Banking and Finance 26, 1317–1334 (2002).
[3] C. Acerbi, and D. Tasche, Journal of Banking and Finance 26, 1487–1503 (2002).
[4] G. C. Pﬂug, in “Probabilistic Constrained Optimization: Methodology and Applications”, S. Uryasev (ed.), Kluwer Aca-

of coherence.

demic Publisher (2000).

[5] R. Rockafellar, and S. Uryasev, The Journal of Risk 2, 21–41 (2000).
[6] S. Pafka, and I. Kondor, Europ. Phys. J. B27, 277 (2002).
[7] I. Kondor, Lectures given at the Summer School on “Risk Measurement and Control”, Rome, June 9-17, 2005; I. Kondor,
S. Pafka, and G. Nagy, Noise sensitivity of portfolio selection under various risk measures, submitted to Journal of Banking
and Finance.

[8] see e.g. F. Guerra, F.L. Toninelli, J. Stat. Phys. 115, 531 (2004) and references therein.
[9] M. M´ezard, G. Parisi, and M. .A. .Virasoro, “Spin Glass theroy and Beyond”, World Scientiﬁc Lecture Notes in Physics

[10] M. Talagrand, Spin Glasses: a Challenge for Mathematicians, Spinger-Verlag (2002).
[11] W. H. Press, S. H. Teukolsky, W. T. Wetterling, and B. P. Flannery, “Numerical Recipes in C”, Cambridge University

Vol. 9, Singapore (1987).

Press (Cambridge, UK, 1992).

[12] B. K. Schmidt, and T. Mattheiss, Mathematics of Operations research 2, 292–296 (1977).
[13] see [3] for the subtleties related to a discrete distribution.
[14] see also S. Ciliberti et al., Proceeding of the Torino Conference on “Applications of Physics in Financial Analysis” (2006),

for a replica approach to the portfolio optimization problem under absolute deviation


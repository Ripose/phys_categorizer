6
0
0
2
 
c
e
D
 
1
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
4
8
0
2
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Volatility: a hidden Markov process in ﬁnancial time series

Zolt´an Eisler∗
Department of Theoretical Physics,
Budapest University of Technology and Economics
Budafoki ´ut 8., H-1111, Budapest, Hungary

Josep Perell´o† and Jaume Masoliver‡
Departament de F´ısica Fonamental, Universitat de Barcelona
Diagonal 647, E-08028 Barcelona, Spain
(Dated: September 13, 2013)

The volatility characterizes the amplitude of price return ﬂuctuations. It is a central magnitude in
ﬁnance closely related to the risk of holding a certain asset. Despite its popularity on trading ﬂoors,
the volatility is unobservable and only the price is known. Diﬀusion theory has many common points
with the research on volatility, the key of the analogy being that volatility is the time-dependent
diﬀusion coeﬃcient of the random walk for the price return. We present a formal procedure to
extract volatility from price data, by assuming that it is described by a hidden Markov process
which together with the price form a two-dimensional diﬀusion process. We derive a maximum
likelihood estimate valid for a wide class of two-dimensional diﬀusion processes. The choice of the
exponential Ornstein-Uhlenbeck (expOU) stochastic volatility model performs remarkably well in
inferring the hidden state of volatility. The formalism is applied to the Dow Jones index. The main
results are: (i) the distribution of estimated volatility is lognormal, which is consistent with the
expOU model; (ii) the estimated volatility is related to trading volume by a power law of the form
σ ∝ V 0.55; and (iii) future returns are proportional to the current volatility which suggests some
degree of predictability for the size of future returns.

PACS numbers: 89.65.Gh, 02.50.Ey, 05.40.Jc, 05.45.Tp
Keywords: random diﬀusion, econophysics, stochastic volatility

I.

INTRODUCTION

The volatility measures the amplitude of return ﬂuctuations and it is one of the central quantities in ﬁnance [? ].
Investors sometimes place even greater emphasis on the level of volatility than on the market trend itself. The reason
for this is mainly that the risk of holding an asset is classically associated with its volatility [? ]. The theoretical
framework used to quantify aspects of price ﬂuctuations has many common points with areas of physics dealing with
noisy signals. The research of random diﬀusion aims to describe the dynamics of particles in random media and its
methods have been applied to a large variety of phenomena in statistical physics and condensed matter [? ]. Time
series describing solar ﬂares, earthquakes, the human heartbeat or climate records show strong correlations, multi-
scaling, non-Gaussian statistics and self-organized behavior [? ? ? ? ]. These are properties also observed in ﬁnancial
time series where volatility is considered to play a key role [? ? ].

The picture that prices follow a simple diﬀusion process was ﬁrst proposed by Bachelier in 1900 [? ]. Later in
1959, the physicist Osborne introduced the geometric Brownian motion and suggested that volatility can be viewed
as the diﬀusion coeﬃcient of this random walk [? ]. The simplest possible assumption – that it is a time-independent
constant – lies at the heart of classical models such as the Black-Scholes option pricing formula [? ]. More recently it
has become widely accepted that such an assumption is inadequate to explain the richness of the markets’ behavior
[? ]. Instead, volatility itself should be treated as a random quantity with its own particular dynamics.

Among its most relevant properties [? ? ? ? ? ], volatility is the responsible for the observed clustering in price
changes. That is: large ﬂuctuations are commonly followed by other large ﬂuctuations and similarly for small changes
[? ? ]. Another related feature is that volatility is a long memory process. In contrast with price changes which show
negligible autocorrelations, volatility autocorrelation is still signiﬁcant for time lags longer than one year [? ? ? ].
Most of these studies introduce a subordinated process which is associated with the volatility in one way or another

∗Electronic address: eisler@maxwell.phy.bme.hu
†Electronic address: josep.perello@ub.edu
‡Electronic address: jaume.masoliver@ub.edu

[? ? ? ? ? ? ? ? ? ? ? ].

The main obstacle of the appropriate analysis of volatility is that it is directly unobservable. As we have mentioned,
volatility provides important information to traders but it is very unclear how reliable the estimates of such a hidden
process can be. Investors use several proxies to infer the level of current asset volatility. The most common ways are:
(i) to make it equivalent to the absolute value of return changes, (ii) to assume a proportional law between volatility
and market volume [? ? ? ], and (iii) to use the information contained in option prices and obtain the so-called
“implied volatility” which, in fact, corresponds to the market’s belief of volatility [? ].

In this paper we present a formal procedure to estimate volatility from the price dynamics only. By assuming that it
is described by a hidden Markov process, the subordinated time series can be estimated through maximum likelihood.
But this already raises the questions: What process is a proper model of volatility and how to adjust the possible
parameters to describe various stocks and markets? Among the possible candidates, multifractals [? ? ? ? ? ] and
stochastic volatility models [? ? ? ? ? ? ] are the most promising. Nevertheless, some of them present mathematical
diﬃculties, while some others are unrealistic or computationally unfeasible.

We have decided to focus on a particular stochastic volatility model that is able to circumvent these diﬃculties.
Our approach takes a two-dimensional diﬀusion process – one dimension for price and the second dimension for the
volatility – and in particular the exponential Ornstein-Uhlenbeck volatility model [? ? ]. In this model, as its name
indicates, the logarithm of the volatility follows an Ornstein-Uhlenbeck process, that is: a mean reverting process with
linear drift. The resulting model is capable of reproducing the statistical properties of the ﬁnancial markets fairly
well [? ].

In Sec.

The paper is organized as follows.

II we outline the general stochastic volatility framework and more
speciﬁcally the exponential Ornstein-Uhlenbeck model. In Sec. III we present a maximum likelihood estimator for
a wide class of stochastic volatility models. For the case of the exponential Ornstein-Uhlenbeck (expOU) model we
present Monte Carlo simulations to show that it performs remarkably well in inferring the hidden state of the volatility
process. In Sec. IV the procedure is applied to the Dow Jones Industrial Average. Conclusions are drawn in Sec. V
and some more technical details are left to the appendices.

II. STOCHASTIC VOLATILITY MODELS

The geometric Brownian motion (GBM) [? ]

is the most widely used model in ﬁnance. In this setting the asset

price S(t) is described through the following Langevin equation (in Itˆo sense):

where σ is the volatility, assumed to be constant, µ is some deterministic drift indicating an eventual trend in the
market, and W1(t) is the Wiener process. We deﬁne the zero-mean return X(t) as

where the symbol
of X(t) the GBM is simply written as

h· · · i

designates the average value and t0 is the initial time which is usually set to be zero. In terms

X(t) = ln [S(t + t0)/S(t0)]

ln [S(t + t0)/S(t0)]

,

− h

i

However, especially after the 1987 market crash, compelling empirical evidence has become available that the
assumption of a constant volatility is doubtful [? ]. Neither is volatility a deterministic function of time as one might
expect on account of the non-stationarity of ﬁnancial data, but a random quantity [? ].

In the most general setting one therefore assumes that the volatility σ is a given function of a random process Y (t):

dS(t)
S(t)

= µdt + σdW1(t),

dX(t) = σdW1(t).

σ(t) = f [Y (t)].

Most stochastic volatility (SV) models assume Y (t) is also a diﬀusion process that may or may not be correlated with
price. The main diﬀerence between various models is only the parametrization of this scheme. In a general notation
the zero-mean return X(t) deﬁned above is described by the following set of stochastic diﬀerential equations

dX(t) = f [Y (t)]dW1(t)
dY (t) =

g[Y (t)]dt + h[Y (t)]dW2(t),

−

where dX = dS/S
and f , g and h are given functions of Y (t). As shown in Eq. (4), f [Y (t)] corresponds to
the volatility, i.e., the amplitude of return ﬂuctuations. However, since f (x) is usually chosen to be a monotonically

dS/S

− h

i

2

(1)

(2)

(3)

(4)

(5)
(6)

increasing function, it is not misleading to think of Y as a measure of volatility. Thus, as far as there is no confusion, we
will refer to the process Y (t) as “volatility” as well. On the other hand, the function g[Y (t)] describes a reverting force
that drives the volatility toward the so-called “normal level”. This force brings the volatility process to a stationary
regime for long time horizons and the normal level is related to the average volatility in that limit. Finally, the
subordinated process Y (t) may have a non-constant diﬀusion coeﬃcient deﬁned in terms of the function h[Y (t)] which
is called the volatility-of-volatility (“vol of vol”). The functions g and h fully describe the volatility process. The
resulting dynamics is comparable with the one described by a Gaussian particle trapped in a potential well V (y)
g(y), where g(y) = V ′(y). In ﬁnance one typically proposes convex potentials with only
whose associated force is
one minimum whose value is related to the normal level of the volatility.

−

In what follows we will mostly work with one particular SV model, the exponential Ornstein-Uhlenbeck (expOU)

model, which follows from the substitutions:

Note that in this model the process Y (t) is precisely the logarithm of the volatility, or “log-volatility” for short. The
main statistical properties of the model are thoroughly discussed in Ref. [? ]. We simply recall that the stationary
distribution of the process Y (t) is a Gaussian (i.e., a lognormal distribution for σ) with zero mean and variance β:

that is,

where

f (x) = mex,

g(x) = αx,

h(x) = k,

dX(t) = meY (t)dW1(t),
dY (t) =

αY (t)dt + kdW2(t).

−

p(y) =

exp

y2/2β

,

1
√2πβ

−
(cid:0)

(cid:1)

k2/2α.

β

≡

3

(7)
(8)

(9)

(10)

III. VOLATILITY ESTIMATION

A. The Wiener measure and volatility estimation

Let X and Y denote a simultaneous realization of the variables X(τ ) and Y (τ ) in the time interval t

≤
t. Omitting all Y-independent terms, we show in Appendix A that the probability density (likelihood) of such a
realization is approximately given by

−

≤

s

τ

ln P(X, Y)

t

1
2

≃ −

t−s "

Z

2

˙X(τ )
f [Y (τ )] #

dτ

−

1
2

t

˙Y + g[Y (τ )]

2

t−s "

Z

h[Y (τ )] #

dτ + . . .

(11)

Before proceeding further, we will discuss the meaning of this expression. We ﬁrst note that Eq. (11) has to be
understood in the sense of generalized functions [? ] since the Wiener process is only diﬀerentiable in this sense and,
˙X(t) and ˙Y (t) do not exist as ordinary functions and Eq. (11) is just a symbolic expression. Nevertheless, the
hence,
formula is still valid when the integral and the derivatives therein are discretized with arbitrary small time steps, a
requirement that is indeed necessary for numerical computations.

Let us now see some qualitative properties of Eq.

(11). The ﬁrst summand measures the ﬂuctuations of the
zero-mean return with respect to the volatility, [ ˙X(τ )/f (Y (τ ))]2, and their contribution to the likelihood (probability
density) of a given return realization. Note that the higher this contribution is, the lower those “relative” ﬂuctuations
are. In the same fashion, the second summand in Eq. (11) measures the ﬂuctuations of the volatility process Y (t)
with respect to the vol of vol h(Y ), although in this case these ﬂuctuations are gauged with the mean reverting force

−

g(Y ). As before, the lower these ﬂuctuations, the higher their contribution to the log-likelihood (11).
While Eqs. (5)-(6) represent a joint model for return and volatility, the stock market data only include recordings
of the return process X. The Y process and, hence, the volatility f (Y ), must be inferred indirectly in a Bayesian
fashion through Eq. (11). Indeed, the conditional probability density that the realization of the hidden Y -process is
Y, given that the observed return is X, reads

ln P(Y

X) = ln P(X, Y)
|

−

ln P(X).

(12)

4

(13)

In consequence, we can ﬁnd the maximum likelihood sample path of the (hidden) volatility process by maximizing Eq.
(12) with respect to Y (recall that Y is a realization of Y (τ ) in the interval t
t). Since the second summand
of Eq. (12) is independent of Y, we can neglect P(X) in this maximization process. Therefore, the maximization of
ln P(Y

X) yields the same result as that of ln P(X, Y).
|

Note that, besides the speciﬁcation of the stochastic volatility model (that is, the explicit forms of f, g, and h), the
only free parameter is s: the duration of past return data to take into account. After substituting the observed return
history as X, we will obtain by maximum likelihood the quantity:

−

≤

≤

s

τ

ˆY = argmaxY ln P(Y

X) = argmaxY ln P(X, Y).
|

We should mention that similar maximization problems have been studied in the context of hidden Markov models,
where this procedure is called “decoding”. When the state space (the number of possible X and Y values) is ﬁnite, the
optimization can be done exactly by the Viterbi algorithm [? ], while there has been limited success in the continuous
case [? ]. To our knowledge the only application of a similar technique to the forecasting of volatility is a binomial
cascade model which, unlike stochastic volatility models, has a ﬁnite state space (see Ref. [? ]).

As we have already stated, our main objective is to design a method able to ﬁlter the Wiener noise dW1(t) out of
Eq. (5) and thus to obtain a reliable estimate ˆY (t) of the hidden volatility process Y (t). The method, an extension
of the deconvolution procedure previously presented in [? ], has basically the following ﬁve steps:

(i) We simulate a random sample path of the Wiener process d ˆW1(τ ) for t
(ii) Then a surrogate realization of Y is generated as

s

τ

−

≤

≤

t.

dX(τ )
d ˆW1(τ ) (cid:12)
(cid:12)
(cid:12)
(cid:12)
t. Note that this equation requires that f (x) is invertible which implies that f (x) be chosen
(cid:12)

ˆYs(τ ) = f −1

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(14)

!

,

where t
≤
to be a monotonic function.

≤

−

s

τ

(iii) Substitute ˆYs and X into Eq. (11) to calculate the log-likelihood of this realization.

(iv) Iterate (i)-(iii) for I steps, keep the highest likelihood random realization and assume this to be the proper

estimate ˆY (t).

(v) The estimate of the hidden process at time t is then ˆY (t). The estimate of the volatility is given by

ˆσ(t) = mf [ ˆY (t)].

B.

Interpretation of the method

Let us further elaborate the meaning of such an estimate. In ﬁnance, the volatility is often identiﬁed with the
absolute value of returns variations. Indeed, as a ﬁrst approximation, we can replace in Eq. (5) the noise term by its
expected value and write

σ(t)

dX(t)
|
|
dW1(t)
|i
h|
which shows that the volatility is approximately proportional to the absolute returns. Eq. (5) can be thus thought of
as a ﬁrst approximation toward estimating volatility. Our method, based on the maximization of Eq. (12), takes this
estimation two steps further. In eﬀect, the ﬁrst step was taken in Ref. [? ] where we replaced the average
|i
by a simulated sample path. We are now taking a second and more reﬁned step in which we are not only replacing
the Wiener noise by a random simulation but, in addition, we perform the maximum likelihood method described by
items (i)-(v).

dW1(t)

(15)

≈

h|

,

Thus we are basically separating the observed returns dX(t) into two sources: σ(t) and dW1(t). To do this, we
have ﬁrst considered a speciﬁc form of stochastic volatility. Secondly, we have taken the driving Wiener noises dW1(t)
and dW2(t) appearing in Eqs. (5) and (6) to be uncorrelated. Finally, we have assumed that σ(t) is approximately
constant over the time step during which we numerically evaluate the derivatives ˙Y (t) and ˙X(t) appearing in Eq.
(11). We incidentally note that if h(x) = 0 – the vol of vol is equal to zero – then the stationary solution of Eq. (6) is
Y (t)
0. Thus the model reduces to the Wiener process in which the volatility is constant and absolute returns are
uncorrelated [? ].

≡

5

 

Y
d
e
a
m

t

i
t
s
e

3

2

1

0

-1

-2

4

2

Y

0

-2

Figure 1: Estimated log-volatility ˆY as a function of the actual log-volatility Y taken from 2 × 105 simulations of the expOU
model. Reconstruction used the last s = 10 values of returns, and I = 105 iterations.

 estimated Y

 estimated = true

-2

-1

0

1

2

3

true Y

 estimated Y, smoothed

 real Y

0

10000

20000

30000

time (day)

Figure 2: Estimated and actual volatility for a typical sample path of the expOU model. The estimated values were smoothed
by 5-neighbor averaging to reduce noise.

C. The performance of the estimator

In order to test the performance of the estimator, we simulate the expOU process by using Eqs. (7)-(8) with the
realistic parameters obtained in Section IV. The relationship between the simulated value of the log-volatility Y (t)
and its estimate ˆY (t) is given in Fig. 1. The two quantities agree within error bars, so we may state that

Y (t)

ˆY (t)

≈

(in mean square sense).

In what follows we will always use s = 10 days of past data and I = 105 iterations for maximization. The time step
for discretization will be ∆t = 1 day. The estimate has negligible bias and it can eﬃciently distinguish between low
and high volatility periods.

An additional veriﬁcation of the good performance of our estimate is shown in Fig. 2, where we give the actual
sample path of Y for a single realization of the expOU model together with the estimated ˆY . As we can see the
estimate follows the true log-volatility Y (t) very closely.

IV. APPLICATION TO STOCK MARKET DATA

In this section we present an application of the method to actual stock market data. We analyze the Dow Jones
Industrial Average (DJIA) index in the period 01/01/1900–05/26/2006, a total of 29, 038 days. In order to work with
zero-mean returns, the mean return was subtracted from the actual data. Trading volumes for the index are only
available for the period 04/01/1993

05/26/2006, a total of 3, 375 days.

−

A. Parameter estimation

We recall that in the estimation procedure presented here one necessarily needs to assume a theoretical model for
the volatility. Having done this, the next step is to estimate the parameters involved in the model chosen. For the
expOU model, Eqs. (7)-(8), these parameters are: m, k and α.

Before proceeding further we remark that time increments in real data have a ﬁnite size since the market always
works on discrete times (for daily data the minimum time increment is 1 day). Thus, in practice, the (inﬁnitesimal)
X(t) where
return variation dX(t) = X(t + dt)
∆t is the time step between two consecutive ticks. Also the Wiener diﬀerentials dW (t) correspond, in mean square
sense [? ], to the increments

X(t) corresponds to a (ﬁnite) return increment ∆X(t) = X(t + ∆t)

−

−

(16)
≈
where ε(t) is a Gaussian process with zero mean and unit variance [? ]. In the present case our time step has a ﬁxed
width and is equal to ∆t = 1 day.

∆W (t)

ε(t)√∆t,

Coming back to the estimation of parameters, we show in Appendix B that

/√∆t
|
is the Euler constant. Taking into account that the third summand can be evaluated from data

∆X
|
(cid:16)

(γ + ln 2)/2 +

ln m

(17)

(cid:17)E

ln

≈

D

,

where γ = 0.5772
we see that Eq. (17) provides a direct estimation of m.

· · ·

On the other hand, if the expOU model is appropriate then the empirical estimate ˆY (t) of the hidden volatility
Y (t) should also be a Gaussian process with a stationary distribution of zero mean and variance given by β = k2/2α
0.05, the distribution of ˆY (t) is Gaussian and coincides
[see Eq. (10)]. As shown in Fig. 3, if one takes β = 0.61
±
with the theoretical distribution of Y (t) given by Eq. (9). The assumption of a Gaussian distribution for our estimate
is robust and it holds for a wide range of parameters.

To fully specify the model we have to obtain the parameter α. We have chosen the value estimated in Ref.

[? ]
which was obtained in order to capture the long range correlations of the volatility. The parameters we will use are
thus m = 7.5

10−3 days−1, and k = 4.7

10−3 days−1/2, α = 1.82

10−2 days−1/2.

The distributions of log-volatility are compared in Fig. 3 for four cases: our maximum likelihood procedure applied
to Dow Jones, a simulation of expOU, the simple estimate of Eq. (18) and the deconvolution procedure introduced
in Ref. [? ] which can be written as

×

×

×

ˆYdecon(t) = ln

,

1
m (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dX(t)
d ˆW1(t) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where W1(t) is a simulation of the Wiener process. Note that this deconvoluted log-return estimator has indeed a
Gaussian distribution but with a larger variance as hinted in Fig. 3 in view of its wider density. We ﬁnally mention that
given by Eq. (15) shows a non Gaussian and biased distribution
the estimate for the log-volatility ln(σ/m)
as was also reported in Ref. [? ]. This suggests that ˆYdecon is an appropriate “null model” to contrast with ˆY . Both
quantities are generated by dividing dX(t) by the increments of a realization of the Wiener process and then taking
the logarithm of the absolute value of this ratio. The diﬀerence lays in the fact that ˆY takes the realizations that
satisfy a maximum likelihood requirement while ˆYdecon takes a Wiener process realization that is purely random. In
such a way, our method keeps the divisor correlated with dX and, as we will see next, it seems to conserve clustering
and memory eﬀects in the log-volatility time series.

dX
|

ln

≈

|

6

(18)

B. Clustering and the estimated volatility

To support our claim that the technique presented is powerful enough in ﬁltering the noise dW1 out of returns we

will give a visual comparison based on the following qualitative experiments.

7

0.6

 estimated Y, DJIA

 simulated Y, expOU

 ln |return|, DJIA

0.5

 ln |deconv. ret.|, DJIA

y
t
i
l
i

b
a
b
o
r
p

0.4

0.3

0.2

0.1

0.0

-10

-8

-6

-4

-2

0

2

4

Y, ln |return|

Figure 3: A comparison of estimates of log-volatility for Dow Jones. Black boxes (“estimated Y , DJIA”) represent our maximum
likelihood method applied to empirical data. Empty circles (“simulated Y , expOU”) represent the distribution of the simulated
sample path of the log-volatility, assuming that Y (t) follows the expOU model. We also plot the empirical distributions of two
estimates: the red line (“ln [return], DJIA”) was obtained through Eq. (15) and the black line (“ln [deconv. ret], DJIA”) via
Eq. (18).

4

 estimated Y,   

 ln |deconv. ret.| (shifted)

 estimated Y,   

 ln |return| (shifted)

|

t

n
r
u
e
r
 
.
v
n
o
c
e
d

|
 

n

l
 
,

 

Y
d
e
a
m

t

i
t
s
e

2

0

-2

-4

-6

-8

-10

2

0

-2

-4

|

n
r
u
e
r
|
 

t

n

l
 
,

 

Y
d
e
a
m

t

i
t
s
e

1000

1200

1400

1600

1800

2000

0

10000

20000

30000

time (day)

time (day)

Figure 4: [Left] A comparison between the estimate ˆY (t) and ˆYdecon(t) for a typical 1000-day period of Dow Jones. These curves
were not smoothed in order to show the substantial reduction of both the noise level and the asymmetry in ˆY (t) compared to
ˆYdecon(t) with a random approximation of the noise term dW . [Right] The estimate ˆY (t) and the logarithm of absolute return
variations for the whole sample of Dow Jones.

Figure 4 [Left] displays a comparison over a 1000-day time interval. One can observe there that the noise level in

ˆY is substantially smaller than in ˆYdecon, as also inferred from Fig. 3.

In order to show that such a correlation is responsible for suppressing large ﬂuctuations in the ratio, we can perform
a second experiment. Thus in Fig. 4 [Right] we see a comparison between the logarithm of absolute returns variations,
, and the estimated volatility ˆY . Note that the proper clustering of volatility becomes clearly visible.
ln
|

dX
|

C. A comparison with trading volume

The hidden nature of the volatility process has been addressed by several authors [? ? ? ]. For instance, Ref. [? ]
suggests that, instead of the volatility, a good estimate would be the square root of the daily trading volume. That is,

M[σ(t)]

V (t)α,

∝

(19)

8

 estimated Y

 ln |dX| + 3

 slope 0.55

 

Y
d
e
a
m

t

i
t
s
e

 
,

 

3
+

 
|

X
d

|
 

n

l

2

1

0

-1

-2

-3

-4

8

9

10

11

ln(volume) (arb.)

Figure 5: Logarithm of daily absolute return and estimated log-volatility ˆY as the function of daily volume. Days with similar
volumes were binned for better visibility. The symbols represent the medians, and the error bars the 25 − 75% quantiles in the
bins.

] denotes the median. In Fig. 5 we show evidence that supports this assumption. Again,
where α = 0.5 and M[
·
the ﬁrst estimation for the volatility is
versus ln V (t) as shown in Fig. 5.
In the same ﬁgure we also present the regression between the maximum likelihood estimate ˆY (t) and ln V (t) which
appears to be less noisy than the former regression in accordance with the smaller variance of ˆY (t) compared to that
0.55 is the same for both regressions. There have been similar [? ],
of ln
albeit controversial [? ], ﬁndings for the price impact of single transactions. However, Eq. (19) does not yet imply
that volatility is proportional to volume, only that its typical value is (i.e., the median). Fluctuations around the
average behavior due to changes in liquidity might have a key role in the process [? ].

. Nevertheless, the exponent α
dX(t)
|
|

. Therefore, we regress ln
dX(t)
|
|

dX(t)
|
|

≈

D. The predictive power of volatility

From Eqs. (7)-(8), we know that for the expOU model simple relationship can be given between ln

dX(t)
|
|

and

Y (t):

Therefore, the conditional median of ln

ln

dX(t)
|
|
dX(t)
|
|

= ln(m

) + Y (t).
dW1(t)
|
|
given Y (t) is

M

ln

Y

= const. + Y (t).

dX(t)
|
|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)

(cid:21)
We point out that this relationship implies some degree of predictability of the absolute changes in return through
their median, if one knows the current value of the log-volatility Y (t). We test Eq. (20) for real data and with Y (t)
replaced by its estimate ˆY (t). As shown by the bottom curve of Fig. 6, the slope of the linear regression between
and ˆY (t) is not equal to 1 – as would have been implied by Eq. (20) – but 0.9 which still suggests
M
dX(t)
|
|
strong predictive power.
(cid:12)
(cid:12)

(cid:2)
Recall that the minimum time step of the empirical data used is 1 day. Hence, Eq. (20) implies the prediction of
tomorrow’s return based on today’s volatility and return. We now want to extend the prediction horizon. To this end
we generalize Eq. (20) and propose the following ansatz:

ln

Y

(cid:3)

(20)

ˆY

= const. + γ(h) ˆY (t),

(21)

M

ln

(cid:20)

dX(t + h)
|
|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

· · ·

where h = 0, 1, 2,
. In Fig. 6 we test this ansatz for several values of the horizon: h = 0, 5, 20, 100 and 1000 days.
We ﬁnd that the slope γ(h) is a decreasing function from the value γ(0) = 0.9 to practically zero when h = 1000 days
which means a complete loss of memory. Note that, when h = 100 trading days we have γ = 0.25 still implying a
slight degree of prediction after approximately ﬁve months, which is of the same order of magnitude than the DJIA
characteristic time scale, 1/α

500 days, for the relaxation of the volatility [? ].

∼

9

0.05

0.25

0.35

0.55

0.90

)
e
g
a
r
e
v
a
 
y
a
d
5
(
 
|

 

X
d

|
 

n

l

2

0

-2

-4

-6

-8

-10

-2

-1

0

1

2

3

estimated Y (5 day average)

Figure 6: The proportionality between the estimated volatility and of the logarithm of absolute return variations. In order to
decrease noise, 5-day moving averages have been used. Numbers on the right indicate the slopes of the corresponding regression
lines. Time shifts from bottom to top: h = 0 days ((cid:4)), 5 days ( ), 20 days (N), 100 days (H), 1000 days ((cid:7)). Days with
similar absolute returns were binned for better visibility. The symbols represent the medians, and the error bars the 25 − 75%
quantiles in the bins.

V. CONCLUSIONS

The volatility is a crucial quantity for ﬁnancial markets since it provides a measure of the amplitude of price
ﬂuctuations. Traders try to follow carefully the level of volatility because it gives the perception of the risk associated
with any asset. Although volatility was originally conceived to be the diﬀusion coeﬃcient of the price return random
walk, there is compelling evidence not to consider it a constant, but a subordinated random process. The framework is
analogous to that of random diﬀusion processes which have been applied to a large variety of phenomena in statistical
mechanics and condensed matter physics.

The main obstacle to get a better knowledge of the volatility’s nature is that it is not directly observed. In fact, this
is precisely the motivation behind the present research. Our main objective has been to develop a tool which visualizes
the sample path of volatility. The procedure derives a maximum likelihood estimate assuming that the volatility is
a hidden Markov process. To do so, one needs also to assume a speciﬁc model for the volatility. We have chosen
a class of two dimensional diﬀusions commonly known as stochastic volatility models, where the volatility acts as a
diﬀusion particle trapped in a potential well. We have focused on the expOU model and obtained promising results,
especially for three reasons: (i) the model is computationally feasible; (ii) its parameters can be easily obtained and
ﬁt the data reasonably well; and (iii) the distribution of the estimated volatility is log-normal, which is consistent
with the assumed expOU model.

We have shown for the Dow Jones index daily data that the sample path of our estimated volatility improves other
estimates. We have compared our estimation with a rather typical one which identiﬁes volatility with absolute return
changes. Our estimation is able to remove the existing bias in the stationary distribution of volatility while still
preserving the clustering in volatility time series. We have also studied the estimate of volatility that deconvolutes
the return by the simulation of a random Wiener path [? ]. This last procedure also provides a Gaussian distribution
for the log-volatility, albeit the distribution has too fat tails and pays the price of losing clustering and memory in
the volatility time series. Our new procedure is in fact a more sophisticated variant of this estimate since it ﬁlters out
Wiener realizations via maximum likelihood. The estimate drastically reduces the noise in the volatility path thus
preserving data clustering.

The median of the estimated volatility has also been related to trading volume by the power-law expression M [σ]
∝
V 0.55. A link between volatility and trading volume has been previously mentioned in diﬀerent studies however our
estimate is again capable to provide a less noisy regression. We must, indeed, stress the fact that this does not imply

that volatility is proportional to a power of the volume, but only that its typical value is and that ﬂuctuations around
the average might play an important role.

We have also seen that current returns are proportional to the estimated volatility, as otherwise expected. How-
ever, the main novelty is that we have observed how future returns are proportional to current volatility and their
predictability diminishes monotonically with the number of time steps ahead. This last ﬁnding implies that our
estimation method can be applied to predict the size of future returns with the knowledge of current volatility.

As a ﬁnal remark we stress the fact that the technique herein presented can be applied to a variety of physical
phenomena besides ﬁnance. One typical problem of this sort is provided by the Brownian motion inside a ﬁeld of
force in which inertial eﬀects are not negligible [? ]. In this situation the dynamics of the particle is described by
a two dimensional diﬀusion process (X(t), V (t)) representing the position and the velocity of the Brownian particle.
The maximum likelihood technique might provide a reliable estimate of the velocity in the case that, for instance, the
only accessible experimental measures are the positions of the particle at wide time steps, so that a measure of the
velocity – which implies the knowledge of two very close positions – is too noisy and unreliable.

Acknowledgments

ZE is grateful to J´anos Kert´esz for his support and to the Universitat de Barcelona for its hospitality during his visit
at the Departament de Fisica Fonamental; also support by OTKA T049238 is acknowledged. JP and JM acknowledge
support from Direcci´on General de Investigaci´on under contract No. FIS2006-05204.

Appendix A: DERIVATION OF THE LIKELIHOOD FUNCTION

In order to make notations more compact, in this Appendix the time dependence of the stochastic processes is

mostly indicated as a lower index. A generic stochastic volatility model is deﬁned as

dXt = f (Yt)dW1(t),
dYt =

g(Yt)dt + h(Yt)dW2(t).

−

dWi(t)

εi(t)√∆t,

(i = 1, 2),

≈

To explain the procedure it is more convenient to work with the discrete time version of the model. To this end,
suppose that ∆t is a small time step and that the driving noises in Eqs. (A1)-(A2) can be approximated by (cf Eq.
(16))

where εi(t) are independent standard Gaussian processes with zero mean and unit variance. We remark that the
approximation (A3) has to be understood in mean square sense [? ]. The discrete time equations of the model thus
read

Xt −
Yt −

Xt−∆t = f (Yt−∆t)ε1(t

∆t)√∆t

−
g(Yt−∆t)∆t + h(Yt−∆t)ε2(t

Yt−∆t =

−

∆t)√∆t

−

from which we get

ε1(t

∆t) =

ε2(t

∆t) =

−

−

,

Xt −
Xt−∆t
f (Yt−∆t)√∆t
Yt −

Yt−∆t + g(Yt−∆t)∆t
h(Yt−∆t)√∆t
Xτ , Yτ }

(τ = t

{

.

−

For a given number of realizations, the probability of the set
obtained, as we will see next.

∆t, t

2∆t, . . . , t

s) can be easily

−

−

Let us denote the set of realizations as

. Then the Markov property of the process ensures that one can
decompose the joint probability density function (pdf) of this set as a chain of products between conditional probability
densities. In consequence, the pdf of the whole sample path can be written as

{

}

X, Y

X, Y

) = P(Xt−s, Yt−s)

P(
{

}

P(Xτ , Yτ |

Xτ −∆t, Yτ −∆t),

t−∆t

τ =t−s
Y

10

(A1)
(A2)

(A3)

(A4)

(A5)

(A6)

(A7)

(A8)

11

(A11)

where the ﬁrst term, P(Xt−s, Yt−s), corresponds to the initial realizations of X and Y s/∆t time steps far from the
present time t; all the remaining terms of the form P(Xτ , Yτ |
Xτ −∆t, Yτ −∆t) are the conditional pdf’s for transitions
between consecutive steps:

The logarithm of Eq. (A8) is

(Xτ −∆t, Yτ −∆t)

(Xτ , Yτ ).

−→

ln P(
{

}

X, Y

) = ln P(Xt−s, Yt−s) +

ln P(Xτ , Yτ |

Xτ −∆t, Yτ −∆t).

(A9)

t−∆t

τ =t−s
X

On the other hand from Eqs. (A4)-(A5) we realize that

P(Xτ , Yτ |
is the Jacobian of the transformation (Xτ , Yτ )

Xτ −∆t, Yτ −∆t) =

J
|

P(ε1(τ
|

(ε1(τ

−

−→

∆t), ε2(τ

−
∆t), ε2(τ

−

∆t)),

−
∆t)) deﬁned by Eqs. (A4)-(A5), that

where
is,

J
|

|

But ε1 and ε2 are independent standard Gaussians, hence

whence

=

J
|

|

1
f (Yτ −∆t)h(Yτ −∆t)∆t

.

P(ε1, ε2) = (1/2π) exp

(ε2

1 + ε2

2)/2

,

(cid:2)

(cid:3)

P(Xτ , Yτ |

Xτ −∆t, Yτ −∆t) =

1/(2π∆t)
f (Yτ −∆t)h(Yτ −∆t)

exp

ε2
1(τ

−

∆t) + ε2
2

2(τ

−

∆t)

−

(cid:20)

.

(cid:21)

(A10)

Substituting Eqs. (A6)-(A7) into this equation and the result into Eq. (A9) we ﬁnally get

ln P(
{

X, Y

) =

}

s ln(2π∆t)
∆t

−

−

+ ln P(Xt−s, Yt−s)

t−∆t

τ =t−s
X
1
2

−

[ln f (Yτ −∆t) + ln h(Yτ −∆t)]

t−∆t

τ =t−s (cid:20)
X

Xτ −∆t
Xτ −
f (Yτ −∆t)∆t

2

∆t

(cid:21)

1
2

−

t−∆t

τ =t−s (cid:20)
X

Yτ −∆t
Yτ −
h(Yτ −∆t)∆t

+

g(Yτ −∆t)
h(Yτ −∆t)

2

(cid:21)

∆t.

−

Let us brieﬂy explain the origin of some of these contributions. The ﬁrst summand comes from the normalization
constant of the Gaussian distribution (A10). It appears in every conditional probability density and this is the reason
s and t. The resulting term does not depend on
for the factor s/∆t, which is the number of time steps between t
the realization, so that we can neglect it for a maximization with respect to Y. The term also goes to
in the
∆t

0 limit, which means that any individual realization has a probability measure zero.

→
The second summand is mostly the sum of the Jacobian transformations of each transition probability and depends
on Y . Stochastic volatility models typically assume that these f and g [cf. Eqs. (A1) and (A2)] are continuous and
monotonically increasing functions or even constants. For instance, in the expOU model we have f (x) = m exp(x)
and g(x) = k. Because of this, we will also neglect this term in the maximization procedure. The contribution shifts
the maximum at the excessive cost of adding more noise to the numerical computations. We can however look at
the situation from another point of view. Ignoring this term is equivalent to omitting the Jacobian transformation
between the two probability density measures [cf. Eq.(A10)]. In this way, we are stating that what we are really going
to maximize is the probability of the realization of ε1(t) and ε2(t) –instead of Y (t)– in terms of the past history of
the process expressed by

The term ln P(Xt−s, Yt−s) is ﬁxed by the initial conditions of the process. If we assume a known initial return X –
which can be set to zero – and take a random Yt−s following the stationary distribution Pst(Yt−s) given by Eq. (9),
then P(Xt−s, Yt−s) = δ(Xt−s −

X) Pst(Yt−s) and hence

X, Y

−∞

}

{

.

ln P(Xt−s, Yt−s) = ln Pst(Yt−s) + ln δ(Xt−s −

X).

(A12)

Had we taken another initial condition, the technique would have given equivalent results (we have checked this by
using several initial distributions). For this reason and in order to improve the convergence of the maximum likelihood
estimate we have neglected also this contribution.

We therefore write

ln P(
{

X, Y

)

}

≃ −

1
2

−

t−∆t

1
2

τ =t−s (cid:20)
X

t−∆t

τ =t−s (cid:20)
X

Xτ −∆t
Xτ −
f (Yτ −∆t)∆t

2

∆t

(cid:21)

Yτ −∆t
Yτ −
h(Yτ −∆t)∆t

+

g(Yτ −∆t)
h(Yτ −∆t)

∆t +

· · ·

2

(cid:21)

We can represent this equation in the continuous time framework if ∆t is suﬃciently small and if f (x), h(x) and g(x)
are continuous. In such a case, Eq. (A13) yields the result given in Eq. (11).

Appendix B: DERIVATION OF EQ. (16)

We start from Eq. (7) which we write in the approximate form

∆X(t)

meY (t)∆W (t),

≃

thus ln

∆X(t)
|
|

= m + Y (t) + ln

and, taking into account that

= 0, we have

∆W1(t)
|
|
ln
h

∆X(t)
|

|i ≃

ln m +

ln
h

Y (t)
i
h
∆W1(t)
|

.
|i

On the other hand, we know that ∆W1(t)

ε√∆t, where ε is a standard Gaussian variable (cf Eq. (16)). Hence

But

≈
∆W1(t)
|

ln
h

|i ≈ h

ln

ε
|

|i

+ (ln ∆t)/2.

ln
h

ε
|

|i

=

1
√2π

∞

2

e−ε

/2 ln

ε
|

dε,
|

−∞

Z

which, after a simple change of variables inside the integral, yields [? ? ]

ln
h

ε
|

|i

=

1
2√2π

∞

0
Z

x−1/2e−x/2 ln xdx =

π/2(γ + ln 2),

−

p

where γ = 0.5772

is the Euler constant. Therefore,

· · ·

|i ≈
Substituting Eq. (B2) into Eq. (B1) proves Eq. (17).

ln
h

∆W1(t)
|

(ln ∆t)/2

(γ + ln 2)/2.

−

12

(A13)

(B1)

(B2)


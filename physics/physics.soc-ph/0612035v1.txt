6
0
0
2
 
c
e
D
 
5
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
5
3
0
2
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

An information-theoretic framework for resolving community structure in
complex networks

Martin Rosvall∗ and Carl T. Bergstrom†

Department of Biology, University of Washington, Seattle, WA 98195-1800

This paper develops a rigorous foundation for the concept of modularity in networks. We present an explicit
method to best partition a complex network by ﬁnding an optimal compression of its topology, capitalizing on
regularities in its structure. To illustrate the power of our approach, we partition a number of real-world and
model networks and explain why our results outperform other methods.

We can comprehend the structure of a dauntingly complex
network by identifying the communities or modules of which
it is composed (1, 2, 3, 4, 5). When we describe a network as
a set of interconnected modules, we are highlighting certain
regularities of the network’s structure while ﬁltering out the
relatively unimportant details. Thus a modular description of
a network is a lossy compression of that network’s topology
— and the problem of community identiﬁcation is at its core a
problem of ﬁnding an eﬃcient compression of the network’s
structure.

This view suggests that we should approach the challenge
of identifying the community structure of a complex network
as a fundamental problem in information theory (6, 7, 8).
We provide the groundwork for this information-theoretic ap-
proach to community detection, and explain why this ap-
proach outperforms other methods for community detection.
Figure 1 illustrates our basic framework for identifying
communities. We envision the process of describing a com-
plex network by a simpliﬁed summary of its module structure
as a communication process. The link structure of a complex
network is a random variable X; a signaler knows the full form
of the network X, and aims to convey much of this information
in a reduced fashion to a signal receiver. To do so, the signaler
encodes information about X as some simpliﬁed description
Y. She sends the encoded message through a noiseless com-
munication channel. The signal receiver observes the message
Y, and then “decodes” this message, using it to make guesses
Z about the structure of the original network X.

There are many diﬀerent ways to describe a network X by a
simpler description Y. Which of these is best? The answer to
this question of course depends on what you want to do with
the description. Nonetheless, information theory oﬀers an ap-
pealing general answer to this question. Given some set of
candidate descriptions Yi, the best description Y of a random
variable X is the one that tells the most about X — that is, the
one that maximizes the mutual information I(X; Y) between
description and network.

Since we are interested in identifying community structure,
we will explore descriptions Y that summarize the structure
of a network X by enumerating the communities or modules

∗Electronic
URL: http://www.tp.umu.se/~rosvall/
†URL: http://octavia.zoology.washington.edu/

address:

rosvall@u.washington.edu;

within X, and describing the relations among them. In this
paper, we will consider one particular method of encoding the
community structure of X. More generally one could and in-
deed should consider alternative “encoders” so as to choose
one best suited for the problem at hand.

We consider an unweighted and undirected network X of
size n with l links, which can be described by the adjacency
matrix

Ai j =

1
0

if there is a link between nodes i and j
otherwise.

(1)





We choose the description

Y =

a =

, M = 

(2)

a1
...
...
an














· · ·
l11
...
. . .
lm1 · · ·

l1m
...
lmm












for m modules, where a is the module assignment vector,
ai ∈ {1, 2, . . . , m}, and M is the module matrix. Given the
assignment vector and the actual network, M = M(X, a) is the
modular description of the actual network, with the m mod-
ules connected pairwise by li j links between module i and j,
and ni nodes in module i connected internally by lii (see Fig.
1).

To ﬁnd the best assignment a∗ we now maximize the mutual
information over all possible assignments of the nodes into m
modules

a∗ = arg max

I(Y; X).

a

(3)

By deﬁnition, the mutual information I(Y; X) = H(X) −
H(X|Y) = H(X) − H(Z), where H(X) is the information neces-
sary to describe X and the conditional information H(X|Y) =
H(Z) is the information necessary to describe X given Y (see
Fig. 1). We therefore seek to to minimize H(Z). This is equiv-
alent to constructing an assignment vector such that the set of
network estimates Z in Fig. 1 is as small as possible. Given
that the description Y assigns nodes to m modules,

m

ni(ni − 1)/2
li

! Yi> j  

H(Z) = log 
Yi=1  


,

nin j
li j !


(4)

where the parentheses denote the binomial coeﬃcients and
the logarithm is taken in base 2. Each of the m binomial

X

Actual
network

Encoder

Decoder

Z

2

Network
estimates

Y
Signal

n
l

l

n
l

l

l

n
l

FIG. 1 Basic framework for detecting communities as a communication process. A signaler knows the full network structure and wants to
send as much information as possible about the network to a receiver over a channel with limited capacity. The signaler therefore encodes the
network into modules in a way that maximizes the amount of information about the original network. This ﬁgure illustrates an encoder that
compresses the network into 3 modules i = ,
, with ni nodes and lii links in each module, and li j links between the modules. The receiver
can then decode the message and construct a set of possible candidates for the original network. The smaller the set of candidates, the more
information the signaler has managed to transfer.

,

coeﬃcients in the ﬁrst product gives the number of diﬀerent
modules that can be constructed with ni nodes and lii links.
Each of the m(m + 1)/2 binomial coeﬃcients in the second
product gives the number of diﬀerent ways module i and j
can be connected to one another.

Figure 2 shows that our cluster-based compression method
splits the network close to the division along which the actual
dolphin groups were observed to split (9). Only 6 links cross
between the two clusters, one with 21 members and the other
with 41. Because it is computationally infeasible to check
all possible partitions of even modestly-sized networks, we
use Markov chain Monte Carlo (MCMC) with Metropolis-
Hastings sampling to search for the partition that maximizes
the mutual information between the description and the orig-
inal network. We have conﬁrmed the results with exhaustive
search in the vicinity of the MCMC solutions.

We compare our results with the partition obtained by using
the modularity approach introduced by Newman and Girvan
in ref. (10); that technique has been widely adopted because of
its appealing simplicity, its strong performance in benchmark
tests (5), and the availability of powerful numerical techniques
for dealing with large networks (11, 12, 13). Given a parti-
tioning into m modules, the modularity Q is the sum of the
contributions from each module i

Q =

lii/l − (di/2l )2,

(5)

m

Xi=1

where lii is the number of links between nodes in the i-th mod-
ule, di the total degree in module i, and l is the total number
of links in the network. When we maximize the modularity,
we are not just minimizing the number of links between mod-

ules, but rather ﬁnding a conﬁguration which maximizes the
number of links within modules in the actual network minus
the expected number of links within comparable modules in a
random network with the same degree sequence. Or equiva-
lently, we aim to divide the network such that the number of
links within modules is higher than expected (12).

This approach works perfectly for networks where the mod-
ules are similar in size and degree sequence (5). However,
when we partition the dolphin network in Fig. 2 using the
modularity approach, we divide the network such that 12 in-
stead of 6 links connect the two modules. Why? Because of
the (2l )2 denominator in the second term of Eq. (5), the choice
of partition is highly sensitive to the total number of links in
the system. Thus modularity-based partitioning is somewhat
arbitrary in the sense that if we add an additional distinct clus-
ter and then partition the whole network allowing one more
module, the division point between the original modules may
shift substantially with the increased number of links in the
total system. For example, imagine that in the dolphin net-
work in Fig. 2, we added a distinct group of dolphins forming
a third cluster. If the third group is equal in size as the origi-
nal network, the resulting three-way partition would divide the
original part of the dolphin network just as our cluster-based
compression method does.

To test our cluster-based compression method quantita-
tively we conducted the benchmark tests described in refs.
(1, 5). In these tests, 128 nodes are divided into four equally
sized groups with average degree 16. As the average number
of links kout from a node to nodes in other groups increases,
it becomes harder and harder to identify the underlying group
structure. We generated 100 diﬀerent networks with the
described procedure for each value kout = 6, 7, 8 and searched
with the method described above for the solution that max-

3

A

B

h
t
g
n
e

l

n
o
i
t
p
i
r
c
s
e
d
m
u
m
n
M

i

i

400

300

FIG. 2 The dolphin network by (9) partitioned with our cluster-based
compression (solid line) and based on the modularity (dashed line).
The stars and circles represent the two observed groups of dolphins.
The right branch of the dashed line represents a split based on max-
imizing the modularity, which is diﬀerent from the left branch solu-
tion based on the spectral analysis presented in ref. (14). The edge-
betweenness algorithm presented in ref. (1) splits the network in the
same way as our cluster-based compression method (10).

imized the mutual information between the description and
the original network. We found a correct assignment with
probability 1.00 (0) for kout = 6, 0.98 (.01) for kout = 7, and
0.89 (.05) for kout = 8, with the standard deviation in the
parenthesis. This accuracy is at the same level as a simulated
annealing approach to maximize the modularity (2, 15) and
outperforms all other approaches (5). The close accord
between the results for the information-theoretic approach
presented here and the best algorithm based on the modularity
show that the two methodologies behave similarly when the
module structure of the network is suﬃciently regular.

In some special cases we will know a priori how many mod-
ules compose our sample network, but in general the task of
resolving community structure is twofold. We must determine
the number of modules in the network, and then we need to
partition the nodes into that number of modules. The catch
is that we cannot determine the optimal number of modules
without also considering the assignments of nodes — so these
problems need to be solved simultaneously. Below, we pro-
vide a solution grounded in algorithmic information theory.

Looking back at Fig. 1, the encoder seeks to ﬁnd an com-
pression of the network so that the decoder can make the best
possible estimate of the actual network. One approach would
be to have the encoder partition the network into n modules,
one for each node, and thereby ensure that decoder can recon-
struct the network completely, but under this approach noth-
ing is gained either in compression or module identiﬁcation.
Therefore the encoder must balance the amount of informa-
tion necessary to describe the network in modular form, as
given by the signal Y in Fig. 1, and the uncertainty that re-
mains once the decoder receives the modular description, as
given by the size of the set of network estimates Z in Fig. 1.
This is an optimal coding problem and can be resolved by the
Minimum Description Length (MDL) principle (6, 17, 18).
The idea is to exploit the regularities in the structure of the
actual network X to summarize it in condensed form, without
overﬁtting it. What do we mean by overﬁtting in this context?

1

2

3
Number of modules

4

5

FIG. 3 Partitioning into an optimal number of modules. The net-
work in panel A consists of 40 journals as nodes from four diﬀerent
ﬁelds: multidisciplinary physics (squares), chemistry (circles), biol-
ogy (stars), and ecology (triangles). The 189 links connect nodes
if at least one article from one of the journals cites an article in the
other journal during 2004 (16). We have selected the 10 journals
with the highest impact factor in the four diﬀerent ﬁelds, but disre-
garded journals classiﬁed in one or more of the other ﬁelds. Panel
B shows the minimum description length for the network in panel A
partitioned into 1 to 5 diﬀerent modules. The optimal partitioning
into four modules is illustrated by the lines in panel A.

Figure 3 illustrates. We want to choose a set of modules for
the journal citation network in Fig. 3 such that if we were to
repeat the experiment next year, each journal would likely be
assigned to the same module again. If we overﬁt the data, we
may capture more of a speciﬁc year’s data, but unwittingly
also the noise that will not recur in next year’s data.

To minimize the description length of the original network
X, we look for the number of modules m such that the de-
scription length of the modular description Y plus the condi-
tional description length — the amount of additional informa-
tion that would be needed to specify X exactly to a receiver
who had already decoded the description Y — is as short as
possible (6). That is, we seek to minimize the sum

L(Y) + L(X|Y),

(6)

4

A

B

where L(Y) is the length in bits of the signal and L(X|Y) is
number of bits needed to specify which of the network esti-
mates implied by the signal Y is actually realized. The de-
scription length is easy to calculate in this discrete case and is
given by

L(Y) + L(Z|Y) = n log m +

m(m + 1) log l + H(Z),

(7)

1
2

where the ﬁrst and second term give the size necessary to en-
code the assignment vector a and the module matrix M(X, a),
and H(Z) is given in Eq. (4). Figure 3B shows the description
length with the journal network partitioned into one to ﬁve
modules. Four modules yield the minimum description length
and we show the corresponding partition in Fig. 3A.

This cluster-based compression assigns 39 of the 40 jour-
nals into the proper categories, but places the central hub
Physical Review Letters (PRL) in the chemistry cluster. This
may seem like a mistake, given that PRL has 9 links to physics
and only 8 to chemistry. Indeed, a partitioning based on the
modularity score Q places PRL among the physics journals.
But whatever its subject matter, the structural role that PRL
plays in the journal network is really that of a chemistry jour-
nal. Like most of the chemistry journals, and unlike its com-
patriots in physics, PRL is closely linked to biology and some-
what connected to ecology.

While doing so yields a somewhat

longer description
length, we can also partition the network into two, three, or
ﬁve modules. When we compress the network into two com-
ponents, physics clusters together with chemistry and biology
clusters together with ecology. When we split into three com-
ponents, ecology and biology separate but physics and chem-
istry remain together in a single module. When we try to split
the network into ﬁve modules, we get essentially the same
partition as with four, only with the singly connected journal
Conservation Biology split oﬀ by itself into its own partition.
We cannot partition the network into more than ﬁve modules
without creating at least one module that has a majority of its
links to nodes in other modules. For purposes of illustration
we have softened this constraint and accepted single nodes
with only one out-link as modules. Because of this concept of
what a module is (19), we constrained our model description
to clusters satisfying lii > li j for all i and j in Eq. (2). In many
cases we get a higher mutual information if we remove this
constraint; in such cases we typically observe that hubs are
clustered together and peripheral nodes are clustered together.
When this is true, we can describe the network structure more
eﬃcient by clustering nodes with similar roles instead of clus-
tering nodes that are closely connected to one another. The
mixture model approach provides an alternative method of
identifying aspects of network structure beyond positive as-
sortment (20).

To visualize the diﬀerence we split Zachary’s classic karate
club network (21) with (panel A) and without (panel B) the
link constraint (Fig. 4). In A the partitioning corresponds ex-
actly to the splitting that was observed by Zachary, but in B
instead the 5 members with the highest degrees are clustered
together. The compression with the hubs in one cluster and
the peripheral nodes in the other cluster is in this case more

FIG. 4 Zachary’s karate club network (21) partitioned into two mod-
ules based on the maximum mutual information with (panel A) and
without (panel B) the link constraint. The partitioning with more
links within modules than between modules in panel A clusters
closely connected nodes together and the unconstrained partitioning
in panel B clusters nodes with similar roles together.

eﬃcient.

Conclusions

We have shown that the process of resolving community
structure in complex networks is at its core a problem in data
compression. By drawing out the relationship between mod-
ule detection and optimal coding we are able to ground the
concept of network modularity in the rigorous formalism pro-
vided by information theory.

Enumerating the modules in a network is an act of descrip-
tion; there is an inevitable tradeoﬀ between capturing most of
the network structure at the expense of needing a long descrip-
tion with many modules, and omitting some aspects of net-
work structure so as to allow a shorter description with fewer
modules. Moreover, our information-theoretic approach sug-
gests that there is a natural scale on which to best describe the
network, a single answer to how to best balance this tradeoﬀ
between under- and over-description.

While the information-theoretic view described here pro-
vides a fully general basis for how to get the most information
out of a network structure, the sort of information that we wish
to extract may vary from application to application. In this pa-
per, we have presented one particular encoder, which extracts
information about the community structure of unweighted,
undirected networks. However, the method is easy to general-

ize to directed and weighted networks as well, by modifying
the modular description appropriately. To illustrate the power
of our cluster based-compression, we have partitioned a num-
ber of real-world and model networks and explained why our
results outperform other methods.

Acknowledgments

We thank Ben Althouse for generating the network used
in Fig. 3. This work was supported by the National Institute
of General Medical Sciences Models of Infectious Disease
Agent Study program cooperative agreement 5U01GM07649.

References

(2002).

(2003).

1. M. Girvan, M. E. J. Newman, Proc Natl Acad Sci USA 99, 7821

2. R. Guimerà, L. A. N. Amaral, Nature 433, 895 (2005).
3. P. Holme, M. Huss, H. Jeong, Bioinformatics pp. 532–538

4. M. E. J. Newman, Eur Phys J B 38, 321 (2004).
5. L. Danon, A. Díaz-Guilera, J. Duch, A. Arenas, J Stat Mech p.

P09008 (2005).

5

6. J. Rissanen, Automatica 14, 465 (1978).
7. C. E. Shannon, W. Weaver, The mathematical theory of commu-

nication (Univ of Illinois Press, 1949).

8. E. Ziv, M. Middendorf, C. H. Wiggins, Phys Rev E 71, 046117

(2005).

9. D. Lusseau, et al., Behav Ecol Sociobiol 54, 396 (2003).
10. M. E. J. Newman, M. Girvan, Phy Rev E 69, 026113 (2004).
11. M. E. J. Newman, Phys Rev E 69, 066133 (2004).
12. M. E. J. Newman, Proc Natl Acad Sci USA 103, 8577 (2006).
13. J. Reichardt, S. Bornholdt, Phy Rev E 74, 016110 (2006).
14. M. E. J. Newman, Phys Rev E 74, 036104 (2006).
15. R. Guimerà, L. A. N. Amaral, J Stat Mech p. P02001 (2005).
16. Journal citation reports (2004). Thompson Scientiﬁc, Institute

for Scientiﬁc Information.

17. A. Barron, J. Rissanen, B. Yu, IEEE T Inform Theory 44, 2743

(1998).

18. P. Grünwald, I. J. Myung, M. Pitt, eds., Advances in minimum
description length: theory and applications (MIT Press, 2005),
chap. 1–2.

19. F. Radicchi, C. Castellano, F. Cecconi, V. Loreto, D. Parisi, Proc

Natl Acad Sci USA 101, 2658 (2004).

20. M. E. J. Newman, E. A. Leicht, e-print physics/0611158.
21. W. W. Zachary, J Anthropol Res 33, 452 (1977).


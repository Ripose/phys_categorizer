6
0
0
2
 
p
e
S
 
1
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
8
8
0
9
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The why of the applicability
of Statistical Physics to Economics

Esteban Guevara Hidalgo
Departamento de F´ısica, Escuela Polit´ecnica Nacional, Quito, Ecuador

We analyze the relationships between game theory and quantum mechanics and the extensions
to statistical physics and information theory. We use certain quantization relationships to assign
quantum states to the strategies of a player. These quantum states are contained in a density
operator which describes the new quantum system. The system is also described through an entropy
over its states, its evolution equation which is the quantum replicator dynamics and a criterion of
equilibrium based on the Collective Welfare Principle. We show how the density operator and
entropy are the bonds between game theories, quantum information theory and statistical physics.
We propose the results of the study of these relationships like a reason of the applicability of physics
in economics and the born of econophysics.

PACS numbers: 03.65.-w, 02.50.Le, 03.67.-a, 89.65.Gh

1.

INTRODUCTION

Why has it been possible to apply some methods of sta-
tistical physics to economics? It is a good reason to say
that physics is a model which tries to describe phenomena
and behaviors and if this model ﬁts and describes almost
perfectly the observed and the measured even in the eco-
nomic world then there is no problem nor impediment to
apply physics to solve problems in economics. But, could
economics and statistical physics be correlated? Could
it have a relationship between quantum mechanics and
game theory? or could quantum mechanics even enclose
theories like games and the evolutionary dynamics. This
possibility could make quantum mechanics a theory more
general that we have though.

Problems in economy and ﬁnance have attracted the
interest of statistical physicists. Ausloos et al [1] ana-
lyzed fundamental problems pertain to the existence or
not long-, medium-, short-range power-law correlations
in economic systems as well as to the presence of ﬁnan-
cial cycles. They recalled methods like the extended de-
trended ﬂuctuation analysis and the multi-aﬃne analysis
emphasizing their value in sorting out correlation ranges
and predictability. They also indicated the possibility of
crash predictions. They showed the well known ﬁnancial
analyst technique, the so called moving average, to raise
questions about fractional Brownian motion properties.
The (m, k)-Zipf method and the i−variability diagram
technique were presented for sorting out short range cor-
relations.

J.-P. Bouchaud [2] analyzed three main themes in the
ﬁeld of statistical ﬁnance, also called econophysics: (i)
empirical studies and the discovery of universal features
in the statistical texture of ﬁnancial time series, (ii) the
use of these empirical results to devise better models of
risk and derivative pricing, of direct interest for the ﬁnan-
cial industry, and (iii) the study of “agent-based models”
in order to unveil the basic mechanisms that are respon-
sible for the statistical “anomalies” observed in ﬁnancial
time series.

Statistical physicists are also extremely interested in
ﬂuctuations [3]. One reason physicists might want to
quantify economic ﬂuctuations is in order to help our
world ﬁnancial system avoid “economic earthquakes”.
Also it is suggested that in the ﬁeld of turbulence, we
may ﬁnd some crossover with certain aspects of ﬁnancial
markets.

Kobelev et al [4] used methods of statistical physics
of open systems for describing the time dependence of
economic characteristics (income, proﬁt, cost, supply,
currency, etc.) and their correlations with each other.
They also oﬀered nonlinear equations (analogies of known
reaction-diﬀusion, kinetic, Langevin equation) to de-
scribe appearance of bifurcations, self-sustained oscilla-
tional processes, self-organizations in economic phenom-
ena.

It is generally accepted that entropy can be used for
the study of economic systems consisting of large num-
ber of components [5]. I. Antoniou et al [6] introduced a
new approach for the presentation of economic systems
with a small number of components as a statistical sys-
tem described by density functions and entropy. This
analysis is based on a Lorenz diagram and its interpo-
lation by a continuos function. Conservation of entropy
in time may indicate the absence of macroscopic changes
in redistribution of resources. Assuming the absence of
macro-changes in economic systems and in related addi-
tional expenses of resources, we may consider the entropy
as an indicator of eﬃciency of the resources distribution.
This approach is not limited by the number of compo-
nents of the economic system and can be applied to wide
class of economic problems. They think that the bridge
between distribution of resources and proposed probabil-
ity distributions may permit us to use the methods of
nonequilibrium statistical mechanics for the study and
forecast of the dynamics of complex economic systems
and to make correct management decisions.

Statistical mechanics and economics study big en-
sembles: collections of atoms or economic agents, re-
law of equilibrium sta-
spectively. The fundamental

tistical mechanics is the Boltzmann-Gibbs law, which
states that the probability distribution of energy E is
P (E) = Ce−E/T , where T is the temperature, and C
is a normalizing constant. The main ingredient that is
essential for the derivation of the Boltzmann-Gibbs law
is the conservation of energy. Thus, one may generalize
that any conserved quantity in a big statistical system
should have an exponential probability distribution in
equilibrium [7]. In a closed economic system, money is
conserved. Thus, by analogy with energy, the equilibrium
probability distribution of money must follow the expo-
nential Boltzmann-Gibbs law characterized by an eﬀec-
tive temperature equal to the average amount of money
per economic agent. Dr˘agulescu and Yakovenko demon-
strated how the Boltzmann-Gibbs distribution emerges
in computer simulations of economic models. They con-
sidered a thermal machine, in which the diﬀerence of tem-
perature allows one to extract a monetary proﬁt. They
also discussed the role of debt, and models with broken
time-reversal symmetry for which the Boltzmann-Gibbs
law does not hold.

Recently the insurance market, which is one of the im-
portant branches of economy, have attracted the atten-
tion of physicists [8]. Some concepts of the statistical
mechanics, specially the maximum entropy principle is
used for pricing the insurance. Darooneh obtained the
price density based on this principle, applied it to multi
agents model of insurance market and derived the utility
function. The main assumption in his work is the cor-
respondence between the concept of the equilibrium in
physics and economics. He proved that economic equi-
librium can be viewed as an asymptotic approximation to
physical equilibrium and some diﬃculties with mechani-
cal picture of the equilibrium may be improved by con-
sidering the statistical description of it. TopsØe [9] also
has suggested that thermodynamical equilibrium equals
game theoretical equilibrium.

In this paper we try to ﬁnd a deeper relationship be-
tween quantum mechanics and game theory and propose
the results of the study of these relationships like a reason
of the applicability of physics in economics.

2. CLASSICAL, EVOLUTIONARY &
QUANTUM GAMES

Game theory [10, 11, 12] is the study of decision mak-
ing of competing agents in some conﬂict situation. It tries
to understand the birth and the development of conﬂict-
ing or cooperative behaviors among a group of individu-
als who behave rationally and strategically according to
their personal interests. Each member in the group strive
to maximize its welfare by choosing the best courses of
strategies from a cooperative or individual point of view.
The central equilibrium concept in game theory is the
Nash Equilibrium. A Nash equilibrium (NE) is a set of
strategies, one for each player, such that no player has an
incentive to unilaterally change his action. Players are in

2

equilibrium if a change in strategies by any one of them
would lead that player to earn less than if he remained
with his current strategy. A Nash equilibrium satisﬁes
the following condition

E(p, p) ≥ E(r, p),

(1)

where E(si, sj) is a real number that represents the payoﬀ
obtained by a player who plays the strategy si against
an opponent who plays the strategy sj. A player can not
increase his payoﬀ if he decides to play the strategy r
instead of p.

Evolutionary game theory [13, 14, 15] does not rely
on rational assumptions but on the idea that the Dar-
winian process of natural selection [16] drives organisms
towards the optimization of reproductive success [17]. In-
stead of working out the optimal strategy, the diﬀerent
phenotypes in a population are associated with the ba-
sic strategies that are shaped by trial and error by a
process of natural selection or learning. The natural se-
lection process that determines how populations playing
speciﬁc strategies evolve is known as the replicator dy-
namics [14, 15, 18, 19] whose stable ﬁxed points are Nash
equilibria [11]. The central equilibrium concept of evolu-
tionary game theory is the notion of Evolutionary Stable
Strategy introduced by J. Smith and G. Price [13, 20].
An ESS is described as a strategy which has the prop-
erty that if all the members of a population adopt it,
no mutant strategy could invade the population under
the inﬂuence of natural selection. ESS are interpreted as
stable results of processes of natural selection.

Consider a large population in which a two person
game G = (S, E) is played by randomly matched pairs of
animals generation after generation. Let p be the strat-
egy played by the vast majority of the population, and let
r be the strategy of a mutant present in small frequency.
Both p and r can be pure or mixed. An evolutionary
stable strategy (ESS) p of a symmetric two-person game
G = (S, E) is a pure or mixed strategy for G which sat-
isﬁes the following two conditions

E(p, p) > E(r, p),
If E(p, p) = E(r, p) then E(p, r) > E(r, r).

(2)

Since the stability condition only concerns to alternative
best replies, p is always evolutionarily stable if (p, p) is
an strict equilibrium point. An ESS is also a Nash equi-
librium since is the best reply to itself and the game is
symmetric. The set of all the strategies that are ESS
is a subset of the NE of the game. A population which
plays an ESS can withstand an invasion by a small group
of mutants playing a diﬀerent strategy. It means that if
a few individuals which play a diﬀerent strategy are in-
troduced into a population in an ESS, the evolutionarily
selection process would eventually eliminate the invaders.
Quantum games have proposed a new point of view
for the solution of the classical problems and dilemmas
in game theory. Quantum games are more eﬃcient than
classical games and provide a saturated upper bound for
this eﬃciency [21, 22, 23, 24, 25, 26].

3. REPLICATOR DYNAMICS & EGT

the replicator dynamics [27]

The model used in EGT is the following: Each agent
in a n-player game where the ith player has as strategy
space Si is modelled by a population of players which
have to be partitioned into groups.
Individuals in the
same group would all play the same strategy. Randomly
we make play the members of the subpopulations against
each other. The subpopulations that perform the best
will grow and those that do not will shrink and eventu-
ally will vanish. The process of natural selection assures
survival of the best players at the expense of the others.
The natural selection process that determines how popu-
lations playing speciﬁc strategies evolve is known as the
replicator dynamics

dX
dt

dX
dt

= [[Q, X] , X] ,

= [Λ, X] .

n
k=1 aikxk) xij − xji (

The matrix Λ is equal to Λ = [Q, X] with (Λ)ij =
n
1
k=1 ajkxk)] and Q is a diag-
2 [(
onal matrix which has as elements qii = 1
2 P

n
k=1 aikxk.

P

P

4. RELATIONSHIPS BETWEEN QUANTUM
MECHANICS & GAME THEORY

In table 1 we compare some characteristic aspects of

quantum mechanics and game theory.

3

(8)

(9)

= [fi(x) − hf (x)i] xi,

Table 1

dxi
dt

dxi
dt

=

n





X
j=1

aijxj −

aklxkxl

xi.

n

X
k,l=1





(3)

(4)

(5)

(6)

P

The probability of playing certain strategy or the relative
frequency of individuals using that strategy is denoted
n
by frequency xi. The ﬁtness function fi =
j=1 aijxj
speciﬁes how successful each subpopulation is, hf (x)i =
n
k,l=1 aklxkxl is the average ﬁtness of the population,
P
and aij are the elements of the payoﬀ matrix A. The
replicator dynamics rewards strategies that outperform
the average by increasing their frequency, and penal-
izes poorly performing strategies by decreasing their fre-
quency. The stable ﬁxed points of the replicator dynam-
ics are Nash equilibria, it means that if a population
reaches a state which is a Nash equilibrium, it will re-
main there.

We can represent the replicator dynamics in matrix

The relative frequencies matrix X has as elements

dX
dt

= G + GT .

xij = (xixj)1/2

form

and

G + GT
(cid:0)

(cid:1)ij =

aikxkxij

1
2

n

X
k=1
n

+

1
2

X
k=1
n

−

X
k,l=1

ajkxkxji

aklxkxlxij

(7)

are the elements of the matrix
. From this ma-
(cid:1)
trix representation we can ﬁnd a Lax representation of

G + GT
(cid:0)

Quantum Mechanics

Game Theory

n system members

Quantum states

n players

Strategies

Density operator

Relative frequencies vector

Von Neumann equation

Replicator Dynamics

Von Neumann entropy

Shannon entropy

System Equilibrium

Payoﬀ

Maximum entropy

Maximum payoﬀ

“Altruism”

Altruism or selﬁsh

Collective Welfare principle Minority Welfare principle

It is easy to realize the clear resemblances and appar-
ent diﬀerences between both theories and between the
properties both enjoy. This was a motivation to try to
ﬁnd an actual relationship between both systems.

We have to remember that Schr¨odinger equation de-
scribes only the evolution of pure states in quantum
mechanics. To describe correctly a statistical mixture
of states it is necessary the introduction of the density
operator

ρ(t) =

pi |Ψi(t)i hΨi(t)|

(10)

n

X
i=1

which contains all the information of the statistical sys-
tem. The time evolution of the density operator is given
by the von Neumann equation

i~ dρ
dt

= h

ˆH, ρi

(11)

which is only a generalization of the Schr¨odinger equation
and the quantum analogue of Liouville’s theorem.

Evolutionary game theory has been applied to the so-
lution of games from a diﬀerent perspective. Through the
replicator dynamics it is possible to solve not only evolu-
tionary but also classical games. That is why EGT has
been considered like a generalization of classical game
theory. The bonestone of EGT is the concept of evo-
lutionary stable strategy (ESS) that is a strengthened

notion of Nash equilibrium. The evolution of relative
frequencies in a population is given by the replicator dy-
namics. In a recent work we showed that vectorial equa-
tion can be represented in a matrix commutative form
(9). This matrix commutative form follows the same dy-
namic than the von Neumann equation and the proper-
ties of its correspondent elements (matrixes) are similar,
being the properties corresponding to our quantum sys-
tem more general than the classical system.

The next table shows some speciﬁc resemblances be-
tween quantum statistical mechanics and evolutionary
game theory.

Table 2

Quantum Statistical Mechanics

Evolutionary Game Theory

n system members

n population members

Each member in the state |Ψki Each member plays strategy si

|Ψki with pk → ρij

ρ,
i~ dρ

Pi ρii= 1
ˆH, ρi
dt = h
S = −T r {ρ ln ρ}

si → xi
X,
Pi xi= 1
dX
dt = [Λ, X]

H = −

Pi xi ln xi

Both systems are composed by n members (particles,
subsystems, players, states, etc.). Each member is de-
scribed by a state or a strategy which has assigned a de-
termined probability. The quantum mechanical system is
described by the density operator ρ whose elements rep-
resent the system average probability of being in a deter-
mined state. For evolutionary game theory, we deﬁned
a relative frequencies matrix X to describe the system
whose elements can represent the frequency of players
playing a determined strategy. The evolution of the den-
sity operator is described by the von Neumann equation
which is a generalization of the Schr¨odinger equation.
While the evolution of the relative frequencies in a pop-
ulation is described by the Lax form of the replicator
dynamics which is a generalization of the replicator dy-
namics in vectorial form.

In table 3 we show the properties of the matrixes ρ and

X.

Table 3

Density Operator Relative freq. Matrix
ρ is Hermitian
T rρ(t) = 1
ρ2(t) 6 ρ(t)
T rρ2(t) 6 1

X is Hermitian
T rX = 1
X 2= X
T rX 2(t) = 1

4

5. QUANTUM REPLICATOR DYNAMICS &
QUANTIZATION RELATIONSHIPS

Let us propose the next quantization relationships

xi →

hi |Ψk i pk hΨk |i i = ρii,

(xixj)1/2 →

hi |Ψk i pk hΨk |j i = ρij.

(12)

n

X
k=1

n

X
k=1

A population will be represented by a quantum system
in which each subpopulation playing strategy si will be
represented by a pure ensemble in the state |Ψk(t)i and
with probability pk. The probability xi of playing strat-
egy si or the relative frequency of the individuals using
strategy si in that population will be represented as the
probability ρii of ﬁnding each pure ensemble in the state
|ii [27].

Through these quantization relationships the replica-
tor dynamics (in matrix commutative form) takes the
form of the equation of evolution of mixed states (11).
And also

X −→ ρ,

Λ −→ −

i
~

ˆH,

(13)

(14)

where ˆH is the Hamiltonian of the physical system.

The equation of evolution of mixed states from quan-
tum statistical mechanics (11) is the quantum analogue
of the replicator dynamics in matrix commutative form
(9) and both systems and their respective matrixes have
similar properties. Through these relationships we could
describe classical, evolutionary and quantum games and
also the biological systems that were described before
through evolutionary dynamics with the replicator dy-
namics.

6. CLASSICAL & QUANTUM GAMES
ENTROPY

Lets consider a system composed by N members, play-
ers, strategies, states, etc. This system is described com-
pletely through certain density operator ρ (10), its evo-
lution equation (the von Neumann equation) (11) and its
entropy. Classically, the system is described through the
matrix of relative frequencies X, the replicator dynam-
ics and the Shannon entropy. For the quantum case we
deﬁne the von Neumann entropy as [27, 28, 29]

Although both systems are diﬀerent, both are anal-
ogous and thus exactly equivalents. The resemblances
between both systems and the similarity in the proper-
ties of their corresponding elements let us to deﬁne and
propose the next quantization relationships.

and for the classical case

S = −T r {ρ ln ρ} ,

H = −

xii ln xii

X
i=1

(15)

(16)

which is the Shannon entropy over the relative frequen-
cies vector x ( the diagonal elements of X). The time
evolution equation of H assuming that x evolves follow-
ing the replicator dynamics is

dH(t)
dt

= T r nU ( ˜H − X)o .

(17)

n
j=1 aijxj −

˜H is a diagonal matrix whose trace is equal to the Shan-
non entropy and its elements are ˜hii = −xi ln xi. The ma-
n
k,l=1 aklxlxk.
trix U has as elements Ui =
P
P
Entropy is the central concept of information theories.
The von Neumann entropy [30, 31] is the quantum ana-
logue of Shannon’s entropy but it appeared 21 years be-
fore and generalizes Boltzmann’s expression. Entropy
in quantum information theory plays prominent roles in
many contexts, e.g., in studies of the classical capacity
of a quantum channel [32, 33] and the compressibility of
a quantum source [34, 35]. Quantum information theory
appears to be the basis for a proper understanding of the
emerging ﬁelds of quantum computation [36, 37], quan-
tum communication [38, 39], and quantum cryptography
[40, 41].

In classical physics, information processing and com-
munication is best described by Shannon information
theory. The Shannon entropy expresses the average in-
formation we expect to gain on performing a probabilistic
experiment of a random variable which takes the values
si with the respective probabilities xi. It also can be seen
as a measure of uncertainty before we learn the value of
that random variable. The Shannon entropy of the prob-
ability distribution associated with the source gives the
minimal number of bits that are needed in order to store
the information produced by a source, in the sense that
the produced string can later be recovered.

We can deﬁne an entropy over a random variable SA
(player’s A strategic space) which can take the values
sA
(or {|Ψii}A) with the respective probabilities (xi)A
i (cid:9)
(cid:8)
ρij(cid:1)A
(or
). We could interpret the entropy of our game
(cid:0)
as a measure of uncertainty before we learn what strat-
egy player A is going to use. If we do not know what
strategy a player is going to use every strategy becomes
equally probable and our uncertainty becomes maximum
and greater while greater is the number of strategies. If
we would know the relative frequency with player A uses
any strategy we can prepare our reply in function of that
most probable player A strategy. Obviously our uncer-
tainty vanish if we are sure about the strategy our oppo-
nent is going to use.

If player B decides to play strategy sB

j against player A
which plays the strategy sA
i our total uncertainty about
the pair (A, B) can be measured by an external “referee”
through the joint entropy of the system. This is smaller
or at least equal than the sum of the uncertainty about
A and the uncertainty about B. The interaction and the
correlation between A and B reduces the uncertainty due
to the sharing of information. The uncertainty decreases
while more systems interact jointly creating a new only
system. If the same systems interact in separated groups

5

the uncertainty about them is bigger. We can measure
how much information A and B share and have an idea of
how their strategies or states are correlated by its mutual
or correlation entropy. If we know that B decides to play
strategy sB
j we can determinate the uncertainty about A
through the conditional entropy.

Two external observers of the same game can measure
the diﬀerence in their perceptions about certain strategy
space of the same player A by its relative entropy. Each
of them could deﬁne a relative frequency vector and the
relative entropy over these two probability distributions
is a measure of its closeness. We could also suppose that
A could be in two possible states i.e. we know that A can
play of two speciﬁc but diﬀerent ways and each way has
its probability distribution (“state”) that also is known.
Suppose that this situation is repeated exactly N times
or by N people. We can made certain “measure”, ex-
periment or “trick” to determine which the state of the
player is. The probability that these two states can be
confused is given by the classical or the quantum Sanov’s
theorem.

7. MAXIMUM ENTROPY & THE COLLECTIVE
WELFARE PRINCIPLE

We can maximize S by requiring that

δS = −

δρii(ln ρii + 1) = 0

(18)

X
i

subject to the constrains δT r (ρ) = 0 and δ hEi = 0. By
using Lagrange multipliers

δρii(ln ρii + βEi + γ + 1) = 0

(19)

X
i

and the normalization condition T r(ρ) = 1 we ﬁnd that

ρii =

e−βEi
Pk e−βEk

which is the condition that the density operator and its
elements must satisfy to our system tends to maximize
its entropy S.
If we maximize S without the internal
energy constrain δ hEi = 0 we obtain

which is the β → 0 limit (“high - temperature limit”) in
equation (20) in where a canonical ensemble becomes a
completely random ensemble in which all energy eigen-
states are equally populated. In the opposite low - tem-
perature limit β → ∞ tell us that a canonical ensemble
becomes a pure ensemble where only the ground state is
populated. The parameter β is related to the “tempera-
ture” τ as follows

ρii =

1
N

β =

1
τ

.

(20)

(21)

(22)

By replacing ρii obtained in the equation (20) in the
von Neumann entropy we can rewrite it in function of the
Pk e−βEk , β and hEi through the
partition function Z =
next equation

S = ln Z + β hEi .

(23)

It is easy to show that the next relationships for the en-
ergy of our system are satisﬁed

We can also analyze the variation of entropy with respect
to the average energy of the system

hEi = −

= −

1
Z

∂Z
∂β
∂ hEi
∂β

,

∂ ln Z
∂β
1
β

∂S
∂β

.

= −

∆E2
(cid:10)

(cid:11)

= −

∂S
∂ hEi

=

1
τ

,

∂2S
∂ hEi2 = −

1
τ 2

∂τ
∂ hEi

∂S
∂β

= −β

∆E2
(cid:10)

,

∂2S
∂β2 =

∂ hEi
∂β

+ β

(cid:11)
∂2 hEi
∂β2 .

(24)

(25)

(26)

(27)

(28)

(29)

and with respect to the parameter β

If our systems are analogous and thus exactly equiva-
lents, our physical equilibrium should be also absolutely
equivalent to our socieconomical equilibrium.
If in an
isolated system each of its accessible states do not have
the same probability, the system is not in equilibrium.
The system will vary and will evolution in time until it
reaches the equilibrium state in where the probability of
ﬁnding the system in each of the accessible states is the
same. The system will ﬁnd its more probable conﬁgura-
tion in which the number of accessible states is maximum
and equally probable. The whole system will vary and
rearrange its state and the states of its ensembles with
the purpose of maximize its entropy and reach its maxi-
mum entropy state. We could say that the purpose and
maximum payoﬀ of a quantum system is its maximum en-
tropy state. The system and its members will vary and
rearrange themselves to reach the best possible state for
each of them which is also the best possible state for the
whole system. This can be seen like a microscopical co-
operation between quantum objects to improve its state
with the purpose of reaching or maintaining the equilib-
rium of the system. All the members of our quantum
system will play a game in which its maximum payoﬀ is
the equilibrium of the system. The members of the sys-
tem act as a whole besides individuals like they obey a
rule in where they prefer the welfare of the collective over
the welfare of the individual. This equilibrium is repre-
sented in the maximum system entropy in where the sys-
tem “resources” are fairly distributed over its members.

6

“A system is stable only if it maximizes the welfare of
the collective above the welfare of the individual. If it is
maximized the welfare of the individual above the welfare
of the collective the system gets unstable and eventually
it collapses” (Collective Welfare Principle [27, 29]).

There exists tacit rules inside a system. These rules
do not need to be speciﬁed nor clariﬁed and search the
system equilibrium under the collective welfare princi-
ple. The other “prohibitive” and “repressive” rules are
imposed over the system when one or many of its mem-
bers violate the collective welfare principle and search
to maximize its individual welfare at the expense of the
group. Then it is necessary to establish regulations over
the system to try to reestablish the broken natural order.
Fundamentally, we could distinguish three states in ev-
ery system: minimum entropy state, maximum entropy
state, and when the system is tending to whatever of
these two states. The natural trend of a physical system
is to the maximum entropy state. The minimum entropy
state is a characteristic of a “manipulated” system i.e.
externally controlled or imposed.

8. THE WHY OF THE APPLICABILITY

Quantum mechanics could be a much more general the-
ory that we had though. It could encloses theories like
EGT and evolutionary dynamics and we could explain
through this theory biological and economical processes.
From this point of view many of the equations, concepts
and its properties deﬁned quantically must be more gen-
eral that its classical versions but they must remain in-
side the foundations of the new quantum version. So, our
quantum equilibrium concept also must be more general
than the one deﬁned classically.

In our model we represent a population by a quantum
system in which each subpopulation playing strategy si
is represented by a pure ensemble in the state |Ψk(t)i and
with probability pk. The probability xi of playing strat-
egy si or the relative frequency of the individuals using
strategy si in that population is represented by the prob-
ability ρii of ﬁnding each pure ensemble in the state |ii.
Through these quantization relationships the replicator
dynamics (in matrix commutative form) takes the form
of the equation of evolution of mixed states. The quan-
tum analogue of the relative frequencies matrix is the
density operator. The relationships between these two
systems described by these two matrixes and their evo-
lution equations would let us analyze the entropy of our
system through the well known von Neumann entropy
in the quantum case and by the Shannon entropy in the
classical case. The properties that these entropies enjoy
would let us analyze a “game” from a diﬀerent point of
view through information and a maximum or minimum
entropy criterion.

Every game can be described by a density operator,
the von Neumann entropy and the quantum replicator
dynamics. The density operator is maybe the most im-

portant tool in quantum mechanics. From the density
operator we can construct and obtain all the statisti-
cal information about our system. Also we can develop
the system in function of its information and analyze it
through information theories under a criterion of max-
imum or minimum entropy. There exists a strong re-
lationship between game theories, statistical mechanics
and information theory. The bonds between these theo-
ries are the density operator and entropy.

It is important to remember that we are dealing with
very general and unspeciﬁc terms, deﬁnitions, and con-
cepts like state, game and system. Due to this, the the-
ories that have been developed around these terms like
quantum mechanics, statistical physics, information the-
ory and game theory enjoy of this generality quality and
could be applicable to model any system depending on
what we want to mean for game, state, or system. Objec-
tively these words can be and represent anything. Once
we have deﬁned what system is in our model, we could try
to understand what kind of “game” is developing between
its members and how they accommodate its “states” in
order to get its objectives. This would let us visualize
what temperature, energy and entropy would represent
in our speciﬁc system through the relationships, proper-
ties and laws that were deﬁned before when we described
a physical system.

Entropy can be deﬁned over any random variable and
can be maximized subject to diﬀerent constrains. In each
case the result is the condition the system must follow to
maximize its entropy. Generally, this condition is a prob-
ability distribution function. For the case analyzed in
this paper, this distribution function depends on certain
parameter “β” which is related inversely with the system
“temperature”. Depending on what the variable over
which we want determinate its grade of order or disorder
is we can resolve if the best for the system is its state of
maximum or minimum entropy. If we would measure the
order or disorder of our system over a resources distri-
bution variable the best state for that system is those in
where its resources are fairly distributed over its mem-
bers which would represent a state of maximum entropy.
By the other hand, if we deﬁne an entropy over a accep-
tation relative frequency of a presidential candidate in a
democratic process the best would represent a minimum
entropy state i.e. the acceptation of a candidate by the
vast majority of the population.

9. CONCLUSIONS

There is a strong relationship between quantum me-
chanics and game theory. The relationships between
these two systems described by the density operator and

7

the relative frequencies matrix and their evolution equa-
tions would let us analyze the entropy of our system
through the well known von Neumann entropy in the
quantum case and by the Shannon entropy in the classi-
cal case. The quantum version of the replicator dynamics
is the equation of evolution of mixed states from quantum
statistical mechanics. Every “game” could be described
by a density operator with its entropy equal to von Neu-
mann’s and its evolution equation given by the quantum
replicator dynamics.

The density operator and entropy are the bonds be-
tween game theories, statistical mechanics and informa-
tion theory. The density operator is maybe the most
important tool in quantum mechanics. From the den-
sity operator we can construct and obtain all the statis-
tical information about our system. Also we can develop
the system in function of its information and analyze it
through information theories under a criterion of maxi-
mum or minimum entropy.

Quantum mechanics could be a much more general the-
ory that we had though. It could encloses theories like
EGT and evolutionary dynamics and we could explain
through this theory biological and economical processes.
Due to the generality of the terms state, game and sys-
tem, quantum mechanics, statistical physics, information
theory and game theory enjoy also of this generality qual-
ity and could be applicable to model any system depend-
ing on what we want to mean for game, state, or system.
Once we have deﬁned what the term system is in our
model, we could try to understand what kind of “game”
is developing between its members and how they accom-
modate its “states” in order to get its objectives. En-
tropy can be deﬁned over any random variable and can
be maximized subject to diﬀerent constrains. Depending
on what the variable over which we want determinate its
grade of order or disorder is we can resolve if the best for
the system is its state of maximum or minimum entropy.
A system can be internally or externally controlled with
the purpose of guide it to a state of maximum or mini-
mum entropy depending of the ambitions of the members
that compose it or the “people” who control it.

The results shown in this study on the relationships be-
tween quantum mechanics and game theories are a reason
of the applicability of physics in economics in the ﬁeld of
econophysics. Both systems described through two ap-
parently diﬀerent theories are analogous and thus exactly
equivalents. So, we can take some concepts and deﬁni-
tions from quantum mechanics and physics for the best
understanding of the behavior of economics. Also, we
could maybe understand nature like a game in where its
players compete for a common welfare and the equilib-
rium of the system that they are members.

[1] M. Ausloos, N. Vandewalle, Ph. Boveroux, A.Minguet,

[2] J.-P. Bouchaud, An introduction to statistical ﬁnance,

K. Ivanova, Physica A 247, 229-240 (1999).

8

Physica A 313, 238-251 (2002).

[3] H. Eugene Stanley, Exotic statistical physics: Applica-
tions to biology, medicine, and economics, Physica A
285, 1-17 (2000).

[4] L.Ya. Kobelev, O.L. Kobeleva, Ya.L. Kobelev, Is it Pos-
sible to Describe Economical Phenomena by Methods of
Statistical Physics of Open Systems?, physics/0005010.
[5] Masanao Aoki, New approaches to Macroeconomics Mod-
eling (Cambridge University Press, Cambridge, 1996).
[6] I. Antoniou, V.V. Ivanov, Yu.L. Korolev, A.V. Kryanev,
V.V. Matokhin, Z. Suchanecki, Physica A 304, 525-534
(2002).

[7] A. Dr˘agulescu and V. M. Yakovenko, Statistical mechan-

[19] R. Cressman, The Stability Concept of Evolutionary
Game Theory: A Dynamic Approach (Springer-Verlag,
New York, 1992).

[20] J. M. Smith and G. R. Price, The logic of animal conﬂict,

Nature 246, 15 (1973).

[21] D. A. Meyer, Phys. Rev. Lett. 82, 1052-1055 (1999).
[22] J. Eisert, M. Wilkens and M. Lewenstein, Phys. Rev.

[23] L. Marinatto and T. Weber, Phys. Lett. A 272, 291

[24] A. P. Flitney and D. Abbott, Proc. R. Soc. (London) A

Lett. 83, 3077 (1999).

(2000).

459, 2463-74 (2003).

[25] E. W. Piotrowski and J. Sladkowski, Int. J. Theor. Phys.

ics of money, Eur. Phys. J. B 17, 723-729 (2000).

42, 1089 (2003).

[8] A. Darooneh, Entropy 8[1], 18-24 (2006).
[9] F. TopsØe, Information Theoretical Optimization Tech-
niques, Kybernetika 15, 8-27 (1979). F. TopsØe, Game
theoretical equilibrium, maximum entropy and minimum
in Maximum Entropy and
information discrimination,
Bayesian Methods (A. Mohammad-Djafari and G. De-
moments (eds.), pp. 15-23, Kluwer, Dordrecht, 1993).
[10] J. von Neumann and O. Morgenstern, The Theory of
Games and Economic Behavior ( Princeton University
Press, Princeton, 1947).

[11] R. B. Myerson, Game Theory: An Analysis of Conﬂict

[26] Azhar Iqbal, PhD thesis, Quaid-i-Azam University, 2004,

quant-ph/0503176.

[27] E. Guevara H., Quantum Replicator Dynamics, PHYS-

ICA A 369/2, 393-407 (2006).
[28] E. Guevara H., quant-ph/0604170.
[29] E.

H.,

Guevara
quant-ph/0606045.

Quantum Games

Entropy,

[30] J. von Neumann, Thermodynamik quantummechanischer

Gesamheiten, G¨ott. Nach. 1 273-291(1927).

[31] J. von Neumann, Mathematische Grundlagen der Quan-

tenmechanik (Springer, Berlin, 1932).

(MIT Press, Cambridge, 1991).

[32] B. Schumacher and M. D. Westmoreland, Phys. Rev. A

[12] M. A. Nowak and K. Sigmund, Nature 398, 367 (1999).
[13] J. M. Smith, Evolution and The Theory of Games (Cam-

bridge University Press, Cambridge, UK, 1982).

[14] J. Hofbauer and K. Sigmund, Evolutionary Games and
Replicator Dynamics (Cambridge University Press, Cam-
bridge, UK, 1998).

[15] J. Weibul, Evolutionary Game Theory (MIT Press, Cam-

bridge, MA, 1995).

[17] P. Hammerstein and R. Selten, Game Theory and Evo-
lutionary Biology (Handbook of Game Theory. Vol 2. El-
sevier B.V., 1994).

[18] P. D. Taylor and L. B. Jonker , Evolutionary stable strate-
gies and game dynamics, Mathematical Biosciences 40,
145-156 (1978).

[16] R. A. Fisher, The Genetic Theory of Natural Selection

(Oxford, Clarendon Press, 1930).

2881 (1992).

56, 131 (1997).

[33] A. S. Holevo, IEEE Trans. Inf. Theory 44, 269 (1998).
[34] B. Schumacher, Phys. Rev. A 51, 2738 (1995).
[35] R. Jozsa and B. Schumacher, J. Mod. Opt. 41, 2343

[36] C. H. Bennett and D. P. DiVincenzo, Nature 377, 389

(1994).

(1995).

[37] D. P. DiVincenzo, Science 270, 255 (1995).
[38] C. H. Bennett and S. J. Wiesner, Phys. Rev. Lett. 69,

[39] C. H. Bennett et al., Phys. Rev. Lett. 70, 1895 (1993).
[40] A. Ekert, Nature 358, 14 (1992).
[41] C. H. Bennett, G. Brassard, and N. D. Mermin, Phys.

Rev. Lett. 68, 557 (1992).


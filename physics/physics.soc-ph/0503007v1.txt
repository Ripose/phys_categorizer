5
0
0
2
 
r
a

M
 
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
7
0
0
3
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Random Matrix Theory and
Robust Covariance Matrix Estimation
for Financial Data

Gabriel Frahm∗ & Uwe Jaekel†

C&C Research Laboratories, NEC Europe Ltd.
Rathausallee 10, 53757 Sankt Augustin, Germany

February 2, 2008

Abstract

The traditional class of elliptical distributions is extended to allow for asym-
metries. A completely robust dispersion matrix estimator (the ‘spectral estima-
It
tor’) for the new class of ‘generalized elliptical distributions’ is presented.
is shown that the spectral estimator corresponds to an M-estimator proposed by
Tyler (1983) in the context of elliptical distributions. Both the generalization of
elliptical distributions and the development of a robust dispersion matrix esti-
mator are motivated by the stylized facts of empirical ﬁnance. Random matrix
theory is used for analyzing the linear dependence structure of high-dimensional
data. It is shown that the Marˇcenko-Pastur law fails if the sample covariance ma-
trix is considered as a random matrix in the context of elliptically distributed and
heavy tailed data. But substituting the sample covariance matrix by the spectral
estimator resolves the problem and the Marˇcenko-Pastur law remains valid.

1 Motivation

Short-term ﬁnancial data usually exhibit similar properties called ‘stylized facts’ like,
e.g., leptokurtosis, dependence of simultaneous extremes, radial asymmetry, volatility
clustering, etc., especially if the log-price changes (called the ‘log-returns’) of stocks,
stock indices, and foreign exchange rates are considered. Particularly, high-frequency
data usually are non-stationary, have jumps, and are strongly dependent. Cf., e.g.,

∗Email: frahm@ccrl-nece.de.
†Email: jaekel@ccrl-nece.de.

1

Bouchaud, Cont, and Potters, 1998, Breymann, Dias, and Embrechts, 2003, Eberlein
and Keller, 1995, Embrechts, Frey, and McNeil, 2004 (Section 4.1.1), Engle, 1982,
Fama, 1965, Junker and May, 2002, Mandelbrot, 1963, and Mikosch, 2003 (Chapter
1).

Figure 1 contains QQ-plots of GARCH(1, 1) residuals of daily log-returns of the NAS-
DAQ and the S&P 500 indices from 1993-01-01 to 2000-06-30. It is clearly indicated
that the normal distribution hypothesis is not appropriate for the loss parts of the dis-
tributions whereas the Gaussian law seems to be acceptable for the proﬁt parts. Hence
the probability of extreme losses is higher than suggested by the normal distribution
assumption.

−4

−2
0
2
theoretical quantile

4

6

−4

−2
0
2
theoretical quantile

4

6

Fig. 1: QQ-plots of NASDAQ (left hand) and S&P 500 (right hand) GARCH(1, 1)
residuals from 1993-01-01 to 2000-06-30 (n = 1892).

The next picture shows the joint distribution of the GARCH residuals considered
above.

e

l
i
t
n
a
u
q
 
l
a
c
i
r
i
p
m
e

6

4

2

0

−2

−4

−6
−6

0
0
5
 
P
&
S

8

6

4

2

0

−2

−4

−6

−8
−8

e

l
i
t
n
a
u
q
 
l
a
c
i
r
i
p
m
e

6

4

2

0

−2

−4

−6
−6

2

−6

−4

−2

4

6

8

0
NASDAQ

2

Fig. 2: NASDAQ vs. S&P 500 GARCH(1, 1) residuals from 1993-01-01 to 2000-06-
30 (n = 1892).

Except for one element all extremes occur simultaneously. The effect of simultaneous
extremes can be observed more precisely in the following picture. It shows the total
numbers of S&P 500 stocks whose absolute values of daily log-returns exceeded 10%
for each trading day during 1980-01-02 to 2003-11-26. On the 19th October 1987 (i.e.
the ‘Black Monday’) there occurred 239 extremes. This is suppressed for the sake of
transparency.

s
e
m
e
r
t
x
e

 
f

o
 
r
e
b
m
u
n

120

100

80

60

40

20

0
0

1000

2000

4000

5000

6000

3000
time points

Fig. 3: Number of extremes in the S&P 500 during 1980-01-02 to 2003-11-26.

The latter ﬁgure shows the concomitance of extremes. If extremes would occur inde-
pendently then the number of extremal events (no matter if losses or proﬁts) should be
small and all but constant over time. Obviously, this is not the case. In contrast one can
see the October Crash of 1987 and several extremes which occur permanently since
the beginning of the bear market in 2000. Hence there is an increasing tendency of
simultaneous losses which is probably due to globalization effects and relaxed market
regulation. The phenomenon of simultaneous extremes is often denoted by ‘asymp-
totic dependence’ or ‘tail dependence’.

The traditional class of elliptically symmetric distributions (Cambanis, Huang, and
Simons, 1981, Fang, Kotz, and Ng, 1990, and Kelker, 1970) is often proposed for
the modeling of ﬁnancial data (cf., e.g., Bingham and Kiesel, 2002). But elliptical
distributions suffer from the property of radial symmetry. The pictures above show
that ﬁnancial data are not always symmetrically distributed. For this reason the authors
will bear on the assumption of generalized elliptically distributed (Frahm, 2004) log-
returns. This allows for the modeling of tail dependence and radial asymmetry.

The quintessence of modern portfolio theory is that the portfolio diversiﬁcation effect
depends essentially on the covariances. But the parameters for portfolio optimization,
i.e. the mean vector and the covariance matrix, have to be estimated. Especially for
portfolio risk minimization a reliable estimate of the covariance matrix is necessary
(Chopra and Ziemba, 1993). For covariance matrix estimation generally one should
use as much available data as possible. But since daily log-returns and all the more

3

high-frequency data are not normally distributed, standard estimators like the sample
covariance matrix may be highly inefﬁcient leading to erroneous implications (see,
e.g., Oja, 2003 and Visuri, 2001). This is because the sample covariance matrix is
very sensitive to outliers. The smaller the distribution’s tail index (Hult and Lindskog,
2002), i.e. the heavier the tails of the log-return distributions the higher the estimator’s
variance. So the quality of the parameter estimates depends essentially on the true
multivariate distribution of log-returns.

In the following it is shown how the linear dependence structure of generalized ellipti-
cal random vectors can be estimated robustly. More precisely, it is shown that Tyler’s
(1987) robust M-estimator for the dispersion matrix Σ of elliptically distributed ran-
dom vectors remains completely robust for generalized elliptically distributed random
vectors. This estimator is not disturbed neither by asymmetries nor by outliers and
all the available data points can be used for estimation purposes. Further, the impact
of high-dimensional (ﬁnancial) data on statistical inference will be discussed. This is
done by referring to a branch of statistical physics called ‘Random Matrix Theory’
(Hiai and Petz, 2000 and Mehta, 1990). Random matrix theory (RMT) is concerned
with the distribution of eigenvalues of high-dimensional randomly generated matrices.
If each component of a sample is independent and identically distributed then the dis-
tribution of the eigenvalues of the sample covariance matrix converges to a speciﬁed
law which does not depend on the speciﬁc distribution of the sample components. The
circumstances under which this result of RMT can be properly adopted to generalized
elliptically distributed data will be examined.

2 Generalized Elliptical Distributions

It is well known that an elliptically distributed random vector X can be represented
Rd×k with r(Λ) = k, U (k) is a
ΛU (k), where µ
stochastically by X =d µ +
k−1, and
k-dimensional random vector uniformly distributed on the unit hypersphere
is a nonnegative random variable stochastically independent of U (k). The positive
R
semi-deﬁnite matrix Σ := ΛΛT characterizes the linear dependence structure of X and
is referred to as the ‘dispersion matrix’.

Rd, Λ

R

∈

∈

S

Deﬁnition 1 (Generalized elliptical distribution) The d-dimensional random vector
X is said to be ‘generalized elliptically distributed’ if and only if

where U (k) is a k-dimensional random vector uniformly distributed on
random variable, µ

Rd, and Λ

Rd×k.

S

k−1,

is a

R

∈

∈

X d= µ +

ΛU (k).

R

4

Note that the deﬁnition of generalized elliptical distributions preserves all the ordinary
). But in contrast
components of elliptically symmetric distributions (i.e. µ, Σ, and
may be negative and even more it may depend on U (k). It is
the generating variate
worth to point out that the class of generalized elliptical distributions contains the class
of skew-elliptical distributions (Branco and Dey, 2001, and Frahm, 2004, Section 3.2).

R

R

ν

χ2

2/χ2
(δ(Λu/

The next ﬁgure shows once again the joint distribution of the GARCH residuals of
the NASDAQ and S&P 500 log-returns from 1993-01-01 to 2000-06-30 from Figure
2. The right hand of Figure 4 contains simulated GARCH residuals on the basis of
a generalized t-distribution. More precisely, the generating variate
corresponds
ν but the number of degrees of freedom ν depends on U (2), i.e. ν =
to
·
k2 = 1). Here δ is a function that measures the
Λu
4 + 996
p
sin (π/4)),
k2 and the reference vector v = (
distance between Λu/
−
δ(u, v) := ∠(u, v)/π = arccos(uTv)/π. Hence, random vectors which are close to
the reference vector (i.e. close to the ‘perfect loss scenario’) are supposed to be t-
distributed with ν = 4 degrees of freedom whereas random vectors which are opposite
are assumed to be nearly Gaussian (ν = 1000) distributed. This is consistent with the
phenomenon observed in Figure 1. The pseudo-correlation coefﬁcient is set to 0.78.

k2, v))3 (
Λu
k

cos (π/4) ,

R

−

u

k

k

·

)
d
e
v
r
e
s
b
o
(
 

 

0
0
5
P
&
S

8

6

4

2

0

−2

−4

−6

−8

−8

−6

−4

4

6

8

−6

−4

4

6

8

−2
0
NASDAQ (observed)

2

−2

0
NASDAQ (simulated)

2

Fig. 4: Observed GARCH(1, 1) residuals of NASDAQ and S&P 500 (left hand) and
simulated generalized t-distributed random noise (n = 1892) (right hand).

3 Robust Covariance Matrix Estimation

It is well-known that the sample covariance matrix corresponds both to the moment
estimator and to the ML-estimator for the dispersion matrix Σ of normally distributed
data. But given any other elliptical distribution family the dispersion matrix usually
does not correspond to the covariance matrix. Generally, robust covariance matrix
estimation means to estimate the dispersion matrix, that is the covariance matrix up
to a scaling constant. There are many applications like, e.g., principal components

i

l

t

)
d
e
a
u
m
s
(
 
0
0
5
P
&
S

 

8

6

4

2

0

−2

−4

−6

−8

−8

5

analysis, canonical correlation analysis, linear discriminant analysis, and multivariate
regression where only the dispersion matrix is demanded (Oja, 2003). Particularly,
by Tobin’s two-fund separation theorem (Tobin, 1958) the optimal portfolio of risky
assets does not depend on the scale of the covariance matrix. Thus in the following
we will loosely speak of ‘covariance matrix estimation’ rather than of estimating the
dispersion matrix for the sake of simplicity.

As mentioned before the true linear dependence structure of elliptically distributed data
can not be estimated efﬁciently by the sample covariance matrix, generally. Especially,
if the data stem from a regularly varying random vector the smaller the tail index, i.e.
the heavier the tails the larger the estimator’s variance. But in the following it is shown
that there exists a completely robust alternative to the sample covariance matrix.

Let X be a d-dimensional generalized elliptically distributed random vector where µ
is supposed to be known, Λ
= 0) = 0. Further, let
the unit random vector generated by Λ be deﬁned as

Rd×k with r(Λ) = d, and P (

R

∈

S :=

ΛU (k)
ΛU (k)

.

|
(cid:12)
(cid:12)

|2
(cid:12)
(cid:12)

Due to the stochastic representation of X the following relations hold,

ΛU (k)
ΛU (k)

a.s.=

±

ΛU (k)
ΛU (k)

=

S,

±

µ
µ

−
−

X
X
|
(cid:12)
(cid:12)
R

d= R
|R
(cid:12)
(cid:12)

|2
(cid:12)
). The random vector
(cid:12)
±

|2
(cid:12)
(cid:12)

|
(cid:12)
(cid:12)

|2
(cid:12)
(cid:12)

±

R

:= sgn(

S does not depend on the absolute value
where
. So it is completely robust against extreme outcomes of the generating variate.
of
still remains and this may depend on U (k), anymore. Suppose for
But the sign of
the moment that
. Then the dispersion matrix
of X can be estimated robustly via maximum-likelihood estimation using the density
function of S which is only a function of Λ. This is given by the next theorem.

is known for each realization of

R

R

±

Theorem 1 The spectral density function of the unit random vector generated by Λ
Rd×k corresponds to

∈

s

7−→

ψ (s) =

d
Γ
2
2πd/2 ·
(cid:0)

(cid:1)

p

where Σ := ΛΛT.

det(Σ−1)

√sTΣ−1s

−d

,

·

s

∀

∈ S

d−1,

Proof. See, e.g., Frahm, 2004, pp. 59-60.

Since ψ is a symmetric density function the sign of
does not matter at all. Hence
the ML-estimation approach works even if the data are skew-elliptically distributed,
for instance.

R

6

The desired ‘spectral estimator’ is given by the ﬁxed-point equation (Frahm, 2004,
Section 4.2.2)

ΣS =

d
n ·

n

j=1
X

sjsT
j
Σ−1
S sj

,

sT
j

where sj := (xj −
for j = 1, ..., n. Since the solution of the ﬁxed-
point equation is only unique up to a scaling constant in the following it is implicitly
required that the upper left element of

ΣS corresponds to 1.

|2
(cid:12)
(cid:12)

µ) /

b

(cid:1)

b
µ
xj −
|
(cid:0)(cid:12)
(cid:12)

The spectral estimator
Tyler, 1987) for elliptical distributions, i.e.

b

ΣS corresponds to Tyler’s robust M-estimator (Tyler, 1983 and

ΣS =

b

b

d
n ·

n

j=1
X

(xj −
µ)T

µ)T
µ) (xj −
Σ−1
S (xj −

(xj −

.

µ)

Hence Tyler’s M-estimator remains completely robust within the class of generalized
elliptical distributions.

b

The following ﬁgure shows the sample covariance matrix (left hand) of a sample
with n = 1000 observations and d = 500 dimensions drawn from a multivariate t-
distribution with ν = 4 degrees of freedom. Note that the tail index of the multivariate
t-distribution corresponds to ν. Each cell of the plots represents a matrix element
where the blue colored cells symbolize small numbers and the red colored cells in-
dicate large numbers. The true dispersion matrix is given in the middle whereas the
spectral estimate is given by the right hand.

Fig. 5: Sample covariance matrix (left hand), true covariance matrix (middle), and
spectral estimate (right hand) of multivariate t-distributed realizations (n = 1000, d =
500, ν = 4).

7

4 Random Matrix Theory

RMT is concerned with the distribution of the eigenvalues of high-dimensional ran-
domly generated matrices. A random matrix is simply a matrix of random variables.
We will consider only symmetric random matrices. Thus the corresponding eigenval-
ues are always real. The empirical distribution function of eigenvalues is deﬁned as
follows.

Deﬁnition 2 (Empirical distribution function of eigenvalues) Let
metric random matrix with eigenvalues

λd . Then the function

λ2, . . . ,

λ1,

Σ be a d

×

d sym-

λ

7−→

b
Wd (λ) :=

b

1
d ·

d

b

11

λi≤ λ

i=1
X

b

c

b

is called the ‘empirical distribution function of the eigenvalues’ of

Σ.

Note that each eigenvalue of a random matrix in fact is random but per se not a random
) but rather
variable since there is no single-valued mapping
∈ {
}
Σ. This can be simply
Σ
b
λd are sorted either in an increasing
ﬁxed by assuming that the eigenvalues
b
b
or decreasing order.

λi (i
Σ) denotes the set of all eigenvalues of
λ1,

Σ) where λ(

λ2, . . . ,

7→

7→

λ(

Σ

b

b

b

b
1, . . . , d

b
Theorem 2 (Marˇcenko and Pastur, 1967) Let U (d)
(n = 1, 2, . . .) be
sequences of independent random vectors uniformly distributed on the unit hyper-
sphere

d−1 and consider the random matrix

, . . . , U (d)
n

, U (d)
2

b

b

1

S

ΣMP :=

j U (d)T
U (d)

j

,

d
n ·

n

j=1
X

b

Wd

p
−→

FMP (

; q) ,

·

where its empirical distribution function of the eigenvalues is denoted by
that n

. Then

, n/d

q <

, d

Wd . Suppose

→ ∞

→ ∞

→

∞

c

at all points where FMP is continuous. More precisely, λ
F Leb

MP (λ ; q) where the Dirac part is given by

c

7→

FMP (λ ; q) = F Dir

MP (λ ; q)+

λ

7−→

F Dir

MP (λ ; q) =

q,

1

0,

−

(

0, 0

q < 1,

≤

λ

≥
else,

8

and the Lebesgue part λ
density function

7→

F Leb

MP (λ ; q) =

λ
−∞ f Leb

MP (x ; q) dx is determined by the

λ

7−→

f Leb
MP (λ ; q) =

q
2π ·
0,

(

λmin < λ < λmax,
else,

R
√(λmax−λ)(λ−λmin)
λ

,

where

λmin,max :=

2

.

1
√q

(cid:19)

1
(cid:18)

±

Proof. Marˇcenko and Pastur, 1967.

In the following
ΣMP will be called ‘Marˇcenko-Pastur operator’. The next corollary
states that the Marˇcenko-Pastur law FMP holds not only for the empirical distribution
function of eigenvalues of the Marˇcenko-Pastur operator but also for that obtained by
the sample covariance matrix if the data are standard normally distributed and inde-
pendent.

b

Corollary 3 Let X, X1, X2, . . . , Xn (n = 1, 2, . . .) be sequences of independent and
standard normally distributed random vectors with uncorrelated components. Then
the empirical distribution function of the eigenvalues of

1
n ·

n

j=1
X

XjX T
j

converges in probability to the Marˇcenko-Pastur law stated in Theorem 2.

Proof. Due to the strong law of large numbers χ2

d/d a.s.
→

1 (d

→ ∞

) and thus

d
n ·

n

j=1
X

χ2
d,j
d ·

ΣMP ∼
b

j U (d)T
U (d)

j

d=

XjX T
j .

1
n ·

n

j=1
X

Moreover, the Marˇcenko-Pastur law holds even if X is an arbitrary random vector
with standardized i.i.d. components provided the second moment is ﬁnite (Yin, 1986).
More precisely, consider the random vector X with E(X) = µ and V ar(X) = σ2Id
where the components of X are supposed to be stochastically independent. Then the
Marˇcenko-Pastur law can be applied on the empirical distribution function of the eigen-
values of

n

1
n ·

j=1 (cid:18)
X

Xj −
σ

µ

b

b

Xj −
σ

T

µ

b

(cid:19)

(cid:19) (cid:18)

b

9

=

Σ/

σ2,

b

b

where

Σ denotes the sample covariance matrix and

b

tr(
Σ)
d
b

σ2 :=

b

d

=

1
d ·

i=1
X

b

λi =: λ.

b

λ1/λ, ...,

Hence, the Marˇcenko-Pastur law can be applied virtually ever on the empirical dis-
λd/λ where the estimated eigenvalues are given by the
tribution function of
sample covariance matrix provided the sample elements, i.e. the realized random vec-
tors consist of stochastically independent components. But within the class of elliptical
distributions this holds only for uncorrelated normally distributed data. Hence linear
independence and stochastical independence are not equivalent for generalized ellip-
tically distributed data. This is because even if there is no linear dependence between
the components of an elliptically distributed random vector another sort of nonlinear
dependence caused by the generating variate
remains, generally.
For instance, consider the unit random vector U (2) = (U1, U2). Then

R

b

U2

a.s.=

U 2
1 ,

1

−

±

q
i.e. U2 depends strongly on U1 though indeed the elements of U (2) are uncorrelated.
Tail dependent random variables cannot be stochastically independent. Especially, if
the random components of an elliptically distributed random vector are heavy tailed,
i.e. if the generating variate is regularly varying then they possess the property of tail
dependence (Schmidt, 2002). In that case the eigenspectrum generated by the sample
covariance matrix may lead to erroneous implications.

For instance, consider a sample (with sample size n = 1000) of 500-dimensional ran-
dom vectors where each vector element is standardized t-distributed with ν = 5 de-
grees of freedom and stochastically independent of each other. Here the eigenspectrum
obtained by the sample covariance matrix indeed is consistent with the Marˇcenko-
Pastur law (upper left part of Figure 6). But if the data stem from a multivariate
t-distribution possessing the same parameters and each vector component is uncorre-
lated then the eigenspectrum obtained by the sample covariance matrix does not corre-
spond to the Marˇcenko-Pastur law (upper right part of Figure 6). Actually, there are 24
eigenvalues exceeding the Marˇcenko-Pastur upper bound λmax = (1 + 1/√2 )2 = 2.91
and the largest eigenvalue corresponds to 10.33. But fortunately the eigenspectra ob-
tained by the spectral estimator are consistent with the Marˇcenko-Pastur law as indi-
cated by the lower part of Figure 6.

10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
t
i
s
n
e
d

y
t
i
s
n
e
d

y
t
i
s
n
e
d

y
t
i
s
n
e
d

1

0.8

0.6

0.4

0.2

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
−1

0

2
1
eigenvalue

3

4

0

2

4

6

8

10

12

eigenvalue

0
−1

0

3

4

0
−1

0

2
1
eigenvalue

2
1
eigenvalue

3

4

Fig. 6: Eigenspectra of univariate (left part) and multivariate (right part) uncorrelated
t-distributed data (n = 1000, d = 500, ν = 5) obtained by the sample covariance
matrix (upper part) and by the spectral estimator (lower part).

Tyler (1987) shows that the spectral estimator converges strongly to the true dispersion
matrix Σ. That means

sjsT
j
Σ−1sj −→

sjsT
j
sT
j Σ−1sj

,

sT
j

n

−→ ∞

, d const.,

for j = 1, 2, . . . and P -almost all realizations. Consequently, if Σ = Id (up to a scaling
constant) then

b

sjsT
j
Σ−1sj −→

sT
j

sjsT

j ≡

j u(d)T
u(d)

j

,

b

→ ∞

as n
and d constant. Hence the spectral estimator and the Marˇcenko-Pastur
operator are asymptotically equivalent provided Σ = σ2Id. The authors believe that
the strong convergence holds even for n
q > 1 for P -almost
all realizations where the spectral estimate exists. The proof of this conjecture is due
1 the spectral estimate does not exist at all.
to a forthcoming work. Note that for q
≤
Further, Tyler (1987) shows that the spectral estimate exists (a.s.) if n > d (d
1),
1. Indeed, this is a sufﬁcient condition for the existency of the spectral
i.e. q > d

→ ∞

→ ∞

, n/d

, d

→

−

−

11

estimator. But in practice the spectral estimator seems to exist in most cases when n is
already slightly larger than d.
We conclude that testing high-dimensional data for the null hypothesis Σ = σ2Id by
means of the sample covariance matrix may lead to wrong conclusions provided the
data are generalized elliptically distributed. In contrast, the spectral estimator seems
to be a robust alternative for applying the results of RMT in the context of generalized
elliptical distributions.

5 Financial Applications

5.1 Portfolio Risk Minimization

In this section it is supposed that n/d
, i.e. from the viewpoint of RMT we study
low-dimensional problems. Let R = (R1, R2, ..., Rd) be an elliptically distributed
random vector of short-term (e.g. daily) log-returns. If the fourth order cross moments
of the log-returns are ﬁnite then the elements of the sample covariance matrix are
multivariate normally distributed, asymptotically. The asymptotic covariance of each
element is given by (see, e.g., Praag and Wesselman, 1989)

→ ∞

ACov (ˆσij, ˆσkl) = (1 + κ)

(σikσjl + σilσjk) + κ

σijσkl,

·

·

where Σ = [σij] denotes the true covariance matrix of R and

κ :=

1
3 ·

E (R4
i )
E2(R2
i ) −

1

is called the ‘kurtosis parameter’. Note that the kurtosis parameter does not depend
. It is well-known that in the case of normality κ = 0. A distribution
on i
with positive (or even inﬁnite) κ is called ‘leptokurtic’. Particularly, regularly varying
distributions are leptokurtic.

1, ..., d

∈ {

}

It is well-known that the portfolio which minimizes the portfolio return variance (the so
called ‘global minimum variance portfolio’) is given by the vector of portfolio weights

Now, suppose for the sake of simplicity that R is spherically distributed, i.e. that µ = 0
and Σ is proportional to the identity matrix. Since the weights of the global minimum
variance portfolio do not depend on the scale of Σ we may assume Σ = Id w.l.o.g.
Then the asymptotic covariances of the sample covariance matrix elements are simply

w :=

Σ−11
1TΣ−11

.

12

given by

ACov (ˆσij, ˆσkl) =

2 + 3κ,

i = j = k = l,

κ,

i = j, k = l, i

= k,

1 + κ,

0,

i = k, j = l, i
else.

= j,

For instance suppose that the random vector R is multivariate t-distributed with ν > 4
4) (see,
degrees of freedom. Then the kurtosis parameter corresponds to κ = 2/(ν
e.g., Frahm, 2004, p. 91). Hence, the smaller ν the larger the asymptotic variances and
covariances and these quantities tend to inﬁnity for ν
4 the sample
covariance matrix even is no longer multivariate normally distributed, asymptotically.

4. Further, if ν

ց

−

≤

In contrast, the asymptotic covariance of each element of the spectral estimator (Frahm,
2004, p. 76) is given by

ACov (ˆσS,ij, ˆσS,kl) =

4

2

·

·

d+2
d ,
d+2
d ,
d+2
d ,
0,

i = j = k = l,

i = j, k = l, i

= k,

i = k, j = l, i
else.

= j,











Note that the same holds even if R is not t-distributed but only generalized elliptically
ΣS does not depend on the generating variate of R. Particularly, the
distributed since
spectral estimator is not disturbed by the tail index of R.

b

Now one may ask when the sample covariance matrix is dominated (in a component-
wise manner) by the spectral estimator provided the data are multivariate t-distributed.
Regarding the main diagonal entries of the covariance matrix estimate this is given by

i.e.
if ν < 4 + 3d/(d + 4) the variance of the spectral estimator’s main diagonal
elements is smaller than the variance of the corresponding main diagonal elements of
the sample covariance matrix, asymptotically. Concerning its off diagonal entries we
obtain

i.e. ν < 4 + d. It is worth to note that several empirical studies indicate that the tail
indices of daily log-returns generally lie between 4 and 7 (see, e.g., Embrechts, Frey,
and McNeil, 2004, p. 81 and Junker and May, 2002).

In the following the daily log-returns from 1980-01-02 to 2003-10-06 of 285 S&P 500
stocks are analyzed for studying the robustness of the spectral estimator vs. the sam-
ple covariance matrix. The considered stocks belong to the ‘survivors’ of the S&P

d + 2
d

4

·

< 2

ν
ν

·

1
4

,

−
−

d + 2
d

<

2
4

,

ν
ν

−
−

13

6
6
6
6
500 composite at the last quarter of 2003. The sample size corresponds to n = 6000.
The total sample period is partitioned into 10 sub-periods each containing 600 daily
log-returns. Further, each sub-period is divided into ‘even’ and ‘odd’ days, i.e. there
is a sub-sample containing the 1st, 3rd, . . . , 599th log-returns and another sub-sample
with the 2nd, 4th, . . . , 600th log-returns. Hence each sub-sample contains 300 daily
log-returns of 285 stocks. Both the sample covariance matrix and the spectral estima-
tor are used for estimating the relative eigenspectrum of the true covariance matrix,
d
i=1 λi for each even and odd sub-sample, separately. If the
i.e. λ1/
covariance matrix estimator is robust against outliers then the estimated eigenspectra
of each sub-sample should be similar since even if the true eigenspectrum changes
dynamically over time this must affect both the even and the odd days, equally. The
eigenspectrum obtained in the even sub-sample can be compared with the eigenspec-
trum given by the odd sub-sample simply by the differences of the ordered (relative)
eigenvalues.

d
i=1 λi, . . . , λd/

P

P

−3

x 10

2

−3

x 10

2

e
c
n
e
r
e
f
f
i
d
 
e
u
a
v
n
e
g
e

l

i

1.5

0.5

1

0

−0.5

−1

−1.5

−2
0

sub−period 1
sub−period 2
sub−period 3
sub−period 4
sub−period 5
sub−period 6
sub−period 7
sub−period 8
sub−period 9
sub−period 10

e
c
n
e
r
e
f
f
i
d
 
e
u
a
v
n
e
g
e

l

i

1.5

0.5

1

0

−0.5

−1

−1.5

−2
0

sub−period 1
sub−period 2
sub−period 3
sub−period 4
sub−period 5
sub−period 6
sub−period 7
sub−period 8
sub−period 9
sub−period 10

50

100

150

200

250

50

100

150

200

250

eigenvalue rank

eigenvalue rank

Fig. 7: Eigenvalue differences for each ordered eigenvalue given by the sample co-
variance matrix (left hand) and by the spectral estimate (right hand).

On the left hand of Figure 7 we see the eigenvalue differences for each 10 sub-periods
caused by the sample covariance matrix. Similarly, the right hand of Figure 7 shows
the eigenvalue differences given by the spectral estimate. Figure 7 indicates that the
spectral estimator leads to more robust estimates of the eigenspectra of ﬁnancial data.
But note that - concerning the overall eigenspectrum - the sample covariance matrix
performs well up to the 4th sub-period. This is the period which contains the famous
October Crash of 1987. In contrast, the spectral estimator is not affected by extreme
values.

14

e
c
n
e
r
e
f
f
i
d
 
e
u
a
v
n
e
g
e

l

i

0.12

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06
1

sub−period 1
sub−period 2
sub−period 3
sub−period 4
sub−period 5
sub−period 6
sub−period 7
sub−period 8
sub−period 9
sub−period 10

e
c
n
e
r
e
f
f
i
d
 
e
u
a
v
n
e
g
e

i

l

0.12

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06
1

sub−period 1
sub−period 2
sub−period 3
sub−period 4
sub−period 5
sub−period 6
sub−period 7
sub−period 8
sub−period 9
sub−period 10

2

3
eigenvalue rank

4

5

2

3
eigenvalue rank

4

5

Fig. 8: Eigenvalue differences for the largest 5 eigenvalues given by the sample co-
variance matrix (left hand) and by the spectral estimate (right hand).

Figure 8 focuses on the differences of the 5 largest eigenvalues.
It shows that the
sample covariance matrix particularly fails for estimating the largest eigenvalue. Once
again this phenomenon is caused by the Black Monday which belongs to the even
sub-sample of the 4th sub-period. Note that the largest eigenvalue of the even sub-
sample exceeds the largest eigenvalue of the odd sub-sample by almost 12 percentage
points. We conclude that although the sample covariance matrix works quite good for
the most time it is not appropriate for measuring the linear dependence structure of
ﬁnancial data. This is due to a few but extreme ﬂuctuations on ﬁnancial markets.

5.2 Principal Components Analysis

Now, consider a d-dimensional vector R = (R1, ..., Rd) of long-term (e.g. yearly)
log-returns. Due to the central limit theorem each vector component of R is
i.i.d.
approximately normal distributed provided the covariance matrix of the short-term
(e.g. daily) log-returns exists and is ﬁnite. Since the sum of i.i.d. elliptical random
vectors is always elliptically distributed, too (see, e.g., Hult and Lindskog, 2002) one
may take for granted that the vector components of R are jointly normally distributed,
approximately. But this is not true if the number of dimensions d is large relative to
the sample size n.

For instance, consider a d-dimensional random vector X which is multivariate t-
distributed with ν > 2 degrees of freedom, location vector µ = 0, and dispersion
matrix Σ = (ν
Id. Due to the multivariate central limit theorem one could
believe that

2)/ν

−

·

Y :=

Xj

Nd (0, Id) ,

1
√n ·

n

j=1
X

·
∼

15

where X1, . . . , Xn are independent copies of X. But indeed Y TY
d holds only if
q := n/d is large rather than n being large (cf. Frahm, 2004, Section 6.2). Thus the
quantity q can be interpreted as ‘effective sample size’.

χ2

·
∼

In the following it is assumed that R is elliptically distributed with location vector µ
and dispersion matrix Σ. Let Σ =

T be a spectral decomposition of Σ. Then

ODO
R d= µ +

√

Y,

D

O
where Y spherically distributed with Σ = Id.
We assume that the elements of
, i.e. the eigenvalues of Σ are given in a descending
order and that the ﬁrst k eigenvalues are large whereas the residual ones are small. The
is orthonormal the
elements of Y are called ‘principal components’ of R. Since
Y remains up to a rotation in Rd. The direction of each principal
distribution of √
component is given by the corresponding column of

O

D

D

.

Hence the ﬁrst k eigenvalues correspond to the variances (up to a scaling constant)
of the ‘driving risk factors’ contained in the ﬁrst part of Y , i.e. (Y1, . . . , Yk). For the
purpose of dimension reduction k shall not be too large. Because the d
k residual risk
factors contained in (Yk+1, . . . , Yd) are supposed to have (relatively) small variances
they can be interpreted as the components of the idiosyncratic risks of each ﬁrm, i.e.

−

O

d

εi :=

Xj=k+1

p

λj OijYj,

i = 1, . . . , d,

where λj :=
Thus we obtain the following principal components model for long-term log-returns,

Djj.

Ri

d= µi + βi1Y1 + . . . + βikYk + εi,

i = 1, . . . , d,

where the driving risk factors Y1, ..., Yk are uncorrelated. Further, each noise term εi
(i = 1, ..., d) is uncorrelated to Y1, ..., Yk, too. But note that ε1, . . . , εd are correlated,
generally. The ‘Betas’ are given by βij =
λj Oij for i = 1, . . . , d and j = 1, . . . , k.
The purpose of principal components analysis is to reduce the complexity caused by
p
the number of dimensions. This can be done successfully only if there is indeed a
number of principal components accountable for the most part of the distribution. Ad-
ditionally, the covariance matrix estimator which is used for extracting the principal
components should be robust against outliers.

For example, let the daily log-returns be multivariate t-distributed with ν degrees of
freedom and suppose that d = 500 and n = 1000. Note that due to the central limit
theorem the normality assumption concerning the long-term log-returns makes sense
whenever ν > 2. The black lines in Figure 9 show the true proportion of the total
variation for a set of 500 eigenvalues. We see that the largest 20% of the eigenvalues

16

accounts for 80% of the overall variance. This is known in economics as ‘80/20 rule’
or ‘Pareto’s principle’. The estimated eigenvalue proportions obtained by the sample
covariance matrix are represented by the red lines whereas the corresponding estimates
based on the spectral estimator are given by the green lines. Each line is an average
over 100 concentration curves drawn from samples of the corresponding multivariate
t-distribution.

If the data have a small tail index as given by the lower right of Figure 9 then the
sample covariance matrix tends to underestimate the number of driving risk factors,
essentially. This is similar to the phenomenon observed in Figure 6 where the number
of large eigenvalues is overestimated. In contrast, the concentration curves obtained by
the spectral estimator are robust against heavy tails. This holds even if the long-term
log-returns are not asymptotically normal distributed.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
a
i
r
a
v
 
l
a
t
o
t
 
e
h
t
 
f
o
 
n
o
i
t
r
o
p
o
r
p

n
o
i
t
a
i
r
a
v
 
l
a
t
o
t
 
e
h
t
 
f
o
 
n
o
i
t
r
o
p
o
r
p

0
0

20

40

60

80

100
140
principal component

120

160

180

200

0
0

20

40

60

80

100
140
principal component

120

160

180

200

0
0

20

40

60

80

100
principal component

120

140

160

180

200

0
0

20

40

60

80

100
140
principal component

120

160

180

200

Fig. 9: True proportion of the total variation (black line) and proportions obtained by
the sample covariance matrix (red lines) and by the spectral estimator (green lines).
The samples are drawn from a multivariate t-distribution with ν =
(i.e. the multi-
variate normal distribution, upper left), ν = 10 (upper right), ν = 5 (lower left), and
ν = 2 (lower right).

∞

In the simulated example of Figure 9 it is assumed that the small eigenvalues are equal.
This is equivalent to the assumption that the residual risk factors are spherically dis-

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
a
i
r
a
v
 
l
a
t
o
t
 
e
h
t
 
f
o
 
n
o
i
t
r
o
p
o
r
p

n
o
i
t
a
i
r
a
v
 
l
a
t
o
t
 
e
h
t
 
f
o
 
n
o
i
t
r
o
p
o
r
p

17

tributed, i.e. that they contain no more information about the linear dependence struc-
ture of R. But even if the true eigenvalues are equal the corresponding estimates
will not share this property because of estimation errors. Yet it is important to know
whether the residual risk factors have structural information or the differences between
the eigenvalue estimates are only caused by random noise. This is not an easy task, es-
pecially if the data are not normally distributed and the number of dimensions is large
which is the issue of the next section.

5.3 Signal-Noise Separation

In the previous section it was mentioned that the central limit theorem fails in the
context of high-dimensional data, i.e. if n/d is small. Hence, now we leave the ﬁeld
of classical multivariate analysis and get to the domain of RMT.
Rd×d be a spectral decomposition where
Let Σ =
shall be a diagonal
matrix containing a ‘bulk’ of small and equal eigenvalues and some large (but not
necessarily equal) eigenvalues. For the sake of simplicity suppose

ODO

D

∈

T

=

D

"

cIk
0

0
bId−k #

c > b > 0,

−

k is large. Hence Σ has two different characteristic manifolds. The ‘major’
where d
(the ‘signal part’ of Σ) whereas
one is determined by the ﬁrst k column vectors of
the ‘minor’ one is given by the d
(the ‘noise part’
of Σ). We are interested in separating signal from noise that is to say estimating k,
properly.

O
k residual column vectors of

O

−

For instance, assume that n = 1000, d = 500, and that a sample consists of normally
distributed random vectors with covariance matrix Σ, where b = 1, c = 5, and k =
100. By using the sample covariance matrix and normalizing the eigenvalues one
obtains exemplarily the histogram of eigenvalues given on the left hand of Figure 10.
As might be expected the Marˇcenko-Pastur law is not valid due to the two different
regimes of eigenvalues. In contrast, when focusing on the smallest 400 eigenvalues,
i.e. on the noise part of
Σ the Marˇcenko-Pastur law becomes valid as we see on the
right hand of Figure 10.

b

18

y
t
i
s
n
e
d

1.4

1.2

1

0.8

0.6

0.4

0.2

0
−1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
t
i
s
n
e
d

0

1

2

3

4

5

6

0
−1

0

eigenvalue

2
1
eigenvalue

3

4

Fig. 10: Histogram of all d = 500 eigenvalues (left hand) and of the noise part (right
k = 400 smallest eigenvalues. The Marˇcenko-Pastur law
hand) consisting of the d
is represented by the green lines.

−

Thus separating signal from noise means sorting out the largest eigenvalues succes-
sively until the residual eigenspectrum is consistent with the Marˇcenko-Pastur law.
This is given, e.g., when there are no more eigenvalues exceeding the Marˇcenko-Pastur
upper bound λmax. In our case-study this is given for 397 eigenvalues (see the ﬁgure
below), i.e.

k = 103.

b

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
t
i
s
n
e
d

0
−1

0

1
2
eigenvalue

3

4

Fig. 11: Histogram of the remaining 397 eigenvalues after signal-noise separation.

As it was shown in Section 4 this approach is promising only if the data are not regu-
larly varying. Hence for ﬁnancial data not the sample covariance matrix but the spec-
tral estimator is proposed for a proper signal-noise separation.

6 Conclusions

Due to the stylized facts of empirical ﬁnance the Gaussian distribution hypothesis is
not appropriate for the modeling of ﬁnancial data. For that reason the authors rely

19

on the broad class of generalized elliptical distributions. This class allows for tail
dependence and radial asymmetry. Although the sample covariance matrix works quite
good with ﬁnancial data for the most time it is not appropriate for measuring their
linear dependence structure. This is due to a few but extreme ﬂuctuations on ﬁnancial
markets.

It is shown that there exists a completely robust ML-estimator (the ‘spectral estima-
tor’) for the dispersion matrix of generalized elliptical distributions. This estimator
corresponds to Tyler’s M-estimator for elliptical distributions. Further, it is shown that
the Marˇcenko-Pastur law fails if the sample covariance matrix is considered as random
matrix in the context of elliptically or even generalized elliptically distributed data.
This is due to the fact that stochastical independence implies linear independence but
conversely uncorrelated random variables are not necessarily independent. In contrast,
the Marˇcenko-Pastur law remains valid if the data are uncorrelated and the spectral
estimator is considered as random matrix.

The robustness property of the spectral estimator can be demonstrated for several ﬁnan-
cial applications like, e.g., portfolio risk minimization, principal components analysis,
and signal-noise separation.
If the data are heavy tailed the principal components
analysis tends to underestimate the number of driving risk factors if the sample covari-
ance matrix is used for extracting the eigenspectrum. This means that the contribution
of the largest eigenvalues to the total variation of the data is overestimated, systemati-
cally. Consequently, in the context of signal-noise separation the largest eigenvalues
are overestimated by the sample covariance matrix. This can be ﬁxed simply by using
the spectral estimator, instead.

References

[1] Bingham, N.H. and Kiesel, R. (2002). ‘Semi-parametric modelling in ﬁnance:

theoretical foundation.’ Quantitative Finance 2, pp. 241-250.

[2] Bouchaud, J.P., Cont, R., and Potters, M. (1998). ‘Scaling in stock market data:
stable laws and beyond.’ In: Dubrulle, B., Graner, F., and Sornette, D. (Eds.),
Scale Invariance and Beyond, Proceedings of the CNRS Workshop on Scale In-
variance, Les Houches, March 1997, Springer.

[3] Branco, M.D. and Dey, D.K. (2001). ‘A general class of multivariate skew-

elliptical distributions.’ Journal of Multivariate Analysis 79: pp. 99-113.

[4] Breymann, W., Dias, A., and Embrechts, P. (2003). ‘Dependence structures for
multivariate high-frequency data in ﬁnance.’ Quantitative Finance 3: pp. 1-14.

[5] Cambanis, S., Huang, S., and Simons, G. (1981). ‘On the theory of elliptically
contoured distributions.’ Journal of Multivariate Analysis 11: pp. 368-385.

20

[6] Chopra, V.K. and Ziemba, W.T. (1993). ‘The effect of errors in means, variances,
and covariances on optimal portfolio choice.’ The Journal of Portfolio Manage-
ment, Winter 1993: pp. 6-11.

[7] Eberlein, E. and Keller, U. (1995). ‘Hyperbolic distributions in ﬁnance.’

Bernoulli 1: pp. 281-299.

[8] Embrechts, P., Frey, R., and McNeil, A.J. (2004). ‘Quantitative methods for ﬁ-
nancial risk management.’ In progress, but various chapters are retrievable from
http://www.math.ethz.ch/˜mcneil/book.html.

[9] Engle, R.F. (1982). ‘Autoregressive conditional heteroskedasticity with estimates
of the variance of united kingdom inﬂation.’ Econometrica 50: pp. 987-1007.

[10] Fama, E.F. (1965). ‘The behavior of stock market prices.’ Journal of Business 38:

pp. 34-105.

[11] Fang, KT., Kotz, S., and Ng, KW. (1990). ‘Symmetric multivariate and related

distributions.’ Chapman & Hall.

[12] Frahm, G. (2004). ‘Generalized elliptical distributions:

theory and applica-
tions.’ Ph.D. thesis, University of Cologne, Faculty of Management, Eco-
nomics, and Social Sciences, Department of Statistics, Germany. Retrievable
from http://kups.ub.uni-koeln.de/volltexte/2004/1319/.

[13] Hiai, F. and Petz, D. (2000). ‘The semicircle law, free random variables and en-

tropy.’ American Mathematical Society.

[14] Hult, H. and Lindskog, F. (2002). ‘Multivariate extremes, aggregation and de-
pendence in elliptical distributions.’ Advances in Applied Probability 34: pp.
587-608.

[15] Junker, M. and May, A.

‘Measurement of aggregate risk with
copulas.’ Working paper, CAESAR, Bonn, Germany. Retrieved 2004-
10-14 from http://www.caesar.de/uploads/media/cae pp 0021
junker 2002-05-09.pdf.

(2002).

[16] Kelker, D. (1970). ‘Distribution theory of spherical distributions and a location-

scale parameter generalization.’ Sankhya A 32: pp. 419-430.

[17] Lindskog, F. (2000). ‘Linear correlation estimation.’ Working paper, Risklab,
Switzerland. Retrieved 2004-10-14 from http://www.risklab.ch/
Papers.html#LCELindskog.

[18] Mandelbrot, B. (1963). ‘The variation of certain speculative prices.’ Journal of

Business 36: pp. 394-419.

21

[19] Mehta, M.L. (1990). ‘Random matrices.’ Academic Press, 2nd edition.

[20] Mikosch, T. (2003). ‘Modeling dependence and tails of ﬁnancial time series.’ In:
Finkenstaedt, B. and Rootz´en, H. (Eds.), Extreme Values in Finance, Telecommu-
nications, and the Environment, Chapman & Hall.

[21] Oja, H. (2003). ‘Multivariate M-estimates of location and shape.’ In: H¨oglund,
R., J¨antti, M., and Rosenqvist, G. (Eds.), Statistics, Econometrics and Society.
Essays in Honor of Leif Nordberg, Statistics Finland.

[22] Praag, B.M.S. van and Wesselman, B.M. (1989). ‘Elliptical multivariate analy-

sis.’ Journal of Econometrics 41: pp. 189-203.

[23] Schmidt, R. (2002). ‘Tail dependence for elliptically contoured distributions.’

Mathematical Methods of Operations Research 55: pp. 301-327.

[24] Tobin, J. (1958). ‘Liquidity preference as behavior towards risk.’ Review of Eco-

nomic Studies 25: pp. 65-86.

Biometrika 70: pp. 411-420.

[25] Tyler, D.E. (1983). ‘Robustness and efﬁciency properties of scatter matrices.’

[26] Tyler, D.E. (1987). ‘A distribution-free M-estimator of multivariate scatter.’ The

Annals of Statistics 15: pp. 234-251.

[27] Visuri, S. (2001). ‘Array and multichannel signal processing using nonparametric
statistics.’ Ph.D. thesis, Helsinki University of Technology, Signal Processing
Laboratory, Finland.

[28] Yin, Y.Q. (1986). ‘Limiting spectral distribution for a class of random matrices.’

Journal of Multivariate Analysis 20: pp. 50-68.

22


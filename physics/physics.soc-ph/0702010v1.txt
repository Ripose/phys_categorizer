7
0
0
2
 
b
e
F
 
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
0
2
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Link-Space and Network Analysis

David M.D. Smith, Chiu Fan Lee, Neil F. Johnson and Jukka-Pekka Onnela

September 3, 2013

Abstract

In cond-mat/0608733, we introduce a detailed analysis appropriate
for analysing a wide range of network growth, decay and modiﬁcation
algorithms. In this paper, we further explore the use and derivation
of this framework. Many networks contain node-node correlations and
often conventional analysis is incapable of incorporating this often es-
sential feature. In fact, any use of walkers on networks must address
this. We explore some of the common oversights when these correla-
tions are not taken into account, highlighting the importance of the
formalism we subsequently introduce.

We explore the usefulness of the analytic tool through the ped-
agogical analysis of the random attachment and preferential attach-
ment mechanisms. We then go on to use the framework to explore
the correlations within such networks, investigating their assortativity.
Furthermore, we then derive the form of a perfectly non-assortative
network.

We then look at the interesting scenario of decaying networks, ap-
plying the framework to derive their properties. Furthermore, we in-
troduce a simple one parameter model which makes use of the correla-
tions within the network that it produces. This can produce tunable
skewness in its degree distributions but requires only local informa-
tion on behalf of the attaching object (node) whilst requiring minimal
information at the microscopic level.

1

Contents

1 Introduction

2 Notation

3 Random Walkers to Generate Networks

4 Node-Space and the Link-Space Formalism

4.1 Node-Space . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Link-Space
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Random Attachment . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
4.4 Preferential Attachment . . . . . . . . . . . . . . . . . . . . .
4.4.1 Exact Solution of Preferential Attachment . . . . . . .

4.3.1 Exact Solution of Random Attachment

5 The Growing Network with Redirection (GNR)

6 Degree Correlations

6.1 Perfect Non-Assortativity . . . . . . . . . . . . . . . . . . . .

7 The External Nearest Neighbour Model (XNN)

7.1 The External Nearest Neighbour Algorithm . . . . . . . . . .
7.2 Resolving the One-Step Random Walk . . . . . . . . . . . . .
Implementing the Link Space . . . . . . . . . . . . . . . . . .
7.3

8 Decaying Networks

8.1 Random Link Removal (RLR) . . . . . . . . . . . . . . . . . .
8.2 Random Node Removal (NR) . . . . . . . . . . . . . . . . . .

9 Discussion

A Variations on the External Nearest Neighbour Model

A.1 The Network Growth Algorithms . . . . . . . . . . . . . . . .
A.1.1 The Internal Nearest Neighbour Algorithm (INN)
. .
A.1.2 The External Nearest Neighbour Plus Plus (XNN++)
A.1.3 Internal Nearest Neighbour Plus Plus (INN++) . . . .

3

5

6

11
11
12
14
15
17
18

25

27
30

32
32
33
35

40
40
45

47

51
51
51
52
53

2

1

Introduction

Many real − world networks exhibit scale-free behaviour ranging from social
networks to the World Wide Web to industry[6]. These networks lie some-
where between the random graph and a fully ordered network. As such, they
have been linked to the well known preferential attachment model Barab´asi-
Albert[1].

Recent work by Holme and Kim[2] and Leary et al[3] have considered
the eﬀect on the resulting network by modifying the preferential nature of
the model. By ranking existing nodes according to degree, and associating
corresponding probabilities of connection with the new node accordingly,
over and under-skewed distributions (as compared to the power law) have
been produced.

In both the original and modiﬁed versions of the preferential attachment
algorithms it is required that the degree of every node in the existing net-
work be known prior to a new node being attached. This might be both an
impossible or impractical level of sophistication on behalf of the network de-
signer or agent in the case that the agent represents a node in a multi-agent
system generating the network. It is possible to recreate this preferential
attachment rule without this knowledge1. Recently, Saramaki and Kaski[8]
and subsequently Evans[9] considered the use of random walkers to decide
node attachment in a network growth algorithm. Although the approxi-
mations used prove adequate for the situation whereby 2 or more links are
added per new node, we inspect the justiﬁcations closely to provide more
accurate analysis. This is discussed in Section 3.

In Section 7, we introduce a simple, one-parameter algorithm which can
reproduce under-skewed, over-skewed and indeed power-law behaviour with-
out the necessity for global information in the node attachment step.

This is similar to the BA preferential attachment model in that at each
time step, one node is added to the system with one link, allowing the net-
work to grow as depicted in Figure 1. It diﬀers in that the attaching node has
no a priori knowledge of the existing system. The model is closely related to

1A trivial example selects random links and is perhaps the simplest scenario for repro-

ducing preferential attahment behaviour and can be described as follows:

i) Select a link from the existing network at random.

ii) Flip a coin to select one of the nodes connected to that link.
iii) New node is linked to this node.

This exactly reproduces the linear preferential model of Barab´asi and Albert in that
the probability of attaching to some node j in the existing network is proportional to
its vertex degree [1]. However, the microscopic nature of this implementation might be
inappropriate for modeling speciﬁc behaviour.

It is interesting to note that if one considers the algorithm in terms of adding a new link
to the system (where nodes are considered to be the joiners of links) then this random
attachment generates scale free behaviour.

3

Existing Network

New
Node

Figure 1: The evolving network.

the slightly more accessible directed GN R (Growing Network with Redirec-
tion) model of Krapivsky and Redner[10] which is discussed in Section 5. We
then present a general formalism for analysing network growth algorithms by
considering the inherent correlations within the network. This Link − Space
Formalism can be implemented to study the evolution of networks over all
possible realisations without the need for exhaustive simulation. This is dis-
cussed in Section 4. This formalism is not disimilar from previous studies
of the evolution of the joint degree distributions for the ends of edges to
measure degree correlations [7, 10] of networks whose degree distributions
are known. Here, we show how the implementation is necessary to obtain
the degree distribution for the algorithm suggested in Section 7.

4

2 Notation

Throughout the subsequent discussions, we shall use the following notation:

N = the number of nodes within a network

ς = the total number of links within a network

Xi = the number of nodes within a network of degree i
ci = the fraction of nodes within a network of degree i

=

Xi
N

pi = the probability of selecting an individual node in a network which has degree i
Pi = the probability of selecting any node in a network with degree i

= Xipi

if selecting at random

Li,j = the number of links from node degree i to node degree j for i 6= j

twice the number of links between nodes degree i in the undirected graph
the number of links between nodes degree i in the directed graph

Li,i =

li,j =

(cid:26)
Li,j
ς

h

1
knn

βi =

1
i ci

h

1
knn

ii

hknnii = average degree of the neighbours of nodes of degree i

ii = average of 1/degree of the neighbours of nodes of degree i

Θi = the probability of attachment to a node of degree i in the existing network (1)

5

3 Random Walkers to Generate Networks

The use of random walkers to generate networks was introduced by Sara-
maki and Kaski[8]. The simplest implementation of their growth algorithm
attaches a new node to the existing network component with m = 2 links
as follows:

1. Select a node at random within the existing network.

2. Move at random to a neighbour. This is now the selected node.

3. Establish a link between the selected node and the new node.

4. Move to a neighbour of the selected node.

5. Establish a link between this node and the new node.

The repetition is clear for m > 2 links per new node attached. To summarise,
a random walk of length m steps is performed within the existing network
from a randomly chosen start point, and a link between the new node and
each of the nodes reached in the random walk is formed with the exception
of the start point.

i

X

5
10

4
10

3
10

2
10

1
10

0
10

0
10

1
10

2
10

degree i

3
10

The degree distribution of an example network using the
Figure 2:
Saramki/Kaski algorithm grown to 100000 nodes adding m = 2 links with
each new node.

Certainly this yields results comparable to the scale-free preferential at-
tachment of the BA model as shown in Figure 2. However, the analytic
approximations do not hold in all cases. The conventional justiﬁcation is

6

as follows: Consider choosing some vertex labelled A initially (at random)
within the existing network comprising N nodes. This occurs with proba-
bility:

We now wish to consider the probability of moving to a neighbour vertex

labelled B after one step of the random walk. Utilising Bayes theorem:

p(A) =

1
N

p′(B) =

p(B | A)p(A)
p(A | B)

Where p′(B) denotes arriving at node B after one step, p(B | A) denotes
the probability of arriving at at B if node A is the initial vertex chosen and
p(A | B) is the conditional probability that given we have arrived at B we
originated at A. The former can be trivially written:

where kA is the degree of vertex A. So far so good. However, the next justi-
ﬁcation is somewhat misunderstood. It is often thought that the condition
probability p(A | B) can be written

which combined with the above yields:

This compares to the the BA preferential attachment probability for attach-
ing to a particular node of degree i:

P
Such that the probability of attaching to a node is proportional to its degree.

To understand the problems with this analysis, consider a real system
of nodes, as described in Figure 3. We can write the probability associated
with arriving at node B after a random walk of one-step in terms of the
initial node selected being one of the neighbours of B and the subsequent
probability of moving to node B:

1
kA

1
kB

p(B | A) =

p(A | B) =

p′(B) =

kB
N kA

pi =

i
j kj

7

(2)

(3)

(4)

(5)

(6)

(7)

C

A

D

B

E

Figure 3: A trivial example network.

p′(B) = p(A)p(A → B) + p(C)p(C → B) + p(D)p(D → B) + p(E)p(E → B)
1
kE

1
kD

1
kA

1
kC

+

+

=

+

(cid:9)

(8)

1
N
25
(cid:8)
12N

=

Clearly this is not the same as the earlier expression of equation 6. Nor
should it be. The earlier expression would yield diﬀerent results depending
on which node was labelled A which is clearly a non-sense.
Another interesting assumption often made is that because the initial node
is chosen at random, its vertex degree will be distributed similarly to that of
the entire network. As such equation 6 can be written in terms of arriving
at a particular of node degree i after randomly choosing an initial vertex
and moving at random to one of its neighbours:

p′
i =

i
N < k >

where < k > is the mean node degree of the existing network component.
Even averaging over all nodes of degree i, and (incorrectly) assuming that
on average the neighbours of these nodes were distributed identically to the
network as a whole, this would have implied the entirely diﬀerent:

< p′

i > =

P ′
i
Xi

=

i
N

<

>

1
k

We will discuss a similar expression in Section 7 and the mean degree of
nearest neighbours in Section 6.

(9)

(10)

8

In fact, the inherent correlations are such that they cannot be ignored.
We can see this by looking at an example network grown to N = 100000
nodes using the simplest form of the Saramaki algorithm outlined earlier.
Transforming our adjacency matrix into a Markov-Chain transition matrix
(dividing the columns by their sums) we can look at these correlations for
all possible random walks for a number of steps. We use the ﬁrst two parts
of equation 10 to infer the probability of arriving at a speciﬁc node of degree
i after one or two steps. We note that preferential attachment corresponds
to an inﬁnite random walk and can been calculated using the eigenvector of
the transition matrix corresponding to eigenvalue one.

−4

x 10

1 step
2 step
PA

1.6

1.4

1.2

1

0.6

0.4

0.2

i

p

0.8

0

0

10

20

30

degree i

40

50

Figure 4: The probability of arriving at a speciﬁc node of degree i compared
to the preferential attachment model after a random walk of one and two
steps. This is for a network comprising 100,000 nodes and is thus truncated
at degree 50.

We note that whilst the degree distribution is described by ci which is
the same as Pi = Xi
N for selecting nodes at random, the BA preferential at-
tachment is described in terms of individual node probabilities pi. As such,
the implementation used to demonstrate the correlations within an example
network generated using this algorithm is carried out using equation 10.

To understand why this model yields the scale-free degree distributions,
we must look at the expected degree of the nodes in the existing network
to which the new node is to be attached. Using the same transition matrix
process, the mean degree of nodes reached after a number of steps of random
walk is can be calculated for a given network (here 100,000 nodes) for all
possible random walks from random starting positions. The expected degree

9

of the ﬁrst node in the walk is much higher than the convergent (preferential
attachment) value, and the second lower as shown in Figure 5. By interpo-
lating between these two, an approach to scale free has been achieved for
this network growth algorithm, even for m = 2 links added (2 step random
walk) per new node.

Preferential Attachment
Random Walk

e
e
r
g
e
d
 
d
e
t
c
e
p
x
e

25

20

15

10

5

0

0

5

10
random walk length

15

20

Figure 5: The expected degree of the node arrived at after a number of
steps in a random walk from random initial node. This is performed on a
realisation of the Saramaki/Kaski network, m = 2 for 100,000 nodes

We will make use of these correlations in our proposed model (see Sec-

tion 7).

10

4 Node-Space and the Link-Space Formalism

4.1 Node-Space

Previous studies of various networks have focussed on the degree distribu-
tions. As such, the analysis has typically involved the construction of a
master (diﬀerential) equation for the number of nodes of some degree. We
can write such an equation for a growing network with the addition of one
node and one undirected link in terms of the probability of attaching to a
node of some degree Θ:

Xi |t+1 = Xi |t + Θi−1 |t − Θi |t

i > 1

(11)

Where Xi denotes the number of nodes degree i and Nt is the number of
nodes in the existing network. The second term describes the probability of
the new node attaching to an existing node of degree i − 1 thereby making
it a degree i node. The third term on the right hand side describes the
new node attaching to an i degree node in the existing network, thereby
decreasing Xi. The i = 1 case can also be written simply:

X1 |t+1 = X1 |t + 1 − Θi |t

(12)

The second term on the right hand side simply reﬂects that at each time
step, a node degree one is added. The third term describes the probability
that the new node attaches to a node degree one in the existing network.
Making a steady state approximation for the process of adding one node at
each timestep:

Xi |t = ciN |t

= ci(N0 + t)

dXi
dt

= ci

≈ Xi |t+1 −Xi |t

(13)

i.e. that the fraction of nodes of degree i in the system is constant. One can
rewrite the master equation (equation 11) for the growth algorithm in the
steady state terms of ci:

ci =

Θi−1
1 + Θi
ci

(14)

The |t notation has been dropped to indicate the steady state. Note that
we have said nothing about the attachment mechanism and subsequent at-
tachment probability kernel yet, although we have made the easily genener-
alisable restriction that only one node is being added per timestep with one
link.

11

4.2 Link-Space

We now introduce a similar analysis but retain the correlations inherent
in the network under consideration in a similar manner as that introcuded
by Callaway et al [5] to analyse degree correlations which in turn is an
interpretation of a quantity introduced by Krapivsky et al [10]. Consider
any link in a network. We can describe it by the degrees of the two vertices
it connects. As such, we can construct a matrix L such that the element
Li,j describes the number of links from nodes degree i to nodes degree j
for i 6= j. Li,i describes twice the number of links from nodes degree i
to nodes degree i for the undirected graph2. This might seem somewhat
obtuse, but a usefull product of this deﬁnition is that the diagonal elements
need not be considered diﬀerent from the other elements in the Link-Space
matrix in the mathematical analysis here after. We shall only be considering
the undirected case. An explicit example is given for the simple network
discussed in Section 3 in Figure 20. This network will be further analysed
in Section 7.2. The Link-Space matrix for this network is then expressed as

C

A

D

B

E

Figure 6: The simple network as discussed in Section 3.

in equation 15. Note the double counting of the 2 ↔ 2 link.

L = 

0 1 2 4
1 0 0 1
2 0 0 1
4 1 1 2











(15)

Clearly, for undirected graphs, the summation over all elements Li,j will
equal 2ς and for directed graphs this will be the total number of links, ς.

2For the directed graph, Li,i represents simply the number of links from nodes degree

i to nodes degree i.

12

This matrix represents a surface denoting the 1st order correlations between
the node degrees (the Link-Space). In order to write the master equations
for the evolution of the network in the link space, we must understand the
impact of connecting a new link into the system. The probability of the
new node connecting to any node within the existing network of degree
i − 1 is given by the attachment kernel Θi−1. The number of links under
consideration if an i − 1 node is selected is simply i − 1. The fraction of
Li−1,j |t
these that are connected to nodes of degree j can be expressed
.
(i−1)Xi−1|t
As such, this term describing the increase in links from nodes of vertex of
degree i to nodes of degree j through the attachment of the new node to an
existing node of degree i − 1 is:

Θi−1 |t (i − 1)Li−1,j |t
(i − 1)Xi−1 |t

We note that as each link has two ends, the value Li,j can increase through
connection to an i − 1 node which is connected to a j degree node or by via
connection to an j − 1 node connected to an i degree node.

We can now write the master equation analogous to equation 11 for the

number of links from i to j degree nodes for i, j > 1:

Li,j |t+1 = Li,j |t +

Θi−1 |t Li−1,j |t
Xi−1 |t

+

Θj−1 |t Li,j−1 |t
Xj−1 |t

−

Θi |t Li,j |t
Xi |t

−

Θj |t Li,j |t
Xj |t

and for i = 1, j > 1 (the undirected graph yields symmetric matrix L):

L1,j |t+1 = L1,j |t +Θj−1 +

Θj−1 |t L1,j−1 |t
Xj−1 |t
Θj |t L1,j |t
Xj |t

−

Θ1 |t L1,j |t
X1 |t

−

The second term reﬂects the increment of one if the new node connects to a
j − 1 node in the existing network making it a j degree node which deﬁnitely
has a link to a degree one vertex (the new node). We note that after the
initial seeding, L1,1 |t = 0 as the network comprises one giant component.
By making a similar steady state assumption as was done in “Node-Space”
as an equivalent to an inﬁnite network generated by this algorithm for the
process of attaching one new link to the system comprising ς |t links at each
timestep, we can write

(16)

(17)

(18)

Li,j |t = li,jς |t

= li,j(ς0 + t)

dLi,j
dt

= li,j

13

≈ Li,j |t+1 −Li,j |t

(19)

The master equation for the Link-Space then becomes for the steady state:

Θi−1

ci−1 li−1,j + Θj−1
cj−1 li,j−1
+ Θj
1 + Θi
ci
cj
Θj−1
cj−1 l1,j−1 + Θj−1

1 + Θ1

c1 + Θj
cj

i, j > 1

j > 1

li,j =

l1,j =

l1,1 = 0

The |t notation has been dropped to indicate the steady state. We note that
the expression ci denoting the fraction of nodes that are of degree i for this
steady state can be expressed:

where similarly

ci =

Xi =

k li,k
i

P

k Li,k
i

P

As such, the degree distribution is retrievable from the Link-Space matrix.
Again, note that we have yet to specify the attachment rules and probability
kernel.

4.3 Random Attachment

If the node in the existing network is to chosen at random for the new vertex
to be connected to, the attachment probability can be written:

As such, substituting into equation 14 we obtain the recurrence relation:

This yields the familiar 2−i distribution of vertex degrees.
Substituting into the Link-Space master equation (equation 20) we obtain:

li−1,j + li,j−1
3
cj−1 + l1,j−1
3

li,j =

l1,j =

l1,1 = 0

i, j > 1

j > 1

It is easy to populate the link space matrix numerically, just from the re-
currence equations 25 and 24.

Θi =

Xi
N
= ci

ci+1 =

ci
2

14

(20)

(21)

(22)

(23)

(24)

(25)

4.3.1 Exact Solution of Random Attachment

At ﬁrst glance, the solution to the master equation 25 would be of the form
(check by substitution):

li,j =

b
4 2i+j 3i+j

(26)

However, the boundary conditions (which could be interpreted as inﬂux of
probability into the diﬀusive matrix) are such that this doesn’t hold.
We can actually solve the link space for this model exactly. Consider the

c

x−1
l
1,x+1

l
1,x
l
2,x

l
i,j

Figure 7: The paths of probability ﬂux from cx−1 inﬂuencing element li,j.
Each arrow (step) represents a further factor of 1
3 .

values ci as being inﬂuxes of probability into the top and left of the link
space matrix. We can compute the eﬀect of one such element on the value
in the matrix li,j. Each step in the path of probability ﬂux reﬂects an extra
factor of 1
3 . First, we will consider the inﬂux eﬀect from the top of the
matrix as in Figure 7. The total path length from inﬂux cx−1 to element
li,j is simply i + j − x. The number of possible paths between cx−1 and
element li,j can be expressed j−x+i−1Cj−x using the conventional binomial
cobinatorial coeeﬁcient or “choose” coeﬃcient. We can similarly write down
the paths and lengths for inﬂuxes into the left hand side of the matrix.

15

As such, the ﬁrst row (and column) can be described:

Subsequent rows can be similarly described. So, for i, j > 1 an element can
be written:

l1,j =

cj−1 + l1,j−1
3
2−(j−1) + l1,j−1
3

1
3k2j−k

j−1

Xk=1

=

=

li,j =

li−1,j + li,j−1
3

=

j

α=2
X
i

α=2
X

(i−1+j−α)C(j−α)
3(i+j−α)2(α−1) +
(i−1+j−α)C(i−α)
3(i+j−α)2(α−1)

(27)

(28)

Here, C denotes the conventional binomial coeﬃcient and the results can
be observed in Figure 8. The Link-Space for both preferential and random

analytic solution
numerical implementation

0
10

−5

10

j
,
i

l

−10

10

−15

10

0
10

1
10
j

2
10

Figure 8: Comparison of the numerically derived Link-Space matrix and
the analytic solution for random node attachment.

attachment are shown explicitly in logarithmic coordinates in ﬁgure 9.

16

j
,
i

l
 

g
o

l

0

−5

−10

−15

−20

−25

−30
0

2

log i

4

0

1

2

log j

3

4

Figure 9: The Link-Space for the random attachment algorithm (blue) and
the preferential attachment rule (green).

4.4 Preferential Attachment

We now introduce the preferential attachment algorithm of Barab´asi and Al-
bert. The attachment probability is proportional to the degree of the node in
the existing network and as such, we can write (with proper normalisation):

Which in the steady state limit can be approximated by

Substitution of 30 into equation 14 yields the familiar recurrence relation

The solution of which is:

which can be checked easily by substitution. This corresponds well to an ac-
tual network grown using this algorithm as shown in Figure 10. Substituting

Θi |t =

iXi
2(N − 1)

Θi ≈

ici
2

ci =

=

(i − 1) ci−1
2

i − 1
i + 2

ci−1

−

i ci
2

ci =

4
i(i + 1)(i + 2)

17

(29)

(30)

(31)

(32)

simulation
Node Space

i

c

10

−3

0
10

−1

10

−2

10

−4

10

−5

10

−6

10

0
10

1
10

2
10

degree i

3
10

Figure 10: The eﬀectiveness of the Node-Space analysis against a single
simulated (grown ) network comprising 50,000 nodes.

equation 29 into equation 20, we obtain the master equations for the Link-
Space for this network growth algorithm.

li,j =

l1,j =

(i − 1)li−1,j + (j − 1)li,j−1
2 + i + j
(j − 1)cj−1 + (j − 1)l1,j−1
3 + j

l1,1 = 0

i, j > 1

j > 1

It is easy to populate the matrix numerically just by implementing the Node
and Link-Space equations.

4.4.1 Exact Solution of Preferential Attachment

At ﬁrst glance, the solution to the master equation 33 would be of the form
(check by substitution):

where w is a constant. This would imply for the degree distribution.

Summing over the entire Node-Space would give w = 6
π2−6 . However, the
boundary conditions (which could be interpreted as inﬂux of probability into
the diﬀusive matrix) are such that this solution doesn’t hold. This is evi-
denced by comparing the node degree distribution with equation 32 which

li,j =

w
i(i + 1)j(j + 1)

ci =

w
i2(i + 1)

18

(33)

(34)

(35)

compares to simulated networks well.

We can obtain an exact although somewhat less pretty solution by trac-
ing ﬂuxes of probability around the matrix and making use of the previously
derived degree distribution. We can rewrite our Link-Space master equation,
equation 20, in terms of vertical and horizontal components:

where trivially,

li,j = Ψi,jli−1,j + Υi,jli,j−1
l1,j = Υi,j
l1,1 = 0

l1,j−1 + cj−1

(cid:1)

(cid:0)

Ψi,j =

Υi,j =

Θi−1
ci−1

+ Θj
cj

1 + Θi
ci
Θj−1
cj−1

1 + Θi
ci

+ Θj
cj

(36)

(37)

c
1

c
2

c
3

c
4

ϒ

1,2

ϒ

1,3

ϒ

1,4

ϒ

1,5

ϒ

1,3

l
1,2

ϒ

1,4

l
1,3

ϒ

1,5

l
1,4

l
1,5

0

l
1,1

c

j−2

c

j−1

ϒ

1,j−1

ϒ

1,j

ϒ

1,j

l
1,j−1

l
1,j

l
i−1,1

l
i−1,2

l
i−1,3

l
i−1,4

l
i−1,5

l
i−1,j−1

l
i−1,j

Ψ

i,2

Ψ

i,3

Ψ

i,4

Ψ

i,5

Ψ

i,j−1

Ψ

i,j

ϒ

i,2

l
i,1

ϒ

i,3

l
i,2

ϒ

i,4

l
i,3

ϒ

i,5

l
i,4

l
i,5

ϒ

i,j

l
i,j−1

l
i,j

Figure 11: The components of ﬂux of probability around the link space
matrix. This is general to any attachment kernel. An element within the link
space matrix can be built up from contributing elements and the appropiate
factors.

19

By considering the probability ﬂuxes as shown in Figure 11, we can write

the individual elements in the link space matrix as:

j

j

l1,j =

cα−1

Υ1,x

α=2 (cid:18)
X
j

x=α
Y

(cid:19)

j

li,j =

li−1,αΨi,α

α=2 (cid:18)
X

+ li,1

Υi,x

j

x=2
Y

l1,1 = 0

Υi,x

(cid:19)

x=α+1
Y

Note that we have yet to introduce the attachment probability Kernels
and the analysis so far is general. Using the preferential attachment Kernel,

we can write our component-wise factors for the master equation

Θi ≈

ici
2

Ψi,j =

Υi,j =

i − 1
i + j + 2
j − 1
i + j + 2

Substituting equation 40 into equation 38 and using the previously derived
degreee distribution of equation 32 yields for the ﬁrst row:

j

x=α
Y

x − 1
x + 3

(cid:19)

j

l1,j =

4
α(α − 1)(α + 1)

α=2 (cid:18)
X
4(j − 1)!
(j + 3)!

j

(α + 2)

α=2
X
2(j + 6)(j − 1)
j(j + 1)(j + 2)(j + 3)

=

=

Subsequent rows can be described:

j

α=2
X

li,j = li,1

(j − 1)!(3 + i)!
(2 + i + j)!

+

li−1,α(i − 1)

(j − 1)!
(2 + i + j)!

(1 + i + α)!
(α − 1)!

=

(j − 1)!
(2 + i + j)!

(cid:26)

Rewriting gives

(3 + i)!li,1 + (i − 1)

li−1,α

j

α=2
X

(1 + i + α)!
(α − 1)!

(cid:27)

li,j =

(j − 1)!
(2 + i + j)!

Ki + Ei−1,j

(cid:27)

(cid:26)

20

(38)

(39)

(40)

(41)

(42)

(43)

where the meaning of Ki and Ei−1,j is transparent. Clearly, we can write
equation 43 for li−1,α:

li−1,α =

Ki−1 + Ei−1,α

(44)

(α − 1)!
(1 + i + α)!

(cid:26)

(cid:27)

Substituting equation 44 into equation 42 yields:

li,j =

Ki + (i − 1)

Ki−1 + Ei−1,α

(45)

(j − 1)!
(2 + i + j)!

(cid:26)

j

α=2
X

(cid:0)

(cid:27)
(cid:1)

In order to solve this recurrence relation we deﬁne an operator for repeated
summation, Sn

j,α such that

Sj,α(f (α)) =

f (α)

j

α=2
X
j

Sn
j,α(f (α)) =

αn

αn−1

α4

α3

α2

· · ·

f (α)

(46)

αn=2
X

αn−1=2
X

αn−2=2
X

α3=2
X

α2=2
X

α=2
X

The subscript denotes the initial variable to be summed over and the ﬁnal
limit. A few examples of this operation:

S0

j,α(f (α)) = f (α)
Sj,α(1) = j − 1
S2

j,α(1) = Sj,α(α) − Sj,α(1)

=

(j2 − j)

(j3 − j)

j(j + 1)
2

− 1

S3

j,α(1) =
etc

Sj,α(α) =

S2

j,α(α) =

(j3 + 3j2 − 4j)

1
2
1
6

1
6

21

etc

(47)

We can use this operator in our expression for the element li,j in equation 45

and expand to the easily derived value E2,α.

li,j =

=

=

Ki + (i − 1)Sj,α

(cid:26)

(j − 1)!
(2 + i + j)!
(j − 1)!
(2 + i + j)!
(j − 1)!
(2 + i + j)!
+(i − 1)(i − 2)(i − 3)Ki−3S3

(cid:26)

(cid:26)

(cid:0)

Ki + (i − 1)Ki−1Sj,α(1) + (i − 1)(i − 2)S2
j,α

Ki−2 + Ei−2,α

Ki + (i − 1)Ki−1Sj,α(1) + (i − 1)(i − 2)Ki−2S2

j,α(1)

(cid:0)

(cid:27)

(cid:1)

j,α(1) + ... + (i − 1)(i − 2)(i − 3) ∗ .. ∗ 2K2Si−2

j,α (1)

Ki−1 + Ei−1,α

(cid:27)

(cid:1)

+(i − 1)(i − 2) ∗ .... ∗ 2Si−2

j,α (E2,α)

(cid:27)

We can express E2,α in terms of the operator S too:

(48)

E2,α = 4Sα,α

2Sα,α(1) + Sα,α(α)

(49)

As such, the element li,j can be expressed

(cid:0)

(cid:1)

li,j =

(j − 1)!
(2 + i + j)!

i

(i − 1)!
(m − 1)!

(cid:26)

m=2
X

KmSi−m

j,α (1) + 4(i − 1)!

(2Si

j,α(1) + Si

j,α(α)

(cid:0)

(cid:27)

(cid:1)

This form is somewhat obtuse as the calculation of the operator values is
less than obvious. However, we can transform to a more easily interpreted
operator W (n) analogous to S but with diﬀerent limits such that

Wj,α(f (α)) =

f (α)

j

α=1
X
j

W n

j,α(f (α)) =

αn

αn−1

α4

α3

α2

· · ·

f (α)

(50)

αn=1
X

αn−1=1
X

αn−2=1
X

α3=1
X

α2=1
X

α=1
X

Whilst, at ﬁrst glance, it looks like little progress has been made, we only
need evaluate the repeated operation on initial function f (α) = 1. This is
exactly solvable and as such, we can rewrite in terms of a function G(n)
dropping the superﬂuous alpha subscripts:

Gj(n) = W n

Gj(n) =

j (1)
(j+n−1)!
n!(j−1)!
0

(

for n ≥ 0
for n < 0

(51)

The inductive proof associated with equation 51 can be understood by path
counting for some repeating binomial process and is an intrisic property of
the combinatorial choose coeﬃcient. Consider a repeated coin toss over x

22

steps. The number of ways of acheiving n+1 successes after these x iterations
is xCn+1. Now, the occurrence of this last success could have happend on
the n + 1th iteration or the following one, or any of the subsequent iterations
till the xth one. For the last successfull outcome to occur on the mth step, n
successfull outcomes must have occurred in the previous steps. The number
of ways this could have occurred is (m−1)Cn. Clearly, summing over all
possible m values, the total possible paths resulting in n + 1 successes must
equate to xCn+1.

For clarity, this is depicted in Figure 12. To reach point B from A in
the binomial process, one of the steps w, x, y or z must be traversed, after
which there is only one route to B. As such, the number of paths between
A and B utilising step w is the same as the number of paths between A
and W. Similarly, the number of paths between A and B utilising step x is
the same as the number of paths between A and X and so on. The number
of paths between A and B can be built expressed as the sum of the paths
A → W, A → X. A → Y and A → Z.

w

W

x

X

y

Y

Z

z

B

A

Figure 12: A binomial process over seven steps. The number of paths
between A and B can thus be expressed as the sum of the paths A → W,
A → X, A → Y and A → Z .

23

We incorporate this behaviour into our proof for the solution of Gj(n).

Gj(n) = (j+n−1)Cn

Gj(n + 1) =

Gj(n)

j

X1
j

=

(j+n−1)Cn

1
X
= (j+n)Cn+1

As G(1) = jC1 = j, this inductive proof should hold for all n. 3

As such the folowing relations hold:

Sn(1) = G(n) − G(n − 1)
Sn(j) = G(n + 1) − G(n − 1)

We can now write the element in our Link-Space matrix for preferential
attachment exactly as our function G(n) is easily evaluated.

li,j =

4(j − 1)!(i − 1)!

(j + i + 2)!  

G(i + 1) + 2G(i) − 3G(i − 1)

+

1
2

i

Xk=1

(k − 1)(k + 6)

G(i − k) − G(i − k − 1)

(cid:16)

!

(cid:17)

Hence, for the ﬁrst few rows of our Link-Space:

l1,j =

l2,j =

2(6 + j)(j − 1)
j(j + 1)(j + 2)(j + 3)

2j(j − 1)(j + 10) + 48
3j(j + 1)(j + 2)(j + 3)(j + 4)

3It is interesting to note the link between this discrete repeated summation and the

continuous repeated deﬁnite integral anology:

(52)

(54)

(55)

(56)

(57)

(53)

Vj(n) =

j

j

j

j

j

Z

0 Z

0 Z

0

Z

0 Z
0

. . .

1dj

n
{z

}

=

|
jn
n!

24

analytic solution
numerical implementation

0
10

−2

10

−4

10

−6

10

−8

10

j
,
i

l

−10

10

0
10

1
10

2
10
j

3
10

4
10

Figure 13: Comparison of the numerically derived Link-Space matrix and
the analytic solution for preferential attachment.

5 The Growing Network with Redirection (GNR)

In ref [10], Krapivsky and Redner suggested a model of network growth
without prior global knowledge using directed links. Within the same paper,
they also provided analysis for the evolution of the joint degree disribution of
simpler, directed growing networks (GN ), which is similar to the Link-Space
analysis outlined above. The GN R algorithm is implemented as follows:

1. Pick a node κ within the existing network at random.

2. With probability a make a directed link pointing to that node or

3. Else, pick the ancestor of κ at random and make a directed link point-

ing to that node.

We note that because each new node is attached to the existing network
with only one link, each node has only one out degree. As such, when
redirected, the destination is predetermined. The degree distribution for
this network is easily obtained. Consider some node of total (in and out)
degree i labelled A. It’s out-degree is one so it’s in-degree is simply i − 1.
As such, it must have i − 1 ‘daughters’. If any of these i − 1 daughter nodes
is selected initially in the GN R algorithm and redirection occurs, then the
new node will link to and point at A. As such, it is very easy to construct
the evolution of the node space for the total degree of nodes within the
system. The attachment probability kernel for the new node linking to any
node degree i within the existing network can be written:

Θi = aci + (1 − a)(i − 1)ci

(58)

25

A

a

(1−a)

initial

redirected

New Node

Figure 14: The Growing Network with Redirection. A random initial node
is selected. The new node links to (and points at) this node with probability
a or else is redirected to and attaches to the ancestor of the node.

The ﬁrst term on the right hand side corresponds to initially selecting a
node of degree i − 1 and linking to it. The second term reﬂects redirection
from any of the daughters of any of the degree i nodes. The static equation
for the steady state degree distribution becomes:

ci =

a + (1 − a)(i − 2)
1 + a + (1 − a)(i − 1)
(cid:0)
(cid:1)

ci−1

(59)

For a = 0.5 the familiar recurrence relation of preferential attachment is
retreived as of equation 31 and the distribution of total degree of nodes will
subsequently be the same too.

26

6 Degree Correlations

As shown in earlier sections, degree correlations are present in many net-
works and are an important feature. There have been few attempts to
quantify the degree correlations of a network, conventionally denominated
the assortativity. We will now explain this network characteristic. Essen-
tially, this is simply the question as to whether high degree nodes are more
likely to be connected to other high degree nodes or vica-versa.
Consider an arbitrary network of undirected links. If a random edge is se-
lected and a random end of that edge is selected, the probability that the
node arrived at has degree j will be proportional to j as higher degree nodes
have more links connected to them. Now, what happens if we consider only
those edges with one end attached to a degree i node? If there were no
correlations present, picking one from this subset of all edges at random
and looking at the degree of the node at the other end, again the proba-
bility that it has degree j would again be proportional to j. This situation
would be deemed non-assortative as described by V´azquez et al [11]. More
explicitly, we can describe this situation as given that one end of a link has
degree i, what is the probability that the other end has degree j. For a
non-assortative network this can be simply expressed (for total number of
edges ∼ total number of nodes):

This is arguably the strongest deﬁnition of assotativity although requir-
ing large networks, it has traditionally been diﬃcult to measure. However,
following the Link-Space analysis, we can easily investigate the quantities
P (j | i) for known Link-Space matrices, namely those of the random attach-
ment growth algorithm and the preferential atttachment of Barab´asi and
Albert which we have exactly. The conditional probability can be written in
terms of the Link-Space matrix (for total number of edges ∼ total number
of nodes):

(60)

(61)

For perfect non-assortivity in a network (no degree correlations), this would
be independent of i and equal the expression of equation 60. Plotting these
values over the range i : 1 → 50 and j : 1 → 1000 for both the random
attachment and preferential attachment algorithms shows that neither ex-
ample is perfectly non-assortative according to this deﬁnition as is evident
in Figure 15.

P (j | i) = P (j)

=

jcj
2

P (j | i) =

Li,j
iXi
li,j
ici

=

27

non−assortative

non−assortative

0
10

−50

10

−100

10

−150

10

−200

10

−250

10

−300

10

)
i

j
(

P

 

)
i

j
(

P

 

−3

10

0
10

−1

10

−2

10

−4

10

−5

10

−6

10

0
10

0
10

1
10

2
10

3
10

1
10

2
10

3
10

j

Figure 15: The conditional probability that one end of a randomly selected
link has degree j given that the other end is of degree i over the range
i : 1 → 50. The left hand plot is for the random attachment algorithm
and the right is preferential attachment. The criterion for non-assortativy
of equation 60 is denoted with red crosses.

V´azquez et al also suggest a slightly more accessible deﬁnition for the de-
gree correlations in a network which is somewhat less restrictive than the
previously described scenario but also consequently less speciﬁc. Calcula-
tion of the mean degree of nearest neighbours of a node as a function of
the degree of that node, hknnii presents an indication as to the degree cor-
relation of the network. This calculation is straightforward if one has the
Link-Space matrix for a particular network.

hknnii =

=

j jLi,j
j Li,j
j jli,j
j li,j

P

P

P

P

(62)

If the curve is constant, non-assortivity is implied. It the curve increases,
positive assortivity is assumed and conversely for hknnii decreasing with i
the network displays negative assortativity (dissassortativity). For the pref-
erential and random attachment algorithms, these quatities can be seen in
Figure 16.

Certainly, the preferential attachment curve in Figure 16 appears to as-
symptote to a constant value, whilst that of random attachment continues
to increase, implying positive assortative mixing in the latter.
In search of a single number representation, Newman[7] further streamlined
the measure by simply considering the correlation coeﬃcient of the degrees
of the nodes at each end of the links in a given network, although he sub-
tracted one from each ﬁrst to represent the remaining degree. Whilst this
might be of interest in some situations, the authors here feel that a correla-
tion of the actual degree would be a more general measure although the two
do not diﬀer much in practise. Using a normalised Pearson’s[7] correlation

28

preferential attcahment
random attachment

2
10

i

>

n
n

k
<

1
10

0
10

0
10

1
10
i

2
10

Figure 16: The mean degree of the nearest neighbours of nodes of degree i,
< knn >i as a function of i for both the preferential and random attachment
algorithms.

coeﬃcient, the measure of assortativity is expressed in terms of the degrees
(i and j) of the nodes at each end of a link, and the averaging is performed
over all links in the network.

r =

< ij > − < i >< j >
< ij >assort − < i >assort< j >assort

(63)

The normalisation factor represents the correlation of a perfectly assortative
network, i.e. one in which nodes of degree i are only connected to nodes of
degree i. As such, for a perfectly assortative network, r = 1. This is of the
same degree distribution as the network under consideration and has the
same number of nodes and links. The physical interpretation is less than
obvious especially as we are considering trees. However, for this assortative
network, li,j = iciδi,j, i.e. all oﬀ-diagonal elements are zero. Whilst this
assortativity measure can be expressed as summations over all links if cal-
culated for a speciﬁc realisation of a network, we can express it in terms of
the previously derived Link-Space correlation matrix.

r =

∞
i=1

∞
j=1

ijli,j
2 −

P

P
∞
j=1

j3cj
2 −

(cid:16) P
∞
j=1

j2cj
2

∞
j=1

2

j2cj
2
2

(cid:17)

(64)

It is interesting that the normalisation factor is degree-distribution speciﬁc.
Evaluating this measure numerically, we obtain for the preferential attach-
ment algorithm the value rpa ∼ 0 and for the random attachment algorithm,

P

(cid:16) P

(cid:17)

29

rra 0.6. So according to this measure of degree mixing, the randomly grown
graph is highly assortative and the preferential attachment graph is not.

6.1 Perfect Non-Assortativity

If adhering to the slightly stricter interpretation of V´azquez et al, it is inter-
esting to ask whether a perfectly non-assortative network can be generated.
In [7], Newman suggests a network-generating mechanism to produce an
arbitrary joint distribution of vertex degree at the ends of each link. As
such, if a Link-Space matrix can be found such that it satisﬁes equation 60,
the network it represents can be constructed.
Assuming that such a network will not have equal numbers of nodes and
links, we must rewrite the conditions of non-assortativity accordingly for
total number of edges ,ς:

Recalling our deﬁnition that li,j = Li,j/ς, we can express this conditional
probablity P (j | i) in terms of the Link-Space.

As such, we can write for li,j

It transpires that the solution to the Link-Space master equation for pref-
erential attachment in the absence of the boundary conditions satisﬁes this
criterion, namely the following surface:

However, the self consistency requirements of the matrix and how it relates
to node degree are such that the number of links must be less than the
number of nodes. We can write the fraction of nodes degree i:

P (j | i) = P (j)
jXj
2ς
jN
2ς

=

=

cj

P (j | i) =

Li,j
iXi
ς li,j
i N ci

=

li,j =

N
ς

2 i ci j cj
2

(cid:18)

(cid:19)

li,j =

2
i(i + 1)j(j + 1)

ci =

=

j li,j
ς
N P
i
2ς
N i2(i + 1)

30

(65)

(66)

(67)

(68)

(69)

non−assortative

)
i

j
(

P

 

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

−8

10

0
10

1
10

2
10

j

3
10

Figure 17: The conditional probability that one end of a randomly selected
link has degree j given that the other end is of degree i over the range
i : 1 → 100. The criterion for non-assortativy of equation 60 is denoted
with red crosses which concurs with a network with Link-Space as deﬁned
in equation 68.

This is clearly consistent with equation 68 and equation 67. Normalising
such that the sum over the degree distribution is one, we obtain:

ς
N

=

3
π2 − 6

(70)

This ratio, which is approximately 0.78, also represents half the mean degree
of such a network. This might not be the only solution.

31

7 The External Nearest Neighbour Model (XNN)

In the following case, we consider a very simple evolving network algorithm.
At each timestep t + 1, we wish to attach a single node, with a single link
to the existing un-directed network as in Figure 1. This comprises a single
component of Nt nodes. We wish to do so without prior knowledge of the
existing network structure, as might be appropriate in many real world sce-
narios. As such, any algorithm employed essentially searches the existing
network until it identiﬁes an existing node for our new node to link to. The
node might be considered an agent searching the existing network before
adhesing to it through a new link.

7.1 The External Nearest Neighbour Algorithm

The External Nearest neighbour algorithm is employed as follows for the
new node:

1. Pick a node κ within the existing network at random.

2. With probability a make a link to that node or

3. Else, pick any of the neighbours of κ at random and link to that node.

Some examples of the networks generated are shown in Figure 18.

Pajek

Pajek

Pajek

Figure 18: External Nearest Neighbour: some examples of the graphs gen-
erated for 300 nodes. Left is a = 1 center is a = 0.2 and right is a = 0. There
is no spatial relevance in these ﬁgures. Graphs are drawn with pajek[4].

The resulting distributions of vertex degree from implementing this growth

algorithm are shown in Figure 19. These are ensemble averages over 100 net-
works per parameter value.

Interesting, this parameter can yield a graph that is dominated by hubs
and spokes for a = 0 (extreme over-skewed)4. For a = 1 the random attach-
ment graph is retained, with values in the middle yielding results somewhere
4 Consider seeding the algorithm with an existing hub and spoke graph, with our
parameter a = 0. The probability that a new node will be linked to the hub tends to 1 as

32

 a = 1

 a = 0.2

 a = 0

i

X

2
10

i

X

2
10

i

X

2
10

6
10

4
10

0
10

−2

10

0
10

6
10

4
10

0
10

−2

10

0
10

6
10

4
10

0
10

−2

10

0
10

1
10
degree i

2
10

2
10
degree i

4
10

5
10

degree i

Figure 19: The degree distribution for the evolving networks grown to
100000 nodes, ensemble averaged over 100 networks per parameter value.
These have been initiated from a 2 node, 1 link component.

in between. For parameter a ∼ 0.2, the algorithm generates graphs that ap-
pear comparable to preferential attachment.

7.2 Resolving the One-Step Random Walk

In similar fashion to the method described in Section 4 we can write master
equations for the evolution of this network.
In order to do so, we must
ﬁrst establish the attachment probability kernel Θi for this algorithm which
requires properly resolving the one-step random walk. We note that this
quantity deos not replicate preferential attachment as is commonly thought
[8, 12]. Utilising the Link-Space formalism enables us to write the probability
P ′
i associated with performing a random walk of length one and arriving at
a node of degree i. This is subtly diﬀerent to arriving at a speciﬁc node
of degree i after a one-step walk (p′
i) as here we consider the possibility of
arriving at any of the nodes which happen to have degree i at some time t.

P ′

i = P1P1→i + P2P2→i + P2P2→i + P3P3→i + . . .
X2 |t L2,i |t
Nt2X2 |t

X3 |t L3,i |t
Nt3X3 |t

=

+

+

+ . . .

X1 |t L1,i |t
Nt1X1 |t
1
Nt

Lk,i |t
k

Xk

=

(71)

the network grows. Indeed, the distribution of vertex degree will approach that of a hub
and spoke graph as the node number gets large. This type of network might be considered
the most ordered in that the mean free path is minimised (tends to 2).

33

We can reconcile this with the alternative expression:

P ′

i =

iXi |t
Nt

h

1
knn

ii

where the average is performed over the neighbours of nodes of degree i. As
such with proper normalisation,

which clearly yields the same results.
Performing this process explicitly for the example network of Figure 20 yields
the following link space matrix for the N = 11,ς = 10 network:

h

1
knn

ii =

=

Li,k |t

k ∗

Xk
1
iXi |t

P

k
X

α Li,α |t
Li,k |t
k

C

A

D

B

E

L = 

0 1 2 4
1 0 0 1
2 0 0 1
4 1 1 2











34

Figure 20: The simple network as discussed in Section 3.

Note the the single edge linking the two degree 4 nodes is counted twice
(the element L4,4). Let us consider the probability of arriving at any node
of degree 1 by performing a random walk of length one from a randomly

(72)

(73)

(74)

(75)

(76)

(77)

(78)

chosen initial vertex.

P ′

1 = P1P1→1 + P2P2→1 + P2P2→1 + P3P3→1 + P4P4→1
1
11

= 0 +

1
11

3
4

1
4

2
3

+

+

+

(cid:16)

(cid:17)

1
1
11
2
Lk,1
k

=

1
N

Xk

This is clearly consistent with the analysis of equation 71.
We can now begin to formulate the master equations for this model. Writing
βi as:

βi =

ii

1
i ci

P
P

h

1
knn
Li,k
k
k
k Li,k
li,k
k
k
k li,k

=

=

P
P
The probability attachment kernel for the External Nearest Neighbour al-
gorithm becomes:

Θi = aci + (1 − a)βiici

7.3 Implementing the Link Space

Substituting equation 77 into equation 14 we obtain for the steady state
node degree:

ci =

ci−1

a + (1 − a)βi−1(i − 1)
1 + a + (1 − a)βii

(cid:0)

(cid:1)

Substituting into equation 20 we get:

li,j =

l1,j =

l1,1 = 0

li−1,j

a + (1 − a)βi−1(i − 1)

+ li,j−1

a + (1 − a)βj−1(j − 1)

(cid:0)
(cid:1)
a + (1 − a)βj−1(j − 1)

li,j−1

(cid:0)
a + (1 − a)βj−1(j − 1)

(cid:1)

+ cj−1

1 + 2a + (1 − a)(iβi + jβj )

(cid:0)

1 + 2a + (1 − a)(β1 + jβj )

(cid:1)

(cid:0)

i, j > 1

(cid:1)

j > 1

(79)

The non-linear terms resulting from β means that an analytical solution
is less than obvious. However, the formalism can be implemented in it’s
non-stationary form numerically (iteratively) with reasonable eﬃciency. As

35

described in Section 4, the master equations for the network growth in the
Link-Space are given for i, j > 1:

Li,j |t+1 = Li,j |t +

Θi−1 |t Li−1,j |t
Xi−1 |t

+

Θj−1 |t Li,j−1 |t
Xj−1 |t

−

Θi |t Li,j |t
Xi |t

−

Θj |t Li,j |t
Xj |t

and for i = 1, j > 1:

L1,j |t+1 = L1,j |t +Θj−1 +

Θj−1 |t L1,j−1 |t
Xj−1 |t
Θj |t L1,j |t
Xj |t

−

Θ1 |t L1,j |t
X1 |t

−

Computationally, it is useful to rewrite these equations ( 80 and 81) in
terms of matrix operations. Deﬁning the following matrices and operators:

(80)

(81)

(82)

0 1 0 0 . . .
0 0 1 0 . . .
0 0 0 1 . . .
. . .

E =

. . .
. . .
. . .
. . .

F =



0 0 0 0
...
...
...
0
0 0 0
0
1 0 0
0
0 1 0
0
0 0 1
...
...
...
. . .
I = the identity matrix
1 0 0 0 . . .
1 0 0 0 . . .
1 0 0 0 . . .
1 0 0 0 . . .
...
...










...

...

K =














































36

⊗ = Hadamard elementwise multiplication such that

for e = f ⊗ g, ei = figi

⊘ = Elementwise divide such that for e = f ⊘ g

ei =

fi/gi
0

for gi 6= 0
otherwise

(cid:26)
γt = Θt ⊘ X t

ρ = 





1
2
3
...
1
1
1
...
φ = 1 ⊘ ρ

1 = 



















= 












1
1
2
1
3

...
x1
0
0
0
...



diag(x) =

0
0
0
x4
...
Armed with these useful building blocks, we can represent the Link-Space

. . .
. . .
. . .
. . .
. . .

0
0
x3
0
...

0
x2
0
0
...















(83)



master equations ( 17 and 18) as follows:

Lt+1 =

(F − I)diag(γt) + I
(cid:16)
+ Fdiag(Θt)K + KT diag(Θt)E

Lt + Ltdiag(γt)(E − I)
(cid:17)

(84)

Note that we have not lost generality in specifying the attachment kernel
Θt = γt ⊗ X t. Although equation 84 looks somewhat intractable, the terms
can be easily explained. Premultiplying by F has the eﬀect of moving all the
elements down one place. Postmultiplying by E moves the elements right.
The last two terms represent the Θ term in equation 81.

We can now tailor for the three systems. For random attachment:

Θt =

γt =

1
Nt
1
Nt

X t

1

37

(85)

For preferential attachment we have:

Θt =

γt =

1
2(Nt − 1)
1
2(Nt − 1)

ρ

ρ ⊗ X t

Things start to get a little trickier for the external nearest neighbour algo-
rithm yielding:

Θt =

X t +

X t ⊗ ρ ⊗ βt

a
Nt
a
Nt

(1 − a)
Nt
(1 − a)
Nt

ρ ⊗ βt

γt =

+

Recall from equation 76

(cid:1)

(cid:0)

(cid:1)

Actual computation is somewhat more straight-forward than the mathe-
matics suggests. We seed with L1,1 |1 = 2 reﬂecting two connected nodes
(such that Nt = t + 1 and total links, ς = t). To account for the ﬁnite size of
the matrices involved, the Link-Space is normalised to 2 ∗ (Nt − 1) at each
iteration although in practise, the very edges of the matrix where numbers
might fall oﬀ have very small values. We retreive the degree distribution c
from the Link-Space matrix. Recall from equation 98:

(86)

(87)

(88)

(89)

Comparison of the matrices Lt+1 and Lt or the vectors ct+1 and ct gives
an idea of the proximity to the steady state. This implementation reﬂects
averaging over inﬁnite realisations and the results can be seen in Figure 21.
Clearly, these are comparable to the numerical numerical simulations of
Figure 19.

βi =

βt =

Li,k|t
k
k
k Li,k |t
⊘

Lt1

P
Ltφ
P
(cid:0)

k Li,k |t
i

Xi |t =

X t = φ ⊗ Lt1

ct =

X t

P

1
Nt

38

i

c

−3

10

0
10

−1

10

−2

10

−4

10

−5

10

−6

10

0
10

j
,
i

l
 
g
o

l

−2

−4

−6

−8

−10

−12

−14
0

a = 1

1
10

a = 0

2
10

3
10

degree i

Figure 21: The degree distributions generated using the Link-Space imple-
mentation of the External Nearest Neighbour algorithm.

log j

1

2

3

3

4

4

0

1

2

log i

Figure 22: The Link-Space for the preferential attachment rule and the
External Nearest Neighbour algorithm for a = 0. The more concave surface
it the latter, reﬂecting the higher expectancy of high degree nodes.

39

8 Decaying Networks

Although somewhat counter-intuitive it is possible to ﬁnd steady states of
networks whereby nodes and/or links are removed from the system. This
is an idea that has been considered before [13]. Aside from the obvious
situation of having no nodes or edges left, there can be a state whereby the
Link-Space correlation matrix and node degree distribution are static. We
shall discuss the two simplest cases, that of link removal and that of node
removal.

8.1 Random Link Removal (RLR)

Consider an inﬁnite network. At each timestep, we select w links at random
and remove them. We shall implement the Link-Space to investigate whether
or not it is possible that such a mechanism can lead to structure. Consider
the Link-Space element Li,j |t denoting the number of links from nodes of
degree i to nodes of degree j. Clearly this can be decreased if an i ↔ j is
removed. Also, if a k ↔ i is removed and that i node has further links to
j degree nodes, then those that were i ↔ j links will now be i − 1 ↔ j,
similarly for k ↔ j links being removed. However, if the link removed is a
k ↔ j+1 link and that j+1 degree node is connected to a i degree node, then
when the j + 1 node becomes an j node, that link will become an i ↔ j link
increasing Li,j as shown in Figure 23. Let us assume that we are removing
links at random from the (initially inﬁnite) network comprising N |t vertices
and ς |t links. A non-random link selection process could be incorporated
into the master equations using a probability kernel. We remove w links per
timestep. The master equation for this process can be written in terms of
the expected increasing and decreasing contributions:

Li,j |t+1 = Li,j |t+1 +w

Lk,j+1 |t jLi,j+1 |t
ς |t (j + 1)Xj+1 |t

Xk

Lk,i+1 |t iLi+1,j |t
ς |t (i + 1)Xi+1 |t

Lk,j |t (j − 1)Li,j |t
ς |t (j)Xj |t

Lk,i |t (i − 1)Li,j |t
ς |t (i)Xi |t

+w

−w

Xk

Xk

−w

Xk
Li,j |t
ς |t

−

40

(90)

This simpliﬁes to

Li,j |t+1 = Li,j |t+1 +

wjLi,j+1 |t
ς |t

+

−

wiLi+1,j |t
ς |t

−

w(j − 1)Li,j |t
ς |t

w(i − 1)Li,j |t
ς |t
wLi,j |t
ς |t

−

(91)

This expression can be simply understood by considering Figure 23. For each
i ↔ j + 1 link (labelled A in the ﬁgure), there are j links (B, C, D) whose
removal would make link A an i ↔ j link. This would be the same for all of
the i ↔ j + 1 links, of which there are Li,j |t in the network. The expected
increase in Li,j through this process corresponds to the second term on the
righthand side of equation91. The other terms are equally simply obtained,
with the last refering to the physical removal of an i ↔ j link.

A

i

j+1

B

C

D

Figure 23: Removal of any of the links B, C, D will result in the link A
becoming an i ↔ j link.

We make a similar steady state argument as before but this time for the

process of removing one link per timestep.

Li,j |t = li,jς |t

= li,j(ς0 − t)

dLi,j
dt

= −li,j

41

≈ Li,j |t+1 −Li,j |t

(92)

The expression of equation 91 can be reduced to:

li,j =

ili+1,j + jli,j+1
i + j − 2

As such, the number of 1 ↔ 1 links does not reach a steady state. This
might be expected, as the removal process for such links requires them to
be physically removed as opposed to the process by which the degree of the
node at one end of the link being reduced or increased. As such, the value
l1,1 would increase in time. However, we can investigate the properties of the
rest of the links in the network which do reach a steady state by neglecting
these two node components. In a similar manner to [10] we can make use
of a substitution to ﬁnd a solution to this recurrence equation, namely for
i + j 6= 2.

As such, we can obtain

This has a trivial solution

li,j = mi,j

(i + j − 3)!
(i − 1)!(j − 1)!

mi,j = mi+1,j + mi,j+1

mi,j =

A
2i+j

li,j =

A
2i+j

(i + j − 3)!
(i − 1)!(j − 1)!

ci =

ς
N

k li,k
i

P

ςA
N

∞

k=2
X

(k − 2)!
(k − 1)!21+k

c1 =

ςA
N i

∞

Xk=1

42

As such, the Link-Space for this system can be written for i + j 6= 2

Although this Link-Space solution doesn’t converge (and is hence not nor-
malisable), we can use the form of equation 97 to infer the shape of the
degree distribution. Recalling that

we can write for the degree distribution, neglecting the two node components
in the network,

ci =

(i + k − 3)!
(i − 1)!(k − 1)!2i+k

i > 1

(100)

(93)

(94)

(95)

(96)

(97)

(98)

(99)

j
,
i

−20

l
 
g
o

l

0

−5

−10

−15

−25

−30

−35

−40
0

2

log i

4

0

0.5

1

1.5

log j

2

2.5

3

3.5

4

Figure 24: The shape of the steady state Link-Space for random link removal
from an inﬁnite network numerically populated using equation 97 for i + j 6=
2.

The equation 99 can be easily solved by considering the Maclaurin Series[14]
(Taylor Series about zero) of the function ln(1 + x) (as studied by Mercator
as early as 1668) and evaluating for x = − 1
2:
(−1)k′
k′ xk′

ln(1 + x) =

∞

Rearranging equation99 and letting k = k′ + 1:

Xk′=1

∞

ln

= −

1
2

(cid:18)

(cid:19)

Xk′=1
= −ln(2)

1
k′ 2k′

c1 =

=

=

1
(k − 1)21+k

1
k′2k′+2

∞

k=2
X
∞

Xk′=1
ln(2)

ςA
N

ςA
N

ςA
4N

43

(101)

(102)

For node degree greater than one, the solution of equation 100 requires a
simple proof by induction. Consider the function Q(i′) deﬁned for positive

integer i′ as:

Q(i′) =

∞

i′+k′
Ck′
2k′+i′

Xk′=0
Where C denotes the conventional choose binomial coeﬃcients. We can
write Q(i′ + 1) with similar ease:

Q(i′ + 1) =

i′+k′+1Ck′
2k′+i′+1

∞

Xk′=0

As such:

2Q(i′ + 1) − Q(i′) =

i′+k′+1Ck′ −i′+k′

Ck′

1
2k′+i′

(cid:19)

∞

Xk′=0 (cid:18)

∞

Xk′=1

=

i′+k′

Ck′−1

1
2k′+i′

A quick substitution of k′′ = k′ − 1 leads to:

2Q(i′ + 1) − Q(i′) =

∞

i′+k′′+1Ck′′

1
2k′′+i′+1

Xk′′=0
= Q(i′ + 1)

⇒ Q(i′ + 1) = Q(i′)

Q(1) = 2

= Q(i′) ∀ i′

Rearranging equation 100 and using some simple substitutions, i′ = i − 2
and k′ = k − 1 we can derive the following:

ci =

(i + k − 3)!
(i − 1)!(k − 1)!2i+k

(i + k − 3)!
(i − 2)!(k − 1)!2i+k

i′+k′
Ck′
2i′+k′+3

∞

ςA
N i

Xk=1
ςA
N i(i − 1)

ςA
N i(i − 1)

ςA
4N

1
i(i − 1)

∞

k=1
X
∞

Xk′=0

=

=

=

44

We can normalise this degree distribution for the network without the

(103)

(104)

(105)

(106)

(107)

two node components (the 1 ↔ 1 links):

A =

c1 =

ci =

4N
ς(1 + ln(2))

ln(2)
1 + ln(2)
1
(1 + ln(2))i(i − 1)

i > 1

(108)

Although diﬃcult to compare to simulations, we can populate a linkspace
matrix numerically using equation 97. With this, we can compare the de-
gree distributions using equation 98 and those of equation 108 as shown in
Figure 25.

numerically derived
analytic expression

0
10

−1

10

i

c

10

−2

−3

10

−4

10

0
10

1
10
i

2
10

Figure 25: Comparison of the degree distributions from the numerically
populated Link-Space matrix and the analytic results of equation108 for
the Random Link Removal model. The two node components have been
neglected in both the numerical matrix population and analytically derived
expression.

8.2 Random Node Removal (NR)

In a similar manner to Subsection 8.1 we will now discuss the possibility of
creating such a steady state via a process of removing nodes from an existing
network at the rate of w nodes per timestep. Clearly, removing some node
which has a link to an i + 1 degree node which in turn has a link to a j
degree node can increase the number of i ↔ j links in the system. The other
processes which can increase or decrease the number of links from i degree

45

nodes to j degree nodes can be similarly easily explained. We consider that
we select a node of some degree k for removal with some probability kernel
Θk (as in the growing algorithms of Section 4). The master equation for
such a process can thus be written for general node selection kernel:

Li,j |t+1 = Li,j |t − w

Θi |t Li,j |t
Xi |t

− w

Θj |t Li,j |t
Xj |t

+ w

− w

Θk |t
Xk |t (cid:26)
Θk |t
Xk |t (cid:26)

k
X

Xk

Lk,i+1 |t

iLi+1,j |t
(i + 1)Xi+1 |t

+ Lk,j+1 |t

jLi,j+1 |t
(j + 1)Xj+1 |t (cid:27)

Lk,i |t

(i − 1)Li,j |t
iXi |t

+ Lk,j |t

(j − 1)Li,j |t
jXj |t

(cid:27)

(109)

We now select a kernel for selecting the node to be removed, namely
the random kernel although the approach is general to any node selection
procedure. As such Θk = ck as a node is selected purely at random. The
steady state assumptions must be clariﬁed slightly. As before

Li,j |t = li,jς |t

(110)

However, because we can remove more than one link (through removing a
high degree node for example) we must approximate for ς |t. Using the
random removal kernel, we can assume that on average, the selected node
will have degree < k >|t= 2ς|t
. As such, we can use this to calculate the
N |t
number of remaing links in the network.

Li,j |t = li,j(ς0 − w < k >|t)

= li,j(ς0 −

2wς |t
N |t

t)

dLi,j
dt

= −

2wς |t
N |t
≈ Li,j |t+1 −Li,j |t

li,j

li,j =

ili+1,j + jli,j+1
i + j − 2

(111)

(112)

Making use of the linkspace identities and equation 111, the master

equation (equation 109) can be written in simple form.

Clearly, this is identical to equation93 for the random link removal model
of Subsection 8.1 and consequently, the analysis of the degree distributions
will be the same too. By considering the average degree of the neighbours of
nodes of degree i, denoted hknnii and making use of equation 62 in Section 6
we can see that both the random link removal and the random node removal
algorithms generate highly assortative networks as evidenced in Figure 26.

46

i

>

n
n

k
<

30

25

20

15

10

5

0

0

5

10

20

25

30

15
 i

Figure 26: The mean degree of the neighbours of nodes of degree i for the
random link and random node removal algorithms.

9 Discussion

We have presented a formalism for studying the nature of evolving net-
works. This formalism generates the network features probabilistically in
that it exactly spans all possible realisations of the networks possible from
a given generating algorithm. This is an important development in that it
enables the algorithm designer to see the macroscopic eﬀects of microscopic
attachment rules without the computationally exhaustive requirement of av-
eraging over many simulations. Through the retention of the correllations
within the networks, this formalism allows detailed analysis of certain net-
work features such as the assortatvity.

We have used this formalism to dispell the myth that a one-step random
walk from random intial position in an arbitrary network is the same as
selecting a node with probability proportional to it’s degree. In fact, using
the results of Subsection 7.2 we can write down this constraint for a graph
comprising N nodes and ς links for the probability of arriving at any node
of degree i after a random walk from random starting position, P ′
i and the

47

preferential attachment kernel, Θi.

P ′

i =

Θi =

li,k
k

=

li,k
k

k
X
iXi
2ς
N
2ς

X

⇒

k
X

li,j = 2

ci = 1

i,j
X

i
X
j li,j
i

ς
N

i P

X

li,k ∀ i

(113)

Also the normalisation conditions must be satisﬁed too for both the Link-
Space and the Node-Space distributions:

(114)

= 1

(115)

Certainly, the perfectly non-assortative graph of Subsection 6.1 fullﬁls

these criterion. It remains a challenge to ﬁnd a general solution.

0
10

−1

10

i

−2

10

−3

10

−4

10

c
 
s
e
d
o
n
 
f
o
 
n
o
i
t
c
a
r
f

−5

10

0
10

0
10

i

c

−2

10

−4

10

−6

10

0
10

a=0.25
pref attach

1
10
degree i

2
10

a=1

1
10

node degree i

a=0

2
10

Figure 27: The range of network distributions using the External Nearest
Neighbour algorithm with preferential attachment shown as a dashed line
for comparison. The inset plot depicts the result of parameter a = 0.25.

We have presented a simple, one-parameter algorithm for generating
networks whose properties span from the exponential degree distribution of

48

random attachment to the fully ordered situation of the hub and spoke, with
the scale-free being a special case midway between as shown in ﬁgure 27.
This models utilises only one simple parameter, a and requires no global
information on behalf of the procedure linking a new node to the existing
network. With parameter a = 1 we have the random attachment. We
know that for a ∼ 0.2 the degree distribution approaches scale free. We
can actually make a better approximation of the value of a. Let us assume
that for some a value, the external nearest neighbour replicates scale free
behaviour for high degree nodes. As such, the attachment kernels must
equal. Recall from equation29 and equation77:

For high i we have:

i
2

= a + (1 − a)βii

a = 1 −

1
2βi

(116)

(117)

0.7

0.65

0.6

0.55

0.5

0.4

0.35

0.3

0.25

i

β

0.45

0.2

0

1000

2000

3000

node degree i

4000

5000

Figure 28: The assymptotic value of β for the prefrential attachment algo-
rithm.

We know that this value βi which represents the averaged value of the
reciprical of the degree of the neighbours of nodes of degree i (divided by the
fraction of degree i nodes in the system and by i itself) must be that of the
scale-free network as for the algorithm to be representative of preferential
attachment. We can use the exact solution of the Link-Space for the pref-
erentrial attchment algorithm (or read oﬀ the graph in Figure 28) to infer
βi in the high i limit. Using the ﬁrst two terms of equation 57 we obtain

49

β ∼ 0.66 (the graph assymptotes to 0.65) and substituting into equation117
gives a = 0.25.

XNN a = 0.25
preferential attachment

i

c

0
10

−1

10

−2

10

−3

10

−4

10

−5

10

−6

10

−7

10

0
10

1
10

2
10

degree i

3
10

Figure 29: Comparison of the preferential attachment algorithm degree
distribution and the Link-Space generated External Nearest Neighbour for
a = 0.25.

There are clearly many alternative yet similar algorithms which could
be designed which might also be analysed with the Link Space formalism.
Some of these are described in Appendix A. This might provide insight into
certain real world networks which not only exhibit similar deviations from
scale-free behaviour but might also have similar constraints in their micro-
scopic growth rules. Speciﬁcally, variations of this external nearest neigh-
bour algorithm might be considered appropriate for modelling the growth
of the World Wide Web with certain modiﬁcation for multiple steps and
directionality.

We have also used the linkspace to analyse decaying networks, showing
that a steady state is indeed possible. For the identical cases of random link
removal and random node removal an analytic expression for the degree
distribution has been obtained.

50

A Variations on the External Nearest Neighbour

Model

In section 7 we introduced the External Nearest Neighbour Algorithm for
network construction using simple rules using local information. Here, we
will discuss some similar algorithms for assembling networks which might be
applicable in certain real-world scenarios. The relationship between these
algortihms is shown schematially in ﬁg. 33.

In each of the following case, we consider a simple evolving network. At
each timestep, we wish to attach a single node, i with a single link to the
existing un-directed network as in Figure 1. This comprises a single com-
ponent of i − 1 nodes labelled numerically. We wish to do so without prior
knowledge of the existing network structure, this might be done randomly:
an existing node j could be selected at random and the link i ↔ j con-
structed yielding the familiar exponential distribution asociated with such
a random network[6].

As such, any algorithm employed essentially searches the existing net-
work until it identiﬁes an existing node for our new node to link to. As such,
the node might be considered an agent searching the existing space before
adhesing to it through a new link.

We refer to two distinct classes of growth algorithm, internal and external.

These refer to how the search starts with reference to ﬁnding a node in the
existing network for the new node to attach. The external class starts the
search at a randomly chosen node (not unlike a Diﬀusion Limited Aggrega-
tor) whilst the internal start from node 1. All models are considered to be
seeded with 2 connected nodes.

A.1 The Network Growth Algorithms

A.1.1 The Internal Nearest Neighbour Algorithm (INN)

The Internal Nearest neighbour algorithm is employed as follows for the new
node:

1. Start at node 1.

2. With probability a make a link to that node or

3. Else, pick any of the neighbours of 1 at random and link to that node.

Some examples of the networks generated are shown in 30. Trivially,
we can infer the behaviour for the extreme values of our single linking pa-
rameter, a. For a = 1 we have a hub and spoke around node 1, for a = 0 we
have a hub and spoke around node 2. Intermediate values will give highly
overskewed graphs with respect to the scale free distribution.

51

Pajek

Pajek

Pajek

Figure 30: Internal Nearest Neighbour: some examples of the graphs gener-
ated for 300 nodes. Left is a = 1 center is a = 0.5 and right is a = 0. There
is no spatial relevance in these ﬁgures. Graphs are drawn with pajek[4].

A.1.2 The External Nearest Neighbour Plus Plus (XNN++)

External Nearest Neighbour Plus Plus repeatedly implements the nearest
neighbour search. As such, the behaviour can be described as follows:

1. Pick a node j within the existing network at random.

2. With probability a make a link to that node or

3. else, select any of the neighbours of j at random.

4. With probability a make a link to the selected node or

5. else, select any of the selected node’s neighbours at random.

6. Repeat steps until connected.

Some examples of the networks generated are shown in 31.

Pajek

Pajek

Pajek

Figure 31: External Nearest Neighbour ++ : some examples of the graphs
generated for 300 nodes. Left is a = 1 center is a = 0.5 and right is a =
0.01. There is no spatial relevance in these ﬁgures. Graphs are drawn with
pajek[4].

Again we can infer the behaviour. For a = 1, the random graph is re-
tained. For a → 0, the number of iterations and hence the path length (λ)

52

explored in the existing network becomes large, being a geometric distribu-
tion:

P (λ = x) = a (1 − a)x

λ =

1 − a
a

(118)

As such, the Barab´asi preferential algorithm is approached and scale free

behaviour results.

A.1.3

Internal Nearest Neighbour Plus Plus (INN++)

The Internal Nearest Neighbour Plus Plus is implemented similarly and can
be described as follows:

1. Start at node 1.

2. With probability a make a link to node 1 or

3. else, select any of the neighbours of 1 at random.

4. With probability a make a link to the selected node or

5. else, select any of the selected node’s neighbours at random.

6. Repeat until connected.

Some examples of the networks generated are shown in 32.

Pajek

Pajek

Pajek

Internal Nearest Neighbour++ : some examples of the graphs
Figure 32:
generated for 300 nodes. Left is a = 1 center is a = 0.5 and right is a =
0.01. There is no spatial relevance in these ﬁgures. Graphs are drawn with
pajek[4].

Again we can infer the behaviour. For a = 1 the hub and spoke network
is generated. For a = 0, the path lengths becomes large and scale free
results.

53

Random
Attachment

Hub and
Spoke

Preferential
Attachment

Random Link
Attachment

External Nearest Neighbour

XNN++

Internal Nearest Neighbour

INN++

Figure 33: The nature of the algorithms in relating to the preferential
and random attachment algorithms. The arrow represents the parameter a
changing from 0 to 1.

References

[1] Barab´asi, A.L. and Albert, R., “Emergence of Scaling in Random Net-

works”, Science, 286:509:512, 1999.

[2] P.Holme and B.J. Kim, Phys.Rev.E 65, 026107 (2002).

[3] Leary, C.C.,Schwehm, M., Eichner, M. and Duerr, H.P. “Tuning Degree

Distribution of Scale-Free Networks”’, physics/0602152 (2006).

[4] V. Batagelj and A. Mrvar, Connections 21, 47 (1998).

[5] D.S. Callaway, J.E. Hopcroft, J.M. Kleinberg, M.E.J. Newman and S.H.

Strogatz, cond-mat/0104546 (2001).

[6] Dorogovtsev, S.N. and Mendes, J.F.F., “Evolution of Networks”, Ox-

ford University Press, 2003.

[7] Newman, M.E, “Assortative Mixing in Networks”, cond-mat/0205405

(2002).

[8] Saramaki, J. P. and Kaski,K. ,“Scale-Free Networks Generated By Ran-

dom Walkers”,cond-mat/0404088 (2004).

[9] Evans, T. S. and Saramaki, J. P.,“Scale Free Networks from Self-

Organisation”’,cond-mat/0411390 (2005).

54

[10] Krapivsky, P.L and Redner,S.,“Organization of Growing Random Net-

works”, cond-mat/0011094 (2001)

[11] V´azquez,A., Pastor-Satorras, R. and Vespignani, A ,“Large-scale topo-
logical and dynamical properties of Internet”’, cond-mat/00112400
(2001).

[12] V´azquez, A., cond-mat/0211528 (2003).

[13] Salath´e, M, May, R.M. and Bonhoeﬀer,S,“The evolution of network
topology by selective removal”’, RJ. R. Soc. Interface (2005) 2, 533536

[14] See

http://mathworld.wolfram.com/MaclaurinSeries.html

and

http://mathworld.wolfram.com/TaylorSeries.html.

55


Evolution of Vocabulary on Scale-free and
Random Networks

Alkiviadis Kalampokis a, Kosmas Kosmidis b and
Panos Argyrakis a
aDepartment of Physics, University of Thessaloniki, 54124 Thessaloniki, Greece
bInstitut f¨ur Theoretische Physik III, Justus-Liebig-Universit¨at, Giessen, Germany

Abstract

We examine the evolution of the vocabulary of a group of individuals (linguistic
agents) on a scale-free network, using Monte Carlo simulations and assumptions
from evolutionary game theory. It is known that when the agents are arranged in
a two-dimensional lattice structure and interact by diﬀusion and encounter, then
their ﬁnal vocabulary size is the maximum possible. Knowing all available words is
essential in order to increase the probability to “survive” by eﬀective reproduction.
On scale-free networks we ﬁnd a diﬀerent result. It is not necessary to learn the
entire vocabulary available. Survival chances are increased by using the vocabulary
of the “hubs” (nodes with high degree). The existence of the “hubs” in a scale-free
network is the source of an additional important ﬁtness generating mechanism.

Key words: Language evolution, Scale-free Networks, Monte Carlo Simulations
PACS: 05.10.Ln; 89.20.-a

1 Introduction

7
0
0
2
 
n
a
J
 
8
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
7
8
0
1
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Human language and its evolution has recently become an attractive subject
within the interdisciplinary scientiﬁc community [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21].
The reason for this interest is a natural consequence of the rapid advances in
the understanding and modeling of complex systems[22]. Statistical physics,
and mainly computational statistical physics, has proven to be quite eﬀective
in the study of systems of many interacting atoms and in the description of
several complex phenomena associated with these interactions, even though
the interacting units are no longer atoms or elementary particles but biolog-
ical species[23], human beings [24,25] or even ﬁnancial tools, such as stocks
[26]. It was also realized that human language, a traditionally qualitative sub-
ject of study, ﬁts adequately in the above quantitative framework. Several

Preprint submitted to Elsevier

08 -01-2007

aspects of language have been studied by diﬀerent groups. The main focus is
on language learning and its evolution [1,7,8], on the quantiﬁcation of language
characteristics (for example, the famous Zipf law) and their explanation from
ﬁrst principles [7,9,19], and on language competition between two [2,6,10,13]
or more languages [3,4,5,11,12,16,17]. In several of the above studies, for ex-
ample [3,6], the authors assume that language is learned by linguistic agents
that move on a regular two-dimensional lattice (a surface) and interact with
each other. The eﬀect of the surface topology or possible disorder is not taken
into account. It has been, however, recently understood that a lattice topol-
ogy may in several cases be an inadequate substrate for the description of
social interactions. In many cases, a better description is achieved if one takes
into account that social systems may be represented as graphs (networks),
i.e. as collections of nodes (representing individuals) which are connected to-
gether if the individuals represented by these nodes know each other. Social
networks have a structure similar to a scale-free network [27,28,29], which is
a graph whose degree (the number of edges that emanates from a node) dis-
tribution follows a power law, P (k) ∼ k−γ, and have attracted considerable
interest[27,28,29,30,31,32,33,34].
During most of human history, words were learned by individuals through dis-
cussions with those close to the learner. In the last decades, however, “modern”
technologies have changed this situation. We now have mobile phones, e-mail
accounts, web cameras and communication with even the most remote ac-
quaintance is not only possible but has become a rather easy task. Motivated
by this fact, we study language evolution on scale-free network structures.
We use Monte Carlo simulations and assumptions from evolutionary game
theory in order to evaluate the way the network topology aﬀects the vocab-
ulary size of a group of individuals. In this way we hope to get an insight
on how “modern” technologies may aﬀect language and its evolution. Monte
Carlo simulations on regular geometries have shown [6] that when agents are
arranged in a two-dimensional lattice structure, their ﬁnal vocabulary size is
the maximum possible. Scale-free networks, in contrast to regular lattices, are
characterized by the existence of nodes with very high degree (“hubs”) and
our results indicate that these hubs have an important impact on the vocab-
ulary size. Network theory has been used in the past to study the structure
of language ([19] and references therein). Here, however, we use it in order to
study vocabulary evolution, independently of any linguistic structure.

2 Model and Methods

We build a computational model to study the time evolution of the vocabulary
known by species which interact with each other. The model is in several as-
pects similar to a previous model used in [6] for describing language evolution

2

on a square lattice topology. In order to determine the eﬀect of the network
topology to the vocabulary learning characteristics, we used scale free net-
works with N = 10000 nodes and with γ = 2.0, 2.5 and 3.0, and Erd¨os - R´enyi
networks also consisting of N = 10000 nodes and connectivity probability
ρ = 0.002. Each node is always occupied by one agent. The language that
these agents have consists of 10 words. Thus, each agent possesses a maxi-
mum vocabulary of 10 words. The number of words that a given agent knows
at any time is not constant, since there are mechanisms to learn and to forget
words, which will be explained in the next paragraphs. Each agent has a num-
ber of attributes that characterize its behavior. The ﬁrst is the vocabulary V
which consists of an array of 10 elements. An element has a value of 1 if the
corresponding word is known to the agent or 0 if the word is unknown. Ini-
tially, each agent has a vocabulary that consists of 5 words, chosen randomly
out of the 10 possible words. The second is the ﬁtness, f , which determines the
probability of each agent to reproduce. The initial ﬁtness has a value of zero,
and agents can gain ﬁtness through successful communication. All agents take
part in the following activities: Communication, reproduction and mortality.
1) Communication: An agent i is chosen randomly and given the possibility
to communicate with one of its neighbors, j. As neighbors we consider the
nodes with which the speciﬁc node is connected. This communication confers
ﬁtness to both agents (i and j) according to the number of words agent i has
in common with agent j with which it communicates. Speciﬁcally:
a) The payoﬀ for the interaction is equal to the number of words i and j have
in common (e.g., three common words means a payoﬀ of 3). This payoﬀ value
is added to the ﬁtness of each agent, as a reward for successful communication.
b) Learn-forget process: Every word in the vocabulary is examined and if agent
i does not know a word which is known to j then there is a probability pL
that i will learn it from j. If this speciﬁc word is learned, then the correspond-
ing vocabulary array element will turn from 0 to 1. However, there is also a
probability pF that j will forget this word not known to i. The same rules
apply for words which are known to i and unknown to j. Thus, words that
are unknown to the majority of the population have increased probability of
being lost from the language.
2) Reproduction: There is a probability pr that a reproduction event will take
place. The selection of the agent to be reproduced is not random but propor-
tional to the agent’s ﬁtness. This means that agents with large ﬁtness have a
higher probability for reproduction. Each agent’s probability for reproduction
is given by the formula:

pi =

fi
Pi fi

(1)

where fi is the ﬁtness of agent i and this sum is over all agents. The fact that
we normalize over the total ﬁtness implies that there is information available
to all agents about the ﬁtness status of their society. Since all the sites (or

3

nodes) of our space are occupied the oﬀspring will have to be born in an already
occupied site, replacing the previous inhabitant. In this way the reproduction
and mortality procedures of the model are combined in one action as opposed
to the model used in [6]. We have two choices for the selection of the site
in which the oﬀspring will be born, and live thereafter. The ﬁrst is to put
it in one of the neighboring sites of the parent, which seems quite rational,
for the child to “live” near its parents. The second is to choose a random
site and place it there. Although both models give qualitatively the same
results, there are numerical diﬀerences. In the “local” model we observed more
ﬂuctuations, while in the “global” model these ﬂuctuations were not persistent
and soon smoothed out. For this reason in the current text we will present
only the “global” model, where the oﬀspring takes a randomly chosen site
in the network. The next choice to be made concerns the amount of ﬁtness
that the oﬀspring will inherit. For simplicity, we assume that the child inherits
the ﬁtness of the parent. Thus, the oﬀspring begins its life having the same
amount of ﬁtness its parent has, without aﬀecting the parents ﬁtness. All
oﬀspring carry the full vocabulary of the parent.
3) After each cycle of communication and reproduction, time is incremented
by 1/N, where N is the total number of agents in the lattice. Thus, one time
unit or Monte Carlo Step (MCS) statistically represents the time necessary
for each agent to execute the communication-reproduction cycle once. The
simulation continues until a predeﬁned total time is reached. For statistical
purposes we average our results over a large number of realizations, typically
1000, in this work. In most cases the time evolution of the system is followed
up to 100000 MCS, but since the system reaches a state of equilibrium much
sooner the data we show here are limited to 20000 MCS. In all simulation
results presented in the present manuscript we have used the values pL = 0.1,
pF = 0.1.

3 Results

3.1 No Reproduction, pr = 0.

First we studied the behavior of our system when no reproduction takes place.
In this case only the communication process is active and the agents can pass
linguistic information to their neighbors. One expects that after suﬃcient time
this process would stop since the system would reach a steady state where all
the agents have acquired exactly the same vocabulary. We studied scale free
networks, with γ = 2.0, 2.5 and 3.0 and random networks with connectivity
ρ = 0.002. Both netowrks consisted of N = 10000 nodes, The results for the
mean number of words known by each agent have shown that there is no real
change in the number of words known by the agents. Instead, there is only

4

some very small ﬂuctuation around the number of words that the inhabitants
of the system know at the start of the simulation. This means that there
is no tendency for “knowledge” to spread around the network. This can be
expected since if there is no reproduction, the knowledge of many words is
not an evolutionary advantage. Moreover, there are no newborn agents who
spread around the network, spreading also their vocabulary. This model is,
therefore, quite static, both in its rules and in the results that we get. This
result is similar to the one obtained in [6] for the case of no reproduction on
a lattice and agrees with intuition.

3.2 Reproduction, pr 6= 0.

In a previous work [6], it was shown that when identical (i.e having the same
initial ﬁtness) linguistic agents are allowed to move on a lattice, to learn and
forget words as described in the methods section and to reproduce with a
probability pr 6= 0 then the ﬁnal state of the system is one where all agents
have learned all possible words. To be precise, if the agents move on a lattice
and initially they know on average 5 words (i.e if the have 5 digits equal to
one in their vocabulary array V which has size equal to 10) we will end up in
a situation where all agents know an average of 10 words. This is reasonable
as language is a ﬁtness generating mechanism and thus, knowing many words
is essential for survival. The “survival of the ﬁttest” implies that in order
to survive, one has to know everything that is available and this is veriﬁed
by simulations. On a scale-free network, however, the situation is diﬀerent.
In Figure 1, we plot the mean number of words, < W >, known by each
agent as a function of time, for scale free networks consisting of N = 10000
nodes, for γ = 2.0, 2.5, 3.0 and random networks consisting of N = 10000
nodes, with connectivity ρ = 0.002 for pr = 0.1. It is obvious, especially in
the case of networks with γ = 2 that the number of words is quite below the
maximum vocabulary size. The reason for this is that in such a network there
are several nodes with very high degree (hubs). A hub has many neighbors
and, consequently, it can communicate with many other nodes and drasticly
increase its ﬁtness. This can be easily understood with the example of an
“extreme” case. Consider a star-like network of say N = 100 nodes. There
is one central node with degree k = 99, i.e. a hub, which is connected to all
other nodes. The remaining 99 nodes have k = 1, thus, they are connected
to the hub only. In one time step each node is on average selected once and
then it selects randomly one of its neighbors to communicate with. In this
extreme case, at the ﬁrst time step, each node has one chance to communicate,
except the hub that has 99 chances because it is the only neighbor that a
randomly selected node has. Thus, the hubs are in an advantageous position
and gain ﬁtness quickly. Then, they are favored in reproduction and ﬁnally
their vocabulary dominates the system. The existence of hubs has as a result

5

8

7

6

5

>
W
<

   =2.0

   =2.5

   =3.0

  ER (0.002)

 

0

5000

10000

15000

20000

Time (MCS)

Fig. 1. Mean number of words, < W >, known by each agent, vs. time, for scale free
networks consisting of N = 10000 nodes, for γ = 2.0, 2.5, 3.0 and random networks
consisting of N = 10000 nodes, with connectivity ρ = 0.002, for pr = 0.1. The
results are the averages of 1000 realizations.

that a node can gain ﬁtness not only by knowing many words, but also by
knowing just the words that are known to the hubs. There is, thus, a new
ﬁtness generating mechanism associated with the existence of the hubs. The
eﬀect is more profound for networks with γ = 2, where there are a lot of large
hubs and less evident for higher γ values where the hubs are fewer. It is also
important that the oﬀspring replaces a randomly chosen node, since thus it
favors the spreading of the vocabulary of the ﬁttest nodes, which in this case
is the vocabulary of the hubs.
The case of the random networks is quite diﬀerent, since here we observe a
signiﬁcant increase in the number of words the agents know, although they
are still lower than those of the square lattice. Thus, random networks are
between the two cases, showing the eﬀect of scale free networks but in a much
lesser extent, a fact we can safely assume is due to the lack of big hubs.

In Figure 2 we plot the fraction of the nodes that know 8 or more words
and 9 or more words, as a function of time. Since now knowledge of more
words is an evolutionary advantage we see that this fraction is much larger
than in the non-reproduction case we saw in the previous paragraph. It is,
however, lower than the values expected for a lattice topology and we can also
observe that low γ values are associated with lower fractions of agents with
“rich vocabulary”, in agreement with what we have previously mentioned.

 

6

>=8 Words

   =2.0

   =2.5

   =3.0

  ER (0.002)

 

>=9 words

0.3

   =2.0

   =2.5

   =3.0

  ER (0.002)

0.5

0.4

0.2

0.1

0.0

 

s
d
r
o
w
n
w
o
n
k
 
f

 

o
n
o

i
t
c
a
r
F

 

0

5000

10000

15000

20000

Time (MCS)

Fig. 2. The fraction of agents that know 8 or more words and 9 or more words, vs.
time, for scale free networks consisting of N = 10000 nodes, for γ = 2.0, 2.5, 3.0 and
random networks consisting of N = 10000 nodes, with connectivity ρ = 0.002 and
pr = 0.1. The results are the averages of 1000 realizations.

3.3 Reproduction and language competition, pr 6= 0.

We have also simulated language competition between two interacting species
on a scale free network using the algorithm described in detail in Ref [6].
The main diﬀerence now is that we have two species speaking two diﬀerent
languages and instead of starting with a random vocabulary knowledge, half
of the population knows perfectly one language and the other half knows
perfectly a completely diﬀerent language. In this case the maximum possible
vocabulary size is 20 (there are two “languages” that have 10 words each) and
we assume that the child inherits 80% of the father’s ﬁtness [35]. It is obvious
that inspite of the diﬀerences between the two algorithms, the diﬀerent initial
conditions on the word distribution and the diﬀerence in the ﬁtness amount
passed from one generation to the next, we can still observe that for scale-free
networks with γ = 2 the total number of words that are ﬁnally known by
the nodes is signiﬁcantly less than for the lattice case. This is an indication
that the role of the hubs is signiﬁcant for the propagation and learning of new
words and that this fact does not strongly depend on the speciﬁc details of
the models.

 

7

 

10

 A-A  =3

 A-B  =3

 B-A  =3

 A-A ER (0.002)

 B-B  =3

 A-B ER (0.002)

 B-A ER (0.002)

 B-B ER (0.002)

 

 A-A  =2

 A-B  =2

 B-A  =2

 B-B  =2

>
W
<

9

8

7

6

5

0

2500

5000

7500

10000

Time

Fig. 3. Plot for the two language model, of the average number of words known
to an agent vs time for scale free networks consisting of N = 10000 nodes with
γ = 2.0, 3.0 and random networks consisting of N = 10000 nodes with connectivity
ρ = 0.002, pr = 0.1 and the assumption that the child inherits 80% of the fathers
ﬁtness. The initial concentration is c = 0.15 for both A and B species

4 Conclusions

We have studied the evolution of the vocabulary of a group of individuals
on a scale-free network. We have demonstrated that there is an important
diﬀerence in this case, compared to the case where the individuals are regu-
larly distributed on a lattice or even with the case where they are allowed to
perform random walks on a lattice. On a lattice structure, the ﬁnal vocab-
ulary size of the individuals is the maximum possible. Knowing everything
is essential in order to increase the probability to “reproduce”. On scale-free
networks, however, the reproduction probability is considerably increased by
using the vocabulary of the “hubs”. This result indicates that the existence
of the “hubs” in a scale-free network is the source of an additional important
ﬁtness generating mechanism and may have profound and unexpected impact
on the evolutionary dynamics of a system.

Acknowledgements

We would like to thank Dr. L.K. Gallos for fruitful discusions. This work was
partially supported by the Hellenic Ministry of Education, via PYTHAGO-

8

RAS project.

References

[1] M. Nowak, D. Krakauer, Proc. Natl. Acad. Sci. USA, 96, 8028 (1999).

[2] D. Abrams, S. Strogatz, Nature 424, 900 (2003).

[3] C. Schulze, D. Stauﬀer, Int. J. Mod. Phys. C, 16,718,(2005) and AIP Conference

proceedings 119, 49 (2005) (8th Granada Seminar).

[4] C. Schulze, D. Stauﬀer, Phys. of Life Rev. 2, 89 (2005).

[5] C. Schulze, D. Stauﬀer, Adv. Complex Syst. 9, 183 (2006).

[6] K. Kosmidis, J.M.Halley, P.Argyrakis, Physica A 353, 595 (2005).

[7] K.Kosmidis, A.Kalampokis, P.Argyrakis, Physica A 366, 495 ( 2006).

[8] A. Baronchelli et al., arXiv:physics/0601114 = J.Stat.Mech. (2006), P06014.

[9] K.Kosmidis, A.Kalampokis, P.Argyrakis, Physica A 370, 808 ( 2006).

[10] V. Schw¨ammle, Int. J. Mod. Phys. C, 16, 1519 (2005).

[11] V. Schw¨ammle, Int. J. Mod. Phys. C, 17,103 (2006).

[12] T. Te¸sileanu, H. Meyer-Ortmanns, Int. J. Mod. Phys. C, 17, 256 (2006).

[13] M. Patriarca, T. Lepp¨annen, Physica A, 338, 296 (2004).

[14] J. Mira and A. Paredes, Europhys. Lett. 69, 1031-1034 (2005).

[15] J. P. Pinasco and L. Romanelli, Physica A 361, 355-360 (2006).

[16] V.M. de Oliveira et al., Physica A 361, 361 (2006).

[17] V.M. de Oliveira et al., Physica A 368, 257 (2006).

[18] S. Havlin, Physica A 216, 148 (1995).

[19] R. Cancho and R. Sole, Proc. Natl. Acad. Sci. USA, 100, 788 (2003).

[20] G.J. Baxter et al., Phys. Rev. E ,73, 046118 (2006).

[21] S. Wichmann et al.,Trans. Philological Soc., in press (2007).

[22] D. Stauﬀer, S.M. de Oliveira, P.M.C. de Oliveira, J.S. Sa Martins, Biology,
Sociology, Geology by Computational Physicists, (Elsevier, Amsterdam, 2006).

[23] M. Droz, A. P¸ekalski, Physica A, 362 , 504 (2006).

[24] T.J.P. Penna et al, Phys. Rev. E, 52, 3309 (1995).

9

[25] L.K. Gallos,Int. J. Mod. Phys. C, 16, 1329 (2005).

[26] R. N. Mantegna and H. E. Stanley, ”Turbulence and Financial Markets,” Nature

383, 587-588 (1996).

[27] L. A. N. Amaral et al., Proc Nat Ac Sci 97, 11149 (2000).

[28] M.E.J. Newman, D.J. Watts and S.H. Strogatz, Proc Nat Ac Sci 99, 2566

(2002).

[29] F. Liljeros et al., Nature 411, 907 (2001).

[30] A.-L. Barab´asi and R. Albert, Science 286, 509 (1999).

[31] R. Albert, H. Jeong and A.-L. Barab´asi, Nature(London) 401, 130 (1999).

[32] M. E. J. Newman, Phys. Rev. E, 66, 016128 (2002).

[33] R. Cohen et al, Phys. Rev. Lett. 85, 4626 (2000).

[34] R. Toivonen et al., arXiv:physics/0601114.

[35] A.Kalampokis, K.Kosmidis, P.Argyrakis, Study of

language evolution and
population dynamics of two interacting species on scale free and random
networks, ECCS ’06, Oxford.

10


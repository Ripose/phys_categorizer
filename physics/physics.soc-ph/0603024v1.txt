6
0
0
2
 
r
a

M
 
4
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
4
2
0
3
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Applying Free Random Variables
to Random Matrix Analysis of Financial Data

Zdzis law Burda,1, ∗ Andrzej Jarosz,2, 1, † Jerzy Jurkiewicz,1, ‡
Maciej A. Nowak,1, § G´abor Papp,3, ¶ and Ismail Zahed4, ∗∗

1M. Smoluchowski Institute of Physics and Mark Kac Center for Complex Systems Research,
Jagellonian University, PL–30–059 Cracow, Poland
2Niels Bohr Institute, DK–2100 Copenhagen, Denmark
3Institute for Physics, E¨otv¨os University, H–1518 Budapest, Hungary
4Department of Physics and Astronomy, SUNY Stony Brook, NY 11794 USA

(Dated: January 18, 2010)

We apply the concept of free random variables to correlated Wishart random matrix models. We
give a comprehensive rederivation of various spectral densities for a number of financial covariance
matrices involving stocks returns without and with exponentially weighted moving averages. We
show through simple models how to identify the pertinent underlying correlations. We extend our
results to L´evy–Wishart random matrix models whereby the risk factors are heavy tailed.

PACS numbers: 02.50.-r, 02.60.-x, 89.90.+n

I.

INTRODUCTION

Today, a large amount of data is stored in the memories of computers, usually in the form of large matrices. The
encoded data is marred by noise, and it is a constant challenge to unravel signal from noise. A very natural framework
for addressing this key problem is random matrix theory with its large corpus of results for Gaussian noise.

A decade ago, Bouchaud et al [1] and Stanley et al [2] have suggested the use of Gaussian random matrix theory
for addressing the issue of noise in financial correlation matrices. Since then, a number of results regarding the
quantification of noise in financial covariances have been derived using random matrix theory [3, 4, 5, 6, 7, 8, 9, 10,
11, 12, 13, 14, 15], some of which may be of relevance to risk management.

In this work, we would like to advertise the concepts of free random calculus as a powerful alternative to standard
random matrix theory both for Gaussian and L´evy noise. The initial and rather abstract mathematical approach
initiated by Voiculescu and coworkers [16, 17] has a concrete realization in the context of random matrix theory,
whereby large random matrices represent asymptotically free random variables (FRV for short). Several years ago,
we have suggested that these concepts are very useful for addressing a much larger class of noise in the context of
financial covariance matrices in a way that is succinct and mostly algebraic [18, 19].

The FRV calculus is a nontrivial generalization of classical probability calculus, where one–dimensional number–
valued probabilities are replaced by matrix–valued probabilities. This simplifies both conceptually and technically
many random matrix calculations, especially in the macroscopic or bulk limit of interest to practical problems. Our
original results [18, 19], have now seen further applications to financial covariances [20, 21] and macroeconomy [22].
We recall that the FRV calculus has been applied already to a number of problems ranging from physics [23, 24, 25]
to wireless telecommunication [26, 27, 28, 29].

The aim of this work is twofold. First, using FRV calculus, we rederive several known results obtained through
other (more laborious) methods of Gaussian random matrix theory. This illustrates the ﬂuency of the FRV calculus
for noisy financial covariances, with new results derived on the way. The generalization of these results to L´evy FRV

∗Electronic address: burda@th.if.uj.edu.pl
†Electronic address: jarosz@nbi.dk
‡Electronic address: jjurkiew@th.if.uj.edu.pl
§Electronic address: nowak@th.if.uj.edu.pl
¶Electronic address: pg@ludens.elte.hu
∗∗Electronic address: zahed@zahed.physics.sunysb.edu

calculus is presented. Although randomly sampled L´evy matrices have one or even no finite spectral moments, the
FRV calculus allows a straighforward analysis of the pertinent Green’s functions and their corresponding spectral
distributions.

In II we brieﬂy recall the basics of correlated Wishart matrices and how their spectral properties encode some
essentials of the financial correlation matrices. In III we give a pedagogical introduction to the FRV calculus including
the basic composition rules for addition and multiplication of free random variables. We show how to use these rules to
deduce spectral properties of large matrices from a doubly correlated Wishart ensemble. We derive a general solution
for the spectra of empirical covariance matrices. In IV we apply this approach to a number of examples, including
correlated Wishart matrices with exponentially weighted moving averages (EWMA) or time–delayed correlations. In V
we generalize the construction to free random L´evy matrices and show how the FRV calculus extends the Gaussian
results to heavy–tailed distributions. The applications are discussed in VI. In VII we conclude the paper and suggest
further extensions of the FRV techniques to problems in finances.

II. CORRELATED WISHART RANDOM MATRIX MODEL

A. Correlated Gaussian Distribution

The basic setting in which we will work throughout this paper can be introduced as follows. Consider T samples
of N real random variables drawn from a Gaussian probability distribution with a correlation matrix A (an N
N
symmetric and positive definite matrix). Each sample can be thought of as a measurement. Assume that these
measurements are not independent but correlated with a correlation matrix B (a T
T symmetric and positive
×
T rectangular matrix X with entries [X]ia (i = 1, . . . , N ,
definite matrix). The sampling results are stored in an N
×
a = 1, . . . , T ) for the i–th random variable (risk factor) in the a–th measurement. The probability distribution of the
matrix X is

×

Pc(X)

exp

∼



−

1
2

N

T

i,j=1
X

Xa,b=1



[X]ia[A−1]ij [X]jb[B−1]ba


1
2

−

(cid:18)

= exp

TrX τ A−1XB−1

,

(1)

(cid:19)

where τ stands for the transpose of the matrix and the index ”c” is short for ”correlated”. Using this measure, we
can compute the average of any quantity f (X),

An important parameter of the distribution (1) is the rectangularity ratio

(cid:1)

DXf (X)exp
DX exp

1
2 TrX τ A−1XB−1

−
1
2 TrX τ A−1XB−1
(cid:0)
−

.

(cid:1)

f
h

ic =

R

R

(cid:0)

N
T

,

r

≡

which plays the role of the noise–to–signal ratio. The quality of the data increases with decreasing r. We will be
interested in the ”thermodynamic limit” that is

N, T

,
→ ∞

and

r fixed.

B. Empirical Correlation Matrices

The (non–random) matrices A and B refer to correlation matrices as can be seen from the two–point correlation

function,

Note also that

[X]iaic = 0.
h

Our major goal is to discuss spectral properties of the empirical correlation matrices,

[X]ia[X]jbic = [A]ij[B]ab.
h

1
T

A ≡

XX τ ,

1
N

B ≡

X τ X,

2

(2)

(3)

(4)

(5)

(6)

which are respectively an N
×
be treated as estimators of the correlation matrices A and B because of the identities,

N and a T

T matrix, in the thermodynamic limit. These empirical correlators can

×

i. e. the averaging over the ensemble reproduces A and B up to some constant factors. To sum up, we will be
interested in the spectral properties of
, with X drawn from the ensemble (1). These two random matrices
B
are both called Wishart random matrix models.

and

A

]ij ic ∼
[
A
h

[A]ij ,

[
B

]abic ∼

h

[B]ab ,

C. From Correlated to Uncorrelated Gaussian Distribution

We now reformulate the problem in the language of uncorrelated Gaussian matrices.

Since A is symmet-
ric and positive definite, we can diagonalize it, A = Q diag(κ1, . . . , κN ) Qτ , and define its square root as
A1/2 = Q diag(√κ1, . . . , √κN ) Qτ , where QQτ = 1, and the eigenvalues κi > 0 for i = 1, . . . , N . Similarly for
B we can define B1/2. Now we use these two to change variables from X to a matrix

The advantage of using Y rather than X is that it has a simpler probability measure,

so averaging over Y is easier,

The price to pay for using Y is that now the estimators (6) assume a more involved form,

R

1
T

A

=

A1/2Y BY τ A1/2,

B1/2Y AY τ B1/2.

=

B

1
N

A−1/2XB−1/2.

Y

≡

P (Y )

exp

∼

1
2

−

(cid:18)

TrY τ Y

,

(cid:19)

=

f
h

i

R

DYf (Y )exp
DY exp (

1

2 TrY τ Y
−
TrY τ Y )
(cid:0)
−

.

(cid:1)

D. Green’s Function

×

GH (z)

1
K

≡

Tr

(cid:28)

1
z1K −

,

H

(cid:29)

We want to construct the (real) spectral densities of these estimators. The standard way of investigating the

distribution of the eigenvalues of an arbitrary K

K symmetric random matrix model H is through the resolvent

from which the eigenvalues density follows through

ρH (λ)

H)
i
i. e. from the discontinuities of the imaginary part. If the support of the spectral density is finite (but not necessarily
simply connected), GH (z) is holomorphic everywhere except some finite cuts on the real axis and can be expanded
in power series of 1/z with some pertinent coefficients mH,n (moments),

Trδ(λ1K −

ImGH (λ + iǫ),

lim
ǫ→0+

(13)

−

≡

=

1
K h

1
π

This can be rewritten in a slightly different form by introducing a function

GH (z) =

Xn≥0

mH,n
zn+1 ,

mH,n ≡

1
K h

TrH n

.

i

which satisfies

MH(z)

zGH(z)

1,

≡

−

MH(z) =

mH,n
zn

,

Xn≥1

so is just the generating function of the moments.

We stress that even if the moments do not exist, the knowledge of the analytical structure of the resolvent GH (z)

is sufficient to extract the spectral density from the pertinent discontinuities (13).

3

(7)

(8)

(9)

(10)

(11)

(12)

(14)

(15)

(16)

4

(17)

(18)

(19)

(20)

The two estimators (6) are related to each other. Indeed, their moments obey mA,n = rn−1mB,n for n > 0 due to

the cyclicity of the trace (and mA,0 = mB,0 = 1). This implies that

E. Duality

GB(z) = r2GA(rz) +

or

GA(z) =

1

r

,

−
z

1
r2 GB(

z
r

) +

1
r

,

1

−
z

and also that

ρB(λ) = r2ρA(rλ) + (1

r)δ(λ),

or

ρA(λ) =

−

1
r2 ρB(

λ
r

) +

1
(cid:18)

−

1
r

(cid:19)

δ(λ).

In terms of the generating function we have

MB(z) = rMA(rz),

or

MA(z) =

MB(

).

1
r

z
r

The current results possess thus the following symmetry,

A

B,

↔

,

N

T (i.e. r

A ↔ B

↔

1
r ),

↔

a sort of duality. Hence we can focus on either on
for

A

B

.
A

or on

; most of the calculations in this paper will be presented

The duality (20) holds even if the moments do not exist, in which case it follows from the relation between two
analytical complex–valued functions. This is readily seen from the definition (12). Noticing that the spectrum of the
N matrix
T

differs from the spectrum of the N

N ) zero eigenvalues, we may write

T matrix

by (T

×

B

A

−

GB(z) =

1
T

Tr

(cid:28)

z1T −

1
1
N X τ X

Tr

(cid:18)(cid:28)

z1N −

1
1
N XX τ

(cid:29)

+

T

N

−
z

=

(cid:19)

×

(cid:29)

=

1
T

= r

1
N

Tr

r
(cid:18)

(cid:28)

1
rz1N −

1
T XX τ

(cid:29)

+

T

N

−
z

(cid:19)

= r2GA(rz) +

1

r

,

−
z

which is the relation (17). Note that this derivation is true regardless of the relation between N and T , as it is
insensitive to whether N

T or N

T .

≤

≥

Let us also emphasize that our whole construction will be valid for the full range of r, i. e. for 0 < r <

.
∞

F. Complex Matrices

Everything said so far carries verbatim to ensembles of complex matrices X provided that the probability measure

(1) is substituted by

N

T

Pc(X)

exp

∼



−

i,j=1
X

Xa,b=1



[X]ia[A−1]ij [X]jb[B−1]ba


−
(cid:0)

(cid:1)

= exp

TrX †A−1XB−1

,

(21)

with A and B Hermitian and positive definite, and

1
T

XX †,

A ≡

1
N

B ≡

X †X.

(22)

Such ensembles have useful applications in telecommunication and quantum information theory. One–point Green’s
functions for infinite real and complex matrix ensembles have identical spectral properties in the thermodynamic
limit (modulo a trivial rescaling of the variance by a factor of 2), so one does not have to repeat any calculations as
long as one is not interested in finite size effects. We will restrict ourselves here to real matrices only, which have
many straightforward realizations in quantitative finance, where for instance the matrix X is composed of historical
logarithmic returns of N risk factors for T days.

III. RESOLVENT OF A CORRELATED WISHART RANDOM MATRIX MODEL FROM FREE
RANDOM VARIABLES

A. FRV Calculus in a Nut–Shell

We now detail the key features of the free random variables (FRV) calculus on the basis of standard probability
calculus. Say we are interested in the probability density function (hereafter ”pdf”) and the moments of the sum
of two random variables x1 + x2 in classical probability, given the pdfs p(x1) and p(x2) or the moments of the two
independent random variables x1 and x2. Most calculations in this problem can be done with the help of the Fourier
transform or the characteristic function of the pdf. Expanding the characteristic function in frequency yields all
can be simplified by
the moments of the pdf. Thus, the problem of calculating the mixed moments
taking the Fourier transforms for both pdfs p(x1) and p(x2), multiplying the resulting characteristic functions and
inverting the Fourier transform to obtain the pdf for x1 + x2. The expansion of the latter generates all the moments
(x1 + x2)n
. In addition, if we take the logarithm of the characteristic functions, the convolution problem reduces
h
i
to an additive one. The moments generated by the logarithm of the characteristic function are the cumulants kx,n.
They are additive under the convolution of two measures, or in other words, under the addition of two independent
random variables [30].

(x1 + x2)n
h

i

It is not obvious how to extend these steps to the case of random matrices. We have already defined the moments’
generating function MH (z) and the resolvent GH (z). The FRV calculus tells us that an analogue of the logarithm of
the characteristic function is another complex function, RH (z), defined as the generating function of free cumulants,
RH (z) =

n≥0 kH,n+1zn. Both complex functions are related to each other [16, 17],

P

GH

RH (z) +

= z,

(cid:18)

1
z

(cid:19)

i. e. the combination RH (z) + 1/z is the functional inverse of the resolvent. This is not surprising, since RH (z) is
for the resolvent what the self–energy is for ordinary Green’s functions. By definition, the R–transform is additive
(as the self–energy should). Therefore, the R–transform of the sum of two independent random matrix ensembles H1
and H2 is a sum of the corresponding R–transforms

RH1+H2 (z) = RH1 (z) + RH2 (z).

This relation is possible because one is able to directly generalize the notion of independence from classical probability
to a corresponding notion in random matrix theory, which is called freeness [16]. What is important, is that random
matrices drawn from factorized distributions asymptotically exhibit freeness in the limit of infinite matrix size N
.
→ ∞
Borrowing an analogy from physics, we may say that freeness is equivalent to planarity in the large N (number of
colors) limit in field theory [31, 32].

The correspondence between classical probability calculus and matrix probability calculus (FRV) is summarized in

the following chart,

pdf

↓
characteristic function

spectral density

↓
Green’s function

↓
logarithm of characteristic function

↓
R–transform

−→

−→

−→

A closely related problem is how to deduce a composition law for multiplication of random variables. The distribution
of the product is not widely discussed in textbooks on classical probability theory since it can be derived from the
exp x2 = exp(x1 + x2), which reduces to the additive case by a change of variable. However, this
relation exp x1 ·
is not the case for random matrices which do not commute.
= exp(H1 + H2). This
notwithstanding, there exists a transformation which allows one to calculate the resolvent for the ensemble of random
matrices H1H2 from the resolvent of each random matrix. This operation is called the S–transformation [16] and is
multiplicative,

In general, exp H1 ·

exp H2 6

It relates to each resolvents as follows

SH1H2 (z) = SH1 (z)SH2(z).

SH (z) =

χH(z),

where

1 + z
z

1
χH (z)

GH

1
χH(z)

(cid:18)

−

(cid:19)

1 = MH

1
χH (z)

(cid:18)

(cid:19)

= z,

5

(23)

(24)

(25)

(26)

(27)

i. e. 1/χH(z) is a functional inverse of the moments’ generating function MH(z).

Similarly as in classical probability theory, there is a corresponding central limit theorem for FRV.

•

•

•

•

•

There is a one–to–one correspondence between classical and free random variables which in particular allows
one to map probability densities of random variables into the corresponding eigenvalues’ densities of large free
random matrices [33].

Also, one can define the analogue of the concept of stability, which in the FRV calculus [34] assumes the form
of spectral stability.

A consequence of the above two observations is that the eigenvalues’ distribution of a properly normalized sum of
many random matrices for which the second spectral moment is finite tends to a universal limiting distribution
known in random matrix theory as Wigner’s semicircle law [35]. Wigner’s distribution in the FRV calculus
corresponds to the Gaussian distribution in standard probability calculus.

Another consequence is that there exists a counterpart of L´evy stable distributions for FRV. Since large random
matrices asymptotically represent free random variables, one can expect the existence of large free random
matrices in the L´evy stability class. We will exploit this fact in sec. V.

Recently it has been proven that FRV exhibits central theorems for extreme values [36], again in a one–to–
one correspondence with extreme values’ distributions known in classical probability from the Fisher–Tippet
theorem, i. e. the Fr´echet, Weibull and Gumbel distributions.

For completeness, let us also mention that FRV can also generate dynamical stochastic processes [37, 38, 39],
like Gaussian distributions generate random walks in classical probability. We will not discuss them in this work,
restricting ourselves to stationary properties of FRV only.

B. Practical Algorithms

For calculational purposes, let us now reformulate some of the above concepts in the form of simple algorithms.

•

Multiplication. As a fundamental tool in the derivations below, we use the S–transformation technique men-
tioned above. For notational convenience we introduce a functional inverse of the generating function MH(z)
(15), which we call N –transform,

It is related to the original χ–transform (27) by

MH(NH (z)) = NH (MH (z)) = z.

χH (z) =

1
NH (z)

.

The algorithm for multiplication of two matrices H1 and H2 goes now in three steps,

1. Knowing GH1 (z) and GH2 (z), we calculate the corresponding MH1 (z) and MH2(z), then we use (28) to get

the two functional inverses, NH1(z) and NH2 (z).

2. We use the multiplication law (26) to get the N –transform for the matrix product H1H2,

NH1(z)NH2 (z) =

NH1H2 (z).

1 + z
z

3. We functionally invert NH1H2 (z) to obtain MH1H2(z), hence also GH1H2 (z).

This completes the algorithm for multiplication.

6

(28)

(29)

(30)

•

Addition. For completeness, we recall the R–transform technique mentioned above, although in this paper we
shall use the law of addition rather marginally. It is convenient to introduce a functional inverse of the resolvent
GH (z) (12), which we call, after Zee [40], Blue’s function,

It is related to the original R–transform by

GH (BH (z)) = BH (GH (z)) = z .

RH (z) = BH (z)

1
z

.

−

The algorithm for addition of free random matrices goes again in three steps,

1. Knowing GH1 (z) and GH2 (z), and using (31), we calculate the corresponding functions BH1 (z) and BH2 (z).
2. We use the law of addition (24) to get the Blue’s functions for the sum H1 + H2,

BH1+H2 (z) = BH1 (z) + BH2 (z)

1
z

.

−

3. We functionally invert BH1+H2 (z) to obtain GH1+H2(z).

This completes the algorithm.

C. Correlated Wishart Random Matrix Model

We will use the uncorrelated measure (9) to compute the resolvent GA(z). We first observe that because of the
is involved, we can substitute the form (11)

cyclicity of the trace, in all calculations where the trace of powers of
of

by an equivalent form,

A

A

•

•

•

˜
A ≡

1
T

Y BY τA.

In particular,

(cid:28)
Choosing the form (34) in place of (11) simplifies our algorithm. Let us discuss it in detail.

(cid:29)

z1N −

1
T A1/2Y BY τA1/2

= G ˜A(z).

GA(z) =

Tr

1
N

1

To determine G ˜A(z), or equivalently M ˜A(z), we use the composition rules for products of free random variables

discussed in subsec. III B. We proceed in three steps,

We observe that 1

T Y BY τ and A are free, so we factorize the problem by use of the S–transformation.

We note that the spectra of 1
between both resolvents.

T Y BY τ and 1

T Y τ Y B differ only by zero modes, so there exists a simple relation

We again note that 1
the S–transformation composition rule.

T Y τ Y B can be viewed as a product of two free matrices, 1

T Y τ Y and B, so we exploit again

These three steps allow us to compute the pertinent resolvent using the FRV method.

Let us now execute this program. Denote for short

M (z)

MA(z) = M ˜A(z) = M 1

T Y BY τ A(z),

≡

and similarly N (z)

NA(z), and search for an equation for M (z) in a closed form.

≡

In the first step we break the matrix ˜
A

other. Using (12) and (15) we can compute the Green’s function of A,

= 1

T Y BY τ A into the product 1

T Y BY τ and A, which are free from each

GA(z) =

i. e.

MA(z) =

1
N

N

i=1
X

1

−

z

,

κi

N

1
N

1

−

i=1
X

1
κi/z −

1,

7

(31)

(32)

(33)

(34)

(35)

(36)

(37)

8

(38)

(39)

(40)

(41)

Now we multiply the numerator and denominator on the LHS by 1/N 1
(30)

T Y BY τ (z), and apply the multiplication law

where κi are the eigenvalues of A. Exploiting the definition (28) of the N –transformation, we get

1
N

N

i=1
X

1

1

−

κi
NA(z)

= 1 + z.

N 1

T Y BY τ (z)NA(z) =

1 + z
z

N (z).

N

1
N

1

i=1
X

−

1
κiM(z)N 1
T

Y BY τ (M(z))

z(1+M(z))

= 1 + M (z),

MA(w(z)) = M (z),

w(z)

z(1 + M (z))

.

≡

M (z)N 1

T Y BY τ (M (z))

Finally we substitute z

M (z) and obtain altogether

→

which can be written as

if we introduce a new variable

This is an equation for M (z). It defines a conformal mapping z
so we now determine it.

→

w(z). The function N 1

T Y BY τ (z) is still unknown,

To do so, in the second step of the construction, we relate the matrices 1

T Y BY τ and 1

T Y τ Y B. Since their spectra

differ only by (T

N ) zero modes,

−

G 1

T Y τ Y B(z) =

1
T

Tr

(cid:28)

z1T −

1
1
T Y τ Y B

=

1
T

(cid:29)

(cid:18)(cid:28)

Tr

z1T −

1
1
T Y BY τ

(cid:29)

+

T

N

−
z

=

(cid:19)

therefore

Note that this calculation is independent of the relation between N and T , and holds for all r.

In the third step, we treat the matrix 1

T Y τ Y B as the product of 1

T Y τ Y and B. Since both factors are free, we

proceed as in the first step. The N –transform of the matrix 1

T Y τ Y is known [25],

which we underline to be valid for all r. For completeness, we will present a convenient derivation of this formula
based on the FRV calculus towards the end of this section. For the matrix B, as before for A, we have

= rG 1

T Y BY τ (z) +

1

r

,

−
z

M 1

T Y τ Y B(z) = rM 1

T Y BY τ (z).

N 1

T Y τ Y (z) =

(1 + z)(r + z)
z

,

T

1
T

1

−

λa
NB (z)

a=1
X

1

= 1 + z,

where λa are the eigenvalues of B. Now we multiply the numerator and denominator of the LHS by 1/N 1
and use the multiplication law (30),

T Y τ Y (z),

N 1

T Y τ Y (z)NB(z) =

N 1

T Y τ Y B(z) .

1 + z
z

We use (41), change z

M 1

T Y τ Y B(z), and then use (40) of the second step of the algorithm to obtain

→

This can be written as

where

T

1
T

1

1

a=1
X

−

λar(1+M 1
T
z

Y BY τ (z))

= 1 + rM 1

T Y BY τ (z) .

MB(u(z)) = rM 1

T Y BY τ (z),

u(z)

≡

z
r(1 + M 1
T Y BY τ (z))

.

This is an equation for M 1

T Y BY τ (z).

The four equations (38), (39), (42) and (43) complete our three–step program. We have reduced the initial problem
to two consecutive conformal mappings. Now it is straightforward to put the equations (38), (39), and (43) together
into a compact form,

u(N 1

T Y BY τ (M (z))) =

N 1

T Y BY τ (M (z))
r(1 + M (z))

=

z
rM (z)w(z)

=

z
rMA(w(z))w(z)

,

which means that the solution has been reduced to solving the two equations,

MA(z) = MA(w(z)),

rMA(w(z)) = MB

z
rMA(w(z))w(z)

.

(cid:18)
In this way we have reproduced the result [15] without any reference to diagrammatic methods. These equations can
be directly used to determine the empirical spectral density function ρA(λ) provided the spectra of A and B are given.

(cid:19)

D. Comment

Let us brieﬂy comment on the basic equations (44) and (45).

The method does not rely on diagrammatics or on the existence of moments.

•

•

•

•

•

•

The method is not specific to Gaussian randomness. This will allow us to extend it to the more general case of
L´evy randomness below.

The method can be generalized to longer strings of free random matrices.

FRV calculus becomes exact in the thermodynamic limit N, T
general there will be ﬁnite size corrections O(1/N p), where p depends on the type of randomness.

, N/T = r ﬁxed. For ﬁnite T and N , in

→ ∞

For the special case of B = 1T , (45) reduces to [13, 41, 42]

w (1 + rMA(w)) = z,

since M1T (z) = 1

z−1 .

The practical meaning of these formulae is that one computes w(z) from (45) and then substitutes the solution
to (44) to get MA(z). Sometimes however it is useful to have an explicit (usually entangled) expression for
MA(z). Indeed, this follows from (45) where we exchange MA(w(z)) with MA(z). Using (44), and solving for
w(z), yield

w(z) =

z
rMA(z)NB(rMA(z))

.

9

(42)

(43)

(44)

(45)

(46)

(47)

Insering this back into (44) we get,

In particular, for A arbitrary and B = 1T ,

whereas for A = 1N and B arbitrary,

MA(z) = MA

z
rMA(z)NB(rMA(z))

.

(cid:19)

(cid:18)

MA(z) = MA

z
1 + rMA(z)

,

(cid:19)

rMA(z) = MB

z
r(1 + MA(z))

.

(cid:19)

E. FRV Derivation of (41)

(cid:18)

(cid:18)

As promised, we are going to show for completeness how to calculate (41) by means of FRV [25].

Let us first assume that N

T (i. e. r

1). Then the T

≤

≤

×

GOE random matrix H,

T matrix 1

T Y τ Y can be written in terms of a T

and a projector

as

PGOE(H)

exp

∼

T
2

−

(cid:18)

TrH 2

,

(cid:19)

P

diag(1N , 0T −N ),

≡

1
T

Y τ Y = HP H.

Note that we have the factor 1/T before Y τ Y , which differs from the corresponding factor 1/N in the estimator
(6). We need 1/T to ensure a proper normalization of H following from (51).

B

The cyclic property of trace implies that it is sufficient to consider a product of the square H 2 and the projector

P . As we show below,

GH2 (z) =

1
2  

1

1
− r

−

4
z !

,

i. e.

NH2 (z) =

(1 + z)2
z

.

The resolvent for the projector is

Using the cyclic property of the trace and the multiplication law (30) we obtain

GP (z) =

r

−

z

1

1

+

r

,

−
z

i. e.

NP (z) = 1 +

r
z

.

which is the result we wanted to reach. Hence we have proved (41) for N

≤
To complete the proof for all N and T , let us still remain in the case N

T .

T Y Y τ . It
N ) zero modes, hence the relation between their resolvents is given by an analogue of

T and consider the matrix 1

≤

differs from 1
(40),

T Y τ Y by (T

−

N 1

T Y τ Y (z) =

(1 + z)(r + z)
z

,

M 1

T Y τ Y (z) = rM 1

T Y Y τ (z),

10

(48)

(49)

(50)

T

×

(51)

(52)

(53)

(54)

(55)

(56)

(57)

N 1

T Y Y τ (z) = N 1

T Y τ Y (rz) =

(1 + z)(1 + rz)
z

.

This is still proven only for N

T .

Now let us assume N

T and return to the matrix 1

T Y τ Y . Call

≤

≥

i. e.

hence

Now we notice that the matrix 1
N
transformation (20), hence its N –transform arises from (58) through r

T follows precisely from the matrix 1

1/r, i. e.

˜Y ˜Y τ for N

≥

T Y Y τ for N

≤

T via the duality

Finally it remains to multiply it by r, according to (59), exploiting the general scaling relation

for any complex g, which gives

N 1

T Y τ Y (z) = rN 1

N

˜Y ˜Y τ (z) =

(1 + z)(r + z)
z

,

i. e. exactly (41). In this way we have proven (41) for both N

T and N

T , so for all r.

≤

≥

Let us now derive the equation for the resolvent GH2 (z) (54) for the matrix H 2. This resolvent can be constructed
from the resolvent GH (z) for H (51). For a Gaussian distribution, the highest non–vanishing cumulant is the second
one, so by the definition of the R–transformation, RH (z) = k1 + k2z. Since the measure (51) is centered around zero,
we have k1 = 0. One can check that the coefficient 1/2 in front of the trace in the measure (51) is set to give the second
cumulant k2 = 1, so eventually we have RH (z) = z and the corresponding Blue’s function becomes BH (z) = z + 1/z.
Inverting it functionally (31), we get a quadratic equation for GH (z) with two solutions,

We choose the lower sign since only then the limiting behavior of GH (z) for large z is GH (z)
1/z, in accordance
with the definition. Taking the imaginary part of this resolvent and performing the limiting procedure of (13), we
arrive at Wigner’s semicircle,

∼

Now we are ready to find the resolvent for H 2. We use the following observation,

1
z21T −
which immediately leads to the formula (54),

H 2 =

1
2z

1
z1T −

(cid:18)

+

H

1
z1T + H

,

(cid:19)

˜Y

Y τ ,

≡

1
T

Y τ Y = r

˜Y ˜Y τ .

1
N

N 1

N

˜Y ˜Y τ (z) =

↔
(1 + z)(1 + z
r )
z

.

NgH (z) = gNH(z),

GH (z) =

1
2

z

±

z2

4

.

−

(cid:16)

p

(cid:17)

ρH (λ) =

1
2π

p

λ2.

4

−

GH2 (z) =

1
2  

1

1
− r

−

4
z !

.

11

(58)

(59)

(60)

(61)

(62)

(63)

(64)

We hope that this short rederivation of classical Gaussian random matrix results, will convince the reader about the
obvious advantages of the FRV calculus.

IV. EXAMPLES

Let us now consider several particular cases of the correlation matrices A and B and find the moments’ generating

function M (z) for the corresponding estimator

.
A

A. Example 1: A = κ1N and B = 1T

First, let us focus on an (uncorrelated) Wishart ensemble with

Here we can use (46), MA(z) = κ/(z

κ), which leads to a quadratic equation for w,

−

A = κ1N ,

R+.

κ

∈

w2 + (rκ

κ

z)w + κz = 0.

−

−

It has two solutions

w =

z

rκ + κ

2(r + 1)κz + (r

1)2κ2

.

1
2

−

(cid:16)

z2

−

±

p

−

(cid:17)

We choose the one with the minus sign to reproduce the large z behavior of the resolvent, GA(z)
substitute it into (44) to obtain the generating function,

∼

1/z. Now we

−
and hence also the Green’s function,

MA(z) =

κ

w

κ

=

1
2rκ

z

rκ

κ

−

−

−

z2

−

(cid:16)

p

2(r + 1)κz + (r

1)2κ2

,

−

(cid:17)

This completes the solution.

GA(z) =

z + rκ

κ

−

−

z2

−

2(r + 1)κz + (r

1)2κ2

.

−

(cid:17)

p

1
2rκz

(cid:16)

Let us comment here on the role of the possible zero modes, depending on the value of (r

1). If r < 1, the 1/z
terms cancel in the above formula since there are no zero modes, contrary to the case of GB(z), which has (T
N )
zero modes. For r > 1, the opposite situation takes place. This sheds more light on the duality relations, which are
valid for any r.

−

−

B. Example 2: A Diagonal with Macroscopic Degeneracies and B = 1T

In the next example we consider A to be an arbitrary diagonal matrix,

A = diag(κ1, . . . , κK),

R+,

κl ∈

where κl appears with the degeneracy nl, where
remains finite and also the fractions

K
l=1 nl = N . Assume that in the limit of large size matrices, K

remain finite. In other words, in this example we are allowing for different variances of our random variables but still
no correlations.

We will use here the second version of the method, namely (49). We have

P
nl
N

,

pl ≡

pl = 1,

K

Xl=1

MA(z) =

K

Xl=1

plκl
κl
z

.

−

12

(65)

(66)

(67)

(68)

(69)

(70)

Therefore we get the following equation for MA(z),

MA(z) =

K

Xl=1

plκl (1 + rMA(z))
rκlMA(z)
z

.

κl −

−

This is a polynomial equation of order K + 1.

Let us see this explicitly for K = 2 (two different eigenvalues in A),

r2κ1κ2M 3 + r (

(κ1 + κ2)z + (r(p1 + p2) + 2) κ1κ2) M 2+

−

((rp1 + 1)κ1 + (rp2 + 1)κ2) z + (2r(p1 + p2) + 1) κ1κ2

M

−

(cid:1)

+

z2

−

(cid:0)

(p1κ1 + p2κ2) z + (p1 + p2)κ1κ2 = 0,

−

which is a 3rd order (Cardano’s type) equation for M

MA(z).

≡

C. Example 3: Exponentially Weighted Moving Average

Now let us consider the problem of the exponentially weighted moving average, a standard tool in sampling financial

covariances [43]. For that, consider first the correlated Gaussian distribution (1),

Pc(X)

exp

∼

1
2

−

(cid:18)

TrX τ A−1XB−1

,

(cid:19)

and the estimator (6)
can easily see that also any other combination of the form

A

= 1

T XX τ of the correlation matrix A. Note that this estimator is not unique. In fact, one

for some fixed symmetric and positive definite T

T matrix E, is also an estimator of A. Indeed, we have

where the proportionality constant is equal to TrEB. But this generalized estimator is easily noticed to reduce to the
previous case by the following change of variables,

which brings the generalized estimator (74) to the previous form (6),

1
T

E ≡

XEX τ

×

hEic ∼ h

ic,
A

˜X

XE1/2,

≡

1
T

=

E

˜X ˜X τ ,

and changes the measure (1) to

i. e. it amounts to change B

1
2

−

Pc( ˜X)

exp

∼

(cid:18)
E1/2BE1/2.

˜B

→

≡

Tr ˜X τ A−1 ˜X(E1/2BE1/2)−1

,

(cid:19)

In practice one frequently uses the freedom in choosing an estimator, and for instance in quantitative finance one
often uses the so–called exponentially weighted moving average (EWMA), which corresponds to a diagonal E of the
form

[E]ab ≡

δabT

1
1

α
αT αa−1,
−
−

13

(71)

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

where α < 1 is a parameter. Let us assume also the simplest case of A = 1N and B = 1T , i. e. we are dealing
with the uncorrelated Gaussian measure. In other words, we are interested in the estimator (77) with the probability
distribution

(cid:18)
which is exactly equivalent to ˜A = A = 1N and ˜B = E.

Pc( ˜X)

exp

∼

1
2

−

Tr ˜X τ ˜XE−1

,

(cid:19)

We will be interested in the eigenvalues’ spectrum of the EWMA estimator (77) in the double scaling limit

N, T

,
→ ∞

1−,

α

→

such that

r =

= fixed,

β = T (1

α) = fixed.

(81)

−

N
T

In this limit the range of the EWMA suppression exp(
from the one used in [21], and is more natural for relating the time cutoff to the length of the time series.

a ln α) is of order T . Note, that the definition of β differs

−

The situation is thus much alike the one in example IV B, i. e. ˜B = E is diagonal, however it does not have

macroscopic degeneracies (finite fracions pl in the thermodynamic limit), being non–degenerate,

We start from the general formula for M ˜B(z) and use (82),

˜B = E = diag(λ1, . . . , λT ),

λa = T

(82)

1
1

α
αT αa−1.
−
−

ME(z) =

1
T

T

λa

λa

z

−

a=1
X

=

1
T

T

a=1
X

1
z(1−αT )
T (1−α)αa−1 −

1

=

1
T

−

zn
1
T n (1
(cid:0)

αT
α)n
(cid:1)

−
−

n

T −1

Xa′=0

Xn≥1

′

n

α−a

1 =

−

then we remove T and α after expressing them in terms of N , r and β through (81),

zn
1
−
T n+1 (1
(cid:0)

αT

−

n

1
α)n (α−n
(cid:1)
(cid:0)

−

α−nT

1) −
(cid:1)

−

1,

=

Xn≥1

ME(z) =

n

N
r

− nN
r

zn

1
(cid:18)

−

(cid:16)

rβ
N

1

−

βn
r N

(cid:17)

(cid:19)

−

1

(cid:18)(cid:16)

rβ
N

1

−

−
−n

(cid:16)

1

−

(cid:19)

1
(cid:18)
β
N

(cid:17)

Xn≥1

(cid:17)

(cid:19)

1.

−

In the double scaling limit (81), which here reduces just to N

, it simplifies to

→ ∞

ME(z) =

zn

1

−

1

n

e−β
βn+1n
(cid:0)
(cid:1)

eβn

−

1 =

1 +

ln

−

(cid:1)

−

1
β

1

1

−

−

eβ

1
β
1
β (1
(cid:0)

−

1

z

−
e−β) z
(cid:1)

.

Xn≥1

(cid:0)

One can easily find the inverse function to ME(z) by solving the last equation for z. It gives the N –transform,

Having computed ME(z), we may proceed with our algorithm. We now use (50) for the moments’ generating

function M

ME (z), which here takes the form

≡

and is entangled. We can rewrite it for the Blue’s function, which is given explicitly by

NE(z) = β

eβ(1+z)

−
1) (eβz

(eβ

−

1

−

.

1)

z = rβ(1 + M )

eβ(1+rM)

−
1) (eβrM

(eβ

−

1

−

,

1)

BE (z) =

1
1
z  

−

1
rβ

ln

1

 

−

1

rβz

.

rβz
eβ −1 !!

−

14

(80)

(83)

(84)

(85)

(86)

(87)

This result is slightly more general than the one recently obtained in [21, 44], where the authors first took the r

0
limit, before taking the double scaling limit, while here we keep r fixed in the double scaling limit through (81). With
such constraints in our formalism, the Marchenko–Pastur limit β

0 is well defined, and we obtain in (87),

→

yielding the celebrated Wishart spectrum. The limit r
independently of β. To recover the result of [21, 44], we define q

0 also exists, defining the pole of the Green’s function at 1,
rβ as a fixed quantity, and in that case

→

BE (z)

1
z

+

→

1

→

,

rz

1

−

≡
eq/r
1

−

BE(z) =

1
z

1
(cid:18)

−

1
r

+

ln

1
q

1
−
e−q/r

qz
qz

−
−

,

(cid:19)

BE (z)

1
z

→

1
q

1
(cid:18)

−

ln(1

qz)

−

−

e−q/r
q

(1 + qz)

,

(cid:19)

In the r

0 limit one gets

→

with the first two terms reproducing [21, 44]. The third term vanishes exponentially for r approaching zero.

The equation (87) is also useful to obtain the support of the underlying spectrum. Following [25, 40], the endpoints

are defined as

x∗ = B(z∗),

where

B′(z∗) = 0.

The problem, for generic r and β, may only be solved numerically. The dependence of the upper and lower endpoints
of the support is shown in Figure 1.

15

(88)

(89)

(90)

(91)

r=0.3
r=0.8

*

x

10-1

102

101

100

10-2

10-3

10-4

 0

 2

 4

 6

 8

 10

rβ

FIG. 1: Endpoints of the support as a function of β = T (1 − α) for r = 0.3 and 0.8.

In Figure 2 we present the numerical solution of (86) for r = 0.2, and different values of β. The general effect of
exponential weighting is to reduce effectively the length of the sample, hence increasing β amounts to increasing the
ratio r = N/T , and results in broadening of the spectrum.

D.

Infinite and Non–Diagonal A

1.

Introduction

Before we restrict ourselves to a particular form of A, let us outline a method to handle infinite matrices that
(95)). Let us consider an

is particularly useful for matrices which are ”translationally invariant” (see below eq.

β=0

analytical
numerical

r=0.2

 2

r=0.2
β=5

β=10

β=3

β=1

 6

 4

 

)
λ
(
ρ
π
 2

 0

 4

)
λ
(
ρ
π

 

 0

 0

 1

 2

 3

 4

 5

 0

 1

 3

 4

 2
λ

λ

FIG. 2: Left: Spectral density for r = 0.2, different β. Right: Comparision to numerical calculations (r = 0.2, β = 5) with
N = 100, T = 500, α = 0.99, over 1000 samples.

infinite matrix C, i. e. with both indices, p, p′, ranging over Z. The trick is to perform the Fourier transformation
Z onto elements indexed by (continuous) real variables
mapping matrix elements indexed by integer indices p, p′
v, v′

π, π),

∈

[
−

∈

or conversely,

[C]vv′

ei(pv−p

′

′

v

)[C]pp′ ,

≡

Xp,p′∈Z

[C]pp′ =

1
4π2

π

π

−π Z
Z

−π

dvdv′e−i(pv−p

′

′

v

)[C]vv′ .

Note that if C is symmetric (or more generally Hermitian) then

The Kronecker’s delta, δpp′ , is transformed to the Dirac’s delta, 2πδ(v
to integration 1
2π

π
−π dv(. . .).

−

v′), and matrix multiplication is translated

Consider now a special class of C, namely the one where the matrix elements [C]pp′ depend only on the distance

R

between the indices, i. e. on (p

p′),

−

where c is some function, which in the case of symmetric (or more generally Hermitian) C satisfies cp = c−p. After
the Fourier transformation it becomes

[C]vv′ = 2πδ(v

v′)cv,

−

where

eipvcp.

cv ≡

Xp∈Z

In the Hermitian case cv is real.

We can safely extend the meaning of the notation 1

N Tr(. . .) for infinite matrices, since from (95) it follows that

[C]vv′ = [C]v′v.

[C]pp′ = cp−p′ ,

1
N

TrC = cp=0.

After this introduction, let us choose our infinite correlation matrix A or B to depend on the distance between
indices, as in (95). Such a dependence is artificial for the covariance matrix which describes correlations between
degrees of freedom, but it is natural for the matrix describing temporal correlations between measurements.

16

(92)

(93)

(94)

(95)

(96)

(97)

Our task is to compute the resolvent for A, GA(z) = 1
A) = 1N , i. e. z[ ˜GA(z)]ij −
z1N −A . This infinite matrix satisfies ˜GA(z)(z1N −
1
Fourier transformation, [ ˜GA(z)]vv′ (z
v′), so
av) = 2πδ(v
−

−

P

N Tr ˜GA(z), where we denote by tilde the matrix ˜GA(z)

≡
k∈Z[ ˜GA(z)]ikak−j = δij . By the

Transforming back we get

Since [ ˜GA(z)]jk depends also only on (j

k), we can use (97) to finally get

−

[ ˜GA(z)]vv′ =

v′)

.

2πδ(v
z

−

−
av

[ ˜GA(z)]jk =

dve−i(j−k)v

1
2π

π

−π

Z

1

z

−

.

av

GA(z) =

1
2π

π

−π

Z

dv

z

1

−

.

av

2. Example 4: ”Nearest–Neighbor” Correlations in A and B = 1T

For illustration, we choose A to have all diagonal elements equal to κ > 0 and all elements with indices differing by

= 1, equal to τ > 0. The nearest neighbors are correlated. Explicitly, ai=0 = κ, ai=1 = ai=−1 = τ , i. e.

1,

i
|

−

j

|

The condition κ > 2τ insures matrix positivity. From (96)

[A]ij = κδij + τ (δi,j+1 + δi,j−1) .

av = κ + 2τ cos v,

we can use (98) to calculate the resolvent for A, which requires just an elementary integration,

Having MA(z) we exploit (49) to write down an equation for M

MA(z); it is of 4th order (Ferrari’s type) in M ,

or equivalently,

GA(z) =

π

dv

1
2π

−π

Z

z

κ

−

−

1
2τ cos v

=

1
κ)2

,

4τ 2

−

(z

−

p

1.

z
κ)2

(z

−

4τ 2 −

−

MA(z) =

p

r2

κ2

4τ 2

M 4 + 2r

(r + 1)

κ2

4τ 2

κz

M 3+

(cid:1)

(cid:0)

(cid:0)

(cid:1)

2(2r + 1)κz +

r2 + 4r + 1

κ2

4τ 2

M 2+

(cid:0)
z2

+

−

−

≡

−

−

(cid:1)

−

(cid:0)

(cid:0)
(r + 2)κz + (r + 1)

(cid:1) (cid:0)
M + κ2

(cid:1)(cid:1)
4τ 2

−

−

2κz = 0.

κ2

4τ 2

−

(cid:0)

(cid:1)(cid:1)

+ 2

z2

−

(cid:0)

3. Example 5: A = 1N and Symmetrized Delay Correlations in B

The analysis of empirical data requires sometimes calculations of symmetrized delay correlation matrices [48] of the

type

sym

C

≡ C

+

τ ,

C

17

(98)

(99)

(100)

(101)

(102)

(103)

(104)

for a real positive constant τ and some integer d, representing the time lag. Although the matrix
is the one of
interest in finances, its asymmetry forces practitioners to consider a simpler object, i. e. the symmetrized version.
The symmetrized delay matrix

C

where

is the (non–symmetric) random matrix

C

corresponds to

so after Fourier transformation (96) we get

However this change can be checked to bring no difference to the integration (101), leading the same result,

Note that the dependence on the time lag d cancels out since the infinite size of the matrix mocks up correlations of
the nearest–neighbor type.

The only difference to the above nearest–neighbor case is that now it is the matrix B and not A that describes
nearest–neighbor correlations. So instead of (49) we should use (50), which leads again to a 4th order (Ferrari’s type)
equation for G

GA(z). For τ = 1/2 it reproduces exactly the quartic equation presented without proof in [48],

≡

r3z2G4

2r2(r

1)zG3

r

z2

−

−

−

(r

−

−

1)2

G2 + 2(r

1)zG + 2

r = 0.

−

−

(110)

4. Example 6: Exponentially Suppressed Correlations in A and B = 1T

(cid:0)

(cid:1)

Another case is given by ai = τ |i|, i. e.

Xk∈Z
Hence we have correlations at all distances, although with exponential suppression. The Fourier transformation (96)
leads to

The resolvent for A is given by an integral similar to the one considered in the previous example; the result reads

GA(z) =

1
z −

1
2 1+τ 2
1−τ 2 z + 1

=

1
z −

z√z2

−

1
2˜τ z + 1

z

z2

q

−

where

The corresponding moments’ generating function is

[
C

]ij ≡

τ
T

T

a=1
X

[Y ]ia[Y τ ]j,a+d,

sym =

Y BY τ ,

C

1
T

[B]ab = τ (δa,b+d + δa,b−d) ,

bv = 2τ cos(dv).

GB(z) =

√z2

4τ 2

.

1

−

[A]ij =

τ |k|δi,j+k.

τ 2
1
2τ cos v + τ 2 .

−

av =

1

−

˜τ

≡

1 + τ 2
τ 2 .
1

−

MA(z) =

1
2˜τ z + 1

.

−

√z2

−

18

(105)

(106)

(107)

(108)

(109)

(111)

(112)

(113)

(114)

(115)

We invert it functionally to get the N -transform of A,

Now we use (49) to write down an equation for M

MA(z); it is again a 4th order (Ferrari’s type) equation in M ,

r2 ˜τ 2M 4 + r

r + 2˜τ 2

r˜τ 2

2˜τ z

M 3 +

2r

2r˜τ 2

2˜τ z + z2

M 2 +

1

2r˜τ 2

M

1 = 0.

(117)

−

−

−

−

−

−

−

(cid:0)
This example is equivalent to the case solved recently by two of us using diagrammatics [15].

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

NA(z) = ˜τ +

˜τ 2

1 +

−

r

1
z2 .

≡
r2 + ˜τ 2

V. CORRELATED L´EVY–WISHART RANDOM MATRIX MODEL

A. Definition of Correlated L´evy–Wishart Random Matrix Model

So far we have discussed spectral properties of the matrix ˜
A

measure (9). The aim of this section is to extend the approach to the matrix

= 1

T Y BY τ A (34) where Y is drawn from the Gaussian

=

˜
A

1
T 2/α LBLτ A,

where L is now a random L´evy matrix. This is the L´evy–Wishart random matrix model with stability index α
(α = 2 is Gauss).

2

≤

For completeness, we now define the L´evy-Wishart measure. Two kinds of L´evy random matrices have been proposed
in the literature: one by Bouchaud and Cizeau [45] and one by some of us [46]. The former samples the matrix entries
from one–dimensional L´evy distributions, while the latter uses results from FRV [34]. Both constructions yield
similar spectra. The original sampling in [45] is not rotationally invariant. When rotational invariance is enforced,
both proposals are identical [47]. Since we are using FRV calculus, we follow in this section the proposal in [46].

Consider a square T

T rotationally invariant L´evy matrix, Λ, whose eigenvalue distribution is implicitly given by

its Blue’s function BΛ(z), which is known for stable free random variables [34] to be

×

BΛ(z) = a + bzα−1 +

for

α

= 1,

BΛ(z) = a

iγ (1 + β)

ln γz +

for

α = 1,

−

1
z

,

1
z

,

2βγ
π

−

with the same parameters as in classical theory, i. e.

∈
and a real shift parameter a. The complex number b is related to α, β and γ as follows,

∈

∈

stability index α

(0, 2),

skewness β

1, 1],

[
−

range γ

R+,

b

b

≡

≡

α
2 −

γ exp

iπ

1

(β + 1)

γ exp

iπ

1 +

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:17)
(β + 1)

(cid:17)

α
2

(cid:17)(cid:17)

,

,

for

for

1 < α < 2,

0 < α < 1.

The results (119) and (120) are valid for z in the upper half–plane,

The branch structure is chosen in such a way that the upper half–plane is mapped onto itself,

z

∈ H+ ≡ {

∈

z

C : Imz

0

.

}

≥

z

∈ H+
0

≤

}

⇒

BΛ(z)

∈ H+.

, the Blue’s function is defined by

BΛ(¯z) = BΛ(z).

On the lower half–plane

H− ≡ {

∈

z

C : Imz

19

(116)

(118)

(119)

(120)

(121)

(122)

(123)

(124)

(125)

(126)

6
It should be noted that since b is complex in general, the functional form of BΛ(z) changes in a non–trivial way under
complex conjugation. As expected, the GOE’s Blue’s function BΛ(z) = a + γz + 1
2.

z , is reproduced in the limit α

The case α = 1 is much more involved due to the logarithm in (120). Note however that if the skewness vanishes,

β = 0, also the logarithm term disappears and the Blue’s function simply reads

BΛ(z) = a

iγ +

−

1
z

,

for

α = 1, β = 0.

It defines an analogue of the Cauchy distribution for free random variables.

In this paper we will focus only on the case without skewness,

(cid:16)
We will also consider only centered distributions, i. e. with a = 0.

(cid:16)

β = 0,

i. e.

b = γ exp

iπ

α
2 −

1

.

(cid:17)(cid:17)

We can now construct the L´evy–Wishart model from the ensemble of square free random matrices Λ. The idea is

to parallel the construction which we used in III E to construct a Wishart ensemble from a GOE square matrix.

Assume N

T , take a T

T L´evy random matrix Λ and define the T

T L´evy–Wishart random matrix as

≤

×

×

1
T 2/α Lτ L

≡

ΛP Λ,

where P is a projector (52), P = diag(1N , 0T −N ). Since we know the Blue’s function for Λ, we can use the same
rules (53) and the composition rule for the product of free random variables based on the S–transform to calculate
the resolvent for the matrix on the LHS of (129) and then for (118).

Since the same procedure as in III E can be used we will just quote the result, for M 1

T 2/α Lτ L(z),

ei 2π

α zM 1

T 2/α Lτ L(z)

−

2
α = (1 + M 1

T 2/α Lτ L(z))

r + M 1

T 2/α Lτ L(z)
(cid:17)

.

(cid:16)

We may rewrite it as an explicit formula for N 1

T 2/α Lτ L(z),

Note that for α = 2 we recover precisely the Wishart relation (41). This is the L´evy analogue of the uncorrelated
Wishart ensemble, announced and used by us some time ago [18]. This part fills in for the missing proof.

Consider the problem discussed in III C, but now for L´evy–Wishart ensembles. A short inspection of the derivation
in III C shows that it remains largely the same with the exception of formula (41), which should be replaced by (131).
The results are

N 1

T 2/α Lτ L(z) =

−

(1 + z)(r + z)

.

ei 2π

α z

2
α

B. Main Result

MA(z) = MA(w(z)),

rMA(w(z)) = MB

ei 2π
α r

2

α −2 zMA(w(z))
w(z)

 −

2
α −2

,

!

which generalize (44) and (45). For α = 2 they reduce back to them. These are the fundamental equations that give the
resolvent for the correlated L´evy–Wishart random matrix (118), therefore, via the discontinuities, the corresponding
spectral distributions. Note that this construction is possible even though most of the moments do not exist.

Again, we can solve the two equations by eliminating w(z) which leads to an analogue of (48),

MA(z) = MA

ei 2π
α r

2

α −2 zMA(z)

2
α −2

.

NB(rMA(z)) !

 −

20

→

(127)

(128)

(129)

(130)

(131)

(132)

(133)

(134)

In particular, for A arbitrary and B = 1T we have

whereas for A = 1N and B arbitrary,

MA(z) = MA

ei 2π
α r

2

α −1 zMA(z)

2
α −1

1 + rMA(z) !

 −

rMA(z) = MB

ei 2π
α r

2

α −2 zMA(z)

2
α −1

1 + MA(z) !

 −

,

.

≤

VI. EXAMPLES

These formulae are extensions of (48), (49) and (50) to the case of general α

2.

A. Example 1: A Diagonal with Macroscopic Degeneracies and B = 1T

Let us start with an extension of example IV B to the L´evy–Wishart case, i. e. B = 1T and A is diagonal with

macroscopic degeneracies, i. e.

MA(z) =

K

Xl=1

plκl
κl
z

.

−

A relevant equation is (135), which gives

K

MA(z) =

ei 2π
α r

Xl=1

−

2

plκl (1 + rMA(z))
2
α −1
κl −

−

α −1zMA(z)

.

rκlMA(z)

For instance, for K = 2 (two different eigenvalues in A) and α = 1/2,

r6z2M 7 + (κ1 + κ2)r4zM 5 + (κ1(p1r + 1) + κ2(p2r + 1)) zr3M 4+

+ (zp1κ1r + zp2κ2r + κ1κ2) r2M 3 + ((p1 + p2)r + 2) κ1κ2rM 2+

which is a 7th order equation for M

MA(z).

≡

+ (2(p1 + p2)r + 1) κ1κ2M + (p1 + p2)κ1κ2 = 0,

(139)

B. Example 2: Exponentially Weighted Moving Average

In the next example let us generalize the EWMA case IV C to the L´evy–Wishart case, i. e.

A relevant equation is now (136), which gives an entangled equation for M

ME (z),

NE(z) = β

eβ(1+z)

−
1) (eβz

(eβ

−

1

−

.

1)

2
α −1

zM
1 + M

=

−

e−i 2π

α r2− 2

α β

eβ(1+rM)

−
1) (eβrM

(eβ

−

≡

1

−

.

1)

21

(135)

(136)

(137)

(138)

(140)

(141)

C. Example 3: ”Nearest–Neighbor” Correlations in A and B = 1T

Let us extend example IV D 2 of B = 1T and nearest–neighbor correlations in A, i. e.

The result (135) gives for M

MA(z),

≡
α −1 + 2κei 2π
α r

4

2

4

ei 4π
α r

α −2z2(2 + M )M

α −1z(1 + M )2(1 + rM )M

α −1 + (κ2

4τ 2)(1 + M )2(1 + rM )2 = 0.

(143)

2

D. Example 4: Exponentially Suppressed Correlations in A and B = 1T

Let us also generalize example IV D 4 of B = 1T and exponentially suppressed correlations in A, i. e.

The result (135) leads to

ei 4π
α r

4

α −2z2M

4

α + 2˜τ ei 2π
α r

2

α −1z(1 + rM )M

α +1 + (M 2

2

1)(1 + rM )2 = 0.

MA(z) =

z
κ)2

(z

−

4τ 2 −

−

1.

p

−

−

NA(z) = ˜τ +

˜τ 2

1 +

−

r

1
z2 .

VII. CONCLUSIONS

22

(142)

(144)

(145)

In the first part of this paper, we rederived several known results for correlated Wishart models using the FRV
calculus. The derivation is entirely algebraic thereby by–passing many of the subtleties of standard Gaussian random
matrix theory. In the second part of our work we have shown how this calculus extends naturally and succinctly to
the L´evy random matrix case, an issue of considerable importance in finances. We do not know of any other approach
that allows for such a derivation. Our results are mostly new. Indeed, a general formulae for exponentially weighted
moving average for Gaussian and L´evy distributed returns, and a full set of correlations for the case of heavy–tailed
data were derived.

The absence of finite moments for L´evy distributions invalidates most of the standard tools of random matrix theory,
in particular the large N diagrammatics. The current formalism is very general and exportable to all known probability
distributions as we have shown.
In this way we have laid theoretical foundations for techniques of multivariate
statistical analysis of financial covariances based on solely on straightforward FRV calculus. Further applications and
numerical results of relevance to risk management in finances will be presented elsewhere.

Acknowledgements

This work was partially supported by Polish Ministry of Science and Information Society Technologies grants
2P03B-08225 (2003–2006) and 1P03B-04029 (2005–2008) and Marie Curie Host Fellowship MTKD-CT-2004-517186
(COCOS) ”Correlations in Complex Systems”. AJ acknowledges the support of the European Network of Random
Geometry (ENRAGE) MRTN-CT-2004-005616. The work of GP was partially supported by the National Office for
Research and Technology grant RET14/2005 (GP), while the work of IZ was partially supported by the US-DOE
grants DE-FG02-88ER40388 and DE-FG03-97ER4014.

[1] J. P. Bouchaud, P. Cizeau, L. Laloux and M. Potters, Phys. Rev. Lett. 83 (1999) 1467.
[2] V. Plerou, P. Gopikrishnan, B. Rosenov, L. N. Amaral and H. E. Stanley, Phys. Rev. Lett. 83 (1999) 1471.
[3] J. P. Bouchaud, P. Cizeau, L. Laloux and M. Potters, Int. J. Theor. App. Finance 3 (2000) 391.

23

[4] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. N. Amaral, T. Guhr and H. E. Stanley, Phys. Rev. E 65 (2002) 066126.
[5] S. Dro˙zd˙z, J. Kwapie´n, F. Grummer, E. Ruf and J. Speth, Physica A 299 (2001) 144.
[6] F. Lillo and R. Mantegna, cond-mat/0305546.
[7] P. Repetowicz and P. Richmond, math-ph/0411020.
[8] A. Utsugi, K. Ino and M. Oshikawa, cond-mat/0312643.
[9] S. Pafka and I. Kondor, Physica A 319 (2003) 487; Physica A 343 (2004) 623.
[10] G. Papp, S. Pafka, M.A. Nowak and I. Kondor, Acta Phys. Polon. B 36 (2005) 2757.
[11] T. Guhr and B. K¨alber, J. Phys. A 36 (2003) 3009.
[12] Y. Malevergna and D. Sornette, Physica A 331 (2004) 660.
[13] Z. Burda, A. G¨orlich, A. Jarosz and J. Jurkiewicz, Physica A 343 (2004) 295.
[14] Z. Burda and J. Jurkiewicz, Physica A 344 (2004) 67.
[15] Z. Burda, J. Jurkiewicz and B. Wac law, Phys. Rev. E 71 (2005) 026111.
[16] D. V. Voiculescu, K. J. Dykema and A. Nica, Free Random Variables, CRM Monograph Series, Vol.1, Am. Math. Soc.,

Providence, 1992.

[17] R. Speicher, Math. Ann. 298 (1994) 611
[18] Z. Burda, J. Jurkiewicz, M.A. Nowak, G. Papp and I. Zahed, Physica A 299 (2001) 181; Physica A 343 (2004) 694.
[19] Z. Burda, J. Jurkiewicz, M. A. Nowak, Acta Phys. Polon. B 34 (2003) 87.
[20] M. A. Nowak talks at: ”Exystence”, Budapest, June 2004; ”Noise in Condensed Matter and Complex Systems”, Citta del
Mare, July 2004 and ”Applications of Random Matrices to Economy and Other Complex Systems”, Krak´ow, May 2005
(unpublished).

[21] M. Potters, J. P. Bouchaud and L. Laloux, Acta Phys. Polon. B 36 (2005) 2767.
[22] J. P. Bouchaud, L. Laloux, M. Augusta Miceli and M. Potters, physics/0512090.
[23] P. Neu and R. Speicher, Z. Phys. B 95 (1994) 19.
[24] R. Gopakumar and D. Gross, Nucl. Phys. B 451 (1995) 379.
[25] R. A. Janik, M.A. Nowak, G. Papp and I. Zahed, Acta Phys. Polon. B 28 (1997) 2949 and references therein.
[26] D. N. Tse and S. V. Hanly, IEEE Trans. Inf. Theor. 45 (1999) 645.
[27] R. M¨uller, IEEE Trans. Inf. Theor. 48 (2002) 2495.
[28] A. Tulino and S. Verd´u, Found. and Trends in Comm. and Inf. Theory 1 (2004) 1.
[29] S. H. Simon and A. L. Moustakas, Phys. Rev. E 69(2004) 065101(R).
[30] W. Fellers, Introduction to probability calculus, Wiley, New York, 1960.
[31] P. Cvitanovic, P. G. Lauwers and P. N. Scharbach, Nucl.Phys. B 203 (1982) 385.
[32] G. ’tHooft, Nucl. Phys. B 72 (1974) 461.
[33] H. Bercovici and V. Pata, Ann. of Math. 149 (1999) 1023, appendix by P. Biane.
[34] H. Bercovici and D. V. Voiculescu, Ind. Univ. Math. J 42 (1993)733.
[35] E. Wigner, Ann. Math. 62 (1955) 548
[36] G. Ben Arous and D. V. Voiculescu, math/0501274.
[37] P. Biane and R. Speicher, Ann. Inst. H. Poincar´e PR 37 (2001) 581.
[38] R. A. Janik and W. Wieczorek, J. Phys. A: Math. Gen. 37 (2004) 6521
[39] E. Gudowska-Nowak, R. A. Janik, J. Jurkiewicz and M. A. Nowak, New Jour. of Phys. 7 (2005) 54.
[40] A. Zee, Nucl. Phys. B 474 (1996) 726.
[41] V. A. Marchenko and L. A. Pastur, Math. USSR-Sb 1 (1967) 457.
[42] J. W. Silverstein and Z. D. Bai, Journal of Multivariate Analysis 54 (1995) 175.
[43] JP Morgan staff (1995) RiscMetricsTM Technical Document, Third Edition.
[44] S. Pafka, I. Kondor and M. Potters, cond-mat/0402573.
[45] P. Cizeau and J. P. Bouchaud, Phys. Rev. E 50 (1994) 1810.
[46] Z. Burda, R. A. Janik, J. Jurkiewicz, M. A. Nowak, G. Papp and I. Zahed, Phys. Rev. E 65 (2002) 021106.
[47] Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp and I. Zahed, cond-mat/0602087.
[48] K. B. K. Mayya and R. E. Amritkar, cond-mat/0601279.


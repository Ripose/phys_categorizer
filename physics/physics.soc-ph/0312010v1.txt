3
0
0
2
 
c
e
D
 
2
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
0
1
0
2
1
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Self-organization in a simple model of adaptive agents playing 2

2 games

with arbitrary payoﬀ matrices

×

H. Fort1 and S. Viola2
1Instituto de F´ısica, Facultad de Ciencias, Universidad de la Rep´ublica, Igu´a 4225, 11400 Montevideo, Uruguay
2Instituto de F´ısica, Facultad de Ingenier´ıa, Universidad de la Rep´ublica,
Julio Herrera y Reissig 565, 11300 Montevideo, Uruguay

We analyze, both analytically and numerically, the self-organization of a system of ”selﬁsh” adap-
tive agents playing an arbitrary iterated pairwise game ( deﬁned by a 2×2 payoﬀ matrix). Examples
of possible games to play are: the Prisoner’s Dilemma (PD) game, the chicken game, the hero game,
etc. The agents have no memory, use strategies not based on direct reciprocity nor ’tags’ and are
chosen at random i.e. geographical vicinity is neglected. They can play two possible strategies:
cooperate (C) or defect (D). The players measure their success by comparing their utilities with an
estimate for the expected beneﬁts and update their strategy following a simple rule.

Two versions of the model are studied: 1) the deterministic version (the agents are either in
deﬁnite states C or D) and 2) the stochastic version (the agents have a probability c of playing C).
Using a general Master Equation we compute the equilibrium states into which the system self-
organizes, characterized by their average probability of cooperation ceq. Depending on the payoﬀ
matrix, we show that ceq can take ﬁve diﬀerent values.

We also consider the mixing of agents using two diﬀerent payoﬀ matrices an show that any value
of ceq can be reached by tunning the proportions of agents using each payoﬀ matrix. In particular,
this can be used as a way to simulate the eﬀect a fraction d of ”antisocial” individuals -incapable
of realizing any value to cooperation- on the cooperative regime hold by a population of neutral or
”normal” agents.

PACS numbers: 89.75.-k, 87.23.Ge, 89.65.Gh, 89.75.Fb

I.

INTRODUCTION

Complex systems pervade our daily life. They are diﬃcult to study because they don’t exhibit simple

cause-and-eﬀect relationships and their interconnections are not easy to disentangle.

Game Theory has demonstrated to be a very ﬂexible tool to study complex systems. It coalesced
in its normal form[1] during the second World War with the work of Von Neumann and Morgenstern [2]
who ﬁrst applied it in Economics.

Later, in the seventies, it was the turn of Biology mainly with the work of J. Maynard-Smith [3],
who shown that the Game Theory can be applied to various problems of evolution, and proposed the
concept of Evolutionary Stable Strategy (ESS), as an important concept for understanding biological
phenomena. Following rules dictated by game theory to attain an ESS requires neither consciousness
nor a brain. Moreover, a recent experiment found that two variants of a RNA virus seem to engage in
two-player games [4].

This opens a new perspective, perhaps the dynamic of very simple agents, of the kind we know
in Physics, can be modeled by Game Theory providing an alternative approach to physical problems.
For instance, energies could be represented as payoﬀs and phenomena like phase transitions understood
as many-agents games. As a particular application of this line of thought we have seen recently a
proliferation of papers addressing the issue of quantum games [5]- [8] which might shed light on the hot
issue of quantum computing. Conversely, Physics can be useful to understand the behavior of adaptive
agents playing games used to model several complex systems in nature. For instance, in some interesting

works Szab´o et al [9],[10] applied the sophisticated techniques developed in non-equilibrium statistical
physics to spatial evolutionary games.

The most popular exponent of Game Theory is the Prisoner’s Dilemma (PD) game introduced in
the early ﬁfties by M. Flood and M. Dresher [11] to model the social behavior of ”selﬁsh” individuals -
individuals which pursue exclusively their own self-beneﬁt.

The PD game is an example of a 2

2 game in normal form: i) there are 2 players, each confronting
2 choices - to cooperate (C) or to defect (D)-, ii) with a 2
2 matrix specifying the payoﬀs of each player
for the 4 possible outcomes: [C,C],[C,D],[D,C] and [D,D][20] and iii) each player makes his choice without
knowing what the other will do. A player who plays C gets the ”reward” R or the ”sucker’s payoﬀ” S
depending if the other player plays C or D respectively, while if he plays D he gets the ”temptation to
defect” T or the ”punishment” P depending if the other player plays C or D respectively. These four
payoﬀs obey the relations:

×

×

and

T > R > P > S,

2R > S + T.

Thus independently of what the other player does, by (1), defection D yields a higher payoﬀ than coop-
eration C (T > R and P > S) and is the dominant strategy. The outcome [D,D] is thus called a Nash
equilibrium [12]. The dilemma is that if both defect, both do worse than if both had cooperated (P < R).
Condition (2) is required in order that the average utilities for each agent of a cooperative pair (R) are
greater than the average utilities for a pair exploitative-exploiter ((T + S)/2).

Changing the rank order of the payoﬀs - the inequalities (1)- gives rise to diﬀerent games. A general
2 games (one-shot games involving two players with two actions each) was constructed
2 game is deﬁned by a payoﬀ matrix MRST P with payoﬀs

taxonomy of 2
by Rapoport and Guyer [13]. A general 2
not necessarily obeying the conditions (1) or (2)[21]

×

×

MRST P =

(R, R)
(T, S)

(cid:18)

(S, T )
(P, P ) (cid:19)

.

The payoﬀ matrix gives the payoﬀs for row actions when confronting with column actions.

Apart from the PD game there are other some well studied games. For instance, when the damage
from mutual defection in the PD is increased so that it ﬁnally exceeds the damage suﬀered by being
exploited:

the new game is called the chicken game. Chicken is named after the car racing game. Two cars
drive towards each other for an apparent head-on collision. Each player can swerve to avoid the crash
(cooperate) or keep going (defect). This game applies thus to situations such that mutual defection is
the worst possible outcome (hence an unstable equilibrium).

When the reward of mutual cooperation in the chicken game is decreased so that it ﬁnally drops

below the losses from being exploited:

T > R > S > P,

T > S > R > P,

it transforms into the leader game. The name of the game stems from the following every day life situation:
Two car drivers want to enter a crowded one-way road from opposite sides, if a small gap occurs in the
line of the passing cars, it is preferable that one of them take the lead and enter into the gap instead of
that both wait until a large gap occurs and allows both to enter simultaneously.

2

(1)

(2)

(3)

(4)

(5)

3

In fact, every payoﬀ matrix, which at a ﬁrst glance could seem unreasonable from the point of view
of selﬁsh individuals, can be applicable to describe real life situations in diﬀerent realms or contexts.
Furthermore, ”unreasonable” payoﬀ matrices can be used by minorities of individuals which depart from
the ”normal” ones (assumed to be neutral) for instance, absolutely D individuals incapable of realizing
any value to cooperation or absolutely C ”altruistic” individuals (more on this later).

In one-shot or non repeated games, where each player has a dominant strategy, as in the PD, then
generally these strategies will be chosen. The situation becomes more interesting when the games are
In these iterated games players can modify their behavior with time in order to
played repeatedly.
maximize their utilities as they play i.e. they can adopt diﬀerent strategies. In order to escape from
the non-cooperative Nash equilibrium state of social dilemmas it is generally assumed either memory of
previous interactions [14] or features (”tags”) permitting cooperators and defectors to distinguish one
another [15]; or spatial structure is required [16].

Recently, it was proposed [17] a simple model of selﬁsh agents without memory of past encounters,
without tags and with no spatial structure playing an arbitrary 2
2 game, deﬁned by a general payoﬀ
matrix like (3). At a given time t, each of the Nag agents, numbered by an index i, has a probability
ci(t) of playing C (1
ci(t) of playing D). Then a pair of agents are selected at random to play. All the
players use the same measure of success to evaluate if they did well or badly in the game which is based
on a comparison of their utilities U with an estimate of the expected income ǫ and the arithmetic mean
of payoﬀs µ
(R + S + T + P )/4. Next, they update their ci(t) in consonance, i.e. a player keeps his
ci(t) if he did well or modiﬁes it if he did badly.

×

−

≡

Our long term goal is to study the quantum and statistical versions of this model. That is, on one
hand to compare the eﬃciency and properties of quantum strategies vs. the classical ones for this model
in a spirit similar to that of ref. [5]. On the other hand, we are also interested in the eﬀect of noise, for
instance by introducing a Metropolis Monte-Carlo temperature, and the existence of power laws in the
space of payoﬀs that parameterize the game, of the type found in ref. [9] and [10], for a spatial structured
version of this model. Before embarking on the quantum or statistical mechanics of this model, the
objective in this paper is to complete the study of the simplest non-spatial M-F version. In particular, to
present an analytic derivation of the equilibrium states for any payoﬀ matrix i.e. for an arbitrary 2
2
game using elemental calculus, both for the deterministic and stochastic versions. In the ﬁrst case the
calculation is elementary and serves as a guide to the more subtle computation of the stochastic model.
These equilibrium states into which the systems self-organizes, which depend on the payoﬀ matrix, are
of three types: ”universal cooperation” or ”all C”, of intermediate level of cooperation and ”universal
defection” or ”all D” with, respectively, ceq = 1.0, 0 < ceq < 1.0 and 0.0. We also consider the eﬀect of
mixing players using two diﬀerent payoﬀ matrices. Speciﬁcally, a payoﬀ matrix producing ceq=0.0 and
the canonical payoﬀ matrix are used to simulate, respectively, absolutely D or ”antisocial” agents and
”normal” agents.

×

II. THE MODEL

We consider two versions of the model introduced in ref.

[17]. First, a deterministic version, in
which the agents are always in deﬁnite states either C or D i.e. ”black and white” agents without
”gray tones”. Nevertheless, it is often remarked that this is clearly an over-simpliﬁcation of the behavior
of individuals.Indeed, their levels of cooperation exhibit a continuous gamma of values. Furthermore,
completely deterministic algorithms fail to incorporate the stochastic component of human behavior.
Thus, we consider also a stochastic version, in which the agents only have probabilities for playing C. In
other words, the variable ci, denoting the state or ”behavior” of the agents, for the deterministic case
takes only two values: ci = 1 (C) or 0 (D) while for the stochastic case ci is a real variable

[0, 1].

The pairs of players are chosen randomly instead of being restricted to some neighborhood. The

∈

implicit assumptions behind this are that the population is suﬃciently large and the system connectivity is
high. In other words, the agents display high mobility or they can experiment interactions at a distance
(for example electronic transactions, etc.). This implies that Nag the number of agents needs to be
reasonably large. For instance, in the simulations presented in this work the population of agents will be
ﬁxed to Nag = 1000.

The update rule for the ck of the agents is based on comparison of their utilities with an estimate.
The simplest estimate ǫk that agent number k for his expected utilities in the game is provided by the
utilities he would made by playing with himself [22], that is:

ǫRST P
k

(t) = (R

S

−

−

T + P )ck(t)2 + (S + T

2P )ck(t) + P,

−

where ck is the probability that in the game the agent k plays C. From equation (6) we see that the
estimate for C-agents (ck = 1) ǫC and D-agents (ck = 0) ǫD are given by

ǫC = R,

ǫD = P.

[17]:
The measure of success we consider here is slightly diﬀerent from the one considered in ref.
To measure his success each player compares his proﬁt Uk(t) with the maximum between his estimate
ǫk(t), given by (6), and the arithmetic mean of the four payoﬀs given by µ
(R + S + T + P )/4 [23]. If
U RST P
the player assumes he is doing well (badly) and he keeps (changes) his
ck(t) as follows: if player k did well he assumes his ck(t) is adequate and he keeps it. On the other hand,
if he did badly he assumes his ck is inadequate and he changes it (from C to D or from D to C in the
deterministic version).

(<) max
{

ǫRST P
k

, µ
}

(t)

≡

≥

k

We are interested in measuring the average probability of cooperation c vs. time, and in particular
in its value of equilibrium ceq, after a transient which is equivalent to the ﬁnal fraction of C-agents fC.

III. COMPUTATION OF THE EQUILIBRIUM STATES

A. Deterministic version

For the deterministic case the values of ceq are obtained by elementary calculus as follows. Once
equilibrium has been reached, the transitions from D to C, on average, must equal those from C to D.
Thus, the average probability of cooperation ceq is obtained by equalizing the ﬂux from C to D, JCD, to
the ﬂux from D to C, JDC. The players who play C either they get R (in [C,C] encounters) or S (in [C,D]
encounters), and their estimate is ǫC = R; thus, according to the update rule, they change to D if R < µ
respectively. For a given average probability of cooperation c, [C,C] encounters occur
or S < max
with probability c2 and [C,D] encounters with probability c(1
c). Consequently, JCD can be written as:

R, µ
}

{

JCD

aCC c2 + aCDc(1

c),

∝

−

−

with

where θ(x) is the step function given by:

aCC = θ(µ

R)

and

aCD = θ(max

R, µ

S),

{

} −

−

θ(x) =

1
0

if x
0
if x < 0

≥

4

(6)

(7)

(8)

(9)

(10)

On the other hand, the players who play D either they get T (in [D,C] encounters) or P (in [D,D]
encounters) and their estimate is ǫD = P ; thus, according to the update rule, they change to C if
c)c and [D,D]
or P < µ respectively. As [D,C] encounters occur with probability (1
T < max
}
encounters with probability (1

c)2, JCD can be written as:

µ, P

−

{

−

JDC

aDC (1

c)c + aDD(1

∝

−

c)2,

−

with

In equilibrium

aDD = θ(µ

P )

and

aDC = θ(max

P, µ

T ).

{

} −

−

and thus we get a set of second order algebraic equations for ceq:

JCD(ceq) = JDC (ceq),

(aCC

aCD + aDC

aDD)c2

eq + (aCD

aDC + 2aDD)ceq

aDD = 0.

−

−

−

−

As there are 2 possibilities for each coeﬃcient aXY , we have a total of 24 = 16 diﬀerent equations
governing all the possible equilibrium states (actually there are 15 since this includes the trivial equation
0

0). The roots[24] of these equations are:

≡

In addition, we have to take into account the case when:

0

3

√5
−
2
1/2
√5
−
2

1

1

aCC = aDD = 0
aCD = aDC = 1.

In this case we can see from (8) and (11) that JCD
co the initial mean probability), whatever the initial conditions are.

≡

JDC identically, so we have that peq

co, (being

≡

For instance, for the canonical payoﬀ matrix we have aCC = 0 = aDC and aCD = 1 = aDD, therefore

we get

ceq(1

ceq) = (1

−

ceq)2,

−

with the root ceq = 1/2 corresponding to the stable dynamic equilibrium in which the agents change their
state in such a way that, on average, half of the transitions are from C to D and the other half from D
to C.

5

(11)

(12)

(13)

(14)

(15)

(16)

(17)

B.

Stochastic version

In the case of a continuous probability of cooperation ck , the calculation is a little bit more subtle:
now the estimate ǫk for the agent k is not only R or P, as it happened in the discrete case, but it can
take a continuum of values as the probability ck varies in the interval [0,1]. From now on we will use the
estimate as given in (6), but instead of a ǫk as a function of time we will use a generic ǫ that is a function
of the cooperation probability (and implicitly of time, oﬀ course), that is:

ǫRST P (c) = (R

T + P )c(t)2 + (S + T

2P )c(t) + P.

S

−

−

−

So we have:

ǫRST P
k

(t) = ǫRST P (ck(t)).

To calculate ceq we begin by writing a balance equation for the probability ci(t). The agents will follow
the same rule as before: they will keep their state if they are doing well (in the sense explained earlier)
and otherwise they will change it. If two agents i and j play at time t, with probabilities ci(t) and cj(t)
respectively, then the change in the probability ci, provided he knows cj(t), would be given by:

ci(t + 1)

ci(t) =

ci(t)cj(t) [1

θ(R

−

−
cj(t)] [1

−
θ(S

−

−

ǫRST P (ci(t)) θ(R

−

µ)]
ǫRST P (ci(t)) θ(S
ǫRST P (ci(t)) θ(T

ci(t)]cj(t) [1

θ(T

ci(t)[1

−

µ)]

−

−

ci(t)][1

−
cj(t)] [1

−
θ(P

µ)]
ǫRST P (ci(t)) θ(P

−

µ)],

−

−
+[1

+[1

−
being θ the step function. The equation of evolution for cj(t) is obtained by simply exchanging i
j
in equation (20). Certainly, the assumption that each agent knows the probability of cooperation of
his opponent is not realistic. Later, when we perform the simulations, we will introduce a procedure to
estimate the opponent’s probability (more on this in Section V.b)

←→

−

−

−

−

In (20) if at time t the payoﬀ obtained by agent i, X ( = R, S, T or P ) is less than
ǫRST P (ci(t)), µ
max
the ﬁrst two terms in the RHS decrease the cooperation probability of
}
agent i, while the two last terms increase it. The terms give no contribution if the payoﬀ X is greater or
equal than max
{

ǫRST P (ci(t)), µ
}

{

.

,

We will use the canonical payoﬀ matrix M 3051 to illustrate how the above equation of evolution for

ci(t) works. In this case, the estimate function is, by (18):

thus it is easy to see that:

ǫ3051(c) =

c2 + 3c + 1 ,

−

θ(3
θ(0
θ(5
θ(1

−
−
−
−

ǫ3051(c)) = 1
ǫ3051(c)) = 0
ǫ3051(c)) = 1
ǫ3051(c)) = 0

c
c
c
c

∀
∀
∀
∀

∈
∈
∈
∈

[0, 1]
[0, 1]
[0, 1]
(0, 1].

6

(18)

(19)

(20)

(21)

(22)

In addition we have for this case µ = 2, 25, thus:

θ(3
θ(0
θ(5
θ(1

−
−
−
−

µ) = 1
µ) = 0
µ) = 1
µ) = 0.

c =

1
Nag

Nag

Xi=1

ci,

∀

−

0 = 1

3c(t) + c(t)2,

ceq =

1
1/2

[µ, ǫRST P
max ]

X /
∈

µ > ǫRST P

max

We can then write, to a very good approximation (we are assuming that the last line of (22) is valid for
c = 0 also):

ci(t + 1)

ci(t) =

ci(t)[1

−

−
= [1

−

−

cj(t)][1

cj(t)] + [1
2ci(t)].

−

−

ci(t)][1

cj(t)]

−

= j

i
∀

Deﬁning the mean probability of cooperation as

summing eq. (24) over i and j leads to:

c(t + 1)

−

c(t) = [1
= 1

2c(t)]

c(t)][1
3c(t) + c(t)2,

−

−
−

within an error of O(1/Nag) since (24) is valid

i

= j but we are summing over all the Nag agents.

Thereof we can calculate the equilibrium mean probability of cooperation ceq:

obtaining the two roots:

being ceq = 1/2 the stable solution. Hence we obtain the same result that in the deterministic case

Using analog reasoning for the general case, we can conclude that if

or

if

the results for the mean cooperation probability for the deterministic version and the stochastic version,
are the same.

There is an easy way to evaluate ǫRST P

max

in practice. It can be seen -see appendix- that

S + T > 2 max
{

R, P

} ⇒

ǫRST P
max = P

1
4

(S+T
S
(R

−

−
−

−

2

2P )
T +P )

7

(23)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

6
6
8

(32)

(33)

while, if

S + T

2 max
{

≤

R, P

} ⇒

ǫRST P
max = max

R, P

.

{

}

When there is a payoﬀ X such that

[µ, ǫRST P
max ]

X

∈

things can change because agents who get X update in general their probability of cooperation ci(t)
ǫ(ci). So as the probability takes diﬀerent values in the
diﬀerently depending whether X < ǫ(ci) or X
interval [0, 1], we have diﬀerent equations of evolution, which somehow ”compete” against each other in
order to reach the equilibrium. The diﬀerent equations that can appear are oﬀ course restricted to the
ones generated by the coeﬃcients aXY as they appear in (14). It is reasonable to expect then that the
ﬁnal equilibrium value for the mean probability will be somewhere in between the original equilibrium
values for the equations competing. We will analyze some particular cases of this type in Section V.b
to illustrate this point.

≥

Although at ﬁrst sight one may think that the universe of possibilities fulﬁlling condition (33) is
very vast, it happens that no more than three diﬀerent balance equations can coexist. This can be seen
as follows: from eqs. (31) and (32), ǫRST P
, and besides we know that the estimate never
max ≥
could be greater than all the payoﬀs, so there is at least one X such that ǫRST P
max < X. So this leaves
us with only two payoﬀs that eﬀectively can be between µ and ǫRST P
max , and this results in at most three
balance equations playing in a given game.

max

R, P

}

{

IV. AN EXAMPLE OF COEXISTENCE OF AGENTS USING DIFFERENT PAYOFF
MATRICES: COOPERATION IN PRESENCE OF ”ALWAYS D” AGENTS

Let us analyze now the situation where there are a mixing of agents using two diﬀerent payoﬀ
matrices, each leading by separate to a diﬀerent value of ceq. For simplicity we consider the deterministic
version but the results for the stochastic version are similar. We call ”antisocial” individuals those for
whom cooperation never pays and thus, although they can initially be in the C state, after playing they
turn to state D and remain forever in this state. They can be represented by players using a payoﬀ matrix
that always update ci to 0; for instance M1053. Notice that these individuals are basically diﬀerent from
those which use a payoﬀ matrix fulﬁlling conditions (1) and (2) who, even though they realize the value
of cooperation i.e. R > P and 2R > T + S, often may be tempted to ”free ride” in order to get a higher
payoﬀ. However, with the proposed mechanism -which implies a sort of indirect reciprocity- when D
grows above 50 % it punishes, on average, this behavior more than C favoring thus a net ﬂux from D
to C. Conversely, if C grows above 50 % it punish, on average, this behavior more than D favoring thus
the opposite ﬂux from C to D. In other words, small oscillations around fC = 0.5 occur. On the other
hand, agents using M 1053 are ”immune” to the former regulating mechanism. Let us analyze the eﬀect
they have on cooperation when they ”contaminate” a population of neutral agents (using the canonical
payoﬀ matrix). In short, the two types of individuals play diﬀerent games (speciﬁed by diﬀerent payoﬀ
matrices) without knowing this fact, a situation which does not seem too far from real life.

The asymptotic average probabilities of cooperation can be obtained by simple algebra combining
the update rules for M3051 and M1053. The computation is completely analogous the one which leads to
(17). We have to calculate JDC and JCD as a function of the variable c and the parameter d and by

equalizing them at equilibrium we get the equation for ceq. To JDC only contribute the fraction (1-d) of
normal players using the canonical payoﬀ matrix who play D against a player who also plays D (normal
or antisocial). That is, JDC is given by

JDC

(1

d)(1

c)2.

∝
On the other hand, contributions to JCD come from one of these 3 types of encounters:
i) [C,D] no
matter if agents are neutral or antisocial, ii) [C,C] of two antisocial agents and iii) [C,C] of a neutral and
antisocial agent (the neutral agent remains C and the antisocial, who started at t = 0 playing C and has
c), d2c2
not played yet, changes from C to D). The respective weights of these 3 contributions are: c(1
and 1

d)c2. Therefore, JCD is given by

−

−

−

2 2d(1

−

−
In equilibrium JDC = JCD and the following equation for ceq arises:

−

−

∝

JCD

c(1

c) + d2c2 + d(1

d)c2 = c(1

c) + dc2.

and solving it:

(1

−

d)(2c2

eq −

2ceq + 1) + ceq = 0,

3

2d

ceq =

−

4d2 + 4d + 1

.

±

√
−
4(1

d)

−

We must take the roots with the ”-” sign because those with ”+” are greater than 1 for non null values
of d. We thus get the following table for ceq for diﬀerent values of the parameter d:

9

(34)

(35)

(36)

(37)

ceq (d=0.0) = 0.5000
ceq (d=0.1) = 0.4538
ceq (d=0.2) = 0.4123
ceq (d=0.3) = 0.3727
ceq (d=0.4) = 0.3333
ceq (d=0.5) = 0.2929
ceq (d=0.6) = 0.2500
ceq (d=0.7) = 0.2029
ceq (d=0.8) = 0.1492
ceq (d=0.9) = 0.0845
ceq (d=1.0) = 0.0

V. SIMULATIONS

A. Deterministic version

Table 1. ceq for agents using M3051 contaminated by a fraction d of antisocial agents using M1053.

In this subsection we present some results produced by simulation for the deterministic version.
Diﬀerent payoﬀ matrices were simulated and it was found that the system self-organizes, after a transient,
in equilibrium states in total agreement with those calculated in (15).

10

The update from ci(t) to ci(t + 1) was dictated by a balance equation of the kind of (20). The
measures are performed over 1000 simulations each and ¯ceq denotes the average of ceq over these milliard
of experiments. In order to show the independence from the initial distribution of probabilities of cooper-
ation, Fig. 1 shows the evolution with time of the average probability of cooperation for diﬀerent initial
proportions of C-agents fC0 for the case of the canonical payoﬀ matrix M3051 (i.e. R = 3, S = 0, T = 5
and P = 1).

<p>

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1000

2000

3000

4000

5000

time

FIG. 1: ¯c vs. time, for diﬀerent initial values of fC0, for the canonical payoﬀ matrix.

Depending on the payoﬀ matrix the equilibrium asymptotic states can be of three types: of ”all C”

(¯ceq = 1.0), ”all D” (¯ceq = 0.0) or something in between (0 < ¯ceq < 1).

We have seen that the canonical payoﬀ matrix M3051 provides an example of matrix which gives

¯ceq = 0.5.

Let us see examples of payoﬀ matrices which produce other values of ¯ceq. A payoﬀ matrix which
produces ¯ceq = 1.0 is obtained simply by permuting the canonical values of S (0) and T (5), i.e. M 3501.
For this matrix we have, by inspection of (9) and (12):

aCC = aCD = 0

aDC = aDD = 1.

Hence, after playing the PD game the pair of agents always ends [C,C] since JCD

0 by (8).

On the other hand, a payoﬀ matrix which leads ¯ceq = 0.0 is obtained simply by permuting the

≡

canonical values of R (3) and P (1), i.e. M 1053, for which:

aCC = aCD = 1

aDC = aDD = 0.

That is, all the changes are from C to D since in this case JDC

0

≡

The rate of convergence to the possible values of ¯ceq depends on the values of JCD and JDC .
Fig. 2 shows the approach of the average probability of cooperation for diﬀerent payoﬀ matrices to

their ﬁnal 5 equilibrium values.

(38)

(39)

11

<p>

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2000

4000

6000

8000

10000

iterations

FIG. 2: Curves of ¯c vs. time for diﬀerent payoﬀ matrices producing the 5 possible values of ceq (from below to
above): payoﬀ matrices M3501 with ceq = 1, M2091 with ceq ≃ 0.62, M3051 with ceq = 0.5, M2901 with ceq ≃ 0.38
and M1035 with ceq = 0.

Finally, we simulated the mixing of agents using payoﬀ matrices M3051 and M1053. The evolution to
equilibrium states for diﬀerent ﬁxed fractions d of agents using M1053 is presented in Fig. 3. The results
are in complete agreement with the asymptotic probabilities of cooperation which appear in Table 1.

B.

Stochastic version

In this case simulations were made updating the probability of cooperation according to eq. (20).
However, as we anticipated, we have to change slightly this eq. to reﬂect reality: two agents i and j
interact and they obtain the payoﬀs Xi and Xj, respectively. For each of them there is no way, from
this only event, to know the probability of cooperation ck of his opponent. What they can do then is to
(roughly) estimate this ck as follows. The player i average utility in an encounter at time t with agent j
is given by:

Uij(t) = R ci(t)cj(t) + S ci(t)[1

cj] + T [1

ci(t)]cj (t) + P [1

ci(t)][1

cj(t)]

.

(40)

−

−

−

−

When he plays he gets the payoﬀ Xi, so his best estimate ˜ci
replacing Uij (t) for Xi in eq. (40). Then he will have:

j for the probability of agent j is obtained by

˜ci
j(t) =

Xi
ci(t)(R

P + ci(t)(P
T
S

S)
−
P ) + T

−

−

−
−

P

−

(41)

Exchanging i for j in this eq. gives the estimate of the probability ci(t) that makes agent j. Equation (41)
can retrieve any value of ˜ci
j(t) and not just in the interval [0, 1], so it is necessary to make the following

<p>

0.55

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

replacements:

1000

2000

3000

4000

5000

time

FIG. 3: The evolution of ¯c with time, for diﬀerent values of the fraction d ”antisocial” agents (using M1053)
embedded in a population of neutral agents (using the canonical payoﬀ matrix).

if
and if

˜ci
j(t) > 1
˜ci
j(t) < 0

=
=

⇒
⇒

˜ci
j(t) = 1
˜ci
j(t) = 0

When this happens, the agent is making the roughest approximation, which is to assume that the other
player acts like in the deterministic case.

For the canonical payoﬀ matrix, the result was the expected one as this is a matrix obeying condition
(29): as predicted by the analytical calculation of Section III.b, the value for the equilibrium mean
probability is ceq = 1/2 as in the deterministic case, despite the change introduced in (41). Simulations
for other payoﬀ matrices satisfying conditions (29) or (30) were also made and in all the cases the
deterministic results were recovered.

We will illustrate the case in which some

[µ, ǫRST P
max ]

X

∈

with two particular examples. One of them is the case of the normalized matrix M 1S10, with S varying
from 1 to 2, both limiting cases in which condition (43) ceases to be valid. So for S
1 the update
equation is given simply by:

≤

being ceq = 1 in this case, while for S > 2:

ci(t + 1)

ci(t) = [1

ci(t)][1

cj(t)]

−

−

ci(t + 1)

ci(t) = 1

ci(t)

ci(t)cj(t)

−

−

−

−

12

(42)

(43)

(44)

(45)

13

for which ceq = √5
−
2
play a role, the general equation for the update follows from eq. (20) applied to this particular case:

is the corresponding equilibrium value. When S

(1, 2], both balance equations

∈

1

ci(t + 1)

ci(t) = [1

ci(t)][1

cj(t)] + [cj(t)

2ci(t)cj(t)][1

θ(1

ǫ)].

(46)

−

−
ǫ, eq. (46) reduces to (44) while if R = T = 1 < ǫ we obtain (45).

−

−

−

−

So we can see that for R = T = 1
When the simulation takes place, cj has to be replaced by ˜ci
j.

≥

The same analysis can be done for the matrices M 11T 0, with T varying from 1 to 2 also. In this

case the other root competing with ceq = 1 is ceq = 3

√5
−
2

.

The results of the simulations for both cases are presented in the next table, data for S > 2 and

T > 2 -for which (29) is valid- is also included [25]:

X

1
1.5
1.9
2
2.1
4
8
16
1000

for

for

¯ceq
X = S
1
1
1
1
0.617
0.581
0.556
0.530
0.548

¯ceq
X = T
1
1
1
1
0.383
0.370
0.403
0.467
0.455

As it can be seen from the data, for 1 < X

2, that is, when condition (43) is valid, the results for
the stochastic case are the same that they would be if we were working with the deterministic model.
This is a consequence of the estimate (41) together with conditions (42).

≤

For values of T and S greater than 2, for which condition (43) does not hold any more, we can observe
what at ﬁrst may seem a curiosity: for T or S near 2, the equilibrium values for the deterministic case
are recovered as expected, but as we increase the values of T or S, the value of ceq = 1/2 is approached.
After a little thought, it is clear that this is also a consecuence of the estimation of (41), since it depends
on the payoﬀs. It can be easily seen that in the case of M 11T 0:

if T

1

then ˜ci

≫

0

j ≃

∀

i , j

(for Xi

= T ).

If we take then cj = 0 in eq. (20), and remembering that T
that ceq = 1/2. In an analogous way for M 1S10:

→ ∞

implies that µ

, we will obtain

→ ∞

if S

1 then ˜ci

≫

1

j ≃

∀

i , j

(for Xi

= S)

which toghether with eq. (20) again leads to ceq = 1/2. The encounters for which Xi = S or T are
responsible for that the exact value ceq = 1/2 is not attained. A similar analysis can be done when R or
P

.
→ ∞

(47)

(48)

6
6
14

VI. SUMMARY AND OUTLOOK

The proposed strategy, the combination of measure of success and update rule, produces cooperation

for a wide variety of payoﬀ matrices.
In particular, notice that:

•

•

•

•

A cooperative regime arises for payoﬀ matrices representing ”Social Dilemmas” like the canonical
one. On the other hand spatial game algorithms like the one of ref. [16] produce cooperative states
(ceq > 0) in general for the case of a ”weak dilemma” in which P = S = 0 or at most when P is
signiﬁcantly below R [26].

Payoﬀ matrices with R = S = 0 which, at least in principle, one would bet that favor D, actually
generate equilibrium states with ceq

= 0, provided that P < µ -see eqs. (8)-(13).

Any value of equilibrium average cooperation can be reached in principle, even in the case of the
deterministic model, by the appropriate mixing of agents using 2 diﬀerent payoﬀ matrices. This is
an interesting result that goes beyond the diﬀerent existent social settings. For instance we have in
mind situations in which one wants to design a device or mechanism with a given value of ceq that
optimizes its performance.

In this work we adopted a Mean Field approach in which all the spatial correlations between agents
were neglected. One virtue of this simpliﬁcation is that it shows the model does not require that
agents interact only with those within some geographical proximity in order to sustain cooperation.
Playing with ﬁxed neighbors is sometimes considered as an important ingredient to successfully
maintain the cooperative regime [16],[19]. (Additionally, the equilibrium points can be obtained by
simple algebra.)

To conclude we mention diﬀerent extensions and applications of this model as possible future work.
We mentioned, at the beginning, ”statistical mechanic” studies. For instance, by dividing the four payoﬀs
between say the reward R reduces the parameters to three: a = S/R, b = T /R and d = P/R, and we
are interested to analyze the dependence of ceq on each one of these 3 parameters in the vicinity of a
transition between two diﬀerent values. It is also interesting to introduce noise in the system, by means
of an inverse temperature parameter β, in order to allow irrational choices. The player i changes his
strategy with a probability Wi given by

1

Wi =

1 + exp[β(Ui

˜ǫi)]

−

,

where ˜ǫi
We are planning also a quantum extension of the model in order to deal with players which use

max
{

ǫi, µ
}

≡

.

superposition of strategies αC

C > +αD
|

D > instead of deﬁnite strategies.
|

The study of the spatial structured version and how the diﬀerent agents lump together is also an

interesting problem to consider. Results on that topic will be presented elsewhere.

Finally, a test for the model against experimental data seems interesting. In the case of humans
the experiments suggest, for a given class of games (i.e. a deﬁnite rank in the order of the payoﬀs), a
dependency of fc with the relative weights of R, S, T and P , which is not observed in the present model.
Therefore, we should change the update rule in such a way to capture this Feature. Work is also in
progress in that direction.

6
APPENDIX: Calculus for the maximum of the Gain Estimate function in the stochastic

We will now show in detail the calculus for the maximum of the gain estimate function ǫRST P (c),
restricted to the interval [0, 1]. First we have to know if the function has a maximum in the open interval
(0, 1). This can be done by noticing that, by (18), for having negative concavity, we have the condition:

case.

By doing

we ﬁnd that the extremum of ǫRST P (c) is attained at

Imposing co > 0, co < 1 and using (49) for consistency, we obtain:

R

S

−

−

T + P < 0

ǫRST P = 0

d
dc

1
2

co =

(S + T

2P )
−
T + P )

−

(R

S

−

−

S + T > 2P

, S + T > 2R

S + T > 2 max
{

R, P

}

Notice that the sum of this two conditions is equivalent to condition (49). In turn, (52) can be expressed
as

so this inequality resumes (49) and (52). It can be seen that if (53) is fulﬁlled, ǫRST P

µ always.

max ≥

So if condition (53) holds, the maximum of the function ǫRST P (c) takes place in the interval (0, 1)

and its value as a function of the parameters R, S, T and P is:

On the other hand, if

then

since

ǫRST P (0) = P ,

ǫRST P (1) = R.

ǫRST P
max = P

1
4

(S + T
S

(R

−

−

2P )2
T + P )

−
−

S + T

2 max
{

≤

R, P

}

ǫRST P
max = max

R, P

{

}

15

(49)

(50)

(51)

(52)

(53)

(54)

(55)

(56)

[1] J. Hofbauer and K. Sigmund, The Theory of Evolution and Dynamical Systems, Cambridge University Press

[2] J. von Neumann and O. Morgenstern, Theory of Games and Economic Behavior, Princeton University Press,

1988.

Princeton, 1944.

16

[3] J. Maynard-Smith, Evolution and the Theory of Games, Cambridge Univ. Press 1982.
[4] P.E. Turner and L. Chao, Nature (London) 398, 441 (1999).
[5] D.A. Meyer, Phys. Rev Lett. 82, 1052-1055 (1999).
[6] J. Eisert, M. Wilkens and M. Lewenstein, Phys. Rev. Lett. 83 3077-3080 (1999).
[7] C.F. Lee and N. Johnson, arXiv.org/quant-ph/abs/0207012 (2002).
[8] C.F. Lee and N. Johnson, Phys. Lett. A 301 343-349; quant-ph/0207080 (2002).
[9] G. Szab´o and C. T¨oke, Phys. Rev. E 58, 69 (1998).
[10] G. Szab´o, T. Antal, P. Szab´o and M. Droz, Phys. Rev. E 62, 1095 (2000).
[11] M. Flood, ”Some Experimental Games”, Research Memorandum, RM-789-1, 20 June 1952, The RAND

Corporation, 1700 Main St., Santa Monica, CA (1952).

[12] J. Nash, Annals of Mathematics 54, 286 (1951).
[13] A. Rapoport and M. Guyer, General Systems 11, 205 (1966).
[14] R. Axelrod, The Evolution of Cooperation, Basic Books, New York, 1984.
[15] J. Epstein, Zones of Cooperation in Demographic Prisoner’s Dilemma Complexity, Vol. 4, Number 2,

[16] M.A. Nowak and R. May, Int. J. Bifurcation and Chaos 3, 35 (1993); M.A. Nowak and R. May, Nature 359,

November-December 1998.

826 (1992).

[17] H. Fort, Phys. Rev. E 68 , 026118 (2003).
[18] J. W. Weibull , Evolutionary Game Theory, MIT Press 1995.
[19] M.D. Cohen, R.L. Riolo and R. Axelrod, Rationality and Society 13, 5 (2001).
[20] [X,Y] means that the ﬁrst player plays X and the second player plays Y (X an Y = C or D ).
[21] We will maintain the letters R, S, T or P to denote the payoﬀs in order to keep the PD standard notation.
[22] One might consider more sophisticated agents which have ”good” information (statistics, surveys, etc) from
which they can extract the average probability of cooperation at ”real time” c(t) to get a better estimate
of their expected utilities. However, the main results do not diﬀer from the ones obtained with this simpler
agents

[23] The reason to include the mean µ is to cover a wider range of situations than the ones permitted by the
so-called Pavlov’s rule. Pavlov strategy consists in to stick to the former move if it earned one of the two
highest payoﬀ but to switch in the contrary case. The measure considered here reduces to it when R > µ > P .

[24] The real roots ∈ [0, 1]
[25] ¯ceq corresponds to the average of ceq over 100 experiments.
[26] In particular, in a spatial game in which each player interacts with his four nearest neighbors, we have checked

that the canonical payoﬀ matrix lead to the an ”all D” state with ceq = 0.


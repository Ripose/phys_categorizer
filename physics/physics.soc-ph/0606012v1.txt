An updated version of this article will be published as chap. 9 in a forthcoming book ”Econo-

physics and Sociophysics: Trends and Perspectives”, Eds. B.K. Chakrabarti, A. Chakraborti and

A. Chatterjee, Wiley-VCH, Berlin.

Econophysics of Stock and Foreign Currency

Exchange Markets

M. Ausloos1∗

1 SUPRATECS, Universit´e de Li`ege,

B5 Sart-Tilman, B-4000 Li`ege, Euroland

(Dated: February 2, 2008)

Abstract

Econophysics is a science in its infancy, born about ten years ago at this time of writing, at the

crossing roads of physics, mathematics, computing and of course economics and ﬁnance. It also

covers human sciences, because all economics is ultimately driven by human decision. From this

human factor, econophysics has no hope to achieve the status of an exact science, but it is interesting

to discover what can be achieved, discovering potential limits and trying try to push further

away these limits. A few data analysis techniques are described with emphasis on the Detrended

Fluctuation Analysis (DF A) and the Zipf Analysis Technique (ZAT ).

Information about the

original data aresketchy, but the data concerns mainly the foreign currency exchange market. The

robustness of the DF A technique is underlined. Additional remarks are given for suggesting further

work. Models about ﬁnancial value evolutions are recalled, again without going into elaborate work

discussing typical agent behaviors, but rather with hopefully suﬃcient information such that the

basic ingredients can be memorized before reading some of the vast literature on price formation.

Crashes being spectacular phenomena retain our attention and do so through data analysis and

basic intuitive models. A few statistical and microscopic models are outlined.

PACS numbers: 89.75.Fb, 89.65.Ef, 64.60.Ak

6
0
0
2
 
n
u
J
 
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
2
1
0
6
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗Electronic address: Marcel.Ausloos@ulg.ac.be

1

I.

INTRODUCTION

Econophysics is a science in its infancy, born about ten years ago at this time of writing, at

the crossing roads of physics, mathematics, computing and of course economics and ﬁnance.

It also covers human sciences, because all economics is ultimately driven by human decision.

From this human factor, econophysics has no hope to achieve the status of an exact science,

but it is interesting to discover what can be achieved, discovering potential limits and trying

try to push further away these limits.

Numerous works have inspired physicists and guided them towards the studies of ﬁnancial

markets. We could start with Bachelier [1] who introduced what shall perhaps remain the

simplest and most successful model for price variations but for the moment, it suﬃces to

mention that it relies on Gaussian statistics. However, Bachelier could have learned from

Pareto [2] that power-laws are ubiquitous in nature. In fact, large price variations in high

frequency seems to be described by a crossover between two power-laws, time correlations

in the variance is also decay as a power-law and so do numerous other quantities related to

ﬁnance. Mandelbrot [3] and Fama [4] helped move forward from these empirical evidences

by proposing to describe price variations using a class of distributions studied by L´evy [5].

Thereafter, these distributions were abruptly truncated by Mantegna and Stanley [6] and

exponentially truncated by Koponen [7] to recover a slow convergence towards a Gaussian

distribution for low frequency data, as observed empirically.

From the previous collection of dates, going as far as 1897 for Pareto, one concludes

that the ﬁeld of research is not new, but the interest has been renewed recently as testiﬁed

by the recent excellent works on econophysics, starting with a conference book published

by Anderson, Arrow and Pines [8] followed by the books of Bouchaud and Potters [9] and

Mantegna and Stanley [10] Paul and Baschnagel [11] and Voit [12] These works show the

variety of interests and successes of the approach of ﬁnance by physicists. From these

works and the many research papers, we gather that there exists a temporary consensus on

empirical data, though there is no such unity in modeling. There is a profusion of models

indeed.

First recall that a “price” can only go up or down, that is, is a one-dimensional quantity.

Therefore, it can be seen as a point diﬀusing on a line. The factors driving this diﬀusive

process are numerous, and their nature is still not fully elucidated. Hence, one can only

2

make plausible assumptions and test afterwards their value by comparing the distribution

of changes generated by the diﬀusion process with actual price evolution changes, price

being also understood as the value of some ﬁnancial index or some exchange rate between

currencies.

A fundamental problem is the existence or not of long-range power-law correlations in

economic systems as well as the presence of economic cycles. Indeed, traditional methods

(like spectral methods) have corroborated that there is evidence that the Brownian motion

idea is only approximately right [3, 4]. Long-range power-law correlations discovered in

economic systems, particularily in ﬁnancial ﬂuctuations [6] have been examined through

the so-called L´evy statistics already mentioned by Stanley et al.

[6] who have shown the

existence of long-range power-law correlations in the Standard and Poor (S&P500) index. A

method based on wavelet analysis has also shown the emergence of hidden structures in the

S&P500 index [13]. We have ﬁrst performed a Detrended Fluctuation Analysis (DF A) of the

USD/DEM ratio [14] and of other foreign currency exchanges and have demonstrated the

existence of successive sequences of economic activity having diﬀerent statistical behaviors.

They will be mentioned in the relevant section.

A word of caution is at once necessary: this review is by no means objective. It is strongly

biased towards studies undergone by a few physicists in the last few years, in order to give

an overview of a few results in the physics literature on only two topics : foreign currency

exchanges and stock market indices. Even so, it is not possible to cover all papers published

or put on arXives.

I do apologize for having missed many papers and ideas. Of course,

since no full review of the diﬀerent developments belonging to the physics literature is here

possible, the more so about the purely economics literature. However on one hand, each of

the research papers presented here has individually justiﬁed its sideline propositions which

would be tiring to reproduce. On the other hand, there is a huge amount of overlaps between

the diﬀerent works summarized here below and models developed by many authors. Trying

to distinguish the main contributions separately is a titan work. Attempting to compare

and discuss details is quasi impossible. Yet this review of econophysics and data analysis

techniques , restricted to stock markets and foreign exchange currency markets should only

seen as to open a gate toward huge undiscovered ﬁelds.

In the following section a few data analysis techniques will be described with emphasis

on the Detrended Fluctuation Analysis (DF A) and the Zipf Analysis Technique (ZAT ).

3

Information about the original data will be sketchy, but the data concerns mainly the foreign

currency exchange market. The robustness of the DF A technique will be underlined and

additional remarks will be given for suggesting further work. In the next section models

about ﬁnancial index value evolutions will be recalled, again without going into elaborate

work discussing typical agent behaviors, but rather with hopefully suﬃcient information

such that the basic ingredients can be memorized before reading some of the vast literature

on price formation. Crashes being spectacular phenomena must retain our attention and do

through data analysis and basic intuitive models. A few “more general” microscopic models

will also be outlined.

II. A FEW ROBUST TECHNIQUES

A. Detrended ﬂuctuation analysis technique

In the basic Detrended Fluctuation Analysis (DF A) technique one divides a time series

y(t) of length N into N/τ equal size nonoverlapping boxes [15]. For smoothing out the

data, the integration of the raw time series can often be performed ﬁrst; in so doing one

has to remember that such an integration shifts the exponent by one unit. The variable t

is discrete, evolves by a single unit at each time step between t = 1 and t = N. No data

point is supposed to be missing, i.e., breaks due to holidays and week-ends are disregarded.

Nevertheless, the τ units are said to be days: often a week has only 5 days, and a year

about 200 days. Let each box contain τ points and N/τ be an integer. The local trend in

each τ -size box is assumed to be linear, i.e. it is taken as z(t) = a t + b . In each τ -size box

one next calculates the root mean square deviation between y(t) and z(t). The detrended

ﬂuctuation function F (τ ) is then calculated following

F 2(τ ) =

1
τ

(k+1)τ

Xt=kτ +1

y(t)

|

−

z(t)

2,

|

k = 0, 1, 2,

, (

N
τ −

1).

· · ·

Averaging F 2(τ ) over all N/τ box sizes centered on time τ gives the ﬂuctuations

i
as a function of τ . The calculation is repeated for all possible diﬀerent values of τ . A power

F 2(τ )

h

law behavior is expected as

(1)

(2)

F 2(τ )

1/2

τ α.

∼

h

i

4

An exponent α

= 1/2 in a certain range of τ values implies the existence of long-range

correlations in that time interval as in the fractional Brownian motion [16]. Such correlations

are said to be “persistent” or “antipersistent” when they correspond to α > 1/2 and α < 1/2

respectively in practice Hu = α. A straight line can well ﬁt the data between logτ = 1 and

logτ = 2.6. This interval is called the scaling range. Outside the scaling range the error

bars are larger often because of so called ﬁnite size eﬀects, and/or the lack of numerous data

points. The α exponent is directly related to the Hurst exponent [17] and the signal fractal

dimension [3].

Most of the time, for the real or virtual foreign exchange currency (F EXC) rates that

we have examined [14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], the scaling range is well deﬁned.

Sometimes it readily appears that the data contains two sets of points which can be ﬁtted

by straight lines. Usually that describing the “large τ ” data has a 0.50 slope, Indicating

correlationless ﬂuctuations. Crossovers from fractional to ordinary Brownian motion can

be well observed. These crossovers suggest that correlated sequences have characteristic

durations with well deﬁned lower and upper scales [14]. The persistence is usually related

to free market (and “runaway”) conditions while the antipersistence develops due to strict

political control allowing for a ﬁnite size bracket in which the F EXC rates can ﬂuctuate.

The case α = 0.50 is surely avoided by speculators.

Notice that to consider overlapping boxes might be useful when not many data points are

available. However it was feared that extra insidious correlations would thereby be inserted.

Nevertheless the analysis has shown that the value of α is rather insensitive to the way boxes

are used. A cubic trend, like z(t) = ct3 + dt2 + et + f , can be also considered [28]. The

parameters a to f are similarly estimated through a best least-square ﬁt of the data points

in each box. Following the procedure described here above the value of the exponent α can

be obtained. Again the diﬀerence is found to be very small. Extensions of DF A to higher

moment components have also been investigated, tending toward some sort of multifractal

analysis. Moreover the linear or cubic or other polynomial law ﬁtting the trend can be

usefully replaced by some moving average, leading to some further insight into the Hurst

exponent value.[29]

[14, 19, 28].

An interesting observation consists at looking for local (more exactly temporal ) α values

This allows one to probe the existence or not of locally correlated or decorrelated sequences.

5

6
In order to observe the local correlations, a local observation box of a given ﬁnite size is

constructed.

Its size depends on the upper value of τ for which a reasonable power law

exponent is found. It is chosen to be large enough in order to obtain a suﬃciently large

number of data points. The local exponent α is then calculated for the data contained

in that ﬁnite size box, as above. Thereafter the box is moved along the time axis by an

arbitrary ﬁnite number of points, often depending on the intended strategy.

The local α exponent seems rougher, and varies with time around the overall (mean) α

value. The variation depends on the box size. Three typical F EXC rate time dependences,

i.e. DEM/JP Y , DEM/CHF , and DEM/DKK have been shown for various time intervals

in [20] and the α value indicated for the scaling range. For example, the DEM/JP Y local α

is consistently above 0.50 indicating a persistent evolution. A positive ﬂuctuation is likely

to be followed by another positive one. The case of DEM/CHF is typically Brownian with

ﬂuctuations around 0.50. However, in 1994 some drift is observed toward a value ca. 0.55

while in 1997-1998 some drift is observed toward a value ca. 0.45.

It is clear that some

economic policy change occurred in 1994, and a drastic one at the end of 1998 in order to

render the system more Brownianlike. The same is true for the DEM/DKK where due

to european economic policy the spread in this exchange rate was changed several times,

leading from a Brownianlike situation to a nowadays antipersistent behavior, i.e. a positive

ﬂuctuation is followed by a negative one, and conversely, such that local α is becoming

pretty low.

Therefore this procedure interestingly leads to a local measurement of the degree of

long-range correlations. In other investigations, [14] it has been found on the GBL/DEM

exchange rate data that the change in slope of local α vs. time corresponds to changes in

the Bundesbank interest rate increase or decrease. The 1985 Plaza agreement had some

inﬂuence in order to curb the run away local α value from a persistent 0.60 back to a

more 0.50 Brownian-like value. Thus such F EXC rate behaviors indicate a measure of

information, an entropy variation indicating how (whatever) “information is managed by

the system. This seems to be an “information” to be taken into account when developing

Hamiltonian or thermodynamic-like models.

At some time,[21, 22] it was also searched for correlations and anticorrelations in exchange

rates of pre-Euro moneys with respect to currencies as CHF , DKK, JP Y and USD in order

to understand the EUR behavior. The power law behavior describing the rms. deviation

6

of the ﬂuctuations as a function of time, whence the Hurst (or α) time-dependent exponent

was obtained. We compared the time dependent α exponent of the DF A as in a correlation

matrix for estimating respective inﬂuences. In doing a similar analysis, it was shown that the

ﬂuctuations of the GBP and EUR with respect to the major currencies were very similar,

indirectly indicating that the GBP was eﬀectively tied to EUR. Same for studies pertaining

to latin american currencies [19].

Since such temporal correlations can be sorted out, a strategy for proﬁt making can

be developed. It is easily observed whether there is persistence or antipersistence in some

exchange rate, - according to the temporal value of α. Thus some probability on the next

ﬂuctuation, to be positive or negative can be made. Therefore a buy or sell decision can be

taken. In so doing and taking the example of (DEM/BEF ), we performed some virtual

game and implemented the most simple strategy [30].

Investment strategies should look for Hu values over diﬀerent time interval windows which

are continuously shifted. In this respect connection to multifractal analysis. The technique

consists in calculating the so-called “qth order height-height correlation function”

where only non-zero terms are considered in the average

iτ taken over all couples (t, t′)
...

h

such that τ =

. The correlation function c(τ ) is supposed to behave like

t

|

−

t′

|

cq(τ ) =

y(t)

h|

y(t′)

q

|

iτ

−

c1(τ ) =

y(t)

h|

−

y(t′)

|iτ ∼

τ H1,

(3)

(4)

where H1 is the Hu exponent. [31, 32] This corresponds to obtaining a spectrum of moving

averages indeed [33]. Notice that a DF A and a multifractal analysis cost much more CPU

time than a moving average method due to multiple loops which are present in the DF A

and cq algorithms [14, 31, 33, 34].

Of course, subtracting some moving average background, instead of the usual linear trend,

is of interest in order to implement a strategy based on diﬀerent horizons.

B. Zipf analysis technique

The same type of consideration for a strategy can be developed from the Zipf analysis

technique (ZAT ) performed on stock market or F EXC data.

In the (DF A) technique

7

the sign of the ﬂuctuations and their persistence, are taken into account, but it falls short

of implementing some strategy from the amplitude of the ﬂuctuations. The so-called Zipf

analysis [35] originally introduced in the context of natural languages can be performed by

calculating the frequency of occurence f of each word in a given text. According to their

frequency, a rank R can be assigned to each word, with R = 1 for the most frequent one. A

power law

is expected [35]. A Zipf plot is simply a transformation of the cumulative distribution.

However, it accentuates the upper tail of the distribution, which makes it a useful to analyze

R−ζ

f

∼

(5)

this part of the distribution.

A simple extension of the Zipf analysis is to consider m-letter words, i.e.

the words

strictly made of m characters without considering the white spaces. The available number

of diﬀerent letters or characters k in the alphabet should also be speciﬁed. A power law

in f (R) is expected as well for correlated sequences [36, 37]. There is no theory at this

time predicting the exponent as a function of (m, k). The technique has a rather weak

value when only two characters and short words are considered [36]. An increase in the

number of allowed characters for the alphabet allows one to consider diﬀerent size and signs

of the ﬂuctuations, i.e. huge, marginal and small (positive or negative) ﬂuctuations can

be considered. After having decided on the number (k) of characters of the alphabet, the

signal can be transformed into a text and thereafter analysed with respect to the frequency

of words of a given size (m) to be ranked accordingly. In our work, words of equal lengths

were always considered

The above procedure does not take into account the trend. Some work has been done on

the matter, but much is still to be performed since the trend deﬁnition can be quite diﬀerent

for later strategy considerations. The relevance of this remark should be emphasized: indeed

for a positive (or negative) trend over the time box which is investigated, a bias can occur

between words. For a two character alphabet, e.g. u and d, the frequency f of u’s, i.e. pu

can be larger (smaller) than the frequency of d’s, i.e. pd. Such a bias can be taken into
account with respect to the equal probability occurrence, e.g. ǫ = pu −
new ranking procedure can be performed by deﬁning the ratio of the observed frequency of a

0.5 =pd + 0.5. A

word divided by the theoretical frequency of occurrence of a word, assuming independence of

characters. E.g. if the word uud occurs say puud times, since the independence of characters

8

would imply that the word would occur pu pu pd times, a relative frequency f /f0 can be

deﬁned as puud/(pu pu pd). A new ranking can be made with quite diﬀerent appearance.

A ZAT variant consists in ranking the words according to their relative frequency and

relative (“normalized”) rank taking into account for the normalization the probability fM of

the most often occurring word [37]. Indeed for m and k large not all words do occur. Even

though, e.g.

for the (m = 6, k = 2) case there are 64 possible words, and the maximum

rank is RM = 64, the frequency of the most often observed word is unknown. Another

extension has been recently proposed in which time windows are used in order to sort better

the relevant exponents and probability of occurrence [38, 39].

In conclusion of this subsection, it is emphasized that diﬀerent strategies following the

Zipf analysis technique can be implemented, according to (m, k) values, how the trend is

eliminated (or not) and how the ranks and frequencies of occurrence are deﬁned.

C. Other techniques for searching correlations in ﬁnancial indices

Quite simultaneously, Laloux et al.

[40, 41] and Plerou et al.

[42, 43] analyzed the

correlations between stocks traded on ﬁnancial markets using the theory of random matrices.

Laloux et al. considered daily price changes for the 1991-1996 period (1309 day) of 406 of

the companies forming the S&P500 index while Plerou et al. analyzed price returns over a

30 minute period of the 1,000 largest US companies for the 1994-1995 period (6448 points

for each company). The correlation coeﬃcient between two stocks i and j is deﬁned by

ρij ≡

< RiRj >

< Ri >< Rj >

−

(< R2

i >

< Ri >2)(< R2

j >

< Rj >2)

−

q

−

where Ri is the price of company i for Laloux et al. and the return of the price for Plerou

et al. The statistical average is a temporal average performed on all trading days of the

investigated periods. The cross-correlation matrix C of elements ρij is measured at equal

times.

Laloux et al [40, 41] focussed on the density ρc(λ) of eigenvalues of C, deﬁned by

(6)

(7)

where n(λ) is the number of eigenvalues of C less than λ and N the number of companies

(or equivalently the dimension of C). The empirically determined ρc(λ) is compared with

ρc(λ)

1
N

dn(λ)
dλ

≡

9

the eigenvalue density of a random matrix, that is, a matrix whose components are random

variables. Random matrix theory (RMT) isolates universal features, which means that

deviations from these features identiﬁes system-speciﬁc properties. Laloux et al. found that

94% of the total number of eigenvalues fall inside the prediction of RMT, which means that

it is very diﬃcult to distinguish correlations from random changes in ﬁnancial markets.

The most striking diﬀerence is that RMT predicts a maximum value for the largest eigen-

value which is much smaller than what is observed empirically. In particular, a very large

eigenvalue is measured, representing the market itself. Plerou et al. [42, 43] observed similar

properties in their empirical study. They measured the number of companies contributing

to each eigenvector and found that a small number of companies (compared to 1,000) con-

tribute to the eigenvectors associated to the largest and the smallest eigenvalues. For the

largest eigenvalues these companies are correlated, while for the smallest eigenvalues, these

are uncorrelated. The observation for the largest eigenvalues does not concern the largest

one, which has an associated eigenvector representing the market and has an approximately

equal contribution from each stock.

Mantegna [44] analyzed the correlations between the 30 stocks used to calculate the Dow

Jones industrial average and the correlations between the companies used to calculate the

S&P500 index, for the July 1989 to October 1995 time period in both cases. Only the

companies which were present in the S&P500 index during the whole period time were

considered, which leaves 443 stocks. The correlation coeﬃcient of the returns for all possible

pairs of stocks was computed. A metric distance between two stocks is deﬁned by dij =

ρ2
ij. These distances are the elements of the distance matrix D.

1

−

Using the distance matrix D, Mantegna determined the topological arrangement which

is present between the diﬀerent stock. His study could also give empirical evidence about

the existence and nature of common economic factors which drive the time evolution of

stock prices, a problem of major interest. Mantegna determined the minimum spanning tree

(MST) connecting the diﬀerent indices, and thereafter, the subdominant ultrametric struc-

distance matrix D< are determined from the MST. d<

ture and heriarchical organization of the indices. In fact, the elements d<

ij of the ultrametric
ij is the maximum Euclidian distance
dlk detected by moving by single steps from i to j through the path connecting i to j in

the MST. Studying the MST and hierarchical organization of the stocks deﬁning the Dow

Jones industrial average, Mantegna showed that the stocks can be divided in three groups.

10

Carrying the same analysis for the stocks belonging to the S&P500, Mantegna obtained a

grouping of the stocks according to the industry they belong to. This suggests that stocks

belonging to the same industry respond in a statistical way to the same economic factors.

Later, Bonanno et al. [45] extended the previous analysis to consider correlations between

diﬀerent (29 stock market indices, one from Africa, eight from America, nine from Asia, one

from Oceania and ten from Europe. Their correlation coeﬃcients ρij was calculated using

the return of the indices instead of the individual stocks. They also modiﬁed slightly the

deﬁnition of the distance, using dij =

2(1

ρij). A hierarchical structure of indices was

−
obtained, showing a distinct regional clustering. Clusters for North-America, South America

p

and Europe emerge, while Asian indices are more diversiﬁed. Japanese and Indian stock

markets are pretty distant from the others. When the same analysis is performed over

diﬀerent periods of time, the clustering is still present, with a slowly evolving stucture of

the ultrametric distance with time. The taxinomy is stable over a long period of time.

Two key parameters are the length L< and the proximity P . The length is L<

d<
i,j,
where the sum runs over nearest neighbouring sites on the MST. It is a kind of average of

P

≡

ﬁrst-neighbour distances over the whole MST. The proximity is

P

≡ P

i,j |

di,j −
i,j di,j

d<
i,j|

(8)

P
where the sums run over all distinct i, j pairs. The proximity characterizes the degree of

resemblance of the subdominant ultrametric space to the Euclidian space. For long time

period, L<

26.9, while L< = 28√2

39.6 for sequences without correlations. Similarly,

→
for long time period, P

→

≈

0.091, to compare with P

0.025 when the same data are

→

shuﬄed to destroy any correlations. The eﬀects of spurious cross-correlations make the

previous time analysis relevant for time periods of the order of three months or longer for

daily data.

Others [46] analyzed the sign of daily changes of the NIKKEI, the DAX and the Dow

Jones industrial average for the Dow Jones only and for the three time series together,

reproducing the evolution of the market through the sign of the ﬂuctuation, as if the latter

was represented by an Ising spin. The authors studied the frequency of occurrence of triplets

of spins for the January 1980 to December 1990 period, after having removed the bias due

to a diﬀerent probability of having a move upward or downward. They showed that the

spin series generated by each index separately is comparable to a randomly generated series.

11

However, they emphasized correlations in the spin series generated by the three indices

showing that market places ﬂuctuate in a cooperative fashion. Three upward moves in a row

are more likely than expected from a series without correlations between successive changes,

and similarly for three downward moves. This behaviour seems to be symmetrical with

respect to ups and downs in the time period investigated. The strength of the correlations

varies with time, with the diﬀerence in frequency of diﬀerent patterns being neatly marked

in the two year period preceding the 1987 crash. Also, during this period, the up-down

symmetry is broken.

Finally let us mention the Recurrence Plot (RP ) and Recurrence Quantiﬁcation Analysis

(RQA) techniques for detecting e.g. critical regimes appearing in ﬁnancial market indices

[47]. Those are graphical tools elaborated by Eckmann, Kamphorst and Ruelle in 1987,

based on Phase Space Reconstruction [48] and extended by Zbilut and Webber [49] in 1992

RP and RQA techniques are usually intended to provide evidence for chaos, but can also be

used for their goodness in working with non stationarity and noisy data [50] and in detecting

changes in data behavior, in particular in detecting breaks, like a phase transition [51], and

other dynamic properties of a time series [48]. It was indicated that they can be used for

detecting bubble nucleation, grow and even indicate bursting. An analysis was made on

two time series, NASDAQ and DAX, taken over a time span of 6 years including the known

(NASDAQ) crash of April 2000. It has been shown that a bubble burst warning could be

given, with some delay ((m

1)d days) with respect to the beginning of the bubble, but

−

with enough time before the crash (3 months in the mentioned cases) [47].

III. STATISTICAL, PHENOMENOLOGICAL AND “MICROSCOPIC” MODELS

A few functions used to ﬁt empirical data should be mentioned. The point is to try to

determine which laws are the best approximation for which data. Some of these laws rely on

plausible and fairly convincing arguments A major step forward to quantify the discrepancies

between real time series and Gaussian statistics was made by Mandelbrot [3] who studied

cotton prices. In addition to being non-Gaussian, Mandelbrot noted that returns respect

time scaling, that is, the distribution of returns for various time scales ∆t from 1 day up

to one month have similar functional form. Motivated by this scaling and the fact that

large events are far more probable than expected, he proposed that the statistics of returns

12

could be described by symmetrical L´evy distributions; other laws can appear to better ﬁt

the data, and also rely on practical arguments, yet they appear as ad hoc modiﬁcations, like

the truncated L´evy ﬂight.

A. ARCH, GARCH, EGARCH, IGARCH, FIGARCH models

The acronym ‘ARCH’ stands for autoregressive conditional heteroscedasticity, a process

introduced by Engle [52]. In short, Engle assumed that the price at a given time is drawn

from a probability distribution which depends on information about previous prices, what he

referred to as a conditional probability distribution (cpd). Typically, in a ARCH(p) process,
the variance of the cpd P (xt|

xt−1, ..., xt−p) at time t is given by

t = α0 + α1x2
σ2

t−1 + α2x2

t−2 + ...αpx2

t−p

(9)

where xt−1, xt2, ... are random variables chosen from sets of random variables with Gaussian

distributions of zero mean and standard deviations σt−1, σt−2, ..., respectively. α0, α1, ...,

αp are control parameters. Locally (in time), σt varies but on long time scale, the overal

process is stationary for a wide range of values of the control parameters.

Bollerslev [53] generalized the previous process by introducing Generalized ARCH or

GARCH(p,q) processes. He suggested to model the time evolution of the variance of the
cpd P (xt|

xt−1, ..., xt−p, σt−1, ...σt−q) at time t with

t = α0 + α1x2
σ2

t−1 + ...αnx2

t−n + β1σ2

t−1 + ... + βqσ2

t−q,

(10)

with an added set

of q control parameters on top of already p control parameters.

β1, ..., βq}

{

Using this process, it is now possible to generate time correlations in the variance. The

previous processes have been extended, as in EGARCH processes [55], IGARCH processes

[54], FIGARCH processes [56], among others.

B. Distribution of returns

Mantegna and Stanley [57] analyzed the returns of the S&P500 index from Jan. 1984 to

Dec. 1989. They found that it is well described by a L´evy stable symmetrical process of index

α = 1.4 for time intervals spanning from 1 to 1,000 minutes, except for the most rare events.

13

Typically, a good agreement with the L´evy distribution is observed when m/σ

6 and an

approximately exponential fall-oﬀ from the stable distribution is observed when m/σ

6,

with the variance σ2 = 2.57.10−3. For ∆t = 1minutes, the kurtosis κ = 43. Their empirical

study relies mainly on the scaling of the ‘probability of return to the origin’ P (R = 0) (not

≤

≥

to be confused with the distribution of returns) as a function of ∆t, which is equal to

P (0)

≡ Lα(R∆t = 0) =

Γ(1/α)
πα(γ∆t)1/α

(11)

±

if the process is L´evy, where Γ is the Gamma function. They obtain α = 1.40

0.05. This

value is roughly constant over the years, while the scale factor γ (related to the vertical

position of the distribution) ﬂuctuates with burst of activity localized in speciﬁc months.

Using a ARCH(1) model with σ2 and κ constrained to their empirical values, Mantegna and

Stanley [58] obtained a scaling value close to 2 (

1.93), in disagreement with α = 1.4. For

∼

a GARCH (1,1) process with similar constraint and the choice β1 = 0.9, In ref. [57, 58] they

obtained a very good agreement for the distribution of returns when ∆t = 1minute, but

they estimated that the scaling index α should be equal to 1.88, in disagreement with the

observed value.

Gopikrishnan et al. [59] extended the previous study of the S&P 500 to the 1984-1996

period for one-minute frequency records, to the 1962-1996 period for daily records and the

1926-1996 period for monthly records. In parallel, they also analyzed daily records of the

NIKKEI index of the Tokyo stock exchange for the 1984-1997 period and daily records

of the Hang-Seng index of the Hong Kong stock exchange for the 1980-1997 period.

In

another study, Gopikrishnan et al.

[60] analyzed the returns of individual stocks in the

three major US stock markets, the NYSE, the AMEX and the NASDAQ for the January

1994 to December 1995 period. Earlier works by Lux [61] focussed on daily returns of the

German stocks making the DAX share index during the period 1988-1994.

To compare the behaviour of the diﬀerent assets on diﬀerent time scales, a normalized

return g

g∆t(t) is deﬁned, with

≡

< R >T

R
−
< R2
T >

< R >2
T

−

g

≡

p

(12)

where the average <

>T is over the entire length T of the time series considered. The

· · ·

denominator of the previous equation corresponds to the time averaged volatility σ(∆t).

For all data and for ∆t from 1 minute to 1 day, the distribution of returns is in agreement

14

with a L´evy distribution of index varying from α

1.35 to α

1.8 in the central part

≈

≈

of the distribution, depending on the size of the region of empirical data used for the ﬁt.

In contrast to the previously suggested exponential truncation for the largest returns, the

distribution is found to scale like a power-law with an exponent µ

1 + α

4. This value

is well outside the L´evy stable regime, which requires 0 < α

towards a truncated L´evy distribution, with a power-law truncation.

≡

≈
2. Hence, these study point

≤

The previous scaling in time is observed for time scales ∆t of up to 4 days. For larger time

scale, the data are consistent with a slow convergence towards a Gaussian distribution. This

convergence is expected as the presence of a power-law cut-oﬀ implies that the distribution

of returns is in the Gaussian stable regime, where the CLT applies. What is surprising is

the existence of a ‘metastable’ scaling regime observed in time scales as long as 4 days.

Gopikrishnan et al.

[59] identiﬁed time dependencies as one of the potential sources of

this scaling regime, by comparing the actual time series ﬁrst to a randomly generated time

series with the same distribution and second to the shuﬄed original time series. The two

latters display a much faster convergence towards the Gaussian statistics, conﬁrming their

hypothesis.

One important conclusion of the previous empirical data is that the theory of truncated

L´evy distributions cannot reproduce empirical data as it stands, because in the current

framework of truncated L´evy distributions, the random variables are still independent, while

it has been shown that time dependencies are a crucial ingredient of the scaling. Also, it

does not explain the ﬂuctuations of γ.

Plerou et al. [62] considered the variance of individual price changes, ω2

< (δpi)2 >

∆t ≡

for all transactions of the 1,000 largest US stocks for the 1994-1995 2-year period. The

cumulative distribution of this variance displays a power-law scaling P (ω∆t > x)

x−γ,

∼

with γ = 2.9

0.1. Using detrended ﬂuctuation analysis, they obtain a correlation function

0.01, that is, weak correlations only, independent

±

characterized by an exponent µ = 0.60

±
variables being characterized by µ = 1/2.

Gopikrishnan et al.

[59] found that σ(∆t)

(∆t)δ. The exponent δ experiences a

crossover from δ = 0.67

±

∼
0.03 when ∆t < 20 minute to δ = 0.51

0.06 when ∆t >

±

20 minutes. This is in agreement with the fact that the autocorrelation function of the

returns is exponentially decreasing in a characteristic time τch of 4 minutes. These results

predict that for ∆t > 20 minutes, the returns are essentially uncorrelated. Hence, important

15

scaling properties of the time series originate from time dependencies, but the autocorrelation

function of the returns or the time averaged variance do not deliver the relevant information

to study these dependencies. Higher order correlations need to be analyzed.

Since the time averaged volatility is a power-law, this invalidates the standard ARCH or

GARCH processes, because they all predict an exponentially decreasing volatility. In order

to explain the long range persistence of the correlations, one needs to include the entire

history of the returns.

Scalas [63] analyzed the statistics of the price diﬀerence between the closing price and

the opening price of the next day, taking week-ends and holidays as overnight variations. He

concentrated his analysis on Italian government bonds (BTP) futures for the 18 Sept 1991-

20 Feb 1997 period. As for other time scales variations, he did not observe short-range nor

long-range correlations in the price changes. In fact he was able to reproduce similar results

with a numerical simulation of a truncated trinomial random walk.

A general framework for treating superdiﬀusive systems is provided by the nonlinear

Fokker-Planck equation, which is associated with an underlying Ito-Langevin process. This

in turn has a very interesting connection to the nonextensive entropy proposed by Tsallis

: the nonlinear Fokker-Planck equation is solved by time-dependent distributions which

maximize the Tsallis entropy. This unexpected connection between thermostatistics and

anomalous diﬀusion gives an entirely new way, much beyond Bachelier [1] like approach, to

study the dynamics of ﬁnancial market as if there are anomalously diﬀusing systems.

Whence the intra-day price changes in the S&P500 stock index have been studied within

this framework by direct analysis and by simulation in refs.

[64, 65, 66]. The power-

law tails of the distributions, and the index’s anomalously diﬀusing dynamics, are very

accurately described. Results show good agreement between market data, Fokker-Planck

dynamics, and simulation. Thus the combination of the Tsallis non-extensive entropy and

the nonlinear Fokker-Planck equation unites in a very natural way the power-law tails of the

distributions and their superdiﬀusive dynamics. In our case the shape and tails of partial

distribution functions (PDF) for a ﬁnancial signal, i.e.

the S&P500 and the turbulent

nature of the markets were linked through Beck model, originally proposed to describe the

intermittent behavior of turbulent ﬂows. Looking at small and large time windows, both for

small and large log-returns. The market volatility (of normalized log-returns) distributions

was well ﬁtted with a χ2-distribution. The transition between the small time scale model

16

of nonextensive, intermittent process and the large scale Gaussian extensive homogeneous

ﬂuctuation picture was found to be at ca. a 200 day time lag. The intermittency exponent

(κ) in the framework of the Kolmogorov log-normal model was found to be related to the

scaling exponent of the PDF moments, -thereby giving weight to the model. The large value

of κ points to a large number of cascades in the turbulent process. The ﬁrst Kramers-Moyal

coeﬃcient in the Fokker-Planck equation is almost equal to zero, indicating ”no restoring

force”. A comparison is made between normalized log-returns and mere price increments.

The case of foreign exchange markets might have not been studied to my knowledge, and

might be reported here for the ﬁrst time (Fig. 1). Consider the time series of the normalized

log returns Z(t, ∆t) = (˜y(t)

< ˜y >∆t)/σ∆t for diﬀerent (selected) values of the time lag

−

FIG. 1: Probability distribution function P (∆x, ∆t) of normalized increments ∆x of daily closing

price value signal of DEM

U SD exchange rate between Jan. 01, 1971 and March 31, 2000, for

−

diﬀerent time lags: ∆t = 1, 2, 4, 8, 16, 32 days. The normalization is with respect to the width of

the PDF for ∆t = 32 days. The PDF symbols for a given ∆t are displaced by a factor 10 with

respect to the previous ∆t ; the curve for ∆t = 1 day is unmoved. See the tendency toward a

gaussian for increasing ∆t

17

∆t = 1, 2, 4, 8, 10, 32 days. Let τ = log2(32/∆t),

d
dτ

p(Z, τ ) =

D(1)(Z, τ ) +

∂
∂Z

(cid:20)−

∂
∂Z 2 D(2)(Z, τ )

(cid:21)

p(Z, τ )

(13)

in terms of a drift D(1)(Z,τ ) and a diﬀusion coeﬃcient D(2)(Z,τ ) (thus values of τ represent

∆ti, i = 1, ...).

The coeﬃcient functional dependence can be estimated directly from the moments M (k)

(known as Kramers-Moyal coeﬃcients) of the conditional probability distributions:

M (k) =

1
∆τ Z

′

′

dZ

(Z

′

Z)kp(Z

, τ + ∆τ

Z, τ )

−

|

D(k)(Z, τ ) =

limM (k)

1
k!

(14)

(15)

for ∆τ

0. According to Fig. 2 the drift coeﬃcient D(1)

0 and the diﬀusion coeﬃcient

→

≈

D(2) (Fig. 3) is not so well represented by a parabola as for the S&P . A greater deviation

from regular curves is also seen if one examines speciﬁc stock evolutions.

FIG. 2: Kramers-Moyal drift coeﬃcient M (1) as a function of normalized increments ∆x of daily

closing price value of DEM

U SD exchange rate between Jan. 01, 1971 and March 31, 2000, with

−

a best linear ﬁt for the central part of the data corresponding to a drift coeﬃcient D(1) =

1.41

−

18

It may be recalled that the observed quadratic dependence of the diﬀusion term D(2) is

essential for the logarithmic scaling of the intermittency parameter in studies on turbulence.

C. Crashes

Johansen and Sornette [67] stressed that the largest crashes of the XX

th century

−

appear as outliers, not belonging to the distribution of normal returns. The claim is easily

observable for the three major crashes (downturns of more than 22%), October 1987, World

War I and Wall Street 1929, in order of magnitude. It is also supported by the fact that in a

simulation of a million-year trading using a GARCH(1,1) model, with a reset every century,

never did 3 crashes occur within the same century. Another important property that points

towards a diﬀerent dynamical process for crashes is that, if anomalously large returns are for

instance deﬁned as a change of 10% or more of an index over a short time interval (a couple

FIG. 3: Kramers-Moyal diﬀusion coeﬃcient M (2) as a function of normalized increments ∆x of

daily closing price value of DEM

U SD exchange rate between Jan. 01, 1971 and March 31, 2000,

−

with a best parabolic ﬁt for the central part of the data corresponding to a diﬀusion coeﬃcient

D(2) = 0.055

19

of days at most), one observes only crashes (downturns) and no upsurge. This contrasts

with usual returns which have been observed to be symmetric for up and down moves.

In agreement with crashes being not part of the usual distribution of returns, Feigenbaum

and Freund [68] and Sornette, Johansen and Bouchaud [69] have independently proposed a

picture of crashes as critical points in a hierarchical system with discrete scaling. When a

system is at a critical point, it becomes ‘scale invariant’, meaning that it becomes statistically

similar to itself when considered at diﬀerent magniﬁcations. The signature of self-similarity

is power-laws, which are scale free laws. For systems with a hierarchical structure, the

system is self-similar only under speciﬁc scale changes. The signature of this property is

that the exponent of the previous power-laws has a non-zero imaginary part. Speciﬁcally, if

y(t) is the price at time t, it is supposed that

y(t) = A + B(tc −

t)µ[1 + C cos[ω log(tc −

t) + φ]]

(16)

in the regime preceeding a crash. The previous equation stipulates that tc is a critical time

where the prices will experience of phase transition. The signature of this transition is a

variation of the price as a power-law of exponent µ. But because of the hierarchical structure

in the system, the divergence is ‘decorated’ by log-periodic oscillations. The phase φ deﬁnes

the chosen unit of time scale.

It has appeared that the value of µ is not robust in the

non-linear ﬁts.

In a latter study, the S&P500 and the Dow Jones 1987 crash were examined [70] after

subtracting an exponential background due to the time evolution of these indices before

a ﬁtting with Eq. (16). Next it has been proposed to consider a logarithmic divergence,

corresponding to the µ = 0 limit, [71] rather than a power law, i.e.

y(t) = A + B ln

t
tc −
tc (cid:19) (cid:20)

(cid:18)

1 + Csin

ω ln

(cid:18)

t
tc −
tc (cid:19)

(cid:18)

+ φ

(cid:19)(cid:21)

f or t < tc

(17)

In so doing the analysis of (closing value) stock market index like the Dow Jones Industrial

average (DJIA), the S&P500 [71, 72] and DAX [73] leads to observe the precursor of so-

called crashes. This was shown on the Oct. 1987 and Oct. 1997 cases, as it has been

reported in the ﬁnancial press in due time [74, 75]. The prediction of the crash date was

made as early as July, in the 1997 case. It should be pointed out that we do not expect any

real divergence in the stock market indices, but to give upper bounds are surely of interest.

However, the reliability of the method has been questioned by Laloux et al.

[41] on the

20

grounds that all empirical ﬁts require a large number of parameters, that the statistical

data for crashes is obviously very restricted and that the method predicts crashes that never

happened. This can be further debated at other places.

Yet the existence of log-periodic corrections to scaling implies the existence of a hierarchy

of time scales δtn ≡
time scales are not expected to be universal, varying from a market to the other, but the

tn determined by half-periods of the cosine in Eq. (16). These

tc −

ratio λ = δtn+1/δtn could be robust with respect to changes in what is traded and when it

is traded, with λ

2.2

2.7, see Sornette [76].

≈

−

Sornette, Johansen and Bouchaud [69] identify the underlying hierarchical structure as

having the currency and trading blocks at its highest level (Euro, Dollar, Yen,...), countries

at the level below, major banks and institutions within a country at the level below, the

various departments of these institutions at the level below, and so on listed Several potential

sources can be listed [70] for discrete scaling: hierarchical structure of investors (see Table

I), investors with speciﬁc time horizon or periodic events like market closure and opening,

quaterly publication of reports, ...

TABLE I: Diﬀerent categories of investors and their relative weight on the London Stock Market

in 1993 after David [77]

Rank

Investor type

Weight

Pension Funds

Individuals

Insurance companies

Foreign

Unit Trusts

Others

Other personal sector

Industrial and commercial companies

Public Sector

Other Financial Institutions

1

2

3

4

5

6

7

8

9

10

11

34.2

17.7

17.3

16.3

6.6

2.3

1.6

1.5

1.3

0.6

0.6

Banks

21

Feigenbaum and Freund [68] had reported log-periodic oscillations for 1986-1987 weekly

data for the S&P500 (October 19, 1987 crash), 1962 monthly data for the S&P500 (NY crash

of 1962), 1929 monthly data for the Dow-Jones (NY 1929 crash) and 1990 scanned data for

the NIKKEI (Tokyo 1990 crash). Sornette, Johansen and Bouchaud[69] had also reported

log-periodic oscillations for July 1985 to the end of 1987 daily data for the S&P500. Since

then, this type of oscillations have been detected before many ﬁnancial indices downturns,

see refs. [71, 72] and ref. [76] for a review.

Johansen and Sornette [67] have also analyzed the symmetric phenomenon with respect

to the time, that is, a power-law decrease of the price after culmination at a maximum

value, the power-law being decorated by log-periodic oscillations.

Interestingly, they do

not obtain empirical evidence of a maximum with log-periodic oscillations before and after

the maximum. The authors advocate as main reason that when the market accelerates to

reach a log-periodic regime, it very often ended in a crash, with an associated symmetry

breaking. According to this argument, the symmetrical phenomenon can only be detected

whenever a crash did not happen before. They found empirical evidence of ‘antibubbles’ for

the NIKKEI, for the 31 Dec. 1989 till 31 Dec. 1998 period and for gold futures after 1980.

It seems that λ for decreasing markets could be higher (around 3.4

3.6) than for increasing

−

markets.

D. Crash models

Eliezer and Kogan [78] have considered the possibility of the existence of a crashing phase,

with diﬀerent dynamics from the normal or quiescent phase of the market. Their extension

is based on a model with two types of traders, but diﬀerent from the previous noise and

rational traders. The distinction comes from the way the agents place their orders. Agents

that place a limit order are visible to every one on the screen, while agents placing market

orders are not. Hence, it is impossible to have an annihilation event between agents placing

market orders, as they are ‘invisible’ to each others. This distinction between market order

agents and limit order agents was emphasized by Cohen et al. [79] as a possible source for

widening the spread between ask and bid prices.

22

E. The Maslov model

Maslov [80] proposed a model similar to the crash model of ref.

[78]. He assumes that

each agent can either trade a stock at the market price or place a limit order to sell or buy.

At each time step, a new trader appears and tries to make a transaction, buying or selling

being equally likely to occur. With probability qlo, he places a limit order, otherwise, he

trades at the best limit order available, the lowest ask for a buyer or the highest bid for

a seller. The price of the last deal is taken as the market price. The price of a new limit

order is equal to the market price oﬀset by a random number ∆ chosen from a probability

distribution, to avoid overlapping buy and sell orders. Agents making a deal are removed

from the system. In contrast to the previous models, the number of agents is not ﬁxed and

the agents are not able to diﬀuse. Once they have chosen a price, they can only wait until

someone matches their order.

Numerically, it is obtained that the model displays volatility clustering, a power-law

decrease of the autocorrelation function of absolute price change with an exponent γ = 1/2

(0.3 on real markets), a non-trivial Hurst exponent H = 1/4 (0.6-0.7 on real markets),

no correlation on the signs of the price changes. Most importantly, the price increment

distribution is non-Gaussian, with a crossover between two power-laws in the wings, from

an exponent 1 + α1 to 1 + α2 for larger changes, with α1 = 0.6

0.10 and α2 = 3.0

0.2. In

±

±

summary, the model is very promising because it has many qualitative interesting features,

but unfortunately a chronic lack of quantitative agreement with real data.

Within a mean-ﬁeld approximation of the model, Slanina [81] has shown that the station-

ary distribution for the price changes has have power-law tails with an exponent 1 + α = 2.

This result disagrees with numerical simulations of the model, both quantitatively and qual-

itatively, for reasons that remain unknown.

F. The sandpile model

It can be conjectured that stock markets are hierarchical objects where each level has

a diﬀerent weight and a diﬀerent characteristics time scale (the horizons of the investors).

The hierarchical lattice might be a fractal tree [72] with loops. The geometry might control

the type of criticality. This led to consider the type of avalanches which occur in a tumbling

23

sandpile as an analogy with the spikes in the index variation within a ﬁnancial bubble grow.

The Bak, Tang and Wiesenfeld (BTW) [82] in its sandpile version [83] was studied on a

Sierpinski gasket [84]. It has been shown that the avalanche dynamics is characterized by a

power law with a complex scaling exponent τ + iω with ω = 2π

ln 2. This was understood as
the result of the underlying Discrete Scale Invariance (DSI) of the gasket, i.e. the lattice is

translation invariant on a log-scale [76]. Such a study of the BTW model was extended to

studies in which were varying both the fractal dimension Df as well as the connectivity of

the lattice. In so doing connectivity-based corrections to power law scaling appeared. For

most avalanche distributions P (s)

s−τ , expressing the scale invariance of the dynamics,

∼

we have checked the power-law exponent (τ ) as a function of the fractal dimension of RSC

lattices and have found that τ seems to be dependent of the real part of the lattice (carpet)

fractal dimension

Df } →
We have observed signiﬁcant deviations of τ from 1.25, i.e. the d = 2 value. It seems that

. Notice that for

2, τ = 1.25

Df }

0.03.

ℜ{

ℜ{

±

the real part of the fractal dimension of the dissipative system is not the single parameter

controlling the value of τ . These oscillations (peaks) can be thought to originate from the

DSI of the RSC lattice as in [84], and to mimic those discussed in the main text for ﬁnancial

indices. We emphasize that the connectivity of the lattice is one of the most relevant

parameters. Notice that such log-periodic oscillations and linearly substructured peaks are

observed in the time distribution of avalanche durations P (t) as well.

G. Percolation models

Percolation theory was pioneered in the context of gelation, and later introduced in the

mathematical literature by Broadbent and Hammersley [85, 86] with the problem of ﬂuid

propagation in porous media. Since then, it has been successfully applied in many diﬀerent

area of research : e.g., earthquakes, electrical properties and immunology. Its application

to ﬁnancial market leads to one major conclusion: herding is a likely explanation for the

fat tails [87]. Percolation models assume a contrario to usual ﬁnancial theories that the

outcomes of decisions of individual agents may not be represented as independent random

variables. Such an assumption would ignore an essential ingredient of market organization,

namely the interaction and communication among agents. In real markets, agents may form

groups of various sizes which then may share information and act in coordination. In the

24

context of a ﬁnancial market, groups of traders may align their decisions and act in unison to

buy or sell; a diﬀerent interpretation of a ‘group’ may be an investment fund corresponding

to the wealth of several investors but managed by a single fund manager.

To capture the eﬀect of the interactions between agents, it is assumed that market par-

ticipants form groups or “clusters” through a random matching process but that no trading

takes places inside a given group: instead, members of a given group adopt a common market

strategy (for example, they decide to buy or sell or not to trade) and diﬀerent groups may

trade with each other through a centralized market process. In the context of a ﬁnancial

market, clusters may represent for example a group of investors participating in a mutual

fund.

H. The Cont-Bouchaud model

The Cont-Bouchaud model is an application of bond percolation on a random graph, an

approach ﬁrst suggested by Kirman [88] in the economics literature. Each site of the graph

is supposed to be an agent, which is able to make a transaction of one unit on a market. Two

agents are connected with each other with probability p = c/N. At any time, an agent is

either buying with probability a, selling with probability a or doing nothing with probability

1

2a, with a

(0, 1/2).

−

∈

The exogenous parameter a controls the transaction rate, or the time scale considered; a

close to 0 means short time horizon of the order of a minute on ﬁnancial markets, with only

a few transactions per time steps. The number of traders which are active during one time

interval increases with the length of this time interval. All agents belonging to the same

cluster are making the same decision at the same time. The cluster distribution represents

the network of social interactions, which induces agents to act cooperatively. This network

is supposed to model the phenomenon of herding. The aggregate excess demand for an asset

at time t is the sum of the decision of all agents,

if the demand of agent i is φi ∈ {−
}
directly accessible, so that it has to be related to the returns through some R = F (D).

1 representing a sell order. D(t) is not

1, 0, +1

−

(18)

D(t) =

φi(t),

N

Xi=1
, φi =

25

There is unfortunately no deﬁnite consensus about the analytic form of F . This will

prevent a convincing quantitative comparison between the model and empirical data. Nev-

ertheless, for purpose of illustration, we will assume that the price change during a time

interval is proportional to the sum of the demand and sell orders from all the clusters

which are active during this time interval. That is, we consider F (D)

D, unless speci-

∼

ﬁed otherwise. For c = 1 the probability density for the cluster size distribution decreases

asymptotically as a power law

(19)

(20)

(21)

while for 0 < 1

c << 1, the cluster size distribution is cut oﬀ by an exponential tail,

−

ns∼s→∞

A
s5/2

ns∼s→∞

A
s5/2 exp

(c

1)s
−
s0 (cid:19)

(cid:18)−

For c=1, the distribution has an inﬁnite variance while for c < 1 the variance becomes

ﬁnite because of the exponential tail. In this case the average size of a coalition is of order

1/(1

c) and the average number of clusters is of order N(1

c/2). Setting the coordination

−

−

parameter c close to 1 means that each agent tends to establish a link with one other agent.

In the limit N

, the number νi of neighbors of a given agent i is a Poisson random

variable with parameter c,

→ ∞

P (νi = ν) = e−c cν
ν!

The previous results are based in the assumption that either exactly one cluster of traders

is making a transaction at each time step or none. If the time interval of observation is in-

creased enough to allow each cluster to be considered once during a time step, numerous

clusters could decide to buy or sell, depending on the value of a. In this case, the probability

distribution of the returns will be diﬀerent from the cluster size distribution. By increasing a

from close to 0 to a close to 1/2, Stauﬀer and Penna (1998) have shown that the probability

distribution of the returns changes from a exponentially truncated power-law distribution

to a Gaussian distribution. At intermediate value, a continuous distribution with a smooth

peak and power-law in the tails only is obtained, in agreement with a Levy distribution.

This situation is similar to a change in time scale. At short time scales, like every minute,

there is either an order that is placed, or none, while for longer time scales, the observed

variations on ﬁnancial markets are the average result of thousands of orders. Increasing a

26

from 0 to 1/2 is similar to changing the time scale from short-time scales to large-time scale.

Gopikrishnan et al. [59] have observed empirically this convergence towards a Gaussian dis-

tribution by changing the time scale for ﬁnancial market indices. Recall that Ausloos and

Ivanova [65] discussed the fat tails in another way, arguing that they are caused by some

”dynamical process” through a hierarchical cascade of short and long-range volatility corre-

lations. Unfortunately, there are no correlations in the Cont-Bouchaud model to compare

with this result.

The Cont-Bouchaud model predicts a power-law distribution for the size of associations,

with an exponential cut-oﬀ. When c reaches one, a ﬁnite fraction of the market shares

simultaneously the same opinion and this leads to a crash. Unfortunately, this crash lacks

any precursor pattern because of the lack of time correlations, amongst other possible short-

comings. From the expected relation R = F (D), it leads similarly to power-law tails for the

distribution of returns. However, the exact value of the exponent of this power-law is still

a matter of debate because of the lack of consensus upon F (D). We report in Table II a

summary of the agreement of the model and its limitations.

TABLE II: Summary of the type of agreement between empirical data and Cont-Bouchaud model

Property Agreement

Fat tails Qualitative

Crossover

Symmetric

Clustering

Crashes

Precursors

No

Yes

No

Yes

No

Eguil´uz and Zimmermann [89] introduced a dynamical version of the Cont-Bouchaud

model. At each time step, an agent is selected at random. With probability 1

2a, she selects

−

another agent and a link between those two agents is created. Otherwise, this agent triggers

her cluster to make a transaction, buying with probability a, selling with probability a.

Eguil´uz and Zimmermann associated the creation of a link with the exchange of information

between agents, while when the agents make a transaction, they make their information

public, that is, this information is lost. D’Hulst and Rodgers [90] have shown that this

27

model is equivalent to the Cont-Bouchaud model, except that with the new interpretation,

the probability that a cluster of size s makes transaction is sns rather than ns as in the

Cont-Bouchaud model. Extension of the model allowing for the cluster distribution not to

change can be envisaged.

Finally, we should mention that numerical simulations show that it is possible to observe

power-laws with higher than expected eﬀective exponent. In other words, size eﬀects can

modify the exponent. As all empirical data are strongly aﬀected by size eﬀects, looking after

a model that reproduces the exact value of the exponent seems less important than trying

to justify why the exponent should have a given value.

I. Crash precursor patterns

In the Cont-Bouchaud model for p

pc, there is a non-zero probability that a ﬁnite

≥

fraction of agents decide to buy or sell. These large cooperative events generate anomalous

wings in the tails of the distribution, that is, it appears as outliers of the distribution.

The same pattern has been observed by Johansen and Sornette for crashes on ﬁnancial

markets, which means that the decision from buying or selling of the percolating cluster can

be compared to a crash. These crashes however lack any precursor patterns, which have

been empirically observed for real crashes, as originally proposed by Sornette, Johansen and

Bouchaud.

As explained by Stauﬀer and Sornette [91] log-periodic oscillations, a crash precursor

signature, can be produced with biased diﬀusion. The sites of a large lattice are randomly

initialised as being accessible with probability p, or forbidden, with probability 1

p. A

−

random-walker diﬀuses on the accessible sites only. With probability B, the random-walk

moves in one ﬁxed direction, while with probability 1

B, it moves to one randomly selected

−

nearest neighbour. In both cases, the move is allowed only if the neighbour is accessible.

Writing the time variation of the mean-square displacement from a starting point as < r2 >

∼
tk, k changes smoothly as a function of the logarithm of the time. That is, k(t) approaches

unity while oscillating according to sin(λ ln t). Behind these log-periodic oscillations is the

fact that the random-walker gets trapped in clusters of forbidden sites. The trapping time

depends exponentially on the length of the traps, that are multiple of the lattice mesh size.

The resulting intermittent random-walk is thus punctuated by the successive encounters of

28

larger and larger clusters of trapping sites. This result is much more eﬀective in reduced

dimensionality, or equivalently when B

1.

→

We have to stress that, even if this model is very closely related to percolation, there exists

no explanation on how to relate the distance traveled by a diﬀusing particle to transactions

on ﬁnancial markets. Connection to the sandpile model is still to be worked out. Hence,

this model is not an extension of the Cont-Bouchaud model of ﬁnancial markets, but rather

a hint of a possible direction for on how implement log-periodic oscillations.

Notice that within the framework of the Langevin equation proposed by Bouchaud and

Cont [92], a crash occurs after an improbable succession of unfavorable events, because they

are initiated by the noise term. Hence, no precursor pattern can be identiﬁed within the

original equation. Bouchaud and Cont extended their model and proposed the following

mechanism to explain log-periodic oscillations. Every time an anomalously negative value u

close to u∗ is reached, the market becomes more ‘nervous’. This is similar to saying that the

width W of the distribution describing the noise term η increases to W + δW . Therefore,

the time ∆t between the next anomalously large negative value will be shorten as large

ﬂuctuations become more likely. The model predicts

∆tn+1 = ∆tnS−δW/W

(22)

where, to linear order in δW , S is some constant. This leads to a roughly log-periodic be-

haviour, with the time diﬀerence between two events being a geometric series. The previous

scenario predicts that a crash is not related to a critical point. That is, there is a crash

because u reaches u∗, not because ∆t

0.

→

IV. THE LUX-MARCHESI MODEL

Lux and Marchesi [93] have introduced a model of ﬁnancial market where the agents are

divided in two groups, fundamentalists and noise traders. Fundamentalists expect the price

to follow the so-called fundamental value of the asset (pf ), which is the discounted sum of

expected future earnings (for example, dividend payments). A fundamentalist strategy is

to buy (sell) when the actual price is believed to be below (above) pf . The noise traders

attempt to identify price trends and patterns and also consider the behaviour of other traders

as a source of information. The noise traders are also considered as optimistic or pessimistic.

29

The former tends to buy because they thing the market will be raising, while the latter bet

on a decreasing market and rush out of the market to avoid losses.

The dynamics of the model are based on the movements of agents from one group to the

other. A switch from one group to the other happens with a certain exponential probability

νeU (t)∆t, varying with time. ν is a parameter for the frequency of revaluation of opinions

or strategies by the agents. U(t) is a forcing term covering the factors that are relevant for

a change of behaviour, and it depends on the type of agent considered. Noise traders use

the price trend in the last time steps and the diﬀerence between the number of optimistic

and pessimistic traders to calculate U(t) corresponding to transitions between the group of

optimists and pessimists. For example, observation of a positive trend and more optimists

than pessimists is an indication of a continuation of the rising market. This would induce

some pessimistic agents to become optimistic. The other type of transitions is between

fundamentalists and noise traders. The U(t) function corresponding to such transitions is

a function of the diﬀerence in proﬁts made by agents in each group. An optimistic trader

proﬁt consists in short-term capital gain (loss) due to price increase (decrease), while a

pessimistic trader gain is given by the diﬀerence between the average proﬁt rate of alternative

investments (assumed to be constant) minus the price change of the asset they sell. A

fundamentalist proﬁt is made oﬀ the diﬀerence between the fundamental price and the

actual price, that is,

is associated to an arbitrage opportunity. In practice, Lux and

p

|

−

pf |

Marchesi (1999) multiply this arbitrage proﬁt by a factor < 1 to take into account the time

it takes some time for the actual price to reverse to its fundamental value.

To complete the model, it remains to specify how the price and its fundamental value

are updated. Price changes are assumed to be proportional to the aggregate excess demand.

Optimistic noise traders are buying, pessimistic noise traders are selling and fundamentalists
are all buying or all selling depending if pf −
relative changes in the fundamental price are Gaussian random variable, ln pf,t−
where ǫt is a normally distributed random variable, with mean zero and time-invariant

p is positive or negative, respectively. Finally,

ln pf,t−1 = ǫt,

variance σ2
ǫ .

Lux and Marchesi claimed that a theoretical analysis of their model shows that the

stationary states of the dynamics are characterized by a price which is on average equal to

its fundamental value. This is supported by numerical simulations of the model, where it

can be seen that the price tracks the variation of its fundamental value. But comparing

30

price returns, by construction, the fundamental returns are Gaussian, while price returns

are not. The distribution of price returns display fat tails that can be characterized by a

power-law of exponent τ = 1 + α. Numerically, Lux and Marchesi obtained α = 2.64 when

sampling at every time step. Increasing the time lag, a convergence towards a Gaussian is

observed. They measured a Hurst exponent H = 0.85 for the price returns, showing strong

persistence in the volatility. The exact value of the exponent is close to empirical data.

The behaviour of the model comes from an alternation between quiet and turbulent

periods. The switch between the diﬀerent types of periods are generated by the movements

of the agents. Turbulent periods are characterized by a large number of noise traders. There

exists a critical value Nc for the number of traders, such as when there are more than Nc

traders, the system looses stability. However, the ensuing destabilization is only temporary

as the presence of fundamentalists and the possibility to become a fundamentalist ensure

that the market always stabilizes again. These temporary destabilizations are known as

on-oﬀ intermittency in physics.

A. The spin models

There are necessarily crash and ﬁnancial market aspects which resemble phase transitions;

this has led into producing a realm of spin models. The superspin model proposed by

Chowdhury and Stauﬀer [94] is related to the Cont-Bouchaud model presented here above.

The cluster of connected agents are replaced by a superspin of size S.

Si|
is chosen initially from a probability

is the number of

|

agents associated with the superspin i. The value of
Si|

Si|
−(1+α). A superspin can be in three diﬀerent states, +

distribution P (

Si|

∼ |

)

|

|

. Associated to each state is a ‘disagreement function’ Ei =

, 0 or

Si|

|
Si(Hi + hi), where

−

j6=i Sj is a local ﬁeld and hi an individual bias. Ei represents the disagreement
between the actual value of a superspin and two other factors. One of these factors, Hi,

P

Si|
−|
Hi = J

is an average over the decision of the other groups of agents. The other factor, hi, is a

personal bias in the character of a group of agents, with groups of optimists for hi > 0, and

pessimists for hi < 0. Groups with hi = 0 are pure noise traders. At each time step, every
Si|
2a (doing nothing). A superspin is allowed to

superspin i chooses one of its three possible states, +
|

with probability a or 0 with probability 1

with probability a (buying) ,

Si|

−|

change from its present state to the new chosen state with probability e−∆Ei/kbT , where ∆Ei

−

31

is the expected change in its disagreement function. The magnetization M corresponds to

the aggregate excess demand, D.

If a linear relation F (D) is assumed between the returns and D, the previous model is

characterized by a probability distribution for the prices with power-law tails with the same

exponent 1 + α, as the distribution of spins. In contrast to the Cont-Bouchaud model, this

result stays true for all values of a, that is, there is no convergence towards a Gaussian for

any value of a, contrary to what is observed on ﬁnancial markets. Using hi, it is possible to
divide the population in noise traders for hi ≪
also possible to introduce a dynamics in hi to reﬂect the diﬀerent opinions in diﬀerent states

Hi and fundamentalists for hi ≫

Hi. It is

of the market. No complete study of these properties has been done to date.

Acknowledgments

The author thanks Ren´e D’Hulst for an important contribution to this work. MA would

like to thank the many coworkers who helped to make this review possible, i.e., in the last

few years, Ph. Bronlet and K. Ivanova. Part of this work has been supported through the

Minister of Education under contract ARC (94-99/174) and (02/07-293) of ULg.

[1] Bachelier, L., Ann. Sci. Ecole Norm. Sup. 3 (1900), p.21

[2] Pareto, V., Cours d’Economie Politique, Lausanne and Paris, 1897

[3] Mandelbrot, B. B., J. Business 36 (1963), p.294-298

[4] Fama, E. F., J. Business 35 (1963), pp.420-429

[5] L´evy, P., Th´eorie de l’Addition des Variables Al´eatoires, Gauthier-Villars, Paris, 1937

[6] Mantegna, R. N., Stanley, H. E., Phys. Rev. Lett. 73 (1994), pp.2946-2949

[7] Koponen, I., Phys. Rev. E 52 (1995), pp.1197-1199

[8] Anderson, P. W., Arrow, K., Pines, D., The Economy as an Evolving Complex System,

Addison-Wesley, Reading, MA, 1988

[9] Bouchaud, J.-P., Potters, M., Th´eorie des Risques Financiers, Al´ea-Saclay, Eyrolles,

1997; Theory of Financial Risks, Cambridge University Press, Cambridge, 2000

32

[10] Mantegna, R. N., Stanley, H. E., An Introduction to Econophysics: Correlations and

Complexity in Finance Cambridge University Press, Cambridge, 1999

[11] Paul, W., Baschnagel, J., Stochastic Processes from Physics to Finance, Springer, 1999

[12] Voit, J., The Statistical Mechanics of Financial Markets, Springer Verlag, 2001

[13] Ramsden, J. J., Kiss-Hayp´al, G., Physica A 277 (2000), pp.220-227

[14] Vandewalle, N., Ausloos, M., Physica A 246 (1997), pp.454-459

[15] Buldyrev, S., Dokholyan, N. V., Goldberger, A. L., Havlin, S., Peng, C. K.,

Stanley, H. E., Viswanathan, G. M., Physica A 249 (1998), pp.430-438

[16] West, B. J., Deering, B., The Lure of Modern Science: Fractal Thinking, World Scientiﬁc,

Singapore, 1995

[17] Malamud, B. D., Turcotte, D. L., J. Stat. Plan. Infer. 80 (1999), pp.173-196

[18] Ivanova, K., Ausloos, M., Physica A 265 (1999), pp.279-286

[19] Ausloos, M., Ivanova, K., Braz. J. Phys. 34 (2004), pp.504-511

[20] Ausloos, M. Physica A 285 (2000), pp.48-65

[21] Vandewalle, N., Ausloos, M., Int. J. Phys. C 9 (1998), pp.711-720

[22] Ausloos, M., Vandewalle, N., Boveroux, Ph., Minguet, A., Ivanova, K., Physica

A 274 (1999), pp.229-240

[23] Ausloos, M., Ivanova, K., Physica A 286 (2000), pp.353-366

[24] Ausloos, M., Ivanova, K., Int. J. Mod. Phys. C 12 (2001), pp.169-196

[25] Ausloos, M., Ivanova, K., Eur. Phys. J. B 27 (2002), pp.239-247

[26] Ivanova, K., Ausloos, M., Eur. Phys. J. B 20 (2001), pp.537-541

[27] Ausloos, M., Ivanova, K., in New Directions in Statistical Physics - Econophysics, Bioin-

formatics, and Pattern Recognition, (Ed. L.T. Wille), Springer Verlag, Berlin, 2004) pp.93-114

[28] Vandewalle, N., Ausloos, M., Int. J. Comput. Anticipat. Syst., 1 (1998), pp.342-349

[29] Alesio, E., Carbone, A., Castelli, G., Frappietro, V., Eur. J. Phys. B 27 (2002),

pp.197-200

[30] Vandewalle, N., Ausloos, M., Boveroux, Ph., Physica A 269 (1999), pp.170-176

[31] Ausloos, M., Ivanova, K., Comp. Phys. Commun. 147 (2002), pp.582-585

[32] Ivanova, K., Ausloos, M., Eur. Phys. J. B 8 (1999), pp.665-669; Err. 12 (1999), 613

[33] Vandewalle, N., Ausloos, M., Eur. J. Phys. B 4 (1998), pp.257-261

[34] Lux, T., Ausloos, M., in The Science of Disaster : Scaling Laws Governing Weather, Body,

33

Stock-Market Dynamics, (Eds. A. Bunde, J. Kropp, H.-J. Schellnhuber), Springer Verlag,

[35] Zipf, G. K., Human Behavior and the Principle of Least Eﬀort, Addison Wesley, Cambridge

Berlin, 2002, pp.377-413

MA, 1949

[36] Vandewalle, N., Ausloos, M., Physica A 268 (1999), pp.240-249

[37] Ausloos, M., Ivanova, K., Physica A 270 (1999), pp.526-542

[38] Bronlet, Ph., Ausloos, M., Int. J. Mod. Phys. C 14 (2003) pp.351-365

[39] Ausloos, M., Bronlet, Ph., Physica A 324 (2003) pp.30-37

[40] Laloux, L., Cizeau, P., Bouchaud, J.-P., Potters, M., Phys. Rev. Lett. 83 (1999),

[41] Laloux, L., Potters, M., Cont, R., Aguilar, J.-P., Bouchaud, J.-P., Europhys. Lett.

[42] Plerou, V., Amaral, L. A. N., Gopikrishnan, P., Meyer, M., Stanley, H. E., Nature

pp.1467-1470

45 (1999), pp.1-5

400 (1999), pp.433-437

[43] Plerou, V., Gopikrishnan, P., Rosenow, B., Amaral, L. A. N., Stanley, H. E.,

Phys. Rev. Lett. 83 (1999), pp.1471-1474

[44] Mantegna, R. N., Eur. Phys. J. B 11 (1999), pp.193-197

[45] Bonanno, G., Vandewalle, N., Mantegna, R. N., Phys. Rev. E 62 (2000), pp.R7615-

R7618

[46] Vandewalle, N., Boveroux, P., Brisbois, F., Eur. Phys. J. B 15 (2000), pp.547-549

[47] Fabretti, A., Ausloos, M., Int. J. Mod. Phys. C 16 (2005), pp.671-706

[48] Eckmann, J. P., Kamphorst, S. O., Ruelle, D., Europhys Lett. 4 (1987), pp.973-977

[49] Zbilut, J. P., Webber, C. L., Phys Lett A 171 (1992), pp.199-203

[50] Zbilut, J. P., Webber, C. L., Giuliani, A., Phys Lett A 246 (1998), pp.122-128

[51] Lambertz, M., Vandenhouten, R., Grebe, R., Langhorst, P., Journal of the Auto-

nomic Nervous System 78 (2000), pp.141-157

[52] Engle, R. F., Econometrica 50 (1982), pp.987-1008

[53] Bollerslev, T., J. Econometrics 31 (1986), pp.307-327

[54] Engle, R. F., Bollerslev, T., Econometric Reviews 5 (1986), pp.1-50

[55] Nelson, D. B., Econometrica 59 (1991), pp.347-370

[56] Baillie, R. T., Bollerslev, T., Mikkelsen, H. O., J. of Econometrics 74 (1996), pp.3-

34

30

[57] Mantegna, R. N., Stanley, H. E., Nature 376 (1995), pp.46-49

[58] Mantegna, R. N., Stanley, H. E., Physica A 254 (1998), pp.77-84

[59] Gopikrishnan, P., Plerou, V., Amaral, L. A. N., Meyer, M., Stanley, H. E., Phys.

[60] Gopikrishnan, P., Meyer, M., Amaral, L. A. N., Stanley, H. E., Eur. Phys. J. B 3

Rev. E 60 (1999), pp.5305-5316

(1998), pp.139-140

[61] Lux, T., Applied Financial Economics 6 (1996), pp.463-475

[62] Plerou, V., Gopikrishnan, P., Amaral, L. A. N., Gabaix, X., Stanley, H. E., Phys.

Rev. E 62 (2000), R3023-R3026

[63] Scalas, E., Physica A 253 (1998), pp.394-402

[64] Michael, F., Johnson, M. D., Physica A 320 (2003), pp.525-534

[65] Ausloos, M., Ivanova, K., Phys. Rev. E 68 (2003), p.046122

[66] Tsallis, C., Anteneodo, C., Borland, L., Osorio, R., Physica A 324 (2003), pp.89-100

[67] Johansen, A., Sornette, D., Eur. Phys. J. B 1 (1998), pp.141-143

[68] Feigenbaum, J. A., Freund, P. G. O., Int. J. Mod. Phys B 10 (1996), pp.3737-3745

[69] Sornette, D., Johansen, A., Bouchaud, J.-P., J. Phys. I 6 (1996), pp.167-175

[70] Vandewalle, N., Boveroux, P., Minguet, A., Ausloos, M., Physica A 255 (1998),

pp.201-210

p.139-141

pp.355-359

[71] Vandewalle, N., Ausloos, M., Boveroux, P., Minguet, A., Eur. Phys. J. B 4 (1998),

[72] Vandewalle, N., Ausloos, M., Boveroux, P., Minguet, A., Eur. J. Phys. B 9 (1999),

[73] Drozdz, S., Ruf, F., Speth, J., W´ojcik, M., Eur. Phys. J. B 10 (1999), pp.589-593

[74] Dupuis, H, Trends/Tendances 22 (38) (1997), p.26; ibid. 22 (44) (1997), p.11

[77] David, B., Transaction Survey 1994 Stock Exchange Quaterly, London Stock Exchange,

[75] Legrand, G., Cash 4 (38) (1997), p.3

[76] Sornette, D., Phys. Rep. 297 (1998), pp.239-270

October-December 1994

[78] Eliezer, D., Kogan, I. I., cond-mat/9808240 (1998)

[79] Cohen, K., Maier, S., Schwartz, R., Whitcomb, W., J. Pol. Econ. 89 (1981), pp.287-

35

305

[80] Maslov, S., Physica A 278 (2000), pp.571-578

[81] Slanina, F., Physica A 286 (2000), pp.367-376

[82] Bak, P., Tang, C., Wiesenfeld, K., Phys. Rev. A 38 (1988), pp.364-374

[83] Bak, P., How Nature Works Copernicus, New York, 1996

[84] Ausloos, M., Ivanova, K., Vandewalle, N., in Empirical sciences of ﬁnancial ﬂuctua-

tions. The advent of econophysics, Tokyo, Japan, Nov. 15-17, 2000 Proc.; (Ed. H. Takayasu)

[85] Stauffer, D., Aharony, A., Introduction to Percolation Theory, 2nd Ed. Taylor & Francis,

Springer Verlag, Berlin, 2002 pp.62-76

London, 1991

[86] Sahimi, M, Applications of Percolation Theory, Taylor & Francis, London, 1994

[87] Cont, R., Bouchaud, J.-P., Macroeconomics Dynamics 4 (2000), pp.170-196

[88] Kirman, A., Economics Letters 12 (1983), pp.101-108

[89] Egu´iluz, V. M., Zimmermann, M. G., Phys. Rev. Lett. 85 (2000), p.5659-5662

[90] D’Hulst, R., Rodgers, G. J., Int. J. Theo. App. Finance 3 (2000), p.609

[91] Stauffer, D., Sornette, D., Physica A 252 (1998), pp.271-277

[92] Bouchaud, J.-P., Cont, R., Eur. Phys. J. B 6 (1998), pp.543-550

[93] Lux, T., Marchesi, M., Nature 397 (1999), pp.498-500

[94] Chowdhury, D., Stauffer, D., Eur. Phys. J. B 8 (1999), pp.477-482

36


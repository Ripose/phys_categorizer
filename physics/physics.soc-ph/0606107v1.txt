6
0
0
2
 
n
u
J
 
3
1
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
7
0
1
6
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

TOPICAL REVIEW

Statistical mechanics of socio-economic systems
with heterogeneous agents

Andrea De Martino† and Matteo Marsili‡
CNR/INFM, Dipartimento di Fisica, Universit`a di Roma “La Sapienza”, p.le A.
†
Moro 2, 00185 Roma (Italy)
The Abdus Salam International Centre for Theoretical Physics, Strada Costiera 11,
‡
34014 Trieste (Italy)

E-mail: andrea.demartino@roma1.infn.it,marsili@ictp.trieste.it

Abstract.

We review the statistical mechanics approach to the study of the emerging collective
behavior of systems of heterogeneous interacting agents. The general framework
is presented through examples is such contexts as ecosystem dynamics and traﬃc
modeling. We then focus on the analysis of the optimal properties of large random
resource-allocation problems and on Minority Games and related models of speculative
trading in ﬁnancial markets, discussing a number of extensions including multi-asset
models, Majority Games and models with asymmetric information. Finally, we
summarize the main conclusions and outline the major open problems and limitations
of the approach.

PACS numbers:

Contents

1 Introduction

3

2 Statistical mechanics of resource allocation: some examples

2.1 General considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 A simple model of ecological resource competition . . . . . . . . . . . . .
2.2.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
2.2.2
2.2.3

5
5
7
7
9
Statics (replica approach)
Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 The El Farol problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3.2 Macroscopic properties . . . . . . . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . 18
2.3.3 Dynamics (continuous-time limit approach)
2.4 Buyers and sellers in the ‘ﬁsh market’ . . . . . . . . . . . . . . . . . . . . 21
2.5 Route choice behavior and urban traﬃc . . . . . . . . . . . . . . . . . . . 24

CONTENTS

2

3 Optimal properties of large random economies

28
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.2 Meeting demands at minimum costs . . . . . . . . . . . . . . . . . . . . . 29
. . . . . . . . . . . . . . . . . 31
3.3 Competitive equilibria of linear economies
3.3.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Statistical mechanics with a single consumer . . . . . . . . . . . . 32
3.3.2
3.3.3 Case of many consumers . . . . . . . . . . . . . . . . . . . . . . . 37
3.4 Economic growth: the Von Neumann problem . . . . . . . . . . . . . . . 38

4 Toy models of ﬁnancial markets: Minority Games

41
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2 From agents’ expectations to the minority (and majority) rule . . . . . . 42
4.3 The simplest Minority Game . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.4 The Minority Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.5 Statistical mechanics of the MG: static approach . . . . . . . . . . . . . . 48
4.6 The role of learning rates and decision noise . . . . . . . . . . . . . . . . 51
4.7 The role of market impact . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.8 Exogenous vs endogenous information . . . . . . . . . . . . . . . . . . . . 58

5 Extensions and generalizations

59
5.1 Grand-canonical Minority Game and stylized facts . . . . . . . . . . . . . 60
5.2 Market ecology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.3 Multi-asset Minority Games . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.3.1 Deﬁnitions and results . . . . . . . . . . . . . . . . . . . . . . . . 65
5.3.2 Dynamics (path-integral approach) . . . . . . . . . . . . . . . . . 68
5.4 The Majority Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.5 Models with interacting trend-followers and contrarians . . . . . . . . . . 75
5.6 Markets with asymmetric information . . . . . . . . . . . . . . . . . . . . 80

6 Conclusions

84

CONTENTS

1. Introduction

3

Collective phenomena in economics, social sciences and ecology are very attractive
for statistical physicists, especially in view of the empirical abundance of non-trivial
ﬂuctuation patterns and statistical regularities – think of returns in ﬁnancial markets or
of allometric scaling in ecosystems – which pose intriguing theoretical challenges. On an
abstract level, the problems at stake are indeed not too diﬀerent from, say, understanding
how spontaneous magnetization may arise in a magnetic system, since what one wants
in both cases is to understand how the eﬀects of interactions at the microscopic scale
can build up to the macroscopic scale. Clearly, ecologies or ﬁnancial markets are quite
more complex systems than magnets, being composed of units which themselves follow
complex (and far from understood) behavioral rules. Still, in many cases it may be
reasonable to assume that the collective behavior of a crowd of individuals presents
aspects of a purely statistical nature which might be appreciated already in highly
stylized models of such systems. This is ultimately the rationale for applying statistical
mechanics to such problems.

In general, statistical physics oﬀers a set of concepts (e.g. order parameters and
scaling laws) and tools (both analytical and numerical) allowing for a characterization in
terms of phases and phase transitions which might be useful in shaping the way we think
about such complex systems. The considerable progress achieved in the last decades
in the statistical mechanics of non-equilibrium processes and of disordered systems,
thanks to which it is now possible to deal eﬀectively with ﬂuctuations and heterogeneity
(respectively) in systems with many interacting degrees of freedom,
is particularly
important for socio-economic applications. In fact, while equilibrium and homogeneity
are important in physics, non-equilibrium and heterogeneity are the rule in economics, as
each individual is diﬀerent both in his/her characteristics and in the way he/she interacts
with the environment. Deriving general macroscopic laws taking the speciﬁc details of
each and every individual’s behavior into account is a desperate task. However, as long
as one is interested in collective properties, a system with complicated heterogeneous
interactions can be reasonably well represented as one with random couplings [1]. In
the limit of inﬁnite system size, some of the relevant macroscopic observables will be
subject to laws of large numbers, i.e. some quantities will be self-averaging, and, if
the microscopic dynamics follows suﬃciently simple rules, one may hope to be able to
calculate them explicitly. It is with these properties – which we call typical – that the
statistical mechanics approach is concerned.

In what follows, we shall mostly concentrate on problems arising in economics
and ﬁnance. When modeling these systems one must be aware that their microscopic
behavior is very diﬀerent from that governing particles or atoms in physics. Economic
agents typically respond to incentives and act in a selﬁsh way. This is usually modeled
assuming that individuals strive to maximize their private utility functions, with no
regard for social welfare. Not only agents might have conﬂicting goals, as their utility
functions will in general be diﬀerent, but their selﬁsh behavior may lead to globally

CONTENTS

4

ineﬃcient outcomes – e.g. to a coordination failure or to a lack of cooperation. Such
outcomes, called Nash equilibria in Game Theory, are in general diﬀerent from socially
optimal states where the total utility is maximized. Hence, generally, in a system of
interacting agents there is no global energy function to be minimized.

Another important diﬀerence between the dynamics of a physical system, such as
a magnetic material, and that of an economic system is that, while in the former spins
at a particular time depend at most on the past states of the system, in the latter the
agents’ choices also depend on the expectations which they harbor about the future
states. This suggests that the collective dynamics may have a non-causal component
(indeed, backward induction in time plays a big role in the strategic reasoning of rational
agents [2]). In many cases, however, it is reasonable to assume that agents are boundedly
rational or ‘inductive’, i.e. that their behavior as well as their expectations adjust as a
result of experience. We shall concentrate our analysis to these cases of adaptive agents
following a learning dynamics. We shall see that the lack of a global Hamiltonian is
reﬂected in the fact that such a dynamics, in general, violates detailed balance.

Actually, in many cases it is realistic to assume that agents behave as if they were
interacting with a system as a whole – be it a market or the crowd – rather than
directly with a number of other individuals. In economics, this is termed a price-taking
assumption, because it amounts to stating that agents act as if prices do not depend
on what they actually decide to buy or sell (i.e. they take prices as given), and it is
usually justiﬁed by saying that the contribution of a single agent to the total demand is
negligible when the number of agents is large. The equilibria of systems where agents
behave as price-takers are called competitive equilibria. However, prices depend on the
aggregate demand and supply and hence on the choice of each agent, and the statistical
physics approach provides a very transparent description of how price-taking behavior
modiﬁes the global properties of a system.

This review gives a survey of some recent quantitative developments on the
statistical mechanics of systems of many interacting adaptive agents. This is a subject
that has been shaped over the past few years around a few basic models (like the El
Farol problem) and a few analytical techniques, mostly borrowed from the mean-ﬁeld
theory of spin glasses (like the replica method). The models, though highly stylized
to an economist’s eyes, possess a strong physical content and in many cases provide
important indications as to whether the phenomenology of real systems is speciﬁc of
each of their particular natures or rather it is generic of large systems of adaptive units
interacting competitively. Ultimately, it is not too unfair to say that separating system-
speciﬁc features from general features can be seen as the main contribution statistical
physics can provide to this ﬁeld (besides techniques).

Our choice of arguments is clearly biased, and the reader may dispose of several
recent books that cover some of the important issues (especially ﬁnance-inspired) we
merely touch here [3–7]. Along with a core of problems related to the emergence of non-
trivial ﬂuctuation phenomena, cooperation and eﬃciency (understanding which has been
the original goal of these studies), other issues such as the impact of diﬀerent information

CONTENTS

5

structures or the interaction between diﬀerent multi-agent systems have just started to
be analyzed and are likely to attract a great deal of attention in the near future. On
the physical side, precisely because of the diﬀerences in the microscopic modeling of
economics and physics, these systems pose a number of fascinating questions that open
several directions for further work, some of which will be outlined here.

The review is organized as follows.

In Sec. 2 we present a general discussion
of resource allocation by complex adaptive systems and a few exemplary models from
diﬀerent contexts like ecology and traﬃc dynamics, including the El Farol problem. Sec.
3 is devoted to the statistical analysis of optimal properties of large random economies,
that is, more precisely, to a survey of the macroscopic properties of classical economic
optimization problems. Most of our attention will be on the model of competitive
equilibrium for linear production economies and on Von Neumann’s model of economic
growth. In Sec. 4 we review the basic properties of the Minority Game, a minimal and
yet highly non-trivial model of speculative trading derived from the El Farol problem,
and discuss the role of the diﬀerent parameters involved in its deﬁnition. Besides its
physical richness, the Minority Game provides a simple adaptable framework where
a number of important issues related to ﬁnancial markets (such as the emergence
of ‘stylized facts’, the role of diﬀerent types of traders and the eﬀect of information
asymmetries) can be analyzed in great detail. Some of them are discussed in Sec.
5. Finally, some concluding remarks are expounded in Sec. 6. The main analytical
techniques employed for these studies will be discussed in some detail only for cases
where details are not available in the published literature: the replica technique for a
model of a competitive ecosystem in Sec. 2; the continuous-time limit approach for the
El Farol problem, also in Sec. 2; the dynamical generating functional for the canonical
multi-asset Minority Game in Section 5.

2. Statistical mechanics of resource allocation: some examples

We start our discussion by introducing a general class of problems where a population
of heterogeneous agents competes for the exploitation of a number of resources. Then
we will discuss a few examples – ranging from ecosystems to urban traﬃc – where this
generic framework can be formalized in speciﬁc models where the nature of resources
and the laws governing the behavior of agents are completely speciﬁed.

2.1. General considerations

In a nutshell, the models we consider address the decentralized allocation of scarce
resources by N heterogeneous selﬁsh agents subject to public and/or private information.
The word ‘allocation’ is to be intended here in a broad sense that includes the exchange
of resources (for example, commodities) among agents, the production of resources by
means of other resources and the consumption of resources. Agents take decisions on the
basis of some type of information aiming at some pre-determined goals, like maximizing

CONTENTS

6

a certain utility function, and are to various degrees adaptive entities. We shall consider
cases in which they are perfect optimizers (or ‘deductive’) as well as cases in which
their decision-making is governed by a learning process (‘inductive’). Heterogeneity
may reside in a number of factors, like the agents’ initial endowments, their learning
abilities or in how diﬀerently they react to the receipt of certain information patterns.
In general, the allocation is a stochastic dynamical process, where the noise may
be present in both the information sources and the agent’s learning process. We shall
mostly be concerned with the steady-state properties and, more than on individual
performances, we shall focus on the resulting distribution of resource loads and in
particular on

a. how evenly are resources exploited on average (i.e. whether the allocation process

leads typically to over- or under-exploitation of some resources)

b. the ﬂuctuations of resource loads (i.e. how large the deviations from the average

can be)

In such contexts as production economies, ecosystems or traﬃc the meaning and the
relevance of the above observables is immediately clear.
In toy models of ﬁnancial
markets, where, as we shall see, the role of resources is played by information bits,
the former quantity plays the role of a ‘predictability’ while the latter measures the
‘volatility’.

It is implicitly assumed that optimal allocations are those where resources are
In an economic
exploited as evenly as possible and where ﬂuctuations are minimal.
setting, this corresponds to allocations with minimal waste whereas in ﬁnancial markets,
optimality implies information being correctly incorporated into prices with minimal
volatility.

In what follows, we shall denote by

time averages performed in the steady

state:

X

h

i

= lim
T,Teq

T

→∞

1

−

Teq

T

Xt=Teq

h· · ·i

X(t)

}

∈ {

1, . . . , N

and resources by the index µ

where Teq is an equilibration time. Moreover, we shall label agents by the index
i
. In the statistical mechanics
approach, the relevant limit is ultimately that where N
and P scales linearly with
N, so that α = P/N remains ﬁnite as N diverges. To give a loose name, we shall call the
relative number of information patterns α, which will be our typical control parameter,
the ‘complexity’ of the system.

}
→ ∞

1, . . . , P

∈ {

Denoting by Qµ(t) the load of resource µ at time t, which is determined by the
aggregate action of all agents (for instance, µ may be a certain commodity and Qµ(t)
the demand for it at time t), one easily understands that the relevant macroscopic
quantities are given respectively by

H =

1
P

µ
X

Aµ

2 ,

h

i

Aµ(t) = Qµ(t)

,

Q
i

− h

(1)

(2)

) which measures the deviation of the distribution of resource
= 0 at least one resource is overexploited or underexploited

CONTENTS

Qµ

i

µ h

Q
i

= (1/P )

(
h
loads from uniformity (if H
with respect to the average load) and by
1
P

(Aµ)2

σ2 =

P

− h

Aµ

µ
X

2

i

=

1
P

µ
X

(Qµ)2

Qµ

2

i

− h

(cid:11)

(cid:2)(cid:10)

(cid:3)
which measures the magnitude of ﬂuctuations. Eﬃcient steady states have H = 0 and
σ2 “small” in a sense that will be speciﬁed from case to case. To ﬁx ideas, whenever
ﬂuctuations are smaller than those which would be obtained by zero-intelligence agents
who act randomly and independently at every time step one can infer that agents are
to some degree cooperating to reduce ﬂuctuations.

(cid:2)(cid:10)

(cid:11)

(cid:3)

An important question we shall typically ask is how eﬃcient are the steady-state
resource loads distributions generated by a particular group of agents with a given
information stream. Besides this, we shall also look at the inverse problem, namely
under which conditions can a steady state satisfy criteria for eﬃciency. For example,
what type of information should one inject into the system in order to facilitate the
reach of a steady state in which H and σ2 are as small as possible? Indeed the structure
of the information agents have access to may drastically aﬀect global eﬃciency in many
cases (e.g. traﬃc models).

2.2. A simple model of ecological resource competition

Ecosystems constitute a foremost example of the class of problems we outlined above [8].
The following can be seen as a minimal model of a competitive ecology with limited
resources. Such a model will be taken as a prototype to illustrate the statistical
mechanics (static) approach. The statistical mechanics approach to ecosystems has
been pioneered in [9] based on the generating functional approach. The central issue is
that of the May’s biodiversity paradox [10], which shows that, contrary to expectations,
increases in biodiversity in a random ecosystem enhance its instability. We shall indeed
ﬁnd the same result.

2.2.1. Deﬁnition Let us consider a system with N species whose populations ni(t)
(i

) are governed by Lotka-Volterra type of equations:

1, . . . , N

7

(3)

(4)

∈ {

}
˙ni(t)
ni(t)

= fi +

Qµ(t)qµ
i

P

µ=1
X

Qµ denotes the abundance of resource µ
(be it a mineral, a particular
habitat, water. . . ) while qµ
is a coeﬃcient saying how much species i beneﬁts from that
i
resource. The constant fi is the population’s decay rate ‘in absence of resources’. To
simplify things, we mimic the complex interdependence between species and resources
by assuming that the qµ
i ’s are independent, identically distributed quenched random
variables.

1, . . . , P

∈ {

}

6
CONTENTS

The abundance of resource µ depends on the population of each species, i.e.

Qµ(t) = Qµ

0 −

qµ
j nj(t)

N

j=1
X

Qµ

0 = P + s√P xµ

where Qµ
fed on it. To ﬁx ideas, let us suppose that

0 is the amount of resource µ that would be present in the system if no species

→ ∞

h
→ ∞

xµxν
i
, or P

where s > 0 is a constant and xµ is a quenched Gaussian random variable with zero
= δµν. (The P -scaling is introduced in order to obtain a well-deﬁned
average and
limit N
) Loosely speaking, the parameter s is related to the variability
of resources: for small s, the resource level is roughly the same for all resources, while
increasing s the distribution of resource levels gets less and less uniform. Clearly, the
number of species that survive (i.e. such that ni(t) > 0) in the steady state will depend
on a number of factors, like the distribution of available resources and how similar the
species are among themselves, that is on the distribution of qµ
i ’s, which we take to have
ﬁrst moments

qµ
i ii

hh

= q,

(qµ

i −

q)2

= 1

(cid:10)(cid:10)

(cid:11)(cid:11)

(here and in what follows we denote averages over the quenched disorder by
) Along
with the questions concerning the resulting resource loads distribution, an interesting
problem to raise is the following: what is the typical maximum number of species that
can be supported asymptotically when the number of resources P is large (P
) as
a function of s?

hh· · ·ii

→ ∞

This issue can be tackled by noting that

H(t) =

Qµ(t)2

fini(t)

1
2

µ
X

Ns

−

i=1
X

is a Lyapunov function of the dynamics (i.e.
0; this can be easily shown by a
direct calculation). This implies that the steady state properties are described by the
minima of H over
. Note that in the steady state

≤

0

˙H(t)

ni ≥

{

}
Qµ

H

≃

h
µ (cid:16)
X

Q
i

i − h

2

,

(cid:17)

=

Q
i

h

1
P

Qµ

h

i

µ
X

In the rest of this section we shall ﬁrst work out in detail the minimization of H

and then discuss the resulting scenario.

8

(5)

(6)

(7)

(8)

(9)

CONTENTS

2.2.2. Statics (replica approach) The task of minimizing H can be carried out by
introducing a ‘partition sum’

Z = Trn e−

βH,

n =

ni}

{

and applying the replica trick:

lim
N
→∞ (cid:28)(cid:28)

min
n

H
N

(cid:29)(cid:29)

=

lim
β
→∞

lim
N
→∞

−

log Z

=

ii

lim
β
→∞

lim
0
r
→

lim
N
→∞

−

1
βN hh

1
βrN

log

Z r

hh

ii

The calculations are relatively straightforward. Using the Hubbard-Stratonovich trick
we can write

P

P

Z = Trn

β

2N (Qµ)2

e−

β
N

e−

i fini = Trn

ei√ β

N zµQµ

β
N

e−

i fini

(12)

#

"
µ=1
Y
iz is an average over the Gaussian variables zµ with

µ=1
Y

+
z

*

P

P

zµ

h

iz = 0 and

h

zµzν

iz =

. . .
where
h
δµν. So we have

Z r

hh

ii

= Tr

na
{

} *

P

N

ei√ β
N (

a zµ

a )Qµ

0

P

i√ β

N (

a zµ

a nia)qµ

i

e−

P

β
N

e−

i fi

a nia

P

P

+
z

µ=1 DD
Y

i=1 DD
Y
where the index a runs over replicas (a = 1, . . . , r). The ﬁrst disorder average is done
over Qµ
0 as given by (6) (thus more properly over xµ), while the second is done over the
qµ
i ’s. The former is easily performed. As for the latter we note that if β/N
1 (which
is the case since we ﬁrst take the limit N

and then the limit β

≪
), then

EE

EE

i√ β

N (

a zµ

a nia)qµ

i

e−

P

EE

→ ∞
i√ β

N hh

qµ
i ii

(
P

e−

≃

a zµ

a ni,a)−

β

2N (

P

→ ∞
a ni,a)2

a zµ

(qµ

i −

q)2

hh

ii

We thus ﬁnd

DD

Z r

hh

ii

= Tr

na
{

} *

P

µ=1
Y

ei√βN

a zµ

a (α

q
N

−

i nia)e−

β
2

a,b zµ

a zµ

b (αs2+ 1

N

i nianib)

P

P

P

P

β
N

e−

i fi

a nia

P

P

+
z

The leading term in the above exponential is the ﬁrst one. However, it corresponds to
an undesirable super-extensive term in the free energy unless

If so, the annoying term acts as a δ-distribution that ensures the above condition:

ei√βN

a zµ

a (α

q
N

−

i nia)

P

P

∝

δ

Nα/q

 

−

nia

! ∝

dw eβ

P

a wa(N α/q

i nia)

−

P

i
X
Furthermore one sees that the relevant macroscopic order parameter is the overlap

a
Y

Z

9

(10)

(11)

(13)

(14)

1
N

nia =

α
q

i
X

Gab =

nianib

1
N

i
X

β
2

e−

P

µ D
Y

Z r

hh

ii

=

Z

α
2rβ

α
rq

−

CONTENTS

10

which can be introduced in the replicated partition sum with the identities

1 =

δ

Gab −

 

1
N

nianib

dGab ∝

!

i
X
b. Noting that when β

Z
Z
for all a
only the minima of H contribute to the partition
sum, it is easy to understand that Gab measures how similar diﬀerent minima a and b
are to each other. We may now factorize over resources to obtain

→ ∞

≥

P

dRab dGab e−

Nαβ2

2 Rab(Gab−

1
N

i nianib)

(15)

a,b zµ

a zµ

b (αs2+Gab)

= e−

P

2 tr log[I+β(αs2+G)]

z

E

so that, ﬁnally, factorizing over species, we arrive at

e−

βrN f (w,G,R)dwdGdR

(16)

with

f (w, G, R) =

tr log

I + β

αs2 + G

+

RabGab

αβ
2r

(cid:2)
wa −

(cid:0)
1
rβ

log

(cid:28)

a
X

(cid:1)(cid:3)
Trn e

αβ2
2

Xa
b
≥
b Rabnanb−

≥

a

P

P

β

a wana

βf
N

−

a na

(17)

P

(cid:29)f

where now
principle of steepest descent, when N
values of the order parameters G, R and w (which we shall denote by a ⋆) so

h· · ·if stands for an average over the distribution of decay rates. By the
Z r
is dominated by the saddle point

→ ∞

hh

ii

,

lim
N
→∞

min
n

= lim
β
→∞

lim
0
r
→

H
N

f (w⋆, G⋆, R⋆)

(18)

(20)

To proceed further, we assume that G⋆, R⋆ and w⋆ take the replica-symmetric (RS)
form
‡

G⋆

ab = g + (G

g)δab

R⋆

ab = 2r

(r + ρ/β)δab

w⋆

a = w

(19)

−

which leads, in the limit r

0, to the free energy density

fRS(g, G, r, ρ, w) =

log [1 + β(G

g)] +

α
2β

α
2

Gρ

−

α
q

w

−

−
1
β

−

α
2

αs2 + g

1 + β(G

αr
2

+

β(G

g)

−

g)

−
βV (n

log

∞

dn e−

z,f )

|

(cid:28)

0
Z

(cid:29)z,f

where the “potential” V is given by

z, f ) =

V (n
|
h· · ·iz,f is over both the unit Gaussian variable z and the decay rate
and the average
f , whose distribution we left unspeciﬁed up to now. It is clear that if this distribution

w + f /N

(21)

−

n

√αrz

(cid:1)

(cid:0)

αρn2 +

This assumption gives the exact results in almost all the cases we shall discuss in this review because
‡
the functions to be minimized have a unique minimum. Should this condition fail, one must resort to
more complicated Ans¨atze known as replica-symmetry breaking.

−

→

1
2

CONTENTS

has ﬁnite moments and does not get broader with N, we can drop the term f /N above.
Now let us take the remaining limit β
, where minima are selected, assuming that
g (there is only one minimum by
H has a unique minimum. In this case, clearly, G
assumption!) and we may look for solutions with

→ ∞

→

lim
β
→∞

β(G

g) = χ

−

ﬁnite. Moreover, the last integral in (20) in the limit β
minimum of V . Therefore we end up with

→ ∞

is dominated by the

fRS(g, G, r, ρ, w) =

lim
β
→∞

α
2
α
2

αs2 + G
1 + χ
α
q

1
2

αr
2

χ

+

Gρ

⋆ + w
(cid:11)
h· · ·i⋆ are averages over the normal variable z, with the n = n⋆(z) which minimizes

n
i⋆ −

w +

(23)

αρ

zn

i⋆

−

−

(cid:10)

h

h

√αr

n2

where
V :

n⋆(z) =

√αr
αρ

(z

z0)θ(z

z0),

−

−

z0 = w/√αr.

Notice that this operation corresponds to an ‘eﬀective species’ problem whose solution
describes the collective behavior of the original N-species system.

The saddle point equations are

11

(22)

(24)

∂fRS
∂w
∂fRS
∂ρ
∂fRS
∂r
∂fRS
∂G
∂fRS
∂χ

= 0

= 0

= 0

= 0

= 0

α
q

n
i⋆ =
h
n2

⋆ = G
(cid:11)
i⋆ = √αrχ
1
1 + χ
αs2 + G
(1 + χ)2

(cid:10)
nz
h

ρ =

r =

⇒

⇒

⇒

⇒

⇒

It is easier to ﬁnd a parametric solution in terms of z0: let us deﬁne

z0
2

erfc

z0

∞

∞

Z

z0) =

−

e−

z2/2(z

I2(z0) =

I1(z0) =

dz
√2π
dz
√2π
dz
√2π
After some manipulations we ﬁnd

Iz(z0) =

z2/2z(z

z2/2(z

e−

e−

−

−

Z

Z

∞

z0

z0

z0)2 =

z0) =

z2
0 /2

e−
√2π −
1
2
1
2

(cid:0)
erfc

(cid:16)

1 + z2
0

erfc

(cid:16)

(cid:1)
z0/√2

(cid:17)

z0/√2
(cid:16)
z0/√2

(cid:17)

(cid:17)

z0e−

z2
0/2

−

√2π

(25)

1
2

(cid:20)

α =

I2 +

2 + 4s2q2I 2
I 2
1

q

(cid:21)

CONTENTS

12

Figure 1. Behavior of αc as a function of sq.

(26)

(27)

G =

χ =

α2I2
q2I 2
1
Iz

α

Iz

−

0 where z⋆
0) [Iz(z⋆
Iz(z⋆
0)
as z0 →

The assumed scaling of parameters with β, and hence the above equations, are valid
only for z0 < z⋆

0 is the solution of

I2(z⋆

0)] = s2q2I 2

1 (z⋆

0).

∞

−
z⋆
Indeed χ
0. This singularity marks a phase transition at a point
→ ∞
αc = Iz(z⋆
0) between a phase α > αc which is described by the equations above, and one
. The critical point αc is a decreasing function of sq (from a value αc = 1/2
where χ =
5 for sq = 4). It is
for sq = 0) which rapidly vanishes as sq increases (it’s already 10−
1 diverges and
reported in Fig. 1. At the transition, the susceptibility χ
αc|
the free energy, which as we said is proportional to the variance of the resource loads
distribution, vanishes. This means that below αc all resources are exploited to the same
extent, while above αc the resource load distribution is not uniform. For the fraction of
surviving species (with ni > 0) we get

∼ |

−

α

−

φ =

θ(z
h

z0)

iz =

−

∞

z0

dz
√2π

1
2

e−

z2/2 =

erfc

z0/√2

= Iz(z0)

(28)

Z
so φ < α for α > αc and φ
α at αc. This means that below αc the number of
surviving species equals that of resources while for α > αc there is on average less than
one species per resource (or φ/α < 1). The behavior of H, G and of the fraction of
surviving species per resource φ/α is displayed in Fig. 2 as a function of α for ﬁxed sq.

→

(cid:16)

(cid:17)

2.2.3. Stability Note that at ﬁxed q the maximal number P/αc of species that can be
sustained in an ecosystem with P resources is an increasing function of s, so that by
increasing the variability of resources the ecosystem gets more stable. The threshold of

CONTENTS

13

Figure 2. Behavior of H, G and of the fraction of surviving species φ as a function
of α for sq = 1.

stability also increases if q increases. Having ﬁxed the variance of qµ
i to 1, increasing q
means that species get more and more similar. This seems at ﬁrst sight a contradictory
scenario. To sort out this issue, let us analyze the linear stability of the system. Let
ni(t) = ni(
) is the asymptotic value of the population of
species i and ηi(t) is a small perturbation. To leading order, the dynamics is given by

)ηi(t) where ni(

ni(

) +

∞

∞

∞

p

N

−

j=1
X

˙ηi(t) =

∆ijηj(t)

with

∆ij =

P

1
P

µ=1
X

p

ni(

) (qµ

i −

∞

q)

qµ
j −

q

nj(

).

∞

(cid:0)

(cid:1) q

The stability is related to the smallest eigenvalue λ
explicitly [11] and it is given by:

−

of ∆ij. This can be computed

=

λ

−

√α

−

φ

1
q

(cid:16)

p

(cid:17)

This shows that the phase transition point, where αc = φ(αc), is the onset of dynamical
instability of the system. The presence of the factor 1/q in λ
causes an interplay of the
eﬀects of increasing s and increasing q (since λ
), ultimately leading to
a maximal stability for intermediate values of q, as can be seen by the behavior of λ
−
is an increasing function of α, for all
versus q, Fig. 3. It is also easy to show that λ
values of s and q. Hence the introduction of new species always decreases the stability
of the ecosystem, in agreement with May’s classical result [10].

0 as q

→ ∞

− →

−

−

(29)

(30)

(31)

CONTENTS

14

Figure 3. Behavior of λ− as a function of q for α = 0.2 and s = 1. The ecosystem is
marginally unstable when λ− = 0, whereas maximal stability occurs when λ− attains
a maximum.

2.3. The El Farol problem

The El Farol problem is the paradigm of resource allocation games with inductive agents.
It can be stated as follows [12]. N customers labeled i have to decide independently on
each night t whether to attend (ai(t) = 1) or not (ai(t) = 0) the El Farol bar, which has
a capacity of L < N seats. The place is enjoyable only if it’s not overcrowded, that is
only if the attendance A(t) =
i ai(t) doesn’t exceed the number of seats. In order to
make their decisions, customers aim at predicting whether the bar will be crowded or
not on any given night based on the past attendances.

P

In his seminal work, Arthur has pointed out the frustration inherent in such a
situation. If everybody expects that the bar will be crowded, no one will go and the bar
will be empty. Conversely all agents may attend the bar at the same time, if they all
expect it to be empty. Hence he argued that this is a situation which forces expectations
of diﬀerent agents to diverge. It is reasonable to think that, if agents start with diﬀerent
expectation models and revise them according to the history of the attendance, their
expectations will never converge and agents’ heterogeneity will be preserved forever.
He then showed by computer experiments with N = 100 and L = 60 that inductive
agents endowed with ﬁxed ‘predictors’ (namely look-up tables associating to each series
of past attendances a binary decision like go/don’t go) are able to self-organize so that
the attendance A(t) ﬂuctuates around the comfort level L.

Note that the El Farol problem can be regarded as an embryonic market where
L units of an asset or a commodity must be allocated on any given day t. They are
oﬀered to N agents who may decide to invest 1e to buy it (ai(t) = 1) or not (ai(t) = 0).
The attendance A(t) is then the demand of the asset (the number of available units,
or supply, is ﬁxed at L). Each unit of asset delivers a return of 1e to its owner at the
end of the period. Imagine that the price at which the asset is sold is determined at
each period by a market clearing condition (demand = supply): A(t) = Lp(t). Then an

CONTENTS

15

001

011

000

010

101

111

100

110

Figure 4. De Bruijn graph of order 3 (from [15]).

agent who invests ai(t)e in the asset, receives ai(t)/p(t) units of it. These will be worth
ai(t)/p(t)e at the end of the period. If p(t) > 1, which occurs if A(t) > L (crowded
bar), it is not convenient to invest (attend). If p(t) < 1 it is instead worthwhile to invest
(attend).

2.3.1. Deﬁnition In what follows, we focus on a tractable version of the model that
diﬀers from Arthur’s original work in the form of the predictors but preserves all the
main qualitative features of the model [13].
In order to formalize the problem it is
reasonable to assume that (i) customers have a ﬁnite memory, that is, their analyzing
power is limited and they must base their prediction on the attendances of a ﬁnite
number (say, m) of past nights, and that (ii) they are insensitive to the actual size of
the attendance (perhaps simply because they don’t have access to it) and rather only
know whether the bar was overcrowded or not on a given night. This means that the
information available to customers on night t is encoded in the string

µ(t) =

θ (L

A(t

1)) , . . . , θ (L

A(t

m))

{

−

−

−

−

} ∈ {

m

0, 1

}

) is the Heaviside function: θ(L

where θ(
·
while θ(L
string µ(t) is governed in time by the map

A(t)) = 1 if the bar is enjoyable (A(t) < L)
A(t)) = 0 if the bar is overcrowded (A(t) > L). The time evolution of the

−

−

µ(t + 1) = [2µ(t) + θ(L

A(t))] mod(2m)

−

The above equation completely deﬁnes the structure of the information available to
agents in the case in which they base themselves on the past attendances. Graphically,
the evolution of history strings is constrained to occur on a de Bruijn graph [14] of order
m, Fig. 4.

We shall consider, for comparison, another possibility, namely that the information
supplied to customers is a random binary string of length m or equivalently a random
integer (‘information pattern’) drawn from
with equal probability at
each time step. We shall refer to the latter as the case of exogenous random information,
as opposed to the former of endogenous information. The obvious diﬀerence between
the two choices is that while in the latter case the space of informations is sampled
uniformly by construction, in the former this is in principle not true. There is however a
deeper diﬀerence that has serious consequences on the analytical solubility of the model:
in the case of random information the dynamics is Markovian.

1, . . . , 2m

≡

P

}

{

(32)

(33)

CONTENTS

16

Having deﬁned the information source, let us specify the agents’ behavior. Even
in a simpliﬁed context, making the optimal decision for each given string requires
an unrealistic computational capacity that should be shared by all agents. Inductive
reasoning requires that customers stick instead to simple decision rules. In particular,
we assume that they have at their disposal a small number S of 2m-dimensional vectors
called ‘strategies’ (analog to Arthur’s predictors) that map information strings into
binary actions (go/don’t go):

aµ
ig ∈ {
In the table below one such possible strategy is shown for m = 3 (or P = 8).

(i = 1, . . . , N; g = 1, . . . , S)

aig :

0, 1

0, 1

→

∋

µ

}

{

}

m

(34)

past attendance string pattern µ decision aµ

000
001
010
011
100
101
110
111

1
2
3
4
5
6
7
8

1
0
0
1
1
0
1
0

Customers are heterogeneous as of course diﬀerent agents have diﬀerent strategies.
ig of every strategy aig is drawn

This is modeled by assuming that each component aµ
independently for all i, µ and g with probability distribution

P (a) = aδ(a

1) + (1

a) δ(a)

−

−

where a is the average attendance frequency of agents. Strategies are assigned to agents
at time t = 0 and are kept ﬁxed throughout the game. In order to decide which strategy
to adopt on every night, agents keep tracks of their performance via a score function
that is updated according to the following rule:

Uig(t + 1)

Uig(t) =

1

−

2aµ(t)
ig

−

[A(t)

L]

−

(cid:16)

(cid:17)

with the rationale that strategies suggesting not to go (aµ(t)
ig = 0) are rewarded when
the attendance is higher than L and punished when it is lower than L (and vice versa
when aµ(t)
ig = 1). Then on each night every agent selects the strategy with the highest
cumulated score:

(35)

(36)

(37)

igi(t). In short, the model’s rules can be summarized as

gi(t) = arg max

Uig(t)

and acts accordingly: ai(t) = aµ(t)
follows:

gi(t) = arg max

Uig(t)

g

g

CONTENTS

17

Figure 5. Average deviation of the attendance from the comfort level (top) and
ﬂuctuations (bottom) versus aN/L for endogenous (solid lines) and exogenous (dashed
lines) information (from [13]).

aµ(t)
igi(t)

A(t) =

i
X
Uig(t + 1)

Uig(t) =

1

−

2aµ(t)
ig

−

[A(t)

L]

−

(cid:16)

(cid:17)

(38)

(from top to bottom: strategy selection; aggregation; updating). It is understood that
scores are initialized at time t = 0 at certain values Uig(0).

h

i

h

A

−

−

(A

L)2

2.3.2. Macroscopic properties After a transient, the dynamics deﬁned by (38) will
reach a steady state whose global eﬃciency can be conveniently characterized by two
L
parameters: the average deviation of the attendance from the comfort level L,
i
and its ﬂuctuations σ2 =
. The former measures the degree to which agents
coordinate to generate attendances around the comfort level. The latter measures the
the larger σ2 the bigger the deviations of the attendance from
waste of resources:
In a nutshell, it quantiﬁes the quality of the
the comfort level (in either direction).
coordination. The behavior of the two quantities (properly normalized with N) at ﬁxed
L = 60, a = 1/2 and m = 2, 3, 6 and varying N is displayed in Fig. 5 for endogenous
(solid lines) and random (dashed lines) information. In the former case, a general feature
that emerges is that the average attendance settles at the comfort level in a window of
values of a centered around L/N whose size shrinks as m increases. Out of this window,
sensible deviations may occur. In parallel, ﬂuctuations are maximal at a = L/N for
m = 2 and the height of the maximum decreases with increasing m until it disappears.
This implies that the waste of resources is comparatively larger when m is smaller, so
that for instance the fraction of losers is larger for small m. Thus one can say that global
eﬃciency increases when m increases. The behavior in the case of random information

CONTENTS

18

Figure 6. Normalized entropy Sm/m versus aN/L for diﬀerent memory lengths m
(from [13]).

is qualitatively similar to the previous case in the vicinity of aN/L = 1. Quantitative
deviations occur outside this region.

Based on this, one expects that with endogenous information the information space
is sampled uniformly around a
L/N. This is indeed so. To see it, one can measure
the frequency with which histories are sampled in the steady state, ρ(µ), and calculate
the entropy

≃

S(m) =

ρ(µ) log2 ρ(µ)

−

µ
X

(39)

such that S(m) = m when the information space is sampled uniformly (the ‘eﬀective’
number of information patterns visited by the dynamics is 2S(m)). As shown in Fig.
L/N. Outside this phase, the entropy decreases,
1 only for when a
6, S(m)/m
signaling that the information dynamics is biased.

≃

≃

These ﬁndings indicate that the degree to which inductive agents are able to
coordinate the exploitation of the limited resource in a way that is collectively eﬃcient
depends on the size of the information space they base themselves on. While the average
level of activity always settles at the resource level, ﬂuctuations get smaller and smaller
as the information space grows. When the average attendance frequency is close to
L/N, then, the particular nature of the information provided to agents doesn’t aﬀect the
stationary macroscopic properties. The relevant requirement is that all agents possess
the same information, independently of whether it’s the true attendance history or a
random string.

2.3.3. Dynamics (continuous-time limit approach) The mathematical analysis of this
model can be carried out in the case of exogenous information by studying the
continuous-time limit of (38) along the lines of [16]. A few simpliﬁcations are necessary
to this aim. First note that the dynamics (38) is non-linear in a way that doesn’t allow

CONTENTS

19

to write it in the form of a gradient descent, that is physically the model is deﬁned by
a set of N globally coupled Markov processes that violate detailed balance and it is not
clear that a Lyapunov function exists. It is however possible to regularize the dynamics
by smoothing the choice rule gi(t) = arg maxg Uig(t) to

Prob
{

gi(t) = g

= C(t) eΓUig(t)

}

C(t) = normalization

(40)

with Γ > 0 the ‘learning rate’ of agents (the original choice is recovered for Γ
).
This modiﬁcation is not without consequences and Γ indeed turns out to play a rather
non-trivial role in the macroscopic properties. With (40), it is possible to construct the
continuous-time limit of (38).

→ ∞

The crucial observation is that there is a ‘natural’ characteristic time scale for
the dynamics given by P (intuitively, agents have to check the eﬃciency of their
strategies against all information patterns before they can evaluate their performance
meaningfully). This implies that if one is interested in steady state properties, time
should be re-scaled as t
Iterating (38) from time t = P τ to time
→
t = P (τ + dτ ) and setting uig(τ ) = Uig(P τ ) one obtains

τ = t/P .

uig(τ + dτ )

uig(τ ) =

−

P (τ +dτ )

1
P

2aµ(t)
ig

1

−

[A(t)

L]

−

Xt=P τ (cid:16)
The arguments of the sum on the right-hand side can be separated into a deterministic
and a ﬂuctuating term:

(cid:17)

1

2aµ(t)
ig

[A(t)

L] = (1

2aig)

[A(t)

−

−
where we used the fact that information is exogenous and random and we denoted by
h· · ·iπ an average over the distributions

−

−

(cid:16)

(cid:17)

h

L]

iπ + Xig(t)

(41)

(42)

(43)

πis(τ ) =

1
P dτ

P (τ +dτ )

Xt=P τ

C(t) eΓUig (t)

We have therefore

uig(τ + dτ )

uig(τ ) = (1
P (τ +dτ )
where dWig(τ ) = (1/P )
t=P τ Xig(t) is a noise term whose statistics (average and
correlations) can be derived by noting that Xig(t) are independent identically-distributed
zero-average random variables, so

iπ dτ + dWig(τ )

[A(t)
h

= 0 and

2aig)

(44)

dWig(τ )

P

L]

−

−

−

dWig(τ )dWjg′(τ ′)

=

i

h

h

i

P (τ +dτ )

P (τ ′+dτ )

1
P 2

*
δ(τ

Xig(t)Xjg′(t′)

+

t=P τ
X
τ ′)

Xt′=P τ ′
Xig(t)Xjg′(t)

=

h

−
P

iπ dτ
iπ can be evaluated from the statistics of disorder
0 one arrives at the following Langevin

(45)

→

The remaining term,
and of A(t). Finally, taking the limit dτ

Xig(t)Xjg′(t)

h

CONTENTS

process:

20

(46)

(47)

(48)

˙uig(τ ) = (1
ηig(τ )
= 0

−

i
ηig(τ )ηjg′(τ ′)

h

h

2aig)

[A(t)
h

−

L]

iπ + ηig(τ )

(A

h

L)2

iπ

−
P

i ≃

(2aig −

1)(2ajg′ −

1)δ(τ

τ ′)

−

where in the last relation we have factorized the average over µ’s. Note also that the
averages on the right-hand side of Eq. (46) are taken at ﬁxed πig(τ ) = C(τ ) exp[Γuig(τ )]
so they are themselves time-dependent. Therefore (46) is a set of complex, non-linear
stochastic diﬀerential equations in which the noise correlation is also time-dependent.
At the same time, the probability to choose a predictor g, πig(τ ), is easily seen to satisfy,
in the re-scaled time Γτ =

τ the stochastic equation

˙πig(

τ ) = πig(

τ )F [π] + √ΓG[π, η]
e

e

e

where F and G are Γ-independent functions whose form is not relevant for our scopes.
This tells us that agents’ preferences are subject to stochastic ﬂuctuations of strength
proportional to √Γ around their average. The larger Γ the longer it takes to average
0, in which the dynamics if the πig’s
ﬂuctuations out. Moreover only in the limit Γ
(and consequently of the uig’s) becomes deterministic, the system performs a gradient
descent with the Lyapunov function

→

H =

1
P

µ
X

µ

(

A
|

h

i −

L)2 ,

A
|

µ
i

h

=

figaµ
ig

i,g
X

denotes a time-average in the steady state conditioned on the occurrence

µ
where
i
of pattern µ:

h· · · |

and fig =

.

πigi

h

X

µ
i

|

h

= lim
T,Teq

→∞

T

1
Tµ

Xt=Teq

T

Xt=Teq

X(t)δµ(t),µ,

Tµ =

δµ(t),µ

(49)

Thus the minima of H over fig (subject to

g fig = 1 for all i) describe the steady
state. From a physical viewpoint, H measures the amount of exploitable information
= L, the signal µ(t) carries
produced in the system, or the ‘predictability’: if e.g.
information which is useful to predict whether one should attend or not to the bar
when µ(t) = ν. The fact that the stationary state corresponds to minimal H means
that agents exploit to their best the system’s predictability. We shall term phases
with H = 0 ‘unpredictable’ or ‘symmetric’, while phases with H > 0 will be called
‘predictable’ or ‘asymmetric’.

A
|
h

P

i 6

ν

Notice also that the noise correlations are proportional to the volatility σ2 =
which in turn depends on the set of all uig’s. Hence calculating the volatility

(A
h
requires solving a much more complex self-consistent problem.

L)2

−

i

The minimization can be carried out analytically resorting again to the replica trick.
The thermodynamic limit to be considered in this case is N
with ℓ = L/N and
α = P/N ﬁnite. The interesting case is that where the average attendance frequency a

→ ∞

CONTENTS

21

Figure 7. Phase diagram of the El Farol bar problem. The solid line encloses the
‘unpredictable’ phase where H = 0. The dashed lines correspond to the trajectories
of systems with L = 60, ¯a = 1/2 and m = 2, . . . , 6 as the number of agents increases
(from bottom to top). The dot-dashed line corresponds to a typical trajectory of a
system with ﬁxed L, N and ¯a > L/N as the agents’ memory changes (from [13]).

ﬂuctuates around ℓ so that a
will always use the strategy that prescribes him to go more (resp.
(resp. a > ℓ). A convenient parametrization is given by

ℓ = O(1/√N ). Indeed, if a

−

−

ℓ = O(1) then each agent
less) often if a < ℓ

a

ℓ = γ

−

r

ℓ(1

ℓ)

−
P

(50)

with γ ﬁnite and independent of N. The resulting phase diagram in the (α, γ) plane is
reported in Fig. 7. We see a region for small α and small γ where H = 0, i.e.
= L.
In this ‘unpredictable’ phase the average attendance converges to the comfort level but
ﬂuctuations are large. On the other hand, the typical attendance diﬀers from L outside
this phase. Looking at the m-dependence, we see that as N varies with L and ¯a and m
ﬁxed, the system follows the trajectories shown in dashed lines. For small values of m
these cross the symmetric phase in the region ¯aN

A
i

L.

h

This rich phenomenology, and speciﬁcally the non-trivial

interplay between
is characteristic of the complexity of many other

predictability and ﬂuctuations,
resource-allocation models, two of which we shall now discuss.

≃

2.4. Buyers and sellers in the ‘ﬁsh market’

Market organization, namely the establishment of stable relationships between buyers
and sellers, is one of the basic mechanisms that determine the eﬃciency of commodity
markets. An important question concerns the eﬀects that organization has on prices
and their ﬂuctuations. This issue has been investigated in detail in [17] in the context
of an empirical study of the Marseille ﬁsh market. This is the sense in which this
section refers to a model of a ‘ﬁsh market’. Loosely speaking, one can think that a

CONTENTS

22

seller with loyal buyers has an incentive to take advantage of the situation by raising
prices, thus removing the incentive of buyers to be loyal to him. Once the relationship
is broken, buyers will seek cheaper sellers thus driving a reduction of the average price.
This mechanism however is expected to cause an increase of ﬂuctuations (and thus a
decrease of cost certainty), since in the ‘disorganized’ phase buyers will be switching from
one seller to another. This elementary scenario, from which it is clear that eﬃciency is
a two-sided concept, is worth of a deeper investigation. A highly stylized yet non-trivial
model addressing this issue was introduced in [18].

→ ∞

One considers a system with N buyers and P sellers, which for simplicity may be
assumed to sell diﬀerent commodities each (say each seller supplies a diﬀerent type of
ﬁsh). Ultimately, the limit N
with n = N/P ﬁnite shall be considered. On
each day t = 1, 2, . . ., every consumer i has to acquire one of S possible bundles of
commodities, for instance for his or her subsistence. A bundle is a vector qig =
such
that qµ
ig denotes the amount of goods buyer i demands from seller µ (µ
).
1, . . . , S
g
labels the diﬀerent feasible bundles. We are interested to model the case
in which buyers are heterogeneous, in the sense that diﬀerent buyers have diﬀerent needs
and thus diﬀerent possible bundles. We therefore assume that bundles qig are quenched
random vectors with probability distribution

qµ
ig}
{
1, . . . , P

∈ {

∈ {

}

}

P (qig) =

(1

q)δ(1

−

−

qµ
ig) + qδ(qµ
ig)

,

µ
Y

(cid:2)

(cid:3)

(51)

(0 < q < 1 being the probability that any given commodity is part of a bundle) that
are assigned to consumers independently on i and g on day t = 0 and are kept ﬁxed.
In this way, we introduce a further simpliﬁcation in that each seller is either visited or
not by a buyer, and the purchased quantities play no role. Moreover, we are implicitly
assuming that the diﬀerent commodities are equivalent to consumers, that is there is
no commodity that all buyers will need to buy. Coming to sellers, we assume that they
set the daily price of their commodity according to the demand they receive, denoted
by Dµ(t), so that the higher the demand the higher the price. Each buyer, on the other
hand, aims at purchasing, on each day, the bundle he or she ﬁnds more convenient,
labeled by gi(t), with the limitation that when the choice is made the price at which
the purchase will take place is not known yet (it is determined by the collective decision
of all consumers, which form the demands). Hence they try to learn the convenience of
diﬀerent bundles from experience in order to be able to predict which bundle will have
the highest marginal utility on any given day. The events taking place on each day t
can be summarized by the following scheme:

gi(t) = arg max

Uig(t)

Dµ(t) =

g
qµ
igi(t)

Uig(t + 1)

Uig(t) =

i
X

−

1
P

µ
X

qµ
ig [k

−

Dµ(t)]

(52)

At the decision stage, each buyer chooses the bundle which carries the highest

23

(53)

(54)

CONTENTS

(cumulated) utility Uig(t). The diﬀerent choices are then aggregated and the demands
are formed. Finally utilities are updated with the following rationale: if the demand of
a commodity µ is above a certain threshold k, consumers perceive that commodity as
too costly and the utility of all of his feasible bundles that include it will tend to be
reduced. Similarly, if the demand has been lower than k the commodity will be seen as
‘cheap’ and will tend to increase the utility of the bundles that contain it. The utility
of a bundle is determined by the demands of all commodities in it. Finally, we assume
that the score updating is initialized at values Uig(0).

It is clear that k plays in this model the role of the comfort level L of the El Farol
problem. Based on the discussion of the previous section, we concentrate on the case
k = N(1

q). The relevant macroscopic observables are given by

−

H =

∆ =

1
P

1
P

µ
X

µ
X

Dµ

h

−

N(1

q)

2
i

−

(Dµ)2

Dµ

2

i

− h

(cid:11)

(cid:2)(cid:10)

(cid:3)
H measures of how evenly buyers are distributed over sellers. Indeed if H = 0, each
seller receives on average the same demand so that none of them is perceived as more
convenient by buyers. In this case, consumers are distributed uniformly over producers.
If H > 0, instead, the distribution of demands is not uniform and some producers are
seen as more or less convenient than others. ∆ represents instead the magnitude of
demand ﬂuctuations. Note that because of our assumptions on the relation between
prices and demands, H is a proxy for the average price whereas ∆ quantiﬁes the typical
spread of prices in the economy. Note also that when H > 0 an external agent who
watches the economy from the outside trying to identify the best bargain would manage
to ﬁnd more convenient sellers and make a proﬁt. When H = 0, instead, this would
not be possible. So transitions from regimes with H > 0 to regimes with H = 0 can
be seen as transitions between ineﬃcient and eﬃcient states of the economy, where by
eﬃcient state we mean one where goods ﬂow from sellers to buyers in such a way that
no information exploitable by an external agent is generated. States that are optimal
from a collective perspective have both H = 0 and ∆ small, because on one hand a
uniform demand distribution is desirable and on the other price ﬂuctuations should be
such that agents have as much cost certainty as possible on a day by day basis. Hence
H and ∆ describe intertwined properties, and it is on their mutual dependence that we
shall focus in what follows.

Results are shown in Fig. 8. The behavior of H indicates that as the number of
buyers increases they tend to distribute more and more uniformly over sellers until,
for n = nc, H vanishes and the distribution becomes uniform. For n < nc the
economy is ineﬃcient as the uneven distribution of demands generates exploitable proﬁt
opportunities. For n > nc the economy is instead eﬃcient. Notice that results are
indeed independent of initial conditions Uig(0) in the ineﬃcient phase, while for n < nc
ergodicity is broken and the steady state depends on initial conditions. Furthermore,

CONTENTS

24

Figure 8. Behavior of H and Σ = ∆ + H as a function of n for various q and ﬂat
(Ui1(0) = Ui2(0)) and biased ([Ui1(0)
Ui2(0)] = 0.2) initial conditions (analytical
curves and numerical simulations, from [18]).

−

we see that in the ineﬃcient phase ﬂuctuations are small whereas when the economy
becomes eﬃcient the dependence on initial conditions may drive the system to both
n), which are rather undesirable, and
states with large price ﬂuctuations where (∆
∼
1/n). This can be interpreted with the
states with small ﬂuctuations (where ∆
∼
following mechanism. When there are few buyers, many sellers receive small demands
and thus the economy presents many proﬁtable opportunities. As more and more
buyers join the opportunity window shrinks and players may be forced to switch bundles
repeatedly in the attempt to identify convenient commodities. This leads to the increase
of ﬂuctuations and ultimately to a loss of day-by-day cost certainty.

Like the models described before, this one can also be studied analytically by
resorting to a replica minimization. It is not diﬃcult to see that the Lyapunov function in
this case is precisely H, so buyers collectively act so as to exploit proﬁtable opportunities
as much as possibles. In [18] a diﬀerent solution method, based on dynamical generating
functionals, is employed. We defer a discussion of this technique to Sec. 5.3.

2.5. Route choice behavior and urban traﬃc

A most striking example of the inﬂuence of diﬀerent information structures on the
stationary properties of these systems has been given in the experimental literature on
behavioral aspects of route-choice dynamics in vehicular traﬃc [19, 20]. Experiments
dealt with groups of people having to choose at each time step (day) between two
alternatives (routes), having at their disposals a certain externally provided information
about the aggregate daily result, a sort of tunable traﬃc bulletin. The payoﬀ for each
choice depends on the number of agents making that choice in such a way that the larger
this number the smaller the payoﬀ. Experiments have shown that while agents were able
to adapt rather well and reach states that were eﬃcient on average, the overreaction,

CONTENTS

25

A

Figure 9. Regular grid with two routes for traveling from A to B.

B

namely the ﬂuctuations or the diﬀerence between the optimal rate of decision change
by agents and the actual rate of change, displayed a strong dependence on the type of
information supplied, for instance with or without impact correction, time-dependent,
user-dependent etc. In particular, the overall best states (smallest overreaction) were
attained when the information is user-speciﬁc (see however [19,20] for additional details
and more results).

The issue of how the information structure aﬀects macroscopic properties has
been tackled in a traﬃc-inspired resource allocation game which can be seen, roughly
speaking, as a lattice version of the previous ‘ﬁsh-market’ model [21]. Let us consider
the following situation. A road network, which for simplicity is taken to be a square
lattice with L2 sites, is given. On each day, each one of N drivers has to travel from
location A to location B (say, work/home) following one of S possible routes. The points
A and B are diﬀerent for diﬀerent drivers while the routes at their disposal are taken to
be S quenched random self-avoiding walks of length ℓ going from A to B (see Fig. 9).
Routes play here the role of the predictors of the El Farol problem and of the feasible
bundles of the ﬁsh market: indexing lattice edges by µ, each route g of every driver i
, where qµ
can be written as a vector qig =
ig = 1 if driver i passes through edge µ in
route g, and qµ
ig = 0 otherwise. Drivers are assumed to be inductive and their behavior
is governed by the following rules:

qµ
ig}

{

Prob
{
Qµ(t) =

gi(t) = g
}
qµ
igi(t)

= C(t) exp [ΓUig(t)]

i
X

−

Uig(t + 1)

Uig(t) =

qµ
igQµ(t) +

δg,gi(t)

ζig(t)

1
P

−

µ
X

1
2

1

−

(cid:0)

(cid:1)

(55)

Let us discuss them in some detail. The ﬁrst one says that agents choose their preferred
route on day t, gi(t), using a probabilistic model with learning rate Γ > 0. Qµ(t) denotes
the traﬃc load on street µ on day t. The score updating process is composed of two
parts:

µ qµ

igQµ(t), says that agents prefer less crowded routes;

the ﬁrst term,

•

1
P

−

P

CONTENTS

26

(56)

•

•

•

1

δg,gi(t)

the second term, 1
ζig(t) is non-zero only for routes the driver has not
2
taken on any given day and represents the information noise, or the inaccuracy
with which he knows the traﬃc load on network edges he hasn’t visited. ζig(t) is a
Gaussian noise with mean η and correlations

−

(cid:0)

(cid:1)

= ∆δijδghδtt′
Diﬀerent information structures correspond to diﬀerent values of η and ∆:

ζig(t)ζjh(t′)

h

i

the case η = ∆ = 0 (no information noise) corresponds to the case in which all
drivers possess complete knowledge of the traﬃc load on each network edge on
every day

for ∆ > 0 the information about unvisited edges is user-speciﬁc and noisy.
particular

In

– for η = 0 the noise is unbiased
– for η > 0 the driver overestimates the performance of routes not taken
– for η < 0 the driver underestimates the performance of routes not taken

Let us notice, en passant, that smart drivers should be aware of the fact that the traﬃc
load on a given route would have been larger had they chosen it and therefore they
should underestimate the eﬃciency of unused routes. In other words, drivers account
for their impact on the traﬃc loads when they are able to disentangle their contribution
In this model, drivers completely
to it (their ‘impact’) before updating their scores.
account for their contribution to the traﬃc for η =
2. We shall see however that
any small η < 0 is suﬃcient to alter signiﬁcantly the collective properties. We shall
distinguish between two types of drivers: ‘random drivers’ with Γ = 0, who choose their
, who
route every day at random with equal probability, and ‘optimizers’ with Γ =
every day choose the route they expect to be faster.

∞

−

As usual, one is interested in the collective properties in the steady state. We have
several control parameters, namely η, ∆, Γ and the vehicle density c = N/P . The
observables we focus on are

H =

σ2 =

1
P

1
P

Qµ

Q
i

− h

µ D
X

2

,

E

2

Qµ

Q
i

− h

(cid:29)

(cid:17)

µ (cid:28)(cid:16)
X

=

Q
i
h

1
P

Qµ

h

i

µ
X

(57)

(58)

i

h

=

Qµ

h· · ·i

Q
i

stands for a time average over the stationary state of the learning
where as usual
dynamics. Just as in the ﬁsh-market model, H describes the distribution of drivers
over the street network in the stationary state. If H = 0, the distribution is uniform
for all µ) and it is not possible to ﬁnd less crowded streets on the grid. If
(
h
H > 0, instead, the distribution is not uniform and fast pathways do exist. Notice that
if transit times are assumed to be proportional to the street loads Qµ, then σ2 measures
the total traveling time of drivers. Then, the optimal road usage is achieved when σ2 is
minimal. Note that since all routes have the same total length ℓ (i.e.
ig = ℓ for all
i and g),

= cℓ is a constant.

µ qµ

P

Q
i

h

CONTENTS

27

1/Γ=0
Γ=0

101

101

Ν

/

2

σ

100

10−1

100

102
1/4
∆ teq

104

100

10

1

0.1
0.5

0.4

0.3

0.2

0.1

N

/

2

σ

/

N
H

0.0

10−2

10−1

101

102

10−1

10−1

100
c

100

101

102

103

104

∆

Figure 10. Left panel: σ2/N (top) and H/N (bottom) for random drivers (open
symbols) and optimizers (closed symbols). Simulation parameters: S = 2, P = 200,
ℓ = 50, Uig(0) = 0 for all i and g. Averages are taken over at least 50 disorder samples
for each point. The solid line in the graphs is the analytic estimate of H and σ2 (for
Γ = 0+) for the model with uncorrelated disorder. Right panel: behavior of σ2/N for
a city of P = 200 streets with N = 1024 drivers (c = 5.12, in the ineﬃcient phase) as
a function of the parameter ∆ with η = 0. The horizontal lines correspond to drivers
with Γ = 0 (dashed) and Γ = 0+ (dotted). Results for optimizers with Γ =
are
shown for equilibration times teq = 100 (full line), 400 (open circles) and 1600 (full
diamonds). In the inset, data are plotted versus ∆t1/4

eq (from [21]).

∞

Numerical simulations for η = ∆ = 0 reveal the picture displayed in Fig. 10).
We see that random drivers lead to a stationary state where a uniform distribution
of vehicles is never achieved, as H > 0 for all c. Optimizers, instead, behave in a
similar way only for small vehicle densities. As c is increased, the traﬃc load becomes
more and more uniform (H decreases) and ﬂuctuations (σ2) decrease, indicating that
inductive drivers manage to behave better than random ones. At a critical point cc ≃
3
the distribution becomes uniform (i.e. H = 0) and vehicles ﬁll the available streets
uniformly. Now drivers can’t ﬁnd a convenient way and are forced to change route very
frequently. As a consequence, global ﬂuctuations increase dramatically. Notice that
above the critical point traﬃc ﬂuctuations are signiﬁcantly smaller for random drivers
than for optimizers. Finally, the stationary state depends on the initial conditions Uig(0)
for c > cc: the larger the initial spread, the smaller the value of σ2. The conclusion is
that random drivers lead to an overall more eﬃcient state in conditions of heavy traﬃc
while optimizers perform better when the car density is low.

Unfortunately, the analytical side of this model is much harder than the previous
examples because the quenched disorder (the feasible routes) is in this case spatially
correlated. It is possible however to solve analytically a milder version with uncorrelated
disorder. Results (shown in the Fig. 10) reproduce the qualitative behavior described
above fairly well, and predict a critical density of cc = 2.97 . . ..).

For η = 0 and ∆ > 0, the dependence on initial conditions disappears and is
replaced by a non-trivial dynamical behavior (see again Fig. 10). In the high density

CONTENTS

28

random drivers 
adaptive drivers, η=0
adaptive drivers, η=−2

1.0

0.8

0.6

0.4

0.2

N

/

2

σ

0.0

10−2

10−1

100

101

Figure 11. Behavior of σ2/N for a city of P = 200 streets with η =
(resp. random) drivers have Γ =

(resp. Γ = 0) (from [21]).

−

2. Adaptive

c

∞

phase, where drivers would behave worse than random with ∆ = 0, global eﬃciency
can improve beyond the random threshold if ∆ > 0. Taking averages after a ﬁxed
equilibration time teq, we ﬁnd that σ2 reaches, for ∆
40, a minimum that is well below
the value of σ2 for Γ = 0+ with the same homogeneous initial conditions yi(0) = 0. For
we recover the behavior of random drivers. However, when we increase teq,
∆
the curve shifts to the left, showing that the system is not in a steady state. Rescaling
∆ by t−
, the decreasing part of the plot collapses, while the the rest of the curve
eq
ﬂattens. This suggests that the equilibrium value of σ2 drops suddenly as soon as
∆ > 0. Loosely speaking: noise-corrupted user-speciﬁc information can avoid crowd
eﬀects when the vehicle density is very high.

→ ∞

≈

1/4

We ﬁnally come to the case η

= 0 and ∆ = 0, Fig. 11. While one observes
no qualitative changes for η > 0, for η < 0 ﬂuctuations are drastically reduced in
the supercritical phase.
2 (when, as we said above, drivers
completely account for their contribution to the traﬃc) the dynamics converges to a
state characterized by no traﬃc ﬂuctuations (σ2 = H) because each driver selects one
route and sticks to it.

In particular, for η =

−

Hence this setup allows to address the impact of diﬀerent information structures,
and thus of diﬀerent types of information broadcasting, on the collective properties of
urban traﬃc. This is perhaps one of the most promising research lines with respect to
applications opened by resource allocation games so far.

3. Optimal properties of large random economies

3.1. Introduction

The standard tenet of microeconomics is that economic activity is aimed at the eﬃcient
allocation of scarce resources [22]. As we said before, ‘allocation’ includes exchange,

6
CONTENTS

29

production and consumption of commodities. The concept of ‘eﬃciency’ is instead
usually connected to the solutions of constrained maximum and/or minimum problems,
as for instance ﬁrms strive to maximize proﬁts at minimum costs while the goal
of consumers is to maximize their utility subject to their budget constraints. The
fundamental concept by which mathematical economists explain the emergence of
eﬃcient states from the disparate choices of individual agents in economic systems is
that of ‘equilibrium’, that is a state where all agents maximize their objective functions
and the waste of resources – in the form of imbalance between demand and supply
– is minimum (actually, zero). Typical results concern the existence and stability
of equilibria for diﬀerent types of economies (see below for a precise deﬁnition).
In
such settings it is however extremely diﬃcult to extract meaningful macroscopic laws
(comparable with empirical data) from the mathematical results, in great part because
of the diﬃculties in handling agents’ heterogeneity eﬀectively.
In what follows, we
will show that when heterogeneity is taken properly into account the structure of
equilibria of model economies (as well as of other related optimization problems of
microeconomics) proves to be rich and non-trivial. We shall review the collective
properties of a few exemplary linear optimization problems of microeconomics, whose
setting will be borrowed from the economic literature [23]. We will see that the
emerging scenario presents in all cases two distinct regimes: an expanding phase where
technological innovations lead to an overall economic growth, and a saturated regime
where growth is not achieved by technological innovation but rather by a diversiﬁcation
of the production. The key technical role in our analysis is played by the replica
method and the transitions between expanding and contracting states can be completely
characterized by a few macroscopic order parameters. Remarkably, it will turn out that
the physical order parameters that arise bear an immediate economic interpretation.

3.2. Meeting demands at minimum costs

To begin with, we consider the simple linear model of production to meet demand
satisfying an optimality criterion [24, 25]. This illustrates the general two-phase
phenomenology described above in an extremely simpliﬁed setting. Let there be N
processes (or technologies) labeled by i and P commodities labeled by µ. Each process
allows the transformation of some commodities (inputs) into others (outputs) and is
characterized by an input-output vector ξi =
where negative (positive) components
represent inputs (outputs). Each process can be operated at any scale si ≥
0. The scales
si must be chosen so that the total amount of commodity µ that is produced (consumed)
i siξµ
i = κµ for all µ, where the thresholds κµ
matches a ﬁxed demand (availability):
may be positive (for goods one wants to be produced) or negative (for goods to be
consumed). Among all feasible states
, one may select the one which minimizes a
particular function of the si. Here we take the simplest choice of a linear combination
i sipi, which can be thought of as the total operating cost, if pi is seen as the operation

P
si}
{

ξµ
i }

{

cost at unit scale.
P

CONTENTS

30

1

φ
n

φ

0.3

0.55

0.5

0.45

0.4

0.35

0.25

0.2

0.15

0.1

0.05

0
0.1

0.1

0.1

1

n

10

1

n

10

Figure 12. Fraction φ of active processes vs n for pi = 1. Inset: φn vs n for the
same parameter values. ξµ
i ’s are taken to be Gaussian variables with variance 1/P
µ ξµ
0.001 for all i. κµ’s are sampled from the bimodal distribution
i =
such that
q(κ) = 1−m
2 δ(κ + 1) + 1+m
P

1) with m = 0.1 (from [25]).

2 δ(κ

−

−

≪

≫

→ ∞

We ask the following question: how does the operation pattern (e.g. the fraction
of active processes such that si > 0) change when N increases, i.e. as more technologies
become available? Indeed, the macroscopic structure of the eﬃcient state must be
expected to depend on the ratio N/P :
P a technology will be more likely
for N
to be active (si > 0) than for N
P , when selection will be stronger and processes
performing the required conversions more eﬃciently will be favored. This problem can
be tackled by methods of statistical mechanics in the limit N
with n = N/P ﬁnite
upon assuming that the ξµ
i ’s are quenched random variables (similarly to what has been
done for other linear optimization problems such as the knapsack problem [26–28]). A
µ ξµ
i < 0 for all i, which ensures that processes
further important requirement is that
cannot be combined to yield a technology with only outputs. We refer the reader to [25]
for details and focus on the emerging picture (see Fig. 12). One sees that for small n
roughly a half of the processes are active. This means that as n increases, that is as
more and more technologies become available, the number of active processes per good
increases (see inset) i.e. the arrival of new technologies favors existing ones. The picture
changes radically for n & 2, as φ starts to decrease and nφ = 1. Now the number of active
processes equals that of commodities and technologies undergo a much stronger selection
which reduces the probability that a randomly drawn input-output vector is active. This
simple model describes in a nutshell a transition to a highly competitive state where all
possible productions are saturated by existing technologies and an increase in activity
levels can be achieved only by increasing P . We shall see below that a similar picture
extends to the more complicated case of general equilibrium.

P

CONTENTS

31

3.3. Competitive equilibria of linear economies

An economy can be seen as a complex system of interacting agents (consumers, ﬁrms,
banks etc.) with conﬂicting goals and complementarities. It is indeed the heterogeneity
of the agents which drives the economic process. Surely, if all agents were identical with
identical endowments, there would be no trade. Modeling an economy as a system of
heterogeneous agents is however a quite complex task [29]. In this section we review
how statistical mechanics may be helpful in deriving the macroscopic properties of large
random economies. Speciﬁcally this approach allows one to derive statistical laws that
provide a picture of how structural properties are aﬀected by changes of macroscopic
parameters. This is the same type of information than random matrix theory provides
about the structure of heavy nuclei [1, 30].

3.3.1. Deﬁnition We stick to the standard microeconomic setup (see e.g. [23]). An
economy is deﬁned as a system of N ﬁrms labeled by i, P commodities labeled by µ
and L consumers labeled by ℓ. Each ﬁrm is endowed with a technology that allows the
transformation of some commodities, called ‘inputs’, into others, called ‘outputs’. Every
technology is completely characterized by its ‘input-output vector’ ξi =
, where
negative (respectively positive) components represent quantities of inputs (respectively
outputs), and can be operated at any scale si ≥
0, meaning that when run at scale si
it produces or consumes a quantity siξµ
i of commodity µ. The price of commodities is
given by the ‘price vector’ p =
. Each consumer is characterized by his/her
0
}
yµ
initial endowment of commodities yℓ =
and by his/her utility function Uℓ,
ℓ ≥
{
xµ
associating to every bundle of goods x =
a real number Uℓ(x) representing
{
his/her degree of satisfaction.

ξµ
i }

}
0

pµ

≥

≥

}

{

{

0

It is assumed that ﬁrms choose their activity levels si so as to maximize their proﬁts

πi for a ﬁxed price vector p:

πi

max
0
si
≥

with πi = si(p · ξi)

On the other hand, consumers choose their consumptions xℓ so as to maximize their
utilities within their budget constraints for a ﬁxed price vector p:

max
xℓ∈
Bℓ

Uℓ(x)

with Bℓ =

x

{

≥

0 s.t. p · yℓ ≥

p · x

}

, p⋆) for which (i) the above problems (59) and (60) are
Equilibria are states (
{
simultaneously solved for all i and ℓ and (ii) the aggregate demand of each commodity
matches the aggregate supply:

x⋆
ℓ }

s⋆
i }

{

,

(59)

(60)

(61)

(x⋆

ℓ −

yℓ) =

s⋆
i ξi

i
X

Xℓ

The ‘market clearing’ condition (61) implies zero waste of resources and ultimately
determines the optimal price vector p⋆.

In order to connect the microscopic eﬃciency to macroscopic laws, one would like
to assess the typical values, relative ﬂuctuations and distributions of consumptions,

CONTENTS

32

operation scales and prices at equilibrium in a large heterogeneous economy, that is,
when agents have diﬀerent technologies, endowments etc. This problem can be tackled
in its most general form by applying techniques of spin-glass physics. However, a rich
qualitative description can be obtained already at a less general level, obtained by
introducing the following assumptions [31, 32]:

a. Consumers: there is only one consumer (the ‘society’) whose utility function is

separable: U(x) =

b. Initial endowments:

P

µ u(xµ); the functions u are such that u′ > 0 and u′′ < 0
the initial bundle y is a quenched random vector whose

components yµ are sampled independently for each µ from a distribution ρ(y)
c. Technologies: the input-output vectors ξi have quenched random components ξµ
i
that are identically distributed Gaussian random variables with zero mean and
variance ∆i/P satisfying
ǫi with ǫi > 0; the quantities ∆i are themselves
quenched random numbers drawn from a distribution g(∆) independently for each
i and ǫi = η√∆i

µ ξµ

i =

P

−

Let us discuss them brieﬂy. The assumption L = 1 simpliﬁes the thermodynamic limit
considerably (in the most general setting, the latter corresponds to diverging N, P and
L). The separability of U implies that commodities are a priori equivalent. Hence the
society can increase its utility only by acquiring scarce commodities (ones with low yµ)
at the expense of abundant commodities (with high yµ). Reaching non-trivial optimal
states then requires that (i) some commodities are initially more abundant than others
y)) and (ii) the
(one can see that no activity takes place in the case ρ(y) = δ(y
productive sector is able to provide scarce goods using abundant goods as inputs. We
will see that this last point constitutes a strong selection criterion for technologies. The
convexity assumptions on u follow the economic literature and are convenient from an
analytic viewpoint, as will become clear later. Finally, the assumptions on technologies
guarantee, as in the previous case, that it is impossible to produce all commodities
without consuming any by simply constructing a suitable combination of technologies
(if this were possible, operation scales and consumptions would diverge while prices
would vanish, a situation that is often described as the ‘Land of Cockaigne’). We shall
refer to the case η
0, which turns out to have special physical properties, as the limit
of ‘marginally eﬃcient technologies’.

→

−

We therefore have a deal of control parameters: N, P , η, g(∆), u(x) and ρ(y). In
what follows we shall concentrate mostly on the role of η and of the relative number of
technologies N/P . In particular, we shall consider the ‘thermodynamic limit’ N
with n = N/P ﬁnite.

→ ∞

Statistical mechanics with a single consumer The problem of ﬁnding the

3.3.2.
equilibrium can easily be seen to be equivalent to calculating

U

max
0
si
}
≥
{

 

y +

siξi

!

i
X

(62)

33

(63)

(64)

(65)

(66)

(68)

CONTENTS

In fact, ﬁrst, if (62) is solved then the society evidently maximizes its utility. On the
other hand, producers also maximize proﬁts since ∂siU =
i ∂xµu = λ∂siπi, where the
last equality follows from the fact that, by virtue of the budget constraint, ∂xµU = λpµ
with λ > 0 a Lagrange multiplier. Thus prices disappear from the problem in explicit
form. However a remarkable outcome of the statistical mechanics approach is that
average prices and price ﬂuctuations, like other relevant macroscopic observables, turn
out to be directly connected to or easily derived from the spin-glass order parameters
that emerge from the calculation, as we shall see later on.

µ ξµ

P

The statistical mechanics approach starts with the observation that if U is a

suﬃciently regular function one expects a self-averaging condition to hold, i.e.

1
N

lim
N
→∞

where

hh· · ·ii

U

max
0
si
}
≥
{

 

y +

siξi

= lim
N
→∞

!

i
X

1
max
N **
0
si
}
≥
{

U

y +

 

stands for an average over the quenched disorder
µ ξµ
µ ξµ
i + ǫ)
P

i + ǫ)

hh· · ·ii

i δ(
Q

= h· · ·
h

i δ(

i
ξ

i

ξ

!++

i
X

siξi

:

ξi}

{

Now the right-hand side of the above expression can be evaluated by introducing the
‘partition function’

P

Q

Z =

∞

dx eβU (x)

∞

0
Z

ds δ

x

y

−

−

 

siξ

!

i
X

0
Z

and deﬁning the ‘free energy’
1
βN hh

f (β) = lim
→∞

N

log Z

.

ii

As usual

1
N **

y +

U

lim
N
→∞

max
0
si
{
}
≥
since in the limit β
conﬁgurations that maximize U give the dominant contribution
to the partition function. The evaluation of f ultimately leads to the identiﬁcation of a
function G of a vector ω of macroscopic order parameters such that

= lim
β
→∞

→ ∞

!++

i
X

f (β)

(67)

 

siξi

f (β) = extrωG(ω)

lim
β
→∞

where extr means that the solution is provided by the saddle-point of G, that is by the
vector ω⋆ solving ∂ωG = 0. The convexity assumptions made on u ensure that the
relevant saddle point is of replica-symmetric form (as in (19)). Under this condition, ω
turns out to be a six-component vector (ω =

) and G takes the form

Q, γ, χ,

χ, κ,

G(ω) =

Q

χ

+

κ

κ +

1
2

γχ
2n

−

1
n

b

+

1
n

max
0
x
≥

b
u(x)
(cid:20)

(cid:28)

{
χs2 + st

∆

1
2

−

(cid:20)

(cid:28)

max
0
s
≥
1
2χ

−

p
nQ + κ

b
y + t

x

−

(cid:16)

p

κ
}

−

2

b
(cid:21)(cid:29)t,y

(cid:17)

∆(γ
b

κ2)
b

−

κs√∆

η

(cid:21)(cid:29)t,∆

b

(69)

CONTENTS

h· · ·ix denotes an average over the random variable x, t is a unit Gaussian random
where
variable and averages over ∆ and y are performed with distributions g(∆) and ρ(y).
Before discussing the economic interpretation of the order parameters let us notice that
G is composed of two “representative agent” problems:

an ‘eﬀective proﬁt’ maximization by a representative ﬁrm, whose solution reads

•

s⋆

≡

s⋆(t, ∆) = 


η
tσ
κ
−
χ√∆
b

0

b

for t

η

κ/σ

≥

otherwise
b

where we deﬁned σ =

γ

κ2


−

•

an ‘eﬀective utility’ maximization by the society with respect to the consumption
of an eﬀective commodity, whose solution, namely

p

b

x⋆

x⋆(t, y)

such that χu′(x⋆) = x⋆

y + t

nQ + κ

(71)

≡

−

is always positive provided the assumptions on u are satisﬁed

p

These two ‘eﬀective’ problems – which have been derived and not postulated a priori –
are interconnected by the remaining terms.

The saddle-point equations ∂ωG = 0 for (69) have the following form:

Q =
n
(cid:10)
σ

χ =

∆(s⋆)2

t,∆

ts⋆√∆
(cid:11)

κ = χ

D
κ + nη

t,∆
E
s⋆√∆

t,∆

E

it,y − h

u′(x⋆)

2
t,y
i

h

κ =

σ =
b

u′(x⋆)
b

D
it,y
u′(x⋆)2
h
q
tu′(x⋆)
χ = h
√nQ

it,y

b

κ represents the optimal average (relative) price. In fact,
One sees immediately that
utility maximization under budget constraint gives ∂xµU = λpµ, with λ > 0 a Lagrange
multiplier that can be set to 1 without any loss of generality. It then follows that σ yields
price ﬂuctuations. It is remarkable that the macroscopic order parameters introduced
with a purely ‘physical’ method can be seen to possess such clear economic meanings. It
is also remarkable that the following laws can be derived, with minimal manipulations,
from the above set of equations:

b

nη

s⋆√∆

x⋆

y

it,y =

−
u′(x⋆) (x⋆

−
y)

h

h

D
it,y = 0

−

t,∆

E

The former expresses the fact that at the relevant saddle point the market-clearing
condition is satisﬁed (to compare, just average (61) for L = 1 over µ taking the constraint
on technologies into account). The latter expresses the fact that at the relevant saddle

34

(70)

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

35

(80)

(81)

(82)

(83)

(84)

(85)

CONTENTS

point the consumer saturates his/her budget when choosing his consumption, a condition
known in economics as Walras’ law [22].

It is possible to obtain a more precise characterization of the macroscopic
properties by calculating the distribution of operation scales, consumptions and prices
at equilibrium. These quantities are given respectively by

P (s) =

δ(s

h

s⋆)

it,∆ =

−

g(∆)P (s

∆)d∆

|

∞

0
Z

∞

ρ(y)P (x
|

y)dy

δ(x
h
δ(p

x⋆)

it,y =

u′(x⋆))

0
Z
it,y

−

−

P (x) =

P (p) =

h
∆) and P (x
|

where P (s
y) denote respectively the probability distributions of operation
scales at ﬁxed ∆ and of consumptions at ﬁxed y. These can be calculated easily from
(70) and (71). One ﬁnds

|

χs√∆+η

κ)2

2σ2

b

θ(s)

(

e−

χ
√2πσ2
b
χu′(x)+κ)2
2nQ

−

−

y

b

(x

P (s

∆) = (1

φ)δ(s) +

|

1

−
χu′′(x)

−
√2πnQ

e−

y) =

P (x
|
erf η

|

∆) = √∆P (s√∆
|
1)dk

k2g(k2/s2)P (k

∞

P (s) =

2
s3

where φ = 1
2[1
Moreover, notice that P (s

κ
σ√2
b

−

] is the fraction of active ﬁrms (i.e. ﬁrms such that si > 0).
1), which implies that

0
Z

|
Thus, power-law distributed operation scales are found for broad classes of distributions
∆γ for ∆
g(∆), as P (s)
1. Recently, some empirical evidence
has been found that distributions of ﬁrm sizes (deﬁned by the number of employees,
proﬁts etc.) have scaling forms [33].

2γ when g(∆)

s−

≪

∝

≃

−

3

≃

≃

1/2 for small n, and a second one for large n where nφ

Numerical solution of the saddle-point equations for a generic choice of the
parameters yields the picture illustrated in Figures 13 and 14. The quantity φ is shown
in Fig. 13 against numerical simulations. One sees that there are two regimes: one
where φ
1, so that the
number of active ﬁrms equals that of commodities, signaling a saturated market. Fig.14,
instead, shows that the average scale of production increases when n grows as long as n
is suﬃciently small. This means that the introduction of new technologies (i.e. from an
increase of n) leads to an increased production activity of existing ﬁrms if the number of
competitors is low. In parallel, relative price ﬂuctuations decrease, as does the average
level of consumption, signaling that ﬁrms are managing to transform abundant goods
into scarce ones. When n is close to 2, operation scales become larger and larger as η
decreases (i.e. as technologies become more and more eﬃcient) and ultimately develop a
singularity at nc in the marginally eﬃcient limit η
0 (see below for more details about
this limit). The ﬂuctuations of relative consumptions start to drop (the sharper the
lower is η), as the distribution of consumptions becomes more and more peaked around
the mean value. Identifying abundant (or scarce) goods becomes increasingly hard. In

→

CONTENTS

36

1

0.8

0.6

0.4

0.2

φ
n

0.5

φ

0

0

1

n

10

0

0

2

4

6

n=N/P

8

10

Figure 13. Behavior of nφ (φ = fraction of active companies) at equilibrium as
a function of n for η = 0.05: analytical prediction (continuous line), computer
experiments with P = 16 (dotted line) and for P = 32 (dashed line) averaged over
100 disorder samples. Dots represent results of a single realization of the technologies.
Inset: φ vs n for η = 0.01, 0.05, 0.1, 0.5 (top to bottom). From [31].

η=0.01
η=0.05
η=0.10
η=0.50

1.5

1

0

1

1.5

0.5

0

0

*

>

s
<

0.5

/

>
p
<
>
p
δ
<

η=0.01
η=0.05
η=0.10
η=0.50

1

*

>

x
<

0.9

1

0.5

*

>

x
<
>

/

*

x
δ
<

1
n=N/P

10

0

0

1
n=N/P

10

−

Figure 14. Typical macroscopic properties of competitive equilibria for g(∆) =
1), u(x) = log x and ρ(y) = e−y. Left panels: typical operation scale (top)
δ(∆
and relative price ﬂuctuations at equilibrium for diﬀerent values of η. Right panels:
typical consumption and relative consumption ﬂuctuations for diﬀerent values of η
(from [31]).

→

high n regime, the introduction of new technologies, by e.g. technological innovation
N + 1), leads to a decrease in the average operation scale, i.e. new proﬁtable
(N
technologies punish existing ones. The economy becomes strongly selective as ﬁrms
cannot take advantage of the spread between scarce and abundant goods any longer.
On the other hand, the average consumption starts growing with n, as is expected
in a competitive economy that selects highly eﬃcient technologies. In this phase the
introduction of new commodities (an increase in P ) leads to an increase in the scale of
operations.

CONTENTS

37

The above results conﬁrm rather clearly that the collective properties of competitive
equilibria display a marked qualitative change when n increases, as one passes from an
expanding to a saturated regime around n
2. Such a change is a smooth crossover
for any ﬁnite η > 0. However, in the limit η
0 in which technologies are ‘marginally
eﬃcient’ the crossover becomes a sharp second-order phase transition characterized by
the fact that φ = 1/2 for n < nc and φ < 1/2 otherwise, whereas

≃
→

(86)

s⋆

n

1/2

−

nc|

n

nc| ≪

1

|

h

−

i ≃ |

−
(see [31] for analytical details). This can be explained intuitively by a simple geometric
argument. Let us write the initial endowments as yµ = y + δyµ, separating a constant
part (y) from a ﬂuctuating part (δyµ) such that
µ δyµ = 0. Now market clearing with
δy, so that all the transformations take place in the space
y = ξi ·
η = 0 implies that ξi ·
δy < 0
orthogonal to the constant vector. This means that those technologies with ξi ·
which reduce the initial spread of endowments δx0 lead to a increase in wealth and
hence will be run at a positive scale. Those with a positive component along δx0 will
have si = 0. Given that the probability to generate randomly a vector in the half-space
δy < 0 is 1/2, when N is large we expect N/2 active ﬁrms. Still the number of
ξi ·
possible active ﬁrms is bounded above by P , hence when n = N/P = 2 the space of
technologies becomes complete and xµ = y for all µ. There is no possibility to increase
welfare further.

P

3.3.3. Case of many consumers
linear activities ξµ
si ≥
that consumers will buy at prices pµ. The proﬁt of ﬁrm i is given by πi = si

In the model just described, there are N ﬁrms running
i , which are vectors in a P -dimensional commodity space, at a scale
0. These ﬁrms face a demand function Qµ(p) from consumers, which is the quantity
µ=1 pµqµ
i .
Let us consider a more general case. Let us assume there are L consumers, each
with an initial endowment yµ
ℓ of commodity µ and each taking a share θiℓ in the proﬁt of
ﬁrm i. We assume that consumers face ﬁxed prices pµ. So the initial wealth of consumer
ℓ = 1, . . . , L is

P

P

P

N

wℓ =

pµyµ

ℓ +

θiℓπi

µ=1
X

i=1
X

P

xµ
ℓ =

wℓ
P pµ

If consumers are identical, apart from the initial endowments, and aim at maximizing a
P
µ=1 log xµ as before, the solution is relatively straightforward:
utility function U(x) =
the problem of consumer ℓ is solved by

(i.e. each consumer distributes his wealth uniformly over commodities, taking prices
into account). Now the total demand function will be

Qµ =

L

Xℓ=1

xµ
ℓ =

W
P

1
pµ ,

W =

wℓ

L

Xℓ=1

(87)

(88)

(89)

38

(90)

(91)

CONTENTS

In a pure exchange economy (without production: si = 0
equal to total initial endowment of each commodity, i.e.

∀

i) the above quantity will

Qµ = yµ

L

yµ
ℓ

≡

Xℓ=1

If yµ
ℓ are drawn independently at random with mean y and variance D, then yµ will have
mean Ly and variance LD and the relative ﬂuctuations of the total initial endowments
will be δy/y = √D/(√L¯y), which decreases as L increases. When we allow ﬁrms to
operate (si > 0), relative ﬂuctuations in the demand must be expected to be of the same
order

Qµ

Q

−
Q

1
√L

,

∼

Q =

1
P

P

Qµ.

µ=1
X
Therefore, by equation (89), relative price ﬂuctuations will also be of the order 1/√L.
This simple argument explains how the diﬀerent macroscopic quantities re-scale in the
presence of L consumers when L
. We remark that Ref. [31] shows that the
scales of production have a non-trivial behavior in the limit of extremely uniform initial
endowments, which suggest an essential singularity

c/√L) as L

→ ∞

exp(

s

.

h

i ∼

−

→ ∞

The case of consumers with diﬀerent utility functions requires a more involved
approach, because the heterogeneity of consumer utility is likely to imply a non-
symmetric demand function (even when prices pµ are all equal). Apart from this, it
is reasonable to expect that the basic insights gained from the above analysis, such as
the presence of a cross-over between two structurally diﬀerent phases of the economy,
will remain valid.

3.4. Economic growth: the Von Neumann problem

Von Neumann’s expanding model addresses the issue of computing the maximum
achievable growth rate of a linear production economy [34]. Economic growth is seen
basically as an autocatalytic chemical process in which technologies play the role of
reactions and commodities of reactants. In spite of its extremely simple setup, the model
has played a key role in the mathematical theory of economic growth, particularly in
view of its connection to dynamical growth via the so-called turnpike theorems [35].

aµ
i }

The time-dependent model is deﬁned as follows. One considers an economy with
P commodities (labeled µ) and N linear technologies (labeled i), each of which can
be operated at a non-negative scale Si ≥
0 and is characterized by an output vector
, such that Siaµ
bµ
ai =
i ) denotes
i }
{
the units of commodity µ produced (respectively used) by process i when run at scale
Si. It is assumed that input/output vectors are ﬁxed in time and that operation scales
are the degrees of freedom to be set, for instance, by ﬁrms. At time t, the economy
is characterized by an aggregate input vector I(t) =
i Si(t)bi and output vector

and by an input vector bi =

i (respectively Sibµ

{

P

CONTENTS

O(t) =
the rest, namely
P

i Si(t)ai. Part of the latter will be used as the input at period t + 1 whereas

C(t)

O(t)

I(t + 1)

≡

−

is consumed. In absence of external sources, in order to ensure stability it is reasonable
to require that inputs at any time do not exceed the outputs at the previous time, i.e.
one must have C µ(t)
0 for all µ at all times. Let us focus on solutions in which input
vectors grow in time at a constant rate, i.e. of the form I(t + 1) = ρI(t) with ρ > 0 a
constant (the growth rate). For these solution, the scales of production have the form
Si(t) = siρt, and likewise C(t) = cρt. Therefore the stability condition can be re-cast
in the form

≥

cµ

≡

si (aµ

i −

ρbµ
i )

0

≥

µ

∀

i
X

0

{

}

si ≥

The (technological) expansion problem amounts to calculating the maximum ρ > 0
such that a conﬁguration s =
satisfying the above condition exists (it is
easy to show that such an optimal growth rate exists [24]).
In such a conﬁguration
the aggregate output of each commodity is at least ρ times its aggregate input. If the
maximum ρ, which we denote by ρ⋆, is larger than 1 the economy is ‘expanding’, whereas
it is ‘contracting’ for ρ⋆ < 1. On the other hand, the actual value of ρ⋆ is expected to
Intuitively, ρ⋆ should increase with the
depend on the input and output matrices.
number N of technologies and decrease when the economy is required to produce a
larger number P of goods.

In [36] this problem was tacked in the limit N
i , bµ

with n = N/P ﬁnite
→ ∞
under the assumption that (aµ
i ) are independent and identically distributed quenched
random variables for each i and µ, with the aim of uncovering the emerging collective
properties that are typical of large random realizations of a complex wiring of input-
i ) and bµ
output relationship. To begin with, let us write aµ
i ), where
a and b are positive constants while αµ
i are zero-average quenched random variables.
Inserting these into (93) one easily sees that, to leading order in N, the optimal growth
rate ρ⋆ is given by the ratio a/b of the average output and average input coeﬃcients,
hence it is independent of the speciﬁc input-output network. The non trivial aspects of
the problem are related to the corrections to the leading part. We therefore write the
growth rate as

i = a(1+αµ

i = b(1+βµ

i , βµ

ρ =

1 +

a
b

(cid:18)

g
√N (cid:19)

so that, assuming a = b for simplicity, (93) becomes

cµ
¯a

=

i
X

si

αµ
(cid:20)

i −

g
√N −

1 +

(cid:18)

g
√N (cid:19)

βµ
i

≥

(cid:21)

0

µ

∀

The problem thus reduces to that of ﬁnding the largest value g⋆ of g for which it is
one may
possible to ﬁnd coeﬃcients

satisfying (95).

In the limit N

0

si ≥

{

}

→ ∞

39

(92)

(93)

(94)

(95)

CONTENTS

40

0

−10

2
/
1

)
n
k
(
/

g

*

−20

0.5

1

0

φ0
ψ0

100
n

10−2 10−1

100

101

n

102

10−2

10−1

101

102

Figure 15. Behavior of g⋆/√kn vs n.
(from [36]).

Inset: φ0 and ψ0 (related by (100)) vs n

resort to a Gardner-type calculus [37]. Deﬁning the characteristic function

χ(s) =

1
√N

θ

"

µ
Y

si

αµ

i −

i
X

(cid:20)

g

√N −

(cid:18)

1 +

g
√N (cid:19)

βµ
i

(cid:21)#

one can write the typical volume of conﬁguration space occupied by micro-states
satisfying (93) for N

at ﬁxed g is given by

Vtyp(g) = lim
→∞
where V (g) is the volume of solutions at ﬁxed disorder:

log V (g)

ii

N

→ ∞

1
N hh

V (g) =

∞

χ(s)δ

0
Z

N

si −

!

ds

 

i
X

i si =
(without aﬀecting the optimal growth rate, we introduced a linear constraint
N). It is reasonable to expect that, when g increases, Vtyp(g) shrinks, and in particular
g⋆. Now after carrying out the disorder average (see [36] for
that Vtyp(g)
details), which only depends on

0 for g

P

→

→

k =

(βµ

i −

i )2
αµ

DD

EE

the key macroscopic order parameters turns out to be the overlap qℓℓ′ = (1/N)
i siℓsiℓ′
between diﬀerent optimal conﬁgurations ℓ and ℓ′. Because the space of solutions
si}
is a convex set (by construction), the replica-symmetric approximation, for which
{
qℓℓ′ = q + χδℓℓ′ is in this case exact. Note that χ, which describes the ﬂuctuations of si
g⋆, hence the conditions g = g⋆ and
among feasible solutions, should also vanish as g
χ = 0 are equivalent and the analysis of optimal states coincides with the study of the
χ

0 limit of the replica-symmetric solution.
Results for the re-scaled quantity g⋆/√nk are shown in Fig. 15. The line separates
g⋆ from the region of unfeasible solutions. g⋆

the region of feasible solutions with g

P

→

→

≤

(96)

(97)

(98)

(99)

CONTENTS

41

(100)

crosses the line g = 0 (i.e. passes from a regime with growth rate ρ < a/b to one with
growth rate ρ > a/b) at nc = 1. In the inset we show the fraction of inactive processes
ψ0 (i.e. such that si = 0) and that of intermediate commodities φ0 (i.e.
such that
cµ = 0) at g = g⋆, as a function of n. These are found to be universal functions of n
independent of the details of the disorder distribution, related by

φ0 = n(1

ψ0)

−

Both φ0 and ψ0 tend to one when n increases, meaning that the ‘expanding phase’ at
n > nc is highly selective. The condition (100) has a simple geometrical interpretation:
it implies that the number of active processes equals that of intermediate commodities
at g⋆. Noting that for any µ such that cµ = 0 we have a linear equation for the scales
si > 0, we see also that (100) simply corresponds to the requirement that the number
of equations should match the number of variables.

Based on these results one can speculate on how long term growth will be aﬀected
by technological innovation. The latter, deﬁned as the introduction of new processes,
i.e. new feasible ways of combining inputs to produce desirable outputs [38] would
just correspond to an increase in the number N of transformation processes which the
economy has at its disposal. Now the change in the growth rate is related to the change
in g⋆/√n, which is given by
g
n3/2√P

(101)

≃ −

δn,

a
b

δρ

Therefore an increase in N can have a large positive impact on long term growth when
1) instead, g⋆/√n increases
n is small. For technologically mature economies (n
much more slowly, hence technological innovation has much smaller eﬀect on long term
growth.

≫

4. Toy models of ﬁnancial markets: Minority Games

4.1. Introduction

The Minority Game (MG for short) [39] is a strict relative of the El Farol problem (it
corresponds roughly to the case L = N/2) that has been proposed to model speculative
trading in ﬁnancial markets, that is systems where agents buy and sell asset shares
with the only goal of proﬁting from price ﬂuctuations. The basic idea is that when most
traders are buying it is proﬁtable to sell and vice-versa, so that it is always convenient to
be in the minority group. Abstracting, one considers the following situation. We have N
agents, each of which has to formulate at every time step t a binary bid bi(t)
1, 1
}
(buy/sell). The payoﬀ received at time t by each agent depends both on his/her action
and on the aggregate action A(t) =
i bi(t) (the ‘excess demand’) and it is given by
πi(t) =
bi(t)A(t). Thus, agents in the minority group win. The minimal measures of
eﬃciency to be employed are the average excess demand and ﬂuctuations in the steady

∈ {−

P

−

CONTENTS

state:

42

A
i

h

= lim
T,Teq

T

→∞

1

−

Teq

T

t=Teq
X

A(t)

and

σ2 =

A2

(102)

(cid:10)
= 0 and σ2 is
where Teq is an equilibration time. An eﬃcient state is one where
small. Notice that the number of people which could have been accommodated in the
/2, hence σ is a measure of the waste of resource. What remains to be
minority is
speciﬁed is how agents make their decisions. Agents who buy or sell at random with
= 0 and σ2 = N. Of
equal probability at every time step lead to a state where
course, it is the way in which agents take their decisions (which needs to be speciﬁed)
and their interactions that gives rise to the complex collective behavior.

(cid:11)
A
i

A
i

A
|

h

h

|

The MG is a useful toy model that allows to elucidate the collective behavior of
systems of heterogeneous interacting agents by addressing directly the interplay between
microscopic behavior and macroscopic properties (ﬂuctuations, predictability, eﬃciency,
etc.). From a purely theoretical viewpoint, the detailed study of the emergence of
cooperation in competitive systems makes the Minority Game a benchmark model of
interacting agents.
It has however also turned out to be able to reproduce, to some
extent, the rich statistical phenomenology of ﬁnancial markets, that are well known (and
at least since [40]) to be characterized by clear statistical regularities, often referred to
as “stylized facts”

.

There are at present a few comprehensive books that cover many aspects of the MG,
from both the theoretical viewpoint and the ﬁnancial market viewpoint [42, 43]. Here
we shall consider some basic and extended aspects of the model that are only marginally
treated elsewhere. In this section we shall concentrate mainly on the original model,
ﬁrst presenting a more thorough derivation of the minority rule, then a simple version of
the MG and ﬁnally discussing the standard model. The next section is instead devoted
to some extensions that have a particularly interesting physical content.

§

4.2. From agents’ expectations to the minority (and majority) rule

The connection between MGs and ﬁnancial markets can be established na¨ıvely by
observing that markets are instruments for allocating goods. This, combined with the no
arbitrage hypothesis according to which no purchase or sale by itself may result in a risk-
less proﬁt, suggests that markets should in principle be zero-sum games. Transaction
costs make it a game that is unfavorable on average, i.e. a Minority Game. It would
however be important to understand whether the minority mechanism can be derived
from a particular microscopic scheme. This is indeed possible [44].

An ever increasing number of such facts are documented in the literature. The best known of these
§
are the following: (a) asset returns are approximately uncorrelated beyond a time scale or the order of
tens of minutes; (b) the unconditional distribution of returns displays a power-law tail with an exponent
ranging from 2 to 4 for diﬀerent stocks and markets; (c) the distribution of returns over a time scale
τ becomes more and more Gaussian as τ increases; (d) volatility is positively autocorrelated over
time scales as long as several days, implying that periods of high volatility cluster in time (‘volatility
clustering’). See [41] for details.

CONTENTS

Let us imagine a market in which N agents submit their orders ai(t) for a certain
asset simultaneously at every time step t = 1, 2, . . .. Let ai(t) > 0 mean that agent
i contributes ai(t)e to the demand for the asset while ai(t) < 0 means that i sells
1)) of
i ai(t), the demand is given by D(t) = N +A(t)
,
1) . Finally, assume that the price is ﬁxed by the

ai(t)/p(t
−
ai(t)
|
whereas the supply is S(t) = N
market clearing condition, p(t) = D(t)/S(t), i.e.

1) units of asset, which is the current equivalent (i.e. at price p(t

−
|e. With ai(t) =

1 and A(t) =
A(t)

−
2p(t

P

±

−

−

2

Taking the logarithm of both sides and expanding to the leading order one gets

p(t) = p(t

1)

−

N + A(t)
A(t)
N

.

−

log p(t)

log p(t

1)

−

−

≃

A(t)
λ

with λ = N. The quantity on the left-hand side is normally called the ‘return’ of the
asset. A(t) is instead the excess demand, namely the diﬀerence between demand and
supply. This equation expresses the dynamics of prices in terms of an aggregate quantity
A(t) that all agents contribute to form [45]. A(t) may thus be considered a proxy for
the return.

Now take agent i and assume he must decide whether to buy or sell at time t. To
do this, he should compare the expected proﬁt (or utility) of the two actions, which
depends on what the price will be at time t + 1. For instance the utility he would face
at time t + 1 if he buys 1e of asset at time t (i.e. ai(t) = 1) is given by

ui(t) =

p(t + 1)

1

p(t) −

(ui(t) > 0 if p(t + 1) > p(t)). At this stage the price p(t + 1) is unknown to him (and
presumably to everybody else). Therefore if our agent i wants to use Eq. (105) to make
his choice at time t, he has to replace p(t+1) by the expectation he has at time t of what
the price will be at time t + 1, denoted by E(i)
t

[p(t + 1)]. Let us assume that that [44]

E(i)
t

[p(t + 1)] = (1

ψi)p(t) + ψip(t

1)

−

−

The parameter ψi allows to distinguish two types of traders, depending on whether ψi
is positive or negative. Agents with ψi > 0 believe that market prices ﬂuctuate around
a ﬁxed value (the ‘fundamental’), so that the future price is an average of past prices.
For this reason these agents are called ‘fundamentalists’. They may also be called
p(t)
contrarians since they believe that the future price increment ∆p(t + 1) = p(t + 1)
is negatively correlated with the last one

−

E(i)
t

[∆p(t + 1)] =

ψi∆p(t).

−

On the other hand, if ψi < 0 the agent believes that the future price increment will
occur in the direction of the trend deﬁned by the last two prices, so that future price
increments ∆p(t + 1) are positively correlated with the past ones, as if the price were
following a monotonic trend. This type of agents are called ‘trend followers’.

43

(103)

(104)

(105)

(106)

(107)

44

−

(108)

CONTENTS

p(t

−

The expected utility for buying at time t will be E(i)
t
1)]/p(t) which, using (103), becomes

[ui(t)

ai(t) = +1] =

ψi[p(t)

|

−

E(i)
t

|

[ui(t)

ai(t) = +1] =

2ψiA(t)/[N + A(t)]

−

−

E(i)
t

2ψiai(t)

[ui(t)] =

A similar calculation can be carried out for the expected utility for selling at time t.
The net result is that the expected utility for action ai(t) at time t can be written as
A(t)
N + ai(t)A(t)
Notice that agents who took the majority action ai(t) = sign[A(t)] expect to receive
a payoﬀ
] whereas agents in the minority group expect to get
A(t)
2ψi|
It is clear that the expected payoﬀ of fundamentalists (resp.
trend-followers) is positive when they are in the minority (resp. majority) group.
Therefore Minority Games are simple schemes for describing the behavior of contrarians
whereas Majority Games are appropriate for trend-followers.

2ψi|
−
/[N
− |

/[N +
].

|
A(t)

(109)

A(t)

A(t)

|

|

|

|

.

In real markets, both groups are present and the resulting price dynamics stems
from a competition between the two groups [46]. Which group dominates and shapes
the price dynamics depends on the evolution of traders’ expectations, which in turn
depends on the behavior of price itself. Common sense suggests that when everybody is
going to buy the price will rise and it will be convenient to buy. Accordingly, speculative
markets in certain regimes (e.g. bubbles) should look more like Majority Games rather
than Minority Games (and vice-versa in other regimes). If all traders base themselves on
the same price history, expectations should converge and traders would end up playing
either a Majority or a Minority Game. But of course agents revise and calibrate their
expectations according to the real price history so fundamentalists and trend-followers
coexist symbiotically in real markets. The problems with arguments in support of either
the Minority or the Majority Game essentially arise from the fact that the objective
assessment of the validity of a trading strategy is a complex inter-temporal problem that
cannot be based on the result of a single transaction: whether buying today is proﬁtable
or not depends on what the price will be when one sells. Hence the payoﬀ of a single
transaction is hardly a meaningful concept unless one considers round-trip (buy/sell or
sell/buy) transactions. From this point of view the MG is a rather crude approximation.
Yet, we shall see below that it provides a remarkably rich and realistic picture of ﬁnancial
markets as complex adaptive systems. Models of interacting fundamentalists and trend-
followers will be addressed in the following section.

4.3. The simplest Minority Game

Before considering the model in its full complexity, it is instructive to to take a glimpse
at a minimal version with inductive agents in which the collective behavior can be
easily understood with simple mathematics [44]. Let us suppose that traders employ a
probabilistic rule of the form

Prob
{

bi(t) = b
}

= C(t) exp [b∆i(t)]

b

∈ {−

1, 1

}

(110)

CONTENTS

45

(111)

(113)

(114)

where C(t) is a normalization factor and ∆i(t) accounts for the agent’s expectations
about what will be the winning action (if ∆i(t) > 0 then he/she will choose bi(t) = 1
with higher probability). The ‘score function’ ∆i is updated according to

∆i(t + 1)

∆i(t) =

ΓA(t)/N

−

−

with Γ > 0 a constant, so that if A(t) < 0 agents increase ∆i and the probability of
choosing action 1. Let us ﬁnally assume that the initial conditions ∆i(0) are drawn
from a distribution p0(∆) with standard deviation s. How does the collective behavior
depend on the parameters Γ and s?
Notice that y(t) = ∆i(t)

1,
the law of large numbers allows us to approximate A(t) by its average with probability
distribution (110). This yields an approximate dynamical equation for y(t):

∆i(0) does not depend on i, for all times. For N

≫

−

Γ

−

y(t)

y(t + 1)

tanh[y(t) + ∆(0)]
h

(112)
≃
i0 is on the distribution p0 of initial conditions. Eq. (112) admits
where the average
a ﬁxed point y(t) = y⋆, with y⋆ the solution of
= 0. Let us
assume that this solution is stable. This describes a stationary state where the relative
scores ∆i(t) are displaced by a quantity y⋆ from the initial conditions. This gives

tanh[y⋆ + ∆(0)]

i0 ≡ h

A
i

. . .

i0

h

h

N

σ2 =

1

2

aii

− h

= N

1

tanh[y⋆ + ∆(0)]2

0

−

i=1
X
Notice that σ2
N and it decreases with the spread of the distribution of initial
conditions. A linear stability analysis of Eq. (112) shows that these solutions are stable
only when

∝

(cid:10)

(cid:11)

(cid:0)

(cid:1)

(cid:3)

(cid:2)

Γ < Γc =

2

1

tanh[y⋆ + ∆(0)]2

− h

=

2N
σ2 .

i0

When Γ > Γc one ﬁnds periodic solutions of the form y(t) = y⋆ + z⋆(
1)t where y⋆ and
z⋆ satisfy certain prescribed conditions. The parameter z⋆ plays the role of an order
parameter of the transition at Γc (z⋆ = 0 for Γ < Γc). Again we have
= 0, but now
tanh[y⋆ + z⋆ + ∆(0)]
i

A
i
h
z⋆ + ∆(0)]
i

tanh[y⋆

(115)

N 2 h

σ2

−

−

2
0

h

2
0 +
2

≃

i.e. ﬂuctuations are proportional to N 2. Hence this is a much less eﬃcient state. The
orbits of the dynamics of y(t) for Γ < Γc and Γ > Γc are shown in Fig. 16 together with
the behavior of σ2/N 2. We conclude that the more heterogeneous the initial condition
is, the more eﬃcient is the ﬁnal state and the more the ﬁxed point y⋆ is stable. The
transition from a state where σ2
N 2 will turn out to be a
generic feature of MGs.

N to a state with σ2

∝

∝

4.4. The Minority Game

In the simple case discussed above, agents base their choice only on their past experience.
The standard Minority Game describes a more general situation in which traders use

CONTENTS

Γ=1.8

Γ=2.5

1

1

46

1

s=1/2
s=1

15

10

5

0

0

6
Γ

)
1
+
t
(
y

0

)
1
+
t
(
y

0

2

Ν

/

2

σ

0.5

Γ

c(s)

−1

−1

0
y(t)

1

−1

−1

0
y(t)

0

2

1

2

s

4

6

4

8

10

Figure 16. Left panels: the map y(t) for Γ = 1.8 < Γc and Γ = 2.5 > Γc for s = 0.
Right panel: global eﬃciency σ2/N 2 as a function of Γ for two diﬀerent sets of initial
conditions: ∆i(0) is drawn from a Gaussian distribution with variance s2. The full
line corresponds to s = 1/2 whereas the dashed line is the result for s = 1. The inset
reports the critical learning rate Γc as a function of the spread s of initial conditions.

1, 1

∈ {−

both their past experience and some (endogenous or exogenous) information pattern.
The model is deﬁned as follows [47]. There are N agents labeled i. At each time
step t agents receive one of P possible information patterns µ(t) (whose precise nature
will be discussed below) based on which each trader must formulate a binary bid
bi(t)
(g = 1, . . . , S) that map informations µ
component aµ
}
with equal probability for every i, g and µ at time t = 0 and is kept ﬁxed throughout the
game. Agents keep tracks of the performance of their strategies by means of valuations
functions or scores Uig that are initialized at some value Uig(0) and whose dynamics
reads

1, 1
ig of every strategy is selected randomly and independently from

. To this aim, each of them is endowed with S strategies aig =
1, . . . , P

aµ
ig}
{
. Each
1, 1

into actions aµ

ig ∈ {−

}
{−

∈ {

}

}

Uig(t + 1)

Uig(t) =

−

aµ(t)
ig A(t)/N

−

(116)

where A(t) =
i bi(t) is the excess demand at time t. At each round, every agent picks
the strategy gi(t) = arg maxg Uig(t) carrying the highest valuation and formulates the
P
corresponding bid: bi(t) = aµ(t)
gi(t). In this way, agents adopt at each time the strategy
they expect to deliver the highest proﬁt (the score of strategies forecasting the correct
minority action increase in time).

The nature of the information patterns µ(t) is still to be speciﬁed. In principle, the
natural choice corresponds to taking the string of the past m minority actions (hence
P = 2m) as the information fed to agents at every time step, with the idea to describe a
closed system where agents process and react to an information they produce themselves
collectively. We refer to this choice as the case of endogenous information. On the other
hand, one may think of replacing for the sake of simplicity the above information (which

CONTENTS

47

}

{

1, . . . , P

has a non-trivial dynamics itself) with an integer drawn at random at each time step
from
with uniform probability. This corresponds to the case of random
exogenous information [48]. Again, this replacement induces a major simpliﬁcation in
the structure of the model by turning a complex non-Markovian system with feedback
into a Markovian one. In addition and at odds with the El Farol problem, it was shown
that collective properties are roughly unaﬀected when real information is substituted
with random information. These results suggest that, to some extent, the feedback is
irrelevant as far as collective properties are concerned. We shall hence focus on the case
of exogenous information for the following sections. A more careful discussion of the
subtle case of endogenous information will be deferred to Sec. 4.8. In summary, the
Minority Game is completely deﬁned by the following rules:

gi(t) = arg max

Uig(t)

g
aµ(t)
igi(t)

A(t) =

i
X
Uig(t + 1)

Uig(t) =

aµ(t)
ig A(t)/N

−

−

(117)

i

−

A2
h

Let us now discuss the macroscopic properties of the model. Early works focused
on the cooperative properties of the system in the stationary state. The central quantity
of interest is the numerical diﬀerence between buyers and sellers at each time step, A(t).
It is easy to anticipate that none of the two actions
1 and 1 will systematically be the
minority one, i.e. that A(t) will ﬂuctuate around zero. Were it not so, agents could easily
improve their scores by adopting that strategy which visits most often that side. The size
of ﬂuctuations of A(t), instead, displays a remarkable non-trivial behavior. The variance
σ2 =
of A(t) in the stationary state measures the eﬃciency with which resources are
distributed, since the smaller σ2, the larger a typical minority group is. In other words
σ2 is a reciprocal measure of the global eﬃciency of the system. Early numerical studies
have shown that the relevant control parameter of the model is the relative number of
information patterns α = P/N. The behavior σ2 is illustrated in Fig. 17. With α ﬁxed,
one typically observes that A(t)
1
the information space is too wide to allow for a coordination and agents essentially
behave randomly as σ2/N
1, the value corresponding to random traders. As α
decreases, that is as more and more agents join the game or as the possible number of
information patterns decreases, σ2/N decreases suggesting that agents manage to exploit
the information in order to coordinate to a state with better-than-random ﬂuctuations.
It turns out that these steady states are ergodic, that is they are reached independently
of the initial conditions Uig(0). Lowering α further, ergodicity is lost and the steady
state depends on Uig(0). For the so-called ﬂat initial conditions, Uig(0) = 0 for all i and
g, which describe agents with no a priori bias toward one of their strategies, one is driven
into highly ineﬃcient steady states where σ2 diverges as α decreases approximately as
N 2. This behavior for has been attributed
σ2
to the occurrence of “crowd eﬀects”. Remarkably this ergodicity breaking transition is
related to a phase transition with symmetry breaking that was ﬁrst discovered by Savit

√N or equivalently that σ2 = O(N). When α

1/α. Notice that this implies σ2

≫

≃

≃

≃

≃

CONTENTS

48

2
10

1
10

0
10

-1

10

-2

10

α

c = 0.3374...

, y(0)=0

, y(0)=5

σ2

σ2

H

10

-3
0.01

0.10

1.00
α

10.00

100.00

Figure 17. Behavior of σ2/N and H/N versus α (analytical and numerical) for
diﬀerent initial conditions y(0) = Ui1(0)

Ui2(0).

−

≫

and coworkers [49] for the case of endogenous information. Reporting the frequency
with which the minority action was 1 conditional on the value of µ, they observed that
1 the minority was falling on either side with equal probability irrespective of µ.
for α
≪
But when α
1 the minority happened to be more likely on one side, depending on the
value of µ. These observations have been sharpened in a study that allowed to locate
0.34 for S = 2 where σ2 attains its minimum
the phase transition at the point αc ≃
(see next section for details). The transition separates a symmetric (α < αc) from an
asymmetric phase (α > αc). The symmetry which is broken is that of the average of
A(t) conditional on the history µ,
= 0 for a certain
µ then the knowledge of µ alone suﬃces for a non-trivial statistical prediction of the
= 0 for at least one µ. Thus the sign of
µ
A
sign of A(t). In the asymmetric phase,
|
h
A(t) is predictable, to some extent, on the basis of µ alone. A measure of the degree of
predictability is given by the function

. The idea is that if
i

A
|

A
|

i 6

i 6

µ

µ

h

h

H =

1
P

P

µ=1
X

2 .

A
|

µ
i

h

(118)

µ

= 0 for all µ and hence H = 0. H is a decreasing function
In the symmetric phase
of the number N of agents (at ﬁxed P ): newcomers exploit the predictability of A(t)
and hence reduce it. The behavior of H is also reported in Fig. 17. Notice that it acts
like a ‘physical’ order parameter.

A
|
h

i

4.5. Statistical mechanics of the MG: static approach

We shall discuss in this review two lines along which the statistical mechanics of the
Minority Game with random external information can be studied. The ﬁrst one is

CONTENTS

49

a static theory whose crucial steps are (a) ﬁnding a (random) Lyapunov function of
the dynamics that allows one to identify the steady states of the learning process
with its minima; (b) calculating the latter via the replica method. The second one
consists in constructing a dynamical mean-ﬁeld theory using the learning dynamics
as a starting point. The two approaches are essentially complementary: the statics
gives more information about the predictability and allows to interpret the collective
properties in terms of a minimized quantity; the dynamics focuses on ergodicity and
is a more appropriate setting to discuss ﬂuctuations. Below we will outline the static
approach to the standard MG for the case S = 2, deferring a discussion of the dynamical
method to Sec. 5.3. Other possibilities, like the ‘crowd-anticrowd’ theory [50] will not
be discussed here (an account can be found in [6]).

It is helpful for a start to introduce the auxiliary variables [47]

ξi =

ai1 −
2

ai2

,

ωi =

ai1 + ai2
2

,

yi(t) =

Ui1(t)

Ui2(t)

−
2

in terms of which (116) can be re-cast as

yi(t + 1)

yi(t) =

−

1
N

−

ξµ(t)
i A(t)

The advantage lies in the fact that the dependence of aigi(t) on the strategy valuation
can be made explicit by noticing that gi(t) = 1 if yi(t) > 0 and gi(t) = 2 if yi(t) < 0 (we
shall therefore refer to yi as the ‘preference’ of agent i). As a consequence, the relevant
microscopic dynamical variable is the Ising spin si(t) = sign[yi(t)]. On has in particular

aigi(t) = ωi + si(t)ξi

A(t) =

ωµ(t)
i + si(t)ξµ(t)

i

Ωµ(t) +

≡

si(t)ξµ(t)
i

i h

X

i
The dynamics (120) is non-linear in a way that doesn’t allow to write it in the form of a
gradient descent. However, as in the El Farol problem, one may regularize the dynamics
via a learning rate Γ > 0 such that [51]

i
X

Prob
{

gi(t) = g

= C(t) eΓUig(t)

}

C(t) = normalization

(123)

It is then possible to construct the continuous-time limit of (120) in view of the fact
that the dynamics possesses a ‘natural’ characteristic time scale given by P . Proceeding
as shown for the El Farol case, one arrives at the following continuous-time Langevin
process [16]:

˙yi(τ ) =

ξiΩ

−

−

ξiξj tanh[yj(τ )] + zi(τ )

zi(τ )zj(τ ′)

h

i ≃

ξiξjδ(τ

τ ′)

−

j
X
Γσ2
αN

where τ = Γt/P is a re-scaled time and σ2 is the volatility
k
an average over µ. One sees that in the limit Γ

and the over-line denotes
0, in which the dynamics becomes

→

Eq. (125) is based on a time-independent volatility approximation which happens to be very well

k
satisﬁed away from the critical line. We refer the interested reader to [16] for further details.

(119)

(120)

(121)

(122)

(124)

(125)

CONTENTS

deterministic, the system performs a gradient descent with a well-deﬁned Hamiltonian.
Indeed, in order to extract the steady state from the above process, one may take its
time average:

˙
yii

h

=

ξiΩ

−

−

ξiξjmj,

mi =

tanh(yi)

h

1, 1]

[
−

i ∈

j
X

It is now clear that the stationary values of the variables mi can be obtained from the
minimization of

50

(126)

(127)

H =

1
P

Ωµ +

ξµ
i mi

µ "
X

i
X

2

#

which coincides with the predictability in the steady state. Hence agents coordinate so
as to make the market as unpredictable as possible. This conclusion remains correct
even for Γ > 0: indeed mi are still given by the minima of H, though the dynamics is
no more deterministic (see [16]). Actually, within the approximation of Eq. (125), it
can be shown (see Sec. 4.6) that for α > αc the steady state is independent of Γ.

As usual, minimization of H is achieved through the replica trick as

1
N

H
N

lim
N
→∞

lim
0
r
→
The calculation is detailed at length in the literature (see e.g. [52]). The resulting phase
structure is as follows:

= lim
β
→∞

lim
N
→∞

Trm e−

min
m

(128)

(cid:28)(cid:28)

(cid:29)(cid:29)

log

(cid:10)(cid:10)(cid:2)

(cid:11)(cid:11)

(cid:3)

βH

r

1
βrN

•

•

for α larger than a critical value αc = 0.3374 . . . there is a unique (Γ-independent)
minimum with H > 0

for α < αc, there is a continuous of minima where H vanishes. The minimum
selected by the dynamics depends on initial conditions (and on Γ)

Hence the system at αc undergoes a phase transition from a predictable to an
unpredictable phase. Such a static transition corresponds to a dynamical instability in
the dynamics of preferences for Γ = 0. To see this, let us ﬁrst mention that in numerical
simulations one observes that yi either grows linearly with time or stays ﬁnite. Based
on this, one can conclude that solutions of (126) are of the form

= vit, with

yii

h

(129)

vi =

ξiΩ

−

−

ξiξjmj

j
X

and that there are two possibilities:

•

•

= 0 and yi(t) diverges as t

either vi 6
ends up using just one of his strategies (we call these agents ‘frozen’)
or vi = 0 and
ﬂipping between his strategies (we call these agents ‘ﬁckle’)

stays ﬁnite, in which case

→ ∞

yii

−

h

, in which case mi = sign(vi) the agent

1 < mi < 1 and the agent keeps

Let us consider the dynamics of preferences for ﬁckle agents. Setting yi(τ ) =
+ ǫi(τ )
where ǫi(τ ) describes small ﬂuctuations about the average, one can expand (124) to ﬁrst
order in ǫi(t):

yii
h

˙ǫi(τ ) =

−

Xj ﬁckle

ξiξj(1

m2

j )ǫj(t)

−

≡ −

Tijǫj(t)

Xj ﬁckle

(130)

CONTENTS

51

m2

−

j ). As long as the matrix T = (Tij) is positive deﬁnite, the
where Tij = ξiξj(1
above dynamical system will be linearly stable. Now T = U V with Uij = ξiξj and
< 1) all eigenvalues of V are positive
mi|
i )δij. But for ﬁckle agents (
Vij = (1
|
deﬁnite, so that det(T ) vanishes together with det(U ). The spectrum of the random
matrix U can be evaluated using random matrix theory. For our purposes it suﬃces to

m2

−

calculate the minimum eigenvalue, which turns out to be λ0 = 1
2

instability sets in when λ0 = 0, that is when

1
(cid:18)

−

1

φ

−
α

q

(cid:19)

2

. The

(131)

1

φ = α

−

This equation and the distinction between ﬁckle and frozen agents only depend on mi,
which are determined for α
αc by the unique minimum of H, independently of Γ.
Hence Eq. (131) and the location αc of the phase transition, are independent of Γ.

≥

4.6. The role of learning rates and decision noise

It is interesting to consider brieﬂy the impact that the introduction of a ﬁnite learning
rate Γ has on the properties of the model. Let us begin by noting that Γ, which at the
level of agents plays a role similar to an ‘inverse temperature’, at the collective level acts
instead as an eﬀective ‘temperature’, since it tunes the ﬂuctuating random component
in agent’s dynamics (see (125)). The larger Γ or, equivalently, the smaller the minimum
score diﬀerence agents can appreciate (this quantity is roughly of order 1/Γ), the more
the response ﬂuctuates and the longer it takes to average ﬂuctuations out and reach a
steady state.

We have anticipated above that Γ aﬀects the steady state only in the sub-critical

phase. Its eﬀect is particularly strong on the volatility, which can be written as

σ2 = H +

ξ2
i (1

m2

i ) +

−

ξiξj h

(tanh yi −

mi)(tanh yj −

mj)

i

(132)

i
X

=j
Xi
The dependence on Γ is only present in the last term on the right-hand side, which
measures ﬂuctuations of tanh yi around its mean. The average is over the distribution
of yi (which in turn depends on σ2 via the noise). The latter can be computed from
the Fokker-Planck equation associated to (124), which itself depends on σ2 (see 125).
Hence σ2 is determined by the solution of a self-consistent problem [16]. For α > αc,
ﬂuctuations of yi are independent and hence the third term of (132) is identically zero.
As a result, σ2 is independent of Γ, as conﬁrmed to a remarkable degree of accuracy
by numerical simulations [16]. When α < αc a correlation arises from the fact that the
dynamics is constrained to the subspace of y which is spanned by the P vectors ξµ, and
which contains the initial condition y(0). The dependence on initial conditions and the
dependence on Γ both arise as a consequence of this fact. Again, numerical simulations
fully conﬁrm this picture [16].

It is worth remarking that, the smoothed choice rule (123) can also be written as

si(t) = sign [yi(t) + ζi(t)/Γ]

(133)

6
CONTENTS

52

2 [1

where ζi(t) are independent identically distributed random variables with probability
density p(ζ) = 1
(tanh ζ)2]. Indeed, for Γ = 0 the noisy part of the argument of the
sign dominates and the agent selects his strategy at random with equal probability
at each time step, while for Γ
one recovers the original deterministic rule
si(t) = sign[yi(t)].

→ ∞

−

On the basis of this observation, Coolen et al. [53] introduce a diﬀerent type of

decision noise, called ‘multiplicative noise’, deﬁned as

si(t) = sign [yi(t) (1 + ζi(t)/Γ)]

(134)

which corresponds to

Prob
{

si(t) =

1
±

}

= C(t) e±

Γsign[yi(t)]

C(t) = normalization

(135)

It is evident that in this case frozen agents are aﬀected as well. Indeed, the critical point
αc turns out to depend rather strongly on Γ: when Γ gets smaller the informationally
eﬃcient phase shrinks as the critical point shifts to smaller values of α.

4.7. The role of market impact

}

≫

Ever since J. Nash’s pioneering work in game theory, that of Nash equilibrium (NE)
has been a reference concept in socio-economic systems of interacting agents. A NE
is in some sense an optimal state of strategic situations, one in which no agent has
incentives to deviate from his behavior unilaterally. It is easy to see that, a priori, the
Minority Game possesses a huge number of such states when N
1. In fact, there
is one symmetric NE in mixed strategies, where agents draw their bid bi at random at
= 1/2 for all i. This state has σ2 = N and H = 0.
bi = +1
every time step with Prob
{
N
pure strategy NE where half of the players take bi = +1
If N is even, there are also
N/2
2k agents play mixed
and the other half takes bi =
1. Moreover, states where N
strategies and the remaining 2k play pure strategies bi = +1 and bi =
1, are also NE.
Thus the game possesses an exponentially large number of Nash equilibria. One can then
ask whether the steady state of the model is one of them. The answer is a resounding
no. In this section we will study this issue and discuss the important question of why it
is so. Why are inductive agents playing sub-optimally? We shall see that at the heart
of the matter lies the consideration which agents have of their market impact, i.e. of
their impact on the aggregate quantity A(t). In fact, the inability to coordinate on a
NE follows from the na¨ıve idea that in a system of N agents every single agent ‘weights’
1/N and is thus negligible in the statistical limit N
. Once this assumption is
dropped and agents account for their own impact, the resulting steady state improves
dramatically and eventually a NE may be reached.

→ ∞

(cid:1)
−

−

−

(cid:0)

To begin with, it is instructive to study the role of market impact in the simplest
MG with P = 1 discussed in Sec. 4.3, in which agents must choose at each time step
between the two actions ai ∈ {−
. Let us consider the following modiﬁcation of the
learning dynamics (111):

1, 1

}

∆i(t + 1)

∆i(t) =

[A(t)

ηai(t)].

−

Γ
N

−

−

(136)

CONTENTS

The term proportional to η in (136) describes the fact that agent i accounts for his
own contribution to A(t). One indeed sees that (136) reduces to (111) for η = 0,
whereas for η = 1 agent i considers only the aggregate action of other agents,
A(t)
=i aj(t), and does not react to his own action ai(t). Values of η
between 0 and 1 tune the extent to which agents account for their “market impact”.

ai(t) =

−

j

It is easy to see that the dynamics for η = 1 behaves in the long run in a radically
diﬀerent way than for η = 0. Let us take the average of (136) in the steady state and
deﬁne mi =

. We note that

P

aii
h

∆i(t + 1)

∆i(t)

=

i

−

i − h

h

Γ
N "

j
X

mj −

ηmi

=

#

Γ
N

∂Hη
∂η

−

where

53

(137)

(138)

Hη =

2

mi

−

!

η
2

1
2  

i
X

m2
i .

i
X

−

∼

−

This implies that the stationary values of the mi’s are given by the minima of Hη.
Notice that H1 is a harmonic function of the mi’s. Hence it attains its minima on the
1, 1]N . So for η = 1 all agents always take the same actions
boundary of the hypercube [
ai(t) = mi = +1 or ai(t) = mi =
1 and the waste of resources is as small as possible,
as σ2 = 0 or 1 if N is even or odd, which is a tremendous improvement with respect to
the case η = 0 (where σ2
N or N 2). These states are indeed Nash equilibria of the
associated N persons minority game. This argument can be extended with some work
to all η > 0, and one can show that the stationary states of the learning process for any
η > 0 are Nash equilibria. Hence as soon as agents start to account for their market
impact (η > 0) the collective behavior of the system changes abruptly and ineﬃciencies
are drastically reduced. Furthermore, the asymptotic state is not unique (Hη possesses
more than one minimum!) and the one in which the system settles is selected by the
initial conditions. The set of equilibria is discrete and the system jumps discontinuously
from an equilibrium to another, as the initial conditions ∆i(0) vary. This also contrasts
with the η = 0 case, where the equilibrium shifts continuously as a function of the initial
conditions.

Let us now consider the full MG with market impact correction with public
information [52] (the above picture is representative of the situation in the MG in the
limit α

0), whose learning dynamics reads

Uig(t + 1)

Uig(t) =

−

aµ(t)
ig
N

−

A(t)

η

−

aµ(t)
igi(t) −

aµ(t)
ig

(139)

h
As before, η allows to interpolate between the naive ‘price-taking’ behavior of the
standard MG in which agents are unaware of their market impact (η = 0) and a more
sophisticated behavior where agents account for it. Note indeed that with η = 1 the
Uig(t) is proportional to the actual payoﬀ that agent i would
reinforcement Uig(t + 1)
have got had he actually played strategy g at time t. Hence in a way the above learning
process assumes that agents are able to disentangle their contribution from the aggregate

(cid:17)i

−

(cid:16)

→

6
CONTENTS

54

A(t). This may not be realistic in practical situations. For example imagine that, as in
the original version of the MG, agents only observe the sign of A(t) and not its value.
This information is not enough to infer the sign of A(t)
and hence the
payoﬀ they would have received if they had played strategy g instead of gi(t). However,
agents can approximately account for the market impact by rewarding the strategy they
have played by a reinforcement factor η, i.e.

aµ(t)
igi(t) + aµ(t)

−

ig

Uig(t) =

Uigi(t + 1)

A(t)
N
In fact, the collective behavior of the learning dynamics above is identical to that
obtained with (139). This is because what matters in the long run is the time average
of the processes, which is the same because

aµ(t)
ig

δggi(t)

(140)

η
N

−

−

+

At ﬁrst sight, the term proportional to η looks negligible with respect to A(t)
because it is of order one whereas A(t) = O(√N). However while A(t) ﬂuctuates
around zero, δs,si(t) has always the same sign. When the term proportional to A(t) is
averaged over the P = αN states µ it also becomes of order one. Hence the eﬀect of
the two terms is comparable in the long run. (A similar phenomenon occurs in spin
glasses where the naive mean ﬁled theory has to be corrected by the Onsager reaction
term to eliminate self-interaction eﬀects.). For generic η (0
1) the steady state is
described by the minima of

≤

≤

η

aigaig′i ≃

h

δg,g′.

Hη = H

η

−

ξ2
i (1

m2
i )

−

i
X

(141)

where H =
for their impact eﬀectively minimize ﬂuctuations.

A
|

µ
i

h

2 is the predictability. Note that H1 = σ2, so players who fully account

Unfortunately, the study of the ground state properties of Hη requires techniques
which are more sophisticated than those used for the MG. Indeed for η > 0 the simple
replica-symmetric solution that we have discussed so far becomes unstable against
perturbations that break replica permutation symmetry (this is related to the fact
that Hη has more than one minimum) and one needs to study more complicated
solution types [54]. The ensuing phase structure is shown in Fig. 18 The critical
line (analog to the de Almeida-Thouless line of spin-glass theory) can be calculated
straightforwardly using the dynamical stability argument mentioned at the end of Sec.
4.5. It suﬃces to replace Uij = ξiξj with Uij = ξiξj −
i . The resulting condition
√η)2 and coincides with the critical line for replica-symmetry
reads 1
−
breaking.

φ = α(1

ηδijξ2

−

The MG behavior (η = 0) is separated from the Nash equilibrium behavior (η = 1)
by a phase transition which is continuous for α > αc. Remarkably for α < αc the
transition occurs at η = 0 and it becomes discontinuous. As shown in Fig. 19, nothing
dramatic happens when crossing the transition for α > αc. For α < αc instead σ2/N
features a discontinuous jump across the transition line at η = 0. The origin of the
discontinuity lies in the dynamic degeneracy of the system for α < αc and η = 0.
Even an inﬁnitesimal change in η can dramatically alter the nature of the minima of

CONTENTS

55

RSB

η

0.5

RS

0.01

0.1

10

100

1
α

Figure 18. Phase diagram of the Minority Game in the (α, η) plane. The RS
region corresponds to the replica-symmetric phase and the RSB region to the replica
symmetry broken phase (from [54]). The mark corresponds to the critical point
RS transition is second order; below it, it is
αc ≃
discontinuous.

0.3374. Above it, the RSB

→

P=64, N=101: minority game
            minimization of H
P= 8, N=101: minority game
            minimization of H

1

0

101

100

N

/

2

σ

10−1

10−2

−0.5

0

0.5

1

1.5

2

η

Figure 19. σ2/N as a function of η for S = 2 and α
0.3374 and
0.63 > αc. Results both of numerical simulations of the minority game and of the
α
numerical minimization of Hη are shown. In both cases the replica symmetry breaks
at η = 0 (from [52]).

0.079 < αc ≃

≃

≃

→

Hη: for negative η there is only one minimum which becomes shallower and shallower
as η
0−. At η = 0 the minimum is always unique but it is no more point-like.
Rather it is a connected set. An inﬁnitesimal positive value of η is enough to lift
this degeneracy. The set of minima becomes suddenly disconnected. At ﬁxed α < αc,
varying η across the transition Hη changes continuously – with a discontinuity in its ﬁrst
derivative – whereas the remaining ﬂuctuation terms in σ2/N change discontinuously
with a jump. The potential implications of this result are quite striking: rewarding the

CONTENTS

56

Analytic
N=22
N=20
N=16

0.8

0.7

0.6

0.5

0.3

0.2

0.1

)
α
(
Σ

0.4

0
10−2

10−1

100
α

101

102

Figure 20. Logarithm of the average number of NE divided by N as a function of α
(from [54]).

strategy played more than those which have not been played by a small amount is always
advantageous. In particular, an inﬁnitesimal reward is suﬃcient to reduce ﬂuctuations
by a ﬁnite amount, for α < αc.

−

Let us ﬁnally come to the case η = 1, corresponding to NE, in which, as we said,
steady states coincide with the states of minimum σ2. One understands that these
, since σ2 attains
minima occur when agents play only one of their available strategies
P
1, 1]N . The statistical properties of
minima in the corners of the conﬁguration space [
the minima of σ2 can again be analyzed with tools of statistical mechanics. As is clear
from Fig. 18, for η = 1 one is always in the phase with broken replica symmetry because
σ2 attains its minima on a disconnected set of points. For S = 2 strategies per agent it
has been shown analytically via the so-called annealed approximation that the number
of NE (i.e. of minima of σ2) is exponentially large in N (see Fig. 20). It is clear that the
global eﬃciency of NE is better than in the standard MG, since ﬂuctuations are smaller.
Furthermore, increasing the number S of strategies the eﬃciency of NE increases (i.e.
σ2 decreases) as shown in [52]. This contrasts with what happens in the MG, where the
eﬃciency generally decreases when S increases. Therefore, not only agents in the MG
play sub-optimally, but the more resources they have the larger is the deviation of their
behavior from an optimum.

We are still left with the question: why do agents in the MG play sub-optimally?
In order to answer, let us consider the case of an external agent with S strategies,

There may also be other NE, which correspond to saddle points of σ2 and are hence stationary
P
points of the multi-population replicator dynamics. Agents do not play evolutionarily stable strategies
in these NE and as we shall see the dynamics of learning never converges to these states. Hence we do
not consider these NE further.

CONTENTS

57

(142)

(143)

an agent who does not take part in the game but just observes its outcome from
the outside. From this position, each of his strategies delivers an average virtual gain
πvir
ag h
g are drawn randomly, the
g =
−
πvir
g ’s are independent random variables. Moreover, since πvir
is the sum of P
1
g
independent variables aµ
/P , their distribution is Gaussian with zero mean and
g h
variance

(g = 1, . . . , S). Given that the strategies aµ

A
i

Aµ

≫

i

Var

πvir
g

=

P

1
P 2

Var(aµ
g )

2 =

A
|

µ
i

h

H
P

.

µ=1
X
Clearly, the strategy g⋆ bearing the highest expected proﬁt πvir
It would be most reasonable for this agent to just stick to this strategy.

(cid:0)

(cid:1)

g⋆ is superior to all others.

However, the same agent inside the game will typically use not only strategy g⋆
since every strategy, when used, delivers a real gain which is reduced with respect to
the virtual one by the “market impact”. Imagine the “experiment” of injecting the new
agent in a MG. Then, neglecting the reaction of other agents to the new-comer, one
would have that

g . Then the real gain of the newcomer is:

µ

µ

A
|

h
πreal
g ≃ −

i → h
ag h

A

A
|

i

i − h

+ aµ
ag agi

= πvir

g −

1.

The agent will then update the score of the strategy he uses (say g) with the real gain
πreal
and those of the strategies he does not use (say g′) with the virtual one, so that
g
Ug′ = πreal
+ 1. Therefore agents in the MG over-estimate the
g′
performance of the strategies they do not play. Then if strategy g is played with a
frequency fg, the virtual score increases on average by

agag′ ≃

πreal
g′

+ 1

−

(144)

vg = Ug(t + 1)

Ug(t) = πreal

fg + 1

−

g −
The fact that a good strategy g is used frequently reduces its perceived success+ and
leads agents to mix their best strategy with less performing ones. This is a consequence
of the fact that agents neglect their impact on the market. It is now clear why, given
that the market impact reduces the perceived performance vg of strategies by an amount
which equals the frequency fg with which strategies are played, agents can improve their
performance if they reward the strategy which they have played by some extra points
(the η factor). This contributes a term ηfg to the rate of growth of strategy g so (144)
becomes vg = πreal
η)fg + 1. Any η > 0 reduces the market impact and improves
agent’s performance. In particular for η = 1 agents properly account for the market
impact and indeed in this case the growth rate vs of their strategies do not depend on
the way they play.

g −

(1

−

+ More precisely the frequency fg with which the agent plays strategy g will be such that the rate
of increase of the scores is the same vg = v⋆ for all strategies with fg > 0. Strategies which are not
played (fg = 0) have πreal
g + 1 < v⋆. Considering the reaction of other agents does not modify these
conclusions.

CONTENTS

58

4.8. Exogenous vs endogenous information

In the El Farol problem and in the MG the state µ(t) is determined by the outcome of
past games, as in (32). In other words µ(t) is an endogenous information which encodes
information on the game itself: agents record which has been the winning action in the
last m = log2 P games and store this information in the binary representation of the
integer µ. How do the results which we derived for exogenous information, i.e. when µ
is just randomly drawn at each time, change if we go back to endogenous information?
This issue has been the subject of much debate and considerable analytical and
numerical work was required to settle it. We will limit ourselves here to a sketch of
the line of reasoning and of the results. As we said, it was at ﬁrst believed, based on
computer simulation, that the MGs with exogenous and endogenous information yield
the same macroscopic pictures. However the situation turned out to be more subtle.
In fact, (32) implies that the dynamics of µ(t) depends on the collective behavior of
the game outcome A(t). The key quantity to understand the dynamics of information
patterns is the stationary state distribution of the process µ(t) which is induced by the
dynamics of A(t). As in the El Farol model, this process is a diﬀusion on a De Bruijn
graph, where the transition probabilities depend on the statistics of A(t) conditional on
a particular site µ of the graph. When the dynamics of A(t) has a strong stochastic
component, which occurs when many agents play in a probabilistic fashion (i.e. when
µ′ occur with a positive, ﬁnite probability.
mi|
|
Hence the stationary state distribution has a support on all the states µ
.
}
At odd with the case of exogenous information, some state may be visited more often
than some other state, but all states are visited. This leads ultimately to the same
qualitative scenario as in the completely random case and explains the early numerical
ﬁnding on the irrelevance of the origin of the information in the MG∗. Roughly speaking,
one can say that this scenario holds whenever

< 1), all possible transition µ

sii |
| h

1, . . . , P

∈ {

→

=

1
N

N

i=1
X

m2

i < 1,

Q(f ) =

δ[f

P ρ(µ)]

−

1
P

µ
X

which in suﬃcient to ensure that agents behave in a probabilistic way.

To be more precise, one can analyze the steady-state distribution of history

frequencies ρ(µ) relative to the uniform case, which is given by

(if ρ(µ) = 1/P for all µ, Q(f ) is a delta-distribution at f = 1) as was done e.g. in [55].
This quantity is reported in Fig. 21. One sees that in the supercritical regime the
distribution is indeed not uniform. This explains why, from a quantitative viewpoint it
turns out that macroscopic observables actually depend on the type of information
in the asymmetric regime α > αc where the deviations of the history frequency

∗ Rather than the origin of information, Ref. [48] speaks of irrelevance of memory. The term “memory”
is used in an improper way. Actually the memory of agents is stored into their scores Uig(t).

(145)

(146)

CONTENTS

59

)
f
(

Q

)
f
(

Q

1

0.8

0.6

0.4

0.2

0
0
1

0.8

0.6

0.4

0.2

0
0

1

2

3

4

5

α=0.1

α=2

1

2

3

4

5

f

Figure 21. Relative distribution of frequencies Q(f ) for at α = 0.1 (top) and α = 2
(bottom). Simulations performed with αN 2 = 30000, with averages over 100 disorder
samples per point.

distribution from uniformity are more signiﬁcant. The arguments just described, though
approximate, are able to account for these deviations rather well. Recently, the dynamics
of the MG with endogenous information was solved exactly by the generating functional
method [56], conﬁrming the general picture outlined above.

Clearly, the situation changes drastically when one considers the MG corrected for
the market impact with η = 1. We know that all agents ultimately freeze in this case,
so that the learning dynamics converges to a state with Q = 1, i.e. with no stochastic
ﬂuctuations; therefore in the long run A(t) becomes a function of µ(t) alone. This
means that the dynamics of µ(t) becomes deterministic:
it locks into periodic orbits
of the order of √P values of µ. As a consequence, only a tiny fraction of information
patterns are generated by the dynamics of A(t) and these few on the periodic orbit are
visited uniformly (one after the other). This dynamic reduction of the size information
space from P to a number of order √P implies a similar reduction of the eﬀective value
of α to something close to 0. Given that σ2/N decreases with α, we conclude that the
performance of the system with endogenous information improves with respect to the
case of exogenous information. For intermediate values of η and endogenous information
the system interpolates between the two extreme behaviors of the standard MG (η = 0)
– where the origin of information is to some extent irrelevant – and of the sophisticated
agents (η = 1) case – where a dynamic selection of a small subset of states µ occurs.

5. Extensions and generalizations

We shall discuss now a few variations on the MG theme, mostly inspired by problems
related to ﬁnancial markets, in particular by the origin of the peculiar intermittent and
non-Gaussian (‘fat tailed’) ﬂuctuation patterns they generate. In the reference model

CONTENTS

60

of price dynamics, which is the simplest one accounting for no-arbitrage hypothesis
and market’s eﬃciency, the logarithm of prices performs a random walk and hence
returns are gaussian. On the other hand, several complex agent-based models are able
to reproduce a realistic phenomenology to a high degree but with little analytic control.
In the context of MGs we shall see that heavy tails in the distribution of returns and
clustering in time emerge close to the phase transition, which suggests that markets
operate close to criticality. Realistic behaviour persists also when agents have a ﬁnite
score memory, but it disappears as soon as agents account for their market impact. We
shall also brieﬂy discuss MGs with many assets, in which agents have to choose among
several assets with diﬀerent information content. Then we shall move on to Majority
Games and review the properties of mixed models in which fundamentalists and trend-
followers interact. A discussion of a model with asymmetric (private) information closes
the section.

5.1. Grand-canonical Minority Game and stylized facts

The following model introduces volume ﬂuctuations in the MG, as the number of agents
involved in the game varies from one time step to the next.
In the grand-canonical
MG [57], each agent i has at his disposal only one quenched random trading strategy
ai =
and has to choose whether to join the market (φi(t) = 1) or not (φi(t) = 0) at
every time step. In order to make this decision the agent compares the expected proﬁt
from joining the market to a ﬁxed standard. The model is completely deﬁned by the
following scheme:

aµ
i }

{

φi(t) = θ[Ui(t)]
A(t) =

φi(t)aµ(t)

i

Ui(t + 1)

Ui(t) =

aµ(t)
i A(t)

−

ǫi

−

i
X

−

(147)

The quantity ǫi represents the benchmark: ǫi < 0 means that agents have an incentive
to take part in the market because, for instance, they are urged to sell or exchange
assets; ǫi > 0 implies that agents receive a ﬁxed positive payoﬀ by staying away from
the market, like a ﬁxed interest from a bank. Alternatively, ǫi can be seen as the a
priori incentive of agent i to enter the market:
if ǫi < 0 (resp. ǫi > 0) the agent has
a small incentive to enter (resp. stay out). One can consider two diﬀerent types of
agents: producers, who always enter the market and are characterized by ǫi =
; and
speculators, who instead aim at taking proﬁt of ﬂuctuations and are characterized by a
ﬁnite ǫi. We set

−∞

ǫi = ǫ
ǫi =

for 1

i

≤
≤
for Ns + 1

Ns
i

−∞

≤
where Ns and Np stand for the number of speculators and producers, respectively.
Speculators act on the market only if they expect to receive a payoﬀ higher than the
benchmark; producers act no matter what.

≤

Ns + Np ≡

N

1.5

1.0

0.5

0.0

1

0.5

>

t
c
a
n
<

P

/

2
σ

 
 
 
 
 

P
/
H

0
0.1

CONTENTS

61

100

10−1

)

A

(

>
P

10−2

10−3

10−4

A

0

t

1000

1

ns

1e+01

10−5

100

101

102
A

103

104

Figure 22. Left panel:
relative number of active agents (top), volatility and
predictability per pattern (bottom) as a function of ns for ǫ = 0.1 (open markers)
0.01 (full markers). Right panel: cumuluative probability distribution
and ǫ =
P>(A) = Prob
Inset: time series A(t)
versus t for ns = 20 (top) and ns = 200 (bottom). From [57].

versus x in the steady state.

> x
}

A(t)
|

{|

−

The relevant control parameters are the relative number of speculators and
producers, respectively: ns = Ns/P and np = Np/P . As usual, one is interested in
the behavior of the volatility σ2 and of the predictability H. Besides, it is interesting
to analyze also the relative number of active speculators, deﬁned as

(148)

nact =

1
P

φii

h

i
X

Results are shown in Fig. 22. On sees that with a ﬁxed number np of producers, the
market becomes more and more unpredictable, i.e. H decreases, as the number ns
of speculators increases, independently of the value of ǫ. At the same time also the
volatility σ2 decreases as agents play in an increasingly coordinated way. In a market
with few speculators (ns < 1 in Fig.), most of the ﬂuctuations in A(t) are due to the
random choice of µ(t) (i.e. σ2
H) and the number nact of active speculators grows
approximately linearly with ns. When ns increases further, the market reaches a point
where it is barely predictable. Now the collective behavior becomes ǫ-dependent:

≃

•

•

for ǫ < 0 the relative number of active speculators continues growing with ns even
0. The volatility σ2 has a minimum and then
if the market is unpredictable H
it increases with ns
for ǫ > 0, instead, the relative number of active traders decreases and ﬁnally
converges to a constant. This means that the market becomes highly selective:
only a negligible fraction of speculators trade (φi(t) = 1) whereas the majority is
inactive (φi(t) = 0). The volatility σ2 also remains roughly constant in this limit

≃

In other words, ǫ = 0 for ns ≥
transition across which Nact and σ2 exhibit a discontinuity.

s(np) (n⋆
n⋆

s(1) = 4.15 . . .) is the locus of a ﬁrst order phase

CONTENTS

62

3
−
2
>
2
A
<
>
4
A
<
=
K

/

103

101

10−1

10−3

Γ=1
Γ=10
Γ=οο

101

P

102

Figure 23. Kurtosis of A(t) in simulations with ǫ = 0.01, ns = 70, np = 1 and several
diﬀerent system sizes P for Γ = 1, 10 and

.
∞

So far for collective properties; what about stylized facts? Numerical simulations
reproduce anomalous ﬂuctuations similar to those of real ﬁnancial markets close to the
phase transition line. As shown in Fig. 22, the distribution of A(t) is roughly Gaussian
for small enough ns (it must tend to a Gaussian when ns →
0), and has fatter and fatter
tails as ns increases. The same behavior is seen for decreasing ǫ: fat tails emerge in the
vicinity of the critical point. In particular the distribution of A(t) shows a power law
behavior P (
2.8, 1.4
for ns = 20, 200 respectively and ǫ = 0.01. With ns = 100 the exponent takes values
1.4, 2.3, 3.1 for ǫ = 0.01, 0.1, 0.5. Note that empirical values of β typically range
β
from 2 to 4. Finally: volatility clustering is observed in conjunction with the power-law
tails (see inset).

β with an exponent which can be estimated to be β

> x)

A
|

x−

∼

≃

≃

|

Let us analyze more closely the emergence of power-law tails in the distribution
In Fig. 23 the kurtosis excess (if x is a generic

of A(t) and of volatility clustering.

x4
random variable with zero mean, K is deﬁned as K = h
3; loosely speaking, it
x2
h
is a convenient proxy for the distance of a certain distribution from a Gaussian, for
which K = 0) K of the distribution is shown as a function of the system size and of the
learning rate Γ for a ‘regularized’ model with choice rule

i
2 −
i

Prob
{

}

φi(t) = 1

= 1/[1 + e−

ΓUi(t)]

(149)

One sees that as the system size increases (or if one introduces a small enough learning
rate Γ,see below) the distribution tends to a Gaussian as K decreases with P . Moreover
we see that for a rage of parameters the appearance of fat tails is sample-dependent, as
both samples with and without fat tails may occur.

This behaviour is reminiscent of well-known ﬁnite-size eﬀects in the theory of critical
phenomena: in the d-dimensional Ising model, for example, at temperature T = Tc + γ
in the magnetization) occur as long as the system size N
critical ﬂuctuations (e.g.
dν the system shows
is smaller than the correlation volume

dν. But for N

γ−

γ−

∼

≫

CONTENTS

63

(150)

(151)

(152)

the normal ﬂuctuations of a paramagnet. Some light on the ﬁnite-size eﬀects in our
case can be shed by studying the continuous-time limit of the score updating dynamics.
Regularizing the choice rule to

prob
{

}

φi(t) = 1

= 1/[1 + e−

ΓUi(t)]

4.5, one can
with learning rate Γ, and applying the machinery described in Sec.
transform the discrete-time learning dynamics into the continuous-time Langevin
process

A

˙Ui(t) =

ai h
−
ηi(t)ηj(t′)
=

iy −
σ2
N

ǫ + ηi(t)

aiajδ(t

t′)

i

h

−
Notice that the noise strength is proportional to the time dependent volatility σ2 =
.
h
i
1/√N is
The noise term is a source of correlated ﬂuctuations because aiaj h
small but non zero, for i
= j if N is ﬁnite. This noise competes with the deterministic
part of (151): if the former outweighs the latter, then one expects that the dynamics
will sustain collective correlated ﬂuctuations in the Ui(t) which otherwise would be
washed away.
In order to obtain an approximate analytic condition for the onset of
volatility clustering one may then compare the noise correlation term, which is of order
σ2/P 3/2 for i
= j, with the square of the deterministic term of (151),
iy/N
aiaj h
2
iy + ǫ
ai h
A
which is given by
volatility clustering can be expected to set in when
h
i

. Rearranging terms, one ﬁnds that

H/P + ǫ

/N

A2

A2

A2

∼

≃

≃

i

i

2

hp

P

r

H
P

(153)

σ2 ≃

H
σ2 + 2ǫ

σ2 + ǫ2 P

B
√P
where B is a constant. This prediction ﬁnds remarkable conﬁrmations in numerical
experiments [57]. Recalling the analogy with magnetic systems made at the beginning
ǫ2 imply that the same occurs
of this section, one understands that (153) and H/P
1/4 when
in the GCMG with dν = 4. In other words, the critical window shrinks as N −
N
. However, because of the long range nature of the interaction, anomalous
ﬂuctuations either concern the whole system or do not aﬀect it at all. In the critical
region the Gaussian phase coexists probabilistically with a phase characterized by
anomalous ﬂuctuations. This, like the discontinuous nature of the transition at ǫ = 0,
is typical of ﬁrst order phase transitions.

→ ∞

∼

5.2. Market ecology

One of the ﬁrst modiﬁcation of the MG has investigated the eﬀects of introducing an
explicit asymmetry in the two possible actions [58]. This is the case of the El Farol bar
problem: the actions ‘go’ or ‘don’t go’ to the bar are not symmetric because (i) if one
takes the wrong action there is still a diﬀerence between going to a crowded bar and not
going to an uncrowded bar and (ii) the comfort level corresponds to a share of 60% of
agents attending. If each agent takes the opposite choice one ends up in an ineﬃcient

6
6
CONTENTS

64

(154)

attendance of 40%. The outcomes of the MG are instead symmetric: If every agent
switches to the opposite choice, all the payoﬀs remain unchanged. Quite generally this
leads to study games where the payoﬀs to agent i at time t is given by

πi(t) =

aµ(t)
igi(t)

−

0 +

Aµ(t)
"

aµ(t)
jgj(t)

#

j
X

Aµ
0 }

{

where A0 =
is some ﬁxed vector. In particular, [58] investigated the case where
Aµ
0 = L independently of µ, as in the El Farol bar, and where information is endogenous.
Interestingly, because of the fact that due to (32) some values of µ occur more often
than others, the conclusion that the collective behavior is independent of whether the
information µ is endogenously generated or is exogenous (i.e.
random), which was
roughly correct for the standard MG, is not true in this case.

There is however a second motivation for considering a model based on (154) which
was explored in [59,60]. Considering the MG as a model of a ﬁnancial market, it can be
argued that there are diﬀerent types of market participants with diﬀerent goals. Some
trade to gain money from transactions with no particular interest in the asset they buy
and sell. Only price ﬂuctuations matter for this kind of traders, which one usually calls
‘speculators’. Another type of market participants are those who use the market for
exchanging goods. This is indeed the reason why markets exist. This type of agents
is interested in the asset itself: they will buy it or sell it irrespective of the history of
recent ﬂuctuations: this type of agents can be called producers. While speculators have
a range of behavioral rules which process the available information in search of arbitrage
opportunities, producers use a trading rule which is constant in time. Producers are
part of the ﬁnancial world and their behavior is correlated with the state of the world µ
which is thought to capture all relevant economic information: in other words, they only
have one strategy at their disposal. This type of traders play a role similar to that of
hedgers♯: they inject information into the market. Their trading activity is completely
predictable given the state of the world µ and the term Aµ
0 represents their aggregate
contribution to the market.

in fact, Aµ

It is easy to understand that in a market composed of producers only the
0 can be regarded
distribution of price changes would be nearly Gaussian:
as the sum of Np random terms, where Np is the number of producers. The process
associated to producers can be considered as the fundamentals, i.e. the price process
which reﬂects the economic performance of the asset. Roughly speaking, one may expect
that speculative trading will color this process and transform its statistical properties.
Actually the discussion may be extended to a further type of agents, the so-called noise
traders. These persons totally disregard the state of the world µ or have no information
at all on it. They rather follow rules of behavior which are statistically uncorrelated
with µ (such as the moon phases) and with the behavior of other agents. The presence
of these agents does not introduce any new qualitative features. The question is: how
do all these “species” of traders interact?

♯ A hedge is an action (e.g. buy/sell) done with the aim of reducing the risk of another action.

CONTENTS

65

101

100

10−1

P
/
p

N

2

0

10−1

100

N/P

101

H
Gain Spec
Gain Prod.

−2

0

1

2
N/P

3

Figure 24. Average gains of producers and speculators as a funcion of the (reduced)
number N/P of adaptive agents (speculators). The plot refers to a system with Np = P
passive agents (producers). The gain of speculators is positive only when they are few
and it decreases when new speculatos join the market. Producers losses are reduced by
speculators. The predicatbility H is also plotted. Inset: Phase diagram in the space
of the reduced numbers of speculators and producers. The shaded region to the right
of the solid line is the symmetric phase where H = 0. The gain of speculators vanishes
on dashed line and it is positive in the region to the left.

An intuitive argument runs more or less as follows. First, note that in a market
composed of producers price changes would depend only on µ. Such a highly predictable
market is very favorable for speculators who may derive considerable gains. However
when more and more speculators join the market, its predictability decreases and the
proﬁt of speculators gets more and more meager. This eﬀect is illustrated in Fig. 24,
which also shows that producers instead beneﬁt from the presence of speculators because
their losses are reduced. When the number of speculators increases beyond a critical
value, which depends on the relative number Np/P of producers, the market enters the
symmetric phase where H = 0 and the outcome A(t) becomes unpredictable from µ.
This shows that the relation between these two species is more similar to symbiosis than
to competition: producers feed speculators by injecting information in the market and
beneﬁt, in their turn, of the liquidity provided by speculators.

5.3. Multi-asset Minority Games

5.3.1. Deﬁnitions and results Minority Games with many assets have been introduced
in order to investigate how speculative trading aﬀects the diﬀerent assets in a market
[61, 62]. A tractable version of these models has been considered in [63], with the aim
of studying how agents modify the composition of their portfolios depending on the
‘complexities’ or information contents of the diﬀerent assets.

The model consists essentially of two coupled MGs with one strategy each. Let us

CONTENTS

66

1, 1
}
1, . . . , Pγ}

and N agents. At each time
consider the case of a market with two assets γ
∈ {−
step ℓ, agents receive two information patterns µγ ∈ {
, chosen at random and
independently with uniform probability. As always, Pγ is taken to scale linearly with N,
and their ratio is denoted by αγ = Pγ/N. Every agent i disposes of one trading strategy
per asset, aiγ =
(buy/sell) for each
iγ ∈ {−
possible information pattern of asset γ. Each component aµγ
iγ is selected randomly and
independently with uniform probability and is kept ﬁxed throughout the game. Traders
keep tracks of their performance in the diﬀerent markets through a score function Uiγ(ℓ).
The behavior of agents is summarized by the following rules:

, that prescribe an action aµγ

aµγ
iγ }

1, 1

{

}

si(t) = sign[yi(t)]

Aγ(t) =

aµγ (t)
jγ

δsj (t),γ

N

j=1
X

−

Uiγ(t + 1)

Uiγ(t) =

aµγ (t)
iγ Aγ(t)/√N

−

where Aγ(t) represents the ‘excess demand’ or the total bid of asset γ, while yi(t) =
γ γUiγ(t). The Ising variable si indicates the asset in which player i invests at time t,
which is simply the one with the largest cumulated score. As usual, it is the minus sign
P
on the right-hand side of (155) that enforces the minority-wins rule in both markets.
It is possible to characterize the asymptotic behaviour of the multi-agent system (155)
with a few macroscopic observables. In the present case, besides traditional observables
such as the predictability H and the volatility σ2, deﬁned respectively as

Aγ|

µγi

h

2 = H+ + H

−

H =

σ2 =

}

1,1
Xγ
∈{−
1
N

1
NPγ

A2
γ

Pγ

µγ =1
X
= σ2

γ
X

(cid:10)

(cid:11)

+ + σ2
−

m =

1
N

N

i=1
X

sii

h

it is important to analyze the relative propensity of traders to invest in a given market,
namely

−

A positive (resp. negative) m indicates that agents invest preferentially in asset +1
(resp.

1).

The phase structure of the model is displayed in Fig. 25. The (α+, α

) plane is
divided in two regions separated by a critical line. In the ergodic regime, the system
produces exploitable information, i.e. H > 0, and the dynamics is ergodic, that is
the steady state turns out to be independent of the initialization Uiγ(0) of (155).
Below the critical line, instead, diﬀerent initial conditions lead to steady states with
diﬀerent macroscopic properties (e.g. diﬀerent volatility), but traders manage to wash
out the information and the system is unpredictable (H = 0). This scenario essentially
reproduces the standard MG phase transition picture.

−

(155)

(156)

(157)

(158)

67

+ α
α
+

 = 0.5
−

CONTENTS

−

α

0.5

0.4

0.3

0.2

0.1

0
0

Ergodic

H>0

H=0
Non-ergodic

m

0

0.6

0.4

0.2

-0.2

-0.4

0.035
0.03
0.025
0.02
0.015
0.01
0.005
0

0.25

0.2

0.15

0.1

0.05

0

H

2

σ

0.1

0.2

0.3

0.4

0.5

α
+

-0.4

-0.2

0.2

0.4

α
+

0
−α
−

Figure 25. Left panel: analytical phase diagram of the canonical two-asset Minority
Game in the (α+, α−) plane. Right panel: behavior of m (top), H (middle) and σ2
(bottom) versus α+
α− for α+ + α− = 0.5. Markers correspond to simulations with
N = 256 agents, averaged over 200 disorder samples per point. Lines are analytical
results (from [63]).

−

The behaviour of the macroscopic observables m, H and σ2 along the cut α+ +α
=
−
1/2 (in the ergodic phase) is also reported in Fig. 25. One sees that agents play
preferentially in the market with smaller information complexity, which is particularly
inconvenient as it coincides with the one with less exploitable information. This is
a somewhat paradoxical result since a na¨ıve argument would suggest that agents are
attracted by information rich markets. It actually turns out that this simple argument
is incorrect and the observed behavior is due to the fact that agents are constrained to
trade in one of the two markets. Rather than seeking the most proﬁtable asset, agents
simply escape the asset where their loss is largest. The conclusion is indeed reversed
when traders may stay out of the market and have negative incentives to trade (that
is, when they have an incentive not to trade).
In this case, which corresponds to a
grand-canonical multi-asset MG, the information-rich asset is chosen preferentially [63],
though the phase structure becomes more complex than usual as new phases (with
broken ergodicity and global predictability) arise. Note however that in this framework
no correlations among the assets emerge, i.e.

= 0. Indeed

AγA
h

γi

−

A+A

=

−i

h

i,j (cid:28)
X

aµ+
i+ aµ
−j
−

1

1 + si
2

−
2

sj

(cid:29)

(159)

Now, the dynamical variables Uiγ(t) evolve on timescales much longer (of order Pγ)
than those over which the µγ evolve. Hence we can safely assume that the distribution
of si is independent of µγ and factorize the average
over the
independent information arrival processes µ
0 the conclusion
A+A
0 follows immediately. The reason for this is that traders’ behavior is aimed

i,+aµ
aµ+
−j,
−i
(t). Given that

aµ+
i,+ih
≃

=
aµ
±i,
±

aµ
−j,
−i

h

h

±

(cid:10)

(cid:11)

h

−i ≃

CONTENTS

68

at detecting excess returns in the market with no consideration about the correlation
among assets. This conclusion is against the empirical evidence, as in real ﬁnancial
markets correlation between stocks are overwhelmingly positive (if it wasn’t so, making
money in a ﬁnancial market would be much easier!). The microscopic origin of this
phenomenon is a rather diﬃcult issue, which will surely receive much attention in the
near future.

Below we describe the dynamical solution of this model, as an example of the

application of the path-integral formalism to this type of problems.

5.3.2. Dynamics (path-integral approach) The dynamical approach to the stationary
macroscopic properties of Minority Games is based on the use of dynamical generating
functionals `a la Martin-Siggia-Rose [64] to turn the original multi-agent process into
a single stochastic equation for the behavior of a single ‘eﬀective agent’, similarly to
what is done to study the dynamics of spin systems with quenched disorder after [65].
This procedure, which was ﬁrst applied to Minority Games in [66], allows ultimately
to derive closed equations for correlation functions, response functions, and all other
relevant time-dependent macroscopic parameters. Typically, the resulting equations are
too complicated to be solved at all times. However, with suitable Ans¨atze one may
restrict the analysis to speciﬁc solvable regimes (in this case, we shall focus on ergodic
steady states). Dynamical phase transitions can then be identiﬁed from the breakdown
of the assumed behavior. The method is very general, it doesn’t rely on the existence of
a Hamiltonian nor on the validity of detailed balance, but requires an analytical tour de
force for solving the most general MGs. Luckily, some reasonable starting simpliﬁcation
help to make it less cumbersome. One is Markovianness, which in MGs corresponds to
models with random external information. Another is changing the updating rule from
the usual ‘on-line’ learning, in which agents modify their preferences at each time step,
to a ‘batch’ learning, in which agents update their preferences only after they have seen
all possible information patterns Strictly speaking, the batch process is not equivalent
to the on-line process but in many cases, including that which we consider here, the
two are qualitatively identical. Both simpliﬁcations will be made in this section, where
we expound the dynamical solution of the canonical multi-asset MG. The method is
described in detail for other models and more general cases in [43].

So we consider two coupled GCMGs,

interpreted as a system with two assets
characterized by diﬀerent sizes of information sets and, on the agents’ side, by diﬀerent
strategies and valuation functions. From (155), one sees that the preferences evolve
according to

yi(t + 1)

yi(t) =

−

−

γaµγ (t)

iγ Aγ(t)/√N

1,1
Xγ
∈{−

}

(160)

The ‘batch’ approximation is obtained by averaging the right-hand side over the µσ’s.
This leads, after a time re-scaling (for simplicity, we denote the re-scaled time again by

jγ are quenched random couplings of

CONTENTS

t), to

N

nγ

J γ
ijφjγ(t)

yi(t + 1)

yi(t) =

−

−

µγ aµγ
where nγ = 1/αγ and J γ
Hebbian type. We also introduced the variable

1,1
Xγ
∈{−
ij = (1/N)

}

j=1
X
iγ aµγ

φiγ(t) = γδsi(t),γ =

P
[γ + si(t)]

1
2
and cij(t, t′) =

si(t)
h

All moments like mi(t) =
– the brackets standing for
an average over all possible time evolutions of the system – and in turn macroscopic
quantities like the magnetization m =
or the autocorrelation function
C(t, t′) =
i cii(t, t′)/N
ei
Z[ψ] =

ii
can be derived formally from the generating functional

i mi(t)/N

si(t)sj(t′)

t ψ(t)
·

P

s(t)

hh

hh

ii

h

i

i

P

by taking suitable derivatives with respect to the auxiliary generating ﬁelds ψ =
for instance

(cid:11)

(cid:10)

C(t, t′) =

Z[ψ]

∂2
ii
∂ψi(t)∂ψi(t′)

hh

lim
0
ψ
→

i
X

The

average is performed by imposing that the si satisfy (161) at each time step:

h· · ·i

Z[ψ]

=

ii

hh

p[y(0)] ei

t ψ(t)
·

s(t)

W [y(t)

y(t + 1)]

dy(t)

(165)

→

++

t
Y
with transition matrix ﬁxed by (161):

Z

P

**

W [y(t)

y(t + 1)] =

δ

yi(t + 1)

yi(t)

hi(t) +

−

−

→

N

nγ

J γ
ijφjγ(t)

(166)

1,1
Xγ
∈{−

}

j=1
X





The ﬁelds hi(t) will be used to generate response functions. At this point the following
steps need to be taken:

a. Introduce the order parameters

P

i
N

−

i
Y





69

(161)

(162)

(163)
ψi}

{

;

(164)

(167)

Q(t, t′) =

si(t)si(t′)

L(t, t′) =

yi(t)

yi(t′)

K(t, t′) =

b
b
yi(t′)
si(t)

i=1
X

si(t)

b

a(t) =

N

i=1
X
N

i=1
X
N

1
N

1
N

1
N

N

i=1
X
N

i=1
X

b

1
N

1
N

k(t) =

yi(t)

CONTENTS

in (165) via such identities as

1 =

dQ(t, t′)δ

NQ(t, t′)

si(t)si(t′)

;

(168)

Z

"

#

N

−

i=1
X

b. Use the integral representation for the δ-distributions;

c. Average over the quenched disorder after isolating the relevant terms with the help

of the variables

xµγ
γ (t) =

wµγ

γ (t) =

1
Pγ
1
p
Pγ

φiγ(t)aµγ

iγ

yi(t)aµγ

iγ

i
X

i
X

These steps require standard manipulations at most. After a factorization over i and
µγ, one arrives at

p

b

Z[ψ]

=

DΘD

Θ eN[Ψ(Θ,

Θ)+Ω(

Θ)+Φ(Θ)]

Z

ii

b
Q(t, t′), L(t, t′), K(t, t′), a(t), k(t)
b

hh
where Θ(t, t′) =
Θ(t, t′) =
multipliers, while the functions Ψ, Φ and Ω are given by
b
Ψ = i

b
a(t) + ℓ(t)
a(t)

{
Q(t, t′),

b
ℓ(t)+

K(t, t′),

L(t, t′),

a(t),

k(t)

b

b

b

{

}

}

b

is the vector of order parameters,
is the conjugate vector of Lagrange

b
+i

b
Q(t, t′)

Q(t, t′) + L(t, t′)

L(t, t′) + K(t, t′)

K(t, t′)

(172)

Ω =

log

y(t)dy(t)p[y(0)] e−

Xt,t′ h
d

i

b
a(t)s(t)+

t[

ℓ(t)

y(t)]

b

i

y(t)[y(t+1)

y(t)

hi (t)]

t

Q(t,t′)s(t)s(t′)+

y(t)

y(t′)+

K(t,t′)s(t)

y(t′)]

P

b

i

−

t,t′[

P

b

b

b
L(t,t′)

b

b

b

b

b

i

b

−

t h
X

1
N

i
X
i ψi(t)s(t)+i

t

Z Y

ei

P

×
Φ =

γ n
X
+αγ log

Z

where

−

b
nγDγk

k

αγ
P
2

b
log

−

d

we−

nγ
2

t,t′

P

b

L(t,t′)

wγ (t)

wγ (t′)

1
2

−

t,t′[AT

γ (nγ Dγ )−

1Aγ](t,t′)

wγ (t)

wγ (t′)

b

b

P

b

b

o

1
4

Dγ(t, t′) =

[1 + γa(t) + γa(t′) + Q(t, t′)]

Aγ(t, t′) = δtt′ −

inγ
2

[γk(t′) + K(t, t′)]

In the limit N

the integral (171) is dominated by the saddle-point where the

order parameters take the values

→ ∞

70

(169)

(170)

(171)

(173)

(174)

(175)

(176)

(177)

C(t, t′) =

s(t)s(t′)

K(t, t′) =

s(t)

i⋆
y(t′)
i⋆

k(t) =

i⋆

b

h

h
y(t)

h

b

a(t) =

L(t, t′) =

y(t)
h
s(t)
i⋆
h
b
C(t, t′) = i

y(t′)

i⋆

b
∂Φ
∂C(t, t′)

b

CONTENTS

∂Φ
∂L(t, t′)

L(t, t′) = i

b
a(t) = i

∂Φ
∂a(t)

K(t, t′) = i

∂Φ
∂K(t, t′)

b
k(t) = i

∂Φ
∂k(t)

where

b
h· · ·i⋆ =

,
{
y(t)
b
denotes an average performed with the measure

{
y(t)

1
N

i R

X

M(

M(

· · ·

{

y(t)
}
)

}

)

t dy(t)d

y(t)

t dy(t)d
Q

y(t)
b

R

t

y(t)[y(t+1)

y(t)

−

Q

b
hi (t)]
−

i

−

b
ℓ(t)
a(t)s(t)+

t[

y(t)]

b
y(t)
}
,
{

}

C(t,t′)s(t)s(t′)+

b

L(t,t′)

y(t)

y(t′)+

P

y(t′)]
K(t,t′)s(t)
b

b

b

M(

y(t)

,

y(t)

{

}

{

) = p[y(0)] ei
t,t′[

i

}
e−

P

×

b

P

b

Now comparing the above averages with the derivatives of

Z
with respect to
ii
ψ and h one easily sees that, in the limit N
, Q(t, t′) may be identiﬁed with the
autocorrelation function C(t, t′), a(t) turns out to coincide with the magnetization m(t),
whereas K(t, t′) may be related to the response function

→ ∞

hh

b

b

b

b

b

through K(t, t′) = iG(t, t′). Working out the remaining equations, and in particular the
expression of Φ, one ﬁnds in addition that

G(t, t′) = lim
→∞

N

∂

si(t)
hhh
∂hi(t′)

iii

1
N

i
X

C =
1
2

b
γ
X

a = 0
1

A−
γ
b

L = k =

T

K

=

−

i
2

−

c
L =

γ
X

b
1
γ (nγDγ)A−
γ

A−

1

k =

1
2

−

1

γA−
γ

γ
X

b

(cid:3)
Therefore M can be seen as describing the single-agent process with noise z(t) given by
nγ
2

(t, t′)φγ(t′) + z(t)

y(t + 1)

y(t) =

(182)

1 +

G

(cid:2)

−

1

−

−

z(t)z(t′)

=

i

h

i

Xγ,t′ h
nγ
1 +
2

1

−

G

(cid:17)

γ (cid:20)(cid:16)
X

(nγDγ)

1 +

(t, t′)

(183)

1

−

nγ
2

G

(cid:21)

(cid:17)

(cid:16)

which is completely equivalent to the original multi-agent system in the limit N

.
→ ∞
Let us now focus on the asymptotic properties of the stationary state, considering
the simplest possibility. Making for the asymptotic behavior of C and G the
assumptions of time-translation invariance,

71

(178)

(179)

(180)

(181)

(184)

(185)

(186)

ﬁnite susceptibility,

lim
t
→∞
lim
t
→∞

lim
t
→∞

C(t + τ, t) = C(τ )

G(t + τ, t) = G(τ )

G(t, t′) <

∞

t
Xt′≤

ergodic stationary states of the dynamics can be fully characterized in terms of a few
parameters. These are, in particular, the persistent autocorrelation

CONTENTS

and weak long-term memory,

lim
t
→∞

G(t, t′) = 0

t′ ﬁnite

∀

and the susceptibility (or integrated response)

the magnetization

c = lim
τ
→∞

C(t)

t<τ
X

m = lim
t
→∞

m(t′)

Xt′

χ = lim
τ
→∞

G(t)

1
τ

1
t

τ
t
X
≤

y(t)
t

y = lim
t
→∞

In this regime, the quantities

are easily seen to be related by
e
s + γ
2

y =

κγ

−

+ z

where

e

κγ =

γ
X

2
2 + nγχ

z2

=

(cid:10)

(cid:11)

γ
X

αγ (1 + 2γm + c)
(2αγ + χ)2

We have the following scenarios:

s = lim
t
→∞

1
t

s(t′)

Xt′

z = lim
t
→∞

1
t

Xt′

z(t′)

(191)

(i) if

(ii) if

(iii) if

y > 0, then s = 1 (the agent is frozen on asset 1): this occurs if z > κ+
1): this occurs if z <
y < 0, then s =
−
e
y = 0, then s = s⋆
e

(the agent is ﬁckle): this occurs if

1 (the agent is frozen on asset

γ κγ
P

γ γκγ

≡

−

−

κ

2z

−

−

κ

−

−
< z < κ+

Separating the contribuctions of diﬀerent cases we end up with the following equations
e
for m, c and χ:

P

m =

c =

−

θ(z
h
θ(z
−
αγχ
2αγ + χ

h

κ+)
κ+)

=

iz +
iz +
θ(z + κ
(cid:10)
h

s⋆θ(z + κ
)θ(κ+ −
z)
h
−
(s⋆)2θ(z + κ
)θ(κ+ −
−
z)
)θ(κ+ −
iz

−

θ(
iz − h
z)
z +
(cid:11)

h

κ

z)

−
θ(

− −
κ

−

− −

iz
z)

iz

γ
X
h· · ·iz is an average over the static Gaussian noise z. The Gaussian integrals
where
can be easily computed and these equations can be solved numerically for c, m and χ.
) implies κ+ < κ
Notice that n+ > n
so that the probability that an

(or α+ < α

−

−

−

72

(187)

(188)

(189)

(190)

(192)

(193)

(194)

(195)

CONTENTS

agents ‘freezes’ on asset γ is larger for γ = +1, i.e. for the asset with less information.
This conclusion is immediately clear from the above equations. A little more work is
required to see that H is given (apart from factors αγ) by the persistent part of the
noise variance (183):

α2

γ (1 + 2γm + c)
(2αγ + χ)2

H =

γ
X

These expressions ﬁnally yield the analytical curves shown in Fig. 25.

5.4. The Majority Game

The simplest way to get a glimpse on the macroscopic properties of the Majority Game
is to consider the simpliﬁed information-free context of Sec. 4.3, where the model is
described by the rules

bi(t) = b
}

Prob
{
∆i(t + 1)

−

= C exp [b∆i(t)]

∆i(t) = ΓA(t)/N

by which agents reward the action taken by the majority and increase the probability
of choosing bi(t + 1) = sign [A(t)]. An analysis similar to that outlined in the case of the
Minority Game easily leads to the conclusion that the dynamics of y(t) = ∆i(t)
∆i(0)
−
(which is i-independent) admits the solution y(t) = y0 + vt where v =
Γ. In this state,
agents behave coherently (bi(t) = b for all i). Consequently,
N and
σ2 = O(N 2) independently of Γ.

±
is either N or

A
i

−

h

The above conclusion that Majority Games generate huge ﬂuctuations is rather
intuitive. However the full Majority Game turns out to be a surprisingly rich model [67].
It is deﬁned by the following setup:

gi(t) = arg max Uig(t)
A(t) =

aµ(t)
igi(t)

i
X
Uig(t + 1)

Uig(t) = aµ(t)
ig

aµ
igi(t) −
−
(cid:16)
where µ(t)
stands for the information pattern presented to agents at time
t (taken to be external and random) and η tunes the agents’ ability to learn to respond
to the action of all other agents by disentangling their own contribution to the game’s
outcome.

1, . . . , P

A(t)

aµ
ig

∈ {

(cid:17)i

−

}

η

h

Using the notation introduced in Sec. 4.5, it is easy to see that

where mi =

vi ≡ h

yi(t + 1)

yi(t)

= ξiΩ +

−

i

ξiξjmj −

ηξ2

i mi

j
X

sign(yi)
h

i

Hη =

. Hence the dynamics minimizes the function
1
2

ξiξjmimj −

ξiΩmi +

i m2
ξ2

η
2

i

−

i,j
X

i
X

i
X

73

(196)

(197)

(198)

(199)

(200)

(201)

CONTENTS

74

(202)

−

Ω2/2 to complete a square with the ﬁrst to terms above, one
Adding the constant
sees that Hη is a downward concave function of the mi’s, which implies that minima
1, 1]N . Thus the solution with vi = 0
occur on the corners of the deﬁnition domain [
corresponding to ﬁckle agents is ruled out in this case and the only remaining solutions
are those with vi 6
), corresponding to frozen agents. For
these,

= 0 (and yi(t)/t ﬁnite as t

→ ∞

−

mi = sign(vi) = sign

ξiΩ +

ξiξjmj −

ηξ2

i mi

!

 

j
X

±

Notice that since the relevant steady states have mi =
1 the last term in Hη plays the
role of a mere constant. Hence impact factors do not alter the steady state properties of
the Majority Game. (Also due to agents’ freezing, the ‘batch’ and ‘on-line’ version yield
the same stationary properties as ﬂuctuations play no role in this case.) Furthermore,
which is a solution of these equations for some
it is clear that any conﬁguration
value of η
Sη of stationary
S1 ⊂ Sη for all η < 1. It is
states is such that
also easy to see that the state with minimal value of Hη lies in
[0, 1]. This
∈
shows that Nash equilibria are stationary states of the majority game for all values of
η, but the converse is not true (except for η = 1 of course).

Sη ⊂ Sη′ for η′ < η and, in particular,

[0, 1] will also be a solution for all η′ < η. Hence the set

S1 for all η

mi}

∈

{

It is possible to draw a complete picture of the model’s behavior by studying the
minima of Hη explicitly via the replica method. The calculation has been carried out
in [67] under the assumption that the two strategies of the same agent can be to some
degree correlated, which is allowed if one takes the disorder distribution

w
2

1

w

−
2

P (a1, a2) =

(δa1,1δa2,1 + δa1,

1δa2,

1) +

−

−

(δa1,1δa2,

1 + δa1,

1δa2,1)

−

−

(203)

aµ
i1 = aµ

{

i2}

Notice that w = Prob
. It turns out that, depending on the parameters, the
system can be in one of two phases: a ‘retrieval’ phase characterised by attractors with a
macroscopic overlap A1 = O(N) with a given pattern (say, µ = 1) and a spin glass phase
with no retrieval (Aµ = O(√N )). The occurrence of ‘retrieval’ may be thought of as the
emergence of crowd eﬀects such as fashions and trends, when a large fraction of agents
behave similarly in some respect, or to economic concentration, when, for example, one
particular place is arbitrarily selected for large scale investments.
Interestingly, one
ﬁnds that the development of these crowd eﬀects requires: (i) that the number of agents
is large compared to the number of resources (α small), (ii) a suﬃcient diﬀerentiation
between strategies of agents (w < 2/3) and (iii) a large enough initial bias (i.e. an
initial macroscopic overlap) towards a particular resource, fashion or place. Finally
in the spin glass
crowd eﬀects can be sustained under more general conditions (i.e.
phase) if agents do not behave strategically, i.e.
if they neglect their impact on the
aggregate (η small). This phenomenon can be attributed to the self-reinforcing term
i si in the dynamics which causes a dramatic increase in the number of stationary
(1
states as η decreases (which can be seen quantitatively by analyzing the entropy).

η)ξ2

−

CONTENTS

75

5.5. Models with interacting trend-followers and contrarians

that is
It is rather easy to understand that the two main groups of traders,
fundamentalists and trend-followers, contribute opposite forces to the price dynamics.
Fundamentalists believe that the market is close to a stationary state and buy (sell) when
they repute the stock to be underpriced (overpriced), thus inducing anti-correlation in
market returns and holding the price close to its ‘fundamental’ value. Trend-followers,
instead, extrapolate trends from recent price increments and buy or sell assuming that
the next increment will occur in the direction of the trend, thus creating positive return
correlations and large price drifts (‘bubbles’). Chartist behavior, which can also be
driven by imitation, is known to cause market instability. Fundamentalists act instead
as a restoring force that dumps market ineﬃciencies and excess volatility. The next
question we address concerns the macroscopic properties of models in which contrarians
and trend-followers interact.

As usual, we start from the simple model with no information. Let us assume
f )N are

that a fraction f of agents are trend followers whereas the remaining (1
fundamentalists. The dynamics is governed by the following scheme:

−

(204)

(205)

(206)

bi(t) = b
}

Prob
{
∆i(t + 1)

= C exp [b∆i(t)]

∆i(t) = ǫiΓA(t)/N

−
where ǫi = 1 for trend-followers (say for i
1 for
}
fundamentalists (say i
). Assuming that ∆i(0) = 0 for simplicity,
}
we can approximate A(t)/N with its average and see that the dynamics of y(t) =
∆i(t)

∆i(t) is given by

f N + 1, . . . , N

) and ǫi =

1, . . . , f N

∆i(0)

∈ {

∈ {

−

−

≡
y(t + 1)

−

y(t) = (2f

1)Γ tanh[y(t)]

−

Linear stability analysis of (206) leads to the following scenario. For f < 1/2 we have
two regimes:

•

•

1

2f the ﬁxed point y⋆ = 0 is stable. One has

for Γ < 1
A
i
−
in the information-free Minority Game with subcritical Γ
for Γ > 1
2f the ﬁxed point y⋆ = 0 is unstable. One has
−
as in the information-free Minority Game with supercritical Γ

A
i
h

h

1

= 0 and σ2 = O(N) as

= 0 and σ2 = O(N 2)

±

−

For f > 1/2 instead the ﬁxed point y⋆ = 0 is unstable and the solution y(t) = y0+vt with
v =
(2f
1)Γ appears. Here, both trend-followers and contrarians behave coherently:
b for all i
bi(t) = b for all i
and bi(t) =
. As a result,
}
1)N and σ2 = O(N 2) as in the information-free Majority
is either (2f
A
h
−
i
Game. The conclusion we draw is that the expectations of the majority group (be it
fundamentalists or trend-followers) are fulﬁlled in the steady state. This is conﬁrmed
by studying the autocorrelation of returns as a function of f in the steady state, see
Fig. 26.

1, . . . , f N
1)N or (2f

f N + 1, . . . , N

∈ {
−

∈ {

−

}

This conclusion extends to the full model, whose properties have been analyzed

in [68]. The mixed Majority-Minority Game is deﬁned by

gi(t) = arg max Uig(t)

CONTENTS

76

0.5

1

0

2

/

N
>
)
t
(

A
)
1
+
t
(

A
<

5

0

3
0
1
 
x
 
m
o
o
Z

−5

0.5

0.7

f

−0.5

0

0.25

0.75

1

0.5
f

Figure 26. (from Ref. [44]) Autocorrelation of returns as a function of the fraction f
of fundamentalists in the market. Autocorrelation is taken in the stationary state of
a system of N = 104 agents with Γ = 2.5. Arrows mark the transitions between the
three regimes described in the text, which occur at f = 0.5 and at f = 0.9. The inset
shows a detail of the central part of the graph.

(207)

(208)

(209)

aµ(t)
igi(t)

A(t) =

i
X
Uig(t + 1)

Uig(t) = ǫiaµ(t)

ig A(t)/N

−
where as before ǫi = 1 for trend-followers (or i
1 for
fundamentalists (or i
). The statistical mechanics of this model
is slightly more involved than previous cases. As before, one ﬁnds that the steady
state can be characterized in terms of the microscopic variables mi =
where
yi(t) = 1
In particular, the stationary mi’s for can be obtained by
solving the following problem:

f N + 1, . . . , N

sign(yi)
h

) and ǫi =

1, . . . , f N

2[Ui1(t)

Ui2(t)].

∈ {

∈ {

−

−

}

}

i

max
m2

min
m1

H(m1, m2)

where

H(m1, m2) =

Ωµ +

ξµ
i mi

1
P

µ "
X

2

#

i
X
and m1 (resp. m2) denote collectively the mi variables of Minority (resp. Majority)
game players. Hence the mixed game where both minority and majority players
are present at the same time requires a minimization of the predictability in certain
directions (the minority ones) and a maximization in others (the majority ones). It is
possible to tackle this type of problem by a replica theory [69]. The idea is to introduce
two ‘inverse temperatures’ β1 and β2 for minority and majority players respectively,
such that [68]

max
m2

min
m1

H(m1, m2) = lim

log Z(β1, β2)

(210)

ii

1
β2 hh

β1,β2

→∞

CONTENTS

77

(211)

(212)

(213)

with the following generalized partition function:

Z(β1, β2) =

Z

dm2 eβ2

1
β1

−

h

R

log

dm1 e−

β1H

dm2

dm1 e−

β1

H

=

i

Z

(cid:20)Z

γ

−

(cid:21)

where γ = β2/β1 > 0. In physical jargon, this describes a system where: ﬁrst, the m1
variables are thermalized at a positive temperature 1/β1 with Hamiltonian H at ﬁxed
m2; then, the m2 variables are thermalized at a negative temperature
1/β2 with an
β1H . The disorder
eﬀective Hamiltonian Heﬀ deﬁned by
average can be carried out with the help of a ‘nested’ replica trick. First, one replicates
γ as a positive integer R (in the end,
the minority variables by treating the exponent
the limit R

γ < 0 must be taken). (211) thus becomes

β1Heﬀ(m2) = log

dm1 e−

−

−

−

R

→ −

R

Z =

dm2

dm1 e−

β1

H

=

dm2

β1

e−

mr
(
r H
{

1}

,m2)

Z

Z
Yr=1,R
Then a second replication is needed, this time on the m2 variables:

(cid:20)Z

(cid:21)

"Z

P

dmr
1

#

Z R′ =

β1

e−

mar
(
a,r H
{

,
1 }

ma
)
2 }
{

dmar

1 dma
2

Z

P

r=1,R
Ya=1,R′ Y

At this point we have two replica indexes with diﬀerent roles: the replicas labeled a
have been introduced to deal with the disorder, and their number R′ will eventually go
to zero, as usual; the replicas labeled r have been introduced to deal with the negative
temperature, and their number R must be set to a negative value. Majority variables
bear just one index, while minority ones have two. We can interpret this fact by saying
that ma
2 indicates a particular conﬁguration of the majority variables, i.e. a given
manifold in the whole m space; and mar
indicates the minority coordinates in that
1
particular manifold. Notice that the min and max operations and hence the meaning
of coordinates in the above interpretation can be interchanged. In general, this leads to
diﬀerent solutions. In our case, however, one can verify that the main results would not
change, though the intermediate steps (e.g. the deﬁnition of γ) would vary.

Following the procedure outlined above it is possible to calculate the phase diagram
of the model (Fig. 27), namely the line of critical points αc(f ) for diﬀerent values of
f separating the asymmetric, information-rich phase (α > αc(f )) from the symmetric,
unpredictable regime (α < αc(f )). One sees that the eﬃcient regime shrinks as the
fraction of trend-followers increases until, for f = 1/2 it disappears. Now trend-followers
are the majority group and the market becomes completely predictable. The dynamical
calculation clariﬁes the phase transition further by relating the critical line to the onset
of ergodicity breaking.

While this model captures one of the basic eﬀects of the presence of trend-followers
in the market, namely a decrease in eﬃciency, it is clear that the properties of mixed
games are to some extent a linear combination of those of pure games and thus a
gross simpliﬁcation with respect to a realistic case. Now it is reasonable to think that
real traders may revise their expectations if they prove wrong or simply may want to
weigh their decisions against other factors than the expected proﬁt. For instance, in

CONTENTS

78

0.4

0.3

0.1

0

0

α

0.2

ASYM

SYM

0.1

0.2

0.3

0.4

0.5

f

Figure 27. Phase diagram of the mixed majority-minority game (from [68]).

certain market regimes (e.g. bubbles) a trader could perceive the market as a Majority
rather than Minority Game and consequently switch from a fundamentalist to a trend-
following behavior. Similarly, in situations of high volatility traders would likely take
into account the risk factor when choosing a trading strategy over another. How would
the macroscopic properties of the Minority Game change if agents were allowed to modify
their behavior and expectations according to the market conditions they perceive?

This issue may be tackled through the introduction of a more general MG setting
with the rationale that traders prefer to adopt a trend-following attitude, and thus
perceive the market as a Majority Game, when ﬂuctuations are small while they revert
to fundamentals, and hence perceive the market as a Minority Game, when the price
dynamics becomes more chaotic [70, 71]. This mechanism leads to a surprisingly rich
phenomenology which includes the formation and disruption of trends and the emergence
of ‘heavy tails’ in the returns distribution. The model is deﬁned through

gi(t) = arg max Uig(t)
A(t) =

aµ(t)
igi(t)

i
X
Uig(t + 1)

−

Uig(t) = aµ(t)

ig Fi[A(t)]

where the function Fi embodies the way in which agent i perceives the performance of
his/her g-th trading strategy in the market. For simplicity we shall henceforth assume
that Fi = F for all i. Clearly, F (A) =
A for a Minority Game whereas F (A) = A for
a Majority Game. The case we consider is

−

F (A) = A

ǫA3

−

0. For ǫ = 0 one has a pure Majority Game. Upon increasing ǫ, the non-linear
A3. A

one obtains a Minority Game with F (A)

≥

with ǫ
gains importance, and for ǫ
couple of remarks are in order.

→ ∞

∝ −

(214)

(215)

CONTENTS

79

D

0.04

0.02

0

-0.02

ε = 0.1
ε = 1
ε = 2
ε = 10

0.1

0.01

0.001

0.0001
0.1

0.01

0.001

ε = 0.3
ε = 0.5
ε = 0.7
ε = 1
ε = 10

0.1

1
α

10

0.0001
0

1

2

3

4

6

7

8

9

10

5
A

Figure 28. Normalized return autocorrelation function D as a function of α = P/N
for diﬀerent values of ǫ (left) and probability distributions P (A) of A > 0 for diﬀerent
values of ǫ for α = 0.05 (top right) and α = 2 (bottom right). From [70].

(i) This mechanism is expected to induce a feed-back in the dynamics of the excess
demand: when it is small, trend-followers dominate and drive it to larger values
until fundamentalists eventually take over and drive it back to smaller values.

(ii) It is reasonable to think that ǫ should ﬂuctuate in time and possibly be coupled
to the system’s performance. A possible microscopic mechanism is the following.
When ǫ is large a high volatility is to be expected as agents are more likely to behave
as trend-followers. As a consequence, they should likely reduce their threshold since
the market is risky; however, for small ǫ fundamentalists are expected to dominate
and the game should acquire a Minority character. Hence the predictability will be
smaller and there will be less proﬁt opportunities. Agents may then decide to adopt
a larger threshold to seek for convenient speculations on a wider scale. If these two
competing eﬀects are appropriately described by an evolution equation for ǫ, the
system should self-organize around an ‘optimal’ value of the parameter. However
such a time evolution should take place on time-scales much longer than those
which the model addresses (intra-day/daily trading) and hence it is reasonable to
study the case of ﬁxed ǫ.

It turns out (see Fig. 28) that while for low enough (resp. high enough) ǫ the behavior
of a pure Majority (resp. Minority) game is recovered (with some qualitative diﬀerences
due to the unconventional nature of the MG in this case), there exists a range of values
of ǫ for which the two tendencies coexist and one can cross over from one to the other
by changing α and/or ǫ. This can be seen from the behavior of the (normalized)
/σ2 as a function of α. The crossover gets sharper
autocorrelation D =
and sharper as α increases and turns into a sharp threshold for α
1. In this case, the
threshold can be estimated analytically. Indeed one has

A(t)A(t + 1)
i

≫

h

yi(t + 1)

vi ≡ h
= 0, then yi(t)

−

yi(t)

= ξµ
i h

i

F (A)

µ
i

|

vit and si(t) tends asymptotically to sign(vi): there is a
As usual, if vi 6
well deﬁned preference towards one of the two strategies and the agent becomes frozen.

∼

(216)

CONTENTS

80

(217)

For large α, we can approximate A(t) with a Gaussian random variable with variance
H. By virtue of Wick’s theorem, this implies that

, so

3H

A3

µ

µ

h

|

i ≃

A
|
h

i

(1

vi ≃

−

3ǫH)ξµ
i h

A
|

µ

i

≤

−

≃

≫

µ
i

3ǫH

), which is unstable for 1

3ǫH > 0, the agents’ spins will freeze on the Majority-type solution si =
If 1
−
sign(ξµ
0. Given that H = 1 for large α,
A
i h
|
we see that the crossover from the Majority- to the Minority-regime takes place at
ǫ

1/3 for α
1, which is signiﬁcantly close to the numerical value of ǫc ≃
For small α, when the contribution of frozen agents is small, we expect the system
to self-organize around a value of A such that F (A) = 0: indeed one can see from Fig.
28 that the peak of the distribution moves as 1/√ǫ. Besides, as ǫ increases, large excess
demands occur with a ﬁnite probability. The emergence of such ‘tails’ in P (A), while not
power-law, is a clear non-Gaussian signature. The dynamics in this regime is particularly
interesting: while the market is mostly chaotic and dominated by contrarians, ‘ordered’
periods can arise where the excess demand is small and trends are formed, signaling
that chartists have taken over the market. These trends, that can be arbitrarily long,
eventually eventually die out restoring the fundamentalist regime.

0.37.

In order to understand the full impact of trend-followers it is however necessary to
emply endogenous information [71]. Indeed, one identiﬁes two regimes in an intermittent
market dynamics. Phases with small ﬂuctuations, dominated by contrarians and in
which the information dynamics is roughly ergodic over the possible patterns, are
followed by phases with large ﬂuctuations dominated by trend-followers, where the
information dynamics is strongly non-ergodic (actually a single information pattern
is dynamically selected).

5.6. Markets with asymmetric information

A crucial assumption in all models we have been dealing with so far is that all agents
possess the same information, be it the real price time series or the bar attendance
sequence or a random integer. As long as all agents process the same information
pattern the system can reach some level of coordination and a more or less complicated
phase structure arises. Unfortunately, it is hard to believe that all agents in real systems
possess the same information. This brings us to the question: how are the coordination
properties aﬀected when the information is private, i.e. agent-dependent?

This question is indeed of fundamental theoretical importance. A substantial part
of economic theory is based on the assumption that markets are informationally eﬃcient.
Roughly speaking, a market is eﬃcient with respect to an information set if the public
revelation of that information would not change the prices of the assets. In other words,
this means that all the relevant information is incorporated into prices. This includes
both public and private information. However, it has been understood [Akerlof] that
asymmetric information may cause ineﬃciency of the equilibrium, given the strategic
incentive of each agent not to reveal the information he has. The salvation comes from
in fact this nefarious eﬀect may vanish in large markets, since the
the system size:

CONTENTS

81

single bits of information possessed by an individual agent become less signiﬁcant the
larger is the number of agents. Hence, the common understanding is that prices reﬂect
information more accurately in large systems.

To conclude our review, we shall now discuss a model in which the above scenario
emerges as a phase transition between an informationally eﬃcient phase and an
informationally ineﬃcient one [72]. The control parameter is, as in the MG, the ratio
between the size of the information space and the number of traders.

We consider a market with one asset. The market can ﬁnd itself in any of P states
of the world, labeled µ. The return of the asset depends on the state of the world only,
and is denoted by Rµ. We assume that each Rµ is given by

Rµ = R +

rµ
√N

{
{

{

where the rµ are independent samples of a Gaussian random variable with zero mean
and variance s drawn at time t = 0 and ﬁxed (quenched disorder). We further assume
that at each time step the state of the market is drawn randomly and independently from
with equal probability. This process determines the time series of returns

}t
≥
N traders act in this market. They have no information concerning the state of
the world but rather they observe a coarse-grained signal on the information space

1, . . . , P
Rµ(t)

}
0 completely.

1, . . . , P

. We denote it as a vector

}

ki :

1, . . . , P

{

µ

} ∋

→

kµ
i ∈ {−

1, 1

}

in which every state of the market is associated to a particular value of a binary variable
(in other words, an agent cannot tell which state the market is in but only knows whether
it is an “up state” or a “down state”). Diﬀerent agents receive diﬀerent signals, as each
component kµ
i of every vector ki is taken to be drawn randomly and independently from
with equal probability for all i and µ. This deﬁnes the private information
{−
structure. Note that if an agent knew simultaneously the partial information of all
agents he would be able to know the state µ, with probability one, for N

1, 1

}

.

At each time step, traders i has to decide an investment. Let zi(t) denote the
amount of money he decides to invest (buying or selling) at time t. We assume that the
price at time t, p(t) is ﬁxed by a market clearing condition, in which the demand of the
asset is determined by the aggregate money invested and the supply is ﬁxed at N:

→ ∞

(218)

(219)

(220)

zi(t) = p(t)

1
N

i
X

We further assume that zi(t) depends on whether his information kµ(t)
is “up” or “down”: zi(t) =
the state since the amount invested by each agent depends on the state: p(t) = pµ(t).

about the state
i
,m. In this way the price depends on

zm
i (t)δkµ(t)

∈{−

1,1

m

}

i

At the end of each period t, each unit of asset pays a monetary amount Rµ(t). If
agent i has invested zi(t) units of money, he will hold zi(t)/p(t) units of asset, so his

P

CONTENTS

82

payoﬀ will be zi(t)( Rµ(t)

1). It follows that the expected payoﬀ is given by

p(t) −
1
P

πi =

δkµ

i ,mzm

i

Rµ
pµ −

(cid:18)

1

=

(cid:19)

1,1
Xm
∈{−

δki,mzm
i

R
p −

(cid:18)

(cid:19)

1

(221)

µ
X

1,1
Xm
∈{−

}

}
Every agent aims at choosing the zm
i ’s so as to maximize their expected payoﬀ. We
consider inductive agents who repeatedly trade in the market. Each agent i has
a propensity to invest U m
. His investment
}
R+
zm
i = χi(U m
0 ) with χi(x)
0
if x
(a convenient choice for numerical experiments
is zm
i θ(U m
i (t) according to the marginal
success of the investment:

i ) at time t is an increasing function of U m
if x

→ ∞
i )). After each period agents update U m

i (t) for each of the signals m

∈ {−
i (t) (χi : R

→ −∞
i = U m

and χi(x)

→ ∞

1, 1

→

→

i (t + 1) = U m
U m

i (t) + Γδkµ(t)

,m

i

R(t)

p(t)

−

(cid:20)

η

zi(t)
N

−

(cid:21)

(222)

The idea is that if the return is larger than the price, the agent’s propensity to invest in
that signal increases, otherwise it decreases. The η term provides the distinction between
na¨ıve (or price-taking) agents (η = 0), who are unaware of their market impact, and
“sophisticated” traders (η = 1) who instead are able to disentangle their contribution
to the price exactly. Γ > 0 is a parameter (In [72] the dynamics (222) is obtained from a
more properly justiﬁed process involving the marginal utility of a certain investment.).
As a measure of coordination we employ the distance between prices and returns

(223)

in the steady state:

H =

R

|

2

p

|

−

≡

(Rµ

pµ)2

−

µ
X

Clearly, if H = 0 prices follow returns and hence incorporate the information about the
states of the world, so that the market is informationally eﬃcient.

Numerical results for the stationary H as a function of α = P/N for η = 0 and

η = 1 (and Γ small enough) are given in Fig. 29.

Let us start from na¨ıve traders (η = 0). As the number of agents increases, i.e.
as α = P/N decreases, agents are collectively more eﬃcient in driving prices close to
Indeed the distance H decreases as α decreases. The price-return distance
returns.
vanishes at a critical point αc which turns out to mark a second order phase transition
in the statistical mechanics approach. The value of αc depends on the intensity s of
ﬂuctuations of returns. The region α < αc is characterized by the condition H = 0,
which means pµ = Rµ for all µ. This means that the market eﬃciently aggregates
the information dispersed across agents into the price.
It can be shown that the
eﬃcient phase, where H = 0, shrinks as s increases. This is reasonable because as the
ﬂuctuations in Rµ increase, it becomes harder and harder for the agents to incorporate
them into prices. This behavior can be understood analytically as usual by constructing
It turns out that H is a Lyapunov function of
the continuous-time limit of (222).
the dynamics: price takers cooperate to make the market as informationally eﬃcient
as possible. From the agent’s point of view the steady states in the eﬃcient phase

CONTENTS

1.0

H/α

0.5

u0=0
u0=2
u0=5
u0=10

1

H/α

0.5

83

0.0

10−1

100
α

101

0
10−1

100
α

101

Figure 29. Behaviour of H/α versus α for η = 0 (left; u(0) is the initial bias in
the score functions) and various η > 0 (right: η = 0.05 (circles), η = 0.25 (squares),
η = 0.5 (diamonds) and η = 0.75 (triangles). From [73].

U m

(α < αc) are not unique and the state in which agents will end up depends on the
initial conditions
(prices, of course, do not depend on the initial condition,
because pµ = Rµ for all µ). It can also be shown that these steady states in which H
is minimum correspond to competitive equilibria, namely conﬁgurations obtained when
agents choose their investments zm
i a priori by solving

i (t = 0)
}

{

R
p −

xδki,m

max
0
x
≥
, namely by maximizing their expected proﬁts.

(cid:18)

(cid:19)

1

for m

∈ {−

1, 1

}

Turning to sophisticated agents (η = 1), one sees that the phase transition
disappears: the distance between prices and returns smoothly decreases as α decreases
and it vanishes only in the limit α
0. Moreover, the steady state is unique in both
prices and investment for all α > 0: the asymptotic behavior of learning dynamics does
not depend on initial conditions. It can be shown that the steady state in this case is
a Nash equilibrium, that is it corresponds to all agents choosing their investments by
solving

→

x

max
0
x
≥
, where pω
−

µ
X

δkµ

i ,m

(cid:18)
i = pω

pω
−

Rω
i + x/N −

1

(cid:19)
i ,mzm
δkω

1, 1

∈ {−

for m
i /N is the contribution of all other
1,1
}
agents to the price (in other words, each trader disentangles his contribution from the
price and optimizes the response to all other traders).

P

∈{−

−

}

m

These ﬁndings defy the intuition that Nash equilibria behave similarly to
. Another striking proof of the diﬀerence between

competitive equilibria when N
the two equilibrium concepts is given by the quantity

→ ∞

(224)

(225)

(226)

N

q =

1
N

i=1 (cid:18)
X

z+
i −
2

2

z−i

(cid:19)

CONTENTS

84

3

2

1

q

0
10−1

u0=0
u0=2
u0=5
u0=10

1.5

1

0.5

q

100
α

101

0
10−1

100
α

101

Figure 30. Behaviour of q versus α for η = 0 (left; u(0) is the initial bias in the score
functions) and various η > 0 (right: η = 0.05 (circles), η = 0.25 (squares), η = 0.5
(diamonds) and η = 0.75 (triangles). From [73].

which measures how diﬀerently agents invest under the two signals, i.e. how much
they use the information they possess (Fig. 30). Price takers exploit their signals
much more than sophisticated traders, who invest very similar amounts of money in the
two states they distinguish. Note that for η = 0 the steady state depends on initial
conditions below αc. The eﬃcient/ineﬃcient transition may then be characterized also
dynamically trhough transition via path-integral methods [73].

There are several other aspects of the model that deserve attention, starting with
the dependence of ﬂuctuations on Γ. We refer the interested reader to [72,73] for a more
detailed discussion.

6. Conclusions

Compared to reality, the models discussed in this review have a marked theoretical
nature. The aim of these models is not that of providing quantitative predictions
but rather to understand under what conditions the rich variety of behaviors, ranging
from anomalous ﬂuctuations to spontaneous coordination, may emerge in a simpliﬁed
controllable setting. This is a complementary approach to that of empirical analysis,
which has been dominating the scene of interdisciplinary ventures of statistical physicists
into economics and ﬁnance.
Indeed, a proper understanding of how interaction
propagates from the micro to the macro scale, is crucial in many cases in order to
infer what empirical analysis should focus on.

Here we have reviewed a number of models with N heterogeneous interacting
agents – be they ﬁrms, species, drivers or traders – who compete for the exploitations
of a number P of resources. The collective behavior of all these systems belongs to
the same generic phenomenology, as discussed in Secion 2.1. A key parameter is the
ratio (α = P/N) between the number of resources and the number of agents, and the

CONTENTS

85

central quantities of interest are the (in)eﬃciency σ2, which is related to the amount of
unexploited resources, and the unevenness H with which resources are exploited.

→ ∞

The collective behavior depends strongly on whether agents account or not for their
impact on the resources. This is somewhat surprising, as one would expect that in the
limit N
, the contribution of each agent to the exploitation of each resource is
vanishing. For the ease of exposition, we distinguish between the two extreme case
of competitive equilibria (CE) and Nash equilibria (NE), where agents fully neglect
or account exactly for their impact, respectively. The stationary state of the learning
dynamics which converges to these equilibria, in Minority Game type models markedly
diﬀer in the following respects:

Equilibrium condition In CE resources are exploited, on average, as evenly as
possible, i.e. H is minimal. In NE ﬂuctuations or wastes are as small as possible
(i.e. σ2 is minimal).

Phase transition A phase transition occurs in CE when the number of agents exceeds
a critical one, i.e. when α < αc. This separates an asymmetric (H > 0 for α > αc)
from a symmetric (H = 0 for α
αc) phase. No phase transition takes place in
NE (i.e. H > 0 for all α > 0)

≤

≤

Degeneracy The stationary state is unique in CE for α > αc and it is degenerate on

a continuous set for α

αc. There is an exponential number of disjoint NE.

Initial conditions The stationary state does not depend on initial conditions for CE
αc. The NE

and α > αc and it depends continuously on initial conditions for α
to which the system converges depends discontinuously on initial conditions.
Fluctuations Agents’ behavior is stochastic in CE (i.e. σ2 > H) whereas it is
deterministic (σ2 = H) in NE. Put diﬀerently, in NE agents always play a single
strategy, whereas in CE agents switch between diﬀerent strategies.

≤

Number of choices Giving more strategies to agents improves coordination in NE but

it can make agents worse oﬀ in CE (typically when α is small).

Convergence Agents converge fast to CE whereas agents may fail to learn to

coordinate on NE [74]

Not all these conclusions apply to the asset market model with private information of
Sec. 5.6, though even there CE and NE diﬀer substantially [72].

There still remain interesting theoretical challenges in this ﬁeld. Some of these are:

•

•

The MG is a prototype model of a systems where the collective ﬂuctuations which
agents produce feed back into their dynamics. Still, there are no analytical tools
which allows us to characterize this feedback in precise terms in the symmetric
phase of the MG, i.e. to compute the volatility σ2 as a function of Γ.

MG based models of ﬁnancial markets show that anomalous ﬂuctuations similar to
the stylized facts observed in real markets arise close to the phase transition line.
Still the critical properties at this phase transition have not yet been characterized.
Detailed numerical studies of critical properties or analytic approaches based on

CONTENTS

86

renormalization group techniques would be very important to shed light on this
issue.

•

•

The MG suggests that real markets operate close to a phase transition but it does
not explicitly describe a mechanism of how markets would “self-organize” to such
a state. Though some arguments have been put forward [57], these have not yet
been formalized in a deﬁnite model.

The extensions to cases where ﬁrms behave strategically, as in Cournot games [2],
of the model of economic equilibria may prove interesting. The conjecture is that,
even in the limit N
if the number of commodities (or markets) also diverges,
→ ∞
the NE may be markedly diﬀerent from a CE.

As a concluding remark, we observe that socio-economic phenomena have features
which are markedly diﬀerent from those addressed in natural sciences. Above all, the
economy and society change at a rate which is probably much faster than that at which
we understand them. For example, many of the things which are traded nowadays in
ﬁnancial markets did not exist few decades ago, not to speak of internet communities. In
addition, we face a situation in which the density and range of interactions are steadily
increasing, thus making theoretical concepts based on eﬀective non-interacting theories
inadequate.

Deﬁnitely, socio-economic systems provide several interesting theoretical challenges.
Our hope is that these eﬀort will help reﬁne our understanding of how individual
behavior, interaction and randomness may conspire in shaping collective phenomena,
which, broadly speaking, is the aim of statistical physics.

Acknowledgments

This review has greatly beneﬁted from the interactions we had with many colleagues
over the last few years. It is our pleasure to thank in particular G Bianconi, D Challet, S
Cocco, ACC Coolen, JD Farmer, FF Ferreira, S Franz, T Galla, I Giardina, JAF Heimel,
E Marinari, R Monasson, R Mulet, G Mosetti, I Perez Castillo, F Ricci Tersenghi,
A Tedeschi, MA Virasoro, R Zecchina and YC Zhang. We acknowledge ﬁnancial
support from the EU grant HPRN-CT-2002-00319 (STIPCO), the EU-NEST project
COMPLEXMARKETS, the MIUR strategic project “Dinamica di altissima frequenza
dei mercati ﬁnanziari”, and from EVERGROW, integrated project no. 1935 in the
complex systems initiative of the Future and Emerging Technologies directorate of the
IST Priority, EU Sixth Framework.

References

Cambridge)

Cambridge)

[1] Wigner E 1955 Ann. of Math 62 548
[2] Vega Redondo F 2003 Game theory and economic applications (Cambridge University Press,

[3] Mantegna RN and Stanley HE 2000 An introduction to econophysics (Cambridge University Press,

CONTENTS

87

[4] Dacorogna MM, Gencay R, M¨uller UA, Olsen RB and Pictet OV 2001 An introduction to high-

frequency ﬁnance (Academic Press, San Diego, CA)

[5] Bouchaud JP and Potters M 2003 Theory of ﬁnancial risk and derivative pricing: from statistical

physics to risk management (Cambridge University Press, Cambridge)

[6] Johnson NF, Jeﬀeries P and Hui PM 2003 Financial market complexity (Oxford, University Press,

Oxford)

Princeton)

Princeton)

[7] Voit J 2005 The statistical mechanics of ﬁnancial markets (Springer-Verlag, Berlin)
[8] Tilman D 1982 Resource competition and community structure (Princeton University Press,

[9] Rieger H 1989 J. Phys. A 22 3447
[10] May RM 1973 Stability and complexity in model ecosystems (Princeton University Press,

[11] Sengupta AM and Mitra PP 1999 Phys. Rev. E 60 3389
[12] Arthur WB 1994 Am. Econ. Rev. Pap. Proc. 84 406
[13] Challet D, Marsili M and Ottino G 2004 Physica A 332 469
[14] Weisstein EW http://mathworld.wolfram.com/deBruijnGraph.html
[15] Challet D and Marsili M 2000 Phys. Rev. E 62 1862
[16] Marsili M and Challet D 2001 Phys. Rev. E 64 056138
[17] Weisbuch G, Kirman A and Herreiner D 2001 Economic J. 110 411
[18] De Martino A and Marsili M 2005 Proceedings of SPIE 5848 165
[19] Selten R et al. 2004 Experimental Investigation of Day-to-Day Route Choice-Behaviour and
Simulation of Autobahn Traﬃc in NRW. In: Traﬃc and Human Behaviour (Selten R and
Schreckenberg M, eds) (Springer, Heidelberg)

[20] Helbing D, Schoenhof M and Kern D 2002 New J. Phys. 4 33
[21] De Martino A, Marsili M and Mulet R 2004 Europhys. Lett. 65 283
[22] Mas-Colell A, Whinston MD and Green JR 1995 Microeconomic theory (Oxford University Press,

Oxford)

[23] Lancaster K 1987 Mathematical economics (Dover, New York)
[24] Gale D 1960 The theory of linear economic models (The University of Chicago Press, Chicago)
[25] De Martino A 2005 Prog. Theor. Phys. Suppl. 157 308
[26] Korutcheva E, Opper M and Lopez B 1994 J. Phys. A 27 L645
[27] Inoue J 1997 J. Phys. A 30 1047
[28] Nishimori H 2001 Statistical physics of spin-glasses and information processing: an introduction

(Oxford University Press, Oxford)

[29] Kirman AP 1992 J. Econ. Persp. 6 117
[30] Wigner E 1958 Ann. of Math. 67 325
[31] De Martino A, Marsili M and Perez Castillo I 2004 JSTAT P04002
[32] De Martino A, Marsili M and Perez Castillo I 2006 Macroeconomic Dynamics (to appear)
[33] Okuyama K, Takayasu M and Takayasu H 1999 Physica A 269 125
[34] Von Neumann J 1937 Ergebn. eines Math. Kolloq. 8. English translation: Von Neumann J 1945

Rev. Econ. Studies 13 1

[35] McKenzie LW 1986 Optimal Economic Growth, Turnpike Theorems and Comparative Dynamics,
in Arrow KJ and Intriligator MD (eds), Handbook of Mathematical Economics, Vol. III (North-
Holland, Amsterdam)

[36] De Martino A and Marsili M 2005 JSTAT L09003
[37] Gardner E 1988 J. Phys. A: Math. Gen. 21 257
[38] Romer P 1990 J. Pol. Econ. 98 S72
[39] Challet D and Zhang YC 1997 Physica A 246 407
[40] Fama EF 1965 J. Business 36 420
[41] Pagan A 1999 J. Empirical Finance 3 15
[42] Challet D, Marsili M and Zhang YC 2005 Minority Games (Oxford University Press, Oxford)

CONTENTS

88

[43] Coolen ACC 2005 The mathematical theory of Minority Games (Oxford University Press, Oxford)
[44] Marsili M 2001 Physica A 299 93
[45] Farmer JD 1999 SFI Technical Report 98-12-117
[46] Lux T and Marchesi M 1999 Nature 397 498
[47] Challet D and Marsili M 1999 Phys. Rev. E 60 R6271
[48] Cavagna A 1999 Phys. Rev. E 59 R3783
[49] Savit R, Manuca R and Riolo R 1999 Phys. Rev. Lett. 82 2203
[50] Hart M, Jeﬀeries P, Hui PM and Johnson NF 2001 Eur. Phys. J. B 20 547
[51] Cavagna A, Garrahan JP, Giardina I and Sherrington D 1999 Phys. Rev. Lett. 83 4429
[52] Marsili M, Challet D and Zecchina R 2000 Physica A 280 522
[53] Coolen ACC, Heimel JAF and Sherrington D 2001 Phys. Rev. E 65 016126
[54] De Martino A and Marsili M 2001 J. Phys. A 34 2525
[55] Challet D and Marsili M 2000 Phys. Rev. E 62 1862
[56] Coolen ACC 2005 J. Phys. A 38 2311
[57] Challet D and Marsili M 2003 Phys. Rev. E 68 036132
[58] Johnson NF, Hui PM, Zheng D and Tai CW 1999 Physica A 269 493
[59] Challet D, Marsili M and Zhang YC 2000 Physica A 276 284
[60] Challet D, Chessa A, Marsili M and Zhang YC 2000 Quant. Finance 1 168
[61] D’Hulst R and Rodgers GJ 1999 Preprint adap-org/9904003
[62] Chow FK and Chau HF 2003 Physica A 319 601
[63] Bianconi G, De Martino A, Ferreira FF and Marsili M 2006 Preprint physics/0603152
[64] Martin PC, Siggia ED and Rose HA 1973 Phys. Rev. A 8 423
[65] De Dominicis C 1978 Phys. Rev. B 18 4913
[66] Heimel JAF and Coolen ACC 2001 Phys. Rev. E 63 056121
[67] Kozlowski P and Marsili M 2003 J. Phys. A 36 11725
[68] De Martino A, Giardina I and Mosetti G 2003 J. Phys. A 36 8935
[69] Varga P 1998 Phys. Rev. E 57 6487
[70] De Martino A, Giardina I, Marsili M and Tedeschi A 2004 Phys. Rev. E 70 025104(R)
[71] Tedeschi A, De Martino A and Giardina I 2005 Physica A 358 529
[72] Berg J, Marsili M, Rustichini A and Zecchina R 2001 Quant. Finance 1 203
[73] De Martino A and Galla T 2005 JSTAT P08008
[74] Marsili M, Mulet RG, Ricci-Tersenghi F and Zecchina R 2001 Phys. Rev. Lett. 87 208701


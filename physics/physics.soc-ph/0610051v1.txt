Structural Inference of Hierarchies in Networks

6
0
0
2
 
t
c
O
 
9
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
1
5
0
0
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Aaron Clauset
Department of Computer Science, University of New Mexico, Albuquerque, NM 87131 USA

aaron@cs.unm.edu

Cristopher Moore
Departments of Computer Science and Physics and Astronomy, University of New Mexico, Albuquerque, NM
87131 USA

moore@cs.unm.edu

Mark Newman
Department of Physics and Center for the Study of Complex Systems, University of Michigan, Ann Arbor, MI
48109 USA

mejn@umich.edu

Abstract
One property of networks that has received
comparatively little attention is hierarchy,
i.e., the property of having vertices that clus-
ter together in groups, which then join to
form groups of groups, and so forth, up
through all levels of organization in the net-
work. Here, we give a precise deﬁnition of hi-
erarchical structure, give a generic model for
generating arbitrary hierarchical structure in
a random graph, and describe a statistically
principled way to learn the set of hierarchical
features that most plausibly explain a partic-
ular real-world network. By applying this ap-
proach to two example networks, we demon-
strate its advantages for the interpretation of
network data, the annotation of graphs with
edge, vertex and community properties, and
the generation of generic null models for fur-
ther hypothesis testing.

1. Introduction

Networks or graphs provide a useful mathematical rep-
resentation of a broad variety of complex systems, from
the World Wide Web and the Internet to social, bio-
chemical, and ecological systems. The last decade has
seen a surge of interest across the sciences in the study
of networks, including both empirical studies of par-
ticular networked systems and the development of new
techniques and models for their analysis and interpre-
tation [1, 12].

Appearing in Proceedings of the 23 rd International Con-
ference on Machine Learning, Pittsburgh, PA, 2006. Copy-
right 2006 by the author(s)/owner(s).

Within the mathematical sciences, researchers have
focused on the statistical characterization of network
structure, and, at times, on producing descriptive gen-
erative mechanisms of simple structures. This ap-
in which scientists have focused on statis-
proach,
tical summaries of network structure, such as path
lengths [21, 10], degree distributions [3], and correla-
tion coeﬃcients [11], stands in contrast with, for exam-
ple, the work on networks in the social and biological
sciences, where the focus is instead on the properties
of individual vertices or groups. More recently, re-
searchers in both areas have become more interested
in the global organization of networks [18, 20].

One property of real-world networks that has received
comparatively little attention is that of hierarchy, i.e.,
the observation that networks often have a fractal-like
structure in which vertices cluster together into groups
that then join to form groups of groups, and so forth,
from the lowest levels of organization up to the level of
the entire network. In this paper, we oﬀer a precise def-
inition of the notion of hierarchy in networks and give
a generic model for generating networks with arbitrary
hierarchical structure. We then describe an approach
for learning such models from real network data, based
on maximum likelihood methods and Markov chain
Monte Carlo sampling. In addition to inferring global
structure from graph data, our method allows the re-
searcher to annotate a graph with community struc-
ture, edge strength, and vertex aﬃliation information.

At its heart, our method works by sampling hierar-
chical structures with probability proportional to the
likelihood with which they produce the input graph.
This allows us to contemplate the ensemble of ran-
dom graphs that are statistically similar to the origi-
nal graph, and, through it, to measure various average

Structural Inference of Hierarchies in Networks

network properties in manner reminiscent of Bayesian
model averaging. In particular, we can

1. search for the maximum likelihood hierarchical
model of a particular graph, which can then be
used as a null model for further hypothesis test-
ing,

2. derive a consensus hierarchical structure from the
ensemble of sampled models, where hierarchical
features are weighted by their likelihood, and

3. annotate an edge, or the absence of an edge, as
“surprising” to the extent that it occurs with low
probability in the ensemble.

To our knowledge, this method is the only one that
oﬀers such information about a network. Moreover,
this information can easily be represented in a human-
readable format, providing a compact visualization
of important organizational features of the network,
which will be a useful tool for practitioners in gener-
ating new hypotheses about the organization of net-
works.

2. Hierarchical Structures

The idea of hierarchical structure in networks is not
new; sociologists, among others, have considered the
idea since the 1970s. For instance, the method known
as hierarchical clustering groups vertices in networks
by aggregating them iteratively in a hierarchical fash-
ion [19]. However, it is not clear that the hierarchical
structures produced by these and other popular meth-
ods are unbiased, as is also the case for the hierarchical
clustering algorithms of machine learning [8]. That is,
it is not clear to what degree these structures reﬂect
the true structure of the network, and to what degree
they are artifacts of the algorithm itself. This conﬂa-
tion of intrinsic network properties with features of the
algorithms used to infer them is unfortunate, and we
speciﬁcally seek to address this problem here.

A hierarchical network, as considered here, is one that
divides naturally into groups and these groups them-
selves divide into subgroups, and so on until we reach
the level of individual vertices. Such structure is most
often represented as a tree or dendrogram, as shown,
for example, in Figure 1. We formalize this notion
precisely in the following way. Let G be a graph
with n vertices. A hierarchical organization of G is a
rooted binary tree whose leaves are the graph vertices
and whose internal (i.e., non-leaf) nodes indicate the
hierarchical relationships among the leaves. We de-
,
note such an organization by

D1, D2, . . . , Dn−1

=

D

{

}

Figure 1. A small network and one possible hierarchical or-
ganization of its nodes, drawn as a dendrogram.

Figure 2. An example hierarchical model H(D, ~θ), showing
a hierarchy among seven graph nodes and the Bernoulli
trial parameter θi (shown as a gray-scale value) for each
group of edges Di.

where each Di is an internal node, and every node-pair
(u, v) is associated with a unique Di, their lowest com-
mon ancestor in the tree. In this way,
partitions the
edges of G.

D

3. A Random Graph Model of
Hierarchical Organization

H

(
D

, ~θ) of the hierarchical
We now give a simple model
organization of a network. Our primary assumption
is that the edges of G exist independently but with
a probability that is not identically distributed. One
may think of this model as a variation on the classical
Erd˝os-R´enyi random graph, where now the probability
that an edge (u, v) exists is given by a parameter θi as-
sociated with Di, the lowest common ancestor of u, v
. Figure 2 shows an example model on seven graph
in
, ~θ) repre-
vertices. In this manner, a particular
sents an ensemble of inhomogeneous random graphs,
where the inhomogeneities are exactly speciﬁed by the
topological structure of the dendrogram
and the cor-
responding Bernoulli trial parameters ~θ. Certainly,
one could write down a more complicated model of

(
D

H

D

D

Structural Inference of Hierarchies in Networks

graph hierarchy. The model described here, however,
is a relatively generic one that is suﬃciently powerful
to enrich considerably our ability to learn from graph
data.

D

H

(
D

(
D

, ~θ) will be statistically similar to G.

turn to the question of ﬁnding the
Now we
, ~θ) that most accurately, or
parametrizations of
rather most plausibly, represent the structure that we
observe in our real-world graph G. That is, we want
and ~θ such that a graph instance drawn
to choose
from the ensemble of random graphs represented by
If we al-
H
ready have a dendrogram
, then we may use the
method of maximum likelihood [5] to estimate the pa-
rameters ~θ that achieve this goal. Let Ei be the num-
ber of edges in G that have lowest common ancestor i
in
, and let Li (Ri) be the number of leaves in the
left- (right-) subtree rooted at i. Then, the maximum
likelihood estimator for the corresponding parameter
is θi = Ei/LiRi, the fraction of potential edges be-
tween the two subtrees of i that actually appear in
our data G. The posterior probability, or likelihood of
the model given the data, is then given by

D

D

, ~θ) =

H(

L

D

n−1

Y
i=1

(θi)Ei (1

θi)LiRi−Ei

.

(1)

−

While it is easy to ﬁnd values of θi by maximum likeli-
hood for each dendrogram, it is not easy to maximize
the resulting likelihood function analytically over the
space of all dendrograms. Instead, therefore, we em-
ploy a Markov chain Monte Carlo (MCMC) method to
estimate the posterior distribution by sampling from
the set of dendrograms with probability proportional
to their likelihood. We note that the number of pos-
sible dendrograms with n leaves is super-exponential,
√2 (2n)n−1e−n where !! de-
growing like (2n
notes the double factorial. We ﬁnd, however, that in
practice our MCMC process mixes relatively quickly
for networks of up to a few thousand vertices. Finally,
to keep our notation concise, we will use
µ to denote
the likelihood of a particular dendrogram µ, when cal-
culated as above.

3)!!

−

≈

L

4. Markov Chain Monte Carlo sampling

Our Monte Carlo method uses
standard
Metropolis-Hastings [14] sampling scheme; we now
brieﬂy discuss the ergodicity and detailed balance is-
sues for our particular application.

the

Let ν denote the current state of the Markov chain,
which is a dendrogram
. Each internal node i of the
dendrogram is associated with three subtrees a, b, and
c, where two are its children and one is its sibling—see

D

a

b

c

a

b

c

a

c

b

Figure 3. Each internal dendrogram node i (circle) has
three associated subtrees a, b, and c (triangles), which
together can be in any of three conﬁgurations (up to a
permutation of the left-right order of subtrees).

Figure 3. As the ﬁgure shows, these subtrees can be in
one of the three hierarchical conﬁgurations. To select a
candidate state transition ν
µ for our Markov chain,
we ﬁrst choose an internal node uniformly at random
and then choose one of its two alternate conﬁgurations
uniformly at random.
It is then straightforward to
show that the ergodicity requirement is satisﬁed.

→

Detailed balance is ensured by making the standard
Metropolis choice of acceptance probability for our
candidate transition: we always accept a transition
that yields an increase in likelihood or no change,
i.e., for which
ν ; otherwise, we accept a tran-
sition that decreases the likelihood with probability
equal to the ratio of the respective state likelihoods
ν = elog Lν −log Lµ . This Markov chain then gen-
L
erates dendrograms µ at equilibrium with probabilities
proportional to

≥ L

µ
L

µ/

L

µ.

L

5. Mixing Time and Point Estimates

With the formal framework of our method established,
we now demonstrate its application to two small,
canonical networks: Zachary’s karate club [22], a so-
cial network of n = 34 nodes and m = 78 edges rep-
resenting friendship ties among students at a univer-
sity karate club; and the year 2000 Schedule of NCAA
college (American) football games, where nodes rep-
resent college football teams and edges connect teams
if they played during the 2000 season, where n = 115
and m = 613. Both of these networks have found use
as standard tests of clustering algorithms for complex
networks [7, 15, 13] and serve as a useful comparative
basis for our methodology.

Figure 4 shows the convergence of the MCMC sam-
pling algorithm to the equilibrium region of model
space for both networks, where we measure the num-
ber of steps normalized by n2. We see that the Markov
chain mixes quickly for both networks, and in practice
we ﬁnd that the method works well on networks with
up to a few thousands of vertices. Improving the mix-

Structural Inference of Hierarchies in Networks

 

−800

 

−80

−100

−120

−140

−160

−180

−200

d
o
o
h

i
l

e
k

i
l

−
g
o

l

−1000

−1200

−1400

−1600

−1800

−2000

karate, n=34

NCAA 2000, n=115

−220
 
10

−5

0
10
time / n2

−2200
 
10

−5

5
10

0
10
time / n2

5
10

Figure 4. Log-likelihood as a function of the number of
MCMC steps, normalized by n2
, showing rapid conver-
gence to equilibrium.

ing time, so as to apply our method to larger graphs,
may be possible by considering state transitions that
more dramatically alter the structure of the dendro-
gram, but we do not consider them here. Addition-
ally, we ﬁnd that the equilibrium region contains many
roughly competitive local maxima, suggesting that any
particular maximum likelihood point estimate of the
posterior probability is likely to be an overﬁt of the
data. However, formulating an appropriate penalty
function for a more Bayesian approach to the calcula-
tion of the posterior probability appears tricky given
that it is not clear to how characterize such an overﬁt.
Instead, we here compute average features of the den-
drogram over the equilibrium distribution of models
to infer the most general hierarchical organization of
the network. This process is described in the following
section.

To give the reader an idea of the kind of dendrograms
our method produces, we show instances that corre-
spond to local maxima found during equilibrium sam-
pling for each of our example networks in Figures 5
(top) and 6 (top). For both networks, we can validate
the algorithm’s output using known metadata for the
nodes. During Zachary’s study of the karate network,
for instance, the club split into two groups, centered
on the club’s instructor and owner (nodes 1 and 34 re-
spectively), while in the college football schedule teams
are divided into “conferences” of 8–12 teams each, with
a majority of games being played within conferences.
Both networks have previously been shown to exhibit
strong community structure [7, 15], and our dendro-
grams reﬂect this ﬁnding, almost always placing leaves

with a common label in the same subtree. In the case
of the karate club, in particular, the dendrogram bipar-
titions the network perfectly according to the known
groups. Many other methods for clustering nodes in
graphs have diﬃculty correctly classifying vertices that
lie at the boundary of the clusters; in contrast, our
method has no trouble correctly placing these periph-
eral nodes.

6. Consensus Hierarchies

Turning now to the dendrogram sampling itself, we
consider three speciﬁc structural features, which we
average over the set of models explored by the MCMC
at equilibrium. First, we consider the hierarchical re-
lationships themselves, adapting for the purpose the
technique of majority consensus, which is widely used
in the reconstruction of phylogenetic trees [4]. Brieﬂy,
this method takes a collection of trees
}
and derives a majority consensus tree Tmaj contain-
ing only those hierarchical features that have majority
weight, where we somehow assign a weight to each
tree in the collection. For our purposes, we take the
weight of a dendrogram
simply to be its likeli-
hood
D, which produces an averaging scheme similar
to Bayesian model averaging [8]. Once we have tabu-
lated the majority-weight hierarchical features, we use
a reconstruction technique to produce the consensus
dendrogram. Note that Tmaj is always a tree, but is
not necessarily strictly binary.

T1, T2, . . . , Tk

D

L

{

The results of applying this process to our example
networks are shown in Figures 5 (bottom) and 6 (bot-
tom). For the karate club network, we observe that the
bipartition of the two clusters remains the dominant
hierarchical feature after sampling a large number of
models at equilibrium, and that much of the partic-
ular structure low in the dendrogram shown in Fig-
ure 5 (top) is eliminated as distracting. Similarly, we
observe some coarsening of the hierarchical structure
in the NCAA network, as the relationships between
individual teams are removed in favor of conference
clusterings.

7. Edge and Node Annotations

We can also assign majority-weight properties to nodes
and edges. We ﬁrst describe the former, where we
assign a group aﬃliation to each node.

Given a vertex, we may ask with what likelihood it is
placed in a subtree composed primarily of other mem-
bers of its group (with group membership determined
by metadata as in the examples considered here). In a
, we say that a subtree rooted at some
dendrogram

D

Structural Inference of Hierarchies in Networks

2
5

8

 

4
1

2

6

3

4

1

0

 3

13

 4

3

3

3

0

2

8

3
4

1

 

5
 

 6

 7

3

3

16

30

27

24

2 8

9

2

3 2

21

19

2 0

2

2

1 8

 2

12

 5

 6

 7

2

7

24

23

21

19

1 6

5

1

1 0

32

29

11

1 7

  2

  8

14

 3

 4

18

2

0

1

1

1

7

 

1

5
31

12
3

 

9

2

2

1

3

1
93

1

22

6 

52

(a)

(b)

Figure 5. Zachary’s karate club network: (a) an exemplar maximum likelihood dendrogram with log L = −73.32, param-
eters θi are shown as gray-scale values, and leaf shapes denote conference aﬃliation; and (b) the consensus hierarchy
sampled at equilibrium. Leaf shapes are common between (a) and (b), but position varies.

node i encompasses a group g if both the majority of
the descendants of i are members of group g and the
majority of members of group g are descendants of i.
We then assign every leaf below i the label of g. We
note that there may be some leaves that belong to no
group, i.e., none of their ancestors simultaneously sat-
isfy both the above requirements, and vertices of this
kind get a special no-group label. Again, by weight-
ing the group-aﬃliation vote of each dendrogram by its
likelihood, we may measure exactly the average proba-
bility that a node belongs to its native group’s subtree.

Second, we can measure the average probability that
an edge exists, by taking the likelihood-weighted aver-
age over the sequence of parameters θi associated with
that edge at equilibrium.

Estimating these vertex and edge characteristics al-
lows us to annotate the network, highlighting the most
plausible features, or the most surprising. Figures 7
and 8 show such annotations for the two example net-
works, where edge thickness is proportional to average
probability, and nodes are shaded proportional to the
sampled weight of their native group aﬃliation (light-
est corresponds to highest probability).

For the karate network, the dendrogram sampling both
conﬁrms our previous understanding of the network
as being composed of two loosely connected groups,
and adds additional information. For instance, node

22

18

8

4

13

6

7

11

17

1

5

12

14

2

28

20

3

10

31

25

26

24

30

32

34

27

15

16

19

9

33

21

29

23

Figure 7. An annotated version of the karate club network.
Line thickness for edges is proportional to their average
probability of existing, sampled at equilibrium. Vertices
have shapes corresponding to their known group associa-
tions, and are shaded according to the sampled weight of
their being correctly grouped (see text).

{

}

25, 26

are found to be more loosely
17 and the pair
bound to their respective groups than other vertices –
a feature that is supported by the average hierarchical
structure shown in Figure 5 (bottom). This looseness
apparently arises because none of these vertices has a
direct connection to the central players 1 and 34, and
they are thus connected only secondarily to the cores
of their clusters. Also, our method correctly places

Structural Inference of Hierarchies in Networks

(

1

1

2

)

(

(

9

(

4

7

6

4

)

3

(

3
6

)

)

A

(

9

E

L

)

o

M

L

2

a

B

)

s

i

u

i

d

C
e
n

(

i

C

t

s

4

r

8

m

n

i

C

i

a

a

(

)

i

c

r

n

t

F

l

T
N
S

o
r

n

i

o

a

t

i

o

g

n

T

u

i

h

n

l

L

a

t

d
a

n

a

e

(
5
9
)

L
o
u

(
5
8
)
L
o
u

i

s

i

i

s

i

a
n
M
o
n
r

a
n
T
e
c
h

)
0
(
 
g
n
u
o
Y
m
a
h
g
i
r
B

)
9
(
 
t
a
t
S
o
g
e
i
D
n
a
S

)
6
1
(
 
g
n
i
m
o
y
W

)
4
(
 
o
c
i
x
e
M
w
e
N

)
3
2
(
 
h
a
t
U

(

7

5

)

S

o

8

H

(

5

(

6

u

(

t

9

)

6

6

h

)

1

e

)

7

)

L

u

s

a

a

M

r

A

l

t

t

a

f

e

n

r

a

o

i

m

N

o

o
t
r

u

i
s

e

e

m

M

p

i

n

n

m

e

s

y

(
8

2
)

(
5
)
T

(8

4)

(

3

)

(

8

(

7

1

)

4

T

)

(

1

(

7

0

(

K

(

a

5

N

n

2

)

e

e

x

(

9
8

a

)

2

2

1

s

(

4

(

8

)

I

)

0

a

o

M

)

s

0

)

(

5

4

)

O

)

T

C

w

s

i

B

n

S

a

t

s

o

a

s

y

a

a

K

b

a

r

a

T
e
x
a
s

s
A
&
M

t
a
t
e

s

k

a

s

e

k

l

l

o

S

o

l

t

(
1

0

0
)

(

1

M

0

)
8
5
(
 
h
c
e
T
n
a
i
s
i
u
o
L

)
9
5
(
 
r
n
o
M
n
a
i
s
i
u
o
L

)
7
9
(
 
f
a
L
a
n
a
i
s
i
u
o
L

)
3
6
(
 
e
t
a
t
S
N
T
d
i
M

(
1
0
7
)

O
K
S

(
8
2
)

N
o
t
r
e
D
a
m
e

)
1
(
 
e
t
a
t
S
a
d
i
r
o
l
F

7)
h (3
)
5
9)
3)
2
c
a (8
ginia (3
e (
e
T
5)
t
arolin
orgia
a
e (4
t
S
C
Vir
k
e
C
N
u
G
o
D
N

h

s

1

8 )

1 )

4)

1)

o
d

)
4
0
1
Stat (4
s (
3)
Stat (2
a
NorthTexas (11)
e (9
g
9)
e
BoiseState (28)
State (6
V
c
s
Oreg o n State (108)
s
r
olora
a
a
UtahState (90)
o
L
s
F
Idaho (50)
n
A riz o n a State (8)
V
Air
a
N
M
Ark
C
N
n a (2 2)
alif o r n ia ( 1
S t a t e  ( 7
A riz o
1 )
A  ( 2
8 )
C
W a
n  ( 6
)
L
7
7
C
o
d   (
g
U
r n C a l  
e
r
o
O r
f
n
a
S o u t h e
S t
W a s h i n g t o n  
( 1 1 4 )
 
i
H a w a i
N e v a d a   ( 6 7 )
T e x a s E l P a s o   ( 8 3 )
FresnoState (46)
TXChristian (110)
Tulsa (88)
SanJoseState (73)
Rice (49)
SouthernMeth (53)
FloridaState (1)
WakeForest (105)
Maryland (109)
Clemson (103)
State (25)
uke (45)
Virginia (3

( 7 )
( 5 1 )

N

C

D

e

o

o

G

C

N

W

t
l
i

s
y
a
a
k
e
s
rid
c
e
n
ntu
s
a
7)Flo
s
3)Ark
e
e
n
6)K
n
(2
e
T
(5
)
6
7
(

a
n
i
l
o
r
a
C
o
S
)
0
7
(

i
p
p
i
s
s
i
s
s
i
M
)
7
8
(

n
r
u
b
u
A
)
7
1
(

a
i
g
r
o
e
G
)
5
9
(

b
r
e
d
n
a
V
)
2
6
(

C

e

a

N

e

s
t
e

T

a

B

o

n
t
r

r

t

h

a
l

l
l

E

o

A

a

S

M

e

l

e

r
gia
r
olin

T

r

n

M

i
c

i
c

3)
h (3
9)

e

c

a (8
 (

h

1

4
)

8
)

7)

O

K

B

k

s

u

r

t

t

r

d

a

n

h

f

o

e

o

M

B

e

a

r

o
w

i

f

r

n

n

o

a

n

t

 

l

 

(

s

 

(

o

1

t

e

Ill (

 
(

 
(

8

2

1

h

 (

3

M

5

i

)

6

)

2

)

l

i

h

5

(

7

 

(

8

c

i

n

a

4

)

1

3

)

l

)

4

)

h

 

(

4

O
h

g
G

l

 

(

3

)

M

i

a
m

L
o
u
s

i

i

M
s
s
S

C
o
n
n
e
c

i

a
n
S

t
a
t
e

t

i

c
u

i

o

 

(

9

r

e

9

)

t

 

(

6
1

e

n

 

)

(

 
(
6
5
)

4
2

)

3

1

)

t
a
t
 
(
9
6
)

h

m

s

D

v

e

m

e

a

O

h

T

c

e

a

s

x

h

o

a

o

w

&

M

i
s

ill

(1
0
kla
2)
(4
Mis
0)C
(72)Io
s
olora
(81)TexasA
uri
aState
d
(107)OKState
o
(98)Texas
(10)Baylor
(3)KansasState
(52)Kansas
(74)Nebraska
(15)Wisconsin
(6)PennState
(64)Illinois
( 1 0 0 ) M i c h i g a n S t a t
( 6 0 ) M i n n e s o t a
I n d i a n a
( 1 0 6 )
t h w e s t e r n
( 4 7 ) O h i o S t a t e
( 1 3 ) N o r
) I o w a
2
a
(
h i g
) M i c
d
r
u
9 ) P
u t g
( 3
4 ) R
(8 0) N a v y
e
9 ) T
( 9
(55)Pitts b urg h
o st o
(35)Syracuse
( 7
(30)W estVirginia
(19)VirginiaTech
(2 9) B
(101)MiamiFlorida
(20)Alabama

n
e
e r s
p le
oll
m
C

1
(1

n

(

3

2

u

i
c

6

)I

h
i
g

(
6

n

T

d

s

o

x

a

t

u

o

e

r

a

h

a

r

a

r

t

i

d

e

m

o

e

i
a

a

c

n

h

a

(6

(4

0)
7)O
(3

M

4

a

n

n

e

)Illin
in
hio
ota
State
urd
u

s

S
t
a
ois
t

e

9)P

(32)Michigan
(15)Wisconsin
(13)Northwestern
(6)PennState
(2)Iowa
(94)Rutgers
(79)Temple
(29)BostonColl
(80)Navy
(101)MiamiFlorida
(55)Pittsburgh
( 3 5 ) S y r a c u s e
( 3 0 ) W e s t V i r g i n i a
( 1 9 ) V i r g i n i a T e c h
( 7 7 ) S t a n f o r d
( 6 8 ) O r e g o n
e
t
a
s h S t
) U C L A
) W a
S t a t e
1
S t a t e
(
n
o
g t o
g
a
e
n
8 ) O r
h i n
o
(1 1 1) C alifornia
( 8 ) A ri z
u t h e r n
(22)A rizo n a
s
(104)N V LasVegas
1 ) W
(93)AirForce
(41)ColoradoStat
( 5
(23)Utah
(7) S
g
Stat
min
o
g
xic
e
n
t
(16)Wyo
o
u
a
e
g
o
t
S
M
Die
Y
h
w
m
a
e
n
a
t
(4)N
U
a
h
(9)S
)
rig
0
9
(0)B
(

al

( 1

8

7

o

a

2

n

(

C

0

S

)

2

1

1

5 )

8 )
6 )

s ( 7

WakeForest (105)
Clemson (103)
Maryland (109)
Louisville (57)
E a stC arolin a (4 4)
M e m p his (66)
n ati (9 2)
M is
e r n
C in cin
1 )
u t h
y  ( 9
n  ( 4
o
a m   (
A r m
s t o
e  ( 8
u
h
g
o
n
A L B i r m i n
2 )
H
u l a
( 6
 
t
T
r b i l
V a n d e
( 9 5 )
G e o r g i a  
( 1 7 )
A u b u r n  
A l a b a m a   ( 2 0 )
F l o r i d a   ( 2 7 )
Kentucky (56)
MissState (65)
SoCarolina (70)
Tennessee (76)
Mississippi (87)
LouisianStat (96)
Arkansas (113)
Akron (18)
NorthernIll (12)
WesternMich (14)
BallState (26)
entralMich (38)
a
stern
ole
o (8
lin
a
l
o

Mic
5)

h (4

C

E

T

d

o

g

B

u

w

G

e

B

o

C

C

K

3)

ff
n
t
F
l
o

e

 (

3

r

e

e

ri

4
)

d

M

e

n

O

i

n

n

M

h

a

F

R

r

a

i

m

S

i

e

r

o

i

o

c

s

s

O

 

(

t
 
(

5

c

ti

4

c

)

u

a

 (

n (3

1)

N

S

e

a

v

n

u

e

n

h

7

h

 

a

1

i

t

(

o

a

h

4

J

l

l

S

 

e

9

(

)

o

4

 
(

6

2

)

d

a

o

s

r

)

9

t

a

 

n

l

e

(

t

9

e

)

1

)

t (

3

6
)

o
h
a
d
I
)
0
5
(

e
t
a
t
S
M
N
)
9
6
(

s
a
x
e
T
h
t
r
o
N
)
1
1
(

e
t
a
t
S
e
s
i
o
B
)
8
2
(

t
a
t
S
s
a
s
n
a
k
r
A
)
4
2
(

H
a
w
a

T
X
C
h
r
i

T
u

T
e
x
a
s
E

l

s
a

 

(

s
t
i

8
8

)

i
i
 
(
1
1
4
)

a
n

 
(
1
1
0
)

S

t

6

M

7

e

)

t

P
a
s
o

a

t

e

 

(

4

6

)

h

 

(

5

3

)

 

(

 

(

8
3

7

3

)

)

(a)

(b)

Figure 6. The NCAA Schedule 2000 network: (a) an exemplar maximum likelihood dendrogram with log L = −884.2,
parameters θi are shown as gray-scale values, and leaf shapes denote conference aﬃliation; and (b) the consensus hierarchy
sampled at equilibrium. Leaf shapes are common between (a) and (b), but position varies.

vertex 3 in the cluster surrounding 1, a placement with
which many other methods have diﬃculty.

The NCAA network shows similarly suggestive results,
with the majority of heavily weighted edges falling
within conferences. Most nodes are strongly placed
within their native groups, with a few notable excep-
tions, such as the independent colleges, vertices 82,
80, 42, 90, and 36, which belong to none of the ma-
jor conferences. These teams are typically placed by
our method in the conference in which they played the
most games. Although these annotations illustrate in-
teresting aspects of the NCAA network’s structure, we
leave a thorough analysis of the data for future work.

8. Discussion and conclusions

As mentioned in the introduction, we are not the ﬁrst
to study hierarchy in networks. In addition to persis-
tent interest in the sociology community, a number of
authors in physics have recently discussed aspects of
hierarchical structure [7, 16, 6, 17], although generally
via indirect or heuristic means. A closely related, and
much studied, concept is that of community structure
in networks [7, 15, 13, 2]. In community structure cal-
culations one attempts to ﬁnd a natural partition of
the network that yields densely connected subgraphs

or communities. Many algorithms for detecting com-
munity structure iteratively divide (or agglomerate)
groups of vertices to produce a reasonable partition;
the sequence of such divisions (or agglomerations) can
then be represented as a dendrogram that is often con-
sidered to encode some structure of the graph itself.
(Notably, a very recent exception among these com-
munity detection heuristics is a method based on max-
imum likelihood and survey propagation [9].)

Unfortunately, while these algorithms often produce
reasonable looking dendrograms, they have the same
fundamental problems as traditional hierarchical clus-
tering algorithms for numeric data [8]. That is, it is
not clear to what extent the derived hierarchical struc-
tures depend on the details of the algorithms used to
extract them. It is also unclear how sensitive they are
to small perturbations in the graph, such as the ad-
dition or removal of a few edges. Further, these algo-
rithms typically produce only a single dendrogram and
provide no estimate of the form or number of plausible
alternative structures.

In contrast to this previous work, our method directly
addresses these problems by explicitly ﬁtting a hier-
archical structure to the topology of the graph. We
precisely deﬁne a general notion of hierarchical struc-
ture that is algorithm-independent and we use this

Structural Inference of Hierarchies in Networks

49

53

46

83

114

88

67

73

110

57

44

66

91

112

86

92

103

37

89

25

33

105

109

93

9

16

1

45

4

0

23

104

41

7

8

68

21

22

78

77

5

10

3

52

84

81

98

40

74

107

72

102

100

51

60

111

108

2

6

47

13

106

64

32

39

15

97

59

36

42

63

90

80

19

24

82

94

28

50

58

11

69

75

48

55

29

35

101

79

30

76

113

17

70

43

26

95

62

27

96

65

20

87

56

34

61

18

31

99

54

71

85

38

14

12

Figure 8. An annotated version of the college football schedule network. Annotations are as in Figure 7. Note that node
shapes here diﬀer from those in Figure 6, but numerical indices remain the same.

deﬁnition to develop a random graph model of a hi-
erarchically structured network that we use in a sta-
tistical inference context. By sampling via MCMC
the set of dendrogram models that are most likely
to generate the observed data, we estimate the pos-
terior distribution over models and, through a scheme
akin to Bayesian model averaging, infer a set of fea-
tures that represent the general organization of the
network. This approach provides a mathematically
principled way to learning about hierarchical organi-
zation in real-world graphs. Compared to the previous
methods, our approach yields considerable advantages,
although at the expense of being more computation-
ally intensive. For smaller graphs, however, for which
the calculations described here are tractable, we be-
lieve that the insight provided by our methods makes
the extra computational eﬀort very worthwhile. In fu-

ture work, we will explore the extension of our meth-
ods to larger networks and characterize the errors the
technique can produce.

In closing, we note that the method of dendrogram
sampling is quite general and could, in principle, be
used to annotate any number of other graph features
with information gained by model averaging. We be-
lieve that the ability to show which network features
are surprising under our model and which are com-
mon is genuinely novel and may lead to a better un-
derstanding of the inherently stochastic processes that
generate much of the network data currently being an-
alyzed by the research community.

Structural Inference of Hierarchies in Networks

[13] Newman, M. E. J. (2004). Detecting community
structure in networks. Eur. Phys. J. B, 38, 321–320.

[14] Newman, M. E. J., & Barkema, G. T. (1999).
Monte carlo methods in statistical physics. Oxford:
Clarendon Press.

[15] Radicchi, F., Castellano, C., Cecconi, F., Loreto,
V., & Parisi, D. (2004). Deﬁning and identifying
communities in networks. Proc. Natl. Acad. Sci.
USA, 101, 2658–2663.

[16] Ravasz, E., Somera, A. L., Mongru, D. A., Olt-
vai, Z. N., & Barab´asi, A.-L. (2002). Hierarchical
organization of modularity in metabolic networks.
Science, 30, 1551–1555.

[17] Sales-Pardo, M., Guimer´a, R., Moreira, A. A.,
& Amaral, L. A. N. (2006). Extracting and rep-
resenting the hierarchical organization of complex
systems. Unpublished manuscript.

[18] S¨oderberg, B. (2002). General formalism for in-
homogeneous random graphs. Phys. Rev. E, 66,
066121.

[19] Wasserman, S., & Faust, K. (1994). Social net-
work analysis. Cambridge: Cambridge University
Press.

[20] Wasserman, S., & Robins, G. L. (2005). An in-
troduction to random graphs, dependence graphs,
and p∗. In P. Carrington, J. Scott and S. Wasser-
man (Eds.), Models and methods in social network
analysis. Cambridge University Press.

[21] Watts, D. J., & Strogatz, S. H. (1998). Collective
dynamics of ‘small-world’ networks. Nature, 393,
440–442.

[22] Zachary, W. W. (1977). An information ﬂow
model for conﬂict and ﬁssion in small groups. Jour-
nal of Anthropological Research, 33, 452–473.

Acknowledgments

AC thanks Cosma Shalizi and Terran Lane for many
stimulating discussions about statistical inference, and
Mason Porter for discussions about hierarchy. MEJN
thanks Michael Gastner for work on an early ver-
sion of the model. This work was funded in part by
the National Science Foundation under grants PHY–
0200909 (AC and CM) and DMS–0405348 (MEJN)
and by a grant from the James S. McDonnell Foun-
dation (MEJN).

References

[1] Albert, R., & Barab´asi, A.-L. (2002). Statistical
mechanics of complex networks. Rev. Mod. Phys.,
74, 47–97.

[2] Bansal, N., Blum, A., & Chawla, S. (2004). Corre-
lation clustering. ACM Machine Learning, 56, 89–
113.

[3] Barab´asi, A.-L., & Albert, R. (1999). Emergence of
scaling in random networks. Science, 286, 509–512.

[4] Bryant, D. (2003). A classiﬁcation of consensus
methods for phylogenies. In M. Janowitz, F.-J. La-
pointe, F. R. McMorris, B. Mirkin and F. Roberts
(Eds.), Bioconsensus, 163–184. DIMACS.

[5] Casella, G., & Berger, R. L. (1990). Statistical

inference. Belmont: Duxbury Press.

[6] Clauset, A., Newman, M. E. J., & Moore, C.
(2004). Finding community structure in very large
networks. Phys. Rev. E, 70, 066111.

[7] Girvan, M., & Newman, M. E. J. (2002). Com-
munity structure in social and biological networks.
Proc. Natl. Acad. Sci. USA, 99, 7821–7826.

[8] Hastie, T., Tibshirani, R., & Friedman, J. (2001).
learning. New York:

The elements of statistical
Springer.

[9] Hastings, M. B. (2006). Community detection as
an inference problem. Preprint cond-mat/0604429.

[10] Kleinberg, J. (2000).

The small-world phe-
nomenon: an algorithmic perspective. 32nd ACM
Symposium on Theory of Computing.

[11] Newman, M. E. J. (2002). Assortative mixing in

networks. Phys. Rev. Lett., 89, 208701.

[12] Newman, M. E. J. (2003). The structure and
function of complex networks. SIAM Review, 45,
167–256.


Memory-Based Boolean game and self-organized phenomena on networks

HUANG Zi-Gang, WU Zhi-Xi, GUAN Jian-Yue, and WANG Ying-Hai∗

Institute of Theoretical Physics, Lanzhou University, Lanzhou Gansu 730000, China

(Dated: February 15, 2014)

Abstract

We study a memory-based Boolean game (MBBG) taking place on the regular ring, wherein each agent

acts according to its local optimal states of the last M time steps recorded in memory, and the agents in

the minority are rewarded. One free parameter p among 0 and 1 is introduced to denote the strength of the

agents’ willing to make a decision according to its memory. We ﬁnd that, given proper willing strength p,

the MBBG system can spontaneously evolve to a state of better performance than the random game; while

for larger p, the herd behavior emerges which reduces the system proﬁt. By analyzing the dependence of the

system’s dynamics on the memory capacity M , we ﬁnd that a higher memory capacity favors the emergence

of the better performance state, and effectively restrains the herd behavior, therefore increases the system

proﬁt. Considering the high cost of long-time memory, the enhancement of memory capacity for restraining

the herd behavior is also discussed, and the M = 5 is suggested to be one good choice.

PACS numbers: 89.75.Hc, 87.23.Kg, 02.50.Le, 87.23.Ge

7
0
0
2
 
n
a
J
 
9
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
2
0
1
1
0
7
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

∗Electronic address: yhwang@lzu.edu.cn

1

Dynamical systems with many elements under mutual regulation or inﬂuence, such as the sys-

tems naturally arise in biology [1] and in the social sciences [2], underlie much of the phenomena

associated with complexity. The perspective of complex adaptive systems (CAS) composed of

agents under mutual inﬂuence have been proposed for understanding the rich and complex dy-

namics of these real-life systems [1, 3, 4, 5].

One of the simplest examples of a complex dynamical system is the minority game [6] (MG)

introduced by Challet and Zhang as a simpliﬁcation of Arthur’s El Farol Bar attendance problem

[7]. Agents in the MG are designed to make choice (1 or 0, i.e. to attend a bar or refrain) based

on the aggregate signal (the global information in memory), i.e., which value was in the majority

for the last several time steps. The agents in the minority are rewarded, and those in the majority

punished since resources are limited. The MG model can serve as a general paradigm for resource

allocation and load balancing in multiagent systems and was study extensively [8, 9, 10, 11].

In contrast to this mean-ﬁeld description of the MG, the Boolean game (BG) on the network of

interconnections between the agents was introduced in Ref. [12] considering that the agent can

also respond to the detailed information it receives from other speciﬁed agents. It was established

that coordination still arises out of local interactions in the BG, and the system as a whole achieves

“better than random” performance in terms of the utilization of resources [13, 14, 15, 16]. This

contributes to the solution of one basic question in studies of complexity, that is, how large systems

with only local information available to the agents may become complex through a self-organized

dynamical process [12].

Many real-life systems often seem a black box to us: the outcome may be observed, but the

underlying mechanism is not visible. Herd behavior, which describes the condition that many

agents display the same action, is one of the outcomes always present in ecosystems while the

corresponding mechanisms are unaware. The herd behavior has been extensively studied in Be-

havioral Finance and is found to be one factor of the origins of complexity that may enhance the

ﬂuctuation and reduce the system proﬁt [18, 19, 20, 21, 22]. Also, the underlying mechanism of

the herd behavior is an interesting issue which has attracted economists’ and physicists’ interests.

Considering that herd behavior still occur although the agents prefer to be in the minority in some

real-life cases, one should seek the mechanism of the herd behavior from some other aspects rather

than the agents’ willing to be in majority [15].

In the previous studies of the BG, each agent acts according to the Boolean function, i.e., gets

its input from some other agents, and maps the input to a state it will adopt in the subsequent

2

round [13, 14]. Inspired by the MG, we argue that the agents should make decisions based on the

knowledge of the past records, and the historical memory of individuals plays a key role in the

evolutionary games. In the present work, we study a memory-based Boolean game (MBBG) in

which each agent modiﬁes its state based on its past experiences gained from the local interaction

with neighbors, and the agents in the minority of the whole system are rewarded.

The global information is not available, and the agents also do not know who are winners

in the previous rounds. They can only make use of the local information and gain experiences

from local interaction. It is worthwhile to emphasize that the agent’s ignorance of who are global

winners is one of the main differences from the previous studies on MG. Due to the lack of the

global information, each agent in our model attempts to be in the minority in its own small region

which consists of its immediate neighbors and itself, considering that there should exist positive

correlation between being the minority in the whole system and in its own local region. We call

this “local optimal assumption” (LOA) of the agent system. Then, our model can be depicted

as: in the lack of the global information and in the belief of the LOA, the agent pins its hope for

winning in the whole system on the effort to act as minority in its own region based on the local

experiences stored in memory.

Let us introduce the rules of the evolutionary MBBG. To simplify, each agent is conﬁned to

a site of a regular network which is a one-dimensional lattice with periodic boundary conditions

and coordination number z = 3 [23]. A local region for each agent thus contain 7 agents. When a

round of game is over, each agent will have the state information (+1 or −1) of its neighbors. Then

the agents are designed to know its local optimal state (LOS) in the past round by means of self-

questioning, i.e., each agent adopts its anti-state to play a virtual game with all its neighbors, and

calculates the virtual result. Comparing the virtual result with the actual one, each agent gets its

LOS which may turn it into minority of its own local region. In condition that the counterbalance

of the groups with +1 and −1 appears in one agent’s neighbors, its optimal state is randomly set

either as +1 or −1, because whichever state the agent chooses, it will break the counterbalance

and compel its own side into majority. Then, the agent records the LOS into memory. Taking into

account the bounded capacity of the agents, we assume that the agents are quite limited in their

power and can only retain the last M LOS in memory. We would like to reiterate that the so called

“local optimal state” does not mean the agent will be rewarded if has adopted it. Only the agents

in the global minority are rewarded by 1, and therefore the system proﬁt equals to the number of

agents in the global minority. This is a main difference of our model from the Local Minority

3

 ran

 1

 2

 3

 5

 10

 15

 20

4

10

3

10

2

10

 

P

 

0.0

0.2

0.4

0.6

0.8

1.0

FIG. 1: The variance of the number of agents choosing +1 as a function of willing strength p with several

different memory capacities on the regular ring of size 2001. The solid line represents the system proﬁt

of the random choice game which corresponds to the M = 0 case. The system performs better than the

random game when p is less than the intersection point pM

inter.

Game [24].

There might be variability of the agents’ belief of the LOA and the willing to make decision

based on records in memory. We deﬁne the willing strength p to add this effect into our model.

That in detail is, at each time step, each agent acts based on its memory at probability p, or acts all

by himself at probability 1 − p. In the former case, the probability of making a decision (choosing

+1 or −1) for each agent depends on the ratio of the numbers of +1 and −1 stored in its memory,

i.e., the agent chooses +1 with the probability P (+) = n+/(n+ + n−) = n+/M and −1 with the

probability P (−) = 1 − P (+), where n+ and n− are the numbers of +1 and −1, respectively.

In the latter case, the agent simply inherits its action in the last time step or chooses the opposite

action at a small probability m, named the mutation probability. Following the previous work [15],

we set it to be 0.01. The introduction of m adds several impulsive and unstable ingredients to our

model in view of the presence of the irrational effect .

Simulations are carried out for a population of N = 2001 agents located on network sites. The
time length T = 104 are ﬁxed. In the initial state, +1 and −1 are uniformly distributed among all

the agents, and the memory information of each agent is randomly assigned. We have checked that

this assignment has no contributions to the evolutionary behavior of the system. All the simulation

results presented in this paper are average of 50 randomly assigned initial states.

The variance of systems σ2 = (1/T ) PT

t=1(At − N/2)2 [15, 16] which is the cumulative

4

standard deviation from the optimal resource utilization over time length T , can be considered as a
global measure of the system’s optimality. The smaller σ2 corresponds to better optimality of the

system and the more system proﬁt. Here, At denotes the number of agents choosing +1 at time
t. The simulation results σ2 as a function of p with different memory capacity M are presented in

Fig. 1. The result of the random game which is same as the MBBG with M = 0 is also plotted for

comparison. In the random choice game, At does not depend on the previous states of the system,

and its expectation is always hAti = N/2. The distribution of At has a Gaussian proﬁle with
the variance to the expectation N/2 as σ2 = 0.25N in the limit of large N. For the MBBG with

M 6= 0, it is noticeable that these systems can perform better than that with random choice game
when p is in a certain interval (see Fig. 1 the interval where σ2 < 0.25N). This is an evidence of

the existence of a self-organized process in the systems. At larger p, the herd behaviors occur and
the subsequent oscillations cause the greater variances σ2 than that of the random choice game.
The intersection points of the curves of the MBBG and that of the random game (at p = pM

inter,

M = 1, 2, ..., 20) denote the same system performance of them.

Let us ﬁrstly consider the extreme case p = 0 which means that the agents act all by themselves

without considering the historical memory. In this case, each agent merely changes its action with

the mutation probability m, and there is no preferential choice for +1 and −1 so that no herd

behaviors occur. Following Ref. [15], the expectation of At+1 is,

hAt+1i = At(1 − m) + m(N − At) = At + m(N − 2At).

(1)

Assuming At > N/2, for 0 < m < 1/2, we have At > At+1 > N/2. Thus, if large event has

taken place initially in the system (e.g., At=0 ≫ N/2, or At=0 ≪ N/2), the effort of m will make

At slowly revert to the equilibrium position N/2. It is easy to prove that even when the mutation

probability m is very small, the system proﬁt will be equal to random choice game on condition

that the evolutionary time T is sufﬁciently long. The simulation results for p = 0 (see Fig. 1) are

in well agreement with the our analysis.

The other extreme case is p = 1 where the herd behavior prevails. Comparing this case to

the p = 0 case, we can say that the occurrence of the herd behavior is intimately related to the

mechanism of the memory-based actions.

In this case, if the agents choosing +1 and −1 are

equally mixed up in the networks, then the number of agents who record +1 as the LOS by self-
questioning (denoted by Sopt

+1,t) has the expectation

hSopt

+1,ti

.
= N − At.

5

(2)

Thus, all over the system the collection of the agents’ newly recorded LOS is close to the anti-state

of the present system. For the system with small memory capacity M, e.g. M = 1, the agents’

new states for the subsequent round t + 1 gained from the records in memory are actually their

optimal states of the latest round, and thus the expectation of At+1 is

hAt+1i = hSopt

+1,ti

.
= N − At,

(3)

with departure |hAt+1i − N/2|

.
= |At − N/2|. One can see that the departure from N/2 does

not reduce in average, while the state of the winning side reverses. Therefore, the prevalence of

the herd behavior which is denoted by the large durative oscillation will occur when p = 1 and

M = 1. On the other hand, for the system with larger values of the memory capacity M, the

agents have also stored more previous information in memory besides the latest LOS. Based on

more information their state updates will not be so intense and irrational as that with M = 1. As

a result, the behavior of the systems are mended by the rationality of their agents. It is clear in

Fig. 1 that the oscillation of the system with larger M is less acute than that with smaller M in the

p = 1 case. Furthermore, in the cases of p ∈ (pM

inter, 1), it can also be found that the high memory

capacity of the agent can effectively restrain the herd behavior and thus increase the system proﬁt.

The existence of the self-organization demonstrated in Fig. 1 can be understood by the dynam-

ics of the system in the mentioned two extreme cases: The action of the agent based on memory

with probability p will induce oscillation, while the independent mechanism with probability 1 − p

will lead to a long reversion process to the equilibrium position N/2. Thus at a proper value of

p, the system can quickly arrive at the equilibrium position after the occurrence of a large event,

which leads to more system proﬁt than the random game. Also, we can see that the underlying

mechanism of the herd behavior is related to the strength of the agents’ willing of making decision

based on the historical memory.

In the following, we discuss the effect of the memory capacity M to the behavior of the system

in detail from two points, the intersection pM

inter and the corresponding transition rate, which will

be deﬁned in the following.

We have known that, the MBBG system can perform either better or worse than the random

game when the value of p is smaller or larger than pM

inter. The case of the better performance is due

to the emergence of the agents’ self-organization, and the case of the worse performance is due to

the prevalence of the herd behavior. The relation between pM

inter and the memory capacity M are

plotted in Fig. 2. It is revealed that, the region (0, pM

inter) where system performs better than random

6

t

i

n
o
P
n
o

 

i
t
c
e
s
r
e
n

t

I

1.0

0.9

0.8

0.7

0.6

 

 

0.1

0.01

t

n
e
m
a
e
r
c
n

I

1E-3

 

 

0

10

20

30

40

1

10

M

M

FIG. 2: The intersection point pM

inter of the random game and the MBBG with different memory capacity

M , for regular ring with N = 2001. The inset is the log-log plot of the increments of pM

inter as a function

of memory capacity M .

game broadens with the memory capacity M. That is to say, the system with larger memory

capacity has more probability of self-organizing to the better-performance case. In addition, the

inset in Fig. 2 presents the increments of the intersection point ∆pM

inter when M increases by one

(i.e. ∆pM

inter = pM +1

inter − pM

inter) as a function of M. This measure corresponds to the “marginal

return” in economics. We can see that, when the memory capacity is large, the increment is small.

The scaling behavior at large M implies that pM

inter is arriving at a level number close to 1. It is

remarkable that the behavior of the ∆pM

inter with M is not monotonic. There exists the special point

at M = 4 which implies that the pM

inter with M = 5 is larger comparing to the value estimated

from the trend exhibited from all the other values of M.

Inspired by the fact that in many situations the agents have to operate in dynamic (and in

general, stochastic) environments, we can imagine that, due to some external impacts the willing

strength p of the agents in our model may be not ﬁxed, but vary with time.

In the case that

p ﬂuctuates around pM

inter, there exists the transition from the case of the better performance to

the case where the herd behavior seriously impacts the system proﬁt. Let us now focus on the

rate of the transition between the two cases when p is ﬂuctuating. For convenience, we call this

rate the “transition rate” which is different from its traditional meaning in the study of the phase

transition. It is noticeable in Fig 1 that, at the intersection point pM

inter, different memory lengthes

M correspond to different values of slope. We study the relation between the slope at pM

inter (i.e.

the transition rate) and the memory capacity M (see Fig. 3). It can easily be found that, the shorter

7

 

e
p
o
s

l

5000

4000

2000

1000

0

3000

- 0.893

-1.813

 

3

10

e
p
o
S

l

2

10

1

10

M

0

5

10

15

20

M

FIG. 3: The memory capacity M and the slope of the variance at the intersection point pM

inter. The inset is

the corresponding log-log plot, where the crossover at M = 5 is obvious.

the memory is, the rapider the transition from the two cases would be. One can also consider the

transition rate as a measure of the system’s risk of suffering from the herd behavior. The results in

Fig. 3 thus is the dependence between the system’s risk and the memory capacity M. It is clear

that, those systems with higher memory capacity can constrain the occurrence of the herd behavior

more efﬁciently.

On the other hand, if the question facing us is to bring down the system’s risk or to design a

system with low risk of suffering from the herd behavior, enhancing the agents’ memory capacity

is indeed an effective way. However, the enhancement of the memory capacity in real-life cases

would cost much. In this context, it is necessary to discuss how large memory capacity would

be proper.

Interestingly, we ﬁnd that the dependence of the slope on M approximately obeys

scaling laws with two exponents (the inset in Fig. 3). That is, for small M (M ≤ 5) the scaling

exponent is about −1.813, after which, at larger M, there is a crossover to −0.893. This behavior

implies that, when the memory length M is already 5 or larger, if increase M, the risk reduces

slower than the small M cases. Also it is obvious that the value of the transition rate at M = 5

is already small. When M > 5 the effort to increase memory capacity which costs much can not

gain good mark in reducing system crisis. Further simulation results show that the two-exponent

scaling behavior divided by M = 5 holds for different system size N, coordination number z and

mutation probability m. Moreover, in the previous intersection analysis we have proved that the

pM
inter at M = 5 is comparatively large. Thus we argue that, M = 5 may be a good choice to

improve the performance of the system.

8

In summary, inspired by the minority game, we studied a memory-based Boolean Game on

regular ring. The simulation results with various memory capacity M are discussed. We found

that, those systems with nonzero M can perform better than that of the random choice game

when willing strength p is in a certain interval. This is reasonable evidence of the existence of

a self-organized process taking place within the agent system, although only local information

is available to the agents. The memory capacity M are found to have remarkable effect on the

agent system. That is, the larger the M is, the more probably the self-organized process would

emerge since the value of pM

inter increases. Moreover, larger memory capacity corresponds to

smaller degree of the herd behavior at large p, and less risk of the system suffering from the herd

behavior when p ﬂuctuates around pM

inter. In addition, we propose the question of designing the

system which is robust to the impact of the herd behavior, and the choice of M is also discussed

considering the high cost of enhancing M in real-life cases.

We thank Dr. Xin-Jian Xu for helpful discussions and valuable suggestions. This work was

supported by the Fundamental Research Fund for Physics and Mathematics of Lanzhou University

under Grant No. Lzu05008.

[1] Kauffman S A 1993 The Origins of Order (New York: Oxford University Press)

[2] Arthur W B 1999 Science 284 107

[3] Levin S A 1998 Ecosystems 1 431

City, CA: Addison-Wesley)

[4] Anderson P W, Arrow K and Pines D 1988 The Economy as an Evolving Complex System (Redwood

[5] Yang C X, Zhou T, Zhou P L, Liu J and Tang Z N 2005 Chin. Phys. Lett. 22 1014

[6] Challet D and Zhang Y C 1997 Physica A 246 407

[7] Arthur W B 1994 Am. Econ. Assoc. Papers Proc. 84 406

[8] Quan H J, Wang B H, Hui P M and Luo X S 2001 Chin. Phys. Lett. 18 1156

[9] Yang W S, Wang B H, Quan H J and Hu C K 2003 Chin. Phys. Lett. 20 1659

[10] Challet D and Marsili M 1999 Phys. Rev. E 60 R6271

[11] See http://www. unifr. ch/econophysics/minority/ for an extensive collection of papers and references

[12] Paczuski M, Bassler K E and Corral ´A 2000 Phys. Rev. Lett. 84 3185

[13] Galstyan A and Lerman K 2002 Phys. Rev. E 66 015103(R)

9

[14] Yuan B S, Wang B H and Chen K cond-mat/0411664

[15] Zhou T, Wang B H, Zhou P L, Yang C X and Liu J 2005 Phys. Rev. E 72 046139

[16] Ma J, Zhou P L, Zhou T, Bai W J, Cai S M physics/0604066

[17] V´azquez A 2000 Phys. Rev. E 62 4497

[18] Eguluz V M and Zimmermann M G 2000 Phys. Rev. Lett. 85 5659

[19] Xie Y B, Wang B H, Hu B and Zhou T 2005 Phys. Rev. E 71 046135

[20] Lee S and Kim Y 2004 J. Korean Phys. Soc. 44 672

[21] Wang J, Yang C X, Zhou P L, Jin Y D, Zhou T and Wang B H 2005 Physica A 354 505

[22] Zhou P L, Yang C X, Zhou T, Xu M, Liu J and Wang B H 2005 New Mathematics and Natural

Computation 1 275

[23] Newman M E J and Watts D J 1999 Phys. Rev. E 60 7332

[24] Moelbert S and Rios P De L 2002 Physica A 303 217

10


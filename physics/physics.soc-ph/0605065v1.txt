6
0
0
2
 
y
a
M
 
7
 
 
]
h
p
-
c
o
s
.
s
c
i
s
y
h
p
[
 
 
1
v
5
6
0
5
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Predictability, Risk and Online Management in a
Complex System of Adaptive Agents

David M.D. Smith and Neil F. Johnson
Physics Department, Oxford University, Oxford OX1 3PU, U.K.

February 2, 2008

Abstract

We discuss the feasibility of predicting, managing and subsequently
manipulating, the future evolution of a Complex Adaptive System.
Our archetypal system mimics a population of adaptive, interacting
objects, such as those arising in the domains of human health and
biology (e.g. cells), ﬁnancial markets (e.g. traders), and mechanical
systems (e.g. robots). We show that short-term prediction yields corri-
dors along which the model system will, with high probability, evolve.
We show how the widths and average direction of these corridors varies
in time as the system passes through regions, or pockets, of enhanced
predictability and/or risk. We then show how small amounts of ‘pop-
ulation engineering’ can be undertaken in order to steer the system
away from any undesired regimes which have been predicted. Despite
the system’s many degrees of freedom and inherent stochasticity, this
dynamical, ‘soft’ control over future risk requires only minimal knowl-
edge about the underlying composition of the constituent multi-agent
population.

1

Introduction

Complex Adaptive Systems (CAS) are of great theoretical interest because
they comprise large numbers of interacting objects or ‘agents’ which, un-
like particles in traditional physics, change their behaviour based on expe-
rience [1]. Such adaptation yields complicated feedback processes at the
microscopic level, which in turn generate complicated global dynamics at
the macroscopic level. CAS also arguably represent the ‘hard’ problem in
biology, engineering, computation and sociology[1]. Depending on the ap-
plication domain, the agents in CAS may be taken as representing species,
people, cells, computer hardware or software, and are typically quite numer-
ous, e.g. 102 − 103[1, 2].

There is also great practical interest in the problem of predicting and sub-
sequently controlling a Complex Adaptive System. Consider the enormous

1

task facing a Complex Adaptive Systems ‘manager’ in charge of overseeing
some complicated computational, biological, medical, sociological or even
economic system. He would certainly like to be able to predict its future
evolution with suﬃcient accuracy that he could foresee the system head-
ing towards any ‘dangerous’ areas. However, prediction is not enough – he
also needs to be able to steer the system away from this dangerous regime.
Furthermore, the CAS manager needs to be able to achieve this without de-
tailed knowledge of the present state of its thousand diﬀerent components,
nor does he want to have to shut down the system completely. Instead he
is seeking for some form of ‘soft’ control which can be applied ‘online’ while
the system is still evolving.

Such online management of a Complex System therefore represents a
signiﬁcant theoretical and practical challenge. However, the motivation for
pursuing such a goal is equally great given the wide range of important real-
world systems that can be regarded as complex – from engineering systems
through to human health, social systems and even ﬁnancial systems.

In purely deterministic systems with only a few degrees of freedom, it
is well known that highly complex dynamics such as chaos can arise [3]
making any control very diﬃcult. The ‘butterﬂy eﬀect’ whereby small per-
turbations can have huge uncontrollable consequences, comes to mind. One
would think that things would be considerably worse in a CAS, given the
much larger number of interacting objects. As an additional complication, a
CAS may also contain stochastic processes at the microscopic and/or macro-
scopic levels, thereby adding an inherently random element to the system’s
dynamical evolution. The Central Limit Theorem tells us that the com-
bined eﬀect of a large number of stochastic processes tends fairly rapidly to
a Gaussian distribution. Hence, one would think that even with reasonably
complete knowledge of the present and past states of the system, the evo-
lution would be essentially diﬀusive and hence diﬃcult to control without
imposing substantial global constraints.

In this paper, we address this question of dynamical control for a sim-
pliﬁed yet highly non-trivial model of a CAS. We show that a surprising
level of prediction and subsequent control can be achieved by introducing
small perturbations to the agent heterogeneity, i.e. ‘population engineering’.
In particular, the system’s global evolution can be managed and undesired
future scenarios avoided. Despite the many degrees of freedom and inher-
ent stochasticity both at the microscopic and macroscopic levels, this global
control requires only minimal knowledge on the part of the ‘system man-
ager’. For the somewhat simpler case of Cellular Automata, Israeli and
Goldenfeld[4] have recently obtained the remarkable result that computa-
tionally irreducible physical processes can become computationally reducible
at a coarse-grained level of description. Based on our ﬁndings, we specu-
late that similar ideas may hold for a far wider class of system comprising
populations of decision-taking, adaptive agents.

2

It is widely believed (see for example, Ref. [5]) that Arthur’s so-called El
Farol Bar Problem [6, 7] provides a representative toy model of a CAS where
objects, components or individuals compete for some limited global resource
(e.g. space in an overcrowded area). To make this model more complete in
terms of real-world complex systems, the eﬀect of network interconnections
has recently been incorporated [8, 9, 10, 11]. The El Farol Bar Problem
concerns the collective decision-making of a group of potential bar-goers
(i.e. agents) who use limited global information to predict whether they
should attend a potentially overcrowded bar on a given night each week.
The Statistical Mechanics community has adopted a binary version of this
problem, the so-called Minority Game (MG) (see Refs.[12] through [25]), as
a new form of Ising model which is worthy of study in its own right because
of its highly non-trivial dynamics. Here we consider a general version of such
multi-agent games which (a) incorporates a ﬁnite time-horizon H over which
agents remember their strategies’ past successes, to reﬂect the fact that the
more recent past should have more inﬂuence than the distant past, and (b)
allows for ﬂuctuations in agent numbers, since agents only participate if they
possess a strategy with a suﬃciently high success rate[15]. The formalism
we employ is applicable to any CAS which can be mapped onto a population
of N objects repeatedly taking actions in the form of some global ‘game’.

The paper has several parts. Initially (section 2), we discuss a very sim-
ple two state ‘game’ to introduce and familiarise the reader to the nature of
the mathematics which is explored in further detail in the rest of the paper.
We then more formally establish a common framework for describing the
spectrum of future paths of the complex adaptive system (section 3). This
framework is general to any complex system which can be mapped onto a
general B-A-R (Binary Agent Resource) model in which the system’s future
evolution is governed by past history over an arbitrary but ﬁnite time win-
dow H (the T ime − Horizon). In fact, this formalism can be applied to
any CAS whose internal dynamics are governed by a Markov Process[15],
providing the tools whereby we can monitor the future evolution both with
and without the perturbations to the population’s composition.
In sec-
tion 4, we discuss the B-A-R model in more detail, further information is
provided in [26]. We emphasize that such B-A-R systems are not limited to
the well-known El Farol Bar Problem and Minority Games - instead these
two examples are speciﬁc limiting cases.
Initial investigations of a ﬁnite
time-horizon version of the Minority Game were ﬁrst presented in [15]. In
section 5, we consider the system’s evolution in the absence of any such per-
turbations, hence representing the system’s natural evolution. In section 6,
we revisit this evolution in the presence of control, where this control is
limited to relatively minor perturbations at the level of the heterogeneity of
the population. In section 7 we revisit the toy model of section 2 to pro-
vide a reduced form of formalism for generating averaged quantities of the
future possibilities. In section 8 we discuss concluding remarks and possible

3

extensions.

2 A Tale of Two Dice

In this section we examine a very simple toy model employed to generate
a time series (analogue output) and introduce the Future-Cast formalism
to describe the model’s properties. This toy model comprises two internal
states, A and B, and two dice also denoted A and B. We make these
dice generic in that we assign their faces values and these are not equal in
likelihood. The rules of the model are very simple. When the system is in
state A, dice A is rolled and similarly for dice B. The outcome, δt, of the
relevant dice is used to increment a time (price) series, whose update can
be written

St+1 = St + δt

(1)

δ >0

A

B

δ <0

Figure 1: The internal state transitions on the De Bruijn graph.

The model employs a very simple rule to govern the transitions between
its internal states. If the outcome δt is greater than zero (recall that we have
re-assigned the values on the faces) the internal state at time t + 1 is A and
consequently dice A will be used at the next step regardless of the dice used
at this time step. Conversely, if δt < 0, the internal state at t + 1 will be
B and dice B used for the next increment1. These transitions are shown in
ﬁgure 1.

Let us prescribe our dice some values and observe the output of the
system, namely rolling dice A could yield values {−5, + 3, + 8} with
probabilities {0.3, 0.5, 0.2} and B yields values {−10, − 8, + 3} with
probabilities {0.1, 0.5, 0.4}. These are shown in ﬁgure 2. Let us consider
that at some time t the system is in state A with the value of the time series
being S(t) and we wish to investigate the possible output over the next U
time-steps (S(t + U )). Some examples of the system’s change in output
over the next 10 time-steps are shown in ﬁgure 3. The circles represent the
system being in state A and the squares the system in B.

1This state transition rule is not critical to the formalism. For example, the rule could
be to use dice A if the last increment was odd and B is even, or indeed any property of
the outcomes.

δ <0

δ >0

4

P{δ}

( 3 , 0.5 )

P{δ}

( −8 , 0.5 )

( 3 , 0.4 )

( −5 , 0.3 )

( 8 , 0.2 )

( −10 , 0.1 )

Dice A

outcome δ

Dice B

outcome δ

Figure 2: The outcomes and associated probabilities of our two dice.

)
t
(

 

S
−
 
)

U
+
t
(

S

 
;
t

u
p

t

u
O

50

40

30

20

10

0

−10

−20

−30

−40

−50

0

2

4

6

time−steps U

8

10

Figure 3: The toy model’s output over 10 time-steps for 20 realisations
where the state at time t is A for all paths.. The circles represent the
system being in state A and the squares the system in B.

The stochasticity inherent in the system is evident in ﬁgure 3. Many pos-
sible paths could be realised even though they all originate from state A, and
only a few of them are depicted. If one wanted to know more about system’s
output after these 10 time-steps, one could look at many more runs and look
at a histogram of the output for U = 10. This Monte-Carlo[28] technique
has been carried out in ﬁgure 4. It denotes the possible change in value of
the systems analogue output after 10 time-steps and associated probability
derived from 106 runs of the system all with the same initial starting state
(A) at time t. However, this method is a numerical approximation. For
accurate analysis of the system over longer time scales, or for more complex
systems, it might prove both inaccurate and/or computationally intensive.
For a more detailed analysis of our model, we must look at the inter-
nal state dynamics and how they map to the output. Let us consider all

5

0.03

0.025

0.02

0.015

0.01

0.005

}
 
)
t
(

 

S
−
 
)
0
1
+
t
(

S
 
{
P

0
−100

−50

0
S(t+10)

50

100

Figure 4: The change in the system’s output S(t + U ) − S(t) at U = 10,
and associated probability as calculated for 106 time-series realisations.

possible eventualities over 2 time-steps, again starting in state A at time
t. All possible paths are denoted on ﬁgure 5. The resulting possible values
of change in the output at time t + 2 time-steps and their associated prob-
abilities are given explicitly too. These values are exact in that they are
calculated from the dice themselves. The F uture − Cast frame work which
we now introduce will allow us to perform similar analysis over much longer
periods. First consider the possible values of S(t + 2) − S(t), which result in
the system being in state A. The state transitions that could have occurred
for these to arise are A → A → A or A → B → A. The paths following
the former can be considered a convolution (explores all possible paths and
probabilities2) of the distribution of possible values of S(t+1)−S(t) in state
A with the distribution corresponding to the A → A transition. Likewise,
the latter a convolution of the distribution of S(t + 1) − S(t) in state B
with the distribution corresponding to a B → A transition. The resultant
distribution of possibilities in state A at time t + 2 is just the superposition
of these convolutions as described in ﬁgure 6. We can set the initial value
S(t) to zero because the absolute position has no bearing on the evolution
on the system. As such S(t + U ) − S(t) ≡ S(t + U ).

So we note that if at some time t + U there exists a distribution of values
in our output which are in state A then the convolution of this with the B
→ A transition distribution will result in a contribution at t + U + 1 to our

convolution operator ⊗ such that

(f ⊗

2We deﬁne and use the discrete
∞
j=−∞ f (i − j)g(i).

g) |i =

P

6

Output

)
t
(

S
−
)
U
+
t
(

S

+8

+3

−5

S(2)

16

P{ S(2) }

0.04

11

0.1+0.1

6

3

0.25

0.06

time
U

−2

0.15+0.12

−5

+8

+8

+3

+3

−5

+3

−8

−10

1

2

δ from State A

δ from State B

−13

−15

0.15

0.03

Figure 5: All possible paths after 2 time-steps and associated probability.

distribution of S which will also be in A. This is evident in the right hand
side of 6. Let us deﬁne all these distributions. We denote all possible values
in our output U time-steps beyond the present (time t) that are in state A
as the function ς U
B (S) is the function that describes the values of
our output time series that are in state B (as shown in ﬁgure 6). We can
similarly describe the transitions as prescribed by our dice. The possible
values allowed and corresponding likelihoods for the A → A transition are
denoted ΥA→A(δ) and similarly ΥA→B(δ) for A → B, ΥB→A(δ) for B →
A and ΥB→B(δ) for the B → B state change. These are shown in ﬁgure 7.

A (S) and ς U

We can now construct the Future-Cast. We can express the evolution of
the output in their corresponding states as the superposition of the required
convolutions:

ς U +1
A (S) = ΥA→A(δ) ⊗ ς U
ς U +1
B (S) = ΥA→B(δ) ⊗ ς U

A (S) + ΥB→A(δ) ⊗ ς U
B (S) + ΥB→B(δ) ⊗ ς U

B (S)
B (S)

(2)

where ⊗ is the discrete convolution operator as deﬁned in footnote 2. Recall
that S(0) has been set to zero such that we can consider the possible changes

7

P{S(t)}

(0,1)

ς0
A

(S)

U = 0

State B 

P{S(t+1)}

(−5,0.3)

ς1
B

(S)

U = 1

S(t)

P{S(t+1)}
(3,0.5)

State A 

ς1
A

(S)

(8,0.2)

S(t+1)

S(t+1)

P{δ}

(3,0.4)

convolved with
B to A transition
From Dice B

convolved with
A to A transition
From Dice A

P{δ}

(3,0.5)

(8,0.2)

outcome δ

outcome δ

Prob

Prob

to give

to give

(6,0.25)

(11,0.2)

(16,0.04)

value

(−2,0.12)

U = 2

Superposition of
contributions from each
state transition path

value

P{S(t+2)}

ς2
A

(S)

(−2,0.12)

(6,0.25)

(11,0.2)

Possible values of the time
series after 2 time−steps
which are in state A

(16,0.04)

S(t+2)

Figure 6: All possible paths after 2 time-steps which could result in the
system being in state A.

in output and the output itself to be identical. We can write this more
concisely:

Where the element Υ1,1 contains the function ΥA→A(δ) and the operator ⊗
and the element ς U +1
A (S). We note that this matrix
of functions and operators is static, so only need computing once. As such
we can rewrite equation 3 as

is the distribution ς U +1

1

such that ς 0 contains the state-wise information of our starting point (time
t). This is the Future-Cast process. For a system starting in state A and

(3)

(4)

ς U +1 = Υ ς U

ς U = ΥU ς 0

8

From Dice A

From Dice B

ϒ

A→ A

 (δ)

P{δ}

(3,0.5)

P{δ}

(3,0.4)

ϒ

B→ A

 (δ)

(8,0.2)

outcome δ

ϒ

A→ B

 (δ)

P{δ}

(−5,0.3)

outcome δ

P{δ}

(−8,0.5)

ϒ

B→ B

 (δ)

(−10,0.1)

outcome δ

outcome δ

Figure 7: The state transition distributions as prescribed by our dice.

with the start value of the time series of zero, the elements of ς 0 are as shown
in ﬁgure 8. Applying the Future-Cast process, we can look precisely at the

P{ S(0)}

(0 , 1)

P{ S(0)}

ς0
1

S(0)

ς0
2

S(0)

Figure 8: The initial elements of our output distributions vector, ς 0 when
starting the system in state A with initial value of zero in our time series.

systems potential output at any number of time-steps into the future. If we
wish to consider the process over 10 steps again, we applying as following:

The resultant distribution of possible outputs (we denote ΠU (S)) is then just
the superposition of the contributions from each state. This distribution
of the possible outputs at some time into the future is what we call the
F uture − Cast.

(5)

(6)

This leads to distribution shown in ﬁgure 9. Although the exact output as
calculated using the Future-Cast process demonstrated in ﬁgure 9 compares
well with the brute force numerical results of the Monte-Carlo technique
in ﬁgure 4, it allows us to perform some more interesting analysis without

ς 10 = Υ10 ς 0

Π10 = ς 10

1 + ς 10
2

9

0.035

0.03

0.025

0.02

0.015

0.01

0.005

}
 
)
0
1
(
S
 
{
P

0
−100

−50

0
system output S(10)

50

100

Figure 9: The actual probability distribution Π10(S)) of the output after
10 time-steps starting the system in state A with initial value of zero in our
time series.

much more work, let alone computational exhaustion. Consider that we
don’t know the initial state of the system or that we want to know char-
acteristic properties of the system. This might include wanting to know
what the system does, on average over one time-step increments. We could
for example look run the system for a very long time and investigate a his-
togram of the output movements over single time-steps as shown in ﬁgure 10.
However, we can use the formalism to generate this result exactly. Imagine
that model has been run for a long time but we don’t know which state it
is in. Using the probabilities associated with the transitions between the
states, we can infer the likelihood that the system’s internal state is either
A or B. Let the element Γt
1 represent the probability that the system is in
state A at some time t and Γt
2 that the system is in state B. We can ex-
press these quantities at time t + 1 by considering the probabilities of going
between the states:

Γt+1
1
Γt+1
2

= TA→AΓt
= TA→BΓt

1 + TB→AΓt
2
2 + TB→BΓt
2

Where TA→A represents the probability that when the system is in state
A it will be in state A at the next time-step. We can express this more
concisely:

(7)

(8)

Γt+1 = T Γt

10

5000

0

−5000

)
t
(

S

 
t
u
p
t
u
O

−10000

0

5000

4000

3000

2000

1000

y
c
n
e
u
q
e
r
F

0
−20

2000

4000

6000

8000

10000

time  t

−15

−10

−5

10

15

20

0
S(t+1) − S(t)

5

Figure 10: The one step increments of our time-series as run over 10000
steps.

Such that T1,1 is equivalent to TA→A. This is a Markov Chain. From the
nature of our dice, we can trivially calculate the elements of T . The value
for TA→A is the sum over all elements in ΥA→A(δ) or more precisely:

The Markov Chain transition matrix T for our system can thus be trivially
written:

TA→A =

ΥA→A(δ)

∞

Xδ=−∞

T =

(cid:18)

0.7 0.4
0.3 0.6

(cid:19)

(9)

(10)

The static probabilities of the system being in either of its two states are
given by the eigenvector solution to equation 11 with eigenvalue 1. This
is equivalent to looking at the relative occurrence of the two states if the
system were to be run over inﬁnite time.

Γ = T Γ

(11)

For our system, the static probability associated with being in state A is 4
7
7 for state B. To look at the characteristic properties we are
and obviously 3
going to construct an initial vector similar to ς 0 in equation 2) for our Future-
Cast formalism to act on but which is related to the static probabilities
contained by the solution to equation 11. This vector is denoted κ and its

11

κ
1

κ
2

P{S(t)}

4
( 0 ,   )
7

P{S(t)}

3
( 0 ,   )
7

S(t)

S(t)

Figure 11: Explicit depiction of the elements of vector κ which is used to
analyse characteristic behaviour of the system.

form is described in ﬁgure 11. We use employ κ in the Future-Cast over one
time-step as in equation 12:

ς 1 = Υ κ

(12)

The resulting distribution of possible outputs from superimposing the el-
ements of ς 1 is the exact representation of the one time-step increments
(S(t + 1) − S(t)) of the system if it were allowed to be run inﬁnitely. We
call this characteristic distribution Π1
char. Applying the process a number
of times will yield the exact distributions ΠU
char equivalent to looking at all
values of S(t + U ) − S(t) which is a rolling window length U over an inﬁnite
time series as in ﬁgure 12. This is also equivalent to running the system
forward in time U time-steps from unknown initial state, investigating all
possible paths. The Markovian nature of the system means that this is not
the same as the convolution of the one time-step characteristic Future-Cast
convolved with it self U times.

Clearly the characteristic Future-Cast over one time-step in ﬁgure 12

compares well with that of ﬁgure 10.

12

0.8

0.6

0.4

0.2

0.4

0.3

0.2

0.1

}
 
)
1
+
t
(

S
 
{
P

}
 
)
2
+
t
(

S
 
{
P

0
−30

0
−30

0.2

0.15

0.1

0.05

}
 
)
3
+
t
(

S
 
{
P

0
−30

−20

−10

10

20

30

0
S(t+1)

−20

−10

10

20

30

0
S(t+2)

−20

−10

10

20

30

0
S(t+3)

Figure 12: The characteristic behaviour of the system for U = 1, 2, 3 time-
steps into the future from unknown initial state. This is equivalent to looking
at the relative frequency of occurrence of changes in output values over 1,2,
and 3 time-step rolling windows

3 The Evolution of the Complex Adaptive System

Here we provide a general formalism applicable to any Complex System
which can be mapped onto a population of N species or ‘agents’ who are
repeatedly taking actions in some form of global ‘game’. At each time-step
each agent makes a (binary) decision aµ(t) in response to the global informa-
tion µ(t) which may reﬂect the history of past global outcomes. This global
information is of the form of a bitstring of length m. For a general game,
there exists some winning outcome w(t) based on the aggregate action of
the agents. Each agent holds a subset of all possible strategies - by assigning
this subset randomly to each agent, we can mimic the eﬀect of large-scale
heterogeneity in the population. In other words, we have a simple way of
generating a potentially diverse ecology of species, some of which may be
similar but others quite diﬀerent. One can hence investigate a typically-

13

diverse ecology whereby all possible species are represented, as opposed to
special cases of ecologies which may themselves generate pathological be-
haviour due to their lack of diversity.

The aggregate action of the population at each time-step t is represented
by D(t), which corresponds to the accumulated decisions of all the agents
and hence the (analogue) output variable of the system at that time-step.
The goal of the game, and hence the winning decision, could be to favour
the minority group (MG), the majority group or indeed any function of the
macroscopic or microscopic variables of the system. The individual agents do
not themselves need to be conscious of the precise nature of the game, or even
the algorithm for deciding how the winning decision is determined. Instead,
they just know the global outcome, and hence whether their own strategies
predicted the winning action3. The agents then reward the strategies in their
possession if the strategy’s predicted action would have been correct if that
strategy was implemented. The global history is then updated according to
the winning decision. It can be expressed in decimal form as follows:

µ(t) =

2i−1[w(t − i) + 1]

(13)

m

i=1
X

The system’s dynamics are deﬁned by the rules of the game. We will consider
here the class of games whereby each agent uses his highest-scoring strategy
at each timestep, and agents only participate if they possess a strategy with
a suﬃciently high success rate.
[N.B. Both of these assumptions can be
relaxed, thereby modifying the actual game being played]. The following
two scenarios might then arise during the system’s evolution:

• An agent has two (or more) strategies which are tied in score and are

above the conﬁdence level, and the decisions from them diﬀer.

• The number of agents choosing each of the two actions is equal, hence

the winning decision is undecided.

We will consider these cases to be resolved with a fair ‘coin toss’, thereby
injecting stochasticity or ‘noise’ into the system’s dynamical evolution. In
the ﬁrst case, each agent will toss his own coin to break the tie, while in the
second the Game-master tosses a single coin. To reﬂect the fact that evolving
systems will typically be non-stationary, and hence the more distant past
will presumably be perceived as less relevant to the agents, the strategies are
rewarded as to whether they would have made correct predictions over the
last H time-steps of the game’s running. There is no limit on the size of H
other than it is ﬁnite and constant. The time-horizon represents a trajectory
of length H on the de Bruijn graph in µ(t) (history) space[15] as shown in

3The algorithm used by the ‘Game-master’ to generate the winning decision could also

incorporate a stochastic factor.

14

001

011

000

010

101

111

100

110

Figure 13: A path of time-horizon length H = 5 (dashed line) superimposed
on the de Bruin graph for m = 3. The 8 global outcome states represent
the 8 possible bitstrings for the global information, and correspond to the
global outcomes for the past m = 3 timesteps.

ﬁgure 13. The stochasticity in the game means that for a given time-horizon
H and a given strategy allocation in the population, the output of the system
is not always unique. We will denote the set of all possible outputs from
the game at some number of time-steps beyond the time-horizon H, as the
Future-Cast.

It is useful to work in a time-horizon space Γt of dimension 2m+H . An
element Γt corresponds to the last m + H elements of the bitstring of global
outcomes (or equivalently, the winning actions) produced by the game. This
dimension is constant in time whereas for a non-time-horizon game it would
grow linearly. For any given time-horizon state, Γt, there exists a unique
score vector G(t) which is the set of scores GR(t) for all the strategies which
an agent could possess. As such, for each particular time-horizon state,
there exists a unique probability distribution of the aggregate action, D(t).
This distribution of possible actions when a speciﬁed state is reached will
necessarily be the same each time that state is revisited. Thus, it is possible
to construct a transition matrix (c.f. Markov Chain[15]) T of probabilities
for the movements between these time-horizon states such that P (Γt) can
be expressed as

P (Γt) = T P (Γt−1)

(14)

where P (Γt) is a vector of dimension 2m+H containing the probabilities of
being in a given state Γ at time t

The transition matrix of probabilities is constant in time and necessarily
sparse. For each state, there are only two possible winning decisions. The
number of non-zero elements in the matrix is thus ≤ 2(m+H+1). We can
use the transition matrix in an eigenvector-eigenvalue problem to obtain the
stationary state solution of P (Γ) = T P (Γ). This also allows calculation of

15

some time-averaged macroscopic quantities of the game [15]4.

To generate the Future-Cast, we want to calculate the quantities in out-

put space. To do this, we require;

• The probability distribution of D(t) for a given time-horizon;

• The corresponding winning decisions, w(t), for given D(t);

• An algorithm generating output in terms of D(t).

To implement the Future-Cast, we need to map from the transitions in
the state space internal to the system to the macroscopic observables in the
output space (often cumulative excess demand). We know that in the transi-
tion matrix, the probabilities represent the summation over a distribution of
possible aggregate actions which is binomial in the case where the agents are
limited to two possible decisions. Using the output generating algorithm, we
can construct an ‘adjacency’ matrix Υ analogous to the transition matrix T ,
with the same dimensions. The elements of Υ, contain probability distribu-
tion functions of change in output corresponding to the non-zero elements
of the transition matrix together with the discrete convolution operator ⊗
whose form depends on that of the output generating algorithm.

The adjacency matrix of functions and operators can then be applied
to a vector, ς U =0(S), containing information about the current state of the
game and of the same dimension as Γt . ς U =0(S) not only describes the time-
horizon state positionally through its elements but also the current value in
the output quantity S within that element. At U = 0, the state of the
system is unique so there is only one non-zero element within ς U =0(S). This
element corresponds to a probability distribution function of the current
output value, its position within the vector corresponding to the current
time-horizon state. The probability distribution function is necessarily of
value unity at the current value or, for a Future-Cast expressed in terms of
change in output from the current value, unity at the origin. The Future-
Cast process for U time-steps beyond the present state can then be described
by

ς U (S) = ΥU ς 0(S)

The actual Future-Cast, Π(S, U ), is then computed by superimposing

the elements of the output/time-horizon state vector:

2(m+H)

ΠU (S) =

ς U
i (S).

i=1
X
4The steady state eigenvector solution is an exact expression equivalent to pre-
. This eﬀectively results in a proba-

multiplying the probability state vector P (Γt) by T ∞
bility state vector which is time-averaged over an inﬁnite time-interval.

(15)

(16)

16

Thus the Future-Cast, ΠU (S), is a probability distribution of the outputs
possible at U time-steps in the future.

As a result of the state dependence of the Markov Chain, Π is non-
Gaussian. As with the steady-state solution of the state space transition
matrix, we would like to ﬁnd a ‘steady-state’ equivalent for the output space5
of the form

Π1

char(S) =

Π1(S)

∞

(17)

(cid:10)

(cid:11)

where the one-timestep Future-Cast is time-averaged over an inﬁnitely long
period. Fortunately, we have the steady state solutions of P (Γ) = T P (Γ)
which are the (static) probabilities of being in a given time-horizon state
at any time. By representing these probabilities as the appropriate func-
tions, we can construct an ‘initial’ vector, κ, similar in form to ς(S, 0) in
equation 15 but equivalent to the eigenvector solution of the Markov Chain.
We can then generate the solution of equation 17 for the characteristic
Future-Cast, Π1
char, for a given initial set of strategies. An element κi is
again a probability distribution which is simply the point (0 , Pi(Γ)), the
static probability of being in the time-horizon state denoted by the elements
position, i. We can then get back to the Future-Cast

Π1

char(S) =

ς 1
i where

ς 1 = Υ κ.

(18)

We can also generate characteristic Future-Casts for any number of time-
steps, U , by pre-multiplying κ by ΥU

ΠU

char(S) =

ς U
i

where

ς U = ΥU κ

(19)

We note that ΠU
char is not equivalent to the convolution of Π1
char with itself
U times and as such is not necessarily Gaussian. The characteristic Future-
Cast over U time-steps is simply the Future-Cast of length U from all the
2m+H possible initial states where each contribution is given the appropriate
weighting factor. This factor corresponds to the probability of being in that
initial state. The characteristic Future-Cast can also be expressed as

2(m+H)

i=1
X

2(m+H)

i=1
X

ΠU

char(S) =

P (Γ) ΠU (S) | Γ

(20)

where ΠU (S) | Γ is a normal Future-Cast from an initial time-horizon
state Γ and P (Γ) is the static probability of being in that state at a given
time.

5Note that we can use this framework to generate time-averaged quantities of any of
the macroscopic quantities of the system (e.g total number of agents playing) or volatility.

2(m+H)

XΓ=1

17

Figure 14: Schematic diagram of the Binary Agent Resource (B-A-R) sys-
tem.

4 The Binary Agent Resource System

The general binary framework of the B-A-R (Binary Agent Resource) system
was discussed in section 3. The global outcome of the ‘game’ is represented
as a binary digit which favours either those choosing option +1 or option
−1 (or equivalently 1 or 0, A or B etc.). The agents are randomly assigned s
strategies at the beginning of the game. Each strategy comprises an action
µ(t) in response to each of the 2m possible histories µ, thereby generating
as
a total of 22m
strategies in the Full Strategy Space 6. At each turn of the
game, the agents employ their most successful strategy, being the one with
the most virtual points. The agents are thus adaptive if s > 1.

We have already extended the B-A-R system by introducing the time-
horizon H, which determines the number of past time-steps over which vir-
tual points are collected for each strategy. We further extend the system
by the introduction of a conﬁdence level. The agents decide whether to
participate or not depending on the success of their strategies. As such,
the number of active agents N (t) is less than or equal to Ntot at any given
time-step. This results in a variable number of participants per time-step

6We note that many features of the game can be reproduced using a Reduced Strategy
Space of 2m+1 strategies, containing strategies which are either anti-correlated or uncor-
related with each other[12]. The framework established in the present paper is general to
both the full and reduced strategy spaces, hence the full strategy space will be adopted
here.

18

V (t), and constitutes a ‘Grand Canonical’ game. The threshold, τ , denotes
the conﬁdence level: each agent will only participate if he has a strategy
with at least r points where

r = T (2τ − 1).

(21)

Agents without an active strategy become temporarily inactive.

In keeping with typical biological, ecological, social or computational
systems, the Game-master takes into account a ﬁnite global resource level
when deciding the winning decision at each time-step. For simplicity, we
will here consider the speciﬁc case7 whereby the resource level L(t) = φV (t)
with 0 ≤φ≤1. We denote the number of agents choosing action +1 (or
equivalently A) as N+1(t), and those that choose action -1 (or equivalently
B) as N−1(t). If L(t) − N+1(t) > 0 the winning action is +1 and vice-versa.
We deﬁne the winning decision 1 or 0 as follows:

where we deﬁne step[x] to be

w(t) = step[L(t) − N+1(t)]

step[x] =

1
0
fair coin toss

if x > 0,
if x < 0,
if x = 0.






When x = 0, there is no deﬁnite winning option since N+1(t) = N−1(t),
hence the Game-master uses a random coin-toss to decide between the two
possible outcomes. We use a binary payoﬀ rule for rewarding strategy scores,
although more complicated versions can, of course, be used. However, we
note that non-binary payoﬀs (e.g. a proportional payoﬀ scheme) will de-
crease the probability of tied strategy scores, hence making the system more
deterministic. Since we are interested in seeing the extent to which stochas-
ticity can prevent control, we are instead interested in preserving the pres-
ence of such stochasticity. The reward function χ can be written

χ[N+1(t), L(t)] =

for w(t) = 1,
1
−1 for w(t) = 0,

(cid:26)
namely +1 for predicting the correct action and -1 for predicting the incor-
rect one. For a given strategy, R, the virtual points score is given by

GR(t) =

aµ(i)
R χ[N+1(i), L(i)],

t−1

i=t−T
X

R

where aµ(t)
is the response of strategy, R, to the global information µ(t)
summed over the rolling window of width H. The global output signal
D(t) = N+1(t)− N−1(t) is calculated at each iteration to generate an output
time series.

7We note that φ itself could be actually be a stochastic function of the known system

parameters.

(22)

(23)

(24)

(25)

19

5 Looking at the System’s Natural Evolution

To realize all possible paths within a given game is necessarily computa-
tionally expensive. For a Future-Cast U timesteps beyond the current game
state, there are necessarily 2U winning decisions to be considered. Fortu-
nately, not all winning decisions are realized by a speciﬁc game and the
numerical generation of the Future-Cast can be made reasonably eﬃcient.
Fortunately we can approach the Future-Cast analytically without having
to keep track of the agents’ individual microscopic properties. Instead we
group the agents together via the population tensor of rank s given by Ω,
which we will refer to as the Quenched Disorder Matrix (QDM) [20]. This
matrix is assumed to be constant over the time-scales of interest, and more
typically is ﬁxed at the beginning of the game. The entry ΩR2,R2,... represents
the number of agents holding the strategies R1, R2, . . . such that

ΩR,R′,... = N

XR,R′,...

(26)

For numerical analysis, it is useful to construct a symmetric version of this
2 (Ω+ Ωtranspose)
population tensor, Ψ . For the case s = 2, we will let Ψ = 1
[17].

The output variable D(t) can be written in terms of the decided agents
Dd(t) who act in a pre-determined way since they have a unique predicted
action from their strategies, and the undecided agents Dud(t) who require
an additional coin-toss in order to decide which action to take. Hence

D(t) = Dd(t) + Dud(t).

(27)

We focus on s = 2 strategies per agent although the approach can be gen-
eralized. The element ΨR,R′ represents the number of agents holding both
strategy R and R′. We can now write Dd(t) as

Dd(t) =

aµ(t)
R H[GR(t) − r]

(1 + sgn[GR(t) − GR′(t)])ΨR,R′

(28)

Q

R=1
X

where Q is the size of the strategy space, H is the Heaviside function and
sgn[x] is deﬁned as

sgn[x] =

if x > 0,
1
−1 if x < 0,
if x = 0.
0

The volume V (t) of active agents can be expressed as

V (t) =

H[GR(t) − r]

sgn[GR(t) − GR′ (t)] +

δ[GR(t) − GR′(t)]

ΨR,R′

1
2

XR,R′

(cid:8)

(29)

(cid:9)

(30)

Q

XR′=1






20

where δ is the Dirac delta. The number of undecided agents Nud(t) is given
by

Nud(t) =

H[GR(t)−r]δ(GR(t)−GR′ (t))[1−δ(aµ(t)

R −aµ(t)

R′ )]ΨR,R′ (31)

XR,R′

We note that for s = 2, because each undecided agent’s contribution to D(t)
is an integer, hence the demand of all the undecided agents Dud(t) can be
written simply as

Dud(t) ǫ 2 Bin

Nud(t),

− Nud(t)

(32)

1
2

(cid:19)

(cid:18)

where Bin(n, p) is a sample from a binomial distribution of n trials with
probability of success p.

For any given time-horizon space-state Γt, the score vector G(t) (i.e., the
set of scores GR(t) for all the strategies in the QDM) is unique. Whenever
this state is reached, the quantity Dd(t) will necessarily always be the same,
as will the distribution of Dud(t). We can now construct the transition
matrix T giving the probabilities for the movements between these time-
horizon states. The element T
which corresponds to the transition
Γt|Γt−1
from state Γt−1 to Γt, is given for the (generalisable) s = 2 case by

T

Γt|Γt−1

=

Nud

x=0 (
X

1
2

(cid:20)

NudCx(

)Nudδ

Sgn(Dd + 2x − Nud + V (1 − 2φ)) +

(2µt%2 − 1)
(cid:21)

+

NudCx(

)(Nud+1)δ

1
2

Sgn(Dd + 2x − Nud + V (1 − 2φ)) + 0
(cid:20)

(cid:21))

(33)

where Nud, Dd implies Nud | Γt−1 and Dd | Γt−1, V implies V (t − 1), φ
sets the resource level as described earlier and µt%2 is the required winning
decision to get from state Γt−1 to state Γt. We use the transition matrix in
the eigenvector-eigenvalue problem to obtain the stationary state solution
of P (Γ) = T P (Γ). The probabilities in the transition matrix represent
the summation over a distribution which is binomial in the s = 2 case.
These distributions are all calculated from the QDM which is ﬁxed from
the outset. To transfer to output-space, we require an output generating
algorithm. Here we use the equation

S(t + 1) = S(t) + D(t)

(34)

hence the output value S(t) represents the cumulative value of D(t), while
the increment S(t + 1) − S(t) is simply D(t). Again, we use the discrete

21

convolution operator ⊗ deﬁned as

∞

(f ⊗ g) |i =

f (i − j) × g(j).

(35)

j=−∞
X
The formalism could be extended for general output algorithms using dif-
ferently deﬁned convolution operators.

An element in the adjacency matrix for the s = 2 case can then be

expressed as

Υ

Γt|Γt−1

=

(Dd + 2x − Nud),

Nud

(

x=0  
X

NudCx(

)Nudδ

1
2

NudCx(

1
2

(cid:20)
)(Nud+1)δ

(cid:20)

Sgn(Dd + 2x − Nud + V (1 − 2φ)) + (2µt%2 − 1)
(cid:21)

+

Sgn(Dd + 2x − Nud + V (1 − 2φ)) + 0

⊗

(36)

(cid:21)!)

where Nud, Dd again implies Nud | Γt−1 and Dd | Γt−1, V implies V (t − 1),
and µt%2 is the winning decision necessary to move between the required
states. The Future-Cast and characteristic Future-Casts (ΠU (S), ΠU
char) U
time-steps into the future can then be computed for a given initial quenched
disorder matrix (QDM).

We now consider an example to illustrate the implementation. In par-
ticular, we provide the explicit solution of a Future-Cast in the regime of
small m and H, given the randomly chosen quenched disorder matrix

Ω =

.

(37)

0
0
0
0
1
1
0
0
2
0
0
0
1
0
0
0





























0
0
1
1
0
0
1
1
1
0
2
0
0
0
2
0

1
0
1
0
0
0
1
0
0
0
0
2
1
0
0
0

0
1
0
0
0
0
0
0
0
0
0
0
1
1
1
1

0
0
1
2
0
1
0
1
0
0
1
0
0
0
0
0

1
0
0
0
1
1
0
1
1
0
0
0
0
0
0
0

0
1
1
1
0
0
0
0
1
1
0
0
0
0
0
1

1
0
0
0
0
0
0
0
1
0
1
0
0
2
0
0

0
2
0
0
0
1
0
0
0
0
1
0
1
1
0
0

0
0
0
0
1
0
1
1
0
3
3
0
0
0
0
0

1
0
0
0
1
0
0
0
0
1
2
0
0
1
1
0

0
1
2
0
1
0
2
1
0
1
0
0
0
0
0
0

0
0
1
0
1
0
1
0
0
0
0
0
0
0
0
1

0
1
0
0
1
1
0
1
0
1
0
0
0
0
1
0

0
0
0
0
0
2
0
0
0
0
1
0
0
1
0
1

0
0
1
2
1
0
1
1
0
1
1
0
0
0
0
1





























We consider the full strategy space and the the following game parameters:

Number of agents Ntot
m
s
φ
H
τ

Memory size
Strategies per agent
Resource level
Time horizon
Threshold

101
2
2
0.5
2
0.51

22

The dimension of the transition matrix is thus 2H+m = 16.



0
0
0
0
0
0
0
0
0.1875
0.8125
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0.0312
0.9688
0
0
0
0

0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0.5
0.5
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0.1875
0.8125
0
0
0
0
0
0
0
0

0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0.1094
0.8906
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0.75
0.25
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0.0625
0.9375
0
0
0
0
0
0
0
0
0
0
0
0

.

T =




















































(38)
Each non-zero element in the transition matrix corresponds to a probability
function in the output space in the Future-Cast operator matrix. Consider
that the initial state is Γ = 10 i.e.
the last 4 bits are {1001} (obtained
from running the game prior to the Future-Casting process). The initial
probability state vector is the point (0,1) in the element of the vector ς
corresponding to time-horizon state 10. We can then generate the Future-
Cast for given U (shown in ﬁgure 15).



0
0
0
0
0
0
0
0
0
0
0
0
0.5
0.5
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0

pre−future−cast time series
distribution means
actual realisation
future−casts

)
0
(
S
−
 
)

 

U
S

(

300

200

100

0

−100

−200

−300

−10

−5

0

5

10

15

20

25

U

Figure 15: The (un-normalized) evolution of a Future-Cast for the given
Ω, game parameters and initial state Γ. The ﬁgure shows the last 10 time-
steps prior to the Future-Cast, the means of the distributions within the
Future-Cast itself, and also an actual realization of the game run forward in
time.

Clearly the probability function in output space becomes smoother as
U becomes larger and the number of successive convolutions increases, as
highlighted by the probability distribution functions at U = 15 and U = 25

23

future−cast distribution
realisation
distribution mean

future−cast distribution
realisation
distribution mean

(ﬁgure 16).

0.045

0.04

0.035

0.03

)
)
0
(
S
−
)
U
S
P

(

(

0.025

0.02

0.015

0.01

0.005

0
−150

−100

−50

0

50

100

150

200

−200

−100

0

100

200

300

400

S(U) − S(0)

S(U) − S(0)

Figure 16: The probability distribution function at U = 15, 25 time-steps
beyond the present state.

We note the non-Gaussian form of the probability distribution for the
Future-Casts, emphasising the fact that such a Future-Cast approach is
essential for understanding the system’s evolution. An assumption of rapid
diﬀusion toward a Gaussian distribution, and hence the future spread in
paths increasing as the square-root of time, would clearly be unreliable.

6 Online Evolution Management via ‘Soft’ Con-

trol

For less simple parameters, the matrix dimension required for the Future-
Cast process become very large very quickly. To generate a Future-Cast
appropriate to larger parameters e.g. m = 3, H = 10, it is still however pos-
sible to carry out the calculations numerically quite easily. As an example,
we generate a random Ω (the form of which is given in ﬁgure 19) and initial
time-horizon appropriate to these parameters. This time-horizon is obtained
by allowing the system to run prior to the Future-Cast. For visual repre-
sentation reasons, the Reduced Strategy Space[20] is employed. The other
game parameters are as previously stated. The game is then instructed to
run down every possible winning decision path exhaustively. The spread of
output at each step along each path is then convolved with the next spread
such that a Future-Cast is built up along each path. Fortunately, not all
paths are realized at every time-step since the stochasticity in the winning-
decision/state-space results from the condition Nud ≥ Dd. The Future-Cast
as a function of U and S, can thus be built up for a randomly chosen initial
quenched disorder matrix (QDM) (17).

We now wish to consider the situation where it is required that the
system should not behave in a certain manner. For example, it may be
desirable that it avoid entering a certain regime characterised by a given

0.018

0.016

0.014

0.012

0.01

0.008

0.006

0.004

0.002

)
)
0
(
S
−
)
U
S
P

(

(

0
−300

24

)
0
(
S
−
 
)

 

(

U
S
P

(

0.5

0.4

0.3

0.2

0.1

0
−5

Future PDFs
Pre−Future−Cast time series
An actual run forward

200

150

100

0

5

timesteps (U)

50

0

−50

10

−100

S(U) − S(0)

Figure 17: Evolution of ΠU (S) for a typical quenched disorder matrix Ω.

value of S(t). Speciﬁcally, we consider the case where there is a barrier in
the output space that the game should avoid, as shown in ﬁgure 18.

The evolution of the spread (i.e. standard deviation) of the distribu-
tions in time, conﬁrms the non-Gaussian nature of the system’s evolution –
we note that this spread can even decrease with time8. In the knowledge
that this barrier will be breached by this system, we therefore perturb the
quenched disorder at U = 0. This perturbation corresponds in physical
terms to an adjustment of the composition of the agent population. This
could be achieved by ‘re-wiring’ or ‘reprogramming’ individual agents in a
situation in which the agents were accessible objects, or introducing some
form of communication channel, or even a more ‘evolutionary’ approach
whereby a small subset of species are removed from the population and a
new subset added in to replace them. Interestingly we note that this ‘evolu-
tionary’ mechanism need neither be completely deterministic (i.e. knowing
exactly how the form of the QDM changes) nor completely random (i.e. a
random perturbation to the QDM). In this sense, it seems tantalisingly close
to some modern ideas of biological evolution, whereby there is some purpose
mixed with some randomness.

8This feature can be understood by appreciating the multi-peaked nature of the distri-
butions in question. The peaks correspond to diﬀering paths travelled in the Future-Cast,
the ﬁnal distribution being a superposition of these. If these individual path distributions
mean-revert, the spread of the actual Future-Cast can decrease over short time-scales.

25

pdf means
pdf spread
barrier

)
0
(
S
−
 
)

 

U
S

(

150

100

50

0

−50

−100

4000

0

r
a
V

2000

0

1

1

2

3

4

5

6

7

8

9

10

11

U

pdf variance (U)

2

3

4

7

8

9

10

5
6
timesteps (U)

Figure 18: The evolution of the Future-Casts, and the barrier to be avoided.
For simplicity the barrier is chosen to correspond to a ﬁxed S(t) value of
110, although there is no reason that it couldn’t be made time-dependent.
Superimposed on the (un-normalised) distributions, are the means of the
Future-Casts, while their variances are shown below.

Figure 20 shows the impact of this relatively minor microscopic pertur-
bation on the Future-Cast and global output of the system. In particular,
the system has been steered away from the potentially harmful barrier into
‘safer’ territory.

This set of outputs is speciﬁc to the initial state of the system. More
typically, we may not know this initial state. Fortunately, we can make use
of the characteristic Future-Casts to make some kind of quantitative assess-
ment of the robustness of the quenched disorder perturbation in avoiding
the barrier, since this procedure provides a picture of the range of possible
future scenarios.

This evolution of the characteristic Future-Casts, for both the initial and
perturbed quenched disorder matrices, is shown in ﬁgure 21 A quantitative
evaluation of the robustness of this barrier avoidance could then be calcu-
lated using traditional techniques of risk analysis, based on knowledge of
the distribution functions and/or their low-order moments.

26

Perturbation Agents
Initial Agents              

2

4

6

8

10

Strategy R

12

14

2

16

0

16

14

12

10

8

6

4

Strategy R’

Figure 19: The initial and resulting quenched disorder matrices (QDM),
shown in schematic form. The x-y axes are the strategy labels for the two
strategies. The absence of a symbol denotes an empty bin (i.e. no agent
holding that particular pair of strategies).

initial QDM future−cast
initial QDM means
perturbed QDM future−cast
perturbed QDM means

y
c
n
a
p
u
c
c
O

5

4

3

2

1

0

0

)
0
(
S
−
 
)

 

U
S

(

150

100

50

0

−50

0

1

2

3

4

5

6

7

8

9

10

11

U

Figure 20: The evolution as a result of the microscopic perturbation to the
population’s composition (i.e. the QDM).

27

initial QDM future−cast
initial QDM means
perturbed QDM future−cast
perturbed QDM means

)
0
(
S
−
 
)

 

U
S

(

100

50

0

−50

−100

−150

0

1

2

3

4

5

6

7

8

9

10

11

U

Figure 21: The characteristic evolution of the initial and perturbed QDMs.

7 A Simpliﬁed Implementation of the Future-Cast

Formalism

We introduced the Future-Cast formalism to map from the internal state
space of a complex system to the observable output space. Although the
formalism exactly generates the probability distributions of the subsequent
output from the system, it’s implementation is far from trivial. This involves
keeping track of numerous distributions and performing appropriate convo-
lutions between them. Often, however it is only the lowest order moments
which are of immediate concern to the system designer. Here, we show how
this information can be generated without the computational exhaustion
previously required. We demonstrate this procedure for a very simple two
state system, although the formalism is general to a system of any number
of states, governed by a Markov chain.

Recall the toy model comprising two dice of section 2 . We previously
broke down the possible outputs of each according to the state transition as
shown in ﬁgure 22. These distributions were used to construct the matrix
Υ to form the Future-Cast process as denoted in equation 3. This acted
on vector ς U to generate ς U +1. The elements of these vectors contain the
partial distribution of outputs which are in the state denoted by the element
number at that particular time, so for the two dice model, ς U
1 (S) contains
the distribution of output values at time t + U (or U time-steps beyond the
present) which correspond to the system being in state A and ς U
2 (S) contains
those for state B. To reduce the calculation process, we will consider only

28

From Dice A

From Dice B

ϒ

A→ A

 (δ)

P{δ}

(3,0.5)

P{δ}

(3,0.4)

ϒ

B→ A

 (δ)

(8,0.2)

outcome δ

ϒ

A→ B

 (δ)

P{δ}

(−5,0.3)

outcome δ

P{δ}

(−8,0.5)

ϒ

B→ B

 (δ)

(−10,0.1)

outcome δ

outcome δ

Figure 22: The state transition distributions as prescribed by our dice.

the moments of each of these individual elements about zero. As such we
construct a vector, nxU , which takes the form:

nxU =

∞
S=−∞ ς U
∞
S=−∞ ς U

1 (S)Sn
2 (S)Sn

(cid:19)

(cid:18) P
P

(39)

The elements are just the nth moments about zero of the partial distribu-
tions within the appropriate state. For n = 0 this vector merely represents
the probabilities of being in either state at some time t + U . We note that
for the n = 0 case, 0xU +1 = T 0xU where T is the Markov Chain transition
matrix as in equation 8. We also note that the transition matrix T = 0X
where we deﬁne the (static) matrix nX in a similar fashion using the partial
distributions (described in ﬁgure 22) to be

P
P

(cid:18) P
P

nX =

∞
δ=−∞ ΥA→A(δ)δn
∞
δ=−∞ ΥA→B(δ)δn

∞
δ=−∞ ΥB→A(δ)δn
∞
δ=−∞ ΥB→B(δ)δn

(40)

(cid:19)

Again, this contains the moments (about zero) of the partial distributions
corresponding to the transitions between states. The evolution of 0x , the
state-wise probabilities with time is trivial as described above. For higher
orders, we must consider the eﬀects of superposition and convolution on their
values. We know that the for the superposition of two partial distributions,
the resulting moments (any order) about zero will be just the sum of the
moments of the individual distributions, it is just a summation. The eﬀects
of convolution, however must be considered more carefully. The elements
of our vector 1xU are the ﬁrst order moments of the values associated with
either of the two states at time t+U . The ﬁrst element of which corresponds
to those values of output in state A at time t + U . Consider that element

29

one step later, 1x1,U +1. This can be written as the superposition of the two
required convolutions.

ς U +1
1

S =

ς U
1 ΥA→A (S + δ) +

ς U
2 ΥB→A (S + δ)

S
X

S
X
1x1,U +1 = 0x1,U

δ
X

1X1,1 + 0X1,1
0x2,U

1X1,2 + 0X1,2

1x1,U +

1x2,U

δ
X

S
X

(41)

In simpler terms,

1xU +1 = 0X 1xU + 1X 0xU
The (S + δ) term in the expression relates to the nature of series generating
algorithm, St+1 = St + δt. If the series updating algorithm were altered, this
would have to be reﬂected in this convolution.

(42)

The overall output of the system is the superposition of the contributions
in each state. As such, the resulting ﬁrst moment about zero (the mean) for
the overall output at U time-steps into the future is simply 1xU +1 · 1 where
1 is a vector containing all ones and · is the familiar dot product.
The other moments about zero can be obtained similarly.

0xU +1 = 0X 0xU
1xU +1 = 0X 1xU + 1X 0xU
2xU +1 = 0X 2xU + 2 1X 1xU + 2X 0xU
3xU +1 = 0X 3xU + 3 1X 2xU + 3 2X 1xU + 3X 0xU

...

...

More generally

nxU +1 =

nCγ

γX n−γxU

n

γ=0
X

where nCγ is the conventional choose function.

To calculate time-averaged properties of the system, for example the

one-time-step mean or variance, we set the initial vectors such that

and βx0 = 0 for β > 0. The moments about zero can then be used
to calculate the moments about the mean. The mean of the one time-step
increments in output averaged over an inﬁnite run will then be 1x1 · 1 and
σ2 will be

σ2 = 2x1 · 1 − ( 1x1 · 1)2

0x0 = 0X 0x0

30

(43)

(44)

(45)

(46)

These can be calculated for any size rolling window. The mean of all U -step
increments, S(t + U ) − S(t)) or conversely the mean of the Future-Cast U
steps into the future from unknown current state is simply 1xU · 1and σ2
U
will be

U = 2xU · 1 − ( 1xU · 1)2
σ2
(47)
again with initial vectors calculated from 0x0 = 0X 0x0 and βx0 = 0 for
β > 0. Examining this explicitly for our two dice model, the initial vectors
are:

and the (static) matrices are:

0x0 =

1x0 =

2x0 =

4
7
3
7 (cid:19)
0
0

(cid:19)

0
0

(cid:19)

(cid:18)

(cid:18)

(cid:18)

0X =

1X =

2X =

0.7 0.4
0.3 0.6

(cid:19)
1.2
3.1
−1.5 −5.0

17.3 3.6
42.
7.5

(cid:19)

(cid:18)

(cid:18)

(cid:18)

(cid:19)

(48)

(49)

These are all we require to calculate the means and variances for our sys-
tem’s potential output at any time in the future. To check that all is well,
we employ the Future-Cast to generate the possible future distributions of
char to Π10
output up till 10 time-steps, Π1
char. The means and variances of
these are compared to the reduced Future-Cast formalism and also a nu-
merical simulation. This is a single run of the game over 100000 time-steps.
The means and variances are then measured over rolling windows of be-
tween 1 and 10 time-steps in length. The comparison is shown in ﬁgure 23.
Fortunately they all concur. The Reduced Future-Cast formalism and the
moments about either the mean or zero from the distributions generated
by the Future-Cast formalism are identical. Clearly numerical simulations
require progressively longer run times to investigate the properties of dis-
tributions further into the future, where the total number of possible paths
gets large.

31

Mean

Future Cast
Reduced Future−Cast
simulation

1

2

3

7

8

9

10

4

5
timesteps U

6

S

 
t
u
p
t
u
o

0

−2

−4

−6

−8

0

700

600

500

400

300

200

100

S

 
t
u
p
t
u
o

Future Cast
Reduced Future−Cast
simulation

0

0

1

2

3

4

5
timesteps U

6

7

8

9

10

Figure 23: The means and variances of the characteristic distributions Π1
to Π10

char
char as compared to a numerical evaluation and the reduced Future-Cast.

8 Discussion

We have presented an analytical formalism for the calculation of the proba-
bilities of outputs from the B-A-R system at a number of time-steps beyond
the present state. The construction of the (static) Future-Cast operator
matrix allows the evolution of the systems output, and other macroscopic
quantities of the system, to be studied without the need to follow the micro-
scopic details of each agent or species. We have demonstrated the technique
to investigate the macroscopic eﬀects of population perturbations but it
could also be used to explore the eﬀects of exogeneous noise or even news
in the context of ﬁnancial markets. We have concentrated on single real-
isations of the quenched disorder matrix, since this is appropriate to the
behaviour and design of a particular realization of a system in practice.
An example could be a ﬁnancial market model based on the B-A-R system
whose derivatives could be analysed quantitatively using expectation values
generated with the Future-Casts. We have also shown that through the nor-
malised eigenvector solution of the Markov Chain transition matrix, we can
use the Future-Cast operator matrix to generate a characteristic probability
function for a given game over a given time period. The formalism is general

32

to any time-horizon game and could, for example, be used to analyse systems
(games) where a level of communication between the agents is permitted,
or even linked systems (i.e.
linked games or ‘markets’). In the context of
linked systems, it will then be interesting to pursue the question as to when
adding one ‘safe’ complex system to another ‘safe’ complex system, results
in an ‘unsafe’ complex system. Or thinking more optimistically, when can
we put together two or more ‘unsafe’ systems and get a ‘safe’ one?

We have also presented a simpliﬁed and altogether more usable interpre-
tation of the Future-Cast formalism for tracking the evolution of the output
variable from a complex system whose internal states can be described as
a Markov process. We have illustrated the application of the results for an
example case both for the evolution from a known state or when the present
state is unknown, to give characteristic information about the output se-
ries generated by such a system. The formalism is generalizable to Markov
Chains whose state transitions are not limited to just two possibilities and
also to systems whose mapping from state transitions to output-space are
governed by continuous probability distributions.

Future work will focus on the ‘reverse problem’ of the broad-brush design
of multi-agent systems which behave in some particular desired way – or
alternatively, ones which will avoid some particular undesirable behaviour.
The eﬀects of any perturbation to the system’s heterogeneity could then be
pre-engineered in such a system. One possible future application would be
to attack the global control problem of discrete actuating controllers[27]. We
will also pursue our goal of tailoring multi-agent model systems to replicate
the behaviour of a range of real-world systems, with a particular focus on
(1) biological and human health systems such as cancer tumours and the
immune system, and (2) ﬁnancial markets.

References

(2000).

1995).

[1] See N. Boccara, Modeling Complex Systems (Springer, New York, 2004)

and references within, for a thorough discussion.

[2] D.H. Wolpert, K. Wheeler and K. Tumer, Europhys. Lett., 49(6)

[3] S. Strogatz, Nonlinear Dynamics and Chaos (Addison-Wesley,Reading,

[4] N. Israeli and N. Goldenfeld, Phys. Rev. Lett. 92, 074105 (2004).

[5] J.L. Casti, Would-be Worlds (Wiley, New York, 1997).

[6] B. Arthur, Amer. Econ. Rev. 84, 406 (1994); Science 284, 107(1999).

33

[7] N.F. Johnson, S. Jarvis, R. Jonson, P. Cheung,Y.R. Kwong, and P.M.

Hui, Physica A 258 230 (1998).

[8] M. Anghel, Z. Toroczkai, K. E. Bassler, G.Kroniss, Phys. Rev. Lett.

92, 058701 (2004).

67, 867 (2004).

(2004).

056102 (2004).

[9] S. Gourley, S.C. Choe, N.F. Johnson, and P.M.Hui, Europhys. Lett.

[10] S.C. Choe, N.F. Johnson, and P.M. Hui, Phys. Rev. E 70, 055101

[11] T.S. Lo, H.Y. Chan, P. M. Hui, and N. F.Johnson, Phys. Rev. E 70,

[12] D. Challet and Y.C. Zhang, Physica A 246, 407 (1997);.

[13] D. Challet, M. Marsilli, and G. Ottino, cond-mat/0306445.

[14] N.F. Johnson, S.C. Choe, S. Gourley, T.Jarrett, and P.M. Hui, in Ad-
vances in Solid State Physics Vol. 44 (Springer, Heidelberg, 2004),p.
427.

[15] We originally introduced the ﬁnite time-horizon MG, plus its ‘grand
canonical’ variable-N and variable-L generalizations, to provide a min-
imal model for ﬁnancial markets. See M.L. Hart, P. Jeﬀeries and N.F.
Johnson, Physica A 311, 275(2002); M.L. Hart, D. Lamper and N.F.
Johnson, Physica A 316, 649 (2002); D. Lamper, S. D. Howison, and N.
F. Johnson, Phys. Rev. Lett. 88, 017902 (2002); N.F. Johnson, P. Jef-
feries, and P.M. Hui,Financial Market Complexity (Oxford University
Press, 2003). See also D. Challet and T. Galla, cond-mat/0404264,which
uses this same model.

[16] D. Challet and Y.C. Zhang, Physica A 256, 514 (1998).

[17] D. Challet, M. Marsili and R. Zecchina, Phys. Rev. Lett. 82, 2203

[18] D. Challet, M. Marsili and R. Zecchina, Phys. Rev. Lett. 85, 5008

[19] See http://www.unifr.ch/econophysics for Minority Game literature.

[20] N.F. Johnson, M. Hart and P.M. Hui, Physica A 269, 1 (1999).

[21] M. Hart, P. Jeﬀeries, N.F. Johnson and P.M. Hui, Physica A 298, 537

[22] N.F. Johnson, P.M. Hui, Dafang Zheng, and M. Hart, J. Phys. A: Math.

Gen. 32, L427 (1999).

(1999).

(2000).

(2001).

34

[23] M.L. Hart, P. Jeﬀeries, N.F. Johnson and P.M. Hui, Phys. Rev. E 63,

[24] P. Jeﬀeries, M. Hart, N.F. Johnson, and P.M. Hui, J. Phys. A: Math.

[25] P. Jeﬀeries, M.L. Hart and N.F. Johnson, Phys. Rev. E 65, 016105

[26] N.F Johnson, D.M.D. Smith and P.M. Hui, Europhys. Lett. (2006) in

[27] S. Bieniawski and I.M. Kroo, AIAA Paper 2003-1941 (2003).

[28] I.M. Sobol, A Primer for the Monte Carlo Method (Boca Raton, CRC

017102 (2001).

Gen. 33, L409 (2000).

(2002).

press.

Press, 1994).

35


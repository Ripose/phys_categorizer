6
0
0
2
 
g
u
A
 
7
 
 
]
s
c
i
t
p
o
.
s
c
i
s
y
h
p
[
 
 
1
v
0
7
0
8
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Probabilistic Regularization in Inverse Optical Imaging

Enrico De Micheli

IBF – Consiglio Nazionale delle Ricerche,

Via De Marini 6, 16149 Genova, Italy

Giovanni Alberto Viano

Dipartimento di Fisica – Universit`a di Genova,

Istituto Nazionale di Fisica Nucleare, sez. di Genova,

Via Dodecaneso 33, 16146 Genova, Italy

The problem of object restoration in the case of spatially incoherent illumination

is considered. A regularized solution to the inverse problem is obtained through a

probabilistic approach, and a numerical algorithm based on the statistical analysis of

the noisy data is presented. Particular emphasis is placed on the question of the posi-

tivity constraint, which is incorporated into the probabilistically regularized solution

by means of a quadratic programming technique. Numerical examples illustrating

the main steps of the algorithm are also given.

I.

INTRODUCTION

The inverse problem in optics consists of recovering the object by starting from its image.

It can be regarded as a backward channel communication problem: messages can be conveyed

back from the data set (the image) to reconstruct the signal (the object). The length of these

messages is limited by the noise aﬀecting the imaging process. This fact can be viewed as the

necessity of truncating the eigenfunction expansions associated with the Fredholm integral

equation, which gives the mathematical formulation of the inverse problem in optics. The

latter can be formulated as follows [1]: Consider a one–dimensional object, illuminated by

spatially incoherent radiation and imaged by a perfect optical instrument (i.e., without focus

error) with a rectangular aperture. If we use f (x) to denote the spatial radiance distribution

in the object plane, then the noise free spatial radiance distribution in the image plane is

given by

(Af )(y) =

K(y

x)f (x) dx = g(y) ,

1 6 y 6 1,

−

(1)

1

−1

Z

−

2

(2)

(3)

where

K(x) =

sin2(cx)
πcx2

,

c =

π
R

,

R being the Rayleigh resolution distance. Here we have assumed ﬁnite extent of the object

and the linear magniﬁcation of the optical instrument to be +1.

When g(y) is given in the geometrical region of the image plane, i.e., the interval [

1, 1],

−

the problem of object restoration is equivalent to solving the Fredholm integral equation

of the ﬁrst kind Af = g, where A is a symmetric compact positive deﬁnite operator. The

solution of the integral equation is unique, i.e., the equation Af = 0 has only the solution

A formal solution to the equation Af = g can be given by means of an eigenfunction

f = 0.

expansion, i.e.,

f (x) =

ψk(x),

∞

Xk=1

gk
λk

where gk = (g, ψk) [(

,

) denoting the scalar product in L2(

1, 1)], and ψk(x) and λk are,

·

·

−

respectively, the eigenfunctions and the eigenvalues associated with the operator A. If we

add to the image g a perturbation such as the one produced by the noise measurement, then

the equation Af + n = ¯g = g + n, n(y) being a function describing the noise or the ex-

perimental error, generally has no solution. Accordingly, the expansion

∞
k=1(¯gk/λk)ψk(x),
[¯gk = (¯g, ψk)] diverges if ¯g does not belong to the range of the operator A. We are faced

P

with the pathology of the ill–posedness of the problem, in the sense of Hadamard [2]: small

perturbations of the data produce wide oscillations of the solutions. The problem requires

regularization. One of the most popular methods in use is due to Tikhonov [3, 4] and con-

sists of restricting the solution space by imposing suitable global bounds on the solutions.

Within the framework of the eigenfunction expansions, Tikhonov’s regularization provides

a criterion for a suitable truncation of the series. Furthermore, the function f (x), which

represents a spatial radiance distribution, is a nonnegative function, and, consequently, a

nonnegativity constraint must be necessarily added in the mathematical formulation of the

problem.

At this point it is worth noting that, although the L2 space is the most natural ambient

when the method of the eigenfunction expansion is used and the Tikhonov’s regularization

is adopted, the L1 space could present several advantages, as we explain below. First, the L1

norm of the intensity is the energy radiated by the object, and therefore an equality of the

3

following type,

f

kL1(−1,1) =

|
clearly be interpreted from a physical viewpoint. Analogously, the quantity

k

f (x)

dx = E, (E is the energy radiated by the object), can

1
−1 |
R

represents the statistical ﬂuctuations of the energy of the image. Finally, the positivity

Af

dy

¯g
|

−

1
−1 |
R

constraint is suﬃcient to restore the continuity of the inverse problem in the topology induced

by the L1 norm (see Appendix A). Conversely, the positivity is not suﬃcient to regularize

the problem in the L2 topology. All these considerations point towards choosing the L1 norm

in the mathematical formulation of the problem. But it can easily be shown, by exhibiting

explicit examples, that the continuity restored in the L1 topology is far from implying a

continuity in the topology induced by the sup norm (see Appendix A). Moreover, it should

be observed that the positivity constraint is suﬃcient to restore the continuity in the L1 norm

but not in the L2 norm, simply because the L2 convergence is more robust [note that f (x) has

compact support] and represents precisely the minimal requirement in the actual numerical

calculations. In view of this last consideration we are then led to choose the L2 norm in the

mathematical formulation of the problem, and, consequently, the eigenfunction expansions

can be used. However, we shall not regularize the problem by using Tikhonov’s variational
kL2(−1,1) 6 M, (M is constant), whose
interpretation is not clear from the physical viewpoint. Instead, we work out the problem

method, which requires a bound of the type:

k

f

with a probabilistic approach, which makes it possible to split the Fourier coeﬃcients of

the noisy data function [i.e., ¯gk = (¯g, ψk)] into two classes: one comprises those coeﬃcients

from which a signiﬁcant amount of information can be extracted; the other, those Fourier

coeﬃcients that can be regarded as random numbers because the noise prevails on the

information content. We can thus construct an approximation that satisﬁes the requirements

of the probabilistic regularization as explained in Subsection II A. Questions related to the

actual numerical computation of the probabilistically regularized solution are discussed in

Subsection II B, where an algorithm based on the analysis of the autocorrelation function of

the noisy data is presented. However, at this stage, the positivity constraint still remains to

be satisﬁed. This point is addressed in Section III, where the numerical issues concerning

the construction of a nonnegative approximation are discussed, and examples of numerical

tests are also given. Finally, in Appendix A, the ill–posedness of the problem in various

topologies and the role played by the positivity constraint will be illustrated.

4

(4)

(5)

II. PROBABILISTIC REGULARIZATION AND STATISTICAL METHODS

A. Probabilistic Regularization

As remarked in Section I, the ill–posed character of the inverse problem is derived precisely

from the fact that the data function g(y) is corrupted by the noise that, hereafter, will be

represented by a bounded function n(y), which is supposed to be integrable in the interval

1, 1]; i.e., ¯g = g + n. Furthermore, we set

[
−

¯g

k

−

g

kL2(−1,1) =

kL2(−1,1) 6 ǫ.
n

k

In general, the perturbation produced by the noise is such that the noisy data function ¯g

does not belong to the range of the integral operator A, and, consequently, the eigenfunc-

tion expansion

∞
k=1(¯gk/λk)ψk generally diverges. Further, even assuming that the noise
perturbation is gentle enough for ¯g to belong to the range of the operator A, in this case the

P

noise n still precludes us from exactly knowing the noiseless data function g. If we have two

f (1) and f (2), if the data distance is less than ǫ, i.e., if

distinct data functions ¯g(1) and ¯g(2), we may not attribute to them two distinct solutions
kL2(−1,1) 6 ǫ. Accordingly,
¯g(1)
kL2(−1,1) > ǫ. In
we can consider to be distinguishable only those data such that
k
∞
k=1(¯gk/λk)ψk converges in the L2 norm, it is meaningless to
push the eigenfunction expansion beyond a certain value k0 (i.e., a certain truncation point)

conclusion, even if the series

¯g(2)

¯g(1)

¯g(2)

−

−

k

P

that depends on ǫ.

The most intuitive and simple truncation method yielding a regularized solution consists

of writing an approximation of the following type,

f0(x) =

ψk(x),

k0(ǫ)

¯gk
λk

Xk=1
where k0(ǫ) is the largest integer such that λk > ǫ [assuming, in this particular case, the a
priori bound that the set of the input signals belongs to the unit ball in L2(

1, 1)]. In fact,

−

it can be proved [5, 6, 7] that f0(x) converges weakly to f (x) even if ¯g does not belong to

the range of the operator A, i.e.,

(f

lim
ǫ→0

−

f0, v)L2(−1,1) = 0 ,

v
[
∀

∈

L2(

1, 1),

−

v

kL2(−1,1) 6 1].

k

(6)

Remark: If ¯g

range (A), then it can be proved by use of the Kolmogorov ǫ–entropy theory

∈

[8] that k0(ǫ) is strictly related to the maximal length of the messages Lmax(ǫ) that can be

5

(7)

conveyed back through the communication channel associated to the operator A; in fact, we

can prove that, for suﬃciently small ǫ (see also, Ref. 9),

Lmax(ǫ)

2k0(ǫ) log2(1/ǫ).

≃

But the approximation f0(x) presents several defects:

1. The solutions must be restricted to a bounded subset such as the unit ball in L2(

or, equivalently, to a bounded subset of the type:

whose physical interpretation is not transparent, as noted in Section I.

1, 1)
kL2(−1,1) 6 M, M = constant,

−

f

k

2. Only a weak convergence of f0(x) to f (x) is guaranteed, whereas at least the L2–norm

convergence should be required for practical applications. In addition, f0(x) does not

generally satisfy the positivity constraint.

3. The truncation criterion λk > ǫ (or λk > ǫ/M) does not guarantee that the approxi-

mation f0(x) really does pick out the Fourier components of the noisy data that are

likely to carry exploitable information about the unknown solution and, at the same

time, reject the ones dominated by the noise.

To overcome all these diﬃculties, we turn the problem in a probabilistic form. With this

in mind we rewrite integral equation (1) in the following form:

Aξ + ζ = η,

(8)

where ξ, ζ and η, which correspond to f , n and ¯g respectively, are Gaussian weak random

variables in the Hilbert space L2(

1, 1) (see Ref. 10). A Gaussian weak random variable

−

is uniquely deﬁned by its mean element and its covariance operator; in the present case we

use Rξξ, Rζζ, and Rηη to denote the covariance operators of ξ, ζ and η, respectively.

Next, we make the following assumptions:

I. ξ and ζ have zero mean; i.e. mξ = mζ = 0.

II. ξ and ζ are uncorrelated, i.e. Rξζ = 0.

III. R−1

ζζ exists.

The third assumption is the mathematical formulation of the fact that all the components of

the data function are aﬀected by noise. As shown by Franklin [see formula (3.11) of Ref. 11],

if both signal and noise satisfy assumptions (I) and (II), then Rηη = ARξξA⋆ +Rζζ, where A⋆
indicates the adjoint of A, and the cross-covariance operator is given by: Rξη = RξξA⋆. We

also assume that Rζζ depends on a parameter ǫ that tends to zero when the noise vanishes,
i.e., Rζζ = ǫ2N, where N is a given operator (e.g., N = I for white noise).

At this point, we turn Eq. (8) into an inﬁnite sequence of one-dimensional equations by

means of orthogonal projections

λkξk + ζk = ηk,

(k = 1, 2, ...),

(9)

where ξk = (ξ, ψk), ζk = (ζ, ψk), ηk = (η, ψk) are Gaussian random variables. Next, we

introduce the variances ρ2

k = (Rηηψk, ψk),
without assuming that the Fourier components ξk of ξ (and analogously also for ζk and ηk)

k = (Rξξψk, ψk), ǫ2ν2

k = (Rζζψk, ψk), λ2

k + ǫ2ν2

kρ2

are mutually uncorrelated. In view of assumptions (I) and (III) the following probability

densities for ξk and ζk can be assumed:

pξk(x) =

pζk(x) =

1
√2π ρk
1
√2π ǫνk

exp

exp

−

(cid:20)

(cid:18)

x2
2ρ2

k (cid:19)(cid:21)
x2
2ǫ2ν2

−

(cid:20)

(cid:18)

k (cid:19)(cid:21)

,

(k = 1, 2, . . .),

,

(k = 1, 2, . . .).

Using Eq.

(9), we can also introduce the conditional probability density pηk (y

x) of the

|

random variable ηk for ﬁxed ξk = x, which reads as

pηk(y

x) =

|

1
√2π ǫνk

exp

(y

λkx)2

−
2ǫ2ν2
k

=

1
√2π ǫνk

(cid:21)

−

(cid:20)

exp

"−

λ2
k
2ǫ2ν2

x

−

k (cid:18)

2

y
λk (cid:19)

.

#

Let us now apply the Bayes formula that provides the conditional probability density of ξk

given ηk through the following expression:

pξk (x
|

y) =

pξk(x)pηk (y
pηk (y)

|

x)

.

Thus, if a realization of the random variable ηk is given by ¯gk, formula (13) becomes

pξk (x
|

¯gk) = Ak exp

x2
2ρ2

k (cid:21)

−

(cid:20)

exp

"−

λ2
k
2ǫ2ν2

x

−

k (cid:18)

2

¯gk
λk (cid:19)

.

#

Now, the amount of information on the variable ξk, which is contained in the variable ηk,

can be evaluated. We have [12]

J(ξk, ηk) =

1
2

−

ln(1

r2
k),

−

6

(10)

(11)

(12)

(13)

(14)

(15)

where

Thus, we obtain

the following sets:

r2
k =

2

E
|
{
2
ξk|
{|

ξkηk} |
E
{|
}

2

ηk|

}

E

=

(λkρk)2
(λkρk)2 + (ǫνk)2 .

J(ξk, ηk) =

ln

1 +

1
2

(cid:18)

λ2
kρ2
k
ǫ2ν2

k (cid:19)

.

Therefore, if λkρk < ǫνk, then J(ξk, ηk) < (ln 2)/2. Thus we are naturally led to introduce

=

k : λkρk > ǫνk}

{

,

I

=

k : λkρk < ǫνk}

{

.

N

If we revert to the conditional probability density (14), this can be regarded as the
product of two Gaussian probability densities: p1(x) = A(1)
A(2)
(¯gk/λk)]2

x2/2ρ2
k) and p2(x) =
k ), whose variances are given by ρ2
k
, the variance associated with the

, (Ak = A(1)
k
·
−
and (ǫνk/λk)2, respectively. Let us note that, if k

k/2ǫ2ν2

k exp (

k exp

k) [x

A(2)

(λ2

−

−

(cid:8)

(cid:9)

∈ I

density p2(x) is smaller than the corresponding variance of p1(x) and vice versa if k

Therefore it is reasonable to consider as an acceptable approximation of

given by the density p2(x) if k

, but the mean value given by the density p1(x) if k

We can then write the following approximation:

∈ I

∈ N
the mean value

ξki

h

¯gk/λk

=

ξki

h




0

k

k

∈ I

.

∈ N

¯gk
λk

ψk.

B¯g =

b

Xk∈I

ξ

Bη

2

k

−

k
n

b

Consequently, given the value ¯g of the weak random variable η, we are led to the following



estimate of ξ, which, using the notation of Ref. 5, reads as

Next, consider the global mean square error E

associated with the operator

B, introduced in (20). We can now prove the following theorem:

o

b
Theorem 1 If

the

covariance operator Rξξ

is of

trace

class,

and furthermore

limk→∞(λkρk/νk) = 0, then the following limit holds true:

Proof: See Ref. 5.

E

lim
ǫ→0

ξ

k

−

Bη

2

k

= 0.

n

o

b

7

(16)

(17)

(18)

.

.

∈ N

(19)

(20)

(21)

(cid:3)

B. Statistical Analysis of the Noisy Data

8

The application of the results achieved in Subsection II A calls for statistical tools able to

determine the two sets

and

. In this section this issue is discussed, and the basic steps

I

N

of a numerical algorithm for constructing the regularized solution

B¯g from the noisy data ¯g

Splitting the Fourier coeﬃcients into the sets

and

can be performed by computing

the correlation function of the random variables ηk, i.e., the probabilistic counterpart of the

b

I

N

are outlined.

coeﬃcients ¯gk,

∆η(k1, k2) =

E

{

E
{
(ηk1 −
{

)(ηk2 −
ηk1}
1/2 E
{
}

(ηk1 −
)
E
}
{
)2
ηk2}
ηk1}
E
{
N
1 of the random variables ηk is usually available,
¯gk}

ηk2}
E
{
(ηk2 −
E
{

1/2 .

(22)

)2

}

In practice, only a ﬁnite realization

from which estimates δ¯g of the autocorrelations can be obtained by regarding the data
¯gk}
{
principle, the assumption of stationarity of the series

N
1 as a ﬁnite length record of a wide–sense–stationary random normal series [13]. In

is not strictly true, because the

ηk}

{

moments of the random variables ηk generally depend on k, but, from the practical point of

view, this is usually the only possible option. However, the stationarity assumption can be

removed whenever estimates of ensemble averages of the series

can be computed. Thus,

ηk}

{

by recalling that the ηk’s are normally distributed, by introducing the working hypothesis

that the process

ηk}

{

is stationary in wide sense [14], i.e., ∆η(k1, k2) = ∆η(k1 −

k2), and

by assuming that the ensemble contains no strictly stationary subensembles that occur

with probability other than zero or one, we can compute estimates of the autocorrelation

coeﬃcients by means of the ergodic hypothesis [14] equating ensemble and time (i.e., the

Among the numerous estimators of the autocorrelation function [15], one which is widely

index k in our case) averages.

used by statisticians is given by

N −n

(¯gk − h

¯gki

)(¯gk+n − h

¯gk+ni

)

(¯gk+n − h

¯gk+ni

)2

)

where

δ¯g(n) =

Xk=1
(¯gk − h

)2

¯gki

N −n

(

Xk=1

N −n

Xk=1

=

¯gki

h

N

1

−

n

N −n

Xk=1

¯gk,

1/2 ,

n = 0, ..., N

1,

(23)

−

¯gk+ni

h

=

N

1

−

n

N −n

Xk=1

¯gk+n.

(24)

9

−

(25)

(26)

(27)

Equation (23), which is based on the scatter diagram of ¯gk+n against ¯gk for k = 1, .., N

n,

represents the maximum–likelihood estimate of the autocorrelation coeﬃcients of two ran-

dom variables ηk and ηk+n whose joint probability distribution function is bivariate normal.

To identify the structure of the series

N
1 so as to separate the correlated components
from the the random ones, it is necessary to test whether δ¯g(n) is eﬀectively zero. This

¯gk}

{

question has been extensively discussed in Ref. 5. Here, we brieﬂy report on the main points.
First we assume that there exists an index n0 such that for n > n0, ∆η(k1 −
vanish. This index n0 is actually recovered recursively as follows: the series

k2) = ∆η(n) will

N
1 is ﬁrst

¯gk}

{

supposed to be purely random, i.e., n0 = 0, the standard error σδ(n; 0) is computed, and

the smallest index n > 0 such that

δ¯g(n)

> 1.96 σδ(n; 0) is searched for. If such an index n

|

|

is found, it becomes the new candidate n0, i.e., we set n0 = n, and the whole procedure is

repeated until no new index n is found. The large-lag standard error σδ(n; n0) is evaluated

by using the following formula, due to Bartlett [16]:

σδ(n, n0) =

1 + 2

δ2
¯g(v)

,

for n > n0.

1

−

N

(

n "

n0

v=1
X

1/2

#)

Formally, n0 can be deﬁned as

n0 = max

n > 0 :

{

n

∀

∈

(n, N

1],

δ¯g(n)

< 1.96 σδ(n, n)

−

|

|

.

}

Accordingly, the set Q of the lags corresponding to autocorrelation values eﬀectively diﬀerent

from zero is deﬁned as

coeﬃcients deﬁned by

Q =

0 < n 6 n0 :

δ¯g(n)

|

> 1.96 σδ(n, 0)
}

.

|

{

Let NQ be the cardinality of Q. From Q we can construct NQ families Fi of pairs of Fourier

Fi =

(¯gki, ¯gki+ni)

{

(N −ni)
ki=1
}

, ni ∈

Q,

i = 1, ..., NQ,

(28)

from which the couples of coeﬃcients ¯gk that are likely to be correlated should be selected.

At this point, the Fourier coeﬃcients that are correlated are determined in a unique way by
means of the following heuristic criterion: for any ni ∈
(¯gk⋆
δ¯g(ni), i.e., we deﬁne k⋆

i +ni) giving the maximum contribution to the corresponding autocorrelation estimate

Q, i = 1, ..., NQ, we select the pair

i , ¯gk⋆

i as

k⋆
i = arg max

k∈[1,N −ni] {|

¯gk ¯gk+ni|}

,

i = 1, ..., NQ.

(29)

10

(30)

(31)

Accordingly, we can deﬁne the set of frequencies

that exhibit correlated Fourier coeﬃcients

=

k⋆
i }

NQ
1 ∪ {

k⋆
i + ni}

{

NQ
1

,

I

I

¯gk
λk

where each element of

is counted only once. Finally, we can construct the approximation

I

I

In theory, that is, for N

are mutually constrained.

fI(x) =

ψk(x).

Xk∈I
, the NI elements ki ∈ I
→ ∞
In fact, any two coeﬃcients kα, kβ ∈ I

and the NQ numbers ni ∈

Q

must satisfy the pair-

wise compatibility constraint requiring

kα −
the number NI of the admissible Fourier coeﬃcients is combinatorially constrained by

it easy to see that

Q. Moreover,

kβ| ∈

|

1 + (1 + 8NQ)1/2

1
2
ﬁnite, and in particular when the signal-to-noise ratio (SNR) of the data ¯g is small, we

6 NI 6 NQ + 1. In practice, that is, when the record length N is

(cid:3)

(cid:2)

cannot demand that all the compatibility constraints be satisﬁed. However, checking the

number of compatibility constraints can provide us with a conﬁdence test on the reliability

of the approximation fI(x).

It is worth noting that, although we used the same notation, the set

of Eq. (30) can be

I

diﬀerent from the set

of Eq. (18). The former is actually the result of an algorithm acting

on a given set of data and can be thought of as a numerical realization of the theoretical set

of (18). A similar role is played by the numerical approximation fI(x) with respect to the

I
theoretical approximation

B¯g of Eq. (20).

b
III. NUMERICAL EXAMPLES

In Section II we illustrated a statistical procedure that allows us to split the Fourier

coeﬃcients into the two classes

and

and, accordingly, to write the approximation fI(x)

I

N

[see formula (31)], which, for the sake of convenience, can be rewritten as

fI(x) =

ψm(x),

(32)

mI (ǫ)

m=1
X

¯gm
λm

where mI(ǫ) represents the cardinality of the set

λk}k∈I (and the cor-
and the eigenvalues
ψk}k∈I) have been suitably relabelled in a monotonic decreasing

I

{

{

responding eigenfunctions

sequence.

In general, the approximation fI(x) does not satisfy the positivity constraint. How to

incorporate eﬀectively the positivity constraint into a regularizing scheme remains an open

question, which has been extensively discussed in the literature (see, for instance, Refs.

17, 18, 19, 20 and the references therein).

If fI(x) is not already a nonnegative function (see Fig. 1), we can aim at constructing a

new positive approximation starting from the function fI(x) itself. The eventual negative

the coeﬃcients

part of fI(x) is mainly due to two factors: (i) the perturbation due to the noise that aﬀects
¯gm}m∈I; (ii) the error due to the truncation of the series expansion. In the
absence of additional prior information neither of these sources of error can be removed,

{

but we can nevertheless look for another approximation that is nonnegative and similar to

fI(x), according to suitable criteria. With this in mind, our task is now to seek a positive

regularizing solution of the following type:

f (p)
I (x) = fI(x) +

dmψm(x),

(33)

mI

Xm=mI (ǫ)+1

where mI is an integer parameter determining the maximum number of eigenfunctions ψm(x)
that can be used to achieve the positive solution f (p)
the unknown coeﬃcients to be determined by requiring the function f (p)
tive. The function f (p)

I (x) in Eq. (33) diﬀers from the approximation fI(x), resulting from

I (x) and the coeﬃcients dm represent

I (x) to be nonnega-

the analysis of the autocorrelation function, in the corrective ﬁnite linear combination of

eigenfunctions ψm(x), whose purpose is to approximate and somehow compensate the errors

leading to the negative part of fI(x).

This kind of approach quite naturally leads us to formulate the problem of ﬁnding the

coeﬃcients dm as a mathematical programming problem [21], i.e., choosing values of a set of

variables subject to various kinds of constraints placed on them. In particular, in our case

we adopt a quadratic programming scheme that can be summarized as follows: Minimize

subject to the constraints

F (d) =

f (p)
I (x)

fI(x)

2
L2(−1,1),

k

−
d = (dmI +1, dmI +2, . . . , dmI ),

k

f (p)
I (xi) > 0 ,

i = 1, 2, . . . , Np,

11

(34)

(35)

(36)

12

where

xi}

{

Np
i=1 is a given set of points distributed on the interval [

1, 1], and

−

λmdm −

|

¯gm|

6 ǫm ,

m = mI + 1, . . . , mI,

(37)

where ǫm represents an upper bound on the mth Fourier component of the noise n(y),

computed with respect to the basis ψm. In practice only a global bound ǫ on the noise is

usually available, whereas the bounds ǫm on the single Fourier components are unknown.

This leads us to substitute, a bit arbitrarily, ǫ for the ǫm’s, thus allowing the coeﬃcients dm

to vary on a wider range. However, it will be shown in the numerical examples that this

question is not a major issue, since the resulting products λmdm generally diﬀer from the

corresponding ¯gm much less than ǫ.

Objective function (34) is of a type that reﬂects our strategy: We look for a positive

solution that is closest, in the sense of the L2 norm, to the approximation fI(x), which

is not necessarily a positive solution to our problem. Constraints (36) simply express the

explicit requirement of positivity on a selected set of points. Regarding the constraints (37),

they basically require the coeﬃcients dm to be compatible with the Fourier coeﬃcients of

the data ¯gm within the noise level. Concerning the actual numerical implementation of the

algorithm, eigenvalues and eigenfunctions of operator A [see Eq. (1)] were computed nu-

merically for diﬀerent values of the parameter c by using the Gauss–Legendre quadrature

[22] and subsequently by diagonalizing the discretized problem by means of standard rou-

tines [22]. The constrained optimization procedure was built around the routine E04NCF

from the Nag Library. For every test function f (x), the corresponding data function g(y)

was computed with Eq. (1), and then noise was added. For simplicity we used only data

corrupted by white noise simulated by computer generated random numbers uniformly dis-

tributed in the interval [

ǫ, ǫ]. However, provided the assumption of independence between

ξ and ζ (see Subsection II A), more general cases involving colored noise could be treated

by using suitable methods such as prewhitening transformations [23], whose discussion is

beyond the scope of this paper. Finally, the performance of the algorithm is evaluated by

direct comparison between the reconstructed solution and the true solution f (x).

In Fig. 1 the main steps of the analysis outlined in Subsection II B are shown. The sample
x) sin2[4(1+x)] with noise boundary ǫ = 10−3 and corresponding SNR

function is f1(x) = (1

40 dB. The ﬁrst noiseless Fourier coeﬃcients gk of the image function, computed by using

≃
the kernel K(x, y) with c = 20, are plotted in Fig. 1A (see the legend for numerical details).

−

−

13

Figure 1B shows the behaviour of the autocorrelation function δ¯g(n) along with the lines that

indicate the statistical conﬁdence limits we used to discriminate whether the autocorrelation

function is substantially null. Note that the autocorrelations at n = 15, 16 have been

correctly rejected, in spite of their quite large value, since they were abnormally inﬂated by

the autocorrelations with n < 15. The approximation fI (dashed curve), computed from the

set

obtained by analysis of the autocorrelation shown in Fig. 1B, and the actual object

I

function f1(x) (solid curve) are displayed in Fig. 1C. The excellent approximation supplied

by fI(x) is clearly evident. Moreover, the approximation does not require any additional

processing, since it is also nonnegative.

in Fig. 1D. The criterion λk > ǫ/M with M =

For the sake of comparison, the approximation f0(x), deﬁned by Eq. (5), is reported
kL2(−1,1) gives k0(ǫ) = 29 as trun-
cation index. However, f0(x) computed with this value of k0 (not plotted) yields a very

f1(x)

k

unsatisfactory approximation of the actual solution, and this failure can be ascribed to the

convergence, only of weak type, of the approximation f0(x) [see Eq. (6)]. In this case, the

sole constraint on the norm of the solution is not suﬃcient for regularizing the problem;

therefore additional a priori information, for instance, on the ﬁrst derivative of the solution,

would be needed to achieve an acceptable approximation. Further prior information would

lead to a smaller value of k0(ǫ) and, consequently, to good solutions, as shown in Fig. 1D,

where an excellent reconstruction obtained with k0(ǫ) = 27 (dashed curve) is shown.

In

contrast, with k0(ǫ) = 28 (dashed–dotted curve) wild oscillations start appearing.

In general, fI(x) does not satisfy the positivity constraint, in particular when the SNR

becomes small, and so the approximation f (p)
the algorithm for constructing the positive regularized solution f (p)
I

I (x) must be computed. The basic points of

are summarized in Fig.

2 for the sample function f2(x) = exp[

(x

x0)2/2σ2] + exp[

(x + x0)2/2σ2] with x0 = 0.5

and σ = 0.1. In this example, ǫ = 3

6.2 dB. Since the eigenvalues of the

−

−
10−2, and SNR

×

≃

−

operator A tend to decrease almost linearly with respect to the order index when k < 4c/π

(i.e., k < 26 in this example), whereas for k > 4c/π they go to zero exponentially fast [24],

we expect to recover quite accurately the object–function f2(x), which is characterized as

having the bulk of its information localized in the ﬁrst values of k, even in the presence of

such a small SNR. The analysis of the autocorrelation function δ¯g(n), shown in Fig. 2B, leads

to selection of the Fourier components

=

1, 3, 5, 7, 9

with a high degree of conﬁdence,

I

{

}

since every element of

satisﬁes all the compatibility constraints (see Subsection II B).

I

14

Moreover, note that only odd components were picked up, yielding an approximation with

the same parity of the original object function. Although the reconstruction fI, illustrated

in Fig. 2C, is quite accurate, it presents regions where it takes nonsense negative values.

Figure 2D shows the result of the constrained optimization procedure we used to achieve

a positive solution. In this case only mI

mI = 8 corrective components have been used,

−

while the positivity constraint has been imposed on Np = 64 equidistant points of the

interval [

1, 1]. It is worth noting that the parameter mI, which determines the number of

−

corrective terms in the solution (33), is arbitrary and must be set manually, with the only

condition being that the constrained optimization problem has a nonempty feasible region.

Therefore diﬀerent values of mI can lead to diﬀerent positive problem solutions.

In this

example we have shown the solution corresponding to the smallest value of mI which gives

a minimum of the objective function (34) compatible with constraints (36) and (37).

Figures 3 and 4 sketch the results of two further examples. In Fig. 3C the unconstrained

reconstruction fI(x), which represents the starting point of the optimization procedure, is

less accurate than in the previous example. By imposing the positivity constraint we can

see that a higher accuracy in the reconstruction of the true solution f2(x) was achieved in

the regions where fI(x) was already positive (see the two leftmost peaks in Fig. 3D).

Finally, Fig. 4 illustrates the restoration of an edge–type object. Evidently, reconstruct-

ing discontinuous functions within a regularizing framework based on a truncated Fourier

expansion is a diﬃcult task because of the Gibbs–like phenomenon. However, the overall

regularizing procedure, including the positivity constraint on the solution, can provide an

acceptable solution, as shown in Fig. 4D. In this case the role of the parameter mI is quite

critical. The price to pay for reaching a reasonable solution is that many corrective terms

must be used (in this example mI = 70) and also that diﬀerent feasible values of mI can

yield quite diﬀerent reconstructions.

15

(A.1)

(A.2)

(A.3)

Appendix A

Let us consider the following set of functions

fn(x)

{

∞
n=1:

}

fn(x) =

1/p

np + 1
2

(1

x
|

− |

)n ,

x

[
−

∈

1, 1] ,

(p > 1).

(cid:19)
They are nonnegative functions if x

(cid:18)

1, 1] and we also have

∈

=

[
−
np + 1
2
(cid:18)
(p > 1),

1/p

(cid:19)

1/p

|

fn(x)

sup
x∈[−1,1] |
fnkLp = 1 ,
k
6 const.
Afn|

|

,

(p > 1),

−→n→∞ ∞

.

1
n + 1

np + 1
2
AfnkLp = 0, if p > 1. Then, for any nonnegative

(A.4)

(cid:19)

k

(cid:18)
From relation (A.4) it follows limn→∞

function f

L1(

1, 1), we have

∈

−

Af

kL1 > l

f

kL1 ,

k

k

l = inf

x∈[−1,1]

K(x, y) dy > 0,

(A.5)

1

−1

Z

and from relation (A.5) it follows that the operator A−1 is continuous in the topology of the

L1 norm.

but not in the Lp topology if p > 1.

We have thus proved that the positivity constraint restores continuity in the L1 topology

Acknowledgments

We thank E.R. Pike for valuable discussions and helpful suggestions.

[1] M. Bertero and P. Boccacci, Introduction to Inverse Problems in Imaging (Institute of Physics

[2] J. Hadamard, Lectures on the Cauchy Problem in Linear Diﬀerential Equations (Yale Univer-

[3] C. W. Groetsch, The Theory of Tikhonov Regularization for Fredholm Equations of the First

Publishing, Bristol, UK, 1998).

sity, New Haven, Conn., 1923).

Kind (Pitman, Boston, Mass., 1984).

1976).

[4] A. Tikhonov and V. Arsenine, M´ethodes de R`esolution de Probl´emes Mal Pos`es (Mir, Moscow,

16

[5] E. De Micheli, N. Magnoli and G. A. Viano, “On the regularization of Fredholm integral

equations of the ﬁrst kind,” SIAM J. Math. Anal. 29, 855–877 (1998).

[6] K. Miller, “Least square methods for ill–posed problems with a prescribed bound,” SIAM

(Soc. Ind. Appl. Math.) J. Math. Anal. 1, 52–74 (1970).

[7] K. Miller and G. A. Viano, “On the necessity of nearly-best-possible methods for analytic

continuation of scattering data,” J. Math. Phys. 14, 1037–1048 (1973).

[8] A. N. Kolmogorov and V. M. Tihomirov, “ǫ–entropy and ǫ–capacity of sets in functional

[9] E. Scalas and G. A. Viano, “Resolving power and information theory in signal recovery,” J.

spaces,” Uspekhi 14, 3–86 (1959).

Opt. Soc. Am. A 10, 991–996 (1993).

[10] A. V. Balakrishnan, Applied Functional Analysis (Springer-Verlag, New York, 1976).

[11] J. N. Franklin, “Well–posed stochastic extensions of ill–posed linear problems,” J. Math. Anal.

Appl. 31, 682–716 (1970).

[12] I. M. Gel’fand and A. M. Yaglom, “Calculation of the amount of information about a random

function contained in another such function,” Am. Math. Soc. Trans. Ser. 2 12, 199–246

[13] J. L. Doob, Stochastic Processes (Wiley, New York, 1953).

[14] D. Middleton, An Introduction to Statistical Communication Theory (McGraw–Hill, New

[15] G. M. Jenkins and D. G. Watts, Spectral Analysis and Its Applications (Holden–Day, San

[16] M. S. Bartlett, Stochastic Processes - Methods and Applications, 3rd ed. (Cambridge U. Press,

(1959).

York, 1960).

Francisco, Calif., 1968).

Cambridge, UK, 1978).

[17] M. Bertero and V. Dov´ı, “Regularized and positive–constrained inverse methods in the prob-

lem of object restoration,” Opt. Acta 28, 1635–1649 (1981).

[18] G. de Villiers, B. McNally, and E. R. Pike, “Positive solutions to linear inverse problems,”

Inverse Probl. 15, 615–635 (1999).

[19] B. McNally and E. R. Pike, “Quadratic programming for positive solutions of linear inverse

problems,” in Proceedings of the Workshop on Scientiﬁc Computing, F.T. Luk and R. J.

Plemmons, eds. (Springer, Berlin, 1997), pp. 101–109.

[20] M. Piana and M. Bertero, “Projected Landweber method and preconditioning,” Inverse Probl.

17

13, 441–463 (1997).

[21] M. S. Bazaraa, H. D. Sherali, and C. M. Shetty, Nonlinear Programming – Theory and Algo-

[22] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical Recipes

rithms, 2nd ed. (Wiley, New York, 1993).

(Cambridge U. Press, Cambridge, UK, 1992).

[23] G. E. P. Box and G. M. Jenkins, Time Series Analysis (Holden–Day, San Francisco, Calif.,

1976).

[24] F. Gori, “Integral equations for incoherent imagery,” J. Opt. Soc. Am. 64, 1237–1243 (1974).

A

B

δ
_|
g

|

18

g

k

0.6

0.4

0.2

0

-0.2

-0.4

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

C

f(x)

k

0
x

0.8

0.6

0.4

0.2

0

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

1

3

5

7

9

11

13

15

17

19

0

2

4

6

8

10 12 14 16 18

n

D

f (x)
0

0

-1

-0.5

0.5

1

-0.2

-1

-0.5

0
x

0.5

1

≃

−

=

x) sin2[4(1 + x)], ǫ = 10−3, c = 20. The global SNR, deﬁned
FIG. 1: Example 1: f1(x) = (1
40 dB. A,
as the ratio of the mean power of the noiseless data to the noise variance, was SNR
Noiseless Fourier coeﬃcients gk. B, Modulus of the autocorrelation function. From the analysis of
δ¯g(n) we have n0 = 14, Q =
, each element with
1, 2, 3, 4, 6, 7, . . . , 14, 15
.
1, 2, . . . , 13, 14
}
{
I
}
{
maximum inner compatibility, i.e., 13. Horizontal straight line, 95% conﬁdence limit 1.96σδ(n; 0)
for a purely random sequence. This limit was used to select the elements of Q for n 6 n0 ≡
14
(solid part). Curve, conﬁdence limit 1.96σδ(n; 14) for n > n0 (solid part) that we used for rejecting
the autocorrelations that are spuriously inﬂated by statistical ﬂuctuations, whereas for n 6 n0
(dashed part) it shows only how the ﬁnal conﬁdence limit 1.96σδ(n; 14) was reached during the
maximization procedure for setting n0 [see text and, in particular, Eq.
(26)]. C, Regularized
solution. Solid curve, actual solution f1(x). Dashed curve, reconstruction fI(x). D, Two examples
of the approximation f0(x) [see Eq. (5)]. k0(ǫ) was chosen as the largest integer such that λk > ǫ/M ,
kL2(−1,1), that is, in the current case, k0 = 29. Solid curve, actual solution f1(x).
where M =
Dashed curve, f0(x) computed with k0 = 27 (see in particular the rightmost peak); dotted–dashed
curve, f0(x) computed with k0 = 28. The approximation f0(x) computed with k0 = 29, as
prescribed by the above truncation criterion, is not displayed since it is extremely diﬀerent from
the real solution.

f1(x)
k

A

B

δ
_|
g

|

19

1

3

5

7

9

11 13 15 17 19

0

2

4

6

8

10 12 14 16 18
n

C

f(x)

D

f(x)

g

k

0.3

0.2

0.1

0

-0.1

-0.2

-0.3

-0.4

0.8

0.6

0.4

0.2

0

k

0
x

0.8

0.6

0.4

0.2

0

0.8

0.6

0.4

0.2

-0.2

-1

-0.5

0.5

1

0

-1

-0.5

0
x

0.5

1

×

≃

−

−

(x

10−2, SNR

x0)2/2σ2] + exp[

(x + x0)2/2σ2] with x0 = 0.5, σ = 0.1,
FIG. 2: Example 2: f2(x) = exp[
6.2 dB, c = 20. A, Noiseless Fourier coeﬃcients gk. B, Modulus of the
ǫ = 3
autocorrelation function. n0 = 8, Q =
. C, Unconstrained regularized
1, 3, 5, 7, 9
,
2, 4, 6, 8
}
{
}
{
solution. Solid curve, actual solution f2(x). Dashed curve, reconstruction fI(x). D, Comparison
between the actual solution f2(x) and the constrained regularized solution f (p)
I (x) [see Eq. (33)].
mI = 8. The positivity constraint was set
The number of corrective eigenfunctions used was mI
over Np = 64 points of the interval [
1, 1] [see relation (36)]. After the optimization procedure the
coeﬃcients dk were such that maxmI 6m6mI |

−
λm¯gm| ≃

dm −

0.01 < ǫ.

=

−

−

I

20

1

3

5

7

9

11 13 15 17 19

0

2

4

6

8

10 12 14 16 18
n

A

g

k

0.5

1

0

-0.5

-1

-1.5

C

f(x)

7

6

5

4

3

2

1

0

-1

-2

-1

k

0
x

B

δ
_|
g

|

D

f(x)

0.8

0.6

0.4

0.2

0

7

6

5

4

3

2

1

-0.5

0.5

1

0

-1

-0.5

0
x

0.5

1

−

3x) + exp(

1, 2, 4, 5, 6, 7, 8, 10, 11, 12
{

FIG. 3: Example 3: f3(x) = sin2[5(1

10−2, SNR
x)] sin2[5(1 + x)][exp(
16.1dB, c = 20. A, Noiseless Fourier coeﬃcients gk. B, Modulus of the autocorrelation function.
≃
n0 = 12, Q =
has maximum
inner compatibility, i.e., 6, with respect to the set Q. C, Unconstrained regularized solution. Solid
curve, actual solution f3(x). Dashed curve, reconstruction fI(x). D, Comparison between the
actual solution f3(x) and the constrained regularized solution f (p)
I (x) [see Eq. (33)]. The number
mI = 40. The positivity constraint was explicitly
of corrective eigenfunctions used was mI
imposed on Np = 64 points of the interval [
1, 1] [see relation (36)]. After the optimization
procedure the coeﬃcients dk were such that maxmI 6m6mI |

. Each element of
}

1, 2, 3, 7, 8, 9, 13
{

λm¯gm| ≃

x)]. ǫ = 5

dm −

0.02 < ǫ.

,
}

−

=

−

×

−

−

I

I

A

B

δ
_|
g

|

21

1

3

5

7

9

11 13 15 17 19

0

2

4

6

8

10 12 14 16 18
n

C

f(x)

D

f(x)

g

k

0.8

0.6

0.4

0.2

0

-0.2

-0.4

1

0.8

0.6

0.4

0.2

0

k

0
x

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

-0.2

-1

-0.5

0.5

1

0

-1

-0.5

0
x

0.5

1

−

=

FIG. 4: Example 4: f4(x) = 1 for

0.6 6 x 6 0.6 and null elsewhere. ǫ = 8

10−2, SNR
3.2dB, c = 20. A, Noiseless Fourier coeﬃcients gk. B, Modulus of the autocorrelation function.
≃
n0 = 17, Q =
. The set Q led to selection of the Fourier components k = 1, 3, 5, 7, 18.
2, 4, 6, 17
}
{
However, the component at k = 18 has the minimum compatibility index, i.e., 1, which strongly
indicates that the correlation at the lag n = 17 was spuriously generated by the noise. Then,
. C, Unconstrained regularized solution. Solid curve, actual solution f4(x). Dashed
1, 3, 5, 7
}
{
I
curve, reconstruction fI(x). D, Comparison between the actual solution f4(x) and the constrained
regularized solution f (p)
mI = 70. The positivity constraint was set over Np = 64 points of the interval [
(36)]. After the optimization procedure the coeﬃcients dk were such that maxmI 6m6mI |
λm¯gm| ≃

I (x) [see Eq. (33)]. The number of corrective eigenfunctions used was mI

−
1, 1] [see relation
dm −

0.01 < ǫ.

×

−


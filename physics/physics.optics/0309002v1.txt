How to apply importance-sampling techniques

to simulations of optical systems

C. J. McKinstrie and P. J. Winzer

Bell Laboratories, Lucent Technologies, Holmdel, New Jersey 07733

Abstract

This report contains a tutorial introduction to the method of importance sampling.

The use of this method is illustrated for simulations of the noise-induced energy

jitter of return-to-zero pulses in optical communication systems.

3
0
0
2
 
g
u
A
 
9
2
 
 
]
s
c
i
t
p
o
.
s
c
i
s
y
h
p
[
 
 
1
v
2
0
0
9
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

1

When one rolls a die, the probability of it stopping with a particular number face-up is 1/6.

To measure such a probability experimentally and accurately, one must roll the die many

more than 6 times. With this fact in mind, consider the design of optical communication

systems. A common way to evaluate system prototypes is to measure (in computer simu-

lations or laboratory experiments) the distance over which they can transmit information
with bit-error-ratios of order 10−9 (without forward error correction). Even with current

computers, it is impractical to simulate pulse transmission many more than 109 times.

One can use importance-sampling techniques to circumvent this diﬃculty. Put simply, bit

errors are caused by large system perturbations (deviations from ideal behavior) that occur

infrequently. When one makes importance-sampled simulations of system performance one

biases them in such a (controlled) way that these large perturbations occur more often

than they should. Because they occur often, one can measure their statistics accurately.

Subsequently, one adjusts the simulation results to remove their artiﬁcial bias. In this way

one obtains results that are both unbiased and accurate.

Pioneering importance-sampled simulations of phase noise in an optical system were made

by Foschini and Vannuci [1]. Recently, importance-sampled simulations of polarization-mode
dispersion were made by Biondini et al. [2]. Importance-sampling methods were reviewed
comprehensively by Smith et al. [3] and ways to combine data from diﬀerent importance-

sampled simulations were discussed by Veach [4].

In this report we make no attempt to

duplicate their discussions. Rather, we show by example how to apply importance-sampling

techniques to optical systems.

It is instructive to consider the die example quantitatively. Each roll of an unbiased die

is a trial, in which the probability of a successful outcome p = 1/6 and the value associated

with a successful outcome (the weight with which a successful outcome is counted) w = 1.

For each trial the expected value of the outcome is pw and the variance is w2p(1−p). Suppose

that an unbiased die is rolled n1 times and let f1 be the measured 1-frequency (the number of

1s divided by the number of rolls). Then the expected value of the 1-frequency E(f1) = 1/6.

By adding n1 terms of magnitude 5/36 and dividing the result by n2

1 one ﬁnds that the

variance V (f1) = 5/36n1: To measure the 1-frequency with an accuracy of 1% (to conduct

an experiment for which the standard deviation of the 1-frequency is 1% of the expected

2

value) one would have to roll the die 5 × 104 times. This would be a time-consuming task.

If the 5- and 6-frequencies were of no interest one could bias the die by marking the 5-

and 6-faces with 1s. Since this change would increase the probability of rolling a 1 (p =

3/6 = 1/2), one should count each successful roll with reduced weight (w = 1/3). Suppose

that this biased die is rolled n2 times and let f2 be the measured (weighted) 1-frequency.

Then the expected value E(f2) = 1/6, as required, and the variance V (f2) = 1/36n2:

By causing the desired event to occur more often than it would naturally, one is able to

measure its probability more accurately, or with fewer rolls. (For rare events that occur in

applications, the performance improvements are much larger than the factor of 5 associated

with this example.) Of course, the price one pays for this increase in accuracy is the loss of

information about the 5- and 6-frequencies.

If these frequencies were of limited, but ﬁnite, interest, one could combine the measure-

ments made with each die separately. If one were to weight the two measurements equally,

by deﬁning the combined 1-frequency f = (f1 + f2)/2, one would ﬁnd that E(f ) = 1/6 and

V (f ) = 5/144n1 + 1/144n2: The accuracy of the combined measurement is limited by the

less-accurate individual measurement (the one with the larger variance coeﬃcient or the one

made with fewer rolls). One should weight the individual measurements in proportion to the

numbers of rolls used to make them, and in inverse proportion to their variance coeﬃcients.

Let f = c1f1 + c2f2, where the condition c1 + c2 = 1 ensures that E(f ) = 1/6. Then a short

calculation shows that the optimal values of c1 and c2 are n1/(n1 + 5n2) and 5n2/(n1 + 5n2),

respectively, in which case the minimal variance V (f ) = 5/36(n1 + 5n2). For the case in

which n1 = n2 = n/2, where n is the total number of rolls, V (f ) = 5/108n.

In this dice example simple formulas for the individual variances exist, which allow one to

determine the optimal weight coeﬃcients precisely. However, in applications such formulas

might be complicated or unknown. Consequently, a diﬀerent method is required. An eﬀective

method, which is called the balance heuristic [4], is to weight each successful outcome (roll of

either die) equally. In this method, if the unbiased die is rolled n1 times and the biased die

is rolled n2 times, the combined probability of rolling a 1 is (n1 + 3n2)/6(n1 + n2). To ensure

that the expected value of the combined measurement is 1/6, each successful roll (of either

die) should be counted with weight w = (n1 + n2)/(n1 + 3n2). By adding n1 contributions

3

of magnitude w2p1(1 − p1) and n2 contributions of magnitude w2p2(1 − p2), and dividing

the result by (n1 + n2)2, one ﬁnds that the combined variance is (5n1 + 9n2)/36(n1 + 3n2)2.

For the case in which n1 = n2 = n/2 the combined variance is 7/144n, which is only 5%

larger than the minimal variance of the preceding paragraph. Thus, one should weight each

outcome in inverse proportion to its combined probability.

Now consider the noise-induced energy (amplitude) jitter of a return-to-zero (RZ) pulse.

Provided that the pulse energy e is greater than the noise energy in the surrounding bit slot,

it evolves according to the stochastic ordinary diﬀerential equation (ODE)

dze = (g − α)e + r,

(1)

where g(z) is the ampliﬁer gain rate, α is the ﬁber loss rate and r(z) is the rate at which

the energy is changed by ampliﬁer noise [5, 6]. This random rate of change is quantiﬁed by
the equations hr(z)i = 0 and hr(z)r(z′)i = (2nsphνge)δ(z − z′), where h i denotes an ensem-

ble average, nsp is the spontaneous-emission factor (1.1–1.3) and hν is the photon energy.

Equation (1) is valid for any isolated pulse and an arbitrary combination of distributed and

lumped ampliﬁcation.

For deﬁniteness, consider a 10 Gb/s system with uniformly-distributed ampliﬁcation

(g = α), in which α = 0.21 dB/Km, β = −0.30 ps2/Km (D = 0.38 ps/Km-nm) and γ =

1.7/Km-W. Then a soliton with a full-width at half-maximum of 30 ps has an energy of 21 fJ

(time-averaged power of 0.21 mW). If the system length l = 10 Mm the output noise power in

both polarizations, in a frequency bandwidth of 12 GHz (wavelength bandwidth of 0.1 nm), is

1.7 µW: The (optical) signal-to-noise ratio is 21 dB. Systems with nonuniformly-distributed

or lumped ampliﬁcation produce the same noise power in shorter distances.

For uniformly-distributed ampliﬁcation Eq. (1) can be rewritten in the canonical form

dx = x1/2dy,

(2)

where x = e/e0 is the energy, normalized to the equilibrium energy (in the absence of noise),

and y is a Wiener process (Gaussian random variable) with hyi = 0 and hy2i = σ2

s z, where

the normalized source-strength σ2

s = 2nsphνg/e0.

In the linear regime the multiplicative

factor x1/2 ≈ 1, from which it follows that x ≈ 1 + y: The probability-density function

(PDF) of the output energies is Gaussian, with mean 1 and variance σ2

s z. (From a logical

4

standpoint the PDF of the non-negative quantity x cannot be exactly Gaussian, because, if

it were, the probability of x < 0 would be ﬁnite for all z > 0. From a practical standpoint

this inconsistency is tolerable if the probability of x < 0 is exponentially small for system
s l = 6.6 × 10−3
lengths of interest.) For the aforementioned system the output variance σ2
(which is of order 10−2) and the output deviation is 8.1 × 10−2 (which is of order 10−1). In

the nonlinear regime the factor x1/2 modiﬁes the tails of the PDF signiﬁcantly. For reference,

the analytical solution of Eq. (2) has the PDF

P (x) ≈

cosh(mx1/2/v) exp[−(m2 + x)/2v]
(2πxv)1/2

,

(3)

where m = 1 − σ2

s z/8 and v = σ2

s z/4 [7].

Equation (2) and solution (3) model energy jitter in a (continuous) system with uniformly-

distributed ampliﬁcation. We simulated a (discrete) system with ni = 100 lumped ampliﬁers.

Between the ampliﬁers the energy x did not change. At the ith ampliﬁer the energy was

changed (kicked) by the random amount δyi, where the properties hδyii = 0 and hδyiδyji =
i=1 δyi)2i = 10−2: The discrete system had the same characteristics
10−4δij ensured that h(Pni
as the continuous system. The output energies were assigned to energy bins of (common)

width 0.02 and each bin probability pj was deﬁned to be the number of pulses whose energies

fell within the bin boundaries (bin count) divided by the total number of pulses. (Since

probabilities cannot be measured by ﬁnite numbers of trials, these quantities should be

called the relative frequencies associated with the bins. We use the term probabilities as an

abbreviation for the correct term.) To facilitate comparisons to the analytical PDF (3), the

simulation probabilities were deﬁned to be the bin probabilities divided by the bin width.

In these (direct) simulations the occurrence of each output energy was counted with unit

weight.

The PDF associated with an ensemble of 106 pulses is displayed in Fig. 1. Although

the simulation results agree well with Eq. (3) near the peak of the PDF, they do not even

begin to sample the tail of the PDF. On a 1-GHz PC these simulations, which are based

on Eq. (2), take a few minutes. Simulating the transmission of many more than 109 pulses

would take many days. Realistic simulations, which are based on the nonlinear Schroedinger

equation, would take even longer.

To probe the tails of the PDF one must make large energy perturbations occur more

5

often than they would naturally. One way to achieve this goal is to increase the standard

deviation of the energy kicks. Let q0 denote the (common) unbiased kick distribution, with
deviation σ0 = 10−2, and q denote the (common) biased kick distribution, with deviation

σ > σ0. Then, at the ith ampliﬁer the probability that the energy is kicked by the amount δyi

is increased by the factor q(δyi)/q0(δyi). Since the kicks at all the ampliﬁers are biased, the

output energy occurs with a probability that is larger than its natural probability by the total
factor ft = Πni

i=1q(δyi)/q0(δyi), which depends on the full kick sequence (δy1, δy2, . . . , δyni).

One can remove this bias by counting the output energy with reduced weight: One increments

the appropriate bin probability by 1/ft, rather than 1. All other aspects of data counting

remain the same. For reference, the probability factor 1/ft is called the likelihood ratio.

The PDF associated with an ensemble of 106 pulses is displayed in Fig. 2 for the case

in which σ = 1.2σ0. The results of these (importance-sampled) simulations diﬀer from

the previous results in two ways. First, they do probe the tail of the PDF. Although the

simulations reproduce the shape of the analytical PDF, the simulation probabilities are not

accurate because the number of data points that sample the tail of the PDF is still small.

Second, by causing large kicks to happen more often, one causes small kicks to happen

less often. Consequently, the body (peak) of the PDF is not reproduced accurately. This

deﬁciency prevents one from increasing σ until the tail of the PDF is sampled accurately.

Another way to achieve the stated goal is to change the (common) mean of the kick dis-

tributions. If the mean kick µ is positive (negative) the energy drifts toward larger (smaller)

values. The distributions associated with 3 ensembles of 3×105 pulses are displayed in Fig. 3
for cases in which σ = σ0, and µ = −3×10−3, 0 and 3×10−3. These values produce data sets

with mean energies of −0.3, 0.0 and 0.3, respectively. Although the simulation distributions

are inaccurate for energies that are far from their mean energies (because the bin counts are

low), they are accurate near their mean energies. It only remains to combine the individual

distributions to produce a composite distribution that is accurate for the entire domain of

interest.

One way to combine the individual distributions is to weight their bin probabilities equally

(which is equivalent to combining the data sets before sorting the output energies and the

associated probability factors into bins). Let pjk be the jth bin probability associated with

6

the kth data set (which was produced by kick distributions with mean µk). For each j, if the

individual bin probabilities pjk were all zero the combined bin probability pj was deﬁned to

be zero and if some of the pjk were nonzero pj was deﬁned to be their average. The results

of this procedure are shown in Fig. 4. Although the composite distribution does cover the

domain of interest, it is inaccurate near the boundaries of the sample spaces. Combining the

individual distributions with equal weight allows the bodies of the distributions (which have

high bin counts) to be polluted by the tails of neighboring distributions (which have low bin

counts).

The dice example suggests that it is better to weight the bin probabilities according to the

associated bin counts. For each j, if the individual bin counts bjk were all zero the combined

bin probability pj was deﬁned to be zero and if some of the bin counts were nonzero pj was
deﬁned to be Pnk
k=1 bjk. The results of this procedure are shown in Fig. 5. The
composite distribution covers, and is accurate throughout, the domain of interest. This ad-
hoc procedure allows one to combine bin probabilities generated at diﬀerent times, without

k=1 bjkpjk/ Pnk

recourse to the data sets on which they were based.

Although the ad-hoc method works, it is not the balance heuristic. Suppose that the

kick sequence (δy1, δy2, . . . , δyni) occurs during the ﬁrst simulation, which is made using

the biased distribution q1 (with mean µ1). Then the associated probability factor ft1 =
Πni

i=1q1(δyi)/q0(δyi). Were the same sequence to occur during the kth simulation, the associ-

ated probability factor would be ftk, which depends on the biased distribution qk. If the simu-
lations involve the same number of pulses, the combined probability factor ft = Pnk

k=1 ftk/nk.

(It is easy to generalize this formula.) When we made the individual simulations and sorted

the output energies and the individual probability factors into bins, we also sorted the com-

bined probability factors into a separate set of bins, which was common to all the simulations.

The results of this procedure are shown in Fig. 6. The balance-heuristic method works well.

In summary, we showed by example how to apply importance sampling techniques to

simulations of optical communication systems. These techniques are easy to apply and

increase signiﬁcantly the accuracy with which rare events can be simulated.

We acknowledge useful discussions with D. Chizhik, G. Foschini, R. Moore and J. Salz.

7

Postscript: Independent simulations of energy jitter were made recently by Moore et al.

[Opt. Lett. 28, 105 (2003)], who showed that the predictions of the energy equation (1) are

consistent with the results of simulations based on the nonlinear Schroedinger equation.

References

[1] G. J. Foschini and G. Vannuci, “Characterizing ﬁltered light waves corrupted by phase

noise,” IEEE Trans. Inform. Theory 34, 1437–1448 (1988).

[2] G. Biondini, W. L. Kath and C. R. Menyuk, “Importance sampling for polarization-

mode dispersion,” IEEE Photon. Technol. Lett. 14, 310–312 (2002).

[3] P. J. Smith, M. Shaﬁ and H. Gao, “Quick simulation: a review of importance sampling

techniques in communication systems,” IEEE J. Sel. Areas in Commun. 15, 597–613

(1997) and references therein.

Stanford University (1997).

[4] E. Veach, “Robust Monte Carlo methods for light transport simulation,” Ph.D. thesis,

[5] C. J. McKinstrie and C. Xie, “Phase jitter in single-channel soliton systems with con-

stant dispersion,” IEEE J. Sel. Top. Quantum Electron. 8, 616–625 (2002) and references

therein.

[6] C. J. McKinstrie, C. Xie and C. Xu, “Eﬀects of cross-phase modulation on phase jitter

in soliton systems with constant disperison.” Opt. Lett. 28, 604-606 (2003).

[7] This approximate solution was discovered during a collaboration with T. I. Lakoba. Its

derivation will be described elsewhere.

8

0.4

0.6

0.8
1.2
1
Normalized energy

1.4

1.6

F
D
P

r
a
e
n
L

i

4

3

2

1

0

F
D
P
c
m
h

i

t
i
r
a
g
o
L

2

0

-2

-4

-6

-8

0

0.5

1
Normalized energy

1.5

2

Figure 1: Probability distribution function of the (normalized) output energies obtained by

solving Eq. (2) analytically (curve) and numerically, for an ensemble of 106 pulses (dots).
The standard deviation of the energy kicks was 10−2.

9

0.4

0.6

0.8
1.2
1
Normalized energy

1.4

1.6

F
D
P

r
a
e
n
L

i

4

3

2

1

0

2

0

-2

-4

-6

-8

F
D
P
c
m
h

i

t
i
r
a
g
o
L

-10

0

0.5

1
Normalized energy

1.5

2

Figure 2: Probability distribution function of the (normalized) output energies obtained by

solving Eq. (2) analytically (curve) and numerically, for an ensemble of 106 pulses (dots).
The standard deviation of the energy kicks was 1.2 × 10−2.

10

0

0.5

1
Normalized energy

1.5

2

0

0.5

1
Normalized energy

1.5

2

i

F
D
P
c
m
h
t
i
r
a
g
o
L

F
D
P
c
m
h

i

t
i
r
a
g
o
L

F
D
P
c
m
h

i

t
i
r
a
g
o
L

2

0
-2
-4
-6
-8
-10
-12

2

0
-2
-4
-6
-8
-10
-12

2

0
-2
-4
-6
-8
-10
-12

0

0.5

1
Normalized energy

1.5

2

Figure 3: Probability distribution functions of the (normalized) output energies obtained by

solving Eq. (2) analytically (curve) and numerically, for 3 ensembles of 3 × 105 pulses (dots).
For each ensemble the standard deviation of the energy kicks was 10−2. The mean energy
kicks were −3 × 10−3, 0 and 3 × 10−3.

11

0.4

0.6

0.8
1.2
1
Normalized energy

1.4

1.6

F
D
P

r
a
e
n
L

i

4

3

2

1

0

F
D
P
c
m
h

i

t
i
r
a
g
o
L

2

0
-2
-4
-6
-8
-10
-12

0

0.5

1
Normalized energy

1.5

2

Figure 4: Probability distribution function of the (normalized) output energies obtained by

solving Eq. (2) analytically (curve) and numerically, for 3 ensembles of 3 × 105 pulses (dots).
For each ensemble the standard deviation of the energy kicks was 10−2. The mean energy
kicks were −3 × 10−3, 0 and 3 × 10−3. The 3 sets of bin probabilities were combined without

weighting.

12

0.4

0.6

0.8
1.2
1
Normalized energy

1.4

1.6

F
D
P

r
a
e
n
L

i

4

3

2

1

0

F
D
P
c
m
h

i

t
i
r
a
g
o
L

2

0
-2
-4
-6
-8
-10
-12

0

0.5

1
Normalized energy

1.5

2

Figure 5: Probability distribution function of the (normalized) output energies obtained by

solving Eq. (2) analytically (curve) and numerically, for 3 ensembles of 3 × 105 pulses (dots).
For each ensemble the standard deviation of the energy kicks was 10−2. The mean energy
kicks were −3 × 10−3, 0 and 3 × 10−3. When the 3 sets of bin probabilities were combined,

they were weighted according to the associated bin counts.

13

0.4

0.6

0.8
1.2
1
Normalized energy

1.4

1.6

F
D
P

r
a
e
n
L

i

4

3

2

1

0

F
D
P
c
m
h

i

t
i
r
a
g
o
L

2

0
-2
-4
-6
-8
-10
-12

0

0.5

1
Normalized energy

1.5

2

Figure 6: Probability distribution function of the (normalized) output energies obtained by

solving Eq. (2) analytically (curve) and numerically, for 3 ensembles of 3 × 105 pulses (dots).
For each ensemble the standard deviation of the energy kicks was 10−2. The mean energy
kicks were −3 × 10−3, 0 and 3 × 10−3. When the 3 data sets were combined, the data were

weighted according to the combined probability factors.

14


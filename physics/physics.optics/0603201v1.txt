6
0
0
2
 
r
a

M
 
4
2
 
 
]
s
c
i
t
p
o
.
s
c
i
s
y
h
p
[
 
 
1
v
1
0
2
3
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

UCRL-JRNL-219850

A uniﬁed evaluation of iterative projection algorithms for phase retrieval

S. Marchesini1, ∗
1Lawrence Livermore National Laboratory, 7000 East Ave., Livermore, CA 94550-9234, USA
(Dated: February 9, 2014)

Iterative projection algorithms are successfully being used as a substitute of lenses to recombine,
numerically rather than optically, light scattered by illuminated objects. Images obtained compu-
tationally allow aberration-free diﬀraction-limited imaging and allow new types of imaging using
radiation for which no lenses exist. The challenge of this imaging technique is transfered from the
lenses to the algorithms. We evaluate these new computational “instruments” developed for the
phase retrieval problem, and discuss acceleration strategies.

PACS numbers: 42.30.Rx 61.10.Nz, 68.37.Yz
Keywords: Phase retrieval, X-ray microscopy, X-ray diﬀraction

Crystallographers routinely image molecular struc-
tures of several thousand atoms by phasing the diﬀrac-
tion pattern of a structure replicated in a periodic sys-
tem. Likewise, computationally retrieving the phase of
a diﬀraction pattern is becoming increasingly successful
at imaging -with several millions of resolution elements-
isolated objects with no translational symmetry as com-
plex as as biological cells, nanotubes and nanoscale aero-
gel structures. Diﬀraction microscopy (the imaging of
isolated objects by diﬀraction and computational phase
retrieval) promises a 3D resolution limited only by radi-
ation damage, wavelength, the collected solid angle and
the number of x-rays or electrons collected. This capabil-
ity provides an extremely valuable tool for understand-
ing nanoscience and cellular biology. Recent estimates
[1] of the dose and ﬂux requirements of x-ray diﬀraction
on single objects, indicate that attractive resolution val-
ues (about 10 nm for life science and 2–4 nm for mate-
rial science) should be possible at a modern synchrotron.
Atomic resolution could be accomplished using pulses of
x-rays that are shorter than the damage process itself
[2] using femtosecond pulses from an x-ray free-electron
laser [3]. Alternatively the radiation damage limit could
be eliminated by continuously replacing the exposed sam-
ples with identical ones such as laser aligned molecules
[4].

In the ﬁelds of electron microscopy [5] and astronomi-
cal imaging [6], iterative projection algorithms have been
used to recover the phase information in a variety of prob-
lems. The evaluation of the aberrations in the Hubble
space telescope by Fienup in [7] remains perhaps the most
prominent example of successful phase reconstructions in
the astronomical community. In electron diﬀraction mi-
croscopy [5, 8] Zuo and coworkers imaged a single isolated
nanotube at atomic resolution [9], Wu et al. imaged de-
fects at atomic resolution [10].

An important review, which attempted to integrate the
approaches of the optical and crystallographic communi-

∗Correspondence and requests for materials should be addressed to
S. Marchesini: smarchesini@llnl.gov

ties, appeared in 1990 [11]. The connection was made be-
tween the “solvent-ﬂattening” or “density-modiﬁcation”
techniques of crystallography [12] and the compact sup-
port requirements of the iterative projection algorithms.
The importance of ﬁne sampling of the intensity of the
measured diﬀraction pattern was recognized at an early
stage [13].

The observation by Sayre in 1952 [14] that Bragg
diﬀraction under-samples the diﬀracted intensity pattern
was important and led to more speciﬁc proposals by
the same author for X-ray diﬀractive microscopy of non-
periodic objects [15, 16]. These ideas, combined with
the rapid development of computational phase retrieval
in the wider optics community especially the “support
constraint” [5, 6, 18, 19], enabled the ﬁrst successful use
of Coherent X-ray Diﬀraction Microscopy.

Since the ﬁrst proof of principle demonstration of Co-
herent X-ray Diﬀraction Microscopy (CXDM) by a team
in Stony Brook [20], a number of groups have been work-
ing to bring these possibilities into reality.

Robinson and coworkers at the University of Illinois
have applied the principles of CXDM to hard x-ray ex-
periments on micro-crystalline particles. Such data have
been reconstructed tomographically to produce a 3D im-
age at 80 nm resolution [21]. Miao (now at UCLA) and
coworkers have made considerable progress in pushing
the CXDM method in Spring-8 Japan to higher resolu-
tion in 2D (7 nm), higher x-ray energies and to a limited
form of 3D [22]. They have also made the ﬁrst applica-
tion of CXDM to a biological sample [23].

A diﬀraction chamber dedicated to diﬀraction mi-
croscopy [24] has been used to image biological cells
[25, 26] at the Advanced Light Source in Berkeley [27].
Using the same chamber a collaboration between Berke-
ley, Livermore labs and Arizona State University pro-
duced 3D imaging at 10x10x40 nm resolution of test sam-
ples [28] as well as aerogel foams [29].

In this article the computational instruments that en-
abled these and other results are reviewed.
Section
I introduces the phase problem and the experimental
requirements for diﬀraction microscopy, Section II de-
scribes the concepts of sets of images and their projec-
In section III the iterative projection algorithms
tors.

published in the literature are summarized and tested on
simple geometric sets. In section IV the connection be-
tween projection and gradient based methods and related
acceleration strategies are discussed.

I. THE PHASE PROBLEM

When we record the diﬀraction pattern scattered by an
object the phase information is missing. Apart from nor-
malization factors, an object with density ρ(r), r being
the coordinates in the object (or real ) space, generates
a diﬀraction pattern equal to the modulus square of the
Fourier Transform (FT) ˜ρ(k):

2
˜ρ(k)
I(k) =
|
|
I(k) = ˜ρ†(k)˜ρ(k) .

(1)

(2)

Where k represent the coordinate in the Fourier (or Re-
ciprocal) space. The inverse Fourier transform (IFT)
of the measured intensity I provides the autocorrelation
ρ(

ρ(r) of the object:

r)

−

∗

IFT[I(k)] = ρ(

r)

ρ(r)

−

∗

The phase retrieval problem consists of solving ˜ρ in Eq.
1 or ρ in Eq. 2 using some extra prior knowledge.

Since the intensity represent the FT of the autocorre-
lation function, and the autocorrelation is twice as big
as the object, the intensity should be sampled at least
twice as ﬁnely as the amplitude to capture all informa-
tion of the object. Finer sampling adds a 0-padding re-
gion around the recovered autocorrelation function which
adds no further information (Shannon theorem). Less
then critical sampling in the Fourier domain causes alias-
ing in the object space. A periodic repetition of the same
structure provides stronger signal, enabling the measure-
ment of the diﬀraction pattern before the structure is
damaged. However, while an isolated object generates
a continuous diﬀraction pattern that can be sampled as
ﬁnely as desired, a periodic repetition of the same ob-
ject generates only a subset of the possible diﬀraction in-
tensities. Crystallography therefore has to deal with an
aliased autocorrelation function, also known as the Pat-
terson function. This reduced information can be com-
pensated by other prior knowledge such as the atomistic
nature of the object being imaged, knowledge of a por-
tion of the object, presence of heavy atoms and informa-
tion obtained with anomalous diﬀraction. Other infor-
mation includes the presence of a solvent in the crystal.
By varying the sampling rate of a diﬀraction pattern it
was shown [16, 17] that less then critical sampling was
suﬃcient to solve the phase problem. This was possible
because the number of equations (measured intensities in
Eq. 1) in the two and three dimensional phase retrieval
problem is larger than the number of unknowns (resolu-
tion elements in the object). The number of unknowns
deﬁnes the number of independent equations, or mini-
mum required sampling rate. Although no general proof

2

(3)

(4)

has been provided that limited sampling removes only re-
dundant equations, such a minimum required sampling
rate suggest that when the solvent exceeds 50% of the
crystal volume, the algorithms developed in the optical
community using techniques to dynamically reﬁne the
solvent regions [30] may be able obtain ab-initio struc-
tural information from crystals.

Coherence is required to properly sample the FT of the
autocorrelation of the object [31]. According to Schell
theorem [32], the autocorrelation of the illuminated ob-
ject obtained from the recorded intensity is multiplied
by the complex degree of coherence. The beam needs
to fully illuminate the isolated object, and the degree of
coherence must be larger than the autocorrelation of the
illuminated object.

Diﬀraction microscopy solves the phase problem using
the knowledge that the object being imaged is isolated,
it is assumed to be 0 outside a region called support S:

ρ(r) = 0, if r /
∈

S

This support is equivalent to the solvent in crystallog-
raphy. Equations 1 and 3 can be combined to obtain
a multidimensional system of quadratic equations in the
ρ(r) variables:

2

= I(k)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
which can be written as:
(cid:12)

r∈S
X

ρ(r) exp(ik · r)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(r − r′

) cos(k

ρ(r)ρ∗(r′

·

r,r′∈S
X

)) = I(k)

(5)

}

{

r

S

∈

Each value I(k) in reciprocal space deﬁnes an ellipsoid
(Eq. 5) in the multidimensional space of the unknowns
ρ(r),
. If the number of independent equations
equals the number of unknowns the system has a sin-
gle solution ρ(r). The intersection of these ellipsoids
forms our solution. Unfortunately this system of equa-
tions is diﬃcult to solve, and has an enormous amount
of local minima. Constant phase factors, inversion with
respect to the origin (enantiomorphs) and origin shifts
ρ(±r + r0)eiφ0 are undetermined and considered equiv-
alent solutions. The presence of multiple non-equivalent
solutions in two and higher dimensional phase retrieval
problems is rare [33] and occurs when the density distri-
bution of the object can be described as the convolution
of two or more non-centrosymmetric distributions. Sim-
ple homometric structures for which the phase problem is
not unique [34] exist in nature, but such non-uniqueness
is less likely for more complex structures.

Presence of noise and limited prior knowledge increases
the number of solutions within the noise level. Conﬁ-
dence that the recovered image is the correct and unique
one can be obtained by repeating the phase retrieval pro-
cess using several random starts. Repeatability of the re-
covered images as a function of resolution measures the
eﬀective phase retrieval transfer function [25, 28], which

can be decomposed in unconstrained amplitudes modes
[25] and phase aberrations [35].

The development of iterative algorithms with feedback
in the early nineteen-eighties by Fienup produced a re-
markably successful optimization method capable of ex-
tracting phase information [6, 19, 36]. The important
theoretical insight that these iterations may be viewed
as projections in Hilbert space [37, 38] has allowed the-
oreticians to analyze and improve on the basic Fienup
algorithm [39, 40, 41, 42].

These algorithms try to ﬁnd the intersection between
two sets, typically the set of all the possible objects with
a given diﬀraction pattern (modulus set), and the set of
all the objects that are constrained within a given area
or volume called support (or outside a solvent region in
crystallography). The search for the intersection is based
on the information obtained by projecting the current
estimate on the two sets. An error metric is obtained
by evaluating the distance between the current estimate
and a given set. The error metric and its gradient are
used in conjugate gradient (CG) based methods such as
SPEDEN [43].

II. SETS, PROJECTORS AND METRICS

An image of a density distribution can be described as
a sequence of n pixel values. For an image of n pixels,
there are n coordinates. The magnitude of the density at
a pixel deﬁnes the value of that coordinate. Thus a single
vector in this n-dimensional space deﬁnes an image. For
complex images the number of coordinates increases by
a factor of two. Axes of the multidimensional space are
formed by any sequence of n-pixels with all but one pixel
equal to 0. For example x = (x, 0, 0) in 3-pixel solution
space. The origin of this space is the image with all the
pixels equal to 0. The components on these axes form the
real or object space. The same object can be described in
terms of any another n-dimensional orthogonal (or lin-
early independent) bases. Axes can be rotated, shifted,
inverted and so on and the proper linear transform must
be applied to obtain the components in the new basis.
The basis used to describe the image must have at least
n components, but more can be used if it helps to describe
the properties of the algorithm. For example one could
let the pixel values have a real and imaginary component,
doubling the number of dimensions used to describe the
object.

One important basis is the momentum or Fourier
space. While the vector in the n-dimensional space rep-
resenting an image is unaltered on transforming from real
to reciprocal space, its components in the new axes are
altered (Fourier transformed). The distance between two
points in the n-dimensional space is independent of this
transformation (Parseval theorem). The lengths and the
angles between vectors will be our guide to describe the
behavior, convergence and error metrics of these algo-
rithms.

3

(6)

We consider two sets, S (support) and M (modulus).
When the image belongs to both sets simultaneously, we
reached an equivalent solution. If the properties of the
object being imaged are known a-priori to be limited in a
certain region called support, we know that of all the pos-
sible values in the n-dimensional space, the pixel values,
some of them must be zero. Images that satisfy this rule
(Eq. 3) form the support constraint set. A projection
onto this set (PS) involves setting to 0 the components
outside the support, while leaving the rest of the values
unchanged (Fig. 1(a)):

ρ(r)

if r

S

∈

otherwise

Psρ(r) = 

0



(
r r

3

)

S

r

(
Im kɶ
r

)

(
r kɶ

)

rɶ
1

(
kɶ
r

)

mP

(
j k

)

m

rRe

(
kɶ

)

rɶ

2

m ± δ

(b)

(
r r

1

)

sP ɶ
r

(
r r

2

)

S

(a)

FIG. 1: Examples of sets and projectors: (a) Support: The
axes represent the values on 3 pixels of an image ρ known to
be 0 outside the support. The vertical axis ρ(r3) represents a
pixel outside the support (r3 ∈ S), while the horizontal plane
represents pixels inside the support S. The projection on this
set is performed simply by setting to 0 all the pixels outside
the support. (b) Modulus: A pixel (in Fourier space) with a
given complex value is projected on the closest point on the
circle deﬁned by the radius m. If there is some uncertainty
in the value of the radius m ± δ, the circle becomes a band.
The circle is a non-convex set, since the linear combination
between two points on the same set, ρ1 and ρ2 does not lie
on the set. Also represented in the ﬁgure is the projection on
the real axis (Reality projection).

The values in every pixel in Fourier space can be de-
scribed using two components, the real and imaginary
parts, or amplitude and phase, both deﬁning a point in
a complex plane. In an intensity measurement we obtain
the amplitude or modulus in every pixel which deﬁnes a
circle in a complex plane. These circles deﬁne the mod-
ulus constraint (1(b)). When every complex valued pixel
lies on the circle deﬁned by the corresponding modulus,
the image satisﬁes this constraint and it belongs to the
set. Chords - segments joining two points on a circle -
do not belong to the circle, therefore linear combination
of two images with all the pixel values lying on the same
complex circles does not satisfy the constraint: the set is

non-convex. Non-convex sets are problematic due to the
presence of local minima and undeﬁned projections.

The projection of a point in each complex plane onto
the corresponding circle is accomplished by taking the
point on the circle closest to the current one, setting the
I(k), and leav-
modulus to the measured one m(k) =
ing the phase unchanged (Fig. 1(b)):

p

(7)

Pm ˜ρ(k) = Pm

eiϕ(k) = m(k)eiϕ(k) .
˜ρ(k)
|
|
This operator is demonstrated to be a projector on the
non-convex (Fig. 1(b)) set of the magnitude constraint
[44]. The same paper discusses the problems of multi-
valued projections for non-convex sets, which do not sat-
isfy the requirements for gradient-based minimization al-
gorithms, and the related non-smoothness of the squared
set distance metric, which may lead to numerical insta-
bilities. See also [45] for a follow-up discussion on the
non-smooth analysis.

r=I
r
[

-P I

] r

rP

rR

FIG. 2: The reﬂector applies the same step as the projector
(P − I) twice: Rρ = Iρ + 2[P − I]ρ

I] = 2P

A projector P is an operator that takes to the clos-
est point of a set from the current point ρ. A rep-
etition of the same projection is equal to one projec-
tion alone (P 2 = P ), therefore its eigenvalues must be
λ = 0, 1. Another operator used here is the reﬂector
I, which applies the same step
R = I + 2[P
as the projector but moves twice as far. In the case of
the support constraint, the whole image space can be de-
scribed in terms of the eigenvectors of the corresponding
linear projector. These eigenvectors with eigenvalues of
1 (0) are the images with all the pixel equal to 0 except
for one pixel inside (outside) the support. The modulus
projector however is a nonlinear operator:

−

−

Pm(a + b)
Pm(αa)

= Pm(a) + Pm(b)
= αPm(a)

(8)

and it cannot be described in terms of eigenvalues and
eigenvectors.

The Euclidean length

of a vector ρ is deﬁned as:

ρ

2 = ρ†
||

||

·

ρ =

2 =
ρ(r)
|

2.
˜ρ(k)
|

(9)

k |
X

ρ

||

||

r |
X

4

(10)

(11)

The sum is extended to the measured portion of the
diﬀraction pattern. If part of the reciprocal space is not
measured, it should not be included in the sum. In fact
the sum should be waited with the experimental noise
σ(k):

ρ

2 =
||

||

k

1
2
˜ρ(k)
σ2(k) |
|
1
k
σ2(k)

.

P

P

∞

for values of k not measured. The dis-
with σ(k) =
is the
tance from the current point to the set
basis for our error metric. Typically the error in real (εs)
and reciprocal space (εm) are deﬁned in terms of their
distance to the corresponding set:

P ρ

−

||

||

ρ

εs(ρ) =
εm(ρ) =

Psρ
Pmρ

,

ρ
||
ρ
||

−
−

,

||
||

or their normalized version εx(ρ) = εx(ρ)
||Pxρ|| . Another error
metric used in the literature is given by the distance be-
. The projec-
tween the two sets: εs,m(ρ) =
||
tor Pm moves ρ to the closest minimum of ε2
m(Pmρ) = 0,
ρε2
providing a simple relation with the gradient
m(ρ)
[19, 44]:

Pmρ

Psρ

∇

−

||

Pmρ = ρ + [Pm

I]ρ = ρ

−

1
2 ∇

−

ρε2

m(ρ) ,

(12)

where

ρε2

m(ρ) is proportional to

ρεm(ρ):

∇

ρε2

m(ρ) = 2εm(ρ)

ρεm(ρ).

(13)

∇

∇

∇

For ˜ρ(k) = 0, εm is non diﬀerentiable, and the projector
Pm is multivalued [44]. The presence of complex zeros
(ρ(k) = 0) is considered of fundamental importance to
the phase retrieval problem [46], and the phase vortices
associated with these zeros cause stagnation in iterative
algorithms [47].

Similarly the projector Ps minimizes the error ε2
s:

[I

−

Ps]ρ = 1

ρε2

s(ρ)

2 ∇

(14)

III.

ITERATIVE PROJECTION ALGORITHMS

Several algorithms based on these concepts have now
been proposed and a visual representation of their be-
havior is useful to characterize the algorithm in various
situations, in order to help chose the most appropriate
one for a particular problem. In this section the projec-
tion algorithms published in the literature are summa-
rized (see also Tab. I) and tested on simple geometrical
sets.

The following algorithms require a starting point ρ0,
which is generated by assigning a random phase to the
measured object amplitude (modulus) in the Fourier do-
I(k). The ﬁrst algorithm called
main
Error Reduction (ER) (Gerchberg and Saxton [5]) (see

= m(k) =

˜ρ(k)
|
|

p

6
6
TABLE I: Summary of various algorithms

Algorithm

Iteration ρ(n+1) =

ER or
Solvent Flattening PsPm ρ(n)
RsPm ρ(n)
Solvent Flip

Diﬀerence Map

HIO

ASR
HPR

RAAR

Pm ρ(n)(r)
r ∈ S
(I − βPm )ρ(n)(r) r /∈ S
(
{I + βPs

[(1 + γs) Pm − γsI]

− βPm [(1 + γm) Ps − γmI]}ρ(n)

1

2 [RsRm + I]ρ(n)
1
2 [Rs (Rm + (β − 1)Pm )
+I + (1 − β)Pm ]ρ(n)
1
2 β (RsRm + I) + (1 − β)Pm

(cid:2)

ρ(n)

(cid:3)

also Alternating Projections Onto Convex Sets [48] or Al-
ternating Projections Onto Nonconvex Sets [37]) is sim-
ply (Fig. 3(a)):

ρ(n+1) = PsPmρ(n) ,

(15)

by projecting back and forth between two sets, it con-
verges to the local minimum. The name of the algorithm
is due to the steps moving along the gradient of the error
metric (see Eq. 12):

PsPmρ = Psρ

1
2 ∇

−

ρ∈S ε2

m(ρ) ,

(16)

ρ∈S ε2

∇

where
m(ρ) is the component of the gradient in
the support. Fig. 3(a) shows that the step size is far
from optimum, but guarantees linear convergence. A line
search along this gradient direction would considerably
speed up the convergence to a local minimum and will
be discussed in IV

The solvent ﬂipping algorithm [49] is obtained by re-
placing the support projector Ps with its reﬂector Rs =
2Ps

I, one obtains (Fig. 3(b)) :

−

5

(HIO(20)+ER(1) in our case). In particular one or more
ER steps are used at the end of the iteration. Elser [39]
pointed out that this may not be the best way of ﬁnding
the ﬁnal solution. The iterate ρn can converge to a ﬁxed
point (ρn+1 = ρn), which may diﬀer from the solution ¯ρ
(Ps ¯ρ = Pm ¯ρ = ¯ρ). However the solution ¯ρ can be easily
obtained from the ﬁxed point:

m = Pmρn ,
¯ρn
¯ρn
s = (1 + 1

β )PsPmρn
where ¯ρm and ¯ρs should coincide, or else their diﬀerence
can be used as an error metric. See [39] for further de-
tails. Diﬀerence Map is a general set of algorithms, [39],

β Psρn ,

−

1

(20)

0r

M

0r

M

1

rP

m

r -

1n

P
m

1
2

2

mr e

2r

S

0

rP

s

1
2

e˛

2
s m

r

S

1

r

= R P
r
s m

0

(a) ER

(b) Solvent Flip

nr

S

n

rP

m

P P
s
m

n

r

b

rP P

s

m

n

O

P P
s m

r

n

M

S

nrP

s

nr

1nr +

(
1




+

)1

b

P
m

1

b

I




n

r

M

S

nr

1nr +

(

1


)1

b

+
P
s

1

b

I




n

r

ρ(n+1) = RsPmρ(n) .

(17)

(c) HIO

(d) Diﬀerence Map

which multiplies the charge density ρ outside the support
by -1. The Hybrid Input Output (HIO) [6, 19] (Fig. 3(c)
is:

ρ(n+1)(x) =

Pmρ(n)(x)
(I

βPm)ρ(n)(x)

S,

if x
∈
otherwise.

(18)

(

−

A similar but not always equivalent way of describing
this algorithm in a recursive form would be:

ρ(n+1) = [PsPm + Ps(I

βPm)]ρ(n) ,

(19)

−

−

with Ps = (I
Ps) the complement of the projector Ps.
Eqs. [12,14] can be used to describe the steps in terms of
the gradients of the error metrics (see Sec. IV for details).
It is often used in conjunction of the ER algorithm,
alternating several HIO iterations and one ER iteration

FIG. 3: Geometric representation of various algorithms using
a simpliﬁed version of the constraint - two lines intersect-
ing: (a) Error Reduction algorithm: we start from a point on
the modulus constraint by assigning a random phase to the
diﬀraction pattern. The projection onto the modulus con-
straint ﬁnds the point on the set which is nearest to the cur-
rent one. The arrows indicate the gradients of the error met-
ric. (b) The speed of convergence is increased by replacing
the projector on the support with the reﬂector. The algo-
rithm jumps between the modulus constraint (solid diagonal
line) and its mirror image with respect to the support con-
straint (dotted line). (c) Hybrid input output, see text (Eq.
18). The space perpendicular to the support set is represented
by the vertical dotted line S .(d) Diﬀerence Map, see text (Eq.
21).

which requires 4 projections (two time-consuming mod-

-
(cid:209)
-
(cid:209)
D
-
-
ulus constraint projections) (Fig. 3(d)):

ρ(n+1) =

{

I +βPs [(1 + γs) Pm
βPm [(1 + γm) Ps

−

−

γsI]
γmI]
}

−

ρ(n) , (21)

the solution corresponding to the ﬁxed point is described
in the same article [39]. We will use in the upcoming
tests what the author suggested as the optimum with
γs =

β−1, γm = β−1.

The Averaged Successive Reﬂections (ASR) [40] is:

−

ρ(n+1) = 1

2 [RsRm + I]ρ(n) .

(22)

The Hybrid Projection Reﬂection (HPR) [41] is derived
from a relaxation of ASR:

ρ(n+1) = 1

2 [Rs (Rm + (β

1)Pm)

−

+ I + (1

β)Pm]ρ(n) .

(23)

−
It is equivalent to HIO if positivity (Sec. III A) is not en-
forced but it is written in a recursive form, instead of a
case by case form such as Eq. 18. It is also equivalent to
1, γ2 = β−1. Finally Re-
the Diﬀerence Map for γ1 =
laxed Averaged Alternating Reﬂectors RAAR (previously
named RASR) [42]

−

ρ(n+1) =

1
2 β (RsRm + I) + (1

β)Pm

ρ(n) .

(24)

−

For β = 1, HIO, HPR, ASR and RAAR coincide.

(cid:2)

(cid:3)

The ﬁrst test is performed on the simplest possible
case: ﬁnd the intersection between two lines. Fig. 4
shows the behavior of the various algorithms, The two
sets are represented by a horizontal blue line (support)
and a tilted black line (modulus). ER simply projects
back and forth between these two lines, and moves along
the support line in the direction of the intersection. Sol-
‘reﬂects’ on the
vent Flip projects onto the modulus,
support, and moves along the reﬂection of the modulus
constraint onto the support. The solvent ﬂipping algo-
rithm is slightly faster than ER due to the increase in the
angle of the projections and reﬂections. HIO and vari-
ants (ASR, Diﬀerence Map, HPR and RAAR) move in
a spiral around the intersection eventually reaching the
intersection. For similar β RAAR behaves somewhere in
between ER and HIO with a sharper spiral, reaching the
solution much earlier. Alternating 20 iteration of HIO
and 1 of ER (HIO(20)+ER(1)) considerably speeds up
convergence.

When a gap is introduced between the two lines (Fig.
4(b)) so that the two lines do not intersect, HIO and
variants move away from this local minimum in search
for another ‘attractor’ or local minimum. This shows
how these algorithms escape from local minima and ex-
plore the multidimensional space for other minima. ER,
Solvent Flip, RAAR converge to or near the local mini-
mum. By varying β RAAR becomes a local minimizer for
small β, and becomes like HIO for β
1. ER, solvent ﬂip
HIO+ER converge to the local minimum in these tests.

≃

6

A more realistic example is shown in Fig. 5. Here the
circumference of two circles represent the modulus con-
straint, while the support constraint is represented by a
line. The two circles are used to represent a non-convex
set with a local minimum. It is diﬃcult to represent a
true modulus constraint in real space. For a represen-
tation of the modulus constraint in reciprocal space see
[44]. The advantage of this example is the simplicity in
the ‘modulus’ projector operator (it projects onto the
closest circle). Although a real modulus constraint pro-
jector is not as simple as the one used in this example,
there are similarities: each Fourier space point provides
an n-dimensional quadratic equation.

We start from a position near the local minimum. ER,
solvent ﬂip and HIO+ER all fall into this trap (Fig.
5(a)), although increasing the interval between ER itera-
tions in the HIO+ER algorithm would allow it to escape
this local minimum. HIO and variants move away from
the local minimum, ‘ﬁnd’ the other circle, but converge
to the center of the circle, with all but Diﬀ. Map. not
reaching a solution. In the center of the circle the pro-
jection on the modulus constraint becomes ‘multivalued’,
and its distance metric is ‘non-smooth’. The introduction
of a small a random number added to the resulting solu-
tion at every step allows all the HIO-type codes to escape
stagnation and ﬁnd the solution (Fig. 5(b)). The ran-
dom number can be as low as the numerical precision of
the computer. For β reduced to 0.9, RAAR would not
reach the solution, but converge close to the local min-
imum. As a latest test in this series Fig. 5(c), shows
the behavior of the algorithms when the support is tan-
gent to the circle, the two solutions coincide, and the the
two constraints are parallel. The only algorithm to reach
the solution is RAAR, but HIO+ER would also reach
the solution if the interval between ER steps was suﬃ-
ciently large. The other algorithms reach a ﬁxed point
from which the solution can be obtained (Eq. 20).

Another variant of the HIO algorithm was proposed by
Takajo et al. [57] in which the Hybrid Input Output al-
gorithm is used with an inﬁnitesimally small β (Inﬁnites-
imal HIO, or IHIO). This approach has some important
IHIO contains an converging
pedagogical advantages.
component which normally dominates, and a diverging
component that has an eﬀect only when a local minimum
is found. Fig. 6 shows the behavior of IHIO compared
to the standard HIO with β = 0.75. While HIO spirals
around local minima, IHIO iterations move smoothly to-
ward a local minimum, then move to the next local mini-
mum. Both algorithms reach a ﬁxed point of the iteration
from which the solution can be found using Eq. 20, [39].

A. Positivity

The situation changes slightly when we consider the
positivity constraint. The previous deﬁnitions of the al-

7

b =0.75

b =0.75

100

50

0

−50

0

−10

−20

−30

−40

−50
100

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

ASR
HPR
HIO

20
RAAR

+ER
1

−100

−100

−50

0

50

100

(a)

0

0

100

−100

−100

(b)

FIG. 4: The basic features of the iterative projection algorithms can understood by this simple model of two lines intersecting
(4(a)). The aim is to ﬁnd the intersection. The ER algorithm and the Solvent ﬂipping algorithms converge in some gradient
type fashion (the distance to the two sets never increases), with the solvent ﬂip method being slightly faster when the angle
between the two lines is small. HIO and variants move slightly in the direction where the gap between the two projections
decreases, but at the same time in the direction of the gap, following a spiral path. When the two lines do not intersect (4(b),
HIO and variants keep moving in the direction of the gap. ER, Solvent Flipping and RAAR converge at (or close to) the local
minimum.

gorithms still apply just replacing PS with PS+:

PS+ =

ρ(x)
0

(

if x
∈
otherwise.

S, & ρ(x)

0

≥

(25)

The only diﬀerence is HIO which becomes:

ρ(n+1) =

Pmρ(n)(x)
(1

βPm)ρ(n)

(

−

if x
∈
otherwise.

S and Pmρ(n)(x)

0

≥

(26)
Fig. 7(a) shows that HIO bounces at the x = 0 axis.
As the positivity constraint gets closer to the solution,
none of the algorithms converges to the solution (Fig.
7(b)), with the HIO-type algorithms bouncing between
the regions closer to the two circles. Only Diﬀerence
Map for β > 1 converges (Fig. 7(c)). Also HIO+ER
would reach the solution for larger intervals between ER
iterations.

IV. STEEPEST DESCENT, CONJUGATE
GRADIENT AND MIN-MAX ALGORITHMS

Conjugate gradient [56] methods (such as [19], and
SPEDEN [43], which uses several other constraints) can-
not be represented by simple geometrical models with

one dimensional support sets used in the previous sec-
tion. For these gradient based methods, we turn to a
simple phase retrieval example. Takajo et al. [57] suggest
using a simple image with 2 unknown pixels to reduce the
phase retrieval problem to a 2-dimensional search. Fig. 8
shows the error metric εm as a function of the unknown
variables (the two unknown pixel values), and the be-
havior of the ER algorithm toward the local minima. ER
moves in the direction of the steepest descent, however
the step length is not optimized to reach the local min-
imum in that direction since it is only one component
of the full gradient (Fig. 8(a). In the standard steepest
descent method, once the gradient of the function to be
minimized is computed, one performs a line search of the
local minimum in the descent direction.

(27)

∆ρ =

m(ρ) =

m (ρ + α∆ρ)
Ps[I

minα ε2
1
sε2
2 ∇
−
where
ρ is the gradient with respect to ρs. If
any further step in the direction of the current step does
not decrease the error metric, the gradient direction must
be perpendicular to the current one. In other words the
current step and the next step become orthogonal:

s = Ps

Pm]ρ,

∇

∇

−

−

∂

∂α ε2

m(ρ + α∆ρ) =
0 =

Ps[I
∆ρ
|
[I
∆ρs
|

h
h

−

Pm] (ρ + α∆ρs)

−
Pm] (ρ + α∆ρs)

ir ,
ir .(28)

10

−2

10

8

6

4

2

0

8

6

4

2

0

8

6

4

2

0

10

b =0.75

b =0.75

8

0

5

10

2

3

4

5

b =0.75

b =0.75

−2

0

5

10

2

3

4

5

b =0.99

b =0.99

2

1

0

2

1

0

2

1

0

−1

−2

1

−1

−2

1

−1

−2

1

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

+ER
1

ASR
HPR
HIO

20
RAAR

(a)

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

+ER
1

ASR
HPR
HIO

20
RAAR

(b)

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

+ER
1

ASR
HPR
HIO

20
RAAR

(c)

−2

0

5

10

2

3

4

5

FIG. 5: The horizontal line represents a support constraint, while the two circles represent a non convex constraint, i.e. the
modulus constraint. The dashed line divides the region closer to one circle from the other. The starting point is on the circle
to the right, possessing a local minimum distance to the line. (a) The gradient-type (ER and Solvent Flip) algorithms converge
to the local minimum, while HIO and variants move away from the local minimum in the direction of the gap (vertical) until
they reach the region where the second circle is closer (delimited by the dashed line). From here they try to move in the same
spiral-like path of the two lines (Fig. 4) until they reach the point where the projection on the circle and the line are parallel,
and start moving toward the the center of the circle which has the correct solution. They stagnate in the center of the circle
where the projection is multivalued. Only the Diﬀ. Map reaches a ﬁxed point from which the solution can be found using
Eq. 20. The addition of a small value of the order of the numerical precision after each iteration solves this stagnation (b).
When one of the circles just touches the other constraint most algorithms either get stuck near the local minimum or stagnate.
RAAR is the only one that reaches the vicinity of the solution (c).

support
modulus
start
HIO (b =.75)
IHIO (b =.01)
fixed points
solution

9

The presence of local minima shown in the previous chap-
ters however will cause stagnation of steepest and con-
jugate gradient methods, preventing global convergence
(Fig. 8(c)).

A. Feedback and the saddle point problem

HIO and variants are able to escape local minima and
reach the solution faster (Fig. 8(d)). However as in ER,
the step length is not optimized, the algorithm keeps
moving in the same direction for several steps. Com-
bining the ideas of the conjugate gradient or the steepest
descent methods and HIO could considerably speed-up
convergence. As discussed earlier, the component of the
iteration step ∆˜ρ inside the support ∆˜ρs = ˜Ps∆ρ de-
scends along the ˜ρs components of the gradient of the
error metric ε2

m(˜ρ):

∆˜ρs =

sε2
†
−∇

m(˜ρ) = ˜Ps[ ˜Pm

I]ρ ,

−

(34)

where ˜ρ and its conjugate ˜ρ† are treated as indepen-
† is the gradient with re-
dent variables and
spect to ˜ρ†
s. The feedback component β∆˜ρs constraints
by minimizing the function
the recovered amplitudes
√I
ϕ(˜ρ) = 2

along the ˜ρs = ˜Ps ˜ρ components:

s = ˜Ps

†
∇

˜ρ
|
|

∇

˜ρ
|
|

P

∆˜ρs =

†

sϕ(˜ρ) =

˜Ps ˜Pm ˜ρ

−

−∇

(35)

Integrating the gradients along a simple path, e.g. from
0 to ˜ρs and from ˜ρs to ˜ρs + ˜ρs provides the Lagrangian
function

:
L

2

[ ˜Pm

2√I

(cid:12)
(cid:12)

L

I]˜ρs

−

−

−

− |

(cid:8)(cid:12)
(cid:12)

˜ρs + ˜ρs

X (cid:12)
(cid:12)
(cid:12)

(˜ρs, ˜ρs) =

†
sL
∇
†
sL
∇

˜ρs
|
(cid:9)(36)
(cid:12)
(cid:12)
HIO/HPR/ASR algorithms move toward the saddle
(cid:12)
point of such Lagrangian, the minimum in the direction
∆˜ρs) and the maximum in the direction
=
˜ρs (
˜ρs (
= ∆˜ρs). They do so using a gradient optimiza-
tion strategy, where the step is proportional to the gradi-
ent but with one sign reversal, moving toward the saddle
point, rather than in the descent direction. This minmax
saddle point problem is common in ﬁelds as various as
game theory, economics, physics, and engineering. Min-
imization problems are helped by the fact that we want
the function to get smaller. The saddle can be higher
or lower than the current value, although the direction
toward the saddle is still indicated by the gradient. The
optimization of the parameter α (β = α) is obtained by
increasing α until the current and next search directions
become perpendicular to one another (Fig. 8(e)):

∆˜ρ

˜Ps[ ˜Pm

|{

I]

−

−

˜Ps ˜Pm

}

(˜ρ + α∆˜ρ)

= 0 (37)

which can be computed avoiding time consuming Fourier
transforms (required by the projectors ˜Ps, ˜Ps), using
˜Psx + ˜Psy
∆˜ρ
In analogy to
x
h
|
i
|
the the conjugate gradient method, one could substitute

∆˜ρs
h

∆˜ρs
h

y
|

.
i

+

=

i

E

FIG. 6:
Inﬁnitesimal HIO and standard HIO in the case of
two tilted circles (modulus) with one intersecting a straight
line (support) in one point solution. The grid lines repre-
sent the three dimensional axes in the object space. Both
algorithm jump through the same local minima, reaching the
same ﬁxed points. The corresponding solution obtained by
Eq. 20 represents the only intersection of the two constraints.
While HIO spirals around local minima, IHIO iterations move
smoothly toward a local minimum, then move to the next lo-
cal minimum.

x†

·

ℜ

y
|

x
h

r =
i

y. The line search algorithm can
where
use ǫ2
m, and/or its derivative in Eq. 28. Evaluation of ε2
m
and its derivative requires multiple modulus projections,
so it is advantageous to use reciprocal space representa-
tions, where the modulus projector is a diagonal operator
and is fast to compute, while the support projection re-
quires two Fourier transforms:

˜ρ(k)
˜ρ(k)
|
Ps

˜Pm ˜ρ(k) =

I(k) ,

(29)

˜Ps ˜ρ(k) =

| p
−1 ˜ρ(k) ,
F
−1 represent the forward and inverse
where
Fourier transforms respectively. At each iteration we add
to the current image ˜ρ, the step ∆˜ρ:

(30)

and

F

F

F

˜ρ
→
∆˜ρs =

˜ρ + α∆˜ρs
˜Ps[I

−

−

˜Pm]˜ρ ,

(31)

with α optimized to minimize the error metric (Fig.
8(b)).

The steepest descent method is known to be ineﬃcient
in the presence of long narrow valleys, where imposing
that successive steps be perpendicular causes the algo-
rithm to zig-zag down the valley. This problem is solved
by the nonlinear conjugate gradient method [55]. Instead
of moving in the direction of steepest descent ∆ρs, we
move in the direction Λρs:

Λρ(n)

s =

∆ρ(n)
s
s + γsΛρ(n−1)
∆ρ(n)

s

(

if n=1
otherwise.

(32)

D

with

γs = max

(cid:26)

∆ρ(n)
s
h

|∆ρ(n)
k∆ρ(n−1)

s −∆ρ(n−1)
k2

s

s

ir

, 0

.

(33)

(cid:27)

10

8

6

4

2

0

8

6

4

2

0

10

10

8

6

4

2

0

b =0.75

b =0.75

10

−2

0

5

10

−1

0

1

2

3

b =0.9

b =0.9

−2

0

5

10

−1

0

1

2

b =1.1

b =1.1

−1

−2

2

1

0

2

1

0

−1

−2

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

+ER
1

ASR
HPR
HIO

20
RAAR

(a)

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

+ER
1

ASR
HPR
HIO

20
RAAR

(b)

2

1

0

−1

−2

Support
Modulus
start
ER
Solvent Flip
HIO
Diff. Map, D−1/b
1/b

ASR
HPR
HIO

20
RAAR

+ER
1

(c)

−2

0

5

10

−1

0

1

2

FIG. 7: This ﬁgure shows the behavior of the algorithms when the positivity constraint is introduced, with the support
constraint represented by a horizontal line originating from the 0 (x ≥ 0). (a) The starting point is again on the circle to the
right, close to a local minimum. HIO and variant move away from the local minimum in the direction of the gap until they
reach the region where the circle to the left is closer. Instead of moving in a spiral like fashion, the iterations move close to the
line joining the center of the left circle to the origin (represented by a dotted line), except for HIO that bounces on the x=0
axis. (b) The solution is very close to 0, and the dotted line becomes more tilted. The various algorithms after moving in the
vertical direction away from the local minimum, reach the dashed line and start moving toward the tilted dotted line, falling
back in the region closer to the ﬁrst minimum. These algorithms bounce between the regions closer to each circle without
reaching the solution. With β > 1, i.e. inverting the order of the operators, Diﬀ. Map converges, and RAAR diverges, while
HIO HPR and ASR stagnate.

11

(a) Error Reduction (ER)

(b) Steepest Descent

(c) Conjugate Gradient

(d) HIO/ASR

(e) Steepest saddle optimization

(f) Conjugate saddle optimization

(g) 2D saddle optimization

(h) 2D conjugate saddle optimization
(1)

(i) 2D conjugate saddle optimization
(2)

FIG. 8: A simple 2-D phase retrieval problem: only two variables are unknown, while the others are known. The solution
-the global minimum- is the top minimum in the ﬁgures. The colormap and contour lines represents the error metric εm(ρs),
and the descent direction is indicated by the arrows. The Error Reduction algorithm (a) proceeds toward the local minimum
without optimizing the step length and stagnates at the local minima. The steepest descent method (b) moves toward the local
minimum with a zig-zag trajectory, while the conjugate gradient method reaches the solution faster (c). The Hybrid Input
Output method avoids most stagnations, however with some rare unlucky initial guess stagnates in a local minimum (d). The
saddle point optimization moves with larger steps but stagnates in the same local minimum as HIO (e). The conjugate gradient
version avoids stagnation(f). The saddle point optimization using a two dimensional search of the saddle point does not stop
at the local minimum but in the “steepest” version the algorithm spends several iterations to escape the local minimum (g).
The conjugate gradient version (h, i) reaches the solution faster if the conjugate direction is chosen independently (i).

the search direction ∆˜ρ with Λ ˜ρ as in Eq. 32 (Fig. 8(f)).
Further improvement should be obtained by replacing the
one dimensional search with a two dimensional optimiza-
tion of the saddle point (Fig. 8(g)):

min
α

max

β L

(˜ρs + α∆˜ρs, ˜ρs + β∆˜ρs),

(38)

increasing α, β in the direction (
components are equal to 0:

−

∂

∂α , ∂

∂β )

L

, until both

∆˜ρs

∆˜ρs

˜Ps[ ˜Pm
|

−
˜Ps ˜Pm

| −




D

D

(cid:0)

I]

˜ρ + α∆˜ρs + β∆˜ρs

= 0

˜ρ + α∆˜ρs + β∆˜ρs

(cid:0)

(39)

r
(cid:1)E
= 0.
r

(cid:1)E



i.e. when the components ∆˜ρs and ∆˜ρs of successive
steps become perpendicular. In the conjugate gradient
version, one can use a common γ value to obtain the
conjugate direction (Eq. 33) Fig. 8(h), or calculate γs, γs
independently 8(i).

V. CONCLUSIONS

Recombining numerically rather than optically light
scattered by illuminated objects is being increasingly use-
ful at tackling unsolved scientiﬁc problems. The new in-
struments replacing lenses are the iterative projection al-
gorithms for phase retrieval. We reviewed the algorithms
from the literature, and evaluated them with geometrical
sets and simple phase retrieval problem. ER is a simple
but powerful local minimizer using a gradient search of
the minimum of the error metric. Its rate of convergence
can be increased optimizing the step length along the de-
scent direction and using either the steepest descent or
conjugate gradient methods (such as SPEDEN [43]) re-
viewed in section IV. HIO and variants are very powerful

12

in escaping local minima, but in several situations fail to
converge. When positivity is introduced, the recursive
version of HIO (HPR) converges more ‘smoothly’ to the
solution without bouncing on the x = 0 axis. Alternat-
ing between HIO and ER with the correct intervals would
have worked in all the examples shown above, however
the correct interval is not known in advance. RAAR is
a good (single parameter) way to change from ‘global’
to local minimizer. Diﬀerence Map is successful in a few
more of the examples shown above for the proper choice
of β, however it involves 2 time consuming modulus con-
straint operations. The connection between the feedback
and the saddle point optimization is discussed in section
IV. Algorithms with feedback (HIO, HPR and ASR)
correspond to a gradient search of the saddle point, and
acceleration strategies such as conjugate gradient method
for the minmax problem have been discussed.

The Solvent ﬂipping algorithm does not show much
success in the examples shown above. Despite this it was
used to improve images [49], and in a modiﬁed form to
solve 3D structures ab-initio [53].

Acknowledgments

This work was performed under the auspices of the
U.S. Department of Energy by the Lawrence Livermore
National Laboratory under Contract No. W-7405-ENG-
48 and the Director, Oﬃce of Energy Research. This
work was funded by the National Science Foundation
through the Center for Biophotonics. The Center for
Biophotonics, a National Science Foundation Science
and Technology Center, is managed by the University
of California, Davis, under Cooperative Agreement No.
PHY0120999. D. R. Luke provided very useful com-
ments.

[1] M. R. Howells, et al. J. Elect. Spect. and Rel. Phen.

(2004), (arXiv:physics/0502059).

[2] J. C. Solem, G. C. Baldwin, Science 218, 229-235 (1982).
[3] R. Neutze et al, Nature 406, (2000) 752.
[4] J. C. H. Spence, R. B. Doak, Phys. Rev. Lett. 92, (2004)

198102.

[5] R. Gerchberg and W. Saxton, Optik 35, 237 (1972).
[6] J. R. Fienup, Opt. Lett. 3, (1978) 27.
[7] J.R. Fienup, J.C. Marron, T.J. Schulz and J.H. Seldin,

Appl. Opt. 32, (1993) 1747-1768.

of Crystallography, Kluwer Academic Publishers, Dor-
drecht/Boston/London 2001.

[13] R. H. T. Bates, Optik 61, (1982) 247.
[14] D. Sayre, Acta Cryst. 5, (1952) 843.
[15] D. Sayre, “Prospects for long-wavelength x-ray mi-
croscopy and diﬀraction”, in Imaging Processes and Co-
herence in Physics, Schlenker, M., M. Fink, J. P. Goedge-
buer, C. Malgrange, J. C. Vi´enot, R. H. Wade, (Ed), Lec-
ture Notes in Physics, Vol. 112, 229-235, Springer-Verlag,
Berlin, 1980.

[8] J. C. H. Spence, U. Weierstall , M. Howells Phil. Trans.

[16] J. Miao, D. Sayre, and H. N. Chapman, J. Opt. Soc. Am

A 360(1974), (2002), 875-895.

A 15 (1998) 1662.

[9] J.M. Zuo, I. Vartanyants, M. Gao, R. Zhang and L.A.

[17] W. McBride, N. L. O’Leary, and L. J. Allen, Phys. Rev.

Nagahara, Science 300, (2003) 1419-1421.

Lett. 93, 233902 (2004).

[10] J. Wu, U. Weierstall, J. C. H. Spence, Nature Materials,

4, (2005), 912.

[11] R. P. Millane, J. Opt. Soc. Am. A 7, 394 (1990).
[12] International Tables for Crystallography Vol. F, Ed. M.
G. Rossmann and E. Arnold, The International Union

[18] J. R. Fienup, Opt. Eng. 19, (1980) 297.
[19] J. R. Fienup, Appl. Opt. 21, (1982) 2758.
[20] J. Miao, P. Charalambous, J. Kirz, D. Sayre, Nature 400,

(1999) 342.

[21] G. J. Williams, M. A. Pfeifer, I. A. Vartanyants, I. K.

13

Robinson, Phys. Rev. Lett. 90, (2003) 175501.
[22] J. Miao, et al. Phys. Rev. Lett., 89 (2002), 088303.
[23] J. W. Miao, K. O. Hodgson, T. Ishikawa, C. A. Larabell,
M. A. LeGros, Y. Nishino, Proc. Nat. Ac. Sci. 100, (2003)
110.

[24] T. Beetz, et al. Nucl. Instrum. Meth. A 545, (2005) 459.
[25] D. Shapiro, P. Thibault, T. Beetz, V. Elser, M. Howells,
C. Jacobsen, J. Kirz, E. lima, H. Miao, A. Neiman, D.
Sayre, PNAS 102 (43), (2005) 1543-1546.
[26] E. Lima, et al. XRM proceedings (2005).
[27] M. R. Howells et al, Proc. SPIE 4783, (2002) 65.
[28] H. N. Chapman, A. Barty, S. Marchesini, A. Noy, C.
Cui, M. R. Howells, R. Rosen, H. He, J. C. H. Spence,
U. Weierstall, T. Beetz, C. Jacobsen, D. Shapiro, J. Opt.
Soc. Am. A , in press (arXiv:physics/0509066).

[29] A. Barty et al. in preparation
[30] S. Marchesini et al. Phys. Rev. B 68, (2003) 140101(R),

(arXiv:physics/0306174).

[40] H. H. Bauschke, P. L. Combettes, and D. R. Luke. J.

Opt. Soc. Am. A 19, 1334-1345 (2002).

[41] H. H. Bauschke, P. L. Combettes, and D. R. Luke, J.

Opt. Soc. Am. A 20, 1025-1034 (2003).

[42] D. R. Luke,

Inverse Problems

21:37-50(2005),

[43] S. P. Hau-Riege, H. Sz¨oke, H. N. Chapman et al. (2004)

(arXiv:math.OC/0405208).

(arXiv:physics/0403091)

[44] D. R. Luke, J. V. Burke, R. G. Lyon, SIAM Review 44

[45] J. V. Burke and D. R. Luke. SIAM J. Control Opt. 42,

169-224 (2002).

576-595 (2003).

[46] P-T. Chen, M. A. Fiddy, C-W. Liao and D. A. Pommet,

J. Opt. Soc. Am. A 13, (1996), 1524-31.

[47] J. R. Fienup, C. C. Wackerman, J. Opt. Soc. Am. A 3,

1897-1907 (Nov. 1986).

[48] L. M. Br`egman, Sov. Math. Dokl. 6, 688-692 (1965).
[49] J. P. Abrahams, A. W. G. Leslie, Acta Cryst. D52, 30-42

[31] J. C. H. Spence, U. Weierstall, and M. R. Howells, Ul-

(1996)

tramicros. 101, (2004) 149.

[50] S. Marchesini et al. Optics Express 11, (2003) 2344,

[32] J. W. Goodman, “Statistical Optics” New York: Wiley

(arXiv:physics/0308064).

(1985).

[33] R. Barakat and G. Newsam, “Necessary conditions for
a unique solution to two-dimensional phase recovery,” J.
Math. Phys. 25, 3190-3193 (1984).

[34] M. J. Buerger, “Vector space and its application in crys-
tal structure investigation”, Wiley, New York (1959).
[35] S. Marchesini, H. N. Chapman, A. Barty, M. R. Howells,
J. C. H. Spence, C. Cui, U. Weierstall, and A. M. Minor,
to be published in the XRM 2005 proceedings, Jap. J.
Appl. Phys. (arXiv:physics/0510033).

[36] J. N. Cederquist, J. R. Fienup, J. C. Marron, R. G. Pax-

man, Opt. Lett. 13, 619. (1988).

[51] J. R. Fienup, A. M. Kowalczyk J. Opt. Soc. Am. A 7,

450 (1990).

[52] V. Elser, Acta Cryst. A59,

201-209

(2003),

(arXiv:cond-mat/0209690).

[53] G. Oszl´anyi and A. S¨uto, Acta Cryst. A60, 134-141

(2004) (arXiv:cond-mat/0308129).

[54] B. Carrozzini, G. L. Cascarano, L.De Caro, et al. Acta
Cryst. A60, 331-338 (2004), (arXiv:physics/0404073).
[55] W. H. Press, S. A. Teukolsky, W. T. Vetterling, Brian P.
Flannery, Numerical Recipes in C, Cambridge University
Press,

[56] M.R. Hestenes, Conjugate Direction Methods in Opti-

[37] A. Levi and H. Stark, J. Opt. Soc. Am. A 1, 932-943

mization, Springer-Verlag, New York, 1980

(1984).

[57] H. Takayo, T. Takahashi, R. Ueda, M. Taninaka J. Opt.

[38] H. Stark, Image Recovery: Theory and applications.

Soc. Am. A 15, 2849-61 (1998)

(Academic Press, New York, 1987).

[39] V. Elser, J. Opt. Soc. Am. A 20, 40 (2003).


5
0
0
2
 
p
e
S
 
8
 
 
]
s
c
i
t
p
o
.
s
c
i
s
y
h
p
[
 
 
1
v
6
6
0
9
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

High-resolution ab initio three-dimensional X-ray diﬀraction microscopy

H. N. Chapman,∗ A. Barty, S. Marchesini, and A. Noy
University of California, Lawrence Livermore National Laboratory,
7000 East Ave., Livermore, CA 94550, USA

C. Cui, M. R. Howells, R. Rosen
Advanced Light Source, Lawrence Berkeley National Laboratory,
1 Cyclotron Road, Berkeley, CA 94720, USA

H. He, J. C. H. Spence, U. Weierstall
Department of Physics and Astronomy, Arizona State University, Tempe, AZ 85287-1504, USA

T. Beetz, C. Jacobsen, and D. Shapiro
Department of Physics and Astronomy, Stony Brook University, Stony Brook, NY 11794-3800, USA

Coherent X-ray diﬀraction microscopy is a method of imaging non-periodic
isolated objects at resolutions only limited, in principle, by the largest scat-
tering angles recorded. We demonstrate X-ray diﬀraction imaging with high
resolution in all three dimensions, as determined by a quantitative analysis
of the reconstructed volume images. These images are retrieved from the 3D
diﬀraction data using no a priori knowledge about the shape or composition
of the object, which has never before been demonstrated on a non-periodic
object. We also construct 2D images of thick objects with inﬁnite depth of
focus (without loss of transverse spatial resolution). These methods can be
used to image biological and materials science samples at high resolution us-
ing X-ray undulator radiation, and establishes the techniques to be used in
atomic-resolution ultrafast imaging at X-ray free-electron laser sources.

OCIS codes: 340.7460, 110.1650, 110.6880, 100.5070, 100.6890, 070.2590, 180.6900

1.

Introduction

In many ﬁelds of science the ability to visualize the three-
dimensional organization of component parts is prov-
ing crucial to our understanding of the mechanisms in-
volved in atomic and molecular processes. This is oc-
curring in ﬁelds as diverse as whole-cell imaging in biol-
ogy, the study of the minimum energy pathway for crack-
propagation in brittle solids, and the internal structure
of the new labyrinthine mesoporous structures developed
by inorganic chemists for a wide range of applications.

The ﬁeld of coherent X-ray diﬀraction imaging (CXDI,
also known as diﬀraction microscopy) is expected to make
a signiﬁcant contribution to this eﬀort. In this method,
ﬁrst put forward and developed by David Sayre1,2, an im-
age is reconstructed from measurements of the far-ﬁeld
scattered intensity of an isolated and non-periodic ob-
ject. The resolution of this form of microscopy is lim-
ited only by the wavelength and the largest scattering
angle recorded. Hence this method is being pursued
as a method for high-resolution X-ray microscopy with-
out the technological limitations of manufacturing high-
resolution optical elements3,4,5,6,7,8 The penetrating na-

∗Electronic address: henry.chapman@llnl.gov

ture of X-rays allows imaging of objects much thicker
than can be examined in a TEM (e.g. 10 µm), at res-
olutions much better than visible microscopes. Prelim-
inary studies of radiation damage suggest that 3D res-
olutions of about 10 nm should be achievable on frozen
hydrated biological material9. The method is also being
pursued in order to push X-ray imaging to its resolution
limits, namely ultrafast near-atomic-resolution imaging
of macromolecules at X-ray free-electron laser (XFEL)
sources10,11 and of laser-aligned molecules12, that will en-
able structure determination without the need for crys-
tallizing material.

High resolution imaging of thick objects can only be
attained in the context of three-dimensional (3D) mea-
surement and reconstruction. In most cases, other than
surface studies or imaging of man-made objects, the anal-
ysis of the structure can only be properly interpreted in
three dimensions. Unless the object itself is a slice of
material that is thinner than the depth of focus of a two-
dimensional (2D) image, artifact-free structural analysis
can only be carried out with knowledge of the surround-
ing material, or by applying imaging modalities whereby
depth information is not strongly transferred to the im-
age (such as confocal imaging). At resolution lengths
very much larger than the wavelength, thickness eﬀects
do not play a signiﬁcant role since, at the correspond-
ingly low numerical aperture, the depth of focus may be

much larger than the size of the object. This is certainly
the case as one satisﬁes the projection approximation of
high energy X-ray tomography, where the depth of fo-
cus approaches inﬁnity. Tomographic imaging in this
mode is limited by detector pixel size, or, if a diverg-
ing beam is used, by Fresnel diﬀraction eﬀects. However,
as one moves to higher resolution, the depth of focus
decreases dramatically, with the ratio of transverse reso-
lution length to longitudinal depth of focus given by the
numerical aperture. For the classes of high-resolution
microscopy and structure determination applications in
which we are interested that imaging can only properly
be carried out in 3D.

Coherent 3D X-ray diﬀraction imaging is especially
suited to high-resolution X-ray microscopy. With a
collimated beam incident on an object, the far-ﬁeld
diﬀraction pattern (recorded on a ﬂat CCD) represents
diﬀracted intensities which, in reciprocal space, sample
points on the momentum- and energy-conserving Ewald
sphere. By rotating the sample about an axis normal to
the beam, this sphere, which passes through the origin,
sweeps through almost all of the reciprocal space volume
of continuous diﬀuse scattering from our non-periodic ob-
ject. In this way we collect the three-dimensional distri-
bution of scattered intensity in reciprocal space, which is
phased using the 3D implementations of iterative meth-
ods, as discussed below. Once the phases of the diﬀrac-
tion intensities in the diﬀraction volume have been deter-
mined, the 3D Fourier transform of the object is known
and the 3D image can be obtained simply by an inverse
Fourier transform. As will be demonstrated in this pa-
per, such datasets can be used for artifact-free analy-
sis of structures. This is also the case for crystallog-
raphy, but is not generally the case for imaging with a
lens. Partially-coherent tomographic imaging techniques,
such as tomography in the scanning transmission X-ray
microscope (STXM)13 or transmission X-ray microscope
(TXM)14,15, lead to a complicated transfer of object spa-
tial frequencies into the measured image and there is no
longer a simple one-to-one mapping of a measurement
on a detector pixel, for example, to a spatial frequency
of the object. For some classes of object, such as pure
phase or amplitude objects, it may be possible to de-
convolve the 3D transfer function, but this is not gen-
erally assured16. As with coherent diﬀraction imaging
and crystallography, coherent imaging with a lens also
leads to a direct mapping of spatial frequencies in the
object to spatial frequencies of the image. Again, a to-
mographic reconstruction from coherent 2D images can
be easily achieved for pure amplitude or phase objects,
but would otherwise require knowing the phase and am-
plitude of the image in order to transform into 3D re-
ciprocal space. Coherent diﬀraction imaging essentially
attempts to emulate coherent lens-based imaging, using
a computer algorithm in place of a lens. The advantage,
for tomography of complex objects, is that the diﬀraction
amplitudes are measured and the phases retrieved from
the oversampling of those amplitudes, so that a direct 3D

2

Fourier synthesis of the object can be achieved.

±

In this paper we perform an important demonstration
of the feasibility of high-resolution diﬀraction microscopy
required for biological and materials characterization, as
well as single-molecule imaging. Signiﬁcantly this is done
without the use of detailed a priori information about
the sample structure or low-resolution data obtained by
other means. We also demonstrate that a full 3D re-
construction can be produced on a 10243 or larger data
cube in a reasonable amount of time using currently avail-
able computational hardware. Three signiﬁcant recent
developments have enabled us to perform full 3D image
reconstructions with high resolution in all three dimen-
sions. The commissioning of a new diﬀraction tomog-
raphy apparatus by Stony Brook University at an un-
dulator beamline of the Advanced Light Source (ALS)17
allows us to acquire diﬀraction patterns at over one hun-
dred orientations of an object, with short exposure times,
over angular ranges of more than
70◦. The Shrinkwrap
phase-retrieval algorithm that we developed6 has proven
to be extremely robust and eﬀective in performing phase
retrieval on diﬀraction datasets with missing data (e.g.
due to a beam-stop) or limited angles. The algorithm
retrieves images ab initio from the measured diﬀraction
intensities.
It does not require additional information
about the object, such as a low-resolution image, and can
retrieve phases of general complex-valued objects. The
third advance is the ability to perform 3D fast Fourier
transforms (FFTs) on the large 10243-element arrays of
diﬀraction data that are assembled from our measure-
ments. Although the familiar increase of computer pro-
cessing power has brought giga-element FFTs in reach
of today’s computers, it has been the development of
computer clusters and speciﬁc software for distributed
computation of FFTs that has made feasible the 3D im-
plementation of the Shrinkwrap algorithm.
In partic-
ular, we utilize the dist fft software18 on a 16-node
cluster of dual-processor Apple G5 Xserves, giving us a
performance of 8.6 s per 10243-element single-precision
complex FFT. We note that this computational advance
should also beneﬁt the ﬁeld of diﬀraction tomography19
(Sec. 13.2), in which both the phase and amplitude of the
scattered ﬁeld are measured as is possible with scattered
ultrasonic waves.

We present here experimental results of high-resolution
3D X-ray diﬀraction imaging of a well-characterized test
object to demonstrate the practical application of these
advances and quantitatively assess the technique. We
show the ﬁrst full 3D X-ray diﬀraction images that have
been reconstructed without prior knowledge of the sam-
ple. We believe that these are the highest resolution
3D X-ray images of non-crystalline objects ever achieved,
with a demonstrable resolution volume of 10 nm
×
40 nm. We demonstrate that artifact-free 2D images can
be created from the 3D diﬀraction dataset of objects very
much thicker than the depth of focus. In Sec. 2 we re-
view diﬀraction imaging, the experimental requirements
for 3D image reconstructions, and our computer imple-

10 nm

×

mentation to perform the 3D phase retrieval and Fourier
synthesis of the image. Our sample preparation and char-
acterization techniques are discussed in Sec. 3 A, and our
particular experimental setup and methods are described
in Secs. 3 B and 3 C.
Image reconstruction results are
presented in Sec. 4. The 3D images are visualized as iso-
surface renderings, inﬁnite depth-of-focus projection im-
ages, maximum value projections, and tomographic slices
through the object. We also compare artifact-free 2D
projections of 3D data to reconstructions of individual 2D
views, and illustrate the artifacts present in single-view
2D images of thick objects. In Sec.5 we quantitatively
assess our 3D image resolution.

2. Three-Dimensional Coherent Diﬀraction
Imaging

The incident X-ray waveﬁeld interacts with a three-
dimensional (3D) periodic or non-period object through
the scattering potential of the object, o(x) = reρ(x),
where ρ(x) is the complex electron density and re the
classical electron radius. This object scattering function
may be decomposed into a Fourier representation of 3D
spatial frequencies u, with complex amplitudes

O(u) =

o(x)

o(x) exp(2π iu

x) dx,

(1)

F {

} ≡

Z

·

and

kout|
|

in which spatial frequency can be thought of as a volume
In the case of coherent diﬀraction imaging a
grating.
plane wave with wave-vector kin is incident on the object
and the intensity of the scattered ﬁeld in the direction of
the wave-vector kout is measured on a 2D pixellated de-
tector (e.g. a bare CCD) in the diﬀraction far ﬁeld. This
detector is typically centered on the forward direction,
but in principle could be oriented in any angle to the in-
cident beam (see Fig. 1). For elastic scattering only the
volume gratings that satisfy Bragg’s law will scatter, and
the wave-vector transfer q = kout −
kin will be equal to
the grating spatial frequency; q = u. Since the magni-
kin|
tudes
are constant and equal to 1/λ, these
|
spatial frequencies u lie on the Ewald sphere of radius
1/λ20,21, where λ is the X-ray wavelength. This construc-
tion is equivalent to the condition that to scatter light by
an angle 2θ from the forward direction (the z axis), the
volume grating must be tilted by an angle θ from per-
pendicular to the forward direction (Bragg’s law). With
= q = 2/λ sin θ.
the convention used here we have
The diﬀraction amplitudes in the direction kout are pro-
portional to O(q), and in diﬀraction imaging we measure
2. In particular, in
O(q)
the intensities, proportional to
|
|
the Born approximation (which can be thought of in this
context as single scattering), the number of photons per
second measured in a CCD pixel, with solid angle Ω, is
given by

q
|
|

I(q; Ω) = I0 Ω P

2,
O(q)
|
|

(2)

where I0 is the ﬂux (photons per second per unit area)
of the incident plane wave on the sample, and P is the

3

polarization factor; P = (1 + cos2 ψ)/2 for unpolarized
light, with ψ = 2θ20.

The complex scattering potential o(x) that we aim to
recover from measurements of I(q) is related to the com-
plex refractive index n(x) of the object by19 (Sec. 13.1)22

o(x) = reρ(x) =

n2(x)

.

(3)

π
λ2

1

−

(cid:0)

(cid:1)

In the soft X-ray region, the complex refractive index
is usually written in terms of the optical constants as
n(x) = 1
iβ(x). For optical constants much less
than unity, which is generally the case for soft X-rays,
Eqn. (3) can then be well approximated by

δ(x)

−

−

o(x)

2π
λ2 (δ(x) + iβ(x)) =

2π
λ2 ∆n(x).

≈

(4)

The validity of Eqn. (2) under the Born approximation
∆n(x)
< 2π λ C, where D is the thickness of
is that D
|
|
the object and C

0.223.

≈

Fig. 1. Scattering geometry for coherent X-ray diﬀraction
imaging. The sample is rotated about the y axis by an
angle φ.

A. Experimental Requirements

The recovery of the 3D image o(x) from O(u) requires
the phases of O(u) to be recovered and combined with
the square root of the measured intensities. Both phase
retrieval and image recovery place requirements on the
3D sampling of the diﬀraction intensities. Image recov-
ery requires that the object be adequately sampled in
real space to resolve the ﬁnest desired feature size over
a given ﬁeld of view. The requirements of the phase re-
trieval step are more demanding, in particular because
our phase retrieval algorithm has the additional task of
overcoming gaps and missing regions in the 3D sampled
data, by performing the interpolation tasks of recover-
ing intensities that were blocked by a beam-stop or that
were missed due to a limited angular range of measure-
ments. The 3D image recovery requires measuring the
complex amplitudes O(u) throughout a volume of recip-
rocal space. Since a single diﬀraction pattern is limited to
frequencies u = q on the Ewald sphere, diﬀraction data
must be collected for various orientations of the sample.

In this work we perform phase retrieval and image re-
covery by full 3D Fourier synthesis, which requires inter-
polating the measured intensities from the Ewald sphere
onto a uniform 3D Cartesian grid.
In reciprocal space
the grid has a width of N samples, spaced by ∆u, and
is centered at the zero spatial frequency. For N even,
the spatial frequencies along each grid axis run from
1) ∆u to (N/2) ∆u. In real space we character-
−
ize the grid by a spacing ∆x and a ﬁeld width w = N ∆x.
Since ∆x ∆u = 1/N we have the relationship ∆u = 1/w,
thus the largest spatial frequency component along a grid
axis is given by ux,max = N ∆u/2 = 1/(2∆x).

(N/2

−

From Eqns. (1) and (2), the inverse Fourier transform
of the intensity diﬀraction pattern is proportional to the
autocorrelation function of the image that would be re-
covered when the phases are known:

i(x) =

1

−

I(q)

F

{

} ∝

o(x)

o∗(x).

⊗

(5)

Consider an object of ﬁnite extent with a maximum
width D along any one axis of the real-space grid. The
autocorrelation image i(x) in that direction has a max-
imum width of 2D, and hence the diﬀraction intensities
are band-limited. That is, the smallest grid spacing re-
quired to record all information present in the diﬀraction
intensities is ∆u = 1/(2D), the Nyquist critical sampling
frequency, and ﬁner samples can be evaluated by a sinc-
series expansion of the measured samples24. We deﬁne
the sampling ratio s per dimension with w = s D, relative
to Bragg sampling (band-limited sampling of intensities
occurs for s = 2). The oversampling of data relative
to the Bragg sampling of ∆u = 1/D is what enables the
phase retrieval techniques to be employed. In practice we
may measure data on a ﬁner grid than strictly required
as a way to increase detector dynamic range, although
successful phase retrieval can also be achieved with less
than this factor of two in each dimension25.

The CCD must be placed so that it intersects large
enough range of scattering angles up to the desired spa-
tial resolution. Usually the CCD is far enough away from
the sample to be in the diﬀraction far-ﬁeld, in which the
angularly-resolved diﬀraction pattern does not vary with
propagation distance. For an object of width D the far
ﬁeld exists beyond distances of zF = 2D2/λ from the
object26. For a detector with pixels of width p placed
a distance zD from the object, we have, for small scat-
tering angles, ∆q = p/(zDλ). That is, to sample a ﬁeld
width of w = s D the detector must be placed a distance
of zD = spD/λ. This will be in the far-ﬁeld if zD > zF ,
which can be satisﬁed if D < sp/2, or the condition that
the sample must be smaller than the CCD pixel spacing
when s = 2. If the CCD is closer to the sample than zF
then the sample and diﬀraction planes are related by a
Fresnel, rather than a Fourier, transform, and the recon-
struction algorithms must be appropriately modiﬁed.

Experimental requirements are placed on the trans-
verse and longitudinal coherence of the incident beam.
The transverse spatial coherence length of the incident
beam must be at least as large as the entire ﬁeld width

4

w = s D27. The eﬀect of partial coherence may be
modeled as an incoherent source located some distance
from the sample, whereby the diﬀraction intensity pat-
tern is convolved with a demagniﬁed intensity image of
the source.
In real space this convolution modulates
the autocorrelation of the object with an envelope func-
tion which is proportional to the modulus of the trans-
form of the source function. By the Van Cittert-Zernike
theorem19, this envelope function is the mutual coherence
of the source. The measured diﬀraction intensity is also
convolved with the pixel response function of the detec-
tor, which modulates the autocorrelation image with an
envelope proportional to the MTF of the detector. The
spectral bandwidth ∆λ/λ of the incident light should be
narrower than 2/N since we require ∆q/q = ∆λ/(2λ)
so that the range of angles diﬀracted from a single spa-
tial frequency by a range of wavelengths spreads by no
more than half a detector pixel. This is equivalent to
a minimum required longitudinal coherence length of
wqmaxλ = 2w sin θ, which will be the maximum path-
length for light scattering by 2θ to the edge of the detec-
tor from points spaced transversely by w, or by the same
angle from points spaced longitudinally by w/ tan 2θ.

In our experiments we rotate the sample about an axis
perpendicular to the incident beam direction to build
up the 3D dataset. At the highest spatial frequencies
recorded, an angular increment of ∆φ leads to a spac-
ing between the Ewald sphere surfaces of ∆q = qmax ∆φ.
That is, the Crowther resolution28 matches the critical
sampling of the diﬀraction intensities (s = 2) when

∆φ = ∆q/qmax = ∆x/D.

(6)

±

Note that this angular increment leads to a higher than
necessary sampling at the lower spatial frequencies. For
the examples in this paper we collected diﬀraction data
with angular increments that are 2–4 times larger than
given by Eq. (6).
In the process of phase retrieval we
additionally recover both the amplitudes and phases of
the missing data between the Ewald surfaces, including
those in a large gap resulting from a limited range (usu-
70◦) of rotation angles, data blocked by a beam-
ally
stop, and the missing “cone” of data resulting from ro-
tating the sample about a single axis. This amplitude
and phase retrieval of the missing data is essentially
a super-resolution technique and is achieved with the
same real-space image constraints that we use for phase
retrieval29. Recovery of unmeasured data behind a beam-
stop has been demonstrated previously in 2D coherent
X-ray diﬀraction imaging using this technique6,30, and
data missing due to a limited number of views have been
recovered in the context of computed tomography using
iterative algorithms31 similar to those we use (described
in Sec. 2 C). Depending on how much amplitude infor-
mation is missing, there will be a null space of images
which are not constrained by the real or reciprocal space
constraints29,32 and which may need to be regularized in
the reconstruction33.

B.

Interpolation of the Diﬀraction Intensities

We interpolate the diﬀraction intensities measured on the
CCD detector onto a regular Cartesian grid. The location
of the CCD pixel indexed by integers (i, j) is given by the
vector pi,j = p(iˆi + j ˆj), relative to the pixel intersecting
the direct beam, as shown in Fig. 1. We have then, with
kin = (1/λ)ˆk,

qi,j = kout −

kin =

1
λ 

pi,j + zDˆk
i,j + z2
p2
D



q

ˆk


,

−



where zD is the distance from the sample to the detector.
Hence, for example,

qx
i,j = qi,j

ˆi =

1
λ

p i
p2(i2 + j2) + z2
D

,

qz
i,j = qi,j

ˆk =

1
p
λ  

zD
p2(i2 + j2) + z2

1

.

!

D −

·

·

In practice each diﬀraction pattern in our data set has a
diﬀerent rotation in the 3D Fourier space of the object,
and the coordinate of each pixel in this space is given by

p

(7)

(8)

(9)

ui,j,φ = Rφ qi,j,

(10)

where Rφ is the 3D rotation matrix derived from the
known object orientation. The coordinates ui,j,φ are
then mapped onto a uniform Cartesian grid by nearest-
neighbor sampling. Where more than one pixel from the
set of intensity measurements contribute to a given voxel,
the pixel values are averaged to determine the appropri-
ate intensity value at that point.

We note that there are alternatives to the nearest-
neighbor interpolation onto a regular grid that we use
in this work. The interpolation could be avoided by solv-
ing the inverse transform by inverse methods, such as
performed in the computer program back34 (Sec. A5.3)
which utilizes a constrained conjugate gradient solver
and which is used in the computer program speden35
(speden is a program to perform 3D phase retrieval, de-
signed speciﬁcally to optimally include prior data and
avoid instabilities caused by interpolation.) Alterna-
tively, it should be possible to employ fast algorithms to
compute discrete Fourier transforms of non-equispaced
data (NDFTs)36. In the non-diﬀracting limit (i.e. com-
puted tomography, or CT) the reconstruction method of
ﬁltered back-projection can be shown to be a computa-
tionally eﬃcient method that is equivalent to Fourier syn-
thesis via a polar-to-Cartesian interpolation37,38. A cor-
responding algorithm, called ﬁltered back-propagation39,
has been developed for the diﬀracting case where the

5

diﬀraction amplitude and phase are measured, which
again can be shown to be equivalent to Fourier synthesis
after interpolation40.
C. Phase Retrieval

Our phase retrieval method follows from previous work
on 2D diﬀraction imaging6,7,8,41. In particular, we have
extended the Shrinkwrap algorithm6 to operate on 3D ar-
rays. This algorithm is based on an iterative transform
algorithm (ITA), which cycles between real and recipro-
cal space, respectively enforcing the constraints of known
object support or known diﬀraction modulus. Usually
an ITA requires knowledge about the shape of the object
to set the support constraint. This support is usually
larger than the actual boundary of the object; what is
termed a loose support. For general complex-valued ob-
jects, where a positivity constraint can not be applied,
the ITA gives higher-quality reconstructions when the
support constraint more closely and tightly matches the
object’s boundary42. The reason for this is explained in
Sec. 4 B. The Shrinkwrap algorithm periodically updates
the estimate of the support based on the current object
estimate. The updated support is chosen by low-pass ﬁl-
tering the current estimate and setting the support to
be the region for which the intensity is above a certain
threshold (usually a prescribed fraction of the maximum
image intensity). The method can be started from an
estimate of a very loose support, from a threshold of the
object’s autocorrelation function, or even the entire ar-
ray. A method which exists for ﬁnding an estimate of the
object support from the autocorrelation function’s sup-
port could also be used43. While the Shrinkwrap method
can be used with any ITA, such as the Hybrid Input-
Output (HIO)44 or Diﬀerence Map45 algorithms, we used
the HIO and Relaxed Averaged Alternating Reﬂections
(RAAR)46 algorithms for this work.

Many of the phase retrieval ITAs can be written as
ﬁxed point iterative equations, which can be written gen-
erally in the form gn+1 =
.
The RAAR algorithm can be represented by the ﬁxed
point iterative equation of the complex-valued real-space
image iterate g46:

gn, for a generic operator

T

T

1
2

(cid:20)

gn+1 =

β(RS RM + I) + (1

β)PM

gn

= [2β PS PM + (1

−

−

(cid:21)
2β)PM + β(PS

I)] gn,

−

(11)

where the operator R = 2P
I is the reﬂector correspond-
−
ing to the projector P , I is the identity operator, and β
is a feedback parameter, which we usually set to β = 0.9.
The two operators PM and PS are the projections onto
the modulus constraint and support constraint, respec-
tively. We apply the modulus according to

6

(12)

PM g =

−

F

G(u)

G(u)

+ǫ

1 


|

|
G(u)

G(u)
|
|
G(u),

+ǫ

I(u) + σ√I (u)
(cid:17)
σ√I (u)
I(u)
(cid:17)

−

,

,

(cid:16)p
(cid:16)p

I(u) + σ√I (u),
σ√I (u),
I(u)

if G(u) >

if G(u) <

p

otherwise, or u

p

−
M

6∈



6∈

F {

g(x)
}

where σ√I is the estimated variance of the measured
diﬀraction amplitudes, G(u) =
, and ǫ is a small
number. M is the set of u where I(u) has been measured.
For example, u
M in the missing sector of data present
when diﬀraction is recorded over a limited range of an-
gles. The operator PM of Eqn. (12) retains the phase
of the complex Fourier amplitude G(u) and projects its
G(u)
to the nearest measured diﬀraction am-
modulus
|
|
I(u)+σ√I (u)]
σ√I (u),
I(u)
plitude, in the interval [
(or does nothing if the modulus already lies within that
range or if I(u) has not been measured). Given the sup-
port S from Shrinkwrap, we apply the support constraint
when retrieving the phase of a complex image using

p

p

−

PS g =

g(x)
0

(

S

if x
∈
otherwise.

(13)

We also perform phase retrieval where we impose real
and positive constraints on the image amplitudes, where
we replace PS with

PS+ g =

g(x)
}

ℜ{
0

(

S and

if x
∈
otherwise.

g(x)
}

ℜ{

> 0

(14)

The HIO algorithm can only be written in terms of
a ﬁxed point iterative equation when applying the sup-
port constraint PS, but not when applying positivity
constraints46. In general the HIO algorithm is given by

gn+1 =

PM gn,
(I

−

(

if x

S′

∈

β PM )gn, otherwise,

(15)

where S′ is the set of elements where PM gn satisﬁes the
support and (if desired) the reality and positivity con-
straints. As with the RAAR algorithm we use a value of
the feedback parameter β = 0.9.

Regardless of algorithm, we monitor the reconstruction

with the real-space image error

E2

S ≡

gn
|

PS gn
2
|

−
PS gn
|

P

2
|

=

x

gn(x)
|
gn(x)
|

S |
6∈
S |

∈

P

x

2
2 .

(16)

P

P

This metric is a measure of the total power in the im-
age that remains outside the support, and is zero for the
case of perfectly satisfying the real-space constraints. We
deﬁne, in a completely analogous way to Eqn. (16), the
error EM corresponding to the modulus constraint de-
ﬁned by PM :

E2

M ≡

gn
|

−
PM gn

PM gn
2
|

|

P

2
|

=

P

√I

2

,

(cid:12)
(cid:12)
(cid:12)

Gn
|
P (cid:12)
(cid:12)
(cid:12)

| −
I

P

(17)

where the equality follows from Parseval’s theorem and is
true only for σ√I = 0. The error metrics ES and EM are
the normalized distances between the current iterate gn
and the support or modulus constraint set, respectively.
The reconstructed image from a reconstruction run (from
a particular set of starting phases) is given by

γM = PM gn,

(18)

for the ﬁnal iterate gn of both the RAAR and HIO algo-
rithms.

The Shrinkwrap algorithm has been used previously to
reconstruct 2D images of thin objects at a resolution of
about 20 nm6. We have found in subsequent studies that
the step of updating the support would sometimes shrink
the support to a shape smaller than the actual boundary
of the object. To counter this eﬀect we have improved the
Shrinkwrap algorithm to prevent it from over-shrinking
the support. Depending on the parameters of the low-
pass ﬁlter and the threshold level, the support may start
to cut oﬀ the extremities of the object. At this point
the support constraint error E2
S increases rapidly and
the reconstructions rapidly degrade with further itera-
tion. This error is thus a good indicator of when the halt
the support reﬁnement. We simply monitor the error
metric and when it increases above a set point we choose
the support saved from 10 iterations prior. This then be-
comes our best estimate of the support and is used as a
ﬁxed support in combination with the RAAR algorithm
for many more (typically 100 to 1000) iterations. We fur-
ther decrease the uncertainty of the retrieved phases by
averaging the retrieved complex images from indepen-
dent and random starting diﬀraction phases using the
Shrinkwrap-derived support constraint33 as described in
Eqn. (18) of Sec. 5 A. If the phase at a particular spa-
tial frequency is randomly recovered from trial to trial,
the average modulus will average to zero, and hence be
ﬁltered out of the recovered image.

The 2D reconstructions shown in this paper were re-
constructed using the RAAR algorithm (Eqn. 11) and the
3D were performed using a combination of HIO (Eqn. 15)
and RAAR. A typical reconstruction process proceeds as
follows. First we deﬁne the initial object support mask by
applying a 2% intensity threshold to the object autocor-
relation, obtained by Fourier transforming the measured
diﬀraction pattern. The support constraint, deﬁned by
the current object mask, is applied to the solution in real
space once per iteration. We typically use a feedback
parameter of β = 0.9 in the RAAR or HIO algorithms.
The object support S is recomputed every 30 iterations

−

n2/n2

by convolving the absolute value of the current recon-
struction γM with a Gaussian of FWHM of initially three
pixels in all dimensions and applying a threshold to the
resultant image at 15% of the maximum value. As the
iterations progress we reduce the width of the Gaussian
blurring function from three pixels to one pixel, follow-
w), with nw
ing the prescription wS = 1 + 2 exp(
regulating the speed at which wS decreases with iter-
ation number n. The reduction in the blurring width
enables the support to better conform to the solution as
the quality of the reconstruction increases. We perform
this Shrinkwrap support determination without applying
any real-space positivity or reality constraint on the im-
age amplitudes (that is, we use the constraint PS in the
RAAR algorithm, or S′ = S in the HIO algorithm). The
ﬁnal support is usually obtained after 300 to 600 iter-
ations, with a stopping criterion that the support con-
straint error E2
S does not exceed 0.2. Once the support
is determined we carry out many iterations of the RAAR
algorithm, starting from random phases, using a feed-
back parameter of β = 0.9.
In some cases, additional
real-space constraints, such as positivity or reality of the
image amplitudes, are also applied.

As shown in Eqn. (12), in diﬀraction space the am-
plitudes of the object guess are matched in magnitude
to the measured diﬀraction pattern amplitude over those
parts of 3D diﬀraction space where the measured inten-
sity is deﬁned. Those parts of 3D diﬀraction space where
there is no measured data are allowed to ﬂoat and are not
constrained. This includes the regions between the mea-
sured Ewald spheres, the missing wedge of data from the
ﬁnite range of rotation angles, the central beamstop re-
gion, and those parts of the diﬀraction pattern where the
measured intensity is suﬃciently low to be regarded as
noise. An additional, optional Fourier space constraint
is to set those pixels beyond the radius of the spatial
frequencies measured by the CCD chip to zero. This as-
serts lack of knowledge of spatial frequencies higher than
those measured by the CCD camera, and eﬀectively pro-
vides a pupil function for the imaging system in three-
dimensional space.

Providing an initial guess for the 3D object support
is not typically necessary but speeds the reconstruction
process and helps break inversion symmetry present in
the object autocorrelation. An initial 3D support esti-
mate can be obtained from the diﬀraction data by ﬁrst
performing Shrinkwrap phase retrieval on a 2D central
section, as described in Sec 4 C. We then extrude the
2D support mask that was generated into 3D to provide
an initial 3D support estimate. If several 2D reconstruc-
tions are available from a range of views, the intersection
of these support functions in 3D can be used to pro-
vide a more detailed initial support estimate. Experience
has shown that even a low-resolution or comparatively
poor support estimate is suﬃcient to almost immediately
break any inversion symmetry in the reconstruction and
hasten convergence of the 3D solution. Performing such
a 2D reconstruction is a common (although not strictly

necessary) step in assessing data quality prior to perform-
ing 3D reconstruction.

7

3. Methods

A. Sample Preparation

×

×

A goal of this study was to be able to unambigu-
ously compare reconstructed X-ray images of a three-
dimensional object with images obtained by another
high-resolution method, such as a scanning electron mi-
croscope (SEM). To accomplish this we fabricated a test
object that consists of a silicon nitride membrane with a
three-dimensional pyramid shape that is decorated with
50-nm-diameter colloidal gold spheres, similar to that
previously described7. The object is three-dimensional
and has a comparable width, height, and depth, measur-
1.8 µm.
ing 2.5 µm

2.5 µm

The pyramid-shaped membrane was fabricated by
lithography using methods similar to those to make sil-
icon nitride windows and silicon nitride atomic-force
microscope (AFM) tips. The starting material was a
double-side polished 200 µm thick wafer of silicon crys-
tal with the crystal 100 axis oriented normal to the sur-
face. Pits with an inverted pyramid shape were etched
into one side of the wafer by anisotropic etching through
a pattern of 2.5 µm-width square holes, lithographically
printed and developed in photo-resist. The anisotropic
etch leaves the 111 crystal planes exposed, so that the
surface normal of any one of the four faces of the pyra-
mid makes an angle of 54.7◦ to the window normal and
the ratio of the depth of the pit to its base width is
1/√2. After removing the photoresist a low-stress sili-
con nitride ﬁlm of 100 nm thickness was grown on the
surface by chemical vapor deposition. Window openings
were then etched from the other side of the wafer after
ﬁrst resist coating and patterning that side, making sure
to align to marks etched in the front surface. The etch
from the back removes silicon, but leaves a free-standing
membrane of silicon nitride, which in this case had one
pyramid-shaped indentation per window. The windows
were made with a slotted shape of about 2 mm width
by 50 µm high. With the 200 µm thickness of the silicon
frame and the pyramid positioned in the center of the
window, this allows a line of sight through the window
at a maximum rotation angle (about an axis in the plane
of the window, parallel to the short window dimension)
of 78◦.

The gold-sphere test object was made by dragging a
small drop of solution of gold balls in water, suspended
from a micro-pipette, across the silicon nitride window
so that it intersected with the pyramid indentation. Best
success was achieved with a slightly hydrophilic silicon
nitride surface, which could be obtained by cleaning the
surface in an oxygen plasma. As the drop was moved
over and away from the indentation, a smaller drop broke
away from the main drop and was captured in the pyra-
mid. This drop quickly evaporated and left the gold balls
in a characteristic pattern where the gold tended to ﬁll in

the edges of the pyramid. The main drop was completely
dragged away from the window, so the only gold balls
on the window were those in the pyramid. A plan-view
SEM image (membrane and wafer perpendicular to the
electron beam) of the object is shown in Fig. 2. The SEM
is however only sensitive to the surface of the object—
the electrons do not penetrate the gold spheres nor the
membrane. The depth of focus of the SEM was larger
than the thickness of the object, and from the plan view
we can determine the lateral coordinates of the topmost
balls and infer the third coordinate from the known ge-
ometry of the pyramid.

The silicon nitride window was glued to a pin so that
the pyramid was close (within about 20 µm) of the ro-
tation axis of the pin. The pin was mounted in a collar
that was attached to a JOEL electron microscope sam-
ple holder. This assembly was mounted into the modiﬁed
goniometer holder of the diﬀraction apparatus17.

B. Data Acquisition

Experiments were carried out at an undulator source at
the Advanced Light Source (ALS) using the Stony Brook
University diﬀraction apparatus17. Pertinent to this ex-
periment, 750 eV (1.65 nm wavelength) X-rays were
selected from the undulator radiation by a zone-plate
monochromator with a spectral resolution of λ/∆λ =
1000. The 5-µm-diameter monochromator exit pinhole
also selects a transversely spatial coherent patch of the
beam. The sample was located 20 mm from this pin-
hole. A direct-detection bare CCD detector, with 20 µm
1300 pixels, was located 142 mm be-
pixel spacing, 1340
hind the sample. We selected sub-arrays of 1200
1200
elements, centered on the location of the zero spatial fre-
quency (direct beam). At these CCD and wavelength
settings we have a real-space sampling interval in x and
y of ∆x = 9.8 nm (in the small-angle approximation) and
a ﬁeld width of w = N ∆x = 11.7 µm. With these settings
the 2.5 µm-wide pyramid object satisﬁes the far-ﬁeld and
sampling conditions discussed in Sec. 2 A. The diﬀraction
from the pyramid object is more than 4
oversampled in
each dimension (s = 4.6).

×

×

×

The frame of the slotted window in which the pyramid
is formed blocks most of the high-angle scatter from the
pinhole that would otherwise illuminate the CCD. This
scatter reveals a projection shadow image of the slotted
window, useful for aligning the pyramid to the beam.
The diﬀraction pattern of the pyramid measured by the
CCD is shielded from this remaining pinhole scatter with
a small aperture placed 6 mm upstream of the sample
(a distance at which the sample can be rotated without
interference). A beam-stop blocks the direct undiﬀracted
beam from impinging on the CCD. More details are given
by Beetz et al.17.

Diﬀraction patterns were collected with the sample ori-
57◦ to +72◦, at 1◦ inter-
ented at rotation angles of
vals (compared with 0.27◦ angular increments required
for full sampling according to Eqn. (6)). The shadow of
57◦ to
the sample support frame limited useful data to

−

−

8

−

+66◦. We additionally collected data at 0.5◦ increments
for a range of 19◦ centered at an object orientation of
26◦ from the head on (φ = 0◦) orientation. To
φ =
keep the sample centered in the 5 µm beam, the posi-
tion of the sample was calibrated by performing a two-
dimensional raster scan of the rotation and y goniometer
motors. The total scattered counts (not including those
blocked by the beam-stop) were collected for each motor
position and the optimum y position (a translation mo-
tion perpendicular to the rotation axis) was then com-
puted for each rotation angle, and these were ﬁt to a
smooth curve as a function of rotation angle. To collect
the 3D dataset, at each rotation angle we took several ex-
posures to accommodate the large dynamic range of 105
of the diﬀraction pattern, and to reduce the area occluded
by the beam-stop (by setting the beam-stop to diﬀerent
positions). After subtracting dark noise, pixel data that
were not saturated and not masked by the beam-stop
were summed over these exposures, and then normalized
by the accumulated incident ﬂux corresponding to that
sum. A typical diﬀraction pattern is shown in Fig. 3
(a), which was composed of 10 or more individual expo-
sures of 0.1 s, 1 s, 10 s, and 60 s duration, for a cumula-
tive exposure of 73 s. The diﬀraction pattern intensities
are displayed on a logarithmic greyscale in Fig. 3. At
the highest angles of the pattern (highest resolution, at
1, along the diagonal) the mean photon count
0.07 nm−
is 1.9 photons per pixel for this sample orientation. The
maximum normalized photon count, which occurs in a
pixel near the center of the pattern is 109,000 photons.
109 photons/s/µm2
The estimated incident ﬂux was 8
(per 400 mA of storage-ring current), and the normalized
incident ﬂuence for the accumulated sum of Fig. 3 (a) was
1011 photons/µm2. The total normalized scattered
3
counts at the CCD over the accumulated exposure time
108 photons (equal
for the pattern in Fig. 3 (a) was 1.6
to the total counts that would be recorded if the detector
had inﬁnite dynamic range and did not saturate).

×

×

×

Views of the diﬀraction data cube are shown in Figs. 4
(a)–(c) and discussed in Sec. 4 A. This cube was as-
sembled from the 123 diﬀraction patterns at 1◦ sample
orientation increments, and 32 patterns at half-degree in-
tervals, by interpolating onto ui,j,φ. The total integrated
exposure time for the complete dataset was 3.2 hours,
with a total incident ﬂuence of 5

1013 photons/µm2.

×

C. Data Diagnostics

As seen in Eqn. (5) the autocorrelation of the object can
be determined from a single inverse Fourier transform of
the measured data without having to recover the diﬀrac-
tion phases. We ﬁnd that computing the autocorrela-
tion image from the measured diﬀraction data is a useful
diagnostic to determine if the measurement fulﬁlls the
sampling requirements, to help identify the object, and
to assess the measurement quality. The inverse Fourier
transform of the diﬀraction pattern shown in Fig. 3 (a)
is given in Fig. 3 (b). The displayed autocorrelation im-
age has been cropped by half in width and height from

9

Fig. 2. (Color online) (a) SEM image of the pyramid test object, consisting of 50-nm diameter gold spheres lining
the inside of a pyramid-shaped indentation in a 100-nm thick silicon nitride membrane. The membrane extends
1.7 mm, the pyramid base width is 2.5 µm, and height is 1.8 µm. (b) An iso-surface
over a window of size 50 µm
rendering of the reconstructed 3D image. (c) Inﬁnite depth of ﬁeld X-ray projection image from a central section of
the 3D diﬀraction dataset, reconstructed using the Shrinkwrap algorithm. (d) A maximum-value projection of the
3D reconstructed image (left) with a vertical white line indicating the location of a tomographic slice (right). The
scale-bar length is 1 µm and applies to all images.

×

the inverse Fourier transform of the diﬀraction pattern,
since with the linear greyscale displayed the rest of the
ﬁeld was black. This autocorrelation image has a well-
deﬁned support which is conﬁned within the image ﬁeld,
showing that we are indeed oversampling the diﬀraction
intensities.

The Fourier relationship of Eqn. (5) oﬀers a further
method to diagnose the diﬀraction pattern I(q) as a func-
tion of the spatial frequency q across the pattern. A
property of the Fourier transform of a function, often dis-
cussed in the context of holography, is that a sub-region
of the transform (or hologram) can be inverse Fourier
transformed to give a spatial-ﬁltered image of the origi-
nal function. The ﬁeld of that image is the full ﬁeld of the
original function. The ﬁltered image may diﬀer from sub-
region to sub-region, since each sub-region contains infor-
mation pertaining to particular spatial frequencies of the
original object function. Nevertheless, for non-periodic
object functions, these images should be consistent and
not vary too dramatically between neighboring regions.
Large inconsistencies in images formed in neighboring re-
gions point to inconsistencies in the measured diﬀraction
data. This gives a convenient way to qualitatively check

the diﬀraction intensities (and also reconstructed phases)
across the diﬀraction pattern, by forming an array of sub-
images, each corresponding to a particular sub-region.
We term this array a “spectrogram”, since it displays
the image information as a function of spatial frequency,
much like the time-frequency spectrograms used in au-
dio analysis. We apply the spectrogram analysis both to
the autocorrelation image, and to reconstructed images
to determine the consistency of the data or reconstructed
phases.

An example of an autocorrelation spectrogram is
shown in Fig. 3 (c), where each image is formed by ap-
plying a 2D Gaussian window function to the diﬀraction
pattern, centered at a location indicated by the image po-
sition in the array. One eﬀect that is immediately notice-
able to the eye is that, at the higher resolution positions,
the images vary according to their azimuthal position in
the spectrograph. In particular features that are tangen-
tial to circles of constant resolution have greater contrast
than features that vary in a radial direction. The eﬀect
gives the appearance that the spectrograph is rendered
onto the surface a sphere, but is in fact a signature of
limited longitudinal coherence of the incident beam. For

10

Fig. 4. (Color online) (a) A rendering of the entire 3D
diﬀraction dataset. (b) A central slice of the dataset in
the a plane rotated by
24◦ about the y axis from the
ux-uy plane.
(c) A central slice of the dataset in the
ux-uz plane. All patterns are displayed on a logarithmic
greyscale (white highest intensity). The half width of
each pattern is ux,max = 0.048 nm−

1.

−

Fig. 3. (a) The diﬀraction pattern for the φ = 0◦ orien-
tation of the pyramid. (b) Autocorrelation image i(x),
formed by Fourier transforming the diﬀraction inten-
sity pattern of (a) after ﬁltering, displayed with a linear
greyscale (white highest intensity). Scale bar is 1 µm.
(c) Autocorrelation spectrogram of the same single-view
diﬀraction pattern of the pyramid, displayed with a log-
arithmic greyscale.

a given q, represented in the spectrograph by the po-
sition of the image, pairs of points of the object that
are perpendicularly bisected by the vector q will inter-
fere with zero path diﬀerence. These points will interfere
with the same phase for all wavelengths (assuming no
dispersion of the scattering factors). The path diﬀerence
at q of other pairs of points along this line perpendicular
to q depend only in second order on their mean displace-
ment from the direction of q. The path diﬀerences of
rays scattering from pairs of points separated in the ra-
dial direction, however, vary to ﬁrst order on their radial
separation. Therefore, a limited longitudinal coherence,
which limits the path diﬀerence over which interference
occurs, will reduce interference for points separated by
a direction parallel to the q vector by a much greater
extent than for the perpendicular direction. The spec-
trograph gives a good visual determination of the degree
of longitudinal coherence, and we see from Fig. 3 (c) that
the longitudinal coherence is adequate for diﬀraction out
to the center edge of the pattern, but not quite adequate
for the corner of the pattern. By comparison to spec-
trographs of simulated diﬀraction patterns, we estimate
that the relative bandwidth of the incident beam in this
case is approximately ∆λ/λ = 1/600.

It is also clear from Fig. 3 (c) that the data are in-
consistent at the low spatial frequencies, since at those q
positions there is noticeable intensity outside the support
of the pyramid object autocorrelation. This is due to the
fact that low-frequency data are missing due to the beam-
stop, and also to a lesser degree due to scattering from the
sample substrate or the coherence-deﬁning pinhole. The
data are visibly noisier in windowed regions located in the
ﬁrst three rows of the fourth column of Fig. 3 (c), due to
the stalk that holds the beamstop and which was moved
over several positions in this region for the cumulative
exposure. The noise and inconsistency can cause the
Shrinkwrap algorithm to fail (in which it keeps shrinking
beyond the object’s boundary), especially when applied
without an image-space constraint such as positivity. We
ﬁnd the Shrinkwrap algorithm consistently converges to
a stable object support when we apply a high-pass ﬁlter
to the diﬀraction intensities prior to interpolation. This
ﬁlter has the form

f (q) =

(q/2a)4 exp(2
1,

−

(

q2/2a2),

if q < 2a,
otherwise,

(19)

q
|
|

where q =
and the ﬁlter radius a is 100 pixels, or less
than 10% of the array width. The image thus formed will
be a high-pass ﬁltered image, equivalent to the coherent
image formed by a lens with a central obscuration. The
ﬁlter was applied to the data of Fig. 3 (a), prior to Fourier
transformation, to diminish the eﬀects of the beamstop.
This ﬁlter also regularizes the inverse transform, which
is ill-posed in part due to the missing data behind the
beamstop29,32, by simply setting the unknown intensities
to be zero. The eﬀect of this high-pass ﬁlter is to con-
volve the coherent image with the Fourier transform of
the ﬁlter. This causes a ringing of the image, which gives

11

rise to negative amplitudes in the image, and a slightly
larger image support. We also zero the diﬀraction inten-
sities of the bright cross streaks seen in the x-z central
section, to suppress artifacts that they may cause.

D. Computational Implementation

The two key computational challenges in implementing
high-resolution 3D phase retrieval at the time or writ-
ing are performing the numerous 3D FFTs required in
a reasonable period of time and managing the memory
requirements of the large 3D data arrays.

−

p

Memory requirements are dictated by the size of the
data sets acquired and by the phase retrieval algorithms
used. For the iterative transform phase retrieval meth-
ods described in Sec. 2 C we require four or more 3D
arrays with the same number of elements as the inter-
polated input diﬀraction volume. Speciﬁcally, the arrays
I(u),
required are the input diﬀraction modulus data (
ﬂoating point), the current and previous iterates (gn(x)
1(x), complex ﬂoating-point data) and the sup-
and gn
port constraint (S, byte data). The estimated modulus
standard deviation, σ√I requires another ﬂoating point
array, but in the 3D reconstructions we set σ√I = 0 to
reduce the memory requirement and speed up the recon-
structions.
In principle fast Fourier transforms can be
performed on arbitrary sized arrays, however it is advan-
tageous to perform reconstructions on a regular grid with
2n grid points on a side. Our interpolated data array is
a giga-voxel data cube containing 10243 = 230 elements
which requires a total of 8 GB of computer memory per
array for single-precision complex data. The minimum
memory footprint for single-precision iterative object re-
construction using either the HIO or RAAR algorithm on
a 10243 data cube is therefore 2
8 GB complex arrays,
plus 1
1 GB support
array, giving a total memory footprint of 21 GB of data,
where we use the deﬁnition of 1 GB = 230 bytes. The
minimum memory footprint for performing basic HIO
and RAAR reconstruction on 3D arrays of diﬀerent sizes
is given in Table 1. Note that this is the minimum mem-
ory footprint needed to perform a HIO reconstruction
and that more memory may be required depending on
the speciﬁc implementation. For example, FFT speed
can be increased through use of temporary “workspace”
arrays which require additional memory, and maintain-
ing a running sum of successive images γM requires an
additional complex-valued array to be retained in mem-
ory. The memory calculations above include only the
data arrays and do not take account of operating system
requirements and the executable code itself.

4 GB diﬀraction cube data and 1

×

×

×

The second computational challenge is eﬃcient eval-
uation of the numerous 3D Fourier transforms required
for 3D phase retrieval. The Fourier transform of a sin-
gle large data set is not trivially parallelizable, in that
the problem problem can be easily broken into separate
parallel tasks and distributed over many computer pro-
cessors as is the case, for example, with ray tracing and
partially coherent imaging where each CPU can work on

Array size Single Precision Double precision

ray sizes on this cluster is given in Table 2.

2563
5123
10243
20483

336 MB
2.6 GB
21 GB
168 GB

592 MB
4.6 GB
37 GB
296 GB

Table 1. Minimum memory footprint required for it-
erative 3D phase retrieval for various array sizes. The
arrays required are the input diﬀraction data (ﬂoating
point), the current and previous iterates (complex single
or double precision ﬂoating-point data) and the support
constraint (byte data).

a sub-set of the entire problem without the need for in-
tensive inter-node communication during execution. The
nature of the Fourier transform means that any one el-
ement of the input array aﬀects all elements of the out-
put, requiring inter-node exchange of array data at each
Fourier transform step to ensure that all CPUs work to-
gether to solve the one large FFT.

We overcome the problem of eﬃciently calculating
distributed Fourier transforms by using the dist fft
distributed giga-element fast Fourier transform library
from Apple Computer speciﬁcally written for this project
by the Apple Advanced Computation Group18. This
FFT library distributes the Fourier transform calcula-
tion load eﬃciently over many processors and has been
hand-optimized to take advantage of the G5 architecture
used in the Apple Macintosh line of computers and the
“Altivec” single-instruction-multiple-data (SIMD) ﬂoat-
ing point vector processing unit. Distributed FFT li-
braries are also available elsewhere, for example in ver-
sion 2 of the FFTW libraries47, but at this time these
do not support SIMD vector processing extensions and
proved to be slower on our platform. dist fft decom-
poses the input 3D data set into nproc discreet data slabs
consisting of a n
(n/nproc) voxel sub-portion of the
original data array. Only a distinct portion of the array
resides on each CPU at any given time enabling data sets
much larger than the memory of each individual node to
be computed, and the distributed memory nature of the
FFT is exploited through parallelization of all steps in
the reconstruction code. Standard message passing inter-
face (MPI)48 commands are used to communicate data
between processes.

×

×

n

We ran fully parallelized reconstruction code on a 16-
node 2.0 GHz dual-processor (32 processors total) Mac-
intosh Xserve G5 cluster with 4 GB RAM per node. To
maximize inter-process communication speed we used
high-speed,
low-latency Mellanox Inﬁniband intercon-
nects to carry MPI traﬃc between compute nodes. Us-
ing this cluster the processing time on a 5123 array is
2.2 seconds per iteration using the HIO phase retrieval
algorithm, and an acceptable 3D reconstruction can be
produced in under 2500 iterations for a total computa-
tion time of 2.5 hours on a 5123 grid. The individual
FFT timing and total reconstruction time for typical ar-

12

Array size

Time per 3D

Time per 3D
Fourier transform reconstruction

2563
5123
10243

73 ms
850 ms
7.9 s

10 min
1.5 hr
14 hr

Table 2. Computing times using a cluster-based Fourier
transform and reconstruction code on 16 G5 dual-
processor Xserve compute nodes. Fourier transform tim-
ings are wall time per individual FFT. Reconstruction
timings are for a complete 3D reconstruction consisting
of 2000 iterations of HIO phase retrieval complete with
two FFTs per iteration plus other operations required to
calculate the reconstruction.

4.

Image Reconstruction

A. Three-Dimensional Images

A full 3D image is obtained by performing phase re-
trieval on the entire 3D diﬀraction dataset. The result-
ing volume image reveals the structure of the object in
all three dimensions and can be visualized in many ways
including forming projections through the data or slices
(tomographs) of the data. Speciﬁc segmentation anal-
yses can be carried out on the volume image to deter-
mine properties such as strength of materials49. Three-
dimensional reconstructions were performed by interpo-
lating the diﬀraction intensities at ui,j,φ onto a 10243
grid. Representations of the interpolated diﬀraction in-
tensities are given in Fig. 4. Note that the 1◦ angular
increments of the object rotation are just less than four
times larger than the 0.27◦ requirement of Eqn. (6) for
this object, and that we have a 40◦ sector of missing
data due to our limited range of object orientations, as
well as data lost to the beamstop. The eﬀect of the 1◦
rotation increment is apparent in Fig. 4 (b), where the
gaps between the measured Ewald spheres are seen in the
ux-uy plane (referred to as a central section) extracted
from the data cube. The limited range of views are read-
ily apparent in Fig. 4 (c), which shows the ux-uz central
section.

The three-dimensional phase retrieval code described
above in Sec. 3 D was applied to the assembled 3D data
to produce a full 3D reconstruction from the diﬀraction
cube. We applied the Shrinkwrap algorithm, as described
in Sec. 2 C, to determine the 3D support mask and the
diﬀraction phases. We performed phase retrieval using
either the real-positive real-space constraint PS+ or the
support constraint PS. For the complex image recon-
struction, as with the case of reconstruction from central
sections discussed below in Sec. 4 C, the solution was reg-
ularized by ﬁrst applying the high-pass ﬁlter of Eqn. (19)
to the diﬀraction intensities. For the real positive recon-
struction the missing amplitudes were unconstrained and

were allowed to be recovered by the algorithm. The re-
construction success with the sparsity of data we have
in this case is undoubtedly due to the sparseness of the
object itself. In essence the object is a membrane, and
the 3D speckles are elongated by up to 50 pixels in direc-
tions perpendicular to the pyramid faces, as can clearly
be discerned in Fig. 4 (c).

Fig. 5. Maximum value projections along three orthogo-
nal directions of the reconstructed 3D image. Projections
were performed along (a) z, (b) x, and (c) y directions.
(d) An enlarged region of (a), for comparison with Fig. 8.
The 3D image was reconstructed using reality and posi-
tivity constraints. The scalebars are 500 nm.

A 3D iso-surface rendering of the real-positive con-
strained reconstructed pyramid is shown in Fig. 2 (c),
where we also display a slice from the volume image in
Fig. 2 (d). Three images from the 3D pyramid image
are shown in Fig. 5, showing the maximum value projec-
tion, along the three orthogonal axes, of the real part of
the 3D image that was reconstructed using the support
constraint with real positivity, PS+. Each pixel of this
image is given by the maximum value encountered along
the path that projects onto that pixel, and illustrates
a visualization method available only with the full 3D
image array.
In initial reconstruction trials using only
a support constraint PS we observed a linear ramp in
the imaginary component. This was essentially a phase
ramp, due to a shift of the diﬀraction data in Fourier
space caused by an inaccurate measurement of the loca-
tion of the zero spatial frequency (the direct beam) on
the CCD. We shifted the diﬀraction data by an amount
that minimized the real-space phase shift, which required
shifting the data by half-pixel amounts. This recentering
of the data was necessary before we could apply the real
positive constraint on the image. Further analysis of the
images is carried out in Sec. 5.

13

B. Two-Dimensional Images

Two-dimensional images are useful for visualizing and
quantifying objects, and most diﬀraction imaging exper-
iments performed to date have been 2D. However, if the
object is thick, then the interpretation of an image recon-
structed from a single Ewald sphere is not trivial. Most
notably, as compared with our full reconstructed 3D im-
age, the 2D image will have defocus artifacts that do not
diminish in power with displacement of the object along
the beam axis. However, in some cases obtaining a full
3D reconstruction may not be achievable, for example
when imaging non-reproducible objects with single pulses
of an XFEL. It is thus instructive to compare 2D images
reconstructed from single-view diﬀraction patterns with
the 3D image.

We ﬁrst consider how thin an object must be to be
considered two dimensional. In a 2D reconstruction from
a single diﬀraction pattern, the spatial frequencies that
are passed to the diﬀraction pattern are constrained on
the Ewald sphere according to

λ
2

(cid:0)

(cid:1)

qz = 1/λ

1/λ2

q2
x −

q2
y ≈ −

−

x + q2
q2
y

,

(20)

−

q

≪

→

where the approximation is for small scattering angles,
or qx
1/λ. We can deﬁne the numerical aperture of
the diﬀraction pattern as NA = qx,max λ, in analogy with
imaging with a lens (of square aperture for the case of a
square detector, with the NA deﬁned here along the half-
width of the square rather than the diagonal), which gives
the expression of maximum longitudinal distance of the
NA2/(2λ). For a 2D object
Ewald surface, qz,max ≈ −
0, the 3D transform will be indepen-
of thickness D
dent of the longitudinal frequency qz (rods in the z di-
rection) and so the measurement on the Ewald sphere is
equivalent to measurement of the 2D diﬀraction intensi-
2. In such case there will be no artifact in
ties
O(qx, qy, 0)
|
|
the image caused by not actually measuring intensities on
qz = 0. An object of ﬁnite thickness D will have a coher-
ent diﬀraction pattern with speckles of width 1/D in the
longitudinal direction. If, at the highest transverse fre-
quencies, the Ewald sphere substantially cuts through a
speckle that is centered at qz = 0, then the measurement
will again be equivalent to the 2D diﬀraction intensities
on the qz = 0 plane. That is, we can consider an object to
be thin or two-dimensional if the Ewald departure is no
more than 1/(4D), or half the speckle half-width, which
corresponds to

D <

λ
2NA2 ,

(21)

or, equivalently, the thickness D must be less than a
depth of focus. For the experiments with the pyramid
object at λ = 1.65 nm and NA = 0.084, this thickness
limit is D = 120 nm, which is considerably smaller than
the 1.8 µm thickness of the pyramid.

Equation (21) does not imply, however, that diﬀraction
imaging performs optical sectioning where only the parts

of the object located within the depth of focus are im-
aged. The thickness limit simply implies that the 2D
single-pattern image of an object thicker than D will
contain artifacts due to the information that is cut oﬀ
by the transfer function. Consider an object contain-
ing two parts (e.g. screens) that are separated by more
than a depth of focus. As with coherent imaging with
the equivalent aberration-free thin lens, partial informa-
tion from both screens of that object will be transferred
in the imaging process. In fact, in diﬀraction imaging,
there is not necessarily any preferred image plane since,
by the Fourier shift theorem, a shift δz of an object along
the beam axis z will cause only a phase shift given by
2πδz qz and hence no change to the diﬀracted intensi-
−
ties (for small enough δz that the change in distance to
the detector does not change the eﬀective NA and scale
the pattern on the detector). Note that from Eqn. (20)
the phase shifts of the 2D spatial frequencies of the im-
age, due to the defocus δz, will be π δz λ(q2
y), as
expected from the Fresnel propagator50. The position of
the focal plane can be chosen in the phase retrieval step,
a fact that was demonstrated computationally and ex-
perimentally by Spence et al.51. In that work the focus
of the retrieved image of an object of two screens sepa-
rated by some depth could be chosen by setting a tight
support for the features in one screen or the other. As
shown by Spence et al., once the phases of the diﬀraction
intensities have been retrieved, images can be generated
at any position through focus, by Fresnel propagating the
image wave-ﬁeld (equivalent to applying the appropriate
quadratic phase term to the diﬀraction phases).

x + q2

Fig. 6. Real part of the image reconstructed from a sin-
gle view diﬀraction pattern (a), and real part of the im-
age formed by numerically propagating (a) by
0.5 µm
(b) and +0.7 µm (c). Line-outs from the image near the
pyramid center (d) and arm extremity (e) for a range
of propagation from -2.5 µm to +2.5 µm. The locations
of these line-outs are indicated by arrows in (a). The
diﬀerence of the plane of best focus for these two image
locations is apparent. Scale bars are 500 nm.

−

The defocus eﬀects of a single view are illustrated in
Fig. 6, where we show 2D images of the waveﬁeld at the
pyramid object, reconstructed from a single-view diﬀrac-

14

±

tion pattern. In this example, we use the diﬀractogram
for the object rotated by φ = 24◦ from the head-on
(plan view) orientation. The image γM reconstructed
by Shrinkwrap, from the single-view diﬀraction pattern,
is shown in Fig. 6 (a). No real-space reality nor positiv-
ity constraint was applied and the reconstructed image
is complex. For this object and view, the edges of the
object (its support) are at a range of heights along the
beam axis. In this case the end-point support that the
Shrinkwrap algorithm arrived at was tightest around the
balls halfway along the arms of the pyramid, and conse-
quently this is the plane of best focus. This focal plane
gives the greatest overall image contrast, which explains
why Shrinkwrap converges to it. The complex image can
be numerically propagated, by convolution with the Fres-
nel propagator, by any arbitrary amount δz. We gener-
ated a series of numerically refocused images, where δz
varies between
2.5 µm, in 50 nm steps. As the refocus
distance is increased the best focus moves along the pyra-
mid arms to their extremities. The diﬀerence in focus of
balls near the vertex and arm extremities can be seen in
Fig. 6 (d) and (e) which show x-δz line-outs of the real
part of the complex image. The diﬀerence between the
best focus for these two cases is 1.2 µm, which agrees with
the 3D image (Sec. 4 A) and the known geometry of the
pyramid. It should be noted that this computational fo-
cusing does not constitute 3D imaging, but is simply the
propagation of a 2D coherent ﬁeld. The optical transfer
function (OTF) for this imaging system is the Ewald sur-
face, and in this situation with coherent illumination the
integrated intensity of the image does not change with
defocus (a consequence of Parseval’s theorem and the in-
variance of the diﬀraction intensities with defocus). That
is, it is unlikely that numerical defocusing of a compli-
cated object could give results that could be as easily in-
terpreted as for the pyramid-membrane test object used
here. This situation is unlike partially-coherent imaging
in a microscope, where out-of-focus objects contribute
less power to the image and some optical sectioning can
be carried out16.

Another consequence of the “defocus artifact” of 2D
images, is that the 2D image of a thick real object is com-
plex, which means that a real-space positivity constraint
cannot be applied during the phase retrieval process. A
positivity constraint, when valid, is known to be very
eﬀective in deriving the diﬀraction phases, and impor-
tant in direct methods in crystallography and a strong
constraint in diﬀraction imaging. Here, a real object is
one in which the object’s 3D complex transmission func-
tion o(u) is real, to a multiplicative complex constant.
Propagation of the out-of-focus parts of the object to
the selected image plane will give rise to a large varia-
tion in the complex values of image, as demonstrated in
Fig. 7. Here we show the complex amplitudes of images
recovered from calculated diﬀraction patterns of simu-
lated objects. The simulated objects consisted of gold
balls of equal size, distributed in a similar way to the
In the ﬁrst case (Fig. 7 a) the z
pyramid test object.

coordinate of all ball centers was set to zero to construct
a quasi-2D object. Ignoring the arbitrary phase shift, the
reconstructed image is real although not strictly positive
(the negativity of the image is due to the truncation of
the diﬀraction pattern). The calculated image values are
complex for the 3D object (Fig. 7 b) and there is a rough
correlation between absolute value and phase of the val-
ues. This non-reality can also be explained by the curva-
ture of the Ewald sphere. The 3D diﬀraction magnitudes
of a real object are centrosymmetric, whereas the Ewald
u)52. In
sphere does not cut through both O(u) and O(
general, a positivity constraint will only be applicable for
the full 3D image, 2D projections (discussed in Sec. 4 C),
2D images of thin objects, and 2D images of objects with
a mirror-plane symmetry.

−

Fig. 7. Distributions of the real-space complex ampli-
tudes γM , in the Argand plane, of simulated single-view
coherent images for a 2D (a) and 3D (b) object consist-
ing of 50 nm diameter gold balls, for an X-ray wavelength
of 1.6 nm. Distributions of complex amplitudes of images
reconstructed from experimental data, for (c) the inﬁnite-
depth of focus 2D projection image shown in Fig. 8, (d)
for the single-view 2D image of Fig. 6, and (e) the full
3D image. Cases (c) and (d) were reconstructed using
PS, and (e) using PS+.

If the object can be considered two-dimensional and
positive, a positivity constraint will have the eﬀect of fo-
cusing the image. Usually the support constraint is loose,
and even if the shape of the object is well known or de-
termined from the Shrinkwrap algorithm, for example,
there may still be room for a defocused image to be con-
tained within the support. The degree of defocus allowed
by the support depends on how tight it is. The defocused
image of a real 2D object with sharp edges or high fre-
quencies will be real but include negative values. The
focused image will be that which is most positive, and
hence a positivity constraint will select that image. This
is true not only for defocus, but for any other low-order
aberration (astigmatism, coma, etc.). That is, without
the positivity constraint, there are slowly-varying phase
modes that cannot be determined, and the number of
these modes depends on how tight the support is. The
same argument applies for 3D reconstruction of positive

15

3D objects.
In this case, the phase errors will be low-
order 3D modes, which cannot be thought of as focus or
other aberrations of an optical system, but are simply
unconstrained phase errors in Fourier space.

C.

Inﬁnite Depth-of-Focus Two-Dimensional Images

Defocus in a 2D image formed from a single diﬀraction
pattern is a consequence of the Ewald sphere OTF, as de-
scribed above. The focal plane of the image may be var-
ied by multiplying the Fourier transform of the 2D image
by a quadratic phase term. In a full three-dimensional re-
construction, there is no concept of defocus. A shift of the
object by δz along the beam axis causes the phase ramp
2π δz uz across the 3D transform. This causes a shift
−
of the image, no diﬀerent to shifts δx or δy in the other
dimensions. There is no optical axis in the reconstructed
3D image, so there is no defocus. Similarly, there is no
defocus in a 2D projection image formed by integrating
the 3D image along a speciﬁc direction. A 2D projection
may be recovered from the diﬀraction intensities with-
out having to ﬁrst undergo a full 3D reconstruction, and
we ﬁnd this is a useful step to quickly examine our 3D
datasets. By the Fourier projection theorem, the projec-
tion image is formed from a central section in reciprocal
space, e.g. the plane uz = 0 gives the projection along
the z axis. We have performed phase retrieval on central
sections of the pyramid diﬀraction data, by ﬁrst extract-
ing the appropriate diﬀraction intensities from all views.
One example of a central section is shown in Fig. 4 (b),
which was generated by linear interpolation of measured
intensities at ui,j,φ onto the uz = 0 plane. The projection
images that we reconstruct from experimental data are
superior to the reconstruction on a single Ewald sphere.
One example is shown in Figs. 8 (a) and (b), which can
be compared with Fig. 6. In the projection images, balls
at the apex of the pyramid are similar to the balls at the
base, whereas in the single view image, the balls at the
apex appear out of focus. The image of Figs. 8 (a) and (b)
was obtained using the Shrinkwrap algorithm (parame-
ters given in Sec. 2 C), after ﬁrst regularizing by ﬁltering
the diﬀraction intensities according to Eqn. (19). The
missing data in the arc-shaped regions seen in Fig. 4 (b)
were allowed to ﬂoat in the reconstruction of the complex
image, according to Eqn. (12).

Figures 8 (a) and (b) depict the real part of the com-
plex image, and the distribution of complex values of the
reconstructed image is given in Fig. 7 (c). As compared
to the distribution of complex values of a reconstructed
image from a single view diﬀraction pattern, given in
Fig. 7 (d), the values of the projection image are clustered
closer to the real axis. This is as expected since there are
no defocus artifacts, and the object consists mostly of a
single material (gold) as was simulated in Figs. 7 (a) and
(b). Thus, we should be able to apply the reality and pos-
itivity constraints of Eqn. (14) to the projection image,
to further improve it and allow this extra information
to help reconstruct the spatial frequency amplitudes and
phases that are missing behind the beamstop. This was

16

brighter regions in the projection image of Fig. 2 (b). We
conﬁrm that the 3D pyramid geometry determined from
the reconstructed volume image is consistent with the
manufacture of the pyramid. We measure an included
angle between opposite faces of the pyramid of 70
1◦,
compared with the included angle of 70.5◦ between the
111 and 1¯11 crystal planes of silicon.

±

The volume images display good spatial resolution in
the x, y and z dimensions. Quantifying resolution is not
straight forward since we do not have an exactly known
3D standard—the SEM only shows the surface of the ob-
ject, for example, and this method cannot reveal the 3D
structure. We estimate the resolution of our images by
examining both their Fourier and real-space representa-
tions. In Fourier space we base measures of resolution
on the signal to noise of measured diﬀraction intensities
and the consistency of recovered phases, whereas in real
space we ﬁt models to 3D images of isolated balls.

A. Reconstruction Consistency and Resolution

The performance of our imaging technique could be quan-
tiﬁed in Fourier space, in principle, by measuring the
modulation transfer function (MTF). For the numerical
reconstruction technique used here this MTF would en-
capsulate resolution limits due to signal-to-noise, data
alignment and regions of missing data, as well as al-
gorithm stability and uniqueness. The direct compu-
tation of the MTF would require computing the ratio
of the image Fourier amplitudes to the Fourier ampli-
tudes of the actual object, which again requires an ac-
curate quantitative model of the actual object structure
at high resolution. Without such a model we can base
an estimate of the upper limit of the modulation trans-
fer frequency cut-oﬀ on the signal to noise of the mea-
sured diﬀraction data plotted in Fig. 9. The largest spa-
tial frequency used in the the interpolated 3D diﬀrac-
tion dataset (recorded near the corner of the CCD) is at
1. At this resolution shell
umax = √2N ∆q = 0.068 nm−
we recorded an average of < 1 photon per pixel, and a
1. (Since
SNR of 1 photon per pixel at u = 0.062 nm−
the noise level of our camera is considerably less than 1
photon, we assume the noise in our diﬀraction patterns
determined by photon shot noise.) If we assume hypo-
thetically that the diﬀraction phases are known then the
image can be fully represented, without loss of informa-
tion, with a pixel sampling of ∆q = 1/D, where D is
the width of the object, corresponding to s = 1, and so
we could rebin our oversampled data into larger pixels
with a correspondingly higher photon count. Summing
in this way over pixels (referred to as pixel binning) is
not the same as resampling however, and such an oper-
ation would multiply the autocorrelation image with the
Fourier transform of the summed pixel shape, which will
be a function that falls to from unity at the image center
to 2/π at the edge of the autocorrelation image. The ef-
fect could be deconvolved from the pattern, but we avoid
that by binning to a pixel sampling of ∆q = 1/(s D),
with s = 2, which is the Nyquist critical sampling inter-

Fig. 8. Inﬁnite depth of focus projection images, for the
object orientation φ = 0◦.
(a) Reconstruction from a
2D central section interpolated from the 3D diﬀraction
dataset. The reconstruction was performed without a
positivity constraint, E2
S = 0.167. (b) Enlargement of
the lower right arm of (a). (c) [and also Fig. 2 (c)] Recon-
struction from the 2D central section, using a positivity
constraint, E2
S = 0.072. (d) Projected image formed by
integrating the full 3D reconstructed image, E2
S = 0.113.
The scalebars are 500 nm.

the case for Figs. 2 (b) and 8 (c), which is the real-positive
constrained reconstruction from the same central section
as for Fig. 8 (a). In this case the diﬀraction data were
not ﬁltered. This reconstruction was carried out using
the same support mask derived by Shrinkwrap and used
for the reconstruction of the complex image. Since they
were constrained, the complex amplitudes of the image
were distributed along the real axis, with some deviation
from real for smaller amplitudes that could be attributed
to noise and scattering material other than gold (e.g. the
silicon nitride pyramid).

5.

Image Analysis

Both the reconstructed X-ray 2D projection image de-
scribed in Sec. 4 C and the 3D image described in Sec. 4 A
clearly show good agreement with the SEM image of
Fig. 2 (a). When we overlay a semi-transparent version
of the projection image of Fig. 2 (b) on the SEM image
(a) we see that the locations of all balls visible in the
SEM match with the balls visible in the X-ray image,
to within a pixel. In the X-ray volume image however
we can locate more balls than visible in the SEM image.
The slice image of Fig. 2 (d) reveals that the gold balls
of the object are not entirely a single layer, but the arms
of the structure are several balls deep in places. The
balls were deposited on the inside surface of the silicon
nitride pyramid, and it is clearly seen that these balls
are indeed ﬂush with the intersecting edges of the pyra-
mid. The regions where the balls are layered give rise
to a higher projected image intensity which shows up as

val of the object’s autocorrelation function. The mea-
sured data were collected at s = 4.6, so resampling to
s = 2 gives an average of 1 photon per pixel (SNR =
1. If we take a measure of resolu-
1) at u = 0.066 nm−
tion as the frequency at which the SNR of the rebinned
data is unity, then we ﬁnd that the average 3D cutoﬀ is
1 or a smallest resolvable half period of 7.5 nm.
0.066 nm−
This is very close to the smallest half period of 7.3 nm
limited by the detector NA.

The phase retrieval process recovers the diﬀraction
phases with a limited accuracy, due to factors includ-
ing SNR of the diﬀraction amplitudes, missing data, the
inconsistency of constraints, and systematic errors in the
data (such as errors in interpolation). These errors in
phase reduce the resolution of the synthesized image.
With a complex image a loose support constraint will
lead to unconstrained low-order aberrations, for example,
as was discussed in Sec. 4 B. In our case of reconstruct-
ing complex 2D images, with low frequencies missing due
to the beamstop, we have observed that phase retrieval
from independent random starts may diﬀer by a phase
vortex (right or left handed), centered at the zero spatial
frequency. This too has the eﬀect of reducing the im-
age resolution. One way to quantify the eﬀect of these
phase variations is to determine the correlation between
phases retrieved from independent random starts of the
phase-retrieval algorithm. For example, we could com-
pute the diﬀerential phase residual of these two solutions
in the same way that independent images are compared
in cryo-electron microscopy53 (Chap. 3, Sec. B). Since
we have the ability to compute an unlimited number of
reconstructions from independent random starts, a more
appropriate choice is to determine the variation in re-
trieved phases as a function of resolution as suggested
by V. Elser33. More speciﬁcally, the average of the in-
dependent complex reconstructions is computed, and the
square of the Fourier amplitudes of this average are com-
pared with the measured diﬀraction intensities. Where
the phases are consistently retrieved to the same value,
the squared modulus of the average will be equal to the
constrained modulus, and the ratio will be unity. Where
the phases are random and completely uncorrelated, the
average will approach zero. Thus, the ratio is eﬀectively a
transfer function for the phase retrieval process, and the
average image is the best estimate of the image: spatial
frequencies are weighted by the conﬁdence in which their
phases are known33. All 2D and 3D images displayed
in this paper are averages of more than 300 independent
phase retrieval trials. That is, the best estimate of the
image is given by

γM =

γM eiφ0

,

(cid:10)

(cid:11)

(22)

where
denotes an average over independent recon-
structions. Analogous to the Modulation Transfer Func-
tion (MTF) of a coherent imaging system, we deﬁne the

h i

17

(23)

Phase Retrieval Transfer Function (PRTF) as

PRTF(u) = |F

u

γM }|
{
I(u)

=

ΓM (u) eiφ0
I(u)

(cid:12)
(cid:10)
(cid:12)

,

(cid:11)(cid:12)
(cid:12)

p

where ΓM is the diﬀraction amplitude with retrieved
phases, the Fourier transform of Eqn. (18). Plots of the
PRTF, averaged over shells of constant u and where I(u)
are non-zero, are shown in Fig. 10 (a) for the 3D image
of Fig. 5 and for the 2D projection image of Fig. 8 (a).

p

Intensity
SNR

i

)
l
e
x
p
/
s
n
o
t
o
h
p
(
 
y
t
i
s
n
e
t
n
I

105

104

103

102

101

100

10-1

0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
u (1/nm)

 

50.0 25.0 16.7 12.5 10.0

 8.3

 7.1

Half period (nm)

Fig. 9. 3D diﬀraction intensities I(u), averaged over
shells of constant u, in units of average photon count
per CCD pixel. The average over constant u of the 3D
signal to noise ratio (SNR) of the measured intensities is
shown with a dashed line.

When computing the average image γM , the arbitrary
multiplicative phase constant φ0 of each image must be
adjusted to a common value so that the random varia-
tion of this constant does not reduce the average, which
would result in a low value of the transfer function. We
do this for the ﬁrst reconstructed image γ(0)
M by ﬁnding
the constant phase that maximizes the real part of that
image, which can be achieved by ﬁnding the value φ0 that
maximizes54

2

,

o

α =

γ(0)
M (k) exp(iφ0)

ℜ

k
X

n
2

= 1
4

k
X

2

γ(0)
M (k)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ γ(0)

M (k)2e2iφ0 +

γ(0)
M (k)∗
(cid:16)

(cid:17)

2

e−

2iφ0 ,

(24)

for an image with with pixels (or voxels) indexed
by integers k and complex values γM (k). We max-
imize the square of the real part to allow for posi-
tive and negative real values. The value α can be
maximized by maximizing either the second or third
terms of Eqn. (24), and we do so by ﬁnding the
M (k)2, and set-
phase φ of the complex value
ting φ0 =
φ/2. The subsequent images γM are ad-
P
justed by ﬁnding the constant phase φ1 which minimizes

k γ(0)

−

18

did not have a vortex mode. The eﬀect of removing the
vortex modes from the average image is shown in Fig. 10
(a). As is seen in that ﬁgure the PRTF is uniformly
increased across all frequencies. This is due to the fact
that the left and right handed vortex modes sum together
to give a variation of the modulus which varies as the
cosine of the azimuthal angle in the diﬀraction pattern,
and which averages to zero in the average around this
complete circle for each u.

The resolution cutoﬀ of the phase retrieval process
can be given by the spatial frequency at which the
PRTF extrapolates to zero. For all cases here, this fre-
quency is greater than the measurement cutoﬀ of umax =
1, or resolution of 7.4 nm. A more conserva-
0.068 nm−
tive estimate of the resolution is given by the frequency
at which the PRTF reaches a value of 0.5. For the vortex-
corrected 2D reconstruction this occurs just at umax, but
1, or a
for the 3D image this corresponds to 0.048 nm−
resolution of 10.4 nm. In this case the average resolution
cutoﬀ is worse than the 2D case because the 3D PRTF is
diminished along the uz direction where the diﬀraction
data are missing (which reduces the average over the u
shell). This is illustrated in Fig. 10 (b), where we display
the 3D PRTF as a surface for which it has a value of
0.5. The PRTF is not deﬁned in the regions of missing
diﬀraction data, which are seen as the missing wedges
in the surface. It is seen that the resolution is approxi-
mately the same in all directions of u where intensities
were measured.

−

When applied to the average image γM , the modulus
constraint error E2
M of Eqn. (17) is equal to the intensity-
2. That is, it
weighted integral over u of
PRTF(u)
1
|
|
gives a single measure of how well diﬀraction intensities
of the average image agree with the measurement. This
is generally higher than the metric E2
M applied to the
iterate gn, which gives an estimate for how well the algo-
rithm ﬁts the intensity data. The value of E2
M applied to
the average 3D image is 0.368, and 0.059 for the average
2D projection image that was corrected for vortex phase
errors (0.312 without vortex correction). We expect that
a similar correction of low-order phase modes in the 3D
image would lead to a similar improvement in the error
metric, and the relatively high value of E2
M for the av-
erage 3D image is due to the overall ﬁltering due to the
variation of these low-order phase modes.

We can also compute the agreement of the average
image γM to the real-space support constraint E2
S of
Eqn. (16). We ﬁnd a value of 0.228 when applied to the
average 3D image and 0.167 for the average 2D complex-
valued projection image reconstructed from the central
section. Note however that in the 3D image the sup-
port S accounts for 0.10% of the image voxels whereas
S covers 4.1% of the pixels in the projection 2D images,
and so the average error per pixel outside the support is
much less for the 3D that the 2D reconstruction. We ﬁnd
with the addition of the real-space positivity constraint
that E2
S of the average 2D projection image decreases
from 0.167 to 0.072. However, in this case the modulus

Fig. 10.
(Color online) (a) The phase retrieval trans-
fer function, averaged over shells of constant u, for the
real-positive 3D projection image (solid line) and aver-
aged over circles of constant u for the complex 2D im-
age (dashed lines). The dashed line with lower values is
for the 2D projection image without correction of vor-
tex phase modes. (b) An iso-surface rendering of the 3D
PRTF, at a threshold level of 0.5. The axis tick-marks
indicate 0.05 nm−

1.

P

γ(0)
M (k)

2. This phase is that which max-
γM (k)
k |
|
γ(0)
, which is simply
M (k) γM (k) exp(iφ1)
imizes
P
}
k γ(0)
M (k)∗ γM (k).
the phase of the complex value

−
k ℜ{

P

In the case of 2D images we also improve the average by
separating out the vortex modes mentioned above. This
was achieved simply by correlating each phase retrieval
solution with the previous solutions and separating the
solutions into three classes (which were found to diﬀer
by left and right-handed phase vortices) based on the
value of the correlation. We found that the class with the
most frequent members (60% of trials) gave rise to the
best image, wherease the other two classes were equally
frequent (20% each) and gave rise to images for which the
balls were larger, had bright edges and reduced intensity
at their centers. Based on the appearance of the balls
we assumed that the most frequent class was that which

constraint error E2

M increases from 0.059 to 0.172.

a

b

c

B. Real-space resolution

The measures of resolution from the SNR and PRTF re-
veal the eﬀects of noise, consistency of the diﬀraction
data, and how well the image obeys the imposed con-
straints. These measures are contributors to the over-
all image resolution. A direct measure of a lower limit
of resolution can be obtained by examining the images
of isolated and closely spaced gold balls. Line-outs of
the isolated ball located on the lower left arm of Fig. 5
(a) are shown in Fig. 11, for all three orthogonal direc-
tions. The ball image has full widths at half maximum
(FWHM) of 30, 35, and 70 nm in the x, y, and z direc-
tions, respectively. Images of other isolated balls in the
object are very similar to that shown in Fig. 11. Assum-
ing the balls are 50 nm in diameter, we obtain a good
ﬁt to the images by modeling a coherent imaging system
with an optical transfer function (OTF) that is unity
1 (centered at the
within a cube of half-width 0.05 nm−
zero frequency) and which is zero within a sector of 60◦
as rotated about the y axis, and centered about the z
axis. Line-outs of the modeled coherent images, com-
puted by convolving an isolated 50 nm sphere with the
Fourier transform of the OTF (that is, the point spread
function, or PSF), are shown as dashed lines in Fig. 11.
The FWHM of the modeled coherent image are 36 nm,
40 nm and 64 nm in the x, y, and z directions, respec-
tively, in good agreement with the reconstructed image.
We do not expect the model to be an exact ﬁt to the
data, since the actual PSF is more complicated and de-
pends on the details of the phase retrieval, which is better
characterized by the PRTF in Fig. 10. However, the ﬁts
are reasonable and the widths of the modeled PSF are in
good agreement with the measures of resolution obtained
from analysis of the diﬀraction intensities and recovered
phases. The modeled point spread function (PSF), given
by the Fourier transform of the OTF, has a half-width of
10 nm
40 nm. Here the half width is deﬁned
as the distance from the central maximum of the PSF to
the ﬁrst zero. Since the imaging process is coherent, the
image width depends on the phase of the PSF, which has
a diﬀerent distribution for the x and y directions. This
explains the variation of image widths in the x and y di-
rections, and why the image FWHM in these directions
are in fact smaller than the ball diameter. As expected,
the resolution in the z direction is much worse than in
the x and y directions, due to the missing sector of data
that arises from recording diﬀraction over a limited range
of angles.

10 nm

×

×

6. Summary and Conclusions

We have presented 3D images reconstructed ab initio
from coherent X-ray diﬀraction, which exhibit high reso-
lution in all three dimensions. These images are the high-
est resolution 3D images of non-periodic objects where
the resolution is comparable in all three dimensions. The

19

t
r
a
p

 
l

a
e
R

300

200

100

0

-100

 

 

 

 

 

 

 

 

-400 -300 -200

x (nm)

 
-400 -300 -200 -100
y (nm)

 
300

400

500

600

z (nm)

Fig. 11. Line-outs of the real part of the reconstructed
complex amplitude 3D image, for three orthogonal direc-
tions (a) x, (b) y, and (c) z, through the isolated single
ball at the pyramid apex. Coordinates are relative to the
center of the 3D image array. Dashed lines show lineouts
from a simulated 3D coherent image with a cube OTF
with a 60◦ missing sector.

work presented here marks an important advance in that
we have fully demonstrated the experimental methods
to collect 3D coherent X-ray diﬀraction and the compu-
tational tools to manage the data, and reconstruct 3D
images of more than 109 voxels.

The coherent X-ray diﬀraction recorded from our 3D
test object comprised of 140 views, at 1◦ intervals, and
1,
extend to a maximum spatial frequency of 0.068 nm−
or a smallest reconstructible half-period of 7.4 nm. Al-
though we cannot exactly quantify the resolution of the
image, which would require knowing the object’s 3D
structure, we have determined the consistency of the re-
trieved phases which gives us an estimate of an upper
bound of the MTF of the imaging process. Our analy-
sis shows we can consistently retrieve phases out to the
maximum spatial frequency recorded. This consistency
measure does not tell us anything about systematic er-
rors, such as interpolation of the data, errors in assigning
spatial frequency u to the intensities (imperfect knowl-
edge of the beam center), and missing data due to the
beam stop or limited range of object orientations. How-
ever, we easily resolve 50 nm spheres that are touching
each other, and from such image line-outs, and compar-
isons of reconstructed X-ray images with the SEM image,
we have conﬁdence that our achieved image resolution is
close to our upper estimate.

We have found that our Shrinkwrap algorithm6, which
determines the object support ab initio, is robust and
works well even with missing Fourier-space data due to
limited object orientations or the beamstop. The phase
retrieval process can be essentially characterized by a 3D
MTF (the Phase Retrieval Transfer Function, or PRTF)
which is inﬂuenced by the noise of the measured diﬀrac-
tion intensities. While the algorithm lets the amplitudes
at the locations of missing data to also be recovered,
these values are not consistently reconstructed and are
averaged to zero, leaving worse resolution in the depth
(z) direction. We expect that with a dataset collected
over the full range of sample orientation angles we would

achieve equal resolution in all three dimensions. As it is,
we obtained an estimate of 10 nm in x and y and 50 nm
in z.

We have shown that high-NA X-ray coherent imaging
of thick objects can only properly be carried out in the
context of three dimensions. Here we deﬁne high-NA
imaging of thick objects to be imaging under conditions
that lead to a depth of focus less than the depth of the
object, in any of its orientations. Since the imaging is
coherent, a 2D image of a thick object in any one view
will exhibit defocus artifacts which do not diminish in
overall power with the degree of defocus and which lead
to diﬃculties in the interpretation of the image. In addi-
tion, these artifacts cause the image of a real positive
object, for example, to be complex, hence hampering
quantitative evaluation of the image. Two-dimensional
images free of defocus artifacts can be quickly generated
from central sections extracted from the diﬀraction data.
Three-dimensional images are synthesized from the en-
tire 3D diﬀraction dataset. The tools are now in place to
perform full 3D reconstructions of thick samples. Cur-
109
rently we have reconstructed arrays with almost 2
×
3√2 relative
elements. If the minimum oversampling of
to Bragg sampling is used in each dimension, then this
would correspond to objects of width 9.5 µm at a pixel
spacing of 10 nm, or a resolution of 7 nm along the diago-
nal. When single-particle XFEL imaging at atomic reso-
lution becomes feasible, then these demonstrated compu-
tational capabilities could be used to reconstruct objects
of 480 nm width at 0.7 nm resolution, for example. This
would correspond to a large virus, or a large protein com-
plex such as the ribosome.

Acknowledgments

We wish to thank Ray Mariella (LLNL) for the idea of us-
ing a silicon nitride pyramid as a test object, and Jackie
Crawford and Dino Ciarlo (LLNL) for determining its
manufacturing process. We thank Janos Kirz (LBNL
and Stony Brook) for technical advice and extensive dis-
cussions about our experiments. We acknowledge stim-
ulating discussions with Abraham Sz¨oke (LLNL), G¨osta
Huldt (U. Uppsala), and Eugene Ingerman (CBST). We
gratefully acknowledge Richard Crandall and the Ad-
vanced Computations Group (Apple Computer, Inc.) for
the development of the dist fft software. This work
was performed under the auspices of the U.S. Depart-
ment of Energy by University of California, Lawrence
Livermore National Laboratory under Contract W-7405-
Eng-48. This work has been supported by funding from
the National Science Foundation. The Center for Bio-
photonics, an NSF Science and Technology Center, is
managed by the University of California, Davis, under
Cooperative Agreement No. PHY 0120999.

References

1. D. Sayre and H. N. Chapman, “X-ray microscopy,”

Acta Cryst. A 51, 237–252 (1995).

20

2. D. Sayre, H. N. Chapman, and J. Miao, “On the
Extendibility of X-ray Crystallography to Noncrys-
tals,” Acta Cryst. A 54, 232–239 (1998).

3. J. Miao, P. Charalambous, J. Kirz, and D. Sayre,
“Extending the methodology of X-ray crystallog-
raphy to allow imaging of micrometre-sized non-
crystalline specimens,” Nature 400, 342–344 (1999).
I. K. Robinson, I. A. Vartanyants, G. J. Williams,
M. A. Pfeifer, and J. A. Pitney, “Reconstruction
of the Shapes of Gold Nanocrystals Using Coher-
ent X-Ray Diﬀraction,” Phys. Rev. Lett. 87, 195505
(2001).

4.

5. G. J. Williams, M. A. Pfeifer, I. A. Vartanyants,
and I. K. Robinson, “Three-Dimensional Imaging
of Microstructure in Au Nanocrystals,” Phys. Rev.
Lett. 90, 175501 (2003).

6. S. Marchesini, H. He, H. N. Chapman, S. P. Hau-
Riege, A. Noy, M. R. Howells, U. Weierstall, and
J. C. H. Spence, “X-ray image reconstruction from a
diﬀraction pattern alone,” Phys. Rev. B 68, 140101
(2003). arXiv:physics/0306174.

7. S. Marchesini, H. N. Chapman, S. P. Hau-Riege,
R. A. London, A. Szoke, H. He, M. R. How-
ells, H. Padmore, R. Rosen, J. C. H. Spence, and
U. Weierstall, “Coherent X-ray diﬀractive imaging:
applications and limitations,” Opt. Express 11,
2344–2353 (2003), arXiv:physics/0308064.

8. H. He, S. Marchesini, M. Howells, U. Weierstall,
H. Chapman, S. Hau-Riege, A. Noy, and J. C. H.
Spence, “Inversion of x-ray diﬀuse scattering to im-
ages using prepared objects,” Phys. Rev. B 67,
174114 (2003).

9. M. R. Howells, T. Beetz, H. N. Chapman, C. Cui,
J. M. Holton, C. J. Jacobsen, J. Kirz, E. Lima,
S. Marchesini, H. Miao, D. Sayre, D. A. Shapiro,
and J. C. H. Spence, “An assessment of the reso-
lution limitation due to radiation-damage in x-ray
diﬀraction microscopy,” J. Electron Spectrosc. Rel.
Phenom. (2005), arXiv:physics/0502059.

10. R. Neutze, R. Wouts, D. van der Spoel, E. Weckert,
and J. Hajdu, “Potential for biomolecular imaging
with femtosecond X-ray pulses,” Nature 406, 753–
757 (2000).

11. J. Miao, K. O. Hodgson,

and D. Sayre,
“An approach to three-dimensional structures of
biomolecules by using single-molecule diﬀraction
images,” Proc. Nat. Acad. Sci. 98, 6641–6645
(2001).

12. J. C. H. Spence and R. B. Doak, “Single Molecule

Diﬀraction,” Phys. Rev. Lett. 92, 198102 (2004).

13. W. S. Haddad, I. McNulty, J. Trebes, E. Anderson,
R. Levesque, and L. Yang, “Ultrahigh-resolution x-
ray tomography,” Science 266, 1213–1215 (1994).

14. D. Weiss, G. Schneider, B. Niemann, P. Guttmann,
D. Rudolph, and G. Schmahl, “Computed tomog-
raphy of cryogenic biological specimens based on x-
ray microscopic images,” Ultramicros. 84, 185–197
(2000).

21

15. C. A. Larabell and M. A. Le Gros, “X-ray Tomog-
raphy Generates 3-D Reconstructions of the Yeast,
Saccharomyces cerevisiae, at 60-nm Resolution,”
Mol. Biol. Cell 15, 957–962 (2004).

16. N. Streibl, “Three-dimensional imaging by a micro-
scope,” J. Opt. Soc. Am. A 2, 121–127 (1985).
17. T. Beetz, M. Howells, C. Jacobsen, C. Kao, J. Kirz,
E. Lima, T. Mentes, H. Miao, C. Sanchez-Hanke,
D. Sayre, and D. Shapiro, “Apparatus for X-ray
diﬀraction microscopy and tomography of cryo spec-
imens,” Nucl. Instrum. Meth. A 545, 459–468
(2005).

18. R. Crandall, E. Jones,

J. Klivington,

D. Kramer,
G5
putation Group,
http://images.apple.com/acg/pdf/20040827 GigaFFT.pdf.

and
on Apple
rep., Advanced Com-
(2004).

“Gigaelement FFTs

Apple Computer

clusters,” Tech.

19. M. Born and E. Wolf, Principles of Optics, 7th ed.

(Cambridge University Press, 2002).

20. R. W. James, The Optical Principles of the Diﬀrac-

tion of X-Rays (Bell, London, 1962).

21. E. Wolf, “Three-dimensional structure determina-
tion of semi-transparent objects from holographic
data,” Opt. Commun. 1, 153–156 (1969).

22. J. Kirz, C. Jacobsen, and M. Howells, “Soft X-ray
microscopes and their biological applications,” Q.
Rev. Biophys 28, 33–130 (1995).

23. F. Natterer, “An error bound for the Born approx-
imation,” Inverse Problems 20, 447–452 (2004).
24. R. N. Bracewell, The Fourier Transform and its Ap-
plications, Second edition (McGraw-Hill, 1986).
25. J. Miao, T. Ishikawa, E. H. Anderson, and K. O.
Hodgson, “Phase retrieval of diﬀraction patterns
from noncrystalline samples using the oversampling
method,” Phys. Rev. B 67, 174104 (2003).

26. J. W. Goodman, Introduction to Fourier Optics,

Second Edition (McGraw-Hill, 1996).

27. J. C. H. Spence, U. Weierstall, and M. Howells, “Co-
herence and sampling requirements for diﬀractive
imaging,” Ultramicros. 101, 149–152 (2004).
28. R. Crowther, D. DeRosier, and A. Klug, “The re-
construction of a three-dimensional structure from
its projections and its applications to electron mi-
croscopy,” Proc. Roy. Soc. Lond. 317, 319–340
(1970).

29. E. Salerno, “Superresolution Capabilities of the
Gerchberg Method in the Band-pass Case: An
Eigenvalue Analysis,” Int. J. Imaging Syst. Technol.
90, 181–188 (1998).

30. Y. Nishino, J. Miao, and T. Ishikawa, “Image re-
construction of nanostructured nonperiodic objects
only from oversampled hard x-ray diﬀraction inten-
sities,” Phys. Rev. B 68, 220101 (2003).

31. T. Sato, S. J. Norton, M. Linzer, O. Ikeda, and
M. Hirama, “Tomographic image reconstruction
from limited projections using iterative revisions in
image and transform spaces,” Appl. Opt. 20, 395–
399 (1981).

32. M. Bertero and E. R. Pike, “Resolution in
diﬀraction-limited imaging, a singular value anal-
ysis I. The case of coherent illumination,” Optica
Acta 29(6), 727–746 (1982).

33. D. Shapiro, P. Thibault, T. Beetz, V. Elser, M. How-
ells, C. Jacobsen, J. Kirz, E. Lima, H. Miao,
A. M. Neiman, and D. Sayre, “Biological Imaing
by Soft X-Ray Diﬀraction Microscopy,” Proc. Nat.
Acad. Sci. (in press, 2005).

34. A. Szoke, H. Szoke, and J. R. Somoza, “Holographic
Methods in X-ray Crystallography. V. Multiple Iso-
morphous Replacement, Multiple Anomalous Dis-
persion and Non-crystallographic Symmetry,” Acta
Cryst. A 53, 291–313 (1997).

35. S. P. Hau-Riege, H. Szoke, H. N. Chapman,
A. Szoke, S. Marchesini, A. Noy, H. He, M. How-
ells, U. Weierstall, and J. C. H. Spence, “SPEDEN:
reconstructing single particles from their diﬀrac-
tion patterns,” Acta Cryst. A 60, 294–305 (2004),
arXiv:physics/0403091.

36. D. Potts, G. Steidl, and M. Tasche, “Fast Fourier
transforms for nonequispaced data: a tutorial,” in
Modern Sampling Theory: Mathematics and Ap-
plications, J. J. Benedetto and P. Ferreira, eds.,
chap. 12, pp. 249–274 (Springer, 2001).

37. H. Choi and D. C. Munson, Jr., “Direct-Fourier re-
construction in tomography and synthetic aperture
radar,” Int. J. of Imaging Systems and Technology
9, 1–13 (1998).

38. F. Natterer, The mathematics of computerized to-

mography (SIAM, Philadelphia, 2001).

39. A. J. Devaney, “A ﬁltered backpropagation algo-
rithm for diﬀraction tomography,” Ultrasonic Imag.
4, 336–350 (1982).

40. S. Pan and A. Kak, “A computational study of re-
construction algorithms for diﬀraction tomography:
Interpolation
ﬁltered-backpropagation,”
IEEE Trans. Sig. Process. 31, 1262–1275 (1983).

versus

41. H. He, S. Marchesini, M. Howells, U. Weierstall,
G. Hembree, and J. C. H. Spence, “Experimen-
tal lensless soft-X-ray imaging using iterative algo-
rithms: phasing diﬀuse scattering,” Acta Cryst. A
59, 143–152 (2003).

42. J. R. Fienup, “Reconstruction of a complex-valued
object from the modulus of its Fourier transform
using a support constraint,” J. Opt. Soc. Am. A 4
118–123 (1987).

43. J. R. Fienup, private communication (2005).
44. J. R. Fienup, “Phase retrieval algorithms: a com-

parison,” Appl. Opt. 21, 2758–2769 (1982).

45. V. Elser, “Phase retrieval by iterated projections,”

J. Opt. Soc. Am. A 20, 40–55 (2003).

46. D. R. Luke, “Relaxed Averaged Alternating Reﬂec-
tions for Diﬀraction Imaging,” Inverse Problems 21,
37–50 (2005), arXiv:math.OC/0405208.

47. M. Frigo and S. G. Johnson, “The Design and Im-
plementation of FFTW3,” Proceedings of the IEEE
93, 216–231 (2005). Special issue on ”Program Gen-

eration, Optimization, and Platform Adaptation”.
48. “The Message Passing Interface (MPI) standard,”

http://www-unix.mcs.anl.gov/mpi/.

49. A. J. Ladd, J. H. Kinney, D. L. Haupt, and S. A.
Goldstein, “Finite-element modeling of trabecular
bone: comparison with mechanical testing and de-
termination of tissue modulus,” J. Orthop. Res. 16,
622–628 (1998).

50. J. M. Cowley, Diﬀraction Physics (North-Holland,

51. J. C. H. Spence, U. Weierstall, and M. Howells,
“Phase recovery and lensless imaging by iterative
methods in optical, X-ray and electron diﬀraction,”
Phil. Trans. R. Soc. Lond. A 360, 875–895 (2002).

52. G. Huldt, private communication (2005).
53. J. Frank, Three-dimensional electron microscopy of
macromolecular assemblies (Academic Press, 1996).
54. J. R. Fienup, “Invariant error metrics for image re-
construction,” Appl. Opt. 36, 8352–8357 (1997).

1981).

22


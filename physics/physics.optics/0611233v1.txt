6
0
0
2
 
v
o
N
 
4
2
 
 
]
s
c
i
t
p
o
.
s
c
i
s
y
h
p
[
 
 
1
v
3
3
2
1
1
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

UCRL-JRNL-226292

Phase retrieval and saddle-point optimization

S. Marchesini1, 2,
1Lawrence Livermore National Laboratory, 7000 East Ave., Livermore, CA 94550-9234, USA
2 Center for Biophotonics Science and Technology, University of California,
Davis, 2700 Stockton Blvd., Ste 1400, Sacramento, CA 95817, USA
(Dated: February 21, 2014)

∗

Iterative algorithms with feedback are amongst the most powerful and versatile optimization
methods for phase retrieval. Among these, the hybrid input-output algorithm has demonstrated
remarkable success in performing giga-element nonlinear optimization, escaping local minima and
producing images at resolutions beyond the capabilities of lens-based optical methods. Here the
input-output iteration is improved by a lower dimensional subspace saddle-point optimization.

Phase retrieval is one of the toughest challenges in op-
timization, requiring the solution of a large-scale non-
linear, non-convex and non-smooth constrained problem.
Eﬃcient algorithms are being used in astronomical imag-
ing, electron microscopy, lensless x-ray imaging and x-ray
crystallography, substituting lenses and other optical el-
ements in the image-forming process.

Novel experimental methods are being developed
thanks to advances in optimization techniques (see e.g.
[1] for a review), primarily the introduction of a control-
feedback method proposed by Fienup (Hybrid Input
Output-HIO [2, 3]). The important theoretical insight
that these iterations may be viewed as projections in
Hilbert space [5, 6] has allowed theoreticians to analyze
and improve on the basic HIO algorithm [7, 8, 9, 10].

The algorithm proposed is a reformulation of the HIO
algorithm from an unusual perspective. Such algorithm
seeks the saddle-point of the diﬀerence between data
and constraint, minimizing this diﬀerence when moving
within the constraints, and maximizing it when moving
outside the constraints [11]. Here, ideas from other op-
timization techniques are utilized to improve upon the
original HIO algorithm.

I. PHASE RETRIEVAL PROBLEM

When we record the diﬀraction pattern intensity of
light scattered by an object the phase information is
missing. Apart from normalization factors, an object
with density ρ(r), r being the coordinates in the object
(or real ) space, generates a diﬀraction pattern intensity
equal to the modulus square of the Fourier Transform
(FT) ˜ρ(k):

I(k) =

2
˜ρ(k)
|
|

(1)

Where k represent the coordinate in the Fourier (or Re-
ciprocal) space. In absence of constraints, any phase ϕ(k)
can be applied to form our solution ˜ρ = √Ieiϕ. Phase

∗Correspondence and requests for materials should be addressed to
S. Marchesini: smarchesini@llnl.gov

retrieval consists in solving Eq. (1) from the measured in-
tensity values I(k) and some other prior knowledge (con-
straints).

Diﬀraction microscopy solves the phase problem using
the knowledge that the object being imaged is isolated,
it is assumed to be 0 outside a region called support S:

ρ(r) = 0, if r /
∈

S

A projection onto this set (Ps) involves setting to 0 the
components outside the support, while leaving the rest
of the values unchanged (Fig. 1(a)):

Psρ(r) = 

0

ρ(r)

if r

S

∈

otherwise

and its complementary projector Ps = I

Ps.



(2)

(3)

(
r r

3

)

S

r

(
r r

2

)

(
r r

1

)

sP ɶ
r

S

(a)

rɶ

2

rɶ
1

(
kɶ
r

)

mP

(
r kɶ

)

rRe

(
kɶ

)

−

(
Im kɶ
r

)

(
j k

)

m

m ± δ

(b)

FIG. 1: Examples of sets and projectors: (a) Support: The
axes represent the values on 3 pixels of an image ρ known to
be 0 outside the support. The vertical axis ρ(r3) represents a
pixel outside the support (r3 ∈ S), while the horizontal plane
represents pixels inside the support S. The projection on this
set is performed simply by setting to 0 all the pixels outside
the support. (b) Modulus: A pixel (in Fourier space) with a
given complex value is projected on the closest point on the
circle deﬁned by the radius m. If there is some uncertainty
in the value of the radius m ± δ, the circle becomes a band.
The circle is a non-convex set, since the linear combination
between two points on the same set, ρ1 and ρ2 does not lie
on the set. Also represented in the ﬁgure is the projection on
the real axis (Reality projection).

The projection to the nearest solution of Eq. (1) in
reciprocal space is obtained by setting the modulus to
I(k), and leaving the phase
the measured one m(k) =
unchanged (Fig. 1(b)):

˜Pm ˜ρ(k) = ˜Pm|

p
eiϕ(k) = √I(k)eiϕ(k) ,
˜ρ(k)
|
Such projector is a “diagonal” operator in Fourier space,
acting element-by-element on each amplitude. When ap-
plied to real-space densities ρ(r), it becomes non-local,
1
mixing every element with a forward
−
Fourier transform:

and inverse

(4)

F

F

Pm =

−

.

1 ˜PmF
of a vector ρ is deﬁned as:

(5)

The Euclidean length

ρ

2 = ρ†
||

||

·

ρ =

2 =
ρ(r)
|

2.
˜ρ(k)
|

(6)

k |
X

If noise is present, it should be used to weight the sum.
The distance from the current point to the corresponding
set

is the basis for our error metric:

P ρ

ρ

||

−

||

F
ρ

||

||

r |
X

εs(ρ) =
εm(ρ) =

Psρ
||
Pmρ
||

,

ρ
||
ρ
||

−
−

,

(7)

or their normalized version εx(ρ) = εx(ρ)
Pxρ
||

||

.

The gradients of the error metrics can be expressed in

terms of projectors: [3, 12]:

ε2
m(ρ) =
ε2
s(ρ) =

2[Pm −
2[Ps −

I]ρ
I]ρ ,

∇
∇
ε2
s,m bring the corresponding error metrics
Steps of
to 0. The solution, hopefully unique, is obtained when
both error metrics are 0.

1
2 ∇

−
−

−

(8)
(9)

II. MINIMIZATION IN FEASIBLE SPACE

One popular algorithm (Gerchberg and Saxton [14])

minimizes the error metric εm(ρ)

min
ρ

ε2
m(ρ)

subject to Psρ = 0.

2

∇s = Ps∇

is the component of the gradient in the
where
ε2
support. Notice that a step of
m(ρ) brings the error
ε2
m(ρ) to 0. The projection of this step is shorter than the
step itself, and gives a lower bound for the optimal step
length. This algorithm is usually written as a projection
algorithm:

1
2 ∇

−

ρ(n+1) = ρ(n) + ∆ρ(n) ,

= PsPmρ(n) .

(13)

By projecting back and forth between two sets, it con-
verges to the local minimum.

The simplest acceleration strategy, the steepest de-
scent method, performs a line search of the local min-
imum in the steepest descent direction:

min
δ

ε2
m (ρ + δ∆ρ) .

(14)

∇s = Ps∇ρ is the gradient with respect to ρs.
where
At a minimum any further movement in the direction of
the current step increases the error metric; the gradient
direction must be perpendicular to the current step. In
other words the current step and the next step become
orthogonal:

∂

∂δ ε2

Pm] (ρ + δ∆ρs)

−
Pm] (ρ + δ∆ρs)

m(ρ + δ∆ρ) =
0 =

Ps[I
∆ρ
h
|
[I
∆ρs|
h
y

ir ,
ir , (15)
. The line search algorithm can
where
use ǫ2
m, and/or its derivative in Eq. (15). This optimiza-
tion should be performed in reciprocal space, where the
modulus projector is fast to compute (Eq. (4)), while the
support projection requires two Fourier transforms:

ir =

x
h

y
|

x†

−

ℜ

(cid:1)

(cid:0)

·

˜Ps =

PsF

F

1 ,

−

(16)

but it needs to be computed once to calculate ∆ρs.

The steepest descent method is known to be ineﬃ-
cient in the presence of long narrow valleys, where im-
posing that successive steps be perpendicular causes the
algorithm to zig-zag down the valley. This problem
is solved by the non-linear conjugate gradient method
[15, 16, 17, 18, 19, 20, 21].

(10)

III. SADDLE-POINT OPTIMIZATION

We apply the constraint Psρ = 0 and move only in the
feasible space ρ = ρs = Psρ and rewrite as:

min
ρs

ε2
m(ρs)

(11)

−

The steepest descent direction is projected onto S:

The following algorithm is a reformulation of the HIO
algorithm from a gradient/constraint perspective. We
seek the saddle point of the error-metric diﬀerence
(ρ) =
ε2
m(ρ)

ε2
s(ρ) [11]:

L

min
ρs

max

ρs L

(ρs + ρs) .

(17)

ρ(n+1) = ρ(n) + ∆ρ(n),
∆ρ(n) =
1

ρ(n)

,

m

2 ∇s ε2
Ps[I
−

−

−

=

(cid:16)
(cid:17)
Pm]ρ,

using equations (8) and (9) we obtain the gradient:

(12)

(ρ) = 2[Ps −
Since we seek the saddle point, the step direction has
) and ascent
to go in the descent direction for ρs (

Pm]ρ .

(18)

∇L

Ps∇

−

(19)

(20)

) for ρs. For reasons discussed below,

∆ρ =

direction (+Ps∇
we reduce the Ps component by a factor 0.5
Ps + ¯βPs}
{−
Ps]
Ps[Pm −
I]
Ps[Pm −
The new iteration point:

(ρ) ,
¯βPs[Pm −
−
¯βPsPm}
ρ .

1
2 ∇L

=

=

−

{

{

ρ ,

Ps]
}

¯β

≤

≤

1:

ρ(n+1) = ρ(n) + ∆ρ(n)

= [PsPm − Ps[I

¯βPm]]ρ(n)

−

can be expressed in a more familiar form of the HIO
algorithm [2, 3]:

ρ(n+1)(x) =

Pmρ(n)(x)
(I

¯βPm)ρ(n)(x)

S,

if x
∈
otherwise.

(21)

(

−

Optimization of the step length is obtained by increas-
ing a multiplication factor δ until the current and next
search directions become perpendicular to one another:

Ps + ¯βPs]

(ρ + δ∆ρ)

∇L

∆ρ

[
−
|

(cid:10)

(22)

r = 0 .
(cid:11)

A more robust strategy involves replacing the one di-
mensional search with a two dimensional optimization of
the saddle point:

min
α

ψ(α, β) ,

max
β
(ρ + α∆ρs + β∆ρs) .

(23)

ψ(α, β) =

L

where both components (Ps, Ps) of successive steps are
perpendicular to one another:

∂ψ
∂α =
∂ψ
∂β =

∆˜ρs|∇L (˜ρ + α∆˜ρs + β∆˜ρs)
h
∆˜ρs|∇L (˜ρ + α∆˜ρs + β∆˜ρs)
h

ir = 0,
ir = 0.

(24)

This two dimensional minmax problem needs to be fast
to provide real acceleration and will be discussed in the
following section.

The minmax problem can be expressed in a dual form:

minρs ε2

[I
m(ρ) = min ˜ρs ||

−

Pm]ρ

2
||






minρs ε2

s(ρ)

ε2
m(ρ) = minρs 2

−

Pmρ
h

ρ
|

ir + c

The upper optimization is similar to the problem treated
in Section II, converging to a local minimum with a
simple projected gradient method. The lower function,
however, can be discontinuous in the presence of zeros
(˜ρs = 0) in Fourier space:

(25)

(26)

˜Pm ˜ρ
˜ρ
i
|
h

=

√I

˜ρs + ˜ρs|
|

X

3

which is a non-smooth v-shaped function of ˜ρs for ˜ρs =
0, √I > 0, and simple gradient methods oscillate around
minima. The projected gradient step can be overesti-
mated and requires the relaxation parameter ¯β. Ze-
ros in Fourier space are candidates (necessary but not
suﬃcient condition) for the location of phase vortices,
phase discontinuities, which are known to cause stagna-
tion [23]. Analytical [25], statistical [23, 24], and deter-
ministic [26, 27] methods have been proposed to over-
come such singularities.

IV. TWO DIMENSIONAL SUBPROBLEM

The local saddle point (Eq. 23) requires two conditions
to be met. The ﬁrst order condition is that the solution
is a stationary point, where the gradient of ψ is 0 (Eq.
24):

τ ψ(τ ) =

∇

∆ρ

∇ρL
|
β ) , ∆ρ =

τ = ( α
(cid:10)

(ρ + τ T ∆ρ

(27)

r = 0,
, ∇ρ =
(cid:11)

s
∇
s
∇

.

(cid:16)

(cid:17)

∆ρs
∆ρs

(cid:16)

(cid:17)

This ensures that that both components (Ps, Ps) of suc-
cessive steps ∆ρ(n), ∆ρ(n+1) are perpendicular. Notice
that 1
.

τ ψ(0) =

2

2 ∇

∆ρs
∆ρs

||
2
||

−||
+
||

The second order conditions (min-max) require the
of ψ (the Jacobian of 27) to be symmetric

Hessian
and indeﬁnite (neither positive nor negative deﬁnite):

H

(cid:16)

(cid:17)

Hτ =

∂α∂α ∂α∂β
∂β ∂α ∂β ∂β

ψ,

Hα,α ≥
Hβ,β ≤

(

0,
0.

(cid:16)

(cid:17)
This Hessian is computed analytically in appendix, it is
small (2
2), and can be used to compute the Newton
step:

×

(28)

∆τ =

1

−

∇τ ψ .

−H
However, the Hessian precise value is not necessary and
requires an eﬀort that could be avoided by other meth-
ods.

The minimal residual method optimizes the norm of

(29)

min
τ

Φ(τ ), Φ = 1

τ ψ(τ )

2 ||∇

2 ,
||

(30)

transforming the saddle point problem in a minimization
problem, and providing the metric Φ to monitor progress.
However by minimizing the norm of the gradients, we
can move to other stationary points and the algorithm
becomes less robust.

The HIO algorithm uses a good guess for the Hessian,
which is often all it is needed to achieve fast convergence:

∆τ =

−
= 2

1

−

ˆ
H
1
0

−

∇
0
1/ ¯β

ˆ
H

τ ψ(τ ) ,

(cid:16)
τ (n+1) = τ (n) + ∆τ (n).

(cid:17)

(∆ρ∆ρT ) ,

(31)

(32)

(33)

A. Discussion

the gradient:

Starting from τ (0) = 0, the ﬁrst iteration gives the stan-
dard HIO step τ (1) =
satisﬁes con-
dition (Eq. (28)), ensuring that ∆τ is less then 90◦ from
the direction of the saddle. We can perform a line search
1:
using the preconditioner
−

. The Hessian ˆ
H

1
¯β

(cid:0)

(cid:1)

H

∆τ

1

−

ˆ
H
|

∇

D

r

E

τ ψ(τ + δ∆τ )

= 0 .

(34)

However the Hessian of ˆ
1ψ is antisymmetric, the al-
−
H
gorithm is unstable and could spiral away from the solu-
tion. The biconjugate gradient method applies to sym-
metric indeﬁnite Hessians and monitors progress of the
algorithm. Conjugate directions Λτ replace the steepest
descent direction in the line search:

Λτ (n+1) = ∆τ (n+1) + γ(n)Λτ (n)
∆τ (n+1)

γ(n) = h

ˆ
(−1)(∇ψ(τ (n+1))−
H
|
∆τ (n)
h

−1∆∇ψ(τ (n))
i

ˆ
H

|

∇ψ(τ (n)))

i

(35)

A better option is to use a quasi-Newton method, by
updating the Hessian based on the new gradient values.
The Symmetric Rank 1 method can be applied to indef-
inite problems [22]:

∇τ ψ(τ )
−
2

y = ∇τ ψ(τ + ∆τ )
1y
−
1y)T
(cid:12)
(cid:12)
·
(cid:12)
(cid:12)
1

1 =

∆τ

1

− H
− H−
H

1 + ∆

(∆τ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−
→ H

−

y

∆

−

H

−

H

(36)

Second order conditions (Eq. 28) can be imposed to
the Hessian, by ﬂipping the sign or setting to 0 the values
that violate them. Φ can be used to monitor progress, as
long as we are in the neighborhood of the solution and
the Hessian satisﬁes second order conditions (Eq. 28).
It was found that the Hessian and step size parameters
where fairly constant for each 2D optimization, therefore
the ﬁrst guess for τ and
was obtained from the average
of the previous 5 of such optimizations.

H

In summary, an eﬃcient algorithm is obtained by
a combination of HIO/quasi-Newton/Newton methods
r:
with a trust region

| ≤
1. calculate step ∆ρ =

∆τ
|

region radius r = rmax.

1
2

−

s
L
∇
s
−∇
L

(cid:16)

(cid:17)

, and set trust

2. if the iteration number is
, τ = (1, ¯β).
= ˆ
H

guess:

H

≤

5, use HIO as ﬁrst

3. otherwise average 5 previous optimized step sizes
, and use the average as initial

τ , and Hessians
guess.

H

4. calculate gradient ∇ψ(τ ). If small, exit loop (go

to 10).

5. compute Newton step using approximate Hessian:
1∇ψ, enforce trust region

∆τ =

< r.

|
6. update Hessian with SR1 method (Eq. 36).

−H

∆τ
|

−

4

FIG. 2: pseudocolor- and contourmap of ψ(α, β) are depicted
in the background. Iterations to the 2D saddle point is shown
in blue.

7. if the Hessian error

∆τ
is too large,
calculate the true Hessian, perform a line search,
decrease trust region radius r.

− H

−

1y

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

8. force Hessian to satisfy second order conditions
(Eq. (28)), by changing the sign of the values that
violate conditions.

9. update τ

τ + ∆τ and go back to 4.

10. update ρ

ρ + τ T ∆ρ, and go back to 1.

→

→

The trust region is used to obtain a more robust algo-
rithm, is reset at each inner loop, it increases if things
are going well, decreases if the iteration is having trou-
ble, but it is kept between (rmin, rmax), typically (0.5, 3).
We can keep track of τ ,
τ computed, and restart the
algorithm once in a while from the root of the 2D linear
ﬁt of ∇ψ(τ ).

∇

We can easily extend this algorithm to two succes-
sive steepest descent-ascent directions, by performing a
higher dimensional saddle-point optimization:

min
α(n,n+1)

max
β(n,n+1) L

ρ + τ (n+1)∆ρ(n+1) + τ (n)∆ρ(n)
(cid:16)

(cid:17)

The 4D optimization is performed using the same
Newton/quasi-Newton trust-region optimization as in
the 2D case.

A. Performance and conclusions

The algorithms were tested with a simple simulation.
Fig. 3 was used to simulate a diﬀraction pattern, and

5

Biophotonics, a National Science Foundation Science
and Technology Center, is managed by the University
of California, Davis, under Cooperative Agreement No.
PHY0120999.

APPENDIX A: TWO DIMENSIONAL
GRADIENT AND HESSIAN

The function

in reciprocal space can be expressed

L

L

[I

(˜ρs + ˜ρs) =

˜Pm](ρs + ˜ρs)
−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜ρs + ˜ρs|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
|
X
and the two components of the gradients:

−
˜ρs|
|

2√I

=

−

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ √I
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

˜ρs

2

(A1)

∇†

L
†sL
†sL

∇

∇

=

˜Ps
˜Ps

†
∇
†
∇
(cid:16)
= 2 ˜Ps[I
=

L
L

−

−

2 ˜Ps ˜Pm(˜ρs + ˜ρs)

=

˜Ps[I

˜Pm]( ˜ρs+ ˜ρs)
˜Ps ˜Pm( ˜ρs+ ˜ρs)

−

−
(cid:17)
˜Pm](˜ρs + ˜ρs)

(cid:16)

(cid:17)

(A2)

and corresponding steps ∆ρs =

.
2 ∇sL
The function ψ(α, β) can be calculated in recipro-
cal space, provided that the components ˜ρs,s, ∆˜ρs,s are
known:

, ∆ρs = + 1

1
2 ∇sL

−

ψ(α, β) =

[I

˜ρ + α∆˜ρs + β∆˜ρs

2

(cid:12)
(cid:1)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2

(cid:12)
(cid:12)
(cid:12)

−
=

=

˜Pm]

−

2

(cid:12)
(cid:12)
(cid:0)
˜ρs + β∆˜ρs
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k (cid:12)
X
(cid:12)
(cid:12)
˜ρs + β∆˜ρs

(cid:12)
(cid:12)

(cid:12)
(cid:12)

2

−

I +

(cid:12)
(cid:12)
k
X
2√I

−

(cid:12)
˜ρ + α∆˜ρs + β∆˜ρs
(cid:12)

√I

−

(cid:12)
(cid:12)

˜ρs + α∆˜ρs|
(cid:12)
(cid:12)
|

2

˜ρ + α∆˜ρs + β∆˜ρs

(cid:12)
(cid:12)

(cid:12)
(cid:12)
= x
x
|
= ℜ(∆x†(x+α∆x))

,

|

x+α∆x

|

|
∆x
x+α∆x
| −
|
ℜ(∆x†(x+α∆x))
x+α∆x

2

|

x+α∆x
x+α∆x

|

|

,

,

∂
∂x |

x
|

∂
∂α |
∂
∂α

x + α∆x
|

x+α∆x
x+α∆x

|

|

=

Using common derivative rules:

∆x†(x + α∆x)

,

(A7)

ℜ(∆x†(x+α∆x))2
(cid:1)
x+α∆x

3

|

|

| −

|

·
2 = 2
x + α∆x
|
x + α∆x
|
2 =
x + α∆x
|

ℜ
∆x
(cid:0)
= |
x+α∆x
|
2 ,
∆x
|
|

∂
∂α |
∂2
∂α2 |
∂2
∂α2 |

2

|

∂2

|

x+α∆x+β∆y

|

∂α∂β

(A3)

(A4)

(A5)

(A6)

, (A8)

(A9)

(A10)

|

(∆x†∆y)
= ℜ
x+α∆x+β∆y
|
ℜ(∆x†(x+α∆x+β∆y))
x+α∆x+β∆y
ℜ(∆y†(x+α∆x+β∆y))
x+α∆x+β∆y

−

2

|

|

,

·

|

|

FIG. 3: Test ﬁgure used for benchmarking

as:

100

)

%

(
 
s
s
e
c
c
u
s

50

 

HIO
SO2D
SO4D

10000

 
0

5000
# iterations

FIG. 4: success rate, starting from random phases.

several phase retrieval tests were performed using diﬀer-
ent random starts. When the error metric εm goes below
a threshold that was seen as providing a good quality
reconstruction it is stopped. Fig. 4 shows the relative
performance of the various algorithms. By adding the
2D or 4D optimization the iterations converge more reli-
ably and in less iterations to a solution (Table I).

Acknowledgments

This work was performed under the auspices of the
U.S. Department of Energy by the Lawrence Livermore
National Laboratory under Contract No. W-7405-ENG-
48 and the Director, Oﬃce of Energy Research. This
work was funded by the National Science Foundation
through the Center for Biophotonics. The Center for

TABLE I: Benchmark of various algorithms

Algorithm No. of iterations for success after

and

HIO
SO2D
SO4D

50% success
2790
656
605

10000 iterations
82%
100%
100%

we can calculate the analytic expression for the gradient
and Hessian.

The gradient components are:

∇tψ = 2

∂ψ
∂α = 2
∂ψ
∂β = 2

∆˜ρ|[ ˜Ps − ˜Pm] `˜ρ + τ T ∆˜ρ´
∆˜ρs|[I − ˜Pm] (˜ρ + α∆˜ρs + β∆˜ρs)
(cid:10)
∆˜ρs|[− ˜Pm] (˜ρ + α∆˜ρs + β∆˜ρs)
(cid:10)

r ,

(cid:11)

(A11)

r ,
r ,
(cid:11)
(cid:11)

and the Hessian (writing ˜ρτ = ˜ρ + α∆˜ρs + β∆˜ρs ) and
starting from the simplest component:

(cid:10)

∂2ψ
∂β2 =

2

∆˜ρs|
D

−

= 2

X

= 2

∂ ˜Pm ˜ρτ
∂β

∆ ˜ρs

E
2√I

|
∆ ˜ρs

|
2√I

|
˜ρτ

|
˜ρτ

−

|

|

|

−
X
∆˜ρs| −
D

|
√I
˜ρτ
2
|

|

= 2

+ ℜ(∆ ˜ρ†

3

s ˜ρτ )2√I
˜ρτ
|
ℜ(∆ ˜ρ†

|

+ √I
˜ρτ
2
|

1

−

(cid:16)

|
˜ρ2
τ
∆ ˜ρ2
s

s ˜ρτ )(∆ ˜ρ†
˜ρτ

τ )

|

s ˜ρτ +∆ ˜ρs ˜ρ†
2
|
∆˜ρs
|

r

2

∆ ˜ρs
|
2
˜ρτ

|

|

|

(cid:17)

E

6

(A13)

Notice that for ∆˜ρs = ˜ρτ the Hessian in parenthesis is 0.

∂2ψ
∂α2 = 2

∆˜ρs

∂[I

˜Pm] ˜ρτ
∂α

−

2

D

(cid:12)
∆˜ρs|
(cid:12)
|
1
∆˜ρs

X

= 2

= 2

E
∆ ˜ρs
|
˜ρτ

|

2√I

−

|

|

√I
˜ρτ
2
|

−

|
1

−

(cid:16)

s ˜ρτ )2√I
˜ρτ

3

+ ℜ(∆ ˜ρ†
˜ρ2
τ
∆ ˜ρ2
s

|
∆ ˜ρs
|
2
˜ρτ

|

|

|

|
2

D

(cid:12)
h
(cid:12)
(cid:12)

∆˜ρs

r

E

(cid:17)i(cid:12)
(cid:12)
(cid:12)

The term () tends to 0 as ∆˜ρs approaches ˜ρt. The cross
terms:

(A12)

∂2ψ
∂β∂α = 2

∆˜ρs

∂[I

˜Pm] ˜ρτ
∂β

−

(A14)

= 2

∆˜ρs

= 2

∆˜ρs

D

D

D

√I
˜ρτ
2
|
√I
˜ρτ
2
|

|

|

−

−

1

E
−

(cid:16)
1

(cid:16)

−

˜ρ2
τ
∆ ˜ρ2
s

˜ρ2
τ
∆ ˜ρ2
s

2

2

∆ ˜ρs
|
2
˜ρτ

|
|
∆ ˜ρs
|
2
˜ρτ

|

|

|

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆˜ρs

∆˜ρs

r

r

E

E

(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:17)(cid:12)
(cid:12)
(cid:12)

[1] P. Hawkes and J.C.H.Spence, Eds. ”Science of Mi-

[16] M. R. Hestenes, Conjugate Direction Methods in Opti-

croscopy” . Springer. in press (2006).
[2] J. R. Fienup, Opt. Lett. 3, (1978) 27.
[3] J. R. Fienup, Appl. Opt. 21, (1982) 2758.
[4] J. N. Cederquist, J. R. Fienup, J. C. Marron, R. G. Pax-

man, Opt. Lett. 13, 619. (1988).

mization, (Springer-Verlag, New York, 1980).

[17] Fletcher, R. and Reeves, C.M., Comp, J. 7, 149-154

[18] M. J. D. Powell, Lecture Notes in Mathematics 1066,

(1964).

122-141 (1984).

[5] A. Levi and H. Stark, J. Opt. Soc. Am. A 1, 932-943

[19] E. Polak, G. Ribi´ere, Revue Fran¸caise d’Informatique et

(1984).

de Recherche Op´erationelle 16, 35 (1969).

[6] H. Stark, Image Recovery: Theory and applications.

[20] R. Fletcher, and C. M. Reeves, Comp, J. 7, 149-154

(Academic Press, New York, 1987).

(1964).

[7] V. Elser, J. Opt. Soc. Am. A 20, 40 (2003).
[8] H. H. Bauschke, P. L. Combettes, and D. R. Luke. J.

[21] E. Polak, Computational Methods in Optimization (New

York: Academic Press 1971).

Opt. Soc. Am. A 19, 1334-1345 (2002).

[22] S. J. Wright, “Primal-Dual Interior-Point Methods”,

[9] H. H. Bauschke, P. L. Combettes, and D. R. Luke, J.

Philadelphia, SIAM 1997.

Opt. Soc. Am. A 20, 1025-1034 (2003).

[23] J. R. Fienup, C. C. Wackerman, J. Opt. Soc. Am. A 3,

[10] D. R. Luke,

Inverse Problems

21:37-50(2005),

1897-1907 (1986).

[13] J. V. Burke and D. R. Luke. SIAM J. Control Opt. 42,

[25] P.-T. Chen, M. A. Fiddy, C.-W. Liao and D. A. Pommet,

(arXiv:math.OC/0405208).

[11] S. Marchesini, arXiv:physics/0603201.
[12] D. R. Luke, J. V. Burke, R. G. Lyon, SIAM Review 44

169-224 (2002).

576-595 (2003).

[14] R. Gerchberg and W. Saxton, Optik 35, 237 (1972).
[15] W. H. Press, S. A. Teukolsky, W. T. Vetterling, Brian P.
Flannery, Numerical Recipes in C, Cambridge University
Press,

[24] S. Marchesini, H. N. Chapman, A. Barty, M. R. How-
ells, J. C. H. Spence, C. Cui, U. Weierstall, and
IPAP Conf. Series 7, 380-382 (2006)
A. M. Minor,
arXiv:physics/0510033.

J. Opt. Soc. Am. A 13, 1524-31 (1996).

[26] T. Isernia, G. Leone, R. Pierri, and F. Soldovieri, J. Opt.

Soc. Am. A 16, 1845-1856 (1999)

[27] G. Oszl´anyi and A. S¨uto, Acta Cryst. (2005).


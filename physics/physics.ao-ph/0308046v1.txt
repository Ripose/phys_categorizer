3
0
0
2
 
g
u
A
 
2
1
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
6
4
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Use of the likelihood for measuring the skill of probabilistic
forecasts

Stephen Jewson∗
Risk Management Solutions, London, United Kingdom

February 20, 2014

Abstract

We deﬁne the likelihood and give a number of justiﬁcations for its use as a skill measure for
probabilistic forecasts. We describe a number of diﬀerent scores based on the likelihood, and brieﬂy
investigate the relationships between the likelihood and the Brier score, the mean square error and
the ignorance.

1 Introduction

Users of forecasts need to know:

whether the forecasts they are receiving have been adequately calibrated

whether the forecasts they are receiving are any better than an appropriate simple model such as
climatology

which of the forecasts they are receiving is the best

•

•

•

To answer these questions, a single measure of forecast quality is needed. For calibration, the measure
serves as a cost function that must be minimized in order to ﬁnd the optimum values for the free
parameters in the calibration algorithm. For comparison with climatology or other forecasts, the measure
serves as a way of deriving a ranking.
There are many standard measures of forecast quality. For example, for calibrating and comparing single-
valued temperature forecasts, mean square error (MSE) is common. For binary probabilistic forecasts,
the Brier score (Brier, 1950) is often used. For continuous probability forecasts, the continuous rank
probability score and the ignorance have been suggested.
In this paper we will argue that likelihood-based measures provide a simple and natural general framework
for the evaluation of all kinds of probabilistic forecast. For example, likelihood based measures can be
used for binary and continuous probability forecasts, for temperature and precipitation, and for one lead
time or many lead times simultaneously.
In section 2 we deﬁne the likelihood and discuss why we think it is a useful measure of forecast skill. In
section 3 we include expressions for the likelihood for the normal distribution and in section 4 we discuss
relations between the likelihood and other forecast scoring methods. Finally in section 5 we summarise
and describe some areas of future work.

2 Probabilistic forecasts and the likelihood

How should we evaluate the skill of a probabilistic forecast? We advocate the use of a particular set of
measures that are taken from classical statistics, and are all based on the likelihood. Likelihood is deﬁned
very simply as the probability of the observations given the forecast. In this phrase the observations refers
to the entire set of observations that we have available to validate a certain forecast, and the forecast
refers to the entire set of corresponding forecasts.
Likelihood was ﬁrst used by Fisher (1912) as a method for ﬁtting parameters to parametric distributions.
Fisher proposed the likelihood as the natural beneﬁt function that one should maximise in order to deﬁne

∗Correspondence address: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com

the best-ﬁt parameters of the distribution. This suggestion was given a mathematical basis when it was
shown that the parameter values that maximise the likelihood are the most accurate possible estimates
for the unknown parameters for most problems (Casella and Berger, 2002).
Fisher’s problem, of how to evaluate the goodness of ﬁt of a distribution to a number of samples, is
exactly the same as the problem of how to evaluate a probabilistic forecast. Instead of the distribution
we have the probabilistic forecast and instead of the samples we have the verifying observations.

2.1 Advantages of the likelihood as a measure for skill

We consider that the likelihood has the following advantages as a measure of probabilistic forecast skill:

It has a simple deﬁnition that, from a purely intuitive point of view, seems to be a reasonable basis
on which to compare forecasts

It is mathematically optimal in the sense that estimates of parameters of calibration models ﬁtted by
maximising the likelihood are usually the most accurate possible estimates (see Casella and Berger
(2002)).

It is a generalisation of two of the most commonly used skill scores: Brier score and RMSE (see
section 4 below for a discussion of this).

It shows how Brier score and RMSE should be generalised to the case of autocorrelated forecast
errors

The properties of the likelihood have been studied at great length over the last 90 years: it is well
understood

It is both a measure of resolution and reliability

Likelihood can be used for both calibration and assessment: this creates consistency between these
two operations

Use of the likelihood also creates consistency with other statistical modelling activities, since most
other statistical modelling uses the likelihood. This is important in cases where use of forecasts is
simply a small part of a larger statistical modelling eﬀort, as is the case for our particular business.

Likelihood can be used for all meteorological variables

Likelihood can be used to compare multiple leads, multiple variables and multiple locations at the
same time in a sensible way (giving a single score) even when these leads, variables and locations
are cross-correlated

2.2 Forecast scores derived from the likelihood

A number of diﬀerent scores can be derived from the likelihood.

The log-likelihood (LL) reduces the range of values of the likelihood to a more manageable scale

Minus the LL (MLL) has the characteristic that better forecasts have lower values: in this way it
is analogous to the MSE

The square root of the MLL (RMLL) has a further compressed scale

All these measures can be transformed into skill scores from zero to one in the usual way

Other transformations are also possible: for instance, one might consider normalising by the number of
data points.

•

•

•

•

•

•

•

•

•

•

•

•

•

•

3 The likelihood for the normal distribution

For a normal distribution the likelihood is given by:

L =

1
√2πdet

exp(

1
2

−

(T

−

T
µ)

Σ

−1

(T

µ))

−

where T is the vector of observations, µ is the vector of means from the forecast, Σ is the covariance
matrix of the forecast errors, and det is the determinant of Σ.
The log-likelihood is then:

1
2
In the case where the forecast errors can be assumed to be uncorrelated in time, the likelihood becomes:

ln(2πdet)

T
µ)

l =

(2)

1
2

µ)

(T

(T

−

−

−

−

Σ

−1

and the log-likelihood is:

L =

1
√2πdet

exp

1
2

 −

l =

1
2

−

ln(2πσi)

1
2

−

i=N

i=1
X

i=N

(Ti

µi)2

−
σ2
i

!

i=1
X

i=N

(Ti

µi)2

−
σ2
i

i=1
X

When evaluating a forecast using the likelihood, calculating the covariance matrix is straightforward
because the forecast errors are known. When calibrating a forecast using the likelihood, calculating the
covariance matrix is more diﬃcult. If it is reasonable to assume that the errors are uncorrelated in time,
then this simpliﬁes the calibration considerably. However, this is generally not the case.

4 Relations between the likelihood and other skill scores

Likelihood is closely related to a number of other measures of forecast skill, as we see below.

4.1 Relation between the likelihood and Brier Score

For event forecasts where the forecast errors are independent in time, the likelihood is given as:

i=N

L =

f o

i=1
X
where f is the forecast probability of the event, with a value from zero to one, and o is the observation,
which has a value of zero if the event does not occur and one if it does. All sums are taken over the set
of observation-forecast pairs, and all values have implicit dependency on the summing index.
The Brier score for the same system is given as:

Expanding equation 6, we see that:

If forecast A is better than forecast B by the Brier score then:

or

B =

2
o)

(f

−

i=N

i=1
X

B =

f 2

2f o + o2

−

i=N

i=1
X

BA < BB

i=N

i=1
X

f 2
A

−

2fAo <

f 2
B

−

2fBo

i=N

i=1
X

i=N

i=N

i=1 −
X

2fAo <

2fBo

i=1 −
X

If we now assume that the two forecasts have been calibrated to have the same sum of squared probabil-
ities, then we see that:

(1)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

or

which is:

i=N

i=N

fAo >

fBo

i=1
X

i=1
X

lA > lB

We see that, with one condition, the Brier score and the likelihood are consistent in that they give the
same ranking of forecasts.

4.2 Relation between the likelihood and RMSE

We now show that the RMSE and the likelihood are consistent (i.e. give the same ranking of forecasts)
in the case of two normally distributed probabilistic forecasts with diﬀerent means but the same constant
spreads. Likelihood is used to compare the whole distribution, while RMSE is used to compare the means.
Suppose we have two forecasts, A and B, and suppose:

Taking logs, this gives:

LA > LB

lA > lB

Substituting in the expression for the log-likelihood for a normal distribution we see that:

N
2

−

ln(2π)

ln(σ)

N
2

−

1
2σ2

−

fA)

2 >

(x

−

N
2

−

ln(2π)

ln(σ)

N
2

−

1
2σ2

−

i=N

i=1
X

2
fB)

(x

−

where N is the number of observations, fa and fb are the time varying forecasts, and x is the time-varying
observations.
Cancelling terms from both sides:

1
2σ2

−

fA)

2 >

(x

−

1
2σ2

−

2
fB)

(x

−

i=N

i=1
X

Cancelling more terms this gives:

or

fA)

2 <

(x

−

2
fB)

(x

−

i=N

i=1
X

MSEA < MSEB

and so we see that comparing these forecasts using likelihood or MSE gives the same results i.e. that
forecast A is better than forecast B.

4.3 Relationship between the likelihood and ignorance

Roulston and Smith (2002) describe a score for the assessment of probabilistic forecasts that they call
the ignorance, and justify its usage on the basis of information theory and use in an optimal betting
strategy. They deﬁne the ignorance for a single forecast-observation pair as minus the log (base 2) of the
probability of the observation given the probabilistic forecast. We see that this is equivalent to minus log
(base 2) of the likelihood for that single forecast-observation pair.
Comparing forecasts using the ignorance or any of the likelihood-based scores described above will give
the same results if the forecasts errors are uncorrelated in time. If the errors are correlated in time, and
this is taken into account in the calculation of the likelihood, then they may give diﬀering results.
One can consider the likelihood as a generalisation of the ignorance to a) forecasts with autocorrelated
forecast errors and b) forecasts for many variables, locations or leads at once. One can consider the
ignorance as a special case of the likelihood when forecast errors are taken to be uncorrelated, and when
looking at only a single variable, location and lead.

i=N

i=1
X

i=N

i=1
X

i=N

i=1
X

5 Summary

We have summarised the use of the likelihood for the evaluation of the skill of probabilistic forecasts.
We believe that likelihood provides a useful general framework for the calibration and evaluation of all
probabilistic forecasts, for all variables. We are in the process of applying the likelihood to various
forecasting situations that are relevant to our business: examples are given in Jewson et al. (2003a)
and Jewson et al. (2003b).
A number of question arise that merit further investigation. These include:

•

•

When calibrating forecasts to maximise the likelihood, what numerical methods can be used to
estimate the forecast error covariance matrix?

Is it really necessary to calculate the likelihood using the correct forecast error covariance matrix,
or is it satisfactory in practice to make the assumption that forecast errors are uncorrelated? One
can argue that if the covariance matrix is not correctly modelled, then forecasts with autocorrelated
errors are given more credit than is their due. However, it may be that in practice the ranking of
forecasts is the same whether or not the covariance is estimated accurately.

•

What are the relationships, if any, between the likelihood and other skill scores apart from those
discussed above?

6 Acknowledgements

The author has had helpful discussions on the topics discussed above with a number of people, including
Anders Brix, Pablo Doblas-Reyes, Renate Hagedorn and Christine Ziehmann.

References

1950.

1912.

G Brier. Veriﬁcation of forecasts expressed in terms of probabilities. Monthly Weather Review, 78:1–3,

G Casella and R L Berger. Statistical Inference. Duxbury, 2002.

R Fisher. On an absolute criterion for ﬁtting frequency curves. Messenger of Mathematics, 41:155–160,

S Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble

temperature forecasts. ASL, 2003a. Submitted.

S Jewson, F Doblas-Reyes, and R Hagedorn. The assessment and calibration of ensemble seasonal
forecasts of equatorial paciﬁc ocean temperature and the predictability of uncertainty. ASL, 2003b.
Submitted.

M Roulston and L Smith. Evaluating probabilistic forecasts using information theory. Mon. Wea. Rev.,

130:1653–1660, 2002.


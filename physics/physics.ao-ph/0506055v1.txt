5
0
0
2
 
n
u
J
 
7
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
5
5
0
6
0
5
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Improving on the empirical covariance matrix using truncated
PCA with white noise residuals

Stephen Jewson∗

January 16, 2014

Abstract

The empirical covariance matrix is not necessarily the best estimator for the population covariance
matrix: we describe a simple method which gives better estimates in two examples. The method
models the covariance matrix using truncated PCA with white noise residuals. Jack-knife cross-
validation is used to ﬁnd the truncation that maximises the out-of-sample likelihood score.

1 Introduction

There are many applications in which it is necessary to estimate population covariance matrices from
sample data. Our own particular interest is in the statistical modelling of weather data for the valuation
of weather-related insurance contracts (Jewson et al., 2005), but there are other uses in ﬁelds as diverse
as ecology and pattern recognition. A simple and commonly used estimator for the population covariance
matrix is the empirical covariance matrix. However, there seems to be no reason why this should be the
best estimator, and we present a recipe that we show generates better estimates in two examples. The
recipe is based on PCA. We apply PCA to the sample data, truncate the series of singular vectors and
model the residuals using white noise. The truncation is then varied and the optimal truncation is chosen
as that which maximises the out-of-sample likelihood in a jack-knife test. The resulting estimate of the
population covariance matrix is a better estimate than the empirical covariance matrix in the sense that
it gives higher out-of-sample likelihood scores for the sample data.
In section 2 we brieﬂy review PCA, in section 3 we describe our method for determining the optimal
truncation, in section 4 we give two examples and in section 5 we summarise.

2 Principal Component Analysis

Consider a matrix of data X with dimensions s by t and rank r. We will think of s and t as representing
space and time, but many other interpretations are possible. Mathematically speaking, we know that r ≤
min(s, t). Practically speaking, for any genuine observed data, we can usually assume that r = min(s, t).
This is because it is inﬁnitely unlikely that there is a linear relation between the columns or the rows
in X (unless one of the columns or rows has deliberately been produced as a linear combination of the
others). Such is the typical nature of real measured data.
The mathematical theory of singular value decomposition states that all matrices can be decomposed in
a certain unique way. Applying this theory to our matrix X gives:

where E has the dimensions s by r, Λ has dimensions r by r and P has dimensions t by r. By the singular
value decomposition theorem these matrices have the following properties (inter alia):

X = EΛP T

(1)

• ET E = I

• P T P = I

• Λ is diagonal
∗Correspondence address: Email: x@stephenjewson.com

PCA is very closely related to eigenvalue decomposition: E contains the eigenvectors of the covariance
matrix XX T , P contains the eigenvectors of the covariance matrix X T X and the two covariance matrices
have the same eigenvalues, which are the diagonal terms of Λ2 (we discuss the relations between PCA
and eigenvalue decomposition in a little more detail in Jewson (2004)).
We can write equation 1 in terms of the elements of the matrices as:

In this form we can see more clearly that we are writing the original data in terms of a sum of r rank
1 matrices, each of which is formed as the product of two vectors and a scalar. Since we are thinking
of the two dimensions as space and time we can think of the two vectors that make up the k’th rank 1
matrix as being a set of weights in space (a spatial pattern eik) and a set of weights in time (a time series
pjk). The ordering of the rank 1 matrices is arbitrary, but by convention is always taken with the highest
values of λ ﬁrst. This has the consequence that the ﬁrst of the r matrices contains the most variance, the
second contains the next-most, and so on. One of the properties of PCA is that the variance accounted
for by the ﬁrst rank 1 matrix is actually the largest possible (among all rank 1 matrices, subject to the
orthonormality constraints), and the variance accounted for by the second is the largest possible from
the remaining variance.
There are various adaptions of this basic version of PCA. For instance, the matrix X may be centred
and/or standardized prior to deriving the patterns.
Given equation 2 we can consider approximating the data by truncating the sum to fewer than r of the
rank 1 matrices. If we let r′ be the number of matrices retained this gives:

xij =

eikλkpjk

r

Xk=1

ˆxij =

eikλkpjk

r′

Xk=1

This truncation may make sense for two reasons. Firstly, the retained patterns together may account for
a large fraction of the total variance, but in only a small number of patterns. PCA can thus act as an
eﬃcient way to represent a large fraction of the information in X. Secondly, the retained patterns are
presumably the more accurately estimated patterns, in a statistical sense. This is useful if the PCA is to
be used for simulation or extrapolation of any kind.
We will now make the restrictive assumption that the data in X is independent in time, dependent in
space and distributed with a multivariate normal distribution. In this case the spatial patterns show
structure while the time series are uncorrelated. We wish to generate surrogate data that has the same
correlation structure in space as X, and this can be done by replacing the time series in expression 2
with simulated values:

r

xsim
ij =

eikλkpsim
jk

Xk=1
It is easy to show that xsim has the same spatial covariance matrix as the original xij . However, the
rank 1 matrices for high values of k are likely to be very poorly estimated, and this may be bad for our
simulations. This motivates the idea that we should perhaps truncate the sum and use only the well
estimated patterns in the simulation, up to the r′’th. There are two problems with this, however: ﬁrst,
that the variance of the resulting simulated data would be lower than the variance of the observations,
and second that the rank of the simulated data could be too low (the dimension of the space spanned
by the simulated data could be smaller than the dimension of the space spanned by the sample data).
This might result in simulations which could never explore the space of possible observations fully, and
we ﬁnd this to be undesirable. These problems can both be corrected by adding appropriate amounts of
white noise as ‘padding’.
This gives:

xsim
ij =

pikλkqsim

jk + σiǫij

r′

Xk=1

where ǫ is white noise and the σi are chosen so that the simulations have the correct variance. The lower
r′, the greater the σi have to be to make up the full variance.
Within this setup the question we wish to ask is: how should the truncation r′ be chosen?

(2)

(3)

(4)

(5)

3 Choosing the truncation

The method we propose for choosing the truncation works as follows. As the truncation r′ is increased,
more information about the correlation structure of X is included in the simulations. But more spurious
information is also included because the higher order patterns are less well estimated. Because of these
competing eﬀects the beneﬁt of increasing r′ presumably disappears at some point: we wish to ﬁnd
exactly the value of r′ at which this occurs. To do so we use a jack-knife cross-validation technique: we
test the extent to which a certain truncation is able to represent data that is outside the sample of data
on which the PCA is estimated. This test allows us to compare diﬀerent truncations in a fair and honest
way, and ﬁnd which performs the best.
What cost function should we use for our test? A particular truncation along with the white noise
padding is eﬀectively an estimate of the multivariate distribution of X. This motivates us to use the
standard cost function used for the ﬁtting of distributions in classical statistics, which is the log-likelihood.
Given a particular truncation, and the amplitudes of the supplementary white noise, we can calculate the
covariance matrix of the multivariate distribution. From this we can calculate the log-likelihood using
the standard expression for the density for the multivariate normal with dimension p:

(6)

(7)

f =

1
p
(2π)
2 D

1
2

1
2

(cid:18)

exp

−

(z − µ)T Σ

−1

(z − µ)

(cid:19)

where Σ is the covariance matrix (size p by p), D is the determinant of the covariance matrix (a single
number), z is a vector length p and µ is a vector length p.
The log-density is then:

logf = −

plog(2π) −

logD −

(z − µ)T Σ

−1

(z − µ)

1
2

1
2

1
2

We will refer to the 2nd and 3rd terms of this equation as the ‘dispersion term’ (− 1
2 logD) and the
‘standardisation term’ (− 1
2 (z − µ)T Σ−1(z − µ)). D is a measure of the dispersion in the multivariate
distribution: for instance, when p = 1 we have D = σ. The dispersion term (which has a negative coeﬃ-
cient) penalizes distributions with a large dispersion. (z − µ)T Σ−1(z − µ) is the ‘z value’ or standardised
value of the spatial pattern z − µ, in the multivariate normal distribution described by Σ. If z − µ is very
unlikely in this distribution then this term will be very large. The standardisation term penalizes the
distribution if there are many points with large standardised values. The distribution which maximises
the log-likelihood is a trade-oﬀ between these two eﬀects: the dispersion has to be small, but not so small
that the standardised values of the out-of-sample data is too large.
One aspect of using log-likelihood as a cost function is that it rejects a distribution and covariance
matrix completely if there is even a single observation that could not have come from the distribution.
For instance, if we use truncated PCA without the white noise padding then many of the out-of-sample
observations would be impossible, simply because they come from a higher dimensional space. We consider
this strict rejection of distributions that do not span the space of the observed data to be desirable.
We now summarise our method. For each truncation we run over the data, missing out each time point
in turn, applying PCA to the remaining data, truncating at the given level, estimating the amplitude
of the supplementary white noise, calculating the covariance matrix for the combination of truncated
singular vector series and white noise, and calculating the log-likelihood for the missed data. We combine
all the log-likelihoods for a particular truncation to give a single score for that truncation. We then
compare these log-likelihood scores across the diﬀerent truncations to ﬁnd which truncation is the best
at predicting the distribution of the out-of-sample data.

4 Examples

We now give two simple examples of the method described above. They are both motivated by our interest
in simulating the risk in weather derivative portfolios, for which we wish to create many thousands of
years of surrogate weather data (see chapter 7 in Jewson et al. (2005)).
In both examples we standardise the data in time before we apply PCA. For the ﬁrst example s < t,
while for the second s > t. This alters the nature of the problem signiﬁcantly, as we will see below.

4.1 Example 1: UK temperatures

In our ﬁrst example we take a matrix X of data consisting of winter average daily average temperatures
for 5 UK locations. There are 44 winters of data and so s = 5 and t = 44. The rank of the data is 5, and

is unaﬀected by the standardisation, which is only applied in the time dimension. The space of possible
spatial patterns, which has dimension 5, can be spanned by the 5 spatial singular vectors if there is no
truncation. If there is truncation then this is no longer the case, and a general spatial pattern could
not be represented as a linear combination of the remaining spatial singular vectors. The ‘padding’ with
white noise solves this problem, as described above.
Figure 1 shows (minus one times) the log-likelihood versus the truncation for this example. We see that
there is a big decrease in the cost function as we move from a purely independent model to one that
uses the ﬁrst singular vector only: we conclude that this data is deﬁnitely correlated in space. There is
a much smaller further decrease when the second singular vector is added, and adding further singular
vectors beyond the second actually increases the cost function. A truncation to two singular vectors
is therefore optimal in this case. Truncations of two, three and four all perform better than using the
empirical covariance matrix (which is a truncation of ﬁve). The covariance matrix based on all ﬁve
singular vectors, and the change in the covariance matrix caused by truncation to the ﬁrst two, are
shown below. We see that the changes in the individual covariances are fairly small (perhaps between
1% and 4%).

46.00
42.40
37.25
41.17
40.49

0.00
1.72
-0.35
0.39
-0.14

42.40
46.00
38.24
42.69
41.17

1.72
0.00
0.16
-0.22
0.27

37.25
38.24
46.00
44.04
44.46

-0.35
0.16
0.00
0.26
0.36

41.17
42.69
44.04
46.00
44.92

0.39
-0.22
0.26
0.00
0.27

40.49
41.17
44.46
44.92
46.00

-0.14
0.27
0.36
0.27
0.00

Going further, we can test whether a truncation of two is signiﬁcantly better than a truncation of one.
We will do this using the method we used in Hall and Jewson (2005) in which we consider each individual
time point of the data and count the number of times each of the two methods beats the other. The
resulting test statistic is distributed as a binomial distribution under the null hypothesis that there is no
signiﬁcant diﬀerence between the two truncations.
The results of this year by year comparison are shown in ﬁgures 2 and 3. We see that, for every comparison
of adjacent truncations, one or the other wins in every year. We conclude that the ordering of the results
in ﬁgure 1 is extremely highly signiﬁcant.
We can also try and understand the variations in the log-likelihood score curve shown in ﬁgure 1 by
breaking the curve down into the determinant and standardization terms in equation 7. This breakdown
is shown in ﬁgure 4. We see that, in this case, the shape of the log-likelihood score curve is ﬁxed by the
determinant term. Had we known this in advance we could have found the optimum truncation by simply
calculating the determinant as a function of truncation. This is a simple in-sample calculation, and much
less complex than the full cross-validation calculation. We suspect that it may always be the case that
the determinant term dominates when s < t, and this possibility seems to merit further investigation.
We also suspect that the dominance of the determinant term explains why the breakdown by year gives
such clear results.
With some trepidation we now attempt to explain the behaviour of the determinant and standardisation
curves. The standardisation curve seems to be the easier of the two to understand. For all 6 truncations
this term is very small: this means that all of the out-of-sample spatial patterns are quite consistent with
the ﬁtted distribution. This is presumably because the out-of-sample patterns live in a 5 dimensional
space, and the ﬁtted distributions have signiﬁcant variance in all of these dimensions. The determinant
curve is a little harder to understand. As the truncation increases it shows a decrease and then an
increase. The decrease seems to be because as the truncation is increased the degree of specialisation
of the model increases. The subsequent increase is presumably because of sampling error on the higher
singular vectors.

4.2 Example 2: US temperatures

In our second example we take a matrix X of data consisting of winter average daily average temperatures
for 308 US locations. There are 54 winters of data and so s = 308 and t = 54. The rank of the data is
53 because of the temporal standardisation. Because s > t we are now in a situation where the space of

possible spatial patterns, which has dimension 308, cannot be spanned by the spatial singular vectors,
of which there are only 53. Truncation and the white noise padding are therefore essential: this is a
case where it seems that we are guaranteed to ﬁnd a better estimate of the covariance matrix than that
given by the empirical covariance matrix, because the empirical covariance matrix will immediately fail.
In fact, the simple example of a purely independent model (a full-rank diagonal covariance matrix) will
always beat the empirical covariance matrix.
The likelihood score versus truncation is shown in ﬁgure 5. We can only evaluate the likelihood score up
to a truncation of 52. This is because the rank of the data is 53, and so the truncation of 53, which has
no white noise padding, gives a correlation matrix that cannot be inverted.
We see that the log-likelihood gradually reduces as the truncation is increased, up to a truncation of 47.
It then rapidly increases to very large values between 47 and 52. 47 is thus the optimum truncation.
In ﬁgure 6 we decompose the log-likelihood curve into determinant and standardization terms. In this
case we see that it is the interplay of these two terms that ﬁxes the minimum, and it would not be possible
to determine the minimum using the determinant curve alone (which is monotonic).
Again, with some trepidation, we attempt to explain the shapes of these two curves. The determinant
curve decreases as the truncation increases: we think this is because adding more singular vectors, at
the expense of white noise variance, makes the multivariate distribution more speciﬁc i.e. it concentrates
the variance into fewer dimensions. Ultimately, for a truncation of 53, there is only non-zero variance in
53 of the 308 dimensions (and the correlation matrix is no longer invertible). The standardisation term
gradually increases as a result of this specialisation. Then, as the truncation approaches 53, the variance
in the other dimensions becomes very small, and the probability of some of the out of sample patterns,
which come from a 308 dimensional space, becomes very low. At this point the standardisation term
becomes very large. We think that this tradeoﬀ between the determinant term and the standardisation
term is likely to occur whenever s > t.

5 Summary

We have investigated a simple approach for making a better estimate of the population covariance matrix
than that given by the empirical covariance matrix. The method is based on truncated PCA with white
noise residuals. The question of how to truncate PCA has been addressed before, but we introduce a
simple new method based on a very straightforward reasoning: we want to choose the truncation so that
we maximise the likelihood of out-of-sample data. Finding the best truncation under this deﬁnition of
optimum is relatively easy. We give two examples, and in both cases we ﬁnd better estimates of the
population covariance matrix than that given by the empirical covariance matrix (where better is deﬁned
as giving higher out-of-sample likelihood scores).
Based on the results from our examples we conclude that using the empirical covariance matrix for
statistical modelling may not be a very good idea since the higher order singular vectors tend to be
poorly estimated and thus decrease the out-of-sample likelihood. In the s > t case there is the additional
problem that the empirical covariance matrix does not describe a space large enough to contain the
observations. Optimal truncation with white noise ‘padding’ solves both these problems, and thus may
give better modelling results.
In some cases, such as the two examples we have used in this study, one of the dimensions of the sample
In this case it may be possible to do even better by modelling
data is a genuine spatial dimension.
the residuals using ‘red’ noise, rather than just white noise. Testing this idea is next.
It would also
be interesting to compare our method with other possible methods for improving the estimate of the
covariance matrix, such as linear combinations of the empirical covariance matrix with an independent
model.

6 Acknowledgements

The author would like to think Dag Lohmann, Sergio Pezzuli and Christine Ziehmann for interesting
discussions on this topic.

7 Legal statement

SJ was employed by RMS at the time that this article was written.

However, neither the research behind this article nor the writing of this article were in the course of his
employment, (where ’in the course of their employment’ is within the meaning of the Copyright, Designs
and Patents Act 1988, Section 11), nor were they in the course of his normal duties, or in the course
of duties falling outside his normal duties but speciﬁcally assigned to him (where ’in the course of his
normal duties’ and ’in the course of duties falling outside his normal duties’ are within the meanings of the
Patents Act 1977, Section 39). Furthermore the article does not contain any proprietary information or
trade secrets of RMS. As a result, the author is the owner of all the intellectual property rights (including,
but not limited to, copyright, moral rights, design rights and rights to inventions) associated with and
arising from this article. The author reserves all these rights. No-one may reproduce, store or transmit,
in any form or by any means, any part of this article without the author’s prior written permission. The
moral rights of the author have been asserted.
The contents of this article reﬂect the author’s personal opinions at the point in time at which this article
was submitted for publication. However, by the very nature of ongoing research, they do not necessarily
In addition, they do not necessarily reﬂect the opinions of the
reﬂect the author’s current opinions.
author’s employers.

References

2004.

T Hall and S Jewson. Statistical modelling of tropical cyclone tracks: a comparison of models for the

variance of trajectories. arXiv:physics/0505103, 2005.

S Jewson. The application of PCA to weather derivative portfolios. http://ssrn.com/abstract=486503,

S Jewson, A Brix, and C Ziehmann. Weather Derivative Valuation. CUP, 2005.

0
5
5

0
0
5

0
5
4

0
0
4

0
5
3

d
o
o
h

i
l

e
k

i
l

−
g
o
l
 
*
 
1
−

0

1

2

3

4

5

truncation

Figure 1: The log-likelihood versus truncation for example 1 described in the text.

0
1

9

8

7

6

5

4

d
o
o
h

i
l

e
k

i
l

−
g
o
l
 
*
 
1
−

0

10

20

30

40

truncation

Figure 2: The log-likelihood on a yearly basis for the six truncations used in example 1.

0
.
5

8
.
4

6
.
4

4
.
4

2
.
4

0
.
4

d
o
o
h

i
l

e
k

i
l

−
g
o
l
 
*
 
1
−

0

10

20

30

40

truncation

Figure 3: Same as ﬁgure 2 but with a diﬀerent scale to clarify the diﬀerences between the curves.

i

m
r
e
t
 
t
n
a
n
m
r
e
t
e
d
 
*
 
1
−

i

m
r
e
t
 
n
o
i
t
a
z
d
r
a
d
n
a
t
s
 
*
 
1
−

0
0
4

0
0
3

0
0
2

6
.
3

2
.
3

8
2

.

0

1

2

3

4

5

truncation

0

1

2

3

4

5

truncation

Figure 4: Decomposition of the log-likelihood curve in ﬁgure 1 into the determinant and standardization
terms. We see that the curve in ﬁgure 1 is completely dominated by the determinant term.

d
o
o
h

i
l

e
k

i
l

−
g
o
l
 
*
 
1
−

d
o
o
h

i
l

e
k

i
l

−
g
o
l
 
*
 
1
−

0
0
0
0
5
1

0

0
0
0
0
1

0
0
0
8

0
0
0
6

0

10

20

30

40

50

truncation

40

42

44

46

48

50

truncation

Figure 5: The log-likelihood versus truncation for example 2 described in the text, with two diﬀerent
vertical and horizontal scales.

i

m
r
e
t
 
t
n
a
n
m
r
e
t
e
d
 
*
 
1
−

i

m
r
e
t
 
n
o
i
t
a
z
d
r
a
d
n
a
t
s
 
*
 
1
−

0

0
0
0
0
4
−

5
0
+
e
 
 
 
2

0
0
+
e

 
 
 

0

0

10

20

30

40

50

truncation

0

10

20

30

40

50

truncation

Figure 6: Decomposition of the log-likelihood curve in ﬁgure 5 into the determinant and standardization
terms. In this case the curve in ﬁgure 5 is not dominated by either term, and the minimum in the curve
in ﬁgure 5 arises from interplay between these two terms.


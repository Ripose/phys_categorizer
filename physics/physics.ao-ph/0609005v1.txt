Abstract.

E-mail: francisco.tapiador@uclm.es

F J Tapiador1 and R Verdejo2
1Universidad de Castilla-La Mancha (UCLM),45071 Toledo, Spain
2Swiss Federal Laboratories for Materials Testing and Research (EMPA), 9014 St
Gallen, Switzerland

6 Single Member Selection in Ensemble Forecasting
0
0
2
 
p
e
S
 
1
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
5
0
0
9
0
6
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Ensemble forecasting is a technique devised to palliate sensitivity to initial
conditions in nonlinear dynamical systems. The basic idea to avoid this sensitivity is
to run the model many times under several slightly-diﬀerent initial conditions, merging
the resulting forecast in a combined product. We argue that this blending procedure is
unphysical, and that a single trajectory should be chosen instead. We illustrate our case
with a climate model. While most of the current climate simulations use the ensemble
average technique as merging procedure, this paper shows that this choice presents
several drawbacks, including a serious underestimation of future climate extremes.
It is also shown that a sensible choice of a single estimate from the ensemble solves
this problem, partly overcoming the inherent sensitivity to initial conditions of those
non-linear systems with a large number of degrees of freedom.

Single Member Selection in Ensemble Forecasting

2

1. Introduction

A numerical model is said to present sensitivity to initial conditions (SIC) when no
matter how close two initial conditions are they diverge as the system evolves [1]. This
behavior arises from nonlinear processes: small initial diﬀerences are multiplied and grow
to high magnitude orders, soon resulting in uncorrelated trajectories for arbitrarily-close
initial points [2]. SIC is one of the characteristics deﬁning chaotic behavior [3].

Climate models present this sensitivity [4] [5] [6]. After a few iterations, a model
may generate diﬀerent forecasts for diﬀerent albeit close initial conditions, making
prediction very diﬃcult [7]. Ensemble forecasting methods have been proposed to
circumvent this problem [8] [9]. The key idea is to make the system evolve under
several slightly-diﬀerent initial conditions mimicking our uncertainty on the true initial
conditions [10]. By using many initial conditions we obtain a bunch of trajectories in
phase space. Since the initial conditions are carefully chosen as to encompass the full
dynamics of the system [11], it is sensible to assume that the true trajectory should lie
within this set [12] [13]. In ensemble literature a trajectory in phase-space from a given
initial condition is called a member of the ensemble.

Climate ensembles provide two major outcomes. First, they give an estimate of
if most of the members present a close evolution
the predictability of the forecast:
the system is considered highly predictable, and unpredictable otherwise [14] [15]. The
second result is an estimate of the future climate, which is given by a combination of
the forecast of the individual members [16] [17].

Ensemble methods are aimed to provide a probabilistic forecast -that is, a prediction
described by probabilistic distribution functions (pdfs)- but the average of the members
is widely used as an estimate of the mean future climate: [18] [19] [20] [21] [22] [23] [24].
The reason is that the average is expected to have less error covariance than any single
member: if u0 is the anomaly of the true trajectory of zero mean, ˆu is the anomaly of
a single forecast, and ¯u is the anomaly of the ensemble average of m members, we have
from [25] that:

D(0
D(ˆu
D(¯u

−

−

−

u0)(0
u0)(ˆu
u0)(¯u

u0)T
E
u0)T
u0)T

E

E

−

−

−

t→∞
→
t→∞
→
t→∞
→

U

2U
(1 + 1

m)U

(1)

Which proves that,

in the inﬁnity limit, the ensemble average has less error

covariance than a randomly-chosen single forecast.

In the ﬁrst part of this article we will show that this averaging procedure results
both in considering phase state points incompatible with the dynamics of the system,
and in producing a merged forecast that does not necessarily capture the extreme values
of the forecast. In terms of future climate, this means that ensemble averages might
be underestimating climatological extremes. We will use a simple numerical example
(a logistic map) and an actual climate ensemble to illustrate the eﬀects of ensemble
averaging. In the second part we will provide an alternative to ensemble averaging.

Single Member Selection in Ensemble Forecasting

3

2. Ulam’s map example

·

µ

−

x2
n, µ = 2 (Ulam’s map) to
Let us use the logistic map with parameter µ, xn+1 = 1
describe the ensemble averaging procedure. This system is widely used in the study of
chaos dynamics. The support of Ulam’s map is the [-1,1] interval, being dense in this
1 and x = 1/2. The map is ergodic and
interval and having two stable points at x =
mixed, and its natural invariant density [26] is ρ(x) =
π(1−x2)1/2 (Fig. 1). The chaotic
behavior of the map dynamics can be established by calculating the Lyapunov exponent,
resulting in log(2). Since this value is positive, the system shows SIC. As the map is
ergodic, ρ(x) can be estimated either as ensemble or time average using an histogram
function.

−

1

∈

[x0 −

To investigate the properties of the ensemble average, we can construct an ensemble
around a non-stable initial point such as x0 = 0.3. We will not assume that we know
the error distribution around the true initial value.
Instead, we will take uniformly-
10−8, x0 + 10−8] (a more careful choice [27] would make no
distributed ICs xi
diﬀerence in this example). Figure 2 shows how the ensemble average of Ulam’s map
compares with a true trajectory after 100 iterates (RK4 scheme). The trajectory of
the ensemble average bears little resemblance to the actual behavior of any possible
trajectory such as the reference orbit shown, being clear from the ﬁgure that if the
ensemble average yields a high correlation it is because it remains almost equidistant to
the

{−
If we calculate the mean value of the average after 10,000 iterations, it compares
well with the true value (-0.0004 vs. 0.0). The variance, however, is 0.00633, which
is well below the actual value (0.5). The same would apply for the derived tent map
x) for x > 1/2, with support [0,1], even when the
xn+1 = 2
natural invariant density of this map is constant ρ(x) = 1. After 10,000 iterations, the
variance of the averaged ensemble is 0.05, compared with 0.72 for an typical orbit.

1/2 and 2

extremes.

x for x

1, 1

(1

≤

−

}

·

·

These examples show that the ensemble average may give a good statistical r2
correlation in the long term, but we are building up a trajectory without a physical
counterpart. Taking the ensemble average as a representative of the true dynamics of
the system does not guarantee capturing the ensemble behavior: there is no initial value
for Ulam’s map capable of generating a trajectory similar to the one described by this
average. In fact, Ulam’s map contra example shows that by using the ensemble average
we could end up selecting what actually is the less likely state of the system as the
system’s dynamics. Any orbit of this map spend most of the time near the extreme
, while the ensemble average wrongly suggests a zero mode.
points

1, 1

{−

}

3. Ensemble averaging in climate models

To understand why the ensemble average gives less error covariance than a randomly-
chosen single forecast ˆu consider that:

D(ˆu

−

u0)(ˆu

−

u0)T

E = DˆuˆuT + u0uT

ˆuuT

0 −

0 −

u0 ˆuT

E = 2U

(2)

Single Member Selection in Ensemble Forecasting

Since the last two terms in the rhs vanish as t

uncorrelated in time, Du0uT

0 E is the true error covariance U, and DˆuˆuT

→ ∞

as the system becomes
E = U as t
.

→ ∞

If, however, we take ˆu = ¯u = 1

DˆuˆuT

E =

1
m X
i

ui

m Pi ui, then
1
m

t→∞
→

1
m X
j

uj

U

D(ˆu

−

u0)(ˆu

−

u0)T

E

t→∞
→

(

1
m

+ 1)U

Thus,

4

(3)

(4)

The result being that the performance of the ensemble average is due to the 1/m
factor in the average. From Eq. 3 also follows that the larger the number of ensemble
the lower the error covariance.

∈ ℜ

Nonetheless, the same reasoning could be applied to any linear function in the form
˜u = 1
mn Pi ui, n
. These means would be also unbiased estimates of the true state
u0 and also unbiased estimates of the mean of the forecast, decreasing the corresponding
error covariance of the D¯u¯uT
mn factor. These other estimates would provide
even lower covariance than the average, but, as the average, none of them meet the basic
physical requirement of being a possible state of the system. Ensemble average is thus
a statistical way of reducing the covariance, but lacks a physical interpretation.

E term by a 1

An additional argument against ensemble averaging arises from probability theory.
As mentioned, ensemble methods are aimed to provide probabilistic forecasts for future
It is known that global precipitation ﬁelds present a lognormal distribution
climate.
[28]. An ensemble average of precipitation forecasts would then be a weighted sum of
lognormal pdfs. Application of the central limit theorem [29] proves that the sum of
those lognormally-distributed pdfs converge in distribution to the normal (Gaussian)
pdf. (The Lyapunov condition to apply the theorem holds for climate ensembles). The
consequence is that the averaging procedure does not conserve the pdf of the members,
but creates a new pdf with a diﬀerent function. This eﬀect is apparent in climate
ensembles averages, as Figure 3 illustrates. Here, the average of the nine members of
a global precipitation seasonal ensemble [30] is compared with validation data (ERA-
40 reanalysis), and with the objective best member (The objective best/worse member
is deﬁned as that member presenting the best/worse r2 correlation against ERA-40).
The ensemble technique is not intented to provide such daily ﬁelds estimates, but the
ﬁgure is used here just to graphically illustrate what is happening with the pdfs when
averaging.

From ﬁgure 3 it is apparent that the ensemble mean shows a smoother ﬁeld which
is not representative of the original pdf of the members, and does not resemble the
true precipitation ﬁeld (ERA-40). In the ensemble average ﬁgure the high precipitation
rates predicted by the ensemble members in the Paciﬁc disappear, and areas of scarce
precipitation such as California present artiﬁcially high estimates due to the averaging.
The ensemble average provides good statistics in terms of r2 correlation (Table 1,
second column), but the maximum precipitation is severely underestimated (Table 1,

Single Member Selection in Ensemble Forecasting

5

third column). The spatial variance of the ﬁeld is also lower than that for any member, as
consequence of compensating dissimilar estimates for the same geographical area. More
importantly, the ensemble mean does not necessarily reﬂect a feasible state of Earth’s
daily precipitation ﬁeld. Figure 4 shows that the pdf of the ensemble average greatly
diﬀers from the members pdfs, with a severe underestimation of high precipitation
values, and a reduction in light precipitation in favor of moderate precipitation rates.
This normalization of the true pdf makes the averaging procedure unsuitable for
predicting climate extremes.

This is specially visible by comparing the characteristics of the objective best
member (member 5) with the ensemble mean. Member 5 appears as a feasible global
precipitation ﬁeld whereas the ensemble average does not represent any likely state of
the atmosphere. As in Ulam’s map, the average of this ensemble has not other physical
interpretation than being a derived quantity of otherwise physically-consistent estimates.
One alternative to this problem is not averaging the many simulations but the
In this case, the ensemble mean is
statistics derived from the individual members.
simply discarded, and the spread of the ensemble is used as an estimate of the limits of
the future state. The drawback of this procedure, however, is that nothing can be said
on the likely dynamics of the system.

4. Single member selection

Even when ensemble averaging does not provide a physically-consistent trajectory,
ensemble forecasting is a way of tackling SIC in dynamical systems. An alternative
to the average is selecting a single member of the ensemble. The problem is that all
the members of the ensemble are equally-likely (since we do not know the actual initial
conditions), and thus they are indistinguishable from each other from a mathematical
point of view.

There is another viewpoint, however. While in dynamical, mathematical models all
the ensemble members are equiprobable, real physical nonlinear systems have enough
degrees of freedom to adjust itself to those trajectories compatible with the constraints
of the system [31] [32]. We cannot observe such adjustment in Ulam’s map or in other
purely numerical dynamical systems, but we certainly can in physical systems [33] [34]
[35] [36] [37] [38].

This observation prompted [39] to propose a method for ranking members. Further
reﬁnement of that technique resulted in that it is possible to select a single member
reducing as much as possible the error covariance while conserving a dynamically-
consistent forecast, which is the method presented here.

What any selection or blending procedure must ensure is that the result is both
dynamically-consistent and the one surveying the ρ(x) of the system. Ideally, it would
be the less-biased choice. To ﬁnd a less-biased member compatible with the constraints
of the system, let us deﬁne a transient probability density function (pdf) of an ensemble

6

(5)

(6)

Single Member Selection in Ensemble Forecasting

member j in phase space at t as:

pt
j(x) = lim
N→∞

1
N

N

X
i=1

I t
x[x]

where I t
x is the indicator function that gives 1 if the phase-space point of the trajectory
δ, x + δ) at time t, and 0 otherwise. For ergodic maps, the natural
x is in the bin (x
invariant density coincides with this pdf. In the general case, however, the time average
does not have to be equal to the ensemble average and this pdf may depend on the ICs.
We calculate the entropy of the pdf for each ensemble member at resolution δ as:

−

St
j ≡ −

i=max

X
i=min

pt
i,j(x)

log(pt

i,j(x))

·

This entropy characterizes each member of the ensemble at time t. Crucially, the entropy
in Eq. 6 is maximum for the ﬂattest pdf compatible with the dynamics, and minimum for
an hypothetical stationary system which orbit would remain in one bin. The diﬀerence
between this estimate and the variance as pdf estimate is that the variance measures the
concentration of the estimate only around the mean, whereas the entropy measures the
spread of the pdf irrespective of the mode location. Also, a Legendre series expansion
reveals that entropy is related to high-order moments of the pdf, unlike the variance.

If we now considering the system generating the future climate pdf as a nonlinear
physical system subject to a set of constraints, the more likely state of the system
at t is the one with the maximum entropy subject to those constraints [40] [41].
In
terms of the ensemble method, the member having the maximum entropy will display
the most wandering trajectory in phase space since by construction it represents the
broadest distribution compatible with the constraints. For those pdfs similar to the
natural invariant density of Ulam’s map (Figure 1) the maximum entropy member would
1, 1] interval. In other words, the maximum
be the one more evenly surveying the [
entropy member in this case would be the member compatible with the dynamics whose
trajectory more evenly preserves the bimodal pdf of the map.

−

The conclusion is that the maximum entropy ensemble member should be the
member to compare with experiments in climate models, because, as in Statistic
Mechanics, the ensemble average might be the average of many peaks and itself
correspond to an impossible value [42]. It is important to note that since we do not have
evidence to decide for any member in particular all we can do is aiming for the least
worse member, which is the one maximizing the entropy. In other words, the procedure
is not about ﬁnding the best member but about using a method that ensures that we
are committing the least possible error when selecting one.

For unimodal distributions, the maximum entropy member corresponds with the
member with the highest variance: if the pdf is normally distributed, the relationship
log(p(i)) = log(σ√2πe) with σ
− Pi p(i)
between the entropy and the variance is S =
the standard deviation. For the lognormal pdf, S = 1
2log(2πσ2). (This relationships
hold for the whole phase space, so they are not to be confused with the daily results in
Table 1). For non-analytical pdfs, the selection procedure requires the actual calculation

·
2 + 1

Single Member Selection in Ensemble Forecasting

7

of the entropy of the ensemble members as the relationship between entropy estimate
and the variance cannot be analytically described.

As the actual trajectory of the system in phase space will be one in a maximum
entropy state, the member with the highest entropy will be the closest to the true state.
By construction, the error covariance of the maximum entropy member compared with
the true trajectory is automatically the minimum error covariance of the whole ensemble.
To test this hypothesis we have used more cases included the DEMETER project.
Even having this validation data, the issue of how to establish which member of the 180-
days forecast meteorological ensemble is the best is not trivial [43] [44]: the objective
best member may change for one day to the next, without a clear overall winner [45].
t , (r2 being the correlation
r2
To minimize this problem, the quantity
coeﬃcient) can be used as goodness-of-ﬁt metric. We have considered that the best
forecast for day T is not the one in which the forecast compares better with the
experimental data, but the one with the best averaged goodness-of-ﬁt from the ﬁrst
day of the forecast. Thus, the best member is deﬁned as the member that is closest
to the validation in the full state space of the model. Therefore we have computed
max nh

iT o (Eq. 6), and compare with max nh

iT o for each member j.

What we found is that the entropy estimate relates with the best member. Figure
5 shows the evolution of the entropy choice and the best and worse objective members
for a set of ﬁve, 180 days, 9 ensemble-members runs. After the spin-up of the model,
the maximum entropy member is consistently closest to the objective best member than
to the worse member. These results are stronger than those reported in [39] making
unnecessary a weighted average of the members.

T
t=1 r2
T P

iT = 1

Sj

rj

h

2

5. Conclusions

Ensemble procedures provide a way to palliate SIC in nonlinear dynamical systems
in general, and climate models in particular. Averaging of climate ensemble forecast,
however, generates an unphysical blended product, reducing the variance of the estimate
and forecasting a spurious pdf for the future climate. The immediate consequence is
that member-averaged climate forecasts severely underestimate climatological extremes
such as high precipitation rates.

The alternative to averaging is to select a single member of the ensemble that
both preserves the physical character of the forecast and maintains the pdf structure.
In absence of any evidence to select a priori equiprobable member, we should lean
toward the maximum entropy member. This member diﬀers from other members in
representing the most likely trajectory of the system which is compatible with the
constraints. Therefore, it is the least worse choice with the available information. In a
problem subject to SIC such as Earth’s climate modeling, this least assumptive feature is
an important characteristic that climate forecasts should seek to avoid underestimating
climatological extremes.

Single Member Selection in Ensemble Forecasting

8

References

Press)

Review 133 441

2753

127 343

[1] Lorenz E N 1963 Journal of the Atmospheric Sciences 20 130
[2] Ehrendorfer M 1994 Monthly Weather Review 122 703
[3] Whitﬁeld J 2005 Nature 436 905
[4] Trevisan A and Pancotti F 1998 Journal of the Atmospheric Sciences 55 390
[5] Tsonis A A 2001 International Journal of Bifurcation and Chaos 11 881
[6] Orrell D 2005 Journal of the Atmospheric Sciences 62 1652
[7] Buizza R et al Monthly Weather Review 133 1076
[8] Hansen J A 2002 Monthly Weather Review 130, 2373
[9] Kalnay E 2003 Atmospheric modeling, data assimilation and predictability (Cambridge University

[10] Vialard J, Vitart F, Balmaseda M A, Stockdale T N and Anderson D L T 2005 Monthly Weather

[11] Anderson J L 1996 J. of the Atm Sci 53
[12] Sivillo J K, Ahlquist J E and Toth Z 1997 Weather and Forecasting 12 809
[13] Kalnay E, Lord S J and McPherson R D 1998 Bulletin of the American Meteorological Society 79

[14] Trevisan A, Pancotti F and Molteni F 2001 Quarterly Journal of the Royal Meteorological Society

[15] Toth Z, Zhu Y J and Marchok T 2001 Weather and Forecasting 16 463
[16] Houtekamer P L and Lefaivre L 1997 Monthly Weather Review 125 2416
[17] Cheung K K W 2001 Meteorological Applications 8 315
[18] Tracton M S and Kalnay E 1993 Weather and Forecasting 8, 379
[19] Toth Z and Kalnay E 1997 Monthly Weather Review 125 3297
[20] Shukla J 1998 Science 282 728
[21] Pan J F and van den Dool H 1998 Weather and Forecasting 13 983
[22] Cai M, Kalnay E and Toth Z 2003 Journal of Climate 16 40
[23] Ananos G F J and Tsallis C 2004 Physical Review Letters 93
[24] Kong F Y, Droegemeier K K and Hickmon N L 2006 Monthly Weather Review 134 807
[25] Leith C E 1974 Monthly Weather Review 102 409
[26] Beck C and Schlgl F 1993 Thermodynamics of Chaotics Systems, (Cambridge University Press)
[27] Barkmeijer J, Van Gijzen M and Bouttier F S 1998 Quarterly Journal of the Royal Meteorological

[28] Kedem B, Pavlopoulos H, Guan X, and Short D A 1994 Journal of Applied Meteorology 33 1486
[29] Feller W 1971 Probability Theory and its Applications (John Wiley & Sons)
[30] Palmer T N et al 2004 Bulletin of the American Meteorological Society 85 853
[31] Bag B C, Chaudhuri J R and Ray D S 2000 Journal of Physics a-Mathematical and General 33

[32] Breymann W G Tel T and Vollmer J 1996 Physical Review Letters 77 2945
[33] Rebhan E 1990 Physical Review A 42 781
[34] Remler E A 1986 Physics Letters B 173 382
[35] Gerard J C, Delcourt D and Francois L M 1990 Quarterly Journal of the Royal Meteorological

[36] Kleidon A, Fraedrich K, Kunz T and Lunkeit F 2003 Geophysical Research Letters 30
[37] Lorenz R D, Lunine J I, Withers P G and McKay C P Titan 2001 Geophysical Research Letters

Society 124 1695

8331

Society 116 1123

28 415

[38] Dewar R C 2005 Journal of Physics a-Mathematical and General 38 L371
[39] Tapiador F J and Gallardo C 2006 Geophysical Research Letters 33 NUMBER
[40] Dewar R 2003 Journal of Physics a-Mathematical and General 36 631
[41] Ozawa H, Ohmura A, Lorenz R D and Pujol T 2003 Reviews of Geophysics 41

Single Member Selection in Ensemble Forecasting

9

[42] Jaynes E T 1957 Physical Review 106 620
[43] Elmore K L 2005 Weather and Forecasting 20 789
[44] Muller W A, Appenzeller C, Doblas-Reyes F and Liniger M A 2005 J. Clim. 18 1513
[45] Roulston M S and Smith L A 2003 Tellus 55A 16

Single Member Selection in Ensemble Forecasting

10

Figure 1. Ulam’s map trajectories for a typical member of the ensemble (orbit
starting at x0 = 0.3, thin line) and the ensemble average (bold line). The ensemble is
10−8, x0 + 10−8]. After 20 iterations,
composed by 50 members starting within [x0
the trajectories start to diﬀer.

−

Single Member Selection in Ensemble Forecasting

11

Figure 2. The natural invariant density of Ulam’s map

Single Member Selection in Ensemble Forecasting

12

Figure 3.
(1/11) Precipitation forecast for April 4th, 1973, comparing validation
data (ERA-40 reanalysis) with the 9 members of the DEMETER ensemble and with
the ensemble average. The ensemble average provides a slightly better correlation than
the objective best member (which is member 6, cfr. table I) at the cost of providing a
forecast which is incompatible with the dynamics.

Single Member Selection in Ensemble Forecasting

13

Figure 3. cont (2/11)

Single Member Selection in Ensemble Forecasting

14

Figure 3. cont (3/11)

Single Member Selection in Ensemble Forecasting

15

Figure 3. cont (4/11)

Single Member Selection in Ensemble Forecasting

16

Figure 3. cont (5/11)

Single Member Selection in Ensemble Forecasting

17

Figure 3. cont (6/11)

Single Member Selection in Ensemble Forecasting

18

Figure 3. cont (7/11)

Single Member Selection in Ensemble Forecasting

19

Figure 3. cont (8/11)

Single Member Selection in Ensemble Forecasting

20

Figure 3. cont (9/11)

Single Member Selection in Ensemble Forecasting

21

Figure 3. cont (10/11)

Single Member Selection in Ensemble Forecasting

22

Figure 3. cont (11/11)

Single Member Selection in Ensemble Forecasting

23

Figure 4. Histograms of the precipitation estimates in ﬁgure 3. Y-axis in logarithmic
scale, X-axis truncated at 0.03m of precipitation

Single Member Selection in Ensemble Forecasting

24

Figure 5.
(1/5)Performance of the entropy selection method for four 180-days
forecasts (DEMETER data). The maximum entropy choice is consistently close (resp.
further) to the best (resp. worse) objective members

Single Member Selection in Ensemble Forecasting

25

Figure 5. cont (2/5)

Single Member Selection in Ensemble Forecasting

26

Figure 5. cont (3/5)

Single Member Selection in Ensemble Forecasting

27

Figure 5. cont (4/5)

Single Member Selection in Ensemble Forecasting

28

Figure 5. cont (5/5)

Single Member Selection in Ensemble Forecasting

29

Statistics for April 4th, 1973,

Table 1.
the
maximum (mm/day), mean (mm/day) and standard deviation (mm/day) values of the
precipitation forecast ﬁelds. ERA-40 reanalysis data is deemed as the truth (validation
data).

featuring the correlation (r2),

Member 1
Member 2
Member 3
Member 4
Member 5
Member 6
Member 7
Member 8
Member 9
Truth
Ensemble avg

r2
P recip maxP recip µP recip Spatial σP recip
0.1053
0.0949
0.1111
0.1053
0.2075
0.1399
0.0894
0.0911
0.1087
(1.0000)
0.2247

187.069
116.356
147.500
145.808
153.411
193.796
165.628
246.747
139.182
277.449
62.509

2.516
2.354
2.380
2.592
2.194
2.258
2.454
2.324
2.248
2.744
2.369

6.964
6.516
6.742
6.776
5.340
6.280
6.677
6.530
6.371
9.174
3.307


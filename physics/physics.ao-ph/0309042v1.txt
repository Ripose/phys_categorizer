3
0
0
2
 
p
e
S
 
8
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
2
4
0
9
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Moment based methods for ensemble assessment and calibration

Stephen Jewson∗
RMS, London, United Kingdom

February 2, 2008

Abstract

We describe various moment-based ensemble interpretation models for the construction of prob-
abilistic temperature forecasts from ensembles. We apply the methods to one year of medium range
ensemble forecasts and perform in and out of sample testing. Our main conclusion is that proba-
bilistic forecasts derived from the ensemble mean using regression are just as good as those based
on the ensemble mean and the ensemble spread using a more complex calibration algorithm. The
explanation for this seems to be that the predictable component of the variability of the forecast
uncertainty is only a small fraction of the total forecast uncertainty. Users of ensemble temperature
forecasts are advised, until further evidence becomes available, to ignore the ensemble spread and
build probabilistic forecasts based on the ensemble mean alone.

1 Introduction

We consider the question of how to make probabilistic forecasts of site-speciﬁc temperatures. An inter-
esting subset of this problem considers the probabilities of temperature falling into a discrete number of
bins or categories. However, we are interested in the more general question of how to make a prediction
of the continuous distribution of possible temperatures. The continuous distribution case is the case most
appropriate for users who make frequent use of forecasts in continuously changing situations, and who
cannot deﬁne any speciﬁc thresholds or categories of temperature as being important in their application.
Examples include most users of forecasts in the energy and weather derivatives industries. This is not
to say that users of forecasts in these industries do not also, on occasion, use categorical forecasts, but
rather that most commonly they simply want the best possible probabilistic forecast, without further
specialisation.
Jewson et al. (2003a) (henceforth JBZ) propose the use of moment-based methods for the assessment
and calibration of ensemble forecasts within the continuous distribution framework. This paper discusses
these methods in more detail, and compares a number of diﬀerent moment-based algorithms. We then
test the methods using both in and out of sample testing, and draw clear conclusions about their relative
usefulness. We follow the guidelines described by Jewson and Ziehmann (2003), which specify a set of
necessary conditions for a methodology to satisfy if one wants to draw conclusions about the usefulness
of real forecasts in real applications. Finally, we come to some surprising conclusions about the value of
the ensemble spread in making probabilistic forecasts.
This paper is part of a wider research program in which the authors are attempting to understand
the potential uses for meteorological forecasts in the weather derivative industry. Within this context,
moment-based ensemble calibration methods were introduced for the following reasons:

•

•

To provide a simple answer to the question of how many days of forecast are better than climatology,
for both the ensemble mean and the standard deviation, and to give clear statistical tests to help
answer this question.

To create a method for calibration of the ensemble that is both simpler and more accurate than the
rather cumbersome methods currently in use such as the rank histogram. In particular, to derive
a method that optimally merges estimates of the forecast uncertainty based on past forecast error
statistics with those based on the ensemble spread.

∗Correspondence address: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com

2 Data

We will base our analyses on one year of ensemble forecast data for the weather station at London’s
Heathrow airport, WMO number 03772. The forecasts are predictions of the daily average temperature,
and the target days of the forecasts run from 1st January 2002 to 31st December 2002. The forecast was
produced from the ECMWF model (Molteni et al., 1996) and downscaled to the airport location using
a simple interpolation routine prior to our analysis. There are 51 members in the ensemble. We will
compare these forecasts to the quality controlled climate values of daily average temperature for the same
location as reported by the UKMO.
There is no guarantee that the forecast system was held constant throughout this period, and as a result
there is no guarantee that the forecasts are in any sense stationary, quite apart from issues of seasonality.
This is clearly far from ideal with respect to our attempts to build statistical interpretation models on
past forecast data but is, however, unavoidable: this is the data we have to work with.
Throughout this paper all equations and all values have had both the seasonal mean and the seasonal
standard deviation removed. Removing the seasonal standard deviation removes most of the seasonality
in the forecast error statistics, and partly justiﬁes the use of non-seasonal parameters in the statistical
models for temperature that we propose.

3 Moment-based ensemble calibration methods

Perhaps the simplest way to make a reasonable probabilistic forecast of temperature is to build a least-
squares minimising linear regression model between past forecasts and observations. Typically in such
a model one might assume that the forecast errors follow a normal distribution and are uncorrelated in
time. The model for temperature Ti on day i can then be written as:

N (ˆα + ˆβmi, ˆγ)

Ti

∼

(1)

where mi is the forecast and ˆα, ˆβ, ˆγ are constants to be determined as part of the calibration. The
reason we have used the symbol γ rather than the more usual σ will become apparent below. In the
ensemble forecast case mi would be the ensemble mean. Least squares linear regression provides analytical
expressions for the parameters ˆα, ˆβ, ˆγ in terms of the data (see Press et al. (1992)). The regression-based
ensemble calibration model says that the ensemble mean mi is converted into an estimate for the mean
of the forecast distribution ˆµ using the linear transformation ˆα + ˆβmi which both corrects a bias in the
forecast and also damps the forecast towards climatology in such a way as to minimise root mean square
error.
The use of least squares minimisation for the ﬁtting of the parameters in the regression model is a special
case of the use of the more general maximum likelihood method of Fisher (1922). Maximum likelihood
is the most common parameter ﬁtting method used in statistics (see, for example, Casella and Berger
(2002)). It works as follows:

The likelihood is deﬁned as the probability density of the entire observed data set given the forecasts
and a particular set of parameters for the model. This probability is a single number.

The parameters are then varied. This changes the likelihood.

The optimum parameter values are chosen to be those that maximise the likelihood, i.e. maximise
the probability density of the data given the parameters and the forecast.

The likelihood is a natural way to deﬁne the goodness of ﬁt between a model and a data set. 1 However,
it is also much more: it can be shown that, for a very large class of models that includes most reasonable
models (and certainly all the models we will consider below) use of the maximum likelihood method gives
the most accurate possible estimates for the parameters.
In the case of equation 1 the likelihood can be derived from the probability density function for the
multivariate normal distribution, and is given by:

L =

1
√2πdet

exp(

1
2

−

(T

−

ˆµ)

T ˆΣ

−1

(T

ˆµ))

−

(2)

1We note that our use of the word likelihood follows the original deﬁnition from classical statistics. Occassionally, and
rather confusingly, the word has been used in a slightly diﬀerent sense in meteorology. We discuss this question in Jewson
(2003a)

•

•

•

where ˆΣ is the estimated covariance matrix of the forecast errors, T is the vector of observations, ˆµ =
ˆα+ ˆβm is the vector of forecast temperatures and det is the determinant of ˆΣ. We see that L is a function
of both the observed data (via T ), the forecasts (via ˆµ, ˆΣ and det) and the parameters of the model (also
via ˆµ, Σ and det). In practice it is often more convenient to consider the log-likelihood rather than the
likelihood. Since the log function is monotonically increasing, ﬁnding the parameters that maximise one
is equivalent to ﬁnding the parameters that maximise the other. The log-likelihood in this case is given
by:

1
2
If we make the assumption that the forecast errors are uncorrelated in time then ˆΣ is diagonal and this
simpliﬁes to:

ln(2πdet)

T ˆΣ

l =

(3)

1
2

ˆµ)

ˆµ)

(T

(T

−

−

−

−

−1

i=n

X
i=1

1
2

l =

−

ln(2πˆσ)

i=n

X
i=1

1
2

−

(Ti

ˆµi)2

−
ˆσ2

(4)

(5)

(6)

(7)

The values of ˆα, ˆβ, ˆσ that maximise this expression can be derived analytically, and yield the expressions
that are used to give the parameters in least-squares linear regression. Expressions for the sampling un-
certainty can also be derived analytically since it can be shown that the approximate sampling uncertainty
on the parameters is given by the inverse of the matrix of second derivatives of the log-likelihood.
The biggest shortcoming of the regression model as a method for creating probabilistic forecasts is that
it cannot predict ﬂow-dependent changes in the uncertainty, since the uncertainty ˆγ is a constant. It
is well established that the ensemble spread contains information about this uncertainty and it would
seem sensible to extend the regression model to attempt to incorporate some of this information. We will
present three such extensions. The ﬁrst simply uses the ensemble spread si to predict the uncertainty,
rather than deriving the uncertainty estimate from past forecasts. This is now a two-parameter model,
which we write as:

N (ˆα + ˆβmi, si)

Ti

∼

N (ˆα + ˆβmi, ˆδsi)

Ti

∼

We will call this the spread only model.
The third model assumes that the ensemble spread is a good predictor of the uncertainty, but that it
may have the wrong amplitude, and so we add a parameter to scale it. This is a three-parameter model
and is written as:

We will call this the spread-scaling model. This model suﬀers from what we call the ”spread-inﬂation
problem”, which is that the mean level of the spread and the variability of the spread are scaled by the
same factor. This is clearly not the best way to optimise forecasts. To take a limiting case as illustration:
if the variability in the spread is meaningless then we should set it to zero, but we should certainly
not set the mean spread to zero. The spread-inﬂation problem is also present in the rank histogram and
best-member (Roulston and Smith, 2003) ensemble calibration methods, which is why we do not consider
them in our analysis.
Our fourth model is the spread-regression model from JBZ, which overcomes the spread-inﬂation problem
at the cost of introducing a fourth parameter:

N (ˆα + ˆβmi, ˆγ + ˆδsi)

Ti

∼

In this model, if the ensemble spread contains no information then ˆδ can be zero and the uncertainty
is modelled entirely on the basis of past forecast error statistics as in equation 1. This idea that the
spread might contain no information is not just a thought experiment: Jewson et al. (2003b) give some
examples of ensembles in which it appears that there is no skill in the spread, and for which ˆδ is thus
zero.
The ˆγ and ˆδ parameters in the spread-regression model can be interpreted as follows. If ˆδ is not sig-
niﬁcantly diﬀerent from one then the variability in the ensemble spread has the correct amplitude and
does not need scaling. If ˆδ is not signiﬁcantly diﬀerent from zero then the variability in the spread has
no information in the context of this interpretation model, and should be ignored. This was the case for
long leads in the forecasts discussed by Jewson et al. (2003b).
Forecasts derived from general circulation models contain no useful information at all about either the
average site-speciﬁc temperature or the average level of uncertainty of forecasts of that temperature, since

(8)

(9)

(10)

(11)

COVSbefore =

sd(s)
s

COVSafter =

ˆδsd(s)
ˆγ + ˆδs

DS ratio =

s
sd(m)

both quantities are highly aﬀected by local conditions not represented in the model. The purpose of the
coeﬃcients ˆα and ˆγ is to use site-speciﬁc observational data to set these averages levels of temperature and
uncertainty correctly. What general circulation models do potentially represent well are the ﬂuctuations
in temperature and the ﬂuctuations in uncertainty of the forecast: these are captured by the ˆβ and ˆδ
terms.
There are various other values that can be derived from the parameters in equation 7 that give further
insight into the performance of the ensemble. One is the ratio of the mean spread after calibration to the
mean spread before calibration. This is given by:

MSR = mean spread ratio =

ˆγ + ˆδs
s

and tells us to what extent the ensemble underestimates the average level of the spread. Another useful
diagnostic is the coeﬃcient of variance of spread before and after calibration, given by:

for the uncalibrated ensemble and

for the calibrated ensemble. The COVS tells us the ratio of the size of the variations in the spread to the
mean spread.
The third is the deterministic-stochastic, or ds, ratio. This is deﬁned as:

This ratio varies from zero for a perfectly deterministic forecast with no uncertainty, to inﬁnity for a
perfectly stochastic forecast with no variability in the prediction of the mean. This ratio thus tells us to
what extent the forecasts are closer to the deterministic limit or the stochastic limit.

3.1 Skill scores

In the context of the regression model the most useful skill score is the root-mean-square-error (RMSE),
which measures the typical sizes of errors. If the forecast errors really are normally distributed, and the
forecasts have been calibrated using equation 1, then the RMSE tells us everything we could ever need
to know about the skill of the forecast.
However, the main diﬀerence between the four models discussed above is in their ability to predict the
uncertainty, and the RMSE does not take that into account. Rather, we need a measure that considers the
ability to predict the whole distribution of possible outcomes. JBZ propose the likelihood, or measures
derived from the likelihood, as an appropriate way to make this comparison. Advantages of the likelihood
are that it avoids having to make arbitrary categories, it incorporates all of the observed data and it
evaluates over the whole range of the forecast. The advantages of using the likelihood as a skill measure
are discussed in more detail in Jewson (2003b).
There are a number of ways the likelihood can be presented such as actual likelihood values, the log-
likelihood, the root of minus the mean of the log-likelihood (RMMLL) or as the likelihood skill score. We
will use the RMMLL.
Since the spread-regression model has four parameters, it is almost guaranteed to perform better than the
other models in in-sample data ﬁtting tests. But this does not mean it is a better model. One way around
this problem is to adjust the in-sample likelihood to penalise each model for the number of parameters
used with a factor of 1
2 Qln(N ) where Q is the number of parameters and N is the number of data points
(this is known as the Bayesian information criterion, or BIC). We will use the BIC adjustment in the
RMMLL scores presented in section 4. Alternatively we can use out of sample testing. We will use a
combination of in-sample and out of sample testing in our model comparisons below.

3.2 Fitting the parameters

We ﬁt our four models under the assumption that the forecast errors are uncorrelated in time, which means
we can calculate the log-likelihood using equation 4. We will see later, however, that this assumption is

not exactly correct, and hence in principle one should use the log-likelihood given by equation 3 to ﬁt
all the models. However, in practice using equation 3 is somewhat awkward since the covariance matrix
of the forecast errors Σ is not known in advance. Developing algorithms to ﬁt the four models using
equation 3 is an area for further work.
The parameters in the regression, spread-only and spread-scaling models (equations 1, 5 and 6) can be
ﬁtted using analytic expressions that can be derived by diﬀerentiating equation 4 with respect to the
parameters, setting the derivatives equal to zero, and solving with respect to the parameters. This does
not appear to be possible for equation 7, however, because the resulting equations are not analytically
tractable. Instead, numerical optimisation routines must be used. The simplest eﬀective optimisation
routines are the so-called quasi-Newton schemes (see Press et al. (1992)). These are simple to use because
they only involve the user having to make calculations of the log-likelihood, and not its derivatives.
Although deriving analytic expressions for the gradient and curvature is easy for equation 4, in general
it can be very diﬃcult (for example for equation 3).
We have found that quasi-Newton schemes are perfectly adequate for calculation of the parameters in
equation 7 and converge within a small number of steps to the maximum value. This takes only a fraction
of a second on even the slowest personal computer.

3.3 Extensions of the spread-regression model

One could criticize our four models for assuming that the forecast errors are normally distributed. In
fact, they can easily be extended to cope with other distributions. There are two ways in which this
could be done. First, one could apply linear transformations to the parameters of a diﬀerent distribution.
This might be appropriate if trying to calibrate a gamma distribution ﬁtted to precipitation, for instance.
Secondly, one could still apply linear transforms to the mean and the standard deviation, but then ﬁt a
distribution other than the normal, such as a kernel density. In that case, the model would have a ﬁfth
parameter for the bandwidth of the kernel. Such a model could again be ﬁtted using maximum likelihood,
and may have the beneﬁt that it would allow a more ﬂexible range of distribution shapes than the normal
distribution. Testing whether such a model really would give beneﬁts over the normal distribution is an
area of current research.

4 Model testing and validation

We will now investigate some of the statistical characteristics of our forecast data, and compare the
performance of the four models described above.

4.1 Characteristics of the ensemble forecasts

We ﬁrst look brieﬂy at some of the characteristics of the variability of the mean and the standard deviation
of the ECMWF ensemble forecast.
The top left panel in ﬁgure 1 shows the variability in the ensemble mean, measured by the standard
deviation over time, versus lead time. We see that the ensemble mean varies less at longer leads. One
way this can be understood is that at short lead times the ensemble is narrow and the ensemble mean
can be positioned almost anywhere within the climatological range of the model forecast. At longer lead
times the ensemble is wider, but still has to lie within roughly the same climatological range. The range
of values the mean can take is therefore reduced.
The upper right panel in ﬁgure 1 shows the mean of the ensemble spread versus lead time. We see that
the ensemble has low spread at short lead times, and that the spread grows with lead time. The lower
left panel shows the standard deviation of the ensemble spread versus lead time i.e. the variations of
the ensemble spread about the mean. This is smallest at short lead times, shows a maximum, and then
reduces at longer lead times. At very long lead times (longer than these shown here) we would expect it
to tend towards a small positive value, determined by sampling variability.
Finally, the bottom right panel in ﬁgure 1 shows the DS ratio versus lead time. Curiously, this ratio
increases almost linearly. At short lead times we see that the ensemble spread is tiny relative to the
variations in the ensemble mean, while at lead times of 8-10 days the standard deviation of the ensemble
is roughly equal to the standard deviation of the ensemble mean.

4.2 Parameter values and uncertainty

We now show the parameters of the four models when ﬁtted to our ensemble data. The four values of ˆα
and ˆβ are shown in the upper two panels of ﬁgure 2. The regression and spread-regression models give
eﬀectively the same parameter values, while the spread-only and spread-scaling models give eﬀectively
the same values. The latter pair of models give slightly lower values of ˆα than the former especially at
short leads.
The two values of ˆγ, for the regression and spread-regression models, are shown in the bottom left panel
of the ﬁgure. In the regression model ˆγ increases roughly linearly with lead time. The values of ˆγ from
the spread-regression model are somewhat lower than the values from the regression model, as some of
the spread has been taken up by the ˆδsi term.
The two values of ˆδ, for the spread-scaling and spread-regression models, are shown in the bottom right
panel of the ﬁgure. In the spread scaling model ˆδ starts oﬀ very high and then reduces to values slightly
above one. In the spread-regression model, which does not have to represent the whole spread using this
term, the values of ˆδ are much lower.
We now show the four parameters of the spread-regression model but with conﬁdence intervals. These
parameter values have already been shown in Jewson et al. (2003a), but we repeat them here for com-
pleteness, and because they tell us a lot about the general behaviour of the ensemble in relation to the
observations. Figure 3 shows the four parameters, conﬁdence intervals representing estimated sampling
uncertainty, and the constant levels zero and one. The conﬁdence intervals are derived in the standard
way from the Fisher information. For the ﬁrst two graphs, the conﬁdence levels are so narrow that they
are impossible to see. The values of ˆα are signiﬁcantly diﬀerent from zero, showing that there is a bias in
the forecast, the size of which is given by ˆα + ˆβm. The values of ˆβ show that the uncalibrated ensemble
mean varies in time too much to be an optimum forecast. The ensemble mean needs damping towards
zero. An inﬁnite size perfect ensemble would not need such damping: presumably the reason it is needed
here is that the ensemble is not inﬁnite size, and the ensemble members are more correlated with each
than they are with reality.
The parameters ˆγ and ˆδ show greater uncertainty than the parameters ˆα and ˆβ, presumably because they
are associated with the second moment of the distribution of temperatures rather than the ﬁrst. Taking
ˆδ ﬁrst, we see that the variability in the ensemble standard deviation (the size of which was shown in
ﬁgure 1, bottom left panel) is too large for the forecast to be optimal beyond lead one. The factor by
which the variability of the spread needs to be reduced lies at around 0.6 up to lead nine. The fact that ˆδ
is signiﬁcantly diﬀerent from zero at all lead times is a strong indication that the ensemble spread contains
real information about the uncertainty around the ensemble mean. This need not necessarily be the case
and raises the hope that the ensemble spread can be used to predict the uncertainty, and hence improve
the probabilistic forecasts that can be made using the ensemble. ˆγ is signiﬁcantly diﬀerent from zero at
all leads, and increases with lead. This suggests that, within this model, the ensemble spread cannot
be used on its own to predict forecast uncertainty, and a better prediction (in the likelihood maximising
sense) would be achieved by using a constant oﬀset too.
We cannot compare the sizes of ˆγ and ˆδ in order to get an idea of the relative importance of the terms
ˆγ and ˆδs.
Instead we need to compare the ratios described in section 3. These were also shown in
JBZ but are again repeated here because of the important insight they give to the behaviour of the
ensemble. The MSR (upper panel of ﬁgure 4) shows that the ensemble spread is much lower than the
actual forecast uncertainty at leads one and two. Combining this with the observation that the values of
ˆδ in the spread-regression model are close to one, we see that the variability in the spread is more or less
correct, but that a large constant oﬀset is needed to increase the mean spread up to realistic levels. Were
the constant oﬀset term ˆγ not included in the calibration model then the mean and the variability of the
ensemble spread would have to be adjusted using the same scaling. This would increase the mean spread
to realistic levels, but would also increase the variability to unrealistic levels. This is apparently what
happens in the spread-scaling model, and we believe this may also be what happens in the rank-histogram
and best-member calibration methods.
At longer leads, the average size of the ensemble spread is more realistic, but the values of ˆδ from
the spread-regression model show us that the variability of the spread is too large to be optimal. The
calibration thus replaces much of the ensemble spread with a constant oﬀset ˆγ at these leads.
The second ratio (lower panel of ﬁgure 4) measures the size of the variability of the ensemble spread
before and after calibration. The horizontal line indicates the value of the COVS that would be expected
simply from sampling variability in an ensemble of this size. Before calibration the ensemble spread shows
high levels of variability, with a COVS of between 20% and 50%, well above sampling variability. If this
variability were realistic, then one might conclude that predictions of the ﬂow dependent uncertainty

would be very important in making good probabilistic forecasts. However, after calibration the COVS
is much lower, and scarcely above the level of sampling variability. We conclude that the optimised
uncertainty forecast does not show very much ﬂow dependence.
The low values of the post-calibration COVS are a ﬁrst indication that the information in the ensemble
spread is unlikely to improve our probabilistic forecasts a great deal, since the variability in the predictable
part of the spread is only a small fraction of the mean spread.
There are two possible reasons for the lower values of post-calibration COVS. First, it could be that the
variability in the ensemble spread is very highly correlated with the variability in the uncertainty, but the
amplitude of the variability in the spread is too large and needs reducing by the factor ˆδ from the spread
regression model to correct it. Secondly, it could be that the amplitude of the variability in the ensemble
spread is about right, but the correlation between the ensemble spread and the actual uncertainty is
rather low. Reality is probably somewhere between these two extremes. Without a reliable estimator
for the linear correlation between the spread and the uncertainty it is rather hard to distinguish between
these two cases. Developing such an estimator is a work in progress. However, given the ad-hoc methods
used to create the ensemble, and the problems with the size of the ensemble spread at short leads as
shown by the MSR, we suspect that the latter case is more likely i.e. that the variations in the ensemble
spread probably have a very low correlation with the actual variations in the uncertainty. This is the
optimistic view since if this is true then there is a great opportunity for further increases in the skill of
ensemble forecasts by increasing our ability to predict ﬂow-dependent uncertainty, perhaps by improving
the methods for generating ensembles.
Finally for this section, we illustrate the eﬀects of the parameters of the spread-regression model discussed
above by showing a sample of the ensemble mean and spread before and after calibration. Figure 5 shows
results for the mean at lead one (upper left), the spread at lead one (upper right), the mean at lead ﬁve
(lower left) and the spread at lead ﬁve (lower right). We see that at neither lead does the calibration
of the mean make much diﬀerence to the forecast. At lead one, however, the calibration of the spread
is very dramatic, with an increase in the mean level, and a decrease in the variability. At lead ﬁve the
eﬀect of the spread calibration is much smaller.

4.3 Model consistency tests

We now address the question of whether the residuals from the spread-regression model are consistent
with the model assumptions.
In particular, we check whether the forecast errors are uncorrelated in
time and whether they are normally distributed. The upper panel of ﬁgure 6 shows the autocorrelation
function of the forecast errors at three diﬀerent leads. The results clearly show that the residuals are
not uncorrelated in time, but rather show persistent weak positive autocorrelations out to lead ten. This
highlights a deﬁnite short-coming of all the models we discuss in this paper, since they have all been ﬁtted
under the assumption that the forecast errors are uncorrelated in time (as have, to our knowledge, all
other ensemble forecast calibration models described in the literature). Extending the model to overcome
this deﬁciency by modelling ˆΣ is not entirely trivial, and is beyond the scope of this paper. It is, however,
a high priority for future work.
The lower panel of ﬁgure 6 shows QQ plots for the residuals versus a normal distribution. It can be seen
that the residuals are in fact very close to normally distributed. In this respect, it would seem that our
models are quite adequate.

4.4 In sample robustness and skill measures

We now consider various in-sample tests of robustness and skill. Although out of sample skill measures
give us a better indication of the likely level of skill of future forecasts, in sample skill measures can
be very useful for distinguishing between diﬀerent calibration models, as we shall see, and for setting
an upper limit on the levels of predictability that we might achieve in real forecasts. In particular, if a
complex calibration model does not beat a simple calibration model in in-sample tests, it is unlikely to
beat it in out of sample tests, or in a real forecasting situation.

4.4.1 Robustness tests

First, we estimate the parameters of the spread-regression model using the ﬁrst and second halves of the
data set separately. Figure 7 shows the four parameters estimated using the whole year of data (solid
line), the ﬁrst half of the data (dashed line) and the second half of the data (dotted line). The results
for the ﬁrst and second halves of the data are somewhat complementary. For instance, if the whole data
shows a certain bias, and the ﬁrst half shows a higher bias, then the second half must show a lower

bias. Overall we see reasonable consistency between the parameters estimated on the ﬁrst and second
halves of the data. For instance, alpha is always greater than zero, beta is less than one up to lead ﬁve,
gamma is always positive, and delta is always less than one but greater than zero. Perhaps surprisingly
the diﬀerences between the values for the ﬁrst and second halves lie well outside the range of sampling
uncertainty indicated by our uncertainty estimates. This is partly because the sampling uncertainty is
greater when only using six months of data, but this cannot explain all the discrepancy. Other possible
explanations are that the log-likelihood is not close to parabolic at the maximum, or that the system is
not non-stationary. Certainly this last reason is very plausible since we are comparing diﬀerent times of
year, and also because, as discussed in section 2, the forecasting system itself is not held constant in time.

4.4.2 Skill of the mean

We now measure the in-sample skill of our four forecasts. First, we measure the skill of the mean ˆµ
using the root mean square error (RMSE). Figure 8 shows the RMSE values for all four models and
climatology. The roughly horizontal line is climatology. Three of the models (regression, spread-scaling
and spread-regression) are almost exactly coincident and cannot be distinguished from each other. The
fourth model (spread-only) does a bit worse than the other models.
If all we care about is the skill of the forecast of the mean temperature, then it does not appear to matter
which of regression, spread-scaling or spread-regression we use, and since regression is the simplest as
it does need the ensemble spread as an input, then we should use that. However, if we care about the
ability of the forecast to predict the whole distribution, then we need to go beyond the RMSE and look
at whole-distribution measures of skill such as the likelihood in order to distinguish which is the best
forecast.

4.4.3 Probabilistic skill

Figure 9 shows the skill of the calibrated ensemble using root mean minus log likelihood (RMMLL). The
solid horizontal line shows the RMMLL for climatology. The sloped solid line shows the RMMLL for the
regression model. We see that the regression model gives better probabilistic forecasts than climatology
at all lead times.
The dashed line shows the RMMLL for the spread only model. The results for this model are terrible:
at leads one and two the model is even worse than climatology. Since the RMSE for this model is nearly
as good as regression at these leads, we can see that this must be almost entirely because the ensemble
spread is an extremely poor prediction for the actual uncertainty. This was perhaps to be expected
given that we have already seen that the ensemble spread at leads one and two is much smaller than
the uncertainty at those leads (ﬁgure 4, upper panel). At longer lead times this model improves, but is
always distinctly worse than the regression model. We conclude that the ensemble spread alone is not at
all a suitable model for the uncertainty in the forecast, and from this point on will we not consider this
model further.
The dotted line shows results for the spread-scaling model. This model is always better than climatology,
but does a lot worse than regression at lead one, and slightly worse than regression at all other leads,
except perhaps leads six and nine, where the two models are indistinguishable. As a result, we will also
reject this model from here on since it does worse overall than a simpler model.
Finally the dot-dash line shows the results for the spread-regression model. This model gives better
results than regression at all lead times, and is always the best of the models. However, it is striking
that the results are only very marginally better than the regression results. One might have hoped that
the inclusion of the ﬂow dependent spread via the ˆδs term would give a major improvement in the skill
of the probabilistic forecast, but this does not appear to be the case.
The explanation for this poor relative performance of the spread-regression model presumably lies in the
small size of the ﬂow dependent changes, as shown in ﬁgure 4, or in the incorrect model assumption
that the residuals are uncorrelated in time. It seems unlikely that the second of these is likely to be the
main cause of the poor relative performance, since this assumption aﬀects the spread-regression and the
regression models equally. We therefore tentatively conclude that the predictable part of the changes in
uncertainty are so small that predicting them does not make a major diﬀerence to the in-sample skill of
the probabilistic forecast.

4.5 Out of sample tests

We now move on to out of sample tests. In-sample tests can only give a true indication of the likely level
of predictability if the system is perfectly stationary, and this is never the case in practice because of

changes in the observing system and forecast model, and because of seasonality. We have already shown
that either the regression model or the spread-regression model are the best of the models considered, and
that if the spread-regression model is better, it is only marginally better. The two particular questions
that remain to be answered are a)how well does the regression model perform versus climatology out of
sample and b)does the spread-regression model maintain its (albeit small) advantage over the regression
model. Figure 10 shows the RMMLL for calibrated predictions of the second half of the year, where
the calibrating parameters were ﬁtted on the ﬁrst half of the year (upper panel), and the same for
predictions of the ﬁrst half of the year made using parameters based on the second half of the year.
We feel that splitting that period into two halves like this gives a fairer indication of the levels of likely
predictability than using leave one out cross-validation (even if whole months are left out) because of
the long-memory characteristic of observed temperatures as documented by Caballero et al. (2002). We
have added conﬁdence intervals onto the RMMLL to give some indication of the range of possible values
that the RMMLL might take with a diﬀerent sample. However, comparison of the two panels, and the
fact that the values in the second panel do not lie within the conﬁdence intervals of the ﬁrst panel,
suggests that these conﬁdence intervals are too narrow. As usual, the conﬁdence intervals are based on
the assumption of stationarity of the underlying time series, and this assumption is probably wrong.
The results themselves show that the calibrated probabilistic forecasts contain useful predictability up to
lead six. The results for the regression and the spread-regression models are virtually indistinguishable,
and the spread-regression is certainly not signiﬁcantly better. The tiny advantage seen in the in-sample
tests is lost when we move to out of sample tests: presumably this is due to non-stationarity and sampling
error. It would seem that a much larger training sample would be needed for the small signal to be
detectable.

5 Conclusions

We have addressed a number of interlinked questions to do with the calibration, assessment and pre-
dictability of site-speciﬁc temperatures using medium range ensemble forecasts. In particular, we have
used a framework which converts an ensemble forecast into probabilistic forecast using a simple moment-
based calibration model. The calibration model is ﬁtted using the likelihood, and the ability of the
probabilistic forecast to capture the behaviour of observed temperatures across the whole of the distri-
bution of possible values is also measured using the likelihood.
Within this framework we have shown that it is possible to detect a correlation between spread and skill,
and that this correlation is robust to splitting the data-set in two. However, the size of the predictable
variability in the uncertainty is small relative to the mean uncertainty.
We have compared a number of speciﬁc calibration models within the moment-based calibration frame-
work. Our results show very clearly that calibrating the ensemble mean but not calibrating the ensemble
spread is a disaster, and at some leads gives a worse probabilistic forecast than climatology itself. Calibrat-
ing the ensemble spread with a single scaling parameter works better, but does not produce probabilistic
forecasts as good as those produced by ignoring the ensemble spread completely and using linear regres-
sion. The only model that compares favourably with linear regression is the spread-regression model in
which the predicted uncertainty is a linear function of the ensemble spread, including an oﬀset. This
model performs marginally better than linear regression using in-sample tests, but the diﬀerence is tiny.
In out of sample tests the two models perform equally well. Certainly it does not seem to be the case
that using the extra information available in the ensemble spread improves the out of sample forecasts.
This would imply that, even though the ensemble spread does contain information about the uncertainty,
the amount of information is not suﬃcient to be able to improve a probabilistic forecast based on simple
regression.
This conclusion suggests that, given the present forecasts systems, and for this location, users of site-
speciﬁc temperature forecasts who already buy an RMSE-minimising single forecast based on an ensemble
mean have no reason to buy ensemble forecasts in addition. The value of the ensemble forecast, in this
case, seems to lie in the mean rather than the spread. If they desire probabilistic forecasts then users
can construct them using past forecast error statistics and these probabilistic forecasts will be as good
as anything based on the ensemble.
One important lesson we can apparently draw from this study is that a signiﬁcant relationship between
ensemble spread and skill does not guarantee we can make better probabilistic forecasts by using the
spread. There is many a slip between cup and lip: the spread still has to be calibrated before it can be
used in a forecast, and performing this calibration is diﬃcult and adds additional uncertainty. We have
seen that prima-facie reasonable methods of doing such a calibration, such as the spread-only method,

can lead to worse results than simple regression, which ignores the spread entirely. The only real test
of whether spread-skill relations are of any practical use is to actually produce the ﬁnal probabilistic
forecast with and without the spread term included, and compare the two in out of sample tests.
There are a number of avenues of future work. Before we ﬁnally conclude that the ensemble spread is
not a useful predictor of uncertainty it is important to explore some of the limitations of this study. One
major issue is that we have only used 12 months of ensemble data to train our probabilistic forecasts,
and only 6 months in our out of sample tests. Clearly it would be preferable to have much more training
data, as long as the data were stationary. It is not clear, however, whether, in practice, current forecast
systems are held suﬃciently constant for suﬃciently long that more data would be better. In fact, it
might even be better to use less. This depends on the size and frequency of changes to the forecasting
system, which are unknown (at least to users of the forecasts). We suspect that the lack of robustness
of the parameters in the spread-regression model between the ﬁrst and second halves of the year is as
much to do with changes in the forecasting system during the second half of the year as it is to do with
seasonality. To solve this problem of non-stationarity it is the authors’ belief that ECMWF should run
two forecasting systems in parallel, and only change one at a time. Such a system could be used to ensure
that there is always at least 1 year, say, of forecasts from a stationary forecast system. Related to this,
it would be useful to establish more clearly the relationship between the length of training data available
and the accuracy achieved in ﬁtting the parameters, and whether it is better to ﬁt the parameters on
annual or seasonal data (this very possibly varies from location to location).
Our analysis made the assumption that the forecast errors are uncorrelated in time, which we have
seen to be untrue. There is a chance that correcting this will improve the skill of the probabilistic
forecast based on spread-regression to a greater extent than it will improve the regression based forecast.
Another assumption was that temperature is normally distributed. Tests of the residuals did not seem to
contradict this assumption, and so we do not believe that this is a big problem for this particular station.
However, other stations are certainly much less normally distributed than Heathrow: see, for example,
the analysis of Miami temperatures in Jewson and Caballero (2002). In such cases extending the spread-
regression model to non-normal distributions is a priority: we have already discussed some ways that this
might be done in section 3.3. It is also of some interest to investigate entirely non-parametric methods,
perhaps broadly similar to the rank-histogram method, that nevertheless do not suﬀer the spread inﬂation
problem. We are in the process of testing such a method.
it would be very valuable to attempt a similar analysis for
Our study has focused on temperature:
precipitation forecasts, in terms of using maximum likelihood methods to ﬁt calibration models, using
the likelihood to compare models, and assessing the role of spread in improving forecasts. It may well be
that ensemble spread is more useful when predicting the distribution of possible precipitation than it is
when predicting the distribution of possible temperatures.
The question of whether major improvements are possible in the prediction of uncertainty remains unan-
swered. As discussed above, it may be that the actual uncertainty varies to a much greater degree that
our optimised forecasts of uncertainty, in which case a lot of further improvement would seem to be
possible. We are in the process of developing stochastic analogues of the forecast system which we believe
will shed light on this question.
Finally, we repeat our main conclusion. We believe in the prudent use of forecasts, by which we mean
that users of forecasts should not use a forecast until it has been proven to be better than other simpler
alternatives. We have not been able to show that using the ensemble spread yields better forecasts than
using regression on the ensemble mean alone, and so we have to advocate that the ensemble spread should
not, at this point, be used for making probabilistic forecasts. However, we would be very happy to see
this result refuted, either using the moment-based calibration framework, or any other.

6 Acknowledgements

The author would like to acknowledge Ken Mylne for providing the ECMWF forecasts used in this study,
Risk Management Solutions for providing us with the observational data, Anders Brix for reformatting
the forecasts and David Anderson, Anders Brix, Francisco Doblas-Reyes, Beth Ebert, Renate Hagedorn
and Christine Ziehmann for useful discussions on the topic of forecast calibration and validation. This
research was funded by the author.

References

R Caballero, S Jewson, and A Brix. Long memory in surface air temperature: Detection, modelling and

application to weather derivative valuation. Climate Research, 21:127–140, 2002.

G Casella and R L Berger. Statistical Inference. Duxbury, 2002.

R Fisher. On the mathematical foundations of statistics. Philosophical Transactions of the Royal Society,

A, 222:309–368, 1922.

S Jewson. A note on the use of the word ’likelihood’ in statistics and meteorology. Arxiv, 2003a.

S Jewson. Use of the likelihood for measuring the skill of probabilistic forecasts. Arxiv, 2003b.

S Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble

temperature forecasts. ASL, 2003a. Submitted.

S Jewson and R Caballero. Seasonality in the dynamics of surface air temperature and the pricing of

weather derivatives. Journal of Applied Meteorology, 2002. Submitted.

S Jewson, F Doblas-Reyes, and R Hagedorn. The assessment and calibration of ensemble seasonal
forecasts of equatorial paciﬁc ocean temperature and the predictability of uncertainty. ASL, 2003b.
Submitted.

S Jewson and C Ziehmann. Five guidelines for the evaluation of site-speciﬁc medium range probabilistic

temperature forecasts. Arxiv, 2003.

F Molteni, R Buizza, T Palmer, and T Petroliagis. The ECMWF ensemble prediction system: Method-

ology and validation. Q. J. R. Meteorol. Soc., 122:73–119, 1996.

W Press, S Teukolsky, W Vetterling, and B Flannery. Numerical Recipes. Cambridge University Press,

M Roulston and L Smith. Combining dynamical and statistical ensembles. Tellus A, 55:16–30, 2003.

1992.

7 Figures

2

4

6

8

10

2

4

6

8

10

lead

lead

n
a
e
m
 
s
n
e

 
f

 

o
d
s

d
s
 
s
n
e
 
f
o
 
d
s

2

.

1

9
0

.

6

.

0

0
2

.

0

0
1
.
0

0
0
.
0

d
s
 
s
n
e

 
f

o

 

n
a
e
m

8

.

0

4

.

0

0

.

0

o
i
t
a
r
 
s
d

0
.
1

0
.
0

2

4

6

8

10

2

4

6

8

10

lead

lead

Figure 1: Diagnostics of the ensemble forecast data used in this study, all for double anomalies, and
all versus lead. Panel a)shows the standard deviation of the ensemble mean, b)shows the mean of the
ensemble standard deviation, c)shows the standard deviation of the ensemble standard deviation and
d)shows the ratio of b) to a), which is a measure of the extent to which the ensemble is close to being
like a pure deterministic system (low values) or a pure stochastic system (high values).

2

4

6

8

10

2

4

6

8

10

lead

lead

a
h
p
a

l

a
m
m
a
g

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

t

a
e
b

a
t
l
e
d

2

.

1

6
0

.

0

.

0

4

3

2

1

0

2

4

6

8

10

2

4

6

8

10

lead

lead

Figure 2: Parameter values for the four models discussed in the text. Panels a) and b) show values for
the parameters alpha and beta for all four models. Panel c) shows values of the parameter gamma for
the spread-only and spread-regression models, and panel d) shows values of the parameter delta for the
spread-scaling and spread regression models. In panels a) and b) regression is given by the solid line,
spread-only by the dashed line, spread-scaling by the dotted line (which cannot be distinguished from
the dashed line) and spread-regression by the dot-dashed line. In panel c) the solid line is regression, and
the dotted line is from spread-regression. In panel d) the solid line is from spread-scaling and the dotted
line is from spread-regression.

2

4

6

8

10

2

4

6

8

10

lead

lead

a
h
p
a

l

a
m
m
a
g

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

t

a
e
b

a
t
l
e
d

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

2

4

6

8

10

2

4

6

8

10

lead

lead

Figure 3: Parameter values for the spread-regression model, with conﬁdence limits.

s
d
a
e
r
p
s
 

n
a
e
m

 
f

o
o

 

i
t

a
r

S
V
O
C

0
4

.

0
3

.

0
2

.

0
1

.

0
6

0
4

0
2

0

2

4

6

8

10

lead

lead

2

4

6

8

10

Figure 4: Two diagnostic ratios from the ensemble forecasts used in the study. The upper panel shows
the ratio of the mean of the ensemble spread to the mean of the standard deviation in the calibrated
ensemble. We see that the uncalibrated ensemble spread is too small at all leads, especially leads 1 and
2. The lower panel shows the coeﬃcient of variation (COV) of the ensemble spread before (solid line)
and after (dashed line) calibration. The dotted line is the level of COV that one would expect even if the
uncertainty was constant, from sampling variability.

50 60 70 80 90

50 60 70 80 90

day

day

n
a
e
m

n
a
e
m

3

1

1
−

3
−

3

1

1
−

3
−

d
s

d
s

6
0

.

3

.

0

0

.

0

2

.

1

6
.
0

0
.
0

50 60 70 80 90

50 60 70 80 90

day

day

Figure 5: Examples of the ensemble mean and standard deviation before (solid line) and after (dotted
line) calibration. In the left hand panels we see the ensemble mean, and in the right hand panels we see
the ensemble spread. In the upper panels we see lead 1 and in the lower panels, lead 5.

2

4

6

8

10

−2

0

1

2

3

lead

qobs 2

l

s
a
u
d
s
e
r
 
f

i

o

 
f
c
a

d
o
m
q

6

.

0

0
0

.

3

1

1
−

3
−

d
o
m
q

d
o
m
q

3

1

1
−

3
−

3

1

1
−

3
−

−3

−1

1 2 3

−3

−1

1 2 3

qobs 5

qobs 9

Figure 6: Analysis of the residuals from the spread-regression model. The upper left panel shows the
ACF of the residuals for 3 diﬀerent lead times. The other panels show QQ plots of the residuals for leads
2, 5 and 9.

2

4

6

8

10

2

4

6

8

10

lead

lead

a
h
p
a

l

a
m
m
a
g

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

t

a
e
b

a
t
l
e
d

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

2

4

6

8

10

2

4

6

8

10

lead

lead

Figure 7: A test of the robustness of the parameters of the spread-regression model. The solid lines are
the all-year parameters, as shown in ﬁgure 3. The dotted lines show the same parameters estimated on
the 1st six months of data, and the dashed lines show the parameters estimated on the 2nd six months
of data.

e
s
m

r

2
1

.

0
1

.

8
0

.

6
0

.

4
.
0

2

4

6

8

10

lead

Figure 8: The RMSEs for the four models described in the text, and climatology. Climatology is the
roughly horizontal solid line. Regression is the sloping solid line. Spread-only is the dashed line, spread-
scaling is the dashed line (which is indistinguishable from the solid line) and spread-regression is the
dot-dashed line (which is also indistinguishable from the solid line).

l
l

m

r

3
1

.

2
1

.

1
1

.

0
1

.

9
0

.

8
.
0

7
.
0

2

4

6

8

10

lead

Figure 9: The RMMLLs for the four models described in the text, and climatology. Climatology is
the roughly horizontal solid line. Regression is the sloping solid line. Spread-only is the dashed line,
spread-scaling is the dotted line, and spread-regression is the dot-dashed line.

 

2
>
−
1

 

 

1
>
−
 
2

0
0
3

0
0
2

0
0
1

0

0
0
3

0
0
2

0
0
1

0

2

4

6

8

10

lead

lead

2

4

6

8

10

Figure 10: The out of sample skill of the spread-regression and regression models, measured using
RMMLL. The solid line is for regression, the dotted lines are the conﬁdence limits for the regression,
and the dashed line is for spread-regression.


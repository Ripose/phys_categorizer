4
0
0
2
 
p
e
S
 
4
2
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
7
2
1
9
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Probabilistic forecasting of temperature: comments on the
Bayesian Model Averaging approach

Stephen Jewson∗

November 20, 2013

Abstract

A speciﬁc implementation of Bayesian model averaging has recently been suggested as a method
for the calibration of ensemble temperature forecasts. We point out the similarities between this new
approach and an earlier method known as kernel regression. We also argue that the Bayesian model
averaging method (as applied) has a number of ﬂaws that would result in forecasts with suboptimally
calibrated mean and uncertainty.

1 Introduction

There is signiﬁcant demand within industry for adequate probabilistic forecasts of temperature. However,
this demand has not been met by the meteorological community and such forecasts are not commercially
available. A small number of forecast vendors do produce probabilistic forecasts but the calibration meth-
ods they use are ﬂawed. A number of academic papers have suggested methods by which such forecasts
could be improved but again the methods described are ﬂawed. To attempt to remedy this situation we
run a program of research aimed at clarifying the issues involved in the creation of probabilistic tem-
perature forecasts and at developing methods that can be used to produce such forecasts. We are not
forecasters ourselves: our hope is that the forecasting community will use the methods we describe to
produce forecasts that we can then use in our industrial applications.
This article discusses a new method with the name Bayesian model averaging (BMA) that has recently
been proposed for the calibration of temperature forecast ensembles (see Raftery et al. (2003)). Our
purpose is twofold:

1. To point out the close connections between BMA and earlier methods known as kernel regression

(KR) and kernel spread regression (KSR)

2. To describe a number of ﬂaws that we believe that the BMA approach suﬀers from that render it

inappropriate as a method to be used for the calibration of real forecast data

We start by describing the KR and BMA approaches. We then compare the two and point out the
problems we see in BMA. Finally we suggest some further methods that take features from both BMA
and KR that could be used to solve the calibration problem that is discussed in Raftery et al. (2003).

2 Kernel Regression

Kernel regression (KR) was described by us in Jewson (2003). It is a method that takes an ensemble
forecast and turns it into a probabilistic forecast. The simplest reasonable way to do this is to use
linear regression on the ensemble mean. KR is a simple extension of linear regression that allows for the
representation of non-normality in the temperature distribution by putting a small kernel of optimum
width around each ensemble member. The probability density forecast from KR can be written as:

where the pi are the individual kernels given by

p(x) =

pi(x)

M

X
i=1

pi(x) ∼ N (xi, λ2

)

∗Correspondence address: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com

(1)

(2)

where xi is the i’th ensemble member and λ is the bandwidth (these equations come from equation 1
in Jewson (2003)).
In addition to applying kernels in this way the mean and the variance of the ensemble members are
calibrated using linear regression. We write the complete model as:

Ti ∼ K(α + βmi, γ, λ)

KR calibrates the ensemble mean using linear regression (which gives an optimal combination between
the ensemble mean and climatology) and ﬁxes the spread and the non-normality using the parameters γ
and λ. The parameter λ is the bandwidth of the kernels used and controls the smoothness of the ﬁnal
predicted distribution. Small values of λ lead to a multimodal distribution while large values of λ lead
to a unimodal smooth distribution.
The mean of the prediction from KR is given by:

while the variance of the prediction, which is constant in time for the anomalies, is given by:

E(x) = α + βmi

var(x) = λ2

2
(xi − µ)

M

X
i=1

+

1
M
+ γ2

= λ2

or

variance of modelled temperatures = λ2

+ sample variance of calibrated ensemble members

(6)

(this equation is equation 9 in Jewson (2003)).
An extension of KR that allows for the uncertainty to vary in time according to variations in the ensemble
spread is also described Jewson (2003), and can be written as

Ti ∼ K(α + βmi, γ + δsi, λ)
This model, known as kernel spread regression (KSR), calibrates the ensemble spread by having separate
parameters for the mean and the variance of the spread. This was shown to be necessary in Jewson et al.
(2003).
The predicted variance from KSR is:

(7)

var(x) = λ2

+

2
(xi − µ)

1
M

M

X
i=1

= λ2

2
+ (γ + δsi)

3 Bayesian model averaging

BMA is a general approach for combining the results from several statistical models using weights (Hoeting et al.,
1999). There are a number of ways that BMA could be used in the creation of probabilistic forecasts.
We will discuss the particular application of BMA given in Raftery et al. (2003). The conclusions we will
draw do not apply to BMA in general, but only to this particular way of using BMA.
The suggestion in Raftery et al. (2003) is that the probability density of future temperatures can be
modelled as a weighted sum of a number of probability densities from diﬀerent sources:

p(x) =

wigi(x)

i=M

X
i=1

where

gi(x) ∼ N (xi, σ2
(10)
i )
where xi are the ensemble members (these equations are equations 2 and 3 from Raftery et al. (2003),
written in our notation).
The variance of the probabilistic forecast is then given by

var(x) =

2
wi(xi − µ)

+

wiσ2
i

M

X
i=1

M

X
i=1

(this is equation 7 from Raftery et al. (2003)).

(3)

(4)

(5)

(8)

(9)

(11)

4 The connection between BMA and KR

We now consider how BMA and KR are related. To see the connection we consider a case where the
individual forecasts are statistically identical. BMA also considers the more general case where the
forecasts are statistically diﬀerent although we will argue that since it doesn’t work in the simplest case
of identical members it certainly can’t be expected to work in the more complex cases.
If the forecasts are statistically identical then we can assume that the BMA weights and σi’s are equal:

(12)

(13)

(14)

(15)

(16)

1
M

wi =

σi = σ

p(x) =

gi(x)

i=M

X
i=1

1
M

E(x) =

xi

M

X
i=1

Equation 9 now gives:

and we can see that this agrees with equation 1 if we deﬁne gi(x) = M pi(x) i.e. if we normalise the kernels
diﬀerently. So this part of the two models is the same up to a simple deﬁnition of the normalisation.
The BMA predicted mean is just

i.e. the ensemble mean, and the BMA predicted variance is

var(x) =

2
(xi − µ)

+ σ2

M

X
i=1

1
M

We can now see the similarities and diﬀerences between BMA and kernel regression very clearly.

1. By comparing equation 4 with equation 15 we see that BMA predicts the expected temperature us-
ing the ensemble mean while KR predicts the expected temperature using an optimum combination
of the ensemble mean with climatology

2. By comparing equations 5 and 8 with equation 16 we see that BMA calibrates the mean level of
uncertainty, the variability of the uncertainty and the smoothness of the distribution using a single
parameter σ. KR uses two parameters to calibrate the mean level of uncertainty and the smoothness
while KSR uses three parameters to calibrate the mean level of uncertainty, the variability of the
uncertainty and the smoothness.

BMA (when applied to the identical members case) is a special case of KSR in which β = 1, γ = 0 and
δ = 1.

5 The problems with BMA

Unfortunately Bayesian model averaging seems to suﬀer from a number of ﬂaws as a method for calibrating
temperature ensembles. These issues discussed below: the research on which these conclusions are based
is summarised in Jewson (2004).
The ﬁrst problem concerns the calibration of the ensemble mean. In the special case that we are consid-
ering BMA predicts the expected temperature using the ensemble mean. However it is well documented
(Leith (1974), von Storch and Zwiers (1999), Jewson and Ziehmann (2003)) that the ensemble mean is
not the optimal forecast for the expected temperature: a ‘damped’ version of the ensemble mean calcu-
lated using linear regression is better. This damping performs an optimal calibration of the ensemble
mean with climatology. An undamped ensemble mean such as that produced by BMA does not have the
correct variance and will not minimise RMSE.
The second problem concerns the calibration of the uncertainty. To correctly calibrate the uncertainty of
a probabilistic forecast one needs to consider (at least) two operations. First, the temporal mean of the
uncertainty must be ﬁxed at an appropriate level. There is no information about the temporal mean of
the uncertainty in the ensemble itself: this information can only come from past forecast error statistics.
Secondly, the amplitude of the variability of the uncertainty must be ﬁxed at an appropriate level. Again,

there is no information about the amplitude of the variability of the uncertainty in the ensemble itself:
this must be ﬁtted from past forecast error statistics too. What the ensemble provides is then the relative
amplitude and phase of the ﬂuctuations of the uncertainty.
The important point is that these two calibration steps (calibrating the mean and the amplitude of the
variability of the spread) are independent. To set the mean level of the uncertainty correctly one typically
needs to inﬂate the ensemble spread. However, to set the amplitude of the variations in the uncertainty
correctly one may need to reduce the amplitude of the variations in the ensemble spread. A statistical
model thus needs at least two parameters in order to calibrate spread correctly. If only one parameter is
available, the calibration of the mean and the variability of the uncertainty will be mixed together, and
the results will be somewhat arbitrary and very possibly less good than a calibration method that ignores
the variability in the ensemble spread altogether. This mixing of diﬀerent aspects of the calibration is
what happens in BMA1.
KR, KSR and BMA add another operation in the calibration of the ensemble, which is the smoothing of
the ensemble towards or away from a normal distribution. If the bandwidth of the kernel (λ in kernel
regression and σ in BMA) is large then the ensemble is smoothed towards a normal while if the bandwidth
is small the probability forecast will likely be rather multimodal and will have a shape that depends more
strongly on the distribution of the individual ensemble members. This smoothing operation needs a
separate parameter to be performed correctly as it is an independent issue from the calibration of the
uncertainty. KR and KSR use a separate parameter for this step while BMA uses the same parameter
as is used to calibrate the uncertainty.
In summary BMA only has a single free parameter (σ) rather than the three that are required to perform
the calibration that is being attempted. Thus the three operations that are being performed (calibration
of the mean level of the uncertainty, calibration of the variability of the uncertainty and calibration of the
smoothness of the forecast distribution) are mixed together. It is easy to imagine situations in which this
would cause problems. For instance it would not be possible for BMA to correctly calibrate an ensemble
for which the variability in the ensemble spread contains very little information (requiring a large value
of σ) but in which the temporal mean of the ensemble spread is close to the correct level (requiring a
small value of σ). Nor would it be possible for BMA to correctly calibrate an ensemble for which the
ensemble spread was larger than the actual uncertainty.

6 The solution

The solution to this problem is to use the correct number of free parameters for the calibration that is
being attempted. Given only a single parameter the most sensible course of action seems to be to assume
a normal distribution, ignore the variations in spread and use the parameter to represent the mean level
of uncertainty. Given two parameters one should calibrate the mean and variability of the uncertainty,
while still assuming a normal distribution. Finally given three parameters one can calibrate all three of
the mean level of uncertainty, the variability of the uncertainty and the smoothness.

7 Weighted kernel regression

In Raftery et al. (2003) BMA was used to combine a number of forecasts that were not statistically
identical. We have argued that BMA does not calibrate correctly in the statistically identical case, and
so cannot be expected to work in more general cases either. How, then, should the original calibration
problem described in Raftery et al. (2003) be solved? The kernel regression models should not be used
as is since they assume that the forecasts are statistically identical.
One can imagine methods that take the best of the KSR and BMA approaches that might include one
or more of the following features:

• the mean is predicted using multiple linear regression on the anomalies

• kernels with diﬀerent widths are used on each ensemble member

• the kernels could be combined with diﬀerent weights

• the uncertainty is predicted using some linear function on the weight ensemble spread
1and to be fair we should note that this problem also arises in other forecast calibration methods that have been suggested

in the academic literature such as the methods of Roulston and Smith (2003) and Mylne et al. (2002)

However, our previous experience of calibration suggests to us that much simpler models might perform
just as well since the eﬀects of non-normality and the beneﬁt of using the spread may well both be small.
In that case multiple linear regression on the anomalies is probably ideal, and whatever method is being
used it should be compared with linear regression on the anomalies as an appropriate minimal model.

8 Summary

We have discussed the question of how to produce probabilistic forecasts of temperature. In particular
we have dissected the Bayesian model averaging approach of Raftery et al. (2003). This approach is very
similar to an earlier approach known as kernel regression (Jewson, 2003). We have argued that BMA does
not calibrate temperatures in an appropriate way. Neither the predicted mean nor the predicted variance
are constructed accurately. With respect to the predicted mean, the issue of ‘damping’ towards climatol-
ogy has been omitted. With respect to the variance, BMA mixes the separate functions of calibrating the
mean level of uncertainty, the amplitude of the variability of the uncertainty and the smoothness of the
forecast distribution into a single factor. We conclude that BMA (as applied in Raftery et al. (2003)) is
not a calibration method at all, but simply a method to ﬁt a distribution to a set of ensemble members.
As such it is more or less the same as the well known kernel density of classical statistics.

9 Acknowledgements

Thanks to Christine Ziehmann for interesting discussions on this topic.

10 Legal statement

SJ was employed by RMS at the time that this article was written.
However, neither the research behind this article nor the writing of this article were in the course of his
employment, (where ’in the course of their employment’ is within the meaning of the Copyright, Designs
and Patents Act 1988, Section 11), nor were they in the course of his normal duties, or in the course
of duties falling outside his normal duties but speciﬁcally assigned to him (where ’in the course of his
normal duties’ and ’in the course of duties falling outside his normal duties’ are within the meanings of the
Patents Act 1977, Section 39). Furthermore the article does not contain any proprietary information or
trade secrets of RMS. As a result, the author is the owner of all the intellectual property rights (including,
but not limited to, copyright, moral rights, design rights and rights to inventions) associated with and
arising from this article. The author reserves all these rights. No-one may reproduce, store or transmit,
in any form or by any means, any part of this article without the author’s prior written permission. The
moral rights of the author have been asserted.
The contents of this article reﬂect the author’s personal opinions at the point in time at which this article
was submitted for publication. However, by the very nature of ongoing research, they do not necessarily
reﬂect the author’s current opinions.
In addition, they do not necessarily reﬂect the opinions of the
author’s employer.

References

Science, 14:382–417, 1999.

arXiv:physics/0310060, 2003.

J Hoeting, D Madigan, A Raftery, and C Volinsky. Bayesian model averaging: a tutorial. Statistical

S Jewson. Do probabilistic medium-range temperature forecasts need to allow for non-normality?

S Jewson. A summary of our recent research into practical methods for probabilistic temperature fore-

casting. arxiv:physics/0409096, 2004.

S Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble

temperature forecasts. Atmospheric Science Letters, 2003.

S Jewson and C Ziehmann. Weather swap pricing and the optimum size of medium range forecast

ensembles. Weather and Forecasting, 18(4):675–681, 2003.

C Leith. Theoretical skill of Monte Carlo forecasts. Monthly Weather Review, 102:409–418, 1974.

K Mylne, C Woolcock, J Denholm-Price, and R Darvell. Operational calibrated probability forecasts
from the ECMWF ensemble prediction system: implementation and veriﬁcation. In Preprints of the
Symposium on Observations, Data Asimmilation and Probabilistic Prediction, pages 113–118. AMS, 1
2002.

A Raftery, F Balabdaoui, T Gneiting, and M Polakowski. Using bayesian model averaging to calibrate
forecast ensembles. University of Washington Department of Statistics Technical Report, 440, 2003.

M Roulston and L Smith. Combining dynamical and statistical ensembles. Tellus A, 55:16–30, 2003.

H von Storch and F W Zwiers. Statistical Analysis in Climate Research. CUP, 1999.


3
0
0
2
 
g
u
A
 
6
1
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
5
6
0
8
0
3
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

The Assessment and Calibration of Ensemble Seasonal Forecasts
of Equatorial Paciﬁc Ocean Temperature and the Predictability
of Uncertainty

Stephen Jewson1∗, Francisco Doblas-Reyes2 and Renate Hagedorn2
1RMS, London, United Kingdom
2ECMWF, Reading, United Kingdom

February 2, 2008

Abstract

We evaluate the performance of two 44 year ensemble seasonal hindcast time series for the Ni˜no3
index produced as part of the DEMETER project. We show that the ensemble mean carries useful
information out to six months. The ensemble spread, however, only carries useful information out to
four months in one of the models, and two months in the other.

1 Introduction

One of the ways that users of seasonal forecasts can access forecast information is to monitor predictions of
sea surface temperatures in the equatorial Paciﬁc. The Ni˜no3 and Ni˜no3.4 regions in particular are good
indicators of the state of ENSO, and numerous forecasts are available for these indices. These forecasts
come from dynamical ensemble models such as those discussed in Stockdale et al. (1998) and Mason et al.
(1999), from intermediate complexity models as described in Prigaud et al. (2000) and from pure statis-
tical models such as that of Penland and Magorian (1993).
Users of any of these forecasts need to be able to answer a number of questions about a forecast before
they can use it with conﬁdence in an application. These questions include:

• Does the forecast need calibration?

• For how many months is the forecast of the mean temperature better than no forecast?

• What is the best way to derive a prediction of the uncertainty around the mean?

• Which is the best of the available forecasts?

These questions have been addressed by a number of authors. Methods used to answer the ﬁrst question
include assessments of bias and incorrect variance (Doblas-Reyes et al., 2003) and rank histograms and
reliability diagrams (Wilks, 2000). The second question can be answered using the anomaly correlation
between forecast and observations or the signiﬁcance level of a regression coeﬃcient between forecast and
observations (Brankovic and Palmer, 2000). The third question is generally harder to answer and there
do not seem to be any very satisfactory methods described in the literature. Finally, the fourth question
can be answered using a combination of methods including anomaly correlation or mean squared error
(MSE), which both address the skill of the mean of the forecast, or the relative operating characteristic
(ROC) (Swets, 1988) and CRPS (continuous rank probability score) (Hersbach, 2000) which look at
aspects of the distribution.
As part of a project to develop simple practical methods for use in industry, we will now show how it is
possible to answer all four of these questions at once in a single consistent framework using the method
described in Jewson et al. (2003) (henceforth JBZ). The values of the parameters from the JBZ method
will give us clear answers to the ﬁrst three questions, along with interesting insights into the predictability
of uncertainty. We will answer the fourth question using a new skill measure that measures the ability
of the forecast to predict the whole temperature distribution.

∗Correspondence address: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com

In section 2 we describe the observational and forecast data used for this study. In section 3 we will
brieﬂy describe the method we will apply and the interpretation of the resulting parameter values. In
section 4 we show results from the analysis of our two time series of ensemble seasonal hindcasts, and in
section 5 we discuss our results and draw some conclusions.

2 Data

All the analyses in this paper are based on sets of seasonal hindcasts from the European Centre for
Medium Range Weather Forecasting (ECMWF) and Meteo France (MF) seasonal forecast models. These
are both dynamical ocean-atmosphere models. The hindcasts were produced as part of the DEMETER
project (Palmer et al., 2003) and consist of 6 month predictions of monthly mean Ni˜no3 temperatures.
Both sets of hindcasts cover the period 1958 to 2001 (44 years) and consist of ensembles of size nine. The
forecasts start at four diﬀerent times of year: February, May, August and November. These forecasts are
compared with temperature observations for the Ni˜no3 region obtained from the GISST2.3 and Reynolds
2D-Var SST data sets.
All values used in this study are deseasonalised in the mean and the standard deviation before any
calculations are performed, and all equations given below apply to these deseasonalised values.

3 Method

We will address the four questions outlined in the introduction using the statistical model described in
JBZ. This model consists of a simple calibration method in which the ensemble mean and the ensemble
standard deviation are considered as inputs for a prediction for the mean temperature and the uncertainty
around that temperature. The predictions of the mean and the uncertainty are derived from the ensem-
ble mean and standard deviation using simple linear transformations. To be more speciﬁc, the model
postulates that observed temperatures come from a normal distribution with estimated mean given by
ˆµi = α + βmi and estimated standard deviation given by ˆσi = γ + δsi, where mi is the ensemble mean
and si is the ensemble spread. We write this model as:

Ti ∼ N (α + βmi, γ + δsi) = N (ˆµ, ˆσ)

(1)

Although neither the mean temperature nor the standard deviation of temperature are actually observed,
the parameters of this model can be ﬁtted easily by maximizing a cost function which measures the
goodness of ﬁt of the normal distribution. There are a number of possible cost functions one could use,
but we choose to use the likelihood, deﬁned as the probability density of the observations given the
calibrated forecast. Likelihood is the standard approach used for parameter estimation in statistics (see
for example the statistics textbooks by Casella and Berger (2002) or Lehmann and Casella (1998)) and
is one of the most natural methods for measuring the goodness of ﬁt of a distribution. It also gives the
most accurate possible parameter estimates for most statistical models.
Having determined the optimum values for the parameters α, β, γ and δ one can interpret them as follows:

• α identiﬁes bias in the forecast. If α is signiﬁcantly diﬀerent from zero then the bias needs correcting.

• β calibrates the variations of the ensemble mean to have the correct amplitude. If β is signiﬁcantly
diﬀerent from one then the forecast needs calibration. It is also used to assess whether the forecast
contains any useful information: if β is not signiﬁcantly diﬀerent from zero then the forecast is useless
(at least in the context of this calibration model) and one should use climatological temperatures
instead.

• γ and δ calibrate the prediction of the uncertainty to have the correct size and variability. If there
is very little information in the variability of the ensemble spread, then δ will be small and γ will
be larger to compensate. In the same way that β can be used to assess whether the ensemble mean
has any useful predictive information, δ can be used to assess the ensemble spread.
If δ is not
signiﬁcantly diﬀerent from zero then the ensemble spread contains no useful information (again, in
the context of this model).

In the case in which δ is not signiﬁcantly diﬀerent from zero then, for that lead time, the hope that
ﬂow-dependent variations in uncertainty can be predicted has to be abandoned and one should re-ﬁt
using the alternative model:

Ti ∼ N (α + βmi, ˆσ0)

where ˆσ0 is constant in time for a given lead time. This model can also be ﬁtted by maximizing the
likelihood, which is equivalent to least squares linear regression in this case. Predictions of the uncertainty
in this simpliﬁed model will come entirely from past forecast error statistics.
Even if δ is signiﬁcantly diﬀerent from zero and we conclude that the ensemble spread of a model does
contain useful ﬂow-dependent information about variations in the uncertainty, it is still not necessarily
the case that the predicted variations in the uncertainty are material.
In other words, they may be
so small relative to the mean level of the uncertainty that they can as well be ignored. This can be
for two reasons: either because the predicted variations in the uncertainty have a very low correlation
with the actual variations in the uncertainty, or because the actual uncertainty itself simply does not
vary very much. We will measure the materiality of the predictable variations in the uncertainty using
the coeﬃcient of variation of the spread (COVS), deﬁned by the ratio of the standard deviation of the
variations in the calibrated spread to the mean calibrated spread:

(2)

(3)

COVS =

δsd(s)
γ + δs

We arbitrarily choose a level of 0.05 to deﬁne the level below which we consider variations in the uncer-
tainty to be immaterial. If uncertainty variations are immaterial then there is no need to use equation 1
to calibrate a forecast, and the simpler linear regression model (equation 2) can be used instead.
There are a number of ways to compare the resolution of forecasts from diﬀerent models. To make a
fair in-sample comparison, the same calibration methods must be used for all the models. Forecasts can
then be compared using methods such as MSE, ROCs, or CRPS. However, to focus on the ability of a
model to predict the probabilities across the whole distribution correctly we prefer to use the likelihood.
Likelihood skill measures can be presented in a number of ways:

• The likelihood itself.

the likelihood

• The log-likelihood, which has a more compressed (and hence more convenient) range of values than

• The square root of minus the log-likelihood (RMLL). This has the advantage that it is equivalent

to use of the root-mean-square error in cases where the uncertainty is not ﬂow-dependent.

• The log-likelihood skill-score (LLSS) deﬁned as one minus the ratio of the log-likelihood to the
climatological log-likelihood. This has the advantage that values range from zero for a useless
forecast to one for a perfect forecast.

Likelihood skill measures can be presented for each lead time, or for all lead times together. We will
present our comparison results in terms of the LLSS for each lead time.

4 Results

Figure 1 shows the optimum values for the parameters in equation 1 for the ECMWF hindcasts, based
on forecasts made at all times of year. Each estimate also has an indication of the statistical or sampling
uncertainty around the optimal parameter estimate. These uncertainties are calculated from the curvature
of the log-likelihood in the standard way. 44 years of forecasts four times per year gives 176 past forecasts,
and from the uncertainty estimates we see that this is enough to give reasonably accurate estimates of
all the parameters.
The value of β is signiﬁcantly diﬀerent from zero at all lead times up to six months. This shows that the
ensemble mean contains useful information at all lead times. However, it certainly needs to be calibrated
to give an optimum forecast, especially at the longer lead times. The value of δ, however, is only
signiﬁcantly diﬀerent from zero at leads one and two. This shows that the ensemble spread only contains
useful information at these two lead times, and for longer leads does not contain useful information. For
leads three to six one thus has to discard the results from the calibration model and reﬁt the ensemble
data using standard linear regression (equation 2). This will give optimal predictions of the mean and
the uncertainty, but the uncertainty prediction will not be ﬂow-dependent.
Figure 2 shows results for the same analysis for the MF model. The value of β again shows that the
ensemble mean contains useful information at all lead times. The values for δ are now signiﬁcantly

diﬀerent from zero up to lead four. Only at leads ﬁve and six does the ensemble spread contain no useful
information. Once again, at these lead times the results from the calibration model must be discarded
and a linear regression model used instead.
We have seen that the ECMWF model can be used to make a ﬂow dependent prediction of uncertainty
at leads one and two, and the MF model can be used to make such a prediction for leads one to four.
We now assess the size of these ﬂow-dependent variations in uncertainty relative to the mean level of the
uncertainty using the COVS. Values for the COVS for leads for which there is signiﬁcant information
about the spread are given in table 1. We see that the ECMWF model only gives material values of COVS
at lead one. However at this lead the standard deviation of predictable variations in the uncertainty is
nearly 14% of the total uncertainty. It would seem likely that this is a useful level of predictability of
the variability of uncertainty for some users. The MF model gives material but rather low values for the
COVS out to lead four.
We now address the question of whether the models show the same levels of predictability at diﬀerent
times of year by repeating the calibration analysis on each season separately. The seasonal parameters
are ﬁtted using only 44 past forecast values (one per year) and this means that the parameters cannot
be estimated as accurately. This is balanced by the fact that we can hopefully pick up more detail in the
structure of the predictability. For instance we might expect to see some signs of the well-known spring
barrier (Webster and Yang, 1992).
The results for the ensemble mean (not shown) show that in all seasons both models contain useful
predictive information out to six months. The results for the ensemble spread are more complex. Figure 3
shows the optimum values for the parameters in equation 1 for the MF forecasts started in May. In this
case the ensemble spread contains detectable useful information from leads one to four, as with the annual
data. Results for the uncertainty for all other seasons and for both models are summarized in table 2,
which lists the months for which skillful ﬂow dependent predictions of the variations in uncertainty can
be made. We detect that the ECMWF model shows skill for the spread only in the ﬁrst month, while
the MF model shows skill for the spread for diﬀerent numbers of months in each season. How is it that,
when analyzed on annual data, the ECMWF model shows two months of signiﬁcant spread, whereas
when analyzed on seasonal data, it only shows one month? The answer is that the spread signal in the
second month is very small, and reducing the number of data points from 176 to 44 means that it can no
longer by detected. The MF model shows more seasonal variability in the predictability of spread than
the ECMWF model. In particular, the model shows the least predictability of spread in February. This
suggests that the spring predictability barrier may aﬀect predictions of spread as well as predictions of
the mean.
Finally we consider which of the two models produces better forecasts, taking into account the skill with
which the model predicts the probabilities across the whole distribution by using a likelihood based skill
measure. Figure 4 shows the LLSS for both models. We see that, by this measure, the ECMWF model is
better at lead 1, but that the MF model is slightly better at all subsequent leads. The biggest fractional
diﬀerences in the LLSS are at leads ﬁve and six. However, the diﬀerences are small and may be partly
due to sampling variability.

5 Conclusions

We have described how the ensemble calibration model of JBZ can be used to assess and calibrate
ensemble seasonal forecasts, and in particular how it can determine the limit of useful information in
the ensemble mean and standard deviation of such forecasts. We have assessed time series of ensemble
seasonal hindcasts from the ECMWF and MF models. The results are striking. For the ECMWF model
although the ensemble mean contains useful predictive information out to the end of the forecast period
at six months, the ensemble spread shows no useful predictive information beyond lead two. At lead
times beyond the second (and beyond the ﬁrst on a seasonal basis) it is more appropriate to calculate the
uncertainty of the forecast from past forecast error statistics than it is to use the ensemble spread. Even
at lead two, the size of the predictable component of the uncertainty could be considered immaterial
relative to the total uncertainty. This ﬁnding contrasts with the results for medium range ensemble
forecasts from ECMWF, for which both the mean and the spread contain useful information out to the
end of the forecast at 10 days (see JBZ). For the MF model the ensemble mean also contains useful
information to the end of the forecast, while the spread contains useful information up to month four.
Only at months ﬁve and six does the spread not contain useful information about the variations in
forecast uncertainty. When we consider the size of the variations in spread that are predictable from
the MF model, we ﬁnd that they are material relative to the total uncertainty, and are probably worth

incorporating into a prediction of the uncertainty if accurate predictions of the uncertainty are important.
They are, however, only a small fraction of the mean uncertainty and some users may decide to ignore
them if less accuracy in the estimates of the uncertainty is required. When analyzed on a seasonal basis
we see strong seasonal variability in the predictability of uncertainty consistent with there being a spring
barrier for predictability of uncertainty as well as for predicting the mean.
We have demonstrated how long seasonal hindcasts are extremely useful both for deﬁning accurate
calibration parameters and for assessing the skill of forecast systems. A prudent user of forecasts would
never use a forecast unless the skill can be identiﬁed statistically in hindcasts or past forecasts, and with
seasonal forecasts skill can often only be identiﬁed with such long data sets.
Finally we have compared the ECMWF and MF hindcasts using a likelihood based skill measure which
assesses the performance of the model in predicting the whole distribution of possible outcomes. We ﬁnd
that, according to this measure, the ECMWF model makes slightly better forecasts at lead one, while
the MF model forecasts are slightly better for leads two to six.
There are a number of directions for future work arising from this study. It is important to assess results
from calibration using the JBZ model in out-of-sample tests, and to compare the reliability of such results
with those from other calibration methods. From a user-perspective it is important to extend this work
to consider site-speciﬁc forecasts in addition to forecasts of the Paciﬁc ocean temperature. Finally, the
optimal predictions of the mean and variance that are produced by the calibration method described form
a good basis for forming optimal multimodel forecasts, and it would interesting to compare the results
from such multimodel forecasts with those derived using other methods.

6 Acknowledgements

We would like to thank ECMWF for providing us with the data on which this study was based, and D.
Anderson, A. Brix, S. Mason and C. Ziehmann for helpful discussions. SJ funded his own research, while
FDR and RH were funded by the DEMETER project (EVK2-1999-00197).

References

2035–2068, 2000.

C Brankovic and T Palmer. Seasonal skill and predictability of ecmwf provost ensembles. QJRMS, 126:

G Casella and R L Berger. Statistical Inference. Duxbury, 2002.

F Doblas-Reyes, V Pavan, and D Stephenson. The skill of multi-model seasonal forecasts of the wintertime

nao. Climate Dynamics, 2003. Accepted.

H Hersbach. Decomposition of the continuous ranked probability score for ensemble prediction systems.

WAF, 15:559–570, 2000.

S Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble

temperature forecasts. ASL, 2003. Submitted.

E Lehmann and G Casella. Theory of Point Estimation. Springer-Verlag, 1998.

S Mason, L Goddard, N Graham, E Yulaeva, L Sun, and P Arkin. The IRI seasonal climate prediction

system and the 1997/1998 El Ni˜no event. BAMS, 80:1853–1873, 1999.

T Palmer, A Alessandri, U Andersen, P Cantelaube, M Davey, P Delecluse, M Deque, E Diez, F Doblas-
Reyes, H Feddersen, R Graham, S Gualdi, J Gueremy, R Hagedorn, M Hoshen, N Keenlyside, M Latif,
A Lazar, E Maisonnave, V Marletto, A Morse, B Orﬁla, P Rogel, J Terres, and M Thomson. Develop-
ment of a European multi-model ensemble system for seasonal to inter-annual prediction (DEMETER).
BAMS, 2003. Submitted.

C Penland and T Magorian. Prediction of Ni˜no3 sea surface temperature using linear inverse modelling.

J Climate, 6:1067–1076, 1993.

C Prigaud, C Cassou, B Dewitte, L Fu, and D Neelin. Using data and intermediate coupled models for

seasonal-to-interannual forecasts. MWR, 128:3025–3049, 2000.

T Stockdale, D Anderson, J Alves, and M Balmaseda. Global seasonal rainfall forecasts using a coupled

ocean-atmosphere model. Nature, 392:370–373, 1998.

J Swets. Measuring the accuracy of diagnogstic system. Science, 240:1285–1293, June 1988.

P Webster and S Yang. Monsoon and ENSO: selectively interactive systems. QJRMS, 118:877–926, 1992.

D Wilks. Diagnostic veriﬁcation of the CPC long-lead outlooks. J Climate, 13:2389–2403, 2000.

1

2

3

4

5

6

1

2

3

4

5

6

lead

lead

a
h
p
a

l

a
m
m
a
g

0

.

1

0
0

.

0

.

1
−

2

.

1

6
.
0

0
.
0

t

a
e
b

a
t
l
e
d

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

1

2

3

4

5

6

1

2

3

4

5

6

lead

lead

Figure 1: The optimum values of the parameters in equation 1 for the ECMWF seasonal hindcasts,
calculated using forecasts made at all times of year.

1

2

3

4

5

6

1

2

3

4

5

6

lead

lead

a
h
p
a

l

a
m
m
a
g

0

.

1

0
0

.

0

.

1
−

2

.

1

6
.
0

0
.
0

t

a
e
b

a
t
l
e
d

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

1

2

3

4

5

6

1

2

3

4

5

6

lead

lead

Figure 2: As for ﬁgure 1 but for the MF seasonal hindcasts.

lead

1

ECMWF 13.9
8.6

MF

4

2
3
4.9 N/A N/A
5.3
12.0
8.8

Table 1: The values, as percentages, for the coeﬃcient of variation of the predictable spread from the
ECMWF and MF models. Values for the ECMWF hindcasts are only given up to lead two since beyond
that there is no detectable skill in predicting the spread. Similarly, values for the MF hindcasts are given
up to lead four only.

1

2

3

4

5

6

1

2

3

4

5

6

lead

lead

a
h
p
a

l

a
m
m
a
g

5
0

.

.

5
0
−

5

.

1
−

2

.

1

6
.
0

0
.
0

t

a
e
b

a
t
l
e
d

2

.

1

6
0

.

0

.

0

2

.

1

6
.
0

0
.
0

1

2

3

4

5

6

1

2

3

4

5

6

lead

lead

Figure 3: As for ﬁgures 1 and 2 but now based on MF hindcasts started in May only.

forecast month Feb May Aug Nov

ECMWF
MF

1
0

1
4

1
1

1
2

Table 2: The number of months for which the forecast started in forecast month showed useful predic-
tive information in the ensemble spread, as judged by a value for the parameter δ in equation 1 being
signiﬁcantly diﬀerent from zero.

S
S
L
L

0
1

.

8
0

.

6
0

.

4
0

.

2

.

0

0
.
0

1

2

3

4

5

6

lead

Figure 4: The log-likelihood skill score (LLSS) for the ECMWF seasonal hindcasts (solid line) and the
MF seasonal hindcasts (dashed line). A zero score indicates no more information than climatology, and
a score of one indicates a perfect forecast.


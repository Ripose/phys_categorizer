4
0
0
2
 
b
e
F
 
5
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
7
2
0
2
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Singular vector ensemble forecasting systems and the prediction
of ﬂow dependent uncertainty

Stephen Jewson∗
RMS, London, United Kingdom
Maarten Ambaum and Christine Ziehmann

February 2, 2008

Abstract

The ECMWF ensemble weather forecasts are generated by perturbing the initial conditions of the
forecast using a subset of the singular vectors of the linearised propagator. Previous results show that
when creating probabilistic forecasts from this ensemble better forecasts are obtained if the mean of
the spread and the variability of the spread are calibrated separately. We show results from a simple
linear model that suggest that this may be a generic property for all singular vector based ensemble
forecasting systems based on only a subset of the full set of singular vectors.

1 Introduction

We are interested in the question of how to make forecasts of the distribution of future temperatures over
time-scales of one or two weeks. The best predictors for this distribution come from numerical weather
forecasting models, and, in particular, from ensemble integrations of such models. The ensembles are
generated by running the forecast model many times from diﬀerent initial conditions, and, in some cases,
by using stochastic parameterisations. The precise methods used to generate the ensemble of diﬀerent
initial conditions diﬀer from one forecast system to another. For instance, ECMWF uses a method based
on perturbing the initial state using the singular vectors of the linearised propagator over a ﬁnite time
period (Molteni et al., 1996) while NCEP uses the breeding vector method (Toth and Kalnay, 1993).
The predictors one can derive from these models are a mixture of information and error, and it is non-
trivial to convert these predictors into optimal probabilistic forecasts. The most straightforward method
used to derive a prediction of the future distribution of temperatures is to build a linear regression
model between the ensemble mean (as input) and the temperatures being predicted (as output). Such
a model gives a mean-square error minimising prediction of the temperature, as well as a prediction of
the uncertainty around that temperature. We will refer to the regression model as a ﬁrst generation
calibration model. It has been in use since the 1970s (see, for example, Leith (1974)).
Second generation calibration models use more information from the ensemble than just the ensemble
mean in an attempt to predict ﬂow dependent variations in the uncertainty of the temperature predic-
tion. Second generation models include the rank histogram (Talagrand et al., 1997), the best members
method (Roulston and Smith, 2003) and the spread-scaling method (Jewson, 2003b). These models are
similar in that they calibrate the mean level of the uncertainty and the variability of the uncertainty in the
same way. This, it turns out, is not ideal, and as a result the second generation models do not, generally,
perform as well as linear regression. To understand why not one can consider the case in which the vari-
ability of the ensemble spread contains no information at all. In such a situation an eﬀective calibration
method would ignore this variability and produce a forecast with a constant level of uncertainty derived
entirely from past forecast error statistics. However, all of the second generation calibration models fail
this test and would actually inﬂate the variability in the uncertainty still further, because of the need to
inﬂate the mean level of spread.
The third generation of calibration models addresses this issue, and calibrates the mean and the variability
of the uncertainty in separate ways. Jewson et al. (2003) describe such a model and show an example
where the mean level of uncertainty needs to be increased while the variability of the uncertainty needs
to be decreased. Why should this be necessary? One explanation is that site speciﬁc temperatures

∗Correspondence address: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com

are aﬀected by small-scale processes that increase the mean level of uncertainty, but do not change
the variability in the uncertainty. However, in this paper we will investigate another possibility: that
it is the methods used to generate the ensembles themselves, and in particular the truncated singular
vector approach used at ECMWF, that result in ensemble forecasts that require that the mean and the
variability of the uncertainty be calibrated separately. We approach this question using a simple linear
stochastic model of the prediction of forecast uncertainty. This model allows us to calculate both the
exact uncertainty and the uncertainty predicted by singular vector methods, and to compare the two.
In section 2 we give a brief description of how the initial conditions are generated in the ECMWF ensemble
forecasting system. In section 3 we describe the simple model we will use to study the properties of singular
vector forecasting systems in general. In section 4 we present our results and in section 5 we summarise.

2 The ECMWF ensemble prediction system

The method used at ECMWF to create an ensemble forecast can be described as follows.

2.1 Step 1: linearisation of the propagator

We write the entire atmospheric model as:

dx
dt = F (x)

where x(t) is the atmospheric state and F is a non-linear function representing the dynamics of the
model. We will assume henceforth that the model is perfect, and hence that F (x) is also an accurate
representation of the non-linear dynamics of the atmosphere. We will only consider forecast errors that
arise due to errors in the initial conditions.
If we now consider a forecast made from the current state x, and write an initial condition error as a
small perturbation e around x we have:

Subtracting equation 1 from equation 2, and ignoring higher order terms, gives:

This is a linear equation for the development of initial condition errors in the forecast.
This can be solved to give:

Writing B(t) =

Adt gives:

R

Expanding this gives:

and ignoring higher order terms:

d(x + e)
dt

= F (x + e)

= F (x) +

e + ...

dF
dx

de
dt =

e

dF
dx
= A(x(t))e

e(t) = exp

Adt

e0

(cid:19)

(cid:18)Z

e(t) = exp(B(t))e0

e = (1 + B + ...)e0

e(t) = (1 + B(t))e0

(1)

(2)

(3)

(4)

(5)

(6)

(7)

This is now a linear equation which gives the forecast error at time t in terms of the initial condition
error at time 0.

2.2 Step 2: creation of initial conditions
The ﬁrst 25 singular vectors of matrix 1 + B are calculated.

2.3 Step 3: creation of the ensemble forecast

Positive and negative versions of each singular vector are propagated forwards using equation 1 to give
50 forecasts. The spread of these forecasts gives an indication of the uncertainty in the forecast.

3 The model

Our model for the process by which forecast errors in the ECMWF model develop is just equation 7:

e = (1 + B)e0
(8)
where e0 is a vector of initial condition errors, e is a vector of ﬁnal forecast errors, and 1 + B is a matrix
representing the process by which the forecast errors grow. For simplicity, we will study a 2 dimensional
system.
We will assume that the matrix B varies in time, to represent variations in the state of the atmosphere.
Our model for B will be that each of the four matrix elements are independent of the other elements, are
independent of themselves in time, and are given by a standard normal distribution.
Our model for the real initial condition errors e0 will be that these errors are drawn from a bivariate
normal distribution with correlation of zero.
Within this model the variations in the statistics of the distribution of forecast errors are driven by
variations in the extent to which the error propagator 1 + B causes the initial condition distribution to
grow or not.

3.1 Generating the real forecast uncertainty
For each point in time (i.e. for each randomly generated 1 + B) we deﬁne the real forecast uncertainty by
performing an ensemble of integrations over 1000 values for e0 sampled from the distribution of possible
values (the bivariate normal). We take the resulting ensemble of 1000 values of e and calculate the
distribution for the ﬁrst element to represent observing a single variable. The standard deviation of this
distribution gives a measure of the real uncertainty in the forecast (by deﬁnition).

3.2 Predicting the forecast uncertainty using a full singular vector system

We now imagine that we want to predict the uncertainty deﬁned in section 3.1 using the singular vector
method. We assume that we know the exact propagator for the forecast errors i.e. we also use 1 + B to
predict the uncertainty. In real forecast systems, the process by which forecast errors grow is not entirely
understood. However, in our system we understand it completely.
We generate the ensemble prediction by calculating the two right singular vectors of 1 + B, and using
them to initialise e0 four times, for positive and negative versions of each singular vector (mimicking the
ECMWF initialisation system). This generates an ensemble of 4 values for e (which are the left singular
vectors scaled by their singular values) and the standard deviation of this ensemble gives the predicted
uncertainty.
We then compare the temporal variations in the predicted uncertainty with the temporal variations in
the actual uncertainty.

3.3 Predicting the forecast uncertainty using the ﬁrst singular vector

We now imagine that we want to predict the uncertainty using the truncated singular vector method.
This is closer to the system used at ECMWF, which is based on the ﬁrst 25 singular vectors of a system
with many thousands of degrees of freedom.
We now generate the ensemble prediction using only the ﬁrst singular vector of 1 + B. We initialise,
as before, using positive and negative versions of this singular vector. This generates an ensemble of 2
values for e, and the standard deviation of this ensemble gives our prediction of the uncertainty.

3.4 Predicting the forecast uncertainty using the second singular vector

Our third and ﬁnal method for using singular vectors to predict the uncertainty uses only the second
singular vector. Otherwise this method is identical to the second method.

4 Results

Figure 1 and ﬁgure 2 show examples of the initial conditions and forecast errors from the random initial
condition model, along with the left singular vectors of the matrix 1 + B scaled by their singular values,
for six arbitrarily chosen forecast days. We see that the initial condition error ball becomes an ellipse
of ﬁnal forecast errors, and that the singular vectors are aligned with the principal axes of this ellipse,
exactly as we would expect.

4.1 Temporal variation of the real forecast uncertainty

We now consider the temporal variations in the real uncertainty, generated from the ensemble of initial
conditions sampled from the bivariate normal distribution.
Figure 3 shows a time series of 50 days of forecast uncertainty generated by the model. We see that
the forecast uncertainty varies in time. This is because of the random variations in the matrix B, which
mimic the ﬂow dependent changes in the processes that control the growth of forecast errors.
The top left panels of ﬁgure 8, ﬁgure 9 and ﬁgure 10 show the distribution of the real uncertainty. This
distribution is repeated as a dotted line in the other panels of these ﬁgures for purposes of comparison.

4.2 Temporal variation of the forecast uncertainty from the full singular vec-

tor model

We now attempt to predict the temporal variations in the uncertainty using the full singular vector
ensemble. A scatter plot of the real uncertainty (horizontal axis) and the predicted uncertainty (vertical
axis) is shown in the lower left panel of ﬁgure 5. We see a strong relation between the two, although
the predicted uncertainty is larger than the real uncertainty. The lower right panel of ﬁgure 8 shows the
distribution of predicted uncertainty (solid line), which is clearly too wide relative to the real uncertainty
(dotted line). We calculate the empirical correlation between this predicted uncertainty and the real
uncertainty: it is very close to 1.0. This shows the value of using singular vectors: we can avoid having
to sample the whole initial condition error ball by using vectors which eﬃciently span the forecast error
space.
We now consider calibration of the forecast, since the mean and the standard deviation of the uncertainty
prediction are both wrong (as shown by the distribution of the uncertainty in ﬁgure 8). We try a
very simple calibration consisting of a scaling of the uncertainty forecast (the ”spread-scaling” method
of Jewson (2003b)). Since we are dealing with a linear system this is equivalent to scaling the initial
condition singular vectors. The eﬀect of this calibration on the distribution is shown in the lower right
panel of ﬁgure 9. We see that this simple calibration method succeeds in setting both the mean and
the standard deviation of the uncertainty to be correct, even though there is only a single calibration
parameter. A scatter plot of the calibrated forecasts is shown in ﬁgure 6, lower left panel.
In summary, our full singular vector uncertainty forecast has a correlation with the real uncertainty of
one, and, post-calibration, the mean and the standard deviation are correct. It is producing a perfect
forecast of the actual uncertainty.

4.3 Temporal variation of the forecast uncertainty predicted from the ﬁrst

singular vector

We now consider the temporal variations in the uncertainty predicted using just the ﬁrst singular vector.
This system is an analogy to the ECMWF prediction system, which uses a truncated set of singular
vectors to create an ensemble.
A time series of the predicted uncertainty is shown in ﬁgure 4 (dotted line) along with the real uncertainty
(solid line). The upper left panel of ﬁgure 5 shows a scatter plot of the real and the predicted uncertainty.
The upper right panel of ﬁgure 8 shows the distribution of the predicted uncertainty. The correlation
between this predicted uncertainty and the actual uncertainty is approximately 0.95: 3 estimates of this
correlation from a set of independent experiments are shown in table 1. From the correlation and the
scatter plot we see that the use of only a single singular vector is not as accurate as using the full set of
singular vectors, as expected.
The mean and the variability of the predicted uncertainty are wrong, and, as before, we can attempt
to calibrate them. This time, however, when we calibrate with a simple scaling designed to correct the
mean uncertainty, this does not correct the standard deviation of the uncertainty correctly, as is shown
in the top right panel of ﬁgure 9. A scatter plot of the results of this calibration are shown in the upper

left panel of ﬁgure 6. The ratio of the standard deviation of the predicted uncertainty to the standard
deviation of the actual uncertainty after the scaling calibration is given in the second column of table 1.
We see that the calibrated forecast overestimates the variability of the uncertainty. In a real forecast
system this would lead to overprediction of extreme events.
We then use a more complex calibration system that corrects the predicted uncertainty using a shift and
a scaling. Because such a calibration system has two parameters it can (and does) correct both the mean
and the variance of the uncertainty (as is shown in the top right panel of ﬁgure 10) although it cannot,
of course, improve the correlation. A scatter plot of the results of this calibration step are shown in the
upper left panel of ﬁgure 7.

4.4 Temporal variation of the forecast uncertainty predicted from the second

singular vector

Finally we consider the temporal variations in uncertainty predicted using only the second singular
vector. The correlation between the actual and the predicted uncertainty is close to zero: values from our
3 independent experiments are shown in table 2, and in the upper right panel of ﬁgure 5. The distribution
of predicted uncertainty is shown in ﬁgure 8, lower left panel.
Calibration using a single scaling again does not succeed in correcting both the mean and the variability of
the uncertainty, and again the ratio of the variability of the predicted uncertainty to the real uncertainty
is too high, as can be seen from the calibrated distribution in ﬁgure 9. In fact, the overestimation of the
variability of the predicted uncertainty is considerably higher than when using the ﬁrst singular vector.
Scatter plots of the results from calibrating with a simple scaling, and with a shift and scaling, are shown
in the top right panels of ﬁgure 6 and 7. The eﬀect of a shift and scaling on the distribution are shown
in ﬁgure 10.

5 Discussion

We have investigated how singular vectors can be used to predict the evolution of errors in a simple
two dimensional linear stochastic system. The system is designed to mimic the development of errors in
forecasts of the real atmosphere, and the use of singular vectors is designed to mimic the use of singular
vectors in the ECMWF ensemble prediction system.
We generate (or rather deﬁne) the real uncertainty in our system by using initial conditions sampled
from a bivariate normal distribution. We then use this real uncertainty as a basis for comparison for
uncertainty predicted using singular vectors.
Our ﬁrst singular vector system uses both singular vectors to predict the uncertainty. The predictions
are very successful, and show a 100% correlation with the actual uncertainty. However, they still need
calibration. A simple calibration consisting of a scaling is suﬃcient to match both the mean and the
variability of the predicted uncertainty to reality.
Our second singular vector system uses only the leading singular vector to predict the uncertainty. These
predictions are designed to mimic the ECMWF system, which uses a truncated set of singular vectors to
predict forecast uncertainty. The predictions we generate from the truncated system are less successful
and do not have 100% correlation with the actual uncertainty. They also overestimate the variability of
the uncertainty by about 20%, even after a ”spread-scaling” calibration step. Because of this if we want
to calibrate the forecast to have the correct mean and variability in the level of uncertainty we need to
use two parameters.
Our third singular vector system uses only the second singular vector to predict the uncertainty. In this
case the correlation with the real uncertainty is very poor. However, the variability of the uncertainty
is again overestimated even after spread-scaling calibration. This shows that overestimation of the vari-
ability of the uncertainty is not caused by the ﬁrst singular vector per se, but just by the use of only a
single singular vector, whichever it is.
In the cases where the correlation between the predicted and actual uncertainty is less than 100% it is
not necessarily the best thing to do to enforce the mean and the variance of the uncertainty to be correct.
In fact in a real forecast system we cannot implement this method for calibration anyway because we
do not know the variability of the real uncertainty. An alternative calibration method is to calibrate
the uncertainty prediction so as to maximise the log-likelihood, as used in Jewson et al. (2003). We
will consider this possibility, and analyse the eﬀect of such a calibration system on the mean and the
variability of the uncertainty in the context of our simple model in a subsequent article.

To the extent that our system captures some of the dynamics of the full ECMWF forecast system, we
conclude that the use of truncated singular vectors is one reason why Jewson et al. (2003) have found
that the predictions of uncertainty from that system need calibration using third generation calibration
models that treat the mean of the uncertainty and the variability of the uncertainty separately. It also
suggests that forecasts from the ECMWF system calibrated using second generation calibration models
will tend to overestimate the variability of the uncertainty and overpredict extreme events.
One idea that arises from this work is that it might be worth trying to calibrate the uncertainty of
ensemble forecasts using a standard CDF-based distribution transform, which would convert the spread
of the forecast ensemble to a prediction of the uncertainty, and enforces a sensible distribution for the
latter. The calibration could ﬁx the parameters of the predicted distribution using maximum likelihood.
This might be a better calibration model than the spread regression model of Jewson et al. (2003), which
only considers predictions of the uncertainty based on linear transformations of the ensemble standard
deviation.

6 Legal statement

The lead author was employed by RMS at the time that this article was written.
However, neither the research behind this article nor the writing of this article were in the course of his
employment, (where ’in the course of his employment’ is within the meaning of the Copyright, Designs
and Patents Act 1988, Section 11), nor were they in the course of his normal duties, or in the course
of duties falling outside his normal duties but speciﬁcally assigned to him (where ’in the course of his
normal duties’ and ’in the course of duties falling outside his normal duties’ are within the meanings of
the Patents Act 1977, Section 39). Furthermore the article does not contain any proprietary information
or trade secrets of RMS. As a result, the lead author is the owner of all the intellectual property rights
(including, but not limited to, copyright, moral rights, design rights and rights to inventions) associated
with and arising from this article. The lead author reserves all these rights. No-one may reproduce, store
or transmit, in any form or by any means, any part of this article without the author’s prior written
permission. The moral rights of the lead author have been asserted.

References

2003a. Technical report.

2003b. Technical report.

S Jewson. Maximum likelihood calibration of ensemble spread in a linear stochastic model. In preparation,

S Jewson. Moment based methods for ensemble assessment and calibration. arXiv:physics/0309042,

S Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble

temperature forecasts. Atmospheric Science Letters, 2003. Submitted.

C Leith. Theoretical skill of Monte Carlo forecasts. Monthly Weather Review, 102:409–418, 1974.

F Molteni, R Buizza, T Palmer, and T Petroliagis. The ECMWF ensemble prediction system: Method-

ology and validation. Q. J. R. Meteorol. Soc., 122:73–119, 1996.

M Roulston and L Smith. Combining dynamical and statistical ensembles. Tellus A, 55:16–30, 2003.

O Talagrand, R Vautard, and B Strauss. Evaluation of probabilistic prediction systems. In Proceedings,
ECMWF Workshop on Predictability, 20-22 October 1997, pages 1 – 25, available from ECMWF,
Shinﬁeld Park, Reading RG2 9AX, UK, 1997.

Z. Toth and E. Kalnay. Ensemble forecasting at NMC: The generation of perturbations. Bull. Am.

Meteorol. Soc., 74:2317–2330, 1993.

Table 1: The second column shows the correlation between the uncertainty predicted using the ﬁrst
singular vector and the real uncertainty. The third column shows the overestimation of the uncertainty
after calibration using spread-scaling (where the real uncertainty is 1). The three rows show three
independent numerical experiments.

expt
1
2
3

correlation
0.95
0.95
0.96

sd ratio
1.22
1.20
1.20

expt
1
2
3

correlation
-0.03
0.04
-0.02

sd ratio
1.89
1.99
1.79

Table 2: As for table 1 but for the uncertainty predicted using the second singular vector.

−3 −2 −1

0

1

2

3

−3 −2 −1

0

1

2

3

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

−3 −2 −1

0

1

2

3

−3 −2 −1

0

1

2

3

Figure 1:
1,2,3

Initial conditions, forecasts and left singular vectors scaled by their singular values...examples

−3 −2 −1

0

1

2

3

−3 −2 −1

0

1

2

3

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

3

2

1

0

1
−

2
−

3
−

−3 −2 −1

0

1

2

3

−3 −2 −1

0

1

2

3

Figure 2:
4,5,6

Initial conditions, forecasts and left singular vectors scaled by their singular values...examples

y

4

8

6

2

0

0

10

20

30

40

50

time

Figure 3: Time series of the real uncertainty, generated using an ensemble of 1000 members based on
initial conditions from a bivariate normal distribution.

y

4

8

6

2

0

0

10

20

30

40

50

time

Figure 4: Time series of the real uncertainty (solid line) with the uncalibrated prediction of the uncer-
tainty from the ﬁrst singular vector (dotted line).

0
1

8

6

4

2

0

0

2

4

6

8

10

0

2

4

6

8

10

0
1

8

6

4

2

0

0
1

8

6

4

2

0

0

2

4

6

8

10

Figure 5: Scatter between the real uncertainty (horizontal axes) and the 3 predictions of the uncertainty
(vertical axes). The top left panel shows the uncertainty predicted from the ﬁrst singular vector, the top
right panel shows the uncertainty predicted from the second singular vector and the lower left panel shows
the uncertainty predicted from both singular vectors. All predicted uncertainties are uncalibrated. In the
top left panel we see that the predicted uncertainty is strongly related to the actual uncertainty. All the
points on the diagonal line are situations where the real uncertainty is dominated by the ﬁrst singular
vector, and hence where the prediction using the ﬁrst singular vector is a good one. The points below
the diagonal line correspond to situations where the ﬁrst singular vector is less important (presumably
because it is more or less orthogonal to the observation axis) and where the second singular vector becomes
important. In these cases the predictions of uncertainty are poor because the second singular vector is not
being used. In the top right panel we see much lower correlation. There is only a very weak diagonal line,
corresponding to these situations when the second singular vector dominates the uncertainty. The points
below this line correspond to situations where the ﬁrst singular vector dominates. The real uncertainty in
these situations is often large, but poorly predicted. In the lower left panel we see a very high correlation
between real and predicted uncertainty. There is a very small spread because of sampling errors. The
slope of the diagonal line is not one: the predictions have a larger standard deviation than the real
uncertainty.

0
1

8

6

4

2

0

0

2

4

6

8

10

0

2

4

6

8

10

0
1

8

6

4

2

0

0
1

8

6

4

2

0

0

2

4

6

8

10

Figure 6: As ﬁgure 5 but for the uncertainties calibrated using spread-scaling. In the lower left panel
the slope of the line is now 1: only a single scaling is needed to calibrate the forecast.

0
1

8

6

4

2

0

0

2

4

6

8

10

0

2

4

6

8

10

0
1

8

6

4

2

0

0
1

8

6

4

2

0

0

2

4

6

8

10

Figure 7: As ﬁgure 5 and ﬁgure 6 but for the uncertainties calibrated using a shift and a scaling.

 

6

 

5

.

0

4
0

.

3
0

.

2

.

0

1
0

.

0
0

.

5
0

.

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

 

6

 

5

.

0

4
0

.

3
0

.

2

.

0

1
0

.

0
0

.

5
0

.

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

0

2

4

8

10 12

0

2

4

8

10 12

0

2

4

6

8

10 12

0

2

4

6

8

10 12

Figure 8: Distributions of uncertainty, derived from 1000 samples using kernel smoothing. The top left
panel shows the distribution of the real uncertainty, and this curve is repeated in the other panels as a
dotted line. The top right panel shows the distribution of uncertainty predicted using the ﬁrst singular
vector. The lower left panel shows the distribution of uncertainty predicted using the second singular
vector. The lower right panel shows the distribution of uncertainty predicted using both singular vectors.
All three predicted uncertainties are uncalibrated. We note that some of these curves show non-zero
density for negative values. This is an artefact of the kernel smoothing. In the top right hand corner we
note that the predicted density (solid line) is much wider than the actual density (dotted line), as is the
case in the lower right hand panel.

 

6

 

5

.

0

4
0

.

3
0

.

2

.

0

1
0

.

0
0

.

5
0

.

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

 

6

 

5

.

0

4
0

.

3
0

.

2

.

0

1
0

.

0
0

.

5
0

.

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

0

2

4

8

10 12

0

2

4

8

10 12

0

2

4

6

8

10 12

0

2

4

6

8

10 12

Figure 9: As for ﬁgure 8 but for predicted uncertainties calibrated using spread-scaling. In the lower
right hand panel the calibration has rendered the predicted distribution more or less correct, while in the
top right hand panel the distribution is still too wide. This, we believe, is analogous to the overestimation
of the variability of the uncertainty seen in real ensemble forecasts by Jewson et al. (2003).

 

6

 

5

.

0

4
0

.

3
0

.

2

.

0

1
0

.

0
0

.

5
0

.

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

 

6

 

5

.

0

4
0

.

3
0

.

2

.

0

1
0

.

0
0

.

5
0

.

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

0

2

4

8

10 12

0

2

4

8

10 12

0

2

4

6

8

10 12

0

2

4

6

8

10 12

Figure 10: As for ﬁgure 8 but for predicted uncertainties calibrated using a shift and a scaling. The
distribution in the top right hand panel is more or less correct now.


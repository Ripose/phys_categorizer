4
0
0
2
 
b
e
F
 
5
 
 
]
h
p
-
o
a
.
s
c
i
s
y
h
p
[
 
 
1
v
6
2
0
2
0
4
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Improving probabilistic weather forecasts using seasonally varying
calibration parameters

Stephen Jewson∗
RMS, London, United Kingdom

January 17, 2014

We show that probabilistic weather forecasts of site speciﬁc temperatures can be dramatically

improved by using seasonally varying rather than constant calibration parameters.

Abstract

1 Introduction

Diﬀerent users of weather forecast are interested in diﬀerent things. One particular group of users,
including weather derivatives traders, is most interested in probabilistic forecasts at speciﬁc locations.
The production of such forecasts on the 0-10 day timescale is what we will consider in this article. We
will derive our forecasts from the output of numerical weather prediction models, as is usual. This
output contains a lot of information about the future weather but needs processing in order to be directly
relevant to individual sites, which are not represented in the models. This processing step, usually known
as calibration or downscaling can usefully be cast as a classical (i.e. non-Bayesian) statistical problem.
In this framework a mixture of climatology and the output from the model are used as predictors and
the observations that we wish to predict are the predictand.
Forecast calibration can be performed for any weather variable, but we will focus on temperature. For
temperature anomalies1 a good starting point for the calibration model is a standard linear regression
taking the ensemble mean as the single predictor, and the distribution of possible future temperatures
as the predictand. Linear regression has been used as a calibration model for at least 30 years, although
it is only recently been fully appreciated that it gives a good probabilistic forecast, rather than just a
good forecast of the expected temperature. The challenge is now to build models that perform better
than linear regression, and that is the subject of this article. We have described the testing of a number
of models versus linear regression in previous articles, and have generally found it hard to beat by more
than minute amounts. For instance, we have tried to improve forecast skill by using the ensemble spread
as a predictor of uncertainty (Jewson et al. (2003) and Jewson (2003c)), but found only a small beneﬁt.
We have also tried to improve the forecast by relaxing the assumption of normality and replacing the
normal distribution with a kernel density, but in that case found almost no beneﬁt at all (Jewson, 2003b).
In another study we investigated whether the beneﬁt of using an ensemble versus a single forecast arises
more from the information content of the ensemble mean or the ensemble spread (Jewson, 2003a). The
answer is very clear: the ensemble mean is vastly more useful. This suggests that the best way to beat
linear regression might be to improve the forecast of the mean, rather than the forecast of the spread,
and that is the approach we follow below.
How could we improve the forecast of the mean? There is a long list of methods we might consider,
including:

• using non-linear models such as neural nets

• using predictors from other locations

• using lagged predictors

• using multiple models
∗Correspondence address: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com
1from which the mean and seasonal cycle have been removed

• using seasonally varying parameters

These are all probably worthy of investigation. However in this paper we will only address the last of
these: can we beat constant parameter linear regression by allowing the parameters to vary seasonally?
On the one hand, from a meteorological point of view, this seems a very reasonable approach since in the
climate one usually ﬁnds that everything varies seasonally. On the other hand, as devotees of parsimony,
we balk at this approach. In a seasonal parameter model each of the parameters in the regression becomes
(at least) 3 parameters. Thus a 3 parameter model becomes a 9 parameter model. For such a model to
be better, the seasonality in the mapping from model forecast to observed temperature had better be
rather strong.
The idea of using seasonally varying parameters is somewhat similar to a method currently used by
some National Meteorological Services for the calibration of single forecasts which builds the calibration
models using only recent training data (e.g. data for the previous 90 days). This method automatically
captures some aspects of seasonality because it allows the calibration parameters to vary through the
year. However, we believe that explicitly modelling seasonality has some beneﬁts, and will be the method
of choice in the long run. This is because:

• Fitting calibration parameters from the previous 90 days suﬀers from the problem that the param-
eters are always slightly behind relative to the present point in the season. They may be the best
parameters for 45 days ago, but will not be the best parameters for today.

• Fitting calibration parmaeters from the previous 90 days only allows us to use a small amount of
past forecast data for estimating the parameters. As the amount of available past forecast data
increases it makes sense to try and use all of this data. This is especially important if we are to
make use of the subtle signals contained in the varying spread of ensemble forecasts.

In section 2 we discuss the data we use for our study. In section 3 we describe the models and how we
will compare them. In section 4 we present the results, and in section 5 we present our conclusions and
discuss areas for future work.

2 Data

We will base our analyses on one year of ensemble forecast data for the weather station at London’s
Heathrow airport, WMO number 03772. The forecasts are predictions of the daily average temperature,
and the target days of the forecasts run from 1st January 2002 to 31st December 2002. The forecast was
produced from the ECMWF model (Molteni et al., 1996) and downscaled to the airport location using
a simple interpolation routine prior to our analysis. There are 51 members in the ensemble. We will
compare these forecasts to the quality controlled climate values of daily average temperature for the same
location as reported by the UKMO.
There is no guarantee that the forecast system was held constant throughout this period, and as a result
there is no guarantee that the forecasts are in any sense stationary, quite apart from issues of seasonality.
This is clearly far from ideal with respect to our attempts to build statistical interpretation models on
past forecast data but is, however, unavoidable: this is the data we have to work with.
Throughout this paper all equations and all values are in terms of double anomalies (have had both the
seasonal mean and the seasonal standard deviation removed). Removing the seasonal standard deviation
removes most of the seasonality in the forecast error statistics, and partly justiﬁes the use of non-seasonal
parameters in the statistical models for temperature that we propose.

3 Models

As mentioned in the introduction, we will take a classical statistical approach to the problem of creating
probabilistic temperature forecasts. This means that we will postulate models which predict the distribu-
tion of temperature directly in terms of a number of predictors. Such models are simple to design, simple
to understand, simple to ﬁt, simple to test and easy to use for making forecasts. They also allow us to
incorporate any number of predictors, including climatology, in an optimum (likelihood maximising) way.
The standard method for estimating the parameters of classical statistical models is to ﬁnd those parame-
ters which maximise the probability of the observations given the model. This quantity, when considered
as a function of the model parameters, is known as the likelihood. A standard (and intuitively very
reasonable) way to compare such models against each other is to compare the maximum likelihoods they

achieve. The forecast that gives the higher likelihood (or log-likelihood) is the better forecast (see Jewson
(2003e) and Jewson (2003d) for more details on this). This method for comparing probabilistic fore-
casts can be used on both continuous and discrete forecasts, and in-sample or out-of-sample. In-sample
testing has two caveats: ﬁrstly that it can only be applied to parsimonious parametric models (because
non-parametric models and models with large numbers of parameters are overﬁtted) and secondly that
the probability used in the comparison has to be adjusted to penalise models with more parameters. A
common way to make this adjustment is to use the AIC criterion, and the AIC score is what we will use
below.
We now present the models we compare in this study.
Our ﬁrst model is simply linear regression between temperature on day i (Ti) and the ensemble mean on
day i (mi), which we write as:

Ti ∼ N (α0 + β0mi, γ0)

This model corrects biases using α0, optimally ”damps” the variability of the ensemble mean and merges
optimally with climatology using β0, and predicts ﬂow-independent uncertainty using γ0. The bias and the
uncertainty produced by this model vary seasonally because of the deseasonalisation and reseasonalisation
steps. All our subsequent models will be judged against this model.
Our second model generalises this model so that the parameters themselves vary seasonally:

where

where

where θi is the time of year. We have represented seasonality in the simplest way possible by using just
one harmonic in order to keep the number of parameters as low as possible.
We now consider three models that are intermediate between the constant-parameter linear regression
(equation 1) and the seasonal-parameter linear regression (equation 2).
The ﬁrst only has seasonal bias correction:

The second has seasonal damping:

and the third has seasonal innovations:

For comparison we will also consider the spread regression model of Jewson et al. (2003):

(where si is the ensemble spread on day i) and a completely seasonal version of the spread regression
model:

Ti ∼ N (αi + βimi, γi)

αi = α0 + αssinθi + αccosθi
βi = β0 + βssinθi + βccosθi
γi = γ0 + γssinθi + γccosθi

Ti ∼ N (αi + β0mi, γ0)

Ti ∼ N (α0 + βimi, γ0)

Ti ∼ N (α0 + β0mi, γi)

Ti ∼ N (α0 + β0mi, γ0 + δ0si)

Ti ∼ N (αi + βimi, γi + δisi)

αi = α0 + αssinθi + αccosθi
βi = β0 + βssinθi + βccosθi
γi = γ0 + γssinθi + γccosθi
δi = δ0 + δssinθi + δccosθi

This last model has the greatest number of parameters of all the models we consider (12) and would be
expected to perform the best because it includes all the other models.
We ﬁt the parameters of all the models by maximising the likelihood using a standard quasi-Newton
method with ﬁnite-diﬀerence gradient.

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

4 Results

Figure 1 shows the AIC scores for the constant parameters and seasonal parameter linear regression
models (upper and lower solid lines respectively). By deﬁnition, lower values of the AIC score are better,
and a value of zero would be a perfect forecast. We see a vast improvement in the skill of the probabilistic
forecast as a result of using seasonal parameters, especially at short lead times. The dotted line shows the
AIC score for the seasonal spread regression model. There is a further improvement at most lags from
making the spread calibration parameters seasonal in addition, but it is small. At lag 9 the ﬁtting of the
parameters of the spread regression failed: the algorithm was unable to ﬁnd a convincing maximum for
the likelihood. This is presumably because the signal is too weak given the number of parameters we are
trying to ﬁt and the amount of data being used.
Where does this vast improvement come from? Is it the seasonality in the bias correction, the damping,
the innovations, or all three in combination together? We address this question using the intermediate
models. Figure 2 shows the same results as ﬁgure 1 in the solid lines in each panel, but also shows the
AIC score for 4 other models as a dotted lines (the top left panel is the seasonal bias model, top right
is the seasonal damping model, lower left is the seasonal noise model and lower right is the non-seasonal
spread regression model). We immediately see that, of these models, the seasonal bias model gives the
greatest beneﬁt over straight linear regression. Figure 3 shows the same data, but now relative to the
AIC score of linear regression (more negative values are better). We see again that the seasonal bias
correction gives a large beneﬁt, the seasonal damping gives no real beneﬁt, the seasonal noise gives a
small beneﬁt at short leads, and that the spread regression gives a small beneﬁt at all leads.
Figure 4 investigates the extent to which there is synergy between the diﬀerent parameterisations. In
the upper panel we compare the beneﬁt from making all of α, β and γ seasonal at once (dotted line)
against the sum of the beneﬁts of making them seasonal separately (solid line). There is deﬁnitely some
synergy: the total is greater than the sum of the parts. In the lower panel we consider the beneﬁt of
using spread regression in a non-seasonal and seasonal model. In our previous research it has been rather
disappointing that using the ensemble spread brings so little beneﬁt to probabilistic forecasting, and we
had hoped that maybe the beneﬁt would be greater in the context of seasonal parameters for the mean.
However, this is not the case. The beneﬁt is more or less the same and is still very small.
Constant parameter linear transformations such as the basic linear regression model cannot improve
the linear correlation of the ensemble mean with the observations. However, seasonal parameter linear
transformations can. Figure 5 shows linear correlations before (solid line) and after (dotted line) the
seasonal transformation. We see a deﬁnite improvement in linear correlation at all lead times.
Figure 6 shows the 9 parameters for the seasonal regression model. The top row shows the alpha param-
eters, the second row the beta parameters and the third row the gamma parameters. We see that the
variability in alpha is dominated by the cosine term at all but the longest leads, the variability in beta is
small relative to the average level of beta, and that gamma is more or less constant throughout the year
(but not with lead, of course).
The ﬁnal ﬁgure, ﬁgure 7, shows the seasonal variability of alpha predicted by the seasonal bias model
for the ﬁrst 9 lead times. We see that there is signiﬁcant seasonal variability in the alpha predicted by
the model, which is consistent with the large eﬀect that this model had on the AIC score. We see that
at short leads the smallest values of α are in spring and the largest in autumn, while at longer leads the
opposite is true.

5 Summary

We have made another attempt at improving the probabilistic forecasts of temperature that can be
made from ensemble forecasts. As before our starting point and basis for comparison is a linear regres-
sion model. Our previous attempts, that have looked at the beneﬁt from using the ensemble spread
(Jewson et al. (2003) and Jewson (2003c)) and the beneﬁt from using the distribution of the individual
ensemble members (Jewson, 2003b), have not shown much improvement over linear regression. This time
we have tried allowing the parameters of the regression model to vary seasonally. We ﬁnd a dramatic
improvement in the skill of the forecasts, much larger than the improvement from our previous attempts.
When we break down which terms are driving the improvement in skill we ﬁnd that adding seasonality
in the bias is the most important factor. However seasonality in all three terms is important and there
is actually synergy between the terms such that the beneﬁt from making all three regression parameters
seasonal is greater than the sum of the beneﬁts of making each one seasonal separately (by the measure
we use for skill). Furthermore, our seasonal regression model also improves the linear correlation between
forecast and observations.

The clear implication of this is that one should always use the seasonal parameter regression model in
preference to the constant parameter linear regression model, and that the seasonal parameter regression
model should become the new baseline for comparison with other methods and models.
There are, as ever, a number of avenues for future work that are suggested by this study. Most obviously,
allowing the bias to vary seasonally seems to be so important that one could try using more harmonics.
It may well be that adding extra parameters in the modelling of the bias is more useful than adding extra
parameters elsewhere. We do not, however, feel it would be justiﬁed with only the limited amount of
data used in this study, and this is why we have not considered higher harmonics here.
At a technical level, our ﬁtting algorithm could be improved if we avoided the assumption that the
forecast errors are uncorrelated in time (they are, in fact, weakly positively correlated). This may aﬀect
the results somewhat, but we doubt it would aﬀect them qualitatively.
One possible criticism of our study might be that, by using up to 12 parameters to model only 365
(weakly correlated) observation pairs we are ﬂirting with overﬁtting. We wouldn’t disagree. We have
compensated for this by using AIC rather than straight log-likelihood as our measure of skill and so it
should be the case that our results would transfer to out of sample likelihood comparisons. Nevertheless
if longer data sets of stationary past forecasts ever become available then it would be very interesting to
repeat this analysis: the parameters of the models we have presented will become much better estimated,
and the results that much better justiﬁed.
Our highest priority is now to repeat this analysis on wind and precipitation forecasts, which present
similar but diﬀerent challenges to the modelling of temperature because they are not close to normally
distributed.

Many thanks to Ken Mylne and Caroline Woolcock for providing the forecast data used in this study,
and for helpful discussions.

6 Acknowledgements

7 Legal statement

The lead author was employed by RMS at the time that this article was written.
However, neither the research behind this article nor the writing of this article were in the course of his
employment, (where ’in the course of his employment’ is within the meaning of the Copyright, Designs
and Patents Act 1988, Section 11), nor were they in the course of his normal duties, or in the course
of duties falling outside his normal duties but speciﬁcally assigned to him (where ’in the course of his
normal duties’ and ’in the course of duties falling outside his normal duties’ are within the meanings of
the Patents Act 1977, Section 39). Furthermore the article does not contain any proprietary information
or trade secrets of RMS. As a result, the lead author is the owner of all the intellectual property rights
(including, but not limited to, copyright, moral rights, design rights and rights to inventions) associated
with and arising from this article. The lead author reserves all these rights. No-one may reproduce, store
or transmit, in any form or by any means, any part of this article without the author’s prior written
permission. The moral rights of the lead author have been asserted.

References

S Jewson. Comparing the ensemble mean and the ensemble standard deviation as inputs for probabilistic

temperature forecasts. arXiv:physics/0310059, 2003a. Technical report.

S Jewson. Do probabilistic medium-range temperature forecasts need to allow for non-normality?

arXiv:physics/0310060, 2003b. Technical report.

S Jewson. Moment based methods for ensemble assessment and calibration. arXiv:physics/0309042,

2003c. Technical report.

2003d. Technical report.

2003e. Technical report.

S Jewson. A note on the use of the word ’likelihood’ in statistics and meteorology. arXiv:physics/0310020,

S Jewson. Use of the likelihood for measuring the skill of probabilistic forecasts. arXiv:physics/0308046,

S Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble

temperature forecasts. Atmospheric Science Letters, 2003. Submitted.

F Molteni, R Buizza, T Palmer, and T Petroliagis. The ECMWF ensemble prediction system: Method-

ology and validation. Q. J. R. Meteorol. Soc., 122:73–119, 1996.

C
A

I

0
0
5

0
0
4

0
0
3

0
0
2

0
0
1

0

0

2

4

6

8

lead

Figure 1: The AIC scores for probabilistic forecasts made using linear regression (top solid line) and
linear regression with seasonal parameters (lower solid line). The dotted line shows spread regression
with seasonal parameters. Low scores are better.

C
A

I

C
A

I

0
0
5

0
0
3

0
0
1

0

0
0
5

0
0
3

0
0
1

0

C
A

I

C
A

I

0
0
5

0
0
3

0
0
1

0

0
0
5

0
0
3

0
0
1

0

0

2

4

6

8

0

2

4

6

8

lead

lead

0

2

4

6

8

0

2

4

6

8

lead

lead

Figure 2: The two solid lines from ﬁgure 1, along with AIC scores for four more models (dotted lines):
a) seasonal bias, b) seasonal damping, c) seasonal noise and d) non-seasonal spread regression.

C
A

I

C
A

I

0

0
2
−

0
6
−

0
0
1
−

0

0
2
−

0
6
−

0
0
1
−

C
A

I

C
A

I

0

0
2
−

0
6
−

0
0
1
−

0

0
2
−

0
6
−

0
0
1
−

0

2

4

6

8

0

2

4

6

8

lead

lead

0

2

4

6

8

0

2

4

6

8

lead

lead

Figure 3: As ﬁgure 2, but all values shown relative to the AIC score for linear regression.

C
A

I

C
A

I

0
2
−

0
6
−

0
0
1
−

0

5
−

0
1
−

5
1
−

0

2

4

6

8

lead

lead

0

2

4

6

8

Figure 4: The synergy among the seasonal regression parameters, and between seasonality and spread
regression.

l

n
o
i
t
a
e
r
r
o
c
 
r
a
e
n

i
l

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0

2

4

6

8

lead

Figure 5: The linear correlation before (solid line) and after (dotted line) calibration with the seasonal
regression model.

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

2

4

6

8

10

2

4

6

8

10

2

4

6

8

10

lead

lead

lead

2

4

6

8

10

2

4

6

8

10

2

4

6

8

10

lead

lead

lead

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

0
.
1

0
.
0

0
.
1
−

2

4

6

8

10

2

4

6

8

10

2

4

6

8

10

lead

lead

lead

Figure 6: The 9 parameters of the seasonal regression model versus lead time. The top row shows the
alphas, the second row the betas and the third row the gammas.

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

−1.0 −0.5

0.0

0.5

1.0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

0

1
0
0

2
0
0

3
0
0

F
i
g
u
r
e

7
:

T
h
e

s
e
a
s
o
n
a
l

v
a
r
i
a
t
i
o
n

o
f

a
l

p
h
a

p
r
e
d
i
c
t
e
d

b
y

t
h
e

s
e
a
s
o
n
a
l

b
i
a
s

m
o
d
e
l
,

f
o
r

l
e
a
d
s

0

t
o

8
.

